Journal of Artificial Intelligence Research 32 (2008) 565-606

Submitted 11/07; published 06/08

SATzilla: Portfolio-based Algorithm Selection for SAT
Lin Xu
Frank Hutter
Holger H. Hoos
Kevin Leyton-Brown

xulin730@cs.ubc.ca
hutter@cs.ubc.ca
hoos@cs.ubc.ca
kevinlb@cs.ubc.ca

Department of Computer Science
University of British Columbia
201-2366 Main Mall, BC V6T 1Z4, CANADA

Abstract
It has been widely observed that there is no single dominant SAT solver; instead, different
solvers perform best on different instances. Rather than following the traditional approach
of choosing the best solver for a given class of instances, we advocate making this decision online on a per-instance basis. Building on previous work, we describe SATzilla, an
automated approach for constructing per-instance algorithm portfolios for SAT that use socalled empirical hardness models to choose among their constituent solvers. This approach
takes as input a distribution of problem instances and a set of component solvers, and constructs a portfolio optimizing a given objective function (such as mean runtime, percent of
instances solved, or score in a competition). The excellent performance of SATzilla was
independently verified in the 2007 SAT Competition, where our SATzilla07 solvers won
three gold, one silver and one bronze medal. In this article, we go well beyond SATzilla07
by making the portfolio construction scalable and completely automated, and improving
it by integrating local search solvers as candidate solvers, by predicting performance score
instead of runtime, and by using hierarchical hardness models that take into account different types of SAT instances. We demonstrate the effectiveness of these new techniques in
extensive experimental results on data sets including instances from the most recent SAT
competition.

1. Introduction
The propositional satisfiability problem (SAT) is one of the most fundamental problems
in computer science. Indeed, entire conferences and journals are devoted to the study of
this problem, and it has a long history in AI. SAT is interesting both for its own sake
and because instances of other problems in N P can be encoded into SAT and solved by
SAT solvers. This approach has proven effective for tackling many real-world applications,
including planning, scheduling, graph colouring, bounded model checking, and formal verification (examples have been described by Kautz & Selman, 1996, 1999; Crawford & Baker,
1994; van Gelder, 2002; Biere, Cimatti, Clarke, Fujita, & Zhu, 1999; Stephan, Brayton, &
Sangiovanni-Vencentelli, 1996).
The conceptual simplicity of SAT facilitates algorithm development, and considerable
research and engineering efforts over the past decades have led to sophisticated algorithms
with highly-optimized implementations. In fact, SAT is probably the N P-complete decision
problem for which the largest amount of research effort has been expended for the developc
2008
AI Access Foundation. All rights reserved.

fiXu, Hutter, Hoos & Leyton-Brown

ment and study of algorithms. Todays high-performance SAT solvers include tree-search
algorithms (see, e.g., Davis, Logemann, & Loveland, 1962; Zhang, Madigan, Moskewicz, &
Malik, 2001; Zhang, 2002; Kullmann, 2002; Dubois & Dequen, 2001; Heule, Zwieten, Dufour, & Maaren, 2004; Een & Sorensson, 2003), local search algorithms (see, e.g., Selman,
Levesque, & Mitchell, 1992; Selman, Kautz, & Cohen, 1994; Hutter, Tompkins, & Hoos,
2002; Hoos, 2002; Li & Huang, 2005; Ishtaiwi, Thornton, Anbulagan, Sattar, & Pham,
2006; Hoos & Stutzle, 2005), and resolution-based approaches (see, e.g., Davis & Putnam,
1960; Dechter & Rish, 1994; Bacchus, 2002b, 2002a; Bacchus & Winter, 2003; Subbarayan
& Pradhan, 2005).
Most of these SAT algorithms are highly complex, and thus have largely resisted theoretical average-case analysis. Instead, empirical studies are often the only practical means
for assessing and comparing their performance. In one prominent and ongoing example, the
SAT community holds an annual SAT competition (http://www.satcompetition.org; see,
e.g., Le Berre & Simon, 2004). This competition is intended to provide an objective assessment of SAT algorithms, and thus to track the state of the art in SAT solving, to assess and
promote new solvers, and to identify new challenging benchmarks. Solvers are judged based
on their empirical performance on three categories of instances, each of which is further divided into satisfiable, unsatisfiable and mixed instances, with both speed and robustness
taken into account. The competition serves as an annual showcase for the state of the art
in SAT solving; more than 30 solvers were entered in 2007.
1.1 The Algorithm Selection Problem
One way in which evaluations like the SAT competition are useful is that they allow practitioners to determine which algorithm performs best for instances relevant to their problem
domain. However, choosing a single algorithm on the basis of competition performance is
not always a good approachindeed, it is often the case that one solver is better than others at solving some problem instances from a given class, but dramatically worse on other
instances. Thus, practitioners with hard SAT problems to solve face a potentially difficult
algorithm selection problem (Rice, 1976): which algorithm(s) should be run in order to
minimize some performance objective, such as expected runtime?
The most widely-adopted solution to such algorithm selection problems is to measure
every candidate solvers runtime on a representative set of problem instances, and then to
use only the algorithm that offered the best (e.g., average or median) performance. We call
this the winner-take-all approach. Its use has resulted in the neglect of many algorithms
that are not competitive on average but that nevertheless offer very good performance on
particular instances.
The ideal solution to the algorithm selection problem, on the other hand, would be to
consult an oracle that knows the amount of time that each algorithm would take to solve a
given problem instance, and then to select the algorithm with the best performance. Unfortunately, computationally cheap, perfect oracles of this nature are not available for SAT or
any other N P-complete problem; we cannot precisely determine an arbitrary algorithms
runtime on an arbitrary instance without actually running it. Nevertheless, our approach to
algorithm selection is based on the idea of building an approximate runtime predictor, which
can be seen as a heuristic approximation to a perfect oracle. Specifically, we use machine
566

fiSATzilla: Portfolio-based Algorithm Selection for SAT

learning techniques to build an empirical hardness model, a computationally inexpensive
predictor of an algorithms runtime on a given problem instance based on features of the
instance and the algorithms past performance (Nudelman, Leyton-Brown, Hoos, Devkar,
& Shoham, 2004a; Leyton-Brown, Nudelman, & Shoham, 2002). By modeling several algorithms and, at runtime, choosing the algorithm predicted to have the best performance,
empirical hardness models can serve as the basis for an algorithm portfolio that solves the
algorithm selection problem automatically (Leyton-Brown, Nudelman, Andrew, McFadden,
& Shoham, 2003b, 2003a).1
In this work we show, for what we believe to be the first time, that empirical hardness
models can be used to build an algorithm portfolio that achieves state-of-the-art performance in a broad, practical domain. That is, we evaluated our algorithm not under idiosyncratic conditions and on narrowly-selected data, but rather in a large, independentlyconducted competition, confronting a wide range of high-performance algorithms and a
large set of independently-chosen interesting data. Specifically, we describe and analyze
SATzilla, a portfolio-based SAT solver that utilizes empirical hardness models for perinstance algorithm selection.
1.2 Algorithm Portfolios
The term algorithm portfolio was introduced by Huberman, Lukose, and Hogg (1997)
to describe the strategy of running several algorithms in parallel, potentially with different
algorithms being assigned different amounts of CPU time. This approach was also studied
by Gomes and Selman (2001). Several authors have since used the term in a broader way
that encompasses any strategy that leverages multiple black-box algorithms to solve a
single problem instance. Under this view, the space of algorithm portfolios is a spectrum,
with approaches that use all available algorithms at one end and approaches that always
select only a single algorithm at the other. The advantage of using the term portfolio to
refer to this broader class of algorithms is that they all work for the same reasonthey
exploit lack of correlation in the best-case performance of several algorithms in order to
obtain improved performance in the average case.
To more clearly describe algorithm portfolios in this broad sense, we introduce some
new terminology. We define an (a, b)-of-n portfolio as a set of n algorithms and a procedure
for selecting among them with the property that if no algorithm terminates early, at least a
and no more than b algorithms will be executed.2 For brevity, we also use the terms a-of-n
portfolio to refer to an (a, a)-of-n portfolio, and n-portfolio for an n-of-n portfolio. It is
also useful to distinguish how solvers are run after being selected. Portfolios can be parallel
(all algorithms are executed concurrently), sequential (the execution of one algorithm only
begins when the previous algorithms execution has ended), or partly sequential (some
1. Similarly, one could predict the performance of a single algorithm under different parameter settings and
choose the best setting on a per-instance basis. We have previously demonstrated that this approach is
feasible in the case where the number of parameters is small (Hutter, Hamadi, Hoos, & Leyton-Brown,
2006). Ultimately, it is conceivable to combine the two lines of research, and to automatically select a
good algorithm along with good parameter settings on a per-instance basis.
2. The termination condition is somewhat tricky. We consider the portfolio to have terminated early if it
solves the problem before one of the solvers has a chance to run, or if one of the solvers crashes. Thus,
when determining a and b, we do not consider crash-recovery techniques, such as using the next best
predicted solver (discussed later in this paper).

567

fiXu, Hutter, Hoos & Leyton-Brown

combination of the two). Thus the classic algorithm portfolios of Huberman et al. (1997)
and Gomes and Selman (2001) can be described as parallel n-portfolios. In contrast, the
SATzilla solvers that we will present in this paper are sequential 3-of-n portfolios since
they sequentially execute two pre-solvers followed by one main solver.
There is a range of other work in the literature that describes algorithm portfolios in
the broad sense that we have defined here. First, we consider work that has emphasized
algorithm selection (or 1-of-n portfolios). Lobjois and Lematre (1998) studied the problem
of selecting between branch-and-bound algorithms based on an estimate of search tree size
due to Knuth (1975). Gebruers, Hnich, Bridge, and Freuder (2005) employed case-based
reasoning to select a solution strategy for instances of a constraint programming problem.
Various authors have proposed classification-based methods for algorithm selection (e.g.,
Guerri & Milano, 2004; Gebruers, Guerri, Hnich, & Milano, 2004; Guo & Hsu, 2004; and,
to some extent, Horvitz, Ruan, Gomes, Kautz, Selman, & Chickering, 2001). One problem
with such approaches is that they use an error metric that penalizes all misclassifications
equally, regardless of their cost. This is problematic because using a suboptimal algorithm
is acceptable, provided it is nearly as good as the best algorithm. Our SATzilla approach
can be considered to be a classifier with an error metric that depends on the difference in
runtime between algorithms.
At the other end of the spectrum, much work has been done that considers switching
between multiple algorithms, or in our terminology building parallel n-portfolios. Gomes
and Selman (2001) built a portfolio of stochastic algorithms for quasi-group completion
and logistics scheduling problems. Low-knowledge algorithm control by Carchrae and Beck
(2005) employed a portfolio of anytime algorithms, prioritizing each algorithm according
to its performance so far. Gagliolo and Schmidhuber (2006b) learned dynamic algorithm
portfolios that also support running several algorithms at once, where an algorithms priority
depends on its predicted runtime conditioned on the fact that it has not yet found a solution.
Streeter, Golovin, and Smith (2007) improved average-case performance by using black-box
techniques for learning how to interleave the execution of multiple heuristics based not on
instance features but only on the runtime of algorithms.
Some approaches fall between these two extremes, making decisions about which algorithms to use on the flywhile solving a problem instanceinstead of committing in
advance to a subset of algorithms. The examples we give here are (1, n)-of-n portfolios.
Lagoudakis and Littman (2001) employed reinforcement learning to solve an algorithm
selection problem at each decision point of a DPLL solver for SAT in order to select a
branching rule. Similarly, Samulowitz and Memisevic (2007) employed classification to
switch between different heuristics for QBF solving during the search.
Finally, we describe the ways in which this paper builds on our own past work. LeytonBrown et al. (2002) introduced empirical hardness models. Nudelman et al. (2004a) demonstrated that they work on (uniform-random) SAT and introduced the features that we use
here, and Hutter et al. (2006) showed how to apply them to randomized, incomplete algorithms. Empirical hardness models were first used as a basis for algorithm portfolios
by Leyton-Brown et al. (2003b, 2003a). The idea of building such an algorithm portfolio for SAT goes back to 2003, when we submitted the first SATzilla solver to the SAT
competition (Nudelman, Leyton-Brown, Devkar, Shoham, & Hoos, 2004b); this version of
SATzilla placed 2nd in two categories and 3rd in another. In the following, we describe a
568

fiSATzilla: Portfolio-based Algorithm Selection for SAT

substantially improved SATzilla solver, which was entered into the 2007 SAT Competition
anddespite considerable progress in the SAT community over this four year interval
placed 1st in three categories, and 2nd and 3rd in two further categories. This solver was
described, along with some preliminary analysis, in a conference paper (Xu, Hutter, Hoos,
& Leyton-Brown, 2007c); it used hierarchical hardness models, described separately (Xu,
Hoos, & Leyton-Brown, 2007a). In this work, we provide a much more detailed description
of this new solver, present several new techniques that have never been previously published
(chiefly introduced in Section 5) and report new experimental results.
1.3 Overview
Overall, this paper is divided into two parts. The first part describes the development of
SATzilla07, which we submitted to the 2007 SAT Competition and the second part demonstrates our most recent, improved portfolio algorithms (SATzilla07+ and SATzilla07 ).
Each part is subdivided into three sections, the first of which describes our approach for
designing a portfolio-based solver at a high level, the second of which explains lower-level
details of portfolio construction, and the third of which provides the results of an extensive
experimental evaluation.
Section 2 (Design I) begins with a general methodology for building algorithm portfolios
based on empirical hardness models. In this work, we apply these general strategies to SAT
and evaluate them experimentally. In Section 3 (Construction I) we describe the architecture
of the portfolio-based solvers that we entered into the 2007 SAT Competition and described
in our previous work (Xu et al., 2007c). In addition, we constructed a new portfolio-based
solver for INDUSTRIAL instances and analyzed it by effectively re-running the INDUSTRIAL
category of the 2007 competition with our portfolio included; the results of this analysis are
reported in Section 4 (Evaluation I).
We then move on to consider ways of extending, strengthening, and automating our
portfolio construction. We present five new design ideas in Section 5 (Design II), and
consider the incorporation of new solvers and training data in Section 6 (Construction
II). Finally, we evaluated these new ideas and quantified their benefits in a second set of
experiments, which we describe in Section 7 (Evaluation II). Section 8 concludes the paper
with some general observations.

2. Design I: Building Algorithm Portfolios with Empirical Hardness
Models
The general methodology for building an algorithm portfolio that we use in this work
follows Leyton-Brown et al. (2003b) in its broad strokes, but we have made significant
extensions here. Portfolio construction happens offline, as part of algorithm development,
and comprises the following steps.
1. Identify a target distribution of problem instances. Practically, this means selecting a
set of instances believed to be representative of some underlying distribution, or using
an instance generator that constructs instances that represent samples from such a
distribution.
569

fiXu, Hutter, Hoos & Leyton-Brown

2. Select a set of candidate solvers that have relatively uncorrelated runtimes on this
distribution and are known or expected to perform well on at least some instances.
3. Identify features that characterize problem instances. In general this cannot be done
automatically, but rather must reflect the knowledge of a domain expert. To be
usable effectively for automated algorithm selection, these features must be related to
instance hardness and relatively cheap to compute.
4. On a training set of problem instances, compute these features and run each algorithm
to determine its running times.
5. Identify one or more solvers to use for pre-solving instances. These pre-solvers will
later be run for a short amount of time before features are computed (step 9 below),
in order to ensure good performance on very easy instances and to allow the empirical
hardness models to focus exclusively on harder instances.
6. Using a validation data set, determine which solver achieves the best performance
for all instances that are not solved by the pre-solvers and on which the feature
computation times out. We refer to this solver as the backup solver. In the absence of
a sufficient number of instances for which pre-solving and feature computation timed
out, we employ the single best component solver (i.e., the winner-take-all choice) as
a backup solver.
7. Construct an empirical hardness model for each algorithm in the portfolio, which predicts the runtime of the algorithm for each instance, based on the instances features.
8. Choose the best subset of solvers to use in the final portfolio. We formalize and
automatically solve this as a simple subset selection problem: from all given solvers,
select a subset for which the respective portfolio (which uses the empirical hardness
models learned in the previous step) achieves the best performance on the validation
set. (Observe that because our runtime predictions are not perfect, dropping a solver
from the portfolio entirely can increase the portfolios overall performance.)
Then, online, to solve a given problem instance, the following steps are performed.
9. Run each pre-solver until a predetermined fixed cutoff time is reached.
10. Compute feature values. If feature computation cannot be completed for some reason
(error or timeout), select the backup solver identified in step 6 above.
11. Otherwise, predict each algorithms runtime using the empirical hardness models from
step 7 above.
12. Run the algorithm predicted to be the best. If a solver fails to complete its run (e.g.,
it crashes), run the algorithm predicted to be next best.
The effectiveness of an algorithm portfolio built using this approach depends on our
ability to learn empirical hardness models that can accurately predict a solvers runtime
on a given instance using efficiently computable features. In the experiments presented in
570

fiSATzilla: Portfolio-based Algorithm Selection for SAT

this paper, we use the same ridge regression method that has previously proven to be very
successful in predicting runtime on uniform random k-SAT, on structured SAT instances,
and on combinatorial auction winner determination problems (Nudelman et al., 2004a;
Hutter et al., 2006; Leyton-Brown et al., 2002).3
2.1 Ridge Regression and Feature Selection
We now explain the construction of the empirical hardness models described in Step 7 above.
To predict the runtime of an algorithm A on an instance distribution D, we first draw n
instances from D uniformly at random. (In this article, the distributions are given implicitly
by a benchmark set of instances, and we simply use all instances in the benchmark set.)
For each instance i, we compute a set of features xi = [xi,1 , . . . , xi,m ] that characterize the
instance. We also run algorithm A on the instance, recording its runtime ri .
Having computed features and runtimes on all n instances, we fit a function f (x) that,
given the features xi of instance i, yields a prediction, yi , of the logarithm of As runtime
yi = log ri . In our experience, we have found this log transformation of runtime to be very
important due to the large variation in runtimes for hard combinatorial problems. Unfortunately, the performance of learning algorithms can deteriorate when some features are
uninformative or highly correlated with other features; it is difficult to construct features
that do not suffer from these problems. Therefore, we first reduce the set of features by performing feature selection, in our case forward selection (see e.g., Guyon, Gunn, Nikravesh,
& Zadeh, 2006), a simple iterative method that starts with an empty feature set and greedily adds one feature at a time, aiming to reduce cross-validation error as much as possible
with every added feature. Next, we add additional pairwise product features xi,j  xi,k for
j = 1 . . . m and k = j . . . m; this is a widely used method typically referred to as quadratic
basis function expansion. Finally, we perform another pass of forward selection on this
extended set to determine our final set of basis functions, such that for instance i we obtain
an expanded feature vector i = (xi ) = [1 (xi ), . . . , d (xi )], where d is the final number
of basis functions used in the model.
We then use ridge regression (see, e.g., Bishop, 2006) to fit the free parameters w of the
function fw (x). Ridge regression works as follows. Let  be an n  d matrix containing the
vectors i for each instance in the training set, let y be the vector of log runtimes, and let I
be the d  d identity matrix. Finally, let  be a small constant to penalize large coefficients
w and thereby increase numerical stability (we used  = 103 in our experiments). Then,
we compute w = (I + > )1 > y, where > denotes the transpose of matrix . For
a previously unseen instance i, we obtain a prediction of log runtime by computing the
instance features xi and evaluating fw (xi ) = w> (xi ).
3. It should be noted that our portfolio methodology can make use of any regression approach that provides
sufficiently accurate estimates of an algorithms runtime and that is computationally efficient enough that
the time spent making a prediction can be compensated for by the performance gain obtained through
improved algorithm selection. For example, in similar settings, we have previously explored many other
learning techniques, such as lasso regression, SVM regression, and Gaussian process regression (LeytonBrown et al., 2002; Hutter et al., 2006). All of these techniques are computationally more expensive
than ridge regression, and in our previous experiments we found that they did not improve predictive
performance enough to justify this additional cost.

571

fiXu, Hutter, Hoos & Leyton-Brown

2.2 Accounting for Censored Data
As is common with heuristic algorithms for solving N P-complete problems, SAT algorithms
tend to solve some instances very quickly, while taking an extremely long amount of time
to solve other instances. Hence, runtime data can be very costly to gather, as individual
runs can take literally weeks to complete, even when other runs on instances of the same
size take only milliseconds. The common solution to this problem is to censor some runs
by terminating them after a fixed cutoff time.
The question of how to fit good models in the presence of censored data has been extensively studied in the survival analysis literature in statistics, which originated in actuarial
questions such as estimating a persons lifespan given mortality data as well as the ages and
characteristics of other people still alive. Observe that this problem is the same as ours,
except that in our case, data points are always censored at the same value, a subtlety that
turns out not to matter.
The best approach that we know for dealing with censored data is to build models that
use all available information about censored runs by using the censored runtimes as lower
bounds on the actual runtimes. To our knowledge, this technique was first used in the
context of SAT by Gagliolo and Schmidhuber (2006a). We chose the simple, yet effective
method by Schmee and Hahn (1979) to deal with censored samples. In brief, this method
first trains a hardness model treating the cutoff time as the true (uncensored) runtime for
censored samples, and then repeats the following steps until convergence.
1. Estimate the expected runtime of censored runs using the hardness model. Since in
ridge regression, predictions are in fact normal distributions (with a fixed variance),
the expected runtime conditional on the runtime exceeding the cutoff time is the mean
of the corresponding normal distribution truncated at the cutoff time.
2. Train a new hardness model using true runtimes for the uncensored instances and the
predictions generated in the previous step for the censored instances.
In earlier work (Xu, Hutter, Hoos, & Leyton-Brown, 2007b), we experimentally compared
this approach with two other approaches for dealing with censored data: dropping such data
entirely, and treating censored runs as though they had finished at the cutoff threshold.
We demonstrated empirically that both of these methods are significantly worse than the
method presented above. Intuitively, both methods introduce bias into empirical hardness
models, whereas the method by Schmee and Hahn (1979) is unbiased.
2.3 Using Hierarchical Hardness Models
Our previous research on empirical hardness models for SAT showed that we can achieve
better prediction accuracy even with simpler models if we restrict ourselves only to satisfiable or unsatisfiable instances (Nudelman et al., 2004a). Of course, in practice we are
interested in making accurate predictions even when we do not know whether an instance
is satisfiable. In recent work (Xu et al., 2007a), we introduced hierarchical hardness models
as a method for solving this problem. We define the subjective probability that an instance
with features x is satisfiable to be the probability that an instance chosen at random from
the underlying instance distribution with features matching x is satisfiable. Hierarchical
572

fiSATzilla: Portfolio-based Algorithm Selection for SAT

hardness models first use a classifier to predict this subjective probability of satisfiability and
then use this probability, as well as the features x, to combine the predictions of so-called
conditional models, which are trained only on satisfiable instances and only on unsatisfiable instances, respectively. In our previous work we conducted extensive experiments on
various types of SAT instances and found that these hierarchical hardness models achieve
better runtime prediction accuracies than traditional empirical hardness models (Xu et al.,
2007a).
Specifically, we begin by predicting an instances satisfiability using a classification
algorithm that depends on the same instance features used by the empirical hardness
models. We chose Sparse Multinomial Logistic Regression, SMLR (Krishnapuram, Carin,
Figueiredo, & Hartemink, 2005), but any other classification algorithm that returns the
probability that an instance belongs to each class could be used instead. Then, we train
conditional empirical hardness models (Msat , Munsat ) using quadratic basis-function regression for both satisfiable and unsatisfiable training instances.
Next, we must decide how to combine the predictions of these two models.4 We observe
a set of instance features x and a classifier prediction s; our task is to predict the expected
value of the algorithms runtime y given this information. We introduce an additional
random variable z  {sat, unsat}, which represents our subjective belief about an oracles
choice of which conditional model will perform best for a given instance. (Observe that this
may not always correspond to the model trained on the data with the same satisfiability
status as the instance.) We can express the conditional dependence relationships between
our random variables using a graphical model, as illustrated in Figure 1.

x, s

z

y

features &
probability of
being satisfiable

model
selection
oracle

runtime

Figure 1: Graphical model for our mixture-of-experts approach.

Then we can write the expression for the probability distribution over an instances
runtime given the features x and s as
P (y | x, s) =

X

P (z = k | x, s)  PMk (y | x, s),

(1)

k{sat,unsat}

where PMk (y | x, s) is the probability of y evaluated according to model Mk (see Figure 1).
Since the models were fitted using ridge regression, we can rewrite Equation (1) as
4. Note that the classifiers output is not used directly to select a modeldoing so would mean ignoring the
cost of making a mistake. Instead, we use the classifiers output as a feature upon which our hierarchical
model can depend.

573

fiXu, Hutter, Hoos & Leyton-Brown

P (y | x, s) =



X

P (z = k | x, s)  

k{sat,unsat}

y  wk> (x)
k


,

(2)

where () denotes the probability distribution function of a Normal distribution with mean
zero and unit variance, wk are the weights of model Mk , (x) is the quadratic basis function
expansion of x, and k is a fixed standard deviation.
In particular, we are interested in the mean predicted runtime, that is, the expectation
of P (y | x, s):
X
P (z = k | x, s)  wk> (x).
(3)
E(y | x, s) =
k{sat,unsat}

Evaluating this expression would be easy if we knew P (z = k | x, s); of course, we do
not. Our approach is to learn weighting functions P (z = k | x, s) to minimize the following
loss function:
2
n 
X
L=
yi  E(y | x, s) ,
(4)
i=1

where yi is the true log runtime on instance i and n is the number of training instances.
As the hypothesis space for these weighting functions we chose the softmax function
(see, e.g., Bishop, 2006)
>

ev [x;s]
P (z = sat | x, s) =
,
1 + ev> [x;s]

(5)

where v is a vector of free parameters that is set to minimize the loss function (4). This
functional form is frequently used for probabilistic classification tasks: if v > [x; s] is large
>
and positive, then ev [x;s] is much larger than 1 and the resulting probability is close to 1; if
it is large and negative, the result is close to zero; if it is zero, then the resulting probability
is exactly 0.5.
This can be seen as a mixture-of-experts problem (see, e.g., Bishop, 2006) with the experts fixed to Msat and Munsat . (In traditional mixture-of-experts methods, the experts are
allowed to vary during the training process described below.) For implementation convenience, we used an existing mixture of experts implementation to optimize v, which is built
around an expectation maximization (EM) algorithm that performs iterative reweighted
least squares in the M step (Murphy, 2001). We modified this code slightly to fix the
experts and initialized the choice of expert to the classifiers output by setting the initial
values of P (z | x, s) to s. Note that other numerical optimization procedures could be used
to minimize the loss function (4) with respect to v.
Having optimized v, to obtain a runtime prediction for an unseen test instance we
simply compute the instances features x and the classifiers output s, and then compute
the expected runtime by evaluating Equation (3).
Finally, note that these techniques do not require us to restrict ourselves to the conditional models Msat and Munsat , or even to the use of only two models. In Section 7,
we describe hierarchical hardness models that rely on six conditional models, trained on
satisfiable and unsatisfiable instances from different data sets.
574

fiSATzilla: Portfolio-based Algorithm Selection for SAT

3. Construction I: Building SATzilla07 for the 2007 SAT Competition
In this section, we describe the SATzilla07 solvers entered into the 2007 SAT Competition, whichlike previous events in the seriesfeatured three main categories of instances,
RANDOM, HANDMADE (also known as CRAFTED) and INDUSTRIAL. We submitted three versions of SATzilla07 to the competition. Two versions specifically targeted the RANDOM and
HANDMADE instance categories and were trained only on data from their target category.
In order to allow us to study SATzilla07s performance on an even more heterogeneous
instance distribution, a third version of SATzilla07 was trained on data from all three
categories of the competition; we call this new category ALL. We did not construct a version
of SATzilla07 for the INDUSTRIAL category, because of time constraints and the limit of
three submissions per team. However, we built such a version after the submission deadline
and report results for it below.
All of our solvers were built using the design methodology detailed in Section 2. Each
of the following subsections corresponds to one step from this methodology.
3.1 Selecting Instances
In order to train empirical hardness models for any of the above scenarios, we needed
instances that would be similar to those used in the real competition. For this purpose
we used instances from the respective categories of all previous SAT competitions (2002,
2003, 2004, and 2005), as well as from the 2006 SAT Race (which only featured INDUSTRIAL
instances). Instances that were repeated in previous competitions were also repeated in our
data sets. Overall, there were 4 811 instances: 2 300 instances in category RANDOM, 1 490 in
category HANDMADE and 1 021 in category INDUSTRIAL; of course, category ALL included all
of these instances. About 68% of the instances were solved within 1 200 CPU seconds on
our reference machine by at least one of the seven solvers we used (see Section 3.2 below;
the computational infrastructure used for our experiments is described in Section 3.4). All
instances that were not solved by any of these solvers were dropped from our data set.
We randomly split our data set into training, validation and test sets at a ratio of
40:30:30. All parameter tuning and intermediate model testing was performed on the validation set; the test set was used only to generate the final results reported here.
In this section, we use the same SATzilla07 methodology for building multiple portfolios
for different sets of benchmark instances. In order to avoid confusion between the changes
to our overall methodology discussed later and differences in training data, we treat the
data set as an input parameter for SATzilla. For the data set comprising the previously
mentioned RANDOM instances, we write Dr ; similarly, we write Dh for HANDMADE and Di for
INDUSTRIAL; for ALL, we simply write D.
3.2 Selecting Solvers
To decide what algorithms to include in our portfolio, we considered a wide variety of solvers
that had been entered into previous SAT competitions and into the 2006 SAT Race. We
manually analyzed the results of these competitions, identifying all algorithms that yielded
the best performance on some subset of instances. Since our focus was on both satisfiable
and unsatisfiable instances, and since we were concerned about the cost of misclassifications,
575

fiXu, Hutter, Hoos & Leyton-Brown

we did not choose any incomplete algorithms at this stage; however, we revisit this issue in
Section 5.4. In the end, we selected the seven high-performance solvers shown in Table 1 as
candidates for the SATzilla07 portfolio. Like the data set used for training, we treat the
component solvers as an input to SATzilla, and denote the set of solvers from Table 1 as
S.
Solver

Reference

Eureka
Kcnfs06
March dl04
Minisat 2.0
Rsat 1.03
Vallst
Zchaff Rand

Nadel, Gordon, Palti, and Hanna (2006)
Dubois and Dequen (2001)
Heule et al. (2004)
Een and Sorensson (2006)
Pipatsrisawat and Darwiche (2006)
Vallstrom (2005)
Mahajan, Fu, and Malik (2005)

Table 1: The seven solvers in SATzilla07; we refer to this set of solvers as S.

In previous work (Xu et al., 2007b), we considered using the Hypre preprocessor (Bacchus & Winter, 2003) before applying one of SATzillas component solvers; this effectively
doubled our number of component solvers. For this work, we re-evaluated this option and
found performance to basically remain unchanged without preprocessing (performance differences in terms of instances solved, runtime, and SAT competition score were smaller
than 1%, and even this small difference was not consistently in favor of using the Hypre
preprocessor). For this reason, we dropped preprocessing in the work reported here.
3.3 Choosing Features
The choice of instance features has a significant impact on the performance of empirical
hardness models. Good features need to correlate well with (solver-specific) instance hardness and need to be cheap to compute, since feature computation time counts as part of
SATzilla07s runtime.
Nudelman et al. (2004a) introduced 84 features for SAT instances. These features can
be classified into nine categories: problem size, variable-clause graph, variable graph, clause
graph, balance, proximity to Horn formulae, LP-based, DPLL probing and local search
probing features  the code for this last group of features was based on UBCSAT (Tompkins
& Hoos, 2004). In order to limit the time spent computing features, we slightly modified
the feature computation code of Nudelman et al. (2004a). For SATzilla07, we excluded a
number of computationally expensive features, such as LP-based and clause graph features.
The computation time for each of the local search and DPLL probing features was limited
to 1 CPU second, and the total feature computation time per instance was limited to 60
CPU seconds. After eliminating some features that had the same value across all instances
and some that were too unstable given only 1 CPU second of local search probing, we ended
up using the 48 raw features summarized in Figure 2.
576

fiSATzilla: Portfolio-based Algorithm Selection for SAT

Proximity to Horn Formula:
28. Fraction of Horn clauses
29-33. Number of occurrences in a Horn clause for
each variable: mean, variation coefficient, min, max
and entropy.

Problem Size Features:
1. Number of clauses: denoted c
2. Number of variables: denoted v
3. Ratio: c/v
Variable-Clause Graph Features:
4-8.
Variable nodes degree statistics: mean,
variation coefficient, min, max and entropy.
9-13. Clause nodes degree statistics: mean, variation coefficient, min, max and entropy.

DPLL Probing Features:
34-38. Number of unit propagations: computed at
depths 1, 4, 16, 64 and 256.
39-40. Search space size estimate: mean depth to
contradiction, estimate of the log of number of nodes.

Variable Graph Features:
14-17. Nodes degree statistics: mean, variation
coefficient, min and max.

Local Search Probing Features:
41-44. Number of steps to the best local minimum
in a run: mean, median, 10th and 90th percentiles for
SAPS.
45. Average improvement to best in a run: mean
improvement per step to best solution for SAPS.
46-47. Fraction of improvement due to first local
minimum: mean for SAPS and GSAT.
48. Coefficient of variation of the number of unsatisfied clauses in each local minimum: mean over
all runs for SAPS.

Balance Features:
18-20. Ratio of positive and negative literals in each
clause: mean, variation coefficient and entropy.
21-25. Ratio of positive and negative occurrences of
each variable: mean, variation coefficient, min, max
and entropy.
26-27. Fraction of binary and ternary clauses

Figure 2: The features used for building SATzilla07; these were originally introduced and described
in detail by Nudelman et al. (2004a).

3.4 Computing Features and Runtimes
All our experiments were performed using a computer cluster consisting of 55 machines with
dual Intel Xeon 3.2GHz CPUs, 2MB cache and 2GB RAM, running Suse Linux 10.1. As in
the SAT competition, all runs of any solver that exceeded a certain runtime were aborted
(censored) and recorded as such. In order to keep the computational cost manageable, we
chose a cutoff time of 1 200 CPU seconds.
3.5 Identifying Pre-solvers
As described in Section 2, in order to solve easy instances quickly without spending any time
for the computation of features, we use one or more pre-solvers: algorithms that are run
unconditionally but briefly before features are computed. Good algorithms for pre-solving
solve a large proportion of instances quickly. Based on an examination of the training
runtime data, we chose March dl04 and the local search algorithm SAPS (Hutter et al., 2002)
as pre-solvers for RANDOM, HANDMADE and ALL; for SAPS, we used the UBCSAT implementation
(Tompkins & Hoos, 2004) with the best fixed parameter configuration identified by Hutter
et al. (2006). (Note that while we did not consider incomplete algorithms for inclusion in
the portfolio, we did use one here.)
Within 5 CPU seconds on our reference machine, March dl04 solved 47.8%, 47.7%, and
43.4% of the instances in our RANDOM, HANDMADE and ALL data sets, respectively. For the
remaining instances, we let SAPS run for 2 CPU seconds, because we found its runtime to be
almost completely uncorrelated with March dl04 (Pearson correlation coefficient r = 0.118
577

fiXu, Hutter, Hoos & Leyton-Brown

Solver
Eureka
Kcnfs06
March dl04
Minisat 2.0
Rsat 1.03
Vallst
Zchaff Rand

RANDOM
Time Solved

HANDMADE
Time Solved

INDUSTRIAL
Time Solved

Time

ALL
Solved

770
319
269
520
522
757
802

561
846
311
411
412
440
562

330
1050
715
407
345
582
461

598
658
394
459
445
620
645

57%
50%
73%
69%
70%
54%
51%

40%
81%
85%
62%
62%
40%
36%

59%
33%
80%
73%
72%
67%
58%

84%
13%
42%
76%
81%
59%
71%

Table 2: Average runtime (in CPU seconds on our reference machine) and percentage of instances
solved by each solver for all instances that were solved by at least one of our seven component solvers within the cutoff time of 1 200 seconds.

for the 487 remaining instances solved by both solvers). In this time, SAPS solved 28.8%,
5.3%, and 14.5% of the remaining RANDOM, HANDMADE and ALL instances, respectively. For
the INDUSTRIAL category, we chose to run Rsat 1.03 for 2 CPU seconds as a pre-solver,
which resulted in 32.0% of the instances in our INDUSTRIAL set being solved. Since SAPS
solved less than 3% of the remaining instances within 2 CPU seconds, it was not used as a
pre-solver in this category.
3.6 Identifying the Backup Solver
The performance of all our solvers from Table 1 is reported in Table 2. We computed
average runtime (here and in the remainder of this work) counting timeouts as runs that
completed at the cutoff time of 1 200 CPU seconds. As can be seen from this data, the
best single solver for ALL, RANDOM and HANDMADE was always March dl04. For categories
RANDOM and HANDMADE, we did not encounter instances for which the feature computation
timed out. Thus, we employed the winner-take-all solver March dl04 as a backup solver
in both of these domains. For categories INDUSTRIAL and ALL, Eureka performed best on
those instances that remained unsolved after pre-solving and for which feature computation
timed out; we thus chose Eureka as the backup solver.
3.7 Learning Empirical Hardness Models
We learned empirical hardness models for predicting each solvers runtime as described in
Section 2, using the procedure of Schmee and Hahn (1979) for dealing with censored data
and also employing hierarchical hardness models.
3.8 Solver Subset Selection
We performed automatic exhaustive subset search as outlined in Section 2 to determine
which solvers to include in SATzilla07. Table 3 describes the solvers that were selected for
each of our four data sets.
578

fiSATzilla: Portfolio-based Algorithm Selection for SAT

Data Set
RANDOM
HANDMADE
INDUSTRIAL
ALL

Solvers used in SATzilla07
March dl04, Kcnfs06, Rsat 1.03
Kcnfs06, March dl04, Minisat 2.0, Rsat 1.03, Zchaff Rand
Eureka, March dl04, Minisat 2.0, Rsat 1.03
Eureka, Kcnfs06, March dl04, Minisat 2.0, Zchaff Rand

Table 3: Results of subset selection for SATzilla07.

4. Evaluation I: Performance Analysis of SATzilla07
In this section, we evaluate SATzilla07 for our four data sets. Since we use the SAT Competition as a running example throughout this paper, we start by describing how SATzilla07
fared in the 2007 SAT Competition. We then describe more comprehensive evaluations of
each SATzilla07 version in which we compared it in greater detail against its component
solvers.
4.1 SATzilla07 in the 2007 SAT Competition
We submitted three versions of SATzilla07 to the 2007 SAT Competition, namely
SATzilla07(S,Dr ) (i.e., SATzilla07 using the seven component solvers from Table 1 and trained on RANDOM instances) SATzilla07(S,Dh ) (trained on HANDMADE), and
SATzilla07(S,D) (trained on ALL). Table 4 shows the results of the 2007 SAT Competition for the RANDOM and HANDMADE categories. In the RANDOM category, SATzilla07(S,Dr )
won the gold medal in the subcategory SAT+UNSAT, and came third in the UNSAT subcategory. The SAT subcategory was dominated by local search solvers. In the HANDMADE
category, SATzilla07(S,Dh ) showed excellent performance, winning the SAT+UNSAT and
UNSAT subcategories, and placing second in the SAT subcategory.
Category

Rank

SAT & UNSAT

SAT

UNSAT

RANDOM

1st
2nd
3rd

SATzilla07(S,Dr )
March ks
Kcnfs04

Gnovelty+
Ag2wsat0
Ag2wsat+

March ks
Kcnfs04
SATzilla07(S,Dr )

HANDMADE

1st
2nd
3rd

SATzilla07(S,Dh )
Minisat07
MXC

March ks
SATzilla07(S,Dh )
Minisat07

SATzilla07(S,Dh )
TTS
Minisat07

Table 4: Results from the 2007 SAT Competition. More that 30 solvers competed in each
category.

Since the general portfolio SATzilla07(S,D) included Eureka (whose source code is not
publicly available) it was run in the demonstration division only. The official competition
results (available at http://www.cril.univ-artois.fr/SAT07/) show that this solver,
which was trained on instances from all three categories, performed very well, solving more
579

fiXu, Hutter, Hoos & Leyton-Brown

Solver

Average runtime [s]

Solved percentage

Performance score

Picosat
TinisatElite
Minisat07
Rsat 2.0

398
494
484
446

82
71
72
75

31484
21630
34088
23446

SATzilla07(S,Di )

346

87

29552

Table 5: Performance comparison of SATzilla07(S,Di ) and winners of the 2007 SAT Competition
in the INDUSTRIAL category. The performance scores are computed using the 2007 SAT
Competition scoring function with a cutoff time of 1 200 CPU seconds. SATzilla07(S,Di )
is exactly the same solver as shown in Figure 6 and was trained without reference to the
2007 SAT Competition data.

instances of the union of the three categories than any other solver (including the other two
versions of SATzilla07).
We did not submit a version of SATzilla07 to the 2007 SAT Competition that was
specifically trained on instances from the INDUSTRIAL category. However, we constructed
such a version, SATzilla07(S,Di ), after the submission deadline and here report on its
performance. Although SATzilla07(S,Di ) did not compete in the actual 2007 SAT Competition, we can approximate how well it would have performed in a simulation of the
competition, using the same scoring function as in the competition (described in detail
in Section 5.3), based on a large number of competitors, namely the 19 solvers listed in
Tables 1, 9 and 10, plus SATzilla07(S,Di ).
Table 5 compares the performance of SATzilla07 in this simulation of the competition
against all solvers that won at least one medal in the INDUSTRIAL category of the 2007
SAT Competition: Picosat, TinisatElite, Minisat07 and Rsat 2.0. There are some
differences between our test environment and that used in the real SAT competition: our
simulation ran on different machines and under a different operating system; it also used a
shorter cutoff time and fewer competitors to evaluate the solvers performance scores. Because of these differences, the ranking of solvers in our simulation is not necessarily the same
as it would have been in the actual competition. Nevertheless, our results leave no doubt
that SATzilla07 can compete with the state-of-the-art SAT solvers in the INDUSTRIAL
category.
4.2 Feature Computation
The actual time required to compute our features varied from instance to instance. In the
following, we report runtimes for computing features for the instance sets defined in Section
3.1. Typically, feature computation took at least 3 CPU seconds: 1 second each for local
search probing with SAPS and GSAT, and 1 further second for DPLL probing. However, for
some small instances, the limit of 300 000 local search steps was reached before one CPU
second had passed, resulting in feature computation times lower than 3 CPU seconds. For
most instances from the RANDOM and HANDMADE categories, the computation of the other
features took an insignificant amount of time, resulting in feature computation times just
580

fi100

100

80

80

% Instances Finished

% Instances Finished

SATzilla: Portfolio-based Algorithm Selection for SAT

60
40
20
0
0

1

2
3
Feature Time [CPU sec]

4

5

60
40
20
0
0

10

20
30
40
Feature Time [CPU sec]

50

60

Figure 3: Variability in feature computation times. The y-axis denotes the percentage of instances
for which feature computation finished in at most the time given by the x-axis. Left:
RANDOM, right: INDUSTRIAL. Note the different scales of the x-axes.

above 3 CPU seconds. However, for many instances from the INDUSTRIAL category, the
feature computation was quite expensive, with times up to 1 200 CPU seconds for some
instances. We limited the feature computation time to 60 CPU seconds, which resulted
in time-outs for 19% of the instances from the INDUSTRIAL category (but for no instances
from the other categories). For instances on which the feature computation timed out, the
backup solver was used.
Figure 3 illustrates this variation in feature computation time. For category RANDOM
(left pane), feature computation never took significantly longer than three CPU seconds.
In contrast, for category INDUSTRIAL, there was a fairly high variation, with the feature
computation reaching the cut-off time of 60 CPU seconds for 19% of the instances. The
average total feature computation times for categories RANDOM, HANDMADE, and INDUSTRIAL
were 3.01, 4.22, and 14.4 CPU seconds, respectively.
4.3 RANDOM Category
For each category, we evaluated our SATzilla portfolios by comparing them against the
best solvers from Table 1 for the respective category. Note that the solvers in this table are
exactly the candidate solvers used in SATzilla.
Figure 4 shows the performance of SATzilla07 and the top three single solvers from
Table 1 on category RANDOM. Note that we count the runtime of pre-solving, as well as the
feature computation time, as part of SATzillas runtime. Oracle(S) provides an upper
bound on the performance that could be achieved by SATzilla: its runtime is that of a
hypothetical version of SATzilla that makes every decision in an optimal fashion, and
without any time spent computing features. Furthermore, it can also choose not to run
the pre-solvers on an instance. Essentially, Oracle(S) thus indicates the performance that
would be achieved by only running the best algorithm for each single instance. In Figure 4
(right), the horizontal line near the bottom of the plot shows the time SATzilla07(S,Dr )
allots to pre-solving and (on average) to feature computation.
581

fiXu, Hutter, Hoos & Leyton-Brown

100

500

% Instances Solved

Average Runtime [CPU sec]

600

400
300
200

90

Oracle(S)
SATzilla07(S,Dr)

80

Kcnfs06
March_dl04
Rsat1.03

70
60
50
40
30
20

100

10
Presolving

0

K

06

fs
cn

dl0
h_

4

rc
Ma

03
t1.
sa

R

07
illa
Tz
A
S

ac
Or

le

AvgFeature

0 1
10

0

10

1

10

2

10

3

10

Runtime [CPU sec]

Figure 4: Left: Average runtime, right: runtime cumulative distribution function (CDF) for dif-

500

100

450

90

Oracle(S)
SATzilla07(S,Dh)

400

80

March_dl04
Minisat2.0
Vallst

% Instances Solved

Average Runtime [CPU sec]

ferent solvers on RANDOM; the average feature computation time was 2.3 CPU seconds
(too insignificant to be visible in SATzilla07s runtime bar). All other solvers CDFs are
below the ones shown here (i.e., at each given runtime the maximum of the CDFs for the
selected solvers is an upper bound for the CDF of any of the solvers considered in our
experiments).

350
300
250
200
150
100

70
60
50
40
30
20

50

10

0

h_

4
dl0

rc
Ma

nis
Mi

.0
at2

V

st
all

SA

T

a
zill

07

0 1
10

cle

O

ra

Presolving

AvgFeature
0

10

1

10

2

10

3

10

Runtime [CPU sec]

Figure 5: Left: Average runtime, right: runtime CDF for different solvers on HANDMADE; the average feature computation time was 4.5 CPU seconds (shown as a white box on top of
SATzilla07s runtime bar). All other solvers CDFs are below the ones shown here.

Overall, SATzilla07(S,Dr ) achieved very good performance on data set RANDOM: It was
more than three times faster on average than its best component solver, March dl04 (see
Figure 4, left), and also dominated it in terms of fraction of instances solved, solving 20%
more instances within the cutoff time (see Figure 4, left). The runtime CDF plot also shows
that the local-search-based pre-solver SAPS helped considerably by solving more than 20%
of instances within 2 CPU seconds (this is reflected in the sharp increase in solved instances
just before feature computation begins).
582

fi450

100

400

90

350

80

% Instances Solved

Average Runtime [CPU sec]

SATzilla: Portfolio-based Algorithm Selection for SAT

300
250
200
150
100

70

Oracle(S)
SATzilla07(S,D)
i

Eureka
Minisat2.0
Rsat1.03

60
50
40
30
20

50

10
Presolving

0

ka
ure

E

nis

.0
at2

Mi

.03

Rs

at1

SA

T

7
a0
zill

cle

0 1
10

a
Or

AvgFeature
0

10

1

10

2

10

Figure 6: Left: Average runtime, right: runtime CDF for different solvers on INDUSTRIAL; the
average feature computation time was 14.1 CPU seconds (shown as a white box on top
of SATzilla07s runtime bar). All other solvers CDFs are below the ones shown here.

4.4 HANDMADE Category
SATzilla07s performance results for the HANDMADE category were also very good. Using the
five component solvers listed in Table 3, its average runtime was about 45% less than that
of the best single component solver (see Figure 5, left). The CDF plot in Figure 5 (right)
shows that SATzilla07 dominated all its components and solved 13% more instances than
the best non-portfolio solver.
4.5 INDUSTRIAL Category
We performed the same experiment for INDUSTRIAL instances as for the other categories in
order to study SATzilla07s performance compared to its component solvers. SATzilla07
was more than 23% faster on average than the best component solver, Eureka (see Figure 6
(left)). Moreover, Figure 6 (right) shows that SATzilla07 also solved 9% more instances
than Eureka within the cutoff time of 1 200 CPU seconds. Note that in this category, the
feature computation timed out on 15.5% of the test instances after 60 CPU seconds; Eureka
was used as a backup solver in those cases.
4.6 ALL
For our final category, ALL, a heterogeneous category that included the instances from all the
above categories, a portfolio approach is especially appealing. SATzilla07 performed very
well in this category, with an average runtime of less than half that of the best single solver,
March dl04 (159 vs. 389 CPU seconds, respectively). It also solved 20% more instances than
any non-portfolio solver within the given time limit of 1 200 CPU seconds (see Figure 7).
583

3

10

Runtime [CPU sec]

fi500

100

450

90

400

80

% Instances Solved

Average Runtime [CPU sec]

Xu, Hutter, Hoos & Leyton-Brown

350
300
250
200
150
100

70
60
50
40
30
20

50

10
Presolving

0

4
dl0

_
rch

Ma

Oracle(S)
SATzilla07(S,D)
March_dl04
Minisat2.0
Rsat1.03

nis

.0
at2

Mi

Rs

.03
at1

SA

07
illa
Tz

le
rac

0 1
10

O

AvgFeature
0

10

1

10

2

10

3

10

Runtime [CPU sec]

Figure 7: Left: Average runtime; right: runtime CDF for different solvers on ALL; the average feature computation time was 6.7 CPU seconds (shown as a white box on top of
SATzilla07s runtime bar). All other solvers CDFs are below the ones shown here.

4.7 Classifier Accuracy
The satisfiability status classifiers trained on the various data sets were surprisingly effective
in predicting satisfiability of RANDOM and INDUSTRIAL instances, where they reached classification accuracies of 94% and 92%, respectively. For HANDMADE and ALL, the classification
accuracy was still considerably better than random guessing, at 70% and 78%, respectively.
Interestingly, our classifiers more often misclassified unsatisfiable instances as SAT than
satisfiable instances as UNSAT. This effect can be seen from the confusion matrices in
Table 6; it was most pronounced in the HANDMADE category, where the overall classification
quality was also lowest: 40% of the HANDMADE instances that were classified as SAT are in
fact unsatisfiable, while only 18% of the instances that were classified as UNSAT are in fact
satisfiable.

5. Design II: SATzilla Beyond 2007
Despite SATzilla07s success in the 2007 SAT Competition, there was still room for improvement. This section describes a number of design enhancements over SATzilla07 that
underly the new SATzilla versions, SATzilla07+ and SATzilla07 , which we describe in
detail in Section 6 and evaluate experimentally in Section 7.
5.1 Automatically Selecting Pre-solvers
In SATzilla07, we identified pre-solvers and their cutoff times manually. There are several
limitations to this approach. First and foremost, manual pre-solver selection does not
scale well. If there are many candidate solvers, manually finding the best combination of
pre-solvers and cutoff times can be difficult and requires significant amounts of valuable
human time. In addition, the manual pre-solver selection we performed for SATzilla07
concentrated solely on solving a large number of instances quickly and did not take into
584

fiSATzilla: Portfolio-based Algorithm Selection for SAT

satisfiable

unsatisfiable

classified SAT

91%

9%

classified UNSAT

5%

95%

satisfiable

unsatisfiable

classified SAT

60%

40%

classified UNSAT

18%

82%

RANDOM data set

HANDMADE data set

satisfiable

unsatisfiable

classified SAT

81%

19%

classified UNSAT

5%

95%

INDUSTRIAL data set

satisfiable

unsatisfiable

classified SAT

65%

35%

classified UNSAT

12%

88%
ALL data set

Table 6: Confusion matrices for the satisfiability status classifier on data sets RANDOM, HANDMADE,
INDUSTRIAL and ALL.

account the pre-solvers effect on model learning. In fact, there are three consequences of
pre-solving.
1. Pre-solving solves some instances quickly before features are computed. In the context
of the SAT competition, this improves SATzillas scores for easy problem instances
due to the speed purse component of the SAT competition scoring function. (See
Section 5.3 below.)
2. Pre-solving increases SATzillas runtime on instances not solved during pre-solving
by adding the pre-solvers time to every such instance. Like feature computation itself,
this additional cost reduces SATzillas scores.
3. Pre-solving filters out easy instances, allowing our empirical hardness models to be
trained exclusively on harder instances.
While we considered (1) and (2) in our manual selection of pre-solvers, we did not consider
(3), namely the fact that the use of different pre-solvers and/or cutoff times results in different training data and hence in different learned models, which can also affect a portfolios
effectiveness.
Our new automatic pre-solver selection technique works as follows. We committed in
advance to using a maximum of two pre-solvers: one of three complete search algorithms
and one of three local search algorithms. The three candidates for each of the search
approaches are automatically determined for each data set as those with highest score on
the validation set when run for a maximum of 10 CPU seconds. We also use a number
of possible cutoff times, namely 2, 5 and 10 CPU seconds, as well as 0 seconds (i.e., the
pre-solver is not run at all) and consider both orders in which the two pre-solvers can be
run. For each of the resulting 288 possible combinations of two pre-solvers and cutoff times,
SATzillas performance on the validation data is evaluated by performing steps 6, 7 and 8
of the general methodology presented in Section 2:
585

fiXu, Hutter, Hoos & Leyton-Brown

6. determine the backup solver for selection when features time out;
7. construct an empirical hardness model for each algorithm; and
8. automatically select the best subset of algorithms to use as part of SATzilla.
The best-performing subset found in this last stepevaluated on validation datais selected as the algorithm portfolio for the given combination of pre-solver / cutoff time
pairs. Overall, this method aims to choose the pre-solver configuration that yields the
best-performing portfolio.
5.2 Randomized Solver Subset Selection for a Large Set of Component Solvers
Our methodology from Section 2 relied on exhaustive subset search for choosing the best
combination of component solvers. For a large number of component solvers, this is impossible (N component solvers would require the consideration of 2N solver sets, for each of which
a model would have to be trained). The automatic pre-solver selection methods described
in Section 5.1 above further worsens this situation: solver selection must be performed for
every candidate configuration of pre-solvers, because new pre-solver configurations induce
new models.
As an alternative to exhaustively considering all subsets, we implemented a randomized
iterative improvement procedure to search for a good subset of solvers. The local search
neighbourhood used by this procedure consists of all subsets of solvers that can be reached
by adding or dropping a single component solver. Starting with a randomly selected subset
of solvers, in each search step, we consider a neighbouring solver subset selected uniformly
at random and accept it if validation set performance increases; otherwise, we accept the
solver subset anyway with a probability of 5%. Once 100 steps have been performed with
no improving step, a new run is started by re-initializing the search at random. After 10
such runs, the search is terminated and the best subset of solvers encountered during the
search process is returned. Preliminary evidence suggests that this local search procedure
efficiently finds very good subsets of solvers.
5.3 Predicting Performance Score Instead of Runtime
Our general portfolio methodology is based on empirical hardness models, which predict an
algorithms runtime. However, one might not simply be interested in using a portfolio to pick
the solver with the lowest expected runtime. For example, in the SAT competition, solvers
are evaluated based on a complex scoring function that depends only partly on a solvers
runtime. Although the idiosyncracies of this scoring function are somewhat particular to the
SAT competition, the idea that a portfolio should be built to optimize a performance score
more complex than runtime has wide applicability. In this section we describe techniques
for building models that predict such a performance score directly.
One key issue is thatas long as we depend on standard supervised learning methods
that require independent and identically distributed training datawe can only deal easily
with scoring functions that actually associate a score with each single instance and combine
the partial scores of all instances to compute the overall score. Given training data labeled
with such a scoring function, SATzilla can simply learn a model of the score (rather than
586

fiSATzilla: Portfolio-based Algorithm Selection for SAT

runtime) and then choose the solver with highest predicted score. Unfortunately, the scoring
function used in the 2007 SAT Competition does not satisfy this independence property: the
score a solver attains for solving a given instance depends in part on its (and, indeed, other
solvers) performance on other, similar instances. More specifically, in the SAT competition
each instance P has a solution purse SolutionP and a speed purse SpeedP ; all instances in
a given series (typically 540 similar instances) share one series purse SeriesP . Algorithms
are ranked by summing three partial scores derived from these purses.
1. For each problem instance P , its solution purse is equally distributed between the
solvers Si that solve the instance within the cutoff time (thereby rewarding robustness
of a solver).
2. The speed purse for P is divided among a set of solvers S that solved the instance as
SF(P,Si )
timeLimit(P )
Score(P, Si ) = SpeedP
, where the speed factor SF(P, S) = 1+timeUsed(P,S)
is a
j SF(P,Sj )
measure of speed that discounts small absolute differences in runtime.
3. The series purse for each series is divided equally and distributed between the solvers
Si that solved at least one instance in that series.5
Si s partial score from problem P s solution and speed purses solely depends on the solvers
own runtime for P and the runtime of all competing solvers for P . Thus, given the runtimes
of all competing solvers as part of the training data, we can compute the score contributions
from the solution and speed purses of each instance P , and these two components are
independent across instances. In contrast, since a solvers share of the series purse will
depend on its performance on other instances in the series, its partial score received from
the series purse for solving one instance is not independent of its performance on other
instances.
Our solution to this problem is to approximate an instances share of the series purse
score by an independent score. If N instances in a series are solved by any of SATzillas
component solvers, and if n solvers solve at least one of the instances in that series, we assign
a partial score of SeriesP/(N  n) to each solver Si (where i = 1, . . . , n) for each instance
in the series it solved. This approximation of a non-independent score as independent is
not always perfect, but it is conservative because it defines a lower-bound on the partial
score from the series purse. Predicted scores will only be used in SATzilla to choose
between different solvers on a per-instance basis. Thus, the partial score of a solver for an
instance should reflect how much it would contribute to SATzillas score. If SATzilla were
perfect (i.e., for each instance, it always selected the best algorithm) our score approximation
would be correct: SATzilla would solve all N instances from the series that any component
solver can solve, and thus would actually achieve the series score SeriesP/(N  n)  N =
SeriesP/n. If SATzilla performed very poorly and did not solve any instance in the series,
our approximation would also be exact, since it would estimate the partial series score as
zero. Finally, if SATzilla were to pick successful solvers for some (say, M ) but not all
instances of the series that can be solved by its component solvers (i.e., M < N ), we would
underestimate the partial series purse, since SeriesP/(N  n)  M < SeriesP/n.
5. See http://www.satcompetition.org/2007/rules07.html for further details.

587

fiXu, Hutter, Hoos & Leyton-Brown

While our learning techniques require an approximation of the performance score as
an independent score, our experimental evaluations of solvers scores employ the actual
SAT competition scoring function. As explained previously, in the SAT competition, the
performance score of a solver depends on the score of all other solvers in the competition. In
order to simulate a competition, we select a large number of solvers and pretend that these
reference solvers and SATzilla are the only solvers in the competition; throughout our
analysis we use the 19 solvers listed in Tables 1, 9 and 10. This is not a perfect simulation,
since the scores change somewhat when different solvers are added to or removed from
the competition. However, we obtain much better approximations of performance score by
following the methodology outlined here than by using cruder measures, such as learning
models to predict mean runtime or the numbers of benchmark instances solved.
Finally, predicting performance score instead of runtime has a number of implications
for the components of SATzilla. First, notice that we can compute an exact score for each
algorithm and instance, even if the algorithm times out unsuccessfully or crashesin these
cases, the score from all three components is simply zero. When predicting scores instead
of runtimes, we thus do not need to rely on censored sampling techniques (see Section 2.2)
anymore. Secondly, notice that the oracles for maximizing SAT competition score and for
minimizing runtime are identical, since always using the solver with the smallest runtime
guarantees that the highest values in all three components are obtained.
5.4 Introducing Local Search Solvers into SATzilla
SAT solvers based on local search are well known to be effective on certain classes of
satisfiable instances. In fact, there are classes of hard random satisfiable instances that
only local search solvers can solve in a reasonable amount of time (Hoos & Stutzle, 2005).
However, all high-performance local-search-based SAT solvers are incomplete and cannot
solve unsatisfiable instances. In previous versions of SATzilla we avoided using local search
algorithms because of the risk that we would select them for unsatisfiable instances, where
they would run uselessly until reaching the cutoff time.
When we shift to predicting and optimizing performance score instead of runtime, this
issue turns out not to matter anymore. Treating every solver as a black box, local search
solvers always get a score of exactly zero on unsatisfiable instances since they are guaranteed
not to solve them within the cutoff time. (Of course, they do not need to be run on an
instance during training if the instance is known to be unsatisfiable.) Hence, we can build
models for predicting the score of local search solvers using exactly the same methods as
for complete solvers.
5.5 More General Hierarchical Hardness Models
Our benchmark set ALL consists of all instances from the categories RANDOM, HANDMADE and
INDUSTRIAL. In order to further improve performance on this very heterogeneous instance
distribution, we extend our previous hierarchical hardness model approach (predicting satisfiability status and then using a mixture of two conditional models) to the more general
scenario of six underlying empirical hardness models (one for each combination of category
and satisfiability status). The output of the general hierarchical model is a linear weighted
combination of the output of each component. As described in Section 2.3, we can approx588

fiSATzilla: Portfolio-based Algorithm Selection for SAT

Old instances from before 2007

New instances from 2007

To (1 925 instances)
Vo (1 443 instances)
Eo (1 443 instances)

Tn (347 instances)
Vn (261 instances)
En (261 instances)

Training (40%)
Validation (30%)
Test (30%)

Table 7: Instances from before 2007 and from 2007 randomly split into training (T), validation (V)
and test (E) data sets. These sets include instances for all categories: RANDOM, HANDMADE
and INDUSTRIAL.

Data set

Training

Validation

Test

D
D0
D+

To
To
To  Tn

Vo
Vo
Vo  Vn

Eo
Eo  En
Eo  En

Table 8: Data sets used in our experiments. D was used in our first series of experiments in
Section 4, D0 and D+ are used in our second series of experiments. Note that data sets
D and D0 use identical training and validation data, but different test data.

imate the model selection oracle by a softmax function whose parameters are estimated
using EM.

6. Construction II: Building the Improved SATzilla Versions
In this section we describe the construction of new SATzilla versions that incorporate new
design elements from the previous section. We also describe two versions based on our old
design, which we use to evaluate the impact of our changes.
6.1 Benchmark Instances
In addition to all instances used in Section 3.1, we added the 869 instances from the 2007
SAT Competition into our four data sets. Overall, this resulted in 5 680 instances: 2 811
instances in category RANDOM, 1 676 in category HANDMADE and 1 193 in category INDUSTRIAL.
Recall that in Section 3.1 we dropped instances that could not be solved by any of the seven
solvers in Table 1. We follow the same methodology here, but extend our solver set by the
12 solvers in Tables 9 and 10. Now, 71.8% of the instances can be solved by at least one of
our 19 solvers within the cutoff time of 1 200 CPU seconds on our reference machine; the
remaining instances were excluded from our analysis.
We randomly split the above benchmark sets into training, validation and test sets,
as described in Table 7. All parameter tuning and intermediate testing was performed on
validation sets, and test sets were used only to generate the final results reported here.
We will be interested in analyzing SATzillas performance as we vary the data that
was used to train it. To make it easy to refer to our different data sets, we describe them
here and assign names (D, D0 , D+ ) to them. Table 7 shows the division of our data into
589

fiXu, Hutter, Hoos & Leyton-Brown

Solver
Kcnfs04
TTS
Picosat
MXC
March ks
TinisatElite
Minisat07
Rsat 2.0

Reference
Dequen and Dubois (2007)
Spence (2007)
Biere (2007)
Bregman and Mitchell (2007)
Heule and v. Maaren (2007)
Huang (2007)
Sorensson and Een (2007)
Pipatsrisawat and Darwiche (2007)

Table 9: Eight complete solvers from the 2007 SAT Competition.
Solver
Ranov
Ag2wsat0
Ag2wsat+
Gnovelty+

Reference
Pham and Anbulagan (2007)
C. M. Li and Zhang (2007)
Wei, Li, and Zhang (2007)
Pham and Gretton (2007)

Table 10: Four local search solvers from the 2007 SAT Competition.
old (pre-2007) and new (2007) instances. Table 8 shows how we combined this data
to construct the three data sets we use for evaluation. Data set D is the one introduced
and used in Section 3.1: it uses only pre-2007 instances for training, validation and testing.
Data set D0 uses the same training and validation data sets, but differs in its test sets, which
include both old and new instances. Data set D+ combines both old and new instances in
its training, validation and test sets.
Thus, note that data sets D0 and D+ use the same test sets, meaning that the performance of portfolios trained using these different sets can be compared directly. However,
we expect a portfolio trained using D+ to be at least slightly better, because it has access
to more data. As before, when we want to refer to only the RANDOM instances from D+ , we
write Dr+ ; likewise, we write Dh+ for HANDMADE, Di+ for INDUSTRIAL, etc.
6.2 Extending the Set of Component Solvers
In addition to the seven old solvers used in SATzilla07 (previously described in Table 1),
we considered eight new complete solvers and four local search solvers from the 2007 SAT
Competition for inclusion in our portfolio; these solvers are described in Tables 9 and 10.
As with training instances, we treat sets of candidate solvers as an input parameter of
SATzilla. The sets of candidate solvers used in our experiments are detailed in Table 11.
6.3 Different SATzilla Versions
Having just introduced new design ideas for SATzilla (Section 5), new training data (Section 6.1) and new solvers (Section 6.2), we were interested in evaluating how much our portfolio improved as a result. In order to gain insights into how much performance improvement
590

fiSATzilla: Portfolio-based Algorithm Selection for SAT

Name of Set

Solvers in the Set

S
S+
S++

all 7 solvers from Table 1
all 15 solvers from Tables 1 and 9
all 19 solvers from Tables 1, 9 and 10

Table 11: Solver sets used in our second series of experiments.
SATzilla version
SATzilla07(S,D0 )
SATzilla07(S+ ,D+ )
SATzilla07+ (S++ ,D+ )
SATzilla07 (S++ ,D+ )

Description
This is the version we entered into the 2007 SAT Competition (Section 3),
but evaluated on an extended test set.
This version is built using the same design as described in Section 3, but
includes new complete solvers (Table 9) and new data (Section 6.1).
In addition to new complete solvers and data, this version uses local search
solvers (Table 10) and all of the new design elements from Section 5 except
more general hierarchical hardness models (Section 5.5).
This version uses all solvers, all data and all new design elements. Unlike
for the other versions, we trained only one variant of this solver for use in
all data set categories.

Table 12: The different SATzilla versions evaluated in our second set of experiments.

was achieved by these different changes, we studied several intermediate SATzilla solvers,
which are summarized in Table 12.
Observe that all of these solvers were built using identical test data and were thus directly
comparable. We generally expected each solver to outperform its predecessors in the list.
The exception was SATzilla07 (S++ ,D+ ): instead of aiming for increased performance,
this last solver was designed to achieve good performance across a broader range of instances.
Thus, we expected SATzilla07 (S++ ,D+ ) to outperform the others on category ALL, but
not to outperform SATzilla07+ (S++ ,D+ ) on the more specific categories.
6.4 Constructing SATzilla07+ (S++ ,D+ ) and SATzilla07 (S++ ,D+ )
The construction of SATzilla07(S,D) was already described in Section 3;
SATzilla07(S,D) differed in the test set we used to evaluate it, but was otherwise identical. The construction of SATzilla07(S+ ,D+ ) was the same as that for
SATzilla07(S,D), except that it relied on different solvers and corresponding training
data. SATzilla07+ (S++ ,D+ ) and SATzilla07 (S++ ,D+ ) incorporated the new techniques introduced in Section 5. In this section we briefly describe how these solvers were
constructed.
We used the same set of features as for SATzilla07 (see Section 3.3). We also used the
same execution environment and cutoff times. Pre-solvers were identified automatically as
described in Section 5.1, using the (automatically determined) candidate solvers listed in
Table 13. The final sets of pre-solvers selected for each version of SATzilla are listed in
Section 7 (Tables 14, 17, 20 and 23). Based on the solvers scores on validation data sets,
591

fiXu, Hutter, Hoos & Leyton-Brown

RANDOM

HANDMADE

INDUSTRIAL

ALL

Complete
Pre-solver
Candidates

Kcnfs06
March dl04
March ks

March dl04
Vallst
March ks

Rsat 1.03
Picosat
Rsat 2.0

Minisat07
March ks
March dl04

Local Search
Pre-solver
Candidates

Ag2wsat0
Gnovelty+
SAPS

Ag2wsat0
Ag2wsat+
Gnovelty+

Ag2wsat0
Ag2wsat+
Gnovelty+

SAPS
Ag2wsat0
Gnovelty+

Table 13: Pre-solver candidates for our four data sets. These candidates were automatically
chosen based on the scores on validation data achieved by running the respective
algorithms for a maximum of 10 CPU seconds.

we automatically determined the backup solvers for RANDOM, HANDMADE, INDUSTRIAL and
ALL to be March ks, March dl04, Eureka and Eureka, respectively.
We built models to predict the performance score of each algorithm. This score is well
defined even in case of timeouts and crashes; thus, there was no need to deal with censored
data. Like SATzilla07, SATzilla07+ used hierarchical empirical hardness models (Xu
et al., 2007a) with two underlying models (Msat and Munsat ) for predicting a solvers score.
For SATzilla07 , we built more general hierarchical hardness models for predicting scores
as described in Section 5.5; these models were based on six underlying empirical hardness
models (Msat and Munsat trained on data from each SAT competition category).
We chose solver subsets based on the results of our local search procedure for subset search as outlined in Section 5.2. The resulting final components of SATzilla07,
SATzilla07+ and SATzilla07 for each category are described in detail in the following
section.

7. Evaluation II: Performance Analysis of the Improved SATzilla Versions
In this section, we investigate the effectiveness of our new techniques by evaluating
the four SATzilla versions listed in Table 12: SATzilla07(S,D0 ), SATzilla07(S+ ,D+ ),
SATzilla07+ (S++ ,D+ ) and SATzilla07 (S++ ,D+ ). To evaluate their performance, we
constructed a simulated SAT competition using the same scoring function as in the 2007
SAT Competition, but differing in a number of important aspects. The participants in our
competition were the 19 solvers listed in Tables 1, 9, and 10 (all solvers were considered for
all categories), and the test instances were Eo  En as described in Tables 7 and 8. Furthermore, our computational infrastructure (see Section 3.4) differed from the 2007 competition,
and we also used shorter cutoff times of 1200 seconds. For these reasons some solvers ranked
slightly differently in our simulated competition than in the 2007 competition.
7.1 RANDOM Category
Table 14 shows the configuration of the three different SATzilla versions designed for
the RANDOM category. Note that the automatic solver selection in SATzilla07+ (S++ ,D+
r )
included different solvers than the ones used in SATzilla07(S+ ,D+
);
in
particular,
it
chose
r
592

fiSATzilla: Portfolio-based Algorithm Selection for SAT

SATzilla version

Pre-Solvers (time)

Component solvers

SATzilla07(S,D0r )

March dl04(5); SAPS(2)

Kcnfs06, March dl04, Rsat 1.03

SATzilla07(S+ ,D+
r )

March dl04(5); SAPS(2)

SATzilla07+ (S++ ,D+
r )

SAPS(2); Kcnfs06(2)

Kcnfs06, March dl04, March ks,
Minisat07
Kcnfs06, March ks, Minisat07, Ranov,
Ag2wsat+, Gnovelty+

Table 14: SATzillas configuration for the RANDOM category; cutoff times for pre-solvers are specified in CPU seconds.

three local search solvers, Ranov, Ag2wsat+, and Gnovelty+, that were not available to
SATzilla07. Also, the automatic pre-solver selection chose a different order and cutoff
time of pre-solvers than our manual selection: it chose to first run SAPS for two CPU
seconds, followed by two CPU seconds of Kcnfs06. Even though running the local search
algorithm SAPS did not help for solving unsatisfiable instances, we see in Figure 8 (left) that
SAPS solved many more instances than March dl04 in the first few seconds.
Table 15 shows the performance of different versions of SATzilla compared to the best
solvers in the RANDOM category. All versions of SATzilla outperformed every non-portfolio
solver in terms of average runtime and number of instances solved. SATzilla07+ and
SATzilla07 , the variants optimizing score rather than another objective function, also
clearly achieved higher scores than the non-portfolio solvers. This was not always the case
for the other versions; for example, SATzilla07(S+ ,D+
r ) achieved only 86.6% of the score
of the best solver, Gnovelty+ (where scores were computed based on a reference set of 20
reference solvers: the 19 solvers from Tables 1, 9, and 10, as well as SATzilla07(S+ ,D+
r )).
Table 15 and Figure 8 show that adding complete solvers and training data did not improve
SATzilla07 much. At the same time, substantial improvements were achieved by the
new mechanisms in SATzilla07+ , leading to 11% more instances solved, a reduction of
average runtime by more than half, and an increase in score by over 50%. Interestingly, the
performance of the more general SATzilla07 (S++ ,D+ ) trained on instance mix ALL and
tested on the RANDOM category was quite close to the best version of SATzilla specifically
designed for RANDOM instances, SATzilla07+ (S++ ,D+
r ). Note that due to their excellent
performance on satisfiable instances, the local search solvers in Table 15 (Gnovelty+ and
Ag2wsat variants) tended to have higher overall scores than the complete solvers (Kcnfs04
and March ks) even though they solved fewer instances and in particular could not solve any
unsatisfiable instance. In the 2007 SAT Competition, however, all winners of the random
SAT+UNSAT category were complete solvers, which lead us to speculate that local search
solvers were not considered in this category (in the random SAT category, all winners were
indeed local search solvers).
Figure 8 presents CDFs summarizing the performance of the best non-portfolio solvers,
SATzilla solvers and two oracles. All non-portfolio solvers omitted had CDFs below those
shown. As in Section 4, the oracles represent ideal versions of SATzilla that choose among
component solvers perfectly and without any computational cost. More specifically, given
an instance, an oracle picks the fastest algorithm; it is allowed to consider SAPS (with a
593

fiXu, Hutter, Hoos & Leyton-Brown

Solver

Avg. runtime [s]

Solved [%]

Performance score

Kcnfs04
March ks
Ag2wsat0
Ag2wsat+
Gnovelty+

852
351
479
510
410

32.1
78.4
62.0
59.1
67.4

38309
113666
119919
110218
131703

SATzilla07(S,D0r )
SATzilla07(S+ ,D+
r )
SATzilla07+ (S++ ,D+
r )
SATzilla07 (S++ ,D+ )

231
218
84
113

85.4
86.5
97.8
95.8

 (86.6%)
 (88.7%)
189436 (143.8%)
 (137.8%)

Table 15: The performance of SATzilla compared to the best solvers on RANDOM. The cutoff time
was 1 200 CPU seconds; SATzilla07 (S++ ,D+ ) was trained on ALL. Scores were computed based on 20 reference solvers: the 19 solvers from Tables 1, 9, and 10, as well as one
version of SATzilla. To compute the score for each non-SATzilla solver, the SATzilla
version used as a member of the set of reference solvers was SATzilla07+ (S++ ,D+
r ).
)
in
the
Since we did not include SATzilla versions other than SATzilla07+ (S++ ,D+
r
set of reference solvers, scores for these solvers are incomparable to the other scores given
here, and therefore we do not report them. Instead, for each SATzilla solver, we indicate
in parentheses its performance score as a percentage of the highest score achieved by a
non-portfolio solver, given a reference set in which the appropriate SATzilla solver took
the place of SATzilla07+ (S++ ,D+
r ).

100

% Instances Solved

80
70

90

March_dl04
March_ks
Gnovelty+

80

% Instances Solved

90

100

Oracle(S++)
SATzilla07+(S++,D+r)

60
50
40
30
20

70
60

Oracle(S++)
Oracle(S)
+ ++ +
SATzilla07 (S ,Dr )
SATzilla07(S+,D+r)
SATzilla07(S,Dr)
SATzilla07*(S++,D+)

50
40
30
20

Presolving(07(S+,D+r),07(S,D ))
r

10
0 1
10

Presolving(07+(S++,D+r))
0

10

+

AvgFeature(07 (S

+
,Dr ))

1

10

AvgFeature(07(S+,D+r),07(S,D ))
r

10

++

Presolving(others)
2

10

3

10

Runtime [CPU sec]

0 1
10

AvgFeature(others)
0

10

1

10

2

10

Figure 8: Left: CDFs for SATzilla07+ (S++ ,D+
r ) and the best non-portfolio solvers on RANDOM;
right: CDFs for the different versions of SATzilla on RANDOM shown in Table 14, where
SATzilla07 (S++ ,D+ ) was trained on ALL. All other solvers CDFs are below the ones
shown here.

maximum runtime of 10 CPU seconds) and any solver from the given set (S for one oracle
and S++ for the other).
Table 16 indicates how often each component solver of SATzilla07+ (S++ ,D+
r ) was
selected, how often it was successful, and the amount of its average runtime. We found
594

3

10

Runtime [CPU sec]

fiSATzilla: Portfolio-based Algorithm Selection for SAT

Pre-Solver (Pre-Time)

Solved [%]

Avg. Runtime [CPU sec]

SAPS(2)
March dl04(2)

52.2
9.6

1.1
1.68

Selected Solver

Selected [%]

Success [%]

Avg. Runtime [CPU sec]

March dl04
Gnovelty+
March ks
Minisat07
Ranov
Ag2wsat+

34.8
28.8
23.9
4.4
4.0
4.0

96.2
93.9
92.6
100
100
77.8

294.8
143.6
213.3
61.0
6.9
357.9

Table 16: The solvers selected by SATzilla07+ (S++ ,D+
r ) for the RANDOM category. Note that
column Selected [%] shows the percentage of instances remaining after pre-solving for
which the algorithm was selected, and this sums to 100%. Cutoff times for pre-solvers
are specified in CPU seconds.

that the solvers picked by SATzilla07+ (S++ ,D+
r ) solved the given instance in most cases.
Another interesting observation is that when a solvers success ratio was high, its average
runtime tended to be lower.
7.2 HANDMADE Category
The configurations of the three SATzilla versions designed for the HANDMADE category are
shown in Table 17. Again, SATzilla07+ (S++ ,D+
h ) included three local search solvers,
Ranov, Ag2wsat+ and Gnovelty+, which had not been available to SATzilla07. Like our
manual choice in SATzilla07, the automatic pre-solver selection chose to run March dl04
for five CPU seconds. Unlike the manual selection, it abstained from using SAPS (or indeed
any other solver) as a second pre-solver. Table 18 shows the performance of the different
versions of SATzilla compared to the best solvers for category HANDMADE. Here, about half
of the observed performance improvement was achieved by using more solvers and more
training data; the other half was due to the improvements in SATzilla07+ . Note that for
the HANDMADE category, SATzilla07 (S++ ,D+ ) performed quite poorly. We attribute this
to a weakness of the feature-based classifier on HANDMADE instances, an issue we discuss
further in Section 7.4.
Table 19 indicates how often each component solver of SATzilla07+ (S++ ,D+
h ) was
selected, how many problem instances it solved, and its average runtime for these runs.
There are many solvers that SATzilla07+ (S++ ,D+
h ) picked quite rarely; however, in most
cases, their success ratios are close to 100%, and their average runtimes are very low.
7.3 INDUSTRIAL Category
Table 20 shows the configuration of the three different SATzilla versions designed for the
INDUSTRIAL category. Local search solvers performed quite poorly for the instances in this
category, with the best local search solver, Ag2wsat0, only solving 23% of the instances
595

fiXu, Hutter, Hoos & Leyton-Brown

SATzilla

Pre-Solver (time)

Components

SATzilla07(S,D0h )

March dl04(5); SAPS(2)

SATzilla07(S+ ,D+
h)

March dl04(5); SAPS(2)

SATzilla07+ (S++ ,D+
h)

March ks(5)

Kcnfs06, March dl04, Minisat 2.0,
Rsat 1.03
Vallst, Zchaff rand, TTS, MXC,
March ks, Minisat07, Rsat 2.0
Eureka, March dl04; Minisat 2.0,
Rsat 1.03, Vallst, TTS, Picosat, MXC,
March ks, TinisatElite, Minisat07,
Rsat 2.0, Ranov, Ag2wsat0, Gnovelty+

Table 17: SATzillas configuration for the HANDMADE category.
Solver

Avg. runtime [s]

Solved [%]

Performance score

TTS
MXC
March ks
Minisat07
March dl04

729
527
494
438
408

41.1
61.9
63.9
68.9
72.4

40669
43024
68859
59863
73226

SATzilla07(S,D0h )
SATzilla07(S+ ,D+
h)
SATzilla07+ (S++ ,D+
h)
SATzilla07 (S++ ,D+ )

284
203
131
215

80.4
87.4
95.6
88.0

 (93.5%)
 (118.8%)
112287 (153.3%)
 (110.5%)

Table 18: The performance of SATzilla compared to the best solvers on HANDMADE. Scores for nonportfolio solvers were computed using a reference set in which the only SATzilla solver

++
was SATzilla07+ (S++ ,D+
,D+ )
h ). Cutoff time: 1 200 CPU seconds; SATzilla07 (S
was trained on ALL.
100

100
AvgFeature(07+(S++,D+h))

90

70
60
50
40

++

Oracle(S )
SATzila07+(S++,D+h)

30

Presolving(07+(S++,D+h))

0

10

1

10

2

10

AvgFeature(07(S+,D+h),07(S,Dh))
AvgFeature(07+(S++,D+h))

AvgFeature(others)

70
60
50
Oracle(S++)
Oracle(S)
+ ++ +
SATzilla07 (S ,Dh)

40
30

SATzilla07(S+,D+h)

20

March_dl04
March_ks
Minisat07

20
10 1
10

Presolving(07(S+,D+h),07(S,Dh))

80 Presolving(others)

80

% Instances Solved

% Instances Solved

90

Presolving(07+(S++,D+h))

SATzilla07(S,D )
h

10
3

10

Runtime [CPU sec]

0 1
10

SATzilla07*(S++,D+)
0

10

1

10

2

10

Runtime [CPU sec]

Figure 9: Left: CDFs for SATzilla07+ (S++ ,D+
h ) and the best non-portfolio solvers on HANDMADE;
right: CDFs for the different versions of SATzilla on HANDMADE shown in Table 17, where
SATzilla07 (S++ ,D+ ) was trained on ALL. All other solvers CDFs are below the ones
shown here.

596

3

10

fiSATzilla: Portfolio-based Algorithm Selection for SAT

Pre-Solver (Pre-Time)

Solved [%]

Avg. Runtime [CPU sec]

March ks(5)

39.0

3.2

Selected Solver

Selected [%]

Success [%]

Avg. Runtime [CPU sec]

Minisat07
TTS
MXC
March ks
Eureka
March dl04
Rsat 1.03
Picosat
Ag2wsat0
TinisatElite
Ranov
Minisat 2.0
Rsat 2.0
Gnovelty+
Vallst

40.4
11.5
7.2
7.2
5.8
5.8
4.8
3.9
3.4
2.9
2.9
1.4
1.4
1.0
0.5

89.3
91.7
93.3
100
100
91.7
100
100
100
100
83.3
66.7
100
100
100

205.1
133.2
310.5
544.7
0.34
317.6
185.1
1.7
0.5
86.5
206.1
796.5
0.9
3.2
<0.01

Table 19: The solvers selected by SATzilla07+ (S++ ,D+
h ) for the HANDMADE category.
SATzilla

Pre-Solver (time)

Components

SATzilla07(S,Di )

Rsat 1.03 (2)

SATzilla07(S+ ,D+
i )

Rsat 2.0 (2)

Eureka, March dl04, Minisat 2.0,
Rsat 1.03
Eureka, March dl04, Minisat 2.0,
Zchaff Rand, TTS, Picosat, March ks

SATzilla07+ (S++ ,D+
i )

Rsat 2.0 (10); Gnovelty+(2)

Eureka, March dl04, Minisat 2.0,
Rsat 1.03, TTS, Picosat, Minisat07,
Rsat 2.0

Table 20: SATzillas configuration for the INDUSTRIAL category.
within the cutoff time. Consequently, no local search solver was selected by the automatic
solver subset selection in SATzilla07+ (S++ ,D+
i ). However, automatic pre-solver selection
did include the local search solver Gnovelty+ as the second pre-solver, to be run for 2 CPU
seconds after 10 CPU seconds of running Rsat 2.0.
Table 21 compares the performance of different versions of SATzilla and the best
solvers on INDUSTRIAL instances. It is not surprising that more training data and more
solvers helped SATzilla07 to improve in terms of all our metrics (avg. runtime, percentage
solved and score). A somewhat bigger improvement was due to the new mechanisms in
SATzilla07+ that led to SATzilla07+ (S++ ,D+
i ) outperforming every non-portfolio solver
with respect to every metric, specially in terms of performance score. Note that the general
SATzilla version SATzilla07 (S++ ,D+ ) trained on ALL achieved performance very close
to that of SATzilla07+ (S++ ,D+
i ) on the INDUSTRIAL data set in terms of average runtime
and percentage of solved instances.
597

fiXu, Hutter, Hoos & Leyton-Brown

Solver

Avg. runtime [s]

Solved [%]

Performance score

Rsat 1.03
Rsat 2.0
Picosat
TinisatElite
Minisat07
Eureka

353
365
282
452
372
349

80.8
80.8
85.9
70.8
76.6
83.2

52740
51299
66561
40867
60002
71505

SATzilla07(S,D0i )
SATzilla07(S+ ,D+
i )
SATzilla07+ (S++ ,D+
i )
SATzilla07 (S++ ,D+ )

298
262
233
239

87.6
89.0
93.1
92.7

 (91.3%)
 (98.2%)
79724 (111.5%)
 (104.8%)

Table 21: The performance of SATzilla compared to the best solvers on INDUSTRIAL. Scores
for non-portfolio solvers were computed using a reference set in which the only
SATzilla solver was SATzilla07+ (S++ ,D+
Cutoff time: 1 200 CPU seconds;
i ).
SATzilla07 (S++ ,D+ ) was trained on ALL.

Pre-Solver (Pre-Time)

Solved [%]

Avg. Runtime [CPU sec]

Rsat 2.0(10)
Gnovelty+ (2)

38.1
0.3

6.8
2.0

Selected Solver

Selected [%]

Success [%]

Avg. Runtime [CPU sec]

Eureka (BACKUP)
Eureka
Picosat
Minisat07
Minisat 2.0
March dl04
TTS
Rsat 2.0
Rsat 1.03

29.1
15.1
14.5
14.0
12.3
8.4
3.9
1.7
1.1

88.5
100
96.2
84.0
68.2
86.7
100
100
100

385.4
394.2
179.6
306.3
709.2
180.8
0.7
281.6
10.6

Table 22: The solvers selected by SATzilla07+ (S++ ,D+
i ) for the INDUSTRIAL category.
As can be seen from Figure 10, the performance improvements achieved by SATzilla
over non-portfolio solvers are smaller for the INDUSTRIAL category than for other categories. Note that the best INDUSTRIAL solver performed very well, solving 85.9% of the
instances within the cutoff time of 1 200 CPU seconds.6 Still, SATzilla07+ (S++ ,D+
i ) had
significantly smaller average runtime (17%) and solved 7.2% more instances than the best
component solver, Picosat. Likewise, the score for SATzilla07+ (S++ ,D+
i ) was 11.5%
higher than that of the top-ranking component solver (in terms of score), Eureka.
6. Recall that this number means the solver solved 85.9% of the instances that could be solved by at least
one solver. Compared to our other data sets, it seems that either solvers exhibited more similar behavior
on INDUSTRIAL instances or that instances in this category exhibited greater variability in hardness.

598

fiSATzilla: Portfolio-based Algorithm Selection for SAT

100

100
+

90

++

+

Presolving(07 (S ,Di ))

+

++

+

AvgFeature(07 (S ,Di ))

90
80

% Instances Solved

% Instances Solved

80
70
60
50
40
30
20
10 1
10

0

10

1

10

70

Presolving(others)

AvgFeature(07*(S++,D+))
AvgFeature(others)

50
++

Oracle(S )
Oracle(S)
+ ++ +
SATzilla07 (S ,Di ))

40
30

Rsat1.03
Picosat

20

2

Presolving(07*(S++,D+))

+ ++ +
AvgFeature(07 (S ,Di ))

60

Oracle(S++)
SATzilla07+(S++,D+i)

10

Presolving(07+(S++,D+i))

SATzilla07(S+,D+i)
SATzilla07(S,Di)
*

++

+

SATzilla07 (S ,D )
3

10

Runtime [CPU sec]

10 1
10

0

10

1

10

2

10

3

10

Runtime [CPU sec]

CDFs for SATzilla07+ (S++ ,D+
i ) and the best non-portfolio solvers on
INDUSTRIAL; right: CDFs for the different versions of SATzilla on INDUSTRIAL shown
in Table 20, where SATzilla07 (S++ ,D+ ) was trained on ALL. All other solvers CDFs
(including Eurekas) are below the ones shown here.

Figure 10: Left:

Table 22 indicates how often each component solver of SATzilla07+ (S++ ,D+
i ) was selected, how many problem instances it solved, and its average runtime for these runs. In this
case, backup solver Eureka was used for problem instances for which feature computation
timed out and pre-solvers did not produce a solution.
7.4 ALL
There are four versions of SATzilla specialized for category ALL. Their detailed configurations are listed in Table 23. The results of automatic pre-solver selection were identical
for SATzilla07+ and SATzilla07 : both chose to first run the local search solver SAPS
for two CPU seconds, followed by two CPU seconds of March ks. These solvers were similar to our manual selection, but their order was reversed. For the solver subset selection,
SATzilla07+ and SATzilla07 yielded somewhat different results, but both of them kept
two local search algorithms, Ag2wsat+ & Ranov, and Ag2wsat+ & Gnovelty+, respectively.
Table 24 compares the performance of the four versions of SATzilla on the ALL category.
Roughly equal improvements in terms of all our performance metrics were due to more
training data and solvers on the one hand, and to the improvements in SATzilla07+ on the
other hand. The best performance in terms of all our performance metrics was obtained by
SATzilla07 (S++ ,D+ ). Recall that the only difference between SATzilla07+ (S++ ,D+ )
and SATzilla07 (S++ ,D+ ) was the use of more general hierarchical hardness models, as
described in Section 5.5.
Note that using a classifier is of course not as good as using an oracle for determining
the distribution an instance comes from; thus, the success ratios of the solvers selected
by SATzilla07 over the instances in the test set for distribution ALL (see Table 25) were
slightly lower than those for the solvers picked by SATzilla07+ for each of the distributions
individually (see Tables 16, 19, and 22). However, when compared to SATzilla07+ on
distribution ALL, SATzilla07 performed significantly better: achieving overall performance
599

fiXu, Hutter, Hoos & Leyton-Brown

SATzilla

Pre-Solver (time)

Components

SATzilla07(S,D)

March dl04(5); SAPS(2)

Eureka, Kcnfs06, March dl04, Minisat
2.0,Zchaff rand

SATzilla07(S+ ,D+ )

March dl04(5); SAPS(2)

SATzilla07+ (S++ ,D+ )

SAPS(2); March ks(2)

SATzilla07 (S++ ,D+ )

SAPS(2); March ks(2)

Eureka, March dl04, Zchaff rand,
Kcnfs04, TTS, Picosat, March ks,
Minisat07
Eureka,
Kcnfs06,
Rsat 1.03,
Zchaff rand, TTS, MXC, TinisatElite,
Rsat 2.0, Ag2wsat+, Ranov
Eureka, Kcnfs06, March dl04, Minisat
2.0,
Rsat 1.03,
Picosat,
MXC,
Minisat07,
Ag2wsat+,
March ks,
Gnovelty+

Table 23: SATzillas configuration for the ALL category.
Solver

Avg. runtime [s]

Solved [%]

Performance score

Rsat 1.03
Kcnfs04
TTS
Picosat
March ks
TinisatElite
Minisat07
Gnovelty+
March dl04

542
969
939
571
509
690
528
684
509

61.1
21.3
22.6
57.7
62.9
47.3
61.8
43.9
62.7

131399
46695
74616
135049
202133
93169
162987
156365
205592

SATzilla07(S,D)
SATzilla07(S+ ,D+ )
SATzilla07+ (S++ ,D+ )
SATzilla07 (S++ ,D+ )

282
224
194
172

83.1
87.0
91.1
92.9

 (125.0%)
 (139.2%)
 (158%)
344594 (167.6%)

Table 24: The performance of SATzilla compared to the best solvers on ALL. Scores for nonportfolio solvers were computed using a reference set in which the only SATzilla solver
was SATzilla07 (S++ ,D+ ). Cutoff time: 1 200 CPU seconds.

improvements of 11.3% lower average runtime, 1.8% more solved instances and 9.6% higher
score. This supports our initial hypothesis that SATzilla07 would perform slightly worse
than specialized versions of SATzilla07+ in each single category, yet would yield the best
result when applied to a broader and more heterogeneous set of instances.
The runtime cumulative distribution function (Figure 11, right) shows that
SATzilla07 (S++ ,D+ ) dominated the other versions of SATzilla on ALL and solved about
30% more instances than the best non-portfolio solver, March dl04 (Figure 11, left).
Table 26 shows the performance of the general classifier in SATzilla07 (S++ ,D+ ). We
note several patterns: Firstly, classification performance for RANDOM and INDUSTRIAL in600

fiSATzilla: Portfolio-based Algorithm Selection for SAT

100

100

Oracle(S++)
Oracle(S)
SATzilla07+(S++,D+)
SATzilla07(S+,D+)
SATzilla07(S,D)
SATzilla07*(S++,D+)

++

Oracle(S )
SATzilla07*(S++,D+)
March_dl04
Gnovelty+

90

80

% Instances Solved

% Instances Solved

80

90

70
60
50
40
30

70
60
50
40
30
20

20

+

+

Presolving(07(S ,D ),07(S,D))

10

++

+

Presolving(07*(S ,D ))

0 1
10

0

10

++

1

10

+

+

AvgFeature(07(S ,D ),07(S,D))

10

+

AvgFeature(07*(S ,D ))

Presolving(others)
2

0 1
10

3

10

10

AvgFeature(others)
0

10

1

2

10

10

3

10

Runtime [CPU sec]

Runtime [CPU sec]

Figure 11: Left: CDF for SATzilla07 (S++ ,D+ ) and the best non-portfolio solvers on ALL; right:
CDFs for different versions of SATzilla on ALL shown in Table 23. All other solvers
CDFs are below the ones shown here.

Pre-Solver (Pre-Time)

Solved [%]

Avg. Runtime [CPU sec]

SAPS(2)
March ks (2)

33.0
13.9

1.4
1.6

Selected Solver

Selected [%]

Success [%]

Avg. Runtime [CPU sec]

Minisat07
March dl04
Gnovelty+
March ks
Eureka (BACKUP)
Eureka
Picosat
Kcnfs06
MXC
Rsat 1.03
Minisat 2.0
Ag2wsat+

21.2
14.5
12.5
9.1
8.9
7.2
6.6
6.5
5.5
4.0
3.5
0.5

85.5
84.0
85.2
89.8
89.7
97.9
90.7
95.2
88.9
80.8
56.5
33.3

247.5
389.5
273.2
305.6
346.1
234.6
188.6
236.3
334.0
364.9
775.7
815.7

Table 25: The solvers selected by SATzilla07 (S++ ,D+ ) for the ALL category.

stances was much better than for HANDMADE instances. Secondly, for HANDMADE instances,
most misclassifications were not due to a misclassification of the type of instance, but
rather of the satisfiability status. Finally, we can see that RANDOM instances were almost perfectly classified as RANDOM and only very few other instances were classified as
RANDOM, while HANDMADE and INDUSTRIAL instances were confused somewhat more often.
The comparably poor classification performance for HANDMADE instances partly explains why
SATzilla07 (S++ ,D+ ) did not perform as well for the HANDMADE category as for the others.
601

fiXu, Hutter, Hoos & Leyton-Brown

R, sat

R, unsat

H, sat

H, unsat

I, sat

I, unsat

92%

5%

1%



1%

1%

4%

94%



1%



1%

classified H, sat





57%

38%



5%

classified H, unsat



1%

23%

71%

1%

4%

classified I, sat





8%



81%

11%

classified I, unsat







5%

6%

89%

classified R, sat
classified R, unsat

Table 26: Confusion matrix for the 6-way classifier on data set ALL.

8. Conclusions
Algorithms can be combined into portfolios to build a whole greater than the sum of its
parts. We have significantly extended earlier work on algorithm portfolios for SAT that
select solvers on a per-instance basis using empirical hardness models for runtime prediction. We have demonstrated the effectiveness of our general portfolio construction method,
SATzilla07, on four large sets of SAT competition instances. Our own experiments show
that our SATzilla07 portfolio solvers always outperform their components. Furthermore,
SATzilla07s excellent performance in the recent 2007 SAT Competition demonstrates the
practical effectiveness of our approach.
In this work, we pushed the SATzilla approach further beyond SATzilla07. For the
first time, we showed that portfolios can optimize complex scoring functions and integrate
local search algorithms as component solvers. Furthermore, we showed how to automate the
process of pre-solver selection, one of the last aspects of our approach that was previously
based on manual engineering. As demonstrated in extensive computational experiments,
these enhancements improved SATzilla07s performance substantially.
SATzilla is now at a stage where it can be applied out of the box given a set of
possible component solvers along with representative training and validation instances. In
an automated built-in meta-optimization process, the component solvers to be used and
the solvers to be used as pre-solvers are automatically determined from the given set of
solvers, without any human effort. The computational bottleneck is to execute the possible
component solvers on a representative set of instances in order to obtain enough runtime
data to build reasonably accurate empirical hardness models. However, these computations
can be parallelized very easily and require no human intervention, only computer time,
which becomes ever cheaper. Matlab code for building empirical hardness models and
C++ code for building SATzilla portfolios that use these models are available online at
http://www.cs.ubc.ca/labs/beta/Projects/SATzilla.
It is interesting to note that the use of local search methods has a significant impact
on the performance of SATzilla. In preliminary experiments, we observed that the overall performance of SATzilla07 was significantly weaker when local search solvers and
local-search-based features were excluded. Specifically, the availability of these local search
602

fiSATzilla: Portfolio-based Algorithm Selection for SAT

components substantially boosted SATzilla07 s performance on the RANDOM instance category and also led to some improvements on INDUSTRIAL, but resulted in weaker performance
on HANDMADE instances. Generally, we believe that a better understanding of the impact
of features on our runtime predictions and instance categorizations will allow us to further
improve SATzilla, and we have therefore begun an in-depth investigation in this direction.
SATzillas performance ultimately depends on the power of all its component solvers
and automatically gets better as they are improved. Furthermore, SATzilla takes advantage of solvers that are only competitive for certain kinds of instances and perform poorly
otherwise, and thus SATzillas success demonstrates the value of such solvers. Indeed,
the identification of more such solvers, which are otherwise easily overlooked, still has the
potential to further improve SATzillas performance substantially.

Acknowledgments
This work builds on contributions from a wide range of past co-authors, colleagues, and
members of the SAT community. First, we have many colleagues to thank for their contributions to the work described in this article. Eugene Nudelman, Alex Devkar and Yoav
Shoham were Kevin and Holgers co-authors on the papers that first introduced SATzilla
(Nudelman et al., 2004a, 2004b); this work grew out of a project on automated algorithm
selection that involved Galen Andrew and Jim McFadden, in addition to Kevin, Eugene
and Yoav (Leyton-Brown et al., 2003b, 2003a). Nando de Freitas, Bart Selman, and Kevin
Murphy gave useful suggestions about machine learning algorithms, SAT instance features,
and mixtures of experts, respectively. Second, while academic research always builds on
previous work, we are especially indebted to the authors of the dozens of SAT solvers we
discuss in this paper, and particularly to their commitment to furthering scientific understanding by making their code publicly available. Without these researchers considerable
efforts, SATzilla could never have been built.

References
Bacchus, F. (2002a). Enhancing Davis Putnam with extended binary clause reasoning. In Proceedings of
the Eighteenth National Conference on Artificial Intelligence (AAAI02), pp. 613619.
Bacchus, F. (2002b). Exploring the computational tradeoff of more reasoning and less searching. In
Proceedings of the Fifth International Conference on Theory and Applications of Satisfiability Testing (SAT02), pp. 716.
Bacchus, F., & Winter, J. (2003). Effective preprocessing with hyper-resolution and equality reduction.
In Proceedings of the Sixth International Conference on Theory and Applications of Satisfiability
Testing (SAT03), pp. 341355.
Biere, A. (2007). Picosat version 535. Solver description, SAT competition 2007.
Biere, A., Cimatti, A., Clarke, E. M., Fujita, M., & Zhu, Y. (1999). Symbolic model checking using SAT
procedures instead of BDDs. In Proceedings of Design Automation Conference (DAC99), pp. 317320.
Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
Bregman, D. R., & Mitchell, D. G. (2007). The SAT solver MXC, version 0.5. Solver description, SAT
competition 2007.
C. M. Li, W. W., & Zhang, H. (2007). Combining adaptive noise and promising decreasing variables in local
search for SAT. Solver description, SAT competition 2007.

603

fiXu, Hutter, Hoos & Leyton-Brown

Carchrae, T., & Beck, J. C. (2005). Applying machine learning to low-knowledge control of optimization
algorithms. Computational Intelligence, 21 (4), 372387.
Crawford, J. M., & Baker, A. B. (1994). Experimental results on the application of satisfiability algorithms to scheduling problems. In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI94), pp. 10921097.
Davis, M., Logemann, G., & Loveland, D. (1962). A machine program for theorem proving. Communications
of the ACM, 5 (7), 394397.
Davis, M., & Putnam, H. (1960). A computing procedure for quantification theory. Journal of the ACM,
7 (1), 201215.
Dechter, R., & Rish, I. (1994). Directional resolution: The Davis-Putnam procedure, revisited. In Principles
of Knowledge Representation and Reasoning (KR94), pp. 134145.
Dequen, G., & Dubois, O. (2007). kcnfs. Solver description, SAT competition 2007.
Dubois, O., & Dequen, G. (2001). A backbone-search heuristic for efficient solving of hard 3-SAT formulae.
In Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence (IJCAI01),
pp. 248253.
Een, N., & Sorensson, N. (2003). An extensible SAT-solver. In Proceedings of the Sixth International
Conference on Theory and Applications of Satisfiability Testing (SAT03), pp. 502518.
Een, N., & Sorensson, N. (2006). Minisat v2.0 (beta). Solver description, SAT Race 2006.
Gagliolo, M., & Schmidhuber, J. (2006a). Impact of censored sampling on the performance of restart
strategies. In Twelfth Internatioal Conference on Principles and Practice of Constraint Programming (CP06), pp. 167181.
Gagliolo, M., & Schmidhuber, J. (2006b). Learning dynamic algorithm portfolios. Annals of Mathematics
and Artificial Intelligence, 47 (3-4), 295328.
Gebruers, C., Hnich, B., Bridge, D., & Freuder, E. (2005). Using CBR to select solution strategies in
constraint programming. In Proceedings of the Sixth International Conference on Case-Based Reasoning (ICCBR05), pp. 222236.
Gebruers, C., Guerri, A., Hnich, B., & Milano, M. (2004). Making choices using structure at the instance
level within a case based reasoning framework. In International Conference on Integration of AI and
OR Techniques in Constraint Programming for Combinatorial Optimization Problems (CPAIOR-04),
pp. 380386.
Gomes, C. P., & Selman, B. (2001). Algorithm portfolios. Artificial Intelligence, 126(1-2), 4362.
Guerri, A., & Milano, M. (2004). Learning techniques for automatic algorithm portfolio selection. In
Proceedings of the 16th European Conference on Artificial Intelligence (ECAI-04), pp. 475479.
Guo, H., & Hsu, W. H. (2004). A learning-based algorithm selection meta-reasoner for the real-time MPE
problem. In Proceedings of the Seventeenth Australian Conference on Artificial Intelligence, pp. 307
318.
Guyon, I., Gunn, S., Nikravesh, M., & Zadeh, L. (2006). Feature Extraction, Foundations and Applications.
Springer.
Heule, M., & v. Maaren, H. (2007). march ks. Solver description, SAT competition 2007.
Heule, M., Zwieten, J., Dufour, M., & Maaren, H. (2004). March eq: implementing additional reasoning into
an efficient lookahead SAT solver. In Proceedings of the Seventh International Conference on Theory
and Applications of Satisfiability Testing (SAT04), pp. 345359.
Hoos, H. H. (2002). An adaptive noise mechanism for WalkSAT. In Proceedings of the Eighteenth National
Conference on Artificial Intelligence (AAAI02), pp. 655660.
Hoos, H. H., & Stutzle, T. (2005). Stochastic Local Search - Foundations & Applications. Morgan Kaufmann
Publishers, San Francisco, CA, USA.
Horvitz, E., Ruan, Y., Gomes, C. P., Kautz, H., Selman, B., & Chickering, D. M. (2001). A Bayesian
approach to tackling hard computational problems. In Proceedings of the Seventeenth Conference on
Uncertainty in Artificial Intelligence (UAI01), pp. 235244.
Huang, J. (2007). TINISAT in SAT competition 2007. Solver description, SAT competition 2007.
Huberman, B., Lukose, R., & Hogg, T. (1997). An economics approach to hard computational problems.
Science, 265, 5154.

604

fiSATzilla: Portfolio-based Algorithm Selection for SAT

Hutter, F., Hamadi, Y., Hoos, H. H., & Leyton-Brown, K. (2006). Performance prediction and automated
tuning of randomized and parametric algorithms. In Twelfth Internatioal Conference on Principles
and Practice of Constraint Programming (CP06), pp. 213228.
Hutter, F., Tompkins, D. A. D., & Hoos, H. H. (2002). Scaling and probabilistic smoothing: Efficient dynamic
local search for SAT. In Proceedings of the Eighth International Conference on Principles and Practice
of Constraint Programming, pp. 233248.
Ishtaiwi, A., Thornton, J., Anbulagan, Sattar, A., & Pham, D. N. (2006). Adaptive clause weight redistribution. In Twelfth Internatioal Conference on Principles and Practice of Constraint Programming (CP06), pp. 229243.
Kautz, H., & Selman, B. (1996). Pushing the envelope: Planning, propositional logic, and stochastic search. In
Proceedings of the Thirteenth National Conference on Artificial Intelligence and the Eighth Innovative
Applications of Artificial Intelligence Conference, pp. 11941201.
Kautz, H. A., & Selman, B. (1999). Unifying SAT-based and graph-based planning. In Proceedings of the
Sixteenth International Joint Conference on Artificial Intelligence (IJCAI99), pp. 318325.
Knuth, D. (1975). Estimating the efficiency of backtrack programs. Mathematics of Computation, 29 (129),
121136.
Krishnapuram, B., Carin, L., Figueiredo, M., & Hartemink, A. (2005). Sparse multinomial logistic regression:
Fast algorithms and generalization bounds. In IEEE Transactions on Pattern Analysis and Machine
Intelligence, pp. 957968.
Kullmann, O. (2002). Investigating the behaviour of a SAT solver on random formulas. http://cssvr1.swan.ac.uk/csoliver/Artikel/OKsolverAnalyse.html.
Lagoudakis, M. G., & Littman, M. L. (2001). Learning to select branching rules in the DPLL procedure for
satisfiability. In LICS/SAT, pp. 344359.
Le Berre, D., & Simon, L. (2004). Fifty-five solvers in Vancouver: The SAT 2004 competition. In Proceedings
of the Seventh International Conference on Theory and Applications of Satisfiability Testing (SAT04),
pp. 321344.
Leyton-Brown, K., Nudelman, E., Andrew, G., McFadden, J., & Shoham, Y. (2003a). Boosting as a metaphor
for algorithm design. In Ninth Internatioal Conference on Principles and Practice of Constraint
Programming (CP03), pp. 899903.
Leyton-Brown, K., Nudelman, E., Andrew, G., McFadden, J., & Shoham, Y. (2003b). A portfolio approach
to algorithm selection. In Proceedings of the Eighteenth International Joint Conference on Artificial
Intelligence (IJCAI03), pp. 15421543.
Leyton-Brown, K., Nudelman, E., & Shoham, Y. (2002). Learning the empirical hardness of optimization
problems: The case of combinatorial auctions. In Eighth Internatioal Conference on Principles and
Practice of Constraint Programming (CP02), pp. 556572.
Li, C., & Huang, W. (2005). Diversification and determinism in local search for satisfiability. In Proceedings
of the Eighth International Conference on Theory and Applications of Satisfiability Testing (SAT05),
pp. 158172.
Lobjois, L., & Lematre, M. (1998). Branch and bound algorithm selection by performance prediction. In
Proceedings of the Fifteenth National Conference on Artificial Intelligence (AAAI98), pp. 353358.
Mahajan, Y. S., Fu, Z., & Malik, S. (2005). Zchaff2004: an efficient SAT solver. In Proceedings of the Eighth
International Conference on Theory and Applications of Satisfiability Testing (SAT05), pp. 360375.
Murphy, K. (2001). The Bayes Net Toolbox for Matlab. In Computing Science and Statistics, Vol. 33.
http://bnt.sourceforge.net/.
Nadel, A., Gordon, M., Palti, A., & Hanna, Z. (2006). Eureka-2006 SAT solver. Solver description, SAT
Race 2006.
Nudelman, E., Leyton-Brown, K., Hoos, H. H., Devkar, A., & Shoham, Y. (2004a). Understanding random
SAT: Beyond the clauses-to-variables ratio. In Tenth Internatioal Conference on Principles and
Practice of Constraint Programming (CP04), pp. 438452.
Nudelman, E., Leyton-Brown, K., Devkar, A., Shoham, Y., & Hoos, H. (2004b). Satzilla: An algorithm
portfolio for SAT. Solver description, SAT competition 2004.
Pham, D. N., & Anbulagan (2007). Resolution enhanced SLS solver: R+AdaptNovelty+. Solver description,
SAT competition 2007.

605

fiXu, Hutter, Hoos & Leyton-Brown

Pham, D. N., & Gretton, C. (2007). gNovelty+. Solver description, SAT competition 2007.
Pipatsrisawat, K., & Darwiche, A. (2006). Rsat 1.03: SAT solver description. Tech. rep. D-152, Automated
Reasoning Group, UCLA.
Pipatsrisawat, K., & Darwiche, A. (2007). Rsat 2.0: SAT solver description. Solver description, SAT
competition 2007.
Rice, J. R. (1976). The algorithm selection problem. Advances in Computers, 15, 65118.
Samulowitz, H., & Memisevic, R. (2007). Learning to solve QBF. In Proceedings of the Twentysecond
National Conference on Artificial Intelligence (AAAI07), pp. 255260.
Schmee, J., & Hahn, G. J. (1979). A simple method for regression analysis with censored data. Technometrics,
21 (4), 417432.
Selman, B., Kautz, H., & Cohen, B. (1994). Noise strategies for improving local search. In Proceedings of
the Twelfth National Conference on Artificial Intelligence (AAAI94), pp. 337343.
Selman, B., Levesque, H., & Mitchell, D. (1992). A new method for solving hard satisfiability problems. In
Proceedings of the Tenth National Conference on Artificial Intelligence (AAAI92), pp. 440446.
Sorensson, N., & Een, N. (2007). Minisat2007. http://www.cs.chalmers.se/Cs/Research/FormalMethods/MiniSat/.
Spence, I. (2007). Ternary tree solver (tts-4-0). Solver description, SAT competition 2007.
Stephan, P., Brayton, R., & Sangiovanni-Vencentelli, A. (1996). Combinational test generation using satisfiability. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 15,
11671176.
Streeter, M., Golovin, D., & Smith, S. F. (2007). Combining multiple heuristics online. In Proceedings of
the Twentysecond National Conference on Artificial Intelligence (AAAI07), pp. 11971203.
Subbarayan, S., & Pradhan, D. (2005). Niver: Non-increasing variable elimination resolution for preprocessing sat instances. Lecture Notes in Computer Science,Springer, 3542/2005, 276291.
Tompkins, D. A. D., & Hoos, H. H. (2004). UBCSAT: An implementation and experimentation environment
for SLS algorithms for SAT & MAX-SAT.. In Proceedings of the Seventh International Conference
on Theory and Applications of Satisfiability Testing (SAT04).
Vallstrom, D. (2005). Vallst documentation. http://vallst.satcompetition.org/index.html.
van Gelder, A. (2002). Another look at graph coloring via propositional satisfiability. In Proceedings of
Computational Symposium on Graph Coloring and Generalizations (COLOR-02), pp. 4854.
Wei, W., Li, C. M., & Zhang, H. (2007). Deterministic and random selection of variables in local search for
SAT. Solver description, SAT competition 2007.
Xu, L., Hoos, H. H., & Leyton-Brown, K. (2007a). Hierarchical hardness models for SAT. In Thirteenth
Internatioal Conference on Principles and Practice of Constraint Programming (CP07), pp. 696711.
Xu, L., Hutter, F., Hoos, H., & Leyton-Brown, K. (2007b). Satzilla-07: The design and analysis of an
algorithm portfolio for SAT. In Thirteenth Internatioal Conference on Principles and Practice of
Constraint Programming (CP07), pp. 712727.
Xu, L., Hutter, F., Hoos, H., & Leyton-Brown, K. (2007c). Satzilla2007: a new & improved algorithm
portfolio for SAT. Solver description, SAT competition 2007.
Zhang, L., Madigan, C. F., Moskewicz, M. W., & Malik, S. (2001). Efficient conflict driven learning in Boolean
satisfiability solver. In Proceedings of the International Conference on Computer Aided Design, pp.
279285.
Zhang, L. (2002). The quest for efficient Boolean satisfiability solvers. In Proceedings of 8th International
Conference on Computer Aided Deduction (CADE-02), pp. 313331.

606

fiJournal of Artificial Intelligence Research 32 (2008) 663-704

Submitted 03/08; published 07/08

Online Planning Algorithms for POMDPs
Stephane Ross
Joelle Pineau

stephane.ross@mail.mcgill.ca
jpineau@cs.mcgill.ca

School of Computer Science
McGill University, Montreal, Canada, H3A 2A7

Sebastien Paquet
Brahim Chaib-draa

spaquet@damas.ift.ulaval.ca
chaib@damas.ift.ulaval.ca

Department of Computer Science and Software Engineering
Laval University, Quebec, Canada, G1K 7P4

Abstract
Partially Observable Markov Decision Processes (POMDPs) provide a rich framework
for sequential decision-making under uncertainty in stochastic domains. However, solving
a POMDP is often intractable except for small problems due to their complexity. Here,
we focus on online approaches that alleviate the computational complexity by computing
good local policies at each decision step during the execution. Online algorithms generally consist of a lookahead search to find the best action to execute at each time step in
an environment. Our objectives here are to survey the various existing online POMDP
methods, analyze their properties and discuss their advantages and disadvantages; and to
thoroughly evaluate these online approaches in different environments under various metrics (return, error bound reduction, lower bound improvement). Our experimental results
indicate that state-of-the-art online heuristic search methods can handle large POMDP
domains efficiently.

1. Introduction
The Partially Observable Markov Decision Process (POMDP) is a general model for sequential decision problems in partially observable environments. Many planning and control problems can be modeled as POMDPs, but very few can be solved exactly because of
their computational complexity: finite-horizon POMDPs are PSPACE-complete (Papadimitriou & Tsitsiklis, 1987) and infinite-horizon POMDPs are undecidable (Madani, Hanks,
& Condon, 1999).
In the last few years, POMDPs have generated significant interest in the AI community and many approximation algorithms have been developed (Hauskrecht, 2000; Pineau,
Gordon, & Thrun, 2003; Braziunas & Boutilier, 2004; Poupart, 2005; Smith & Simmons,
2005; Spaan & Vlassis, 2005). All these methods are offline algorithms, meaning that they
specify, prior to the execution, the best action to execute for all possible situations. While
these approximate algorithms can achieve very good performance, they often take significant time (e.g. more than an hour) to solve large problems, where there are too many
possible situations to enumerate (let alone plan for). Furthermore, small changes in the
environments dynamics require recomputing the full policy, which may take hours or days.
c
2008
AI Access Foundation. All rights reserved.

fiRoss, Pineau, Paquet, & Chaib-draa

On the other hand, online approaches (Satia & Lave, 1973; Washington, 1997; Barto,
Bradtke, & Singhe, 1995; Paquet, Tobin, & Chaib-draa, 2005; McAllester & Singh, 1999;
Bertsekas & Castanon, 1999; Shani, Brafman, & Shimony, 2005) try to circumvent the complexity of computing a policy by planning online only for the current information state. Online algorithms are sometimes also called agent-centered search algorithms (Koenig, 2001).
Whereas an offline search would compute an exponentially large contingency plan considering all possible happenings, an online search only considers the current situation and a small
horizon of contingency plans. Moreover, some of these approaches can handle environment
changes without requiring more computation, which allows online approaches to be applicable in many contexts where offline approaches are not applicable, for instance, when the
task to accomplish, as defined by the reward function, changes regularly in the environment.
One drawback of online planning is that it generally needs to meet real-time constraints,
thus greatly reducing the available planning time, compared to offline approaches.
Recent developments in online POMDP search algorithms (Paquet, Chaib-draa, & Ross,
2006; Ross & Chaib-draa, 2007; Ross, Pineau, & Chaib-draa, 2008) suggest that combining
approximate offline and online solving approaches may be the most efficient way to tackle
large POMDPs. In fact, we can generally compute a very rough policy offline using existing
offline value iteration algorithms, and then use this approximation as a heuristic function
to guide the online search algorithm. This combination enables online search algorithms
to plan on shorter horizons, thereby respecting online real-time constraints and retaining a
good precision. By doing an exact online search over a fixed horizon, we can guarantee a
reduction in the error of the approximate offline value function. The overall time (offline
and online) required to obtain a good policy can be dramatically reduced by combining
both approaches.
The main purpose of this paper is to draw the attention of the AI community to online
methods as a viable alternative for solving large POMDP problems. In support of this, we
first survey the various existing online approaches that have been applied to POMDPs, and
discuss their strengths and drawbacks. We present various combinations of online algorithms
with various existing offline algorithms, such as QMDP (Littman, Cassandra, & Kaelbling,
1995), FIB (Hauskrecht, 2000), Blind (Hauskrecht, 2000; Smith & Simmons, 2005) and
PBVI (Pineau et al., 2003). We then compare empirically different online approaches in
two large POMDP domains according to different metrics (average discounted return, error
bound reduction, lower bound improvement). We also evaluate how the available online
planning time and offline planning time affect the performance of different algorithms. The
results of our experiments show that many state-of-the-art online heuristic search methods
are tractable in large state and observation spaces, and achieve the solution quality of stateof-the-art offline approaches at a fraction of the computational cost. The best methods
achieve this by focusing the search on the most relevant future outcomes for the current
decision, e.g. those that are more likely and that have high uncertainty (error) on their longterm values, such as to minimize as quickly as possible an error bound on the performance of
the best action found. The tradeoff between solution quality and computing time offered by
the combinations of online and offline approaches is very attractive for tackling increasingly
large domains.
664

fiOnline Planning Algorithms for POMDPs

2. POMDP Model
Partially observable Markov decision processes (POMDPs) provide a general framework
for acting in partially observable environments (Astrom, 1965; Smallwood & Sondik, 1973;
Monahan, 1982; Kaelbling, Littman, & Cassandra, 1998). A POMDP is a generalization
of the MDP model for planning under uncertainty, which gives the agent the ability to
effectively estimate the outcome of its actions even when it cannot exactly observe the state
of its environment.
Formally, a POMDP is represented as a tuple (S, A, T, R, Z, O) where:
 S is the set of all the environment states. A state is a description of the environment
at a specific moment and it should capture all information relevant to the agents
decision-making process.
 A is the set of all possible actions.
 T : S  A  S  [0, 1] is the transition function, where T (s, a, s0 ) = Pr(s0 |s, a)
represents the probability of ending in state s0 if the agent performs action a in state
s.
 R : S  A  R is the reward function, where R(s, a) is the reward obtained by
executing action a in state s.
 Z is the set of all possible observations.
 O : S  A  Z  [0, 1] is the observation function, where O(s0 , a, z) = Pr(z|a, s0 ) gives
the probability of observing z if action a is performed and the resulting state is s0 .
We assume in this paper that S, A and Z are all finite and that R is bounded.
A key aspect of the POMDP model is the assumption that the states are not directly
observable. Instead, at any given time, the agent only has access to some observation
z  Z that gives incomplete information about the current state. Since the states are not
observable, the agent cannot choose its actions based on the states. It has to consider
a complete history of its past actions and observations to choose its current action. The
history at time t is defined as:
ht = {a0 , z1 , . . . , zt1 , at1 , zt }.

(1)

This explicit representation of the past is typically memory expensive. Instead, it is
possible to summarize all relevant information from previous actions and observations in a
probability distribution over the state space S, which is called a belief state (Astrom, 1965).
A belief state at time t is defined as the posterior probability distribution of being in each
state, given the complete history:
bt (s) = Pr(st = s|ht , b0 ).

(2)

The belief state bt is a sufficient statistic for the history ht (Smallwood & Sondik, 1973),
therefore the agent can choose its actions based on the current belief state bt instead of
all past actions and observations. Initially, the agent starts with an initial belief state b0 ,
665

fiRoss, Pineau, Paquet, & Chaib-draa

representing its knowledge about the starting state of the environment. Then, at any time
t, the belief state bt can be computed from the previous belief state bt1 , using the previous
action at1 and the current observation zt . This is done with the belief state update function
 (b, a, z), where bt =  (bt1 , at1 , zt ) is defined by the following equation:
bt (s0 ) =  (bt1 , at1 , zt )(s0 ) =

1
Pr(zt |bt1 , at1 )

O(s0 , at1 , zt )

X

T (s, at1 , s0 )bt1 (s), (3)

sS

where Pr(z|b, a), the probability of observing z after doing action a in belief b, acts as a
normalizing constant such that bt remains a probability distribution:
Pr(z|b, a) =

X

O(s0 , a, z)

s0 S

X

T (s, a, s0 )b(s).

(4)

sS

Now that the agent has a way of computing its belief, the next interesting question is
how to choose an action based on this belief state.
This action is determined by the agents policy , specifying the probability that the
agent will execute any action in any given belief state, i.e.  defines the agents strategy
for all possible situations it could encounter. This strategy should maximize the amount of
reward earned over a finite or infinite time horizon. In this article, we restrict our attention
to infinite-horizon POMDPs where the optimality criterion is to maximize the expected
sum of discounted rewards (also called the return or discounted return). More formally, the
optimal policy   can be defined by the following equation:
#
"
X
X X
(5)
bt (s)
R(s, a)(bt , a) |b0 ,
  = argmax E
t


t=0

sS

aA

where   [0, 1) is the discount factor and (bt , a) is the probability that action a will be
performed in belief bt , as prescribed by the policy .
The return obtained by following a specific policy , from a certain belief state b, is
defined by the value function equation V  :
"
#
X
X


V (b) =
(b, a) RB (b, a) + 
Pr(z|b, a)V ( (b, a, z)) .
(6)
aA

zZ

Here the function RB (b, a) specifies the immediate expected reward of executing action a
in belief b according to the reward function R:
RB (b, a) =

X

b(s)R(s, a).

(7)

sS

The sum over Z in Equation 6 is interpreted as the expected future return over the infinite
horizon of executing action a, assuming the policy  is followed afterwards.
Note that with the definitions of RB (b, a), Pr(z|b, a) and  (b, a, z), one can view a
POMDP as an MDP over belief states (called the belief MDP), where Pr(z|b, a) specifies
the probability of moving from b to  (b, a, z) by doing action a, and RB (b, a) is the immediate
reward obtained by doing action a in b.
666

fiOnline Planning Algorithms for POMDPs

The optimal policy   defined in Equation 5 represents the action-selection strategy
that will maximize equation V  (b0 ). Since there always exists a deterministic policy that
maximizes V  for any belief states (Sondik, 1978), we will generally only consider deterministic policies (i.e. those that assign a probability of 1 to a specific action in every belief
state).
The value function V  of the optimal policy   is the fixed point of Bellmans equation
(Bellman, 1957):
"
#
X
V  (b) = max RB (b, a) + 
Pr(z|b, a)V  ( (b, a, z)) .
(8)
aA

zZ

Another useful quantity is the value of executing a given action a in a belief state b,
which is denoted by the Q-value:
Q (b, a) = RB (b, a) + 

X

Pr(z|b, a)V  ( (b, a, z)).

(9)

zZ

Here the only difference with the definition of V  is that the max operator is omitted. Notice
that Q (b, a) determines the value of a by assuming that the optimal policy is followed at
every step after action a.
We now review different offline methods for solving POMDPs. These are used to guide
some of the online heuristic search methods discussed later, and in some cases they form
the basis of other online solutions.
2.1 Optimal Value Function Algorithm
One can solve optimally a POMDP for a specified finite horizon H by using the value
iteration algorithm (Sondik, 1971). This algorithm uses dynamic programming to compute
increasingly more accurate values for each belief state b. The value iteration algorithm
begins by evaluating the value of a belief state over the immediate horizon t = 1. Formally,
let V be a value function that takes a belief state as parameter and returns a numerical
value in R of this belief state. The initial value function is:
V1 (b) = max RB (b, a).
aA

(10)

The value function at horizon t is constructed from the value function at horizon t  1 by
using the following recursive equation:
"
#
X
Vt (b) = max RB (b, a) + 
Pr(z|b, a)Vt1 ( (b, a, z)) .
(11)
aA

zZ

The value function in Equation 11 defines the discounted sum of expected rewards that the
agent can receive in the next t time steps, for any belief state b. Therefore, the optimal
policy for a finite horizon t is simply to choose the action maximizing Vt (b):
"
#
X
Pr(z|b, a)Vt1 ( (b, a, z)) .
(12)
t (b) = argmax RB (b, a) + 
aA

zZ

667

fiRoss, Pineau, Paquet, & Chaib-draa

This last equation associates an action to a specific belief state, and therefore must be
computed for all possible belief states in order to define a full policy.
A key result by Smallwood and Sondik (1973) shows that the optimal value function for
a finite-horizon POMDP can be represented by hyperplanes, and is therefore convex and
piecewise linear. It means that the value function Vt at any horizon t can be represented by
a set of |S|-dimensional hyperplanes: t = {0 , 1 , . . . , m }. These hyperplanes are often
called -vectors. Each defines a linear value function over the belief state space associated
with some action a  A. The value of a belief state is the maximum value returned by one
of the -vectors for this belief state. The best action is the one associated with the -vector
that returned the best value:
X
(s)b(s).
(13)
Vt (b) = max
t

sS

A number of exact value function algorithms leveraging the piecewise-linear and convex
aspects of the value function have been proposed in the POMDP literature (Sondik, 1971;
Monahan, 1982; Littman, 1996; Cassandra, Littman, & Zhang, 1997; Zhang & Zhang,
2001). The problem with most of these exact approaches is that the number of -vectors
needed to represent the value function grows exponentially in the number of observations
at each iteration, i.e. the size of the set t is in O(|A||t1 ||Z| ). Since each new -vector
requires computation time in O(|Z||S|2 ), the resulting complexity of iteration t for exact
approaches is in O(|A||Z||S|2 |t1 ||Z| ). Most of the work on exact approaches has focused
on finding efficient ways to prune the set t , such as to effectively reduce computation.
2.2 Offline Approximate Algorithms
Due to the high complexity of exact solving approaches, many researchers have worked
on improving the applicability of POMDP approaches by developing approximate offline
approaches that can be applied to larger problems.
In the online methods we review below, approximate offline algorithms are often used
to compute lower and upper bounds on the optimal value function. These bounds are
leveraged to orient the search in promising directions, to apply branch-and-bound pruning
techniques, and to estimate the long term reward of belief states, as we will show in Section
3. However, we will generally want to use approximate methods which require very low
computational cost. We will be particularly interested in approximations that use the
underlying MDP1 to compute lower bounds (Blind policy) and upper bounds (MDP, QMDP,
FIB) on the exact value function. We also investigate the usefulness of using more precise
lower bounds provided by point-based methods. We now briefly review the offline methods
which will be featured in our empirical investigation. Some recent publications provide a
more comprehensive overview of offline approximate algorithms (Hauskrecht, 2000; Pineau,
Gordon, & Thrun, 2006).
2.2.1 Blind policy
A Blind policy (Hauskrecht, 2000; Smith & Simmons, 2005) is a policy where the same
action is always executed, regardless of the belief state. The value function of any Blind
1. The MDP defined by the (S, A, T, R) components of the POMDP model.

668

fiOnline Planning Algorithms for POMDPs

policy is obviously a lower bound on V  since it corresponds to the value of one specific
policy that the agent could execute in the environment. The resulting value function is
specified by a set of |A| -vectors, where each -vector specifies the long term expected
reward of following its corresponding blind policy. These -vectors can be computed using
a simple update rule:
at+1 (s) = R(s, a) + 

X

T (s, a, s0 )at (s),

(14)

s0 S

where a0 = minsS R(s, a)/(1). Once these -vectors are computed, we use Equation 13
to obtain the lower bound on the value of a belief state. The complexity of each iteration
is in O(|A||S|2 ), which is far less than exact methods. While this lower bound can be
computed very quickly, it is usually not very tight and thus not very informative.
2.2.2 Point-Based Algorithms
To obtain tighter lower bounds, one can use point-based methods (Lovejoy, 1991; Hauskrecht,
2000; Pineau et al., 2003). This popular approach approximates the value function by updating it only for some selected belief states. These point-based methods sample belief
states by simulating some random interactions of the agent with the POMDP environment,
and then update the value function and its gradient over those sampled beliefs. These approaches circumvent the complexity of exact approaches by sampling a small set of beliefs
and maintaining at most one -vector per sampled belief state. Let B represent the set of
sampled beliefs, then the set t of -vectors at time t is obtained as follows:
a (s)
a,z
t
bt
t

=
=
=
=

R(s, a),
P
a,z
0 , a, z)0 (s0 ), 0  
{a,z
 s0 S T (s, a, s0 )O(s
t1 },
i
i
i |i (s) = P
P
a
a
a
a,z
{b |b =  + zZ argmaxt
sS (s)b(s), a  A},
P
{b |b = argmaxb sS b(s)(s), b  B}.

(15)

t

To ensure that this gives a lower bound, 0 is initialized with a single -vector 0 (s) =

mins0 S,aA R(s0 ,a)
.
1

Since |t1 |  |B|, each iteration has a complexity in O(|A||Z||S||B|(|S|+
|B|)), which is polynomial time, compared to exponential time for exact approaches.
Different algorithms have been developed using the point-based approach: PBVI (Pineau
et al., 2003), Perseus (Spaan & Vlassis, 2005), HSVI (Smith & Simmons, 2004, 2005)
are some of the most recent methods. These methods differ slightly in how they choose
belief states and how they update the value function at these chosen belief states. The
nice property of these approaches is that one can tradeoff between the complexity of the
algorithm and the precision of the lower bound by increasing (or decreasing) the number of
sampled belief points.
2.2.3 MDP
The MDP approximation consists in approximating the value function V  of the POMDP
by the value function of its underlying MDP (Littman et al., 1995). This value function is
an upper bound on the value function of the POMDP and can be computed using Bellmans
equation:
669

fiRoss, Pineau, Paquet, & Chaib-draa

"

M DP
Vt+1
(s) = max R(s, a) + 
aA

X

#

T (s, a, s0 )VtM DP (s0 ) .

s0 S

(16)

P
The value V (b) of a belief state b is then computed as V (b) = sS V M DP (s)b(s). This
can be computed very quickly, as each iteration of Equation 16 can be done in O(|A||S|2 ).
2.2.4 QMDP
The QMDP approximation is a slight variation of the MDP approximation (Littman et al.,
1995). The main idea behind QMDP is to consider that all partial observability disappear
after a single step. It assumes the MDP solution is computed to generate VtM DP (Equation
16). Given this, we define:
DP
QM
t+1 (s, a) = R(s, a) + 

X

T (s, a, s0 )VtM DP (s0 ).

(17)

s0 S

This approximation defines an -vector for each action, and gives an upper bound on V 
that is tighter than V M DP ( i.e. VtQM DP (b)  VtM DP (b) for all belief b). Again, to
obtain the value of a belief state, we use Equation 13, where t will contain one -vector
DP (s, a) for each a  A.
a (s) = QM
t
2.2.5 FIB
The two upper bounds presented so far, QMDP and MDP, do not take into account the
partial observability of the environment. In particular, information-gathering actions that
may help identify the current state are always suboptimal according to these bounds. To
address this problem, Hauskrecht (2000) proposed a new method to compute upper bounds,
called the Fast Informed Bound (FIB), which is able to take into account (to some degree)
the partial observability of the environment. The -vector update process is described as
follows:
at+1 (s) = R(s, a) + 

X

zZ

a0

max

t t

X

O(s0 , a, z)T (s, a, s0 )t (s0 ).

(18)

s0 S

can be initialized to the -vectors found by QMDP at convergence, i.e.
The -vectors
a0 (s) = QM DP (s, a). FIB defines a single -vector for each action and the value of a belief
state is computed according to Equation 13. FIB provides a tighter upper bound than
QMDP ( i.e. VtF IB (b)  VtQM DP (b) for all b ). The complexity of the algorithm remains
acceptable, as each iteration requires O(|A|2 |S|2 |Z|) operations.

3. Online Algorithms for POMDPs
With offline approaches, the algorithm returns a policy defining which action to execute in
every possible belief state. Such approaches tend to be applicable only when dealing with
small to mid-size domains, since the policy construction step takes significant time. In large
POMDPs, using a very rough value function approximation (such as the ones presented
in Section 2.2) tends to substantially hinder the performance of the resulting approximate
670

fiOnline Planning Algorithms for POMDPs

Offline Approaches
Policy Construction

Policy Execution

Online Approaches

Small policy construction step between policy execution steps

Figure 1: Comparison between offline and online approaches.
policy. Even more recent point-based methods produce solutions of limited quality in very
large domains (Paquet et al., 2006).
Hence in large POMDPs, a potentially better alternative is to use an online approach,
which only tries to find a good local policy for the current belief state of the agent. The
advantage of such an approach is that it only needs to consider belief states that are reachable from the current belief state. This focuses computation on a small set of beliefs. In
addition, since online planning is done at every step (and thus generalization between beliefs
is not required), it is sufficient to calculate only the maximal value for the current belief
state, not the full optimal -vector. In this setting, the policy construction steps and the
execution steps are interleaved with one another as shown in Figure 1. In some cases, online
approaches may require a few extra execution steps (and online planning), since the policy is
locally constructed and therefore not always optimal. However the policy construction time
is often substantially shorter. Consequently, the overall time for the policy construction
and execution is normally less for online approaches (Koenig, 2001). In practice, a potential
limitation of online planning is when we need to meet short real-time constraints. In such
case, the time available to construct the plan is very small compared to offline algorithms.
3.1 General Framework for Online Planning
This subsection presents a general framework for online planning algorithms in POMDPs.
Subsequently, we discuss specific approaches from the literature and describe how they vary
in tackling various aspects of this general framework.
An online algorithm is divided into a planning phase, and an execution phase, which
are applied alternately at each time step.
In the planning phase, the algorithm is given the current belief state of the agent and
computes the best action to execute in that belief. This is usually achieved in two steps.
First a tree of reachable belief states from the current belief state is built by looking at
several possible sequences of actions and observations that can be taken from the current
belief. In this tree, the current belief is the root node and subsequent reachable beliefs (as
calculated by the  (b, a, z) function of Equation 3) are added to the tree as child nodes of
their immediate previous belief. Belief nodes are represented using OR-nodes (at which we
must choose an action) and actions are included in between each layer of belief nodes using
AND-nodes (at which we must consider all possible observations that lead to subsequent
671

fiRoss, Pineau, Paquet, & Chaib-draa

b0

[14.4, 18.7]

1

3

[14.4, 17.9]

[12, 18.7]

a1

a2

0.7

z1
[13.7, 16.9]

b1

[15, 20]

b2

z1
[6, 14] b
5

b3

0.5

z2
[9, 15]

b4

[10, 18]

4

a1
0.6

z1

z2

-1

[5.8, 11.5]

0.5

0.3

a2
0.2

0.4

z2
b6

[9, 12]

z1

[13.7, 16.9]
0.8

[11, 20]

z2
b8

b7

[10, 12]

Figure 2: An AND-OR tree constructed by the search process for a POMDP with 2 actions and 2 observations. The belief states OR-nodes are represented by triangular nodes and the action AND-nodes
by circular nodes. The rewards RB (b, a) are represented by values on the outgoing arcs from
OR-nodes and probabilities Pr(z|b, a) are shown on the outgoing arcs from AND-nodes. The
values inside brackets represent the lower and upper bounds that were computed according to
Equations 19 - 22, assuming a discount factor  = 0.95. Also notice in this example that the
action a1 in belief state b1 could be pruned since its upper bound (= 11.5) is lower than the
lower bound (= 13.7) of action a2 in b1 .

beliefs). Then the value of the current belief is estimated by propagating value estimates
up from the fringe nodes, to their ancestors, all the way to the root, according to Bellmans
equation (Equation 8). The long-term value of belief nodes at the fringe is usually estimated
using an approximate value function computed offline. Some methods also maintain both
a lower bound and an upper bound on the value of each node. An example on how such a
tree is contructed and evaluated is presented in Figure 2.
Once the planning phase terminates, the execution phase proceeds by executing the best
action found for the current belief in the environment, and updating the current belief and
tree according to the observation obtained.
Notice that in general, the belief MDP could have a graph structure with cycles. Most
online algorithms handle such a structure by unrolling the graph into a tree. Hence, if they
reach a belief that is already elsewhere in the tree, it is duplicated. These algorithms could
always be modified to handle generic graph structures by using a technique proposed in the
LAO* algorithm (Hansen & Zilberstein, 2001) to handle cycles. However there are some
advantages and disadvantages to doing this. A more in-depth discussion of this issue is
presented in Section 5.4.
A generic online algorithm implementing the planning phase (lines 5-9) and the execution
phase (lines 10-13) is presented in Algorithm 3.1. The algorithm first initializes the tree to
contain only the initial belief state (line 2). Then given the current tree, the planning phase
of the algorithm proceeds by first selecting the next fringe node (line 6) under which it should
pursue the search (construction of the tree). The Expand function (line 7) constructs the
672

fiOnline Planning Algorithms for POMDPs

1: Function OnlinePOMDPSolver()
Static: bc : The current belief state of the agent.
T : An AND-OR tree representing the current search tree.
D: Expansion depth.
L: A lower bound on V  .
U : An upper bound on V  .

2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:

bc  b0
Initialize T to contain only bc at the root
while not ExecutionTerminated() do
while not PlanningTerminated() do
b  ChooseNextNodeToExpand()
Expand(b , D)
UpdateAncestors(b )
end while
Execute best action a for bc
Perceive a new observation z
bc   (bc , a, z)
Update tree T so that bc is the new root
end while

Algorithm 3.1: Generic Online Algorithm.
next reachable beliefs (using Equation 3) under the selected leaf for some pre-determined
expansion depth D and evaluates the approximate value function for all newly created
nodes. The new approximate value of the expanded node is propagated to its ancestors
via the UpdateAncestors function (line 8). This planning phase is conducted until some
terminating condition is met (e.g. no more planning time is available or an -optimal action
is found).
The execution phase of the algorithm executes the best action a found during planning
(line 10) and gets a new observation from the environment (line 11). Next, the algorithm
updates the current belief state and the search tree T according to the most recent action a
and observation z (lines 12-13). Some online approaches can reuse previous computations
by keeping the subtree under the new belief and resuming the search from this subtree at
the next time step. In such cases, the algorithm keeps all the nodes in the tree T under the
new belief bc and deletes all other nodes from the tree. Then the algorithm loops back to
the planning phase for the next time step, and so on until the task is terminated.
As a side note, an online planning algorithm can also be useful to improve the precision
of an approximate value function computed offline. This is captured in Theorem 3.1.
Theorem 3.1. (Puterman, 1994; Hauskrecht, 2000) Let V be an approximate value function and  = supb |V  (b)  V (b)|. Then the approximate value V D (b) returned by a Dstep lookahead from belief b, using V to estimate fringe node values, has error bounded by
|V  (b)  V D (b)|   D .
We notice that for   [0, 1), the error converges to 0 as the depth D of the search tends to
. This indicates that an online algorithm can effectively improve the performance obtained
by an approximate value function computed offline, and can find an action arbitrarily close
to the optimal for the current belief. However, evaluating the tree of all reachable beliefs
within depth D has a complexity in O((|A||Z|)D |S|2 ), which is exponential in D. This
becomes quickly intractable for large D. Furthermore, the planning time available during
the execution may be very short and exploring all beliefs up to depth D may be infeasible.
673

fiRoss, Pineau, Paquet, & Chaib-draa

Hence this motivates the need for more efficient online algorithms that can guarantee similar
or better error bounds.
To be more efficient, most of the online algorithms focus on limiting the number of reachable beliefs explored in the tree (or choose only the most relevant ones). These approaches
generally differ only in how the subroutines ChooseNextNodeToExpand and Expand
are implemented. We classify these approaches into three categories : Branch-and-Bound
Pruning, Monte Carlo Sampling and Heuristic Search. We now present a survey of these
approaches and discuss their strengths and drawbacks. A few other online algorithms do
not proceed via tree search; these approaches are discussed in Section 3.5.
3.2 Branch-and-Bound Pruning
Branch-and-Bound pruning is a general search technique used to prune nodes that are
known to be suboptimal in the search tree, thus preventing the expansion of unnecessary
lower nodes. To achieve this in the AND-OR tree, a lower bound and an upper bound
are maintained on the value Q (b, a) of each action a, for every belief b in the tree. These
bounds are computed by first evaluating the lower and upper bound for the fringe nodes
of the tree. These bounds are then propagated to parent nodes according to the following
equations:

L(b),
if b  F(T )
LT (b) =
(19)
maxaA LT (b, a), otherwise
X
LT (b, a) = RB (b, a) + 
Pr(z|b, a)LT ( (b, a, z)),
(20)
zZ



U (b),
if b  F(T )
maxaA UT (b, a), otherwise
X
UT (b, a) = RB (b, a) + 
Pr(z|b, a)UT ( (b, a, z)),
UT (b) =

(21)
(22)

zZ

where F(T ) denotes the set of fringe nodes in tree T , UT (b) and LT (b) represent the upper
and lower bounds on V  (b) associated to belief state b in the tree T , UT (b, a) and LT (b, a)
represent corresponding bounds on Q (b, a), and L(b) and U (b) are the bounds used at fringe
nodes, typically computed offline. These equations are equivalent to Bellmans equation
(Equation 8), however they use the lower and upper bounds of the children, instead of V  .
Several techniques presented in Section 2.2 can be used to quickly compute lower bounds
(Blind policy) and upper bounds (MDP, QMDP, FIB) offline.
Given these bounds, the idea behind Branch-and-Bound pruning is relatively simple: if a
given action a in a belief b has an upper bound UT (b, a) that is lower than another action as
lower bound LT (b, a), then we know that a is guaranteed to have a value Q (b, a)  Q (b, a).
Thus a is suboptimal in belief b. Hence that branch can be pruned and no belief reached
by taking action a in b will be considered.
3.2.1 RTBSS
The Real-Time Belief Space Search (RTBSS) algorithm uses a Branch-and-Bound approach
to compute the best action to take in the current belief (Paquet et al., 2005, 2006). Starting
674

fiOnline Planning Algorithms for POMDPs

1: Function Expand(b, d)
Inputs: b:
d:
Static: T :
L:
U:

2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:

The belief node we want to expand.
The depth of expansion under b.
An AND-OR tree representing the current search tree.
A lower bound on V  .
An upper bound on V  .

if d = 0 then
LT (b)  L(b)
else
Sort actions {a1 , a2 , . . . , a|A| } such that U (b, ai )  U (b, aj ) if i  j
i1
LT (b)  
while i  |A| and U (b, ai ) >
PLT (b) do
LT (b, ai )  RB (b, ai ) +  zZ Pr(z|b, ai )Expand( (b, ai , z), d  1)
LT (b)  max{LT (b), LT (b, ai )}
ii+1
end while
end if
return LT (b)

Algorithm 3.2: Expand subroutine of RTBSS.
from the current belief, it expands the AND-OR tree in a depth-first search fashion, up to
some pre-determined search depth D. The leaves of the tree are evaluated by using a lower
bound computed offline, which is propagated upwards such that a lower bound is maintained
for each node in the tree.
To limit the number of nodes explored, Branch-and-Bound pruning is used along the way
to prune actions that are known to be suboptimal, thus excluding unnecessary nodes under
these actions. To maximize pruning, RTBSS expands the actions in descending order of their
upper bound (first action expanded is the one with highest upper bound). By expanding
the actions in this order, one never expands an action that could have been pruned if actions
had been expanded in a different order. Intuitively, if an action has a higher upper bound
than the other actions, then it cannot be pruned by the other actions since their lower
bound will never exceed their upper bound. Another advantage of expanding actions in
descending order of their upper bound is that as soon as we find an action that can be
pruned, then we also know that all remaining actions can be pruned, since their upper
bounds are necessarily lower. The fact that RTBSS proceeds via a depth-first search also
increases the number of actions that can be pruned since the bounds on expanded actions
become more precise due to the search depth.
In terms of the framework in Algorithm 3.1, RTBSS requires the ChooseNextNodeToExpand subroutine to simply return the current belief bc . The UpdateAncestors function does not need to perform any operation since bc has no ancestor (root of the tree
T ). The Expand subroutine proceeds via depth-first search up to a fixed depth D, using
Branch-and-Bound pruning, as mentioned above. This subroutine is detailed in Algorithm
3.2. After this expansion is performed, PlanningTerminated evaluates to true and the
best action found is executed. At the end of each time step, the tree T is simply reinitialized
to contain the new current belief at the root node.
The efficiency of RTBSS depends largely on the precision of the lower and upper bounds
computed offline. When the bounds are tight, more pruning will be possible, and the search
will be more efficient. If the algorithm is unable to prune many actions, searching will
675

fiRoss, Pineau, Paquet, & Chaib-draa

be limited to short horizons in order to meet real-time constraints. Another drawback of
RTBSS is that it explores all observations equally. This is inefficient since the algorithm
could explore parts of the tree that have a small probability of occurring and thus have a
small effect on the value function. As a result, when the number of observations is large,
the algorithm is limited to exploring over a short horizon.
As a final note, since RTBSS explores all reacheable beliefs within depth D (except some
reached by suboptimal actions), then it can guarantee the same error bound as a D-step
lookahead (see Theorem 3.1). Therefore, the online search directly improves the precision of
the original (offline) value bounds by a factor  D . This aspect was confirmed empirically in
different domains where the RTBSS authors combined their online search with bounds given
by various offline algorithms. In some cases, their results showed a tremendous improvement
of the policy given by the offline algorithm (Paquet et al., 2006).
3.3 Monte Carlo Sampling
As mentioned above, expanding the search tree fully over a large set of observations is
infeasible except for shallow depths. In such cases, a better alternative may be to sample a
subset of observations at each expansion and only consider beliefs reached by these sampled
observations. This reduces the branching factor of the search and allows for deeper search
within a set planning time. This is the strategy employed by Monte Carlo algorithms.
3.3.1 McAllester and Singh
The approach presented by McAllester and Singh (1999) is an adaptation of the online MDP
algorithm presented by Kearns, Mansour, and Ng (1999). It consists of a depth-limited
search of the AND-OR tree up to a certain fixed horizon D where instead of exploring all
observations at each action choice, C observations are sampled from a generative model. The
probabilities Pr(z|b, a) are then approximated using the observed frequencies in the sample.
The advantage of such an approach is that sampling an observation from the distribution
Pr(z|b, a) can be achieved very efficiently in O(log |S| + log |Z|), while computing the exact
probabilities Pr(z|b, a) is in O(|S|2 ) for each observation z. Thus sampling can be useful to
alleviate the complexity of computing Pr(z|b, a), at the expense of a less precise estimate.
Nevertheless, a few samples is often sufficient to obtain a good estimate as the observations
that have the most effect on Q (b, a) (i.e. those which occur with high probability) are
more likely to be sampled. The authors also apply belief state factorization as in Boyen
and Koller (1998) to simplify the belief state calculations.
For the implementation of this algorithm, the Expand subroutine expands the tree up
to fixed depth D, using Monte Carlo sampling of observations, as mentioned above (see
Algorithm 3.3). At the end of each time step, the tree T is reinitialized to contain only the
new current belief at the root.
Kearns et al. (1999) derive bounds on the depth D and the number of samples C needed
to obtain an -optimal policy with high probability and show that the number of samples
required grows exponentially in the desired accuracy. In practice, the number of samples
required is infeasible given realistic online time constraints. However, performance in terms
of returns is usually good even with many fewer samples.
676

fiOnline Planning Algorithms for POMDPs

1: Function Expand(b, d)
Inputs: b:
d:
Static: T :
C:

2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

The belief node we want to expand.
The depth of expansion under b.
An AND-OR tree representing the current search tree.
The number of observations to sample.

if d = 0 then
LT (b)  maxaA RB (b, a)
else
LT (b)  
for all a  A do
Sample Z = {z1 , z2 , . . . zC } from distribution Pr(z|b, a)
P
N (Z)
LT (b, a)  RB (b, a) +  zZ|Nz (Z)>0 zC Expand( (b, a, z), d  1)
LT (b)  max{LT (b), LT (b, a)}
end for
end if
return LT (b)

Algorithm 3.3: Expand subroutine of McAllester and Singhs Algorithm.

One inconvenience of this method is that no action pruning can be done since Monte
Carlo estimation is not guaranteed to correctly propagate the lower (and upper) bound
property up the tree. In their article, the authors simply approximate the value of the
fringe belief states by the immediate reward RB (b, a); this could be improved by using any
good estimate of V  computed offline. Note also that this approach may be difficult to
apply in domains where the number of actions |A| is large. Of course this may further
impact performance.
3.3.2 Rollout
Another similar online Monte Carlo approach is the Rollout algorithm (Bertsekas & Castanon, 1999). The algorithm requires an initial policy (possibly computed offline). At each
time step, it estimates the future expected value of each action, assuming the initial policy is followed at future time steps, and executes the action with highest estimated value.
These estimates are obtained by computing the average discounted return obtained over a
set of M sampled trajectories of depth D. These trajectories are generated by first taking
the action to be evaluated, and then following the initial policy in subsequent belief states,
assuming the observations are sampled from a generative model. Since this approach only
needs to consider different actions at the root belief node, the number of actions |A| only
influences the branching factor at the first level of the tree. Consequently, it is generally
more scalable than McAllester and Singhs approach. Bertsekas and Castanon (1999) also
show that with enough sampling, the resulting policy is guaranteed to perform at least as
well as the initial policy with high probability. However, it generally requires many sampled
trajectories to provide substantial improvement over the initial policy. Furthermore, the
initial policy has a significant impact on the performance of this approach. In particular, in
some cases it might be impossible to improve the return of the initial policy by just changing
the immediate action (e.g. if several steps need to be changed to reach a specific subgoal to
which higher rewards are associated). In those cases, the Rollout policy will never improve
over the initial policy.
677

fiRoss, Pineau, Paquet, & Chaib-draa

1: Function Expand(b, d)
Inputs: b: The belief node we want to expand.
d: The depth of expansion under b.
Static: T : An AND-OR tree representing the current search tree.
: A set of initial policies.
M : The number of trajectories of depth d to sample.

2: LT (b)  
3: for all a  A do
4: for all    do
5:
Q (b, a)  0
6:
for i = 1 to M do
7:
b  b
8:
a  a
9:
for j = 0 to d do
1 j
10:
Q (b, a)  Q (b, a) + M
 RB (b, a)
11:
z  SampleObservation(b, a)
12:
b   (b, a, z)
13:
a  (b)
14:
end for
15:
end for
16: end for
17: LT (b, a) = max Q (b, a)
18: end for

Algorithm 3.4: Expand subroutine of the Parallel Rollout Algorithm.
To address this issue relative to the initial policy, Chang, Givan, and Chong (2004)
introduced a modified version of the algorithm, called Parallel Rollout. In this case, the
algorithm starts with a set of initial policies. Then the algorithm proceeds as Rollout for
each of the initial policies in the set. The value considered for the immediate action is the
maximum over that set of initial policies, and the action with highest value is executed.
In this algorithm, the policy obtained is guaranteed to perform at least as well as the best
initial policy with high probability, given enough samples. Parallel Rollout can handle
domains with a large number of actions and observations, and will perform well when the
set of initial policies contain policies that are good in different regions of the belief space.
The Expand subroutine of the Parallel Rollout algorithm is presented in Algorithm 3.4.
The original Rollout algorithm by Bertsekas and Castanon (1999) is the same algorithm
in the special case where the set of initial policies  contains only one policy. The other
subroutines proceed as in McAllester and Singhs algorithm.
3.4 Heuristic Search
Instead of using Branch-and-Bound pruning or Monte Carlo sampling to reduce the branching factor of the search, heuristic search algorithms try to focus the search on the most relevant reachable beliefs by using heuristics to select the best fringe beliefs node to expand.
The most relevant reachable beliefs are the ones that would allow the search algorithm to
make good decisions as quickly as possible, i.e. by expanding as few nodes as possible.
There are three different online heuristic search algorithms for POMDPs that have been
proposed in the past: Satia and Lave (1973), BI-POMDP (Washington, 1997) and AEMS
(Ross & Chaib-draa, 2007). These algorithms all maintain both lower and upper bounds
on the value of each node in the tree (using Equations 19 - 22) and only differ in the
specific heuristic used to choose the next fringe node to expand in the AND/OR tree. We
678

fiOnline Planning Algorithms for POMDPs

first present the common subroutines for these algorithms, and then discuss their different
heuristics.
Recalling the general framework of Algorithm 3.1, three steps are interleaved several
times in heuristic search algorithms. First, the best fringe node to expand (according to
the heuristic) in the current search tree T is found. Then the tree is expanded under
this node (usually for only one level). Finally, ancestor nodes values are updated; their
values must be updated before we choose the next node to expand, since the heuristic
value usually depends on them. In general, heuristic search algorithms are slightly more
computationally expensive than standard depth- or breadth-first search algorithms, due to
the extra computations needed to select the best fringe node to expand, and the need to
update ancestors at each iteration. This was not required by the previous methods using
Branch-and-Bound pruning and/or Monte Carlo sampling. If the complexity of these extra
steps is too high, then the benefit of expanding only the most relevant nodes might be
outweighed by the lower number of nodes expanded (assuming a fixed planning time).
In heuristic search algorithms, a particular heuristic value is associated with every fringe
node in the tree. This value should indicate how important it is to expand this node in
order to improve the current solution. At each iteration of the algorithm, the goal is to find
the fringe node that maximizes this heuristic value among all fringe nodes. This can be
achieved efficiently by storing in each node of the tree a reference to the best fringe node
to expand within its subtree, as well as its associated heuristic value. In particular, the
root node always contains a reference to the best fringe node for the whole tree. When a
node is expanded, its ancestors are the only nodes in the tree where the best fringe node
reference, and corresponding heuristic value, need to be updated. These can be updated
efficiently by using the references and heuristic values stored in the lower nodes via a
dynamic programming algorithm, described formally in Equations 23 and 24. Here HT (b)
denotes the highest heuristic value among the fringe nodes in the subtree of b, bT (b) is a
reference to this fringe node, HT (b) is the basic heuristic value associated to fringe node b,
and HT (b, a) and HT (b, a, z) are factors that weigh this basic heuristic value at each level
of the tree T . For example, HT (b, a, z) could be Pr(z|b, a) in order to give higher weight to
(and hence favor) fringe nodes that are reached by the most likely observations.

HT (b)
if b  F(T )

HT (b) =

maxaA HT (b, a)HT (b, a) otherwise
(23)
HT (b, a) = maxzZ HT (b, a, z)HT ( (b, a, z))

b
if b  F(T )

bT (b) =
bT (b, aTb ) otherwise
T ))
bT (b, a) = bT ( (b, a, zb,a
(24)
T
ab = argmaxaA HT (b, a)HT (b, a)
T
zb,a
= argmaxzZ HT (b, a, z)HT ( (b, a, z))
This procedure finds the fringe node b  F(T ) that maximizes the overall heuristic value
QdT (b)
HT (bi , ai )HT (bi , ai , zi ), where bi , ai and zi represent the ith belief,
HT (bc , b) = HT (b) i=1
action and observation on the path from bc to b in T , and dT (b) is the depth of fringe
node b. Note that HT and bT are only updated in the ancestor nodes of the last expanded
node. By reusing previously computed values for the other nodes, we have a procedure
679

fiRoss, Pineau, Paquet, & Chaib-draa

1: Function Expand(b)
Inputs: b:
Static: bc :
T:
L:
U:

2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:

An OR-Node we want to expand.
The current belief state of the agent.
An AND-OR tree representing the current search tree.
A lower bound on V  .
An upper bound on V  .

for each a  A do
for each z  Z do
b0   (b, a, z)
UT (b0 )  U (b0 )
LT (b0 )  L(b0 )
HT (b0 )  HT (b0 )
bT (b0 )  b0
end for
P
LT (b, a)  RB (b, a) +  P zZ Pr(z|b, a)LT ( (b, a, z))
UT (b, a)  RB (b, a) +  zZ Pr(z|b, a)UT ( (b, a, z))

T  argmax
zb,a
zZ HT (b, a, z)HT ( (b, a, z))

T ))

T
HT (b, a) = HT (b, a, zb,a )HT ( (b, a, zb,a
T


bT (b, a)  bT ( (b, a, zb,a ))
end for
LT (b)  max{maxaA LT (b, a), LT (b)}
UT (b)  min{maxaA UT (b, a), UT (b)}

aT
b  argmaxaA HT (b, a)HT (b, a)
 (b, aT )
)H
HT (b)  HT (b, aT
b
T
b
bT (b)  bT (b, aT
b )

Algorithm 3.5: Expand : Expand subroutine for heuristic search algorithms.
that can find the best fringe node to expand in the tree in time linear in the depth of the
tree (versus exponential in the depth of the tree for the exhaustive search through all fringe
nodes). These updates are performed in both the Expand and the UpdateAncestors
subroutines, each of which is described in more detail below. After each iteration, the
ChooseNextNodeToExpand subroutine simply returns the reference to this best fringe
node stored in the root of the tree, i.e. bT (bc ).
The Expand subroutine used by heuristic search methods is presented in Algorithm 3.5.
It performs a one-step lookahead under the fringe node b. The main difference with respect
to previous methods in Sections 3.2 and 3.3 is that the heuristic value and best fringe node
to expand for these new nodes are computed at lines 7-8 and 12-14. The best leaf node
in bs subtree and its heuristic value are then computed according to Equations 23 and 24
(lines 18-20).
The UpdateAncestors function is presented in Algorithm 3.6. The goal of this function is to update the bounds of the ancestor nodes, and find the best fringe node to expand
next. Starting from a given OR-Node b0 , the function simply updates recursively the ancestor nodes of b0 in a bottom-up fashion, using Equations 19-22 to update the bounds and
Equations 23-24 to update the reference to the best fringe to expand and its heuristic value.
Notice that the UpdateAncestors function can reuse information already stored in the
node objects, such that it does not need to recompute  (b, a, z), Pr(z|b, a) and RB (b, a).
However it may need to recompute HT (b, a, z) and HT (b, a) according to the new bounds,
depending on how the heuristic is defined.
Due to the anytime nature of these heuristic search algorithms, the search usually keeps
going until an -optimal action is found for the current belief bc , or the available planning
680

fiOnline Planning Algorithms for POMDPs

1: Function UpdateAncestors(b0 )

2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:

Inputs: b0 : An OR-Node for which we want to update all its ancestors.
Static: bc : The current belief state of the agent.
T : An AND-OR tree representing the current search tree.
L: A lower bound on V  .
U : An upper bound on V  .
while b0 6= bc do
Set (b, a) so that action aPin belief b is parent node of belief node b0
LT (b, a)  RB (b, a) +  P zZ Pr(z|b, a)LT ( (b, a, z))
UT (b, a)  RB (b, a) +  zZ Pr(z|b, a)UT ( (b, a, z))

T  argmax
zb,a
zZ HT (b, a, z)HT ( (b, a, z))
T )H  ( (b, a, z T ))
HT (b, a)  HT (b, a, zb,a
T
b,a
T ))
bT (b, a)  bT ( (b, a, zb,a
LT (b)  maxa0 A LT (b, a0 )
UT (b)  maxa0 A UT (b, a0 )
0
0

aT
b  argmaxa0 A HT (b, a )HT (b, a )
 (b, aT )
)H
HT (b)  HT (b, aT
T
b
b
bT (b)  bT (b, aT
b )
b0  b
end while

Algorithm 3.6: UpdateAncestors : Updates the bounds of the ancestors of all ancestors
of an OR-Node
time is elapsed. An -optimal action is found whenever UT (bc )  LT (bc )   or LT (bc ) 
UT (bc , a0 ), a0 6= argmaxaA LT (bc , a) (i.e. all other actions are pruned, in which case the
optimal action has been found).
Now that we have covered the basic subroutines, we present the different heuristics
proposed by Satia and Lave (1973), Washington (1997) and Ross and Chaib-draa (2007).
We begin by introducing some useful notation.
Given any graph structure G, let us denote F(G) the set of all fringe nodes in G and
HG (b, b0 ) the set of all sequences of actions and observations that lead to belief node b0 from
belief node b in the search graph G. If we have a tree T , then HT (b, b0 ) will contain at most
0
0
a single sequence which we will denote hb,b
T . Now given a sequence h  HG (b, b ), we define
Pr(hz |b, ha ) the probability that we observe the whole sequence of observations hz in h, given
that we start in belief node b and perform the whole sequence of actions ha in h. Finally, we
define Pr(h|b, ) to be the probability that we follow the entire action/observation sequence
h if we start in belief b and behave according to policy . Formally, these probabilities are
computed as follows:
d(h)

Pr(hz |b, ha ) =

Y

Pr(hiz |bhi1 , hia ),

(25)

i=1

d(h)

Pr(h|b, ) =

Y

Pr(hiz |bhi1 , hia )(bhi1 , hia ),

(26)

i=1

where d(h) represents the depth of h (number of actions in the sequence h), hia denotes the
ith action in sequence h, hiz the ith observation in the sequence h, and bhi the belief state
obtained by taking the first i actions and observations of the sequence h from b. Note that
bh0 = b.
681

fiRoss, Pineau, Paquet, & Chaib-draa

3.4.1 Satia and Lave
The approach of Satia and Lave (1973) follows the heuristic search framework presented
above. The main feature of this approach is to explore, at each iteration, the fringe node b
in the current search tree T that maximizes the following term:
bc ,b

HT (bc , b) =  d(hT

)

c ,b
c ,b
Pr(hbT,z
|bc , hbT,a
)(UT (b)  LT (b)),

(27)

where b  F(T ) and bc is the root node of T . The intuition behind this heuristic is simple:
recalling the definition of V  , we note that the weight of the value V  (b) of a fringe node b
bc ,b
c ,b
c ,b
c ,b
is a sequence of optimal
), provided hbT,a
|bc , hbT,a
on V  (bc ) would be exactly  d(hT ) Pr(hbT,z
actions. The fringe nodes where this weight is high have the most effect on the estimate
of V  (bc ). Hence one should try to minimize the error at these nodes first. The term
UT (b)  LT (b) is included since it is an upper bound on the (unknown) error V  (b)  LT (b).
Thus this heuristic focuses the search in areas of the tree that most affect the value V  (bc )
and where the error is possibly large. This approach also uses Branch-and-Bound pruning,
such that a fringe node reached by an action that is dominated in some parent belief b is
never going to be expanded. Using the same notation as in Algorithms 3.5 and 3.6, this
heuristic can be implemented by defining HT (b), HT (b, a) and HT (b, a, z), as follows:
HT (b) = 
UT (b)  LT (b),
1 if UT (b, a) > LT (b),
HT (b, a) =
0 otherwise,
HT (b, a, z) =  Pr(z|b, a),

(28)

The condition UT (b, a) > LT (b) ensures that the global heuristic value HT (bc , b0 ) = 0 if
bc ,b0
is dominated (pruned). This guarantees that such fringe
some action in the sequence hT,a
nodes will never be expanded.
Satia and Laves heuristic focuses the search towards beliefs that are most likely to be
reached in the future, and where the error is large. This heuristic is likely to be efficient in
domains with a large number of observations, but only if the probability distribution over
observations is concentrated over only a few observations. The term UT (b)  LT (b) in the
heuristic also prevents the search from doing unnecessary computations in areas of the tree
where it already has a good estimate of the value function. This term is most efficient when
the bounds computed offline, U and L, are sufficiently informative. Similarly, node pruning
is only going to be efficient if U and L are sufficiently tight, otherwise few actions will be
pruned.
3.4.2 BI-POMDP
Washington (1997) proposed a slightly different approach inspired by the AO algorithm
(Nilsson, 1980), where the search is only conducted in the best solution graph. In the case
of online POMDPs, this corresponds to the subtree of all belief nodes that are reached by
sequences of actions maximizing the upper bound in their parent beliefs.
b
The set of fringe nodes in the best solution graph of G, which we denote F(G),
can be
b
defined formally as F(G) = {b  F(G)|h  HG (bc , b), Pr(h|b, G ) > 0}, where G (b, a) = 1
if a = argmaxa0 A UG (b, a0 ) and G (b, a) = 0 otherwise. The AO algorithm simply specifies
682

fiOnline Planning Algorithms for POMDPs

expanding any of these fringe nodes. Washington (1997) recommends exploring the fringe
node in Fb(G) (where G is the current acyclic search graph) that maximizes UG (b)  LG (b).
Washingtons heuristic can be implemented by defining HT (b), HT (b, a) and HT (b, a, z), as
follows:
HT (b) = 
UT (b)  LT (b),
1 if a = argmaxa0 A UT (b, a0 ),
HT (b, a) =
0 otherwise,
HT (b, a, z) = 1.

(29)

This heuristic tries to guide the search towards nodes that are reachable by promising
actions, especially when they have loose bounds on their values (possibly large error). One
nice property of this approach is that expanding fringe nodes in the best solution graph is
the only way to reduce the upper bound at the root node bc . This was not the case for
Satia and Laves heuristic. However, Washingtons heuristic does not take into account the
probability Pr(hz |b, ha ), nor the discount factor  d(h) , such that it may end up exploring
nodes that have a very small probability of being reached in the future, and thus have
little effect on the value of V  (bc ). Hence, it may not explore the most relevant nodes
for optimizing the decision at bc . This heuristic is appropriate when the upper bound U
computed offline is sufficiently informative, such that actions with highest upper bound
would also usually tend to have highest Q-value. In such cases, the algorithm will focus
its search on these actions and thus should find the optimal action more quickly then if it
explored all actions equally. On the other hand, because it does not consider the observation
probabilities, this approach may not scale well to large observation sets, as it will not be
able to focus its search towards the most relevant observations.
3.4.3 AEMS
Ross and Chaib-draa (2007) introduced a heuristic that combines the advantages of BIPOMDP, and Satia and Laves heuristic. It is based on a theoretical error analysis of tree
search in POMDPs, presented by Ross et al. (2008).
The core idea is to expand the tree such as to reduce the error on V  (bc ) as quickly as
possible. This is achieved by expanding the fringe node b that contributes the most to the
error on V  (bc ). The exact error contribution eT (bc , b) of fringe node b on bc in tree T is
defined by the following equation:
bc ,b

eT (bc , b) =  d(hT

)

Pr(hbTc ,b |bc ,   )(V  (b)  LT (b)).

(30)

This expression requires   and V  to be computed exactly. In practice, Ross and Chaibdraa (2007) suggest approximating the exact error (V  (b)  LT (b)) by (UT (b)  LT (b)),
as was done by Satia and Lave, and Washington. They also suggest approximating   by
some policy T , where T (b, a) represents the probability that action a is optimal in its
parent belief b, given its lower and upper bounds in tree T . In particular, Ross et al. (2008)
considered two possible approximations for   . The first one is based on a uniformity
assumption on the distribution of Q-values between the lower and upper bounds, which
yields:
683

fiRoss, Pineau, Paquet, & Chaib-draa

T (b, a) =

(

2

T (b,a)LT (b))
 (U
UT (b,a)LT (b,a)
0

if UT (b, a) > LT (b),
otherwise,

(31)

where  is a normalization constant such that the sum of the probabilities T (b, a) over all
actions equals 1.
The second is inspired by AO and BI-POMDP, and assumes that the action maximizing
the upper bound is in fact the optimal action:

1 if a = argmaxa0 A UT (b, a0 ),
(32)
T (b, a) =
0 otherwise.
Given the approximation T of   , the AEMS heuristic will explore the fringe node b
that maximizes:
bc ,b
)

HT (bc , b) =  d(hT

Pr(hbTc ,b |bc , T )(UT (b)  LT (b)).

(33)

This can be implemented by defining HT (b), HT (b, a) and HT (b, a, z) as follows:
HT (b) = UT (b)  LT (b),
HT (b, a) = T (b, a),
HT (b, a, z) =  Pr(z|b, a).

(34)

We refer to this heuristic as AEMS1 when T is defined as in Equation 31, and AEMS2
when it is defined as in Equation 32.2
Let us now examine how AEMS combines the advantages of both the Satia and Lave,
and BI-POMDP heuristics. First, AEMS encourages exploration of nodes with loose bounds
and possibly large error by considering the term UT (b)  LT (b) as in previous heuristics.
Moreover, as in Satia and Lave, it focuses the exploration towards belief states that are likely
to be encountered in the future. This is good for two reasons. As mentioned before, if a belief
state has a low probability of occurrence in the future, it has a limited effect on the value
V  (bc ) and thus it is not necessary to know its value precisely. Second, exploring the highly
probable belief states increases the chance that we will be able to reuse these computations
in the future. Hence, AEMS should be able to deal efficiently with large observation sets,
assuming the distribution over observations is concentrated over a few observations. Finally,
as in BI-POMDP, AEMS favors the exploration of fringe nodes reachable by actions that
seem more likely to be optimal (according to T ). This is useful to handle large action
sets, as it focuses the search on actions that look promising. If these promising actions
are not optimal, then this will quickly become apparent. This will work well if the best
actions have the highest probabilities in T . Furthermore, it is possible to define T such
that it automatically prunes dominated actions by ensuring that T (b, a) = 0 whenever
UT (b, a) < LT (b). In such cases, the heuristic will never choose to expand a fringe node
reached by a dominated action.
As a final note, Ross et al. (2008) determined sufficient conditions under which the
search algorithm using this heuristic is guaranteed to find an -optimal action within finite
time. This is stated in Theorem 3.2.
2. The AEMS2 heuristic was also used for a policy search algorithm by Hansen (1998).

684

fiOnline Planning Algorithms for POMDPs

Theorem 3.2. (Ross et al., 2008) Let  > 0 and bc the current belief. If for any tree T and
parent belief b in T where UT (b)  LT (b) > , T (b, a) > 0 for a = argmaxa0 A UT (b, a0 ),
then the AEMS algorithm is guaranteed to find an -optimal action for bc within finite time.
We observe from this theorem that it is possible to define many different policies T
under which the AEMS heuristic is guaranteed to converge. AEMS1 and AEMS2 both
satisfy this condition.
3.4.4 HSVI
A heuristic similar to AEMS2 was also used by Smith and Simmons (2004) for their offline
value iteration algorithm HSVI as a way to pick the next belief point at which to perform
-vector backups. The main difference is that HSVI proceeds via a greedy search that
descends the tree from the root node b0 , going down towards the action that maximizes the
upper bound and then the observation that maximizes Pr(z|b, a)(U ( (b, a, z))L( (b, a, z)))
at each level, until it reaches a belief b at depth d where  d (U (b)  L(b)) < . This
heuristic could be used in an online heuristic search algorithm by instead stopping the
greedy search process when it reaches a fringe node of the tree and then selecting this node
as the one to be expanded next. In such a setting, HSVIs heuristic would return a greedy
approximation of the AEMS2 heuristic, as it may not find the fringe node which actually
bc ,b
maximizes  d(hT ) Pr(hbTc ,b |bc , T )(UT (b)  LT (b)). We consider this online version of the
HSVI heuristic in our empirical study (Section 4). We refer to this extension as HSVI-BFS.
Note that the complexity of this greedy search is the same as finding the best fringe node
via the dynamic programming process that updates HT and bT in the UpdateAncestors
subroutine.
3.5 Alternatives to Tree Search
We now present two alternative online approaches that do not proceed via a lookahead
search in the belief MDP. In all online approaches presented so far, one problem is that no
learning is achieved over time, i.e. everytime the agent encounters the same belief, it has to
recompute its policy starting from the same initial upper and lower bounds computed offline.
The two online approaches presented next address this problem by presenting alternative
ways of updating the initial value functions computed offline so that the performance of
the agent improves over time as it stores updated values computed at each time step.
However, as is argued below and in the discussion (Section 5.2), these techniques lead to
other disadvantages in terms of memory consumption and/or time complexity.
3.5.1 RTDP-BEL
An alternative approach to searching in AND-OR graphs is the RTDP algorithm (Barto
et al., 1995) which has been adapted to solve POMDPs by Geffner and Bonet (1998). Their
algorithm, called RTDP-BEL, learns approximate values for the belief states visited by
successive trials in the environment. At each belief state visited, the agent evaluates all
possible actions by estimating the expected reward of taking action a in the current belief
685

fiRoss, Pineau, Paquet, & Chaib-draa

1: Function OnlinePOMDPSolver()
Static: bc : The current belief state of the agent.
V0 : Initial approximate value function (computed offline).
V : A hashtable of beliefs and their approximate value.
k: Discretization resolution.

2: Initialize bc to the initial belief state and V to an empty hashtable.
3: while not ExecutionTerminated() do
P
4: For all a  A: Evaluate Q(bc , a) = RB (b, a) +  zZ Pr(z|b, a)V (Discretize( (b, a, z), k))
5: a  argmaxaA Q(bc , a)
6: Execute best action a for bc
7: V (Discretize(bc , k))  Q(bc , a)
8: Perceive a new observation z
9: bc   (bc , a, z)
10: end while

Algorithm 3.7: RTDP-Bel Algorithm.
state b with an approximate Q-value equation:
X
Q(b, a) = RB (b, a) + 
Pr(z|b, a)V ( (b, a, z)),

(35)

zZ

where V (b) is the value learned for the belief b.
If the belief state b has no value in the table, then it is initialized to some heuristic value.
The authors suggest using the MDP approximation for the initial value of each belief state.
The agent then executes the action that returned the greatest Q(b, a) value. Afterwards,
the value V (b) in the table is updated with the Q(b, a) value of the best action. Finally,
the agent executes the chosen action and it makes the new observation, ending up in a new
belief state. This process is then repeated again in this new belief.
The RTDP-BEL algorithm learns a heuristic value for each belief state visited. To
maintain an estimated value for each belief state in memory, it needs to discretize the
belief state space to have a finite number of belief states. This also allows generalization of
the value function to unseen belief states. However, it might be difficult to find the best
discretization for a given problem. In practice, this algorithm needs substantial amounts
of memory (greater than 1GB in some cases) to store all the learned belief state values,
especially in POMDPs with large state spaces. The implementation of the RTDP-Bel
algorithm is presented in Algorithm 3.7.
The function Discretize(b, k) returns a discretized belief b0 where b0 (s) = round(kb(s))/k
for all states s  S, and V (b) looks up the value of belief b in a hashtable. If b is not present in
the hashtable, the value V0 (b) is returned by V . Supported by experimental data, Geffner
and Bonet (1998) suggest choosing k  [10, 100], as it usually produces the best results.
Notice that for a discretization resolution of k there are O((k + 1)|S| ) possible discretized
beliefs. This implies that the memory storage required to maintain V is exponential in |S|,
which becomes quickly intractable, even for mid-size problems. Furthermore, learning good
estimates for this exponentially large number of beliefs usually requires a very large number
of trials, which might be infeasible in practice. This technique can sometimes be applied
in large domains when a factorized representation is available. In such cases, the belief can
be maintained as a set of distributions (one for subset of conditionaly independent state
variables) and the discretization applied seperately to each distribution. This can greatly
reduce the possible number of discretized beliefs.
686

fiOnline Planning Algorithms for POMDPs

Algorithm
RTBSS
McAllester
Rollout
Satia and Lave
Washington
AEMS
HSVI-BFS
RTDP-Bel
SOVI

-optimal
yes
high probability
no
yes
acyclic graph
yes
yes
no
yes

Anytime
no
no
no
yes
yes
yes
yes
no
yes

Branch &
Bound
yes
no
no
yes
implicit
implicit
implicit
no
no

Monte
Carlo
no
yes
yes
no
no
no
no
no
no

Heuristic
no
no
no
yes
yes
yes
yes
no
no

Learning
no
no
no
no
no
no
no
yes
yes

Table 1: Properties of various online methods.

3.5.2 SOVI
A more recent online approach, called SOVI (Shani et al., 2005), extends HSVI (Smith &
Simmons, 2004, 2005) into an online value iteration algorithm. This approach maintains a
priority queue of the belief states encountered during the execution and proceeds by doing
-vector updates for the current belief state and the k belief states with highest priority at
each time step. The priority of a belief state is computed according to how much the value
function changed at successor belief states, since the last time it was updated. Its authors
also propose other improvements to the HSVI algorithm to improve scalability, such as a
more efficient -vector pruning technique, and avoiding to use linear programs to update and
evaluate the upper bound. The main drawback of this approach is that it is hardly applicable
in large environments with short real-time constraints, since it needs to perform a value
iteration update with -vectors online, and this can have very high complexity as the number
of -vectors representing the value function increases (i.e. O(k|S||A||Z|(|S| + |t1 |)) to
compute t ).
3.6 Summary of Online POMDP Algorithms
In summary, we see that most online POMDP approaches are based on lookahead search.
To improve scalability, different techniques are used: branch-and-bound pruning, search
heuristics, and Monte Carlo sampling. These techniques reduce the complexity from different angles. Branch-and-bound pruning lowers the complexity related to the action space
size. Monte Carlo sampling has been used to lower the complexity related to the observation space size, and could also potentially be used to reduce the complexity related to the
action space size (by sampling a subset of actions). Search heuristics lower the complexity
related to actions and observations by orienting the search towards the most relevant actions and observations. When appropriate, factored POMDP representations can be used
to reduce the complexity related to state. A summary of the different properties of each
online algorithm is presented in Table 1.
687

fiRoss, Pineau, Paquet, & Chaib-draa

4. Empirical Study
In this section, we compare several online approaches in two domains found in the POMDP
literature: Tag (Pineau et al., 2003) and RockSample (Smith & Simmons, 2004). We consider a modified version of RockSample, called FieldVisionRockSample (Ross & Chaib-draa,
2007), that has a higher observation space than the original RockSample. This environment
is introduced as a means to test and compare the different algorithms in environments with
large observation spaces.
4.1 Methodology
For each environment, we first compare the real-time performance of the different heuristics
presented in Section 3.4 by limiting their planning time to 1 second per action. All heuristics
were given the same lower and upper bounds such that their results would be comparable.
The objective here is to evaluate which search heuristic is most efficient in different types of
environments. To this end, we have implemented the different search heuristics (Satia and
Lave, BI-POMDP, HSVI-BFS and AEMS) into the same best-first search algorithm, such
that we can directly measure the efficiency of the heuristic itself. Results were also obtained
for different lower bounds (Blind and PBVI) to verify how this choice affects the heuristics
efficiency. Finally, we compare how online and offline times affect the performance of each
approach. Except where stated otherwise, all experiments were run on an Intel Xeon 2.4
Ghz with 4GB of RAM; processes were limited to 1GB of RAM.
4.1.1 Metrics to compare online approaches
We compare performance first and foremost in terms of average discounted return at execution time. However, what we really seek with online approaches is to guarantee better
solution quality than that provided by the original bounds. In other words, we seek to
reduce the error of the original bounds as much as possible. This suggests that a good
metric for the efficiency of online algorithms is to compare the improvement in terms of the
error bounds at the current belief before and after the online search. Hence, we define the
error bound reduction percentage to be:
UT (b)  LT (b)
,
(36)
U (b)  L(b)
where UT (b), LT (b), U (b) and L(b) are defined as in Section 3.2. The best online algorithm
should provide the highest error bound reduction percentage, given the same initial bounds
and real-time constraint.
Because the EBR metric does not necessarily reflect true error reduction, we also compare the return guarantees provided by each algorithm, i.e. the lower bounds on the expected
return provided by the computed policies for the current belief. Because improvement of
the lower bound compared to the initial lower bound computed offline is a direct indicator
of true error reduction, the best online algorithm should provide the greatest lower bound
improvement at the current belief, given the same initial bounds and real-time constraint.
Formally, we define the lower bound improvement to be:
EBR(b) = 1 

LBI(b) = LT (b)  L(b).
688

(37)

fiOnline Planning Algorithms for POMDPs

In our experiments, both the EBR and LBI metrics are evaluated at each time step for the
current belief. We are interested in seeing which approach provides the highest EBR and
LBI on average.
We also consider other metrics pertaining to complexity and efficiency. In particular,
we report the average number of belief nodes maintained in the search tree. Methods that
have lower complexity will generally be able to maintain bigger trees, but the results will
show that this does not always relate to higher error bound reduction and returns. We will
also measure the efficiency of reusing part of the search tree by recording the percentage of
belief nodes that were reused from one time step to the next.
4.2 Tag
Tag was initially introduced by Pineau et al. (2003). This environment has also been
used more recently in the work of several authors (Poupart & Boutilier, 2003; Vlassis &
Spaan, 2004; Pineau, 2004; Spaan & Vlassis, 2004; Smith & Simmons, 2004; Braziunas &
Boutilier, 2004; Spaan & Vlassis, 2005; Smith & Simmons, 2005). For this environment, an
approximate POMDP algorithm is necessary because of its large size (870 states, 5 actions
and 30 observations). The Tag environment consists of an agent that has to catch (Tag)
another agent while moving in a 29-cell grid domain. The reader is referred to the work of
Pineau et al. (2003) for a full description of the domain. Note that for all results presented
below, the belief state is represented in factored form. The domain is such that an exact
factorization is possible.
To obtain results in Tag, we run each algorithm in each starting configuration 5 times,
( i.e. 5 runs for each of the 841 different starting joint positions, excluding the 29 terminal
states ). The initial belief state is the same for all runs and consists of a uniform distribution
over the possible joint agent positions.
Table 2 compares the different heuristics by presenting 95% confidence intervals on the
average discounted return per run (Return), average error bound reduction percentage per
time step (EBR), average lower bound improvement per time step (LBI), average belief
nodes in the search tree per time step (Belief Nodes), the average percentage of belief nodes
reused per time step (Nodes Reused), the average online planning time used per time step
(Online Time). In all cases, we use the FIB upper bound and the Blind lower bound. Note
that the average online time is slightly lower than 1 second per step because algorithms
sometimes find -optimal solutions in less than a second.
We observe that the efficiency of HSVI-BFS, BI-POMDP and AEMS2 differs slightly
in this environment and that they outperform the three other heuristics: RTBSS, Satia
and Lave, and AEMS1. The difference can be explained by the fact that the latter three
methods do not restrict the search to the best solution graph. As a consequence, they
explore many irrelevant nodes, as shown by the lower error bound reduction percentage,
lower bound improvement, and nodes reused. This poor reuse percentage explains why
Satia and Lave, and AEMS1 were limited to a lower number of belief nodes in their search
tree, compared to the other methods which reached averages around 70K. The results of the
three other heuristics do not differ much here because the three heuristics only differ in the
way they choose the observations to explore in the search. Since only two observations are
possible after the first action and observation, and one of these observations leads directly
689

fiRoss, Pineau, Paquet, & Chaib-draa

Heuristic
RTBSS(5)
Satia and Lave
AEMS1
HSVI-BFS
BI-POMDP
AEMS2

Return
-10.31  0.22
-8.35  0.18
-6.73  0.15
-6.22  0.19
-6.22  0.15
-6.19  0.15

EBR (%)
22.3  0.4
22.9  0.2
49.0  0.3
75.7  0.4
76.2  0.5
76.3  0.5

LBI
3.03  0.07
2.47  0.04
3.92  0.03
7.69  0.06
7.81  0.06
7.81  0.06

Belief
Nodes
45066  701
36908  209
43693  314
64870  947
79508  1000
80250  1018

Nodes
Reused (%)
0
10.0  0.2
25.1  0.3
54.1  0.7
54.6  0.6
54.8  0.6

Online
Time (ms)
580  9
856  4
814 4
673  5
622  4
623  4

Table 2: Comparison of different search heuristics on the Tag environment using the Blind
policy as a lower bound.

EXIT

A

Figure 3: RockSample[7,8].
to a terminal belief state, the possibility that the heuristics differed significantly was very
limited. Due to this limitation of the Tag domain, we now compare these online algorithms
in a larger and more complex domain: RockSample.
4.3 RockSample
The RockSample problem was originally presented by Smith and Simmons (2004). In this
domain, an agent has to explore the environment and sample some rocks (see Figure 3),
similarly to what a real robot would do on the planet Mars. The agent receives rewards by
sampling rocks and by leaving the environment (at the extreme right of the environment).
A rock can have a scientific value or not, and the agent has to sample only good rocks.
We define RockSample[n, k] as an instance of the RockSample problem with an n  n
grid and k rocks. A state is characterized by k + 1 variables: XP , which defines the position
of the robot and can take values {(1, 1), (1, 2), . . . , (n, n)} and k variables, X1R through XkR ,
representing each rock, which can take values {Good, Bad}.
The agent can perform k + 5 actions: {N orth, South, East, W est, Sample, Check1 , . . . ,
Checkk }. The four motion actions are deterministic. The Sample action samples the
rock at the agents current location. Each Checki action returns a noisy observation from
{Good, Bad} for rock i.
The belief state is represented in factored form by the known position and a set of k
probabilities, namely the probability of each rock being good. Since the observation of a rock
690

fiOnline Planning Algorithms for POMDPs

Heuristic
Satia and Lave
AEMS1
RTBSS(2)
BI-POMDP
HSVI-BFS
AEMS2
AEMS1
Satia and Lave
RTBSS(2)
BI-POMDP
AEMS2
HSVI-BFS

Belief
Nodes
EBR (%)
LBI
Nodes
Reused (%)
Blind: Return:7.35, || = 1, Time:4s
7.35  0
3.64  0
00
509  0
8.92  0
10.30  0.08
9.50  0.11
0.90  0.03
579  2
5.31  0.03
10.30  0.15
9.65  0.02
1.00  0.04
439  0
00
18.43  0.14
33.3  0.5
4.33  0.06
2152  71
29.9  0.6
20.53  0.31
51.7  0.7
5.25  0.07
2582  72
36.5  0.5
20.75  0.15
52.4  0.6
5.30  0.06
3145  101
36.4  0.5
PBVI: Return:5.93, |B| = 64, || = 54, Time:2418s
17.10  0.28
26.1  0.4
1.39  0.03
1461  28
12.2  0.1
19.09  0.21
16.9  0.1
1.17  0.01
2311  25
13.5  0.1
19.45  0.30
22.4  0.3
1.37  0.04
426  1
00
21.36  0.22
49.5  0.2
2.73  0.02
2781  38
32.2  0.2
21.37  0.22
57.7  0.2
3.08  0.02
2910  46
38.2  0.2
21.46  0.22
56.3  0.2
3.03  0.02
2184  33
37.3  0.2
Return

Online
Time (ms)
900
916
886
953
885
859








0
1
2
2
5
6

954
965
540
892
826
826








2
1
7
2
3
2

Table 3: Comparison of different search heuristics in RockSample[7,8] environment, using
the Blind policy and PBVI as a lower bound.

state is independent of the other rock states (it only depends on the known robot position),
the complexity of computing Pr(z|b, a) and  (b, a, z) is greatly reduced. Effectively, the
computation of Pr(z|b, Checki ) reduces to: Pr(z|b, Checki ) = Pr(Accurate|XP , Checki ) 
Pr(XiR = z) + (1  Pr(Accurate|XP , Checki ))  (1  Pr(XiR = z)). The probability that
1+(Xp ,i)
, where (Xp , i) =
the sensor is accurate on rock i, Pr(Accurate|XP , Checki ) =
2
d(X
p ,i)/d0
2
, d(Xp , i) is the euclidean distance between position Xp and the position of rock i,
and d0 is a constant specifying the half efficiency distance. Pr(XiR = z) is obtained directly
from the probability (stored in b) that rock i is good. Similarly,  (b, a, z) can be computed
quite easily as the move actions deterministically affect variable XP , and a Checki action
only changes the probability associated to XiR according to the sensors accuracy.
To obtain our results in RockSample, we run each algorithm in each starting rock configuration 20 times (i.e. 20 runs for each of the 2k different joint rock states). The initial
belief state is the same for all these runs and consists of 0.5 that each rock is good, plus the
known initial robot position.
4.3.1 Real-Time Performance of Online Search
In Table 3, we present 95% confidence intervals on the mean of our metrics of interest, for
RockSample[7,8] (12545 states, 13 actions, 2 observations), with real-time contraints of 1
second per action. We compare performance using two different lower bounds, the Blind
policy and PBVI, and use QMDP for the upper bound in both cases. The performance of
the policy defined by each lower bound is shown in the comparison header. For RTBSS, the
notation RTBSS(k) indicates a k-step lookahead; we use the depth k that yields an average
online time closest to 1 second per action.
Return In terms of the return, we first observe that the AEMS2 and HSVI-BFS heuristics
obtain very similar results. Each of these obtains the highest return by a slight margin with
one of the lower bounds. BI-POMDP obtains a similar return when combined with the
691

fiRoss, Pineau, Paquet, & Chaib-draa

PBVI lower bound, but performs much worse with the Blind lower bound. The two other
heuristics, Satia and Lave, and AEMS1, perform considerably worse in terms of return with
either lower bound.
EBR and LBI In terms of error bound reduction and lower bound improvement, AEMS2
obtains the best results with both lower bounds. HSVI-BFS is a close second. This indicates that AEMS2 can more effectively reduce the true error than the other heuristics,
and therefore, guarantees better performance. While BI-POMDP tends to be less efficient
than AEMS2 and HSVI-BFS, it does significantly better than RTBSS, Satia and Lave, and
AEMS1, which only slightly improve the bounds in both case. Satia and Lave is unable
to increase the Blind lower bound, which explains why it obtains the same return as the
Blind policy. We also observe that the higher the error bound reduction and lower bound
improvement, the higher the average discounted return usually is. This confirms our intuition that guiding the search such as to minimize the error at the current belief bc is a good
strategy to obtain better return.
Nodes Reused In terms of the percentage of nodes reused, AEMS2 and HVSI-BFS
generally obtain the best scores. This allows these algorithms to maintain a higher number
of nodes in their trees, which could also partly explain why they outperform the other
heuristics in terms of return, error bound reduction and lower bound improvement. Note
that RTBSS does not reuse any node in the tree because the algorithm does not store the
tree in memory. As a consequence, the reuse percentage is always 0.
Online Time Finally, we also observe that AEMS2 requires less average online time per
action than the other algorithms to attain its performance. In general, a lower average
online time means the heuristic is efficient at finding -optimal actions in a small amount
of time. The running time for RTBSS is determined by the chosen depth, as it cannot stop
before completing the full lookahead search.
Summary Overall, we see that AEMS2 and HSVI-BFS obtain similar results. However
AEMS2 seems slightly better than HSVI-BFS, as it provides better performance guarantees
(lower error) within a shorter period of time. But the difference is not very significant. This
may be due to the small number of observations in this environment, in which case the two
heuristics expand the tree in very similar ways. In the next section, we explore a domain
with many more observations to evaluate the impact of this factor.
The lower performances of the three other heuristics can be explained by various reasons.
In the case of BI-POMDP, this is due to the fact that it does not take into account the
observation probabilities Pr(z|b, a) and discount factor  in the heuristic value. Hence
it tend to expand fringe nodes that did not affect significantly the value of the current
belief. As for Satia and Lave, its poor performance in the case of the Blind policy can be
explained by the fact that the fringe nodes that maximize this heuristic are always leaves
reached by a sequence of Move actions. Due to the deterministic nature of the Move actions
(Pr(z|b, a) = 1 for these actions, whereas Check actions have Pr(z|b, a) = 0.5 initially), the
heuristic value for fringe nodes reached by Move actions is much higher until the error is
reduced significantly. As a result, the algorithm never explores any nodes under the Check
actions, and the robot always follows the Blind policy (moving east, never checking or
sampling any rocks). This demonstrates the importance of restricting the choice of which
692

fiOnline Planning Algorithms for POMDPs

30

25

V(b0)

20

15

AEMS2
AEMS1
BIPOMDP
HSVIBFS
Satia

10

5 2
10

1

10

0

1

10

10

2

10

3

10

Time (s)

Figure 4: Evolution of the upper and lower bounds on RockSample[7,8].

leaves to explore to those reached by a sequence of actions maximizing the upper bound, as
done in AEMS2, HSVI-BFS and BI-POMDP. In the case of AEMS1, it probably behaves
less efficiently because the term it uses to estimate the probability that a certain action
is optimal is not a good approximation in this environment. Moreover, because AEMS1
does not restrict the exploration to the best solution graph, it probably also suffers, in part,
from the same problems as the Satia and Lave heuristic. RTBSS also did not perform very
well with the Blind lower bound. This is due to the short depth allowed to search the
tree, required to have a running time  1 second/action. This confirms that we can do
significantly better than an exhaustive search by having good heuristics to guide the search.
4.3.2 Long-Term Error Reduction of Online Heuristic Search
To compare the long term performance of the different heuristics, we let the algorithms run
in offline mode from the initial belief state of the environment, and log changes in the lower
and upper bound values of this initial belief state over 1000 seconds. Here, the initial lower
and upper bounds are provided by the Blind policy and QMDP respectively. We see from
Figure 4 that Satia and Lave, AEMS1 and BI-POMDP are not as efficient as HSVI-BFS
and AEMS2 at reducing the error on the bounds. One interesting thing to note is that
the upper bound tends to decrease slowly but continuously, whereas the lower bound often
increases in a stepwise manner. We believe this is due to the fact that the upper bound is
much tighter than the lower bound. We also observe that most of the error bound reduction
happens in the first few seconds of the search. This confirms that the nodes expanded earlier
in the tree have much more impact on the error of bc than those expanded very far in the
tree (e.g. after hundreds of seconds). This is an important result in support of using online
(as opposed to offline) methods.
693

fi22

22

20

20

Average Discounted Return

Average Discounted Return

Ross, Pineau, Paquet, & Chaib-draa

18
16
AEMS2
HSVIBFS
BIPOMDP

14
12
10
8
6 1
10

0

10

18
16

12
10
8
6 1
10

1

10

AEMS2 & Blind
AEMS2 & PBVI(8)
AEMS2 & PBVI(16)

14

Online Time (s)

0

10

1

10

Online Time (s)

Figure 5: Comparison of the return as a Figure 6: Comparison of the return as a
function of the online time in
function of the online time in
RockSample(10,10) for different
RockSample(10,10) for different
online methods.
offline lower bounds.

4.3.3 Influence of Offline and Online Time
We now compare how the performance of online approaches is influenced by the available
online and offline times. This allows us to verify if a particular method is better when the
available online time is shorter (or longer), or whether increasing the offline time could be
beneficial.
We consider the three approaches that have shown best overall performance so far (BIPOMDP, HSVI-BFS and AEMS2) and compare their average discounted return as a function of the online time constraint per action. Experiments were run in RockSample[10,10]
(102,401 states, 15 actions, 2 observations) for each of the following online time constraints:
0.1s, 0.2s, 0.5s, 1s, 2s, 5s and 10s. To vary the offline time, we used 3 different lower
bounds: Blind policy, PBVI with 8 belief points, and PBVI with 16 belief points, taking
respectively 15s, 82s, and 193s. The upper bound used is QMDP in all cases. These results
were obtained on an Intel Xeon 3.0 Ghz processor.
In Figure 5, we observe that AEMS2 fares significantly better than HSVI-BFS and
BI-POMDP for short time constraints. As the time constraint increases, AEMS2 and
HSVI-BFS performs similarly (no significant statistical difference). We also notice that
the performance of BI-POMDP stops improving after 1 second of planning time. This can
be explained by the fact that it does not take into account the observation probabilities
Pr(z|b, a), nor the discount factor. As the search tree grows bigger, more and more fringe
nodes have small probability of being reached in the future, such that it becomes more and
more important to take these probabilities into account in order to improve performance.
Otherwise, as we observe in the case of BI-POMDP, most expanded nodes do not affect the
quality of the solution found.
From Figure 6, we observe that increasing the offline time has a beneficial effect mostly
when we have very short real-time constraints. When more online planning time is available,
694

fiOnline Planning Algorithms for POMDPs

the difference between the performances of AEMS2 with the Blind lower bound, and AEMS2
with PBVI becomes insignificant. However, for online time constraints smaller than one
second, the difference in performance is very large. Intuitively, with very short real-time
constraints the algorithm does not have enough time to expand a lot of nodes, such that the
policy found relies much more on the bounds computed offline. On the other hand, with
longer time constraints, the algorithm has enough time to significantly improve the bounds
computed offline, and thus the policy found does not rely as much on the offline bounds.
4.4 FieldVisionRockSample
It seems from the results presented thus far that HSVI-BFS and AEMS2 have comparable
performance on standard domains. We note however that these environments have very
small observation sets (assuming observations with zero probability are removed). We
believe AEMS2 is especially well suited for domains with large observation spaces. However,
there are few such standard problems in the literature. We therefore consider a modified
version of the RockSample environment, called FieldVisionRockSample (Ross & Chaib-draa,
2007), which has an observation space size exponential in the number of rocks.
The FieldVisionRockSample (FVRS) problem differs from the RockSample problem only
in the way the robot is able to perceive the rocks in the environment. Recall that in
RockSample, the agent has to do a Check action on a specific rock to observe its state
through a noisy sensor. In FVRS, the robot observes the state of all rocks, through the
same noisy sensor, after any action is conducted in the environment. Consequently, this
eliminates the use of Check actions, and the remaining actions for the robot include only
the four move actions {North, East, South, West} and the Sample action. The robot can
perceive each rock as being either Good or Bad, thus the observation space size is 2k for
an instance of the problem with k rocks. As in RockSample, the efficiency of the sensor is
defined through the parameter  = 2d/d0 , where d is the distance of the rock and d0 is the
half efficiency distance. We assume the sensors observations are independent for each rock.
In FVRS, the partial observability of the environment is directly proportional to the
parameter d0 : as d0 increases, the sensor becomes more accurate and the uncertainty on
the state of the environment decreases. The value d0 defined for the different instances of
RockSample in the work of Smith and Simmons (2004) is too high for the FVRS problem
(especially in the bigger instances of RockSample), making it almost completely observable.
Consequently, we re-define the value d0 for the different instances of the FieldVisionRockSample according to the size of the grid (n). By considering the fact that
p in an n  n grid,
the largest possible distance between a rock and the robot is (n  1) (2), it seems reasonable that at this distance, the probability of observing the real state of the rock should
be close to 50%
p for the problem to remain partially observable. Consequently, we define
d0 = (n  1) (2)/4.
To obtain results for the FVRS domain, we run each algorithm in each starting rock
configurations 20 times (i.e. 20 runs for each of the 2k different joint rock states). The
initial belief state is the same for all runs and corresponds to a probability of 0.5 that each
rock is good, as well as the known initial position of the robot.
695

fiRoss, Pineau, Paquet, & Chaib-draa

Heuristic
RTBSS(2)
AEMS1
Satia and Lave
HSVI-BFS
AEMS2
BI-POMDP
RTBSS(1)
BI-POMDP
Satia and Lave
AEMS1
AEMS2
HSVI-BFS

Belief
Nodes
Return
EBR (%)
LBI
Nodes
Reused (%)
FVRS[5,5] [Blind: Return:8.15, || = 1, Time=170ms]
16.54  0.37
18.4  1.1
2.80  0.19
18499  102
00
16.88  0.36
17.1  1.1
2.35  0.16
8053  123
1.19  0.07
18.68  0.39
15.9  1.2
2.17  0.16
7965  118
0.88  0.06
20.27  0.44
23.8  1.4
2.64  0.14
4494  105
4.50  0.80
21.18  0.45
31.5  1.5
3.11  0.15
12301  440
3.93  0.22
22.75  0.47
31.1  1.2
3.30  0.17
12199  427
2.26  0.44
FVRS[5,7] [Blind: Return:8.15, || = 1, Time=761ms]
20.57  0.23
7.72  0.13
2.07  0.11
516  1
00
22.75  0.25
11.1  0.4
2.08  0.07
4457  61
0.37  0.11
22.79  0.25
11.1  0.4
2.05  0.08
3683  52
0.36  0.07
23.31  0.25
12.4  0.4
2.24  0.08
3856  55
1.36  0.13
23.39  0.25
13.3  0.4
2.35  0.08
4070  58
1.64  0.14
23.40  0.25
13.0  0.4
2.30  0.08
3573  52
1.69  0.27

Online
Time (ms)
3135  27
876  5
878  4
857  12
854  13
782  12
254
923
947
942
944
946








1
2
3
3
2
3

Table 4: Comparison of different search heuristics on different instances of the FieldVisionRockSample environment.

4.4.1 Real-Time Performance of Online Search
In Table 4, we present 95% confidence intervals on the mean for our metrics of interest. We
consider two instances of this environment, FVRS[5,5] (801 states, 5 actions, 32 observations) and FVRS[5,7] (3201 states, 5 actions, 128 observations). In both cases, we use the
QMDP upper bound and Blind lower bound, under real-time constraints of 1 second per
action.
Return In terms of return, we do not observe any clear winner. BI-POMDP performs surpringly well in FVRS[5,5] but significantly worse than AEMS2 and HSVI-BFS in FVRS[5,7].
On the other hand, AEMS2 does significantly better than HSVI-BFS in FVRS[5,5] but both
get very similar performances in FVRS[5,7]. Satia and Lave performs better in this environment than in RockSample. This is likely due to the fact that the transitions in belief
space are no longer deterministic (as was the case with the Move actions in RockSample).
In FVRS[5,5], we also observe that even when RTBSS is given 3 seconds per action to
perform a two-step lookahead, its performance is worse than any of the heuristic search
methods. This clearly shows that expanding all observations equally in the search is not
a good strategy, as many of these observations can have negligible impact for the current
decision.
EBR and LBI In terms of error bound reduction and lower bound improvement, we observe that AEMS2 performs much better than HSVI-BFS in FVRS[5,5], but not significantly
better in FVRS[5,7]. On the other hand, BI-POMDP obtains similar results to AEMS2 in
FVRS[5,5] but does significantly worse in terms of EBR and LBI than in FVRS[5,7]. This
suggests that AEMS2 is consistently effective at reducing the error, even in environments
with large branching factors.
Nodes Reused The percentage of belief nodes reused is much lower in FVRS due to the
much higher branching factor. We observe that HSVI-BFS has the best reuse percentage
696

fiOnline Planning Algorithms for POMDPs

26

35

24
30

22
20

16
14

25
AEMS2
AEMS1
BIPOMDP
HSVIBFS
Satia

V(b0)

V(b0)

18

20

AEMS2
AEMS1
BIPOMDP
HSVIBFS
Satia

15

12
10

10

8
6 2
10

1

10

0

1

10

10

2

10

5 1
10

3

10

Time (s)

0

10

1

10
Time (s)

2

10

3

10

Figure 7: Evolution of the upper and lower Figure 8: Evolution of the upper and lower
bounds on FieldVisionRockSambounds on FieldVisionRockSample[5,5].
ple[5,7].

in all environments, however not significantly higher than AEMS2. Both of these methods
reuse significantly larger portion of the tree than the other methods. This confirms that
these two methods are able to guide the search towards the most likely beliefs.
4.4.2 Long-Term Error Reduction of Online Heuristic Search
Overall, while Table 4 confirms the consistent performance of HSVI-BFS and AEMS2, the
difference with other heuristics is modest. Considering the complexity of this environment,
this may be due to the fact that the algorithms do not have enough time to expand a
significant number of nodes within 1 second. The long-term analysis of the bounds evolution
in Figures 7 and 8 confirms this. We observe in these figures that the lower bound converges
slightly more rapidly with AEMS2 than with other heuristics. The AEMS1 heuristic also
performs well in the long run on this problem, and seems to be the second best heuristic,
while Satia and Lave is not far behind. On the other hand, the HSVI-BFS heuristic is far
worse in this problem than in RockSample. This seems to be in part due to the fact that
this heuristic takes more time to find the next node to expand than the others, and thus
explores fewer belief states.

5. Discussion
The previous sections presented and evaluated several online POMDP algorithms. We now
discuss important issues that arise when applying online methods in practice, and summarize
some of their advantages and disadvantages. This should help researchers decide whether
online algorithms are a good approach for solving a given problem.
697

fiRoss, Pineau, Paquet, & Chaib-draa

5.1 Lower and Upper Bound Selection
Online algorithms can be combined with many valid lower and upper bounds. However,
there are some properties that these bounds should satisfy for the online search to perform efficiently in practice. One of the desired properties is that the lower and upper
bound functions
should
property states that b : L(b) 


P be monotone. The monotone
maxaA RB (b, a) +  PzZ Pr(z|b, a)L( (b, a, z))  for the lower bound and b : U (b) 
maxaA RB (b, a) +  zZ Pr(z|b, a)U ( (b, a, z)) for the upper bound. This property
guarantees that when a certain fringe node is expanded, its lower bound is non-decreasing
and its upper bound is non-increasing. This is sufficient to guarantee that the error bound
UT (b)  LT (b) on b is non-increasing after the expansion of b, such that the error bound
given by the algorithm on the value of the root belief state bc , cannot be worse than the
error bound defined by the initial bounds given. Note however that monotonicity is not
necessary for AEMS to converge to an -optimal solution, as shown in previous work (Ross
et al., 2008); boundedness is sufficient.
5.2 Improving the Bounds over Time
As we mentioned in our survey of online algorithms, one drawback of many online approaches is that they do not store improvements made to the offline bounds during the
online search, such that, if the same belief state is encountered again, the same computations need to be performed again, restarting from the same offline bounds. A trivial way
to improve this is to maintain a large hashtable (or database) of belief states for which we
have improved the lower and upper bounds in previous search, with their associated new
bounds. There are however many drawbacks in doing this. First every time we want to
evaluate the lower and upper bound of a fringe belief, a search through this hashtable needs
to be performed to check if we have better bounds available. This may require significant
time if the hashtable is large (e.g. millions of beliefs). Furthermore, experiments conducted
with RTDP-Bel in large domains, such as RockSample[7,8], have shown that such a process
usually runs out of memory (i.e. requires more than 1 GB) before good performance is
achieved and requires several thousands episodes before performing well (Paquet, 2006).
The authors of RTBSS have also tried combining their search algorithm with RTDPBel such as to preserve the improvements made by the search (Paquet, 2006). While this
combination usually performed better and learned faster than RTDP-Bel alone, it was found
that in most domains, a few thousand episodes are still required before any improvement
can be seen (in terms of return). Hence, such point updates of the offline bounds tend to
be useful in large domains only if the task to accomplish is repeated a very large number
of times.
A better strategy to improve the lower bound might be to save some time to perform
-vector updates for some of the beliefs expanded during the search, such that the offline
lower bound improves over time. Such updates have the advantage of improving the lower
bound over the whole belief space, instead of at a single belief state. However this can be
very time consuming, especially in large domains. Hence, if we need to act within very short
time constraints, such an approach is infeasible. However if several seconds of planning time
are available per action, then it might be advantageous to use some of this time to perform
-vector updates, rather than use all the available time to search through the tree. A good
698

fiOnline Planning Algorithms for POMDPs

idea here would be to perform -vector updates for a subset of the beliefs in the search tree,
where the lower bound most improves.
5.3 Factored POMDP Representations
The efficiency of online algorithms relies heavily on the ability to quickly compute  (b, a, z)
and Pr(z|b, a), as these must be computed for evey belief state in the search tree. Using
factored POMDP representations is an effective way to reduce the time complexity of computing these quantities. Since most environments with large state spaces are structured and
can be described by sets of features, obtaining factored representation of complex systems
should not be an issue in most cases. However, in domains with significant dependencies
between state features, it may be useful to use algorithms proposed by Boyen and Koller
(1998) and Poupart (2005) to find approximate factored representations where most features
are independent, with minimal degradation in the solution quality. While the upper and
lower bounds might not hold anymore if they are computed over the approximate factored
representation, usually it may still yield good results in practice.
5.4 Handling Graph Structure
As we have mentioned before, the general tree search algorithm used by online algorithms
will duplicate belief states whenever there are multiple paths leading to the same posterior
belief from the current belief bc . This greatly simplifies the complexity related to updating
the values of ancestor nodes, and it also reduces the complexity related to finding the
best fringe node to expand (using the technique in Section 3.4 which is only valid for
trees). The disadvantage of using a tree structure is that inevitably, some computations
will be redundant, as the algorithm will potentially expand the same subtree under every
duplicate belief. To avoid this, we could use the LAO algorithm proposed by Hansen and
Zilberstein (2001) as an extension of AO that can handle generic graph structure, including
cyclic graphs. After each expansion, it runs a value (or policy) iteration algorithm until
convergence among all ancestor nodes in order to update their values.
The heuristics we surveyed in Section 3.4 can be generalized to guide best-first search
algorithms that handle graph structure, like LAO . The first thing to notice is that, in
any graph, if a fringe node is reached by multiple paths, then its error contributes multiple
times to the error on the value of bc . Under this error contribution perspective, the heuristic
value of such a fringe node should be the sum of its heuristic values over all paths reaching
it. For instance, in the case of the AEMS heuristic, using the notation we have defined in
Section 3.4, the global heuristic value of a given fringe node b, on the current belief state
bc in any graph G, can be computed as follows:
HG (bc , b) = (U (b)  L(b))

X

 d(h) Pr(h|bc , G ).

(38)

hHG (bc ,b)

Notice that for cyclic graphs, there can be infinitely many paths in HG (bc , b). In such
case, we could use dynamic programming to estimate the heuristic value.
Because solving HG (bc , b) for all fringe nodes b in the graph G will require a lot of time
in practice, especially if there are many fringe nodes, we have not experimented with this
method in Section 4. However, it would be practical to use this heuristic if we could find an
699

fiRoss, Pineau, Paquet, & Chaib-draa

alternative way to determine the best fringe node without computing HG (bc , b) separately
for each fringe node b and performing an exhaustive search over all fringe nodes.
5.5 Online vs. Offline Time
One important aspect determining the efficiency and applicability of online algorithms is
the amount of time available during the execution for planning. This of course is often taskdependent. For real-time problems like robot navigation, this amount of time may be very
short, e.g. between 0.1 to 1 second per action. On the other hand for tasks like portfolio
management, where acting every second is not necessary, several minutes could easily be
taken to plan any stock buying/selling action. As we have seen from our experiments, the
shorter the available online planning time, the greater the importance of having a good
offline value function to start with. In such case, it is often necessary to reserve sufficient
time to compute a good offline policy. As more and more planning time is available online,
the influence of the offline value function becomes negligible, such that a very rough offline
value function is sufficient to obtain good performance. The best trade-off between online
and offline time often depends on how large the problem is. When the branching factor
(|A||Z|) is large and/or computing successor belief states takes a long time, then more online
time will be required to achieve a significant improvement over the offline value function.
However, for small problems, an online time of 0.1 second per action may be sufficient to
perform near-optimally even with a very rough offline value function.
5.6 Advantages and Disadvantages of Online Algorithms
We now discuss the advantages and disadvantages of online planning algorithms in general.
5.6.1 Advantages
 Most online algorithms can be combined with any offline solving algorithm, assuming
it provides a lower bound or an upper bound on V  , such as to improve the quality
of the policy found offline.
 Online algorithms require very little offline computation before being executable in an
environment, as they can perform well even using very loose bounds, which are quick
to compute.
 Online methods can exploit the knowledge of the current belief to focus computation
on the most relevant future beliefs for the current decision, such that they scale well
to large action and observation spaces.
 Anytime online methods are applicable in real-time environments, as they can be
stopped whenever planning time runs out, and still provide the best solution found
so far.
5.6.2 Disadvantages
 The branching factor depends on the number of actions and observations. Thus if
there are many observations and/or actions, it might be impossible to search deep
700

fiOnline Planning Algorithms for POMDPs

enough, to provide significant improvement of the offline policy. In such cases, sampling methods designed to reduce the branching factor could be useful. While we
cannot guarantee that the lower and upper bounds are still valid when sampling is
used, we can guarantee that they are valid with high probability, given that enough
samples are drawn.
 Most online algorithms do not store improvements made to the offline policy by the
online search, and so the algorithm has to plan again with the same bounds each time
the environment is restarted. If time is available, it could be advantageous to add
-vector updates for some belief states explored in the tree, so that the offline bounds
improve with time.

6. Conclusion
POMDPs provide a rich and elegant framework for planning in stochastic partially observable domains, however their time complexity has been a major issue preventing their
application to complex real-world systems. This paper thoroughly surveys the various existing online algorithms and the key techniques and approximations used to solve POMDPs
more efficiently. We empirically compare these online approaches in several POMDP domains under different metrics: average discounted return, average error bound reduction
and average lower bound improvement, and using different lower and upper bounds: PBVI,
Blind, FIB and QMDP.
From the empirical results, we observe that some of the heuristic search methods, namely
AEMS2 and HSVI-BFS, obtain very good performances, even in domains with large branching factors and large state spaces. These two methods are very similar and perform well
because they orient the search towards nodes that can improve the current approximate
value function as quickly as possible; i.e. the belief nodes that have largest error and are
most likely to be reached in the future by promising actions. However, in environments
with large branching factors, we may only have time to expand a few nodes at each turn.
Hence, it would be interesting to develop further approximations to reduce the branching
factor in such cases.
In conclusion, we believe that online approaches have an important role to play in
improving the scalability of POMDP solution methods. A good example is the succesful
applications of the RTBSS algorithm to the RobocupRescue simulation by Paquet et al.
(2005). This environment is very challenging as the state space is orders of magnitude
beyond the scope of current algorithms. Offline algorithms remain very important to obtain
tight lower and upper bounds on the value function. The interesting question is not whether
online or offline approaches are better, but how we can improve both kinds of approaches,
such that their synergy can be exploited to solve complex real-world problems.

Acknowledgments
This research was supported by the Natural Sciences and Engineering Council of Canada
and the Fonds Quebecois de la Recherche sur la Nature et les Technologies. We would also
like to thank the anonymous reviewers for their helpful comments and suggestions.
701

fiRoss, Pineau, Paquet, & Chaib-draa

References
Astrom, K. J. (1965). Optimal control of Markov decision processes with incomplete state
estimation. Journal of Mathematical Analysis and Applications, 10, 174205.
Barto, A. G., Bradtke, S. J., & Singhe, S. P. (1995). Learning to act using real-time dynamic
programming. Artificial Intelligence, 72 (1), 81138.
Bellman, R. (1957). Dynamic Programming. Princeton University Press, Princeton, NJ,
USA.
Bertsekas, D. P., & Castanon, D. A. (1999). Rollout algorithms for stochastic scheduling
problems. Journal of Heuristics, 5 (1), 89108.
Boyen, X., & Koller, D. (1998). Tractable inference for complex stochastic processes. In
In Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence
(UAI-98), pp. 3342.
Braziunas, D., & Boutilier, C. (2004). Stochastic local search for POMDP controllers. In The
Nineteenth National Conference on Artificial Intelligence (AAAI-04), pp. 690696.
Cassandra, A., Littman, M. L., & Zhang, N. L. (1997). Incremental pruning: a simple, fast,
exact method for partially observable Markov decision processes. In Proceedings of the
Thirteenth Conference on Uncertainty in Artificial Intelligence (UAI-97), pp. 5461.
Chang, H. S., Givan, R., & Chong, E. K. P. (2004). Parallel rollout for online solution
of partially observable Markov decision processes. Discrete Event Dynamic Systems,
14 (3), 309341.
Geffner, H., & Bonet, B. (1998). Solving large POMDPs using real time dynamic programming. In Proceedings of the Fall AAAI symposium on POMDPs, pp. 6168.
Hansen, E. A. (1998). Solving POMDPs by searching in policy space. In Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI-98), pp. 211219.
Hansen, E. A., & Zilberstein, S. (2001). LAO * : A heuristic search algorithm that finds
solutions with loops. Artificial Intelligence, 129 (1-2), 3562.
Hauskrecht, M. (2000). Value-function approximations for partially observable Markov
decision processes. Journal of Artificial Intelligence Research, 13, 3394.
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning and acting in
partially observable stochastic domains. Artificial Intelligence, 101, 99134.
Kearns, M. J., Mansour, Y., & Ng, A. Y. (1999). A sparse sampling algorithm for nearoptimal planning in large markov decision processes. In Proceedings of the Sixteenth
International Joint Conference on Artificial Intelligence (IJCAI-99), pp. 13241331.
Koenig, S. (2001). Agent-centered search. AI Magazine, 22 (4), 109131.
Littman, M. L. (1996). Algorithms for sequential decision making. Ph.D. thesis, Brown
University.
Littman, M. L., Cassandra, A. R., & Kaelbling, L. P. (1995). Learning policies for partially observable environments: scaling up. In Proceedings of the 12th International
Conference on Machine Learning (ICML-95), pp. 362370.
702

fiOnline Planning Algorithms for POMDPs

Lovejoy, W. S. (1991). Computationally feasible bounds for POMDPs. Operations Research,
39 (1), 162175.
Madani, O., Hanks, S., & Condon, A. (1999). On the undecidability of probabilistic planning
and infinite-horizon partially observable Markov decision problems. In Proceedings of
the Sixteenth National Conference on Artificial Intelligence. (AAAI-99), pp. 541548.
McAllester, D., & Singh, S. (1999). Approximate Planning for Factored POMDPs using Belief State Simplification. In Proceedings of the 15th Annual Conference on Uncertainty
in Artificial Intelligence (UAI-99), pp. 409416.
Monahan, G. E. (1982). A survey of partially observable Markov decision processes: theory,
models and algorithms. Management Science, 28 (1), 116.
Nilsson, N. (1980). Principles of Artificial Intelligence. Tioga Publishing.
Papadimitriou, C., & Tsitsiklis, J. N. (1987). The complexity of Markov decision processes.
Mathematics of Operations Research, 12 (3), 441450.
Paquet, S. (2006). Distributed Decision-Making and Task Coordination in Dynamic, Uncertain and Real-Time Multiagent Environments. Ph.D. thesis, Laval University.
Paquet, S., Chaib-draa, B., & Ross, S. (2006). Hybrid POMDP algorithms. In Proceedings
of The Workshop on Multi-Agent Sequential Decision Making in Uncertain Domains
(MSDM-06), pp. 133147.
Paquet, S., Tobin, L., & Chaib-draa, B. (2005). An online POMDP algorithm for complex
multiagent environments. In Proceedings of The fourth International Joint Conference
on Autonomous Agents and Multi Agent Systems (AAMAS-05), pp. 970977.
Pineau, J., Gordon, G., & Thrun, S. (2003). Point-based value iteration: an anytime algorithm for POMDPs. In Proceedings of the International Joint Conference on Artificial
Intelligence (IJCAI-03), pp. 10251032.
Pineau, J., Gordon, G., & Thrun, S. (2006). Anytime point-based approximations for large
POMDPs. Journal of Artificial Intelligence Research, 27, 335380.
Pineau, J. (2004). Tractable planning under uncertainty: exploiting structure. Ph.D. thesis,
Carnegie Mellon University.
Poupart, P. (2005). Exploiting structure to efficiently solve large scale partially observable
Markov decision processes. Ph.D. thesis, University of Toronto.
Poupart, P., & Boutilier, C. (2003). Bounded finite state controllers. In Advances in Neural
Information Processing Systems 16 (NIPS).
Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc.
Ross, S., & Chaib-draa, B. (2007). Aems: An anytime online search algorithm for approximate policy refinement in large POMDPs. In Proceedings of the 20th International
Joint Conference on Artificial Intelligence (IJCAI-07), pp. 25922598.
Ross, S., Pineau, J., & Chaib-draa, B. (2008). Theoretical analysis of heuristic search
methods for online POMDPs. In Advances in Neural Information Processing Systems
20 (NIPS).
703

fiRoss, Pineau, Paquet, & Chaib-draa

Satia, J. K., & Lave, R. E. (1973). Markovian decision processes with probabilistic observation of states. Management Science, 20 (1), 113.
Shani, G., Brafman, R., & Shimony, S. (2005). Adaptation for changing stochastic environments through online POMDP policy learning. In Proceedings of the Workshop on
Reinforcement Learning in Non-Stationary Environments, ECML 2005, pp. 6170.
Smallwood, R. D., & Sondik, E. J. (1973). The optimal control of partially observable
Markov processes over a finite horizon. Operations Research, 21 (5), 10711088.
Smith, T., & Simmons, R. (2004). Heuristic search value iteration for POMDPs. In Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence (UAI-04), pp.
520527.
Smith, T., & Simmons, R. (2005). Point-based POMDP algorithms: improved analysis and
implementation. In Proceedings of the 21th Conference on Uncertainty in Artificial
Intelligence (UAI-05), pp. 542547.
Sondik, E. J. (1971). The optimal control of partially observable Markov processes. Ph.D.
thesis, Stanford University.
Sondik, E. J. (1978). The optimal control of partially observable Markov processes over the
infinite horizon: Discounted costs. Operations Research, 26 (2), 282304.
Spaan, M. T. J., & Vlassis, N. (2004). A point-based POMDP algorithm for robot planning.
In In Proceedings of the IEEE International Conference on Robotics and Automation
(ICRA-04), pp. 23992404.
Spaan, M. T. J., & Vlassis, N. (2005). Perseus: randomized point-based value iteration for
POMDPs. Journal of Artificial Intelligence Research, 24, 195220.
Vlassis, N., & Spaan, M. T. J. (2004). A fast point-based algorithm for POMDPs. In
Benelearn 2004: Proceedings of the Annual Machine Learning Conference of Belgium
and the Netherlands, pp. 170176.
Washington, R. (1997). BI-POMDP: bounded, incremental partially observable Markov
model planning. In Proceedings of the 4th European Conference on Planning, pp.
440451.
Zhang, N. L., & Zhang, W. (2001). Speeding up the convergence of value iteration in partially observable Markov decision processes. Journal of Artificial Intelligence Research,
14, 2951.

704

fiJournal of Artificial Intelligence Research 32 (2008) 169-202

Submitted 10/07; published 05/08

Communication-Based Decomposition Mechanisms
for Decentralized MDPs
Claudia V. Goldman

c.goldman@samsung.com

Samsung Telecom Research Israel
Yakum, Israel

Shlomo Zilberstein

shlomo@cs.umass.edu

Department of Computer Science
University of Massachusetts, Amherst, MA 01003 USA

Abstract
Multi-agent planning in stochastic environments can be framed formally as a decentralized Markov decision problem. Many real-life distributed problems that arise in manufacturing, multi-robot coordination and information gathering scenarios can be formalized
using this framework. However, finding the optimal solution in the general case is hard,
limiting the applicability of recently developed algorithms. This paper provides a practical approach for solving decentralized control problems when communication among the
decision makers is possible, but costly. We develop the notion of communication-based
mechanism that allows us to decompose a decentralized MDP into multiple single-agent
problems. In this framework, referred to as decentralized semi-Markov decision process
with direct communication (Dec-SMDP-Com), agents operate separately between communications. We show that finding an optimal mechanism is equivalent to solving optimally a
Dec-SMDP-Com. We also provide a heuristic search algorithm that converges on the optimal decomposition. Restricting the decomposition to some specific types of local behaviors
reduces significantly the complexity of planning. In particular, we present a polynomialtime algorithm for the case in which individual agents perform goal-oriented behaviors
between communications. The paper concludes with an additional tractable algorithm
that enables the introduction of human knowledge, thereby reducing the overall problem
to finding the best time to communicate. Empirical results show that these approaches
provide good approximate solutions.

1. Introduction
The decentralized Markov decision process has become a common formal tool to study
multi-agent planning and control from a decision-theoretic perspective (Bernstein, Givan,
Immerman, & Zilberstein, 2002; Becker, Zilberstein, Lesser, & Goldman, 2004; Guestrin &
Gordon, 2002; Guestrin, Koller, & Parr, 2001; Nair, Tambe, Yokoo, Pynadath, & Marsella,
2003; Petrik & Zilberstein, 2007; Peshkin, Kim, Meuleau, & Kaelbling, 2000). Seuken
and Zilberstein (2008) provide a comprehensive comparison of the existing formal models
and algorithms. Decentralized MDPs complement existing approaches to coordination of
multiple agents based on on-line learning and heuristic approaches (Wolpert, Wheeler, &
Tumer, 1999; Schneider, Wong, Moore, & Riedmiller, 1999; Xuan, Lesser, & Zilberstein,
2001; Ghavamzadeh & Mahadevan, 2004; Nair, Tambe, Roth, & Yokoo, 2004).
Many challenging real-world problems can be formalized as instances of decentralized
MDPs. In these problems, exchanging information constantly between the decision makers
c
2008
AI Access Foundation. All rights reserved.

fiGoldman & Zilberstein

is either undesirable or impossible. Furthermore, these processes are controlled by a group
of decision makers that must act based on different partial views of the global state. Thus, a
centralized approach to action selection is infeasible. For example, exchanging information
with a single central controller can lead to saturation of the communication network. Even
when the transitions and observations of the agents are independent, the global problem
may not decompose into separate, individual problems, thus a simple parallel algorithm
may not be sufficient. Choosing different local behaviors could lead to different global
rewards. Therefore, agents may need to exchange information periodically and revise their
local behaviors. One important point to understand the model we propose is that although
eventually each agent will behave following some local behavior, choosing among possible
behaviors requires information from other agents. We focus on situations in which this
information is not freely available, but it can be obtained via communication.
Solving optimally a general decentralized control problem has been shown to be computationally hard (Bernstein et al., 2002; Pynadath & Tambe, 2002). In the worst case,
the general problem requires a double-exponential algorithm1 . This difficulty is due to two
main reasons: 1) none of the decision-makers has full-observability of the global system and
2) the global performance of the system depends on a global reward, which is affected by
the agents behaviors. In our previous work (Goldman & Zilberstein, 2004a), we have studied the complexity of solving optimally certain classes of Dec-MDPs and Dec-POMDPs2 .
For example, we have shown that decentralized problems with independent transitions and
observations are considerably easier to solve, namely, they are NP-complete. Even in these
cases, agents behaviors can be dependent through the global reward function, which may
not decompose into separate local reward functions. The latter case has been studied within
the context of auction mechanisms for weakly coupled MDPs by Bererton et al. (2003).
In this paper, the solution to this type of more complex decentralized problems includes
temporally abstracted actions combined with communication actions. Petrik and Zilberstein (2007) have recently presented an improved solution to our previous Coverage Set
algorithm (Becker et al., 2004), which can solve decentralized problems optimally. However, the technique is only suitable when no communication between the agents is possible.
Another recent study by Seuken and Zilberstein (2007a, 2007b) produced a more general
approximation technique based on dynamic programming and heuristic search. While the
approach shows better scalability, it remains limited to relatively small problems compared
to the decomposition method presented here.
We propose an approach to approximate the optimal solutions of decentralized problems
off-line. The main idea is to compute multiagent macro actions that necessarily end with
communication. Assuming that communication incurs some cost, the communication policy
is computed optimally, that is the algorithms proposed in this paper will compute the
best time for the agents to exchange information. At these time points, agents attain full
knowledge of the current global state. These algorithms also compute for each agent what
domain actions to perform between communication, these are temporally abstracted actions
that can be interrupted at any time. Since these behaviors are computed for each agent
1. Unless NEXP is different from EXP, we cannot prove the super-exponential complexity. But, it is
generally believed that NEXP-complete problems require double-exponential time to solve optimally.
2. In Dec-MDPs, the observations of all the agents are sufficient to determine the global state, while in
Dec-POMDPs the global state cannot be fully determined by the observations.

170

fiCommunication-Based Decomposition Mechanism

separately and independently from each other, the final complete solution of communication
and action policies is not guaranteed to be globally optimal. We refer to this approach
as a communication-based decomposition mechanism: the algorithms proposed compute
mechanisms to decompose the global behavior of the agents into local behaviors that are
coordinated by communication. Throughout the paper, these algorithms differ in the space
of behaviors in which they search: our solutions range from the most general search space
available (leading to the optimal mechanism) to more restricted sets of behaviors.
The contribution of this paper is to provide a tractable method, namely communicationbased decomposition mechanisms, to solve decentralized problems, for which no efficient
algorithms currently exist. For general decentralized problems, our approach serves as a
practical way to approximate the solution in a systematic way. We also provide an analysis
about the bounds of these approximations when the local transitions are not independent.
For specific cases, like those with independent transitions and observations, we show how
to compute the optimal decompositions into local behaviors and optimal policies of communication to coordinate the agents behaviors at the global level.
Section 3 introduces the notion of communication-based mechanisms. We formally frame
this approach as a decentralized semi-Markov decision process with direct communication
(Dec-SMDP-Com) in Section 4. Section 5 presents the decentralized multi-step backup
policy-iteration algorithm that returns the optimal decomposition mechanism when no restrictions are imposed on the individual behaviors of the agents. Due to this generality,
the algorithm is applicable in some limited domains. Section 6 presents a more practical
solution, considering that each agent can be assigned local goal states. Assuming local
goal-oriented behavior reduces the complexity of the problem to polynomial in the number of states. Empirical results (Section 6.2) support these claims. Our approximation
mechanism can also be applied when the range of possible local behaviors are provided
at design time. Since these predetermined local behaviors alone may not be sufficient to
achieve coordination, agents still need to decide when to communicate. Section 7 presents
a polynomial-time algorithm that computes the policy of communication, given local policies of domain actions. The closer the human-designed local plans are to local optimal
behaviors, the closer our solution will be to the optimal joint solution. Empirical results for
the Meeting under Uncertainty scenario (also known as the Gathering Problem in robotics,
Suzuki and Yamashita, 1999) are presented in Section 7.1. We conclude with a discussion
of the contributions of this work in Section 8.

2. The Dec-MDP model
Previous studies have shown that decentralized MDPs in general are very hard to solve
optimally and off-line even when direct communication is allowed (Bernstein et al., 2002;
Pynadath & Tambe, 2002; Goldman & Zilberstein, 2004a). A comprehensive complexity
analysis of solving optimally decentralized control problems revealed the sources of difficulty in solving these problems (Goldman & Zilberstein, 2004a). Very few algorithms were
proposed that can actually solve some classes of problems optimally and efficiently.
We define a general underlying process which allows agents to exchange messages directly
with each other as a decentralized POMDP with direct communication:

171

fiGoldman & Zilberstein

Definition 1 (Dec-POMDP-Com) A decentralized partially-observable Markov decision
process with direct communication, Dec-POMDP-Com is given by the following tuple:
M =< S, A1 , A2 , , C , P, R, 1 , 2 , O, T >, where
 S is a finite set of world states, that are factored and include a distinguished initial
state s0 .
 A1 and A2 are finite sets of actions. ai denotes the action performed by agent i.
  denotes the alphabet of messages and i   represents an atomic message sent by
agent i (i.e., i is a letter in the language).
 C is the cost of transmitting an atomic message: C :   <. The cost of transmitting a null message is zero.
 P is the transition probability function. P (s0 |s, a1 , a2 ) is the probability of moving
from state s  S to state s0  S when agents 1 and 2 perform actions a1 and a2
respectively. This transition model is stationary, i.e., it is independent of time.
 R is the global reward function. R(s, a1 , a2 , s0 ) represents the reward obtained by the
system as a whole, when agent 1 executes action a1 and agent 2 executes action a2 in
state s resulting in a transition to state s0 .
 1 and 2 are finite sets of observations.
 O is the observation function. O(o1 , o2 |s, a1 , a2 , s0 ) is the probability of observing o1
and o2 (respectively by the two agents) when in state s agent 1 takes action a1 and
agent 2 takes action a2 , resulting is state s0 .
 If the Dec-POMDP has a finite horizon, it is represented by a positive integer T . The
notation  represents the set of discrete time points of the process.
The optimal solution of such a decentralized problem is a joint policy that maximizes
some criteriain our case, the expected accumulated reward of the system. A joint policy
is a tuple composed of local policies for each agent, each composed of a policy of action
and a policy of communication: i.e., a joint policy  = (1 , 2 ), where iA : i    Ai
and i : i    . That is, a local policy of action assigns an action to any possible
sequence of local observations and messages received. A local policy of communication
assigns a message to any possible sequence of observations and messages received. In each
cycle, agents can perform a domain action, then perceive an observation and then can send
a message.
We assume that the system has independent observations and transitions (see Section 6.3
for a discussion on the general case). Given factored system states s = (s1 , s2 )  S,
the domain actions ai and the observations oi for each agent, the formal definitions3 for
decentralized processes with independent transitions, and observations follow. We note
that this class of problems is not trivial since the reward of the system is not necessarily
independent. For simplicity, we present our definitions for the case of two agents. However,
the approach presented in the paper is applicable to systems with n agents.

3. These definitions are based on Goldman and Zilberstein (2004a). We include them here to make the
paper self-contained.

172

fiCommunication-Based Decomposition Mechanism

Definition 2 (A Dec-POMDP with Independent Transitions) A Dec-POMDP has
independent transitions if the set S of states can be factored into two components S = S1 S2
such that:
s1 , s01  S1 , s2 , s02  S2 , a1  A1 , a2  A2 ,
P r(s01 |(s1 , s2 ), a1 , a2 , s02 ) = P r(s01 |s1 , a1 ) 
P r(s02 |(s1 , s2 ), a1 , a2 , s01 ) = P r(s02 |s2 , a2 ).
In other words, the transition probability P of the Dec-POMDP can be represented as
P = P1  P2 , where P1 = P r(s01 |s1 , a1 ) and P2 = P r(s02 |s2 , a2 ).
Definition 3 (A Dec-POMDP with Independent Observations) A Dec-POMDP has
independent observations if the set S of states can be factored into two components S =
S1  S2 such that:
o1  1 , o2  2 , s = (s1 , s2 ), s0 = (s01 , s02 )  S, a1  A1 , a2  A2 ,
P r(o1 |(s1 , s2 ), a1 , a2 , (s01 , s02 ), o2 ) = P r(o1 |s1 , a1 , s01 )
P r(o2 |(s1 , s2 ), a1 , a2 , (s01 , s02 ), o1 ) = P r(o2 |s2 , a2 , s02 )
O(o1 , o2 |(s1 , s2 ), a1 , a2 , (s01 , s02 )) = P r(o1 |(s1 , s2 ), a1 , a2 , (s01 , s02 ), o2 )P r(o2 |(s1 , s2 ), a1 , a2 , (s01 , s02 ), o1 ).
In other words, the observation probability O of the Dec-POMDP can be decomposed into
two observation probabilities O1 and O2 , such that O1 = P r(o1 |(s1 , s2 ), a1 , a2 , (s01 , s02 ), o2 )
and O2 = P r(o2 |(s1 , s2 ), a1 , a2 , (s01 , s02 ), o1 ).
Definition 4 (Dec-MDP) A decentralized Markov decision process (Dec-MDP) is a DecPOMDP, which is jointly fully observable, i.e., the combination of both agents observations
determine the global state of the system.
In previous work (Goldman & Zilberstein, 2004a), we proved that Dec-MDPs with independent transitions and observations are locally fully-observable. In particular, we showed
that exchanging the last observation is sufficient to obtain complete information about the
current global state and it guarantees optimality of the solution.
We focus on the computation of the individual behaviors of the agents taking into
account that they can exchange information from time to time. The following sections
present the communication-based decomposition approximation method to solve Dec-MDPs
with direct communication and independent transitions and observations.

3. Communication-based Decomposition Mechanism
We are interested in creating a mechanism that will tell us what individual behaviors are
the most beneficial in the sense that these behaviors taken jointly will result in a good
approximation of the optimal decentralized solution of the global system. Notice that
even when the system has a global objective, it is not straightforward to compute the
individual behaviors. The decision problem that requires the achievement of some global
objective does not tell us which local goals each decision maker needs to reach in order to
173

fiGoldman & Zilberstein

maximize the value of a joint policy that reaches the global objective. Therefore, we propose
communication-based decomposition mechanisms as a practical approach for approximating
the optimal joint policy of decentralized control problems. Our approach will produce two
results: 1) a set of temporarily abstracted actions for each global state and for each agent
and 2) a policy of communication, aimed at synchronizing the agents partial information
at the time that is most beneficial to the system.
Formally, a communication-based decomposition mechanism CDM is a function from
any global state of the decentralized problem to two single agent behaviors or policies:
CDM : S  (Opt1 , Opt2 ). In general, a mechanism can be applied to systems with n
agents, in which case the decomposition of the decentralized process will be into n individual behaviors. In order to study communication-based mechanisms, we draw an analogy
between temporary and local policies of actions and options. Options were defined by
Sutton et al. (1999) as temporally abstracted actions, formalized as triplets including a
stochastic single-agent policy, a termination condition, and a set of states in which they can
be initiated: opt =<  : S  A  [0, 1],  : S +  [0, 1], I  S >. An option is available in a
state s if s  I.
Our approach considers options with terminal actions (instead of terminal states). Terminal actions were also considered by Hansen and Zhou (2003) in the framework of indefinite
POMDPs. We denote the domain actions of agent i as Ai . The set of terminal actions only
includes the messages in . For one agent, an option is given by the following tuple:
S
opti =<  : Si    Ai , I  Si >, i.e., an option is a non-stochastic policy from
the agents partial view (local states) and time to the set of its primitive domain actions
and terminal actions. The local states Si are given by the factored representation of the
Dec-MDP with independent transitions and observations. Similarly, the transitions between
local states are known since P (s0 |s, a1 , a2 ) = P1 (s01 |s1 , a1 )  P2 (s02 |s2 , a2 ).
In this paper, we concentrate on terminal actions that are necessarily communication
actions. We assume that all options are terminated whenever at least one of the agents
initiates communication (i.e., the option of the message sender terminates when it communicates and the hearers option terminates due to this external event). We also assume
that there is joint exchange of messages, i.e., whenever one agent initiates communication,
the global state of the system is revealed to all the agents receiving those messages: when
agent 1 sends its observation o1 to agent 2, it will also receive agent 2s observation o2 .
This exchange of messages will cost the system only once. Since we focus on finite-horizon
processes, the options may also be artificially terminated if the time limit of the problem is
reached. The cost of communication C may include, in addition to the actual transmission
cost, the cost resulting from the time it takes to compute the agents local policies.
Communication-based decomposition mechanisms enable the agents to operate separately for certain periods of time. The question, then, is how to design mechanisms that
will approximate best the optimal joint policy of the decentralized problem. We distinguish
between three cases: general options, restricted options, and predefined options.
General options are built from any primitive domain action and communication action
given by the model of the problem. Searching over all possible pairs of local single-agent
policies and communication policies built from these general options will lead to the best
approximation. It is obtained when we compute the optimal mechanism among all possible
mechanisms. Restricted options limit the space of feasible options to a much smaller set de174

fiCommunication-Based Decomposition Mechanism

fined using certain behavior characteristics. Consequently, we can obtain mechanisms with
lower complexity. Such tractable mechanisms provide approximation solutions to decentralized problems for which no efficient algorithms currently exist. Obtaining the optimal
mechanism for a certain set of restricted options (e.g., goal-oriented options) becomes feasible, as we show in Sections 4-6. Furthermore, sometimes, we may consider options that are
pre-defined. For example, knowledge about effective individual procedures may already exist. The mechanism approach allows us to combine such domain knowledge into the solution
of a decentralized problem. In such situations, where a mapping between global states and
single-agent behaviors already exists, the computation of a mechanism returns the policy of
communication at the meta-level of control that synchronizes the agents partial information. In Section 7, we study a greedy approach for computing a policy of communication
when knowledge about local behaviors is given.
Practical concerns lead us to the study of communication-based decomposition mechanisms. In order to design applicable mechanisms, two desirable properties need to be
considered:
 Computational complexity  The whole motivation behind the mechanism approach is based on the idea that the mechanism itself has low computational complexity. Therefore, the computation of the CDM mapping should be practical in the
sense that individual behaviors of each agent will have complexity that is lower than
the complexity of the decentralized problem with free communication. There is a
trade-off between the complexity of computing a mechanism and the global reward
of the system. There may not be a simple way to split the decentralized process into
separate local behaviors. The complexity characteristic should be taken into account
when designing a mechanism; different mechanisms can be computed at different levels
of difficulty.
 Dominance  A mechanism CDM1 dominates another mechanism CDM2 if the
global reward attained by CDM1 with some policy of communication is larger than
the global reward attained by CDM2 with any communication policy. A mechanism
is optimal for a certain problem if there is no mechanism that dominates it.

4. Decentralized Semi-Markov Decision Problems
Solving decentralized MDP problems with a communication-based decomposition mechanism translates into computing the set of individual and temporally abstracted actions that
each agent will perform together with a policy of communication that stipulates when to
exchange information. Hereafter, we show how the problem of computing a mechanism can
be formalized as a semi-Markov decision problem. In particular, the set of basic actions
of this process is composed of the temporally abstracted actions together with the communication actions. The rest of the paper presents three algorithms aimed at solving this
semi-Markov problem optimally. The algorithms differ in the sets of actions available to the
decision-makers, affecting significantly the complexity of finding the decentralized solution.
It should be noted that the optimality of the mechanism computed is conditioned on the
assumptions of each algorithm (i.e., the first algorithm provides the optimal mechanism over
all possible options, the second algorithm provides the optimal mechanism when local goals
are assumed, and the last algorithm computes the optimal policy of communication assum175

fiGoldman & Zilberstein

ing that the local behaviors are given). Formally, a decentralized semi-Markov decision
problem with direct communication (Dec-SMDP-Com) is given as follows:
Definition 5 (Dec-SMDP-Com) A factored, finite-horizon Dec-SMDP-Com over an underlying Dec-MDP-Com M is a tuple
< M , Opt1 , Opt2 , P N , RN > where:
 S, , C , 1 , 2 , P , O and T are components of the underlying process M defined
in definitions 4 and 1.
 Opti is the set of actions available to agent i. It comprises the possible options that
agent i can choose to perform, which terminate necessarily with a communication act:
S
opti =<  : Si    Ai , I  Si >.
 P N (s0 , t+N |s, t, opt1 , opt2 ) is the probability of the system reaching state s0 after exactly
N time units, when at least one option terminates (necessarily with a communication
act). This probability function is given as part of the model for every value of N , such
that t + N  T . In this framework, after N time steps at least one agent initiates
communication (for the first time since time t) and this interrupts the option of the
hearer agent. Then, both agents get full observability of the synchronized state. Since
the decentralized process has independent transitions and observations, P N is the probability that either agent has communicated or both of them have. The probability that
agent i terminated its option exactly at time t + N , PiN , is given as follows:

1




0




 0
0
PiN (s0i , t+N |si , t, opti ) =








 P

if
if
if
if

(opti (si , t)  )  (N = 1)  (s0i = si ))
(opti (si , t)  )  (N = 1)  (s0i 6= si ))
(opti (si , t)  A)  (N = 1))
(opti (si , t)  )  (N > 1))

if (opti (si , t)  A)  (N > 1))
N 0
P
(q
|s
i
i
i , opti (si , t))Pi (si , (t+1)+(N 1)|qi , t+1, opti )
qi Si

The single-agent probability is one when the policy of the option instructs the agent
to communicate (i.e., opti (si , t)  ), in which case the local process remains in the
same local state.
We use the notation s = (s1 , s2 ) and s0 = (s01 , s02 ) to refer to each agents local state.
N
Then, we denote by P i (s0i , t+N |si , t, opti ) the probability that agent i will reach state
0
si in N time steps when it follows the option opti . It refers to the probability of
reaching some state s0i without having terminated the option necessarily when this
state is reached. This transition probability can be computed recursively since the
transition probability of the underlying Dec-MDP is known:
N
P i (s0i , t+N |si , t, opti )

=


0
 Pi (si |si , opti (si , t)))

if N = 1
otherwise
 P
N 0
0
qi Si P (si |qi , opti (qi , t))P i (si , (t+1)+(N 1)|si , t+1, opti )

Finally, we obtain that:
N

P N (s0 , t+N |s, t, opt1 , opt2 ) = P1N (s01 , t+N |s1 , t, opt1 )  P 2 (s02 , t+N |s2 , t, opt2 )+
N

P2N (s02 , t+N |s2 , t, opt2 )  P 1 (s01 , t+N |s1 , t, opt1 )
P1N (s01 , t+N |s1 , t, opt1 )  P2N (s02 , t+N |s2 , t, opt2 )
176

fiCommunication-Based Decomposition Mechanism

 RN (s, t, opt1 , opt2 , s0 , t+N ) is the expected reward obtained by the system N time steps
after the agents started options opt1 and opt2 respectively in state s at time t, when
at least one of them has terminated its option with a communication act (resulting in
the termination of the other agents option). This reward is computed for t+N  T .
(
0

N

R (s, t, opt1 , opt2 , s , t+N ) =

C(opt1 , opt2 , s, s0 , N )
if t+N = T
0
C(opt1 , opt2 , s, s , N ) + C otherwise

C(opt1 , opt2 , s, s0 , N ) is the expected cost incurred by the system when it transitions
between states s and s0 and at least one agent communicates after N time steps. We
define the probability of a certain sequence of global states being transitioned by the
system when each agent follows its corresponding option as P (< s0 , s1 , . . . , sN >):
0

1

N

P (< s , s , . . . , s >) = 

N1
Y

P (sj+1 |sj , opt1 (sj1 ), opt2 (sj2 ))

j=0

 is a normalizing factor that makes sure that over all possible sequences, the probability adds up to one for a given s0 , sN and N steps going through intermediate steps s1 , . . . , sN 1 . Then, we denote by Rseq the reward attained by the system when it traverses a certain sequence of states. Formally, Rseq (< s0 , . . . , sN >
P
j
j
j
j
j+1 ) where 
) = N1
opti (si )) refers to the primitive action
j=0 R(s , opt1 (s1 ), opt2 (s2 ), s
that is chosen by the option at the local state sji . Finally, we can define the expected
cost C(opt1 , opt2 , s, s0 , N ) as follows:
C(opt1 , opt2 , s, s0 , N ) =

X

P (< s, q 1 , . . . , q N1 , s0 >)Rseq (< s, q 1 , . . . , q N1 , s0 >)

q 1 ,...,q N1 S

The dynamics of a semi-Markov decentralized process are as follows. Each agent performs its option starting in some global state s that is fully observed. Each agents option is
a mapping from local states to actions, so agent i starts the option in state si at time t until
it terminates in some state s0i , k time steps later. Whenever the options are terminated,
the agents can fully observe the global state due to the terminal communication actions. If
they reach state s0 at time t+k < T , then the joint policy chooses a possible different pair
of options at state s0 at time t+k and the process continues.
Communication in our model leads to a joint exchange of messages. Therefore all the
agents observe the global state of the system once information is exchanged. This means that
all those states of the decentralized semi-Markov process are fully-observable (as opposed
to jointly fully-observable states as in the classical Dec-MDP-Com).
The local policy for agent i in the Dec-SMDP-Com is a mapping from the global states
to its options (as opposed to a mapping from sequences of observations as in the general
Dec-POMDP case, or a mapping from a local state as in the Dec-MDPs with independent
transitions and observations):
i : S    Opti
177

fiGoldman & Zilberstein

A joint policy is a tuple of local policies, one for each agent, i.e., a joint policy instructs
each agent to choose an option in each global state. Thus, solving for an optimal mechanism is equivalent to solving optimally a decentralized semi-Markov decision problems with
temporally abstracted actions.
Lemma 1 A Dec-SMDP-Com is equivalent to a multi-agent MDP.
Proof. Multiagent MDPs (MMDPs) represent a Markov decision process that is controlled
by several agents (Boutilier, 1999). One important feature of this model is that all the
agents have a central view of the global state. Formally, MMDPs are tuples of the form
< Ag, {Ai }iAg , S, P r, R >, where:





Ag is a finite collection of n agents.
{Ai }iAg represents the joint action space.
S is a finite set of system states.
P r(s0 |s, a1 , . . . , an ) is the transition probability between global states s and s0 when
the agents perform a joint action.
 R : S  < is the reward that the system obtains when a global state is reached.
A decentralized semi-Markov problem with direct communication can be solved optimally by solving the corresponding MMDP. For simpicity of exposition we show the proof
for systems with two agents. Following Definition 5, a 2-agent Dec-SMDP-Com is given
by the tuple: < M , Opt1 , Opt2 , P N , RN >. The mapping between these two models is as
follows: Ag is the same finite collection of agents that control the MMDP and the semiMarkov process. The set S is the set of states of the world in both cases. In the MMDP
model, these states are fully observable by definition. In the semi-Markov decentralized
model these global states are also fully observable because agents always exchange information at the end of any option that they perform. The set of joint actions {Ai }iAg is given
in the semi-Markov process as the set of options available to each agent (e.g., if n = 2 then
{Ai } = {Opt1 , Opt2 }). The difference is that the joint actions are chosen from primitive
domain actions in the MMDP and the options are temporarily abstracted actions which
terminate with a communication act. The probability transition and the reward functions
can be easily mapped between the models by matching P N with P r and RN with R.
The solution to an MMDP (or Dec-SMDP-Com) problem is a strategy that assigns a
joint action (or a set of options) to each global state. Solving an MMDP with actions
given as options solves the semi-Markov problem. Solving a semi-Markov problem when
the options are of length two, i.e., each option is composed of exactly one primitive action
followed by a communication action that tells the agent to communicate its observation
solves the corresponding MMDP problem.
2
Solving a decentralized semi-Markov process with communication is P-complete because
of Lemma 1 and the polynomial complexity of single agent MDPs (Papadimitriou & Tsitsiklis, 1987). However, the input to this problem not only includes the states but also a
double exponential number of domain actions for each agent. As explained in the next
section, each option can be represented as a tree, where: 1) the depth of an option is limited by the finite horizon T and 2) the branching factor of an option is constrained by the
number of states in S. Therefore, the maximal number of leaves an option might have is
178

fiCommunication-Based Decomposition Mechanism

T

bounded by |S|T . Consequently, there can be |A||S| assignments of primitive domain and
communication acts to the leaves in each possible option.
The naive solution to a Dec-SMDP-Com problem is to search the space of all possible
pairs of options and find the pair that maximizes the value of each global state. The multistep policy-iteration algorithm, presented in Section 5, implements a heuristic version of this
search that converges to the optimal mechanism. The resulting search space (after pruning)
can become intractable for even very simple and small problems. Therefore, we propose
to apply communication-based decomposition mechanisms on restricted sets of options.
Solving a Dec-SMDP-Com with a restricted set of options means to find the optimal policy
that attains the maximal value over all possible options in the restricted set (Sutton et al.,
1999; Puterman, 1994). Sections 6 and 7 present two additional algorithms that solve
Dec-SMDP-Com problems when the options considered are goal-oriented options, i.e., the
mechanism assigns local goals to each one of the agents at each global state, allowing them
to communicate before having reached their local goals.

5. Multi-step Backup Policy-Iteration for Dec-SMDP-Com
Solving a Dec-SMDP problem optimally means computing the optimal pair of options for
each fully-observable global state. These options instruct the agents how to act independently of each other until information is exchanged. In order to find these options for each
global state, we apply an adapted and extended version of the multi-step backup policyiteration algorithm with heuristic search (Hansen, 1997). We show that the decentralized
version of this algorithm converges to the optimal policy of the decentralized case with
temporally abstracted actions.
We extend the model of the single-agent POMDP with observations costs to the DecSMDP-Com model. From a global perspective, each agent that follows its own option
without knowing the global state of the system, is following an open-loop policy. However,
locally, each agent is following an option, which does depend on the agents local observations. We first define a multi-step backup for options, when s and s0 are global states of the
decentralized problem: V (s, t, T ) =
min{b,Tt}

max
{
opt1 ,opt2 OPTb

X

X

k=1

s0

P N (s0 , t+k|s, t, opt1 , opt2 )[RN (s, t, opt1 , opt2 , s0 , t+k) + V (s0 , t+k, T )]}

OPTb is the set of options of length at most b, where the length is defined as follows:
Definition 6 (The length of an Option) The length of an option is k if the option can
perform at most k domain actions in one execution.
As in Hansens work, b is a bound on the length of the options (k  b). Here, the finite
horizon Dec-SMDP-Com case is analyzed, therefore b  T . P N (s0 , t+k|s, t, opt1 , opt2 ) and
RN (s, t, opt1 , opt2 , s0 , t+k) are taken from the Dec-SMDP-Com model (Definition 5).
We apply the multi-step backup policy-iteration algorithm (see Figure 2) using the
pruning rule introduced by Hansen (1997), which we adapt to work on pairs of policies
instead of linear sequences of actions. The resulting optimal multi-step backup policy is
equivalent to the optimal policy of the MMDP (Lemma 1), i.e., it is equivalent to the optimal
decentralized policy of a Dec-SMDP-Com with temporally abstracted actions. In order to
179

fiGoldman & Zilberstein

s1

a1

s1

s2

s3

a1

!2

!3

s1

s4

a2

a3

Figure 1: A policy tree of size k=3.
explain the pruning rule for the decentralized case with temporally abstracted actions, we
define what policy-tree structures are.
Definition 7 (Policy-tree) A policy-tree is a tree structure, composed of local state nodes
and corresponding action nodes at each level. Communication actions can only be assigned
to leaves of the tree. The edges connecting an action a (taken at the parent state si ) with a
resulting state s0i have the transition probability Pi (s0i |si , a) assigned to them.
Figure 1 shows a possible policy-tree. An option is represented by a policy-tree with
all its leaves assigned communication actions. We denote a policy-tree sroot , by the state
assigned to its root (e.g., sroot ), and an assignment of domain actions and local states to
the rest of the nodes (e.g., ). The size of a policy-tree is defined as follows:
Definition 8 (Size of a Policy-tree) The size of a policy-tree is k if the longest branch
of the tree, starting from its root is composed of k  1 edges (counting the edges between
actions and resulting states). A policy-tree of size one includes the root state and the action
taken at that state.
(k ) is the policy induced by the assignment  with at most k actions in its implementation. The expected cost g of a policy-tree sroot k is the expected cost that will be incurred
by an agent when it follows the policy (k ). We denote the set of nodes in a tree that do
not correspond to leaves as N L and the set of states assigned to them SN L . The notation
 \ n refers to the  assignment excluding node n. The expected cost of a tree, g(sroot k ),
is computed as follows:
(

g(sroot k ) =

C(aroot )
if k = 1
P
C(aroot ) + s0 SN L [P r(s0i |sroot , aroot )g(s0i ( \ root)k1 )] if 1 < k  T
i

Since the decentralized process has factored states, we can write a global state s as a
pair (s1 , s2 ). Each agent can act independently of each other for some period of time k while
it performs an option. Therefore, we can refer to the information state of the system after
k time steps as s1 ks2 k , where s1 k and s2 k correspond to each agents policy tree of
size k. We assume that at least one agent communicates at time t+k. This will necessarily
180

fiCommunication-Based Decomposition Mechanism

interrupt the other agents option at the same time t+k. Therefore, it is sufficient to look
at pairs of trees of the same size k. The information state refers to the belief an agent
forms about the world based on the partial information available to it while it operates
locally. In our model, agents get full observability once they communicate and exchange
their observations.
The heuristic function that will be used in the search for the optimal decentralized joint
policy of the Dec-SMDP-Com follows the traditional notation, i.e., f (s) = g(s) + h(s).
In our case, these functions will be defined over pairs of policy-trees, i.e., f (sk k ) =
G(sk k ) + H(sk k ). The f value denotes the backed-up value for implementing policies
(k ) and (k ), respectively by the two agents, starting in state s at time t. The expected
value of a state s at time t when the horizon is T is given by the multi-step backup for state
s as follows:
V (s, t, T ) = max {f (s)}.
||,|| b

Note that the policy-trees corresponding to the assignments  and  are of size at most
b  T . We define the expected cost of implementing a pair of policy-trees, G, as the sum
of the expected costs of each one separately. If the leaves have communication actions, the
cost of communication is taken into account in the g functions. As in Hansens work, when
the leaves are not assigned a communication action, we assume that the agents can sense
at no cost to compute the f function.
G(s1 ks2 k ) = g(s1 k ) + g(s2 k ).
An option is a policy-tree with communication actions assigned to all its leaves. That
option is denoted by opt1 (k ) (or opt2 (k )). The message associated with a leaf corresponds
to the local state that is assigned to that leaf by  (or ). We define the expected value of
perfect information of the information state s after k time steps:
H(sk k ) =

X

P N (s0 , t+k|s, t, opt1 (k ), opt2 (k ))V (s0 , t+k, T )

s0 S

The multi-step backup policy-iteration algorithm adapted from Hansen to the decentralized control case appears in Figure 2. Intuitively, the heuristic search over all possible
options unfolds as follows: Each node in the search space is composed of two policy-trees,
each representing a local policy for one agent. The search advances through nodes whose f
value (considering both trees) is greater than the value of the global root state (composed
of the roots of both policy-trees). All nodes whose f value does not follow this inequality
are actually pruned and are not used for updating the joint policy. The policy is updated
when a node, composed of two options is found for which f > V . All the leaves in these
options (at all possible depths) include communication acts. The updated policy  0 maps
the global state s to these two options. When all the leaves in one policy-tree at current
depth i have communication actions assigned, the algorithm assigns communication acts to
all the leaves in the other policy-tree at this same depth. This change in the policies is correct because there is joint exchange of information (i.e., all the actions are interrupted when
at least one agent communicates). We notice, though, that there may be leaves in these
policy-trees at depths lower than i that may still have domain actions assigned. Therefore,
these policy-trees cannot be considered options yet and they remain in the stack. Any leaves
181

fiGoldman & Zilberstein

1. Initialization: Start with an initial joint policy  that assigns a pair of options to each global state s.
2.
Evaluation: s  S, V  (s, t, T ) =
PTPolicy
t P
P N (s0 , t+k|s, t, opt1 (s1 , t), opt2 (s2 , t))[RN (s, t, opt1 (s1 , t), opt2 (s2 , t), s0 , t+k) + V  (s0 , t+k, T )]
k=1
s0
3. Policy Improvement: For each state s = (s1 , s2 )  S :
a. Set-up:
Create a search node for each possible pair of policy-trees with length 1: (s1 1 , s2 1 ).
Compute f (s1 1 ) = G(s1 1 ) + H(s1 1 ).
Push the search node onto a stack.
b. While the search stack is not empty, do:
i. Get the next pair of policy-trees:
Pop a search node off the stack and let it be (s1 i , s2 i )
(the policy-trees of length i starting in state s = (s1 , s2 ))
Let f (si i ) be its estimated value.
ii. Possibly update policy:
if (f (si i ) = G(si i ) + H(si i )) > V (s, t, T ), then
if all leaves at depth i in either i or i have a communication action assigned, then
Assign a communication action to all the leaves in the other policy-tree at depth i
if all leaves in depths  i in both i and i have a communication action assigned, then
Denote these new two options opti1 and opti2 .
Let  0 (s) = (opti1 , opti2 ) and V (s, t, T ) = f (si i ).
iii. Possibly expand node:
If (f (si i ) = G(si i ) + H(si i )) > V (s, t, T ), then
if ((some of the leaves in either i or i have domain actions assigned) and
((i+2)  T )) then
/*At t+1 the new action is taken and there is a transition to another state at t+2*/
Create the successor node of the two policy-trees of length i,
by adding all possible transition states and actions to each leaf of each tree
that does not have a communication action assigned to it.
Calculate the f value for the new node (i.e., either f (si+1 i+1 ) if both policy
trees were expanded, and recalculate f (si i ) if one of them has communication
actions in all the leaves at depth i)
Push the node onto the stack.
/*All nodes with f < V are pruned and are not pushed to the stack.*/
4. Convergence test:
if  =  0 then
return  0
else set  =  0 , GOTO 2.

Figure 2: Multi-step Backup Policy-iteration (MSBPI) using depth-first branch-and-bound.

that remain assigned to domain actions will be expanded by the algorithm. This expansion
requires the addition of all the possible next states, that are reachable by performing the
domain-actions in the leaves, and the addition of a possible action for each such state. If all
the leaves at depth i of one policy-tree are already assigned communication acts, then the
algorithm expands only the leaves with domain actions at lower levels in both policy-trees.
No leaf will be expanded beyond level i because at the corresponding time one agent is
going to initiate communication and this option is going to be interrupted anyways.
In the next section, we show the convergence of our Multi-step Backup Policy-iteration
(MSBPI) algorithm to the optimal decentralized solution of the Dec-SMDP-Com, when
agents follow temporally abstracted actions and the horizon is finite.
182

fiCommunication-Based Decomposition Mechanism

5.1 Optimal Decentralized Solution with Multi-step Backups
In this section, we prove that the MSBPI algorithm presented in Figure 2 converges to
the optimal decentralized control joint policy with temporally abstracted actions and direct
communication. We first show that the policy improvement step in the algorithm based on
heuristic multi-step backups improves the value of the current policy if it is sub-optimal.
Finally, the policy iteration algorithm iterates over improving policies and it is known to
converge.
Theorem 1 When the current joint policy is not optimal, the policy improvement step in
the multi-step backup policy-iteration algorithm always finds an improved joint policy.
Proof. We adapt Hansens proof to our decentralized control problem, when policies are
represented by policy-trees. Algorithm MSBPI in Figure 2 updates the current policy when
the new policy assigns a pair of options that yield a greater value for a certain global state.
We show by induction on the size of the options, that at least for one state, a new option
is found in the improvement step (step 3.b.ii).
If the value of any state can be improved by two policy-trees of size one, then an improved
joint policy is found because all the policy-trees of size one are evaluated. We initialized 
with such policy-trees. We assume that an improved joint policy can be found with policytrees of size at most k. We show that an improved joint policy is found with policy-trees
of size k. Lets assume that k is a policy tree of size k, such that f (sk ) > V (s) with
communication actions assigned to its leaves. If this is the case then the policy followed
by agent 2 will be interrupted at time k at the latest. One possibility is that sk  is
evaluated by the algorithm. Then, an improved joint policy is indeed found. If this pair of
policy-trees was not evaluated by the algorithm, it means that  was pruned earlier. We
assume that this happened at level i. This means that f (si ) < V (s). We assumed that
f (sk ) > V (s) so we obtain that: f (sk ) > f (si ).
If we expand the f values in this inequality, we obtain the following:
g(si )+g(s)+

X

P N (s0 , t+i|s, opt1 (i ), opt2 ())[g(s0 (i, k))+g(s0 )+

X

P N (s00 , t+i+ki)V (s00 )] >

s00

s0

g(si ) + g(s) +

X

P N (s0 , t+i|s, opt1 (k ), opt2 ())V (s0 , t+i)

s0

where g(s0 (i, k)) refers to the expected cost of the subtree starting at level i and ending
at level k starting from s0 . After simplification we obtain:
X

P N (s0 , t+i|s, opt1 (i ), opt2 ())[g(s0 (i, k)) + g(s0 ) +

s0

X

P N (s00 , t+i+ki)V (s00 )] >

s00

X

P N (s0 , t+i|s, opt1 (k ), opt2 ())V (s0 , t+i)

s0

That is, there exists some state s0 for which f (s0 (i, k)) > V (s0 ). Since the policy-tree
(i, k) has size less than k, by the induction assumption we obtain that there exists some
state s0 for which the multi-step backed-up value is increased. Therefore, the policy found
in step 3.b.ii is indeed an improved policy.
2
183

fiGoldman & Zilberstein

Lemma 2 The complexity of computing the optimal mechanism over general options by the
(T 1)
MSBPI algorithm is O(((|A1 | + ||)(|A2 | + ||))|S|
). (General options are based on any
possible primitive domain action in the model, and any communication act).

Proof. Each agent can perform any of the primitive domain actions in Ai and can communicate any possible message in . There can be at most |S|(T 1) leaves in a policy tree
with horizon T and |S| possible resulting states from each transition. Therefore, each time
the MSBPI algorithm expands a policy tree (step 3.b.iii in Figure 2), the number of result(T 1)
ing trees is ((|A1 | + ||)(|A2 | + ||))|S|
. In the worst case, this is the number of trees
that the algorithm will develop in one iteration. Therefore, the size of the search space is a
function of this number times the number of iterations until convergence.
2
Solving optimally a Dec-MDP-Com with independent transitions and observations has
been shown to be in NP (Goldman & Zilberstein, 2004a). As we show here, solving for the
optimal mechanism is harder, although the solution may not be the optimal. This is due
to the main difference between these two problems. In the Dec-MDP-Com, we know that
since the transitions and observations are independent, a local state is a sufficient statistic
for the history of observations. However, in order to compute an optimal mechanism we
need to search in the space of options, that is, no single local state is a sufficient statistic.
When options are allowed to be general, the search space is larger since each possible
option that needs to be considered can be arbitrarily large (with the length of each branch
bounded by T ). For example, in the Meeting under Uncertainty scenario (presented in
Section 7.1), agents aim at meeting in a stochastic environment in the shortest time as
possible. Each agent can choose to perform anyone of six primitive actions (four move
actions, one stay action and a communication action). Even in a small world composed of
100 possible locations, implementing the MSBPI algorithm is intractable. It will require
the expansion of all the possible combinations of pairs of policy-trees leading to a possible
(T 1)
addition of 36100
nodes to the search space at each iteration. Restricting the mechanism
to a certain set of possible options, for example goal-oriented options leads to a significant
reduction in the complexity of the algorithm as we shown in the following two sections.

6. Dec-SMDP-Com with Local Goal-oriented Behavior
The previous section provided an algorithm that computes the optimal mechanism, searching over all possible combinations of domain and communication actions for each agent.
On the one hand, this solution is the most general and does not restrict the individual
behaviors in any aspect. On the other hand, this solution may require the search of a very
large space, even after this space is pruned by the heuristic search technique. Therefore, in
order to provide a practical decomposition mechanism algorithm, it is reasonable to restrict
the mechanism to certain sets of individual behaviors. In this section, we concentrate on
goal-oriented options and propose an algorithm that computes the optimal mechanism with
respect to this set of options: i.e., the algorithm finds a mapping from each global state to
a set of locally goal oriented behaviors with the highest value. The algorithm proposed has
the same structure as the MSBPI algorithm; the main difference is in how the options are
built.
184

fiCommunication-Based Decomposition Mechanism

Definition 9 (Goal-oriented Options) A goal-oriented option is a local policy that achieves
a given local goal.
We study locally goal-oriented mechanisms, which map each global state to a pair of
goal-oriented options and a period of time k. We assume here that a set of local goals Gi
is provided with the problem. For each such local goal, a local policy that can achieve it is
considered a goal-oriented option. When the mechanism is applied, each agent follows its
policy to the corresponding local goal for k time steps. At time k + 1, the agents exchange
information and stop acting (even though they may not have reached their local goal). The
agents, then, become synchronized and they are assigned possibly different local goals and
a working period k 0 .
The algorithm presented in this section, LGO-MSBPI, solves the decentralized control
problem with communication by finding the optimal mapping between global states to local
goals and periods of time (the algorithm finds the best solution it can given that the agents
will act individually for some periods of time). We start with an arbitrary joint policy that
assigns one pair of local goal states and a number k to each global state. The current joint
policy is evaluated and set as the current best known mechanism. Given a joint policy
 : S    G1  G2   , (Gi  Si  S), the value of a state s at a time t, when T is
the finite horizon is given in Equation 1: (this value is only computed for states in which
t+k  T ).

0
if t = T
 P
0
 0
N
N 0
(s
,
t+k|s,
t,

(s
),

(s
P
V  (s, t, T ) =
g1 1
g2 2 ))[Rg (s, t, g1 , g2 , s , k)+V (s , t+k, T )] (1)
s0 S g

s.t. (s, t) = (g1 , g2 , k)

Notice that RgN (s, g1 ,g2 , s0 , k) can be defined similarly to RN () (see Definition 5), taking
into account that the options here are aimed at reaching a certain local goal state (g1 and
g2 are aimed at reaching the local goal states g1 and g2 , respectively).
RgN (s, t, g1 , g2 , s0 , k) = C(g1 , g2 , s, s0 , k)+C =
C +

X

P (< s, q 1 , . . . , q k1 , s0 >)  Cseq (< s, q 1 , . . . , sk1 , s0 >

q 1 ,...,q k1

There is a one-to-one mapping between goals and goal-oriented options. That is, the
policy gi assigned by  can be found by each agent independently by solving optimally
each agents local process M DPi = (Si , Pi , Ri , Gi , T ): The set of global states S is factored
so each agent has its own set of local states. The process has independent transitions, so
Pi is the primitive transition probability known when we described the options framework.
Ri is the cost incurred by an agent when it performs a primitive action ai and zero if the
agent reaches a goal state in Gi . T is the finite horizon of the global problem.
PgN (with the goal g subscript) is different from the probability function P N that appears
in Section 4. PgN is the probability of reaching a global state s0 after k time steps, while
trying to reach g1 and g2 respectively following the corresponding optimal local policies.
PgN (s0 , t+k|s, t+i, g1 (s1 ), g2 (s2 )) =
185

fiGoldman & Zilberstein

if i = k and s = s0
if i = k and s =
6 s0
if i < k


1




0






P


N 0





s S P (s |s, g1 (s1 ), g2 (s2 ))  Pg (s , t+k|s , t+i+1, g1 (s1 ), g2 (s2 ))


s.t. (s, t+i) = (g1 , g2 , k)

Each iteration of the LGO-MSBPI algorithm (shown in Figure 3) tries to improve the
value of each state by testing all the possible pairs of local goal states with increasing number
of time steps allowed until communication. The value of f is computed for each mapping
from states to assignments of local goals and periods of time. The f function for a given
global state, current time, pair of local goals and a given period of time k expresses the cost
incurred by the agents after having acted for k time steps and having communicated at time
k+1, and the expected value of the reachable states after k time steps (these states are those
reached by the agents while following their corresponding optimal local policies towards g1
and g2 respectively). The current joint policy is updated when the f value for some state
s, time t, local goals g1 and g2 and period k is greater than the value V  (s, t, T ) computed
for the current best known assignment of local goals and period of time. Formally:
f (s, t, g1 , g2 , k) = G(s, t, g1 , g2 , k) + H(s, t, g1 , g2 , k)

(2)

G(s, t, g1 , g2 , k) = C(g1 , g2 , s, t, k) + C

(3)

H(s, t, g1 , g2 , k) =



 0

if t = T
if t < T

 P 0 P N (s0 , t + k|s, t,  (s ),  (s ))V  (s0 , t + k, T )
g1 1
g2 2
s S g

(4)

C(g1 , g2 , s, t, k) is the expected cost incurred by the agents when following the corresponding options for k time steps starting from state s. This is defined similarly to the
expected cost explained in Definition 5. We notice that the computation of f refers to the
goals being evaluated by the algorithm, while the evaluation of the policy (step 2) refers to
the goals assigned by the current best policy.
6.1 Convergence of the Algorithm and Its Complexity
Lemma 3 The algorithm LGO-MSBPI in Figure 3 converges to the optimal solution.
Proof. The set of global states S and the set of local goal states Gi  S are finite.
The horizon T is also finite. Therefore, step 3 in the algorithm will terminate. Like the
classical policy-iteration algorithm, the LGO-MSBPI algorithm also will converge after a
finite numbers of calls to step 3 where the policy can only improve its value from one
iteration to another.
2
Lemma 4 The complexity of computing the optimal mechanism based on local goal-oriented
behavior following the LGO-MSBPI algorithm is polynomial in the size of the state space.
Proof. Step 2 of the LGO-MSBPI algorithm can be computed with dynamic programming
in polynomial time (the value of a state is computed in a backwards manner from a finite
horizon T ). The complexity of improving a policy in Step 3 is polynomial in the time,
186

fiCommunication-Based Decomposition Mechanism

1. Initialization: Start with an initial joint policy  that assigns local goals
gi  Gi and time periods k  N
s  S, t : (s, t) = (g1 , g2 , k)
2. Policy Evaluation: s  S, Compute V  (s, t, T ) based on Equation 1.
3. Policy Improvement:
a. k = 1
b. While (k < T ) do
i. s, t, g1 , g2 : Compute f (s, t, g1 , g2 , k) based on Equations 2,3 and 4.
ii. Possible update policy
if f (s, t, g1 , g2 , k) > V  (s, t, T ) then
(s, t)  (g1 , g2 , k) / Communicate at k + 1 /
V  (s, t, T )  f (s, t, g1 , g2 , k)
iii. Test joint policies for next extended period of time
k k+1
4. Convergence test:
if  did not change in Step 3 then
return 
else GOTO 2.

Figure 3: Multi-step Backup
(LGO-MSBPI).

Policy-iteration

with

local

goal-oriented

behavior

number of states and number of goal states, i.e., O(T 2 |S||G|). In the worst case, every
component of a global state can be a local goal state. However, in other cases, |Gi | can be
much smaller than |Si | when Gi is a strict subset of Si , decreasing even more the running
time of the algorithm.
2
6.2 Experiments - Goal-oriented Options
We illustrate the LGO-MSBPI decomposition mechanism in a production control scenario.
We assume that there are two machines, which can control the production of boxes and
cereals: machine M1 can produce two types of boxes a or b. The amount of boxes of type
a produced by this machine is denoted by Ba (Bb represents the amount of boxes of type b
produced respectively). Machine M2 can produce two kinds of cereals a and b. Ca (and Cb
respectively) denotes the number of bags of cereals of type a (we assume that one bag of
cereals is sold in one box of the same type). The boxes differ in their presentation so that
boxes of type a advertise their content of type a and boxes of type b advertise their content
of type b. We assume that at each discrete time t, machine M1 may produce one box or
no boxes at all, and the other machine may produce one bag of cereals or may produce
no cereal at all. This production process is stochastic in the sense that the machines are
not perfect: with probability PM1 , machine one succeeds in producing the intended box
(either a or b) and with probability 1  PM1 , the machine does not produce any box in that
particular time unit. Similarly, we assume PM2 expresses the probability of machine two
producing one bag of cereals of type a or b that is required for selling in one box. In this
example, the reward attained by the system at T is equal to the number of products ready
187

fiGoldman & Zilberstein

for sale, i.e., min{Ba , Ca } + min{Bb , Cb }. A product that can be sold is composed of one
box together with one bag of cereals corresponding to the type advertised in this box.
A goal-oriented option is given by the number of products that each machine should
produce. Therefore, an option opti in this scenario is described by a pair of numbers (Xa , Xb )
(when i is machine one then X refers to boxes and when i is machine two, X refers to bags
of cereals). That is, machine i is instructed to produce Xa items of type a, followed by
Xb items of type b, followed by Xa items of type a and so forth until either the time limit
is over or anyone of the machines decides to communicate. Once the machines exchange
information, the global state is revealed, i.e., the current number of boxes and cereals
produced so far is known. Given a set of goal-oriented options, the LGO-MSBPI algorithm
returned the optimal joint policy of action and communication that solves this problem.
We counted the time units that it takes to produce the boxes with cereals. We compared
the locally goal oriented multi-step backup policy iteration algorithm (LGO-MSBPI) with
two other approaches: 1) the Ideal case when machines can exchange information about
their state of production at each time and at no cost. This is an idealized case, since
in reality exchanging information does incur some cost, for example changing the setting
of a machine takes valuable time and 2) the Always Communicate ad-hoc case, when the
machines exchange information at each time step and they also incur a cost when they
do it. Tables 1, 2, and 3 present the average utility obtained by the production system
when the cost of communication was set to 0.1, 1 and 10 respectively, the cost of a
domain action was set to 1 and the joint utility was averaged over 1000 experiments. A
state is represented by the tuple (Ba , Bb , Ca , Cb ). The initial state was set to (0,0,0,8),
there were no boxes produced and there were already 8 bags of cereals of type B. The
finite horizon T was set to 10. The set of goal-oriented options (Xa , Xb ) tested included
(0,1),(1,4),(2,3),(1,1),(3,2),(4,1) and (1,0).

PM1 , PM2
0.2, 0.2
0.2, 0.8
0.8, 0.8

Ideal C = 0
-17.012
-16.999
-11.003

Average Utility
Always Communicate
-18.017
-17.94
-12.01

LGO-MSBPI
-17.7949
-18.0026
-12.446

Table 1: C = 0.10, Ra = 1.0.

PM1 , PM2
0.2, 0.2
0.2, 0.8
0.8, 0.8

Ideal C = 0
-17.012
-16.999
-11.003

Average Utility
Always Communicate
-26.99
-26.985
-20.995

LGO-MSBPI
-19.584
-25.294
-17.908

Table 2: C = 1.0, Ra = 1.0.
The LGO-MSBPI algorithm computed a mechanism that resulted in three products on
average when the uncertainty of at least one machine was set to 0.2 and 1000 tests were
run, each for ten time units. The number of products increased on average between 8 to
9 products when the machines succeeded 80% of the cases. These numbers of products
188

fiCommunication-Based Decomposition Mechanism

PM1 , PM2
0.2, 0.2
0.2, 0.8
0.8, 0.8

Ideal C = 0
-17.012
-16.999
-11.003

Average Utility
Always Communicate
-117
-117.028
-110.961

LGO-MSBPI
-17.262
-87.27
-81.798

Table 3: C = 10.0, Ra = 1.0.
were always attained either when the decomposition mechanism was implemented or when
the ad-hoc approaches were tested. Ideal or Always Communicate algorithms only differ
with respect to the cost of communication, and they do not differ in the actual policies of
action. Although the machines incur a higher cost when the mechanism is applied compared
to the ideal case (due to the cost of communication), the number of final products ready
to sell were almost the same amount. That is, it will take some more time in order to
produce the right amount of products when the policies implemented are those computed
by the locally goal oriented multi-step backup policy iteration algorithm. The cost of
communication in this scenario can capture the cost of changing the setting of one machine
from one production program to another. Therefore, our result is significant when this cost
of communication is very high compared to the time that the whole process takes. The
decomposition mechanism finds what times are most beneficial to synchronize information
when constant communication is not feasible nor desirable due to its high cost.
6.3 Generalization of the LGO-MSBPI Algorithm
The mechanism approach assumes that agents can operate independent of each other for
some period of time. However, if the decentralized process has some kind of dependency
in its observations or transitions, this assumption will be violated, i.e., the plans to reach
the local goals can interfere with each other (the local goals may not be compatible). The
LGO-MSBPI algorithm presented in this paper can be applied to Dec-MDPs when their
transitions and observations are not assumed to be independent. In this section, we bound
the error in the utilities of the options computed by the LGO-MSBPI algorithms when such
dependencies do exist. We define independent decentralized processes to refer to nearlyindependent processes whose dependency can be quantified by the cost of their marginal
interactions.
Definition 10 (independent Process) Let C Ai (s  gk |gj ) be the expected cost incurred by agent i when following its optimal local policy to reach local goal state gk from
state s, while the other agent is following its optimal policy to reach gj . A decentralized
control process is independent if  = max{1 , 2 }, where 1 and 2 are defined as
follows: g1 , g10  G1  S1 , g2 , g20  G2  S2 and s  S:
1 = max{max{max0 {C A1 (s0  g1 |g20 )  C A1 (s0  g1 |g2 )}}}
s

g1

g2 ,g2

2 = max{max{max0 {C A2 (s0  g2 |g10 )  C A2 (s0  g2 |g1 )}}}
s

g2

g1 ,g1

189

fiGoldman & Zilberstein

That is,  is the maximal difference in cost that an agent may incur when trying to
reach one local goal state that interferes with any other possible local goal being reached by
the other agent.
The computation of the cost function C Ai (s  gk |gj ) is domain dependent. We do not
address the issue of how to compute this cost but we provide the condition. The individual
costs of one agent can be affected by the interference that exists between some pair of local
goals. For example, assume a 2D grid scenario: one agent can move only in four directions
(north, south, east and west) and needs to reach location (9,9) from (0,0). The second agent
is able of moving and also of collecting rocks and blocking squares in the grid. Assuming
that the second agent is assigned the task of blocking all the squares in even rows, then the
first agents solution to its task is constrained by the squares that are free to cross. In this
case, agent ones cost to reach (9,9) depends on the path it will choose that depends very
strongly on the state of the grid resulting from the second agents actions.
The  value denotes the amount of interference that might occur between the agents
locally goal-oriented behaviors. When the Dec-MDP has independent transitions and observations, the value of  is zero. The LGO-MSBPI algorithm proposed in this paper
computes the mechanism for each global state as a mapping from states to pairs of local
goal states ignoring the potential interference. Therefore, the difference between the actual
cost that will be incurred by the options found by the algorithm and the optimal options
can be at most . Since the mechanism is applied for each global state for T time steps
and this loss in cost can occur in the worst case for both agents, the algorithm presented
here is 2T optimal in the general case.

7. A Myopic-greedy Approach to Direct Communication
In some cases, it is reasonable to assume that single-agent behaviors are already known
and fixed, ahead of time for any possible global state. For example, this may occur in
settings where individual agents are designed ahead of the coordination time (e.g., agents
in a manufacturing line can represent machines, which are built specifically to implement
certain procedures). To achieve coordination, though, some additional method may be
needed to synchronize these individual behaviors. In this section, we present how to apply
the communication-based decomposition approach to compute the policy of communication
that will synchronize the given goal-oriented options. We take a myopic-greedy approach
that runs in polynomial-time: i.e., each time an agent makes a decision, it chooses the
action with maximal expected accumulated reward assuming that agents are only able
to communicate once along the whole process. Notice that the LGO-MSBPI was more
general in the sense that it also computed what local goals should be pursued by each
agent together with the communication policy that synchronizes their individual behaviors.
Here, each time the agents exchange information, the mechanism is applied inducing two
individual behaviors (chosen from the given mapping from states to individual behaviors).
The given optimal policies of action (with no communication actions) are denoted 1A and
2A respectively.
The expected global reward of the system, given that the agents do not communicate
at all and each follows its corresponding optimal policy iA is given by the value of the
initial state s0 : nc (s0 , 1A , 2A ). This value can be computed by summing over all possible
190

fiCommunication-Based Decomposition Mechanism

next states and computing the probability of each agent reaching it, the reward obtained
then and the recursive value computed for the next states.
nc (s0 , 1A , 2A ) =
X

P1 (s01 |s01 , 1A (s01 ))  P2 (s02 |s02 , 2A (s02 ))(R(s0 , 1A (s01 ), 2A (s02 ), s0 ) + nc (s0 , 1A , 2A ))

(s01 ,s02 )

We denote the expected cost of the system computed by agent i, when the last synchronized
state is s0 , and when the agents communicate once at state s and continue without
any communication, c (s0 , si , 1A , 2A ):
c (s0 , s1 , 1A , 2A ) =
X

P2 (s2 |s02 , 2A )(R(s0 , 1A (s01 ), 2A (s02 ), (s1 , s2 )) + nc ((s1 , s2 ), 1A , 2A ) + C  F lag)

s2

Flag is zero if the agents reached the global goal state before they reached state s. The
time stamp in state s is denoted t(s). P (s|, s0 , 1A , 2A ) is the probability of reaching state
s from state s0 , following the given policies of action.

1


 P (s0 |s,  A (s ),  A (s ))
1
2
1
2
P (s0 |s, 1A , 2A ) =
 0P

00

0 00 A A
A A
s00 S P (s |s , 1 , 2 )  P (s |s, 1 , 2 )

if s = s0
if t(s0 ) = t(s) + 1
if t(s0 ) < t(s) + 1
otherwise

Similarly, P1 (P2 ) can be defined for the probability of reaching s01 (s02 ), given agent 1 (2)s
current partial view s1 (s2 ) and its policy of action 1A (2A ). The accumulated reward
attained while the agents move from state s0 to state s is given as follows:

R(s0 , 1A (s01 ), 2A (s02 ), s)
if t(s) = t(s0 ) + 1



if t(s) > t(s0 ) + 1
P
00
00
R(s0 , 1A , 2A , s) =
A A 0
A A
2 , s )

s00 P (s |1 , 200 , s )  P00(s|1 , 00

00

0 A A
A
(R(s , 1 , 2 , s ) + R(s , 1 (s1 ), 2A (s2 ), s))

At each state, each agent decides whether to communicate its partial view or not based on
whether the expected cost from following the policies of action, and having communicated
is larger or smaller than the expected cost from following these policies of action and not
having communicated.
Lemma 5 Deciding a Dec-MDP-Com with the myopic-greedy approach to direct communication is in the P class.
Proof. Each agent executes its known policy iA when the mechanism is applied. If local
goals are provided instead of actual policies, finding the optimal single-agent policies that
reach those goal states can be done in polynomial time. The complexity of finding the
communication policy is the same as dynamic programming (based on the formulas above),
therefore computing the policy of communication is also in P. There are |S| states for which
nc and c need to be computed, and each one of these formulas can be solved in time
polynomial in |S|.
2
In previous work, we have also studied the set of monotonic goal-oriented Dec-MDPs,
for which we provide an algorithm that finds the optimal policy of communication assuming
a set of individual behaviors is provided (Goldman & Zilberstein, 2004b).
191

fiGoldman & Zilberstein

7.1 Meeting Under Uncertainty Example
We present empirical results obtained when the myopic-greedy approach was applied to the
Meeting under Uncertainty example4 . The testbed we consider is a sample problem of a DecMDP-Com involving two agents that have to meet at some location as early as possible. This
scenario is also known as the gathering problem in robotics (Suzuki & Yamashita, 1999).
The environment is represented by a 2D grid with discrete locations. In this example, any
global state that can be occupied by both agents is considered a global goal state. The set
of control actions includes moving North, South, East and West, and staying at the same
location. Each agents partial view (which is locally fully-observable) corresponds to the
agents location coordinates. The observations and the transitions are independent. The
outcomes of the agents actions are uncertain: that is, with probability Pi , agent i arrives at
the desired location after having taken a move action, but with probability 1  Pi the agent
remains at the same location. Due to this uncertainty in the effects of the agents actions,
it is not clear that setting a predetermined meeting point is the best strategy for designing
these agents. Agents may be able to meet faster if they change their meeting place after
realizing their actual locations. This can be achieved by exchanging information on the
locations of the agents, that otherwise are not observable. We showed that exchanging the
last observation guarantees optimality in a Dec-MDP-Com process with constant message
costs (Goldman & Zilberstein, 2004a). In the example tested, the messages exchanged
correspond to the agents own observations, i.e., their location coordinates.
We have implemented the locally goal-oriented mechanism that assigns a single local
goal to each agent at each synchronized state. These local goals were chosen as the location
in the middle of the shortest Manhattan path between the agents locations (this distance
is revealed when information is exchanged).
Intuitively, it is desirable for a mechanism to set a meeting place in the middle of the
shortest Manhattan path that connects the two agents because in the absence of communication, the cost to meet at that point is minimal. This can be shown by computing the
expected time to meet, nc , for any pair of possible distances between the two agents and
any location in the grid, when no communication is possible. To simplify the exposition,
we use a function that takes advantage of the specific characteristics of the example. The
notation is as follows: agent 1 is at distance d1 from the meeting location, agent 2 is at
distance d2 from that location, the system incurs a cost of one at each time period if the
agents have not met yet. If both agents are at the meeting location, the expected time to
meet is zero, nc (0, 0) = 0. If only agent 2 is at the meeting location, but agent 1 has not
reached that location yet, then the expected time to meet is given by
nc (d1 , 0) = P1  (1 + nc (d1 1, 0)) + (1P1 )  (1 + nc (d1 , 0)) =
= P1  nc (d1 1, 0)) + (1P1 )  nc (d1 , 0))  1

That is, with probability P1 agent 1 succeeds in decreasing its distance to the meeting location by one, and with probability 1  P1 it fails and remains at the same location. Recursively, we can compute the remaining expected time to meet with the updated parameters.
Similarly for agent 2: nc (0, d2 ) = P2  (1 + nc (0, d2 1)) + (1P2 )  (1+nc (0, d2 )). If
none of the agents has reached the meeting place yet, then there are four different cases in
which either both, only one, or none succeeded in moving in the right direction and either
4. Some of the empirical results in this section were described first by Goldman and Zilberstein (2003).

192

fiCommunication-Based Decomposition Mechanism

or not decreased their distances to the meeting location respectively:
nc (d1 , d2 ) = P1  P2  (1 + nc (d1 1, d2 1)) + P1  (1P2 )  (1 + nc (d1 1, d2 ))+
+(1P1 )  P2  (1 + nc (d1 , d2 1)) + (1P1 )  (1P2 )  (1 + nc (d1 , d2 )) =
= P1  P2  nc (d1 1, d2 1) + P1  (1P2 )  nc (d1 1, d2 ) + (1P1 )  P2  nc (d1 , d2 1)+
+(1P1 )  (1P2 )  nc (d1 , d2 )  1

The value of nc (d1 , d2 ) was computed for all possible distances d1 and d2 in a 2D grid of
size 10  10. The minimal expected time to meet was obtained when d1 = d2 = 9 and the
expected cost was 12.16.
In summary, approximating the optimal solution to the Meeting under Uncertainty
example when direct communication is possible and the mechanism applied is the one described above will unfold as follows: At time t0 , the initial state of the system s0 is fully
observable by both agents. The agents set a meeting point in the middle of a Manhattan path that connects them. Denote by d0 the distance between the agents at t0 and
gt0 = (gt10 , gt20 ) the goal state set at t0 . Each one of the agents can move optimally towards
its corresponding component of gt0 . Each agent moves independently in the environment
because the transitions and observations are independent. Each time t, when the policy of
communication instructs an agent to initiate exchange of information, the current Manhattan distance between the agents dt is revealed to both. Then, the mechanism is applied,
setting a possibly new goal state gt , which decomposes into two components one for each
agent. This goal state gt is in the middle of the Manhattan path that connects the agents
with length dt revealed through communication.
7.2 Experiments - Myopic-greedy Approach
In the following experiments, we assumed that the transition probabilities P1 and P2 are
equal. These uncertainties were specified by the parameter Pu . The mechanism that is
applied whenever the agents communicate at time t results in each agent adopting a local
goal state, that is set at the location in the middle of the Manhattan path connecting the
agents (the Manhattan distance between the agents is revealed at time t). We compare the
joint utility attained by the system in the following four different scenarios:
1. No-Communication  The meeting point is fixed at time t0 and remains fixed along
the simulation. It is located in the middle of the Manhattan path that connects
between the agents, known at time t0 . Each agent follows its optimal policy of action
to this location without communicating.
2. Ideal  This case assumes that the agents can communicate freely (C = 0) at every
time step resulting in the highest global utility that both agents can attain. Notice,
though, that this is not the optimal solution we are looking for, because we do assume
that communication is not free. Nevertheless, the difference in the utility obtained in
these first two cases shed light on the trade-off that can be achieved by implementing
non-free communication policies.
3. Communicate SubGoals  A heuristic solution to the problem, which assumes that
the agents have a notion of sub-goals. They notify each other when these sub-goals
are achieved, eventually leading the agents to meet.
193

fiGoldman & Zilberstein

A1
A1

A2

A2
Time t

A new subgoal is set after agent 2 arrived
at the subgoal set at time t.

Figure 4: Goal decomposition into sub-goal areas.
4. Myopic-greedy Approach  Agents act myopically optimizing the choice of when to
send a message, assuming no additional communication is possible. For each possible
distance between the agents, a policy of communication is computed such that it
stipulates the best time to send that message. By iterating on this policy agents are
able to communicate more than once and thus approximate the optimal solution to
the decentralized control problem with direct communication. The agents continue
moving until they meet.
The solution to the No-Communication case can be solved analytically for the Meeting
under Uncertainty example, by computing the expected cost nc (d1 , d2 ) incurred by two
agents located at distances d1 and d2 respectively from the goal state at time t0 (the complete
mathematical solution appears in Section 7.1). In the Ideal case, a set of 1000 experiments
was run with cost of communication set to zero. Agents communicate their locations at
every time instance, and update the location of the meeting place accordingly. Agents move
optimally to the last synchronized meeting location.
For the third case tested (Communicate SubGoals) a sub-goal was defined by the cells
of the grid with distance equal to p  d/2 and with center located at d/2 from each one of
the agents. p is a parameter of the problem that determines the radius of the circle that
will be considered a sub-goal. Each time an agent reaches a cell inside the area defined
as a sub-goal, it initiates exchange of information (therefore, p induces the communication
strategy). d expresses the Manhattan distance between the two agents, this value is accurate
only when the agents synchronize their knowledge. That is, at time t0 the agents determine
the first sub-goal as the area bounded by a radius of p  d0 /2 and, which center is located at
d0 /2 from each one of the agents. Each time t that the agents synchronize their information
through communication, a new sub-goal is determined at p  dt /2. Figure 4 shows how
new sub-goals are set when the agents transmit their actual location once they reached a
sub-goal area. The meeting point is dynamically set at the center of the sub-goal area.
Experiments were run for the Communicate SubGoals case for different uncertainty values, values of the parameter p and costs of communication (for each case, 1000 experiments
were run and averaged). These results show that agents can obtain higher utility by adjusting the meeting point dynamically rather than having set one fixed meeting point. Agents
can synchronize their knowledge and thus they can set a new meeting location instead of
acting as two independent MDPs that do not communicate and move towards a fixed meeting point (see Figure 5). Nevertheless, for certain values of p, the joint utility of the agents
194

fiCommunication-Based Decomposition Mechanism

p ratio
-20.5
0

0.1

0.2

0.3
-20.768
-20.852

-21

-21.038
-21.13
-21.15

-21.132

0.4

0.5

-20.808
-20.8586 -20.824
-21.0888

0.6
-20.726

0.7
-20.758

-21.3665
-21.5

0.9
-20.804

-20.9302
-21.0255

-21.0433

-21.26

-21.2636

-21.284

0.8
-20.742

-21.3405

-21.4254
-21.5715

Avg. Joint Utlity

-21.7747
-21.9355

-22

-21.9794

-22.308
-22.414
-22.5

-22.581
-22.771

-23

Pu=0.8 Cost Com.=0
Pu=0.8 Cost Com.=0.1
Pu=08 Cost Com.=0.3
Pu=0.8 Cost Com.=0.5

-23.5

2MDPs Pu=0.8

-23.739
-24

Figure 5: The average joint utility obtained when sub-goals are communicated.
is actually smaller than the joint utility achieved in the No-Communication case (2 MDPs
in the figure). This points out the need to empirically tune up the parameters required by
the heuristic.
In the Myopic-greedy case, we design the agents to optimize the time when they should
send a message, assuming that they can communicate only once. At the off-line planning
stage, the agents compute their expected joint cost to meet for any possible state of the
system (s0 ) and time t (included in the local state si ), c (s0 , si , 1A , 2A ). The global states
revealed through communication correspond to the possible distances between the agents.
Each time the agents get synchronized, the mechanism is applied assigning local goals and
instructing the agents to follow the optimal local policies to achieve them. In the Meeting
under Uncertainty scenario we study, c is the expected joint cost incurred by taking control
actions during t time steps, communicating then at time t + 1 if the agents have not met
so far, and continuing with the optimal policy of control actions without communicating
towards the goal state (the meeting location agreed upon at t + 1) at an expected cost of
nc (d1 , d2 ) as computed for the No-Communication case. When the agents meet before the
t time steps have elapsed, they only incur a cost for the time they act before they met.
At each time t, each one of the agents knows a meeting location, that is the goal location
computed from the last exchange of information. Consequently, each agent moves optimally
towards this goal state. In addition, the myopic-greedy policy of communication is found
by computing the earliest time t, for which c (d1 + d2 , s1 , 1A , 2A ) < nc (d1 , d2 ), that is,
what is the best time to communicate such that the expected cost to meet is the least. The
myopic-greedy policy of communication is a vector that states the time to communicate for
each possible distance between the agents.
Tables 4, 5, and 6 present the myopic-greedy communication policies computed for the
Meeting under Uncertainty problem with Pu values taken from {0.2, 0.4, 0.6, 0.8}. The cost
of taking a control action is Ra = 1.0 and the costs of communicating C tested were
195

fiGoldman & Zilberstein

{0.1, 1.0, 10.0}. Each row corresponds to a configuration tested with different statetransition uncertainties. Each column corresponds to a synchronized state, given by the
possible Manhattan distance between the agents moving in a 2D grid of size 10x10. Given
a certain value for Pu and a certain global distance, each agent interprets the value in
any entry as the next time to communicate its position. Time is reset to zero when the
agents exchange information. As long as the distance between the agents is larger and the
communication cost increases, the policy instructs the agents to communicate later, i.e.,
the agents should keep operating until the information exchanged will have a better effect
on the rescheduling of the meeting place.
Pu
0.2
0.4
0.6
0.8

1
2
2
2
2

d0 =distance between agents when last synchronized, g
2 3 4 5 6 7 8 9 10 11 12 13 14
3 2 3 2 3 2 3 2 3
2
3
2
3
2 2 3 2 3 2 3 2 3
2
3
2
3
2 2 3 2 3 2 3 2 3
2
3
2
3
2 2 3 2 4 2 4 2 4
2
4
2
4

located at
15 16
2
3
2
3
2
3
2
4

d0 /2
17
2
2
2
2

18
3
3
3
4

Table 4: Myopic-greedy policy of communication: C = 0.1, Ra = 1.0.

Pu
0.2
0.4
0.6
0.8

1
3
2
2
2

d0 =distance between agents when last synchronized, g
2 3 4 5 6 7 8 9 10 11 12 13 14
4 3 5 3 6 4 7 4 7
5
7
5
8
3 3 4 4 5 4 6 5 7
5
7
6
8
2 3 4 4 5 5 6 6 7
6
8
7
8
2 3 3 4 4 5 5 6 6
7
7
8
8

located at
15 16
5
8
6
8
7
9
9
9

d0 /2
17
6
7
8
10

18
9
9
10
10

Table 5: Myopic-greedy policy of communication: C = 1.0, Ra = 1.0.

Pu
0.2
0.4
0.6
0.8

1
9
5
4
3

2
9
6
4
3

d0 =distance between agents when last
3
4
5
6
7
8
9 10
11 13 14 17 18 20 21 23
7
8
9
10 11 12 13 14
5
6
6
7
8
9
9
10
4
4
5
5
6
7
7
8

synchronized, g
11 12 13
25 27 28
15 16 17
11 12 12
8
9
10

located at d0 /2
14 15 16 17
30 32 34 35
18 19 20 21
13 14 15 15
10 11 11 12

18
37
22
16
12

Table 6: Myopic-greedy policy of communication: C = 10.0, Ra = 1.0.
For the smallest cost of communication tested, it is always beneficial to communicate
rather early, no matter the uncertainty in the environment, and almost no matter what
d0 is. For larger costs of communication and a given Pu , the larger the distance between
the agents, the later they will communicate (e.g., when Pu = 0.4, C = 1 and d = 5,
agents should communicate at time 4, but if C = 10, they should communicate at time
9). For a given C , the larger the distance between the agents is, the later the agents will
communicate (e.g., when Pu = 0.4, C = 10 and d = 5, agents should communicate at
time 9, but if d = 12, they should communicate at time 16). The results from averaging
over 1000 runs show that for a given cost C as long as Pu decreases (the agent is more
uncertain about its actions outcomes), the agents communicate more times.
196

fiCommunication-Based Decomposition Mechanism

In the 1000 experiments run, the agents exchange information about their actual locations at the best time that was myopically found for d0 (known to both at time t0 ). After
they communicate, they know the actual distance dt , between them. The agents follow the
same myopic-greedy communication policy to find the next time when they should communicate if they did not meet already. This time is the best time found by the myopic-greedy
algorithm given that the distance between the agents was dt . Iteratively, the agents approximate the optimal solution to the decentralized control problem with direct communication
by following their independent optimal policies of action, and the myopic-greedy policy
for communication. Results obtained from averaging the global utility attained after 1000
experiments show that these myopic-greedy agents can perform better than agents who communicate sub-goals (that was shown already to be more efficient than not communicating
at all).

Pu
0.2
0.4
0.6
0.8

No-Comm.
-104.925
-51.4522
-33.4955
-24.3202

Average Joint Utility
Ideal C = 0 SubGoals5
-62.872
-64.7399
-37.33
-38.172
-26.444
-27.232
-20.584
-20.852

Myopic-Greedy
-63.76
-37.338
-26.666
-20.704

Table 7: C = 0.10, Ra = 1.0.
The Myopic-greedy approach attained utilities statistically significantly greater than
those obtained by the heuristic case when C = 0.1 (see Table 7)6 . When C = 0.1 and
Pu = 0.4, Myopic-greedy even attained utilities not significantly different (with significance
level 98%) than Ideal.

Pu
0.2
0.4
0.6
0.8

No-Comm.
-104.925
-51.4522
-33.4955
-24.3202

Average Joint Utility
Ideal C = 0 Comm. SubGoals 
-62.872
-65.906
-37.33
-39.558
-26.444
-27.996
-20.584
-21.05

Best p
0.3
0.2
0.2
0.1

Myopic-greedy
-63.84
-37.774
-27.156
-21.3

Table 8: C = 1.0 in SubGoals and Myopic-greedy, Ra = 1.0.

Pu
0.2
0.4
0.6
0.8

No-Comm.
-104.925
-51.4522
-33.4955
-24.3202

Average Joint Utility
Ideal C = 0 Comm. SubGoals 
-62.872
-69.286
-37.33
-40.516
-26.444
-28.192
-20.584
-21.118

Best p
0.1
0.1
0.1
0.1

Myopic-greedy
-68.948
-40.594
-28.908
-22.166

Table 9: C = 10.0 in SubGoals and Myopic-greedy, Ra = 1.0.
5. The results are presented for the best p, found empirically.
6. Statistical significance has been established with t-test.

197

fiGoldman & Zilberstein

When C = 1 (see Table 8) the utilities attained for the Myopic-greedy approach
when Pu < 0.8 are significantly greater than the results obtained in the heuristic case.
When Pu = 0.8, the heuristic case was found to be better than Myopic-greedy for the best
choice of p (Myopic-greedy obtained -21.3, and the SubGoals with p = 0.1 attained -21.05
(variance=2.18)). The utilities attained by the Myopic-greedy agents, when C = 10 (see
Table 9) and Pu in {0.2, 0.4}, were not significantly different from the SubGoals case for
the best p with significance levels 61% and 82%, respectively. However, the heuristic case
yielded smaller costs for the other values of Pu = {0.6, 0.8}. One important point to notice
is that these results consider the best p found for the heuristic after trying a set of discrete
values for p (see the x-axis in Figure 5). In general trying and tuning a heuristic parameter
can be very time consuming and the best choice may not be known ahead of time to the
designer. On the other hand, the Myopic-greedy approach does not require any tuning of
any parameter. In all the settings tested, Myopic-greedy always attain utilities higher than
those attained in the SubGoals case with the worst p.

Pu
0.2
0.4
0.6
0.8

Average Communication Acts Performed
No-Comm. Ideal C = 0 SubGoals Myopic-greedy
0
31.436
5.4
21.096
0
18.665
1
11.962
0
13.426
1
8.323
0
10.292
1
4.579

Table 10: C = 0.10, Ra = 1.0.

Pu
0.2
0.4
0.6
0.8

No-Comm.
0
0
0
0

Average Communication Acts Performed
Ideal C = 0 Comm. SubGoals Myopic-greedy
31.436
1.194
6.717
18.665
1
3.904
13.426
1
2.036
10.292
0
1.296

Table 11: C = 1.0 in Myopic-greedy and SubGoals, Ra = 1.0.

Pu
0.2
0.4
0.6
0.8

No-Comm.
0
0
0
0

Average Communication Acts Performed
Ideal C = 0 Comm. SubGoals Myopic-greedy
31.436
0
0.416
18.665
0
0.417
13.426
0
0.338
10.292
0
0.329

Table 12: C = 10.0 in Myopic-greedy and SubGoals, Ra = 1.0.
Tables 10, 11 and 12 present the average number of communication acts performed in
each one of these cases.
198

fiCommunication-Based Decomposition Mechanism

[Bernstein et al.
2002]

NEXP-C

NP-C
[GZ 2004]

Dec-MDP

IO and IT

NP-C

Goaloriented

Poly-C

Poly-C
|G|=1

[GZ 2004]

[GZ 2004]

Goaloriented
With
Information
Sharing

|G| > 1
No Information
Sharing

[GZ 2004]

Optimal
Mechanism
NEXP

NEXP-C
[GZ 2004]

[Rabinovich et al.
2003]

Approx.
NEXP-C

P

LGO
Mechanism
Myopic
Greedy

P

P

Backward
Induction

Figure 6: The complexity of solving Dec-MDPs.

8. Discussion
Solving optimally decentralized control problems is known to be very hard. Figure 6 summarizes the complexity results (the rectangles stand for optimal solutions while the parallelograms stand for solutions proposed in the framework of communication-based decomposition mechanisms). This taxonomy helps us understand the characteristics of different
classes of decentralized control problems and their effect on the complexity of these problems.
The Coverage-set (Becker et al., 2004), Opt1Goal and OptNGoals (Goldman & Zilberstein,
2004a) were the first algorithms to solve optimally some non-trivial classes of Dec-MDPs.
This paper presents communication-based decomposition mechanisms as a way to approximate the optimal joint solution of decentralized control problems. This approach is
based on two key ideas: (1) separating the questions of when to communicate and what to do
between communications, and (2) exploiting the full observability of the global states after
each communication to generate individual behaviors that the agents will follow between
communications. Communication between the decision makers serves as a synchronization
point where local information is exchanged in order to assign an individual behavior to each
controller. This addresses effectively applications in which constant communication is not
desirable or not feasible. Many practical reasons could prevent agents from communication
constantly. Communication actions may incur some costs that reflect the complexity of
transmitting the information, the utilization of limited bandwidth that may be shared with
other applications, or the risk of revealing information to competitive parties operating in
the same environment. Our communication-based decomposition mechanism divides the
global problem into individual behaviors combined with communication acts to overcome
the lack of global information.
199

fiGoldman & Zilberstein

We formalized communication-based decomposition mechanisms as a decentralized semiMarkov process with communication (Dec-SMDP-Com). We proved that solving optimally such problems with temporally abstracted actions is equivalent to solving optimally
a multi-agent MDP (MMDP).We adapted the multi-step backup policy iteration algorithm
to the decentralized case that can solve the Dec-SMDP-Com problem optimally. This algorithm produces the optimal communication-based decomposition mechanism. To provide
a tractable algorithm, we can restrict the set of individual behaviors that are allowed. We
proposed the LGO-MSBPI polynomial-time algorithm that computes the assignment of a
pair of local goals and period of time k with the highest value to each possible global state.
Adding local goals to the model seems more natural and intuitive than computing local
behaviors based on general options. It is easier to state when a local behavior is completed
when some local goal is reached, rather than stating sequences of local actions that eventually should achieve some desired global behavior. Furthermore, an unrestricted set of
options is larger and therefore it is computationally cheaper to compute the decomposition
mechanism when local goals are assumed. This intuition is confirmed by our experiments.
But the general question remain open, namely to determine when it is beneficial to compute
local behaviors out of general options rather than assuming local goals.
The paper also presents a simpler approximation method. It assumes that a certain
mechanism is given, i.e., human knowledge is incorporated into the model to provide agents
with individual policies of actions (not including communication acts). A greedy-approach
is presented that computes the best time to communicate assuming there is only one opportunity for exchanging information. The paper concludes with an empirical assessment
of these approaches.
In summary, this paper contributes a communication-based decomposition mechanism
that can be applied to many of the hard decentralized control problems shown in Figure 6.
This approach enables us to compute tractable individual behaviors for each agent together
with the most beneficial time to communicate and change these local behaviors. The analytical results in the paper support the validity of the approach with respect to Dec-MDPs
with independent transitions and observations. However, it is straightforward to apply the
approach to general Dec-POMDPs or Dec-MDPs with dependent transitions or observations, and we believe it offers a viable approximation technique for these problems as well.
Our approach is scalable with respect to the number of agents since all the complexity
results presented will increase linearly as more agents are added to the system. Exchange of
information is assumed to be via a broadcast to all the other agents. An interesting future
extension is to study how agents can efficiently choose partners for communication to avoid
global broadcasting.

Acknowledgments
This work was supported in part by the National Science Foundation under grants IIS0219606, by the Air Force Office of Scientific Research under grants F49620-03-1-0090 and
FA9550-08-1-0181, and by NASA under grant NCC 2-1311. Any opinions, findings, and
conclusions or recommendations expressed in this material are those of the authors and do
not reflect the views of the NSF, AFOSR or NASA.
200

fiCommunication-Based Decomposition Mechanism

References
Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. V. (2004). Solving transition independent decentralized MDPs. Journal of Artificial Intelligence Research, 22, 423455.
Bererton, C., Gordon, G., Thrun, S., & Khosla, P. (2003). Auction mechanism design for
multi-robot coordination. In Proceedings of the Seventeenth Annual Conference on
Neural Information Processing Systems, Whistler, BC, Canada.
Bernstein, D., Givan, R., Immerman, N., & Zilberstein, S. (2002). The complexity of decentralized control of Markov decision processes. Mathematics of Operations Research,
27 (4), 819840.
Boutilier, C. (1999). Sequential optimality and coordination in multiagent systems. In
Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence,
pp. 478485, Stockholm, Sweden.
Ghavamzadeh, M., & Mahadevan, S. (2004). Learning to act and communicate in cooperative multiagent systems using hierarchical reinforcement learning. In Proceedings
of the Third International Joint Conference on Autonomous Agents and Multi-Agent
Systems, pp. 11141121, New York City, NY.
Goldman, C. V., & Zilberstein, S. (2003). Optimizing information exchange in cooperative
multi-agent systems. In Proceedings of the Second International Joint Conference on
Autonomous Agents and Multi-Agent Systems, pp. 137144, Melbourne, Australia.
Goldman, C. V., & Zilberstein, S. (2004a). Decentralized control of cooperative systems:
Categorization and complexity analysis. Journal of Artificial Intelligence Research,
22, 143174.
Goldman, C. V., & Zilberstein, S. (2004b). Goal-oriented Dec-MDPs with direct communication. Technical Report 0444, University of Massachusetts at Amherst, Computer
Science Department.
Guestrin, C., & Gordon, G. (2002). Distributed planning in hierarchical factored MDPs.
In Proceedings of the Eighteenth Conference on Uncertainty in Artificial Intelligence,
pp. 197206, Edmonton, Canada.
Guestrin, C., Koller, D., & Parr, R. (2001). Multiagent planning with factored MDPs.
In Advances in Neural Information Processing Systems, pp. 15231530, Vancouver,
British Columbia.
Hansen, E. A. (1997). Markov decision processes with observation costs. Technical Report
97-01, University of Massachusetts at Amherst, Computer Science Department.
Hansen, E. A., & Zhou, R. (2003). Synthesis of hierarchical finite-state controllers for
POMDPs. In Proceedings of the Thirteenth International Conference on Automated
Planning and Scheduling, Trento, Italy.
Nair, R., Tambe, M., Roth, M., & Yokoo, M. (2004). Communication for improving policy
computation in distributed POMDPs. In Proceedings of the Third International Joint
Conference on Autonomous Agents and Multi-Agent Systems, pp. 10981105, New
York City, NY.
201

fiGoldman & Zilberstein

Nair, R., Tambe, M., Yokoo, M., Pynadath, D., & Marsella, S. (2003). Taming decentralized
POMDPs: Towards efficient policy computation for multiagent settings. In Proceedings
of the Eighteenth International Joint Conference on Artificial Intelligence, pp. 705
711, Acapulco, Mexico.
Papadimitriou, C. H., & Tsitsiklis, J. (1987). The complexity of Markov decision processes.
Mathematics of Operations Research, 12 (3), 441450.
Peshkin, L., Kim, K.-E., Meuleau, N., & Kaelbling, L. P. (2000). Learning to cooperate via
policy search. In Proceedings of the Sixteenth Conference on Uncertainty in Artificial
Intelligence, pp. 489496, Stanford, CA.
Petrik, M., & Zilberstein, S. (2007). Anytime coordination using separable bilinear programs. In Proceedings of the Twenty Second Conference on Artificial Intelligence, pp.
750755, Vancouver, BC, Canada.
Puterman, M. L. (1994). Markov Decision Processes  Discrete Stochastic Dynamic Programming. Wiley & Sons, Inc., New York.
Pynadath, D. V., & Tambe, M. (2002). The communicative multiagent team decision
problem: Analyzing teamwork theories and models. Journal of Artificial Intelligence
Research, 16, 389423.
Schneider, J., Wong, W.-K., Moore, A., & Riedmiller, M. (1999). Distributed value functions. In Proceedings of the Sixteenth International Conference on Machine Learning,
pp. 371378, Bled, Slovenia.
Seuken, S., & Zilberstein, S. (2007a). Improved memory-bounded dynamic programming
for Dec-POMDPs. In Proceedings of the Twenty Third Conference on Uncertainty in
Artificial Intelligence, Vancouver, BC, Canada.
Seuken, S., & Zilberstein, S. (2007b). Memory-bounded dynamic programming for DecPOMDPs. In Proceedings of the Twentieth International Joint Conference on Artificial Intelligence, pp. 20092015, Hyderabad, India.
Seuken, S., & Zilberstein, S. (2008). Formal models and algorithms for decentralized decision
making under uncertainty. Autonomous Agents and Multi-agent Systems. To appear.
Sutton, R. S., Precup, D., & Singh, S. (1999). Between MDPs and semi-MDPs: A framework
for temporal abstraction in reinforcement learning. Artificial Intelligence, 112, 181
211.
Suzuki, I., & Yamashita, M. (1999). Distributed anonymous mobile robots: Formation of
geometric patterns. SIAM Journal on Computing, 28 (4), 13471363.
Wolpert, D. H., Wheeler, K. R., & Tumer, K. (1999). General principles of learning-based
multi-agent systems. In Proceedings of the Third International Conference on Autonomous Agents, pp. 7783, Seattle, Washington.
Xuan, P., Lesser, V., & Zilberstein, S. (2001). Communication decisions in multi-agent cooperation: Model and experiments. In Proceedings of the Fifth International Conference
on Autonomous Agents, pp. 616623, Montreal, Canada.

202

fiJournal of Artificial Intelligence Research 32 (2008) 453486

Submitted 02/08; published 06/08

Adaptive Stochastic Resource Control:
A Machine Learning Approach
Balazs Csanad Csaji

balazs.csaji@sztaki.hu

Computer and Automation Research Institute,
Hungarian Academy of Sciences
Kende utca 1317, Budapest, H1111, Hungary

Laszlo Monostori

laszlo.monostori@sztaki.hu

Computer and Automation Research Institute,
Hungarian Academy of Sciences; and
Faculty of Mechanical Engineering,
Budapest University of Technology and Economics

Abstract
The paper investigates stochastic resource allocation problems with scarce, reusable
resources and non-preemtive, time-dependent, interconnected tasks. This approach is a
natural generalization of several standard resource management problems, such as scheduling and transportation problems. First, reactive solutions are considered and defined as
control policies of suitably reformulated Markov decision processes (MDPs). We argue that
this reformulation has several favorable properties, such as it has finite state and action
spaces, it is aperiodic, hence all policies are proper and the space of control policies can be
safely restricted. Next, approximate dynamic programming (ADP) methods, such as fitted
Q-learning, are suggested for computing an efficient control policy. In order to compactly
maintain the cost-to-go function, two representations are studied: hash tables and support
vector regression (SVR), particularly, -SVRs. Several additional improvements, such as
the application of limited-lookahead rollout algorithms in the initial phases, action space
decomposition, task clustering and distributed sampling are investigated, too. Finally,
experimental results on both benchmark and industry-related data are presented.

1. Introduction
Resource allocation problems (RAPs) are of high practical importance, since they arise in
many diverse fields, such as manufacturing production control (e.g., production scheduling),
warehousing (e.g., storage allocation), fleet management (e.g., freight transportation), personnel management (e.g., in an office), scheduling of computer programs (e.g., in massively
parallel GRID systems), managing a construction project or controlling a cellular mobile
network. RAPs are also central to management science (Powell & Van Roy, 2004). In
the paper we consider optimization problems that include the assignment of a finite set of
reusable resources to non-preemtive, interconnected tasks that have stochastic durations
and effects. Our objective is to investigate efficient reactive (closed-loop) decision-making
processes that can deal with the allocation of scarce resources over time with a goal of
optimizing the objectives. For real world applications, it is important that the solution
should be able to deal with large-scale problems and handle environmental changes, as well.
c
2008
AI Access Foundation. All rights reserved.

fiCsaji & Monostori

1.1 Industrial Motivations
One of our main motivations for investigating RAPs is to enhance manufacturing production
control. Regarding contemporary manufacturing systems, difficulties arise from unexpected
tasks and events, non-linearities, and a multitude of interactions while attempting to control
various activities in dynamic shop floors. Complexity and uncertainty seriously limit the
effectiveness of conventional production control approaches (e.g., deterministic scheduling).
In the paper we apply mathematical programming and machine learning (ML) techniques to achieve the suboptimal control of a general class of stochastic RAPs, which can
be vital to an intelligent manufacturing system (IMS). The term of IMS can be attributed
to a tentative forecast of Hatvany and Nemes (1978). In the early 80s IMSs were outlined as
the next generation of manufacturing systems that utilize the results of artificial intelligence
research and were expected to solve, within certain limits, unprecedented, unforeseen problems on the basis of even incomplete and imprecise information. Naturally, the applicability
of the different solutions to RAPs are not limited to industrial problems.
1.2 Curse(s) of Dimensionality
Different kinds of RAPs have a huge number of exact and approximate solution methods,
for example, in the case of scheduling problems (Pinedo, 2002). However, these methods
primarily deal with the static (and often strictly deterministic) variants of the various problems and, mostly, they are not aware of uncertainties and changes. Special (deterministic)
RAPs which appear in the field of combinatorial optimization, such as the traveling salesman problem (TSP) or the job-shop scheduling problem (JSP), are strongly NP-hard and,
moreover, they do not have any good polynomial-time approximation, either.
In the stochastic case RAPs can be often formulated as Markov decision processes
(MDPs) and by applying dynamic programming (DP) methods, in theory, they can be
solved optimally. However, due to the phenomenon that was named curse of dimensionality
by Bellman (1961), these methods are highly intractable in practice. The curse refers
to the combinatorial explosion of the required computation as the size of the problem increases. Some authors, e.g., Powell and Van Roy (2004), talk about even three types of
curses concerning DP algorithms. This has motivated approximate approaches that require
a more tractable computation, but often yield suboptimal solutions (Bertsekas, 2005).
1.3 Related Literature
It is beyond our scope to give a general overview on different solutions to RAPs, hence, we
only concentrate on the part of the literature that is closely related to our approach. Our
solution belongs to the class of approximate dynamic programming (ADP) algorithms which
constitute a broad class of discrete-time control techniques. Note that ADP methods that
take an actor-critic point of view are often called reinforcement learning (RL).
Zhang and Dietterich (1995) were the first to apply an RL technique for a special RAP.
They used the T D() method with iterative repair to solve a static scheduling problem,
namely, the NASA space shuttle payload processing problem. Since then, a number of
papers have been published that suggested using RL for different RAPs. The first reactive
(closed-loop) solution to scheduling problems using ADP algorithms was briefly described
454

fiAdaptive Stochastic Resource Control

by Schneider, Boyan, and Moore (1998). Riedmiller and Riedmiller (1999) used a multilayer perceptron (MLP) based neural RL approach to learn local heuristics. Aydin and
Oztemel (2000) applied a modified version of Q-learning to learn dispatching rules for production scheduling. Multi-agent versions of ADP techniques for solving dynamic scheduling
problems were also suggested (Csaji, Kadar, & Monostori, 2003; Csaji & Monostori, 2006).
Powell and Van Roy (2004) presented a formal framework for RAPs and they applied
ADP to give a general solution to their problem. Later, a parallelized solution to the
previously defined problem was given by Topaloglu and Powell (2005). Our RAP framework, presented in Section 2, differs from these approaches, since in our system the goal
is to accomplish a set of tasks that can have widely different stochastic durations and
precedence constraints between them, while Powell and Van Roys (2004) approach concerns with satisfying many similar demands arriving stochastically over time with demands
having unit durations but not precedence constraints. Recently, support vector machines
(SVMs) were applied by Gersmann and Hammer (2005) to improve iterative repair (local
search) strategies for resource constrained project scheduling problems (RCPSPs). An agentbased resource allocation system with MDP-induced preferences was presented by Dolgov
and Durfee (2006). Finally, Beck and Wilson (2007) gave proactive solutions for job-shop
scheduling problems based on the combination of Monte Carlo simulation, solutions of the
associated deterministic problem, and either constraint programming or tabu-search.
1.4 Main Contributions
As a summary of the main contributions of the paper, it can be highlighted that:
1. We propose a formal framework for investigating stochastic resource allocation problems with scarce, reusable resources and non-preemtive, time-dependent, interconnected tasks. This approach constitutes a natural generalization of several standard
resource management problems, such as scheduling problems, transportation problems, inventory management problems or maintenance and repair problems.
This general RAP is reformulated as a stochastic shortest path problem (a special
MDP) having favorable properties, such as, it is aperiodic, its state and action spaces
are finite, all policies are proper and the space of control policies can be safely restricted. Reactive solutions are defined as policies of the reformulated problem.
2. In order to compute a good approximation of the optimal control policy, ADP methods are suggested, particularly, fitted Q-learning. Regarding value function representations for ADP, two approaches are studied: hash tables and SVRs. In the latter,
the samples for the regression are generated by Monte Carlo simulation and in both
cases the inputs are suitably defined numerical feature vectors.
Several improvements to speed up the calculation of the ADP-based solution are suggested: application of limited lookahead rollout algorithms in the initial phases to
guide the exploration and to provide the first samples to the approximator; decomposing the action space to decrease the number of available actions in the states;
clustering the tasks to reduce the length of the trajectories and so the variance of the
cumulative costs; as well as two methods to distribute the proposed algorithm among
several processors having either a shared or a distributed memory architecture.
455

fiCsaji & Monostori

3. The paper also presents several results of numerical experiments on both benchmark
and industry-related problems. First, the performance of the algorithm is measured
on hard benchmark flexible job-shop scheduling datasets. The scaling properties of
the approach are demonstrated by experiments on a simulated factory producing
mass-products. The effects of clustering depending on the size of the clusters and
the speedup relative to the number of processors in case of distributed sampling are
studied, as well. Finally, results on the adaptive features of the algorithm in case of
disturbances, such as resource breakdowns or new task arrivals, are also shown.

2. Markovian Resource Control
This section aims at precisely defining RAPs and reformulating them in a way that would
allow them to be effectively solved by ML methods presented in Section 3. First, a brief
introduction to RAPs is given followed by the formulation of a general resource allocation
framework. We start with deterministic variants and then extend the definition to the
stochastic case. Afterwards, we give a short overview on Markov decision processes (MDPs),
as they constitute a fundamental theory to our approach. Next, we reformulate the reactive
control problem of RAPs as a stochastic shortest path (SSP) problem (a special MDP).
2.1 Classical Problems
In this section we give a brief introduction to RAPs through two strongly NP-hard combinatorial optimization problems: the job-shop scheduling problem and the traveling salesman
problem. Later, we will apply these two basic problems to demonstrate the results.
2.1.1 Job-Shop Scheduling
First, we consider the classical job-shop scheduling problem (JSP) which is a standard deterministic RAP (Pinedo, 2002). We have a set of jobs, J = {J1 , . . . , Jn }, to be processed
through a set of machines, M = {M1 , . . . , Mk }. Each j  J consists of a sequence of nj
tasks, for each task tji  T , where i  {1, . . . , nj }, there is a machine mji  M which can
process the task, and a processing time pji  N. The aim of the optimization is to find a
feasible schedule that minimizes a given performance measure. A solution, i.e., a schedule,
is a suitable task to starting time assignment. Figure 1 visualizes an example schedule
by using a Gantt chart. Note that a Gantt chart (Pinedo, 2002) is a figure using bars, in
order to illustrate the starting and finishing times of the tasks on the resources.
The concept of feasibility will be defined in Section 2.2.1. In the case of JSP a feasible
schedule can be associated with an ordering of the tasks, i.e., the order in which they will
be executed on the machines. There are many types of performance measures available for
JSP, but probably the most commonly applied one is the maximum completion time of the
tasks, also called makespan. In case of applying makespan, JSP can be interpreted as the
problem of finding a schedule which completes all tasks in every job as soon as possible.
Later, we will study an extension of JSP, called the flexible job-shop scheduling problem
(FJSP) that arises when some of the machines are interchangeable, i.e., there may be tasks
that can be executed on several machines. In this case the processing times are given by
a partial function, p : M  T , N. Note that a partial function is a binary relation
456

fiAdaptive Stochastic Resource Control

that associates the elements of its domain set with at most one element of its range set.
Throughout the paper we use , to denote partial function type binary relations.

Figure 1: A possible solution to JSP, presented in a Gantt chart. Tasks having the same
color belong to the same job and should be processed in the given order. The
vertical gray dotted line indicates the maximum completion time of the tasks.

2.1.2 Traveling Salesman
One of the basic logistic problems is the traveling salesman problem (TSP) that can be
stated as follows. Given a number of cities and the costs of travelings between them,
which is the least-cost round-trip route that visits each city exactly once and then returns
to the starting city. Several variants of TSP are known, the most standard one can be
formally characterized by a connected, undirected, edge-weighted graph G = hV, E, wi,
where V = {1, . . . , n} is the vertex set corresponding to the set of cities,E  V  V is
the set of edges which represents the roads between the cities, and w : E  N defines the
weights of the edges: the durations of the trips. The aim of the optimization is to find a
Hamilton-circuit with the smallest possible weight. Note that a Hamilton-circuit is a graph
cycle that starts at a vertex, passes through every vertex exactly once, and returns to the
starting vertex. Take a look at Figure 2 for an example Hamilton-circuit.

Figure 2: A possible solution to TSP, a closed path in the graph. The black edges constitute
a Hamilton-circuit in the given connected, undirected, edge-weighted graph.
457

fiCsaji & Monostori

2.2 Deterministic Framework
Now, we present a general framework to model resource allocation problems. This framework can be treated as a generalization of several classical RAPs, such as JSP and TSP.
First, a deterministic resource allocation problem is considered: an instance of the
problem can be characterized by an 8-tuple hR, S, O, T , C, d, e, ii. In details the problem
consists of a set of reusable resources R together with S that corresponds to the set of
possible resource states. A set of allowed operations O is also given with a subset T  O
which denotes the target operations or tasks. R, S and O are supposed to be finite and
they are pairwise disjoint. There can be precedence constrains between the tasks, which are
represented by a partial ordering C  T  T . The durations of the operations depending on
the state of the executing resource are defined by a partial function d : S  O , N, where
N is the set of natural numbers, thus, we have a discrete-time model. Every operation can
affect the state of the executing resource, as well, that is described by e : S  O , S which
is also a partial function. It is assumed that dom(d) = dom(e), where dom() denotes the
domain set of a function. Finally, the initial states of the resources are given by i : R  S.
The state of a resource can contain all relevant information about it, for example, its type
and current setup (scheduling problems), its location and load (transportation problems) or
condition (maintenance and repair problems). Similarly, an operation can affect the state
in many ways, e.g., it can change the setup of the resource, its location or condition. The
system must allocate each task (target operation) to a resource, however, there may be
cases when first the state of a resource must be modified in order to be able to execute
a certain task (e.g., a transporter may need, first, to travel to its loading/source point, a
machine may require repair or setup). In these cases non-task operations may be applied.
They can modify the states of the resources without directly serving a demand (executing
a task). It is possible that during the resource allocation process a non-task operation is
applied several times, but other non-task operations are completely avoided (for example,
because of their high cost). Nevertheless, finally, all tasks must be completed.
2.2.1 Feasible Resource Allocation
A solution for a deterministic RAP is a partial function, the resource allocator function,
% : R  N , O that assigns the starting times of the operations on the resources. Note
that the operations are supposed to be non-preemptive (they may not be interrupted).
A solution is called feasible if and only if the following four properties are satisfied:
1. All tasks are associated with exactly one (resource, time point) pair:
v  T : ! hr, ti  dom(%) : v = %(r, t).
2. Each resource executes, at most, one operation at a time:
u, v  O : u = %(r, t1 )  v = %(r, t2 )  t1  t2 < t1 + d(s(r, t1 ), u).
3. The precedence constraints on the tasks are satisfied:
 hu, vi  C : [u = %(r1 , t1 )  v = %(r2 , t2 )]  [t1 + d(s(r1 , t1 ), u)  t2 ] .
4. Every operation-to-resource assignment is valid:
 hr, ti  dom(%) : hs(r, t), %(r, t)i  dom(d),
458

fiAdaptive Stochastic Resource Control

where s : R  N  S describes the states of the

 i(r)
s(r, t  1)
s(r, t) =

e(s(r, t  1), %(r, t))

resources at given times
if t = 0
if hr, ti 
/ dom(%)
otherwise

A RAP is called correctly specified if there exists at least one feasible solution. In what
follows it is assumed that the problems are correctly specified. Take a look at Figure 3.

Figure 3: Feasibility  an illustration of the four forbidden properties, using JSP as an
example. The presented four cases are excluded from the set of feasible schedules.

2.2.2 Performance Measures
The set of all feasible solutions is denoted by S. There is a performance (or cost) associated
with each solution, which is defined by a performance measure  : S  R that often depends
on the task completion times, only. Typical performance measures that appear in practice
include: maximum completion time or mean flow time. The aim of the resource allocator
system is to compute a feasible solution with maximal performance (or minimal cost).
Note that the performance measure can assign penalties for violating release and due
dates (if they are available) or can even reflect the priority of the tasks. A possible generalization of the given problem is the case when the operations may require more resources simultaneously, which is important to model, e.g., resource constrained project scheduling problems. However, it is straightforward to extend the framework to this case: the definition of
d and e should be changed to d : S hki O  N and e : S hki O  S hki , where S hki = ki=1 S i
and k  |R|. Naturally, we assume that for all hs, oi  dom(e) : dim(e(s, o)) = dim(s).
Although, managing tasks with multiple resource requirements may be important in some
cases, to keep the analysis as simple as possible, we do not deal with them in the paper.
Nevertheless, most of the results can be easily generalized to that case, as well.
459

fiCsaji & Monostori

2.2.3 Demonstrative Examples
Now, as demonstrative examples, we reformulate (F)JSP and TSP in the given framework.
It is straightforward to formulate scheduling problems, such as JSP, in the presented
resource allocation framework: the tasks of JSP can be directly associated with the tasks of
the framework, machines can be associated with resources and processing times with durations. The precedence constraints are determined by the linear ordering of the tasks in each
job. Note that there is only one possible resource state for every machine. Finally, feasible
schedules can be associated with feasible solutions. If there were setup-times in the problem, as well, then there would be several states for each resource (according to its current
setup) and the set-up procedures could be associated with the non-task operations.
A RAP formulation of TSP can be given as follows. The set of resources consists
of only one element, namely the salesman, therefore, R = {r}. The possible states
of resource r (the salesman) are S = {s1 , . . . , sn }. If the state (of r) is si , it indicates
that the salesman is in city i. The allowed operations are the same as the allowed tasks,
O = T = {t1 , . . . , tn }, where the execution of task ti symbolizes that the salesman travels to
city i from his current location. The constraints C = {ht2 , t1 i , ht3 , t1 i . . . , htn , t1 i} are used
for forcing the system to end the whole round-tour in city 1, which is also the starting city,
thus, i(r) = s1 . For all si  S and tj  T : hsi , tj i  dom(d) if and only if hi, ji  E. For all
hsi , tj i  dom(d) : d(si , tj ) = wij and e(si , tj ) = sj . Note that dom(e) = dom(d) and the first
feasibility requirement guarantees that each city is visited exactly once. The performance
measure  is the latest arrival time, (%) = max {t + d(s(r, t), %(r, t)) | hr, ti  dom(%)}.
2.2.4 Computational Complexity
If we use a performance measure which has the property that a solution can be precisely
defined by a bounded sequence of operations (which includes all tasks) with their assignment
to the resources and, additionally, among the solutions generated this way an optimal
one can be found, then the RAP becomes a combinatorial optimization problem. Each
performance measure monotone in the completion times, called regular, has this property.
Because the above defined RAP is a generalization of, e.g., JSP and TSP, it is strongly
NP-hard and, furthermore, no good polynomial-time approximation of the optimal resource
allocating algorithm exits, either (Papadimitriou, 1994).
2.3 Stochastic Framework
So far our model has been deterministic, now we turn to stochastic RAPs. The stochastic
variant of the described general class of RAPs can be defined by randomizing functions d,
e and i. Consequently, the operation durations become random, d : S  O  (N), where
(N) is the space of probability distributions over N. Also the effects of the operations
are uncertain, e : S  O  (S) and the initial states of the resources can be stochastic,
as well, i : R  (S). Note that the ranges of functions d, e and i contain probability
distributions, we denote the corresponding random variables by D, E and I, respectively.
The notation X  f indicate that random variable X has probability distribution f . Thus,
D(s, o)  d(s, o), E(s, o)  e(s, o) and I(r)  i(r) for all s  S, o  O and r  R. Take a
look at Figure 4 for an illustration of the stochastic variants of the JSP and TSP problems.
460

fiAdaptive Stochastic Resource Control

2.3.1 Stochastic Dominance
In stochastic RAPs the performance of a solution is also a random variable. Therefore,
in order to compare the performance of different solutions, we have to compare random
variables. Many ways are known to make this comparison. We may say, for example,
that a random variable has stochastic dominance over another random variable almost
surely, in likelihood ratio sense, stochastically, in the increasing convex sense or
in expectation. In different applications different types of comparisons may be suitable,
however, probably the most natural one is based upon the expected values of the random
variables. The paper applies this kind of comparison for stochastic RAPs.

Figure 4: Randomization in case of JSP (left) and in case of TSP (right). In the latter, the
initial state, the durations and the arrival vertex could be uncertain, as well.

2.3.2 Solution Classification
Now, we classify the basic types of resource allocation techniques. First, in order to give a
proper classification we begin with recalling the concepts of open-loop and closed-loop
controllers. An open-loop controller, also called a non-feedback controller, computes its
input into a system by using only the current state and its model of the system. Therefore,
an open-loop controller does not use feedback to determine if its input has achieved the
desired goal, and it does not observe the output of the process being controlled. Conversely,
a closed-loop controller uses feedback to control states or outputs of a dynamical system
(Sontag, 1998). Closed-loop control has a significant advantage over open-loop solutions in
dealing with uncertainties. Hence, it has improved reference tracking performance, it can
stabilize unstable processes and reduced sensitivity to parameter variations.
In deterministic RAPs there is no significant difference between open- and closed-loop
controls. In this case we can safely restrict ourselves to open-loop methods. If the solution
is aimed at generating the resource allocation off-line in advance, then it is called predictive.
Thus, predictive solutions perform open-loop control and assume a deterministic environment. In stochastic resource allocation there are some data (e.g., the actual durations)
that will be available only during the execution of the plan. Based on the usage of this
information, we identify two basic types of solution techniques. An open-loop solution that
can deal with the uncertainties of the environment is called proactive. A proactive solution
allocates the operations to resources and defines the orders of the operations, but, because
the durations are uncertain, it does not determine precise starting times. This kind of
461

fiCsaji & Monostori

technique can be applied only when the durations of the operations are stochastic, but, the
states of the resources are known perfectly (e.g., stochastic JSP). Finally, in the stochastic
case closed-loop solutions are called reactive. A reactive solution is allowed to make the
decisions on-line, as the process actually evolves providing more information. Naturally,
a reactive solution is not a simple sequence, but rather a resource allocation policy (to be
defined later) which controls the process. The paper focuses on reactive solutions, only. We
will formulate the reactive solution of a stochastic RAP as a control policy of a suitably
defined Markov decision process (specially, a stochastic shortest path problem).
2.4 Markov Decision Processes
Sequential decision-making under the presence of uncertainties is often modeled by MDPs
(Bertsekas & Tsitsiklis, 1996; Sutton & Barto, 1998; Feinberg & Shwartz, 2002). This
section contains the basic definitions, the notations applied and some preliminaries.
By a (finite, discrete-time, stationary, fully observable) Markov decision process (MDP)
we mean a stochastic system characterized by a 6-tuple hX, A, A, p, g, i, where the components are as follows: X is a finite set of discrete states and A is a finite set of control actions.
Mapping A : X  P(A) is the availability function that renders each state a set of actions
available in the state where P denotes the power set. The transition-probability function is
given by p : X  A  (X), where (X) is the space of probability distributions over X.
Let p(y | x, a) denote the probability of arrival at state y after executing action a  A(x)
in state x. The immediate-cost function is defined by g : X  A  R, where g(x, a) is the
cost of taking action a in state x. Finally, constant   [0, 1] denotes the discount rate or
discount factor. If  = 1, then the MDP is called undiscounted, otherwise it is discounted.
It is possible to extend the theory to more general state and action spaces, but at
the expense of increased mathematical complexity. Finite state and action sets are mostly
sufficient for digitally implemented controls and, therefore, we restrict ourselves to this case.

Figure 5: Markov decision processes  the interaction of the decision-maker and the uncertain environment (left); the temporal progress of the system (right).
An interpretation of an MDP can be given if we consider an agent that acts in an
uncertain environment, which viewpoint is often taken in RL. The agent receives information
about the state of the environment, x, in each state x the agent is allowed to choose an
action a  A(x). After an action is selected, the environment moves to the next state
according to the probability distribution p(x, a), and the decision-maker collects its one462

fiAdaptive Stochastic Resource Control

step cost, g(x, a), as illustrated by Figure 5. The aim of the agent is to find an optimal
behavior (policy) such that applying this strategy minimizes the expected cumulative costs.
A stochastic shortest path (SSP) problem is a special MDP in which the aim is to find
a control policy such that reaches a pre-defined terminal state starting from a given initial
state, additionally, minimizes the expected total costs of the path, as well. A policy is
called proper if it reaches the terminal state with probability one. A usual assumption
when dealing with SSP problems is that all policies are proper, abbreviated as APP.
2.4.1 Control Policies
A (stationary, Markov) control policy determines the action to take in each possible state.
A deterministic policy,  : X  A, is simply a function from states to control actions. A
randomized policy,  : X  (A), is a function from states to probability distributions
over actions. We denote the probability of executing action a in state x by (x)(a) or, for
short, by (x, a). Naturally, deterministic policies are special cases of randomized ones and,
therefore, unless indicated otherwise, we consider randomized control policies.
For any x
e0  (X) initial probability distribution of the states, the transition probabilities p together with a control policy  completely determine the progress of the system in
a stochastic sense, namely, they define a homogeneous Markov chain on X,
x
et+1 = P ()e
xt ,

where x
et is the state probability distribution vector of the system at time t, and P ()
denotes the probability transition matrix induced by control policy , formally defined as
X
[P ()]x,y =
p(y | x, a) (x, a).
aA

2.4.2 Value Functions
The value or cost-to-go function of a policy  is a function from states to costs. It is defined
on each state: J  : X  R. Function J  (x) gives the expected value of the cumulative
(discounted) costs when the system is in state x and it follows policy  thereafter,
J  (x) = E

"

N
X
t=0

#
fi
fi
t g(Xt , At ) fifi X0 = x ,

(1)

where Xt and At are random variables, At is selected according to control policy  and the
distribution of Xt+1 is p(Xt , At ). The horizon of the problem is denoted by N  N  {}.
Unless indicated otherwise, we will always assume that the horizon is infinite, N = .
Similarly to the definition of J  , one can define action-value functions of control polices,
" N
#
fi
X
fi


t
 fi
Q (x, a) = E
 g(Xt , At ) fi X0 = x, A0 = a ,
t=0

where the notations are the same as in equation (1). Action-value functions are especially
important for model-free approaches, such as the classical Q-learning algorithm.
463

fiCsaji & Monostori

2.4.3 Bellman Equations
We say that 1  2 if and only if, for all x  X, we have J 1 (x)  J 2 (x). A control policy
is (uniformly) optimal if it is less than or equal to all other control policies.
There always exists at least one optimal policy (Sutton & Barto, 1998). Although there
may be many optimal policies, they all share the same unique optimal cost-to-go function,
denoted by J  . This function must satisfy the Bellman optimality equation (Bertsekas &
Tsitsiklis, 1996), T J  = J  , where T is the Bellman operator, defined for all x  X, as
(T J)(x) = min

aA(x)

h

g(x, a) + 

X

yX

i
p(y | x, a)J(y) .

(2)

The Bellman equation for an arbitrary (stationary, Markov, randomized) policy is
(T  J)(x) =

X

h
i
X
(x, a) g(x, a) + 
p(y | x, a)J(y) ,
yX

aA(x)

where the notations are the same as in equation (2) and we also have T  J  = J  .
From a given value function J, it is straightforward to get a policy, e.g., by applying a
greedy and deterministic policy (w.r.t. J) that always selects actions of minimal costs,
h
i
X
p(y | x, a)J(y) .
(x)  arg min g(x, a) + 
aA(x)

yX

MDPs have an extensively studied theory and there exist a lot of exact and approximate solution methods, e.g., value iteration, policy iteration, the Gauss-Seidel method,
Q-learning, Q(), SARSA and TD() - temporal difference learning (Bertsekas & Tsitsiklis, 1996; Sutton & Barto, 1998; Feinberg & Shwartz, 2002). Most of these reinforcement
learning algorithms work by iteratively approximating the optimal value function.
2.5 Reactive Resource Control
In this section we formulate reactive solutions of stochastic RAPs as control policies of
suitably reformulated SSP problems. The current task durations and resource states will
only be incrementally available during the resource allocation control process.
2.5.1 Problem Reformulation
A state x  X is defined as a 4-tuple x = h, , %, i, where   N is the current time and the
function  : R  S determines the current states of the resources. The partial functions %
and  store the past of the process, namely, % : RN 1 , O contains the resources and the
times in which an operation was started and  : R  N 1 , N describes the finish times
of the already completed operations, where N = {0, . . . ,  }. Naturally, dom()  dom(%).
By TS (x)  T we denote the set of tasks which have been started in state x (before the
current time  ) and by TF (x)  TS (x) the set of tasks that have been finished already in
state x. It is easy to see that TS (x) = rng(%)  T and TF (x) = rng(%|dom() )  T , where
rng() denotes the range or image set of a function. The process starts from an initial state
464

fiAdaptive Stochastic Resource Control

xs = h0, , , i, which corresponds to the situation at time zero when none of the operations
have been started. The initial probability distribution, , can be calculated as follows
(xs ) = P ((r1 ) = I(r1 ), . . . , (rn ) = I(rn )) ,
where I(r)  i(r) denotes the random variable that determines the initial state of resource
r  R and n = |R|. Thus,  renders initial states to resources according to the (multivariate)
probability distribution I that is a component of the RAP. We introduce a set of terminal
states, as well. A state x is considered as a terminal state (x  T) in two cases. First, if all
the tasks are finished in the state, formally, if TF (x) = T and it can be reached from a state
x, where TF (x) 6= T . Second, if the system reached a state where no tasks or operations
can be executed, in other words, if the allowed set of actions is empty, A(x) = .
It is easy to see that, in theory, we can aggregate all terminal states to a global unique
terminal state and introduce a new unique initial state, x0 , that has only one available
action which takes us randomly (with  distribution) to the real initial states. Then, the
problem becomes a stochastic shortest path problem and the aim can be described as finding
a routing having minimal expected cost from the new initial state to the goal state.
At every time  the system is informed on the finished operations, and it can decide
on the operations to apply (and by which resources). The control action space contains
operation-resource assignments avr  A, where v  O and r  R, and a special await
control that corresponds to the action when the system does not start a new operation at
the current time. In a non-terminal state x = h, , %, i the available actions are
await  A(x)  TS (x) \ TF (x) 6= 
v  O : r  R : avr  A(x)  (v  O \ TS (x)   hr, ti  dom(%) \ dom() : r 6= r 
 h(r), vi  dom(d)  v  T  (u  T : hu, vi  C  u  TF (x)))
Thus, action await is available in every state with an unfinished operation; action avr is
available in states in which resource r is idle, it can process operation v, additionally, if v
is a task, then it was not executed earlier and its precedence constraints are satisfied.
If an action avr  A(x) is executed in a state x = h, , %, i, then the system moves
with probability one to a new state x = h, , %, i, where % = %  {hhr, ti , vi}. Note that
we treat functions as sets of ordered pairs. The resulting x corresponds to the state where
operation v has started on resource r if the previous state of the environment was x.
The effect of the await action is that from x = h, , %, i it takes to an x = h + 1, , %, i,
where an unfinished operation %(r, t) that was started at t on r finishes with probability
P(hr, ti  dom() | x, hr, ti  dom(%) \ dom()) =

P(D((r), %(r, t)) + t =  )
,
P(D((r), %(r, t)) + t   )

where D(s, v)  d(s, v) is a random variable that determines the duration of operation v
when it is executed by a resource which has state s. This quantity is called completion rate
in stochastic scheduling theory and hazard rate in reliability theory. We remark that for
operations with continuous durations, this quantity is defined by f (t)/(1  F (t)), where f
denotes the density function and F the distribution of the random variable that determines
the duration of the operation. If operation v = %(r, t) has finished (hr, ti  dom()), then
465

fiCsaji & Monostori

(r, t) =  and (r) = E(r, v), where E(r, v)  e(r, v) is a random variable that determines
the new state of resource r after it has executed operation v. Except the extension of its
domain set, the other values of function  do not change, consequently,  hr, ti  dom() :
(r, t) = (r, t). In other words,  is a conservative extension of , formally,   .
The immediate-cost function g for a given  performance measure is defined as follows.
Assume that  depends only on the operation-resource assignments and the completion
times. Let x = h, , %, i and x = h , , %, i. Then, in general, if the system arrives at
state x after executing control action a in state x, it incurs cost (%, )  (%, ).
Note that, though, in Section 2.2.2 performance measures were defined on complete
solutions, for most measures applied in practice, such as total completion time or weighted
total lateness, it is straightforward to generalize the performance measure to partial solutions, as well. One may, for example, treat the partial solution of a problem as a complete
solution of a smaller (sub)problem, namely, a problem with fewer tasks to be completed.
If the control process has failed, more precisely, if it was not possible to finish all tasks,
then the immediate-cost function should render penalties (depending on the specific problem) regarding the non-completed tasks proportional to the number of these failed tasks.
2.5.2 Favorable Features
Let us call the introduced SSPs, which describe stochastic RAPs, RAP-MDPs. In this
section we overview some basic properties of RAP-MDPs. First, it is straightforward to see
that these MDPs have finite action spaces, since |A|  |R| |O| + 1 always holds.
Though, the state space of a RAP-MDP is denumerable in general, if the allowed number of non-task operations is bounded and the random variables describing the operation
durations are finite, the state space of the reformulated MDP becomes finite, as well.
We may also observe that RAP-MDPs are acyclic (or aperiodic), viz., none of the states
can appear multiple times, because during the resource allocation process  and dom(%) are
non-decreasing and, additionally, each time the state changes, the quantity  + |dom(%)|
strictly increases. Therefore, the system cannot reach the same state twice. As an immediate
consequence, all policies eventually terminate (if the MDP was finite) and, thus, are proper.
For the effective computation of a good control policy, it is important to try to reduce
the number of states. We can do so by recognizing that if the performance measure  is
non-decreasing in the completion times, then an optimal control policy of the reformulated
RAP-MDP can be found among the policies which start new operations only at times when
another operation has been finished or in an initial state. This statement can be supported
by the fact that without increasing the cost ( is non-decreasing) every operation can be
shifted earlier on the resource which was assigned to it until it reaches another operation,
or until it reaches a time when one of its preceding tasks is finished (if the operation was
a task with precedence constrains), or, ultimately, until time zero. Note that most of the
performance measures used in practice (e.g., makespan, weighted completion time, average
tardiness) are non-decreasing. As a consequence, the states in which no operation has been
finished can be omitted, except the initial states. Therefore, each await action may lead to
a state where an operation has been finished. We may consider it, as the system executes
automatically an await action in the omitted states. By this way, the state space can be
decreased and, therefore, a good control policy can be calculated more effectively.
466

fiAdaptive Stochastic Resource Control

2.5.3 Composable Measures
For a large class of performance measures, the state representation can be simplified by
leaving out the past of the process. In order to do so, we must require that the performance
measure be composable with a suitable function. In general, we call a function f : P(X) 
R -composable if for any A, B  X, AB =  it holds that (f (A), f (B)) = f (AB), where
 : R  R  R is called the composition function, and X is an arbitrary set. This definition
can be directly applied to performance measures. If a performance measure, for example, is
-composable, it indicates that the value of any complete solution can be computed from the
values of its disjoint subsolutions (solutions to subproblems) with function . In practical
situations the composition function is often the max, the min or the + function.
If the performance measure  is -composable, then the past can be omitted from the
state representation, because the performance can be calculated incrementally. Thus, a
state can be described as x = h , , , TU i, where   N, as previously, is the current
time,   R contains the performance of the current (partial) solution and TU is the set of
unfinished tasks. The function  : R  S  (O  {})  N determines the current states of
the resources together with the operations currently executed by them (or  if a resource is
idle) and the starting times of the operations (needed to compute their completion rates).
In order to keep the analysis as simple as possible, we restrict ourselves to composable
functions, since almost all performance measures that appear in practice are -composable
for a suitable  (e.g., makespan or total production time is max-composable).
2.5.4 Reactive Solutions
Now, we are in a position to define the concept of reactive solutions for stochastic RAPs. A
reactive solution is a (stationary, Markov) control policy of the reformulated SSP problem.
A reactive solution performs a closed-loop control, since at each time step the controller is
informed about the current state of system and it can choose a control action based upon
this information. Section 3 deals with the computation of effective control policies.

3. Solution Methods
In this section we aim at giving an effective solution to large-scale RAPs in uncertain and
changing environments with the help of different machine learning approaches. First, we
overview some approximate dynamic programming methods to compute a good policy.
Afterwards, we investigate two function approximation techniques to enhance the solution.
Clustering, rollout algorithm and action space decomposition as well as distributed sampling are also considered, as they can speedup the computation of a good control policy
considerably and, therefore, are important additions if we face large-scale problems.
3.1 Approximate Dynamic Programming
In the previous sections we have formulated RAPs as acyclic (aperiodic) SSP problems.
Now, we face the challenge of finding a good policy. In theory, the optimal value function of a
finite MDP can be exactly computed by dynamic programming (DP) methods, such as value
iteration or the Gauss-Seidel method. Alternatively, an exact optimal policy can be directly
calculated by policy iteration. However, due to the curse of dimensionality, computing
467

fiCsaji & Monostori

an exact optimal solution by these methods is practically infeasible, e.g., typically both
the required amount of computation and the needed storage space, viz., memory, grows
quickly with the size of the problem. In order to handle the curse, we should apply
approximate dynamic programming (ADP) techniques to achieve a good approximation of
an optimal policy. Here, we suggest using sampling-based fitted Q-learning (FQL). In each
trial a Monte-Carlo estimate of the value function is computed and projected onto a suitable
function space. The methods described in this section (FQL, MCMC and the Boltzmann
formula) should be applied simultaneously, in order to achieve an efficient solution.
3.1.1 Fitted Q-learning
Watkins Q-learning is a very popular off-policy model-free reinforcement learning algorithm (Even-Dar & Mansour, 2003). It works with action-value functions and iteratively
approximates the optimal value function. The one-step Q-learning rule is defined as follows
Qi+1 (x, a) = (1  i (x, a))Qi (x, a) + i (x, a)(Tei Qi )(x, a),
(Tei Qi )(x, a) = g(x, a) +  min Qi (Y, B),
BA(Y )

where i (x, a) are the learning rates and Y is a random variable representing a state generated from the pair (x, a) by simulation, that is, according to the probability distribution
p(x, a). It is known (Bertsekas & Tsitsiklis, 1996) that if i (x)  [0, 1] and they satisfy

X

i (x, a) = 

and


X

i2 (x, a) < ,

i=0

i=0

then the Q-learning algorithm converges with probability one to the optimal action-value
function, Q , in the case of lookup table representation when each state-action value is
stored independently. We speak about the method of fitted Q-learning (FQL) when the
value function is represented by a (typically parametric) function from a suitable function
space, F, and after each iteration, the updated value function is projected back onto F.
A useful observation is that we need the learning rate parameters only to overcome
the effects of random disturbances. However, if we deal with deterministic problems, this
part of the method can be simplified. The resulting algorithm simply updates Q(x, a) with
the minimum of the previously stored estimation and the current outcome of the simulation,
which is also the core idea of the LRTA* algorithm (Bulitko & Lee, 2006). When we dealt
with deterministic resource allocation problems, we applied this simplification, as well.
3.1.2 Evaluation by Simulation
Naturally, in large-scale problems we cannot update all states at once. Therefore, we perform Markov chain Monte Carlo (MCMC) simulations (Hastings, 1970; Andrieu, Freitas,
Doucet, & Jordan, 2003) to generate samples with the model, which are used for computing the new approximation of the estimated cost-to-go function. Thus, the set of states to
be updated in episode i, namely Xi , is generated by simulation. Because RAP-MDPs are
acyclic, we apply prioritized sweeping, which means that after each iteration, the cost-to-go
estimations are updated in the reverse order in which they appeared during the simulation.
468

fiAdaptive Stochastic Resource Control


	
Assume, for example, that Xi = xi1 , xi2 , . . . , xiti is the set of states for the update of the
value function after iteration i, where j < k implies that xij appeared earlier during the
simulation than xik . In this case the order in which the updates are performed, is xiti , . . . , xi1 .
Moreover, we do not need a uniformly optimal value function, it is enough to have a good
approximation of the optimal cost-to-go function for the relevant states. A state is called
relevant if it can appear with positive probability during the application of an optimal policy. Therefore, it is sufficient to consider the case when xi1 = x0 , where xi1 is the first state
in episode i and x0 is the (aggregated) initial state of the SSP problem.
3.1.3 The Boltzmann Formula
In order to ensure the convergence of the FQL algorithm, one must guarantee that each
cost-to-go estimation be continuously updated. A technique used often to balance between
exploration and exploitation is the Boltzmann formula (also called softmin action selection):
i (x, a) =

exp(Qi (x, a)/ )
,
P
exp(Qi (x, b)/ )

bA(x)

where   0 is the Boltzmann (or Gibbs) temperature, i is the episode number. It is easy
to see that high temperatures cause the actions to be (nearly) equiprobable, low ones cause
a greater difference in selection probability for actions that differ in their value estimations.
Note that here we applied the Boltzmann formula for minimization, viz., small values result
in high probability. It is advised to extend this approach by a variant of simulated annealing
(Kirkpatrick, Gelatt, & Vecchi, 1983) or Metropolis algorithm (Metropolis, Rosenbluth,
Rosenbluth, Teller, & Teller, 1953), which means that  should be decreased over time, at
a suitable, e.g., logarithmic, rate (Singh, Jaakkola, Littman, & Szepesvari, 2000).
3.2 Cost-to-Go Representations
In Section 3.1 we suggested FQL for iteratively approximating the optimal value function.
However, the question of a suitable function space, onto which the resulted value functions
can be effectively projected, remained open. In order to deal with large-scale problems (or
problems with continuous state spaces) this question is crucial. In this section, first, we
suggest features for stochastic RAPs, then describe two methods that can be applied to
compactly represent value functions. The first and simpler one applies hash tables while
the second, more sophisticated one, builds upon the theory of support vector machines.
3.2.1 Feature Vectors
In order to efficiently apply a function approximator, first, the states and the actions of the
reformulated MDP should be associated with numerical vectors representing, e.g., typical
features of the system. In the case of stochastic RAPs, we suggest using features as follows:
 For each resource in R, the resource state id, the operation id of the operation being
currently processed by the resource (could be idle), as well as the starting time of the
last (and currently unfinished) operation can be a feature. If the model is available
to the system, the expected ready time of the resource should be stored instead.
469

fiCsaji & Monostori

 For each task in T , the task state id could be treated as a feature that can assume
one of the following values: not available (e.g., some precedence constraints are not
satisfied), ready for execution, being processed or finished. It is also advised to
apply 1-out-of-n coding, viz., each value should be associated with a separate bit.
 In case we use action-value functions, for each action (resource-operation assignment)
the resource id and the operation id could be stored. If the model is available, then
the expected finish time of the operation should also be taken into account.
In the case of a model-free approach which applies action-value functions, for example,
the feature vector would have 3|R|+|T |+2 components. Note that for features representing
temporal values, it is advised to use relative time values instead of absolute ones.
3.2.2 Hash Tables
Suppose that we have a vector w = hw1 , w2 , . . . , wk i, where each component wi corresponds
to a feature of a state or an action. Usually, the value estimations for all of these vectors
cannot be stored in the memory. In this case one of the simplest methods to be applied is
to represent the estimations in a hash table. A hash table is, basically, a dictionary in which
keys are mapped to array positions by hash functions. If all components can assume finite
values, e.g., in our finite-state, discrete-time case, then a key could be generated as follows.
Let us suppose that for all wi we have 0  wi < mi , then w can be seen as a number in a
mixed radix numeral system and, therefore, a unique key can be calculated as
(w) =

k
X

wi

i=1

i1
Y

mj ,

j=1

where (w) denotes the key of w, and the value of an empty product is treated as one.
The hash function, , maps feature vector keys to memory positions. More precisely,
if we have memory for storing only d value estimations, then the hash function takes the
form  : rng()  {0, . . . , d  1}, where rng() denotes the range set of a function.
It is advised to apply a d that is prime. In this case a usual hashing function choice is
(x) = y if and only if y  x (mod d), namely, if y is congruent to x modulo d.
Having the keys of more than one item map to the same position is called a collision. In
the case of RAP-MDPs we suggest a collision resolution method as follows. Suppose that
during a value update the feature vector of a state (or a state-action pair) maps to a position
that is already occupied by another estimation corresponding to another item (which can be
detected, e.g., by storing the keys). Then we have a collision and the estimation of the new
item should overwrite the old estimation if and only if the MDP state corresponding to the
new item appears with higher probability during execution starting from the (aggregated)
initial state than the one corresponding to the old item. In case of a model-free approach,
the item having a state with smaller current time component can be kept.
Despite its simplicity, the hash table representation has several disadvantages, e.g., it
still needs a lot of memory to work efficiently, it cannot easily handle continuous values
and, it only stores individual data, moreover, it does not generalize to similar items. In
the next section we present a statistical approach that can deal with these issues, as well.
470

fiAdaptive Stochastic Resource Control

3.2.3 Support Vector Regression
A promising choice for compactly representing the cost-to-go function is to use support vector
regression (SVR) from statistical learning theory. For maintaining the value function estimations, we suggest using -SVRs which were proposed by Scholkopf, Smola, Williamson,
and Bartlett (2000). They have an advantage over classical -SVRs according to which,
through the new parameter , the number of support vectors can be controlled. Additionally, parameter  can be eliminated. First, the core ideas of -SVRs are presented.
In general, SVR addresses the problem as follows. We are given a sample, a set of data
points {hx1 , y1 i , . . . , hxl , yl i}, such that xi  X is an input, where X is a measurable space,
and yi  R is the target output. For simplicity, we shall assume that X  Rk , where k  N.
The aim of the learning process is to find a function f : X  R with a small risk
Z
l(f, x, y)dP (x, y),
(3)
R[f ] =
X

where P is a probability measure, which is responsible for the generation of the observations
and l is a loss function, such as l(f, x, y) = (f (x)  y)2 . A common error function used
in SVRs is the so-called -insensitive loss function, |f (x)  y| = max {0, |f (x)  y|  }.
Unfortunately, we cannot minimize (3) directly, since we do not know P , we are given the
sample, only (generated, e.g., by simulation). We try to obtain a small risk by minimizing
the regularized risk functional in which we average over the training sample
1

kwk2 + C  Remp
[f ],
2

(4)

where, kwk2 is a term that characterizes the model complexity and C > 0 a constant that
determines the trade-off between the flatness of the regression and the amount up to which

[f ] is defined as follows
deviations larger than  are tolerated. The function Remp
l


Remp
[f ] =

1X
|f (xi )  yi | .
l
i=1

It measures the -insensitive average training error. The problem which arises when we
try to minimize (4) is called empirical risk minimization (ERM). In regression problems we
usually have a Hilbert space F, containing X  R type (typically non-linear) functions,
and our aim is to find a function f that is close to yi in each xi and takes the form
X
wj j (x) + b = wT (x) + b,
f (x) =
j

where j  F, wj  R and b  R. Using Lagrange multiplier techniques, we can rewrite the
regression problem in its dual form (Scholkopf et al., 2000) and arrive at the final -SVR
optimization problem. The resulting regression estimate then takes the form as follows
f (x) =

l
X

(i  i )K(xi , x) + b,

i=1

where i and i are the Lagrange multipliers, and K denotes an inner product kernel
defined by K(x, y) = h(x), (y)i, where h, i denotes inner product. Note that i , i 6= 0
471

fiCsaji & Monostori

holds usually only for a small subset of training samples, furthermore, parameter b (and )
can be determined as well, by applying the Karush-Kuhn-Tucker (KKT) conditions.
Mercers theorem in functional analysis characterizes which functions correspond to an
inner product in some space F. Basic kernel types include linear, polynomial, Gaussian
and sigmoid functions. In our experiments with RAP-MDPs we have used Gaussian kernels
which are also called radial basis function (RBF) kernels. RBF kernels are defined by
K(x, y) = exp( kx  yk2 /(2 2 )), where  > 0 is an adjustable kernel parameter.
A variant of the fitted Q-learning algorithm combined with regression and softmin action
selection is described in Table 1. It simulates a state-action trajectory with the model and
updates the estimated values of only the state-action pairs which appeared during the
simulation. Most of our RAP solutions described in the paper are based on this algorithm.
The notations of the pseudocode shown in Table 1 are as follows. Variable i contains
the episode number, ti is the length of episode i and j is a parameter for time-steps inside
an episode. The Boltzmann temperature is denoted by  , i is the control policy applied in
episode i and x0 is the (aggregated) initial state. State xij and action aij correspond to step
j in episode i. Function h computes features for state-action pairs while i denotes learning
rates. Finally, Li denotes the regression sample and Qi is the fitted value function.
Although, support vector regression offers an elegant and efficient solution to the value
function representation problem, we presented the hash table representation possibility not
only because it is much easier to implement, but also because it requires less computation,
thus, provides faster solutions. Moreover, the values of the hash table could be accessed
independently; this was one of the reasons why we applied hash tables when we dealt with
distributed solutions, e.g., on architectures with uniform memory access. Nevertheless,
SVRs have other advantages, most importantly, they can generalize to similar data.

1.
2.
3.
4.
5.
6.
7.

Regression Based Q-learning
Initialize Q0 , L0 ,  and let i = 1.
Repeat (for each episode)
Set i to a soft and semi-greedyh policy w.r.t. Qi1 , e.g.,
i
P
exp(Q
(x,
b)/
)
.
i (x, a) = exp(Qi1 (x, a)/ )/
i1
bA(x)

Simulate a state-action trajectory from x0 using policy i .
For j = ti to 1 (for each state-action pair in the episode) do
Determine the features of the state-action pair, yji = h(xij , aij ).
i
i
Compute the new action-value
h estimation for xj and aj , e.g.,
zji = (1  i )Qi1 (xij , aij ) + i g(xij , aij ) +  minbA(xi

j+1

8.
9.
10.
11.
12.

i
i
,
b)
.
Q
(x
i1
)
j+1

End loop (end of state-action 

processing)
	
ff
Update sample set Li1 with yji , zji : j = 1, . . . , ti , the result is Li .
Calculate Qi by fitting a smooth regression function to the sample of Li .
Increase the episode number, i, and decrease the temperature,  .
Until some terminating conditions are met, e.g., i reaches a limit
or the estimated approximation error to Q gets sufficiently small.
Output: the action-value function Qi (or (Qi ), e.g., the greedy policy w.r.t. Qi ).
Table 1: Pseudocode for regression-based Q-learning with softmin action selection.
472

fiAdaptive Stochastic Resource Control

3.3 Additional Improvements
Computing a (close-to) optimal solution with RL methods, such as (fitted) Q-learning, could
be very inefficient in large-scale systems, even if we apply prioritized sweeping and a capable
representation. In this section we present some additional improvements in order to speed
up to optimization process, even at the expense of achieving only suboptimal solutions.
3.3.1 Rollout Algorithms
During our experiments, which are presented in Section 4, it turned out that using a suboptimal base policy, such as a greedy policy with respect to the immediate costs, to guide
the exploration, speeds up the optimization considerably. Therefore, at the initial stage
we suggest applying a rollout policy, which is a limited lookahead policy, with the optimal
cost-to-go approximated by the cost-to-go of the base policy (Bertsekas, 2001). In order to
introduce the concept more precisely, let  be the greedy policy w.r.t. immediate-costs,
(x)  arg min g(x, a).
aA(x)

The value function of  is denoted by J  . The one-step lookahead rollout policy  based
on policy , which is an improvement of  (cf. policy iteration), can be calculated by
h
i
(x)  arg min E G(x, a) + J  (Y ) ,
aA(x)

where Y is a random variable representing a state generated from the pair (x, a) by simulation, that is, according to probability distribution p(x, a). The expected value (viz., the
expected costs and the cost-to-go of the base policy) is approximated by Monte Carlo simulation of several trajectories that start at the current state. If the problem is deterministic,
then a single simulation trajectory suffices, and the calculations are greatly simplified.
Take a look at Figure 6 for an illustration. In scheduling theory, a similar (but simplified)
concept can be found and a rollout policy would be called a dispatching rule.

Figure 6: The evaluation of state x with rollout algorithms in the deterministic (left) and
the stochastic (right) case. Circles denote states and rectangles represent actions.
The two main issues why we suggest the application of rollout algorithms in the initial
stages of value function approximation-based reinforcement learning are as follows:
473

fiCsaji & Monostori

1. We need several initial samples before the first application of approximation techniques
and these first samples can be generated by simulations guided by a rollout policy.
2. General reinforcement learning methods perform quite poorly in practice without any
initial guidance. However, the learning algorithm can start improving the rollout
policy , especially, in case we apply (fitted) Q-learning, it can learn directly from the
trajectories generated by a rollout policy, since it is an off-policy learning method.
Our numerical experiments showed that rollout algorithms provide significant speedup.
3.3.2 Action Space Decomposition
In large-scale problems the set of available actions in a state may be very large, which can
slow down the system significantly. In the current formulation of the RAP the number
of available actions in a state is O(|T | |R|). Though, even in real world situations |R|
is, usually, not very large, but T could contain thousands of tasks. Here, we suggest
decomposing the action space as shown in Figure 7. First, the system selects a task, only,
and it moves to a new state where this task is fixed and an executing resource should be
selected. In this case the state description can be extended by a new variable   T  {},
where  denotes the case when no task has been selected yet. In every other case the
system should select an executing resource for the selected task. Consequently, the new
action space is A = A1  A2 , where A1 = { av | v  T }  {a } and A2 = { ar | r  R }. As
a result, we radically decreased the number of available actions, however, the number of
possible states was increased. Our experiments showed that it was a reasonable trade-off.

Figure 7: Action selection before (up) and after (down) action space decomposition.
474

fiAdaptive Stochastic Resource Control

3.3.3 Clustering the Tasks
The idea of divide-and-conquer is widely used in artificial intelligence and recently it has
appeared in the theory of dealing with large-scale MDPs. Partitioning a problem into
several smaller subproblems is also often applied to decrease computational complexity in
combinatorial optimization problems, for example, in scheduling theory.
We propose a simple and still efficient partitioning method for a practically very important class of performance measures. In real world situations the tasks very often have
release dates and due dates, and the performance measure, e.g., total lateness and number
of tardy tasks, depends on meeting the deadlines. Note that these measures are regular.
We denote the (possibly randomized) functions defining the release and due dates of the
tasks by A : T  N and B : T  N, respectively. In this section we restrict ourselves to
performance measures that are regular and depend on due dates. In order to cluster the
tasks, we need the definition of weighted expected slack time which is given as follows
h
i
X
w(s) E B(v)  A(v)  D(s, v) ,
Sw (v) =
s(v)

where (v) = { s  S | hs, vi  dom(D) } denotes the set of resource states in which task v
can be processed, and w(s) are weights corresponding, for example, to the likelihood that
resource state s appears during execution, or they can be simply w(s) = 1/ |(v)|.

Figure 8: Clustering the tasks according to their slack times and precedence constraints.
In order to increase computational speed, we suggest clustering the tasks in T into successive disjoint subsets T1 , . . . , Tk according to the precedence constraints and the expected
slack times; take a look at Figure 8 for an illustration. The basic idea behind our approach
is that we should handle the most constrained tasks first. Therefore, ideally, if Ti and Tj are
two clusters and i < j, then tasks in Ti had expected slack times smaller than tasks in Tj .
However, in most of the cases clustering is not so simple, since the precedence constraints
must also be taken into account and this clustering criterion has the priority. Thus, if
hu, vi  C, u  Ti and v  Tj , then i  j must hold. During learning, first, tasks in T1 are
allocated to resources, only. After some episodes, we fix the allocation policy concerning
tasks in T1 and we start sampling to achieve a good policy for tasks in T2 , and so on.
Naturally, clustering the tasks is a two-edged weapon, making too small clusters may
seriously decrease the performance of the best achievable policy, making too large clusters
475

fiCsaji & Monostori

may considerably slow down the system. This technique, however, has several advantages,
e.g., (1) it effectively decreases the search space; (2) further reduces the number of available
actions in the states; and, additionally (3) speeds up the learning, since the sample trajectories become smaller (only a small part of the tasks is allocated in a trial and, consequently,
the variance of the total costs is decreased). The effects of clustering relative to the size of
the clusters were analyzed experimentally and are presented in Section 4.5.
3.3.4 Distributed Sampling
Finally, we argue that the presented approach can be easily modified in order to allow
computing a policy on several processors in a distributed way. Parallel computing can
further speed up the calculation of the solution. We will consider extensions of the algorithm
using both shared memory and distributed memory architectures. Let us suppose we have
k processors, and denote the set of all processors by P = {p1 , p2 , . . . , pk }.
In case we have a parallel system with a shared memory architecture, e.g., UMA (uniform
memory access), then it is straightforward to parallelize the computation of a control policy.
Namely, each processor p  P can sample the search space independently, while by using
the same, shared value function. The (joint) policy can be calculated using this common,
global value function, e.g., the greedy policy w.r.t. this function can be applied.
Parallelizing the solution by using an architecture with distributed memory is more
challenging. Probably the simplest way to parallelize our approach to several processors
with distributed memory is to let the processors search independently by letting them
working with their own, local value functions. After a given time or number of iterations,
we may treat the best achieved solution as the joint policy. More precisely, if we denote the
aggregated initial state by x0 , then the joint control policy  can be defined as follows
  arg min J p (x0 )

or

  arg min min Qp (x0 , a),
p (pP) aA(x0 )

p (pP)

where J p and Qp are (approximate) state- and action-value functions calculated by processor p  P. Control policy p is the solution of processor p after a given number of
iterations. During our numerical experiments we usually applied 104 iterations.
Naturally, there could be many (more sophisticated) ways to parallelize the computation
using several processors with distributed memory. For example, from time to time the
processors could exchange some of their best episodes (trajectories with the lowest costs)
and learn from the experiments of the others. In this way, they could help improve the
value functions of each other. Our numerical experiments, presented in Section 4.3, showed
that even in the simplest case, distributing the calculation speeds up the optimization
considerably. Moreover, in the case of shared memory the speedup was almost linear.
As parallel computing represents a very promising way do deal with large-scale systems,
their further theoretical and experimental investigation would be very important. For example, by harmonizing the exploration of the processors, the speedup could be improved.

4. Experimental Results
In this section some experimental results on both benchmark and industry-related problems
are presented. These experiments highlight some characteristics of the solution.
476

fiAdaptive Stochastic Resource Control

4.1 Testing Methodology
In order to experimentally study our resource control approach, a simulation environment
was developed in C++. We applied FQL and, in most of the cases, SVRs which were
realized by the LIBSVM free library for support vector machines (Chang & Lin, 2001).
After centering and scaling the data into interval [0, 1], we used Gaussian kernels and
shrinking techniques. We always applied rollout algorithms and action decomposition, but
clustering was only used in tests presented in Section 4.5, furthermore, distributed sampling
was only applied in test shown in Section 4.3. In both of the latter cases (tests for clustering
and distributed sampling) we used hash tables with approximately 256Mb hash memory.
The performance of the algorithm was measured as follows. Testing took place in two
main fields: the first one was a benchmark scheduling dataset of hard problems, the other
one was a simulation of a real world production control problem. In the first case the best
solution, viz., the optimal value of the (aggregated) initial state, J  (x0 ) = mina Q (x0 , a),
was known for most of the test instances. Some very hard instances occurred for which
only lower and upper bounds were known, e.g., J1 (x0 )  J  (x0 )  J2 (x0 ). In these cases
we assumed that J  (x0 )  (J1 (x0 ) + J2 (x0 ))/2. Since these estimations were good (viz.,
the length of the intervals were short), this simplification might not introduce considerable
error to our performance estimations. In the latter test case we have generated the problems
with a generator in a way that J  (x0 ) was known concerning the constructed problems.
The performance presented in the tables of the section, more precisely the average, E i ,
and the standard deviation, (Ei ), of the error in iteration i were computed as follows
v
u
N
N h
i2
X
u1 X


1
i

Gj  J (x0 ) ,
and
(Ei ) = t
Gji  J  (x0 )  E i ,
Ei =
N
N
j=1

j=1

where Gji denotes the cumulative incurred costs in iteration i of sample j and N is the
sample size. Unless indicated otherwise, the sample contained the results of 100 simulation
trials for each parameter configuration (which is associated with the rows of the tables).
As it was shown in Section 2.5.2, RAP-MDPs are aperiodic, moreover, they have the
APP property, therefore, discounting is not necessary to achieve a well-defined problem.
However, in order to enhance learning, it is still advised to apply discounting and, therefore,
to give less credit to events which are farther from the current decision point. Heuristically,
we suggest applying  = 0.95 for middle-sized RAPs (e.g., with few hundreds of tasks),
such as the problems of the benchmark dataset, and  = 0.99 for large-scale RAPs (e.g.,
with few thousands of tasks), such as the problems of the industry-related experiments.
4.2 Benchmark Datasets
The ADP based resource control approach was tested on Hurinks benchmark dataset
(Hurink, Jurisch, & Thole, 1994). It contains flexible job-shop scheduling problems (FJSPs)
with 630 jobs (30225 tasks) and 515 machines. The applied performance measure is
the maximum completion time of the tasks (makespan). These problems are hard, which
means, e.g., that standard dispatching rules or heuristics perform poorly on them. This
dataset consists of four subsets, each subset contains about 60 problems. The subsets (sdata,
edata, rdata, vdata) differ in the ratio of machine interchangeability (flexibility), which is
477

fiCsaji & Monostori

shown in the flex(ib) columns in Tables 3 and 2. The columns with label n iters (and
avg err) show the average error after carrying out altogether n iterations. The std
dev columns in the tables of the section contain the standard deviation of the sample.
Table 2 illustrates the performance on some typical dataset instances and also gives
some details on them, e.g., the number of machines and jobs (columns with labels mcs
and jbs). In Table 3 the summarized performance on the benchmark datasets is shown.
benchmark configuration
dataset inst mcs jbs flex
sdata
mt06
6
6
1
sdata
mt10
10
10
1
sdata
la09
5
15
1
sdata
la19
10
10
1
sdata
la39
15
15
1
sdata
la40
15
15
1
edata
mt06
6
6 1.15
edata
mt10
10
10 1.15
edata
la09
5
15 1.15
edata
la19
10
10 1.15
edata
la39
15
15 1.15
edata
la40
15
15 1.15
rdata
mt06
6
6
2
rdata
mt10
10
10
2
rdata
la09
5
15
2
rdata
la19
10
10
2
rdata
la39
15
15
2
rdata
la40
15
15
2
vdata
mt06
6
6
3
vdata
mt10
10
10
5
vdata
la09
5
15
2.5
vdata
la19
10
10
5
vdata
la39
15
15
7.5
vdata
la40
15
15
7.5

average error (standard deviation)
1000 iters
5000 iters
10 000 iters
1.79 (1.01) %
0.00 (0.00) %
0.00 (0.00) %
9.63 (4.59) %
8.83 (4.37) %
7.92 (4.05) %
5.67 (2.41) %
3.87 (1.97) %
3.05 (1.69) %
11.65 (5.21) %
6.44 (3.41) %
3.11 (1.74) %
14.61 (7.61) % 12.74 (5.92) % 11.92 (5.63) %
10.98 (5.04) %
8.87 (4.75) %
8.39 (4.33) %
0.00 (0.00) %
0.00 (0.00) %
0.00 (0.00) %
18.14 (8.15) % 12.51 (6.12) %
9.61 (4.67) %
7.51 (3.33) %
5.23 (2.65) %
2.73 (1.89) %
8.04 (4.64) %
4.14 (2.81) %
1.38 (1.02) %
22.80 (9.67) % 17.32 (8.29) % 12.41 (6.54) %
14.78 (7.14) %
8.08 (4.16) %
6.68 (4.01) %
6.03 (3.11) %
0.00 (0.00) %
0.00 (0.00) %
17.21 (8.21) % 12.68 (6.81) %
7.87 (4.21) %
7.08 (3.23) %
6.15 (2.92) %
3.80 (2.17) %
18.03 (8.78) % 11.71 (5.78) %
8.87 (4.38) %
24.55 (9.59) % 18.90 (8.05) % 13.06 (7.14) %
23.90 (7.21) % 18.91 (6.92) % 14.08 (6.68) %
0.00 (0.00) %
0.00 (0.00) %
0.00 (0.00) %
8.76 (4.65) %
4.73 (2.23) %
0.45 (0.34) %
9.92 (5.32) %
7.97 (3.54) %
4.92 (2.60) %
14.42 (7.12) % 11.61 (5.76) %
6.54 (3.14) %
16.16 (7.72) % 12.25 (6.08) %
9.02 (4.48) %
5.86 (3.11) %
4.08 (2.12) %
2.43 (1.83) %

Table 2: Performance (average error and deviation) on some typical benchmark problems.
Simple dispatching rules (which are often applied in practice), such as greedy ones,
perform poorly on these benchmark datasets. Their average error is around 2530 %. In
contrast, Table 3 demonstrates that using our method, the average error is less than 5 %
after 10 000 iterations. It shows that learning is beneficial for this type of problems.
The best performance on these benchmark datasets was achieved by Mastrolilli and
Gambardella (2000). Though, their algorithm performs slightly better than ours, their
solution exploits the (unrealistic) specialties of the dataset, e.g., the durations do not depend
on the resources; the tasks are linearly ordered in the jobs; each job consists of the same
478

fiAdaptive Stochastic Resource Control

number of tasks. Moreover, it cannot be easily generalized to stochastic resource control
problem our algorithm faces. Therefore, the comparison of the solutions is hard.
benchmark
dataset flexib
sdata
1.0
edata
1.2
rdata
2.0
vdata
5.0
average
2.3

1000 iterations
avg err std dev
8.54 %
5.02 %
12.37 %
8.26 %
16.14 %
7.98 %
10.18 %
5.91 %
11.81 %
6.79 %

5000 iterations
avg err std dev
5.69 %
4.61 %
8.03 %
6.12 %
11.41 %
7.37 %
7.73 %
4.73 %
8.21 %
5.70 %

10 000 iterations
avg err std dev
3.57 %
4.43 %
5.26 %
4.92 %
7.14 %
5.38 %
3.49 %
3.56 %
4.86 %
4.57 %

Table 3: Summarized performance (average error and deviation) on benchmark datasets.
4.3 Distributed Sampling
The possible parallelizations of the presented method was also investigated, i.e., the speedup
of the system relative to the number of processors (in practise, the multiprocessor environment was emulated on a single processor, only). The average number of iterations was
studied, until the system could reach a solution with less than 5% error on Hurinks dataset.
The average speed of a single processor was treated as a unit, for comparison.
In Figure 9 two cases are shown: in the first case (rear dark bars) each processor could
access a common global value function. It means that each processor could read and write
the same global value function, but otherwise, they searched (sampled the search space)
independently. Figure 9 demonstrates that in this case the speedup was almost linear.
In the second case (front light bars) each processor had its own (local) value function
(which is more realistic in a strongly distributed system, such as a GRID) and, after the
search had been finished, these individual value functions were compared. Therefore, all of
the processors had estimations of their own, and after the search, the local solution of the
best performing processor was selected. Figure 9 shows the achieved speedup in case we
stopped the simulation if any of the processors achieved a solution with less than 5 % error.

Figure 9: Average speedup relative to the number of processors.
The experiments show that the computation of the resource allocator function can be
effectively distributed, even if there is not a commonly accessible value function available.
479

fiCsaji & Monostori

4.4 Industry Related Tests
We also initiated experiments on a simulated factory by modeling the structure of a real
plant producing customized mass-products, especially, light bulbs. These industrial data
came from a huge national industry-academia project, for research and development of solutions which support manufacturing enterprises in coping with the requirements of adaptiveness, realtimeness and cooperativeness (Monostori, Kis, Kadar, Vancza, & Erdos, 2008).
optimal
slack ratio
50 %
40 %
30 %
20 %
10 %
0%

1000 iterations
avg err std dev
0.00 %
0.00 %
0.12 %
0.10 %
0.52 %
0.71 %
1.43 %
1.67 %
5.28 %
3.81 %
8.89 %
5.17 %

5000 iterations
avg err std dev
0.00 %
0.00 %
0.00 %
0.00 %
0.24 %
0.52 %
1.11 %
1.58 %
4.13 %
3.53 %
7.56 %
5.04 %

10 000 iterations
avg err std dev
0.00 %
0.00 %
0.00 %
0.00 %
0.13 %
0.47 %
1.05 %
1.49 %
3.91 %
3.48 %
6.74 %
4.83 %

Table 4: Summarized performance relative to the optimal slack ratio of the system.
Since, we did not have access to historical data concerning past orders, we used randomly
generated orders (jobs) with random due dates. The tasks and the process-plans of the jobs,
however, covered real products; as well as, the resources covered real machine types. In this
plant the machines require product-type dependent setup times, and there are some special
tasks that have durations but that do not require any resources to be processed, for example,
cooling down. Another feature of the plant is that at some previously given time points
preemptions are allowed, e.g., at the end of a work shift. The applied performance measure
was to minimize the number of late jobs, viz., jobs that are finished after their due dates,
and an additional secondary measure was to minimize the total cumulative lateness, which
can be applied to compare two schedules having the same number of late jobs.
During these experiments the jobs and their due dates were generated by a special
parameterizable generator in a way that optimally none of the jobs were late. Therefore, it
was known that J  (x0 ) = 0 and the error of the algorithm was computed accordingly.
In the first case, shown in Table 4, we applied 16 machines and 100 random jobs, which
altogether contained more than 200 tasks. The convergence properties were studied relative
to the optimal slack ratio. In the deterministic case, e.g., the slack ratio of a solution is
n

1 X B(Ji )  F (Ji )
(%) =
,
n
B(Ji )  A(Ji )
i=1

where n is the number of jobs; A(J) and B(J) denote the release and due date of job J,
respectively; F (J) is the finish time of job J relative to solution %, namely, the latest finish
time of the tasks in the job. Roughly, the slack ratio measures the tightness of the solution,
for example, if (%) > 0, then it shows that the jobs were, on the average, finished before
their due dates and if (%) < 0, then it indicates that, approximately, many jobs were late.
If (%) = 0, then it shows that if all the jobs meet their due dates, each job was finished
480

fiAdaptive Stochastic Resource Control

just in time, there were no spare (residual) times. Under the optimal slack ratio we mean
the maximal achievable slack ratio (by an optimal solution). During the experiments these
values were known because of the special construction of the test problem instances. We
applied the optimal slack ratio to measure how hard a problem is. The first column of
Table 4 shows the optimal slack ratio in percentage, e.g., 30 % means a 0.3 slack ratio.
configuration
machs
tasks
6
30
16
140
25
280
30
560
50
2000
100
10000

1000 iterations
avg err std dev
4.01 %
2.24 %
4.26 %
2.32 %
7.05 %
2.55 %
7.56 %
3.56 %
8.69 %
7.11 %
15.07 % 11.89 %

5000 iterations
avg err std dev
3.03 %
1.92 %
3.28 %
2.12 %
4.14 %
2.16 %
5.96 %
2.47 %
7.24 %
5.08 %
10.31%
7.97 %

10 000 iterations
avg err std dev
2.12 %
1.85 %
2.45 %
1.98 %
3.61 %
2.06 %
4.57 %
2.12 %
6.04 %
4.53 %
9.11 %
7.58 %

Table 5: Summarized performance relative to the number of machines and tasks.
In the second case, shown in Table 5, we have fixed the optimal slack ratio of the
system to 10 % and investigated the convergence speed relative to the plant size (number
of machines) and the number of tasks. In the last two experiments (configuration having
2000 and 10 000 tasks) only 10 samples were generated, because of the long runtime. The
computation of 10 000 iterations took approximately 30 minutes for the 50 machines & 2000
tasks configuration and 3 hours for the 100 machines & 10000 tasks configuration1 .
The results demonstrate that the ADP and adaptive sampling based solution scales well
with both the slack ratio and the size (the number of machines and task) of the problem.
4.5 Clustering Experiments
The effectiveness of clustering on industry-related data was also studied. We considered
a system with 60 resources and 1000 random tasks distributed among 400500 jobs (there
were approximately 10002000 precedence constraints). The tasks were generated in a way
that, in the optimal case, none of them are late and the slack ratio is about 20 %.
First, the tasks were ordered according to their slack times and then they were clustered.
We applied 104 iterations on each cluster. The computational time in case of using only
one cluster was treated as a unit. In Table 6 the average and the standard deviation of the
error and the computational speedup are shown relative to the number tasks in a cluster.
The results demonstrate that partitioning the search space not only results in a greater
speed, but it is often accompanied by better solutions. The latter phenomenon can be explained by the fact that using smaller sample trajectories generates smaller variance that is
preferable for learning. On the other hand, making too small clusters may decrease the performance (e.g., making 50 clusters with 20 tasks in the current case). In our particular case
applying 20 clusters with approximately 50 tasks in each cluster balances good performance
(3.02 % error on the average) with remarkable speedup (approximately 3.28 ).
1. The tests were performed on a Centrino (Core-Duo) 1660Mhz CPU ( P4 3GHz) with 1Gb RAM.

481

fiCsaji & Monostori

configuration
clusters
tasks
1
1000
5
200
10
100
20
50
30
33
40
25
50
20

performance after 10 000 iterations per cluster
late jobs avg error
std dev
speed speedup
28.1
6.88 %
2.38 %
423 s
1.00 
22.7
5.95 %
2.05 %
275 s
1.54 
20.3
4.13 %
1.61 %
189 s
2.24 
13.9
3.02 %
1.54 %
104 s
3.28 
14.4
3.15 %
1.51 %
67 s
6.31 
16.2
3.61 %
1.45 %
49 s
8.63 
18.7
4.03 %
1.43 %
36 s
11.65 

Table 6: Speedup and performance relative to the number of tasks in a cluster.
As clustering the tasks represents a considerable help in dealing with large-scale RAPs,
their further theoretical and experimental investigations are very promising.
4.6 Adaptation to Disturbances
In order to verify the proposed algorithm in changing environments, experiments were
initiated and carried out on random JSPs with the aim of minimizing the makespan. The
adaptive features of the system were tested by confronting it with unexpected events, such
as: resource breakdowns, new resource availability (Figure 10), new job arrivals or job
cancellations (Figure 11). In Figures 10 and 11 the horizontal axis represents time, while
the vertical one, the achieved performance measure. The figures were made by averaging
hundred random samples. In these tests 20 machines were used with few dozens of jobs.
During each test episode there was an unexpected event (disturbance) at time t = 100.
After the change took place, we considered two possibilities: we either restarted the iterative
scheduling process from scratch or continued the learning, using the current (obsolete) value
function. We experienced that the latter approach is much more efficient.
An explanation of this phenomenon can be that the value functions of control policies
and the optimal value function Lipschitz continuously depend on the transition-probability
and the immediate-cost functions of the MDP (Csaji, 2008). Therefore, small changes in
the environmental dynamics cannot cause arbitrary large changes in the value function.
The results of our numerical experiments, shown in Figures 10 and 11, are indicative of the
phenomenon that the average change of the value function is not very large. Consequently,
applying the obsolete value function after a change took place in the MDP is preferable
over restarting the whole optimization process from scratch. This adaptive feature makes
ADP/RL based approaches even more attractive for practical applications.
The results, black curves, show the case when the obsolete value function approximation
was applied after the change took place. The performance which would arise if the system
recomputed the whole schedule from scratch is drawn in gray in part (a) of Figure 10.
One can notice that even if the problem became easier after the change in the environment (at time t = 100), for example, a new resource was available (part (b) of Figure 10)
or a job was cancelled (part (b) of Figure 11), the performance started to slightly decrease
( started to slightly increase) after the event. This phenomenon can be explained by the
fact that even in these special cases the system had to explore the new configuration.
482

fiAdaptive Stochastic Resource Control

Figure 10: The black curves, (t), show the performance measure in case there was a resource breakdown (a) or a new resource availability (b) at t = 100; the gray
curve, (t), demonstrates the case the policy would be recomputed from scratch.

Figure 11: The black curves, (t), show the performance measure during resource control
in case there was a new job arrival (a) or a job cancellation (b) at time t = 100.

5. Concluding Remarks
Efficient allocation of scarce, reusable resources over time in uncertain and dynamic environments is an important problem that arises in many real world domains, such as production
control. The paper took a machine learning (ML) approach to this problem. First, a general
resource allocation framework was presented and, in order to define reactive solutions, it
was reformulated as a stochastic shortest path problem, a special Markov decision process
(MDP). The core idea of the solution was the application of approximate dynamic programming (ADP) and reinforcement learning (RL) techniques with Monte Carlo simulation to
stochastic resource allocation problems (RAPs). Regarding compact value function representations, two approaches were studied: hash table and support vector regression (SVR),
specially -SVRs. Afterwards, several additional improvements, such as the application of
limited-lookahead rollout algorithms in the initial phases, action space decomposition, task
clustering and distributed sampling, were suggested for speeding up the computation of a
good control policy. Finally, the effectiveness of the approach was demonstrated by results
of numerical simulation experiments on both benchmark and industry-related data. These
experiments also supported the adaptive capabilities of the proposed method.
483

fiCsaji & Monostori

There are several advantages why ML based resource allocation is preferable to other
kinds of RAP solutions, e.g., classical approaches. These favorable features are as follows:
1. The presented RAP framework is very general, it can model several resource management problems that appear in practice, such as scheduling problems, transportation
problems, inventory management problems or maintenance and repair problems.
2. ADP/RL based methods essentially face the problem under the presence of uncertainties, since their theoretical foundation is provided by MDPs. Moreover, they can
adapt to unexpected changes in the environmental dynamics, such as breakdowns.
3. Additionally, for most algorithms theoretical guarantees of finding (approximately)
optimal solutions, at least in the limit, are known. As demonstrated by our experiments, the actual convergence speed for RAPs is usually high, especially in the case
of applying the described improvements, such as clustering or distributed sampling.
4. The simulation experiments on industrial data also demonstrate that ADP/RL based
solutions scale well with the workload and the size of the problem and, therefore, they
can be effectively applied to handle real world RAPs, such as production scheduling.
5. Domain specific knowledge can also be incorporated into the solution. The base policy
of the rollout algorithm, for example, can reflect a priori knowledge about the structure
of the problem; later this knowledge may appear in the exploration strategy.
6. Finally, the proposed method constitutes an any-time solution, since the sampling can
be stopped after any number of iterations. By this way, the amount of computational
time can be controlled, which is also an important practical advantage.
Consequently, ML approaches have great potentials in dealing with real world RAPs, since
they can handle large-scale problems even in dynamic and uncertain environments.
Several further research directions are possible. Now, as a conclusion to the paper, we
highlight some of them. The suggested improvements, such as clustering and distributed
sampling, should be further investigated since they resulted in considerable speedup. The
guidance of reinforcement learning with rollout algorithms might be effectively applied in
other applications, as well. The theoretical analysis of the average effects of environmental
changes on the value functions could result in new approaches to handle disturbances.
Another promising direction would be to extend the solution in a way which also takes risk
into account and, e.g., minimizes not only the expected value of the total costs but also the
deviation, as a secondary criterion. Finally, trying to apply the solution in a pilot project
to control a real plant would be interesting and could motivate further research directions.

6. Acknowledgments
The work was supported by the Hungarian Scientific Research Fund (OTKA), Grant No.
T73376, and by the EU-funded project Coll-Plexity, Grant No. 12781 (NEST). Balazs
Csanad Csaji greatly acknowledges the scholarship of the Hungarian Academy of Sciences.
The authors express their thanks to Tamas Kis for his contribution related to the tests on
industrial data and to Csaba Szepesvari for the helpful discussions on machine learning.
484

fiAdaptive Stochastic Resource Control

References
Andrieu, C., Freitas, N. D., Doucet, A., & Jordan, M. I. (2003). An introduction to MCMC
(Markov Chain Monte Carlo) for machine learning. Machine Learning, 50, 543.
Aydin, M. E., & Oztemel, E. (2000). Dynamic job-shop scheduling using reinforcement
learning agents. Robotics and Autonomous Systems, 33, 169178.
Beck, J. C., & Wilson, N. (2007). Proactive algorithms for job shop scheduling with probabilistic durations. Journal of Artificial Intelligence Research, 28, 183232.
Bellman, R. E. (1961). Adaptive Control Processes. Princeton University Press.
Bertsekas, D. P. (2005). Dynamic programming and suboptimal control: A survey from
ADP to MPC. European Journal of Control, 11 (45), 310334.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific,
Belmont, Massachusetts.
Bertsekas, D. P. (2001). Dynamic Programming and Optimal Control (2nd edition). Athena
Scientific, Belmont, Massachusetts.
Bulitko, V., & Lee, G. (2006). Learning in real-time search: A unifying framework. Journal
of Artificial Intelligence Research, 25, 119157.
Chang, C. C., & Lin, C. J. (2001). LIBSVM: A library for support vector machines. Software
available on-line at http://www.csie.ntu.edu.tw/cjlin/libsvm.
Csaji, B. Cs. (2008). Adaptive Resource Control: Machine Learning Approaches to Resource Allocation in Uncertain and Changing Environments. Ph.D. thesis, Faculty of
Informatics, Eotvos Lorand University, Budapest.
Csaji, B. Cs., Kadar, B., & Monostori, L. (2003). Improving multi-agent based scheduling
by neurodynamic programming. In Proceedings of the 1st International Conference on
Holonic and Mult-Agent Systems for Manufacturing, September 13, Prague, Czech
Republic, Vol. 2744 of Lecture Notes in Artificial Intelligence, pp. 110123.
Csaji, B. Cs., & Monostori, L. (2006). Adaptive sampling based large-scale stochastic resource control. In Proceedings of the 21st National Conference on Artificial Intelligence
(AAAI 2006), July 1620, Boston, Massachusetts, pp. 815820.
Dolgov, D. A., & Durfee, E. H. (2006). Resource allocation among agents with MDP-induced
preferences. Journal of Artificial Intelligence Research, 27, 505549.
Even-Dar, E., & Mansour, Y. (2003). Learning rates for Q-learning. Journal of Machine
Learning Research, 5, 125.
Feinberg, E. A., & Shwartz, A. (Eds.). (2002). Handbook of Markov Decision Processes:
Methods and Applications. Kluwer Academic Publishers.
Gersmann, K., & Hammer, B. (2005). Improving iterative repair strategies for scheduling
with the SVM. Neurocomputing, 63, 271292.
Hastings, W. K. (1970). Monte Carlo sampling methods using Markov chains and their
application. Biometrika, 57, 97109.
485

fiCsaji & Monostori

Hatvany, J., & Nemes, L. (1978). Intelligent manufacturing systems - a tentative forecast.
In Niemi, A. (Ed.), A link between science and applications of automatic control;
Proceedings of the 7th IFAC World Congress, Vol. 2, pp. 895899.
Hurink, E., Jurisch, B., & Thole, M. (1994). Tabu search for the job shop scheduling problem
with multi-purpose machines. Operations Research Spektrum, 15, 205215.
Kirkpatrick, S., Gelatt, C. D., & Vecchi, M. P. (1983). Optimization by simulated annealing.
Science, 220 (4598), 671680.
Mastrolilli, M., & Gambardella, L. M. (2000). Effective neighborhood functions for the
flexible job shop problem. Journal of Scheduling, 3 (1), 320.
Metropolis, N., Rosenbluth, A., Rosenbluth, M., Teller, A., & Teller, E. (1953). Equation
of state calculations by fast computing machines. Journal of Chemical Physics, 21,
10871092.
Monostori, L., Kis, T., Kadar, B., Vancza, J., & Erdos, G. (2008). Real-time cooperative enterprises for mass-customized production. International Journal of Computer
Integrated Manufacturing, (to appear).
Papadimitriou, C. H. (1994). Computational Complexity. Addison-Wesley.
Pinedo, M. (2002). Scheduling: Theory, Algorithms, and Systems. Prentice-Hall.
Powell, W. B., & Van Roy, B. (2004). Handbook of Learning and Approximate Dynamic
Programming, chap. Approximate Dynamic Programming for High-Dimensional Resource Allocation Problems, pp. 261283. IEEE Press, Wiley-Interscience.
Riedmiller, S., & Riedmiller, M. (1999). A neural reinforcement learning approach to learn
local dispatching policies in production scheduling. In Proceedings of the 16th International Joint Conference on Artificial Intelligence, Stockholm, Sweden, pp. 764771.
Schneider, J. G., Boyan, J. A., & Moore, A. W. (1998). Value function based production
scheduling. In Proceedings of the 15th International Conference on Machine Learning,
pp. 522530. Morgan Kaufmann, San Francisco, California.
Scholkopf, B., Smola, A., Williamson, R. C., & Bartlett, P. L. (2000). New support vector
algorithms. Neural Computation, 12, 12071245.
Singh, S., Jaakkola, T., Littman, M., & Szepesvari, Cs. (2000). Convergence results for
single-step on-policy reinforcement-learning algorithms. Machine Learning, 38 (3),
287308.
Sontag, E. D. (1998). Mathematical Control Theory: Deterministic Finite Dimensional
Systems. Springer, New York.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement learning. The MIT Press.
Topaloglu, H., & Powell, W. B. (2005). A distributed decision-making structure for dynamic resource allocation using nonlinear function approximators. Operations Research, 53 (2), 281297.
Zhang, W., & Dietterich, T. (1995). A reinforcement learning approach to job-shop scheduling. In Proceedings of the 14th International Joint Conference on Artificial Intelligence, pp. 11141120. Morgan Kauffman.

486

fiJournal of Artificial Intelligence Research 32 (2008) 607629

Submitted 01/08; published 06/08

A Unifying Framework for Structural Properties of CSPs:
Definitions, Complexity, Tractability
Lucas Bordeaux

lucasb@microsoft.com

Microsoft Research
7 J J Thomson Avenue
Cambridge, CB3 0FB, United Kingdom

Marco Cadoli
Toni Mancini

cadoli@dis.uniroma1.it
tmancini@dis.uniroma1.it

Dipartimento di Informatica e Sistemistica
Sapienza Universita di Roma
Via Ariosto 25, I-00185 Roma, Italy

Abstract
Literature on Constraint Satisfaction exhibits the definition of several structural properties that can be possessed by CSPs, like (in)consistency, substitutability or interchangeability. Current tools for constraint solving typically detect such properties efficiently by
means of incomplete yet effective algorithms, and use them to reduce the search space and
boost search.
In this paper, we provide a unifying framework encompassing most of the properties known
so far, both in CSP and other fields literature, and shed light on the semantical relationships among them. This gives a unified and comprehensive view of the topic, allows new,
unknown, properties to emerge, and clarifies the computational complexity of the various
detection problems.
In particular, among the others, two new concepts, fixability and removability emerge, that
come out to be the ideal characterisations of values that may be safely assigned or removed from a variables domain, while preserving problem satisfiability. These two notions
subsume a large number of known properties, including inconsistency, substitutability and
others.
Because of the computational intractability of all the property-detection problems, by following the CSP approach we then determine a number of relaxations which provide sufficient conditions for their tractability. In particular, we exploit forms of language restrictions
and local reasoning.

1. Introduction
Many Constraint Satisfaction Problems (CSPs) which arise in the modelling of real-life
applications exhibit structural properties that distinguish them from random instances.
Detecting such properties has been widely recognised to be an effective way for improving
the solving process. To this end, several of them have already been identified, and different
techniques have been developed in order to exploit them, with the goal of reducing the
search space to be explored. Good examples are value inconsistency (Mackworth, 1977;
Montanari, 1974), substitutability and interchangeability (Freuder, 1991), more general
c
2008
AI Access Foundation. All rights reserved.

fiBordeaux, Cadoli, & Mancini

forms of symmetries (Crawford, Ginsberg, Luks, & Roy, 1996; Gent & Smith, 2000), and
functional dependencies among variables (Li, 2000; Mancini & Cadoli, 2007).
Unfortunately, checking whether such properties hold, is (or is thought to be) often
computationally hard. As an example, let us consider interchangeability. Value a is said
to be interchangeable with value b for variable x if every solution which assigns a to x
remains a solution if x is changed to b, and vice versa (Freuder, 1991). The problem of
checking interchangeability is coNP-complete (cf. Proposition 4). Analogously, detecting
some other forms of symmetry reduces to the graph automorphism problem (Crawford,
1992) (for which there is no known polynomial time algorithm, even if there is evidence
that it is not NP-complete, Kobler, Schoning, & Toran, 1993).
To this end, in order to allow general algorithms to exploit such properties efficiently,
different approaches can be followed. First of all, syntactic restrictions on the constraint
languages can be enforced, in order to allow the efficient verification of the properties of
interest. Alternatively, local versions of such properties can be defined, that can be
used to infer their global counterparts, and that can be verified in polynomial time. As
an instance of this local reasoning approach, instead of checking whether a value is fully
interchangeable for a variable, Freuder (1991) proposes to check whether that value is neighbourhood, or k-interchangeable. This task involves considering only bounded-sized subsets
of the constraints, and hence can be performed in polynomial time. Neighbourhood and
k-interchangeability are sufficient (but not necessary) conditions for full interchangeability,
and have been proven to be highly effective in practice (cf., e.g., Choueiry & Noubir, 1998;
Lal, Choueiry, & Freuder, 2005).
In this paper we give a formal characterisation of several properties of CSPs which can be
exploited in order to save search, and present a unifying framework for them that allows for
their semantical connections to emerge. Some of the properties are well-known in the Constraint Programming literature, others have been used in other contexts (as in databases),
while some others are, to the best of our knowledge, original, and, in our opinion, play
a key role in allowing a deep understanding of the topic. In particular, we reconsider the
notions of inconsistency, substitutability and interchangeability, and propose those of fixable,
removable, and implied value for a given variable, which are instances of the more general
definition of satisfiability-preserving transformation and those of determined, dependent, and
irrelevant variable. These properties make it possible to transform a problem into a simpler
one. Depending on the case, this transformation is guaranteed to preserve all solutions, or
the satisfiability of the problem, i.e., at least a solution, if one exists. In general, each of
these properties can be detected either statically, during a preprocessing stage of the input
CSP (cf., e.g., Cadoli & Mancini, 2007), or dynamically, during search (since they may arise
at any time). Moreover, in some cases we dont even need to explicitly verify whether some
properties hold, because this is guaranteed by the intrinsic characteristics of the problem.
For instance, some problems are guaranteed to have a unique solution. Such cases are referred to as promise problems in the literature (Even, Selman, & Yacobi, 1984), meaning
that in addition to the problem description we are informed of certain properties it verifies
(cf. forthcoming Example 1 for an example).
The formal characterisation of the properties and their connections allow us to shed light
on the computational complexity of their recognition task in a very elegant way, proving
608

fiA Unifying Framework for Structural Properties of CSPs

that, in the worst case, the detection of each of them is as complex as the original problem.
In particular, as we will see in Section 3.1, detecting each of the proposed properties is a
coNP-complete task. This holds also for Freuders substitutability and interchangeability
(this result is, to the best of our knowledge, original). Hence, in order to be able to
practically make the relevant checks during preprocessing and search, we investigate two
different approaches for the efficient verification of the proposed properties: additions of
suitable restrictions to the constraint language, and exploitation of efficient forms of local
reasoning, i.e., by checking them for single constraints.
The outline of the paper is as follows: after giving an intuitive example and recalling
some preliminaries, in Section 2 we present the properties we are interested in, and discuss
their connections. Then, in Section 3 we focus on the complexity of the various propertydetection tasks. In particular, in Section 3.1 we prove that all of them are intractable; hence,
in Section 3.2 we focus on relaxations that guarantee tractability of reasoning, investigating
the two aforementioned approaches. Finally, in Section 4 we draw conclusions and address
future work.

2. A Hierarchy of Properties
In this section, we formally define several structural properties of CSPs, discuss the semantical relationships that hold among them, and show how they can be exploited during
constraint solving.
2.1 An Intuitive Example
In order to allow for a gentle introduction of the main properties investigated in the forthcoming sections, we first introduce the following example.
Example 1 (Factoring, Lenstra & Lenstra, 1990; Pyhala, 2004). This problem is a simplified version of one of the most important problems in public-key cryptography. Given a
(large) positive integer Z and the fact that it is a product of two different unknown prime
numbers X and Y (different from 1), the goal is to identify these two numbers.
An intuitive formulation of any instance of this problem (i.e., for any given Z) as a
CSP, adequate for arbitrarily large numbers, amounts to encode the combinatorial circuit of
integer multiplication, and is as follows: assuming Z has n digits (in base b) z1 , . . . , zn , we
consider 2n variables x1 , . . . , xn and y1 , . . . , yn one for each digit (in base b) of X and Y
(x1 and y1 being the least significant ones). The domain for all these variables is [0, b  1].
In order to maintain information about the carries, n + 1 additional variables c1 , . . . , cn+1
must be considered, with domain [0..(b  1)2 n/b].1
As for the constraints (cf. Figure 1 for the intuition, where x4 , x5 , x6 , y4 , y5 , y6 are
equal to 0, and are omitted for readability), they are the following:
1. Constraints on factors:
1. In this intuitive example, with a little abuse with respect to what will be permitted by forthcoming
Definition 1, we allow, to enhance readability, different variables to be defined over different domains.
However, we observe that it is easy to recover from this by using standard and well-known techniques
(e.g., adding new monadic constraints to model smaller domains).

609

fiBordeaux, Cadoli, & Mancini

7 8 7 
7 9 7=
0 6 13 18 12

4

0

49
63 72
49 56 49
6 2 7 2

56
63

3

49


9

c7

c6

c5

x3
y3

x2
y2

x1 
y1 =

c3

c2

c1

c4

x3 y1 x2 y1 x1 y1
x3 y2 x2 y2 x1 y2

x3 y3 x2 y3 x1 y3


z6 z5 z4 z3 z2 z1

Figure 1: Factoring instance 627239, n = 6, b = 10

(a) Factors must be different from 1, or, equivalently, X 6= Z and Y 6= Z must hold;
P
(b) For every digit i  [1, n]: zi = ci + j,k[1,n]:j+k=i+1 (xj  yk mod b);
2. Constraints on carries:
(a) Carry on the least significant digit is 0: c1 = 0;
(b) Carries on other digits: i  [2, n + 1], ci = ci1 +

P

j,k[1,n]:j+k=i

xj yk
b ;

(c) Carry on the most significant digit is 0: cn+1 = 0.

When a guess on the two factors X and Y (i.e., on variables x1 , . . . , xn and y1 , . . . , yn ) has
been made, values for variables c1 , . . . , cn+1 are completely determined, since they follow
from the semantics of the multiplication. This is called a functional dependence among
variables.
Functional dependencies arise very often in, e.g., problems for which an intermediate
state has to be maintained, and their detection and exploitation has been recognized to be of
great importance from an efficiency point of view, since it can lead to significant reductions
of the search space (cf., e.g., Giunchiglia, Massarotto, & Sebastiani, 1998; Mancini & Cadoli,
2007; Cadoli & Mancini, 2007).
Moreover, the presence of functional dependencies among variables of a CSP highlights
a second interesting problem, i.e., that of computing the values of dependent variables after
a choice of the defining ones has been made. This problem, which is always a subproblem of
a CSP with dependencies, has exactly one solution, hence, the knowledge of such a promise
can be useful to the solver. It is worth noting that there are also problems which intrinsically
exhibit promises. This is the case of, e.g., Factoring where we add the symmetry-breaking
constraint forcing x1 , . . . , xn to be lexicographically less than or equal to y1 , . . . , yn . This
new formulation is guaranteed to have exactly one solution (since both X and Y are prime).
The Factoring problem exhibits also other interesting properties: let us consider an
instance such that Z is given in binary notation (i.e., b = 2) and with the least significant
digit z1 being equal to 1. This implies that the last digit of both factors X and Y must be 1.
Hence, we can say that value 1 is implied for variables x1 and y1 , and that 0 is removable for
them and, more precisely inconsistent. Moreover, for this problem, which, if the symmetry
is broken, has a unique solution, we also know that all variables x1 , . . . , xn and y1 , . . . , yn
610

fiA Unifying Framework for Structural Properties of CSPs

are determined (cf. forthcoming Definition 2), regardless of the instance, and because of
the functional dependence already discussed, we have that variables encoding carries, i.e.,
ci (i  [1, n]), are dependent on {x1 , . . . , xn , y1 , . . . , yn }.
As for problems with unique solutions, it is known that, unfortunately, their resolution remains intractable (cf., e.g., Papadimitriou, 1994; Valiant & Vijay V. Vazirani, 1986; Calabro,
Impagliazzo, Kabanets, & Paturi, 2003). However, this does not exclude the possibility to
find good heuristics for instances with such a promise, or to look for other properties that
are implied by the existence of unique solutions, that can be exploited in order to improve
the search process. In particular, determined and implied values play an important role in
this and other classes of problems. As the Factoring example shows, such problems arise
frequently in practice, either as subproblems of other CSPs, as in presence of functional dependencies (cf. also Mancini & Cadoli, 2007; Mancini, Cadoli, Micaletto, & Patrizi, 2008, for
more examples), or because of intrinsic characteristics of the problem at hand. In general,
if a problem has a unique solution, all variables have a determined value.
Another central role is played by the removability property, that characterises precisely
the case when a value can be safely removed from the domain of a variable, while preserving
satisfiability. This property is of course weaker than inconsistency (since some solutions may
be lost), but can be safely used in place of it when we are interested in finding only a solution
of a CSP, if one exists, and not all of them.
2.2 Preliminaries
Definition 1 (Constraint Satisfaction Problem (CSP), Dechter, 1992). A Constraint Satisfaction Problem is a triple hX, D, Ci where:
 X is a finite set of variables;
 D is a finite set of values, representing the domain for each variable;
 C is a finite set of constraints {c1 , . . . c|C| }, with each of them being of the form
ci = ri (Vi ), where Vi is a list of k  |X| variables in X (the constraint scope), and
ri  Dk is a k-ary relation over D (the constraint relation). We sometimes will
denote the set Vi of variables of constraint ci by var(ci ).

Given a set of variables V and a domain D, a V -tuple t over D is a mapping which
associates a value tx  D to every x  V . Such value is called the x-component of t. Given
a V -tuple t and a subset U  V of its variables, we denote by t|U the restriction of t to
U , which has the same value as t on the variables of U and is undefined elsewhere. The
explicit assignment of the value a  D to the x-component of a V -tuple t (x  V ) is written
t[x := a].
Given a CSP hX, D, Ci, an X-tuple t satisfies a constraint ci =
T ri (Vi )  C if t|Vi  ri .
We denote by Sol(ci ) the set of X-tuples which satisfy ci . The set cC Sol(c) of X-tuples
which satisfy all the constraints is called the solution space, and denoted Sol(C).
By solving a CSP we mean to decide whether the set Sol(C) is non-empty and, if so,
compute one (or all) the solutions.
611

fiBordeaux, Cadoli, & Mancini

'$
'$

1

2

'$ '$
PP
&%
PP
...
&%
P
..

4

...
...
...

5

...
...
"
...
" &% &%
'$
.
"
"
"

3

&%

Figure 2: A graph to be 3-colored

The set of X-tuples t over D is called the search space and denoted by SD , or simply S
if the domain is implicit from the context. The relational operators of selection, projection
and complement will be useful: given a V -relation c, a subset U of V and a value a  D,
we denote by x=a (ci ) (resp. x6=a (ci )) the V -relation which contains the tuples of ci whose
value on x is a (resp. is different from a), by U (ci ) the set of restrictions to U of tuples of
ci (i.e., the set of U -tuples {t | t0  ci (t = t0 |U )}) and by ci the set of V -tuples {t | t 6 ci }.
Note that x=a (S) denotes the search space obtained by fixing the value of x to a. For
the sake of simplicity, the sets X and C will be considered as globally defined and shall
therefore be omitted from the parameters of most definitions; only the search space will be
explicitly mentioned.
Example 2. Consider the CSP hX, D, Ci modeling the 3-coloring problem for the graph in
Figure 2. We have that:
 X = {x1 , . . . , x5 } is the set of variables (one for each node),
 D = {R, G, B} is the set of colors, and
 C is the following finite set of constraints, one for each edge:
C = {N E(x2 , x3 ), N E(x3 , x4 ), N E(x2 , x4 ), N E(x4 , x5 )},
where N E (not-equal) is a binary relation defined as
({R, G, B}  {R, G, B}) \ {hR, Ri, hG, Gi, hB, Bi}.

2.3 Definitions
In this section, we formally present the properties already introduced in Section 2.1, and
show their applicability on some examples.
Definition 2. The following properties are defined for a search space S, variables x and y,
values a and b, and for a set of variables V :
612

fiA Unifying Framework for Structural Properties of CSPs

fixable(S, x, a) 
substitutable(S, x, a, b) 

t  S (t  Sol(C)  t[x := a]  Sol(C))
t  S



tx = a  t  Sol(C) 
t[x := b]  Sol(C)



tx = a  t  Sol(C) 
b 6= a (t[x := b]  Sol(C))

removable(S, x, a) 

t  S

inconsistent(S, x, a) 

t  S

t  Sol(C)  tx 6= a



implied(S, x, a) 

t  S

t  Sol(C)  tx = a



determined(S, x) 

t  S





t  Sol(C) 
b 6= tx (t[x := b] 6 Sol(C))







t  Sol(C)
  ty = t0y 
t, t0  S  t0  Sol(C)
0
x  V (tx = tx )


dependent(S, V, y) 

irrelevant(S, x) 

t  S



t  Sol(C) 
a  D (t[x := a]  Sol(C))



As for interchangeability, it is well-known (Freuder, 1991) that it can be defined in terms
of substitutability:
interchangeable(S, x, a, b) 

substitutable(S, x, a, b)  substitutable(S, x, b, a)

In the few cases where an ambiguity arises on the considered set of constraints, we will
indicate it using subscript (e.g., irrelevantC (S, x)). Note that all the definitions but the
last three are value-oriented, in that they are properties of specific values of the domain.
On the contrary, determinacy, irrelevance, and dependence are variable-oriented properties
which do not directly express results on particular values of the domains but have important
relations with the value-oriented notions (cf. forthcoming Section 2.4).
As already claimed in Section 1, some of the properties of Definition 2 are already
known, as well as their beneficial effects to search. In particular, the notion of consistency
was proposed by Montanari (1974) and Mackworth (1977), and is one of the best-studied
notions in CSP. Substitutability and interchangeability were introduced by Freuder (1991).
Implied values, which are also known in the literature as backbones, were seemingly first
studied explicitly by Monasson, Zecchina, Kirkpatrick, Selman, and Troyansky (1999). To
the best of our knowledge, the notions of removable and fixable values (which, as we show
in Section 2.4 play a key role in the unifying framework proposed in this paper) have on
the contrary not been considered. Determined, irrelevant and dependent variables have
been studied in a number of contexts as logic, SAT, and databases, cf., e.g., Beth definability (Chang & Keisler, 1990), dont care variables in propositional formulae (Thiffault,
613

fiBordeaux, Cadoli, & Mancini

Bacchus, & Walsh, 2004; Safarpour, Veneris, Drechsler, & Lee, 2004), and the Audit problem (Jonsson & Krokhin, 2008), but we are aware of little work concerning their application
in the context of CSPs.
The following examples illustrate some of the properties.
Example 3 (Example 2 continued). Consider a CSP modeling the coloring problem for the
graph in Figure 2. Let  denote the search space in which all five variables x1 , . . . , x5 have
domain {R, G, B}. The following properties hold:
 fixable(,x1 ,R), since for every solution t, t[x1 := R] is a solution as well;
 substitutable(,x1 ,R,G), since for every solution t such that tx1 = R, t[x1 := G] is a
solution as well;
 interchangeable(,x1 ,R,G), from
substitutable(,x1 ,G,R) also holds;

the

previous

point

and

the

fact

that

 removable(,x1 ,G), because for every solution t such that tx1 = G, there exists a
different color K  {R, B} for x1 such that t[x1 := K] is a solution as well;
 irrelevant(,x1 ), because we can actually replace the x1 -component of any solution t
with any other value, since x1 denotes a disconnected node of the graph.
The above properties holding for variable x1 that encodes the disconnected node give some
initial suggestions on the relationships that exist among the different notions. As for the
other nodes, we have, for example:
 removable(,x5 ,G), because for every solution t such that tx5 = G, there exists a
different color K  {R, B} for x5 such that t[x5 := K] is a solution as well. This is
due to the fact that node 5 is connected only to node 4.
As another, more complex, example, consider the following:
Example 4. Let a CSP be given over boolean variables x, y, z, w, p, q, r, whose constraints
are encoded by the formula below:
x  (x  y)  ((z  w)  p)  ((z  y)  (q  r))
Denoting as  the search space in which all variables range over {true, false}, we have,
among the others:
 inconsistent(,x,false),

 determined(,y),

 fixable(,x,true),

 dependent(,{z, w},p),

 implied(,x,true),

 dependent(,{z, y},q),

 implied(,y,true),

 dependent(,{z, y},r),

 inconsistent(,y,false),

 fixable(,q,true),
614

fiA Unifying Framework for Structural Properties of CSPs

 implied(,q,true),

 implied(,r,true).

 fixable(,r,true),
Some of the definitions of Definition 2 can be used to construct solution-preserving transformations, i.e., mappings which transform solutions into solutions.
Definition 3 (solution-preserving transformation). A solution-preserving transformation
is a total mapping  from S to S such that
t  S (t  Sol(C)   (t)  Sol(C))
To understand the connection between solution-preserving transformations and the aforementioned properties, consider the following mappings:
1 (t) = t[x := a]
(
t[x := b]
2 (t) =
t



t[x := b] if tx = a
3 (t) = t[x := a] if tx = b


t
otherwise

if tx = a
otherwise

Checking whether value a is fixable for variable x, whether value a is substitutable to value
b for variable x, and whether values a and b are interchangeable for variable x amounts to
check whether mappings 1 , 2 and 3 (respectively) are solution-preserving.
Solution-preserving transformations are interesting because they allow us to remove values from the search space while preserving the satisfiability of the problem. Moreover, the
correspondence between some properties and the existence of particular solution-preserving
mappings shows that interesting connections hold among these properties and other concepts, like symmetries. As an example, Mancini and Cadoli (2005) give a logical characterisation of symmetries in problem specifications, which is very similar to, and in fact stronger
than, that of Definition 3. In addition, more general forms of solution-preserving transformations could be defined, that, e.g., allow also for the modification of the constraints, i.e.,
as pairs (, ) such that t  S (t  Sol(C)   (t)  Sol((t, C))). This interesting topic,
that may lead to the definition of further and more general properties of a CSP, is left for
future research.
2.4 Semantical Relationships
As already observed (cf. also Examples 3 and 4), several semantical relationships exist
among the notions presented in Definition 2, some of which appear weaker, while some
others stronger. The main connections are clarified by the following theorem.
Theorem 1. The relationships shown in Figure 3 hold between the properties of Definition 2.
Proof.
Dependence-determinacy. We have dependent(S, {x1 , . . . , xi }, y) iff every solution t has a
value on y which is given by a function f of the values it assigns to x1 . . . xi , iff in any search
615

fiBordeaux, Cadoli, & Mancini

irrelevance

dependence

dependent(S, {x1 , . . . , xi }, y) 
a1  D . . . ai  D determined(x1 =a1 ,...,xi =ai (S), y)
determinacy
irrelevant(S, x) 
a  D fixable(S, x, a)

determined(S, x) 
b  D implied(S, x, b)

implication
implied(S, x, b) 
fixable(S, x, b)

implied(S, x, a) 
b  D \ {a} inconsistent(S, x, b)

fixability

inconsistency

fixable(S, x, b) 
a  D substitutable(S, x, a, b)

inconsistent(S, x, a) 
b  D substitutable(S, x, a, b)

substitutability

inconsistent(S, x, a) 
removable(S, x, a)
removable(S, x, a) 
b  D \ {a} substitutable(S, x, a, b)
removability

Figure 3: Semantical relationships among the properties.

space x1 =a1 ,...,xi =ai (S) (where all these variables receive a fixed value), all solutions assign
the same value f (a1 , . . . , an ) to y.
Irrelevance-fixability. t  Sol(C)  a  D (t[x := a]  Sol(C)) rewrites to a  D (t 
Sol(C)  t[x := a]  Sol(C)).
Determinacy-implication. If implied(S, x, b) holds for some b, then for each t and a 6= b we
have t[x := a] 6 Sol(C).
Implication-fixability. implied(S, x, b) means that every t  Sol(C) has tx = b. Hence for
every t  Sol(C), we have t[x := b] = t  Sol(C).
Implication-inconsistency. implied(S, x, a) holds iff t (tx 6= a  t 6 Sol(C)), i.e., iff
t b  D \ {a} (tx = b  t 6 Sol(C)). This rewrites to b  D \ {a} inconsistent(S, x, b).
V
Fixability-substitutability. Let D = {a1 , .., ad }. We have i1..d substitutable(S, x, ai , b) iff
t ((tx = a1      tx = ad )  t  Sol(C)  t[x := b]  Sol(C)), which rewrites to
fixable(S, x, b).
Inconsistency-substitutability. Suppose inconsistent(S, x, a) holds. No solution t with tx = a
exists, hence the implication tx = a  t  Sol(C)  t[x := b]  Sol(C) is true for all choices
of b.
Inconsistency-removability. Same argument as for inconsistency-substitutability.
Substitutability-removability. Suppose substitutable(S, x, a, b) holds for some value b 6= a.
This can be written b t (tx = a  t  Sol(C)  t[x := b]  Sol(C)), which implies
t b(tx = a  t  Sol(C)  t[x := b]  Sol(C)). The latter rewrites to t (tx = a  t 
Sol(C)  b t[x := b]  Sol(C)).
616

fiA Unifying Framework for Structural Properties of CSPs

Note also that implied values and determined variables are strongly related to problems
with a unique solution: if a problem has a unique solution, then all its variables have an
implied value (cf. Example 1), hence they are determined.
2.5 Exploiting Properties in Constraint Solving
An important reason why the aforementioned properties are interesting is that, when detected, they allow us to reduce the search space by removing values from the active domains
of the variables. In particular, several properties have successfully been exploited for this
purpose, like inconsistency (Montanari, 1974; Mackworth, 1977), substitutability (Freuder,
1991), irrelevance (Thiffault et al., 2004; Safarpour et al., 2004), implication (Monasson
et al., 1999), dependence (Mancini & Cadoli, 2007). However, thanks to the unifying framework of Figure 3, we can now show that the wide interest in the aforementioned properties
essentially relies in their relations with the two fundamental properties of removability and
fixability.
Theorem 2. Let  be a CSP hX, D, Ci. If value a is fixable for variable x  X, then  is
satisfiable if and only if the CSP  0 = hX, D, C  {x = a}i obtained from  by instantiating
variable x to value a is satisfiable.
Proof. Assume that  0 is satisfiable. Then there exists a X-tuple t that satisfies all the
constraints. Since the constraints of  0 are a superset of those of , t is also a solution of .
As for the other direction, assume that  is satisfiable. Then there exists a solution
t. Since value a if fixable for x, it follows that t[x := a] is a solution as well. t[x := a]
satisfies also the additional constraint x = a of  0 , hence the latter problem is satisfiable
as well.
Theorem 3. Let  be a CSP hX, D, Ci. If value a is removable for variable x  X, then 
is satisfiable if and only if the CSP  0 = hX, D, C  {x 6= a}i obtained from  by removing
value a from the domain of variable x is satisfiable.
Proof. If  0 is satisfiable, then, by the same arguments of the proof of Theorem 2,  is
satisfiable as well.
As for the other direction, assume that  is satisfiable. Then there exists a solution t.
If tx 6= a, t is of course also a solution of  0 . On the other hand, if tx = a, since value a is
removable for x in , it follows that there exists b 6= a such that t[x := b] is a solution as
well. t[x := b] satisfies also the additional constraint x 6= a of  0 , hence the latter problem
is also satisfiable.
The above results show the key roles played by fixability and removability. They are
the ideal properties that should be checked in order to reduce the domain for a variable.
The interest in the other notions essentially relies on their relationships with fixability and
removability. As an example, an implied value is of interest essentially because it is fixable,
an irrelevant variable is of interest essentially because it is fixable to any value of its domain,
a substitutable value is of interest essentially because it is removable, etc.
Also the interest in inconsistent values relies on the fact that they are removable. However, inconsistency is much stronger that removability, because removing inconsistent values
617

fiBordeaux, Cadoli, & Mancini

guarantees that all solutions (and not only the satisfiability of the problem) are preserved.
Hence removability plays exactly the same role of inconsistency in case we do not want to
find all solutions of a problem but simply want to find one. In such situations removability
is the ideal property to use.
As for the other properties, it is worth noting that, although they may appear very strong
and unlikely at a first sight, they can still play a precious role when detected dynamically
during search. As an example, Thiffault et al. (2004) show that dynamically detecting
variables that become irrelevant during search (called dont care variables in the paper) can
greatly speed-up non-CNF SAT solvers, by actually separating problems into independent
components.

3. Complexity of reasoning
In this section we show that the problem of checking whether properties of Definition 2 hold
is coNP-complete. Hence, in Section 3.2, we try to determine special cases where checking
can be done efficiently (i.e., in polynomial time).
3.1 Intractability Results
From now on, we assume that the input is given as a set of constraints C over a set of
variables X. We also assume that the problem of checking whether t  Sol(C) is polynomial
in the size of the representation of the input. Such properties hold for propositional logic
and for CSPs, in the sense of Dechter (1992).
We note that the problem of checking whether properties of Definition 2 hold is in
coNP, because, for each of them it is possible to find a counter-example by guessing a tuple
in S (two, in case of dependency) in non-deterministic polynomial time, and checking, in
polynomial time, whether the negation of the subformula between parentheses holds (as
for interchangeability, we note that the logical and of two properties in coNP is still in
coNP). Alternatively, coNP-membership follows observing that succinct certificates exist
proving that the various properties do not hold (as an example, a certificate proving that
variable x is not fixable to a is a V -tuple t  Sol(C) such that t[x := a] 6 Sol(C)). In the
rest of this section, proofs are therefore restricted to the coNP-hardness part.
Theorem 4 (coNP-completeness of properties of Definition 2). Given a CSP, the following
tasks are coNP-complete:
 Checking whether value a is fixable, removable, inconsistent, implied, or determined
for variable x  X;
 Checking whether value a  D is substitutable to, or interchangeable with value b  D
for variable x  X;
 Checking whether variable y  X is dependent on variables in a set V  X;
 Checking whether variable x  X is irrelevant.
Proof. To prove that checking such properties is hard for coNP, we reduce a coNP-complete
problem, i.e., that of checking that an arbitrary CSP is unsatisfiable, to the problem of
618

fiA Unifying Framework for Structural Properties of CSPs

checking these properties. In particular, the proofs hold even if the domain is boolean,
in which case the CSP can be written as a propositional formula , e.g., in CNF. Hence,
let  be an arbitrary propositional formula in CNF over set of variables X, and let x be
a variable such that x 6 X: the unsatisfiability problem of  can be reduced into the
problem of checking the various properties. Moreover, the semantical relationships defined
in Theorem 1 allow to infer in an elegant way hardness results for several properties starting
from those of the others.
Irrelevance. Consider  defined as   x.  is unsatisfiable if and only if  is unsatisfiable.
We now show that  is unsatisfiable if and only if x is irrelevant in formula . Let us first
assume that  is unsatisfiable. It follows that x is irrelevant in , because  has no models.
On the other hand, let M be a model of . Interpretation M  {x  true} is a model of ,
while M  {x  false} is not, implying that x is not irrelevant in .
Fixability, Substitutability. Results follow from that of irrelevance, combined with the semantical relationships that define irrelevance in terms of fixability, and fixability in terms
of substitutability.
Dependence. Consider  defined as   (x  x).  is unsatisfiable if and only if  is
unsatisfiable, and that x is dependent on X in  if and only if  is unsatisfiable.
Determinacy. The result follows from that of dependence, combined with the semantical
relationship that defines dependence in terms of determinacy.
Implication. Consider  defined as   x. Value false is implied for x in  if and only if 
is unsatisfiable.
Inconsistency. The result follows from that of implication, combined with the semantical
relationship that defines implication in terms of inconsistency.
Removability. Consider  defined as   x. Value true is removable for x in  if and only
if  is unsatisfiable.
From the proof of Theorem 4, it can be observed that the intractability of checking each of
the properties holds also for binary CSPs (i.e., CSPs in which all constraints relate at most
two variables).
Theorem 5 (coNP-completeness of properties of Definition 2 for binary CSPs). Given
a CSP with only binary constraints on a domain of size greater than two, checking the
properties of Definition 2 is coNP-complete.
Proof. We give the proof for irrelevance only: the others can be derived similarly.
Let  = hX, D, Ci be a binary CSP. Consider an arbitrary variable x 6 X and let a and b
be arbitrary distinct values in D. Let  denote the CSP hX 0 , D, C 0 i with X 0 = X {x}, and
C 0 = C  {x = a}.  is binary and, similarly to the proof of Theorem 4,  is unsatisfiable
if and only if variable x is irrelevant for .
From the observation that a CSP encoding of the graph 3-colourability problem can be
made using only binary constraints, the thesis follows, since checking unsatisfiability of this
problem (which is coNP-hard) can be reduced to checking irrelevance in a binary CSP.
619

fiBordeaux, Cadoli, & Mancini

3.2 Tractability Results
Since detecting any of the properties we are interested in is a computationally hard problem,
a natural question is to determine special cases where checking can be done efficiently. To
this end, we investigate two approaches: we exhibit syntactical restrictions which make the
problem tractable, and we study local relaxations of the definitions which are polynomialtime checkable, and which therefore provide incomplete algorithms for detecting the various
properties.
3.2.1 Tractability for Restricted Constraint Languages
A number of syntactical restrictions to the constraint satisfaction problem are known which
make it tractable. For instance, in the case of boolean constraints, i.e., propositional formulae, the satisfiability problem becomes tractable if the instance is expressed using only
Horn clauses, only dual Horn clauses (i.e., clauses with at most one negative literal), only
clauses of size at most 2, or only affine constraints (i.e., formulae built using XOR). These
are known as the Schaefers (1978) classes. It is natural to wonder if all the properties
identified in Definition 2 are also easy to determine for these classes of formulae. This is indeed the case for most of them, and we give a more general condition under which tractable
classes for the consistency property are also tractable for other properties of our framework.
We note that a recent paper (Jonsson & Krokhin, 2004) gives a complete characterisation
of tractable cases for a related property.
In what follows we are interested in classes of CSPs. To this end, we define a constraint
language D over domain D to be a finite set of relations (of finite arity) with elements
in D, and denote by CSP(D ) the set of CSPs of the form hX, D, Ci with every element
ci = ri (Vi )  C being such that Vi  X and ri  D . (We observe that once the constraint
language D is fixed, the domain D for all instances of CSP(D ) is fixed as well.)
A constraint language  is said to be closed under instantiation (resp. under complementation) if whenever a constraint ci = ri (Vi ) is expressible in the language (i.e., ri  ),
the relation X\{x} (x=a (ci )), a  D (resp. the complementation ci ) can be represented
by a conjunction of constraints of the language. This means that there exist constraints
c01 = r10 (V10 ), . . . , c0k = rk0 (Vk0 ), with Vj0  X and rj0   for each j, such that X\{x} (x=a (ci ))
(resp. ci ) is equivalent to c01   c0k .2 Well known constraint languages on boolean domains
which are closed under instantiation and complementation are those of Horn clauses, dual
Horn clauses, 2CNF clauses or affine constraints (since the instantiation or the complement
of a Horn /dual Horn/2CNF clause or affine constraint can be expressed as a conjunction
of Horn /dual Horn/2CNF clauses or affine constraints).
Theorem 6. Given a constraint language , if the satisfiability problem CSP() is tractable
and if  is closed under instantiation, then the problem of checking determinacy for CSPs
in the class CSP() is tractable as well.
2. Note that we define closure with respect to complementation with a slightly different non-standard
meaning, as the negation of the constraint needs be expressible as a conjunction of constraints. Some
definitions impose that it be definable as a single constraint of the language.

620

fiA Unifying Framework for Structural Properties of CSPs

Proof. Let us consider an arbitrary instance hX, D, Ci  CSP(). Variable x  X is not
determined if and only if there exist two different domain values, a and b  D, such that
X\{x} (x=a (Sol(C)))  X\{x} (x=b (Sol(C)))
is not empty, i.e., if and only if one of the CSPs hX, D, Ca,b i, with:
 X = X \ {x}, and
 Ca,b = {X\{x} (x=a (c)) | c  C}  {X\{x} (x=b (c)) | c  C},
is satisfiable. If  is closed under instantiation, constraints in Ca,b can all be written as
conjunctions of constraints in . Hence, we have reduced the determinacy-testing problem
to solving O(|D|2 ) instances of CSP(), which is tractable.
Theorem 7. Given a constraint language , if the satisfiability problem CSP() is tractable
and if  is closed under instantiation and complementation, then the problem of checking any
property among fixability, substitutability, interchangeability, inconsistency or irrelevance
for CSPs in the class CSP() is tractable as well.
Proof. We start with the substitutability property and note that, given an arbitrary instance
hX, D, Ci  CSP(), value a  D is substitutable by b  D for variable x  X if and only if
X\{x} (x=a (Sol(C)))  X\{x} (x=b (Sol(C))).
This inclusion is false, i.e., substitutability does not hold, if and only if the set
X\{x} (x=a (Sol(C)))  X\{x} (x=b (Sol(C)))

(1)

T
T
is non-empty. Since x=b (Sol(C)) = x=b ( cC Sol(c)) =
cC (x=b (Sol(c))), we have:
X\{x} (x=b (Sol(C))) = X\{x}

T

cC (x=b (Sol(c)))



.

Although the projection of an intersection of relations is not equal to the intersection of
their projections in general, the latter rewrites to:
\
X\{x} (x=b (Sol(c))).
cC

This is due to the fact that we select on x before eliminating it byTprojection. We only prove
the inclusion which does not hold in general: suppose we have t  cC X\{x} (x=b (Sol(c))).
This means that c  C, there exists a tuple tc such that tc |X\{x} = t and tc  x=b (Sol(c)).
It follows that tcx = b and that we have indeed a unique T
t with tx = b and tc |X\{x} = t which
is such that c  C (t  x=b (Sol(c))), i.e., t  X\{x} ( cC (x=b (Sol(c)))).
Formula (1) is therefore equivalent to:
\

X\{x} (x=a (Sol(c))) 

cC

[
cC

621

X\{x} (x=b (Sol(c)))

fiBordeaux, Cadoli, & Mancini

A solution exists (and therefore substitutability does not hold) if one of the sets
\
X\{x} (x=a (Sol(c)))  X\{x} (x=b (Sol(c)))
cC

obtained for every c  C has a solution. If language  is closed under instantiation and complement, we can express the new constraint X\{x} (x=b (Sol(c))) as a conjunction C 0 of constraints in . Each of the sets has a solution iff the CSP hX \ {x}, D, {X\{x} (x=a (c)) | c 
C}  C 0 i is satisfiable. We have reduced the substitutability testing problem to solving |C|
instances of a constraint satisfaction problem whose constraints are all in the language ,
which is tractable.
The results for fixability, interchangeability and irrelevance follow directly, because of
the semantical relationships shown in Figure 3. Consistency of value a for variable x can
directly be expressed as the satisfiability of X\{x} (x=a (Sol(C))), which can be expressed
in , and the proof for implication follows from this result.
A slightly different closure property is needed for the removability of value a for variable x
since it is expressed as X\{x} (Sol(C))  X\{x} (x6=a (Sol(C))).
Nevertheless, since on boolean domains a value v is removable if v is substitutable by
v, and from the remarks on the closure properties of Schaefers classes and the previous
theorem, we obtain that:
Corollary 1. Testing fixability, substitutability, interchangeability, inconsistency, determinacy, irrelevance and removability is tractable for a boolean CSP where constraints are
either Horn clauses, dual Horn clauses, clauses of size at most two or affine constraints.
Unfortunately, we dont have tractability results for dependence.
Table below summarizes Theorems 6 and 7, and Corollary 1:

Property
Determinacy
Fixability, substitutability,
interchangeability, inconsistency, irrelevance
Removability

Polynomial if  is
Tractable and closed under instantiation
Tractable and closed under inst. and compl.

Boolean Schaefer

It is worth noting that conditions become more restrictive when reading the table top-down.
Moreover, in all the cases, it can be observed that tractability of the various propertydetection problems derives from tractability of the constraint language . This leads to
serious concerns about the practical applicability of such results: if CSP() is tractable,
why worrying for identifying such properties? Actually, preliminary studies show that better results are unlikely to hold: as an example, it can be proven that if the constraint
language is intractable, then there is no hope for detecting properties like fixability, irrelevance, substitutability, inconsistency in polynomial time. So these results become of
622

fiA Unifying Framework for Structural Properties of CSPs

interest, suggesting two main directions of further research: the first is of course that of
investigating the practical benefit of detecting such properties in real cases; the second is to
exploit sufficient but efficiently evaluable conditions for these properties to hold, that can
be regarded as a form of approximate reasoning. One of the most used forms of such kind
of reasoning is called local reasoning, which is addressed below.
3.2.2 Tractability Through Locality
An important class of incomplete criteria to determine in polynomial time whether a complex property holds are those based on local reasoning. This approach has proved extremely
successful for consistency (Mackworth, 1977) and interchangeability (Freuder, 1991) (cf.
also Choueiry & Noubir, 1998, where a classification of different local forms of interchangeability are studied and classified). We propose in this section a systematic investigation of
whether a local approach can be used for value-based properties.
Verifying a property P (C) of a set of constraints C locally means that we verify the
property on a well-chosen number of sub-problems. We must ensure that this approach is
sound for the considered property:
Definition 4 (soundness of local reasoning). We say that local reasoning
S on a property P
is sound if, for all subsets of constraints C1  C, . . . , Ck  C such that i1..k Ci = C, we
have (depending on the property):


W
V
 P (C).
 P (C) or
i1..k P (Ci )
i1..k P (Ci )
A typical choice of granularity is to simply consider that each Ci contains one of the constraints of C as is done, for instance, for arc-consistency. On the other extreme, if we take
a unique C1 = C, we have a global checking. Between these two extremes, a wide range of
intermediate levels can be defined (cf., e.g., Freuder, 1978, 1991).
Example 5. Consider the CSP hX, D, Ci with X = {x, y, z}, D = {0, 1, 2} and C =
{c1 , c2 , c3 }, whose elements are defined as follows:
x y
0 1
1 2
2 1
2 2
c1 (x, y)

x z
1 0
1 2
2 0
2 2
c2 (x, z)

y z
1 1
1 2
2 1
2 2
c3 (y, z)

It can be observed that value 1 is substitutable to 2 for variable x. In order to check this
property locally, we consider a suitable covering C1 , . . . , Ck of the set C, and verify it on the
induced subproblems. As an example, by taking C1 = {c1 }, C2 = {c2 }, C3 = {c3 }, we have
substitutableCi (S, x, 1, 2) for every i  {1, 2, 3}. Since local reasoning is sound for substitutability (cf. Freuder, 1991), we can infer that the global property substitutableC (S, x, 1, 2)
holds.
Reasoning locally is typically tractable if we focus on a moderate number of subsets of C,
and under the condition that we can bound the complexity of reasoning on each of these
623

fiBordeaux, Cadoli, & Mancini

subsets. A typical assumption in CSP is that we can bound the arity of the constraints,
and that every constraint is for instance binary. In this case, the cost of determining any
property of the constraint is polynomial; and if we choose to reason locally by considering
each constraint separately, or by taking groups of constraints of bounded size, then local
checking is tractable.
Theorem 8. Local reasoning is sound for the properties of substitutability, interchangeability, fixability, inconsistency, implication, irrelevance,
determinacy, dependence. In particuS
lar, for any C1  C, . . . , Ck  C such that i1..k Ci = C:

V

 substitutableC (S, x, a, b);
i1..k substitutableCi (S, x, a, b)

 V
 interchangeableC (S, x, a, b);

i1..k interchangeableCi (S, x, a, b)


 fixableC (S, x, b);

W
 inconsistentC (S, x, a);

i1..k inconsistentCi (S, x, a)
 W


 impliedC (S, x, a);
i1..k impliedCi (S, x, a)
V

i1..k

fixableCi (S, x, b)



 irrelevantC (S, x);

W

 determinedC (S, x);
i1..k determinedCi (S, x)

 W
 dependentC (S, V, y).

i1..k dependentCi (S, V, y)


V

i1..k

irrelevantCi (S, x)



Proof. The result is well-known for consistency (Mackworth, 1977), substitutability and
interchangeability (Freuder, 1991). Fixability of variable x to value b can be expressed as
a 6= b (substitutableC (S, x, a, b))
V
V V
Therefore, if we have
(S,
x,
b)
(which
is
equivalent
to
C
i1..k fixable
i a6=b
V
Vi
substitutable
(S,
x,
a,
b)
and
to
substitutable
(S,
x,
a,
b)),
then
we
have
Ci
Ci
a6=b i
V
substitutable
(S,
x,
a,
b),
which
means
fixable
(S,
x,
b).
A
similar
argument
works
C
C
a6=b
for irrelevance, which can be analogously defined in terms of fixability (cf. Figure 3). As
for implication, if a value a is implied for variable x in any Ci , then all tuples t with tx 6= a
violate the constraints of Ci and do a fortiori not belong to Sol(C). Similarly for determinacy: if a variable x is determined in any Ci , then all tuples t  Sol(Ci ) will be such
that t[x := b] 6 Sol(Ci ) for any b 6= tx . Hence there cannot be any solution t  Sol(C)
such that t[x := b]  Sol(C) with b 6= tx . Finally, as for dependence, if there exists a Ci
for which dependentC (S, V, y) holds (it is enough to consider sets of constraints Ci s such
i
that V  var(Ci ) 6=  and y  var(Ci ) for which we have dependentC (S, V  var(Ci ), y)),
i
we have, by definition, that t, t0  Sol(Ci ) (x  V  var(Ci ) (tx = t0x ))  ty = t0y , and
so dependentC (S, V, y) also holds, since any solution of the whole problem must satisfy also
Ci .
Example 6 (Example 5 continued). Value 2 is fixable for z. This can be inferred by
performing local reasoning as follows:
624

fiA Unifying Framework for Structural Properties of CSPs

 fixableC1 (S, z, 2) holds, since z does not occur in the scope of c1 ;
 fixableC2 (S, z, 2) holds, since for any tuple t that belongs to c2 , also t[z := 2] belongs
to c2 ;
 fixableC3 (S, z, 2) holds, because of the same argument.
Since by Theorem 8 local reasoning is sound for fixability, we can infer that fixableC (S, z, 2)
holds.
There is only one (value-based) property, namely removability, for which the local approach
is unfortunately not sound:
Theorem 9. Local reasoning is not sound for the removability property.
Proof. Take C = C1  C2 , where C1 is defined as x  y and C2 as x  y. Suppose the
domain has values {1, 2, 3}. Value 2 for x is removable from both constraints considered
independently since, in both cases, we can change the value of any solution which assigns 2
to x to another value. Still, value 2 is not removable for x from their conjunction. To see
why, consider the solution with hx, yi = h2, 2i, and observe that neither h1, 2i or h3, 2i are
solutions.
Note that removing values which are shown to be removable only locally can even make
a satisfiable problem unsatisfiable: if furthermore we add the constraints C3 , defined as
x 6= 1 and C4 , defined as x 6= 3, then value 2 for x is removable in each constraint, while
the only (global) solution actually assigns value 2 to x.
This result, although negative, is in fact interesting, because it gives an ex-post justification of the extensive use that has been made in the last decades of stronger notions, like
inconsistency or substitutability, both of which imply removability (cf. Figure 3). The main
reason why current tools and frameworks for CP try to detect these properties is in order
to remove values from the active domain of some variables. This naturally relies on the removability property (cf. Section 2.5). However, the reason why removability is not directly
used is because it is intractable. For that reason, stronger notions like consistency or substitutability are the forms of removability that have been more commonly used. Actually,
unlike full-fledged removability (cf. Theorem 9), these properties can be detected efficiently,
but incompletely, through local reasoning. Hence, this raises an interesting open issue: do
there exist new (i.e., other than substitutability and inconsistency) properties for which
local reasoning is sound and which imply removability?
We end this section by noting that the local version of the fixability property is indeed a
generalisation to arbitrary domains of the pure literal rule (Davis & Putnam, 1960) which
is well-known in the case of boolean constraints in conjunctive normal form. The pure
literal rule exploits the cases where no constraint (clause) of the problem has a positive
(resp. negative) occurrence of some variable x. In this case, assigning value 0 (resp. 1) to x
preserves the satisfiability of the problem: if a solution t with tx = 1 exists, then t[x := 0]
will also be a solution since no clause constrains x to have value 1.
625

fiBordeaux, Cadoli, & Mancini

Example 7. Consider the following propositional formula  in CNF:
(x  y  z)  (x  y  z)  (y  z)
Since x does not occur in any clause, we can assign x to 1, and maintain the satisfiability
of the formula: for any solution t of , the assignment t[x := 1] is a solution of  as well.
It is clear that the pure literal rule detects fixability based on a reasoning local to each
clause (a variable x is fixable to, say, 1 in a clause iff this clause does not contain the literal
x, and the pure literal rule checks that this condition holds for every constraint).
No generalisation of the pure literal rule has, to the best of our knowledge, been proposed
for CSP, while a generalisation of the pure literal rule for QBF has been applied to solvers
for Quantified CSP under the name pure value rule (Gent, Nightingale, & Stergiou, 2005).
It can be observed that such proposal is in fact a local relaxation of the generalisation
to quantified constraints of fixability. As will be shortly discussed in Section 4, all the
properties presented in this paper can be generalized to Quantified CSP in an elegant way,
and many local relaxations remain valid.

4. Conclusions and Perspectives
In this paper we reconsidered most of the structural properties of CSPs extensively studied
and exploited in order to simplify search. Such properties may be of course useful also in
other tasks, e.g., for the classification or update of solutions, or for compacting the solution
space, and for supporting explanation and interaction with users.
We provided a unifying framework for the properties, that clarifies their semantical
relationships and allows new ones to emerge. We argued that some of the new notions,
namely fixability and removability play a key role in a deep understanding of the topic,
being the ideal characterisations of values that can be fixed or removed preserving the
satisfiability of the problem. Known properties, like inconsistency and substitutability are
thus suitable specialisations of them.
We then tackled the questions related to the automated detection of the different properties and of their exploitation by the solving engine for simplifying problems. In particular,
we showed how detecting each of the proposed properties is in general as hard as the original
CSP. Hence, in order to find efficient ways for their verification, we investigated, according
to the CSP approach, two main lines: addition of suitable restrictions of the constraint language and approximation of the reasoning task by exploiting local versions of the various
notions. Moreover, we discussed how in some cases such properties may arise from explicit
promises made by users. This is the case of problems with properties such as functional
dependencies and unique solutions.
Two of the perspectives raised by our work concern the new central properties which
have emerged from it. We have identified the removability property as an ideal characterisation of the values which can be removed while preserving satisfiability. Unfortunately,
negative results (coNP-completeness of the detection of this property and impossibility of local reasoning) make it impossible to directly use the removability property in practice. This
gives an ex-post justification for the extensive use that has been made in the last decades of
stronger notions (like inconsistency or substitutability) which imply removability, yet can be
626

fiA Unifying Framework for Structural Properties of CSPs

checked by tractable means (of course at the price of losing completeness). An interesting
problem is thus to determine new cases where removability-checking is tractable.
Also, the benefits of fixability have long been known in the boolean case, since this
property has been used in the form of the pure literal rule in many SAT solvers. However,
no generalisation of this property to CSPs has been considered so far.
Finally, the proposed framework allows a natural and elegant generalisation to the case
of Quantified CSP. In particular, in related work (Bordeaux, Cadoli, & Mancini, 2008) we
propose the new notion of outcome as the natural counterpart at the quantified level of
the concept of solution of a CSP. With such notion in mind, all the properties studied
in this paper can be straightforwardly restated for Quantified CSP, as well as their local
relaxations, and new, even more general concepts emerge (the so-called shallow properties,
that may have an impact also at the pure existential CSP level). This opens important
new horizons, allowing QCSP solvers to perform a smarter reasoning on the input problem,
by taking into proper account the quantifiers prefix, that today is usually ignored.

Acknowledgments
This paper is an extended and revised version of Bordeaux, Cadoli, and Mancini (2004).

References
Bordeaux, L., Cadoli, M., & Mancini, T. (2004). Exploiting fixable, substitutable and determined values in constraint satisfaction problems. In Baader, F., & Voronkov, A.
(Eds.), Proceedings of the Eleventh International Conference on Logic for Programming and Automated Reasoning (LPAR 2004), Vol. 3452 of Lecture Notes in Computer
Science, pp. 270284, Montevideo, Uruguay. Springer.
Bordeaux, L., Cadoli, M., & Mancini, T. (2008). Generalizing consistency and other constraint properties to quantified constraints. ACM Transactions on Computational
Logic. To appear.
Cadoli, M., & Mancini, T. (2007). Using a theorem prover for reasoning on constraint
problems. Applied Artificial Intelligence, 21 (4/5), 383404.
Calabro, C., Impagliazzo, R., Kabanets, V., & Paturi, R. (2003). The complexity of Unique
k-SAT: An isolation lemma for k-CNFs. In Proceedings of the Eighteenth IEEE Conference on Computational Complexity (CCC 2003), p. 135 ff., Aarhus, Denmark. IEEE
Computer Society Press.
Chang, C. C., & Keisler, H. J. (1990). Model Theory, 3rd ed. North-Holland.
Choueiry, B. Y., & Noubir, G. (1998). On the computation of local interchangeability in
Discrete Constraint Satisfaction Problems. In Proceedings of the Fifteenth National
Conference on Artificial Intelligence (AAAI98), pp. 326333, Madison, WI, USA.
AAAI Press/The MIT Press.
Crawford, J. M. (1992). A theoretical analysis of reasoning by symmetry in first-order
logic (extended abstract). In Proceedings of Workshop on Tractable Reasoning, in
conjunction with the Tenth National Conference on Artificial Intelligence (AAAI92),
San Jose, CA, USA.
627

fiBordeaux, Cadoli, & Mancini

Crawford, J. M., Ginsberg, M. L., Luks, E. M., & Roy, A. (1996). Symmetry-breaking
predicates for search problems. In Proceedings of the Fifth International Conference
on the Principles of Knowledge Representation and Reasoning (KR96), pp. 148159,
Cambridge, MA, USA. Morgan Kaufmann, Los Altos.
Davis, M., & Putnam, H. (1960). A computing procedure for Quantification Theory. Journal
of the ACM, 7 (3), 201215.
Dechter, R. (1992). Constraint networks (survey). In Encyclopedia of Artificial Intelligence,
2nd edition, pp. 276285. John Wiley & Sons.
Even, S., Selman, A., & Yacobi, Y. (1984). The complexity of promise problems with
applications to public-key cryptography. Information and Control, 61 (2), 159173.
Freuder, E. C. (1978). Synthesizing constraint expressions. Communications of the ACM,
21 (11), 958966.
Freuder, E. C. (1991). Eliminating interchangeable values in Constraint Satisfaction Problems. In Proceedings of the Ninth National Conference on Artificial Intelligence
(AAAI91), pp. 227233, Anaheim, CA, USA. AAAI Press/The MIT Press.
Gent, I. P., & Smith, B. (2000). Symmetry breaking during search in constraint programming. In Proceedings of the Fourteenth European Conference on Artificial Intelligence
(ECAI 2000), pp. 599603, Berlin, Germany.
Gent, I., Nightingale, P., & Stergiou, K. (2005). QCSP-Solve: A solver for Quantified
Constraint Satisfaction Problems. In Proceedings of the Nineteenth International Joint
Conference on Artificial Intelligence (IJCAI 2005), pp. 138143, Edinburgh, Scotland.
Morgan Kaufmann, Los Altos.
Giunchiglia, E., Massarotto, A., & Sebastiani, R. (1998). Act, and the rest will follow:
Exploiting determinism in planning as satisfiability. In Proceedings of the Fifteenth
National Conference on Artificial Intelligence (AAAI98), pp. 948953, Madison, WI,
USA. AAAI Press/The MIT Press.
Jonsson, P., & Krokhin, A. (2004). Recognizing frozen variables in constraint satisfaction
problems. Theoretical Computer Science, 329 (13), 93113.
Jonsson, P., & Krokhin, A. (2008). Computational complexity of auditing discrete attributes
in statistical databases. Journal of Computer and System Sciences. To appear.
Kobler, J., Schoning, U., & Toran, J. (1993). The graph isomorphism problem: its computational complexity. Birkhauser Press.
Lal, A., Choueiry, B., & Freuder, E. C. (2005). Interchangeability and dynamic bundling
for non-binary finite CSPs. In Proceedings of the Twentieth National Conference
on Artificial Intelligence (AAAI 2005), pp. 397404, Pittsburgh, PA, USA. AAAI
Press/The MIT Press.
Lenstra, A., & Lenstra, H. W. (1990). Algorithms in number theory. In van Leeuwen, J.
(Ed.), The Handbook of Theoretical Computer Science, vol. 1: Algorithms and Complexity. The MIT Press.
628

fiA Unifying Framework for Structural Properties of CSPs

Li, C. M. (2000). Integrating equivalency reasoning into Davis-Putnam procedure.
In Proceedings of the Seventeenth National Conference on Artificial Intelligence
(AAAI 2000), pp. 291296, Austin, TX, USA. AAAI Press/The MIT Press.
Mackworth, A. K. (1977). Consistency in networks of relations. Artificial Intelligence, 8,
99118.
Mancini, T., & Cadoli, M. (2005). Detecting and breaking symmetries by reasoning on
problem specifications. In Proceedings of the Sixth International Symposium on Abstraction, Reformulation and Approximation (SARA 2005), Vol. 3607 of Lecture Notes
in Artificial Intelligence, pp. 165181, Airth Castle, Scotland, UK. Springer.
Mancini, T., & Cadoli, M. (2007). Exploiting functional dependencies in declarative problem
specifications. Artificial Intelligence, 171 (1617), 9851010.
Mancini, T., Cadoli, M., Micaletto, D., & Patrizi, F. (2008). Evaluating ASP and commercial
solvers on the CSPLib. Constraints, 13 (4).
Monasson, R., Zecchina, R., Kirkpatrick, S., Selman, B., & Troyansky, L. (1999). Determining computational complexity from characteristic phase transitions. Nature, 400,
133137.
Montanari, U. (1974). Networks of constraints: Fundamental properties and applications to
picture processing. Information Sciences, 7 (2), 85132.
Papadimitriou, C. H. (1994). Computational Complexity. Addison Wesley Publishing Company, Reading, Massachussetts, Reading, MA.
Pyhala, T. (2004). Factoring benchmarks for SAT solvers. Tech. rep., Helsinki university
of technology.
Safarpour, S., Veneris, A., Drechsler, R., & Lee, J. (2004). Managing dont cares in Boolean
Satisfiability. In Proceedings of Design Automation and Test Conference in Europe
(DATE 2004), pp. 260265, Paris, France. IEEE Computer Society Press.
Schaefer, T. J. (1978). The complexity of satisfiability problems. In Proceedings of the Tenth
ACM Symposium on Theory of Computing (STOC78), pp. 216226, San Diego, CA,
USA. ACM Press.
Thiffault, C., Bacchus, F., & Walsh, T. (2004). Solving non-clausal formulas with DPLL
search. In Proceedings of the Tenth International Conference on Principles and Practice of Constraint Programming (CP 2004), Vol. 3258 of Lecture Notes in Computer
Science, pp. 663678, Toronto, Canada. Springer.
Valiant, L. G., & Vijay V. Vazirani, V. V. (1986). NP is as easy as detecting unique
solutions. Theoretical Computer Science, 47 (3), 8593.

629

fiJournal of Artificial Intelligence Research 32 (2008) 419 - 452

Submitted 11/07; published 06/08

Dynamic Control in Real-Time Heuristic Search
Vadim Bulitko

BULITKO @ UALBERTA . CA

Department of Computing Science, University of Alberta
Edmonton, Alberta, T6G 2E8, CANADA

Mitja Lustrek

MITJA . LUSTREK @ IJS . SI

Department of Intelligent Systems, Jozef Stefan Institute
Jamova 39, 1000 Ljubljana, SLOVENIA

Jonathan Schaeffer

JONATHAN @ CS . UALBERTA . CA

Department of Computing Science, University of Alberta
Edmonton, Alberta, T6G 2E8, CANADA

Yngvi Bjornsson

YNGVI @ RU . IS

School of Computer Science, Reykjavik University
Kringlan 1, IS-103 Reykjavik, ICELAND

Sverrir Sigmundarson

SVERRIR . SIGMUNDARSON @ LANDSBANKI . IS

Landsbanki London Branch, Beaufort House,
15 St Botolph Street, London EC3A 7QR, GREAT BRITAIN

Abstract
Real-time heuristic search is a challenging type of agent-centered search because the agents
planning time per action is bounded by a constant independent of problem size. A common problem
that imposes such restrictions is pathfinding in modern computer games where a large number of
units must plan their paths simultaneously over large maps. Common search algorithms (e.g., A*,
IDA*, D*, ARA*, AD*) are inherently not real-time and may lose completeness when a constant
bound is imposed on per-action planning time. Real-time search algorithms retain completeness
but frequently produce unacceptably suboptimal solutions. In this paper, we extend classic and
modern real-time search algorithms with an automated mechanism for dynamic depth and subgoal
selection. The new algorithms remain real-time and complete. On large computer game maps, they
find paths within 7% of optimal while on average expanding roughly a single state per action. This
is nearly a three-fold improvement in suboptimality over the existing state-of-the-art algorithms
and, at the same time, a 15-fold improvement in the amount of planning per action.

1. Introduction
In this paper we study the problem of agent-centered real-time heuristic search (Koenig, 2001).
The distinctive property of such search is that an agent must repeatedly plan and execute actions
within a constant time interval that is independent of the size of the problem being solved. This
restriction severely limits the range of applicable heuristic search algorithms. For instance, static
search algorithms such as A* (Hart, Nilsson, & Raphael, 1968) and IDA* (Korf, 1985), re-planning
algorithms such as D* (Stenz, 1995), anytime algorithms such as ARA* (Likhachev, Gordon, &
Thrun, 2004) and anytime re-planning algorithms such as AD* (Likhachev, Ferguson, Gordon,
Stentz, & Thrun, 2005) cannot guarantee a constant bound on planning time per action. LRTA*
c
2008
AI Access Foundation. All rights reserved.

fiB ULITKO , L U STREK , S CHAEFFER , B J ORNSSON , S IGMUNDARSON

can, but with potentially low solution quality due to the need to fill in heuristic depressions (Korf,
1990; Ishida, 1992).
As a motivating example, consider an autonomous surveillance aircraft in the context of disaster response (Kitano, Tadokoro, Noda, Matsubara, Takahashi, Shinjou, & Shimada, 1999). While
surveying a disaster site, locating victims, and assessing damage, the aircraft can be ordered to fly
to a particular location. Radio interference may make remote control unreliable thereby requiring a
certain degree of autonomy from the aircraft by using AI. This task presents two challenges. First,
due to flight dynamics, the AI must control the aircraft in real time, producing a minimum number
of actions per second. Second, the aircraft needs to reach the target location quickly due to a limited
fuel supply and the need to find and rescue potential victims promptly.
We study a simplified version of this problem which captures the two AI challenges while abstracting away from robot-specific details. Specifically, in line with most work in real-time heuristic
search (e.g., Furcy & Koenig, 2000; Shimbo & Ishida, 2003; Koenig, 2004; Botea, Muller, & Schaeffer, 2004; Hernandez & Meseguer, 2005a, 2005b; Likhachev & Koenig, 2005; Sigmundarson &
Bjornsson, 2006; Koenig & Likhachev, 2006) we consider an agent on a finite search graph with the
task of traveling a path from its current state to a given goal state. Within this context we measure
the amount of planning the agent conducts per action and the length of the path traveled between the
start and the goal locations. These two measures are antagonistic as reducing the amount of planning per action leads to suboptimal actions and results in longer paths. Conversely, shorter paths
require better actions that can be obtained by larger planning effort per action.
We use navigation in grid world maps derived from computer games as a testbed. In such games,
an agent can be tasked to go to any location on the map from its current location. Examples include
real-time strategy games (e.g., Blizzard, 2002), first-person shooters (e.g., id Software, 1993), and
role-playing games (e.g., BioWare Corp., 1998). Size and complexity of game maps as well as the
number of simultaneously moving units on such maps continues to increase with every new generation of games. Nevertheless, each game unit or agent must react quickly to the users command
regardless of the maps size and complexity. Consequently, game companies impose a time-peraction limit on their pathfinding algorithms. For instance, Bioware Corp., a major game company
that we collaborate with, sets the limit to 1-3 ms for all units computing their paths at the same time.
Search algorithms that produce an entire solution before the agent takes its first action (e.g., A*
of Hart et al., 1968) lead to increasing action delays as map size increases. Numerous optimizations
have been suggested to remedy these problems and decrease the delays (for a recent example deployed in a forthcoming computer game refer to Sturtevant, 2007). Real-time search addresses the
problem in a fundamentally different way. Instead of computing a complete, possibly abstract, solution before the first action is to be taken, real-time search algorithms compute (or plan) only a few
first actions for the agent to take. This is usually done by conducting a lookahead search of fixed
depth (also known as search horizon, search depth or lookahead depth) around the agents
current state and using a heuristic (i.e., an estimate of the remaining travel cost) to select the next
few actions. The actions are then taken and the planning-execution cycle repeats (e.g., Korf, 1990).
Since the goal state is not reached by most such local searches, the agent runs the risks of heading
into a dead end or, more generally, selecting suboptimal actions. To address this problem, real-time
heuristic search algorithms update (or learn) their heuristic function with experience. Most existing
algorithms do a constant amount of planning (i.e., lookahead search) per action. As a result, they
tend to waste CPU cycles when the heuristic function is fairly accurate and, conversely, do not plan
enough when the heuristic function is particularly inaccurate. Additionally, they compute heuris420

fiDYNAMIC C ONTROL IN R EAL -T IME H EURISTIC S EARCH

tic with respect to a distant global goal state which can put unrealistic requirements on heuristic
accuracy as we demonstrate in this paper.
In this paper we address both problems by making the following three contributions. First, we
propose two ways for selecting lookahead search depth dynamically, on a per action basis. Second,
we propose a way for selecting intermediate subgoals on a per action basis. Third, we apply these
extensions to the classic LRTA* (Korf, 1990) and the state-of-the-art real-time PR LRTS (Bulitko,
Sturtevant, Lu, & Yau, 2007) and demonstrate the improvements in performance. The resulting
algorithms are the new state of the art in real-time search. To illustrate, on large computer game
maps the new algorithms find paths within 7% of the optimal while expanding only a single state
for any action. For comparison, the previous state-of-the-art, PR LRTS, is 15 times slower per
action while finding paths that are between two and three times more suboptimal. Furthermore, the
dynamically controlled LRTA* and PR LRTS are one to two orders of magnitude faster per action
than A*, weighted A* and the state-of-the-art Partial Refinement A* (PRA*) (Sturtevant & Buro,
2005). Finally, unlike A* and its modern extensions used in games, the new algorithms are provably
real-time and do not slow down as maps become larger.
The rest of the paper is organized as follows. In Section 2 we formulate the problem of real-time
heuristic search and show how the core LRTA* algorithm can be extended with dynamic lookahead
and subgoal selection. Section 3 analyzes related research. Section 4 provides intuition for dynamic
control in search. In Section 5 we describe two approaches to dynamic lookahead selection: one
based on induction of decision-tree classifiers (Section 5.1) and one based on precomputing a depth
table using state abstraction (Section 5.2). In Section 6 we present an approach to selecting subgoals
dynamically. Section 7 evaluates the efficiency of these extensions in the domain of pathfinding. We
conclude with a discussion of applicability of the new approach to general planning.
This paper extends our conference publication (Bulitko, Bjornsson, Lustrek, Schaeffer, & Sigmundarson, 2007) with a new set of features for the decision tree approach, a new way of selecting
subgoals, an additional real-time heuristic search algorithm (PR LRTA*) extended with dynamic
control, numerous additional experiments and a more detailed presentation.

2. Problem Formulation
We define a heuristic search problem as a directed graph containing a finite set of states and weighted
edges, with a single state designated as the goal state. At every time step, a search agent has a single
current state, vertex in the search graph, and takes an action by traversing an out-edge of the current
state. Each edge has a positive cost associated with it. The total cost of edges traversed by an agent
from its start state until it arrives at the goal state is called the solution cost. We require algorithms
to be complete and produce a path from start to goal in a finite amount of time if such a path exists.
In order to guarantee completeness for real-time heuristic search we make the assumption of safe
explorability of our search problems. Namely, all costs are finite and the goal state is reachable from
any state that the agent can possibly reach from its start state.
Formally, all algorithms discussed in this paper are applicable to any such heuristic search problem. To keep the presentation focused and intuitive as well as to afford a large-scale empirical
evaluation, we will use a particular type of heuristic search problems, pathfinding in grid worlds,
for the rest of the paper. However, we will discuss applicability of the new methods we suggest to
other heuristic search problems in Section 5.3 and to general planning problems in Section 9.
421

fiB ULITKO , L U STREK , S CHAEFFER , B J ORNSSON , S IGMUNDARSON

In computer-game map settings, states are vacant square grid cells. Each cell is connected to
four cardinally (i.e., west, north, east, south) and four diagonally neighboring cells. Outbound
edges of a vertex are moves available in the corresponding cell and in the rest of the paper we
will
 use the terms action and move interchangeably. The edge costs are 1 for cardinal moves and
2 for diagonal moves. An agent plans its next action by considering states in a local search space
surrounding its current position. A heuristic function (or simply heuristic) estimates the (remaining)
travel cost between a state and the goal. It is used by the agent to rank available actions and select
the most promising one. In this paper we consider only admissible heuristic functions which do not
overestimate the actual remaining cost to the goal. An agent can modify its heuristic function in any
state to avoid getting stuck in local minima of the heuristic function, as well as to improve its action
selection with experience.
The defining property of real-time heuristic search is that the amount of planning the agent does
per action has an upper bound that does not depend on the problem size. We enforce this property
by setting a real-time cut-off on the amount of planning for any action. Any algorithm that exceeds
such a cut-off is discarded. Fast planning is preferred as it guarantees the agents quick reaction to a
new goal specification or to changes in the environment. We measure mean planning time per action
in terms of CPU time as well as a machine-independent measure  the number of states expanded
during planning. A state is called expanded if all of its successor states are considered/generated
in search. The second performance measure of our study is sub-optimality defined as the ratio of
the solution cost found by the agent to the minimum solution cost. Ratios close to one indicate
near-optimal solutions.
The core of most real-time heuristic search algorithms is an algorithm called Learning RealTime A* (LRTA*) (Korf, 1990). It is shown in Figure 1 and operates as follows. As long as the goal
state sglobal goal is not reached, the algorithm interleaves planning and execution in lines 4 through 7.
In our generalized version we added a new step at line 3 for selecting a search depth d and goal sgoal
individually at each execution step (the original algorithm uses fixed d and sglobal goal for all planning
searches). In line 4, a d-ply breadth-first search with duplicate detection is used to find frontier states
precisely d actions away from the current state s. For each frontier state s, its value is the sum of
the cost of a shortest path from s to s, denoted by g(s, s), and the estimated cost of a shortest path
from s to sgoal (i.e., the heuristic value h(s, sgoal )). We use the standard path-max technique (Mero,
1984) to deal with possible inconsistencies in the heuristic function when computing g + h values.
As a result, g + h values never decrease along any branch of such a lookahead tree. The state that
minimizes the sum is identified as sfrontier in line 5. The heuristic value of the current state s is
updated in line 6 (we keep separate heuristic tables for the different goals). Finally, we take one step
towards the most promising frontier state sfrontier in line 7.

3. Related Research
Most algorithms in single-agent real-time heuristic search use fixed search depth, with a few notable
exceptions. Russell and Wefald (1991) proposed to estimate the utility of expanding a state and use
it to control lookahead search on-line. To do so one needs to estimate how likely an additional search
is to change an actions estimated value. Inaccuracies in such estimates and the overhead of metalevel control led to reasonable but unexciting benefits in combinatorial puzzle and pathfinding.
An additional problem is the relatively low branching factor of combinatorial puzzles which makes
it difficult to eliminate parts of search space early on. The same problem is likely to occur in grid422

fiDYNAMIC C ONTROL IN R EAL -T IME H EURISTIC S EARCH

LRTA*(sstart , sglobal goal )
1 s  sstart
2 while s 6= sglobal goal do
3
select search depth d and goal sgoal
4
expand successor states up to d actions away, generating a frontier
5
find a frontier state sfrontier with the lowest g(s, sfrontier ) + h(sfrontier , sgoal )
6
update h(s, sgoal ) to g(s, sfrontier ) + h(sfrontier , sgoal )
7
change s one step towards sfrontier
8 end while
Figure 1: LRTA* algorithm with dynamic control.
based pathfinding. Finally, their method adds substantial implementation complexity and requires
non-trivial changes to the underlying search algorithm. In contrast, our approach to search depth
selection can be easily interfaced with any real-time search algorithm with a search depth parameter
without modifying the existing code.
Ishida (1992) observed that LRTA*-style algorithms tend to get trapped in local minima of their
heuristic function, termed heuristic depressions. The proposed remedy was to switch to a limited
A* search when a heuristic depression is detected and then use the results of the A* search to
correct the depression at once. This is different from our approach in two ways: first, we do not
need a mechanism to decide when to switch between real-time and A* search and thus avoid the
need to hand-tune control parameters of Ishidas control module. Instead, we employ an automated
approach to decide on search horizon depth for every action. Additionally, we do not spend extra
time filling in all heuristic values within the heuristic depression by A* estimates.
Bulitko (2003a) showed that optimal search depth selection can be highly beneficial in realtime heuristic search. He linked the benefits to avoiding the so-called lookahead pathologies where
deeper lookahead leads to worse moves but did not suggest any practical way of selecting lookahead depth dynamically. Such a way was proposed in 2004 via the use of a generalized definition
of heuristic depressions (Bulitko, 2004). The proposed algorithm extends the search horizon incrementally until the search finds a way out of the depression. After that all actions leading to the found
frontier state are executed. A cap on the search horizon depth is set by the user. The idea of precomputing a depth table of heuristic values for real-time pathfinding was first suggested by Lustrek
and Bulitko (2006). This paper extends their work as follows: (i) we introduce intermediate goals,
(ii) we propose an alternative approach that does not require map-specific pre-computation and (iii)
we extend and evaluate a state-of-the-art algorithm in addition to the classic LRTA*.
There is a long tradition of search control in two-player search. High-performance game-playing
programs for games like chess and checkers rely extensively on search to decide on which actions
to take. The search is performed under strict real-time constraints where programs have typically
only minutes or seconds for deliberating on the next action. Instead of using a fixed-depth lookahead strategy the programs employ sophisticated search control mechanisms for maximizing the
quality of their action decisions within the given time constraints. The search control techniques
can be coarsely divided into three main categories: move ordering, search extensions/reductions,
and time allotment. One of the earlier works on dynamic move ordering is the history heuristic technique (Schaeffer, 1989), and more recent attempts include work on training neural networks (Kocsis, 2003). There exist a large variety of techniques for adjusting the search horizon
423

fiB ULITKO , L U STREK , S CHAEFFER , B J ORNSSON , S IGMUNDARSON

for different branches within the game tree; interesting continuations are explored more deeply
while less promising ones are terminated prematurely. Whereas most of the early techniques were
static, the research focus has shifted towards more dynamic control as well using machine-learning
approaches for automatic parameterization (Buro, 2000; Bjornsson & Marsland, 2003). To the best
of our knowledge, none of these techniques have been applied to single-agent real-time search.

4. Intuition for Dynamic Search Control
It has been observed in the literature that common heuristic functions are not uniformly inaccurate (Pearl, 1984). Namely, they tend to be more accurate closer to the goal state and less accurate
farther away. The intuition for this fact is as follows: heuristic functions usually ignore certain constraints of the search space. For instance, the Manhattan distance heuristic in a sliding tile puzzle
would be perfectly accurate if the tiles could pass through each other. Likewise, Euclidian distance
on a map ignores obstacles. The closer a state is to a goal the fewer constraints a heuristic function
is likely to ignore and, as a result, the more accurate (i.e., closer to the optimal solution cost) the
heuristic is likely to be.
This intuition motivates adaptive search control in real-time heuristic search. First, when heuristic values are inaccurate, the agent should conduct a deeper lookahead search to compensate for the
inaccuracies and maintain the quality of its actions. Deeper lookaheads have been generally found
beneficial in real-time heuristic search (Korf, 1990), though lookahead pathologies (i.e., detrimental
effects of deeper lookaheads on action quality) have been observed as well (Bulitko, Li, Greiner, &
Levner, 2003; Bulitko, 2003b; Lustrek, 2005; Lustrek & Bulitko, 2006). As an illustration, consider
Figure 2. Every state on the map is shaded according to the minimum lookahead depth that an
LRTA* agent should use to select an optimal action. Darker shades correspond to deeper lookahead
depths. Notice that many areas are bright white, indicating that the shallowest lookahead of depth
one will be sufficient. We use this intuition for our first control mechanism: dynamic selection of
lookahead depth in Section 5.

Figure 2: A partial grid world map from a computer game Baldurs Gate (BioWare Corp., 1998).
Shades of grey indicate optimal search depth values with white representing one ply.
Completely black cells are impassable obstacles (e.g., walls).
424

fiDYNAMIC C ONTROL IN R EAL -T IME H EURISTIC S EARCH

Dynamic search depth selection helps eliminate wasted computation by switching to shallower
lookahead when the heuristic function is fairly accurate. Unfortunately, it does not help when the
heuristic function is grossly inaccurate. Instead, it calls for very deep lookahead in order to select
an optimal action. Such a deep search tremendously increases planning time and, sometimes, leads
to violating a real-time cut-off on planning time per move. To address this issue, in Section 6 we
propose our second control mechanism: dynamic selection of subgoals. The idea is straightforward:
if being far from the goal leads to grossly inaccurate heuristic values, let us move the goal closer
to the agent, thereby improving heuristic accuracy. We do this by computing the heuristic function
with respect to an intermediate, and thus nearby, goal as opposed to a distant global goal  the
final destination of an agent. Since an intermediate goal is closer than the global goal, the heuristic
values of states around an agent will likely be more accurate and thus the search depth picked by
our first control mechanism is likely to be shallower. Once the agent gets to an intermediate goal, a
next intermediate goal is selected so that the agent makes progress towards its actual global goal.

5. Dynamic Search Depth Selection
First, we define optimal search depth as follows. For each (s, sglobal goal ) state pair, a true optimal action a (s, sglobal goal ) is to take an edge that lies on an optimal path from s to sglobal goal (there can be
more than one optimal action). Once a (s, sglobal goal ) is known, we can run a series of progressively
deeper LRTA* searches from state s. The shallowest search depth that yields a (s, sglobal goal ) is the
optimal search depth d (s, sglobal goal ). Not only may such search depth forfeit LRTA*s real-time
property but it is also impractical to compute. Thus, in the following subsections we present two
different practical approaches to approximating optimal search depth. Each of them equips LRTA*
with a dynamic search depth selection (i.e., realizing the first part of line 3 in Figure 1). The first
approach uses a decision-tree classifier to select the search depth based on features of the agents
current state and its recent history. The second approach uses a pre-computed depth database based
on an automatically built state abstraction.
5.1 Decision-Tree Classifier Approach
An effective classifier needs input features that are not only useful for predicting the optimal search
depth, but are also efficiently computable by the agent in real time. The features we use for our
classifier were selected as a compromise between these two considerations, as well as for being domain independent. The features were calculated based on properties of states an agent has recently
visited, as well as features gathered by a shallow pre-search from an agents current state. Example
features are: the distance from the state the agent was in n steps ago, estimate of the distance to
agents goal, the number of states visited during the pre-search phase that have updated heuristics.
In Appendix A all the features are listed and the rationale behind them is explained.
The classifier predicts the optimal search depth for the current state. The optimal depth is the
shallowest search depth that returns an optimal action. For training the classifier we must thus label
our training states with optimal search depths. However, to avoid pre-computing optimal actions, we
make the simplifying assumption that a deeper search always yields a better action. Consequently, in
the training phase the agent first conducts a lookahead search to a pre-defined maximum depth, dmax ,
to derive the optimal action (under our assumption). The choice of the maximum depth is domain
dependent and would typically be set as the largest depth that still guarantees the search to return
within the acceptable real-time requirement for the task at hand. Then a series of progressively
425

fiB ULITKO , L U STREK , S CHAEFFER , B J ORNSSON , S IGMUNDARSON

shallower searches are performed to determine the shallowest search depth, dDT , that still returns
the optimal action. During this process, if at any given depth an action is returned that differs
from the optimal action, the progression is stopped. This enforces all depths from dDT to dmax to
agree on the best action. This is important for improving the overall robustness of classification, as
the classifier must generalize over a large set of states. The depth dDT is set as the class label for the
vector of features describing the current state.
Once we have a classifier for choosing the lookahead depth, LRTA* can be augmented with it
(line 3 in Figure 1). The overhead of using the classifier consists of the time required for collecting
the features and running them through the classifier. Its overhead is negligible as the classifier itself
can be implemented as a handful of nested conditional statements. Collecting the features takes
somewhat more time but, with a careful implementation, such overhead can be made negligible as
well. Indeed, the four history-based features are all efficiently computed in small constant time, and
by keeping the lookahead depth of the pre-search small (e.g., one or two) the overhead of collecting
the pre-search features is usually dwarfed by the time the planning phase (i.e., the lookahead search)
takes. The process of gathering training data and building the classifier is carried out off-line and its
time overhead is thus of a lesser concern.
5.2 Pattern Database Approach
A nave approach would be to precompute the optimal depth d for each (s, sgoal ) state pair. There
are two problems with this approach. First, d (s, sgoal ) is not a priori upper-bounded independently
of the map size, thereby forfeiting LRTA*s real-time property. Second, pre-computing d (s, sgoal )
or a (s, sgoal ) for all pairs of (s, sgoal ) states on, for instance, a 512  512 cell computer game map
has prohibitive time and space complexity. We solve the first problem by capping d (s, sgoal ) at a
fixed constant c  1 (henceforth called cap). We solve the second problem by using an automatically built abstraction of the original search space. The entire map is partitioned into regions (or
abstract states) and a single search depth value is pre-computed for each pair of abstract states. During run-time a single search depth value is shared by all children of the abstract state pair (Figure 3).
The search depth values are stored in a table which we will refer to as pattern database or PDB for
short. In the past, pattern databases have been used to store approximate heuristic values (Culberson
& Schaeffer, 1998) and important board features (Schaeffer, 2000). Our work appears to be the first
use of pattern databases to store search depth values.
Computing search depths for abstract states speeds up pre-computation and reduces memory
overhead (both important considerations for commercial computer games). In this paper we use
previously published clique abstraction (Sturtevant & Buro, 2005). It preserves the overall topology
of a map but requires storing the abstraction links explicitly.1 The clique abstraction works by
finding fully connected subgraphs (i.e., the cliques) of the original graph and abstracting all states
within such a clique into a single abstract state. Two abstract states are connected by an abstract
action if and only if there is a single original action that leads from a state in the first clique to a
state in the single clique (Figure 4). The costs of the abstract actions are computed as Euclidean
distances between average coordinates of all states in the cliques.
In typical grid world computer-game maps, a single application of clique abstraction reduces
the number of states by a factor of two to four. On average, at the abstraction level of five (i.e., after
five applications of the abstraction procedure), each region contains about one hundred original
1. An alternative is to use the regular rectangular tiles (e.g., Botea et al., 2004).

426

fiDYNAMIC C ONTROL IN R EAL -T IME H EURISTIC S EARCH

Figure 3: A single optimal lookahead depth value shared among all children of an abstract state.
This is a memory-efficient approximation to the true per-ground-state values in Figure 2.

Level 0 (original graph)

Level 1

Level 2

Figure 4: Two iterations of the clique abstraction procedure produce two abstract levels from the
ground-level search graph.
(or ground-level) states. Thus, a single search depth value is shared among about ten thousand
state pairs. As a result, five-level clique abstraction yields a four orders of magnitude reduction in
memory and about two orders of magnitude reduction in pre-computation time (as analyzed later).
On the downside, higher levels of abstraction effectively make the search depth selection less and
less dynamic as the same depth value is shared among progressively more states. The abstraction
level for a pattern database is a control parameter that trades pre-computation time and pattern
database size for on-line performance of the algorithm that uses such a database.
Two alternatives to storing the optimal search depth are to store an optimal action or the optimal
heuristic value. The combination of abstraction and real-time search precludes both of them. Indeed,
sharing an optimal action computed for a single ground-level representative of an abstract region
among all states in the region may cause the agent to run into a wall (Figure 5, left). Likewise,
sharing a single heuristic value among all states in a region leaves the agent without a sense of
427

fiB ULITKO , L U STREK , S CHAEFFER , B J ORNSSON , S IGMUNDARSON

A
G

5.84

5.84

5.84

5.84

5.84

5.84

5.84

5.84

5.84

5.84

A

5.84

5.84

5.84

G

Figure 5: Goal is shown as G, agent as A. Abstract states are the four tiles separated by dashed lines.
Diamonds indicate representative states for each tile. Left: Optimal actions are shown for
each representative of an abstract tile; applying the optimal action of the agents tile in the
agents current location leads into a wall. Right: Optimal heuristic value (h ) for lower
left tiles representative state (5.84) is shared among all states of the tile. As a result, the
agent has no preference among the three legal actions shown.
direction as all states in its vicinity would look equally close to the goal (Figure 5, right). This is in
contrast to sharing a heuristic value among all states within an abstract state (known as pattern)
when using optimal non-real-time search algorithms such as A* or IDA* (Culberson & Schaeffer,
1996). In the case of real-time search, agents using either alternative are not guaranteed to reach
a goal, let alone minimize travel. On the contrary, sharing the search depth among any number of
ground-level states is safe because LRTA* is complete for any search depth.
We compute a single depth table per map off-line (Figure 6). In line 1 the state space is abstracted ` times. Lines 2 through 7 iterate through all pairs of abstract states. For each pair (s0 , s0goal ),
representative ground-level states s and sgoal (i.e., ground-level states closest to centroids of the regions) are picked and the optimal search depth value d is calculated for them. To do this, Dijkstras
algorithm (Dijkstra, 1959) is run over the ground-level search space (V, E) to compute the true
minimal distances from any state to sgoal . Once the distances are known for all successors of s, an
optimal action a (s, sgoal ) can be computed greedily. Then the optimal search depth d (s, sgoal ) is
computed as previously described and capped at c (line 5). The resulting value is stored for the pair
of abstract states (s0 , s0goal ) in line 6. Figures 2 and 3 show optimal search depth values for a single
goal state on a grid world game map with and without abstraction respectively.
During run-time, an LRTA* agent going from state s to state sgoal takes its search depth from the
depth table value for the pair (s0 , s0goal ), where s0 and s0goal are images of s and sgoal under an `-level
abstraction. The additional run-time complexity is minimal as s0 , s0goal , d(s0 , s0goal ) can be computed
with a small constant-time overhead on each action.
In building such a pattern database Dijkstras algorithm is run V` times2 on the graph (V, E)
 a time complexity of O(V` (V log V + E)) on sparse graphs (i.e., E = O(V )). The optimal
search depth is computed V`2 times. Each time, there are at most c LRTA* invocations with the total
2. For brevity, we use V and E to mean both sets of vertices/edges and their sizes (i.e., |V | and |E|).

428

fiDYNAMIC C ONTROL IN R EAL -T IME H EURISTIC S EARCH

BuildPatternDatabase(V, E, c, `)
1 apply an abstraction procedure ` times to (V, E) to compute abstract space S` = (V` , E` )
2 for each pair of states (s0 , s0goal )  V`  V` do
3
select s  V as a representative of s0  V`
4
select sgoal  V as a representative of s0goal  V`
5
compute c-capped optimal search depth value d for state s with respect to goal sgoal
6
store capped d for pair (s0 , s0goal )
7 end for
Figure 6: Pattern database construction.
complexity of O(bc ) where b is the maximum degree of V . Thus, the overall time complexity is
O(V` (V log V + E + V` bc )). The space complexity is lower because we store optimal search depth
values only for all pairs of abstract states: O(V`2 ). Table 1 lists the bounds for sparse graphs.
Table 1: Reduction in complexity due to state abstraction.

time
space

no abstraction
O(V 2 log V )
O(V 2 )

`-level abstraction
O(V` V log V )
O(V`2 )

reduction
V /V`
(V /V` )2

5.3 Discussion of the Two Approaches
Selecting the search depth with a pattern database has two advantages. First, the search depth values
stored for each pair of abstract states are optimal for their non-abstract representatives, unless either
the value was capped or the states in the local search space have been visited before and their heuristic values have been modified. This (conditional) optimality is in contrast to the classifier approach
where no optimal actions are ever computed as deeper searches are merely assumed to lead to a
better action. The assumption does not always hold  a phenomenon known as lookahead pathology, found in abstract graphs (Bulitko et al., 2003) as well as in grid-based pathfinding (Lustrek &
Bulitko, 2006). The second advantage is that we do not need features of the current state, recent
history and pre-search. The search depth is retrieved from the depth table simply on the basis of the
current states identifier, such as its coordinates.
The decision-tree classifier approach has two advantages over the depth table approach. First,
the classifier training does not need to happen in the same search space that the agent operates in. As
long as the training maps used to collect the features and build the decision tree are representative
of run-time maps, this approach can run on never-before-seen maps (e.g., user-created maps in
a computer game). Second, there is a much smaller memory overhead with this method as the
classifier is specified procedurally and no pattern database needs to be loaded into memory.
Note that both approaches assume that there is a structure to the heuristic search problem at
hand. Namely, the pattern database approach shares a single search depth value across a region of
states. This works most effectively if the states in the region are indeed such that the same lookahead
depth is the best for all of them. Our abstraction mechanism forms regions on the basis of the search
graph structure, with no regard for search depth. As the empirical study will show, clique abstraction
429

fiB ULITKO , L U STREK , S CHAEFFER , B J ORNSSON , S IGMUNDARSON

seems to be the right choice for pathfinding. However, the choice of the best abstraction technique
for a general heuristic search problem is an open question.
Similarly, the decision-tree approach assumes that states that share similar feature values will
also share the best search depth value. It appears to hold to a large extent in our pathfinding domain
but feature selection for arbitrary heuristic search problems is an open question as well.

6. Dynamic Goal Selection
The two methods just described allow the agent to select an individual search depth for each state.
However, as in the original LRTA*, the heuristic is still computed with respect to the global goal
sgoal . To illustrate: in Figure 7, the map is partitioned into eight abstract states (in this case, 4  4
square tiles) whose representative states are shown as diamonds (18). An optimal path between
the agent (A) and the goal (G) is shown as well. A straight-line distance heuristic will ignore the
wall between the agent and the goal and will lead the agent in a south-western direction. An LRTA*
search of depth 11 or higher is needed to produce an optimal action (such as ). Thus, for any
cap value below 11, the agent will be left with a suboptimal action and will spend a long time
above the horizontal wall raising heuristic values. Spending large amounts of time in corners and
other heuristic depressions is the primary weakness of real-time heuristic search agents and, in this
example, is not remedied by dynamic search depth selection due to the cap.

1

2

3

4

A

5

G

7

6

8

Figure 7: Goal is shown as G, agent as A. Abstract states are the eight tiles separated by dashed
lines. Diamonds indicate ground-level representative for each tile. An optimal path is
shown. Entry points of the path into abstract states are marked with circles.

5a compute sintermediate goal goal for (s, sgoal )
5b compute capped optimal search depth value d for s with respect to sintermediate goal
6 store (d , sintermediate goal ) for pair (s0 , s0goal )
Figure 8: Switching sgoal to sintermediate goal ; replaces lines 56 of Figure 6.
430

fiDYNAMIC C ONTROL IN R EAL -T IME H EURISTIC S EARCH

Figure 9: The three maps used in our experiments.
To address this issue, we switch to intermediate goals in our pattern-database construction as well as
on-line LRTA* operation. In the example in Figure 7 we now compute the heuristic around A with
respect to an intermediate goal marked with a double-border circle on the map. Consequently, an
eleven times shallower search depth is needed for an optimal action towards the next abstract state
(right-most upper tile). Our approach replaces lines 5 - 6 in Figure 6 with those in Figure 8. In line
5a, we compute an intermediate goal sintermediate goal as the ground-level state where an optimal path
from s to sgoal enters the next abstract state. These entry points are marked with circles in Figure 7.
We compared entry states to centroids of abstract states as intermediate goals (Bulitko et al., 2007)
and found the former superior in terms of algorithms performance. Note that an optimal path is
easily available off-line after we run the Dijkstras algorithm (Section 5.2).
Once an intermediate goal is computed, line 5b computes a capped optimal search depth for s
with respect to the intermediate goal sintermediate goal . The depth computation is done as described
in Section 5.2. The search depth and the intermediate goal are then added to the pattern database
in line 6. At run-time, the agent executes LRTA* with the stored search depth and computes the
heuristic h with respect to the stored goal (i.e., sgoal is set to sintermediate goal in line 3 of Figure 1). In
other words, both search depth and agents goal are selected dynamically, per action.
This approach works because heuristic functions used in practice tend to become more accurate for states closer to the goal state. Therefore, switching from a distant global goal to a nearby
intermediate goal makes the heuristics around the current state s more accurate and leads to a shallower search depth necessary to achieve an optimal action. As a result, not only does the algorithm
run more quickly with the shallower search per move but also the search depth cap is reached less
frequently and therefore most search depth values actually result in optimal moves.

7. Empirical Evaluation
This section presents results of an empirical evaluation of algorithms with dynamic control of search
depth and goals against classic and state-of-the-art published algorithms. All algorithms avoid reexpanding states during planning for each move via a transposition table. We report sub-optimality
in the solution found and the average amount of computation per action, expressed in the number
of states expanded. We believe that all algorithms can be implemented in such a way that a single
expanded state takes the same amount of time. This was not the case in our testbed as some code
was more optimized than other. For that reason and to avoid clutter, we report CPU times only in
Section 7.7. We used a fixed tie-breaking scheme for all real-time algorithms.
431

fiB ULITKO , L U STREK , S CHAEFFER , B J ORNSSON , S IGMUNDARSON

We use grid world maps from a computer game as our testbed. Game maps provide a realistic
and challenging environment for real-time search and have been seen in a number of recent publications (e.g., Nash, Daniel, & Felner, 2007; Hernandez & Meseguer, 2007). The original maps were
sized 161161 to 193193 cells (Figure 9). In line with Sturtevant and Buro (2005) and Sturtevant
and Jansen (2007), we also experimented with the maps upscaled up to 512  512  closer in size
to maps used in modern computer games. Note that while all three maps depicted in the figure are
outdoor-type maps, we also ran preliminary experiments in indoor-type game maps (e.g., the one
shown in Figure 2). The trends were similar and we decided to focus on the larger outdoor maps.
There were 100 search problems defined on each of the three original size maps. The start and
goal locations were chosen randomly, although constrained such that optimal solution paths cost
between 90 and 100 in order to generate more difficult instances. The upscaled maps had the 300
problems upscaled as well. Each data point in the plots below is an average of 300 problems (3
maps 100 runs each). A different legend entry is used for each algorithm, and multiple points
with the same legend entry represent alternative parameter instantiation of the same algorithm. The
heuristic function used is octile distance  a natural extension of the Manhattan distance for maps
with diagonal actions. To enforce the real-time constraint we disqualified all parameter settings that
caused an algorithm to expand more than 1000 states for any move on any problem. Such points
were excluded from the empirical evaluation. Maps were known a priori off-line in order to build
decision-tree classifiers and pattern databases.
We use the following notation to identify all algorithms and their variants: AlgorithmName
(X, Y) where X and Y are defined as follows. X denotes search depth control: F for fixed search
depth, DT for search depth selected dynamically with a decision tree, ORACLE for search depth
selected with a decision-tree oracle (see the next section for more details) and PDB for search depth
selected dynamically with pattern databases. Y denotes goal state selection: G when the heuristic
is computed with respect to a single global goal, PDB when the heuristic is computed with respect
to an intermediate goal with pattern databases. For instance, the classic LRTA* is LRTA* (F, G).
Our empirical evaluation is organized into eight parts as follows. Section 7.1 describes six
algorithms that compute their heuristic with respect to a global goal and discusses their performance.
Section 7.2 describes five algorithms that use intermediate goals. Section 7.3 compares global and
intermediate goals. Section 7.4 studies the effects of path-refinement with and without dynamic
control. Secton 7.5 pits the new algorithms against state-of-the-art real-time and non-real-time
algorithms. We then provide an algorithm selection guide for different time limits on planning per
move in Section 7.6. Finally, Section 7.7 considers the issue of amortizing off-line pattern-database
build time over on-line pathfinding.
7.1 Algorithms with Global Goals
In this subsection we describe the following algorithms that compute their heuristic with respect to
a single global goal (i.e., do not use intermediate goals):
1. LRTA* (F, G) is Learning Real-Time A* (Korf, 1990). For each action it conducts a breadthfirst search of fixed depth d around the agents current state. Then the first move towards
the best depth d state is taken and the heuristic of the agents previous state is updated using
Korfs mini-min rule.3 We used d  {4, 5, . . . , 20}.
3. Instead of using LRTA* we could have used RTA*. Our experiments showed that in grid pathfinding there is no
significant performance difference between the two for a search depth beyond one. Indeed for deeper searches the

432

fiDYNAMIC C ONTROL IN R EAL -T IME H EURISTIC S EARCH

2. LRTA* (DT, G) is LRTA* in which the search depth d is dynamically controlled by a decision
tree as described in Section 5.1. We used the following parameters: dmax  {5, 10, 15, 20}
and a history trace of length n = 60. For building the decision-tree classifier in WEKA (Witten & Frank, 2005) the pruning factor was set to 0.05 and the minimum number of data items
per leaf to 100 for the original size maps and 25 for the upscaled ones. As opposed to learning
a tailor-made classifier for each game map, a single common decision-tree classifier was built
based on data collected from all the maps (using 10-fold cross-validation). This was done to
demonstrate the ability of the classifier to generalize across maps.
3. LRTA* (ORACLE, G) is LRTA* in which the search depth is dynamically controlled by
an oracle. Such an oracle always selects the best search depth to produce a move given by
LRTA* (F, G) with a fixed lookahead depth dmax (Bulitko et al., 2007). In other words,
the oracle acts as a perfect decision-tree and thus sets an upper bound on LRTA* (DT, G)
performance. The oracle was run for dmax  {5, 10, 15, 20}, and only on the original size
maps as it proved prohibitively expensive to compute it for upscaled maps. Note that this is
not a practical real-time algorithm and is used only as a reference point in our experiments.
4. LRTA* (PDB, G) is LRTA* in which the search depth d is dynamically controlled by a
pattern database as described in Section 5.2. For original size maps, we used an abstraction
level `  {0, 1, . . . , 5} and a depth cap c  {10, 20, 30, 40, 50, 1000}. For upscaled maps,
we used an abstraction level `  {3, 4, . . . , 7} and a depth cap c  {20, 30, 40, 50, 80, 3000}.
Considering the size of our maps, a cap value of 1000 or 3000 means virtually capless search.
5. K LRTA* (F, G) is a variant of LRTA* proposed by Koenig (2004). Unlike the original
LRTA*, it uses A*-shaped lookahead search space and updates heuristic values for all states
within it using Dijkstras algorithm.4 The number of states that K LRTA* expands per move
took on these values: {10, 20, 30, 40, 100, 250, 500, 1000}.
6. P LRTA* (F, G) is Prioritized LRTA*  a variant of LRTA* proposed by Rayner, Davison,
Bulitko, Anderson, and Lu (2007). It uses a lookahead of depth 1 for all moves. However, for
every state whose heuristic value is updated, all its neighbors are put onto an update queue,
sorted by the magnitude of the update. Thus, the algorithm propagates heuristic function
updates in the space in the fashion of Prioritized Sweeping (Moore & Atkeson, 1993). The
control parameter (queue size) was set to {10, 20, 30, 40, 100, 250, 500, 1000} for original
size maps and {10, 20, 30, 40, 100, 250} for upscaled maps.
In Figure 10 we evaluate the performance of the new dynamic depth selection algorithms on the
original size maps. We see that both the decision-tree and the pattern-database approach do improve
significantly upon the LRTA* algorithm, expanding two to three times fewer states for generating
solutions of comparable quality. Furthermore, they perform on par with current state-of-the-art realtime search algorithms without abstraction, as can seen when compared with K LRTA* (F, G). The
solutions generated are of acceptable quality for our domain (e.g., 50% suboptimal), even when
expanding only 100 states per action. Also of interest is that the decision-tree approach performs
likelihood of having multiple actions with equally low g + h cost is very high, reducing the distinction between RTA*
and LRTA*. By using LRTA* we can have agents learn over repeated trials.
4. We also experimented with A*-shaped lookahead in our new algorithms and found it inferior to breadth-first lookahead for deeper searches.

433

fiB ULITKO , L U STREK , S CHAEFFER , B J ORNSSON , S IGMUNDARSON

Original size maps

Realtime cutoff: 1000

4
LRTA* (F, G)
LRTA* (ORACLE, G)
LRTA* (DT, G)
LRTA* (PDB, G)
P LRTA* (F, G)
K LRTA* (F, G)

Suboptimality (times)

3.5
3
2.5
2
1.5

1

0

100

200
300
400
Mean number of states expanded per move

500

600

Figure 10: Global-goal algorithms on original size maps.
quite close to its theoretical best case, as seen when compared to LRTA* (ORACLE, G). This
shows that the features we use, although seemingly simplistic, do a good job at predicting the most
appropriate search depth.
We ran similar sets of experiments on the upscaled maps. However, none of the global goal
algorithms generated solutions of acceptable quality given the real-time cut-off (the solutions were
between 300 and 1700% suboptimal). The experimental results for the upscaled maps are provided
in Appendix B. This shows the inherent limitations of global goal approaches; in large search
spaces they cannot compete on equal footing with abstraction-based methods. This brings us to the
intermediate goal selection methods.
7.2 Algorithms with Intermediate Goals
In this section we describe the algorithms that use intermediate goals during search. To the best
of our knowledge, there is only one previously published real-time heuristic search algorithm that
does so. Thus, we compare it to the new algorithms proposed in this paper. Given that intermediate
goals increase the performance of all algorithms significantly, we present results only on the more
challenging upscaled maps. The full roster of algorithms used in this section is as follows:
1. PR LRTA* (F, G) is Path Refinement Learning Real-Time Search (Bulitko et al., 2007).
The algorithm has two components: it runs LRTA* with a fixed search depth d and a global
goal in an abstract space (abstraction level ` in a clique abstraction hierarchy) and refines
the first move using a corridor-constrained A* running on the original ground-level map.5
Constraining A* to a small set of states, collectively called a corridor by Sturtevant and Buro
5. The algorithm was actually called PR LRTS (Bulitko et al., 2007). Based on findings by Lustrek and Bulitko (2006),
we modified it to refine only a single abstract action in order to reduce its susceptibility to lookahead pathologies.
This modification is equivalent to substituting the LRTS component with LRTA*. Hence, in the rest of the paper, we
call it PR LRTA*.

434

fiDYNAMIC C ONTROL IN R EAL -T IME H EURISTIC S EARCH

(2005) or tunnel by Furcy (2006), speeds it up and makes it real-time if the corridor size
is independent of map size (Bulitko, Sturtevant, & Kazakevich, 2005). While the heuristic
is computed in the abstract space with respect to a fixed global goal, the A* component
computes a path from the current state to an intermediate goal. This qualifies PR LRTA* to
enter this section of empirical evaluation. The control parameters are as follows: abstraction
level `  {3, 4, . . . , 7}, LRTA* lookahead depth d  {1, 3, 5, 10, 15} and LRTA* heuristic
weight   {0.2, 0.4, 0.6, 1.0} ( is imposed on g in line 5 of Figure 1).
2. LRTA* (F, PDB) is LRTA* with fixed search depth that uses a pattern database only to select
intermediate goals. The control parameters are as follows: abstraction level `  {3, 4, . . . , 7}
and search depth d  {1, 2, . . . , 9, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30}.
3. LRTA* (PDB, PDB) is LRTA* generalized with dynamic search depth and intermediate goal selection with pattern databases as presented in Sections 5.2 and 6. The control parameters are as follows: abstraction level `  {3, 4, . . . , 7} and lookahead cap
c  {20, 30, 40, 50, 80, 3000}.
4. PR LRTA* (PDB, G) is PR LRTA* whose LRTA* component is equipped with dynamic search depth but uses a global (abstract) goal with respect to which it computes its abstract heuristic. The pattern database for the search depth is constructed
for the same abstraction level ` that the LRTA* component runs on, making the component as optimal as the lookahead cap allows. We used abstraction level ` 
{3, 4, . . . , 7} and lookahead cap c  {5, 10, 15, 20, 1000}.
We also ran a version of PR LRTA* (PDB, G) where the pattern database is constructed at abstraction level `2 above the level ` where LRTA* operates (Table 2). We used (`, `2 ) 
{(1, 3), (2, 4), (3, 5), (4, 6), (5, 7), (1, 4), (2, 6), (3, 7), (4, 8), (5, 9)}.
5. PR LRTA* (PDB, PDB) is the same as the two-database version of PR LRTA* (PDB, G)
except it uses the second database for goal selection as well as depth selection. We used
(`, `2 )  {(1, 3), (2, 4), (3, 5), (4, 6), (5, 7), (1, 4), (2, 6), (3, 7), (4, 8), (5, 9)} (Table 2).
Table 2: PR LRTA* (PDB, G and PDB) uses LRTA* at abstraction level ` to define a corridor within
which it refines the path using A*. Dynamic depth (and goal) selection is performed either
at abstraction level ` or `2 > `.
Abstraction level
`2
`
0

Single abstraction PR LRTA*(PDB,G)
abstract-level LRTA*
dynamic depth selection
corridor-constrained ground-level A*

Dual abstraction PR LRTA*(PDB,{G,PDB})
dynamic depth (and goal) selection
abstract-level LRTA*
corridor-constrained ground-level A*

The pattern database for the algorithms presented above stores a depth value and an intermediate
ground-level goal for each pair of abstract states. We present performance results for algorithms
with intermediate goals in Sections 7.37.6 and then analyze the complexity of pattern database
computation and its effects on performance in Section 7.7.
435

fiB ULITKO , L U STREK , S CHAEFFER , B J ORNSSON , S IGMUNDARSON

Upscaled maps

Realtime cutoff: 10000

20
LRTA* (F, G)
LRTA* (F, PDB)

18

Suboptimality (times)

16
14
12
10
8
6
4
2
0

200

400

600
800
1000
1200
Mean number of states expanded per move

1400

1600

Figure 11: Effects of intermediate goals: LRTA* (F, G) versus LRTA* (F, PDB).

7.3 Global versus Intermediate Goals
Sections 7.1 and 7.2 presented algorithms with global and intermediate goals respectively. In this
section we compare algorithms across the two groups. To include LRTA* (PDB, G), we increased
the real-time cut-off from 1000 to 10000 for all graphs in this section. We start with the baseline LRTA* with fixed lookahead. The effects of adding intermediate goal selection are dramatic:
LRTA* with intermediate goals (F, PDB) finds five times better solutions while being three orders
of magnitude faster than LRTA* with global goals (F, G) (see Figure 11). We believe that this is a
result of the octile distance heuristic being substantially more accurate around a goal. Consequently,
LRTA* (F, PDB) is benefiting from a much better heuristic function.
In the second experiment, we equip both versions with dynamic search depth control and compare LRTA* (PDB, G) with LRTA* (PDB, PDB) in Figure 12. The performance gap is now less
dramatic: while the planning speed-up is still around three orders of magnitude, the suboptimality
advantage went down from five to two times. Again, note that we had to increase the real-time
cut-off by an order of magnitude to get more points in the plot.
Finally, we evaluate what is more beneficial: dynamic depth control or dynamic goal control by
comparing the baseline LRTA* (F, G) with LRTA* (PDB, G) and LRTA* (F, PDB) in Figure 13. It is
clear that dynamic goal selection is a much stronger addition to the baseline LRTA* than dynamic
search depth selection. Dynamic depth selection sometimes actually performs worse than fixed
depth, as evidenced by the data points above the LRTA* (F, G) line. This happens primarily with
high abstraction levels and small caps. When the optimal lookahead depth is computed at a high
abstraction level, the same depth value is shared among many ground-level states. The selected
depth value can be beneficial near the entry point into the abstract state, but if the abstract state
is too large, the depth is likely to become inappropriate for ground-level states further away. For
example, if the optimal depth at the entry point is 1, it can be worse than a moderate fixed depth
436

fiDYNAMIC C ONTROL IN R EAL -T IME H EURISTIC S EARCH

Upscaled maps

Realtime cutoff: 10000
LRTA* (PDB, G)
LRTA* (PDB, PDB)

14

Suboptimality (times)

12
10
8
6
4
2
0

100

200

300
400
500
600
Mean number of states expanded per move

700

800

Figure 12: Effects of intermediate goals: LRTA* (PDB, G) versus LRTA* (PDB, PDB).
Upscaled maps

Realtime cutoff: 10000

20
LRTA* (F, G)
LRTA* (F, PDB)
LRTA* (PDB, G)

18

Suboptimality (times)

16
14
12
10
8
6
4
2
0

200

400

600
800
1000
1200
Mean number of states expanded per move

1400

1600

Figure 13: Dynamic search depth control versus dynamic goal control.

in ground-level states far from the entry point. Small caps compound the problem by sometimes
preventing the selection of the optimal depth even at the entry point.
While not shown in the plot, running both (i.e., LRTA* (PDB, PDB)) leads to only marginal further improvements. This is because the best parameterizations of LRTA* (F, PDB) already expands
only a single state per move virtually at all times. Consequently, the only benefit of adding dynamic
depth control is a slight improvement in suboptimality  more on this in the next section.
437

fiB ULITKO , L U STREK , S CHAEFFER , B J ORNSSON , S IGMUNDARSON

Upscaled maps

Realtime cutoff: 1000

1.5
LRTA* (F, PDB)
LRTA* (PDB, PDB)
PR LRTA* (F, G)
PR LRTA* (F, PDB)
PR LRTA* (PDB, PDB)
PR LRTA* (PDB, G)

Suboptimality (times)

1.4

1.3

1.2

1.1

1

0

5

10
15
Mean number of states expanded per move

20

25

Figure 14: Effects of path refinement: LRTA* versus PR LRTA*.
7.4 Effects of Path Refinement
Path-refinement algorithms (denoted by the PR prefix) run learning real-time search (LRTA*) in an
abstract space and refine the path by running A* at the ground level. Non-PR algorithm do not run
A* at all as their real-time search happens in the ground-level space. We examine the effects of pathrefinement by comparing LRTA* and PR LRTA*. Note that even the statically controlled baseline
PR LRTA* (F, G) uses intermediate goals in refining its abstract actions. We match it by using
dynamic intermediate goal selection in LRTA*. Thus, we compare four versions of PR LRTA*: (F,
G), (PDB, G), (F, PDB) and (PDB, PDB) to two versions of LRTA*: (F, PDB) and (PDB, PDB).
The results are found in Figure 14. For the sake of clarity, we show the high performance area by
capping the number of states expanded per move at 25 and suboptimality at 1.5.
The best parameterizations of LRTA* find near-optimal solutions while expanding just one state
per move at virtually all times. This is astonishing performance because one state expansion per
move corresponds to search depth of one and is the fastest possible operation of any algorithm in
our framework. Thus, LRTA* (F, PDB) and LRTA* (PDB, PDB) are virtually unbeatable in terms of
planning time. On the other hand, PR LRTA* incurs planning overhead due to its path-refinement
component (i.e., running a corridor-constrained A*). As a result, PR LRTA* also finds nearlyoptimal solutions but incurs at least five times higher planning cost per move. Dynamic control in
PR LRTA* results in moderate performance gains.
7.5 Comparison to the Existing State of the Art
Traditionally, computer games have used A* for pathfinding needs (Stout, 2000). As map size and
the number of simultaneously planning agents increase, game developers find even highly optimized
implementations of A* insufficient. As a result, variants of A* that use state abstraction have been
used (Sturtevant, 2007). Another way of speeding up A* is to introduce a weight in computing travel
cost through a state. If this is done as f = g + h,   0 then values of  below 1 make the agent
438

fiDYNAMIC C ONTROL IN R EAL -T IME H EURISTIC S EARCH

more greedy (more weight is put on h) which usually leads to fewer states expanded at the price of
suboptimal solutions. In this section, we compare the new algorithms to weighted A* (Korf, 1993)
and state-of-the-art Partial Refinement A* (PRA*) (Sturtevant & Buro, 2005). Note that neither
algorithm is real-time and, thus, the planning times per move are map-size specific. That is, with
larger maps, A*s and PRA*s planning times per move will increase as these algorithms compute a
complete (abstract) path between start and goal states before they take the first move. For instance,
for the maps we used PRA* expands 3454 states on its most expensive move. Weighted A* with
 = 15 expands 40734 states and the classic A* expands 88138 states on their worst moves. Thus, to
include these two algorithms in our comparison we had to effectively remove the real-time cut-off.
The results are found in Table 3. Dynamically controlled LRTA* is one to two orders of magnitude faster in average planning time per move. It produces shorter paths than the existing stateof-the-art real-time algorithm (PR LRTA*) and the fastest weighted A* we tried. The original A*
is provably optimal in solution quality and PRA* is nearly optimal. We argue that with hundreds
of units simultaneously planning their paths in a computer game, LRTA* (PDB, PDB)s low planning time per move and real-time guarantees are worth its 6.1% path-length suboptimality (e.g., 106
screen pixels versus the optimal 100 screen pixels).

Table 3: Comparison of high-performance algorithms, best values are in bold. Standard errors are
reported after .
Algorithm, parameters
PR LRTA* (F, G), ` = 4, d = 5,  = 1.0
LRTA* (PDB, PDB), ` = 3, c = 3000
A*
weighted A*, f = 15 g + h
PRA*

Planning per move
15.06 0.0722
1.032 0.0054
119.8 3.5203
24.86 1.4404
10.83 0.0829

Suboptimality (times)
1.161 0.0177
1.061 0.0027
1 0.00
1.146 0.0072
1.001 0.0003

7.6 Best Solution Quality Under a Time Limit
In this section we identify the algorithms that deliver the best solution quality under a time limit.
Specifically, we impose a hard limit on planning time per move, expressed in the number of states
expanded. Any algorithm that exceeds the limit on even a single move made in any of the 300
problems on upscaled maps is excluded from consideration. Among the remaining algorithms, we
select the one with the highest solution quality (i.e., the lowest suboptimality). The results are found
in Table 4. All algorithms expand at least one state per move for some move, leaving the first row
empty. LRTA* (F, PDB) d = 1, ` = 3 is the best choice when the time limit is between one and
eight states expanded per move. As the limit rises, more expensive but more optimal algorithms
become affordable. Note that all the best choices are dynamically controlled algorithms until the
time limit rises to 3454 states. At this point, non-real-time PRA* takes over ending the domain
of real-time algorithms. Such cross-over point is specific to problem and map sizes. With larger
problems/maps, PRA*s maximum planning time per move will necessarily increase, making it the
best choice only for progressively higher planning-time-per-move limits.
439

fiB ULITKO , L U STREK , S CHAEFFER , B J ORNSSON , S IGMUNDARSON

Table 4: Best solution quality under a strict limit on planning time per move. Planning time is
in the states expanded per move. For the sake of readability, suboptimality is shown as
percentage (e.g., 1.102267 = 10.2267%).
Planning time limit
0
[1, 8]
[9, 24]
[25, 48]
[49, 120]
[121, 728]
[729, 753]
[754, 1223]
[1224, 1934]
[1935, 3453]
[3454, 88137]
[88138, )

Algorithm, parameters
LRTA* (F, PDB) d = 1, ` = 3
LRTA* (F, PDB) d = 2, ` = 3
LRTA* (F, PDB) d = 3, ` = 3
LRTA* (F, PDB) d = 4, ` = 3
LRTA* (F, PDB) d = 6, ` = 3
LRTA* (F, PDB) d = 14, ` = 4
PR LRTA* (PDB, G) c = 15,  = 1.0, ` = 3
PR LRTA* (PDB, G) c = 20,  = 1.0, ` = 3
PR LRTA* (PDB, G) c = 1000,  = 1.0, ` = 3
PRA*
A*

Suboptimality (%)
10.2267%
8.6692%
5.6793%
5.6765%
5.6688%
5.6258%
4.2074%
3.6907%
3.5358%
0.1302%
0%

7.7 Amortization of Pattern-database Build Time
Our pattern-database approach invests time into computing a PDB for each map. In this section
we study the amortization of this off-line investment over multiple problem instances. PDB build
times on a 3 GHz Pentium CPU are listed in Table 5 for a single map. Consider algorithm LRTA*
(PDB, PDB) with a cap c = 20 and with pattern databases built at level ` = 3. On average, it has
solution suboptimality of 1.058 while expanding 1.536 states per move in 31.065 microseconds. Its
closest statically controlled competitor is PR LRTA* (F, G) with ` = 4, d = 15,  = 0.6 which has
suboptimality of 1.059 while expanding an average of 28.63 states per move in 131.128 microseconds. Thus, LRTA* (PDB, PDB) is about 100 microseconds faster on each move. Consequently,
4.7  108 moves are necessary to recoup the off-line PDB build time of 13 hours. With each move
taking about 31 microseconds, LRTA* will have a lower total run-time after the first four hours
of pathfinding. We computed such recoup times for all parameterizations of LRTA* (PDB, PDB)
whose closest statically controlled competitor was slower per move. The results are found in Table 6
and demonstrate that LRTA* (PDB, PDB) recoups the PDB build time in the first 1.4 to 27 hours of
its pathfinding time. Note that the numbers are highly implementation and domain-specific. In particular, our code for building PDBs leaves substantial room for optimization. For the completeness
sake, we report detailed times in Appendix C.

8. Discussion of Empirical Results
In this section we recap the trends we have observed in the previous sections. Dynamic selection of
lookahead with either the decision-tree or the PDB approach helps reduce planning time per move
as well as solution suboptimality (Section 7.1). As a result, LRTA* becomes competitive with such
modern algorithms as Koenigs LRTA*. However, all real-time search algorithms with global goals
do not scale well to large maps.
440

fiDYNAMIC C ONTROL IN R EAL -T IME H EURISTIC S EARCH

Table 5: Pattern database for an average 512512 map, computed for intermediate goals. Database
size is listed as the number of abstract state pairs. Suboptimality and planning per move
are listed for a representative algorithm: LRTA* (PDB, PDB) with a cap c = 20.
Abstraction level
0
1
2
3
4
5
6
7

Size
1.1  1010
7.4  108
5.9  107
6.1  106
8.6  105
1.5  105
3.1  104
6.4  103

Time
est. 2 years
est. 1.5 months
est. 4 days
13 hours
3 hours
1 hour
24 minutes
10 minutes

Planning per move
1.5
3.2
41.3
104.4
169.3

Suboptimality (times)
1.058
1.059
1.535
2.315
2.284

Table 6: Amortization of PDB build times. For each dynamically controlled LRTA*, we list the
statically controlled PR LRTA* that is the closest in terms of solution suboptimality.
LRTA* (PDB, PDB)
c = 20, ` = 3
c = 20, ` = 4
c = 30, ` = 3
c = 40, ` = 3
c = 40, ` = 4
c = 50, ` = 3
c = 50, ` = 4
c = 80, ` = 3

PR LRTA* (F, G)
` = 4, d = 15,  = 0.6
` = 4, d = 15,  = 0.6
` = 4, d = 15,  = 0.6
` = 4, d = 15,  = 0.6
` = 4, d = 15,  = 0.4
` = 4, d = 15,  = 0.6
` = 4, d = 15,  = 0.6
` = 4, d = 15,  = 0.6

Amortization moves
4.7  108
1.2  108
5.1  108
5.3  108
3.4  108
6.2  108
6.7  108
1.1  109

Amortization run-time
4 hours
1.4 hours
5.1 hours
6 hours
9.3 hours
9 hours
21.1 hours
27 hours

Adding intermediate goals brings even the classic LRTA* on par with the previous state-of-theart real-time search algorithm PR LRTA* and is a much stronger addition than dynamic lookahead
depth selection (Section 7.3). Using both dynamic lookahead depth and subgoals brings further
improvements. As Section 7.5 details, LRTA* equipped with both dynamic lookahead depth and
subgoal selection expands barely over a state per move and has less than 7% solution suboptimality.
While it is not better than previous state-of-the-art algorithms PR LRTA*, PRA* and A* in both
solution quality and planning time per move, we believe that the trade-offs it makes are appealing in
practice. To aid practitioners further, we provide an algorithm selection guide in Section 7.6 which
makes it clear that LRTA* with dynamic subgoal selection are the best algorithms when the time
per move is severely limited. The speed advantage they deliver over the state-of-the-art PR LRTA*
algorithm allows them to recoup the PDB build time in several hours of pathfinding.

9. Current Limitations and Future Work
This project opens several interesting avenues for future research. In particular, it would be worthwhile to investigate performance of the algorithms in this paper in dynamic environments (e.g., a
bridge gets destroyed in a real-time strategy game or the goal moves away from the agent).
441

fiB ULITKO , L U STREK , S CHAEFFER , B J ORNSSON , S IGMUNDARSON

Another area of future research is application of the proposed algorithms to general planning.
Heuristic search has been a successful approach in planning with such planners as ASP (Bonet,
Loerincs, & Geffner, 1997), the HSP-family (Bonet & Geffner, 2001), FF (Hoffmann, 2000),
SHERPA (Koenig, Furcy, & Bauer, 2002) and LDFS (Bonet & Geffner, 2006). In line with recent
planning work (Likhachev & Koenig, 2005) and Bonet and Geffner (2006), we did not evaluate
proposed algorithms for general STRIPS-style planning problem. Nevertheless, we do believe that
our new real-time heuristic search algorithms may also offer benefits to a wider range of planning
problems. Indeed, the core heuristic search algorithm extended in this paper (LRTA*) was previously applied to general planning (Bonet et al., 1997). The extensions we introduced may have a
beneficial effect in a similar way to how the B-LRTA* improved the performance of ASP planner.
Subgoal selection has been long studied in planning and is a central part of our intermediate-goal
depth-table approach. Decision trees for search depth selection are induced from sample trajectories through the space and appear scalable to general planning problems. The only part of our
approach that requires solving numerous ground-level problems optimally is pre-computation of
optimal search depth in the PDB approach. We conjecture that the approach will still be effective if,
instead of computing the optimal search depth based on an optimal action a , one were to solve a
relaxed planning problem and use the resulting action in place of a . The idea of deriving heuristic
guidance from solving relaxed problems is quite common to both planning and the heuristic search
community.

10. Conclusions
Real-time pathfinding is a non-trivial problem where algorithms must trade solution quality for the
amount of planning per move. These two measures are antagonistic and thus we are interested in
Pareto optimal algorithms which are not outperformed in both measures by any other algorithms.
The classic LRTA* provides a smooth trade-off curve, parameterized by the lookahead depth. Since
its introduction in 1990, a variety of extensions have been proposed. The most recent extension,
PR LRTS (Bulitko et al., 2005) was the first application of automatic state abstraction in real-time
search. In a large-scale empirical study with pathfinding on game maps, PR LRTS outperformed
many other algorithms with respect to several antagonistic measures (Bulitko et al., 2007).
In this paper we also employ automatic state abstraction but instead of using it for pathrefinement, we pre-compute pattern databases and use them to select the amount of planning and
intermediate goals dynamically, per move. Several mechanisms for such dynamic control are proposed and can be used with virtually any existing real-time search algorithm. As a demonstration,
we equip both the classic LRTA* and the state-of-the-art PR LRTS with our dynamic control. The
resulting improvements are substantial. For instance, LRTA* equipped with PDB-based control for
lookahead and intermediate goal selection significantly outperforms the existing state of the art (PR
LRTS) simultaneously in planning per move and solution quality. Furthermore, on average it expands only a little more than one state per move which is the minimum amount of planning for an
LRTA*-based algorithm.
The new algorithms compare favorably to A* and its state-of-the-art extension, PRA*, which
are presently popular industrial choices for pathfinding in computer games (Stout, 2000; Sturtevant,
2007). First, per-move planning time of our algorithms is provably unaffected by any increase in
map size. Second, we are two orders of magnitude faster than A* and one order of magnitude
faster than PRA* in planning time per move. These improvements come at the price of about 7%
442

fiDYNAMIC C ONTROL IN R EAL -T IME H EURISTIC S EARCH

suboptimality, likely to be unnoticed by a computer game player in most scenarios. Thus it appears
that not only the new algorithms redefine the state of the art in the real-time search arena but also
that they are well-suited for industrial applications.

Acknowledgments
Sverrir Sigmundarson was at the School of Computer Science, Reykjavik University during this
project. We appreciate consultation by Robert C. Holte and detailed feedback from the anonymous
reviewers. This research was supported by grants from the National Science and Engineering Research Council of Canada (NSERC); Albertas Informatics Circle of Research Excellence (iCORE);
Slovenian Ministry of Higher Education, Science and Technology; Icelandic Centre for Research
(RANNIS); and by a Marie Curie Fellowship of the European Community programme Structuring
the ERA under contract number MIRG-CT-2005-017284. Special thanks to Nathan Sturtevant for
his development and support of HOG.

Appendix A. Decision-Tree Features
We devised two different categories of classifier features: the first consists of features based on the
agents recent history, whereas the second contains features sampled by a shallow pre-search from
the agents current state. Thus, collectively, the features in the two categories make predictions
based on both the agents recent history as well as its current situation.
The first category has the four features listed in Table 7. These features are computed at each
execution step. Some of them are aggregated over the most recent states the agent was in, which is
done in an incremental fashion for an improved performance. The parameter n is set by the user and
controls how long a history to aggregate over. We use the notation s1 to refer to the state the agent
was in one step ago, s2 for the state two steps ago, etc.; the agent thus aggregates over states s1 ,
..., sn . Feature f1 provides a rough estimate of the location of the agent relative to the goal. The
distance to the goal state can affect the required lookahead depth, for example because heuristics
closer to the goal are usually more accurate. This feature makes it possible for the classifier to make
decisions based on that if deemed necessary. Features f2 (known as mobility) and f3 provide a
measure of how much progress the agent has made towards reaching the goal in the past few steps.
Frequent state revisits may indicate a heuristic depression and a deeper search is usually beneficial
in such situations (Ishida, 1992). Feature f4 is a measure of inaccuracies and inconsistencies in the
heuristic around the agent; again, many heuristic updates may warrant a deeper search.
The features in the second category are listed in Table 8. They are also computed at each execution step. Before the planning phase starts, a shallow lookahead pre-search is performed to gather
information about the nearby part of the search space. The types of features in this category can
be coarsely divided into features that (i) compute the fraction of states on the pre-search lookahead
frontier that satisfy some property, (ii) compare the action chosen by the pre-search to previous
actions (either of the previous state or taken the last time the current state was visited), and (iii)
check heuristic estimates of the immediate successors of the current state. Feature f5 is a rough
measure of the density of obstacles in the agents vicinity: the more obstacles there are, the more
beneficial a deeper search might be. Feature f6 is an indicator of the difficulty of traversing the
local area. If the proportion is high, many states have been updated, possibly suggesting a heuristic
depression. As for feature f7 , if a pre-search selects the same action again this might indicate that
443

fiB ULITKO , L U STREK , S CHAEFFER , B J ORNSSON , S IGMUNDARSON

Table 7: History based classifier features.
Feature
f1
f2
f3
f4

Description
The initial heuristic estimate of the distance from the current state to the goal:
hoctile (s, sglobal goal ).
The heuristic estimate of the distance between the current state and the state the
agent was in n steps ago: h(s, sn ).
The number of distinct states the agent visited in the last n steps:
|{s1 , s2 , ..., sn }|.
P
The total volume of heuristic updates over the last n steps: ni=1 hafter update (si ) 
hbefore update (si ) (line 6 in Figure 1).
Table 8: Pre-search based classifier features.

Feature
f5
f6
f7

f8
f9
f10
f11

Description
The ratio of the actual number of states on the pre-search frontier to the expected
number of states if there were no obstacles on the map.
The fraction of frontier states with an updated heuristic value.
A boolean feature telling whether the action chosen by the pre-search is the same
as the action chosen by the planning phase the last time this state was visited. If
this is the first time the state is visited this feature is false.
A boolean feature telling whether the direction suggested by the pre-search is the
same as the direction the agent took the previous step.
The ratio between the current states heuristic and the best successor state suggested
by the pre-search: h(s, sgoal )/h(s, sgoal ).
A boolean feature telling whether the best action proposed by the pre-search phase
would lead to a successor state with an updated heuristic value.
A boolean feature telling whether the heuristic value of the current state is larger
than the heuristic value of the best immediate successor found by the pre-search.

the heuristic values in this part of the search space are already mutually consistent and thus only a
shallow lookahead is needed; the same applies to feature f8 . Features f9 to f11 compare the current
state to the successor state suggested by the pre-search.

Appendix B. Experiments on Upscaled Maps Using Global Goals
Empirical results of running the global-goal algorithms on upscaled maps are shown in Figure 15.
The LRTA* (DT, G) shows a significant improvement over LRTA* (F, G), making it comparable in
quality to the existing state-of-the-art algorithms: on par with P LRTA* (F, G) and slightly better
than K LRTA* (F, G) when allowed to expand over 200 states per move. It is also worth noting that
LRTA* (PDB, G) is no longer competitive with the other algorithms and, in fact, does not make
the real-time cut-off of 1000 states for any of its parameters combinations (and thus is not shown in
the plot). The reason lies with the fact that the problems are simply too difficult for LRTA* to find
an optimal move with a small lookahead depth. For instance, with abstraction level ` = 3 and cap
444

fiDYNAMIC C ONTROL IN R EAL -T IME H EURISTIC S EARCH

c = 80, LRTA* (PDB, G) has suboptimality of 1.36. Unfortunately, its lookahead depth hits the cap
in 11% of all visited states. As a result, the algorithm expands an average of 1214 states per move
which disqualifies it under a cut-off of 1000.
Upscaled maps

Realtime cutoff: 1000

20
LRTA* (F, G)
LRTA* (DT, G)
P LRTA* (F, G)
K LRTA* (F, G)

18

Suboptimality (times)

16
14
12
10
8
6
4
2
0

100

200

300
400
500
600
Mean number of states expanded per move

700

800

Figure 15: Performance of global-goal algorithms on upscaled maps.
Looking collectively at the small and upscaled up map results, LRTA* (DT, G) demonstrates
excellent performance among the global goal algorithms as it is both robust with respect to map
upscaling and one of the more efficient ones (the only comparable algorithm is K LRTA* (F, G)).
However, within the provided 1000 states cut-off limit, none of the real-time global-goal algorithms
returned solutions that would be considered of an acceptable quality in pathfinding. Indeed, even
the best solutions found are approximately four times worse than the optimal.

Appendix C. Pattern Database Build Times
In order to operate LRTA* and PR LRTA* that use both lookahead depth and intermediate goals controlled dynamically, we build pattern databases. Each pattern database is built off-line and contains
a single entry for each pair of abstract states. There are three types of entries: (i) intermediate goal
which is a ground-level entry state in the next abstract state; (ii) capped optimal lookahead depth
with respect to the intermediate goal and (iii) optimal lookahead depth with respect to the global
goal. When running algorithms with capped lookaheads (i.e., c < 1000) we need two databases
per map: one containing intermediate goals and one containing capped optimal lookahead depths.
When running effectively uncapped algorithms (i.e., c = 1000 or c = 3000) we also need a third
database with lookahead depths for global goals (see Appendix D for further discussion). Tables 5
and 912 report build times and LRTA* (PDB, PDB) performance when capped (i.e., when we have
to build only two pattern databases). Tables 13 and 14 report build times and performance with
effectively no cap (i.e., when we have built all three pattern databases).
Finally, in the interest of speeding up experiments we did not in fact compute pattern databases
for all pairs of abstract states. Instead, we took advantage of prior benchmark problem availability
445

fiB ULITKO , L U STREK , S CHAEFFER , B J ORNSSON , S IGMUNDARSON

Table 9: Pattern databases for an average 512  512 map, computed for intermediate goals.
Database size is listed as the number of abstract state pairs. Suboptimality and planning
per move are listed for LRTA* (PDB, PDB) with a cap c = 30.
Abstraction level
0
1
2
3
4
5
6
7

Size
1.1  1010
7.4  108
5.9  107
6.1  106
8.6  105
1.5  105
3.1  104
6.4  103

Time
est. 2 years
est. 1.5 months
est. 4 days
13.4 hours
3.1 hours
1.1 hours
24 minutes
11 minutes

Planning per move
2.1
12.3
60.7
166.6
258.8

Suboptimality (times)
1.058
1.083
1.260
1.843
1.729

Table 10: Pattern databases for an average 512  512 map, computed for intermediate goals.
Database size is listed as the number of abstract state pairs. Suboptimality and planning
per move are listed for LRTA* (PDB, PDB) with a cap c = 40.
Abstraction level
0
1
2
3
4
5
6
7

Size
1.1  1010
7.4  108
5.9  107
6.1  106
8.6  105
1.5  105
3.1  104
6.4  103

Time
est. 2 years
est. 1.5 months
est. 4 days
13.1 hours
3.1 hours
1.0 hours
24 minutes
10 minutes

Planning per move
2.7
10.2
53.4
217.3
355.4

Suboptimality (times)
1.058
1.060
1.102
1.474
1.490

and computed PDBs only for abstract goal states that come into play in the problems that our agents
were to solve. Thus, the times in the tables are our estimates for all possible pairs.

Appendix D. Intermediate Goals and Loops
As shown by Korf in his original paper, LRTA* is complete for any lookahead depth when its
heuristic is taken with respect to a single global goal. Such completeness guarantee is lost when one
uses intermediate goals (i.e., for LRTA* (F, PDB), LRTA* (PDB, PDB) as well as the PR LRTA*
counter-parts). Indeed, while in an abstract tile A, the dynamic goal control module will guide
the agent towards an entry state in tile B. However, on its way, the agent may stumble in different
abstract tile C. As soon as it happens, the dynamic control module may select an entry state in tile
A as its new intermediate goal. The unsuspecting agent heads back to A and everything repeats.
To combat such loops we equipped all algorithms that use intermediate goals with a state reentrance detector. Namely, as soon as an agent re-visits a ground-level state, the dynamic control
switches from an intermediate goal to the global goal. Additionally, a new lookahead depth is selected. Ideally, such lookahead depth should be the optimal depth with respect to the global goal,
446

fiDYNAMIC C ONTROL IN R EAL -T IME H EURISTIC S EARCH

Table 11: Pattern databases for an average 512  512 map, computed for intermediate goals.
Database size is listed as the number of abstract state pairs. Suboptimality and planning
per move are listed for LRTA* (PDB, PDB) with a cap c = 50.
Abstraction level
0
1
2
3
4
5
6
7

Size
1.1  1010
7.4  108
5.9  107
6.1  106
8.6  105
1.5  105
3.1  104
6.4  103

Time
est. 2 years
est. 1.5 months
est. 4 days
13.6 hours
3.1 hours
1.0 hours
24 minutes
11 minutes

Planning per move
3.5
11.1
68.5
279.4
452.3

Suboptimality (times)
1.058
1.059
1.098
1.432
1.386

Table 12: Pattern databases for an average 512  512 map, computed for intermediate goals.
Database size is listed as the number of abstract state pairs. Suboptimality and planning
per move are listed for LRTA* (PDB, PDB) with a cap c = 80.
Abstraction level
0
1
2
3
4
5
6
7

Size
1.1  1010
7.4  108
5.9  107
6.1  106
8.6  105
1.5  105
3.1  104
6.4  103

Time
est. 2 years
est. 1.5 months
est. 4 days
13.5 hours
3.2 hours
1.0 hours
25 minutes
10 minutes

Planning per move
6.6
22.9
109.7
523.3
811.5

Suboptimality (times)
1.058
1.059
1.087
1.411
1.301

capped at c. Unfortunately, computing optimal lookahead depths for global goals is quite expensive
off-line (Tables 13 and 14). Given that loops occur fairly infrequently, we do not normally compute
optimal lookahead depths for global goals. Instead, when a state re-visit is detected, we switch to
global goals and simply set the lookahead to cap c. Doing so saves off-line PDB computation time
but sometimes causes the agent to conduct a deeper search (c plies) than really necessary.6
An alternative solution to be investigated in future research is to progressively increase lookahead on-line when re-visits are detected (i.e., every time a re-visit occurs, lookahead depth at that
state is increased by a certain number of plies).

6. The only exception to this practice were the cases of c = 1000 and c = 3000 where setting lookahead depth d to c
would have immediately disqualified the algorithm, provided a reasonable real-time cut-off. Consequently, for these
two cap values, we did invest a large amount of time and computed effectively uncapped optimal lookahead depth
with respect to global goals.

447

fiB ULITKO , L U STREK , S CHAEFFER , B J ORNSSON , S IGMUNDARSON

Table 13: Pattern databases for an average 512  512 map, computed for global goals. Database
size is listed as the number of abstract state pairs. Suboptimality and planning per move
are listed for LRTA* (PDB, PDB) with a cap c = 3000.
Abstraction level
0
1
2
3
4
5
6
7

Size
1.1  1010
7.4  108
5.9  107
6.1  106
8.6  105
1.5  105
3.1  104
6.4  103

Time
est. 350 years
est. 25 years
est. 2 years
73 days
10.3 days
1.7 days
9.8 hours
2.5 hours

Planning per move
1.0
4.8
27.9
86.7
174.1

Suboptimality (times)
1.061
1.062
1.133
3.626
3.474

Table 14: Pattern databases for an average 512  512 map, computed for global goals. Database
size is listed as the number of abstract state pairs. Suboptimality and planning per move
are listed for LRTA* (PDB, G) with a cap c = 20.
Abstraction level
0
1
2
3
4
5
6
7

Size
1.1  1010
7.4  108
5.9  107
6.1  106
8.6  105
1.5  105
3.1  104
6.4  103

Time
est. 12 years
est. 6 months
est. 13 days
38.0 hours
7.5 hours
2.3 hours
52 minutes
21 minutes

448

Planning per move
349.9
331.6
281.0
298.1
216.1

Suboptimality (times)
6.468
8.766
10.425
8.155
14.989

fiDYNAMIC C ONTROL IN R EAL -T IME H EURISTIC S EARCH

References
BioWare Corp. (1998). Baldurs Gate., Published by Interplay, http://www.bioware.com/bgate/,
November 30, 1998.
Bjornsson, Y., & Marsland, T. A. (2003). Learning extension parameters in game-tree search. Inf.
Sci, 154(34), 95118.
Blizzard (2002). Warcraft 3: Reign of chaos. http://www.blizzard.com/war3.
Bonet, B., & Geffner, H. (2001). Planning as heuristic search. Artificial Intelligence, 129(12),
533.
Bonet, B., & Geffner, H. (2006). Learning depth-first search: A unified approach to heuristic search
in deterministic and non-deterministic settings, and its application to MDPs. In Proceedings
of the International Conference on Automated Planning and Scheduling (ICAPS), pp. 142
151, Cumbria, UK.
Bonet, B., Loerincs, G., & Geffner, H. (1997). A fast and robust action selection mechanism for
planning.. In Proceedings of the National Conference on Artificial Intelligence (AAAI), pp.
714719, Providence, Rhode Island. AAAI Press / MIT Press.
Botea, A., Muller, M., & Schaeffer, J. (2004). Near optimal hierarchical path-finding. Journal of
Game Development, 1(1), 728.
Bulitko, V. (2003a). Lookahead pathologies and meta-level control in real-time heuristic search.
In Proceedings of the 15th Euromicro Conference on Real-Time Systems, pp. 1316, Porto,
Portugal.
Bulitko, V. (2003b). Lookahead pathologies and meta-level control in real-time heuristic search. In
Proceedings of the 15th Euromicro Conference on Real-Time Systems, pp. 1316.
Bulitko, V. (2004). Learning for adaptive real-time search. Tech. rep. http://arxiv.org/abs/cs.AI/
0407016, Computer Science Research Repository (CoRR).
Bulitko, V., Bjornsson, Y., Lustrek, M., Schaeffer, J., & Sigmundarson, S. (2007). Dynamic Control in Path-Planning with Real-Time Heuristic Search. In Proceedings of the International
Conference on Automated Planning and Scheduling (ICAPS), pp. 4956, Providence, RI.
Bulitko, V., Li, L., Greiner, R., & Levner, I. (2003). Lookahead pathologies for single agent search.
In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), pp.
15311533, Acapulco, Mexico.
Bulitko, V., Sturtevant, N., & Kazakevich, M. (2005). Speeding up learning in real-time search via
automatic state abstraction. In Proceedings of the National Conference on Artificial Intelligence (AAAI), pp. 13491354, Pittsburgh, Pennsylvania.
Bulitko, V., Sturtevant, N., Lu, J., & Yau, T. (2007). Graph Abstraction in Real-time Heuristic
Search. Journal of Artificial Intelligence Research (JAIR), 30, 51100.
Buro, M. (2000). Experiments with Multi-ProbCut and a new high-quality evaluation function for
Othello. In van den Herik, H. J., & Iida, H. (Eds.), Games in AI Research, pp. 7796. U.
Maastricht.
Culberson, J., & Schaeffer, J. (1996). Searching with pattern databases. In CSCI (Canadian AI
Conference), Advances in Artificial Intelligence, pp. 402416. Springer-Verlag.
449

fiB ULITKO , L U STREK , S CHAEFFER , B J ORNSSON , S IGMUNDARSON

Culberson, J., & Schaeffer, J. (1998). Pattern Databases. Computational Intelligence, 14(3), 318
334.
Dijkstra, E. W. (1959). A note on two problems in connexion with graphs.. Numerische Mathematik,
1, 269271.
Furcy, D. (2006). ITSA*: Iterative tunneling search with A*. In Proceedings of the National
Conference on Artificial Intelligence (AAAI), Workshop on Heuristic Search, Memory-Based
Heuristics and Their Applications, Boston, Massachusetts.
Furcy, D., & Koenig, S. (2000). Speeding up the convergence of real-time search. In Proceedings
of the National Conference on Artificial Intelligence (AAAI), pp. 891897.
Hart, P., Nilsson, N., & Raphael, B. (1968). A formal basis for the heuristic determination of
minimum cost paths. IEEE Transactions on Systems Science and Cybernetics, 4(2), 100107.
Hernandez, C., & Meseguer, P. (2005a). Improving convergence of LRTA*(k). In Proceedings of
the International Joint Conference on Artificial Intelligence (IJCAI), Workshop on Planning
and Learning in A Priori Unknown or Dynamic Domains, Edinburgh, UK.
Hernandez, C., & Meseguer, P. (2005b). LRTA*(k). In Proceedings of the 19th International Joint
Conference on Artificial Intelligence (IJCAI), Edinburgh, UK.
Hernandez, C., & Meseguer, P. (2007). Improving real-time heuristic search on initially unknown
maps. In Proceedings of the International Conference on Automated Planning and Scheduling
(ICAPS), Workshop on Planning in Games, p. 8, Providence, Rhode Island.
Hoffmann, J. (2000). A heuristic for domain independent planning and its use in an enforced hillclimbing algorithm. In Proceedings of the 12th International Symposium on Methodologies
for Intelligent Systems (ISMIS), pp. 216227.
id Software (1993). Doom., Published by id Software, http://en.wikipedia.org/ wiki/Doom, December 10, 1993.
Ishida, T. (1992). Moving target search with intelligence. In Proceedings of the National Conference
on Artificial Intelligence (AAAI), pp. 525532.
Kitano, H., Tadokoro, S., Noda, I., Matsubara, H., Takahashi, T., Shinjou, A., & Shimada, S. (1999).
Robocup rescue: Search and rescue in large-scale disasters as a domain for autonomous agents
research. In Man, Systems, and Cybernetics, pp. 739743.
Kocsis, L. (2003). Learning Search Decisions. Ph.D. thesis, University of Maastricht.
Koenig, S. (2004). A comparison of fast search methods for real-time situated agents. In Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems
(AAMAS), pp. 864871.
Koenig, S. (2001). Agent-centered search. AI Magazine, 22(4), 109132.
Koenig, S., Furcy, D., & Bauer, C. (2002). Heuristic search-based replanning. In Proceedings of the
Int. Conference on Artificial Intelligence Planning and Scheduling, pp. 294301.
Koenig, S., & Likhachev, M. (2006). Real-time adaptive A*. In Proceedings of the International
Joint Conference on Autonomous Agents and Multiagent Systems, pp. 281288.
Korf, R. (1985). Depth-first iterative deepening : An optimal admissible tree search. Artificial
Intelligence, 27(3), 97109.
450

fiDYNAMIC C ONTROL IN R EAL -T IME H EURISTIC S EARCH

Korf, R. (1990). Real-time heuristic search. Artificial Intelligence, 42(23), 189211.
Korf, R. (1993). Linear-space best-first search. Artificial Intelligence, 62, 4178.
Likhachev, M., Ferguson, D., Gordon, G., Stentz, A., & Thrun, S. (2005). Anytime dynamic A*: An
anytime, replanning algorithm. In Proceedings of the International Conference on Automated
Planning and Scheduling (ICAPS).
Likhachev, M., Gordon, G. J., & Thrun, S. (2004). ARA*: Anytime A* with provable bounds on
sub-optimality. In Thrun, S., Saul, L., & Scholkopf, B. (Eds.), Advances in Neural Information Processing Systems 16. MIT Press, Cambridge, MA.
Likhachev, M., & Koenig, S. (2005). A generalized framework for lifelong planning A*. In Proceedings of the International Conference on Automated Planning and Scheduling (ICAPS),
pp. 99108.
Lustrek, M. (2005). Pathology in single-agent search. In Proceedings of Information Society Conference, pp. 345348, Ljubljana, Slovenia.
Lustrek, M., & Bulitko, V. (2006). Lookahead pathology in real-time path-finding. In Proceedings of
the National Conference on Artificial Intelligence (AAAI), Workshop on Learning For Search,
pp. 108114, Boston, Massachusetts.
Mero, L. (1984). A heuristic search algorithm with modifiable estimate. Artificial Intelligence, 23,
1327.
Moore, A., & Atkeson, C. (1993). Prioritized sweeping: Reinforcement learning with less data and
less time. Machine Learning, 13, 103130.
Nash, A., Daniel, K., & Felner, S. K. A. (2007). Theta*: Any-angle path planning on grids. In
Proceedings of the National Conference on Artificial Intelligence, pp. 11771183.
Pearl, J. (1984). Heuristics. Addison-Wesley.
Rayner, D. C., Davison, K., Bulitko, V., Anderson, K., & Lu, J. (2007). Real-time heuristic search
with a priority queue. In Proceedings of the International Joint Conference on Artificial
Intelligence (IJCAI), pp. 23722377, Hyderabad, India.
Russell, S., & Wefald, E. (1991). Do the Right Thing: Studies in Limited Rationality. MIT Press.
Schaeffer, J. (1989). The history heuristic and alpha-beta search enhancements in practice. IEEE
Transactions on Pattern Analysis and Machine Intelligence, PAMI-11(1), 12031212.
Schaeffer, J. (2000). Search ideas in Chinook. In van den Herik, H. J., & Iida, H. (Eds.), Games in
AI Research, pp. 1930. U. Maastricht.
Shimbo, M., & Ishida, T. (2003). Controlling the learning process of real-time heuristic search.
Artificial Intelligence, 146(1), 141.
Sigmundarson, S., & Bjornsson, Y. (2006). Value Back-Propagation vs. Backtracking in RealTime Search. In Proceedings of the National Conference on Artificial Intelligence (AAAI),
Workshop on Learning For Search, pp. 136141, Boston, Massachusetts, USA.
Stenz, A. (1995). The focussed D* algorithm for real-time replanning. In Proceedings of the
International Joint Conference on Artificial Intelligence (IJCAI), pp. 16521659.
Stout, B. (2000). The basics of A* for path planning. In Game Programming Gems. Charles River
Media.
451

fiB ULITKO , L U STREK , S CHAEFFER , B J ORNSSON , S IGMUNDARSON

Sturtevant, N. (2007). Memory-efficient abstractions for pathfinding. In Proceedings of the third
conference on Artificial Intelligence and Interactive Digital Entertainment, pp. 3136, Stanford, California.
Sturtevant, N., & Buro, M. (2005). Partial pathfinding using map abstraction and refinement. In
Proceedings of the National Conference on Artificial Intelligence, pp. 13921397.
Sturtevant, N., & Jansen, R. (2007). An analysis of map-based abstraction and refinement. In
Proceedings of the 7th International Symposium on Abstraction, Reformulation and Approximation, Whistler, British Columbia.
Witten, I. H., & Frank, E. (2005). Data Mining: Practical machine learning tools and techniques
(2nd edition). Morgan Kaufmann, San Fransisco.

452

fiJournal of Artificial Intelligence Research 32 (2008) 289353

Submitted 09/07; published 05/08

Optimal and Approximate Q-value Functions
for Decentralized POMDPs
Frans A. Oliehoek

f.a.oliehoek@uva.nl

Intelligent Systems Lab Amsterdam, University of Amsterdam
Amsterdam, The Netherlands

Matthijs T.J. Spaan

mtjspaan@isr.ist.utl.pt

Institute for Systems and Robotics, Instituto Superior Tecnico
Lisbon, Portugal

Nikos Vlassis

vlassis@dpem.tuc.gr
Department of Production Engineering and Management, Technical University of Crete
Chania, Greece

Abstract
Decision-theoretic planning is a popular approach to sequential decision making problems, because it treats uncertainty in sensing and acting in a principled way. In single-agent
frameworks like MDPs and POMDPs, planning can be carried out by resorting to Q-value
functions: an optimal Q-value function Q is computed in a recursive manner by dynamic
programming, and then an optimal policy is extracted from Q . In this paper we study
whether similar Q-value functions can be defined for decentralized POMDP models (DecPOMDPs), and how policies can be extracted from such value functions. We define two
forms of the optimal Q-value function for Dec-POMDPs: one that gives a normative description as the Q-value function of an optimal pure joint policy and another one that is
sequentially rational and thus gives a recipe for computation. This computation, however,
is infeasible for all but the smallest problems. Therefore, we analyze various approximate
Q-value functions that allow for efficient computation. We describe how they relate, and
we prove that they all provide an upper bound to the optimal Q-value function Q . Finally,
unifying some previous approaches for solving Dec-POMDPs, we describe a family of algorithms for extracting policies from such Q-value functions, and perform an experimental
evaluation on existing test problems, including a new firefighting benchmark problem.

1. Introduction
One of the main goals in artificial intelligence (AI) is the development of intelligent agents,
which perceive their environment through sensors and influence the environment through
their actuators. In this setting, an essential problem is how an agent should decide which
action to perform in a certain situation. In this work, we focus on planning: constructing
a plan that specifies which action to take in each situation the agent might encounter over
time. In particular, we will focus on planning in a cooperative multiagent system (MAS):
an environment in which multiple agents coexist and interact in order to perform a joint
task. We will adopt a decision-theoretic approach, which allows us to tackle uncertainty in
sensing and acting in a principled way.
Decision-theoretic planning has roots in control theory and in operations research. In
control theory, one or more controllers control a stochastic system with a specific output
c
2008
AI Access Foundation. All rights reserved.

fiOliehoek, Spaan & Vlassis

as goal. Operations research considers tasks related to scheduling, logistics and work flow
and tries to optimize the concerning systems. Decision-theoretic planning problems can be
formalized as Markov decision processes (MDPs), which have have been frequently employed
in both control theory as well as operations research, but also have been adopted by AI for
planning in stochastic environments. In all these fields the goal is to find a (conditional)
plan, or policy, that is optimal with respect to the desired behavior. Traditionally, the main
focus has been on systems with only one agent or controller, but in the last decade interest
in systems with multiple agents or decentralized control has grown.
A different, but also related field is that of game theory. Game theory considers agents,
called players, interacting in a dynamic, potentially stochastic process, the game. The goal
here is to find optimal strategies for the agents, that specify how they should play and
therefore correspond to policies. In contrast to decision-theoretic planning, game theory
has always considered multiple agents, and as a consequence several ideas and concepts
from game theory are now being applied in decentralized decision-theoretic planning. In
this work we apply game-theoretic models to decision-theoretic planning for multiple agents.
1.1 Decision-Theoretic Planning
In the last decades, the Markov decision process (MDP) framework has gained in popularity
in the AI community as a model for planning under uncertainty (Boutilier, Dean, & Hanks,
1999; Guestrin, Koller, Parr, & Venkataraman, 2003). MDPs can be used to formalize a
discrete time planning task of a single agent in a stochastically changing environment, on
the condition that the agent can observe the state of the environment. Every time step
the state changes stochastically, but the agent chooses an action that selects a particular
transition function. Taking an action from a particular state at time step t induces a
probability distribution over states at time step t + 1.
The agents objective can be formulated in several ways. The first type of objective
of an agent is reaching a specific goal state, for example in a maze in which the agents
goal is to reach the exit. A different formulation is given by associating a certain cost with
the execution of a particular action in a particular state, in which case the goal will be
to minimize the expected total cost. Alternatively, one can associate rewards with actions
performed in a certain state, the goal being to maximize the total reward.
When the agent knows the probabilities of the state transitions, i.e., when it knows the
model, it can contemplate the expected transitions over time and construct a plan that is
most likely to reach a specific goal state, minimizes the expected costs or maximizes the
expected reward. This stands in some contrast to reinforcement learning (RL) (Sutton &
Barto, 1998), where the agent does not have a model of the environment, but has to learn
good behavior by repeatedly interacting with the environment. Reinforcement learning
can be seen as the combined task of learning the model of the environment and planning,
although in practice often it is not necessary to explicitly recover the environment model. In
this article we focus only on planning, but consider two factors that complicate computing
successful plans: the inability of the agent to observe the state of the environment as well
as the presence of multiple agents.
In the real world an agent might not be able to determine what the state of the environment exactly is, because the agents sensors are noisy and/or limited. When sensors are
290

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs

noisy, an agent can receive faulty or inaccurate observations with some probability. When
sensors are limited the agent is unable to observe the differences between states that cannot
be detected by the sensor, e.g., the presence or absence of an object outside a laser rangefinders field of view. When the same sensor reading might require different action choices,
this phenomenon is referred to as perceptual aliasing. In order to deal with the introduced
sensor uncertainty, a partially observable Markov decision process (POMDP) extends the
MDP model by incorporating observations and their probability of occurrence conditional
on the state of the environment (Kaelbling, Littman, & Cassandra, 1998).
The other complicating factor we consider is the presence of multiple agents. Instead of
planning for a single agent we now plan for a team of cooperative agents. We assume that
communication within the team is not possible.1 A major problem in this setting is how the
agents will have to coordinate their actions. Especially, as the agents are not assumed to
observe the stateeach agent only knows its own observations received and actions taken
there is no common signal they can condition their actions on. Note that this problem is
in addition to the problem of partial observability, and not a substitution of it; even if
the agents could freely and instantaneously communicate their individual observations, the
joint observations would not disambiguate the true state.
One option is to consider each agent separately, and have each such agent maintain
an explicit model of the other agents. This is the approach as chosen in the Interactive
POMDP (I-POMDP) framework (Gmytrasiewicz & Doshi, 2005). A problem in this approach, however, is that the other agents also model the considered agent, leading to an
infinite recursion of beliefs regarding the behavior of agents. We will adopt the decentralized
partially observable Markov decision process (Dec-POMDP) model for this class of problems
(Bernstein, Givan, Immerman, & Zilberstein, 2002). A Dec-POMDP is a generalization to
multiple agents of a POMDP and can be used to model a team of cooperative agents that
are situated in a stochastic, partially observable environment.
The single-agent MDP setting has received much attention, and many results are known.
In particular it is known that an optimal plan, or policy, can be extracted from the optimal
action-value, or Q-value, function Q (s,a), and that the latter can be calculated efficiently.
For POMDPs, similar results are available, although finding an optimal solution is harder
(PSPACE-complete for finite-horizon problems, Papadimitriou & Tsitsiklis, 1987).
On the other hand, for Dec-POMDPs relatively little is known except that they are
provably intractable (NEXP-complete, Bernstein et al., 2002). In particular, an outstanding
issue is whether Q-value functions can be defined for Dec-POMDPs just as in (PO)MDPs,
and whether policies can be extracted from such Q-value functions. Currently most algorithms for planning in Dec-POMDPs are based on some version of policy search (Nair,
Tambe, Yokoo, Pynadath, & Marsella, 2003b; Hansen, Bernstein, & Zilberstein, 2004; Szer,
Charpillet, & Zilberstein, 2005; Varakantham, Marecki, Yabu, Tambe, & Yokoo, 2007), and
a proper theory for Q-value functions in Dec-POMDPs is still lacking. Given the wide range
of applications of value functions in single-agent decision-theoretic planning, we expect that
such a theory for Dec-POMDPs can have great benefits, both in terms of providing insight
as well as guiding the design of solution algorithms.
1. As it turns out, the framework we consider can also model communication with a particular cost
that is subject to minimization (Pynadath & Tambe, 2002; Goldman & Zilberstein, 2004). The noncommunicative setting can be interpreted as the special case with infinite cost.

291

fiOliehoek, Spaan & Vlassis

1.2 Contributions
In this paper we develop theory for Q-value functions in Dec-POMDPs, showing that an
optimal Q-function Q can be defined for a Dec-POMDP. We define two forms of the optimal
Q-value function for Dec-POMDPs: one that gives a normative description as the Q-value
function of an optimal pure joint policy and another one that is sequentially rational and
thus gives a recipe for computation. We also show that given Q , an optimal policy can
be computed by forward-sweep policy computation, solving a sequence of Bayesian games
forward through time (i.e., from the first to the last time step), thereby extending the
solution technique of Emery-Montemerlo, Gordon, Schneider, and Thrun (2004) to the
exact setting.
Computation of Q is infeasible for all but the smallest problems. Therefore, we analyze
three different approximate Q-value functions QMDP , QPOMDP and QBG that can be more
efficiently computed and which constitute upper bounds to Q . We also describe a generalized form of QBG that includes QPOMDP , QBG and Q . This is used to prove a hierarchy
of upper bounds: Q  QBG  QPOMDP  QMDP .
Next, we show how these approximate Q-value functions can be used to compute optimal
or sub-optimal policies. We describe a generic policy search algorithm, which we dub
Generalized MAA (GMAA ) as it is a generalization of the MAA algorithm by Szer et al.
(2005), that can be used for extracting a policy from an approximate Q-value function. By
varying the implementation of a sub-routine of this algorithm, this algorithm unifies MAA
and forward-sweep policy computation and thus the approach of Emery-Montemerlo et al.
(2004).
Finally, in an experimental evaluation we examine the differences between QMDP ,
QPOMDP , QBG and Q for several problems. We also experimentally verify the potential
benefit of tighter heuristics, by testing different settings of GMAA on some well known
test problems and on a new benchmark problem involving firefighting agents.
This article is based on previous work by Oliehoek and Vlassis (2007)abbreviated OV
herecontaining several new contributions: (1) Contrary to the OV work, the current work
includes a section on the sequential rational description of Q and suggests a way to compute
Q in practice (OV only provided a normative description of Q ). (2) The current work
provides a formal proof of the hierarchy of upper bounds to Q (which was only qualitatively
argued in the OV paper). (3) The current article additionally contains a proof that the
solutions for the Bayesian games with identical payoffs given by equation (4.2) constitute
Pareto optimal Nash equilibria of the game (which was not proven in the OV paper). (4)
This article contains a more extensive experimental evaluation of the derived bounds of
Q , and introduces a new benchmark problem (firefighting). (5) Finally, the current article
provides a more complete introduction to Dec-POMDPs and existing solution methods, as
well as Bayesian games, hence it can serve as a self-contained introduction to Dec-POMDPs.
1.3 Applications
Although the field of multiagent systems in a stochastic, partially observable environment
seems quite specialized and thus narrow, the application area is actually very broad. The
real world is practically always partially observable due to sensor noise and perceptual
aliasing. Also, in most of these domains communication is not free, but consumes resources
292

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs

and thus has a particular cost. Therefore models as Dec-POMDPs, which do consider
partially observable environments are relevant for essentially all teams of embodied agents.
Example applications of this type are given by Emery-Montemerlo (2005), who considered multi-robot navigation in which a team of agents with noisy sensors has to act to
find/capture a goal. Becker, Zilberstein, Lesser, and Goldman (2004b) use a multi-robot
space exploration example. Here, the agents are Mars rovers and have to decide on how to
proceed their mission: whether to collect particular samples at specific sites or not. The
rewards of particular samples can be sub- or super-additive, making this task non-trivial.
An overview of application areas in cooperative robotics is presented by Arai, Pagello, and
Parker (2002), among which is robotic soccer, as applied in RoboCup (Kitano, Asada, Kuniyoshi, Noda, & Osawa, 1997). Another application that is investigated within this project
is crisis management: RoboCup Rescue (Kitano, Tadokoro, Noda, Matsubara, Takahashi,
Shinjoh, & Shimada, 1999) models a situation where rescue teams have to perform a search
and rescue task in a crisis situation. This task also has been modeled as a partially observable system (Nair, Tambe, & Marsella, 2002, 2003, 2003a; Oliehoek & Visser, 2006; Paquet,
Tobin, & Chaib-draa, 2005).
There are also many other types of applications. Nair, Varakantham, Tambe, and
Yokoo (2005), Lesser, Ortiz Jr., and Tambe (2003) give applications for distributed sensor
networks (typically used for surveillance). An example of load balancing among queues is
presented by Cogill, Rotkowitz, Roy, and Lall (2004). Here agents represent queues and
can only observe queue sizes of themselves and immediate neighbors. They have to decide
whether to accept new jobs or pass them to another queue. Another frequently considered
application domain is communication networks. Peshkin (2001) treated a packet routing
application in which agents are routers and have to minimize the average transfer time of
packets. They are connected to immediate neighbors and have to decide at each time step
to which neighbor to send each packet. Other approaches to communication networks using
decentralized, stochastic, partially observable systems are given by Ooi and Wornell (1996),
Tao, Baxter, and Weaver (2001), Altman (2002).
1.4 Overview of Article
The rest of this article is organized as follows. In Section 2 we will first formally introduce
the Dec-POMDP model and provide background on its components. Some existing solution
methods are treated in Section 3. Then, in Section 4 we show how a Dec-POMDP can be
modeled as a series of Bayesian games and how this constitutes a theory of Q-value functions
for BGs. We also treat two forms of optimal Q-value functions, Q , here. Approximate
Q-value functions are described in Section 5 and one of their applications is discussed in
Section 6. Section 7 presents the results of the experimental evaluation. Finally, Section 8
concludes.

2. Decentralized POMDPs
In this section we define the Dec-POMDP model and discuss some of its properties. Intuitively, a Dec-POMDP models a number of agents that inhabit a particular environment,
which is considered at discrete time steps, also referred to as stages (Boutilier et al., 1999) or
(decision) epochs (Puterman, 1994). The number of time steps the agents will interact with
293

fiOliehoek, Spaan & Vlassis

their environment is called the horizon of the decision problem, and will be denoted by h. In
this paper the horizon is assumed to be finite. At each stage t = 0,1,2, . . . ,h  1 every agent
takes an action and the combination of these actions influences the environment, causing a
state transition. At the next time step, each agent first receives an observation of the environment, after which it has to take an action again. The probabilities of state transitions
and observations are specified by the Dec-POMDP model, as are rewards received for particular actions in particular states. The transition- and observation probabilities specify the
dynamics of the environment, while the rewards specify what behavior is desirable. Hence,
the reward model defines the agents goal or task: the agents have to come up with a plan
that maximizes the expected long term reward signal. In this work we assume that planning
takes place off-line, after which the computed plans are distributed to the agents, who then
merely execute the plans on-line. That is, computation of the plan is centralized, while
execution is decentralized. In the centralized planning phase, the entire model as detailed
below is available. During execution each agent only knows the joint policy as found by the
planning phase and its individual history of actions and observations.
2.1 Formal Model
In this section we more formally treat the basic components of a Dec-POMDP. We start by
giving a mathematical definition of these components.
Definition 2.1. A decentralized
partially observable
Markov decision process (Dec

ff
POMDP) is defined as a tuple n,S,A,T,R,O,O,h,b0 where:
 n is the number of agents.
 S is a finite set of states.
 A is the set of joint actions.
 T is the transition function.
 R is the immediate reward function.
 O is the set of joint observations.
 O is the observation function.
 h is the horizon of the problem.
 b0  P(S), is the initial state distribution at time t = 0.2
The Dec-POMDP model extends single-agent (PO)MDP models by considering joint
actions and observations. In particular, we define A = i Ai as the set of joint actions. Here,
Ai is the set of actions available to agent i. Every time step, one joint action a = ha1 ,...,an i is
taken. In a Dec-POMDP, agents only know their own individual action; they do not observe
each others actions. We will assume that any action ai  Ai can be selected at any time.
So the set Ai does not depend on the stage or state of the environment. In general, we will
2. P() denotes the set of probability distributions over ().

294

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs

denote the stage using superscripts, so at denotes the joint action taken at stage t, ati is the
individual action of agent i taken at stage t. Also, we write a6=i = ha1 , . . . ,ai1 ,ai+1 , . . . ,an i
for a profile of actions for all agents but i.
Similarly to the set of joint actions, O = i Oi is the set of joint observations, where
Oi is a set of observations available to agent i. Every time step the environment emits one
joint observation o = ho1 ,...,on i, from which each agent i only observes its own component
oi , as illustrated by Figure 1. Notation with respect to time and indices for observations
is analogous to the notation for actions. In this paper, we will assume that the actionand observation sets are finite. Infinite action- and observation sets are very difficult to
deal with even in the single-agent case, and to the authors knowledge no research has been
performed on this topic in the partially observable, multiagent case.
Actions and observations are the interface between the agents and their environment.
The Dec-POMDP framework describes this environment by its states and transitions. This
means that rather than considering a complex, typically domain-dependent model of the
environment that explains how this environment works, a descriptive stance is taken: A
	
Dec-POMDP specifies an environment model simply as the set of states S = s1 ,...,s|S|
the environment can be in, together with the probabilities of state transitions that are
dependent on executed joint actions. In particular, the transition from some state to a next
state depends stochastically on the past states and actions. This probabilistic dependence
models outcome uncertainty: the fact that the outcome of an action cannot be predicted
with full certainty.
An important characteristic of Dec-POMDPs is that the states possess the Markov
property. That is, the probability of a particular next state depends on the current state
and joint action, but not on the whole history:
P (st+1 |st ,at ,st1 ,at1 ,...,s0 ,a0 ) = P (st+1 |st ,at ).

(2.1)

Also, we will assume that the transition probabilities are stationary, meaning that they are
independent of the stage t.
In a way similar to how the transition model T describes the stochastic influence of
actions on the environment, the observation model O describes how the state of the environment is perceived by the agents. Formally, O is the observation function, a mapping
from joint actions and successor states to probability distributions over joint observations:
O : A  S  P(O). I.e., it specifies
P (ot |at1 ,st ).

(2.2)

This implies that the observation model also satisfies the Markov property (there is no
dependence on the history). Also the observation model is assumed stationary: there is no
dependence on the stage t.
Literature has identified different categories of observability (Pynadath & Tambe, 2002;
Goldman & Zilberstein, 2004). When the observation function is such that the individual
observation for all the agents will always uniquely identify the true state, the problem
is considered fully- or individually observable. In such a case, a Dec-POMDP effectively
reduces to a multiagent MDP (MMDP) as described by Boutilier (1996). The other extreme
is when the problem is non-observable, meaning that none of the agents observes any useful
295

fiOliehoek, Spaan & Vlassis

actions

a0n

a1n

h1
an

..
.

..
.

..
.

a01

a11

a1h1

o01

o0n

o1n

observations

o0

states

s0

t

0

o11

h1
on

o1

a0

s1

oh1

...

a1

...

o1h1

ah2

sh1

ah1

h1

1

Figure 1: An illustration of the dynamics of a Dec-POMDP. At every stage the environment
is in a particular state. This state emits a joint observation, of which each agent
observes its individual observation. Then each agent selects an action forming
the joint action.

information.
 	This is modeled by the fact that agents always receive a null-observation,
i Oi = oi, . Under non-observability agents can only employ an open-loop plan. Between
these two extremes there are partially observable problems. One more special case has been
identified, namely the case where not the individual, but the joint observation identifies the
true state. This case is referred to as jointly- or collectively observable. A jointly observable
Dec-POMDP is referred to as a Dec-MDP.
The reward function R(s,a) is used to specify the goal of the agents and is a function of states and joint actions. In particular, a desirable sequence of joint actions should
correspond to a high long-term reward, formalized as the return.
Definition 2.2. Let the return or cumulative reward of a Dec-POMDP be defined as total
of the rewards received during an execution:
r(0) + r(1) +    + r(h  1),

(2.3)

where r(t) is the reward received at time step t.
When, at stage t, the state is st and the taken joint action is at , we have that r(t) =
R(st ,a). Therefore, given the sequence of states and taken joint actions, it is straightforward
to determine the return by substitution of r(t) by R(st ,a) in (2.3).
296

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs

In this paper we consider as optimality criterion the expected cumulative reward, where
the expectation refers to the expectation over sequences of states and executed joint actions.
The planning problem is to find a conditional plan, or policy, for each agent to maximize the
optimality criterion. In the Dec-POMDP case this amounts to finding a tuple of policies,
called a joint policy that maximizes the expected cumulative reward.
Note that, in a Dec-POMDP, the agents are assumed not to observe the immediate
rewards: observing the immediate rewards could convey information regarding the true state
which is not present in the received observations, which is undesirable as all information
available to the agents should be modeled in the observations. When planning for DecPOMDPs the only thing that matters is the expectation of the cumulative future reward
which is available in the off-line planning phase, not the actual reward obtained. Indeed, it
is not even assumed that the actual reward can be observed at the end of the episode.
Summarizing, in this work we consider Dec-POMDPs with finite actions and observation
sets and a finite planning horizon. Furthermore, we consider the general Dec-POMDP setting, without any simplifying assumptions on the observation, transition, or reward models.
2.2 Example: Decentralized Tiger Problem
Here we will describe the decentralized tiger problem introduced by Nair et al. (2003b).
This test problem has been frequently used (Nair et al., 2003b; Emery-Montemerlo et al.,
2004; Emery-Montemerlo, Gordon, Schneider, & Thrun, 2005; Szer et al., 2005) and is a
modification of the (single-agent) tiger problem (Kaelbling et al., 1998). It concerns two
agents that are standing in a hallway with two doors. Behind one of the doors is a tiger,
behind the other a treasure. Therefore there are two states: the tiger is behind the left door
(sl ) or behind the right door (sr ). Both agents have 3 actions at their disposal: open the
left door (aOL ), open the right door (aOR ) and listen (aLi ). But they cannot observe each
others actions. In fact, they can only receive 2 observations. Either they hear a sound left
(oHL ) or right (oHR ).
At t = 0 the state is sl or sr with probability 0.5. As long as no agent opens a door the
state doesnt change, when a door is opened, the state resets to sl or sr with probability 0.5.
The full transition, observation and reward model are listed by Nair et al. (2003b). The
observation probabilities are independent, and identical for both agents. For instance, when
the state is sl and both perform action aLi , both agents have a 85% chance of observing
oHL , and the probability of both hearing the tiger left is 0.85  0.85 = 0.72.
When the agents open the door for the treasure they receive a positive reward, while
they receive a penalty for opening the wrong door. When opening the wrong door jointly,
the penalty is less severe. Opening the correct door jointly leads to a higher reward.
Note that, when the wrong door is opened by one or both agents, they are attacked
by the tiger and receive a penalty. However, neither of the agents observe this attack nor
the penalty and the episode continues. Arguably, a more natural representation would be
to have the episode end after a door is opened or to let the agents observe whether they
encountered the tiger or treasure, however this is not considered in this test problem.
297

fiOliehoek, Spaan & Vlassis

2.3 Histories
As mentioned, the goal of planning in a Dec-POMDP is to find a (near-) optimal tuple
of policies, and these policies specify for each agent how to act in a specific situation.
Therefore, before we define a policy, we first need to define exactly what these specific
situations are. In essence such situations are those parts of the history of the process that
the agents can observe.
Let us first consider what the history of the process is. A Dec-POMDP with horizon h
specifies h time steps or stages t = 0,...,h  1. At each of these stages, there is a state st ,
joint observation ot and joint action at . Therefore, when the agents will have to select
their k-th actions (at t = k  1), the history of the process is a sequence of states, joint
observations and joint actions, which has the following form:


s0 ,o0 ,a0 ,s1 ,o1 ,a1 ,...,sk1 ,ok1 .

Here s0 is the initial state, drawn according to the initial state distribution b0
. The initial
ff
joint observation o0 is assumed to be the empty joint observation: o0 = o = o1, ,...,on, .
From this history of the process, the states remain unobserved and agent i can only
observe its own actions and observations. Therefore an agent will have to base its decision
regarding which action to select on the sequence of actions and observations observed up
to that point.
Definition 2.3. We define the action-observation history for agent i, ~i , as the sequence
of actions taken by and observations received by agent i. At a specific time step t, this is:

t
~it = o0i ,a0i ,o1i . . . ,at1
i ,oi .

~ is the action-observation history for all agents:
The joint action-observation history, ,
~ t = h~1t , . . . ,~nt i.
~ t = t (Oi  Ai ). The
Agent is set of possible action-observation histories at time t is 
i
~ i = h1 
~t 3
set of all possible action-observation histories for agent i is 
t=0 i . Finally the set
h1 ~ t
~
~ tn ). At
of all possible joint action-observation histories is given by  = t=0 (1  ...  
t = 0, the action-observation history is empty, denoted by ~ 0 = ~ .
We will also use a notion of history only using the observations of an agent.
Definition 2.4. Formally, we define the observation history for agent i, ~oi , as the sequence
of observations an agent has received. At a specific time step t, this is:

~oit = o0i ,o1i , . . . ,oti .
The joint observation history, ~o, is the observation history for all agents:
~o t = h~o1t , . . . ,~ont i.
3. Note that in a particular Dec-POMDP, it may be the case that not all of these histories can actually be
realized, because of the probabilities specified by the transition and observation model.

298

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs

~ t = t Oi . Similar to the
The set of observation histories for agent i at time t is denoted O
i
~ i and O
~ and the empty observation
notation for action-observation histories, we also use O
history is denoted ~o .
Similarly we can define the action history as follows.
Definition 2.5. The action history for agent i, ~ai , is the sequence of actions an agent has
performed. At a specific time step t, we write:

~ait = a0i ,a1i , . . . ,at1
.
i

Notation for joint action histories and sets are analogous to those for observation histories. Also write ~o6=i ,~6=i , etc. to denote a tuple of observation-, action-observation histories,
etc. for all agents except i. Finally we note that, clearly, an (joint) action-observation


ff
history consists of an (joint) action- and an (joint) observation history: ~ t = ~o t ,~a t .

2.4 Policies

As discussed in the previous section, the action-observation history of an agent specifies
all the information the agent has when it has to decide upon an action. For the moment
we assume that an individual policy i for agent i is a deterministic mapping from actionobservation sequences to actions.
The number of possible action-observation histories is usually very large as this set
grows exponentially with the horizon of the problem. At time step t, there are (|Ai |  |Oi |)t
action-observation histories for agent i. As a consequence there are a total of
h1
X

(|Ai |  |Oi |)t =

t=0

(|Ai |  |Oi |)h  1
(|Ai |  |Oi |)  1

of such sequences for agent i. Therefore the number of policies for agent i becomes:
|Ai

(|Ai ||Oi |)h 1
| (|Ai ||Oi |)1

,

(2.4)

which is doubly exponential in the horizon h.
2.4.1 Pure and Stochastic Policies
It is possible to reduce the number of policies under consideration by realizing that a lot
of policies specify the same behavior. This is illustrated by the left side of Figure 2, which
clearly shows that under a deterministic policy only a subset of possible action-observation
histories are reached. Policies that only differ with respect to an action-observation history
that is not reached in the first place, manifest the same behavior. The consequence is that
in order to specify a deterministic policy, the observation history suffices: when an agent
takes its action deterministically, he will be able to infer what action he took from only the
observation history as illustrated by the right side of Figure 2.
Definition 2.6. A pure or deterministic policy, i , for agent i in a Dec-POMDP is a
~ i  Ai . The set of pure policies of
mapping from observation histories to actions, i : O
agent i is denoted i .
299

fiOliehoek, Spaan & Vlassis

act.-obs. history
aLi
oHL

oHR

aOL

aOR
aLi

aOL
aLi

aOR

oHL

aOL

aOR
aOL

oHR

aLi

aLi

oHL

oHR

oHL

oHR

aOL

aLi

aLi

aOR

Figure 2: A deterministic policy can be represented as a tree. Left: a tree of actionobservation histories ~i for one of the agents from the Dec-Tiger problem. An
arbitrary deterministic policy i is highlighted. Clearly shown is that i only
reaches a subset of of histories ~i . (~i that are not reached are not further expanded.) Right: The same policy can be shown in a simplified policy tree.

Note that also for pure policies we sometimes write i (~i ). In this case we mean the
action that i specifies for the observation history contained in ~i . For instance, let ~i =
h~oi ,~ai i, then i (~i ) = i (~oi ). We use  = h1 ,...,n i to denote a joint policy, a profile
specifying a policy for each agent. We say that a pure joint policy is an induced or implicit
~  A. That is, the mapping
mapping from joint observation histories to joint actions  : O
is induced by individual policies i that make up the joint policy. Also we use 6=i =
h1 , . . . ,i1 ,i+1 , . . . ,n i, to denote a profile of policies for all agents but i.
Apart from pure policies, it is also possible to have the agents execute randomized
policies, i.e., policies that do not always specify the same action for the same situation, but
in which there is an element of chance that decides which action is performed. There are
two types of randomized policies: mixed policies and stochastic policies.
Definition 2.7. A mixed policy, i , for an agent i is a set of pure policies, M  i , along
with a probability distribution over this set. Thus a mixed policy i  P(M) is an element
of the set of probability distributions over M.
Definition 2.8. A stochastic or behavioral policy, i , for agent i is a mapping from action~ i  P(Ai ).
observation histories to probability distributions over actions, i : 
When considering stochastic policies, keeping track of only the observations is insufficient, as in general all action-observation histories can be realized. That is why stochastic
policies are defined as a mapping from the full space of action-observation histories to probability distributions over actions. Note that we use i and i to denote a policy (space) in
general, so also for randomized policies. We will only use i , i and i when there is a need
to discriminate between different types of policies.
300

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs

A common way to represent the temporal structure in a policy is to split it in decision
rules i that specify the policy for each stage. An individual policy is then represented as
a sequence of decision rules i = (i0 , . . . ,ih1 ). In case of a deterministic policy, the form
of the decision rule for stage t is a mapping from length-t observation histories to actions
~ t  Ai .
it : O
i
2.4.2 Special Cases with Simpler Policies.
There are some special cases of Dec-POMDPs in which the policy can be specified in a
simpler way. Here we will treat three such cases: in case the state s is observable, in
the single-agent case and the case that combines the previous two: a single agent in an
environment of which it can observe the state.
The last case, a single agent in a fully observable environment, corresponds to the regular
MDP setting. Because the agent can observe the state, which is Markovian, the agent does
not need to remember
any history, but can simply specify the decision rules  of its policy

0
h1
 =  , . . . ,
as mappings from states to actions: t  t : S  A. The complexity of
the policy representation reduces even further in the infinite-horizon case, where an optimal
policy   is known to be stationary. As such, there is only one decision rule , that is used
for all stages.
The same is true for multiple agents that can observe the state, i.e., a fully observable
Dec-POMDP as defined in Section 2.1. This is essentially the same setting as the multiagent
Markov decision process (MMDP) introduced by Boutilier (1996). In this case, the decision
rules for agent is policy are mappings from states to actions t it : S  Ai , although
in this case some care needs to be taken to make sure no coordination errors occur when
searching for these individual policies.
In a POMDP, a Dec-POMDP with a single agent, the agent cannot observe the state,
so it is not possible to specify a policy as a mapping from states to actions. However, it
turns out that maintaining a probability distribution over states, called belief, b  P(S), is
a Markovian signal:
P (st+1 |at ,ot ,at1 ,ot1 , . . . ,a0 ,o0 ) = P (st+1 |bt ,at ),
where the belief bt is defined as
st

bt (st )  P (st |ot ,at1 ,ot1 , . . . ,a0 ,o0 ) = P (st |bt1 ,at1 ,ot ).

As a result, a single agent in a partially observable environment can specify its policy as a
series of mappings from the set of beliefs to actions t  t : P(S)  A.
Unfortunately, in the general case we consider, no such space-saving simplifications of
the policy are possible. Even though the transition and observation model can be used
to compute a joint belief, this computation requires knowledge of the joint actions and
observations. During execution, the agents simply have no access to this information and
thus can not compute a joint belief.
2.4.3 The Quality of Joint Policies
Clearly, policies differ in how much reward they can expect to accumulate, which will serve
as a criterion of a joint policys quality. Formally, we consider the expected cumulative
reward of a joint policy, also referred to as its value.
301

fiOliehoek, Spaan & Vlassis

Definition 2.9. The value V () of a joint policy  is defined as
V ()  E

h1
hX
t=0

fi
i
fi
R(st ,at )fi,b0 ,

(2.5)

where the expectation is over states, observations andin the case of a randomized 
actions.
In particular we can calculate this expectation as
V () =

h1 X X
X

P (st ,~ t |,b0 )

t=0 ~ t 
~ t st S

X

R(st ,at )P (at |~ t ),

(2.6)

at A

where P (at |~ t ) is the probability of a as specified by , and where P (st ,~ t |,b0 ) is recursively defined as
X
P (st ,~ t |,b0 ) =
P (st ,~ t |st1 ,~ t1 ,)P (st1 ,~ t1 |,b0 ),
(2.7)
st1 S

with
P (st ,~ t |st1 ,~ t1 ,) = P (ot |at1 ,st )P (st |st1 ,at1 )P (at1 |~ t1 )

(2.8)

a term that is completely specified by the transition and observation model and the joint
policy. For stage 0 we have that P (s0 ,~ |,b0 ) = b0 (s0 ).
Because of the recursive nature of P (st ,~ t |,b0 ) it is more intuitive to specify the value
recursively:


X
X X
V (st ,~ t ) =
P (at |~ t ) R(st ,at ) +
P (st+1 ,ot+1 |st ,at )V (st+1 ,~ t+1 ) ,
at A

st+1 S ot+1 O

(2.9)

with ~ t+1 = (~ t ,at ,ot+1 ). The value of joint policy  is then given by
X
V () =
V (s0 ,~ )b0 (s0 ).

(2.10)

s0 S

For the special case of evaluating a pure joint policy , eq. (2.6) can be written as:
V () =

h1 X
X

P (~ t |,b0 )R(~ t ,(~ t )),

(2.11)

t=0 ~ t 
~t

where
R(~ t ,at ) =

X

R(st ,at )P (st |~ t ,b0 )

(2.12)

st S

denotes the expected immediate reward. In this case, the recursive formulation (2.9) reduces
to
X X

Vt (st ,~o t ) = R st ,(~o t ) +
P (st+1 ,ot+1 |st ,(~o t ))Vt+1 (st+1 ,~o t+1 ).
(2.13)
st+1 S ot+1 O

302

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs

Note that, when performing the computation of the value for a joint policy recursively,
intermediate results should be cached. A particular (st+1 ,~o t+1 )-pair (or (st+1 ,~ t+1 )-pair for
a stochastic joint policy) can be reached from |S| states st of the previous stage. The value
Vt+1 (st+1 ,~o t+1 ) is the same, however, and should be computed only once.
2.4.4 Existence of an Optimal Pure Joint Policy
Although randomized policies may be useful, we can restrict our attention to pure policies
without sacrificing optimality, as shown by the following.
Proposition 2.1. A Dec-POMDP has at least one optimal pure joint policy.
Proof. See appendix A.1.

3. Overview of Dec-POMDP Solution Methods
In order to provide some background on solving Dec-POMDPs, this section gives an overview
of some recently proposed methods. We will limit this review to a number of finite-horizon
methods for general Dec-POMDPs that are related to our own approach.
We will not review the work performed on infinite-horizon Dec-POMDPs, such as the
work by Peshkin, Kim, Meuleau, and Kaelbling (2000), Bernstein, Hansen, and Zilberstein
(2005), Szer and Charpillet (2005), Amato, Bernstein, and Zilberstein (2006, 2007a). In this
setting policies are usually represented by finite state controllers (FSCs). Since an infinitehorizon Dec-POMDP is undecidable (Bernstein et al., 2002), this line of work, focuses on
finding -approximate solutions (Bernstein, 2005) or (near-) optimal policies for given a
particular controller size.
There also is a substantial amount of work on methods exploiting particular independence assumptions. In particular, transition and observation independent Dec-MDPs
(Becker et al., 2004b; Wu & Durfee, 2006) and Dec-POMDPs (Kim, Nair, Varakantham,
Tambe, & Yokoo, 2006; Varakantham et al., 2007) have received quite some attention.
These models assume that each agent i has an individual state space Si and that the actions of one agent do not influence the transitions between the local states of another agent.
Although such models are easier to solve, the independence assumptions severely restrict
their applicability. Other special cases that have been considered are, for instance, goal
oriented Dec-POMDPs (Goldman & Zilberstein, 2004), event-driven Dec-MDPs (Becker,
Zilberstein, & Lesser, 2004a), Dec-MDPs with time and resource constraints (Beynier &
Mouaddib, 2005, 2006; Marecki & Tambe, 2007), Dec-MDPs with local interactions (Spaan
& Melo, 2008) and factored Dec-POMDPs with additive rewards (Oliehoek, Spaan, Whiteson, & Vlassis, 2008).
A final body of related work which is beyond the scope of this article are models and
techniques for explicit communication in Dec-POMDP settings (Ooi & Wornell, 1996; Pynadath & Tambe, 2002; Goldman & Zilberstein, 2003; Nair, Roth, & Yohoo, 2004; Becker,
Lesser, & Zilberstein, 2005; Roth, Simmons, & Veloso, 2005; Oliehoek, Spaan, & Vlassis,
2007b; Roth, Simmons, & Veloso, 2007; Goldman, Allen, & Zilberstein, 2007). The DecPOMDP model itself can model communication actions as regular actions, in which case the
semantics of the communication actions becomes part of the optimization problem (Xuan,
Lesser, & Zilberstein, 2001; Goldman & Zilberstein, 2003; Spaan, Gordon, & Vlassis, 2006).
303

fiOliehoek, Spaan & Vlassis

In contrast, most approaches mentioned typically assume that communication happens outside the Dec-POMDP model and with pre-defined semantics. A typical assumption is that
at every time step the agents communicate their individual observations before selecting an
action. Pynadath and Tambe (2002) showed that, under assumptions of instantaneous and
cost-free communication, sharing individual observations in such a way is optimal.
3.1 Brute Force Policy Evaluation
Because there exists an optimal pure joint policy for a finite-horizon Dec-POMDP, it is in
theory possible to enumerate all different pure joint policies, evaluate them using equations
(2.10) and (2.13) and choose the best one. The number of pure joint policies to be evaluated
is:


n(|O |h 1)
O |A | |O |1
,
(3.1)
where |A | and |O | denote the largest individual
action and observation sets. The cost

of evaluating each policy is O |S|  |O|h . The resulting total cost of brute-force policy
evaluation is


n(|O |h 1)
nh
O |A | |O |1  |S|  |O |
,
(3.2)
which is doubly exponential in the horizon h.
3.2 Alternating Maximization
Nair et al. (2003b) introduced Joint Equilibrium based Search for Policies (JESP). This
method guarantees to find a locally optimal joint policy, more specifically, a Nash equilibrium: a tuple of policies such that for each agent i its policy i is a best response for the
policies employed by the other agents 6=i . It relies on a process we refer to as alternating
maximization. This is a procedure that computes a policy i for an agent i that maximizes
the joint reward, while keeping the policies of the other agents fixed. Next, another agent is
chosen to maximize the joint reward by finding its best-response to the fixed policies of the
other agents. This process is repeated until the joint policy converges to a Nash equilibrium,
which is a local optimum. The main idea of fixing some agents and having others improve
their policy was presented before by Chades, Scherrer, and Charpillet (2002), but they used
a heuristic approach for memory-less agents. The process of alternating maximization is
also referred to as hill-climbing or coordinate ascent.
Nair et al. (2003b) describe two variants of JESP, the first of which, Exhaustive-JESP,
implements the above idea in a very straightforward fashion: Starting from a random joint
policy, the first agent is chosen. This agent then selects its best-response policy by evaluating
the joint reward obtained for all of its individual policies when the other agents follow their
fixed policy.
The second variant, DP-JESP, uses a dynamic programming approach to compute the
best-response policy for a selected agent i. In essence, fixing the policies of all other agents
allows for a reformulation of the problem as an augmented POMDP. In this augmented
POMDP a state s = hs,~o6=i i consists of a nominal state s and the observation histories of
304

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs

the other agents ~o6=i . Given the fixed deterministic policies of other agents 6=i , such an
augmented state s is a Markovian state, and all transition and observation probabilities can
easily be derived from 6=i .
Like most methods proposed for Dec-POMDPs, JESP exploits the knowledge of the
initial belief b0 by only considering reachable beliefs b(s) in the solution of the POMDP.
However, in some cases the initial belief might not be available. As demonstrated by
Varakantham, Nair, Tambe, and Yokoo (2006), JESP can be extended to plan for the entire
space of initial beliefs, overcoming this problem.
3.3 MAA
Szer et al. (2005) introduced a heuristically guided policy search method called multiagent
A* (MAA ). It performs a guided A*-like search over partially specified joint policies,
pruning joint policies that are guaranteed to be worse than the best (fully specified) joint
policy found so far by an admissible heuristic.
In particular MAA considers joint policies that are partially specified with respect
to time: a partial joint policy t = ( 0 , 1 , . . . , t1 ) specifies the joint decision rules for
the first t stages. For such a partial joint policy t a heuristic value Vb (t ) is calculated
by taking V 0...t1 (t ), the actual expected reward t achieves over the first t stages, and
adding Vb t...h1 , a heuristic value for the remaining h  t stages. Clearly when Vb t...h1 is an
admissible heuristica guaranteed overestimationso is Vb (t ).
MAA starts by placing the completely unspecified joint policy 0 in an open list.
Then, it proceeds by selecting partial joint policies t = ( 0 , 1 , . . . , t1 ) from the list and
expanding them: generating all t+1 = ( 0 , 1 , . . . , t1 , t ) by appending all possible joint
decision rules  t for next time step (t). The left side of Figure (3) illustrates the expansion
process. After expansion, all created children are heuristically valuated and placed in the
open list, any partial joint policies t+1 with Vb (t+1 ) less than the expected value V () of
some earlier found (fully specified) joint policy , can be pruned. The search ends when the
list becomes empty, at which point we have found an optimal fully specified joint policy.
3.4 Dynamic Programming for Dec-POMDPs
MAA incrementally builds policies from the first stage t = 0 to the last t = h  1. Prior to
this work, Hansen et al. (2004) introduced dynamic programming (DP) for Dec-POMDPs,
which constructs policies the other way around: starting with a set of 1-step policies
(actions) that can be executed at the last stage, they construct a set of 2-step policies to
be executed at h  2, etc.
It should be stressed that the policies maintained are quite different from those used
by MAA . In particular a partial policy in MAA has the form t = ( 0 , 1 , . . . , t1 ). The
policies maintained by DP do not have such a correspondence to decision rules. We define
the time-to-go  at stage t as
 = h  t.
(3.3)
Now qi =k denotes a k-steps-to-go sub-tree policy for agent i. That is, qi =k is a policy
tree that has the same form as a full policy for the horizon-k problem. Within the original
horizon-h problem qi =k is a candidate for execution starting at stage t = hk. The set of ksteps-to-go sub-tree policies maintained for agent i is denoted Qi =k . Dynamic programming
305

fiOliehoek, Spaan & Vlassis

a

a
o

o

o

a

o

a

a

a

o

o

o

o

o

o

o

o

a

a

a

a

a

a

a

a

Figure 3: Difference between policy construction in MAA (left) and dynamic programming
(right) for an agent with actions a,a and observations o,o. The dashed components are newly generated, dotted components result from the previous iteration.
MAA expands a partial policy from the leaves, while dynamic programming
backs up a set of sub-tree policies forming new ones.

for Dec-POMDPs is based on backup operations: constructing Qi =k+1 a set of sub-tree
policies qi =k+1 from a set Qi =k . For instance, the right side of Figure 3 shows how qi =3 , a
3-steps-to-go sub-tree policy, is constructed from two qi =2  Qi =2 . Also illustrated is the
difference between this process and MAA expansion (on the left side).
Dynamic programming consecutively constructs Qi =1 ,Qi =2 , . . . ,Qi =h for all agents i.
However, the size of the set Qi =k+1 is given by
|Qi =k+1 | = |Ai | |Qi =k ||Oi | ,
and as a result the sizes of the maintained sets grow doubly exponential with k. To counter
this source of intractability, Hansen et al. (2004) propose to eliminate dominated sub-tree
policies. The expected reward of a particular sub-tree policy qi =k depends on the probability
over states when qi =k is started (at stage t = h  k) as well as the probability with which
 =k denote a
the other agents j 6= i select their sub-tree policies qj =k  Qj =k . If we let q6=
i

=k
sub-tree profile for all agents but i, and Q6=i the set of such profiles, we can say that qi =k
is dominated if it is not maximizing at any point in the multiagent belief space: the simplex
 =k . Hansen et al. test for dominance over the entire multiagent belief space by
over S  Q6=
i
linear programming. Removal of a dominated sub-tree policy qi =k of an agent i may cause
a sub-tree policy qj =k of an other agent j to become dominated. Therefore Hansen et al.
propose to iterate over agents until no further pruning is possible, a procedure known as
iterated elimination of dominated policies (Osborne & Rubinstein, 1994).
Finally, when the last backup step is completed the optimal policy can be found by
evaluating all joint policies   Q1 =h      Qn =h for the initial belief b0 .
306

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs

3.5 Extensions on DP for Dec-POMDPs
In the last few years several extensions to the dynamic programming algorithm for DecPOMDPs have been proposed. The first of these extensions is due to Szer and Charpillet
(2006). Rather than testing for dominance over the entire multiagent belief space, Szer
and Charpillet propose to perform point-based dynamic programming (PBDP). In order
to prune the set of sub-tree policies Qi =k , the set of all the belief points Bi,reachable 
 =k ) that can possibly be reached by deterministic joint policies are generated.
P(S  Q6=
i
Only the sub-tree policies qi =k that maximize the value at some bi  Bi,reachable are kept.
The proposed algorithm is optimal, but intractable because it needs to generate all the
multiagent belief points that are reachable through all joint policies. To overcome this
bottleneck, Szer and Charpillet propose to randomly sample one or more joint policies and
use those to generate Bi,reachable .
Seuken and Zilberstein (2007b) also proposed a point-based extension of the DP algorithm, called memory-bounded dynamic programming (MBDP). Rather than using a
randomly selected policy to generate the belief points, they propose to use heuristic policies. A more important difference, however, lies in the pruning step. Rather than pruning
dominated sub-tree policies qi =k , MBDP prunes all sub-tree policies except a few in each
iteration. More specifically, for each agent maxTrees sub-tree policies are retained, which
is a parameter of the planning method. As a result, MBDP has only linear space and time
complexity with respect to the horizon. The MBDP algorithm still depends on the exhaus|Oi | sub-tree policies.
tive generation of the sets Qi =k+1 which now contain
n |Ai | maxT rees
|O
|
Moreover, in each iteration all |A | maxT rees 
joint sub-tree policies have to be evaluated for each of the sampled belief points. To counter this growth, Seuken and Zilberstein
(2007a) proposed an extension that limits the considered observations during the backup
step to the maxObs most likely observations.
Finally, a further extension of the DP for Dec-POMDPs algorithm is given by Amato,
Carlin, and Zilberstein (2007b). Their approach, bounded DP (BDP), establishes a bound
not on the used memory, but on the quality of approximation. In particular, BDP uses
-pruning in each iteration. That is, a qi =k that is maximizing in some region of the
multiagent belief space, but improves the value in this region by at most , is also pruned.
Because iterated elimination using - pruning can still lead to an unbounded reduction in
value, Amato et al. propose to perform one iteration of -pruning, followed by iterated
elimination using normal pruning.
3.6 Other Approaches for Finite-Horizon Dec-POMDPs
There are a few other approaches for finite-horizon Dec-POMDPs, which we will only briefly
describe here. Aras, Dutech, and Charpillet (2007) proposed a mixed integer linear programming formulation for the optimal solution of finite-horizon Dec-POMDPs. Their approach is based on representing the set of possible policies for each agent in sequence form
(Romanovskii, 1962; Koller, Megiddo, & von Stengel, 1994; Koller & Pfeffer, 1997). In sequence form, a single policy for an agent i is represented as a subset of the set of sequences
(roughly corresponding to action-observation histories) for the agent. As such the problem
can be interpreted as a combinatorial optimization problem, which Aras et al. propose to
solve with a mixed integer linear program.
307

fiOliehoek, Spaan & Vlassis

Oliehoek, Kooij, and Vlassis (2007a) also recognize that finding a solution for DecPOMDPs in essence is a combinatorial optimization problem and propose to apply the
Cross-Entropy method (de Boer, Kroese, Mannor, & Rubinstein, 2005), a method for combinatorial optimization that recently has become popular because of its ability to find
near-optimal solutions in large optimization problems. The resulting algorithm performs a
sampling-based policy search for approximately solving Dec-POMDPs. It operates by sampling pure policies from an appropriately parameterized stochastic policy, and then evaluates
these policies either exactly or approximately in order to define the next stochastic policy
to sample from, and so on until convergence.
Finally, Emery-Montemerlo et al. (2004, 2005) proposed to approximate Dec-POMDPs
through series of Bayesian games. Since our work in this article is based on the same
representation, we defer a detailed explanation to the next section. We do mention here
that while Emery-Montemerlo et al. assume that the algorithm is run on-line (interleaving
planning and execution), no such assumption is necessary. Rather we will apply the same
framework during a off-line planning phase, just like the other algorithms covered in this
overview.

4. Optimal Q-value Functions
In this section we will show how a Dec-POMDP can be modeled as a series of Bayesian
games (BGs). A BG is a game-theoretic model that can deal with uncertainty (Osborne
& Rubinstein, 1994). Bayesian games are similar to the more well-known normal form, or
matrix games, but allow to model agents that have some private information. This section
will introduce Bayesian games and show how a Dec-POMDP can be modeled as a series
of Bayesian games (BGs). This idea of using a series of BGs to find policies for a DecPOMDP has been proposed in an approximate setting by Emery-Montemerlo et al. (2004).
In particular, they showed that using series of BGs and an approximate payoff function,
they were able to obtain approximate solutions on the Dec-Tiger problem, comparable to
results for JESP (see Section 3.2).
The main result of this section is that an optimal Dec-POMDP policy can be computed
from the solution of a sequence of Bayesian games, if the payoff function of those games
coincides with the Q-value function of an optimal policy   , i.e., with the optimal Qvalue function Q . Thus, we extend the results of Emery-Montemerlo et al. (2004) to
include the optimal setting. Also, we conjecture that this form of Q can not be computed
without already knowing an optimal policy   . By transferring the game-theoretic concept
of sequential rationality to Dec-POMDPs, we find a description of Q that is computable
without knowing   up front.
4.1 Game-Theoretic Background
Before we can explain how Dec-POMDPs can be modeled using Bayesian games, we will
first introduce them together with some other necessary game theoretic background.
308

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs

D
C

D
1,  1
0, + 2

C
+2,0
+1, + 1

A
B

A
+2
0

B
0
+2

Figure 4: Left: The game Chicken. Both players have the option to (D)rive on or (C)hicken
out. Right: The meeting location problem. Because the game has identical
payoffs, each entry contains just one number.

4.1.1 Strategic Form Games and Nash Equilibria
At the basis of the concept of a Bayesian game lies a simpler form of game: the strategic- or
normal form game. A strategic game consists of a set of agents or players, each of which has
a set of actions (or strategies). The combination of selected actions specifies a particular
outcome. When a strategic game consists of two agents, it can be visualized as a matrix
as shown in Figure 4. The first game shown is called Chicken and involves two teenagers
who are driving head on. Both have the option to drive on or chicken out. Each teenagers
payoff is maximal (+2) when he drives on and his opponent chickens out. However, if both
drive on, a collision follows giving both a payoff of 1. The second game is the meeting
location problem. Both agents want to meet in location A or B. They have no preference
over which location, as long as both pick the same location. This game is fully cooperative,
which is modeled by the fact that the agents receive identical payoffs.
Definition 4.1. Formally, a strategic game is a tuple hn,A,ui, where n is the number of
agents, A = i Ai is the set of joint actions, and u = hu1 , . . . ,un i with ui : A  R is the
payoff function of agent i.
Game theory tries to specify for each agent how to play. That is, a game-theoretic
solution should suggest a policy for each agent. In a strategic game we write i to denote
a policy for agent i and  for a joint policy. A policy for agent i is simply one of its actions
i = ai  Ai (i.e., a pure policy), or a probability distribution over its actions i  P(Ai )
(i.e., a mixed policy). Also, the policy suggested to each agent should be rational given
the policies suggested to the other agent; it would be undesirable to suggest a particular
policy to an agent, if it can get a better payoff by switching to another policy. Rather, the
suggested policies should form an equilibrium, meaning that it is not profitable for an agent
to unilaterally deviate from its suggested policy. This notion is formalized by the concept
of Nash equilibrium.
Definition 4.2. A pure policy profile  = h1 , . . . ,i , . . . ,n i specifying a pure policy for
each agent is a Nash Equilibrium (NE) if and only if
ff


(4.1)
ui (h1 , . . . ,i , . . . ,n i)  ui ( 1 , . . . ,i , . . . ,n ), i:1in , i Ai .
This definition can be easily extended to incorporate mixed policies by defining
ui (h1 , . . . ,n i) =

X

ui (ha1 , . . . ,an i)

ha1 ,...,,an i

309

n
Y
i=1

Pi (ai ).

fiOliehoek, Spaan & Vlassis

Nash (1950) proved that when allowing mixed policies, every (finite) strategic game contains
at least one NE, making it a proper solution for a game. However, it is unclear how such a
NE should be found. In particular, there may be multiple NEs in a game, making it unclear
which one to select. In order to make some discrimination between Nash equilibria, we can
consider NEs such that there is no other NE that is better for everyone.
Definition 4.3. A Nash Equilibrium  = h1 , . . . ,i , . . . ,n i is referred to as Pareto Optimal (PO) when there is no other NE  that specifies at least the same payoff for all agents
and a higher payoff for at least one agent:



i ui ( )  ui ()  i ui ( ) > ui () .

In the case when multiple Pareto optimal Nash equilibria exist, the agents can agree
beforehand on a particular ordering, to ensure the same NE is chosen.
4.1.2 Bayesian Games
A Bayesian game (Osborne & Rubinstein, 1994) is an augmented normal form game in
which the players hold some private information. This private information defines the type
of the agent, i.e., a particular type i  i of an agent i corresponds to that agent knowing
some particular information. The payoff the agents receive now no longer only depends on
their actions, but also on their private information. Formally, a BG is defined as follows:
Definition 4.4. A Bayesian game (BG) is a tuple hn,A,,P (), hu1 ,...un ii, where n is the
number of agents, A is the set of joint actions,  = i i is the set of joint types over
which a probability function P () is specified, and ui :   A  R is the payoff function
of agent i.
In a normal form game the agents select an action. Now, in a BG the agents can
condition their action on their private information. This means that in BGs the agents
use a different type of policies. For a BG, we denote a joint policy  = h1 ,...,n i, where
the individual policies are mappings from types to actions: i : i  Ai . In the case of
identical payoffs for the agents, the solution of a BG is given by the following theorem:
Theorem 4.1. For a BG with identical payoffs, i.e., i,j  a ui (,a) = uj (,a), the solution
is given by:
X
  = arg max
P ()u(,()),
(4.2)




where () = h1 (1 ),...,n (n )i is the joint action specified by  for joint type . This
solution constitutes a Pareto optimal Nash equilibrium.
Proof. The proof consists of two parts: the first shows that   is a Nash equilibrium, the
second shows it is Pareto optimal.
Nash equilibrium proof. It is clear that   satisfying 4.2 is a Nash equilibrium by
rewriting from the perspective of an arbitrary agent i as follows:
310

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs

i

"

= arg max max
i

"

6=i

= arg max max
i

"

6=i

= arg max max
i

= arg max
i

6=i

X

X

#

P ()u(,()) ,



XX
X

P (i )

i

P (i )

X
6=i

i

P (6=i |i )

6=i

i

X

"
|

X
6=i

#

#

P (hi ,6=i i) u(,()) ,
{z

P (i )

#

}

P (6=i |i )u(,()) ,

6=i



ff

P (6=i |i )u(hi ,6=i i , i (i ),6=
i (6=i ) ),

 . Since no special assumptions were made
which means that i is a best response for 6=
i
on i, it follows that   is a Nash equilibrium.

Pareto optimality proof. Let us write Vi (ai ,6=i ) for the payoff agent i expects for i
when performing ai when the other agents use policy profile 6=i . We have that
Vi (ai ,6=i ) =

X

P (6=i |i )u(hi ,6=i i , hai ,6=i (6=i )i).

6=i

Now, a joint policy   satisfying (4.2) is not Pareto optimal if and only if there is another
Nash equilibrium   that attains at least the same payoff for all agents i and for all types i
and strictly more for at least one agent and type. Formally   is not Pareto optimal when
  such that:
i i








Vi (i (i ),6=
i )  Vi (i (i ),6=i )  i i Vi (i (i ),6=i ) < Vi (i (i ),6=i ). (4.3)

 i is a
We prove that no such   can exist by contradiction. Suppose that   = hi  ,6=
i
NE such that (4.3) holds (and thus   is not Pareto optimal). Because   satisfies (4.2) we
know that:
X
X
P ()u(,  ()),
(4.4)
P ()u(,  ()) 




and therefore, for all agents i




P (i,1 )Vi,1 (i (i,1 ),6=
i ) + ... + P (i,|i | )Vi,|i | (i (i,|i | ),6=i ) 



P (i,1 )Vi,1 (i (i,1 ),6=
i ) + ... + P (i,|i | )Vi,|i | (i (i,|i | ),6=i )

holds. However, by assumption that   satisfies (4.3) we get that
j




Vi,j (i (i,j ),6=
i ) < Vi,j (i (i,j ),6=i ).

Therefore it must be that
X
X


P (i,k )Vi,k (i (i,k ),6=
P (i,k )Vi,k (i (i,k ),6=
i ),
i) >
k6=j

k6=j

311

fiOliehoek, Spaan & Vlassis

t=0

joint actions
joint observations
joint act.-obs. history

ha1 ,a2 i

ha1 ,a2 i
ha1 ,a2 i
t=1
ho1 ,o2 i

ha1 ,a2 i
ho1 ,o2 i

ho1 ,o2 i
ho1 ,o2 i

Figure 5: A Dec-POMDP can be seen as a tree of joint actions and observations. The
indicated planes correspond with the Bayesian games for the first two stages.

and thus that
k




Vi,k (i (i,k ),6=
i ) > Vi,k (i (i,k ),6=i ),

contradicting the assumption that   satisfies (4.3).
4.2 Modeling Dec-POMDPs with Series of Bayesian Games
Now we will discuss how Bayesian games can be used to model Dec-POMDPs. Essentially,
a Dec-POMDP can be seen as a tree where nodes are joint action-observation histories
and edges represent joint actions and observations, as illustrated in Figure 5. At a specific
stage t in a Dec-POMDP, the main difficulty in coordinating action selection is presented
by the fact that each agent has its own individual action-observation history. That is, there
is no global signal that the agents can use to coordinate their actions. This situation can
be conveniently modeled by a Bayesian game as we will now discuss.
At a time step t, one can directly associate the primitives of a Dec-POMDP with those
of a BG with identical payoffs: the actions of the agents are the same in both cases, and
~ t . Figure 6 shows
the types of agent i correspond to its action-observation histories i  
i
the Bayesian games for t = 0 and t = 1 for a fictitious Dec-POMDP with 2 agents.
We denote the payoff function of the BG that models a stage of a Dec-POMDP by
~
Q( t ,a). This payoff function should be naturally defined in accordance with the value
function of the planning task. For instance, Emery-Montemerlo et al. (2004) define Q(~ t ,a)
312

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs

as the QMDP -value of the underlying MDP. We will more extensively discuss the payoff
function in Section 4.3.
The probability P () is equal to the probability of the joint action-observation history
to which  corresponds and depends on the past joint policy t = ( 0 , . . . , t1 ) and the
initial state distribution. It can be calculated as the marginal of (2.7):
P () = P (~ t |t ,b0 ) =

X

P (st ,~ t |t ,b0 ).

(4.5)

st S

~
When only considering pure joint policies t , the action probability component P (a|)
in (2.7) is 1 for joint action-observation histories ~ t that are consistent with the past joint
policy t and 0 otherwise. We say that an action-observation ~i history is consistent with
a pure policy i if it can occur when executing i , i.e., when the actions in ~i would be
selected by i . Let us more formally define this consistency as follows.

Definition 4.5 (Consistency). Let us write ~it for the restriction of ~it to stage 0, . . . ,t
(with 0  t < t). An action-observation history ~it of agent i is consistent with a pure
policy i if and only if at each time step t with 0  t < t



i (~it ) = i (~oit ) = ati

is the (t + 1)-th action in ~it . A joint action-observation history ~ t = h~1t , . . . ,~nt i is consistent with a pure joint policy  = h1 , . . . ,n i if each individual ~it is consistent with the
corresponding individual policy i . C is the indicator function for consistency. For instance
C(~ t ,) filters out the action-observation histories ~ t that are inconsistent with a joint
pure policy :
C(~ t ,) =

(

1 , ~ t = o0 ,(o0 ),o1 ,(o0 ,o1 ),... )
0 , otherwise.

(4.6)

~ t  {~ t | C(~ t ,) = 1} for the set of ~ t consistent with .
We will also write 
This definition allows us to write
P (~ t |t ,b0 ) = C(~ t ,t )

X

P (st ,~ t |b0 )

(4.7)

st S

with
P (st ,~ t |b0 ) =

X

P (ot |at1 ,st )P (st |st1 ,at1 )P (st1 ,~ t1 |b0 ).

(4.8)

st1 S

Figure 6 illustrates how the indicator function filters out policies, when  t=0 (~ t=0 ) =
ha1 ,a2 i, only the non-shaded part of the BG for t = 1 can be reached (has positive probability).
313

fiOliehoek, Spaan & Vlassis

~2t=0
~1t=0
()
~2t=1
~1t=1
(a1 ,o1 )
(a1 ,o1 )
(a1 ,o1 )
(a1 ,o1 )

a1
a1
a1
a1
a1
a1
...

a1
a1

()
a2
a2
+2.75 4.1
0.9
+0.3

(a2 ,o2 )
a2
a2
0.3
+0.6
0.6
+2.0
+3.1
+4.4
+1.1
2.9
0.4
0.9
0.9
4.5
...
...

(a2 ,o2 )
a2
a2
0.6
+4.0
1.3
+3.6
1.9
+1.0
+2.0
0.4
0.5
1.0
1.0
+3.5
...
...

...
...
...
...
...
...
...

Figure 6: The Bayesian game for the first and second time step (top: t = 0, bottom: t = 1).
The entries ~ t , at are given by the payoff function Q(~ t ,at ). Light shaded entries
indicate the solutions. Dark entries will not be realized given ha1 ,a2 i the solution
of the BG for t = 0.

4.3 The Q-value Function of an Optimal Joint Policy
Given the perspective of a Dec-POMDP interpreted as a series of BGs as outlined in the
previous section, the solution of the BG for stage t is a joint decision rule  t . If the payoff
function for the BG is chosen well, the quality of  t should be high. Emery-Montemerlo
et al. (2004) try to find a good joint policy  = ( 0 , . . . , h1 ) by a procedure we refer to
as forward-sweep policy computation (FSPC): in one sweep forward through time, the BG
for each stage t = 0,1, . . . ,h  1 is consecutively solved. As such, the payoff function for the
BGs constitute what we call a Q-value function for the Dec-POMDP.
Here, we show that there is an optimal Q-value function Q : when using this Q as
the payoff functions for the BGs, forward-sweep policy computation will lead to an optimal
joint policy   = ( 0, , . . . , h1, ). We first give a derivation of this Q . Next, we will
discuss that Q can indeed be used to calculate   , but computing Q seems impractical
without already knowing an optimal joint policy   . This issue will be further addressed in
Section 4.4.
4.3.1 Existence of Q
We now state a theorem identifying a normative description of Q as the Q-value function
for an optimal joint policy.
Theorem 4.2. The expected cumulative reward over stages t, . . . ,h  1 induced by   , an
optimal joint policy for a Dec-POMDP, is given by:
V t (  ) =

X

P (~ t |b0 )Q (~ t ,  (~ t )),

~t
~ t 


314

(4.9)

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs



ff
where ~ t = ~o t ,~a t , where   (~ t ) =   (~o t ) denotes the joint action that pure joint policy
  specifies for ~o t , and where
X
Q (~ t ,a) = R(~ t ,a) +
P (ot+1 |~ t ,a)Q (~ t+1 ,  (~ t+1 ))
(4.10)
ot+1 O

is the Q-value function for   , which gives the expected cumulative future reward when
taking joint action a at ~ t given that an optimal joint policy   is followed hereafter.
Proof. By filling out (2.11) for an optimal pure joint
policy   , we obtain its expected
fi

cumulative reward as the summation of E R(st ,at )fi  the expected rewards it yields for
each time step:


V ( ) =

h1
X
t=0

h1 X
fi  X

t t fi 
P (~ t |  ,b0 )R(~ t ,  (~ t )).
E R(s ,a )  =

(4.11)

t=0 ~ t 
~t

In this equation, P (~ t |  ,b0 ) is given by (4.7). As a result, the influence of   on P (~ t |  ,b0 )
is only through C. I.e.,   is only used to filter out inconsistent histories. Therefore we
can write:
X
fi 

P (~ t |b0 )R(~ t ,  (~ t )),
(4.12)
E R(st ,at )fi  =
~t
~ t 


where P (~ t |b0 ) is given by directly taking the marginal of (4.8). Now, let us define the
value starting from time step t:

X
fi 

P (~ t |b0 )R(~ t ,  (~ t )) + V t+1 (  ). (4.13)
V t (  ) = E R(st ,at )fi  + V t+1 (  ) =
~t
~ t 


For the last time step h  1 there is no expected future reward, so we get:
X
P (~ h1 |b0 ) R(~ h1 ,  (~ h1 )) .
V h1 (  ) =
|
{z
}
h1
~ 
~ h1 


(4.14)

Q (~ h1 ,  (~ h1 ))

For time step h  2 this becomes:

h
fi i
V h2 (  )  E R(sh2 ,ah2 )fi  + V h1 (  ) =
X
X
P (~ h2 |b0 )R(~ h2 ,  (~ h2 )) +

P (~ h1 |b0 )Q (~ h1 ,  (~ h1 )).

~ h1
~ h1 


~ h2
~ h2 


(4.15)
Because P (~ h1 ) = P (~ h2 )P (oh1 |~ h2 ,  (~ h2 )), (4.15) can be rewritten to:
V h2 (  ) =

X

P (~ h2 |b0 )Q (~ h2 ,  (~ h2 )),

~ h2
~ h2 


315

(4.16)

fiOliehoek, Spaan & Vlassis

with
Q (~ h2 ,  (~ h2 )) = R(~ h2 ,  (~ h2 ))+
X
P (oh1 |~ h2 ,  (~ h2 ))Q (~ h1 ,  (~ h1 )). (4.17)
oh1

Reasoning in the same way we see that (4.9) and (4.10) constitute a generic expression for
the expected cumulative future reward starting from time step t.
Note that in the above derivation, we explicitly included b0 as one of the given arguments.
In the rest of this text, we will always assume b0 is given and therefore omit it, unless
necessary.
4.3.2 Deriving an Optimal Joint Policy from Q
At this point we have derived Q , a Q-value function for an optimal joint policy. Now, we
extend the results of Emery-Montemerlo et al. (2004) into the exact setting:
Theorem 4.3. Applying forward-sweep policy computation using Q as defined by (4.10)
yields an optimal joint policy.
Proof. Note that, per definition, the optimal Dec-POMDP policy   maximizes the expected
future reward V t (  ) specified by (4.9). Therefore  t, , the optimal decision rule for stage t,
is identical to an optimal joint policy  t, for the Bayesian game for time step t, if the payoff
function of the BG is given by Q , that is:
X

 t,   t, = arg max
t

P (~ t )Q (~ t , t (~ t )).

(4.18)

~t
~ t 


Equation (4.18) tells us that  t,   t, . This means that it is possible to construct the
complete optimal Dec-POMDP policy   = ( 0, , . . . , h1, ), by computing  t, for all t.
A subtlety in the calculation of   is that (4.18) itself is dependent on an optimal joint
~ t   {~ t | C(~ t ,  ) = 1}. This is resolved
policy, as the summation is over all ~ t  

by realizing that only the past actions influence which action-observation histories can be
reached at time step t. Formally, let t = ( 0, , . . . , t1, ) denote the past joint policy, which
is a partial joint policy  specified for stages 0,...,t  1. If we denote the optimal past joint
~t = 
~ t t, , and therefore that:
policy by t, , we have that 


 t, = arg max
t

X

P (~ t )Q (~ t , t (~ t )).

(4.19)

~t
~ t 
t,


This can be solved in a forward manner for time steps t = 0,1,2,...,h  1, because at every
time step t, = ( 0, , . . . , t1, ) will be available: it is specified by ( 0, ,..., t1, ) the
solutions of the previously solved BGs.
316

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs

4.3.3 Computing Q
So far we discussed that Q can be used to find an optimal joint policy   . Unfortunately,
when an optimal joint policy   is not known, computing Q itself is impractical, as we
will discuss here. This is in contrast with the (fully observable) single-agent case where the
optimal Q-values can be found relatively easily in a single sweep backward through time.
For MDPs and POMDPs we can compute the Q-values for time step t from those for
t + 1 by applying a backup operator. This is possible because there is a single agent that
perceives a Markovian signal. This allows the agent to (1) select the optimal action (policy)
for the next time step and (2) determine the expected future reward given the optimal
action (policy) found in step 1. For instance, the backup operator for a POMDP is given
by:
X
Q (bt ,a) = R(bt ,a) +
P (o|bt ,a) max Q (bt+1 ,a),
a

o

which can be rewritten as a 2-step procedure:

1.  t+1, (bt+1 ) = arg maxa Q (bt+1 ,a )
P
2. Q (bt ,a) = R(bt ,a) + o P (o|bt ,a)Q (bt+1 , t+1, (bt+1 )).

In the case of Dec-POMDPs, step 2 would correspond to calculating Q using (4.10) and
thus depends on  t+1, an optimal joint policy at the next stage. However, step 1 that
calculates  t+1, , corresponds to (4.19) and therefore is dependent on t+1, (an optimal
joint policy for time steps 0,...,t). So to calculate the Qt, the optimal Q-value function
as specified by (4.10) for stage t, an optimal joint policy up to and including stage t is
needed. Effectively, there is a dependence on both the future and the past optimal policy,
rather than only on the future optimal policies as in the single agent case. The only clear
solution seems to be evaluation for all possible past policies, as detailed next. We conjecture
that the problem encountered here is inherent to all decentralized decision making with
imperfect information. For example, we can also observe this in exact point-based dynamic
programming for Dec-POMDPs, as described in Section 3.5, where it is necessary to to
generate all (multiagent belief points generated by all) possible past policies.
4.4 Sequential Rationality for Dec-POMDPs
We conjectured that computing Q as introduced in Section 4.3 seems impractical without
knowing   . Here we will relate this to concepts from game theory. In particular, we
discuss a different formulation of Q based on the principle of sequential rationality, i.e.,
also considering joint action-observation histories that are not realized given an optimal
joint policy. This formulation of Q is computable without knowing an optimal joint policy
in advance, and we present a dynamic programming algorithm to perform this computation.
4.4.1 Sub-game Perfect and Sequential Equilibria
The problem we are facing is very much related to the notion of sub-game perfect equilibria
from game theory. A sub-game perfect Nash equilibrium  = h1 , . . . ,n i has the characteristic that the contained policies i specify an optimal action for all possible situationseven
317

fiOliehoek, Spaan & Vlassis

situations that can not occur when following . A commonly given rationale behind this
concept is that, by a mistake of one of the agents during execution, situations that should
not occur according to , can occur, and also in these situations the agents should act optimally. A different rationale is given by Binmore (1992), who remarks that it is tempting
to shrug ones shoulders at these difficulties [because] rational players will not stray from
the equilibrium path, but that would clearly be a mistake, because the agents remain
on the equilibrium path because of what they anticipate would happen if they were to
deviate. This implies that agents can decide upon a Nash equilibrium by analyzing what
the expected outcome would be by following other policies: That is, when acting optimally
from other situations. We will perform a similar reasoning here for Dec-POMDPs, which
in a similar fashionwill result in a description that allows to deduce an optimal Q-value
function and thus joint policy.
A Dec-POMDP can be modeled as an extensive form game of imperfect information
(Oliehoek & Vlassis, 2006). For such games, the notion of sub-game perfect equilibria is
inadequate; because this type of games often do not contain proper sub-games, every Nash
equilibrium is trivially sub-game perfect.4 To overcome this problem different refinements of
the Nash equilibrium concept have been defined, of which we will mention the assessment
equilibrium (Binmore, 1992) and the closely related, but stronger sequential equilibrium
(Osborne & Rubinstein, 1994). Both these equilibria are based on the concept of an assessment, which is a pair h,bi consisting of a joint policy  and a belief system b. The belief
system maps each possible situation, or information set, of an agentalso the ones that are
not reachable given to a probability distribution over possible joint histories. Roughly
speaking, an assessment equilibrium requires sequential rationality and belief consistency.5
The former entails that the joint policy  specifies optimal actions for each information
set given b. Belief consistency means that all the beliefs that are assigned by b are Bayes
rational given the specified joint policy . For instance, in the context of Dec-POMDPs b
would prescribe, for a particular ~it of agent i, a belief over joint histories P (~ t |~it ). If all
beliefs prescribed by belief system b are Bayes-rational (i.e., computed as the appropriate
conditionals of (4.5)), b is called belief consistent.6
4.4.2 Sequential Rationality and the Optimal Q-value Function
The dependence of sequential rationality on a belief system b indicates that the optimal
action at a particular point is dependent on the probability distribution over histories. In
Section 4.3.3 we encountered a similar dependence on the history as specified by t+1, .
Here we will make this dependence more exact.
At a particular stage t, a policy is optimal or, in game-theoretic terms, rational if
it maximizes the expected return from that point on. In Section 4.3.1, we were able to
express this expected return as Q (~ t ,a) assuming an optimal joint policy   is followed up
4. The extensive form of a Dec-POMDP indeed does not contain proper sub-games, because agent can
never discriminate between the other agents observations.
5. Osborne and Rubinstein (1994) refer to this second requirement as simply consistency. In order to
avoid any confusion with definition 4.5 we will use the term belief consistency.
6. A sequential equilibrium includes a more technical part in the definition of belief consistency that addresses what beliefs should be held for information sets that are not reached according to . For more
information we refer to Osborne and Rubinstein (1994).

318

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs

to the current stage t. However, when no such previous policy is assumed, the maximal
expected return is not defined.
Proposition 4.1. For a pair (~ t ,at ) with t < h  1 the optimal value Q (~ t ,at ) cannot
be

defined without assuming some (possibly randomized) past policy t+1 =  0 , . . . , t . Only
for the last stage t = h  1 such expected reward is defined as
Q (~ h1 ,ah1 )  R(~ h1 ,ah1 )
without assuming a past policy.
Proof. Let us try to deduce Q (~ t ,at ) the optimal value for a particular ~ t assuming the
Q -values for the next time step t + 1 are known. The Q (~ t ,at )-values for each of the
possible joint actions can be evaluated as follows
a

Q (~ t ,at ) = R(~ t ,at ) +

X

P (ot+1 |~ t ,at )Q (~ t+1 , t+1, (~ t+1 )).

ot+1

where  t+1, is an optimal decision rule for the next stage. But what should  t+1, be? If
we assume that up to stage t + 1 we followed a particular (possibly randomized) t+1 ,
t+1, = arg max
 t+1

X

P (~ t+1 |t+1 ,b0 )Q (~ t+1 , t+1 (~ t+1 )).

~ t+1
~ t+1 

is optimal. However, there are many pure and infinite randomized past policies t+1 that are
consistent with ~ t ,at , leading to many t+1, that might be optimal. The conclusion we can
draw is that Q (~ t ,at ) is ill-defined without P (~ t+1 |t+1 ,b0 ), the probability distribution
(belief) over joint action-observation histories, which is induced by t+1 , the policy followed
for stages 0, . . . ,t.
Let us illustrate this by reviewing the optimal Q-value function as defined in Section 4.3.1. Consider   (~ t+1 ) in (4.10). This optimal policy is a mapping from observation
~  A induced by the individual policies and observation histories.
histories to actions   : O
This means that for two joint action-observation histories with the same joint observation
history   results in the same joint action. That is ~a,~o,~a   (h~a,~oi) =   (h~a ,~oi). Effectively
~ t  , say through a mistake7 ,   continues to
this means that when we reach some ~ t 6 

specify actions as if no mistake ever happened: That is, still assuming that   has been
~ t  . Which in
followed up to this stage t. In fact,   (~ t ) might not even be optimal if ~ t 6 


t1
t
~
~
turn means that Q ( ,a), the Q-values for predecessors of  , might not be the optimal
expected reward.
We demonstrated that the optimal Q-value function for a Dec-POMDP is not welldefined without assuming a past joint policy. We propose a new definition of Q that
explicitly incorporates t+1 .
7. The question as to how the mistake of one agent should be detected by another agent is a different
matter altogether and beyond the scope of this text.

319

fiOliehoek, Spaan & Vlassis

t=0

21

a1
o1

t=1

t=2

a2
o2

o1

a1

22

a2

a1

2

o2
a2

o1

o1

o1

o1

o2

o2

o2

o2

a1

a1

a1

a1

a2

a2

a2

a2

2,

Figure 7: Computation of sequential rational Q . 2, is the optimal decision rule for stage
t = 2, given that 2 is followed for the first two stages. Q (~ 1 ,2 ) entries are
computed by propagating relevant Q -values of the next stage. For instance,
for the highlighted joint history ~ 1 = h(a1 ,o1 ),(a2 ,o2 )i, the Q -value under 2 is
computed by propagating the values of the four successor joint histories, as per
(4.20).

Theorem 4.4 (Sequentially rational Q ). The optimal Q-value function is properly defined
as a function of joint action-observation histories and past joint policies, Q (~ t ,t+1 ). This
Q specifies the optimal value given for all (~ t ,t+1 ), even for ~ t that are not reached by
execution of an optimal policy   , and therefore is referred to as sequentially rational.
Proof. For all ~ t ,t+1 , the optimal expected return is given by

t=h1
R(~ t ,t+1 (~ t )),
X
 ~ t t+1
t+1 ~ t t+1 ~ t
 ~ t+1 t+2,
Q ( , ) = R(~ t ,t+1 (~ t )) +
P (o | , ( ))Q ( ,
), 0  t < h  1

ot+1

where

t+2,

=



t+1 ,t+1,



(4.20)

and

t+1, = arg max
 t+1

X

~ t+1
~ t+1 


P (~ t+1 |t+1 ,b0 )Q (~ t+1 , t+1 , t+1 ).

(4.21)

which is well-defined.
The above equations constitute a dynamic program. When assuming that only pure
joint past policies  can be used, (4.21) transforms to
X

(4.22)
P (~ t+1 )Q (~ t+1 , t+1 , t+1 )
t+1, = arg max
 t+1

~ t+1
~ t+1 


320

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs

~
and for all (,)
such that ~ is consistent with  the dynamic program can be evaluated
from the end (t = h  1) to the begin (t = 0). Figure 7 illustrates the computation of Q .
When arriving at stage 0, the 1 reduce to joint actions and it is possible to select
 0, = arg max Q (~ ,a) = arg max Q (~ 0 ,1 ).
a

1

Then given 1 =  0, we can determine  1, = 1, using (4.22), etc. This essentially is
the forward-sweep policy computation using the optimal Q-value function Q (~ t ,t+1 ) as
defined by (4.20).
The computation of Q is also closely related to point-based dynamic programming for
Dec-POMDPs as discussed in section 3.5. Suppose that t = 2 in Figure 7 is the last stage
(i.e., h = 3). When the  2, for all 2 have been computed, it is easy to construct the sets of
non-dominated action for each agent: every action ai of agent i that is specified by some i2,
is non-dominated. Once we have computed the values for all (~ 1 ,2 ) at t = 1, each 2 has
an associated optimal future policy 2, . This means that each individual history ~i1 has an
associated sub-tree policy qi =2 for each 2 and as such each (~ 1 ,2 )-pair has an associated
joint sub-tree policy (e.g, the shaded trees in Figure 7). Clearly, Q (~ 1 ,2 ) corresponds to
expected value of this associated joint sub-tree policy. Rather than keeping track of these
sub-trees policies, however, the algorithm presented here keeps track of the values.
The advantage of the description of Q using (4.21) rather than (4.10) is twofold. First
the description treated here describes the way to actually compute the values which can
then be used to construct   , while the latter only gives a normative description and needs
  in order to compute the Q-values.
Second, this Q (~ t ,t+1 ) describes sequential rationality for Dec-POMDPs. For any
past policy (and corresponding consistent belief system) the optimal future policy can be
computed. A variation of this might even be applied on-line. Suppose agent i makes a
mistake at stage t, executing an action not prescribed by i , assuming the other agents
execute their policy 6=i without mistakes, agent i knows the actually executed previous
policy t+1 . Therefore it can compute a new individual policy by

D
E
X
t+1,
t+1
P (~ t+1 )Q (~ t+1 , t+1 , i t+1 ,6=
).
i,
t+1 = arg max
i
i t+1

~ t+1
~ t+1 


4.4.3 The Complexity of Computing a Sequentially Rational Q
Although we have now found a way to compute Q , this computation is intractable for all
P
|Oi |t 1
t
but the smallest problems, as we will now show. At stage t1 there are t1
t =0 |Oi | = |Oi |1
observation histories for agent i, leading to
|A |

(

n |O |t 1
|O |1

)

~ t | = |O|t1 consistent joint actionpure joint past policies t . For each of these there are |O
observation histories (for each observation history ~o t1 , t specifies the actions forming
~ t1 ). This means that for stage h  2 (for h  1, the Q-values are easily calculated), the
321

fiOliehoek, Spaan & Vlassis

number of entries to be computed is the number of joint past policies h1 times the number
of joint histories
!
n(|O |h1 1)
O |A | |O |1
 |O|h2 ,
indicating that computation of this function is doubly exponential, just as brute force policy
evaluation. Also, for each joint past policy h1 , we need to compute h = (h1 ,h1, )
by solving the next-stage BG:


X
P (~ h1 )Q (~ h1 , h1 , h1 ).
h1, = arg max
 h1

h1
~
~ h1 

To the authors knowledge, the only method to optimally solve these BGs is evaluation of
all


h1
O |A |n|O |

joint BG-polices, which is also doubly exponential in the horizon.

5. Approximate Q-value Functions
As indicated in the previous section, although an optimal Q-value function Q exists, it
is costly to compute and thus impractical. In this section, we review some other Q-value
b that can be used as an approximation for Q . We will discuss underlying
functions, Q,
assumptions, computation, computational complexity and other properties, thereby providing a taxonomy of approximate Q-value functions for Dec-POMDPs. In particular we
will treat two well-known approximate Q-value functions, QMDP and QPOMDP , and QBG
recently introduced by Oliehoek and Vlassis (2007).
5.1 QMDP
QMDP was originally proposed to approximately solve POMDPs by Littman, Cassandra,
and Kaelbling (1995), but has also been applied to Dec-POMDPs (Emery-Montemerlo
et al., 2004; Szer et al., 2005). The idea is that Q can be approximated using the stateaction values QM (s,a) found when solving the underlying MDP of a Dec-POMDP. This
underlying MDP is the horizon-h MDP defined by a single agent that takes joint actions
a  A and observes the nominal state s that has the same transition model T and reward
model R as the original Dec-POMDP. Solving this underlying MDP can be efficiently done
using dynamic programming techniques (Puterman, 1994), resulting in the optimal nonstationary MDP Q-value function:
X
t
t
(st+1 ,a).
(5.1)
P (st+1 |st ,a) max Qt+1,
Qt,
M
M (s ,a) = R(s ,a) +
a

st+1 S

t+1,
In this equation, the maximization is an implicit selection of M
, the optimal MDP policy
at the next time step, as explained in Section 4.3.3. Note that Qt,
M also is an optimal Qvalue function, but in the MDP setting. In this article Q will always denote the optimal
t
value function for the (original) Dec-POMDP. In order to transform the Qt,
M (s ,a)-values
b M (~ t ,a)-values to be used the original Dec-POMDP, we compute:
to approximate Q

322

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs

b M (~ t ,a) =
Q

X

~t
Qt,
M (s,a)P (s| ),

(5.2)

sS

where P (s|~ t ) can be computed from (4.8). Combining (5.1) and (5.2) and making the
t+1,
selection of M
explicit we get:
b M (~ t ,a) = R(~ t ,a) +
Q

X

P (st+1 |~ t ,a)

st+1 S

max

t+1 t+1
M
(s
)

t+1 t+1
Qt+1,
(st+1 ,M
(s )),
M

(5.3)

which defines the approximate Q-value function that can be used as payoff function for the
b M is consistent with the established definition
various BGs of the Dec-POMDP. Note that Q
of Q-value functions since it is defined as the expected immediate reward of performing
(joint) action a plus the value of following an optimal joint policy (in this case the optimal
MDP-policy) thereafter.
Because calculation of the QtM (s,a)-values by dynamic programming (which has a cost
of O(|S|h) can be performed in a separate phase, the cost of computation of QMDP is only
dependent on the cost of evaluation of (5.3), which is O(|S|). When we want to evaluate
P
(|A||O|)h 1
t
QMDP for all h1
t=0 (|A| |O|) = (|A||O|)1 joint action-observation histories is, the total
computational cost becomes:
!
(|A| |O|)h  1
O
|A||S| .
(5.4)
(|A| |O|)  1
However, when applying QMDP in forward-sweep policy computation, we do not have to
consider all action-observation histories, but only those that are consistent with the policy
found for earlier stages. Effectively we only have to evaluate (5.3) for all observation histories
and joint actions, leading to:
!
(|O|)h  1
|A||S| .
(5.5)
O
(|O|)  1
When used in the context of Dec-POMDPs, QMDP solutions are known to undervalue
actions that gain information (Fernandez, Sanz, Simmons, & Dieguez, 2006). This is explained by realizing that the QMDP solution assumes that the state will be fully observable
in the next time step. Therefore actions that provide information about the state, and
thus can lead to a high future reward (but might have a low immediate reward), will be
undervalued. When applying QMDP in the Dec-POMDP setting, this effect can also be
expected. Another consequence of the simplifying assumption is that the QMDP -value function is an upper bound to the optimal value function when used to approximate a POMDP
(Hauskrecht, 2000), as a consequence it is also an upper bound to the optimal value function
of a Dec-POMDP. This is intuitively clear, as a Dec-POMDP is a POMDP but with the
additional difficulty of decentralization. A formal argument will be presented in Section 5.4.
5.2 QPOMDP
Similar to the underlying MDP, one can define the underlying POMDP of a Dec-POMDP
as the POMDP with the same T , O and R, but in which there is only a single agent that
323

fiOliehoek, Spaan & Vlassis

takes joint actions a  A and receives joint observations o  O. QPOMDP approximates Q
using the solution of the underlying POMDP (Szer et al., 2005; Roth et al., 2005).
In particular, the optimal QPOMDP value function for an underlying POMDP satisfies:
~t

~t

QP (b ,a) = R(b ,a) +

X

~t

P (ot+1 |b ,a)

ot+1 O

max

t+1
t+1 
P
(b~
)

~ t+1

QP (b

~ t+1

,Pt+1 (b

)),

(5.6)

~t

where b is the joint belief of the single agent that selects joint actions and receives joint
observations at time step t, where
X
~t
~t
R(b ,a) =
R(s,a)b (s)
(5.7)
sS

~ t+1

~t

is the immediate reward, and where b
is the joint belief resulting from b by action a
t+1
and joint observation o , calculated by
P
~t
P (o|a,s ) sS P (s |s,a)b (s)
~ t+1 
s b
(s ) = P
.
(5.8)
P

~ t

sS P (s |s,a)b (s)
s S P (o|a,s )

~t
For each ~ t there is one joint belief b , which corresponds to P (s|~ t ) as can be derived
from (4.8). Therefore it is possible to directly use the computed QPOMDP values as payoffs
for the BGs of the Dec-POMDP, that is, we define:

b P (~ t ,a)  QP (b~ t ,a).
Q

(5.9)

The maximization in (5.6) is stated in its explicit form: a maximization over time step
t + 1 POMDP policies. However, it should be clear that this maximization effectively is one
over joint actions, as it is conditional on the received joint observation ot+1 and thus the
~ t+1
resulting belief b .
For a finite horizon, QP can be computed by generating all possible joint beliefs and
solving the belief MDP. Generating all possible beliefs is easy: starting with b0 corresponding to the empty joint action-observation history ~ t=0 , for each a and o we calculate
~1
the resulting ~ t=1 and corresponding belief b and continue recursively. Solving the belief
MDP amounts to recursively applying (5.6).
In the computation of QMDP we could restrict our attention to only those (~ t ,a)-pairs
b M (~ t ,a)-values do
that were specified by forward-sweep policy computation, because the Q
t+1
~
b
not depend on the values of successor-histories QM ( ,a). For QPOMDP , however, there
is such a dependence, meaning that it is necessary to evaluate for all ~ t ,a. In particular,
the cost of calculating QPOMDP can be divided in the cost of calculating the expected
immediate reward for all ~ t ,a, and the cost of evaluating future reward for all ~ t ,a, with
t = 0,...,h  2. The former operation is given by (5.7) and has cost O(|S|) per ~ t ,a and
thus a total cost equal to (5.4). The latter requires selecting the maximizing joint action
for each joint observation for all ~ t ,a with t = 0,...,h  2, leading to
!
(|A| |O|)h1  1
O
|A| (|A| |O|) .
(5.10)
(|A| |O|)  1
324

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs

~2t=1
~1t=1
~2t=0
~1t=0
()

a1
a1

()
a2
+3.1
0.9

(a1 ,o1 )
a2
4.1
+0.3

(a1 ,o1 )
(a1 ,o1 )
(a1 ,o1 )

a1
a1
a1
a1
a1
a1
...

(a2 ,o2 )
a2
a2
0.3
+0.6
0.6
+2.0
+3.1
+4.4
+1.1
2.9
0.4
0.9
0.9
4.5
...
...

(a2 ,o2 )
a2
a2
0.6
+4.0
1.3
+3.6
1.9
+1.0
+2.0
0.4
0.5
1.0
1.0
+3.5
...
...

...
...
...
...
...
...
...

Figure 8: Backward calculation of QPOMDP -values. Note that the solutions (the highlighted
entries) are different from those in Figure 6: QPOMDP assumes that the actions
can be conditioned on the joint action-observation history. The highlighted +3.1
entry for the Bayesian game for t = 0 is calculated as the expected immediate
reward (= 0) plus a weighted sum of the maximizing entry (joint action) per
next joint observation history. When assuming a uniform distribution over joint
observations given ha1 ,a2 i the future reward is given by: +3.1 = 0 + 0.25  2.0 +
0.25  4.0 + 0.25  4.4 + 0.25  2.0.

Therefore the total complexity of computing QPOMDP becomes
O

!
(|A| |O|)h  1
(|A| |O|)h1  1
|A| (|A| |O|) +
|A||S| .
(|A| |O|)  1
(|A| |O|)  1

(5.11)

~ t can be done in a single
Evaluating (5.6) for all joint action-observation histories ~ t  
backward sweep through time, as we mentioned in Section 4.3.3. This can also be visualized
in Bayesian games as illustrated in Figure 8; the expected future reward is calculated as a
maximizing weighted sum of the entries of the next time step BG.
Nevertheless, solving a POMDP optimally is also known as an intractable problem.
As a result, POMDP research in the last decade has focused on approximate solutions for
POMDPs. In particular, it is known that the value function of a POMDP is piecewise-linear
and convex (PWLC) over the (joint) belief space (Sondik, 1971). This property is exploited
by many approximate POMDP solution methods (Pineau, Gordon, & Thrun, 2003; Spaan
& Vlassis, 2005). Clearly such methods can also be used to calculate an approximate
QPOMDP -value function for use with Dec-POMDPs.
It is intuitively clear that QPOMDP is also an admissible heuristic for Dec-POMDPs, as
it still assumes that more information is available than actually is the case (again a formal
proof will be given in Section 5.4). Also it should be clear that, as fewer assumptions are
made, QPOMDP should yield less of an over-estimation than QMDP . I.e., the QPOMDP -values
should lie between the QMDP and optimal Q -values.
In contrast to QMDP , QPOMDP does not assume full observability of nominal states.
As a result the latter does not share the drawback of undervaluing actions that will gain
information regarding the nominal state. When applied in a Dec-POMDP setting, however,
QPOMDP does share the assumption of centralized control. This assumption might also
cause a relative undervaluation: there might be situations where some action might gain
325

fiOliehoek, Spaan & Vlassis

information regarding the joint (i.e., each others) observation history. Under QPOMDP this
will be considered redundant, while in decentralized execution this might be very beneficial,
as it allows for better coordination.
5.3 QBG
QMDP approximates Q by assuming that the state becomes fully observable in the next
time step, while QPOMDP assumes that at every time step t the agents know the joint
action-observation history ~ t . Here we present a new approximate Q-value function, called
QBG , that relaxes the assumptions further: it assumes that the agents know ~ t1 , the joint
action-observation history up to time step t  1, and the joint action at1 that was taken
at the previous time step. This means that the agents are uncertain regarding each others
last observation, which effectively defines a BG for each ~ t1 ,a. Note, that these BGs are
different from the BGs used in Section 4.2: the BGs here have types that correspond to
single observations, whereas the BGs in 4.2 have types that correspond to complete actionobservation histories. Hence, the BGs of QBG are much smaller in size and thus easier to
solve. Formally QBG is defined as:
QB (~ t ,a) = R(~ t ,a) + max


X

P (ot+1 |~ t ,a)QB (~ t+1 ,(ot+1 )),

(5.12)

ot+1 O

t+1
where  = h1 (ot+1
1 ),...,n (on )i is a tuple of individual policies i : Oi  Ai for the BG
constructed for ~ t ,a.
Note that the only difference between (5.12) and (5.6) is the position and argument
of the maximization operator: (5.12) maximizes over a (conditional) BG-policy, while the
maximization in (5.6) is effectively over unconditional joint actions.
The BG representation of the fictitious Dec-POMDP in Figure 6 illustrates the com~1
putation of QBG .8 The probability distribution P (
ha1 ,a2 i ) over joint action-observation
histories that can be reached given ha1 ,a2 i at t = 0 is uniform and the immediate reward
for ha1 ,a2 i is 0. Therefore, we have that 2.75 = 0.25  2.0 + 0.25  3.6 + 0.25  4.4 + 0.25  1.0.
The cost of computing QBG for all ~ t ,a can be split up in the cost of computing the
immediate reward (see (5.4)) and the cost of computing the future reward (solving a BG
over the last received observation), which is
!
(|A| |O|)h1  1
O
|A|  |A |n|O | ,
(|A| |O|)  1

leading to a total cost of:
O

!
(|A| |O|)h1  1
(|A| |O|)h  1
n|O |
|A|  |A |
+
|A||S| .
(|A| |O|)  1
(|A| |O|)  1

(5.13)

Comparing to the cost of computing QPOMDP , this contains an additional exponential term,
but this term does not depend on the horizon of the problem.
8. Because the BG representing t = 1 of a Dec-POMDP also involves observation histories of length 1, the
illustration of such a BG corresponds to the BGs as considered in QBG . For other stages this is not the
case.

326

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs

As mentioned in Section 5.2, QPOMDP can be approximated by exploiting the PWLCproperty of the value function. It turns out that the QBG -value function corresponds to an
optimal value function for the situation where the agents can communicate freely with a onestep delay (Oliehoek et al., 2007b). Hsu and Marcus (1982) showed how a complex dynamic
program can be constructed for such settings and that the resulting value function also
preserves the PWLC property. Not surprisingly, the QBG -value function also is piecewiselinear and convex over the joint belief space and, as a result, approximation methods for
POMDPs can be transferred to the computation of QBG (Oliehoek et al., 2007b).
5.4 Generalized QBG and Bounds
We can think of an extension of the QBG -value function framework to the case of k-steps delayed communication, where each agent perceives the joint action-observation history with
k stages delay. That is, at stage t, each agent i knows ~ tk the joint action-observation history of k stages before in addition to its own current action-observation history ~it . Similar
k-step delayed observation models for decentralized control have been previously proposed
by Aicardi, Davoli, and Minciardi (1987) and Ooi and Wornell (1996). In particular Aicardi
et al. consider the Dec-MDP setting in which agent is observations are local states si and
where a joint observation identifies the state s = hs1 , . . . ,sn i. Ooi and Wornell examine
the decentralized control of a broadcast channel over an infinite horizon, where they allow
the local observations to be arbitrary, but still require the joint state to be observed with
a k-steps delay. Our assumption is less strong, as we only require observation of ~ tk and
because we assume the general Dec-POMDP (not Dec-MDP) setting.
Such a k-step delayed communication model for the Dec-POMDP setting allows expressing the different Q-value functions defined in this article as optimal value functions
of appropriate k-step delay models. More importantly, by resorting to such a k-step delay
model we can prove a hierarchy of bounds that hold over the various Q-functions defined
in this article:
Theorem 5.1 (Hierarchy of upper bounds). The approximate Q-value functions QBG and
QPOMDP correspond to the optimal Q-value functions of appropriately defined k-step delayed
communication models. Moreover these Q-value functions form a hierarchy of upper bounds
to the optimal Q of the Dec-POMDP:
Q  QBG  QPOMDP  QMDP .

(5.14)

Proof. See appendix.
The idea is that a POMDP corresponds to a system with no (0-steps) delayed communication, while the QBG -setting corresponds to a 1-step delayed communication system.
The appendix shows that the Q-value function of a system with k steps delay forms an
upper bound to that of a decentralized system with k + 1 steps delay. We note that the last
inequality of (5.14) is a well-known result (Hauskrecht, 2000).

6. Generalized Value-Based Policy Search
The hierarchy of approximate Q-value functions implies that all of these Q-value functions
can be used as admissible heuristics in MAA policy search, treated in Section 3.3. In
327

fiOliehoek, Spaan & Vlassis

Algorithm 1 GMAA
1: v  
2: P  {0 = ()}
3: repeat
4:
t  Select(P)
5:
Next  Next(t )
6:
if Next contains a subset of full policies Next  Next then
7:
   arg maxNext V ()
8:
if V (  ) > v then
9:
v  V (  )
10:
  n 
o
11:
P    P | Vb () > v {prune the policy pool}
12:
end if
13:
Next  Next \ Next {remove full policies}
14:
end if
n
o
15:
P  (P \ t )    Next | Vb () > v {remove processed/add new partial policies}
16: until P is empty
this section we will present a more general heuristic policy search framework which we will
call Generalized MAA (GMAA ), and show how it unifies some of the solution methods
proposed for Dec-POMDPs.
GMAA generalizes MAA (Szer et al., 2005) by making explicit different procedures
that are implicit in MAA : (1) iterating over a pool of partial joint policies, pruning this pool
whenever possible, (2) selecting a partial joint policy from the policy pool, and (3) finding
some new partial and/or full joint policies given the selected policy. The first procedure is
the core of GMAA and is fixed, while the other two procedures can be performed in many
ways.
The second procedure, Select, chooses which policy to process next and thus determines the type of search (e.g., depth-first, breadth-first, A*-like) (Russell & Norvig, 2003;
Bertsekas, 2005). The third procedure, which we will refer to as Next, determines how
the set of next (partial) joint policies are constructed, given a previous partial joint policy.
The original MAA can be seen as an instance of the generalized case with a particular
Next-operator, namely that shown in algorithm 2.
6.1 The GMAA Algorithm
In GMAA we refer to a policy pool P rather than an open list, as it is a more neutral
word which does not imply any ordering. This policy pool P is initialized with a completely
unspecified joint policy 0 = () and the maximum lower bound (found so far) v is set
to .   denotes the best joint policy found so far.
At this point GMAA starts. First, the selection operator, Select, selects a partial joint
policy  from P. We will assume that, in accordance with MAA , the partial policy with
the highest heuristic value is selected. In general, however, any kind of selection algorithm
may be used. Next, the selected policy is processed by the policy search operator Next,
328

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs

Algorithm 2 Next(t )  MAA
o
n
ff t+1

 t+1

t , t ,  t : O
t A
~
|

=

1: t+1  t+1 = 1 ,...,t+1
i
n
i i
i
i
i
fi t+1 

t+1
0...t1
t
t
b
fi
+ Vb (t+1)...h (t+1 )
2: t+1 t+1 V (
)V
( ) + E R(s ,a) 
t+1
3: return 
which returns a set of (partial) joint policies Next and their heuristic values. When Next
returns one or more full policies   Next , the provided values Vb () = V () are a lower
bound for an optimal joint policy, which can be used to prune the search space. Any found
partial joint policies   Next with a heuristic value Vb () > v are added to P. The
process is repeated until the policy pool is empty.
6.2 The Next Operator
Here we describe some different choices for the Next-operator and how they correspond to
existing Dec-POMDP solution methods.
6.2.1 MAA
GMAA reduces to standard MAA by using the Next-operator described by Algorithm 2.
Line 1 expands t forming t+1 the set of partial joint policies for one extra stage. Line 2
valuates all these child policies, where
fi


V 0...t (t+1 ) = V 0...t1 (t ) + E R(st ,a)fit+1

gives the true expected reward over the first t + 1 stages. Vb (t+1)...h (t+1 ) is the heuristic
value over stages (t + 1)...h given that t+1 has been followed the first t + 1 stages.
When using an admissible heuristic, GMAA will never prune a partial policy that can
be expanded into an optimal policy. When combining this with the fact that the MAA Next operator returns all possible t+1 for a t , it is clear that when P becomes empty an
optimal policy has been found.
6.2.2 Forward-Sweep Policy Computation
Forward-sweep policy computation, as introduced in Section 4.3.1, is described by algorithms 1 and 3 jointly. Given a partial joint policy t , the Next operator now constructs
and solves a BG for time step t. Because Next in algorithm 3 only returns the best-ranked
policy, P will never contain more than 1 joint policy and the whole search process reduces
to solving BGs for time steps 0,...,h  1.
The approach of Emery-Montemerlo et al. (2004) is identical to forward-sweep policy
computation, except that 1) smaller BGs are created by discarding or clustering low probability action-observation histories, and 2) the BGs are approximately solved by alternating
maximization. Therefore this approach can also be incorporated in the GMAA policy
search framework by making the appropriate modifications in Algorithm 3.
329

fiOliehoek, Spaan & Vlassis

Algorithm 3 Next(t )  Forward-sweep policy computation
E
D
bt
~ t t ),Q
~ t t ,P (
1: BG  A,


~ t  Ai do
2: for all  = h1 ,...,n i s.t. i : O
i
P
t
t
t
t
~
~
b
3:
Vb ()  ~ t 
P
(

)
Q
(

,(
~ t ))
~t
t

4:
t+1  t ,
5:
Vb (t+1 )  V 0...t1 (t ) + Vb t ()
6: end for
b (t+1 )
7: return arg maxt+1 V
6.2.3 Unification
Here we will give a unified perspective of the MAA and forward-sweep policy computation
by examining the relation between the corresponding Next-operators. In particular we
show that, when using any of the approximate Q-value functions described in Section 5 as
a heuristic, the sole difference between the two is that FSPC returns only the joint policy
with the highest heuristic value.
b has the following form
Proposition 6.1. If a heuristic Q
X
b t (~ t ,a) = R(~ t ,a) +
P (ot+1 |~ t ,a)Vb t+1 (~ t+1 ),
Q

(6.1)

ot+1


then for a partial policy t+1 = t , t
X
fi


b t (~ t ,(~ t )) = E R(st ,a)fit+1 + Vb (t+1)...h (t+1 )
P (~ t )Q

(6.2)

~t
~ t 


holds.

Proof. The expectation of Rt given t+1 can be written as
X
X
X
fi


P (~ t )R(~ t ,t+1 (~ t )).
P (~ t )
R(s,t+1 (~ t ))P (s|~ t ) =
E R(st ,a)fit+1 =
sS

~t
~ t 


~t
~ t 


Also, we can rewrite Vb (t+1)...h (t+1 ) as
X
X
P (ot+1 |~ t ,t+1 (~ t ))Vb (t+1)...h (~ t+1 ),
P (~ t )
Vb (t+1)...h (t+1 ) =
ot+1

~t
~ t 


such that
X
fi


P (~ t )
E R(st ,a)fit+1 + Vb (t+1)...h (t+1 ) =
"

~t
~ t 


~t

t+1

R( ,

~t

( )) +

X

ot+1

Therefore, assuming (6.1) yields (6.2).
330

P (o

t+1

~t

t+1

| ,

~t

b (t+1)...h

( ))V

~ t+1

(

#

)

(6.3)

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs

~o  go house 3
flames  go house 3
no flames  go house 1
flames, flames  go house 1
flames, no flames  go house 1
no flames, flames  go house 2
no flames, no flames  go house 2

~o  go house 2
flames  go house 2
no flames  go house 2
flames, flames  go house 1
flames, no flames  go house 1
no flames, flames  go house 1
no flames, no flames  go house 1

Figure 9: Optimal policy for FireFighting hnh = 3,nf = 3i, horizon 3. On the left the
policy for the first agent, on the right the second agents policy.

This means that if a heuristic satisfies (6.1), which is the case for all the Q-value functions
we discussed in this paper, the Next operators of algorithms 2 and 3 evaluate the expanded
policies the same. I.e., algorithms 2 and 3 calculate identical heuristic values for the same
next time step joint policies. Also the expanded policies t+1 are formed in the same way:
by considering all possible  t respectively  t to extend t . Therefore, the sole difference in
this case is that the latter returns only the joint policy with the highest heuristic value.
Clearly there is a computation time/quality trade-off between MAA and FSPC: MAA
is guaranteed to find an optimal policy (given an admissible heuristic), while FSPC is
guaranteed to finish in one forward sweep. We propose a generalization, that returns the
k-best ranked policies. We refer to this as the k-best joint BG policies GMAA variant, or
k-GMAA . In this way, k-GMAA reduces to forward-sweep policy computation for k = 1
and to MAA for k = .

7. Experiments
In order to compare the different approximate Q-value functions discussed in this work,
as well as to show the flexibility of the GMAA algorithm, we have performed several
experiments. We use QMDP , QPOMDP and QBG as heuristic estimates of Q . We will
provide some qualitative insight in the different Q-value functions we considered, as well
as results on computing optimal policies using MAA , and on the performance of forwardsweep policy computation. First we will describe our problem domains, some of which are
standard test problems, while others are introduced in this work.
7.1 Problem Domains
In Section 2.2 we discussed the decentralized tiger (Dec-Tiger) problem as introduced by
Nair et al. (2003b). Apart from the standard Dec-Tiger domain, we consider a modified
version, called Skewed Dec-Tiger, in which the start distribution is not uniform. Instead,
initially the tiger is located on the left with probability 0.8. We also include results from the
BroadcastChannel problem, introduced by Hansen et al. (2004), which models two nodes
that have to cooperate to maximize the throughput of a shared communication channel.
Furthermore, a test problem called Meeting on a Grid is provided by Bernstein et al.
(2005), in which two robots navigate on a two-by-two grid. We consider the version with 2
observations per agent (Amato et al., 2006).
331

fiOliehoek, Spaan & Vlassis

We introduce a new benchmark problem, which models a team of n fire fighters that
have to extinguish fires in a row of nh houses. Each house is characterized by an integer
parameter f , or fire level. It indicates to what degree a house is burning, and it can have nf
different values, 0  f < nf . Its minimum value is 0, indicating the house is not burning. At
every time step, the agents receive a reward of f for each house and each agent can choose
to move to any of the houses to fight fires at that location. If a house is burning (f > 0) and
no fire fighting agent is present, its fire level will increase by one point with probability 0.8
if any of its neighboring houses are burning, and with probability 0.4 if none of its neighbors
are on fire. A house that is not burning can only catch fire with probability 0.8 if one of
its neighbors is on fire. When two agents are in the same house, they will extinguish any
present fire completely, setting the houses fire level to 0. A single agent present at a house
will lower the fire level by one point with probability 1 if no neighbors are burning, and
with probability 0.6 otherwise. Each agent can only observe whether there are flames or
not at its location. Flames are observed with probability 0.2 if f = 0, with probability 0.5
if f = 1, and with probability 0.8 otherwise. Initially, the agents start outside any of the
houses, and the fire level f of each house is drawn from a uniform distribution.
We will test different variations of this problems, where the number of agents is always
2, but which differ in the number of houses and fire levels. In particular, we will consider
hnh = 3,nf = 3i and hnh = 4,nf = 3i. Figure 9 shows an optimal joint policy for horizon 3
of the former variation. One agent initially moves to the middle house to fight fires there,
which helps prevent fire from spreading to its two neighbors. The other agent moves to
house 3, and stays there if it observes fire, and moves to house 1 if it does not observe
flames. As well as being optimal, such a joint policy makes sense intuitively speaking.
7.2 Comparing Q-value Functions
Before providing a comparison of performance of some of the approximate Q-value functions
described in this work, we will first give some more insights in their actual values. For the
h = 4 Dec-Tiger problem, we generated all possible ~ t and the corresponding P (sl |~ t ),
according to (4.8). For each of these, the maximal Q(~ t ,a)-value is plotted in Figure 10.
Apart from the three approximate Q-value functions, we also plotted the optimal value
for each joint action-observation history ~ t that can be realized when using   . Note that
different ~ t can have different optimal values, but induce the same P (sl |~ t ), as demonstrated
in the figure: there are multiple Q -values plotted for some P (sl |~ t ). For the horizon 3
Meeting on a Grid problem we also collected all ~ t that can be visited by the optimal
policy, and in Figure 11 we again plotted maximal Q(~ t ,a)-values. Because this problem
has many states, a representation as in Figure 10 is not possible. Instead, we ordered the
~ while
~ according to their optimal value. We can see that the bounds are tight for some ,

for others they can be quite loose. However, when used in the GMAA framework, their
~ not shown by
actual performance as a heuristic also depends on their valuation of ~  
Figure 11, namely those that will not be visited by an optimal policy: especially when
these are overestimated, GMAA will first examine a sub-optimal branch of the search tree.
A tighter upper bound can speed up computation to a very large extent, as it allows the
algorithm to prune the policy pool more, reducing the number of Bayesian games that need
332

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs

Qheuristics for horizon=4 DecTiger at t=0
60

Qheuristics for horizon=4 DecTiger at t=1
60

Q

Q

BG
POMDP

Qmax = maxa Q(t,a)

MDP

Q*

40
30
20
10
0
0

QPOMDP

50

Q

a

Qmax = max Q(t,a)

BG

Q

50

Q

MDP
*

40

Q

30
20
10
0

t

P(sl |  )

10
0

1

Qheuristics for horizon=4 DecTiger at t=2
40

P(sl | t )

1

Qheuristics for horizon=4 DecTiger at t=3
20

Q

QBG

BG

QPOMDP
Q

*

Q

20

a

20

POMDP

QMDP

t

MDP
*

Q

0

Q

Qmax = max Q( ,a)

Qmax = maxa Q(t,a)

30

10
0
10
20
0

40
60
80

P(sl | t )

100
0

1

t

P(sl |  )

1

Figure 10: Q-values for horizon 4 Dec-Tiger. For each ~ t , corresponding to some P (sl |~ t ),
the maximal Q(~ t ,a)-value is plotted.

to be solved. Both figures clearly illustrate the main property of the upper bounds we
discussed, namely that Q  QBG  QPOMDP  QMDP (see Theorem 5.1).
7.3 Computing Optimal Policies
As shown above, the hierarchy of upper bounds Q  QBG  QPOMDP  QMDP is not
just a theoretical construct, but the differences in value specified can be significant for
particular problems. In order to evaluate what the impact is of the differences between
the approximate Q-value functions, we performed several experiments. Here we describe
our evaluation of MAA on a number of test problems using QBG , QPOMDP and QMDP as
heuristic. All timing results in this paper are CPU times with a resolution of 0.01s, and
were obtained on 3.4GHz Intel Xeon processors.
333

fiOliehoek, Spaan & Vlassis

2
1.8

Qmax = maxa Q(t,a)

1.6
1.4
1.2
1
0.8
Q

MDP

0.6

Q

POMDP

Q

BG
*

0.4

Q

0.2

t



Figure 11: Comparison of maximal Q(~ t ,a)-values for Meeting on a Grid. We plot the
value of all t that can be reached by an optimal policy, ordered according their
optimal value.

h

V

3

5.1908

4

4.8028

QMDP
QPOMDP
QBG
QMDP
QPOMDP
QBG

n
105,228
6,651
6,651
37,536,938,118
559,653,390
301,333,698

TGMAA
0.31 s
0.02 s
0.02 s
431,776 s
5,961 s
3,208 s

TQ
0s
0s
0.02 s
0s
0.13 s
0.94 s

Table 1: MAA results for Dec-Tiger.

h

V
QMDP

3

5.8402

4

11.1908

QPOMDP
QBG
QMDP
QPOMDP
QBG

n
151,236
19,854
13,212
33,921,256,149
774,880,515
86,106,735

TGMAA
0.46 s
0.06 s
0.04 s
388,894 s
8,908 s
919 s

Table 2: MAA results for Skewed Dec-Tiger.

334

TQ
0s
0.01 s
0.03 s
0s
0.13 s
0.92 s

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs

h

V
QMDP

4

3.8900

5

4.7900

QPOMDP
QBG
QMDP
QPOMDP
QBG

n
328,212
531
531
N/A
196,883
196,883

TGMAA
3.54 s
0s
0s
> 4.32e5 s
5.30 s
5.15 s

TQ
0s
0.01 s
0.03 s
0s
0.20 s
0.53 s

Table 3: MAA results for BroadcastChannel.

h

V
QMDP

2

0.9100

3

1.5504

QPOMDP
QBG
QMDP
QPOMDP
QBG

n
1,275
1,275
194
29,688,775
3,907,525
1,563,775

TGMAA
0s
0s
0s
81.93 s
10.80 s
4.44 s

TQ
0s
0s
0s
0s
0.15 s
1.37 s

Table 4: MAA results for Meeting on a Grid.

Table 1 shows the results MAA obtained on the original Dec-Tiger problem for horizon
3 and 4. It shows for each heuristic the number of partial joint policies evaluated n , CPU
time spent on the GMAA phase TGMAA , and CPU time spent on calculating the heuristic
TQ . As QBG , QPOMDP and QMDP are upper bounds to Q , MAA is guaranteed to find the
optimal policy when using them as heuristic, however the timing results may differ.
For h = 3 we see that using QPOMDP and QBG only a fraction of the number of policies
are evaluated when compared to QMDP which reflects proportionally in the time spent on
GMAA . For this horizon QPOMDP and QBG perform the same, but the time needed to
compute the QBG heuristic is as long as the GMAA -phase, therefore QPOMDP outperforms
QBG here. For h = 4, the impact of using tighter heuristics becomes even more pronounced.
In this case the computation time of the heuristic is negligible, and QBG outperforms both,
as it is able to prune much more partial joint policies from the policy pool. Table 2 shows
results for Skewed Dec-Tiger. For this problem the QMDP and QBG results are roughly the
same as the original Dec-Tiger problem; for h = 3 the timings are a bit slower, and for
h = 4 they are faster. For QPOMDP , however, we see that for h = 4 the results are slower
as well and that QBG outperforms QPOMDP by an order of magnitude.
Results for the Broadcast Channel (Table 3), Meeting on a Grid (Table 4) and a Fire
fighting problem (Table 5) are similar. The N/A entry in Table 3 indicates the QMDP was
not able to compute a solution within 5 days. For these problems we also see that the
performance of QPOMDP and QBG is roughly equal. For the Meeting on a Grid problem,
QBG yields a significant speedup over QPOMDP .
335

fiOliehoek, Spaan & Vlassis

h

V

3

5.7370

4

6.5788

QMDP
QPOMDP
QBG
QMDP
QPOMDP
QBG

n
446,724
26,577
26,577
25,656,607,368
516,587,229
516,587,229

TGMAA
1.58 s
0.08 s
0.08 s
309,235 s
5,730 s
5,499 s

TQ
0.56 s
0.21 s
0.33 s
0.85 s
7.22 s
11.72 s

Table 5: MAA results for Fire Fighting hnh = 3, nf = 3i.
~o  aLi
oHL  aLi
oHR  aLi
oHL , oHL  aLi
oHL , oHR  aLi
oHR , oHL  aLi
oHR , oHR  aLi
oHL , oHL , oHL  aOR
oHL , oHL , oHR  aLi
oHL , oHR , oHL  aLi
oHL , oHR , oHR  aLi
oHR , oHL , oHL  aLi
oHR , oHL , oHR  aLi
oHR , oHR , oHL  aLi
oHR , oHR , oHR  aOL

~o  aLi
oHL  aLi
oHR  aLi
oHL , oHL  aOR
oHL , oHR  aLi
oHR , oHL  aLi
oHR , oHR  aOL
oHL , oHL , oHL  aLi
oHL , oHL , oHR  aLi
oHL , oHR , oHL  aLi
oHL , oHR , oHR  aLi
oHR , oHL , oHL  aLi
oHR , oHL , oHR  aLi
oHR , oHR , oHL  aLi
oHR , oHR , oHR  aLi

Figure 12: Policies found using forward-sweep policy computation (i.e., k = 1) for the h = 4
Dec-Tiger problem. Left: the policy resulting from QMDP . Right: the optimal
policy as calculated by QPOMDP and QBG . The framed entries highlight the
crucial differences.

7.4 Forward-Sweep Policy Computation
The MAA results described above indicate that the use of a tighter heuristic can yield
substantial time savings. In this section, the approximate Q-value functions are used in
forward-sweep policy computation. We would expect that when using a Q-value function
that more closely resembles Q , the quality of the resulting policy will be higher. We also
tested whether k-GMAA with k > 1 improved the quality of the computed policies. In
particular, we tested k = 1,2, . . . ,5.
For the Dec-Tiger problem, k-GMAA with k = 1 (and thus also 2  k  5) found
the optimal policy (with V (  ) = 5.19) for horizon 3 using all approximate Q-value functions. For horizon h = 4, also all different values of k produced the same result for each
approximate Q-value function. In this case, however, QMDP found a policy with expected
return of 3.19. QPOMDP and QBG did find the optimal policy (V (  ) = 4.80). Figure 12
illustrates the optimal policy (right) and the one found by QMDP (left). It shows that QMDP
overestimates the value for opening the door in stage t = 2.
336

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs

6

15
10

4

V

V

5
Q

MDP

Q

MDP

5

Q

3

Q

POMDP

POMDP

Q

Q

BG

2

1

2

3

k

4

BG

0

5

(a) Skewed Dec-Tiger, h = 3.

2

3

k

4

6.575

1.5505

6.58

V

1.55

Q

MDP

Q

MDP

6.585

Q

1.5495

Q

POMDP

POMDP

Q

Q

BG

1.549

5

(b) Skewed Dec-Tiger, h = 4.

1.551

V

1

1

2

3

k

4

BG

6.59

5

(c) GridSmall, h = 3.

1

2

3

k

4

5

(d) FireFighting with hnh = 3,nf = 3i, h = 4.

11.05

14

11.1

14.1

Q

MDP

Q

POMDP

Q

V

V

BG

11.15

Q

14.2

MDP

Q

11.2

14.3

POMDP

Q

BG

11.25

1

2

3

k

4

14.4

5

(e) FireFighting with hnh = 4,nf = 3i, h = 3.

1

2

3

k

4

5

(f) FireFighting with hnh = 4,nf = 3i, h = 4.

Figure 13: k-GMAA results for different problems and horizons. The y-axis indicates value
of the initial joint belief, while the x-axis denotes k.

For the Skewed Dec-Tiger problem, different values of k did produce different results. In
particular, for h = 3 only QBG finds the optimal policy (and thus attains the optimal value)
for all values of k, as shown in Figure 13(a). QPOMDP does find it starting from k = 2,
and QMDP only from k = 5. Figure 13(b) shows a somewhat unexpected result for h = 4:
here for k = 1 QMDP and QBG find the optimal policy, but QPOMDP doesnt. This clearly
illustrates that a tighter approximate Q-value function is not a guarantee for a better joint
policy, which is also illustrated by the results for GridSmall in Figure 13(c).
We also performed the same experiment for two settings of the FireFighting problem.
For hnh = 3,nf = 3i and h = 3 all Q-value functions found the optimal policy (with value
5.7370) for all k, and horizon 4 is shown in Figure 13(d). Figures 13(e) and 13(f) show
the results for hnh = 4,nf = 3i. For h = 4, QMDP did not finish for k  3 within 5 days.
It is encouraging that for all experiments k-GMAA using QBG and QPOMDP with k  2
found the optimal policy. Using QMDP the optimal policy was also always found with k  5,
except in horizon 4 Dec-Tiger and the hnh = 4,nf = 3i FireFighting problem. These results
seem to indicate that this type of approximation might be likely to produce (near-) optimal
results for other domains as well.
337

fiOliehoek, Spaan & Vlassis

8. Conclusions
A large body of work in single-agent decision-theoretic planning is based on value functions,
but such theory has been lacking thus far for Dec-POMDPs. Given the large impact of value
functions on single-agent planning under uncertainty, we expect that a thorough study of
value functions for Dec-POMDPs can greatly benefit multiagent planning under certainty.
In this work, we presented a framework of Q-value functions for Dec-POMDPs, providing a
significant contribution to fill this gap in Dec-POMDP theory. Our theoretical contributions
have lead to new insights, which we applied to improve and extend solution methods.
We have shown how an optimal joint policy   induces an optimal Q-value function

Q (~ t ,a), and how it is possible to construct the optimal policy   using forward-sweep
policy computation. This entails solving Bayesian games for time steps t = 0 ,..., h  1
which use Q (~ t ,a) as the payoff function. Because there is no clear way to compute
Q (~ t ,a), we introduced a different description of the optimal Q-value function Q (~ t ,t+1 )
that is based on sequential rationality. This new description of Q can be computed using
dynamic programming and can then be used to construct   .
Because calculating Q is computationally expensive, we examined approximate Q-value
functions that can be calculated more efficiently and we discussed how they relate to Q .
We covered QMDP , QPOMDP , and QBG , a recently proposed approximate Q-value function.
Also, we established that decreasing communication delays in decentralized systems cannot
decrease the expected value and thus that Q  QBG  QPOMDP  QMDP . Experimental
evaluation indicated that these upper bounds are not just of theoretical interest, but that
significant differences exist in the tightness of the various approximate Q-value functions.
Additionally we showed how the approximate Q-value functions can be used as heuristics in a generalized policy search method GMAA , thereby unifying forward-sweep policy
computation and the recent Dec-POMDP solution techniques of Emery-Montemerlo et al.
(2004) and Szer et al. (2005). Finally, we performed an empirical evaluation of GMAA
showing significant reductions in computation time when using tighter heuristics to calculate
optimal policies. Also QBG generally found better approximate solutions in forward-sweep
policy computation and the k-best joint BG policies GMAA variant, or k-GMAA .
There are quite a few directions for future research. One is to try to extend the results
of this paper to partially observable stochastic games (POSGs) (Hansen et al., 2004), which
are Dec-POMDPs with an individual reward function for each agent. Since the dynamics of
the POSG model are identical to those of a Dec-POMDP, a similar modeling via Bayesian
games is possible. An interesting question is whether also in this case, an optimal (i.e.,
rational) joint policy can be found by forward-sweep policy computation.
Staying within the context of Dec-POMDPs, a research direction could be to further
generalize GMAA , by defining other Next or Select operators, with the hope that the
resulting algorithms will be able to scale to larger problems. Also it is important to establish
bounds on the performance and learning curves of GMAA in combination with different
Next operators and heuristics. A different direction is to experimentally evaluate the use
of even tighter heuristics such as Q-value functions for the case of observations delayed by
multiple time steps. This research should be paired with methods to efficiently find such
Q-value functions. Finally, future research should further examine Bayesian games. In
particular, the work of Emery-Montemerlo et al. (2005) could be used as a starting point
338

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs

for further research to approximately modeling Dec-POMDPs using BGs. Finally, there is
a need for efficient approximate methods for solving the Bayesian games.

Acknowledgments
We thank the anonymous reviewers for their useful comments. The research reported here
is part of the Interactive Collaborative Information Systems (ICIS) project, supported by
the Dutch Ministry of Economic Affairs, grant nr: BSIK03024. This work was supported
by Fundacao para a Ciencia e a Tecnologia (ISR/IST pluriannual funding) through the
POS Conhecimento Program that includes FEDER funds, and through grant PTDC/EEAACR/73266/2006.

Appendix A. Proofs
A.1 There is At Least One Optimal Pure Joint Policy
Proposition (2.1). A Dec-POMDP has at least one optimal pure joint policy.
Proof. This proof follows a proof by Schoute (1978). It is possible to convert a Dec-POMDP
to an extensive game and thus to a strategic game, in which the actions are pure policies for
the Dec-POMDP (Oliehoek & Vlassis, 2006). In this strategic game, there is at least one
maximizing entry corresponding to a pure joint policy which we denote max . Now, assume
that there is a joint stochastic policy  = h1 , . . . ,n i that attains a higher payoff. Kuhn
(1953) showed that for each stochastic i policy, there is a corresponding mixed policy i .
Therefore  corresponds to a joint mixed policy  = h1 , . . . ,n i. Let us write i,i for
the support of i .  now induces a probability distribution P over the set of joint policies
 = 1,1      n,n   which is a subset of the set of all joint policies. The expected
payoff can now be written as
V () = EP (V ()|   )  max V () = V (max ),


contradicting that  is a joint stochastic policy that attains a higher payoff.
A.2 Hierarchy of Q-value Functions
This section lists the proof of theorem 5.1. It is ordered as follows. First, Section A.2.1
presents a model and resulting value functions for Dec-POMDPs with k-steps delayed communication. Next, Section A.2.2 shows that QPOMDP , QBG and Q correspond with the
case that k is respectively 0, 1 and h. Finally, Section A.2.3 shows that when the communication delay k increases, the optimal expected return cannot decrease, thereby proving
theorem 5.1.
A.2.1 Modeling Dec-POMDPs with k-Steps Delayed Communication
Here we present an augmented MDP that can be used to find the optimal solution for
Dec-POMDPs with k steps delayed communication. This is a reformulation of the work by
Aicardi et al. (1987) and Ooi and Wornell (1996), extended to the Dec-POMDP setting.
339

fiOliehoek, Spaan & Vlassis

q1t

q2t

a1
o1

o1

o1

a1

a1 ...

a...1

t+k1

a2

o2

o1

a1

o2

a2

a1
o1

t

o2

o1

a1

a2

o2
a...2

a2

o2

o2

a2

a2

t+k
 t+k

ot+1 = ho1 ,o2 i

q1t+1

q2t+1

a1
o1

o2

o1

a1

a1

a2

a2

t+1
o2
a2

t+k

Figure 14: Policies specified by the states of the augmented MDP for k = 2. Top: policies
for st . The policy extended by augmented MDP action at =  is shown dashed.
Bottom: The resulting policies after for joint observation ho1 ,o2 i.

We define this augmented MDP as M = hS,A,T ,Ri, where the augmented MDP stages are
indicated t.
The state space is S = (S t=0 , . . . ,S t=h1 ). An augmented state is composed of a joint
action-observation history, and a joint policy tree q t .
st=t =

(

h~ t ,q t i
h~ t ,q  =ht,t i

,0  t  h  k  1
.
,h  k  t  h  1

The
q t is a joint depth-k (specifying actions for k stages) joint policy tree q t =
ff

 t contained
q1 ,...,qnt , to be used starting at stage t. For the last k stages, the contained joint policy
q  =ht,t specifies  = h  t  k stages.
A is the set of augmented actions. For 0  t  h  k  1, an action at  A is a joint
policy at=t =  t+k = h1t+k . . . nt+k i implicitly mapping length-k observation histories to
~ k  At+k . For the last k stages
joint actions to be taken at stage t + k. I.e., it+k : O
i
i
h  k  t  h  1 there only is one empty action a that has no influence whatsoever.
The augmented actions are used to expand the joint policy trees. When appending a
policy  t+k to q t we form a depth k + 1 policy, which we denote q  =k+1,t = hq t   t+k i. After
execution of its initial joint action q  =k+1,t (~o ) and receiving a particular joint observation o,
340

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs

st
a1
~ t ,

o1

a2
o2

o1

a1

t
o2

a2

a1

a2

t+1

ot+1 = ho1 ,o2 i
st+1
a2

a1
~ t+1 ,

o1
a1

o2

o1
a1

a2

t+1
o2
a2

t+k

Figure 15: An illustration of the augmented MDP with k = 2, showing a transition from
st to st+1 by action a =  t . In this example ~ tk+1 = (~ tk+1 , ha1 ,a2 i , ho1 ,o2 i).
The actions specified for stage t are given by  t (ho1 ,o2 i) as depicted in Figure 14.

a q  =k+1,t reduces to its depth k sub-tree policy for that particular joint observation, denoted
q tk+1 = q  =k+1,t (o) = hq tk   t i(o). This is illustrated in Figure 14.
T is the transition model. A probability P (st+1 |st ,at ) for stage t = t translates as follows
for 0  t  h  k  1
(
P (ot+1 |~ t ,q t (~o )) if conditions hold,
t+1
t+1
t
t
t+k
P (h~ ,q i|h~ ,q i, ) =
(A.1)
0
otherwise,
where the conditions are: 1) q t+1 = hq t   t+k i(ot+1 ), and 2) ~ t+1 = (~ t ,at ,ot+1 ). For
h  k  t  h  1,  t+k in (A.1) reduces to a . The probabilities are unaffected, but the
first condition changes to q  =ht1,t+1 = q  =ht,t (ot+1 ).
Finally, R is the reward model, which is specified as follows:
0th1

R(st=t ) = R(h~ t ,q t i) = R(~ t ,q t (~o )),

(A.2)

where q t (~o ) is the initial joint action specified by q t . R(~ t ,a) is defined as before in (2.12).
The resulting optimality equations Qt (s,a) for the augmented MDP are as follows. We
will write Qk for the optimal Q-value function for a k-steps delayed communication system.
We will also refer to this as the k-QBG value function.
X
P (ot+1 |~ t ,q t (~o ))Qk (~ t+1 ,q t+1 ), (A.3)
0thk1 Qk (~ t ,q t , t+k ) = R(~ t ,q t (~o ))+
ot+1 O

341

fiOliehoek, Spaan & Vlassis

with q t+1 = hq t   t+k i(ot+1 ) and where
Qk (~ t ,q t )  max Qk (~ t ,q t , t+k ).

(A.4)

 t+k

For the last k stages, h  k  t  h  1, there are   = h  t stages to go and we get




Qk (~ t ,q  = ,t ) = R(~ t ,q  = ,t (~o )) +

X



P (ot+1 |~ t ,q  = ,t (~o ))Qk (~ t+1 ,q  =

 1,t+1

). (A.5)

ot+1

Note that (A.5) does not include any augmented actions at=t =  t+k . Therefore, the last
k stages should be interpreted as a Markov chain. Standard dynamic programming can be
applied to calculate all Q (~ t ,q t )-values.
A.2.2 Relation of k-QBG with Other Approximate Q-value Functions
Here we briefly show how k-QBG in fact reduces to some of the cases treated earlier.
For k = 0, k-QBG (A.3) reduces to QPOMDP . In the k = 0 case, q tk becomes a depth0, i.e. empty, policy. Also,  t becomes a mapping from length-0 observation histories to
actions, i.e., it becomes a joint action. Substitution in (A.3) yields
E
E
D
D
X
Q0 ( ~ t , ,at ) = R(~ t ,at ) +
P (ot+1 |~ t ,at ) max Q0 ( ~ t+1 , ,at+1 ).
at+1

ot+1

b P (~ t ,a)  Q (b~ t ,a), this clearly corresponds to the QPOMDP -value function
Now, as Q
P
(5.6).
1-QBG reduces to regular QBG . Notice that for k = 1, q  =k,t reduces to at . Filling out
yields:
E
D
E
D
X
Q1 ( ~ t ,at , t+1 ) = R(~ t ,at ) +
P (ot+1 |~ t ,at ) max Q1 ( ~ t+1 , t+1 (ot+1 ) , t+2 ).
 t+2

ot+1

Now using (A.4) we obtain the QBG -value function (5.12).
A Dec-POMDP is identical to an h-steps delayed communication system. Augmented
states have the form st=0 = h~ ,q 0 i, where q 0 =  specifies a full length h joint policy. The
first stage t = 0 in this augmented MDP, is also one of the last k (= h) stages. Therefore,
the applied Q-function is (A.5), which means that the Markov chain evaluation starts immediately. Effectively this boils down to evaluation of all joint policies (corresponding to
all augmented start states). The maximizing one specifies the value function of an optimal
joint policy Q .
A.2.3 Shorter Communication Delays cannot Decrease the Value
First, we introduce some notation. Let us write Po for all the observation probabilities given
~ t ,q t and the sequence of intermediate observations (ot+1 , . . . ,ot+l1 )
h
i
Po (ot+l )  P ot+l |(~ t ,q t (~o ),ot+1 ,q t (ot+1 ), . . . ,ot+l1 ),q t (ot+l1 ) ,
342

l  k.

(A.6)

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs

t for a policy that implicitly maps k-length
In order to avoid confusion, we write |k|
t
observation histories to actions, and |k+1|
for one that is a mapping from length (k + 1)
observation-histories to actions.
~t t
~t t
Now we give a reformulation of Qk . Qt,
k ( ,q ) specifies the expected return for  ,q
over stages t,t + 1, . . . ,h  1. Here, we will split this

Qk (~ t ,q t ) = Kk (~ t ,q t ) + Fkt, (~ t ,q t )

(A.7)

in Kk (~ t ,q t ), the expected k-step reward, i.e., the expected return over for stages t, . . . ,t+k1
and Fkt, (~ t ,q t ), the expected return over stages t + k,t + k + 1, . . . ,h  1, referred to as the
in k-steps expected return.
The former is defined as
Kk (~ t ,q t )  E

"t+k1
X
t =t

#
fi
fi
R(~ ,a ) fi ~ t ,q t .
t

t

(A.8)

Let us define K  =i (~ t ,q  =i,t ) as the expected reward for the next i stages, i.e.,
Kk (~ t ,q t ) = K  =k (~ t ,q t ).

(A.9)

We then have K  =1 (~ t ,at ) = R(~ t ,at ) and
K  =i (~ t ,q  =i,t ) = R(~ t ,q  =i,t (~o ))+
X
P (ot+1 |~ t ,q  =i,t (~o ))K  =i1 (~ t+1 ,q  =i1,t+1 (ot+1 )), (A.10)
ot+1

where q  =i1,t+1 (ot+1 ) is the depth-(i  1) joint policy that results from q  =i,t after observation of ot+1 .
t+k
If we define Fk =i,t (~ t ,q t ,|k|
) to be the expected reward for stages t + i,t + i + 1, . . . ,h  1.
That is, the time-to-go  = i denotes how much time-to-go before we start accumulating
expected reward. The in k-steps expected return is then given by
t+k
t+k
Fkt (~ t ,q t ,|k|
) = Fk =k,t (~ t ,q t ,|k|
).

The evaluation is then performed by
t+k
t+k
)
) = Qk (~ t ,q t ,|k|
Fk =0,t (~ t ,q t ,|k|
X
t+k
Po (ot+1 )Fk =i1,t+1, (~ t+1 ,q t+1 ),
Fk =i,t (~ t ,q t ,|k|
) =

(A.11)
(A.12)

ot+1

t+k
i(ot+1 ), and where
where q t+1 = hq t+1  |k|
t+k
).
Fk =i,t, (~ t ,q t ) = max Fk =i,t (~ t ,q t ,|k|
t+k
|k|

343

(A.13)

fiOliehoek, Spaan & Vlassis

Theorem A.1 (Shorter communication delays cannot decrease the value). The optimal
Q-value function Qk of a finite horizon Dec-POMDP with k-steps delayed communication
is an upper bound to Qk+1 , that of a k + 1-steps delayed communication system. That is
t ~ t q =k,t , t+k
|k|

t+k
t+k+1
t+k
i,|k+1|
).
Qk (~ t ,q  =k,t ,|k|
)  max Qk+1 (~ t ,hq  =k,t  |k|
t+k+1
|k+1|

(A.14)

Proof. The proof is by induction. The base case is that (A.14) holds for stages h  (k +
1)  t  h  1, as shown by lemma A.1. The induction hypothesis states that, assuming
(A.14) holds for some stage t + k, it also holds for stage t. The induction step is proven in
lemma A.2.
Lemma A.1 (Base case). For all h  k  1  t  h  1, the expected cumulative future
reward under k steps delay is equal to that under k + 1 steps delay if the same policies are
followed from that point. That is,
hkth1 ~ t q =ht,t

Qk (~ t ,q  =ht,t ) = Qk+1 (~ t ,q  =ht,t ),

(A.15)

and ~ hk1 q =k,hk1 , h1
|k|

h1
h1
Qk (~ hk1 ,q  =k,hk1 ,|k|
) = Qk+1 (~ hk1 ,hq  =k,hk1  |k|
i).

(A.16)


Proof. For a particular stage t = h    with h  k  t  h  1 and an arbitrary ~ t ,q  = ,t ,
we can write


Qk (~ t ,q  = ,t ) = Qk+1 (~ t ,q  = ,t ),

because both are given by the evaluation of (A.5), and this evaluation involves no actions:
Basically (A.5) has reduced to a Markov chain, and this Markov chain is the same for Qk
and Qk+1 . We can conclude that
hkth1 ~ t ,q =  ,t



Qk (~ t ,q  = ,t ) = Qk+1 (~ t ,q  = ,t ).

Now we will prove (A.16). The left side of (A.16) is given by application of (A.3)
h1
Qk (~ hk1 ,q  =k,hk1 ,|k|
) = R(~ hk1 ,q  =k,hk1 (~o ))+
X
P (ohk |~ hk1 ,q  =k,hk1 (~o ))Qk (~ hk ,q  =k,hk ),
ohk

h1
with q  =k,hk = hq  =k,hk1  |k|
i(ohk ). The right side is given by application of (A.5)
h1
Qk+1 (~ hk1 ,hq  =k,hk1  |k|
i) = R(~ hk1 ,q  =k,hk1 (~o ))+
X
P (ohk |~ hk1 ,q  =k,hk1 (~o ))Qk+1 (~ hk ,q  =k,hk )
ohk

h1
with q  =k,hk = hq  =k,hk1  |k|
i(ohk ). Now, because the policies q  =k,hk are the
same, we get
Qk (~ hk ,q  =k,hk ) = Qk+1 (~ hk ,q  =k,hk )

and thus (A.16) holds.
344

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs

Lemma A.2 (Induction step). Given that
~ t q =k,t , t +k
|k|















t +k
t +k
t +k+1
Qk (~ t ,q  =k,t ,|k|
)  max
Qk+1 (~ t ,hq  =k,t  |k|
i,|k+1|
) (A.17)

t +k+1
|k+1|

holds for t = t + (k + 1), then
~ t q =k,t , t+k
|k|

t+k
t+k+1
t+k
i,|k+1|
)
Qk (~ t ,q  =k,t ,|k|
)  max Qk+1 (~ t ,hq  =k,t  |k|

(A.18)

t+k+1
|k+1|

holds for stage t.
Proof. For the k-steps delay Q-function, we can write
t+k
Qk (~ t ,q  =k,t ,|k|
) = R(~ t ,q  =k,t (~o ))+
h
i
X
t+k+1
) (A.19)
Po (ot+1 |~ t ,q  =k,t (~o )) max Kk (~ t+1 ,q t+1 ) + Fkt+1 (~ t+1 ,q  =k,t+1 ,|k|
t+k+1
|k|

ot+1

t+k
t+k+1
where q  =k,t+1 = hq  =k,t |k|
i(ot+1 ). Because Kk is independent of |k|
, we can regroup
the terms to get

~t

Qk ( ,q

 =k,t

t+k
,|k|
)

"

~t

= R( ,q

 =k,t

"

(~o )) +

X

Po (o

t+1

)Kk (

ot+1

X

ot+1

~ t+1

,q

 =k,t+1

#

) +
#

t+k+1
) . (A.20)
Po (ot+1 ) max Fkt+1 (~ t+1 ,q  =k,t+1 ,|k|
t+k+1
|k|

In the case of k + 1-steps delay, we can write
t+k
t+k
t+k
t+k
t+k+1
t
Qk+1 (~ t ,hq  =k,t  |k|
i,|k+1|
) = Kk+1 (~ t ,hq  =k,t  |k|
i) + Fk+1
(~ t ,hq  =k,t  |k+1|
i,|k+1|
)
(A.21)
where, per definition (by (A.9) and (A.10))
t+k
Kk+1 (~ t ,hq  =k,t  |k|
i) = R(~ t ,q  =k,t (~o )) +

X

Po (ot+1 )K  =k (~ t+1 ,q  =k,t+1 ),

ot+1

= R(~ t ,q  =k,t (~o )) +

X

Po (ot+1 )Kk (~ t+1 ,q  =k,t+1 ),

(A.22)

ot+1

where q  =k,t+1 = hq  =k,t   t+k i(ot+1 ).
Equation (A.22) is equal to the first part in (A.20). Therefore, for an arbitrary ~ t ,q  =k,t
t+k
, we know that (A.18) holds if and only if
and |k|
X

ot+1

t+k
t+k+1
t+k+1
t
(~ t ,hq  =k,t  |k|
i,|k+1|
)
)  max Fk+1
Po (ot+1 ) max Fkt+1 (~ t+1 ,q  =k,t+1 ,|k|
t+k+1
|k+1|

t+k+1
|k|

(A.23)
345

fiOliehoek, Spaan & Vlassis

t+k
t
where q  =k,t+1 = hq  =k,t  |k|
i(ot+1 ). When filling this out and expanding Fk+1
using
(A.12) we get

X

t+k
t+k+1
i(ot+1 ),|k|
)
Po (ot+1 ) max Fk =k,t+1 (~ t+1 ,hq  =k,t  |k|
t+k+1
|k|

ot+1

max

t+k+1
|k+1|

X

 =k,t+1, ~ t+1
t+k
t+k+1
Po (ot+1 )Fk+1
( ,hhq  =k,t  |k|
i  |k+1|
i(ot+1 )). (A.24)

ot+1

This clearly holds if
X
t+k
t+k+1
i(ot+1 ),|k|
)
Po (ot+1 ) max Fk =k,t+1 (~ t+1 ,hq  =k,t  |k|
t+k+1
|k|

ot+1

X

 =k,t+1, ~ t+1
t+k
t+k+1
( ,hhq  =k,t  |k|
i(ot+1 )  |k|
i), (A.25)
Po (ot+1 ) max Fk+1
t+k+1
|k|

ot+1

because the second part of (A.25) is an upper bound to the second part of (A.24). Therefore,
the induction step is proved if we can show that
ot+1  t+k+1
|k|

t+k
t+k+1
i(ot+1 ),|k|
)
Fk =k,t+1 (~ t+1 ,hq  =k,t  |k|
 =k,t+1, ~ t+1
t+k
t+k+1
Fk+1
( ,hhq  =k,t  |k|
i(ot+1 )  |k|
i). (A.26)

t+k
which through (A.13) and q  =k,t+1 = hq  =k,t  |k|
i(ot+1 ) transforms to

q =k,t+1  t+k+1
|k|

t+k+1
Fk =k,t+1 (~ t+1 ,q  =k,t+1 ,|k|
)
 =k,t+1 ~ t+1
t+k+1
t+k+2
( ,hq  =k,t+1  |k|
i,|k+1|
). (A.27)
max Fk+1

t+k+2
|k+1|

Now, we apply (A.11) to the induction hypothesis (A.17) and yield
~ t q =k,t , t +k
|k|





 =0,t ~ t
t +k
t +k
t +k+1
Fk =0,t (~ t ,q  =k,t ,|k|
)  max
Fk+1
( ,hq  =k,t  |k|
i,|k+1|
).

t +k+1
|k+1|

(A.28)
Application of lemma A.4 to this transformed induction hypothesis asserts (A.27) and
thereby proves the lemma.
Auxiliary Lemmas.
Lemma A.3. If, at stage t, the in i-steps expected return for a k-steps delayed system is
higher than a (k + 1)-steps delayed system, then at t  1 the in (i + 1)-steps expected return
for a k-steps delayed system is higher than the (k + 1)-steps delayed system. That is, if for
t1+k
a particular q  =k,t = hq  =k,t1  |k|
i(ot )
 t+k ot
|k|

t+k
t1+k
t+k
Fk =i,t (~ t ,q  =k,t ,|k|
) = Fk =i,t (~ t ,hq  =k,t1  |k|
i(ot ),|k|
)
 =i,t ~ t
t+k
t+k
t+k+1
( ,hhq  =k,t1  |k|
i(ot )  |k|
i,|k+1|
) (A.29)
max Fk+1

t+k+1
|k+1|

346

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs

holds, then
 =i+1,t1 ~ t1
t1+k
t+k
t1+k
( ,hq  =k,t1  |k|
i,|k+1|
).
Fk =i+1,t1 (~ t1 ,q  =k,t1 ,|k|
)  max Fk+1
t+k
|k+1|

(A.30)
Proof. The following derivation
t1+k
Fk =i+1,t1 (~ t1 ,q  =k,t1 ,|k|
)
h
i
X
t1+k
t+k
Po (ot ) max Fk =i,t (~ t ,hq  =k,t1  |k|
i(ot ),|k|
)
=
t+k
|k|

ot



X

t

Po (o ) max
t+k
|k|

ot

 max

t+k
|k+1|

= max

t+k
|k+1|

X

"

Po (ot )

 =i,t ~ t
( ,hhq  =k,t1
max Fk+1
t+k+1
|k+1|

"

 =i,t ~ t
( ,hhq  =k,t1
max Fk+1
t+k+1
|k+1|

ot
 =i+1,t1 ~ t1
Fk+1
( ,hq  =k,t1



t1+k
|k|
i(ot )



#

t+k
t+k+1
|k|
i,|k+1|
)

#

t1+k
t+k
t+k+1
 |k|
i  |k+1|
i(ot ),|k+1|
)

t1+k
t+k
 |k|
i,|k+1|
)

proves the lemma.
Lemma A.4. If, for some stage t
~ t ,q =k,t , t+k
|k|

 =0,t ~ t
t+k
t+k+1
t+k
( ,hq  =k,t  |k|
i,|k+1|
)
Fk =0,t (~ t ,q  =k,t ,|k|
)  max Fk+1
t+k+1
|k+1|

(A.31)

holds, then i ~ ti ,q =k,ti , ti+k
|k|

 =i,ti ~ ti
ti+k
ti+k+1
ti+k
( ,hq  =k,ti  |k|
i,|k+1|
).
Fk =i,ti (~ ti ,q  =k,ti ,|k|
)  max Fk+1
ti+k+1
|k+1|

(A.32)
t+k
Proof. If (A.31) holds for all ~ t , q  =k,t , |k|
, then eq. (A.29) is satisfied for all ~ t , q  =k,t ,
t+k
, and lemma (A.3) yields ~ t1 ,q =k,t1 , t1+k
|k|
 =1,t1 ~ t1
t1+k
t+k
Fk =1,t1 (~ t1 ,q  =k,t1 ,|k|
)  max Fk+1
( ,hq  =k,t1   t+k1 i,|k+1|
). (A.33)
t+k
|k+1|

At this point we can apply the lemma again, etc. The i-th application of the lemma yields
(A.32).

347

fiOliehoek, Spaan & Vlassis

References
Aicardi, M., Davoli, F., & Minciardi, R. (1987). Decentralized optimal control of Markov
chains with a common past information set. IEEE Transactions on Automatic Control,
32 (11), 10281031.
Altman, E. (2002). Applications of Markov decision processes in communication networks.
In Feinberg, E. A., & Shwartz, A. (Eds.), Handbook of Markov Decision Processes:
Methods and Applications. Kluwer Academic Publishers.
Amato, C., Bernstein, D. S., & Zilberstein, S. (2006). Optimal fixed-size controllers for
decentralized POMDPs. In Proc. of the AAMAS Workshop on Multi-Agent Sequential
Decision Making in Uncertain Domains (MSDM).
Amato, C., Bernstein, D. S., & Zilberstein, S. (2007a). Optimizing memory-bounded controllers for decentralized POMDPs. In Proc. of Uncertainty in Artificial Intelligence.
Amato, C., Carlin, A., & Zilberstein, S. (2007b). Bounded dynamic programming for decentralized POMDPs. In Proc. of the AAMAS Workshop on Multi-Agent Sequential
Decision Making in Uncertain Domains (MSDM).
Arai, T., Pagello, E., & Parker, L. (2002). Editorial: Advances in multirobot systems. IEEE
Transactions on Robotics and Automation, 18 (5), 655661.
Aras, R., Dutech, A., & Charpillet, F. (2007). Mixed integer linear programming for exact finite-horizon planning in decentralized POMDPs. In Proc. of the International
Conference on Automated Planning and Scheduling.
Becker, R., Lesser, V., & Zilberstein, S. (2005). Analyzing myopic approaches for multiagent communication. In Proc. of the International Conference on Intelligent Agent
Technology, pp. 550557.
Becker, R., Zilberstein, S., & Lesser, V. (2004a). Decentralized Markov decision processes
with event-driven interactions. In Proc. of the International Joint Conference on
Autonomous Agents and Multi Agent Systems, pp. 302309.
Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. V. (2004b). Solving transition independent decentralized Markov decision processes. Journal of Artificial Intelligence
Research, 22, 423455.
Bernstein, D. S. (2005). Complexity Analysis and Optimal Algorithms for Decentralized
Decision Making. Ph.D. thesis, University of Massachusets Amherst.
Bernstein, D. S., Givan, R., Immerman, N., & Zilberstein, S. (2002). The complexity
of decentralized control of Markov decision processes. Mathematics of Operations
Research, 27 (4), 819840.
Bernstein, D. S., Hansen, E. A., & Zilberstein, S. (2005). Bounded policy iteration for
decentralized POMDPs. In Proc. of the International Joint Conference on Artificial
Intelligence, pp. 12871292.
Bertsekas, D. P. (2005). Dynamic Programming and Optimal Control (3rd edition)., Vol. I.
Athena Scientific.
348

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs

Beynier, A., & Mouaddib, A.-I. (2005). A polynomial algorithm for decentralized Markov
decision processes with temporal constraints. In Proc. of the International Joint
Conference on Autonomous Agents and Multi Agent Systems, pp. 963969.
Beynier, A., & Mouaddib, A.-I. (2006). An iterative algorithm for solving constrained
decentralized Markov decision processes. In Proc. of the National Conference on
Artificial Intelligence.
Binmore, K. (1992). Fun and Games. D.C. Heath and Company.
de Boer, P.-T., Kroese, D. P., Mannor, S., & Rubinstein, R. Y. (2005). A tutorial on the
cross-entropy method. Annals of Operations Research, 134 (1), 1967.
Boutilier, C. (1996). Planning, learning and coordination in multiagent decision processes.
In Proc. of the 6th Conference on Theoretical Aspects of Rationality and Knowledge,
pp. 195210.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions and computational leverage. Journal of Artificial Intelligence Research,
11, 194.
Chades, I., Scherrer, B., & Charpillet, F. (2002). A heuristic approach for solving
decentralized-POMDP: assessment on the pursuit problem. In Proc. of the 2002 ACM
Symposium on Applied Computing, pp. 5762.
Cogill, R., Rotkowitz, M., Roy, B. V., & Lall, S. (2004). An approximate dynamic programming approach to decentralized control of stochastic systems. In Proc. of the 2004
Allerton Conference on Communication, Control, and Computing.
Emery-Montemerlo, R. (2005). Game-Theoretic Control for Robot Teams. Ph.D. thesis,
Carnegie Mellon University.
Emery-Montemerlo, R., Gordon, G., Schneider, J., & Thrun, S. (2004). Approximate solutions for partially observable stochastic games with common payoffs. In Proc. of the
International Joint Conference on Autonomous Agents and Multi Agent Systems, pp.
136143.
Emery-Montemerlo, R., Gordon, G., Schneider, J., & Thrun, S. (2005). Game theoretic
control for robot teams. In Proc. of the IEEE International Conference on Robotics
and Automation, pp. 11751181.
Fernandez, J. L., Sanz, R., Simmons, R. G., & Dieguez, A. R. (2006). Heuristic anytime
approaches to stochastic decision processes. Journal of Heuristics, 12 (3), 181209.
Gmytrasiewicz, P. J., & Doshi, P. (2005). A framework for sequential planning in multiagent settings. Journal of Artificial Intelligence Research, 24, 4979.
Goldman, C. V., Allen, M., & Zilberstein, S. (2007). Learning to communicate in a decentralized environment. Autonomous Agents and Multi-Agent Systems, 15 (1), 4790.
Goldman, C. V., & Zilberstein, S. (2003). Optimizing information exchange in cooperative
multi-agent systems. In Proc. of the International Joint Conference on Autonomous
Agents and Multi Agent Systems, pp. 137144.
349

fiOliehoek, Spaan & Vlassis

Goldman, C. V., & Zilberstein, S. (2004). Decentralized control of cooperative systems:
Categorization and complexity analysis.. Journal of Artificial Intelligence Research,
22, 143174.
Guestrin, C., Koller, D., Parr, R., & Venkataraman, S. (2003). Efficient solution algorithms
for factored MDPs. Journal of Artificial Intelligence Research, 19, 399468.
Hansen, E. A., Bernstein, D. S., & Zilberstein, S. (2004). Dynamic programming for partially observable stochastic games. In Proc. of the National Conference on Artificial
Intelligence, pp. 709715.
Hauskrecht, M. (2000). Value-function approximations for partially observable Markov
decision processes.. Journal of Artificial Intelligence Research, 13, 3394.
Hsu, K., & Marcus, S. (1982). Decentralized control of finite state Markov processes. IEEE
Transactions on Automatic Control, 27 (2), 426431.
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning and acting in
partially observable stochastic domains. Artificial Intelligence, 101 (1-2), 99134.
Kim, Y., Nair, R., Varakantham, P., Tambe, M., & Yokoo, M. (2006). Exploiting locality of interaction in networked distributed POMDPs. In Proc. of the AAAI Spring
Symposium on Distributed Plan and Schedule Management.
Kitano, H., Asada, M., Kuniyoshi, Y., Noda, I., & Osawa, E. (1997). RoboCup: The robot
world cup initiative. In Proc. of the International Conference on Autonomous Agents.
Kitano, H., Tadokoro, S., Noda, I., Matsubara, H., Takahashi, T., Shinjoh, A., & Shimada,
S. (1999). Robocup rescue: Search and rescue in large-scale disasters as a domain for
autonomous agents research. In Proc. of the International Conference on Systems,
Man and Cybernetics, pp. 739743.
Koller, D., Megiddo, N., & von Stengel, B. (1994). Fast algorithms for finding randomized strategies in game trees. In Proc. of the 26th ACM Symposium on Theory of
Computing, pp. 750759.
Koller, D., & Pfeffer, A. (1997). Representations and solutions for game-theoretic problems.
Artificial Intelligence, 94 (1-2), 167215.
Kuhn, H. (1953). Extensive games and the problem of information. Annals of Mathematics
Studies, 28, 193216.
Lesser, V., Ortiz Jr., C. L., & Tambe, M. (Eds.). (2003). Distributed Sensor Networks: A
Multiagent Perspective, Vol. 9. Kluwer Academic Publishers.
Littman, M., Cassandra, A., & Kaelbling, L. (1995). Learning policies for partially observable environments: Scaling up. In Proc. of the International Conference on Machine
Learning, pp. 362370.
Marecki, J., & Tambe, M. (2007). On opportunistic techniques for solving decentralized
Markov decision processes with temporal constraints. In Proc. of the International
Joint Conference on Autonomous Agents and Multi Agent Systems, pp. 18.
Nair, R., Tambe, M., & Marsella, S. (2003). Team formation for reformation in multiagent
domains like RoboCupRescue. In Proc. of RoboCup-2002 International Symposium.
350

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs

Nair, R., Roth, M., & Yohoo, M. (2004). Communication for improving policy computation in distributed POMDPs. In Proc. of the International Joint Conference on
Autonomous Agents and Multi Agent Systems, pp. 10981105.
Nair, R., Tambe, M., & Marsella, S. (2002). Team formation for reformation. In Proc. of
the AAAI Spring Symposium on Intelligent Distributed and Embedded Systems.
Nair, R., Tambe, M., & Marsella, S. (2003a). Role allocation and reallocation in multiagent
teams: towards a practical analysis. In Proc. of the International Joint Conference on
Autonomous Agents and Multi Agent Systems, pp. 552559.
Nair, R., Tambe, M., Yokoo, M., Pynadath, D. V., & Marsella, S. (2003b). Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings. In
Proc. of the International Joint Conference on Artificial Intelligence, pp. 705711.
Nair, R., Varakantham, P., Tambe, M., & Yokoo, M. (2005). Networked distributed
POMDPs: A synthesis of distributed constraint optimization and POMDPs. In Proc.
of the National Conference on Artificial Intelligence, pp. 133139.
Nash, J. F. (1950). Equilibrium points in N-person games. Proc. of the National Academy
of Sciences of the United States of America, 36, 4849.
Oliehoek, F., & Vlassis, N. (2006). Dec-POMDPs and extensive form games: equivalence
of models and algorithms. Ias technical report IAS-UVA-06-02, University of Amsterdam, Intelligent Systems Lab, Amsterdam, The Netherlands.
Oliehoek, F. A., Kooij, J. F., & Vlassis, N. (2007a). A cross-entropy approach to solving
Dec-POMDPs. In Proc. of the International Symposium on Intelligent and Distributed
Computing, pp. 145154.
Oliehoek, F. A., Spaan, M. T. J., & Vlassis, N. (2007b). Dec-POMDPs with delayed communication. In Proc. of the AAMAS Workshop on Multi-Agent Sequential Decision
Making in Uncertain Domains (MSDM).
Oliehoek, F. A., Spaan, M. T. J., Whiteson, S., & Vlassis, N. (2008). Exploiting locality of
interaction in factored Dec-POMDPs. In Proc. of the International Joint Conference
on Autonomous Agents and Multi Agent Systems.
Oliehoek, F. A., & Visser, A. (2006). A hierarchical model for decentralized fighting of large
scale urban fires. In Proc. of the AAMAS06 Workshop on Hierarchical Autonomous
Agents and Multi-Agent Systems (H-AAMAS), pp. 1421.
Oliehoek, F. A., & Vlassis, N. (2007). Q-value functions for decentralized POMDPs. In
Proc. of the International Joint Conference on Autonomous Agents and Multi Agent
Systems, pp. 833840.
Ooi, J. M., & Wornell, G. W. (1996). Decentralized control of a multiple access broadcast channel: Performance bounds. In Proc. of the 35th Conference on Decision and
Control, pp. 293298.
Osborne, M. J., & Rubinstein, A. (1994). A Course in Game Theory. The MIT Press.
Papadimitriou, C. H., & Tsitsiklis, J. N. (1987). The complexity of Markov decision processes. Mathematics of Operations Research, 12 (3), 441451.
351

fiOliehoek, Spaan & Vlassis

Paquet, S., Tobin, L., & Chaib-draa, B. (2005). An online POMDP algorithm for complex multiagent environments. In Proc. of the International Joint Conference on
Autonomous Agents and Multi Agent Systems.
Peshkin, L. (2001). Reinforcement Learning by Policy Search. Ph.D. thesis, Brown University.
Peshkin, L., Kim, K.-E., Meuleau, N., & Kaelbling, L. P. (2000). Learning to cooperate via
policy search. In Proc. of Uncertainty in Artificial Intelligence, pp. 307314.
Pineau, J., Gordon, G., & Thrun, S. (2003). Point-based value iteration: An anytime
algorithm for POMDPs. In Proc. of the International Joint Conference on Artificial
Intelligence, pp. 10251032.
Puterman, M. L. (1994). Markov Decision ProcessesDiscrete Stochastic Dynamic Programming. John Wiley & Sons, Inc.
Pynadath, D. V., & Tambe, M. (2002). The communicative multiagent team decision
problem: Analyzing teamwork theories and models. Journal of Artificial Intelligence
Research, 16, 389423.
Romanovskii, I. (1962). Reduction of a game with complete memory to a matrix game.
Soviet Mathematics, 3, 678681.
Roth, M., Simmons, R., & Veloso, M. (2005). Reasoning about joint beliefs for executiontime communication decisions. In Proc. of the International Joint Conference on
Autonomous Agents and Multi Agent Systems, pp. 786793.
Roth, M., Simmons, R., & Veloso, M. (2007). Exploiting factored representations for decentralized execution in multi-agent teams. In Proc. of the International Joint Conference
on Autonomous Agents and Multi Agent Systems, pp. 467463.
Russell, S., & Norvig, P. (2003). Artificial Intelligence: A Modern Approach (2nd edition).
Pearson Education.
Schoute, F. C. (1978). Symmetric team problems and multi access wire communication.
Automatica, 14, 255269.
Seuken, S., & Zilberstein, S. (2007a). Improved memory-bounded dynamic programming
for decentralized POMDPs. In Proc. of Uncertainty in Artificial Intelligence.
Seuken, S., & Zilberstein, S. (2007b). Memory-bounded dynamic programming for DECPOMDPs.. In Proc. of the International Joint Conference on Artificial Intelligence,
pp. 20092015.
Sondik, E. J. (1971). The optimal control of partially observable Markov decision processes.
Ph.D. thesis, Stanford University.
Spaan, M. T. J., Gordon, G. J., & Vlassis, N. (2006). Decentralized planning under uncertainty for teams of communicating agents. In Proc. of the International Joint
Conference on Autonomous Agents and Multi Agent Systems, pp. 249256.
Spaan, M. T. J., & Melo, F. S. (2008). Interaction-driven Markov games for decentralized
multiagent planning under uncertainty. In Proc. of the International Joint Conference
on Autonomous Agents and Multi Agent Systems.
352

fiOptimal and Approximate Q-Value Functions for Dec-POMDPs

Spaan, M. T. J., & Vlassis, N. (2005). Perseus: Randomized point-based value iteration for
POMDPs. Journal of Artificial Intelligence Research, 24, 195220.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction. The MIT
Press.
Szer, D., & Charpillet, F. (2005). An optimal best-first search algorithm for solving infinite
horizon DEC-POMDPs. In Proc. of the European Conference on Machine Learning,
pp. 389399.
Szer, D., & Charpillet, F. (2006). Point-based dynamic programming for DEC-POMDPs..
In Proc. of the National Conference on Artificial Intelligence.
Szer, D., Charpillet, F., & Zilberstein, S. (2005). MAA*: A heuristic search algorithm for
solving decentralized POMDPs. In Proc. of Uncertainty in Artificial Intelligence, pp.
576583.
Tao, N., Baxter, J., & Weaver, L. (2001). A multi-agent policy-gradient approach to network
routing. In Proc. of the International Conference on Machine Learning, pp. 553560.
Varakantham, P., Marecki, J., Yabu, Y., Tambe, M., & Yokoo, M. (2007). Letting loose
a SPIDER on a network of POMDPs: Generating quality guaranteed policies. In
Proc. of the International Joint Conference on Autonomous Agents and Multi Agent
Systems.
Varakantham, P., Nair, R., Tambe, M., & Yokoo, M. (2006). Winning back the cup for
distributed POMDPs: planning over continuous belief spaces. In Proc. of the International Joint Conference on Autonomous Agents and Multi Agent Systems, pp.
289296.
Wu, J., & Durfee, E. H. (2006). Mixed-integer linear programming for transitionindependent decentralized MDPs. In Proc. of the International Joint Conference on
Autonomous Agents and Multi Agent Systems, pp. 10581060.
Xuan, P., Lesser, V., & Zilberstein, S. (2001). Communication decisions in multi-agent
cooperation: Model and experiments. In Proc. of the International Conference on
Autonomous Agents.

353

fiJournal of Artificial Intelligence Research 32 (2008) 525-564

Submitted 10/07; published 06/08

Efficiency and Envy-freeness in Fair Division of Indivisible
Goods: Logical Representation and Complexity
Sylvain Bouveret

sylvain.bouveret@onera.fr

ONERA Centre de Toulouse.
2, avenue Edouard Belin, BP74025.
31055 Toulouse cedex 4, FRANCE.

Jerome Lang

lang@irit.fr

IRIT-CNRS. 118, route de Narbonne.
31062 Toulouse cedex, FRANCE.

Abstract
We consider the problem of allocating fairly a set of indivisible goods among agents
from the point of view of compact representation and computational complexity. We start
by assuming that agents have dichotomous preferences expressed by propositional formulae. We express efficiency and envy-freeness in a logical setting, which reveals unexpected
connections to nonmonotonic reasoning. Then we identify the complexity of determining
whether there exists an efficient and envy-free allocation, for several notions of efficiency,
when preferences are represented in a succinct way (as well as restrictions of this problem).
We first study the problem under the assumption that preferences are dichotomous, and
then in the general case.

1. Introduction
Allocating goods to agents is an important issue that has been considered from different perspectives in economics (especially social choice theory) and in computer science (especially
Artificial Intelligence and Operations Research), and arises in various real-world settings:
auctions, divorce settlements, electronic spectrum and frequency allocation, airport traffic management, or the fair and efficient exploitation of Earth Observation Satellites (see
the survey from Chevaleyre, Dunne, Endriss, Lang, Lematre, Maudet, Padget, Phelps,
Rodrguez-Aguilar, & Sousa, 2006, for a detailed description). The general issue also covers
a huge variety of allocation problems, depending on the following parameters (see again the
work from Chevaleyre et al., 2006, for a detailed taxonomy):
 the nature of the resources to be allocated (are they divisible or not? single-unit or
multi-unit?);
 the nature of the preferences of the agents (are they numerical or simply ordinal? can
there be preferential dependencies between goods?)
 the nature of the permitted allocations (can goods be shared among several agents?
do all goods have to be allocated? can allocations be accompanied by side payments?);
 the evaluation of the quality of an allocation (Pareto-efficiency, utilitarian or egalitarian social welfare etc.);
c
2008
AI Access Foundation. All rights reserved.

fiBouveret & Lang

 the nature of the process that leads to the allocation (centralized or decentralized).
For instance, standard combinatorial auctions (Cramton, Shoham, & Steinberg, 2005)
typically correspond to indivisible goods (possible multi-unit), numerical preferences with
possible dependencies between goods, monetary payments, maximization of the total value
of sold items, and a centralized computation.
In this paper we focus on fair division of indivisible goods without money transfers1 . Fair
division makes a prominent use of ex-post fairness criteria such as equity and envy-freeness,
and on this point departs from auctions, that rather focus on other kinds of fairness (as
well as on efficiency of the procedure), such as truthful mechanisms, or fairness of the
procedure itself (Brams & Taylor, 1996; Young, 1995). A key concept in the literature on
fair division is envy-freeness: an allocation is envy-free if and only if each agent likes her
share at least as much as the share of any other agent. Ensuring envy-freeness is considered
a desirable property; however, envy-freeness alone does not suffice as a criterion for finding
satisfactory allocations (this is especially obvious if it is not compulsory to allocate all
goods: in this case, not allocating anything to anyone results in an envy-free allocation, yet
totally unsatisfactory), therefore it has to be paired with some efficiency criterion, such as
Pareto optimality or maximum social welfare. However, it is known that for any reasonable
notion of efficiency, there are profiles for which no efficient and envy-free allocation exists2
(Brams, Edelman, & Fishburn, 2003). This is even trivial if every good must be assigned
to someone: in this case, there are profiles for which not even an envy-free allocation exists.
Another well-known notion of fairness (that we do not consider in this paper, except for one
result) is Rawlsian egalitarianism, which says that an allocation is equitable if it maximizes
the satisfaction of the least satisfied agent. Unlike envy-freeness, egalitarianism requires
interpersonal comparability of preferences.
Whereas social choice theory has developed an important literature on fair division,
computational issues have rarely been considered. On the other hand, artificial intelligence
has studied these issues extensively, but until now has focused mainly on combinatorial
auctions and related classical utilitarian problems. Combinatorial auctions, aiming at maximizing the auctioneers revenue (consisting in the sum of the prices paid by all agents),
are only a specific form of allocation process, namely a pure utilitarian form with money
transfers in which considerations such as equity and fairness are not relevant. The literature
on combinatorial auctions and related problems has been investigating issues such as compact representation (so as to allow agents to express their bids in a concise way  see the
work from Nisan, 2005, for an overview) as well as computational complexity, algorithms,
tractable subclasses and approximation. Complexity issues for negotiation (where agents
exchange goods by means of deals) have also been studied (e.g. Dunne, Wooldridge, &
Laurence, 2005; Chevaleyre, Endriss, Estivie, & Maudet, 2004).
The above discussion reveals the existence of a gap (summarized in Table 1): compact
representation and complexity issues for fair division have received very little attention
until now, apart of the recent work (Lipton, Markakis, Mossel, & Saberi, 2004) which
1. Note that the possibility of money transfers reintroduces divisibility to some extent, considering money
as a particular  and divisible  good.
2. Consider for example the situation where we have a single item and two agents who both want it: in
that case, any allocation is either efficient but not envy-free (when the item is given to one of the two
agents), or envy-free but not efficient (if the item is not allocated to anyone).

526

fiEfficiency and Envy-freeness in Fair Division of Indivisible Goods

studies approximation schemes for envy-freeness. The need for compact representation
arises from the following dilemma, formulated by several social choice theorists: either (a)
allow agents to express any possible preference relation on the set of all subsets of items,
and end up with an exponentially large representation (which actually happens for example3
in the work from Herreiner & Puppe, 2002); or (b) severely restrict the set of expressible
preferences, typically by assuming additive independence between items, and then design
procedures where agents express preferences between single items, these preferences being
then extended to sets of items by assuming additivity, thus giving up the possibility of
expressing preferential dependencies such as complementarity and substitutability effects
among items; this is the path followed by Brams et al. (2003), Brams and King (2005) and
Demko and Hill (1998). Yet, as we advocate in this paper, conciliating conciseness and
expressivity is possible, by means of compact representation.
axiomatic study
auctions
(and related problems)

economics

fair division

economics
(especially social choice)

computational study
computer science
(especially AI)
?

Table 1: Computational issues in fair division.
As in most works on fair allocation of indivisible items, we focus on the joint search
for envy-freeness and efficiency. The impossibility to guarantee the existence of an efficient
envy-free allocation implies that determining whether there exists such an allocation is a
crucial task, since a positive answer leads to choose such an allocation whereas a negative
answer calls for a relaxation of one of the criteria, which has been investigated in few papers
(Lipton et al., 2004; Chevaleyre, Endriss, & Maudet, 2007b).
We consider the problem of determining whether there exists an efficient and envy-free
allocation from the point of view of compact representation and computational complexity.
First, since in most cases agents have preferential dependencies (or synergies) between
goods, we raise the issue of how a fair division problem of indivisible goods should be
expressed. We focus first in the simple case where agents have dichotomous preferences,
that is, they simply express a partition between satisfactory and unsatisfactory shares. The
interest of such a restriction is that in spite of the expressivity loss it imposes, it will be
shown to be no less complex than the general case, while being much simpler to expose.
Dichotomous preferences have been considered before in social choice contexts, such as by
Bogomolnaia, Moulin, and Stong (2005) in a fair division context, and of course in approval
voting (Brams & Fishburn, 1978), where every voter specifies a dichotomous preference on
the set of candidates. The most natural representation of a dichotomous preference (with
preferential dependencies between formulae  otherwise the problem is trivial) is by a single
3. Quoting from the work of Brams et al. (2003): Herreiner and Puppe (...) assume that each person has a
linear preference order in 2B . This allows for complementarity and substitutability effects among items
(...). In view of interdependencies that may beset subset evaluations (...), the procedures of Herreiner
and Puppe offer a creative way of dealing with subset preference. On the other hand, the sheer number
of subsets (more than a million when n = 20) and their presumption of clear preference between subsets,
could detract from the practicability of their procedures.

527

fiBouveret & Lang

propositional formula, where variables correspond to goods. Expressing envy-freeness and
efficiency within this logical representation reveals unexpected connections to nonmonotonic
reasoning; this issue will be addressed in Section 3.
The following Sections are devoted to a detailed complexity study of the following problem: given some fair division problem, does there exist an efficient end envy-free allocation?. The latter problem will be studied for different notions of efficiency and various
restrictions. We start (in Sections 3 and 4) by assuming that preferences are dichotomous,
and we identify the complexity of the existence of an envy-free and Pareto-efficient allocation, which turns out to be p2 -complete. Then we consider several restrictions of the
latter problem, namely, (a) fixing the number of agents to two; (b) forcing all agents to
have identical preferences; (c) restricting the syntax of the propositional formulae expressing the preferences of the agents. Then we study variations of the problem obtained by
replacing Pareto-efficiency by other notions of efficiency, namely: (a) asking for complete
allocations (such that every good is allocated to an agent); (b) requiring that a maximum
number of agents be satisfied. In Section 5 we then consider the more general problem
obtained by removing the assumption that preferences are dichotomous. As the problem
then becomes very dependent on the choice of a particular language for compact preference
representation, we pick a particular one (weighted propositional formulae) that extends in
a simple way the pure propositional representation considered in Section 3, and identify the
complexity of the existence of envy-free and efficient allocations, again for several notions
of efficiency. Finally, in Section 6 we sum up our contributions and discuss related work
and further issues.

2. Background
In this section we provide some basic concepts and definitions that we will use all along the
paper.
2.1 Fair Division Problems
Definition 1 (Fair division problem) A fair division problem4 is a tuple P = hI, X, Ri
where
 I = {1, . . . , N } is a set of agents;
 X = {x1 , . . . , xp } is a set of indivisible goods;
 R = hR1 , . . . , RN i is a preference profile, where each Ri is a reflexive, transitive and
complete relation on 2X .
Ri is the preference relation of agent i. ARi B is alternatively denoted by Ri (A, B) or
by A i B; we write A i B (strict preference) for (A i B and not B i A) and A i B
(indifference) for (A i B and B i A).
In addition, Ri is said to be monotonic if and only if for all A, B, A  B  X implies
B i A. R = hR1 , . . . , RN i is monotonic if and only if Ri is monotonic for every i.
4. In the following, we will use indifferently the terms fair division and resource allocation.

528

fiEfficiency and Envy-freeness in Fair Division of Indivisible Goods

Definition 2 (Allocation) An allocation for P = hI, X, Ri is a mapping  : I  2X
such that for all i and j 6= i, (i)  (j) = . If for every x  X there exists an i such that
x  (i) then  is a complete allocation.
In other words, it is not possible in our framework to give the same good to different
agents at the same time, but it is possible to throw away some goods. In this paper, we
will focus especially on two desirable properties of the allocations: Pareto-efficiency and
envy-freeness.
Definition 3 Let ,  0 two allocations.  dominates  0 if and only if (a) for all i, (i) i
 0 (i) and (b) there exists an i such that (i) i  0 (i).  is (Pareto-) efficient if and only
if there is no  0 such that  0 dominates .
Definition 4 An allocation  is envy-free if and only if (i) i (j) holds for all i and
all j 6= i.
2.2 Propositional Logic
Let V be a finite set of propositional variables. LV is the propositional language generated
from V , the usual connectives ,  and  and the Boolean constants > and  in the usual
way5 .
An interpretation M for LV is an element of 2V , i.e., a truth assignment to symbols:
for all x  V , x  M (resp. x 6 M ) means that M assigns x to true (resp. to false).
M od() = {M  2V | M |= } is the set of all models of  (the satisfaction relation |= is
defined as usual, as well as satisfiability and logical consequence).
A literal is a formula of LV of the form x or of the form x, where x  V . A formula
 is under negative normal form (or NNF) if and only if any negation symbol in  appears
only in literals. Any formula can be turned in polynomial time into an equivalent NNF
formula. For instance, a  (b  c) is not under NNF but is equivalent to the NNF formula
a  (b  c).
A formula is positive if it contains no occurrence of the negation symbol. For instance,
a  (b  c) and a  (a  b) are not positive, whereas a  (b  c) and (a  c)  (a  b) are.
> and  are considered positive as well.
Let   LV . V ar()  V is the set of propositional variables appearing in . For
instance, V ar((a  c)  (a  b)) = {a, b, c} and V ar(>) = .
V
Lastly, if S = {1 , . . . , n } is a finite
W set of formulae then S = 1  . . .  n is the
conjunction of all formulae of S, and S = 1  . . .  n is the disjunction of all formulae
of S.
2.3 Computational Complexity
In this paper we will refer to some complexity classes located in the polynomial hierarchy.
BH2 (also referred to as DP) is the class of all languages of the form L1  L2 where L1 is in
NP and L2 in coNP. p2 = PNP is the class of all languages recognizable by a deterministic
Turing machine working in polynomial time using NP oracles. Likewise, p2 = NPNP . p2 =
5. Note that connectives  and  are not allowed; this is important for the definition of positive formulae
(to come).

529

fiBouveret & Lang

p2 [O(log n)] is the subclass of p2 of problems that only need a logarithmic number of
oracles. See for instance the book by Papadimitriou (1994) for further details.

3. Fair Division Problems with Dichotomous Preferences: Logical
Representation
We start by considering in full detail the case where preferences are dichotomous.
Definition 5 Ri is dichotomous if and only if there exists a subset Goodi of 2X such that
for all A, B  X, A i B if and only if A  Goodi or B 
6 Goodi . R = hR1 , . . . , RN i is
dichotomous if and only if every Ri is dichotomous.
There is an obvious way of representing dichotomous preferences compactly, namely by
a propositional formula i (for each agent i) of the language LX (a propositional symbol
vx for each good x) such that M od() = Goodi . Formally:
Definition 6 Let Ri be a dichotomous preference on 2X , with Goodi its associated subset
of 2X , and i a propositional formula on the propositional language LX . We say that i
represents Ri if and only if M od(i ) = Goodi .
Clearly,
 preference Ri there is a formula i representing Ri : i =
Vfor any dichotomous
V
v

v
x . Furthermore, this formula is unique up to logical equivAGoodi
xA x
x6A
alence.
W

Example 1 X = {a, b, c} and Goodi = {{a, b}, {b, c}}. Note that Ri is not monotonic.
Then i = (a  b  c)  (a  b  c) represents Ri . So does 0i = b  ((a  c)  (a  c)),
which is logically equivalent to .
An easy but yet useful result:
Proposition 1 Let Ri be a dichotomous preference on 2X . The following statements are
equivalent:
1. Ri is monotonic;
2. Goodi is upward closed, that is, A  Goodi and B  A imply B  Goodi .
3. Ri is representable by a positive propositional formula.
Proof (1)  (2). Suppose Ri is monotonic, and let A  Goodi and B  A. Then we must have
B i A, and therefore B  Goodi (since A  Goodi ).
(2)  (3). Suppose that Goodi is upward closed,
and consider
`V the set
 min (Goodi ) of all inclusionW
minimal sets in Goodi . Then the formula i = Amin (Goodi )
xA vx represents Ri for the following
reasons. For each B  Goodi , there is a set A  min (Goodi ) such that A  B. Thus the corresponding
conjunction in i is satisfied, then i is satisfied. Conversely, for each set B 6 Goodi , there is no
A  min (Goodi ) such that A  B. Therefore, none of the terms in i is satisfied, then i is not
satisfied. Moreover, i is clearly a positive propositional formula.
(3)  (1). Suppose that Ri is representable by a positive propositional formula i , and let A and
B be two sets of items such that A  B. If A 6 Goodi then clearly B i A. If A  Goodi , then
A  M od(i ). Since i is positive, then B  M od(i ) also. Therefore B  Goodi , and finally B i A.


530

fiEfficiency and Envy-freeness in Fair Division of Indivisible Goods

From now on, we assume that allocation problems P are represented in propositional
form, namely, instead of I, X and R we only specify h1 , . . . , N i. I and X are obviously
determined from h1 , . . . , N i. In the following, we will also write the propositional variables
corresponding to the items x instead of vx for short, since it is often unambiguous.
Let P = h1 , . . . , N i be an allocation problem with dichotomous preferences; then for
each i  N , we rewrite i into i obtained from i by replacing every variable x by the
new symbol xi . For instance, if 1 = a  (b  c) and 2 = a  d then 1 = a1  (b1  c1 ) and
2 = a2  d2 .
Example 2 Consider the following allocation problem: 1 = a  (b  c), 2 = a and
3 = ab (therefore I = {1, 2, 3} and X = {a, b, c}). All the formulae are positive, therefore
preferences are monotonic. For instance, Good1 is the set composed of all supersets of {a}
and of all supersets of {b, c}. Then we have
1 = a1  (b1  c1 );
2 = a2 ;
3 = a3  b3
For all i  N , let Xi = {xi , x  X}. An allocation for a standard allocation problem
P corresponds to a model of V = X1  . . .  XN satisfying at most one xi for each x  X.
In other terms, there is a bijective mapping
the set of possible allocations and the
V between
V
models of the following formula: P = xX i6=j (xi  xj ).
V
If allocation are required to be complete, then P is replaced by C
P = P  xX (x1 
. . .  xn ). The rest is unchanged.
Let V = {xi | i = 1, . . . , N, x  X}. Any interpretation M of M od(P ) is such that
it is never the case that xi and xj are simultaneously true for i 6= j, therefore we can map
M  M od(P ) to an allocation simply defined by (i) = {x | M |= xi }. This mapping is
obviously bijective; we will write F () the model of M od(P ) corresponding to allocation
, and of course F 1 (M ) the allocation corresponding to an interpretation M of M od(P ).
Example 2 (continued) For the allocation problem of Example 2, we have:
P = (a1  a2 )  (a1  a3 )  (a2  a3 )
 (b1  b2 )  (b1  b3 )  (b2  b3 )
 (c1  c2 )  (c1  c3 )  (c2  c3 )
The interpretation M such that M only sets a1 , b3 , and c1 to true is clearly a model
of P . It corresponds to the allocation F 1 (M ) = , with (1) = {a, c}, (2) =  and
(3) = {c}.
3.1 Envy-freeness
We now show how the search for an envy-free allocation can be mapped to a model finding
problem. Let j|i be the formula obtained from i by substituting every symbol xi in i
by xj : for instance, if 1 = a1  (b1  c1 ) then 2|1 = a2  (b2  c2 ). Notice that obviously,
we have i|i = i .
We first give the following lemma, which is easy but yet useful:
531

fiBouveret & Lang

Lemma 1 For all i, j, (j)  Goodi if and only if F () |= j|i .
In particular, when i = j we have (i)  Goodi if and only if F () |= i .
Proof By definition of F , (j)  Goodi if and only if {x | F () |= xj }  Goodi , that is, {x | F () |=
xj } |= i . The latter relation is equivalent to {xi | F () |= xj } |= i , and finally to {xj | F () |=
xj } |= j|i by definition of j|i , from which we can deduce the result.


Using this lemma, we can now map the envy-freeness property to the satisfiability of a
logical formula:
Proposition 2 Let P = h1 , . . . , N i be an allocation problem with dichotomous preferences under propositional form, and the formulae j|i and mapping F as defined above.
Let



^
^
i    
P =
j|i
i=1,...,N

j6=i

Then  is envy-free if and only if F () |= P .
Proof Let  be an allocation.  is not envy-free if and only if there is a pair (i, j), with i 6= j such
that (j) i (i), that is (j)  Goodi and (i) 6 Goodi , which is in turn equivalent to F () |= j|i

and F () 6|= i by lemma 1. Therefore  is envy-free if and only if F () |= P .

The intuitive meaning of this result is that an allocation  is envy-free if and only if,
for every agent i, either i is satisfied by her share (i), or she is not but she envies no one,
that is, for every j, she would not be satisfied by js share either.
The search for envy-free allocations can thus be reduced to a model finding problem:
{F 1 (M ) | M |= P  P } is the set of envy-free allocations for P. Note that, importantly,
P  P has a polynomial size (precisely, quadratic) in the size of the input data.
The problem of existence of an envy-free allocation without any other property is not
very interesting, because such an allocation always exists : it suffices to consider the allocation that gives an empty share to everyone. However,
 the problem of deciding whether there exists an envy-free allocation satisfying some
property expressible by a polynomial size formula (e.g. completeness) can be reduced
to a satisfiability problem;
 the problem of finding (resp. counting) all envy-free allocations comes down to the
problem of finding (resp. counting) all models of P  P .
Example 2 (continued) For the allocation problem of Example 2, we have:
P =

((a1  (b1  c1 ))  ((a2  (b2  c2 ))  (a3  (b3  c3 ))))
(a2  (a1  a3 ))
((a3  b3 )  ((a1  b1 )  (a2  b2 )))

M od(P  P ) = {{c1 }, {c1 , b3 }, {c2 , b3 }, {c2 }, {b3 }, {c3 }, }.
There are therefore 7 envy-free allocations, namely (c, , ), (c, , b), (, c, b), (, c, ),
(, , b), (, , c) and (, , ). Note that none of them is complete.
532

fiEfficiency and Envy-freeness in Fair Division of Indivisible Goods

3.2 Efficient Allocations
Similarly as for the envy-freeness property, Pareto-efficiency can be expressed as a logical
property. The logical expression of this property requires the definition of a maximal consistent subset of a set of formulae.
Definition 7 Let  = {1 , . . . , m } be a set of formulae
and  be a formula. S   is a
V
maximal -consistent subset of Vif and only if (a) S   is consistent and (b) there is
no S 0 such that S  S 0   and S 0   is consistent. Let M axCons(, ) be the set of
all maximal -consistent subsets of . Moreover, we will write M axCons() the set of all
maximal-consistent subsets of , that is, the set M axCons(, >).
Proposition 3 Let P = h1 , . . . , N i an allocation problem. Let P = {1 , . . . , N }.
Then  is Pareto-efficient for P if and only if {i | F () |= i } is a maximal P -consistent
subset of P .
Proof Let  be an allocation. Let Sat() be the set of agents satisfied by , that is, by Lemma 1,
of Sat(), and F () |= P by
|= i by definition V
the set {i | F () |= i }. For all i  Sat(),
V F ()

|
i

Sat()}
=

{i | F () |= i }. Therefore
definition
of
F
().
Therefore
F
()
|=

{
P 
P 
i
V 

{i | F () |= i }  P is consistent.
By definition,  is Pareto-dominated if and only if there is an allocation  0 such that Sat( 0 ) )
Sat(). Therefore, if  is Pareto-dominated there is a consistent subset S  P (corresponding
to
V
{i | F ( 0 ) |= i }) such that {i | F () |= i }  S. Moreover, since  0 is an allocation, S  P is
consistent.
V
Conversely,
{i | F () |= i }  S and S  P is consistent, and M be
V let S  P be such that
0
1
a model of S  P . By definition,  = F (M ) is a well-defined allocation, and  0 (i)  Goodi for all
i  S. Since {i | F () |= i }  S, we have Sat( 0 ) ) Sat(). Therefore  is Pareto-dominated. 

This simple result suggests that efficient allocations can be computed from the logical
expression P of the problem, namely, by computing the maximal
P -consistent subsets of
V
P ; call them {S1 , . . . , Sq }. Then for each Si , let Mi = M od( Si P ) and let M = qi=1 Mi .
Then F 1 (M ) is the set of all efficient allocations for P . Note that there are in general
exponentially many maximal P -consistent subsets of P (and therefore exponentially many
efficient allocations). This can be tempered by (a) there are many practical cases where
the number of maximal consistent subsets is small; (b) it is generally not asked to look for
all efficient allocations; if we look for just one, then this can be done by computing one
maximal P -consistent subset of P .
Example 2 (continued) The maximal
P -consistent subsets of P are S1 =V{1 , 2 },
V




S2 = {1 , 3 } and S3 = {2 , 3 }.
S1 VP has only one model: {b1 , c1 , a2 }.
S 2  P
has two models: {a1 , b3 } and {b1 , c1 , a3 }. S3  P has one model: {a2 , b3 }. Therefore the
four efficient allocations for P are (bc, a, ), (a, , b), (bc, , a) and (, a, b). None of them
is envy-free.
3.3 Efficient and Envy-free Allocations
We are now in position of putting things together. Since envy-free allocations correspond to
the models of P and efficient allocations to the models of maximal P -consistent subsets
of P , the existence of an efficient and envy-free (EEF) allocation is equivalent to
V the
following condition: there exists a maximal P -consistent subset S of P such that S 
533

fiBouveret & Lang

P P is consistent. In this case, the models of the latter formula are the EEF allocations.
Interestingly, this is an instance of a well-known problem in nonmonotonic reasoning:
Definition 8 A supernormal default theory6 is a pair D = h, i with  = {1 , . . . , m },
where 1 , . . . , m and  are propositional formulae. A propositional formula  is a skeptical
consequence of D, denoted by D | , if and only if for all S  M axCons(, ) we have
V
S   |= .
Proposition 4 Let P = h1 , . . . , N i be a fair division problem. Let DP = hP , P i.
Then there exists an efficient and envy-free allocation for P if and only if DP 6| P .
Proof Let P = h1 , . . . , N i be a fair division problem. Let  be a Pareto-efficient and envy-free
V
allocation, and S = {i | F () |= i }.
2. We also have F () |= S
V Then F () |= P by Proposition
V
by definition
of S, and then F () |= S  P , which proves that S  P is consistent, or, in other
V
terms, S 6|= P . Moreover,
V S is a maximal P -consistent subset of P by Proposition 3. Thus
S  M axCons(P , P ), and S  P 6|= P , which implies hP , P i 6| P by definition of 6| .

there is a set S  M axCons(P , P ) such that
V Conversely, suppose that hP , P i 6| P . Then
S  P V
P has a model M . By Proposition 3, F 1 (M ) is a Pareto-efficient allocation (since M is
a model of S  P ), and by Proposition 2, F 1 (M ) is envy-free (since M is a model of P ).


This somewhat unexpected connection to nonmonotonic reasoning
has several impliV
cations. First, EEF allocations correspond to the models of
S  P  P for S 
M axCons(P , P ); however, M axCons(P , P ) may be exponentially large, which argues for avoiding to start computing efficient allocations and then filtering out those that
are not envy-free, but rather compute EEF allocations in a single step, using default reasoning algorithms. Thus, fair division may benefit from computational work in default logic
and connected domains such as belief revision and answer set programming (Baral, 2003;
Gebser, Liu, Namasivayam, Neumann, Schaub, & Truszczynski, 2007). Moreover, alternative criteria for selecting extensions in default reasoning (such as cardinality, weights or
priorities) correspond to alternative efficiency criteria in allocation problems.

4. Allocation Problems with Dichotomous Preferences: Complexity
We will study in this section the complexity of the problem of the existence of an EEF
allocation and some of its restrictions, in the case where preferences are dichotomous, for
several notions of efficiency.
4.1 Complexity of the General Problem
It is known that skeptical inference is p2 -complete (Gottlob, 1992); now, after Proposition
4, the problem of the existence of an EEF allocation can be reduced to the complement of
a skeptical inference problem, which immediately tells that it is in p2 . Less obviously, we
now show that it is complete for this class, even if preferences are required to be monotonic.
To prove hardness, we will use the following restriction of the skeptical inference problem:
Problem 1: restricted skeptical inference (rsi)
INSTANCE: A set of propositional formulae  = {1 , . . . , n }.
QUESTION: do all maximal-consistent subsets of  contain 1 ?
6. Supernormal defaults are also called normal defaults without prerequisites (e.g. Reiter, 1980).

534

fiEfficiency and Envy-freeness in Fair Division of Indivisible Goods

Proposition 5 The problem rsi is p2 -complete.
Proof Membership to p2 comes easily from the fact that the problem rsi is a restriction of the
skeptical inference problem, where the formula  to infer is simply 1 . Hardness comes from the fact
that from any instance h, , i of the skeptical inference problem, we have h, i |  if and only if
h{}  , i | , if and only if h{  }  {1  , . . . , n  }, >i | , if and only if all maximal
-consistent subsets of {, 1 , . . . , n } contain , which is an instance of rsi.


Proposition 6 The problem eef existence of determining whether there exists an efficient and envy-free allocation for a given problem P with monotonic, dichotomous preferences under logical form is p2 -complete.
We show hardness by the following reduction from rsi (the complement problem of rsi,
that is, is there one maximal-consistent subset of  that does not contain 1 ?) to eef
existence. Given any finite set  of propositional formulae, let V = V ar() be the set
of propositional symbols appearing in , and let P() = hI, X, P() i be the following
instance of eef existence:
1. I = {1, 2, ..., n + 3};
2. X = {v i |v  V , i  1...n}  {v i |v  V , i  1...n}  {xi |i  1...n + 1}  {y};
3. for each i = 1, . . . , n, let i be obtained from i by the following sequence of operations:
(i) put i into NNF form (let i0 be the result); (ii) for every v  V , replace, in i0 ,
each (positive) occurrence of v by v i and each occurrence of v by v i ; let i be the
formula obtained. Then:
 for i = 1, . . . , n, i = i  xi ,
V



Vn
Vn
i 
i
n+1  y,
 n+1 =
v
v

x
vV
i=1
i=1
 n+2 = y,
 n+3 = 1 .
We will now prove Proposition 6 using several lemmas.
Lemma 2 An allocation  for P is said to be regular if and only if for all i  n + 3,
(i)  (i), where
S
S
 for all i  n, (i) = vV {v i , v i } {xi };
S
S
 (n + 1) = vV ,i=1,...,n {v i , v i } {xn+1 , y};
 (n + 2) = {y}.
 (n + 3) = (1).
Given some allocation , let now R be defined by R (i) = (i)  (i). Then
1. R is regular;
2.  is efficient if and only if R is efficient;
535

fiBouveret & Lang

3. if  is envy-free then R is envy-free.
Proof (1) is obvious. For all i, the goods outside (i) do not have any influence on the satisfaction
of i (since they do not appear in i ), therefore R (i) i (i), from which (2) follows. The formulae
i being positive, the preference relations i are monotonic, therefore (j) i R (j) holds for all i, j.
Now, if  is envy-free then for all i, j we have (i) i (j), therefore R (i) i (i) i (j) i R (j)
and therefore R is envy-free, from which (3) follows.


Lemma 3 If  is regular then
1. 1 can only envy n + 3;
2. n + 3 can only envy 1;
3. 2, . . . , n envy no one;
4. n + 1 can only envy n + 2;
5. n + 2 can only envy n + 1;
Proof First, note that for any i, j 6= i, i envies j if and only if (i) |= i and (j) |= i .
1. Let i = 1 and j  {2, . . . , n, n + 2}. If 1 envies j, then x1  (j).  being regular, x1 6 (j),
therefore i cannot envy j.
2. Since n+3 = 1 , the same holds for agent n + 3.
3. Let i  {2, . . . , n} and j 6= i. If i envies j then (j) |= i  xi , which is impossible because
xi 6 (j), due to the regularity of .
4. Assume n + 1 envies j for j  {1, . . . , n,n + 3}. Then (j) |= n+1 . Since
(j) |= y is impossible
`V n
 `V n

V
n+1
i
i
(because  is regular), we have (j) |=

x
, thus (j) |= xn+1
v

v
vV
i=1
i=1
which is impossible as well, since  is regular.
5. Let i = n + 2 and j  {1, . . . , n, n + 3}. If i envies j then (j) |= y, which is impossible because
 is regular.


Lemma 4 Let  be a regular allocation satisfying n + 1 and n + 2 and leaving 1 and n + 3
unsatisfied. Let M () be the interpretation on V obtained from  by: for all v  V ,
M () |= v (i.e., v  M ()) if n + 1 receives v 1 , . . . , v n , and M () |= v otherwise, i.e., if
n + 1 receives v 1 , . . . , v n . Then  is efficient and envy-free only if M () 6|= 1 .
Proof Let  be a regular allocation satisfying n + 1 and n + 2. Since  satisfies n + 2, y  (n + 2).
Now,  satisfies n + 1 without giving her y, therefore, for any v, n + 1 receives either all the v i s or all
the v i s. This shows that our definition of M () is well-founded.
Now assume that  is efficient and envy-free, and suppose that M () |= 1 . One S
of the agents 1
1
and n +
3
is
satisfiable
without
spoiling
any
agent
j

6
{1,
n
+
3},
by
giving
her
{x
}

(v i | M () |=
S i
i
i
v )  (v | M () |= v ). Then since  is efficient it must satisfy at least one of 1 and n + 3. It cannot
satisfy them simultaneously (because of x1 ). Thus only one among 1 and n + 3 is satisfied by , the
other one envying her. It contradicts the envy-freeness of , thus proving that M () 6|= 1 .


Lemma 5 For each interpretation M over V , let us define M : I  2X by:
 for each i  1, . . . , n, M (i) = {v i | M |= v}  {v i | M |= v}  {xi };
 M (n + 1) = {xn+1 }  {v i | M |= v, i = 1, . . . , n}  {v i | M |= v, i = 1, . . . , n};
 M (n + 2) = {y}
536

fiEfficiency and Envy-freeness in Fair Division of Indivisible Goods

 M (n + 3) = .
Then:
1. M is a well-defined and regular allocation satisfying n + 1 and n + 2;
2. M (M ) = M (M (M ) is obtained from M as in Lemma 4).
3. for any i  1, . . . , n, M satisfies i if and only if M |= i .
4. M is efficient if and only if M satisfies a maximal consistent subset of .
Proof
1. One can easily check that M does not give the same good to more than one individual,
and that M can only give to each agent i a set of items that are in (i). Therefore it is a
well-defined and regular allocation. This allocation obviously satisfies n + 1 and n + 2.
2. If M |= v then M (n + 1) contains {v i | i = 1, . . . , n} and therefore M (M ) |= v. The case
M |= v is similar.
3. Let i  1, . . . , n. Since M gives xi to i, M satisfies i if and only if F (M (i)) |= i , which is
equivalent to M |= i .
4. From point 3, {i | M satisfies i} = {i | M |= i }  {n + 1, n + 2} (obviously, n + 3 is not
satisfied). Now, since preferences are dichotomous, an allocation  is efficient if and only if the
set of individuals it satisfies is maximal with respect to inclusion. Therefore, M is efficient if
and only if M satisfies a maximal consistent subset of .


Lemma 6 Let  be a regular and efficient allocation satisfying n + 1 and n + 2. Then M ()
satisfies a maximal consistent subset of .
Proof  is regular and satisfies n+1 and n+2, therefore obviously (n+2) = {y} and by lemma 4 M ()
is well-defined. We will now consider the allocation M () , defined from  like in the previous lemmas.
We have M () (n + 1) = {xn+1 }  {v i |M () |= v}  {v i |M () |= v} = {xn+1 }  {v i |{v 1 , . . . , v n } 
(n + 1)}  {v i |{v 1 , . . . , v n }  (n + 1)}. Since n + 1 is satisfied by , (n + 1) must contain {xn+1 },
from what we assert that M () (n + 1)  (n + 1).
Let i  {2, . . . , n}. Since  is regular, we have (i)  (i). Since M () is a complete allocation by
definition, and regular by lemma 5, (i)  M () (i)  M () (n + 1). Since M () (n + 1)  (n + 1) we
have (i)  M () (i)  (n + 1), and thus (i)  M () (i)  (n + 1).  being an allocation, we have
of course (i)  (n + 1) = , from what follows (i)  M () (i).
 being regular, we have (1)(n+3)  (1)(n+3). Since (1) = (n+3), the latter inclusion
comes down to (1)  (n + 3)  (1). Now, we have (1)  M () (1)  M () (n + 1)  M () (n + 3) for
similar reasons as for i  {2, . . . , n}, which, together with M () (n+1)  (n+1) and M () (n+3) = ,
comes down to (1)  M () (1)  (n + 1), and, with (1)  (n + 1) = , we deduce the inclusion
(1)  (n + 3)  M () (1).
We can now prove that M () is efficient. Since preferences are monotonic, all individuals but n + 3
satisfied by  are satisfied by  0 as well (since i 6= n + 3, (i)  M () (i)).
 If n + 3 was not satisfied by , then immediately we deduce that M () is efficient.
 If n + 3 was satisfied by , suppose that M () is not efficient. In that case, there is an allocation
 0 such that i, M () satisfies i implies  0 satisfies i and there is a particular j 6= 1 such that  0
satisfies j and M () does not satisfy j. Clearly,  0 satisfies 1 (since M () does), thus j 6= n + 3
(satisfying simultaneously 1 and n + 3 is impossible). Consider the allocation  00 deduced from
 0 just from swapping the shares of 1 and n + 3. We have, for all i  {2, . . . , n + 2},  satisfies i
implies M () satisfies i implies  0 satisfies i implies  00 satisfies i. We also have  satisfies n + 3
and does not satisfy 1, which is the same for  00 . Thus  satisfies i implies  00 satisfies i for all
i. Moreover,  00 satisfies j  {2, . . . , n + 2} (the same j as above) while M () does not, and
therefore neither does . It proves that  is Pareto-dominated, which is contradictory with the
hypotheses.

537

fiBouveret & Lang

Therefore M () is efficient, from which we conclude, together with lemma 5 (point 4), that M ()
satisfies a maximal consistent subset of .


Lemma 7 Any envy-free and efficient allocation for P() satisfies n + 1 and n + 2, and
leaves 1 and n + 3 unsatisfied.
Proof Suppose that  does not satisfy n + 1; then y 6 (n + 1); now, if y  (n + 2) then n + 1 envies
n + 2; if y 6 (n + 2) then  is not efficient because giving y to n + 2 would satisfy n + 2 and thus lead
to a better allocation than .
Now, suppose  does not satisfy n + 2, that is, y 6 (n + 2); if y  (n + 1) then n + 2 envies n + 1;
if y 6 (n + 1) then again,  is not efficient because giving y to n + 2 would satisfy n + 2 and thus lead
to a better allocation than .
Concerning agents 1 and n + 3, one can notice that since they have identical preferences, any
envy-free allocation must either satisfy them both, or leave them both unsatisfied. Since they cannot
be simultaneously satisfied (because of x1 ), any envy-free allocation leaves them both unsatisfied. 

Lemma 8 If there exists an EEF allocation, then there exists a maximal consistent subset
of  that does not contain 1 .
Proof Let  be an efficient and envy-free allocation. By Lemma 2, R is regular, efficient and envyfree. By Lemma 7, R satisfies n + 1 and n + 2 and leaves 1 and n + 3 unsatisfied. Then by Lemma 6,
M (R ) satisfies a maximal consistent subset of , and by Lemma 4, M (R ) 6|= 1 . Therefore {i 
 | M (R ) |= i } is a maximal consistent subset of  and does not contain 1 .


Lemma 9 If there exists a maximal consistent subset of  that does not contain 1 then
there exists an EEF allocation.
Proof Assume that
V there exists a maximal consistent subset S of  that does not contain 1 , and let
M be a model of S. By point 4 of Lemma 5, M is efficient.
By point 1 of Lemma 5, M is regular; then by Lemma 3, M is envy-free if and only if (i) 1 does
not envy n + 3, (ii) n + 3 does not envy 1 (iii) n + 1 does not envy n + 2 and (iv) n + 2 does not envy
n + 1. By definition of M , M does not satisfy n + 3, hence (i) holds. By point 3 of Lemma 5, M 6|= 1
implies that M does not satisfy 1, therefore (ii) holds as well. And finally, by point 1 of lemma 4, n + 1
and n + 2 are satisfied by M , thus (iii) and (iv) also hold. Therefore M is envy-free.


We are now in position of putting things together and proving Proposition 6:
Proof (Proposition 6) From Lemmas 8 and 9, the existence of a maximal consistent subset of  that
does not contain 1 and the existence of an efficient and envy-free allocation for P() are equivalent.
Clearly, P() is computed in polynomial time. Therefore, P is a polynomial reduction from rsi to eef
existence, which shows that the latter problem is p2 -hard, and therefore p2 -complete.


As a corollary, this p2 -completeness result holds for general (not necessarily monotonic)
dichotomous preferences:
Corollary 1 eef existence for general, dichotomous preference under logical form is p2 complete.
4.2 Restrictions on the Language
As a consequence of this high complexity, it is worth studying restrictions and variants of
the latter problem for which complexity may fall down. We will investigate three kind of
intuitive restrictions of the eef existence problem, defined by:
 fixing the number of agents, and especially, restricting the problem to the case where
there are only 2 agents;
538

fiEfficiency and Envy-freeness in Fair Division of Indivisible Goods

 forcing the agents to have identical preferences;
 restricting the syntax of the agents goals, by limiting their expression to some subclasses of propositional formulae (e.g. clauses, cubes, . . . ).
Contrary to the general eef existence problem, the complexity of these restrictions
may be sensitive to whether the preferences are monotonic or not.
4.2.1 Identical Preferences
We start by considering identical dichotomous preference profiles, that is, all agents have
the same preference, i.e. the same formula .
Proposition 7 eef existence with N identical dichotomous, monotonic preferences is
NP-complete. This result holds even for a fixed number of agents N  2.
Proof If preferences are identical, an envy-free allocation satisfies either all agents or none; now, if
preferences are monotonic, it is always possible to satisfy at least one agent (by giving her all the items).
Therefore, an allocation is EEF if and only if it satisfies all the agents. Clearly, it can be checked in
polynomial time that a given allocation satisfies all agents, hence membership to NP.
Hardness comes from an simple reduction from set splitting:
Problem 2: set splitting
INSTANCE: A collection C = {X1 , . . . , Xn } of subsets of a finite set S.
QUESTION: Is there a partition hS1 , S2 i of S such that no subset in C is entirely contained in either
S1 or S2 ?
Given an instance hC, Si of set splitting, let P(C, S) be the following eef existence instance:
Agents:
Objects:
Preferences:

2 agents,
one object x(a) per element a  S,
V
W
V
W
1 = 2 = Xi C aXi x(a) (and as usual k = Xi C aXi xk (a)): each agent
wants at least one object from each set.

It is easy to see that if there is a set splitting hS1 , S2 i of hC, Si, it is possible to find an allocation
that satisfy the two agents, by giving them respectively x(S1 ) and x(S2 ). Conversely, suppose that
there is an efficient and envy-free allocation , then this allocation must satisfy the two agents. Let
1
hS1 , S2 i = hx1 ((1)),
Wx ((2))i. Suppose that there is an Xi  C such that Xi  S1 or Xi  S2 (say
e.g Xi  S1 ). Then aXi x2 (a) is false, thus making 2 false, which is contradictory with the initial
hypothesis. Therefore hS1 , S2 i is a set splitting of hC, Si.
This is clearly a polynomial-time reduction, hence NP-hardness of eef existence with 2 agents
having identical dichotomous and monotonic preferences.


Unlike Proposition 6, Proposition 7 is sensitive to whether preferences are required to
be monotonic or not.
Proposition 8 eef existence with N identical dichotomous preferences is coBH2 -complete.
This result holds even for a fixed number of agents N  2.
Proof If preferences are identical, an envy-free allocation satisfies either all agents or none. Let  be
the formula representing one agents preferences (of course  is identical for all the agents). If  is
satisfiable then it is possible to satisfy at least one agent. In that case, an allocation  is EEF if and
only if  satisfies all the agents. If  is not satisfiable, then every allocation is EEF. Therefore, there
exists an EEF allocation if and only if   1 . . . . . . N is satisfiable or  is unsatisfiable. This shows
membership to coBH2 .
Hardness comes from a simple reduction from sat-or-unsat. Let I = h, i be a pair of propositional formulae, assumed w.l.o.g. to have no variables in common; we map I to the following allocation
problem:

539

fiBouveret & Lang

Agents:
Objects:
Preferences:

2 agents;
2 objects v and v 0 per propositional variable v appearing in , one object w per propositional variable w appearing in , and one object y;
1 = 2 =   0  (y  ) where 0 denotes the formula  where each variable v has
been replaced by v 0 .

1. Suppose that  is not satisfiable, but  is (corresponding to a negative instance of sat-orunsat). Then it is possible to satisfy at least one agent by giving her y and the objects v
corresponding to the variables assigned to true in the model of . However it is not possible to
satisfy simultaneously the second agent because  is unsatisfiable (and by the way so is 0 ), and
the first agent has already taken y. Therefore in that case there is no EEF allocation.
2. Suppose now that  is satisfiable or  is unsatisfiable (corresponding to a positive instance of
sat-or-unsat). There are two cases:
  is satisfiable. In that case, no matter whether  is satisfiable or not, it is possible to satisfy
the two agents by satisfying simultaneously  for the first one and 0 for the second one.
Consequently there is an EEF allocation.
  and  are both unsatisfiable (recall that the case  unsatisfiable and  satisfiable is covered
by point 1). In that case it is clearly impossible to satisfy any agent. In that case, the empty
allocation is efficient and envy-free.
Therefore there is an EEF allocation if and only if  is satisfiable or  is unsatisfiable, which proves
the proposition.


4.2.2 Two Agents
Note that for the two previous results, the hardness result holds for any fixed number of
agents ( 2). Things are different with Proposition 6, for which hardness does not hold
when N is fixed. Namely, we have the following results.
Proposition 9 eef existence for two agents with monotonic dichotomous preferences is
NP-complete.
Proof Hardness is a corollary of Proposition 7. Membership is obtained as follows. Let h1 , 2 i be the
preference profile, where 1 , 2 are both positive. The formulae ,  and the formulae i are defined
as earlier (see section 3), as well as F () for all allocation . Now  is efficient if and only if either (a)
it satisfies both agents, or (b) it satisfies only one agent, and   1  2 is unsatisfiable, or (c) it is
impossible to satisfy even one agent, i.e., both 1 and 2 are unsatisfiable. (c) is impossible because
1 , 2 are positive. Now,  is envy-free if and only if M |= . Therefore,  is EEF if and only if (a)
M |=   1  2 or (b) M |=   (1  2 )  . Thus, there exists an EEF allocation if and only if
(  1  2 )  (  (1  2 )  ) is satisfiable, hence membership to NP.


Proposition 10 eef existence for 2 agents with dichotomous preferences is coBH2 complete.
Proof Membership comes from the following reduction to sat-or-unsat. Consider an instance P of
the EEF problem with 2 agents having respectively the preferences 1 and 2 . We translate this instance
to an instance h,  0 i of sat-or-unsat defined as follows ( is defined as usual):  = (  1  2 ) 
(1  2 )  (1  2 ) and  0 = (1  2 ). We will prove that  is satisfiable or  0 is unsatisfiable if
and only if there is an EEF allocation for P.
1. Suppose that  is not satisfiable, but  0 is. Since  is not satisfiable, no valid allocation can
satisfy both agents (because   1  2 is not satisfiable). Since  0 is satisfiable, it is possible
to satisfy at least one agent (because 1  2 is satisfiable). We can then deduce that every
efficient allocation satisfies exactly one agent. Since (1  2 )  (1  2 ) is not satisfiable,
M od(1 ) = M od(2 ) (in other words, 1 and 2 are logically equivalent). Let  be an allocation

540

fiEfficiency and Envy-freeness in Fair Division of Indivisible Goods

satisfying the agent 1 (the case is similar with agent 2), F () |= 1 , and thus F () |= 2|1 .
Since F () 6|= 2 (because it is impossible to satisfy both agents), F () 6|=  and thus  is not
envy-free. Hence every efficient allocation raises envy: there is no EEF allocation.
2. Suppose now that  is satisfiable or  0 is not satisfiable.
  0 is not satisfiable. This case is easy because it is not possible to satisfy even one agent.
Therefore every allocation is efficient and envy free.
  and  are both satisfiable. There are two cases:
   1  2 is satisfiable. In this case, there is an allocation, corresponding to the model
of   1  2 , that satisfies both agents. This allocation is clearly EEF.
 1 2 is satisfiable but 1 2 is not satisfiable (the case with 2 1 is similar).
In that case it is not possible to satisfy both agents. However, since  is satisfiable, it
is possible to satisfy at least one, and, like in point 1, every efficient allocation satisfies
exactly one agent. Since 1  2 is satisfiable, there is a model of 1 which is not
a model of 2 . The allocation corresponding to this model is such that the agent 1 is
satisfied and not the agent 2, but the latter agent cannot envy the first one.
It finally proves the correctness of the reduction, which is clearly polynomial.
Hardness comes directly from Proposition 8.



4.2.3 Restriction on the Propositional Language
In the previous results, we made no specific assumptions on the formulae expressing the
agents preferences, except (sometimes) monotonicity. However, if we restrict the set of possible propositional formulae, it can decrease the complexity of the eef existence problem.
We investigate first two natural restrictions on the propositional language: in the first case
we restrict the formulae to the set of clauses (disjunctions of literals), in the second case
the preferences are expressed using cubes only (conjunctions of literals). These restrictions
match two different kinds of real-world problems.
 The case, where the agents preferences are represented by clauses, corresponds to the
kind of problems where each agent only wants one single object of a certain class. One
can consider for example a set of patients all waiting for a kidney transplant. Each
patient only needs one kidney, but several ones may be compatible.
 The case, where the agents preferences are represented by cubes, corresponds to the
kind of problems where each agent needs a single bundle of objects. It is typical from
the kind of problems where the agents build the object they want from a set of basic
material (or virtual) components: the set of objects stand for the basic components,
and the cube of one agent stands for the complete device she wants.
Making one of these two assumptions actually decreases the complexity of the eef existence problem: it even renders it tractable in the case of clauses of objects.
Proposition 11 eef existence for agents having dichotomous preferences restricted to
clauses of objects can be solved in polynomial time.
Proof We will first make two additional assumptions and prove that the complexity of the problem
does not decrease under these two assumptions. (1) We suppose first that the agents preferences are
monotonic. If one agent has non-monotonic preferences, it means that there is one negative literal in
her clause. Giving her an empty share will satisfy her without spoiling another agent, thus she can be
safely removed from the problem. (2) We also suppose that each agent wants at least one object. If one

541

fiBouveret & Lang

agent has an empty clause as her goal, it means that no matter what she gets, she cannot be satisfied.
Thus she can also be safely removed from the problem. In the rest of the proof, we will only consider
problems that verify the assumptions (1) and (2).
The proof is based on the following result: when the agents preferences are disjunction of objects
under assumptions (1) and (2), an allocation is Pareto-efficient and envy-free if and only if it satisfies
every agent. The implication  is immediate. To prove the implication  we need to notice that an
agent is satisfied if and only if she receives at least one object of her clause. Now take an allocation 
such that there is an agent i which is not satisfied by . Then either it is possible to satisfy her without
spoiling another agent, because one object of her clause is given to an agent that does not want it, or
that is already satisfied by another agent: in that case  is not Pareto-efficient. Or it is not possible to
do so because all the objects of agent is disjunction have been given to some other agents that truly
want them: in that case, the agent i envies these other agents, and  is thus not envy-free.
Therefore, finding a Pareto-efficient and envy-free allocation here comes down to finding an allocation that gives to each agent one object that she wants. Thus, an instance P of the eef existence
problem can be reduced to a maximal matching problem, in a bipartite graph GP where there is one
node per agent on one side, and one node per object on the other side, and where there is an edge
between an agent-node i and an object-node o if and only if o is in the agent is clause. It can be easily
checked that there is a Pareto-efficient and envy-free allocation if and only if there is a matching of size
n in GP . The latter problem can be solved in time O(nm) (Ford & Fulkerson, 1962), where m is the
size of the biggest disjunction.


Now we investigate the case where the agents preferences are cubes of objects. Not
very surprisingly, this case is harder that the previous one, but remains in NP.
Proposition 12 eef existence for agents having dichotomous preferences restricted to
cubes of objects is NP-complete. The same result holds if we require the preferences to be
monotonic.
Proof The proof is organized as follows. We will first prove membership to NP without any assumption
on the monotonicity of the preferences. Then we will show hardness in the monotonic case.
We first introduce some additional notations. In the following, we will denote by Obj + (i) (resp.

Obj (i)) the set of objects appearing as positive (resp. negative) literals in agent is cube. Let 
be an allocation.  will be said minimally regular if i, either (i) = Obj + (i) or (i) = . For a
given allocation , we will denote by M R its corresponding minimally regular allocation, that is, the
allocation such that for all i, M R (i) =  if Obj + (i) 6 (i), and M R (i) = Obj + (i) if Obj + (i)  (i).
+
We will also write
S Sat() the set of agents satisfied by : Sat(M R ) = {i | M R (i) = Obj (i)}), and
Allocated() = iI (i) (the set of objects allocated to an agent).
We have the following result:
Lemma 10 Let  be an allocation. We have:
 M R is minimally regular;
 Sat()  Sat(M R );
 If  is Pareto-efficient, then M R is also Pareto-efficient.
Proof
 For all agent i, M R (i) =  or M R (i) = Obj + (i) by definition of M R . Therefore
M R is minimally regular.
 Let  be an allocation, and let i be an agent. If i is satisfied by , then Obj + (i)  (i) and
Obj  (i)  (i) = . By definition of M R , we have M R (i) = Obj + (i), and thus we still have
Obj + (i)  (i) and Obj  (i)  (i) = , therefore the agent i is still satisfied by M R . It
proves that Sat()  Sat(M R ).
 Suppose that  is Pareto-efficient, and suppose that there is a  0 that Pareto-dominates
M R . Then we have Sat()  Sat(M R ) ( Sat( 0 ), which contradicts the fact that  is
Pareto-efficient. It proves the third point.


542

fiEfficiency and Envy-freeness in Fair Division of Indivisible Goods

Lemma 11 A minimally regular allocation M R is Pareto-efficient if and only if there is no i such
that (a) i 6 Sat(M R ) and (b) Obj + (i)  Allocated(M R ) = .
Proof Let M R be a minimally regular allocation, and suppose that there is some i such that
i 6 Sat(M R ) and Obj + (i)  X \ Allocated(M R ). Then the allocation  0 such that j 6= i
 0 (j) = M R (j) and  0 (i) = Obj + (i) is well-defined (since Obj + (i) is among the set of unallocated
objects for M R ), and Pareto-dominates M R , since all the agents satisfied by M R are also satisfied
by  0 , and i is now satisfied by  0 whereas she was not by M R .
Conversely, suppose that M R is not Pareto-efficient, and let  0 be a Pareto-efficient alloca0
tion that Pareto-dominates M R . Then by Lemma 10, M
R is Pareto-efficient, and also Pareto0
dominates M R . For all i  Sat(M R ), M R (i) = M R (i) = Obj + (i) since these two allocations are
0
both minimally regular, and each agent satisfied by M R is also satisfied by S
M R . Moreover, there
0
+
is a j 6 Sat(M R ) such that M R (j) = Obj (j). Since Allocated(M R ) = iSat(M R ) Obj + (i)
S
0
+
+
and since M
R (j)  X \
iSat(M R ) Obj (i), we have Obj (j)  X \ Allocated(M R ), which
finally proves the lemma.

The two lemmas provide a procedure to check if a given allocation  is Pareto-efficient: first
compute M R (which can be done in polynomial time). By Lemma 10, Sat()  Sat(M R ). If the
inclusion is strict (that is, Sat() ( Sat(M R )), then obviously  is not Pareto-efficient since M R
Pareto-dominates it. Otherwise, checking if  is Pareto-efficient comes down to checking if M R is
Pareto-efficient, which comes down, according to Lemma 11, to n set inclusion tests. Now, checking if
 is envy-free is still polynomial. Hence the problem is in NP.
We will now prove the hardness of the problem by focusing on monotonic preferences (that is, such
that Obj  (i) =  for all i). For the hardness proof, we will need a few additional lemmas.
Lemma 12 Let  be an allocation and suppose that the agents have monotonic preferences. If  is
envy-free then M R is envy-free.
Proof Let  be an allocation. By Lemma 10, Sat()  Sat(M R ). Conversely, let i  Sat(M R ).
We have M R (i)  (i), which proves that i  Sat(), because we deal with monotonic preferences.
Now suppose that i envies j in M R . Then i is not satisfied by M R , and thus neither satisfied by
. Since M R (j)  (j) and agent is preferences are monotonic, i will still envy j in . Thus if
 is envy-free, so is M R .

An important corollary of this lemma is that when we deal with monotonic cubes, the existence of
a Pareto-efficient and envy-free allocation is equivalent to the existence of a minimally regular Paretoefficient and envy-free allocation. Therefore we can restrict our existence problem to the minimally
regular allocations.
Lemma 13 Let i and j be two different agents (and still suppose that the agents have monotonic
preferences). Then (there exists a minimally regular allocation M R such that i envies j) if and only if
Obj + (i)  Obj + (j).
Proof Let M R be a minimally regular allocation, and suppose that i envies j. Then obviously
i is not satisfied but j is; hence M R (j) = Obj + (j). Since i envies j, we thus have directly
Obj + (i)  Obj + (j).
Conversely, suppose that Obj + (i)  Obj + (j). Then the allocation M R that gives Obj + (j) to
j and nothing to the other agents is clearly minimally regular, and is also obviously such that i
envies j.

Now we introduce the NP-complete problem that we will use to prove NP-hardness (in the monotonic
case):
Problem 3: exact cover by 3-sets (Karp, 1972)
INSTANCE: A set S of size 3q, and a collection C = hS1 , . . . , S|C| i of 3-element subsets of S
QUESTION: Does C contain an exact cover for S, i.e. a sub-collection C 0  C such that every element
of S occur in exactly one member of C 0 ?
Given an instance hS, C = hS1 , . . . , S|C| ii of exact cover by 3-sets (we will assume w.l.o.g. that
the Si are all different), let P(S, C) be the following eef instance:

543

fiBouveret & Lang

Agents:

a set of |C| + 2|S| agents I = I1  I2 , with I1 = {1, . . . , |C|} and I2 = {|C| + 1, . . . , |C| +
2|S|},

Objects:

a set of 2|S| items X = X1  X2 , with X = {x1 , . . . , x|S| } and X 0 = {x01 , . . . , x0|S| },
each pair (xi , x0i ) corresponding to a different element ai of S,
V
for each agent i  I1 , i = aj Si xj , and for each k  {1, . . . , |S|}, |C|+2k1 =
|C|+2k = xk  x0k .

Preferences:

In other words, the first |C| agents preferences correspond to the sets in the collection C, and the last
2|S| agents are gathered by pairs, each member of the same pair having the same preferences as the
other member.
Since all the Si are different and of size 3, for all i 6= j, Si 6 Sj , and thus Obj + (i) 6 Obj + (j).
By definition of the preferences, we also have Obj + (i) 6 Obj + (j) for each (i, j)  I1  I2 and for each
(i, j)  I2  I1 as well. Hence, by Lemma 13, the only potential source of envy in such an instance
comes from an agent in I2 envying her partner. Since it is impossible to satisfy the two agents of the
same pair at the same time, an allocation is envy-free if and only if it does not satisfy any agent from
I2 .
By Lemma 11, a minimally regular allocation M R is Pareto-efficient if and only if there is no i such
that i 6 Sat(M R ) and Obj + (i)  X \ Allocated(M R ). Therefore a minimally regular allocation M R
is Pareto-efficient and envy-free if and only if there is no k  {1, . . . , |S|} such that M R (|C| + 2k  1) =
{xk , x0k } or M R (|C| + 2k) = {xk0 , x0k0 }, and there is no k0  {1, . . . , |S|} such that {xk0 , x0k0 } 
X \ Allocated(M R ) (this last condition comes down to xk0 6 Allocated(M R ), since xk0 and x0k0 must
be allocated together in a minimally regular allocation). Finally, M R is Pareto-efficient and envy-free
if
S and only if kS  {1, . . . , |S|}, there is an i  I1 such that xk  M R (i), that is, if and only if
iI1 M R (i) =
aj S {xj }.
Let M R be a minimally regular allocation. Then we can define the sub-collection g(M R ) by
g(M R ) = {Si  C | M R (i) = Obj + (i)}. The mapping g clearly defines a bijection between the set of
non-overlapping
sub-collections
S
S
S and the set of minimally regular allocations, and one can notice that
iI1 M R (i) =
Sj g(M R )
ak Sj {xk }.
1
0
Let C 0  CSis an exact cover for
S S. Then
S g (C ) exists
S and is a valid minimally regular allocation.
1
0
We also have iI1 g (C )(i) = Sj C ak Sj {xk } = aj S {xj } because C 0 is a cover. Therefore

g 1 (C 0 ) is Pareto-efficient and envy-free by the previous result.
Conversely, suppose that there is a minimally regular Pareto-efficient andSenvy-free allocation
M R .
S
Then g(M R ) is a non-overlapping sub-collection of C, and is such that Si g(M R ) aj Si {aj } =
S
S
S
{aj } = xj {M R (i) | iI1 } {aj }. M R being Pareto-efficient and envy-free, we have
SiI1 xj M R (i)S
S
S
iI1 M R (i) =
aj S {xj }, and hence
xj {M R (i) | iI1 } {aj } =
xj {xj | aj S} {aj } = S. It proves
that g(M R ) is an exact cover for S.
The reduction is clearly polynomial; hence the NP-hardness.


The previous proof (and especially Lemma 13) sheds the light on the hard case of the
eef existence problem with conjunctive preferences. In an instance of this problem, the
only possible source of envy comes from Obj + (i)  Obj + (j) when we deal we minimally
regular allocations. In that case, we cannot satisfy j without raising envy from i. Now if
Obj + (i) ( Obj + (j), one can remove j from the instance, because if she is satisfied, then
i will envy her (note that this is not true for non-monotonic preferences, because one can
give an object to an agent that does not want it to prevent envy from another agent, and
thus we cannot restrict our problem to minimally regular allocations).
If there is no pair (i, j), i 6= j, such that Obj + (i) = Obj + (j), then one can remove every
agent i such that there is a j 6= i such that Obj + (j) ( Obj + (i). After that, it is easy to
see that every minimally regular allocation is envy-free. Since there is at least one Paretoefficient minimally regular allocation, it guarantees the existence of a Pareto-efficient and
envy-free allocation in that case, may the preferences be monotonic or not. More formally:
544

fiEfficiency and Envy-freeness in Fair Division of Indivisible Goods

Proposition 13 There always exists an efficient and envy-free allocation for an instance
of the eef existence problem with agents having dichotomous preferences restricted to
cubes of objects when the following condition holds:

(i, j)  I 2 , i 6= j, (i = j )  k such that k 6= i, k 6= j and Obj + (k) ( Obj + (i) . (1)
Of course, there is no equivalence between condition 1 and the existence of a Paretoefficient and envy-free allocation7 , because it may happen that, given two agents i and j
having the same preferences, the satisfaction of one of these two agents is prevented by
another agent k such that Obj + (i)  Obj + (k) 6= , but Obj + (k) 6 Obj + (i). This is the
hard case: when two agents i and j have identical preferences, but no agent k is such that
Obj + (k) ( Obj + (i), it may however be possible to prevent i and j from being satisfied
by satisfying another agent k 0 such that Obj + (i)  Obj + (k 0 ) 6= , as the following example
shows: 1 = 2 = x1  x2 , and 2 = x2  x3 . Satisfying the agent 2 only leads to an efficient
and envy-free allocation, whereas the condition 1 does not hold.
Proof (Proposition 13) In the following we will denote by I1 the set of all the agents whose preferences are inclusion-minimal, that is, I1 = {i | @j such that Obj + (j)  Obj + (i)}. We will denote by
I2 the other agents: I2 = I \ I1 .
Here is a simple procedure for finding a Pareto-efficient and envy-free allocation: greedily select
a maximal set S of agents from I1 , such that each agent i  S receives Obj + (i) (until it becomes
impossible to select another unsatisfied agent in I1 ).
The allocation  resulting from this procedure is minimally regular, and by Lemma 13 it is clearly
envy-free (by definition of I1 ). Moreover, suppose there is an i 6 Sat() such that Obj + (i)  X \
Allocated(). Then i 6 I1 , since if it was the case, the procedure would have selected her and therefore
she should be satisfied. We have also i 6 I2 , because if it was the case, then there is a j  I1 such
that Obj + (j)  Obj + (i), and therefore Obj + (j)  X \ Allocated(), which is impossible for the same
reasons as above. Therefore,  is also Pareto-efficient by Lemma 11.


After having investigated two natural restrictions on the propositional language used
for dichotomous preferences, we introduce a more general result, based on the fact that
the hardness result of Proposition 6 is clearly linked to the NP-completeness of the sat
problem. Now what happens if we restrict the expression of preferences to a certain class C
such that sat(C) can be solved in polynomial time ? In the general case, where no additional
assumption is made on C we cannot say anything more about the complexity of the eef
existence problem than about the complexity of the general problem. However, if C is
also closed by conjunction, the complexity falls down to NP:
Proposition 14 Let C be a class of propositional formulae closed by conjunction and such
that sat(C) is in P. Then the eef existence for agents having dichotomous preferences
expressed with formulae from class C is in NP.
Proof Membership to NP comes from the fact that, after having non-deterministically guessed an
allocation , checking if it is envy-free and Pareto-efficient can be done in polynomial time. Given an
allocation, checking if it is envy-free can be done in time O(nm) (where m is the length of the biggest
CNF), just by checking, for each unsatisfied agent, if it would have been satisfied with the share of
another agent. Given the set Sat() ofVthe agents satisfied by , checking the Pareto-efficiency of 
comes down to check i  I \ Sat() if jsatI j  i is unsatisfiable. This can be done by making a
linear number of calls to a sat(C) oracle, since all the preferences are in C, and since this class is closed
by conjunction. It thus proves that the eef existence with formulae in C is in NP.

7. Otherwise, the Proposition 12 would be false, or we would have proved that P = NP.

545

fiBouveret & Lang

As a corollary, for all classes of propositional formulae such that sat(C) is polynomial,
and that contain cubes, the eef existence problem is NP-complete. This applies for
example to the class of 2-CNF formulae or to the class of Horn clauses.
4.3 Alternative Efficiency Criteria
The main reason for the high complexity of the eef existence problem is that the Paretoefficiency of an allocation is hard to check. As a consequence, complexity can decrease if
we choose an alternative notion of efficiency. We investigate here two alternative efficiency
criteria: completeness of the allocation, and maximal number of satisfied agents.
First, we weaken the Pareto-efficiency by only requiring the allocations to be complete.
Unsurprisingly, this makes the complexity fall down to NP.
Proposition 15 The problem of deciding whether a complete envy-free allocation for agents
with dichotomous preferences exists is NP-complete, even for 2 agents with identical preferences.
Proof Since checking that an allocation is complete can be done in polynomial time, membership to
NP is straightforward.
We will prove hardness by a reduction from the sat problem. Let  be a propositional formula. We
create the following instance of the resource allocation problem : we map each propositional variable
of  to a different object and we add another object y ; the two agents have the same preferences,
represented by the formula   y. Obviously, every complete allocation satisfies at least one agent (the
one who receives y). If  is satisfiable, it is possible to satisfy the other agent as well with a share
that corresponds to a model of : thus in this case there exists a complete and envy-free allocation.
Conversely, suppose that there exists a complete and envy-free allocation. Then one of the two agents
must be satisfied thanks to  (since y cannot be given to both agents), hence proving that  is satisfiable.


Notice that the hardness proof above is no longer valid if we require preferences to be
monotonic. As an anonymous referee pointed out, it can be proved by a reduction from
exact-cover-by-3-sets that the result holds if we require monotonicity, while relaxing
the restriction on the number of agents. However, we do not know if NP-hardness holds for
two agents with monotonic preferences (we conjecture it does).
Secondly, we can think of looking for cardinality-maximal subsets of satisfied agents,
instead of inclusion-maximal subsets like Pareto-efficiency does.
Proposition 16 The problem of deciding whether an envy-free allocation satisfying a maximal number of agents with monotonic dichotomous preferences exists is p2 -complete.
Proof Checking whether there exists an envy-free allocation satisfying at least k agents is in NP;
therefore, the maximal number of agents who can be satisfied simultaneously can be computed by
dichotomy within log n NP oracles. It then suffices, after this step is done, to guess an allocation and
check that it is envy-free and satisfies a maximal number of agents, adding just one more NP oracle.
Hence the membership to p2 .
Hardness is obtained by a reduction from the following problem8 :
Problem 4: max-index-satodd (Wagner, 1990)
INSTANCE: A sequence of propositional formulae h1 , . . . , n i such that (i is unsatisfiable)  (i+1
is unsatisfiable.)
QUESTION: Is the maximum index i such that i is satisfiable an odd number ?
8. This problem is referred to several times in the literature, but it does not seem to have a name.

546

fiEfficiency and Envy-freeness in Fair Division of Indivisible Goods

First notice that the complexity of the latter problem does not decrease under the following assumptions:
 n is even (if it is not, we can just add the  formula at the end of the sequence);
 the sets of propositional variables of the formulae i are pairwise-disjoint (if two formulae i and
i+1 share some variables, just transform each variable v of say i into a copy v 0 : it will not
change the (un)satisfiability of i but the set of propositional variables of i and i+1 will now
be disjoint).
Let h1 , . . . , n i be an instance of max-index-satodd with the two additional latter assumptions,
and let Vi denote the set of propositional variables appearing in i . We translate this instance to the
following instance P(1 , . . . , n ):
Agents:
Objects:
Preferences:

2n agents : I1,2  I3,4      In1,n , where the group I2i1,2i contains the four agents
{4i  3, 4i  2, 4i  1, 4i}.
we create for each v  Vi (for each i  {1, . . . , n}) four objects xv , xv , yv , yv , and we
add n dummy objects dk (k  {1, . . . , n});
for each group I2i1,2i (i  {1, . . . , n/2}), the preferences of the agents are:
 4i3 = 4i2 = (02i1  d2i1 )  (02i  d2i ),
V
 4i1 = vV2i1 V2i xv  xv ,
V
 4i = vV2i1 V2i yv  y v ,
where 0k is the formula k where v has been replaced by xv  yv , and v has been
replaced by xv  y v .

The proof of the proposition is primarily based on the fact that the problem can be split into n/2
subproblems concerning the agents from I2i1,2i :
Lemma 14 We denote by P|i the restriction of P(1 , . . . , n ) to the set of agents I2i1,2i and to the
objects they want. An allocation  will be said to be splittable if i 6= j, (I2i1,2i )  (I2j1,2j ) = .
The restriction of a splittable allocation  to (I2i1,2i ) will be written |i .
There exists an envy-free allocation satisfying a maximal number of agents for P(1 , . . . , n ) if and
only if there exists a splittable allocation  such that i  {1, . . . , n/2}, |i is envy-free and satisfies a
maximal number of agents for P|i .
Proof First, we restrict our attention to regular allocations, where regular means, like in Lemma 2,
that an allocation gives an object to an agent only if she wants it. This is safe for the same reasons
as in Lemma 2: the existence of an envy-free allocation satisfying a maximal number of agents
is equivalent to the existence of a regular envy-free allocation satisfying a maximal number of
agents. Since the sets Vi are pairwise-disjoint, two different subproblems P|i and P|j do not share
any object, and therefore any regular allocation is also splittable.
0
Let  be a regular allocation. Suppose that there is an allocation |i
for the problem P|i that
satisfies more agents than |i . Then the splittable allocation made from the sub-allocations |j
0
for j 6= i and |i
is valid, regular, and satisfies more agents than . Conversely, suppose that
there is a regular allocation  0 that satisfies more agents than . Then there is at least one i such
0
that strictly more agents from I2i1,2i are satisfied by |i
than by |i . It proves that any regular
allocation  satisfies a maximal number of agents if and only if for each i, |i satisfies a maximal
number of agents.
Suppose now that  is envy-free. Then obviously all the |i are as well. Conversely, suppose
that all the |i are envy-free. Then  is envy-free, because (1) no agent can envy another agent
from the same group, because the |i are envy-free, and (2) no agent from a group i can envy an
agent from another group j, since (I2i1,2i )  (I2j1,2j ) = .

For each interpretation Intk of Vk , we define the following sets of objects:
 f (Intk ) = {xv | Intk |= v}  {xv | Intk 6|= v};
 g(Intk ) = {yv | Intk |= v}  {yv | Intk 6|= v};
 f (Intk ) = {xv | Intk 6|= v}  {xv | Intk |= v};

547

fiBouveret & Lang

 g(Intk ) = {yv | Intk 6|= v}  {yv | Intk |= v}.
Moreover, given two interpretations Int2i1 and Int2i of respectively V2i1 and V2i , we will write
Int2i1 ,Int2i the following allocation of P|i :
 Int2i1 ,Int2i (4i  3) = f (Int2i1 )  g(Int2i1 )  {d2i1 };
 Int2i1 ,Int2i (4i  2) = f (Int2i )  g(Int2i )  {d2i };
 Int2i1 ,Int2i (4i  1) = f (Int2i1 )  f (Int2i );
 Int2i1 ,Int2i (4i) = g(Int2i1 )  g(Int2i ).
Lemma 15 Let Int2i1 and Int2i be two respective interpretations of V2i1 and V2i .
 Int2i1 ,Int2i satisfies both agents 4i  1 and 4i;
 Int2i1 ,Int2i satisfies 4i  3 if and only if Int2i1 |= 2i1 , and Int2i1 ,Int2i satisfies 4i  2 if
and only if Int2i |= 2i ;
Proof Let Int2i1 and Int2i be two respective interpretations of V2i1 and V2i .
 By definition, f (Intk ) contains xv or xv for each v  Vk , thus Int2i1 ,Int2i (4i1) contains xv
or xv for each v  V2i1  V2i . Therefore the agent 4i  1 is satisfied by Int2i1 ,Int2i (4i  1).
The same reasoning holds for the agent 4i.
 By definition, 2i1 is satisfied by Int2i1 if and only if 02i1 is satisfied by the interpretation
defined by setting to true all the xv and yv (resp. all the xv and yv ) such that Int2i1 |= v
(resp. Int2i1 6|= v). Thus, if Int2i1 |= 2i1 , Int2i1 ,Int2i satisfied 02i1 . Since it also
satisfies d2i1 , then 4i  3 is satisfied by Int2i1 ,Int2i . Conversely, if 4i  3 is satisfied by
Int2i1 ,Int2i , then obviously 02i1 must be satisfied by Int2i1 ,Int2i (because 4i  3 does
not receive d2i ), which proves that 2i1 is satisfied by Int2i1 . The same result holds for
2i and the agent 4i  2.

Lemma 16 Consider the restricted problem P|i .
 If neither 2i1 nor 2i is satisfiable, then for any interpretations Int2i1 and Int2i of V2i1 and
V2i respectively, Int2i1 ,Int2i is envy-free and satisfies a maximal number of agents.
 If only 2i1 is satisfiable, then if M2i1 is a model of 2i1 , M2i1 ,Int2i satisfies a maximal
number of agents. Moreover, there is no envy-free allocation satisfying a maximal number of
agents in this case.
 If both 2i1 and 2i are satisfiable, then if M2i1 and M2i are respective models of 2i1 and
2i , M2i1 ,M2i satisfies a maximal number of agents and is envy-free.
Proof Suppose that neither 2i1 nor 2i is satisfiable. Then any allocation |i satisfying 4i  3
(resp. 4i  2) must be such that there is at least one v  V2i1  V2i such that {xv , xv , yv , yv } 
|i (4i  3) (resp. |i (4i  2)), because otherwise one could deduce a model of 2i1 or 2i from
|i (4i  3) (resp. |i (4i  2)). Thus any of the agents 4i and 4i  1 can be satisfied in this case:
the maximal number of agents it is possible to satisfy is 2. Since every allocation of the form
Int2i1 ,Int2i satisfies both agents 4i  1 and 4i, it satisfies a maximal number of agents in this
case. It is also obviously envy-free, since neither d2i nor d2i1 are in the shares of agents 4i  1
and 4i, and thus the 2 other agents cannot envy them.
Suppose that only 2i1 is satisfiable. Then any allocation satisfying both agents 4i  3 and
4i  2 must satisfy 02i1 for one of these two agents, and 02i for the other one (because of d2i
and d2i1 ). Since 2i is not satisfiable, in that case neither 4i  1 nor 4i can be satisfied by |i ,
for the same reasons as above. From that we can deduce that it is not possible to satisfy the 4
agents. Neither is it possible to satisfy 3 agents with both 4i  3 and 4i  2 satisfied. Now consider
the allocation M2i1 ,Int2i , M2i1 being a model of 2i1 . By Lemma 15, M2i1 ,Int2i satisfies
3 agents: 4i  3, 4i  1 and 4i. This allocation is not envy-free, but no allocation satisfying a
maximal number of agents can be in this case (because either 4i  3 or 4i  2 remains unsatisfied
in such an allocation, envying her partner).
Lastly, suppose that both 2i1 and 2i are satisfiable, and let M2i1 and M2i be their models.
Then by Lemma 15, M2i1 ,M2i satisfies the 4 agents, thus satisfying a maximal number of agents
and obviously being envy-free.


548

fiEfficiency and Envy-freeness in Fair Division of Indivisible Goods

We can now conclude the proof. By Lemma 14, there exists an envy-free allocation satisfying a
maximal number of agents for P(1 , . . . , n ) if and only if there exists a splittable allocation  such
that i  {1, . . . , n/2}, |i is envy-free and satisfies a maximal number of agents for P|i . By Lemma 15,
there exists an envy-free allocation |i satisfying a maximal number of agents for P|i if and only if either
none of the two formulae 2i1 and 2i are satisfiable, or both are. Now suppose that the maximum
index j such that j is satisfiable is an odd number (say 2i  1). In that case, there is not any envyfree allocation satisfying a maximal number of agents for P|i since 2i1 is satisfiable but 2i is not.
Conversely, suppose that the maximum index j such that j is satisfiable is an even number (say 2i).
In that case, there is an envy-free allocation satisfying a maximal number of agents for all P|k , since for
each P|k either the two formulae 2k1 and 2k are satisfiable (if k  i), or none of them is (if k > i).
We thus have a reduction from the latter problem to the problem of existence of an envy-free
allocation satisfying a maximal number of agents. It proves the p2 -completeness.


5. Non-dichotomous Preferences
We will now consider the case where preferences are no longer dichotomous.
5.1 General Logical Preferences
Again, since an explicit description of preferences is exponentially large, the need for a
compact description thereof is clear. Many languages exist for succinct representation of
preference. We limit our investigation to the following class of languages:
Definition 9 (Compact language under logical form) Let L be a language representing a set of preference relations over a set of alternatives 2X . L will be said to be a compact
language under logical form if and only if :
(a) it is able to express any dichotomous preference as compactly as the previous language
introduced, that is, the language expressing dichotomous preferences in propositional
form can be polynomially reduced to L;
(b) comparing two sets of goods can be done in polynomial time.
These two previous conditions are in practise not very restricting, and are met by many
languages for succinct representation of representation. See for instance the paper by Lang
(2004) for a survey of logical languages for compact preference representation. Note that
several widely studied representation languages, such as CP-nets and other graphical languages, are not under logical form, because they fail to represent preferences expressed
by logical formulas within polynomial space9 . Interestingly, Proposition 6 extends to any
compact representation language under logical form:
Corollary 2 eef existence with monotonic compact preference under logical form is p2 complete.
Proof The eef existence problem can be solved using the following algorithm:
1. non-deterministically guess an allocation ;
2. check that it is envy-free;
3. check that it is Pareto-efficient.
9. A natural question is the complexity of the fair division problems when preferences are expressed in
these languages. This is left for further study.

549

fiBouveret & Lang

By condition (b), step two can be done in polynomial time, since it requires at most a quadratic number
of polynomial oracles. By condition (b) also, the problem of checking whether a given allocation is
Pareto-efficient is in co-NP. Therefore, the previous non-deterministic algorithm uses 1 NP oracle, and
runs in polynomial-time. Hence membership to p2 .
Hardness is a corollary of Proposition 6 together with condition (a).


5.2 Numerical Preferences under Logical Form
For the latter result preferences do not have to be numerical since Pareto-efficiency and
envy-freeness are purely ordinal notions. Now, if preferences are numerical, which implies
the possibility of intercomparing and aggregating preferences of several agents, then, besides
Pareto-efficiency, we may consider efficiency based on social welfare functions. We consider
here only the two most classical way of aggregating a collection of utility functions into a
social welfare function:
Definition 10 (Classical utilitarianism and egalitarianism) Given a collection of individual utility functions hu1 , . . . , un i, with for each i, ui : 2X  Z:
?
 the
P classical utilitarian social welfare function is the function defined by sw :  7
i ui ((i));

 the egalitarian social welfare function is the function defined by sw(e) :  7 mini ui ((i));
Maximizing the egalitarian social welfare function is often viewed as an alternative criterion of fairness, encoding the Rawlsian egalitarian point of view (Rawls, 1971). However,
as we will see in Proposition 17, egalitarianism (as well as classical utilitarianism) is not
always compatible with envy-freeness. The link between these two alternative points of
view about fairness is deeply investigated by Brams and King (2005).
Since we do not deal anymore with ordinal (or dichotomous) preferences, we have to
define precisely what we will mean by compact representation of numerical preferences. Here
we will pick as the basic numerical language one of the most simple compact languages,
consisting in associating numerical weights to propositional formulae  see e.g. (Chevaleyre,
Endriss, & Lang, 2006), or (Ieong & Shoham, 2005) in the context of coalitional games:
Definition 11 (Weighted propositional language) Given a set of goods X, the weighted
propositional language associated to X is the set of all possible subsets of LX  Z.
Given a set of weighted propositional formulae  = {h1 , w1 i, . . . , hr , wr i}, the utility
function associated is:

1 if S |= k
X
u : 2
 Z
, with k =
Pr
0 otherwise.
S
7
k=1 wk  k
Using this language, preferences are monotonic if all formulae are positive and all weights
are positive.
Now, we can define our notion of compact numerical language:
Definition 12 (Compact numerical language under logical form) Let L be a language representing a set of utility functions over a set of alternatives 2X . L will be said to
be a compact numerical language under logical form if and only if :
550

fiEfficiency and Envy-freeness in Fair Division of Indivisible Goods

(a) it is able to express any utility function as compactly as the weighted propositional
language, that is, the weighted propositional language can be polynomially reduced to
L;
(b) computing the utility of one set of goods can be done in polynomial time.
Of course, since a compact numerical language under logical form is also a compact
language under logical form, the complexity result form Corollary 2 still holds. However, it
appears that the complexity of the problem of deciding whether an efficient and envy-free
allocation exists decreases when the Pareto-efficiency is replaced by a weaker notion: the
maximization of one of the two latter social welfare functions.
Proposition 17 Given a collection of utility functions on 2R given in a compact numerical
language under logical form:
 the problem of deciding whether there exists an envy-free allocation among those that
maximize utilitarian social welfare is p2 -complete, even if N = 2, and even if the
agents have identical preferences.
 the problem of deciding whether there exists an envy-free allocation among those that
maximize egalitarian social welfare is p2 -complete, even if N = 2.
Proof For both results, membership comes easily from the fact that the maximum value of social
welfare can be computed by dichotomy over the set of all possible social welfare values; there are
exponentially many, therefore we need a polynomial number of NP oracles to do this; after this step
is done, it suffices to guess an allocation and check that it is envy-free and that it maximizes social
welfare, adding just one more NP oracle.
Hardness is obtained in both utilitarian and egalitarian cases by a simple reduction to an instance
of the fair division problem with preferences expressed in the weighted propositional language from the
following problem:
Problem 5: max-sat-asgeven (Wagner, 1987)
INSTANCE: A propositional formula  in Conjunctive Normal Form, over a set of propositional
variables V = {v1 , . . . , vn }, and a weight function w over interpretations I : V  {0, 1},
def P
i1
defined by w(I) =
.
i I(vi )  2
QUESTION: Is maxM model of  w(M ) an even number (in other words, is v1 falsified in the model of
maximal weight) ?
We will suppose that the formula  has at least on model M such that M 6|= v1 . It does not
change the complexity, because if v1 is verified in every model of , clearly the answer to the problem
max-sat-asgeven is no: as a consequence, every instance h, V i without any assumption on  can be
solved by first checking if v1   is unsatisfiable (which is a coNP-complete problem), and then, if
not, by solving the unsat-or-max-sat-asgeven problem on an instance that has at least one model
falsifying v1 .
Utilitarian social welfare: From an instance h, V i of max-sat-asgeven , we create the following
instance P(, V ):
Agents:
Objects:
Preferences:

2 agents;
for each literal vi of , we create two objects xi and x0i , except for v1 , for which only
one object x1 is created, and we add two objects y and y 0 ;
the agents 1 and 2 have identical preferences, and both ask for h(  y)  ( 0 
y 0 ), 2n+1 i, hx1  y, 1i, . . . , hxn  y, 2n1 i, hx02  y 0 , 2i, . . . , hx0n  y 0 , 2n1 i, with  the formula  in which each symbol vi has been replaced by xi , and  0 the formula  in which
each symbol vi (except v1 replaced by x1 ) has been replaced by x0i .

551

fiBouveret & Lang

Let (M1 , M2 ) be a pair of models of  (with possibly M1 = M2 ) such that M2 6|= v1 . Then we define
the allocation M1 ,M2 by: M1 ,M2 (1) = {y}  {xi |M1 |= vi } and M1 ,M2 (2) = {y 0 }  {x0i |M2 |= vi }.
The proof is based on the following lemma:
Lemma 17 There exists an envy-free allocation among those that maximize utilitarian social welfare if
and only if there are two models M1 and M2 of  (possibly M1 = M2 ) with M2 6|= v1 , such that M1 ,M2
is envy-free and maximizes utilitarian social welfare.
Proof Let  be an allocation maximizing the utilitarian social welfare. Let M be a model of
 falsifying v1 (our hypothesis is that there is at least one). Then F (M,M (1)) |=   y and
F (M,M (2)) |=  0  y 0 , which proves that the individual utility of the two agents is at least 2n+1 .
Hence there is at least one allocation whose utilitarian social welfare is greater than or equal
to 2n+2 . Therefore, an allocation  maximizing the utilitarian social welfare must be such that
F ((1)) |= y   and F ((2)) |= y 0   0 , or vice versa. Moreover, either x1 6 (1), or x1 6 (2).
Suppose that x1  (2): swapping the shares of the agents leads to an allocation  0 which is
completely equivalent to  with respect to Pareto-efficiency and envy-freeness, due to the identical
preferences. We can therefore assume w.l.o.g that  is such that x1 6 (2)
Since F ((1)) |= , then there is a model M1 of  such that (1) = {y}  {xi |M1 |= vi }  S1 ,
where S1  {x1 , x02 , . . . , x0n }. Similarly, there is a model M2 such that M2 6|= v1 , and (2) =
{y 0 }  {x0i |i > 1, M2 |= vi }  S2 , where S2  {x1 , . . . , xn }. Now consider the allocation M1 ,M2 ,
that is well-defined since M2 6|= v1 . We have u1 () = u1 (M1 ,M2 ), since the x0i do not satisfy any
formula of the preferences of the agent 1 without y 0 (given to the agent 2), and u2 () = u2 (M1 ,M2 )
for the same reasons. In other terms, M1 ,M2 gives the same utility as  to both agents. M1 ,M2
is thus envy-free and maximizes the utilitarian social welfare.

By Lemma 17, we can thus restrict our problem to the allocations of the form M1 ,M2 . We
have, for each M1 and M2 as defined earlier, u1 (M1 ,M2 ) = 2n+1 + w(M1 ) and u2 (M1 ,M2 ) = 2n+1 +
w(M2 ); thus sw? (M1 ,M2 ) = 2n+2 + w(M1 ) + w(M2 ). We have: argmaxM ,M sw? (M1 ,M2 ) =
1
2
argmax(M ,M ) {w(M1 )+w(M2 )|M1 6|=v1 or M2 6|=v1 } . Given the symmetry of the problem, we can assume
1
2
that only M2 has to satisfy M2 6|= v1 , thus the latter allocation becomes: Mopt ,argmaxM {w(M2 )|M2 6|=v1 } ,
2
where Mopt is the model of  of maximal weight.
Suppose Mopt 6|= v1 , then the allocation maximizing the utilitarian social welfare is Mopt ,Mopt and
it is clearly envy-free, because both agents have the same utility. Now suppose that Mopt |= v1 , then
the allocation maximizing the utilitarian social welfare is Mopt ,Mopt0 , where Mopt0 is the model of 
of maximal weight that assigns v1 to false. We have w(Mopt0 ) < w(Mopt ), thus u1 (Mopt ,Mopt0 ) >
u2 (Mopt ,Mopt0 ), hence this allocation is not envy-free.
The latter reduction is clearly polynomial (recall that the weights 2n+1 can be encoded using a
linear space). This proves the proposition for the utilitarian case.
Egalitarian social welfare: From an instance h, V i of max-sat-asgeven , we create the following
instance P(, V ):
Agents:
Objects:
Preferences:

2 agents;
for each literal vi of , we create two objects xi and x0i , and we add two objects y and
y0 ;
the preferences of the agent 1 are hx1 , 1i, . . . , hxn , 2n1 i, h  y, 2n i, and the preferences
of the agent 2 are hy  y 0 , 22n i, hx1 , 1i, with  the formula  in which each symbol vi
has been replaced by xi .

Every allocation that maximizes egalitarian social welfare must give at least a utility of 22n to
agent 2. In that case, the egalitarian social welfare will be given by the utility of agent 1, because this
utility cannot be greater than 22n . Therefore, maximizing social welfare comes down to maximizing the
utility of agent 1, or in other words to give her the items corresponding to the model of  of maximum
weight. If v1 is set to true in this model, then x1 is given to agent 1, and since y is also given to
agent 1, agent 2 could get a strictly higher utility with agent 1s share. Therefore this allocation is not
envy-free. If v1 is set to false in the latter model, then x1 is not given to agent 1, and thus can be given
to agent 2, producing an envy-free allocation. Since the reduction is polynomial, it proves hardness for
the egalitarian case.


552

fiEfficiency and Envy-freeness in Fair Division of Indivisible Goods

We can notice here that the combination of envy-freeness with a numerical criterion
such as classical utilitarianism or egalitarianism induces a complexity gap, since, as stated
by Bouveret, Fargier, Lang, and Lematre (2005), the complexity of the problems of maximizing the classical utilitarian or egalitarian collective utility functions, with agents having
weighted logical preferences, are only NP-complete.
From the previous proof in the utilitarian case, we can notice that the hardness result still
holds if we require the allocation to be Pareto-efficient instead of maximizing the utilitarian
social welfare. It suggests that in the case of a language extending the weighted propositional
formulae, the eef existence problem with identical preferences is much harder than in
the case of agents having identical dichotomous preferences. We have actually the following
result:
Proposition 18 Given a collection of N identical utility functions on 2R given in compact
numerical language under logical form the problem of deciding whether a Pareto-efficient
and envy-free allocation exists is p2 -complete, even if N = 2, even if the preferences are
monotonic.
Proof Since the preferences are identical, any envy-free allocation satisfies all the agents equally. Thus,
a Pareto-efficient and envy-free allocation, if there is one, is an allocation that gives everyone a utility of
M , maximal among the set of allocations that satisfy everyone equally. This value M can be computed
using a polynomial number of NP oracles (like in the previous proof). Now that we have this value
of M , checking if there is a Pareto-efficient and envy-free allocation comes down to check if there is
no allocation giving at least M to all the agents, and at least M + 1 to at least one agent, which is a
problem in coNP, and hence adds only one call to an NP oracle.
For the hardness proof, one may notice that the same reduction as the one used in the utilitarian
case of the proof of Proposition 17 works in this case, because an allocation is Pareto-efficient and
envy-free in this particular problem if and only if it is envy-free and maximizes the utilitarian social
welfare.


5.3 Additive Numerical Preferences
A last case that we consider here is the case of additive numerical preferences. Additive
numerical preferences is a degenerate case of weighted logical preferences, where all the
formulae are single positive literals. In other words, the preferences of an agent i are given
by a set i of pairs hxk , wk i, where xk is an object and wk is the weight (possibly 0)
associated with the object. The utility function associated with such preferences is thus the
following:
u : 2X  Z
P
S
7
xk S wk .
Notice that the agents preferences are monotonic if and only if all the numbers wk are
positive.
This preference representation language is the most natural one when dealing with
resource allocation problems; however, it is unable to express compactly any kind of dependencies (superadditivity or subadditivity) between the objects. In particular, it does
not extend dichotomous preferences. Hence the previous hardness results do not extend
to additive preferences. However, since we are still able to compare two alternatives in
polynomial time, membership to p2 is guaranteed.
553

fiBouveret & Lang

Our intuition is that this problem is as hard as the eef existence problem with
dichotomous preferences:
Conjecture 1 eef existence with additive numerical preferences is p2 -complete, even if
the preferences are monotonic.
All we know about this problem is that it is NP-hard (this is implied by Proposition 20
presented later) and in p2 , but its precise complexity remains open. However, things become
much easier if we only require the allocation to be complete, instead of Pareto-efficient. This
case has already been investigated by Lipton et al. (2004), and we have the following result:
Proposition 19 (Lipton et al., 2004) The problem of deciding whether there exists a
complete envy-free allocation for agents having additive preferences is NP-complete, even if
their preferences are monotonic.
Other restrictions of the eef existence problem with additive preferences are worth
being studied. First, we study as in the dichotomous case the restriction to identical additive
preferences:
Proposition 20 eef existence with N identical additive numerical preferences is NPcomplete, for any fixed N  2. The same result holds if we require the preferences to be
monotonic.
Proof Membership is easy to prove. Since all the preferences are identical (we write hu(x1 ), . . . , u(xp )i
the utility vector associated to the set of object), an allocation is Pareto-efficient if and only if it gives
each object xj such that u(xj ) > 0 to one agent, and trashes each object xj such that u(xj )  0.
Moreover, an allocation is envy-free if and only if all the agents have the same utility. These two latter
properties can be checked in polynomial-time, hence membership to NP.
Hardness comes from a reduction from partition:
Problem 6: partition
INSTANCE: A finite set S and a size s(a)  N for
P each a  S.P
QUESTION: Is there a subset S  S such that aS 0 s(a) = aS\S 0 s(a) ?
From a given instance hS, si of the partition problem, we create the following instance P(S, s) of
the eef existence problem:
Agents:
Objects:
Preferences:

2 agents;
to each a  S, we associate an object xa ;
The two agents preferences are identical and defined by the size of the elements in the
initial set: u(x(a)) = s(a).

There isP
a Pareto-efficient
P and envy-free allocation for P(S, s) if and only if there is an allocation
 such that x(1) u(x) = x(2) u(x), that is, the partition problem returns true. The reduction
can be clearly done in polynomial time, which proves the proposition.


Now another interesting case is the case where the preferences are not necessary identical,
but where all the atomic utilities ui (xj ) are either 0 or 1. In other words, an agent either
wants an object or does not want it, and each agent wants to maximize the number of
desired objects she gets.
Proposition 21 eef existence with additive 01-preferences (i.e. i, j, ui (xj )  {0, 1})
is NP-complete.
554

fiEfficiency and Envy-freeness in Fair Division of Indivisible Goods

Proof Pareto-efficiency is easy to check in this case. We can first safely remove the objects that
do not appear anywhere in the preferences. Afterwards, an allocation is Pareto-efficient if and only
if each object xj is given to an agent i such that ui (xj ) = 1, for the following reasons. () Let 
be a Pareto-efficient allocation, and suppose that there is an xj which is not given to any agent, or
which is given to an agent such that ui (xj ) = 0. Let k be an agent such that uk (xj ) = 1 (there
is one since we previously trashed the undesired objects). Then giving xj to the agent k increases
ks utility while the others utilities remains the same. Thus  is Pareto-dominated. () Let  be
an allocation such that each object xj is given
such P
that ui (xj ) = 1, and
P to an agent iP
P suppose  is
Pareto-dominated by an allocation  0 . Then iI ui ( 0 (i)) = iI xj 0 (i) ui (xj ) > iI ui ((i)) =
P P
iI
xj  0 (i) ui (xj ) = p. Therefore there is at least one ui (xj ) such that ui (xj ) > 1, which is not
possible due to our restriction to 01-preferences.
It thus give an simple way to check Pareto-efficiency, by just checking if the sum of utilities is equal
to the number of objects p that are desired by at least one agent. As usual, envy-freeness can be verified
in polynomial time; therefore eef existence with additive 01-preferences is in NP.
Hardness can be proved by a polynomial reduction from exact cover by 3-sets (problem 3).
Given an instance hS, C = hS1 , . . . , S|C| ii of exact cover by 3-sets, create the following eef existence instance P(C, S) (we will suppose that the elements of S will be written ai , with i  {1, . . . , |S|}):
Agents:
Objects:
Preferences:

a set of 3|C| agents gathered by triples {3i  2, 3i  1, 3i};
a set of |S| + 3|C| items X =SM  D (M for main, and D for dummy), with
M = {m1 , . . . , m|S| }, and D = i{1,...,|C|},j{1,2,3} {di,j };
S
the agents from {3i  2, 3i  1, 3i} all desire the same set of objects ak Si {mk } 
{di,1 , di,2 , di,3 } (the three objects corresponding to Si plus the three dummy objects
di,j ).

If there is an exact cover C 0 for the instance hC, Si, then we will consider the following allocation:
each agent from the triple {3i  2, 3i  1, 3i} gets respectively di,1 , di,2 , and di,3 , and if Si  C 0 , each
one of these three agents gets one of the three objects mk corresponding to the elements of the set
Si . This allocation is admissible and Pareto-efficient (because all the objects are allocated). It is also
envy-free, for the following reasons:
 The agents from the same triple cannot envy each other, because they are equally satisfied.
 An agent k1 cannot envy any agent k2 from another triple, because the only objects k1 could
envy from k2 s share are the mi . k2 having at most one mi , and k1 having a utility of at least
one, k1 cannot envy k2 .
The rest of the proof is based on the following result: if an allocation
S  is Pareto-efficient and
envy-free for P(C, S) then we must have (3i  2)  (3i  1)  (3i) = ak Si {mk }  {di,1 , di,2 , di,3 }
or (3i)  (3i  1)  (3i) = {di,1 , di,2 , di,3 }. This is easy to show. Since the agents from the triple
{3i  2, 3i  1, 3i} are the only ones to desire the objects di,k , these three objects must be given to these
three agents, for the allocation to be efficient. Since these three agents have the same preferences, the
allocation has to satisfy them equally in order to be envy-free. Thus the number of objects allocated
to the three agents must be divisible by 3, which gives only two possible numbers, 3 and 6, and hence
only two possible allocations.
Suppose that there is a Pareto-efficient and envy-free allocation  for P(C, S). Consider the subcollection C 0 S
= hS1 , . . . , S|C 0 | i made of the triples Si from the collection C such that (3i  2)  (3i 
1)  (3i) = ak Si {mk }  {di,1 , di,2 , di,3 }. Then we have the following results.
 The Si are pairwise disjoints. Suppose that there is a pair (i, j) such that i 6= j and there is
an element ak belonging to both Si and Sj . Then mk is allocated to two different agents: one
member of triple {3i  2, 3i  1, 3i}, and one member of triple {3j + 1, 3j + 2, 3j + 3}, which is
impossible.
S
 i{1,...,|C 0 |} Si = S. Let ak be an element of S. Since  is Pareto-efficient, then mk must be
allocated to one agent that wants it (say
S this agent belongs to the triple {3j + 1, 3j + 2, 3j + 3}),
unless no one wants it, which occurs if i{1,...,|C 0 |} Si 6= S. Then, by the previous result, all the
S
objects from al Sj {ml } must be allocated to this triple. Consequently, Sj  C 0 . Since ak  Sj ,
ak belongs to at least one set from the collection C 0 .

555

fiBouveret & Lang

Therefore, C 0 is an exact cover of S, which finally proves the proposition.



We can see with Proposition 21 and Conjecture 1 that there is a huge complexity gap
between the problem where we allow the weights to be freely given and the problem where
we require the weights to be 0 or 1 (at least if the conjecture is true). The natural question
it raises is to know if this complexity fall is specific to the 01-preferences, or if it occurs
for any fixed upper bound of the weights.
Conjecture 2 The complexity of the eef existence problem with additive 01. . . k
preferences for k  2 fixed is as hard as the general problem with unbounded additive preferences.
The precise complexity of this problem remains an open problem, but as it is stated
in the conjecture, our intuition is that it is as hard as the eef existence problem with
unbounded additive preferences.
Another natural problem is raised by the Proposition 21: what is the complexity of the
eef existence problem with stratified 01-preferences ? By stratified 01-preferences,
we mean that the preferences are given by a set of pairs hxk , pi, where xk is an object and p
is a priority level. Comparing two sets of objects comes down to compare lexicographically
the vectors where the component at index i is the number of objects of priority i in the
share of the agent. Notice that this problem is not an instance of the eef existence
problem with additive preferences, nor an instance of the eef existence problem with
logical numerical preferences under logical form. However it is easy to see that it remains
in p2 , but its precise complexity remains unknown.
Finally, we investigate the case where the number of objects is less than the number
of agents. One could think intuitively that the problem is trivial in this case. However, it
is not always the case, as we will see. To begin with, the following results shows that the
problem is easy for monotonic preferences:
Proposition 22 Let P be an allocation problem with n agents having additive monotonic
numerical preferences and all wanting at least one object, and p objects that are desired by
at least one agent.
 If p < n, then there is no Pareto-efficient and envy-free allocation.
 If p = n, the problem of deciding whether there exists an efficient and envy-free allocation for agents having monotonic additive preferences is in P.
Proof
 Each object being desired by at least one agent, every Pareto-efficient allocation is complete. If the number of objects p is strictly lower then the number of agents N , then at least one
agent i is unsatisfied. Consequently, there is an agent j that obtains an object wanted by i, hence
creating envy. Thus no Pareto-efficient allocation can be envy-free.
 Since there are as many objects as agents, each agent should receive one object that she values
the most (that is, such that ui ({x}) is maximal), for an allocation  to be Pareto-efficient and
envy-free. Indeed, if an agent i receives an object that is not her preferred one, it means that
another agent j receives it (otherwise the allocation would not be Pareto-efficient), creating envy
from i. Therefore, checking the existence of a Pareto-efficient and envy-free allocation comes
down in this case to checking if it is possible to give to every agent one of her best-valued objects.
It comes down to checking if there is a perfect matching in the bipartite graph made with one
node per agent on the one side, and one node per object on the other side, and connecting an

556

fiEfficiency and Envy-freeness in Fair Division of Indivisible Goods

agent i to an object x if and only if x is among the best-valued objects in agent is preferences.
Such a perfect matching can be computing in polynomial time, hence the result.


Interestingly, the latter result does not hold at all if we allow non-monotonic preferences.
In that case, the complexity increases up to the complexity of the general eef existence
problem with additive preferences.
Proposition 23 The eef existence problem with additive numerical preferences and such
that the number of objects is less than the number of agents has the same complexity as the
eef existence problem with additive numerical preferences, but no assumption on the
number of objects.
Proof Let us consider an instance hI, X, h. . . , ui (xj ), . . . i of the eef existence problem with additive numerical preferences, with N agents and p objects (p > N ). We create the following instance
P(hI, X, h. . . , ui (xj ), . . . i):
Agents:
Objects:
Preferences:

p + 3 agents (the number of agents is not important, it just has to be greater than the
number of objects and of the initial number of agents);
the p initial objects xi plus two dummy ones d1 and d2 ;
the preferences of the N first agents are the same as in hI, X, h. . . , ui (xj ), . . . i; the
preferences of the (N +1)st agent are uN +1 ({d1 }) = uN +1 ({d2 }) = 1 and u{N +1} (xj ) =
0 for the other items xj ; and the preferences of the remaining agents are uN +1 ({d1 }) =
1, uN +1 ({d2 }) = 2 and uN +1 (xj ) = 0 for the remaining objects.

If there is an efficient and envy free allocation  for hI, X, h. . . , ui (xj ), . . . i then it can be easily
checked that the allocation that gives the same items to the N first agents of P(hI, X, h. . . , ui (xj ), . . . i),
{d1 , d2 } to the (N + 1)st agent, and nothing to the remaining ones is efficient and envy-free. Conversely,
any Pareto-efficient and envy-free allocation for P(hI, X, h. . . , ui (xj ), . . . i) yields a Pareto-efficient and
envy-free allocation for hI, X, h. . . , ui (xj ), . . . i by restricting it to the N first agents and all the objects
but the two dummy ones.


6. Related Work and Discussion
As already argued in the Introduction, computational studies in resource allocation either
concern divisible goods, or focus on classical utilitarianism such as in combinatorial auctions.
Existing work on fair division of indivisible goods, on the other hand, is mainly axiomatic,
and its computational aspects have been neglected, except in a few papers that we are
mentioning below.
Our results have of course a lot in common with complexity results for combinatorial
auctions. After all, the structure of the problems are, to some extent, similar: items are
indivisible, allocations are preemptive10 , and each agent has preferences over sets of items
expressed in some compact representation language. Logical bidding languages have also
been designed (Boutilier & Hoos, 2001). However, the complexity results completely differ:
the standard decision problem for combinatorial auctions is NP-complete (Rothkopf, Pekec,
& Harstad, 1998) while the decision problems considered here are typically located at the
second level of the polynomial hierarchy, even in the degenerate case where preferences
are dichotomous. This can be explained by the fact that combinatorial auctions care only
about efficiency, not about envy-freeness. Requiring both criteria together (efficiency and
10. Here, preemptive means that an object cannot be allocated to more than one agent. This assumption
can be absent from some problems implying for example virtual objects, such as software licences.

557

fiBouveret & Lang

envy-freeness) makes things much more difficult: while, under the usual assumption that
preferences are monotonic, efficiency is a monotonic property (allocating more goods never
makes an allocation less efficient), envy-freeness is not (allocating more goods to an agent
may generate envy)11 . This is the reason why there may not exist an EEF allocation, and
this is also the source of the high complexity of the problem.
Moreover, due to the failure of monotonicity for envy-freeness, searching for an EEF
allocation cannot simply be formulated as the maximization or minimization of a simple
criterion, which is problematic when designing local search algorithms or approximation
schemes. A few authors (Lipton et al., 2004; Chevaleyre, Endriss, Estivie, & Maudet,
2007a; Brams, Jones, & Klamler, 2007) have suggested to relax the envy-freeness criterion
and make it a gradual notion, by defining a measure on envy-freeness. Lipton et al. (2004)
assume that the input consists of numerical utility functions over sets of goods; the degree
to which agent i envies agent j in allocation  is either defined as the envy difference
di,j () = max(0, ui ((j))  ui ((i))) or the envy ratio rij () = max 1, uuii((j))
((i)) . In both
cases, the global degree of envy is then the maximum degree of envy between any pair of
players. Alternative definitions of the degree of envy in a society have been proposed by
Chevaleyre et al. (2007a), such as the number of envious agents, the number of pairs (i, j)
such as i envies j, or the sum of all local degrees of envy, which is relatively similar to
the measure of envy considered by Brams et al. (2007), based on the maximum number
of agents that any single agent may envy. Chevaleyre et al. (2007b) suggest a radically
different way of relaxing envy-freeness: the society comes along with an undirected graph
(which reflects acquaintance between agents), and an allocation is envy-free if and only if
no agent envies an agent to whom she is connected.
Then, Lipton et al. (2004) focus on the search for a complete allocation with minimum
envy; moreover, in the case of additive utilities, they provide approximation schemes. Another work on approximation algorithms for the fair allocation of indivisible goods is the
one by Asadpour and Saberi (2007), who assume from the beginning that utilities are linear
and consider the problem of finding a maximally equitable allocation, that is, an allocation maximizing the utility of the least satisfied agent (cf. the problem considered in our
Proposition 17); they do not consider envy-freeness at all12 .
All the approaches we have considered so far (including ours) assume that the allocation
is computed in a centralized way by a neutral, objective agent. In other contexts, this
centralized approach is not possible or not realistic, and the allocation is obtained in a
decentralized way, by successive negotiations between groups of agents. Such an approach
has been initiated by Sandholm (1998), who studies convergence properties on the allocation
depending on the structural restrictions made on exchanges of goods that may occur. It
has been further investigated by Dunne (2005) and Dunne et al. (2005), who study the
computational complexity of these negotiation problems, and by Endriss, Maudet, Sadri,
and Toni (2006). In these approaches, the optimality criterion is the classical utilitarian
11. Note that it is not antimonotonic either: allocating less goods to an agent may generate envy as well.
12. Note that alternative ways of relaxing the search for EEF allocations exist. One may for instance
keep envy-freeness as a hard requirement and relax efficiency, or allow for relaxing both and look for
an allocation that shows a good trade-off between efficiency and envy-freeness (considering then the
problem as a two-criteria optimization problem).

558

fiEfficiency and Envy-freeness in Fair Division of Indivisible Goods

social welfare. Envy-freeness is considered in a distributed setting by Chevaleyre et al.
(2007a, 2007b).
So far, by computational issues we referred to the design and the study of algorithms
to be run on computers so as to find an allocation, and not to designing and studying
protocols that query agents interactively so as to gather enough information for a solution to
be determined. These procedural issues, although extensively studied in the literature of
fair division of divisible goods (see below), as well as in voting (Conitzer & Sandholm, 2005)
have rarely been considered for allocating indivisible goods, with the notable exception
of Herreiner and Puppe (2002), who study the properties of interactive protocols where
two agents enumerate their preferred bundles one by one, until an allocation is found. As
mentioned in the Introduction, the drawback of such protocols is that they are exponentially
long, and henceforth infeasible as soon as the number objects is more than a few units.
Beyond these few works on the computational aspects of fair division of indivisible goods,
there are much more that consider the computational and procedural aspects of fair division
of divisible goods (or, at least, assume that at least one good is divisible  e.g., money).
The literature on the subject is vast and the techniques are quite far from those used for
allocating indivisible goods (see in particular the literature on cake-cutting algorithms, e.g.
Brams & Taylor, 1996; Robertson & Webb, 1998) so we do not find it relevant to give here a
detailed bibliography on this subject. The interested reader can refer to the book by Brams
(2008) that covers fair resource allocation in both the indivisible and the divisible case.

7. Conclusion
We have studied several computational aspects the search for efficient and envy-free allocations in fair division problems with indivisible goods. Our results of Section 3 show that
in the case of dichotomous preferences, the search for such allocations can be reduced to
the search for preferred models in prerequisite-free default logic. Such a connection was
rather unexpected, and it implies that the practical search for EEF allocations can be done
using existing algorithms for default logic. However this search is likely to be very timeconsuming, due to our complexity results: indeed, our extensive study of the complexity of
deciding whether an efficient and envy-free allocation exists, under various restrictions (dichotomous preferences or not, two agents or more, all agents having identical preferences or
not, monotonic preferences or not) and for various notions of efficiency (Pareto-efficiency,
completeness, maximum number of satisfied agents, maximum social welfare  classical
utilitarian or egalitarian), seems to show that the problem is intrinsically very difficult,
since it lies at the second level of the polynomial hierarchy, even if preferences are dichotomous and monotonic. This implies that designing fast algorithms for solving the problem
in the general case is out of reach. We may first focus on those restrictions for which the
problem is at most NP-complete. Unfortunately, these restrictions (agents with identical
preferences; only two agents; purely conjunctive, purely disjunctive or 2-CNF preferences;
search for complete allocations without any other efficiency requirement; additive 0-1 preferences) are very compelling and imply a huge loss of generality.
The complexity results introduced in this paper are summed-up in Figure 1 and Table 2.
Several issues for further research remain to be explored.
559

fiBouveret & Lang

12
12

1

16

?

1

p2

22

13

?

p2

15

14

15

p2

p2

7
7

p2

p2
5

3

coBH2

coBH2
4
11

NP

2

11
6

9

9

6

?

18
6?

17

18
17

19

NP

8
8

P

10

21

10

P

20

O(1)

O(1)
? Proof not included in this paper.

1

A problem whose complexity has been proved in this paper (the mapping between
the numbers and the problems is specified in table 2).

17

A problem whose complexity was already known in the literature.

22

i

?

A problem whose complexity remains unknown.

j

The intersection of the problems corresponding to its outgoing edges.
Problem i is included in problem j. Arcs that can be obtained by transitivity are
omitted.

Figure 1: The different problems and their complexity classes and inclusion relations.

560

fiEfficiency and Envy-freeness in Fair Division of Indivisible Goods

Efficiency

number of agents

preferences

monotonicity

comp.

any

yes (1) or no (1)

p2 -c.

identical

yes

NP-c.

identical

no

coBH2 -c.

any
any

yes
no

NP-c.
coBH2 -c.

ident. or not

yes (6) or no (6, 6)

NP-c.

any

yes (7) or no (7)

p2 -c.

disjunctions
conjunctions
conj. with
condition 1
C st sat(C)  P
and closed by 

yes (8) or no (8)
yes (9) or no (9)

P
NP-c.

yes (10) or no (10)

O(1)

yes (11) or no (11)

NP-c.

numerical

yes (12) or no (12)

p2 -c.

numerical

no

p2 -c.

numerical

no

p2 -c.

Dichotomous preferences
1, 1

Pareto-eff.

not fixed

2

Pareto-eff.

3

Pareto-eff.

4
5

Pareto-eff.
Pareto-eff.

6, 6, 6

complete all.

7, 7

max nb of ag.

not fixed (6, 6, 6)
or fixed with N  2
(6, 6)
not fixed

8, 8
9, 9

Pareto-eff.
Pareto-eff.

any
any

10, 10

Pareto-eff.

any

11, 11

Pareto-eff.

any

12, 12

Pareto-eff.

not fixed

13

utilitarian sw

14

egalitarian sw

15, 15

Pareto-eff.

16
17, 17

Pareto-eff.
complete all.

18, 18

Pareto-eff.

19
20
21
22

Pareto-eff.
Pareto-eff.
Pareto-eff.
Pareto-eff.

not fixed or fixed
with N  2
not fixed or fixed
with N  2
2 agents
2 agents

Non-dichotomous preferences
not fixed or fixed
with N  2
not fixed or fixed
with N  2
not fixed or fixed
with N  2
not fixed
not fixed
not fixed or fixed
with N  2
any
> Nb of objects
= Nb of objects
 Nb of objects

numerical,
identical
additive
additive

yes (15) or no (15)

p2 -c.

no
yes (17) or no (17)

p2 -c. ?
NP-c.

additive ident.

yes (17) or no (17)

NP-c.

additive 01
additive
additive
additive

yes
yes
yes
no

NP-c.
O(1)
P
p2 -c. ?

Table 2: The set of resource allocation problems studied in this paper. Their complexity
classes are represented in figure 1.

561

fiBouveret & Lang

First, knowing that there is no efficient and envy-free allocation for a given problem
is not very helpful in practice when an allocation has to be found anyway. The solution
then consists in defining functions that return an allocation in all cases, even when envyfreeness and efficiency cannot be jointly met. A way of addressing this issue consists in
defining suitable relaxations of the problem, such as: (a) using a measure of envy instead
of seeing envy-freeness as a strict criterion (as suggested by Lipton et al., 2004; Chevaleyre
et al., 2007b); (b) make envy-freeness a relative notion, for instance by introducing an
acquaintance graph between agents (Chevaleyre et al., 2007b); or (c) keeping envy-freeness
as a strict criterion and relaxing efficiency. In all cases, new problems arise, the complexity
of which has to be identified.
Second, our results are mostly negative, since most of the interesting problems we studied are NP-hard (and often even worse), therefore, it would be worth pursuing work and
design practical algorithms for these problems. Most likely this would require coming up
with appropriate optimization criteria and then either (a) giving polynomial algorithms
that can approximate the desired objective (Lipton et al., 2004) or (b) implementing and
experimenting local search algorithms with relevant heuristics.
Third, throughout our paper it is assumed that everyones preferences are completely
known. In reality, presumably agents need to report their preferences, which introduces the
issue of strategic misreporting (manipulation). One direction for future research would be
to investigate how to prevent this, that is, mechanism design aspects.

Acknowledgments
We thank Michel Lematre for stimulating discussions about fair division and compact
representation; Thibault Gajdos for stimulating discussions about envy-freeness and for
pointing to us some relevant papers; Steven Brams, for giving us some feedback on an
earlier version of the paper and pointing to us some relevant references; and the participants
of the AgentLink Technical Forum Group on Multiagent Resource Allocation. This work
has been partly supported by the project ANR-05-BLAN-0384 Preference Handling and
Aggregation in Combinatorial Domains, funded by Agence Nationale de la Recherche.

References
Asadpour, A., & Saberi, A. (2007). An approximation algorithm for max-min fair allocation
of indivisible goods. Tech. rep., Department of Management Science and Engineering,
Stanford University, Stanford.
Baral, C. (2003). Knowledge Representation, Reasoning and Declarative Problem Solving.
Cambridge University Press.
Bogomolnaia, A., Moulin, H., & Stong, R. (2005). Collective choice under dichotomous
preferences. Journal of Economic Theory, 122, 165184.
Boutilier, C., & Hoos, H. H. (2001). Bidding languages for combinatorial auctions. In Proc.
of the 17th International Joint Conference on Artificial Intelligence (IJCAI-01), pp.
12111217, Seattle, Washington, USA.
562

fiEfficiency and Envy-freeness in Fair Division of Indivisible Goods

Bouveret, S., Fargier, H., Lang, J., & Lematre, M. (2005). Allocation of indivisible goods:
a general model and some complexity results. In Dignum, F., Dignum, V., Koenig, S.,
Kraus, S., Singh, M. P., & Wooldridge, M. (Eds.), Proceedings of AAMAS05, Utrecht,
The Nederlands. ACM Press.
Brams, S., & Fishburn, P. (1978). Approval voting. American Political Science Review,
72 (3), 831857.
Brams, S. J. (2008). Mathematics and Democracy: Designing Better Voting and FairDivision Procedures. Princeton University Press.
Brams, S. J., Edelman, P. H., & Fishburn, P. C. (2003). Fair division of indivisible items.
Theory and Decision, 55 (2), 147180.
Brams, S. J., Jones, M. A., & Klamler, C. (2007). Divide-and-conquer: A proportional,
minimal-envy cake-cutting procedure. Tech. rep., NYU Department of Politics.
Brams, S. J., & King, D. L. (2005). Efficient fair division: Help the worst off or avoid envy?.
Rationality and Society, 17 (4), 387421.
Brams, S. J., & Taylor, A. (1996). Fair Division: From Cake-Cutting to Dispute Resolution.
Cambridge Univ. Press.
Chevaleyre, Y., Dunne, P. E., Endriss, U., Lang, J., Lematre, M., Maudet, N., Padget, J.,
Phelps, S., Rodrguez-Aguilar, J. A., & Sousa, P. (2006). Issues in multiagent resource
allocation. Informatica, 30, 331. Survey paper.
Chevaleyre, Y., Endriss, U., Estivie, S., & Maudet, N. (2004). Multiagent resource allocation with k-additive utility functions. In Proc. DIMACS-LAMSADE Workshop on
Computer Science and Decision Theory, Vol. 3 of Annales du LAMSADE, pp. 83100.
Chevaleyre, Y., Endriss, U., Estivie, S., & Maudet, N. (2007a). Reaching envy-free states
in distributed negotiation settings. In Veloso, M. (Ed.), Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI-2007), pp. 12391244.
Chevaleyre, Y., Endriss, U., & Lang, J. (2006). Expressive power of weighted propositional
formulas for cardinal preference modelling. In Proceedings of the 10th International
Conference on Principles of Knowledge Representation and Reasoning (KR), pp. 145
152, Lake District, UK. AAAI Press.
Chevaleyre, Y., Endriss, U., & Maudet, N. (2007b). Allocating goods on a graph to eliminate
envy. In Proceedings of AAAI-07, pp. 700705.
Conitzer, V., & Sandholm, T. (2005). Communication complexity of common votiong rules.
In Proceedings of the EC-05.
Cramton, P., Shoham, Y., & Steinberg, R. (Eds.). (2005). Combinatorial Auctions. MIT
Press.
Demko, S., & Hill, T. P. (1998). Equitable distribution of indivisible items. Mathematical
Social Sciences, 16, 145158.
Dunne, P. E. (2005). Extremal behaviour in multiagent contract negotiation. Journal of
Artificial Intelligence Research, 23, 4178.
563

fiBouveret & Lang

Dunne, P. E., Wooldridge, M., & Laurence, M. (2005). The complexity of contract negotiation. Artificial Intelligence, 164 (1-2), 2346.
Endriss, U., Maudet, N., Sadri, F., & Toni, F. (2006). Negotiating socially optimal allocations of resources. Journal of Artificial Intelligence Research, 25, 315348.
Ford, L. R., & Fulkerson, D. R. (1962). Flows in Networks. Princeton University Press.
Gebser, M., Liu, L., Namasivayam, G., Neumann, A., Schaub, T., & Truszczynski, M.
(2007). The first answer set programming system competition. In Baral, C., Brewka,
G., & Schlipf, J. (Eds.), Proceedings of the Ninth International Conference on Logic
Programming and Nonmonotonic Reasoning (LPNMR07), Vol. 4483 of Lecture Notes
in Artificial Intelligence, pp. 317. Springer-Verlag.
Gottlob, G. (1992). Complexity results for nonmonotonic logics. Journal of Logic and
Computation, 2, 397425.
Herreiner, D. K., & Puppe, C. (2002). A simple procedure for finding equitable allocations
of indivisible goods. Social Choice and Welfare, 19, 415430.
Ieong, S., & Shoham, Y. (2005). Marginal contribution nets: a compact representation
scheme for coalitional games. In Proceedings of EC05.
Karp, R. M. (1972). Reducibility among combinatorial problems. In Miller, R. E., &
Watcher, J. W. (Eds.), Complexity of Computer Computations, pp. 85103, New York.
Plenum Press.
Lang, J. (2004). Logical preference representation and combinatorial vote. Annals of Mathematics and Artificial Intelligence, 42 (1), 3771.
Lipton, R. J., Markakis, E., Mossel, E., & Saberi, A. (2004). On approximately fair allocations of indivisible goods. In EC 04: Proceedings of the 5th ACM conference on
Electronic commerce, pp. 125131, New York, NY, USA. ACM Press.
Nisan, N. (2005). Bidding Languages for Combinatorial Auctions. MIT Press.
Papadimitriou, C. H. (1994). Computational complexity. AddisonWesley.
Rawls, J. (1971). A Theory of Justice. Harvard University Press, Cambridge, Mass.
Reiter, R. (1980). A logic for default reasoning. Artificial Intelligence, 13, 81132.
Robertson, J., & Webb, W. (1998). Cake-Cutting Algorithms. A.K. Peters.
Rothkopf, M., Pekec, A., & Harstad, R. (1998). Computationally manageable combinational
auctions. Management Science, 8 (44), 11311147.
Sandholm, T. (1998). Contract types for satisficing task allocation: I theoretical results. In
Proc. AAAI Spring Symposium: Satisficing Models.
Wagner, K. W. (1987). More complicated questions about maxima and minima, and some
closures of NP. Theoretical Computer Science, 51, 5380.
Wagner, K. W. (1990). Bounded query classes. SIAM Journal on Computing, 19 (5), 833
846.
Young, H. P. (1995). Equity in Theory and Practice. Princeton University Press.

564

fiJournal of Artificial Intelligence Research 32 (2008) 1-36

Submitted 05/07; published 5/08

Enhancing Cooperative Search with Concurrent Interactions
MANISTER @ BIU .013. NET. IL

Efrat Manisterski
Department of Computer Science,
Bar-Ilan University, Ramat Gan, 52900 Israel

SARNED @ CS . BIU . AC . IL

David Sarne
Department of Computer Science,
Bar-Ilan University, Ramat Gan, 52900 Israel

SARIT @ CS . BIU . AC . IL

Sarit Kraus
Department of Computer Science,
Bar-Ilan University, Ramat Gan, 52900 Israel

Abstract
In this paper we show how taking advantage of autonomous agents capability to maintain
parallel interactions with others, and incorporating it into the cooperative economic search model
results in a new search strategy which outperforms current strategies in use. As a framework for
our analysis we use the electronic marketplace, where buyer agents have the incentive to search
cooperatively. The new search technique is quite intuitive, however its analysis and the process of
extracting the optimal search strategy are associated with several significant complexities. These
difficulties are derived mainly from the unbounded search space and simultaneous dual affects of
decisions taken along the search. We provide a comprehensive analysis of the model, highlighting,
demonstrating and proving important characteristics of the optimal search strategy. Consequently,
we manage to come up with an efficient modular algorithm for extracting the optimal cooperative
search strategy for any given environment. A computational based comparative illustration of the
system performance using the new search technique versus the traditional methods is given, emphasizing the main differences in the optimal strategys structure and the advantage of using the
proposed model.

1. Introduction
Coalition formation is well recognized as a key process in a multi-agent systems, mostly desirable in
environments where a group of agents can perform a task more efficiently than any single agent can
(Lermann & Shehory, 2000). In recent years many coalition formation models have been suggested,
for various domains (Talukdar, Baerentzen, Gove, & de Souza, 1998; Dias, 2004), particularly for
electronic commerce (Tsvetovat, Sycara, Chen, & Ying, 2000; Yamamoto & Sycara, 2001; Sarne
& Kraus, 2005). In the latter context, the most common coalition is a coalition of buyers, derived
mainly from the potential of obtaining volume discounts (Tsvetovat et al.; Sarne & Kraus, 2003)
and the ability to search cooperatively for market opportunities in a more efficient manner (Sarne &
Kraus, 2005).

c
2008
AI Access Foundation. All rights reserved.

fiM ANISTERSKI , S ARNE , & K RAUS

The cooperative economic search1 incentive derives principally from the existence of search
costs found in MAS. These costs reflect the resources (not necessarily monetary) that need to be
invested or consumed while searching for opportunities in the environment (Sarne & Kraus, 2005)
(e.g., searching for an opportunity to buy a product in the context of the electronic marketplace).
The scenario of having search costs is common in MAS where the agent needs (for its decision
making process) immediate information concerning market opportunities. Given the richness of
opportunities and the dynamic and open nature of these environments, central mechanisms are usually incapable of supplying such information with the level of completeness and accuracy required
by the agent, certainly not without a cost. Thus the agent needs to spend some of its resources on
search related activities. Despite the reduction in the magnitude of these search costs in the electronic commerce era, the continuous growth in the number of retailers and virtual stores over the
Internet, followed by a phenomenal increase in the number of opportunities available, makes the
overall search cost an important parameter affecting buyers search strategy (Choi & Liu, 2000;
Kephart & Greenwald, 2002; Sarne & Kraus, 2005; Bakos, 1997).
In this context, the cooperative search offers the advantage of sharing, reusing and re-allocating
opportunities among the coalition members (Sarne & Kraus, 2005). For example, using cooperative
search agents can exploit opportunities which would have been discarded otherwise if each of the
agents would have conducted an alternative separate search. Nevertheless, the process of forming
and maintaining the coalition induces some overhead, derived mainly from the communication and
coordination activities (Sarne & Kraus, 2003), thus the coalition should set its search strategy in a
cost/effective manner.
A classic example of the above in traditional markets is the procurement management officer
of a corporation. Instead of having each individual in the cooperation spend time and resources
on locating its specific requested supplies, the task is delegated to the procurement management
officer. Here, in addition to the price discounts obtained for aggregated demands of identical items,
the procurement management officer becomes highly updated with the different offers and specific
supplies available by the different merchants in the markets. As a result the cost of locating the
best deal for each request becomes significantly smaller (in comparison to the equivalent search
conducted by each of the individuals).
The basic concepts by which a coalition should manage the cooperative search, including an
analysis and computational means for extracting its optimal search strategies, are given by Sarne
and Kraus (2005). Nevertheless, the assumption used in that model for constructing the coalitions
strategy is that the coalition interacts with one seller agent at a time. Such an assumption ignores
the inherent strength of autonomous agents, which is their capability to efficiently interact with
several other agents in parallel. This capability derives primarily from their improved communication capabilities and their ability to process an enormous amount of information in a short time
compared to people. In this paper, we take advantage of this capability and incorporate it into the
cooperative economic search model, supplying a comprehensive analysis of the resulting parallel
cooperative search variant. As we show throughout the paper, the parallel model weakly dominates
the existing sequential cooperative search model described by Sarne and Kraus: it has the potential
of significantly improving the searchers performance in various environments, and always guarantees reaching at least the performance of the existing cooperative search model. In particular, the
parallel interaction is preferable whenever an agents search cost is non-linear or combines fixed
1. As opposed to the classical AI search (Hart, Nilsson, & Raphael, 1968) in which an agent seeks a sequence of actions
that will bring it from an initial state to a desired goal state.

2

fiE NHANCING C OOPERATIVE S EARCH

WITH

C ONCURRENT I NTERACTIONS

components (e.g. operational costs), depending on the number of interactions maintained (e.g. advantage of size). In such cases, the adoption of the parallel technique by the coalition suggests a
reduction in the average cost per interaction with seller agents.
Moreover, the improvement achieved using the parallel technique increases when there is a finite decision horizon (i.e., whenever the coalition has a deadline for finishing its search). In addition
to the advantage of reducing the average cost per interaction, in finite horizon settings the coalition
benefits from the fact that it can increase the intensity of the search and, thus, scan more opportunities (in comparison to the sequential search model described by Sarne & Kraus, 2005) prior to
reaching the deadline.
While the integration of parallel interactions technique into a single agents search process is
quite intuitive, finding the optimal (overall utility maximization) strategy for the cooperative search
case is not trivial at all. The major difficulty derives from the fact that different coalition members
may have heterogeneous multi-attribute utility functions. Therefore, extracting the value encapsulated in future streams of opportunities is complex. We overcome this difficulty and present an
algorithm for extracting the coalition strategy. Our algorithm facilitates the calculation process of
the coalition strategy and it is polynomial in the number of parallel interactions. This is a significant improvement over the brute-force algorithm which is exponential in the number of parallel
interactions.
Similar to the model introduced by Sarne and Kraus (2005), we apply the multi-attribute utility
theory (MAUT) (Keeney & Raiffa, 1976) to analyze preferences with multiple attributes in our agent
based search mechanism. This enables a set of preferences to be represented by a numerical utility
function. We consider the agents to be heterogeneous, i.e. each having its own utility function. The
model is general, though we emphasize several specific implementation aspects relating to the B2C
market (businesses selling products or services to end-user consumers), where sellers can supply
almost any demanded volume, and to the C2C market (transactions between consumers), where
sellers offer single items for sale. Based on the proposed analysis, the coalition can calculate its
optimal strategy given the utility functions of the coalition members and the specific environment
in which it operates.
Three basic stages are common to all coalition formation models (Sandholm, Larson, Andersson, Shehory, & Tohme, 1999; Tsvetovat et al., 2000): coalition structure generation (where the
agents form/join the coalition), executing the coalition task, and dividing the generated value among
the coalition members. Among these three stages our focus is on finding the optimal search strategy
for the coalition, given its structure and the opportunity distribution. As suggested by Sarne and
Kraus (2005), the coalition operates in its environment alongside many other coalitions differing in
their size, their members utility functions and the products they are seeking. These other coalitions,
as well as the different individual utility functions play an important role when studying the stability
of a coalition and the issue of revealing the true utility function (truth telling). The analysis of these
important issues is based on the ability to properly derive the coalitions utility given any specific
self structure and the environment within which it operates. 2 This paper aims to supply this functionality, laying the foundation and enabling research of many of the important aspects of coalition
formation given above in the context of cooperative search (truth telling, stability, payoff division,
etc.).
2. The utility considered is the agents reported, not necessarily true, utility function, since the goal is to extract the
optimal strategy for a given input.

3

fiM ANISTERSKI , S ARNE , & K RAUS

The main contributions of this work are fourfold: First, we formally model and analyze the
parallel cooperative search problem of agents operating in a costly environment. The parallel cooperative search model is a general search model and can be applied in various domains in addition
to the electronic marketplace that is used as a framework for our work. Second, we show that in
many environments the parallel cooperative search outperforms the previous search strategies (either
when each agent searches by itself or when using a cooperative sequential search). Furthermore,
we draw attention to scenarios where sequential cooperative search is proven to be non-beneficial,
however parallel cooperative search is a favorable technique. Third, we supply an algorithm that
facilitates the calculation of the coalitions optimal strategy, and significantly reduces the complexities associated with the attempt to extract this strategy from an appropriate set of equations. Finally
we provide a comprehensive analysis of the parallel model for a finite decision horizon. We draw
attention to the significant improvement that can be achieved by integrating the parallel technique
into cooperative search in the finite decision horizon.
The rest of the paper is organized as follows. Section 2 reviews the related work, emphasizing
the uniqueness of the proposed parallel cooperative search model. The model description and its
underlying assumptions are given in section 3. In section 4, we formally describe the performance
of the coalition when using the parallel cooperative search as a function of the strategy used and
present the complexities associated with extracting the optimal search strategy. By exploring the
unique characteristics of the coalitions optimal strategy when using the cooperative parallel search
we manage to overcome this computational complexity. This process is described in section 5. Consequently, we present an efficient algorithm for extracting the optimal cooperative search strategy.
Some interesting properties of the new search model, with regard to the market in which it takes
place, are illustrated in section 6. In section 7, a finite decision horizon variant of the model is
discussed. The parallel cooperative search performance and its advantages over the current search
models, both in infinite and finite decision horizons, are illustrated in section 8. Finally, we conclude
with a discussion and suggested future research directions in section 9.

2. Related Work
In many scenarios autonomous agents in multi-agent environments may cooperate in order to perform tasks. The need for cooperation may arise either when an agent is incapable of completing a
task by itself or when operating as a group can improve the overall performance (Breban & Vassileva, 2001; Lermann & Shehory, 2000; Tsvetovat et al., 2000). Group based cooperative behavior
can be found in various domains, such as solving complex optimization problems (Talukdar et al.,
1998), military and rescue domains (Dias, 2004), e-business applications (Tsvetovat et al., 2000;
Yamamoto & Sycara, 2001) and many more. The recognition of the advantages encapsulated in
teamwork and cooperative behaviors, is the main driving force of many coalition formation models
in the area of cooperative game theory and MAS (Lermann & Shehory, 2000; Li, Rajan, Chawla, &
Sycara, 2003; Shehory & Kraus, 1998). Many examples from the the extensive literature on coalition formation can be found in books and journals (Kahan & Rapoport, 1984). In the electronic
market domain, most authors focus on coalitions formed to obtain volume discounts Tsvetovat
et al., Yamamoto and Sycara. Additional coalition formation models for the electronic marketplace
consider extensions of the transaction-oriented coalitions into long-term ones Breban and Vassileva,
and for large-scale electronic markets Lermann and Shehory.

4

fiE NHANCING C OOPERATIVE S EARCH

WITH

C ONCURRENT I NTERACTIONS

Traditionally, the majority of this research effort has focused on issues concerning the optimal
division of agents into disjoint exhaustive coalitions (Sandholm et al., 1999; Yamamoto & Sycara,
2001), division of coalition payoffs Yamamoto and Sycara and enforcement methods for interaction
protocols. Only a few authors have considered the coalitions problem of determining its strategy
in the electronic commerce domain, once the coalition is formed (Ito, Ochi, & Shintani, 2002).
Nevertheless, other than a single exception (Sarne & Kraus, 2005), none of the proposed models
have considered a coalition search in a costly environment, and in particular none of them (including
Sarne & Kraus, 2005) have made use of its capabilities to maintain parallel interactions.
The problem of a searcher operating in a costly environment, seeking to maximize his long term
utility is addressed in classical search theory (Lippman & McCall, 1976; McMillan & Rothschild,
1994, and references therein). There are three main search models that can be found in the literature.
The first search model is the Fixed Sample Size (FSS) model, introduced by Stigler (1961). In this
model the searcher first chooses the sample size and then draws a single sample where all observations are made simultaneously. The second model is the Single Agent Sequential Search (SASS)
strategy (Rothschild, 1974; Lippman and McCall). In this model the searcher draws exactly one observation at a time. Based on the value of the observations drawn till that time, the searcher decides
whether to draw another observation. His decision depends upon what he observed. Attempts to
adopt the sequential search model for agent-based electronic trading environments associated with
search costs are suggested by Choi and Liu (2000), and Kephart and Greenwald (2002), though the
main focus is on establishing the appropriate characteristics of the environment and search strategy
rather than the computational aspects of extracting it. The last search method, Single Agent Parallel Search (SAPS) (Benhabib & Bull, 1983; Gal, Landsberger, & Levykson, 1981; Morgan, 1983;
Morgan & Manning, 1985), encompasses the above search models as special cases. In this model
the searcher may choose both the number of samples taken and the sample size in each period. This
latter method, which outperforms the other two, is in fact the single agents equivalent to our parallel
cooperative search model considered in this paper. Nevertheless, search theory has mainly focused
on a single searcher, looking for a single opportunity, either as a one sided (taking the environments
reaction to the search strategy used by the agent to be static) or two sided (as a matching model,
analyzed from the equilibrium perspective) model. The analysis of a cooperative search is lacking.
This, is in-spite of the fact that cooperative search has been proven (Sarne & Kraus, 2005) to be
inherently different from a single agents search in relation to its complexity, strategy structure and
solution methodology.
Several extensions of economic search theory have been suggested for the case of a consumer
searching for multiple different commodities, while facing imperfect information about prices (Gatti,
1999; Carlson & McAfee, 1984; Burdett & Malueg, 1981). Here, we can find different variants
where the consumer visits one or more stores in order to minimize the total expenditure. Nevertheless, the attempt to adjust the proposed methods suggested in these models to support our parallel
cooperative search process results in a solution complexity that is exponential in the number of parallel interactions. In contrast our algorithm for extracting the optimal search strategy is polynomial
in the number of parallel interactions.

3. The Parallel Cooperative Search Model
We base our model description and formulation on the definitions given by Sarne and Kraus (2005)
and extend them to reflect the agents parallel search capabilities. We consider an electronic mar5

fiM ANISTERSKI , S ARNE , & K RAUS

ketplace where numerous buyer and seller agents can be found. Each agent is interested in buying
or offering to sell a well defined product. A product can be offered by many different seller agents
under various terms and policies (including price). We assume that while buyer agents are ignorant
of individual seller agents offers, they are acquainted with the overall distribution of opportunities
(where an opportunity is defined as the option to buy the product under specific terms and policies)
in the marketplace.
Assuming there are no central mechanisms or mediators which can supply the agents with full
immediate information concerning current market opportunities, each agent needs to search for
appropriate opportunities to buy its requested product. Throughout the search the buyer agents
locate seller agents and learn about their offers by interacting with them. Each buyer agent evaluates
opportunities using its own multi-attribute utility function. Buyer agents may have heterogeneous
preferences and thus the utility from a given opportunity differs according to the evaluating buyer
agent.
In its most basic form, each buyer agent interacts with several sellers in parallel at each stage of
its search thus learns about a new set of opportunities. Based on the agents evaluation of the utility
that can be gained from each opportunity in the set, the agent makes a decision whether to exploit
any of the opportunities it encountered throughout its search (i.e. buy from any of the sellers) or
resume its search in a similar method. A decision to resume the search is always accompanied by
the number of parallel interactions to be conducted next.
The search activity is assumed to be costly (Choi & Liu, 2000; Kephart & Greenwald, 2002;
Sarne & Kraus, 2005; Bakos, 1997). For each search stage in which the buyer agent locates, interacts
and evaluates seller agents, the process induces a search cost. This cost is a function of the number
of parallel interactions initiated and maintained by the agent. The search cost structure is principally
a parameter of the markets liquidity and volatility, and thus it is assumed to be shared by all buyer
agents operating in the specific marketplace. Recognizing the benefits of a cooperative search,
buyer agents, interested in similar products or interchangeable products, may form coalitions (Sarne
& Kraus, 2005). There are various methods by which the coalition members can coordinate their
cooperative search (e.g. assign a representative agent to search on behalf of the coalition or simply
take turns searching), each deriving a different search cost overhead structure. The coalitions search
costs are assumed to increase both as a function of the number of parallel interactions it forms
and the number of buyer agents in the coalition. 3 We assume a buyer agents utility from a given
opportunity may be interpreted into monetary terms. Thus the utilities are additive and the total
search utility can be obtained by subtracting the search cost that the process induces from this
value.
As part of its search process, the coalition needs to set a strategy for determining, given any set
of known opportunities, whether to terminate or resume its search. In the latter case, the coalition
also needs to determine the number of parallel interactions to be used in the next search round.
The optimal strategy is the one maximizing its expected total search utility (opportunities utility
minus search costs). As discussed in detail in the cooperative search model (Sarne & Kraus, 2005),
given the option of side-payments the overall utility maximization strategy taken by the coalition
is always the preferred one by all coalition members (i.e. no conflict of interests), regardless of
the pre-set coalitions payoff division protocol. Given the coalitions goal of maximizing the overall coalition utility, its decision is not influenced by the payoff division protocol, nor by coalition
3. The reason for correlating the coalitions search cost with the number of coalition members is mainly associated with
some coordination overhead (Sarne & Kraus, 2005).

6

fiE NHANCING C OOPERATIVE S EARCH

WITH

C ONCURRENT I NTERACTIONS

stability considerations, but rather influences these two factors (Sarne & Kraus, 2005). Any of the
agents pre-determined portion of the coalitions utility will increase in its absolute value along with
the increase of the net coalition utility, thus the overall utility maximization strategy is the preferred
strategy by all agents at every stage of the search.

4. Parallel Cooperative Search (PCS) Analysis
The following section formally defines the search environment and the coalitions search strategy.
For convenience, all the notations given, and their meanings, are summarized in a table at the end
of the paper.
Let B = (B1 , B2 , ..., Bk ) be the set of the attributes defining any of the potentially available
opportunities in the market, where each attribute B i can be assigned a value from the finite set
(bimin , ..., bimax ). An opportunitys type is defined by the vector ~o i = (b1 , b2 , ...bk ), assigning a value
bi to each specific attribute Bi .4 We use O p to denote the space of potential opportunity types the
coalition may encounter. The opportunity types distribution in the marketplace is denoted by the
probability function p(~o), ~oO p p(~o) = 1. We consider a coalition A = {a 1 , a2 , ..., a|A| } of a general
size, where a j is the j th buyer agent in the coalition. Each buyer agent, a j , evaluates opportunities
using a utility function U j : O p  R, where U j (~o) is the agents utility from opportunity type ~o. We
denote the search cost associated with having a coalition of n agents maintaining w simultaneous
interactions with seller agents over a search round by the function c(w, n).
Let  be the collection of all possible sets of opportunities in the environment in which the
agents reside. Given a set of known opportunities  known   the coalition needs to determine
its strategy (whether to terminate/resume the search and the number of parallel interactions in the
later case). Similar to the analysis suggested for the Sequential Cooperative Search (SCS) model
(Sarne & Kraus, 2005) we can reduce the large number of world states in which a coalition can
be, by adopting a representation of states through sets of effective known opportunities. For that
purpose we consider a function alloc :   O np that maps a given set of opportunities  to the
coalition members in A (i.e. an allocation) in a way that the aggregated agents utility when using
this allocation is maximized.5 In B2C markets the same opportunity may be allocated to more than
one agent, while in C2C markets each opportunity is restricted to only one agent. Given a coalition

/ to represent the allocation resulting from applying
A, we use alloc() = (~y1 , ...
yn ), ~yi  (  {0})
the function alloc over the set , where ~y i denotes the opportunity associated with agent a i (yi = 0/
denotes that no opportunity was allocated to agent a i ).
The computation method used by the function alloc is market-dependent. While in B2C markets
the function assigns each agent the opportunity that maximizes its utility, ~y j  = arg max~y U j (~y), j =
1, ..., n, in C2C markets alloc can be computed by solving a maximum weighted matching in a
bipartite graph (Avis & Lai, 1988). Specifically for a set of opportunities    found in the C2C
market we construct a graph G = (V1 ,V2 , E), where each vertex of V1 corresponds to an agent in A
and each vertex in V2 corresponds to an opportunity ~o  . An edge connects an agent a j in V1 and
an opportunity ~o in V2 (each member in the two groups has edges connecting it to all the members
4. Notice that ~o is noted as a vector since it assigns a specific value to each of the different attributes, terms and
conditions associated with a specific opportunity. For example, a specific opportunity to buy a calculator can be
represented by the vector ~o = (scienti f ic, 20$, small display, pocket, 1Y R warranty).
5. If there is more than one allocation that maximizes the overall coalition utility then the function alloc chooses one of
them according to a pre-defined order.

7

fiM ANISTERSKI , S ARNE , & K RAUS

of the other group). The weight of such an edge is the utility for agent a j from opportunity ~o, U j (~o).
Here alloc() = (~y1 , .., y~n ), where {(a1 , y~1 ), .., (an , y~n )} is a maximum weighted matching in G  . To
illustrate the computation used by the function alloc, we use the following example:
Example 1. Consider the following environment:
Environment 1. There are three agents, a 1 , a2 and a3 , searching for a product (e.g., memory chip)
characterized by two attributes, B 1 (e.g., quality) and B2 (e.g., store rating). Each attribute can have
either the value 1 or 2, with an equal probability of 1/2. This means that there are four possible
opportunities ~o1 = (1, 1) (both attributes values are equal to 1), ~o 2 = (1, 2), ~o3 = (2, 1), and ~o4 =
(2, 2). The utility function of agents a 1 , a2 and a3 are U1 (~o) = 9B1 + B2 , U2 (~o) = 4B1 + 5B2 , and
U3 (~o) = B1 + 10B2 , respectively. Table 1 summarizes the environments setting.
Opportunity
~o1
~o2
~o3
~o4

(Attribute1,
Attribute2)
(1,1)
(1,2)
(2,1)
(2,2)

Probability
Agent a1
10
11
19
20

1
4
1
4
1
4
1
4

Utility
Agent a2
9
14
13
18

Agent a3
11
21
12
22

Table 1: Agents utilities for the four opportunities in Environment 1
Assume the coalition has already interacted with 4 sellers, encountering two opportunities of
type ~o1 and two single opportunities of types ~o 3 and ~o4 . Here, the set of known opportunities is
known = {~o1 ,~o1 ,~o3 ,~o4 }.
We first calculate alloc(known ) for the coalition operating in the B2C market. In this market
we assume that sellers can supply any demanded volume. Therefore the allocation that maximizes
the coalitions overall utility is assigning each agent the opportunity that maximizes its utility over
known . Since opportunity ~o4 maximizes the utility of each of the agents, we obtain alloc( known ) =
(~o4 ,~o4 ,~o4 ).
In C2C markets the above allocation is impossible, since opportunity ~o 4 can be assigned only to
one of the agents (each seller offers a single item for sale). In this case the optimal allocation can
be calculated by solving a maximum matching problem resulting in the assignment alloc( known ) =
(~o3 ,~o1 ,~o4 ).
Given a set of known opportunities  known , we can use the function alloc and its consequent
allocation alloc(known ) to calculate the immediate utility of the coalition if it terminates the search
at the current time point. This utility, defined as the aggregated agents utility when each of the
agents is allocated according to allocation alloc( known ), is denoted by Vt (known ) (abbreviation for
Vterminate ) and can be calculated using:
n

Vt (known ) =

 U j (~y j )

j=1

/ = 0,  j.
where alloc(known ) = (~y1 , ...~yn ) and U j (0)
8

(1)

fiE NHANCING C OOPERATIVE S EARCH

WITH

C ONCURRENT I NTERACTIONS

Notice that up to this point, the world state space upon which the coalition defines its strategy
is defined by the set of opportunities  known known to the coalition. This space is potentially very
large. In order to reduce the strategys space, we introduce the concept of equivalence between
different sets of opportunities within the context of cooperative search. We consider two sets of
opportunities 0 , 00   to be equivalent (0  00 ), if the following hold: (a) Vt (00 ) = Vt (0 ); and
(b) Vt (0  ) = Vt (00  ) for any set of opportunities    that the coalition may encounter in the
future. For any two equivalent sets  0 , 00 , the coalition is indifferent to knowing the opportunities
in 0 or the opportunities in 00 . This is because any set of opportunities the coalition will encounter
in the future results in a similar utility, thus the coalitions overall utility will be the same in both
cases. Moreover, since the coalitions decisions are merely determined by its overall utility and
in both cases similar utilities are reached with similar probabilities, the coalition uses the same
search strategy (either terminates the search or uses the same number of parallel interactions in the
subsequent search stage) for both opportunities sets.
Notice that according to the definition above equivalent is a transitive relation ( 0  00 , 00 
000
  0  000 ). Moreover, 0  00 implies that (0  )  (00  ),   . Given an allocation
` = (~y1 , ..., y~n ) of a set , we use {`} to denote the set of opportunities that appear in `.
Similar to former cooperative search models (Sarne & Kraus, 2005; Manisterski, 2007) the
following theorem holds in our cooperative search model (and can be proven in a similar manner).
Theorem 1. Any set of opportunities  is equivalent to the set of opportunities returned by the
function alloc(). Formally stated:   {alloc()}.
Theorem 1 enables us to reduce the set of known opportunities that affect the coalitions strategy.
The immediate implication of the Theorem 1 is that the coalitions strategy is affected only by the
subset of known defined by {alloc(known )}. Thus the coalition does not need to keep all its known
opportunities. It can reduce its set of known opportunities with which it determines its strategy
to a subset, s. Given this result, we define state to be the set s of opportunities that are members
of {alloc(known )}. Formally, we can calculate the state s of a coalition A acquainted with a set
known of known opportunities by using the function s = state( known ) = {alloc(known )}. We use
SA to denote the set of all possible states of a coalition A. This definition significantly simplifies our
analysis and enables the coalition to calculate its optimal strategy exclusively based on the sets of
opportunities that are in SA . The following example illustrates the computation of a state.
Example 2. Consider the environment and the set of known opportunities described in Example
1. As computed in Example 1, the allocation that maximizes the coalitions overall utility in the
B2C market is alloc(known ) = (~o4 ,~o4 ,~o4 ). Thus the coalitions current state in the B2C market is
state(known ) = {~o4 }. As a result the coalition can ignore the other opportunities it encountered
{~o1 ,~o1 ,~o3 } and its strategy (terminate or resume the search and the number of parallel interactions
in the later case) is the same as its strategy when  known = {~o4 }. Similarly, the coalitions state in
the C2C market includes the opportunities in alloc( known ) thus state(known ) = {~o3 ,~o1 ,~o4 }.
The coalitions transition from one state to another during the search in the B2C and the C2C
markets can be described as a directed acyclic graph (DAG). The vertices of this graph present
all potential coalition states. A directed edge (s, s 0 ) connects two states, s and s0 , if there is an
opportunity ~o  O p that changes the current coalitions state from s to s 0 (i.e ~o, s.t state(s ~o) = s0 ).
To better understand the use of the DAG in these markets, we use the following two environments:
9

fiM ANISTERSKI , S ARNE , & K RAUS

Environment 2. There are two agents a 1 and a2 searching for a product (e.g., a computer mouse)
in a B2C market associated with 3 types of opportunities (e.g., 3 models): o~ 1 , o~2 and o~3 . Table 2
summarizes the environments setting.
Opportunity
~o1
~o2
~o3

Utility
Agent a1 Agent a2
5
10
10
5
20
21

Table 2: Agents utilities for the three opportunities in Environment 2
Environment 3. There are two agents a 1 and a2 searching for a product (e.g., a used book) in a
C2C market associated with two types of opportunities (e.g., English edition and American edition):
o~1 and o~2 . Table 3 summarizes the environments setting.
Opportunity
~o1
~o2

Utility
Agent a1 Agent a2
5
10
10
5

Table 3: Agents utilities for the two opportunities in Environment 3
Figure 1(a) and Figure 1(b) show the DAG of states for Environment 2 (the B2C market) and
Environment 3 (the C2C market) described below, respectively. Notice that it is possible to have
opportunities that do not change the coalitions current state (since these opportunities do not increase the coalitions overall utility). To simplify the graph we did not mark these opportunities. It
is notable that in the C2C market all sets that include up to n opportunities are feasible states (state
s is feasible if it belongs to SA , i.e. there is a set  such that state() = s). This does not hold in
the B2C market. For example, the set {~o 1 ,~o3 } is not a feasible state since ~o3 maximizes all agents
utility. Thus if the coalition encounters this opportunity all other opportunities can be ignored.
Any coalition A that reaches a state s along its search can change its state to s 0 only if there is a
sequence of directed edges from state s to state s 0 . The coalition can conduct parallel interactions,
thus can transition within a single search round to a state s 0 which is not directly connected to
state s. For example, in Environment 3, when in state s = {} the coalition can change its state to
s0 = {~o1 ,~o2 } after one search round (even though there is no a directed edge between them). This
can happen when the coalition conducts two or more parallel interactions in which it encounters
opportunities ~o1 and ~o2 . The transition to a new state suggests that in the new state the coalition has
a termination utility equal to or higher than the utility at its current state.
We define a strategy as a function x : S A  N, where x(s) = 0 if the agent decides to terminate
its search; otherwise x(s) is the number of parallel interactions the coalition should maintain next,
when in state s. We denote the optimal strategy by x  .
We define V (s, w) as the coalitions expected utility when using w parallel interactions when
in state s (assuming any search decision taken at a future state s 0 6= s will make use of the optimal
number of parallel interactions). The term V (s, 0) denotes the immediate utility obtained, if the
10

fiE NHANCING C OOPERATIVE S EARCH

{o1}

o1
{o1,o2}

o3

{}

o1

o2

o2
o3

C ONCURRENT I NTERACTIONS

o3

{}

o1

WITH

o2

{o1}

{o2}

{o2}

o1
o3

o1

o2

{o1,o1}
o2

{o3}

(a)

o2
{o2,o2}

{o1,o2}

o1

(b)

Figure 1: States Diagram of (a) Environment 2 (B2C market); (b) Environment 3 (C2C market).

coalition decides to terminate the search at state s, thus: V (s, 0) = Vt (s). The value w (w  N,
w  0) that maximizes the coalitions expected utility V (s, w), is equal to x  (s):
x (s) = arg max V (s, w)
w

(2)

The coalitions expected utility from this point onwards when using its optimal strategy, denoted
V  (s), can be expressed as:
V  (s) = maxwV (s, w) = V (s, x (s))

(3)

In order to formulate the appropriate equation for V (s, w) (from which x  (s) and V  (s) can be
derived) we make use of several additional notations and definitions. Consider a search round in
which the coalition interacts simultaneously with w seller agents, yielding a set  w = {~
o1 , ..., o~w },
~oi  O p of opportunities. Let w be the collection of all w-sized sets of opportunities that can be
produced in the environment in which the coalition operates. We denote by p w (w ) the probability
of encountering a specific set of opportunities  w , when maintaining w random interactions with
seller agents.
Similar to the basic cooperative search (Sarne & Kraus, 2005) we divide the w-sized opportunities space, w , into two sub-spaces, containing improving and non-improving w-sized sets of opportunities for the coalitions utility, respectively. Nevertheless, this definition needs to be extended
to fit the scenario of parallel search as follows. Given the number of simultaneous interactions,
w, and a state s, let simprovew be the collection of all w-size sets of opportunities,  w , that change
the coalitions current state (formally stated as:  simprovew = {w |w  w and state(s  w ) 6= s}).
We denote the complementary set of  simprovew by sstayw (the set that includes all w-size sets of
opportunities w that do not change the coalitions current state).
Therefore when the coalition encounters the set of opportunities  w , we can distinguish two
possible scenarios:
(1) w belongs to sstayw and the coalitions current state is still s. In this case the coalitions future
expected utility (i.e., from this point on) remains V (s, w). This is derived from the stationary nature 6
6. Stationary strategy is a strategy in which each player chooses the same moves in every structurally equivalent subgame
(Baron & Ferejohn, 1989).

11

fiM ANISTERSKI , S ARNE , & K RAUS

of the problem - if no better state has been reached, the search resumes using the same number of
parallel interactions w, yielding the same expected utility.
(2) w belongs to simprovew and the coalitions current state changes to s 0 = state(s  w ) 6= s. Since
we assume that the coalitions decision taken at a future state s 0 6= s will make use of the coalitions
optimal strategy x (s0 ), the coalitions expected utility can be expressed as V  (state(s  w )).
By using the above analysis, the expected utility when using w parallel interactions while in
state s, V (s, w), can now be expressed as (w > 0):
V (s, w) =c(w, n)+



pw (w )V
w simprovew



(s0 )+



pw (w )V (s, w)
w sstayw

(4)

where s0 is the coalitions new state after it encounters a set of opportunities  w , s0 = state(s  w ).
After applying some basic mathematical manipulations on the above term we obtain:
V (s, w) =

c(w, n) + w simprove pw (w )V  (s0 )
w

1  w sstayw pw (w )

(5)

Since 1  w sstayw pw (w ) = w simprove pw (w ) we obtain:
w

V (s, w) =

c(w, n) + w simprove pw (w )V  (s0 )
w

w simprovew pw (w )

(6)

Notice that in the case where no new better state can be reached, the denominator becomes zero, and
V (s, w) = . This is quite straightforward since the coalition maintains an endless costly search
(trying to reach a better state that actually does not exist). Here, inevitably the coalitions optimal
strategy is to terminate the search. This important characteristic is used later to design the proposed
algorithms for extracting the optimal search strategy.
At this point, one may attempt to compute the coalitions strategy x  by solving a set of equations
of types 1, 3 and 6. Nevertheless, this straightforward solution approach is accompanied by many
inherent complexities, derived from the structure of the equations. First, notice that Equation 6
is a recursive equation and one needs to know the optimal strategy taken in future states s 0 when
extracting the optimal strategy of a given state s. Second, the computation of V (s, w) in Equation
6 is exponential in the number of parallel interactions, w, used (affecting the number of sets in
simprovew , both in the denominator and numerator). Last, according to the above formulation, the
potential number of parallel interactions that may be used is not bounded 7 , thus reaching a local
maximum does not guarantee that a higher utility cannot be obtained. Therefore, an algorithmic
approach that can reduce the complexity of extracting the optimal cooperative parallel search is
favorable.

5. Algorithmic Approach
In this section we present a comprehensive analysis of the problem, emphasizing some unique
characteristics of the coalitions optimal strategy. These findings lead to an algorithm for computing
V (s, w) with a polynomial complexity in the number of potential interactions, w (which is the key
component for computing x (s) and V  (s)).
7. The number of opportunities is theoretically infinite due to the high arrival rate of new opportunities as derived from
such a dynamic environment.

12

fiE NHANCING C OOPERATIVE S EARCH

WITH

C ONCURRENT I NTERACTIONS

5.1 Reducing Calculation Complexity
Recall that when attempting to solve the problem as a set of equations (see section 4) the potential
number of parallel interactions that may be used is unbounded. Nevertheless, in order to extract
x (s) it is essential to supply the coalition with an upper bound, w smax , for the optimal number of
parallel interactions to be used when in state s. In order to overcome this difficulty and to suggest
an upper bound for wsmax we make use of the following notation. We use S s = (s1 , ...s|SA | ) to denote
the states constituting SA , sorted by their termination utilities Vt (s)8 , where s1 is the state with the
highest expected utility Vt (s) in SA .
The following proposition suggests an efficient upper bound for x  (s).
i
i
Proposition 1. For each state si an upper bound wsmax
, to x (si ) can be calculated using wsmax
= dwe,
where w is the solution to the following equation:

(7)

c(w, n) = Vt (s1 ) Vt (si )

i
The suggested bound is valid simply because for every value of w greater than w smax
the search cost
associated with the following immediate search round c(w, n) is greater than any possible future
improvement in the coalitions utility Vt (s1 ) Vt (si ) (since the maximum additional utility that the
coalition can gain is bounded by the difference between the coalitions overall maximum utility
Vt (s1 ) and its immediate utility from its current state Vt (si )). Later on, we show that the above upper
bound value is a byproduct of the main loop in our proposed algorithm, thus it does not even need
to be directly calculated.
Having an upper bound for x (s) is an important step towards a solution, however the calculation
of V (s, w) (from which x (s) can be derived) is still exponential in the number of parallel interactions
used, w. Our analysis, which is based on the restructuring of the different elements composing
V (s, w), allows us to bypass this complexity through the introduction of a finite algorithm with a
polynomial computational complexity in w that will inevitably identify the optimal strategy for the
coalition.
In order to reduce the complexity of computing the coalitions best strategy, we make use of the
following notations and definitions:

 We use pstay (s, w) to denote the probability the coalition will remain in the same state s after
conducting w parallel interactions. This can be calculated as the probability of none of the
encountered w opportunities changing the coalitions state:
pstay (s, w) = (pstay (s, 1)) = (
w



p(~o))w

(8)

{~o}sstay1

The term 1(pstay (s, 1))w can now be used as a better structured representation of the element
w simprovew pw (w ) in Equation 6.
 We use V new (s, k) to denote the coalitions expected utility obtained from potentially reaching
new states (e.g. different than s) after maintaining k parallel interactions, while using the
optimal strategy x (s0 ) for each new future state s0 . The term V new (s, k) does not take into
account the cost associated with the current k interactions. However it does consider the
8. If there are several states with equal utility they are sorted according to a pre-defined order.

13

fiM ANISTERSKI , S ARNE , & K RAUS

search costs associated with any further search stages, originating in new states. Notice that
V new (s, k) is equal to zero if the coalition remains in state s after k interactions. The term
V new (s, k) can be expressed as:
(
)
 (state(s   )) k > 0
s
p
(
)V

k
k improve k k
k
V new (s, k) =
.
(9)
0
k=0
Note that V new (s, w) is actually one of the elements in Equation 6. Therefore, Equation 6 can
now be formulated as (w > 0):
V (s, w) =

c(w, n) +V new (s, w)
1  pstay (s, 1)w

(10)

The calculation of V new (s, w) using Equation 9 is still exponential in the number of the parallel
interactions. In order to efficiently compute V new (s, w) in Equation 10 we consider the w simultaneous interactions as w sequential interactions, associated with no search costs. This fully complies
with the definition of V new (s, w) as given above (as the cost of the w interactions is already considered). The justification for the above representation method is given in the Lemma 1 which follows
directly from Theorem 1.
Lemma 1. A new state reached by obtaining a new set of opportunities is equivalent to a state
reached by sequentially obtaining pairwise disjoint subsets of this set. Formally stated, given a set
/ i 6= j, 1w1  ...  rwr = w and a
w and any number of subsets 1w1 , ..., rwr , iwi  w , iwi  wj j = 0,
state s, then the following holds:
r
state(s, w )  state(state(...state(state(s  1w1 )  2w2 )...  r1
wr1 )  wr )

(11)

Proof. We begin by proving the following supporting lemma:
Lemma 2. Let 0 , 00   be two sets of opportunities. If  0  00 then   , state(0  ) 
state(00  ).
Proof. Since 0  00 , then from the definition of the equivalence relation it follows that  0   00 
. From Theorem 1 and the state definition it follows that  0    {alloc(0  )} = state(0  )
and 00    {alloc(00  )} = state(00  ). From the transitive characteristic of the equivalence
relation it follows that state(0  )  state(00  ).
Let s be a state and let w be a set of opportunities. We prove Lemma 1 by induction on the
number of disjoint sets r. For r = 2, let  0w0 and 00w00 be any sets that satisfy 0w0  00w00 = w and
/ From Theorem 1 and the state definition we obtain:
0w0  00w00 = 0.
s  0w0  {alloc(s  0w0 )} = state(s  0w0 )

(12)

From 12 and Lemma 2 it follows that
state(s  w ) = state(s  0w0  00w00 )  state(state(s, 0w0 )  00w00 )

(13)

We assume that for any number of disjoint sets r  M and r > 2,  1w1 , ..., rwr , iwi  w , iwi  wj j =
/ i 6= j, 1w1  ...  rwr = w Equation 11 holds and we prove that Equation 11 holds for r=M+1.
0,
14

fiE NHANCING C OOPERATIVE S EARCH

WITH

C ONCURRENT I NTERACTIONS

Given w and sets 1w1 , ..., rwr , we can decompose w to two disjoint sets wwr and rwr , where
wwr = w \ rwr . Therefore as we proved for r = 2 the following holds:
state(s  w )  state(state(s, wwr )  rwr ).

(14)

In addition we can decompose the set  wwr to r  1  M sets 1w1 , ..., r1
wr1 . Therefore using the
induction assumption:
state(s  wwr )  state(...state(state(s,  1w1 )  2w2 )...  r1
wr1 )

(15)

From Lemma 2 and 15 we obtain:
r
state(state(s  wwr )  rwr )  state(state(...state(state(s   1w1 )  2w2 )...  r1
wr1 )  wr )

(16)

From 14, 16 and the transitive characteristic of the equivalence relation it follows that:
r
state(s  w )  state(state(...state(state(s   1w1 )  2w2 )...  r1
wr1 )  wr )
A specific case of the above Lemma 1 is when each subset consists of a single opportunity. Thus
the calculation of V new (s, k) can recursively rely on the values V new (s0 , k  1), where s0 represents
any of the new states reached after obtaining one additional opportunity from O p . Therefore in
order to compute V new (s, k), we merely consider the expected utility after conducting one interaction
from the planned k interactions. Here, with a probability of p stay (s, 1) the coalition remains in the
same state s, where the expected utility (having k  1 additional interactions to go) is V new (s, k  1)
(recall that V new (s, 0) = 0 according to Equation 9). Otherwise, if a new state s 0 is reached after
a single interaction, then there is the possibility of reaching further new states in the remaining
k  1 interactions (taking these states utility into consideration by the term V new (s0 , k  1)) or with a
probability of pstay (s0 , k  1) remaining in the new state s0 even after the additional k  1 interactions
(in which case the utility is the one obtained by resuming the search from this state and on using the
optimal strategy, V  (s0 )). The above description is encapsulated in the following recursive equation:
V

new

(s, k) =

(

pstay (s, 1)V new (s, k  1) + {~o}simprove p(~o)(V new (s0 , k  1) + pstay(s0 , k  1)V  (s0 )) k > 0
1
0
k=0
(17)

where s0 = state(s ~o ). Notice that when repeating the calculation using the above equation with
an increasing k value starting from k = 1, each iteration includes only a single unknown parameter,
(V new (s, k)). In addition, the values V new (s0 , k  1) and V  (s0 ) do not depend on values that were
computed for state s (s0 precedes s in the set Ss ).
5.2 The Algorithm for Computing the Coalitions Optimal Strategy
The above analysis leads to an algorithm for computing the coalitions optimal strategy (Algorithm
1). This algorithm computes the coalitions strategy and its expected utility for all possible states
init
SA . Its execution time is polynomial in w init
max (for convenience we use wmax to denote the bound for
the number of parallel interactions w, when the coalition begins its search, i.e., coalitions state is
s = sinit = {}).
Notice that at each stage of its execution, algorithm 1 reuses components computed in earlier
stages. For example, V new (s, w) appears both in the computation of V (s, w) (using Equation 10), in
the computation of V new (s0 , w + 1) (using Equation 17, when ~o0  O p such that state(s0 ~o0 ) = s)
15

)

fiM ANISTERSKI , S ARNE , & K RAUS

Algorithm 1 An algorithm for computing the optimal search strategy x 
Input: O p - set of potential opportunity types in the market; p(~o) - opportunity types probability
function; n - coalitions size; U j (), j = 1, ..., n - coalition members utility functions; c(w, n) search cost function; SA - set of all possible states of a coalition A.
Output: x (s) s  SA - the coalitions optimal strategy.
1: Build the set of ordered states S s
2: for i=1 to |Ss | do
3:
Set V (si , 0) = Vt (si ) using Equation 1
4:
Set V new (si , 0) = 0
5:
Set w = 1
6:
while c(w, n)  (Vt (s1 ) Vt (sinit )) do
7:
Compute V new (si , w) using Equation 17
8:
Compute V (si , w) using Equation 10
9:
w++;
10:
end while
11:
Set x (si ) = arg maxw0 (0,...,w1) V (si , w0 ), V  (si ) = V (si , x (si ))
12: end for
13: Return (x (si ), i = 1, ..., s|SA | )
and in the computation of V new (s00 , w + 1) (using Equation 17, when ~o00  O p such that state(s00 
~o00 ) = s). Storing the result for each such computational element in the memory, for the purpose of
reusing it at later stages, significantly improves the efficiency of the algorithm. This is accomplished
by using two matrixes V and V new of size |SA |  (winit
max + 1), in which the corresponding V (s, w) and
V new (s, w) values are stored for each pair (s, w), representing a state and the correlated result of each
number of parallel interactions that is used for the calculations. Additionally, we store x  and V 
values by using two arrays of size |S A | for reusing x (s0 ) and V  (s) in the computation of V new (s, w).
Theorem 2. Algorithm 1 returns the optimal strategy of the coalition in a polynomial time of w init
max .
Proof. In our proof, we assume that we use the matrices V and V new for storing the values computed along the execution of the algorithm as described above. In order to build the set of ordered
states Ss , the algorithm needs to compute the coalitions termination utility for each of the states
in SA , according to equation 1. Computing the coalitions termination utility for a given state
takes O(|A|3 ) in C2C markets9 and O(|A|2 ) in B2C markets10 . The coalitions termination utility is calculated for each state s  S A . Thus the overall complexity of computing the coalitions
termination utility for all states is O(|S A ||A|2 ) and O(|SA ||A|3 ) in B2C and C2C markets, respectively (the sets Ss and SA have the same size). Sorting the members in S A in order to build Ss takes
O(|SA | log |SA |). The computation of step 3 takes a constant time assuming that we stored for each
9. As mentioned previously, computing the coalitions termination utility in C2C markets is equivalent to finding a
maximum matching in a weighted bipartite graph. Finding maximum matching in a weighted bipartite graph can be
done in O(n(n log n + m)), where n is the number of vertices and m is the number of edges (Wang, Makedon, & Ford,
2004). Since in our graph each agent is connected to all opportunities, and the number of opportunities in a given
state is bounded by the number of agents, the process of computing the coalitions termination utility takes O(|A|3 ).
10. As mentioned previously, computing the coalitions termination utility in B2C markets can be done by finding an
opportunity for each agent in the coalition that maximizes its utility. As the number of opportunities in a given state
is bounded by the number of agents, computing the coalitions termination utility takes O(|A|2 ).

16

fiE NHANCING C OOPERATIVE S EARCH

WITH

C ONCURRENT I NTERACTIONS

state its coalitions termination utility when S s was built. Steps 3-5 are performed for all states,
thus, the overall complexity is O(|S A |). The loop in step 6 is performed at most w init
max times. This
init
is because when reaching a value w = w max + 1 in step 6, the loop condition no longer holds (i.e.,
c(w, n) > (Vt (s1 ) Vt (sinit ))). Notice that the first elements calculated are for state s 1 (i.e. the one
with the maximum utility) according to the loop in step 2. Here, as explained in section 4, the expected utility from any strategy by which the search is resumed (i.e., using w  1) is V (s, w) = 
1
(formally, since simprove
= 0/ w then pstay (s1 , w) = 1 and V new (s1 , w) = 0 w  (1, .., winit
max ), thus,
w
init
V (s1 , w) =  w  (1, .., wmax )). Therefore, the optimal strategy in state s 1 is to terminate the
search, i.e. x (s1 ) = 0 and V (s1 , 0) = Vt (s1 ). For any other state, s, when reaching step 7, the
algorithm has already computed both V  (s0 ) and V new (s0 , w) w  (0, ..winit
max ) for each potential future new state s0 originating from state s. This is due to the fact that all future states of state s
appear before s in the set of ordered states S s (either because of a higher utility, or equal utility
yet sorted before s according to the function alloc). In addition, for any number of parallel interactions w  1, the algorithm has already computed V new (s, w  1). Therefore, the computation
time in step 7 sums up to an order of |O p | components that have already been computed. Then,
when reaching step 8, the value of V new (which is part of the numerator in equation 10) has been
already computed. Therefore, if we ignore, for now, the time of computing the coalitions new
state s0 when it encounters opportunity ~o given it is in a state s and the time of computing p stay
values, the computation of step 7 takes O(|O p |) and the computation of step 8 takes a constant
time. Since these steps are performed at most w init
max |SA | times, the overall complexity of computing these steps is O(|O p |winit
|S
|).
In
step
11,
the
algorithm chooses the maximum value among
A
max
winit
values
that
have
already
been
computed.
Since
this step is performed for each state s  S s ,
max
the overall complexity of performing this step is O(w init
max |SA |). The complexity of computing the
coalitions new state s0 when it encounters opportunity ~o given that its current state is s depends
on the market type. In B2C markets, the complexity is O(|A|) if we store for each state s also its
alloc(s) value (we compute this value when we compute the coalitions termination utility). In
C2C markets, computing the new state takes O(|A| 3 )11 . We can store these values using a matrix S f uture of size |SA |  |O p | in which the new state s0 = state(s ~o) is stored for each pair (s,~o).
Therefore, the overall complexity of computing future states for all states is O(|S A ||A||O p |) in B2C
markets, and O(|SA ||A|3 |O p |) in C2C markets. Computing pstay (s, w) can be done in a constant
time based on pstay (s, w  1). Moreover, we can store these values using a matrix Pstay of size
stay (s, w) is stored for each pair (s, w). Therefore, the total com|SAg |  winit
max in which the value p
plexity of computing pstay values is O(|SA |  winit
max ). Given this analysis, the overall complexity
of the algorithm is O(|SA ||A|2 + |SA | log |SA | + |SA ||O p |winit
max + |SA ||A||O p |) in B2C markets, and
O(|SA | log |SA | + |SA ||A|3 |O p | + |SA ||O p |winit
)
in
C2C
markets.
Hence the algorithm is polynomial
max
init
in wmax .

Note the algorithm uses winit
max as an upper bound for the optimal number of interactions x (si ),
i
i = 1, ..., |Ss | . This bound is valid since according to Equation 7 the value of w smax
increases
as Vt (si ) decreases and the coalitions termination utility reaches its minimum in the initial state
(Vt (sinit ) = Vt ({}) = 0). This suggests a further significant improvement of the above algorithm.
si
Instead of using winit
max for all states, we calculate a specific upper bound, w max , for each state si in
step 6, according to Equation 7. At some point this may require re-computation of V (s i , w) for w

11. Computing the coalitions new state in C2C markets is done by finding a maximum matching in a weighted bipartite
graph (see footnote 10 for complexity analysis).

17

fiM ANISTERSKI , S ARNE , & K RAUS

values that were not used in previous execution stages of the algorithm; however, the total number
of calculations for each state si in many cases will significantly decrease 12 .

6. Properties of the Parallel Cooperative Search (PCS) Model
Prior to extending the above analysis to scenarios where the coalition faces a finite decision horizon,
we emphasize some interested unique characteristics of the parallel cooperative search derived from
the general analysis. First we wish to emphasize that the PCS model is a generalization of both the
Single Agent Parallel Search (SAPS) and the Sequential Cooperative Search (SCS) model as stated
in the following proposition.
Proposition 2. The cooperative parallel search is a generalization of both the single agent parallel
search and the cooperative sequential search models 13 .
Proof. From the analysis given in section 4 it is clear that Algorithm 1 results in the same strategy
used in the cooperative sequential search and in the single agents parallel search, for the specific
i
cases in which si wsmax
= 1 parallel interactions or n = 1 agents, respectively. 14
Furthermore, we emphasize that the coalitions expected utility never decreases when using our
proposed mechanism in comparison to the sequential cooperative search. Indeed in the case where
maintaining more than a single interaction is not a favorable strategy Algorithm 1 results in one
interaction at a time strategy, as used in the sequential cooperative search. Obviously if using the
parallel interaction does not decrease the search cost (i.e. the search cost of conducting any number
of interactions sequentially is equal or smaller than conducting the interactions in parallel), then the
sequential cooperative search is the dominating strategy as stated in the next proposition.
Proposition 3. If the cost of conducting any number of parallel interactions is equal or higher than
the cost of conducting these interactions sequentially (i.e w, w  c(1, n)  c(w, n)) then the use of
the parallel search can at most match the expected utility of the sequential cooperative search.
Proof. Consider an optimal cooperative parallel search strategy x par . Obviously there is a state
s  SA satisfying: (1) The coalitions strategy in s is to conduct more than one interaction, x par (s) > 1
(2) For any future state s0 of s the coalition conducts at most one interaction (or terminates search),
i.e., xpar (s0 )  1. If we replace the parallel search strategy when in state s with the sequential
search from this state on then the expected utility can only increase. This is because in the worst
case scenario the coalition will execute w interactions incurring at most the same cost as in x par ,
ending up with the same expected termination utility. For any other case, whenever the coalition
reaches state s0 where its strategy according to xpar is to terminate its search prior to completing w
interactions, the expected utility is at least the expected utility achieved by using x par (otherwise
the coalitions strategy in s0 according to xpar will be to resume the search) . Therefore the use of
the new strategy can only improve the expected utility. Using backward induction we can apply
the same logic to all former states for which x par implies more than one interaction in parallel.
12. The extent of the achieved improvement is highly correlated with the specific environment in which the coalition
operates.
13. Notice that in this context the single agent sequential search is a specific case of the single agent parallel search,
where the agent interacts with a single seller agent at a time.
14. Assuming similar cost structures in all three models.

18

fiE NHANCING C OOPERATIVE S EARCH

WITH

C ONCURRENT I NTERACTIONS

Consequently, we obtain that the sequential strategy results in at least as large an expected utility as
xpar .
Next we consider the case where the agents are fully homogeneous (in terms of their utility
functions) and operate in B2C markets. Here, we can prove that the optimal strategy of the coalition
is stationary (i.e. does not change according to the current state). Furthermore, we show that
the stationary strategy characteristic holds not only for fully homogeneous agents but also when
the agents have correlated preferences. Two agents a i and a j have correlated preferences when
agent ai prefers ~o0 over ~o00 if and only if agent a j prefers opportunity ~o0 over ~o00 (i.e ~o0 ,~o00  O p ,
U j (~o0 )  U j (~o00 )  Ui (~o0 )  Ui (~o00 )) and vice versa.
Theorem 3. In the B2C market, if all the agents have correlated preferences, the search strategy
is based on a reservation value15 Urv . In such a scenario the number of parallel interactions the
coalition uses according to the optimal strategy (in case it resumes the search) is fixed during the
search (s  SA such that Vt (s) < Urv exists x (s) = w f ixed ).
Proof. In the above scenario, the search problem is equivalent to the problem of a single agent
with a utility function equal to the sum of the different agents utilities, U = U 1 +U2 + ... +Un . In
this case after terminating the search, all of the coalition members are always assigned the same
opportunity. Therefore, the search strategy is reservation value based, and the search is terminated
upon reaching such an opportunity with a utility exceeding a pre-set reservation value U rv . Since
the probability of reaching such an opportunity does not depend on the coalitions state, the number
of parallel interactions used throughout the search is fixed.
Nevertheless Theorem 3 does not hold in the C2C market even when the agents are homogeneous as the next lemma states.
Lemma 3. In the C2C market even for fully homogeneous agents (all agents have the same utility
functions) the search strategy is not always stationary and the optimal number of parallel interactions can be changed during the search according to the coalitions current state.
Proof. In order to prove this lemma, we consider a coalition that operates in a C2C market in the
following environment:
Environment 4. Assume a coalition of two agents, a 1 and a2 , searching for a product (e.g., a used
book) characterized by one attribute (e.g., indicating whether the book is signed by the author) with
two possible values, 1 (signed) and 2 (not signed). This results in two opportunity types, o~ 1 = (1)
and o~2 = (2). Assume opportunity o~1 is rare and can be found with a probability of 1/100, while
opportunity o~2 is very common (a probability of 99/100). Both agents utilities are 100 for the rare
opportunity and 1 for the common opportunity. Consider that the cost of conducting w parallel
interactions is equal to the cost of conducting w interactions sequentially, c(w, n) = wc(1, n), where
the cost of conducting a single interaction is c(1, n) = 0.1 + 0.05n, (n > 1). Table 4 summarizes
environments setting.
We consider the two following states: (1) The coalition starts its search (the coalitions current
state is its initial state s = {}) (2) The coalition encounters opportunity o~ 1 (the coalitions current
15. A reservation value is a value that the coalition sets a-priori and terminates the search if reached an opportunity
associated with a utility greater than or equal to this value (Sarne & Kraus, 2005).

19

fiM ANISTERSKI , S ARNE , & K RAUS

Opportunity

Attribute

Probability

~o1
~o2

signed
not signed

0.01
0.99

Utility
Agent a1 Agent a2
100
100
1
1

Table 4: Agents utilities for the four opportunities in Environment 4
state is s0 = {~
o1 }).
In order for the coalition to terminate its search all its members should exploit opportunity o~ 1 (otherwise the expected utility from resuming the search exceeds the search cost). Thus in order for the
coalition to terminate the search in the former case (the coalitions state is s = {}) it must encounter
opportunity o~1 twice and in the latter case (the coalitions state is s 0 = {~
o1 }) it has to encounter
opportunity o~1 once. Thus we expect that the number of optimal interactions, where the coalitions
state is s, to be different from the number of optimal interactions, where its state is s 0 . Indeed
the computation of the coalitions optimal number of interactions (using Algorithm 1) results in
x (s) = 299 and x (s0 ) = 161.

7. Finite Decision Horizon
An important variant of the cooperative parallel search is the one where the agents forming the
coalition are restricted by a deadline for finalizing their search. For example, consider a coalition
that searches for costumes for its members to wear to a costume party. In this case the coalition
can not search for the costumes forever since the customs have a value for the coalition members
only if purchased prior to the party. This type of environment is often referred to in search theory as
a finite decision horizon environment. Specifically, within the context of this paper we consider
finite decision horizon environments where the coalition as a whole should terminate its search prior
to or within the next r search rounds. Note that in the sequential search this definition is equivalent
to the definition stating that the coalition can conduct at most r additional interactions.
In addition to the general advantages recognized in the parallel cooperative search, in finite decision horizon environments the model can enable the coalition to conduct more than the maximum
r interactions facilitated by the sequential model (whenever necessary). Moreover, in some environments where the coalition cannot improve its performance by interacting with several sellers in
parallel (e.g., in cases such as those described in Proposition 3), the introduction of a finite decision
horizon constraint creates a strong incentive to interact with more than a single seller at a time. This
is illustrated in the following example:
Example 3. Consider a coalition that operates in a B2C market, having the same characteristics
as in Environment 4, which was introduced in section 6. In this environment the coalitions optimal
strategy when having an infinite decision horizon is to search sequentially (see Proposition 3). Nevertheless, for various finite decision horizon values the coalition can benefit from using the parallel
search technique. The most trivial example in this case is when the coalition has to terminate its
search within the next search round. In this case, if the coalition conducts several interactions in
the next round, its probability to encounter opportunity o~1 increases in comparison to the case in
which it conducts only a single search. For example if the coalition conducts 100 interactions it
has a probability of 0.99100 to encounter only opportunity o~2 during its search and a probability of
20

fiE NHANCING C OOPERATIVE S EARCH

WITH

C ONCURRENT I NTERACTIONS

1  0.99100 to encounter opportunity o~1 during its search and its expected utility can be expressed
as: (1  0.99100 )  Vt ({~
o1 }) + 0.99100Vt ({~
o2 })  c(100, 2) = (1  0.99100 )  200 + (0.99)100  2 
0.2  100 = 107.52. This value is significantly greater than its expected utility when using the sequential cooperative search which is 0.01 Vt ({~
o1 }) + 0.99Vt ({~
o2 })  c(1, 2) = 3.78.
In order to compute the coalitions strategy in a finite decision horizon model variant we extend
our definitions to include the number of remaining search rounds, r. We use V (s, w, r) to denote the
coalitions expected utility, when it conducts w interactions in the next round and has to terminate
its search within the next r rounds. The term V (s, 0, r) denotes the immediate utility obtained, if the
coalition decides to terminate the search at state s, thus: V (s, 0, r) = Vt (s). We denote by x (s, r) and
V  (s, r) the coalitions optimal strategy and its expected utility (when using its optimal strategy) at
state s, if it should terminate its search within the next r rounds.
The V  (s, r) and x (s, r) calculation is similar to the infinite decision horizon case, except that
whenever the coalition reaches its decision deadline it inevitably terminates its search. Namely, if
r = 0, then x (s, 0) = 0 and the coalitions expected utility V  (s, 0) is equal to Vt (s).

arg maxw V (s, w, r) r > 0
(18)
x (s, r) =
0
r=0
(19)

V  (s, r) = V (s, x (s), r)

We begin by computing V (s, w, r), when r > 0 (since V (s, w, 0) = 0, w). Here, we can apply the
same analysis methodology used in section 4. However, the expected utility of the coalition when
resuming its search following the current search stage should reflect the change in the decision
horizon. Therefore, instead of using Equation 4, we should use the following modification:



V (s, w, r) = c(w, n) +

w simprovew

pw (w )V  (s0 , r  1) + pstay (s, w)V  (s, r  1),

w > 0, r > 0
(20)

where = state(s  w ).
The computation of Equation 20 is exponential in the number of parallel interactions. In order
to reduce the complexity of V(s,w,r) for r > 0 we consider the two following cases:
s0

 w = 1: the coalition encounters a single opportunity thus Equation 20 can be expressed as:
V (s, 1, r) = c(1, n) +



{~o}simprove

p(~o)V  (state(s ~o ), r  1) +



p(~o)V  (s, r  1),

r > 0

{~o}sstay1

1

(21)
In this case, no additional computational complexity is introduced, in comparison to the SCS
model (Sarne & Kraus, 2005).
 w > 1: here we attempt to find a computational means for extracting the value of the term
w simprovew V  (s0 , r  1) + pstay (s, w)V  (s, r  1) (in Equation 20) in complexity polynomial
in w. This expression denotes the coalitions expected utility when it conducts w interactions
without considering the cost associated with conducting the w interactions. In order to efficiently compute this value we consider the coalitions expected utility after conducting one of
its w interactions and obtaining opportunity ~o. In this case, the coalitions expected utility is
21

fiM ANISTERSKI , S ARNE , & K RAUS

equal to the coalitions expected utility when starting from state state(s ~o) and conducting
w  1 interactions plus a cost equivalent to the cost of maintaining these w  1 interactions,
c(w  1, n) (these are added since they were already subtracted from the expected utility in
Equation 20 and V (state(s ~o), w  1, r) subtracts them again):
V (s, w, r) = c(w, n) +



p(~o)(V (state(s ~o), w  1, r) + c(w  1, n)),

~oO p

w > 1, r > 0
(22)

The sum in Equation 22 can be represented as the sum of the expected utility over the opportunities that change the coalitions current state and the sum over the opportunities that do not change
the coalitions current state:
V (s, w, r) = c(w, n)+



p(~o)V (s0 , w1, r)+

{~o}simprove
1

p(~o)V (s, w1, r)+c(w1, n),

{~o}sstay1

w > 1, r > 0
(23)

where = state(s ~o).
Equations 23 and 21 facilitate the calculation of V (s, w, r) in a polynomial time of w. In order
to find the coalitions optimal strategy, the efficient bound for the optimal number of interactions
given in Equation 7 is also valid for the finite decision horizon variant.
The above analysis leads to Algorithm 2 which is a modification of Algorithm 1. This algorithm
computes the coalitions strategy and expected utility for all states in S A . In order to compute
V  (s, r), this algorithm uses backward induction. It starts by computing V  (si , 0) for all states.
Here, the coalition is forced to terminate its search thus the algorithm sets the expected utility to be
equal to the termination utility: V  (si , 0) = Vt (si ). Then, the algorithm computes V (s i , r, w) for r > 0
using Equation 21 and 23 starting from w = 1 till c(w, n)  Vt (s1 ) Vt (sinit ). The algorithm sets the
coalitions optimal strategy to be the number of interactions w that maximizes V (s i , r, w).
Similar to the infinite decision horizon case, Algorithm 2 reuses components computed in earlier
stages in each stage of its execution. Nevertheless, in this case in order to store the results in the
memory for reuse later, we need a three dimensional matrix V of size |S A |  (winit
max + 1)  (rmax + 1)
where rmax is the initial decision horizon. The corresponding V (s, w, r) values are stored for each
triplet (s, w, r), representing a coalitions state, the number of simultaneous interactions used and
the limit on the number of rounds that the coalition can conduct. Additionally, we store V  (s, r)
and x (s, r) values in two matrixes V and X of size |S A |  (rmax + 1) for reusing x (s0 , r  1) and
V  (s0 , r  1) in the computation of V (s, w, r).
In the following theorem we prove that algorithm 2 returns the coalitions optimal strategy and
its expected utility, in a polynomial time of w init
max (the bound on the number of parallel interactions
computed using Equation 7) and rmax .
s0

Theorem 4. Algorithm 2 returns the optimal strategy of the coalition and its expected utility from
following this strategy, where it should terminate its search within the next r max rounds, in a polynomial time of winit
max and rmax .
Proof. In our proof, we assume the use of matrices for storing the computed values as described
above. The process of extracting Ss can be done exactly as in the finite decision horizon. Thus, step
1 is of complexity O(|SA ||A|2 + |SA | log |SA |) and O(|SA ||A|3 + |SA | log |SA |) for B2C markets, and in
22

fiE NHANCING C OOPERATIVE S EARCH

WITH

C ONCURRENT I NTERACTIONS

Algorithm 2 An algorithm for computing the optimal search strategy x  when the coalition should
terminate its search within the next r max rounds
Input: O p - set of potential opportunity types in the market; p(~o) - opportunity types probability
function; n - coalitions size; U j (), j = 1, ..., n - coalition members utility functions; r max - the
maximum number of search rounds until terminating the search (the decision horizon); c(w, n)
- search cost function; SA - set of all possible states of a coalition A.
Output: x (s, r),V  (s, r) s  Ss , 0  r  rmax - the coalitions optimal strategy and its expected
utility.
1: Build the set of ordered states S s
2: for i = 1 to |Ss | do
3:
Set x (si , 0) = 0
4:
Set V  (si , 0) = Vt (si ) using Equation 1
5: end for
6: for r = 1 to rmax do
7:
for i=1 to |SA | do
8:
Set V (si , 0, r) = Vt (si ) using Equation 1
9:
Compute V (si , 1, r) using Equation 21
10:
Set w = 2
11:
while c(w, n)  (Vt (s1 ) Vt (sinit )) do
12:
Compute V (si , w, r) using Equation 23
13:
w++;
14:
end while
15:
Set x (si , r) = arg maxw0 (0,...,w1) V (si , w0 , r), V  (si , r) = V (si , x (si , r), r)
16:
end for
17: end for
18: Return (x (si , r), i = 1, ..., s|SA | , r = 0, ..., rmax )
C2C markets, respectively. The computation in steps 3 and 4 takes a constant time, assuming that we
stored for each state its coalitions termination utility when we built S s . These steps are performed
for all states, thus, the overall complexity of computing these steps throughout the loop is O(|S A |).
Again, step 8 can be performed in a constant time, assuming the coalitions termination utility is
stored for each state. For any state, s, and for any limit on the number of search rounds r > 0 when
reaching step 9 of the algorithm, the algorithm has already computed V  (s0 , r  1) for each potential
future new state s0 originating from state s. Therefore, the computation in step 9 is simply the summing of order of |O p | components that have already been computed. Consequently, if we ignore
the time of computing the coalitions new states s 0 (we will add the time for computing these values
later), the computation of this step takes O(|O p |). Since steps 8-9 are performed rmax |SA | times,
the overall complexity of computing these steps (when ignoring the time of computing the new
states) is O(|O p |rmax |SA |). Notice that when w = winit
max + 1 we obtain c(w, n) > (Vt (s1 ) Vt (sinit )).
Thus, the loop in step 11 is performed at most w init
max times. For any w > 0 when reaching step 12,
0
the algorithm has already computed V (s , w  1, r) for each potential future new state s 0 originating
from state s. Therefore, the computation in step 12 takes O(|O p |) when ignoring the time of computing the new states. Since this step is performed r max |SA |winit
max times, the overall complexity of
computing this step (when ignoring the time of computing the new states) is O(|O p |rmax |SA |winit
max ).
23

fiM ANISTERSKI , S ARNE , & K RAUS

In step 15, the coalition chooses the maximum value among the w init
max values that have already been
computed. Since this step is performed for each state s  S s , and for each r  {0, ..., rmax }, the
overall complexity of performing this step is O(w init
max |SA |rmax ). Computing the coalitions new state
s0 when it encounters opportunity ~o given that its current state is s can be done as described in the
infinite decision horizon case. Thus, the overall complexity of computing future states for all states
is O(|SA ||A||O p |) in B2C markets, and O(|SA ||A|3 |O p |) in C2C markets. Given this analysis, the
2
overall complexity of the algorithm is O(|S A | log |SA | + |SA ||O p |winit
max rmax + |SA ||A| + |SA ||A||O p |)
3
in B2C markets, and O(|SA | log |SA | + |SA ||O p |winit
max rmax + |SA ||A| |O p |) in C2C markets. Hence the
16
init
algorithm is polynomial in wmax and rmax .
Similar to the infinite decision horizon a significant improvement in the above algorithms peri , for each state, s
formance can be achieved by calculating and using the specific upper bound, w smax
i
in step 11, according to Equation 7.
In the following section we illustrate some properties of the PCS model both for the infinite and
finite decision horizons model variants.

8. Illustrative Examples of the PCS Model
With an efficient means for calculating the coalitions optimal strategy when using parallel cooperative search, we can now demonstrate some specific properties of this search method. As a reference
we use the Single Agents Parallel Search (SAPS), the Single Agents Sequential Search (SASS)
and the Sequential Cooperative Search (SCS) models.
8.1 Infinite Decision Horizon
We begin by illustrating the parallel cooperative search with an infinite decision horizon. First we
demonstrate the influence of the level of heterogeneity in the utility functions of the different coalition members on the coalitions performance (in terms of the expected utility achieved). In order
to demonstrate this we use the following environment, which was used originally for evaluating the
performance of the SCS model (Sarne & Kraus, 2005):
Environment 5. A coalition of two agents, a 1 and a2 , searching for opportunities defined by two
attributes, B1 (e.g., quality) and B2 (e.g., store rating), where each attribute can have a value from
the discrete range of (1, ..., 5) with an equal probability for each of the values. The agents are
heterogeneous in respect to the way they evaluate each potential opportunity. Agent a 1 is associated with the utility function U1 (~o) = B1 + B2 , while agent a2 is associated with the utility function
U2 (~o) = 2(1  )B1 + 2B2 . Thus, the parameter  indicates the level of the agents similarity/heterogeneity. The search cost of any single agent for conducting a single interaction is c base for
w parallel interactions (w > 1): c(w, 1) = c base + c parallel  (w  1). The search cost of a coalition
of size n is c(w, n) = c(w, 1)ln(n + 1), (n > 1).
Figure 2 depicts the expected utility per agent when using the different search methods 17 in the
C2C market (left hand-side) and the B2C market (right hand-side) as a parameter of the similarity
level, , between the utility functions of the agents constituting the coalition.

16. The algorithm uses winit
max as an upper bound for the optimal number of interactions x (si ), i = 1, ..., |SA |. This bound
si
is valid since according to Equation 7 the value of wmax increases as Vt (si ) decreases and the coalitions termination
utility reaches its minimum in the initial state (Vt ({}) = 0).
17. For the cooperative models the average expected utility per coalition member measure is used.

24

fiE NHANCING C OOPERATIVE S EARCH

utility
9.3

utility
9.3
9.1

1

WITH

9.1

PCS

C ONCURRENT I NTERACTIONS

PCS
1

8.9

8.9

2

2
SAPS
8.5
8.3

SAPS

8.7

8.7

8.5

3

SCS

8.3

4

8.1

SASS

7.9
7.7

3
SCS

8.1
7.9

4

SASS

7.7

0.01 0.11 0.21 0.31 0.41 0.51 0.61 0.71 0.81 0.91

0.01 0.11 0.21 0.31 0.41 0.51 0.61 0.71 0.81 0.91

similarity level ( )

Figure 2: Average expected utility per buyer agent as a function of similarity level for different models in
different markets

Curve 1 in each graph depicts the average expected utility when the two agents form a coalition as a function of the similarity level  between the agents utility functions, making use of
the suggested parallel cooperative search. Here, the search cost of conducting a single interaction
was set at 0.2 (cbase = 0.2) and the search cost to conduct additional interactions was set to 0.05
(c parallel = 0.05). As expected our model (represented by curve 1) outperforms the SCS model (represented by curve 3) in terms of the expected utility for the agents. The other two curves describe
the average expected utility of the agents when each searches separately using the Single Agent
Parallel Search (SAPS) (represented by curve 2) and the Single Agent Sequential Search (SASS)
(represented by curve 4) models. In this specific environment the use of the cooperative parallel
search also outperforms the single agent parallel search model though this is not always the case.
Notice that the results obtained for the cooperative parallel search are consistent with a general
characteristic of cooperative search (Sarne & Kraus, 2005) by which the use of the method in the
B2C market results in a better expected utility than in the C2C market. In the case of the separated
single searches (SAPS and SASS models) the market type does not affect the strategy structure nor
the expected utility since each agent searches only for a single opportunity for its own benefit.
Figure 2 also reflects an interesting insight which contradicts an important strategy domination
relationship found between single and cooperative sequential search techniques of fully homogeneous agents (i.e. with the same utility function, as in the case where  = 0.5 in our example)
operating in C2C markets. While for the sequential search the use of the single agent search always outperforms the cooperative search in the C2C markets (when considering fully homogeneous
agents) (Sarne & Kraus, 2005), here we have actual evidence that where parallel search is concerned,
the cooperative search technique may outperform the aggregated result of the single homogeneous
agents search.
Figure 3 shows the expected utility per agent when using the different search methods in the
C2C market (left hand-side) and the B2C market (right hand-side) as a parameter of the cost of
conducting an additional interaction c parallel (notice that the agents performance is not affected by
this value in the SASS and the SCS models). The results are based on Environment 5 (the same
environment we used for Figure 2), where  = 0.1 and c base = 0.2. As expected as the cost c parallel
25

fiM ANISTERSKI , S ARNE , & K RAUS

decreases the average expected utility using the parallel models (PSC and SAPS models) increases
(and thus the greater the improvement in comparison to the sequential models). Note that whenever
c parallel  cbase the performance converges to the one achieved by obtaining one observation at a
time, as used in the sequential models. This behavior is correlated with Proposition 3.

0.17

0.21

9.7

utility

utility

9.7

9.5

9.5

9.3

9.3

9.1

1

9.1

PCS

1
8.9

8.9

SAPS

8.7

2

2
3

8.5

SCS

8.3

4

8.1

SAPS

8.7

8.5

3

PCS

SCS

8.3

4

SASS

SASS

8.1

0.01

0.05

0.09

0.13

0.17

0.21

0.01

0.05

0.09

0.13

0.17

0.21

cost of parallel interaction

Figure 3: Average expected utility per buyer agent as a function of c parallel when using the different search
methods in different markets

The cost of conducting an additional interaction c parallel also influences the optimal number
of interactions that the coalition should conduct. Figure 4 shows the optimal number of parallel
interactions that the coalition should conduct at the beginning of its search, x  ({}), as a function
of c parallel in the same environment that we used for Figure 3 (Environment 5). As expected, the
optimal number of interactions, x ({}), increases as c parallel decreases and is equal to 1, when
c parallel  cbase .
In Figure 4 the coalitions optimal number of interactions using the PCS model is higher when
the coalition operates in the C2C market than when in the B2C market. While this can have an
intuitive explanation (in the C2C market each opportunity can be exploited by only one of the agents
so the coalition needs to encounter more opportunities in the C2C market) it cannot be generalized.
The following example illustrates a scenario in which the optimal number of interactions is actually
greater when operating in a B2C market.
Environment 6. A coalition of two agents, a 1 and a2 , searching for a product (e.g., a computer
game) associated with 4 types of opportunities {~o 1 ,~o2 ,~o3 ,~o4 } (e.g., representing different configurations). The agents utilities and opportunities distribution are given in Table 5. The search cost
of any single agent to conduct a single interaction is c base = 0.2, and the search cost of conducting
an additional search is c parallel = 0.1, c(w, 1) = cbase + c parallel  (w  1). The search cost for a
coalition is c(w, n) = c(w, 1)ln(n + 1), (n > 1).
Figure 5 shows the optimal number of interactions used by the coalition (in the SCS model)
or the agent (in the SAPS model) at the beginning of the search, when operating in Environment
6. As one can see the coalitions optimal number of interactions using the PCS model in the B2C
26

fiE NHANCING C OOPERATIVE S EARCH

optimal # of interactions

32

WITH

C ONCURRENT I NTERACTIONS

(1) Coalition in B2C

28
(2) Coalition in C2C
24

1
(3) Agent A1

20
4

(4) Agent A2

16
12

3

2

8
4
0
0.01

0.06

0.11

cost of parallel interaction

0.16

0.21

Figure 4: Coalitions or agents optimal number of parallel interactions at the beginning of its search as a
function of c parallel

Opportunity

Probability

~o1
~o2
~o3
~o4

0.499
0.25
0.25
0.001

Utility
Agent a1 Agent a2
0
0
0
1
1
0
100
100

Table 5: Agents utilities for the four opportunities in Environment 6
market (44) is larger than the coalitions optimal number of parallel interactions in the C2C market
(8). Moreover this figure contradicts two additional hypothesis that one may presume about the
optimal number of a coalitions parallel interactions. The first is that the coalitions optimal number
of parallel interactions is at most equal to the overall number of parallel interactions, when each of
the agents conducts the search autonomously. As can be seen in Figure 5, the optimal number of
parallel interactions in the B2C market is greater than the total number of parallel interactions, when
each of the agents conducts its search autonomously (10 + 10 = 20 < 44). The second hypothesis
suggests that the coalitions optimal number of parallel interactions is at least the number of parallel
interactions, when each of the agents conducts the search autonomously. This, again is proven
wrong in Figure 5, where the coalitions optimal number of parallel interactions in the C2C market
(8) is smaller than the number of parallel interactions of agent a 1 (10), and the number of parallel
interactions of agent a2 (10).
Next we introduce and make use of a simpler sample environment for demonstrating some
additional properties of the cooperative parallel search.
Environment 7. A coalition of two agents, a 1 and a2 , searching for opportunities defined by two
attributes, B1 (e.g., quality) and B2 (e.g., store rating), where each attribute can have a value from
the discrete range of (1, 2) with an equal probability for each of the values. The utility functions
used are U1 (~o) = 1.9B1 + 0.1B2 and U2 (~o) = 0.1B1 + 1.9B2 . The search cost of a single agent
27

fiM ANISTERSKI , S ARNE , & K RAUS

x*({})
45

36
27
18
9
0
PCS in
C2C

PCS in
B2C

SAPS of
Agent 1

SAPS of
Agent 2

Figure 5: Coalitions optimal number of parallel interactions at the beginning of its search
is c(w, 1) = 0.5 + 0.05w, and for a coalition it is c(w, n) = c(w, 1)  ln(n + 1), (n > 1). Table 6
summarizes the environments setting.
Opportunity
~o1
~o2
~o3
~o4

(Attribute1,
Attribute2)
(1,1)
(1,2)
(2,1)
(2,2)

Probability
1
4
1
4
1
4
1
4

Utility
Agent a1 Agent a2
2
2
2.1
3.9
3.9
2.1
4
4

Table 6: Agents utilities for the four opportunities in Environment 7

7.1
utility
7
6.9
6.8
6.7
6.6
6.5
6.4
1

2

3

4

5

6

7

8

9

10

# of parallel interactions (w)

Figure 6: Coalitions overall expected utility as a function of w
Figure 6 depicts the expected coalitions overall utility with respect to the number of interactions
conducted at the beginning of the search (i.e. the first search stage, before the coalition knows about
any of the opportunities), assuming that in all the other states the coalition uses the optimal number
28

fiE NHANCING C OOPERATIVE S EARCH

WITH

C ONCURRENT I NTERACTIONS

of parallel interactions, x (s). Here, we can see the effect of two conflicting forces: as the number of
parallel interactions the coalition uses in this stage increases, the probability of associating a better
opportunity with any of the two coalition members increases. However the overall search cost
associated with the search stage increases. From the figure, we conclude that the optimal number of
parallel interactions to be used in this stage is x  ({}) = 5.
An additional important characteristic of the cooperative parallel search we wish to emphasize
concerns the number of parallel interactions used as part of the optimal search strategy along the
search. While in a single agents parallel search the search strategy is stationary (i.e. the number
of parallel interactions used does not change along the search process) in our model the number of
parallel interactions along the search that needs to be maintained depends on the coalitions state
(i.e. the set of opportunities known to the coalition). This is demonstrated in the directed acyclic
graph (DAG) given in Figure 7, which describes the search process in Environment 7. The vertices
of this graph present potential coalitions states (the state is determined according to the relevant
set of known opportunities, correlated with the definition given in section 4). An edge connects
two states s and s0 only if there is a possibility to reach state s over the following search round if
the coalition conducts the search according to the optimal search strategy. For example a directed
edge connects between s = {} and s0 = {(1, 2), (2, 1)}, since the coalition can proceed from s to s 0
if it conducts the five parallel interactions according to the optimal search strategy and encounters
opportunities (1, 2) and (2, 1). Notice that when reaching states {(1, 2), (2, 1)} and {(2, 2), (2, 2)}
the optimal strategy of the coalition is to terminate the search. Therefore there is no edge originating
in these states. As illustrated in Figure 7, the number of parallel interactions the coalition should
use according to the optimal search strategy (denoted by x  (s)) depends on the coalitions state. For
comparison purposes, notice that in any of the single agents separate search (i.e. the single agent
parallel search model) the optimal strategy is to constantly use 4 parallel interactions (as long as the
agents strategy is to resume the search).

x*(s)=4
{(1,2)}
x*(s)=5

{}
Vt(s)=0

x*(s)=5

Vt(s)=6

{(1,1)}
Vt(s)=4

x*(s)=0
{(1,2),(2,1)}
Vt(s)=7.8
x*(s)=0

x*(s)=4
{(2,1)}
Vt(s)=6

{(2,2)}
Vt(s)=8

Figure 7: Optimal strategy and potential transitions between states in a simple B2C market

8.2 Finite Decision Horizon
In this section we demonstrate some properties of the PCS model variant where the coalition is
given a finite decision horizon. First we explore the influences of the cost of conducting an addi29

fiM ANISTERSKI , S ARNE , & K RAUS

tional interaction, c parallel , on an agents expected utility. Figure 8 depicts the results obtained from
varying c parallel between 0.01 and 0.3. In this figure we use the same environment used for Figure
3 (Environment 5), where  = 0.1, cbase = 0.2 with a decision horizon of two search rounds. As
the cost of conducting an additional interaction c parallel decreases, the coalitions expected utility
using the parallel models PSC and SAPS increases and its superiority over the sequential models
increases. Moreover as shown in Figure 8 the parallel model outperforms the sequential model even
when c parallel  cbase (e.g., for c parallel = 0.2). Recall that when c parallel  cbase the parallel technique does not improve the expected utility when having an infinite decision horizon (as suggested
in Proposition 3).
9.6

utility

utility

9.6

9.1

9.1

8.6

1

2
8.1

PCS

SAPS
8.1

SAPS

7.6
7.1

1 PCS
2

8.6

7.6

4

SASS

3

7.1

SCS
4

6.6

6.6

3
6.1
0.01

SASS

SCS
6.1
0.06

0.11

0.16

0.21

0.26

0.01

0.06

0.11

0.16

0.21

0.26

cost of parallel search

Figure 8: The average expected utility per buyer agent as a function of c parallel when the coalition should
terminate its search within the two next rounds.

Another factor that affects the expected utility is the decision horizon, represented by the value
of the parameter r. Figure 9 presents the coalitions expected utility as a function of r. In this figure
we used the same environment that we used in Figure 8, where we set the cost of each additional
search, c parallel = 0.2. As observed in Figure 9, in all search models, the earlier the coalition should
terminate its search the smaller its expected utility. Moreover, the expected utility improvement
obtained in the PCS (in comparison to the SCS) and SAPS (in comparison to the SASS) models
increases as r decreases. In this case the ability to conduct parallel interactions compensates for the
small number of search rounds that the coalition can conduct. Note that as r increases, the average
expected utility per buyer agent converges to the average expected utility per buyer agent in the
infinite decision horizon model.
Finally, Figure 10 depicts the optimal number of interactions for a coalition (in the PCS model)
or an agent (in the SAPS model) at the beginning of the search, as a function of r. For this figure
we used the same environment parameters that were used in Figure 9. As expected the number of
parallel interactions increases as r decreases.
As we have demonstrated throughout this section, the PCS outperforms the sequential cooperative search. Generally, the magnitude of improvement depends on the size of the domain, the search
cost structure, and the different utility functions used.

30

fiE NHANCING C OOPERATIVE S EARCH

utility
2

7.7

utility

PCS

1

8.2

WITH

SAPS

4

SASS

PCS
1
3

8.3

SCS

SAPS
2

7.2
6.7

C ONCURRENT I NTERACTIONS

4

7.8

SASS

6.2
7.3

5.7
5.2

6.8

3

4.7

SCS

4.2

6.3

3.7
3.2

5.8
1

5

9

13

1

4

7

10

13

limit on number of search rounds- r

Figure 9: The average expected utility per buyer agent as a function of the limit of the number of search

optimal # of interactions

rounds, r

(1) Coalition in B2C

8

(2) Coalition in C2C
6

3

(3) Agent A1
2
(4) Agent A2

4

4
1
2

0

1

5

9

limit on number of search rounds- r

13

Figure 10: Optimal number of interactions as a function of the decision horizon, r

9. Discussion and Conclusions
The capability of using parallel interactions as part of a search process is inherent in the infrastructure of autonomous information agents. When using the cooperative parallel search, the coalition
should use a new strategy, different in its structure in comparison to the optimal strategy used in the
cooperative sequential search and in a single agents parallel search. As expected, the use of the new
model has the potential of significantly improving the coalitions expected utility as demonstrated
in the previous section. Furthermore, we emphasize that the coalitions expected utility will never
decrease when using our proposed mechanism in comparison to pure sequential cooperative search.
This is mainly because the suggested algorithm will converge to one interaction at a time strategy,
as used in the sequential cooperative search (which is a specific case of our model) in the case where
31

fiM ANISTERSKI , S ARNE , & K RAUS

maintaining more than a single interaction in some of the world states is not favorable. Obviously
if the search cost is linear and depends solely on the number of interactions being maintained, then
there is no use for the coalition to increase the number of sellers with whom it interacts in a given
search round. Nevertheless, scenarios in which the coalitions search cost combines additional fixed
components or non-linear dependency on the number of interactions maintained are much more realistic (Sarne & Kraus, 2005). In these scenarios the parallel cooperative search yields large benefits
for searchers.
While the parallel cooperative search weakly dominates the sequential cooperative search, it
does not necessarily dominate autonomous search (where the agents search by themselves instead
of cooperatively). The decision of whether to use the parallel cooperative search autonomously
depends on the amount of coalition overhead costs induced by the cooperative search. Therefore,
having the computational means developed in this work enables the agents to identify fruitful opportunities for searching cooperatively. Generally, the introduction of the parallel cooperative search
model substantially increases the number of scenarios in which agents will prefer to search cooperatively.
The novelty of the analysis given in this paper is threefold. First it supplies us with a better
understanding of the space of opportunities, dividing it into improving and non-improving areas.
Thus, instead of having dual simultaneous dependencies between states we can now define a single
directional dependency for each pair of states. Second, it supplies a bound for the optimum number
of parallel interactions that the coalition uses in each state in its optimal strategy. Third we represent the parallel search as a sequential process, without breaching any of the model assumptions.
These three features allow us to overcome the main complexity associated with the attempt to solve
the problem as a set of equations and proffer a finite algorithm that is polynomial in the number
of parallel interactions (rather than the brute-force algorithm which is exponential in the number
of parallel interactions) and inevitably reaches the optimal strategy. Moreover, we provide a comprehensive analysis of the parallel model for extracting the optimal search strategy given a finite
decision horizon. Here, as we illustrate, the coalition can benefit significantly more than the infinite
decision horizon from the integration of parallel interactions into the cooperative search. The PCS
model (both in its finite and infinite decision horizon forms) is general and can be applied to any
coalition regardless of the cost function and the preferences used by its agents. Further adaptation
to additional markets (other than C2C and B2C) can be achieved by appropriately modifying the
allocation function used for assigning each agent with one of the opportunities found along the
search.
The focus of this research is on finding the optimal search strategy for the coalition, given its
structure, the opportunity distribution, and the reported members preferences. It treats the coalition as a unified entity sharing a common goal (to maximize the sum of its members utilities).
Nevertheless, there are various, important aspects of coalition formation, in the context of cooperative search, that do not always correlate with the assumptions used and should therefore be further
addressed. For example, there may be an incentive for coalition members to misreport their preferences when side-payments are used. Alternatively, agents may be able to form side coalitions,
and free-ride actively searching coalitions by having only a single agent as a member of the actively
searching coalition (that member would act as a spy for the side coalition). Moreover, there
can be environments where agents may face tight budget constraints that could be violated within
coalition side-payments. The analysis of these important issues is based on the ability to properly
derive the coalitions utility given any specific setting. This paper supplies this functionality, laying
32

fiE NHANCING C OOPERATIVE S EARCH

WITH

C ONCURRENT I NTERACTIONS

the foundation and enabling such research. Future work should encompass and extend the scope of
research to include these additional topics associated with the coalition formation process, such as
the coalition stability and the division of payoffs between the coalition members. Other important
extensions should include the relaxation of some assumptions in the underlying opportunity model
(e.g., different deadlines on different opportunities).

Acknowledgments
This work was supported in part by NSF grant no IIS0705587 and ISF. Kraus is also affiliated with
UMIACS.

Appendix A. A Summary of Notations
Notation

Meaning

B = (B1 , B2 , ..., Bk )

The set of the attributes defining each of the potentially available opportunities in the market, where each attribute B i can be
assigned a value from the finite set (b imin , ..., bimax ).
The space of potential opportunity types the coalition may encounter.
A coalition of agents.
Agent a j s utility from opportunity type ~o.
The search cost associated with having a coalition of n agents
maintaining w simultaneous interactions with seller agents.
A function that maps a given set of opportunities  known to the
coalition members in A in a way that the aggregated agents utility
is maximized
The state s of a coalition A acquainted with a set  known of known
opportunities.
The set of all possible states of a coalition A throughout its search.
The immediate utility for the coalition if it terminates the search
given a set of known opportunities  known .
The coalitions expected utility when using w parallel interactions
when in state s.
The immediate utility obtained, if the coalition decides to terminate the search at state s.
The number of parallel interactions that the coalition conducts at
state s according to its optimal strategy.
The collection of all w-sized sets of opportunities that can be produced in the environment the coalition operates.

Op
A
U j (~o)
c(w, n)
alloc(known )

state(known )
SA
Vt (known ):
V (s, w)
V (s, 0)
x (s)
w

33

fiM ANISTERSKI , S ARNE , & K RAUS

Notation
pw (w )
simprovew
sstayw
Ss
pstay (s, w)
wsmax
winit
max
V new (s, k)

r
x (s, r)
V  (s, r)

V (s, w, r)

Meaning
The probability of encountering a specific set of opportunities  w ,
when maintaining w random interactions with seller agents.
The collection of all w-sized sets of opportunities,  w , that change
the coalitions current state s.
The collection of all w-sized sets of opportunities,  w , that does
not change the coalitions current state s.
The set of all states belonging to SA , sorted according to their
termination utilities Vt (s).
The probability the agent stays at the same state s after conducting
w parallel interactions.
The upper bound for the optimal number of parallel interactions
to be used when the coalition in state s.
The upper bound for the optimal number of parallel interactions
to be used when the coalition begins its search (s = {}).
The coalitions expected utility obtained by potentially reaching
new states (e.g. different than s) after executing k parallel interactions (without incorporating the search cost of conducting these
k interactions), assuming future strategy uses x  (s0 ) in each new
future state s0 .
Finite Decision Horizon
The maximum number of search rounds that the coalition can
conduct.
The coalitions optimal strategy at state s, if it needs to terminate
its search within the next r search rounds.
The expected utility of a coalition restricted to maximum r further
search rounds, when reaching a state s assuming it acts optimally
throughout its search.
The coalitions expected utility when it conducts w interactions
in the next round and has to terminate its search within the next r
rounds.

References
Avis, D., & Lai, C. (1988). The probabilistic analysis of a heuristic for the assignment problem.
SIAM J. Comput., 17(4), 732741.
Bakos, Y. (1997). Reducing buyer search costs: Implications for electronic marketplaces. Management Science, 42(12), 167692.
Baron, D. P., & Ferejohn, J. A. (1989). Bargaining in legislatures. American Political Science
Review, 83(4), 11811206.
Benhabib, J., & Bull, C. (1983). Job search: The choice of intensity. J. of Political Economy, 91(5),
747764.

34

fiE NHANCING C OOPERATIVE S EARCH

WITH

C ONCURRENT I NTERACTIONS

Breban, S., & Vassileva, J. (2001). Long-term coalitions for the electronic marketplace. In B.
Spencer, ed., Proceedings of E-Commerce Applications Workshop.
Burdett, K., & Malueg, D. A. (1981). The theory of search for several goods. Journal of Economic
Theory, 24, 362376.
Carlson, J. A., & McAfee, R. P. (1984). Joint search for several goods. Journal of Economic Theory,
32, 337345.
Choi, S., & Liu, J. (2000). Optimal time-constrained trading strategies for autonomous agents. In
Proceedings of MAMA-2000, pp. 1113.
Dias, M. (2004). TraderBots: A New Paradigm for Robust and Efficient Multirobot Coordination in
Dynamic Environments. Ph.D. thesis, Robotics Institute, Carnegie Mellon University.
Gal, S., Landsberger, M., & Levykson, B. (1981). A compound strategy for search in the labor
market. International Economic Review, 22(3), 597608.
Gatti, J. (1999). Multi-commodity consumer search. Journal of Economic Theory, 86(2), 219244.
Hart, P. E., Nilsson, N. J., & Raphael, B. (1968). A formal basis for the heuristic determination of
minimum cost paths. IEEE Transactions on Systems, Science and Cybernetics, 4(2), 100107.
Ito, T., Ochi, H., & Shintani, T. (2002). A group-buy protocol based on coalition formation for
agent-mediated e-commerce. International Journal of Computer and Information Science
(IJCIS), 3(1), 1120.
Kahan, J., & Rapoport, A. (1984). Theories of Coalition Formation. Hillsdale, NJ:Lawrence Erlbaum Associates.
Keeney, R., & Raiffa, H. (1976). Decisions with Multiple Objectives: Preferences and Value Tradeoffs. New York, US:John Wiley & Sons.
Kephart, J., & Greenwald, A. (2002). Shopbot economics. JAAMAS, 5(3), 255287.
Lermann, K., & Shehory, O. (2000). Coalition formation for large scale electronic markets. In
Proceedings of ICMAS-00, pp. 167174.
Li, C., Rajan, U., Chawla, S., & Sycara, K. (2003). Mechanisms for coalition formation and cost
sharing in an electronic marketplace. In Proceedings of ICEC-03, pp. 68  77.
Lippman, S., & McCall, J. (1976). The economics of job search: A survey. Economic Inquiry,
14(3), 155189.
Manisterski, E. (2007). Protocols and Strategies for Agents Teamwork. Ph.D. thesis, Department of
Computing Science, Bar Ilan University.
McMillan, J., & Rothschild, M. (1994). Search. In Aumann, R. J., & Sergiu Hart, A. (Eds.),
Handbook of Game Theory with Economic Applications, pp. 905927. Elsevier.
Morgan, P. (1983). Search and optimal sample size. Review of Economic Studies, 50(4), 659675.
Morgan, P., & Manning, R. (1985). Optimal search. Econometrica, 53(4), 923944.
Rothschild, M. (1974). Searching for the lowest price when the distribution of prices is unknown.
Journal of Political Economy, 82(4), 689711.
Sandholm, T., Larson, K., Andersson, M., Shehory, O., & Tohme, F. (1999). Coalition structure
generation with worst case guarantees. Artificial Intelligence, 111(1-2), 209238.
35

fiM ANISTERSKI , S ARNE , & K RAUS

Sarne, D., & Kraus, S. (2003). The search for coalition formation in costly environments. In
Proceedings of CIA-03, pp. 117136.
Sarne, D., & Kraus, S. (2005). Cooperative exploration in the electronic marketplace. In Proceedings of AAAI-05, pp. 158163.
Shehory, O., & Kraus, S. (1998). Methods for task allocation via agent coalition formation. Artificial
Intelligence, 101(1-2), 165200.
Stigler, G. (1961). The economics of information. Journal of Political Economy, 69(3), 213225.
Talukdar, S., Baerentzen, L., Gove, A., & de Souza, P. S. (1998). Asynchronous teams: Cooperation
schemes for autonomous agents. Journal of Heuristics, 4(4), 295321.
Tsvetovat, N., Sycara, K., Chen, Y., & Ying, J. (2000). Customer coalitions in electronic markets.
In Proceedings of AMEC-00, pp. 121138.
Wang, Y., Makedon, F., & Ford, J. (2004). A bipartite graph matching framework for finding correspondences between structural elements in two proteins. In Proceedings of EMBC-04, Vol. 42,
pp. 297275.
Yamamoto, J., & Sycara, K. (2001). A stable and efficient buyer coalition formation scheme for
e-marketplaces. In Proceedings of Agents-01, pp. 576583.

36

fiJournal of Artificial Intelligence Research 32 (2008) 757791

Submitted 01/08; published 08/08

Compositional Belief Update
James Delgrande
Yi Jin

jim@cs.sfu.ca
yij@cs.sfu.ca

School of Computing Science
Simon Fraser University, Burnaby, BC, Canada V5A 1S6

Francis Jeffry Pelletier

jeffpell@sfu.ca

Departments of Philosophy and Linguistics
Simon Fraser University, Burnaby, BC, Canada V5A 1S6

Abstract
In this paper we explore a class of belief update operators, in which the denition of the
operator is compositional with respect to the sentence to be added. The goal is to provide an
update operator that is intuitive, in that its denition is based on a recursive decomposition
of the update sentences structure, and that may be reasonably implemented. In addressing
update, we rst provide a denition phrased in terms of the models of a knowledge base.
While this operator satises a core group of the benchmark Katsuno-Mendelzon update
postulates, not all of the postulates are satised. Other Katsuno-Mendelzon postulates can
be obtained by suitably restricting the syntactic form of the sentence for update, as we show.
In restricting the syntactic form of the sentence for update, we also obtain a hierarchy of
update operators with Winsletts standard semantics as the most basic interesting approach
captured. We subsequently give an algorithm which captures this approach; in the general
case the algorithm is exponential, but with some not-unreasonable assumptions we obtain
an algorithm that is linear in the size of the knowledge base. Hence the resulting approach
has much better complexity characteristics than other operators in some situations. We
also explore other compositional belief change operators: erasure is developed as a dual
operator to update; we show that a forget operator is denable in terms of update; and
we give a denition of the compositional revision operator. We obtain that compositional
revision, under the most natural denition, yields the Satoh revision operator.

1. Introduction
A knowledge base is typically not a static entity, but rather evolves over time. New information may be added, and old or out-of-date information may be removed. A fundamental
issue concerns how such change should be managed. A major body of research addresses this
question via the specication of rationality postulates, or standards that a change operator
should satisfy. These postulates describe belief change at the knowledge level, independent
of how beliefs are represented and manipulated. There are various rationales for motivating a change in an evolving knowledge base, and these diering rationales have been seen
as calling for dierences in the background knowledge-level postulates. For example, one
may think that some alteration in the world has occurred, with the result that we should
update the knowledge bases representation of the world in some appropriate way. Or, we
may think that our previous sources of information were fallible or incomplete and that we
now have better, more accurate information about the world. So, in this case we should
revise our beliefs. Another motivation might be to merge already-existing stores of beliefs,
c
2008
AI Access Foundation. All rights reserved.

fiDelgrande, Jin, & Pelletier

without giving any a priori preference to one or the other of the belief sets, but aiming to
achieve a balanced resolution of conicts. Such a merging might be used to combine the
belief states of dierent agents, so as to come up with a joint course of action based on
some sort of all things considered assimilation of the knowledge and preferences of the
agents that are involved. And we can also imagine a linguistic reform, so that a concept (or
rather, the associated word) was no longer to be used. In such a case one might say that
the users forgot about this concept/word.
These dierences in motivation have led to specic dierences in the sorts of postulates
that are associated with the dierent motivations. Initially, in the AGM approach (Alchourron, Gardenfors, & Makinson, 1985; Gardenfors, 1988), standards for belief revision
and contraction functions were given, wherein it was assumed that a knowledge base is
receiving information concerning a static1 domain, and that it is the increased amount or
accuracy of information that is responsible for the changes in the knowledge base. Subsequently, Katsuno and Mendelzon (1992) explored a distinct notion of belief change, with
functions for belief update and erasure, wherein an agent changes its beliefs in response to
what it perceives as changes in the environment. The concept of forget goes back to George
Boole (1854), but was reintroduced in the work of Lin and Reiter (1994) and Lin (2001) as
a way to characterize how an agent may bring its knowledge base up-to-date, by forgetting
about facts that are no longer relevant and in such a way as to not aect any possible
future actions. This approach is syntactic in nature: it deals with the issue of removing
facts by removing the ability to describe the facts.2 Finally, the notion of knowledge base
merging was introduced as a generalization of the long-standing problem of information
sharing between databases, where dierent databases might contain conicting information
(see Bright, Hurson, & Pakzad, 1992, for a survey). With the work of Revesz (1993), there
came an interest in constructing a merged knowledge base that best represents the information in a set of other knowledge bases. One use for this was thought to be a way of
determining a course of action that best represents the desires and goals of a divergent
set of knowledge bases, thereby forming a group-level, all-things-considered knowledge base.
The formal properties of merging have been discussed in previous works (e.g., see Lin &
Mendelzon, 1998; Konieczny & Pino Perez, 1998; Everaere, Konieczny, & Marquis, 2007).
The distinctions between the formal properties of the dierent types of change were
brought out in each of the papers after the initial AGM publications; for instance, Katsuno
and Mendelzon (1992) compared update with revision; Konieczny and Pino Perez (1998)
compared merging with revision; Nayak et al. (2006) compared forgetting with update.
Some of the postulates suggested by the initial authors of these dierent conceptions of
belief change have been challenged by other writers. And since our own approach towards
update conicts with some of Katsuno and Mendelzons postulates, we wish to show that
1. Note that static does not imply with no mention of time. For example, one could have information
in a knowledge base about the state of the world at different points in time, and revise information at
these points in time. Thus, belief revision is also applicable to the situation where an agent investigates
a past event and tries to reason about what was the real state of the world when this event took place.
Further considerations on how revision and update are interrelated are in the work of Lang (2006).
2. Nayak, Chen, and Lin (2006) described this difference thus: While belief erasure purports to answer the
question What should I believe if I can no longer support the belief that the cook killed Cock Robin?,
forgetting purports to answer the question What should I believe if Killing was a concept not afforded
in my language?.

758

fiCompositional Belief Update

this is not, by itself, a reason to reject our theory  every theory has met with objurgation
concerning its foundational postulates.
Although our focus in this paper is with update  and hence with the postulates given
by Katsuno and Mendelzon (1992) and the objections related to these postulates  we
believe that considerations similar to the ones we bring forward in this arena would hold
with respect to the other sorts of belief change postulates. That is, we think that the
rationale we have for imposing a compositionality constraint on belief update should be
brought to bear on the cases of belief revision, belief merging, and forgetting.
The knowledge level specications of these types of belief change allow for dierent ways
to implement any of them. Various researchers have proposed specic change operators for
belief revision (Borgida, 1985; Dalal, 1988; Satoh, 1988), belief update (Forbus, 1989; Weber, 1986; Winslett, 1988), belief merging (Subrahmanian, 1994; Konieczny, 2000; Everaere,
Konieczny, & Marquis, 2005), and forgetting (Lang, Liberatore, & Marquis, 2003; Nayak
et al., 2006). These approaches are formulated in terms of the distance between models
of the knowledge base and models of a sentence for revision or update. In general there
has been less work dealing with systems that may be readily implementable (but see, e.g.,
Williams, 1996; Delgrande & Schaub, 2003).
In this paper we develop a specic update operator where the operator is intended to be
compositional, in that an update    can be expressed recursively in terms of the syntactic
structure of . Thus, if a knowledge base is to be updated by a disjunction  = a  b,
the idea is that this update will be a function of the update by a in a certain combination
with the update by b. The update of the knowledge base by a conjunction  = a  b will
also be a function (a dierent one) of the update by a in combination with the update by
b. The goal is to arrive at an operator whose results are intuitive, in that its denition
is based on a recursive decomposition of a formula; hence the (generally abstract) notion
of update will be anchored in part in a more familiar computational setting. Second, the
hope is that these operators will be eciently implementable, at least in some cases, by
exploiting restrictions to the syntactic form of the formula. The focus here is on the form of
the formula for update; presumably the approach described may be combined with one in
which the knowledge base is itself divided into relevant and irrelevant parts for an update
(Parikh, 1999).
These goals are generally realised. First, the operators have reasonable properties: many
of the Katsuno and Mendelzon benchmark properties are satised, including those deemed
essential by Herzig and Ri (1999). While we dont obtain full irrelevance of syntax, we
do obtain weaker results in this regard; as well we show how irrelevance of syntax can be
obtained by restricting the syntactic form of the sentence for update. The approach is
also related to other approaches in the literature, and hence serves to establish some links
between approaches. In fact, the family of compositional update operators obtained by
imposing various syntactic restrictions can be regarded as constituting a family of operators
of which Winsletts standard semantics makes up the most basic nontrivial approach. As
well, the general approach to update presented here can capture the forget operator (Lin &
Reiter, 1994; Lang et al., 2003; Nayak et al., 2006), and so in a certain sense can be regarded
as generalizing forget. We also dene a revision operator using the obvious denition for
such an operator; it proves to be the case that this operator corresponds with the revision
operator in the work of Satoh (1988).
759

fiDelgrande, Jin, & Pelletier

The approach leads to a straightforward algorithm for implementing these operators.
This algorithm is ecient, compared to the model-based denition of this and other distancebased operators. For a knowledge base in disjunctive normal form, the size of the knowledge
base contributes only a linear factor to the overall complexity. As well, further eciency is
obtained when the size of the input sentence is bounded by a constant.
The next section reviews belief revision, update, forgetting, and merging, and describes
two specic approaches to update. The section following describes our approach, after
which, in the next section, we give a discussion and analysis. The last section contains
concluding remarks; proofs of theorems are given in an Appendix.

2. Background
As described, our goal is to introduce a compositional method of carrying out belief change.
But since part of our overall goal also is to examine the place of a compositional belief change
operation in all the various arenas where this can take place, we start by outlining some of
the details for each of these dierent conceptions that motivate belief change, along with
some motivational considerations and some areas where the dierent types of belief change
part ways. These operators were introduced implicitly, by means of a set of postulates that
any legitimate such operator was required to obey. However, in all these areas there has
been some dispute concerning the correctness of the various postulates, and we mention
some of these as we proceed, since our own approach in the case of update does not obey all
the standard postulates for update. We start with the historically earlier case of revision
before moving to our central concern of update. These are followed by short expositions
concerning forgetting and merging.
2.1 Formal Preliminaries
We consider a propositional language L, over a nite set of atoms, that is, propositional
letters, P = {, a, b, c, . . . }, and truth-functional connectives , , and . Where convenient,  and  are also used, and are considered as being introduced by denition. We
use  for logical equivalence; that is,    is an abbreviation for  (  ). Lits is the
set of literals P  {l | l  P}. In particular,  is also denoted as . A set of literals
 is consistent just if  6  and for no atom p  P do we have p, p  . For a literal
l, we use l to denote l if l  P, or l  P if l = l. Similarly, for a set of literals , we
use  to denote the set {l | l  }. The expression atom() denotes the set of atoms in
formula . An interpretation  of L is a maximal consistent set of literals, i.e.,    and
for every other p  P precisely one of p  , p   holds. A model of a sentence  is an
interpretation that makes  true, according to the usual denition of truth. M od() denotes the set of models of sentence . We also make use of the notation M odL() to denote
the set of models of sentence  over the language of  (that is to say, over the language
atom().) For interpretation  we write  |=  to mean  is true in . For interpretation
 and set of literals , we dene    =  \ (  ). That is,    is the set of literals
in  but containing neither l nor l for each l  . For example, if  = {a, b, c} then
  {b, c} = {a}.
We denote the negation-normal form (in which negation applies to atoms only) of a
sentence  by nnf (). Similarly, we denote the conjunctive normal form and the disjunctive
760

fiCompositional Belief Update

3
normal form of  by cnf ()
W and dnf () respectively. For a setVof sentences  (which will
always be nite), we use  to denote the disjunction and  the conjunction of the
sentences in . Proofs will often be based on the structure of a formula, specically on
the depth of a formula; for formula , the depth of , depth() is the maximum nesting of
connectives in . Hence depth(a  (b  c)) = 3.
Later we make use of the notion of the prime implicants of a sentence. A consistent set
of literals  is a prime implicant of  i:    and for any    we have  6 .4 In the
limiting case where  , we take the (sole) prime implicant of  to be {}.

2.2 Belief Revision and Contraction
In the seminal approach of AGM (Alchourron et al., 1985), postulates are proposed to
constrain belief revision. In this approach, a knowledge base K is assumed to be a belief
set, a set of sentences closed under logical consequence. The revision of a belief set by a
formula, K  , is a new belief set in which the formula  is believed. The interesting case is
that in which  is initially believed, and so to attain a consistent belief set (assuming that
 is satisable), some beliefs have to be dropped. Exactly which beliefs must be dropped
is not stipulated in the AGM approach; however, constraints in the form of postulates
that govern what are seen as legitimate revision operators are given. In contrast, in their
development of belief update Katsuno and Mendelzon (1992) represented the knowledge
base by a formula in some language L. Hence, in this paper we also express things in terms
of postulates phrased in terms of formulas, rather than belief sets.
The following R-postulates comprise Katsuno and Mendelzons reformulation of the
AGM revision postulates, where  is a function from L  L to L.
(R1)     .
(R2) If    is satisable, then       .
(R3) If  is satisable then    is also satisable.
(R4) If 1  2 and 1  2 then 1  1  2  2 .
(R5) (  )      (  ).
(R6) If (  )   is satisable then   (  )  (  )  .
A dual operation, called contraction is also dened, in which a formula is deleted from
the knowledge base. This operation can be seen as governed by the C-postulates, again
using a Katsuno and Mendelzon formulation in terms of a function from L  L to L.
3. Of course for formula , there are many different but logically equivalent ways to express cnf () and
dnf (). We assume a fixed procedure for converting to cnf (or dnf), by converting to negation normal
form, and then distributing disjunctions over conjunctions (or vice versa for dnf), hence justifying the use
of the term the conjunctive (disjunctive) normal form of a formula, rather than a (disjunctive) normal
form.
4. The notion of prime implicant should not be confused with the dual notion of a prime implicate. A
prime implicate of a formula  is a clause, or disjunction of literals, , such that    but for any proper
subclause  of , we have  6  .

761

fiDelgrande, Jin, & Pelletier

(C1)     .
(C2) If  6  then     .
(C3) If 6  then    6 .
(C4) If 1  2 and 1  2 then 1  1  2  2 .
(C5) (  )    .
Revision and contraction are related in the AGM approach by what have come to be
known as the Levi and Harper identities. They may be expressed as follows (using formulas
rather than belief sets):
    (  )  

(1)

      (  ).

(2)

The rst case asserts that revising  by  corresponds to the contraction of  by  conjoined with . The second asserts that contracting  from  corresponds to the disjunction
of  with the result of  updated by .
Although this makes a nice picture, there have been various objections to some of the
presuppositions of the AGM model (e.g., the representation of belief states by theories, that
is, by innite sets of formulas) and to some of the postulates that are said to govern the
operations of revision and contraction (especially (C5), the postulate of recovery). Issues
involved with (C5) have been discussed by Fuhrmann, 1991; Tennant, 1997; Hansson &
Rott, 1998; Rott & Pagnucco, 1999, and others.
2.3 Belief Update and Erasure
The account of revision and contraction described in the preceding subsection is usually seen
as applying most straightforwardly to the case where one has a store of information about
an unchanging, static world but where new information about that world is received by
the agent, thereby forcing a change in the representation of this unchanging, static world.
But a dierent picture was put forward by Katsuno and Mendelzon (1992), where there was
a changing, dynamic world. In such a conception, the new information that is gathered
by the agent reects the idea that the world is dierent than it was when the knowledge
base was previously constructed. The sorts of changes to the knowledge base that are
required by this type of new information are seen as dierent from the sorts envisaged
when it is thought that changes to the knowledge base are only going to make its contents
successively more accurate. Although this simplistic distinction is not all there is to the
dierences between the two pictures (as we mentioned in Footnote 1), it has led to a large
body of work that does point to a dierent conception. Distinct operations that change
knowledge bases have been proposed: update, which makes changes to the knowledge base
given information concerning a change in the state of the world, and erasure, for removing
out-of-date information.
A formula is said to be complete just if it implies the truth or falsity of every other
formula. In the approach of (Katsuno & Mendelzon, 1992), update is a function  from
L  L to L satisfying the following U-postulates.
762

fiCompositional Belief Update

(U1)     .
(U2) If    then (  )  .
(U3) If  and  are satisable then so is   .
(U4) If 1  2 and 1  2 then (1  1 )  (2  2 ).
(U5) (  )      (  ).
(U6) If   1  2 and   2  1 then (  1 )  (  2 ).
(U7) If  is complete then (  1 )  (  2 )    (1  2 ).
(U8) (1  2 )    (1  )  (2  )
These postulates are not, however, uncontentious. Herzig and Ri (1999) discussed the
plausibility of the postulates given; they assert that U2, U5, and U6 are undesirable,
while U7 is unimportant. This leaves (according to the authors) U1, U3, U4, and U8 as
being desirable.
Erasure is also dened, in a manner analogous to the way we described how contraction
was related to belief revision. In both cases, some specied formula is not believed in the
result. The erasure of  from  is denoted  , and the formula  is not believed in the
resulting state. As with all our other operations, there is a set of postulates characterizing
erasure (given in Katsuno & Mendelzon, 1992). Update and erasure are also interdenable
by means of identities, analogous to the Levi and Harper identities, which related revision
and contraction:
    ( )  

(3)

     (  ).

(4)

The rst case asserts that update by  corresponds to erasing  along with the conjunction
with . The second asserts that erasing  from  corresponds to disjoining  with the result
of  updated by .
There have been various specic update (and revision) operators proposed based on the
distance between interpretations. We focus on two update operators, both due to Winslett.
The rst, the Possible Models Approach (PMA) of (Winslett, 1988) is a well-known example of an update operator satisfying the Katsuno and Mendelzon update postulates. The
second, the standard semantics of (Winslett, 1990) is a weak (in fact, arguably the weakest
reasonable) approach to update. We denote these operators by pma and ss respectively.
For  pma , we have that, for each interpretation w of , pma selects from the interpretations of  those that are closest to w. The update is determined by the set of these
closest interpretations. The notion of closeness between two interpretations w1 and w2 is
the Hamming distance, given as follows:
Definition 1 diff (w1 , w2 ) = The set of all propositional letters on which w1 and w2 differ.
763

fiDelgrande, Jin, & Pelletier

Interpretation w1 is not less close to w than w2 , w1 w w2 , just if diff (w, w1 )  diff (w, w2 ).
It follows that w is a partial order on interpretations. The w -minimal set with respect
to  is designated M in(M od(), w). From this we can specify the PMA update operator:
[
M od( pma ) =
M in(M od(), w).
wM od()

The update operator  ss  is dened so that for each model of , those models of 
that retain the truth values of atoms not in  are chosen. That is:
[
M od( ss ) =
{w2  M od() | diff (w1 , w2 )  atom()}
w1 M od()

The operator  ss  is the weakest reasonable update operator in the following sense
(Winslett, 1990): First, for an update  ss ,  is true in every model of  ss . Second,
every model of  over the language excluding atoms in  is a model of  ss  (again over
this restricted language). Moreover,  ss  consists of the maximal set of interpretations
that satises the preceding two properties. Hence in the update of  by , the truth values
of atoms in  but not in  are unaected by the update.
Example 1 (Katsuno & Mendelzon, 1992) Let L = {b, m} be the language of discourse. Let  = (b  m)  (b  m), and  = b. The interpretations of  are w1 = (b, m),
w2 = (b, m); and the interpretations of  are: w1 = (b, m), w2 = (b, m). Thus
diff (w1 , w1 ) = {b} and diff (w1 , w2 ) = {b, m}, hence w1 w1 w2 and w2 6w1 w1 , so
M in(M od(), w1 ) = {w1 }. Similarly, M in(M od(), w2 ) = {w2 }. Hence, ( pma )  b.
The same result obtains for ss .
For concreteness, take b to mean the book is on the oor, and m to mean the magazine
is on the oor. So  means that either the book or the magazine is on the oor, but not
both. A robot is ordered to put the book on the oor. Intuitively, at the end of this action
the book will be on the oor, and the location of the magazine will be unknown. Both
operators give this result.
Example 2 Let  = (b  m) and  = (b  m). Then ( pma )  (b  m), whereas
( ss )  (b  m).
Here, neither the book nor the magazine is on the oor. The robot is ordered to put at
least one of them on the oor. According to the pma operator, exactly one will be on the
oor after this action, while according to the ss operator, at least one will be on the oor.
2.4 Forget
While our focus is on a specic approach to update and erasure, we also relate our approach
to that of the forget operator. The notion of forgetting goes back to George Boole (1854),
though it has received more recent attention in Articial Intelligence by, e.g., Lin & Reiter,
1994; Lin, 2001; Lang et al., 2003; Nayak et al., 2006. In a propositional context, to forget
an atom, or set of atoms, is to remove all information concerning the atom or set of atoms.
764

fiCompositional Belief Update

It has been suggested (in Nayak et al., 2006) that forgetting corresponds to the removal of
literals or atoms from the language of discourse in the case of propositional forgetting (i.e.,
0-place predicate forgetting). In the more general case, it is seen as removing a predicate or
relation from the language, and hence removing any further consequences that might have
been due to this predicates presence.
Let [p/q] denote the formula  where all occurrences of atom p are replaced by q.
Then the usual denition for forgetting (again, going back to Boole) atom p in  is given
by [p/]  [p/]. In order to forget a set of atoms , one takes the disjunction of the
substitution of all 2|| combinations of ,  for elements of .
We have the following denitions. For single atoms we basically follow Nayak et al.
(2006); for sets of atoms we use the denition from (Lin & Reiter, 1994). To begin, the
p-dual of an interpretation  is the interpretation like  but where the truth value assigned
to p is changed to its negation. A set of interpretations is closed under p-duals just if, for
any interpretation  in the set, the p-dual of  is also in the set.
Definition 2 Given a set of interpretations  and atom p, the operator
least set of interpretations containing  and closed under p-duals.

U

(, p) yields the

Given this, we can dene forget for an atom and set of atoms, where the latter is dened
recursively in terms of the former:
Definition 3 Basis Case: Let  be a formula and p an atom. Then forget of p with respect
to  is given by:
]
M od(  p) =
(M od(), p)
= M od([p/]  [p/]).

Inductive Case: Let  be a formula and  = {p1 , . . . , pn } a set of atoms. Then forget
of  with respect to  is given by:
   = (  ( \ {pn }))  pn .
For example, a  (b  c)  a  a  (b  c)  a  b  c. (Given a knowledge base that has
stored that Alberta is in Canada and also that either Vancouver is in British Columbia or
Charlottetown is in Ontario, forgetting that Alberta is in Canada would yield that either
Vancouver is in British Columbia or Charlottetown is in Ontario. This would be the same
result if the initial knowledge base had that Alberta was not in Canada, but that either
Vancouver is in British Columbia or Charlottetown is in Ontario.) For another example,
(a  b)  a  . This last example illustrates that forget is distinct from erasure, since a
property of erasure is that if  does not imply  then ( )   (Katsuno & Mendelzon,
1992).
2.5 Belief Merging
Merging diers formally from the preceding three pictures of how knowledge bases are
changed. The preceding operators had a knowledge base and a sentence that may need to
occasion a change in the knowledge base. If one rephrases this in terms of agents, these
765

fiDelgrande, Jin, & Pelletier

other types of change postulate an agent, with a store of beliefs, who is now faced with a
new belief that needs to be accommodated. In the case of merging, however, we start with
many belief sets that need all to be dealt with in some way that yields the best, overall
single belief state. In terms of agents, again, we have here a number of agents, each with
a belief set, and we are trying to construct that belief set which best represents the total
beliefs of the community of agents. So, rather than being a function that maps a belief set
and a sentence onto a belief set, it is instead a function that maps a number of belief sets
into a single one. Following our earlier practice of representing belief sets by a single formula
(in the manner of Katsuno & Mendelzon, 1992), we can see that the earlier rationales for
belief change envision it as a function L  L  L, whereas merging envisions a function
L  L  . . .  L  L. Note that the general case allows for some of the knowledge bases on
this list to be identical to one another, thus the list is actually a multi-set (bag).
The goal in merging, then, is to construct, from a nite list of knowledge bases E,
some appropriate, single merged knowledge base. Despite this formal dierence from the
earlier three types of belief change, we nevertheless include a discussion here because of the
conceptual similarities that hold between merging and any of the other versions of belief
change. Indeed, it seems plausible to suggest that merging might be denable in terms of
the others, or maybe that it is some sort of generalization of the others. In these cases, our
considerations about compositionality of belief change operators may be relevant.
Definition 4 A knowledge set is a multi-set (bag) of knowledge bases.
V
Definition 5 If E is a knowledge set, then E is the conjunction of the formulas representing all the knowledge bases that are in E.
Konieczny and Pino Perez (1998, 2002) proposed the following M-principles to govern all
merging operators. A merge function  is a function from a knowledge set E to a knowledge
base (E) satisfying the following postulates, where  is multiset union.5
(M1) (E) is consistent
V
V
(M2) If E is consistent then (E) = E.

(M3) If E1 and E2 are knowledge sets such that E1  E2 , then (E1 )  (E2 )
(M4) If K1 and K2 are knowledge bases that are not mutually consistent, then (K1 
K2 ) 6 K1
(M5) (E1 )  (E2 )  (E1  E2 )
(M6) If (E1 )  (E2 ) is consistent, then (E1  E2 )  (E1 )  (E2 )
Some of these merging postulates have been contested: For example, Meyer (2000) argued
that M4 and M6 should be rejected. (He argues this on the grounds that there are many
plausible merging operations that do not obey these postulates).
5. For simplicity, we list the postulates of (Konieczny & Pino Perez, 1998), which do not include integrity
constraints.

766

fiCompositional Belief Update

A natural method for determining whether a formula  should be in the merged knowledge base is to determine whether it appears in the majority of the members of the knowledge set that is being merged (the merged knowledge base should allow the opinion of the
majority to prevail). Liberatore and Schaerf (1998) introduced the method of arbitration,
whereby the goal is to adopt as many dierent opinions as possible from the members of the
knowledge set (try to take as many diering opinions as possible into account). Konieczny
and Pino Perez (1998) proved that there is no arbitration operator (at least, not of the sort
that they characterize) that obeys M1  M6.6 The interplay between various merging
operations and the ability of an agent to hide, lie, or otherwise camouage its preferences
from other agents as they try to construct a merged knowledge base has been surveyed in
Everaere et al. (2007).

3. The Approach
This section discusses our approach. Following intuitions and motivation of the formal
approach, we introduce compositional update and, subsequently, erasure. We also consider
the notion of compositional belief revision, but conclude that, at least with respect to our
specic approach, there is no separate, distinct, notion of compositional revision. Analysis
of properties of these operators is covered in the next section.
3.1 Intuitions
Our goal is to dene update operators in a compositional fashion so that, for updating by
formula , update is dened in terms of the syntactic components of . The general idea
behind update is that for   , each model of  is replaced by the closest model(s) in 
(Katsuno & Mendelzon, 1992). In our approach, the notion of close for each model of  is
determined in part by the syntactic structure of . That is,  is recursively decomposed; the
resulting (base case) literals are used to determine models of the update by sets of literals;
and the results are combined depending on the connective(s) in .
Consider how this may be carried out. We are given a knowledge base  and a sentence
, and we wish to determine a new knowledge base where  is believed. For a base case,
 = l is a literal, and we wish to update the knowledge base  by literal l. If  implies l
then we need do nothing. If  does not imply l, then we wish to arrive at a knowledge base
in which l is believed. That is, we want to change the knowledge base only enough so that
it entails l. Clearly, we can do this by replacing each model  of  by the interpretation
  = (  {l})  {l}.7 Thus, we would have that every resulting interpretation entails l.
Consider next updating a knowledge base  by a conjunction of literals  = l1  l2 . A
knowledge base in which l1 l2 is believed will, obviously, be one in which every model of the
knowledge base entails both l1 and l2 . We carry this out by replacing each interpretation
  M od() with an interpretation   = (  {l1 , l2 })  {l1 , l2 }. There is a limiting case
that needs to be taken care of, where l1 is l2 . In this situation, there is no interpretation in
which l1 , l2 are true, and in this case   does not exist, reecting an attempt to update by
an inconsistent formula.
6. This forms a part of the rationale for Meyer (2000, 2001) to deny M4 and M6.
7. To be clear, if  |= l then   = ; and if  6|= l then   is like  but with l replacing its complement.

767

fiDelgrande, Jin, & Pelletier

To update a knowledge base  by a disjunction of literals  = l1  l2 , we want to
modify models of  so that at least one of l1 or l2 is true. Consider   M od() such that
 6|= l1  l2 . Then 1 = (  {l1 })  {l1 } is an interpretation that involves the least change
to  in which l1 is true, while 2 = (  {l2 })  {l2 } does the same for l2 . Arguably then 
should be replaced by 1 and 2 .
Last, we generalize the above considerations to deal with arbitrary formulas. So to
update by a disjunction of formulas, we recursively determine the update given by the
individual disjuncts and return the union of the resulting sets of interpretations.
3.2 A Compositional Update Operator
Based on the preceding intuitions, we dene an update operator c . We begin with some
preliminary denitions. In the following, UL is a function from an interpretation  and
nite set of formulas  to a set of interpretations. Informally,  is a model of the knowledge
base and  is a set of formulas resulting from the partial decomposition of a formula for
update. The value of UL is the set of interpretations closest to , according to . To ease
notation, in the case of a single formula we sometimes write UL(, ) for UL(, {}).
Definition 6 For interpretation  and finite   L, define UL(, ) as follows:
1. If   Lits then
UL(, ) =



{(  )  } if  6 

otherwise

2. If  = {  }   then UL(, ) = UL(, {, }   )
3. If  = {  }   then UL(, ) = UL(, {}   )  UL(, {}   )
4. If  = {(  )}   then UL(, ) = UL(, {, }   )
5. If  = {(  )}   then UL(, ) = UL(, {}   )  UL(, {}   )
6. If  = {}   then UL(, ) = UL(, {}   )
It is worth noting that the recursion steps of the above denition resemble closely the
procedure which we use to convert a formula to its disjunctive normal form. Before dening
update in terms of this operator, we rst investigate some of its properties. Foremost,
we need to show that UL is well-dened. That is, in specifying UL(, ), the denition
is phrased in terms of some member of ; it needs to be shown that the order in which
elements are selected in the recursion does not aect the result.
Theorem 1 UL is well-defined.
The next two results reect the inuence of the structure of a formula on the recursive
decomposition in the denition of UL.
V
Theorem 2 UL(, ) = UL(, nnf ( )).

V
Theorem 3 UL(, ) = UL(, dnf ( )).

768

fiCompositional Belief Update

Note that a similar result does not extend to conjunctive normal form. A counterexample
is given by the following:
UL(, {a  (b  c)}) = UL(, {a})  UL(, {b, c})
6= UL(, {a})  UL(, {a, c})  UL(, {b, a})  UL(, {b, c})
= UL(, {a, a  c})  UL(, {b, a  c})
= UL(, {(a  b), (a  c)})
= UL(, {(a  b)  (a  c)}).
We consider next a couple of fundamental properties of UL:
Theorem 4 For every    and w  UL(, ) we have w |= .
Theorem 5 UL(, ) =  iff   .
We next dene our update operator directly in terms of UL.
Definition 7
M od( c ) = {  |    UL(, {}),   M od()}.
Recall Example 1 in which  = b and  = (b  m)  (b  m). We have that M od( c
) = {  |    UL(, {}),   M od()} = {(  {b})  {b} |   M od()}. Thus,
M od( c ) = {{b, m}, {b, m}}, and so ( c )  b. This is the same result as we obtain
with both Winsletts approaches.8
For Example 2, where  = b  m and  = (b  m), we obtain M od( c ) =
{{b, m}, {b, m}}. In this case, our update operator behaves the same as pma , but differently from ss .
We can similarly dene an erasure operator via UL. To erase  from , and in analogy
to the Harper Identity, one can update by  and add the result to . Thus:
Definition 8
M od( c ) = M od()  {  |    UL(, {}),   M od()}.
We get the results:
Theorem 6
 c   ( c )  
 c     ( c ).
8. We note however that these approaches differ. Specifically, the PMA update operator satisfies all of the
KM postulates, whereas our operator does not; see Section 4 for details.

769

fiDelgrande, Jin, & Pelletier

3.3 Erasure
In Denition 8 we dened a dual to update, called erasure, directly in terms of UL. We
can equally well dene a function analogous to UL, call it EL, to directly dene an erasure
operator from rst principles. We do this now, toward such a denition of erasure. Briey,
our motivation is: if we want to erase  as a consequence of , then semantically we want
to add interpretations to the models of . If  corresponds to a single literal, then for
each model of  we would want to add an interpretation in which l was replaced by l. If 
corresponds to a conjunction, then  can be erased by erasing either of the conjuncts; if 
corresponds to a disjunction, then to erase  both disjuncts must be erased. By continuing
in this fashion we obtain the following denition:
Definition 9 For interpretation  and finite   L, define EL(, ) as follows:
1. If   Lits then
EL(, ) =



W
 6 
{(  )  } if

otherwise

2. If  = {  }   then EL(, ) = EL(, {}   )  EL(, {}   )
3. If  = {  }   then EL(, ) = EL(, {, }   )
4. If  = {(  )}   then EL(, ) = EL(, {}   )  EL(, {}   )
5. If  = {(  )}   then EL(, ) = EL(, {, }   )
6. If  = {}   then EL(, ) = EL(, {}   )
The following results are analogous to Theorems 2 and 3; note the occurrence of cnf in
Theorem 8, in contrast to dnf in Theorem 3.
V
Theorem 7 EL(, ) = EL(, nnf ( )).
V
Theorem 8 EL(, ) = EL(, cnf ( )).
We can now directly dene an erasure operator

Definition 10 M od(


c )


c

in terms of EL:

= M od()  {  |    EL(, {}),   M od()}

Unsurprisingly, this notion of erasure and that given in Denition 8 are equivalent. We
show this by rst establishing the following result:
V
Lemma 1 For interpretation  and   L, we have EL(, ) = UL(, { })

From this, it follows that our notions of erasure as given via the Harper Identity, and by
direct denition via Denition 9 coincide:
Theorem 9  c   


c .

Hence we just use the symbol
well-denedness of c .9

c

for erasure. As a corollary, Theorem 9 also establishes the

9. That is, since UL is well-defined (Theorem 1), so is
equivalence.

770

c

(Definition 8) and hence so is


c

by the above

fiCompositional Belief Update

3.4 Revision
In this section we consider extending the compositional approach to belief revision. To
begin, it might be pointed out that there is nothing about the underlying motivation that
makes c an update operator, and this point suggests that c might also be regarded as a
revision operator, albeit with weak properties. However, regardless of intuitions, the recursive decomposition implicit in Denition 6 yields an operator with update-like properties,
in that for sentence for update , one eectively deals with the models of the disjuncts in
dnf (). For revision, in contrast, the intuition is that one deals with models of  that are
(in some sense) closest to those of the knowledge base . Hence, the operator c is not
really appropriate as a revision operator.
This suggests a possibly-feasible approach to dening compositional revision: To dene
a revision  , one rst uses the operator c to nd a candidate set of models of , and then
employs some distance function to determine the subset of these models that are closest to
models of  as a whole. That is, for formulas , , an update of  by  is dened (in one
fashion or another) with respect to all the models of . For revision in contrast, a denition
of the revision of  by  makes reference to only a subset of the models of , those that are
closest (in some sense) to the models of . In this sense then, update is a logically weaker
operator than revision. Thus a revision operator can be dened with respect to  and  by
rst applying some (compositional) update operator to get a candidate set of models of .
This set can then be ltered, by removing those models that are not of minimal distance
to the closest models of . So depending on the notion of distance employed, one might
expect to obtain dierent revision operators for a given compositional update operator.
There are two common notions of distance that are used for model-based belief change,
one based on set containment and the other on cardinality. In the rst case, for formulas
, , dene
min (, )

=

min ({M1 M2 | M1  M od(), M2  M od()}),

where for sets A and B, AB is the symmetric dierence of A and B. Satohs (1988)
revision operator  S  is dened as follows.
Definition 11
M od( S ) = {w  M od() | w  M od(), ww  min (, )}.
For example, let  = abc and let  = a(bc). Then S  = (abc)(abc).
We can dene a corresponding compositional revision operator as follows:
Definition 12
M od(  ) = {w | w  UL(w, {}), where w  M od(), ww  min (, )}.
However, it turns out that this revision operator in fact coincides the Satoh revision operator:
Theorem 10      S .
771

fiDelgrande, Jin, & Pelletier

It follows as a straightforward corollary that if we use a distance metric based on the number of diering propositional symbols between two interpretations, we obtain the revision
operator of (Dalal, 1988).10 So in the obvious approaches to compositional revision, we
do not obtain new revision operators; which is to say, the recursive decomposition in the
denition of UL does not serve to select among models of  in any interesting sense with
respect to revision.
However, these considerations do lead to one interesting result, and that is they point
the way to algorithms that may more eciently compute the Satoh or Dalal revision: To
compute the Satoh revision for example, one can use Denition 6 to determine a relevant
subset of models of , and then use min (, ) to determine the closest subset of these
models to the set of models of . As we discuss in Section 5, this initial ltering of models
of  may be done eciently in certain syntactically-restricted cases.

4. Analysis of Compositional Update and Erasure
To start, we consider which of the Katsuno-Mendelzon update postulates our operator
satises. We do not consider the set of corresponding compositional erasure postulates,
since the results are analogous to those of the update postulates, and so are of limited
additional interest. After considering the update postulates, we further explore the update
and erasure operators, including properties resulting from the restriction of the syntactic
form of the formula for update, and a comparison to related approaches.
Theorem 11 c satisfies U1, U3, U5, U7, U8.
For a counterexample to U2, consider the rst example given above, illustrating the
approaches of Winslett, where for  c  we have  = (b  m)  (b  m) and  = b  m.
In our approach, for the update (as given in Denition 7) the rst disjunct of  viz., b,
yields interpretations {b, m} and {b, m} and the update by the second disjunct, m, gives
interpretations {b, m} and {b, m}. Hence c (bm) is characterized by the interpretations
{b, m}, {b, m}, and {b, m} and so we get  c (b  m)  (b  m). U2 would dictate
that the result be ; however, the above example suggests that U2 is problematic in the
context of update. To borrow an example from other works (Herzig & Ri, 1999; Brewka
& Herzberg, 1993), suppose an agent believes p (that a certain coin shows heads). Now
the world changes because of a toss of this coin (where the agent does not see the result).
Letting p be that the coin shows tails, we note that the agent should believe (p  p). Yet
note that p  (p  p); so U2 would stipulate that p  (p  p) should be p, contrary to what
we want. The operator c , on the other hand, includes an additional model. This appears
to make sense, because by updating by b  m we are really telling the knowledge base that
the world has changed so that one of b  m or b  m or b  m is true. Thus, in this case
the update operator behaves like the Gricean belief change operator of Delgrande, Nayak,
and Pagnucco (2005), where the goal is to incorporate all and only the new information.
10. That is, for fixed formulas, any model of the Dalal revision is a model of the Satoh revision. Rephrasing
Definitions 11 and 12 for cardinality-based distance gives a result analogous to Theorem 10 for Dalal
revision. We omit the details.

772

fiCompositional Belief Update

We note that we can modify our c operator in a simple fashion to satisfy U2 as follows:11


if   

 c  =
 c  otherwise
But for our purposes, although U2 is indeed now satised, this modication sheds no light
on our original goal of investigating ramications of developing a compositional update
operator, and so we do not further pursue this modication.
We next consider a counterexample for U4. Although ((a  b)  b)  b, nonetheless
M od(ac ((ab)b)) = {{a, b}, {a, b}} while M od(ac b) = {{a, b}}. So U4 is not satised
since in our compositional approach parts of a sentence may provide implicit results not
explicit in the sentence. Consider (ab)(bc) to further illustrate this point. Updating
by this sentence is eected by updating by the individual components, viz., (a  b) and
(b  c). However, implicit in these parts is the fact that (a  c) is also true, and the
addition of this (implied) sentence would aect the result of the update. We consider this
behaviour further below.
A counterexample for U6 is given by the following. Let
 = ab
1 = (a  a)
2 = 
We have that
M od((a  b) c (a  a)) = M od()
But we also have:
M od((a  b) c ) = M od(a  b)
So we have a case where  c 1  2 and also  c 2  1 . Thus the antecedent conditions
of U6 are satised, but not  c 1   c 2 .
While c does not satisfy U4 (substitution of logical equivalents) in general, it does
satisfy some weaker conditions. First, our update obviously satises substitution of logical
equivalents in the rst argument of c . As well, in light of Theorems 2 and 3, if 1 and
2 share the same negation normal form or disjunctive normal form, then they may be
substituted one for the other as a formula for update. We summarize these results as
follows:
Observation 1
1. If 1  2 then (1 c )  (2 c ).
2. If nnf (1 ) = nnf (2 ) then ( c 1 )  ( c 2 ).
3. If dnf (1 ) = dnf (2 ) then ( c 1 )  ( c 2 ).
11. Borgida (1985) employed a similar definition with respect to a revision operator.

773

fiDelgrande, Jin, & Pelletier

Despite failing to satisfy some postulates (which, it can be noted, overlap with the
postulates that Herzig & Ri, 1999, think are undesirable), c does exhibit a nice property,
reecting the compositional nature of our operator, but which operators appearing in the
literature and satisfying the Katsuno and Mendelzon postulates fail to satisfy. The following
version of the disjunction property holds.
Theorem 12  c (1  2 )  ( c 1 )  ( c 2 )
Corollary 1 ( c 1 )  ( c 2 ) implies  c (1  2 ).
The corollary can be observed to be a strengthening of U7.
Our update operator satises those postulates deemed desirable by Herzig and Ri
(1999), with the exception of U4. As discussed above, U4 is not satised due to the
interaction of parts of a sentence. It would seem that if we could compile out the implicit
information in a sentence then we would obtain the full substitution of equivalents, as
expressed in U4. So, one way to satisfy U4 is to redene c so that we rst get this
information implicit in the interaction of the compositionally distinct parts of the update
sentence. We do this by dening operators that consider the set of prime implicants of a
sentence. We call this modied operator pi
c . Let P I() be the set of prime implicants of .
W
Definition 13  pi
P I()
c  =  c
Theorem 13 pi
c satisfies U4

Although pi
c satises U4, we now lose U7. A counter-example for U7 is given by
 = abcd
1 = (a  d)  (c  d)
2 = (a  d)  (c  d).
We have that
M od( pi
c 1 ) = {{a, b, c, d}, {a, b, c, d}}
M od(

pi
c

and

2 ) = {{a, b, c, d}, {a, b, c, d}}

pi
Hence M od( pi
c 1 )  M od( c 2 ) = {{a, b, c, d}}. On the other hand
pi
M od( pi
c (1  2 )) = M od( c ((a  d)  (c  d)  (a  d)  (c  d)))

= M od( pi
c d)
= {{a, b, c, d}}.
Conversion to prime implicants in eect removes irrelevant or redundant syntactic information, as illustrated in the preceding example where 1  2 was in fact equivalent to the
atom d. We can further pursue this line of inquiry by considering, for a formula for update
, a syntactic representation of the proposition expressed by  over the language of . For a
given formula
WV , recall that M odL() is the set of models of , over the language of . The
formula
M odL() then would be this formula expressed in disjunctive normal form; for
example M odL((a  b)  c) would be expressed as (a  b  c)  (a  b  c)  (a  b  c).
We dene an update operator as follows:
774

fiCompositional Belief Update

WV
Definition 14 M od( ss
M odL())).
c ) = M od( c (

We obtain that this update operator is in fact the same as that of Winsletts standard
semantics:
Theorem 14  ss
c    ss .
We can pursue this direction one step further, and dene an update operator where the
update formula  is characterized by its models expressed in dnf. That is we can dene:
WV
) = M od( c (
M od())).
M od( triv
c

This is the same as Denition 14, except over models of , rather than models of  in the
language of . However it is easily shown that this is not an interesting operator, since it
removes all old information and we have:
)  .
Observation 2 ( triv
c

We next proceed in a slightly dierent direction and compare our update operator with
the forget operator. Recall that forgetting a set of atoms from a formula  basically removes
all information having to do with this set of atoms; in a sense forgetting is analogous to
decreasing the language by this set of atoms.
To begin with, it can be noted that our update operator can have contraction-like
properties similar to forget. For example, (a  b) c (a  a) is readily shown to be equivalent
to b. So again, in a sense, this update can be read as updating by precisely a  a, which
in this case would indicate tautologous information concerning a. In fact, we have the
following result (recall that we use  to denote forget):
Theorem 15 Let   L and let   P. Then


^
   =  c  (p  p).
p

Hence forgetting a set of atoms is a special case of our update operator. (Given Theorem 6, forget is of course also expressible via erasure in an analogous fashion.) Last, we
establish a result between the forget operator and Winsletts standard semantics. While in
hindsight obvious, this result does not appear to have been previously noted.
V
Theorem 16 For formula  and   P, let  = l (l  l). Then
   =  ss
c .

In summary, it can be observed from the above discussion that we have obtained a
hierarchy of operators, based on the extent to which information in  is made explicit.
For the most basic case, we have  triv
, where the update formula  is a syntactic
c
representation of all models of the language; a trivial update operator results. The most
basic interesting operator is given by  ss
c , which is the same as Winsletts standard
pi
semantics, followed by  c  and  c . As well, by introducing tautologies, we also
capture the notion of forgetting of atoms.
We have already noted that our update operator c is distinct from the Winslett PMA
approach. To the best of our knowledge it is also distinct from all other specic approaches
appearing in the literature, including those surveyed in (Herzig & Ri, 1999).
775

fiDelgrande, Jin, & Pelletier

5. Algorithms and Complexity
In this section we present a syntactic characterization as well as an algorithm for computing
compositional update. We also analyze the complexity of this algorithm under a variety of
assumptions. Specically, we analyze the complexity of the algorithm when applied to any
propositional sentences in general, to any sentences in disjunctive normal form, and to any
sentences whose sizes are bounded by some specied constant.
We start with some background notions. Recall that [p/q] denotes the formula 
where all occurrences of atom p are replaced by q. We write  p. to denote the formula
[p/]  [p/]. If P = {p1 ,    , pn } is a set of atoms then  P., called an eliminant of
P in , stands for  p1 .(    (pn .)) (Brown, 1990). Intuitively, an eliminant of P in  can
be viewed as a formula representing the same knowledge of  that is not concerned with
atoms in P . We have eliminated information about members of P by replacing them by
their two possible values,  and , thus leaving only the other information in .
It has been shown that Winsletts standard semantics can be syntactically captured
based on the notion of eliminant (Doherty, Lukaszewicz, & Madalinska-Bugaj, 1998). Let
P = atom(), then
 ss   ( P.)  
(5)
5.1 Syntactic Characterization and Algorithms
We are now ready to provide a syntactical characterization of compositional update. The
idea is quite similar to that of (Doherty et al., 1998). However, our approach rst converts
the update formula to disjunctive normal form, then deals with each disjunct.
_
U pdate(, ) = {( P.)  t | t  dnf (), P = atom(t)}

The following results establish the correspondence between the semantical denition and
syntactic characterizations of compositional update.
Lemma 2 Suppose t is a term (a conjunction of literals) and P = atom(t). Then
M od(( P.)  t) = {w | w  UL(w, t), w  M od()}
Theorem 17 M od( c ) = M od(U pdate(, )).
Corollary 2  c   U pdate(, )
All we need to compute compositional update, therefore, is the ability to compute
eliminants. As proposed by Brown (1990), an eliminant  P. can be constructed as follows.
1. Convert  to dnf t1      tn (each ti is a conjunction of literals)
2. Replace each ti by ti  P .
Now we are ready to provide the algorithms for compositional update. We will assume
that dnf () refers to the disjunctive normal form of  represented in clause form, in which
a formula is represented by sets of sets of literals. In this case, the members of dnf ()
are implicitly disjoined, while a set of literals making up a member of dnf () is implicitly
conjoined.
In the following algorithms, let ,   L and P be a set of atoms:
776

fiCompositional Belief Update

Algorithm Eliminant(P, )
1.
  
2.
for each term t  dnf ()
3.
t  
4.
for each literal l  t
/P
5.
if l 
/ P and l 
6.
t  t  l
7.
      t
8.
return  
Algorithm U pdate(, )
1.
  
2.
for each term t  dnf ()
3.
P = atom(t)
4.
      (Eliminant(P, )  t)
5.
return  
Lets consider again Example 1 in which  = b and  = (b  m)  (b  m). We
have that U pdate(, ) =   (Eliminant({b}, )  b). Since Eliminant({b}, ) =  
(  m)  (  m), which is equivalent to , we obtain that U pdate(, )  b. Thus,
U pdate(, )   c , as we have already shown that  c   b.
For Example 2, where  = b  m and  = (b  m), we obtain U pdate(, ) =
(b  m)  (b  m). Again, this result is same as what we obtain with c .
5.2 Complexity
In the sequel, we analyze the space complexity of the update algorithm; that is, we are
interested in how large the updated knowledge base could be. Unfortunately, when applied
to arbitrary formulas, the algorithm U pdate may cause exponential space blowup, as the
disjunctive normal form of a formula could be exponentially large.
Theorem 18 The space complexity of U pdate(, ) is O(2(||+||) ) for ,   L;
However, we are able to show that such exponential space blowup is inevitable for any
algorithm of compositional update. To this end, we need to introduce so-called advice-taking
Turing machine (TM) and non-uniform complexity class, see (Johnson, 1990).
An advice-taking TM is a TM with an advice oracle, which can be considered as a
function a from positive integers to strings. On input x, the machine loads string a(|x|)
and then continues as usual based on two inputs x and a(|x|). Note that the oracle string
a(|x|) only depends on the size of the input x. We call an advice oracle a polynomial
i |a(n)| < p(n) for some xed polynomial p and all positive integers n. If X is a usual
complexity class dened in terms of resource-bounded machines (e.g., P or NP) then X/poly
is the class of the problem that can be decided on machines with the same resource bound
augmented by polynomial advice oracles. Any class X/poly is also known as the non-uniform
X; in particular, P/poly appears to be much more powerful than P. However, it has been
shown very unlikely that NP  P/poly, otherwise the polynomial hierarchy would collapse
777

fiDelgrande, Jin, & Pelletier

at p2 (Karp & Lipton, 1980). This result is used to show that it is unlikely that there exists
an algorithm for compositional update with a polynomial space bound.
Theorem 19 Assume there exist a polynomial p and an algorithm Update of compositional
update such that U pdate(, )   c  and |U pdate(, )|  p(|| + ||), for any belief
base  and formula . Then NP  P/poly.
We can pursue the above result one step further, and show that algorithms for any sensible update operators will cause exponential blowup. Formally, we say an update operator
 is sensible i for any consistent set of literals :
^
M od( 
) = {  |    (w  )  ,   M od()}

Arguably, the above condition is very intuitive and natural (cf. discussions in Section 3).
In fact, almost all update operators in the literature are sensible.
Theorem 20 If there exists a polynomially space bounded algorithm for any sensible update
operator, then NP  P/poly.

We remark that the above result also proves Winsletts conjecture stating that there does
not exist a polynomially space bounded algorithm for her standard semantics (see Winslett,
1990).
The algorithm becomes tractably better when applied to formulas in disjunctive normal
form, and to update formulas whose sizes are bounded.
Theorem 21 The space complexity of U pdate(, ) is:
1. O(||  ||) for ,  in dnf; and
2. O(||) for  in dnf and || < k for some constant k.
Arguably, in practice, the update formula  (representing the changes of the world) will be
relatively small. Therefore, it is relatively easy to convert  to dnf, and it is also reasonable
to assume the size of  is bounded. As we usually do not restrict the size of the belief
base , converting  to dnf could be computationally much more expensive. Fortunately,
we only need to compile (o-line) the original belief base once into dnf, and the output of
U pdate algorithm is automatically the dnf of the updated belief base. This will considerably
facilitate the further update of the belief base.

6. Conclusion
We have presented belief change operators for updating a knowledge base where the denition of these operators is compositional with respect to the sentence to be added. The intent
is to provide operators with transparent denitions, based on the structure of the formula
for belief change. As a result we lose some of the standard postulates for update, although
we do satisfy a core group of the standard postulate set. We achieve full irrelevance of
syntax if the sentence for update is replaced by the disjunction of its prime implicants.
778

fiCompositional Belief Update

The approach is interesting because rst, it is founded on diering intuitions than other
operators, in that it is based on a decomposition of the formula rather than on the models
of the formula, and second, it allows a straightforward and (under reasonable assumptions)
ecient implementation. While distinct from previous update operators that have appeared
in the literature, we can capture Winsletts standard semantics approach to update in a
restriction of our approach. In fact, the update operator, under dierent syntactic restrictions, may be regarded as constituting a family of update operators of which Winsletts
standard semantics is the weakest interesting approach. When we turn from update to revision, we discover there is no new, interesting compositional revision operator; nevertheless,
our results indicate that by rst computing the compositional update, one can implement
the Satoh or Dalal revision operator more eciently, because we consider only a subset
of the models of the formula of revision, and in certain cases this will have a signicant
speedup over a naive algorithm.
An open question concerns combining this approach with one that is designed to exploit
the structure of the knowledge base (such as discussed in Parikh, 1999 and characterized
in terms of PMA updates in Peppas, Chopra, & Foo, 2004). A second, technical question
that is not fully explored concerns the behaviour of c as an erasure operator. For example,
let  = (a  b)  (a  b). Then, we get that  c (a  b)  a  b. So, in updating the
knowledge base with a formula already implied by the knowledge base, we have actually
removed information. This, as discussed earlier, is quite reasonable if one considers that
an update (in contrast to a revision) by a  b asserts that the world has changed so that
one of {a, b}, {a, b}, {a, b} is now true. Finally, it would be of interest to apply the
compositional approach to the merging of knowledge bases.

Acknowledgments
An early precursor of this paper was presented at FLAIRS 2007 (Delgrande, Pelletier,
& Suderman, 2007). We are grateful to that audience for comments; and we have also
beneted from the perceptive comments of three JAIR referees. Delgrande and Pelletier
also acknowledge the support of the Canadian NSERC granting agency.

779

fiDelgrande, Jin, & Pelletier

Appendix A. Proof of Theorems
Proof 1.
Observe that UL is associative and commutative with respect to top-level conjunctions
and top-level disjunctions. That is, for example
UL(, {  (  )}   ) = UL(, {(  )  }   ).
A similar observation can be made about negations of such top-level conjunctions and
disjunctions; for example we have
UL(, {(  (  ))}   ) = UL(, {((  )  )}   ).
We use such basic facts without comment in the sequel.
The above means in particular that for showing the order-independence of UL with
respect to its second argument, we just need consider the general caseVof UL(, ) where
 = {1 }  {2 }, since we have that UL(, {1 , . . . , n }) = UL(, {1 , ni=2 i }).
Given this preamble, what we need to show is that for formulas 1 and 2 , that
UL(, {1 }  {2 }) is independent of whether the initial recursion is in terms of 1 or
2 .
The proof is on the depth of a formula.
BASE:
Assume that depth(1 )  1 and depth(2 )  1:
1. If depth(1 ) = depth(2 ) = 0 then 1 , 2 are atoms and the result follows trivially.
2. If the only connective for 1 , 2 is negation then 1 , 2 are literals and again the
result follows trivially.
3. If the connectives for 1 , 2 are from {, }, then 1 , 2 reduce to sets of literals,
and the previous case applies.
4. 1 is a1  a2 , and 1 is a literal, then only Step 3 of the denition applies, and our
result obtains easily. The converse where 2 is a1  a2 , and 2 is a literal of course
yields the same result.
5. If 1 is a1  a2 and 2 is b1  b2 , then we have
UL(, {a1  a2 }  {2 }) = UL(, {a1 , a2 }  {2 })
= UL(, {a1 , a2 }  {b1  b2 })
= UL(, {b1  b2 }  {a1 , a2 })
= UL(, {b1 }  {a1 , a2 })  UL(, {b2 }  {a1 , a2 })
= UL(, {b1 }  {a1  a2 })  UL(, {b2 }  {a1  a2 })
= UL(, {b1  b2 }  {a1  a2 })
= UL(, {b1  b2 }  {1 })
780

fiCompositional Belief Update

6. If 1 is a1  a2 and 2 is b1  b2 , then we have
UL(, {a1  a2 }  {2 }) = UL(, {a1 }  {2 })  UL(, {a2 }  {2 })
= UL(, {a1 }  {b1  b2 })  UL(, {a2 }  {b1  b2 })
= UL(, {b1  b2 }  {a1 })  UL(, {b1  b2 }  {a2 })
= (UL(, {b1 }  {a1 })  UL(, {b2 }  {a1 })) 
(UL(, {b1 }  {a2 })  UL(, {b2 }  {a2 }))
= UL(, {b1 , a1 })  UL(, {b2 , a1 }) 
UL(, {b1 , a2 })  UL(, {b2 , a2 })
Analogous manipulations show that UL(, {b1  b2 }  {1 }) yields the same result.
STEP:
For the induction hypothesis, assume that our result holds for depth(1 )  n and
depth(2 )  n. We show that the desired result obtains for depth(1 )  (n + 1) and
depth(2 )  (n + 1).
A: Consider rst where depth(1 )  n and depth(2 ) = n + 1.
1. 2 is of the form :
UL(, {1 }  {2 }) is the same as UL(, {1 }  {}), and our result follows by the
induction hypothesis.
2. 2 is   :
UL(, {1 }  {2 }) = UL(, {1 }  {  }) = UL(, {1 , , }) = UL(, {  } 
{1 }).
3. 2 is   :
UL(, {1 }  {2 }) = UL(, {1 }  {  }) = UL(, {1 , })  UL(, {1 , }) while
UL(, {2 }  {1 }) = UL(, {  }  {1 }) = UL(, {1 , })  UL(, {1 , }).
4. 2 is (  ) or 2 is (  ):
This is handled the same as    or    respectively.
B: Consider next where depth(1 ) = n + 1 and depth(2 ) = n + 1.
1. 1 or 2 is :
This is the same as 1 or 2 being , from which our result holds via the induction
hypothesis.
2. 1 or 2 is 1  1 :
Assume without loss of generality that 1 is 1  1 .
(a) 2 is 2  2 :
UL(, {1 }  {2 }) = UL(, {1  1 }  {2  2 }) = UL(, {1 , 1 , 2 , 2 }) =
UL(, {2 }  {1 }).
781

fiDelgrande, Jin, & Pelletier

(b) 2 is 2  2 :
This case is handled the same as in the base case, where 1 , 1 , 2 , 2 are atoms.
(c) 2 is (2  2 ) or 2 is (2  2 ):
This is handled the same as 2  2 or 2  2 respectively.
3. 1 or 2 is   :
The proof here is the same as in the base case, where ,  are atoms.
4. 1 (2 ) is (  ) or 1 (2 ) is (  ):
This is handled the same as    or    respectively.
Since this covers all cases, our result follows by induction.



Proof 2.
The proof follows straightforwardly from the observations that for arbitrary , , we
have:
UL(, {}  ) = UL(, {}  )
UL(, {(  )}  ) = UL(, {  }  )
UL(, {(  )}  ) = UL(, {  }  )
An induction argument establishes that the value of UL doesnt change under conversion
of elements of its second argument to negation normal form.

Proof 3.
This result follows from the preceding, plus the fact that for arbitrary ,  we have
that: UL(, {  (  )}  ) = UL(, {(  )  (  )}  ); that is, UL is invariant
under distribution of conjunction over disjunction.

Proof 4.
Proof is by induction on the maximum depth of a formula in .
If the maximum depth is 0, then all members of  are literals, and the result is immediate
from Denition 6. Otherwise the induction hypothesis is that the result holds where the
maximum depth of a formula in  is n, and the step is easily shown by appeal to truth
conditions in classical propositional logic.

Proof 5.
Right-to-left: This is a corollary of Theorem 4.
Left-to-right:
For arbitrary  we have by Theorem 3 that UL(, ) = UL(, dnf ()).
Let dnf () = 1      n where each i is a conjunction of literals.
Via Denition 6 we have that UL(, dnf ()) = UL(, {1 })      UL(, {n }). For
each i , if i contains a complementary pair of literals then UL(, {i }) = ; otherwise
UL(, {i }) 6= .
782

fiCompositional Belief Update

If we assume that  6 , then there is some i with no complementary literals, consequently UL(, {i }) 6=  and so  =
6 UL(, dnf ()) = UL(, ).
Thus  6  implies UL(, ) 6|= , which was to be shown.


Lemma 3      c .
Proof of Lemma 3. If      then the result is immediate.
Consequently assume that    is satisable, and let   M od(  ). We show that
  M od( c ). Given Denition 7, and since we already have   M od(), we just need
to show that   UL(, {}).
We have by assumption that   M od(), whence (Theorem 3)   M od(dnf ()).
Since M od(dnf ()) = M od(dnf (1 ))      M od(dnf (n )) for dnf () = 1      n we
get   M od(dnf (i )) for some disjunct i of dnf ().
Since   M od(dnf (i )) it follows from the denition of UL that  = UL(, {i }); hence
  M od(dnf ()) and so   UL(, {}), which was to be shown.

Proof 6. The second part of the theorem follows immediately from Denitions 7 and 8.
For the rst part: Since      c  (Lemma 3), we have
 c   (  )  ( c )
 (  )  (( c )  )
 (  ( c ))  
 ( c )  
The last step applies the other part of the theorem, established above.



Proof 7.
The proof is the same as that for Theorem 2 with minor modications.



Proof 8.
The proof is analogous to that of Theorem 3, and is omitted.



Proof of Lemma 1.
The proof is straightforward, except setting up the induction is a bit ddly. The induction is based on the maximum depth of a formula in . For   L, let depth() =
max depth(). We then stipulate that  precedes  in the ordering for the induction if
depth() < depth( ), or if depth() = depth( ) = n and the number of formulas in  of
depth n is less than that in  .
BASE:
Let  be a set of literals.
W
If    then EL(, ) =  = UL(, {  }).
783

fiDelgrande, Jin, & Pelletier

If

W

 6  then:
EL(, ) = (  )  
= (  )  
= UL(, )
 n ^ o
= UL ,  

STEP:
Assume that the result holds for the rst n sets of formulas in the ordering, and let 
be a formula of maximum depth in . Let  =  \ {}.
If  =   , then
EL(, ) = EL(, {  }   )
= EL(, {}   )  EL(, {}   )
= EL(, {({}   )})  EL(, {({}   )})
= UL(, {  ({}   )})  UL(, {  ({}   )})
= UL(, {  (   )})  UL(, {  (   )})
= UL(, {  (   )    (   )})
= UL(, {    (   )})
= UL(, {(    ( ))})
= UL(, {()}).
The change from EL to UL above is justied by the induction hypothesis; otherwise all
steps are by denition of UL or EL, or simple manipulation.
If  =   , then
EL(, ) = EL(, {  }   )
= EL(, {, }   )
= UL(, {  ({, }   )})
= UL(, {()}).
Again, the change from EL to UL above is justied by the induction hypothesis.
Other cases are handled analogously; their proofs are omitted.
Hence our result follows by induction.



Proof 9.
M od( c ) = M od()  {  |    UL(, {}),   M od()}
= M od()  {  |    EL(, {}),   M od()}
= M od(


c )

The rst and last steps above are justied by Denitions 8 and 10 respectively; the middle
step follows from Lemma 1.

784

fiCompositional Belief Update

Proof 10. It follows immediately from Denitions 11 and 12 that M od( )  M od( S
).
To show the converse, we let w  M od( S ) and show that w  M od(  ).
Given that the Satoh revision operator satises irrelevance of syntax (R4), we can
assume without loss of generality that  is in dnf; i.e.  = 1      n where each i is a
conjunction of literals.
We have by assumption that w  M od( S ); hence w  M od() such that ww 
min
 (, ).
Since w  M od( S ), we have that w  M od() = M od(1      n ). Thus there
is a clause from , i , such that w  M od(i ). Assume without loss of generality that i
is subset-minimal among the sets of literals making up the disjuncts of .
If we can show w  UL(w, {i }) then we will have shown that w satises the conditions
to be a member of M od(  ). We show this as follows. Let Ui be the set of literals in i .
We have that for l 6 Ui that l  w i l  w (since otherwise this would contradict
ww  min (, )).
It follows that (w  Ui )  Ui = w . But this means that w  UL(w, Ui ), or w 
UL(w, {ui }) and so w  UL(w, {}).
Hence w  M od(  ), which was to be shown.

Proof 11.
U1: By Theorem 4 , UL(, {}) |=  for every   M od(), whence M od(c )  M od()
or  c   .
U3: By assumption  6 , and so M od() 6= . Our result then follows immediately from
Theorem 5 and Denition 7.
U5: If M od( c )  M od() =  then our result follows vacuously.
Otherwise, let   M od( c )  M od().
Since   V
M od( c ) then, by Theorem 3, there exists    M od() and   Lits
such that  is a disjunct of dnf () where  = (   )  .
V
Because   M od() there is a set of literals  such that  is a disjunct of dnf ()
such that   .
V
By denition,  is a clause in dnf (). We note that  = (   ( ))( )
so by Denition 7 we have   M od( c (  )).
U7:
M od( c 1 )  M od( c 2 )
 M od( c 1 )  M od( c 2 )
 M od( c (1  2 ))
The last step follows from Theorem 3, using the fact that dnf () = dnf ()dnf ().
Hence ( c 1 )  ( c 2 ) implies  c (1  2 ).
785

fiDelgrande, Jin, & Pelletier

U8:
M od((1  2 ) c ) = {  |    UL(, {}),   M od(1  2 )}
= {  |    UL(, {}),   M od(1 )  M od(2 )}
= {  |    UL(, {}),   M od(1 )}
 {   UL(, {}),   M od(2 )}
= M od(1 c )  M od(2 c ).
From M od((1 2 )c ) = M od(1 c )M od(2 c ) it follows that ((1 2 )c ) 
(1 c )  (2 c ).

Proof 12.
M od( c (1  2 )) = {  |    UL(, {1  2 }),   M od()}
= {  |    UL(, {1 }),   M od()} 
{  |    UL(, {2 }),   M od()}
= M od( c 1 )  M od( c 2 )
= M od(( c 1 )  ( c 2 ))

Proof 13.
We need to show that if 1  2 and 1  2 then we have that (1 c 1 ) W
 (2 c 1 ).
12 Since ( 
Since W


by
assumption,
we
have
that
P
I(
)
=
P
I(
).
1
2
1W
2
1 c W P I(1 )) 
W
(2 c W P I(1 )) and (2 c P I(1 ))  (2 c P I(2 )), we have (1 c P I(1 )) 
pi
(2 c P I(2 )), whence ( pi

c 1 )  ( c 2 ).

Proof 14.
WV
We have that M od( ss
c (V ( M odL()))). Using Denition 7, the
c ) = M od( W
right hand side is equal to {  |    UL(, { ( M odL())}),   M od()}. Hence, each
  M od() is replaced by a set of interpretations  where  = {(    )    |   
M odL()}. Which is to say,   M od() is replaced by a set of interpretations  where
    just if  and   dier only over the language of . But this is just the denition

for M od( ss
c ).
Proof 15.
Let  = {p1 , . . . , pn }.
   =   {p1      pn }
= [(  {p1      pn }) c pn ]  [(  {p1      pn }) c pn ]

12. Equality isnt quite right here. Rather we have equality modulo associativity and commutativity, which
is all that we need for our result.

786

fiCompositional Belief Update

The second step above is just Denition 3 for forget expressed in terms of update. Denition 3 can be successively reapplied to eventually terminate with a disjunction with 2n
disjuncts, where each disjunct is a sequence of n updates of literals from . Moreover, every
maximum consistent set of literals from  appears in some disjunct.
We have the easy result, that we state without proof, that for disjoint sets of literals 1 ,
2 , that ( c (1 )) c (2 ) = ( c (1  2 )).
Hence we get nally that
^
 =
 c (()    ( \ ))




=  c 

^



(p  p).

p



Proof 16.
From Theorem 15 we have that
   =  c

^

!

(a  a).

a

=  c .

From Observation
1 we get that  c  =  c dnf (). An easy argument shows that
WV
dnf () =
M odL(), and so Denition 14 yields  c dnf () =  ss
c . Theorem 14 is
ss
ss
 c  =  ss . Putting this all together we get   () =  c .

Proof of Lemma 2.
Equation (5) implies that M od(( P.)  t) = M od( W
t). According to Theorem 14
ss V
and Denition 14, we then have M od( ss t) = M od( c ( ( M odL(t)))) = M od( c t).
From Denition 7, it follows that M od(( P.)  t) = {w | w  UL(w, t), w  M od()}. 
Proof 17.
According to Denition 7, M od(c ) = {w | w  UL(w, {}), w  M od()}. By Theorem 3, we have {w | w  UL(w, {}), w  M od()} = {w | w  UL(w, {dnf ()}), w 
M od()}. From Denition 6, it follows that M od( c ) = {w | w  UL(w, t), t 
dnf (), w  M od()}. According to Lemma 2, thus M od( c ) = M od(U pdate(, )). 
Proof 18.
The size of dnf () is O(2|| ). Hence, the size of Eliminant(P, ) is also O(2|| ). Similarly, the size of dnf () is O(2|| ). Therefore, |U pdate(, )| = O(2||  2|| ) = O(2||+|| ).

Proof 19.
This proof is inspired by the ideas in (Cadoli, Donini, Liberatore, & Schaerf, 1995),
where it was shown that many revision operators cause exponential blowup. We show that
787

fiDelgrande, Jin, & Pelletier

if there exists a polynomially space bounded algorithm of compositional update, then 3SAT
is in P/poly.13 The proof consists of two steps.
STEP 1:
For any integer n, we rst construct a belief base n and a formula n , whose sizes
are polynomial wrt. n. Let X = {x1 ,    , xn } and Y = {y1 ,    , yn } be two disjoint set
of atoms and let C be a set of new atoms for each 3-literal clause over X, i.e., C = {ci |
i is a 3-literal clause of X}. We obtain n and n as follows:
n = V
{i  ci | i is a 3-literal clause of X}
n = ni=1 (x1  yi )

It is easy to see that |n |  O(n3 ) and |n |  O(n).
Then we show that for any 3CNF  of size n, there exists an interpretation  (on
atoms X  Y  C) such that  |= n c n i  is satisable. We assume, without loss of
generality, that atom()  X; or otherwise, we can always substitute atoms of  respectively
by elements of X to obtain a new sentence X such that  is satisable i X is satisable.
Then w can be obtained as follows:
w = {ci  C | i is a clause of }  {ci  C | i is not a clause of }  X  Y
We now show that  is satisable i  |= n c n .
 Assume  is satisable. Let  be a model of . We construct another interpretation
  = UL(, {ci  C | i is not a clause of }). It is easy to see that   |= n and
 = UL(  , {xi , yi | 1  i  n}). It follows that  |= n c n .
 Assume  |= n c n . Then there exists an interpretation  such that  |= n and
 = UL(, {xi , yi | 1  i  n}). We claim that  |= . Assume  6|= . Then
there exists a 3-literal clause i of  such that  6|= i . From  = UL(, {xi , yi |
1  i  n}) and ci   , if follows that ci  . This implies  6|= i  ci , which
contradicts  |= n . Thus,  is indeed satisable.
STEP 2:
Suppose U pdate is a polynomial space bounded algorithm of compositional update.
Then 3SAT can be solved by an advice taking TM as follows: Given an arbitrary 3CNF  of
size n, the machine rst loads the advice string U pdate(n , n ) and computes (in polynomial
time)  ; then it veries  |= U pdate(n , n ). Since |n |  O(n3 ), |n |  O(n), and
|U pdate(n , n )|  p(|n | + |n |), we can do the verication in polynomial time. Since
U pdate(n , n )  n c n , we have  |= n c n i  |= U pdate(n , n ). Therefore,
 is satisable i  |= U pdate(n , n ). This shows that 3SAT  P/poly. As 3SAT is
NP-complete, we have NP  P/poly.

Proof 20.
13. A 3-literal clause is clause consists of precisely 3 literals and a 3CNF is a conjunction of 3-literal clauses.
3SAT is the satisfiability problem for 3CNFs, which has been shown NP-complete.

788

fiCompositional Belief Update

This proof is exactly same as that of Theorem 19, as the update formula n used there
is a consistent conjunction of literals.

Proof 21. Since ,  are in dnf, || = |dnf ()| and || = dnf (). Thus |Eliminant(P, )| =
O||. Therefore |U pdate(, )| = O(||  ||).
In case || < k, we have |U pdate(, )| = O(||  k) = O().


References
Alchourron, C., Gardenfors, P., & Makinson, D. (1985). On the logic of theory change:
Partial meet functions for contraction and revision. Journal of Symbolic Logic, 50 (2),
510530.
Boole, G. (1854). An Investigation of the Laws of Thought. Walton, London. (Reprinted
by Dover Books, New York, 1954).
Borgida, A. (1985). Language features for exible handling of exceptions in information
systems. ACM Transactions on Database Systems, 10.
Brewka, G., & Herzberg, J. (1993). How to do things with worlds: On formalizing actions
and plans. J. Logic Computation, 3, 517532.
Bright, M., Hurson, A., & Pakzad, S. (1992). A taxonomy and current issues in multidatabase systems. Computer, 25, 5059.
Brown, F. (1990). Boolean Reasoning. Kluwer Academic Publishers.
Cadoli, M., Donini, F. M., Liberatore, P., & Schaerf, M. (1995). The size of a revised
knowledge base. In PODS 95: Proceedings of the fourteenth ACM SIGACT-SIGMODSIGART symposium on Principles of database systems, pp. 151162, New York, NY,
USA. ACM Press.
Dalal, M. (1988). Investigations into theory of knowledge base revision.. In Proceedings
of the AAAI National Conference on Artificial Intelligence, pp. 449479, St. Paul,
Minnesota.
Delgrande, J., Nayak, A., & Pagnucco, M. (2005). Gricean belief change. Studia Logica, 79,
97113.
Delgrande, J., & Schaub, T. (2003). A consistency-based approach for belief change. Artificial Intelligence, 151 (1-2), 141.
Delgrande, J., Pelletier, F., & Suderman, M. (2007). Compositional belief update. In
Proceedings of FLAIRS-20, pp. 6873, Key West.
Doherty, P., Lukaszewicz, W., & Madalinska-Bugaj, E. (1998). The PMA and relativizing
change for action update. In Cohn, A. G., Schubert, L., & Shapiro, S. C. (Eds.),
Proceedings of the International Conference on the Principles of Knowledge Representation and Reasoning, pp. 258269. Morgan Kaufmann, San Francisco, California.
Everaere, P., Konieczny, S., & Marquis, P. (2005). Quota and Gmin merging operators.
In Proceedings of the International Joint Conference on Artificial Intelligence, pp.
424429, Edinburgh.
789

fiDelgrande, Jin, & Pelletier

Everaere, P., Konieczny, S., & Marquis, P. (2007). The strategy-proofness landscape of
merging. Journal of Artificial Intelligence Research, 28, 49105.
Forbus, K. (1989). Introducing actions into qualitative simulation. In Proceedings of the
International Joint Conference on Artificial Intelligence, pp. 12731278.
Fuhrmann, A. (1991). Theory contraction through base contraction. Journal of Philosophical Logic, 20, 175203.
Gardenfors, P. (1988). Knowledge in Flux: Modelling the Dynamics of Epistemic States.
The MIT Press, Cambridge, MA.
Hansson, S. O., & Rott, H. (1998). Beyond recovery. Erkenntnis, 49, 387392.
Herzig, A., & Ri, O. (1999). Propositional belief update and minimal change. Artificial
Intelligence, 115 (1), 107138.
Johnson, D. (1990). A catalog of complexity classes. In van Leeuwen, J. (Ed.), Handbook
of Theoretical Computer Science: Volume A: Algorithms and Complexity, pp. 67161.
Elsevier, Amsterdam.
Karp, R. M., & Lipton, R. J. (1980). Some connections between non-uniform and uniform
complexity classes. In Proc. of the 12th ACM sym. on Theory of Computing (STOC80), pp. 302309.
Katsuno, H., & Mendelzon, A. (1992). On the dierence between updating a knowledge
base and revising it. In Gardenfors, P. (Ed.), Belief Revision, pp. 183203. Cambridge
University Press.
Konieczny, S., & Pino Perez, R. (2002). Merging information under constraints: A logical
framework. Journal of Logic and Computation, 12 (5), 773808.
Konieczny, S. (2000). On the dierence between merging knowledge bases and combining
them. In Cohn, A. G., Giunchiglia, F., & Selman, B. (Eds.), KR2000: Principles
of Knowledge Representation and Reasoning, pp. 135144, San Francisco. Morgan
Kaufmann.
Konieczny, S., & Pino Perez, R. (1998). On the logic of merging. In Cohn, A. G., Schubert, L., & Shapiro, S. C. (Eds.), KR98: Principles of Knowledge Representation and
Reasoning, pp. 488498. Morgan Kaufmann, San Francisco, California.
Lang, J. (2006). About time, revision, and update. In Dix, J., & Hunter, A. (Eds.),
Proceedings of the Eleventh International Workshop on Non-Monotonic Reasoning
(NMR 2006).
Lang, J., Liberatore, P., & Marquis, P. (2003). Propositional independence : Formulavariable independence and forgetting. Journal of Artificial Intelligence Research, 18,
391443.
Liberatore, P., & Schaerf, M. (1998). Arbitration (or how to merge knowledge bases). IEEE
Transactions on Knowledge and Data Engineering, 10 (1), 7690.
Lin, F., & Reiter, R. (1994). Forget it!. In AAAI Fall Symposium on Relevance, New
Orleans.
790

fiCompositional Belief Update

Lin, F. (2001). On strongest neccessary and weakest sucient conditions. Artificial Intelligence, 128 (1-2), 143159.
Lin, J., & Mendelzon, A. O. (1998). Merging databases under constraints. International
Journal of Cooperative Information Systems, 7 (1), 5576.
Meyer, T. (2000). Merging epistemic states. In Pacific Rim International Conference on
Artificial Intelligence, pp. 286296.
Meyer, T. (2001). On the semantics of combination operations. Journal of Applied NonClassical Logics, 11 (1-2), 5984.
Nayak, A., Chen, Y., & Lin, F. (2006). Forgetting and knowledge update. In Sattar,
A., & Kang, B. (Eds.), Proceedings of the Nineteenth Australian Joint Conference of
Artificial Intelligence (AI-06), Vol. 4304 of Lecture Notes in Artificial Intelligence, pp.
131140. Springer Verlag.
Parikh, R. (1999). Beliefs, belief revision, and splitting languages. In Moss, L., Ginzburg, J.,
& de Rijke, M. (Eds.), Logic, Language and Computation, Vol 2, pp. 266278. CSLI
Publications.
Peppas, P., Chopra, S., & Foo, N. (2004). Distance semantics for relevance-sensitive belief
revision. In KR2004: Principles of Knowledge Representation and Reasoning, San
Francisco. Morgan Kaufmann.
Revesz, P. (1993). On the semantics of theory change: Arbitration between old and new
information. In Beeri, C. (Ed.), Proceedings of the Twelth ACM Symposium on Principles of Database Systems, pp. 7182, Washington D.C.
Rott, H., & Pagnucco, M. (1999). Severe withdrawal (and recovery). Journal of Philosophical
Logic, 28 (5), 501547.
Satoh, K. (1988). Nonmonotonic reasoning by minimal belief revision. In Proceedings of
the International Conference on Fifth Generation Computer Systems, pp. 455462,
Tokyo.
Subrahmanian, S. (1994). Amalgamating knowledge bases. ACM Transactions on Database
Systems, 19, 291331.
Tennant, N. (1997). On having bad contractions or: No room for recovery. Journal of
Applied Non-Classical Logic, 7, 241266.
Weber, A. (1986). Updating propositional formulas. In Proc. First Conference on Expert
Database Systems, pp. 487500.
Williams, M.-A. (1996). Towards a practical approach to belief revision: Reason-based
change. In Aiello, L., Doyle, J., & Shapiro, S. (Eds.), Proceedings of the Fifth International Conference on the Principles of Knowledge Representation and Reasoning,
pp. 412421, Cambridge, MA.
Winslett, M. (1988). Reasoning about action using a possible models approach. In Proceedings of the AAAI National Conference on Artificial Intelligence, pp. 8993, St. Paul,
Minnesota.
Winslett, M. (1990). Updating Logical Databases. Cambridge University Press, Cambridge.

791

fiJournal of Artificial Intelligence Research 32 (2008) 631-662

Submitted 11/07; published 06/08

A General Theory of Additive State Space Abstractions
Fan Yang
Joseph Culberson
Robert Holte

fyang@cs.ualberta.ca
joe@cs.ualberta.ca
holte@cs.ualberta.ca

Computing Science Department, University of Alberta
Edmonton, Alberta T6G 2E8 Canada

Uzi Zahavi

zahaviu@cs.biu.ac.il

Computer Science Department, Bar-Ilan University
Ramat-Gan, Israel 92500

Ariel Felner

felner@bgu.ac.il

Information Systems Engineering Department,
Deutsche Telekom Labs,
Ben-Gurion University.
Beer-Sheva, Israel 85104

Abstract
Informally, a set of abstractions of a state space S is additive if the distance between
any two states in S is always greater than or equal to the sum of the corresponding distances in the abstract spaces. The first known additive abstractions, called disjoint pattern
databases, were experimentally demonstrated to produce state of the art performance on
certain state spaces. However, previous applications were restricted to state spaces with
special properties, which precludes disjoint pattern databases from being defined for several commonly used testbeds, such as Rubiks Cube, TopSpin and the Pancake puzzle. In
this paper we give a general definition of additive abstractions that can be applied to any
state space and prove that heuristics based on additive abstractions are consistent as well
as admissible. We use this new definition to create additive abstractions for these testbeds
and show experimentally that well chosen additive abstractions can reduce search time
substantially for the (18,4)-TopSpin puzzle and by three orders of magnitude over state of
the art methods for the 17-Pancake puzzle. We also derive a way of testing if the heuristic
value returned by additive abstractions is provably too low and show that the use of this
test can reduce search time for the 15-puzzle and TopSpin by roughly a factor of two.

1. Introduction
In its purest form, single-agent heuristic search is concerned with the problem of finding a
least-cost path between two states (start and goal) in a state space given a heuristic function
h(t, g) that estimates the cost to reach the goal state g from any state t. Standard algorithms
for single-agent heuristic search such as IDA (Korf, 1985) are guaranteed to find optimal
paths if h(t, g) is admissible, i.e. never overestimates the actual cost to the goal state from
t, and their efficiency is heavily influenced by the accuracy of h(t, g). Considerable research
has therefore investigated methods for defining accurate, admissible heuristics.
A common method for defining admissible heuristics, which has led to major advances
in combinatorial problems (Culberson & Schaeffer, 1998; Hernadvolgyi, 2003; Korf, 1997;
Korf & Taylor, 1996) and planning (Edelkamp, 2001), is to abstract the original state
c
2008
AI Access Foundation. All rights reserved.

fiYang, Culberson, Holte, Zahavi & Felner

space to create a new, smaller state space with the key property that for each path p~ in the
original space there is a corresponding abstract path whose cost does not exceed the cost
of p~. Given an abstraction, h(t, g) can be defined as the cost of the least-cost abstract path
from the abstract state corresponding to t to the abstract state corresponding to g. The
best heuristic functions defined by abstraction are typically based on several abstractions,
and are equal to either the maximum, or the sum, of the costs returned by the abstractions
(Korf & Felner, 2002; Felner, Korf, & Hanan, 2004; Holte, Felner, Newton, Meshulam, &
Furcy, 2006).
The sum of the costs returned by a set of abstractions is not always admissible. If it
is, the set of abstractions is said to be additive. The main contribution of this paper is
to identify general conditions for abstractions to be additive. The new conditions subsume
most previous notions of additive as special cases. The greater generality allows additive
abstractions to be defined for state spaces that had no additive abstractions according to
previous definitions, such as Rubiks Cube, TopSpin, the Pancake puzzle, and related realworld problems such as the genome rearrangement problem described by Erdem and Tillier
(2005). Our definitions are fully formal, enabling rigorous proofs of the admissibility and
consistency of the heuristics defined by our abstractions. Heuristic h(t, g) is consistent if for
all states t, g and u, h(t, g)  cost(t, u) + h(u, g), where cost(t, u) is the cost of the least-cost
path from t to u.
The usefulness of our general definitions is demonstrated experimentally by defining
additive abstractions that substantially reduce the CPU time needed to solve TopSpin and
the Pancake puzzle. For example, the use of additive abstractions allows the 17-Pancake
puzzle to be solved three orders of magnitude faster than previous state-of-the-art methods.
Additional experiments show that additive abstractions are not always the best abstraction method. The main reason for this is that the solution cost calculated by an individual
additive abstraction can sometimes be very low. In the extreme case, which actually arises
in practice, all problems can have abstract solutions that cost 0. The final contribution of
the paper is to introduce a technique that is sometimes able to identify that the sum of the
costs of the additive abstractions is provably too small (infeasible).
The remainder of the paper is organized as follows. An informal introduction to abstraction is given in Section 2. Section 3 presents formal general definitions for abstractions
that extend to general additive abstractions. We provide lemmas proving the admissibility
and consistency of both standard and additive heuristics based on these abstractions. This
section also discusses the relation to previous definitions. Section 4 describes successful applications of additive abstractions to TopSpin and the Pancake puzzle. Section 5 discusses
the negative results. Section 6 introduces infeasibility and presents experimental results
showing its effectiveness on the sliding tile puzzle and TopSpin. Conclusions are presented
in Section 7.

2. Heuristics Defined by Abstraction
To illustrate the idea of abstraction and how it is used to define heuristics, consider the
well-known 8-puzzle (the 3  3 sliding tile puzzle). In this puzzle there are 9 locations in
the form of a 3  3 grid and 8 tiles, numbered 18, with the 9th location being empty (or
blank). A tile that is adjacent to the empty location can be moved into the empty location;
632

fiA General Theory of Additive State Space Abstractions

every move has a cost of 1. The most common way of abstracting this state space is to treat
several of the tiles as if they were indistinguishable instead of being distinct (Culberson &
Schaeffer, 1996). An extreme version of this type of abstraction is shown in Figure 1. Here
the tiles are all indistinguishable from each other, so an abstract state is entirely defined by
the position of the blank. There are therefore only 9 abstract states, connected as shown
in Figure 1. The goal state in the original puzzle has the blank in the upper left corner,
so the abstract goal is the state shown at the top of the figure. The number beside each
abstract state is the distance from the abstract state to the abstract goal. For example, in
Figure 1, abstract state e is 2 moves from the abstract goal. A heuristic function h(t, g) for
the distance from state t to g in the original space is computed in two steps: (1) compute
the abstract state corresponding to t (in this example, this is done by determining the
location of the blank in state t); and then (2) determine the distance from that abstract
state to the abstract goal. The calculation of the abstract distance can either be done in a
preprocessing step to create a heuristic lookup table called a pattern database (Culberson
& Schaeffer, 1994, 1996) or at the time it is needed (Holte, Perez, Zimmer, & MacDonald,
1996; Holte, Grajkowski, & Tanner, 2005; Felner & Adler, 2005).
abstract goal

id: distance to goal

a:0

b: 1

c: 1
e: 2

f: 2

d: 2
h: 4
g: 3

i: 3

Figure 1: An abstraction of the 8-puzzle. The white square in each state is the blank and
the non-white squares are the tiles, which are all indistinguishable from each other
in this abstraction.

Given several abstractions of a state space, the heuristic hmax (t, g) can be defined as the
maximum of the abstract distances for t given by the abstractions individually. This is the
standard method for defining a heuristic function given multiple abstractions (Holte et al.,
2006). For example, consider state A of the 3  3 sliding tile puzzle shown in the top left
of Figure 2 and the goal state shown below it. The middle column shows an abstraction
of these two states (A1 and g1 ) in which tiles 1, 3, 5, and 7, and the blank, are distinct
while the other tiles are indistinguishable from each other. We refer to the distinct tiles as
distinguished tiles and the indistinguishable tiles as dont care tiles. The right column
633

fiYang, Culberson, Holte, Zahavi & Felner

shows the complementary abstraction, in which tiles 1, 3, 5, and 7 are the dont cares
and tiles 2, 4, 6, and 8 are distinguished. The arrows in the figure trace out a least-cost
path to reach the abstract goal gi from state Ai in each abstraction. The cost of solving A1
is 16 and the cost of solving A2 is 12. Therefore, hmax (A, g) is 16, the maximum of these
two abstract distances.
abstract state A2

abstract state A1

state A

1

2

4

8

5

6

7

3

7

h=max(16,12)

1

2

3

4

5

6

7

8

goal state g

2

1
5

4

3

6

8

16 moves

12 moves

2

1
3

4

5
7
abstract goal g1

6

8
abstract goal g 2

Figure 2: Computation of hmax (A, g), the standard, maximum-based heuristic value for
state A (top left) using the two abstractions shown in the middle and right
columns. Solid arrows denote distinguished moves, dashed arrows denote dont
care moves.

2.1 Additive Abstractions
Figure 3 illustrates how additive abstractions can be defined for the sliding tile puzzle (Korf
& Felner, 2002; Felner et al., 2004; Korf & Taylor, 1996). State A and the abstractions are
the same as in Figure 2, but the costs of the operators in the abstract spaces are defined
differently. Instead of all abstract operators having a cost of 1, as was the case previously,
an operator only has a cost of 1 if it moves a distinguished tile; such moves are called
distinguished moves and are shown as solid arrows in Figures 2 and 3. An operator that
moves a dont care tile (a dont care move) has a cost of 0 and is shown as a dashed
arrow in the figures. Least-cost paths in abstract spaces defined this way therefore minimize
the number of distinguished moves without considering how many dont care moves are
made. For example, the least-cost path for A1 in Figure 3 contains fewer distinguished
moves (9 compared to 10) than the least-cost path for A1 in Figure 2and is therefore
lower cost according to the cost function just describedbut contains more moves in total
(18 compared to 16) because it has more dont care moves (9 compared to 6). As Figure
3 shows, 9 distinguished moves are needed to solve A1 and 5 distinguished moves are needed
to solve A2 . Because no tile is distinguished in both abstractions, a move that has a cost of
1 in one space has a cost of 0 in the other space, and it is therefore admissible to add the
634

fiA General Theory of Additive State Space Abstractions

two distances. The heuristic calculated using additive abstractions is referred to as hadd ; in
this example, hadd (A, g) = 9 + 5 = 14. Note that hadd (A, g) is less than hmax (A, g) in this
example, showing that heuristics based on additive abstractions are not always superior
to the standard, maximum-based method of combining multiple abstractions even though
in general they have proven very effective on the sliding tile puzzles (Korf & Felner, 2002;
Felner et al., 2004; Korf & Taylor, 1996).
abstract state A2

abstract state A1

state A

1

2

4

8

5

6

7

3

7

h=9+5=14

1

2

3

4

5

6

7

8

goal state g

2

1
5

4

3

6

8

5 distinguished moves

9 distinguished moves

2

1
3

4

5
7
abstract goal g1

6

8
abstract goal g 2

Figure 3: Computation of hadd (A, g), the additive heuristic value for state A. Solid arrows
denote distinguished moves, dashed arrows denote dont care moves.

The general method defined by Korf, Felner, and colleagues (Korf & Felner, 2002; Felner
et al., 2004; Korf & Taylor, 1996) creates a set of k additive abstractions by partitioning
the tiles into k disjoint groups and defining one abstraction for each group by making the
tiles in that group distinguished in the abstraction. An important limitation of this and
most other existing methods of defining additive abstractions is that they do not apply to
spaces in which an operator can move more than one tile at a time, unless there is a way
to guarantee that all the tiles that are moved by the operator are in the same group.
An example of a state space that has no additive abstractions according to previous
definitions is the Pancake puzzle. In the N -Pancake puzzle, a state is a permutation of N
tiles (0, 1, ..., N  1) and has N  1 successors, with the lth successor formed by reversing
the order of the first l + 1 positions of the permutation (1  l  N  1). For example, in the
4-Pancake puzzle shown in Figure 4, the state at the top of the figure has three successors,
which are formed by reversing the order of the first two tiles, the first three tiles, and all four
tiles, respectively. Because the operators move more than one tile and any tile can appear
in any location there is no non-trivial way to partition the tiles so that all the tiles moved
by an operator are distinguished in just one abstraction. Other common state spaces that
have no additive abstractions according to previous definitionsfor similar reasonsare
Rubiks Cube and TopSpin.
635

fiYang, Culberson, Holte, Zahavi & Felner

Figure 4: In the 4-Pancake puzzle each state has three successors.

The general definition of additive abstractions presented in the next section overcomes
the limitations of previous definitions. Intuitively, abstractions will be additive provided
that the cost of each operator is divided among the abstract spaces. Our definition provides
a formal basis for this intuition. There are numerous ways to do this even when operators
move many tiles (or, in other words, make changes to many state variables). For example,
the operator cost might be divided proportionally across the abstractions based on the
percentage of the tiles moved by the operator that are distinguished in each abstraction.
We call this method of defining abstract costs cost-splitting. For example, consider two
abstractions of the 4-Pancake puzzle, one in which tiles 0 and 1 are distinguished, the other
in which tiles 2 and 3 are distinguished. Then the middle operator in Figure 4 would have
a cost of 23 in the first abstract space and 13 in the second abstract space, because of the
three tiles this operator moves, two are distinguished in the first abstraction and one is
distinguished in the second abstraction.
A different method for dividing operator costs among abstractions focuses on a specific location (or locations) in the puzzle and assigns the full cost of the operator to the
abstraction in which the tile that moves into this location is distinguished. We call this
a location-based cost definition. In the Pancake puzzle it is natural to use the leftmost
location as the special location since every operator changes the tile in this location. The
middle operator in Figure 4 would have a cost of 0 in the abstract space in which tiles 0
and 1 are distinguished and a cost of 1 in the abstract space in which tiles 2 and 3 are
distinguished because the operator moves tile 2 into the leftmost location.
Both these methods apply to Rubiks Cube and TopSpin, and many other state spaces in
addition to the Pancake puzzle, but the hadd heuristics they produce are not always superior
to the hmax heuristics based on the same tile partitions. The theory and experiments in the
remainder of the paper shed some light on the general question of when hadd is preferable
to hmax .

3. Formal Theory of Additive Abstractions
In this section, we give formal definitions and lemmas related to state spaces, abstractions,
and the heuristics defined by them, and discuss their meanings and relation to previous
work. The definitions of state space etc. in Section 3.1 are standard, and the definition of
state space abstraction in Section 3.2 differs from previous definitions only in one important
detail: each state transition in an abstract space has two costs associated with it instead
of just one. The main new contribution is the definition of additive abstractions in Section
3.3.
636

fiA General Theory of Additive State Space Abstractions

The underlying structure of our abstraction definition is a directed graph (digraph)
homomorphism. For easy reference, we quote here standard definitions of digraph and
digraph homomorphism (Hell & Nesetril, 2004).
Definition 3.1 A digraph G is a finite set V = V (G) of vertices, together with a binary
relation E = E(G) on V. The elements (u, v) of E are called the arcs of G.
Definition 3.2 Let G and H be any digraphs. A homomorphism of G to H, written as
f : G  H is a mapping f : V (G)  V (H) such that (f (u), f (v))  E(H) whenever
(u, v)  E(G).
Note that the digraphs G and H may have self-loops, (u, u), and a homomorphism is
not required to be surjective in either vertices or arcs. We typically refer to arcs as edges,
but it should be kept in mind that, in general, they are directed edges, or ordered pairs.
3.1 State Space
Definition 3.3 A state space is a weighted directed graph S = hT, , Ci where T is a finite
set of states,   T  T is a set of directed edges (ordered pairs of states) representing state
transitions, and C :   N = {0, 1, 2, 3, . . . } is the edge cost function.
In typical practice, S is defined implicitly. Usually each distinct state in T corresponds to
an assignment of values to a set of state variables.  and C derive from a successor function,
or a set of planing operators. In some cases, T is restricted to the set of states reachable
from a given state. For example, in the 8-puzzle, the set of edges  is defined by the rule a
tile that is adjacent to the empty location can be moved into the empty location, and the
set of states T is defined in one of two ways: either as the set of states reachable from the
goal state, or as the set of permutations of the tiles and the blank, in which case T consists
of two components that are not connected to one another. The standard cost function C
for the 8-puzzle assigns a cost of 1 to all edges, but it is easy to imagine cost functions for
the 8-puzzle that depend on the tile being moved or the locations involved in the move.
A path from state t to state g is a sequence of edges beginning at t and ending at
g. Formally, p~ is a path from state t to state g if p~ = h 1 , . . . ,  n i,  j   where  j =
(tj1 , tj ), j  {1, . . . , n} and t0 = t, tn = g. Note the use of superscripts rather than
subscripts to distinguish states and edges
P within a state space. The length of p~ is the
number of edges n and its cost is C(~
p) = nj=1 C( j ). We use P aths(S, t, g) to denote the
set of all paths from t to g in S.
Definition 3.4 The optimal (minimum) cost of a path from state t to state g in S is defined
by
OPT(t, g) =

min

p
~P aths(S,t,g)

C(~
p)

A pathfinding problem is a triple hS, t, gi, where S is a state space and t, g  T , with
the objective of finding the minimum cost of a path from t to g, or in some cases finding a
minimum cost path p~  P aths(S, t, g) such that C(~
p) = OPT(t, g). Having just one goal
state may seem restrictive, but problems having a set of goal states can be accommodated
with this definition by adding a virtual goal state to the state space with zero-cost edges
from the actual goal states to the virtual goal state.
637

fiYang, Culberson, Holte, Zahavi & Felner

3.2 State Space Abstraction
Definition 3.5 An Abstraction System is a pair hS, i where S = hT, , Ci is a state space
and  = {hAi , i i | i : S  Ai , 1  i  k} is a set of abstractions, where each abstraction
is a pair consisting of an abstract state space and an abstraction mapping, where abstract
state space and abstraction mapping are defined below.
Note that these abstractions are not intended to form a hierarchy and should be considered a set of independent abstractions.
Definition 3.6 An abstract state space is a directed graph with two weights per edge,
defined by a four-tuple Ai = hTi , i , Ci , Ri i.
Ti is the set of abstract states and i is the set of abstract edges, as in the definition of
a state space. In an abstract space there are two costs associated with each i  i , the
primary cost Ci : i  N and the residual cost Ri : i  N . The idea of having
two costs per abstract edge, instead of just one, is inspired by the practice, illustrated in
Figure 3, of having two types of edges in the abstract space and counting distinguished
moves differently than dont care moves. In that example, our primary cost is the cost
associated with the distinguished moves, and our residual cost is the cost associated with
the dont care moves. The usefulness of considering the cost of dont care moves arises
when the abstraction system is additive, as suggested by Lemmas 3.6 and 3.10 below. These
indicate when the additive heuristic is infeasible and can be improved, the effectiveness of
which will become apparent in the experiments reported in Section 6.
Like edges, each abstract path p~i = hi1 , . . . , in i in Ai has a primary and residual cost:
P
P
pi ) = nj=1 Ri (ij ).
Ci (~
pi ) = nj=1 Ci (ij ), and Ri (~
Definition 3.7 An abstraction mapping i : S  Ai between state space S and abstract
state space Ai is defined by a mapping between the states of S and the states of Ai , i :
T  Ti , that satisfies the two following conditions.
The first condition is that the mapping is a homomorphism and thus connectivity in the
original space is preserved, i.e.,

(1)

(u, v)  , (i (u), i (v))  i

In other words, for each edge in the original space S there is a corresponding edge in the
abstract space Ai . Note that if u 6= v and i (u) = i (v) then a non-identity edge in S gets
mapped to an identity edge (self-loop) in Ai . We use the shorthand notation tji = i (tj )
for the abstract state in Ti corresponding to tj  T , and ij = i ( j ) = (i (uj ), i (v j )) for
the abstract edge in i corresponding to  j = (uj , v j )  .
The second condition that the state mapping must satisfy is that abstract edges must
not cost more than any of the edges they correspond to in the original state space, i.e.,

(2)

  , Ci (i ) + Ri (i )  C()
638

fiA General Theory of Additive State Space Abstractions

As a consequence, if multiple edges in the original space map to the same abstract edge
  i , as is usually the case, Ci () + Ri () must be less than or equal to all of them, i.e.,
  i , Ci () + Ri () 

min

C()

,i ()=

Note that if no edge maps to an edge in the abstract space, then no bound on the cost of
that edge is imposed.
For example, the state mapping used to define the abstraction in the middle column
of Figure 3 maps an 8-puzzle state to an abstract state by renaming tiles 2, 4, 6, and 8
to dont care. This mapping satisfies condition (1) because dont care tiles can be
exchanged with the blank whenever regular tiles can. It satisfies condition (2) because each
move is either a distinguished move (Ci (i ) = 1 and Ri (i ) = 0) or a dont care move
(Ci (i ) = 0 and Ri (i ) = 1) and in both cases Ci (i ) + Ri (i ) = 1, the cost of the edge 
in the original space.
The set of abstract states Ti is usually equal to i (T ) = {i (t) | t  T }, but it can be a
superset, in which case the abstraction is said to be non-surjective (Hernadvolgyi & Holte,
2000). Likewise, the set of abstract edges i is usually equal to i () = {i () |   }
but it can be a superset even if Ti = i (T ). In some cases, one deliberately chooses an
abstract space that has states or edges that have no counterpart in the original space. For
example, the methods that define abstractions by dropping operator preconditions must, by
their very design, create abstract spaces that have edges that do not correspond to any edge
in the original space (e.g. Pearl, 1984). In other cases, non-surjectivity is an inadvertent
consequence of the abstract space being defined implicitly as the set of states reachable
from the abstract goal state by applying operator inverses. For example, if a tile in the
2  2 sliding tile puzzle is mapped to the blank in the abstract space, the puzzle now has
two blanks and states are reachable in the abstract space that have no counterpart in the
original space (Hernadvolgyi & Holte, 2000). For additional examples and an extensive
discussion of non-surjectivity see the previous paper by Holte and Hernadvolgyi (2004).
All the lemmas and definitions that follow assume an abstraction system hS, i containing k abstractions has been given. Conditions (1) and (2) guarantee the following.
Lemma 3.1 For any path p~  P aths(S, u1 , u2 ) in S, there is a corresponding abstract path
i (~
p) from u1i to u2i in Ai and Ci (i (~
p)) + Ri (i (~
p))  C(~
p).
Proof: By definition, p~  P aths(S, u1 , u2 ) in S is a sequence of edges h 1 , . . . ,  n i,  j  
where  j = (tj1 , tj ), j  {1, . . . , n} and t0 = u1 , tn = u2 . Because i  i (), each of the
corresponding abstract edges exists (ij  i ). Because i1 = (u1i , t1i ) and in = (tin1 , u2i ),
the sequence i (~
p) = hi1 , . P
. . , in i is a path from u1i to u2i .
By definition, C(~
p) = nj=1 C( j ). For each  j , Condition (2) ensures that C( j ) 
P
P
P
Ci (ij )+Ri (ij ), and therefore C(~
p)  nj=1 (Ci (ij )+Ri (ij )) = nj=1 Ci (ij )+ nj=1 Ri (ij ) =
Ci (i (~
p)) + Ri (i (~
p)).
For example, consider state A and goal g in Figure 3. Because of condition (1), any path
from state A to g in the original space is also a path from abstract state A1 to abstract goal
state g1 and from abstract state A2 to g2 in the abstract spaces. Because of condition (2),
the cost of the path in the original space is greater than or equal to the sum of the primary
cost and the residual cost of the corresponding abstract path in each abstract space.
639

fiYang, Culberson, Holte, Zahavi & Felner

We use P aths(Ai , u, v) to mean the set of all paths from u to v in space Ai .
Definition 3.8 The optimal abstract cost from abstract state u to abstract state v in Ai
is defined as
OPTi (u, v) =
min
Ci (~q) + Ri (~q)
q~P aths(Ai ,u,v)

Definition 3.9 We define the heuristic obtained from abstract space Ai for the cost from
state t to g as
hi (t, g) = OPTi (ti , gi ).
Note that in these definitions, the path minimizing the cost is not required to be the image,
i (~
p), of a path p~ in S.
The following prove that the heuristic generated by each individual abstraction is admissible (Lemma 3.2) and consistent (Lemma 3.3).
Lemma 3.2 hi (t, g)  OPT(t, g) for all t, g  T and all i  {1, . . . , k}.
Proof: By Lemma 3.1, C(~
p)  Ci (i (~
p)) + Ri (i (~
p)), and therefore
min

p
~P aths(S,t,g)

C(~
p) 

min

p
~P aths(S,t,g)

Ci (i (~
p)) + Ri (i (~
p)).

The left hand side of this inequality is OPT(t, g) by definition, and the right hand side
is proved in the following Claim 3.2.1 to be greater than or equal to hi (t, g). Therefore,
OPT(t, g)  hi (t, g).
Claim 3.2.1 minp~P aths(S,t,g) Ci (i (~
p)) + Ri (i (~
p))  hi (t, g) for all t, g  T .
Proof of Claim 3.2.1:
By Lemma 3.1 for every path p~ there is a corresponding abstract path. There may also be additional paths in the abstract space, that is,
{i (~
p) | p~  P aths(S, t, g)}  P aths(Ai , ti , gi ). It follows that {Ci (i (~
p)) + Ri (i (~
p)) | p~ 
P aths(S, t, g)}  {Ci (~q) + Ri (~q) | ~q  P aths(Ai , ti , gi )}. Therefore,
min

p
~P aths(S,t,g)

Ci (i (~
p)) + Ri (i (~
p)) 

min

q~P aths(Ai ,ti ,gi )

Ci (~q) + Ri (~q) = OPTi (ti , gi ) = hi (t, g)

Lemma 3.3 hi (t1 , g)  OPT(t1 , t2 ) + hi (t2 , g) for all t1 , t2 , g  T and all i  {1, . . . , k}.
Proof: By the definition of OPTi as a minimization and the definition of hi (t, g), it follows
that hi (t1 , g) = OPTi (t1i , gi )  OPTi (t1i , t2i ) + OPTi (t2i , gi ) = OPTi (t1i , t2i ) + hi (t2 , g).
To complete the proof, we observe that by Lemma 3.2, OPT(t1 , t2 )  hi (t1 , t2 ) =
OPTi (t1i , t2i ).
Definition 3.10 The hmax heuristic from state t to state g defined by an abstraction system
hS, i is
k

hmax (t, g) = max hi (t, g)
i=1

From Lemmas 3.2 and 3.3 it immediately follows that hmax is admissible and consistent.
640

fiA General Theory of Additive State Space Abstractions

3.3 Additive Abstractions
In this section, we formalize the notion of additive abstraction that was introduced intuitively in Section 2.1. The example there showed that hadd (t, g), the sum of the heuristics
for state t defined by multiple abstractions, was admissible provided the cost functions in
the abstract spaces only counted the distinguished moves. In our formal framework, the
cost of distinguished moves is captured by the notion of primary cost.
Definition 3.11 For any pair of states t, g  T the additive heuristic given an abstraction
system is defined to be
hadd (t, g) =

k
X

Ci (ti , gi ).

i=1

where
Ci (ti , gi ) =

min

q~P aths(Ai ,ti ,gi )

Ci (~q)

is the minimum primary cost of a path in the abstract space from ti to gi .
In Figure 3, for example, C1 (A1 , g1 ) = 9 and C2 (A2 , g2 ) = 5 because the minimum
number of distinguished moves to reach g1 from A1 is 9 and the minimum number of
distinguished moves to reach g2 from A2 is 5.
Intuitively, hadd will be admissible if the cost of edge  in the original space is divided
among the abstract edges i that correspond to , as is done by the cost-splitting and
location-based methods for defining abstract costs that were introduced at the end of
Section 2.1. This leads to the following formal definition.
P
Definition 3.12 An abstraction system hS, i is additive if   , ki=1 Ci (i )  C().
The following prove that hadd is admissible (Lemma 3.4) and consistent (Lemma 3.5)
when the abstraction system hS, i is additive.
Lemma 3.4 If hS, i is additive then hadd (t, g)  OPT(t, g) for all t, g  T .
Proof: Assume
g) = C(~
p), where p~ = h 1 , . . . ,  n i  P aths(S, t, g). Therefore,
Pnthat OPT(t,
j
OPT(t, g) = j=1 C( ). Since hS, i is additive, it follows by definition that
n
X

C( j ) 

j=1

n X
k
X

Ci (ij ) =

j=1 i=1



k
X

k X
n
X

Ci (ij )

i=1 j=1

Ci (ti , gi ) = hadd (t, g)

i=1

where the last line follows from the definitions of Ci and hadd .
Lemma 3.5 If hS, i is additive then hadd (t1 , g)  OPT(t1 , t2 )+hadd (t2 , g) for all t1 , t2 , g 
T.
641

fiYang, Culberson, Holte, Zahavi & Felner

Proof: Ci (t1i , gi ) obeys the triangle inequality: Ci (t1i , gi )  Ci (t1i , t2i ) + Ci (t2i , gi ) for all
P
P
P
t1 , t2 , g  T . It follows that ki=1 Ci (t1i , gi )  ki=1 Ci (t1i , t2i ) + ki=1 Ci (t2i , gi ).
Pk
P
Because i=1 Ci (t1i , gi ) = hadd (t1 , g) and ki=1 Ci (t2i , gi ) = hadd (t2 , g), it follows that
P
hadd (t1 , g)  ki=1 Ci (t1i , t2i ) + hadd (t2 , g).
P
Since hS, i is additive, by Lemma 3.4, OPT(t1 , t2 )  ki=1 Ci (t1i , t2i ).
Hence hadd (t1 , g)  OPT(t1 , t2 ) + hadd (t2 , g) for all t1 , t2 , g  T .
We now develop a simple test that has important consequences for additive heuristics.
Define P~i (ti , gi ) = {~q | ~q  P aths(Ai , ti , gi ) and Ci (~q) = Ci (ti , gi )}, the set of abstract
paths from ti to gi whose primary cost is minimal.
Definition 3.13 The conditional optimal residual cost is the minimum residual cost among
the paths in P~i (ti , gi ):
Ri (ti , gi ) = min Ri (~q)
~i (ti ,gi )
q~P

Note that the value of (Ci (ti , gi ) + Ri (ti , gi )) is sometimes, but not always, equal to the
optimal abstract cost OPTi (ti , gi ). In Figure 3, for example, OPT1 (A1 , g1 ) = 16 (a path
with this cost is shown in Figure 2) and C1 (A1 , g1 ) + R1 (A1 , g1 ) = 18, while C2 (A2 , g2 ) +
R2 (A2 , g2 ) = OPT2 (A2 , g2 ) = 12. As the following lemmas show, it is possible to draw
important conclusions about hadd by comparing its value to (Ci (ti , gi ) + Ri (ti , gi )).
Lemma 3.6 Let hS, i be any additive abstraction system and let t, g  T be any states. If
hadd (t, g)  Cj (tj , gj ) + Rj (tj , gj ) for all j  {1, . . . , k}, then hadd (t, g)  hmax (t, g).
Proof: By the definition of OPTi (ti , gi ), j  {1, . . . , k}, Cj (tj , gj )+Rj (tj , gj )  OPTj (tj , gj ).
Therefore, j  {1, . . . , k}, hadd (t, g)  Cj (tj , gj ) + Rj (tj , gj )  OPTj (tj , gj )  hadd (t, g) 
max1ik OPTi (ti , gi ) = hmax (t, g).
Lemma 3.7 For an additive hS, i and path p~  P aths(S, t, g) with C(~
p) =
Cj (j (~
p)) = Cj (tj , gj ) for all j  {1, . . . , k}.

Pk


i=1 Ci (ti , gi ),

Proof: Suppose for a contradictionP
that there exists some i1 , such that Ci1 (i1 (~
p)) >
k


Ci1 (ti1 , gi1 ). Then because C(~
p) =
i=1 Ci (ti , gi ), there must exist some i2 , such that
Ci2 (i2 (~
p)) < Ci2 (ti2 , gi2 ), which contradicts the definition of Ci . Therefore, such an i1
does not exist and Cj (j (~
p)) = Cj (tj , gj ) for all j  {1, . . . , k}.
Lemma 3.8 For an additive hS, i and a path p~  P aths(S, t, g) with C(~
p) =
Ri (i (~
p))  Ri (ti , gi ) for all i  {1, . . . , k}.

Pk


i=1 Ci (ti , gi ),

Proof: Following Lemma 3.7 and the definition of P~i (ti , gi ), i (~
p)  P~i (ti , gi ) for all i 

{1, . . . , k}. Because Ri (ti , gi ) is the smallest residual cost of paths in P~i (ti , gi ), it follows
that Ri (i (~
p))  Ri (ti , gi ).
Lemma 3.9 For an additive hS, i and a path p~  P aths(S, t, g) with C(~
p) =
Pk
 (t , g )  C  (t , g ) + R (t , g ) for all j  {1, . . . , k}.
C
j j j
j j j
i=1 i i i
642

Pk


i=1 Ci (ti , gi ),

fiA General Theory of Additive State Space Abstractions

Proof: By Lemma 3.1, C(~
p)  Cj (j (~
p)) + Rj (j (~
p)) for all j  {1, . . . , k}. By Lemma 3.7
Cj (j (~
p)) = Cj (tj , gj ), and by Lemma 3.8 Rj (j (~
p))  Rj (tj , gj ). Therefore C(~
p) 
Pk



Cj (tj , gj ) + Rj (tj , gj ), and the lemma follows from the premise that C(~
p) = i=1 Ci (ti , gi ).

Lemma 3.10 Let hS, i be any additive abstraction system and let t, g  T be any states.
If hadd (t, g) < Cj (tj , gj ) + Rj (tj , gj ) for some j  {1, . . . , k}, then hadd (t, g) 6= OP T (t, g).
Proof: This lemma follows directly as the contrapositive of Lemma 3.9.
Lemma 3.6 gives a condition under which hadd is guaranteed to be at least as large as
hmax for a specific states t and g. If this condition holds for a large fraction of the state
space T , one would expect that search using hadd to be at least as fast as, and possibly
faster than, search using hmax . This will be seen in the experiments reported in Section 4.
The opposite is not true in general, i.e., failing this condition does not imply that hmax will
result in faster search than hadd . However, as Lemma 3.10 shows, there is an interesting
consequence when this condition fails for state t: we know that the value returned by hadd
for t is not the true cost to reach the goal from t. Detecting this is useful because it allows
the heuristic value to be increased without risking it becoming inadmissible. Section 6
explores this in detail.
3.4 Relation to Previous Work
The aim of the preceding formal definitions is to identify fundamental properties that guarantee that abstractions will give rise to admissible, consistent heuristics. We have shown
that the following two conditions guarantee that the heuristic defined by an abstraction is
admissible and consistent
(P 1)

(u, v)  , (i (u), i (v))  i

(P 2)

  , C()  Ci (i ) + Ri (i )

and that a third condition
(P 3)

  , C() 

k
X

Ci (i )

i=1

guarantees that hadd (t, g) is admissible and consistent.
Previous work has focused on defining abstraction and additivity for specific ways of
representing states and transition functions. These are important contributions because
ultimately one needs computationally effective ways of defining the abstract state spaces,
abstraction mappings, and cost functions that our theory takes as given. The importance
of our contribution is that it should make future proofs of admissibility, consistency, and
additivity easier, because one will only need to show that a particular method for defining
abstractions satisfies the three preceding conditions. These are generally very simple conditions to demonstrate, as we will now do for several methods for defining abstractions and
additivity that currently exist in the literature.
643

fiYang, Culberson, Holte, Zahavi & Felner

3.4.1 Previous Definitions of Abstraction
The use of abstraction to create heuristics began in the late 1970s and was popularized in
Pearls landmark book on heuristics (Pearl, 1984). Two abstraction methods were identified at that time: relaxing a state space definition by dropping operator preconditions
(Gaschnig, 1979; Guida & Somalvico, 1979; Pearl, 1984; Valtorta, 1984), and homomorphic abstractions (Banerji, 1980; Kibler, 1982). These early notions of abstraction were
unified and extended by Mostow and Prieditis (1989) and Prieditis (1993), producing a
formal definition that is the same as ours in all important respects except for the concept
of residual cost that we have introduced.1
Todays two most commonly used abstraction methods are among the ones implemented
in Prieditiss Absolver II system (Prieditis, 1993). The first is domain abstraction, which
was independently introduced in the seminal work on pattern databases (Culberson &
Schaeffer, 1994, 1998) and then generalized (Hernadvolgyi & Holte, 2000). It assumes a
state is represented by a set of state variables, each of which has a set of possible values
called its domain. An abstraction on states is defined by specifying a mapping from the
original domains to new, smaller domains. For example, an 8-puzzle state is typically
represented by 9 variables, one for each location in the puzzle, each with the same domain
of 9 elements, one for each tile and one more for the blank. A domain abstraction that
maps all the elements representing the tiles to the same new element (dont care) and
the blank to a different element would produce the abstract space shown in Figure 1. The
reason this particular example satisfies property (P1) is explained in Section 3.2. In general,
a domain abstraction will satisfy property (P1) as long as the conditions that define when
state transitions occur (e.g. operator preconditions) are guaranteed to be satisfied by the
dont care symbol whenever they are satisfied by one or more of the domain elements
that map to dont care. Property (P2) follows immediately from the fact that all state
transitions in the original and abstract spaces have a primary cost of 1.
The other major type of abstraction used today, called drop by Prieditis (1993), was
independently introduced for abstracting planning domains represented by grounded (or
propositional) STRIPS operators (Edelkamp, 2001). In a STRIPS representation, a state
is represented by the set of logical atoms that are true in that state, and the directed edges
between states are represented by a set of operators, where each operator a is described by
three sets of atoms, P (a), A(a), and D(a). P (a) lists as preconditions: a can be applied to
state t only if all the atoms in P (a) are true in t (i.e., P (a)  t). A(a) and D(a) specify the
effects of operator a, with A(a) listing the atoms that become true when a is applied (the
add list) and D(a) listing the atoms that become false when a is applied (the delete
list). Hence if operator a is applicable to state t, the state u = a(t) it produces when applied
to t is the set of atoms u = (t  D(a))  A(a).
In this setting, Edelkamp defined an abstraction of a given state space by specifying a
subset of the atoms and restricting the abstract state descriptions and operator definitions
to include only atoms in the subset. Suppose Vi is the subset of the atoms underlying
abstraction mapping i : S  Ai , where S is the original state space and Ai is the abstract
state space based on Vi . Two states in S will be mapped to the same abstract state if and
1. Prieditiss definition allows an abstraction to expand the set of goals. This can be achieved in our
definition by mapping non-goal states in the original space to the same abstract state as the goal.

644

fiA General Theory of Additive State Space Abstractions

only if they contain the same subset of atoms in Vi , i.e., i (t) = i (u) iff t  Vi = u  Vi .
This satisfies property (P1) because operator a being applicable to state t (P (a)  t)
implies abstract operator ai = i (a) is applicable to abstract state ti (P (a)  Vi  t  Vi )
and the resulting state a(t) = (t  D(a))  A(a) is mapped by i to ai (i (t)) because
set intersection distributes across set subtraction and union (Vi  ((t  D(a))  A(a)) =
((Vi  t)  (Vi  D(a)))  (Vi  A(a))). Again, property (P2) follows immediately from the
fact that all operators in the original and abstract spaces have a primary cost of 1.
Recently, Helmert et al. (2007) described a more general approach to defining abstractions for planning based on transition graph abstractions. A transition graph is a directed
graph in which the arcs have labels, and a transition graph abstraction is a directed graph
homomorphism that preserves the labels.2 Hence, Helmert et al.s method is a restricted
version of our definition of abstraction and therefore satisfies properties (P1) and (P2).
Helmert et al. make the following interesting observations that are true of our more general
definition of abstractions:
 the composition of two abstractions is an abstraction. In other words, if  : S  A
is an abstraction of S and  : A  B is an abstraction of A, then () : S  B is
an abstraction of S. This property of abstractions was exploited by Prieditis (1993).
 the product A1  A2 of two abstractions, A1 and A2 , of S is an abstraction of S,
where the state space of the product is the Cartesian product of the two abstract
state spaces, and there is an edge 12 in the product space from state (t1 , t2 ) to state
(u1 , u2 ) if there is an edge 1 from t1 to u1 in A1 and there is an edge 2 from t2 to
u2 in A2 . The primary cost of 12 is the minimum of C1 (1 ) and C2 (2 ) and the
residual cost of 12 is taken from the same space as the primary cost. Because they
are working with labelled edges Helmert et al. require the edge connecting t1 to u1
to have the same label as the edge connecting t2 to u2 ; this is called a synchronized
product and is denoted A1  A2 (refer to Definition 6 defined by Helmert et al. (2007)
for the exact definition of synchronized product).
Figure 5 shows the synchronized product, B, of two abstractions, A1 and A2 , of the
3-state space S in which the edge labels are a and b. A1 is derived from S by mapping
states s1 and s2 to the same state (s1,2 ), and A2 is derived from S by mapping states s2
and s3 to the same state (s2,3 ). Note that B contains four states, more than the original
space. It is an abstraction of S because the mapping of original states s1 , s2 , and s3 to
states (s1,2 , s1 ) (s1,2 , s2,3 ) and (s3 , s2,3 ), respectively, satisfies property (P1), and property
(P2) is satisfied automatically because all edges have a cost of 1. From this point of view
the fourth state in B, (s3 , s1 ), is redundant with state (s1,2 , s1 ). Nevertheless it is a distinct
state in the product space.
Haslum et al. (2005) introduce a family of heuristics, called hm (for any fixed m 
{1, 2, ...}), that are based on abstraction, but are not covered by our definition because the
value of the heuristic for state t, hm (t), is not defined as the distance from the abstraction
of t to the abstract goal state. Instead it takes advantage of a special monotonicity property
2. Homomorphism here means the standard definition of a digraph homomorphism (Definition 3.2),
which permits non-surjectivity (as discussed in Section 3.2), as opposed to Helmert et al.s definition of
homomorphism, which does not allow non-surjectivity.

645

fiYang, Culberson, Holte, Zahavi & Felner

a
a

s1

b

s2

s3

a

a

b

s1, 2

s3

a

b
a

s2 , 3

s1

B=



( s3 , s2 , 3 )

b
a

a

a

( s1, 2 , s1 ) ( s1, 2 , s2, 3 )

( s3 , s1 )

Figure 5: S is the original state space. A1 and A2 are abstractions of S. B = A1  A2 is
the synchronized product of A1 and A2 .

of costs in planning problems: the cost of achieving a subset of the atoms defining the goal
is a lower bound on the cost of achieving the goal. When searching backwards from the
goal to the start state, as Haslum et al. do, this allows an admissible heuristic to be defined
in the following recursive minimax fashion (|t| denotes the number of atoms in state t):

0,
t  start



min C(s, t) + hm (s), |t|  m
m
h (t) =
(s,t)


|t| > m
 max hm (s),
st,|s|m

The first two lines of this definition are the standard method for calculating the cost of a
least-cost path. It is the third line that uses the fact that the cost of achieving any subset
of the atoms in t is a lower bound on the cost of achieving the entire set of atoms t. The
recursive calculation alternates between the min and max calculation depending on the
number of atoms in the state currently being considered in the recursive calculation, and
is therefore different than a shortest path calculation or taking the maximum of a set of
shortest path calculations.
3.4.2 Previous definitions of additive abstractions
Prieditis (1993) included a method (Factor) in his Absolver II system for creating additive
abstractions, but did not present any formal definitions or theory.
The first thorough discussion of additive abstractions is due to Korf and Taylor (1996).
They observed that the sliding tile puzzles Manhattan Distance heuristic, and several of
its enhancements, were the sum of the distances in a set of abstract spaces in which a small
number of tiles were distinguished. As explained in Section 2.1, what allowed the abstract
distances to be added and still be a lower bound on distances in the original space is that
only the moves of the distinguished tiles counted towards the abstract distance and no tile
646

fiA General Theory of Additive State Space Abstractions

was distinguished in more than one abstraction. This idea was later developed in a series
of papers (Korf & Felner, 2002; Felner et al., 2004), which extended its application to other
domains, such as the 4-peg Towers of Hanoi puzzle.
In the planning literature, the same idea was proposed by Haslum et al. (2005), who
described it as partitioning the operators into disjoint sets B1 , ...Bk and counting the cost
of operators in set Bi only in abstract space Ai . The example they give is that in the
Blocks World operators that move block i would all be in set Bi , effectively defining a set
of additive abstractions for the Blocks World exactly analogous to the Korf and Taylor
abstractions that define Manhattan Distance for the sliding tile puzzle.
Edelkamp (2001) took a different approach to defining additive abstractions for STRIPS
planning representations. His method involves partitioning the atoms into disjoint sets
V1 , ...Vk such that no operator changes atoms in more than one group. If abstract space
Ai retains only the atoms in set Vi then the operators that do not affect atoms in Vi will
have no effect at all in abstract space Ai and will naturally have a cost of 0 in Ai . Since
no operator affects atoms in more than one group, no operator has a non-zero cost in more
than one abstract space and distances in the abstract spaces can safely be added. Haslum et
al. (2007) extended this idea to representations in which state variables could have multiple
values. In a subsequent paper Edelkamp (2002) remarks that if there is no partitioning of
atoms that induces a partitioning of the operators as just described, additivity could be
enforced by assigning an operator a cost of zero in all but one of the abstract spacesa
return to the Korf and Taylor idea.
All the methods just described might be called all-or-nothing methods of defining
abstract costs, because the cost of each edge C() is fully assigned as the cost of the
corresponding abstract edge Ci (i ) in one of the abstractions and the corresponding edges
in all the other abstractions are assigned a cost of zero. Any such method obviously satisfies
property (P3) and is therefore additive.
Our theory of additivity does not require abstract methods to be defined in an all-ornothing manner, it allows C() to be divided in any way whatsoever among the abstractions
as long as property (P3) is satisfied. This possibility has been recognized in one recent publication (Katz & Domshlak, 2007), which did not report any experimental results. This
generalization is important because it eliminates the requirement that operators must move
only one tile or change atoms/variables in one group, and the related requirement that
tiles/atoms be distinguished/represented in exactly one of the abstract spaces. This requirement restricted the application of previous methods for defining additive abstractions,
precluding their application to state spaces such as Rubiks Cube, the Pancake puzzle, and
TopSpin. As the following sections show, with our definition, additive abstractions can be
defined for any state space, including the three just mentioned.
Finally, Helmert et al. (2007) showed that the synchronized product of additive abstractions produces a heuristic hsprod that dominates hadd , in the sense that hsprod (s)  hadd (s)
for all states s. This happens because the synchronized product forces the same path to be
used in all the abstract spaces, whereas the calculation of each Ci in hadd can be based on
a different path. The discussion of the negative results and infeasibility below highlight the
problems that can arise because each Ci is calculated independently.
647

fiYang, Culberson, Holte, Zahavi & Felner

4. New Applications of Additive Abstractions
This section and the next section report the results of applying the general definition of
additive abstraction given in the previous section to three benchmark state spaces: TopSpin,
the Pancake puzzle and Rubiks Cube. A few additional experimental results may be found
in the previous paper by Yang, Culberson, and Holte (2007). In all our experiments all edges
in the original state spaces have a cost of 1 and we define Ri (i ) = 1  Ci (i ), its maximum
permitted value when edges cost 1. We use pattern databases to store the heuristic values.
The pre-processing time required to compute the pattern databases is excluded from the
times reported in the results, because the PDB needs to be calculated only once and this
overhead is amortized over the solving of many problem instances.

4.1 Methods for Defining Costs
We will investigate two general methods for defining the primary cost of an abstract state
transition Ci (i ), which we call cost-splitting and location-based costs. To illustrate
the generality of these methods we will define them for the two most common ways of
representing statesas a vector of state variables, which is the method we implemented in
our experiments, and as a set of logical atoms as in the STRIPS representation for planning
problems.
In a state variable representation a state t is represented by a vector of m state variables,
each having its own domain of possible values Dj , i.e., t = (t(0), ..., t(m  1)), where
t(j)  Dj is the value assigned to the j th state variable in state t. For example, in puzzles
such as the Pancake puzzle and the sliding tile puzzles, there is typically one variable for
each physical location in the puzzle, and the value of t(j) indicates which tile is in location
j in state t. In this case the domain for all the variables is the same. State space abstractions
are defined by abstracting the domains. In particular, in this setting domain abstraction i
will leave specific domain values unchanged (the distinguished values according to i ) and
map all the rest to the same special value, dont care. The abstract state corresponding
to t according to i is ti =(ti (0), ..., ti (m  1)) with ti (j) = i (t(j)). As in previous research
with these state spaces a set of abstractions is defined by partitioning the domain values into
disjoint sets E1 , ..., Ek with Ei being the set of distinguished values in abstraction i. Note
that the theory developed in the previous section does not require the distinguished values
in different abstractions to be mutually exclusive; it allows a value to be distinguished in
any number of abstract spaces provided abstract costs are defined appropriately.
As mentioned previously, in a STRIPS representation a state is represented by the
set of logical atoms that are true in the state. A state variable representation can be
converted to a STRIPS representation in a variety of ways, the simplest being to define an
atom for each possible variable-value combination. If state variable j has value v in the
state variable representation of state t then the atom variable-j-has-value-v is true in the
STRIPS representation of t. The exact equivalent of domain abstraction can be achieved
by defining Vi , the set of atoms to be used in abstraction i, to be all the atoms variable-jhas-value-v in which v  Ei , the set of distinguished values for domain abstraction i.
648

fiA General Theory of Additive State Space Abstractions

4.1.1 Cost-splitting
In a state variable representation, the cost-splitting method of defining primary costs works
as follows. A state transition  that changes b state variables has its cost, C(), split
among the corresponding abstract state transitions 1 , . . . , k in proportion to the number
of distinguished values they assign to the variables, i.e., in abstraction i
Ci (i ) =

bi  C()
b

if  changes b variables and bi of them are assigned distinguished values by i .3 For
example, the 3  3  3 Rubiks cube is composed of twenty little moveable cubies and
each operator moves eight cubies, four corner cubies and four edge cubies. Hence b = 8
for all state transitions . If a particular state transition moves three cubies that are
distinguished according to abstraction i , the corresponding abstract state transition, i ,
would cost 38 . Strictly speaking, we require abstract edge costs to be integers, so the
fractional edge costs produced by cost-splitting must be scaled appropriately to become
integers. Our implementation of cost-splitting actually does this scaling but it will simplify
our presentation of cost-splitting to talk of the edge costs as if they were fractional.
If each domain value is distinguished in at most one abstraction (e.g. if the abstractions
are defined byPpartitioning the domain values) cost-splitting produces additive abstractions,
i.e., C()  ki=1 Ci (i ) for all   . Because C() is known to be an integer, hadd can
P
be defined to be the ceiling of the sum of the abstract distances, d ki=1 Ci (i )e, instead of
just the sum.
With a STRIPS representation, cost-splitting could be defined identically, with b being
the number of atoms changed (added or deleted) by operator  in the original space and bi
being the number of atoms changed by the corresponding operator in abstraction i.
4.1.2 Location-based Costs
In a location-based cost definition for a state variable representation, a state variable loc
is associated with state transition  and s full cost C() is assigned to abstract state
transition i if i changes the value of variable loc to a value that is distinguished according
to i .4 Formally:

 C(), if i = (t1i , t2i ), t1i (loc ) 6= t2i (loc ), and
t2i (loc ) is a distinguished value according to i .
Ci (i ) =

0,
otherwise.
Instead of focusing on the value that is assigned to variable loc , location-based costs
can be defined equally well on the value that variable loc had before it was changed. In
either case, if each domain value is distinguished in at most one abstraction location-based
3. Because i might correspond to several edges in the the original space, each with a different cost or
moving a different set of tiles, the technically correct definition is:
Ci (i ) =

min
,i ()=i

bi  C()
b

4. As in footnote 3, the technically correct definition has min,i ()=i C() instead of C().

649

fiYang, Culberson, Holte, Zahavi & Felner

costs produce additive abstractions. The name location-based is based on the typical
representations used for puzzles, in which there is a state variable for each physical location
in the puzzle. For example, in Rubiks Cube one could choose the reference variables to be
the ones representing the two diagonally opposite corner locations in the puzzle. Note that
each possible Rubiks cube operator changes exactly one of these locations. An abstract
state transition would have a primary cost of 1 if the cubie it moved into one of these
locations was a distinguished cubie in its abstraction, and a primary cost of 0 otherwise.
For a STRIPS representation of states, location-based costs can be defined by choosing
an atom a in the Add list for each operator  and assigning the full cost C() to abstraction
i if a appears in the Add list of i . If atoms are partitioned so that each atom appears in
at most one abstraction, this method will define additive costs.
Although the cost-splitting and location-based methods for defining costs can be applied
to a wide range of state spaces, they are not guaranteed to define heuristics that are superior
to other heuristics for a given state space. We determined experimentally that heuristics
based on cost-splitting substantially improve performance for sufficiently large versions of
TopSpin and that heuristics based on location-based costs vastly improve the state of the
art for the 17-Pancake puzzle. In our experiments additive heuristics did not improve the
state of the art for Rubiks Cube. The following subsections describe the positive results in
detail. The negative results are discussed in Section 5.
4.2 TopSpin with Cost-Splitting
In the (N, K)-TopSpin puzzle (see Figure 6) there are N tiles (numbered 1, . . . , N ) arranged
on a circular track, and two physical movements are possible: (1) the entire set of tiles may
be rotated around the track, and (2) a segment consisting of K adjacent tiles in the track
may be reversed. As in previous formulations of this puzzle as a state space (Felner, Zahavi,
Schaeffer, & Holte, 2005; Holte et al., 2005; Holte, Newton, Felner, Meshulam, & Furcy,
2004), we do not represent the first physical movement as an operator, but instead designate
one of the tiles (tile 1) as a reference tile with the goal being to get the other tiles in increasing
order starting from this tile (regardless of its position). The state space therefore has N
operators (numbered 1, . . . , N ), with operator a reversing the segment of length K starting
at position a relative to the current position of tile 1. For certain combinations of N and K
all possible permutations can be generated from the standard goal state by these operators,
but in general the space consists of connected components and so not all states are reachable
(Chen & Skiena, 1996). In the experiments in this section, K = 4 and N is varied.
The sets of abstractions used in these experiments are described using a tuple written
as a1 a2 . . . aM , indicating that the set contains M abstractions, with tiles 1 . . . (a1 ) distinguished in the first abstraction, tiles (a1 + 1) . . . (a1 + a2 ) distinguished in the second
abstraction, and so on. For example, 6-6-6 denotes a set of three abstractions in which the
distinguished tiles are (1 . . . 6), (7 . . . 12), and (13 . . . 18) respectively.
The experiments compare hadd , the additive use of a set of abstractions, with hmax , the
standard use of the same abstractions, in which, as described in Section 2, the full cost of
each state transition is counted in each abstraction and the heuristic returns the maximum
distance to goal returned by the different abstractions. Cost-splitting is used to define
operator costs in the abstract spaces for hadd . Because K = 4, each operator moves 4 tiles.
650

fiA General Theory of Additive State Space Abstractions

3

4

5

6

7

2

8

9

1

10
11

20
19

12
18

17

16

15

14

13

Figure 6: The TopSpin puzzle.

If bi of these are distinguished tiles when operator op is applied to state si in abstraction i,
applying op to si has a primary cost of b4i in abstraction i.
In these experiments the heuristic defined by each abstraction is stored in a pattern
database (PDB). Each abstraction would normally be used to define its own PDB, so that
a set of M abstractions would require M PDBs. However, for TopSpin, if two (or more)
abstractions have the same number of distinguished tiles and the distinguished tiles are all
adjacent, one PDB can be used for all of them by suitably renaming the tiles before doing
the PDB lookup. For the 6-6-6 abstractions, for example, only one PDB is needed, but
three lookups would be done in it, one for each abstraction. Because the position of tile 1
is effectively fixed, this PDB is N times smaller than it would normally be. For example,
with N = 18, the PDB for the 6-6-6 abstractions contains 17  16  . . .  13 entries. The
memory needed for each entry in the hadd PDBs is twice the memory needed for an entry
in the hmax PDBs because of the need to represent fractional values.
We ran experiments for the values of N and sets of abstractions shown in the first two
columns of Table 1. Start states were generated by a random walk of 150 moves from the
goal state. There were 1000, 50 and 20 start states for N = 12, 16 and 18, respectively.
The average solution length for these start states is shown in the third column of Table 1.
The average number of nodes generated and the average CPU time (in seconds) for IDA
to solve the given start states is shown in the Nodes and Time columns for each of hmax
and hadd . The Nodes Ratio column gives the ratio of Nodes using hadd to Nodes using
hmax . A ratio less than one (highlighted in bold) indicates that hadd , the heuristic based on
additive abstractions with cost-splitting, is superior to hmax , the standard heuristic using
the same set of abstractions.
When N = 12 and N = 16 the best performance is achieved by hmax based on a pair
of abstractions each having N2 distinguished tiles. As N increases the advantage of hmax
decreases and, when N = 18, hadd outperforms hmax for all abstractions used. Moreover,
even for the smaller values of N hadd outperforms hmax when a set of four abstractions
with N4 distinguished tiles each is used. This is important because as N increases, memory
limitations will preclude using abstractions with N2 distinguished tiles and the only option
will be to use more abstractions with fewer distinguished tiles each. The results in Table 1
show that hadd will be the method of choice in this situation.
651

fiYang, Culberson, Holte, Zahavi & Felner

N
12
12
12
16
16
18
18

Abs
6-6
4-4-4
3-3-3-3
8-8
4-4-4-4
9-9
6-6-6

Average
Solution
Length
9.138
9.138
9.138
14.040
14.040
17.000
17.000

hmax
Nodes
14,821
269,974
1,762,262
1,361,042
4,494,414,929
38,646,344
18,438,031,512

Time
0.05
1.10
8.16
3.42
13,575.00
165.42
108,155.00

hadd based on
cost-splitting
Nodes
Time
53,460
0.16
346,446
1.33
1,388,183
6.44
2,137,740
4.74
251,946,069
851.00
21,285,298
91.76
879,249,695 4,713.00

Nodes
Ratio
3.60
1.28
0.78
1.57
0.056
0.55
0.04

Table 1: (N, 4)-TopSpin results using cost-splitting.
4.3 The Pancake Puzzle with Location-based Costs
In this section, we present the experimental results on the 17-Pancake puzzle using locationbased costs. The same notation as in the previous section is used to denote sets of abstractions, e.g. 5-6-6 denotes a set of three abstractions, with the first having tiles (0 . . . 4) as
its distinguished tiles, the second having tiles (5 . . . 10) as its distinguished tiles, and the
third having tiles (11 . . . 16) as its distinguished tiles. Also as before, the heuristic for each
abstraction is precomputed and stored in a pattern database (PDB). Unlike TopSpin, there
are no symmetries in the Pancake puzzle that enable different abstractions to make use
of the same PDB, so a set of M abstractions for the Pancake puzzle requires M different
PDBs.
Additive abstractions are defined using the location-based method with just one reference location, the leftmost position. This position was chosen because the tile in this
position changes whenever any operator is applied to any state in the original state space.
This means that every edge cost in the original space will be fully counted in some abstract
space as long as each tile is a distinguished tile in some abstraction. As before, we use hadd
to denote the heuristic defined by adding the values returned by the individual additive
abstractions.
Our first experiment compares IDA using hadd with the best results known for the
17-Pancake puzzle (Zahavi, Felner, Holte, & Schaeffer, 2006) (shown in Table 2), which
were obtained using a single abstraction having the rightmost seven tiles (1016) as its
distinguished tiles and an advanced search technique called Dual IDA (DIDA ).5 DIDA
is an extension of IDA that exploits the fact that, when states are permutations of tiles
as in the Pancake puzzle, each state s has an easily computable dual state sd with the
special property that inverses of paths from s to the goal are paths from sd to the goal. If
paths and their inverses cost the same, DIDA defines the heuristic value for state s as the
maximum of h(s) and h(sd ), and sometimes will decide to search for a least-cost path from
sd to goal when it is looking for a path from s to goal.
5. In particular, DIDA with the jump if larger (JIL) policy and the bidirectional pathmax method
(BPMX) to propagate the inconsistent heuristic values that arise during dual search. Zahavi et al.
(2006) provided more details. BPMX was first introduced by Felner et al. (2005).

652

fiA General Theory of Additive State Space Abstractions

The results of this experiment are shown in the top three rows of Table 3. The Algorithm column indicates the heuristic search algorithm. The Abs column shows the set of
abstractions used to generate heuristics. The Nodes column shows the average number of
nodes generated in solving 1000 randomly generated start states. These start states have
an average solution length of 15.77. The Time column gives the average number of CPU
seconds needed to solve these start states on an AMD Athlon(tm) 64 Processor 3700+ with
2.4 GHz clock rate and 1GB memory. The Memory column indicates the total size of each
set of PDBs.
N

Algorithm

17

DIDA

Abs
rightmost-7

Average
Solution
Length
15.77

h based on
a single large PDB
Nodes
Time
Memory
124,198,462 37.713 98,017,920

Table 2: The best results known for the 17-Pancake puzzle (Zahavi et al., 2006), which were
obtained using a single abstraction having the rightmost seven tiles (10  16) as its
distinguished tiles and an advanced search technique called Dual IDA (DIDA ).

N

Algorithm

17
17
17
17
17
17

IDA
IDA
IDA
DIDA
DIDA
DIDA

Abs
4-4-4-5
5-6-6
3-7-7
4-4-4-5
5-6-6
3-7-7

Average
Solution
Length
15.77
15.77
15.77
15.77
15.77
15.77

hadd based on
Location-based Costs
Nodes Time
Memory
14,610,039 4.302
913,920
1,064,108 0.342
18,564,000
1,061,383 0.383 196,039,920
368,925 0.195
913,920
44,618 0.028
18,564,000
37,155 0.026 196,039,920

Table 3: 17-Pancake puzzle results using hadd based on location-based costs.
Clearly, the use of hadd based on location-based costs results in a very significant reduction in nodes generated compared to using a single large PDB, even when the latter
has the advantage of being used by a more sophisticated search algorithm. Note that the
total memory needed for the 4-4-4-5 PDBs is only one percent of the memory needed for
the rightmost-7 PDB, and yet IDA with 4-4-4-5 generates 8.5 times fewer nodes than
DIDA with the rightmost-7 PDB. Getting excellent search performance from a very small
PDB is especially important in situations where the cost of computing the PDBs must be
taken into account in addition to the cost of problem-solving (Holte et al., 2005).
The memory requirements increase significantly when abstractions contain more distinguished tiles, but in this experiment the improvement of the running time does not increase
accordingly. For example, the 3-7-7 PDBs use ten times more memory than the 5-6-6 PDBs,
but the running time is almost the same. This is because the 5-6-6 PDBs are so accurate
653

fiYang, Culberson, Holte, Zahavi & Felner

there is little room to improve them. The average heuristic value on the start states using
the 5-6-6 PDBs is 13.594, only 2.2 less than the actual average solution length. The average
heuristic value using the 3-7-7 PDBs is only slightly higher (13.628).
The last three rows in Table 3 show the results when hadd with location-based costs
is used in conjunction with DIDA . These results show that combining our additive abstractions with state-of-the-art search techniques results in further significant reductions in
nodes generated and CPU time. For example, the 5-6-6 PDBs use only 1/5 of the memory
of the rightmost-7 PDB but reduce the number of nodes generated by DIDA by a factor
of 2783 and the CPU time by a factor of 1347.
To compare hadd to hmax we ran plain IDA with hmax on the same 1000 start states,
with a time limit for each start state ten times greater than the time needed to solve
the start state using hadd . With this time limit only 63 of the 1000 start states could
be solved with hmax using the 3-7-7 abstraction, only 5 could be solved with hmax using
the 5-6-6 abstraction, and only 3 could be solved with hmax using the 4-4-4-5 abstraction.
To determine if hadd s superiority over hmax for location-based costs on this puzzle could
have been predicted using Lemma 3.6, we generated 100 million random 17-Pancake puzzle
states and tested how many satisfied the requirements of Lemma 3.6. Over 98% of the
states satisfied those requirements for the 3-7-7 abstraction, and over 99.8% of the states
satisfied its requirements for the 5-6-6 and 4-4-4-5 abstractions.

5. Negative Results
Not all of our experiments yielded positive results. Here we explore some trials where our
additive approaches did not perform as well. By examining some of these cases closely, we
shed light on the conditions which might indicate when these approaches will be useful.
5.1 TopSpin with Location-Based Costs
In this experiment, we used the 6-6-6 abstraction of (18, 4)-TopSpin as in Section 4.2 but
with location-based costs instead of cost-splitting. The primary cost of operator a, the
operator that reverses the segment consisting of locations a to a + 3 (modulo 18), is 1
in abstract space i if the tile in location a before the operator is applied is distinguished
according to abstraction i and 0 otherwise.
This definition of costs was disastrous, resulting in Ci (ti , gi ) = 0 for all abstract states
in all abstractions. In other words, in finding a least-cost path it was never necessary to
use operator a when there was a distinguished tile in location a. It was always possible
to move towards the goal by applying another operator, a0 , with a primary cost of 0. To
illustrate how this is possible, consider state 0 4 5 6 3 2 1 of (7, 4)-T opSpin.
This state can be transformed into the goal in a single move: the operator that reverses the
four tiles starting with tile 3 produces the state 3 4 5 6 0 1 2 which is equal to
the goal state when it is cyclically shifted to put 0 into the leftmost position. With the 4-3
abstraction this move has a primary cost of 0 in the abstract space based on tiles 4...6, but
it would have a primary cost of 1 in the abstract space based on tiles 0...3 (because tile 3
is in the leftmost location changed by the operator). However the following sequence maps
tiles 0...3 to their goal locations and has a primary cost of 0 in this abstract space (because
a dont care tile is always moved from the reference location):
654

fiA General Theory of Additive State Space Abstractions

0

*

*

*

3

2

1

0

*

*

1

2

3

*

0

*

3

2

1

*

*

0

1

2

3

*

*

*

5.2 Rubiks Cube
The success of cost-splitting on (18,4)-TopSpin suggested it might also provide an improved heuristic for Rubiks Cube, which can be viewed as a 3-dimensional version of
(20,8)-TopSpin. We used the standard method of partitioning the cubies to create three
abstractions, one based on the 8 corner cubies, and the others based on 6 edge cubies each.
The standard heuristic based on this partitioning, hmax , expanded approximately three
times fewer nodes than hadd based on this partitioning and primary costs defined by costsplitting. The result was similar whether the 24 symmetries of Rubiks Cube were used to
define multiple heuristic lookups or not.
We believe the reason for cost-splitting working well for (18,4)-TopSpin but not Rubiks
Cube is that an operator in Rubiks Cube moves more cubies than the number of tiles
moved by an operator in (18,4)-TopSpin. To test if operators moving more tiles reduces the
effectiveness of cost-splitting we solved 1000 instances of (12,K)-TopSpin for various values
of K, all using the 3-3-3-3 abstraction. The results are shown in Table 4. The Nodes Ratio
column gives the ratio of Nodes using hadd to Nodes using hmax . A ratio less than one
(highlighted in bold) indicates that hadd is superior to hmax . The results clearly show that
hadd based on cost-splitting is superior to hmax for small K and steadily loses its advantage
as K increases. The same phenomenon can also be seen in Table 1, where increasing N
relative to K increases the effectiveness of additive heuristics based on cost-splitting.

K
3
4
5
6

hmax
Nodes
486,515
1,762,262
8,978
193,335,181

Time
2.206
8.164
0.043
901.000

hadd based on
cost-splitting
Nodes
Time
207,479
0.952
1,388,183
6.437
20,096
0.095
2,459,204,715 11,457.000

Nodes
Ratio
0.42
0.78
2.23
12.72

Table 4: (12, K)-TopSpin results using cost-splitting.
We also investigated location-based costs for Rubiks Cube. The cubies were partitioned
into four groups, each containing three edge cubies and two corner cubies, and an abstraction
was defined using each group. Two diagonally opposite corner positions were used as the
reference locations (as noted above, each Rubiks Cube operator changes exactly one of these
locations). The resulting hadd heuristic was so weak we could not solve random instances
of the puzzle with it.
655

fiYang, Culberson, Holte, Zahavi & Felner

5.3 The Pancake Puzzle with Cost-Splitting
Table 5 compares hadd and hmax on the 13-Pancake puzzle when costs are defined using
cost-splitting. The memory is greater for hadd than hmax because the fractional entries that
cost-splitting produces require more bits per entry than the small integer values stored in
the hmax PDB. In terms of both run-time and number of nodes generated, hadd is inferior
to hmax for these costs, the opposite of what was seen in Section 4.3 using location-based
costs.
N
13

Abs
6-7

Average
Solution
Length
11.791

hmax
Nodes
166,479

Time
0.0466

hadd based on
costing-splitting
Nodes
Time
1,218,903 0.3622

Table 5: hadd vs. hmax on the 13-Pancake puzzle.
Cost-splitting, as we have defined it for the Pancake puzzle, adversely affects hadd because it enables each individual abstraction to get artificially low estimates of the cost of
solving its distinguished tiles by increasing the number of dont care tiles that are moved.
For example, with cost-splitting the least-cost sequence of operators to get tile 0 into
its goal position from abstract state * 0 * * * is not the obvious single move of
reversing the first two positions. That move costs 21 , whereas the 2-move sequence that
reverses the entire state and then reverses the first four positions costs only 15 + 14 .
As a specific example, consider state 7 4 5 6 3 8 0 10 9 2 1 11 of
the 12-Pancake puzzle. Using the 6-6 abstractions, the minimum number of moves to get
tiles 05 into their goal positions is 8, and for 611 it is 7, where in each case we ignore the
final locations of the other tiles. Thus, hmax is 8. By contrast, hadd is 6.918, which is less
than even the smaller of the two numbers used to define hmax . The two move sequences
whose costs are added to compute hadd for this state each have slightly more moves than
the corresponding sequences on which hmax is based (10 and 9 compared to 8 and 7), but
involve more than twice as many dont care tiles (45 and 44 compared to 11 and 17) and
so are less costly.
There is hope that this pathological situation can be detected, at least sometimes, by
inspecting the residual costs. If the residual costs are defined to be complementary to the
primary costs (i.e. Ri (i ) = C()  Ci (i )), as we have done, then decreasing the primary
cost increases the residual cost. If the residual cost is sufficiently large in one of the abstract
spaces the conditions of Lemma 3.10 will be satisfied, signalling that the value returned by
hadd is provably too low. This is the subject of the next section, on infeasibility.

6. Infeasible Heuristic Values
This section describes a way to increase the heuristic values defined by additive abstractions
in some circumstances. The key to the approach is to identify infeasible valuesones that
cannot possibly be the optimal solution cost. Once identified the infeasible values can be
increased to give a better estimate of the solution cost. An example of infeasibility occurs
656

fiA General Theory of Additive State Space Abstractions

with the Manhattan Distance (MD) heuristic for the sliding tile puzzle. It is well-known
that the parity of MD(t) is the same as the parity of the optimal solution cost for state t. If
some other heuristic for the sliding tile puzzle returns a value of the opposite parity, it can
safely be increased until it has the correct parity. This example relies on specific properties
of the MD heuristic and the puzzle. Lemma 3.10 gives a problem-independent method for
testing infeasibility, and that is what we will use.
To illustrate how infeasibility can be detected using Lemma 3.10 consider the example
in Figure 3. The solution to the abstract problem shown in the middle part of the figure
requires 9 distinguished moves, so C1 (A1 ) = 9. The abstract paths that solve the problem
with 9 distinguished moves require, at a minimum, 9 dont care moves, so R1 (A1 ) = 9.
A similar calculation for the abstract space on the right of the figure yields C2 (A2 ) = 5
and R2 (A2 ) = 7. The value of hadd (A, g) is therefore C1 (A1 ) + C2 (A2 ) = 9 + 5 = 14.
This value is based on the assumption that there is a path in the original space that makes
C1 (A1 ) = 9 moves of tiles 1, 3, 5, and 7, and C2 (A2 ) = 5 moves of the other tiles. However,
the value of R1 (A1 ) tells us that any path that uses only 9 moves of tiles 1, 3, 5, and 7 to
put them into their goal locations must make at least 9 moves of the other tiles, it cannot
possibly make just 5 moves. Therefore there does not exist a solution costing as little as
C1 (A1 ) + C2 (A2 ) = 14.
To illustrate the potential of this method for improving additive heuristics, Table 6
shows the average results of IDA solving 1000 test instances of the 15-puzzle using two
different tile partitionings (shown in Figure 7) and costs defined by the method described
in Section 2.1. These additive heuristics have the same parity property as Manhattan
Distance, so when infeasibility is detected 2 can be added to the value. The hadd columns
show the average heuristic value of the 1000 start states. As can be seen infeasibility
checking increases the initial heuristic value by over 0.5 and reduces the number of nodes
generated and the CPU time by over a factor of 2. However, there is a space penalty for
this improvement, because the R values must be stored in the pattern database in addition
to the normal C  values. This doubles the amount of memory required, and it is not clear
if storing R is the best way to use this extra memory. This experiment merely shows that
infeasibility checking is one way to use extra memory to speed up search for some problems.

1 2 3
4 5 6 7
8 9 10 11
12 13 14 15

4

1
5

2
6

3
7

8 9 10 11
12 13 14 15

Figure 7: Different tile partitionings for the 15-puzzle (left: 5-5-5; right: 6-6-3).
Slightly stronger results were obtained for the (N, 4)-TopSpin puzzle with costs defined
by cost-splitting, as described in Section 4.2. The No Infeasibility Check columns in Table
7 are the same as the hadd based on cost-splitting columns of the corresponding rows in
Table 1. Comparing these to the Infeasibility Check columns shows that in most cases
infeasibility checking reduces the number of nodes generated and the CPU time by roughly
a factor of 2.
657

fiYang, Culberson, Holte, Zahavi & Felner

N

Abs

15
15

5-5-5
6-6-3

Average
Solution
Length
52.522
52.522

hadd based on zero-one cost-splitting
No Infeasibility Check
Infeasibility Check
hadd
Nodes Time
hadd
Nodes Time
41.56 3,186,654 0.642 42.10 1,453,358 0.312
42.13 1,858,899 0.379 42.78
784,145 0.171

Table 6: The effect of infeasibility checking on the 15-puzzle.

When location-based costs are used with TopSpin infeasibility checking adds one to the
heuristic value of almost every state. However, this simply means that most states have a
heuristic value of 1 instead of 0 (recall the discussion in Section 5.1), which is still a very
poor heuristic.

N

Abs

12
12
12
16
16
18

6-6
4-4-4
3-3-3-3
8-8
4-4-4-4
6-6-6

Average
Solution
Length
9.138
9.138
9.138
14.040
14.040
17.000

hadd based on costing-splitting
No Infeasibility Check
Infeasibility Check
Nodes
Time
Nodes
Time
53,460
0.16
20,229
0.07
346,446
1.33
174,293
0.62
1,388,183
6.44
1,078,853
4.90
2,137,740
4.74
705,790
1.80
251,946,069
851.00 203,213,736
772.04
879,249,695 4,713.00 508,851,444 2,846.52

Table 7: The effect of infeasibility checking on (N, 4)-TopSpin using cost-splitting.

Infeasibility checking produces almost no benefit for the 17-Pancake puzzle with locationbased costs because the conditions of Lemma 3.10 are almost never satisfied. The experiment discussed at the end of Section 4.3 showed that fewer than 2% of the states satisfy
the conditions of Lemma 3.10 for the 3-7-7 abstraction, and fewer than 0.2% of the states
satisfy the conditions of Lemma 3.10 for the 5-6-6 and 4-4-4-5 abstractions.
Infeasibility checking for the 13-Pancake puzzle with cost-splitting also produces very
little benefit, but for a different reason. For example, Table 8 shows the effect of infeasibility
checking on the 13-Pancake puzzle; the results shown are averages over 1000 start states.
1
Cost-splitting in this state space produces fractional edge costs that are multiples of 360360
(360360 is the Least Common Multiple of the integers from 1 to 13), and therefore if
1
infeasibility is detected the amount added is 360360
. But recall that hadd , with cost-splitting,
Pk
is defined as the ceiling of i=1 Ci (i ). The value of hadd will therefore be the same, whether
1
360360 is added or not, unless the sum of the Ci (i ) is exactly an integer. As Table 8 shows,
this does happen but only rarely.
658

fiA General Theory of Additive State Space Abstractions

N

Abs

13

6-7

Average
Solution
Length
11.791

hadd based on costing-splitting
No Infeasibility Check Infeasibility Check
Nodes
Time
Nodes
Time
1,218,903
0.3622 1,218,789 0.4453

Table 8: The effect of infeasibility checking on the 13-Pancake puzzle using cost-splitting.

7. Conclusions
In this paper we have presented a formal, general definition of additive abstractions that
removes the restrictions of most previous definitions, thereby enabling additive abstractions to be defined for any state space. We have proven that heuristics based on additive
abstractions are consistent as well as admissible. Our definition formalizes the intuitive
idea that abstractions will be additive provided the cost of each operator is divided among
the abstract spaces, and we have presented two specific, practical methods for defining abstract costs, cost-splitting and location-based costs. These methods were applied to three
standard state spaces that did not have additive abstractions according to previous definitions: TopSpin, Rubiks Cube, and the Pancake puzzle. Additive abstractions using
cost-splitting reduce search time substantially for (18,4)-TopSpin and additive abstractions
using location-based costs reduce search time for the 17-Pancake puzzle by three orders of
magnitude over the state of the art. We also report negative results, for example on Rubiks
Cube, demonstrating that additive abstractions are not always superior to the standard,
maximum-based method for combining multiple abstractions.
A distinctive feature of our definition is that each edge in an abstract space has two costs
instead of just one. This was inspired by previous definitions treating distinguished moves
differently than dont care moves in calculating least-cost abstract paths. Formalizing this
idea with two costs per edge has enabled us to develop a way of testing if the heuristic value
returned by additive abstractions is provably too low (infeasible). This test produced no
speedup when applied to the Pancake puzzle, but roughly halved the search time for the
15-puzzle and in most of our experiments with TopSpin.

8. Acknowledgments
This research was supported in part by funding from Canadas Natural Sciences and Engineering Research Council (NSERC). Sandra Zilles and Jonathan Schaeffer suggested useful
improvements to drafts of this paper. This research was also supported by the Israel Science
Foundation (ISF) under grant number 728/06 to Ariel Felner.

References
Banerji, R. B. (1980). Artificial Intelligence: A Theoretical Approach. North Holland.
Chen, T., & Skiena, S. (1996). Sorting with fixed-length reversals. Discrete Applied Mathematics, 71 (13), 269295.
659

fiYang, Culberson, Holte, Zahavi & Felner

Culberson, J. C., & Schaeffer, J. (1996). Searching with pattern databases. In Advances
in Artificial Intelligence (Lecture Notes in Artificial Intelligence 1081), pp. 402416.
Springer.
Culberson, J. C., & Schaeffer, J. (1994). Efficiently searching the 15-puzzle. Tech. rep.
TR94-08, Department of Computing Science, University of Alberta.
Culberson, J. C., & Schaeffer, J. (1998). Pattern databases. Computational Intelligence,
14(3), 318334.
Edelkamp, S. (2001). Planning with pattern databases. In Proceedings of the 6th European
Conference on Planning, pp. 1324.
Edelkamp, S. (2002). Symbolic pattern databases in heuristic search planning. In Proceedings of Sixth International Conference on AI Planning and Scheduling (AIPS-02), pp.
274283.
Erdem, E., & Tillier, E. (2005). Genome rearrangement and planning. In Proceedings of the
Twentieth National Conference on Artificial Intelligence (AAAI-05), pp. 11391144.
Felner, A., & Adler, A. (2005). Solving the 24 puzzle with instance dependent pattern
databases. Proc. SARA-2005, Lecture Notes in Artificial Intelligence, 3607, 248260.
Felner, A., Korf, E., & Hanan, S. (2004). Additive pattern database heuristics. Journal of
Artificial Intelligence Research, 22, 279318.
Felner, A., Zahavi, U., Schaeffer, J., & Holte, R. (2005). Dual lookups in pattern databases.
In Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence (IJCAI-05), pp. 103108.
Gaschnig, J. (1979). A problem similarity approach to devising heuristics: First results.
In Proceedings of the Sixth International Joint Conference on Artificial Intelligence
(IJCAI-79), pp. 301307.
Guida, G., & Somalvico, M. (1979). A method for computing heuristics in problem solving.
Information Sciences, 19, 251259.
Haslum, P., Bonet, B., & Geffner, H. (2005). New admissible heurisitics for domainindependent planning. In Proceedings of The Twentieth National Conference on Artificial Intelligence (AAAI-05), pp. 11631168.
Haslum, P., Botea, A., Helmert, M., Bonet, B., & Koenig, S. (2007). Domain-independent
construction of pattern database heuristics for cost-optimal planning. In Proceedings
of The Twenty-Second National Conference on Artificial Intelligence (AAAI-07), pp.
10071012.
Hell, P., & Nesetril, J. (2004). Graphs and homomorphisms. The Oxford Lecture Series in
Mathematics and its Applications, 28.
Helmert, M., Haslum, P., & Hoffmann, J. (2007). Flexible abstraction heuristics for optimal
sequential planning. In Proceedings of the 17th International Conference on Automated
Planning and Scheduling (ICAPS-07), pp. 176183.
Hernadvolgyi, I., & Holte, R. C. (2000). Experiments with automatically created memorybased heuristics. Proc. SARA-2000, Lecture Notes in Artificial Intelligence, 1864,
281290.
660

fiA General Theory of Additive State Space Abstractions

Hernadvolgyi, I. T. (2003). Solving the sequential ordering problem with automatically
generated lower bounds. In Proceedings of Operations Research 2003 (Heidelberg,
Germany), pp. 355362.
Holte, R. C., Perez, M. B., Zimmer, R. M., & MacDonald, A. J. (1996). Hierarchical A*:
Searching abstraction hierarchies efficiently. In Proceedings of the Thirteenth National
Conference on Artificial Intelligence (AAAI-96), pp. 530535.
Holte, R. C., Felner, A., Newton, J., Meshulam, R., & Furcy, D. (2006). Maximizing over
multiple pattern databases speeds up heuristic search. Artificial Intelligence, 170,
11231136.
Holte, R. C., Grajkowski, J., & Tanner, B. (2005). Hierarchical heuristic search revisited.
Proc. SARA-2005, Lecture Notes in Artificial Intelligence, 3607, 121133.
Holte, R. C., & Hernadvolgyi, I. T. (2004). Steps towards the automatic creation of search
heuristics. Tech. rep. TR04-02, Computing Science Department, University of Alberta,
Edmonton, Canada T6G 2E8.
Holte, R. C., Newton, J., Felner, A., Meshulam, R., & Furcy, D. (2004). Multiple pattern
databases. In Proceedings of the Fourteenth International Conference on Automated
Planning and Scheduling (ICAPS-04), pp. 122131.
Katz, M., & Domshlak, C. (2007). Structural patterns heuristics: Basic idea and concrete instance. In Proceedings of ICAPS-07 Workshop on Heuristics for Domain-independent
Planning: Progress, Ideas, Limitations, Challenges.
Kibler, D. (1982). Natural generation of admissible heuristics. Tech. rep. TR-188, University
of California at Irvine.
Korf, R. E. (1985). Depth-first iterative-deepening: An optimal admissible tree search.
Artificial Intelligence, 27 (1), 97109.
Korf, R. E. (1997). Finding optimal solutions to Rubiks Cube using pattern databases. In
Proceedings of the Fourteenth National Conference on Artificial Intelligence (AAAI97), pp. 700705.
Korf, R. E., & Felner, A. (2002). Disjoint pattern database heuristics. Artificial Intelligence,
134, 922.
Korf, R. E., & Taylor, L. A. (1996). Finding optimal solutions to the twenty-four puzzle. In
Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI96), pp. 12021207.
Mostow, J., & Prieditis, A. (1989). Discovering admissible heuristics by abstracting and
optimizing: A transformational approach. In Proceedings of the International Joint
Conference on Artificial Intelligence (IJCAI-89), pp. 701707.
Pearl, J. (1984). Heuristics. Addison Wesley, Reading, MA.
Prieditis, A. E. (1993). Machine discovery of effective admissible heuristics. Machine Learning, 12, 117141.
Valtorta, M. (1984). A result on the computational complexity of heuristic estimates for
the A* algorithm. Information Science, 34, 4859.
661

fiYang, Culberson, Holte, Zahavi & Felner

Yang, F., Culberson, J., & Holte, R. (2007). A general additive search abstraction. Tech. rep.
TR07-06, Computing Science Department, University of Alberta, Edmonton, Canada
T6G 2E8.
Zahavi, U., Felner, A., Holte, R., & Schaeffer, J. (2006). Dual search in permutation
state spaces. In Proceedings of the Twenty-First Conference on Artificial Intelligence
(AAAI-06), pp. 10761081.

662

fiJournal of Artificial Intelligence Research 32 (2008) 901-938

Submitted 08/07; published 08/08

The Ultrametric Constraint and its
Application to Phylogenetics
Neil C.A. Moore

ncam@cs.st-andrews.ac.uk

Computer Science, University of St. Andrews, Scotland

Patrick Prosser

pat@dcs.gla.ac.uk

Computing Science, Glasgow University, Scotland

Abstract
A phylogenetic tree shows the evolutionary relationships among species. Internal nodes
of the tree represent speciation events and leaf nodes correspond to species. A goal of
phylogenetics is to combine such trees into larger trees, called supertrees, whilst respecting
the relationships in the original trees. A rooted tree exhibits an ultrametric property; that
is, for any three leaves of the tree it must be that one pair has a deeper most recent common
ancestor than the other pairs, or that all three have the same most recent common ancestor.
This inspires a constraint programming encoding for rooted trees. We present an efficient
constraint that enforces the ultrametric property over a symmetric array of constrained
integer variables, with the inevitable property that the lower bounds of any three variables
are mutually supportive. We show that this allows an efficient constraint-based solution
to the supertree construction problem. We demonstrate that the versatility of constraint
programming can be exploited to allow solutions to variants of the supertree construction
problem.

1. Introduction
One of the grand challenges of phylogenetics is to build the Tree of Life (ToL), a representation of the evolutionary history of every living thing. To date, biologists have catalogued
about 1.7 million species, yet estimates of the total number of species range from 4 to 100
million. Of the 1.7 million species identified only about 80,000 have been placed in the ToL
so far (Pennisi, 2003). There are applications for the ToL: to help understand how pathogens
become more virulent over time, how new diseases emerge, and to recognise species at risk of
extinction (Pennisi, 2003; Mace, Gittleman, & Purvis, 2003). One approach to building the
ToL is divide and conquer: combining smaller trees such as those available from TreeBase
(TreeBASE, 2003) into so-called supertrees (Bininda-Emonds, 2004) to approach a more
complete ToL.
To date, supertree construction has been dominated by imperative techniques (Semple
& Steel, 2000; Semple, Daniel, Hordijk, Page, & Steel, 2004; Daniel, 2003; Bordewich,
Evans, & Semple, 2006; Ng & Wormald, 1996; Bryant & Steel, 1995; Page, 2002) but
recently new declarative approaches have emerged using constraint programming (Gent,
Prosser, Smith, & Wei, 2003; Prosser, 2006; Beldiceanu, Flener, & Lorca, 2008) and answer
set programming (Wu, You, & Lin, 2007). One of the properties of rooted trees that suits
these approaches is that trees are by their nature ultrametric: in rooted trees the root node
has depth 0, and the depth of other nodes is 1 plus the depth of their parent. Taking any
c
2008
AI Access Foundation. All rights reserved.

fiMoore & Prosser

three leaves a, b and c in pairs it must be that one pair has a deeper most recent common
ancestor (mrca) than the other pairs, or that all three pairs have the same mrca. And this
is what we mean by ultrametric, that for any three there is a tie for the minimum. In fact,
if we know the depth of the mrca of all pairs of leaves the structure of the tree is uniquely
determined. This inspires a constraint programming encoding for rooted trees, using the
ultrametric constraint that we will define later. We explore solutions to the phylogenetic
supertree problem and its variants. In so doing, we show the practicality of an ultrametric
encoding for rooted tree problems, as well as arguing that this is a valuable addition to the
set of techniques for supertree problems.
The paper is organised as follows. First, we introduce constraint programming and the
supertree construction problem. We then propose a specialised ultrametric constraint, in
terms of its propagation procedures, that maintains bounds(Z)-consistency (Bessiere, 2006)
on three variables. We show that this specialised constraint is required because models that
use toolkit primitives cannot guarantee the ultrametric property on the supertree problem via propagation alone. Furthermore, the space complexity of such models becomes
prohibitive. The ultrametric constraint is then extended to maintain the property over a
symmetric matrix of variables. We then go on to show that this constraint can be efficiently
applied to the problem of supertree construction, in particular that applying propagation to
this model gives a polynomial time procedure for supertree construction. We then demonstrate this on real data and give justification that there has been an improvement in time
and space over previous constraint encodings. One of the benefits of the constraint programming approach is that variants of the supertree problem can be addressed within this
one model. We justify this assertion by proposing a constraint solution finding essential
relations in the supertree (Daniel, 2003), addressing ancestral divergence dates (Semple
et al., 2004; Bryant, Semple, & Steel, 2004), modelling nested taxa (Page, 2004; Daniel &
Semple, 2004) and coping with conflicting data.

2. Background
In this section we give necessary definitions and descriptions of the Constraint Satisfaction
Problem (Tsang, 1993), Constraint Programming, and the Supertree problem.
2.1 Constraint Programming and the CSP
Constraint Programming (CP) (Rossi, van Beek, & Walsh, 2007) is a declarative style of
programming where problems are modelled as a CSP, i.e., as a set of variables that have
to be assigned values from those variables domains to satisfy a set of constraints. Values
might typically be integers drawn from finite domains, real numbers from ranges, or more
complex entities like sets or graphs. We will only be considering integers.
Definition 1. A constraint satisfaction problem (CSP) is a triple (V, D, C) where V is a set
of n variables {v1 , . . . , vn }; D = {dom(v1 ), . . . , dom(vn )} is a collection of domains, each a
totally ordered set of integer values; and C = {c1 , . . . , ce } is a set of e constraints, each with a
scope of variables scope(c) = (vc1 , . . . , vck ) and a relation rel(c)  dom(vc1 ). . .dom(vck ).
An assignment of value x  dom(v) to variable vi  V is denoted by (vi , x). A constraint
c  C is satisfied by an assignment {(vc1 , xc1 ), . . . , (vck , xck )} when scope(c) = (vc1 , . . . , vck )
902

fiThe Ultrametric Constraint and Phylogenetics

and (xc1 , . . . , xck )  rel(c). A set of assignments {(v1 , x1 ), . . . , (vn , xn )} involving every
variable in the problem is a solution when it satisfies all the constraints in C.
A constraint solver finds a solution to a CSP via a process of constraint propagation and
search. Constraint propagation is an inferencing process that takes place when a variable
is initialised or loses values. Propagation maintains a level of consistency, such as arcconsistency (Mackworth, 1977), across the variables, removing values from domains that
cannot occur in any solution (i.e., removing unsupported values). We use the definitions of
(generalized) arc-consistency ((G)AC) due to Bessiere (2006):
Definition 2. Given a CSP (V, D, C), a constraint c  C with scope(c) = (vc1 , . . . , vck ),
and a variable v  scope(c), a value x  dom(v) is consistent with respect to c (alternatively,
supported by c) iff there exists a satisfying assignment  = {(vc1 , a1 ), . . . , (vck , ak )} for c
such that (v, x)   and i, ai  dom(vci ). The domain dom(v) is (generalized) arcconsistent on c iff all values in dom(v) are consistent with respect to c, and the CSP is
(generalized) arc-consistent when all variable domains are (generalized) arc-consistent on
all constraints in C.
Arc-consistency can be established on a CSP using an algorithm such as AC3 (Mackworth, 1977). For sake of exposition we assume all constraints in C are binary and
that for each constraint c we have a counterpart c such that scope(c) = (va , vb ) and
scope(c ) = (vb , va ) with rel(c ) = rel1 (c). For example, if we have the constraint
cxy = x < y then we also have the constraint cyx = y > x. At the heart of AC3 is the revise
function, which takes a binary constraint c as its argument and delivers a Boolean result.
The function removes from dom(va ) all values that have no support in dom(vb ) w.r.t. the
constraint c, and returns true if any removals take place. Initially all constraints are added
to a set S. Constraints are iteratively removed from S and revised. If revise(ckm ) returns
true then S becomes S  {cik |cik  C  i 6= k  i 6= m}. This step can be considered as
the propagation of a domain reduction on variable vk to variables constrained by vk . The
iteration terminates when S is empty or a variables domain becomes empty. When S is
empty the arc-consistency algorithm has reached a fixed point (i.e., a further application
of the arc-consistency process will have no effect on the domains of the variables) and the
problem has been made arc-consistent. When a domain empties, we have shown the there
are no solutions globally and hence we can stop. The AC3 algorithm has O(e  d3 ) time
complexity, where e is the number of constraints and d the size of the largest domain, however other algorithms can achieve a time bound of O(e  d2 ) (Yuanlin & Yap, 2001; Bessiere
& Regin, 2001).
We demonstrate arc-consistency with the example of Figure 1 by Smith (1995). We
have three constrained integer variables x, y and z, each with an integer domain {1..5},
and binary constraints cxy : x < y  2, cyz : y + z is even, and czx : z < 2x + 1. Since the
constraints are binary we can represent the problem as a constraint graph, where nodes are
vertices and edges are constraints. Initially the constraint cxy is revised with respect to x
and the values {3..5} are removed from dom(x). Then cxy is revised w.r.t. y and dom(y)
becomes {4, 5}. cyz is then revised w.r.t. y with no effect and then revised w.r.t. z, again with
no effect. Revising czx w.r.t. z reduces dom(z) such that it becomes {1..4}, consequently
the constraint cyz is added into the set of constraints pending revision. Constraint czx is
903

fiMoore & Prosser

y {1..5}

se

y

zi

2

y+
n

x<

ve

x
{1..5}

z {1..5}

z < 2x + 1

Figure 1: A binary constraint satisfaction problem.
import choco.Problem;
import choco.ContradictionException;
import choco.integer.*;
public class BMStut {
public static void main(String[] args) throws ContradictionException {
Problem pb
= new Problem();
IntDomainVar x
= pb.makeEnumIntVar("x",1,5); // x in {1..5}
IntDomainVar y
= pb.makeEnumIntVar("y",1,5); // y in {1..5}
IntDomainVar z
= pb.makeEnumIntVar("z",1,5); // y in {1..5}
IntDomainVar even = pb.makeEnumIntVar("even",new int[] {2,4,6,8,10});
pb.post(pb.gt(pb.minus(y,2),x));
// x < y - 2
pb.post(pb.gt(pb.plus(pb.mult(2,x),1),z)); // z < 2x + 1
pb.post(pb.eq(even,pb.plus(y,z)));
// y + z is even
pb.solve();

// solve using MAC

}
}

Figure 2: A JChoco constraint program for the CSP of Figure 1.
revised w.r.t. x and then cyz w.r.t. to y, both with no effect. The revision set at that point
is empty and arc-consistency has been established with variable domains dom(x) = {1, 2},
dom(y) = {4, 5}, and dom(z) = {1..4}.
Solving a CSP may involve search, i.e., we might need to try different values for variables in order to determine if a solution exists. Typically a constraint solver will begin by
establishing arc-consistency, and then repeatedly select a variable and assign it a value from
its domain (instantiate it). This effectively reduces that variables domain to a singleton,
and arc-consistency is then re-established. If this succeeds another instantiation is made,
but if it fails we backtrack by undoing the most recent instantiation. This is called MAC,
for maintaining arc-consistency (Sabin & Freuder, 1994).
Figure 2 shows a constraint program for the problem in Figure 1 using the choco constraint programming toolkit for the Java language (Choco, 2008), and it finds solution
x = 1, y = 4, and z = 2 first.
Constraint toolkits tend to be based around the AC5 algorithm (van Hentenryck, Deville,
& Teng, 1992), allowing propagators to be specialised for specific constraints resulting in
improved efficiency and adaptability. AC5 amends set S from AC3 to contain triples of
the form (v, c, ) where v  scope(c) and  is the set of values lost by v, consequently
904

fiThe Ultrametric Constraint and Phylogenetics

revision is more efficient because propagation can focus of values that may have lost support,
rather than having to check every value for support. In an object-oriented toolkit language
a constraint has associated propagation methods that should be implemented, and these
methods are activated when a domain event occurs on a variable involved in that constraint.
Domain events can be the initialisation of a variable, an increase in the lower bound, a
decrease in the upper bound, the removal of a value between the bounds, or the instantiation
of that variable. This is an exhaustive list, however some toolkits allow only one event:
that one or more values have been lost and the propagator writer must then determine
what action to take. To give examples of using a toolkit with specialised constraints, when
modelling a routing problem we might have a constrained integer variable for each location
to be visited, with a domain of values corresponding to the index of the next destination (the
so-called single-successor model). A subtour elimination constraint (Caseau & Laburthe,
1997) might then be used to ensure that only legal tours are produced, and Regins alldifferent constraint (Regin, 1994) could be added to increase domain filtering. In a pick
up and delivery variant, side constraints could be added to ensure that some locations are
visited before others. For a job shop scheduling problem we might have a model that uses
0/1 variables to decide the relative order of pairs of activities that share a resource, and we
might increase propagation by adding Carlier and Pinsons edge finding constraint (1994).
The constraint programming approach is general and practical for modelling and solving
problems, and provides a framework for the combination of problem specific algorithms in
one solver. This allows us to solve many classes of problems efficiently and to model even
more problems via the addition of side constraints.
2.2 The Supertree Problem
Supertree construction is a problem in phylogenetics where we are to combine leaf-labelled
species trees, where the sets of leaf labels intersect, into a single tree that respects all
arboreal relationships in each input tree (Bininda-Emonds, 2004). Species trees describe
part of the evolutionary history of a set of species. Labels on leaves correspond to existing
species and internal nodes represent divergence events in evolutionary history where one
species split into at least two other species. Species trees may also be annotated with dates
on internal nodes, representing the time at which the divergence event happened.
We now define the term displays, which makes precise what we mean by respects
arboreal relationships: supertree T1 displays a tree T2 if and only if T2 is equivalent to T4
(i.e. they induce the same hierarchy on the leaf labels) where T4 is obtained by the following
steps (Semple & Steel, 2000):
1. Let L be the set of leaves of T1 that are in T2 .
2. Let T3 be the unique subtree of T1 that connects all leaves in L.
3. To obtain T4 : wherever there is a subpath (p1 , . . . , pk ) of a path from the root to a
leaf in T3 where p2 , . . . , pk1 are all interior nodes of degree 2, contract this into a
single edge.
The problem is then to produce a rooted species tree T from a forest of input trees F, so
that T contains all the species in F and displays every tree in F . Figures 3 and 4 illustrate
the displays property.
905

fiMoore & Prosser

1
0

T1

1
0

1
0

T3

1
0

b

c

d

1
0

a

d

a

d

e

1 0
0
1

a

2

0
1
1
0
1 0
0
1 0
1

1
0

e

0
1
1
0
1 0
0
1 0
1

T

T = T
4
2

1
0

1
0

1 0
0
1 0
1 0
1 0
1

a

1
0

L={a,d,e}

d

e

e

Figure 3: An example of a tree T1 that displays a tree T2

1
0

T1

1
0

L={a,d,e}

1
0

1
0

1 0
0
1 0
1 0
1 0
1

a

d

c

b

e

T4 = T 2

1
0

1
0
0
1
1 0
0
1 0
1

a

1 0
0
1

a
T2

1
0

T3

1
0

d

d

e

1
0

e

0
1
1
0
1 0
0
1 0
1

a

d

e

Figure 4: An example of a tree T1 that does not display tree T2

906

fiThe Ultrametric Constraint and Phylogenetics

We say that two trees T1 and T2 are compatible (incompatible) if there exists (doesnt
exist) a third tree T3 that displays T1 and T2 . Variants on the supertree problem that
have previously been published and solved in the specialist bioinformatics literature include
finding all solutions1 , counting solutions, finding conserved relationships in all supertrees
(Daniel, 2003), incorporating nested taxa (Semple et al., 2004), incorporating ancestral divergence dates (Semple et al., 2004) and the possibility of contradictory input data (Semple
& Steel, 2000).

3. The Ultrametric Constraint
The ultrametric constraint was first proposed by Gent et al. (2003) within the context of
supertree construction (Bininda-Emonds, 2004), and was implemented using toolkit primitives. We review this encoding and show that in most constraint toolkits this is inefficient
in terms of both space and time. This motivates the creation of a specialised ultrametric
propagator over three variables, that maintains the ultrametric property over the bounds
of those variables. It is presented by describing the necessary propagation methods. We
then extend it to a specialised propagator that maintains the ultrametric property on a
symmetric matrix of variables.
3.1 Previous Work on the Ultrametric Constraint
First, we give a definition of the ultrametric constraint.
Definition 3. An ultrametric constraint on three variables (henceforth, Um-3) x, y and z
constrains them such that:
(x > y = z)  (y > x = z)  (z > x = y)  (x = y = z)

(1)

This constraint ensures that there is a tie for the least element of the three, i.e., either
all three are the same, or two are the same and the other is greater. The constraint was
proposed by Gent et al. (2003), used again by Prosser (2006) and both times implemented
as a literal translation of Equation 1 using toolkit primitives. Evidence obtained from the
JChoco, ECLiPSe and ILog constraint programming toolkits shows that no propagation
is done to lower bounds by this combination of primitive constraints. This is due to the disjunctive constraints since in many constraint programming toolkits propagation is delayed
until only one of the disjuncts can be true, this is known as delayed-disjunction consistency
(van Hentenryck, Saraswat, & Deville, 1998). Consequently, in the above encoding values
that cannot occur in any satisfying assignment might not be pruned from the domain of a
variable. Consider the case for three variables: x  {1, 2, 3}, y  {2, 3} and z  {3}. The
domains of the variables are already at a fixed point with respect to delayed-disjunction
consistency but there is no ultrametric assignment where x takes the value 1, i.e., delayeddisjunction propagation does not achieve arc-consistency. As we shall see later, finding
a solution to the supertree problem using toolkit constraints can result in a backtracking
search, and we prefer to avoid this. Of course, higher levels of consistency would overcome
this, such as constructive-disjunction consistency (van Hentenryck et al., 1998), singleton
1. There may be multiple supertrees for the same set of input trees.

907

fiMoore & Prosser

arc-consistency (Debruyne & Bessiere, 1997) or the filtering algorithm of Lhomme (2003).
However, the cost of these is greater in the average case than delayed-disjunction, preventing their use in toolkits. In fact for the Um-3 constraint it is especially unfortunate that
the lower bounds may not be trimmed properly:
Lemma 1. In the Um-3 constraint, when lower bounds are supported (i.e., form an ultrametric instantiation with values in the other constrained variables), they support each other.
Proof. Consider three supported lower bounds. Suppose for a contradiction that the two
least of these are distinct. One of these is distinct lowest and it cannot be supported on
account of the fact that it is not equal to anything or larger than anything. Therefore by
contradiction the two least must be equal. However the other lower bound is at least as
large as these, so the lower bounds are mutually supportive.
This Lemma will have important implications for the species tree model to be presented
in detail in Section 4: in particular, the lower bounds of a bounds(Z)-consistency model
form a solution, where bounds(Z)-consistency (Bessiere, 2006) is defined as follows
Definition 4. Given (V, D, C) and constraint c  C with scope(c) = (vc1 , . . . , vck ), a tuple
 = (xc1 , . . . , xck ) is a bound support when   rel(c) and for all xci   , min(dom(vci )) 
xci  max(dom(vci )). A constraint c is bounds(Z)-consistent if for all vci  scope(c) there
exist bound supports involving both min(dom(vci )) and max(dom(vci )). A CSP is bounds(Z)consistent when every constraint c  C is bounds(Z)-consistent.
We will henceforth abbreviate bound support to support and bounds(Z)-consistency to
BC(Z). BC(Z) differs from AC because it puts weaker conditions on the values that comprise
the support: rather than them having to be in the domain, they need to only be between
the lower and upper bounds of the domain. This means that BC(Z) prunes a subset of
the values that AC can, in general. Weaker levels of consistency such as BC(Z) are useful
because in certain problems they can prune the same number of values as AC more easily,
or fewer values much more quickly. In our case, BC(Z) is interesting because this level of
consistency is just enough to ensure that the problem can be solved by propagation with
no search, as we shall see.
3.2 Design of a BC(Z) UM-3 Propagator
In this Section we describe a Um-3 propagator that enforces BC(Z), namely UM-3-BCZ.
3.2.1 Analysis of Lower and Upper Bounds
In this section we dont take account of domains becoming empty. Since we analyse lower
and upper bounds in isolation a lower bound may pass an upper bound or vice-versa,
thereby emptying a domain. If this happens then the propagator described in Section 3.2.2
will not enforce BC(Z), rather it will terminate. This is not a problem, because a domain
becoming empty means that there is no solution and to continue would be a waste of time.
Concordantly, in this section, when we analyse lower bounds we will assume the upper
bound is  so that the domain cannot become null for this reason, and vice-versa.
908

fiThe Ultrametric Constraint and Phylogenetics

Code style: Variables v1 , v2 , v3 , S, M and L below are constrained integer variables, and are synonymous with their domains. Consequently a variable x can be considered as a domain where x.lb
and x.ub return references to lower and upper bounds respectively; SortOnLowerBounds(x,y,z)
returns a tuple of references to variables x, y and z in non-decreasing order of their lower bounds;
SortOnUpperBounds is analogous; let (S,M ,L)  . . . names the references S, M and L; as a
result of S.lb  M .lb, the lower bound of S is assigned equal to the value of the lower bound of M ,
although if M .lb subsequently changes they will be distinct again; the expression x b y returns the
intersection of their domains [max(x.lb,y.lb) . . . min(x.up,y.up)].

Algorithm UM-3-BCZ
LBFix(v1 ,v2 ,v3 )
A1
let (S, M, L)  SortOnLowerBounds(v1 , v2 , v3 )
A2
if (S.lb < M .lb) then
A2.1
S.lb  M .lb
UBFix(v1 ,v2 ,v3 )
A3
let (S, M, L)  SortOnUpperBounds(v1 , v2 , v3 )
A4
if (S.ub < M .ub) then
A4.1
if (S b L = ) then
A4.2
M .ub  S.ub
A4.3
else if (S b M = ) then
A4.4
L.ub  S.ub
Min event(v1 ,v2 ,v3 )
A5
LBFix(v1 ,v2 ,v3 )
A6
if all domains are non-empty then
A6.1
UBFix(v1 ,v2 ,v3 )
Max event(v1 ,v2 ,v3 )
A7
UBFix(v1 ,v2 ,v3 )
Fix event(v1 ,v2 ,v3 )
A8
LBFix(v1 ,v2 ,v3 )
A9
if all domains are non-empty then
A9.1
UBFix(v1 ,v2 ,v3 )

Figure 5: Algorithm for UM-3-BCZ propagator

909

fiMoore & Prosser

(1)

(2)
l
m
s

(3)

(4)
l
s=m

s=m=l

m=l
s

Figure 6: Cases in the analysis of LBFix

L
(1)

l
M m
S s

L M
(3)

(2)

S

(4)

Figure 7: Cases in the analysis of UBFix
The procedure LBFix in Figure 5 takes as input three variables and removes any unsupported values at the lower bounds of the domain. The intuition for the algorithm achieving
this is that each one needs to be involved in a tie for least element, hence if the smallest
lower bound is strictly less than the others then it must be unsupported.
The possible states of lower bounds when LBFix is invoked are summarised in Figure 6.
Either all three are different (case 1), all three are the same (case 2) or two are the same and
one different (cases 3 and 4). These give relationships between bounds at a point in time
when some lower bound may be unsupported. The boxes in Figure 6 are shaded as follows:
regions shaded black are removed by propagation whereas gray regions are supported. What
the diagrams are not supposed to suggest is that, for example in case 1, the bounds differ
by 1. Rather when two bounds are lined up they are the same bound and when one is
different from another they are different by some non-zero but unspecified amount. Hence
they describe relationships and not actual values.
The following shows that LBFix removes all unsupported values and does not remove
any supported values.
Lemma 2. After LBFix is invoked all lower bounds of the argument variables are supported
w.r.t. the Um-3 constraint and no supported values are removed.
Proof. For cases 1 and 4 (Figure 6) the condition on line A2 is satisfied, so line A2.1
is executed, this results in the removal of the unsupported range and by inspection the
remaining bounds are mutually supportive. In cases 2 and 3, the condition on line A2 is
failed and so no changes are made to the domains; the bounds are mutually supportive.
The procedure UBFix in Figure 5 does the same job to upper bounds that LBFix does
to lower bounds. The following Lemma justifies this assertion and the cases used in the
proof are shown in Figure 7:
Lemma 3. After UBFix is invoked either
910

fiThe Ultrametric Constraint and Phylogenetics

 all upper bounds of the argument variables are supported w.r.t. the UM-3-BCZ constraint and no supported values are removed, or
 a domain is null as a result of removing unsupported values.

Proof. Let S, M and L be the domains with smallest, middle and largest upper bounds,
breaking ties arbitrarily. Let s, m and l be these upper bounds.
First we will prove that in case 1 (Figure 7) the shaded region in M is supported if and
only if L b S 6= : Potentially, the bound can be supported by
 equal values in S and L at least as small as it (i.e., S b L 6= ), or
 an equal value in either S or L, and a value at least as large in the remaining domain.
However, notice that the latter is impossible due to the fact that only L contains an equal
value, and S has no value as large as this.
Similar arguments can establish that the shaded regions in L in case 1, M in case 3 and
L in case 3 are supported if and only if M b S 6= , L b S 6=  and M b S 6= , respectively.
Now we will establish the Lemma for each of the cases 1, 2, 3 and 4:
Cases 2 and 4 The condition on line A4 is false, and so no domains are changed. The
upper bounds are mutually supportive in each case.
Case 1 and shaded region of M is unsupported From above L b S = . Hence
UBFix line A4.2 will be executed and the unsupported region removed. Now the upper
bounds l  L, s  M and s  S are mutually supportive.
Case 1 and shaded region of M is supported From above Lb S 6= . If the shaded
region of L is also supported then M b S 6=  and so neither line A4.2 nor A4.4 is executed
and no changes are made to the domains. The upper bound of S is also supported, by
m  L, m  M and s  S. If the shaded region of L is not supported then M b S = 
and so line A4.4 is executed resulting in the removal of the region. The new bounds s  S,
m  M and s  L are mutually supportive.
Case 3 and shaded regions of M and L supported From above, L b S 6=  and
M b S 6= . Hence no domain changes result from executing UBFix. l  L and s  S are
supported by l  L, s  M and s  S. m  M is supported by s  L, m  M and s  S.
Case 3 and shaded regions of M and L are unsupported From above, L b S = 
and M b S =  and so line A4.2 of UBFix is executed and this results in M becoming null.
Case 3, shaded region of M supported but shaded region of L not supported
From above Lb S 6=  and M b S = , so that A4.4 is executed to remove the unsupported
region. The new bounds of s  S, m  M and s  L are mutually supportive.
Case 3, shaded region of L supported but shaded region of M not supported
Symmetric with previous case.

Note that there is no analog of Lemma 1 for upper bounds since, for example, the bounds
of x = {1, 2, 3}, y = {1, 2} and z = {1} are all supported, but not mutually supportive.
911

fiMoore & Prosser

3.2.2 The Propagation Algorithm
Having presented LBFix and UBFix we are now in a position to present the complete propagation algorithm. The propagator works with arbitrary domains and it enforces BC(Z),
except when a domain becomes empty, in which case it does no further work. The algorithm
is described by the action taken when any of three domain events occur:
min The domain has lost its lower bound since propagator was last invoked.
max The domain has lost its upper bound since propagator was last invoked.
fix The domain is a singleton, i.e., the variable is instantiated and upper and lower bounds
are equal.
i.e. we only consider events on the bounds of the variables. The algorithm is listed in lines
A5-A9 of Figure 5. Intuitively these procedures work because, as we will show, a change
to an upper bound can affect the support for other upper bounds, but a change in a lower
bound can affect support for both lower and other upper bounds. Hence we need only run
LBFix when a lower bound may have changed, but UBFix must be run for a change of either
lower or upper bounds. Whilst it would be correct to cycle between trimming upper and
lower bounds until a fixed point is reached (i.e. no more changes occur), we can guarantee
a fixed point more easily.
Lemma 4. It is possible for a change in a lower bound to result in the loss of support for
another lower bound.
Proof. All the bounds in the diagram have support, but when the black shaded lower
bound is lost the dark gray shaded lower bound loses support.

Lemma 5. It is possible for a change in a lower bound to result in the loss of support for
an upper bound.
Proof. All the bounds in the diagram have support, but when the black shaded lower
bound is lost the dark gray shaded upper bound loses support.

Lemma 6. It is possible for the loss of an upper bound to cause the loss of support for
another upper bound.
912

fiThe Ultrametric Constraint and Phylogenetics

Proof. All the bounds in the diagram have support, but when the black shaded upper
bound is lost the dark gray shaded upper bound loses support.

Corollary 1. It is impossible for a change in an upper bound to result in the loss of support
for a lower bound.
Proof. By Lemma 1 a lower bound retains support as long as the other lower bounds are
intact, hence losing an upper bound has no effect.
Why the asymmetry between upper and lower bounds? It is due to the asymmetry in
the definition of UM-3-BCZ and has the practical repercussion that BC(Z) lower bounds
must be mutually supportive whereas BC(Z) upper bounds may not be and may require
support from other values including lower bounds.
Corollary 1 suggests that a further improvement on the algorithm in Figure 5 is to
execute Line A6 and A9 if and only if any lower bound lost is the only remaining support
for an upper bound. However, the conditionals intrinsic in UBFix amount to much the
same thing and there is little point in repeating them.
The point of these theorems has been to build up a complete proof of correctness and
BC(Z) status:
Theorem 1. The code for min, max and fix events listed in Figure 5 does not remove any
values involved in bound supports for the UM-3-BCZ constraint, and, if all the domains are
non-null after propagation, the resultant domains will be BC(Z).
Proof. First we must establish that no values are removed during propagation that could be
involved in a support, and that the result domains are subsets of the input domains. The
former is immediate from Lemmas 2 and 3, because values are only removed as a result of
executing LBFix and UBFix. The latter is immediate from inspection of LBFix and UBFix,
because they only ever make lower bounds larger and upper bounds smaller.
The final thing to establish is that BC(Z) is enforced, unless a domain becomes null.
If any domain becomes empty as a result of running the algorithm then the Theorem is
trivially true.
If no domain becomes empty then we must show that all the bounds are supported. For
lower bounds, by Lemma 4 and Corollary 1 we know that only the loss of a lower bound
can result in the need to change a lower bound during propagation. Lower bounds can
change as a result of either fix or min events, hence the propagator in Figure 5 runs LBFix
in either event. When LBFix runs it leaves all lower bounds supported, as was shown in
Lemma 2. For upper bounds, by Lemmas 5 and 6 we know that the loss of either a lower
or upper bound can result in the loss of an upper bound. Hence upper or lower bounds can
change as a result of any event, and lower bounds can also change as a result of LBFix,
hence the propagator runs UBFix in all events, and it runs after LBFix has finished, if
necessary. When UBFix runs it leaves all upper bounds supported provided no domain
becomes empty, as was shown in Lemma 3.
913

fiMoore & Prosser

1
0
0
1
0
1
0
1
0
1
0
1

(a)

1
0
0
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1

(b)

Figure 8: Propagation done when 2 domains are singleton
The propagation algorithm runs in (1) time. This is because all of the operations in
LBFix, UBFix, min, max and fix events are (1), provided that our domain representation
allows access to the upper and lower bounds in (1). This can be guaranteed if domain
reductions only occur at the bounds, as is the case here, or if domains are represented using
one of the structures proposed by van Hentenryck et al. (1992).
3.3 Entailment
Schulte and Carlsson (2006) define entailment as when all possible constractions of the
domains in a constraints scope are consistent. If we can detect that this has happened we
can stop running the propagator henceforth, since it cannot prune any more values.
Definition 5. A propagator is entailed by domains D = {d1 , . . . , dn } when any set of
domains that are subsets of these, i.e., any E = {e1 , . . . , en } s.t. i.ei  di , are at a fixed
point.
We now describe a sufficient condition for the UM-3-BCZ constraint to be entailed, i.e., the
UM-3-BCZ constraint becomes entailed as soon as two variables have singleton domains:
Theorem 2. UM-3-BCZ becomes entailed as soon as two variables have singleton domains.
Proof. Consider the possible scenarios: either the two singletons are the same (case (a) in
Figure 8) or they are distinct (case (b) in Figure 8). The domains before propagation are
shown in Figure 8 as boxes; the domains after propagation are shaded gray. Clearly all
remaining choices for the third variable are valid instantiations and since the propagation
algorithm is safe they cannot be removed by further propagation and by definition the
propagation will be at a fixed point.
3.4 Ultrametric Matrix Constraint
The supertree model presented in Section 4 makes use of the ultrametric constraint, however
in this context the desired end product is to constrain a whole matrix to be an ultrametric
matrix, and not merely to constrain three variables.
914

fiThe Ultrametric Constraint and Phylogenetics

Code style: let (i, j)  index(v) declares i and j to be the indices of variable v in the matrix M that
the constraint is over.

Algorithm UM-Matrix-BCZ
Min event(v)
A1
let (i, j)  index(v)
A2
for k  1 . . . n do
A2.1
if k 6= i and k 6= j then
A2.2
Min event(Mij , Mik , Mjk )
A2.3
Max event(Mij , Mik , Mjk )
Max event(v)
A3
let (i, j)  index(v)
A4
for k  1 . . . n do
A4.1
if k 6= i and k 6= j then
A4.2
Max event(Mij , Mik , Mjk )
Fix event(v)
A5
let (i, j)  index(v)
A6
for k  1 . . . n do
A6.1
if k 6= i and k 6= j then
A6.2
Min event(Mij , Mik , Mjk )
A6.3
Max event(Mij , Mik , Mjk )

Figure 9: Algorithm for UM-Matrix-BCZ propagator

Definition 6. A symmetric matrix M is an ultrametric matrix if and only if for every set
of three distinct indices i, j and k, there is a tie for the minimum of Mij , Mik and Mjk ;
and Mii = 0 for all i.
The ultrametric matrix constraint can be achieved for matrix M by posting the constraint UM-3-BCZ(M
ij , Mik , Mjk ) over all choices of distinct i, j and k, but at a cost of

introducing n3 constraints. In practical constraint solvers any model containing this constraint will have (n3 ) space complexity, since the solver must have a list of all (n3 )
constraints stored somewhere. However, when a domain event occurs for any matrix variable Mij it is straightforward to iterate over all k indices doing the same propagation as
UM-Matrix-BCZ from Figure 5. This replaces a (n3 ) space list representation of the set
of UM-3-BCZ constraints by a (1) code representation. Hence we propose the ultrametric
matrix constraint propagator UM-Matrix-BCZ in Figure 9.
This propagator mimics part of the AC3 algorithm (Mackworth, 1977) since it (a)
receives a propagation event on a variable, (b) identifies which constraints are over that
variable, and (c) arranges for the propagation to be carried out. Any events caused as a
result are queued and dispatched by the underlying propagator as normal and this may
cause UM-Matrix-BCZ to be run again. Each variable can be involved in up to n  2
constraints, since each variable has two indices in the matrix and we have a constraint
involving each choice of three different indices.
n
3

The algorithm propagates in (n) time, which is more expensive per event than using
Um-3 constraints, but a factor of n fewer propagators wake up as a result of each event.
915

fiMoore & Prosser

a

b

c

a

c

b

b

c

a

a

b

c

Figure 10: The four possible relationships between three leaf nodes in a tree: i.e. the three
triples (ab)c, (ac)b, and (bc)a, and the fan (abc).

4. Supertree Construction
We now review imperative solutions to the supertree construction problem, review the first
constraint programing solution (Gent et al., 2003), and present a new encoding that exploits
the specialised UM-Matrix-BCZ constraint.
4.1 Imperative Solutions to the Supertree Problem
The earliest imperative techniques are due to Bryant and Steel (1995) and Ng and Wormald
(1996). Both present a OneTree algorithm which is based on the Build algorithm of
Aho, Sagiv, Szymanski, and Ullman (1981). OneTree is based on the observation that
in a tree any three leaf nodes define a unique relation with respect to their most recent
common ancestor (mrca), such that mrca(a, b) is the interior node furthest from the root
that has both leaf nodes a and b as descendants. We abuse notation by writing mrca(a, b) >
mrca(c, d) when the former has a greater depth than the latter, and similarly mrca(a, b) =
mrca(c, d) if they have the same depth. Given three different leaf nodes/species (labelled
a, b, and c) one of the following four relations must hold:
(1)

mrca(a, b) > mrca(a, c) = mrca(b, c)

(2)

mrca(a, c) > mrca(a, b) = mrca(c, b)

(3)

mrca(b, c) > mrca(b, a) = mrca(c, a)

(4)

mrca(a, b) = mrca(a, c) = mrca(b, c)

We now say that in (1), (2) and (3) we have the triples (ab)c, (ac)b, and (bc)a (where
(xy)z can be read as x is closer to y than z) and in (4) we have the fan (abc), i.e., in
a fan the relationship between species is unresolved as we dont specify which pair is most
closely related. This is shown in Figure 10. Prior to applying the OneTree algorithm two
(or more) species trees are broken up into triples and fans using the BreakUp algorithm
(Ng & Wormald, 1996), resulting in a linear sized encoding of those trees. The supertree is
then constructed (if possible) using this encoding as input.
Figure 11 shows an example of the BreakUp algorithm process. Two variants of the
process are shown; at the top we have a hard breakup, where fans are considered as hard
evidence that must be respected (hard polytomies as described by Ng and Wormald, 1996)
and below a soft breakup where fans are taken as a lack of evidence (soft polytomies as
described by Bryant and Steel, 1995). For hard breakup the algorithm is modified such
916

fiThe Ultrametric Constraint and Phylogenetics

e

a

b

c

b

c

g

BREAK UP

BREAK UP

BREAK UP

(a b c)
(a b d)
(a c d)
(b c d)

(cd)e

(de)f

(ef)g

d

d

e

c

e

a

f

BREAK UP

f

f

g

d

e

f

g

e

f

g

d

BREAK UP

BREAK UP

BREAK UP

BREAK UP

(ab)e,
(bc)e

(cd)e

(de)f

(ef)g

e

g

c

{(abc),(abd),
}
(bcd),(cd)e,
(de)f,(ef)g}

f

g

d

e

f

g

e

f

{(ab)e,(bc)e,
}
(cd)e,(de)f,
(ef)g}

g

d

Figure 11: Example execution of the BreakUp algorithm. On the top, a hard breakup,
and below a soft breakup (no fans produced)

Code style: The function sortedInteriorNodes(T ) delivers the set of interior nodes of the tree T in nonincreasing order of depth in that tree; degree(v) delivers the out degree of node v; function child(v, i)
delivers the ith child of interior node v; uncleOrCousin(l) delivers a leaf node that is descended from
a sibling of the parent of leaf node l; function becomesLeaf(v, l) transforms interior node v into a leaf
node labelled as l; removeChild(l, v) removes the leaf node l from the list of children of interior node v.

Algorithm HardBreakup
HardBreakup(T )
1
let V  sortedInteriorNodes(T )
2
let S  
3
let i  0
4
while notRoot(V [i])  degree(V [i]) > 2 do
5
let v  V [i]
6
let c0  child(v, 0)
7
if degree(v) = 2
8
then let c1  child(v, 1)
9
let c2  uncleOrCousinOf(c0 )
10
S  S  {triple(c0 , c1 , c2 )}
11
v  becomesLeaf(v, c0 )
12
ii+1
13
else for j  1 to degree(v)  2 do
14
for k  j + 1 to degree(v)  1 do
15
let c1  child(v, j)
16
let c2  child(v, k)
17
S  S  {fan(c0 , c1 , c2 )}
18
v  removeChild(c0 , v)
19 return S

Figure 12: Hard breakup of a tree T , producing triples and fans.

that when encountering a kfan this is broken up into n3 3-fans, and in a soft breakup a
fan is broken into a linear number of rooted triples. Algorithms for hard and soft breakups
are given in Figures 12 and 13, and are used by the imperative OneTree algorithm here
and the constraint programming models.
917

fiMoore & Prosser

Algorithm SoftBreakup
SoftBreakup(T )
1
let V  sortedInteriorNodes(T )
2
let S  
3
let i  0
4
while notRoot(V [i]) do
5
let v  V [i]
6
let c0  child(v, 0)
7
let c1  child(v, 1)
8
let c2  uncleOrCousinOf(c0 )
9
S  S  {triple(c0 , c1 , c2 )}
10
if degree(v) = 2
11
then v  becomesLeaf(v, c0 )
12
ii+1
13
else v  removeChild(c0 , v)
14 return S

Figure 13: Soft breakup of a tree T , producing only triples.

c

a

b

c

e

c

b

a

c

d

d

e

b

e

Figure 14: A toy input (left) and single solution to the supertree problem (right). Input
trees are distorted to make relationships in resultant supertree more obvious.

A toy set of input triples and single solution are shown in Figure 14. The triples have
been drawn to reflect how the solution is compatible with them.
Ng and Wormald (1996) give the complexity of OneTree as O(h(n)) where h(n) =
n(n + t + bn)(n + t + f ), n is the number of labels, t the number of triples, f the number
of fans, b is the sum of the squares of the number of leaves in the fans, and  is the inverse
Ackermann function (and is less than 4 for all conceivable inputs and so behaves like a
constant). Therefore if the input trees are fully resolved (i.e., have no fans) then running
918

fiThe Ultrametric Constraint and Phylogenetics

time complexity is O(n2 ) but in the worst case complexity grows to O(n4 ). This should be
contrasted with the O(t  n) complexity of Bryant and Steels OneTree (1995).
4.2 A Constraint Encoding using Toolkit Constraints
This second stage, i.e., OneTree equivalent, was first solved as a constraint program by
Gent et al. (2003). The encoding takes advantage of an equivalence between ultrametric
trees and ultrametric matrices:
Definition 7. Let M be a real symmetric n  n matrix. An ultrametric tree for M is a
rooted tree T such that:
1. T has n leaves, each corresponding to a unique row of M;
2. each internal node of T has at least 2 children;
3. for any two leaves i and j, Mij is the label of the most recent common ancestor of i
and j; and
4. along any path from the root to a leaf, the labels strictly increase.
Theorem 3. A symmetric matrix M has an ultrametric tree T if and only if it is an
ultrametric matrix. Furthermore, the tree T uniquely determines the matrix M and the
matrix M uniquely determines the tree T .
Proof. A proof is given by Gusfield (1997).
There is a clear correspondence between Definition 7 and the description of a species
tree given in Section 4: a species tree T is an ultrametric tree for matrix M , where Mij is
the depth of the mrca of species i and j or Mij is the divergence date of those two species.
For this reason we can use an ultrametric matrix model to solve the supertree problem.
4.2.1 The Model of Gent et al.
Given as input a forest F with n distinct leaf labels, a symmetric
constrained integer variables is created with domains {1, . . . , n  1}
diagonal. Variable Mij is the depth of the mrca of species i and j.
are posted to make the whole matrix ultrametric thus ensuring that
ultrametric:
Mij > Mik = Mjk
 Mik > Mij = Mjk
 Mjk > Mij = Mik
 Mij = Mik = Mjk

n  n matrix M of
or {0} on the main
Initially, constraints
any resulting tree is

(2)

for each i < j < k. The input trees are then broken up into triples and fans using either of
the breakup algorithms of Figures 12 and 13. For each triple (ij)k produced the constraint
Mij > Mik = Mjk
919

(3)

fiMoore & Prosser

1
a
b
c
d
e
f
g

3
{e,g}
5
{c,d}

a

a b c d e f g
0 5 3 3 1 5 1
5 0
3
0
3
0
1
0
5
0
0
1

{b,f}

Figure 15: One iteration of an algorithm to convert an ultrametric matrix to a tree
is posted and for each 3-fan (ijk)
Mij = Mik = Mjk

(4)

is posted. These constraints break the disjunctions of Equation 2. The model has (n2 n)/2
variables and
t+f +

 
n
= O(n3 ) + O(n3 ) + (n3 ) = (n3 )
3

(5)

constraints, where t is the number of triples and f the number of fans. There are O(n3 ) of
each because each one breaks the disjunction in at most one constraint from Equation 2,
and there are (n3 ) of those.
4.2.2 Converting Back to Tree Representation
The final step is to use an algorithm based on the constructive proof by Gusfield (1997) of
the  direction of Theorem 3 to build a tree from the matrix M produced by a constraint
solver. We will not describe this algorithm in detail, but for the sake of intuition it works
as follows
 Pick an arbitrary leaf s. Let the number of distinct entries in row s be d.
 Partition the other leaves into sets p1 , . . . , pd based on their entry in row s.
 Solve the problem recursively on each pi by ignoring all the rows and columns in the
matrix D not in pi .
 Combine into overall solution by attaching subproblem solutions at the correct depth
on the path to s.
Figure 15 shows one recursion of the algorithm with a choice of leaf a and shows that
row a fully describes the path to a in the corresponding tree.
920

fiThe Ultrametric Constraint and Phylogenetics

Algorithm CPBuild
CPBuild(F )
1 let (V, D, C)  CPModel(F )
2 for T  F do
3
for t  BreakUp(T ) do C  post(t, C)
4 if propagate(V,D,C) then return UMToTree(V,D)
5 else fail()

Figure 16: Build a supertree from forest F using the ultrametric constraint model.
4.2.3 Time Complexity of the Model of Gent et al.
BreakUp, and the procedures to build a constraint model and to convert an ultrametric
matrix to a tree are all polynomial time. However the complexity of backtracking search
2
over O(n2 ) variables with O(n)-size domains is worst case O(nn ). This is an upper bound
on the time taken to solve the supertree problem. We have not attempted to derive a lesser
upper bound on the time complexity, since, as we will show in the following section, our
new model has provably achieved a polynomial time bound.
4.3 A Constraint Encoding Using the New Propagator Design
This issue of potentially exponential solution time model of Section 4.2 is worrying, but in
our experiments the time taken to solve instances has not been a major issue. Conversely
the memory requirements are a problem in practice, but not in theory! The model requires
(n3 ) space for n species, but the constant factor is inhibiting. Posting the constraint of
Equation 1 literally (using toolkit propagators) as described in Section 3 uses 23 propagators in the JChoco toolkit. This requires roughly 23 times the runtime memory of a
single propagator, since each corresponds to a single Java object, and each of these have
comparable footprints. As we will show in the empirical study of Section 5, this prevents
modest instances from being loaded on typical current workstations.

Using the new propagator of Section 3 we replace these n3 propagators with a single
compact propagator and as a result memory usage is reduced asymptotically from (n3 )
to (n2 ) since now the model memory is dominated by the (n2 ) space needed for the
matrix M . Also reducing the amount of space to be initialised delivers a proportional
saving in build time. But most importantly, using the new constraint provides a solution
to exponential time complexity, because enforcing BC(Z) on the model allows a solution to
be read out of the lower bound of each domain. Theorem 4 is a proof of correctness for this
algorithm.
Figure 16 gives a schema for the constraint programming algorithm for supertree construction, CPBuild. The algorithm takes as input a forest F of trees. In line 1 a constraint
model is produced, i.e., an n  n symmetric array of constrained integers variables is created, where there are n unique species in the forest, and the UM-Matrix-BCZ constraint
is then posted over those variables. Lines 2 and 3 breaks these input trees into their triples
and fans using either of the breakup algorithms given in Figures 12 and 13, and posts them
into the model as constraints. Propagators for the constraints are executed to a fixed point
in line 4; if this succeeds a tree is created from the lower bounds of the ultrametric matrix
otherwise we fail.
921

fiMoore & Prosser

Lemma 7. If the propagator for every constraint in a model enforces BC(Z) and furthermore the lower bounds are mutually supportive, then after executing all propagators to a
fixed point, either the lower bounds are a solution, or we have an empty domain and fail.
Proof. If we reduce each domain to just the lower bound after a fixed point is obtained
then each bound is supported because they are mutually supportive by supposition. Hence
every constraint is simultaneously satisfied by these singleton domains and, by definition,
we have a solution.
Theorem 4. CPBuild is a polynomial time solution to the supertree problem.
Proof. The only constraints involved in the model are those for triples and fans and ultrametric constraints. By Theorem 1 we know that all lower bounds are supported after the
propagators run, and by Lemma 1 we know that the lower bounds are mutually supportive. The same is true of the disjunction-breaking propagators. Hence by Lemma 7, and as
shown in Figure 16, we can either read out a solution or fail. We can enforce BC(Z) on the
problem in polynomial time as shown below.
Immediate from this is that we can preserve the polynomial time solution with the
addition of a polynomial number of side-constraints, so long as these additional constraints
preserve the property that lower bounds are mutually supportive. In fact, CSPs such as
these with ordered domains where all constraints have the property that lower bounds are
mutually supportive belong to a known tractable class called min-closed (Jeavons & Cooper,
1995).
4.3.1 Time Complexity of CPBuild
The algorithm can be implemented to run in O(n4 ) time using a variation on the AC3
algorithm. AC3 (Mackworth, 1977) begins with a queue containing all constraints. It
repeatedly removes a constraint until none remain and runs the associated propagator.
Any constraints over affected variables are re-queued, if necessary. Once the queue empties,
all propagators are at a fixed point. We need O(n3 ) constraints so the worst case complexity
is
O(n3 )
| {z }

+

build initial Q

O(n)O(n3 )
| {z }

worst case re-queues with 1 value removed at a time

.

O(1)
| {z }

propagation time

or O(n4 ) overall. This matches the worst case complexity of OneTree (Ng & Wormald,
1996). Our constraint solution has its worst case when the problem is unsolvable, since
when it is unsolvable domains are emptied by propagation, whereas for solvable instances
propagation reaches a fixed point sooner.

5. Empirical Study
We present an empirical study to determine if any practical improvements have been
achieved in constraint solutions to the supertree problem and, if so, what size of improvement. Experiments were run using a 1.7GHz Pentium 4 processor with 768MB of memory,
using Sun Java build 1.5.0 06-b05. The constraint toolkit used was JChoco version 1.1.04.
922

fiThe Ultrametric Constraint and Phylogenetics

Input trees were broken up using a hard breakup, consequently in all cases fans were treated
as hard polytomies2 .
Our benchmark is real-life seabird data previously used by Kennedy and Page (2002)
and Beldiceanu et al. (2008) and we present statistics on various techniques for producing supertrees, namely OneTree, and the CP solutions of Section 4 (entries Toolkit and
CPBuild). For completeness we reproduce the results of Beldiceanu et al. (2008) over the
same data set, and tabulate this as TreeCon. TreeCon uses a single-successor model,
where constrained integer variables represent nodes within a tree, and domains correspond
to possible successors3 . A unique variable represents the root and loops on itself (i.e.,
vroot = root), and leaf nodes have an indegree of zero. Precedence and incomparability
constraints are then generated from the input trees.
The TreeCon results were encoded in the same constraint programming toolkit as ours
but on a processor that was approximately twice as fast (3GHz). We do not correct the times
to compensate for this factor. We mark in bold results that differ very significantly between
the CPBuild and TreeCon results, specifically those whose runtimes would undoubtedly
be a factor of 10 different on the same processor. Results are reported for combinations of
seabird trees (input trees named A to G) and the following data is tabulated below:
Data The combination attempted.
n Total distinct species in input trees
Sol T iff supertree is possible
Technique Type of algorithm used to solve
Build Time in milliseconds to initialise CP model
Solve Time in milliseconds to first solution, if any
Total = Build + Solve
Nodes Number of nodes in search tree
Mem Model memory in MB
In the table, DNL means that the model could not be loaded (as it was too large) and DNF
means that it could not be solved within 30 mins, but succeeded in loading. We have not
provided the memory usage of OneTree; however it is smaller than that of any of the
constraint encodings.
The most obvious thing to note is how much faster the imperative approach is compared
to the constraint techniques. Why is this? Primarily it is due to the lower complexity of
OneTree in the absence of fans (we have not investigated if we can benefit from this),
and partly due to the generality of the constraint programming approach. The imperative
approach is highly specialised to only one class of problem whereas the constraint approach
sits within a toolkit, and runs on top of a general purpose constraint maintenance system.
We should not expect that the constraint approach will compete in raw speed but what we
later demonstrate (in Section 6) is that the approach benefits from its versatility, i.e., the
2. In a later section we use soft breakup.
3. An alternative constraint model of a tree might use 0/1 variables corresponding to potential edges
within an adjacency matrix (Prosser & Unsworth, 2006), or indeed the CP(Graph) computation domain
(Dooms, 2006).

923

fiMoore & Prosser

costs of space and time is repaid by the ease of accommodating variants of the problem into
the same model.
Data
AB

n
23

Sol
T

AC

32

F

AD

47

T

AE

95

F

AF

31

T

AG

46

T

BC

29

F

BD

42

T

BE

94

F

BF

30

T

BG

40

T

CD

45

T

CE

68

T

CF

34

T

CG

44

F

Technique
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon

Build
2056
183

Solve
374
131

2670
189

327
153

8235
220

946
248

DNL
340

DNL
1477

2497
188

379
137

7671
222

871
252

2056
171

21931
107

5833
201

930
251

DNL
335

DNL
16340

2405
174

343
99

5098
203

651
353

10056
224

1134
276

DNL
516

DNL
1451

3101
180

563
133

6683
210

587
215

924

Total
2430
314
302
13
2997
342
406
12
9181
468
398
22
DNL
1817
10393
37
2876
325
127
20
8542
474
409
21
23987
278
32
8
6763
452
301
17
DNL
16675
892
11
2748
273
144
8
5749
556
1440
13
11190
500
630
14
DNL
1967
27180
36
3662
313
393
11
7270
425
1530

Nodes
23
23

Mem
26.92
0.24

0
0

36.34
0.34

38
38

118.51
0.70

DNL
0

> 629
2.79

19
18

32.99
0.32

31
31

111.07
0.68

171
0

26.90
0.27

33
33

84.26
0.55

DNL
0

> 629
2.71

29
29

29.83
0.28

30
30

72.71
0.51

45
45

143.91
0.77

DNL
68

> 629
2.72

30
30

43.72
0.36

0
0

97.10
0.61

fiThe Ultrametric Constraint and Phylogenetics

DE

104

F

DF

44

T

DG

56

F

EF

94

F

EG

97

F

FG

38

F

ABDF

72

T

ABDG

78

F

ACDF

72

F

ACDG

81

F

ACE

97

F

OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
TreeCon
OneTree
Toolkit
CPBuild
OneTree

DNL
360

DNL
2021

6613
203

987
250

14090
252

2280
640

DNL
331

DNL
9546

DNL
344

DNL
8900

4299
195

DNL
212

27032
277

5291
722

60847
301

DNF
3633

31067
286

1931
649

DNL
307

DNL
1711

DNL
737

DNL
1632

14
DNL
2381
1126
34
7600
453
630
17
16370
892
910
19
DNL
9877
1035
12
DNL
9244
1211
15
DNL
407
62
10
32323
999
8139
34
DNF
3934
347
29
32998
935
8690
28
DNL
2018
12650
35
DNL
2369
38

DNL
0

> 629
3.31

37
37

97.10
0.60

1
0

201.42
0.99

DNL
0

> 629
2.71

DNL
0

> 629
2.89

DNL
0

61.41
0.46

63
59

382.52
1.48

DNF
0

553.49
1.91

0
0

434.84
1.61

DNL
0

> 629
2.06

DNL
0

> 629
2.91

The most impressive aspect of the matrix model of Section 4.3 over that of Section 4.2
is the improvement in memory requirements, so that all instances can now be loaded comfortably. This also has a dramatic impact on the build time. These improvements dominate
the reduction in solve time in practice. The toolkit model is outperformed by CPBuild by
an order of magnitude on each instance; moreover, there are two cases of search occurring
in the toolkit model (on data sets BC and ABDG) whereas CPBuild never has to search.
The polynomial time complexity is due to the provable absence of search.
Our results also compare well against those of Beldiceanu et al. (2008). There is one
case, BE, where CPBuild is an order of magnitude slower than TreeCon; so far we do not
have an explanation for this. There are four cases when TreeCon is significantly worse
than CPBuild. No results are available for TreeCon over the data set ACE. It should be
noted that Beldiceanu et al. do not yet have a complete filtering algorithm for this problem
based on their constraint model and, from personal communication, although the TreeCon
925

fiMoore & Prosser

Figure 17: Supertree with largest compatible data sets of birds ABDF. This took 737ms to
model and 270ms to solve using cpbuild.

model never backtracked over the birds data set there is as yet no proof that the complexity
of their model is polynomial. It should also be noted that we see CPBuild taking more time
on unsolvable instances than solvable instances, as predicted.
Figure 17 shows the supertree, displayed with treeView (Page, 1996), produced from
the largest compatible forest {A, B, D, F }. This supertree has 72 leaves and takes about
1 second to produce. Although the result is not printed in the table, finding the forest
{A, B, C, D, E, F, G} incompatible takes about 12 seconds in total (1.5 seconds to build the
model and 10 seconds to determine incompatibility).
926

fiThe Ultrametric Constraint and Phylogenetics

6. Versatility of the Constraint Model
One of the strengths of constraint programming is its versatility: given a constraint model
of a core problem this model can then be enhanced to address variants of the original pure
problem. We demonstrate this versatility with respect to the ultrametric model, presenting
four variants of the supertree problem (a) incorporating ancestral divergence dates into
the model, (b) nested taxa, (c) determining if an induced triple or fan is common to all
supertrees, and (d) coping with incompatibilities.
6.1 Ancestral Divergence Dates
Semple et al. (2004) and Bryant et al. (2004) add temporal information to the input trees.
Interior nodes may be labelled with integer ranks such that if interior node v2 is a proper
descendant of v1 then rank(v1 ) < rank(v2 ), resulting in a ranked phylogenetic tree. Additionally relative divergence dates may be expressed in the form div(c,d) predates div(a,b)
and this is interpreted as the divergence of species c and d predates that of species a and b.
The RankedTree algorithm (Bryant et al., 2004) takes as input a collection of precedence
constraints derived from input ranked species trees and predates relations. The algorithm
outputs a ranked tree that respects those relations or returns not compatible.
This is trivial to incorporate into the constraint model. If trees have been ranked then
for each pair of species (i, j) in the leaf set we instantiate the constrained integer variable
Mij to the value of mrca(i, j). For a predates relation div(c,d) predates div(a,b) we post
the constraint Mcd < Mab . This is done before step 4 of CPBuild (Figure 16), i.e., ranks
and predates relations become side constraints. Similarly time bounds on speciation events
are posted as unary constraints, i.e. in a dated phylogenetic tree upper and lower divergence
bounds are given on interior nodes, such that l(a, b) and u(a, b) give respectively the lower
and upper bounds on the divergence dates of species a and b. In the constraint program
the following two side constraints are then posted (again, before step 4): l(a, b)  Mab and
Mab  u(a, b).
A demonstration of ranked trees is given in Figure 18. On the left we have two ranked
species trees of cats used recently by Semple et al. (2004) and originally by Janczewski,
Modi, Stephens, and OBrien (1995). The branch lengths of the source trees have been
translated into rankings and added to the interior vertices of those trees. On the right we
have one of the 17 possible resultant supertrees. In total, 7 of the 17 solutions contain
interior nodes with ranges. If interior nodes are labelled with specific values rather than
ranges then 30 solutions are produced, some of which are structurally identical. This goes
some way to addressing the issue of enumerating all supertrees compactly, raised as a
challenge by Semple et al. (2004). In Figure 19 we show the effect of adding a predates
constraint to a supertree construction. The data has previously been used by Bryant et al.
(2004) in their Figures 5 and 6.
6.2 Nested Taxa
A taxon (plural, taxa) is a group of organisms comprising a single common ancestor and its
descendents (Dawkins & Wong, 2004). For example the species lion and the class birds
are taxa. So far, all our species trees have been leaf-labelled, however this is restrictive
927

fiMoore & Prosser

Figure 18: Two ranked trees of cats. On the right one of the 17 possible supertrees produced
by CPBuild. Displayed using Pages treeView.

Figure 19: Two input trees T1 = ((a, c), x) and T2 = (b, x) with a resultant supertree shown
in the 3rd position. The tree on the far right is a supertree from T1 and T2
with the side constraint div(a,c) predates div(a,b), produced by CPBuild and
displayed using Pages treeView.

928

fiThe Ultrametric Constraint and Phylogenetics

P

P

Q

Q

P

Q

a

b

c

d

e

g

b

f

d

e

a

g

b

c

f

d

e

Figure 20: Two input rooted X-trees T1 and T2 (left) and an output tree T3 (right) that
perfectly displays them.

because trees may be annotated with taxa names on both leaves and internal nodes, giving
nested taxa. For example, Figure 20 shows tree T1 with an internal node labelled P which
has descendents a and b, i.e., the a and b taxa are nested within the P taxon. Problems
related to creating compatible supertrees for this type of data were raised by Page (2004)
and defined and solved by Daniel and Semple (2004). A set of input trees and possible
solution to the problem are shown in Figure 20: notice that all labels are conserved in the
solution, all ancestral relationships are conserved and, for any labels li and lj from the same
input tree, li is ancestor of lj in the input tree if and only if li is ancestor of lj in the solution
tree. This is an instance of the problem Higher Taxa Compatibility defined by Daniel and
Semple (2004) and Semple et al. (2004), where the result tree must perfectly display all of
the input trees. We now define the problem more formally.
Definition 8. A rooted X-tree (Daniel & Semple, 2004) is a species tree where internal
nodes as well as leaves may be labelled from the set X.
In the following we will be slightly loose and may use a label l to identify the labelled node,
as well as the label itself, e.g., descendants of l means the same as descendants of the
node labelled l.
Definition 9. A rooted X-tree T perfectly displays a rooted X  -tree T  when
1. X   X;
2. T  displays T , neglecting internal labels;
3. if a is a descendant of b in T  then a is a descendant of b in T ; and
4. if a is not a descendant of b in T  then a is not a descendant of b in T .
A rooted X-tree T perfectly displays a forest of phylogenetic trees F = {T1 , . . . , Tn } when it
perfectly displays every Ti .
6.2.1 Constraint Encoding
Our constraint encoding is implemented by the addition of variables and side constraints
to the standard model of Section 4. We describe how to transform the input to make the
constraint solution simpler, and then describe the variables and constraints needed.
929

fiMoore & Prosser

T1

T2

T2

P

Q

P

Q

Q
a

b

c

d

e

f

P

e

g

f

a

b

c

e

g

f

Figure 21: Two input trees T1 and T2 with an enclosing taxon P . By a process of substitution T2 is replaced with T2 .

Substitution Taxon P in Figure 21 appears on an internal node of T1 , we will call such
a label an enclosing taxon. Note also that it appears on a leaf in T2 . The input trees
are preprocessed to replace any tree with an enclosing taxon P on a leaf by the same tree
with any single subtree rooted at P substituted in its place. There must be such a subtree
elsewhere in the input forest, or we have a contradiction that M is an enclosing taxon. This
process does not add or remove any information, since the relationships between M and
everything in the tree still holds, and the new relationships between the taxa in the subtree
at M and the rest of the tree were always implicit in the input.
The aim of this process was just to obtain a set of inputs where enclosing taxa appear
on internal nodes only, because without loss of generality our constraint encoding assumes
that this is the case. Figure 21 shows an example of the substitution process applied to
trees T1 and T2 , and T2 would be replaced with T2 .
Variables and constraints The variables added are one integer variable vl per enclosing
taxa/label l, each with a domain of {1, . . . , n  1}. The value of this variable in a solution
is the tree depth of the internal node which it labels, and the labels position in the final
tree is determined because it must be at the unique node of that depth on a path from one
of its nested taxa to the root. See Figure 15 and suppose for the sake of argument that
we have an enclosing taxa M which labels b, and the variable lM = 1 in a solution. The
unique location where the label M can go is at the root node.
Properties (1) and (2) from Definition 9 above are immediate from the properties of
the earlier model which is the foundation for this one. Before explaining how to enforce
property (3) we introduce some notations for convenience. Function desc(l, F ) returns the
set of all descendants of label l in any tree T in the forest F , and notDesc(l, T ) returns the
set of labels that are not descendants of l in tree T .
We first need a constraint that l must label every single species that it labels in an input.
For every enclosing label l, post the following set of constraints:
{vl  Mij | i  desc(l, F )  j  desc(l, F )  i 6= j}

(6)

so that the label must settle at least as shallow as any mrca of its descendants in input, and
hence they must remain descendants. Notice that it is necessary to consider pairs of species
from distinct input trees. An alternative of taking pairs from the same tree does not work,
because it is necessary for all pairs to be under the same internal node l, rather than two
930

fiThe Ultrametric Constraint and Phylogenetics

distinct nodes that happen to be at the correct depth. Next, the label must be constrained
so that no label that was not already a descendant becomes one. For each X-tree T and
enclosing label l  X, post the following set of constraints:
{vl > Mij | i  desc(l, {T })  j  notDesc(l, T )}

(7)

so that the label l must be placed strictly deeper than any mrca of a descendant and
something thats not a descendant, i.e., no non-descendents of l can be a descendent in the
result. As an illustration we list the generated constraints for the example of Figure 20.
1. Equation 6 and l = P : {vP  Mab , vP  Mag , vP  Mbg }
2. Equation 6 and l = Q: {vQ  Mde , vQ  Mdf , vQ  Mef }
3. Equation 7, l = P and T  {T1 , T2 }: {vP > Mac , vP > Mad , vP > Mae , vP >
Mbc , vP > Mbd , vP > Mbe , vP > Mgf , vP > Mgd , vP > Mge , vP > Mbf }
4. Equation 7, l = Q and T  {T1 , T2 }: {vQ > Mad , vQ > Mae , vQ > Mbd , vQ >
Mbe , vP > Mcd , vQ > Mce , vQ > Mgf , vP > Mbf , vP > Mgd , vP > Mge }
The number of new constraints created by both Equations 6 and 7 is bounded by the
number of distinct pairs of species, i.e. (n2 ) new constraints.
6.3 Necessity
There may be many possible supertrees for a given input forest. One question is then,
what relationships are common to all supertrees? The problem of determining if a derived
induced triple (or fan) in a supertree is necessary (i.e., common to all possible supertrees) is
introduced by (Daniel, 2003) along with the polynomial time decision procedure Necessity.
The algorithm Necessity in Figure 22 takes as arguments a forest F of trees, assumed
to be compatible, and a rooted triple or fan  and determines if  occurs in every supertree
that displays the trees in F . The algorithm is a simple modification of CPBuild, where
lines 1 to 3 are essentially the same. In line 4 the negation of the triple  is posted to the
problem, where  is posted as
Mik 6= Mjk  Mij  Mik  Mij  Mik

(8)

when  = (ij)k and posted as
Mij 6= Mik  Mij 6= Mjk  Mik 6= Mjk

(9)

when  = (ijk). A call is then made to propagate to make the problem arc-consistent (line
5), and if this fails then  is necessary, otherwise it is not necessary. The algorithm has the
same complexity as CPBuild.
6.4 Coping with Conflict
When a supertree cannot be produced from a pair of trees some of the input triples and fans
must be in conflict with one another, either directly or indirectly. Junkers quickXPlain
method (Junker, 2004) discovers a minimal subset of constraints that when posted and
propagated result in a failure. This set is not necessarily the smallest possible set but is
931

fiMoore & Prosser

Algorithm Necessity
Necessity(F,  )
1 let (V, D, C)  CPModel(F )
2 for T  F do
3
for t  BreakUp(T ) do C  post(t, C)
4 C  post(, C)
5 return propagate(V, D, C)

Figure 22: Does the triple/fan  occur in every supertree that displays the trees in F ?
minimal in the sense that the removal of any element from this set will not constitute a
sound explanation, and the addition of any constraint would be redundant. When the set of
constraints are input triples and fans, this minimal set is semantically a collection of input
data that is incompatible. Junker (2004) state that this method can be achieved by a worst
case of 2k  log2 (n/k) + 2k propagations4 , where k is the size of the minimal explanation
found and n is the number of constraints.
An alternative approach is to satisfy as many of the input triples and fans as is possible
within a reasonable amount of time, i.e., polynomial time. Semple and Steel propose such
an algorithm, MinCutSupertree (2000), and this has been refined by Page (2002). We
now propose a similar scheme within the constraint programming framework. We call
this algorithm GreedyBuild and it works as follows. We associate a constrained integer
variable x, with a domain of {0, 1}, to each triple and fan. If the variable is assigned the
value 0 then the triple (or fan) is respected, otherwise it is ignored. Therefore for a triple
(ij)k we post the constraint of equation 10 and for a 3-fan (ijk) the constraint of equation
11.

(x = 0  Mij > Mik = Mjk )  (x = 1  (Mik 6= Mjk  Mij  Mik  Mij  Mik ))

(10)

(x = 0  Mij = Mik = Mjk )  (x = 1  (Mij 6= Mik  Mij 6= Mjk  Mik 6= Mjk ))

(11)

GreedyBuild then instantiates in turn each of the x variables, i.e. the decision variables,
preferring the value 0 to the value 1, and after each instantiation the problem is made arcconsistent. The algorithm is shown in Figure 23. In line 1 a constraint model is produced,
i.e., an n  n symmetric array of constrained integers variables is created, where there are n
unique species in the forest, and the UM-Matrix-BCZ constraint is then posted over those
variables. The variable X of line 2 is then the set of decision variables. The input trees
are broken up as before, and a new variable x is created for each triple or fan. In line
6 the constraints of equations 10 and 11 are posted into the model. The loop of lines 10
to 12 in turn select a decision variable, set it to its lowest possible value, and then make
the problem arc-consistent. This might in turn cause uninstantiated variables to have the
value 0 removed from their domain if their associated triple or fan conflicts with the triple
4. Our BC(Z) propagator is O(1), however, so the time complexity is the same as the number of
propagations.

932

fiThe Ultrametric Constraint and Phylogenetics

Algorithm GreedyBuild
GreedyBuild(F )
1
let (V, D, C)  CPModel(F )
2
let X  
3
for T  F do
4
for t  BreakUp(T ) do
5
let x  newV ar(0, 1)
6
let c  newConstraint((t  x = 0)  (t  x = 1))
7
X  X  {x}
8
V  V  {x}
9
C  post(c, C)
10 for x  X do
11
instantiate(x)
12
propagate(V, D, C)
13 return UMToTree(V, D)

Figure 23: Greedily Build a supertree from forest F using the ultrametric constraint model.

or fan that has just been enforced. This process terminates without failure, because any
conflicting triples or fans are essentially ignored. In line 13 the ultrametric matrix is then
converted to a tree. The complexity of GreedyBuild is then O((t + f )  n4 ) where there
are t triples and f fans.
GreedyBuild was applied to the forest of bird data {A, B, C, D, E, F, G} from section 5, using a soft breakup. This data is incompatible when we use CPBuild, however
GreedyBuild produces the supertree in Figure 24. This supertree contains 121 species.
SoftBreakup produced 201 triples, and of those 17 were rejected. It took less than 2
seconds to build the model and about 100 seconds to solve that model. This should be
compared to CPBuild over the same data set, taking 1.5 seconds to build the model and
10 seconds to determine incompatibility. GreedyBuild was also applied to the data set
ABDF, producing the identical supertree to CPBuild, in comparable time (890ms to build
the model and 578ms to solve).
Having executed GreedyBuild the decision variables in the set X (lines 2, 7, 10 and
11) can be analysed to identify the set of triples and fans that have been excluded from the
supertree, i.e., if an x variable has been instantiated with the value 1 then its corresponding
triple or fan has been ignored.
Note that we do not claim any biological significance in the arbitrary order we use
to suppress triples. GreedyBuild could be amended to follow the order of MinCutSupertree but we have not investigated this. GreedyBuild can also be enhanced as
follows. Currently if a triple or fan exists in multiple input trees then it occurs only once
as a constraint. This information could be exploited by weighting the decision variables to
take into consideration the relative weight of evidence for a triple or fan, e.g., the number of
times that a triple or fan occurs as input. The decision variables can then be instantiated
in non-increasing order of weight, i.e., a variable ordering heuristic can be used. In the
extreme GreedyBuild can be modified to become OptBuild where a full backtracking
search is performed with the objective of minimising the sum of the decision variables, but
933

fiMoore & Prosser

Figure 24: Supertree with largest data set of birds, ABCDEFG, with 121 species. This
took about 2 seconds to model and 100 seconds to solve using GreedyBuild.
Displayed using Rod Pages treeView.

934

fiThe Ultrametric Constraint and Phylogenetics

at a potentially exponential cost in time. This would return the tree with the fewest possible
input triples suppressed.
6.5 Summary
With little effort, the constraint model has been adapted to deal with ancestral divergence
dates and nested taxa. Both have been achieved by adding side constraints. This has an
added advantage with respect to ancestral divergence as it can result in a more compact
enumeration of output trees when interior nodes are labelled with ranges rather than specific
values.
When input trees conflict we propose two options: use quickXPlain to determine the
cause of that conflict or greedily build a supertree using GreedyBuild. Bryant et al. (2004)
state that they have essentially an all-or-nothing approach to supertree construction when
using RankedTree and what is needed is something akin to MinCutSupertree, i.e.
when trees are incompatible build a supertree that violates the minimum number of triples
or fans, and do this in polynomial time (Page, 2002; Semple & Steel, 2000). This has since
been done by Bordewich et al. (2006) and can also be done in our constraint model by
incorporating the constraints identified in section 6.1 into GreedyBuild.
Although not shown, it is obvious that ancestral divergence data and nested taxa can
be combined in the one model, simply by adding all the necessary constraint and auxiliary
variables for both variants into the one model. This, again, could be done in GreedyBuild,
but would require some heuristic or rule to be used when deciding what constraints to ignore
when the input trees and side constraints are incompatible.
In our opinion, deriving, combining and analysing the results of imperative algorithms
for supertree problems is much more difficult than the above. Most algorithms for variants
required far more complex data structures and tailored algorithms for processing them.
Moreover, to combine the algorithms once produced seems practically impossible on account
of their intricacy. Finally constraint programming provides various generic methods like
quickXPlain out of the box that turn out to be of interest in supertree problems.

7. Conclusion
We have presented a new constraint propagator for the ultrametric constraint over three
integer variables, and shown how this can be extended to a symmetric matrix of constrained
integer variables. When bounds(Z)-consistency is established on the symmetric array the
lower bounds of variables give mutual support. This is sufficient for modelling and solving
the supertree construction problem in O(n4 ) time and O(n2 ) space, comparable to the complexity of OneTree (Ng & Wormald, 1996) but inferior to that of the algorithm of Bryant
and Steel (1995). So, why bother with the CPBuild approach when efficient imperative
approaches already exist? The answer lies in the versatility of constraint programming.
Rather than develop a new algorithm for a new variant of the supertree problem we add
side constraints to a base model, and we have shown that a polynomial time bound can
often be achieved. We have done this for ancestral divergence dates and nested taxa, we
have shown how our model can be used to deliver necessary triples and fans, and we have
proposed GreedyBuild as a way of dealing with incompatible trees.
935

fiMoore & Prosser

Acknowledgments
We would like to thank Pierre Flener and Xavier Lorca; Barbara Smith, Ian Gent and Christine Wei Wu; Charles Semple, Mike Steel and Rod Page; Muffy Calder and Joe Sventek;
Stanislav Zivny; Chris Unsworth; and our three anonymous reviewers/co-authors.

References
Aho, A., Sagiv, Y., Szymanski, T., & Ullman, J. (1981). Inferring a tree from lowest
common ancestors with an application to the optimization of relational expressions.
SIAM J. Comput, 10 (3), 405421.
Beldiceanu, N., Flener, P., & Lorca, X. (2008). Combining tree partitioning, precedence,
and incompatibility constraints. Constraints, 13, 131.
Bessiere, C., & Regin, J.-C. (2001). Refining the basic constraint propagation algorithm.
In IJCAI, pp. 309315.
Bessiere, C. (2006). Constraint propagation. In Handbook of constraint programming. Elsevier. Chapter 3.
Bininda-Emonds, O. (2004). Phylogenetic Supertrees: Combining information to reveal the
tree of life. Springer.
Bordewich, M., Evans, G., & Semple, C. (2006). Extending the limits of supertree methods.
Annals of combinatorics, 10, 3151.
Bryant, D., & Steel, M. (1995). Extension Operations on Sets of Leaf-labeled Trees. Advances in Applied Mathematics, 16, 425453.
Bryant, D., Semple, C., & Steel, M. (2004). Supertree methods for ancestral divergence
dates and other applications. In Bininda-Emonds, O. (Ed.), Phylogenetic Supertrees:
Combining information to reveal the tree of life, pp. 151171. Computational Biology
Series Kluwer.
Carlier, J., & Pinson, E. (1994). Adjustment of heads and tails for the jobshop scheduling
problem. European Journal of Operational Research, 78, 146161.
Caseau, Y., & Laburthe, F. (1997). Solving small TSPs with constraints. In Proceedings
International Conference on Logic Programming, pp. 115.
Choco (2008). http://www.choco-constraints.net/ home of the choco constraint programming system..
Daniel, P. (2003). Supertree methods: Some new approaches. Masters thesis, Department
of Mathematics and Statistics, University of Canterbury.
Daniel, P., & Semple, C. (2004). Supertree algorithms for nested taxa. In Bininda-Emonds,
O. (Ed.), Phylogenetic Supertrees: Combining information to reveal the tree of life,
pp. 151171. Computational Biology Series Kluwer.
936

fiThe Ultrametric Constraint and Phylogenetics

Dawkins, R., & Wong, Y. (2004). The Ancestors Tale. Weidenfeld and Nicholson.
Debruyne, R., & Bessiere, C. (1997). Some practicable filtering techniques for the constraint
satisfaction problem. In Proceedings of IJCAI97, pp. 412417.
Dooms, G. (2006). The CP(Graph) Computation Domain in Constraint Programming.
Ph.D. thesis, Universite catholique de Louvain, Faculte des sciences appliquees.
Gent, I., Prosser, P., Smith, B., & Wei, W. (2003). Supertree construction with constraint
programming. In Principles and Practice of Constraint Programming, pp. 837841.
Springer.
Gusfield, D. (1997). Algorithms on strings, trees, and sequences: computer science and
computational biology. Cambridge University Press, New York, NY, USA.
Janczewski, D., Modi, W., Stephens, J., & OBrien, S. (1995). Molecular evolution of
mitochondrial 12S RNA and Cytochrome b sequences in the pantherine lineage of
Felidae. Mol. Biol. Evol., 12, 690707.
Jeavons, P. G., & Cooper, M. C. (1995). Tractable constraints on ordered domains. Artif.
Intell., 79 (2), 327339.
Junker, U. (2004). QUICKXPLAIN: Preferred Explanations and Relaxations for OverConstrained Problems. In Proceedings AAAI2004, pp. 167172.
Kennedy, M., & Page, R. (2002). Seabird supertrees: Combining partial estimates of procellariiform phylogeny. The Auk, 69, 88108.
Lhomme, O. (2003). An efficient filtering algorithm for disjunction of constraints. In
Principles and Practice of Constraint Programming, pp. 904908. Springer.
Mace, G. M., Gittleman, J. L., & Purvis, A. (2003). Preserving the Tree of Life. Science,
300, 17071709.
Mackworth, A. (1977). Consistency in networks of relations. Artificial Intelligence, 8,
99118.
Ng, M. P., & Wormald, N. C. (1996). Reconstruction of rooted trees from subtrees. Discrete
Appl. Math., 69 (1-2), 1931.
Page, R. (1996). TREEVIEW: An application to display phylogenetic trees on personal
computers. Computer Applications in the Biosciences, 12, 357358.
Page, R. (2004). Taxonomy, supertrees, and the tree of life. In Bininda-Emonds, O. (Ed.),
Phylogenetic Supertrees: Combining information to reveal the tree of life, pp. 247265.
Computational Biology Series Kluwer.
Page, R. D. M. (2002). Modified mincut supertrees. In WABI 02: Proceedings of the Second
International Workshop on Algorithms in Bioinformatics, pp. 537552 London, UK.
Springer-Verlag.
937

fiMoore & Prosser

Pennisi, E. (2003). Modernizing the Tree of Life. Science, 300, 16921697.
Prosser, P. (2006). Supertree construction with constraint programming: recent progress
and new challenges. In WCB06 - Workshop on Constraint Based Methods for Bioinformatics, pp. 7582.
Prosser, P., & Unsworth, C. (2006). Rooted Tree and Spanning Tree Constraints. In 17th
ECAI Workshop on Modelling and Solving Problems with Constraints.
Regin, J.-C. (1994). A filtering algorithm for constraints of difference in CSPs. In Proceedings AAAI94, pp. 362367.
Rossi, F., van Beek, P., & Walsh, T. (2007). Handbook of Constraint Programming. Elsevier.
Sabin, D., & Freuder, E. (1994). Contradicting conventional wisdom in constraint satisfaction. In Proceedings of ECAI-94, pp. 125129.
Schulte, C., & Carlsson, M. (2006). Finite domain constraint programming systems. In
Handbook of constraint programming. Elsevier. Chapter 14.
Semple, C., Daniel, P., Hordijk, W., Page, R., & Steel, M. (2004). Supertree algorithms for
ancestral divergence dates and nested taxa. Bioinformatics, 20 (15), 23552360.
Semple, C., & Steel, M. (2000). A supertree method for rooted trees. Discrete Appl. Math.,
105 (1-3), 147158.
Smith, B. M. (1995). A Tutorial on Constraint Programming. Technical Report 95.14,
University of Leeds.
TreeBASE (2003). http://www.treebase.org/ TreeBASE: a database of phylogenetic knowledge..
Tsang, E. (1993). Foundations of Constraint Satisfaction. Academic Press.
van Hentenryck, P., Deville, Y., & Teng, C.-M. (1992). A generic arc-consistency algorithm
and its specializations. Artificial Intelligence, 57, 291321.
van Hentenryck, P., Saraswat, V., & Deville, Y. (1998). Design, implementation, and
evaluation of the constraint language cc(fd). Journal of Logic Programming, 37, 139
164.
Wu, G., You, J.-H., & Lin, G. (2007). Quartet-based phylogeny reconstruction with answer
set programming. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 4, 139152.
Yuanlin, Z., & Yap, R. H. C. (2001). Making AC-3 an optimal algorithm. In IJCAI, pp.
316321.

938

fiJournal of Artificial Intelligence Research 32 (2008) 793-824

Submitted 12/07; published 08/08

Analogical Dissimilarity: Definition, Algorithms
and Two Experiments in Machine Learning
Laurent Miclet
Sabri Bayoudh
Arnaud Delhay

LAURENT. MICLET @ UNIV- RENNES 1. FR
SABRI . BAYOUDH @ UNIV- ST- ETIENNE . FR
ARNAUD . DELHAY @ UNIV- RENNES 1. FR

IRISA/CORDIAL, 6, rue de Kerampont
BP 80518 - F-22305 Lannion Cedex, France

Abstract
This paper defines the notion of analogical dissimilarity between four objects, with a special
focus on objects structured as sequences. Firstly, it studies the case where the four objects have a
null analogical dissimilarity, i.e. are in analogical proportion. Secondly, when one of these objects
is unknown, it gives algorithms to compute it. Thirdly, it tackles the problem of defining analogical
dissimilarity, which is a measure of how far four objects are from being in analogical proportion. In
particular, when objects are sequences, it gives a definition and an algorithm based on an optimal
alignment of the four sequences. It gives also learning algorithms, i.e. methods to find the triple
of objects in a learning sample which has the least analogical dissimilarity with a given object.
Two practical experiments are described: the first is a classification problem on benchmarks of
binary and nominal data, the second shows how the generation of sequences by solving analogical
equations enables a handwritten character recognition system to rapidly be adapted to a new writer.

1. Introduction
Analogy is a way of reasoning that has been studied throughout the history of philosophy and has
been widely used in Artificial Intelligence and Linguistics. We focus in this paper on a restricted
concept of analogy called analogical proportion.
1.1 Analogical Proportion between Four Elements
An analogical proportion between four elements A, B, C and D in the same universe is usually
expressed as follows: A is to B as C is to D. Depending on the elements, analogical proportions1
can have very different meanings. For example, natural language analogical proportions could be:
a crow is to a raven as a merlin is to a peregrine or vinegar is to wine as a
sloe is to a cherry. They are based on the semantics of the words. By contrast, in the formal
universe of sequences, analogical proportions such as abcd is to abc as abbd is to abb or
g is to gt as gg is to ggt are morphological.
Whether morphological or not, the examples above show the intrinsic ambiguity in
defining an analogical proportion.
We could as well accept, for other good reasons:
g is to gt as gg is to ggtt or vinegar is to wine as vulgar is to wul. Obviously,
such ambiguities are inherent in semantic analogies, since they are related to the meaning of words
(the concepts are expressed through natural language). Hence, it seems important, as a first step,
to focus on formal morphological properties. Moreover, solving such analogies in sequences is an
1. When there is no ambiguity, we may use analogy for short instead of analogical proportion.
c
2008
AI Access Foundation. All rights reserved.

fiM ICLET, BAYOUDH & D ELHAY

operational problem in several fields of linguistics, such as morphology and syntax, and provides a
basis to learning and data mining by analogy in the universe of sequences.
In this paper, we will firstly consider analogical proportions in sets of objects and we will secondly present how they may be transferred to sequences of elements of these sets.
1.2 Solving Analogical Equations
When one of the four elements is unknown, an analogical proportion turns into an equation. For
instance, on sequences of letters, the analogical proportion wolf is to leaf as wolves is to x
corresponds to the equation S = {x | wolf is to leaf as wolves is to x}. Resolving this equation consists in computing the (possibly empty) set S of sequences x which satisfy the analogy. The
sequence leaves is an exact semantic and morphological solution. We shall see that, however, it
is not straightforward to design an algorithm able to solve this kind of equation, in particular when
looking for an approximate solution if necessary.
Solving analogical equations on sequences is useful for linguistic analysis tasks and has been
applied (with empirical resolution techniques, or in simple cases) mainly to lexical analysis tasks.
For example, Yvon (1999) presents an analogical approach to the grapheme-to-phoneme conversion,
for text-to-speech synthesis purposes. More generally, the resolution of analogical equations can
also be seen as a basic component of learning by analogy systems, which are part of the lazy
learning techniques (Daelemans, 1996).
1.3 Using Analogical Proportions in Machine Learning
Let S = {(x, u(x))} be a finite set of training examples, where x is the description of an example
(x may be a sequence or a vector in Rn , for instance) and u(x) its label in a finite set. Given
the description y of a new pattern, we would like to assign to y a label u(y), based only from the
knowledge of S. This is the problem of inductive learning of a classification rule from examples,
which consists in finding the value of u at point y (Mitchell, 1997). The nearest neighbor method,
which is the most popular lazy learning technique, simply finds in S the description x which
minimizes some distance to y and hypothesizes u(x ), the label of x , for the label of y.
Moving one step further, learning from analogical proportions consists in searching in S for a
triple (x , z  , t ) such that x is to z  as t is to y and predicts for y the label u(y) which is
solution of the equation u(x ) is to u(z  ) as u(t ) is to u(y). If more than one triple is found,
a voting procedure can be used. Such a learning technique is based on the resolution of analogical
equations. Pirrelli and Yvon (1999) discuss the relevance of such a learning procedure for various
linguistic analysis tasks. It is important to notice that y and u(y) are in different domains: for
example, in the simple case of learning a classification rule, y may be a sequence whereas u is a
class label.
The next step in learning by analogical proportions is, given y, to find a triple (x , z  , t ) in
S such that x is to z  as t is to y holds almost true, or, when a closeness measure is defined,
the triple which is the closest to y in term of analogical proportion. We study in this article how to
quantify this measure, in order to provide a more flexible method of learning by analogy.
794

fiA NALOGICAL D ISSIMILARITY

1.4 Related Work
This paper is related with several domains of artificial intelligence. Obviously, the first one is that
of reasoning by analogy. Much work has been done on this subject from a cognitive science point
of view, which had led to computational models of reasoning by analogy: see for example, the
classical paper (Falkenhainer, Forbus, & Gentner, 1989), the book (Gentner, Holyoak, & Kokinov,
2001) and the recent survey (Holyoak, 2005). Usually, these works use the notion of transfer,
which is not within the scope of this article. It means that some knowledge on solving a problem
in a domain is transported to another domain. Since we work on four objects that are in the same
space, we implicitly ignore the notion of transfer between different domains. Technically speaking,
this restriction allows us to use an axiom called exchange of the means to define an analogical
proportion (see Definition 2.1). However, we share with these works the following idea: there may
be a similar relation between two couples of structured objects even if the objects are apparently
quite different. We are interested in giving a formal and algorithmic definition of such a relation.
Our work also aims to define some supervised machine learning process (Mitchell, 1997; Cornujols & Miclet, 2002), in the spirit of lazy learning. We do not seek to extract a model from the
learning data, but merely conclude what is the class, or more generally the supervision, of a new
object by inspecting (a part of) the learning data. Usually, lazy learning, like the k-nearest neighbors
technique, makes use of unstructured objects, such as vectors. Since distance measures can be also
defined on strings, trees and even graphs, this technique has also been used on structured objects, in
the framework of structural pattern recognition (see for example the work of Bunke & Caelli, 2004;
Blin & Miclet, 2000; Basu, Bunke, & Del Bimbo, 2005). We extend here the search of the nearest
neighbor in the learning set to that of the best triple (when combined with the new object, it is the
closest to make an analogical proportion). This requires defining what is an analogical proportion
on structured objects, like sequences, but also to give a definition of how far a 4-tuple of objects is
from being in analogy (that we call analogical dissimilarity).
Learning by analogy on sequences has already being studied, in a more restricted manner, on
linguistic data (Yvon, 1997, 1999; Itkonen & Haukioja, 1997). Reasoning and learning by analogy has proven useful in tasks like grapheme to phoneme conversion, morphology and translation.
Sequences of letters and/or of phonemes are a natural application to our work, but we are also interested in other type of data, structured as sequences or trees, such as prosodic representations for
speech synthesis, biochemical sequences, online handwriting recognition, etc.
Analogical proportions between four structured objects of the same universe, mainly strings,
have been studied with a mathematical and algorithmic approach, like ours, by Mitchell (1993) and
Hofstadter et al. (1994), Dastani et al. (2003), Schmid et al. (2003). To the best of our knowledge
our proposition is original: to give a formal definition of what can be an analogical dissimilarity
between four objects, in particular between sequences, and to produce algorithms that enable the
efficient use of this concept in machine learning practical problems. We have already discussed
how to compute exact analogical proportions between sequences in the paper by Yvon et al. (2004)
and given a preliminary attempt to compute analogical dissimilarity between sequences in the paper
by Delhay and Miclet (2004). Excerpts of the present article have been presented in conferences
(Bayoudh, Miclet, & Delhay, 2007a; Bayoudh, Mouchre, Miclet, & Anquetil, 2007b).
To connect with another field of A.I., let us quote Aamodt and Plaza (1994) about the use of
the term analogy in Case-Based Reasoning (CBR): Analogy-based reasoning: This term is sometimes used, as a synonym to case-based reasoning, to describe the typical case-based approach.
795

fiM ICLET, BAYOUDH & D ELHAY

However, it is also often used to characterize methods that solve new problems based on past cases
from a different domain, while typical case-based methods focus on indexing and matching strategies for single-domain cases. According to these authors, who use the word analogy in its broader
meaning, typical CBR deals with single domain problems, as analogical proportions also do. In that
sense, our study could be seen as a particular case of CBR, as applied in this paper to supervised
learning of classification rules.
1.5 Organization of the Paper
This paper is organized in six sections. After this introduction, we present in section 2 the general
principles which govern the definition of an analogical proportion between four objects in the same
set and we define what is an analogical equation in a set. We apply these definitions in Rn and
{0, 1}n . Finally, this section defines analogical proportion between four sequences on an alphabet
in which an analogy is defined, using an optimal alignment method between the four sequences.
Sections 3 introduces the new concept of analogical dissimilarity (AD) between four objects, by
measuring in some way how much these objects are in analogy. In particular, it must be equivalent
to say that four objects are in analogy and that their analogical dissimilarity is null. Then we extend
it to sequences. The end of this section gives two algorithms: SEQUANA4 computes the value of
AD between four sequences and SOLVANA solves analogical equations in a generalized manner:
it can produce approximate solutions (i.e. of strictly positive AD).
Section 4 begins to explore the use of the concept of analogical dissimilarity in supervised
machine learning. We give an algorithm (FADANA) for the fast search of the k-best analogical
3-tuples in the learning set.
Section 5 presents two applications of these concepts and algorithms on real problems. We
firstly apply FADANA to objects described by binary and nominal features. Experiments are conducted on classical benchmarks and favorably compared with standard classification techniques.
Secondly, we make use of SOLVANA to produce new examples in a handwritten recognition system. This allows training a classifier from a very small number of learning patterns.
The last section presents work to be done, particularly in discussing more real world application
of learning by analogy, especially in the universe of sequences.

2. Analogical Proportions and Equations
In this section, we give a formal definition of the analogical proportion between four objects and
explain what is to solve an analogical equation. Instanciations of the general definitions are given
when the objects are either finite sets (or equivalently binary vectors), or vectors of real numbers or
sequences on finite alphabets.
2.1 The Axioms of Analogical Proportion
The meaning of an analogical proportion A : B :: C : D between four objects in a set X depends
on the nature of X, in which the is to and the as relations have to be defined. However, general
properties can be required, according to the usual meaning of the word analogy in philosophy and
linguistics. According to Lepage (2003) three basic axioms can be given:
Definition 2.1 (Analogical proportion) An analogical proportion on a set X is a relation on X 4 ,
i.e. a subset A  X 4 . When (A, B, C, D)  A, the four elements A, B, C and D are said to be
796

fiA NALOGICAL D ISSIMILARITY

in analogical proportion, and we write: the analogical proportion A : B :: C : D holds true,
or simply A : B :: C : D , which reads A is to B as C is to D. For every 4-tuple in analogical
proportion, the following equivalences must hold true:
Symmetry of the as relation: A : B :: C : D  C : D :: A : B
Exchange of the means: A : B :: C : D  A : C :: B : D
The third axiom (determinism) requires that one of the two following implications holds true
(the other being a consequence):
A : A :: B : x
A : B :: A : x




x=B
x=B

According to the first two axioms, five other formulations are equivalent to the canonical form
A : B :: C : D :
B : A :: D : C
D : C :: B : A

D : B :: C : A
and

C : A :: D : B
B : D :: A : C

Consequently, there are only three different possible analogical proportions between four objects,
with the canonical forms:
A : B :: C : D

A : C :: D : B

A : D :: B : C

2.2 Analogical Equations
To solve an analogical equation consists in finding the fourth term of an analogical proportion, the
first three being known.
Definition 2.2 (Analogical equation) D is a solution of the analogical equation A : B :: C : x if
and only if A : B :: C : D .
We already know from previous sections that, depending on the nature of the objects and the definition of analogy, an analogical equation may have either no solution or a unique solution or several
solutions. We study in the sequel how to solve analogical equations in different sets.
2.3 Analogical Proportion between Finite Sets and Binary Objects
When the as relation is the equality between sets, Lepage has given a definition of an analogical
proportion between sets coherent with the axioms. This will be useful in section 2.3.2 in which
objects are described by sets of binary features.
2.3.1 A N A NALOGICAL P ROPORTION IN F INITE S ETS
Definition 2.3 (Analogical proportion between finite sets) Four sets A, B, C and D are in analogical proportion A : B :: C : D if and only if A can be transformed into B, and C into D, by
adding and subtracting the same elements to A and C.
This is the case, for example, of the four sets: A = {t1 , t2 , t3 , t4 , }, B = {t1 , t2 , t3 , t5 } and C =
{t1 , t4 , t6 , t7 }, D = {t1 , t5 , t6 , t7 }, where t4 has been taken off from, and t5 has been added to A
and C, giving B and D.
797

fiM ICLET, BAYOUDH & D ELHAY

2.3.2 S OLVING A NALOGICAL E QUATIONS IN F INITE S ETS
Considering analogy in sets, Lepage (2003) has shown the following theorem, with respect to the
axioms of analogy (section 2.1):
Theorem 2.4 (Solution of an analogical equation in sets) Let A, B and C be three sets. The analogical equation A : B :: C : D where D is the unknown has a solution if and only if the following
conditions hold true:
A  B  C and A  B  C
The solution is then unique, given by:
D = ((B  C)\A)  (B  C)
2.3.3 A NALOGICAL P ROPORTIONS IN {0, 1}n
Let now X be the set {0, 1}n . For each x  X and each i  [1, n], fi (x) = 1 (resp. fi (x) = 0)
means that the binary feature fi takes the value T RU E (resp. F ALSE) on the object x.
Let A : B :: C : D be an analogical equation. For each feature fi , there are only eight different
possibilities of values on A, B and C. We can derive the solutions from the definition and properties
of analogy on sets, with the two following principles:
 Each feature fi (D) can be computed independently.
 The following table gives the solution fi (D):
fi (A)
fi (B)
fi (C)
fi (D)

0
0
0
0

0
0
1
1

0
1
0
1

0
1
1
?

1
0
0
?

1
0
1
0

1
1
0
0

1
1
1
1

In two cases among the eight, fi (D) does not exists. This derives from the defining of X by binary
features, which is equivalent to defining X as a finite set. Theorem 2.4 imposes conditions on the
resolution of analogical equations on finite sets, which results in the fact that two binary analogical
equations have no solution.
2.4 Analogical Proportion in Rn
2.4.1 D EFINITION
Let O be the origin of Rn . Let a = (a1 , a2 , . . . , an ) be a vector of Rn , as defined by its n
coordinates. Let a, b, c and d be four vectors of Rn . The interpretation of an analogical proportion
a : b :: c : d is usually that a, b, c, d are the corners of a parallelogram, a and d being opposite
corners (see Figure 1).
Definition 2.5 (Analogical proportion in Rn ) Four elements of Rn are in the analogical propor 
  

tion (a : b :: c : d) if and only if they form a parallelogram, that is when Oa + Od = Ob + Oc

 




or equivalently ab = cd or equivalently 
ac = bd
It is straightforward that the axioms of analogy, given in section 2.1 are verified by this definition.
798

fiA NALOGICAL D ISSIMILARITY

d
b
c
a
Figure 1: Analogical parallelogram in Rn .

2.4.2 S OLVING A NALOGICAL E QUATIONS IN Rn
Solving the analogical equation a : b :: c : x , where a, b and c are vectors of Rn and x is the
unknown derives directly from the definition of analogy in vector spaces: the four vectors must
form a parallelogram. There is always one and only one solution given by the equation:
 
 
 
Ox = Ob + Oc  Oa
2.5 Analogical Proportion between Sequences
2.5.1 N OTATIONS
A sequence2 is a finite series of symbols from a finite alphabet . The set of all sequences is denoted
 . For x, y in  , xy denotes the concatenation of x and y. We also denote | x | = n the length
of x, and we write x as x = x1 . . . x|x| or x = x[1] . . . x[n], with xi or x[i]  . We denote  the
empty word, of null length, and + =  \{}.
A factor (or subword) f of a sequence x is a sequence in  such that there exists two sequences
u and v in  with: x = uf v. For example, abb and bbac are factors of abbacbbaba.
A subsequence of a sequence x = x1 . . . x|x| is composed of the letters of x with the indices
i1 . . . ik , such that i1 < i2 . . . < ik . For example, ca and aaa are two subsequences of abbacbaba.
2.5.2 D EFINITION
Let  be an alphabet. We add a new letter to , that we denote , giving the augmented alphabet
 . The interpretation of this new letter is simply that of an empty symbol, that we will need in
subsequent sections.
Definition 2.6 (Semantic equivalence) Let x be a sequence of  and y a sequence of  . x and
y are semantically equivalent if the subsequence of y composed of letters of  is x. We denote this
relation by .
For example, ab  a  a  abaa.
Let us assume that there is an analogy in  , i.e. that for every 4-tuple a, b, c, d of letters of  ,
the relation a : b :: c : d is defined as being either T RU E or F ALSE.
Definition 2.7 (Alignment between two sequences) An alignment between two sequences x, y 
 , of lengths m and n, is a word z on the alphabet ( )  ( ){(, )} whose first projection
is semantically equivalent to x and whose second projection is semantically equivalent to y.
2. More classically in language theory, a word or a sentence.

799

fiM ICLET, BAYOUDH & D ELHAY

Informally, an alignment represents a one-to-one letter matching between the two sequences, in
which some letters  may be inserted. The matching (, ) is not permitted. An alignment can
be presented as an array of two rows, one for x and one for y, each word completed with some ,
resulting in two words of  having the same length.
For instance, here is an alignment between x = abgef and y = acde :
x

=

y

=

a
|
a

b
|
c


|
d

g
|


e
|
e

f
|


We can extend this definition to that of an alignment between four sequences.
Definition 2.8 (Alignment between four sequences) An alignment between four sequences
u, v, w, x   , is a word z on the alphabet (  {})4 {(, , , )} whose projection on
the first, the second, the third and the fourth component is respectively semantically equivalent to
u, v, w and x.
The following definition uses alignments between four sequences.
Definition 2.9 (Analogical proportion between sequences) Let u, v, w and x be four sequences
on  , on which an analogy is defined. We say that u, v, w and x are in analogical proportion in 
if there exists four sequences u , v  , w and x of same length n in  , with the following properties:
1. u  u, v   v, w  w and x  x.
2. i  [1, n] the analogies ui : vi :: wi : xi hold true in  .
One has to note that Lepage (2001) and Stroppa and Yvon (2004) have already proposed a definition of an analogical proportion between sequences with applications to linguistic data. Basically,
the difference is that they accept only trivial analogies in the alphabet (such as a : b :: a : b or
a ::: a :).
For example, let  = {a, b, , , B, C, } with the non trivial analogies a : b :: A : B ,
a :  :: b :  and A :  :: B :  . The following alignment between the four sequences aBA,
bBA, ba and ba is an analogical proportion on  :
a

b


B
B
a
a


b

b

A
A



3. Analogical Dissimilarity
3.1 Motivation
In this section, we are interested in defining what could be a relaxed analogy, which linguistic
expression would be a is to b almost as c is to d. To remain coherent with our previous definitions,
we measure the term almost by some positive real value, equal to 0 when the analogy stands true,
and increasing when the four objects are less likely to be in analogy. We also want this value, that we
call analogical dissimilarity (AD), to have good properties with respect to the analogy. We want it
800

fiA NALOGICAL D ISSIMILARITY

to be symmetrical, to stay unchanged when we permute the mean terms of the analogy and finally
to respect some triangle inequality. These requirements will allow us, in section 4, to generalize
a classical fast nearest neighbor search algorithm and to exhibit an algorithmic learning process
which principle is to extract, from a learning set, the 3-tuple of objects that has the least AD when
combined with another unknown object. This lazy learning technique is a therefore a generalization
of the nearest neighbor method.
We firstly study the definition of the analogical dissimilarity on the same structured sets as in
the previous sections, and secondly extend it to sequences.
3.2 A Definition in {0, 1}n
Definition 3.1 (Analogical dissimilarity in {0, 1}) The analogical dissimilarity between four binary values is given by the following table:
u
v
w
x
AD(u, v, w, t)

0
0
0
0
0

0
0
0
1
1

0
0
1
0
1

0
0
1
1
0

0
1
0
0
1

0
1
0
1
0

0
1
1
0
2

0
1
1
1
1

1
0
0
0
1

1
0
0
1
2

1
0
1
0
0

1
0
1
1
1

1
1
0
0
0

1
1
0
1
1

1
1
1
0
1

1
1
1
1
0

In other words, the AD between four binary values is the minimal number of bits that have to be
switched in order to produce an analogical proportion. It can be seen as an extension of the edit
distance in four dimensions which supports the coherence with analogy.
Definition 3.2 (Analogical dissimilarity in {0, 1}n ) The analogical dissimilarity AD(u, v, w, t) between four objects u, v, w and t of a finite set X defined by binary features is the sum of the values
of the analogical dissimilarities between the features.
3.2.1 P ROPERTIES
With this definition, the analogical dissimilarity has the following properties:
Property 3.1 (Properties of AD in {0, 1}n )
Coherence with analogy.
(AD(u, v, w, x) = 0)  u : v :: w : x
Symmetry for as. AD(u, v, w, x) = AD(w, x, u, v)
Exchange of medians. AD(u, v, w, x) = AD(u, w, v, x)
Triangle inequality. AD(u, v, z, t)  AD(u, v, w, x) + AD(w, x, z, t)
Asymmetry for is to. In general: AD(u, v, w, x) 6= AD(v, u, w, x)
The first properties are quite straightforward from the definition. The demonstration of the third one
is simple as well. If the property
AD(fi (u), fi (v), fi (z), fi (t))  AD(fi (u), fi (v), fi (w), fi (x))
+ AD(fi (w), fi (x), fi (z), fi (t))
801

fiM ICLET, BAYOUDH & D ELHAY

t
w
b

x

AD(u, v, w, x) = 2 (t, x)

v
u
Figure 2: Analogical dissimilarity in vector spaces with distance 2 .

holds true for every 6-tuple of elements and every feature fi , then property (4) is true. The demonstration being done by examining all possible cases: it is impossible to find 6 binary features a, b,
c, d, e, f such that AD(a, b, e, f ) = 2 and AD(a, b, c, d) + AD(c, d, e, f ) < 2. More precisely,
if AD(a, b, e, f ) = 2, AD(a, b, c, d) + AD(c, d, e, f ) is also equal to 2 for all the four values that
(c, d) can take.
3.3 Analogical Dissimilarity in Rn
The analogical dissimilarity between four vectors must reflect in some way how far they are from
constructing a parallelogram. Four vectors u, v, w and x are in analogical proportion (i.e., form a
   
 and 
 if and only if 
parallelogram) with opposite sides 
uv
wx
Ou + Ox = Ov + Ow, or equivalently
u + x = v + w, we have chosen the following definition (see Figure 2):
Definition 3.3 (Analogical dissimilarity between vectors) The analogical dissimilarity between
four vectors u, v, w and x of Rn in which is defined the norm k kp and the corresponding distance
p is given by the real positive value AD(u, v, w, x) = p (u + x, v + w) = k(u + x)  (v + w)kp .
It is also equal to p (t, x), where t is the solution of the analogical equation u : v :: w : t.
Property 3.2 (Properties of AD between vectors) This definition of analogical dissimilarity in
Rn guarantees that the following properties hold true: coherence with analogy, symmetry for as,
exchange of medians, triangle inequality and asymmetry for is to.
The first two properties are quite straightforward from the definition. Since k kp is a norm, it
respects the triangle inequality which involves the third property:
AD(u, v, z, t)  AD(u, v, w, x) + AD(w, x, z, t)
3.4 Analogical Dissimilarity between Sequences
We present in the following a definition and two algorithms. Firstly, we extend the notion of analogical dissimilarity to sequences. The first algorithm, called SEQUANA4, computes the analogical
dissimilarity between four sequences of  . The second one, called SOLVANA, given an analogical
equation on sequences, produces the Directed Acyclic Graph (DAG) of all the solutions. If there is
no solution, it gives the DAG of all the sentences that have the least analogical dissimilarity when
associated with the three known sentences of the equation.
802

fiA NALOGICAL D ISSIMILARITY

These two algorithms are quite general, since they make no particular assumption on the alphabet of the sequences. This alphabet  is simply augmented to  =   {} to produce
alignments as described in section 2.5. The analogical dissimilarity on  must be such that:
AD(, , a, a) = 0, and AD(, a, b, c) > 0 for every a, b, c  , but no more constraint is
required.
3.4.1 D EFINITION
Let  be a set on which is defined an analogical dissimilarity AD. We augment it to  by adding
the special symbol . We assume now that there is an analogical dissimilarity AD on  .
Definition 3.4 (Analogical dissimilarity between four sequences) The cost of an alignment between four sequences is the sum of the analogical dissimilarities between the 4-tuples of letters
given by the alignment.
The analogical dissimilarity AD(u, v, w, x) between four sequences in  is the cost of an
alignment of minimal cost of the four sequences.
This definition ensures that the following properties hold true: coherence with analogy, symmetry
for as, exchange of medians and asymmetry for is to3 .
Depending on what are we looking for, many methods have been developed for multiples alignment in bio-informatics (Needleman & Wunsch, 1970; Smith & Waterman, 1981) :
1. For structure or functional similarity like in protein modelization, pattern identification or
structure prediction in DNA, methods using simultaneous alignment like MSA (Wang &
Jiang, 1994) or DCA (Dress, Fllen, & Perrey, 1995), or iterative alignment like MUSCLE
(Edgar, 2004) are the best.
2. For Evolutionary similarity like in phylogenic classification, methods using progressive alignment and tree structure, like ClustalW (Thompson, Higgins, & Gibson, 1994), are the most
fitted.
However, all of these alignment methods (global or local) are heuristic algorithms to overcome the
problem of time and space complexity introduced first by the length of sequences and second by the
number of the sequences to align. In our generation problem neither the sequence length which is
around 30 characters nor the number of sequences to align which is always four in analogy need a
heuristic alignment to speed up the algorithm. But techniques used in bio-informatics to compute
automatically the substitution matrix could be very helpful and interesting in handwritten characters
recognition. Introducing Gap (Gep, Gop) penalties like in DNA or protein sequences should also be
an interesting idea to explore.
3.5 Computing the Analogical Dissimilarity between Four Sequences: the SEQUANA4
Algorithm
We compute AD(u, v, w, x) with a dynamic programming algorithm, called SEQUANA4, that progresses in synchronicity in the four sequences to build an optimal alignment.
3. With this definition of AD, the triangle inequality property is not always true on sequences.

803

fiM ICLET, BAYOUDH & D ELHAY

The input of this algorithm is the augmented alphabet  on which there an analogical dissimilarity AD(a, b, c, d). The output is the analogical dissimilarity between four sentences of  ,
namely AD(u, v, w, x).
We give below the basics formulas of the recurrence. When implementing the computation, one
has to check the correct progression of the indexes i, j, k and l.
Initialisation
Cwu00vx00  0 ;
u

v

0
+ AD(ui , , , ) done ;
for i = 1, |u| do Cwui0vx00  Cwi1
0 x0

u v

u v

for j = 1, |v| do Cw00 xj0  Cw00 xj1
+ AD(, vj , , ) done ;
0
v0
for k = 1, |w| do Cwu0kvx00  Cwu0k1
x0 + AD(, , wk , ) done ;

for l = 1, |x| do Cwu00vx0l  Cwu00vx0l1 + AD(, , , xl ) done ;
Recurrence

uv

Cwik xjl

 u v

C i1 j1 + AD(ui , vj , wk , xl )

 wu k1vxl1

j1


Cwi1
+ AD(ui , vj , wk , )

k1 xl

 ui1 vj1


Cwk xl1 + AD(ui , vj , , xl )


 ui1 vj1


+ AD(ui , vj , , )
Cwk xl



u
v
i
j1


Cwk1 xl1 + AD(, vj , wk , xl )



uv


Cwik xj1
l1 + AD(, vj , , xl )



u
v
i
j1

Cwk1 xl + AD(, vj , wk , )

uv
= M in Cwik xj1
+ AD(, vj , , )
l


ui1 vj

Cwk1 xl1 + AD(ui , , wk , xl )




u
vj

Cwi1

k xl1 + AD(ui , , , xl )



u
vj

Cwi1

k1 xl + AD(ui , , wk , )


u
vj


+ AD(ui , , , )
Cwi1

k xl


u i vj


Cwk1 xl1 + AD(, , wk , xl )



uv


Cwik xjl1 + AD(, , , xl )



 u i vj
Cwk1 xl + AD(, , wk , )



i  i + 1; j  j + 1; k  k + 1; l  l + 1


i  i + 1; j  j + 1; k  k + 1


i  i + 1; j  j + 1; l  l + 1


i  i + 1; j  j + 1


j  j + 1; k  k + 1; l  l + 1


j  j + 1; l  l + 1


i  i + 1; k  k + 1


j j+1


i  i + 1; j  j + 1; l  l + 1


i  i + 1; l  l + 1


i  i + 1; k  k + 1


ii+1


k  k + 1; l  l + 1


l l+1


k k+1

End
When i = |u| and j = |v| and k = |w| and l = |x|.
Result
u v|v|
Cw|u|
is AD(u, v, w, x) in  .
|w| x|x|
Complexity

This algorithms runs in a time complexity in O |u|.|v|.|w|.|x| .
Correctness
The correctness of this algorithm is demonstrated by recurrence, since it uses the dynamic programming principles. It requires only the analogical dissimilarity in  to have the properties that
we have called: coherence with analogy, symmetry for as and exchange of medians. The triangle
inequality property is not necessary.
804

fiA NALOGICAL D ISSIMILARITY

3.6 Generalized Resolution of Analogical Equations in Sequences: the SOLVANA Algorithm
3.6.1 A PPROXIMATE S OLUTIONS TO AN A NALOGICAL E QUATION
Up to now, we have considered that an analogical equation has either one (or several) exact solutions,
or no solution. In the latter case, the concept of analogical dissimilarity is useful to define an
approximate solution.

Definition 3.5 (Best approximate solution to an analogical equation) Let X be a set on which is
defined an analogy and an analogical dissimilarity AD. Let a : b :: c : x be an analogical
equation in X. The set of best approximate solutions to this equation is given by:


	
y : arg min AD(a, b, c, y)
yX

In other words, the best approximate solutions are the objects y  X that are the closest to be
in analogical proportion with a, b and c. Obviously, this definition generalizes that of a solution
to an analogical equation given at section 2.2. Since we have defined AD with good properties on
several alphabets and on sequences on these alphabets, we can compute an approximate solution to
analogical equations in all these domains.
We can easily enlarge this concept and define the set of the k-best solutions to the analogical
equation a : b :: c : x . Informally, it is any subset of k elements of X which have a minimal AD
when associated in fourth position with a, b and c.
In Rn and {0, 1}n , there is only one best approximate solution to an analogical equation, which
can be easily computed (see sections 3.2 and 3.3). Finding the set of the k-best solutions is also a
simple problem.
Let us turn now to an algorithm which finds the set of the best approximate solutions to the
equation u : v :: w : x when the objects are sequences on an alphabet on which an AD has been
defined. We will also make some comments to extend its capacity to find the set of the k-best
solutions.
3.6.2 T HE SOLVANA A LGORITHM
This algorithm uses dynamic programming to construct a 3-dimensional array. When the construction is finished, a backtracking is performed to produce the DAG of all the best solutions.
An alignment of four sequences of different lengths is realized by inserting letters  so that all
the four sequences have the same length. Once this is done, we consider in each column of the
alignment the analogical dissimilarity in the augmented alphabet.
We construct a three dimensional n1  n2  n3 matrix M (respectively the length of the first,
second and third sequences A, B and C of the analogical equation A is to B as C is to x).
To find the fourth sequence, we fill up M with the following recurrence:
805

fiM ICLET, BAYOUDH & D ELHAY

M [i, j, k]
1i,j,kn1 ,n2 ,n3



M [i  1, j  1, k  1] + M in AD(ai , bj , ck , x)


x



M [i, j  1, k  1] + M in AD(, bj , ck , x)



x



M
[i,
j,
k

1]
+
M
in
AD(, , ck , x)


x

M [i, j  1, k] + M in AD(, bj , , x)
= M in
x


 M [i  1, j, k  1] + M in AD(ai , , ck , x)


x



 M [i  1, j  1, k] + M in AD(ai , bj , , x)


x



 M [i  1, j, k] + M in AD(ai , , , x)
x

ai is the

ith

object of the sequence A.  =   {}.

At each step, we save in the cell M [i, j, k] not only the cost but also the letter(s) found by analogical
resolution along the optimal way of progression. When M is completed, a backward propagation
gives us all the optimal generated sequences with the same optimal analogical dissimilarity, strucured as a DAG.
The computational complexity of this algorithm is O(m  n3 ), where m = Card( ) and n is the
maximum length of sequences
3.6.3 E XAMPLE
Let  = {a, b, c, A, B, C} be an alphabet defined by 5 binary features, as follows:

a
b
c
A
B
C


f1
1
0
0
1
0
0
0

f2
0
1
0
0
1
0
0

f3
0
0
1
0
0
1
0

f4
1
1
1
0
0
0
0

f5
0
0
0
1
1
1
0

The first three features indicates what is the letter (for example, f1 is true on a and A only) and the
last two indicate the case of the letter (f4 holds true for lower case letters, f5 for upper case letters).
For example, let ab : Bc :: Bc : x be an analogical equation. There is no exact solution, but six
best approximate solutions y such that AD(ab, Bc, Bc, y) = 4, for example y = BB or y = Cc.
Figure 3 displays the DAG of the results produced by SOLVANA on this example.

4. Analogical Dissimilarity and Machine Learning
4.1 Motivation
We assume here that there exists an analogy defined on the set X and an analogical dissimilarity
AD with the following properties: coherence with analogy, symmetry for as, triangle inequality,
exchange of medians and asymmetry for is to.
Let S be a set of elements of X, which is of cardinality m, and let y be another element of X
with y 6 S. The problem that we tackle in this section is to find the triple of objects (u, v, w) in S
806

fiA NALOGICAL D ISSIMILARITY

Figure 3: Result of SOLVANA: the DAG of all best approximate solutions to an analogical equation
on sequences. Each path displays a different alignment of optimal cost.

such that:
AD(u, v, w, y) = arg min AD(t1 , t2 , t3 , y)
t1 ,t2 ,t3 S

This will directly lead us to use the notion of AD in supervised machine learning, e.g. of a classification rule.
4.2 The Brute Force Solution
An obvious solution is to examine all the triples in S. This brute force method requires m3 calls
to a procedure computing the analogical dissimilarity between four objects of X. According to
the properties of analogical dissimilarity, this number can actually be divided by 8, but it does not
change the theoretical and practical complexity of the search.
The situation is similar to that of the search for the nearest neighbor in Machine Learning, for
which the naive algorithm requires m distance computations. Many proposals have been made to
decrease this complexity (see for example the work of Chvez, Navarro, Baeza-Yates, & Marroqun,
2001). We have chosen to focus on an extension of the AESA algorithm, based on the property
of triangle inequality for distances (Mic, Oncina, & Vidal, 1994). Since we have defined the
concept of analogical dissimilarity with a similar property, it is natural to explore how to extend this
algorithm.
807

fiM ICLET, BAYOUDH & D ELHAY

4.3 FADANA: FAst search of the least Dissimilar ANAlogy
This section describes a fast algorithm to find, given a set of objects S of cardinalty m and an
object y, the three objects (z  , t , x ) in S such that the analogical dissimilarity AD(z  , t , x , y)
is minimal. It is based on the AESA technique, which can be extended to analogical dissimilarity.
Thanks to its properties, an analogical dissimilarity AD(z, t, x, y) can be seen as a distance between
the two couples (z, t) and (x, y), and consequently we will basically work on couples of objects. We
use equivalently in this paragraph the terms (analogical) distance between the two couples (u, v)
and (w, x) and (analogical) dissimilarity between the four elements u, v, w and x to describe
AD(u, v, w, x).
4.3.1 P RELIMINARY C OMPUTATION
In this part, which is done off line, we have to compute the analogical dissimilarity between every
four objects in the data base. This step has a complexity in time and space of O(m4 ), where m
is the size of S. We will come back to this point in section 4.4, where we will progress from an
AESA-like to a LAESA-like technique and reduce the computational complexity.
4.3.2 P RINCIPLE OF THE A LGORITHM
The basic operation is to compose a couple of objects by adding to y an object xi  S where i =
1, m. The goal is now to find the couple of objects in S having the lowest distance with (xi , y), then
to change xi into xi+1 . Looping m times on an AESA-like select and eliminate technique insures to
finally find the triple in S having the lowest analogical dissimilarity when associated with y.
4.3.3 N OTATIONS
Let us denote:
 C the set of couples (u, v) which distance to (xi , y) has already been computed.
  = arg min(AD(z, t, xi , y))
(z,t)U

 i =

arg min (AD(z, t, xi , y))
(z,t)U ,1ji

 Dist = {AD(z, t, xi , y), (z, t)  C}
 Dist(j) the j th element of Dist
 QuadU = {(z, t, xi , y), (z, t)  C}
 QuadU (j) the j th element of QuadU
The algorithm is constructed in the three following phases:
4.3.4 I NITIALIZATION
Each time that xi changes (when i is increased by 1), the set U is refilled with all the possible
couples of objects  S.
808

fiA NALOGICAL D ISSIMILARITY

The set C and Dist which contain respectively the couples and the distances to (xi , y) that have
been measured during one loop, are initialized as empty sets.
The local minimum M in, containing the minimum of analogical dissimilarities of one loop is
set to infinity.
k = Card(C) represents the number of couples where the distance have been computed with
(xi , y) in the current loop. k is initialized to zero.
Algorithm 1 Algorithm FADANA: initialization.
begin
U  {(xi , xj ), i = 1, m and j = 1, m};
C  ;
M in  +;
Dist  ;
k  0;
end

4.3.5 S ELECTION
The goal of this function is to extract from the set U the couple (zz, tt) that is the more promising
in terms of the minimum analogical dissimilarity with (xi , y), using the criterion:
fi
fi
(zz, tt) = arg min M ax fi AD(u, v, z, t)  AD(z, t, xi , y) fi
(u,v)U

(z,t)C

Algorithm 2 Algorithm FADANA: selection of the most promising couple.
selection(U, C, (xi , y), Dist)
begin
s0
for i = 1,P
Card(U) do
if s  PjC |AD(zj , tj , ui , vi )  Dist(j)| then
s  jC |AD(zj , tj , ui , vi )  Dist(j)|;
arg min  i;
end if
end for
Return (uarg min , varg min );
end

4.3.6 E LIMINATION
During this section all the couples (u, v)  U where the analogical distance with (xi , y) can not be
less than what we already found are eliminated thanks to the two criteria below:
AD(u, v, z, t)  AD(z, t, y, xi )    AD(u, v, xi , y)  
and
AD(u, v, z, t)  AD(z, t, y, xi ) +   AD(u, v, xi , y)  
809

fiM ICLET, BAYOUDH & D ELHAY

where  = AD(z  , t , x , y) represents the minimum analogical dissimilarity found until now (see
figure 4). Note that  is updated during the whole algorithm and is never reinitialized when i is
increased.
Algorithm 3 Algorithm FADANA: elimination of the useless couples.
eliminate(U, C, (xi , y), , k)
(zk , tk ) is the k th element of QuadU
begin
for i = 1, Card(U) do
if AD(zk , tk , ui , vi )  Dist(k) +  then
U  U  {(ui , vi )};
C  C  {(ui , vi )};
else if AD(zk , tk , ui , vi )  Dist(k)   then
U  U  {(ui , vi )};
C  C  {(ui , vi )};
end if
end for
end

Algorithm 4 Algorithm FADANA: main procedure.
begin
S  {xi , i = 1, m};
AD  +;
for i = Card(S) do
Initialize;
while U =
6  do
(z, t)  selection(U, C, (xi , y), Dist);
Dist(k)  AD(z, t, xi , y);
k = k + 1;
U  U  {(z, t)};
C  C  {(z, t)};
if Dist(k)  M in then
eliminate(U, C, (xi , y), , k)
else
M in  Dist(k);
if Dist(k) < AD then
AD  Dist(k);
z   z, t  t, x  xi ;
end if
for k = 1, Card(C) do
eliminate(U, C, (xi , y), , k)
end for
end if
end while
end for
The best triple in S is (z  , t , x ) ;
The least analogical dissimilarity is AD = AD(z  , t , x , y) ;
end

810

fiA NALOGICAL D ISSIMILARITY

(u2 , v2 )
(z, t)

(y, xi )

b

b

(u1 , v1 )

b

(z  , t )

(u3 , v3 )
Figure 4: Elimination process in FADANA.

4.4 Selection of Base Prototypes in FADANA
So far, FADANA has the drawback of requiring a precomputing time and storage in O(m4 ), which
is in practice impossible to handle for m > 100.
To go further, we have devised an ameliorated version of the FADANA algorithm, in which the
preliminary computation and storage is limited to N.m2 , where N is a certain number of couples of
objects. The principle is similar to that of LAESA (Mic et al., 1994). N base prototypes couples
are selected among the m2 possibilities through a greedy process, the first one being chosen at
random, the second one being as far as possible from the first one, and so on. The distance between
couples of objects is, according to the definition of the analogical dissimilarity:

 (x, y), (z, t) = AD(z, t, x, y)
4.5 Efficiency of FADANA
We have conducted some experiments to measure the efficiency of FADANA. We have tested this
algorithm on four databases from the UCI Repository (Newman, Hettich, Blake, & Merz, 1998), by
noting each time the percentage of AD computed in-line for different numbers of base prototypes
compared to those made by the naive method (see Figure 5, the scales are logarithmic). The number
of base prototypes is expressed as percentage on the learning set. Obviously, if the learning set
contains m elements, the number of possible 3-tuples that can be built is m3 . This point explains
why the percentage of base prototypes compared to the size of the learning set can rise above 100%.
The number of in-line computations of the AD is the mean over the test set.
We observe in these results that the optimal number of base prototypes is between 10% and 20%
if we aim to optimize the computation time performance.

5. Two Applications in Machine Learning Problems
5.1 Classification of Objects Described by Binary and Nominal Features
The purpose of this first experiment is to measure the benefit of analogical dissimilarity applied to
a basic problem of classification, compared to standard classifiers such k-nearest neighbors, neural
networks, and decision trees. In this benchmarking, we are not yet interested in classifying sequences, but merely to investigate what the basic concept of analogical dissimilarity can bring to
the learning of a classification rule for symbolic objects.
811

fiM ICLET, BAYOUDH & D ELHAY

Figure 5: Efficiency of FADANA w.r.t. the number of base prototypes
5.1.1 M ETHOD D ESCRIPTION


	
Let S = oi , h(oi ) | 1  i  m be a learning set, where h(oi ) is the class of the object oi . The
objects are defined by binary attributes. Let x be an object not in S. The learning problem is to find
the class of a new object x, using the learning set S. To do this, we define a learning rule based on
the concept of analogical dissimilarity depending on an integer k, which could be called the k least
dissimilar 3-tuple rule.
The basic principle is the following: among all the 3-tuples (a, b, c) in S 3 , we consider the subset
of those which produce the least analogical dissimilarity when associated with x (the FADANA
algorithm is used here). For a part of them, the analogical equation h(a) : h(b) :: h(c) : g has an
exact solution in the finite set of the classes. We keep only these 3-tuples and we choose the class
which takes the majority among these values g as the class for x.
More precisely, the procedure is as follows:
1. Compute the analogical dissimilarity between x and all the n 3-tuples in S which produce a
solution for the class of x.
2. Sort these n 3-tuples by the increasing value of their AD when associated with x.
3. If the k-th object has the value p, then let k  be the greatest integer such that the k  -th object
has the same value p.
4. Solve the k  analogical equations on the label of the class. Take the winner of the votes among
the k  results.
To explain, we firstly consider the case where there are only two classes 0 and 1 . An example
with 3 classes will follow.
Point 1 means that we retain only the 3-tuples which have one of the four4 configurations for
their class displayed in Table 1. We ignore the 3-tuples that do not lead to an equation with a trivial
solution on classes:
4. There are actually two more, each one equivalent to one of the four (by exchange of the means objects).

812

fiA NALOGICAL D ISSIMILARITY

h(a)
0
1
1
0

h(b)
0
0
1
1

:
:
:
:
:

::
::
::
::
::

h(c)
0
1
1
0

: h(x)
: ?
: ?
: ?
: ?

resolution
h(x) = 0
h(x) = 1

Table 1: Possible configurations of a 3-tuple
o 1 o 2 o3
b a d
b d e
c d e
a b d
c a e
d c e
d b c
a c e
a c c
a b e
b a e
b c d
c c c
a a c
... ... ...

h(o1 ) h(o2 ) h(o3 )
0
0
1
0
1
2
1
1
2
0
0
1
1
0
2
1
1
2
1
0
1
0
1
2
0
1
1
0
0
2
0
0
2
0
1
1
1
1
1
0
0
1
...
...
...

h(x)
1

2
1

2
0


2
2

1
1
...

AD
0
1
1
1
2
2
2
2
3
3
3
3
4
4
...

k
1
2
3
4
5

6
7
8
9
...

Table 2: An example of classification by analogical dissimilarity. Analogical proportions whose
analogical resolution on classes have no solution (represented by ) are not taken into
account. AD is short for AD(o1 , o2 , o3 , x).

h(a) : h(b) :: h(c) : h(x)
0 : 1 :: 1 : ?
1 : 0 :: 0 : ?
Example
Let S = {(a, 0 ), (b, 0 ), (c, 1 ), (d, 1 ), (e, 2 )} be a set of five labelled objects and let x 6 S
be some object to be classified. According to the analogical proportion axioms, there is only 75
(= (Card(S)3 + Card(S)2 )/2) non-equivalent analogical equations among 125(= Card(S)3 )
equations that can be formed between three objects from S and x. Table (2) shows only the first 14
lines after sorting with regard to some arbitrarily analogical dissimilarity. The following table gives
the classification of an object x according to k:
k
k
classification of x

813

1 2 3 4 5 6 7
1 3 3 5 5 7 7
1 1 1 ? ? 2 2

fiM ICLET, BAYOUDH & D ELHAY

5.1.2 W EIGHTING

THE

ATTRIBUTES

The basic idea in weighting the attributes is that they do not have the same importance in the classification, and that more importance has to be given to the most discriminative. The idea of selecting
or enhancing interesting attributes is classical in Machine Learning, and not quite new in the framework of analogy. In a paper of Turney (2005), a discrimination is done by keeping the most frequent
patterns in words. Therefore, a greater importance is given to the attributes that are actually discriminant. However, in an analogical classification system, there are several ways to find the class of the
unknown element. Let us take again the preceding two class problem example (see table 1) to focus
on this point.
We notice that there are two ways to decide between the class 0 and the class 1 (there is also
a third possible configuration which is equivalent to the second by exchange of the means). We
therefore have to take into account the equation used to find the class. This is why we define a set
of weights for each attribute, depending on the number of classes. These sets are stored in what we
call an analogical weighting matrix.
Definition 5.1 An analogical weighting matrix (W ) is a three dimensional array. The first dimension is for the attributes, the second one is for the class of the first element in an analogical
proportion and the third one is for the class of the last element in an analogical proportion. The
analogical proportion weighting matrix is a d  C  C matrix, where d is the number of attributes
and C is the number of classes.
For a given attribute ak of rank k, the element Wkij of the matrix indicates which weight must
be given to ak when encountered in an analogical proportion on classes whose first element is i ,
and for which j is computed as the solution.
Hence, for the attribute ak :

First element

Last element (decision)
class i class j
class i
Wkii
Wkij
class j
Wkji
Wkjj

Since we only take into account the 3-tuples that give a solution on the class decision, all the possible
situations are of one of the three patterns:
Possible patterns
i : i :: j : j
i : j :: i : j
i : i :: i : i

First
element
i
i
i

Decision
class
j
j
i

This observation gives us a way to compute the values Wkij from the learning set.
5.1.3 L EARNING THE W EIGHTING M ATRIX FROM THE T RAINING S AMPLE
The goal is now to fill the three dimensional analogical weighting matrix using the learning set.
We estimate Wkij by the frequency that the attribute k is in an analogical proportion with the first
element class i , and solves in class j .
Firstly, we tabulate the splitting of every attribute ak on the classes i :
814

fiA NALOGICAL D ISSIMILARITY

ak = 0
ak = 1

. . . class i . . .
...
n0i . . .
...
n1i . . .

where ak is the attribute k and n0i (resp. n1i ) is theP
number
Pof objects in the class i that have the
value 0 (resp. 1) for the binary attribute k. Hence, 1k=0 C
i=1 nki = m (the number of objects
in the training set). Secondly, we compute Wkij by estimating the probability to find a correct
analogical proportion on attribute k with first element class i which solves in class j .
In the following table we show all the possible ways of having an analogical proportion on the
binary attribute k. 0i (resp. 1i ) is the 0 (resp. 1) value of the attribute k that has class i .
1st
2sd
3rd

4th
5th
6th

0i : 0i :: 0j : 0j
0i : 1i :: 0j : 1j
0i : 0i :: 1j : 1j

1i : 1i :: 1j : 1j
1i : 0i :: 1j : 0j
1i : 1i :: 0j : 0j

Pk (1st ) estimates the probability that the first analogical proportion in the table above occurs.
Pk (1st ) = n0i n0i n0j n0j /m4
..
.
From Wkij = Pk (1st ) +    + Pk (6th ), we compute

Wkij = (n20i + n21i )(n20j + n21j ) + 2  n0i n0j n1i n1j /(6  m4 )
The decision algorithm of section 5.1.1 is only modified at point 1, which turns into Weighted
Analogical Proportion Classifier (W AP C):
 Given x, find all the n 3-tuples in S which can produce a solution for the class of x. For every
3-tuple among these n, say (a, b, c), consider the class i of the first element a and the class
j of the solution. Compute the analogical dissimilarity between x and this 3-tuple with the
weighted AD:
d
X
Wkij AD(ak , bk , ck , xk )
AD(a, b, c, x) =
k=1

Otherwise, if point 1 is not modified, the method is called Analogical Proportion Classifier (AP C).
5.1.4 E XPERIMENTS

AND

R ESULTS

We have applied the weighted analogical proportion classifier (W AP C) to eight classical data
bases, with binary and nominal attributes, of the UCI Repository.
MONK 1,2 and 3 Problems (MO.1, MO.2 and MO.3), MONK3 problem has noise added.
SPECT heart data (SP.). Balance-Scale (B.S) and Hayes Roth (H.R) database, both multiclass
database. Breast-W (Br.) and Mushroom (Mu.), both data sets contain missing values. kr-vs-kp
Kasparov vs Karpov (k.k.).
In order to measure the efficiency of W AP C, we have applied some standard classifiers to the
same databases, and we have also applied AP C to point out the contribution of the weighting matrix
(Sect.5.1.2). We give here the parameters used for the comparison method in Table 3:
815

fiM ICLET, BAYOUDH & D ELHAY

 Decision Table: the number of non improving decision tables to consider before abandoning
the search is 5.
 Id3: unpruned decision tree, no missing values allowed.
 Part: partial C4.5 decision tree in each iteration and turns the best leaf into a rule, One-pervalue encoding.
 Multi layer Perceptron: back propagation training, One-per-value encoding, one hidden
layer with (# classes + # attributes)/2 nodes.
 LMT (logistic model trees): classification trees with logistic regression functions at the
leaves, One-per-value encoding.
 IB1: Nearest-neighbor classifier with normalized Euclidean distance, which have better results than IB10.
 JRip: propositional rule learner, Repeated Incremental Pruning to Produce Error Reduction
(RIPPER), optimized version of IREP. .
We have worked with the WEKA package (Witten & Frank, 2005), choosing 6 different classification rules on the same data. Some are well fit to binary data, like ID3, PART, Decision Table.
Others, like IB1 or Multilayer Perceptron, are more adapted to numerical and noisy data.
The results are given in Table 3. We have arbitrarily taken k = 100 for our two rules. The value
k is not very sensitive in the case of nominal and binary data and on small databases such as the ones
that are used in the experiments (see Figure 6). However, it is possible to set k using a validation
set.

Recognition rates in %

100
rs
ut

ut

rs
ut

rs

rs
rs
ut

ut

rs

rs
rs

rs

ut
ut

ut
ut

80

60
100

101

102

103

value of k
Figure 6: Modification in recognition rate subject to k. Full line and dotted line are respectively the
recognition rates on the database breast-w and vote.

We draw the following conclusions from this study: firstly, according to the good classification
rate of W AP C in Br. and Mu. databases, we can say that W AP C handles the missing values
well. Secondly, W AP C seems to belong to the best classifiers for the B.S and H.R databases,
816

fiA NALOGICAL D ISSIMILARITY

Methods
MO.1 MO.2 MO.3 SP. B.S Br. H.R Mu. k.k.
nb. of nominal atts.
7
7
7
22
4
9
4
22
34
nb. of binary atts.
15
15
15
22
4
9
4
22
38
nb. of train instances
124
169 122 80 187 35
66
81
32
nb. of test instances
432
432 432 172 438 664 66 8043 3164
nb. of classes
2
2
2
2
3
2
4
2
2
WAPC (k = 100)
98% 100% 96% 79% 86% 96% 82% 98% 61%
APC (k = 100)
98% 100% 96% 58% 86% 91% 74% 97% 61%
Decision Table
100% 64% 97% 65% 67% 86% 42% 99% 72%
Id3
78% 65% 94% 71% 54%  71%  71%
PART
93% 78% 98% 81% 76% 88% 82% 94% 61%
Multi layer Perceptron 100% 100% 94% 73% 89% 96% 77% 96% 76%
LMT
94% 76% 97% 77% 89% 88% 83% 94% 81%
IB1
79% 74% 83% 80% 62% 96% 56% 98% 71%
IBk (k = 10)
81% 79% 93% 57% 82% 86% 61% 91% 
IB1 (k = 5)
73% 59% 97% 65% 78% 95% 80% 97% 
JRip
75% 62.5% 88% 80% 69% 86% 85% 97% 94%

Table 3: Comparison Table between W AP C and other classical classifiers on eight data sets. Best
classifiers on a database are in bold with a significance level equal to 5%.

which confirms that W AP C deals well with multiclass problems. Thirdly, as shown by the good
classification rate of W AP C in the MO.3 problem, W AP C handles well noisy data. Finally, the
results on MO. and B.S database are exactly the same with the weighted decision rule W AP C than
with AP C. This is due to the fact that all AD that are computed up to k = 100 are of null value.
But on the other data bases, the weighting is quite effective. Unfortunatly, the last database show
that W AP C have a poor recognition rate on some databases, which means that analogy do not fit
all classification problems.
5.2 Handwritten Character Recognition: Generation of New Examples
5.2.1 I NTRODUCTION
In a number of Pattern Recognition systems, the acquisition of labeled data is expensive or user
unfriendly process. For example, when buying a smartphone equipped with a handwritten recognition system, the customer is not likely to write dozens of examples of every letter and digit in order
to provide the system with a consequent learning sample. However, to be efficient, any statistical
classification system has to be retrained to the new personal writing style or the new patterns with
as many examples as possible, or at least a sufficient number of well chosen examples.
To overcome this paradox, and hence to make possible the learning of a classifier with very
few examples, a straightforward idea is to generate new examples by randomly adding noise to the
elements of a small learning sample. In his recent book, Bishop (2007) gives no theoretical coverage
of such a procedure, but rather draws a pragmatic conclusion:  . . . the addition of random noise to
the inputs . . . has been shown to improve generalization in appropriate circumstances.
As far as character recognition is concerned, generating synthetic data for the learning of a
recognition system has mainly be used with offline systems (which process an image of the char817

fiM ICLET, BAYOUDH & D ELHAY

u=
v=
w=
x0 =

9  9 99999  E
1 L  8 9 9 9 9 9 10 E
  9 8 9 9 9 9 9 10 E
1 L  8 9 9 9 9 9 10 E
(a)

1 24L 6 99 9
2 2 4L88 9
2 2 3 3L 8 99
2 2 3 3L 8 88

(b)
Figure 7: (a) Resolution on Freeman direction sequences by AP. (b) the corresponding characters
representation.

acter). For offline character recognition, several image distortions have however been used (Cano,
Prez-Cortes, Arlandis, & Llobet, 2002): slanting, shrinking, ink erosion and ink dilatation. For
online character recognition, several online distortions have been used, such as speed variation and
angular variation (Mouchre, Anquetil, & Ragot, 2007).
We therefore are interested in the quick tuning of a handwritten character recognition to a new
user, and we consider that only a very small set of examples of each character (typically 2 or 3) can
be required from the new user. As we learn a writer-dependent system, the synthetic data have to
keep the same handwriting style as the original data.
5.2.2 A NALOGY BASED G ENERATION
In this second experiment, we are interested in handwritten characters, which are captured online.
They are represented by a sequence of letters of , where  = {1, 2, ..., 16, 0, C, ..., N } is the alphabet of the Freeman symbols code augmented of symbols for anchorage points. These anchorage
points come from an analysis of the stable handwriting properties, as defined in (Mouchre et al.,
2007): pen-up/down, y-extrema, angular points and in-loop y-extrema.
Having a learning set that contains a few examples of each letter, we generate synthetic examples
by analogical proportion as described in section 3.6 (see Figure 7). Hence, by generating artificial
examples of the letter f by analogical proportion using only three instances we augment the learning
set with new and different examples as shown in the following pictures.

| {z }

Original letters

=

|

{z

}

Analogy based generated letters

5.2.3 E XPERIMENTS
In this section we show that our generation strategies improves the recognition rate of three classical
classifiers learned with few data.
Experimental Protocol In the data base that we use (Mouchre et al., 2007), twelve different
writers have written 40 times the 26 lowercase letters (1040 characters) on a PDA. We use a 4-fold
818

fiA NALOGICAL D ISSIMILARITY

stratified cross validation. The experiments are composed of two phases in which three writerdependent recognition systems are learned: a Radial Basis Function Network (RBFN), a K-Nearest
Neighbor (K-NN) and a one-against-all Support Vector Machine (SVM).
Firstly, we compute two Reference recognition Rates without data generation: RR10 which is
the recognition rate achievable with 10 original characters without character generation and RR30
gives an idea of achievable recognition rates with more original data. Practically speaking, in the
context of on the fly learning phase we should not ask the user to input more than 10 characters per
class.
Secondly the artificial character generation strategies are tested. For a given writer, one to ten
characters per class are randomly chosen. Then 300 synthetic characters per class are generated to
make a synthetic learning database. This experiment is done 3 times per cross validation split and
per writer (12 times per user). The mean and the standard deviation of these 12 performance rates
are computed. Finally the means of these measurements are computed to give a writer dependent
mean recognition rate and the associated standard deviation.
We study three different strategies for the generation of synthetic learning databases. The strategy Image Distortions chooses randomly for each generation one among several image distortions. In the same way the strategy Online and Image Distortions chooses randomly one distortion
among the image distortions and online distortions. The Analogy and Distortions strategy generates two-thirds of the base with the previous strategy and the remaining third with AP generation.
Results Figure 8 compares the recognition rates achieved by the three generation strategies for
the three classifiers. Firstly we can note that the global behavior is the same for the three classifiers.
Thus the following conclusions do not depend on the classifier type. Secondly the three generation
strategies are complementary because using Online and Image Distortions is better than Image
Distortions alone and Analogy and Distortions is better than using distortions. Furthermore using only four original character with the complete generation strategy is better than the RR10. The
RR30 is achieved by using 9 or 10 original characters. Thus we can conclude that using our generation strategies learns classifier with very few original data as efficiently as using original data from
a long input phase : we need about three times fewer original data to achieve the same recognition
rate.
Comparing Image Distortions, Online Distortions and Analogy alone shows that Analogy
is less efficient than the ad-hoc methods. Nevertheless, generating sequences by approximate analogical proportion is meaningful and somewhat independant of classical distorsions. In other words,
the analogy of character shapes, which is used in natural intelligence, has been somehow captured
by our definition and algorithms.
Our aim here is to know if the difference of the average of the three methods is significant. We
have performed two methods of validation to evaluate the difference between two stategies. The first
method is parametric: the T-T EST (Gillick & Cox, 1989). The second method is non-parametric:
the S IGN T EST (Hull, 1993). In both methods, the comparaison is between the first and the second
strategy then between the second and the third strategy on each number of original characters.
The T-T EST compares the value of the difference between the two generation methods regarding
to the variation between the differences. The assumption is that the errors are in a normal distribution
and that the errors are independent. If the mean difference is large comparing to the standard
deviation, the two strategies are statistically different. In our case, the probability that our results
are a random artefact is less than 1012 .
819

fiM ICLET, BAYOUDH & D ELHAY

The S IGN T EST is non-parametric comparison method. Its benefit is to avoid assumptions on
the normal distribution of the observations and the errors. This test replaces each difference by the
sign of this difference. The sum of these occurrences is compared to the value of the hypothesis H0
(H0 : the difference between the methods is not significant). Thus if a strategy is frequently better
than the expected mean, then this strategy is significantly better. In our case, the probability that the
hypothesis H0 is true is less than 1030 . Hence, the difference is significantly better.
96

RR30

95

RR30

94

Recognition rate (%)

Recognition rate (%)

90

RR10

92

RBFN

85

RR10

KNN

90

80

88

Image Distortions

Image Distortions
Online and Image Distortions
Distortions and Analogy

Online and Image Distortions

75

86

Distortions and Analogy
2

3

4

5

6

7

Number of original characters

8

9

10

2

3

4

5

6

7

Number of original characters

8

9

10

RR30

Recognition rate (%)

96

94

RR10
92

SVM

90

88

Image Distortions
Online and Image Distortions

86

Distortions and Analogy
2

3

4

5

6

7

8

9

10

Number of original characters

Figure 8: Writer-dependent recognition rates (mean and standard deviation) depending on the number of used original characters compared to reference rates using 10 or 30 characters per
class for RBFN, KNN and SVM classifiers.

6. Conclusions and Future Work
In this article, we have investigated a formal notion of analogy between four objects in the same
universe. We have given definitions of analogy, formulas and algorithms for solving analogical
equations in some particular sets. We have given a special focus on objects structured as sequences,
with an original definition of analogy based on optimal alignments. We also have introduced, in a
coherent manner, the new notion of analogical dissimilarity, which quantifies how far four objects
are from being in analogy. This notion is useful for lazy supervised learning: we have shown
how the time consuming brute force algorithm could be ameliorated by generalizing a fast nearest
neighbor search algorithm, and given a few preliminary experiments. However, much is left to be
done, and we want especially to explore further the following questions:
820

fiA NALOGICAL D ISSIMILARITY

 What sort of data are particularly suited for lazy learning by analogy? We know from the
bibliography that linguistic data have been successfully processed with learning by analogy
techniques, in fields such as grapheme to phoneme transcription, morphology, translation.
We are currently working on experiments on phoneme to grapheme transcription, which can
be useful in some special cases in speech recognition (for proper names, for example). We
also are interested on other sequential real data, such as biosequences, in which the analogical
reasoning technique is (rather unformally) presently already used. The selection of the data
and of the supervision are equally important, since both the search of the less dissemblant
analogic triple and the labelling process are based on the same concept of analogy.
 What sort of structured data can be processed? Sequences can naturally be extended to ordered trees, in which several generalizations of alignments have already been defined. This
could be useful, for example, in extending the nearest neighbor technique in learning prosodic
trees for speech synthesis (Blin & Miclet, 2000). We could also imagine that sequences models, like Hidden Markov Models (HMM) could be combined through an analogical construction.
 What sort of algorithms can be devised to let large amount of data be processed by such
techniques? We have given a first answer with the FADANA algorithm, and we believe that
the quality of the results can be still increased. More experiments remain to be done with this
type of algorithm. We have to notice also that not all the properties of analogical dissimilarity
have been used so far. We believe that an algorithm with a precomputing and a storage in
O(m) can be devised, and we are currently working on it.
In conclusion, we are confident in the fact that the new notion of analogical dissimilarity and the
lazy learning technique that we have associated with it can be extended to more real data, other
structures of data and larger problems.

Acknowledgments
The authors would like to thank the anonymous referrees for their constructive and detailed comments on the first version of this article.

References
Aamodt, A., & Plaza, E. (1994). Case-based reasoning: Foundational issues, methodological variations, and system approaches. Artificial Intelligence Communications, 7(1), 3959.
Basu, M., Bunke, H., & Del Bimbo, A. (Eds.). (2005). Syntactic and Structural Pattern Recognition,
Vol. 27 of Special Section of IEEE Trans. Pattern Analysis and Machine Intelligence. IEEE
Computer Society.
Bayoudh, S., Miclet, L., & Delhay, A. (2007a). Learning by analogy : a classification rule for
binary and nominal data. In Veloso, M. M. (Ed.), International Joint Conference on Artificial
Intelligence, Vol. 20, pp. 678683. AAAI Press.
Bayoudh, S., Mouchre, H., Miclet, L., & Anquetil, E. (2007b). Learning a classifier with very
few examples: analogy based and knowledge based generation of new examples for character
821

fiM ICLET, BAYOUDH & D ELHAY

recognition.. In European Conference on Machine Learning, Vol. 18. Springer Verlag LNAI
4701.
Bishop, C. (2007). Pattern Recognition and Machine Learning. Springer.
Blin, L., & Miclet, L. (2000). Generating synthetic speech prosody with lazy learning in tree structures. In Proceedings of CoNLL-2000 : 4th Conference on Computational Natural Language
Learning, pp. 8790, Lisboa, Portugal.
Bunke, H., & Caelli, T. (Eds.). (2004). Graph Matching in Pattern Recognition and Machine Vision, Special Issue of International Journal of Pattern Recognition and Artificial Intelligence.
World Scientific.
Cano, J., Prez-Cortes, J., Arlandis, J., & Llobet, R. (2002). Training set expansion in handwritten
character recognition.. In 9th Int. Workshop on Structural and Syntactic Pattern Recognition,
pp. 548556.
Chvez, E., Navarro, G., Baeza-Yates, R., & Marroqun, J.-L. (2001). Searching in metric spaces.
ACM Comput. Surv., 33(3), 273321.
Cornujols, A., & Miclet, L. (2002). Apprentissage artificiel : concepts et algorithmes. Eyrolles,
Paris.
Daelemans, W. (1996). Abstraction considered harmful: lazy learning of language processing. In
den Herik, H. J. V., & Weijters, A. (Eds.), Proceedings of the sixth Belgian-Dutch Conference
on Machine Learning, pp. 312, Maastricht, The Nederlands.
Dastani, M., Indurkhya, B., & Scha, R. (2003). Analogical projection in pattern perception. Journal
of Experimental and Theoretical Artificial Intelligence, 15(4).
Delhay, A., & Miclet, L. (2004). Analogical equations in sequences : Definition and resolution.. In
International Colloquium on Grammatical Induction, pp. 127138, Athens, Greece.
Dress, A. W. M., Fllen, G., & Perrey, S. (1995). A divide and conquer approach to multiple
alignment. In ISMB, pp. 107113.
Edgar, R. (2004). Muscle: a multiple sequence alignment method with reduced time and space
complexity. BMC Bioinformatics, 5(1), 113.
Falkenhainer, B., Forbus, K., & Gentner, D. (1989). The structure-mapping engine: Algorithm and
examples. Artificial Intelligence, 41, 163.
Gentner, D., Holyoak, K. J., & Kokinov, B. (2001). The analogical mind: Perspectives from cognitive science. MIT Press.
Gillick, L., & Cox, S. (1989). Some statistical issues in the comparison of speech recognition
algorithms.. In IEEE Conference on Acoustics, Speech and Signal Processing, pp. 532535,
Glasgow, UK.
Hofstadter, D., & the Fluid Analogies Research Group (1994). Fluid Concepts and Creative Analogies. Basic Books, New York.
Holyoak, K. (2005). Analogy. In The Cambridge Handbook of Thinking and Reasoning, chap. 6.
Cambridge University Press.
Hull, D. (1993). Using statistical testing in the evaluation of retrieval experiments. In Research and
Development in Information Retrieval, pp. 329338.
822

fiA NALOGICAL D ISSIMILARITY

Itkonen, E., & Haukioja, J. (1997). A rehabilitation of analogy in syntax (and elsewhere), pp. 131
177. Peter Lang.
Lepage, Y. (2001). Apparatus and method for producing analogically similar word based on pseudodistances between words..
Lepage, Y. (2003). De lanalogie rendant compte de la commutation en linguistique. Universit
Joseph Fourier, Grenoble. Habilitation  diriger les recherches.
Mic, L., Oncina, J., & Vidal, E. (1994). A new version of the nearest-neighbour approximating and
eliminating search algorithm aesa with linear preprocessing-time and memory requirements.
Pattern Recognition Letters, 15, 917.
Mitchell, M. (1993). Analogy-Making as Perception. MIT Press.
Mitchell, T. (1997). Machine Learning. McGraw-Hill.
Mouchre, H., Anquetil, E., & Ragot, N. (2007). Writer style adaptation in on-line handwriting
recognizers by a fuzzy mechanism approach: The adapt method. Int. Journal of Pattern
Recognition and Artificial Intelligence, 21(1), 99116.
Needleman, S. B., & Wunsch, C. D. (1970). A general method applicable to the search for similarities in the amino acid sequence of two proteins.. J Mol Biol, 48(3), 443453.
Newman, D., Hettich, S., Blake, C., & Merz, C. (1998). UCI repository of machine learning
databases..
Pirrelli, V., & Yvon, F. (1999). Analogy in the lexicon: a probe into analogy-based machine learning
of language. In Proceedings of the 6th International Symposium on Human Communication,
Santiago de Cuba, Cuba.
Schmid, U., Gust, H., Khnberger, K.-U., & Burghardt, J. (2003). An algebraic framework for
solving proportional and predictive analogies. In F. Schmalhofer, R. Y., & Katz, G. (Eds.),
Proceedings of the European Conference on Cognitive Science (EuroCogSci 2003), pp. 295
300, Osnabrck, Germany. Lawrence Erlbaum.
Smith, T. F., & Waterman, M. S. (1981). Identification of common molecular subsequences. Journal
of Molecular Biology, 147, 195197.
Stroppa, N., & Yvon, F. (2004). Analogie dans les squences : un solveur  tats finis. In TALN
2004.
Thompson, J. D., Higgins, D. G., & Gibson, T. J. (1994). Improved sensitivity of profile searches
through the use of sequence weights and gap excision. Computer Applications in the Biosciences, 10(1), 1929.
Turney, P. D. (2005). Measuring semantic similarity by latent relational analysis. Proceedings
Nineteenth International Joint Conference on Artificial Intelligence (IJCAI-05), 05, 1136.
Wang, L., & Jiang, T. (1994). On the complexity of multiple sequence alignment. Journal of
Computational Biology, 1(4), 337348.
Witten, I. H., & Frank, E. (2005). Data Mining: Practical machine learning tools and techniques,
2nd Edition. Morgan Kaufmann Publishers.
823

fiM ICLET, BAYOUDH & D ELHAY

Yvon, F. (1997). Paradigmatic cascades: a linguistically sound model of pronunciation by analogy.
In Proceedings of the 35th annual meeting of the Association for Computational Linguistics
(ACL), Madrid, Spain.
Yvon, F. (1999). Pronouncing unknown words using multi-dimensional analogies. In Proceeding
of the European conference on Speech Application and Technology (Eurospeech), Vol. 1, pp.
199202, Budapest, Hungary.
Yvon, F., Stroppa, N., Delhay, A., & Miclet, L. (2004). Solving analogical equations on words.
Tech. rep. ENST2004D005, cole Nationale Suprieure des Tlcommunications.

824

fiJournal of Artificial Intelligence Research 32 (2008)

Submitted 11/07; published 07/08

M-DPOP: Faithful Distributed Implementation of
Efficient Social Choice Problems
Adrian Petcu
Boi Faltings

ADRIAN . PETCU @ EPFL . CH
BOI . FALTINGS @ EPFL . CH

Artificial Intelligence Lab, Ecole Polytechnique Federale de Lausanne,
Station 14, 1015 Lausanne, Switzerland

David C. Parkes

PARKES @ EECS . HARVARD . EDU

School of Engineering and Applied Sciences, Harvard University
33 Oxford Street, Cambridge, MA 02138 USA

Abstract
In the efficient social choice problem, the goal is to assign values, subject to side constraints, to
a set of variables to maximize the total utility across a population of agents, where each agent has
private information about its utility function. In this paper we model the social choice problem as
a distributed constraint optimization problem (DCOP), in which each agent can communicate with
other agents that share an interest in one or more variables. Whereas existing DCOP algorithms
can be easily manipulated by an agent, either by misreporting private information or deviating from
the algorithm, we introduce M-DPOP, the first DCOP algorithm that provides a faithful distributed
implementation for efficient social choice. This provides a concrete example of how the methods of
mechanism design can be unified with those of distributed optimization. Faithfulness ensures that
no agent can benefit by unilaterally deviating from any aspect of the protocol, neither informationrevelation, computation, nor communication, and whatever the private information of other agents.
We allow for payments by agents to a central bank, which is the only central authority that we
require. To achieve faithfulness, we carefully integrate the Vickrey-Clarke-Groves (VCG) mechanism with the DPOP algorithm, such that each agent is only asked to perform computation, report
information, and send messages that is in its own best interest. Determining agent is payment
requires solving the social choice problem without agent i. Here, we present a method to reuse
computation performed in solving the main problem in a way that is robust against manipulation
by the excluded agent. Experimental results on structured problems show that as much as 87% of
the computation required for solving the marginal problems can be avoided by re-use, providing
very good scalability in the number of agents. On unstructured problems, we observe a sensitivity
of M-DPOP to the density of the problem, and we show that reusability decreases from almost
100% for very sparse problems to around 20% for highly connected problems. We close with a discussion of the features of DCOP that enable faithful implementations in this problem, the challenge
of reusing computation from the main problem to marginal problems in other algorithms such as
ADOPT and OptAPO, and the prospect of methods to avoid the welfare loss that can occur because
of the transfer of payments to the bank.

1. Introduction
Distributed optimization problems can model environments where a set of agents must agree on a
set of decisions subject to side constraints. We consider settings in which each agent has its own
preferences on subsets of these decisions. The agents are self interested, and each one would like to
obtain the decision that maximizes its own utility. However, the system as whole agrees (or some
social designer determines) that a solution should be selected to maximize the total utility across all
c
2008
AI Access Foundation. All rights reserved.

fiP ETCU , FALTINGS , & PARKES

agents. Thus, this is a problem of efficient social choice. As motivation, we have in mind massively
distributed problems such as meeting scheduling, where the decisions are about when and where
to hold each meeting, or allocating airport landing slots to airlines, where the decisions are what
airline is allocated what slot, or scheduling contractors in construction projects.
One approach to solve such problems is with a central authority that computes the optimal solution. In combination with an incentive mechanism such as the Vickrey-Clarke-Groves (VCG)
mechanism (Jackson, 2000), we can also prevent manipulation through the misreporting of preferences. However, in many practical settings it is hard to bound the problem so that such a central
authority is feasible. Consider meeting scheduling: while each agent only participates in a few
meetings, it is in general not possible to find a set of meetings that has no further constraints with
any other meetings and thus can be optimized separately. Similarly, contractors in a construction
project simultaneously work on other projects, again creating an web of dependencies that is hard to
optimize in a centralized fashion. Privacy concerns also favor decentralized solutions (Greenstadt,
Pearce, & Tambe, 2006).
Algorithms for distributed constraint reasoning, such as ABT and AWC (Yokoo & Hirayama,
2000), AAS (Silaghi, Sam-Haroud, & Faltings, 2000), DPOP (Petcu & Faltings, 2005b) and
ADOPT (Modi, Shen, Tambe, & Yokoo, 2005), can deal with large problems as long as the influence of each agent on the solution is limited to a bounded number of variables. However, the
current techniques assume cooperative agents, and do not provide robustness against misreports of
preferences of deviations from the algorithm by self-interested agents. This is a major limitation. In
recent years, faithful distributed implementation (Parkes & Shneidman, 2004) has been proposed as
a framework within which to achieve a synthesis of the methods of (centralized) MD with distributed
problem solving. Faithfulness ensures that no agent can benefit by unilaterally deviating from any
aspect of the protocol, neither information-revelation, computation, nor communication, and whatever the private information of other agents. Until now, distributed implementation has been applied
to lowest-cost routing (Shneidman & Parkes, 2004; Feigenbaum, Papadimitriou, Sami, & Shenker,
2002), and policy-based routing (Feigenbaum, Ramachandran, & Schapira, 2006), on the Internet,
but not to efficient social choice, a problem with broad applicability.
In this paper, we make the following contributions:
 We show how to model the problem of efficient social choice as a DCOP, and adapt the
DPOP algorithm to exploit the local structure of the distributed model and achieve the same
scalability as would be possible in solving the problem on a centralized problem graph.
 We provide an algorithm whose first stage is to faithfully generate the DCOP representation
from the underlying social choice problem. Once the DCOP representation is generated, the
next stages of our M-DPOP algorithm are also faithful, and form an ex post Nash equilibrium
of the induced non-cooperative game.
 In establishing that DCOP models of social choice problems can be solved faithfully, we
observe that the communication and information structure in the problem are such that no
agent can prevent the rest of the system, in aggregate, from correctly determining the marginal
impact that allowing for the agents (reported) preferences has on the total utility achieved by
the other agents. This provides the generality of our techniques to other DCOP algorithms.
 Part of achieving faithfulness requires solving the DCOP with each agents (reported) preferences ignored in turn, and doing so without this agent able to interfere with this computational
706

fiM-DPOP: FAITHFUL D ISTRIBUTED I MPLEMENTATION OF E FFICIENT S OCIAL C HOICE P ROBLEMS

process. We provide an algorithm with this robustness property, that is nevertheless able to
reuse, where possible, intermediate results of computation from solving the main problem
with all agents.
 In experimental analysis, on structured meeting scheduling problems that are a common
benchmark in the literature, we demonstrate that as much as 87% of the computation required
for solving the marginal problems can be avoided through reuse. Results are also provided for
unstructured resource allocation problems 1 , and show M-DPOP to be sensitive to problem
density: for loose problems, up to around 80% of the computation can be reused, and this
decreases for highly connected problems.
The M-DPOP algorithm defines a strategy for each agent in the extensive-form game induced by
the DCOP for efficient social choice. In particular, the M-DPOP algorithm defines the messages that
an agent should send, and the computation that an agent should perform, in response to messages
received from other agents. In proving that M-DPOP forms a game-theoretic equilibrium, we show
that no agent can benefit by unilaterally deviating, whatever the utility functions of other agents and
whatever the constraints. Although not as robust as a dominant strategy equilibrium, because this (ex
post) equilibrium requires every other agent to follow the algorithm, Parkes and Shneidman (2004)
have earlier commented that this appears to be the necessary cost of decentralization.
The total payment made by each agent to the bank is always non-negative and M-DPOP never
runs at a deficit (i.e. the bank always receives a non-negative net payment from the agents). In some
settings, this transfer of utility to the bank is undesirable and would be best avoided. We provide
some statistics for the problem domains studied that show that this loss can represent as much as
35% of the total utility achieved from the solution in some problems studied. While the payments
cannot be naively redistributed back to agents without breaking faithfulness, extant work on redistribution mechanisms for VCG payments suggests that this can be mitigated (Guo & Conitzer, 2007;
Faltings, 2004; Cavallo, 2006; Moulin, 2007; Bailey, 1997). We defer this extension to M-DPOP,
the details of which are surprisingly involved and interesting in their own right, to future work.
The reuse of computation, in solving the marginal problems with each agent removed in turn, is
especially important in settings of distributed optimization because motivating scenarios are those
for which the problem size is massive, perhaps spanning multiple organizations and encompassing
thousands of decisions. For example, consider project scheduling, inter-firm logistics, intra-firm
meeting scheduling, etc. With appropriate problem structure, DCOP algorithms in these problems
can scale linearly in the size of the problem. For instance, DPOP is able to solve such problems
through a single back-and-forth traversal over the problem graph. But without re-use the additional
cost of solving each marginal problem would make the computational cost quadratic rather than
linear in the number of agents, which could be untenable in such massive-scale applications.
The rest of this paper is organized as follows: after preliminaries (Section 2), in Section 3 we
describe the DPOP (Petcu & Faltings, 2005b) algorithm for distributed constraint optimization,
which is the focus of our study. Section 4 introduces our model of self-interested agents and defines
the (centralized) VCG mechanism. Section 4.4 provides a simple method, Simple M-DPOP to make
DPOP faithful and serves to illustrate the excellent fit between the information and communication
structure of DCOPs and faithful VCG mechanisms. In Section 5 we describe our main algorithm, MDPOP, in which computation is re-used in solving the marginal problems with each agent removed
1. We consider distributed combinatorial auctions, with instances randomly generated using a distribution in the CATS
problem suite (Leyton-Brown & Shoham, 2006).

707

fiP ETCU , FALTINGS , & PARKES

in turn. We present experimental results in Section 6. In Section 7 we discuss adapting other DCOP
algorithms for social choice (ADOPT and OptAPO, see Section 7.2), and about the waste due to
payments in Section 7.3. We conclude in Section 8.
1.1 Related Work
This work draws on two research areas: distributed algorithms for constraint satisfaction and optimization, and mechanism design for coordinated decision making in multi-agent systems with
self-interested agents. We briefly overview the most relevant results in these areas.
1.1.1 C ONSTRAINT S ATISFACTION

AND

O PTIMIZATION

Constraint satisfaction and optimization are powerful paradigms that can model a wide range of
tasks like scheduling, planning, optimal process control, etc. Traditionally, such problems were
gathered into a single place, and a centralized algorithm was applied to find a solution. However,
social choice problems are naturally distributed, and often preclude the use of a centralized entity to
gather information and compute solutions.
The Distributed Constraint Satisfaction (DisCSP) (Yokoo, Durfee, Ishida, & Kuwabara, 1992;
Sycara, Roth, Sadeh-Koniecpol, & Fox, 1991; Collin, Dechter, & Katz, 1991, 1999; Solotorevsky,
Gudes, & Meisels, 1996) and the Distributed Constraint Optimization (DCOP) (Modi et al., 2005;
Zhang & Wittenburg, 2003; Petcu & Faltings, 2005b; Gershman, Meisels, & Zivan, 2006) formalisms were introduced to enable distributed solutions. The agents involved in such problems
must communicate with each other to find a solution to the overall problem (unknown to any one
of them). Briefly, these problems consist of individual subproblems (each agent holds its own subproblem), which are connected with (some of) its peers subproblems via constraints that limit what
each individual agent can do. The goal is to find feasible solutions to the overall problem (in the
case of DisCSP), or optimal ones in the case of DCOP.
Many distributed algorithms for DCOP have been introduced, none of which deals with selfinterested agents. The most well known ones are ADOPT, DPOP and OptAPO:
 ADOPT (Modi et al., 2005) is a backtracking based, bound propagation algorithm. ADOPT
is completely decentralized and message passing is asynchronous. While ADOPT has the
advantage of requiring linear memory, and linear-size messages, its applicability for large
problems 2 is questionable due to the fact that it produces a number of messages which is
exponential in the depth of the DFS tree chosen.
 OptAPO (Mailler & Lesser, 2005) is a centralized-distributed hybrid that uses mediator nodes
to centralize subproblems and solve them in dynamic and asynchronous mediation sessions.
The authors show that its message complexity is significantly smaller than ADOPTs. However, it is designed for cooperative settings, and in settings with self-interested agents like the
social choice problem, it is unclear whether agents would agree revealing their constraints
and utility functions to (possibly many) other agents, such that they can solve the partially
centralized subproblems.
 DPOP (Petcu & Faltings, 2005b) is a complete algorithm based on dynamic programming
which generates only a linear number of messages. In DPOP, the size of the messages depends
2. The largest ADOPT experiments that we are aware of comprise problems with around 20 agents and 40 variables.

708

fiM-DPOP: FAITHFUL D ISTRIBUTED I MPLEMENTATION OF E FFICIENT S OCIAL C HOICE P ROBLEMS

on the structure of the problem: the largest message is exponential in the induced width of
the problem (see Section 3.1.4) As with ADOPT, DPOP maintains the full distribution of the
problem. These features suggest that DPOP is a good foundation for an efficient distributed
implementation of a VCG-based mechanism for social choice problems.
A further discussion about the features of these algorithms and their applicability to social choice
problems is provided in Section 7. In this paper, we will focus on DPOP and provide appropriate
modifications and payments so that it can be effective for environments with self-interested agents.
In Section 7.2 we will also provide a brief discussion about the opportunites and challenges in
applying our methodology to ADOPT and OptAPO.
1.1.2 M ECHANISM D ESIGN

AND

D ISTRIBUTED I MPLEMENTATION

There is a long tradition of using centralized incentive mechanisms within Distributed AI, going
back at least to the work of Ephrati and Rosenschein (1991) who considered the use of the VCG
mechanism to compute joint plans; see also the work of Sandholm (1996) and Parkes et al. (2001)
for more recent discussions. Also noteworthy is the work of Rosenschein and Zlotkin (1994, 1996)
on rules of encounter, which provided non-VCG based approaches for task allocation in systems
with two agents.
On the other hand, there are very few known methods for distributed problem solving in the
presence of self-interested agents. For example, while T RACO N ET (Sandholm, 1993) improved
upon the C ONTRACT N ET system (Davis & Smith, 1983) of negotiation-based, distributed task reallocation, by providing better economic realism, T RACO N ET was nevertheless studied for simple,
myopically-rational agent behaviors and its performance with game-theoretic agents was never analyzed; this remains true for more recent works (Endriss, Maudet, Sadri, & Toni, 2006; Dunne,
Wooldridge, & Laurence, 2005; Dunne, 2005). Similarly, Wellmans work on market-oriented programming (Wellman, 1993, 1996) considers the role of virtual markets in the support of optimal
resource allocation, but is developed for a model of price-taking agents (i.e. agents that treat
current prices as though they are final), rather than game-theoretic agents.
The first step in providing a more satisfactory synthesis of distributed algorithms with MD was
provided by the agenda of distributed algorithmic mechanism design (DAMD), due to the work
of Feigenbaum and colleagues (Feigenbaum et al., 2002; Feigenbaum & Shenker, 2002). These
authors (FPSS) provided an efficient algorithm for lowest-cost interdomain routing on the Internet,
terminating with optimal routes and the payments of the VCG mechanism. The up-shot was that
agents in this case autonomous systems running network domains could not benefit by misreporting information about their own transit costs. But missing from this analysis was any consideration
about the robustness of the algorithm itself to manipulation. Distributed implementation (Parkes
& Shneidman, 2004) introduces this additional requirement. An algorithm is faithful if an agent
cannot benefit by deviating from any of its required actions, including information-revelation, computation and message passing. A number of principles for achieving faithfulness in an ex post Nash
equilibrium are provided by Parkes and Shneidman (2004). By careful incentive design and a small
amount of cryptography they are able to remove the remaining opportunities for manipulation from
the lowest-cost routing algorithm of FPSS. Building on this, Feigenbaum et al. (2006) recently provide a faithful method for policy-based interdomain routing, better capturing the typical business
agreements between Internet domains.
709

fiP ETCU , FALTINGS , & PARKES

Ours is the first work to achieve faithfulness for general DCOP algorithms, demonstrated here
via application to efficient social choice. In other work, Monderer and Tennenholtz (1999) consider a distributed single item allocation problem, but focus on (faithful) communication and do
not provide distributed computation. Izmalkov et al. (2005) adopt cryptographic primitives such
as ballot boxes to show how to convert any centralized mechanisms into a DI on a fully connected
communication graph. There interest is in demonstrating the theoretical possibility of ideal mechanism design without a trusted center. Our work has a very different focus: we seek computational
tractability, do not require fully connected communication graphs, and make no appeal to cryptographic primitives. On the other hand, we are content to retain desired behavior in some equilibrium
(remaining consistent with the MD literature) while Izmalkov et al. avoid the introduction of any
additional equilibria beyond those that exist in a centralized mechanism.
We briefly mention two other related topics. Of note is the well established literature on iterative
VCG mechaisms (Mishra & Parkes, 2007; Ausubel, Cramton, & Milgrom, 2006; Bikhchandani,
de Vries, Schummer, & Vohra, 2002). These provide a partially distributed implementation for
combinatorial allocation problems, with the center typically issuing demand queries of agents
via prices, these prices triggering computation on the part of agents in generating a demand set in
response. These auctions can often be interpreted as decentralized primal-dual algorithms (Parkes
& Ungar, 2000; de Vries & Vohra, 2003). The setting differs from ours in that there remains a center
that performs computation, solving a winner determination problem in each round, and each agent
communicates directly with the center and not peer-to-peer. Mualem (2005) initiates an orthogonal
direction within computer science related to the topic of Nash implementation (Jackson, 2001) in
economics, but her approach relies on information that is part private and part common knowledge,
so that no one agent has entirely private information about its preferences.

2. Preliminaries: Modeling Social Choice
We assume that the social choice problem consists of a finite but possibly large number of decisions
that all have to be made at the same time. Each decision is modeled as a variable that can take
values in a discrete and finite domain. Each agent has private information about the variables on
which it places relations. Each relation associated with an agent defines the utility of that agent for
each possible assignment of values to the variables in the domain of the relation. There may also be
hard constraints that restrict the space of feasible joint assignments to subsets of variables.
Definition 1 (Social Choice Problem - SCP) An efficient social choice problem can be modeled as
a tuple < A, X , D, C, R > such that:
 X = {X1 , ..., Xm } is the set of public decision variables (e.g. when and where to hold
meetings, to whom should resources be allocated, etc);
 D = {d1 , ..., dm } is the set of finite public domains of the variables X (e.g. list of possible
time slots or venues, list of agents eligible to receive a resource, etc);
 C = {c1 , ..., cq } is a set of public constraints that specify the feasible combinations of values
of the variables involved. A constraint cj is a function cj : dj1  ..  djk  {, 0}
that returns 0 for all allowed combinations of values of the involved variables, and  for
disallowed ones. We denote by scope(cj ) the set of variables associated with constraint cj ;
710

fiM-DPOP: FAITHFUL D ISTRIBUTED I MPLEMENTATION OF E FFICIENT S OCIAL C HOICE P ROBLEMS

 A = {A1 , ..., An } is a set of self-interested agents involved in the optimization problem;
X(Ai )  X is a (privately known) 3 set of variables in which agent Ai is interested and
on which it has relations.
 R = {R1 , ..., Rn } is a set of private relations, where Ri is the set of relations specified by
agent Ai and relation rij  Ri is a function rij : dj1  ..  djk  R specified by agent
Ai , which denotes the utility Ai receives for all possible values on the involved variables
{j1 , . . . , jk } (negative values mean costs). We denote by scope(rij ) the domain of variables
that rij is defined on.
The private relations of each agent may, themselves, be induced by the solution to local optimization problems on additional, private decision variables and with additional, private constraints.
These are kept local to an agent and not part of the SCP definition.
The optimal solution to the SCP is a complete instantiation X  of all variables in X , s.t.
X
X
Ri (X) +
cj (X),
(1)
X   arg max
XD

i{1,..,n}

cj C

rj (X) is agent Ai s total utility for assignment X. This is the natural
where Ri (X) =
rij Ri i
problem of social choice: the goal is to find a solution that maximizes the total utility of all agents,
while respecting hard constraints; notice that the second sum is  if X is infeasible and precludes
this outcome. We assume throughout that there is a feasible solution. In introducing the VCG
mechanism we will require the solution to the SCP with the influence of each agents relations
removed in turn. For this, let SCP (A) denote the main problem
in Eq. (1)
Pand SCP (Ai ) denote
P
the marginal problem without agent Ai , i.e. maxXD j6=i Rj (X) + cj C cj (X). Note that
all decision variables remain. The only difference between SCP (A) and SCP (Ai ) is that the
preferences of agent Ai are ignored in solving SCP (Ai ).
For variable Xj , refer to the agents Ai for which Xj  X(Ai ) as forming the community for
Xj . We choose to emphasize the following assumptions:
P

 Each agent knows the variables in which it is interested, together with the domain of any such
variable and the hard constraints that involve the variable.
 Each decision variable is supported by a community mechanism that allows all interested
agents to report their interest and learn about each other. For example, such a mechanism can
be implemented using a bulletin board.
 For each constraint cj  C, every agent Ak in a community Xl  scope(cj ), i.e. with
Xl  X(Ak ), can read the membership lists of all other communities Xm  scope(cj ) for
Xm 6= Xl . In other words, every agent involved in a hard constraint knows about all other
agents involved in that hard constraint.
 Each agent can communicate directly with all agents in all communities in which it is a
member, and with all other agents involved in the same shared hard constraints. No other
communication between agents is required.
3. Note that the private knowledge of variables of interest is not a requirement; the algorithms we present work with
both public and private knowledge of variables of interest. What is required is that agents interested in the same
variable know about each other - see assumptions below.

711

fiP ETCU , FALTINGS , & PARKES

Figure 1: An operator placement problem: (a) A centralized model (each variable is a server load possible
values are feasible combinations of services to be run by each server , and the edges correspond
to relations and represent agent preferences). (b) A decentralized (DCOP) model with replicated
variables. Each agent has a local replica of variables of interest and inter-agent edges denote
equality constraints that ensure agreement. The preferences modeled by relations are now hyperedges local to the respective agents.

In Section 4 we will establish that the step of identifying the SCP, via the community mechanism, is itself faithful so that self-interested agents will choose to volunteer the communities of
which they are a member (and only those communities.)
2.1 Modeling Social Choice as Constraint Optimization
We first introduce a centralized, constraint optimization problem (COP) model of the efficient social choice problem. This model is represented as a centralized problem graph. Given this, we
then model this as a distributed constraint optimization problem (DCOP), along with an associated
distributed problem graph. The distributed problem graph makes explicit the control structure of
the distributed algorithm that is ultimately used by the multi-agent system to solve the problem.
Both sections are illustrated by reference to an overlay network optimization problem (Huebsch,
Hellerstein, Lanham, et al., 2003; Faltings, Parkes, Petcu, & Shneidman, 2006; Pietzuch, Ledlie,
Shneidman, Roussopoulos, Welsh, & Seltzer, 2006):
OVERLAY N ETWORK O PTIMIZATION Consider the problem of optimal placement of data aggregation and processing operators on an overlay network such as a large-scale sensor network (Huebsch
et al., 2003; Pietzuch et al., 2006). In this application, there are multiple users and multiple servers.
Each user is associated with a query and has a client machine located at a particular node on an
overlay network. A query has an associated set of data producers, known to the user and located
712

fiM-DPOP: FAITHFUL D ISTRIBUTED I MPLEMENTATION OF E FFICIENT S OCIAL C HOICE P ROBLEMS

at nodes on the network. Each query also requires a set of data aggregation and processing operators, which should be placed on server nodes between the nodes with data producers and the
users node. Each user assigns a utility to different assignments of operators to servers to represent her preferences for different kinds of data aggregation. Examples of in-network operators for
data aggregation include database style join operators; e.g., a user may desire volcano data X
and earthquake data Y joined and sent to them. To address this, a specific operator that we call
VolcanoXEarthquakeY Join is created and put into the network. Naturally, each user prefers to
have their operators placed on the best servers in the network, without regard to the costs incurred, overloading servers, denying service to other users, etc. The problem is to find the optimal
allocation of operators to servers, subject to capacity and compatibility constraints.
Faltings et al. (2006) model this problem as one of efficient social choice. A distributed algorithm, to be executed by user clients situated on network nodes, is used to determine the assignment
of data aggregation and processing operators to server nodes.
2.1.1 A C ENTRALIZED COP M ODEL AS A M ULTI G RAPH
Viewed as a centralized problem, the SCP can be defined as a constraint optimization problem on a
multigraph, i.e. a graph in which several distinct edges can connect the same set of nodes. We denote
this COP (A), and provide an illustration in Figure 1(a). The decision variables are the nodes, and
relations defined over subsets of the variables form edges of the multigraph; hyperedges that connect
more than two vertices at once in the case of a relation involving more than two variables. There
can be multiple edges that involve the same set of variables, with each edge corresponding to the
relations of a distinct agent on the same set of variables. The hard constraints are also be represented
as edges on the graph.
Example 1 (Centralized Model for Overlay Optimization) The example in Figure 1(a) contains
3 users Ai and 3 servers Sj . For simplicity reasons, assume that each user Ai has one single operator oi that they want to have executed on some server. According to prerequisites and compatibility
issues, assume that S1 can execute both o1 and o2 , but not o3 . Similarly, assume that S2 can execute
both o2 and o3 , but not o1 , and S3 can execute any combination of at most two out of the three operators. Agents have preferences about where their operators are executed (e.g. because of proximity
to data sources, computational capabilities of the servers, cost of electricity, etc). For example, A1
extracts utility 10 when o1 is executed by S1 , and utility 5 when o1 is executed by S3 .
To model the problem as an optimization problem, we use the following:
1. variables: for each server Si , we create a variable Si that denotes the set of operators that Si
will execute.
2. values: each variable Si can take values from the set of all possible combinations of operators
the server can execute. For example, S1 = {null, o1 , o2 , o1 + o2 }, where null means the
server executes no operator, oi that it executes operator oi , and o1 + o2 that it executes both
o1 and o2 .
3. constraints: restrict the possible combinations of assignments. Example: no two servers
should execute the same operator.
4. relations: allow agents to express preferences about combinations of assignments. A1 models
its preference for the placement of o1 by using the relation r10 , defined over the variables S1
713

fiP ETCU , FALTINGS , & PARKES

and S3 . This relation associates an utility value to each combination of assignments to S1
and S3 (in total 4  8 = 32 combinations) as follows:
 0 to all combinations where o1 is executed neither on S1 , or on S3 (e.g. hS1 = o2 , S3 =
o3 i)
 10 to all combinations where o1 is executed only on S1 (e.g. hS1 = o1 , S3 = o2 + o3 i)
 5 to all combinations where o1 is executed only on S3 (e.g. hS1 = o2 , S3 = o1 i)
We depict variables as nodes in a graph, and constraints and relations as (hyper)edges (see
Figure 1(a)). The problem can get arbitrarily complex, with multiple operators per agent, groups of
servers being able to execute only certain groups of compatible operators, etc.
2.1.2 A D ECENTRALIZED COP (DCOP) M ODEL U SING R EPLICATED VARIABLES
It is useful to define an alternate graphical representation of the SCP, with the centralized problem
graph replaced with a distributed problem graph. This distributed problem graph has a direct correspondence with the DPOP algorithm for solving DCOPs. We denote by DCOP (A) the problem
with all agents included, which corresponds to the main social choice problem, SCP (A). Similarly,
DCOP (Ai ) is the problem with agent Ai removed, which corresponds to SCP (Ai ). In our
distributed model, each agent has a local replica of the variables in which it is interested.4 For each
public variable, Xv  X(Ai ), in which agent Ai is interested, the agent has a local replica, denoted
Xvi . Agent Ai then models its local problem COP (X(Ai ), Ri ), by specifying its relations rij  Ri
on the locally replicated variables.
Refer to Figure 1(b) for the translation of the centralized problem from Figure 1(a) into a DCOP
model. Each agent has as local variables the loads of the servers that are of interest to itself, i.e.
servers that can execute one of its operators (e.g. S12 represents A2 s local replica of the variable
representing server S1 ). Local edges correspond to local all-different constraints between an agents
variables and ensure that it does not execute its operator on several servers at the same time. Equality
constraints between local replicas of the same value ensure global agreement about what operators
will run on which servers.
Agents specify their relations via local edges on local replicas. For example, agent A1 with
its relation on the load of servers S1 and S3 can now express a preference for the placement of its
operator o1 with relation r10 , which can assign e.g. utility 5 to S3 executing o1 , and utility 10 to S1
executing o1 .
We can begin to understand the potential for manipulation by self-interested agents through this
example. Notice that although the globally optimal solution may require assigning o1 to S3 , this is
less preferable to A1 , providing utility 5 instead of 10. Therefore, in the absence of an incentive
mechanism, A1 could benefit from a simple manipulation: declare utility + for hS1 = o1 i, thus
changing the final assignment to a suboptimal one that is nevertheless better for itself.
4. An alternate model designates an owner agent for each decision variable. Each owner agent would then centralize
and aggregate the preferences of other agents interested in its variable. Subsequently, the owner agents would use a
distributed optimization algorithm to find the optimal solution. This model limits the reusability of computation from
the main problem in solving the marginal problems in which each agent is removed in turn because when excluding
the owner agent of a variable, one needs to assign ownership to another agent and restart the computational process
in regards to this variable and other connected variables. This reuse of computation is important in making M-DPOP
scalable. Our approach is disaggregated and facilitates greater reuse.

714

fiM-DPOP: FAITHFUL D ISTRIBUTED I MPLEMENTATION OF E FFICIENT S OCIAL C HOICE P ROBLEMS

The neighborhood of each local copy Xvi of a variable is composed of three kinds of variables:
Neighbors(Xvi ) = Siblings(Xvi )  Local neighbors(Xvi )  Hard neighbors(Xvi ).

(2)

The siblings are local copies of Xv that belong to other agents Aj 6= Ai also interested in Xv :
Siblings(Xvi ) = {Xvj | Aj 6= Ai and Xv  X(Aj )}

(3)

All siblings of Xvi are connected pairwise with an equality constraint. This ensures that all
agents eventually have a consistent value for each variable. The second set of variables are the
local neighbors of Xvi from the local optimization problem of Ai . These are the local copies of the
other variables that agent Ai is interested in, which are connected to Xvi via relations in Ai s local
problem:
Local neighbors(Xvi ) = {Xui | Xu  X(Ai ), and rij  Ri s.t. Xui  scope(ri )}

(4)

We must also consider the set of hard constraints that contain in their scope the variable Xv and
some other public variables: Hard (Xv ) = {cs  C|Xv  scope(cs )}. These constraints connect
Xv with all the other variables Xu that appear in their scope, which may be of interest to some
other agents as well. Consequently, Xvi should be connected with all local copies Xtj of the other
variables Xt that appear in these hard constraints:
Hard neighbors(Xvi ) = {Xtj |cs  Hard (Xv ) s.t. Xt  scope(cs ), and Xt  X(Aj )}

(5)

In general, each agent can also have private variables, and relations or constraints that involve
private variables, and link them to the public decision variables. For example, consider a meeting
scheduling application for employees of a company. Apart from the work-related meetings they
schedule together, each one of the employees also has personal items on her agenda, like appointments to the doctor, etc. Decisions about the values for private variables and information about
these local relations and constraints remain private. These provide no additional complications and
will not be discussed further in the paper.
2.2 Example Social Choice Problems
Before continuing to present our main results we describe three additional problems of social choice
that serve to motivate our work. In fact, the problem of efficient social choice is fundamental to
microeconomics and political science (Mas-Colell, Whinston, & Green, 1995). Each problem that
we present is both large scale and distributed, and involves actors in the system that are businesses
and cannot be expected to cooperate, either in revealing their preferences or in following the rules
of a distributed algorithm.
A IRPORT S LOT A LLOCATION . As airports become more congested, governments are turning to
market-based approaches to allocate landing and takeoff slots. For instance, the U.S. Federal Aviation Administration recently commisioned a study on the use of an auction to allocate slots at
New Yorks congested LaGuardia airport (Ball, Donohue, & Hoffman, 2006). This problem is large
scale when it expands to include airports throughout the U.S., and eventually the World, exhibits
self-interest (airlines are profit-maximizing agents with private information about their utilities for
715

fiP ETCU , FALTINGS , & PARKES

different slot allocations), and is one in which privacy is a major concern because of the competitiveness of the airline industry. A typical policy goal is to maximize the total utility of the allocation,
i.e. one of efficient social choice. This problem motivates our study of combinatorial auctions in
Section 6. A combinatorial auction (CA) is one in which a set of heterogeneous, indivisible goods
are to be allocated to agents, each of which has values expressed on sets of goods; e.g., I only want
the 9am slot if I also get the 10am slot or I am indifferent between the 9am and the 9:05am slot.
The airport slot allocation problem motivated the first paper on CAs (Rassenti, Smith, & Bulfin,
1982), in which it was recognized that airlines would likely need to express utilities on sets of slots
that correspond to the right to fly a schedule in and out of an airport.
O PEN -ACCESS W IRELESS N ETWORKS . Most wireless spectrum today is owned and operated as
closed networks, for example by cellular companies such as T-Mobile and AT&T. However there is
plenty of debate about creating open-access wireless networks in which bandwidth must be available
for use on any phone and any software.5 Some have recently proposed using an auction protocol to
allow service providers to bid in a dynamic auction for the right to use spectrum for a given period of
time to deliver services.6 Taken to its logical conclusion, and in an idea anticipated by Rosenschein
and Zlotkin (1994) for wired telephony, this suggests a secondary market for wireless spectrum and
corresponds to a problem of efficient social choice: allocate spectrum to maximize the total utility
of consumers. This problem is large scale, exhibits self-interest, and is inherently decentralized.
T HE M EETING S CHEDULING P ROBLEM . Consider a large organization with dozens of departments, spread across dozens of sites, and employing tens of thousands of people. Employees from
different sites and departments want to setup thousands of meetings each week. Due to privacy
concerns among different departments, centralized problem solving is not desirable. Furthermore,
although the organization as a whole desires to minimize the cost of the whole process, each department and employee is self interested in that it wishes to maximize its own utility. An artificial
currency is created for this purpose and a weekly assignment is made to each employee. Employees
express their preferences for meeting schedules in units of this currency.
Refer to Figure 2 for an example of such a problem, where 3 agents want to setup 3 meetings.
Figure 2(b) shows that each agent has as local variables the time slots corresponding to the meetings
it participates in (e.g. M12 represents A2 s local replica of the variable representing meeting M1 ).
Local edges correspond to local all-different constraints between an agents variables and ensure
that it does not participate in several meetings at the same time. Equality constraints between local
replicas of the same value ensure global agreement. Agents specify their relations via local edges on
local replicas. For example, agent A1 with its relation on the time of meeting M1 can now express a
preference for a meeting later in the day with relation r10 , which can assign low utilities to morning
time slots and high utilities to afternoon time slots. Similarly, if A2 prefers holding meeting M2
after meeting M1 , then it can use the local relation r20 to assign high utilities to all satisfactory
combinations of timeslots and low utility otherwise. For example, hM1 = 9AM, M2 = 11AM i
gets utility 10, and hM1 = 9AM, M2 = 8AM i gets utility 2.
5. In a breakthrough ruling, the U.S. Federal Communications Commission (FCC) will require open access
for around one-third of the spectrum to be auctioned in early 08.
But it stopped short of mandating that this spectrum be made available in a wholesale market to would be service providers.
See
http://www.fcc.gov/073107/700mhz news release 073107.pdf
6. Google proposed such an auction in a filing made to the FCC on May 21st, 2007.
See
http://gullfoss2.fcc.gov/prod/ecfs/retrieve.cgi?native or pdf=pdf&id document=6519412647.

716

fiM-DPOP: FAITHFUL D ISTRIBUTED I MPLEMENTATION OF E FFICIENT S OCIAL C HOICE P ROBLEMS

Figure 2: A meeting scheduling problem. (a) A centralized model (each vertex is a meeting variable, red
edges correspond to hard constraints on non-overlap for meetings that share a participant (that
for agent A2 is a hyperedge because it particpates in every meeting), blue edges correspond to
relations and represent agent preferences). (b) A decentralized (DCOP) model with replicated
variables. Each agent has a local replica of variables of interest and inter-agent edges denote
equality constraints that ensure agreement. The hard constraint for non-overlap between meetings
M1 , M2 and M3 is now a local hyperedge to agent A2 . (c) A DFS arrangement of the decentralized
problem graph. Used by the DPOP algorithm to control the order of problem solving.

In the experimental results presented in Section 6 we adopt meeting scheduling as prototypical
of structured social choice problems with the problem instances associated with an organizational
hierarchy. Meeting scheduling was introduced in Section 2.1. For a second set of experiments we
consider combinatorial auctions (CAs), in which agents bid for bundles of goods, and there we
consider a set of problem instances that are unstructured and provide a comparison point to that of
meeting scheduling. CAs provide a nice abstraction of the kinds of allocation problems that exist in
the airport and wireless network domains.

3. Cooperative Case: Efficient Social Choice via DPOP
In this section, we review DPOP (Petcu & Faltings, 2005b), which is a general purpose distributed
optimization algorithm. DPOP (Distributed Pseudotree Optimization Protocol) is based on dynamic
programming and adapts Dechters (Dechter, 2003) general bucket elimination scheme to the distributed case. Its main advantage is that it only generates a linear number of messages. This is in
contrast to other optimization algorithms like ADOPT (Modi et al., 2005) and ensures minimal network overhead produced by message exchange. On the other hand, a concern in DPOP can be the
size of individual messages since this grows exponentially with a parameter of the constraint graph
called the induced width (see Section 3.1.4). Nevertheless, for problems that exhibit local structure,
DPOP typically scales to much larger problems, and is orders of magnitude more efficient, than
717

fiP ETCU , FALTINGS , & PARKES

other techniques (Petcu & Faltings, 2005b, 2007). To simplify the exposition, we first illustrate
DPOP in a general DCOP context, and then show how to instantiate DPOP for social choice problems. In particular, we explain how to leverage the structure provided by local replicas. We consider
only cooperative agents throughout this section.
3.1 The DPOP Algorithm for DCOPs
This section presents the DPOP algorithm for generic DCOPs. To simplify the exposition, we
assume  in this section only  that each agent Ai represents a single variable Xi , and that the
constraint graph is given.
DPOP is composed of three phases:
 Phase one constructs a DFS arrangement, DFS (A), which defines the control flow of message passing and computation in DPOP.
 Phase two is a bottom-up utility propagation along the tree constructed in phase 1. In this
phase utilities for different values of variables are aggregated to reflect optimal decisions that
will be made in subtrees rooted at each node in the tree.
 Phase three is a top-down value assignment propagation along the tree constructed in phase
1. In this phase decisions are made based on the aggregate utility information from phase 2.
In describing these phases we refer to Figure 3 for a running example. We also introduce an
explicit numerical example to illustrate phases two and three in more detail.
3.1.1 DPOP P HASE O NE : DFS T REE G ENERATION
This first phase performs a depth-first search (DFS) traversal of the problem graph, thereby constructing a DFS arrangement of the problem graph. The DFS arrangement is subsequently used to
provide control flow in DPOP and guide the variable elimination order. When the underlying problem graph is a tree then the DFS arrangement will also be a tree. In general, the DFS arrangement is
a graph that we define as the union of a set of tree edges and additional back edges, which connect
some of the nodes with their ancestors.7
Definition 2 (DFS arrangement) A DFS arrangement of a graph G defines a rooted tree on a
subset of the edges (the tree edges) with the remaining edges included as back edges. The tree
edges are defined so that adjacent nodes in G fall in the same branch of the tree.
Figure 3 shows an example DFS arrangement. The tree edges are shown as solid lines (e.g.
1  3) and the back edges are shown as dashed lines (e.g. 12  2, 4  0). Two nodes Xi and Xv are
said to be in the same branch of the DFS arrangement if there is a path from the higher node to the
lower node along tree edges; e.g., nodes X0 and X11 in Figure 3. DFS arrangements have already
been investigated as a means to boost search in constraint optimization problems (Freuder & Quinn,
1985; Modi et al., 2005; Dechter & Mateescu, 2006). Their advantage is that they allow algorithms
to exploit the relative independence of nodes lying in different branches of the DFS arrangement
7. For simplicity, we assume in what follows that the original problem is connected. However there is no difficulty
in applying DPOP to disconnected problems. The DFS arrangement becomes a DFS forest, and agents in each
connected component can simply execute DPOP in parallel in a separate control thread. The solution to the overall
problem is just the union of optimal solutions for each independent subproblem.

718

fiM-DPOP: FAITHFUL D ISTRIBUTED I MPLEMENTATION OF E FFICIENT S OCIAL C HOICE P ROBLEMS

Figure 3: A DFS arrangement for a problem graph. Tree edges are shown in solid and back edges are
dashed. The DFS arrangement is constructed by initializing token-passing from X0 . Any k-ary
constraints, such as C4 , are treated as if they are cliques.

(i.e. nodes that are not direct descendants or ancestors of one-another), in that it is possible to
perform search in parallel on independent branches and then combine the results.
We introduce some definitions related to DFS arrangements:
Definition 3 (DFS concepts) Given a node Xi in the DFS arrangement, we define:
 parent Pi / children Ci : Xi s ancestor/descendants connected to Xi via tree-edges (e.g.
P4 = X1 , C4 = {X9 , X10 }).
 pseudo-parents PP i : Xi s ancestors connected to Xi via back-edges (PP 5 = {X0 }).
 pseudo-children PC i : Xi s descendants connected to Xi via back-edges (e.g. PC 1 =
{X8 }).
 separator Sep i of Xi : ancestors of Xi which are directly connected with Xi or with descendants of Xi (e.g. Sep 3 = {X1 } and Sep 11 = {X0 , X2 , X5 }).
 tree neighbors TN i of Xi are the nodes linked to Xi via tree edges, that is TN i = Pi  Ci
(e.g. TN 4 = {X1 , X9 , X10 }).
Removing the nodes in Sep i completely disconnects the subtree rooted at Xi from the rest of
the problem. In case the problem is a tree, then Sep i = {Pi }, Xi  X . In the general case, Sep i
contains Pi , all PP i and all the pseudoparents of descendants of Xi where these pseudoparents are
also ancestors of Xi . For example, in Figure 3, the separator of node X4 contains its parent X1 , and
its pseudoparent X0 . It is both necessary and sufficient for the values on variables {X0 , X1 } to be
set before the problem rooted at node X4 is independent from the rest of the problem. Separators
play an important role in DPOP because contingent solutions must be maintained when propagating
utility information up the DFS arrangement for different possible assignments to separator variables.
Constructing the DFS Tree Generating DFS trees in a distributed manner is a task that has
received a lot of attention, and there are many algorithms available: for example Collin and
Dolev (1994), Barbosa (1996), Cidon (1988), Cheung (1983) to name just a few. For the purposes of executing DPOP, we can assume for example the algorithm of Cheung (1983), which we
briefly outline below. When we instantiate DPOP for SCPs, we will present our own adaptation of
this DFS generation algorithm to exploit the particulars of SCP.
The simple DFS construction algorithm starts with all agents labeling internally their neighbors
as not-visited. One of the agents in the graph is designated as the root, using for example a leader
719

fiP ETCU , FALTINGS , & PARKES

election algorithm such as that of Abu-Amara (1988),8 or by simply picking the agent with the
lowest ID. The root then initiates the propagation of a token, which is a unique message that will be
circulated to all the agents in the graph, thus visiting them. Initially, the token contains just the
ID of the root. The root sends it to one of its neighbors, and waits for its return before sending it to
each one of its (still) unvisited neighbors. When an agent Xi first receives the token, it marks the
sender as its parent. All neighbors of Xi contained in the token are marked as Xi s pseudoparents
(PP i ).
After this, Xi adds its own ID to the token, and sends the token in turn to each one of its notvisited neighbors Xj , which become its children. Every time an agent receives the token from one
of its neighbors, it marks the sender as visited. The token can return either from Xj (the child to
whom Xi has sent it in the first place), or from another neighbor, Xk . In the latter case, it means
that there is a cycle in the subtree, and Xk is marked as a pseudochild.
When a dead end is reached, the last agent backtracks by sending the token back to its parent.
When all its neighbors are marked visited, Xi has finished exploring all its subtree. Xi then removes
its own ID from the token, and sends the token back to its parent; the process is finished for Xi .
When the root has marked all its neighbors visited, the entire DFS construction process is over.
Handling Non-binary Constraints. No special treatment is required to construct neighbors to a
variable that correspond to k-ary constraints, for k > 2. For example, in Figure 3 (left), there is a
4-ary constraint C4 involving {X0 , X2 , X5 , X11 }. By Eq. 2, this implies that {X0 , X2 , X5 , X11 }
are neighbors, and in the DFS construction process and they will appear along the same branch in
the tree. This produces the result in Figure 3 (right).
3.1.2 DPOP P HASE T WO : UTIL P ROPAGATION (I NFERENCE )
Phase two is a bottom-to-top pass on the DFS arrangement in which utility information is aggregated
and propagated from the leaves towards the root from each node to its parent and through tree edges
but not back edges. At a high level, the leaves start by computing and sending UTIL messages to
their parents, where a UTIL message informs the parent about its local utility for solutions to the
rest of the problem, minimally specified in terms of its local utility for different value assignments
to separator variables. Subsequently each node propagates a UTIL message that represents the
contingent utility of the subtree rooted at its node for assignments of values to separator variables.
In more detail, all nodes perform the following steps:
1. Wait for UTIL messages from all their children, and store them.
2. Perform an aggregation: join messages from children, and also the relations they have with
their parents and pseudoparents.
3. Perform an optimization: project themselves out of the resulting join by picking their optimal
values for each combination of values of the other variables in the join.
4. Send the result to parent as a new UTIL message.
8. In cases where the problem is initially disconnected, then it is required to choose multiple roots, one for each connected component. A standard leader election algorithm, when executed by all agents in the problem, will elect
exactly as many leaders as there are connected components.

720

fiM-DPOP: FAITHFUL D ISTRIBUTED I MPLEMENTATION OF E FFICIENT S OCIAL C HOICE P ROBLEMS

A UTIL message sent by a node Xi to its parent Pi is a multidimensional matrix which informs
Pi how much utility, ui (Sep i ) the subtree rooted at Xi receives for different assignments of values
to variables that define the separator Sep i for the subtree. One of these variables, by definition, is the
variable managed by parent Pi . This UTIL message already represents the result of optimization,
where variables local to the subtree have been optimized for different assignments of separator
variables. To compute a UTIL message a node uses two operations: aggregation and optimization.
Aggregations apply the JOIN operator and optimizations apply the PROJECT operator as described
by Petcu and Faltings (2005b), and briefly summarized here.
Let UTILij and UTILkj denote UTIL messages sent from nodes Xi and Xk to their parent
node Xj . We denote by dim(UTILkj ) the set of dimensions of such a matrix, i.e. the set of
variables in the separator of sending node Xk . Assuming Xj is the node receiving these messages,
we define:
Definition 4 (JOIN operator) The  operator (join): UTILij  UTILkj is the join of two
UTIL matrices. This is also a matrix with dim(UTILij )  dim(UTILkj ) as dimensions. The
value of each cell in the join is the sum of the corresponding cells in the two source matrices.
Definition 5 (PROJECT operator) The  operator (projection): if Xj  dim(UTILij ),
UTILij Xj is the projection through optimization of the UTILij matrix along the Xj axis:
for each instantiation of the variables in {dim(UTILij ) \ Xj }, all the corresponding values from
UTILij (one for each value of Xj ) are tried, and the maximal one is chosen. The result is a matrix
with one less dimension (Xj ).
Notice that the subtree rooted at Xi is influenced by the rest of the problem only through Xi s
separator variables. Therefore, a UTIL message contains the optimal utility obtained in the subtree
for each instantiation of variables Sep i and the separator size plays a crucial role in bounding the
message size.
Example 2 (UTIL propagation) Figure 4 shows a simple example of a UTIL propagation. The
problem has a tree structure (Figure 4(a)), with 3 relations r31 , r21 , and r10 detailed in Figure 4(b).
The relations are between variables (X3 , X1 ), (X2 , X1 ) and (X1 , X0 ) respectively. These are all
individual variables and there are no local replicas. In the UTIL phase X2 and X3 project themselves out of r21 and r31 , respectively. The results are the highlighed cells in r21 and r31 in Figure 4(b).
For instance, the optimal value for X2 given that X1 := a is to assign X2 := c and this has utility
5. These projections define the UTIL messages they send to X1 . X1 receives the messages from X2
and X3 , and joins them together with its relation with X0 (adds the utilities from the messages into
the corresponding cells of r10 ). It then projects itself out of this join. For instance, the optimal value
for X1 given X0 := b is X1 := a because 2 + 5 + 6  max{3 + 4 + 4, 3 + 6 + 3}. The result
is depicted in Figure 4(d). This is the UTIL message that X0 receives from X1 . Each value in the
message represents the total utility of the entire problem for each value of X0 . We return to this
example below in the context of the third phase of value propagation.
Non-binary Relations and Constraints. As with binary constraints/relations, a k-ary constraint
is introduced in the UTIL propagation only once, by the lowest node in the DFS arrangement that is
part of the scope of the constraint. For example, in Figure 3, the constraint C4 is introduced in the
UTIL propagation only once, by X11 , while computing its message for its parent, X5 .
721

fiP ETCU , FALTINGS , & PARKES

Figure 4: Numerical example of UTIL propagation. (a) A simple DCOP problem in which there are three
relations r31 , r21 and r10 between (X3 , X1 ), (X2 , X1 ) and (X1 , X0 ) respectively. (b) Projections of
X2 and X3 out of their relations with X1 . The results are sent to X1 as UTIL21 , and UTIL31
respectively. (c) X1 joins UTIL21 and UTIL31 with its own relation with X0 . (d) X1 projects
itself out of the join and sends the result to X0 .

3.1.3 DPOP P HASE T HREE : VALUE P ROPAGATION
Phase three is a top-to-bottom pass that assigns values to variables, with decisions made recursively
from the root down to the leaves. This VALUE propagation phase is initiated by the root agent
X0 once it has received UTIL messages from all of its children. Based on these UTIL messages,
the root assigns to variable X0 the value v  that maximizes the sum of its own utility and that
communicated by all its subtrees. It then sends a VALUE(X0r  v  ) message to every child. The
process continues recursively to the leaves, with agents Xi assigning the optimal values to their
variables. At the end of this phase, the algorithm finishes, with all variables being assigned their
optimal values.
Example 3 (Value propagation) Return to the example in Figure 4. Once X0 receives the UTIL
message from node X1 it can simply choose the value for X0 that produces the largest utility for the
whole problem: X0 = a (X0 = a and X0 = c produce the same result in this example, so either one
can be chosen). Now in the value-assignment propagation phase X0 informs X1 of its choice via a
message VALUE (X0  a). Node X1 then assigns optimal value X1 = c and the process continues
with a message V ALU E(X1  c) sent to its children, X2 and X3 . The children assign X2 = b and
X3 = a and the algorithm terminates with an optimal solution hX0 = a, X1 = c, X2 = b, X3 = ai
and total utility of 15.
3.1.4 C OMPLEXITY A NALYSIS OF DPOP
DPOP produces a number of messages that scales linearly in the size of the problem graph, i.e.
linearly in the number of nodes and edges in the DCOP model (Petcu & Faltings, 2005b). The
complexity of DPOP lies in the size of the UTIL messages (note that the tokens passed around in
722

fiM-DPOP: FAITHFUL D ISTRIBUTED I MPLEMENTATION OF E FFICIENT S OCIAL C HOICE P ROBLEMS

constructing the DFS(A) and the VALUE messages are of size linear in the problem graph). Petcu
and Faltings (2005b) show that the size of the largest UTIL message is exponential in a parameter
called the induced width (Kloks, 1994; Dechter, 2003).
The induced width, denoted w, of a constraint graph given by a chosen DFS arrangement is a
structural parameter that equals the size of the largest separator of any node in the DFS arrangement
(see Definition 3.):
w = max |Sep i |.
Xi X

(6)

In the example from Figure 3, the induced width of the graph given this particular DFS ordering
is w = 3, given by Sep11 = {X0 , X2 , X5 }. Intuitively, the more a problem has a tree-like structure,
the lower its induced width. In particular, if the problem graph is a tree then it will have an induced
width equal to 1 because the DFS arrangement will always be a tree. Problem graphs that are cliques,
on the other hand, have an induced width equal to the number of nodes minus 1, irrespective of the
DFS-tree arrangement.
Proposition 1 (DPOP Complexity) (Petcu & Faltings, 2005b) The number of messages passed in
DPOP is 2m, (n  1) and (n  1) for phases one, two and three respectively, where n and m are
the number of nodes and edges in the DCOP model with replicated variables. The maximal number
of utility values computed by any node in DPOP is O(Dw+1 ), and the largest UTIL message has
O(Dw ) entries, where w is the induced width of the DFS ordering used.
In the case of trees, DPOP generates UTIL messages of dimension equal to the domain size of
the variable defining the parent of each node. In the case of cliques, the maximal message size in
DPOP is exponential in n  1. Not all DFS arrangements yield the same width, and it is desirable to
construct DFS arrangements that provide low induced width. However, finding the tree arrangement
with the lowest induced width is an NP-hard optimization problem (Arnborg, 1985). Nevertheless,
good heuristics have been identified for finding tree arrangements with low width (Kloks, 1994;
Bayardo & Miranker, 1995; Bidyuk & Dechter, 2004; Petcu & Faltings, 2007, 2005b). Although
most were designed and explored in a centralized context, some of them (notably max-degree and
maximum cardinality set) are easily amenable to a distributed environment.
3.2 DPOP Applied to Social Choice Problems
In this section, we instantiate DPOP for efficient social choice problems. Specifically, we first show
how the optimization problem is constructed by agents from their preferences and potential variables
of interest. Subsequently, we show the changes we make to DPOP to adapt it to the SCP domain.
The most prominent such adaptation exploits the fact that several variables represent local replicas
of the same variable, and can be treated as such both during the UTIL and the VALUE phases. This
adaptation improves efficiency significantly, and allows complexity claims to be stated in terms of
the induced width of the centralized COP problem graph rather than the distributed COP problem
graph (see Section 3.2.5).
3.2.1 I NITIALIZATION : C OMMUNITY F ORMATION
To initialize the algorithm, each agent first forms the communities around its variables of interest,
X(Ai ), and defines a local optimization problem COP i (X(Ai ), Ri ) with a replicated variable Xvi
723

fiP ETCU , FALTINGS , & PARKES

for each Xv  X(Ai ). Shorthand Xvi  COP i denotes that agent Ai has a local replica of variable
Xv . Each agent owns multiple nodes and we can conceptualize each node as having an associated virtual agent operated by the owning agent. Each such virtual agent is responsible for the
associated variable.
All agents subscribe to the communities in which they are interested, and learn which other
agents belong to these communities. Neighboring relations are established for each local variable
according to Eq. 2, as follows: all agents in a community Xv connect their corresponding local
copies of Xv with equality constraints. By doing so, the local problems COP i (X(Ai ), Ri ) are
connected with each other according to the interests of the owning agents. Local relations in each
COP i (X(Ai ), Ri ) connect the corresponding local variables. Hard constraints connect local copies
of the variables they involve. Thus, the overall problem graph is formed.
For example, consider again Figure 2(b). The decision variables are the start times of the three
meetings. Each agent models its local optimization problem by creating local copies of the variables
in which it is interested and expressing preferences with local relations. Formally, the initialization
process is described in Algorithm 1.
Algorithm 1: DPOP init: community formation and building DCOP (A).
DPOP init(A, X , D, C, R):
1 Each agent Ai models its interests as COP i (X(Ai ), Ri ): a set of relations Ri imposed on
a set X(Ai ) of variables Xvi that each replicate a public variable Xv  X(Ai )
2 Each agent Ai subscribes to the communities of Xv  X(Ai )
3 Each agent Ai connects its local copies Xvi  X(Ai ) with the corresponding local copies
of other agents via equality constraints

3.2.2 DFS T RAVERSAL
The method for DFS traversal is described in Algorithm 2. The algorithm starts by choosing one of
the variables, X0 , as the root. This can be done randomly, for example using a distributed algorithm
for random number generation, with a leader election algorithm like Ostrovski (1994), or by simply
picking the variable with the lowest ID. The agents involved in the community for X0 then randomly
choose one of them, Ar as the leader. The local copy X0r of variable X0 becomes the root of the
DFS. Making the assumption that virtual agents act on behalf of each variable in the problem,
the functioning of the token passing mechanism is similar to that described in Section 3.1.1, with
additional consideration given to the community structure. Once a root has been chosen, the agents
participate in a distributed depth-first traversal of the problem graph. For convenience, we describe
the DFS process as a token-passing algorithm in which all members within a community can observe
the release or pick up of the token by the other agents. The neighbors of each node are sorted (in
line 7) to prioritize for copies of variables held by other agents, and then other local variables, and
finally other variables linked through hard constraints.
Example 4 Consider the meeting scheduling example in Figure 2. Assume that M3 was chosen as
the start community and A2 was chosen within the community as the leader. A2 creates an empty
token DFS =  and adds M32 s ID to the token (DFS = {M32 }). As in Eq. 2, Neighbors(M32 ) =
{M33 , M31 , M12 , M22 }. A2 sends the token DFS = {M32 } to the first unvisited neighbor from this
724

fiM-DPOP: FAITHFUL D ISTRIBUTED I MPLEMENTATION OF E FFICIENT S OCIAL C HOICE P ROBLEMS

Algorithm 2: DPOP Phase One: DFS construction.
Inputs: each Ai knows its COP i , and Neighbors(Xvi ), Xvi  COP i
Outputs: each Ai knows P (Xvi ), PP (Xvi ), C(Xvi ), PC (Xvi ), Xvi  COP i .
1
2
3

4
5
6
7

8
9
10

Procedure Initialization
The agents choose one of the variables, X0 , as the root.
Agents in X0 s community elect a leader, Ar .
Ar initiates the token passing from X0r to construct the DFS
Procedure Token Passing (performed by each virtual agent Xvi  COP i )
if Xvi is root then P (Xvi ) = null; create empty token DFS := 
else DFS :=Handle incoming tokens()
Let DFS := DFS  {Xvi }
Sort Neighbors(Xvi ) by Siblings(Xvi ), then Local neighbors(Xvi ), then
Hard neighbors(Xvi ). Set C(Xvi ) := null.
forall Xl  Neighbors(Xvi ) s.t. Xl not visited yet do
C(Xvi ) := C(Xvi )  Xl . Send DFS to Xl wait for DFS token to return.
Send DFS token back to P (Xvi ).
Procedure Handle incoming tokens() //run by each virtual agent Xvi  COP i

11
12
13
14

15

Wait for any incoming DFS message; let Xl be the sender
Mark Xl as visited.
if this is the first DFS message (i.e. Xl is my parent) then
P (Xvi ) := Xl ; PP (Xvi ) := {Xk 6= Xl |Xk  Neighbors(Xvi )  DFS }; PP (Xvi ) := 
else
if Xl 
/ C(Xvi ) (i.e. this is a DFS coming from a pseudochild) then
PC (Xvi ) := PC (Xvi )  Xl

list, i.e. M33 , which belongs to A3 . A3 receives the token and adds its copy of M3 (now DFS =
{M32 , M33 }). A3 then sends the token to M33 s first unvisited neighbor, M31 (which belongs to A1 ).
Agent A1 receives the token and adds its own copy of M3 to it (now DFS = {M32 , M33 , M31 }).
M31 s neighbor list is Neighbors(M31 ) = {M32 , M33 , M11 }. Since the token that A1 has received
already contains M32 and M33 , this means that they were already visited. Thus, the next variable
to visit is M11 , which happens to be a variable that also belongs to A1 . The token is passed
to M11 internally (no message exchange required), and M11 is added to the token (now DFS =
{M32 , M33 , M31 , M11 }).
The process continues, exploring sibling variables from each community in turn, and then passing on to another community, and so on. Eventually all replicas of a variable are arranged in a
chain and have equality constraints (back-edges) with all the predecessors that are replicas of the
same variable. When a dead end is reached, the last agent backtracks by sending the token back to
its parent. In our example, this happens when A3 receives the token from A2 in the M2 community.
Then, A3 sends back the token to A2 and so on. Eventually, the token returns on the same path all
the way to the root and the process completes.
725

fiP ETCU , FALTINGS , & PARKES

3.2.3 H ANDLING

THE

P UBLIC H ARD C ONSTRAINTS .

Social choice problems, as defined in Definition 1 can contain side constraints, in the form of publicly known hard constraints, that represent domain knowledge such as a resource can be allocated
only once, this hotel can accomodate 100 people, no person can be in more than one meeting
at the same time. etc. These constraints are not owned by any agent, but are available to all agents
interested in any variable involved in the domain of any such constraint. Handling these constraints
is essentially unchanged from handling the non-binary constraints in standard DPOP, as described in
Section 3.1.1 for the DFS construction phase, and in Section 3.1.2 for the UTIL phase. Specifically:
DFS Construction: Neighboring relationships as defined in Eq. 2 require for each local variable
that other local copies that share a hard constraint are considered as neighbors. Because of the
prioritization in line 7 of Algorithm 2 (for DFS construction), the DFS traversal is mostly made
according to the structure defined by the relations of the agents and most hard constraints will
appear as backedges in the DFS arrangement of the problem graph.
UTIL Propagation: Hard constraints are introduced in the UTIL propagation phase by the lowest
agent in the community of the variable from the scope of the hard constraint, i.e. the agent with the
variable that is lowest in the DFS ordering. For example, if there was a constraint between M2 and
M3 in Figure 2 to specify that M2 should occur after M3 then this becomes a backedge between the
2 communities and would be assigned to A3 for handling.
3.2.4 H ANDLING R EPLICA VARIABLES
Our distributed model of SCP replicates each decision variable for every interested agent and connects all these copies with equality constraints. By handling replica variables carefully we can
avoid increasing the induced width k of the DCOP model when compared to the induced width
w of the centralized model. With no further adaptation, the UTIL messages in DPOP on the distributed problem graph would be conditioned on as many variables as there are local copies of an
original variable. However, all the local copies represent the same variable and must be assigned the
same value; thus, sending many combinations where different local copies of the same variable take
different values is wasteful. Therefore, we handle multiple replicas of the same variable in UTIL
propagation as though they are the single, original variable, and condition relations on just this one
value. This is realized by updating the JOIN operator as follows:
Definition 6 (Updated JOIN operator for SCP) Defined in two steps:
Step 1: Consider all UTIL messages received as in input. For each one, consider each variable
Xvi on which the message is conditioned, and that is also a local copy of an original variable Xv .
Rename Xvi from the input UTIL message as Xv , i.e. the corresponding name from the original
problem.
Step 2: Apply the normal JOIN operator for DPOP.
Applying the updated JOIN operator makes all local copies of the same variable become indistinguishable from each other, and merges them into a single dimension in the UTIL message and
avoids this exponential blow-up.
Example 5 Consider the meeting scheduling example in Figure 2. The centralized model in Figure 2(a) has a DFS arrangement that yields induced width 2 because it is a clique with 3 nodes.
726

fiM-DPOP: FAITHFUL D ISTRIBUTED I MPLEMENTATION OF E FFICIENT S OCIAL C HOICE P ROBLEMS

Nevertheless, the corresponding DCOP model in Figure 2(b) has induced width 3, as can be seen
in the DFS arrangement from Figure 2(c), in which Sep M22 = {M32 , M33 , M12 }. Applying DPOP to
this DFS arrangement, M22 would condition its UTIL message UTILM22 M12 on all variables in its
separator: {M32 , M33 , M12 }. However, both M32 and M33 represent the same variable, M3 . Therefore, M22 can apply the updated JOIN operator, which leverages the equality constraint between the
two local replicas and collapse them into a single dimension (called M3 ) in its message for M12 .
The result it that the outgoing message only has 2 dimensions: {M3 , M12 }, and it takes much less
space. This is possible because all 3 agents involved, i.e. A1 , A2 and A3 know that M31 , M32 and
M33 represent the same variable.
With this change, the VALUE propagation phase is modified so that only the top most local
copy of any variable solve an optimization problem and compute the best value, announcing this
result to all the other local copies which then assume the same value.
3.2.5 C OMPLEXITY A NALYSIS OF DPOP A PPLIED TO S OCIAL C HOICE
By this special handling of replica variables, DPOP applied to SCPs will scale with the induced
width of the centralized problem graph, and independently of the number of agents involved and in
the number of local replica variables.
Consider a DFS arrangement for the centralized model of the SCP that is equivalent to the
DFS arrangement for the DCOP model. Equivalent here means that the original variables from
SCP are visited in the same order in which their corresponding communities are visited during the
distributed DFS construction. (Recall that the distributed DFS traversal described in Section 3.1.1
visits all local copies from a community from DCOP before moving on to the next community). Let
w denote the induced width of this DFS arrangement of the centralized SCP. Similarly, let k denote
the induced width of the DFS arrangement of the distributed model. Let D = maxm |dm | denote
the maximal domain of any variable. Then, we have the following:
Theorem 1 (DPOP Complexity for SCP) The number of messages passed in DPOP in solving a
SCP is 2m, (n  1) and (n  1) for phases one, two and three respectively, where n and m are
the number of nodes and edges in the DCOP model with replicated variables. The maximal number
of utility values computed by any node in DPOP is O(Dw+1 ), and the largest UTIL message has
O(Dw+1 ) entries, where w is the induced width of the centralized problem graph.
P ROOF. The first part of the claim (number of messages) follows trivially from Proposition 1. For
the second part (message size and computation): given a DFS arrangement of a DCOP, applying
Proposition 1 trivially gives that in the basic DPOP algorithm, the maximal amount of computation
on any node is O(Dk+1 ), and the largest UTIL message has O(Dk ) entries, where k is the induced width of the DCOP problem graph. To improve this analysis we need to consider the special
handling of the replica variables.
Consider the UTIL messages which travel up along the DFS tree, and whose sets of dimensions
contain the separators of the sending nodes. Recall that the updated JOIN collapses all local replicas
into the original variables. The union of the dimensions of the UTIL messages to join in the DPOP
on the DCOP model becomes identical to the set of dimensions of the nodes in the DPOP on the
centralized model. Thus, each node in the DCOP model performs the same amount of computation
as its counterpart on the centralized model. It follows that the computation required in DPOP scales
as O(Dw+1 ) rather than O(Dk+1 ) by this special handling.
727

fiP ETCU , FALTINGS , & PARKES

There remains one additional difference between DPOP on the DFS arrangement for the centralized SCP versus DPOP on the DFS arrangement for the DCOP. A variable Xv that is replicated
across multiple agents can only be projected out from the UTIL propagation through local optimization by the top-most agent handling a local replica of Xv . This is the first node at which all
relevant information is in place to support this optimization step. In particular, whenever a node
with the maximal separator set is not also associated with the top-most replica of its variable then
it must retain dependence on the value assigned to its variable in the UTIL message that it sends to
its parent. This increases the worst case message size of DPOP to O(Dw+1 ), as opposed to O(Dw )
for the normal DPOP. Computation remains O(Dw+1 ) because the utility has to be determined for
each value of Xv anyway, and before projecting Xv out. 2
To see the effect on message size described in the proof, in which a local variable cannot be
immediately removed during UTIL propagation, consider again the problem from Figure 2. Suppose now that agent A3 is also involved in meeting M1 . This introduces an additional back-edge
M23  M13 in the DFS arrangement for the decentralized model shown in Figure 2(c). The DFS
arrangement of the COP model that corresponds to the decentralized model is simply a traversal
of the COP in the order in which the communities are visited during the distributed DFS construction. This corresponds to a chain: M3  M1  M2 . The introduction of the additional back-edge
M23  M13 in the distributed DFS arrangement does not change the DFS of the COP model, and its
width remains w = 2. However, as M23 is not the top most copy of M2 , agent A3 cannot project
M2 out of its outgoing UTIL message. The result is that it sends a UTIL message with w + 1 = 3
dimensions, as opposed to just w = 2.

4. Handling Self-interest: A Faithful Algorithm for Social Choice
Having adapted DPOP to remain efficient for SCPs, we now turn to the issue of self-interest. Without further modification, an agent can manipulate DPOP by misreporting its private relations and
deviating from the algorithm in various ways. In the setting of meeting scheduling, for example,
an agent might benefit by misrepresenting its local preferences (I have massively more utility for
the meeting occurring at 2pm than at 9am), incorrectly propagating utility information of other
(competing) agents (The other person on my team has very high utility for the meeting at 2pm),
or by incorrectly propagating value decisions (It has already been decided that some other meeting
involving the other person on my team will be at 9am so this meeting must be at 2pm.)
By introducing carefully crafted payments, by leveraging the information and communication
structure inherent to DCOPs for social choice, and by careful partitioning of computation so that
each agent is only asked to reveal information, perform optimization, and send messages that are
in its own interest, we are able to achieve faithfulness. This will mean that each agent will choose,
even when self-interested, to follow the modified algorithm. We first define the VCG mechanism
for social choice and illustrate its ability to prevent manipulation in centralized problem solving in
a simple example. With this in place, we next review the definitions of faithful distributed implementation and the results of a useful principle, the partition principle. We then describe the Simple
M-DPOP algorithm  without reuse of computation  and prove its faithfulness.
728

fiM-DPOP: FAITHFUL D ISTRIBUTED I MPLEMENTATION OF E FFICIENT S OCIAL C HOICE P ROBLEMS

4.1 Review: Mechanism Design and the VCG Mechanism
Mechanism design (MD) addresses the problem of optimizing some criteria, frequently social welfare, in the presence of self-interested agents that each have private information relevant to the
problem at hand. In the standard story, agents report private information to a center, that solves
an optimization problem and enforces the outcome.
The second-price, sealed-bid (Vickrey) auction is a simple example of a mechanism: each agent
makes a claim about its value for an item to an auctioneer, who allocates the item to the highest
bidder for the second-highest price (Krishna, 2002). The Vickrey auction is useful because it is
non-manipulable, in that the weakly dominant strategy of each agent is to report its true value, and
efficient, in that the item is allocated to the agent with the highest value.
In our setting of efficient social choice, we will assume the existence of a currency so that agents
can make payments, and make the standard assumption of quasilinear utility functions, so that agent
Ai s net utility is,
ui (X, p) = Ri (X)  p,

(7)

for an assignment X  D to variables X and payment p 
center, i.e., its net utility
P R to the
j
(X),
minus the amount of
r
is that defined by its utility for the assignment, Ri (X) =
j
ri Ri i
its payment. One of the most celebrated results of MD is provided by the Vickrey-Clarke-Groves
(VCG) mechanism, which generalizes Vickreys second price auction to the problem of efficient
social choice:
Definition 7 (VCG mechanism for Efficient Social Choice) Given knowledge of public constraints C, and public decision variables X , the Vickrey-Clarke-Groves (VCG) mechanism works
as follows:
 Each agent, Ai , makes a report Ri about its private relations.
 The centers decision, X  , is that which solves SCP (A) given the reports R = (R1 , . . . , Rn ).
 Each agent Ai , makes payment
Tax (Ai ) =

X



Rj (Xi
)  Rj (X  ) ,

(8)

j6=i

 , for each A , is the solution to SCP (A ) given reports R
to the center, where Xi
i
i
i =
(R1 , . . . , Ri1 , Ri+1 , . . . , Rn ).

Each agent makes a payment that equals the negative marginal externality that its presence
imposes on the rest of the system, in terms of the impact of its preferences on the solution to the
SCP.
The VCG mechanism has a number of useful properties:
 Strategyproofness: Each agents weakly dominant strategy, i.e. its utility-maximizing strategy whatever the strategies and whatever the private information of other agents, is to truthfully report its preferences to the center. This is the sense in which the VCG mechanism is
non-manipulable.
729

fiP ETCU , FALTINGS , & PARKES

 Efficiency: In equilibrium, the mechanism makes a decision that maximizes the total utility
to agents over all feasible solutions to the SCP.
 Participation:
In P
equilibrium, the utility to agent Ai , Ri (X  )  Tax (Ai ) = (Ri (X  ) +
P


j6=i Rj (Xi ), is non-negative, by the principle of optimality, and therej6=i Rj (X )) 
fore agents will choose to participate.
 No-Deficit:
The payment
made by agent Ai is non-negative in the SCP, because
P
P


j6=i Rj (X ) by the principle of optimality, and therefore the entire
j6=i Rj (Xi ) 
mechanism runs at a budget surplus.
To begin to understand why the VCG mechanism is strategyproof, notice that the first term in
Tax (Ai ) is independent of Ai s report. The second term, when taken together
with the agents own
P

true utility from the decision, provides Ai with net utility Ri (X ) + j6=i Rj (X  ). This is the
total utility for all agents, and to maximize this the agent should simply report its true preference
information, because the center will then explicitly solve this problem in picking X  .
Example 6 Return to the example in Figure 4. We can make this into a SCP by associating agents
A1 , A2 and A3 with relations r10 , r21 and r31 on variables {X0 , X1 }, {X1 , X2 }, and {X1 , X3 } respectively. Breaking ties as before, the solution to SCP (A) is < X0 = a, X1 = c, X2 = b, X3 =
a > with utility < 6, 6, 3 > to agents A1 , A2 and A3 respectively. Removing agent A1 , the solution
would be < X0 =?, X1 = a, X2 = c, X3 = a > with utility < 5, 6 > to agents A2 and A3 . The ?
indicates that agents A2 and A3 are indifferent to the value on X0 . Removing agent A2 , the solution
would be < X0 = c, X1 = b, X2 =?, X3 = c >, with utility < 7, 4 > to agents A1 and A3 . Removing agent A3 , the solution would be < X0 = a, X1 = c, X2 = b, X3 =? >, with utility < 6, 6 >
to agents A1 and A2 . The VCG mechanism would assign < X0 = a, X1 = c, X2 = b, X3 = a >,
with payments (5 + 6)  (6 + 3) = 2, (7 + 4)  (6 + 3) = 2, (6 + 6)  (6 + 6) = 0 collected from
agents A1 , A2 and A3 respectively. A3 has no negative impact on agents A1 and A2 and does not
incur a payment. The other agents make payments: the presence of A1 helps A2 but hurts A3 by
more, while the presence of A2 hurts both A1 and A3 . The only conflict in this problem is about the
value assigned to variable X1 . Agents A1 , A2 and A3 each prefer that X1 be assigned to b, c and
a respectively. In the chosen solution, only agent A2 gets its best outcome. Considering the case of
A3 , it can force either a or b to be selected by reporting a suitably high utility for this choice, but
for X1 = a it must pay 4 while for X1 = b it must pay 1, and in either case it weakly prefers the
current outcome in which it makes zero payment.
Having introduced the VCG mechanism, it is important to realize that the VCG mechanism
provides the only known, general purpose, method that exists to solve optimization problems in
the presence of self-interest and private information. On the positive side, it is straightforward to
extend the VCG mechanism (and the techniques of our paper) to maximize a linear weighted sum
of the utility of each agent, where these weights are fixed and known, for instance by a social
planner (Jackson, 2000). Roberts (1979) on the other hand, established that the Groves mechanisms  of which the VCG mechanism is the most important special case  are the only non-trivial
strategyproof mechanisms in the domain of social choice unless there is some known structure to
agent preferences; e.g., everyone prefers earlier meetings, or more of a resource is always weakly
preferred to less. Together with another technical assumption, Roberts theorem has also been extended by Lavi et al. (2003) to domains with this kind of structure, for instance to combinatorial
730

fiM-DPOP: FAITHFUL D ISTRIBUTED I MPLEMENTATION OF E FFICIENT S OCIAL C HOICE P ROBLEMS

auctions. We see that there is a very real sense in which it is only possible to address self-interested
in DCOPs when maximizing something like the total utility of participants.
4.2 Faithful Distributed Implementation
Our goal in faithful distributed implementation is to distribute the computation required to solve
the SCP and determine payments to the population of agents, but to do this while retaining an
analog to strategyproofness. This can be challenging because it opens up additional opportunities
for manipulation beyond those in the centralized VCG mechanism.
In presenting our results, we introduce the following additional assumptions over-and-above
those made so far:
 Agents are rational but helpful, meaning that although self-interested, they will follow a protocol whenever there is no deviation that will make them strictly better off (given the behavior
of other agents).
 Each agent is prevented from posing as several independent agents by an external technique
(perhaps cryptographic) for providing strong (perhaps pseudonymous) identities.
 Catastrophic failure will occur if all agents in the community of a variable do not eventually
choose the same value for the variable.
 There is a trusted bank, connected with a trusted communication channel to each agent, and
with the authority to collect payments from each agent.
The property of rational but helpful is required in being able to rely upon agents to compute the
payments that other agents should make. Strong identities is required to avoid known vulnerabilities
of the VCG mechanism as shown by Yokoo, Sakurai and Matsubara (2004), wherein agents can
sometimes do better by participating under multiple identities. Catastrophic failure ensures that
the decision determined by the protocol is actually executed. It prevents a hold-out problem,
where an unhappy agent refuses to adopt the consensus decision. An alternative solution would
be to have agents report the final decision to a trusted party, responsible for enforcement. By a
trusted communication channel, we mean that each agent can send a message to the bank without
interference by any other agent. These messages are only sent by an agent upon termination of
M-DPOP, to inform the bank about other agents payments. The bank is also assumed in other work
on distributed MD (Feigenbaum et al., 2002, 2006; Shneidman & Parkes, 2004), and is the only
trusted entity that we require. Its purpose is to ensure that payments can be used to align incentives.
To provide a formal definition of a distributed implementation we need the concept of a local
state. The local state of an agent Ai corresponds to the sequence of messages that the agent has
received and sent, together with the initial information available to an agent (including both its own
relations, and public information such as constraints). Given this, a distributed implementation,
dM =< g, , s >, is defined in terms of three components (Shneidman & Parkes, 2004; Parkes &
Shneidman, 2004):
 Strategy space, , which defines the set of feasible strategies i   available to agent Ai ,
where strategy i defines the message(s) that agent Ai will send in every possible local state.
 Suggested protocol, s = (s1 , . . . , sn ), which defines a strategy that is parameterized by the
private relations Ri of agent Ai .
731

fiP ETCU , FALTINGS , & PARKES

 Outcome rule, g = (g1 , g2 ), where g1 : n  D defines the assignment of values, g1 ()  D,
to variables X given a joint strategy,  = (1 , . . . , n )  n , and g2 : n  Rn defines the
payment g2,i ()  R made by each agent Ai given joint strategy   n .
By defining the message(s) that are sent in every state, a strategy i   encompasses all
computation performed internally to an agent, all information that an agent reveals about its private
inputs (e.g. its relations), and all decisions that an agent makes about how to propagate information
received as messages from other agents.9 The suggested protocol si corresponds to an algorithm,
which takes as input the private information available to an agent and relevant details about the
agents local state, and generates a message or messages to send to neighbors in the network. When
applied to distributed input R = (R1 , . . . , Rn ) and the known parts of the input such as hard
constraints C, the protocol s induces a particular execution trace of the algorithm. This in turn
induces the outcome g(), for  = s(R), where g1 () is the final assignment of values (information
about which is distributed across agents) and g2 () is the vector of payments that the bank will
collect from agents.10
The main question that we ask, given a distributed algorithm and its corresponding suggested
protocol, is whether the suggested protocol forms an ex post Nash equilibrium of the induced game:
Definition 8 (Ex post Nash equilibrium.) Given distributed implementation dM =< g, , s >,
the suggested protocol s = (s1 , . . . , sn ) is an an ex post Nash equilibrium (EPNE) if, for all agents
Ai , all relations Ri , all relations of other agents Ri , and all alternate strategies i  ,
Ri (g1 (si (Ri ), si (Ri )))  g2 (si (Ri ), si (Ri ))  Ri (g1 (i , si (Ri )))  g2 (i , si (Ri ))
(9)
In an EPNE, no agent Ai can benefit by deviating from protocol, si , whatever the particular
instance of DCOP (i.e. for all private relations R = (R1 , . . . , Rn )), so long as the other agents also
choose to follow the protocol. It is this latter requirement that makes EPNE weaker than dominantstrategy equilibrium, in which si would be the best protocol for agent i even if the other agents
followed an arbitrary protocol.
Definition 9 (Faithfulness) Distributed implementation, dM = < g, , s >, is ex post faithful if
suggested protocol s is an ex post Nash equilibrium.
That is, when a suggested protocol, s, is said to be ex post faithful (or simply faithful) then it is
in the best interest of every agent Ai to follow all aspects of the algorithm  information revelation,
computation and message-passing  whatever the private inputs of the other agents, as long as every
other agent follows the algorithm.
9. The idea that each agent only has a limited set of possible messages that can be sent in a local state  as implied by
the notion of a (restricted) strategy space   is justified in the following sense. Agents in the model are autonomous
and self-interested and, of course, free to send any message in any state. But on the other hand, and if the suggested
protocol is followed by every other agent, then only some messages will be semantically meaningful to the recipient
agent(s) and trigger a meaningful change in local state in the recipient agent(s); i.e. a change in local state that will
changes the future (external) behavior of the recipient agent. In this way, the strategy space characterizes the complete
set of interesting behaviors available to an agent given that the other agents follow the suggested protocol. This is
sufficient, from a technical perspecitve, to define an ex post Nash equilibrium.
10. The outcome rule must be well-defined for any unilateral deviation from s, i.e. where any one agent deviates and does
not follow the suggested protocol. Either the protocol still reaches a terminal state so that decisions and payments
are defined, or the protocol reaches some bad state with suitably negative utility to all participants, such as livelock
or deadlock. We neglect this latter possibility for the rest of our analysis, but it can be easily treated by introducing
special notation for this bad outcome.

732

fiM-DPOP: FAITHFUL D ISTRIBUTED I MPLEMENTATION OF E FFICIENT S OCIAL C HOICE P ROBLEMS

4.3 The Partition Principle Applied to Efficient Social Choice
One cannot achieve a faithful DI for efficient SCP by simply running DPOP, n + 1 times on the
same problem graph, once for the main problem and then with each agents effect nullified in turn
by asking it to simply propagate messages. Agent Ai would seek to do the following: (a) interfere
with the computational process for SCP (Ai ), to make the solution as close as possible to that
to SCP (A), so that its marginal impact appears small; and (b) otherwise decrease its payment, for
example by increasing the apparent utility of other agents for the solution to SCP (A), and in turn
increases the value of the second term in its VCG payment (Eq. 8).
This opportunity for manipulation was recognized by Parkes and Shneidman (2004) in a more
general setting, who proposed the partition principle as a method for achieving faithfulness in distributed VCG mechanisms, instantiated here in the context of efficient SCPs:
Definition 10 (partition principle) A distributed algorithm, corresponding to suggested protocol
s, satisfies the partition principle in application to efficient social choice, if:
1. (Correctness) An optimal solution is obtained for SCP (A) and SCP (Ai ) when every agent
follows s, and the bank receives messages that instruct it to collect the correct VCG payment
from every agent.
2. (Robustness) Agent Ai cannot influence the solution to SCP (Ai ), or the report(s) that
the bank receives about the negative externality that Ai imposes on the rest of the system
conditioned on solutions to SCP (A) and SCP (Ai ).
3. (Enforcement) The decision that corresponds to SCP (A) is enforced, and the bank collects
the payments as instructed.
Theorem 2 (Parkes & Shneidman, 2004) A distributed algorithm for efficient social choice that
satisfies the partition principle is an ex post faithful distributed implementation.
For some intuition behind this result, note that the opportunity for manipulation by an agent
Ai is now restricted to: (a) influencing the solution computed to SCP (A); and (b) influencing the
payments made by other agents. Agent Ai cannot prevent the other agents from correctly solving
SCP (Ai ) or from correctly reporting the negative externality that Ai imposes on the other agents
by its presence. As long as the other agents follow the algorithm, then ex post faithfulness follows
from the strategyproofness of the VCG mechanism because the additional opportunity for manipulation, over and above that available from misreporting preferences in the centralized context, is to
change (either increase or reduce) the amount of some other agents payment. This is opportunity
(b). Opportunity (a) is not new. An agent can always influence the solution in the context of a
centralized VCG mechanism by misreporting its preferences.
Remark: As has been suggested in previous work, the weakening from dominant-strategy equilibrium in the centralized VCG mechanism, to ex post Nash equilibrium in a distributed implementation, can be viewed as the cost of decentralization. The incentive properties necessarily rely on
the payments that are collected which rely in turn on the computation performed by other agents
and in turn on the strategy followed by other agents.11
11. An exception is provided by Izmalkov et al. (2005), who are able to avoid this through the use of cryptographic
primitives, in their case best thought of as physical devices such as ballot boxes.

733

fiP ETCU , FALTINGS , & PARKES

4.4 Simple M-DPOP
Algorithm 3 describes simple-M-DPOP. In this variation the main problem, SCP (A) is solved,
followed by the social choice problem, SCP (Ai ) with each agent removed in turn.12 Once these

n + 1 problems are solved, every agent Aj knows the local part of the solution to X  and Xi
for all Ai 6= Aj , which is the part of the solution that affects its own utility. This provides enough
information to allow the system of agents without some agent Ai , for any Ai , to each send a message
to the bank about a component of the payment that agent Ai should make.
Algorithm 3: Simple-M-DPOP.
1 Run DPOP for DCOP (A) on DFS (A); find X 
2 forall Ai  A do

3
Build DFS (Ai ); run DPOP for DCOP (Ai ) on DFS (Ai ); find Xi
 )  R (X  ) and report to bank.
4
All agents Aj P
6= Ai compute Tax j (Ai ) = Rj (Xi
j
5
Bank deducts j6=i Tax j (Ai ) from Ai s account
6

Each Ai assigns values in X  as the solution to its local COPi

The computation of payments
P is disaggregated across the agents. The tax payment collected
from agent Ai is Tax (Ai ) = j6=i Tax j (Ai ), where

Tax j (Ai ) = Rj (Xi
)  Rj (X  ),

(10)

is the component of the payment that occurs because of the negative effect that agent Ai has on the
utility of agent Aj . This information is communicated to the bank by agent Aj in the equilibrium.
The important observation, in being able to satisfy the partition principle, is that these components of Ai s payment satisfy a locality property, so that each agent Aj can compute this component of Ai s payment with just its private information about its relations and its local information
 that affect its own utility. All of this information is availabout the parts of solutions X  and Xi
able upon termination of simple-M-DPOP. Correctly determining this payment, once we condition
 , does not rely on any aspect of any other agents algorithm, including that
on solutions X  and Xi
13
of Ai .
Figure 5 provides an illustration of Simple M-DPOP on the earlier meeting scheduling example,
and shows how the marginal problems (and the DFS arrangements for each such problem) are
related to the main problem.
Theorem 3 The simple-M-DPOP algorithm is a faithful distributed implementation of efficient social choice and terminates with the outcome of the VCG mechanism.
P ROOF. To prove this we establish that simple-M-DPOP satisfies the partition principle and then
by appeal to Theorem 2. First, DPOP computes optimal solutions to SCP (A) and SCP (Ai ) for
12. Simple M-DPOP is presented for a setting in which the main problem and the subproblems are connected but extends
immediately to disconnected problems. Indeed, it may be that the main problem is connected but one or more
subproblems are disconnected. To see that there are no additional incentive concerns notice that it is sufficient to
recognize that the correctness and robustness properties of the partition principle would be retained in this case.
13. A similar disaggregation was identified by Feigenbaum et al. (2002) for lowest-cost interdomain routing on the
Internet. Shneidman and Parkes (2004) subsequently modified the protocol by those authors so that agents other than
Ai had enough information to report the payments to be made by agent Ai .

734

fiM-DPOP: FAITHFUL D ISTRIBUTED I MPLEMENTATION OF E FFICIENT S OCIAL C HOICE P ROBLEMS

Figure 5: Simple M-DPOP: Each agent Ai is excluded in turn from the optimization DCOP (Ai ). This is
illustrated on the meeting scheduling example.

all Ai  A when every agent follows the protocol. This is immediate because of the correctness
of the DCOP model of SCP and the correctness of DPOP. The correct VCG payments are collected
when every agent follows the algorithm by the correctness of the disaggregation of VCG payments
in Eq. 10. Second, agent Ai cannot influence the solution to SCP (Ai ) because it is not involved
in that computation in any way. The DFS arrangement is constructed, and the problem solved, by
the other agents, who completely ignore Ai and any messages that agent Ai might send. (Any hard
constraints that Ai may have handled in SCP (A) are reassigned automatically to some other agent
in SCP (Ai ) as a consequence of the fact that the DFS arrangement is reconstructed). DPOP
still solves SCP (Ai ) correctly in the case that the problem graph corresponding to SCP (Ai )
becomes disconnected (in this case the DFS arrangement is a forest). The robustness of the value of
the reports from agents 6= Ai about the negative externality imposed by Ai , conditioned on solutions
to SCP (A) and SCP (Ai ), follows from the locality property of payment terms Tax j (Ai ) for all
Aj 6= Ai . For enforcement, the bank is trusted and empowered to collect payments, and all agents
will finally set local copies of variables as in X  to prevent catastrophic failure. Agent Ai will
not deviate as long as other agents do not deviate. Moreover, if agent Ai is the only agent that is
interested in a variable then its value is already optimal for agent Ai anyway. 2
The partition principle, and faithfulness, has sweeping implications. Not only will each agent
follow the subtantive aspects of simple-M-DPOP, but each agent will also choose to faithfully participate in the community discovery phase, in any algorithm for choosing a root community, and in
selecting a leader agent in Phase one of DPOP.14
14. One can also observe that is not useful for an agent to misreport the local utility of another agent Aj while sending
UTIL messages around the system. On one hand, such a deviation could of course change the selection of X  or

Xk
for some k 6= {i, j} and thus the payments by other agents or the solution ultimately selected. But, by deviating

735

fiP ETCU , FALTINGS , & PARKES

Remark on Antisocial Behavior: Note that reporting exaggerated taxes hurts other agents but
does not increase ones own utility so this is excluded by our assumption that the agents are selfinterested but helpful.

5. M-DPOP: Reusing Computation While Retaining Faithfulness
In this section, we present our main result, which is the M-DPOP algorithm. In simple-M-DPOP, the
computation to solve the main problem is completely isolated from the computation to solve each
of the marginal problems. In comparison, in M-DPOP we re-use computation already performed in
solving the main problem in solving the marginal problems. This enables the algorithm to scale well
to problems where each agents influence is limited to a small part of the entire problem because
little additional computation is required beyond that of DPOP. These problems in which an agents
influence is limited are precisely those of interest because they are also those for which the induced
tree width is small and for which DPOP scales.
The challenge that we face, in facilitating this re-use of computation, is to retain the incentive
properties that are provided by the partition principle. A possible new manipulation is for agent
Ai to deviate in the computation in DCOP (A), with the intended effect to change the solution
to DCOP (Ai ) via the indirect impact of the computation performed in DCOP (A) when it is
reused in solving DCOP (Ai ). To prevent this, we have to determine which UTIL messages in
DCOP (A) could not have been influenced by agent Ai .
Example 7 Refer to Figure 6. Here agent Ai controls only X3 and X10 . Then it has no way of
influencing the messages sent in the subtrees rooted at {X14 , X15 , X2 , X7 , X5 , X11 }. We want to
be able to reuse as many of these UTIL messages as possible. In solving the problem with agent
Ai removed we will strive to construct a DFS i arrangement for problem DCOP (Ai ) that is as
similar as possible to the DFS for the main problem. This is done with the goal of maximizing the
re-use of computation across problems. See Figure 6(b). Notice that this is now a DFS forest, with
three distinct connected components. The UTIL messages that were sent by the shaded nodes can be
re-used in solving DCOP (Ai ). These are all the UTIL messages sent by nodes in the subtrees that
were not influenced by agent Ai except for {X14 , X15 , X5 } and also X9 , which now has a different
local DFS arrangement.
M-DPOP uses the safe reusability idea suggested by this example. See Algorithm 4. In its first
stage, M-DPOP solves the main problem just as in Simple-M-DPOP. Once this is complete, each
marginal problem DCOP (Ai ) is solved in parallel. To solve DCOP (Ai ), a DFS i forest (it
will be a forest in the case that DCOP (Ai ) becomes disconnected) is constructed as a modification
to DFS (A), retaining as much of the structure of DFS (A) as possible. A new DPOP (Ai )
execution is performed on the DFS i and U T IL messages are determined to be either reusable or
not reusable by the sender of the message based on the differences between DFS i and DFS (A).
We will explain below how DFS i is constructed.
in this way the agent cannot change the utility information that is finally used in determining its own payments. This
is because it is agent Aj itself that computes the marginal effect of agent Ai on its local solution, and component
Tax j (Ai ) of agent Ai s payment.

736

fiM-DPOP: FAITHFUL D ISTRIBUTED I MPLEMENTATION OF E FFICIENT S OCIAL C HOICE P ROBLEMS

Figure 6: Reconstructing DFS (Ai ) from DFS (A) in M-DPOP. The result is in general a DFS forest. The
bold nodes from main DFS initiate DFS i propagation. The one initiated by X5 is redundant
and eventually stopped by X9 . The ones from X4 and X15 are useful, as their subtrees become
really disconnected after removing Ai . X14 does not initiate any propagation since it has X1
as a pseudoparent. X1 is not controlled by Ai , and will eventually connect to X14 . Notice that
X0  X9 and X1  X14 are turned into tree edges.

5.1 Phase One of M-DPOP for a Marginal Problem: Constructing DFS i
Given a graph DCOP (A) and a DFS arrangement DFS (A) of DCOP (A), if one removes a set of
nodes X(Ai )  DCOP (A) (the ones that belong to Ai ), then we need an algorithm that constructs
a DFS arrangement, DFS i , for DCOP (A) \ X(Ai ). We want to achieve the following properties:
1. DFS i must represent a correct DFS arrangement for the graph DCOP (Ai ) (a DFS forest
in the case DCOP (Ai ) becomes disconnected).
2. DFS i must be constructed in a way that is non-manipulable by Ai , i.e. without allowing
agent Ai to interfere with its construction.
3. DFS i should be as similar as possible to DFS (A). This allows for reusing UTIL messages
from DPOP (A), and saves on computation and communication.
The main difficulty stems from the fact that removing the nodes that represent variables of interest to agent Ai from DFS (A) can create disconnected subtrees. We need to reconnect and possibly
rearrange the (now disconnected) subtrees of DFS (A) whenever this is possible. Return to the example in Figure 6. Removing agent Ai and nodes X3 and X10 disrupts the tree in two ways: some
subtrees become completely disconnected from the rest of the problem (e.g. X15  X18  X19 );
some other ones remain connected only via back-edges, thus forming an invalid DFS arrangement
737

fiP ETCU , FALTINGS , & PARKES

Algorithm 4: M-DPOP: faithfully reuses computation from the main problem.
1 Run DPOP for DCOP (A) on DFS (A); find X 
2 forall Ai  A do
in parallel
3

Create DFS i with Algorithm 5 by adjusting DFS (A)

4

Run DPOP for DCOP (Ai ) on DFS i :
if leaves in DFS i observe no changes in their DFS i then
they send null UTILi messages

5

6

7
8

else they compute their UTILi messages anew, as in DPOP
subsequently, all nodes Xk  DF S i do:
if Xk receives only null UTILi msgs  (Pk = Pki  P Pk = P Pki  Ck = Cki ) then
Xk sends a null UTILi message
else
node Xk computes its UTILi message, reusing:
forall Xl  N eighbors(Xk ) s.t. Xl sent UTILi = null do
Xk reuses the UTIL message Xl had sent in DCOP (A)
Compute and levy taxes as in simple-M-DPOP;
Each Ai assigns values in X  as the solution to its local COPi ;

(e.g. X5  X8  X9 ). The basic principle we use is to reconnect disconnected parts via back-edges
from DFS (A) whenever possible. This is intended to preserve as much of the structure of as possible. For example, in Figure 6, the back edge X0  X9 is turned into a tree edge, and X5 becomes
X9 s child. Node X8 remains X5 s child.
The DFS i reconstruction algorithm is presented in Algorithm 5. The high-level overview is
as follows (in bold we state the purpose of each step):
1. (Similarity to DFS (A) :) All nodes retain the DFS data structures from constructing
DFS (A); i.e., the lists of their children, pseudo parents/children, and their parents from
DFS (A). They will use this data as a starting point for building the DFS arrangements,
DFS (Ai ), for marginal problems.
2. (At least one traversal of each connected component on a DFS forest:) The root of
DFS (A) and the children15 of removed nodes each initiate a DFS i token passing as in
DFS (A), except for these changes:
 Each node Xk sends the token only to neighbors not owned by Ai .
 The order in which Xk sends the token to its neighbors is based on DFS (A): First Xk s
children from DFS (A), then its pseudochildren, then its pseudoparents, and then its
parent. This order helps preserve structure from DFS (A) into DFS (Ai ).
15. Children which have pseudoparents above the excluded node, for instance X14 in Figure 6, do not initiate DFS token
passing because it would be redundant: they would eventually receive a DFS token from their pseudoparent.

738

fiM-DPOP: FAITHFUL D ISTRIBUTED I MPLEMENTATION OF E FFICIENT S OCIAL C HOICE P ROBLEMS

Algorithm 5: Reconstruction of DFS i from DFS (A).
All data structures for the DFS i are denoted with superscript i .

Procedure Token passing for DFS i (executed by all nodes Xk 
/ X(Ai )) :
1
2

3
4
5

forall Xl  Neighbors(Xk ) s.t. Xl belongs to Ai do
Remove Xl from Neighbors(Xk ) and from Ck , PC k , PP k //i.e. send nothing to Ai
Sort Neighbors(Xk ) in this order: Ck , PC k , PP k , Pk //mimic DFS (A)
if Xk is root, or Pk  X(Ai ) (i.e. executed by the root and children of Ai ) then
Initiate DFS i as in normal DFS (Algorithm 2)
else do Process incoming tokens()
Send DFS i (Xk ) back to Pki // Xk s subtree completely explored
Procedure Process incoming tokens()

6
7
8
9
10
11

Wait for any incoming DFS i token; Let Xl be its sender
if Xl  Ai then ignore message
else
if this is first token received then
i
i
Pki = Xl ; PP i
k = {Xj 6= Pk |Xj  Neighbors(Xi )  DFS }
i
i
rootk = first node in the token DFS

17

else
let Xr be the first node in DFS i
i
traversal then
if Xr 6= rooti
k //i.e. this is another DFS
if depth of Xr in DFS (A) < depth of root i
k in DFS (A) then
i
i
i
i
Reset Pk , PP k , Ck , PC k //override redundant DFS from lower root
i
i
Pki = Xl ; PP i
k = {Xj 6= Pk |Xj  Neighbors(Xi )  DFS }
i
root k = Xr

18

Continue as in Algorithm 2

12
13
14
15
16

3. (Unique traversal of each connected component on a DFS forest:) Each node Xk retains
its root path in DFS (A) and knows its depth in the DFS arrangement. When a new token
DFS i arrives:
 If it is the first DFS i token that arrives, then the sender (let this be Xl ) is marked as
the parent of Xk in DFS i : Pki = Xl . Notice that Xl could be different from the
parent of Xk from DFS (A). Xk stores the first node from the received token DFS i ,
as root i
k : the (provisional) root of the connected component to which Xk belongs in
DCOP (Ai ).
 If this is not the first DFS i token that arrives, then there are two possibilities:
 the token received is part of the same DFS i traversal process. Xk recognizes
this by the fact that the first node in the newly received token is the same as the
previously stored root i
k . In this case, Xk proceeds as normal, as in Algorithm 2:
marks the sender as pseudochild, etc.
739

fiP ETCU , FALTINGS , & PARKES

 the token received is part of another DFS i traversal process, initiated by another
node than root i
k (see below in text for when this could happen). Let Xr be the first
node in the newly received token. Xk recognizes this situation by the fact that Xr
i
is not the same as the previously stored rooti
traversal
k . In this case, the DFS
initiated by the higher node in DFS (A) prevails, and the other one is dropped. To
determine which traversal to pursue and which one to drop, Xk compares the depths
i
of rooti
k and Xr in DFS (A). If Xr is higher, then it becomes the new rootk . Xk
overrides all the previous DFS i information with the one from the new token. It
then continues the token passing with the new token as in Algorithm 2.
To see why it is necessary to also start propagations from the children of removed nodes (step
2), consider again the example from Figure 6. Removing X10 and X3 completely disconnects the
subtree {X4 , X6 , X11 , X7 , X12 , X13 }. Had X4 not started a propagation, this subtree would not
have been visited at all since there are no connections between the rest of the problem and any
nodes in the subtree.16 17
Lemma 1 (DFS correctness) Algorithm 5 constructs a correct DFS arrangement (or forest),
DFS i for DCOP (Ai ) given a correct DFS arrangement DFS (A) for DCOP (A).
P ROOF. First, since a DFS i is started from each child of a node that was controlled by Ai , and also
from the root, it is ensured that each connected component is DFS-traversed at least once (follows
from Step 2). Second, each DFS process is similar to a normal DFS construction, in that each node
sends the token to all its neighbors (except for the ones controlled by Ai ); it is just that they do so in
a pre-specified order (the one given by DFS (A)). It follows that all nodes in a connected component
will eventually be visited (follows from Step 3). Third, higher-priority DFS traversals override the
lower priority ones (i.e. DFS traversals initiated by nodes higher in the tree have priority), again by
Step 3. Eventually one single DFS-traversal is performed in a single connected component. 2
Lemma 2 (DFS robustness) The DFS arrangement, DFS i , constructed by Algorithm 5 is nonmanipulable by agent Ai , for any input DFS arrangement from the solution phase to DCOP (A).
P ROOF. This follows directly from Step 3, since Ai does not participate in the process at all: its
neighbors do not send it any messages (see Algorithm 5, line 1), and any messages it may send are
simply ignored (see Algorithm 5, line 7) 2
In fact, no additional links are created while constructing DFS i . The only possible changes
are that some edges can reverse their direction (parents/children or pseudoparents-pseudochildren
16. Some of the DFS traversals initiated in Step 2 are redundant and the same part of the problem graph can be visited
more than once. The simple overriding rule in Step 3 ensures that only a single DFS i tree is eventually adopted
in each connected component, namely the one that is initiated by the highest node in the original DFS (A). For
example, in Figure 6, X5 starts an unnecessary DFS i propagation, which is eventually stopped by X9 , which
receives a higher priority DFS i token from X0 . Since X9 knows that X0 is higher in DFS (A) than X5 , it drops
the propagation initiated by X5 , and relays only the one initiated by X0 . It does so by sending X5 the token for
DFS i received from X0 to which it adds itself. Upon receiving the new token from X9 , node X5 realizes that
X9 is its new parent in DFS i . Thus, the redundant propagation initiated by X5 is eliminated and the result is a
consistent DFS subtree for the single connected component P1 .
17. A simple time-out mechanism can be used to ensure that each agent knows when its provisional DFS ordering is final
(i.e. no higher priority DFS traversals will arrive in the future).

740

fiM-DPOP: FAITHFUL D ISTRIBUTED I MPLEMENTATION OF E FFICIENT S OCIAL C HOICE P ROBLEMS

can switch places), and existing back-edges can turn into tree edges. Again, one can see this in
Figure 6.18
5.2 Phase Two of M-DPOP for a Marginal Problem: UTILi propagations
Once DFS i is built, the marginal problem without Ai is then solved on DFS i . Utility propagation proceeds as in normal DPOP except that nodes determine whether the UTIL message that was
sent in DPOP (A) can be reused. This is signaled to their parent by sending a special null UTIL
message. More specifically, the process is as follows:
 The leaves in DFS i initiate UTILi propagations:
1. If the leaves in DFS i observe no changes in their local DFS i arrangement as compared to DFS (A) then the UTIL message they sent in DCOP (A) remains valid and
they announce this to their parents by sending instead a null UTILi message.
2. Otherwise, a leaf node computes its UTIL message anew and sends it to their (new)
parent in DFS i .
 All other nodes wait for incoming UTILi messages and:
1. If every incoming messages a node Xk receives from its children is null and there are
no changes in the parent/pseudoparents then it can propagate a null UTILi message
to its parent.
2. Otherwise, Xk has to recompute its UTILi message. It does so by reusing all the UTIL
messages that it received in DCOP (A) from children that have sent it null messages in
DCOP (Ai ) and joining these with any new UTIL messages received.
For example, consider DCOP (Ai ) in Figure 6, where X16 and X17 are children of X14 . X14
has to recompute a UTIL message and send it to its new parent X1 . To do this, it can reuse the
messages sent by X16 and X17 in DCOP (A), because neither of these sending subtrees contain Ai .
16
By doing so, X14 reuses the effort spent in DCOP (A) to compute the messages UTIL16
20 , UTIL21 ,
14
UTIL14
16 and UTIL17 .
Theorem 4 The M-DPOP algorithm is a faithful distributed implementation of efficient social
choice and terminates with the outcome of the VCG mechanism.
P ROOF. From the partition principle and appeal to Theorem 3 (and in turn to Theorem 2). First,
agent Ai cannot prevent the construction of a valid DFS i for DCOP (Ai ) (Lemmas 1 and 2).
Second, agent Ai cannot influence the execution of DPOP on DCOP (Ai ) because all messages
that Ai influenced in the main problem DCOP (A) are recomputed by the system without Ai . The
rest of the proof follows as for simple-M-DPOP, leveraging the locality of the tax payment messages
and the enforcement provided by the bank and via the catastrophic failure assumption. 2
18. A simple alternative is to have children of all nodes Xki that belong to Ai , create a bypass link to the first ancestor of
Xki that does not belong to Ai . For example, in Figure 6, X4 and X5 could each create a link with X1 to bypass X3
completely in DFS (Ai ). However, additional communication links may be required in this approach.

741

fiP ETCU , FALTINGS , & PARKES

6. Experimental Evaluation: Understanding the Effectiveness of M-DPOP
We present the results of our experimental evaluation of DPOP, Simple M-DPOP and M-DPOP in
two different domains: distributed meeting scheduling problems (MS), and combinatorial auctions
(CAs). In the first set of experiments we investigate the performance of M-DPOP on a structured
constraint optimization problem (MS) which has received a lot of attention in cooperative distributed
constraint optimization. In the second set of experiments (CAs), we investigate unstructured domains, and observe the performance  specifically the ability to re-use computation in computing
payments  of M-DPOP with respect to problem density. CAs provide an abstract model of many
real world allocation problems and are much studied in mechanism design (Cramton, Shoham, &
Steinberg, 2006).
6.1 Distributed Meeting Scheduling
In distributed meeting scheduling, we consider a set of agents working for a large organization and
representing individuals, or groups of individuals, and engaged in scheduling meetings for some
upcoming period of time. Although the agents themselves are self interested, the organization as
a whole requires an optimal overall schedule, that minimizes cost (alternatively, maximizes the
utility of the agents). This makes it necessary to use a faithful distributed implementation such as
M-DPOP. In enabling this, we suppose that the organization distributes a virtual currency to each
agent (perhaps using this currency allocation to prioritize particular participants.) All relations held
by agents and defining an agents utility for a solution to the scheduling problem are thus stated in
units of this currency.
Each agent Ai has a set of local replicate variables Xji for each meeting Mj in which it is
involved. The domain of each variable Xj (and thus local replicas Xji ) represents the feasible
time slots for the meeting. An equality constraint is included between replica variables to ensure
that meeting times are aligned across agents. Since an agent cannot participate in more than one
meeting at once there is an all-different constraint on all variables Xij belonging to the same agent.
This is modeled as a clique constraint between these meeting variables. Each agent assigns a utility
to each possible time for each meeting by imposing a unary relation on each variable Xji . Each such
relation is private to Ai , and denotes how much utility Ai associates with starting meeting Mj at
each time t  dj , where dj is the domain for meeting Mj . The social objective is to find a schedule
in which the total utility is maximized while satisfying the all-different constraints for each agent.
Following Maheswaran et al. (2004), we model the organization by providing a hierarchical
structure. In a realistic organization, the majority of interactions are within departments, and only
a small number are across departments and even then these interactions will typically take place
between two departments adjacent in the hierarchy. This hierarchical organization provides structure
to our test instances: with high probability (around 70%) we generate meetings within departments,
and with a lower probability (around 30%) we generate meetings between agents belonging to
parent-child departments. We generated random problems having this structure, with an increasing
number of agents: from 10 to 100 agents. Each agent participates in 1 to 5 meetings, and has a
uniform random utility between 0 and 10 for each possible schedule for each meeting in which it
participates. The problems are generated such that they have feasible solutions.19
19. The test instances can be found at http://liawww.epfl.ch/People/apetcu/research/mdpop/MSexperiments.tgz

742

fiM-DPOP: FAITHFUL D ISTRIBUTED I MPLEMENTATION OF E FFICIENT S OCIAL C HOICE P ROBLEMS

For each problem size, we averaged the results over 100 different instances. We solved the main
problems using DPOP and the marginal ones using simple-M-DPOP, and M-DPOP respectively. All
experiments were performed in the FRODO multiagent simulation environment (Petcu, 2006), on
a 1.6Ghz/1GB RAM laptop. FRODO is a simulated multiagent system, where each agent executes
asynchronously in its own thread, and communicates with its peers only via message exchange.
The experiments were geared towards showing how much effort M-DPOP is able to reuse from
the main to the marginal problems. Figure 6.1 shows the absolute computational effort in terms
of number of messages (Figure 6.1(a)), and in terms of the total size of the messages exchanged,
in bytes (Figure 6.1(b)). The curves for DPOP represent just the number of messages (total size
of messages, respectively) required for solving the cooperative problem. The curves for simpleM-DPOP and M-DPOP represent the total number (size, respectively) of UTIL messages, for both
main and marginal economies.
We notice several interesting facts. First, the number of messages required by DPOP increases
linearly with the number of agents because DPOPs complexity in terms of number of messages is
always linear in the size of the problem. On the other hand, the number of messages of simple-MDPOP increases roughly quadratically with the number of agents, since it solves a linear number
of marginal economies from scratch using DPOP, each requiring a linear number of messages.
The performance of M-DPOP lies somewhere between the DPOP and simple-M-DPOP with more
advantage achieved over simple-M-DPOP as the size of the problem increases, culminating with
almost an order of magnitude improvement over Simple M-DPOP for the largest problem sizes (i.e.
with 100 agents in the problem). Similar observations can be made about the total size of the UTIL
messages, also a good measure of computation, traffic and memory requirements, by inspecting
Figure 6.1(b). For both metrics we find that the performance of M-DPOP is only slightly superlinear in the size of the problem.
Figure 8 shows the percentage of the additional effort required for solving the marginal problems
that can be reused from the main problem, i.e. the probability that a UTIL message required in solving a marginal problem can be taken directly from the message already used in the main problem.
We clearly see that as the problem size increases we can actually reuse more and more computation
from the main problem. The intuition behind this is that in large problems, each individual agent
is localized in a particular area of the problem. This translates into the agent being localized in a
specific branch of the tree, thus rendering all computation performed in other branches reusable for
the marginal problem that corresponds to that respective agent. Looking also at the percentage of
reuse when defined in terms of message size rather than the number of messages we see that this is
also trending upwards as the size of the problem increases.
6.2 Combinatorial Auctions
Combinatorial Auctions (CAs) are a popular means to allocate resources to multiple agents. In CAs,
bidders can bid on bundles of goods (as opposed to bidding on single goods). Combinatorial bids
can model both complementarity and substitutability among the goods, i.e. when the valuation for
the bundle is more, respectively less than the sum of the valuations for individual items. In our
setting the agents are distributed (geographically or logically), and form a problem graph in which
neighbors are agents with whom their bids overlap. The objective is to find the feasible solution (i.e.
declare bids as winning or losing such that no two winning bids share a good) that maximizes the
total utility of the agents.
743

fiP ETCU , FALTINGS , & PARKES

100000

1e+07

10000
# of messages

DPOP
simple_M-DPOP
M-DPOP

Total Size of UTIL Messages

DPOP
simple_M-DPOP
M-DPOP

1000

100

1e+06

100000

10000

10

1000
10

20

30

40

50

60

70

80

90 100

10

20

Number of agents

30

40

50

60

70

80

90 100

Number of agents

(a) Number of messages

(b) Total size of UTIL messages (in valuations)

Figure 7: Meeting scheduling problem: measures of absolute computational effort (in terms of the number

% of effort for marginals reused from main

of messages sent and the total size of the UTIL messages) in DPOP, simple-M-DPOP and MDPOP. The curves for DPOP represent effort spent just on the main problem, while the ones for
simple-M-DPOP and M-DPOP represent total effort over the main and the marginal problems.

90
85
80
75
70
65
60
55

Total information
Number of messages

50
10

20

30

40

50

60

70

80

90

100

Number of agents

Figure 8: Meeting scheduling problem: Percentage of effort required for the marginal problems that is
reused by M-DPOP from the main problem. Reuse is measured both in terms of the percentage
of the UTIL messages that can be reused (dashed) and also in terms of the total size of the UTIL
messages that are reused as a fraction of the total UTIL message size (solid).

744

fiM-DPOP: FAITHFUL D ISTRIBUTED I MPLEMENTATION OF E FFICIENT S OCIAL C HOICE P ROBLEMS

CAs are adopted here as a stylized model of distributed allocation problems such as airport slot
allocation and wireless spectrum allocation as discussed in the Introduction. The CA instances also
provide a counterpoint to the meeting scheduling problems because they represent problems with
less structure. In our DCOP model, each agent holds a variable for each one of its bids, with two
possible values: 0 when the bid is rejected, and 1 when the bid is accepted. Any pair of overlapping
bids (bids that share at least one good) is connected by a at most one constraint that specifies
that they cannot be both accepted. When multiple bids are submitted by an agent then they can be
connected by additional constraints to capture the bid logic, for instance exclusive-or constraints if
only one bid can be accepted.
We generated random problems using CATS (Leyton-Brown, Pearson, & Shoham, 2000), using
the L3 distribution from Sandholm (2002). L3 is the Constant distribution in which each agent
demands a bundle of 3 goods, selected uniformly at random, and with a value distributed uniformly
on [0, 1]. In our simulations we consider a market with 50 goods and vary the number of agents
between 5 and 40. We recorded the performance of DPOP, simple-MDPOP and M-DPOP in the
graphs from Figures 9 and 10. Figure 9 shows that as the density of the problems increase, all three
algorithms require more effort in solving them (both in terms of number of messages, and in terms
of total information exchange).
Figure 10 shows how reusability varies with problem density: one can see that for loose problems the reusability is very good, close to 100% for problems with 5 agents. As the density of the
problems increases with the number of agents, reusability decreases as well, and is around 20% for
the most dense problems, with 40 agents. We explain this phenomenon as follows: for very loose
problems (many goods and few bidders), the bids are mostly non-overlapping, which in turn ensures
that removing individual agents for solving the marginal problems does not affect the computation
performed while solving the main problem. At the other end of the spectrum, very dense problems
tend to be highly connected, which produces DFS trees which are very similar to chains. In such a
case, removing agents which are close to the bottom of the chain invalidates much of the computation performed while solving the main problem. Therefore, only a limited amount of computation
can be reused.
While noting that L3 is recognized as one of the hardest problem distributions in the CATS
suite (Leyton-Brown et al., 2000), we remark that we need to limit our experiments to this distribution because other problems have a large induced tree width (and high density problem graphs).
Consider for example a problem in which every agent bids for a bundle that overlaps with every
other agent. The problem graph is a clique and DPOP does not scale. While we leave a detailed
examination for future work, a recent extension of DPOP  H-DPOP (Kumar, Petcu, & Faltings,
2007)  can immediately address this issue. In H-DPOP, consistency techniques are used in order
to compactly represent UTIL messages, and on tightly constrained problems, orders of magnitude
improvements over DPOP are reported (see Section 7.1).

7. Discussion
In this section we discuss alternatives for improving the computational performance of M-DPOP,
the possibility of faithful variations of other DCOP algorithms (ADOPT (Modi et al., 2005) and
OptAPO (Mailler & Lesser, 2004)), and the loss in utility for the agents that can occur due to the
transfer of payments to the bank, mentioning an approach to address this problem.
745

fiP ETCU , FALTINGS , & PARKES

10000

1e+08
Total Size of UTIL Messages

DPOP
simple_M-DPOP
M-DPOP
# of messages

1000

100

10

DPOP
simple_M-DPOP
M-DPOP

1e+07
1e+06
100000
10000
1000
100
10

1

1
5

10

15
20
25
30
Number of agents

35

40

5

(a) Number of messages

10

15
20
25
30
Number of agents

35

40

(b) Total size of UTIL messages (in valuations)

Figure 9: Combinatorial Auctions problems: measures of absolute computational effort (in terms of the

% of effort for marginals reused from main

number of messages sent and the total size of the UTIL messages) in DPOP, simple-M-DPOP and
M-DPOP. The curves for DPOP represent effort spent just on the main problem, while the ones
for simple-M-DPOP and M-DPOP represent effort on both the main and the marginal problems.
The higher the number of agents (and thus bids, and thus constraints in the problem graph and
problem density), the greater the computational effort to solve the problem.

100
90
80
70
60
50
40
30
Total information
Number of messages

20
10
5

10

15

20

25

30

35

40

Number of agents

Figure 10: Combinatorial Auctions problems: Percentage of effort required for the marginal problems that
is reused by M-DPOP from the main problem. Reuse is measured both in terms of the percentage
of the UTIL messages that can be reused (dashed) and also in terms of the total size of the UTIL
messages that are reused as a fraction of the total UTIL message size (solid).

746

fiM-DPOP: FAITHFUL D ISTRIBUTED I MPLEMENTATION OF E FFICIENT S OCIAL C HOICE P ROBLEMS

7.1 Algorithmic Alternatives for Improved Performance
M-DPOP scales very well with problem size as long as the induced width of the problem remains
low. This is a characteristic M-DPOP inherits from DPOP, on which it is based. For problems
with high induced width, DPOP/M-DPOP require producing, sending and storing large messages,
which may be unfeasible or undesirable. To mitigate this problem, several advances to the basic
DPOP algorithm have been recently proposed. Some of these new algorithms sacrifice optimality in
return for computational tractability, which makes them difficult to combine with a VCG payment
mechanism in such a way that faithfulness be guaranteed. Nevertheless, H-DPOP (Kumar et al.,
2007) and MB-DPOP (Petcu & Faltings, 2007) employ two different techniques that preserve the
optimality guarantees, and can be fitted to M-DPOP.
H-DPOP leverages the observation that many real problems contain hard constraints that significantly reduce the space of feasible assignments. For example, in auctions, it is not possible to
allocate an item to more than one bidder. In meeting scheduling, it is not possible to set two different start times for a given meeting. Unfortunately, DPOP does not take advantage of the pruning
power of these hard constraints, and sends messages that explicitly represent all value combinations, including many infeasible ones. H-DPOP addresses this issue by using Constraint Decision
Diagrams (CDD) introduced by Cheng and Yap (2005) to compactly represent UTIL messages by
excluding unfeasible combinations. Performance improvements of several orders of magnitude can
be achieved, especially on highly constrained problems (Kumar et al., 2007).
MB-DPOP (Petcu & Faltings, 2007) uses the idea of cycle cutsets (Dechter, 2003) to explore
parts of the search space sequentially. Dense parts of the problem are explored by iterating through
assignments of a subset of nodes designated as cycle cuts, and for each assignment performing
a limited UTIL propagation similar to the one from DPOP. Easy parts of the problem are explored
with one-shot UTIL messages, exactly as in DPOP. MB-DPOP offers thus a configurable tradeoff
between the number of the messages exchanged, and the size of these messages and the memory
requirements.
7.2 Achieving Faithfulness with other DCOP Algorithms
The partition principle, described in Section 4.3, is algorithm independent. The question as to
whether another, optimal DCOP algorithm can be made faithful therefore revolves, critically, around
whether the algorithm will satisfy the robustness requirement of the partition priciple. We make the
following observations:
 Robustness in the first sense, i.e. that no agent Ai can influence the solution to the efficient SCP without agent Ai , is always achievable at the cost of restarting computation on the
marginal problem with each agent removed in turn, just as we proposed for simple-M-DPOP.
 Robustness in the second sense, i.e. that no agent Ai can influence the report(s) that the bank
receives about the negative externality that Ai imposes on the rest of the system, conditioning
on the solutions to the main problem and the problem without Ai , requires that the DCOP
algorithm terminates with every agent knowing the part of the solution that is relevant in
defining its own utility; the robustness property then follows by disaggregation of payments.
Thus, if one is content to restart the DCOP algorithm multiple times, then the same kinds of
results that we provide for simple-M-DPOP are generally available. This is possible because of
747

fiP ETCU , FALTINGS , & PARKES

the already mentioned locality property of payments, which follows from the disaggregation of
the VCG payment across agents in Eq. (10) and because of the information and communication
structure of DCOP.
The other useful property of DCOP in the context of self-interested agents, and worth reemphasizing, is that it is possible to retain faithfulness even when one agent plays a pivotal role in
connecting the problem graph. Suppose that problem, DCOP (Ai ), becomes disconnected without Ai . But, if this is the case then its optimal solution is represented by the union of the optimal
solutions in each connected subcomponent of the problem, and no information needs to flow between disconnected components either for the purpose of solving the problem or for the purpose of
reporting the components of agent Ai s tax.
We discuss in the following two sections the adaptation of the two other most prominent complete DCOP algorithms: ADOPT (Modi et al., 2005) and OptAPO (Mailler & Lesser, 2004).
We discuss in the following two sections the adaptation of the two other most prominent complete DCOP algorithms: ADOPT (Modi et al., 2005) and OptAPO (Mailler & Lesser, 2004). We
consider the computational aspects of making these algorithms faithful, specifically the issues related to the efficient handling of replica variables and to providing for reusability from the main to
the marginal problems.
7.2.1 U SING ADOPT

FOR

FAITHFUL , E FFICIENT S OCIAL C HOICE

ADOPT is a polynomial-space search algorithm for DCOP that is guaranteed to find the globally
optimal solution while allowing agents to execute asynchronously and in parallel. The agents in
ADOPT make local decisions based on conservative cost estimates. ADOPT also works on a DFS
arrangement, constructed as detailed in Section 3.1.1. Roughly speaking, the main process that is
executed in ADOPT is a backtrack search on the DFS tree.
Adaptation of ADOPT to the DCOP Model with Replicated Variables. ADOPTs complexity
is given by the number of messages, which is exponential in the height of the DFS tree. Similar to
DPOP, using the DCOP model with replicated variables could artificially increase the complexity
of the solving process. Specifically, the height of the DFS tree is increased when using replicated
variables compared to the centralized problem graph. ADOPT can be modified to exploit the special
structure of these replicated local variables in a similar way as DPOP. Specifically, ADOPT should
explore sequentially only the values of the original variable, and ignore assignments where replicas
of the same variable take different values. This works by allowing just the agent that owns the
highest replica of each variable to freely choose values for the variable. This agent then announces
the new value of the variable to all other agents owning replicas of the variable. These other agents
would then consider just the announced value for their replicas, add their own corresponding utilities, and continue the search process. Using this special handling of the replica variables, the
resulting complexity is no longer exponential in the height of the distributed DFS tree, but in the
height of the DFS tree obtained by traversing the original problem graph. For example, in Figure 2, it is sufficient to explore the values of M32 , and directly assign these values to M33 and M31
via VALUE messages, without trying all the combinations of their values. This reduces ADOPTs
complexity from exponential in 6, to exponential in 3.
Reusability of Computation in ADOPT. Turning to the re-use of computation from the main to
the marginal problems, we note that because ADOPT uses a DFS arrangement then it is easy to
identify which parts of the DFS arrangement for the main problem are impossible for an agent to
748

fiM-DPOP: FAITHFUL D ISTRIBUTED I MPLEMENTATION OF E FFICIENT S OCIAL C HOICE P ROBLEMS

manipulate, and therefore can be reused while computing the solution to the marginal problem
with that agent removed. However, a major difference between DPOP and ADOPT is that in DPOP,
each agent stores its outgoing UTIL message, and thus has available all the utilities contingent to
all assignments of the variables in the agents separator. This makes it possible for the agent to
simply reuse that information in all marginal economies where the structure of the DFS proves that
this is safe. In contrast, ADOPT does not store all this information because of its linear memory
policy. This in turn makes it impossible to reuse computation from the main problem to the marginal
problems. All marginal problems have to be solved from scratch, and thus the performance would
scale poorly as problem size increases and even in structured problems such as meeting scheduling.
We see two alternatives for addressing this problem: (a) renounce linear memory guarantees,
and use a caching scheme like for example in NCBB (Chechetka & Sycara, 2006): this would allow
for a similar reusability as in M-DPOP, where previously computated utilities can be extracted from
the cache instead of having to be recomputed. Alternatively, (b) one can devise a scheme where
the previously computed best solution can be saved as a reference, and subsequently used as an
approximation while solving the marginal problems. This could possibly provide better bounds and
thus allow for better pruning, such that some computation could be saved. Both these alternatives
are outside the scope of this paper, and considered for future work.
7.2.2 U SING O PTAPO

FOR

FAITHFUL , E FFICIENT S OCIAL C HOICE

OptAPO (Mailler & Lesser, 2004) is the other most popular algorithm for DCOP. Similar to the
adaptations of DPOP and ADOPT to social choice, OptAPO can also be made to take advantage of
the special features of the DCOP model with replicated variables. Its complexity then would not
be artificially increased by the use of this DCOP model. OptAPO has the particularity that it uses
mediator agents to centralize subproblems and solve them in dynamic and asynchronous mediation sessions, i.e. partial centralization. The mediator agents then announce their results to the other
agents, who have previously sent their subproblems to the mediators. This process alone would
introduce additional possibility for manipulation in a setting with self interested agents. However,
using the VCG mechanism addresses this concern and agents will choose to behave correctly according to the protocol.
As with ADOPT, the main issue with using OptAPO for faithful social choice is the reusability
of computation from the main to the marginal problems. Specifically, consider that while solving the
main problem, a mediator agent Ai has centralized and aggregated the preferences of a number of
other agents, while solving mediation problems as dictated by the OptAPO protocol. Subsequently,
when trying to compute the solution to the marginal problem without agent Ai , all this computation
has to go to waste, as it could have been manipulated by Ai while solving the main problem. Furthermore, since OptAPOs centralization process is asynchronous and conflict-driven as opposed
to structure-driven as in M-DPOP, it is unclear whether any computation from the main problem
could be safely reused in any of the marginal problems. To make matters worse, experimental studies (Davin & Modi, 2005; Petcu & Faltings, 2006) show that in many situations, OptAPO ends up
relying on a single agent in the system to centralize and solve the whole problem. This implies
that while solving the marginal problem without that agent, one can reuse zero effort from the main
problem.
749

fiP ETCU , FALTINGS , & PARKES

7.3 Loss in Utility due to Wasting the VCG Taxes
In the VCG mechanism, each agents net utility is the difference between the utility it derives from
the optimal solution and the VCG tax it has to pay. The net utility of the whole group of agents is
the sum of individual net utilities of the agents, i.e. the total utility from the assignment of values
to variables but net of the total payment made by agents to the bank. This loss in utility while
using M-DPOP can be as great as 35% of the total utility of the optimal solution in the meeting
scheduling domain. As the problem size increases, more and more money has to be burnt in the
form of VCG taxes. Similar waste has been observed by others; e.g., Faltings (2004), also in the
context of efficient social choice.
One cannot naively redistribute the payment back to the agents, for instance sharing the payments equally across all agents would break faithfulness. For example, agent Ai would prefer for
the other agents to make greater payments, in order to receive a larger repayment from the bank.
The faithfulness properties of M-DPOP would unravel. On the other hand, when the problem has
inherent structure then it is possible to redistribute some fraction of the payments back to agents.
This idea of careful redistribution was suggested in Bailey (1997), and subsequently extended by
Cavallo (2006), Guo and Conitzer (2007) and Moulin (2007). Another approach, advocated for example by Faltings (2004), is to simply preclude an agent from the problem and transfer the payments
to this agent. All this work is in a centralized context.
An important issue for future work, then, is to study the budget surplus that accrues to the bank
in M-DPOP and seek to mitigate this welfare loss in a setting of distributed implementation. We
defer any further discussion of this topic to future work, in which we will investigate methods to
leverage structure in the problem in redistributing the majority of these payments back to agents
without compromising either efficiency or faithfulness.

8. Conclusions
We have developed M-DPOP, which is a faithful, distributed algorithm with which to solve efficient
social choice problems in multi-agent systems with private information and self-interest. No agent
can improve its utility either by misreporting its local information or deviating from any aspect of
the algorithm (e.g., computation, message-passing, information revelation.) The only centralized
component is that of a bank that is able to receive messages about payments and collect payments.
In addition to promoting efficient decisions, we minimize the amount of additional computational
effort required for computing the VCG payments by reusing effort from the main problem. A first
set of experimental results shows that a significant amount of the computation required in all the
marginal problems can be reused from the main problem, sometimes above 87%. This provides
near-linear scalability in massive, distributed social choice problems that have local structure so
that the maximal induced tree width is small. A second set of experiments performed on problems
without local structure shows that as the problem density increases, the amount of effort required increases, and the reusability of computation decreases. These results suggest that M-DPOP is a very
good candidate for solving loose problems that exhibit local structure such that the induced width
remains small. In addition to addressing the need to reduce the total payments made by agents to the
bank, one issue for future work relates to the need to provide robustness when faced with adversarial
or faulty agents: the current solution is fragile in this sense, with its equilibrium properties relying
on other agents following the protocol. Some papers (Lysyanskaya & Triandopoulos, 2006; Aiyer,
Alvisi, Clement, Dahlin, Martin, & Porth, 2005; Shneidman & Parkes, 2003) provide robustness to
750

fiM-DPOP: FAITHFUL D ISTRIBUTED I MPLEMENTATION OF E FFICIENT S OCIAL C HOICE P ROBLEMS

mixture models (e.g. some rational, some adversarial) but we are not aware of any work with these
mixture models in the context of efficient social choice. Another interesting direction is to find ways
to allow for approximate social choice, for example with memory-limited DPOP variations (Petcu
& Faltings, 2005a) while retaining incentive properties, perhaps in approximate equilibria. Future
research should also consider the design of distributed protocols that are robust against false-name
manipulations in which agents can participate under multiple pseudonyms (Yokoo et al., 2004),
and seek to mitigate the opportunities for collusive behavior and the possibility of multiple equilibria that can exist in incentive mechanisms (Ausubel & Milgrom, 2006; Andelman, Feldman, &
Mansour, 2007; Katz & Gordon, 2006).

Acknowledgments
Parkes is supported in part by National Science Foundation grants IIS-0238147, IIS-0534620 and an
Alfred P. Sloan Foundation award. Petcu was supported by the Swiss National Science Foundation
grant 200020-103421/1. The authors would like to thank Wei Xue for valuable feedback on several
parts of the paper. We thank Jeffrey Shneidman for his feedback on an early version of this paper.
We also thank Aaron Bernstein for valuable insights on the DFS reconstruction process. The three
anonymous reviewers also provided excellent suggestions for improving the exposition of this work.
An earlier version of this paper appeared in the Proc. Fifth International Joint Conference on
Autonomous Agents and Multiagent Systems (AAMAS), 2006.

References
Abu-Amara, H. H. (1988). Fault-tolerant distributed algorithm for election in complete networks. IEEE
Trans. Comput., 37(4), 449453.
Aiyer, A. S., Alvisi, L., Clement, A., Dahlin, M., Martin, J.-P., & Porth, C. (2005). Bar fault tolerance for
cooperative services. In 20th ACM Symposium on Operating Systems Principles.
Andelman, N., Feldman, M., & Mansour, Y. (2007). Strong price of anarchy. In ACM-SIAM Symposium on
Discrete Algorithms 2007 (SODA07).
Arnborg, S. (1985). Efficient algorithms for combinatorial problems on graphs with bounded decomposability
- a survey. BIT, 25(1), 223.
Ausubel, L., Cramton, P., & Milgrom, P. (2006). The clock-proxy auction: A practical combinatorial auction
design. In Cramton et al. (Cramton et al., 2006), chap. 5.
Ausubel, L., & Milgrom, P. (2006). The lovely but lonely Vickrey auction. In Cramton et al. (Cramton et al.,
2006), chap. 1.
Bailey, M. J. (1997). The demand revealing process: To distribute the surplus. PublicChoice, 107126.
Ball, M., Donohue, G., & Hoffman, K. (2006). Auctions for the safe, efficient, and equitable allocation of
airspace system resources. In Cramton, Shoham, S. (Ed.), Combinatorial Auctions. MIT Press.
Barbosa, V. (1996). An Introduction to Distributed Algorithms. The MIT Press.
Bayardo, R., & Miranker, D. (1995). On the space-time trade-off in solving constraint satisfaction problems.. In Proceedings of the 15th International Joint Conference on Artificial Intelligence, IJCAI-95,
Montreal, Canada.
Bidyuk, B., & Dechter, R. (2004). On finding minimal w-cutset. In AUAI 04: Proceedings of the 20th
conference on Uncertainty in artificial intelligence, pp. 4350, Arlington, Virginia, United States.
AUAI Press.
751

fiP ETCU , FALTINGS , & PARKES

Bikhchandani, S., de Vries, S., Schummer, J., & Vohra, R. V. (2002). Linear programming and Vickrey
auctions. In Dietrich, B., & Vohra, R. (Eds.), Mathematics of the Internet: E-Auction and Markets, pp.
75116. IMA Volumes in Mathematics and its Applications, Springer-Verlag.
Cavallo, R. (2006). Optimal decision-making with minimal waste: Strategyproof redistribution of vcg payments. In Proc. of the 5th Int. Joint Conf. on Autonomous Agents and Multi Agent Systems (AAMAS06).
Chechetka, A., & Sycara, K. (2006). An any-space algorithm for distributed constraint optimization. In
Proceedings of AAAI Spring Symposium on Distributed Plan and Schedule Management.
Cheng, K. C. K., & Yap, R. H. C. (2005). Constrained decision diagrams.. In Proceedings of the National
Conference on Artificial Intelligence, AAAI-05, pp. 366371, Pittsburgh, USA.
Cheung, T.-Y. (1983). Graph traversal techniques and the maximum flow problem in distributed computation..
IEEE Trans. Software Eng., 9(4), 504512.
Cidon, I. (1988). Yet another distributed depth-first-search algorithm. Inf. Process. Letters, 26(6), 301305.
Collin, Z., Dechter, R., & Katz, S. (1991). On the Feasibility of Distributed Constraint Satisfaction. In
Proceedings of the 12th International Joint Conference on Artificial Intelligence, IJCAI-91, pp. 318
324, Sidney, Australia.
Collin, Z., Dechter, R., & Katz, S. (1999). Self-stabilizing distributed constraint satisfaction. Chicago Journal
of Theoretical Computer Science.
Collin, Z., & Dolev, S. (1994). Self-stabilizing depth-first search. Information Processing Letters, 49(6),
297301.
Cramton, P., Shoham, Y., & Steinberg, R. (Eds.). (2006). Combinatorial Auctions. MIT Press.
Davin, J., & Modi, P. J. (2005). Impact of problem centralization in distributed constraint optimization
algorithms. In AAMAS 05: Proceedings of the fourth international joint conference on Autonomous
agents and multiagent systems, pp. 10571063, New York, NY, USA. ACM Press.
Davis, R., & Smith, R. G. (1983). Negotiation as a metaphor for distributed problem solving. Artificial
Intelligence, 63109.
de Vries, S., & Vohra, R. V. (2003). Combinatorial auctions: A survey. Informs Journal on Computing, 15(3),
284309.
Dechter, R. (2003). Constraint Processing. Morgan Kaufmann.
Dechter, R., & Mateescu, R. (2006). AND/OR search spaces for graphical models. Artificial Intelligence. To
appear.
Dunne, P. E. (2005). Extremal behaviour in multiagent contract negotiation. Journal of Artificial Intelligence
Research (JAIR), 23, 4178.
Dunne, P. E., Wooldridge, M., & Laurence, M. (2005). The complexity of contract negotiation. Artificial
Intelligence Journal, 164(1-2), 2346.
Endriss, U., Maudet, N., Sadri, F., & Toni, F. (2006). Negotiating socially optimal allocations of resources.
Journal of Artificial Intelligence Research, 25, 315348.
Ephrati, E., & Rosenschein, J. (1991). The Clarke tax as a consensus mechanism among automated agents.
In Proceedings of the National Conference on Artificial Intelligence, AAAI-91, pp. 173178, Anaheim,
CA.
Faltings, B. (2004). A budget-balanced, incentive-compatible scheme for social choice. In Workshop on
Agent-mediated E-commerce (AMEC) VI. Springer Lecture Notes in Computer Science.
Faltings, B., Parkes, D., Petcu, A., & Shneidman, J. (2006). Optimizing streaming applications with selfinterested users using M-DPOP. In COMSOC06: International Workshop on Computational Social
Choice, pp. 206219, Amsterdam, The Netherlands.
752

fiM-DPOP: FAITHFUL D ISTRIBUTED I MPLEMENTATION OF E FFICIENT S OCIAL C HOICE P ROBLEMS

Feigenbaum, J., Papadimitriou, C., Sami, R., & Shenker, S. (2002). A BGP-based mechanism for lowest-cost
routing. In Proceedings of the 2002 ACM Symposium on Principles of Distributed Computing, pp.
173182.
Feigenbaum, J., Ramachandran, V., & Schapira, M. (2006). Incentive-compatible interdomain routing. In
Proceedings of the 7th Conference on Electronic Commerce, pp. 130139.
Feigenbaum, J., & Shenker, S. (2002). Distributed Algorithmic Mechanism Design: Recent Results and
Future Directions. In Proceedings of the 6th International Workshop on Discrete Algorithms and
Methods for Mobile Computing and Communications, pp. 113.
Freuder, E. C., & Quinn, M. J. (1985). Taking advantage of stable sets of variables in constraint satisfaction
problems. In Proceedings of the 9th International Joint Conference on Artificial Intelligence, IJCAI85, pp. 10761078, Los Angeles, CA.
Gershman, A., Meisels, A., & Zivan, R. (2006). Asynchronous forward-bounding for distributed constraints
optimization. In Proceedings of the 17th European Conference on Artificial Intelligence (ECAI-06),
Riva del Garda, Italy.
Greenstadt, R., Pearce, J. P., & Tambe, M. (2006). Analysis of privacy loss in distributed constraint optimization. In Proc. of Twenty-First National Conference on Artificial Intelligence (AAAI-06).
Guo, M., & Conitzer, V. (2007). Worst-case optimal redistribution of vcg payments. In Proceedings of the
8th ACM Conference on Electronic Commerce (EC-07), pp. 3039.
Huebsch, R., Hellerstein, J. M., Lanham, N., et al. (2003). Querying the Internet with PIER. In VLDB.
Izmalkov, S., Micali, S., & Lepinski, M. (2005). Rational secure computation and ideal mechanism design.
In FOCS 05: Proceedings of the 46th Annual IEEE Symposium on Foundations of Computer Science,
pp. 585595, Washington, DC, USA. IEEE Computer Society.
Jackson, M. O. (2000). Mechanism theory. In The Encyclopedia of Life Support Systems. EOLSS Publishers.
Jackson, M. O. (2001). A crash course in Implementation theory. Social Choice and Welfare, 18(4), 655708.
Katz, J., & Gordon, S. D. (2006). Rational secret sharing, revisited. In Proc. Security and Cryptography for
Networks.
Kloks, T. (1994). Treewidth, Computations and Approximations, Vol. 842 of Lecture Notes in Computer
Science. Springer.
Krishna, V. (2002). Auction Theory. Academic Press.
Kumar, A., Petcu, A., & Faltings, B. (2007). H-DPOP: Using hard constraints to prune the search space. In
IJCAI07 - Distributed Constraint Reasoning workshop, DCR07, pp. 4055, Hyderabad, India.
Lavi, R., Mualem, A., & Nisan, N. (2003). Towards a characterization of truthful combinatorial auctions. In
Proc. 44th Annual Symposium on Foundations of Computer Science.
Leyton-Brown, K., Pearson, M., & Shoham, Y. (2000). Towards a universal test suite for combinatorial
auction algorithms. In Proceedings of ACM Conference on Electronic Commerce, EC-00, pp. 235
245.
Leyton-Brown, K., & Shoham, Y. (2006). A test suite for combinatorial auctions. In Cramton, P., Shoham,
Y., & Steinberg, R. (Eds.), Combinatorial Auctions, chap. 18. MIT Press.
Lysyanskaya, A., & Triandopoulos, N. (2006). Rationality and adversarial behavior in multi-party computation. In 26th Annual Int. Cryptology Conference (CRYPTO06).
Maheswaran, R. T., Tambe, M., Bowring, E., Pearce, J. P., & Varakantham, P. (2004). Taking DCOP to the
real world: Efficient complete solutions for distributed multi-event scheduling. In AAMAS-04.
Mailler, R., & Lesser, V. (2004). Solving distributed constraint optimization problems using cooperative mediation. Proceedings of Third International Joint Conference on Autonomous Agents and MultiAgent
Systems (AAMAS 2004), 1, 438445.
753

fiP ETCU , FALTINGS , & PARKES

Mailler, R., & Lesser, V. (2005). Asynchronous partial overlay: A new algorithm for solving distributed
constraint satisfaction problems. Journal of Artificial Intelligence Research (JAIR).
Mas-Colell, A., Whinston, M. D., & Green, J. R. (1995). Microeconomic Theory. Oxford University Press.
Mishra, D., & Parkes, D. (2007). Ascending price Vickrey auctions for general valuations. Journal of
Economic Theory, 132, 335366.
Modi, P. J., Shen, W.-M., Tambe, M., & Yokoo, M. (2005). ADOPT: Asynchronous distributed constraint
optimization with quality guarantees. AI Journal, 161, 149180.
Monderer, D., & Tennenholtz, M. (1999). Distributed games: From mechanisms to protocols. In Proc. 16th
National Conference on Artificial Intelligence (AAAI-99), pp. 3237.
Moulin, H. (2007). Efficient, strategy-proof and almost budget-balanced assignment. Tech. rep., Rice University.
Mualem, A. (2005). On decentralized incentive compatible mechanisms for partially informed environments.
In Proc. ACM Conf. on Electronic Commerce (EC).
Ostrovsky, R., Rajagopalan, S., & Vazirani, U. (1994). Simple and efficient leader election in the full information model. In STOC 94: Proceedings of the twenty-sixth annual ACM symposium on Theory of
computing, pp. 234242, New York, NY, USA. ACM Press.
Parkes, D. C., Kalagnanam, J. R., & Eso, M. (2001). Achieving budget-balance with Vickrey-based payment
schemes in exchanges. In Proc. 17th International Joint Conference on Artificial Intelligence (IJCAI01), pp. 11611168.
Parkes, D. C., & Shneidman, J. (2004). Distributed implementations of Vickrey-Clarke-Groves mechanisms.
In Proc. 3rd Int. Joint Conf. on Autonomous Agents and Multi Agent Systems, pp. 261268.
Parkes, D. C., & Ungar, L. H. (2000). Iterative combinatorial auctions: Theory and practice. In Proc. 17th
National Conference on Artificial Intelligence (AAAI-00), pp. 7481.
Petcu, A. (2006). FRODO: A FRamework for Open and Distributed constraint Optimization. Technical
report no. 2006/001, Swiss Federal Institute of Technology (EPFL), Lausanne, Switzerland. http:
//liawww.epfl.ch/frodo/.
Petcu, A., & Faltings, B. (2005a). A-DPOP: Approximations in distributed optimization. In Proceedings
of the Eleventh International Conference on Principles and Practice of Constraint Programming
(CP05), pp. 802806, Sitges, Spain.
Petcu, A., & Faltings, B. (2005b). DPOP: A scalable method for multiagent constraint optimization. In
Proceedings of the 19th International Joint Conference on Artificial Intelligence, IJCAI-05, pp. 266
271, Edinburgh, Scotland.
Petcu, A., & Faltings, B. (2006). PC-DPOP: A partial centralization extension of DPOP. In In Proceedings of
the Second International Workshop on Distributed Constraint Satisfaction Problems, ECAI06, Riva
del Garda, Italy.
Petcu, A., & Faltings, B. (2007). MB-DPOP: A new memory-bounded algorithm for distributed optimization. In Proceedings of the 20th International Joint Conference on Artificial Intelligence, IJCAI-07,
Hyderabad, India.
Pietzuch, P., Ledlie, J., Shneidman, J., Roussopoulos, M., Welsh, M., & Seltzer, M. (2006). Network-Aware
Operator Placement for Stream-Processing Systems. In ICDE.
Rassenti, S. J., Smith, V. L., & Bulfin, R. L. (1982). A combinatorial mechanism for airport time slot allocation. Bell Journal of Economics, 13, 402417.
Roberts, K. (1979). The characterization of implementable rules. In Laffont, J.-J. (Ed.), Aggregation and
Revelation of Preferences, pp. 321348. North-Holland, Amsterdam.
Rosenschein, J. S., & Zlotkin, G. (1994). Designing conventions for automated negotiation. AI Magazine.
Fall.
754

fiM-DPOP: FAITHFUL D ISTRIBUTED I MPLEMENTATION OF E FFICIENT S OCIAL C HOICE P ROBLEMS

Sandholm, T. (2002). Algorithm for optimal winner determination in combinatorial auctions. Artificial
Intelligence, 135, 154.
Sandholm, T. W. (1993). An implementation of the Contract Net Protocol based on marginal-cost calculations. In Proc. 11th National Conference on Artificial Intelligence (AAAI-93), pp. 256262.
Sandholm, T. W. (1996). Limitations of the Vickrey auction in computational multiagent systems. In Second
International Conference on Multiagent Systems (ICMAS-96), pp. 299306.
Shneidman, J., & Parkes, D. C. (2003). Rationality and self-interest in peer to peer networks. In 2nd Int.
Workshop on Peer-to-Peer Systems (IPTPS03).
Shneidman, J., & Parkes, D. C. (2004). Specification faithfulness in networks with rational nodes. In Proc.
23rd ACM Symp. on Principles of Distributed Computing (PODC04), St. Johns, Canada.
Silaghi, M.-C., Sam-Haroud, D., & Faltings, B. (2000). Asynchronous search with aggregations. In
AAAI/IAAI, pp. 917922, Austin, Texas.
Solotorevsky, G., Gudes, E., & Meisels, A. (1996). Modeling and Solving Distributed Constraint Satisfaction
Problems (DCSPs). In Proceedings of the Second International Conference on Principles and Practice
of Constraint Programming (CP96), pp. 561562, Cambridge, Massachusetts, USA.
Sycara, K., Roth, S. F., Sadeh-Koniecpol, N., & Fox, M. S. (1991). Distributed constrained heuristic search.
IEEE Transactions on Systems, Man, and Cybernetics, 21(6), 14461461.
Wellman, M. P. (1993). A market-oriented programming environment and its application to distributed multicommodity flow problems. Journal of Artificial Intelligence Research, 1, 123.
Wellman, M. P. (1996). Market-oriented programming: Some early lessons. In Clearwater, S. H. (Ed.),
Market-Based Control: A Paradigm for Distributed Resource Allocation, chap. 4, pp. 7495. World
Scientific.
Yokoo, M., Durfee, E. H., Ishida, T., & Kuwabara, K. (1992). Distributed constraint satisfaction for formalizing distributed problem solving. In International Conference on Distributed Computing Systems, pp.
614621.
Yokoo, M., & Hirayama, K. (2000). Algorithms for distributed constraint satisfaction: A review. Autonomous
Agents and Multi-Agent Systems, 3(2), 185207.
Yokoo, M., Sakurai, Y., & Matsubara, S. (2004). The effect of false-name bids in combinatorial auctions:
New Fraud in Internet Auctions. Games and Economic Behavior, 46(1), 174188.
Zhang, W., & Wittenburg, L. (2003). Distributed breakout algorithm for distributed constraint optimization
problems - DBArelax. In Proceedings of the International Joint Conference on Autonomous Agents
and Multi Agent Systems (AAMAS-03), Melbourne, Australia.
Zlotkin, G., & Rosenschein, J. S. (1996). Mechanisms for automated negotiation in state oriented domains.
Journal of Artificial Intelligence Research, 5, 163238.

755

fiJournal of Artificial Intelligence Research 32 (2008) 879-900

Submitted 01/08; published 08/08

Latent Tree Models and
Approximate Inference in Bayesian Networks
Yi Wang
Nevin L. Zhang
Tao Chen

wangyi@cse.ust.hk
lzhang@cse.ust.hk
csct@cse.ust.hk

Department of Computer Science and Engineering
The Hong Kong University of Science and Technology
Clear Water Bay Road, Kowloon, Hong Kong, China

Abstract
We propose a novel method for approximate inference in Bayesian networks (BNs).
The idea is to sample data from a BN, learn a latent tree model (LTM) from the data
oine, and when online, make inference with the LTM instead of the original BN. Because
LTMs are tree-structured, inference takes linear time. In the meantime, they can represent
complex relationship among leaf nodes and hence the approximation accuracy is often good.
Empirical evidence shows that our method can achieve good approximation accuracy at
low online computational cost.

1. Introduction
Latent tree models (LTMs) are tree-structured Bayesian networks where leaf nodes represent
manifest variables which are observed, while internal nodes represent latent variables which
are hidden. They are previously known as hierarchical latent class models (Zhang, 2004). In
this paper, we do not distinguish between variables and nodes, and assume that all variables
are categorical.
Pearl (1988) was the rst to identify LTMs as a potentially useful class of models. There
are two reasons. First, inference in LTMs takes time linear in the number of nodes, while
it is intractable in general BNs. Second, the latent variables capture complex relationships
among the manifest variables. In an LTM, the manifest variables are mutually independent
given the latent variables, while eliminating all the latent variables results in a completely
connected BN.
We study the possibility of exploiting those two properties for approximate inference in
BNs. Here is the most natural idea:
1. Oine: Obtain an LTM M that approximates a BN N in the sense that the joint
distribution of the manifest variables in M approximately equals the joint distribution
of the variables in N .
2. Online: Use M instead of N to compute answers to probabilistic queries.
The cardinalities of the latent variables play a crucial role in the approximation scheme.
They determine inferential complexity and inuence approximation accuracy. At one extreme, we can represent a BN exactly using an LTM by setting the cardinalities of the latent
variables large enough. In this case, the inferential complexity is very high. At the other
c
2008
AI Access Foundation. All rights reserved.

fiWang, Zhang, & Chen

extreme, we can set the cardinalities of the latent variables at 1. In this case, the manifest
variables become mutually independent. The inferential complexity is the lowest and the
approximation quality is the poorest. We seek an appropriate middle point between those
two extremes.
We assume that there is a predetermined constraint on the cardinalities of the latent
variables to control inferential complexity. We develop an algorithm for nding an LTM
that satises the constraint and approximates the original BN well. The idea is to sample
data from the BN, and learn an LTM from the data. The model structure is determined
using hierarchical clustering of manifest variables. In each step, two closely correlated sets
of manifest variables are grouped, and a new latent variable is introduced to account for
the relationship between them. The cardinalities of the latent variables are set at the
predetermined value. The parameters are optimized using the Expectation-Maximization
(EM) algorithm (Dempster, Laird, & Rubin, 1977).
We have empirically evaluated our inference method on an array of networks. The possibility to tradeo between inferential complexity and approximation accuracy has been
demonstrated by adjusting the cardinality constraints. It turns out that our method is
able to achieve good approximation accuracy before the cardinality becomes too high. We
compared our method with loopy belief propagation (LBP) (Pearl, 1988), a standard approximate inference method which has been successfully used in many real world domains
(Frey & MacKay, 1997; Murphy, Weiss, & Jordan, 1999). Given the same amount of time,
our method achieves signicantly higher accuracy than LBP in most cases. To achieve the
same accuracy, LBP needs one to three orders of magnitude more time than our method.
Our inference method is fast because LTM is tree-structured. One can also construct a
Chow-Liu tree (Chow & Liu, 1968) to approximate the original BN and use it for inference.
We refer to this approach as the CL-based method. In comparison with our method, CLbased method is always faster, but it is not as accurate as our method.
Our scheme exploits the strong expressive capability of latent variable models. One can
of course use other latent variable models instead of LTMs in the scheme. A straightforward
choice is latent class model (LCM) (Hagenaars & McCutcheon, 2002). An LCM is an
LTM with only one latent variable1 . It assumes local independence, that is, the manifest
variables are mutually independent conditioning on the latent variable. We also compare our
method with this alternative. The results show that, under the same inferential complexity
constraints, our method is more accurate than the LCM-based method.
It should be noted that our approximate scheme needs a lot of time in the oine phase.
This is because that EM usually takes a long time to converge. Moreover, the time complexity of EM scales up linearly with the sample size, which should be set as large as possible to
achieve high-quality approximation. Therefore, our method is suitable only for applications
that allow a long oine phase.
The remainder of this paper is organized as follows. In Section 2, we review LTMs. In
Section 3, we describe our method of constructing LTMs to approximate BNs. In Section 4,
we describe our scheme for approximate inference formally. Section 5 reports empirical results. Section 6 discusses the relationship between our approach and existing work. Finally,
in Section 7, we conclude this paper and point out some future directions.
1. In machine learning community, LCM is also referred to as naive Bayes model with latent variable.

880

fiLatent Tree Models and Approximate Inference in Bayesian Networks

2. Latent Tree Model
An LTM is a pair M = (m,  m ). The rst component m denotes the rooted tree and the set
of cardinalities of the latent variables. We will refer to m as the model, and the rooted tree
as the model structure. The second component  m denotes the collection of parameters in
M. It contains a conditional probability table for each node given its parent.
Let X and Y be the set of manifest variables and the set of latent variables in M,
respectively. We use P (X, Y|m,  m ), or PM (X, Y) in short, to denote the joint distribution
represented by M. Two LTMs M and M are marginally equivalent if they share the same
set of manifest variables X and PM (X) = PM (X). A model m includes another model m
if for any  m there exists  m such that (m,  m ) and (m ,  m ) are marginally equivalent.
Two models m and m are marginally equivalent if m includes m and vice versa.
Let |Z| denote the cardinality of a variable Z. For a node Z in m, we use nb(Z) to
denote the set of its neighbors. A model m is regular if for any latent node Y ,
1. If Y has only two neighbors, then at least one of the neighbors must be a latent node
and

Znb(Y ) |Z|
.
|Y | <
maxZnb(Y ) |Z|
2. If Y has more than two neighbors, then

|Y | 

Znb(Y ) |Z|

maxZnb(Y ) |Z|

.

If a model m is irregular, it is over-complicated. It can be reduced to a regular model m
that is marginally equivalent to and contains fewer parameters
than m (Zhang, 2004). In a
Q
|Z|

regular model, a latent node Y is saturated if |Y | = maxZnb(Y ) |Z| . In this case, we say that
Znb(Y )
Y subsumes all its neighbors except the one with the largest cardinality.

3. Approximating Bayesian Networks with Latent Tree Models
In this section, we study the problem of approximating a BN with an LTM. Let N be the
BN to be approximated. Let X be the set of variables in N . For an LTM M to be an
approximation of N , it should use X as its manifest variables, and the cardinalities of its
latent variables should not exceed a predetermined threshold C. Figure 1(b), 1(c), and 1(d)
show three example LTMs that approximate the BN in Figure 1(a). They will be used to
illustrate various steps in our method.
Let PN (X) be the joint distribution represented by N . An approximation M is of high
quality if PM (X) is close to PN (X). We measure the quality of the approximation by the
KL divergence (Cover & Thomas, 1991)
D[PN (X)PM (X)] =


X

PN (X) log

PN (X)
.
PM (X)

Our objective is to nd an LTM that minimizes the KL divergence, i.e.,
M = arg min D[PN (X)PM (X)].
M

881

fiWang, Zhang, & Chen

X1 (2)

X2 (2)

Y5 (8)

Y4 (8)

X3 (2)

X4 (2)

Y2 (8)

Y1 (8)

X5 (2)

X1 (2)

X6 (2)

X4 (2)

(a) Bayesian network N

Y3 (8)

X5 (2)

X2 (2)

X3 (2)

X6 (2)

(b) Latent tree model m

Y4 (8)

Y2 (8)

Y1 (4)

X4 (2)

X1 (2)

X5 (2)

Y2 (8)

Y3 (4)

X2 (2)

Y4 (8)

X3 (2)

X4 (2)

X6 (2)

(c) Regularized model m

X5 (2)

X6 (2)

X1 (2)

X2 (2)

X3 (2)

(d) Simplified model m

Figure 1: An illustrative example. The numbers within the parentheses are the cardinalities
of the variables.

An LTM M consists of two components, the model m and the parameters  m . Therefore,
the optimization problem can be naturally decomposed into two subproblems.

1. Find an optimal model m .

2. Optimize the parameters  m for a given model m.

In the remainder of this section, we will discuss these two subproblems in details.
3.1 Parameter Optimization
We start by addressing the second subproblem. Given a model m, our target is to nd
 m = arg min D[PN (X)P (X|m,  m )].
m

882

fiLatent Tree Models and Approximate Inference in Bayesian Networks

It turns out that, due to the presence of latent variables, the KL divergence is dicult to
directly minimize. This can be seen by expanding the KL divergence as follows,


PN (X)
P (X|m,  m )
X


=
PN (X) log PN (X) 
PN (X) log P (X|m,  m )

D[PN (X)P (X|m,  m )] =

PN (X) log

X

=



X

PN (X) log PN (X) 

X



PN (X) log

X



P (X, Y|m,  m ).

Y

The rst term on the last line can be neglected because it is independent of  m . The
diculty lies in maximizing the second term. The summation over latent variables Y
appearing inside the logarithm makes this term indecomposable. Therefore, no closed-form
solution can be obtained for  m by taking the derivative of this term with respect to  m
and setting it to zero.
We transform the problem into an asymptotically equivalent maximum likelihood estimation (MLE) problem. The idea is as follows.
1. Generate a data set D with N independently and identically distributed samples from
PN (X).
2. Find the MLE of  m with respect to D, i.e.,
 m = arg max P (D|m,  m ).
m

It is well known that  m converges almost surely to  m as the sample size N approaches
innity (Huber, 1967).
We now discuss the implementation of this solution. We start by generating D from
PN (X). Since PN (X) is represented by BN N , we use logic sampling (Henrion, 1988) for
this task. Specically, to generate a piece of sample from PN (X), we process the nodes
in a topological ordering2 . When handling node X, we sample its value according to the
conditional distribution P (X|(X) = j), where (X) denotes the set of parents of X and j
denote their values that have been sampled earlier. To obtain D, we repeat the procedure
N times.
Given D, the next step is to nd the MLE. Note that the values of latent variables Y
are missing in D. We thus use the EM algorithm (Dempster et al., 1977). Starting with
a random guess, the EM algorithm iteratively improves the estimate until the change in
loglikelihoods of two consecutive iterations is smaller than a predetermined threshold. A
practical issue is that EM can converge to local maxima on the likelihood surface. The
local maxima can be far from the global maxima, and thus can be poor approximations to
 m . Fortunately, the local maxima issue is not severe for LTMs (Wang & Zhang, 2006).
In practice, one can also use various techniques such as multiple restart (Chickering &
Heckerman, 1997) and data permutation (Elidan et al., 2002) to alleviate this issue.
Note that EM takes a long time to converge, especially when the sample size N is large.
This is why our algorithm has an expensive oine phase.
2. A topological ordering sorts the nodes in a DAG such that a node always precedes its children.

883

fiWang, Zhang, & Chen

3.2 Exhaustive Search for the Optimal Model
We now consider the rst subproblem, i.e., to nd the best model m . A straightforward
way to solve this problem is to exhaust all possible models, nd the optimal parameters  m
for each model m, compute the KL divergence D[PN (X)P (X|m,  m )], and then return a
model m with the minimum KL divergence.
The problem with this solution is its high computational complexity. Given a set of
manifest variable X, there are innitely many models. One can always obtain new models
by inserting latent variables to an existing model. As we will show in Section 3.5, it is
sucient to consider a nite subspace, i.e., the subspace of regular models. However, there
are still super-exponentially many regular models (Zhang, 2004). For each model, we need
to optimize its parameters by running EM, which is a time-consuming process. Therefore,
the exhaustive search is computationally infeasible. In the following 4 subsections, we will
present a heuristic method.
3.3 Heuristic Construction of Model Structure
We rst present a heuristic for determining the model structure. In an LTM, two manifest
variables are called siblings if they share the same parent. Our heuristic is based on two
ideas: (1) In an LTM M, siblings are generally more closely correlated than variables that
are located far apart; (2) If M is a good approximation of N , then two variables Xi and
Xj are closely correlated in M if and only if they are closely correlated in N . So we can
examine each pair of variables in N , pick the two variables that are most closely correlated,
and introduce a latent variable as their parent in M.
We measure the strength of correlation between a pair of variables Xi and Xj by the
mutual information (Cover & Thomas, 1991)
IN (Xi ; Xj ) =



PN (Xi , Xj ) log

Xi ,Xj

PN (Xi , Xj )
.
PN (Xi )PN (Xj )

To compute IN (Xi ; Xj ), one need to make inference in N . This could be computationally
hard in the rst place. So we use sampling technique to address this issue. Specically, we
generate a data set D with N samples from the BN N , and compute the empirical mutual
 i ; Xj ) using the empirical distribution P (Xi , Xj ) based on D. By the strong
information I(X
 i ; Xj ) will almost surely converge to IN (Xi ; Xj ) as the sample
law of large numbers, I(X
size N goes to innity.
We now use the BN shown in Figure 1(a) as an example to illustrate the idea. It
contains 6 binary variables X1 , X2 , . . ., X6 . Suppose the empirical mutual information
based on some data set D is as presented in Table 1. As discussed above, we regard those
as approximation to mutual information between variables in N and hence regard them as
approximation to mutual information between variables in the nal LTM M that we are
to construct. We nd that X4 and X6 are the pair with the largest mutual information.
Therefore, we create a latent variable Y1 and make it the parent of X4 and X6 .
The next step is to nd, among Y1 , X1 , X2 , X3 , and X5 , the pair of variables with the
largest mutual information in M. There is one diculty: Y1 is not in the original Bayesian
network and hence not observed in the data set. The mutual information between Y1 and
884

fiLatent Tree Models and Approximate Inference in Bayesian Networks

X1
X2
X3
X4
X5
X6

X1
0.0000
0.0003
0.0015
0.0017
0.0102

X2
0.0971
0.0654
0.0311
0.0252

X3
0.0196
0.0086
0.0080

X4
0.1264
0.1817

X5
0.0486

Table 1: Empirical mutual information between manifest variables
the other variables cannot be computed directly. We hence seek an approximation. In the
nal model M, Y1 would d-separate X4 and X6 from the other variables. Therefore, for
any X  {X1 , X2 , X3 , X5 }, we have
IM (Y1 ; X)  IM (X4 ; X), IM (Y1 ; X)  IM (X6 ; X).
We hence approximate IM (Y1 ; X) using the lower bound
max{IM (X4 ; X), IM (X6 ; X)}.
Back to our running example, the estimated mutual information between Y1 and X1 ,
X2 , X3 , X5 is as presented in Table 2. We see that the next pair to pick is Y1 and X5 . We
introduce a latent variable Y2 as the parent of Y1 and X5 . The process continues. The nal
model structure is a binary tree as shown in Figure 1(b).
Y1

X1
0.0102

X2
0.0654

X3
0.0196

X5
0.1264

Table 2: Estimated mutual information between Y1 and manifest variables
3.4 Cardinalities of Latent Variables
After obtaining a model structure, the next step is to determine the cardinalities of the
latent variables. We set the cardinalities of all the latent variables at a predetermined value
C. In the following, we discuss how the choice of C inuences quality of approximation and
inferential eciency.
We rst discuss the impact of the value of C on the
 approximation quality. We start
by considering the case when C equals to Cmax = XX |X|, i.e., the product of the
cardinalities of all the manifest variables. In this case, each latent variable can be viewed
as a joint variable of all the manifest variables. We can therefore set the parameters  m so
that P (X|m,  m ) = PN (X). That is, m can capture all the interactions among the manifest
variables.
What happens if we decrease C? It can be shown that the approximation quality will
degrade. Let m be a model obtained with value C and m be another model obtained with
a smaller value C  . It is easy to see that m includes m . The following lemma states that
the approximation quality of m is no better than that of m.
885

fiWang, Zhang, & Chen

Lemma 1 Let P (X) be a joint probability distribution of X. Let m and m be two models
with manifest variables X. If m includes m , then
min D[P (X)P (X|m,  m )]  min D[P (X)P (X|m ,  m )].
m

Proof: Dene

m

 m = arg min D[P (X)P (X|m ,  m )].
m

Because m includes m , there must be parameters  m of m such that
P (X|m,  m ) = P (X|m ,  m ).
Therefore,
min D[P (X)P (X|m,  m )]  D[P (X)P (X|m,  m )]
m

= D[P (X)P (X|m ,  m )]

= min D[P (X)P (X|m ,  m )]
m

Q.E.D.
As mentioned earlier, when C is large enough, model m can capture all the interactions
among the manifest variables and hence can represent the joint distribution PN (X) exactly.
If C is not large enough, we can only represent PN (X) approximately. According to the
previous discussion, as C decreases, the approximation accuracy (in terms of KL divergence)
will gradually degrade, indicating that model m can capture less and less interactions among
the manifest variables. The worst case occurs when C = 1. In this case, all the interactions
are lost. The approximation accuracy is the poorest.
The parameter C also determines the computational cost of making inference in m. We
use the clique tree propagation (CTP) algorithm for inference. So we measure the cost by
the inferential complexity, which is dened to be the sum of the clique sizes in the clique
tree of m. It is given by

|X|  C.
(1)
(|X|  2)  C 2 +
XX

Note that |X| is the number of manifest variables, while |X| is the cardinality of a manifest
variable X. Therefore, one can control the inferential complexity by changing the value of
C. The smaller the value of C, the lower the complexity.
In summary, one can achieve a tradeo between the approximation quality and the
inferential complexity of the resultant model m by tuning the parameter C. In Figure 1(b),
we set C = 8.
3.5 Model Regularization
Suppose we have obtained a model m using the technique described in Section 3.3 and
by setting the cardinalities of the latent variables at a certain value. In the following two
subsections, we will show that it is sometimes possible to simplify m without compromising
the approximation quality.
886

fiLatent Tree Models and Approximate Inference in Bayesian Networks

We rst notice that m could be irregular. As an example, let us consider the model
in Figure 1(b). It is constructed as an approximation to the BN N in Figure 1(a) with
C = 8. By checking the latent variables, we nd that Y5 violates the regularity condition. It has only two neighbors and |Y5 |  |X1 ||Y4 |/ max{|X1 |, |Y4 |}. Y1 and Y3 also
violate the regularity condition because |Y1 | > |X4 ||X6 ||Y2 |/ max{|X4 |, |X6 |, |Y2 |} and
|Y3 | > |X2 ||X3 ||Y4 |/ max{|X2 |, |X3 |, |Y4 |}. The following proposition suggests that irregular models should always be simplied until they become regular.
Proposition 1 If m is an irregular model, then there must exists a model m with lower
inferential complexity such that
min D[PN (X)P (X|m,  m )] = min D[PN (X)P (X|m ,  m )].
m

m

(2)

Proof: Let Y be a latent variable in m which violates the regularity condition. Denote its
neighbors by Z1 , Z2 , . . . , Zk . We dene another model m as follows:
1. If Y has only two neighbors, then remove Y from m and connect Z1 with Z2 .
2. Otherwise, replace Y with a saturated latent variable Y  , i.e.,
k

i=1 |Zi |
|Y | =
.
maxki=1 |Zi |
As shown by Zhang (2004), for any parameters  m of m, there exists parameters  m
of m such that (m,  m ) and (m ,  m ) are marginally equivalent. The reverse is also true.
Therefore, m and m are marginally equivalent. Equation 2 thus follows from Lemma 1.
To show that the inferential complexity of m is lower than that of m, we compare the
clique trees of m and m . Consider the aforementioned two cases:
1. Y has only two neighbors. In this case, cliques {Y, Z1 } and {Y, Z2 } in the clique tree
of m are replaced with {Z1 , Z2 } in the clique tree of m . Assume |Z2 |  |Z1 |. The
dierence in the sum of clique sizes is
sum(m)  sum(m ) = |Y ||Z1 | + |Y ||Z2 |  |Z1 ||Z2 |
 |Z1 ||Z1 | + |Z1 ||Z2 |  |Z1 ||Z2 |
= |Z1 ||Z1 |
> 0.
2. Y has more than two neighbors. In this case, for all i = 1, 2, . . . , k, clique {Y, Zi } in
the clique tree of m is replaced with a smaller clique {Y  , Zi } in the clique tree of m .
In both cases, the inferential complexity of m is lower than that of m.
Q.E.D.
The proof of Proposition 1 presents a way to handle a latent variable that violates the
regularity condition, i.e., either eliminating it or decreasing its cardinality. To regularize an
irregular model, we handle all the latent variables in the order by which they are created in
887

fiWang, Zhang, & Chen

Section 3.3. In the following, we use the irregular model m in Figure 1(b) to demonstrate
the regularization process.
We begin with latent variable Y1 . It has three neighbors and violates the regularity
condition. So we decrease its cardinality to |X4 ||X6 ||Y2 |/ max{|X4 |, |X6 |, |Y2 |} = 4. Then
we consider Y2 . It satises the regularity condition and hence no changes are made. The
next latent variable to examine is Y3 . It violates the regularity condition. So we decrease
its cardinality to |X2 ||X3 ||Y4 |/ max{|X2 |, |X3 |, |Y4 |} = 4. We do not change Y4 because
it does not cause irregularity. At last, we remove Y5 , which has only two neighbors and
violates the regularity condition, and connect Y4 with X1 . We end up with the regular
model m as shown in Figure 1(c).
3.6 Further Simplifications
After regularization, there are sometimes still opportunities for further model simplication.
Take the model m in Figure 1(c) as an example. It contains two adjacent latent variables
Y1 and Y2 . Both variables are saturated. Y1 subsumes X4 and X6 , and Y2 subsumes Y1 and
X5 . Y2 can be viewed as a joint variable of Y1 and X5 , while Y1 can be in turn viewed as a
joint variable of X4 and X6 . Intuitively, we can eliminate Y1 and directly make Y2 the joint
variable of X4 , X5 , and X6 . This intuition is formalized by the following proposition.
Proposition 2 Let m be a model with more than one latent node. Let Y1 and Y2 be two
adjacent latent nodes. If both Y1 and Y2 are saturated while Y2 subsumes Y1 , then there exist
another model m that is marginally equivalent to and has lower inferential complexity than
m. Therefore,
min D[PN (X)P (X|m,  m )] = min D[PN (X)P (X|m ,  m )].
m

m

Proof: We enumerate the neighbors of Y1 as Y2 , Z11 , Z12 , . . . , Z1k , and the neighbors of
Y2 as Y1 , Z21 , Z22 , . . . , Z2l . Dene another model m by removing Y1 from m and connecting Z11 , Z12 , . . . , Z1k to Y2 . See Figure 2. We now prove that m and m are marginally
equivalent, while the inferential complexity of m is lower than that of m.
We start by proving the marginal equivalence. For technical convenience, we will work
with unrooted models. An unrooted model is a model with all directions on the edges
dropped. Parameters of an unrooted model include a potential for each edge in the model.
The potential is a non-negative function of the two variables that are connected by the edge.
The concept of marginal equivalence can be dened the same way as for rooted models.
As shown by Zhang (2004), a model is marginally equivalent to its unrooted version.
Therefore, to prove the marginal equivalence between m and m , it is sucient to show that
the unrooted versions of m and m are marginally equivalent. For simplicity, we abuse m
and m to denote the unrooted models. We also use f () to denote a potential in  m , and
g() to denote a potential in  m .
Note that Y1 and Y2 are saturated, while Y2 subsumes Y1 . When all variables have no
less than two states, this implies that:
1. Y1 subsumes Z11 , Z12 , . . . , Z1k .
2. Suppose that |Z2l | = maxlj=1 |Z2j |. Then Y2 subsumes Z21 , Z22 , . . . , Z2l1 .
888

fiLatent Tree Models and Approximate Inference in Bayesian Networks

Z11

Z12

Z11

Z22

Z12

Z21

Z22

Y2

...

...

Z1k

...

Y2

...

Y1

Z21

Z1k

Z2l

Z2l

(a)

(b)

Figure 2: Further simplication. (a) A part of a model that contains two adjacent and
saturated latent nodes Y1 and Y2 , with Y2 subsuming Y1 . (b) Simplied model
with Y1 eliminated.

Therefore, a state of Y1 can be written as y1 =< z11 , z12 , . . . , z1k >, while a state of Y2
can be written as y2 =< y1 , z21 , z22 , . . . , z2l1 >. The latter can be further expanded as
y2 =< z11 , z12 , . . . , z1k , z21 , z22 , . . . , z2l1 >.
We rst show that m includes m. Let  m be parameters of m. We dene parameters
 m of m as follows:
 Potential for edge Y2  Z2l :
g(Y2 =< z11 , z12 , . . . , z1k , z21 , z22 , . . . , z2l1 >, Z2l = z2l )


=

f (Y1 , Y2 )

k


f (Y1 , Z1i = z1i )

i=1

Y1 ,Y2

l


f (Y2 , Z2j = z2j ).

j=1

 Potential for edge Y2  Z1i , i = 1, 2, . . . , k:
g(Y2 =< z11 , z12 , . . . , z1k , z21 , z22 , . . . , z2l1 >, Z1i =


z1i
)


=


1 z1i = z1i
0 otherwise

 Potential for edge Y2  Z2j , j = 1, 2, . . . , l  1:
g(Y2 =< z11 , z12 , . . . , z1k , z21 , z22 , . . . , z2l1 >, Z2j =


z2j
)


=


1 z2j = z2j
0 otherwise

 Set the other potentials in  m the same as those in  m .
It is easy to verify that

Y1 ,Y2

Therefore,

f (Y1 , Y2 )

k

i=1

f (Y1 , Z1i )

l


f (Y2 , Z2j ) =

j=1

k

Y2

g(Y2 , Z1i )

i=1

P (X|m,  m ) = P (X|m ,  m ).
889

l


g(Y2 , Z2j ).

(3)

j=1

(4)

fiWang, Zhang, & Chen

Next, we prove that m includes m . Given parameters  m of m , we dene parameters
 m of m as follows:
 Potential for edge Y1  Y2 :
f (Y1 =< z11 , z12 , . . . , z1k >, Y2 = y2 ) =

k


g(Y2 = y2 , Z1i = z1i ).

i=1

 Potential for edge Y1  Z1i , i = 1, 2, . . . , k:
f (Y1 =< z11 , z12 , . . . , z1k >, Z1i =


z1i
)


=


1 z1i = z1i
0 otherwise

 Set the other potentials in  m the same as those in  m .
It can be veried that Equation 3 and 4 also hold. Therefore, m and m are marginally
equivalent.
We now compare the inferential complexity of m and m . According to the construction
of m , the clique tree of m is dierent from the clique tree of m in that it contains one less
clique {Y1 , Y2 } and replaces clique {Y1 , Z1i } with {Y2 , Z1i } for all i = 1, 2, . . . , k. Therefore,
the dierence between the the sum of clique sizes is


|Y1 ||Z1i | 
|Y2 ||Z1i |
sum(m)  sum(m ) = |Y1 ||Y2 | +
= |Y2 |



i

|Z1i | +

i



i

|Y1 ||Z1i | 

i



|Y2 ||Z1i |

i




|Z1i |) +
|Y1 ||Z1i |.
= |Y2 |( |Z1i | 
i

i

i


The rst term on the last line is non-negative because i |Z1i |  i |Z1i | when |Z1i |  2
for all i = 1, 2, . . . , k. Therefore, the inferential complexity of m is always lower than that
of m when Z1i is nontrivial.
Q.E.D.
Given the regularized model, we check each pair of adjacent latent variables and apply
Proposition 2 to eliminate redundant latent variables. We use the model m in Figure 1(c)
as an example to demonstrate the process. The rst pair to check are Y1 and Y2 . Both of
them are saturated while Y2 subsumes Y1 . We thus remove Y1 and connect Y2 to X4 and
X6 . We then check Y3 and Y4 . It turns out that Y3 is redundant. Therefore, we remove
it and connect Y4 to X2 and X3 . The last pair to check are Y2 and Y4 . They are both
saturated, but neither of them subsumes the other. Hence, they cannot be removed. The
nal model m is shown in Figure 1(d).


3.7 The Algorithm LTAB
To summarize, we have outlined an algorithm for approximating BNs using LTMs. We call
the algorithm LTAB, a shorthand for Latent Tree Approximation of Bayesian network. It
has 3 inputs: a BN N , a predetermined cardinality C for latent variables, and a sample
890

fiLatent Tree Models and Approximate Inference in Bayesian Networks

size N . The output of LTAB is an LTM that approximates PN (X), the joint probability
distribution represented by N . LTAB is briey described as follows.
1. Generate a data set D of N i.i.d. samples from PN (X). (Section 3.1)
2. Obtain an LTM structure by performing hierarchical clustering of variables, using
empirical mutual information based on D as the similarity measure. (Section 3.3)
3. Set cardinalities of latent variables at C and simplify the model. (Section 3.4  3.6)
4. Optimize parameters by running EM. (Section 3.1)
5. Return the resultant LTM.

4. LTM-based Approximate Inference
The focus of this paper is approximate inference in Bayesian networks. We propose the
following two-phase method:
1. Oine: Given a BN N , use LTAB to construct an approximation M. The sample size
N should be set as large as possible, while the cardinality C should be determined to
meet the requirement on inferential complexity.
2. Online: Make inference in M instead of N . More specically, given a piece of evidence E = e and a querying variable Q, return PM (Q|E = e) as an approximation to
PN (Q|E = e).

5. Empirical Results
In this section, we empirically evaluate our approximate inference method. We rst examine
the impact of sample size N and cardinality C on the performance of our method. Then
we compare the our method with CTP, LBP, the CL-based method, and the LCM-based
method.
We used 8 networks in our experiments. They are listed in Table 3. CPCS54 is a
subset of the CPCS network (Pradhan et al., 1994). The other networks are available at
http://www.cs.huji.ac.il/labs/compbio/Repository/. Table 3 also reports the characteristics of the networks, including the number of nodes, the average/max indegree and
cardinality of the nodes, and the inferential complexity (i.e., the sum of the clique sizes in
the clique tree). The networks are sorted in ascending order with respect to the inferential
complexity.
For each network, we simulated 500 pieces of evidence. Each piece of evidence was set
on all the leaf nodes by sampling based on the joint probability distribution. Then we
used the CTP algorithm and the approximate inference methods to compute the posterior
distribution of each non-leaf node conditioned on each piece of evidence. The accuracy of
an approximate method is measured by the average KL divergence between the exact and
the approximate posterior distributions over all the query nodes and evidence.
All the algorithms in the experiments were implemented in Java and run on a machine
with an Intel Pentium IV 3.2GHz CPU and 1GB RAM.
891

fiWang, Zhang, & Chen

Network
ALARM
WIN95PTS
HAILFINDER
INSURANCE
CPCS54
WATER
MILDEW
BARLEY

Number
of Nodes
37
76
56
27
54
32
35
48

Average/Max
Indegree
1.24/4
1.47/7
1.18/4
1.93/3
2/9
2.06/5
1.31/3
1.75/4

Average/Max
Cardinality
2.84/4
2/2
3.98/11
3.3/5
2/2
3.62/4
17.6/100
8.77/67

Inferential
Complexity
1,038
2,684
9,706
29,352
109,208
3,028,305
3,400,464
17,140,796

Table 3: Networks and their characteristics.
5.1 Impact of N and C
We discussed the impact of N and C on the performance of our method in Section 3. This
subsection empirically veries the claims.
Three sample sizes were chosen in the experiments: 1k, 10k, and 100k. For each network,
we also chose a set of C. LTMs were then learned using LTAB with dierent combination
of the values of N and C. For parameter learning, we terminated EM either when the
improvement in loglikelihoods is smaller than 0.1, or when the algorithm ran for two months.
The multiple restarting strategy by Chickering and Heckerman (1997) was used to avoid
local maxima. The number of starting points was set at 16.
The running time of LTAB is plotted in Figure 3. The y-axes denote the time in hours,
while the x-axes denote the parameter C for LTAB. The three curves correspond to dierent
values of N . In general, the running time increases with N and C, ranging from seconds to
weeks. For some settings, EM failed to converge in two months. Those settings are indicated
by arrows in the plots. We emphasize that LTAB is executed oine and its running time
should not be confused with the time for online inference, which will be reported next.
After obtaining the LTMs, we used clique tree propagation to make inference. The
approximation accuracy are shown in Figure 4. The y-axes denote the average KL divergence, while the x-axes still denote the parameter C for LTAB. There are ve curves and one
horizontal line in each plot. The three curves labeled as LTM are for our method, which
correspond to the three sample sizes we used. The remaining two curves and the horizontal
line are for the other approximate inference methods. We will discuss them in Sections 5.3
 5.5.
We rst examine the impact of sample size by comparing the corresponding curves in
each plot. We nd that, in general, the curves for larger samples are located below those
for smaller ones. This shows that the approximation accuracy increases with the size of the
training data.
To see the impact of C, we examine each individual curve from left to right. According
to our discussion, the curve is expected to drop monotonically as C increases. This is
generally true for the results with sample size 100k. For sample sizes 1k and 10k, however,
there are cases in which the approximation becomes poorer as C increases. See Figure
4(e) and 4(f). This phenomenon does not conict with our claims. As C increases, the
892

fiLatent Tree Models and Approximate Inference in Bayesian Networks

Time (hour)

10
10
10
10
10

2

2

10
N=1k
N=10k
N=100k

1

1

10
Time (hour)

10

0

1

2

0

10

1

10

2

10

3

3

1

2

4

10

8

1

2

4
C

C

(a) ALARM
10

10

8

16

(b) WIN95PTS

4

3

10

2

10

10

10

Time (hour)

Time (hour)

1

0

10

1

10

2

4

3

1

4

16

10

32

1

4

C

(c) HAILFINDER

Time (hour)

10

10

10

10

4

4

10

2

2

10

0

2

2

10

4

1

0

10

4

2

4
C

8

10

16

1

4

10

10

10

64

16

64

(f) WATER

4

4

10

2

2

10
Time (hour)

Time (hour)

10

16
C

(e) CPCS54
10

32

(d) INSURANCE

Time (hour)

10

16
C

0

2

2

10

4

1

0

10

4

4

16

10

64

C

1

4
C

(g) MILDEW

(h) BARLEY

Figure 3: Running time of LTAB. Settings for which EM did not converge are indicated by
arrows.
893

fiWang, Zhang, & Chen

10

10

0

10

Average KL divergence

Average KL divergence

10

1

2

LTM (1k)
LTM (10k)
LTM (100k)
LBP
CL (100k)
LCM (100k)

1

2

4

10

10

10

8

0

1

2

3

1

C

(a) ALARM

10

10

10

10

0

10

1

2

3

4

1

4
C

4

16

10

10

10

32

1

2

3

1

4

0

2

Average KL divergence

Average KL divergence

10

3

4

5

1

2

4
C

8

10

10

10

10

16

1

2

3

4

1

4

10

64

16

64

(f) WATER

0

10

Average KL divergence

Average KL divergence

10

16
C

(e) CPCS54
10

32

(d) INSURANCE
10

10

16
C

(c) HAILFINDER

10

16

0

C

10

8

(b) WIN95PTS

Average KL divergence

Average KL divergence

10

2

1

10

0

1

2

1

4

16

10

64

2

1

C

4
C

(g) MILDEW

(h) BARLEY

Figure 4: Approximation accuracy.
894

fiLatent Tree Models and Approximate Inference in Bayesian Networks

expressive power of the learned LTM increases. So it tends to overt the data. On the
other hand, the empirical distribution of a small set of data may signicantly deviate from
the joint distribution of the BN. This also suggests that the sample size should be set as
large as possible.
Finally, let us examine the impact of N and C on the inferential complexity. Figure 5
plots the running time for dierent methods to answer all the queries. For now, we only
consider the three curves that are labeled as LTM. It can be seen that the three curves
overlap in all plots. This implies that the running time is independent of the sample size
N . On the other hand, all the curves are monotonically increasing. This conrms our claim
that the inferential complexity is positively dependent on C.
In the following subsections, if not stated explicitly otherwise, we will only consider the
results for N = 100k and the largest C. Under these settings, our method achieves the
highest accuracy.
5.2 Comparison with CTP
We now compare our method with CTP, a state-of-the-art exact inference algorithm. The
rst concern is that, how accurate is our method. By examining Figure 4, we argue that our
method always achieves good approximation accuracy: For HAILFINDER, CPCS54, WATER, the
average KL divergence of our method is around or less than 103 ; For the other networks,
the average KL divergence is around or less than 102 .
We next compare the inferential eciency of our method and the CTP algorithm. The
running time of CTP is denoted by dashed horizontal lines in the plots of Figure 5. It can
be seen that our method is more ecient than the CTP algorithm. In particular, for the
ve networks with the highest inferential complexity, our method is faster than CTP by
two to three orders of magnitude.
To summarize, the results suggest that our method can achieve good approximation
accuracy at low computational cost.
5.3 Comparison with LBP
We now compare our method with LBP. The latter is an iterative algorithm. It can be used
as an anytime inference method by running a specic number of iterations. In our rst set
of experiments, we let LBP run as long as our method and compare their approximation
accuracy. We did this for each network and each value of C. The accuracy of LBP are
denoted by the curves labeled as LBP in Figure 4. By comparing those curves with the LTM
curves for N = 100k, we see that our method achieves signicantly higher accuracy than
LBP in most cases: For WATER, the dierence in average KL divergence is up to three orders
of magnitude; For the other networks, the dierence is up to one order of magnitude. For
HAILFINDER with C = 32, LBP is two times more accurate than our method. However, our
method also achieves good approximation accuracy in this case. The average KL divergence
is smaller than 103 . Finally, we noticed that LBP curves are horizontal lines for CPCS54,
MILDEW, and BARLEY. Further investigation on those cases shows that LBP nished only one
iteration in the given time period.
We next examine how much time it takes for LBP to achieve the same level of accuracy
as our method. For each piece of evidence, we ran LBP until its average KL divergence
895

fiWang, Zhang, & Chen

10

10

1

10
LTM (1k)
LTM (10k)
LTM (100k)
CTP
LBP
CL (100k)
LCM (100k)

0

Time (second)

Time (second)

10

10

10

2

1

0

1

1

2

4

10

8

1

1

C

(a) ALARM
10

2

10

1

Time (second)

Time (second)

10

0

10
10
10

10

1

1

4

4
C

16

10

32

2

1

0

1

2

1

4

3

10

4

2

Time (second)

Time (second)

10

1

0

10

10

2

0

1

1

2

4
C

8

10

16

2

1

4

5

10

3

Time (second)

Time (second)

10

1

10

10

10

1

1

4

64

16

64

(f) WATER

10
10

16
C

(e) CPCS54
10

32

(d) INSURANCE

10

10

16
C

(c) HAILFINDER

10

16

3

C

10

8

(b) WIN95PTS

10
10

2

16

10

64

C

6

4

2

0

2

1

4
C

(g) MILDEW

(h) BARLEY

Figure 5: Running time of the online inference.
896

fiLatent Tree Models and Approximate Inference in Bayesian Networks

is comparable with that of our method or the number of iterations exceeds 100. The
running time of LBP are denoted by the curves labeled as LBP in Figure 5. Comparing
those curves with the LTM curves, we found that LBP takes much more time than our
method: For MILDEW, LBP is slower than our method by three orders of magnitude; For the
other networks except HAILFINDER, LBP is slower by one to two orders of magnitude; For
HAILFINDER with C = 32, the running time of the two methods are similar. The results
show that our method compares more favorably to LBP in the networks that we examined.
5.4 Comparison with CL-based Method
In this subsection, we compare our method with the CL-based method. More specically,
for each network, we learn a tree model from the 100k samples using the maximum spanning
tree algorithm developed by Chow and Liu (1968). We then use the learned tree model to
answer the queries.
The approximation accuracy of the CL-based method are shown as solid horizontal lines
in the plots in Figure 4. Comparing with the CL-based method, our method achieves higher
accuracy in all the networks except for MILDEW. For INSURANCE, WATER, and BARLEY, the differences are signicant. For MILDEW, our method is competitive with the CL-based method.
In the meantime, we notice that the CL-based method achieves good approximations in all
the networks except for BARLEY. The average KL divergence is around or less than 102 .
An obvious advantage of CL-based method is its high eciency. This can be seen from
the plots in Figure 5. In most of the plots, the CL line locates below the second data point
on the LTM curve. The exception is MILDEW, for which the running time of the CL-based
method is as long as our method with C = 16.
In summary, the results suggest that the CL-based method is a good choice for approximate inference if the online inference time is very limited. Otherwise, our method is more
attractive because it is able to produce more accurate results when more time is allowed.
5.5 Comparison with LCM-based Method
Lowd and Domingos (2005) have previously investigated the use of LCM for density estimation. Given a data set, they determine the cardinality of the latent variable using
hold-out validation, and optimize the parameters using EM. It is shown that the learned
LCM achieves good model t on a separate testing set. The LCM was also used to answer
simulated probabilistic queries and the results turn out to be good.
Inspired by their work, we also learned a set of LCMs from the 100k samples and
compared them with LTMs on the approximate inference task. Our learning strategy is
slightly dierent. Since LCM is a special case of LTM, its inferential complexity can also be
controlled by changing the cardinality of the latent variable. In our experiments, we set the
cardinality such that the sum of the clique sizes in the clique tree of the LCM is roughly the
same as that for the LTM learned with a chosen C. In this way, the inferential complexity
of the two models are comparable. This can be veried by examining the LCM curves in
Figure 5. We then optimize the parameters of the LCM using EM with the same setting as
in the case of LTM.
As shown in Figure 4, for ALARM, WIN95PTS, CPCS54, WATER, and BARLEY, the LCM
curves are located above the LTM curves. That is, our method consistently outperforms
897

fiWang, Zhang, & Chen

the LCM-based method for all C. For HAILFINDER and MILDEW, our method is worse than
the LCM-based method when C is small. But when C becomes large, our method begins
to win. For INSURANCE, the performance of the two methods are very close. The results
suggest that unrestricted LTM is more suitable for approximation inference than LCM does.

6. Related Work
The idea of approximating complex BNs by simple models and using the latter to make inference has been investigated previously. The existing work mainly falls into two categories.
The work in the rst category approximates the joint distributions of the BNs and uses
the approximation to answer all probabilistic queries. In contrast, the work in the second
category is query-specic. It assumes the evidence is known and directly approximates the
posterior distribution of the querying nodes.
Our method falls in the rst category. We investigate the use of LTMs under this
framework. This possibility has also been studied by Pearl (1988) and Sarkar (1995). Pearl
(1988) develops an algorithm for constructing an LTM that is marginally equivalent to a
joint distribution P (X), assuming such an LTM exists. Sarkar (1995) studies how to build
good LTMs when only approximations are amenable. Their methods, however, can only
deal with the cases of binary variables.
Researchers have also explored the use of other models. Chow and Liu (1968) consider
tree-structured BNs without latent variables. They develop a maximum spanning tree
algorithm to eciently construct the tree model that is closest to the original BN in terms
of KL divergence. Lowd and Domingos (2005) learn an LCM to summarize a data set.
The cardinality of the latent variable is determined so that the logscore on a hold-out set is
maximized. They show that the learned model achieves good model t on a separate testing
set, and can provide accurate answers to simulated probabilistic queries. In both work, the
approximation quality and the inferential complexity of the learned model are xed. Our
method, on the other hand, provides a parameter C to let users make the tradeo between
approximation quality and inferential complexity.
The work in the second category is mainly carried out under the variational framework.
The mean eld method (Saul, Jaakkola, & Jordan, 1996) assumes that the querying nodes
are mutually independent. It constructs an independent model that is close to the posterior distribution. As an improvement to the mean eld method, the structured mean eld
method (Saul & Jordan, 1996) preserves a tractable substructure among the querying nodes,
rather than neglecting all interactions. Bishop et al. (1997) consider another improvement,
i.e., mixtures of mean eld distributions. It essentially ts an LCM to the posterior distribution. All these methods directly approximate posterior distributions. Therefore, they
might be more accurate than our method when used to make inference. However, these
methods are evidence-specic and construct approximations online. Moreover, they involve
an iterative process for optimizing the variational parameters. Consequently, the online running time is unpredictable. With our method, in contrast, one can determine the inferential
complexity beforehand.
898

fiLatent Tree Models and Approximate Inference in Bayesian Networks

7. Concluding Remarks
We propose a novel scheme for BN approximate inference using LTMs. With our scheme
one can trade o between the approximation accuracy and the inferential complexity. Our
scheme achieves good accuracy at low costs in all the networks that we examined. In
particular, it consistently outperforms LBP. We also show that LTMs are superior to LCMs
when used for approximate inference.
The current bottleneck of the oine phase is parameter learning. We used EM algorithm
to optimize parameters, which is known to be time consuming. The problem is especially
severe when the parameter C and the sample size are large. One way to speed up parameter
learning is to adapt the agglomerative clustering technique for learning the cardinality of
a latent variable from data (Elidan & Friedman, 2001). The basic idea is to complete the
training data by setting the cardinality of the latent variable large enough and assigning
each record to a latent state. In each step, one selects two states of the latent variable
to merge. The process repeats until the (penalized) likelihood ceases to improve. For our
parameter learning problem, we can terminate the process when the desired cardinality
C is achieved. We also need to deal with multiple latent variables. Since the data set is
completed, we expect this method to yield a good starting point for EM in a very short
time, which will in turn drastically shorten the oine phase.

Acknowledgments
We thank Haipeng Guo and Yiping Ke for insightful discussions. We are also grateful to
the anonymous reviewers for their valuable comments and suggestions on the earlier version
of this paper. Research on this work was supported by Hong Kong Grants Council Grants
#622105 and #622307, and the National Basic Research Program of China (aka the 973
Program) under project No. 2003CB517106. The work was completed when the rst author
was on leave at the HKUST Fok Ying Tung Graduate School, Guangzhou, China.

References
Bishop, C. M., Lawrence, N., Jaakkola, T., & Jordan, M. I. (1997). Approximating posterior
distributions in belief networks using mixtures. In Proceedings of the 10th Conference
on Advances in Neural Information Processing Systems, pp. 416422.
Chickering, D. M., & Heckerman, D. (1997). Ecient approximations for the marginal
likelihood of Bayesian networks with hidden variables. Machine Learning, 29, 181
212.
Chow, C. K., & Liu, C. N. (1968). Approximating discrete probability distributions with
dependence trees. IEEE Transactions on Information Theory, 14 (3), 462467.
Cover, T. M., & Thomas, J. A. (1991). Elements of Information Theory. Wiley-Interscience,
New York.
Dempster, A. P., Laird, N. M., & Rubin, D. R. (1977). Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B
(Methodological), 39 (1), 138.
899

fiWang, Zhang, & Chen

Elidan, G., & Friedman, N. (2001). Learning the dimensionality of hidden variables. In
Proceedings of the 17th Conference on Uncertainty in Artificial Intelligence.
Elidan, G., Ninio, M., Friedman, N., & Schuurmans, D. (2002). Data perturbation for
escaping local maxima in learning. In Proceedings of the 18th National Conference on
Artificial Intelligence, pp. 132139.
Frey, B. J., & MacKay, D. J. C. (1997). A revolution: belief propagation in graphs with
cycles. In Advances in Neural Information Processing Systems, Vol. 10.
Hagenaars, J. A., & McCutcheon, A. L. (2002). Applied Latent Class Analysis. Cambridge
University Press, Cambridge, UK.
Henrion, M. (1988). Propagating uncertainty in Bayesian networks by probabilistic logic
sampling. In Uncertainty in Artificial Intelligence 2, pp. 317324.
Huber, P. J. (1967). The behavior of maximum likelihood estimates under nonstandard
conditions. In Proceedings of the 5th Berkeley Symposium on Mathematical Statistics
and Probability, pp. 221233.
Lowd, D., & Domingos, P. (2005). Naive Bayes models for probability estimation. In
Proceedings of the 22nd International Conference on Machine Learning, pp. 529536.
Murphy, K. P., Weiss, Y., & Jordan, M. I. (1999). Loopy belief propagation for approximate
inference: an empirical study. In Proceedings of the 15th Conference on Uncertainty
in Artificial Intelligence, pp. 467475.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann Publishers, San Mateo, CA.
Pradhan, M., Provan, G., Middleton, B., & Henrion, M. (1994). Knowledge engineering
for large belief networks. In Proceedings of the 10th Conference on Uncertainty in
Artificial Intelligence, pp. 484490.
Sarkar, S. (1995). Modeling uncertainty using enhanced tree structures in expert systems.
IEEE Transactions on Systems, Man, and Cybernetics, 25 (4), 592604.
Saul, L. K., Jaakkola, T., & Jordan, M. I. (1996). Mean eld theory for sigmoid belief
networks. Journal of Artificial Intelligence Research, 4, 6176.
Saul, L. K., & Jordan, M. I. (1996). Exploiting tractable substructures in intractable
networks. In Advances in Neural Information Processing Systems, Vol. 8, pp. 486
492.
Wang, Y., & Zhang, N. L. (2006). Severity of local maxima for the em algorithm: Experiences
with hierarchical latent class models. In Proceedings of the 3rd European Workshop
on Probabilistic Graphical Models, pp. 301308.
Zhang, N. L. (2004). Hierarchical latent class models for cluster analysis. Journal of Machine
Learning Research, 5 (6), 697723.

900

fiJournal of Artificial Intelligence Research 32 (2008) 825877

Submitted 06/07; published 08/08

Qualitative System Identification from Imperfect Data
George M. Coghill

g.coghill@abdn.ac.uk

School of Natural and Computing Sciences
University of Aberdeen, Aberdeen, AB24 3UE. UK.

Ashwin Srinivasan

ashwin.srinivasan@in.ibm.com

IBM India Research Laboratory
4, Block C, Institutional Area
Vasant Kunj Phase II, New Delhi 110070, India.
and
Department of CSE and Centre for Health Informatics
University of New South Wales, Kensington
Sydney, Australia.

Ross D. King

rdk@aber.ac.uk

Deptartment of Computer Science
University of Wales, Aberystwyth, SY23 3DB. UK.

Abstract
Experience in the physical sciences suggests that the only realistic means of understanding complex systems is through the use of mathematical models. Typically, this has
come to mean the identification of quantitative models expressed as differential equations.
Quantitative modelling works best when the structure of the model (i.e., the form of the
equations) is known; and the primary concern is one of estimating the values of the parameters in the model. For complex biological systems, the model-structure is rarely known
and the modeler has to deal with both model-identification and parameter-estimation. In
this paper we are concerned with providing automated assistance to the first of these problems. Specifically, we examine the identification by machine of the structural relationships
between experimentally observed variables. These relationship will be expressed in the
form of qualitative abstractions of a quantitative model. Such qualitative models may
not only provide clues to the precise quantitative model, but also assist in understanding the essence of that model. Our position in this paper is that background knowledge
incorporating system modelling principles can be used to constrain effectively the set of
good qualitative models. Utilising the model-identification framework provided by Inductive Logic Programming (ILP) we present empirical support for this position using a series
of increasingly complex artificial datasets. The results are obtained with qualitative and
quantitative data subject to varying amounts of noise and different degrees of sparsity.
The results also point to the presence of a set of qualitative states, which we term kernel
subsets, that may be necessary for a qualitative model-learner to learn correct models. We
demonstrate scalability of the method to biological system modelling by identification of
the glycolysis metabolic pathway from data.

1. Introduction
There is a growing recognition that research in the life sciences will increasingly be concerned with ways of relating large amounts of biological and physical data to the structure
and function of higher-level biological systems. Experience in the physical sciences suggests
c
2008
AI Access Foundation. All rights reserved.

fiCoghill, Srinivasan, & King

that the only realistic means of understanding such complex systems is through the use of
mathematical models. A topical example is provided by the Physiome Project which seeks
to utilise data obtained from sequencing the human genome to understand and describe the
human organism using models that: . . . include everything from diagrammatic schema,
suggesting relationships among elements composing a system, to fully quantitative, computational models describing the behaviour of the physiological systems and an organisms
response to environmental change (see http://www.physiome.org/). This paper is concerned with a computational tool that aims to assist in the identification of mathematical
models for such complex systems.
Broadly speaking, system identification can be viewed as the field of modelling dynamic systems from experimental data (Soderstrom & Stoica, 1989). We can distinguish
here between: (a) classical system identification techniques, developed by control engineers and econometricians; and (b) machine learning techniques, developed by computer
scientists. There are two main aspects to this activity. First, an appropriate structure has to
be determined (the model-identification problem). Second, acceptably accurate values for
parameters in the model are to be obtained (the parameter-estimation problem). Classical
system identification is usually (but not always) used when the possible model structure is
known a priori. Machine learning methods, on the other hand, are of interest when little or
nothing is known about the model structure. The tool described here is a machine learning
technique that identifies qualitative models from observational data. Qualitative models are
non-parametric; therefore all the computational effort is focussed on model-identification
(there are no parameters to be estimated). The task is therefore somewhat easier than
more ambitious machine learning programs that attempt to identify parametric quantitative models (Bradley, Easley, & Stolle, 2000; Dzeroski, 1992; Dzeroski & Todorovski, 1995;
Todorovski, Srinivasan, Whiteley, & Gavaghan, 2000). Qualitative model-learning has a
number of other advantages: the models are quite comprehensible; system-dynamics can
be obtained relatively easily; the space of possible models is finite; and noise-resistance is
fairly high. On the down-side, qualtitative model-learners have often produced models that
are under- or over-constrained; the models can only provide clues to the precise mathematical structure; and the models are largely restricted to being abstractions of ordinary
differential equations (ODEs). We attempt to mitigate the first of these shortcomings by
adopting the framework of Inductive Logic Programming (ILP) (see Bergadano & Gunetti,
1996; Muggleton & Raedt, 1994). Properly constrained models are identified using a library of syntactic and semantic constraintspart of the background knowledge in the ILP
systemon physically meaningful models. Like all ILP systems, this library is relatively
easily extendable. Our position in this paper is that:
Background knowledge incorporating physical (and later, biological) system modelling principles can be used to constrain the set of good qualitative models.
Using some some classical physical systems as test-beds we demonstrate empirically that:
 A small set of constraints, in conjunction with a Bayesian scoring function, is sufficient
to identify correct models.
 Correct models can be identified from qualitative or quantitative data which need not
contain measurements for all variables in the model; and they can be learned with
826

fiQualitative System Identification

sparse data with large amounts of noise. That is, the correct models can be identified
when the input data are incomplete, incorrect, or both.
A closer examination of the performance on these test systems has led to the discovery of
what we term kernel subsets: minimal qualitative states that when present guarantee our
implementation will identify a model correctly. This concept may be of value to other model
identification systems.
Our primary interests, as made clear at the outset, lie in biological system identification.
The completion of the sequencing of the key model genomes and the rise of new technologies
have opened up the prospect of modelling cells in silico in unprecedented detail. Such models will be essential to integrate the ever-increasing store of biological knowledge, and have
the potential to transform medicine and biotechnology. A key task in this emerging field
of systems biology is to identify cellular models directly from experimental data. In applying qualitative system identification to systems biology we focus on models of metabolism:
the interaction of small molecules with enzymes (the domain of classical biochemistry).
Such models are the best established in systems biology. To this end, we demonstrate
that the approach scales up to identify the core of a well-known, complex biological system
(glycolysis) from qualitative data. This system is modelled here by a set of 25 qualitative relations, with several unmeasured variables. The scale-up is achieved by augmenting
the background knowledge to incorporate general chemical and biological constraints on
enzymes and metabolites.
The rest of the paper is organised as follows. In the next section we present the learning
approach ILP-QSI by means of an example: the u-tube. We also describe the details of
the learning algorithm in this section. In Section 3 we apply the learning experiments to a
number of other systems in the same class as the u-tube, present the results obtained, and
discuss the results for all the experiments reported thus far. Section 4 extends the work
from learning from qualitative data to a set of proof-of-concept experiments to assess the
ability of ILP-QSI to learn from quantitative data. The scalability of the method is tested
in Section 5 by application to a large scale metabolic pathway: glycolysis. In Section 6 we
discuss related work; and finally in Section 7 we provide a general discussion of the research
and draw some general conclusions.

2. Qualitative System Identification Using ILP
In order to aid understanding of the method presented in this paper we will first present
a detailed description of the process as applied to an illustrative system: the u-tube. The
u-tube has been chosen because it is a well understood system, and is one that has been
used in the literature (Muggleton & Feng, 1990; Say & Kuru, 1996). The results emerging
from this set of experiments will allow us to draw some tentative conclusions regarding
qualitative systems identification.
In subsequent sections we will present the results of applying the method described
in this section to further examples from the same class of system; this will enable us to
generalise our tentative conclusions. We will also apply the method to a large scale biological
system to demonstrate the scalability of the method.
827

fiCoghill, Srinivasan, & King

State
1
2
3
4
5
6

h1
< 0, std >
< 0, inc >
< (0, ), dec >
< (0, ), dec >
< (0, ), std >
< (0, ), inc >

h2
< 0, std >
< (0, ), dec >
< 0, inc >
< (0, ), inc >
< (0, ), std >
< (0, ), dec >

qx
< 0, std >
< (, 0), inc >
< (0, ), dec >
< (0, ), dec >
< 0, std >
< (, 0), inc >

qx
< 0, std >
< (0, ), dec >
< (, 0), inc >
< (, 0), inc >
< 0, std >
< (0, ), dec >

Table 1: The envisionment states used for the u-tube experiments. The qualitative values
are in the standard form used by QSIM. Positive values for the magnitude are
represented by the interval (0, ), negative values by the interval (, 0) and zero
by 0. The directions of change are self explanatory with increasing represented by
inc, decreasing by dec and steady by std.
2.1 An Illustrative System: The U-tube
The u-tube system (Fig. 1) is a closed system consisting of two tanks containing (or potentially containing) fluid, joined together at their base by a pipe. Assuming there is fluid in
the system it passes from one tank to the other via the pipe  from the tank with the higher
fluid level to the tank with the lower fluid level (as a function of the difference in height).
If the height of fluid is the same in both tanks then the system is in equilibrium and there
is no fluid flow.
The u-tube can be represented by a system of ordinary differential equations as follows:
dh1
dt
dh2
dt



= k  (h1  h2 ) 


(1)


= k  (h2  h1 ) 

A qualitative model may be obtained simply by abstracting from the real numbers, which
would normally be associated with Equation 1, into the quantity space of the signs. A
common formalism used to represent qualitative models is QSIM (Kuipers, 1994). In this
representation models are conjunctions of constraints, each of which are two or three place
predicates representing abstractions of real valued arithmetic and functional operations. All
variables in a model have values represented by two element vectors consisting of (in the
most abstract case) the sign of both the magnitude and direction of change of the variable.
In order to accommodate this restriction on the number of variables in a constraint we may
rewrite Equation 1 as follows:

h = (h1  h2 ) 



qx = k  h
(2)
dh1


dt = qx


dh2
dt = qx

where h1 and h2 are the height of fluid in Tank 1 and Tank 2 respectively; h is the
difference in the height of fluid in the tanks; and q x is the flow of fluid between the tanks.
This can be converted directly to QSIM constraints as shown in Fig. 1.

828

fiQualitative System Identification

Tank 2

Tank 1

Delta_h

h1

+

h

h2
dt

dt

h1
h2

-qx

-

qx

M+

DERIV(h1,qx ),
DERIV(h2,qx ),
ADD(h2,Delta h, h1 ),
M+ (Delta h,qx ),
MINUS(qx,qx ).

qx

Figure 1: The u-tube: (left) physical; (middle) QSIM diagrammatic; (right) QSIM constraints. In the QSIM version of the model Delta h corresponds to h in the
physical model. In QSIM, M+ (, ) is the qualitative version of a functional relation
which indicates that there is a monotonically increasing relation between the two
variables which are its arguments. The M + (, ) constraint represents a family of
functions that includes both linear and non-linear relations.

2

6
5

3

1

4

Figure 2: The u-tube envisionment graph.
Appropriate qualitative analysis of the u-tube will produce the states shown in Table 1,
which are the states of the envisionment. These represent all the distinct qualitative states
in which the u-tube may exist and Fig. 2 depicts all the possible behaviours (in terms of
transitions between states)1 . This figure represents a complete envisionment of the system,
which is the graph containing all the qualitative states of the system and all the transitions
between them for a particular input value. In the case of the u-tube presented here there
is no input (which is equivalent to a value of zero). On the other hand the behaviours
of a u-tube may be observed under a number of experimental (initial) conditions, with
measurements being taken of the height of fluid in each tank and the flow between the
tanks. These can be converted (by means of a quantitative-to-qualitative converter) into a
set of qualitative observations (or states). If sufficient temporal information is available to
enable the calculation of qualitative derivatives, each observation will be a tuple stating the
magnitude and direction of change of the measured variable. These observations will also
contain the states in the complete envisionment of Table 1 (or some subset thereof).
1. State 1 represents the situation where there is no fluid in the system, so nothing happens and it is not
interesting.

829

fiCoghill, Srinivasan, & King

The u-tube is a member of a large class of dynamic systems which are defined by their
states: state systems. In such systems the values of the variables at all future times are
defined by the current state of the system regardless of how that state was achieved (Healey,
1975). This means that for simulation, any system state can act as an initial state. In the
current context it means that in order to learn the structure of such systems we need
only focus on the states themselves and may ignore the transitions between states. This
enables us to explore the power set of the envisionment to ascertain the conditions under
which system identification is possible. Given these qualitative observations as examples,
background knowledge consisting of constraints on models (described later) and QSIM
relations, the learning system (which we name ILP-QSI) performs a search for acceptable
models. To a suitable first approximation, the basic task can be viewed as a discrete
optimisation problem of finding the lowest cost elements amongst a finite set of alternatives.
That is, given a finite discrete set of models S and a real-valued cost-function f : S  <,
find a subset H  S such that H = {H|H  S and f (H) = min Hi S f (Hi )}. This problem
may be solved by employing a procedure that searches through a directed acyclic graph
representation of possible models. In this representation, a pair of models are connected in
the graph if one can be transformed into another by an operation called refinement. Fig. 3
shows some parts of a graph for the u-tube in which a model is refined to another by the
addition of a qualitative constraint. An optimal search procedure (the branch-and-bound
procedure) traverses this graph in some order, at all times keeping the cost of the best nodes
so far. Whenever a node is reached where it is certain that it and all its descendents have
a cost higher than that of the best nodes, then the node and its descendents are removed
from the search.
There are a number of features apparent in the u-tube model that are relevant to the
learning method utilised in this work (and discussed in Section 2.3) that will be described
here since they regard general modelling issues relevant to the learning of qualitative models
of dynamic systems.
The first thing that may be noted in this regard is that the expressions in Equation
2 and the resulting qualitative constraints are ordered; that is, given the values for the
exogenous variables and the magnitude of the state variables (the height of fluid in the
tanks in this case) the equations can be placed in an order such that the variables on the
left hand side all may have their values calculated before they appear on the right hand
side of an equation2 . This particular form of ordering in known as causal ordering (Iwasaki
& Simon, 1986). A causally ordered system can be depicted graphically as shown in Fig. 4.
A causally ordered model contains no algebraic loops. In quantitative systems one tries
to avoid algebraic loops because they are hard to simulate, requiring additional simultaneous
equation solvers to be used.
A qualitative model combined with a Qualitative Reasoning (QR) inference engine will
provide an envisionment of the system of interest. That is, it will generate all the qualitatively distinct states in which the system may exist. In the case of the u-tube there are six
such states as given in Table 1. Example behaviours resulting from these states are shown
in Fig. 2.
2. This ordering is not required by QSIM in order to preform qualitative simulation. However, the ability
to order equations in this manner can be utilised as a filter in the learning system in order to eliminate
models containing algebraic loops.

830

fiQualitative System Identification

ADD( h1, h 2, h1)

ADD( h1, h 2, h1)
DERIV( h2,h1 )
MPLUS( h2,f12)

ADD(h 2, h2, h1)

ADD( h1, h 2, x1)

ADD( h1, h 2, h1)
DERIV( h2,h1 )
MPLUS( h2,f12)
MINUS( x8,f12 )
MPLUS( x8,h1 )

ADD( h1, h 2, x1)
MPLUS( x1,f12 )
MPLUS( h2,x4)

DERIV(h1,x2 )

DERIV( h1,x2 )

[]

DERIV( h1,h2 )

DERIV( h1,f12)

DERIV( h1,f12)
DERIV( h2,x5)
ADD( h2,x6,h1 )

MPLUS( h1,f12)

DERIV( h1,f12)
DERIV( h2,x5)
ADD( h2,x6,h1 )
MPLUS( x6,f12 )
MINUS( f12,x6 )

MPLUS( h1,f12)

MPLUS( h1,f12)

MPLUS( h1,f12)
ADD( f12,x7,h2 )
DERIV( h2,f12)

Figure 3: Some portions of the u-tube lattice (with the target model in the box).
It may be noted that the differential equation model captures the essence of the explanation given in the first paragraph of this section. It is sufficient to explain the operation
of such a system, as well as to predict the way it will behave, and it contains only those
variables and constants necessary to achieve this task - i.e. the model is parsimonious. 3
Furthermore, examination of the causal diagram in Fig. 4 indicates that the causal
ordering is in a particular direction  from the magnitudes of the state variables to their
derivatives. The link between the derivatives and the magnitudes of the state variables is
through an integration over time. This is integral causality and is the preferred kind
3. It is possible that for didactic purposes we may want to include more detail, for example a relation
between the intertank flow and the pressure difference, or between the height of fluid and the pressure.
There is no reason why we would expect such relations to be found; although in the context of an
adequate theoretical framework into which the model fits, the model provides pointers in that direction.
On the other hand, one can envisage simpler models existing which may be suitable for prediction but
inadequate for the required kind of explanation. See Section 6 for more on this.

831

fiCoghill, Srinivasan, & King

k

h1

h1

h

qx

h2

h2

Figure 4: A causal ordering of the u-tube model given in Equation 2.
of causality in systems engineering modelling; and simulation generally. This is because
integration smooths out noise whereas differentiation introduces it.
All variables are either endogenous or exogenous. Exogenous variables influence the system but are not influenced by it. Well posed models do not have any flapping variables;
that is, endogenous variables that appear in only one constraint. Because QSIM includes
a DERIV constraint linking the state variables directly to their derivatives, and all the systems in which we are interested are regulatory, containing feedback paths, all endogenous
variables must appear in at least two constraints.
Well posedness and parsimony are mandatory properties of the model, the other properties are desirable but not always achievable and so may have to be relaxed. However, for
all the systems examined in this paper each of these properties holds.
A final feature of the u-tube model is that it represents a single system. It is an
assumption implicit in all the learning experiments described in this paper that the data
measured belongs to a single coherent system. This is in keeping with general experimental
approaches where it is assumed that the measurements are related in some way by being
part of the same system. Of course we may get this wrong and have to relax the requirement
because we discover that what we thought were related cannot actually be brought together
in a single model. This generalises the requirement for parsimony in line with Einsteins
adage that a model should be as simple as possible and no simpler. In this case it
translates to minimising the number of disjoint sub-systems identified.
2.2 A Qualitative Solution Space
In Section 2.3 we shall present an algorithm for automatically constructing models from
data. With this method we utilise background knowledge consisting of QSIM modelling
primitives combined with systems theory meta-knowledge (e.g. parsimony and causality).
Later we shall also provide an analysis of the models learned and the states utilised to learn
them in order to ascertain which, if any, states are more important for successful learning.
One way to facilitate this analysis is to make use of a solution space to relate the qualitative
states to the critical points of the relevant class of systems (via the isoclines of the system) 4
4. The critical points of a dynamic system are points where one or more of the derivatives of the state
variables is zero. The isoclines are contours of critical points.

832

fiQualitative System Identification

(Coghill, 2003; Coghill, Asbury, van Rijsbergen, & Gray, 1992). As stated previously, a
qualitative analysis of the u-tube will generate an envisionment containing six states, as
shown in Table 1, and depicted in the envisionment graph given in Fig. 2. Continuing with
h2
h1
h2

height

6

5

5

f12 = 0

2

4
2

time

1

3

h1

Figure 5: The qualitative states of the u-tube system presented on representative time
courses (left) and on the solution space (right). The state numbers refer to the
states of the u-tube described above. (State 5 represents the steady state which
is strictly speaking only reached at t = , but is in practice taken to occur when
the two trajectories are sufficiently close, as shown here.)

the u-tube; there are two ways it can behave (ignoring state 1), captured in Fig. 5. Either
the head of fluid in tank 1 is greater than that in tank 2 (state 4) (in the extreme tank
1 is empty  state 3), or the head is greater in tank 2 than tank 1 (state 6). Fig. 5 (left)
shows the transient behaviour for the extreme case where tank 1 is empty (state 2); it can
be seen from this diagram that while the head starts in this condition its eventual end is
equilibrium (state 5). In this state Equation 1 can be rewritten as:


0 = k  (h1  h2 ) 


(3)


0 = k  (h2  h1 ) 

By definition k must be non-zero; so the only solution to this pair of equations is:
h2 = h 1
This relation can be plotted on a graph as shown on the right hand side of Fig. 5. Now
the qualitative states of the u-tube may be placed on this solution space graph in relation
to the equilibrium line. This representation (similar in form to a phase space diagram) is
useful because it provides a global picture of the location of the qualitative states of an
envisionment relative to the equilibria or critical points of the system. It has also been
utilised in the construction of diagnostic expert systems (Warren, Coghill, & Johnstone,
2004). For further details of this means of analysing envisionments see the work of Coghill
(2003) and Coghill et al. (1992).
833

fiCoghill, Srinivasan, & King

bb(i, , f ) : Given an initial element i from a discrete set S; a successor function  : S  2 S ; and a cost function
f : S  <, return H  S such that H contains the set of cost-minimal models. That is for all h i,j  H, f (hi ) =
f (hj ) = fmin and for all s0  S\H f (s0 ) > fmin .
1. Active := h(i, )i.
2. worst := 
3. selected := 
4. while Active 6= hi
5. begin
(a) remove element (k, costk ) from Active
(b) if costk < worst
(c) begin
i. worst := costk
ii. selected := {k}
iii. let P rune1  Active s.t. for each j  P rune1 , f (j) > worst where f (j) is the lowest cost
possible from j or its successors
iv. remove elements of P rune1 from Active
(d) end
(e) elseif costk = worst
i. selected := selected  {k}
(f) Branch := (k)
(g) let P rune2  Branch s.t. for each j  P rune2 , fmin (j) > best where fmin (j) is the lowest cost
possible from j or its successors
(h) Bound := Branch\P rune2
(i) for x  Bound
i. add (x, f (x)) to Active
6. end
7. return selected

Figure 6: A basic branch-and-bound algorithm. The type of Active determines specialised
variants: if Active is a stack (elements are added and removed from the front)
then depth-first branch-and-bound results; if Active is a queue (elements added
to the end and removed from the front) then breadth-first branch-and-bound
results; if Active is a prioritised queue then best-first branch-and-bound results.

2.3 The Algorithm
The ILP learner used in this research is a multistage procedure, each of which addresses
a discrete optimisation problem. In general terms, this is posed as follows: given a finite
discrete set S and a cost-function f : S  <, find a subset H  S such that H =
{s|s  S and f (s) = minsi S f (si )}. An optimal algorithm for solving such problems is the
branch-and-bound algorithm, shown in Fig. 6 (the correctness, complexity and optimality
properties of this algorithm are presented in a paper by Papadimitriou & Steiglitz, 1982).
A specific variant of this algorithm is available within the software environment comprising
Aleph (Srinivasan, 1999). The modified procedure is in Fig. 7. The principal differences
from Fig. 6 are:
1. The procedure is given a set of starting points H 0 , instead of a single one (i in Fig. 6);
834

fiQualitative System Identification

2. A limitation on the number of nodes explored (n in Fig. 7);
3. The use of a boolean function acceptable : H  B  E  {F ALSE, T RU E}.
acceptable(k,B,E) is TRUE, if and only if: (a) Hypothesis k explains the examples
E, given B in the usual sense understood in ILP (that is, B  k |= E in the absence
of noise); and (b) Hypothesis k is consistent with any constraints I contained in the
background knowledge (that is B kI 6|= 2). In practice, it is possible to merge these
requirements by encoding the requirement for entailing some or all of the examples
as a constraint in B;
4. Inclusion of background knowledge and examples (B and E in Fig. 7). These are
arguments to both the refinement operator  (the reason for this will become apparent
shortly) and the cost function f .
The following points are relevant for the implementation used here:
 Each qualitative model is represented as a single definite clause. Given a definite
clause C, the qualitative constraints in the model (the size of the model) are obtained
by counting the number of qualitative constraints in C. This will also be called the
size of C.
 Constraints, such as the restriction to well-posed models (described below), are assumed to be encoded in the background knowledge;
 The initial set H0 in Fig. 7 consists of the empty clause denoted here as . That is,
H0 = {};
 acceptable(C, B, E) is T RU E for any qualitative model C that is consistent with the
constraints in B, given E.
 Active is a prioritised queue sorted by f ;
 The successor function used is A . This is defined as follows. Let S be the size of
an acceptable model and C be a qualitative model of size S 0 with n = S  S 0 . We
assume B contains a set of mode declarations in the form described by (Muggleton,
1995). Then, given a definite clause C, obtain a definite C 0  A (C, B, E) where A =
n1
nA = hD 0 | D  A
(C, B, E) s.t. D 0  1A (D, B, E)i, (n  2). C 0  1A (C, B, E) is
obtained by adding a literal L to C, such that:
 Each argument with mode +t in L is substituted with any input variable of type
t that appears in the positive literal in C or with any variable of type t that
occurs in a negative literal in C;
 Each argument with mode t in L is substituted with any variable in C of type
t that appears before that argument or by a new variable of type t;
 Each argument with mode #t in L is substituted with a ground term of type t.
This assumes the availability of a generator of elements of the Herbrand universe
of terms; and
 acceptable(C 0 , B, E) is T RU E.
835

fiCoghill, Srinivasan, & King

bbA (B, E, H0 , , f, n) : Given background knowledge B  B; examples E  E; a non-empty set of initial elements
H0 from a discrete set of possible hypotheses H; a successor function  : H  B  E  2 H ; a cost function
f : H  B  E  <; and a maximum number of nodes n  N (n  0) to be explored, return H  H such that
H contains the set of cost-minimal models of the models explored.
1. Active = hi
2. for i  H0
(a) add (i, ) to Active
3. worst := 
4. selected := 
5. explored := 0
6. while (explored < n and Active 6= hi)
7. begin
(a)
(b)
(c)
(d)

(e)
(f)
(g)
(h)
(i)

remove element (k, costk ) from Active
increment explored
if acceptable(k, B, E)
begin
i. if costk < worst
ii. begin
A. worst := cost
B. selected := {k}
C. let P rune1  Active s.t. for each j  P rune1 , f (j, B, E) > worst where f (j, B, E) is the
lowest cost possible from j or its successors
D. remove elements of P rune1 from Active
iii. end:
iv. elseif costk = worst
A. selected := selected  {k}
end
Branch := (k, B, E)
let P rune2  Branch s.t. for each j  P rune2 , f (j, B, E) > worst where f (j, B, E) is the lowest
cost possible from j or its successors
Bound := Branch\P rune2
for x  Bound
i. add (x, f (x, B, E)) to Active

8. end
9. return selected

Figure 7: A variant of the basic branch-and-bound algorithm, implemented within the
Aleph system. Here B and E are sets of logic programs; and N the set of
natural numbers.

The following properties of 1A (and, in turn of A ) can be shown to hold (Riguzzi,
2005):
 It is locally finite. That is, 1A (C, B, E) is finite and computable (assuming the
constraints in B are computable);
 It is weakly complete. That is, any clause containing n literals can be obtained
in n refinement steps from the empty clause;
836

fiQualitative System Identification

 It is not proper. That is, C 0 can be equivalent to C;
 It is not optimal. That is, C 0 can be obtained multiply by refining different
clauses.
In addition, it is clear by definition that given a qualitative model C, acceptable(C 0 , B, E)
is T RU E for any model C 0  1A (C, B, E). In turn, it follows that acceptable(C 0 , B, E)
is T RU E for any C 0  A (C, B, E).
 The cost function used (following Muggleton, 1996) is f Bayes (C, B, E) = P(C|B, E)
where P(C|B, E) is the Bayesian posterior probability estimate of clause C, given
background knowledge B and positive examples E. Finding the model with the maximal posterior probability (that is, lowest cost) involves maximising the function (McCreath, 1999):
1
Q(C) = logDH (C) + p log
g(C)
where DH is a prior probability measure over the space of possible models; p is the
number of positive examples (that is, p = |E|); and g is the generality of a model.
We use the approach used in the ILP system C-Progol to obtain values for these two
functions. That is, the prior probability is related to the complexity of models (more
complex models are taken to be less probable, a priori); and the generality of a model
is estimated using the number of random examples entailed by the model, given the
background knowledge B (the details of this are presented by Muggleton in his paper
of 1996).
We have selected this Bayesian function to score hypotheses since it represents, to
the best of our knowledge, the only one in the ILP literature explicitly developed
for the case where data consist of positive examples only (as is the situation in this
paper, where examples are observations of system behaviour: system identification
from non-behaviour does not represent the usual understanding of the task we are
attempting here).
It is evident that these choices make the branch-and-bound procedure a simple generateand-score approach. Clearly, the approach is only scalable if the constraints encoding
well-posed models are sufficient to restrict acceptable models to some reasonable number:
we describe a set of such constraints that are sufficient for the models examined in this
paper. In the rest of the paper, the term ILP-QSI will be taken to mean the Aleph
branch-and-bound algorithm with the specific choices above.
2.3.1 Well-posed models
Well-posed models were introduced in Section 2.1; in the current implementation they are
defined as satisfying at least the following syntactic constraints:
1. Size. The model must be of a particular size (measured by the number of qualitative
relations for physical models in Sections 2.4 and 3 or the number of metabolites for
the biological model in Section 5). This size is pre-specified.
2. Complete. The model must contain all the measured variables.
837

fiCoghill, Srinivasan, & King

3. Determinate. The model must contain as many relations as variables (a basic principle
of systems theorythe reader may recall a version from school algebra, where a system
of equations contains as many equations as unknowns).
4. Language. The number of instances of any qualitative relation in the model must be
below some pre-specified limit. This kind of restriction has been studied in greater
detail in the work of Camacho (2000).
and at least the following semantic constraints:
5. Sufficient. The model must adequately explain the observed data. By adequate, we
intend to acknowledge here that due to noise in the measurements, not all observations
may be logical consequences of the model 5 . The percentage of observations that must
be explainable in this sense is a user-defined value.
6. Redundant. The model must not contain relations that are redundant. For example, the relation ADD(inf low, outf low, x1) is redundant if the model already has
ADD(outf low, inf low, x1).
7. Contradictory. The model must not contain relations that are contradictory given
other relations present in the model.
8. Dimensional. The model must contain relations that respect dimensional constraints.
This prevents, for example, addition of relations like ADD(inf low, outf low, amount)
that perform arithmetic on variables that have different units of measurement.
The following additional constraints which are here incorporated in the algorithm could be
ignored (because they are preferences rather than absolute rules), but all results presented
in this paper require them to be satisfied:
9. Single. The model must not contain two or more disjoint models. The assumption
is that if a set of measurements are being made within a particular context then the
user desires a single model that includes those measurement variables.
10. Connected. All intermediate variables should appear in at least two relations.
11. Causal. The model must be causally ordered (Iwasaki & Simon, 1986) with an integral
causality (Gawthrop & Smith, 1996). That is, the causality runs through the algebraic
constraints of the model from the magnitudes of the state variables to their derivatives;
and from the derivatives to the magnitudes through a DERIV constraint only.
This list is not intended to be exhaustive: we fully expect that they would need to be augmented by other domain-specific constraints (the biological system identification problem
described in Section 5 provides an instance of this). The advantage of using ILP is that
such augmentation is possible in a relatively straightforward manner.
5. Strictly speaking, the model in conjunction with the background knowledge.

838

fiQualitative System Identification

2.4 Experimental Investigation of Learning the U-tube System
In this section we present a comprehensive experimental test of the learning algorithm
described in the previous section. We again focus on the u-tube to illustrate the approach
and explain the results obtained. In a subsequent section we will present the results of
applying ILP-QSI to learning the structure of a number of different systems of a similar
kind. The data utilised in these experiments is qualitative. It is assumed that either the
measurements themselves yield qualitative values or that they are quantitative time series
that have been converted to qualitative values. This latter may be necessary in situations
where the quantitative time series data are not available in sufficient quantity to permit
quantitative system identification to be performed.
The following is the general method applied to learning all the systems studied for this
paper.
2.4.1 Experimental Aim
Using the u-tube system, investigate the model identification capabilities of ILP-QSI using
qualitative data that are subject to increasing amounts of noise and are made increasingly sparse in order to ascertain the circumstances under which the target system may be
accurately identified, as a function of the number of qualitative observations available.
2.4.2 Materials and Method
The model learning system ILP-QSI seeks to learn qualitative structural models from qualitative data; therefore the focus of the experiments is on learning from qualitative data.
Data There are no inputs (exogenous variables) to this system. The data required for
learning are combinations of the qualitative states (of which there are 6) from the envisionment shown in Table 1.
Method There are two distinct sets of experiments reported here: those based on noise
free data and those based on noisy data. The former assume that the data provided are
correct and are used to test the capability of ILP-QSI in handling sparse data. The latter set
of experiments captures the situation where the qualitative data may be incorrect because
of measurement errors due to noise in the original signal, or through errors introduced in
a quantitative to qualitative transformation (which may occur in cases where the original
data is numerical).
Noise-free data. We use the following method for evaluating ILP-QSIs system-identification
performance from noise-free data:
For the system under investigation:
1. Obtain the complete envisionment from specific values of exogeneous variables.
(In the particular case of the u-tube discussed in this section there are exogenous
variables and the envisionment states are as shown in Table 1, as stated above.)
839

fiCoghill, Srinivasan, & King

2. With non-empty subsets of states in the envisionment as training data construct a
set of models using ILP-QSI and record the precision of the result. 6 The number of
possible non-empty sets of states for the different test scenarios for the u-tube is 63.
(2N  1, where N is the number of states in the complete envisionment)
3. Plot learning curves showing average precision versus size of training data.
Noisy data. We use the following method for evaluating ILP-QSIs system-identification
performance from noisy qualitative data:
For the system under investigation:
1. Obtain the complete envisionment from specific values of exogeneous variables.
2. Replace non-empty subsets of states in the envisionment with randomly generated
noise states. With each such combination of correct and random states, as training
data construct a set of models using ILP-QSI and record the precision of the result. 7
Given a complete envisionment of N states, replacing a random subset k > 0 of these
with random states will result in a noisy envisionment consisting of N  k noisefree states and k random states. As with Step 2 for noise-free data, an exhaustive
replacement of all possible subsets of the complete envisionment with random states
will result in 2N  1 noisy test sets.
3. Plot learning curves showing average precision versus size of training data.
2.4.3 Results
The results of performing these experiments, showing the precision of learning the target
model versus the number of states used (for both noise-free and noisy data) are shown in
Fig. 8. It is evident that for both situations precision improves with the number of states
used and that the results from the experiments with noisy data have lower precision than
those with the noise-free data (though the curves have the same general shape). Both these
results are as one would expect.
With noise-free data we find that it was not possible to identify the target model using
just one state as data. However it was possible to identify the target model using pairs of
states in 53% of cases. These states are:
[2, 3], [2, 4], [2, 5], [3, 5], [3, 6], [4, 5], [4, 6], [5, 6]
We refer to these as Kernel sets. For the time being we merely report this finding and delay
a discussion of its significance until after reporting the results for the experiments on the
other systems in the class.
6. This is the proportion of the models in the result that are equivalent to the correct model. Thus, for each
training data set, the result returned by ILP-QSI will have a precision between 0.0 and 1.0. The term
precision as used here has the meaning usually associated with it in the Machine Learning community
rather than that familiar in Qualitative Reasoning.
7. As with the non-noisy data, for each training data set, the result returned by ILP-QSI will have a
precision between 0.0 and 1.0.

840

fiQualitative System Identification

1

0.9

0.8

Precision

0.7

0.6

0.5

0.4

0.3

0.2

Clean
Noisy

0.1

0
1

2

3

4

5

6

Number of States

Figure 8: Precision of models obtained for the u-tube.

3. Experiments on Other Systems
In this section we present the same experimental setup applied to a number of other systems:
coupled tanks, cascaded tanks and a mass spring damper. These systems are representative
of a class of system appearing in industrial contexts (e.g. the cascaded tanks system has
been used as a model for diagnosis of an industrial Ammonia Washer system by Warren
et al., 2004) as well as being useful analogs to metabolic and compartmental systems.
In each case the experimental method is identical to that utilised for the u-tube as
described in Section 2.4. For each system we give a description of the system and the target
model, the envisionment associated with the system, a statement of the data used in the
experiments, and a summary of the results obtained from the experiments.
3.1 Experimental Aim
For three physical systems: coupled tanks, cascaded tanks and mass-spring-damper (a well
known example of a servomechanism), investigate the model identification capabilities of
ILP-QSI using qualitative data that are subject to increasing amounts of noise and are
made increasingly sparse.
3.2 Materials and Method
Data Qualitative data available consist of the complete envisionment arising from specific
values for input variables. The precise details of the data are given with each experiment.
Method

The method used is the same as that for the u-tube and described in Section 2.4.
841

fiCoghill, Srinivasan, & King

h1
< 0, std >
< 0, inc >
< (0, ), dec >
< (0, ), dec >
< (0, ), inc >
< (0, ), dec >
< (0, ), std >
< (0, ), dec >
< (0, ), dec >
< (0, ), dec >

State
1
2
3
4
6
7
8
9
10
11

h2
< 0, std >
< (0, ), dec >
< 0, inc >
< (0, ), inc >
< (0, ), dec >
< (0, ), std >
< (0, ), dec >
< (0, ), dec >
< (0, ), dec >
< (0, ), dec >

qx
< 0, std >
< (, 0), inc >
< (0, ), dec >
< (0, ), dec >
< (, 0), inc >
< (0, ), dec >
< 0, inc >
< (0, ), dec >
< (0, ), std >
< (0, ), inc >

qo
< 0, std >
< (0, ), dec >
< 0, inc >
< (0, ), inc >
< (0, ), dec >
< (0, ), std >
< (0, ), dec >
< (0, ), dec >
< (0, ), dec >
< (0, ), dec >

Table 2: The envisionment states used for the coupled tanks experiments. (The states are
labeled to be in accord with those for the u-tube; since state 5 in the u-tube does
not appear in the coupled tanks envisionment there is no state labeled 5 in this
table.)
3.3 The Coupled Tanks
This is an open system consisting of two reservoirs as shown in Fig. 9. Essentially, it can
be seen as a u-tube with an input and an output. The input, q i , flows into the top of tank
1 and the output, qo , flows out of the base of tank 2 (see Fig. 9).
Tank 2

Tank 1
qi

h2

h1

+
h

h

M+

dt

h1

h 1

h2
qi
qx

+

M+

dt
h 2

qx

+

qo

DERIV(h1,h01 ),
DERIV(h2,h02 ),
ADD(h2 ,Delta h, h1 ),
M+ (Delta h,qx ),
M+ (h2 ,qo ),
ADD(h02 ,qo ,qx ),
ADD(qx,h01 ,qi ).

qo

Figure 9: The coupled tanks: (left) physical; (middle) QSIM diagram; (right) QSIM relations.
In these experiments we assume that we can observe: q i , qx , h1 , h2 , and qo . Thus system
identification must discover a model with three intermediate variables, h 01 , h02 and h.
Data There is one exogenous variable, namely the flow of liquid into tank 1 (q i ). In the
experiments described here the input flow is kept at zero (that is, q i = h0, stdi), making
the system for this particular case just moderately more complex than the u-tube. The
complete envisionment consists of 10 states, as shown in Table 2 and Fig. 10, which means
there are 1024 experiments in this set.

842

fiQualitative System Identification

1

8

9

3

6

7

10

4

11

2

Figure 10: Coupled tanks envisionment graph.
3.3.1 Results
The precision graphs for the coupled tanks experiments are shown in Fig. 11. Here again
results show the improvement in precision as the number states used increases and also the
deterioration in precision when noise is added. The effect of noise is worse when fewer states
are used than was the case for the u-tube, though its effect is nullified when all states are
used.
1

0.9

0.8

Precision

0.7

0.6

0.5

0.4

0.3

Clean
Noisy

0.2

0.1

0

1

2

3

4

5

6

7

8

9

10

Number of States

Figure 11: Coupled tanks precision graphs.
For the noise free data it was again not possible to identify any models using a single
datum but utilising pairs of states yielded the target model in 11% of cases. The relevant
pairs of states (kernel sets) are:
[2, 7], [3, 8], [4, 8], [6, 7], [7, 8]
843

fiCoghill, Srinivasan, & King

Whereas in the u-tube experiments all the states in which the target model was successfully
learned were supersets of the pairs, in the coupled tanks case there are sets of three states
(which are not supersets of the pairs listed above) that result in successful identification of
the target model:
[2, 3, 9], [2, 3, 10], [2, 3, 11]
[2, 4, 9], [2, 4, 10], [2, 4, 11]
[3, 6, 9], [3, 6, 10], [3, 6, 11]
[4, 6, 9], [4, 6, 10], [4, 6, 11]
3.4 Cascaded Tanks
This system is also an open system. However, flow through the system is always unidirectional (unlike the coupled tanks system). In principle, the system can be broken into
two sub-systems each containing one reservoir, each with their own input and output.
An example of the system is shown in Fig. 12. Liquid flows into tank 1, and then unidirectionally from tank 1 into tank 2. As is apparent from the figure, the flow is into the
top of tank 1 and out of the base of tank 2.
qi

h1

h1
qx

h2
M+

dt
h 1

Tank A

M+

dt
h 2

qo

h2

qi

+

qx

+

qo

DERIV(h1,h01 ),
DERIV(h2,h02 ),
M+ (h1 ,qx ),
M+ (h2 ,qo ),
ADD(h02 ,qo ,qx ),
ADD(qx,h01 ,qi ).

Tank B

Figure 12: The cascaded tanks: (left) physical; (middle) QSIM diagrammatic; (right) QSIM
relations.
We assume that we can observe: qi , h1 , h2 , and qx . Thus system identification must
discover a model with two intermediate variables, h 01 and h02 . The numbered list of states
(or complete envisionment) for this case is shown in Fig. 13 and Table 3.

Data There is one exogenous variable, namely the flow of liquid into tank 1 (q i ). We
increase the complexity by allowing a steady positive input flow (that is, q i = h(0, ), stdi).
The complete envisionment consists of 14 states, as shown in Fig. 13 and Table 3 which
means 16,383 experiments are required .
3.4.1 Results
The precision graphs for the cascaded tanks are shown in Fig. 14. The graphs are similar
in shape to the coupled systems, but showing generally lower precision with noisy-data.
Further examination shows that we are unable to identify the target model from fewer than
three states. The subset triples (which form the kernel sets in this case) from which the
844

fiQualitative System Identification

1

12

8

4

2

13

9

5

11

14

10

6

7

3

Figure 13: Cascaded tanks envisionment graph.
State
1
2
3
4
5
6
7
8
9
10
11
12
13
14

h1
< 0, inc >
< 0, inc >
< (0, ), dec >
< (0, ), dec >
< (0, ), dec >
< (0, ), dec >
< (0, ), std >
< (0, ), std >
< (0, ), std >
< (0, ), std >
< (0, ), inc >
< (0, ), inc >
< (0, ), inc >
< (0, ), inc >

h2
< 0, std >
< (0, ), dec >
< 0, inc >
< (0, ), dec >
< (0, ), std >
< (0, ), inc >
< 0, inc >
< (0, ), dec >
< (0, ), std >
< (0, ), inc >
< 0, inc >
< (0, ), dec >
< (0, ), std >
< (0, ), inc >

qx
< 0, inc >
< 0, inc >
< (0, ), dec >
< (0, ), dec >
< (0, ), dec >
< (0, ), dec >
< (0, ), std >
< (0, ), std >
< (0, ), std >
< (0, ), std >
< (0, ), inc >
< (0, ), inc >
< (0, ), inc >
< (0, ), inc >

qo
< 0, std >
< (0, ), dec >
< 0, inc >
< (0, ), dec >
< (0, ), std >
< (0, ), inc >
< 0, inc >
< (0, ), dec >
< (0, ), std >
< (0, ), inc >
< 0, inc >
< (0, ), dec >
< (0, ), std >
< (0, ), inc >

Table 3: The envisionment states used for the cascaded tanks experiments.

target model was identified are:
[1, 3, 4], [1, 3, 5], [1, 3, 8], [1, 3, 9], [1, 7, 4], [1, 7, 5], [1, 7, 8], [1, 7, 9]

845

fiCoghill, Srinivasan, & King

1

Clean
Noisy

0.9

0.8

Precision

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

2

4

6

8

10

12

14

Number of States

Figure 14: Cascaded tanks precision graph.
3.5 Mass-Spring-Damper
The final physical system considered is an abstraction of a wide variety of servomechanisms
with a displacing force. An example of the system is shown in Fig. 15. In this situation, a
mass is held in equilibrium between two forces. If the equilibrium is disturbed, oscillatory
behaviour is observed. The motion of the mass is damped so that the oscillations do not
continue indefinitely, and will eventually return to the original equilibrium position. If an
external force is applied (for example pulling the mass down) its final resting place will be
displaced from the natural equilibrium point (see Fig. 15). The mass M has displacement
dispM from its rest position, and at any time, t, it is moving with velocity vel M and
accelerating at rate accM . We assume that we can observe the variables: disp M , velM , accM ,
disp M M+ H 1
+

dt
Damping

Spring

vel M M+

H2

H
+

force

dt
vel
disp

a=v/sec

accM M+

H3

DERIV(dispM ,velM ),
DERIV(velM ,accM ),
M+ (dispM ,H1 ),
M+ (velM ,H2 ),
M+ (accM ,H3 ),
ADD(H1,H2 ,H4 ),
ADD(H3,H4 ,force).

force

Figure 15: The spring system (a) physical; (b) QSIM diagrammatic; (c) QSIM relations
and f orce. Qualitative system identification must now find a model with four intermediate
variables, H1 , H2 , H3 and H4 ; as well as a intermediate relation ADD(H 1 ,H2 ,H4 ), between
846

fiQualitative System Identification

three of these variables. The input force, force, is exogenous. In the experiments here,
we only consider the case where there is a steady force being applied to the system (that
is, ForceA = h(0, ), stdi). The complete envisionment for this case is shown in Fig. 16,
where the equilibrium point is represented by state 2. The precision graphs are shown in
2
19

24

29

17

0

3

18

22

27

16

23

28

20

21

26

12

7

5

6

10

8

9

15

13

14

11

4

1

30

25

Figure 16: Mass-spring-damper envisionment graph
Fig. 17. For this system the envisionment contains 31 states, which makes exhaustive testing
unrealistic. Instead sets of clean and noisy states were randomly selected from the space of
possible experiments. Nonetheless it can be observed that the average precision graphs are
in-line with those obtained for the tanks experiments. However, the actual precision values
suggest that both sparse data and noise have less of an effect here than the other systems.
This may be due to the tight relationship between the two derivatives in the spring model,
making the system extremely constrained.
3.6 Discussion of Results
An inspection of the experimental results reveals an expected pattern: in all cases the
precision curves (for both noisy and noise free experiments) have the same general shape.
Experiments which utilise fewer states identify the target model less often than when a
greater number of states are used. However, a closer examination of the results reveals
that even when few states are used (pairs or triples) the target model may be consistently
found when particular combinations of states are used. In order to understand why this is
so requires us to look at the solution spaces for the systems concerned. 8
We will examine the u-tube and coupled tanks together because they are very closely
related systems and both had zero input. The cascaded tanks system is slightly different
and had a non-zero input and so will be discussed later in the section.
8. We do not discuss the spring system here because of its complexity.

847

fiCoghill, Srinivasan, & King

1

0.9

0.8

Precision

0.7

0.6

0.5

0.4

0.3

0.2

Clean data
Noisy data

0.1

0
1

4

7

10

13

16

19

22

25

28

31

Number of States

Figure 17: Mass-spring-damper precision graph
3.6.1 The U-tube and Coupled Tanks
The bare results, while interesting, do not give any indication of why the particular pairs or
triples highlighted should precisely identify the target model. In order to ascertain why?
we must examine the envisionment states given in Tables 1 and 2, from these we can itemise
the relevant features of the sets of states as follows:
 For both the u-tube and coupled tanks there is at least one critical point in each pair.
 For both these systems each pair of states contains one state for each branch in the
envisionment graph (Fig. 2 & Fig. 10); and of these at least one is at the extreme of
its branch. That is, states where one tank is either empty or the state immediately
succeeding this, and the other tank is relatively full so that that the derivatives for
that height in each tank have opposite signs.
 For both systems all supersets of these minimal sets will precisely learn the target
model.
These observations lead us to suggest that for coupled systems the ability of the learning
system to identify the structure of a model is dependent on the data used including the
critical points and on having data that covers all the different types of starting point that
the system behaviours can have. This is in keeping with what systems theory would lead
us to expect (Gawthrop & Smith, 1996).
In order properly to appreciate what is indicated by these kernel sets and the relation
of the systems to each other we need to look at the solution spaces (Coghill et al., 1992;
848

fiQualitative System Identification

Coghill, 2003) for the two systems. These are shown in Fig. 18 (and their derivation is
similar to that given in Section 2.2 and detailed in Appendix A). From these we can get a
clear picture of where the kernel pairs and triples lie with respect to the critical points of
the system.
h1=h2=0

h2

h1=0

h2

5

8

6

6

2

11

2

10

4

h2=0

9
7
4

1

3

h1

1

3

h1

Figure 18: The solution spaces for the u-tube and coupled tanks systems.
From the system diagrams provided in Fig. 1 and Fig. 9 it can be seen that the u-tube
and coupled tanks systems differ only in the fact that the coupled tanks has an outlet orifice,
whereas the u-tube does not. This accounts for the major difference in their solution spaces;
namely that the coupled tanks has two critical points (states 7 and 9) whereas the u-tube
has only one (state 5)  which is actually the steady state. This gives rise to the additional
states: 9, 10 and 11 which lie between the critical points. 9 It can be observed that as the
outlet orifice from tank 2 in the coupled tanks system decreases in size the space between
the isoclines in the solution space will become narrower until it disappears when the orifice
closes. This can be seen more formally by comparing equations 6 and 10 in Appendix A.
There it is clear that as k2 approaches zero, equation 6 approximates equation 10 (and when
k2 = 0 the two equations are the same).
If we now look again at the sets of pairs we can observe that they are related in ways
that reflect the relationship between the two coupled systems. Firstly, looking at the pairs.
For the u-tube there are 4 pairs which include the critical point (steady state), state 5: [2,
5], [3, 5], [4, 5], and [5, 6]. Now noting from the discussion above that state 5 in the u-tube
relates to either of states 7 or 8 in the coupled tanks then we find that the analogous pairs
exist in the kernel set for the coupled tanks: [2, 7], [3,8], [4, 8], and [6, 7]. This leaves one
pair from the coupled tanks pairs unaccounted for: [7, 8]. However, this is no surprise since
that pair is taken to map to state 5 in the u-tube; and it is the consistent finding that no
singleton state is sufficient to learn a model of the system.
9. There are three states here because they differ only in the magnitude of the qx or qdir of h01 and h02 ,
neither of which appear explicitly in the solution space. Readers may convince themselves of this by
comparing Table 1 with the envisionment in Table 2.

849

fiCoghill, Srinivasan, & King

There are still 4 pairs in the u-tube experiments from which we are able to learn reliably
the target model that do not have a corresponding coupled tanks pair. These are: [2, 3],
[2, 4], [3, 6], and [4, 6]. A comparison with the triples for the learning of the coupled tanks
model reveals that these states are the pairs which are conjoined with either state 9, 10 or
11 to make up the triples. The inclusion of these states warrants further explanation since
they are the states which distinguish the closed u-tube from the open coupled tanks. In all
three of these states the state variables both have the value h(0, ), dec)i; a situation that
cannot occur in the u-tube. Combining this with the fact that the four pairs listed above
do not contain a critical point and are qualitatively identical in both systems leads one to
the conclusion that the additional information contained in these triple kernel sets enables
one to distinguish between the u-tube and coupled tanks in such a case.
These results extend, strengthen and deepen those reported in Coghill et al. (2004) and
Garrett et al. (2007).
3.6.2 The Cascaded Tanks
The cascaded tanks system is asymmetrical with the flow only being possible in one direction. The fact that the input is a positive steady flow makes the setup marginally more
complex in that regard than for the coupled systems, where there was no input flow.
The kernel sets from which a model of this system may be learned (presented in Section
3.4.1 are depicted schematically in Fig. 19 in order to explain the results obtained. If we
look at the first and middle columns of this diagram and ignore, for the time being, the
downstream tank, we can see that what is represented are two pairs of states: the tank
empty combined with the tank being at steady state, or the tank empty combined with the
state where the amount of fluid in the tank is greater than steady state. We have confirmed
experimentally that these are kernel sets from which a single tank model can be learned.
If we now ignore the upstream tank (apart from its outflow) and examine the middle
and third columns of the diagram we can see that these divide into two groups according to
whether the input to the downstream tank is steady or decaying (positive and decreasing).
For each of these there are two pairs of states, which are the same as for the upstream tank:
the tank empty combined with the tank being at steady state, or the tank empty combined
with the state where the amount of fluid in the tank is greater than steady state. In the
case of this tank it can be seen that the cross product of states appear in the kernel sets
because each case represents a valid possible situation.
These results lead to two major conclusions with regard to the cascaded tanks system:
1. ILP-QSI effectively identifies the individual components of the cascade and combines
them through the cascade point.
2. The situation with the downstream tank, where the input was either a steady flow or a
decreasing flow, indicates that utilising a variety of inputs can aid in the identification
process.
The former conclusion may serve as a pointer to the possibility of incremental learning
of cascaded systems.
850

fiQualitative System Identification

1

3

8

4

5

9

7

8

4

5

9

Figure 19: A schematic representation of the triples of states from which the target model
for the cascaded tanks systems is learned.

851

fiCoghill, Srinivasan, & King

4. Experiments with Quantitative Data
This part of the experimental testing of our system is a  proof-of-concept test. As has been
stated our system has been designed to learn qualitative models from qualitative data. As
such it is assumed that the conversion of any quantitative data has already been performed,
not least because the needed qualitative data analysis would require another research project
and is beyond the scope of this paper. However, in order to test the usability of our system
with quantitative data and to test its ability to go through the whole process from receiving
the data to producing the model, we have implemented a rudimentary data analysis package
to facilitate this. Of course this is not exhaustive, but it will permit us to test the results
produced via such a process for consistency with those produced from the experiments with
qualitative data.
4.1 Experimental Aim
Using the four physical systems, investigate the model identification capabilities of ILPQSI using numeric traces of system behaviour that are subject to increasing amounts of
noise.
4.1.1 Quantitative to Qualitative Conversion
Before proceeding to describe the experiments carried out, we present the method used
to convert numerical data into the qualitative form required by ILP-QSI because this is
utilised in each set of experiments.
We have adopted a straightforward and simple approach to performing the conversion.
For a quantitative variable x, values at N real-valued time series steps were numerically
differentiated by means of a central difference approach (Shoup, 1979) such that,
(xi xi1 )+(xi+1 xi )
2





dxi
dt

=

d2 x i
dt2


= (xi  xi1 )  (xi+1  xi ) 

i = 2N  1

A quantitative variable x is converted into a qualitative variable q = hqmag, qdiri, where
qmag  {(-,0), 0, (0,)} is generated from x, and qdir  {dec, std, inc} is generated from
dx/dt. The qualitative derivative of q, q, is obtained in a similar manner but is generated
from from dx/dt and d2 x/dt2 respectively.
The data are typically noisyeither inherently, or because of the process of differentiation
and we perform some simple smoothing of the first and second derivatives using a Blackman
filter (a relative of the moving average filter  see Blackman & Tukey, 1958). In each case,
the filter is actually applied to the result of a Fast Fourier Transform (FFT) and the result obtained by taking the real part of the inverse FFT. We note here that this form of
smoothing is appropriate only when a sufficient number of time steps are present.
Having obtained a (smoothed) numerical value x i for variable x at instant i, its qualitative magnitude qmag(xi ) is, in principle, simply obtained by the following:
qmag(xi ) =



 (, 0)

if xi < 0
0
if xi = 0


(0, +) otherwise
852

fiQualitative System Identification

In practice, since floating-point values are unlikely to be exactly zero, we have found
it advantageous to re-apply the filtering process to data straddling zero to eliminate small
fluctuations around this value. Despite these measures, in addition to generating correct
qualitative states (true positives) the conversion can produce errors: states generated may
not correspond to true states (false positives); and some true states may not be generated
(false negatives). Fig. 20 shows an example of this (the problem is, of course, exacerbated
further if the original quantitative data are noisy). The reason for these imperfect results
from noise free quantitative data are twofold: one is the smoothing process on small fluctuations around zero; but the main reason is that, as discussed above, creating a full qualitative
state involves numerical differentiation which introduces noise into the data for the derivatives that affects the ability of the process to convert from quantitative to qualitative with
absolute accuracy.
System
u-tube
Coupled
Cascaded
Spring

True
States
6
10
14
33

Generated
States
6
14
8
33

True
Positives
4
6
5
20

False
Positives
2
8
3
13

False
Negatives
0
0
6
0

Figure 20: An example of the errors resulting from generating qualitative states from traces
of system behaviour. Here, the traces were generated by the following initial
conditions: h1 = 2.0, h2 = 0.0 for all three tank systems; and disp M = 2.0,
velM = 0.0 for the spring.

4.2 Materials and Method
Numerical simulations of the four physical systems were constructed using the same general
relations as the qualitative models. Once again the experiments were carried out utilising
both noise free and noisy data, as described in the rest of this section.
4.2.1 Data
The models used for the numerical simulations had the same structure as the qualitative
models, but with the substitution of a real valued parameter for each monotonic function
relation. This gives a linear relation between the two variables; more complex, non-linear
functions might have been used, but linear functions provided a suitable approximation of
the known behavior of these systems, as shown graphically in Fig. 21 (a)(d); which is as
much as is required for this proof-of-concept study.
For a given set of function parameter values, initial conditions, and input value, a
quantitative model produces a single quantitative behaviour (this contrasts with qualitative
models that can produce a list of all possible behaviours for the model). Parameter values
were chosen so that the models approached a steady state during the time period of the
test. The models were implemented in Matlab 5.3 using the ODE15s ODE solver. Each
853

fiCoghill, Srinivasan, & King

3

3

Tank A
Tank B

2.9

Tank A
Tank B
2.5

2.8

2.7
2

Level

Level

2.6

2.5

1.5

2.4
1
2.3

2.2
0.5
2.1

2

0

50

100

150

200

250
300
Time in seconds

350

400

450

0

500

3

0

50

100

150

200

250
300
Time in seconds

350

400

450

500

20

Tank A
Tank B

Displacement
Velocity

15

2.5
10

Displacement/Velocity

Level

2

1.5

5

0

5

1
10

0.5
15

0

0

50

100

150

200

250
300
Time in seconds

350

400

450

20

500

0

50

100

150

200

250
300
Time in seconds

350

400

450

500

Figure 21: A graph of example numeric behavior of (a) the u-tube, top left; (b) the coupled
tanks, top right; (c) the cascaded tanks, bottom left, and (d) damped spring,
bottom right.

time point generated by the simulation was made available as part of the sampled data.
This ensures that the sampling rate is suitably fast with respect to the Nyquist criterion.
It also guarantees that a sufficient number of data points are available as required by the
Beckman filter.
4.2.2 Method
Noise-free data. We use the following method for evaluating ILP-QSIs system-identification
performance from noise-free data:
For each of the four test-systems:
(a) Obtain the system behaviour of the test system with a number of different initial
conditions and input values. Convert each of these into qualitative states using
the procedure in Section 4.1.1.
(b) Using all the qualitative states obtained as training data construct a set of models
using ILP-QSI and record the precision of the result (this is the proportion of
854

fiQualitative System Identification

the models in the result that are equivalent to the correct model). Thus, for each
training data set, the result returned by ILP-QSI will have a precision between
0.0 and 1.0.
The following details are relevant: (a) The quantitative models were each put into three
separate initial conditions, so that the magnitude of the two state variables were set to (2, 0),
(0, 3) and (2, 3). Specifically, these were the initial values of the two tank levels for all three
tank systems, and the displacement and velocity for the spring. These values were not
crucial but were chosen for the initial conditions because they caused the numerical models
to converge on the steady state for each system in a reasonable number of iterations; (b)
Each initial condition gave rise to a system behaviour and hence a set of qualitative states.
In the second step above, qualitative states from all behaviours is used as training data.
This is because kernel subsets necessary for correct system identification usually contain
qualitative states from multiple quantitative behaviours. (c) The conversion process results
in erroneous qualitative states (see Section 4.1.1). Thus, the training data used here contain
both false positives and false negatives.
Noisy data. We use the following method for evaluating ILP-QSIs system-identification
performance from noisy quantitative data:
For each of the four test-systems:
(a) Obtain the system behaviour of the test system with a number of different initial
conditions and input values.
(b) Corrupt each system behaviour with additive noise;
(c) Convert each corrupted behaviour into qualitative states using the procedure in
Section 4.1.1.
(d) Using all the qualitative states obtained as training data construct a set of models
using ILP-QSI and record the precision of the result (this is the proportion of
the models in the result that are equivalent to the correct model). Thus, for each
training data set, the result returned by ILP-QSI will have a precision between
0.0 and 1.0.
In the second step noise was added to the numerical data sets as follows. A Gaussian noise
signal (with a  of 0.0 and  of 1.0) was generated using by the built-in Matlab function
normrnd and scaled to three orders of magnitude of the original noise, namely 0.01, 0.1 and
1.0 (representing low, medium and high amounts of noise respectively). These scaled
noise variants were added to the numerical values of the system behaviour obtained from
each initial condition.
4.3 Quantitative Experimental Results
The process of converting from quantitative to qualitative states introduces errors, even
for noise free data. Table 4 shows the proportion of correct qualitative states to the total
number of qualitative states that were obtained from the numerical signal, including noisy
states. The table shows this proportion for all four systems, under the different degrees
855

fiCoghill, Srinivasan, & King

Model
u-tube

Coupled

Cascaded

Spring

Noise level
0
0.01
0.1
1
0
0.01
0.1
1
0
0.01
0.1
1
0
0.01
0.1
1

Initial States
(2,0)
(0,3)
(2,3)
4/6
4/6
3/5
2/8
2/8
2/9
2/10
2/10
2/15
2/37
2/37
2/53
6/14
5/13
5/14
6/16
4/14
4/12
6/16
5/25
4/15
6/58
6/61
4/46
5/8
5/8
3/8
4/10
4/12
2/9
4/17
4/21
2/13
4/39
4/49
4/38
20/33 19/35 22/39
23/48 20/36 18/41
23/49 20/38 18/44
20/65 20/52 19/53

Table 4: The input data for the numeric experiments, described as the proportion of the
number of clean states / the total number of converted states for different systems
and degrees of noise.

of noise. The numerical simulations were not intended to be exhaustive and do not cover
every possible such behaviour; so it is not surprising to observe that there is no case where
all the states of the complete envisionment are generated.
The results of the qualitative experiments detailed in the previous section indicate that
in order successfully to learn the target model data from all branches of the envisionment
are required, and the greater the number of such states used the greater the liklihood of
learning the target model structure. Therefore in these experiments we utilised all the states
generated from the numerical simulations.
The results from the numerical data experiments are shown in Fig. 22. These experiments show that it is possible to learn models from clean and noisy numerical data even
when the qualitative states generated from the clean numerical data contain a number of
unavoidable data transformation errors. The results for each of the systems used are as
follows:
The spring system This system has 31 states in the complete envisionment and Table 4
shows that the quantitative to qualitative conversion process yields around 20 of those
states. It can be seen from Fig. 17 that learning from 20 states gives 100% precision
in learning this target model, even in the presence of noise. It is not surprising,
therefore, to find that the learning precision is perfect up to the highest noise level.
Since the qualitative experiments were done by sampling, the slight downturn at the
highest noise level could be due to the large number of noisy states generated in this
856

fiQualitative System Identification

1
0.9
0.8

Utube
Coupled Tanks
Cascaded Tanks
Spring

Precision

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
0.01
0.1
1.0

Amount of Noise

Figure 22: A comparison of the results from the numerical learning tests, averaged over all
three initial conditions. Tests that attempted to learn from few states, such as
the cascaded tanks, are more likely to fail than those that have large numbers
of states, such as the spring. This is consistent with the kernel subset principle
introduced in Section 2.4 since model-learning is not precise without the presence
of certain key states in the input data.

experiment. Hence we can say that these results are in keeping with the qualitative
experiments.
u-tube & coupled tanks The complete envisonments for these systems contain 6 and 10
states respectively. Table 4 shows that the number of true states generated is less than
the complete envisionment, and significantly less than the number of noisy states in
each case. As one would expect from the results presented in Fig. 8 the u-tube gives
better results than the coupled tanks (having a higher proportion of the envisionment
states present). Ultimately, the ability to learn the model is completely curtailed
by the noise; though sooner in the case of the coupled tanks (which the qualitative
experiments show to be more sensitive to the presence of noise). This is consistent
with the results of the qualitative experiments.
The cascaded tanks In the qualitative learning experiments all the kernel subset triples
had state 0 included. This is the state representing the situation where both tanks
are empty to begin with, and is not one of the initial states included in the numerical
857

fiCoghill, Srinivasan, & King

simulations. Also, a perusal of Fig. 14 reveals that the introduction of noise radically
reduces the learning precision, and for 4 states (the average number of true states
generated by the qualitative to quantitative conversion process) it is zero. Taking
account of all these facts it was to be expected that the cascaded tanks model would
not be successfully identified by these experiments. This is again consistent with the
findings of the qualitative experiments.

5. Application to Biological System Identification
The work reported thus far has been aimed at demonstrating the viability of ILP-QSI and of
identifying the bounds of operation of the approach. In this section we examine scalability
of the method to identify a complex real world biological network. We use the glycolysis
pathway as a test case for identification.
5.1 The Test System: Glycolysis
We chose to study the metabolic pathway of glycolysis as a test case. Glycolysis is one of
the most important and ubiquitous in biology, it was also historically one of the first to be
discovered, and still presents a challenge to model accurately.
The QSIM primitives are sufficient to model adequately the qualitative behaviour of
the glycolysis pathway. There are, however, two problems. First, few biologists would
understand such a model, as he or she would reason at a much higher level of abstraction. Second, the computational complexity of the corresponding system identification task
for glycolsis (a qualitative model with 100 or more QSIM relations) is, at least currently,
intractable. We address both these by modelling metabolic pathways in a more abstract
manner using biologically meaningful metabolic components (MC) (a similar approach to
constructing complex qualitative models of the human heart was used in Bratko, Mozetic,
& Lavrac, 1989). Specifically, we note that in metabolic pathways, there are essentially two
types of object: metabolites (small molecules) and enzymes (larger molecules that catalyze
reactions). We use component models of each of these objects as described below (King,
Garrett, & Coghill, 2005).
5.1.1 Modelling Metabolites and Enzymes
The concentrations of metabolites vary over time as they are synthesised or utilised by
enzymatically catalysed reactions. As a result their concentration at any given time-point
is a function of: (a) their concentration at the previous time-point; and (b) the degree to
which they are used or created by various enzyme reactions.
When modelling enzymes, each enzyme is assumed to have at most two substrates and
at most two products. If there are two substrates or products these are considered to form
a substrate or product complex, such that the amount of the complex is proportional to
the amount of the substrates or products multiplied together. This models the probability
that both substrates (or products) will collide with the enzyme with sufficient timeliness to
be catalysed into the product complex (or substrate complex). The substrate complex is
converted into the product complex, which then disassociates into the product metabolites,
and vice versa. We shall use the phrase flow through the enzyme to denote the amount
858

fiQualitative System Identification

of substrate complex formed minus the amount of product complex formed. (See the work
of Voit and Radivoyevitch (2000) for details of enzyme kinetics.)

Metabolite

Metabolites1

Metabolites2
E nzm

Metab

*
S_for
w

dt
Mtb

M_dt

M+

-

M+

P_rev

Sum

-

FlowFlow+

*
Flow1  Flown

Metabolitep1

Metabolitep2

Figure 23: The Metabolic Components (MCs) used in the biological experiments, with the
underlying QSIM primitives.
Quantitative, and corresponding qualitative representations of the metabolite and enzymes using QSIM relations, are therefore:
Enzymes:

Metabolites:
n

X
dM
=
F lowi
dt
i=1
DERIV(M etabolite,Mdt),
SUM(F low1 , . . ., F lown ,
Md t).

(4)

F lowi = f (

S
Y

s=1

M etabolites)  g(

P
Y

M etabolitep)

(5)

p=1

PROD(M etabolite1, . . ., M etabolites, S-for),
PROD(M etabolite1, . . ., M etabolitep, P-rev),
M+(S-for, Ds),
M+(P-rev, Dp),
SUB(Ds, Dp, F low),
MINUS(F low,F lowminus).

Here, S refers to the input metabolites to an enzymatic reaction, or its substrates, and P
refers to the products of an enzymatic reaction. The SUM() and PROD() predicates are simply
extensions of the ADD() and MULT() predicates, over any number of inputs. Fig. 23 shows
how these constraints are grouped together as metabolic components (MCs). This permits
us to create more general constraints representing the metabolite and enzyme components
as follows:
ENZYME((S1 , S2 ) (P1 , P2 ) enzymeF low)
METABOLITE(metaboliteConc metaboliteF low (enzymeF low 1 . . . enzymeF lown ))
859

fiCoghill, Srinivasan, & King

1.
2.
3.
4.
5.
6.
7.
8.
9.
10.

(Hexokinase):
(Phosphoglucose isomerase):
(Phosphofructokinase):
(Aldolase):
(Triose phosphate isomerase):
(Glyceraldehyde 3-phosphate dehydrogenase):
(Phosphoglycerate kinase):
(Phosphoglycerate mutase):
(Enolase):
(Pyruvate kinase):

Glc + ATP  G6P + ADP.
G6P  F6P.
F6P + ATP  F16BP + ADP
F16BP  DHAP + G3P
DHAP  G3P
G3P + NAD  13BP + NADH.
13BP + ADP  3PG + ATP.
3PG  2PG.
2PG  PEP
PEP + ADP  Pyr + ATP.

Figure 24: The reactions included in our qualitative model of glycolysis. The reactions that
consume ATP and NADH are not explicitly included.

Here the ENZYME predicate identifies the substrates and products (the first argument) and
returns a single variable representing the flow through the enzyme (the second argument).
The METABOLITE predicate relates the level and flow of metabolites (the first and second
arguments) with the flow through enzymes (the third argument).
5.1.2 Modelling Glycolysis
Using qualitative components representing metabolites and enzymes, we construct a qualitative model of glycolysis. Our model uses 15 metabolites, namely: pyruvate (Pyr), glucose (Glc), phosphoenolpyruvate (PEP), fructose 6-phosphate (F6P), glucose 6-phosphate
(G6P), dihydroxyacetone phosphate (DHAP), 3-phosphoglycerate (3PG), 1,3-bisphosphoglycerate
(13BP), fructose 1,6-biphosphate (F16BP), 2-phosphoglycerate (2PG), glyceraldehyde 3phosphate (G3P), ADP, ATP, NAD, and NADH. We have not included H+, H 2 O, or Orthophosphate as they are assumed to be ubiquitous (in addition, the restriction of substrates
and products to being at most three in number prevents their inclusion).
The qualitative state of glycolysis is defined by the set of qualitative states of the 15
metabolites. Table 5 is a representation of one such qualitative state. To understand this
state consider the first entry intended to represent the qualitative state of NAD (that is,
NAD concentration: < 0, ), dec >, and NAD flow: < (, 0), dec >). The meaning
of this is that the concentration of NAD is positive (0, ) and decreasing (dec), and the
rate of change of the concentration of NAD (in analogy to the physical systems, the flow
of NAD) is negative (, 0) and decreasing (dec). Similar meanings apply to the other
metabolites. Note that metabolic concentrations must be between 0 and ; it cannot be
negative, and the 0 state is uninteresting.

Using this representation, a possible model for glycolysis is shown in Fig. 25. The model describes constraints on the levels and flows of metabolites. Thus, the constraint enzyme((G3Pc,
NADc), (13BPc, NADHc), Enz6f) states that the flow through enzyme 6 (Enz6f) controls the transformation of the concentrations G3Pc and NADc into the levels 13BPc and
NADHc; whereas the constraint metabolite(NADc, NADc, (Enz6f, -)) states that the
860

fiQualitative System Identification

Metabolite
NAD
NADH
ATP
ADP
Pyr
Glc
PEP
F6P
G6P
DHAP
3PG
13BP
F16PB
2PG
G3P

Concentration
< (0, ), dec >
< (0, ), inc >
< (0, ), dec >
< (0, ), dec >
< (0, ), inc >
< (0, ), dec >
< (0, ), dec >
< (0, ), dec >
< (0, ), dec >
< (0, ), dec >
< (0, ), inc >
< (0, ), std >
< (0, ), inc >
< (0, ), dec >
< (0, ), inc >

Flow
< (, 0), dec >
< (0, ), inc >
< (, 0), dec >
< (, 0), dec >
< (0, ), dec >
< (, 0), inc >
< (, 0), dec >
< (, 0), dec >
< (, 0), dec >
< (, 0), dec >
< (0, ), std >
< 0, inc >
< (0, ), dec >
< (, 0), dec >
< (0, ), inc >

Table 5: A legal qualitative state of the 15 metabolites observed during glycolysis.
ENZYME((Glcc, ATPc),(G6Pc,ADPc),Enz1f),
ENZYME((G6Pc),(F6Pc),Enz2f),
ENZYME((F6Pc,ATPc),(F16BPc,ADPc),Enz3f),
ENZYME((F16BPc),(G3Pc,DHAPc),Enz4f),
ENZYME((DHAPc),(G3Pc),Enz5f),
ENZYME((G3Pc,NADc),(13BPc,NADHc),Enz6f),
ENZYME((13BPc,ADPc),(3PGc,ATPc),Enz7f),
ENZYME((3PGc),(2PGc),Enz8f),
ENZYME((2PGc),(PEPc),Enz9f),
ENZYME((PEPc,ADPc),(Pyrc,ATPc),Enz10f),
METABOLITE(ATPc,ATPf, (Enz10f +), (Enz7f, +),(Enz1f, -),(Enz3f, -)),
METABOLITE(ADPc,ADPf,(Enz1f, +),(Enz3f, +),(Enz10f, -)(Enz7f, -)),
METABOLITE(NADc,NADf,(Enz6f, -)),
METABOLITE(NADHc,NADHf,(Enz6f, +)),
METABOLITE(Pyrc,Pyrf,(Enz10f, +)),
METABOLITE(Glcc,Glcf,(Enz1f, -)),
METABOLITE(PEPc,PEPf,(Enz9f, +),(Enz10f, -)),
METABOLITE(F6Pc,F6Pf,(Enz2f, +),(Enz3f, -)),
METABOLITE(G6Pc,G6Pf,(Enz1f, +),(Enz2f, -)),
METABOLITE(DHAPc,DHAPf,(Enz4f, +),(Enz5f, -)),
METABOLITE(3PGc,3PGf,(Enz7f, +),(Enz8f, -)),
METABOLITE(13BPc,13BPf,(Enz6f, +),(Enz7f, -)),
METABOLITE(F16BPc,F16BPf,(Enz3f, +), (Enz4f, -)),
METABOLITE(2PGc,2PGf,(Enz8f, +),(Enz9f, -)),
METABOLITE(G3Pc,G3Pf,(Enz5f, +),(Enz4f, +),(Enz6f, -)).

Figure 25: A representation of a qualitative model of glycolysis (see text for details).

concentration (NADc) and flow (NADf) of the metabolite NAD is controlled by flow through
the single enzyme number 6 (Enz6f : Glyceraldehyde 3-phosphate dehydrogenase), and that
this enzyme removes (signified by the -) NAD (+ would mean the enzyme flow adds the
corresponding metabolite).
861

fiCoghill, Srinivasan, & King

5.2 Experimental Aim
The specific system identification task we were interested in is: Given qualitative observations of metabolic states, can ILP-QSI identify a correct qualitative model for glycolysis?
5.3 Materials and Method
Our methodology is depicted in Fig. 26, where we describe two separate ways of identifying
biochemical pathways. We make the following assumptions:
1. The data are sparse and not necessarily measured as part of a continuous time series.
This is realistic given current experimental limitations in metabolomics, and rules out
the possibility of numerical system identification approaches.
2. Only metabolites of known structure are involved in the model. The reason for this
is that we employ a chemoinformatic heuristic to decrease the number of possible
reactions. The heuristic is based on the reasonable assumption that any chemical
reaction catalysed by an enzyme only breaks a few chemical bonds. Full details are in
the paper by King et al. (2005). This is the strongest assumption we make. Even given
the rapid advance of metabolomics (NMR, mass-spectroscopy, etc.), it is not currently
realistic to assume that all the relevant metabolites in a pathway are observed and
their structure determined.
3. Only metabolites of known structure are involved in a particular pathway. This is
a restriction because current metabolomics technology can observe more compounds
than can be structurally identified.
4. All reactions involve at most three substrates and three products.
5. For the qualitative states: we can measure the direction of change in metabolite level
and first-derivative. This requires sampling the level at least three times in succession.
5.3.1 Logical/Graph-based Constraints
We first considered the logical/graph-based (LG) nature of the problem. The specific domain of metabolism imposes strong constraints on possible LG based models. We used
these constraints in the following way:
1. Chemical reactions conserve matter and atom type (Valdes-Perez, 1994). For glycolysis we generated all possible ways of combining the 18 metabolites to form matter and
atom type balance reactions ( 3 reactants and  3 products). This produced 172
possible reactions where the substrates balanced the products in the number and type
of each element. The number compares well with the 2,300,000 possible reactions
which would naively be possible.
2. Typical biochemical reactions only make/break a few bonds, and cannot arbitrarily
rearrange atoms to make new compounds. A reaction was considered plausible if it
broke 1 bond per reactant. This analysis was done originally by hand, and we have
subsequently developed a general computer program that can automate this task.
862

fiQualitative System Identification

LG Modelling

All possible
combinations of
metabolites
Conservation of mass
and element type.

QR Modelling

Bonds broken

Typical
biochemistry

QR System
Identification

QR

2

Glycolysis

Glycolysis

Figure 26: Our Metabolic System Identification methodology.
Of the 172 balanced reactions 18 were considered chemically plausible. Of these 18
reactions, 10 are the actual reactions of glycolysis and 8 are decoy reactions.
5.3.2 Qualitative Reasoning Constraints
We used a simple generate and test approach to learning. For the first computational
experiment we used the 10 reactions of glycolysis and the 8 decoy reactions that were
considered chemically feasible, see Fig. 24. All these reactions, in the absence of evidence
to the contrary, are considered to be irreversible. We first generated all possible ways of
combining the 18 reactions which connected all the 15 main substrates in glycolysis (models
are non-disjoint). This generated 27,254 possible models with  10 reactions - it was not
necessary to look for models with more reactions than that of the target (parsimony), as
the models can be generated in size order. The smallest number of reactions necessary to
include all 15 metabolites was of size 5. All of the 27,254 models involved the reaction:
glyceraldehyde 3-phosphate + NAD  1,3-bisphosphoglycerate + NADH (reaction 6); so
we could immediately conclude that this reaction occurred in glycolysis.
863

fiCoghill, Srinivasan, & King

We formed example qualitative states of glycolysis using our QR simulator (in a pseudo
random manner) to test these models. The states thus generated did not contain any noise.
The 27,254 possible models were then tested against these states, and if a model could
not generate a particular state it was removed from consideration (accuracy constraint).
Note that the flows of the metabolites through each enzyme are not observed - they are
intermediate variables. All we observe are the overall levels and flows of the metabolites.
This makes the system identification task much harder.
For efficiency, we used the fast YAP Prolog compiler. We also formed compiled down
versions of the enzyme and metabolite MCs (input/output look-up tables), and compiled
down parts of QSIM. We also adopted a resource allocation method that employed increasingly computationally expensive tests: i.e. forming filter tests with exponentially increasing
numbers of example states.
5.3.3 Results
After several months of compute time on a 65 node Beowulf cluster we reduced the 27,254
possible models down to 35 (a 736 fold reduction). These models included the target model
(glycolysis), plus 34 other models that could not be qualitatively distinguished from it. All
35 models included the following six reactions (see Fig. 24):
3. F6P + ATP  F16BP + ADP
4. F16BP  DHAP + G3P
5. DHAP  G3P
6. G3P + NAD  13BP + NADH
8. 3PG  2PG
9. 2PG  PEP
These reactions form the core of glycolysis.
Examining the 35 models also revealed that the correct model had the fewest cycles,
however we do not know if this is a general phenomenon.
We attempted to use the Progol positive only compression measure to distinguish between these models. This is based on comparing the models on randomly generated states.
However, this was unsuccessful as no model covered any of the 100,000 random states we
generated! We believe this is due to the extremely large state space. However, a simple
modification of this approach does work. If we produce random states from glucogenesis
(glycolysis driven in the reverse direction), then the true model of glycolysis covers fewer
examples than any of the 34 alternatives, and so is identified as the true model. Note that
this approach, unlike the use of Progol positive only compression measure, requires that
new experimental data is obtained.
Thus we have demonstrated that the method for learning qualitative models of dynamic
systems is scalable to handle a relatively large metabolic system. We have achieved this by
means of MCs that represent meaningful units in the domain, and which map directly to
the QSIM constraints from which they are abstracted. They also enable us to present these
more complex models in a more user friendly manner, removing the need to understand
the structure of high order differential equations.
864

fiQualitative System Identification

6. Related Work
System identification has a long history within machine learning: we present below some
of the important signposts that were directly relevent to the work here. We focus on the
strand of research which deals with learning qualitative models of dynamic systems.
The earliest description of which we are aware concerning a computer program identifying a quantitative model to explain experimental data is the work by Collins (1968). There
a procedure is described that heuristically searches through equation structures, which are
linear combination of functions of the observed variables. Better known is the Bacon
system (Langley, 1981), early versions of which largely concentrated on the parameter estimation problem, in particular selecting the most appropriate values for any exponents in
the equations. For example, given a class of algebraic equation structures Bacon.1 was able
to reconstruct Keplers model for planetary motion from data. While later work (for example the work of Nordhausen & Langley, 1993) attempted to extend this work to deal with
identifying both the algebraic structure and the relevant parameters, Bacon highlighted
the importance of bias (Mitchell, Keller, & Kedar-Cabelli, 1986) in machine learning, both
in constraining the possible model structures and in the space of possible models conforming to those structures. Other quantitative equation discovery systems in this lineage are:
Coper (Kokar, 1985), that uses dimensional analysis to restrict the space of equations;
Fahrenheit/EF (Langley & Zytkow, 1989) and E  that only examine the space of bivariate equations; Abacus (Falkenhainer & Michalski, 1986) that can identify piecewise
equations; Sds that uses type and dimensionality restrictions to constrain the space of
equations; the Lagrange family of equation finders (Dzeroski & Todorovski, 1993; Todorovski & Dzeroski, 1997; Todorovski et al., 2000; Todorovski, 2003) which attempt to identify
models in the form of ordinary and partial differential equations; and IPM (Langley, George,
Bay, & Saito, 2003) with its extensions and developments, Prometheus/RPM (Bridewell,
Sandy, & Langley, 2004; Asgharbeygi, Bay, Langley, & Arrigo, 2006), which incorporate
process descriptions (Forbus, 1984) to aid the construction and revision of quantitative
dynamic models.
Focussing specifically on non-classical system identification for metabolic models, perhaps the most notable work on identification is that of Arkin, Shen, and Ross (1997) who
identified a graphical model of the reactions in a part of glycolysis from experimental data.
The work of Reiser, King, Kell, Muggleton, Bryant, and Oliver (2001) presents a unified
logical approach to simulation (deduction) and system identification (induction and abduction). An interesting recent approach, presented by Koza, Mydlowec, Lanza, Yu, and Keane
(2000), examines the identification of metabolic ODE models using genetic programming
techniques. In this, the cellular system is viewed as an electrical circuit and the space of
possible circuits is searched by means of a genetic programming approach.
The earliest reported work on the identification of qualitative models is that of Mozetic,
1987, and colleagues, who identified a model of the electrical activity of the heart. This work,
reported more fully in (Bratko et al., 1989) remains a landmark effort in the qualitative
modelling of a complex biological system. However, as other researchers have noted (Bratko,
Muggleton, & Varsek, 1992), these results were obtained only for static models and did not
provide insight into how models of dynamic systems should be identified.
865

fiCoghill, Srinivasan, & King

The first machine learning system for learning qualitative models of dynamic systems
was Genmodel (Coiera, 1989a, 1989b). Genmodel did not need any negative examples of
system behaviour and models learned were restricted to qualitative relationships amongst
the observed variables only (that is, no intermediate, or hidden, variables were hypothesized). The model, obtained using the notion of a most specific generalization of observed
variables (in the sense of Plotkin, 1971), was usually over-constrained. That is, it contained
more constraints than necessary to characterize fully the dynamics of the system being
modeled. An updated version of Genmodel developed by Hau and Coiera (1993) showed
that dimensional analysis (Bhaskhar & Nigam, 1990) could be used as a form of directed
negative example generation. The new version could learn from from real-valued experimental data (which were converted internally into a qualitative form), but still required all
variables to be known and measured from the outset. The system MISQ, entirely similar
in complexity and abilities to the earlier version of Genmodel was developed by Kraan,
Richards, and Kuipers (1991). This was later re-implemented in a general-purpose relational learning program Forte (Richards & Mooney, 1995), which allowed the hypothesis
of intermediate variables (Richards, Kraan, & Kuipers, 1992). The relational pathfinding
approach used by MISQ (through the auspices of Forte) is a special form of Inductive Logic
Programming, the general framework of which is much more powerful
Bratko and colleagues were the first to view the problem of learning dynamic qualitative
models explicitly as an exercise in Inductive Logic Programming (ILP) and first demonstrated the possibility of introducing intermediate (unobserved) variables in the models.
They used the ILP system Golem (Muggleton & Feng, 1990) along with the QSIM representation to produce a model of the u-tube system. The model identified was equivalent to
the accepted model (in the sense that it predicted the same behaviour) but the structure
generated was not in a form that could help explain the behaviour (Coghill & Shen, 2001).
Like Genmodel, the model produced was over constrained. Unlike Genmodel, Golem
required both positive and negative examples of system behaviour and was shown by Hau
and Coiera (1993) to be sensitive to the actual negative examples used.
Say and Kuru (1996) describe a program for system identification from qualitative data
called QSI. QSI first finds correlations between variables, and then iteratively introduces
new relations (and intermediate variables), building a model and comparing the output
of that model with the known states until a satisfactory model is found. Say and Kuru
characterized this approach as one of diminishing oscillation as it approaches the correct
model. Like Genmodel and MISQ, QSI does not require negative observations of system
behaviour. Unlike those systems, it does not use dimensional analysis and there does not
appear to be any mechanism of incorportating such constraints easily within the program.
The importance of dimensional analysis is recognised though: the authors suggest that it
should be central to the search procedure.
Thus, while the identification of quantitative models has had a longer history in machine
learning, learning qualitative models has also been the subject of notable research efforts.
In our view, MISQ (the version as implemented within Forte) and QSI probably represent
the state-of-the-art in this area. Their primary shortcomings are these:
866

fiQualitative System Identification

 It is not apparent from the description or experimental evaluation of MISQ whether
or not it is able to handle imperfect data (the correctness theorem presented only
applies for complete, noise-free data).
 MISQ seeks the most constrained model that is consistent with the data. Often,
exactly the opposite is sought (that is, we want the most parsiomonius model).
 QSI only deals with qualitative data and does not appear to include any easy mechanism for the incorporation of new constraints to guide its search.

7. General Discussion
In this paper we have presented a method for learning qualitative models of dynamic systems
from time-series data (both qualitative and quantitative). In this section we discuss the
general findings and limitations, as well as suggesting a number of directions for developing
this research theme.
7.1 Computational Limitations
A major limitation of the ILP-QSI system in identifying glycolysis was the time taken
(several months on a Beowulf cluster) to reduce the models from the 27,000 possible ones
generated using chemoinformatic constraints, to the single correct one using the qualitative
state constraints. While it would be preferable for this process to be faster, it is important
to note that identifying a system with 10 reactions and 15 metabolites from scratch is an
extremely hard identification task. We doubt that any human could achieve it, and we
believe it would be a challenge for all the system identification methods we are aware of.
It is difficult to compare system identification methods and we believe there is a need for
competitions such as those run by KDD to compare methods.
The computational time of identification is dominated by the time taken to test if a
particular model can produce certain observed states: examining 27,000 models is not
unusual for a machine learning program, but it is unusual for a program to take hours
to test if individual examples are covered. The slow speed of our identification method is
therefore not a problem with what is normally considered the learning method (i.e. how
the search of the space of possible models is done), but rather, is intrinsic to the complex
relationship between a model and the states it defines. The cover-test method is, in the worst
case, exponential in the maximum size of the model. Note that our lack of an efficient, i.e.
polynomial algorithm, to determine cover is not because we are using qualitative states. We
believe that the inherent difficulty of this task applies to both quantitative and qualitative
models. In some areas of mathematics moving from the discrete to the real domain can
simplify problems - this is the basis of much of the power of analysis. However, there
is currently little evidence that this is the case in system identification, and quantitative
models would seem to aggravate the problem. As cover tests are essentially deductions: can
a set of axioms and rules (computer program/model) produce a particular logical sentence
(observed state); they are in general non-computable. However, in real scientific systems,
as they are bounded in space and time, non-computability is not a problem, however we
expect all system identification methods to struggle with the task (Sadot, Fisher, Barak,
Admanit, Stern, Hubbard, & Harel, 2008).
867

fiCoghill, Srinivasan, & King

7.2 Kernel Subsets
From our presentation of the results of the experimentation it is clear that certain subsets
of states (termed the kernel subsets) guarantee that the target model will be learned. From
the analysis of these kernel subsets of state sets, we hypothesise that the kernel sets reflect
the qualitative structure of the system of interest.
For a coupled system, in order to learn the structure of a system with a high degree of
precision, the data used should come from tests yielding qualitatively different behaviors:
i.e. behaviors which would appear as distinct branches in an envisionment graph. However,
this hypothesis only provides a necessary, not a sufficient, condition for learning because it
does not identify which states in each branch are suitable starting points for an experiment.
For example consider the coupled tanks system. One could select states 9 and 11; these are
from different branches yet they do not form a kernel subset. On the other hand, it was
noted in presenting the results for this system that the key states in these kernel subsets
were states 7 and 8. These states are in different branches and represent the critical points
of the first derivative of the state variables of the system. This indicates the importance of
these states to the definition of a system.
If a test were set up in which all the state variables were at their critical points then the
test could be run for a very short time and the correct model structure identified. However,
it is probably impossible in practice to set up such a test; especially in the situation where
the structure of the system is completely unknown. An alternative is to set up multiple tests
with the state variables set to their extrema: from which initial conditions all the states
of the envisionment will eventually be passed through. However it still may be difficult to
set up such tests, and they could take a long time to complete. These two scenarios form
the ends of a spectrum within which the most practical experimental setting will lie. The
identification of the best strategies is an important area of research to which the present
work is clearly relevant.
On the other hand, for cascaded systems the kernel sets capture the asymmetry in the
structure. Here again the extrema and critical points play an important role; but in this
case it is subordinate to the fact that ILP-QSI automatically decomposes the system into
its constituent parts for learning. This fact points to an important conclusion for learning
larger scale complex systems; namely that the learning can be facilitated by, where possible,
decomposing the system into cascaded subsystems.
7.3 Future Work
Having validated ILP-QSI on data derived from real biological systems, the next step is to
explore how successful it can be at modelling real experimental data. It would be relatively
straightforward to obtain data from water tanks and springs, but it would be much more
interesting to work on real biological data. For such work to be successful it is likely that
the quantitative to qualitative conversion process will need to be improved. Although not
the focus of the work here, developing a more rigorous approach would be crucial in using
ILP-QSI in a laboratory setting (Narasimhan, Mosterman, & Biswas, 1998). Once this has
been done it will be much easier to use real experimental data for analysis by ILP-QSI.
Specifically, the improvements required are the ability to extract all the qualitative states
868

fiQualitative System Identification

passed through during a numerical simulation, whilst minimizing noise. Nevertheless, this
is not a direct limitation of our ILP-QSI method.
The following possibilities would benefit from further investigation:
 The QR representation used could be changed from QSIM to a more detailed and
flexible one such as Morven (Coghill & Chantler, 1994; Coghill, 1996).
 The hypotheses presented about kernel subsets, such as why they are formed from
some states and not others, need to be confirmed and analyzed further.
 The ability to map and explore the features of the model space would be of great
use in planning further enhancements and, alongside kernel subsets, will help give an
understanding of exactly which states will allow reliable learning.
 Large scale complex systems are generally identified piece by piece. The results from
the cascaded tanks experiments indicate some circumstances under which this may
be most easily facilitatied. Further investigation of this is warranted.
 As an alternative to the methods described in this paper, an incremental approach that
identifies subsystems of the complete system is an interesting avenue of investigation
(Srinivasan & King, 2008).

8. Summary and Conclusions
In this paper we have presented a novel system, named ILP-QSI, which learns qualitative
models of dynamic processes. This system stands squarely in a strand of research that
integrates Machine Learning with Qualitative Reasoning and extends the work in that area
in the following ways:
The ILP-QSI algorithm itself extends the work; it is a branch and bound algorithm that
makes use of background knowledge of (at least) three kinds in order to focus and guide
the search for well posed models of dynamic processes.
Syntactic Constraints: The model size is prespecified; models must be complete and
determinate; and must not proliferate instances of qualitative relations.
Semantic Constraints: The model must adequately explain the data; it must not contain
relations that are redundant or contradictory; and the relations in the model must
respect dimensional constraints.
System Theoretic Constraints: The model should be singular and not disjoint; all endogenous variables must appear in at least two relations; and the model should be
causally ordered.
We have thoroughly tested the system on a number of well known dynamic processes.
This has enabled us to ascertain that ILP-QSI is capable of learning under a variety of
conditions of noisy and noise free data. This testing has also allowed us to identify some
conditions under which it is possible to learn an appropriate model of a dynamic system.
The conclusions from this aspect of the work are:
869

fiCoghill, Srinivasan, & King

 Learning precision is related to the richness (or sparcity) and noisiness of the data
from which the learning is performed.
 The target model is precisely learnt if the data used is a kernel subset.
 These kernels are made up of states from different branches in the envisionment graph.
 The system critical points play an important role in identifying the model structure.
 There is a spectrum of possibilities with regard to the setting up of suitable experiments to garner data from which to learn models of the physical or biological systems
of interest.
 Cascaded parts of systems help to identify suitable points of decomposition for model
learning.
While ILP-QSI is designed to learn a qualitative structural model from qualitative data,
it is sometimes the case that the original measurements are quantitative (albeit sparse and
possibly noisy). In order to ascertain how ILP-QSI would cope with qualitative data generated from quantitative measurements we carried out a proof-of-concept set of experiments
from each of the physical process models previously utilised. The results from these were
in keeping with the results obtained from the qualitative experiments. This adds weight
to the conclusions regarding the viability of our approach to learning structural models of
dynamic systems under adverse conditions.
Finally, in order to test the scalability of the method, we applied ILP-QSI to a large
scale metabolic pathway: glycolysis. In this case the search space was deemed too large to
attempt learning the QSIM primitives alone. However, knowledge of the domain enabled us
to group these primitives into a set of Metabolic Components from which models of metabolic
pathways can more easily be constructed. Also, for this part of the research Logical graph
based models were used to represent background domain knowledge. Utilising these, we
were able to identify 35 possible structures for the glycolysis pathway (out of a possible
27,254); of these the target model had the fewest cycles (though we do not know if this is a
general phenomenon) and minimally covered the data generated from the reverse pathway
of glucogenesis.
The overall conclusions of the this work are that qualitative reasoning methods combined
with machine learning (specifically ILP) can successfully learn qualitative structural models
of systems of high complexity under a number of adverse circumstances. However, the work
reported herein constitutes a step in a line of research that has only recently begun; and,
as with all interesting lines of research, it raises in its turn interesting questions that need
to be addressed.
Acknowledgments
This work was supported in part by BBSRC/EPSRC grant BIO10479. The authors would
like to thank Stephen Oliver and Douglas Kell for their helpful discussions on the biological
aspects of this paper. We would also like to thank Simon Garrett for many interesting and
fruitful interactions.
870

fiQualitative System Identification

Appendix A. The Derivation of the Solution Space for the Tanks Systems
In this appendix we provide a summary of whence the solution spaces for the tanks systems
utilised in this project are constructed. Further details regarding envisionments and their
associated solution spaces may be found in the work of Coghill et al. (1992) and Coghill
(2003).
In order to facilitate this analysis we will need to make use of a quantitative version of
the system models. For ease of exposition we will make the additional assumption that the
systems are linear.10
A.1 The U-tube
A quantitative model of the u-tube system is
dh1
= k(h1  h2 )
dt
dh2
= k(h2  h1 )
dt
By inspection of these two equations it is easy to see that (ignoring the trivial case where
k = 0) the derivatives in these two equations are both zero when:
h1 = h 2

(6)

That is:
dh2
dh1
=
=0
dt
dt
This accounts for the relationship, depicted in Fig. 18, between h 1 and h2 when the
derivatives are zero. From the envisionment table for the u-tube (Table 1 in Section 2.2) we
see that the only state with zero derivatives is state 5; hence it is represented by this line.
h1 = h 2 

A.2 The Coupled Tanks
A quantitative model of the coupled tanks system is
dh1
= qi  k1 (h2  h1 )
dt
dh2
= k1 (h2  h1 )  k2  h2
dt
When

dh1
dt

(7)
(8)

= 0 Equation 7 can be rewritten as:
0 = qi  k1 (h2  h1 )
= q i  k 1 h2  k 1 h1

10. In fact for the types of non-linearity normally associated with systems of this kind the solution spaces
are qualitatively identical to those described here, although the analysis required to construct them is
slightly more complicated.

871

fiCoghill, Srinivasan, & King

which can be re-arranged to give
h2 =

qi
 h1
k1

When qi is zero this reduces to
h2 = h 1
When

dh2
dt

(9)

= 0 Equation 8 can be rewritten as:
0 = k1 (h2  h1 )  k2 h2
= k 1 h1  k 1 h1  k 2 h2
= (k1  k2 )h2  k1 h1

so
(k1  k2 )h2 = k1 h1
Re-arranging gives
h2 =

k1
h1
(k1  k2 )

(10)

This accounts for the relations between h 1 and h2 depicted in the solution space of Fig.
18.
A.3 The Cascaded Tanks
A quantitative model of the cascaded tanks system is:

When

dh1
dt

dh1
= q i  k 1 h1
dt

(11)

dh2
= k 1 h1  k 2 h2
dt

(12)

= 0 Equation 11 can be re-arranged as:
qi = k 1 h1

or
h1 =
When

dh2
dt

qi
k1

= 0 Equation 12 can be rewritten as:
k2 h2 = k 1 h1

or
872

fiQualitative System Identification

h2 =

k1
h1
k2

This accounts for the relations between h 1 and h2 depicted in the solution space of Fig.
27.

h2

h'1 = 0

3

7

11

1

h'2 = 0

4
12

8
5
9

13

h1

0

6

10

2

Figure 27: The solution space for the cascaded tanks system.

References
Arkin, A., Shen, P., & Ross, J. (1997). A test case of correlation metric construction of a
reaction pathway from measurements. Science, 277, 12751279.
Asgharbeygi, N., Bay, S., Langley, P., & Arrigo, K. (2006). Inductive revision of quantitative
process models. Ecological modelling, 194, 7079.
Bergadano, F., & Gunetti, D. (1996). Inductive Logic Programming: From Machine Learning
to Software Engineering. MIT Press.
Bhaskhar, R., & Nigam, A. (1990). Qualitative physics using dimensional analysis. Artificial
Intelligence, 45, 73111.
Blackman, R. B., & Tukey, J. W. (1958). The Measurement of Power Spectra. John Wiley
and Sons, New York.
Bradley, E., Easley, M., & Stolle, R. (2000). Reasoning about nonlinear system identification. Tech. rep. CU-CS-894-99, University of Colorado.
Bratko, I., Mozetic, I., & Lavrac, N. (1989). KARDIO: A Study in Deep and Qualitative
Knowledge for Expert Systems. MIT Press, Cambridge, Massachusetts.
873

fiCoghill, Srinivasan, & King

Bratko, I., Muggleton, S., & Varsek, A. (1992). Learning qualitative models of dynamic
systems. In Muggleton, S. (Ed.), Inductive Logic Programming, pp. 437452. Academic
Press.
Bridewell, W., Sandy, J., & Langley, P. (2004). An interactive environment for the modeling
and discovery of scientific knowledge.. Tech. rep., Institute for the Study of Learning
and Expertise, Palo Alto, CA.
Camacho, R. (2000). Inducing Models of Human Control Skills using Machine Learning
Algorithms. Ph.D. thesis, University of Porto.
Coghill, G. M. (1996). Mycroft: A Framework for Constraint-Based Fuzzy Qualitative Reasoning. Ph.D. thesis, Heriot-Watt University.
Coghill, G. M. (2003). Fuzzy envisionment. In Proc. of the Third International Workshop
on Hybrid Methods for Adaptive Systems, Oulu, Finland.
Coghill, G. M., Asbury, A. J., van Rijsbergen, C. J., & Gray, W. M. (1992). The application of vector envisionment to compartmental systems.. In Proceedings of the first
international conference on Intelligent Systems Engineering, pp. 123128, Edinburgh,
Scotland.
Coghill, G. M., & Chantler, M. J. (1994). Mycroft: a framework for qualitative reasoning. In Proceedings of the Second International Conference on Intelligent Systems
Engineering, pp. 4955, Hamburg, Germany.
Coghill, G. M., Garret, S. M., & King, R. D. (2004). Learning qualitative models of
metabolic systems. In Proceedings of the European Conference on Artificial Intelligence ECAI-04, Valencia, Spain.
Coghill, G. M., & Shen, Q. (2001). On the specification of multiple models for diagnosis of
dynamic systems. AI Communications, 14 (2), 93104.
Coiera, E. W. (1989a). Generating qualitative models from example behaviours. Tech. rep.
8901, University of New South Wales, Deptartment of Computer Science.
Coiera, E. W. (1989b). Learning qualitative models from example behaviours. In Proc.
Third Workshop on Qualitative Physics, Stanford.
Collins, J. (1968). A regression analysis program incorporating heuristic term selection. In
Dale, E., & Michie, D. (Eds.), Machine Intelligence 2. Oliver and Boyd.
Dzeroski, S. (1992). Learning qualitative models with inductive logic programming. Informatica, 16 (4), 3041.
Dzeroski, S., & Todorovski, L. (1993). Discovering dynamics. In International Conference
on Machine Learning, pp. 97103.
Dzeroski, S., & Todorovski, L. (1995). Discovering dynamics: from inductive logic programming to machine discovery. J. Intell. Information Syst., 4, 89108.
Falkenhainer, B., & Michalski, R. S. (1986). Integrating quantitative and qualitative discovery: The abacus system. Machine Learning, 1 (4), 367401.
Forbus, K. D. (1984). Qualitative process theory. Artificial Intelligence, 24, 169204.
874

fiQualitative System Identification

Garrett, S. M., Coghill, G. M., Srinivasan, A., & King, R. D. (2007). Learning qualitative
models of physical and biological systems. In Dzeroski, S., & Todorovski, L. (Eds.),
Computational discovery of communicable knowledge, pp. 248272. Springer.
Gawthrop, P. J., & Smith, L. P. S. (1996). Metamodelling: Bond Graphs and Dynamic
Systems. Prentice Hall, Hemel Hempstead, Herts, England.
Hau, D. T., & Coiera, E. W. (1993). Learning qualitative models of dynamic systems.
Machine Learning, 26, 177211.
Healey, M. (1975). Principles of Automatic Control. Hodder and Stoughton.
Iwasaki, Y., & Simon, H. A. (1986). Causality in device behavior. Artificial Intelligence,
29, 332. See also De Kleer and Browns rebuttal and Iwasaki and Simons reply to
their rebuttal in the same volume of this journal.
King, R. D., Garrett, S. M., & Coghill, G. M. (2005). On the use of qualitative reasoning
to simulate and identify metabolic pathways.. Bioinformatics, 21 (9), 2017  2026.
Kokar, M. M. (1985). Coper: A methodology for learning invariant functional descriptions.
In Mitchell, T., Carbonell, J., & Michalski, R. (Eds.), Machine Learning: A Guide to
Current Research, pp. 151154. Kluwer Academic Press.
Koza, J. R., Mydlowec, W., Lanza, G., Yu, J., & Keane, M. A. (2000). Reverse engineering
and automatic synthesis of metabolic pathways from observed data using genetic
programming.. Tech. rep. SMI-2000-0851, Stanford University.
Kraan, I. C., Richards, B. L., & Kuipers, B. J. (1991). Automatic abduction of qualitative
models. In Proceedings of Qualitative Reasoning 1991 (QR91).
Kuipers, B. (1994). Qualitative Reasoning. MIT Press.
Langley, P. (1981). Data-driven discovery of physical laws. Cognitive Science, 5, 3154.
Langley, P., George, D., Bay, S., & Saito, K. (2003). Robust induction of process models from
time series data.. In Proc. twentieth International Conference on Machine Learning,
pp. 432439, Washington, DC. AAAI Press.
Langley, P., & Zytkow, J. (1989). Data-driven approaches to empirical discovery. Artificial
Intelligence, 40, 283312.
McCreath, E. (1999). Induction in first order logic from noisy training examples and fixed
example set sizes. Ph.D. thesis, University of New South Wales.
Mitchell, T. M., Keller, R. M., & Kedar-Cabelli, S. (1986). Explanation-based generalization: A unifying view. Machine Learning, 1, 4780.
Mozetic, I. (1987). Learning of qualitative models. In Bratko, I., & Lavrac, N. (Eds.),
Progress in Machine Learning: Proceedings of EWSL 87: 2nd European Working Session on Learning, pp. 201217. Sigma Press.
Muggleton, S. (1995). Inverse Entailment and Progol. New Gen. Comput., 13, 245286.
Muggleton, S. (1996). Learning from positive data. Lecture Notes in AI, 1314, 358376.
Muggleton, S., & Feng, C. (1990). Efficient induction of logic programs. In Proc. of the
First Conf. on Algorithmic Learning Theory. OHMSHA, Tokyo.
875

fiCoghill, Srinivasan, & King

Muggleton, S., & Raedt, L. D. (1994). Inductive logic programming: Theory and methods.
Journal of Logic Programming, 19,20, 629679.
Narasimhan, S., Mosterman, P., & Biswas, G. (1998). A systematic analysis of measurement selection algorithms for fault isolation in dynamic systems. In Proc. Ninth Intl.
Workshop on Principles of Diagnosis (DX-98), pp. 94101, Cape Cod, MA.
Nordhausen, B., & Langley, P. (1993). An integrated framework for empirical discovery.
Machine Learning, 12, 1747.
Papadimitriou, C., & Steiglitz, K. (1982). Combinatorial Optimisation. Prentice-Hall,
Edgewood-Cliffs, NJ.
Plotkin, G. (1971). Automatic Methods of Inductive Inference. Ph.D. thesis, Edinburgh
University.
Reiser, P., King, R., Kell, D., Muggleton, S., Bryant, C., & Oliver, S. (2001). Developing a
logical model of yeast metabolism. Electronic Transactions on Artificial Intelligence,
5, 233244.
Richards, B. L., Kraan, I., & Kuipers, B. J. (1992). Automatic abduction of qualitative
models. In Proc. of the Tenth National Conference on Artificial Intelligence (AAAI92), pp. 723728. MIT Press.
Richards, B. L., & Mooney, R. J. (1995). Automated refinement of first-order horn-clause
domain theories. Machine Learning, 19 (2), 95131.
Riguzzi, F. (2005). Two results regarding refinement operators. In Kramer, S., & Pfahringer,
B. (Eds.), Late Breaking Papers, 15th International Workshop on Inductive Logic
Programming (ILP05), August 1013, 2005, pp. 5358, Munich, Germany.
Sadot, A., Fisher, J., Barak, D., Admanit, Y., Stern, M. J., Hubbard, E. J. A., & Harel, D.
(2008). Towards verified biological models.. IEEE/ACM Trans. Comput. Biology and
Bioinformatics., 5(2), 112.
Say, A. C. C., & Kuru, S. (1996). Qualitative system identification: deriving structure from
behavior. Artificial Intelligence, 83, 75141.
Shoup, T. E. (1979). A Practical Guide to Computer Methods for Engineers. Prentice-Hall
Inc., Englewood Cliffs, N. J. 07632.
Soderstrom, T., & Stoica, P. (1989). System Identification. Prentice Hall.
Srinivasan, A. (1999). The Aleph Manual. Available at http://www.comlab.ox.ac.uk/oucl/
research/areas/machlearn/Aleph/.
Srinivasan, A., & King, R. D. (2008). Incremental identification of qualitative models of
biological systems using inductive logic programming. J. Machine Learning Research
to appear.
Todorovski, L. (2003). Using domain knowledge for automated modeling of dynamic systems
with equation discovery. Ph.D. thesis, Faculty of Electrical Engineering and Computer
Science, University of Ljubljana, Slovenia.
Todorovski, L., Srinivasan, A., Whiteley, J., & Gavaghan, D. (2000). Discovering the structure of partial differential equations from example behavior. In Proceedings of the
876

fiQualitative System Identification

Seventeenth International Conference on Machine Learning, pp. 991998, San Francisco.
Todorovski, L., & Dzeroski, S. (1997). Declarative bias in equation discovery. In Proc. 14th
International Conference on Machine Learning, pp. 376384. Morgan Kaufmann.
Valdes-Perez, R. E. (1994). Heuristics for systematic elucidation of reaction pathways.. J.
Chem. Informat. Comput. Sci., 34, 976983.
Voit, E. O., & Radivoyevitch, T. (2000). Biochemical systems analysis of genome-wide
expression data. Bioinformatics, 16 (11), 10231037.
Warren, P., Coghill, G. M., & Johnstone, A. (2004). Top down and botton up development
of a fuzzy rule-based diagnostic system. In Proc. of the Fourth International Workshop
on Hybrid Methods for Adaptive Systems, Aachen, Germany.

877

fiJournal of Artificial Intelligence Research 32 (2008) 385-417

Submitted 12/07; published 05/08

On the Qualitative Comparison of Decisions Having Positive
and Negative Features
Didier Dubois
Helene Fargier

dubois@irit.fr
fargier@irit.fr

Universite de Toulouse
IRIT- CNRS, 118 route de Narbonne
31062 Toulouse Cedex, France

Jean-Francois Bonnefon

bonnefon@univ-tlse2.fr

Universite de Toulouse
CLLE (CNRS, UTM, EPHE)
Maison de la Recherche, 5 al. Machado
31058 Toulouse Cedex 9, France

Abstract
Making a decision is often a matter of listing and comparing positive and negative
arguments. In such cases, the evaluation scale for decisions should be considered bipolar,
that is, negative and positive values should be explicitly distinguished. That is what is done,
for example, in Cumulative Prospect Theory. However, contrary to the latter framework
that presupposes genuine numerical assessments, human agents often decide on the basis of
an ordinal ranking of the pros and the cons, and by focusing on the most salient arguments.
In other terms, the decision process is qualitative as well as bipolar. In this article, based
on a bipolar extension of possibility theory, we define and axiomatically characterize several
decision rules tailored for the joint handling of positive and negative arguments in an ordinal
setting. The simplest rules can be viewed as extensions of the maximin and maximax
criteria to the bipolar case, and consequently suffer from poor decisive power. More decisive
rules that refine the former are also proposed. These refinements agree both with principles
of efficiency and with the spirit of order-of-magnitude reasoning, that prevails in qualitative
decision theory. The most refined decision rule uses leximin rankings of the pros and the
cons, and the ideas of counting arguments of equal strength and cancelling pros by cons. It
is shown to come down to a special case of Cumulative Prospect Theory, and to subsume
the Take the Best heuristic studied by cognitive psychologists.

1. Introduction
Not only personal experience, but also psychological experiments suggest that making a
decision is often a matter of listing and comparing the positive and negative features of
the alternatives (Cacioppo & Berntson, 1994; Osgood, Suci, & Tannenbaum, 1957; Slovic,
Finucane, Peters, & MacGregor, 2002). Individuals evaluate alternatives or objects by
considering their positive and negative aspects in parallel  for instance, when choosing a
movie, the presence of a good actress is a positive argument; a noisy theater or bad critiques
are negative arguments. Under this bipolar perspective, comparing two decisions amounts
to comparing two pairs of sets, that is the sets of pros and cons attached to one decision with
the sets of pros and cons attached to the other. Cumulative Prospect Theory (Tversky &
Kahneman, 1992) is an explicit attempt at accounting for positive and negative arguments
c
!2008
AI Access Foundation. All rights reserved.

fiDubois, Fargier, Bonnefon

in the numerical setting. It proposes to compute a net predisposition of a decision, as
the difference between two set functions (capacities) taking values on the positive real line,
the first one measuring the importance of the group of positive features, the second one
the importance of the group of negative features. More general numerical models, namely
bi-capacities (Grabisch & Labreuche, 2005) and bipolar capacities (Greco, Matarazzo, &
Slowinski, 2002) encompass more sophisticated situations where criteria are not independent
from each other. These numerical approaches to bipolar decision contrast with standard
decision theory, which does not account for the bipolarity phenomenon. Indeed, utility
functions are defined up to an increasing affine transformation, which does not preserve the
value 0.
However, cognitive psychologists have claimed that while arguments featured in a decision process can be of different strengths, decision-makers are likely to consider these
degrees of strength at the ordinal level rather than at the cardinal level (Gigerenzer, Todd,
& the ABC group, 1999). Individuals appear to consider very few arguments (i.e., the most
salient ones) when making their choice, rather than to attempt an exact numerical computation of the merits of each decision (Brandstatter, Gigerenzer, & Hertwig, 2006). In sum,
cognitive psychologists have claimed that human decision processes are likely to be largely
qualitative as well as bipolar.
The last 10 years have also witnessed the emergence of qualitative decision theory in Artificial Intelligence (Doyle & Thomason, 1999). For instance, qualitative criteria like Walds
rule (Wald, 1950/1971) have been axiomatized along the line of decision theory (Brafman
& Tennenholtz, 2000) as well as variants or extensions thereof; see the survey of Dubois
and Fargier (2003). So-called conditional preference networks (Boutilier, Brafman, Domshlak, Hoos, & Poole, 2004) have been introduced for an easier representation of preference
relations on multidimensional sets of alternatives, using local preference statements interpreted in the ceteris paribus style. This recent emergence of qualitative decision methods in
Artificial Intelligence is partly motivated by the traditional stress on qualitative representations in reasoning methods. It is also due to the fact that numerical data are not available
in many AI applications, for example because there is no point in requiring very precise
evaluations from the user (e.g., for recommender systems).
These models use preference relations which express statements like decision a is better than decision b for an agent. However, these preference relations cannot deal with
bipolaritymore precisely, they cannot express the simple notion that people know what
is good and what is bad for them, and that these judgments are orthogonal to judgments
about what is best in a given situation. Sometimes, the best available choice can be
detrimental anywayand yet, on other occasions, even the worst option is still somewhat
desirable. (Note that the best or the worst option can be the statu quo option, i.e., the
option of not making any active choice.) To fully capture these situations, it is necessary
to have an absolute landmark or reference point in the model, which expresses neutrality
and separates the positive values from the negative values. While emphasizing qualitative
approaches to decision and choice, the Artificial Intelligence literature in this area has somewhat neglected the fact that preference orderings are not enough to express the fact that
an option can be good or bad per se.
Other qualitative formalisms capable of representing preference exploit value scales that
include some reference points, e.g., fuzzy constraint satisfaction problems (Dubois, Fargier,
386

fiQualitative Bipolarity in Decision

& Prade, 1996) and possibilistic logic (Benferhat, Dubois, & Prade, 2001). There, the merit
of a decision is evaluated on different criteria by means of some kind of utility functions
mapping into a bounded ordinal scale whose bottom value expresses an unacceptable degree
of violation, and whose top value expresses the absence of a violation. Decisions are then
ranked according to the merit of their worst evaluation, following a pessimistic attitude.
But this kind of approach is not bipolar, as it only handles negative arguments: the absolute
landmark expresses unacceptability, not neutrality. Neutrality is only present by omission,
when no constraint is violated, and no decision exists that could be better than neutral.
Another kind of bipolarity is accounted for by Benferhat, Dubois, Kaci, and Prade
(2006), who distinguish between prioritized constraints on the one hand, and goals or desires
on the other hand. Constraints (expressed as logical formulas) are given a prominent role:
they guide the initial selection of the most tolerated decisions. Positive preferences (goals
and desires) are then taken in consideration to discriminate among this set of tolerated
decisions. As a consequence, positive evaluations (no matter how positive) can never trump
negative evaluations (no matter how negative).
Finally, the topic of argumentation in reasoning has gained considerable interest in
artificial intelligence in the last ten years or so. This is a natural way of coping with
inconsistency in knowledge bases (Besnard & Hunter, 2008). Argumentation is naturally
of a bipolar nature, since the construction of arguments consists in collecting reasons for
deriving a proposition and reasons for deriving its negation, before proceeding to arbitration
between arguments of various weights (Cayrol & Lagasquie-Schiex, 2005). However it is not
clear that the strength of an argument in inconsistency-tolerant reasoning should be defined
as a number. It sounds more natural to adopt a qualitative approach to the bipolar nature
of argument-based reasoning. Moreover AI-based decision procedures also naturally rely
on argumentation, so as to facilitate the process of explicating the merits of a decision
(Amgoud & Prade, 2004, 2006).
In the present paper,1 we aim at proposing a bipolar and qualitative setting, equipped
with a family of decision rules, in which the decision is based on the comparison of positive
and negative arguments whose strength can only be assessed in an ordinal fashion. We
insist on the assumption that positive and negative evaluations share a common scale; then
evaluations having one polarity can trump evaluations having the other polarity, e.g., a
strong positive (resp. negative) argument can win against a weaker negative (resp. positive)
argument. Indeed, this is precisely the idea behind the intuitive procedure of weighing
the pros against the cons, i.e., finding out the heavier side.This cannot be done without a
common importance scale. We also adopt a systematic approach, in which we formalize and
axiomatically characterize a set of procedures that are simultaneously ordinal and bipolar.
The paper is structured as follows. Section 2 introduces our framework for qualitative
bipolar choice. Then, Section 3 presents two basic qualitative bipolar decision rules. In
Section 4, we show how the basic properties of bipolar reasoning can be expressed axiomatically, and, taking one step further, which axioms can capture the principles of qualitative
bipolar decision-making. Section 5 studies decision rules that are more decisive than the
basic rules of Section 3, without giving up their qualitative nature. Finally, Sections 6
1. This paper is an extended version of the work of Dubois and Fargier (2006). It analyzes additional
decision rules, and provides a full axiomatization of the cardinality based rule.

387

fiDubois, Fargier, Bonnefon

and 7 relate some of our rules to a range of other approaches, and identify avenues of future
research. Proofs of properties and representation theorems are provided in the appendix.

2. A Framework for Qualitative Bipolar Decisions
A formal framework for qualitative bipolar multicriteria decision should consist of a finite
set D of potential decisions a, b, c, . . . ; a set X of criteria or arguments, viewed as attributes
ranging on a bipolar scale, say V ; and a totally ordered scale L expressing the relative
importance of criteria or groups of criteria. In this article, we use the simplest possible
bipolar scale V = {, 0, +}, whose elements reflect negativity, neutrality, and positivity,
respectively. With this scale, any argument in X is either completely against, totally
irrelevant, or totally in favor of each decision in D. But for the focus on bipolarity, this
is a simpler approach than many multi-criteria decision-making frameworks where each
criterion x  X is rated on a numerical scale, like multi-attribute utility theory. However,
qualitative evaluations are often closer to human capabilities than numerical ones. On this
basis it is useful to see how far we can go with a very rough modelling of preference in the
bipolar situation. Indeed, if a problem can be solved in a rigorous way without resorting to
numerical evaluations, more sophisticated techniques are not needed.
Let A = {x, x(a) #= 0} be the set of relevant arguments for decision a. It only contains
arguments that matter about a, either because they are good things or because they are
bad things. Now let A = {x, x(a) = } be the set of arguments against decision a, and
A+ = {x, x(a) = +} the set of arguments in favor of a. Considering the sets A and A+
amounts to enumerating the pros and the cons of a. Thus, comparing decisions a and b
amounts to comparing the pairs of disjoint sets (A , A+ ) and (B  , B + ). Obviously, if
A  B  , B +  A+ , then a should clearly be preferred to b in the wide sense. This is the
basic property bivariate monotony that any bipolar decision rule should obey.
For the sake of simplicity, we assume in the following that X is divided into two subsets.
+
X is the set of positive arguments taking their value in {0, +}; and X  is the set of
negative arguments taking their value in {, 0}. In this simplified model, it is no longer
possible to have A  B + #=  nor A+  B  #= . This, however, is done without loss of
generality and will not affect the validity of our results in the ordinal setting. Indeed, any
x whose evaluation may range in the full domain {, 0, +} can be duplicated, leading to
an attribute x+ in X + and an attribute x in X  . Furthermore, this transformation can
generalize our framework to arguments that have both a positive and a negative side (e.g.,
eating chocolate).
The scale L measuring the importance of the arguments has top 1L (full importance)
and bottom 0L (no importance). Within a qualitative approach, L can be finite. As it
is common with set functions, we make the hypothesis that the importance of a group
of arguments only depends on the importance of the individual arguments in the group.
With this assumption of independence, levels of importance can be directly attached to
the elements of X by a function  : X ' L, from which the importance of any group of
arguments can also be derived. (x) = 0L means that the decision-maker is indifferent to
argument x; (x) = 1L means that the argument possesses the highest level of attraction or
repulsion (according to whether it applies to a positive or negative argument).  is supposed
to be non trivial, that is, at least one x has a positive importance level.
388

fiQualitative Bipolarity in Decision

Example 1 (Lucs Holidays). Luc will provide us with one of our running examples. He is
considering two holiday destinations, and has listed the pros and cons of each. Option [a]
has scenic landscapes (a strong pro), but it is very expensive, and the local airline has a
terrible reputation (two strong cons). Option [b] is in a non-democratic region, which Luc
considers a strong con. On the other hand, option [b] has a tennis court, a disco, and a
swimming pool. These are three pros, but not very decisive. They do matter, but not as
much as the other arguments. Note that Luc can only give a rough evaluation of how strong
a pro or a con is. He can only say that the gorgeous landscapes, the indecent price, the
terrible reputation of the airline company, and the non-democratic governance are four arguments of comparable importance; and that the swimming pool, tennis and disco are three
arguments of comparable importance, but not as important as the previous ones. Formally,
let:
X + = {landscape ++ , tennis + , pool + , disco + }
X  = {price  , airline  , governance  }

be the subset of pros,
be the subset of cons.

Strong arguments are landscape ++ , price  , airline  , and governance  . The arguments
tennis + , pool + , and disco + are weaker. Thus, letting  >  > 0L , we have:
(lanscape ++ ) = (price  ) = (airline  ) = (governance  ) = 
(tennis + )
= (pool + )
= (disco + )
= .
Finally options [a] and [b] are described by the following sets of arguments:
Options [a] : A+ = {landscape ++ }
A = {airline  , price  }
+
+
+
+
Options [b] : B = {tennis , pool , disco } B  = {governance  }.
In sum, each attribute x  X is Boolean (presence vs. absence), but has a polarity (its
presence is either good or bad, its absence is always neutral), and an importance (x) 
L. Now, since we are interested in qualitative decision rules, our approach relies on two
modelling assumptions:
Qualitative Scale: In L, there is a big step between one level of merit and the next one.
Arguments are ranked in terms of the order of magnitude of their figure of importance
by means of the mapping .
Focalization: The order of magnitude of the importance of a group A of arguments with
a prescribed polarity is the one of the most important arguments in the group. This
assumption suits the use of a qualitative scale, as it means that weak arguments are
negligible compared to stronger ones.
Technically, these assumptions suggest the use of the following measure of importance
of a set of arguments
OM(A) = max (x).
xA

In other terms, we simply use qualitative possibility measures (Lewis, 1973; Dubois,
1986) interpreted in term of order of magnitude of importance.
The next sections examine several decision rules relying on the use of OM(A), that can
be defined for balancing pros and cons. We will see that in the Luc example, some will
389

fiDubois, Fargier, Bonnefon

prefer option [a], some will prefer option [b], some will regard the two options as equally
attractive, and some will find it impossible to make a decision. We begin in Section 3
with two basic rules that only take into account the most important arguments. The
corresponding ordering can be complete or partial, and is usually weakly discriminant.
This is due to the immediate use of the order-of-magnitude evaluations, that leads to very
rough decision rules. Refinements of these basic rules will be proposed in Section 5. They
apply elementary principles of simplification (discarding arguments that are relevant to both
decisions, sometimes cancelling opposite arguments of the same strength) before making a
choice. Most of these refinements thus obey a form of preferential independence.

3. Elementary Qualitative Bipolar Decision Rules
The two elementary decision rules in this Section differ by one basic feature: the first one
treats positive and negative arguments separately; the second one allows for a comparison
of the relative strengths of positive and negative arguments, one side possibly overriding
the other.
Each decision rule defines a preference relation. Since the relations presented here are
not necessarily complete nor transitive, let us recall some definitions, prior to presenting
these two decision rules.
Definition 1. For any relation ), one can define:
 Its symmetric part:
A  B  A ) B and B ) A
 Its asymmetric part:
A , B  A ) B and not(B ) A)
 An incomparability relation: A ! B  not(A ) B) and not(B ) A)

) is said to be quasi-transitive when , is transitive. The transitivity of ) obviously
implies its quasi-transitivity, whether or not it is complete. The converse implication generally does not hold. When the relation is complete, ! is empty. ) is said to be a weak
order if and only if it is complete (and thus reflexive) and transitive.
In the following, we also use the notion of refinement of a relation:
Definition 2. )$ refines ) if and only if A, B : A , B  A ,$ B

The refined relation )$ thus follows the strict preference of ) when any, but can also
make a difference between decisions in case ) cannotthat is, it may happen that A ,$ B
while A  B or A ! B.

3.1 A Bipolar Qualitative Pareto Dominance Rule
The order of magnitude of a bipolar set A is no longer a unique value in L like in the unipolar
case, but a pair (OM(A+ ), OM(A )). Pairs and more generally vectors of evaluations can
be easily compared to others using the classical principle of Pareto comparison. This yields
the following rule, which does not assume commensurateness between the evaluations of
positive and negative arguments:
Definition 3. A )Pareto B  OM(A+ )  OM(B + ) and OM(A )  OM(B  ).

What would ,Pareto conclude on Lucs example? Luc has a strong argument for option
[a], but only weak arguments for option [b] : OM(A+ ) > OM(B + ). In parallel, Luc has
390

fiQualitative Bipolarity in Decision

strong arguments both against option [a] and against option [b]: OM(A ) = OM(B  ). As
a consequence, A ,Pareto B, and Luc will choose option [a].
Let us lay bare in more details the cases where A Pareto B, A ,Pareto B, A !Pareto B.
 A and B are indifferent if and only if their salient positive aspects as well as their
salient negative aspects share the same order of magnitude, i.e., OM(A+ ) = OM(B + )
and OM(A ) = OM(B  );
 B is negligible compared to A (A ,Pareto B) in two cases: either OM(A+ )  OM(B + )
and OM(A ) < OM(B  ), or OM(A+ ) > OM(B + ) and OM(A )  OM(B  ). The
case B ,Pareto A can be described symmetrically.
 In other cases, there is a conflict and A is not comparable with B (A !Pareto B).
)Pareto is obviously reflexive and transitive. It collapses to Walds pessimistic ordering
(Wald, 1950/1971) when X = X  , and to its optimistic max-based counterpart when
X = X + . Note that )Pareto is partial  and maybe too partial. For example, when a
decision has both pros and cons, )Pareto concludes that it is incomparable to a decision
that has not any pro nor con. This can be quite counter-intuitive, as shown in the following
example.
Example 2 (Lucy and the Riviera estate). Being short of money, Lucy was planning to
spend the summer at home. But she is now offered to spend part of the summer at her
brothers paradisiacal estate on the Riviera. The only inconvenient of this arrangement is
that Lucy finds her sister-in-law mildly annoying.
Formally, let: X = {estate ++ , inlaw  }, with (estate ++ ) > (inlaw  ).
The two options, going to the riviera or staying home are described as follows:
Option [a]: A+ = {estate ++ } and A = {inlaw  }.
Option [b]: B + = {}
B  = {}
The common intuition about the Lucy case is that if really her sister-in-law is only
mildly annoying, and if the estate is so fantastic, Lucy is likely to prefer to go there rather
than stay at home. However, )Pareto cannot predict this preference because it finds the two
options incomparable: OM(A+ ) > OM(B + ), but OM(A ) > OM(B  ).
Another drawback of )Pareto becomes clear when the two decisions have the same order
of magnitude on one of the two dimensions:
Example 3 (Luka and the gyms). Luka is considering buying membership in one of two
gyms. Option [a] is very expensive, which is a strong con. Option [b] is also very expensive,
but it comes with the small bonus of having a squash court (this is a small bonus because
Luka is not sure yet he will want to use that court). On the other hand, option [b] also
has a drawback of medium importance, that is, it is inconveniently located. Location is not
as important as price in Lukas mind, but it is still more important than the presence of a
squash court. Formally:
with
X = {squash + , location  , price  }
+


).
(squash ) < (location ) < (price
391

fiDubois, Fargier, Bonnefon

The options are described as follows:
Option [a]: A+ = {squash + } A = {location  , price  },
Option [b]: B + = 
B  = {price  }.
It follows from Definition 3 that Luka will prefer option [a], because OM(A+ ) > OM(B + )
whilst OM(A ) = OM(B  ). This is intuitively unsatisfying, however, for we would expect
Luka to examine more carefully the fact that option [a] is inconveniently located, which
is a moderately strong con, rather than to decide on the basis of a very weak argument,
that is, the squash court. In other terms, )Pareto does not completely obey the principle of
Focalization discussed in the introduction: An argument of a lower level (the squash court)
can determine a choice even though an argument of a higher level (the location) would have
pointed at the opposite direction.
This problem with )Pareto is partly rooted in the fact that it does not capture the
assumption that positive and negative evaluations share a common scale. The fact that an
argument may be stronger than an argument of the opposite polarity is never taken into
account. We will now propose a more realistic rule that captures this assumption.
3.2 The Bipolar Possibility Relation
In this section, we propose a decision rule for comparing A and B that focuses on arguments
of maximal strength in A  B, i.e., those at level  = maxyAB (y) = OM(A  B). The
principle underlying this rule is simple: any argument against A (resp. against B) is an
argument pro B (resp. pro A), and conversely. The most supported decision is then
preferred.
Definition 4. A )BiPoss B  OM(A+  B  )  OM(B +  A ).
This rule decides that A is at least as good as B iff, at the highest level of importance,
there are arguments in favor of A or arguments attacking B. Clearly A ,BiPoss B iff, at
the highest level, there is at least a positive element for A or an element against B, but no
element against A and no element pro B. Obviously, )BiPoss collapses to Walds pessimistic
ordering if X = X  , and to its optimistic counterpart when X = X + . In some sense, this
comparison yields the most straightforward way of generalizing possibility orderings to the
bipolar case.
Proposition 1. )BiPoss is complete and quasi-transitive.
In other terms, the strict part of the relation )BiPoss is transitive, but the associated
indifference relation is generally not: A BiPoss B and B BiPoss C do not imply A BiPoss
C. For instance, let us denote a+ = OM(A+ ), a = OM(A ), b+ = OM(B + ), b =
OM(B  ), c+ = OM(C + ), and c = OM(C  ). Assume max(a+ , b ) = max(a , b+ ) and
max(b+ , c ) = max(b , c+ ). Assume b+ = b = 1L . Then the two equalities hold regardless
of the values a+ , a , c+ , c . So the values max(a+ , c ) and max(a , c+ ) can be anything.
Similarly A ,BiPoss B and B BiPoss C do not imply A ,BiPoss C  counter examples can
be built setting c+ = c = 1L .
In the case of Luc (Example 1), A = {landscape ++ , airline  , price  }, and B =
{governance  , tennis + , swimming + , disco + }. From the perspective of )BiPoss , both options are equivalently bad, since OM(A+  B  ) = OM(B +  A ). Likewise, in the case of
392

fiQualitative Bipolarity in Decision

Luka (Example 3), )BiPoss will regard the two gyms as equivalently bad because of their
high price. Now, in the case of Lucy (Example 2), remember that )Pareto regarded options
[a] and [b] as incomparable, where A = {estate ++ , inlaws  } and B is the empty set. In
contrast, )BiPoss will consider, in line with common intuition, that Lucy will prefer to go
to the Riviera estate, since OM(A+  B  ) > OM(B +  A ).
)BiPoss is very different and arguably less dubious than )Pareto . But, as shown by
Luc and Lukass examples, it is a very rough rule that may be not decisive enough. This
weakness of )BiPoss is rooted in the usual drowning effect of possibility theory: when an
argument of high importance is attached to both decisions (e.g., the ludicrous price for the
two gyms), it will trump all arguments of lesser importance (e.g., the squash court, but
also the location).2 Variants of )BiPoss will be presented in Section 5 that overcome this
difficulty. Nevertheless, the rule )BiPoss alone has the merit of capturing the essence of
ordinal decision-making, as shown by the axiomatic study presented in the next section.

4. Axioms for Ordinal Comparison on a Bipolar Scale
In the previous sections, we have proposed a framework and some decision rules that intend
to capture the essence of qualitative bipolar decision-making. In the present section, we
adopt the opposite strategy, that is, we formalize natural properties that such a qualitative bipolar preference relation should obeyand we show that our framework is not only
sound (it obeys the aforementioned properties) but also complete. Our main result is a
representation theorem stating that any preference relation that satisfies these properties
is equivalent to )BiPoss .
Let ) be an abstract preference relation on 2X , A ) B meaning that decision A is
at least as good as decision B. As ) compares sets of decisions, we call it a set-relation.
First of all, we introduce general properties (e.g., reflexivity or monotony) ) should sensibly
obey to be a well-behaved bipolar set-relation, be it qualitative or not. Then, we introduce
axioms that characterize qualitative bipolar set-relations.
4.1 Axioms for Monotonic Bipolar Set-Relations
First of all, as for any preference relation, we shall assume minimal working conditions for
a sensible framework, such as reflexivity (R) and quasi-transitivity (QT). 3
The basic notion of bipolar reasoning over sets of arguments is the separation of X into
good and bad arguments. The first axiom thus states that any argument is either positive
or negative in the wide sense, i.e., either not worse or nor better worse than nothing:
Clarity of Arguments (CA) x  X, {x} )  or  ) {x}.
2. The drowning effect is also at work in !Pareto , within the comparison of A+ and B + , or within the
comparison of A and B  . Ceteris paribus, a destination with gorgeous landscapes plus a swimming
pool is not preferred to a destination with gorgeous landscapes and no pool.
3. Although weak, these assumptions are relaxed in some relational approaches to multicriteria evaluation,
where the aggregation process produces cycles in the preference relation. Nevertheless, in such methods,
even when the resulting relation is not transitive, the next step is to build a transitive approximation to
it.

393

fiDubois, Fargier, Bonnefon

One can then partition X, differentiating positive, negative and null arguments:
X + = {x, {x} , }
X  = {x,  , {x}}
X 0 = {x,   {x}}

Now, arguments to which the decision-maker is indifferent should obviously not affect
his or her preference. This is the meaning of the next axiom, which allows to forget about
X 0 without loss of generality:
Status Quo Consistency (SQC)
If {x}  then A, B : A ) B  A  {x} ) B  A ) B  {x} .
Let us now discuss the property of monotony. Monotony in the sense of inclusion
(A  B = B ) A) can obviously not be obeyed as such in a bipolar framework. Indeed,
if B is a set of negative arguments, it generally holds that A , A  B. We rather need axioms of monotony specific to positive and negative argumentsbasically, the one of bipolar
capacities (Greco et al., 2002), expressed in a comparative way.
Positive
Negative

Monotony
Monotony

C, C $  X + , A, B :
C, C $  X  , A, B :

A)B
A)B




C A
C \A

)
)

B \ C $.
B  C $.

We will see that the bivariate monotony property is captured by this pair of axioms.
Now, another assumption is that only the positive side and the negative side of A and
B are to be taken into account when comparing them: if A is at least as good as B on both
the positive and the negative sides, then A is at least as good as B. This is expressed by
the axiom of weak unanimity.
Weak Unanimity A, B, A+ ) B + and A ) B   A ) B.
The set-relations presented in the previous Section obviously satisfy weak unanimity.
Finally, we add a classical axiom of non triviality:
Non-Triviality: X + , X  .
It leads to the following generalization of comparative capacities:
Definition 5. A relation on a power set 2X is a monotonic bipolar set-relation if and only
if it is reflexive, quasi-transitive and satisfies the properties CA, SQC, Non-Triviality, Weak
unanimity, Positive and Negative Monotony.
As expected, a monotonic bipolar set-relation safisfies the bivariate monotony property:
using the above conventions for positive and negative arguments in subsets A and B, if
B +  A+ (resp. A  B  ), then using Clarity of Arguments and Positive (resp. Negative)
Monotony, it follows that A+ ) B + (resp. A ) B  ), hence A ) B due to weak unanimity.
Proposition 2. )BiPoss is a monotonic bipolar set-relation.
394

fiQualitative Bipolarity in Decision

In the present work, we are interested in set-relations that are entirely determined by
the strength and the polarity of the individual arguments in X. We denote by X = X  {0}
the set of individual arguments in X, adding an element 0 so as to keep track of the polarity
of the arguments. The basic information about arguments is captured by the restriction of
) to X. Formally it is defined by:
x )X y  {x} ){ y}
x )X 0  {x} )
0 )X x   ) {x}

From now on, we will call )X the ground relation of ).
In agreement with the existence of a totally ordered scale for weighting arguments, the
ground relation )X is supposed to be a weak order. As a consequence, a minimal condition
of coherence for )X is that a preference cannot be reversed when an argument in the preferred set (resp., the least preferred set) is replaced by an even better one (resp., a worse
one). This can be viewed as a condition of monotony with respect to )X :
Monotony w.r.t. )X or X-monotony
A, B, x, x$ such that A  {x, x$ } =  and x$ )X x:
A  {x} , B
A  {x}  B
B , A  {x$ }
B  A  {x$ }






A  {x$ } , B
A  {x$ } ) B
B , A  {x}
B ) A  {x}

This very natural axiom is richer than it seems. For example, it implies a property of
substitutability of equally strong arguments of the same polaritya kind of property that
is often called anonymity in social choice and decision theory. A kindred property to
anonymity should also be required, for positive arguments to block negative arguments of
the same strength. This blocking effect should not depend on the arguments themselves,
but only on their position on the scale. Hence the axioms of positive and negative cancellation:
Positive Cancellation(POSC)
x, z  X + , y  X  , {x, y}  and {z, y}    x X z.
Negative Cancellation (NEGC)
x, z  X  , y  X + , {x, y}  and {z, y}    x X z.
It makes sense to summarize the above requirements into a single axiom, that we call
Simple Grounding:
Simple Grounding
A bipolar set-relation ) is said to be simply grounded if and only if )X is a weak order, )
is monotonic with respect to )X and satisfies positive and negative cancellation.
395

fiDubois, Fargier, Bonnefon

Proposition 3. The set-relation )BiPoss is simply grounded.4
4.2 Axiomatizing Qualitative Bipolar Set-Relations
Our definition of a monotonic bipolar set-relation (Definition 5) is very general and encompasses numerous models, be they qualitative (e.g., the two rules in section 3) or not
(e.g., cumulative prospect theory in its full generality). As we are interested in preference
rules that derive from the principles of ordinal reasoning only, we now focus on axioms that
account for ordinality.
The ordinal comparison of sets has been extensively used, especially in Artificial Intelligence. The basic principle in qualitative reasoning is Negligibility, which assumes that each
level of importance can be interpreted as an order of magnitude, much higher than the next
lower level.
Negligibility (NEG)
A, B, C  X + : A , B and A , C  A , B  C.
Axiom NEG has already been featured around in AI, directly under this form or through
more demanding versions. Let us mention the union property of nonmonotonic reasoning,
or Halperns (1997) Qualitativeness axioms (see Dubois & Fargier, 2004, for a discussion).
Lehmann (1996) introduced an axiom of negligibility inside Savage decision theory axiomatics.
Qualitative reasoning generally also comes along with a notion of closeness preservation
(which does away with the notion of counting):
Closeness Preservation (CLO)
A, B, C  X + : A  B and A  C  A  B  C
B, C  X + :
B )C B BC

These axioms were proposed and justified in ordinal reasoning when only one scale needs
to be considered (namely, the positive one). But they are not sufficient when a negative
scale also needs to be taken into account. We need for example to express that if there is
a very bad consequence B, so bad that A , B and C , B, then whatever the negative
arguments in A and C, B is still worse than A  C:
A, B, C : A , B and C , B  A  C , B.

This property is meaningful for negative sets of arguments, and trivial on X + ; hence it
can be introduced soundly in the framework.
Other cases where sets with both negative and positive elements are compared should
also be encompassed. For example, if A is so good that it can cope with globally negative
B and also win the comparison with C, then A  B is still better than C:
A, B, C : A  B ,  and A , C  A  B , C.

4. Technically, !Pareto is also a monotonic bipolar simply grounded set-relation. But this result is quite
irrelevant, since this rule never compares the strengths of positive and negative arguments. In other
terms, it never happens that arguments of opposite polarities cancel each other, and the condition parts
of POSC and NEGC are thus never fulfilled.

396

fiQualitative Bipolarity in Decision

And similarly, if a globally negative A (A  ) is so bad that it is outperformed by C
(C , A) and cannot be enhanced by B ( , A  B), then C , A  B, i.e.:
A, B, C :  , A  B and C , A  C , A  B.
All these properties can be expressed in the following axiom of global negligibility:
Global Negligibility (GNEG)
A, B, C, D : A , B and C , D  A  C , B  D
This is a classic property for purely positive qualitative scalesin that case, it is a
consequence of NEG and positive monotony. That is why it is usually not explicitly required
in positive frameworks. But when a framework with a positive and a negative scale is
needed, the NEG condition is no longer sufficient for getting GNEG. So, in order to keep
the property that is a foundation of pure order-of-magnitude reasoning, bipolar qualitative
frameworks must explicitly require GNEG.
A similar argument applies to axiom CLO. We must expicitly require a property that is
more general than that which is usual for unipolar qualitative scales:
Global Closeness Preservation (GCLO)
A, B, C, D : A ) B and C ) D  A  C ) B  D
Proposition 4. The set-relation )BiPoss satisfies GNEG and GCLO.
Propositions 2, 3 and 4 show that the bipolar possibility relation is a simply grounded
monotonic bipolar set-relation satisfying GNEG and GCLO. Applying the principles of
qualitative bipolar reasoning described by the previous axioms can also lead to many different but more or less intuitive qualititative rules, for instance the Pareto rule (see Dubois
and Fargier (2006) for a full characterization of this rule). But if we are looking for a simple
complete decision rule, these axioms provide a full characterization of the Biposs preference
relation. Note that the strict part of this rule is governed by axiom GNEG. The indifference
part includes pure case of indifference but also cases when one would expect incomparability
between decisions rather than indifference proper. This is the most debatable part of the
Biposs rule, that will be refined in the sequel.
Theorem 1. The following propositions are equivalent:
1. ) is a simply grounded complete monotonic bipolar set-relation on 2X that satisfies
GNEG and GCLO.
2. There exists a mapping  : X ' [0L , 1L ] such that )  )BiPoss .
The detailed proof of Theorem 1 is provided in Appendix A. In short, we show that,
when ) is complete and simply grounded, a ranking  can be built that ranks the arguments
with respect to their strength. Within X + and within X  ,  simply obeys the information
captured in )X :
 x, y  X +  X 0 , x  y  x )X y;
397

fiDubois, Fargier, Bonnefon

 x, y  X   X 0 , x  y  x 5X y.

The relative strength of elements of different signs is deduced from blocking effects. Indeed,
when {x, y} is preferred to the empty set, the positive argument must be stronger than
the negative one (and symmetrically). The two arguments are of equivalent strength when
none of them can win: {x, y}  . Formally:
 x  X + , y  X  , x  y  {x, y} ) ;
 x  X + , y  X  , y  x   ) {x, y}.

The condition of simple grounding then ensures that  is a weak order. It can thus be
encoded by a mapping  : X ' [0L , 1L ] such that (x) = 0L  x  X 0 . Note that this
construction is valid for any simple grounded complete bipolar relation, and not only for
qualitative ones. The sequel of the proof then uses the axioms for closeness and negligibility
to show that ))BiPoss .
4.3 The Principles of Efficiency and Preferential Independence
In summary, the previous Section has shown that )BiPoss is a natural model of preferences
based on bipolar orders of magnitude. In particular, any rule in accordance with GNEG
has to follow the strict preference prescribed by ,BiPoss .
Nonetheless, we have seen in Section 3.2 that )BiPoss suffers from a drowning effect, as
usual in standard possibility theory. For instance, when B is included in A and even if all
their elements are positive, then A is not necessarily strictly preferred to B. This problem is
rooted into the fact that CLO concludes to indifference even in some cases where we would
like to appeal to the so-called principle of efficiency to make the decision. Just like the
monotony principle, this axiom is well known on positive sets. Its proper extension to the
bipolar framework obviously has one positive and one negative side:
Positive efficiency B  A and A \ B ,   A , B
Negative efficiency B  A and A \ B    A  B
The set-relations )BiPoss and )Pareto also fail to obey the classical condition of preferential independence, also called the principle of additivity. This condition simply states that
arguments present in both A and B should not influence the decision:
Preferential Independence: A, B, C, (A  B)  C =  : A ) B  A  C ) B  C
This axiom is well known in uncertain reasoning, as one of the fundamental axioms of
comparative probabilities (see Fishburn, 1986, for a survey). Note that it implies the above
conditions of efficiency (provided that completeness holds).
Except in very special cases where all arguments are of different levels of importance
(when )X is a linear order), these new axioms are incompatible with axioms of ordinality
when completeness or transitivity are enforced. It is already true in the purely positive
case, i.e. when X  is empty (Fargier & Sabbadin, 2005). But this impossibility result is
not insuperable, as shown in the next Section.
398

fiQualitative Bipolarity in Decision

5. Refining the Basic Order-of-Magnitude Comparison
In order to overcome the lack of decisiveness of )BiPoss we can propose comparison principles
that refine it, that is, more decisive set-relations ) that are still compatible with )BiPoss ,
such that A ,BiPoss B  A , B. In the following, we shall not consider refining )Pareto
because of its important drawbacks.
5.1 The Implicative Bipolar Decision Rule
The implicative decision rule (Dubois & Fargier, 2005) follows the basic focalization principle
of )BiPoss : When comparing A and B, it focuses on arguments of maximal strength OM(A
B) = maxxAB (x) in A B. It adds to this principle the following very simple existential
principle: A is at least as good as B iff, at level s the existence of arguments in favor of
B is counterbalanced by the existence of arguments in favor of A, and the existence of
arguments against A is counterbalanced by the existence of arguments against B. Formally,
the implicative bipolar rule can be described as follows:
Definition 6. Let  = maxxAB (x);
A )Impl B  and

OM(B + ) = 
OM(A ) = 

= OM(A+ ) = 
= OM(B  ) = 

Let us go back to the examples of Luc, Lucy, and Luka. In the Luc case (Example 1),
where )BiPoss concluded to indifference, )Impl will rather select Option [a], because there
are important arguments against both decisions, whilst only Option [a] is supported by
an important pro (there is no important pro supporting option [b]). In the Lucy case
(Example 2), )Impl will follow the strict preference of )BiPoss and send Lucy to the Riviera.
Finally, in the Luka case (Example 3), where the highest level of argument importance
features only one con and no pro, on both sides, )Impl will opine with )BiPoss and conclude
to indifference.
Let us lay bare the cases where A )Impl B. Once again, let us denote a+ = OM(A+ ),
a = OM(A ), b+ = OM(B + ), b = OM(B  ). By definition, A )Impl B in any of the four
following situations:
1. a+ = b+ = a = b ;
2. a+ = b+  max(a , b ) > min(a , b );
3. a = b  max(a+ , b+ ) > min(a+ , b+ );
4. max(a+ , b ) > max(a , b+ ).
We thus get the following decomposition of the )Impl rule:
Proposition 5.
 A Impl B  either a+ = b+ = a = b , or a+ = b+ > max(a , b ), or yet
a = b > max(a+ , b+ ).
 A !Impl B  either a+ = a > max(b , b+ ), or b+ = b > max(a , a+ ).
399

fiDubois, Fargier, Bonnefon

 A ,Impl B  either max(a+ , b ) > max(a , b+ ), or a+ = a = b > b+ , or yet
b+ = b = a+ > a .
It is then easy to check that )Impl is a bipolar monotonic set-relation. Like )BiPoss ,
when there are positive arguments only, the set-relation )Impl collapses to the max rule. It
also obeys the principle of weak unanimity. Moreover:
Proposition 6. The set-relation )Impl is transitive.
Since max(a+ , b ) > max(a , b+ ), i.e., A ,Biposs B, is one of the conditions for A ,Impl
B, it obviously follows that:
Proposition 7. Relation )Impl is a refinement of )BiPoss .
Finally, the situation of incomparability A !Impl B arises in two cases only, when
a+ = a > max(b , b+ ), or in the symmetric case b+ = b > max(a , a+ ). In other terms,
incomparability occurs when one of the two sets displays an internal contradiction at the
highest level, while the arguments in the other set are too weak to matter. In particular,
a+ = a > 0L if and only if A ! . For instance, a dangerous travel in an exceptional, and
mysterious part of a far tropical forest displays such an internal conflict, and I do not know
whether I prefer to stay at home or not. Such a conflict appears also in Lucs first option
(which is very attractive but high priced). On the other hand, considering a non conflicting
(and non-empty set) A, either OM(A+ ) > OM(A ) and then A ,Impl  (well, the travel
is reasonably dangerous, so I prefer to go), or OM(A ) > OM(A+ ) and then  ,Impl A
(there is a war near the border and I prefer to stay at home); in these two latter cases, the
Pareto rule would have concluded to an incomparability. The range of incompleteness of
)Impl is thus very different from the one of )Pareto , which does not account for any notion
of internal conflict.
The )Impl rule is very interesting from a theoretic descriptive point of view, both because
of the way it handles the conflict, and because of the fact that it refines )BiPoss . However,
)Impl is not decisive enough to fully overcome the drowning effect: only the most salient
arguments are taken into account. For instance, a very expensive hotel without swimming
pool is undistinguishable from a very expensive hotel that does include a swimming pool.
Even if it is more decisive than )BiPoss the )Impl rule does not satisfy Preferential
Independence: like the previous rules, it collapses with Walds criterion (resp., the max
rule) on the negative (resp., positive) sub-scale X  (resp., X + ), and can thus suffer from
the drowning effect. To solve this problem, we now leave this family of set-relations and
focus on a set of refinements that do satisfy Preferential Independence, and are thus efficient
both positively and negatively.
5.2 Efficient Refinements of )BiPoss

The following so-called Discri rule adds the principle of preferential independence to the
ones proposed by )BiPoss , cancelling the elements that appear in both sets before applying
the )BiPoss rule:
Definition 7. A )Discri B  A \ B )BiPoss B \ A.
400

fiQualitative Bipolarity in Decision

Note that the simplification of options A and B (by cancellation of their common aspects) is not inconsistent with the focalization assumption (i.e., the importance of a group
of arguments is that of the most important argument in the group). Rather, focalization
applies once the options have been simplified so as to delete arguments that, because they
are common to both options, do not contribute to making a difference between them.
The Luka case (Example 3) is a typical example of the way )Discri outperforms )BiPoss .
In that case, A+ = {squash + }, A = {location  , price  }, B + is empty, and B  =
{price  }. Recall that )BiPoss concludes to indifference between the two options because
of the common strong cons. However, after cancelling away the price  argument that is
present in both options, )Discri will choose B, which no longer has any pros nor cons, over
A which is now described by a moderately strong con and a weak pro. On the Luc and
Lucy cases (Examples 1 and 2), the two options do not feature any common argument, and
)Discri will therefore share the preferences of )BiPoss (indifference in the Luc case, going to
the Riviera in the Lucy case).
)Discri is complete and quasi-transitive (its strict part, ,Discri is transitive but its symmetric part is not necessarily transitive). Unsurprisingly, when X = X + , )Discri does not
collapse with the max rule, but rather with the discrimax procedure (Brewka, 1989; Dubois
& Fargier, 2004), that is, the comparison between OM(A \ B) and OM(B \ A).
As it has already been noted, )Discri simply cancels any argument appearing in both A
and B. One could further accept the cancellation of any positive (resp. negative) argument
in A by another positive (resp. negative) argument in B, as long as these two arguments
share the same order of magnitude. This yields the following )BiLexi rule (and, later on, the
)Lexi rule), which is based on a levelwise comparison by cardinality. The arguments in A
and B are scanned top down, until a level is reached such that the numbers of positive and
negative arguments pertaining to the two alternatives are different; then, the set with the
least number of negative arguments and the greatest number of positive ones is preferred.
Note that it is perfectly legitimate for a qualitative decision rule to count arguments of the
same strength. This simply means that one argument on one side cancels one argument
of the same strength on the other side. And this is what people seem to do. What must
remain qualitative is the scale expressing the importance of the arguments on which the
decision is based; but it would be unreasonable to consider that the number of arguments
at any given importance level is not, indeed, a number, that is, a natural integer.
Let us first define the -section of a set A of arguments:
Definition 8. For any level   L:
A = {x  A, (x) = } is the -section of A.

+

A+
 = A  X (resp. A = A  X ) is its positive (resp. negative) -section.

Now, a lexicographic two-sided partial ordering (called Levelwise Bivariate Tallying by
Bonnefon et al.(in press)) can be introduced:

Definition 9.
+


A )BiLexi B  |A+
 |  |B | and |A |  |B |,
+


where  = Argmax{ : |A+
 | #= |B | or |A | #= |B |}.

It is easy to show that )BiLexi is reflexive, transitive, but not complete. Indeed, if at
the decisive level ( ) one of the set wins on the positive side and the other set wins on the
negative side, a conflict is revealed and the procedure concludes to an incomparability.
401

fiDubois, Fargier, Bonnefon

The )BiLexi rule is well-behaved with respect to the Lucy and Luka examples. Lucy will
go to the Riviera, and Luka will chose the best located gym. In the Luka case, {price  }
is preferred to {price  , location  , squash + }: Once the price argument featured in both
options is cancelled away, {location  , squash + } and its moderately strong con is judged as
worse than . In the Luc story, the difficulty of the dilemma is clearly pointed out by the
rule. Remember that option [a] involves three arguments at the highest level of importance,
landscape ++ , airline  , and price  , whilst option [b] involves only one, governance  .
Since |{governance  }| < |{airline  , price  }| while |{landscape ++ }| > ||, )BiLexi concludes to incomparability, reflecting the difficulty of the decision.5 More generally, the
)BiLexi rule concludes to incomparability if and only if there is a conflict between the positive side and the negative side at the decisive level. From a descriptive point of view, this
range of incomparability is not necessarily a shortcoming of )BiLexi .
Now, if one can assume a form of compensation between positive and negative arguments
of the same level within a given option, the following refinement of )BiLexi can be obtained,
that will take care of complex cases such as Lucs:
Definition 10.
A

)Lexi

B    L \ 0L such that

!


+

 > , |A+
 | | A | = |B |  |B |
+

+
and
|A | | A | > |B |  |B |.

Let us look one more time at Lucs dilemma. One of the strong pros of Option [a] is
cancelled by one of its strong cons, and they are discarded. What remains is one strong con
for each option: no option wins at this level. Therefore, the procedure then examines the
next lower importance level. At this level, there are three weak pros and no con for option
[b], and neither pro nor con for option [a]: Option [b] winsand this is the first time in
this article that a decision rule yields a strict preference on the Luc case. On the Lucy and
Luka examples, )Lexi breaks the ties in )BiPoss just as )BiLexi and )Discri did: Lucy will go
to the riviera, and Luka will chose the best located gym.
The three rules proposed in this Section obviously define monotonic bipolar set-relations.
Each of them refines )BiPoss and satisfies Preferential Independence. They can be ranked
from the least to the most decisive (,Lexi ), which is moreover complete and transitive.
Proposition 8. A ,BiPoss B  A ,Discri B  A ,BiLexi B  A ,Lexi B

Both decision rules ,BiLexi and ,Lexi satisfy a strong form of unanimity, namely, if
A+ ) B + and A ) B  and moreover at least one of A , B  or A+ , B + holds,
then A , B follows. However this kind of strong unanimity alone is not strong enough to
achieve good and sound discrimination among options (for instance, it is the key-feature
of )Pareto ). In fact, ,BiLexi and ,Lexi satisfy much stronger efficiency properties, since
A BiLexi B only if A and B have the same number of positive and negative arguments
at each level of importance, and A Lexi B only if A and B have the same number of
additional arguments of the same polarity at each level of importance after cancellation of
equally important opposite arguments inside each option.
5. We take this opportunity to insist on the fact that incomparability should not be confused with indifference, and signals a more complex situation. In situations of incomparability, the decision-maker is
perplex because not only does no alternative look better than the other, but choosing at random would
not be a satisfactory way out, because any choice leads to some reason to regret. In case of indifference,
both alternatives are equally attractive (or repulsive), and a random choice makes sense.

402

fiQualitative Bipolarity in Decision

Notice that the restriction of both )BiLexi and )Lexi on X + amounts to the leximax
preference relation (Deschamps & Gevers, 1978). We can thus use the classical encoding of
the leximax (unipolar) procedure by a sum in the finite case (Moulin, 1988). A capacity is
a set-function monotonic under inclusion. It is then easy to show that:
Proposition 9. There exist two capacities  + on 2X

+



and   on 2X such that:

A )Lexi B   + (A+ )!   (A )   + (B + )    (B  )
 + (A+ )    (B + )
A )BiLexi B  and
 + (B  )    (A )
For example, denoting 1 = 0L < 2 <    < l = 1L the l elements of L, we can use
the (integer-valued) capacity:
"
 + (A+ ) =
|A + |  |X|i .
i L

#
This set-function is said to be a big-stepped capacity because |X|i > j<i |X|j (see Dubois
& Fargier, 2004). We can similarly use a similar big-stepped capacity to represent the
importance of sets of negative arguments:
"
i
  (A ) =
|A
 |  |X| .
i L

This proposition means that the )Lexi and )BiLexi rankings are particular cases (using
big-stepped probabilities) of the Cumulative Prospect Theory decision rule (Tversky &
Kahneman, 1992). See Section 6 for further discussion.
In summary, )Lexi complies with the spirit of qualitative bipolar reasoning while being
efficient. In the meantime, it has the advantages of numerical measures (transitivity and
representability by a pair of functions). Finally, in order to make a full case for the attractive
)Lexi , we will show that )Lexi is the only rule that at the same time (i) refines )BiPoss ,
(ii) is a weak order, and (iii) satisfies the principle of preferential independence without
introducing any bias in the order on X.
Definition 11. A refinement )$ of a monotonic bipolar set relation ) is unbiased if it
preserves the ground relation of the latter: )$X  )X .
In order to prove our claim, let us first establish the following proposition that applies to
any transitive relation satisfying preferential independence (note that completeness is not
required). This proposition establishes a principle of anonymity, according to which two
equivalent subsets are interchangeable provided that they do not overlap the other sets of
arguments:
Proposition 10. Let ) be a transitive monotonic bipolar set-relation that satisfies preferential independence. Then for any A, B, C, D such that A  (C  D) =  and C  D:
 A  C ) B  A  D ) B;
 B ) A  C  B ) A  D.
403

fiDubois, Fargier, Bonnefon

A direct consequence of this property it that a set of arguments that cancel each other
can be added to another set without changing the preferences involving the latter (just let
D  ):
Corollary 1. Let ) be a transitive monotonic bipolar set-relation that satisfies preferential
independence. Then for any A, B, C such that A  C = , C  ,
 A ) B  A  C ) B;
 B ) A  B ) A  C.
Another noteworthy consequence of Proposition 10 is an extended principle of preferential independence:
Corollary 2. Let ) be a transitive monotonic bipolar set-relation that satisfies preferential
independence. Then for any A, B, C, D such that (A  B)  (C  D) =  and C  D:
A ) B  A  C ) B  D
Finally, Proposition 10 allows to show that6 :
Corollary 3. Let ) be a transitive monotonic bipolar set-relation that satisfies preferential
independence. Then for any A, B  X, x, y  X such that x 
/ A, y 
/ B, {x} { y}:
A ) B  A  {x} ) B  {y}
The three above results are instrumental when proving the third representation result
of the paper:
Theorem 2. Let ) be a monotonic bipolar set-relation and  be an order-of-magnitude
distribution over the elements of X. The following propositions are equivalent:
1. ) is complete, transitive, satisfies preferential independence and is an unbiased refinement of )BiPoss ;
2. )  )Lexi .
This theorem concludes our argumentation in favor of )Lexi , which is shown to be the
only rule to satisfy the natural principles of order-of-magnitude reasoning while being decisive and practical (e.g., completeness, transitivity, representability by a pair of functions).

6. Related Works
In this section, we relate our decision rules and our general approach to several traditions
of research, in Cognitive Psychology as well as in Artificial Intelligence.
An interesting connection can be made between our decision rules and the Take the
Best heuristic that has been extensively studied by psychologists since its introduction by
6. Note that, in Corollary 3, neither condition {x}  B =  nor condition {y}  A =  are required; this
is the main difference with Corollary 2; on the other hand, Corollary 3 is restricted to the addition of
singletons.

404

fiQualitative Bipolarity in Decision

Gigerenzer and Goldstein (1996). Take the Best is a so-called fast and frugal heuristic
for comparing two objects based on their values in a series of binary cues. It is fast and
frugal because it focuses on a limited subset of the available information in order to make a
decision; and it has heuristic value because it shows reasonable accuracy as compared to less
frugal comparison rules (see Gigerenzer et al., 1999; Katsikopoulos & Martignon, 2006, for
simulations and empirical tests). As a consequence, evolutionary psychologists have argued
that Take the Best is an adapted strategy for the kind of binary cue-based comparison we
are investigating.
Take the Best requires that all arguments under consideration have different orders of
magnitude, so that they can be ranked lexicographically: there does not exist x, y such
that (x) = (y). Furthermore, each argument is considered to generate its polar opposite:
if the pro x+ is attached to an option [a], then the con x such that (x ) = (x+ )
is automatically attached to all options that do not feature x+ (and reciprocally). Then
applying Take the Best amounts to scanning the arguments top-down, starting from the
most important, and to stop as soon as an argument is found that favors one option but
not the other. Interestingly, when applied to such linearly ranked arguments, the )Discri ,
)BiLexi , and )Lexi rules coincide with the Take the Best heuristic. But the new decision
rules proposed here are able to account for more choice situations than the Take the Best
heuristice.g., several criteria can share the same degree of importance. In this sense, they
are natural extensions of the Take the Best qualitative rule advocated by psychologists.
Originally a descriptive model of risky decisions, Cumulative Prospect Theory (Tversky
& Kahneman, 1992) provides another psychological account of decisions involving positive
and negative arguments. Contrary to the Take the Best approach, it is oriented towards
the quantitative evaluation of decisions. The key notions of Cumulative Prospect Theory
are that individuals assess outcomes relatively to some reference point, rather than absolutely; that they are more concerned with losses than with gains, ceteris paribus; and that
individuals overweight extreme outcomes that occur with a small probability.
More technically, Cumulative Prospect Theory assumes that the reasons supporting a
decision and the reasons against it can be measured by means of two capacities  + and
  ;  + reflects the importance of the group of positive arguments,   the importance of
the group of negative arguments. The higher  + , the more convincing the set of positive
arguments; and conversely, the higher   , the more deterring the set of negative arguments. Furthermore, Cumulative Prospect Theory assumes that it is possible to map these
evaluations to a so-called net predisposition score, expressed on a single scale:
A  X, NP(A) =  + (A+ )    (A ),
where A+ = A  X + , A = A  X  . Alternatives are then ranked according to this net
predisposition: A )NP B   + (A+ )    (A )   + (B + )    (B  ). Proposition 9
(Section 5) actually shows that )Lexi is a particular case of this rule (using big-stepped
capacities). Interestingly, the )BiPoss rule is not foreign to the comparison of net predispositions. More precisely, it can be viewed as the ordinal counterpart to net predisposition
comparison. Let us first write A )NP B as  + (A+ ) +   (B  )   + (B + ) +   (A ). Now,
it is immediate that changing + into max, and changing  + and   in this inequality into
possibility measures, yields the )BiPoss rule.
405

fiDubois, Fargier, Bonnefon

Cumulative Prospect Theory and its variants assume some kind of independence between X + and X  , but this assumption does not always hold. Bi-capacities (Grabisch &
Labreuche, 2002, 2005) were introduced to handle such non-separable bipolar preferences:
a measure  is defined on Q(X) := {(U, V )  2X , U  V = }, and increases (resp., decreases) with the addition of elements in U (resp., in V )7 . Bi-capacities originally stemmed
from bi-cooperative games (Bilbao, Fernandez, Jimenez Losada, & Lebron, 2000), where
players are divided into two groups, the pros and the cons: a player x is sometimes
a pro, sometimes a con, but cannot be both simultaneously. In the context of bipolar
decision, we typically set U = A+ and V = A and measure the attractivity of A by
(A+ , A ). The net predisposition of Cumulative Prospect Theory is recovered by letting
(A+ , A ) =  + (A+ )    (A ) = NP(A).

Since the comparison of net predispositions (and, more generally, bi-capacities or bicooperative games) systematically provides a complete and transitive preference, it can fail
to capture a large range of decision-making attitudes: contrasting affects make decision
difficult, so why should the comparison of objects having bipolar evaluations systematically
yield a complete relation? It might imply some incompatibilities. That is why bi-capacities
were generalized by means of bipolar capacities (Greco et al., 2002). The idea underlying
bipolar capacities is to use two measures, a measure of positiveness (that increases with
the addition of positive arguments and the deletion of negative arguments) and a measure
of negativeness (that increases with the addition of negative arguments and the deletion
of positive arguments), but not to combine them. In other terms, a bipolar capacity 
can be equivalently defined by a pair of bi-capacites  + and   , namely by: (A) =
( + (A+ , A ),   (A , A+ )). Then A is preferred to B with respect to  if and only if A
is preferred to B with respect to both  + and    that is, according to the sole Pareto
principle. This allows for the representation of conflicting evaluations and leads to a partial
order.
Our approach provides clear qualitative counterparts of Cumulative Prospect Theory,
bi-capacities, and bipolar capacities only to a certain extent. Indeed, )Lexi belongs to
the Cumulative Prospect Theory family, and can thus be represented by a bi-capacity, or
more generally a bipolar capacity. In contrast, )Pareto obviously belongs to the family of
bipolar capacities. But rules )BiPoss , )Discri, )BiLexi and )Impl are not in the spirit of
bi-capacities nor bi-cooperative games. First of all, and contrary to these models, some of
these decision rules do not provide a complete preorder between alternatives  they argue
on the contrary that conflicts between positive and negative arguments of the same strength
should lead to a conflict, thus to incomparable alternatives. Other decision rules, like )BiPoss
cannot be understood as a comparison of some (A+ , A ) to some (B + , B  ); this rule
indeed compares max(OM(A+ ), OM(B  )) to max(OM(B + ), OM(A )). For )BiPoss , the
right scheme would rather be the comparison of some (A+ , B  ) to some (B + , A ). This
7. For a bibliography about bi-capacities and bi-cooperative games see the work of Grabisch and colleagues
(Grabisch & Labreuche, 2005; Grabisch & Lange, 2007). Most of the developments about these notions
concern the computation of their Shapley value, or the definition of Choquet integrals. But apart from
Cumulative Prospect Theory, very few instances of this general framework are presented. The point is
probably that, because it provides a complete and transitive comparison, it cannot highlight the presence
of conflicting information.

406

fiQualitative Bipolarity in Decision

suggests that neither the framework of bi-cooperative games, nor the ones of bi-capacities
nor of bipolar capacities are yet general enough.
Finally, notice that we have couched our results in a terminology borrowing to argumentation and decision theories, and indeed we consider they can be relevant for both. The
paper is also relevant to argumentative reasoning for the evaluation of sets of arguments in
inference processes (Cayrol & Lagasquie-Schiex, 2005). The connection between reasoning
and decision in the setting of argumentation is laid bare by Amgoud, Bonnefon, and Prade
(2005). They propose an extensive framework where arguments are constructed from a
knowledge base and a base of logically defined criteria. Arguments are evaluated in terms
of certainty, strength and degree of attainment of the corresponding criterion. Assuming
a common scale for these three aspects, they separately compare individual positive arguments, and negative ones, using a simple aggregation of these weights, but never compare
arguments of different polarity. The idea of using logical arguments to compare decisions is
first discussed in the setting of possibilistic logic by Amgoud and Prade (2004). Amgoud and
Prade (2006) further elaborated this framework, where the explicit use of a knowledge base
and a goal base in the construction of arguments builds a bridge between argumentative
reasoning and qualitative decision under uncertainty.

7. Conclusion
This paper has focused on a particular class of bipolar decision making situations, namely
those that are qualitative rather than quantitative. What we have proposed is an extension
of possibility theory to the handling of sets containing arguments considered as positive
or negative. We have laid bare the importance of )Lexi and )BiLexi as decision rules for
qualitative bipolar decision-making. Both rules separately evaluate the positive and negative sets of arguments, by means of big-stepped capacities  + and   . Then, the )Lexi
rule aggregates the two measures in agreement with Cumulative Prospect Theorys net
predisposition. In contrast, the )BiLexi rule does not merge the positive and negative
measures, allowing for the expression of conflict and incomparability. In a sense, these two
rules combine the best of two worlds: while they agree with the spirit of order of magnitude
reasoning, they are more decisive and efficient than basic rules such as )BiPoss , and they
offer the same practical advantages as the quantitative Cumulative Prospect Theorye.g.,
transitivity and representability by a pair of functions.
This paper has adopted a prescriptive point of view in the sense that the rules were
studied with respect to the properties that a qualitative theory of bipolar decision-making
should obey. The representation theorems of Sections 4 and 5 show that the use of )BiP oss
and )Lexi are the only well-behaved ones when dealing with qualitative bipolar information.
In parallel, we tested the descriptive power of our rules, i.e., their accuracy when predicting
the behavior of human decision makers. Our experimental results (Bonnefon et al., in press;
Bonnefon & Fargier, 2006) confirm ,BiPoss as a basic ordinal decision-making rule: when
,BiPoss yields a strict preference, it is very uncommon that human decision-makers disagree
with this preference. Furthermore, results strongly suggest that )Lexi is the decision rule
generally followed by decision-makers: )Lexi accurately predicted nearly 80% of the 2,000
decisions we collected, and was always the one decision-makers individually agreed with the
407

fiDubois, Fargier, Bonnefon

most. Finally, results suggest that human decision-makers do sometimes find some decisions
incomparable; and when they do, it is usually in situations when )BiLexi would predict so.
In the present paper, we did not address the question of the computational complexity
of comparing two objects with a bipolar decision rule. Actually, the presence of bipolar
information does not change the range of complexity of the comparison of objects. A detailed
complexity study is out of the scope of the paper, but we can at least say that for all the
rules presented in this paper, the comparison of two alternatives is at most polynomial in the
number of criteria in X. This comparison indeed relies on the computation of the strength
of four subsets of X  computation that is sometimes preceded by a simple deletion step
(for the rules based on a discri- or a lexi- comparison). The computation of the strength of a
set is itself linear, since performed by means of an aggregation operator. The complexity of
comparing two alternatives with the )BiPoss or the )Lexi rules is for instance in O(Card(X))
(it can be computed as a simple aggregation of individual strengths). Finally, notice that
all our rules provide a transitive strict preference, so no cycle can appear as it may be
the case in CP-nets. If combinatorial alternatives were to be considered, we could easily
use a branch and bound algorithm for looking for the best alternative(s); the problem
of optimization with a bipolar aggregation is not harder than with a unipolar one: the
corresponding decision problem remains NP-complete (Fargier & Wilson, 2007).
This concludes our study of qualitative bipolar reasoning. However, we are aware of
some unresolved issues this paper has raised. For example, Section 6 suggests that the
framework of bi-capacities, and even bipolar capacities, is not rich enough to accommodate
the full range of qualitative bipolar rules we have introduced. Secondly, our results were
established within a restricted framework, where a relevant criterion is either a complete
pro or a complete opponent w.r.t. each decision, in the spirit of bi-cooperative games. This
is clearly a simpler approach than usual multi-criteria decision-making frameworks, where
each x  X is a full-fledged criterion rated on a bipolar utility scale like Lx = [1x , +1x ],
containing a neutral value 0x . Thus, a natural extension of the present work would be to
address qualitative bipolar criteria whose satisfaction is a matter of degree.

Appendix A. Proofs
To simplify notations, let a+ = OM(A+ ), a = OM(A ), b+ = OM(B + ), b = OM(B  ),
c+ = OM(C + ), c = OM(C  ). Hence OM(A) = max(a+ , a ), OM(A+ B  ) = max(a+ , b )
and so on.
Proposition 1. The proof of completeness is trivial, as OM(A+  B  ) and OM(B +  A )
can always be compared. To prove the transitivity of ,BiPoss , let us assume max(a+ , b ) >
max(a , b+ ) and max(b+ , c ) > max(b , a+ ). Then, letting b = max(b+ , b ), we get
max(a+ , b, c ) > max(a , b, c+ ). As a consequence, max(a+ , c ) > b. Hence, max(a+ , c ) >
max(a , b, c+ )  max(a , c+ ).
Proposition 2. Quasi-transitivity is proved in Proposition 1. Positive and negative monotony,
as well as SQC, follow from the monotony of OM, which is a possibility measure: i.e.,
U, V, OM(U  V )  OM(V ). Non-triviality of )BiPoss is obtained from the non-triviality
of  (there exists x such as (x) = 0), which implies OM((X + )+  (X  ) ) = OM(X + 
408

fiQualitative Bipolarity in Decision

X  ) > 0 while OM((X + )  (X  )+ ) = OM() = 0. Clarity of argument is also trivial: if x  X + then OM({x}+  () ) = (x)  OM({x}  ()+ ) = OM() = 0:
{x} )BiPoss . If x is a con, we get in the same way {x} 5BiPoss . If x has null importance, we get OM({x}+  () ) = OM({x}  ()+ ) = OM() = 0: {x} BiPoss .
To prove weak unanimity, suppose that A+ )BiPoss B + and A )BiPoss B  . Obviously
A+ )BiPoss B +  OM(A+ )  OM(B + ) and A )BiPoss B   OM(B  )  OM(A ).
Hence OM(A+  B  )  OM(B +  A ) : A )BiPoss B.
Proposition 3. When restricted to singletons, )BiPoss is a weak order that ranks the positive
arguments by decreasing values of , then the null arguments ( = 0), then the negative
arguments by increasing value of . This ranking defines a complete and transitive relation.
This proves that the relation )X induced by )BiPoss is a weak order. Axioms POSC is easy
to check, since {x+ , y  }  (resp., {z + , y  } BiPoss ) if and only if (x+ ) = (y  ) (resp.
(z + ) = (y  )). Then {x+ , y  } BiPoss  and {z + , y  }   imply (x+ ) = (z + ). So,
{x+ } BiPoss {z + }, i.e., x+ X z + . The proof of NEGC is similar. The X-monotony of
)BiPoss is shown as follows. Let A, x, x$ be such that A  {x, x$ } =  and x$ )X x. Three
cases are possible:
1. x  X + . Then x$  X + and (x$ )  (x).
 if A  {x} ,BiPoss B: then OM(A+  {x}  B  ) > OM(A  B + ). Since (x$ ) 
(x), we get OM(A+  {x$ }  B  )OM(A  B + ): A  {x$ } ,BiPoss B.
 if A  {x} BiPoss B: then OM(A+  {x}  B  ) = OM(A  B + ). Replacing x
by x$ , i.e., (x) by (x$ ), the first OM level increases, so we get OM(A+  {x$ } 
B  )  OM(A  B + ): A  {x$ } )BiPoss B.
 if B , A  {x$ }, then OM(B +  A ) > OM(A+  {x$ }  B  ). Replacing x$ by
x, i.e., (x$ ) by (x), the second OM level decreases, so we get OM(B +  A ) >
OM(A+  {x}  B  ), i.e. B ,BiPoss A  {x}.
 if B  A  {x$ }, then OM(B +  A ) = OM(A+  {x$ }  B  ). Replacing x$
by x, i.e. (x$ ) by (x), the second OM level decreases, so OM(B +  A ) 
OM(A+  {x}  B  ), i.e. B )BiPoss A  {x}.
2. x$  X  . Then x  X  and (x)  (x$ ). The same kind of four-case proof can be
carried out.
3. x$  X + and x  X  .
 if A  {x} ,BiPoss B, i.e., OM(A+  B  ) > OM(A  B +  {x}), it follows that
OM(A+  B   {x$ }) > OM(A  B + ), so A  {x$ } ,BiPoss B.
 Similarly, if A{x} BiPoss B, i.e., OM(A+ B  ) = OM(A B + {x}), it follows
that OM(A+ B  {x$ })  OM(A B + {x}), and then OM(A+ B  {x$ }) 
OM(A B + {x}), which implies OM(A+ B  {x$ })  OM(A B + ). Hence
(x$ being positive) A  {x$ } )BiPoss B.
 if B , A  {x$ }, which means OM(B +  A ) > OM(A+  {x$ }  B  ) since x$
is positive. Obviously, OM(B +  A  {x})  OM(B +  A ) and OM(A+ 
{x$ }  B  )  OM(A+  B  ). Hence OM(B +  A  {x}) > OM(A+  B  ):
B ,BiPoss A  {x}.
409

fiDubois, Fargier, Bonnefon

 Similarly, if B BiPoss A  {x$ } we have: OM(B +  A  {x})  OM(B +  A ) =
OM(A+  {x$ }  B  )  OM(A+  B  ): B )BiPoss A  {x}.
Proposition 4.
)BiPoss satisfies GCLO Recall that A )BiPoss B  max(a+ , b )  max(b+ , a ) and
C )BiPoss D  max(c+ , d )  max(d+ , c ). Hence, max(a+ , b , c+ , d ) 
max(b+ , a , d+ , c ), i.e., A  C )BiPoss B  D.
)BiPoss satisfies GNEG Recall that A ,BiPoss B  max(a+ , b ) > max(b+ , a ) and
C ,BiPoss D  max(c+ , d ) > max(d+ , c ). Hence: max(a+ , b , c+ , d ) >
max(b+ , a , d+ , c ), i.e., A  C ,BiPoss B  D.
Theorem 1. Let us first build . By CA, any singleton {x} is comparable to . So X + =
{x, {x} , }, X  = {x, {x}  } and X 0 = {x, {x}  } are soundly defined.
Let us define a relation  on X as follows:
 x, y  X 0 : x  y and y  x.

 x, y  X + : x  y  {x} ) {y}
 x, y  X  : x  y  {y} ){ x}
 x  X + , y  X  : x  y  not( , {x, y})
 x  X +, y  X 0 : x > y
 x  X , y  X 0 : x > y

Because of axiom CA, X + , X  and X 0 are disjoint. The previous definition is thus well
founded.  is complete by definition. We prove now that  is transitive. Suppose that
x  y and y  z and let us perform the following case analysis:
 x, y, z  X + : Then x  z is trivial because )X is transitive and can be identified
with  within X + .
 x, y, z  X  : Then x  z is trivial because )X is transitive and can be identified
with  within X  .
 x  X 0 : then x  y means by STQ that y is also in X 0 , which in turn implies z  X 0 .
So, by STQ again, x  z.
 y  X 0 : y  z implies by STQ that z is also in X 0 . So, by STQ again, x  z.
 z  X 0 : x  z is always true (by status quo consistency again).
 x  X + , y  X  and z  X  . Then by definition, x  y means {x, y} )  and
y  z means {z} ){ y}. By X-monotony we can replace y by z without reversing the
preference: {x, z} ) , i.e. x  z.
410

fiQualitative Bipolarity in Decision

 x  X + , y  X + and z  X  : the proof of x  z is similar to the one in the previous
item.
 y  X + x  X  and z  X  . Suppose that {x, y} 5 (x  y), {z, y} )  (y  z) and
{x} > {z} (z > x for negative arguments). If {z, y} , , then X-monotony implies
{x, y} ,  (thus a contradiction). If  , {x, y} then X-monotony implies  , {z, y}
(second contradiction). Last case, if {x, y}   and {z, y}  , then POSC implies
{x} { z} (last contradiction). So, z > x does not hold and thus, by completeness of
, x  z.
 y  X  x  X + and z  X + . The proof of x  z is similar to the one in the previous
item, using NEGC instead of POSC.
So,  is a weak order. It can be encoded by a distribution  : X ' [0L , 1L ], [0L , 1L ]
being a totally ordered scale. Level 0L is mapped to elements of X0 . Now, we have to show
the equivalence between ) and the relation )BiPoss induced from , namely to prove that
A ) B  OM(A+ B  )  OM(A B + ). Since ) is complete, this amounts to showing
that OM(A+  B  ) = OM(A  B + ) implies A  B and that OM(A+  B  )OM(A  B + )
implies A , B. Let us first prove that OM(A+ B  )OM(A B + ) implies A , B. Suppose
that the element of highest  value in A+ B  is x  A+ . So, for any y  A , {x, y} ,  and
w  B + , {x} ,{ w}. By GNEG we get {x}A , B + and positive and negative monotony
then imply A+  A , B +  B  . If the element of highest  value in A+  B  were some
v  B  then for any w  B + , {w, v}   and for any y  A , {y} , {v}. By GNEG we
get {v}  B +  A and positive and negative monotony then imply A+  A , B +  B  .
Let us now prove that OM(A+  B  )  OM(A  B + ) implies A ) B. Suppose that the
element of highest  value in A+  B  is x  A+ . So, for any y  A , {x, y} ) and
w  B + , {x} ) {w}. By GCLO we get {x}  A ) B + and positive and negative monotony
then imply A+  A ) B +  B  . If the element of highest  value in A+  B  were some
v  B  then for any w  B + , {w, v} 5  and for any y  A , {y} ){ v}. By GCLO we get
{v}  B + 5 A and positive and negative monotony then implies A+  A ) B +  B  .
Proposition 5. Consider the four cases identified in the text. Clearly situation 1 corresponds
to a case where A Impl B. In case 4, strict dominance A ,Impl B prevails. Case 2 may
lead to three different conclusions. Equivalence arises when a+ = b+ max(a , b ): the
cons being of low level w.r.t to the pros, they are not taken into account and indifference
prevails based on the pros. If a+ = b+ = b > a , then A )Impl B holds but not
B )Impl A. So, A ,Impl B since the arguments against A are too weak. By symmetry,
a+ = b+ = a > b+ concludes to B ,Impl A. Case 3 is handled in a similar manner:
equivalence arises when a = b > max(a+ , b+ ); if a = b = a+ > b+ , A ,Impl B
holds since the arguments for B are too weak; if a = b = b+ > a+ , B ,Impl A Finally,
A "Impl B is concluded when neither A )Impl B nor B )Impl A. This arises in two cases
only, when a+ = a > max(b , b+ ) or in the symmetric case b+ = b max(a , a+ ).
Proposition 6. Assume A )Impl B and B )Impl C. Using the above conventions, consider:
1. Any of the following situations ensuring A )Impl B:
411

fiDubois, Fargier, Bonnefon

(a) a+ = b+ = a = b ;
(b) a+ = b+  max(a , b );
(c) a = b  max(a+ , b+ );

(d) max(a+ , b ) > max(a , b+ ).
2. And any of the following situations ensuring B )Impl C the following group:
(a) b+ = c+ = b = c ;
(b) b+ = c+  max(b , c );
(c) b = c  max(b+ , c+ );

(d) max(b+ , c ) > max(b , c+ ).
Combining one condition in the first group with one in the second group yields one of the
corresponding conditions ensuring A )Impl C. For instance,
 Combining conditions 1b and 2b yields a+ = c+  max(a , b , c )  max(a , c ).
 Combining conditions 1b and 2c yields a+  max(a , c ) and c  max(a+ , c+ ).
Hence a+  max(a , a+ , c+ ), c  max(a , c , c+ ) and a+ = c . Hence a+ = c 
max(a , c+ ). If the inequality is strict this is condition d. If a+ = c = max(a , c+ ),
this is condition b or c.
 Combining condition 1d and condition 2b yields max(a+ , b ) > max(a , c+ ) and
c+  max(b , c ). Hence a+ > max(a , c+ ).
The other cases can be handled similarly.
Proposition 7. A ,BiPoss B if and only if max(a+ , b ) > max(a , b+ ) which thanks to
Proposition 5 implies A ,Impl+ B.
Proposition 8.
 Suppose that A ,BiPoss B: then x  A+ B  such that x  A B + , (x )(x). If
many arguments satisfy this property, let x be one of those maximizing . x is thus
not in A  B +. So, it is either in A+ \ B + or in B  \ A . Since A+ \ B + = (A \ B)+
and B  \ A = (B \ A) , we can write x  (A \ B)+  (B \ A) . On the other hand
(B \ A)+  (A \ B)  A  B + and no element in A  B + has a higher degree than
x . So, OM((B \ A)+  (A \ B) ) < (x ). So, A ,Discri B. This proves that )Discri
refines )BiPoss .
 Suppose that A ,Discri B: then x  A+ \ B +  B  \ A such that x  B + \ A+ 
A \ B  , (x ) > (x). If many elements satisfy this property, let x be one of those
maximizing . So, at any level  > (x ), A+ \ B +  B  \ A is empty; thus for
+


these levels, A+
 = B and A = B , which imply the equality of the cardinalities.

On the other hand, at level  = (x ), there is no element in B + \ A+  A \ B  .
So, |(B + \ A+ ) | = 0 and |(A \ B  ) | = 0. And there is at least one element
in A+ \ B +  B  \ A so |(A+ \ B + ) |  1 or |(B  \ A ) |  1 (or even both).
412

fiQualitative Bipolarity in Decision

Suppose now |(A+ \ B + ) |  1: from |(B + \ A+ ) | = 0 and adding the common
+
elements, we get |B+ | = |(A+  B + ) |. Since |(A+ \ B + ) |  1 we get |A+
 ||B |.
From |(A \ B  ) | = 0 and adding the common element we get |A | = |B   A |
thus |A |  |B  |. So A ,BiLexi B. We get A ,BiLexi B in the same way from
|(B  \ A ) |  1. Hence )BiLexi refines )Discri .
+
 Finally, suppose that A ,BiLexi B, i.e., at any level  higher than  , |A+
 | = |B | and



|A | = |B | and that at level  , there is a difference in favor of A. Then necessarily,

+

at any level higher  than  , |A+
 ||A | = |B ||B |. Let first suppose that at level
+

+

 , the difference is made on the positive scale, i.e., |A+
 | > |B | and |A |  |B |.
+

+

Summing up the inequalities we get: |A |  |A ||B |  |B |. Then A ,Lexi B. If
the difference is rather made on the negative side, we get A ,Lexi B in a similar way.
So, A ,Lexi B. Hence )Lexi refines )BiLexi .

Proposition 10 . From C  D and A  (C  D) = , preferential independence implies
A  C  A  D. Then, by transitivity: A  C ) B implies A  D ) B; A  D ) B implies
A  C ) B; B ) A  C implies B ) A  D; and B ) A  D implies B ) A  C.
Corollary 1 . Since C   and A  C = , we can apply the principle of preferential
independence and get A  C  A. Then, by transitivity, A ) B implies A  C ) B.
Conversely, A  C ) B and A  C  A implies, also by transitivity, that A ) B. Similarly,
from B ) A and AC  A, transitivity implies B ) AC; from B ) AC and AC  A,
transitivity implies B ) A.
Corollary 2 . From the axiom of preferential independence, and because (A  B)  C = :
A ) B  A  C ) B  C. Then applying Proposition 10 and because B  (C  D) = ,
A ) B  A  C ) B  D.
Corollary 3.
Case x  B: Then by Proposition 10 we can replace x by y in B and get A ) B 
A ) (B \ {x})  {y}. Let us then apply preferential independence and add x on
both sides of ): A ) (B \ {x})  {y}  A  {x} ) (B \ {x})  {y}  {x}, i.e.,
A ) (B \ {x})  {y}  A  {x} ) B  {y}, thus A ) B  A  {x} ) B  {y}.
Case x # B: Then preferential independence means that A ) B  A  {x} ) B  {x}
and Proposition 10 is used to get A ) B  A  {x} ) B  {y}.
Theorem 2 . It is easy to show that item 2 implies item 1. We have already seen that )Lexi
is complete, transitive, satisfies preferential independence, and refines )BiPoss . It is also
easy to show that the grounding relations on X induced by both relations are equivalent,
i.e., that )Lexi is an unbiased refinement of )BiPoss .
Let us now prove that item 1 implies item 2. Let ) be a complete and transitive
monotonic bipolar set-relation that satisfies preferential independence, and is an unbiased
. It is
refinement of )BiPoss . Since ) is an unbiased refinement of )BiPoss , )X )BiPoss
X
413

fiDubois, Fargier, Bonnefon

equivalently defined by . The notions of -section, positive -section and negative section are thus well defined.

+

1. Let us first suppose that, for any , |A+
 |  |A | = |B | | B |. Then we show that
, A  B . Two cases are possible:

A

Case |A+
 |  |A |  0: Then it is possible to partition A into n = |A | pairs of
+

+

+
{x , x } and n = |A |  |A | singletons {x }. Similarly, it is possible to partition B into nB = |B | pairs of {y + , y  } and n singletons {y + }the same n as

+

for A, since |A+
 | | A | = |B |  |B |. So, from  = , we can use Proposition
+
3 to add the n singletons {x } and the n singletons {y + } on each side of the
equivalence (left side for the x+ , right side for the y + ). Then Corollary 1 allows
us to add the pairs {x+ , x } on the left side, the pairs {y + , y  } on the right side.
Then we arrive at A  B .


A
Case |A+
 |  |A |  0: The proof is similar. It is possible to partition A into n =
+ 
+


|A
 | pairs of {x , x } and n = |A | | A | singletons {x }. Similarly, it is
B

+

possible to partition B into n = |B | pairs of {y , y } and n singletons {y  }.
So, from  = , we can use Proposition 3 and Corollary 1 allow us to add the pairs
{x+ , x } and the remaining singletons {x } on the left side, the pairs {y + , y  }
and the remaining singletons {y  } on the right side. We arrive at A  B .

The -sections of A  B are pairwise disjoint.
Thanks
to Corollary 2 we can then
$
$
sum the equivalences A  B and get  A   B , i.e., A  B.


2. Let us now suppose that there is a  > 0L such that (i) for any  > , |A+
 |  |A | =
+

+

|B+ | | B | and
$ (ii) |A | $|A | > |B |  |B |. From a proof similar to the one just
before, we get > A  > B . Three cases are then possible:


A

Case |A+
 |  |A |  0: Then it is possible to partition A into n = |A | pairs of
+

+

+
{x , x } and n = |A | | A | singletons {x }. Similarly, it is possible to
partition B into nB = min(|B+ |, |B |) pairs of {y + , y  } and m singletons
{y + } and a number m$ of singletons {y  }, with max(m, m$ ) = 0. Because


|B+ | | B | < |A+
 | | A | = n, m < n. Let x be one of the n positive
BiPoss 
+
singletons in the partition of A . Obviously, x < A
< B and
 ,

BiPoss

thus by monotony x < A ,
< B . Then x < A , < B
(this is the refinement hypothesis). Then we can add the n singletons x+ and
n singleton y + on the left and right side of the strict preference without modifying it (this is Proposition 3). By monotony, we can then add the singletons
y  , if any, and the remaining m  n  1 singletons x+ , if any. Since the pairs
{x+ , x } (resp., {y + , y  }) are equivalent to , they can be added on the left
(resp., right) side without modifying the inequality (we use Corollary 1). We get
 A ,  B .

Case |B+ | | B |  0: The proof if very similar to the previous one, from a negative
y  . We partition B into a maximum number of pairs, and a given number n
of negative singletons. A is also partitioned into a maximum number of pairs,
414

fiQualitative Bipolarity in Decision

possibly m < n negative singleton or (the two conditions are exclusive) some
positive singletons we show that < A , < B  {y  }, then add m the
negative singletons x and the negative singletons y  on their respective sides,
add the remaining negative y  or the remaining positive x+ . The pairs {x+ , x }
(resp., {y + , y  }) being equivalent to , they can be added on the left (resp., right)
side without modifying the inequality. We get  A ,  B .


Finally, |B+ |  |B | > 0 and |A+
 | | A | < 0 is incompatible with the fact that
+

+

|A |  |A | > |B |  |B |.

We thus get  A ,  B . The -sections of A  B are pairwise disjoint.
Thanks to Corollary 2 we$can then$sum the equivalences A  B ,  > to the
string preference and get  A ,  B , i.e. A , B.

3. We have shown that A ,lexi B = A , B and A lexi B = A  B. Because the
relations are complete, this means that they are equivalent: A )lexi B  A ) B.

References
Amgoud, L., Bonnefon, J. F., & Prade, H. (2005). An argumentation-based approach for
multiple criteria decision. Lecture Notes in Computer Science, 3571, 269280.
Amgoud, L., & Prade, H. (2004). Using arguments for making decisions: A possibilistic
logic approach. In M. Chickering & J. Halpern (Eds.), Proc. of the 20th Conference of
Uncertainty in Artificial Intelligence (UAI04) (pp. 1017). Menlo Park, CA: AUAI
Press.
Amgoud, L., & Prade, H. (2006). Explaining qualitative decision under uncertainty by
argumentation. In Proc. of the 21st National Conf. on Artificial Intelligence. Menlo
Park, Ca: AAAI Press.
Benferhat, S., Dubois, D., Kaci, S., & Prade, H. (2006). Bipolar possibility theory in
preference modeling: Representation, fusion and optimal solutions. International
Journal on Information Fusion, 7, 135150.
Benferhat, S., Dubois, D., & Prade, H. (2001). Towards a possibilistic logic handling of
preferences. Applied Intelligence, 14 (3), 303317.
Besnard, P., & Hunter, A. (2008). Elements of Argumentation. Cambridge, Mass.: The
MIT Press.
Bilbao, J. M., Fernandez, J. R., Jimenez Losada, A., & Lebron, E. (2000). Bicooperative
games. In J. M. Bilbao (Ed.), Cooperative games on combinatorial structures (p.
23-26). Dordrecht: Kluwer Academic Publishers.
Bonnefon, J. F., Dubois, D., Fargier, H., & Leblois, S. (in press). Qualitative heuristics for
balancing the pros and cons. Theory & Decision.
Bonnefon, J. F., & Fargier, H. (2006). Comparing sets of positive and negative arguments: Empirical assessment of seven qualitative rules. In G. Brewka, S. Coradeschi,
A. Perini, & P. Traverso (Eds.), Proceedings of the 17th European Conference on
Artificial Intelligence (ECAI2006) (pp. 1620). Zurich: IOS Press.
415

fiDubois, Fargier, Bonnefon

Boutilier, C., Brafman, R. I., Domshlak, C., Hoos, H. H., & Poole, D. (2004). CP-nets: A
tool for representing and reasoning with conditional ceteris paribus preference statements. J. Artif. Intell. Res. (JAIR), 21, 135-191.
Brafman, R., & Tennenholtz, M. (2000). An axiomatic treatment of three qualitative
decision criteria. Journal of the ACM, 47 (3), 452482.
Brandstatter, E., Gigerenzer, G., & Hertwig, R. (2006). The priority heuristic: Making
choices without trade-offs. Psychological Review, 113, 409432.
Brewka, G. (1989). Preferred subtheories: An extended logical framework for default
reasoning. In Int. Joint Conf. on Artificial Intelligence (p. 1043-1048). Menlo Park,
Ca: AAAI Press.
Cacioppo, J. T., & Berntson, G. G. (1994). Relationship between attitudes and evaluative
space: A critical review, with emphasis on the separability of positive and negative
substrates. Psychological Bulletin, 115, 401423.
Cayrol, C., & Lagasquie-Schiex, M.-C. (2005). Graduality in argumentation. Journal of
Artificial Intelligence Research, 23, 245297.
Deschamps, R., & Gevers, L. (1978). Leximin and utilitarian rules: A joint characterization.
Journal of Economic Theory, 17, 143163.
Doyle, J., & Thomason, R. (1999). Background to qualitative decision theory. The AI
Magazine, 20 (2), 5568.
Dubois, D. (1986). Belief structures, possibility theory and decomposable confidence measures on finite sets. Computers and Artificial Intelligence, 5 (5), 403416.
Dubois, D., & Fargier, H. (2003). Qualitative decision rules under uncertainty. In
G. Della Riccia, D. Dubois, R. Kruse, & H.-J. Lenz (Eds.), Planning Based on Decision Theory (Vol. 472, pp. 326). Wien: Springer.
Dubois, D., & Fargier, H. (2004). An axiomatic framework for order of magnitude confidence
relations. In M. Chickering & J. Halpern (Eds.), Proceedings of the 20th Conference
of Uncertainty in Artificial Intelligence (UAI04) (pp. 138145). Menlo Park, CA:
AUAI Press.
Dubois, D., & Fargier, H. (2005). On the qualitative comparison of sets of positive and
negative affects. Lecture Notes in Computer Science, 3571, 305316.
Dubois, D., & Fargier, H. (2006). Qualitative decision making with bipolar information. In
P. Doherty, J. Mylopoulos, & C. Welty (Eds.), Proceedings of the 10th International
Conference on Principles of Knowledge Representation and Reasoning (pp. 175186).
Menlo Park, CA: AAAI Press.
Dubois, D., Fargier, H., & Prade, H. (1996). Possibility theory in constraint satisfaction
problems: Handling priority, preference and uncertainty. Applied Intelligence, 6 (4),
287-309.
Fargier, H., & Sabbadin, R. (2005). Qualitative decision under uncertainty: back to expected utility. Artificial Intelligence, 164, 245280.
Fargier, H., & Wilson, N. (2007). Algebraic structures for bipolar constraint-based reasoning. In Symbolic and quantitative approaches to reasoning with uncertainty, 9th european conference, ecsqaru 2007, proceedings (Vol. 4724, p. 623-634). Berlin: Springer.
Fishburn, P. (1986). The axioms of subjective probabilities. Statistical Science, 1 (3),
335345.
416

fiQualitative Bipolarity in Decision

Gigerenzer, G., & Goldstein, D. (1996). Reasoning the fast and frugal way: Models of
bounded rationality. Psychological Review, 103, 650669.
Gigerenzer, G., Todd, P. M., & the ABC group. (1999). Simple heuristics that make us
smart. New York: Oxford University Press.
Grabisch, M., & Labreuche, C. (2002). Bi-capacities for decision making on bipolar scales.
In Eurofuse02 Workshop on Information Systems (p. 185-190).
Grabisch, M., & Labreuche, C. (2005). Bi-capacities  parts I and II. Fuzzy Sets and
Systems, 151 (2), 211260.
Grabisch, M., & Lange, F. (2007). Games on lattices, multichoice games and the Shapley
value: a new approach. Mathematical Methods of Operations Research, 65 (1), 153
167.
Greco, S., Matarazzo, B., & Slowinski, R. (2002). Bipolar Sugeno and Choquet integrals.
In B. De Baets, J. Fodor, & G. Pasi (Eds.), Proceedings of the 7th Meeting of the
EURO Working Group on Fuzzy Sets (EUROFUSE 2002 (pp. 191196).
Halpern, J. Y. (1997). Defining relative likelihood in partially-ordered structures. Journal
of Artificial Intelligence Research, 7, 124.
Katsikopoulos, K. V., & Martignon, L. (2006). Nave heuristics for paired comparisons:
Some results on their relative accuracy. Journal of Mathematical Psychology, 50,
488494.
Lehmann, D. J. (1996). Generalized qualitative probability: Savage revisited. In E. Horvitz
& F. Jensen (Eds.), Proceedings of the 12th Conference on Uncertainty in Artificial
Intelligence (UAI-96) (pp. 381388). San Francisco, CA: Morgan Kaufman.
Lewis, D. L. (1973). Counterfactuals and comparative possibility. Journal of Philosophical
Logic, 2, 418446.
Moulin, H. (1988). Axioms of cooperative decision making. New-York: Wiley.
Osgood, C. E., Suci, G., & Tannenbaum, P. H. (1957). The Measurement of Meaning.
Chicago: University of Illinois Press.
Slovic, P., Finucane, M., Peters, E., & MacGregor, D. G. (2002). Rational actors or rational
fools? Implications of the affect heuristic for behavioral economics. The Journal of
Socio-Economics, 31, 329342.
Tversky, A., & Kahneman, D. (1992). Advances in prospect theory: Cumulative representation of uncertainty. Journal of Risk and Uncertainty, 5, 297323.
Wald, A. (1971). Statistical Decision Functions. New York: Wiley. (Original work published
1950)

417

fiJournal of Artificial Intelligence Research 32 (2008) 95-122

Submitted 10/07; published 05/08

Graphical Model Inference in Optimal Control of Stochastic
Multi-Agent Systems
Bart van den Broek
Wim Wiegerinck
Bert Kappen

B.vandenBroek@science.ru.nl
W.Wiegerinck@science.ru.nl
B.Kappen@science.ru.nl

SNN, Radboud University Nijmegen, Geert Grooteplein 21,
Nijmegen, The Netherlands

Abstract
In this article we consider the issue of optimal control in collaborative multi-agent
systems with stochastic dynamics. The agents have a joint task in which they have to
reach a number of target states. The dynamics of the agents contains additive control and
additive noise, and the autonomous part factorizes over the agents. Full observation of the
global state is assumed. The goal is to minimize the accumulated joint cost, which consists
of integrated instantaneous costs and a joint end cost. The joint end cost expresses the joint
task of the agents. The instantaneous costs are quadratic in the control and factorize over
the agents. The optimal control is given as a weighted linear combination of single-agent
to single-target controls. The single-agent to single-target controls are expressed in terms
of diffusion processes. These controls, when not closed form expressions, are formulated
in terms of path integrals, which are calculated approximately by Metropolis-Hastings
sampling. The weights in the control are interpreted as marginals of a joint distribution
over agent to target assignments. The structure of the latter is represented by a graphical
model, and the marginals are obtained by graphical model inference. Exact inference of the
graphical model will break down in large systems, and so approximate inference methods are
needed. We use naive mean field approximation and belief propagation to approximate the
optimal control in systems with linear dynamics. We compare the approximate inference
methods with the exact solution, and we show that they can accurately compute the optimal
control. Finally, we demonstrate the control method in multi-agent systems with nonlinear
dynamics consisting of up to 80 agents that have to reach an equal number of target states.

1. Introduction
The topic of control in multi-agent systems is characterized by many issues, originating from
various sources, including a wide variety of possible execution plans, uncertainties in the
interaction with the environment, limited operation time and supporting resources, and a
demand for robustness of the joint performance of the agents. Such issues are encountered
in, for example, air traffic management (Tomlin, Pappas, & Sastry, 1998; van Leeuwen,
Hesseling, & Rohling, 2002), formation flight (Ribichini & Frazzoli, 2003; Hu, Prandini,
& Tomlin, 2007), radar avoidance for unmanned air vehicles or fighter aircraft (Pachter &
Pachter, 2001; Kamal, Gu, & Postlethwaite, 2005; Larson, Pachter, & Mears, 2005; Shi,
Wang, Liu, Wang, & Zu, 2007), and persistent area denial (Subramanian & Cruz, 2003;
Liu, Cruz, & Schumacher, 2007; Castanon, Pachter, & Chandler, 2004).
In many control approaches in multi-agent systems, stochastic influences in the dynamics
of the agents are not taken into account or assumed negligible, and the dynamics are
c
2008
AI Access Foundation. All rights reserved.

fivan den Broek, Wiegerinck & Kappen

modeled deterministically. If the system is truly deterministic, then the agents can be
optimally controlled by open loop controls. However, when the stochastic influences in
the dynamics are too large to be ignored, open loop controls become far from optimal,
and the multi-agent system should no longer be modeled deterministically.
The usual
approach to control in multi-agent systems with stochastic dynamics is to model the system
by a Markov Decision Processes (MDP) (Boutilier, 1996; Sadati & Elhamifar, 2006). In
principle, these are solved in discrete space and time by backward dynamic programming.
However, the discretization will make the joint state space of the multi-agent system increase
exponentially in the number of agents, and a basic dynamic programming approach will
generally be infeasible (Boutilier, 1996). An attempt to overcome this is to exploit structures
in the problem and describe the system by a factored MDP. In general these structures will
not be conserved in the value functions, and exact computations remain exponential in the
system size. Guestrin, Koller, and Parr (2002a) and Guestrin, Venkataraman, and Koller
(2002b) assumed a predefined approximate structure of the value functions, and thereby
provided an efficient approximate MDP model for multi-agent systems. A similar approach
was taken by Becker, Zilberstein, Lesser, and Goldman (2003, 2004), assuming independent
collaboration of the agents with a global reward function, resulting in transition-independent
decentralized MDPs.
In this paper we concentrate on multi-agent systems where the agents have a joint task
in which they have to reach a number of target states. We model the multi-agent system
in continuous space and time, following the approach of Wiegerinck, van den Broek, and
Kappen (2006). We make the following assumptions. The agents are assumed to have
complete and accurate knowledge of the global state of the system (assumption 1). The
dynamics of each agent is additive in the control and disturbed by additive Wiener noise
(assumption 2). The performance of the agents is valued by a global cost function, which is
an integral of instantaneous costs plus an end cost. The joint task of the agents is modeled
by the end cost. The instantaneous costs are assumed to be quadratic in the control
(assumption 3). The noise level in the dynamics of the agents is inversely proportional to
the control cost (assumption 4). Finally, we assume that both the autonomous dynamics
and the instantaneous costs factorize over the agents (assumption 5).
Under the assumptions 1 and 2, the optimal control problem is partially solved by finding
the optimal expected cost-to-go, which satisfies the so-called stochastic Hamilton-JacobiBellman (SHJB) equation. Once the optimal expected cost-to-go is given, the optimal
control is provided as the gradient of the optimal expected cost-to-go by adopting assumption 3. The SHJB equation is a nonlinear partial differential equation (PDE), and this
nonlinearity makes it difficult to solve. A common approach to solving the SHJB equation
is to assume, in addition to assumption 3, that the instantaneous costs and the end cost
in the cost function are quadratic in the state, and that the dynamics are linear in the
state as wellthis is known as linear-quadratic control. The optimal expected cost-to-go
then is quadratic in the state with time-varying coefficients, and the problem reduces to
solving the Riccati equations that these coefficients satisfy (Stengel, 1993; ksendal, 1998).
Otherwise, approximation methods are needed. An approximate approach is given by the
iterative linear-quadratic Gaussian method (Todorov & Li, 2005); this yields a locally optimal feedback control, and is valid in case there is little noise. We instead follow the approach
of Fleming (1978) and adopt assumption 4. Under this assumption the SHJB equation can
96

fiGraphical Model Inference in MAS Optimal Control

be transformed into a linear PDE by performing a logarithmic transformation. Its solution
equals the expectation value of a stochastic integral of a diffusion process. In general, this is
not a closed form expression. In this paper we will estimate this expression by formulating
it as a path integral (Kappen, 2005a, 2005b), and we estimate the latter using MetropolisHastings sampling. There are several other ways to estimate the path integral, such as
Hamilton Monte Carlo sampling and the Laplace approximation, but these are not covered
in this paper.
The structure of the optimal expected cost-to-go will generally be very complex due to
the dynamic couplings between the agents. By adopting assumption 5, the agents will only
be coupled through the joint end cost, which then solely determines the structure of the
optimal expected cost-to-go. This will result in state transition probabilities that factorize
over the agents. It follows that the optimal control becomes a weighted combination of
single-agent to single-target controls. The weights are given by a joint distribution over
agent to target assignments. The joint distribution has the same structure as the joint end
cost. The structure of the joint distribution is representable by a factor graph, and the
optimal control problem becomes a graphical model inference problem (Wiegerinck et al.,
2006). The complexity of the graphical model inference is exponential in the tree width
of the factor graph. Exact inference will be possible by using the junction tree algorithm,
given that the graph is sufficiently sparse and the number of agents is not too large. In
more complex situations approximate inference methods are necessary, and we show that
the optimal control can accurately be approximated in polynomial time, using naive mean
field (MF) approximation or belief propagation (BP). This makes distributed coordination
possible in multi-agent systems that are much larger than those that could be treated with
exact inference.
The paper is organized as follows. In Sections 2 and 3, we provide a review of both
the single and the multi-agent stochastic optimal control framework, developed by Kappen (2005a, 2005b) and Wiegerinck et al. (2006). As an example, we will rederive linear
quadratic control. The general solution is given in terms of a path integral, and we explain
how it can be approximated with Metropolis-Hastings sampling.
In Section 4, we give a factor graph representation of the end cost function. We discuss two graphical model approximate inference methods: naive mean field approximation
and belief propagation. We show that the approximation of the optimal control in both
methods is obtained by replacing the exact weights in the controls with their respective
approximations.
In Section 5, we present numerical results. We make a comparison of the approximate
optimal controls, infered by the naive mean field approximation, belief propagation and
a greedy method, with the exact optimal control; this we do in a multi-agent system of
18 agents with linear dynamics in a two-dimensional state space, and with two target
states. Furthermore, we present results from control in multi-agent systems with nonlinear
dynamics and a four-dimensional state space, in which agents control their forward velocity
and driving direction. The controls are approximated by a combination of MetropolisHastings sampling, to infer the path integrals, and naive mean field approximation, to infer
the agent to target assignments. This allowed us to control systems of up to 80 agents
with 80 target states. These results regarding nonlinear dynamics have only an illustrative
purpose.
97

fivan den Broek, Wiegerinck & Kappen

2. Stochastic Optimal Control of a Single Agent
We consider an agent in a k-dimensional continuous state space Rk , its state x(t) evolving
over time according to the controlled stochastic differential equation
dx(t) = b(x(t), t)dt + u(x(t), t)dt + dw(t),

(1)

in accordance with assumptions 1 and 2 in the introduction. The control of the agent is the
Rk -valued function u of x(t) and t. The noise in the dynamics is modeled by the Wiener
process w(t), i.e., a normally distributed k-dimensional stochastic process in continuous
time with mean 0 and variance t, and the k  k matrix  which represents the variance
of the noise. Any autonomous dynamics are modeled by b, which is a Rk -valued function
of x(t) and t. The state change dx(t) is the sum of the noisy control and the autonomous
dynamics.
The behavior of the agent is valued by a cost function. Given the agents state x(t) = x
at the present time t, and a control u, there is an expected future cost for the agent:



Z T
1
d
C u (x, t) = Eux,t (x(T )) +
kRu(x(), )k2 + V (x(), ) .
(2)
2
t
The expectation Eux,t is taken with respect to the probability measure under which x(t)
is the solution to (1) given the control law u and the condition x(t) = x. The cost is a
combination of the end cost (x(T )), which is a function of the end state x(T ), and an
integral of instantaneous costs. The instantaneous cost is a sum of a state and a control
dependent term. The state dependent term V (x(), ) is the cost of being in state x()
at time . The function V is arbitrary, and represents the environment of the agent. The
control dependent term 12 kRu(x(), )k2 is the cost of the control in state x() at time ,
where kzk2 = z  z is the Euclidean norm, and R is a full rank k  k matrix. It is quadratic
in the control, in accordance with assumption 3 in the introduction, and by assumption 4,
R is related to the variance of the noise in the control via the relation
  = (R R)1 ,

(3)

where  is a scalar.
The expected cost-to-go at time t minimized over all controls u defines the optimal
expected cost-to-go
J(x, t) = min C u (x, t).
(4)
u

In Appendix A, it is explained that due to the linear-quadratic form of the optimization
problemthe dynamics (1) is linear in the action u, the cost function (2) is quadratic in
the actionthe minimization can be performed explicitly, yielding a nonlinear partial differential equation in J, the so-called stochastic Hamilton-Jacobi-Bellman (SHJB) equation.
The minimum is attained in
u(x, t) = (R R)1 x J(x, t).

(5)

This is the optimal control. Note that it explicitely depends on the state x of the agent at
time t, making it a feedback control.
98

fiGraphical Model Inference in MAS Optimal Control

The optimal expected cost-to-go can be re-expressed in terms of a diffusion process (for
a derivation, we again refer to Appendix A):
J(x, t) =  log Z(x, t)

(6)

where Z(x, t) is the expectation value



Z
1
1 T
Z(x, t) = Ex,t exp  (y(T )) 
d V (y(), )

 t

(7)

and y() is a diffusion process with y(t) = x and satisfying uncontrolled dynamics:
dy() = b(y(), )d + dw().

(8)

Substituting relations (3) and (6) in (5), we find the optimal control in terms of Z(x, t):
u(x, t) =   x log Z(x, t).

(9)

Example 1. Consider an agent in one dimension with a state x(t) described by the dynamical equation (1) without autonomous dynamics (b = 0). The instantaneous cost V is zero,
and the end cost  is a quadratic function around a target state :
(y) =


|y  |2 .
2

The diffusion process y() that satisfies the uncontrolled dynamics (8) is normally distributed
around the agents state x = y(t) at time t and with a variance  2 (  t), hence the state
transition probability for the agent to go from (x, t) to (y, T ) in space-time is given by the
Gaussian density


|y  x|2
(y, T |x, t) = p
.
exp  2
2 (T  t)
2 2 (T  t)
1

The expectation value (7) is given by the integral

Z(x, t) =

Z

1

(y)

dy(y, T |x, t)e

=

s



R2 /
|x  |2
exp  2
,
T  t + R2 /
2 (T  t + R2 /)

where relation (3) is used. The optimal control follows from (6) and (9) and reads
u(x, t) =

x
.
T  t + R2 /

This result is well known (Stengel, 1993).
99

(10)

fivan den Broek, Wiegerinck & Kappen

2.1 A Path Integral Formulation
Example 1 shows that for a simple system with no autonomous dynamics (b = 0) or costs
due to the environment (V = 0), we can write down the control explicitly. This is because
the uncontrolled dynamics is normally distributed, and consequently the expectation value
(7) with quadratic end cost has a closed form expression. In the general situation where b
and V are arbitrary, there no longer exists an explicit expression for the expectation value,
and the optimal control can only be obtained by approximation. We will now discuss how
this is done by taking a path integral approach (Kleinert, 2006). A detailed derivation of
the expressions presented here is given in Appendix B.
In the path integral approach, we write the expectation value (7) as a path integral:
Z(x, t) = lim Z (x(t0 ), t0 )

(11)

0

where x(t0 ) = x, t0 = t and
1

Z (x(t0 ), t0 ) = p
det(2 2 )N

Z

dx(t1 ) . . .

Z

1

dx(tN ) e  S (x(t0 ),...,x(tN ),t0 ) .

It is an integral over paths (x(t0 ), . . . , x(tN )) in discrete time, the start x(t0 ) kept fixed
and N = T  t, taken in a continuous time limit of sending the length of the time steps
 = ti+1  ti to zero. Note that in this limit N goes to infinity and the paths become infinite
dimensional objects. The function in the exponent is the cost of the path:
S (x(t0 ), . . . , x(tN ), t0 ) =
(x(T )) +

N
1
X

 V (x(ti ), ti ) +

N
1
X
i=0

i=0

 
2

1
x(ti+1 )  x(ti )

 R
 b(x(ti ), ti ) 
 ,
2


The optimal control becomes a weighted average over controls that are derived from a single
path:
u(x(t0 ), t0 ) = lim
0

Z

dx(t1 ) . . .

Z

dx(tN ) p(x(t0 ), . . . , x(tN ), t0 ) u(x(t0 ), . . . , x(tN ), t0 ). (12)

The weights are given by
1

p(x(t0 ), . . . , x(tN ), t0 ) = p

e  S (x(t0 ),...,x(tN ),t0 )
det(2 2 )N Z (x(t0 ), t0 )

.

The control derived from a path (x(t0 ), . . . , x(tN )) reads
u(x(t0 ), . . . , x(tN ), t0 ) =

x(t1 )  x(t0 )
 b(x(t0 ), t0 ).


Note that it only depends on the first two entries x(t0 ) and x(t1 ) in the path.
100

(13)

fiGraphical Model Inference in MAS Optimal Control

2.2 Path Integration by Metropolis-Hastings Sampling
The path integral formulation (12) of the optimal control can generally not be computed,
because it is an integral over uncountably many paths, but there exist several ways to
approximate it. A natural approach goes by stochastic sampling of paths. Several methods
of stochastic sampling exist, the one we will use here is known as Metropolis-Hastings
sampling (Hastings, 1970). In its implementation time will be discretized: we do not take
the limit in (12) of  decreasing to zero, but instead keep  at a fixed value. A sample path
will be a sequence (xs (t0 ), . . . , xs (tN )) of vectors in the state space Rk , with x(t0 ) = x the
current state of the agent at the current time t0 = t. According to equation (13), we only
need xs (t0 ) and xs (t1 ) to derive the control from a sample path (x(t0 ), . . . , x(tN )). The
Metropolis-Hastings sampling ensures that different paths are properly weighted, hence the
optimal control is approximated as follows:
u(x(t0 ), t0 ) 

hx(t1 )i  x(t0 )
 b(x(t0 ), t0 ),
t1  t0

(14)

where hx(t1 )i is the mean value of xs (t1 ) taken over the sample paths. Pseudo-code for the
algorithm is given in Algorithm 1.
Algorithm 1: Metropolis-Hastings sampling
Input: initial path (x(t0 ), . . . , x(tN ))
1: s = 1
2: repeat M times:
3: define Gaussian proposal distribution centered around (x(t1 ), . . . , x(tN ))
with variance equal to the noise
4: draw sample path (x (t1 ), . . . , x (tN )) from proposal distribution

5: a = exp 1 S (x(t0 ), x(t1 ), . . . , x(tN ), t0 )  1 S (x(t0 ), x (t1 ), . . . , x (tN ), t0 )
6: if a  1
7:
set (x(t1 ), . . . , x(tN )) = (x (t1 ), . . . , x (tN ))
8: else
9:
set (x(t1 ), . . . , x(tN )) = (x (t1 ), . . . , x (tN )) with probability a
10: end if
11: (xs (t0 ), . . . , xs (tN )) = (x(t0 ), . . . , x(tN ))
12: s = s + 1
13: end repeat
14: compute approximate control with equation (14)

3. Stochastic Optimal Control of a Multi-Agent System
We now turn to the issue of optimally controlling a multi-agent system of n agents. In
principle, the theory developed for a single agent straightforwardly generalizes to the multiagent situation. Each agent a has a k-dimensional state xa that satisfies a dynamics similar
to (1):
dxa (t) = ba (xa (t), t)dt + ua (x(t), t)dt + a dwa (t),
(15)
in accordance with assumptions 1, 2 and 5 in the introduction. Note that the control of
each agent not only depends on its own state xa , but on the joint state x = (x1 , . . . , xn )
101

fivan den Broek, Wiegerinck & Kappen

of the system. The system has a joint cost function similar to (2), depending on the joint
state x and joint control u = (u1 , . . . , un ) of the system:



n Z T
X
1
u
u
2
d
C (x, t) = Ex,t (x(T )) +
kRa ua (x(), )k + V (xa (), ) .
2
t
a=1

The expectation Eux,t is taken with respect to the probability measure under which x(t) is
the solution to (15) given the control law u and the condition that x(t) = x. The cost is a
combination of the joint end cost (x(T )), which is a function of the joint end state x(T ),
and an integral of instantaneous costs. The instantaneous cost factorizes over the agents,
in accordance with assumption 5 in the introduction. For each agent, it is a sum of a state
dependent term V (xa (), ) and a control dependent term 12 kRa ua (xa (), )k2 , similar to
the single agent case. In accordance with assumption 4 in the introduction, the control cost
of each agent is related to the noise in the agents dynamics via the relation
a a = (Ra Ra )1 ,
where  is the same for each agent. The joint cost function is minimized over the joint
control, yielding the optimal expected cost-to-go J. The optimal expected cost-to-go is
expressed in terms of a diffusion process via the relation
J(x, t) =  log Z(x, t),
where Z(x, t) is the joint expectation value
!#
"
n Z
1X T
1
d V (ya (), )
Z(x, t) = Ex,t exp  (y(T )) 


t

(16)

a=1

and the y1 (t), . . . , yn (t) are diffusion processes, with y = (y1 , . . . , yn ) and y(t) = x, satisfying
uncontrolled dynamics
dya () = ba (ya (), )d + a dwa (),

a = 1, . . . , n.

(17)

The multi-agent equivalent of the optimal control (9) reads
ua (x, t) = a a  xa log Z(x, t).

(18)

We will now show that the optimal control of an agent can be understood as an expected
control, that is, an integral over target states ya of a transition probability to the target
times the optimal control to that target. To this end, we write the expectation (16) as an
integral over the end state:
Z
n
Y
1
Z(x, t) = dye  (y)
Za (ya , T ; xa , t),
(19)
a=1

where the Za (ya , T ; xa , t) are implicitly defined by



Z
Z
1 T
d V (ya (), )
dya Za (ya , T ; xa , t)f (ya ) = Exa ,t f (ya (T )) exp 
 t
102

fiGraphical Model Inference in MAS Optimal Control

for arbitrary functions f . Substituting (19) into (18) yields
Z
ua (x, t) = dya pa (ya |x, t) ua (ya ; xa , t)

(20)

where
ua (ya ; xa , t) = a a  xa log Za (ya , T ; xa , t)

(21)

is the optimal control for agent a to go from state xa at the current time t to state ya at
the end time T , and pa (ya |x, t) is a marginal of
n

Y
1
1
p(y|x, t) =
Za (ya , T ; xa , t).
e  (y)
Z(x, t)
a=1

3.1 Discrete End States
The agents have to fulfill a task of arriving at a number of target states at the end time
according to an initially specified way: for example, they should all arrive at the same
target, or they should all arrive at different targets. The targets are considered regions
G1 , . . . , Gm in the state space, and the end cost  is modeled as follows:
1

e  (y) =

X
s

w(s)

n
Y

wa (ya ; sa ),

1

wa (ya ; sa ) = e  a (ya ;sa ) ,

(22)

a=1

where the sum runs over assignments s = (s1 , . . . , sn ) of agents a to regions Gsa . a (ya ; sa )
is a cost function associated to region Gsa , returning a low cost if the end state ya of agent
a lies in the region Gsa and a high cost otherwise. w(s) is a weight, grading the assignments
s and thereby specifying the joint task of the agents. Assignments that result in a better
fulfillment of the task have a higher weight. In a situation where all agents have to go to
the same target, for example, a vector s that assigns each agent to a different target will
have a low weight w(s).
With this choice of end cost, equation (19) factorizes as
Z(x, t) =

X
s

where
Za (sa ; xa , t) =

Z

w(s)

n
Y

Za (sa ; xa , t)

a=1

dya Za (ya , T ; xa , t)wa (ya ; sa ).

(23)

The interpretation of Za (sa ; xa , t) is that  log Za (sa ; xa , t) is the expected cost for agent
a to move from xa to target sa . The optimal control (20) of a single agent a becomes
ua (x, t) =

m
X

p(sa |x, t)ua (sa ; xa , t),

(24)

sa =1

where
ua (sa ; xa , t) = a a  xa log Za (sa ; xa , t)
103

(25)

fivan den Broek, Wiegerinck & Kappen

is the control for agent a to go to target sa , and the weights p(sa |x, t) are the single-agent
marginals
X
p(s|x, t)
(26)
p(sa |x, t) =
s\sa

of the joint distribution
n

Y
1
p(s|x, t) =
Za (sa ; xa , t).
(27)
w(s)
Z(x, t)
a=1


1
1
The weight p(s|x,
Pnt) equals the ratio exp   J(s; x, t) / exp   J(x, t) , where J(s; x, t) =
 log w(s)  a=1  log Za (sa ; x, t) is the optimal expected cost-to-go in case the agents
have predetermined targets that are specified by the assignment s; an assignment of agents
to targets that has a low expected cost J(s; x, t) will yield a high weight p(s|x, t), and
the associated single-agent to single-target controls ua (sa ; xa , t) will be predominant in the
optimal controls ua (x, t).
3.2 Metropolis-Hastings Sampling in Multi-Agent Systems
In general, both the controls ua (sa ; xa , t) and the marginals p(sa |x, t) in the optimal control (24) do not have a closed form solution, but have to be inferred approximately. The
controls ua (sa ; xa , t) can be approximated by the Metropolis-Hastings sampling discussed
in Section 2.2. Inference of the marginals involves the inference of the path integral formulations of the Za (sa ; xa , t):
Z
Z
1
1
Za (sa ; xa , t) = lim p
dxa (t1 ) . . . dxa (tN )e  S (xa (t0 ),...,xa (tN ),t0 ;sa )
2
N
0
det(2 )
with xa (t0 ) = xa , t0 = t and

S(xa (t0 ), . . . , xa (tN ), t0 ; sa ) = a (xa (T ); sa )
+

N
1
X

 V (xa (ti ), ti ) +

i=0

N
1
X
i=0

 
2

1
xa (ti+1 )  xa (ti )

 Ra
 ba (xa (ti ), ti ) 
 .
2


The value of Za (sa ; xa , t) is generally hard to determine (MacKay, 2003). Possible approximations include the maximum a posteriori (MAP) estimate and the inclusion of the variance
in the sample paths. A third approximation is to take the average of the path costs as an
estimate of log Za (sa ; xa , t); this means that the entropy of the distribution in the path
integral is neglected.

4. Graphical Model Inference
The additional computational effort in multi-agent control compared to single-agent control
lies in the computation of the marginals p(sa |x, t) of the joint distribution p(s|x, t), which
involves a sum over all mn assignments s. For small systems this is feasible, but for large
systems this is only so if the summation can be performed efficiently. An efficient approach
is provided by graphical model inference, which relies on a factor graph representation of
the joint distribution.
104

fiGraphical Model Inference in MAS Optimal Control

1,4

1,2

1

2,4

4

3,4

2

2,3

3

Figure 1: Example of a factor graph for a multi-agent system of four agents. The couplings
are represented by the factors A, with A = {1, 4}, {1, 2}, {2, 4}, {3, 4}, {2, 3}.

4.1 A Factor Graph Representation of the Joint Distribution
The complexity of the joint distribution is in part determined by the weights w(s) in the end
cost function (22). These weights determine how the agents consider the states of the other
agents. In the most complex case, the way one agent takes the state of another agent into
account will depend on the states of all the other agents. The situation is less complicated
when an agent considers the states of some agents independently of the states of the others.
This means that the joint end cost has a factorized form:
Y
w(s) =
wA (sA ),
(28)
A

the A being subsets of agents. This structure is represented graphically by a so-called factor
graph (Kschischang, Frey, & Loeliger, 2001). See Figure 1 for an example. The agents a and
the factors A are nodes in the factor graph, represented by circles and squares respectively,
and there is an edge between an agent a and a factor A when a is a member of subset A,
that is, when wA in the factorization of w depends on sa . From (27) it is immediate that
the joint distribution p(s|x, t) factorizes according to the same factor graph.
4.2 The Junction Tree Algorithm
Efficient inference of the distribution p(s|x, t) by means of its factor graph representation is
accomplished by using the junction tree algorithm (Lauritzen & Spiegelhalter, 1988). The
complexity of this algorithm is exponential in the induced tree width of the graph. A small
tree width can be expected in systems where the factor graph is sparse, which is the case
when the agents take the states into account of a limited number of other agents. This
implies that multi-agent systems with sparse graphs and a limited number of targets are
tractable (Wiegerinck et al., 2006). The factor graph in Figure 1 is an example of a sparse
graph. On the other hand, should each agent take the state of each other agent into account,
then the junction tree algorithm does not really help: the underlying factor graph is fully
connected and the tree width of the graph equals the number of agents in the system.
Exact computation of the optimal control will be intractable in large and complex multiagent systems, since the junction tree algorithm requires memory exponential in the tree
width of the factor graph. Instead we can use graphical model approximate inference
methods to approximately infer the marginals (26). We will proceed with a discussion of
two such methods: naive mean field (MF) approximation (Jordan, Ghahramani, Jaakkola,
& Saul, 1999) and belief propagation (BP) (Kschischang et al., 2001; Yedidia, Freeman, &
Weiss, 2001).
105

fivan den Broek, Wiegerinck & Kappen

4.3 Naive Mean Field Approximation
Our starting point is to note that the optimal expected cost-to-go is a log partition sum,
also known as a free energy. Consider the variational free energy
X
F (q) = h log wiq 
hlog Za iqa  H(q),
a

where h iq and h iqa denote expectation values with respect to distribution q and marginals
qa respectively, and H(q) is the entropy of q:
X
H(q) = 
q(s) log q(s).
s

The optimal expected cost-to-go equals the variational free energy minimized over all distributions q. In the naive mean field approximation
one considers the variational free energy
Q
restricted to factorized distributions q(s) = a qa (sa ). The minimum
F (q)
JMF = min
Q
q=

a qa

is an upper bound for the optimal expected cost-to-go J, it equals J in case the agents are
uncoupled. F has zero gradient in its local minima, that is,
0=

F (q1 (s1 )    qn (sn ))
qa (sa )

a = 1, . . . , n,

(29)

with additional constraints for normalization of the distributions qa . Solutions to this set
of equations are implicitly given by the mean field equations
Za (sa )hw|sa iq
qa (sa ) = Pn


sa =1 Za (sa )hw|sa iq

(30)

where hw|sa iq is the conditional expectation of w under q given sa :

X Y
qa (sa ) w(s1 , . . . , sn ).
hw|sa iq =
s1 ,...,sn \sa

a 6=a

The mean field equations are solved by means of iteration; this procedure results in a
convergence to a local minimum of the free energy.
The mean field approximation of the optimal control is found by taking the gradient
with respect to x of the minimum JMF of the free energy. This is similar to the exact case
where the optimal control is the gradient of the optimal expected cost-to-go, equation (18).
Using (29), we find
X
1
ua (x, t) =  a a  xa JMF (x, t) =
qa (sa )ua (xa , t; sa ).

s
a

Similar to the exact case, it is an average of the single-agent to single-target optimal controls
ua (xa , t; sa ) given by equation (25), where the average is taken with respect to the mean
field approximate marginal qa (sa ) of agent a.
106

fiGraphical Model Inference in MAS Optimal Control

4.4 Belief Propagation
In belief propagation, we approximate the free energy by the Bethe free energy, and we
minimize the latter. The Bethe free energy is defined by
FBethe ({qa , qA }) = 

X

h log wA iqA 

A

X

h log Za iqa  

a

X

H(qA ) + 

A

X

(na  1)H(qa ).

a

(31)
It is a function of beliefs qa (sa ) and qA (sA ), which are non-negative normalized functions
that satisfy consistency relations:
a A  a :

X

qA (sA ) = qa (sa ).

sA\a

The H(qa ) and H(qA ) are the entropies of the beliefs qa and qA , na denotes the number of
neighbors of node a in the factor graph.
Belief propagation is an algorithm that computes the beliefs (Kschischang et al., 2001).
In case the joint distribution p has a factor graph representation that is a tree, belief propagation will converge to beliefs that are the exact marginals of p, and the Bethe free energy
of these beliefs equals the optimal expected cost-to-go J. If the factor graph representation
of p contains cycles, we may still apply belief propagation. Yedidia et al. (2001) showed
that the fixed points of the algorithm correspond to local extrema of the Bethe free energy.
In particular, more advanced variations on the algorithm (Heskes, Albers, & Kappen, 2003;
Teh & Welling, 2001; Yuille, 2002) are guaranteed to converge to local minima of the Bethe
free energy (Heskes, 2003).
We find the BP approximation of the optimal control by taking the gradient of the
minimum JBethe of the Bethe free energy:
X
1
ua (x, t) =  a a  xa JBethe (x, t) =
qa (sa )ua (xa , t; sa ),

s
a

with the ua (xa , t; sa ) given by equation (25). Similar to the exact case and the mean field
approximation, the BP approximation of the optimal control is an average of single-agent
single-target optimal controls, where the average is taken with respect to the belief qa (sa ).

5. Numerical Results
In this section, we present numerical results of simulations of optimal control in multiagent systems. The problem of computing the optimal controls (24) consists of two parts:
the inference of the single-agent to single-target controls (25), and the inference of the
marginals (26) of the global distribution over agent to target assignments. When the dynamics are linear, and the instantaneous costs V are zero, the single-agent to single-target
controls can be given in closed form. Such multi-agent systems therefore only know the issue
of infering marginal distributions. In Section 5.1 we will consider multi-agent systems of
this kind. Section 5.2 deals with the general problem of infering the optimal controls when
the dynamics are nonlinear and the instantaneous costs V are nonzero. In both sections
107

fiExpected Target

van den Broek, Wiegerinck & Kappen

Position

1
0
1
0

0.5

1
Time

1.5

2

1
0
1
0

(a) Positions

0.5

1
Time

1.5

2

(b) Expected Targets

Figure 2: Two agents, with noise and control in their positions, need to reach target locations at -1 and 1 at end time t = 2, each agent at a different target location. The
positions (a) and expected targets (b) over time.

the joint end cost is given by equation (22), with
w(s) =

n
Y
a,b

wa,b (sa , sb ),

 c

wa,b (sa , sb ) = exp  sa ,sb ,
n

(32)




1
(33)
a (ya ; sa ) = |ya  sa |2 ,
wa (ya ; sa ) = exp  a (ya ; sa ) ,

2
where c determines the coupling strength between the agents, and the sa are the target
states.
5.1 Linear Dynamics
We begin with an illustration of optimal control by showing a simulation of an exactly
solvable stochastic multi-agent system. In this system of two agents in one dimension, the
agents satisfy dynamics (15) with ba equal to zero. There are two target states, x = 1 = 1
and x = 2 = 1. The task of the agents is for each one to go to a different target. The
instantaneous costs V in the cost function are zero, and the end cost function is given by
equations (22), (32) and (33) with  = 20 and c = 4. The negative sign of the coupling
strength c implies a repulsion between the agents. The control cost parameter R equals 1,
the noise level  2 lies at 0.5. The agents start at x = 0 at time t = 0, the end time lies at
T = 2. To prevent overshooting the targets, udt should be small compared to the distance
to the target states. This is done by choosing dt = 0.05(T  t + 0.05).
P
Figure 2 shows the agents positions and expected targets
sa =1,2 p(sa |x, t)sa over
time. We see that up to time t = 1, the agents have not decided to which target each of
them will go, and they remain between the two targets. Then, after t = 1, a final decision
seems to have been made. This delayed choice is due to a symmetry breaking in the cost-togo as time increases. Before the symmetry breaking, it is better to keep options open, and
see what the effect of the noise is. After the symmetry breaking, time is too short to wait
longer and a choice has to be made. This phenomenon is typical for multi-modal problems.
We proceed with a quantitative comparison of the different control methods that arise
from the exact or approximate inferences of the marginals of the joint distribution (27).
108

fiGraphical Model Inference in MAS Optimal Control

3

10

4

2

CPU Time

Cost Difference

5
3
2
1
0
1
0

10

1

10

0

10

1

0.2

0.4 0.6
Noise

0.8

10

1

(a) Costs

0

0.2

0.4 0.6
Noise

0.8

1

(b) CPU Time

Figure 3: The deviation from the optimal cost (a) and the required CPU Time in seconds
(b) as functions of the noise. The lines represent exact (  ), Greedy (   ), MF
() and BP () control.

The example we consider is a multi-agent system of n = 18 agents in a two-dimensional
state space with zero instantaneous costs (V = 0) and no autonomous dynamics (ba = 0).
The end cost function is given by equations (22), (32) and (33). The two targets are located
at 1 = (1, 0) and 2 = (1, 0).  = 20 and c = 0.5. The control cost matrix R equals
the identity matrix. The agents start in (0, 0) at time t = 0, the end time lies at T = 2,
and time steps are of size dt = 0.05(T  t + 0.05).
The approximations are naive mean field approximation and belief propagation, as described in Section 4, and greedy control. By greedy control we mean that at each time step
each agent chooses to go to its nearest target. We include this approximation because it is
simple and requires little computation time, and for those reasons it is an obvious choice
for a naive approximation. Because a greedy control policy neglects the choices of the other
agents, we expect that it will give an inferior performance.
For each approximation, Figure 3(a) shows the cost under the approximate (optimal)
control minus the cost under exact (optimal) control, averaged over 100 simulations, and for
different noise levels. The same noise samples were used for the approximate and the exact
control. We see that both naive mean field approximation and belief propagation yield
costs that on average coincide with the cost under exact control: the average cost difference
under both methods does not significantly differ from zero. Greedy control, on the other
hand, yields costs that are significantly higher than the costs under exact control; only in
the deterministic limit does it converge to the cost under exact control, when both controls
coincide. Figure 3(b) shows the CPU time required for the calculation of the controls under
the different control methods. This is the average CPU time of an entire simulation. Each
simulation consists of 73 time steps, and at each time step the control is calculated for each
agent. We observe that greedy control is at least 10 times faster than the other methods,
and exact control is nearly 100 times more time consuming than the other methods. Belief
propagation gives a performance that for all considered noise levels is a bit quicker than the
naive mean field approximation, but this may be the result of implementation details. We
have also done simulations with attractive coupling c = 0.5; this returned results similar to
the ones with repulsive coupling c = 0.5 that we presented here.
109

fiCumulative Control Cost

van den Broek, Wiegerinck & Kappen

20
15
10
5
0
0

0.5

1
Time

1.5

2

Figure 4: The cumulative control cost over time, in case of a strong repulsive coupling
c = 2 and a low noise level  2 = 0.1. The curves represent exact (  ), MF
(), and BP control ().

Although Figure 3 suggests that belief propagation and naive mean field approximation
perform equally well, this is not always the case, since for certain combinations of the noise
level and the coupling strength the BP control is more costly than MF control and exact
control. The origin of this difference lies in the symmetry breaking, which tends to occur
later under BP and earlier under MF when compared to exact control. We observe this
in Figure 4, which shows the cumulative cost over time for the control methods in the
multi-agent system, now with a coupling strength c = 2 and a fixed noise level  2 = 0.1.
The cumulative costs are averages over 100 simulations. The cost under MF control lies a
bit higher than the cost under exact control, whereas the cost under BP control initially
is lower than the cost under the other control methods, but at t = 1.7 it starts to increase
much faster and eventually ends up higher. Including the end costs, we found total costs
26.13  0.12 under exact control, 26.19  0.12 under MF control, and 35.5  0.4 under BP
control. This suggests that it is better to have an early symmetry breaking than a late
symmetry breaking.
The time required for computing the control under the various methods depends on the
number of agents in the multi-agent system. Figure 5 shows the required CPU time as a
function of the number of agents n in the two-dimensional multi-agent system considered
above. We see that the exact method requires a CPU time that increases exponentially
with the number of agents. This is what may be expected from the theory, because the
exact method uses the junction tree algorithm which has a complexity that is exponential
in the tree width of the underlying graph, i.e., exponential in n. For the greedy method,
the CPU time increases linearly with the number of agents, which is in agreement with the
fact that under greedy control there is no coupling between the agents. The required CPU
time increases polynomially for both the mean field approximation and belief propagation.
5.2 Nonlinear Dynamics
We now turn to multi-agent systems with nonlinear dynamics. To control these systems, we
must approximate both the graphical model inference as well as the single-agent to singletarget control problem (12). We consider a multi-agent system in which the agents move in
110

fiGraphical Model Inference in MAS Optimal Control

3

10
CPU Time

2

10

1

10

0

10

1

10

10

15

20
Agents

25

30

Figure 5: The required CPU time in seconds for the calculation of the controls at a different
number of agents. Exact (  ), greedy (  ), MF (), and BP control ().

two dimensions and have a four-dimensional state that is specified by the agents location
(xa , ya ), its forward velocity va , and its driving direction a . The dynamics of each agent
is given by the equations
dxa = va cos a dt
dya = va sin a dt
dva = ua dt + a dwa
da = a dt + a da .
The first two equations model the kinematics of the agents position for a given forward
velocity and driving direction. The last two equations describe the control of the speed and
the driving direction by application of a forward acceleration ua and an angular velocity
a . The noise in the control is modeled by the standard normal Wiener processes wa and
a and the noise level parameters a and a . Note that the noise does not act in dimensions
other than those of the control. Although the control space counts less dimensions than
the state space, the example does fit in the general framework: we refer to Appendix C for
details.
We look at two different tasks. The first task is that of obstacle avoidance in a multiagent system of three agents. The agents each have to reach one of three target locations and
avoid any obstacles in the environment. Each target location should be reached by precisely
one agent; we model this with an end cost function, given by equations (22), (32) and (33),
with  =  and c = 0.5. The targets are located at (10, 15), (45, 12) and (26, 45), and the
agents should arrive with zero velocity. The control cost matrix R is the identity matrix.
 = 0.1. The instantaneous cost V equaled 1000 at the locations of the obstacles, and zero
otherwise. The agents start at time t = 0, the end time lies at T = 20, and time steps dt
are of size 0.2. The starting locations of the agents are (18, 31), (25, 12) and (39, 33), and
the agents start with zero velocity. The sample paths are discrete time paths in the twodimensional space of the forward velocity v and the driving direction . They are specified
by their values at times ti = t + i , i = 0, . . . , N  1, with  = NT t
1 and N = 7, the value
at time t0 equals the current state of one of the agents, and the value at time tN equals
one of the target end states. The control for each agent to one of the targets is computed
111

fivan den Broek, Wiegerinck & Kappen

50

50

40

40

30

30

20

20

10

10

0

0

10

20

30

40

50

0

(a) Trajectories

0

10

20

30

40

50

(b) Sample paths

Figure 6: Three agents, with noise and control in their forward velocities and driving directions, have to reach three targets (marked by X) in an environment containing
also a number of walls. Each agent starts at a different location (marked by O)
and with zero forward velocity, and each agent should arrive at a different target
with zero velocity without hitting the walls. (a) The trajectories that the agents
followed to reach the targets. (b) Sample paths.

with a Metropolis-Hastings sampling of paths, according to Subsection 3.2. The proposal
distribution is a 2N -dimensional Gaussian, centered around the agents current planned
path, and with a variance equal to the noise level in the agents dynamics. The expectation
values Za (sa ; xa , t) are estimated by the average costs of the sample paths. We have also
tried MAP estimation of Za (sa ; xa , t) and an inclusion of the variance in the sample paths,
but the former did not show a significant difference, and the latter returned estimates that
fluctuated heavily. Figure 6(a) shows the environment and the trajectories of the agents
from their starting locations to the targets. Each agent manages to avoid the obstacles and
arrive at one of the targets with zero velocity, such that each target is reached by a different
agent.
The second task is that of coordination in the multi-agent system as shown in Figure 7(a). In this system there are no instantaneous costs (V = 0). The agents have to move
from their initial positions to a number of target locations. They should arrive at these
locations with zero velocity and horizontal driving direction. There is an equal number
of agents and target locations, and each agent has to reach a different target. The initial
locations are aligned vertically, and so are the target locations, but there is a vertical displacement between the two. Thus the agents have to coordinate their movements in order
to reach the targets in a satisfactory way.
The agents start at time 0, the end time lies at 100, and they make time steps of size
T t
dt = 2(N
1) , with N = 7, until dt < 0.01. At each time step the controls are computed by
a Metropolis-Hastings sampling of paths and a naive mean field approximation to infer the
marginals pa (sa |x, t) that weigh the single-agent to single-target controls, equations (24)
and (26). The sample paths were discretized into seven equidistant time points from the
present time to the end time. The proposal distribution was taken a Gaussian, which was
112

fiGraphical Model Inference in MAS Optimal Control

centered around the agents current planned path and with a variance equal to the noise
level in the agents dynamics. Figure 7(a) shows an example of the trajectories of a system
of 10 agents. It was obtained with 10 sample paths per agent-target combination. We
observe that the agents reach the targets, and that each target is reached by precisely one
agent, as required. Due to the noise in the second order dynamics of the agents, it takes
the agents less effort to approach a target than to remain there, since the former allows
exploitation of the noise while the latter requires a constant correction of the state changes
caused by the noise. The result is that the trajectories of the agents are more curved and
elongated than what would be expected in the situation without noise. The simulation was
carried out as well for a larger number of agents. Figure 7(b) shows the required CPU time
as a function of the number of agents, both under exact and MF inference of the marginals
of the agents. Note that the complexity of the graphical model inference problem scales as
nn , with n the number of agents. Exact inference using the junction tree algorithm was
only feasible for n < 10.

6. Discussion
We studied the use of graphical model inference methods in optimal control of stochastic
multi-agent systems in continuous space and time where the agents have a joint task to
reach a number of target states. Rather than discretizing, as is commonly done and typically
makes large systems intractable due to the curse of dimensionality, we followed the approach
developed by Wiegerinck et al. (2006), modeling the system in continuous space and time.
Under certain assumptions on the dynamics and the cost function, the solution can be given
in terms of a path integral.
The path integral can be computed in closed form in a few special cases, such as the
linear-quadratic case, but in general it has to be approximated. This can be done by a
variety of methods. The method we considered in this paper is MCMC sampling. The
dimension of the sample paths was kept low (N = 7) to limit the curvature of the sample
paths. The gain of limiting the curvature is that the variance in the samples is reduced
and less samples are needed. By limiting the curvature, however, we introduce a bias. In
addition, in the presence of obstacles insufficient curvature would make the sampler return
sample paths that run through the obstacles. We believe that more advanced MCMC
methods such as Hybrid MC sampling (Duane, Kennedy, Pendleton, & Roweth, 1987) and
overrelaxation (Neal, 1998) can improve the inference of the path integrals.
Apart from MCMC sampling, there are other approximation methods that one could
consider, such as the Laplace approximation or a variational approximation. The Laplace
approximation becomes exact in the noiseless limit and could be useful in low noise regimes
as well. The variational approximation approximates the path integral (11) by a Gaussian
process (Archambeau, Opper, Shen, Cornford, & Shawe-Taylor, 2007), and could be particularly useful in the high noise regime. A drawback of the variational approach, however, is
that it cannot be straightforwardly applied to situations with infinite instantaneous costs,
like hard obstacles in the environment that we considered here.
Wiegerinck et al. (2006) showed that for systems that are sufficiently sparse and in which
the single-agent to single-target controls can be determined in closed form, e.g. linearquadratic control with time-independent coefficients, exact inference can be achieved using
113

fivan den Broek, Wiegerinck & Kappen

50

40

30

20

10

0

10

20

30

40

50
20

15

10

5

0

5

10

15

(a) Trajectories
5

10

4

CPU time

10

3

10

2

10

1

10

0

20

40
60
Number of Agents

80

(b) CPU time

Figure 7: (a) The trajectories of 10 agents from starting locations O to 10 targets X. (b)
The required CPU time in seconds as a function of the number of agents, with
the number of targets equal to the number of agents. The lines represent exact
(  ) and MF () inference of the marginals.

114

fiGraphical Model Inference in MAS Optimal Control

the junction tree algorithm. Van den Broek, Wiegerinck, and Kappen (2007) considered
a multi-agent system with second-order dynamics, linear autonomous dynamics and zero
instantaneous costs, and showed that graphical model inference by naive mean field approximation significantly outperformed a greedy inference. Here we showed that a close to
optimal result can be achieved as well in dense systems, using graphical model approximate
inference methods. The approximation methods that we considered were naive mean field
approximation and belief propagation. We demonstrated their performances in an example
system where exact inference is significantly more time consuming. Mean field approximation showed to work very well, returning costs for control equal to the optimal ones, belief
propagation performed similarly. Below a certain value for the ratio of coupling strength
to the noise level, the symmetry breaking in the control process takes place earlier under
mean field approximation when compared to exact inference, and later under belief propagation. An early symmetry breaking does not increase the costs for coordination much,
however, a late symmetry breaking does, making the performance under belief propagation
suboptimal.
Some variations on the considered case are also possible within the general framework.
Wiegerinck, van den Broek, and Kappen (2007) discuss situations where agents sequentially
visit a number of targets, and where the end time is not fixed. It focusses on prefered
trajectories in state space over time, instead of prefered states at the end time; this is
achieved by modeling the path cost in a way similar to how we have modeled the end cost.
The problem where agents have to intercept a moving target with noisy dynamics is also
covered there.
The control formalism developed by Kappen (2005a, 2005b) and applied to multi-agent
coordination by Wiegerinck et al. (2006) and in this article, demands that the noise and
the control act in the same dimensions. One way to satisfy this constraint is to assume
that the agents are identical. In addition, the single agent dynamics should be such that
the noise and the control act in the same dimensions. We saw that for the two-dimensional
second order system in Section 5.2 this condition was satisfied in a natural way. However,
in general one can think of examples of control problems where equation (3) is violated. An
interesting future direction of research is to investigate to what extend the path integral
approach can be used as an approximation in such cases.
The paper assumes that the joint state space of the agents is observable to all agents.
For large multi-agent systems, however, it will be more realistic that an agent only observes
its own state and the states of agents that are physically nearby. Our approach does not
directly apply to such situations. Depending on the joint task of the agents, it may be
a valid approximation to do optimal control in the sub-system consisting of those agents
that one agent does observe. If the task of the agents is to avoid collisions, then it will be
sufficient to consider only the states of agents that are nearby, but if the task is to all go
to the same target then it will be crucial to have information about the states of all other
agents. A natural alternative to deal with partial observability is to describe the multi-agent
system by a decentralized POMDP (Seuken & Zilberstein, 2008). It is not clear however,
how such an approach would combine with the path integral formalism.
The topic of learning has not been addressed in this paper, but clearly is of great
interest. However, one could argue that a sampling procedure to compute the path integral
115

fivan den Broek, Wiegerinck & Kappen

corresponds to a learning of the environment. A discussion on this line of thought can be
found in (Kappen, 2007).
There are many more possible model extensions worthwhile exploring in future research.
Obvious examples are bounded controls, or a limited observation of the global state of the
system; these issues are already of interest to study in the single agent situation. Others
apply typically to the multi-agent situation. In the context of physical agents, introducing penalties for collisions between agents would become relevant. Typically, these types
of model extensions will not have a solution in closed form, and will require additional
approximate numerical methods. Some suggestions are given by Kappen (2005a, 2005b).
Acknowledgments
We thank the reviewers for their useful comments. We thank Joris Mooij for making
available useful software (www.mbfys.ru.nl/~jorism/libDAI/). This research is part of
the Interactive Collaborative Information Systems (ICIS) project, supported by the Dutch
Ministry of Economic Affairs, grant BSIK03024.

Appendix A. Stochastic Optimal Control
In this appendix we give a derivation of (5), (6) and (7), starting from (1), (2), (3) and
(4). Detailed discussions can be found in many works on stochastic optimal control, for
example that of Kushner (1967), Fleming and Rishel (1975), Fleming (1978), ksendal
(1998), Stengel (1993), and Kappen (2005a, 2005b).
The optimal expected cost-to-go J in a state x at time t is defined as
J(x, t) = min C u (x, t),
u

where
u

C (x, t) =

Eux,t


Z
(x(T )) +

t

T

d




1
2
kRu(x(), )k + V (x(), )
2

(34)

(35)

is the expected cost given the control law u. These are the equations (4) and (2) in the
main text. We first show that J satisfies the stochastic Hamilton-Jacobi-Bellman (SHJB)
equation


1
1   2 
2

t J = min
kRuk + (b + u) x J + Tr  x J + V ,
(36)
u
2
2

with boundary condition J(x, T ) = (x). This equation is derived in the following way. For
any moment in time  between t and T it holds that


Z  
1
2
u
ds
kRu(x(s), s)k + V (x(s), s)
J(x, t) =
C (x(), ) +
2
t


Z  
1
2
u
kRu(x(s), s)k + V (x(s), s) .
ds
= min Ex,t J(x(), ) +
u
2
t
min Eux,t
u

The first line follows from dividing the integral from t to T into two integrals, one from t
to  and one from  to T , and using the definition of the cost function C, the second line
116

fiGraphical Model Inference in MAS Optimal Control

follows from the definition of J. A rewriting yields


Z  
J(x(), )  J(x, t)
1
1
u
2
0 = min Ex,t
ds
+
kRu(x(s), s)k + V (x(s), s) .
u
t
t t
2
Taking the limit   t we obtain


dJ(x(t), t) 1
2
u
+ kRu(x(t), t)k + V (x(t), t) .
0 = min Ex,t
u
dt
2

(37)

Subsequently, we apply to dJ(x(t), t) the well known chain rule for diffusion processes:
dJ(x(t), t) =

X J(x(t), t)
i

xi

dxi (t) +

J(x(t), t)
1 X  2 J(x(t), t)
dt +
dxi (t)dxj (t). (38)
t
2
xi xj
i,j

It differs from the chain rule for deterministic processes in that it also contains a term
quadratic in dx. This extra term does not vanish, because the Wiener process appearing in
the dynamics (1) has quadratic variation that increases linear in time:
Eux,t [dwi (t)dwj (t)] = ij dt.

(39)

It follows that in expectation dxi (t)dxj (t) is equal to (  )ij dt. By substituting the dynamics (1) in (38), taking expectation values, and using (39), we obtain


2
J(x, t)
 J(x, t)
  J(x, t)
u
dt + (b(x, t) + u(x, t))
dt + Tr 
dt.
Ex,t [dJ(x(t), t)] =
t
x
xx
Substitution into equation (37) then yields equation (36).
The minimum of the right-hand side of equation (36) is given by
u = (R R)1 x J.
This is the optimal control.
The minimization in (36) is removed by inserting the optimal control. This yields a
nonlinear equation for J. We can remove the nonlinearity by using a logarithmic transformation: if we introduce a constant , and define Z(x, t) through J(x, t) =  log Z(x, t),
then
1  
1
u R Ru + u x J =  2 Z 2 (x Z) (R R)1 x Z,
2
2


1 2
1
1   2 
Tr  x J
=
Z (x Z)   x Z  Z 1 Tr   x2 Z .
2
2
2

The terms quadratic in x Z vanish when   and R are related via equation (3),
  = (R R)1 .
When this relation is satisfied, the SHJB equation becomes



V
1

 2
t Z =
 b x  Tr  x Z

2
= HZ,
117

(40)

fivan den Broek, Wiegerinck & Kappen

where H a linear operator acting on the function Z.
Equation (40) must be solved backwards in time with boundary condition Z(x, T ) =
1

e (x) . We present a solution in terms of a forward diffusion process. It is a common approach in the theory of stochastic processes to give solutions to partial differential equations
in terms of diffusion processes. The solution to equation (40) is the expectation value



Z
1
1 T
Z(x, t) = Ex,t exp  (y(T )) 
d V (y(), ) ,

 t

(41)

where y() is a process that satisfies the uncontrolled dynamics
dy() = b(y(), )d + dw(),
and y(t) = x. The expectation Ex,t is taken with respect to the probability measure under
which y() satisfies the uncontrolled dynamics with condition y(t) = x. It is clear that (41)
matches the boundary condition. To verify that it satisfies equation (40), we let


Z
1 T
d V (y(), ) .
I(t) = exp 
 t
We see that

1
V (y(t), t)I(t)dt.


Let f be the function f (y) = exp  1 (y) . We again use the chain rule for stochastic
processes and apply it to f (y(T  )) to find
dI(t) =

k
k
X
f (y(T  ))
1 X  2 f (y(T  ))
dyi (T  ) +
dyi (T  )dyj (T  )
df (y(T  )) =
yi
2
yi yj
i=1
i,j=1


f (y(T  )) 
(b(y(T  ), T  )d + dw(T  ))
=
y


2
1
  f (y(T  ))
 Tr 
d.
2
yy

We then choose  = 0 and d = dt and combine this identity with the previous one to
obtain
df (y(T ))I(t) = f (y(T ))dI(t) + I(t)df (y(T ))
= Hf (y(T ))I(t)dt + y f (y(T ))I(t)dw(T ).
Taking the expectation value on both sides makes the term y f (y(T ))I(t)dw(T ) disappear,
and the remaining part,
dE [f (y(T ))I(t)] = HE [f (y(T ))I(t)] dt,
is just equation (40).
118

fiGraphical Model Inference in MAS Optimal Control

Appendix B. The Path Integral Formulation
We are going to write the expectation value (7) as a path integral. Partitioning the time
interval from t to T into N intervals of equal length , t = t0 < t1 < . . . < tN = T , the
expectation value can be written as follows:
Z(x, t) =

Z

dx1 . . .

Z

1

dxN e  (xN )

N
1
Y

Z(xi+1 , ti+1 ; xi , ti )

(42)

i=0

where x0 = x and the Z(xi+1 , ti+1 ; xi , ti ) are implicitly defined by



fi
Z
Z
fi
1 ti+1
fi
dxi+1 Z(xi+1 , ti+1 ; xi , ti )f (xi+1 ) = E f (xi+1 ) exp 
d V (y(), ) fiy(ti ) = xi
 ti

for arbitrary functions f . In the limit of infinitesimal , the Z(xi+1 , ti+1 ; xi , ti ) satisfy


1
Z(xi+1 , ti+1 ; xi , ti ) = (xi+1 , ti+1 |xi , ti ) exp  V (xi , ti ) ,
(43)


where (xi+1 , ti+1 |xi , ti ) is the transition probability of the uncontrolled dynamics (8) to go
from (xi , ti ) to (xi+1 , ti+1 ) in space-time. The transition probability is given by


1
k 1 (xi+1  xi  b(xi , ti ))k2
.
(xi+1 , ti+1 |xi , ti ) = p
exp 
2
det(2 2 )
This follows from the dynamics

xi+1  xi = b(xi , ti ) + w
over the infinitesimal time interval and the observation that the Wiener process w is normally distributed around zero with variance . Using equation (3), we may rewrite the
transition probability as
 
2 !

1 
1
x

x
i+1
i
exp  
(xi+1 , ti+1 |xi , ti ) = p
R
 b(xi , ti ) 
(44)

  .
2
2

det(2 )

We obtain the path integral representation of Z(x, t) by combining equations (42), (43)
and (44) in the limit of  going to zero:
Z(x, t) = lim Z (x0 , t0 )

(45)

0

with x0 = x, t0 = t,
1

and

Z (x0 , t0 ) = p
det(2 2 )N

S (x0 , . . . , xN , t0 ) = (xN ) +

N
1
X

Z

dx1 . . .

 V (xi , ti ) +

Z

N
1
X
i=0

i=0

119

1

dxN e  S (x0 ,...,xN ,t0 )

 
2

xi+1  xi
1

 b(xi , ti ) 
 R
 .
2


fivan den Broek, Wiegerinck & Kappen

The optimal control is given by equation (9) and is proportional to the gradient of
log Z(x, t). Substituting the path integral representation (45) of Z(x, t), we find that
u(x0 , t0 ) = lim
0

= lim
0

Z

Z

dx1 . . .
dx1 . . .

Z

Z

1

e  S (x0 ,...,xN ,t0 )

dxN p
 x0
det(2 2 )N Z (x, t0 )




1
 S (x0 , . . . , xN , t0 )


dxN p(x0 , . . . , xN , t0 )u(x0 , . . . , xN , t0 )

where
u(x0 , . . . , xN , t0 ) =
and



x1  x0
 b(x0 , t0 )

1

e  S (x0 ,...,xN ,t0 )
p(x0 , . . . , xN , t0 ) = p
.
det(2 2 )N Z (x0 , t0 )

Note that the control u(x0 , . . . , xN , t0 ) that results from a path (x0 , . . . , xN ) only depends
on the first two entries x0 and x1 of the path.

Appendix C. Dimension Reduction
The derivation of the path integral in Appendix B was given for the case that both the
state and the control are k-dimensional. The particular case that only some dimensions of
the state are controlled can be deduced by taking the limit of infinite control cost along the
dimensions without control. The control along the latter dimensions then becomes zero, as
can be seen from equation (5). The noise in these dimensions is equal to zero in accordance
with relation (3). In the path integral formalism the transition probabilities (44) then
reduce to delta functions along the dimensions without control. The implications for the
MCMC sampling are that the dimension of the space in which to sample is also reduced,
since sampling has only to be performed in the dimensions where there is noise.

References
Archambeau, C., Opper, M., Shen, Y., Cornford, D., & Shawe-Taylor, J. (2007). Variational inference
for diffusion processes. In Advances in Neural Information Processing Systems.
Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. V. (2003). Transition-independent decentralized Markov decision processes. In Proceedings of the Second International Joint Conference
on Autonomous Agents and Multiagent Systems, pp. 4148.
Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. V. (2004). Solving transition independent
decentralized Markov decision processes. Journal of Artificial Intelligence Research, 22, 423
455.
Boutilier, C. (1996). Planning, learning and coordination in multiagent decision processes. In
Proceedings of the Sixth Conference on Theoretical Aspects of Rationality and Knowledge, pp.
195210.
Castanon, D. A., Pachter, M., & Chandler, P. R. (2004). A game of deception. In Proceedings of
the 43rd IEEE Conference on Decision and Control, pp. 33643369.
Duane, S., Kennedy, A., Pendleton, B., & Roweth, D. (1987). Hybrid Monte Carlo. Physics Letters
B, 195 (2), 216222.
120

fiGraphical Model Inference in MAS Optimal Control

Fleming, W. H. (1978). Exit probabilities and optimal stochastic control. Applied Mathematics and
Optimization, 4, 329346.
Fleming, W. H., & Rishel, R. W. (1975). Deterministic and Stochastic Optimal Control. SpringerVerlag, New York.
Guestrin, C., Koller, D., & Parr, R. (2002a). Multiagent planning with factored MDPs. In Advances
in Neural Information Processing Systems, Vol. 14, pp. 15231530.
Guestrin, C., Venkataraman, S., & Koller, D. (2002b). Context-specific multiagent coordination and
planning with factored MDPs. In Eighteenth National Conference on Artificial Intelligence,
pp. 253259.
Hastings, W. (1970). Monte Carlo sampling methods using Markov chains and their applications.
Biometrika, 57 (1), 97109.
Heskes, T. (2003). Stable fixed points of loopy belief propagation are minima of the Bethe free
energy. In Advances in Neural Information Processing Systems, Vol. 15, pp. 343350.
Heskes, T., Albers, K., & Kappen, B. (2003). Approximate inference and constrained optimization.
In Proceedings of the 19th Conference on Uncertainty in Artificial Intelligence, pp. 313320.
Hu, J., Prandini, M., & Tomlin, C. (2007). Conjugate points in formation constrained optimal
multi-agent coordination: A case study. SIAM Journal on Control and Optimization, 45 (6),
21192137.
Jordan, M., Ghahramani, Z., Jaakkola, T., & Saul, L. (1999). An introduction to variational methods
for graphical models. In Learning in Graphical Models. MIT Press, Cambridge.
Kamal, W. A., Gu, D.-W., & Postlethwaite, I. (2005). Real time trajectory planning for UAVs using
MILP. In Proceedings of the 4th IEEE Conference on Decision and Control, and the European
Control Conference 2005, pp. 33813386.
Kappen, H. J. (2005a). Path integrals and symmetry breaking for optimal control theory. Journal
of statistical mechanics: theory and experiment, P11011.
Kappen, H. J. (2005b). Linear theory for control of nonlinear stochastic systems. Physical Review
Letters, 95 (20), 200201.
Kappen, H. J. (2007). An introduction to stochastic control theory, path integrals and reinforcement
learning. In AIP conference proceedings, Vol. 887, pp. 149181.
Kleinert, H. (2006). Path Integrals in Quantum Mechanics, Statistics, Polymer Physics, and Financial Markets. World Scientific, Singapore.
Kschischang, F. R., Frey, B. J., & Loeliger, H.-A. (2001). Factor graphs and the sum-product
algorithm. IEEE Transactions on Information Theory, 47 (2), 498519.
Kushner, H. J. (1967). Stochastic Stability and Control. Academic Press Inc., New York.
Larson, R. A., Pachter, M., & Mears, M. (2005). Path planning by unmanned air vehicles for
engaging an integrated radar network. In Proceedings of the AIAA Guidance, Navigation, and
Control Conference and Exhibit.
Lauritzen, S., & Spiegelhalter, D. (1988). Local computations with probabilities on graphical structures and their application to expert systems (with discussion). J. Royal Statistical Society
Series B, 50, 157224.
Liu, Y., Cruz, J. B., & Schumacher, C. J. (2007). Pop-up threat models for persistent area denial.
IEEE Transactions on Aerospace and Electronic Systems, 43 (2), 509521.
MacKay, D. J. (2003). Information Theory, Inference, and Learning Algorithms. Cambridge University Press.
Neal, R. M. (1998). Learning in Graphical Models, pp. 205225. Kluwer Academic Publishers.
121

fivan den Broek, Wiegerinck & Kappen

ksendal, B. (1998). Stochastic Differential Equations: An Introduction with Applications. SpringerVerlag.
Pachter, L., & Pachter, M. (2001). Optimal paths for avoiding a radiating source. In Proceedings of
the 40th IEEE Conference on Decision and Control, pp. 35813586.
Ribichini, G., & Frazzoli, E. (2003). Efficient coordination of multiple-aircraft systems. In Proceedings
of the 42nd IEEE Conference on Decision and Control, Vol. 1, pp. 10351040.
Sadati, N., & Elhamifar, E. (2006). Semi-decentralized control of multi-agent systems based on
redundant manipulator optimization methods. In Proceedings of the 9th IEEE International
Workshop on Advanced Motion Control, pp. 278283.
Seuken, S., & Zilberstein, S. (2008). Formal models and algorithms for decentralized decision making
under uncertainty. Journal of Autonomous Agents and Multi-Agent Systems.
Shi, X., Wang, X., Liu, Y., Wang, C., & Zu, C. (2007). Optimization of fighter aircraft evasive trajectories for radar threats avoidance. In Proceedings of the 2007 IEEE International Conference
on Control and Automation, pp. 303307.
Stengel, R. (1993). Optimal Control and Estimation. Dover Publications, New York.
Subramanian, S. K., & Cruz, J. B. (2003). Adaptive models of pop-up threats for multi-agent
persistent area denial. In Proceedings of the 42nd IEEE Conference on Decision and Control,
pp. 510515.
Teh, Y., & Welling, M. (2001). The unified propagation and scaling algorithm. In Advances in
Neural Information Processing Systems, Vol. 14, pp. 953960.
Todorov, E., & Li, W. (2005). A generalized iterative LQG method for locally-optimal feedback
control of constrained nonlinear stochastic systems. In Proceedings of the American Control
Conference, pp. 300306.
Tomlin, C., Pappas, G. J., & Sastry, S. (1998). Conflict resolution for air traffic management: A study
in multiagent hybrid systems. IEEE Transactions on Automatic Control, 43 (4), 509521.
van den Broek, B., Wiegerinck, W., & Kappen, B. (2007). Optimal control in large stochastic multiagent systems. In Proceedings of the Seventh Symposium on Adaptive Learning Agents and
Multi-Agent Systems, pp. 920.
van Leeuwen, P., Hesseling, H., & Rohling, J. (2002). Scheduling aircraft using constraint satisfaction.
Electronic Notes in Theoretical Computer Science, 76, 252268.
Wiegerinck, W., van den Broek, B., & Kappen, B. (2006). Stochastic optimal control in continuous
space-time multi-agent systems. In Proceedings of the 22nd Conference on Uncertainty in
Artificial Intelligence, pp. 528535.
Wiegerinck, W., van den Broek, B., & Kappen, B. (2007). Optimal on-line scheduling in stochastic
multi-agent systems in continuous space-time. In Proceedings of the Sixth International Joint
Conference on Autonomous Agents and Multiagent Systems, pp. 744751.
Yedidia, J., Freeman, W., & Weiss, Y. (2001). Generalized belief propagation. In Advances in Neural
Information Processing Systems, Vol. 13, pp. 689695.
Yuille, A. (2002). CCCP algorithms to minimize the Bethe and Kikuchi free energies: Convergent
alternatives to belief propagation. Neural Computation, 14 (7), 16911722.

122

fiJournal of Artificial Intelligence Research 32 (2008) 939-982

Submitted 01/31; published 08/08

Optimal Strategies for Bidding Agents Participating in
Simultaneous Vickrey Auctions with Perfect Substitutes
Enrico H. Gerding
Rajdeep K. Dash
Andrew Byde
Nicholas R. Jennings

eg@ecs.soton.ac.uk
rkd@ecs.soton.ac.uk
ab06v@ecs.soton.ac.uk
nrj@ecs.soton.ac.uk

Intelligence, Agents, Multimedia Group
School of Electronics and Computer Science
University of Southampton, Southampton, UK

Abstract
We derive optimal strategies for a bidding agent that participates in multiple, simultaneous
second-price auctions with perfect substitutes. We prove that, if everyone else bids locally
in a single auction, the global bidder should always place non-zero bids in all available
auctions, provided there are no budget constraints. With a budget, however, the optimal
strategy is to bid locally if this budget is equal or less than the valuation. Furthermore, for
a wide range of valuation distributions, we prove that the problem of finding the optimal
bids reduces to two dimensions if all auctions are identical. Finally, we address markets
with both sequential and simultaneous auctions, non-identical auctions, and the allocative
efficiency of the market.

1. Introduction
In recent years, there has been a surge in the application of auctions, both online and
within multi-agent systems (Wellman, Greenwald, & Stone, 2007; Clearwater, 1996; Gerding, Rogers, Dash, & Jennings, 2007b; Rogers, David, & Jennings, 2005; Rosenthal & Wang,
1996; Roth & Ockenfels, 2002; Dash, Parkes, & Jennings, 2003). As a result, there are an
increasing number of auctions offering very similar or even identical goods and services.1 To
take advantage of this fact, automated support needs to be developed that can monitor, bid
in, and make decisions across this large set of possibilities. Such software, in the form of
intelligent bidding agents (hereafter shortened to bidder), can increase the likelihood of winning an item and at a lower price, and thus result in a considerable advantage for a buyer.
Now, whereas participating in many auctions is an arduous task when done manually, such
a problem is ideally suited for autonomous agents who can execute the proper actions on
a buyers behalf (Stone, Schapire, Littman, Csirik, & McAllester, 2003). To this end, in
this paper we devise and analyse optimal bidding strategies for one such auction setting
 namely, a bidder that participates in multiple, simultaneous second-price auctions for
goods that are perfect substitutes. As we will show, however, this analysis also applies to a
wider context with markets consisting of both sequential (i.e., where the auctions close one
1. In eBay alone, for example, there are often hundreds or sometimes even thousands of concurrent auctions
running worldwide selling such substitutable items. To illustrate, at the time of writing, over 1600 eBay
auctions were selling the Apple iPhone worldwide.
c
2008
AI Access Foundation. All rights reserved.

fiGerding, Dash, Byde & Jennings

after the other), as well as simultaneous (i.e., where the auctions close at the same time)
auctions.
To date, much of the existing literature on multiple auctions focuses either on sequential
auctions (Krishna, 2002) or on simultaneous auctions with complementarities, where the
value of items together is greater than the sum of the individual items (see Section 2 for
related research on simultaneous auctions). In contrast, here we consider bidding strategies
for the case of simultaneous auctions and perfect substitutes. In particular, our focus is
on Vickrey or second-price sealed bid auctions. We choose these because they have a low
communication overhead (in terms of the number of required interactions) and are well
known for their capacity to induce truthful bidding. As a result, this type of auction
and its generalisations, such as the Vickrey-Clarke-Groves mechanism, have been used in a
number of multi-agent system settings (Dash, Rogers, Reece, Roberts, & Jennings, 2005;
Varian, 1995; Mes, van der Heijden, & van Harten, 2007; Dash, Vytelingum, Rogers, David,
& Jennings, 2007). Moreover, these auctions are (weakly) strategically equivalent to the
widely used English auctions.2 However, we find that, when there are multiple such auctions
running simultaneously, truthful bidding is no longer optimal. Given this, we characterise,
for the first time, a bidding agents utility-maximising strategy for bidding in any number
of such auctions and for any type of bidder valuation distribution.
In more detail, we consider a market where a single bidder, called the global bidder,
can bid in any number of auctions, whereas the other bidders, called the local bidders, are
assumed to bid in only a single auction. We distinguish between two types of settings for
this market: one in which the auctions are identical and the global bidder is indifferent
between the auctions, and one in which the global bidder prefers some auctions over others.
For these settings our main results are as follows:
 Whereas in the case of a single second-price auction a bidder has a weakly dominant
strategy to bid its true value, this is no longer the case when there are several simultaneous
auctions. The best strategy for a global bidder is then to bid below its true value.
 We prove that, even if a global bidder requires only one item and assuming free disposal,
the expected utility is maximised by participating (i.e., bidding a non-zero amount) in
all the auctions that are selling the desired item.
 Finding the optimal bid for each auction can be an arduous task when considering all
possible combinations. However, when the global bidder is indifferent between the auctions, we are able to significantly reduce the search space for common bidder valuation
distributions. As a result, optimal bids can be efficiently calculated for any number of
auctions. Although the setting where the global bidder has preferences over auctions is
more involved, we can still apply analytical methods to obtain tractable optimal results.
 We prove that, if the auctions are identical, the bidders expected utility is maximised
by bidding either uniformly across all auctions, or relatively high in one of the auctions,
and the same, low value in all others (which of these two behaviours is optimal depends
on the bidder valuation and market conditions such as the number of other bidders). If
2. More specifically, Vickrey and English auction are strategically equivalent assuming private valuations
of the good, i.e., where a bidders value for the item remains unchanged during the bidding process.

940

fiOptimal Strategies for Simultaneous Vickrey Auctions with Perfect Substitutes

this is not the case and a global bidder has different preferences over the auctions, we
show that it is optimal to bid relatively higher in auctions which are preferred over other
auctions.
 We argue that, even though a global bidder has a significantly higher expected utility than
a local one, not all bidders should necessarily bid globally. For example, if bidders have
budget considerations which constrain the amount they can bid, we show analytically
that, if the budget is equal or less than the valuation, it is optimal to bid in a single
auction under certain conditions.
Finally, we consider the issue of market efficiency when there are such simultaneous
auctions. Efficiency is an important system-wide consideration within the area of multiagent systems since it characterises how well the allocations in the system maximise the
overall utility (Dash et al., 2003). Now, efficiency is maximised when the goods are allocated
to those who value them the most. However, a certain amount of inefficiency is inherent
to a distributed market where the auctions are held separately. Given this, in this paper,
we measure the efficiency of markets with local bidders only and consider the impact of
global bidders on this inefficiency. In so doing, we find that the presence of a global bidder
generally has a positive impact on the efficiency.
The remainder of the paper is structured as follows. We first discuss related work in
Section 2. In Section 3 we describe the bidders and the auctions in more detail. In Section 4
we characterise the optimal bidding behaviour for the base setting where all auctions are
identical, and in Section 5 we explore a number of extensions: non-identical auctions, budget
constraints and sequential auctions. In Section 6 we address the market efficiency and the
impact of a global bidder. Finally, Section 7 concludes. Most of the proofs are placed in
the appendix.

2. Related Work
Research in the area of simultaneous auctions can be segmented along two broad lines.
On the one hand, there is the game-theoretic analysis of simultaneous auctions which concentrates on studying the equilibrium strategy of rational agents (Engelbrecht-Wiggans &
Weber, 1979; Krishna & Rosenthal, 1996; Lang & Rosenthal, 1991; Rosenthal & Wang,
1996; Szentes & Rosenthal, 2003). Such analyses are typically used when the auction format employed in the simultaneous auctions is the same (e.g., there are m Vickrey auctions
or m first-price auctions). On the other hand, heuristic strategies have been developed
for more complex settings when the sellers offer different types of auctions or the buyers
need to buy bundles of goods over distributed auctions (Stone et al., 2003; Byde, Preist, &
Jennings, 2002; Yuen, Byde, & Jennings, 2006; Greenwald, Kirby, Reiter, & Boyan, 2001;
Greenwald & Boyan, 2004; Wellman, Reeves, Lochner, & Vorobeychik, 2004). This paper
adopts the former approach in studying a market of m simultaneous Vickrey auctions since
this approach yields provably optimal bidding strategies.
Related to our approach is the seminal paper by Engelbrecht-Wiggans and Weber (1979),
which provides one of the starting points for the game-theoretic analysis of distributed
markets where buyers have substitutable goods. Their work analyses a market consisting
of couples having equal valuations that want to bid for a dresser. Thus, the couples bid
941

fiGerding, Dash, Byde & Jennings

space can at most contain two bids since the husband and wife can be at most at two
geographically distributed auctions simultaneously. They derive a mixed strategy Nash
equilibrium for the special case where the number of buyers is large. Our analysis differs
from theirs in that we use a decision-theoretic approach as opposed to a game-theoretic
one. As argued among others by Rothkopf (2007), and Jiang and Leyton-Brown (2007), a
decision-theoretic analysis is often sufficient in practice in the case of auctions.3 Moreover,
the decision-theoretic framework allows us to study a more complex setting in which bidders
have different valuations and the global bidder can bid in all the auctions simultaneously
(which is entirely possible for online auctions).
Subsequently, Krishna and Rosenthal (1996) studied the case of simultaneous auctions
with complementary goods. They analyse the case of both local and global bidders and
characterise the bidding of the buyers and resultant market efficiency. The setting provided
by Krishna and Rosenthal (1996) is further extended to the case of common values by
Rosenthal and Wang (1996). However, neither of these works extend easily to the case of
substitutable goods which we consider. This case is studied by Szentes and Rosenthal (2003),
but the scenario considered is restricted to three sellers and two global bidders and with
each bidder having the same value (and thereby knowing the value of other bidders). The
space of symmetric mixed equilibrium strategies is derived for this special case. However,
as mentioned earlier, our results are based on decision theory, rather than game theory, but
our setting is more general (i.e., we consider an arbitrary number of auctions). A number of
other authors study settings where bidders face multiple simultaneous sealed-bid auctions,
e.g. McAfee (1993), Peters and Severinov (1997), Gerding et al. (2007b), Leyton-Brown,
Shoham, and Tennenholtz (2000). All of these papers, however, assume that bidders bid in
only a single auction and choose this auction randomly. As we show here, however, this is
not optimal. Finally, Shehory (2002) considers the case of concurrent English auctions, in
which bidding algorithms are developed for buyers with different risk attitudes. However,
in his setting the auctions never close at the same time, and he forces the bids to be the
same across auctions. Although this strategy may be effective for the described setting, as
we show in this paper, this is not always optimal when auctions close simultaneously (and
the buyers bid very late in the auctions).
Related to this paper is also the literature that considers bidding with budget constraints. Although it is beyond the scope of this paper to provide a full literature review
on this topic, here we highlight the most relevant work. A more extensive recent overview
is presented by Pitchik (2006). A number of papers, such as those by Rothkopf (1977)
and Palfrey (1980), study optimal bidding in simultaneous first-price auctions when bidders have constraints on exposure, which refers to the sum of bids. These papers, however,
make the very strong assumption that the value accrued in one auction is independent of
that in others. In other words, winning or losing one auction does not affect the expected
utility in other auctions. In contrast, here we consider complete substitutes where bidders
require only a single item. As a result, if one of the auctions is won, the value accrued
in other auctions is zero. This interdependency between auctions makes the analysis sig3. Essentially, a decision-theoretic setup requires only (1) information about the distribution of the best
competitive bid, and (2) that the bidders optimize their decision given this assessment (Rothkopf, 2007).
Such information about the distribution can be obtained by observing previous bids, e.g., using a learning
approach as described by Jiang and Leyton-Brown (2007).

942

fiOptimal Strategies for Simultaneous Vickrey Auctions with Perfect Substitutes

nificantly more difficult. Krishna and Benoit (2001) also consider this interdependency,
but the valuations and budget constraints are assumed to be common knowledge (and the
problem would thus be trivial without any budget constraints). Others, such as Che and
Gale (1998), consider the effect of budget constraints in single auctions, where bidder types
consist of two dimensions: their valuation and the budget. Sequential auctions have also
been extensively studied with regards to budget constraints, e.g., by Krishna and Benoit
(2001), Pitchik (2006). Furthermore, in addition to constraints on exposure, other types
of budget constraints have been considered, in particular limits on the expected expenditures (Engelbrecht-Wiggans, 1987). None of these papers, however, address the particular
setting that we consider here.
Finally, a number of researchers have investigated online auctions such as eBay. On the
one hand, Hendricks, Onur, and Wiseman (2005), Stryszowska (2004), Zeithammer (2005),
Peters and Severinov (2006), Rogers, David, Schiff, and Jennings (2007) have sought to
explain the bidding behaviour of buyers in online auctions, which include multiple bidding
(i.e., successively increasing the amount placed as the maximum bid in the eBay proxy agent,
as opposed to bidding the true value) and sniping (i.e., starting to bid near the end of the
auction). In fact, as a result of the sniping behaviour of the agents, the English auctions
run by eBay can be approximated as Vickrey auctions since bidders no longer have as much
information about the status of bidding in the auction. All of these papers, however, focus
only on the sequential auction problem, and do not discuss the bidding strategies when
multiple auctions close simultaneously. On the other hand, Boutilier, Goldszmidt, and
Sabata (1999), Gopal, Thompson, Tung, and Whinston (2005), Juda and Parkes (2006)
have explored means to improve the efficiency of simultaneous auctions by either proposing
sequential auction mechanisms or the use of options.4 Whereas these works are aimed at
changing or augmenting the auction mechanism to improve their outcome, in our case we
take the auctions as given and focus on the strategic aspects from a bidders perspective.

3. Bidding in Multiple Vickrey Auctions
The model consists of m sellers, each of whom acts as an auctioneer. Each seller auctions
one item; these items are complete substitutes (i.e., they are equal in terms of value and a
bidder obtains no additional benefit from winning more than one of them). The m auctions
are executed simultaneously; that is, they end simultaneously and no information about the
outcome of any of the auctions becomes available until the bids are placed.5 However, in
Section 5.3 we briefly investigate markets with both sequential and simultaneous auctions
since such markets are common in practice, especially online.
Furthermore, we generally assume that all the auctions are identical (i.e., a bidder is
indifferent between them), but we relax this assumption in Section 5.2 where the auctions
4. The notion is similar to financial options, in that these auction options provide the buyer the right,
but not the obligation, to buy a specific item at a specified price (the strike price) during a specified
period of time. However, the analysis of these options differs from the financial perspective due to very
different assumptions when modelling them.
5. We note that, although this paper focuses on sealed-bid auctions, where this is the case, the conditions
are similar for last-minute bidding or sniping in iterative auctions such as eBay (Roth & Ockenfels,
2002); when some of the auctions close at almost the same time, due to network delays there may be
insufficient time to obtain the results of one auction before proceeding to bid in the next one.

943

fiGerding, Dash, Byde & Jennings

have different valuation distributions and/or numbers of participating bidders and the global
bidder may thus prefer one auction over another. Finally, we assume free disposal and
that bidders maximise their expected profit. These two assumptions, together with the
assumption about the complete substitutes are implicit throughout the paper.
3.1 The Auctions
The sellers auction is implemented as a second-price sealed bid auction, where the highest
bidder wins but pays the second-highest price. This format has several advantages for agentbased settings. Firstly, it is communication efficient in terms of the number of interactions,
since it requires each bidder to place a single bid once, and the auctioneer to respond once to
each bidder with the outcome. In contrast, an iterative auction such as the English auction
usually requires several interactions and is typically more time consuming. Secondly, for the
single-auction case (i.e., where a bidder places a bid in at most one auction), the optimal
strategy is to bid the true value and thus requires no computation (once the valuation of
the item is known). This strategy is also weakly dominant (i.e., it is independent of the
other bidders decisions), and therefore it requires no information about the preferences of
other agents (such as the distribution of their valuations).
3.2 Global and Local Bidders
We distinguish between global and local bidders. The former can bid in any number of
auctions, whereas the latter bid in only a single one. Local bidders are assumed to bid
according to the weakly dominant strategy and bid their true valuation.6 We consider two
ways of modelling local bidders: static and dynamic. In the first model, the number of local
bidders is assumed to be known and equal to n for each auction. In the latter model, on the
other hand, the average number of bidders is equal to n, but the exact number is unknown
and may vary for each auction. This uncertainty is modelled using a Poisson distribution
(more details are provided in Section 4.1).
As we will later show, a global bidder that bids optimally has a higher expected utility
compared to a local bidder, even though the items are complete substitutes and a bidder
requires only one of them. Nevertheless, we can identify a number of compelling reasons
why not all bidders may choose to bid globally:
 Participation Costs. Although the bidding itself may be automated by an autonomous
agent, it still takes time and/or money, such as entry fees and time to setup an account,
to participate in a new auction. In that case, if the marginal benefits from bidding in two
auctions instead of a single auctions are less than the participation costs, then a buyer is
better off choosing and bidding in only one of the auctions.
 Information. Bidders may simply not be aware of other auctions selling the same type of
item. Even if this is known, however, a bidder may not have sufficient information about
the distribution of the valuations of other bidders and the number of participating bidders.
Whereas this information is not required when bidding in a single auction (because of the
6. Note that, since bidding the true value is optimal for local bidders irrespective of what others are bidding,
their strategy is not affected by the presence of global bidders.

944

fiOptimal Strategies for Simultaneous Vickrey Auctions with Perfect Substitutes

dominance property in a second-price auction), it is important when bidding in multiple
simultaneous auctions. Such information can be obtained by an expert user or be learned
over time, but is often not available to a novice.
 Risk Attitude. Although a global bidder obtains a higher utility on average, such a
bidder runs a risk of incurring a loss (i.e., a negative utility) when winning multiple
auctions. A risk averse bidder may not be willing to take that chance, and so may choose
to participate in fewer or even just a single auction to avoid such a potential loss. Whether
a bidder chooses to reduce the number of auctions to a single one depends of the degree
of risk aversion. In general, we would expect an agent to take less risk and bid in fewer
auctions if the stakes are higher, i.e., in the case of high-value transactions. A global
bidding agent, on the other hand, could be representing a large firm with sufficient funds
to cover any losses, and these agents are more likely to be risk neutral.
 Budget Constraints. Related to the previous point, a bidder may not have sufficient
funds to take a loss in case it wins more than one auction. In more detail, for a fixed
budget b, the sum of bids should not exceed b, thereby limiting the number of auctions a
bidder can participate in and/or lowering the actual bids that are placed in those auctions
(see also Section 5.1 where we investigate budget constraints in more detail).
 Bounded Rationality. As will become clear from this paper, an optimal strategy for
a global bidder is harder to compute than a local one. A bidder will therefore only
bid globally if the costs of computing the optimal strategy outweigh the benefits of the
additional utility. Moreover, the concept of a local bidder also captures the notion of a
manual bidder who does not use an intelligent bidding agent to compute the optimal
bid. For a human bidder it is clearly easier to bid their true value in a single auction
which requires no calculation at all (again, given that the value is known).

4. Identical Simultaneous Auctions
In this section, we provide a theoretical analysis of the optimal bidding strategy when a
global bidder participates in identical simultaneous auctions. In particular, we address
the case where all other bidders are local and simply bid their true valuation. However,
the analysis is more general and applies to any setting where the distribution of the best
competitive bid is known and satisfies certain properties. After we describe the global
bidders expected utility in Section 4.1, we show in Section 4.2 that it is always optimal for
a global bidder to participate in the maximum number of simultaneous auctions available.
Subsequently, in Section 4.3 we significantly reduce the complexity of finding the optimal
bids for the multi-auction problem in case all auctions are equivalent, and we then apply
these methods to find optimal strategies for specific examples.
4.1 The Global Bidders Expected Utility
In what follows, the number of sellers (auctions) is m  2 and M = {1, . . . , m} denotes
the set of available auctions. Let G denote the cumulative distribution function of the best
competitive bid in a particular auction and g the corresponding density function. Equiva945

fiGerding, Dash, Byde & Jennings

lently, G(b) is the probability of winning a specific auction conditional on placing bid b in
this auction. We introduce the following assumptions regarding function G:
Assumption 1. The cumulative distribution G(x) has bounded support [0, vmax ], is continuous within this range, and is strictly increasing for 0 < x < vmax .
A global bidder has a valuation v  (0, vmax ] (NB. we do not consider the trivial case where
v = 0, and we assume that v is within the bounds of G) and places a global bid b, which is
a vector that specifies a (possibly different) bid bi  [0, vmax ] for each auction i  M . Now,
given the setting described in Section 3 with identical auctions, the expected utility U for
a global bidder with global bid b and valuation v is given by:
"

U (b, v) = v 1 

Y

#

(1  G(bi )) 

iM

XZ

iM

bi

yg(y)dy.

(1)

0

Here, the left part of the equation is the valuation multiplied by the probability that the
global bidder wins at least one of the m auctions and thus corresponds to the expected
benefit. In more Q
detail, note that (1  G(bi )) is the probability of not winning auction i
whenQbidding bi , iM (1  G(bi )) is the probability of not winning any auction, and thus
[1  iM (1  G(bi ))] is the probability of winning at least one auction. The right part
of Equation (1) corresponds to the total expected costs or payments. To see the latter,
that the expected payment of a single second-price auction when bidding b equals
Rnote
b
0 yg(y)dy and is independent of the expected payments for other auctions.
Now, Equation (1) can be used to address the setting where all bidders except the
global bidder are local. This is done as follows. Let the number of local bidders be n 
1. A local bidders valuation v  [0, vmax ] is independently drawn from a cumulative
distribution F with probability density f , where F (x) has the same properties as G(x) (i.e.,
has support [0, vmax ], is continuous within this range, and is strictly positive for 0 < x <
vmax ). Note that, since it is a dominant strategy for a local bidder to bid the true value, no
additional assumptions are needed about the knowledge of these local bidders regarding the
distributions of other bidders . In case the local bidders are static, i.e., there are exactly n
local bidders with equal distributions, G is simply the highest-order statistic G(b) = F (b)n ,
and g(b) = dG(b)/db = nF (b)n1 f (b). However, we can use the same equation to model
dynamic local bidders in the following way:
Lemma 1. By replacing the highest-order statistic G(y) and the corresponding density
function g(y) with:
G(y) = en(F (y)1) ,
g(y) = dG(y)/dy = n f (y)en(F (y)1) ,
Equation (1) becomes the expected utility where the number of local bidders in each auction
is described by a Poisson distribution with average n (i.e., where the probability that n local
bidders participate is given by P (n) = nn en /n!).
Proof To prove this, we first show that G() and F () can be modified such that the
number of bidders per auction is given by a binomial distribution (where a bidders decision
946

fiOptimal Strategies for Simultaneous Vickrey Auctions with Perfect Substitutes

to participate is given by a Bernoulli trial) as follows:
G (y) = F  (y)N = (1  p + p F (y))N ,

(2)

where p is the probability that a bidder participates in the auction, and N is the total
number of bidders. To see this, note that not participating is equivalent to bidding zero.
As a result, F  (0) = 1  p since there is a 1  p probability that a bidder bids zero at a
specific auction, and F  (y) = F  (0) + p F (y) since there is a probability p that a bidder
bids according to the original distribution F (y). Now, the average number of participating
bidders is given by n = p N . By replacing p with n/N , Equation (2) becomes G (y) =
(1  n/N + (n/N )F (y))N . Note that a Poisson distribution is given by the limit of a
binomial distribution. By keeping n constant and taking the limit N  , we then obtain
G (y) = en(F (y)1) = G(y).

The results that follow apply to both the static and dynamic model unless stated otherwise.
4.2 Participation in Multiple Auctions
We now show that, for any valuation 0 < v < vmax , a utility-maximising global bidder
should always place non-zero bids in all available auctions.7 To prove this, we show that
the expected utility increases when placing an arbitrarily small bid compared to not participating in an auction. This holds even when the auctions are not identical. In the following
let bbj =bj denote the global bid b where the j th bid bj = bj . More formally,
Theorem 1. Under Assumption 1 and given that the global bidder has a valuation 0 <
v < vmax , consider a global bid b with bi  v for all i  M . Suppose that bj = 0 for some
auction j  M , then Equation (1) is not maximised, i.e., there exists a bj > 0 such that
U (bbj =bj , v) > U (b, v).
Proof We need to show that there exists a bj > 0 such that U (bbj =bj , v)  U (b, v) >
0. Using Equation (1), the marginal expected utility for participating in auction j w.r.t.
bidding zero in that auction can be written as:
Y

U (bbj =bj , v)  U (b, v) = vG(bj )

(1  G(bi )) 

iM \{j}

Now, using integration by parts, we have
equation can be rewritten as:


U (bbj =bj , v)  U (b, v) = G(bj ) v

R bj
0

yg(y) = bj G(bj ) 

Y

iM \{j}

Z

bj

yg(y)dy.

0

R bj
0



G(y)dy and the above

(1  G(bi ))  bj  +

Z

bj

G(y)dy.

(3)

0

7. We note that this does not necessarily hold in the boundary case where v = vmax . However, in practice,
we find that even here the optimal strategy is to bid below the true value in multiple auctions instead
of the true value in a single auction. This is especially the case when the number auctions is large (see
also Section 4.3.4).

947

fiGerding, Dash, Byde & Jennings

Clearly, since bj > 0 and g(x) > 0 for x > 0 (due to Assumption 1) we have that G(bj ) and
R bj
positive. Moreover, given that bi  v < vmax for i  M and that
0 G(y)dy are both strictly
Q
Q
v > 0, it follows that v iM \{j} (1  G(bi )) > 0. Now, suppose we set bj = 12 v iM \{j} (1 
h Q
i R
b
G(bi )), then U (bbj =bj , v)  U (b, v) = G(bj ) 12 v iM \{j} (1  G(bi )) + 0 j G(y)dy > 0 and

thus SU (bbj =bj , v) > U (b, v).
The above proof applies to the setting where auctions are identical. However, it is easy
to see that the same argument holds if G differs for each auction.
This result states that, even though there is risk of winning and having to pay for more
than one item (and a buyer disposes of any additional items won), the best strategy is to
participate in all auctions. Therefore, in expectation, the increasing probability of winning
a single item outweighs the possible loss incurred when winning more than one of them.
To obtain a better understanding of why this is true, consider the following more intuitive
argument. Suppose that a global bidder bids in k < m auctions. In that case, there is
some non-zero probability that the bidder wins none of the auctions, and thus there is a
non-zero expected demand for at least one of the items in the remaining auctions. Since this
argument holds for any k < m, by induction a global bidder should bid in all m auctions.
Note that Theorem 1 holds only when v is strictly smaller than vmax . In the case that
v = vmax there are two possibilities: either it is optimal to bid vmax in one auction in which
case the bids in the other auctions should be zero (since the bidder is guaranteed to win),
or it is optimal to bid below vmax but strictly positive in all auctions. As we will show in
Section 4.3.4 we find emperically that the first is true only when the number of auctions is
small, and the latter is the case for large numbers of auctions.
4.3 The Optimal Global Bid
A general solution to the optimal global bid requires the maximisation of Equation (1) in
m dimensions, an arduous task, even when applying numerical methods. In this section,
however, we show how to reduce the entire bid space to a single dimension in most cases,
thereby significantly simplifying the problem at hand. First, however, in order to find the
optimal solutions to Equation (1), we set the partial derivatives to zero:


Y
U
(1  G(bj ))  bi  = 0.
(4)
= g(bi ) v
bi
jM \{i}

Q
Now, equality (4) holds either when g(bi ) = 0 or when bj b\{bi } (1  G(bj ))v  bi = 0.
In the model with dynamic local bidders, g(bi ) is always greater than zero, and can therefore
be ignored (since g(0) = nf (0)en and we assume f (y) > 0). In other cases, g(bi ) = 0 only
when bi = 0. However, Theorem 1 shows that the optimal bid is non-zero for 0 < v < vmax .
Therefore, we can ignore the first part, and the second part yields:
bi = v

Y

(1  G(bj )).

(5)

jM \{i}

In other words, the optimal bid in auction i is equal to the bidders valuation multiplied
by the probability of not winning any of the other auctions. It is straightforward to show that
948

fiOptimal Strategies for Simultaneous Vickrey Auctions with Perfect Substitutes

the second partial derivative is negative, confirming that the solution is indeed a maximum
when keeping all other bids constant. Moreover, since the optimal bid requires that 0 < bi <
vmax for v < vmax (due to Theorem 1 and since bidding more than the valuation is clearly
suboptimal), we only need to consider interior solutions. Thus, Equation (5) provides a
means to derive the optimal bid for auction i, given the bids in all other auctions.
Now, by taking the partial derivatives for each auction and rewriting Equation (5), the
optimal global bid must obey the following relationship: b1 (1  G(b1 )) = b2 (1  G(b2 )) =
. . . = bm (1  G(bm )). By defining H(b) = b(1  G(b)) we can rewrite the equation to:
Y
H(b1 ) = H(b2 ) = . . . = H(bm ) = v
(1  G(bj )).
(6)
jM

In what follows, we apply Equation (6) to reduce the search space, and first show that,
for a large class of probability distributions, the optimal global bid consists of at most two
different values, thereby reducing the search space to two dimensions. In Section 4.3.2 we
further reduce this to a single dimension. In Section 4.3.3 we consider limit results when
the number of auctions goes to infinity. Finally, in Section 4.3.4 we perform a numerical
analysis of the optimal bidding strategy for specific cases when the number of auctions is
finite, and we consider to what extent a global bidder benefits compared to bidding locally.
4.3.1 Reducing the Search Space
In this section, we first show that the optimal global bids consists of at most two different
values when the function H(b) = b(1  G(b)) has a unique critical point. More formally, we
introduce the following requirement:
Assumption 2. H(b) = b(1  G(b)) has a unique critical point bf , i.e., there exists a bf
d
d
s.t. db
H(bf ) = 0 and for all b 6= bf , db
H(b) 6= 0.
We then go on to show that this requirement is met for a wide class of distributions which
are characterised by a non-decreasing hazard rate. Let the two bid values be denoted by b
and b+ , where b  b+ . Formally:
Theorem 2. Under Assumptions 1 and 2, the global bid b maximising Equation (1)
contains at most two distinct bid values: b and b+ . In addition, b  bf  b+ , where bf
is the unique critical point of H(b) = b(1  G(b)).
Proof Suppose H has a unique critical point, bf . H(0) = H(vmax ) = 0, and H(b) > 0
in (0, vmax ), so bf must in fact be the global maximum of H. Furthermore, for b < bf
H is strictly increasing while for b > bf H is strictly decreasing. This also implies that
H(x) = y has at most two solutions: if y > H(bf ) then there are no solutions, since bf is
the global maximiser of H; if y = H(bf ) there is a unique solution, namely bf ; if y < H(bf )
then applying the intermediate value theorem to H on the interval [0, bf ] gives one solution,
namely b  bf , and on the interval [bf , vmax ] gives the other, namely b+  bf . There can
be only one solution in each interval because H is strictly monotonic on each.
Now, Equation (6) implies that H(bi ) is equal for all i  M . Therefore, given that
H(x) = y has at most two solutions, there can be at most two distinct interior bids bi ,
949

fiGerding, Dash, Byde & Jennings

namely b and b+ . As mentioned before, from Theorem 1 the utility maximizing solution
has 0 < bi < vmax when v < vmax , and therefore all solutions are interior ones. In the case
that v = vmax either b+ = vmax and b = 0, or we have as before that b+ < vmax and
b > 0.

We now show that the unique critical point of H is guaranteed if G has a non-increasing
hazard rate within the interval [0, vmax ]. We choose this property since it encompasses a
large number of distributions, including those with log-concave density functions such as
uniform, normal and exponential (we refer to the work by Barlow, Marshall, and Proschan,
1993 and Bergstrom and Bagnoli, 2005 for a list of such functions). Formally, a hazard rate
(see e.g., Krishna, 2002) of a cumulative distribution function F is denoted by F and is
defined by:
F (x) 

f (x)
.
1  F (x)

We have the following result:
Lemma 2. Under Assumption 1, if G (b) is non-decreasing in (0, vmax ), then the function
H(b) = b(1  G(b)) has a unique critical point bf in that interval.
Proof See Appendix A.1.
We now extend the proof to the distribution functions of the local bidders F by showing
that, if the hazard rate of F is non-decreasing, the hazard rate of G is also non-decreasing,
and thus the reduction also applies. This holds both for local and static bidders. Formally:
Theorem 3. If F (b) is non-decreasing, then G (b) and Gb (b) are also non-decreasing,
b = en(F (y)1) for n  2.
where G(b) = F (b)n and G

Proof See Appendix A.1.

4.3.2 Characterising the Optimal Bids
Using the above results we are able to reduce the optimal global bid to two values, a high
bid, b+  bf , and a low bid, b  bf . However, we do not know the number of auctions
in which to bid high and low. In this section we show that it is optimal to bid high in at
most one auction, and low in all other auctions. Using this result, we can then write the
high bid in terms of the low bids, reducing the search space even further.
Theorem 4. Under Assumptions 1 and 2, the global bid b that maximises Equation 1
has at most one high bid, i.e., at most one i  M for which bi > bf , where bf is the unique
critical point of H(b) = b(1  G(b)).
Proof See Appendix A.1.
950

fiOptimal Strategies for Simultaneous Vickrey Auctions with Perfect Substitutes

Together with Lemma 2 this implies that, for distributions with non-decreasing hazard
rates, either there exists one high bid and m  1 low bids, or all bids are low. Note that
we can now calculate the value of the high bid analytically given the low-bid value by using
Equation (5). Consequently, finding the optimal global bid reduces to optimising a single
variable (i.e., the value for the low bids). This value can then be computed numerically
using standard optimisation techniques such as the Quasi-Newton method, or, alternatively,
the bids can be discretised and a brute-force search can be applied to find the optimum.
Whatever method is selected it is important to notice that, due to the reduction of the
search space, the computational complexity of calculating the optimal outcome numerically
is independent of the number of auctions (or indeed the number of bidders).
The result of Theorem 4 suggests that it is optimal to restrict attention to a single
auction by bidding high in that auction, and then using the remaining auctions as a backup
in case the high-bid auction fails. As we will show in Section 4.3.4 this is often the case
in practise when the number of auctions is small. When the number of auctions is large,
however, we find that it is optimal to bid low in all of the auctions, irrespective of the
bidders valuation. We derive theoretical results for the limit case where the number of
auctions goes to infinity in Section 4.3.3, and then consider empirical results for the finite
case in Section 4.3.4.
4.3.3 Limit Results
In this section we investigate how the optimal bidding changes as the number of auctions
becomes very large and consider whether there are general patterns that characterise the
optimal strategy. The first, most basic result is that, as the number of auctions increases,
the agent is able to extract an increasingly greater utility and this approaches the maximum
possible utility, v. To prove this, without loss of generality we restrict the strategy of the
global bidder by only considering uniform bidding (i.e., where all bids are equal). Let bu
m
denote the optimal global bid when bidding in m auctions when the bidder is confined to
using uniform bids:
Theorem 5. Under Assumption 1, the expected utility as defined by Equation 1 from
playing the optimal uniform bid bu
m converges to v, in the sense that for all  > 0 there is
a constant m such that m > m implies U (bu
m ) > v  .
Proof See Appendix A.2.

Note that, since v is an upper bound on the utility that can be achieved by any global
bidder, this result implies that, as the number of auctions increases, eventually uniform
bidding will always be superior to non-uniform bidding. More formally:
Corollary 1. Under Assumption 1, for sufficiently large m the globally optimal bid b
maximising Equation (1) is equal to the optimal uniform bid bu
m , independent of v.

951

fiGerding, Dash, Byde & Jennings

R bf
Proof This corollary follows from Theorem 5 with  = EP (bf ) = 0 xg(x) dx: for m > m
uniform bidding gives utility at least v  , whereas non-uniform bidding gives strictly less:
U (b ) < v  EP (b+ ) < v  EP (bf ) = v  .

In practice, we find that it is not necessary for the number of auctions to be particularly
large before uniform bidding is optimal for all v. To this end, in the next section we provide
examples of optimal bidding strategy for specific settings.
4.3.4 Empirical Evaluation
In this section, we present results from an empirical study and characterise the optimal
global bid for specific cases with finite numbers of auctions. Furthermore, we measure
the actual utility improvement that can be obtained when using the global strategy. The
results presented here are based on a uniform distribution of the valuations with vmax = 1,
and the static local bidder model, but they generalise to the dynamic model and other
distributions. Figure 1 illustrates the optimal global bids and the corresponding expected
utility for various m and n = 5, but again the bid curves for different values of m and n
follow a very similar pattern.
As shown in Figure 1, for bidders with a relatively low valuation, the optimal strategy
is to submit m equal bids at, or very close to, the true value. After the valuation reaches
a certain point, however, placing equal bids is no longer the optimal strategy when the
number of auctions is small (in Figure 1 this occurs at m = 4 and m = 6). At this point,
a so-called pitchfork bifurcation is observed and the optimal bids split into two values: a
single high bid and m  1 low ones. In all experiments, however, we consistently observe
that the optimal strategy is always to place uniform bids when the valuation is relatively
low. Moreover, the bifurcation point moves to the right as m increases, and disappears
altogether when m becomes sufficiently large (m  10 in Figure 1) at which point the
optimal bids are uniform (note that this holds even when v = vmax ). Note also that the
uniform optimal bids move closer to zero as m tends to infinity.
As illustrated in Figure 1, the utility of a global bidder becomes progressively higher
with more auctions. Note that, consistent with the limit results from Section 4.3.3, the
utility approaches the upper bound v as the number auctions becomes large. In absolute
terms, the improvement is especially high for bidders that have an above average valuation,
but not too close to vmax . The bidders in this range thus benefit most from bidding globally.
This is because bidders with very low valuations have a very small chance of winning any
auction, whereas bidders with a very high valuation have a high probability of winning a
single auction and benefit less from participating in more auctions. In contrast, as shown in
Figure 1, if we consider the utility relative to bidding in a single auction, this is much higher
for bidders with relatively low valuations. In particular, we notice that a global bidder with
a low valuation can improve its utility by up to m times the expected utility of bidding
locally. Intuitively, this is because the chance of winning one of the auctions increases by
up to a factor m, whereas the increase in the expected cost is negligible. For high valuation
buyers, however, the benefit is not that obvious because the chances of winning are relatively
high even in case of a single auction.
952

fiOptimal Strategies for Simultaneous Vickrey Auctions with Perfect Substitutes

1

0.8

bid

0.6

m=1
m=4
m=6
m = 10
m = 102
m = 106
m = 1010

0.4

0.2

0
0

0.2

0.4

0.6

0.8

1

0.8

1

valuation (v)
1

expected utility

0.8

0.6

0.4

m=1
m=4
m=6
m = 10
m = 102
m = 106
m = 1010


0.2

0
0

0.2

0.4

0.6

valuation (v)
10

m=1
m=4
m=6
m = 10

relative utility

8

6

4

2

0
0

0.2

0.4

0.6

0.8

1

valuation (v)

Figure 1: The optimal global bid, the corresponding expected utility, and the expected
utility proportional to that of a local bidder for a setting with n = 5 static local
bidders and varying number of auctions (m). Note that the results for m = 1
correspond to those of a local bidder. For comparison, the expected utility is also
shown as the number of auctions approaches infinity.
953

fiGerding, Dash, Byde & Jennings

5. Extensions: Budgets, Non-Identical and Sequential Auctions
In the previous section, we considered the best response strategy when a global bidder faces
multiple identical simultaneous auctions and has no financial constraints. In this section,
we generalise the results to other settings. In particular, we investigate three important
extensions:
1. In Section 5.1 we investigate a setting where the global bidder has a limited budget which
constrains the sum of bids or exposure.
2. In Section 5.2 we investigate a setting where the auctions differ in their probability of
winning. Such differences arise, for example, when the auctions have different numbers
of local bidders participating.
3. In Section 5.3 we extend the results to sequential auctions and consider a setting where
the resources of interest are auctioned both simultaneously and sequentially.
5.1 Budget-Constrained Bidding
The derivation of the optimal global bid has shown that it is an optimal strategy for the
global bidder to bid in all the simultaneous auctions. Now, such a strategy implicitly
assumes that the bidder does not face financial constraints (i.e., a bidder can pay for all the
items won). However, often a bidder has limited resources which may restrict its bidding
strategy. In this section we study how a budget can limit the space of possible strategies
available to a global bidder and affect its optimal strategy. In particular, we consider the
case where a budget constrains the exposure, i.e., the sum of the bids.8 This occurs, for
example, when the global bidder has limited liquidity and faces very negative consequences
when it cannot pay for all the items it wins (e.g., going bankrupt or being thrown out of
the system).
Now, the importance of taking budget constraints into account becomes even more
pronounced from the following result which shows that, as the number of auctions increases,
the required budget or exposure in the unconstrained case will exceed any given limit:9
Theorem 6. Under Assumption 1 and assuming that g(b) is bounded throughout [0, vmax ],
for all v > 0 and for any C, there exists an m forP
which the exposure of the optimal global

bid b maximising Equation (1) exceeds C, i.e., iM bi > C.
Proof See Appendix A.3.

Thus, despite the low probability of the bidder having to pay the sum of its bids (especially,
given that each auction is a second-price auction), in practice a bidder may still want to
limit this amount.
8. We note that, although in practice a budget constraint is on the payments rather than the bids, even
in the second-price auctions the worst-case outcome is that a bidder pays their bids in all auctions, and
therefore a hard budget constraint is equivalent to constraining the sum of bids.
9. Note that this occurs despite the fact that bids tend to zero as m goes to infinity. However, the exposure
does not (see the proof of Theorem 5).

954

fiOptimal Strategies for Simultaneous Vickrey Auctions with Perfect Substitutes

In more detail, we can formulate the budget-constrained problem faced by the bidder
as:
max U (b, v)

s.t.

b[0,v]m

X

bi  C,

(7)

iM

where C is the budget limit and U (b, v) is the utility of the global bidder as given by
Equation (1). We consider budget constraints by distinguishing between three cases. In the
following, b = (b1 , . . . , bm ) refers to the unconstrained optimal solution (i.e., the optimal
c
solution in the absence of any budget constraints), whereas bc = (bc
1 , . . . , bm ) refers to the
optimal global bid subject to budget constraints.
P

Case (1)
iM bi > C and v  C.

Here, the sum of the unconstrained optimal bids exceeds the budget and it is
therefore required to recompute the optimal bid given the constraint. Moreover,
the budget constraint is equal or less than the valuation. For this case, we are
able to show that it is a best strategy to bid in a single auction and to place bid
bi = C in this auction for fairly general probability density functions of g. By so
doing, this provides one of the justifications for the existence of local bidders (as
outlined in Section 3.2). Although this result is intuitive, it is not straightforward
since a bidder may still decide to divide its budget across several auctions. Below
we provide the proof of this result.
P

Case (2)
iM bi > C and v < C.

As in the previous case, we need to recompute the optimal bid. However, as we will
show, the optimal strategy for the global bidder in this case is to bid in multiple
auctions (as in the unconstrained case). Moreover, in contrast to the unconstrained
case, the global bid may consist of more than two different values.
P

Case (3)
iM bi  C.
In this case, the sum of the unconstrained optimal bids is less than the budget.
Thus the result is trivial and we have b = bc .

Examples of these three different cases are depicted in Figure 2. This figure shows the
optimal strategy for bidding in 4 simultaneous auctions when local bidders have uniformly
distributed valuations within the range [0, 1] and the global bidder has a budget constraint
C = 0.8.10 Clearly, for a global bidder with a low valuation the budget constraint does not
affect the optimal bidding strategy (case 3). Case 1 occurs when the bidder has a valuation
at or above 0.8, and case 2 occurs in the in-between range. As this figure shows, the optimal
strategy in the latter two cases is qualitatively very different from the unconstrained case;
whereas the unconstrained optimal strategy is to bid high in at most a single auction, now
this consists of at most a single low bid, several high bids, and placing zero in the remaining
auctions. As the budget becomes tighter relative to the valuation, the number of auctions
in which the bidder participates decreases until a single auction remains (see Figure 2, case
1). In what follows, we first consider conditions under which such behaviour is optimal for
10. Similar patterns are observed in the optimal strategy with varying number of auctions and budget. These
patterns can always be grouped in these three cases.

955

fiGerding, Dash, Byde & Jennings

1

0.8

1

bid

0.6

case (1)
0.4

2

case (3)

3

0.2

case (2)
4

0
0

1
0.2

1
0.4

1
0.6

0.8

1

valuation (v)
Figure 2: The solid lines denote the optimal bids of a global bidder with budget C = 0.8
in a setting where n=5 and m=4. Here, the numbers indicate the number of
auctions in which a certain bid is placed. As the valuation increases (and the
budget remains constant), the bids in the auctions taper off one by one, until a
single high bid remains and all other bids are zero. The dotted line represents
the unconstrained solution.

case 1, and subsequently we address case 2 in more detail. Case 3 is trivial and is therefore
not considered further.
We now provide a formal result for the optimal strategy in case 1 and show that, when
the budget constraint imposed on a global bidder is equal to or less than the value it
attaches to the item it wishes to acquire, it is a best strategy to act as a local bidder under
the following conditions:
Assumption 3. The probability density function of the best competitive bid g(x) is convex
and g(0) = 0.
The result is stated formally as follows:
Theorem 7. Under Assumptions 1 and 3, if the global bidder has a budget C  v, then
Equation (7) is satisfied for bc = (C, 0, . . . , 0), i.e., it is optimal to bid C in exactly one
auction.
Proof See Appendix A.3.
Intuitively, the convexity of g(x) is necessary to ensure that the probability of winning
an auction increases sufficiently quickly as the bid increases. This way, a higher bid in a
956

fiOptimal Strategies for Simultaneous Vickrey Auctions with Perfect Substitutes

single auction results in a higher probability of winning compared to dividing the same
amount over several auctions. Although placing a higher bid in a single auction may lead
to a higher expected payment, the proof shows that the utility obtained from an increased
probability of winning outweighs the expected payment increase. At the same time, the
condition g(0) = 0 is important, otherwise the probability of winning increases sufficiently
quickly such that it is optimal to spread the budget over multiple auctions. The importance
of the two conditions becomes apparent by the following corollary which shows that, for the
special case where C = v, the condition that g(0) = 0 is in fact a necessary condition for
bidding in a single auction to be optimal, and, furthermore, that this strategy is no longer
optimal in case g(x) is concave.
Corollary 2. Under Assumption 1, if either g(0) > 0 or g(x) is strictly concave, and the
global bidder has a budget C = v, then Equation (7) is satisfied by bidding strictly positive
in at least two auctions.
Proof See Appendix A.3.
Note that, in case of static local bidders, the condition that g(x) is convex holds, for
example, when f (x) is convex, increasing, and non-negative (and thus F (x) is also convex),
since g(x) = nF n1 (x)f (x). However, the condition holds in other more general cases as
well; especially if the number of local bidders is high, a concave local bidder distribution
can easily result in a convex g(x). Also note that the condition g(0) = 0 holds in the case
of static local bidders and n  2, even if f (0) > 0 (but not in the case of dynamic local
bidders unless f (0) = 0). Finally, we note that convexity of g(x) implies a non-decreasing
hazard rate and therefore the conditions are stronger than those imposed by Assumption 2
in Section 4.3.
We now move on to consider cases 2 and 3. Case 2 cannot be analysed as easily as
case 1. However, a good insight into the constrained optimal strategy can be obtained by
considering the Lagrangian of Equation (7). Since the budget constraint is an inequality
constraint, in addition to the regular Langrangian multiplier we need to introduce the slack
variable  to convert the inequality into an equality. The variable is first P
squared to ensure
a positive value and is then added to the constraint which becomes C  iM bi +  2 = 0.
The Langrange function becomes as follows:
!
X
(B, , ) = U (b, v) + 
bi  C +  2 .
(8)
iM

By setting partial derivatives to zero, this results in the following m + 2 equations to be
solved:
X
(.)
=
bi  C +  2 = 0,

iM

(.)
= 2 = 0,

(.)
U (b, v)
=
+  = 0 i  M.
bi
bi
957

(9)

fiGerding, Dash, Byde & Jennings

2.5

1

2

sum of bids

0.8

Case 2

Constrained
Unconstrained

bid

0.6

0.4

=0

1

0.5

0.2

0
0

1.5

0.2

0.4

0.6

0.8

1

0
0

0.2

0.4

0.6

0.8

1

valuation (v)

valuation (v)
(a) Optimal bids

(b) Sum of bids

Figure 3: The optimal global bid and the corresponding sum of bids both for the unconstrained case and in the case of a budget constraint C = 1.5. Here, n=10 and
m=3.

From these equations, it can readily be observed that for case 1,  = 0 thereby leading
to
P the case where the solution of Equation (9) forces the total budget to be spent, i.e.,
iM bi = C.

For case 2, either  = 0 corresponding to a local maximum of Equation (4), in which
the total budget is not spent or  = 0 whereby the total budget is spent. These two
possible situations are highlighted in Figure 3, showing the optimal global bidding strategy
for n = 10, m = 3 and budget C = 1.5, and the corresponding sum of bids (note that here
case 1 does not arise since the budget exceeds the highest valuation). The unconstrained
solution is also provided for comparison. This example shows that the total amount spent is
not necessarily equal to the available budget, even when the unconstrained optimal solution
exceeds the budget. Thus, for case 2 we cannot solely consider solutions whereby the sum
of bids is equal to the budget since it may be the case that bidding less than the budget
yields a greater utility (i.e., when  = 0).
Furthermore, as can be observed from Figures 2 and 3(a), the introduction of a budget
constraint changes the shape of the optimal strategy for cases 1 and 2. Recall from Section 4.3 that the optimal strategy in the unconstrained case is to either bid equally in all
auctions or to bid high in one auction and low and equally in all of the remaining ones.
However, with a budget constraint, we now have a region where the best strategy is to
bid high in more than one auction and low in the remaining ones. Moreover, parts of the
solution now consist of three different bids: high, low, and zero. Hence, Equation (5) no
longer holds in case of budget constraints, and the structure of bids observed in Section 4.3
need not be satisfied. This means that we cannot apply the same reduction in search space
to efficiently compute the optimal strategy in the case of budget constraints for case 2, and
the complexity of computing the optimum using brute force increases exponentially with
the number of auctions.
958

fiOptimal Strategies for Simultaneous Vickrey Auctions with Perfect Substitutes

5.2 Non-Identical Auctions
Whereas previously we assumed all auctions to be equivalent, we now relax this assumption
to the more general case with non-identical auctions. Here, we assume that these auctions
differ not in the global bidders valuation for the item that is sold, but rather in the probability of obtaining the item at a given bid. These differences arise, for example, when the
number of bidders and/or the local bidders valuation distribution vary from one auction
to another. This can be used in practice when specific information about the individual
auctions is available.11 In more detail, we assume that each auction i  M has an individual
cumulative distribution function, denoted by Gi (b), and a corresponding density function
gi (b). Now, the expected utility is given by:
"
#
Y
X Z bi
ygi (y)dy,
(10)
U (b, v) = v 1 
(1  Gi (bi )) 
iM

iM

0

where b = (b1 , . . . , bm ) is the global bid, specifying a bid for each auction as before.
It is easy to see that Theorem 1 in Section 4.2 extends to the setting with non-identical
auctions and that the optimal strategy is to bid a strictly positive amount in each auction,
provided Gi (b) is strictly positive for all i  M, 0 < b  vmax . But, the auctions are now
non-identical and this complicates the bidding space. We revisit the search space problem
for non-identical auctions in Section 5.2.1 where a different method is employed to reduce
the bidding space. In Section 5.2.2, we show that a global bidder should always bid higher
in a more preferred auction. The optimal bidding strategy for a number of non-identical
auctions is empirically evaluated in Section 5.2.3.
5.2.1 Calculating the Optimal Global Bid
Although a reduction of the search space as described in Section 4.3 is no longer possible
when the auctions are not identical, we can still significantly reduce the computation needed
to find the optimal bid. To this end, we first set the partial derivatives U/bi to zero for
i  M . Given that gi (bi ) > 0 we have:
Y
bi = v
(1  Gk (bk )).
(11)
kM \{i}

By combining the partial derivatives, we then obtain the following relationship:
Y
(1  Gk (bk )).
b1 (1  G1 (b1 )) = b2 (1  G2 (b2 )) = . . . = bm (1  Gm (bm )) = v

(12)

kM

Generally, it is not possible to find the optimal global bid analytically. Using expression (12),
however, we can confine our search to only one of the bids and then, for each value of this
bid, determine the values of m  2 other bids by the computationally less demanding onedimensional root-finding operations (see e.g., Burden & Faires, 2004). The remaining bid
value can be found analytically using Equation (11).
11. For example, eBay auctions reveal the number of visits to web pages of particular items, which can be
used to estimate the number of participating bidders at individual auctions (see http://www.ebay.com
for examples).

959

fiGerding, Dash, Byde & Jennings

To clarify this reduction in search space, we show how this can be applied to a discrete
bid space and using a brute-force search approach. To find the optimal (discrete) global
bid, we iterate through the discrete space of one of the bids, say b1 . For each value of b1 we
then find the corresponding m1 bid values such that expression (12) is satisfied as follows.
We first calculate b1 (1  G1 (b1 )) and then search through all discrete values of b2 such that
b1 (1  G1 (b1 )) = b2 (1  G2 (b2 )).12 Typically there will be at least two values of b2 for
which this equality holds.13 All the solutions are stored into memory and this is repeated
for each bi < bm . The value for bm is calculated using Equation (11). This way we can
calculate the expected utility given b1 such that expression (12) is satisfied. The optimal
solution is then found by maximising the expected utility across all possible values of b1 and
all combinations of solutions which have been stored into memory. Note that the amount of
computation required to find the expected utility for a single value of b1 increases linearly
with the number of auctions. However, the number of combinations increases exponentially
with the number of auctions. Nevertheless, since the base of this exponential is typically
2 (in particular in the case of non-decreasing hazard rates, see Footnote 13), computation
remains tractable when the number of auctions is relatively small but becomes intractable
for very large settings.14 In specific cases, however, where one auction is clearly better
than another auction in a precise sense, it is possible to reduce the number of combinations
and thus the required computation becomes linear with the number of auctions. This issue
is addressed in the next section.
5.2.2 Preferred Auctions and their Optimal Bids
In many cases, it is possible to find auctions which are more favourable than others in
terms of their expected utility. For example, all else being equal, a bidder is expected to
do better on average in auctions with fewer other bidders. A bidder would therefore prefer
such auctions over less profitable ones, irrespective of a bidders own valuation. In this
section, we formalise the notion of a preferred auction, and investigate the optimal bids
when bidding in multiple auctions with respect to these preferences over auctions.
We use the concept of stochastic orders (Shaked & Shanthikumar, 1994) to rank the
auctions in terms of a global bidders preferences. Formally, auction j stochastically dominates auction i when Gj (b)  Gi (b) for all 0  b  vmax (Krishna, 2002, Appendix B).
The expected utility is then at least as high when bidding only in auction i compared to
bidding the same amount in auction j. This is shown
R bas follows. We can write the expected
utility U of auction i/j as: Ui/j = (v  b)Gi/j (b) + 0 Gi/j (x)dx. Since both Gi (b)  Gj (b)
Rb
Rb
and 0 Gi (x)dx  0 Gj (x)dx when auction j dominates auction i, the expected utility of
bidding in auction i is at least as high as in auction j. We therefore refer to auction i as
12. In case of discrete bids this equality rarely holds exactly but this can be resolved by minimising the
difference instead, i.e., minimising b1 (1  G1 (b1 ))  b2 (1  G2 (b2 )).
13. More precisely, if the function b (1  G(b)) has a single critical value, then there will be at most two
solutions. As shown earlier by Lemma 2 this is the case when G has a non-decreasing hazard rate. Note
that, in case of discrete bids, these solutions are local minima when taking the difference.
14. To provide some idea of what this means in practice, we implemented a brute-force search in Java which
finds the optimal global bid for 100 discrete valuations and, in doing so, searches through 100 bids per
valuation and per auction. Using these settings, on a 1.66GHz Intel Centrino the optimal solution is
found within 4 seconds when m = 10, and takes about 35 seconds when m = 15.

960

fiOptimal Strategies for Simultaneous Vickrey Auctions with Perfect Substitutes

the (weakly) preferred auction. Furthermore, we say that auction i is strictly preferred over
auction j at some b if Gi (b) > Gj (b).
When bidding in multiple auctions, we can show that it is optimal to bid higher in
preferred auctions. Intuitively, this is because, in case of second-price auctions, preferred
auctions give an agent a higher probability of winning the good at a lower price. This
observation can reduce the computation required since only bid values which are higher
than the bids in less preferred auctions need to be considered. In practice this usually
reduces the number of solutions for each auction which satisfy expression (12) to a single
one (in which case the computation of the optimal global bid becomes linear in the number
of auctions). In addition to the computational benefit, this observation also provides some
guidelines and intuition about the strategies. The relationship between preferred auctions
and the optimal global bid is more precisely described as follows:
Theorem 8. Under Assumption 1, for any i, j  M, i 6= j: if auction i is (weakly) preferred
over auction j, i.e., if Gi (b)  Gj (b) for 0  b  vmax , and if auction i is strictly preferred
at both bi and bj , where bi and bj maximise Equation (10), then bi > bj for any v > 0.
Proof We first prove bi  bj by contradiction, and then go on to show that bi 6= bj .
Suppose the opposite holds and there exist i, j  M such that bi , bj  b are optimal and
bi < bj . We then show that the expected utility is strictly higher when interchanging the
two bids, i.e., when bidding bi in auction j and bj in auction i. Let bbi bj denote the global
bid b where the two bids bi , bj are interchanged. We now show that U (bbi bj , v)  U (b, v)
is strictly positive for 0  v  vmax . Using Equation (10) and itegration by parts we obtain
the following:
U (bbi bj , v)  U (b, v) = (v c  bi )(Gj (bi )  Gi (bi )) + (v c  bj )(Gi (bj )  Gj (bj ))
Z b
j
Gi (y)  Gj (y)dy, (13)
 v c Gj (bi )Gi (bj ) + v c Gj (bj )Gi (bi ) +
bi

Q
where c = kM \{i,j} (1  Gk (bk )). First we note that c > 0 because the strict preference of
the auctions at bi and bj requires that 0 < bi < vmax ,0 < bj < vmax (since Gi (0) = Gj (0) =
Gi (vmax ) = Gj (vmax ) = 0), and therefore due to Theorem 7 it holds that 0 < bk < vmax
for k  M \{i, j}. Next, note that the last term in Equation (13) is always positive since
bj > bi and Gi (b)  Gj (b). We can therefore ignore this term if the remaining part is also
positive. Let this term be denoted by  in the following. Since we assumed bi and bj to be
optimal, from Equation (11) the following holds:
bi = v c (1  Gj (bj ))  v c  bi = v c Gj (bj )),
bj = v c (1  Gi (bi ))  v c  bj = v c Gi (bi )).

(14)

By replacing v c  bi and v c  bj in Equation (13) by v c Gj (bj ) and v c Gi (bi ) respectively,
and by rearranging the terms, we obtain the following:
U (bbi bj , v)  U (b, v) = v c (Gi (bi )  Gj (bi ))(Gi (bj )  Gj (bj )) + .
961

(15)

fiGerding, Dash, Byde & Jennings

1
0.8
bid fraction x = b/v

bid fraction x = b/v

1
0.8
n1
n1
n1
n1
n1

0.6
0.4
0.6

= 6, n2 = 7
= 6, n2 = 8
= 6, n2 = 9
= 6, n2 = 15
= n2 = 6

0.7

0.8

0.6
0.4
0.2
0
0

0.9

valuation (v)

n = {5, 6, 7, 8, 9, 10, 11}
n = {5, 5, 5, 7, 7, 7, 7}

0.5

1

valuation (v)

(a) 2 auctions

(b) 7 auctions

Figure 4: Optimal global bid as a fraction of the valuation for (a) two and (b) seven simultaneous auctions for various settings. The auctions differ in the number of local
bidders which are present at each auction.

Now, since Gi (bi ) > Gj (bi ), Gi (bj ) > Gj (bj ), v > 0, c > 0, and   0, it follows that
U (bbi bj , v) > U (b, v). As a result, bi and bj cannot be optimal bids for auctions i and j
respectively, and this contradicts our initial proposition.
The above proves that bi  bj . We now show that bi must be strictly higher than

bj . This follows directly from Equation (14). Suppose bi = bj , then from Equation (14)
it follows that Gi (bi ) = Gj (bi ) and Gi (bj ) = Gj (bj ). However, this conflicts with the
requirement that auction i must be strictly preferred at bi and bj , thereby completing the
proof.

The above proof is based on the requirement that auction i is strictly preferred over
auction j for at least the two optimal bids in these auctions. Although this requires the
optimal bid to be known in advance, the proof also applies to cases where auction i is strictly
preferred over auction j over the entire range 0 < b < vmax . This is the case, for example,
when the number of bidders in auction i is strictly less than in auction j. Furthermore, we
note that similar results hold for slightly different conditions. Specifically, if the auction
is strictly preferred at a set of points with non-zero measure anywhere within the range
[bj , bi ], then bi  bj since  > 0 in Equation (14). Consequently, the condition bi  bj
holds when auction i is preferred over auction j and Gi , Gj can be described by analytic
functions.
5.2.3 Empirical Evaluation
In this section, we examine how the bidding strategy of the global bidder is affected when the
auctions have different numbers of local bidders. To this end, we find the optimal bids that
maximise the utility given by Equation (10) using a standard optimisation technique (Press,
Flannery, Teukolsky, & Vetterling, 1992). The numerical results shown here assume the
962

fiOptimal Strategies for Simultaneous Vickrey Auctions with Perfect Substitutes

bidder valuation distribution to be uniform, although similar results are obtained with
other commonly used distributions. Two different scenarios are evaluated. In the first, we
investigate how the optimal bids change with increasing differences between the local bidder
numbers in the case of two simultaneous auctions (see Figure 4(a)). Here, the number of
local bidders in the first auction (n1 ) is fixed to 6 whereas the number of local bidders in
the second auction (n2 ) is varied between 6 and 15 in the second (note that we depict the
optimal strategy as a fraction of the valuation (bi /v) rather than the actual bid value since
this more clearly demonstrates the effect of preferred auctions). Figure 4(a) shows that
the global bidder bids higher in the preferred auction (i.e., the one with fewer bidders),
consistent with Lemma 8. As the valuation increases, the bid in the preferred auction first
decreases relative to the valuation, and then increases, similar to the case with the same
number of local bidders. However, the higher bid is now much closer to the true valuation,
especially if the difference between the auctions is large.
The simulation is extended to more auctions in the second scenario. The results for
two different settings are shown in Figure 4(b). In one setting (solid lines), each of the
seven auctions has a different number of local bidders. In the other settings (dashed lines)
the results are shown where 3 auctions have 5 local bidders each and the other 4 auctions
have 7 local bidders. As before, we observe that the global bidder always bids higher in
the auctions with fewer local bidders. Although the appearance of the bifurcation point is
common for simultaneous auctions with the same number of bidders, this is not the case
when the number of bidders is not identical. This is because the global bidder always bids
different amounts for auctions with different numbers of bidders. As a result, the bifurcation
phenomenon that indicates the transition from equal bids to high-low bids does not occur.
5.3 Sequential and Simultaneous Auctions
In this section we extend our analysis of the optimal bidding strategy to sequential auctions.
Specifically, the auction process consists of R rounds, and in each round any number of
auctions are running simultaneously. Such a combination of sequential and simultaneous
auctions is very common in practice, especially online.15 It turns out that the analysis for
the case of simultaneous auctions can be easily extended to include sequential auctions. In
the following, the set of simultaneous auctions in round r is denoted by M r , and the vector
of bids in that round by br . As before, the analysis assumes that all other bidders are
local and bid in a single auction. We initially assume that the global bidders have complete
knowledge about the number of rounds and the number of auctions in each round but we
then relax these assumptions.
The expected utility in round r, denoted by U r , is similar to before (Equation (1) in
Section 4.1) except that now additional benefit can be obtained from future auctions if the
desired item is not won in one of the current set of simultaneous auctions. For convenience,
U r (br , M r ) is abbreviated to U r in the following. The expected utility thus becomes:
15. Rather than being purely sequential in nature, online auctions also often overlap (i.e., new auctions
can start while others are still ongoing). In that case, however, it is optimal to wait and bid in the
new auctions only after the outcome of the earlier auctions is known, thereby reducing the chance
of unwittingly winning multiple items. Using this strategy, overlapping auctions effectively become
sequential and can thus be analysed using the results in this section.

963

fiGerding, Dash, Byde & Jennings

r

r

r

U = v  P (b ) 

X Z

iM r

=U

r+1

+ (v  U

r+1

bri



yg(y)dy + U r+1  (1  P r (br ))

0

r

r

)P (b ) 

X Z

iM r

bri



yg(y)dy ,
0

(16)

Q
where P r (br ) = 1  iM r (1  G(bri )) is the probability of winning at least one auction in
round r. Now, we take the partial derivative of Equation (16) in order to find the optimal
bid brj for auction j in round r:


r
Y
U
(1  G(bri ))  brj  .
(17)
= g(brj ) (v  U r+1 )
brj
r
iM \{j}

Note that Equation (17) is almost identical to Equation (4) in Section 4.3, except that
the valuation v is now replaced by v  U r+1 . The optimal bidding strategy can thus be
found by backward induction (where U R+1 = 0) using the procedure outlined in Section 4.3.
Now, we first relax the assumption that the global bidder has complete knowledge about
the number of auctions in future rounds. Let p(m) denote the probability that there are m
auctions in the next round and let mmax denote the maximum number of auctions. Furthermore, let Mj be the set of j auctions and U r (br , Mj ) the expected utility in round r when
there are j auctions in that round. The uncertainty about the number
can be inPmmaxof auctions
r+1
r+1
corporated into Equations (16) and (17) by replacing U
with j=0 p(j)U
(br+1 , Mj ).
Furthermore, uncertainty about the number of rounds can be addressed by adding a discount factor 0 <   1 which represents the probability that there are no more auctions after
this round. Finally, we note that the equation can be similarly extended to non-identical
auctions and to settings where G depends on the number of auctions and/or the round.

6. Market Efficiency
Efficiency is an important system-wide property since it characterises to what extent the
market maximises social welfare (i.e., the sum of utilities of all agents in the market). To
this end, in this section we study the efficiency of markets with either static or dynamic
local bidders, and the impact that a global bidder has on the efficiency in these markets.
Specifically, efficiency in this context is maximised when the bidders with the m highest
valuations (recall that m denotes the number of auctions, but also the total number of
items) in the entire market obtain a single item each. For simplicity we assume that the
total number of bidders in the market is at least m. More formally, we define the efficiency
of an allocation as:
Definition 1. Efficiency of Allocation. The efficiency K of an allocation K is the
obtained social welfare proportional to the maximum social welfare that can be achieved in
any type of market and is given by:
PnT
vi (K)
K = Pni=1
,
(18)
T

i=1 vi (K )
964

fiOptimal Strategies for Simultaneous Vickrey Auctions with Perfect Substitutes

P T
where K  = arg maxKK ni=1
vi (K) is an efficient allocation, K is the set of all possible
allocations (assuming bidders can be allocated to any auction), vi (K) is bidder is utility
for the allocation K  K, and nT is the total number of bidders participating in the market.

Now, in order to measure the efficiency of the market and the impact of a global bidder,
we run simulations for the markets with and without a global bidder and for the different
types of local bidders. The experiments are carried out as follows. Each bidders valuation
(both local and global) is independently drawn from a uniform distribution with support
[0, 1]. In the experiments without a global bidder, an additional local bidder is placed in one
of the auctions such that the overall number of bidders is the same on average compared
to the case with a global bidder. The local bidders bid their true valuations, whereas the
global bidder bids optimally in each auction as described in Section 4.3. The experiments are
repeated 10000 times in order to get statistically significant results with a 99% confidence
interval.
The results of these experiments are shown in Figure 5. Note that a degree of inefficiency
is inherent to a multi-auction market with only local bidders.16 For example, if there are
two auctions selling one item each, and the two bidders with the highest valuations both
bid locally in the same auction, then the bidder with the second-highest value does not
obtain the good. Thus, the allocation of items to bidders is inefficient, and this inefficiency
increases as the number of auctions increases (keeping the average number of bidders per
auction equal). As can be observed from Figure 5, however, the efficiency increases when
n becomes larger. This is because the differences between the bidders with the highest
valuations become smaller, thereby decreasing the loss of efficiency.
Furthermore, Figure 5 shows that if one of the local bidders is replaced by a global
bidder this generally creates a positive effect on the efficiency when the number of bidders
is small, but that no significant change occurs when there are many local bidders (this
holds both for static and dynamic local bidders). The latter comes as no surprise since the
impact of a single bidder diminishes as there are more bidders competing in an auction. The
former, on the other hand, is not obvious; the introduction of a global bidder potentially
leads to a decrease of efficiency since this bidder can unwittingly win more than one item.
Furthermore, a global bidder will generally bid below its true valuation which can also result
in inefficient outcomes. However, the results show that, on average, the opposite occurs.
This is because, when there is no global bidder and only few local bidders, there is a high
probability that a local bidder with a low valuation will win the item. A global bidder,
on the other hand, is likely to win that auction if it has a sufficiently high valuation (even
though the global bids are below the true valuation, as was shown in Section 4.3.4 bids are
often uniform and fairly close to the true value). This effect is particularly pronounced in
the case of dynamic local bidders since it may occur that an auction has no local bidder
whatsoever, in which case the global bidder wins the item for sure.
16. An exception is when n = 1 and bidders are static, since the market is then completely efficient without
a global bidder. However, since this is a very special case and does not apply to other settings, we do
not discuss it further here.

965

fiGerding, Dash, Byde & Jennings

efficiency (K )

1

0.95

2
1
4
6

0.9

m

5
3

1
2
3
4
5
6
7
8

8

0.85
7

0.8
2

Local
Bidders

Global
Bidder

2
2
2
2

Static
Static
Dynamic
Dynamic

No
Yes
No
Yes

6
6
6
6

Static
Static
Dynamic
Dynamic

No
Yes
No
Yes

4
6
8
10
12
(average) number of local bidders (n)

Figure 5: Average efficiency for different market settings as shown in the legend, where m
is the number of auctions (items), and n is the average number of local bidders
per auction. The error-bars indicate 99% confidence intervals.

7. Conclusions
In this paper, we derive utility-maximising strategies for an agent that has to bid in multiple,
simultaneous second-price auctions. We first analyse the case where a single global bidder
bids in all auctions, whereas all other bidders are local and bid in a single auction. For this
setting, we find that it is optimal to place non-zero bids in all auctions that sell the desired
item, even when a bidder requires only a single item and derives no additional benefit from
having more. Thus, a potential buyer can achieve considerable benefit by participating in
multiple auctions and employing an optimal bidding strategy.
For most common valuation distributions, we show analytically that the optimal bids
for identical auctions consist of at most two values, a high bid and a low bid, and that it
is optimal to bid high in at most one auction. Moreover, by writing the high bid in terms
of the low ones, the problem of finding the optimal bids reduces to optimising a single
variable. This considerably simplifies the original optimisation problem and can thus be
used in practice to compute the optimal bids for any number of auctions. Furthermore, we
show that, when the number of auctions becomes large, it is optimal to bid uniformly across
all auctions. We also analyse the setting where auctions are not identical and differ in their
probability of winning. Although it is still optimal to participate in all auctions, we find
that the best strategy is then to bid relatively high in more favourable auctions (i.e., where
the probability of winning is highest). We investigate other practical considerations as well.
We show that budget constraints limit the number of auctions that bidders participate in.
Specifically, if a global bidders budget is equal to or less than its valuation, the optimal
strategy reverts to bidding in a single auction under certain conditions, thereby justifying
966

fiOptimal Strategies for Simultaneous Vickrey Auctions with Perfect Substitutes

the presence of local bidders. Furthermore, we consider sequential auctions and show that
our results can be readily applied to markets where auctions occur both sequentially and
simultaneously. Finally, we compare the efficiency of a market with multiple simultaneous
auctions with and without a global bidder. We show that, if a local bidder is replaced by a
global one, this increases the average efficiency and thus social welfare of the sysstem when
the number of bidders is small, but that no significant effect is found if there are many local
bidders.
There are a number of interesting directions for future work. First of all, whereas this
paper focuses on the buyer, we intend to extend the analysis and consider the revenue
from the sellers point of view. This is particularly relevant when sellers have a more proactive role and can set auction parameters such as the reserve price to try and maximise
their expected revenue. This is closely related to the research on competing sellers, where
it is shown that the optimal auction parameters depend on the competition with other
sellers, since this affects the number of potential buyers that are attracted to a particular
auction (McAfee, 1993; Peters & Severinov, 1997; Burguet & Sakovics, 1999; Gerding et al.,
2007b). However, the current literature on competing sellers assumes that the buyers only
participate in a single auction, which is shown here to be suboptimal. The case where
sellers optimise auction parameters and buyers can participate in any number of auctions
simultaneously has so far not been investigated. Furthermore, this paper has taken a
decision-theoretic approach by analysing the case of a single global bidder. An interesting
open problem is characterising the game-theoretic solution in the case that multiple global
bidders interact strategically.
Acknowledgments
This paper has been significantly extended from a very preliminary version that was published previously (Gerding, Dash, Yuen, & Jennings, 2007a).
This research was undertaken as part of the EPSRC (Engineering and Physical Research
Council) funded project on Market-Based Control (GR/T10664/01). This is a collaborative
project involving the Universities of Birmingham, Liverpool and Southampton and BAE
Systems, BT and HP. This research was also undertaken as part of the ALADDIN (Autonomous Learning Agents for Decentralised Data and Information Systems) project and is
jointly funded by a BAE Systems and EPSRC strategic partnership. In addition, we would
like to thank Alex Rogers, Ioannis Vetsikas, Wenming Bian, and Florin Constantin for their
input. The authors are also very grateful to Adam Prugel-Bennett for his valuable help
with some of the proofs, and Richard Engelbrecht-Wiggans for the useful discussions and
comments. Finally, we would like to thank the reviewers for their thorough and detailed
feedback.

Appendix A. Proofs
A.1 Reduction of Search Space
Lemma 2. Under Assumption 1, if G (b) is non-decreasing in (0, vmax ), then the function
H(b) = b(1  G(b)) has a unique critical point bf in that interval.
967

fiGerding, Dash, Byde & Jennings

Proof At a critical point of H the following equation must hold:
d
d
H(b) =
[b(1  G(b))] = 1  bg(b)  G(b)
db
db 



1  G(b)
1
= g(b)
 b = g(b)
 b = 0.
g(b)
G (b)

(19)

Now, since g(b) > 0 (due to Assumption 1) , equality (19) only holds when 1/G (b)  b = 0.
Therefore, in order to show that (19) has at most one solution, it is sufficient to show
that 1/G (b)  b is either strictly increasing or strictly decreasing. However, since at the
boundaries b = 0 and b = vmax we have h(0) = 1 and h(vmax ) = vmax g(vmax ), and since
g(b) > 0, we only need to consider the latter. Since b is strictly decreasing, it is sufficient
to show 1/G (b) is non-increasing. By assumption we have that G (b) is non-decreasing,
and thus 1/G (b) is non-increasing.


Theorem 3. If F (b) is non-decreasing, then G (b) and Gb (b) are also non-decreasing,
b = en(F (y)1) for n  2.
where G(b) = F (b)n and G

Proof In the case of static local bidders we prove that
thus showing that
1G(b)
g(b)

g(b)
1G(b)

1G(b)
g(b)

is a non-increasing function,

is a non-decreasing function. In more detail, we can refactor

as follows:
1  F n (b)
1  G(b)
=
g(b)
nF n1 (b)f (b)



1  F (b) 1 + F (b) + . . . + F n1 (b)
=
f (b)
nF n1 (b)


1 1  F (b)
=
[1 + F 1 (b) + . . . + F 1n (b)].
n
f (b)

(20)

1F (b)
f (b)

is non-increasing

Due to the non-decreasing hazard rate of f (b), we can derive that
as follows:

b



1  F (b)
f (b)




=
v



f (b)
1  F (b)

h



1  F (b)
f (b)

2

i2
1F (b)
 0. Furthermore, the first order condition on the
f (b)
[1 + F 1 (b) + . . . + F 1n (b)] is F 2 (b)  . . .  (n  1)F n (b), is

since

0

second part of equation

20,
negative for all b. Thus,
this second part is strictly decreasing. Therefore, given that the first part is non-increasing
and the second part is strictly decreasing, it implies that overall 1G(b)
g(b) is strictly decreasing.
Hence G (b) is non-decreasing.
968

fiOptimal Strategies for Simultaneous Vickrey Auctions with Perfect Substitutes

In the case of dynamic local bidders, we can rewrite Gb (b) as:

g(b)
nf (b)en(F (b)1)
=
b
1  en(F (b)1)
1  G(b)
#

"
nf (b)
(1  F (b))en(F (b)1)
=
1  F (b)
1  en(F (b)1)



1  F (b)
nf (b)
.
=
1  F (b) en(1F (b))  1

(21)

Since F is non-decreasing, clearly the first part of equation (21) is non-decreasing. We shall
1F (b)
now prove that the second part of equation (21), (b) = en(1F
(b)) 1 , is also non-decreasing.
The first order condition on (b) is given by:
f (b)(en(1F (b))  1) + (1  F (b))(nf (b)en(1F (b)) )
(b)
=
.
b
(en(1F (b))  1)2

(22)

Since the denominator of equation (22) is always non-negative, it is thus sufficient to show
that the numerator is always non-negative. The numerator of equation (22) can be rewritten
as:
h
i
f (b)en(1F (b)) n  1  nF (b) + en(F (b)1) ,

in which the first and second terms are > 0 b. Thus, in order to prove
that the numerator iis
h
always non-negative, it remains to be shown that the third term, n1nF (b)+en(F (b)1) ,

is non-negative. This term is equal to n  1 + en > 0 (since n > 1) and 0 at the extremums
bmin and bmax respectively. Furthermore, the first order differential on this term yields
nf (b)[1  en(F (b)1) ] < 0, b. Hence the third term is also > 0, b.

Lemma 3. If H(b) = b(1  G(b)) has a unique critical point then
b > bf



g(b) > (1  G(b))/b,

b < bf



g(b) < (1  G(b))/b.

and similarly

Proof As we saw already, if H has a unique critical point then b > bf iff H is decreasing
at b. Therefore
b > bf  H  (b) < 0  1  G(b)  bg(b) < 0  g(b) > (1  G(b))/b.
The other result follows analagously.



Theorem 4. Under Assumptions 1 and 2, the global bid b that maximises Equation 1
has at most one high bid, i.e., at most one i  M for which bi > bf , where bf is the unique
critical point of H(b) = b(1  G(b)).
969

fiGerding, Dash, Byde & Jennings

Proof The second derivatives of U at an interior critical point b are as follows:
2U
b2i
2U
bi bj


 Y
= g  (bi ) v (1  G(bj ))  bi  g(bi ) = g(bi ),
j6=i

= g(bi )g(bj )v

Y

(1  G(bk ))

k6=i,j

g(bj )bj
= g(bi )
.
1  G(bi )

(23)

The proof is by contradiction: we show that if there are two high bids then the critical
point cannot be a local maximum of U . This is done by showing that if there are two high
bids at b, then the Hessian matrix with entries (23) has a positive eigenvalue, or equivalently
that there exists a vector a = (a1 , a2 , . . . , am ) for which
X

ai aj

i,j

2U
(b) > 0.
bi bj

(24)

If we can show that the Hessian matrix at b has a positive eigenvalue, this means that b is
either a local minimum or a saddle point (Magnus & Neudecker, 1999, Chapter 6), which
in turn means that a small enough displacement in the direction of a leads to an increase
in U , contradicting the local optimality of b.
Assume without loss of generality that the auctions are rearranged such that b1 = b2 =
b+ > bf . From Lemma 3 this implies that g(b+ ) > (1  G(b+ ))/b+ . Then we choose
a = (1, 1, 0, . . . , 0), so that (24) becomes:
X
i,j

ai aj

2U
(b) =
bi bj

2U
2U
2U
(b)
+
(b)

2
(b)
b2
b1 b2
b21


g(b+ )b+
= 2g(b+ )
1
1  G(b+ )
> 0.


A.2 Limit Cases
In this section we examine a global bidders expected utility when the number of auctions
goes to infinity. In particular, we prove that for a large enough number of auctions, the
optimal behaviour is always to bid uniformly.
First of all, note from (5) and the definition of the optimal uniform bid bu
m , that
u m1
bu
.
m = v(1  G(bm ))

(25)

Lemma 4. The smallest bid b in the optimal (possibly non-uniform) bid vector b tends
to 0 uniformly in v as m  .
Rb
Proof This follows from the fact that the expected payment function EP (b) = 0 xg(x) dx
is a strictly monotonically increasing continuous function of b, and hence has a monotonically
970

fiOptimal Strategies for Simultaneous Vickrey Auctions with Perfect Substitutes

increasing continuous inverse, EP 1 : [0, EP (vmax )]  [0, vmax ] with EP 1 (0) = 0.
0 < U (b)
=

b

<
<
<

v  mEP (b )

EP 1 v/m


EP 1 vmax /m

 0

as m  

Our first step towards proving that uniform bidding is eventually globally optimal is to
show that the utility from optimal bidding converges to its maximum value of v:

Theorem 5. Under Assumption 1, the expected utility as defined by Equation 1 from
playing the optimal uniform bid bu
m converges to v, in the sense that for all  > 0 there is
a constant m such that m > m implies U (bu
m ) > v  .
Proof First of all note that the result is trivial for v <  since U (bu
m )  0 always. From
the definition of utility (1), and using (25),


u m
U (bu
)
=
v
1

(1

G(b
))
 mEP (bu
m
m
m)
u
u
= v  bu
m (1  G(bm ))  mEP (bm ).

(26)

u
u
From Lemma 4 we know that bu
m  0, which implies bm (1  G(bm ))  0, so we turn our
attention to the expected payment term, showing that it tends to zero as m  . From
the definition (25) of bu
m,
ln(bm /v)
m=
+ 1,
ln(1  G(bm ))
so that


ln(bu
m /v)
u
lim mEP (bm ) = lim
+ 1 EP (bu
m)
m
m ln(1  G(bu
))
m
u
ln(bu
m /v)EP (bm )
.
(27)
= lim
m ln(1  G(bu
m ))

To prove that the above limit is zero we demonstrate the stronger limit
ln(b/v)EP (b)
= 0,
b0 ln(1  G(b))
lim

(28)

which we do by applying LHopitals rule multiple times. First we apply it to the numerator
of (28):


EP (b)
lim ln(b/v)EP (b)
= lim
b0
b0 (ln(b/v))1
bg(b)
= lim 1
b0 b (ln(b/v))2


= lim bg(b) b(ln(b/v))2
b0

= 0,
971

fiGerding, Dash, Byde & Jennings

where the last limit holds uniformly on   v  vmax .
Now that the numerator has been shown to converge to zero, it is possible to apply
LHopitals rule directly to (28),
ln(b/v)EP (b)
b0 ln(1  G(b))
lim

b1 EP (b) + g(b)b ln(b/v)
b0
g(b)(1  G(b))1


EP (b)
+ b ln(b/v)
=  lim
b0
bg(b)


bg(b)
=  lim
+ b ln(b/v) .
b0 g(b) + bg  (b)
= lim

(29)
(30)

g  (b)  0 for all small enough b in order for g(b)  0 to be true everywhere, which implies
that
bg(b)
< b  0,
g(b) + bg  (b)
and the limit b ln(b/v)  0 is obvious, so the limit in (30) must be zero. This in turn implies
that the expected payment from uniform bidding tends to zero, while the expected value
tends to v, and thus the theorem is proved.

A.3 Budget Constraints
In what follows we provide the proof for the proposition that the optimal strategy is to bid
in a single auction under certain conditions, if the exposure is constrained to at most its
valuation. The proof for any number of auctions is complex. Therefore, we first provide a
formal proof for the case of two auctions, and then generalise the result to more than two.
The proof for the two-auction case also provides a building block for the inductive proofs
contained in the more general case.
First we show that the unconstrained optimum bid eventually exceeds any given budget:

Theorem 6. Under Assumption 1 and assuming that g(b) is bounded throughout [0, vmax ],
for all v > 0 and for any C, there exists an m forP
which the exposure of the optimal global
bid b maximising Equation (1) exceeds C, i.e., iM bi > C.

Proof To begin with, note that the conditions of Corollary 1 are met, so that in fact
we can restrict attention to uniform bidding when m is sufficiently large, in particular as
m  . What we then show is that for all v > V > 0 and any C > 0 there is an m such
that mbu
m > C.
The proof is by contradiction. Assume that there is a constant C such that bu
mm < C
u < KC/m, so that (using
for all m. By the bounded density assumption, G(bu
)

Kb
m
m
Equation (25)):
bu
m

=

m1
v(1  G(bu
m ))

>

v(1  KC/m)m1

>

V (1  KC/m)m1

 V eKC
972

as m  .

(31)

fiOptimal Strategies for Simultaneous Vickrey Auctions with Perfect Substitutes

u
The fact that bu
m is thus bounded below contradicts the bound bm m < C.



The following Lemma characterises the bidding strategy when the budget is C  v and
m = 2. This result is then used in Theorem 7 to prove the more general case where m  2.
Lemma 5. For m = 2 the optimal bidding strategy of a global bidder with budget C  v
is given by bc = (C, 0) when the probability density function g(x) is convex and g(0) = 0.
Proof In order to prove the lemma, we consider the difference between the optimal bid
bc and an arbitrary bid b = b1 , b2 where b 6= bc and b1 + b2 = C:
Z C
h 

yg(y)dy  v 1  (1  G(b1 ))(1  G(b2 ))
U (bc , v)  U (b , v) = vG(C) 
0
Z b1
Z b2
i

yg(y)dy 
yg(y)dy .
0

0

Hence,
U (bc , v)  U (b , v) = (v  C)G(C) +

Z

C

G(y)dy  vG(b1 )  vG(b2 ) + b1 G(b1 )
Z b1
Z b2
+ vG(b1 )G(b2 ) 
G(y)dy + b2 G(b2 ) 
G(y)dy
0
0
Z C
= (v  C)(G(C)  G(b1 )  G(b2 )) +
G(y)dy
0

b2

 (C  b1 )G(b1 )  (C  b2 )G(b2 ) 

Z

b1

G(y)dy

(32)

0

+ vG(b1 )(G(b2 )
= (v  C)(G(C)  G(b1 )  G(b2 )) +
 b1 G(b2 ) 

Z

b1

Z

C

G(y)dy  b2 G(b1 )
b2

G(y)dy + vG(b1 )G(b2 ).

0

Now, to prove that bc is indeed the optimal bid, we need to show that the above difference
is positive for any b that satisfies the budget constraint. RIn order to do so, we first separate
C
the above equation into two parts, namely, X(b1 , b2 ) = b2 G(y)dy  b2 G(b1 )  b1 G(b2 ) 
R b1
0 G(y)dy and (v  C)Y where Y (b1 , b2 ) = (G(C)  G(b1 )  G(b2 )). We now show that
both X and Y are positive; thus implying that, since (v  C) is positive, U (bc , v)  U (b , v)
is also positive.
Now, we rewrite X in terms of b1 only by replacing b2 with C  b1, and X becomes:
X(b1 ) =

Z

C

G(y)dy  (C  b1 )G(b1 )  b1 G(C  b1 ) 
Cb1

Z

0

973

b1

G(y)dy.

(33)

fiGerding, Dash, Byde & Jennings

In order to find the local maxima and minima we set the derivative

X
b1

to zero:

X
= G(C  b1 )  (C  b1 )g(b1 ) + G(b1 ) + b1 g(C  b1 )  G(C  b1 )  G(b1 )
b1
= b1 g(C  b1 )  (C  b1 )g(b1 ) = 0.

(34)

Since g(0) = 0, it is easy to see that there are at least three solutions to Equation (34),
namely b1 = {0, C, C/2}. We shall now show that the first derivative of X is always
non-negative within the range b1 = [0, C/2], implying that X increases within this range.
Since X(0) = 0 and the solution is symmetric around C/2, it follows that X is always
non-negative. More formally, we need to show that:
X
 0, b1  [0, C/2].
b1
We now prove that this holds if g(x) is convex. Now, from the definition of a convex
function we have:
g(x1 + (1  )x2 ))  g(x1 ) + (1  )g(x2 ),   1.
Now, let x2 = 0. Then:
g(x1 )  g(x1 ) + (1  )g(0).
Since g(0) = 0, this becomes:
g(x1 )  g(x1 ) 
1
g(x1 )  g(x1 ).

Let x1 = (C  b1 ) and  =

b1
Cb1 .

(35)

Then, Equation (35) becomes:

C  b1
g(b1 )  g(C  b1 ) 
b1
b1 g(C  b1 )  (C  b1 )g(b1 )  0.

(36)

X
Thus, this shows that b
 0, which, in turn, proves that X  0.
1
We now prove that Y = G(C)  G(b1 )  G(b2 )  0. Since we assume that g(x) is convex
and positive Rin the interval [0, 1] and that g(0) = 0, it follows that g(x) is increasing and
x
thus G(x) = 0 g(x)dx is also convex. As a result, we can use the condition in Equation (35)
x
1 +b2 )
for G(x). Now, by replacing  by b1 +b
and x1 by b1 + b2 , we have G(x)  G(b
b1 +b2 x for
2
any x  b1 + b2 and it follows that:

G(b1 + b2 )
G(b1 + b2 )
+ b2

b1 + b2
b1 + b2


b1
b2
G(b1 ) + G(b2 )  G(b1 + b2 )
+

b1 + b2 b1 + b2
G(b1 ) + G(b2 )  G(b1 + b2 ).

G(b1 ) + G(b2 ) b1

974

fiOptimal Strategies for Simultaneous Vickrey Auctions with Perfect Substitutes

Hence, Y = G(C)  G(b1 )  G(b2 )  0. As a result, U (b , v)  U (b , v)  0 for any
b1 + b2 = C. Note that so far we have assumed that the sum of bids is equal to the budget
constraint, and we have not mentioned the case where b1 + b2 < C. We now show that it is
optimal to bid the full budget (i.e., b1 + b2 = C).
Consider any arbitrary B  = b1 , b2 where b1 + b2 = S  C. Then, by replacing C with
S in the above, we know that U ((S, 0), v)  U ((b1 , b2 ), v). Hence, it remains to be shown
that U ((C, 0), v)  U ((S, 0), v). We again consider the difference between these two bids,
which from Equation (1) is:
Z S
Z C
yg(y)dy
yg(y)dy  vG(S) +
U ((C, 0), v)  U ((S, 0), v) = vG(C) 
0
0


Z C
G(y)dy
= v (G(C)  G(S))  CG(C) 
0


Z S
+ SG(S) 
G(y)dy
0

= (v  C)(G(C)  G(S)) +
 (C  S)G(S).

Z

C

G(y)dy
S

Now, G(C)  G(S) is positive since S  C by definition and G(x) is non decreasing and,
RC
therefore, S G(y)dy  (C  S)G(S) is also positive. Hence, U ((C, 0), v)  U ((S, 0), v)  0.

We have thus shown that a global bidder will bid in only one auction if its budget is
constrained such that C  v when bidding in two simultaneous auctions, provided that
g(x) is convex and g(0) = 0 This results can now be generalised to the case that m  2. In
more detail, the following theorem holds:
Theorem 7. Under Assumptions 1 and 3, if the global bidder has a budget C  v, then
Equation (7) is satisfied for bc = (C, 0, . . . , 0), i.e., it is optimal to bid C in exactly one
auction.
c
Proof As before, we consider the difference
P between the optimal bid b and another bid


c
b = (b1 , . . . , bm ) where b 6= b and C = iM bi , and show that this is positive:

U (bc , v)  U (b , v) =vG(C) 


Z

XZ

iM

C

yg(y)dy 

0
bi

yg(y)dy
0

=(v  C)G(C) +

Z

!

C

+

bi G(bi ) 

iM

975

iM

h

G(y)dy  v 1 

0

X

h
i
Y
v 1
(1  G(bi ))

Z

bi



G(y)dy .
0

Y

iM

i
(1  G(bi ))

(37)

fiGerding, Dash, Byde & Jennings

By inserting (v  C)
rewritten as:

P

G(bi )  (v  C)

iM

c

P

iM

G(bi ) into Equation (37) this can be
X



U (b , v)  U (b , v) = (v  C) G(C) 

!

G(bi )

iM

C

X

iM

h

v 1

= (v  C) G(C) 


X

X Z

G(bi ) 
Y

(1  G(bi )) 

iM

X

G(bi )

iM

(C  bi )G(bi ) +

Z

+

Z

X

i
G(bi )

C

G(y)dy

0

bi

G(y)dy

0

iM

G(y)dy

0

iM

!

C


G(y)dy  bi G(bi )

bi

0

iM

+

Z



h
i
Y
X
v 1
(1  G(bi )) 
G(bi ) .
iM

iM

Now, define the variables Xm and Zm as:
Z
Z C
X
G(y)dy 
(C  bi )G(bi ) +
Xm =
0

h
Y
Xi
Zm =  1 
(1  G(bi )) 
.



G(y)dy ,

0

iM

iM

bi

iM

Then, we can rewrite Equation (37) as:
c



U (b , v)  U (b , v) = (v  C) G(C) 

X

!

G(bi )

iM

+ Xm + vZm .

(38)

P
We now prove that each part of the equation is positive
P (i.e., (v C)(G(C) iM G(bi )) 
0, Xm  0, vZ  0). We first prove that G(C)  iM G(bi )  0 using the relationship in
P
Equation (35) for convex functions. Specifically, by taking  = bCi and since C = iM bi
this yields:
C
G(bi ) 
bi
X
X
bi G(C) 
CG(bi ) 
G(C) 

P

iM

iM

X
iM bi
G(C) 
G(bi ) 
C
iM
X
G(C) 
G(bi ).
iM

976

fiOptimal Strategies for Simultaneous Vickrey Auctions with Perfect Substitutes

Since, by definition of the case being studied, v  C, we have (vC)(G(C)
0.

P

iM

G(bi )) 

P We now prove that Xm  0 using an inductive argument. In more detail, let S =
iM \{m} bi = C  bm . Then Xm can be written in terms of Xm1 as follows:
Xm =
=

Z

Z

X

C

G(y)dy 
0

(C  bi )G(bi ) +

iM
S

G(y)dy 
0

X

iM \{m}

bi

G(y)dy

0


Z
(S  bi )G(bi ) +

(C  bm )G(bm )  (C  S)
= Xm1 +

Z

X

C

0

Z

Z

C

P

iM \{m} G(bi ),

G(y)dy

S

bm

G(y)dy
0

X

G(bi ) 

iM \{m}

Since G(S) = G(C  bm ) 
Z



G(y)dy +

G(y)dy  (C  bm )G(bm )  bm

S

Xm  Xm1 +

bi

G(bi ) 

iM \{m}

Z



Z

bm

G(y)dy.

0

then :

C

G(y)dy  (C  bm )G(bm )  bm G(C  bm ) 

Cbm

Z

bm

G(y)dy.
0

RC
From Lemma 5, we have shown that Cbm G(y)dy  (C  bm )G(bm )  bm G(C  bm ) 
R bm
0 G(y)dy  0 and therefore Xm  Xm1 . The base case is X2 which has been shown
within the proof of Lemma 5 to be positive. Hence Xm  0.
We again use an inductive argument to finally prove that Zm  0. The base case of Zm
is when m = 2 which yields:
h
i
Y
X
Z2 =  1 
(1  G(bi )) 
G(bi )
iM

iM

h
i
=  1  (1  G(b1 ))(1  G(b2 ))  G(b1 )  G(b2 )

= G(b1 )G(b2 )
 0.

The inductive hypothesis is then formulated as Zm  0 if Zm1  0. In order to prove this
hypothesis, we express Zm as a function of Zm1 :
977

fiGerding, Dash, Byde & Jennings

Zm =

Y

(1  G(bi )) +

iM

X

G(bi )  1

iM

= (1  G(bm ))

Y

(1  G(bi )) +

iM

iM \{m}



= (1  G(bm )) Zm1 

X

iM \{m}

= Zm1  G(bm )Zm1 + G(bm )


= Zm1 + G(bm ) 


X

iM \{m}

= Zm1 + G(bm ) 1 



G(bi ) + 1 +
X

iM \{m}

X

G(bi )  1
X

G(bi )  1

iM

G(bi )


G(bi )  Zm1 

Y

iM \{m}

 Zm1 .



(1  G(bi ))

The above thus proves the inductive step, which along with the base case, thereby proves
that Zm  0. Hence the third part of Equation (38) is also positive. Since all three parts
of this equation are positive, this impliesPthat U (bc , v) is indeed optimal when bc =
(C, 0, . . . , 0). Note that we assumed C = iM bi . However, using the same argument as
in Lemma 5 it is easy to see that it is indeed optimal to bid the full budget.


Corollary 2. Under Assumption 1, if either g(0) > 0 or g(x) is strictly concave, and the
global bidder has a budget C = v, then Equation (7) is satisfied by bidding strictly positive
in at least two auctions.
Proof The proof is largely based on the reverse arguments from Lemma 5. Without loss of
generality we take m = 2. Let bL = {C, 0} denote the single-auction bid and b = {b1 , b2 }
an arbitrary bid such that b 6= bL and b1 + b2 = C, b1 , b2  0 as before. Since v  C = 0
we have U (bL , v)  U (b , v) = X(b1 ), where X is given by Equation (33). Furthermore,
X
b1 = b1 g(C  b1 )  (C  b1 )g(b1 ) (see Equation (34)). Now, in order to prove that it is
optimal to bid strictly positive in both auctions, it is sufficient to show that there exists a
C > b1 > 0 such that X(b1 ) < 0 when one of the two conditions holds.
X
We first show that this holds when g(0) > 0. It is easy to see that b
(0) = Cg(0) < 0
1
and X(0) is thus strictly decreasing. Since X(0) = 0 this proves that X(b1 ) < 0 for b1
slightly larger than zero. Since b2 = C  b1 it follows that b2 > 0 as long as b1 < C, and an
agent can thus do better by bidding in both auctions.
We now consider the case that g(0) = 0 but g(x) is strictly concave. By replacing the
condition for convex functions in Lemma 5 with that of strictly concave functions it follows
b1
that 1 g(x1 ) > g(x1 ) for 12 C > x1 > 0. By setting x1 = (C  b1 ) and  = Cb
as
1
978

fiOptimal Strategies for Simultaneous Vickrey Auctions with Perfect Substitutes

before, this gives (see Equations (35) and (36))
b1  (0, C/2).

X
b1

= b1 g(C  b1 )  (C  b1 )g(b1 ) < 0 for


References
Barlow, R. E., Marshall, A. W., & Proschan, F. (1963). Properties of probability distributions with monotone hazard rate. The Annals of Mathematical Statistics, 34 (2),
375389.
Bergstrom, T., & Bagnoli, M. (2005). Log-concave probability and its applications. Economic Theory, 26 (2), 445469.
Boutilier, C., Goldszmidt, M., & Sabata, B. (1999). Sequential auctions for the allocation
of resources with complementarities. In Proceedings of the 16th International Joint
Conference on Artificial Intelligence, pp. 527523.
Burden, R. L., & Faires, J. D. (2004). Numerical Analysis, 8th edition. Brooks Cole.
Burguet, R., & Sakovics, J. (1999). Imperfect competition in auction design. International
Economic Review, 40 (1), 231247.
Byde, A., Preist, C., & Jennings, N. R. (2002). Decision procedures for multiple auctions.
In Proceedings of the 1st International Joint Conference on Autonomous Agents and
Multi-Agent Systems, pp. 613620.
Che, Y. K., & Gale, I. (1998). Standard auctions with financially constrained bidders.
Review of Economic Studies, 65 (1), 121.
Clearwater, S. H. (Ed.). (1996). Market-Based Control: A Paradigm for Distributed Resource Allocation. World Scientific Publishing.
Dash, R. K., Parkes, D. C., & Jennings, N. R. (2003). Computational mechanism design:
A call to arms. IEEE Intelligent Systems, 18 (6), 4047.
Dash, R. K., Rogers, A., Reece, S., Roberts, S., & Jennings, N. R. (2005). Constrained
bandwidth allocation in multi-sensor information fusion: A mechanism design approach. In Proceedings of the 8th International Conference on Information Fusion,
pp. 11851192.
Dash, R. K., Vytelingum, P., Rogers, A., David, E., & Jennings, N. R. (2007). Marketbased task allocation mechanisms for limited capacity suppliers. IEEE Transactions
on Systems, Man and Cybernetics: Part A, 37 (3), 391405.
Engelbrecht-Wiggans, R. (1987). Optimal constrained bidding. International Journal of
Game Theory, 16 (2), 115121.
Engelbrecht-Wiggans, R., & Weber, R. (1979). An example of a multiobject auction game.
Management Science, 25, 12721277.
979

fiGerding, Dash, Byde & Jennings

Gerding, E. H., Dash, R. K., Yuen, D. C. K., & Jennings, N. R. (2007a). Bidding optimally
in concurrent second-price auctions of perfectly substitutable goods. In Proceedings
of the 6th International Joint Conference on Autonomous Agents and Multi-Agent
Systems, pp. 267274.
Gerding, E. H., Rogers, A., Dash, R. K., & Jennings, N. R. (2007b). Sellers competing for
buyers in online markets: Reserve prices, shill bids, and auction fees. In Proceedings
of the 20th International Joint Conference on Artificial Intelligence, pp. 12871293.
Gopal, R., Thompson, S., Tung, Y. A., & Whinston, A. B. (2005). Managing risks in
multiple online auctions: An options approach. Decision Sciences, 36 (3), 397425.
Greenwald, A., & Boyan, J. (2004). Bidding under uncertainty: Theory and experiments.
Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence, 209216.
Greenwald, A., Kirby, R. M., Reiter, J., & Boyan, J. (2001). Bid determination in simultaneous auctions: A case study. In Proceedings of the 3rd ACM Conference on
Electronic Commerce, pp. 115124.
Hendricks, K., Onur, I., & Wiseman, T. (2005). Preemption and delay in eBay auctions.
Working Paper, University of Texas at Austin.
Jiang, A. X., & Leyton-Brown, K. (2007). Bidding agents for online auctions with hidden
bids. Machine Learning, 67 (1), 117143.
Juda, A. I., & Parkes, D. C. (2006). The sequential auction problem on eBay: An empirical
analysis and a solution. In Proceedings of the 7th ACM Conference on Electronic
Commerce, pp. 180189.
Krishna, V. (2002). Auction Theory. Academic Press.
Krishna, V., & Benoit, J. P. (2001). Multiple object auctions with budget constrained
bidders. Review of Economic Studies, 68 (1), 155179.
Krishna, V., & Rosenthal, R. (1996). Simultaneous auctions with synergies. Games and
Economic Behavior, 17, 131.
Lang, K., & Rosenthal, R. (1991). The contractors game. RAND Journal of Economics,
22, 329338.
Leyton-Brown, K., Shoham, Y., & Tennenholtz, M. (2000). Bidding clubs: Institutionalized collusion in auctions. In Proceedings of the 2nd ACM Conference on Electronic
Commerce, pp. 253259. ACM Press New York, NY, USA.
Magnus, J. R., & Neudecker, H. (1999). Matrix Differential Calculus with Applications in
Statistics and Econometrics (2nd edition). John Wiley & Sons.
McAfee, R. P. (1993). Mechanism design by competing sellers. Econometrica, 61 (6), 1281
1312.
980

fiOptimal Strategies for Simultaneous Vickrey Auctions with Perfect Substitutes

Mes, M., van der Heijden, M., & van Harten, A. (2007). Comparison of agent-based scheduling to look-ahead heuristics for real-time transportation problems. European Journal
of Operational Research, 181 (1), 5975.
Palfrey, T. R. (1980). Multi-object, discriminatory auctions with bidding constraints: A
game-theoretic analysis. Management Science, 26 (9), 935946.
Peters, M., & Severinov, S. (1997). Competition among sellers who offer auctions instead
of prices. Journal of Economic Theory, 75, 141179.
Peters, M., & Severinov, S. (2006). Internet auctions with many traders. Journal of Economic Theory, 130 (1), 220245.
Pitchik, C. (2006). Budget-constrained sequential auctions with incomplete information.
Working Paper 230, Department of Economics, University of Toronto.
Press, W. H., Flannery, B. P., Teukolsky, S. A., & Vetterling, W. T. (1992). Numerical
Recipes in C: The Art of Scientific Computing (2nd edition). Cambridge University
Press.
Rogers, A., David, E., & Jennings, N. R. (2005). Self organised routing for wireless microsensor networks. IEEE Transactions on Systems, Man and Cybernetics: Part A,
35 (3), 349359.
Rogers, A., David, E., Schiff, J., & Jennings, N. R. (2007). The effects of proxy bidding
and minimum bid increments within eBay auctions. ACM Transactions on the Web,
1 (2), article 9, 28 pages.
Rosenthal, R., & Wang, R. (1996). Simultaneous auctions with synergies and common
values. Games and Economic Behavior, 17 (1), 3255.
Roth, A. E., & Ockenfels, A. (2002). Last-minute bidding and the rules for ending secondprice auctions: Evidence from eBay and Amazon auctions on the Internet. The American Economic Review, 92 (4), 10931103.
Rothkopf, M. H. (1977). Bidding in simultaneous auctions with a constraint on exposure.
Operations Research, 25 (4), 620629.
Rothkopf, M. H. (2007). Decision analysis: The right tool for auctions. Decision Analysis,
4 (3), 167172.
Shaked, M., & Shanthikumar, J. G. (1994). Stochastic Orders and Their Applications.
Academic Press.
Shehory, O. (2002). Optimal bidding in multiple concurrent auctions. International Journal
of Cooperative Information Systems, 11, 315327.
Stone, P., Schapire, R. E., Littman, M. L., Csirik, J. A., & McAllester, D. (2003). Decisiontheoretic bidding based on learned density models in simultaneous, interacting auctions. Journal of Artificial Intelligence Research, 19, 513567.
981

fiGerding, Dash, Byde & Jennings

Stryszowska, M. (2004).
Late and multiple bidding in competing second price.
Working paper 2004.16, Fondazione Eni Enrico Mattei.
Available at
http://ideas.repec.org/p/fem/femwpa/2004.16.html.
Szentes, B., & Rosenthal, R. (2003). Three-object two-bidder simultaneous auctions: Chopsticks and tetrahedra. Games and Economic Behavior, 44, 114133.
Varian, H. R. (1995). Economic mechanism design for computerized agents. In Proceedings
of the 1st USENIX Workshop on Electronic Commerce, pp. 1321.
Wellman, M. P., Greenwald, A., & Stone, P. (2007). Autonomous Bidding Agents: Strategies
and Lessons from the Trading Agent Competition. MIT Press.
Wellman, M. P., Reeves, D. M., Lochner, K. M., & Vorobeychik, Y. (2004). Price prediction
in a trading agent competition. Journal of Artificial Intelligence Research, 21, 1936.
Yuen, D., Byde, A., & Jennings, N. R. (2006). Heuristic bidding strategies for multiple
heterogeneous auctions. In Proceedings of the 17th European Conference on Artificial
Intelligence, pp. 300304.
Zeithammer, R. (2005). An equilibrium model of a dynamic auction marketplace. Working
Paper, University of Chicago.

982

fiJournal of Artificial Intelligence Research 32 (2008) 203-288

Submitted 11/07; published 05/08

New Islands of Tractability of Cost-Optimal Planning
Michael Katz,
Carmel Domshlak,

dugi@tx.technion.ac.il,
dcarmel@ie.technion.ac.il

Faculty of Industrial Engineering and Management,
Technion - Israel Institute of Technology, Haifa, Israel

Abstract
We study the complexity of cost-optimal classical planning over propositional state
variables and unary-effect actions. We discover novel problem fragments for which such optimization is tractable, and identify certain conditions that differentiate between tractable
and intractable problems. These results are based on exploiting both structural and syntactic characteristics of planning problems. Specifically, following Brafman and Domshlak
(2003), we relate the complexity of planning and the topology of the causal graph. The
main results correspond to tractability of cost-optimal planning for propositional problems
with polytree causal graphs that either have O(1)-bounded in-degree, or are induced by
actions having at most one prevail condition each. Almost all our tractability results are
based on a constructive proof technique that connects between certain tools from planning
and tractable constraint optimization, and we believe this technique is of interest on its
own due to a clear evidence for its robustness.

1. Precis
AI problem solving is inherently facing a computational paradox. On the one hand, most
general tasks of AI reasoning are known to be very hard, and this to a degree that membership in NP by itself is sometimes perceived as good news. On the other hand, if the
intelligence is somehow modeled by a computation, and the computation is delegated to
computers, then artificial intelligence has to escape the traps of intractability as much as
possible. Planning is one of such reasoning tasks, corresponding to finding a sequence of
state-transforming actions that achieve a goal from a given initial state. It is well known that
planning is intractable in general (Chapman, 1987), and that even the simple classical
planning with propositional state variables is PSPACE-complete (Bylander, 1994).
While there were ups and downs in the interest of the planning community in the formal
complexity analysis of planning problems, it is of a growing understanding these days that
computational tractability is a fundamental issue in all problem solving. The pragmatic
reasons for that are twofold.
1. Many planning problems in the manufacturing and other process controlling systems
are believed to be highly structured, thus have a potential to allow for efficient planning if exploiting this structure (Klein, Jonsson, & Backstrom, 1998). In fact, if
this structure is not accounted for explicitly, a general-purpose planner is likely to
go on tour in an exponential search space even for tractable problems. Moreover,
since intractable theories provide no guarantees about the performance of engineering systems, in cases where such guarantees are required it is unavoidable to design
c
2008
AI Access Foundation. All rights reserved.

fiKatz & Domshlak

the controlled system in a complexity-aware manner so that planning for it will be
provably tractable (Williams & Nayak, 1996, 1997).
2. Computational tractability can be an invaluable tool even for dealing with problems
that fall outside all the known tractable fragments of planning. For instance, tractable
fragments of planning provide the foundations for most (if not all) rigorous heuristic
estimates employed in planning as heuristic search (Bonet & Geffner, 2001; Hoffmann,
2003; Helmert, 2006; Hoffmann & Nebel, 2001; Edelkamp, 2001). This is in particular
true for admissible heuristic functions for planning that are typically defined as the
optimal cost of achieving the goals in an over-approximating abstraction of the planning problem in hand. Such an abstraction is obtained by relaxing certain constraints
in the specification of the original problem, and the purpose of the abstraction is
to provide us with a provably tractable abstract problem (Haslum, 2006; Haslum &
Geffner, 2000; Haslum, Bonet, & Geffner, 2005).
Unfortunately, the palette of known tractable fragments of planning is still very limited,
and the situation is even more severe for tractable optimal planning. To our knowledge,
just less than a handful of non-trivial fragments of optimal planning are known to be
tractable. While there is no difference in theoretical complexity of regular and optimal
planning in the general case (Bylander, 1994), many of the classical planning domains are
provably easy to solve, but hard to solve optimally (Helmert, 2003). Practice also provides
a clear evidence for strikingly different scalability of satisficing and optimal general-purpose
planners (Hoffmann & Edelkamp, 2005).
In this work we show that the search for new islands of tractability of optimal classical
planning is far from being exhausted. Specifically, we study the complexity of optimal
planning for problems specified in terms of propositional state variables, and actions that
each changes the value of a single variable. In some sense, we continue the line of complexity
analysis suggested by Brafman and Domshlak (2003), and extend it from satisficing to
optimal planning. Our results for the first time provide a dividing line between tractable
and intractable such problems.
1.1 The UB (Optimal) Planning Problems
Problems of classical planning correspond to reachability analysis in state models with
deterministic actions and complete information. In this work we focus on state models
describable in a certain fragment of the SAS+ formalism (Backstrom & Nebel, 1995) that
allows only for propositional state variables and unary-effect actions. Following Backstrom
and Nebel (1995), in what follows we refer to this subclass of SAS+ as UB (short for
unary-effect, binary-valued). Somewhat surprisingly, even non-optimal planning for UB
is PSPACE-complete, that is, as hard as general propositional planning (Bylander, 1994).
Definition 1 A SAS+ problem instance is given by a quadruple  = hV, A, I, Gi, where:
 V = {v1 , . . . , vn } is a set of state variables, each associated with a finite domain
Dom(vi ); the initial state I is a complete assignment, and the goal G is a partial
assignment to V , respectively.
204

fiTractable Cost-Optimal Planning

 A = {a1 , . . . , aN } is a finite set of actions, where each action a is a pair hpre(a), eff(a)i
of partial assignments to V called preconditions and effects, respectively. Each action
a  A is associated with a non-negative real-valued cost C(a). An action a is applicable
in a state s  Dom(V ) iff s[v] = pre(a)[v] whenever pre(a)[v] is specified. Applying
an applicable action a changes the value of each variable v to eff(a)[v] if eff(a)[v] is
specified.
A SAS+ problem instance belongs to the fragment UB of SAS+ iff all the state
variables in V are binary-valued, and each action changes the value of exactly one variable,
that is, for all a  A, we have |eff(a)| = 1.
Different sub-fragments of UB can be defined by placing syntactic and structural restrictions on the actions sets of the problems. For instance, Bylander (1994) shows that
planning in UB domains where each action is restricted to have only positive preconditions
is tractable, yet optimal planning for this UB fragment is hard. In general, the seminal
works by Bylander (1994) and Erol, Nau, and Subrahmanian (1995) indicate that extremely
severe syntactic restrictions on single actions are required to guarantee tractability, or even
membership in NP. Backstrom and Klein (1991) consider syntactic restrictions of a more
global nature, and show that UB planning is tractable if no two actions have the same effect,
and the preconditions of no two actions require different values for the variables that are
not affected by these actions. Interestingly, this fragment of UB, known as PUBS, remains
tractable for optimal planning as well. While the characterizing properties of PUBS are
very restrictive, this result of Backstrom and Klein has provided an important milestone in
the research on planning tractability.
Given the limitations of syntactic restrictions observed by Bylander (1994), Erol et al.
(1995), and Backstrom and Klein (1991), more recent works have studied the impact of
posing structural and mixed structural/syntactic restrictions on the action sets. In the
scope of UB, most of these works relate the complexity of planning and the topological
properties of the problems causal graph structure.
Definition 2 The causal graph CG() of a SAS+ problem  = hV, A, I, Gi is a digraph
over the nodes V . An arc (v, v  ) belongs to CG() iff v 6= v  and there exists an action
a  A changing the value of v  while being preconditioned by some value of v, that is, both
eff(a)[v  ] and pre(a)[v] are specified.
Informally, the immediate predecessors of v in CG() are all those variables that directly
affect our ability to change the value of v, and it is evident that constructing the causal
graph CG() of any given UB planning problem  is straightforward. For instance, consider
the action set depicted in Figure 1a. It is easy to verify that all the actions in this set are
unary effect. The causal graph induced by this action set is depicted in Figure 1b. The
actions a1 and a2 are the only actions that change the values of v1 and v2 , respectively, and
these actions have no preconditions outside the affected variables. Hence, the causal graph
contains no arcs incoming to the nodes v1 and v2 . On the other hand, the actions changing
v3 and v4 are preconditioned (in both cases) by the values of v1 and v2 , and thus both v3
and v4 have incoming arcs from v1 and v2 .
Way before being used for the complexity analysis, causal graphs have been (sometimes
indirectly) considered in the scope of hierarchically decomposing planning tasks (Newell &
205

fiKatz & Domshlak

A
a1
a2
a3
a4
a5

v1
0

pre(a)
v2 v3

v4

v1
1

eff(a)
v2 v3

v4

0
0
1

1
0
1

v2

BB
BB |||
B|
|| BBB
 ~||


1
0
0

v1

v3

1
0

(a)

v4

v1

v2

||
||
|
|
 |~ |


v3

(b)

v4

(c)

Figure 1: Example of two simple action sets that fit the characteristics of the UB fragment.
(a) Unary-effect action set A over propositional variables V = {v1 , . . . , v4 }, (b)
Causal graph induced by A, (c) Causal graph induced by A \ {a4 }.

Simon, 1963; Sacerdoti, 1974; Knoblock, 1994; Tenenberg, 1991; Bacchus & Yang, 1994).
The first result relating between the complexity of UB planning and the structure of the
causal graph is due to Backstrom and Jonsson (1995, 1998b) that identify a fragment
of UB, called 3S, which has an interesting property of inducing tractable plan existence
yet intractable plan generation. One of the key characteristics of 3S is the acyclicity of
the causal graphs. A special case of 3S is also independently studied by Williams and
Nayak (1997) in the scope of incremental planning for more general SAS+ problems.
More recently, Brafman and Domshlak (2003) provide a detailed account of the complexity of finding plans for UB problems with acyclic causal graphs. These results are most
closely related to the problems examined in our paper, and thus we survey them in more
details. For ease of presentation, we now introduce certain notation that is heavily used
throughout the paper.
 For each node v  CG(), by In(v) and Out(v) we denote the in- and out-degrees of v,
respectively, and In(CG())/Out(CG()) stand for the maximal in-degree/out-degree
of the CG() nodes.
 Assuming CG() is connected1 , we provide a special notation to the following topologies of acyclic causal graphs, also depicted in Figure 2. A causal CG() is a
T tree if In(CG())  1, and there exists v  V such that In(v) = 0.
I inverted tree if Out(CG())  1, and there exists v  V such that Out(v) = 0.
P polytree if CG() contains no undirected cycles. (For an example of a polytree that
is neither tree nor inverted tree see Figure 1c or Figure 2.)
S directed-path singly connected if there is at most one directed path from each node
v  CG() to any other node v   CG(). (For an example of a directed-path
singly connected DAG see Figure 1b or Figure 2.)
1. If CG() consists of a few connected components, then these components identify independent subproblems of  that can be easily identified and treated separately.

206

fiTractable Cost-Optimal Planning

Figure 2: Examples of the causal graphs topologies considered in the paper, along with the
inclusion relations between the induced fragments of UB.

In what follows, we use T, I, P, and S to refer to the corresponding fragments of UB, and
we use subscript/superscript b to refer to a fragment induced by the additional constraint
of in-degree/out-degree being bounded by a constant. It is not hard to verify that we
have T, I  P  S, with T  Pb and I  Pb ; the complete inclusion hierarchy of these
sub-fragments of UB is shown in Figure 3a.
The key tractability result by Brafman and Domshlak (2003) corresponds to a polynomial time plan generation procedure for Pb , that is, for UB problems inducing polytree
causal graphs with all nodes having O(1)-bounded indegree. In addition, Brafman and
Domshlak show that plan generation is NP-complete for the fragment S, and we note that
their proof of this claim can be easily modified to hold for Sbb . These results of tractability
and hardness (as well as their immediate implications) are depicted in Figure 3b by the
shaded bottom-most and the transparent top-most free-shaped regions. The empty freeshaped region in between corresponds to the gap left by Brafman and Domshlak (2003). This
gap has been recently closed by Gimenez and Jonsson (2008) that prove NP-completeness
of plan generation for P. We note that the proof of Gimenez and Jonsson actually carries
out to the I fragment as well, and so the gap left by Brafman and Domshlak (2003) is now
entirely closed.
1.2 Summary of Results
The complexity results by both Brafman and Domshlak (2003) and Gimenez and Jonsson
(2008) correspond to satisficing planning, and do not distinguish between the plans on the
207

fiKatz & Domshlak

(a)

(b)

Figure 3: Inclusion-based hierarchy and complexity of plan generation for some UB problems with acyclic causal graphs. (a) The hierarchy of STRIPS fragments corresponding to tree, inverted tree, polytree, and directed-path singly connected
topologies of the causal graph, and (possibly) O(1) bounds on the causal graph
in-degree and/or out-degree. (b) Plan generation is tractable for the fragments
in the (bottom-most) shaded region, and NP-complete for all other depicted fragments. The top-most and intermediate (transparent) regions correspond to the
results by Brafman and Domshlak (2003) and Gimenez and Jonsson (2008), respectively.

basis of their quality. In contrast, we study the complexity of optimal plan generation for
UB, focusing on (probably the most canonical) cost-optimal (also known as sequentiallyoptimal) planning. Cost-optimal
planning corresponds to the task of finding a plan   A
P
that minimizes C() = a C(a). We provide novel tractability results for cost-optimal
planning for UB, and draw a dividing line between the tractable and intractable such
problems. Almost all our tractability results are based on a proof technique that connects
between certain tools from planning and tractable constraint optimization. We strongly
believe that this proof-technical contribution of the paper is of interest on its own due
to a clear evidence for its robustnessour different algorithms exploit this proof technique,
but in very much different manners.
In the rest of this section we aim at providing an adequate description of our results for
readers who do not want to delve into formal details, or just prefer not to do it in the first
reading of the paper.2 Hence, most formal definitions, constructions, and proofs underlying
these results are given later, starting in Section 2.

2. We adopted this format from the seminal paper by Bylander (1994) as we feel the format has contributed
something to making that paper an extremely enjoyable read.

208

fiTractable Cost-Optimal Planning

1.2.1 Cost-Optimal Planning for Pb
Following Brafman and Domshlak (2003), we relate the complexity of (cost-optimal) UB
planning and the topology of the causal graph. For that we consider the structural hierarchy depicted in Figure 3a. We begin with considering cost-optimal planning for Pb it
is apparent from Figure 3b that this is the most expressive fragment of the hierarchy that
is still a candidate for tractable cost-optimal planning. Our first positive result affirms
this possibility, showing that the complexity map of the cost-optimal planning for the UB
fragments in Figure 3a is identical to this for satisficing planning (that is, Figure 3b).
Our algorithm for Pb is based on compiling a given Pb problem  into a constraint
optimization problemP
COP = (X , F) over variables X , functional components F, and the
global objective min F (X ) such that
(I) COP can be constructed in time polynomial in the description size of ,

(II) the tree-width of the cost network of COP is bounded by a constant, and the optimal
tree-decomposition of the network is given by the compilation process,
(III) if  is unsolvable then all the assignments to X evaluate the objective function to
, and otherwise, the optimum of the global objective is obtained on and only on
the assignments to X that correspond to cost-optimal plans for ,
(IV) given an optimal solution to COP , the corresponding cost-optimal plan for  can
be reconstructed from the former in polynomial time.
Having such a compilation scheme, we then solve COP using the standard, poly-time algorithm for constraint optimization over trees (Dechter, 2003), and find an optimal solution
for . The compilation is based on a certain property of the cost-optimal plans for Pb that
allows for conveniently bounding the number of times each state variable changes its value
along such an optimal plan. Given this property of Pb , each state variable v is compiled
into a single COP variable xv , and the domain of that COP variable corresponds to all
possible sequences of value changes that v may undergo along a cost-optimal plan. The
functional components F are then defined one for each COP variable xv , and the scope of
such a function captures the family of the original state variable v in the causal graph,
that is, v itself and its immediate predecessors in CG(). For an illustration, Figure 4a
depicts a causal graph of a P problem , with the family of the state variable v4 being
depicted by the shaded region, and Figure 4b shows the cost network induced by compiling
 as a Pb problem, with the dashed line surrounding the scope of the functional component
induced by the family of v4 . It is not hard to verify that such a cost network induces a tree
over variable-families cliques, and for a Pb problem, the size of each such clique is bounded
by a constant. Hence, the tree-width of the cost-network is bounded by a constant as well.
1.2.2 Causal Graphs and k-Dependence
While causal graphs provide important information about the structure of the planning
problems, a closer look at their definition reveals that some information used for defining
causal graphs actually gets hidden by this structure. To start with an example, let us
consider the multi-valued encoding of the Logistics-style problems (Helmert, 2006). In
209

fiKatz & Domshlak

these problems, each variable representing the location of a package has as its parents
in the causal graph all the variables representing alternative transportation means (i.e.,
tracks, planes, etc.), and yet, each individual action affecting the location of a package is
preconditioned by at most one such parent variable. (You cannot load/unload a package
into/from more than one vehicle.) This exemplifies the fact that, even if the in-degree of the
causal graph is proportional to some problem domains parameters, the number of variables
that determine applicability of each action may still be bounded by a constant.
In other words, while the causal graph provides an aggregate view on the independence
relationships between the problem variables, the individual dependencies of the problem
actions on the unaffected variables are suppressed by this view. Targeting these actual
individual dependencies of the actions, here we define a (tangential to the causal graphs
topology) classification of the UB problems, and study the connection between this classification and the computational tractability of both general and cost-optimal plan generation
for UB.
Definition 3 For any k  Z , and any SAS+ problem instance  = (V, A, I, G), we say
that  is k-dependent if it satisfies
max |{v  V | pre(a)[v] 6= u  eff(a)[v] = u}|  k,
aA

with = u standing for unspecified.
In other words, a SAS+ problem is k-dependent if no action in its action set depends
on more than k unaffected variables. Combining our two classifications of the problems,
for any structural fragment F of UB (such as, e.g., these in Figure 3), and any k  Z , by
F(k) we denote the set of all k-dependent problems within F.
Recall that the fragment P of UB is NP-hard even for satisficing planning (Gimenez
& Jonsson, 2008). Our main result is positiveat least for the most extreme (yet, says
the Logistics example above, not unrealistic) setting of k = 1, satisfying k-dependence does
bring us to an island of tractability P(1).
Similarly to our treatment of Pb , our algorithm for P(1) exploits the idea of compiling a
planning problem  into a tractable constraint optimization problem COP . However, the
planning-to-COP compilation scheme for P(1) is very much different from that devised for
Pb . In fact, this difference is unavoidable since our construction for Pb heavily relies on the
assumption that In(CG()) = O(1), and we do not have this luxury in P(1). Instead, we
identify certain properties of the cost-optimal plan sets of the P(1) problems, and exploit
these properties in devising suitable planning-to-COP compilation schemes.
We begin with considering only P(1) problems with uniform-cost actions; the cost of a
plan for such a problem is proportional to the length of the plan3 . We show that any such
solvable problem has a cost-optimal plan that makes all the changes of each variable to a
certain value using exactly the same (type of) action. And while devising a correct and
tractable planning-to-COP compilation scheme is more than a step away from identifying
this property of P(1), the latter provides a critical brick that everything else relies upon.
Relying on this property of P(1), each state variable v and each edge (v, v  ) are uniquely
3. This is probably the origin for the term sequential optimality.

210

fiTractable Cost-Optimal Planning

(a)

(b)

(c)

Figure 4: Cost networks induced by the planning-to-COP compilation schemes for P. (a)
Causal graph of a P problem , with the family of the state variable v4 being
depicted by the shaded region. (b) Cost network induced by compiling  as a Pb
problem, with the dashed line surrounding the scope of the functional component
induced by the family of v4 . (c) Cost network induced by compiling  as a P(1)
problem, with the dashed lines surrounding the scopes of the four functional
components induced by the family of v4 .

compiled into COP variables xv and xvv (see Figure 4c). A certain set of functional components are then defined for each COP variable xv . Both the domains of the COP variables
and the specification of the functional components are technically involved, and thus relegated to later in the paper. It is important, however, to note already here that the cost
networks of such COPs are guaranteed to induce trees over cliques of size  3, and thus
having tree-width bounded by a constant. The reader can get an intuition of where these
cliques of size  3 are coming from by looking on the example depicted in Figure 4c.
Unfortunately, the aforementioned helpful property of the P(1) problems with uniformcost actions does not hold for more general action-cost schemes for P(1). Turns out, however, that all problems in P(1) satisfy another property that still allows for devising a
general, correct, and tractable planning-to-COP scheme for P(1). Specifically, we show
that any solvable problem in P(1) has a cost-optimal plan that makes all the changes of
each variable using at most three types of action. The algorithm resulting from exploiting
this property is more complex and more costly than that devised for the P(1) problems with
uniform-cost actions, yet it is still poly-time. Interestingly, the cost networks of such COPs
are topologically identical to these for problems with uniform-cost actions, with the difference being in the domains of the COP variables, and in the specification of the functional
components.
Having read this far, the reader may rightfully wonder whether O(1)-dependence is not
a strong enough property to make the cost-optimal planning tractable even for some more
complex than polytree forms of the causal graph. Turns out that the dividing line between
211

fiKatz & Domshlak

Pb
P(k)
Sbb

k=1

k=2

k=3

k = (n)


P
NPC









P
NPC
NPC

Pb
P(k)
Sbb

(a)

k=1

k=2

k=3

k = (n)


P





NPC



P
NPC
NPC

(b)

Figure 5: Complexity of (a) cost-optimal and (b) satisficing plan generation for fragments
of UB. The  mark indicates that the complexity is implied by other results in
the row. All other shaded and regular cells correspond to the results obtained in
this work and in the past, respectively. Empty cells correspond to open questions.
Note that the only difference in our understanding of cost-optimal and satisficing
planning for the fragments in question is the complexity of planning for S(1).

tractable and intractable problems is much more delicate. Figure 5 summarizes our current
understanding of time complexity of both cost-optimal and satisficing plan generation for
the P and S fragments of UB. First, in this paper we show that even the satisficing planning
with directed-path singly connected, bounded in- and out-degree causal graphs is hard under
2-dependence, and that cost-optimal planning for this structural fragment of UB is hard
even for 1-dependent such problems. Note that the complexity of (both cost-optimal and
satisficing) plan generation for P(k) for k = O(1) remains an interesting open problem. An
additional question that remains open is the complexity of satisficing plan generation for
S(1).
1.3 Remarks
Our goal in this work has been identifying new islands of tractability for cost-optimal planning, and improving by that our understanding of what makes the planning problems hard
or easy to solve. Of our lesser interest was to make the poly-time algorithms practically
efficient by reducing their (quite prohibitive) polynomial time complexity. In fact, in some
places we intentionally sacrificed some possible optimizations to keep the already involved
constructions as apprehensible as possible. Therefore, it is more than likely that the time
complexity of our planning-to-COP algorithms can be further improved, or some conceptually different algorithmic ideas will be found more appropriate for the problems in question.
In addition, much more efficient algorithms may work for some special cases of the general
tractable families. For instance, in the paper we illustrate such possibility by presenting a
low poly-time algorithm for UB problems with tree causal graphs (that is, the T fragment)
and uniform-cost actions.
Of course, the reader may ask whether striving for practical efficiency in solving various
special fragments of planning is at all motivated. As our discussion at the beginning of the
paper suggests, we believe that the answer to this question is yes. While most of the
research on AI planning is rightfully devoted to solving general planning problems, many
tools developed and employed in that research rely on tractable fragments of planning. For
212

fiTractable Cost-Optimal Planning

instance, if one works on devising an effective heuristic estimator for a planning problem
by projecting it to (or embedding it in) this or another relaxed problem, then she will be
happy to know that the latter can be solved in low poly-time. On the other hand, making
a tractable fragment also efficiently solvable in practical terms is probably worth the effort
only in face of some concrete customer of that fragment in practice.

2. Definitions and Notation
Starting with Definitions 1-3 from the previous section, in this section we introduce some
additional definitions and notation that are used throughout the paper.
In contrast to the well-known STRIPS formalism for propositional planning, we assume
that all our actions are value changing, and this in contrast to value setting. That
is, we have eff(a)[v] being specified only if pre(a)[v] is also specified, in which case we
have eff(a)[v] 6= pre(a)[v]. While in general this assumption requires an exponential time
translation, in case of unary-effect actions the translation takes only linear time. Given a
UB problem  = hV, A, I, Gi, by Av  A we denote the actions that change the value of
v. Note that the unary-effectness of  implies that Av1 , . . . , Avn is a partition of the problem
actions A. Considering the applicability of actions, in SAS+ it also helps to give a special
attention and notation to the action preconditions that are left unaffected by the action.
The customary name for such preconditions is prevail conditions (Backstrom & Klein,
1991). For example, having truck T and package P at location L are both preconditions of
loading P into T in L, but only the former is a prevail condition of this action because the
truck is still in L after loading P , while P is no longer there (but inside T ).
Given a UB problem  = hV, A, I, Gi, a variable subset V   V , and an arbitrary
sequence of S
actions   A , by V  we denote the order-preserving restriction of  to
the actions vV  Av . If the restriction is with respect to a singleton set V  = {v}, then we
allow writing {v} simply as v . One of the key properties of cost-optimal plans for the
UB problems with directed-path singly connected causal graphs is immediately derivable
from Lemma 1 by Brafman and Domshlak (2003), and it is given by Corollary 1 below.
Henceforth, a valid plan  for a given problem  is called irreducible if any subplan 
of  is not a plan for , in the following sense4 : Removal of any subset of (not necessarily
subsequent) actions from  makes the resulting plan either illegal, or its initial state is not
I, or its end state is not one of the states specified by G.
Lemma 1 (Brafman and Domshlak, 2003) For any solvable problem   S over n
state variables, any irreducible plan  for , and any state variable v in , the number of
value changes of v along  is  n, that is, |v |  n.
Corollary 1 For any solvable problem   S over n state variables, any cost-optimal plan
 for , and any state variable v in , we have |v |  n.
4. The notion of irreducible plans was introduced by Kambhampati (1995), where it was called minimal
plans, and exploited for admissible pruning of partial plans during search. We adopt the terminology
suggested by Brafman and Domshlak (2003) to prevent ambiguity between minimal as irreducible and
minimal as optimal.

213

fiKatz & Domshlak

Given an S problem  = hV, A, I, Gi, we denote the initial value I[v] of for each variable
v  V by bv , and the opposite value by wv (short for, black/white). Using this notation
and exploiting Corollary 1, by (v) we denote the longest possible sequence of values
obtainable by v along a cost-optimal plan , with |(v)| = n + 1, bv occupying all the
odd positions of (v), and wv occupying all the even positions of (v). In addition, by  (v)
we denote a per-value time-stamping of (v)
(
b1v  wv1  b2v  wv2    bj+1
v , n = 2j,
 (v) =
, j  N.
n = 2j  1,
b1v  wv1  b2v  wv2    wvj ,
The sequences (v) and  (v) play an important role in our constructions both by themselves and via their prefixes and suffixes. In general, for any sequence seq, by [seq] and
[seq] we denote the set of all non-empty prefixes and suffixes of seq, respectively. In our
context, a prefix    [(v)] is called goal-valid if either the goal value G[v] is unspecified,
or the last element of   equals G[v]. The set of all goal-valid prefixes of (v) is denoted by
 [(v)]  [(v)]. The notion of goal-valid prefixes is also similarly specified for  (v).
Finally, given a SAS+ problem  = hV, A, I, Gi, a subset of state variables V   V , and
an action sequence   A , we say that  is applicable with respect to V  if restricting
the preconditions and effects of the actions in  to the variables V  makes  applicable in I.

3. Cost-Optimal Planning for Pb
This section is devoted to the proof of tractability of cost-optimal planning for the problem
fragment Pb . We begin with describing our planning-to-COP scheme for Pb , and then prove
its correctness and complexity. Finally, we present a interesting subset of Pb for which costoptimal planning is not only tractable, but also provably solvable in low polynomial time.
3.1 Construction
Before we proceed with the details of the construction, we make an assumption that our
actions are fully specified in terms of the variables parents in the causal graph. If pred(v) 
V denotes the set of all the immediate predecessors of v in the causal graph CG(), then
we assume that, for each action a  Av , pre(a)[w] is specified for each w  pred(v). While
in general such an assumption requires an exponential translation, this is not the case with
Pb . Let A be such a translation of the original problem actions A. To obtain A , for
every variable v  V , every action in Av is represented in A by a set of actions that are
preconditioned by complete assignments to pred(v). If |pred(v)| = k, and the precondition
of a is specified only in terms of some 0  k  k parents of v, then a is represented in
A by a set of actions, each extending the precondition pre(a) by a certain instantiation
of the previously unspecified k  k parents of v, and having the cost C(a ) = C(a). Note
that the expansions of two or more original actions may overlap, and thus A may contain
syntactically identical yet differently priced actions. Without loss of generality, we assume
that only a minimally-priced such clone is kept in A . The key point is that compiling A
into A for the Pb problems is poly-time, as the procedure is linear in |A | = O(n2In()+1 ).
Finally, the (straightforward to prove) Proposition 1 summarizes the correctness of our
assumption with respect to the cost-optimal planning for UB.
214

fiTractable Cost-Optimal Planning

Proposition 1 For any UB problem  = hV, A, I, Gi, the cost of the optimal plans for 
is equal to this for  = hV, A , I, Gi, with optimal plans for  being reconstructible in
linear time from the optimal plans for  and vice versa.
We now specify our compilation of a given Pb problem  into a constraint optimization
problem COP . The COP variable set X contains a variable xv for each planning variable
v  V , and the domain Dom(xv ) consists of all valid prefixes of  (v). That is,
X = {xv | v  V }
Dom(xv ) =  [ (v)]

(1)

Informally, the domain of each variable xv contains all possible sequences of values that
the planning variable v may undergo along a cost-optimal plan. Now, for each planning
variable v with parents pred(v) = {w1 , . . . , wk }, the set of COP functions F contains a
single non-negative, real-valued function v with the scope
Qv = {xv , xw1 , . . . , xwk }

(2)

The purpose of these functions is to connect the value-changing sequences of v and these
of its parents pred(v). The specification of these functions is the more involved part of the
compilation.
First, for each planning variable v with pred(v) = , and each of its goal-valid (timestamped) value-changing sequences     [ (v)], we set


|  | = 1

0,
|  | = 2
(3)
v (  ) = C(a
j  k
j  wkv ),


 | |  C(awv ) + | |1  C(abv ), otherwise
2

2

where eff(awv )[v] = {wv }, eff(abv )[v] = {bv }, and C(a) = C(a) if a  A, and , otherwise.
It is not hard to verify that v (  ) corresponds to the optimal cost of performing |  |  1
value changes of v in .
Now, for each non-root variable v with pred(v) = {w1 , . . . , wk }, k  1, we specify the
function v as follows. For each goal-valid value-changing sequence     [ (v)] of v, and
each set of such goal-valid value-changing sequences {1   [ (w1 )], . . . , k   [ (wk )]}
of vs parents, we want to set v (  , 1 , . . . , k ) to the optimal cost of performing |  |  1
value changes of v, given that w1 , . . . , wk change their values |1 |  1, . . . , |k |  1 times,
respectively. In what follows, we reduce setting the value v (  , 1 , . . . , k ) to solving a
single-source shortest path problem on an edge-weighted digraph Ge (v) that slightly enhances a similarly-named graphical structure suggested by Brafman and Domshlak (2003).
Despite the substantial similarity, we provide our construction of Ge (v) in full details to
save the reader patching the essential differences.
Given the value-changing sequences 1 , . . . , k as above, the digraph Ge (v) is created in
three steps. First, we construct a labeled directed graph G(v) capturing information about
all sequences of assignments on pred(v) that can enable n or less value flips of v. The graph
G(v) is defined as follows:
1. G(v) consist of  = max   [ (v)] |  | nodes.
215

fiKatz & Domshlak

2. G(v) forms a 2-colored multichain, i.e., (i) the nodes of the graph are colored with
black (b) and white (w), starting with black; (ii) there are no two subsequent nodes
with the same color; (iii) for 1  i    1, edges from the node i are only to the node
i + 1.
Observe that such a construction of G(v) promises that the color of the last node will
be consistent with the goal value G[v] if such is specified.
3. The nodes of G(v) are denoted precisely by the elements of the longest goal-valid
value-changing sequence     [ (v)], that is, biv stands for the ith black node in
G(v).
4. Suppose that there are m operators in Av that, under different preconditions, change
the value of v from bv to wv . In this case, for each i, there are m edges from biv to
wvi , and |Av |  m edges from wvi to bi+1
v . Each such edge e is labeled with the cost of
the corresponding action, as well as with the prevail conditions of that action, which
is a k-tuple of the values of w1 , . . . , wk . This compound label of e is denoted by l(e),
and the prevail condition and cost parts of l(e) are henceforth denoted by prv(e) and
C(e), respectively.
As the formal definition of G(v) is somewhat complicated, we provide an illustrating
example. Suppose that we are given a Pb problem over 5 variables, and we consider a
variable v with pred(v) = {u, w}, I[v] = bv , and G[v] = wv . Let

 a1 : pre(a1 ) = {bv , bu , ww }, eff(a1 ) = {wv }, C(a1 ) = 1
Av =
a : pre(a2 ) = {wv , bu , bw }, eff(a2 ) = {bv }, C(a2 ) = 2
 2
a3 : pre(a3 ) = {wv , wu , ww }, eff(a3 ) = {bv }, C(a3 ) = 3

The corresponding graph G(v) is depicted in Figure 6a. Informally, the graph G(v)
captures information about all potential executions of the actions in Av along a cost-optimal
plan for . Each path from the source node of G(v) uniquely corresponds to one such
execution. Although the number of these alternative executions may be exponential in
n, their graphical representation via G(v) is compactthe number of edges in G(v) is
O(n  |Av |). Note that the information about the number of times each action in Av can be
executed is not captured by G(v). The following two steps add this essential information
into the graphical structure.
At the second step, the digraph G(v) = (V, E) is expanded into a digraph G (v) =

(V , E  ) by substituting each edge e  E with a set of edges (between the same nodes),
but with the labels corresponding to all possible assignments of the elements of 1 , . . . , k
to prv(e). For example, an edge e  E labeled with kbw1 bw2 , 10k might be substituted in
E  with edges labeled with {kb1w1 b1w2 , 10k, kb1w1 b2w2 , 10k, kb2w1 b1w2 , 10k, . . . }. Finally, we set
V  = V  {sv , tv }, and add a single edge labeled with the first elements of 1 , . . . , k and
zero cost (that is, kb1w1    b1wk , 0k) from sv to the original source node b1v , plus a single edge
labeled with the last elements of 1 , . . . , k and zero cost from the original sink node of G(v)
to tv . Informally, the digraph G (v) can be viewed as a projection of the value-changing
sequences 1 , . . . , k on the base digraph G(v). Figure 6b illustrates G (v) for the example
1  b2  w2 .
above, assuming u = b1u  wu1  b2u  wu2  b3u and w = b1w  ww
w
w



At the third step, a digraph Ge (v) = (Ve , Ee ) is constructed from G (v) as follows.
216

fiTractable Cost-Optimal Planning

bu bw ,2
bu ww ,1 1
/w
b1v
v

$
:

bu bw ,2
bu ww ,1 2
/w
b2v
v

wu ww ,3

$

bu ww ,1 3
/w
v

3
: bv

wu ww ,3

(a)

sv

b3u b1w ,2

b3u b2w ,2

b3u b2w ,2

b2u b1w ,2

b2u b1w ,2

1 ,
b3u ww
1

b2u b2w ,2

1 ,
b3u ww
1

b2u b2w ,2

1 ,
b3u ww
1

1 ,
b2u ww
1

b1u b1w ,2

1 ,
b2u ww
1

b1u b1w ,2

1 ,
b2u ww
1

1 ,
b1u ww
1

b1u b1w ,0

b3u b1w ,2

/ b1
v

& 
1
8 wH K v

b1u b2w ,2

%   
2
9 bH K L v

1 ,
b1u ww
1

& 

b1u b2w ,2

2
8 wv

%   

1 ,
b1u ww
1

3
9 bv

HK

3
8 wv

H KL

HK

2 ,
b1u ww
1

1 w1 ,
wu
w 3

2 ,
b1u ww
1

1 w1 ,
wu
w 3

2 ,
b1u ww
1

2 ,
b2u ww
1

1 w2 ,
wu
w 3

2 ,
b2u ww
1

1 w2 ,
wu
w 3

2 ,
b2u ww
1

2 ,
b3u ww
1

2 w1 ,
wu
w 3

2 ,
b3u ww
1

2 w1 ,
wu
w 3

2 ,
b3u ww
1

2 w2 ,
wu
w 3

& 

2 ,0
b3u ww

/ tv

2 w2 ,
wu
w 3

(b)
Figure 6: Example of the graphs (a) G(v), and (b) G (v).
(i) The nodes Ve correspond to the edges of G (v).
(ii) The edges (ve , ve )  Ee correspond to all pairs of immediately consecutive edges
e, e  E  such that, for 1  i  k, either prv(e)[wi ] = prv(e )[wi ], or prv(e )[wi ]
appears after prv(e)[wi ] along i .
(iii) Each edge (ve , ve )  Ee is weighted with C(e ).
Figure 7 depicts the graph Ge (v) for our example.
Assuming 3  2 , the dashed edges correspond to the minimal-cost path of length 5
from the dummy source node b1u b1w . Note that, if the costs of actions Av is all we care
about, this path corresponds to the cost-optimal sequence of 5 value changes of v starting
from its initial value bv in . In fact, not only this path corresponds to such a cost-optimal
sequence, but it also explicitly describes the underlying sequence of actions from Av , as
well as the total cost of these actions. Finally, for all 0  i  n, such minimal-cost paths
of length i can be determined by running on Ge (v) a low-polynomial single-source shortest
217

fiKatz & Domshlak

b3u b2w , 2

b3 b2 , 

b1u b1w , 2

b1u b1w , 2

2
u Fw
F J K 999
J K 999






99
99
 
 
99
99
 
 


9
9
3  1
3  1


9
 bubw , 2
 bubw , 2 999
99


99LLL 99
99LLL 99


  
  
99 LLL 99
99 LLL 99




  
  
99 LLL 99
99 LLL 99




L
L% 
  
  
99
99
% 




 2 2

9
9


3
2
2
2
2 ,
3
2
_
_
/

/

99 bu ww , 1 
99 b3u ww
 w ,
w
bu ww , 1  w
w ,

C
  uBF H J w 3
  uBF H J w 3
99rr9 BF H J K
99rr9 B FH J K 88 1





r 9 
r 9 
88

 
 
  
  
rrr 99 
rrr 99
88


r
r


r
r





r
 
 9 
88
 9


  
rr

r






    2 1
 / 3  1
   2 1
8
 / 3  1
 3 1








b
w
,

w
w
,

 uww , 1    w
 uww , 1 88
 u ww , 3   b
1      u B H w
3  b
u9 w




B
J
H
B
B

H
H
K
K



















JJ 88
t
JJ 88
    
    
 tttt
    
    




JJ 8














      
      
 tt
      
     
JJ 8




t




      
    
% 
t
      
    
   /  2 2      2 2
   /  2 2
   2 2



2
2
1
1
/
/ b3 w2 , 0


bu ww ,  1   b u bw , 2    b u ww ,  1   b u bw , 2    b u ww , 1
bu bw , 0
//7 JJ
9 uC G w
     r r9 F
  9 B F H J
  9  B F H J    9 F
tt
  rr r r 
  rr r r     rrr r 
t
//7 JJJJ
   rr rr 
t



r
r
r







t
  r  
  rr    
  rr        rr  
JJ
// 7
tt  
J%
tt 
rrr  
rrr   
rrr    rrr   
// 7
 
1 ,     b2 b1 ,      /b2 w1 ,     b2 b1 ,     /b2 w1 , 
// 7 7 b2u ww
1 
1  
2   
1 
2   

// 7
    u w     uB H w     u w     uB H w

 
// 7
            
      
    





 
       
       
  
// 7     
 
         
    
//     










2 ,
2 ,       b1 w2 , 
/ w1 w2 , 
      b1 w2 ,  
b//1u ww


 1_ _/ w9u1 ww
   
9u w 3     uBF w 1
3     uBF w 1
//
r
r

r
r
       rrrr
     
//  

rrr
r









r
r



 rrr
      
   
/  rrr

   
   
1 ,
1 w1 , 
1 w1 , 
     b1 w1 , 
     b1 w1 , 
/
/
b1u ww
w
w


1
u w   3    u B w L1
u w  3    u B w
L1LL
LLL
     
     
LLL
L








LLL
LLL
     
     
LL
L%
    
%
   
     
     
b1u b2w , 
b1u b2w , 
2 
2 
   
   





Figure 7: The graph Ge (v) constructed from the graph G (v) in Fugure 6b.

paths algorithm by Dijkstra (Cormen, Leiserson, & Rivest, 1990). This property of the
graph Ge (v) provides us with the last building block for our algorithm for cost-optimal
planning for Pb .
The overall algorithm for cost-optimal planning for Pb that is based on the above construction is depicted in Figure 8. Given a problem   Pb , the algorithm compiles it into
the constraint optimization problem COP , and solves it using a standard algorithm for
constraint optimization over tree constraint networks (Dechter, 2003). The specification of
COP has already been explained inline. We believe it is already intuitive that this compilation takes time polynomial in the description size of , but in the next section we also
prove it formally. Solving COP using the algorithm for tree-structured constraint networks
can be done in time polynomial in the description size of COP because
218

fiTractable Cost-Optimal Planning

procedure polytree-k-indegree( = (V, A, I, G))
takes a problem   Pb
returns an optimal plan for  if solvable, and fails otherwise
create a set of variables X and set their domains as in Eq. 1
create a set of functions F = {v | v  V } with scopes as in Eq. 2
for each v  V do
if pred(v) =  then
specify v according to Eq. 3
elseif pred(v) = {w1 , . . . , wk } then
construct graph G(v)
for each k-tuple 1   [ (w1 )], . . . , k   [ (wk )] do
construct graph G (v) from graph G(v) and sequences 1 , . . . , k
construct graph Ge (v) from graph G (v)
for each goal-valid sequence     [ (v)] do
 := minimal-cost path of |  |  1 edges
from the source node hbw1    bwk i of Ge (v)
if returned  then
v (  , 1 , . . . , k ) := C()
else
v (  , 1 , . . . , k ) := 
endif
endfor
endfor
endif
endfor
P
set COP := (X , F) with global objective min F (X )
x :=
P solve-tree-cop(COP )
if F (x) =  then return failure
P
extract plan  from x with C() = F (x)
return 
Figure 8: Algorithm for cost-optimal planning for Pb .
(i) the tree-width of the cost network of COP is bounded by the same constant that
bounds the in-degree of the causal graph, and
(ii) optimal tree-decomposition of the COP s cost network is given by any topological
ordering of the causal graph.
3.2 Correctness and Complexity
We now proceed with proving both the correctness and the polynomial time complexity of
our algorithm for Pb . We begin with proving in Theorem 1 below a rather general property
of polytrees that helps us analyzing both the Pb fragment in question, as well as the P(1)
fragment considered later in the paper. We note that a special case of this property has
219

fiKatz & Domshlak

been already exploited in the past in the proof of Lemma 2 by Brafman and Domshlak
(2003), but, to our knowledge, this property has never been formulated as a generic claim
of Theorem 1. Throughout the paper we then demonstrate that this generic claim can be
helpful in numerous situations; the proof of Theorem 1 is in Appendix A, p. 245.
Theorem 1 Let G be a polytree over vertices V = {1, . . . , n}, and pred(i)  V denote the
immediate predecessors of i in G. For each i  V , let Oi be a finite set of objects associated
with the vertex i, with the sets O1 , . . . , On being pairwise disjoint. For each i  V , let >i
be a strict partial order over Oi , and, for each j  pred(i), let >i,j be a strict partial order
over Oi  Oj .
If, for each i  V, j  pred(i), the transitively closed >i  >i,j and >j  >i,j induce
(strict) partial orders over Oi  Oj , then so does the transitively closed


[
[
 >i 
>i,j 
> =
iV

over O =

S

iV

jpred(i)

Oi .

Using Theorem 1 we now proceed with proving the correctness and complexity of the
polytree-k-indegree algorithm.
Theorem 2 Let  be a planning problem in Pb , COP = (X , F) be the corresponding
constraint
optimization problem, and x  Dom(X ) be an optimal solution for COP with
P
F (x) = .
(I) If  < , then a plan of cost  for  can be reconstructed from x in time polynomial
in the description size of .

(II) If  has a plan, then  < .
Proof Sketch: The proof of Theorem 2Pis in Appendix A. p. 247. To prove (I), given
a COP solution x = {v1 , . . . , vn } with F (x) =  < , we construct a plan  for
 with C() = . This is done by constructing action sequences v for each v  V , as
well as constructing partial orders over the elements of these sequences of each variable
and its parents. These orders are then combined and linearized
1) into
P (using Theorem
P
an
P action sequence  that is a valid plan for  with C() = vV C(v ) = vV v (x) =
problem  and some irreducible
F (x) = . To prove (II), given a solvable
P
Pplan , we
construct a COP assignment x such that F (x ) = C(). Then, from   F (x )
and C() < , we obtain the claimed  < .

Theorem 3 Cost-optimal planning for Pb is tractable.
Proof: Given a planning problem   Pb , we show that the corresponding constraint optimization problem COP can be constructed and solved in time polynomial in the description
size of .
Let n be the number of state variables in , and  be the maximal node in-degree
in the causal graph CG(). In polytree-k-indegree, for each planning variable v  V with
pred(v) = {w1 , . . . , wk }, and each k-tuple 1   [ (w1 )], . . . , k   [ (wk )], we
220

fiTractable Cost-Optimal Planning

(i) construct the graph Ge (v), and
(ii) use the Dijkstra algorithm to compute shortest paths from the source node of Ge (v)
to all other nodes in that graph.
For each wi , we have  (wi ) = n, and thus the number of k-tuples as above for each v  V
is O(nk ). For each such k-tuple, the corresponding graph Ge (v) can be constructed in
time linear in the number of its edges = O(n2k+2  |Av |2 ) = O(n2k+2  22k+2 ) (Brafman &
Domshlak, 2003). The time complexity of the Dijkstra algorithm on a digraph
 G = (N, E) is
O(E log (N )), and on Ge (v) it gives us O n2k+2  22k+2  log nk+1  2k+1 . Putting things
together, the complexity of constructing COP is

O n3+3  22+2  log n+1  2+1 .
(4)

Applying a tree-decomposition of COP along the scopes of its functional components we
arrive into an equivalent, tree-structured constraint optimization problem over n variables
with domains of size O(n+1 ). This COP is defined by the hard binary compatibility
constraints between the variables, and costs associated with the variables values. Such a
tree-structured COP can be solved in time O(xy 2 ) where x is the number of variables and y
is an upper bound on the size of a variables domain (Dechter, 2003). Therefore, solving our
COP can be done in time O(n2+3 ). As the expression in Eq. 4 dominates both O(n2+3 ),
and the time complexity of extracting a plan from the optimal solution to COP (see the
proof of (I) in Theorem 2), the overall complexity of the algorithm polytree-k-indegree is
given by Eq. 4. And since in Pb we have  = O(1), we conclude that the complexity of
polytree-k-indegree is polynomial in the description size of .

3.3 Towards Practically Efficient Special Cases

The polytree-k-indegree algorithm for Pb is polynomial, but is rather involved and its complexity is exponential in In(CG()). It is quite possible that more efficient algorithms for
Pb , or some of its fragments can be devised. Indeed, below we show that a simple algorithm for T  Pb problems has already appeared in the literature in a different context,
but it was never checked to when (if at all) it provides cost-optimal solutions. This is
the TreeDT algorithm for preferential reasoning with tree-structured CP-nets (Boutilier,
Brafman, Domshlak, Hoos, & Poole, 2004), and it turns out that its straightforward adaptation for T planning problems always provides cost-optimal solutions for T problems with
uniform-cost actions. The algorithm is depicted in Figure 9, and it is not hard to verify that
its time complexity is linear in the length of the generated plan all it does is iteratively
removing the parts of the problem that can be safely ignored in the later steps, and then
applying a value-changing action on a lowest (in the causal graph) variable for which such
an action exists.
Theorem 4 Given a T problem  with uniform-cost actions over n state variables,
(I) the algorithm tree-uniform-cost finds a plan if and only if  is solvable,
(II) if the algorithm tree-uniform-cost finds a plan for , then this plan is cost-optimal,
and
221

fiKatz & Domshlak

procedure tree-uniform-cost( = (V, A, I, G))
takes a problem   T with uniform-cost actions A
returns a cost-optimal plan for  if  is solvable, and fails otherwise
 = hi, s := I, V  := V
loop
V  := remove-solved-leafs(s, V  )
if V  =  return 
else
find v  V  , a  Av such that a  A(s) and
u  Desc(v, V  ) : Au  A(s) = 
if not found return failure
 :=   hai, s := (s \ pre(a))  eff(a)
Figure 9: A simple algorithm for cost-optimal planning for T problems with uniform-cost
actions. The notation Desc(v, V  ) stands for the subset of V  containing the
descendants of v in CG(), and A(s) stands for the set of all actions applicable
in the state s.

(III) the time complexity of tree-uniform-cost is (n2 ).
Proof: Without loss of generality, in what follows we assume that the actions of  are all
unit-cost, that is, for each plan  for , C() = ||.
(I) Straightforward by reusing as is the proof of Theorem 11 by Boutilier et al. (2004).
(II) Assume to the contrary that the plan  provided by tree-uniform-cost is not optimal,
that is, there exists a plan  such that | | < ||. In particular, this implies existence of a
variable v such that | v | < |v |. The semantics of planning implies that
| v |  |v |  (v + 1)

(5)

where v = 1 if G[v] is specified, and 0 otherwise. Likewise, since the causal graph CG()
forms a directed tree, there exists a variable v satisfying Eq. 5 such that, for all the descendants u of v in CG() holds:
| u |  |u |
(6)
Let Ch(v) be the set of all the immediate descendants of v in CG(). By the construction
of tree-uniform-cost, we have that:
1. If Ch(v) = , then |v |  v , and this contradicts Eq. 5 as | v | is a non-negative
quantity by definition.
2. Otherwise, if Ch(v) 6= , then, by the construction of tree-uniform-cost, there exists
u  Ch(v) such that changing its value |u | times requires changing the value of v
at least |v |  v times. In other words, there is no action sequence  applicable
222

fiTractable Cost-Optimal Planning

in I such that |u |  |u | while |v | < |v |  v . However, from Eq. 6 we have
| u |  |u |, and thus | v | has to be at least |v |  v . This, however, contradicts
Eq. 5.
Hence, we have proved that | v |  |v |, contradicting our assumption that | | < ||.
(III) Implied by Theorems 12 and 13 by Boutilier et al. (2004).



The requirement in Theorem 4 for all actions to have the same cost is essential. The
example below shows that in more general case the algorithm tree-uniform-cost is no longer
cost-optimal. Consider  = (V, A, I, G)  T with V = {v, u}, I = {bv , bu }, G = {bv , wu },
and A = {a1 , a2 , a3 , a4 } with
eff(a1 ) = {wv }, pre(a1 ) = {bv }
eff(a2 ) = {bv }, pre(a2 ) = {wv }
eff(a3 ) = {wu }, pre(a3 ) = {bu , wv }
eff(a4 ) = {wu }, pre(a4 ) = {bu , bv }
C(a1 ) = C(a2 ) = C(a3 ) = 1
C(a4 ) = 4
On this problem, the tree-uniform-cost algorithm returns  = ha4 i with C() = 4, while the
optimal plan is  = ha1 , a3 , a2 i with C( ) = 3.

4. Cost-Optimal Planning for P(1) with Uniform-Cost Actions
In this section we provide a polynomial time algorithm for cost-optimal planning for P(1)
problems with uniform-cost actions. We begin with showing that such problems exhibit an
interesting property, then we exploit this property for devising a planning-to-COP scheme
for these problems, and then prove the correctness and complexity of the algorithm.
We begin with providing some useful notation. Given a P(1) problem  = (V, A, I, G),
for each v  V , each w  pred(v), and each   {bv , wv },   {bw , ww }, by a| we denote
the action a with eff(a)[v] =  and pre(a)[w] = . Since  is 1-dependent, the applicability
of a| is prevailed only by the value of w. It is important to keep in mind that a| is just
a notation; the action a| may not belong to the action set A of .
4.1 Post-Unique Plans and P(1) Problems
We now proceed with introducing the notion of post-unique action sequences that plays a
key role in our planning-to-COP compilation here.
Definition 4 Let  = (V, A, I, G) be a UB problem instance. An action sequence   A
is called post-unique if, for each pair of actions a, a  , we have eff(a) = eff(a ) only if
a = a . That is, all the changes of each variable to a certain value along  are performed
by the same (type of ) action. The (possibly empty) set of all post-unique plans for  is
denoted by P pu () (or simply P pu , if the identity of  is clear from the context).
223

fiKatz & Domshlak

The notion of post-unique action sequences is closely related to the notion of postunique planning problems (Backstrom & Klein, 1991; Backstrom & Nebel, 1995), but is
considerably weaker than the latter. While action sets of post-unique planning problems
are not allowed to contain two actions with the same effect, Definition 4 poses a similar
restriction only on action sequences, and not on the underlying planning problems. Still,
the property of post-uniqueness for plans is strong. In general, solvable problems in UB
may not exhibit post-unique plans at all. Turns out, however, that for the problems in P(1)
this is very much not the case.
Theorem 5 For any solvable P(1) problem  = (V, A, I, G), we have P pu () 6= . Moreover, if the actions A are uniform-cost, then P pu () contains at least one cost-optimal
plan.
Proof: As the correctness of the second claim immediately implies the correctness of the
first one, we focus on the proof the second claim. Given a P(1) problem  = (V, A, I, G)
with uniform-cost actions, and plan  = ha1 , . . . , am i for , we construct a sequence of
actions  such that:
  is a post-unique plan for ,
 C( ) = C().
This construction is two-step. First, for each v  V , we map the subsequence v =
hai1 , . . . , aik i into a post-unique sequence of actions v = hai1 , . . . , aik i. Note that the
indexes i1 , . . . , ik of the action elements of each v are the global indexes of these actions
along , and exactly the same indexes are used for marking the elements of the constructed
sequences v . Having constructed the sequences v1 , . . . , vn , we then merge them into a
single actions sequence  , and show that  is a valid plan for . The two properties of 
as required above will then hold immediately because | | = ||, and post-uniqueness of 
is implied by the individual post-uniqueness of all its per-variable components v .
The mapping of subsequences v of  to the desired sequences v for all variables v is
performed top-down, consistently with a topological ordering of the causal graph CG().
This top-down processing allows us to assume that, when constructing v , the subsequences
w for all w  pred(v) are already constructed. Given that, while mapping each v =
hai1 , . . . , aik i to the corresponding v , we distinguish between the following three cases.
(1) The subsequence v is already post-unique.
In this case, we simply set v to v . In addition, we construct the following sets of
ordering constraints. First, we set a binary relation >v over the action elements of
v = hai1 , . . . , aik i to
>v = {ai < aj | ai , aj  v , i < j}.
(7)
It is immediate from Eq. 7 that >v is a strict total order over the elements of v as >v
simply follows the action indexing inherited by v from plan  via v .
Now, for each w  pred(v), we set a binary relation >v,w over the elements of v and
w to
>v,w =

(S

 

a
i v ,aj w

{ai < aj | i < j}  {aj < ai | j < i},

,

pre(a)[w] is specified for some a  v
otherwise

.

(8)

224

fiTractable Cost-Optimal Planning

For each w  pred(v), the relation >v,w defined by Eq. 8 is a strict total order over its
domain because the ordering constraints between the elements of v and w are a subset
of the constraints induced by the total-order plan  over the (corresponding) actions
from v and w . For the same reason, from Eqs. 7 and 8, we have that, for each
w  pred(v), >v  >v,w is a strict total order over the union of the elements of v and
w .
S
From Eqs. 7-8 we can now derive that any linearization of >v  wpred(v) >v,w defines
a sequence of actions that is applicable with respect to {v}  pred(v). In addition,
|v | = |v | implies that this action sequence provides to v the value G[v] if the latter
is specified.
(2) The subsequence v is not post-unique, but the actions in v are all prevailed by the
value of a single parent w  pred(v).
Since v is not post-unique, v in this case has to contain instances of at least three
action types from {abv |bw , abv |ww , awv |bw , awv |ww }. Thus, in particular, it must be that
(a) |w |  1, and
(b) for some   {bw , ww }, we have awv | , abv |  v .
Given that, we set v = hai1 , . . . , aik i to
1  j  k :

aij

(
awv | ,
=
abv | ,

j is odd
.
j is even

Both post-uniqueness of such v , as well as its applicability with respect to v are straightforward. The ordering constraints >v are then set according to Eq. 7. Likewise, if
w = haj1 , . . . , ajl i, we set
S


 = bw

Sai v {ai < aj1 },

>v,w =
(9)
 = ww , l = 1
ai v {ai > aj1 },

S

   {a > aj }  {a < aj },  = ww , l > 1
ai v

i

1

i

2

Finally, the ordering constraints >v,w for the rest of the parents w  pred(v) \ {w} are
set to empty sets.

The relation >v here is identical to this in case (1), and thus it is a strict total order
over the elements of v . From Eq. 9, it is easy to verify that >v,w is also a strict partial
order over the union of the elements of v and w . Finally, as all the elements of v
are all identically constrained with respect to the elements of w , we have >v  >v,w
forming a strict partial order over the union of the elements of v and w . (For all other
parents w  pred(v), we simply have >v  >v,w = >v .)
S
From Eqs. 7 and 9 we can now derive that any linearization of >v  wpred(v) >v,w
defines a sequence of actions that is applicable with respect to {v}pred(v). In addition,
|v | = |v | implies that this action sequence provides to v the value G[v] if the latter
is specified.
225

fiKatz & Domshlak

(3) The subsequence v is not post-unique, and the actions of v are prevailed by more
than one parent of v.
The setting of this case in particular implies that there is a pair of vs parents {u, w} 
pred(v) such that awv | , abv |  v for some   {bu , wu },   {bw , ww }. Given that,
we set v to
(
awv | , j is odd
,
1  j  k : aij =
abv | , j is even
and, similarly to case (2), both post-uniqueness of v , and its applicability with respect
to v are straightforward.
Here as well, the ordering constraints >v are set according to Eq. 7. Likewise, if
w = haj1 , . . . , ajl i, and u = haj1 , . . . , aj  i, we set >v,w according to Eq. 9 above, and
l
>v,u according to Eq. 10 below.

>v,u

S


 = bu

Sai v {ai < aj1 },

 = wu , l = 1
=
ai v {ai > aj1 },

S

   {a > a  }  {a < a  },  = wu , l > 1
j1
j2
i
i
a v

(10)

i

Finally, the ordering constraints >v,w for the rest of the parents w  pred(v) \ {u, w}
are set to empty sets.
The relation >v here is identical to this in cases (1-2), and relations >v,u and >v,w
are effectively identical to the relation >v,w in case (2). Thus, we have >v  >v,u and
>v  >v,w forming strict partial orders over the unions of the elements of v and u ,
and v and w , respectively.
S
From Eqs. 7, 9, and 10 we can now derive that any linearization of >v  wpred(v) >v,w
defines a sequence of actions that is applicable with respect to {v}pred(v). In addition,
|v | = |v | implies that this action sequence provides to v the value G[v] if the latter
is specified.
As the last step, we now prove that, for each v  V and each w  pred(v), we have
>w  >v,w being a strict partial order over the union of the elements of w and v .
- If >v,w is constructed according to Eq. 8, then >w  >v,w is a subset of the constraints
induced by plan  over the (corresponding to v and w ) actions from v and w .
- Otherwise, if >v,w is constructed according to Eq. 9 or (in this case, equivalent)
Eq. 10, then >v,w (i) addresses at most two elements of w , (ii) orders these elements
consistently with >w .
In both cases, the argued properties of >w  >v,w implies that it forms a strict partial order
over the union of the elements of v and w .
Until now, we have specified the sequences v , the orders >v induced by these sequences,
the orders >v,w , and proved that all >v  >v,w and >w  >v,w form strict partial orders
226

fiTractable Cost-Optimal Planning

over their domains. This construction allows us to apply now Theorem 1 to the (considered
as sets) sequences v and orders >v and >v,w , proving that
[
[
>=
(>v 
>v,w )
vV

wpred(v)

forms a strict partial order over the union of v1 , . . . , vn . Putting thing together, the
above implies that any linearization  of > is a plan for , and post-uniqueness of all its
subsequences v1 , . . . , vn then implies   P pu (). Moreover, if  is an optimal plan for
, then | | = || implies the optimality of  .

4.2 Construction, Correctness, and Complexity
The main impact of Theorem 5 on our planning-to-COP scheme for uniform-cost P(1) is
that we can now restrict our attention to post-unique plans only. Given that, the constraint
optimization problem COP = (X , F) for a uniform-cost problem  = (V, A, I, G)  P(1)
is specified as follows.
The variable set X contains a variable xv for each planning variable v  V , and a
variable xw
v for each edge (w, v)  CG(). That is,
X = X V  X E,
X V = {xv | v  V }

(11)

X E = {xw
v | (w, v)  CG()}
For each variable xv  X V , the domain Dom(xv ) consists of all goal-valid prefixes of
E
w
(v). For each variable xw
v  X , the domain Dom(xv ) consists of all triples of integers
[w , b , ]] satisfying Eq. 12.
Dom(xv ) =  [(v)]
Dom(xw
v ) = {[[w , b , ]] | w , b  {0, 1}, 0    n}

(12)

The semantics of Eq. 12 is as follows. Let {w1 , . . . , wk } be an arbitrary fixed ordering
of pred(v). If xv takes the value v  Dom(xv ), then v is forced to provide the sequence
i
of values v . In turn, if xw
v takes the value [w , b , ]], then  corresponds to the number
of value changes of v, w = 1 (b = 1) forces the subset of parents {w1 , . . . , wi }  pred(v)
to support (that is, prevail) all the changes of v to wv (respectively, to bv ), and w = 0
(b = 0) relieves this subset of parents {w1 , . . . , wi } from that responsibility.
For each variable x  X , the set F contains a non-negative, real-valued function x with
the scope


{xv },
x = xv , k = 0



{x , xwk },
x = xv , k > 0
v v
(13)
Qx =
w1 , k > 0
w
1

},
x
=
x
,
x
{x
w

v
1
v


{xwj , xwj1 , x }, x = xwj , 1 < j  k
v
v
v
wj
where pred(v) = {w1 , . . . , wk } (and k = 0 means pred(v) = ). Proceeding now with
specifying these functional components F of COP , first, for each xv with pred(v) = , and
227

fiKatz & Domshlak

for each v   [(v)], we set xv (v ) to


0,



1,
xv (v ) =

|v |  1,



,

|v | = 1,
(|v | = 2)  (awv  Av ),
(|v | > 2)  (awv , abv  Av ),
otherwise

(14)

In turn, for each planning variable v  V with pred(v) = {w1 , . . . , wk }, k > 0, the
function xv is set to


0,



1,
xv (v , [w , b , ]]) =

|v |  1,



,

(|v | = 1)  ([[w , b , ]] = [0, 0, 0]]),
(|v | = 2)  ([[w , b , ]] = [1, 0, 1]]),
(|v | > 2)  ([[w , b , ]] = [1, 1, |v |  1]]),
otherwise

(15)

The functions xv capture the, marginal over the actions Av , cost of providing a sequence
v of value changes of v in , given that (in case of Eq. 15) the parents of v are ready to
support these value changes. In specifying the remaining functional components we use
an indicator function  specified in Eq. 16.
8
>
>0,
>
>
>
>
0,
>
>
>
>
>
<0,
 ([[w , b , ]] , w ) = 0,
>
>
>
0,
>
>
>
>
>
0,
>
>
>
:,

w = 0, b = 0,
w = 1, b = 0, (awv |bw  Av )  ((|w | > 1)  (awv |ww  Av )),
w = 0, b = 1, (abv |bw  Av )  ((|w | > 1)  (abv |ww  Av )),
w = 1, b = 1, (awv |bw , abv |bw  Av )  ((|w | > 1)  (awv |ww , abv |ww  Av )),
w = 1, b = 1, |w |  , awv |bw , abv |ww  Av ,
w = 1, b = 1, |w | > , awv |ww , abv |bw  Av ,
otherwise
(16)

The semantics of  is that, for each planning variable v  V , each w  pred(v), and each
([[w , b , ]] , w )  Dom(xw
v )  Dom(xw ), we have ([[w , b , ]] , w ) = 0 if the value sequence
w of w can support all the changes of v to wv (if w = 1) and all the changes of v to bv (if
b = 1), out of  value changes of v in . Given this indicator function , for each v  V ,
the functional component xwv 1 is specified as
xwv 1 ([[w , b , ]] , w1 ) =  ([[w , b , ]] , w1 ) ,

(17)

and the rest of the functions xwv 2 , . . . , xwv k are specified as follows. For each 2  i  k,



i
the value of the function xwj at the combination of [w , b , ]]  Dom(xw
v ), [w , b ,  ] 
v
w
Dom(xv i1 ), and wi  Dom(xwi ) =  [(wi )] is specified as
`



] , w , b ,   , wj =
xw
i [w , b , ]
v

( `

 [w  w , b  b , ]] , wj ,


228

 =    w  w  b  b
otherwise

(18)

fiTractable Cost-Optimal Planning

procedure polytree-1-dep-uniform( = (V, A, I, G))
takes a problem   P(1) with uniform-cost actions A
returns a cost-optimal plan for  if  is solvable, and fails otherwise
create a set of variables X as in Eqs. 11-12
create a set of functions F = {x | x  X } with scopes as in Eq. 13
for each x  X do
specify x according to Eqs. 14-18
endfor
P
set COP := (X , F) with global objective min F (X )
x :=
P solve-tree-cop(COP  )
if F (x) =  then return failure
P
extract plan  from x with C() = F (x)
return 
Figure 10: Algorithm for cost-optimal planning for P(1) problems with uniform-cost actions.

This finalizes the construction of COP , and this construction constitutes the first three
steps of the algorithm polytree-1-dep-uniform in Figure 10(a). The subsequent steps of this
algorithm are conceptually similar to these of the polytree-k-indegree algorithm in Section 3,
with the major difference being in the plan reconstruction routines. It is not hard to verify
from Eqs. 11-13, and the fact that the causal graph of   P(1) forms a polytree that
(i) for each variable x  X , |Dom(x)| = poly(n),
(ii) the tree-width of the cost network of F is  3, and
(iii) the optimal tree-decomposition of the COP s cost network is given by any topological
ordering of the causal graph that is consistent with the (arbitrary yet fixed at the time
of the COP s construction) orderings of each planning variables parents in the causal
graph.
For an illustration, we refer the reader to Figure 4 (p. 211) where Figure 4(a) depicts
the causal graph of a problem   P(1), and Figure 4(c) depicts the cost network of the
corresponding COP . The top-most variables and the cliques in the cost network correspond
to the functional components of COP .
We now proceed with proving the correctness and complexity of the polytree-1-depuniform algorithm.
Theorem 6 Let  be a P(1) problem with uniform-costs actions, COP = (X , F) be the
corresponding constraint optimization problem, and x be an optimal assignment to X with
P
F (x) = .
(I) If  < , then a plan of cost  for  can be reconstructed from x in time polynomial
in the description size of .

(II) If  has a plan, then  < .
229

fiKatz & Domshlak

Proof Sketch: The proof of Theorem 6 is in Appendix A. p. 249. The overall flow of the
proof is similar to this of the proof of Theorem 2, yet the details are very much different.
The main source of the proofs complexity is that, in proving (I), we must distinguish
between several cases based on the roles taken by the (up to) two parents supporting the
value changes of each variable in question.

Theorem 7 Cost-optimal planning for P(1) with uniform cost actions is tractable.
Proof: Given a planning problem   P(1) with uniform cost actions, we show that the
corresponding constraint optimization problem COP can be constructed and solved in time
polynomial in the description size of .
Let n be the number of state variables in . In polytree-1-dep-uniform, we first construct the constraint optimization problem COP over (n2 ) variables X with domain sizes
bounded by O(n), and (n2 ) functional components F, each defined over at most three
COP variables. The construction is linear in the size of the resulting COP, and thus is
accomplished in time O(n5 ).
Applying then to COP a tree-decomposition along the scopes of the functional components F, we arrive into an equivalent, tree-structured constraint optimization problem
over (n2 ) variables with domains of size O(n3 ). Such a tree-structured COP can be solved
in time O(xy 2 ) where x is the number of variables and y is an upper bound on the size
of a variables domain (Dechter, 2003). Therefore, solving COP can be done in time
O(n8 ). As this dominates both the time complexity of constructing COP , and the time
complexity of extracting a plan from the optimal solution to COP (see the proof of (I) in
Theorem 6), the overall complexity of the algorithm polytree-1-dep-uniform is O(n8 ), and
therefore polynomial in the description size of .


5. Cost-Optimal Planning for P(1) with General Action Costs
We now consider cost-optimal planning for P(1) problems without the constraints on actions
to be all of the same cost. While Theorem 5 in Section 4 shows that any solvable P(1)
problem  has at least one post-unique plan, it is possible that no such plan is cost-optimal
for , and Example 1 below affirms this possibility.
Example 1 Let  = hV, A, I, Gi be the P(1) problem instance over variables V = {v1 , . . . , v5 },
I = {0, 0, 0, 0, 0}, G = {v1 = 0, v2 = 1, v3 = 1}, and actions A as depicted in Figure 11a.
The polytree causal graph of  is shown in Figure 11b, and it is easy verify from the table
in Figure 11a that   P(1).
Ignoring the non-uniformity of the action costs, this problem has a post-unique costoptimal plan  = ha4  a2  a1  a5  a3  a4 i. However, considering the action costs as in the last
column in Figure 11a, then the cost of each optimal plan, such as  = ha4 a2 a1 a5 a3 a7 a6 i
will be C( ) = 16 < 24 = C(). Note that the plan  is not post-unique because it changes
the value of v3 to 1 using both actions a4 and a6 . In fact, any plan for this problem will
have at least two action instances that change the value of v3 to 1. However, the cheap such
action a6 cannot be applied twice because it requires v4 = 1, and the only action a5 that
sets v3 = 0 cannot be applied after a6 a5 requires v4 = 0, and there is no action that has
230

fiTractable Cost-Optimal Planning

A
a1
a2
a3
a4
a5
a6
a7

v1

v2

1
0
1

0

pre(a)
v3 v4

v5

v1

v2

eff(a)
v3 v4

1
1
0
0
1
0

1
0
0

1
0
1

0
1
0

1
(a)

v5

C(a)
1.0
1.0
1.0
10.0
1.0
1.0
1.0

v4

>>
>>
>>


v5
v3


v1


v2
(b)

Figure 11: Action set and the causal graph for the problem in Example 1.
this effect. Therefore, any post-unique plan for this problem will have to invoke twice the
action a4 , and thus will have cost of at least 2  C(a4 ) = 20.
Fortunately, we show that any solvable P(1) problem is guaranteed to have a costoptimal plan satisfying a certain relaxation of action sequence post-uniqueness that still
allows us to devise a (more costly than polytree-1-dep-uniform) planning-to-COP scheme for
general P(1) problems.
5.1 Post-3/2 Plans and P(1) Problems
We now proceed with introducing the notion of post-3/2 property for action sequences that
relaxes the post-uniqueness property exploited in the previous section.
Definition 5 Let  = (V, A, I, G) be a UB problem instance. An action sequence   A
is called post-3/2 if, for each v  V , a  v , there exist  6=   {bv , wv }, a parent
w  pred(v), ,   {bw , ww }, and   {bu , wu | u  pred(v)}, such that a  {a| , a| , a| }.
That is, all the changes of each variable are done using at most three types of actions which
are prevailed by at most two parents, and if u is different from w, then different actions
prevailed by w perform different value changes of v.
The (possibly empty) set of all post-3/2 plans for  is denoted by P 3/2 () (or simply
P 3/2 , if the identity of  is clear from the context).
To illustrate the rather involved definition of post-3/2 plans, consider the following four
action sequences of four actions each. The only value changes made by these sequences are
changes of variable v with pred(v) = {x, y, z}.
 hawv |bx  abv |wx  awv |bx  abv |by i is post-3/2 because it uses three types of actions, each
prevailed by one of the two parents x, y.
 hawv |bx  abv |by  awv |bx  abv |by i is post-3/2 because it uses two types of actions, each
prevailed by one of the two parents x, y.
231

fiKatz & Domshlak

 hawv |bx  abv |by  awv |bx  abv |bz i is not post-3/2 because it uses three types of actions,
each prevailed by one of the three parents x, y, z.
 hawv |bx  abv |wx  awv |by  abv |by i is not post-3/2 because it uses four types of actions,
each prevailed by one of the two parents x, y.
It is not hard to verify that post-3/2 is a relaxation of post-uniquenessif a plan is
post-unique, then it is post-3/2, but not necessarily the other way around. Turns out that,
for any P(1) problem , this relaxed property is guaranteed to be satisfied by at least one
cost-optimal plan for .
Theorem 8 For every solvable P(1) problem  = (V, A, I, G), the plan set P 3/2 () contains at least one cost-optimal plan.
Proof Sketch: The proof of Theorem 8 is in Appendix A, pp. 255-278. This proof is
flow-wise similar to the proof of Theorem 5, but is technically much more involved. Here
we provide only a sketch of the proof. We note, however, that many building blocks of this
proof are then used in the correctness proof for our planning-to-COP algorithm (notably,
in Theorem 9).
Given a P(1) problem  = (V, A, I, G), and cost-optimal plan  for , we construct a
post-3/2 plan  for , such that C( ) = C(). In nutshell, first, for each v  V , we map
the subsequence v = ha1 , . . . , ak i of  into a sequence of actions v = ha1 , . . . , ak i that
(i) satisfy the post-3/2 property, and (ii) C(v )  C(v ). Then, we merge the constructed
sequences {v }vV into  , and show that  is a valid plan for . The two properties of 
as required above will then hold immediately because C( ) = C(), and  being post-3/2
is implied by all its per-variable components v being post-3/2.
For each variable v  V , with pred(v) = , we set v = v . In turn, for each variable
v  V with pred(v) 6= , given {w }wpred(v) , such that |w | = |w | + 1, let ai be the ith
cheapest action that changes variable v to   {bv , wv } and prevailed by some value from
{w }wpred(v) (that is, applicable given the sequences of values {w }wpred(v) respectively
obtained by the parents of v). The proof considers (in groups) all possible settings for
b
b
aw
aw
2 = awv | , a1 = abv | , a2 = abv | (that is, all possible combinations of
1 = awv | , S
{, , , }  wpred(v) {bw , ww }) on a case-by-case basis. Specifically, the cases correspond
to
(I)  =   {bw , ww }.
(II)   {bw , ww },   {bu , wu }, such that w 6= u.
(III)  = bw ,  = ww ; here we distinguish between a few cases based on w and v .
(1) |v | = 2y + 1, |w | = 2x, |w |  |v |.
(2) |v | = 2y + 1, |w | = 2x, |w | > |v |.
(3) |v | = 2y, |w | = 2x, |w | < |v |.
(4) |v | = 2y, |w | = 2x, |w |  |v |.
(5) |v | = 2y + 1, |w | = 2x + 1, |w | < |v |.
(6) |v | = 2y + 1, |w | = 2x + 1, |w |  |v |.
232

fiTractable Cost-Optimal Planning

(7) |v | = 2y, |w | = 2x + 1, |w |  |v |.
(8) |v | = 2y, |w | = 2x + 1, |w | > |v |.
(IV)  = ww ,  = bw ; here as well we distinguish between eight cases that specification-wise
almost identical to these of (III), but lead to different settings of v .

5.2 Construction, Correctness, and Complexity
Given a post-3/2 action sequence  from A and a variable v  V , we can distinguish between
the following exhaustive roles of each parent w  pred(v) with respect to v along .
R1 All the actions in  that change the value of v are supported by the same value of w.
That is, for some   {bw , ww }, if a  v , then a  {abv | , awv | }.
R2 All the actions in  that change the value of v to wv are supported by the same value
of w, and all the actions in  that change the value of v to bv are supported by another
value of w.
That is, for some  6=   {bw , ww }, if a  v , then a  {abv | , awv | }.
R3 All the actions in  that change the value of v to wv are supported by the same value
of w, and none of the actions in  that change the value of v to bv are supported by w.
That is, for some   {bw , ww } and  6 {bw , ww }, if a  v , then a  {abv | , awv | }.
R4 All the actions in  that change the value of v to bv are supported by the same value
of w, and none of the actions in  that change the value of v to wv are supported by w.
That is, for some   {bw , ww } and  6 {bw , ww }, if a  v , then a  {abv | , awv | }.
R5 All the actions in  that change the value of v to wv are supported by the same value of
w, and all the actions in  that change the value of v to bv are supported by two values
of w.
That is, for some  6=   {bw , ww }, if a  v , then a  {awv | , abv | , abv | }.
R6 All the actions in  that change the value of v to bv are supported by the same value
of w, and all the actions in  that change the value of v to wv are supported by two
values of w.
That is, for some  6=   {bw , ww }, if a  v , then a  {abv | , awv | , awv | }.
R7 All the actions in  that change the value of v to wv are supported by the same value
of w, and some of the actions in  that change the value of v to bv are supported by
another value of w and others are supported by another parent.
That is, for some  6=   {bw , ww } and  6 {bw , ww }, if a  v , then a  {awv | , abv | , abv | }.
R8 All the actions in  that change the value of v to bv are supported by the same value
of w, and some of the actions in  that change the value of v to wv are supported by
another value of w and others are supported by another parent.
That is, for some  6=   {bw , ww } and  6 {bw , ww }, if a  v , then a  {abv | , awv | , awv | }.
233

fiKatz & Domshlak

R9 Part of the actions in  that change the value of v to bv are supported by the same
value of w, and none of the actions in  that change the value of v to wv are supported
by the same value of w.
R10 Part of the actions in  that change the value of v to wv are supported by the same
value of w, and none of the actions in  that change the value of v to bv are supported
by the same value of w.
R11 None of the actions in  are supported by w.
That is, if a|  , then  6 {bw , ww }.
For a given post-3/2 action sequence  from A and a variable v  V , each parent of v
performs one of the roles R1-R11 with respect to v along , and each of the roles R1-R10
is performed by at most one of the parents of v. In addition, there are sets of roles that
cannot be simultaneously performed by the parents of v with respect to v and the same
action sequence , and there are roles that have to be performed in pairs. Specifically,
 If one of the roles {R1,R2,R5,R6} is played by some parent w  pred(v), then R11
must be played by all other parents w  pred(v) \ {w }.
 If R3/R7/R8 is played by some parent w1  pred(v), then R4/R9/R10, respectively,
must be played by some parent w2  pred(v) \ {w1 }, and R11 must be played by all
other parents w  pred(v) \ {w1 , w2 }.
Considering a variable v and its parents pred(v) through the lens of these eleven roles,
suppose we now aim at assigning these roles to pred(v) by considering them one after another
in some arbitrary order. Given the aforementioned constraints on the role assignment, at
each step of this sequential process we can be in one of the following eight states, with the
whole process being described by a state machine depicted in Fig. 12.
S1 All the roles R1-R11 are still available (to be assigned to the parents of v).
S2 Only roles {R3,R11} are still available.
S3 Only roles {R4,R11} are available.
S4 Only roles {R7,R11} are available.
S5 Only roles {R8,R11} are available.
S6 Only roles {R9,R11} are available.
S7 Only roles {R10,R11} are available.
S8 Only role R11 is available.
Given this language of roles and states, we now proceed with specifying our constraint optimization problem COP = (X , F) for a problem  = (V, A, I, G)  P(1). In
what follows, for each variable v  V , we assume a fixed (arbitrarily chosen) numbering
{w1 , . . . , wk } of pred(v) with respect to v.
234

fiTractable Cost-Optimal Planning

R11

start



@ABC
GFED
p7 S2 NNNN
ppp
R11NNN
p
p
NNN
p


p
p
NNN
pp
@ABC
GFED
p
NNN
p
T
5
S3
TTTT
p
jj
p
j
NNNR3
p
j
T
j
R4 pp
T
j
R11
T
j
TTTT
NN
jj
pp
j
p
j


T
j
p
TTTTR4 NNNN
p R3 jjjj
p
p
@ABC
GFED
T
j
TTTT NNNN
Y
pppjjjjjj
eeeeee2 S4 YYYYYYYYYYYY
e
TT N
p
e
e
e
p
e
j
e
R9
YYYR7
R11pppjjjj
YYYYYYY TTTTNTNTNNN
eeee
e
e
e
e
e
p
j
e
YYYYYY TTTNN 


 ppjpjjjeeeeeee
YYYYT, ) '
R1,R2,R5,R6
jeeee
@ABC
@ABC
89:;
?>=<
/ GFED
/ GFED
S1 NTYNTYTYTYYYYYY
ejejejpejp2 5 7 S8
e
e
e
e
e
NNNTTT YYYYYYYR10
e
e
j
p
e
e
R8
R11
YYYYYY
NNNTTTT
eeeeee
jjjj pp
YYYYYYY


NNN TTTTT
eeeeeee R9 jjjjjjppppp
e
e
e
Y
e
Y
e
Y
R7
e
Y
, GFED
e
NNN TTTT
@ABC
jjjj ppppp
S5
TTTT
NNN
jjjR10
j
j
TTTT
pp
NNR8
j
R11 jjj
NNN
TTTT
ppp
p
p
NNN
TT) 
 jjjjjj
ppp
NNN
@ABC
GFED
S6
ppp
NNN
p
p
NNN
pp
NNN 
 pR11
ppp
'
p
@ABC
GFED
S7

Figure 12: State machine describing the process of sequential role assignment to the parents of v (with respect to v). Each transition is labeled with a set of roles, one
of which is getting assigned to a parent of v at the corresponding step.

1. Similarly to the uniform-cost case, the variable set X contains a variable xv for each
planning variable v  V , and a variable xw
v for each edge (w, v)  CG(). That is,
X = XV  XE
X V = {xv | v  V }

(19)

X E = {xw
v | (w, v)  CG()}
2. For each variable xv  X V , the domain Dom(xv ) consists of all possible valid prefixes
E
wi
i
of (v). For each variable xw
v  X , the domain Dom(xv ) consists of all possible
quadruples satisfying Eq. 20.
Dom(xv ) = {v   [(v)]}

wi
Dom(xv ) = [S, #w , #b , ]]

fi

fi 0    n, 0  #w , #b    
2
fi
fi S  {S1, . . . , S8}

(20)

The semantics of Eq. 20 is as follows. Let {w1 , . . . , wk } be an arbitrary fixed ordering of
pred(v). If xv takes the value v  Dom(xv ), then v is forced to provide the sequence of
i
values v . In turn, if xw
v takes the value [S, #w , #b , ]], then  corresponds to the number
of value changes of v, #w and #b correspond to the number of value changes of v to wv
and bv , respectively, that should be performed by the actions prevailed by the values of
235

R11

fiKatz & Domshlak

{w1 , . . . , wi }, and the state-component S captures the roles that can be assigned to the
parents {w1 , . . . , wi }.
3. Similarly to the uniform-cost case, for each variable x  X , the set F contains a nonnegative, real-valued function x with the scope


{xv },



{x , xwk },
v v
Qx =
w

{xv 1 , xw1 },



{xwj , xwj1 , x },
v
v
wj

x = xv , k = 0
x = xv , k > 0
1
x = xw
v ,k > 0
wj
x = xv , 1 < j  k

(21)

where pred(v) = {w1 , . . . , wk } (with k = 0 meaning pred(v) = ).
Proceeding now with specifying the functional components F of COP , first, for each
xv with pred(v) = , and for each v   [v], we set xv (v ) according to Eq. 22.


0,
|v | = 1,



C(a ),
|v | = 2, awv  Av ,
wv
xv (v ) =
(22)
|
|1
|
|1
v
v

 2   C(awv ) +  2   C(abv ), |v | > 2, awv , abv  Av ,



,
otherwise

In turn, for each planning variable v  V with pred(v) = {w1 , . . . , wk }, k > 0, the function
xv is set as in Eq. 23.


|v | = 1, [S, #w , #b , ]] = [hhS8, 0, 0, 0]] ,
0,

ii
xv (v , [S, #w , #b , ]]) = 0,
|v | > 1, [S, #w , #b , ]] = S1,  |v2|1 ,  |v2|1 , |v |  1 ,


, otherwise

(23)
The semantics of Eq. 23 is simpleif no value changes of v are required, then trivially
no support of pred(v) to v is needed; otherwise, all possible roles for pred(v) should be
considered.
Now, we proceed with specifying a generic function  that, for each v  V , each
w  pred(v), and each (R, [S, #w , #b , ]] , w )  {R1, . . . , R10}  Dom(xw
v )  Dom(xw ),
provides the marginal over the actions Av cost of w taking the role R, and under this role,
supporting #w changes of v to wv and #b changes of v to bv , out of total  changes of v
needed. For ease of presentation, let (x1 , x2 , y1 , y2 ) denote the cost of an action sequence
consisting of x1 actions of type awv |bw , x2 actions of type awv |ww , y1 actions of type abv |ww ,
y2 actions of type abv |bw , that is
(x1 , x2 , y1 , y2 ) = x1  C(awv |bw ) + x2  C(awv |ww ) + y1  C(abv |ww ) + y2  C(abv |bw )

(24)

While the notation v,w is probably more appropriate for the semantics of , we adopt the
latter for its shortness because the identity of v and w will always be clear from the context.
The Eqs. 25-34 below specify  (R, [S, #w , #b , ]] , w ) for R  {R1, . . . , R10}. The semantics of  (R, [S, #w , #b , ]] , w ) is to capture the minimal cumulative cost over the actions from Av to achieve #w and #b (out of ) value changes of v under the support of the
236

fiTractable Cost-Optimal Planning

parent w playing the role R with respect to v. For example, role R3 means supporting all
the actions that change the value of v to wv , and Eq. 27 gives us the minimal cost of this
support in terms of the cumulative cost of the supported actions from Av . These minimal
costs are taken from the relevant cases in the proof of Theorem 8, notably
(Eq. 25) Case (I).
(Eq. 26) Cases {(III), (IV )}.{2, 4, 6, 8}.
(Eq. 27) Case (II), the cost of all actions that change the value of v to wv .
(Eq. 28) Case (II), the cost of all actions that change the value of v to bv .
(Eq. 29) Cases {(III), (IV )}.{1, 3, 5, 7}.a, the minimal cost.
(Eq. 30) Cases {(III), (IV )}.{1, 3, 5, 7}.b, the minimal cost.
(Eq. 31) Cases {(III), (IV )}.{1, 3, 5, 7}.a, the cost of all actions prevailed by one parent.
(Eq. 32) Cases {(III), (IV )}.{1, 3, 5, 7}.b, the cost of all actions prevailed by one parent.
(Eq. 33) The residue of cases {(III), (IV )}.{1, 3, 5, 7}.a. (Together with Eq. 31 this gives
us the full cost of changing v as required.)
(Eq. 34) The residue of cases {(III), (IV )}.{1, 3, 5, 7}.b. (Together with Eq. 31 this gives
us the full cost of changing v as required.)
8
>
(#w , 0, 0, #b ),
>
>
)
(
>
<
(#w , 0, 0, #b ),
 (R1, [S, #w , #b , ]] , w ) = min
,
>
(0, #w , #b , 0)
>
>
>
:,

8
>
(#w , 0, #b , 0),
>
>
)
(
>
<
(#w , 0, #b , 0),
(R2, [S, #w , #b , ]] , w ) = min
,
>
(0, #w , 0, #b )
>
>
>
:,

|w | > 1, #w =  2 , #b =  2 

(25)

otherwise

|w | =   2, #w =  2 , #b =  2 
|w | >   2, #w =  2 , #b =  2 

(26)

otherwise

8
#w (
C(awv |bw ),
>
>
)
>
<
#w  C(awv |bw ),
,
(R3, [S, #w , #b , ]] , w ) = min
>
#w  C(awv |ww )
>
>
:
,
8
C(abv |bw ),
>
>#b  (
)
>
<
#b  C(abv |bw ),
(R4, [S, #w , #b , ]] , w ) = min
,
>
#b  C(abv |ww )
>
>
:
,

237

|w | = 1, #w =  2 , #b =  2 

|w | = 1, #w =  2 , #b = 0
|w | > 1, #w =  2 , #b = 0

(27)

otherwise
|w | = 1, #w = 0, #b =  2 
|w | > 1, #w = 0, #b =  2 
otherwise

(28)

fiKatz & Domshlak

8
(
)
>
(y + 1, 0, x  1, y  x + 1),
>
>
min
,
>
>
>
(0, y + 1, y  x + 1, x  1)
>
>
>
>
>
(
)
>
>
>
(y,
0,
x,
y

x),
>
>
>
min
,
>
>
(0, y, y  x + 1, x  1)
>
>
>
>
>
>
>
>
>
>
(y, 0, 1, y  1),
>
>
>
>
>
>
>
>
>
>
>
(0, y, 1, y  1),
>
>
>
>
>
<
(
)
(R5, [S, #w , #b , ]] , w ) =
(y + 1, 0, x, y  x),
>min
,
>
>
>
(0, y + 1, y  x + 1, x  1)
>
>
>
>
>
>
>
>
>
(y + 1, 0, 1, y  1),
>
>
>
>
>
>
>
>
>
>
>
>
(0, y + 1, 1, y  1),
>
>
>
>
>
>
>
(
)
>
>
>
(y, 0, x, y  x),
>
>
min
,
>
>
>
(0, y, y  x, x)
>
>
>
>
>
:,

(R6, [S, #w , #b , ]] , w ) =

(
)
8
>
(x, y + 1  x, y, 0),
>
>
min
,
>
>
(y + 1  x, x, 0, y)
>
>
>
>
(
)
>
>
>
>
(x, y  x, y, 0),
>
>
min
,
>
>
>
(y  x + 1, x  1, 0, y)
>
>
>
>
>
>
>
>
>
(1, y  1, y, 0),
>
>
>
>
>
>
>
>
>
>
>
<(1, y  1, 0, y),
>
(
)
>
>
>
(x + 1, y  x, y, 0),
>
>
>
min
,
>
>
(y  x + 1, x, 0, y)
>
>
>
>
>
>
>
>
>
>
(1, y, 0, y),
>
>
>
>
>
>
(
)
>
>
>
>
(x, y  x, y, 0),
>
>min
,
>
>
(y  x, x, 0, y)
>
>
>
>
>
:
,

238

 = 2y + 1, |w | = 2x, 1 < x  y,
#w = y + 1, #b = y
 = 2y, |w | = 2x, 1 < x < y,
#w = # b = y
 = 2y, |w | = 2, 1 < y,
#w = # b = y
 = |w | = 2y, 1 < y,
#w = # b = y
 = 2y + 1, |w | = 2x + 1, 1 < x < y,
#w = y + 1, #b = y
 = 2y + 1, |w | = 3, 1 < y,
#w = y + 1, #b = y
 = |w | = 2y + 1, 1 < y,
#w = y + 1, #b = y
 = 2y, |w | = 2x + 1, 1  x < y,
#w = # b = y
otherwise
(29)

 = 2y + 1, |w | = 2x, 1  x  y,
#w = y + 1, #b = y
 = 2y, |w | = 2x, 1 < x < y,
#w = # b = y
 = 2y, |w | = 2, 1 < y,
#w = # b = y
 = |w | = 2y, 1 < y,
#w = # b = y
 = 2y + 1, |w | = 2x + 1, 1  x < y,
#w = y + 1, #b = y
 = |w | = 2y + 1, 1  y,
#w = y + 1, #b = y
 = 2y, |w | = 2x + 1, 1  x < y,
#w = # b = y
otherwise
(30)

fiTractable Cost-Optimal Planning

(R7, [S, #w , #b , ]] , w ) =

(
)
8
>
(y + 1, 0, x  1, 0),
>
>
min
,
>
>
>
(0, y + 1, 0, x  1)
>
>
>
>
>
>
>
>
>
>
>
>
>
(y, 0, x, 0),
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
(0, y, 0, x  1),
>
>
>
>
>
>
>
>
>
<
>
>
>
(y + 1, 0, x, 0),
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>(0, y + 1, 0, x  1),
>
>
>
>
>
>
>
>
>
>
(
)
>
>
>
(y, 0, x, 0),
>
>
>
min
,
>
>
(0, y, 0, x)
>
>
>
>
>
>
:
,

(R8, [S, #w , #b , ]] , w ) =

(
)
8
>
(x, 0, y, 0),
>
>
min
,
>
>
>
(0, x, 0, y)
>
>
>
>
>
>
>
>
>
>
>
>
(x, 0, y, 0),
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>(0, x  1, 0, y),
>
>
>
<
>
>
>
>
>
>
(x + 1, 0, y, 0),
>
>
>
>
>
>
>
>
>
>
>
>
>
(0, x, 0, y),
>
>
>
>
>
>
>
(
)
>
>
>
(x, 0, y, 0),
>
>min
>
,
>
>
(0, x, 0, y)
>
>
>
>
>
>
:
,

239

 = 2y + 1, |w | = 2x, 1 < x  y,
#w = y + 1, #b = x  1
C(awv |bw ) < C(awv |ww ),
 = 2y, |w | = 2x, 1  x < y,
#w = y, #b = x
C(awv |bw )  C(awv |ww ),
 = 2y, |w | = 2x, 1 < x  y,
#w = y, #b = x  1
C(awv |bw ) < C(awv |ww ),
 = 2y + 1, |w | = 2x + 1, 1  x < y,
#w = y + 1, #b = x
C(awv |bw )  C(awv |ww ),
 = 2y + 1, |w | = 2x + 1, 1 < x  y,
#w = y + 1, #b = x  1
 = 2y, |w | = 2x + 1, 1  x < y,
#w = y, #b = x
otherwise
(31)

 = 2y + 1, |w | = 2x, 1  x  y,
#w = x, #b = y
C(awv |bw ) < C(awv |ww ),
 = 2y, |w | = 2x, 1  x < y,
#w = x, #b = y
C(awv |bw )  C(awv |ww ),
 = 2y, |w | = 2x, 1 < x  y,
#w = x  1, #b = y
 = 2y + 1, |w | = 2x + 1, 1  x < y,
#w = x + 1, #b = y
 = 2y + 1, |w | = 2x + 1, 1  x  y,
#w = x, #b = y
 = 2y, |w | = 2x + 1, 1  x < y,
#w = x, #b = y
otherwise

(32)

fiKatz & Domshlak

8
>
C(abv |ww ),
|w | = 1, #w = 0, #b <  2 
>
>#b  (
)
>
<
#b  C(abv |ww ),
(R9, [S, #w , #b , ]] , w ) = min
,
|w | > 1, #w = 0, #b <  2 
>
#b  C(abv |bw )
>
>
>
:,
otherwise
8
>
#w  C(awv |bw ),
|w | = 1, #w <  2 , #b = 0
>
>
(
)
>
<
#w  C(awv |bw ),
(R10, [S, #w , #b , ]] , w ) = min
,
|w | > 1, #w <  2 , #b = 0
>
#
w  C(awv |ww )
>
>
>
:,
otherwise

(33)

(34)

Having specified the function , we now use it, in particular, for specifying the functional
component xwv 1 as in Eq. 35. This equation actually emulates movements in the state
machine for v as in Figure 12 to the terminal state S8.

xwv 1 ([[S, #w , #b , ]] , w1 ) =




(R1, [S, #w , #b , ]] , w1 ),







 (R2, [S, # , # , ]] ,  ),


w
w1
b


min



(R5, [S, #w , #b , ]] , w1 ),







 (R6, [S, # , # , ]] ,  )


w
w1
b








(R3, [S, #w , #b , ]] , w1 ),










(R4, [S, #w , #b , ]] , w1 ),









(R7, [S, #w , #b , ]] , w1 ),





(R8, [S, #w , #b , ]] , w1 ),










(R9, [S, #w , #b , ]] , w1 ),










(R10, [S, #w , #b , ]] , w1 ),















0,













,













, S = S1,

S = S2,
S = S3,
S = S4,
S = S5,

(35)

S = S6,
S = S7,
S = S8,
#w = 0,
#b = 0
otherwise

We now proceed with the rest of the functional components xwv 2 , . . . , xwv k . For each
w
w
2  j  k, each [S, #w , #b , ]]  Dom(xv j ), each [S , #w , #b ,  ]  Dom(xv j1 ), and each
w  Dom(xwj ) =  [wj ], the value of xwj is set according to Eq. 36. This equation also
v

240

fiTractable Cost-Optimal Planning

emulates movements in the state machine for v as in Figure 12each sub-case of Eq. 36
deals with a certain transition in that state machine.


xwj ( [S, #w , #b , ]] , S , #w , #b ,   , wj ) =
v
8
8
(R1, [S, #w  #w , #b  #b , ]] , wj ),
>
>
>
>
>
>
>
< (R2, [S, #  # , #  # , ]] ,  ),
>
>
w
wj
b
w
b
>
>
min
>


>
>
(R5,
[
S,
#

#
,
#

#
,
]
]
,

>
w
wj ),
b
>
w
b
>
>
>
:
>


>
(R6,
[
S,
#

#
,
#

#
,
]
]
,

w
wj )
b
>
w
b
>
>
>
>
>
>
>
>
(R4, [S, #w  #w , #b  #b , ]] , wj ),
>
>
>
>
>
>
>
>
>
>
>


>
>
>(R3, [S, #w  #w , #b  #b , ]] , wj ),
>
>
>
>
>
>
>
>
>
>
(R9, [S, #w  #w , #b  #b , ]] , wj ),
>
>
>
>
>
>
>
>
>
>
>(R10, [S, #  # , #  # , ]] ,  ),
>
wj
w
b
>
w
b
>
>
>
>
>
>
>
>
>
>
>
(R7, [S, #w  #w , #b  #b , ]] , wj ),
>
>
>
>
>
>
>
>
>
>
>(R8, [S, #w  # , #b  # , ]] , w ),
>
w
<
b
j
>
>
>
>
>
>
(R3, [S, #w  #w , #b  #b , ]] , wj ),
>
>
>
>
>
>
>
>
>
>
>
>
(R4, [S, #w  #w , #b  #b , ]] , wj ),
>
>
>
>
>
>
>
>
>
>
>
(R7, [S, #w  #w , #b  #b , ]] , wj ),
>
>
>
>
>
>
>
>
>
>
>
>
(R8, [S, #w  #w , #b  #b , ]] , wj ),
>
>
>
>
>
>
>
>
>
>
>


>
>(R9, [S, #w  #w , #b  #b , ]] , wj ),
>
>
>
>
>
>
>
>
>
>(R10, [S, #  # , #  # , ]] ,  ),
>
w
wj
>
b
w
b
>
>
>
>
>
>
>
>
>
>
>
0,
>
>
>
>
>
>
>
:
,

9
>
>
>
=
>
>
>
;

,

S = S1, S = S8,  =   ,
#w  #w , #b  #b
S = S1, S = S2,  =   ,
#w  #w , #b  #b
S = S1, S = S3,  =   ,
#w  #w , #b  #b
S = S1, S = S4,  =   ,
#w  #w , #b  #b
S = S1, S = S5,  =   ,
#w  #w , #b  #b
S = S1, S = S6,  =   ,
#w  #w , #b  #b
S = S1, S = S7,  =   ,
#w  #w , #b  #b

(36)

S = S2, S = S8,  =   ,
#w  #w , #b  #b
S = S3, S = S8,  =   ,
#w  #w , #b  #b
S = S4, S = S8,  =   ,
#w  #w , #b  #b
S = S5, S = S8,  =   ,
#w  #w , #b  #b
S = S6, S = S8,  =   ,
#w  #w , #b  #b
S = S7, S = S8,  =   ,
#w  #w , #b  #b
S = S ,  =   ,
#w = #w , #b = #b
otherwise

This finalizes the construction of COP , and this construction constitutes the first three
steps of the algorithm polytree-1-dep in Figure 13(a). The subsequent steps of this algorithm
are conceptually similar to these of the polytree-1-dep-uniform algorithm in Section 4. It is
241

fiKatz & Domshlak

procedure polytree-1-dep( = (V, A, I, G))
takes a problem   P(1)
returns a cost-optimal plan for  if  is solvable, and fails otherwise
create a set of variables X as in Eqs. 19-20
create a set of functions F = {x | x  X } with scopes as in Eq. 21
for each x  X do
specify x according to Eqs. 22-36
endfor
P
set COP := (X , F) with global objective min F (X )
x :=
P solve-tree-cop(COP  )
if F (x) =  then return failure
P
extract plan  from x with C() = F (x)
return 
Figure 13: Algorithm for cost-optimal planning for P(1) problems.
not hard to verify from Eqs. 19-21, and the fact that the causal graph of   P(1) forms a
polytree that
(i) for each variable x  X , |Dom(x)| = poly(n),
(ii) the tree-width of the cost network of F is  3, and
(iii) the optimal tree-decomposition of the COP s cost network is given by any topological
ordering of the causal graph that is consistent with the (arbitrary yet fixed at the time
of the COP s construction) orderings of each planning variables parents in the causal
graph.
Theorem 9 Let  be a P(1) problem, COP = (X , F) be the P
corresponding constraint
optimization problem, and x be an optimal assignment to X with F (x) = .
(I) If  < , then a plan of cost  for  can be reconstructed from x in time polynomial
in the description size of .

(II) If  has a plan, then  < .
Proof Sketch: The proof of Theorem 9 is in Appendix A, pp. 278-286; here we provide
only aPsketch of the proofs skeleton. To prove (I), given a COP solution x = {v1 , . . . , vn }
with F (x) =  < , we construct a plan  for  with C() = . This is done by
constructing action sequences v for each v  V , as well as constructing partial orders over
the elements of these sequences of each variable and its parents. This construction distinguishes between numerous possibilities of the (joint) role that can be taken by the parents
of the variable in question. The constructed orders over the local actions sequences are
then combined and linearized
1) into an P
action sequence  that is a valid
P (using Theorem
P
=
plan for  with C() =
C(
)
=

(x)
v
v
F (x) = . To prove (II),
vV
vV
given solvable problem P
and some irreducible post-3/2 plan P
for , we construct a COP
assignment x such that F (x ) = C(). Then, from   F (x ) and C() < ,
we obtain the claimed  < .

242

fiTractable Cost-Optimal Planning

Theorem 10 Cost-optimal planning for P(1) is tractable.
Proof: Given a planning problem   P(1), we show that the corresponding constraint
optimization problem COP can be constructed and solved in time polynomial in the description size of . Let n be the number of state variables in . In polytree-1-dep, we first
construct the constraint optimization problem COP over (n2 ) variables X with domain
sizes being bounded either by O(n) or by O(n3 ) (for COP variables representing state variables and causal graph edges, respectively). The number of functional components in COP
is (n2 ), each defined over one variable with domain size of O(n) and either one or two
variables with domain sizes of O(n3 ). The construction is linear in the size of the resulting
COP, and thus is accomplished in time O(n9 ).
Applying then to COP a tree-decomposition that clusters the scopes of the functional
components F, we arrive into an equivalent, tree-structured constraint optimization problem
over (n2 ) variables with domains of size O(n7 ). Such a tree-structured COP can be solved
in time O(xy 2 ) where x is the number of variables and y is an upper bound on the size of
a variables domain (Dechter, 2003). Therefore, solving COP can be done in time O(n16 ).
As this dominates both the time complexity of constructing COP , and the time complexity
of extracting a plan from the optimal solution to COP (see the proof of (I) in Theorem 9),
the overall complexity of the algorithm polytree-1-dep is O(n16 ), and therefore polynomial
in the description size of .


6. Drawing the Limits of k-Dependence
Having read this far, the reader may wonder whether 1-dependence is not a strong enough
property to make the cost-planning tractable even for some more complex than polytree
forms of the causal graph. In this last technical section we discuss the limits of the power
of k-dependence (and, in particular, of 1-dependence), and present some negative results
that draw a boundary between the tractable and intractable k-dependent UB problems.
 In Theorem 11 we show that cost-optimal planning is already hard for the Sbb (1)
problem class, that is, the class of 1-dependent UB problems inducing directed-path
singly connected causal graphs with both in- and out-degrees being bounded by a
constant. This result further stresses the connection between the undirected cycles in
the causal graph and the complexity of various planning tasks, which has been first
discussed by Brafman and Domshlak (2003).
 In Theorem 12 we show that even non-optimal planning is hard for the Sbb (2) problem
class. This results suggests that 1-dependence is a rather special case of k-dependence
in terms of the connection to computational tractability. However, given the (still)
empty entries in Figures 5a and 5b, further analysis of the criticality of 1-dependence
is needed.
Theorem 11 Cost-optimal planning for Sbb (1) is NP-complete.
Proof: The membership in NP is implied by Theorem 2 by Brafman and Domshlak (2003).
The proof of NP-hardness is by a polynomial reduction from the well-known Vertex Cover
problem (Garey & Johnson, 1978). The problem of Vertex Cover is: given an undirected
243

fiKatz & Domshlak

graph G = (V, E), find a minimal-size subset V of V such that each edge in E has at least
one of its two end-nodes in V . Given an undirected graph G = (V, E), let the planning
problem G = hVG , AG , IG , GG i be defined as follows.
 VG = {v1 , . . . , v|V| , u1 , . . . , u|E| }, and, for all vi , uj , Dom(vi ) = Dom(uj ) = {T, F },
S
 IG = {vi = F | vi  VG } {ui = F | ui  VG },
 GG = {ui = T | ui  VG },

 Actions AG = AV  AE , where AV = {av1 , . . . , av|V| } with
pre(avi ) = {vi = F }, eff(avi ) = {vi = T }, C(avi ) = 1

(37)

and AE = {au1 , au1 , . . . , au|E| , au|E| } with
pre(aui ) = {ui = F, vi1 = T },
pre(aui ) = {ui = F, vi2 = T },
eff(aui ) = eff(aui ) = {ui = T },

(38)

C(aui ) = C(aui ) = 0
where the variables vi1 , vi2 correspond to the endpoints of the edge corresponding to
the variable ui .
Given this construction of G , it is easy to see that (i) any plan  for G provides
us with a vertex cover V for G such that |V | = C() and vice versa, and thus (ii) costoptimal plans for G (and only such plans for G ) provide us with minimal vertex covers
for G. The topology of the causal graph of G is as required, and 1-dependence of G
is immediate from Eqs. 37-38. This finalizes the proof of NP-completeness of cost-optimal

planning for Sbb (1).
Theorem 12 Planning for Sbb (2) is NP-complete.
Proof: The proof is basically given by the construction in the proof by Brafman and
Domshlak (2003) for their Theorem 2. The polynomial reduction there is from 3-SAT to
planning for S. Observing that 3-SAT remains hard even if no variable participates in
more than five clauses of the formula (Garey & Johnson, 1978), and that the reduction
of Brafman and Domshlak from such 3-SAT formulas is effectively to planning for Sbb (2),
accomplishes the proof of our claim.


7. Conclusion and Future Work
One of the key conclusions by Bylander (1994) in his seminal article on planning complexity
was that . . . the analysis strongly suggests that there is no such thing as a set of generallyapplicable domain independent properties that lead to efficient planning. The later works
by, e.g., Backstrom and Nebel (1995), Jonsson and Backstrom (1998a), and Brafman and
Domshlak (2003, 2006) have shown that this conclusion was too pessimistic. By considering
not only local restrictions on actions, but also global restrictions on the action sets, as well
244

fiTractable Cost-Optimal Planning

as some structural properties of the problems, these works managed to identify numerous
domain-independent tractable fragments of classical planning. Having said that, the palette
of known tractable fragments of planning remains limited, and even less is known about
tractable optimal planning. While there is no difference in theoretical complexity of regular
and optimal planning in the general case (Bylander, 1994), many of the classical planning
domains are provably easy to solve, but hard to solve optimally (Helmert, 2003).
In this work we have studied the complexity of cost-optimal classical planning over
propositional state variables and unary-effect actions. We discovered novel problem fragments for which such optimization is tractable, and identified certain conditions that differentiate between tractable and intractable problems. The results are based on exploiting certain structural and syntactic characteristics of planning problems. Almost all our
tractability results are based on a proof technique that connects between certain tools from
planning and tractable constraint optimization, and we believe that this technique is in
itself interesting due to a clear evidence for its robustnessour different algorithms exploit
this proof technique, but in very much different manners.
Our results suggest that discovering new islands of tractability of optimal planning is
not hopeless, and we strongly believe that this is indeed the case. In particular, our ongoing
work is devoted both to the questions that have been left open in this paper (see Figure 5), as
well as to planning problems with simple causal graphs but with multi-valued (in contrast to
propositional) state variables. In fact, recently we have reported some preliminary positive
results in the latter direction (Katz & Domshlak, 2007). Interestingly, these recent results
have been presented in a context of a potential customer for such tractability results,
namely, in the context of homomorphism abstractions for admissible heuristics for general
planning as heuristic search.
Acknowledgments
This research is supported in part by the Israel Science Foundations grants 2008100 and
2009589, as well as by the C. Wellner Research Fund. We thank Adele Howe and the anonymous reviewers whose attentive comments and helpful suggestions have greatly improved
this paper.

Appendix A. Proofs
Theorem 1 Let G be a polytree over vertices V = {1, . . . , n}, and pred(i)  V denote the
immediate predecessors of i in G. For each i  V , let Oi be a finite set of objects associated
with the vertex i, with the sets O1 , . . . , On being pairwise disjoint. For each i  V , let >i
be a strict partial order over Oi , and, for each j  pred(i), let >i,j be a strict partial order
over Oi  Oj .
If, for each i  V, j  pred(i), the transitively closed >i  >i,j and >j  >i,j induce
(strict) partial orders over Oi  Oj , then so does the transitively closed

> =

[

iV



 >i 
245

[

jpred(i)



>i,j 

fiKatz & Domshlak

over O =

S

iV

Oi .

Proof: In what follows, by oi we denote an arbitrary object from Oi . Assume to the
contrary that all >i  >i,j and >j  >i,j are (strict) partial orders, and yet > is not so.
That is, there exists a pair of objects oi , oj  O for which hold both oi > oj and oj > oi . By
the construction of >, we have that there is a, possibly empty, path between the vertices i
and j in the undirected graph induced by G. Since G is a polytree, we know that such an
undirected path
i = i0  i1  . . .  im1  im = j
(39)
between i and j is unique. Thus, we must have a cycle  in > such that
 : oi = o1i0 < . . . < oxi00 < o1i1 < . . . < oxi11 < . . . . . . < o1im < . . . < oximm = oj
 : oi = o1i0 > . . . > oyi00 > o1i1 > . . . > oyi11 > . . . . . . > o1im > . . . > oyimm = oj

(40)

where, for 0  k  m, both xk  1 and yk  1, and each step in both chains  and  is
directly implied by some local relation >l or >l,l constructing >.
Without loss of generality, we assume that the cycle in > induced by  and  is lengthwise minimal among all such cycles in >. In particular, this implies that
(i) for 0  k  m, we have 1  xk , yk  2 (one object from Oik is required to connect the
local relations >ik1 and >ik+1 , and no more than two elements from Oik are required
because >ik is transitively closed),
(ii) for each pair of objects o  , o  , we have o 6= o , unless o = o = oi or o = o = oj ,
(or otherwise there would be a shorter cycle than ) and
(iii) for each pair of objects o  , o  , no >l (and no >l,l ) implies o >l o (respectively,
o >l,l o), or otherwise, again, there would be a shorter cycle than .
First, let us show that at least one of the chains  and  contains at least one internal
element. Assume, to the contrary, that both  and  contain no internal elements. If i = j,
then we have oi >i oi (where oi = oj ) and oi >i oi , contradicting our assumption that >i is
a partial order. (If >i is not a partial order, then neither all >i  >i,j .) Otherwise, if i 6= j,
then either i  pred(j) or j  pred(i). Assuming the latter, (oi > oj )  (oi > oj ) implies
(oi >i,j oj )  (oi >i,j oj ), contradicting our assumption that >i,j is a partial order.
Given that, let us now prove that oximm 6= oyimm , contradicting the assumption that the
chains  and  as in Eq. 40 exist. We do it on a case-by-case basis of possible combinations
of xm , ym , and length-minimality of the cycle  implies that there are only four such cases
to consider.
[xm = 2, ym = 2 ] In this case, Eq. 40 implies o1im >im o2im = o2im >im o1im . The transitivity
of >im then implies o1im > o1im , contradicting our assumption of minimality of the
cycle .
[xm = 1, ym = 1 ] From Eq. 39 we have either im1  pred(im ) or im  pred(im1 ). If
xm1
ym1
. The
>im ,im1 o1im = o1im >im ,im1 oim1
im1  pred(im ), then Eq. 40 implies oim1
ym1
xm1
transitivity of >im ,im1 then implies oim1 > oim1 , contradicting our assumption
246

fiTractable Cost-Optimal Planning

of minimality of the cycle . Otherwise, if im  pred(im1 ), then Eq. 40 implies
ym1
xm1
oim1
>im1 ,im o1im = o1im >im1 ,im oim1
. Again, the transitivity of >im1 ,im then
ym1
xm1
implies oim1 > oim1 , contradicting our assumption of minimality of the cycle .
[xm = 2, ym = 1 ] In this case as well, Eq. 39 implies that we have either im1  pred(im )
ym1
or im  pred(im1 ). If im1  pred(im ), then Eq. 40 implies oim1
>im ,im1 o1im =
ym1
> o1im , contrao2im >im o1im . Then, the transitivity of >im  >im ,im1 implies oim1
dicting our assumption of minimality of the cycle . Otherwise, if im  pred(im1 ),
ym1
then Eq. 40 implies oim1
>im1 ,im o1im = o2im >im o1im . Then, the transitivity of
ym1
>im  >im1 ,im implies oim1 > o1im , contradicting our assumption of minimality of
the cycle .
[xm = 1, ym = 2 ] This case is similar to the previous case of xm = 2, ym = 1, mutatis
mutandis.

Theorem 2 Let  be a planning problem in Pb , COP = (X , F) be the corresponding
constraint optimization problem, and x  Dom(X ) be an optimal solution for COP with
P
F (x) = .
(I) If  < , then a plan of cost  for  can be reconstructed from x in time polynomial
in the description size of .

(II) If  has a plan, then  < .

Proof:
P
(I) Given a COP solution x = {v1 , . . . , vn } with F (x) =  < , we construct a
plan  for  with C() = .
First, for each variable v  V with pred(v) = , let a sequence v of actions from Av be
defined as
(

|v | = 1
v =
,
(41)
|v |1
1
av  . . .  av
otherwise
where, for 1  j  |v |  1,

(
abv , j is even
,
ajv =
awv , j is odd

(42)

with eff(abv ) = {bv }, and eff(awv ) = {wv }. From Eq. 3 and v (v ) < , we immediately
have (i) {awv }  Av if |v |  2, and {abv , awv }  Av if |v | > 2, and (ii) C(v ) = v (v ).
Now, for a purpose that gets clear below, let a binary relation >v over the action elements
of v be defined as the transitive closure of {avj1 < ajv | 1 < j  |v |  1}. Clearly, >v
constitutes a strict total ordering over the elements of v .
Next, for each non-root variable v  V with pred(v) = {w1 , . . . , wk }, we construct the
graph Ge (v) with respect to w1 , . . . , wk , and determine in Ge (v) a minimal-cost path of
247

fiKatz & Domshlak

|v |  1 edges from the source node hb1w1    b1wk i. The existence of such a path is implied by
v (v , w1 , . . . , wk ) < . By the construction of Ge (v) we also know that, for 1  j  |v |
1, the j-th edge on this path is from a node labeled with hw1 [l1j1 ]   wk [lkj1 ]i to a node
labeled with hw1 [l1j ]   wk [lkj ]i, where for 1  l  k, we have li0 = 1 and lij1  lij . Having
that, let a sequence v of actions from Av be defined as in Eq. 41, with, for 1  j  |v |  1,
eff(ajv ) = {v [j + 1]}
n
o
pre(ajv ) = v [j], w1 [l1j ], w2 [l2j ], . . . , wk [lkj ]

(43)

| |1

Note that {a1v , . . . , av v }  Av is implied by the construction of Ge (v) and the presence
of the considered minimal-cost path in it.
Now, similarly to the case of the root variables, let a binary relation >v over the action
elements of v be defined as the transitive closure of {avj1 < ajv | 1 < j  |v |  1}. Here
as well, >v constitutes a strict total ordering over the elements of v . In addition, for each
parent wi of v, let a binary relation >v,wi over the union of the action elements of v and
+
wi be defined as the transitive closure of >
v,wi  >v,wi , which in turn are defined as
 j

li 1
j

j
awi < av | 1  j  |v |  1, li > 1
>v,wi =


(44)
lij
j
+
j
>v,wi = av < awi | 1  j  |v |  1, li < |wi | .
It is not hard to verify from Eq. 44 that, for each v  V and each w  pred(v), not only
>v,w constitutes a strict partial ordering, but so are the transitively closed >v  >v,w and
>w  >v,w . Given that,
 By the definition of w = ha1w  . . .  alw i, and the polytree structure of the causal
graph CG(), restricting the preconditions and effects of each aiw to the variables
{v}  pred(v), we have pre(aiw ) = {bw }, eff(aiw ) = {ww } for i being odd, and pre(aiw ) =
{ww }, eff(aiw ) = {bw } for i being even. For each 1  i  k, from Eq. 43 we have
lj 1

j
i
eff(a
S wi )  pre(av ). From Eq. 44 we can now derive that any linearization of >v
 wpred(v) >v,w defines a sequence of actions that is applicable with respect to {v} 
pred(v). In addition, the construction of the graph Ge (v) implies that this action
sequence provides to v the value G[v] if the latter is specified.

 The polytree structure of the causal graph CG() and Theorem 1 together imply that
the transitively closed relation
[
[
>=
(>v 
>v,w )
vV

wpred(v)

is a strict partial order over the union of the action elements of v1 , . . . , vn .
Putting thing together, the above implies that any linearization of > constitutes a valid
plan  for  with cost
X
X
C() =
C(v ) =
v (x),
vV

vV

248

fiTractable Cost-Optimal Planning

which is exactly what we had to prove. We also note that the plan extraction step of the
algorithm polytree-k-indegree corresponds exactly to the above construction along Eqs. 4144, providing us in polynomial time with a concrete cost-optimal plan corresponding to the
optimal solution for COP .
(II) We now prove that if  is solvable, then we must have  < . Assume to the
contrary that this is not the case. Let  be a solvable planning problem, and let  be
an irreducible plan for . Given such , let x = {v1 , . . . , vn } be an COP assignment
with each |vi | = |vi |  1. Note that x is well-defined (that is, for 1  i  n, we
have vi   [ (vi )])
P by the definition of  (vi ), Corollary 1, and  being irreducible. Let
us now
show
that
F (x )  C(), contradicting our assumption that  =  due to
P
  F (x ) and C() < .
First, for each variable v with pred(v) = , Eq. 3 immediately implies v (x )  C(v ).
Next, for each non-root variable v  V with pred(v) = {w1 , . . . , wk }, consider the graph
Ge (v) constructed with respect to w1 , . . . , wk . Let {a1 , . . . , a|v | } be the actions of v
numbered in the order of their appearance along v . Let {yw1 (1), . . . , ywk (1)} denote the
prevail condition of a1 with each ywi (1) being time-stamped with its earliest appearance
1 }. Now, for 2  j  | |, we set {y (j), . . . , y (j)}
along wi , that is, ywi (1)  {b1wi , ww
v
w1
wk
i
to the prevail condition of ai with each ywi (j) being time-stamped with the lowest possible
time index along wi satisfying ywi (j  1) does not come after ywi (j) along wi . Given
that
(i) v is a complete order-preserving restriction of  to the v-changing actions Av ,
| |

(ii) the sequence of time-stamped prevail conditions {{yw1 (j), . . . , ywk (j)}}j=1v is constructed as above, and
(iii) |v | = |v |  1 by the construction of x ,
we have that Ge (v) contains a path
hb1w1    b1wk i  hyw1 (1)   ywk (1)i  . . .  hyw1 (|v |)  ywk (|v |)i
and the cost of this path is C(v ) < . However, from the constructive definition of v in
the algorithm polytree-k-indegree, we have v (x ) being the cost of the minimal-cost path
of |v |  1 edges in Ge (v) originated in hb1w1    b1wk i, and thus v (x )  C(v ). The latter
argument is valid for all planning variables v  V , and thus we have
X
X
C(v ) = C(),
(x ) 
F

vV

which is what we had to prove.



Theorem 6 Let  be a P(1) problem with uniform-costs actions, COP = (X , F) be the
corresponding
constraint optimization problem, and x be an optimal assignment to X with
P
(x)
=
.
F
249

fiKatz & Domshlak

(I) If  < , then a plan of cost  for  can be reconstructed from x in time polynomial
in the description size of .
(II) If  has a plan, then  < .

Proof:
P
(I) Given a COP solution x with F (x) =  < , we construct a plan  for  with
C() = . We construct this plan by
1. Traversing the planning variables in a topological ordering of the causal graph CG(),
and associating each variable v with a sequence v  Av .
2. Merging the constructed sequences v1 , . . . , vn into the desired plan .
For each variable xv  X , let v denote the value provided by x to xv . First, for each
variable v  V with pred(v) = , let a sequence v of actions from Av be defined as
(

|v | = 1
v =
,
(45)
|v |1
1
otherwise
av  . . .  av
where, for 1  j  |v |  1,

(
abv , j is even
,
ajv =
awv , j is odd

(46)

with eff(abv ) = {bv }, and eff(awv ) = {wv }. From Eq. 14 and v (v )   < , we
immediately have (i) {awv }  Av if |v |  2, and {abv , awv }  Av if |v | > 2, and (ii)
C(v ) = v (v ). Let a binary relation >v over the action elements of v be defined as the
transitive closure of {avj1 < ajv | 1 < j  |v |  1}, that is


>v = {ajv < ajv | 1  j  < j  |v |  1}

(47)

Clearly, >v constitutes a strict total ordering over the elements of v , making v an applicable sequence of actions that provides to v the value G[v] if the latter is specified.
Next, for each variable v  V with pred(v) 6= , let pred(v) = {w1 , . . . , wk } be numbered
according to their ordering used for constructing COP . Likewise, for each wi  pred(v),
i
let [w (i), b (i), (i)]] be the value provided by x to xw
v . Given that, let a pair of indexes
0  hwi, hbi  k be defined as


0, w (k) = 0,
(48)
hwi = 1, w (1) = 1,


j, w (j  1) < w (j), 2  j  k


0, b (k) = 0,
hbi = 1, b (1) = 1,


j, b (j  1) < b (j), 2  j  k
250

(49)

fiTractable Cost-Optimal Planning

In other words, hwi captures the smallest 1  j  k such that w (j) = 1, and 0, if there is
no such j at all; the semantics of hbi is similar, mutatis mutandis.
Informally, in our next-coming construction of the action sequence v for the state
variable v, hwi and hbi will indicate the parents prevailing the value changes of v to wv and
to bv , respectively, along v . Note that Eqs. 48-49 are well-defined because, for 2  j  k,
Eq. 18 implies
w (j  1)  w (j)  b (j  1)  b (j)  (j  1) = (j).
Given this notation, the action sequence v and the partial orders >v,w1 , . . . , >v,wk are
constructed as follows.
[ hwi = 0, hbi = 0 ] In this case, the constructed plan  should perform no value changes of
v, and thus v is set to an empty action sequence, and, consequently, both >v and all
>v,w are set to empty sets.
[ hwi > 0, hbi = 0 ] In this case, the constructed plan  should perform exactly one value
change of v (from bv to wv ), and thus v is set to contain exactly one action a1v with
eff(a) = {wv }, and
(
{bv , bwhwi }, awv |bw  Av
hwi
pre(a1v ) =
(50)
{bv , wwhwi }, otherwise
Note that a1v is well-defined, as  <  and Eq. 16 together imply that {awv |bw , awv |bw }
hwi
hwi
Av 6=  (see case (2) in Eq. 16). In both outcomes of Eq. 50, we set >v = . If
a1v = awv |bw , we set
hwi

>v,whwi = {a1v < a1whwi | a1whwi  whwi }

(51)

Otherwise, if a1v = awv |ww , then from case (2) in Eq. 16, awv |bw 6 Av , and  < ,
hwi
hwi
we have |whwi | > 1, and thus |whwi |  1. Given that, we set
>v,whwi = {a1whwi < a1v }  {a1v < a2whwi | a2whwi  whwi }

(52)

In both cases, it is easy to verify that >v  >v,whwi  >whwi constitutes a strict total
order over the action elements of v and whwi . (In particular, this trivially implies
that >v  >v,w and >v,w  >w are strict partial orderings over their domains.)
S
From Eqs. 47, 51, and 52 we can now derive that any linearization of >v  wpred(v) >v,w
defines a sequence of actions that is applicable with respect to {v}  pred(v). In addition, Eq. 12 implies that this action sequence provides to v the value G[v] if the latter
is specified.
[ hwi > 0, hbi > 0, hwi = hbi ] In this case, the constructed plan  should perform more
than one value change of v, and all these value changes should be performed by (a
pair of types of) actions prevailed by the value of whwi . From  < , we have
([[w (hwi), b (hwi), ]] , whwi ) = 0. The specification of the case in question (that is,
hwi = hbi > 0) thus implies that one of the conditions of the cases (4-6) of Eq. 16
should hold. Given that, we distinguish between the following four settings.
251

fiKatz & Domshlak

(1) If {awv |bw , abv |bw }  Av , then v is specified according to Eq. 45, and its
hwi
hwi
action elements are specified as

aw |b
, i is odd
v whwi
aiv =
.
(53)
abv |bw , i is even
hwi

The relation >v is set according to Eq. 47, and >v,whwi is set to
>v,whwi = {aiv < a1whwi | aiv  v , a1whwi  whwi }

(54)

Finally, for all w  pred(v) \ {whwi }, we set >v,w = .
(2) Otherwise, if {awv |ww , abv |ww }  Av and |whwi | > 1, then we have |whwi |  1.
hwi
hwi
Given that, we again set v according to Eq. 45, but now with its action elements
being set as

aw |w
, i is odd
v whwi
aiv =
.
(55)
abv |ww , i is even
hwi

The relation >v is set according to Eq. 47, and >v,whwi is set to

>v,whwi = {a1whwi < aiv | aiv  v }  {aiv < a2whwi | aiv  v , a2whwi  whwi }

(56)

Finally, for all w  pred(v) \ {whwi }, we set >v,w = .
(3) Otherwise, if {awv |bw , abv |ww }  Av , and |whwi |  |v |1, then v is specified
hwi
hwi
according to Eq. 45, and its action elements are specified as

aw |b
, i is odd
v whwi
i
av =
.
(57)
abv |ww , i is even
hwi

The relation >v is set according to Eq. 47, and >v,whwi is set to
[
{aiv < ajwhwi | i  j}  {ajwhwi < aiv | i > j}
>v,whwi =

(58)

aiv v ,ajwhwi whwi

For all w  pred(v) \ {whwi }, we set >v,w = .
(4) Otherwise, if {awv |ww1 , abv |bw1 }  Av , and |whwi |  |v |, then v is specified
according to Eq. 45, and its action elements are specified as

aw |w
, i is odd
v whwi
aiv =
.
(59)
abv |bw , i is even
hwi

The relation >v is set according to Eq. 47, and >v,whwi is set to
[
{aiv < ajwhwi | i < j}  {ajwhwi < aiv | i  j}
>v,whwi =
aiv v ,ajwhwi whwi

For all w  pred(v) \ {whwi }, we set >v,w = .
252

(60)

fiTractable Cost-Optimal Planning

In all the four cases above, >v  >v,whwi  >whwi constitutes a strict total order over
the elements of v and whwi .
From
S Eqs. 47, 54, 56, and 58, 60 we can now derive that any linearization of >v
 wpred(v) >v,w defines a sequence of actions that is applicable with respect to {v} 
pred(v). In addition, Eq. 12 implies that this action sequence provides to v the value
G[v] if the latter is specified.
[ hwi > 0, hbi > 0, hwi =
6 hbi ] In this case, the constructed plan  should perform more than
one value change of v, with changes of v to wv and bv being performed by (a pair of
types of) actions prevailed by the value of whwi and whbi , respectively. From  < ,
we have ([[w (hwi), b (hwi), ]] , whwi ) = ([[w (hbi), b (hbi), ]] , whbi ) = 0, and this is
due to the respective satisfaction of the conditions of cases (2) and (3) in Eq. 16.
Given that, we distinguish between the following four settings5 .
(1) If {awv |bw , abv |bw }  Av , then v is specified according to Eq. 45, and its
hwi
hbi
action elements are specified as

aw |b
, i is odd
v whwi
aiv =
.
(61)
abv |bw , i is even
hbi

The relation >v over the action elements of v is set according to Eq. 47, the
relation >v,whwi over the action elements of v and whwi is set to
>v,whwi = {aiv < a1whwi | i is odd, aiv  v , a1whwi  whwi }

(62)

and the relation >v,whbi over the action elements of v and whbi is set to
>v,whbi = {aiv < a1whbi | i is even, aiv  v , a1whbi  whbi }

(63)

For all w  pred(v) \ {whwi , whbi }, we set >v,w = .
(2) Otherwise, if {awv |ww , abv |bw }  Av and |whwi | > 1, then we have |whwi |  1.
hwi
hbi
Given that, we again set v according to Eq. 45, but now with its action elements
being set as

aw |w
, i is odd
v whwi
aiv =
.
(64)
abv |bw , i is even
hbi

The relation >v is set according to Eq. 47, >v,whwi is set to
[
>v,whwi =
{a1whwi < aiv }  {aiv < a2whwi | a2whwi  whwi }

(65)

aiv v , i is odd

and >v,whbi is set to
>v,whbi = {aiv < a1whbi | i is even, aiv  v , a1whbi  whbi }

(66)

For all w  pred(v) \ {whwi , whbi }, we set >v,w = .
5. While the details are slightly different, the four settings in this case are conceptually similar to these in
the previously considered case of hwi > 0, hbi > 0, hwi = hbi.

253

fiKatz & Domshlak

(3) Otherwise, if {abv |ww , awv |bw }  Av , and |whbi | > 1, then we have |whbi |  1.
hbi
hwi
Given that, we v is specified according to Eq. 45, and its action elements are
specified as

aw |b
, i is odd
v whwi
aiv =
.
(67)
abv |ww , i is even
hbi

The relation >v is set according to Eq. 47, >v,whwi is set to

>v,whwi = {aiv < a1whwi | i is odd, aiv  v , a1whwi  whwi }

(68)

and >v,whbi is set to
[

>v,whbi =

aiv v ,

{a1whbi < aiv }  {aiv < a2whbi | a2whbi  whbi }

(69)

i is even

For all w  pred(v) \ {whwi }, we set >v,w = .
(4) Otherwise, if {awv |ww , abv |ww }  Av , |whwi | > 1, and |whbi | > 1, then we
hwi
hbi
have both |whwi |  1 and |whbi |  1. Given that, we again set v according to
Eq. 45, and its action elements are specified as

aw |w
, i is odd
v whwi
aiv =
.
(70)
abv |ww , i is even
hbi

The relation >v is set according to Eq. 47, >v,whwi is set to
>v,whwi =

[

{a1whwi < aiv }  {aiv < a2whwi | a2whwi  whwi }

(71)

[

{a1whbi < aiv }  {aiv < a2whbi | a2whbi  whbi }

(72)

aiv v ,

i is odd

and >v,whbi is set to
>v,whbi =

aiv v , i is even

For all w  pred(v) \ {whwi }, we set >v,w = .
In all the four cases above, both >v  >v,whwi  >whwi and >v  >v,whbi  >whbi
constitute strict total orders over their respective domains.
From Eqs.
S 47, 62, 63, 65, 66, 68, 69, 71, and 72 we can now derive that any linearization
of >v  wpred(v) >v,w defines a sequence of actions that is applicable with respect
to {v}  pred(v). In addition, Eq. 12 implies that this action sequence provides to v
the value G[v] if the latter is specified.
Until now, for each variable v  V , we have specified an action sequence v and the
order >v over the elements of v . For each w  pred(v), we have specified the order >v,w ,
and proved that all >v  >v,w and >w  >v,w form strict partial orders over their domains,
254

fiTractable Cost-Optimal Planning

S
and any linearization of >v  wpred(v) >v,w defines a sequence of actions that is applicable
with respect to {v}  pred(v) and provides to v the value G[v] if the latter is specified. This
construction allows us to apply now Theorem 1 on the (considered as sets) sequences v
and orders >v and >v,w , proving that
[
[
>=
(>v 
>v,w )
vV

wpred(v)

forms a strict partial order over the union of v1 , . . . , vn .
Here we also note that the plan extraction step of the algorithm polytree-1-dep-uniform
corresponds exactly to the above construction along Eqs. 45-72, providing us in polynomial
time with a concrete cost-optimal plan corresponding to the optimal solution for COP .
(II) We now prove that if  is solvable, then we must have  < . Assume to the contrary
that this is not the case. Let  be a solvable P(1) problem, and let (using Theorem 5)  be
an irreducible, post-unique plan for . Given such , let a COP assignment x be defined
as follows.
1. For each COP variable xv , the assignment x provides the value v   [(v)] such
that |v | = |v | + 1.

 w w
i
2. For each COP variable xw
v , the assignment x provides the value wv , bv , |v |  1 ,
where wwv = 1 just if some action in v changes the value of v to wv while (considering
the pre-fixed ordering of the vs parents) being preconditioned by the value of some
wj , j  i, and wwv = 0, otherwise. bwv is defined similarly to wwv , mutatis mutandis.
From Eq. 14-18 we then directly have that, for all v  V , xv (x ) = |v |, and for all
w  pred(v), xwv (x ) = 0. Therefore, we have
X
X
C(v ) = C(),
(x ) =
F

vV

which is what we had to prove.



Theorem 8 For every solvable P(1) problem  = (V, A, I, G), the plan set P 3/2 () contains at least one cost-optimal plan.
Proof: Given a P(1) problem  = (V, A, I, G), and cost-optimal plan  for , we construct
a sequence of actions  such that:
  is a post-3/2 plan for ,
 C( ) = C().
In nutshell, first, for each v  V , we map the subsequence v = ha1 , . . . , ak i of 
into a sequence of actions v = ha1 , . . . , ak i that (i) satisfy the post-3/2 property, and (ii)
C(v )  C(v ). Then, we merge the constructed sequences {v }vV into  , and show
that  is a valid plan for . The two properties of  as required above will then hold
255

fiKatz & Domshlak

immediately because C( ) = C(), and  being post-3/2 is implied by all its per-variable
components v being post-3/2.
For each variable v  V , with pred(v) = , we set v = v and
>v = {ai < aj | ai , aj  v , i < j}.

(73)

It is immediate from Eq. 73 that >v is a strict total order over the elements of v .
In turn, for each variable v  V with pred(v) 6= , given {w }wpred(v) , such that
|w | = |w | + 1, let ai be the ith cheapest action that changes variable v to   {bv , wv }
and prevailed by some value from {w }wpred(v) (that is, applicable given the sequences
of values {w }wpred(v) respectively obtained by the parents of v). Let us now focus on
S
6
b
b
w
aw
1 = awv | , a2 = awv | , a1 = abv | , a2 = abv | (that is, {, , , } 
wpred(v) {bw , ww }).
(I) If  =   {bw , ww }, we set

ai =

(

i = 2j  1, j  N
otherwise

awv |
abv |

(74)

In addition, we construct the following sets of ordering constraints. First, we set a
binary relation >v over the action elements of v = ha1 , . . . , ak i to
>v = {ai < aj | ai , aj  v , i < j}.

(75)

It is immediate from Eq. 75 that >v is a strict total order over the elements of v .
Likewise, if w = haj1 , . . . , ajl i, we set
S


 = bw

Sai v {ai < aj1 },

(76)
>v,w =
 = ww , l = 1
ai v {ai > aj1 },

S

   {a > aj }  {a < aj },  = ww , l > 1
ai v

i

1

i

2

Finally, the ordering constraints >v,w for the rest of the parents w  pred(v) \ {w}
are set to empty sets.

For each w  pred(v), it is easy to verify that the relation >v,w defined by Eq. 76 is a
strict total order over its domain. Also, from Eqs. 75 and 76, we have that, for each
w  pred(v), >v  >v,w is a strict total order over the union of the elements of v
and w .
S
From Eqs. 75-76 we can now derive that any linearization of >v  wpred(v) >v,w
defines a sequence of actions that is applicable with respect to {v}  pred(v). In
addition, |v | = |v | together with Eq. 74 implies that this action sequence provides
to v the value G[v] if the latter is specified.
6. It is possible that some of these actions do not exist, and our case by case analysis in the proof transparently takes this possibility into account. Specifically, if aw1 does not exist, then variable v simply
unchangeable, meaning v = . Next, if ab1 does not exist, then v can be changed at most once (from b
to w), and this is covered by a subcase of (I). If aw2 does not exist, and ab2 does exist, then only sub-cases
(a) of the cases {(III), (IV )}.{1, 3, 5, 7} are possible. Similarly, if ab2 does not exist, and aw2 does exist,
then only sub-cases (b) of the cases {(III), (IV )}.{1, 3, 5, 7} are possible. Finally, if both aw2 and ab2 do
not exist, then cases {(III), (IV )}.{1, 3, 5, 7} are not possible at all.

256

fiTractable Cost-Optimal Planning

(II) If   {bw , ww } and   {bu , wu }, such that w 6= u, we set
(
awv | i = 2j  1, j  N
ai =
abv | otherwise

(77)

In this case as well, the ordering constraints >v are set according to Eq. 75. Likewise,
if w = ha1 , . . . , al i, and u = ha1 , . . . , al i, we set >v,w according to Eq. 76 above,
and >v,u according to Eq. 78 below.
S



 = bu

Sai v {ai < a1 },


>v,u =
(78)
 = wu , l = 1
ai v {ai > a1 },

S

   {a > a }  {a < a },  = wu , l > 1
ai v

i

1

i

2

Finally, the ordering constraints >v,w for the rest of the parents w  pred(v) \ {u, w}
are set to empty sets.

The relations >v in this case are identical to these in the previous case, and relations
>v,u and >v,w are effectively identical to the relation >v,w in previous case. Thus,
we have >v  >v,u and >v  >v,w forming strict partial orders over the unions of the
elements of v and u , and v and w , respectively.
S
From Eqs. 75, 76, 78 we can now derive that any linearization of >v  wpred(v) >v,w
defines a sequence of actions that is applicable with respect to {v}  pred(v). In
addition, |v | = |v | together with Eq. 77 implies that this action sequence provides
to v the value G[v] if the latter is specified.
(III) If  = bw ,  = ww , we distinguish between a few cases based on w and v .
(1) If |v | = 2y + 1, |w | = 2x, |w |  |v |, then we construct two post-3/2
candidates for v , and then assign v to the cheapest among the two, proving
that its cost has to be lower than C(v ).
(a) All the changes of v to wv are done using action aw
1 , and then the largest
possible number of changes to bv are done using action ab1 , with the remaining
changes to bv being done using action ab2 . For this candidate for v , we set

b

a1 i = 2j, j  N, j < x
(79)
ai = ab2 i = 2j, j  N, x  j  y

 w
a1 otherwise
And the cost in this case is

b
b
(y + 1)  C(aw
1 ) + (x  1)  C(a1 ) + (y  x + 1)  C(a2 )

(80)

The ordering constraints >v are set according to Eq. 75. Likewise, if w =
ha1 , . . . , a2x1 i, and u = ha1 , . . . , al i, we set >v,w according to Eq. 81, and
>v,u according to Eq. 82.
>v,w =

[

{ai < aj | i  j < 2x  1}  {ai < a2x1 }  {aj < ai | j < i, j < 2x  1}


a

v ,aj w
i

(81)

257

fiKatz & Domshlak

For each u  pred(v) \ {w} we set,
S

{a < a1 },


Sai v i

   {a > a },
1
>v,u = Sai v i





a v {ai > a1 }  {ai < a2 },


 i
,

 = bu
 = wu , l = 1
 = wu , l > 1

.

(82)

otherwise

It is not hard to verify that the relation >v,w defined by Eq. 81 is a strict
total order over its domain. Suppose to the contrary that for some i, j, both
aj < ai and ai < aj . Then from first inequality we have either i  j < 2x  1
or j = 2x  1, and from second we have j < i, j < 2x  1.
The relations >v and >v,u are effectively identical to these in case (II). Thus,
we have >v  >v,u and >v  >v,w forming strict partial orders over the
unions of the elements of v and u , and v and w , respectively. S
From Eqs. 75, 81, 82 we can now derive that any linearization of >v  wpred(v) >v,w
defines a sequence of actions that is applicable with respect to {v}  pred(v).
In addition, |v | = |v | together with Eq. 79 implies that this action sequence
provides to v the value G[v] if the latter is specified.
(b) All the changes of v to bv are done using action ab1 , and then the largest
possible number of changes to wv are done using action aw
1 , with the remaining
.
For
this
candidate
for v , we set
changes to wv being done using action aw
2

w

a1 i = 2j  1, j  N, j  x
(83)
ai = aw
i = 2j  1, j  N, x < j  y + 1
2

 b
a1 otherwise
And the cost in this case is

w
b
x  C(aw
1 ) + (y + 1  x)  C(a2 ) + y  C(a1 )

(84)

The ordering constraints >v are set according to Eq. 75. Likewise, if w =
ha1 , . . . , a2x1 i, and u = ha1 , . . . , al i, we set >v,w according to Eq. 85, and
>v,u according to Eq. 86.
[
{ai < aj | i  j}  {aj < ai | j < i}
(85)
>v,w =
ai v ,aj w

For each u  pred(v) \ {w} we set,
S




ai v {ai < a1 },

S




 {a > a1 },
>v,u = Sai v i





ai v {ai > a1 }  {ai < a2 },



,

 = bu
 = wu , l = 1
 = wu , l > 1

.

(86)

otherwise

The relation >v,w defined by Eq. 85 is a strict total order over its domain.
The relations >v and >v,u are effectively identical to these in case (II). Thus,
258

fiTractable Cost-Optimal Planning

we have >v  >v,u and >v  >v,w forming strict partial orders over the
unions of the elements of v and u , and v and w , respectively. S
From Eqs. 75, 85, 86 we can now derive that any linearization of >v  wpred(v) >v,w
defines a sequence of actions that is applicable with respect to {v}  pred(v).
In addition, |v | = |v | together with Eq. 83 implies that this action sequence
provides to v the value G[v] if the latter is specified.
Now, for each cost-optimal plan , v cannot contain more than y + x actions
b
of both types aw
1 and a1 totally. Suppose to the contrary that v contain at
b
least y + x + 1 actions of types aw
1 and a1 . Then it contains no more than y  x
actions of other types. Let bw  ww  . . .  bw sequence of 2y + 1 values of w that
support a cost-optimal plan for v given that w can change its value any number
of times. Then each action of any other type will decrease the needed length of
this sequence by at most 2. Therefore at most y  x actions of other type will
decrease the length by at most 2y  2x, and we are left with the sequence of
length  2y + 1  (2y  2x) = 2x + 1. Therefore w cannot support more than
b
y + x actions of types aw
1 and a1 . Now, suppose that in some given cost-optimal
b
plan v for v there are  actions of type aw
1 and  actions of type a1 . Then
+ y+x

(87)

and
w
b
b
C(v )    C(aw
1 ) + (y + 1  )  C(a2 ) +   C(a1 ) + (y  )  C(a2 )

(88)

For (80)  (84), we have
w
C(ab2 )  C(ab1 )  C(aw
2 )  C(a1 )

(89)

Now suppose to the contrary that the plan in first case is not cost-optimal. Then
from Eq. 88 we have
w
b
b
  C(aw
1 ) + (y + 1  )  C(a2 ) +   C(a1 ) + (y  )  C(a2 ) <
b
b
(y + 1)  C(aw
1 ) + (x  1)  C(a1 ) + (y  x + 1)  C(a2 )

and from it
w
b
b
(y + 1  )  (C(aw
2 )  C(a1 )) < (  x + 1)  (C(a2 )  C(a1 ))

(90)

From Eq. 87 we have y + 1      x + 1, together with Eq. 89 contradicting
Eq. 90.
For (84)  (80), we have
w
b
b
C(aw
2 )  C(a1 )  C(a2 )  C(a1 )

(91)

Now suppose to the contrary that the plan in second case is not cost-optimal.
Then from Eq. 88 we have
w
b
b
  C(aw
1 ) + (y + 1  )  C(a2 ) +   C(a1 ) + (y  )  C(a2 ) <
w
b
x  C(aw
1 ) + (y  x + 1)  C(a2 ) + y  C(a1 )

259

fiKatz & Domshlak

and from it
w
(y  )  (C(ab2 )  C(ab1 )) < (  x)  (C(aw
2 )  C(a1 ))

(92)

From Eq. 87 we have y      x, together with Eq. 91 contradicting Eq. 92.
(2) If |v | = 2y + 1, |w | = 2x, |w | > |v |, then the actions of v are set to
(
awv |bw i = 2j  1, j  N
ai =
abv |ww otherwise

(93)

In this case as well, the ordering constraints >v are set according to Eq. 75.
Likewise, if w = ha1 , . . . , a2x1 i, we set >v,w according to Eq. 85 above. Finally,
the ordering constraints >v,w for the rest of the parents w  pred(v) \ {u, w} are
set to empty sets.
The relations >v and >v,w are identical to the previous case. Thus, we have
>v  >v,w forming strict partial order over the union of the elements of v and
w .
S
From Eqs. 75, 85 we can now derive that any linearization of >v  wpred(v) >v,w
defines a sequence of actions that is applicable with respect to {v}  pred(v). In
addition, |v | = |v | together with Eq. 93 implies that this action sequence
provides to v the value G[v] if the latter is specified.
(3) If |v | = 2y, |w | = 2x, |w | < |v |, then we construct two post-3/2 candidates
for v , and then assign v to the cheapest among the two, proving that its cost
has to be lower than C(v ).
(a) All the changes of v to wv are done using action aw
1 , and then the largest
possible number of changes to bv are done using action ab1 , with the remaining
changes to bv being done using action ab2 . For this candidate for v , we set

b

a2 i = 2j, j  N, j  y  x
(94)
ai = ab1 i = 2j, j  N, y  x < j  y

 w
a1 otherwise
And the cost in this case is

b
b
y  C(aw
1 ) + x  C(a1 ) + (y  x)  C(a2 )

(95)

The ordering constraints >v are set according to Eq. 75. Likewise, if w =
ha1 , . . . , a2x1 i, we set >v,w according to Eq. 96.
[
>v,w =
{ai < aj | i  2y  2x + j}  {aj < ai | i > 2y  2x + j}
ai v ,aj w

(96)
For each u  pred(v) \ {w} we set >v,u according to Eq. 82. It is easy to
verify that the relation >v,w defined by Eq. 96 is a strict total order over its
domain. The relations >v and >v,u are effectively identical to the previous
260

fiTractable Cost-Optimal Planning

case. Thus, we have >v  >v,u and >v  >v,w forming strict partial orders
over the unions of the elements of v and u , and v and w , respectively.
S
From Eqs. 75, 82, 96 we can now derive that any linearization of >v  wpred(v) >v,w
defines a sequence of actions that is applicable with respect to {v}  pred(v).
In addition, |v | = |v | together with Eq. 94 implies that this action sequence
provides to v the value G[v] if the latter is specified.
(b) All the changes of v to bv are done using action ab1 , and then the largest
possible number of changes to wv are done using action aw
1 , with the remaining
w
changes to wv being done using action a2 . For this candidate for v , we set

w

a1

ai = aw
2

 b
a1

i = 2j  1, j  N, j  x
i = 2j  1, j  N, x < j  y
otherwise

(97)

And the cost in this case is

w
b
x  C(aw
1 ) + (y  x)  C(a2 ) + y  C(a1 )

(98)

The ordering constraints >v are set according to Eq. 75. Likewise, if w =
ha1 , . . . , a2x1 i, we set >v,w according to Eq. 85 above.
For each u  pred(v) \ {w} we set >v,u according to Eq. 86. The relations >v ,
>v,w and >v,u are effectively identical to the previous case. Thus, again, we
have >v  >v,u and >v  >v,w forming strict partial orders over the unions
of the elements of v and u , and v and w , respectively.
S
From Eqs. 75, 85, 86 we can now derive that any linearization of >v  wpred(v) >v,w
defines a sequence of actions that is applicable with respect to {v}  pred(v).
In addition, |v | = |v | together with Eq. 97 implies that this action sequence
provides to v the value G[v] if the latter is specified.
Now, for each cost-optimal plan , v cannot contain more than y + x actions of
b
both types aw
1 and a1 totally. Suppose to the contrary that v contain at least
b
y + x + 1 actions of types aw
1 and a1 . Then it contains no more than y  x  1
actions of other types. Let bw  ww  . . .  ww sequence of 2y values of w that
support a cost-optimal plan for v given that w can change its value any number
of times. Then each action of any other type will decrease the needed length of
this sequence by at most 2. Therefore at most y  x  1 actions of other type will
decrease the length by at most 2y  2x  2, and we are left with the sequence of
length  2y  (2y  2x  2) = 2x + 2. Therefore w cannot support more than
b
y + x actions of types aw
1 and a1 . Now, suppose that in some given cost-optimal
b
plan v for v there are  actions of type aw
1 and  actions of type a1 . Then
+ y+x

(99)

and
w
b
b
C(v )    C(aw
1 ) + (y  )  C(a2 ) +   C(a1 ) + (y  )  C(a2 )

261

(100)

fiKatz & Domshlak

For (95)  (98), we have
w
C(ab2 )  C(ab1 )  C(aw
2 )  C(a1 )

(101)

Now suppose to the contrary that the plan in first case is not cost-optimal. Then
from Eq. 100 we have
w
b
b
  C(aw
1 ) + (y  )  C(a2 ) +   C(a1 ) + (y  )  C(a2 ) <
b
b
y  C(aw
1 ) + x  C(a1 ) + (y  x)  C(a2 )

and from it
w
b
b
(y  )  (C(aw
2 )  C(a1 )) < (  x)  (C(a2 )  C(a1 ))

(102)

From Eq. 99 we have y      x, together with Eq. 101 contradicting Eq. 102.
For (98)  (95), we have
w
b
b
C(aw
2 )  C(a1 )  C(a2 )  C(a1 )

(103)

Now suppose to the contrary that the plan in second case is not cost-optimal.
Then from Eq. 100 we have
w
b
b
  C(aw
1 ) + (y  )  C(a2 ) +   C(a1 ) + (y  )  C(a2 ) <
w
b
x  C(aw
1 ) + (y  x)  C(a2 ) + y  C(a1 )

and from it
w
(y  )  (C(ab2 )  C(ab1 )) < (  x)  (C(aw
2 )  C(a1 ))

(104)

From Eq. 99 we have y      x, together with Eq. 103 contradicting Eq. 104.
(4) If |v | = 2y, |w | = 2x, |w |  |v |, then the actions of v are set to
(
awv |bw i = 2j  1, j  N

ai =
abv |ww otherwise

(105)

The ordering constraints >v are set according to Eq. 75. Likewise, if w =
ha1 , . . . , a2x1 i, we set >v,w according to Eq. 85 above. Finally, the ordering
constraints >v,w for the rest of the parents w  pred(v) \ {u, w} are set to empty
sets. The relations >v and >v,w are identical to the previous case. Thus, we have
>v  >v,w forming strict partial order over the union of the elements of v and
w .
S
From Eqs. 75, 85 we can now derive that any linearization of >v  wpred(v) >v,w
defines a sequence of actions that is applicable with respect to {v}  pred(v). In
addition, |v | = |v | together with Eq. 105 implies that this action sequence
provides to v the value G[v] if the latter is specified.
(5) If |v | = 2y + 1, |w | = 2x + 1, |w | < |v |, then we construct two post-3/2
candidates for v , and then assign v to the cheapest among the two, proving
that its cost has to be lower than C(v ).
262

fiTractable Cost-Optimal Planning

(a) All the changes of v to wv are done using action aw
1 , and then the largest
possible number of changes to bv are done using action ab1 , with the remaining
changes to bv being done using action ab2 . For this candidate for v , we set

b

a2 i = 2j, j  N, j  y  x
(106)
ai = ab1 i = 2j, j  N, y  x < j  y

 w
a1 otherwise
And the cost in this case is

b
b
(y + 1)  C(aw
1 ) + x  C(a1 ) + (y  x)  C(a2 )

(107)

The ordering constraints >v are set according to Eq. 75. Likewise, if w =
ha1 , . . . , a2x i, we set >v,w according to Eq. 96 above. For each u  pred(v) \
{w} we set >v,u according to Eq. 82 above. The relations >v , >v,w and >v,u
are effectively identical to the previous case. Thus, we have >v  >v,u and
>v  >v,w forming strict partial orders over the unions of the elements of v
and u , and v and w , respectively.
S
From Eqs. 75, 82, 96 we can now derive that any linearization of >v  wpred(v) >v,w
defines a sequence of actions that is applicable with respect to {v}  pred(v).
In addition, |v | = |v | together with Eq. 106 implies that this action sequence provides to v the value G[v] if the latter is specified.
(b) All the changes of v to bv are done using action ab1 , and then the largest
possible number of changes to wv are done using action aw
1 , with the remaining
.
For
this
candidate
for v , we set
changes to wv being done using action aw
2

w

a1 i = 2j  1, j  N, j  x or j = y + 1

(108)
ai = aw
i = 2j  1, j  N, x < j  y
2

 b
a1 otherwise
And the cost in this case is

w
b
(x + 1)  C(aw
1 ) + (y  x)  C(a2 ) + y  C(a1 )

(109)

The ordering constraints >v are set according to Eq. 75. Likewise, if w =
ha1 , . . . , a2x i, we set >v,w according to Eq. 110.
>v,w =

[

ai v ,aj w

{ai < aj | i  j < 2x}  {ai < a2x | i  2y}
{aj < ai | j < i, j < 2x}  {a2x < a2y+1 }

(110)

For each u  pred(v) \ {w} we set >v,u according to Eq. 86 above.
It is easy to verify that the relation >v,w defined by Eq. 110 is a strict total
order over its domain. The relations >v and >v,u are effectively identical to
the previous case. Thus, we have >v  >v,u and >v  >v,w forming strict
partial orders over the unions of the elements of v and u , and v and w ,
respectively.
263

fiKatz & Domshlak

S
From Eqs. 75, 86, 110 we can now derive that any linearization of >v  wpred(v) >v,w
defines a sequence of actions that is applicable with respect to {v}  pred(v).
In addition, |v | = |v | together with Eq. 108 implies that this action sequence provides to v the value G[v] if the latter is specified.
Now, for each cost-optimal plan , v cannot contain more than y + x+ 1 actions
b
of both types aw
1 and a1 totally. Suppose to the contrary that v contain at least
b
y + x + 2 actions of types aw
1 and a1 . Then it contains no more than y  x  1
actions of other types. Let bw  ww  . . .  bw sequence of 2y + 1 values of w that
support a cost-optimal plan for v given that w can change its value any number
of times. Then each action of any other type will decrease the needed length of
this sequence by at most 2. Therefore at most y  x  1 actions of other type will
decrease the length by at most 2y  2x  2, and we are left with the sequence of
length  2y + 1  (2y  2x  2) = 2x + 3. Therefore w cannot support more
b
than y + x + 1 actions of types aw
1 and a1 . Now, suppose that in some given
cost-optimal plan v for v there are  actions of type aw
1 and  actions of type
b
a1 . Then
+  y+x+1
(111)
and
w
b
b
C(v )    C(aw
1 ) + (y + 1  )  C(a2 ) +   C(a1 ) + (y  )  C(a2 )

(112)

For (107)  (109), we have
w
C(ab2 )  C(ab1 )  C(aw
2 )  C(a1 )

(113)

Now suppose to the contrary that the plan in first case is not cost-optimal. Then
from Eq. 112 we have
w
b
b
  C(aw
1 ) + (y + 1  )  C(a2 ) +   C(a1 ) + (y  )  C(a2 ) <
b
b
(y + 1)  C(aw
1 ) + x  C(a1 ) + (y  x)  C(a2 )

and from it
w
b
b
(y + 1  )  (C(aw
2 )  C(a1 )) < (  x)  (C(a2 )  C(a1 ))

(114)

From Eq. 111 we have y + 1      x, together with Eq. 113 contradicting
Eq. 114.
For (109)  (107), we have
w
b
b
C(aw
2 )  C(a1 )  C(a2 )  C(a1 )

(115)

Now suppose to the contrary that the plan in second case is not cost-optimal.
Then from Eq. 112 we have
w
b
b
  C(aw
1 ) + (y + 1  )  C(a2 ) +   C(a1 ) + (y  )  C(a2 ) <
w
b
(x + 1  C(aw
1 ) + (y  x)  C(a2 ) + y  C(a1 )

264

fiTractable Cost-Optimal Planning

and from it
w
(y  )  (C(ab2 )  C(ab1 )) < (  x  1)  (C(aw
2 )  C(a1 ))

(116)

From Eq. 111 we have y      x  1, together with Eq. 115 contradicting
Eq. 116.
(6) If |v | = 2y + 1, |w | = 2x + 1, |w |  |v |, the actions of v are set to
(
awv |bw i = 2j  1, j  N

ai =
abv |ww otherwise

(117)

The ordering constraints >v are set according to Eq. 75. Likewise, if w =
ha1 , . . . , a2x i, we set >v,w according to Eq. 85 above. Finally, the ordering constraints >v,w for the rest of the parents w  pred(v) \ {u, w} are set to empty
sets. The relations >v and >v,w are identical to the previous case. Thus, we have
>v  >v,w forming strict partial order over the union of the elements of v and
w .
S
From Eqs. 75, 85 we can now derive that any linearization of >v  wpred(v) >v,w
defines a sequence of actions that is applicable with respect to {v}  pred(v). In
addition, |v | = |v | together with Eq. 117 implies that this action sequence
provides to v the value G[v] if the latter is specified.
(7) If |v | = 2y, |w | = 2x + 1, |w |  |v |, then we construct two post-3/2
candidates for v , and then assign v to the cheapest among the two, proving
that its cost has to be lower than C(v ).
(a) All the changes of v to wv are done using action aw
1 , and then the largest
possible number of changes to bv are done using action ab1 , with the remaining
changes to bv being done using action ab2 . For this candidate for v , we set

b

a2 i = 2j, j  N, j  y  x
(118)
ai = ab1 i = 2j, j  N, y  x < j  y

 w
a1 otherwise
And the cost in this case is

b
b
y  C(aw
1 ) + x  C(a1 ) + (y  x)  C(a2 )

(119)

The ordering constraints >v are set according to Eq. 75. Likewise, if w =
ha1 , . . . , a2x i, we set >v,w according to Eq. 96 above. For each u  pred(v) \
{w} we set >v,u according to Eq. 82 above. The relations >v , >v,w and >v,u
are effectively identical to the previous case. Thus, we have >v  >v,u and
>v  >v,w forming strict partial orders over the unions of the elements of v
and u , and v and w , respectively.
S
From Eqs. 75, 82, 96 we can now derive that any linearization of >v  wpred(v) >v,w
defines a sequence of actions that is applicable with respect to {v}  pred(v).
In addition, |v | = |v | together with Eq. 118 implies that this action sequence provides to v the value G[v] if the latter is specified.
265

fiKatz & Domshlak

(b) All the changes of v to bv are done using action ab1 , and then the largest
possible number of changes to wv are done using action aw
1 , with the remaining
w
changes to wv being done using action a2 . For this candidate for v , we set

w

a1

ai = aw
2

 b
a1

i = 2j  1, j  N, j  x
i = 2j  1, j  N, x < j  y
otherwise

(120)

And the cost in this case is

b
w
x  C(aw
1 ) + (y  x)  C(a2 ) + y  C(a1 )

(121)

The ordering constraints >v are set according to Eq. 75. Likewise, if w =
ha1 , . . . , a2x i, we set >v,w according to Eq. 122.
>v,w =

[

{ai < aj | i  j < 2x}  {aj < ai | j < i, j < 2x}  {ai < a2x }

ai v ,aj w

(122)

For each u  pred(v) \ {w} we set >v,u according to Eq. 86 above.
It is easy to verify that the relation >v,w defined by Eq. 122 is a strict total
order over its domain. The relations >v and >v,u are effectively identical to
the previous case. Thus, we have >v  >v,u and >v  >v,w forming strict
partial orders over the unions of the elements of v and u , and v and w ,
respectively.
S
From Eqs. 75, 86, 122 we can now derive that any linearization of >v  wpred(v) >v,w
defines a sequence of actions that is applicable with respect to {v}  pred(v).
In addition, |v | = |v | together with Eq. 120 implies that this action sequence provides to v the value G[v] if the latter is specified.
Now, for each cost-optimal plan , v cannot contain more than y + x actions of
b
both types aw
1 and a1 totally. Suppose to the contrary that v contain at least
b
y + x + 1 actions of types aw
1 and a1 . Then it contains no more than y  x  1
actions of other types. Let bw  ww  . . .  ww sequence of 2y values of w that
support a cost-optimal plan for v given that w can change its value any number
of times. Then each action of any other type will decrease the needed length of
this sequence by at most 2. Therefore at most y  x  1 actions of other type will
decrease the length by at most 2y  2x  2, and we are left with the sequence of
length  2y  (2y  2x  2) = 2x + 2. Therefore w cannot support more than
b
y + x actions of types aw
1 and a1 . Now, suppose that in some given cost-optimal
b
plan v for v there are  actions of type aw
1 and  actions of type a1 . Then
+ y+x

(123)

and
w
b
b
C(v )    C(aw
1 ) + (y  )  C(a2 ) +   C(a1 ) + (y  )  C(a2 )

266

(124)

fiTractable Cost-Optimal Planning

For (119)  (121), we have
w
C(ab2 )  C(ab1 )  C(aw
2 )  C(a1 )

(125)

Now suppose to the contrary that the plan in first case is not cost-optimal. Then
from Eq. 124 we have
b
w
b
  C(aw
1 ) + (y  )  C(a2 ) +   C(a1 ) + (y  )  C(a2 ) <
b
b
y  C(aw
1 ) + x  C(a1 ) + (y  x)  C(a2 )

and from it
w
b
b
(y  )  (C(aw
2 )  C(a1 )) < (  x)  (C(a2 )  C(a1 ))

(126)

From Eq. 123 we have y    x, together with Eq. 125 contradicting Eq. 126.
For (121)  (119), we have
w
b
b
C(aw
2 )  C(a1 )  C(a2 )  C(a1 )

(127)

Now suppose to the contrary that the plan in second case is not cost-optimal.
Then from Eq. 124 we have
w
b
b
  C(aw
1 ) + (y  )  C(a2 ) +   C(a1 ) + (y  )  C(a2 ) <
w
b
x  C(aw
1 ) + (y  x)  C(a2 ) + y  C(a1 )

and from it
w
(y  )  (C(ab2 )  C(ab1 )) < (  x)  (C(aw
2 )  C(a1 ))

(128)

From Eq. 123 we have y   x, together with Eq. 127 contradicting Eq. 128.
(8) If |v | = 2y, |w | = 2x + 1, |w | > |v |, then the actions of v are set to
(
awv |bw i = 2j  1, j  N

(129)
ai =
abv |ww otherwise
The ordering constraints >v are set according to Eq. 75. Likewise, if w =
ha1 , . . . , a2x i, we set >v,w according to Eq. 85 above. Finally, the ordering constraints >v,w for the rest of the parents w  pred(v) \ {u, w} are set to empty
sets. The relations >v and >v,w are identical to the previous case. Thus, we have
>v  >v,w forming strict partial order over the union of the elements of v and
w .
S
From Eqs. 75, 85 we can now derive that any linearization of >v  wpred(v) >v,w
defines a sequence of actions that is applicable with respect to {v}  pred(v). In
addition, |v | = |v | together with Eq. 129 implies that this action sequence
provides to v the value G[v] if the latter is specified.
(IV) If  = ww ,  = bw , we distinguish between a few cases based on w and v .
267

fiKatz & Domshlak

(1) If |v | = 2y + 1, |w | = 2x, |w |  |v |, then we construct two post-3/2
candidates for v , and then assign v to the cheapest among the two, proving
that its cost has to be lower than C(v ).
(a) All the changes of v to wv are done using action aw
1 , and then the largest
possible number of changes to bv are done using action ab1 , with the remaining
changes to bv being done using action ab2 . For this candidate for v , we set

b

a2 i = 2j, j  N, j  y  x + 1
(130)
ai = ab1 i = 2j, j  N, y  x + 1 < j  y

 w
a1 otherwise
And the cost in this case is

b
b
(y + 1)  C(aw
1 ) + (x  1)  C(a1 ) + (y  x + 1)  C(a2 )

(131)

The ordering constraints >v are set according to Eq. 75. Likewise, if w =
ha1 , . . . , a2x1 i, we set >v,w according to Eq. 132.
[
>v,w =
{ai < aj | i < j}  {aj < ai | j  i}
(132)
ai v ,aj w

For each u  pred(v) \ {w} we set >v,u according to Eq. 82.
It is easy to verify that the relation >v,w defined by Eq. 132 is a strict total
order over its domain. The relations >v and >v,u are effectively identical to
the previous case. Thus, we have >v  >v,u and >v  >v,w forming strict
partial orders over the unions of the elements of v and u , and v and w ,
respectively.
S
From Eqs. 75, 82, 132 we can now derive that any linearization of >v  wpred(v) >v,w
defines a sequence of actions that is applicable with respect to {v}  pred(v).
In addition, |v | = |v | together with Eq. 130 implies that this action sequence provides to v the value G[v] if the latter is specified.
(b) All the changes of v to bv are done using action ab1 , and then the largest
possible number of changes to wv are done using action aw
1 , with the remaining
.
For
this
candidate
for v , we set
changes to wv being done using action aw
2

w

a2 i = 2j  1, j  N, j  y  x + 1
(133)
ai = aw
i = 2j  1, j  N, y  x + 1 < j  y + 1
1

 b
a1 otherwise
And the cost in this case is

w
b
x  C(aw
1 ) + (y + 1  x)  C(a2 ) + y  C(a1 )

(134)

The ordering constraints >v are set according to Eq. 75. Likewise, if w =
ha1 , . . . , a2x1 i, we set >v,w according to Eq. 135.
[
>v,w =
{ai < aj | i  2y  2x + 1 + j}  {aj < ai | i > 2y  2x + 1 + j}
ai v ,aj w

(135)

268

fiTractable Cost-Optimal Planning

For each u  pred(v) \ {w} we set >v,u according to Eq. 86.
It is easy to verify that the relation >v,w defined by Eq. 135 is a strict total
order over its domain. The relations >v and >v,u are effectively identical to
the previous case. Thus, we have >v  >v,u and >v  >v,w forming strict
partial orders over the unions of the elements of v and u , and v and w ,
respectively.
S
From Eqs. 75, 86, 135 we can now derive that any linearization of >v  wpred(v) >v,w
defines a sequence of actions that is applicable with respect to {v}  pred(v).
In addition, |v | = |v | together with Eq. 133 implies that this action sequence provides to v the value G[v] if the latter is specified.
Now, for each cost-optimal plan , v cannot contain more than y + x actions
b
of both types aw
1 and a1 totally. Suppose to the contrary that v contain at
b
least y + x + 1 actions of types aw
1 and a1 . Then it contains no more than y  x
actions of other types. Let ww  wb  . . .  ww sequence of 2y + 1 values of w that
support a cost-optimal plan for v given that w can change its value any number
of times. Then each action of any other type will decrease the needed length of
this sequence by at most 2. Therefore at most y  x actions of other type will
decrease the length by at most 2y  2x, and we are left with the sequence of
length  2y + 1  (2y  2x) = 2x + 1. Therefore w cannot support more than
b
y + x actions of types aw
1 and a1 . Now, suppose that in some given cost-optimal
b
plan v for v there are  actions of type aw
1 and  actions of type a1 . Then
+ y+x

(136)

and
w
b
b
C(v )    C(aw
1 ) + (y + 1  )  C(a2 ) +   C(a1 ) + (y  )  C(a2 )

(137)

For (131)  (134), we have
w
C(ab2 )  C(ab1 )  C(aw
2 )  C(a1 )

(138)

Now suppose to the contrary that the plan in first case is not cost-optimal. Then
from Eq. 137 we have
w
b
b
  C(aw
1 ) + (y + 1  )  C(a2 ) +   C(a1 ) + (y  )  C(a2 ) <
b
b
(y + 1)  C(aw
1 ) + (x  1)  C(a1 ) + (y  x + 1)  C(a2 )

and from it
w
b
b
(y + 1  )  (C(aw
2 )  C(a1 )) < (  x + 1)  (C(a2 )  C(a1 ))

(139)

From Eq. 136 we have y + 1      x + 1, together with Eq. 138 contradicting
Eq. 139.
For (134)  (131), we have
w
b
b
C(aw
2 )  C(a1 )  C(a2 )  C(a1 )

269

(140)

fiKatz & Domshlak

Now suppose to the contrary that the plan in second case is not cost-optimal.
Then from Eq. 137 we have
w
b
b
  C(aw
1 ) + (y + 1  )  C(a2 ) +   C(a1 ) + (y  )  C(a2 ) <
w
b
x  C(aw
1 ) + (y  x + 1)  C(a2 ) + y  C(a1 )

and from it
w
(y  )  (C(ab2 )  C(ab1 )) < (  x)  (C(aw
2 )  C(a1 ))

(141)

From Eq. 136 we have y   x, together with Eq. 140 contradicting Eq. 141.
(2) If |v | = 2y + 1, |w | = 2x, |w | > |v |, then the actions of v are set to
(
awv |ww i = 2j  1, j  N
(142)
ai =
abv |bw otherwise
The ordering constraints >v are set according to Eq. 75. Likewise, if w =
ha1 , . . . , a2x1 i, we set >v,w according to Eq. 132 above. Finally, the ordering
constraints >v,w for the rest of the parents w  pred(v) \ {u, w} are set to empty
sets. The relations >v and >v,w are identical to the previous case. Thus, we have
>v  >v,w forming strict partial order over the union of the elements of v and
w .
S
From Eqs. 75, 132 we can now derive that any linearization of >v  wpred(v) >v,w
defines a sequence of actions that is applicable with respect to {v}  pred(v). In
addition, |v | = |v | together with Eq. 142 implies that this action sequence
provides to v the value G[v] if the latter is specified.
(3) If |v | = 2y, |w | = 2x, |w |  |v | + 1, then we construct two post-3/2
candidates for v , and then assign v to the cheapest among the two, proving
that its cost has to be lower than C(v ).
(a) All the changes of v to wv are done using action aw
1 , and then the largest
possible number of changes to bv are done using action ab1 , with the remaining
changes to bv being done using action ab2 . For this candidate for v , we set

b

a1 i = 2j, j  N, j < x
(143)
ai = ab2 i = 2j, j  N, x  j  y

 w
a1 otherwise
And the cost in this case is

b
b
y  C(aw
1 ) + (x  1)  C(a1 ) + (y  x + 1)  C(a2 )

(144)

The ordering constraints >v are set according to Eq. 75. Likewise, if w =
ha1 , . . . , a2x1 i, we set >v,w according to Eq. 132 above.
For each u  pred(v) \ {w} we set >v,u according to Eq. 82.
The relations >v , >v,w and >v,u are effectively identical to the previous case.
Thus, we have >v  >v,u and >v  >v,w forming strict partial orders over
the unions of the elements of v and u , and v and w , respectively.
270

fiTractable Cost-Optimal Planning

S
From Eqs. 75, 82, 132 we can now derive that any linearization of >v  wpred(v) >v,w
defines a sequence of actions that is applicable with respect to {v}  pred(v).
In addition, |v | = |v | together with Eq. 143 implies that this action sequence provides to v the value G[v] if the latter is specified.
(b) All the changes of v to bv are done using action ab1 , and then the largest
possible number of changes to wv are done using action aw
1 , with the remaining
w
changes to wv being done using action a2 . For this candidate for v , we set

w

a2
ai = aw
1

 b
a1

i = 2j  1, j  N, j  y  x + 1
i = 2j  1, j  N, y  x + 1 < j  y
otherwise

(145)

And the cost in this case is
w
b
(x  1)  C(aw
1 ) + (y  x + 1)  C(a2 ) + y  C(a1 )

(146)

The ordering constraints >v are set according to Eq. 75. Likewise, if w =
ha1 , . . . , a2x1 i, we set >v,w according to Eq. 135 above.
For each u  pred(v) \ {w} we set >v,u according to Eq. 86.
The relations >v , >v,w and >v,u are effectively identical to the previous case.
Thus, we have >v  >v,u and >v  >v,w forming strict partial orders over
the unions of the elements of v and u , and v and w , respectively.S
From Eqs. 75, 86, 135 we can now derive that any linearization of >v  wpred(v) >v,w
defines a sequence of actions that is applicable with respect to {v}  pred(v).
In addition, |v | = |v | together with Eq. 145 implies that this action sequence provides to v the value G[v] if the latter is specified.
Now, for each cost-optimal plan , v cannot contain more than y + x  1
b
actions of both types aw
1 and a1 totally. Suppose to the contrary that v contain
b
at least y + x actions of types aw
1 and a1 . Then it contains no more than y  x
actions of other types. Let ww  bw  . . .  bw sequence of 2y values of w that
support a cost-optimal plan for v given that w can change its value any number
of times. Then each action of any other type will decrease the needed length
of this sequence by at most 2. Therefore at most y  x actions of other type
will decrease the length by at most 2y  2x, and we are left with the sequence of
length  2y (2y 2x) = 2x, which have to be a subsequence of w , contradicting
with the fact that w is of the same or smaller size and starts with a different
character. Therefore w cannot support more than y + x actions of types aw
1 and
b
a1 . Now, suppose that in some given cost-optimal plan v for v there are 
b
actions of type aw
1 and  actions of type a1 . Then
+ y+x

(147)

and
w
b
b
C(v )    C(aw
1 ) + (y  )  C(a2 ) +   C(a1 ) + (y  )  C(a2 )

271

(148)

fiKatz & Domshlak

For (144)  (146), we have
w
C(ab2 )  C(ab1 )  C(aw
2 )  C(a1 )

(149)

Now suppose to the contrary that the plan in first case is not cost-optimal. Then
from Eq. 148 we have
w
b
b
  C(aw
1 ) + (y  )  C(a2 ) +   C(a1 ) + (y  )  C(a2 ) <
b
b
y  C(aw
1 ) + x  C(a1 ) + (y  x)  C(a2 )

and from it
w
b
b
(y  )  (C(aw
2 )  C(a1 )) < (  x)  (C(a2 )  C(a1 ))

(150)

From Eq. 147 we have y    x, together with Eq. 149 contradicting Eq. 150.
For (146)  (144), we have
w
b
b
C(aw
2 )  C(a1 )  C(a2 )  C(a1 )

(151)

Now suppose to the contrary that the plan in second case is not cost-optimal.
Then from Eq. 148 we have
w
b
b
  C(aw
1 ) + (y  )  C(a2 ) +   C(a1 ) + (y  )  C(a2 ) <
w
b
x  C(aw
1 ) + (y  x)  C(a2 ) + y  C(a1 )

and from it
w
(y  )  (C(ab2 )  C(ab1 )) < (  x)  (C(aw
2 )  C(a1 ))

(152)

From Eq. 147 we have y   x, together with Eq. 151 contradicting Eq. 152.
(4) If |v | = 2y, |w | = 2x, |w | > |v | + 1, then the actions of v are set to
(
awv |ww i = 2j  1, j  N

(153)
ai =
abv |bw otherwise
The ordering constraints >v are set according to Eq. 75. Likewise, if w =
ha1 , . . . , a2x1 i, we set >v,w according to Eq. 132 above. Finally, the ordering
constraints >v,w for the rest of the parents w  pred(v) \ {u, w} are set to empty
sets. The relations >v and >v,w are identical to the previous case. Thus, we have
>v  >v,w forming strict partial order over the union of the elements of v and
w .
S
From Eqs. 75, 132 we can now derive that any linearization of >v  wpred(v) >v,w
defines a sequence of actions that is applicable with respect to {v}  pred(v). In
addition, |v | = |v | together with Eq. 153 implies that this action sequence
provides to v the value G[v] if the latter is specified.
(5) If |v | = 2y + 1, |w | = 2x + 1, |w |  |v | + 1, then we construct two post-3/2
candidates for v , and then assign v to the cheapest among the two, proving
that its cost has to be lower than C(v ).
272

fiTractable Cost-Optimal Planning

(a) All the changes of v to wv are done using action aw
1 , and then the largest
possible number of changes to bv are done using action ab1 , with the remaining
changes to bv being done using action ab2 . For this candidate for v , we set

b

a1 i = 2j, j  N, j < x
ai = ab2 i = 2j, j  N, x  j  y
(154)

 w
a1 otherwise
And the cost in this case is

b
b
(y + 1)  C(aw
1 ) + (x  1)  C(a1 ) + (y  x + 1)  C(a2 )

(155)

The ordering constraints >v are set according to Eq. 75. Likewise, if w =
ha1 , . . . , a2x i, we set >v,w according to Eq. 156.
[
>v,w =
{ai < aj | i < j < 2x}  {aj < ai | j  i, j < 2x}  {ai < a2x }
ai v ,aj w

(156)

For each u  pred(v) \ {w} we set >v,u according to Eq. 82.
It is easy to verify that the relation >v,w defined by Eq. 156 is a strict total
order over its domain. The relations >v and >v,u are effectively identical to
the previous case. Thus, we have >v  >v,u and >v  >v,w forming strict
partial orders over the unions of the elements of v and u , and v and w ,
respectively.
S
From Eqs. 75, 82, 156 we can now derive that any linearization of >v  wpred(v) >v,w
defines a sequence of actions that is applicable with respect to {v}  pred(v).
In addition, |v | = |v | together with Eq. 154 implies that this action sequence provides to v the value G[v] if the latter is specified.
(b) All the changes of v to bv are done using action ab1 , and then the largest
possible number of changes to wv are done using action aw
1 , with the remaining
w
changes to wv being done using action a2 . For this candidate for v , we set

w

a2 i = 2j  1, j  N, j  y  x or j = y + 1
(157)
ai = aw
i = 2j  1, j  N, y  x < j  y
1

 b
a1 otherwise
And the cost in this case is

w
b
x  C(aw
1 ) + (y + 1  x)  C(a2 ) + y  C(a1 )

(158)

The ordering constraints >v are set according to Eq. 75. Likewise, if w =
ha1 , . . . , a2x1 i, we set >v,w according to Eq. 159.
[
>v,w =
{ai < aj | i < 2y  2x + j}  {aj < ai | i  2y  2x + j}
ai v ,aj w

(159)

273

fiKatz & Domshlak

For each u  pred(v) \ {w} we set >v,u according to Eq. 86.
It is easy to verify that the relation >v,w defined by Eq. 159 is a strict total
order over its domain. The relations >v and >v,u are effectively identical to
the previous case. Thus, we have >v  >v,u and >v  >v,w forming strict
partial orders over the unions of the elements of v and u , and v and w ,
respectively.
S
From Eqs. 75, 86, 159 we can now derive that any linearization of >v  wpred(v) >v,w
defines a sequence of actions that is applicable with respect to {v}  pred(v).
In addition, |v | = |v | together with Eq. 157 implies that this action sequence provides to v the value G[v] if the latter is specified.
Now, for each cost-optimal plan , v cannot contain more than y + x+ 1 actions
b
of both types aw
1 and a1 totally. Suppose to the contrary that v contain at least
b
y + x + 2 actions of types aw
1 and a1 . Then it contains no more than y  x  1
actions of other types. Let ww  bw  . . .  ww sequence of 2y + 1 values of w that
support a cost-optimal plan for v given that w can change its value any number
of times. Then each action of any other type will decrease the needed length of
this sequence by at most 2. Therefore at most y  x  1 actions of other type will
decrease the length by at most 2y  2x  2, and we are left with the sequence of
length  2y + 1  (2y  2x  2) = 2x + 3. Therefore w cannot support more
b
than y + x + 1 actions of types aw
1 and a1 . Now, suppose that in some given
cost-optimal plan v for v there are  actions of type aw
1 and  actions of type
ab1 . Then
+  y+x+1
(160)
and
w
b
b
C(v )    C(aw
1 ) + (y + 1  )  C(a2 ) +   C(a1 ) + (y  )  C(a2 )

(161)

For (155)  (158), we have
w
C(ab2 )  C(ab1 )  C(aw
2 )  C(a1 )

(162)

Now suppose to the contrary that the plan in first case is not cost-optimal. Then
from Eq. 161 we have
w
b
b
  C(aw
1 ) + (y + 1  )  C(a2 ) +   C(a1 ) + (y  )  C(a2 ) <
b
b
(y + 1)  C(aw
1 ) + x  C(a1 ) + (y  x)  C(a2 )

and from it
w
b
b
(y + 1  )  (C(aw
2 )  C(a1 )) < (  x)  (C(a2 )  C(a1 ))

(163)

From Eq. 160 we have y + 1      x, together with Eq. 162 contradicting
Eq. 163.
For (158)  (155), we have
w
b
b
C(aw
2 )  C(a1 )  C(a2 )  C(a1 )

274

(164)

fiTractable Cost-Optimal Planning

Now suppose to the contrary that the plan in second case is not cost-optimal.
Then from Eq. 161 we have
w
b
b
  C(aw
1 ) + (y + 1  )  C(a2 ) +   C(a1 ) + (y  )  C(a2 ) <
w
b
(x + 1  C(aw
1 ) + (y  x)  C(a2 ) + y  C(a1 )

and from it
w
(y  )  (C(ab2 )  C(ab1 )) < (  x  1)  (C(aw
2 )  C(a1 ))

(165)

From Eq. 160 we have y      x  1, together with Eq. 164 contradicting
Eq. 165.
(6) If |v | = 2y + 1, |w | = 2x + 1, |w | > |v | + 1, then the actions of v are set to
(
awv |ww i = 2j  1, j  N
(166)
ai =
abv |bw otherwise
The ordering constraints >v are set according to Eq. 75. Likewise, if w =
ha1 , . . . , a2x1 i, we set >v,w according to Eq. 132 above.
Finally, the ordering constraints >v,w for the rest of the parents w  pred(v) \
{u, w} are set to empty sets.
The relations >v and >v,w are identical to the previous case. Thus, we have
>v  >v,w forming strict partial order over the union of the elements of v and
w .
S
From Eqs. 75, 132 we can now derive that any linearization of >v  wpred(v) >v,w
defines a sequence of actions that is applicable with respect to {v}  pred(v). In
addition, |v | = |v | together with Eq. 166 implies that this action sequence
provides to v the value G[v] if the latter is specified.
(7) If |v | = 2y, |w | = 2x + 1, |w |  |v |, then we construct two post-3/2
candidates for v , and then assign v to the cheapest among the two, proving
that its cost has to be lower than C(v ).
(a) All the changes of v to wv are done using action aw
1 , and then the largest
possible number of changes to bv are done using action ab1 , with the remaining
changes to bv being done using action ab2 . For this candidate for v , we set

b

a2 i = 2j, j  N, j  y  x
(167)
ai = ab1 i = 2j, j  N, y  x < j  y

 w
a1 otherwise
And the cost in this case is

b
b
y  C(aw
1 ) + x  C(a1 ) + (y  x)  C(a2 )

(168)

The ordering constraints >v are set according to Eq. 75. Likewise, if w =
ha1 , . . . , a2x i, we set >v,w according to Eq. 169.
>v,w =

[

{ai < aj | i < 2y  2x + j, j > 1}  {aj < ai | i  2y  2x + j}  {a1 < ai }


a

v ,aj w
i

(169)

275

fiKatz & Domshlak

For each u  pred(v) \ {w} we set >v,u according to Eq. 82.
It is easy to verify that the relation >v,w defined by Eq. 169 is a strict total
order over its domain. The relations >v and >v,u are effectively identical to
the previous case. Thus, we have >v  >v,u and >v  >v,w forming strict
partial orders over the unions of the elements of v and u , and v and w ,
respectively.
S
From Eqs. 75, 82, 169 we can now derive that any linearization of >v  wpred(v) >v,w
defines a sequence of actions that is applicable with respect to {v}  pred(v).
In addition, |v | = |v | together with Eq. 167 implies that this action sequence provides to v the value G[v] if the latter is specified.
(b) All the changes of v to bv are done using action ab1 , and then the largest
possible number of changes to wv are done using action aw
1 , with the remaining
changes to wv being done using action aw
.
For
this
candidate
for v , we set
2

w

a2
ai = aw
1

 b
a1

i = 2j  1, j  N, j  y  x
i = 2j  1, j  N, y  x < j  y
otherwise

(170)

And the cost in this case is
w
b
x  C(aw
1 ) + (y  x)  C(a2 ) + y  C(a1 )

(171)

The ordering constraints >v are set according to Eq. 75. Likewise, if w =
ha1 , . . . , a2x1 i, we set >v,w according to Eq. 159 above.
For each u  pred(v) \ {w} we set >v,u according to Eq. 86 above.
The relations >v , >v,w and >v,u are effectively identical to the previous case.
Thus, we have >v  >v,u and >v  >v,w forming strict partial orders over
the unions of the elements of v and u , and v and w , respectively.S
From Eqs. 75, 86, 159 we can now derive that any linearization of >v  wpred(v) >v,w
defines a sequence of actions that is applicable with respect to {v}  pred(v).
In addition, |v | = |v | together with Eq. 170 implies that this action sequence provides to v the value G[v] if the latter is specified.
Now, for each cost-optimal plan , v cannot contain more than y + x actions of
b
both types aw
1 and a1 totally. Suppose to the contrary that v contain at least
b
y + x + 1 actions of types aw
1 and a1 . Then it contains no more than y  x  1
actions of other types. Let ww  bw  . . .  bw sequence of 2y values of w that
support a cost-optimal plan for v given that w can change its value any number
of times. Then each action of any other type will decrease the needed length of
this sequence by at most 2. Therefore at most y  x  1 actions of other type will
decrease the length by at most 2y  2x  2, and we are left with the sequence of
length  2y  (2y  2x  2) = 2x + 2. Therefore w cannot support more than
b
y + x actions of types aw
1 and a1 . Now, suppose that in some given cost-optimal
b
plan v for v there are  actions of type aw
1 and  actions of type a1 . Then
+ y+x
276

(172)

fiTractable Cost-Optimal Planning

and
w
b
b
C(v )    C(aw
1 ) + (y  )  C(a2 ) +   C(a1 ) + (y  )  C(a2 )

(173)

For (168)  (171), we have
w
C(ab2 )  C(ab1 )  C(aw
2 )  C(a1 )

(174)

Now suppose to the contrary that the plan in first case is not cost-optimal. Then
from Eq. 173 we have
w
b
b
  C(aw
1 ) + (y  )  C(a2 ) +   C(a1 ) + (y  )  C(a2 ) <
b
b
y  C(aw
1 ) + x  C(a1 ) + (y  x)  C(a2 )

and from it
w
b
b
(y  )  (C(aw
2 )  C(a1 )) < (  x)  (C(a2 )  C(a1 ))

(175)

From Eq. 172 we have y    x, together with Eq. 174 contradicting Eq. 175.
For (171)  (168), we have
w
b
b
C(aw
2 )  C(a1 )  C(a2 )  C(a1 )

(176)

Now suppose to the contrary that the plan in second case is not cost-optimal.
Then from Eq. 173 we have
w
b
b
  C(aw
1 ) + (y  )  C(a2 ) +   C(a1 ) + (y  )  C(a2 ) <
w
b
x  C(aw
1 ) + (y  x)  C(a2 ) + y  C(a1 )

and from it
w
(y  )  (C(ab2 )  C(ab1 )) < (  x)  (C(aw
2 )  C(a1 ))

(177)

From Eq. 172 we have y   x, together with Eq. 176 contradicting Eq. 177.
(8) If |v | = 2y, |w | = 2x + 1, |w | > |v |, then the actions of v are set to
(
awv |ww i = 2j  1, j  N
(178)
ai =
abv |bw otherwise
The ordering constraints >v are set according to Eq. 75. Likewise, if w =
ha1 , . . . , a2x1 i, we set >v,w according to Eq. 132 above.
Finally, the ordering constraints >v,w for the rest of the parents w  pred(v) \
{u, w} are set to empty sets.
The relations >v and >v,w are identical to the previous case. Thus, we have
>v  >v,w forming strict partial order over the union of the elements of v and
w .
277

fiKatz & Domshlak

Until now, we have specified the sequences v , the orders >v induced by these sequences,
the orders >v,w , and proved that all >v  >v,w and >w  >v,w form strict partial orders
over their domains. This construction allows us to apply now Theorem 1 to the (considered
as sets) sequences v and orders >v and >v,w , proving that
[
[
>=
(>v 
>v,w )
vV

wpred(v)

forms a strict partial order over the union of v1 , . . . , vn . Putting thing together, the
above implies that any linearization  of > is a plan for , and post-3/2ness of all its
subsequences v1 , . . . , vn then implies   P 3/2 (). Moreover, if  is an optimal plan for
, then C( ) = C() implies the optimality of  .

Theorem 9 Let  be a P(1) problem, COP = (X , F) be the corresponding
constraint
P
optimization problem, and x be an optimal assignment to X with F (x) = .

(I) If  < , then a plan of cost  for  can be reconstructed from x in time polynomial
in the description size of .

(II) If  has a plan, then  < .

Proof:
P
(I) Given a COP solution x with F (x) =  < , we construct a plan  for  with
C() = . We construct this plan by
1. Traversing the planning variables in a topological ordering of the causal graph CG(),
and associating each variable v with a sequence v  Av .
2. Merging the constructed sequences v1 , . . . , vn into the desired plan .
For each v  V with pred(v) =  we set v = ha1  . . .  al i, where l = |xv |  1, and ai is
defined as in Eq 179 below.
(
awv , i is odd,
(179)
ai =
abv , i is even,
Note that Eq. 179 is well-definedthe existence of the essential for Eq. 179 actions awv /abv
is implied by Eq. 22 and  < .
wk
1
In turn, for each v  V with pred(v) = {w1 , . . . , wk }, given xw
v , . . . , xv , we distinguish
between the following cases.
[ R1 is played ] R1 is played by one of the parents, while all other parents play role R11.
[ R1 is played by w1 ] Eq. 35 then implies
w1
1
xwv 1 (xw
v , xw1 ) = (R1, xv , xw1 )
w

j

1
and xw
v  S = S1. From Eq. 36 we then have xv  S = S1 for each 1 < j  k,
giving us
w
w
xwj (xv j , xv j1 , xwj ) = 0
v

278

fiTractable Cost-Optimal Planning

[ R1 is played by wj , j > 1 ] Eq. 36 then implies


xwj ([[S, #w , #b , ]] , S , #w , #b ,   , wj ) =
v


(R1, S, #w  #w , #b  #b ,  , wj )

and, for all 1 < i 6= j  k,

w

i1
i
xwv i (xw
, xwi ) = 0.
v , xv

From Eq. 35 we also have
1
xwv 1 (xw
v , x w1 ) = 0

In both these sub-cases, v , >v and >v,w are specified as in the proof of Theorem 8,
case I.
[ R2 is played ] R2 is played by one of the parents, while all other parents play role R11.
[ R2 is played by w1 ] Eq. 35 then implies
w1
1
xwv 1 (xw
v , xw1 ) = (R2, xv , xw1 )

and, for each 1 < j  k, Eq. 36 implies
w

w

xwj (xv j , xv j1 , xwj ) = 0
v

If (R2, [S, #w , #b , ]] , w1 ) = (#w , 0, #b , 0), then v , >v and >v,w are specified
as in the proof of Theorem 8, case III.2, otherwise, as in the case IV.2.
[ R2 is played by wj , j > 1 ] Eq. 36 then implies


xwj ([[S, #w , #b , ]] , S , #w , #b ,   , wj ) =
v


(R2, S, #w  #w , #b  #b ,  , wj )

and, for all 1 < i 6= j  k,

w

i1
i
xwv i (xw
, xwi ) = 0.
v , xv

From Eq. 35 we also have
1
xwv 1 (xw
v , x w1 ) = 0

If (R2, [S, #w  #w , #b  #b , ]] , wj ) = (#w  #w , 0, #b  #b , 0), then v , >v
and >v,w are specified as in the proof of Theorem 8, case III.2, otherwise, as in
the case IV.2.
[ R3 and R4 are played ] Those roles are played by two of the parents, while all other
parents play role R11.
279

fiKatz & Domshlak

[ R3 is played by w1 , R4 is played by wj , j > 1 ] From Eqs. 35 and 36 we then
have
w1
1
xwv 1 (xw
v , xw1 ) = (R3, xv , xw1 )
and


xwj ([[S, #w , #b , ]] , S , #w , #b ,   , wj ) =
v


(R4, S, #w  #w , #b  #b ,  , wj )

and, for all 1 < i  k, such that i 6= j:

w

i1
i
, x wi ) = 0
xwv i (xw
v , xv

[ R4 is played by w1 , R3 is played by wj , j > 1 ] From Eqs. 35 and 36 we then
have
w1
1
xwv 1 (xw
v , xw1 ) = (R4, xv , xw1 )
and


xwj ([[S, #w , #b , ]] , S , #w , #b ,   , wj ) =
v


(R3, S, #w  #w , #b  #b ,  , wj )

and, for all 1 < i 6= j  k,

w

i1
i
, x wi ) = 0
xwv i (xw
v , xv

[ R3 is played by wj , R4 is played by wt , j 6= t, j, t > 1 ] From Eqs. 35 and 36
we have
1
xwv 1 (xw
v , x w1 ) = 0
and

and



xwj ([[S, #w , #b , ]] , S , #w , #b ,   , wj ) =
v


(R3, S, #w  #w , #b  #b ,  , wj )


xwv t ([[S, #w , #b , ]] , S , #w , #b ,   , wt ) =


(R4, S, #w  #w , #b  #b ,  , wt )

and, for all 1 < i  k such that i 6 {j, t},
w

i1
i
xwv i (xw
, x wi ) = 0
v , xv

In all these three sub-cases, v , >v and >v,w are specified as in the proof of Theorem 8,
case II.
[ R5 is played ] R5 is played by one of the parents, while all other parents play role R11.
280

fiTractable Cost-Optimal Planning

[ R5 is played by w1 ] Eqs. 35 and 36 imply
w1
1
xwv 1 (xw
v , xw1 ) = (R5, xv , xw1 )

and, for each 1 < j  k,
w

w

xwj (xv j , xv j1 , xwj ) = 0
v

Considering now the specification of the function  in Eq. 29,
 If the first case holds, and the minimum is obtained at the first expression,
then v , >v and >v,w are defined as in the proof of Theorem 8, case III.1.a.
 If the first case holds, and the minimum is obtained at the second expression,
then v , >v and >v,w are defined as in the proof of Theorem 8, case IV.1.a.
 If the second case holds, and the minimum is obtained at the first expression,
then v , >v and >v,w are defined as in the proof of Theorem 8, case III.3.a.
 If the second case holds, and the minimum is obtained at the second expression, then v , >v and >v,w are defined as in the proof of Theorem 8, case
IV.3.a.
 If the third case holds, then v , >v and >v,w are defined as in the proof of
Theorem 8, case III.3.a.
 If the forth case holds, then v , >v and >v,w are defined as in the proof of
Theorem 8, case IV.3.a.
 If the fifth case holds, and the minimum is obtained at the first expression,
then v , >v and >v,w are defined as in the proof of Theorem 8, case III.5.a.
 If the fifth case holds, and the minimum is obtained at the second expression,
then v , >v and >v,w are defined as in the proof of Theorem 8, case IV.5.a.
 If the sixth case holds, then v , >v and >v,w are defined as in the proof of
Theorem 8, case III.5.a.
 If the seventh case holds, then v , >v and >v,w are defined as in the proof
of Theorem 8, case IV.5.a.
 If the eighth case holds, and the minimum is obtained at the first expression,
then v , >v and >v,w are defined as in the proof of Theorem 8, case III.7.a.
 If the eighth case holds, and the minimum is obtained at the second expression, then v , >v and >v,w are defined as in the proof of Theorem 8, case
IV.7.a.
[ R5 is played by wj , j > 1 ] Eq. 36 then implies


xwj ([[S, #w , #b , ]] , S , #w , #b ,   , wj ) =
v


(R5, S, #w  #w , #b  #b ,  , wj )
and, for all 1 < i 6= j  k,

w

i1
i
, xwi ) = 0.
xwv i (xw
v , xv

From Eq. 35 we also have
1
xwv 1 (xw
v , x w1 ) = 0

Here v , >v and >v,w are specified exactly as in the previous case.
281

fiKatz & Domshlak

[ R6 is played ] R6 is played by one of the parents, while all other parents play role R11.
[ R6 is played by w1 ] Eqs. 35 and 36 imply
w1
1
xwv 1 (xw
v , xw1 ) = (R6, xv , xw1 )

and, for each 1 < j  k,
w

w

xwj (xv j , xv j1 , xwj ) = 0
v

Considering now the specification of the function  in Eq. 30,
 If the first case holds, and the minimum is obtained at the first expression,
then v , >v and >v,w are defined as in the proof of Theorem 8, case III.1.b.
 If the first case holds, and the minimum is obtained at the second expression,
then v , >v and >v,w are defined as in the proof of Theorem 8, case IV.1.b.
 If the second case holds, and the minimum is obtained at the first expression,
then v , >v and >v,w are defined as in the proof of Theorem 8, case III.3.b.
 If the second case holds, and the minimum is obtained at the second expression, then v , >v and >v,w are defined as in the proof of Theorem 8, case
IV.3.b.
 If the third case holds, then v , >v and >v,w are defined as in the proof of
Theorem 8, case III.3.b.
 If the forth case holds, then v , >v and >v,w are defined as in the proof of
Theorem 8, case IV.3.b.
 If the fifth case holds, and the minimum is obtained at the first expression,
then v , >v and >v,w are defined as in the proof of Theorem 8, case III.5.b.
 If the fifth case holds, and the minimum is obtained at the second expression,
then v , >v and >v,w are defined as in the proof of Theorem 8, case IV.5.b.
 If the sixth case holds, then v , >v and >v,w are defined as in the proof of
Theorem 8, case III.5.b.
 If the seventh case holds, and the minimum is obtained at the first expression,
then v , >v and >v,w are defined as in the proof of Theorem 8, case III.7.b.
 If the seventh case holds, and the minimum is obtained at the second expression, then v , >v and >v,w are defined as in the proof of Theorem 8,
case IV.7.b.
[ R6 is played by wj , j > 1 ] Eq. 36 then implies


xwj ([[S, #w , #b , ]] , S , #w , #b ,   , wj ) =
v


(R6, S, #w  #w , #b  #b ,  , wj )
and, for all 1 < i 6= j  k,

w

i1
i
, xwi ) = 0.
xwv i (xw
v , xv

From Eq. 35 we also have
1
xwv 1 (xw
v , x w1 ) = 0

Here v , >v and >v,w are specified exactly as in the previous case.
282

fiTractable Cost-Optimal Planning

[ R7 and R9 are played ] Those roles are played by two of the parents, while all other
parents play role R11.
[ R7 is played by w1 , R9 is played by wj , j > 1 ] From Eqs. 35 and 36 we then
have
w1
1
xwv 1 (xw
v , xw1 ) = (R7, xv , xw1 )
and


xwj ([[S, #w , #b , ]] , S , #w , #b ,   , wj ) =
v


(R9, S, #w  #w , #b  #b ,  , wj )

and ,for all 1 < i 6= j  k,

w

i1
i
xwv i (xw
, x wi ) = 0
v , xv

Considering now the specification of the function  in Eq. 31,
 If the first case holds, and the minimum is obtained at the first expression,
then v , >v and >v,w are defined as in the proof of Theorem 8, case III.1.a.
 If the first case holds, and the minimum is obtained at the second expression,
then v , >v and >v,w are defined as in the proof of Theorem 8, case IV.1.a.
 If the second case holds, then v , >v and >v,w are defined as in the proof of
Theorem 8, case III.3.a.
 If the third case holds, then v , >v and >v,w are defined as in the proof of
Theorem 8, case IV.3.a.
 If the forth case holds, then v , >v and >v,w are defined as in the proof of
Theorem 8, case III.5.a.
 If the fifth case holds, then v , >v and >v,w are defined as in the proof of
Theorem 8, case IV.5.a.
 If the sixth case holds, and the minimum is obtained at the first expression,
then v , >v and >v,w are defined as in the proof of Theorem 8, case III.7.a.
 If the sixth case holds, and the minimum is obtained at the second expression,
then v , >v and >v,w are defined as in the proof of Theorem 8, case IV.7.a.
[ R9 is played by w1 , R7 is played by wj , j > 1 ] From Eqs. 35 and 36 we then
have
w1
1
xwv 1 (xw
v , xw1 ) = (R9, xv , xw1 )
and


xwj ([[S, #w , #b , ]] , S , #w , #b ,   , wj ) =
v


(R7, S, #w  #w , #b  #b ,  , wj )

and, for all 1 < i 6= j  k,

w

i1
i
xwv i (xw
, x wi ) = 0
v , xv

Here v , >v and >v,w are specified exactly as in the previous case.
283

fiKatz & Domshlak

[ R7 is played by wj , R9 is played by wt , j 6= t, j, t > 1 ] From Eqs. 35 and 36
we have
1
xwv 1 (xw
v , x w1 ) = 0
and

and



xwj ([[S, #w , #b , ]] , S , #w , #b ,   , wj ) =
v


(R7, S, #w  #w , #b  #b ,  , wj )


xwv t ( [S, #w , #b , ]] , S , #w , #b ,   , wt ) =


(R9, S, #w  #w , #b  #b ,  , wt )

and, for all 1 < i  k, such that i 6 {j, t},
w

i1
i
xwv i (xw
, x wi ) = 0
v , xv

Then, v , >v and >v,w are specified exactly as in the two previous cases.
[ R8 and R10 are played ] Those roles are played by two of the parents, while all other
parents play role R11.
[ R8 is played by w1 , R10 is played by wj , j > 1 ] From Eqs. 35 and 36 we then
have
w1
1
xwv 1 (xw
v , xw1 ) = (R8, xv , xw1 )
and


xwj ([[S, #w , #b , ]] , S , #w , #b ,   , wj ) =
v


(R10, S, #w  #w , #b  #b ,  , wj )

and, for all 1 < i 6= j  k,

w

i1
i
xwv i (xw
, x wi ) = 0
v , xv

Considering now the specification of the function  in Eq. 32,
 If the first case holds, and the minimum is obtained at the first expression,
then v , >v and >v,w are defined as in the proof of Theorem 8, case III.1.b.
 If the first case holds, and the minimum is obtained at the second expression,
then v , >v and >v,w are defined as in the proof of Theorem 8, case IV.1.b.
 If the second case holds, then v , >v and >v,w are defined as in the proof of
Theorem 8, case III.3.b.
 If the third case holds, then v , >v and >v,w are defined as in the proof of
Theorem 8, case IV.3.b.
 If the forth case holds, then v , >v and >v,w are defined as in the proof of
Theorem 8, case III.5.b.
 If the fifth case holds, then v , >v and >v,w are defined as in the proof of
Theorem 8, case IV.5.b.
284

fiTractable Cost-Optimal Planning

 If the sixth case holds, and the minimum is obtained at the first expression,
then v , >v and >v,w are defined as in the proof of Theorem 8, case III.7.b.
 If the sixth case holds, and the minimum is obtained at the second expression,
then v , >v and >v,w are defined as in the proof of Theorem 8, case IV.7.b.
[ R10 is played by w1 , R8 is played by wj , j > 1 ] From Eqs. 35 and 36 we then
have
w1
1
xwv 1 (xw
v , xw1 ) = (R10, x v , xw1 )
and


xwj ([[S, #w , #b , ]] , S , #w , #b ,   , wj ) =
v


(R8, S, #w  #w , #b  #b ,  , wj )

and, for all 1 < i 6= j  k,

w

i1
i
xwv i (xw
, x wi ) = 0
v , xv

Here v , >v and >v,w are specified exactly as in the previous case.
[ R8 is played by wj , R10 is played by wt , j 6= t, j, t > 1 ] From Eqs. 35 and 36
we have
1
xwv 1 (xw
v , x w1 ) = 0
and

and



xwj ([[S, #w , #b , ]] , S , #w , #b ,   , wj ) =
v


(R8, S, #w  #w , #b  #b ,  , wj )


xwv t ( [S, #w , #b , ]] , S , #w , #b ,   , wt ) =


(R10, S, #w  #w , #b  #b ,  , wt )

and, for all 1 < i  k, such that i 6 {j, t},
w

i1
i
, x wi ) = 0
xwv i (xw
v , xv

Then, v , >v and >v,w are specified exactly as in the two previous cases.
Until now, for each variable v  V , we have specified the action sequence v and the
order >v over the elements of v . For each w  pred(v), we have specified the order >v,w ,
and proved that all >v  >v,w and >w  >v,w form strict partial orders over their domains.
Similarly to the uniform cost case, this construction allows us to apply now Theorem 1 on
the (considered as sets) sequences v and orders >v and >v,w , proving that
[
[
>=
(>v 
>v,w )
vV

wpred(v)

forms a strict partial order over the union of v1 , . . . , vn .
285

fiKatz & Domshlak

Finally, we note that the plan extraction step of the algorithm polytree-1-dep corresponds exactly to the above construction along Eqs. 74-79, 81-83, 85-86, 93-94, 9697, 105-106, 108, 110, 117-118, 120, 122, 129-130, 132-133, 135, 142-143, 145, 153-154, 156157, 159, 166-167, 169-170, 178, providing us in poly-time with concrete cost-optimal plan
corresponding to the optimal solution for COP .
(II) We now prove that if  is solvable, then we must have  < . Assume to the contrary
that this is not the case. Let  be a solvable P(1) problem, and let (using Theorem 8) 
be an irreducible, post-3/2 plan for . Given such , let a COP assignment x be defined
as follows.
1. For each COP variable xv , the assignment x provides the value v   [(v)] such
that |v | = |v | + 1.
2. For each variable v  V , such that pred(v) 6= , find the (at most two) parents
that prevail the actions in v . Let w be such a parent that performs a role R 
{R1, R2, R3, R5, R6, R7, R8}, and w be the other such parent that performs one of
the roles R  {R4, R9, R10, R11}. (By definition of post-3/2 action sequences, the
rest of the parents all perform role R11.) Given that, if |pred(v)| = k > 0, we adopt an
k
xw
ordering of pred(v) such that w1 hh= w and wk = w . First, the assignment
v to COP
ii
|
|1
|
|1
v
v
k
variable xw
v provides the value S1,  2 ,  2 , |v |  1 . Then, for 1  i < k,
wi
i
the assignment xw
v to COP variable xv provides the value [S, #w , #b , |v |  1]], where


S2, R = R4



S4, R = R9
S=

S5, R = R10



S1, R = R11
and #w and #b are the numbers of actions in v that change the value of v to wv
and bv , respectively, while being prevailed by the value of w1 .

From Eq. 22-36 we then have that, for each v  V , if pred(v) = , then
P xv (xv ) = C(v ).
w
k
Otherwise, if pred(v) = {w1 , . . . , wk }, then xv (xv , xv ) = 0, and wpred(v) xwv (x ) =
C(v ). Therefore, we have
X
X
(x ) =
C(v ) = C(),
F

vV

which is what we had to prove.



References
Bacchus, F., & Yang, Q. (1994). Downward refinement and the efficiency of hierarchical
problem solving. Artificial Intelligence, 71 (1), 43100.
Backstrom, C., & Klein, I. (1991). Planning in polynomial time: The SAS-PUBS class.
Computational Intelligence, 7 (3), 181197.
286

fiTractable Cost-Optimal Planning

Backstrom, C., & Nebel, B. (1995). Complexity results for SAS+ planning. Computational
Intelligence, 11 (4), 625655.
Bonet, B., & Geffner, H. (2001). Planning as heuristic search. Artificial Intelligence, 129 (1
2), 533.
Boutilier, C., Brafman, R., Domshlak, C., Hoos, H., & Poole, D. (2004). CP-nets: A tool
for representing and reasoning about conditional ceteris paribus preference statements.
Journal of Artificial Intelligence Research, 21, 135191.
Brafman, R. I., & Domshlak, C. (2003). Structure and complexity of planning with unary
operators. Journal of Artificial Intelligence Research, 18, 315349.
Brafman, R. I., & Domshlak, C. (2006). Factored planning: How, when, and when not.
In Proceedings of the 18th National Conference on Artificial Intelligence (AAAI), pp.
809814, Boston, MA.
Bylander, T. (1994). The computational complexity of propositional STRIPS planning.
Artificial Intelligence, 69 (1-2), 165204.
Chapman, D. (1987). Planning for conjunctive goals. Artificial Intelligence, 32 (3), 333377.
Cormen, T. H., Leiserson, C. E., & Rivest, R. L. (1990). Introduction to Algorithms. MIT
Press.
Dechter, R. (2003). Constraint Processing. Morgan Kaufmann.
Edelkamp, S. (2001). Planning with pattern databases. In Proceedings of the European
Conference on Planning (ECP), pp. 1334.
Erol, K., Nau, D. S., & Subrahmanian, V. S. (1995). Complexity, decidability and undecidability results for domain-independent planning. Artificial Intelligence, Special Issue
on Planning, 76 (12), 7588.
Garey, M. R., & Johnson, D. S. (1978). Computers and Intractability: A Guide to the Theory
of NP-Completeness. W.H. Freeman and Company, New-York.
Gimenez, O., & Jonsson, A. (2008). The complexity of planning problems with simple
causal graphs. Journal of Artificial Intelligence Research, 31, 319351.
Haslum, P. (2006). Admissible Heuristics for Automated Planning. Ph.D. thesis, Linkoping
University, Department of Computer and Information Science.
Haslum, P., Bonet, B., & Geffner, H. (2005). New admissible heuristics for domainindependent planning. In Proceedings of the 20th National Conference on Artificial
Intelligence (AAAI), pp. 11631168, Pittsburgh, PA.
Haslum, P., & Geffner, H. (2000). Admissible heuristics for optimal planning. In Proceedings of the 15th International Conference on Artificial Intelligence Planning Systems
(AIPS), pp. 140149, Breckenridge, CO.
Helmert, M. (2003). Complexity results for standard benchmark domains in planning.
Artificial Intelligence, 146 (2), 219262.
Helmert, M. (2006). The Fast Downward planning system. Journal of Artificial Intelligence
Research, 26, 191246.
287

fiKatz & Domshlak

Hoffmann, J. (2003). Utilizing Problem Structure in Planning: A Local Search Approach.
No. 2854 in LNAI. Springer-Verlag.
Hoffmann, J., & Edelkamp, S. (2005). The deterministic part of IPC-4: An overview. Journal
of Artificial Intelligence Research, 24, 519579.
Hoffmann, J., & Nebel, B. (2001). The FF planning system: Fast plan generation through
heuristic search. Journal of Artificial Intelligence Research, 14, 253302.
Jonsson, P., & Backstrom, C. (1995). Incremental planning. In New Directions in AI
Planning: EWSP95-3rd European Workshop on Planning, pp. 7990, Assisi, Italy.
Jonsson, P., & Backstrom, C. (1998a). State-variable planning under structural restrictions:
Algorithms and complexity. Artificial Intelligence, 100 (12), 125176.
Jonsson, P., & Backstrom, C. (1998b). Tractable plan existence does not imply tractable
plan generation. Annals of Mathematics and Artificial Intelligence, 22 (3-4), 281296.
Kambhampati, S. (1995). Admissible pruning strategies based on plan minimality for planspace planning. In Proceedings of the 14th International Joint Conference on Artificial
Intelligence, pp. 16271635, Montreal, Canada.
Katz, M., & Domshlak, C. (2007). Structural patterns heuristics. In ICAPS-07 Workshop on Heuristics for Domain-independent Planning: Progress, Ideas, Limitations,
Challenges, Providence, RI.
Klein, I., Jonsson, P., & Backstrom, C. (1998). Efficient planning for a miniature assembly
line. Artificial Intelligence in Engineering, 13 (1), 6981.
Knoblock, C. (1994). Automatically generating abstractions for planning. Artificial Intelligence, 68 (2), 243302.
Newell, A., & Simon, H. A. (1963). GPS: A program that simulates human thought. In
Feigenbaum, E. A., & Feldman, J. (Eds.), Computers and Thought, pp. 279293.
Oldenbourg.
Sacerdoti, E. (1974). Planning in a hierarchy of abstraction spaces. Artificial Intelligence,
5, 115135.
Tenenberg, J. D. (1991). Abstraction in planning. In Allen, J. F., Kautz, H. A., Pelavin,
R. N., & Tenenberg, J. D. (Eds.), Reasoning About Plans, chap. 4, pp. 213283. Morgan Kaufmann.
Williams, B., & Nayak, P. (1996). A model-based approach to reactive self-configuring
systems. In Proceedings of the 13th National Conference on Artificial Intelligence
(AAAI), pp. 971977, Portland, OR.
Williams, B., & Nayak, P. (1997). A reactive planner for a model-based executive. In Proceedings of the 15th International Joint Conference on Artificial Intelligence (IJCAI),
pp. 11781185, Nagoya, Japan.

288

fiJournal of Artificial Intelligence Research 32 (2008) 487-523

Submitted 11/07; published 06/08

Refining the Execution of Abstract Actions
with Learned Action Models
Freek Stulp
Michael Beetz

stulp@cs.tum.edu
beetz@cs.tum.edu

Intelligent Autonomous Systems Group
Technische Universitat Munchen
Boltzmannstrae 3, D-85747 Garching bei Munchen, Germany

Abstract
Robots reason about abstract actions, such as go to position l, in order to decide what
to do or to generate plans for their intended course of action. The use of abstract actions
enables robots to employ small action libraries, which reduces the search space for decision
making. When executing the actions, however, the robot must tailor the abstract actions
to the specific task and situation context at hand.
In this article we propose a novel robot action execution system that learns success and
performance models for possible specializations of abstract actions. At execution time, the
robot uses these models to optimize the execution of abstract actions to the respective task
contexts. The robot can so use abstract actions for efficient reasoning, without compromising the performance of action execution. We show the impact of our action execution
model in three robotic domains and on two kinds of action execution problems: (1) the
instantiation of free action parameters to optimize the expected performance of action sequences; (2) the automatic introduction of additional subgoals to make action sequences
more reliable.

1. Introduction
In motor control, the main challenge is to map high-level goals and plans to low-level motor
commands. For instance, in the soccer scenario in Figure 1(a), what needs to be done at
an abstract level can be informally declared as: To achieve a scoring opportunity, first
approach the ball, and then dribble it towards the opponents goal. To actually execute
this abstract plan, the robot must map it to low-level motor commands, such as translational
and rotational velocities.
Using durative actions to bridge this gap has proven to be a successful approach in both
nature (Wolpert & Ghahramani, 2000) and robotics (Arkin, 1998). Durative parameterizable actions, or simply actions 1 , encapsulate knowledge about how certain goals can be
achieved in certain task contexts, and produce streams of motor command to achieve these
goals. For instance, human and robot soccer players will typically have dribbling, kicking,
and passing actions, that are only relevant in the context of soccer. Also, each of these actions achieves different goals within different soccer contexts. The abstract plan is executed
by mapping it to a sequence of actions. In the running example, the plan is mapped to the
action sequence approachBall, dribbleBall.
1. In cognitive science, actions are known as inverse models or control rules, and in robotics also as behaviors,
routines, policies, or controllers.

2008 AI Access Foundation. All rights reserved.

fiStulp & Beetz

(a) Abstract action chain.

(b) Suboptimal execution with an
abrupt transition.

(c) Time-optimal execution that
exhibits smooth motion.

Figure 1: An abstract action chain, and two alternative executions of this chain. The
center of each line on the trajectory represents the robots position. The lines are
drawn perpendicular to the robots orientation, and their width represents the
translational velocity at that point. These values are recorded at 10Hz.

Although these actions specify how to achieve the goal, there are often several ways to
execute them. Figure 1 depicts two executions of the same action sequence. In the first,
the robot naively executes the first action, and arrives at the ball with the goal at its back,
as depicted in Figure 1(b). This is an unfortunate position from which to start dribbling
towards the goal. An abrupt transition occurs between the actions, as the robot needs to
brake, and carefully maneuver itself behind the ball in the direction of the goal. Preferably,
the robot should go to the ball in order to dribble it towards the goal afterwards. The
robot should, as depicted in Figure 1(c), perform the first action sub-optimally in order
to achieve a much better position for executing the second action. In this article, the
performance measure is execution duration. The behavior shown in Figure 1(c) is defined
to be optimal, as it minimizes the time needed to execute the action sequence.
The reason why such suboptimal behavior is often witnessed in robots is that robot
planners, but also designers of robot controllers, view actions at a level of abstraction that
ignores the subtle differences between actions. Abstracting away from action details is
essential to keep action selection programming and planning tractable. However, because
the planning system considers actions as black boxes with performance independent of the
prior and subsequent steps, the planning system cannot tailor the actions to the execution
contexts. This often yields suboptimal behavior with abrupt transitions between actions,
as in Figure 1(b). In this example, the problem is that in the abstract view of the planner,
being at the ball is considered sufficient for dribbling the ball and the dynamical state of the
robot arriving at the ball is considered to be irrelevant for the dribbling action. Whereas
the angle of approach is not relevant to the validity of the plan and therefore free to choose,
it must be reasoned about in order to optimize plan execution.
So, given an abstract plan, what is the concrete action sequence that is predicted to
have minimal total cost (execution duration) and most likely succeed? This problem can be
broken down into three subproblems: 1) How can cost and success be predicted? 2) How
can action sequences be optimized to have minimal cost? 3) How can sequences of actions
be transformed to increase successful execution?
488

fiRefining the Execution of Abstract Actions with Learned Action Models

We take inspiration from findings in cognitive science (Wolpert & Flanagan, 2001),
and solve the first subproblem by acquiring and applying a third kind of motor control
knowledge: being able to predict the outcome and performance of actions. In the running
example, if the robot could predict the performance of alternative executions beforehand,
it could choose and commit to the fastest execution. To predict the execution duration
of action sequences, the robot must predict the execution duration of individual actions.
The robot can learn these action models through experimentation, observation and generalization. It does so by observing executions of actions with various parameterizations, and
learning general models from them with tree-based induction.
We have implemented our approach on a diversity of real and simulated robotic platforms, which will be introduced in the next section. Afterwards, we discuss related work in
Section 3. The following four sections are organized according to the flowchart in Figure 2.
The representation of (abstract) actions and action models are described in Section 4, along
with the generation of abstract plans. We explain how action models are learned from observed experience in Section 5. The first applications of action models, subgoal refinement,
is described in Section 6. In Section 7, we present subgoal assertion, which transforms plans
that are predicted to fail into successful plans. Throughout the article, we will describe the
representations and algorithms used to implement the flowchart in Figure 2. Finally, we
conclude with a summary and outlook on future work in Section 8.

Figure 2: Outline of the article as a flowchart of the overall system.

2. Robotic Domains
In this article, robots learn and apply action models in three domains: robotic soccer,
service robotics and arm control. Such a variety of robots and domains has been chosen to
emphasize the generality of the approach. Also, the different characteristics of the domains
allow different aspects of action model applications to be investigated.
The first domain is robotic soccer (Kitano, Asada, Kuniyoshi, Noda, & Osawa, 1997).
In this adversarial domain, performance and efficiency are essential to achieving the goals
of the team. Tailoring actions to perform well within a given task context is therefore a
necessity. We use the customized Pioneer I robots of our mid-size league team the Agilo
RoboCuppers (Beetz, Schmitt, Hanek, Buck, Stulp, Schroter, & Radig, 2004), one of which
is depicted in Figure 3(a). These robots have differential drive, and localize themselves with
a single forward facing camera. Experiments in this domain were conducted on the real
robots, as well as in simulation (Buck, Beetz, & Schmitt, 2002a).
To test our approach on robots with more degrees of freedom, operating in richer environments, we have included an articulated B21 robot in a simulated kitchen environ489

fiStulp & Beetz

(a) Agilo soccer robot.

(b) B21 in the kitchen environment.

(c) PowerCube arm.

Figure 3: The three robotic domains considered in this article.
ment (Muller & Beetz, 2006). Household tasks are also less reactive than robotic soccer,
and require more complex and long-term planning, which is relevant in the context of the
action sequence optimization presented in Section 6. The simulator is based on the Gazebo
simulator of the Player Project (Gerkey, Vaughan, & Howard, 2003). The environment,
depicted in Figure 3(b) is a typical kitchen scenario, with furniture, appliances, cups and
cutlery.
The scenarios in this domain are related to fetching and delivering cups from one place
to another. To do so, the robot has navigation actions, and actions for reaching for the cup,
or putting it down.
Finally, we evaluate our approach on an articulated manipulator, which performs reaching movements. The experiments were performed with a PowerCube arm from Amtec
Robotics with 6 degrees of freedom, shown in Figure 3(c). For the experiments, only two
joints were used.

3. Related Work
In this section, we discuss work related to learning predictive models of actions, optimal
and hybrid control, as well as learning preconditions.
3.1 Learning Predictive Models of Actions
Jordan and Rumelhart (1992) introduced Distal Learning, which explicitly learns forward
models with neural networks. These action models are used to bridge the gap between
distal target values and motor commands, which enables motor control learning. Recently,
robotic forward models have also been learned using Bayesian networks (Dearden & Demiris,
2005). Infantes, Ingrand, and Ghallab (2006) present recent work that also includes the use
of dynamic Bayesian networks. This work extends the approach by Fox, Ghallab, Infantes,
and Long (2006), in which action models are learned with Hidden Markov Models. In
general, the advantage of Bayesian networks is that they allow the causal nature of a
robots control system to be modelled using a probabilistic framework. However, they are
more appropriate for predicting the outcome of a motor command in the next time step,
rather than the expected performance of a durative action.
490

fiRefining the Execution of Abstract Actions with Learned Action Models

The Modular Selection And Identification for Control (MOSAIC) architecture (Haruno,
Wolpert, & Kawato, 2001) also integrates forward models into a computational model for
motor control. This framework is intended to model two problems that humans must solve:
how to learn inverse models for tasks, and how to select the appropriate inverse model,
given a certain task. MOSAIC uses multiple pairs of forward and inverse models to do so.
This architecture has not been designed for robot control. We are not aware of (robotic)
controllers in which action models are an integral and central part of the computational
model, and which are acquired automatically, represented explicitly, and used as modular
resources for different kinds of control problems.
Haigh (1998) and Belker (2004) also developed robots that optimize their high-level
plans with action models. These models are used to determine the best path in a hallway
environment. Our approach rather focuses on optimizing the parameterization of subgoals
at fixed points in the plan, and is not limited to navigation plans or tasks. As the authors
report good experiences with tree-based induction for learning robot action models from
observed data, we have also chosen this learning algorithm.
3.2 Optimal Control
Optimal control refers to the use of online, optimal trajectory generation as a part of the
feedback stabilization of a typically nonlinear system (Astrom & Murray, 2008). Receding
horizon control is a subclass of optimal control approaches, in which an (optimal) trajectory
is planned up to a state x(t + Hp ) which lies between between the current state x(t) and
goal state xgoal (Astrom & Murray, 2008). After planning the next Hp steps, He steps
of this trajectory are executed (with 1  He  Hp ), and a new trajectory is computed
from the new current state x(t + He ) to x(t + He + Hp ). An example of RHC is the
work of Bellingham, Richards, and How (2002), who apply RHC to simulated autonomous
aerial vehicles. The rationale behind receding horizon control (RHC) is that there is a
diminishing return in optimizing later parts of the trajectory before beginning execution.
The experiments described in Section 6.3.1 verify this effect. A comparison between RHC
and our methods will also follow in Section 6.3.1.
Todorov, Li, and Pan (2005) take a hierarchical approach to optimal control. A highlevel controller uses a more abstract state and command representation, to control a lowlevel controller, which in its turn controls the physical plant. The main difference with
our work is that the purpose of the low-level controller is not to solve a specific subtask,
as do our actions, but rather to perform an instantaneous feedback transformation as an
abstraction for more high-level controllers.
Using redundant degrees of freedom to optimize subordinate criteria has been well studied in the context of arm control, both in humans (Schaal & Schweighofer, 2005; Wolpert
& Ghahramani, 2000; Uno, Wolpert, Kawato, & Suzuki, 1989) and robots (Simmons &
Demiris, 2004; Nakanishi, Cory, Mistry, Peters, & Schaal, 2005; Bertram, Kuffner, Dillmann, & Asfour, 2006). Arm poses are said to be redundant if there are many arm configurations that result in an equivalent pose. In the work cited above, all these configurations
are called uncontrolled manifold, motion space, or null space, and finding the best configuration is called redundancy resolution or null-space optimization. In the approaches above,
optimization is performed analytically, and specific to the arm control domain. Learning
491

fiStulp & Beetz

the models from observed experience enables whole range of different robots to apply them
to a variety tasks.
Smooth motion also arises if the control signals of several actions are combined using a
weighted interpolation scheme. Such motion blending approaches are found in robotics (Utz,
Kraetzschmar, Mayer, & Palm, 2005; Jaeger & Christaller, 1998; Saffiotti, Ruspini, & Konolige, 1993), as well as computer graphics (Perlin, 1995; Kovar & Gleicher, 2003). Since there
are no discrete transitions between actions, they are not visible in the execution. Hoffmann
and Duffert (2004) propose another method to generate smooth transitions between different gaits of quadruped robots, based on smoothing signals in the frequency domain. Since
the actions we use are not periodic, these methods do not apply. In all these approaches, it is
assumed that smooth motion will lead to a better performance. However, achieving optimal
behavior is not an explicit goal, and no objective performance measures are optimized.

3.3 Reinforcement Learning
Reinforcement Learning (RL) is another method that seeks to optimize performance, specified by a reward function. RL approaches most often model the optimal control problem
as a Markov Decision Process (MDP), which is usually solved with dynamic programming.
Recent attempts to combat the curse of dimensionality in RL have turned to principled
ways of exploiting temporal abstraction (Barto & Mahadevan, 2003). In a sense, Hierarchical Reinforcement Learning algorithms also learn action models, as the Q-value predicts
the future reward of executing an action. Note that Q-values are learned for one specific
environment, with one specific reward function. The values are learned for all states, but
for a single goal function. Our action models are more general, as they describe the action
independent of the environment, or the context in which they are called. Therefore, action
models can be transfered to other task contexts. Haigh (1998) draws the same conclusion
when comparing action models with RL. In our view, our action models also provide more
informative performance measures with a physical meaning, such as execution time, and
scale better to continuous and complex state spaces.
The only approach we know of that explicitly combines planning and Reinforcement
Learning is RL-TOPS (Reinforcement Learning - Teleo Operators) (Ryan & Pendrith, 1998).
In this approach, sequences of actions are first generated based on their preconditions
and effects, using Prolog. Reinforcement Learning within this action sequence is done
with HSMQ (Dietterich, 2000). Between actions, abrupt transitions arise too, and the
author recognizes that cutting corners would improve performance, but does not present
a solution.
Most similar to our work, from the point of view of smoothness as an emergent property
of optimality requirements with redundant subgoals, is the approach of Kollar and Roy
(2006). In this work, a simulated robot maps its environment with range measurements
by traversing a set of waypoints. A policy that minimizes the error in the resulting map
is learned with RL. As a side-effect, smooth transitions at waypoints arise. This approach
has not been tested on real robots.
492

fiRefining the Execution of Abstract Actions with Learned Action Models

3.4 Hybrid Control
Problems involving choice of actions and action chains are often regarded as planning problems. However, most planning systems do not aim at optimizing resources, such as time.
While scheduling systems are better at representing time constraints and resources, most
could not deal with the selection of actions in this problem. Systems that integrate planning
and scheduling, such as the work by Smith, Frank, and Jonsson (2000), are able to optimize
resources, but ignore interactions between actions and intermediate dynamical states. That
is why they do not apply well to continuous domain problems.
In PDDL (Fox & Long, 2003), resource consumption of actions can be represented at
an abstract level. Planners can take these resources into account when generating plans. In
contrast to such planners, our system generates action sequences that are optimized with
respect to very realistic, non-linear, continuous performance models, which are grounded in
the real world as they are learned from observed experience.
Other recent approaches to using symbolic planning for robots focus on different problems that arise when such plans are executed on real robots. For instance, Bouguerra and
Karlsson (2005) use probabilistic planners in the abstract planning domain, which enables
them to exploit uncertainties determined in probabilistic state estimation. aSyMov reasons
about geometric preconditions and consequences of actions in a simulated 3-D world (Cambon, Gravot, & Alami, 2004). Note that our methods are complementary rather than
incompatible those described by Bouguerra and Karlsson (2005) and Cambon et al. (2004),
and merging them would combine their advantages. Cambon et al. (2004) mention that the
resulting plan is improved and optimized in some way, but does not describe how. In probabilistic motion planning, such a post-processing step for smoothing the generated paths
is a common procedure. Subgoal refinements might well be integrated in this optimization
step.
Belker (2004) uses action models learned with model trees to optimize Hierarchical
Transition Network (HTN) plans. HTN plans are structured hierarchically from high level
goals to the most low level commands. To optimize performance, the order of actions, or
the actions themselves are changed at varying levels of the hierarchy. Rather than refining
plans, the system modifies the HTN plans themselves, and therefore applies to HTN plans
only. On the other hand, we refine an existing action chain, so the planner can be selected
independently of the optimization process
Generating collision-free paths from an initial to a final robot configuration is also known
as robot motion planning. A common distinction between algorithms that generate such
paths is between global, local and hybrid approaches (Brock & Khatib, 1999). The approach presented in this article is a global approach, as the planning is performed offline.
One disadvantage of offline algorithms is that there is no guarantee that the planned trajectory will actually succeed on execution. With respect to robotic motion planning, movable
obstacles or unforeseen dynamics are just two examples why correctly planned trajectories
can fail. The same holds for all global approaches, of which the work by Bouguerra and
Karlsson (2005), Cambon et al. (2004) are examples discussed previously. Hybrid motion
planning approaches, compute and commit plans offline, but leave freedom in the plans
to react to unforeseen circumstances (Brock & Khatib, 1999). Whereas in hybrid motion
planning approaches the freedom between the subgoals considered, subgoal refinement con493

fiStulp & Beetz

siders the freedom at the subgoal itself. Note that this implies that hybrid approaches are
not incompatible with subgoal refinement, and they might complement each other well.
3.5 Learning Preconditions
In Section 7, we demonstrate how failure action models, which predict whether an action will
fail or succeed, are learned. This is similar to learning preconditions of actions. In most
of the research on learning preconditions, the concept that is being induced is symbolic.
Furthermore, the examples consist only of symbols that are not grounded in the real world.
The precondition is then learned from these examples, for instance through Inductive Logic
Programming (Benson, 1995) or more specialized methods of logic inference (Shahaf &
Amir, 2006; Clement, Durfee, & Barrett, 2007). These approaches have not been applied to
robots. We believe this is because symbolic representations only do not suffice to encapsulate
the complex conditions that arise from robot dynamics and action parameterizations. Such
models are best learned from experience, as described in Section 5.
Schmill, Oates, and Cohen (2000) present a system in which non-symbolic planning
operators are learned from actual interaction of the robot with the real world. The experiences of the robot are first partitioned into qualitatively different outcome classes, through
a clustering approach. The learned operators are very similar to previously hand-coded
operators. Once these classes are known, the robot learns to map sensory features to these
classes with a decision tree, similar to our approach. This approach aims at learning to predict what the robot will perceive after executing an action from scratch, whereas condition
refinement aims at refining an already existing symbolic preconditions based on changing
goals.
Buck and Riedmiller (2000) propose a method for learning the success rate of passing
actions in the context of RoboCup simulation league. Here a neural network is trained with
8000 examples in which a pass is attempted, along with the success or failure of the pass.
This information is used to manually code action selection rules such as Only attempt a
pass if it is expected to succeed with probability > 0.7. This is also a good example of
integrating human-specified and learned knowledge in a controller.
Sussman (1973) was the first to realize that bugs in plans do not just lead to failure,
but are actually an opportunity to construct improved and more robust plans. Although
this research was done in the highly abstract symbolic blocks world domain, this idea is
still fundamental to transformational planning. In the XFRMLearn framework proposed by
Beetz and Belker (2000), human-specified declarative knowledge is combined with robotlearned knowledge. Navigation plans are optimized with respect to execution time by
analyzing, transforming and testing structured reactive controllers (SRCs) (Beetz, 2001).
One difference with XFRMLearn is that in our work, the analysis phase is learned instead of
human-specified. Another difference is that XFRMLearn improves existing plans, whereas
condition refinement learns how to adapt to changing action requirements, such as refined
goals.

4. Conceptualization
Our conceptualization for the computational problem is based on the dynamic system
model (Dean & Wellmann, 1991). In this model, the world changes through the inter494

fiRefining the Execution of Abstract Actions with Learned Action Models

action of two processes: the controlled process and the controlling process. The controlled
process is essentially the environment, including the body and sensors of the robot. In the
dynamic system model, it is assumed that the environment can be described by a set of
state variables.
The controlling process performs two tasks. First, it uses state estimation to estimate
the values of the state variables in the environment, based on the percepts that the sensors
provide. The observable state variables are encapsulated in the belief state. Second, the
controller takes a belief state as input, and determines appropriate motor commands that
direct the state variables to certain target values. These motor commands are dispatched
to the motor system of the robot in the environment. For each robotic domain, the state
estimation methods, the state variables in the belief state, and motor commands are listed
in Table 5 in Appendix A. The rest of this section will focus on the concepts used inside
the controller.
4.1 Actions and Action Libraries
Actions are control programs that produce streams of motor commands, based on the
current belief state. In robotics, actions usually take parameters that allow them to be
used in a wide range of situations. Instead of programming an action dribbleBallToCenter, it is preferable to program an action dribbleBall(Pose) that can dribble the ball
to any location on the field, including the center. Because actions only apply to certain task
contexts, they are easier to design or learn than a controller that must be able to deal with
all possible contexts (Haruno, Wolpert, & Kawato, 1999; Jacobs & Jordan, 1993; Ginsberg,
1989).
One of the actions used is goToPose, which navigates the robot from the current state
[x,y,,v] to a future target state [xg ,yg ,g ,vg ] by setting the translational and rotational
velocity [v,] of the robot. Table 6 in Appendix A lists the actions used by the different
robots. The action parameters are separated into the values of variables in the current state
and target values for those variables. The first are the so-called observable state variables,
whereas the second exist only internally in the controller. As action models are learned from
observed experience, learning and applying them is independent of action implementations.
We do not describe the hand-coded implementations here, but rather refer to Stulp (2007,
Appendix A).
Action libraries contain a set of actions that are frequently used within a given domain.
If each action is designed to cover a large set of tasks, usually only a small set of actions
is needed to achieve most tasks in a given domain. Having only a few actions has several
advantages: 1) The controller is less complex, making it more robust. 2) Fewer interactions
between actions need to be considered, facilitating action selection design and autonomous
planning. 3) If the environment changes, only a few actions need to be redesigned or
relearned, making the system more adaptive, and easier to maintain.
In this article, we assume that actions are innate, and do not change over time. They
are the building blocks that are combined and concatenated to solve tasks that single action
could not achieve alone. Actions themselves are never modified or optimized in any way.
Rather, their parameters are chosen such that their expected performance is optimal within
a given task context.
495

fiStulp & Beetz

4.2 Action Models
Action models predict the performance or outcome of an action. Before an action is executed with certain parameters, the corresponding action model can be called with the same
parameters, for instance to predict the execution duration. Action models will be discussed
in more detail in Section 5.
4.3 Abstract Actions
To achieve more complex tasks, actions are combined and concatenated. There is a long
tradition in using symbolic planners to generate abstract action chains for robot control.
Shakey, one of the first autonomous mobile robots, used symbolic representations to determine action sequences that would achieve its goal (Nilsson, 1984). More recent examples
include the work of Coradeschi and Saffiotti (2001), Beetz (2001), Cambon et al. (2004)
and Bouguerra and Karlsson (2005). One of the main reasons why symbolic planning and
abstract actions are of interest to robotics is that they abstract away from many aspects of
the continuous belief state, so planning and replanning are faster, and more complex problems can be dealt with. Planners are good at fixing the general abstract structure of the
task. Also, action sequences or action hierarchies must not be specified in advance, but are
generated online, depending on the situation at hand, making the system more adaptive.
Furthermore, robots can reason about plans offline before execution, to recognize and repair failures in advance (Beetz, 2000), which is preferable to encountering them during task
execution. Finally, symbolic planning for robotics enables designers to specify constraints
on actions symbolically (Cambon et al., 2004).
Abstract actions consist of the preconditions and effects of an action (Nilsson, 1994).
They constitute the controllers abstract declarative knowledge. The effects represent the
intended goal of the action. It specifies a region in the state space in which the goal is
satisfied. The precondition region with respect to a temporally extended action is defined
as the set of world states in which continuous execution of the action will eventually satisfy
the effects. Figure 4 shows two abstract actions, along with their preconditions and effects.

Figure 4: An abstract action chain consisting of two abstract actions.

In the system implementation, the Planning Domain Description Language (PDDL2.1)
is used to describe abstract actions, goals, states and plans (Fox & Long, 2003). The declarative knowledge about the preconditions and effects (add- and delete-list) of the abstract
actions in the action library are specified manually by the designer. The PDDL planner we
use is the Versatile Heuristic Partial Order Planner (Younes & Simmons, 2003)2 . VHPOP is,
as all PDDL planners, a general purpose planner not specifically tailored to robot planning.
2. VHPOP can be downloaded free of cost at http://www.tempastic.org/vhpop/

496

fiRefining the Execution of Abstract Actions with Learned Action Models

Other work focusses on problems that need to be solved to exploit symbolic plannning in
robotics, such as uncertainty, failure recovery and action monitoring (Bouguerra & Karlsson,
2005), geometric constraints (Cambon et al., 2004), and anchoring (Coradeschi & Saffiotti,
2001). The system presented in this section abstracts away from these problems to focus on
the main contribution: the optimization of already generated plans. The output of VHPOP
is a list of symbolic actions and causal links, of which we shall see an example in Figure 10.
4.4 System Overview
Subgoal refinement and subgoal assertion both operate on sequences of actions. In this
article, these action sequences are derived from abstract action chains, which are generated
by a symbolic planner. A general computational model of robot control based on symbolic
plans is depicted in Figure 5, and is similar to the models proposed by Bouguerra and
Karlsson (2005) and Cambon et al. (2004).
The goal is passed as an input to the system. Usually, the abstract state is derived from
the belief state through a process called anchoring (Coradeschi & Saffiotti, 2001). As in the
International Planning Competition, we consider a limited number of scenarios, enabling
us to specify the scenarios in PDDL in advance. The name of the scenario is contained in
the belief state, and the corresponding abstract state is read from a PDDL file.

Figure 5: Computational model of robot control based on symbolic plans.

: pddl goal, (represented in PDDL)
beliefState, (belief state with state variables)
action lib (library with PDDL representations, actions, and action models)
output : exe action seq (an optimized sequences of executable actions)
input

1
2
3
4
5
6

pddl state = readFromFile (beliefState.scenario name) // Anchoring
pddl plan = makePlan (pddl state, pddl goal, action lib.pddl) // VHPOP
exe action seq = instantiateAction (pddl plan, belief state, action lib.actions) // Section 6.1
exe action seq = assertSubgoals (exe action seq,belief state, action lib) // Section 7
exe action seq = refineSubgoals (exe action seq, action lib.models) // Section 6.2
return exe action seq;

Algorithm 1: System Overview.
497

fiStulp & Beetz

The pseudo-code for the complete system described in this article is listed in Algorithm 1.
Data structures from the abstract declarative planning domain (see Figure 5) have the
prefix pddl . In this section, we have presented the implementations of the functions
in lines 1 and 2. In the next section, we will describe how the action models used by
subgoal refinement and assertion (line 4 and 5) are learned from observed experience. The
implementations of the functions in lines 3 to 5 are presented in Section 6 and 7. Note
that subgoal refinement and assertion only modify existing action sequences. They do not
interfere with the planning or instantiation process. This means that they are compatible
with other planning systems.
Before action models can be applied online during task execution, they must be learned.
This learning phase is described in the next section.

5. Learning Action Models
In this section, we describe how robots learn action models from observed experience. The
advantage of learning action models over analytical methods is that 1) it is based on real
experience, and therefore takes all factors relevant to performance into account. 2) many
(hand-coded) actions are difficult to formalize analytically (Beetz & Belker, 2000). For
instance, the exact interactions of the ball with the guide rail are difficult to predict with
model-predictive control, as it is the result of the subtle interplay of many complex factors,
such as the robots dynamics, sensing capabilities, the smoothness of the ball and guide rail,
parameterizations of the control program, etc. All these factors must be modeled in order
to make accurate predictions. 3) analysis is sometimes impossible because the implementation the action are unknown, for instance when they have been learned. 4) although not
implemented in our work, learned action models can also adapt to changing environments
if trained online (Dearden & Demiris, 2005; Kirsch & Beetz, 2007). 5) it enables robots to
learn models for other robots, by observing their behavior, as demonstrated by Stulp, Isik,
and Beetz (2006).
5.1 Data Acquisition
Training examples are gathered by executing an action, and observing the results. In this
article, robots record and learn to predict the execution duration and success of actions,
given the parameterization of the action. To generate training data for an action, parameter
values for the initial and goal state are sampled uniformly from the possible values the action
parameters can take. The possible parameter value ranges depend on the preconditions and
effects of an action, and are specified semi-automatically. The user defines ranges for the
action parameters that ensure that the preconditions and effects of the action are met, and
the action parameters used are uniformly sampled from these ranges. The execution of an
action from an initial to a goal state is called an episode.
The running example in this section will be learning to predict the execution duration
of the goToPose action for the simulated B21 and the Agilo robot. Figure 6(a) displays a
concrete example of gathered training data with the simulated B21 robot. Here, 30 of 2948
executions of goToPose with random initial and goal states are shown. The total number of
executions is denoted with ne . For instance, ne =2948 for the example above. We split this
498

fiRefining the Execution of Abstract Actions with Learned Action Models

data into a training and test set. The number of examples in the training set is N = 43 ne ,
which in the current example yields 2200 episodes.

(a) Visualization of 30
of 2948 goToPose executions.

(b) The original state space, and two derived feature spaces. The top figures
depict the features used. The lower graphs plot the time remaining until
reaching the goal against one of these features.

Figure 6: Gathering and transforming experience for goToPose in the kitchen domain.
Will a learning algorithm trained with these 2200 examples likely make erroneous predictions on previously unseen cases? In general, a hypothesis that is consistent with a
sufficiently large training set is deemed probably approximately correct (PAC) (Russell &
Norvig, 2003). To obtain a model that has an error of at most  with probability 1   (i.e.
is PAC), the learning algorithm must be trained with at least N  1 (ln 1 + ln|H|) training
examples, in which |H| is the number of possible hypotheses. We do not use this formula to
exactly compute the number of training examples needed, but rather to determine strategies
to learn more accurate models with a limited amount of costly training episodes. We use
three approaches:
Reduce the number of hypotheses |H|. By exploiting invariances, we can map
the data from the original direct state space to a lower-dimensional derived feature space.
This limits the number of possible hypotheses |H|. For navigation actions for instance, the
translational and rotational invariances in the original 8-D state space (listed in Table 6)
are exploited to transform it to a 5-D features space, depicted in Figure 6(b). The features
in this 5-D state space correlate better with the performance measure. Haigh (1998) calls
such features projective.
By exploiting the invariances, we are reducing the dimensionality of the feature space,
which leads to a decrease of |H|. This equation specifies that with lower |H|, fewer training
examples are needed to learn a PAC model. By the same reasoning, more accurate models
(i.e. lower ) can be learned on lower dimensional feature spaces, given the same amount of
data.
We have experimentally verified this by training the model tree learning algorithm (to
be presented in Section 5.2) with data mapped to each of the three different feature spaces
499

fiStulp & Beetz

in Figure 6(b). For each feature space, the model is trained with N =2200 of the ne =2948
executed episodes. The Mean Absolute Error (MAE) of each of these models is determined
on the separate test containing the remaining episodes3 . As can be seen in Figure 6(b),
the MAE is lower when lower dimensional feature spaces are used. Of course, this lower
dimensionality should not be achieved by simply discarding informative features, but rather
by composing features into projective features by exploiting invariances.
Increase the number of training examples N by including intermediate examples. Instead of using only the first initial example of each episode (large red dots
in Figure 6(b)), we could also include all the intermediate examples until the goal state.
As these are recorded at 10Hz, this yields many more examples, and higher N . However,
one characteristic of projective features is that they pass through the point of origin, as
can be seen in the right-most graph in Figure 6(b). Therefore, including all intermediate
examples will lead to a distribution that is skewed towards the origin. This violates the
stationarity assumption, which poses that the training and test set must be sampled from
the same probability distribution. Most learning algorithms trained with this abundance
of data around the origin will be biased towards states that are close to the goal, and will
tend to predict these states very accurately, at the cost of inaccurate prediction of states
further from the goal.
Therefore, we include only the first ni examples of each episode. This means that the
number of training examples is roughly ne  ni instead of just ne , but still represents the
original distribution of initial states. Since the best value of ni is not clear analytically, we
determine it empirically, as described by Stulp (2007)
Track the error  empirically. Instead of defining  in advance, we compute the
Mean Absolute Error (MAE) over time as more data is gathered, and determine when it
stabilizes visually. At this point, we assume that N is sufficiently large, and stop gathering
data.
5.2 Tree-based Induction
Learning algorithms that are used for learning robot action models from observed experience
include neural networks (Buck, Beetz, & Schmitt, 2002b), and tree-based induction (Belker,
2004; Haigh, 1998). We have shown that there is no significant difference in the accuracy
of action models learned with neural networks or tree-based induction (Stulp et al., 2006).
However, decision and model trees have the advantage that they can be converted into sets
of rules, which can then be visually inspected. Furthermore, model trees can be optimized
analytically as described in Section 6.2. For these reasons, we will focus only on decision
and model trees in this article.
Decision trees are functions that map continuous or nominal input features to a nominal
output value. The function is learned from examples by a piecewise partitioning of the
feature space. One class is chosen to represent the data in each partition. Model trees are a
generalization of decision trees, in which the nominal values at the leaf nodes are replaced
by line segments. A line is fitted to the data in each partition with linear regression.
3. We prefer the MAE over the Root Mean Square Error (RMSE), as it is more intuitive to understand,
and the cost of a prediction error is roughly proportional to the size of the error. There is no need to
weight larger errors more.

500

fiRefining the Execution of Abstract Actions with Learned Action Models

This linear function interpolates between data in the partition. This enables model trees
to approximate continuous functions. For more information on decision and model trees,
and how they are learned with tree-based induction, we refer to (Quinlan, 1992; Stulp
et al., 2006; Wimmer, Stulp, Pietzsch, & Radig, 2008). In our implementation, we use
WEKA (Witten & Frank, 2005) to learn decision and model trees, and convert its output
to a C++ function.
5.3 Action Model: Execution Duration Prediction
To visualize action models learned with model trees, an example of execution duration
prediction for a specific situation is depicted in Figure 7. This model for the goToPose
action in the soccer domain (real robots) is learned from ne =386 episodes. The first ni =20
examples per episode are used. The features used are dist, angle to, angle at, v and vg , as
depicted in Figure 6(b).
In the situation depicted in Figure 6(b), the variables dist, angle to, v, and vg are set
to 1.5m, 0 , 0m/s, and 0m/s respectively. The model is much more general, and predicts
accurate values for any dist, angle to, v, and vg ; these variables are fixed for visualization
purposes only. For these fixed values, Figure 7 shows how the predicted time depends on
angle at, once in a Cartesian, once in a polar coordinate system.

Figure 7: An example situation, two graphs of time prediction for this situation with varying
angle at, and the model tree rule for one of the line segments.

In the Cartesian plot there are five line segments. This means that the model tree has
partitioned the feature space for dist=1.5m angle to=0 , v=0m/s, and vg =0m/s into five
areas, each with its own linear model. Below the two plots, one of the learned model tree
rules that applies to this situation is displayed. Arrows in the graphs indicate the linear
model that corresponds to this rule.
501

fiStulp & Beetz

5.3.1 Empirical Evaluation
The different domains and actions for which action models of execution duration are learned
are listed in the first two columns of Table 1. The subsequent columns list the number of
episodes executed to gather data for the training set 34 ne , the mean execution duration per
episode t, the total duration of data gathering for the training set t  34 ne , as well as the
models error (MAE) on a separate test set with the remaining 41 ne episodes. Note that
the final model stored in the action library is trained with data from all ne episodes. In
the next sections, we demonstrate that these action models are accurate enough to enable
a significant improvement of action execution.
Domain

Action

3
4 ne

Soccer
(real)
Soccer
(simulated)
Kitchen

goToPose
dribbleBall
goToPose
dribbleBall
goToPose
reach
reach

290
202
750
750
2200
2200
1100

Arm control

t
(s)
6.4
7.7
6.2
7.4
9.0
2.6
2.9

t  34 ne
(h:mm)
0:31
0:26
1:18
1:32
5:45
1:38
0:53

MAE
(s)
0.32
0.43
0.22
0.29
0.52
0.10
0.21

Table 1: List of actions and their action model statistics.

5.4 Action Model: Failure Prediction
The simulated soccer robots also learn to predict failures in approaching the ball with the
goToPose action. A failure occurs if the robot collides with the ball before the desired state
is achieved. The robots learn to predict these failures from observed experience. To acquire
experience, the robot executes goToPose a thousand times, with random initial and goal
states. The ball is always positioned at the destination state. The initial and goal state
are stored, along with a flag that is set to Fail if the robot collided with the ball before
reaching its desired position and orientation, and to Success otherwise. The feature space
is the same as for learning the temporal prediction model of goToPose, as listed in Table 7.
The learned tree, and a graphical representation, are depicted in Figure 8. The goal
state is represented by the robot, and different areas indicate if the robot can reach this
position with goToPose without bumping into the ball first. Remember that goToPose has
no awareness of the ball at all. The model simply predicts when its execution leads to
a collision or not. Intuitively, the rules seem correct. When coming from the right, for
instance, the robot always clumsily stumbles into the ball, long before reaching the desired
orientation. Approaching the ball is fine from any state in the green area labeled S.
5.4.1 Empirical Evaluation
To evaluate the accuracy of this model, the robot executes another thousand runs. The
resulting confusion matrix is depicted in Table 2. The decision tree predicts collisions
correctly in almost 90% of the cases. The model is quite pessimistic, as it predicts failure
502

fiRefining the Execution of Abstract Actions with Learned Action Models

Figure 8: The learned decision tree that predicts whether a collision will occur.

61%, whereas in reality it is only 52%. In 10% of cases, it predicts a collision when it
actually does not happen. This is preferable to an optimistic model, as it is better to be
safe than sorry. This pessimism is actually no coincidence; it is caused because a cost matrix
that penalizes incorrect classification of Fail more than it does Success is passed to the
decision tree (Witten & Frank, 2005).

Predicted

Fail
Success

Total Observed

Observed
Fail Success
51%
10%
1%
38%


52%
48%

Total
Predicted
 61%
 39%

89%

Table 2: Confusion matrix for ball collision prediction.

6. Subgoal Refinement
When it comes to elegant motion, robots do not have a good reputation. Jagged movements are actually so typical of robots that people trying to imitate robots will do so by
executing movements with abrupt transitions between them. Figure 1(b) demonstrates an
abrupt transition that arises when approaching the ball to dribble it to a certain location.
Such jagged motion is not just inefficient and aesthetically displeasing, but also reveals a
fundamental problem that inevitably arises from the way robot controllers and actions are
designed and reasoned about.
Figure 9(a) depicts the abstract action chain of the scenario, with preconditions and
effects represented as subsets of the entire state space. Note that the intersection of preconditions and effects at the intermediate subgoal contain many intermediate states, eight
of which are also depicted in the scenario in Figure 9(a). In this set, all variables are equal,
except the angle with which the ball is approached. This action parameter is therefore
called free.
503

fiStulp & Beetz

As discussed in Section 1, the reason for abrupt transitions is that although the preconditions and effects of abstract actions justly abstract away from action parameters that do not
influence the high-level selection of actions, these same free parameters might influence the
performance of actually executing them. For instance, the angle of approach is abstracted
away when selecting the actions, although it obviously influences the execution duration.
The first step in subgoal refinement is therefore to determine free action parameters in a
sequence of abstract actions.

(a) Abstract action chain before subgoal refinement.

(b) Abstract action chain optimized with subgoal refinement.

Figure 9: Computational model of subgoal refinement.
In contrast to robot motion, one of the impressive capabilities of animals and humans
is their ability to perform sequences of actions efficiently, and with seamless transitions between subsequent actions. It is assumed that these typical patterns are those that minimize
a certain cost function (Wolpert & Ghahramani, 2000; Schaal & Schweighofer, 2005). So,
in nature, fluency of motion is not a goal in itself, but rather an emergent property of time,
energy and accuracy optimization.
Subgoal refinement exploits free action parameters in a similar way. Since all the states
in the intermediate set lead to successful execution of the action sequence, we are free to
choose whichever state we want. Execution will succeed for any value for the free angle
of approach. As we saw in Figure 1 some values are better than others, with respect to
the expected execution duration. Therefore, subgoal refinement determines values for the
free action parameters that minimize the predicted execution duration of the entire action
sequence. The prediction is made by action models.
The behavior shown after applying subgoal refinement in Figure 1(c) has better performance, achieving the ultimate goal in less time. A pleasing side-effect is that it exhibits
seamless transitions between actions.
We will now proceed by explaining how executable actions are instantiated from the
abstract PDDL plans generated by VHPOP, and how free action parameters arise in this
process. Then, we describe how subgoal refinement optimizes these free action parameters.
6.1 Action Instantiation and Free Action Parameters
Causal links specify which action was executed previously to achieve an effect which meets
the precondition of the current action. A chain of such abstract actions represents a valid
504

fiRefining the Execution of Abstract Actions with Learned Action Models

plan to achieve the goal, an example of which is shown in Figure 10. The next step in our
system is to map such plans to the executable actions in the action library. This process is
also known as operator instantiation (Schmill et al., 2000).
Initial 0 : (robot pos1) (ball pos2) (final pos3)
Step 2

: (approachball pos1 pos2)
0
-> (ball pos2)
0
-> (robot pos1)

Step 1

: (dribbleball pos2 pos3)
2
-> (atball pos2)

Goal

:
0
1

-> (final pos3)
-> (atball pos3)

Figure 10: The output of VHPOP is a PDDL plan with causal links.
PDDL plans are instantiated with executable actions by first extracting symbolic actions and causal links in the plan, and then instantiating the symbolic actions one by one,
as listed in Algorithm 2. For each symbolic action, the executable action is retrieved by
its name (line 5), after which its parameters are requested (line 6). The next step is to
determine the parameter values of the executable action, by considering the corresponding
symbolic parameters of the PDDL plan. The correspondence between the executable action parameter and a symbolic action parameter is determined based on an index in the
executable action parameter (line 8).
The symbolic parameters themselves have no meaning in the belief state. They are just
labels used in the PDDL plan. However, causal links define predicates over these labels
which do have a meaning in the belief state. These predicates are therefore retrieved (line
9), and used to extract the correct values from the belief state (line 10).
Mapping symbolic predicates to continuous values is done in the belief state, with the
call made in line 10. If the predicate holds in the current belief state, which is the case
if it starts with a 0 (the initial state is considered the first action), it simply retrieves
the value. For 0robot and x it would return the x-coordinate of the current position of
the robot. Predicates that do not hold in the current state can also constrain values. For
instance, the atball predicate restricts the translational velocity between 0 and 0.3m/s.
If predicates impose no such constraints, default values for the parameter types are returned. For instance, the values of x-coordinates must be within the field, and angles are
always between - and . If several predicates hold, the ranges and values they return are
composed.
6.2 Optimizing Free Action Parameters
Mapping abstract PDDL plans to executable actions often leads to free action parameters.
Humans also have a high level of redundancy in their actions. As Wolpert and Ghahramani
(2000) note, there are many ways you can bring a glass of water to your lips, and some
are sensible and some are silly. The reason why we typically witness stereotypical sensi505

fiStulp & Beetz

input : pddl output (the output of VHPOP, see Figure 10 for an example)
output : exe actions (a parameterized sequence of executable actions)
1 pddl actions = parseActions(pddl output);
2 pddl links = parseCausalLinks(pddl output);

3
4
5
6
7
8
9
10
11
12
13
14

15

// For the example in Figure 10, the following now holds:
// pddl actions = [(approachball pos1 pos2),(dribbleball pos2 pos3)]
// pddl links = {pos3=[0center,1atball], pos1=[0robot], pos2=[0ball,2atball]}
exe actions = {};
foreach pddl action in pddl actions do
exe action = getAction(pddl action.name) // e.g. exe action = approachBall
exe params = exe action.getParameters() // then exe params = [x0,y0,...]
foreach exe par in exe params do
pddl par = pddl action.params[exe par.index] ;
// e.g. if exe par = x0, then exe par.index = 0 and pddl par = pos1
pddl predicates = pddl links[pddl par] ;
// e.g. if pddl par = pos1, then pddl predicates = [0robot]
value = beliefState.getValue(exe par.name, pddl predicates) ;
exe action.setParameter(exe par, value);
end
exe actions.add(exe action);
end
// For the example in Figure 10 and Figure 9(a), the following now holds:
// exe actions = [ approachBall(x=0,y=1,=0,v=0, xg=3,yg=1,g=[-,],vg=[0,0.3]),
// dribbleBall(x=3,y=1,=[-,],v=[0,0.3], xg=1,yg=3,g=2.6,vg=0) ]
return exe actions;

Algorithm 2: Action Instantiation. Implementation of line 3 of Algorithm 1.

ble and fluent movement is because the redundancy is exploited to optimize subordinate
criteria (Schaal & Schweighofer, 2005), or cost functions (Wolpert & Ghahramani, 2000),
such as energy efficiency or variance minimization.
We also take this approach, and optimize free parameters in action sequences with
respect to the expected execution duration. To optimize an action sequence, the system will
have to find values for the free action parameters such that the overall predicted execution
duration of the sequence is minimal. This overall performance is estimated by simply
summing over the action models of all actions that constitute the sequence (Stulp & Beetz,
2005).
Let us again take the example action sequence below line 14 in Algorithm 2 as an
example. In Figure 11, Figures 1 and 7 are combined. The first two polar plots represent
the predicted execution duration of the two individual actions for different values of the
free angle of approach. The overall duration is computed by simply adding those two, as is
depicted in the third polar plot. The fastest time to execute the first approachBall can be
read in the first polar plot. It is 2.5s, for an angle of approach of 0.0 degrees, as indicated in
the first plot. However, the total time for executing both approachBall and dribbleBall
for this angle is 7.4s, because the second action takes 4.9s. The third plot clearly shows
that this is not the optimal overall performance. The minimum is actually 6.5s, for an angle
of 50 . Beneath the polar plots, the situation of Figure 1 is repeated, this time with the
predicted performance for each action.
506

fiRefining the Execution of Abstract Actions with Learned Action Models

Figure 11: Selecting the optimal subgoal by finding the optimum of the summation of all
action models in the chain.

For reasons of clarity, only one parameter is optimized in this example, and we simply
read the minima from the plot. Online however, the robots must be able to determine
this minimum automatically, possibly with several free action parameters and resulting
high-dimensional search spaces. The next sections describe two optimization methods.
6.2.1 Analytical Optimization of Model Trees
In Figure 7, the action model clearly consists of line segments in the 1-dimensional feature
space. In general, model trees partition the d-dimensional feature space into k partitions,
and represent the data in each partition with a d-dimensional hyperplane.
This representation allows an analytical minimization of model trees. The solution idea
is that the minimum of a hyperplane can be found quickly by determining the values at its
corners, and taking the corner with the minimum value. This procedure should be repeated
for all k hyperplanes, which leads to k corner minima. The global minimum can then be
determined by choosing the minimum of all minimal corners. This novel analytical method
has a complexity of O(kd), in which k is the number of hyperplanes (i.e. the number of
rules, or leaves in the model tree), and d the number of dimensions. Therefore, this method
does not suffer from the curse of dimensionality, as the complexity linearly depends on the
number of dimensions, instead of exponentially.
Determining the minimum of two or more model trees is done by first merging the model
trees into one, and then determining the minimum of the new model tree. Unfortunately,
there are cases in which model trees cannot be merged, for instance when there is a nonlinear mapping between action parameters and the features derived from them. In these
cases, analytical optimization of summations of trees is not possible, and a genetic algorithm
is used. The implementation of the analytical optimization, model tree merging, as well as
507

fiStulp & Beetz

a formalization of the special cases in which model trees cannot be merged are presented
by Stulp (2007).
6.2.2 Optimization with a Genetic Algorithm
In the cases when model trees cannot be optimized analytically, we use a genetic algorithm
for optimization. Note that the problem of optimizing the action sequence optimization
has been simplified to a straightforward function optimization in which the search space is
determined by the free action parameters and their ranges, and the target function whose
minimum is sought is determined by the model trees, as in Figure 11. Since model trees have
many discontinuities and can therefore not be differentiated, we have chosen a genetic algorithm for optimization, as it can be applied well to such non-differentiable non-continuous
functions. Figure 12 depicts how optimization with a genetic algorithm (Goldberg, 1989)
is integrated in the overall system. At the top, an instantiated action sequence with bound
and free action parameters is requested to be optimized.
Note that the parameters are labeled with an identification number (ID). Action parameters have the same ID if the symbolic parameters from which they were derived are the
same, see line 8 in Algorithm 2. For reasons of brevity, ID allocation is not included in Algorithm 2. The IDs reflect that certain parameters in different actions always have the same
value, as they are identical. For instance, the goal orientation (g ) of the approachBall is
equivalent to the initial orientation () of dribbleBall. Therefore they share the ID 13.
Note that this is a free action parameter.
The first step is to partition the action parameters in the action sequence into two sets:
one set contains action parameters that are bound to a certain value during instantiation,
and the other set contains free action parameters, along with the range of values they can
take. Note that action parameters with the same ID are only stored once in these sets, as
they should have the same value.
Each free action parameter is represented as a floating point gene on a chromosome.
The number of chromosomes in the population is the number of free parameters multiplied
by 25. The chromosomes in the initial population are initialized with random values from
their respective ranges. The standard genetic algorithm (GA) loop is then started. The loop
halts if the best fitness has not changed over the last 50 generations, or if 500 generations
are evaluated. The optimal values of the free parameters are then also bound in the action
sequence.
For a chromosome, the predicted execution duration is determined by calling the action
models with the fixed values from the set of bound parameters, and the values of the free
parameters represented in the chromosome. For fitness proportionate selection, the fitness
should be a non-negative number which is larger with increased fitness. Therefore, for each
chromosome c the fitness f is computed with fc = tmax + tmin  tc , where tmax and tmin
are the maximum and minimum execution duration over all chromosomes respectively.
Our implementation of the genetic algorithm uses elitarianism (2% best individuals
passes to the next generation unmodified), mutation (on the remaining 98%), two-point
crossover (on 65% of individuals), and fitness proportionate selection (the chance of being
selected for crossover is proportionate to an individuals fitness). To test and evaluate our
GA implementation in C++, we first applied it to several optimization benchmarks, such
508

fiRefining the Execution of Abstract Actions with Learned Action Models

Figure 12: Optimization in subgoal refinement with a genetic algorithm. Implementation
of line 5 of Algorithm 1.

as the De Jongs function, Schwefels function and Ackleys Path function. The results and
optimization times are reported by Koska (2006).
In the subgoal refinement scenarios to be presented in Section 6.3, the optimization time
is usually small in comparison to the gain in performance. In the most complex kitchen
scenario, where up to 4 actions with more than 10 action parameters are optimized, our
implementation of the GA still takes less than 0.5s to get a good result.
6.3 Empirical Evaluation
In this section, we introduce the different scenarios in which the results of of applying
subgoal refinement are evaluated. In the robotic soccer domain, the action sequence to be
optimized is the approachBall action, followed by a dribbleBall action, as in Figure 1.
The free action parameters at the intermediate state are the angle of approach and the
translational velocity.
To evaluate the effect of subgoal refinement in the service robotics domain, two scenarios
are tested. In the first scenario, the goal is to put a cup from one table to another, which is
achieved by a sequence of navigation and grasping actions. In each evaluation episode, the
topology of the environment in each scenario stays the same, but the initial robot position,
509

fiStulp & Beetz

the tables, and the cups are randomly displaced. Scenario 2 is a variation of Scenario 1, in
which two cups had to be delivered.
The kitchen scenarios have many free action parameters. Because preconditions usually
bind either navigation (goToPose) or manipulation (grip or put) actions but never both
(they are independent), one of these action parameter sets is always free. Furthermore, the
distance of the robot to the table in order to grab a cup must be between 40 and 80cm (as
fixed in the precondition of grip). This range is another free parameter. As in the soccer
domain, the velocity and orientation at waypoints are also not fixed, so free for optimization
as well.
In the arm control domain, sequences of reaching movements are performed. Because
this particular task does not require abstract planning, we did not use VHPOP. For demonstration purposes, we had the arm draw the first letter of the first name of each author of the
paper by Stulp, Koska, Maldonado, and Beetz (2007), and chose 4/5 waypoints accordingly.
Figure 13(a) shows the PowerCube arm, which is attached to a B21 robot, drawing an F.
To draw these letters, only two of the six degrees of freedom of the arm are used. The free
action parameters are the angular velocities at these waypoints.
Table 3 lists the results of applying subgoal refinement to the different domains and
scenarios, where a is the number of actions in the sequence, and n is the number of episodes
tested. The baseline with which subgoal refinement is compared is a greedy approach, in
which the next subgoal is optimized with respect to the execution duration of only the
current action. In this case, we say the horizon h of optimization is 1. The downside of the
greedy baseline is that it also depends on the accuracy of the action model. However, we
chose this as a baseline, because setting all free action parameters to zero certainly leads to
worse execution times, and optimizing them manually introduces a human bias.
Scenario
Soccer (real)
Soccer (simulated)
Kitchen (scenario 1)
Kitchen (scenario 2)
Arm control

a
2
2
4
13
4-5

n
100
1000
100
100
4

t1
10.6s
9.8s
46.5s
91.7s
10.6s

t2
9.9s
9.1s
41.5s
85.4s
10.0s

t2/1
6.1%
6.6%
10.0%
6.6%
5.7%

p
0.00
0.00
0.00
0.00
0.08

Table 3: Subgoal refinement results.

The execution time of a single action is denoted teh , which has two indices referring
to the horizon and the episode. For instance t64
1 refers to the total execution time of the
64th episode, performed with a horizon of 1. The mean
P overall execution duration over
all episodes is denoted t1 , and computed with t1 = n1 ne=1 te1 . Since subgoal refinement
optimizes the execution duration of the current and next action, it has a horizon of 2. The
fourth column lists thePmean overall execution duration with subgoal refinement t2 , and
computed with t2 = n1 ne=1 te2 .
P
The equation t2/1 = n1 ne=1 (1  te2 /te1 ) is used to compute the mean improvement over
all episodes achieved with subgoal refinement. The p-value of the improvement is computed
using a dependent t-test with repeated measures, as each episode is performed twice, once
510

fiRefining the Execution of Abstract Actions with Learned Action Models

with, and once without subgoal refinement. A significant improvement occurs in all but one
domain.
To visualize the qualitative effect of applying subgoal refinement, the results from the
arm control domain are depicted in Figure 13(b). The angular velocities are set to zero
(upper row) or optimized with subgoal refinement (lower row). The axes represent the
angles of the two joints. This figure demonstrates that the trajectories are smoother with
subgoal refinement: the arm mostly draws one long stroke, rather than discernible line
segments. Since the arm control domain is mainly included for visualization purposes,
there are only four test runs. For this reason the overall improvement is not significant
(p > 0.05).

(a) The B21 robot drawing an F with
its PowerCube arm.

(b) Drawing letters without (upper row) and with (lower row)
subgoal refinement. With refinement, letters are drawn faster
and smoother.

Figure 13: Arm control experiment.
Although optimizing execution duration leads to smoother motion in this domain,
smooth human arm motion arises from variability rather than time optimization (Harris &
Wolpert, 1998; Simmons & Demiris, 2004). In this article, the main goal is not to explain
or model human motion, but rather to demonstrate the effects of optimizing sequences of
actions.
6.3.1 Sequences with More Actions
So far, we have seen optimization with horizons of h = 1 (greedy) and h = 2. The standard
approach with h = 2 can easily be extended, so that subgoal refinement optimizes the
execution duration of the next h > 2 actions. The higher the horizon h, the more subgoal
refinement is preparing for actions further in the future.
To evaluate the effect of optimizing more than two actions, sequences of four and more
actions are optimized using subgoal refinement with different horizons. Two scenarios are
used: a simulated soccer scenario in which the robot has to traverse four waypoints with the
goToPose action, and the two kitchen scenarios. After each action execution, the subgoals
between subsequent actions are recomputed with the same horizon, or less if the number
of remaining actions is smaller than h. The results are summarized in Table 4. The first
row represents the baseline greedy approach with h = 1, and the second row represents the
results reported so far with h = 2. The next two rows list the results of optimizing 3 and 4
511

fiStulp & Beetz

action execution durations. Again, the reported times represent the execution duration of
the entire action sequence, averaged over 100 episodes.
horizon
h=1
h=2
h=3
h=4

P

22.7
20.3
20.2
20.2

Soccer
Imp. p-value
10.6%
0.7%
0.2%

0.000
0.001
0.053

Kitchen (Scen.1)
P
Imp. p-value
46.5
41.5 10.0%
0.000
40.6
1.5%
0.041
-

Kitchen (Scen.2)
P
Imp. p-value
91.7
85.4 6.6%
0.041
85.3 0.1%
0.498
-

Table 4: Effect of the subgoal refinement horizon h on performance improvement.

In all three scenarios, there is a substantial improvement from h = 1 to h = 2, but from
h = 2 to h = 3 the improvement is marginal or insignificant. When executing an action in
these scenarios, it is apparently beneficial to prepare for the next action, but not for the action after that. This insight is also the main motivation behind receding horizon control, as
described in Section 3.2. However, there are also some important differences between RHC
and subgoal refinement. First of all, optimal control plans and optimizes motor commands
of fixed duration, whereas subgoal refinement does so for durative actions of varying duration, and at a higher level of temporal abstraction. Furthermore, RHC optimizes the first
Hp motor commands, whereas subgoal refinement optimizes action parameters at partially
fixed subgoals, and is not concerned with the actual motor commands needed to reach the
subgoals. The planner can therefore fix the general structure of the plan, rather than committing to only the first few steps. Finally, optimal control assumes that precise analytic
models of the actions and the systems behavior are available. For many robotic systems,
as well as humans, this does not hold. However, the lack of analytic models does not keep
us from acquiring models from experience. By learning action models, our system is also
flexible enough to acquire action models for changing actions, or actions for which no model
can be acquired through analysis.
6.3.2 Predicting Performance Decrease
There are cases in which subgoal refinement does not have an effect. In the ball approach
scenario for instance, if the robot, the ball and the final destination are perfectly aligned,
there is not much to do for subgoal refinement, as the greedy approach already delivers the
optimal angle of approach: straight towards the ball. On the contrary, refining subgoals
in these cases might put unnecessary constraints on the execution. Due to inaccuracies in
the action models and the optimization techniques, it is sometimes even the case that the
greedy approach does slightly better than subgoal refinement.
To evaluate these effects, 1000 episodes were executed in simulation with both h = 1 and
h = 2. Although the overall improvement with h = 2 was 6.6% (see Table 3), 160 of 1000
episodes actually lead to an increased execution duration. These episodes were labeled -,
and the remaining with +. We then trained a decision tree to predict this nominal value.
This tree yields four simple rules which predict the performance difference correctly in 87%
of given cases. The learned decision tree is essentially an action model too. Rather than
512

fiRefining the Execution of Abstract Actions with Learned Action Models

predicting the outcome of an individual action, it predicts the outcome of applying action
models to actions.
We performed another 1000 test episodes, as described above, but only applied subgoal
refinement if the decision tree predicted that applying it would yield a better performance.
The overall improvement was so raised from 6.6% to 8.6%.

7. Subgoal Assertion
In practice, learning actions hardly starts from scratch, and knowledge about previously
learned actions is transfered to novel task contexts. For both humans and robots for instance, approaching a ball is very similar to navigating without considering the ball. On
an abstract level, both involve going from some state to another on the field, as in Figure 14(a), and both should be implemented to execute as efficiently and fast as possible.
However, there are also slight differences between these two tasks. When approaching the
ball it is important to not bump into it before achieving the desired state, as depicted in
Figure 14(b).

(a) Original scenario.

(b) Refined goal, learned with
condition refinement.

(c) Subgoal assertion based on condition refinement.

Figure 14: Computational model of condition refinement and subgoal assertion.
Figure 14 illustrates these small differences. In the first scenario, there is no ball, and
the effects of both goToPose actions satisfy the goal, so both actions achieve the goal.
Figure 14(b) is basically the same scenario, with the added requirement that the robot
must be in possession of the ball at the goal state. The same goToPose action is used to
achieve these goals. Since goToPose is not aware of the ball, it often collides with the ball
before achieving the desired state. The new goal of approaching the ball is essentially a
refined subset of the former goal of simply navigating there. When executing goToPose,
the robot to the left succeeds in approaching the ball, but the robot to the right does not,
as it bumps into the ball beforehand. This is the case because the effects of goToPose no
longer satisfy the refined goal, as is depicted below the scenario.
The effects of goToPose are now partitioned into a subset which does satisfy the new
refined goal, and a subset which does not. These are represented with blue (S) and red (F)
respectively. Analogously, the preconditions are partitioned into a subset Success which
leads to a final state which is in the subset of the effects that satisfy the refined goal,
and a subset Fail for which this is not the case. Because the effects, and consequently,
preconditions of an action are refined for a new task, we call this condition refinement.
513

fiStulp & Beetz

Once the refined precondition of a novel goal is known, it is easy to determine if a
particular initial state will lead to a successful execution or not. If it does, the action is
executed as is. For instance, the robot to the left can simply execute the goToPose action,
as it is in the refined precondition. As we shall see, the goToPose action can be used to
approach the ball successfully almost half the time.
However, the robot to the right cannot simply use goToPose to approach the ball. This
robot now needs a novel action, e.g. approachBall, that enables it to go from any of the
states in the Fail to the refined goal. Or does it? Instead, the goToPose action is used
again, to take the robot from the Fail subset to the Success subset. Once this is done, a
goToPose action that will succeed at approaching the ball is executed.
The key to reuse is therefore being able to predict when an action will fail, and when
it will succeed at a novel task. When it is predicted to succeed, the action is executed
as is. If the action will fail, another action should be executed beforehand, such that the
robot ends up in a state from which the action will succeed, as depicted in Figure 14(c).
This intermediate state between actions is a new subgoal. This approach is therefore called
subgoal assertion.

input : exe actions (a sequence of (partially) instantiated actions)
output : exe actions2 (a sequence of (partially) instantiated actions with asserted subgoals)
1 exe actions2 = {};
2 foreach exe action in exe actions do
3
switch exe action.name do
4
case goToPose
exe actions2.add (exe action) // Subgoal assertion never needed for this action
5
6
end
7
case approachBall

8
9
10
11
12

13
14
15
16
17
18
19
20
21

// Get the parameters related to the from and to states.
// Uses the same indexing scheme as in lines 6-8 of Algorithm 2..
exe params0 = exe action.getParameters(0);
exe params1 = exe action.getParameters(1);
if goToPose.approachBallSuccess( exe params0, exe params1) then
// goToPose will do the job, subgoal assertion not needed
exe actions2.add (new goToPose(exe params0, exe params1));
else
// exe params2 is set to the default ranges of the action parameters of goToPose.
// Same as in lines 10-11 of Algorithm 2.
exe params2 = ...;
exe actions2.add (new goToPose(exe params0, exe params2));
exe actions2.add (new goToPose(exe params2, exe params1));
end
end
...
end
end
return exe actions2;

Algorithm 3: Implementation of line 4 of Algorithm 1.

514

fiRefining the Execution of Abstract Actions with Learned Action Models

7.1 Integration in the Overall System
Subgoal assertion takes a sequence of actions, and returns the same sequence with asserted
subgoal that are needed to assure successful execution, as listed in Algorithm 3. The main
loop goes through all actions, and leaves goToPose actions untouched. approachBall has
no implementation itself, and is replaced by goToPose actions. Only one goToPose is needed
if it is predicted to succeed at approaching the ball. This is the case if the initial state is in
the Success subset in Figure 14(b). Determining these subsets manually is a difficult task,
due to complex interactions between the dynamics and shape of the robot, as well as the
specific characteristics of the action. Therefore, these subsets are learned with a decision
tree, as described in Section 5.4.
If a success is predicted, one goToPose is executed as is, with the same parameters as
the approachBall action. If it is predicted to fail, a subgoal is asserted (exe params2),
and inserted between two goToPose actions. The action parameters exe params2 initially
receive default ranges. All parameters in exe params2 are free, and are optimized with
subgoal refinement. This immediately follows subgoal assertion, as listed in Algorithm 1.
This ensures that the values for exe params2 minimize the predicted execution duration,
and that the transition between the two goToPose actions is smooth.
One issue remains open. The intermediate goal between the actions must lie within the
Success subset in Figure 14, which for the ball approach task is any position in the green
area to the left in Figure 8. This requirement puts constraints on the values of exe params2.
It must be ensured that the optimization process in subgoal refinement only considers
states from the Success subset of the refined precondition of the second goToPose action.
Therefore, the action model for this action is modified so that it returns an INVALID
flag for these states. This approach has been chosen as it requires little modification of
the optimization module. Chromosomes that lead to an invalid value simply receive a low
fitness.
1 goToPose.executionDurationApproachBall (x,y,,v,xg ,yg ,g ,vg ) {
2
if goToPose.approachBallSuccess ( x,y,,v,xg ,yg ,g ,vg ) then
3
return goToPose.executionDuration (x,y,,v,xg ,yg ,g ,vg );
4
else
5
return INVALID;
6
end
7 }

Algorithm 4: Modified goToPose action model for approaching the ball.
Analogously to Figure 11, the predicted execution durations of the two actions, as well
as their summation are depicted in Figure 15. Invalid values are not rendered. The second
graph depicts the function described in Algorithm 4. Note that due to removal of invalid
values, the shape of the functions on the ground plane in the last two graphs corresponds
to Figure 8 and 16(a).
In Figure 16(a), three instances of the problem are depicted. Since the robot to the
left is in the area in which no collision is predicted, it simply executes goToPose, without
asserting a subgoal. The model predicts that the other two robots will collide with the ball
515

fiStulp & Beetz

Figure 15: Search space for subgoal refinement in subgoal assertion.

when executing goToPose, and a subgoal is asserted. The subgoals, determined by subgoal
refinement, are depicted as well.

Fail
Predicted
Fail
Success

(a) Three examples. Subgoal assertion is applied to two of them.

2%

Observed
Success

(52%50%)

60%

1%

3%

(10%+50%)

37%

97%




62%
38%

(b) Mean results over 100 episodes. Almost all predicted failures
(50% of 52%) are transformed to yield a successful execution.

Figure 16: Results of subgoal assertion.

7.2 Empirical Evaluation
To evaluate automatic subgoal assertion, a hundred random ball approaches are executed
in simulation, once with subgoal assertion, and once without. The results are summarized
in Table 16(b). Without assertion, the results are similar to the results reported in Table 2.
A collision is again correctly predicted approximately half the time: 52% of these hundred
episodes. Subgoal assertion is applied in these cases, and is almost always successful: 50% of
all episodes that are predicted to fail are now executed successfully. Only 2% of the episodes
still have a collision, despite subgoal assertion. Because no subgoal assertion is applied when
Success is predicted, there is no change in the lower prediction row. Consciously choosing
not to apply subgoal assertion and not applying it are equivalent.
Subgoal assertion is applied unnecessarily in 10% of all episodes, which means that subgoal assertion is applied, even though the original sequence would already be successful.
However, the execution with subgoal assertion and consequent subgoal refinement is a significant 8% slower than executing only the one goToPose action. The performance loss in
these cases seems an acceptable cost compared to the pay-off of the dramatic increase in
the number of successful task completions.
516

fiRefining the Execution of Abstract Actions with Learned Action Models

Summarizing: if subgoal assertion is not necessary, it is usually not applied. A subgoal
is introduced half of the time, which raises successful task completion from 47 to 97%. Infrequently, subgoals are introduced inappropriately, which leads to a small loss of performance
in terms of execution duration.

8. Conclusion and Outlook
Durative actions provide a conceptual abstraction that is reasoned about either by the
designer during action selection design, or, if the abstraction is explicitly coded into the
controller, by the action selection system itself. Action abstractions partially achieve their
abstraction by ignoring action parameters. Although these parameters are not relevant to
the action on an abstract level, they often are relevant to the performance and success of
executing the plan.
If robots learn to predict the effects and performance of their actions, they can use this
knowledge to optimize their behavior through subgoal refinement, and avoid failures through
subgoal assertion. The empirical evaluations verify that this leads to more efficient, robust,
and effective behavior. We believe these are important contributions towards bridging the
gap between abstract planning systems and robot action execution.
In multi-robot scenarios such as robotic soccer, robots can be provided with the action
models of teammates, enabling each robot to reason about the actions of others. We have
shown that this enables robots to implicitly coordinate their actions with others, without
having to resort to utility communication (Stulp et al., 2006). We have also successfully
applied this approach to a heterogeneous team of robots, with robots from two different
research groups (Isik, Stulp, Mayer, & Utz, 2006).
Also, we have preliminary results showing that even more accurate models can be learned
when data is gathered online during operation time (Kirsch & Beetz, 2007). Extended
operation times not only enable the robot to gather more training data, but actions are also
called with parameterizations typical for the domain and the context in which the robot is
used. This in contrast to our current method, which generates action parameterizations by
randomly choosing them from all possible parameterizations. Given the same amount of
data, a model that has to generalize over all possible parameterizations will tend to be less
accurate than a model that has to do so over only a subset of parameterizations.
The main assumption underlying this article is that human-specified knowledge and
robot-learned knowledge complement each other well in robot controllers. It is exemplary that recent winners of two well-known robotic benchmarks, the RoboCup mid-size
league (Gabel, Hafner, Lange, Lauer, & Riedmiller, 2006) and the DARPA challenge (Thrun
et al., 2006), emphasize that their success could only be achieved through the combination
of manual coding and experience-based learning. More specifically, we believe that ideally
human designers should only have to specify action abstractions offline. During task execution, the robot should then automatically optimize aspects of actions that are relevant for
its execution with learned action models.
Future work includes learning several models for different performance measures, and
optimizing several performance measures simultaneously. For instance, energy consumption
is another important performance measure in autonomous mobile robots. By specifying objective functions that consist of the combinations of energy consumption and execution
517

fiStulp & Beetz

duration, they can both be optimized. By weighting individual performance functions differently, the function to be optimized can be customized to specific scenarios. For instance,
in mid-size league robotic soccer, with its short operation time of 15 minutes, speed is far
more important then energy consumption. In service robotics it is the other way around.
Also, we intend to evaluate the use of other learning algorithms for predicting the failure
of actions, for instance Neural Networks (Buck & Riedmiller, 2000) or Support Vector Machines (Hart, Ou, Sweeney, & Grupen, 2006). If these learning algorithms can predict action
failures more accurately, even better results are to be expected from subgoal refinement and
subgoal assertion.

Acknowledgments
We would like to thank Andreas Fedrizzi and the anonymous reviewers for their valuable remarks and suggestions. The research described in this article is partly funded by
the German Research Foundation (DFG) in the SPP-1125, Cooperating Teams of Mobile
Robots in Dynamic Environments, and also by the CoTeSys cluster of excellence (Cognition for Technical Systems, http://www.cotesys.org), part of the Excellence Initiative of
the DFG.

Appendix A
Robot

State Estimation

Soccer
(real)
Soccer
(simulated)
Kitchen

Probabilistic state estimation based on camera
Probabilistic state estimation based on camera
Ground truth, with noise based on noise model
Ground truth if in field of view, with noise
Ground truth
Ground truth
Ground truth if in field of view

Arm control

Joint angles read directly from motor encoders










Belief State

Motor comm.

x, y, , v,
xball , yball
x, y, , v,
xball , yball
x, y, , v,
ax, ay, az
[xo , yo , zo ]n
a , a , b , b

v, 
v, 
v, 
ax, ay, az
I a, I b

Table 5: State variables in the belief state, state estimation process used to acquire them,
and the motor command in each domain. x, y, , v is dynamic pose of robot body,
ax, ay, az is relative position of arm to robot body, [xo , yo , zo ]n are absolute positions of n objects, v,  is translational/rotational velocities, I is current sent to
arm joint motor.

518

fiRefining the Execution of Abstract Actions with Learned Action Models

Robot

Action

Soccer

goToPose
dribbleBall
goToPose
reach
reach

Kitchen
Arm control

Action Parameters
Current
Goal
x, y, , v
xg , yg , g , vg
x, y, , v, xball , yball
xg , yg , g , vg
x, y, , v
xg , yg , g , vg
x, y, z, ax, ay, az xg , yg , zg , axg , ayg , azg
a , a , b , b
ga , ga , gb , gb

Table 6: List of actions used in the application domains. The list of actions might be shorter
than expected. For instance, it is doubtful that robots could play soccer if they
can only navigate to a certain pose. It is the aim of this article to demonstrate
how only a few actions can be reused and customized to perform well in varying
task contexts.
Robot

Action

Features

Soccer

goToPose and
dribbleBall

Kitchen

goToPose

p
v, dist = (x  xg )2 + (y  yg )2 ,
angle to = |angle tosigned |,
angle at = sgn(angle tosigned )
norm(g  atan2(yg  y, xg  x))
vg , dist, angle to, angle at,
angle =p
|norm(g  )|
distxyz =p (x  xg )2 + (y  yg )2 + (z  zg )2
distxy = (x  xg )2 + (y  yg )2 , distxz , distyz ,
anglexyq= atan2(yg  y, xg  x), anglexz , angleyz

reach

Arm control

reach

dist =

(a  ga )2 + (b  gb )2 ,

angle1 = norm(atan2(gb  b , ga  a )  atan2(b , a )),
b   b ,  a   a ) + atan2(b , a ))
angle
g
g g
q2 = norm(atan2(
q g
2
2
2
2
v = a + b , vg = ga + gb
norm(a): adds or subtracts 2 to a until is in range [, ]
angle tosigned = norm(atan2(yg  y, xg  x)  )
Table 7: The feature spaces used to learn action models

References
Arkin, R. (1998). Behavior based Robotics. MIT Press, Cambridge, Ma.
Astrom, K. J., & Murray, R. M. (2008). Feedback Systems: An Introduction for Scientists and
Engineers, chap. Supplement on optimization-based control. Princeton University Press.
Barto, A., & Mahadevan, S. (2003). Recent advances in hierarchical reinforcement learning. Discrete
event systems, 13 (1-2), 4177.
Beetz, M., & Belker, T. (2000). XFRMLearn - a system for learning structured reactive navigation
plans. In Proceedings of the 8th International Symposium on Intelligent Robotic Systems.
519

fiStulp & Beetz

Beetz, M. (2000). Concurrent Reactive Plans: Anticipating and Forestalling Execution Failures, Vol.
LNAI 1772 of Lecture Notes in Artificial Intelligence. Springer Publishers.
Beetz, M. (2001). Structured Reactive Controllers. Journal of Autonomous Agents and Multi-Agent
Systems. Special Issue: Best Papers of the International Conference on Autonomous Agents
99, 4, 2555.
Beetz, M., Schmitt, T., Hanek, R., Buck, S., Stulp, F., Schroter, D., & Radig, B. (2004). The AGILO
robot soccer team  experience-based learning and probabilistic reasoning in autonomous robot
control. Autonomous Robots, 17 (1), 5577.
Belker, T. (2004). Plan Projection, Execution, and Learning for Mobile Robot Control. Ph.D. thesis,
Department of Applied Computer Science, University of Bonn.
Bellingham, J., Richards, A., & How, J. P. (2002). Receding horizon control of autonomous aerial
vehicles. In Proceedings of the 2002 American Control Conference, Vol. 5, pp. 37413746.
Benson, S. (1995). Inductive learning of reactive action models. In International Conference on
Machine Learning (ICML), pp. 4754.
Bertram, D., Kuffner, J., Dillmann, R., & Asfour, T. (2006). An integrated approach to inverse
kinematics and path planning for redundant manipulators. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), pp. 18741879.
Bouguerra, A., & Karlsson, L. (2005). Symbolic probabilistic-conditional plans execution by a mobile
robot. In IJCAI-05 Workshop: Reasoning with Uncertainty in Robotics (RUR-05).
Brock, O., & Khatib, O. (1999). Elastic Strips: A framework for integrated planning and execution.
In Proceedings of the International Symposium on Experimental Robotics, pp. 329338.
Buck, S., Beetz, M., & Schmitt, T. (2002a). M-ROSE: A Multi Robot Simulation Environment for
Learning Cooperative Behavior. In Asama, H., Arai, T., Fukuda, T., & Hasegawa, T. (Eds.),
Distributed Autonomous Robotic Systems 5, Lecture Notes in Artificial Intelligence, LNAI.
Springer-Verlag.
Buck, S., Beetz, M., & Schmitt, T. (2002b). Reliable Multi Robot Coordination Using Minimal
Communication and Neural Prediction. In Beetz, M., Hertzberg, J., Ghallab, M., & Pollack,
M. (Eds.), Advances in Plan-based Control of Autonomous Robots. Selected Contributions of
the Dagstuhl Seminar Plan-based Control of Robotic Agents, Lecture Notes in Artificial
Intelligence. Springer.
Buck, S., & Riedmiller, M. (2000). Learning situation dependent success rates of actions in a
RoboCup scenario. In Pacific Rim International Conference on Artificial Intelligence, p. 809.
Cambon, S., Gravot, F., & Alami, R. (2004). A robot task planner that merges symbolic and
geometric reasoning.. In Proceedings of the 16th European Conference on Artificial Intelligence
(ECAI), pp. 895899.
Clement, B. J., Durfee, E. H., & Barrett, A. C. (2007). Abstract reasoning for planning and coordination. Journal of Artificial Intelligence Research, 28, 453515.
Coradeschi, S., & Saffiotti, A. (2001). Perceptual anchoring of symbols for action. In Proceedings of
the International Joint Conference on Artificial Intelligence (IJCAI), pp. 407416.
Dean, T., & Wellmann, M. (1991). Planning and Control. Morgan Kaufmann Publishers.
Dearden, A., & Demiris, Y. (2005). Learning forward models for robotics. In Proceedings of the
Nineteenth International Joint Conference on Artificial Intelligence (IJCAI), pp. 14401445.
Dietterich, T. G. (2000). Hierarchical reinforcement learning with the MAXQ value function decomposition. Journal of Artificial Intelligence Research, 13, 227303.
Fox, M., & Long, D. (2003). PDDL2.1: An extension of PDDL for expressing temporal planning
domains.. Journal of Artificial Intelligence Research, 20, 61124.
520

fiRefining the Execution of Abstract Actions with Learned Action Models

Fox, M., Ghallab, M., Infantes, G., & Long, D. (2006). Robot introspection through learned hidden
markov models. Artificial Intelligence, 170 (2), 59113.
Gabel, T., Hafner, R., Lange, S., Lauer, M., & Riedmiller, M. (2006). Bridging the gap: Learning in
the RoboCup simulation and midsize league. In Proceedings of the 7th Portuguese Conference
on Automatic Control.
Gerkey, B., Vaughan, R. T., & Howard, A. (2003). The Player/Stage Project: Tools for multirobot and distributed sensor systems. In Proceedings of the 11th International Conference on
Advanced Robotics (ICAR), pp. 317323.
Ginsberg, M. L. (1989). Universal planning: an (almost) universally bad idea. AI Magazine, 10 (4),
4044.
Goldberg, D. E. (1989). Genetic Algorithms in Search, Optimization and Machine Learning. Kluwer
Academic Publishers.
Haigh, K. Z. (1998). Situation-Dependent Learning for Interleaved Planning and Robot Execution.
Ph.D. thesis, School of Computer Science, Carnegie Mellon University.
Harris, C. M., & Wolpert, D. M. (1998). Signal-dependent noise determines motor planning. Nature,
394 (20), 780784.
Hart, S., Ou, S., Sweeney, J., & Grupen, R. (2006). A framework for learning declarative structure.
In Workshop on Manipulation for Human Environments, Robotics: Science and Systems.
Haruno, M., Wolpert, D. M., & Kawato, M. (2001). MOSAIC model for sensorimotor learning and
control. Neural Computation, 13, 22012220.
Haruno, M., Wolpert, D. M., & Kawato, M. (1999). Multiple paired forward-inverse models for
human motor learning and control. In Proceedings of the 1998 conference on Advances in
neural information processing systems II, pp. 3137, Cambridge, MA, USA. MIT Press.
Hoffmann, J., & Duffert, U. (2004). Frequency space representation and transitions of quadruped
robot gaits. In Proceedings of the 27th Australasian computer science conference (ACSC), pp.
275278.
Infantes, G., Ingrand, F., & Ghallab, M. (2006). Learning behavior models for robot execution
control. In Proceedings of the 17th European Conference on Artificial Intelligence (ECAI),
pp. 678682.
Isik, M., Stulp, F., Mayer, G., & Utz, H. (2006). Coordination without negotiation in teams of
heterogeneous robots. In Proceedings of the RoboCup Symposium, pp. 355362.
Jacobs, R. A., & Jordan, M. I. (1993). Learning piecewise control strategies in a modular neural
network. IEEE Transactions on Systems, Man and Cybernetics, 23 (3), 337345.
Jaeger, H., & Christaller, T. (1998). Dual dynamics: Designing behavior systems for autonomous
robots. Artificial Life and Robotics, 2 (3), 108112.
Jordan, M. I., & Rumelhart, D. E. (1992). Forward models: Supervised learning with a distal teacher.
Cognitive Science, 16, 307354.
Kirsch, A., & Beetz, M. (2007). Training on the job  collecting experience with hierarchical hybrid
automata. In Hertzberg, J., Beetz, M., & Englert, R. (Eds.), Proceedings of the 30th German
Conference on Artificial Intelligence (KI-2007), pp. 473476.
Kitano, H., Asada, M., Kuniyoshi, Y., Noda, I., & Osawa, E. (1997). RoboCup: The robot world
cup initiative. In Proceedings of the first international conference on autonomous agents
(AGENTS), pp. 340347.
Kollar, T., & Roy, N. (2006). Using reinforcement learning to improve exploration trajectories for error minimization. In Proceedings of the International Conference on Robotics and Automation
(ICRA), pp. 33383343.
521

fiStulp & Beetz

Koska, W. (2006). Optimizing autonomous service robot plans by tuning unbound action parameters.
Masters thesis, Technische Universiat Munchen.
Kovar, L., & Gleicher, M. (2003). Flexible automatic motion blending with registration curves. In
Proceedings of the 2003 ACM SIGGRAPH/Eurographics symposium on computer animation
(SCA), pp. 214224.
Muller, A., & Beetz, M. (2006). Designing and implementing a plan library for a simulated household
robot. In Beetz, M., Rajan, K., Thielscher, M., & Rusu, R. B. (Eds.), Cognitive Robotics:
Papers from the AAAI Workshop, Technical Report WS-06-03, pp. 119128, Menlo Park,
California. American Association for Artificial Intelligence.
Nakanishi, J., Cory, R., Mistry, M., Peters, J., & Schaal, S. (2005). Comparative experiments on task
space control with redundancy resolution. In IEEE International Conference on Intelligent
Robots and Systems (IROS), pp. 39013908.
Nilsson, N. J. (1984). Shakey the robot. Tech. rep. 323, AI Center, SRI International.
Nilsson, N. J. (1994). Teleo-reactive programs for agent control. Journal of Artificial Intelligence
Research, 1, 139158.
Perlin, K. (1995). Real time responsive animation with personality. IEEE Transactions on Visualization and Computer Graphics, 1 (1), 515.
Quinlan, R. (1992). Learning with continuous classes. In Adams, A., & Sterling, L. (Eds.), Proceedings of the 5th Australian Joint Conference on Artificial Intelligence, pp. 343348.
Russell, S., & Norvig, P. (2003). Artificial Intelligence - A Modern Approach. Prentice Hall, Upper
Saddle River, New Jersey.
Ryan, M., & Pendrith, M. (1998). RL-TOPs: an architecture for modularity and re-use in reinforcement learning. In Proceedings 15th International Conference on Machine Learning (ICML),
pp. 481487.
Saffiotti, A., Ruspini, E. H., & Konolige, K. (1993). Blending reactivity and goal-directedness in a
fuzzy controller. In Proceedings of the IEEE International Conference on Fuzzy Systems, pp.
134139, San Francisco, California. IEEE Press.
Schaal, S., & Schweighofer, N. (2005). Computational motor control in humans and robots. Current
Opinion in Neurobiology, 15, 675682.
Schmill, M. D., Oates, T., & Cohen, P. R. (2000). Learning planning operators in real-world,
partially observable environments. In Proceedings of the 5th International Conference on
Artificial Intelligence Planning Systems (ICAPS), pp. 246253.
Shahaf, D., & Amir, E. (2006). Learning partially observable action schemas.. In Proceedings of the
21st National Conference on Artificial Intelligence (AAAI).
Simmons, G., & Demiris, Y. (2004). Biologically inspired optimal robot arm control with signaldependent noise. In Proceedings of IEEE International Conference on Intelligent Robots and
Systems (IROS), pp. 491496.
Smith, D., Frank, J., & Jonsson, A. (2000). Bridging the gap between planning and scheduling.
Knowledge Engineering Review, 15 (1), 4783.
Stulp, F. (2007). Tailoring Robot Actions to Task Contexts using Action Models. Ph.D. thesis,
Technische Universitat Munchen.
Stulp, F., & Beetz, M. (2005). Optimized execution of action chains using learned performance
models of abstract actions. In Proceedings of the Nineteenth International Joint Conference
on Artificial Intelligence (IJCAI).
Stulp, F., Isik, M., & Beetz, M. (2006). Implicit coordination in robotic teams using learned prediction models. In Proceedings of the IEEE International Conference on Robotics and Automation
(ICRA), pp. 13301335.
522

fiRefining the Execution of Abstract Actions with Learned Action Models

Stulp, F., Koska, W., Maldonado, A., & Beetz, M. (2007). Seamless execution of action sequences.
In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA),
pp. 36873692.
Sussman, G. J. (1973). A computational model of skill acquisition. Ph.D. thesis, Massachusetts
Institute of Technology.
Thrun, S. et al. (2006). Stanley, the robot that won tFhe DARPA grand challenge. Journal of Field
Robotics, 23 (9), 661692.
Todorov, E., Li, W., & Pan, X. (2005). From task parameters to motor synergies: A hierarchical
framework for approximately optimal control of redundant manipulators. Journal of Robotic
Systems, 22 (11), 691710.
Uno, Y., Wolpert, D. M., Kawato, M., & Suzuki, R. (1989). Formation and control of optimal
trajectory in human multijoint arm movement - minimum torque-change model. Biological
Cybernetics, 61 (2), 89101.
Utz, H., Kraetzschmar, G., Mayer, G., & Palm, G. (2005). Hierarchical behavior organization. In
Proceedings of the 2005 International Conference on Intelligent Robots and Systems (IROS),
pp. 25982605.
Wimmer, M., Stulp, F., Pietzsch, S., & Radig, B. (2008). Learning local objective functions for robust
face model fitting. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI),
30 (8). to appear.
Witten, I. H., & Frank, E. (2005). Data Mining: Practical machine learning tools and techniques
(2nd edition). Morgan Kaufmann, San Francisco.
Wolpert, D., & Ghahramani, Z. (2000). Computational principles of movement neuroscience. Nature
Neuroscience Supplement, 3, 12121217.
Wolpert, D. M., & Flanagan, J. (2001). Motor prediction. Current Biology, 11 (18), 729732.
Younes, H. L. S., & Simmons, R. G. (2003). VHPOP: Versatile heuristic partial order planner.
Journal of Artificial Intelligence Research, 20, 405430.

523

fiJournal of Artificial Intelligence Research 32 (2008) 355-384

Submitted 10/07; published 05/08

Spectrum of Variable-Random Trees
Fei Tony Liu
Kai Ming Ting

TONY. LIU @ INFOTECH . MONASH . EDU . AU
KAIMING . TING @ INFOTECH . MONASH . EDU . AU

Gippsland School of Information Technology,
Monash University, Australia

Yang Yu
Zhi-Hua Zhou

YUY @ LAMDA . NJU . EDU . CN
ZHOUZH @ LAMDA . NJU . EDU . CN

National Key Laboratory for Novel Software Technology,
Nanjing University, China

Abstract
In this paper, we show that a continuous spectrum of randomisation exists, in which most existing tree randomisations are only operating around the two ends of the spectrum. That leaves a
huge part of the spectrum largely unexplored. We propose a base learner VR-Tree which generates
trees with variable-randomness. VR-Trees are able to span from the conventional deterministic
trees to the complete-random trees using a probabilistic parameter. Using VR-Trees as the base
models, we explore the entire spectrum of randomised ensembles, together with Bagging and Random Subspace. We discover that the two halves of the spectrum have their distinct characteristics;
and the understanding of which allows us to propose a new approach in building better decision
tree ensembles. We name this approach Coalescence, which coalesces a number of points in the
random-half of the spectrum. Coalescence acts as a committee of experts to cater for unforeseeable conditions presented in training data. Coalescence is found to perform better than any single
operating point in the spectrum, without the need to tune to a specific level of randomness. In
our empirical study, Coalescence ranks top among the benchmarking ensemble methods including
Random Forests, Random Subspace and C5 Boosting; and only Coalescence is significantly better
than Bagging and Max-Diverse Ensemble among all the methods in the comparison. Although
Coalescence is not significantly better than Random Forests, we have identified conditions under
which one will perform better than the other.

1. Introduction
When building ensemble-classifiers, randomisation plays an important role in forming diverse models that are generated from deterministic algorithms. Through the use of ensemble methods, diverse
models are aggregated to improve the generalisation capability of the resulting classifiers.
Traditionally, ensemble methods are based on deterministic algorithms with randomisations injected to produce diverse variants. Representatives of these are Bagging (Breiman, 1996a), Random
Forests (Breiman, 2001), Randomised C4.5 (Dietterich, 2000) and Random Subspace (Ho, 1998).
Recently, a completely random approach (Fan, Wang, Yu, & Ma, 2003; Fan, 2004; Liu, Ting,
& Fan, 2005) is proposed using trees that are generated without any deterministic heuristic; this
approach represents a departure from the traditional approaches. In this paper, we show that the
complete-random approach and some of the traditional approaches can be used as two extremes to
form a continuous spectrum of randomisation; and better predictive accuracy can often be found
within the spectrum.
c
2008
AI Access Foundation. All rights reserved.

fiL IU , T ING , Y U , & Z HOU

In this paper, we propose a novel algorithm which is capable of generating a range of models,
end-to-end continuously from completely random to purely deterministic. The most striking fact is
that though each tree-node is created either randomly or deterministically, the resulting randomness
can span from completely random to purely deterministic without any modification to the ensemble method. This algorithm enables us to explore the whole spectrum between the two extremes
and we show that, this new algorithm can be easily incorporated into existing ensemble methods,
such as Bagging and Random Subspace. Together they generate ensembles of different degrees of
randomness, which are largely unexplored until now.
We reveal that most of the existing random ensemble methods such as Bagging and Random
Subspace focus on the deterministic-end of the spectrum, and ignore a major part of the spectrum.
We show that Bagging, Random Subspace and the simple complete-random trees find their better
counterparts inside the spectrum.
As there is no known way to measure a priori the level of randomness required for any given
problem, we analyse the spectrum and discover that the two halves of the spectrum have their
distinctive characteristics. With this new understanding, a new ensemble approach is proposed
in this paper, which coalesces a number of points in the spectrum to form the final ensembles.
Empirically, we find that this new approach performs better than any single point in the spectrum
across a wide range of data sets. This new approach is an off-the-shelf solution, which provides a
high level of accuracy without the need of knowing or tuning to the level of randomness required.
This paper is presented as follows. A brief overview of existing decision tree randomisation
methods is provided in Section 2. It serves as a primer to decision tree randomisation. The algorithm to generate variable-random-trees is presented in Section 3. In Section 4, the different
ensemble methods used in our experiment is introduced, followed by Section 5, which presents a
comprehensive empirical evaluation of the spectrum as well as the proposed ensemble approach.
Section 6.1 details the key differences between the randomisation framework of Random Forests
and the proposed framework of variable-randomness. Other related work is provided in Section 6.2,
and we conclude in the last section.

2. Randomisation Methods for Decision Trees
Many randomisation methods have been proposed to produce diverse decision trees for ensembleclassifiers. In this section, as a general introduction to decision tree randomisation, we give an
overview of the ways in which they are applied. The following list of decision tree randomisation is
not meant to be exhaustive, the purpose of this list is to demonstrate the mechanism and side effects
of randomisation methods, which guides us in designing better approaches.
For any conventional decision tree algorithm, one deterministic model is produced for any given
training set. Randomisation helps to produce multiple variants of this deterministic model to fulfil
the requirement of ensemble learning. A common characteristic of popular methods is that the
same heuristic is used in every tree node, which often restricts the possible range of randomness
and reduces their impact on performance.
In the literature, most of the proposed randomisation methods can be grouped into three categories, depending on the dimension in which they are applied. The first category is to randomise
the instance dimension. This includes (i) Bagging (Breiman, 1996a) and Wagging (Bauer & Kohavi, 1999), which generate different sets of training examples through random sampling or assigning randomly generated probability distributions on the given training set; (ii) Output flipping
356

fiS PECTRUM

OF

VARIABLE -R ANDOM T REES

(Breiman, 2000) in which the classes of the training examples are flipped randomly according to
a ratio; and (iii) Adding random examples (Melville & Mooney, 2003) in which diverse classifiers
are constructed using additional artificial training examples. A conventional decision tree algorithm
is used to generate a model for each random sample of training examples. For type (i), a user has
no control over the degree of randomisation applied; for types (ii) and (iii), randomness is at the
expense of data integrity.
The second category is to randomise the feature dimension by randomly selecting a subset
of features before generating each model. A representative of this method is Random Subspace
(Ho, 1998) of which 50% of the features are randomly selected to produce models of an ensemble.
Random Subspace is not designed to adjust the level of randomness, the default setting as mentioned
is commonly used.
The third category is to randomise the test-selection at each decision node during the tree growing process. Since it is meant to produce variants of the deterministic model, the randomisation
is usually applied in a small degree at each node while maintaining the key deterministic characteristic. Examples of this category are Randomised C4.5 (Dietterich, 2000) and Random Forests
(Breiman, 2001). As reported by Breiman, the performance of Random Forests is not significantly
impacted by the different values of the parameter used.
For all the methods mentioned above, deterministic models are their common starting point.
Randomisations are then injected to produce diverse variants from these deterministic models. On
the contrary, a totally different approach is to start with complete-random models, for example,
Random Decision Trees (Fan et al., 2003) and Max-Diverse Ensemble (Liu et al., 2005). The
distinction between the two starting points is the inclusion of a deterministic heuristic. For any
method that uses any deterministic or a weakened heuristic in each node, their starting point is
deterministic models. These two starting points seem to be mutually exclusive, however, we provide
a way to connect them in order to maximize the possible range of randomness and, in turn, predictive
performance gain.
In this paper, we show that a largely unexplored set of randomised models can be found between
the extremes of both deterministic and complete-random models. While Random Forests provides a
mean to adjust its randomness, the degrees of randomness are constrained by the number of features
and the mandatory use of deterministic heuristic at each node. Details of this limitation will be
discussed in Section 6.
In the next section, we propose a new algorithm that constructs trees with a controllable randomisation in test-selection. It allows us to explore the whole spectrum of variable-random trees.

3. Trees with Variable Randomness
We name a tree VR-Tree when it is generated using random test-selection in some of its nodes.
In this section, we first describe the process of random test-selection and then the mechanism that
induces trees with a controllable mix of random and deterministic test-selections.
In the framework of conventional tree building algorithms, random test-selection can be used
as a direct replacement of deterministic test-selection. This is depicted in Algorithm 1. First, random test-selection randomly picks a feature from the list of available features to form a decision
node. Then, a nominal feature of m possible values will form m branches or a continuous-valued
feature with a random cut-point will form 2 branches. The random split-point selection procedure
357

fiL IU , T ING , Y U , & Z HOU

is described in Algorithm 2. This random test-selection becomes an alternative to the deterministic
test-selection in the mechanism to create variable-randomness.
Algorithm 1: VR-Tree(Dt , Q, ) - Building a Variable-Random Tree
Input: Dt - Training set, Q - Feature set,  - probability of using deterministic test-selection
Output: node - tree node
if all classes  Dt are the same or Q is empty or |Dt | < nmin then
/* nmin is the
minimum number of instances required before a split is
allowed. */
return a leaf with class frequency
else
let r be a randomly generated value, where 0 < r  1
if r   then
/* Deterministic Test-Selection. */
node  DeterministicT estSelection(Dt , Q)
else
/* Random Test-Selection. */
randomly select an   Q
construct a node with test label 
if  is a continuous-valued feature then
/* Handling a
continuous-valued feature. */
node.splitpoint  RandomSplit(, Dt )
D1  f ilter(Dt ,  > node.splitpoint)
D2  f ilter(Dt ,   node.splitpoint)
node.branch(1)  VR-Tree(D1 , Q, )
node.branch(2)  VR-Tree(D2 , Q, )
else
/* Handling a discrete feature. */
let {v1 ...vm } be possible values of 
for i  m do
/* m-ary split. */
Di  f ilter(Dt ,  == vi )
node.branch(i)  VR-Tree(Di , Q  , )
return node

Algorithm 2: RandomSplit(, Dt ) - Random split point selection
Input:  - a continuous-valued feature, Dt - training data
Output: a split point
r1  randomly select a value of  in Dt
r2  randomly select a value of  in Dt
while r1 == r2 do
r2  randomly select a value of 
return the mid point between r1 and r2
To generate variable-randomness, the test-selection process is split into two stages at each node.
The first stage decides which test-selection to use, either random or deterministic test-selection.
The second stage proceeds with the selected test-selection to produce the actual test for the node.
An  parameter is provided to control the probability of choosing deterministic test-selection over
358

fiS PECTRUM

OF

VARIABLE -R ANDOM T REES

the random one, where 0    1.  also approximates the percentage of deterministic nodes
generated in trees. Note that by setting  = 1, this procedure generates trees which are identical to
conventional decision trees; and by setting  = 0, it generates complete-random trees. The procedure
of the above mechanism can be found in Algorithm 1.
In the next section, we introduced the three ensemble methods used in our experiment based on
VR-Trees.

4. Ensemble Methods
Using VR-Tree as the base learner, we explore three ensemble methods that are employed in this
investigation. They are listed as follows:
 Aggregating, in which trees are generated from the same training data using the full set of
features.
 Subspacing, in which trees are generated with subsets of randomly selected features.  parameter is used to determine the percentage of features to be used.
 Bagging, in which trees are generated from a bootstrap sample using the full set of features.
The details of these ensemble methods are shown in Algorithms 3, 4 and 5.
Algorithm 3: Agg.VR-Trees(Dt , Q, N, )
Input: Dt - Training set, Q - Feature set, N - Number of trees,  - probability of using
deterministic test-selection
Output: E - a collection of trees
for i  N do
E  E  VR-Tree(Dt , Q, )
return E

Algorithm 4: Subspace.VR-Trees(Dt , Q, N, , )
Input: Dt - Training set, Q - Feature set, N - Number of trees,  - probability of using
deterministic test-selection,  - the percentage of features used, where 0 <   1
Output: E - a collection of trees
for i  N do
Qs  randomly generate a set percentage  of features from Q
E  E  VR-Tree(Dt , Qs , )
return E
While none of these ensemble methods are new, the incorporation of VR-Tree as the base learner
help to unleash the potentials of these methods. The predictive performance gain is shown in Section
5. Note that Subspacing is equivalent to the Random Subspace method (Ho, 1998) when =50%.
Since =50% provides the maximum number of distinct subspaces, which is an important factor
to increase diversity, we will use =50% as the default setting for Subspacing. Also, notice that
Bag.VR-Trees with  = 1 is equivalent to the conventional Bagging method (Breiman, 1996a).
359

fiL IU , T ING , Y U , & Z HOU

Algorithm 5: Bag.VR-Trees(Dt , Q, N, )
Input: Dt - Training set, Q - Feature set, N - Number of trees,  - probability of using
deterministic test-selection
Output: E - a collection of trees
for i  N do
Db  generate a bootstrap sample from Dt
E  E  VR-Tree(Db , Q, )
return E
We use probability averaging to combine the outputs from individual models of an ensemble.
In order to predict a class given a test case, the predicted class is obtained by:
N
X
ni,y
), y  Y
arg max(
y
ni

(1)

i=1

where N is the number of trees in an ensemble, ni,y is the number of class y training instances and
ni is the total number of training instances at a leaf of a tree i in which the test case falls into.

5. Empirical Study of The Spectrum
We design our experiment in four parts. The first part investigates the predictive performance spectrum of Aggregating, Bagging and Subspacing using VR-Trees. We then use the result to characterize the two-halves of the spectrum. The second part examines the diversity of base learners generated by these ensembles using the strength and correlation curves as defined by Breiman (2001).
This part highlights the range of randomness one can achieve by using VR-Trees. The third part
explores an alternative to using only a single  value to produce models in an ensemble. This alternative combines a number of points in the spectrum, which is our proposed ensemble method in
this paper. The fourth part investigates the strengths and weaknesses of the proposed method.
Forty-five data sets from the UCI repository (Asuncion & Newman, 2007) are used in this paper.
The characteristics of all these data sets are provided in Appendix A. Ten-fold cross-validation is
conducted for each data set and the average error rate is reported. 100 trees are used for each
ensemble. Random Forests and C5 Boosting (www.rulequest.com) are used as benchmarks, in
addition to Bagging, Subspacing and Aggregating of VR-Trees. We use the Friedman test with the
Bonferroni test as the post-hoc test at 95% confidence level to compare classifiers (Demsar, 2006).
For the Random Forests implementation used in this paper, the default settings of mtry =
floor(sqrt(q)) and nodesize=1 is used, where mtry is the number of features randomly
sampled before each split, q is the number of features and nodesize is the minimum size of
terminal nodes. The implementation is taken from R (www.r-project.org).
Our implementation including of VR-Trees is based on C4.5 (Quinlan, 1993). The default
C4.5s stop-splitting rules are applied: (i) the minimum number of training samples nmin = 4 are
required before splitting is considered, and (ii) the deterministic test-selection stops splitting when
all possible splits report negative scores. Gain ratio is used as the test-selection criterion. Probability
averaging is implemented with curtailment (Zadrozny & Elkan, 2001), where the minimum leaf size
for probability estimation is always greater than one. These default settings are used for VR-Trees
in this paper.
360

fiS PECTRUM

OF

VARIABLE -R ANDOM T REES

5.1 The Predictive Performance Spectrum of Aggregating, Bagging and Subspacing

Figure 1: The spectrum of predictive performance for Aggregating, Bagging and Subspacing, as
well as Bagging plus Subspacing: error rates against , average over forty-five data sets.
21
Agg.VR-Trees
Bag.VR-Trees
Subspace.VR-Trees
Bag-Subspace.VR-Trees

Average Error %

20

19

18

17

16

15
0

0.2

0.4

0.6

0.8

1



Figure 1 shows the spectrum of performance 1 for four ensemble methods using VR-Trees as
the base models. Note that the conventional Bagging and Random Subspace are two points on the
deterministic-end ( = 1) of the spectrum for Bag.VR-Trees and Subspace.VR-Trees; and MaxDiverse ensemble is at the complete-random-end ( = 0) of the Aggregating spectrum. Figure 2
shows the results of Friedman tests for each of Aggregating, Bagging and Subspacing and we have
the following observations.
i From Figure 2, it is interesting to note that the best operating region for Agg.VR-Trees is having
 values between 0.1 and 0.6. This shows that Max-Diverse ensemble which operating at  =
0 can improve its performance by moving further into the middle of the spectrum. The best
operating region for Bag.VR-Trees is between 0.1 and 0.6, which is mainly in the first half of
the spectrum. This is significantly different from what the conventional Bagging is normally
applied at, namely  = 1. The best operating region for Subspace.VR-Trees is between 0.4 and
0.8. This is also different from what Random Subspace is normally applied at  = 1.
ii A balanced mix of random and deterministic heuristics, i.e.,  = 0.5, produces the best ensemble
or no significantly difference to the best ensemble for any one of the three ensemble methods.
iii Out of the four curves shown in Figure 1, Agg.VR-Trees have the largest swing in performance,
followed by Bag.VR-Trees and Subspace.VR-Trees. This is expected as the two end-points in
the Agg.VR-Trees curve represents a single deterministic model and an ensemble of completerandom models. As  decreases from 1 to 0.5, a substantial improvement in predictive performance for Agg.VR-Trees, takes effect due to the increased diversity in the ensemble, which
reduces the average error rate from 20.6% to 16.0%.
iv Although both Bag.VR-Trees and Agg.VR-Trees perform best in the region of 0    0.5,
the result in Figure 1 indicates that they have no significant difference in terms of predictive
1. Averaged over forty-five data sets.

361

fiL IU , T ING , Y U , & Z HOU

accuracy. Because of additional computational requirement to generate bootstrap samples for
Bagging, Aggregating becomes the preferred method in the first half of the spectrum. Bagging
and Subspacing are preferred to Aggregating in the second half of the spectrum because the
latter is uncompetitive in that region.
v The use of both Bagging and Subspacing in a single ensemble is not recommended, as shown
by the result in Figure 1 that Bag-Subspace.VR-Trees always performs worse than its parents,
Bag.VR-Trees and Subspace.VR-Trees.

Figure 2: Friedman test results for classifiers produced by (a) Aggregating, (b) Bagging, (c) Subspacing of VR-Trees, from eleven  values in the spectrum over forty-five data sets. We use Bonferroni test as the post-hoc test and  = 0 as the control condition for each comparison. The vertical
axis indicates the  values, and the horizontal axis indicates the rank values. The circle is the average rank for each  value and the bar indicates the critical values for a two-tailed test at 95%
significance level. When two classifiers having no overlapping bars, it indicates that they are significantly different. Significantly worse results are presented in dotted bars (coloured in red) located
on the right-hand-side of the diagram. The best results are presented in solid bars (coloured in blue)
or the left-most bar in each diagram.



0

0

0.1

0.1

0.2

0.2

0.3

0.3

0.4

0.4

0.5

0.5

0.6

0.6

0.7

0.7

0.8

0.8

0.9

0.9

1

1
2

3

4

5

6

7

8

9

10

11

3

(a) Agg.VR-Trees,  = 0.4 has the highest ranking

4

5

0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
4

4.5

5

5.5

7

8

9

(b) Bag.VR-Trees,  = 0.6 has the highest ranking



3.5

6

6

6.5

7

7.5

8

8.5

(c) Subspace.VR-Trees,  = 0.5 has the highest ranking

362

fiS PECTRUM

OF

VARIABLE -R ANDOM T REES

We summarise the characteristics of the two halves of the spectrum in Table 1. In the next subsection, we continue our analysis over the various points of the spectrum in relation to generalisation
error.
Table 1: Characteristics of the two halves of the spectrum.
Complete-Random-end,   [0, 0.5]
 Models at this extreme end are
generated in a completely random
fashion.
 Candidate models are all possible
trees of larger sizes.

Deterministic-end,   (0.5, 1]
 Models are variants of a deterministic model.
 Candidate models are models of
smaller sizes (because each model
reaches pure leaves early).
 Maintaining high predictive accuracy while providing some degree of diversity.
 Subspacing and Bagging are preferred in this region.

 Models have a higher diversity.

 Aggregating is preferred in this
region.

5.2 Strength-Correlation Analysis
In this section, we examine the strength and correlation profiles produced by Aggregating, Bagging
and Subspacing. Firstly, we illustrate the relationship between generalisation error, strength and
correlation using a map. Secondly, we plot the strength-correlation curves for the actual data sets to
characterise their behaviours. Thirdly, we continue the analysis from Section 5.1 to further explore
the two halves of the spectrum in light of the strength-correlation analysis.
Breimans (2001) inequality, P E  (1  s2 )/s2 is useful for discerning the relationship between generalisation error P E, strength s and correlation . Briefly, the strength of an ensemble
measures the expected margin function of the ensemble. A margin function is the probability of
correct classification minus the maximum probability of incorrect classification of an instance. Correlation of an ensemble is a measure on how similar the predictions are among individual trees. A
high reading of correlation indicates that individual trees are making similar predictions. We use
the strength and correlation estimation procedures of Buttrey and Kobayashi (2003) as they are the
corrected version of Breiman (2001). To make this paper self-contained, the estimation procedures
are given in Appendix C.
Figure 3 shows a map of strength-correlation profiles with grey scale indicating the generalisation error. We can see that the error rate is lower at the high-strength and low-correlation corner
(at the bottom-right), where the error rate is lower with darker grey level. As for all the ensemble
classifiers in general, their goal is to get themselves to a region where the estimated error rate can
be as low as possible. Aggregating with different values of  typically spans in the fashion of either
curve (a) or curve (b) as shown in Figure 3, where each curve starts from  = 0 (at the bottom-left)
to  = 1 (at the top-right). For (a), the lowest error rate can be found at  = 0. For (b), lower error
363

fiL IU , T ING , Y U , & Z HOU

rates can be found with a larger value of . However, error rates also increase when  approaches
1. In this case, a search would be necessary to determine the optimal .
Figure 3: Ensemble generalisation error distribution using Breimans inequality on strength and
correlation. Curves (a) and (b) represent two typical spans of Agg.VR-Trees with 0    1.

=1

(a)
=0

(b)

Figure 4 shows that the three different ensemble methods provide different ranges of strength
and correlation profile. The most effective way to use VR-Trees is Aggregating. This is because
it consistently produces the longest span of strength-correlation curve in each of the data sets we
used. Bagging usually has the second longest span, followed by Subspacing. Note that Aggregating
usually spans in both strength and correlation dimensions; whereas Bagging and Subspacing have
significantly smaller span, especially in the correlation dimension. Table 2 shows the range between
the minimum and the maximum values of strength as well as correlation, averaged over forty-five
data sets. The result shows that Aggregating produces the largest range of trees in comparison with
Bagging and Subspacing. Most interestingly, the best of Aggregating is usually located in a lower
or similar error region to those of Bagging and Subspacing.
Table 2: Average ranges of strength and correlation over the forty-five data sets.
Strength Correlation
Agg.VR-Trees(  [0, 1])
0.135
0.696
0.136
0.160
Bag.VR-Trees(  [0, 1])
0.069
0.088
Subspace.VR-Trees(  [0, 1])
It is also interesting to note that =0.5 is always close to or at the changing corner between
strength-varying leg and correlation-varying leg in all the examples shown in Figure 4. This means
that  = 0.5 is either close to or at the lowest generalisation error region, if the ensemble exhibits
364

fiS PECTRUM

OF

VARIABLE -R ANDOM T REES

Figure 4: Strength-Correlation distribution of Aggregating, Bagging and Subspacing for different
values of . The solid-filled marks represent =0.5. For Aggregating, the first half of the  range
(  [0, 0.5]) is characterised by lower correlation and lower strength, and it is located at the bottomleft corner of the diagrams. The second half (  [0.5, 1]) is characterised by higher correlation and
higher strength, and it is located at the top-right corner of the diagrams.
Dataset: coding

Dataset: credit-a

1

1
Agg.VR-Trees
Bag.VR-Trees
Subspace.VR-Trees

0.8

correlation

correlation

0.8

Agg.VR-Trees
Bag.VR-Trees
Subspace.VR-Trees

0.6

0.4

0.2

0.6

0.4

0.2

0
0.2

0.25

0.3

0.35

0.4

0.45

0
0.45

0.5

0.5

0.55

strength

Dataset: DNA

Agg.VR-Trees
Bag.VR-Trees
Subspace.VR-Trees

0.8

correlation

correlation

0.7

1
Agg.VR-Trees
Bag.VR-Trees
Subspace.VR-Trees

0.8

0.6

0.4

0.2

0.6

0.4

0.2

0

0
0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

0.5

0.55

0.6

0.65

strength

0.7

0.75

0.8

0.85

strength

Dataset: segment

Dataset: sick

1

1
Agg.VR-Trees
Bag.VR-Trees
Subspace.VR-Trees

Agg.VR-Trees
Bag.VR-Trees
Subspace.VR-Trees

0.8

correlation

0.8

correlation

0.65

Dataset: ionosphere

1

0.6

0.4

0.2

0.6

0.4

0.2

0
0.65

0.7

0.75

0.8

0.85

0.9

0
0.78

0.95

0.8

0.82

0.84

strength

0.86

0.88

0.9

0.92

0.94

0.96

0.7

0.75

strength

Dataset: threeOf9

Dataset: tic-tac-toe

1

1
Agg.VR-Trees
Bag.VR-Trees
Subspace.VR-Trees

Agg.VR-Trees
Bag.VR-Trees
Subspace.VR-Trees

0.8

correlation

0.8

correlation

0.6

strength

0.6

0.4

0.2

0.6

0.4

0.2

0
0.4

0.5

0.6

0.7
strength

0.8

0.9

0
0.35

1

365

0.4

0.45

0.5

0.55
0.6
strength

0.65

fiL IU , T ING , Y U , & Z HOU

curve (b) like behaviour as in Figure 3. Thus,  = 0.5 often serves as an upper limit in a range of 
values that performs well.
Combining the above result with the analysis on the two halves of the spectrum from Section
5.1, we find that Aggregating exhibits the following characteristics in most of the data sets in Figure
4:
 At the first-half of the spectrum, strength increases rapidly from  = 0 and slows down at
about  = 0.5; however, correlation only varies in a small degree or not vary at all.
 At the second-half of the spectrum, correlation increases rapidly from  = 0.5 and peaks at 
= 1. In this range, both strength and correlation are very high, error rates are not optimal.
In summary,  for better performing Aggregating models can be found in the range between 0
and 0.5. Most single operating points   [0, 0.5] have been shown to work well in Section 5.1.
In the next section, we show an alternative approach that achieves even better result by using
the range of   [0, 0.5] that we have identified thus far.
5.3 Coalescence of Different Operating Points
In this section, we show that combining single models from multiple points in the spectrum will
achieve similar or better performance as compared to using any single point in the spectrum. We
coalesce VR-Trees with  sampled at a fixed interval. For example, to form a 100-model ensemble,
we construct trees with  = {0, 0.005, 0.01, ..., 0.495}. We call this approach Coalescence.
The Coalescence approach is appealing because we do not need to know which  value produces
the most accurate ensemble for any given data. By introducing members of different talents from
the random-half of the spectrum; Coalescence forms a committee of different members. In this
committee, as far as we know, some members are good at approximating non-axis-parallel boundary
and some members are good at avoiding over-fitting. The make-up of this committee helps to handle
unforeseeable conditions that arise in training sample.
As a result, Coalescence provides a comparable predictive performance to a search-for-the-best approach (Liu & Ting, 2006) without the cost of searching for the optimal  value. In Figure 5,
Coalescence is shown to be better than any single operating point in the first-half of the spectrum
using the Friedman test, over forty-five data sets.
An additional comparison is also conducted with some other well known tree ensemble classifiers. The complete result for Coalescence, Aggregating VR-Trees (=0) (i.e. Max-Diverse Ensemble), Bagging, Subspacing (=1) (i.e. Random Subspace), C5 Boosting and Random Forests are
presented in Table 3. The average error rates over forty-five data sets are provided in the last row of
the table. Figure 6 shows the result of the Friedman test and we have the following observations:
 Coalescence ranks the highest among the five benchmarking ensembles in the Friedman test.
 The second highest ranking ensembles: Random Forests and C5 Boosting have almost identical
ranking in the Friedman test, and similar average error rates.
 The third highest ranking ensemble is Random Subspace; and the lowest ranking ensembles are
Bagging and Aggregating VR-Trees ( = 0). Both ensemble methods have similar average
error rates.
366

fiS PECTRUM

OF

VARIABLE -R ANDOM T REES

Figure 5: Friedman test result for comparing Coalescence with five operating points of Agg.VRTrees in the first-half of the spectrum. Horizontal axis indicates the rank values. Coalescence ranks
top as compared to different points in the first-half of the spectrum.

Coalescence
Agg
( = 0)
( = 0.1)
( = 0.2)
( = 0.3)
( = 0.4)
( = 0.5)
2.5

3

3.5

4

4.5

5

5.5

6

Figure 6: Friedman test result for comparing Coalescence with five benchmarking methods. Horizontal axis indicates the rank values. Notice that Coalescence is the only method that is significantly
better than Agg. = 0 (Max-Diverse Ensemble) and Bag. = 1 (Bagging).

Coalescence
Agg
( = 0)
Bag
( = 1)
Subspace
( = 1)
C5 Boost
Random
Forests
2

2.5

3

3.5

367

4

4.5

5

fiL IU , T ING , Y U , & Z HOU

Table 3: Experimental results: Average error rate of ten-fold cross-validation.
data sets
abalone
anneal
audiology
autos
balance
breastw
breasty
chess
cleveland
coding
credit-a
credit-g
dna
echo
flare
glass
hayes
hepatitis
horse
hypo
ionosphere
iris
labor
led24
led7
liver
lymph
nursery
pima
post
primary
satimage
segment
sick
solar
sonar
soybean
threeOf9
tic-tac-toe
vehicle
vote
waveform21
waveform40
wine
zoo
mean

Coalescence
30.0
1.5
17.2
18.1
14.4
3.0
28.7
0.7
42.9
16.4
12.3
23.1
5.3
33.5
18.6
21.0
18.1
18.0
13.3
0.8
5.7
4.7
7.0
27.9
26.7
27.0
14.9
0.9
23.4
40.0
55.2
8.8
2.1
2.1
29.4
15.9
5.4
0.0
3.0
24.5
4.1
14.5
15.7
3.4
1.0
15.6

Agg.
VR-Trees
(=0)
30.2
1.4
17.7
22.5
12.3
2.4
25.9
1.6
41.6
16.8
13.0
25.7
26.5
34.2
19.2
22.9
21.9
15.5
17.9
1.7
8.5
4.7
3.3
30.3
26.9
27.9
14.3
2.2
24.6
36.7
57.2
10.4
3.1
5.7
30.3
15.9
6.0
0.6
9.7
27.1
5.3
14.7
17.0
1.1
2.0
16.8

Bag.
VR-Trees
(=1)
30.7
3.5
16.2
19.5
18.1
3.4
27.3
2.3
44.2
24.2
12.8
22.7
6.0
29.0
17.7
21.5
17.5
20.0
15.2
0.8
5.4
4.0
10.7
28.1
26.3
27.3
20.4
3.7
24.0
37.8
56.6
9.0
2.4
2.2
27.2
24.0
6.6
1.4
14.3
24.9
4.6
16.1
16.5
3.4
3.0
16.7

368

Subspace.
VR-Trees
(=1)
30.0
3.2
20.3
14.7
11.4
3.0
25.1
1.8
41.9
16.7
13.0
24.8
3.7
32.0
17.1
21.5
17.5
18.6
16.0
1.3
5.4
6.0
12.0
29.1
27.5
31.9
15.6
5.9
23.2
32.2
51.9
8.3
2.3
4.3
27.8
18.7
5.3
10.0
22.3
25.4
4.6
14.7
15.3
2.3
4.0
16.4

C5
Boost
31.1
5.0
15.0
15.6
18.9
3.1
26.9
0.3
41.6
15.4
14.3
22.4
4.8
37.4
17.5
21.4
16.9
14.1
22.5
0.8
5.4
4.0
15.7
28.1
27.8
29.6
19.1
0.9
25.0
30.0
56.9
8.1
1.8
2.2
25.7
15.9
6.2
0.0
1.2
23.3
4.8
15.6
15.1
5.6
3.0
15.9

Random
Forests
30.9
9.7
20.8
15.2
16.3
3.4
29.7
1.1
41.9
17.5
13.0
23.2
3.4
32.0
18.5
21.0
16.9
17.3
14.1
1.0
6.5
3.3
5.0
28.5
26.3
26.1
16.9
2.3
23.2
32.2
56.9
8.1
2.1
2.1
27.2
13.9
5.7
0.8
1.3
26.1
4.1
14.7
14.9
2.3
5.0
15.6

fiS PECTRUM

OF

VARIABLE -R ANDOM T REES

 Of all the ensembles, only Coalescence is shown to be significantly better than Aggregating
VR-Trees ( = 0) and Bagging in the Friedman test.
It is interesting to note that when employing an oracle to find the best , the optimal error rate
is 14.6% 2 (Liu & Ting, 2006). Thus, the Coalescence approach comes close to this optimal result
without using any oracle.
Figure 7 shows the predictive performance of Coalescence in relation to Aggregating in eight
data sets. Coalescence sometimes outperforms its parents (  [0, 0.5]), and often comes close to
the best performing Aggregating.
5.4 Strengths and Weaknesses
This section will examine the strengths and weaknesses of VR-Tree. Although Coalescence is not
significantly better than Random Forests in the Friedman test, Table 3 clearly shows that Coalescence is better than Random Forests in some data sets but worse in others. This section will also
examine some of the conditions under which Coalescence performs better than Random Forests and
vice versa.
We find that the ensembles of complete-random-trees have the following strengths: 1) capable
of approximating non-axis-parallel boundary, and 2) they are highly stable learners in terms of
Pointwise Hypothesis Stability (Bousquet & Elisseeff, 2002). An analysis that is based on Pointwise
Hypothesis Stability can be found in Appendix B. To verify the complete-random-trees ability to
approximate non-axis-parallel boundary, in Figure 8, we provide a visualization example using a
Gaussian mixture data set (Hastie, Tibshirani, & Friedman, 2001). Figure 8 shows that an ensemble of complete-random-trees (using 100 trees) is able to better approximate the non-axis-parallel
boundary as compared to a single deterministic tree.
Moreover, as described in the analysis in Appendix B, one of the strengths of complete-random
trees is also one of their weaknesses: complete-random trees trend to over-fit; and this problem
stems mainly from the ability to approximate non-axis-parallel boundary. We also find that the
overfitting problem is aggravated by irrelevant attributes, class noise and small training size as
shown in the following empirical examination.
We denote X as the input space and Y as the output space. A learning algorithm outputs a
function that approximates the underlying true function f by inspecting the training set. A training
set S = {zi }m
i=1 , where zi = (xi , yi ) and yi = f (xi ), contains m i.i.d. examples in Z = X  Y
that are drawn from uniform distribution in the instance space. We define the input space with two
variables, xi = hi,a , i,b i and the output space with two possible classes Y = {+1, 1}. In this
case, the instance space is a square from point (a = 1, b = 1) to point (a = 1, b = 1).
We define the two concepts as follows:
(
+1 if (i,a > i,b )
, xi = hi,a , i,b i
Concept A: f A (xi ) = yi =
1 else
(
+1 if (i,a > 0)
Concept B: f B (xi ) = yi =
, xi = hi,a , i,b i
1 else
2. Averaged over the same forty-five data sets used in this paper.

369

fiL IU , T ING , Y U , & Z HOU

Figure 7: Detail results comparing Coalescence with Aggregating in eight data sets.
Dataset: abalone
32.5
Agg.VR-Trees
Coalescence

Dataset: balance
24

32

Agg.VR-Trees
Coalescence

Error %

20

31

18
16

30.5
14

30
0

0.2

0.4

0.6

0.8

12

1

0

0.2

0.4

0.6



0.8

1



Dataset: DNA

Dataset: ionosphere

30

9
Agg.VR-Trees
Coalescence

Agg.VR-Trees
Coalescence

8.5

25

8
7.5
Error %

Error %

20
15

7
6.5

10

6
5

5.5

0

5
0

0.2

0.4

0.6

0.8

1

0

0.2

0.4



Dataset: nursery

0.8

1

0.8

1

0.8

1

Dataset: segment
10

Agg.VR-Trees
Coalescence

Agg.VR-Trees
Coalescence

8

8

6

6

Error %

Error %

0.6


10

4

2

4

2

0

0
0

0.2

0.4

0.6

0.8

1

0

0.2

0.4



0.6


Dataset: solar

Dataset: waveform 40
28

Agg.CR-Tree
Coalescence

34

Agg.VR-Trees
Coalescence

26
24
Error %

32
Error %

Error %

22

31.5

30

28

22
20
18
16

26

14
0

0.2

0.4

0.6

0.8

1

0



0.2

0.4

0.6


370

fiS PECTRUM

OF

VARIABLE -R ANDOM T REES

Figure 8: Visualisation of a non-axis-parallel decision boundary on Gaussian mixture data using an
ensemble of complete-random-trees as compared to a single deterministic tree (C4.5).

Error 11.3%

Error 14.2%

(a) Complete-random trees

(b) Single deterministic tree

positive class
negative class

(c) Training sample

(d) Actual decision boundary

Concept A is useful for illustrating the ability to approximate non-axis-parallel boundary. Concept
B is an axis-parallel concept, which is used as a control condition in this experiment. We conduct
four experiments using (a) a training sample of k = 1024 instances, (b) a training sample with
irrelevant attributes, (c) a training sample with class noise and (d) a size-reduced training sample of
k = 64. We train ensemble models using Coalescence, VR-Trees ( = 0,  = 0.5) and Random
Forests. We use 100 trees for each ensemble. Finally, we evaluate the models by 10000 lattice
samples from the instance space, and the average error rate over 10 runs is reported.
Table 4 shows that:
 VR-Trees ( = 0) is the best performer in approximating non-axis-parallel boundary in concept
A, even with small training size of k = 64. However, it is the worst performer in axis-parallel
concept B in both k = 1024 and k = 64.
 VR-Trees ( = 0.5) performs the best under class noise and irrelevant attribute conditions in
both concepts A and B. When irrelevant attributes are added, the error rate of VR-Trees ( =
0) increases at a faster rate as compare to VR-Trees ( = 0.5). Similarly, class noise affects
the complete-random-end ( = 0) of the spectrum more severely than the middle point of the
spectrum ( = 0.5).
371

fiL IU , T ING , Y U , & Z HOU

Table 4: Averaged error rates of VR-Trees ( = 0,  = 0.5), Coalescence and Random Forests on
Concepts A and B. The default number of training samples is k = 1024, unless otherwise specified.
The best error rate in each row is bold faced.
VR-Trees
VR-Trees
( = 0) Coalescence ( = 0.5) Random Forests
A
1.4%
1.6%
2.0%
1.9%
A, 8 irr. att.
7.6%
3.4%
3.0%
3.2%
A, 40% class noise
30%
21.9%
14.7%
33.4%
A, k = 64
6.7%
8.9%
10.4%
7.3%
B
0.3%
0%
0%
0%
B, 8 irr. att.
5.2%
0%
0%
0%
B, 40% class noise
30.7%
4.2%
2.6%
32.5%
B, k = 64
2.4%
1.1%
1.1%
0.8%
 Between Coalescence and Random Forests, Coalescence performs better under class noise condition; but Random Forests is a better performer under small training size in both concepts A
and B.
The above empirical results show that, while complete-random trees, i.e. VR-Trees ( = 0) are
good at approximating non-axis-parallel boundary, they are easily over-fitted to irrelevant attributes,
class noise and small training sample. Although it is the case, we find that Coalescence is a way
to manage these propensities without a search for a specific  value. In Table 4, we find that
Coalescence tends to have performance closer to the better performing learner of either VR-Trees
( = 0) or VR-Trees ( = 0.5); the only exception is when learning the non-axis-parallel concept A
with a small training sample.
As to further our understanding on how irrelevant attributes affect Coalescence, we perform
a simple check on the eighteen data sets in which Random Forests performs better (ignoring two
artificial data sets: led and waveform in which we have already known the results with and without
irrelevant attributes).
We remove the less-important half of all attributes according to Random Forests variableimportance (Breiman, 2001). We then evaluate these eighteen data sets again using Coalescence
and Random Forests under the same 10-fold cross validation. The result in Table 5 shows that Coalescence performs better in ten out of the eighteen data sets; and Random Forests performs better
only in four data sets. This result indicates the influence of irrelevant attributes on Coalescence is
greater than that on Random Forests. Among the eighteen data sets, Coalescences error rates are
reduced by more than half in labor, hayes and tic-tac-toe data sets. The result indicates that further
management of irrelevant attributes can indeed improve the performance of Coalescence.

6. Related Work
In this section, we first highlight the differences between VR-Trees and Random Forests in their
capability to vary the degree of randomness, so to distinguish VR-Trees from others. Then, in order
to better position VR-Trees, we discuss various different decision tree ensembles that are related to
VR-Trees.
372

fiS PECTRUM

OF

VARIABLE -R ANDOM T REES

Table 5: Evaluation of Coalescence and Random Forests with respect to the condition of irrelevant attribute: eighteen data sets are listed with the less-important half of all attributes removed.
Boldfaced indicates improvement in error rate using the reduced number of attributes
Coalescence
Random Forests
data sets Full att. Half att. Full att. Half att.
autos
18.1
13.6
15.2
14.6
cleveland
42.9
45.2
41.9
43.9
dna
5.3
3.9
3.4
3.2
echo
33.5
39.0
32.0
42.8
flare
18.6
19.0
18.5
18.9
hayes
40.6
18.1
41.3
16.9
hepatitis
18.0
16.7
17.3
17.4
iris
4.7
4.0
3.3
3.3
labor
7.0
3.3
5.0
5.0
liver
27.0
33.6
26.1
33.1
pima
23.4
24.5
23.2
24.5
post
40.0
37.8
32.2
36.7
satimage
8.8
9.4
8.1
9.3
solar
29.4
30.3
27.2
28.7
sonar
15.9
13.0
13.9
16.4
tic-tac-toe
18.8
3.0
18.3
1.3
vote
4.1
4.4
4.1
4.4
wine
3.4
1.7
2.3
2.8

6.1 Relationship with Random Forests
It is interesting to note that Breiman (2001) found that Random Forests accuracy is not overly
sensitive to the value of F , which is the parameter intended to vary the degree of randomness. The
F parameter corresponds to the  parameter in VR-Tree. Yet, our experiments in section 5.1 clearly
shows that varying the degree of randomness (using ) has a significant impact on the predictive
performance of the resulting ensembles. It is thus important to identify the differences between the
two ensembles that cause the different behaviours. We will do so in the following paragraphs.
Algorithm 6: The random feature selection framework of Random Forests
Input: Dt - Training set, F - number of features
Output: node: tree node
randomly select F features from all available features
D 0 = a subset of Dt according to the F features
node = DeterministicTestSelection(D 0 )
return node
On the surface, Random Forests is very similar to Agg.VR-Trees because both of them use the
deterministic and random test-selections in the tree induction process. However, they differ in the
way in which the test-selection is applied in each decision node. The randomisation framework of
Random Forests is described in Algorithm 6.
373

fiL IU , T ING , Y U , & Z HOU

In applying test-selection, Random Forests applies both random feature selection and deterministic test-selection in each node; however VR-Tree only applies either random or deterministic
test-selection in each node.  controls the probability of whether deterministic or random testselection is applied in each node; whereas the mixed application of the two selection processes in
each node constrains the amount of randomness that can be introduced to Random Forests. In
case of Random Forests, F only controls the number of features to be randomly selected. Once selected, the deterministic test-selection chooses the best feature. Thus, if the best feature is readily
selected in the first place, then no matter what the F is, the best feature will always be chosen
by the deterministic test-selection. This agrees with Breimans observation that the error rate is not
overly sensitive to the different values of F in Random Forests.
The accessibility to the different degrees of randomness directly affects the diversity of models
that are produced, Figure 9 shows the strength-correlation curves for Random Forests (using all the
available F values, 19 for segment data and 21 for waveform21 data), in comparison with Agg.VRTrees using eleven  values sampled with an equal interval. We find that Random Forests produces
ensembles which are highly correlated to each other and many of which have similar strength. The
same result is also reported by Breiman (2001). Note that the fitted curves for Random Forests are
visual aids which do not mean to represent accessibility between points. In contrast, Agg.VR-Trees
produces ensembles which are accessible along the curve and spread along a wider range.
In a nutshell, the randomisation framework used in Random Forests significantly limits its ability to scale to the different levels of randomness when the total number of features is small. On the
other hand, VR-Trees is able to scale to different levels of randomness regardless of the number of
features.
6.2 Other Related Work
An approach to search for the best  value is proposed (Liu & Ting, 2006). This approach searches
the optimal  value based on average progressive training errors. An estimated optimal 
b for a task
is generated as follows:
N
1 X
err(, i, Dt )]
(2)

b = arg min[
00.5 N
i=1

where N is the total number of trees in an ensemble, err() returns the training error rate of an
ensemble of size i, while Agg.VR-Trees is set at  with training set Dt . After obtaining 
b, i.e., the
best performing , the ensemble employs the model with 
b for actual predictive tasks. Note that
each unpruned tree in the ensemble stops growing if the number of training examples in the node is
four or less (the default setting as in C4.5.) This avoids generating zero training error trees. Though
the method has comparable prediction performance to that of the Coalescence approach, it requires
a substantial computational overhead where the ensembles of all  values in the search must be
produced.
In contrary to common belief, Fan et al (2003) first propose the use of complete-random trees
to produce accurate predictive models. Fan (2004) explains the reason why the combination of
complete-random trees and probability averaging produces accurate models. Using completerandom trees, Liu et al. (2005) show that their ensembles perform comparably to Bagging and
Random Forests.
Cutler and Zhao (2001) propose PERT (Perfect Random Tree Ensembles) which randomises
the test-selection for continuous-valued features to achieve higher randomisation. At each potential
374

fiS PECTRUM

OF

VARIABLE -R ANDOM T REES

Figure 9: Strength-Correlation plots for Random Forests and Aggregating VR-Trees at different
F and  values. Aggregating VR-Trees has a wider range of correlation as compared to Random
Forests.
Dataset: segment
1
Agg.VR-Trees
Random Forests

0.9
0.8
correlation

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.55

0.6

0.65

0.7

0.75
0.8
strength

0.85

0.9

0.95

Dataset: waveform 21
1
Agg.VR-Trees
Random Forests

0.9
0.8
correlation

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.34

0.36

0.38

0.4

0.42 0.44
strength

375

0.46

0.48

0.5

0.52

fiL IU , T ING , Y U , & Z HOU

split, PERT first randomly selects two examples of different classes from the local training set. A
feature is then selected randomly. A cut point on the feature is randomly selected between the values
of that two samples. A leaf is formed if two examples of different classes cannot be found after ten
trials. PERT is also shown to be competitive to Bagging and Random Forests. We believe that this
method is likely to be close to the complete-random-end of the spectrum. However, it is unclear on
how the different degrees of randomisation can be introduced to the PERT framework.
Extra-Trees (Geurts, Ernst, & Wehenkel, 2006) relies on the same framework as in Algorithm
6. However, a random split-selection is used instead of the deterministic split-selection. Different
from PERT, Extra-Trees cut points are randomly selected between the maximum and the minimum
values of the given samples. As compared to PERT, Extra-Trees requires an additional data-scan
at every node of a tree to find the maximum and minimum values, which can be a disadvantage
in terms of computational complexity. For categorical features, a random subset split is used in
Extra-Trees. This method is also shown to be competitive to Random Forests.
Robnik-Sikonja (2004) reports an improved version of Random Forests by using five different
deterministic test-selection criteria instead of one. This achieves the effect of increased diversity by
producing different variants of deterministic models  the same starting point as Random Forests,
Bagging and Random Subspace.
MultiBoosting (Webb, 2000; Webb & Zheng, 2004) is another approach that combines more
than one type of ensembles. MultiBoosting is a tightly-coupled method that incorporates bagging
(random sample weight assignment) into the main Boosting procedure (the incremental sample
weight modifier) in order to increase model diversity. Like Bagging and Random Forests, it is
focusing on increasing diversity at the deterministic-end of the spectrum.
There are many algorithms reported in the literature, other than those listed in this paper, suggesting different ways to combine models generated from one algorithm or different algorithms,
(e.g., see Breiman, 1996b; Ting & Witten, 1999; Perrone & Cooper, 1993) . All of these require
some kind of learning or estimation in order to either selectively choose some available models
or build a meta-model to combine them. The Coalescence approach is more simple than these
approaches because it does not require to learn a meta-model or/and some kind of estimation.

7. Conclusions
In this paper, we make the following contributions:
 Propose a new algorithm, which generates a spectrum of VR-Trees that span between completerandom trees and deterministic trees. We show that different points in the spectrum can be
significantly different in terms of predictive accuracy. This opens up new opportunities for
decision tree ensembles.
 Show that existing ensemble methods such as Bagging, Random Subspace, and Max-Diverse
Ensemble are only at either end of the spectrum. The performance of these ensembles can
be improved by moving towards the middle of the spectrum and the improvements can be
significant.
 Discover that the two halves of the spectrum have their distinctive characteristics, separated by
a critical point of =0.5. This point has two interesting characteristics. First, it produces an
equal percentage of random and deterministic decision nodes in VR-Trees. Second, it often
376

fiS PECTRUM

OF

VARIABLE -R ANDOM T REES

lies on the lowest generalisation error region (or close to it) in a typical strength-correlation
curve. Ensembles generated from   [0.0, 0.5] often out-perform those generated from
  [0.5, 1.0].
 Propose a new approach in building better performing ensembles. The Coalescence approach
coalesces a number of points in the first-half of the spectrum. We show that it ranks better
than any single operating point in the whole spectrum.
 Identify the key differences between ensembles constructed under the frameworks of Random
Forests and VR-Tree and explain why the predictive accuracy is sensitive to a parameter in
the VR-Tree framework, but not in the Random Forests framework.
In our empirical evaluation, Coalescence is compared with five benchmarking ensemble methods: Max-Diverse Ensemble, Bagging, Random Forests, Random Subspace and C5 Boosting. The
study reveals that Coalescence ranks top and it is the only ensemble method that is significantly
better than Bagging and Max-Diverse Ensemble using the Friedman test.
Although Coalescence is not significantly better than Random Forests, we have identified that:
while Random Forests performs better than Coalescence under the conditions of irrelevant attribute
and small training size, Coalescence performs better in learning non-axis-parallel concepts and
under class noise.

Acknowledgments
Y. Yu and Z.-H. Zhou were supported by the National Science Foundation of China under Grant
Nos. 60635030 and 60721002. Part of the research was conducted when K. M. Ting was visiting
the LAMDA group, Nanjing University.

377

fiL IU , T ING , Y U , & Z HOU

Appendix A. Data characteristics of all data sets

Table 6: Data characteristics of all forty-five data sets used in the experiments. Data are taken from
the UCI repository (Asuncion & Newman, 2007).
data sets
size
#att. #class description
abalone
4177
1n, 7c
2 Abalone growth
anneal
898 13n, 6c, 19b
6 Steel annealing
audiology
226
8n, 61b
23 Standardised Audiology Database
auto
205
6n, 15c, 4b
7 1985 Auto Imports Database
balance
625
4c
3 Balance Scale weight and Distance Database
breast-w
699
10c
2 Winconsin breast cancer database
breast-y
286
6n, 3b
2 Ljubljana Breast cancer database
chess
3196
35n, 1b
2 Chess end games
cleveland
303
4n, 6c, 3b
5 Cleveland heart disease database
coding 20000
15n
2 Coding database
credit-a
690
4n, 6c, 4b
2 Australian Credit database
credit-g
1000
12c, 12b
2 German credit database
dna
3186
60n
3 Primate splice-junction gene sequences
echo
133
6c, 1b
2 Echocardiogram data
flare
1066
3n, 2c, 5b
2 Predicting solar flare
glass
214
9c
7 Glass identification database
hayes
160
4c
3 Hayes-Roth & Hayes-Roth database
hepatitis
155
6c, 13b
2 Hepatitis Domain
horse
368
13n, 7c, 2b
2 Horse colic database
hypo
3163
7c, 18b
2 Thyroid disease database
ionosphere
351
34c
2 Radar returns from the ionosphere
iris
150
4c
3 Iris plants database
labor
57
5n, 8c, 3b
2 Final settlements in labour negotiations
led24
3200
24b
10 LED display + 17 irrelevant attributes
led7
3200
7b
10 LED display with no irrelevant attribute
liver
345
6c
2 BUPA liver disorder
lymph
148
6n, 3c, 9b
4 Lymphography domain
nursery 12960
8n
5 Nursery database
pima
768
8c
2 Diabetes of female Pima Indians
post
90
7n, 1c
3 Postoperative patient data
primary
339
3n, 14b
22 Primary tumor domain
satimage
6435
36c
7 Satellite image data set from NASA
segment
2310
19c
7 Image segmentation data
sick
3163
7c, 18b
2 Sick-euthyroid data
solar
323
3n, 3c, 6b
6 Solar data set
sonar
208
60c
2 Classification of sonar signals
soybean
683
19n, 16b
19 Soy bean disease diagnosis
threeOf9
512
9b
2 The concept of three of nine
tic-tac-toe
958
9n
2 Tic-Tac-Toe board configurations
vehicle
846
18c
4 Vehicle silhouette data set
vote
435
16n
2 Votes for U.S. Congressmen
waveform21
5000
21c
3 Waveform data
waveform40
5000
40c
3 Waveform data with 19 noise attributes
wine
178
13c
3 Wine recognition data
zoo
101
1n, 15b
7 Zoo database
Attribute type is indicated by n: nominal, c: continuous, and b: binary.

378

fiS PECTRUM

OF

VARIABLE -R ANDOM T REES

Appendix B. Theoretical Analysis of VR-Trees ( = 0)
The notations below are similar to those of Bousquet and Elisseeff (2002). Denote X as the input
space and Y as the output space. A learning algorithm is a function T : Z k  F, where F  Y X
is a function space. We denote f as an function in F. The learning algorithm outputs a function
that approximates the underlying true function f  by inspecting the training set. A training set
S = {zi }ki=1 , where zi = (xi , yi ) and yi = f  (xi ), contains k i.i.d. examples in Z = X  Y
drawn from an unknown distribution D. We consider the case that X is a bounded Real space and
Y = {1, +1}. Denote the training set after removing the i-th example as
S \i = {z1 , . . . , zi1 , zi+1 , . . . , zk }
Given a learning algorithm T trained on S, we denote its generalisation error as
R(T, S) = Ez=(x,y)[`(fT,S (x), y)] ,
and its empirical error as

1 Xk
`(fT,S (xi ), yi ) ,
i=1
k
= T (S) and ` : Y  Y  R is a loss function.
Re (T, S) =

where fT,S

We study the generalisation ability from the viewpoint of the Pointwise Hypothesis Stability
(Bousquet & Elisseeff, 2002).
Definition 1 (Pointwise Hypothesis Stability)
An algorithm T has pointwise hypothesis
stability  with respect to the loss function ` if it holds for all i  {1, . . . , k} that
ESD [|`(fT,S (xi ), yi )  `(fT,S \i (xi ), yi )|]  
Theorem 11 of Bousquet and Elisseeff (2002) reveals the relationship between the Pointwise Hypothesis Stability and the generalisation error. To be self-contained, we write the theorem as
Lemma 1.
Lemma 1 For any learning algorithm T with pointwise hypothesis stability  with respect to a loss
function ` such that for some M , 0  `(, )  M , we have with probability 1  ,
r
M 2 + 12M k
R(T, S)  Re (T, S) +
2k
Assume that (i) there is only one attribute in X and the attribute has Real values, (ii) every
training example has unique attribute values, (iii) each internal node has non-empty subsets, (iv)
the tree building process stops at nodes which contain only one example, and (v) the output of a
VR-Tree is +1 or 1 for an input instance in the case of binary classification. When building an
ensemble of VR-Trees, we run the VR-Tree algorithm N times to produce a set of trees {fi }N
i=1 .
Given a test instance z = (x, y), the output of the ensemble is:
f N (x) =

1 XN
fi (x)
i=1
N
379

fiL IU , T ING , Y U , & Z HOU

whose range is [1, +1]. In the following analysis, we let N approach infinity.
In order to study the Pointwise Hypothesis Stability of VR-Trees ( = 0) ensembles, we need
to bound
ESD [|`(fT,S (xi ), yi )  `(fT,S \i (xi ), yi )|].
We specify the loss function as


1,
`(y1 , y2 ) = 0.5,


0,

|y1  y2 | > 1
|y1  y2 | = 1 .
|y1  y2 | < 1

Let S denote the number of class transitions, which are example pairs (zj , zk ) in S such that
yj 6= yk and there is no example between xj and xk . S will be used to bound  later.
Now, when a training instance z 0 = (x0 , y 0 )  S is being held out as the test example and
leaving S \i , we want to know how a VR-Tree ensemble makes its predictions on z 0 . We find that
one of the following four events will happen when classifying z 0 , as illustrated in Figure 10.

Figure 10: Illustration of four possible places of a test instance. Circles of +1, 1 and ? denote
positive, negative and test instances, respectively.
?
(a )
+1
(b )

+1

(c)
(d )

?
+1

-1

+1
?

?

+1

a) z 0 is a duplicate of a training example zi = (xi , yi ), as illustrated in Figure 10(a). We have
f n (x0 ) = yi , because every tree leaf is pure, which means the empirical error is always zero. Since
we assume that every instance has a unique location, we ignore this case for S .
b) z 0 is located between two training examples zi = (xi , yi ) and zj = (xj , yj ) with yi = yj , as
illustrated in Figure 10(b). In this case, z 0 will fall into a leaf node containing zi or zj . Therefore,
we have f n (x0 ) = yi = yj . When z 0 is wrongly classified, that is, z 0 has a different label from yi
and yj , two counts are added to S .
c) z 0 is located outside the training set and zi = (xi , yi ) is the nearest training example, as
illustrated in Figure 10(c). In this case z 0 will fall into a leaf node that must contain zi , thus
f n (x0 ) = yi . When z 0 is wrongly classified, one count is added to S .
d) z 0 is located between two training examples zi = (xi , yi ) and zj = (xj , yj ) with yi 6= yj . When
z 0 is wrongly classified, that is, z 0 has a different label from either yi or yj , one count is added to S .

380

fiS PECTRUM

OF

VARIABLE -R ANDOM T REES

Therefore, given an example zi = (xi , yi )  S, we have `(fT,S (xi ), yi ) = 0, because yi is
included in S, which is the case of Figure 10(a). By the above analysis, we also have the number of
errors upper-bounded by S . Then we have
 = ESD [|`(fT,S (xi ), yi )  `(fT,S \i (xi ), yi )|]
= ESD [`(fT,S \i (xi ), yi )]
 ESD [S ]/k .
Note that S varies on training sets, but it is upper bounded by the class transitions of the
underlying true function f  , that is, let  be
fi
fi
 = fi{x  X | > 0 x0 : f  (x)f  (x0 ) < 0  kx  x0 k < }fi ,

which is a constant. We then have

S  D : S   ,

and S =  when S is large enough. Therefore, we have
   /k .
Since  is a constant, the ensemble of VR-Trees ( = 0) is a stable learner, whose generalisation
error is bounded, with probability 1  ,
r
1 + 12
.
R(T, S) 
2k
It can be observed that features with large  weaken the predictive performance.
Moreover, we find that irrelevant attributes, class noise and insufficient training sample can
cause a large  . For irrelevant attributes, they perform an almost random projection from X to Y.
If f  is a random projection, i.e. P (f  (x) = +1) = 0.5, the probability that there are S class
transitions in training set S is
P (S )  (1/2)|S|/S 1
by considering that the S transitions divide S into equal segments. This implies that irrelevant
attributes weaken the predictive performance of VR-Trees ( = 0). For class noise and insufficiency
of training samples, it is not hard to see that they also increase  and hence also weaken the
performance of VR-Trees ( = 0).

381

fiL IU , T ING , Y U , & Z HOU

Appendix C. Estimation of Strength and Correlation
To make this paper self contained, we provide the estimation of strength and correlation which is
defined by Breiman (2001) and corrected by Kobayashi (2002). Given an ensemble of N trees
{f1 , ..., fN }, a hold-out set Dh = {(x1 , y1 ), ..., (xk , yk )} and k is the number of test cases, we can
estimate the followings:
 Strength - it is estimated by the average of margin over Dh .
 Correlation - it is estimated by taking the variance of margin over the standard deviation of
random vectors, which represent trees.
C.1 Estimation of Strength s
The estimation of Strength is given by:
k
1X
si
k

(3)

N
N
1 X
1 X
I(fj (xi ) = yi ) 
I(fj (xi ) = c(i))
N
N

(4)

s =

i=1

The margin si is given by:
si =

j=1

j=1

For each test case i, c(i) is the class label that receives the maximum votes among N trees and c(i)
can be any class label other than the true label yi . I(.) is the indicator function that returns 1 when
it is true, 0 otherwise.
C.2 Estimation of Correlation 
The estimation of Correlation is given by:
 =

var(s)
N
1 P
sd(j)
N

(5)

j=1

The variance of margin var(s) is given by:

var(s) =

k
1X 2
si  s2
k

(6)

i=1

The standard deviation of each random vector sd(j) is given by:
sd(j) = [p1 + p2  (p1  p2 )2 ]1/2

(7)

k
1X
p1 =
I(fj (xi ) = yi )
k

(8)

i=1
k

1X
I(fj (xi ) = c(i))
p2 =
k
i=1

382

(9)

fiS PECTRUM

OF

VARIABLE -R ANDOM T REES

References
Asuncion, A., & Newman, D. J. (2007).
UCI
http://www.ics.uci.edu/mlearn/MLRepository.html.

machine

learning

repository..

Bauer, E., & Kohavi, R. (1999). An empirical comparison of voting classification algorithms: Bagging, boosting, and variants. Machine Learning, 36(1-2), 105139.
Bousquet, O., & Elisseeff, A. (2002). Stability and generalization. Journal of Machine Learning
Research, 2, 499526.
Breiman, L. (1996a). Bagging predictors. Machine Learning, 24(2), 123140.
Breiman, L. (1996b). Stacked regressions. Machine Learning, 24(1), 4964.
Breiman, L. (2000). Randomizing outputs to increase prediction accuracy. Machine Learning,
40(3), 229242.
Breiman, L. (2001). Random forests. Machine Learning, 45(1), 532.
Buttrey, S. E., & Kobayashi, I. (2003). On strength and correlation in random forests. In Proceedings
of the 2003 Joint Statistical Meetings, Section on Statistical Computing, San Francisco, CA.
American Statistical Association.
Cutler, A., & Zhao, G. (2001). PERT - perfect random tree ensembles. In Computing Science and
Statistics, Vol. 33, pp. 490497, Costa Mesa, Orange Country, California.
Demsar, J. (2006). Statistical comparisons of classifiers over multiple data sets. Journal Machine
Learning Research, 7, 130.
Dietterich, T. G. (2000). An experimental comparison of three methods for constructing ensembles
of decision trees: Bagging, boosting, and randomization. Machine Learning, 40(2), 139157.
Fan, W. (2004). On the optimality of probability estimation by random decision trees. In Proceedings of the Nineteenth National Conference on Artificial Intelligence, Sixteenth Conference
on Innovative Applications of Artificial Intelligence (AAAI), pp. 336341, California, USA.
AAAI Press / The MIT Press.
Fan, W., Wang, H., Yu, P. S., & Ma, S. (2003). Is random model better? on its accuracy and efficiency. ICDM 03: Proceedings of the Third IEEE International Conferenceon Data Mining,
5158.
Geurts, P., Ernst, D., & Wehenkel, L. (2006). Extremely randomized trees. Machine Learning,
63(1), 342.
Hastie, T., Tibshirani, R., & Friedman, J. (2001). The elements of statistical learning : Data mining,
Inference, and Prediction. Springer-Verlag.
Ho, T. K. (1998). The random subspace method for constructing decision forests. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 20(8), 832844.
Kobayashi, I. (2002). Randomized Ensemble Methods for Classification Trees. Ph.D. thesis, Naval
Postgraduate School, Monterey, CA.
Liu, F. T., & Ting, K. M. (2006). Variable randomness in decision tree ensembles. In Advances
in Knowledge Discovery and Data Mining, 10th Pacific-Asia Conference (PAKDD 2006), pp.
8190, Singapore.
383

fiL IU , T ING , Y U , & Z HOU

Liu, F. T., Ting, K. M., & Fan, W. (2005). Maximizing tree diversity by building complete-random
decision trees. In Advances in Knowledge Discovery and Data Mining, 9th Pacific-Asia Conference (PAKDD 2005), pp. 605610, Hanoi, Vietnam.
Melville, P., & Mooney, R. (2003). Constructing diverse classifier ensembles using artificial training examples. In Proceedings of the Eighteenth International Joint Conference on Artificial
Intelligence, pp. 505510, Mexico.
Perrone, M. P., & Cooper, L. N. (1993). When networks disagree: ensemble methods for hybrid
neural networks. Artificial Neural Networks for Speech and Vision, 126142.
Quinlan, R. J. (1993). C4.5: Programs for Machine Learning. Morgan Kaufmann, San Mateo,
Calif.
Robnik-Sikonja, M. (2004). Improving random forests.. In Boulicaut, J.-F., Esposito, F., Giannotti,
F., & Pedreschi, D. (Eds.), Proceedings of The 15th European Conference on Machine Learning (ECML 2004), Vol. 3201 of Lecture Notes in Computer Science, pp. 359370, Pisa, Italy.
Springer.
Ting, K. M., & Witten, I. H. (1999). Issues in stacked generalization. Journal of Artifical Intelligence
Research (JAIR), 10, 271289.
Webb, G. I. (2000). Multiboosting: A technique for combining boosting and wagging. Machince
Learning, 40(2), 159196.
Webb, G. I., & Zheng, Z. (2004). Multistrategy ensemble learning: Reducing error by combining
ensemble learning techniques. IEEE Transactions on Knowledge and Data Engineering,
16(8), 980991.
Zadrozny, B., & Elkan, C. (2001). Obtaining calibrated probability estimates from decision trees and
naive bayesian classifiers. ICML 01: Proceedings of the Eighteenth International Conference
on Machine Learning, 609616.

384

fiJournal of Artificial Intelligence Research 32 (2008) 123167

Submitted 09/07; published 05/08

A Constraint Programming Approach for Solving a
Queueing Control Problem
Daria Terekhov
J. Christopher Beck

dterekho@mie.utoronto.ca
jcb@mie.utoronto.ca

Department of Mechanical & Industrial Engineering
University of Toronto, Canada

Abstract
In a facility with front room and back room operations, it is useful to switch workers
between the rooms in order to cope with changing customer demand. Assuming stochastic
customer arrival and service times, we seek a policy for switching workers such that the
expected customer waiting time is minimized while the expected back room staffing is
sufficient to perform all work. Three novel constraint programming models and several
shaving procedures for these models are presented. Experimental results show that a model
based on closed-form expressions together with a combination of shaving procedures is the
most efficient. This model is able to find and prove optimal solutions for many problem
instances within a reasonable run-time. Previously, the only available approach was a
heuristic algorithm. Furthermore, a hybrid method combining the heuristic and the best
constraint programming method is shown to perform as well as the heuristic in terms
of solution quality over time, while achieving the same performance in terms of proving
optimality as the pure constraint programming model. This is the first work of which we
are aware that solves such queueing-based problems with constraint programming.

1. Introduction
The original motivation for the study of scheduling and resource allocation problems within
artificial intelligence (AI) and constraint programming (CP) was that, in contrast to Operations Research (OR), the full richness of the problem domain could be represented and
reasoned about using techniques from knowledge representation (Fox, 1983). While much of
the success of constraint-based scheduling has been due to algorithmic advances (Baptiste,
Le Pape, & Nuijten, 2001), recently, there has been interest in more complex problems
such as those involving uncertainty (Policella, Smith, Cesta, & Oddi, 2004; Sutton, Howe,
& Whitley, 2007; Beck & Wilson, 2007). In the broader constraint programming community there has been significant work over the past five years on reasoning under uncertainty
(Brown & Miguel, 2006). Nonetheless, it is recognized that constraint solving under change
and uncertainty is in its infancy (Brown & Miguel, 2006, p. 754).
Queueing theory has intensively studied the design and control of systems for resource
allocation under uncertainty (Gross & Harris, 1998). Although much of the study has been
descriptive in the sense of developing mathematical models of queues, there is prescriptive
work that attempts to develop queue designs and control policies so as to optimize quantities
of interest (e.g., a customers expected waiting time) (Tadj & Choudhury, 2005). One of
the challenges to queueing theory, however, is that analytical models do not yet extend to
c
2008
AI Access Foundation. All rights reserved.

fiTerekhov & Beck

richer characteristics encountered in real-world problems such as rostering for call centres
(Cezik & LEcuyer, 2008).
Our long-term goal is to integrate constraint programming and queueing theory with two
ends in mind: the extension of constraint programming to reason better about uncertainty
and the expansion of the richness of the problems for which queueing theory can be brought
to bear. We do not achieve this goal here. Rather, this paper represents a first step where
we solve a queueing control problem with constraint programming techniques. Specifically,
we develop a constraint programming approach for a queueing control problem which arises
in retail facilities, such as stores or banks, which have back room and front room operations.
In the front room, workers have to serve arriving customers, and customers form a queue
and wait to be served when all workers are busy. In the back room, work does not directly
depend on customer arrivals and may include such tasks as sorting or processing paperwork.
All workers in the facility are cross-trained and are assumed to be able to perform back
room tasks equally well and serve customers with the same service rate. Therefore, it makes
sense for the managers of the facility to switch workers between the front room and the
back room depending both on the number of customers in the front room and the amount
of work that has to be performed in the back room. These managers are thus interested in
finding a switching policy that minimizes the expected customer waiting time in the front
room, subject to the constraint that the expected number of workers in the back room is
sufficient to complete all required work. This queueing control problem has been studied in
detail by Berman, Wang and Sapna (2005), who propose a heuristic for solving it.
Our contributions are twofold. Firstly, constraint programming is, for the first time,
used to solve a stochastic queueing control problem. Secondly, a complete approach for a
problem for which only a heuristic algorithm existed previously is presented.
The paper is organized as follows. Section 2 presents a description of the problem
and the work done by Berman et al. (2005). In the next section, three CP models for this
problem are proposed. Sections 4 and 5 present methods for improving the efficiency of these
models, focusing on dominance rules and shaving procedures, respectively. Section 6 shows
experimental results comparing the proposed CP models and combinations of inference
methods. The performance of the CP techniques is contrasted with that of the heuristic
method of Berman et al. Based on these results, a hybrid method is proposed and evaluated
in Section 7. In Section 8, a discussion of the results is presented. Section 9 describes related
problems and states some directions for future work. Section 10 concludes the paper. An
appendix containing the derivations of some expressions used in the paper is also included.

2. Problem Description
Let N denote the number of workers in the facility, and let S be the maximum number of
customers allowed in the front room at any one time.1 When there are S customers present,
arriving customers will be not be allowed to join the front room queue and will leave without
service. Customers arrive according to a Poisson process with rate . Service times in the
front room follow an exponential distribution with rate . The minimum expected number
of workers that is required to be present in the back room in order to complete all of the
necessary work is assumed to be known, and is denoted by Bl , where l stands for lower
1. The notation used by Berman et al. (2005) is adopted throughout this paper.

124

fiA CP Approach for a Queueing Control Problem

bound. Only one worker is allowed to be switched at a time, and both switching time and
switching cost are assumed to be negligible. The goal of the problem is to find an optimal
approach to switching workers between the front room and the back room so as to minimize
the expected customer waiting time, denoted Wq , while at the same time ensuring that the
expected number of workers in the back room is at least Bl . Thus, a policy needs to be
constructed that specifies how many workers should be in the front room and back room at
a particular time and when switches should occur.
2.1 Policy Definition
The term policy is used in the queueing control literature to describe a rule which prescribes,
given a particular queue state, the actions that should be taken in order to control the queue.
Most of the research on the optimal control of queues has focused on determining when a
particular type of policy is optimal, rather than on finding the actual optimal values of
the parameters of the policy (Gross & Harris, 1998). The term optimal policy is used in
the literature to mean both the optimal type of policy and the optimal parameter values
for a given policy type. The distinction between the two is important since showing that
a particular policy type is optimal is a theoretical question, whereas finding the optimal
values for a specific policy type is a computational one.
The policy type adopted here is the one proposed by Berman et al. (2005). A policy is
defined in terms of quantities ki , for i = 0, . . . , N and states that there should be i workers
in the front room whenever there are between ki1 + 1 and ki customers (inclusive) in
the front room, for i = 1, 2, . . . , N . As a consequence of this interpretation, the following
constraints have to hold: ki  ki1  1, k0  0 and kN = S. For example, consider a facility
with S = 6 and N = 3, and suppose the policy (k0 , k1 , k2 , k3 ) = (0, 2, 3, 6) is employed.
This policy states that when there are k0 + 1 = 1 or k1 = 2 customers in the front room,
there is one worker in the front room; when there are 3 customers, there are 2 workers; and
when there are 4, 5, or 6 customers, all 3 workers are employed in the front. Alternatively,
ki can be interpreted as an upper bound on the number of customers that will be served
by i workers under the given policy. Yet another interpretation of this type of switching
policy comes from noticing that as soon as the number of customers in the front room is
increased by 1 from some particular switching point ki , the number of workers in the front
room changes to i + 1. This definition of a policy forms the basis of the model proposed by
Berman et al., with the switching points ki , i = 0, . . . , N  1, being the decision variables
of the problem, and kN being fixed to S, the capacity of the system.
This policy formulation does not allow a worker to be permanently assigned to the back
room: the definition of the ki s is such that every worker will, in some system state, be
serving customers. Due to this definition, there exist problem instances which are infeasible with this policy type yet are feasible in reality. Consequently, the proposed policy
formulation is sub-optimal (Terekhov, 2007). However, the goal of the current work is to
demonstrate the applicability of constraint programming to computing the optimal values
for the given policy type and not to address theoretical optimality questions. Therefore,
the term optimal policy is used throughout the paper to refer to the optimal numerical
parameters for the policy type proposed by Berman et al. (2005).
125

fiTerekhov & Beck

2.2 Berman et al. Model
In order to determine the expected waiting time and the expected number of workers in the
back room given a policy defined by particular values of ki , Berman et al. first define a set
of probabilities, P (j), for j = k0 , k0 + 1, . . . , S. Each P (j) denotes the steady-state (longrun) probability of the queue being in state j, that is, of there being exactly j customers
in the facility. Based on the Markovian properties of this queueing system (exponential
inter-arrival and service times), Berman et al. define a set of detailed balance equations for
the determination of these probabilities:
P (j) = P (j + 1)1

j = k0 , k0 + 1, . . . , k1  1

P (j) = P (j + 1)2
..
..
.
.

j = k1 , k1 + 1, . . . , k2  1
..
.

P (j) = P (j + 1)i
..
..
.
.

j = ki1 , ki1 + 1, . . . , ki  1
..
.

P (j) = P (j + 1)N 

j = kN 1 , kN 1 + 1, . . . , kN  1.

(1)

PS
The probabilities P (j) also have to satisfy the equation
j=k0 P (j) = 1. Intuitively, in
steady-state, the average flow from state j to state j + 1 has to be equal to the average
flow from state j + 1 to state j (Gross & Harris, 1998). Since P (j) can be viewed as the
long-run proportion of time when the system is in state j, the mean flow from state j to
j + 1 is P (j) and the mean flow from state j + 1 to j is P (j + 1)i for some i between 1
and N depending on the values of the switching points.
From these equations, Berman et al. derive the following expressions for each P (j):
P (j) = j P (k0 ),

(2)

where

j

=

Xi =





1 if j = k0

 

  jk0


,

1 jki1
Xi
i

if ki1 + 1  j  ki i = 1, . . . , N

i1  kg kg1
Y
1

g=1

(3)

(4)

g

(X1  1), i = 1, . . . , N.
P (k0 ) can be calculated using the following equation, which is derived by summing both
sides of Equation (2) over all values of j:

P (k0 )

S
X

j = 1.

j=0

126

(5)

fiA CP Approach for a Queueing Control Problem

All quantities of interest can be expressed in terms of the probabilities P (j). Expected
number of workers in the front room is
F

=

N
X

ki
X

iP (j),

(6)

i=1 j=ki1 +1

while the expected number of workers in the back room is
B = N  F.

(7)

The expected number of customers in the front room is
L =

S
X

jP (j).

(8)

j=k0

Expected waiting time in the queue can be expressed as
Wq =

L
1
 .
(1  P (kN )) 

(9)

This expression is derived using Littles Laws for a system of capacity kN = S.
Given a family of switching policies K = {K; K = {k0 , k1 , ..., kN 1 , S}, ki integers,
ki  ki1  1, k0  0, kN 1 < S}, the problem can formally be stated as:
minimizeKK Wq

(10)

s.t. B  Bl
PS
j=k0 P (j) = 1

equations (1), (6), (7), (8), (9).
Berman et al. (2005) refer to this problem as problem P1 . It is important to note that B,
F and L are expected values and can be real-valued. Consequently, the constraint B  Bl
states that the expected number of workers in the back room resulting from the realization
of any policy should be greater than or equal to the minimum expected number of back
room workers needed to complete all work in the back room. At any particular time point,
there may, in fact, be fewer than Bl workers in the back room.
As far as we are aware, the computational complexity of problem P1 has not been
determined. Berman et al. (p. 354) state that solving problem P1 exactly is extremely
difficult since the constraints set (the detailed balance equations) changes when the policy
changes.
2.2.1 Berman et al.s Heuristic
Berman et al. (2005) propose a heuristic method for the solution of this problem. This
method is based on two corollaries and a theorem, which are stated and proved by the
authors. These results are key to the understanding of the problem, and are, therefore,
repeated below.
127

fiTerekhov & Beck

Theorem 2.1 (Berman et al.s Theorem 1) Consider two policies K and K 0 which are
equal in all but one ki . In particular, suppose that the value of kJ0 equals kJ  1, for some
J from the set {0, ..., N  1} such that kJ  kJ1  2, while ki0 = ki for all i 6= J. Then (a)
Wq (K)  Wq (K 0 ), (b) F (K)  F (K 0 ), (c) B(K)  B(K 0 ).
As a result of Theorem 2.1, it can be seen that two policies exist which have special
properties. Firstly, consider the policy
K = {k0 = 0, k1 = 1, k2 = 2, ..., kN 1 = N  1, kN = S} .
This policy results in the largest possible F , and the smallest possible B and Wq . Because
this policy yields the smallest possible expected waiting time, it is optimal if it is feasible.
On the other hand, the smallest possible F and the largest possible Wq and B are obtained
by applying the policy

K = {k0 = S  N, k1 = S  N + 1, ..., kN 1 = S  1, kN = S} .
Therefore, if this policy is infeasible, the problem (10) is infeasible also.
Berman et al. propose the notions of eligible type 1 and type 2 components. An eligible
type 1 component is a switching point ki satisfying the condition that ki  ki1 > 1 for
0 < i < N or ki > 0 for i = 0. A switching point ki is an eligible type 2 component if
ki+1  ki > 1 for 0  i < N . More simply, an eligible type 1 component is a ki variable
which, if decreased by 1, will still be greater than ki1 , while an eligible type 2 component
is a ki variable which, if increased by 1, will remain smaller than ki+1 . Eligible type 1
components and eligible type 2 components will further be referred to simply as type 1 and
type 2 components, respectively.

Based on the definitions of policies K and K, the notions of type 1 and 2 components,
and Theorem 2.1, Berman et al. propose a heuristic, which has the same name as the
problem it is used for, P1 :

1. Start with K = K.
2. If B(K) < Bl , the problem is infeasible. Otherwise, let imb Wq = Wq (K) and
imb K = K. Set J = N .
3. Find the smallest j  s.t. 0  j  < J and kj  is a type 1 component. If no such j 
exists, go to 5. Otherwise, set kj  = kj   1. If B(K) < Bl , set J = j  and go to 5.
If B(K)  Bl , go to 4.
4. If Wq (K) < imb Wq , let imb Wq = Wq (K) and imb K = K. Go to 3.
5. Find the smallest j  s.t. 0  j  < J and kj  is a type 2 component. If no such j 
exists, go to 6. Otherwise, set kj  = kj  + 1. If B(K) < Bl , repeat 5. If B(K)  Bl ,
go to 4.
6. Stop and return imb K as the best solution.
128

fiA CP Approach for a Queueing Control Problem

Parameter
S
N


Bl

Meaning
front room capacity
number of workers in the facility
arrival rate
service rate
expected number of workers required in the back room
Table 1: Summary of problem parameters.

Notation
F
B
L
Wq

Meaning
expected number of workers in the front room
expected number of workers in the back room
expected number of customers in the front room
expected customer waiting time

Definition
Equation (6)
Equation (7)
Equation (8)
Equation (9)

Table 2: Summary of quantities of interest.

Limiting the choice of j  to being between 0 and J, and resetting J every time an
infeasible policy is found, prevents the heuristic from entering an infinite cycle. The heuristic

guarantees optimality only when the policy it returns is K or K.
Empirical results regarding the performance of heuristic P1 are not presented in the
paper by Berman et al. (2005). In particular, it is not clear how close policies provided by
P1 are to the optimal policies.
2.3 Summary of Parameters and Quantities of Interest
Berman et al.s model of problem P1 requires five input parameters (Table 1) and has
expressions for calculating four main quantities of interest (Table 2), most of which are
non-linear.

3. Constraint Programming Models
Some work has been done on extending CP to stochastic problems (Tarim, Manandhar,
& Walsh, 2006; Tarim & Miguel, 2005; Walsh, 2002). Our problem is different from the
problems addressed in these papers because all of the stochastic information can be explicitly
encoded as constraints and expected values, and there is no need for either stochastic
variables or scenarios.
It is known that creating an effective constraint programming model usually requires
one to experiment with various problem representations (Smith, 2006). Consequently, here
we present, and experiment with, three alternative models for problem P1 (Equation (10)).
Although the first model is based directly on the formulation of Berman et al., further
129

fiTerekhov & Beck

models were motivated by standard CP modelling techniques, such as the use of dual
variables (Smith, 2006). We investigate the following three CP models:
 The If-Then model is a CP version of the formal definition of Berman et al.
 The PSums model uses a slightly different set of variables, and most of the constraints
are based on closed-form expressions derived from the constraints that are used in the
If-Then model.
 The Dual model includes a set of dual decision variables in addition to the variables
used in the If-Then and PSums models. Most of the constraints of this model are
expressed in terms of these dual variables.
3.1 Common Model Components
There are a number of modelling components that are common to each of the constraint
models. Before presenting the models, we therefore present their common aspects.
3.1.1 Decision Variables
All three of the proposed models have a set of decision variables ki , i = 0, 1, . . . , N , representing the switching policy. Each ki from this set has the domain [i, i + 1, . . . , S  N + i]
and has to satisfy the constraint ki < ki+1 (since the number of workers in the front room, i,
increases only when the number of customers, ki , increases). Due to Berman et al.s policy
definition, kN must equal S.
3.1.2 Additional Expressions
All models include variables and constraints for the representation of the balance equations
(Equation (1)), and expressions for F , the expected number of workers in the front room,
and L, the expected number of customers in the front room. However, these representations
differ slightly depending on the model, as noted below inP
Sections 3.2, 3.3 and 3.4.
ki
A set of auxiliary variables, Sum(ki ), defined as
j=ki1 +1 j , for all i from 1 to
N  1, is included in each of the models (see Equations (2)(4) for the definition of j ).
These are necessary for representing Equation (11), which relates these variables to P (k0 ), a
floating point variable with domain [0..1] representing the probability of having k0 customers
in the facility. These auxiliary variables and constraint ensure that an assignment of all
decision variables leads to a unique solution of the balance equations. We discuss the formal
definition of these auxiliary variables in Section 3.2.4.
P (k0 )

N
X

Sum(ki ) = 1

(11)

i=0

The back room constraint, B  Bl , is stated in all models as N  F  Bl . The equation
for Wq is stated in all models as Equation (9).
3.2 If-Then Model
The initial model includes the variables P (j) for j = k0 , k0 + 1, . . . , k1 , k1 + 1, . . . , kN 
1, kN , each representing the steady-state probability of there being j customers in the front
130

fiA CP Approach for a Queueing Control Problem

minimize Wq
subject to
ki

<

ki+1 i  {0, 1, . . . , N  1};

kN

=

S;

(ki  j  ki+1  1)



P (j) = P (j + 1)(i + 1),
i  {0, 1, . . . , N  1}, j  {0, 1, . . . , S  1};

(j < k0 )



(P (j) = 0), j  {0, 1, . . . , S  N  1};

P (j)

=

1;

(k0 = j)



P (j)

S
X
j=0

N
X

Sum(ki ) = 1,

i=0

j  {0, 1, . . . , S};
L

=

(ki1 + 1  j  ki )



S
X

jP (j);

j=0

r(i, j) = iP (j),
i  {1, 2, . . . , N }, j  {0, 1, . . . , S};

(ki1 + 1 > j  j > ki )



r(i, j) = 0,
i  {1, 2, . . . , N }, j  {0, 1, . . . , S};

F

=

S
N X
X

r(i, j);

i=1 j=0

Wq

=

N F



auxiliary

L
1
 ;
(1  P (kN )) 
Bl ;
constraints.

Figure 1: Complete If-Then Model
room. These floating point variables with domain [0..1] have to satisfy a system of balance
equations (Equation (1)) and are used to express L and F .
The complete If-Then model is presented in Figure 1.
3.2.1 Balance Equation Constraints
The balance equations are represented by a set of if-then constraints. For example, the first
balance equation, P (j) = P (j + 1) for j = k0 , k0 + 1, ..., k1  1, is represented by the
constraint (k0  j  k1  1)  P (j) = P (j + 1). Thus, somewhat inelegantly, an if-then
131

fiTerekhov & Beck

constraint of this kind has to be added for each j between 0 and S  1 (inclusive) in order
to represent one balance equation. In order to represent the rest of these equations, this
technique has to be applied for each pair of switching points ki , ki+1 for i from 0 to N  1.
This results in a total of N  S if-then constraints.
P
The probabilities P (j) also have to satisfy the constraint Sj=k0 P (j) = 1. The difficulty
with this constraint is the fact that the sum starts at j = k0 , where k0 is a decision variable.
In order to deal with this complication, we add the meta-constraint ((j < k0 )  (P (j) = 0))
for each j from the set {0, 1, . . . , S  N  1}.2 This implies that all values of P (j) with
jPless than k0 will be 0 and allows us to express the sum-of-probabilities constraint as
S
j=0 P (j) = 1.
3.2.2 Expected Number of Workers Constraints
A set of if-then constraints also has to be included in order to represent Equation (6) as
a constraint in our model. This is due to the dependence of this constraint on sums of
variables between two switching points, which are decision variables. More specifically,
we add a set of variables r(i, j) for representing the product of i and P (j) when j is
between ki1 + 1 and ki , and the constraints (ki1 + 1  j  ki )  r(i, j) = iP (j) and
(ki1 + 1 > j  j > ki )  r(i, j) = 0 for all i from 1 to N and for all j from 0 to S. The
total number of these if-then constraints is 2N (S + 1). F can then be simply stated as a
sum over the indices i and j of variables r(i, j).
3.2.3 Expected Number of Customers Constraint
L is defined according to Equation (8). Since the meta-constraint ((j < k0 )  (P (j) = 0))
has been added to the model in order to ensure that P (j) = 0 for all j < k0 , the constraint
for L can be simply stated as the sum of the products of j and P (j) over all j from 0 to S:

L =

S
X

jP (j).

(12)

j=0

3.2.4 Auxiliary Variables and Constraints
The model includes a set of N  2 auxiliary expressions Xi for all i from 3 to N (X1 and
X2 are always equal to 1). Instead of including S j variables (refer to Equation (2) for
the definition of j ), we use N + 1 continuous auxiliary variables Sum(ki ) with domain
 S
[0 . . . 1 + S  ], which represent sums of j variables between j = ki1 + 1 and j = ki
(inclusive). Sum(k0 ) is constrained to equal 1, while the rest of these variables are defined

2. We do not need to add this constraint for all j from 0 to S because the upper bound of the domain of
k0 is S  N .

132

fiA CP Approach for a Queueing Control Problem

according to Equation (13). The validity of this equation is proved in the appendix.


! ki ki1 


 ki1 k0 +1   1




1 

i !





if i
6= 1
X

i




k

i
i

X
1
i
Sum(ki ) =
j =


j=ki1 +1


 ki1 k0 +1  




1


Xi
(ki  ki1 ) otherwise.

i

(13)

i  {1, . . . , N };

P
P N
 can then be expressed as N
The sum kj=k
i=0 Sum(ki ). The requirement that P (k0 )
0 j
PkN
 has to equal 1 is stated as a set of if-then constraints (k0 = j)  P (j)
0 j
Pj=k
N
i=0 Sum(ki ) = 1 for all j  {0, 1, . . . , S}.
In summary, the auxiliary constraints that are present in the model are: Sum(k0 ) =
Qi1  1 kg kg1
1, Equation (13) and Xi = g=1
i  {3, . . . , N }.
g

The If-Then model includes a total of 3N S + 2N + S + 1 if-then constraints, which are
often ineffective in search because propagation only occurs either when the left-hand side is
satisfied or when the right-hand side becomes false. Consequently, the next model attempts
to avoid, as much as possible, the use of such constraints.
3.3 PSums Model

The second CP model is based on closed-form expressions derived from the balance equations
(the details of the derivations are provided in the appendix). The set of P (j) variables
from the formulation of Berman et al. is replaced by a set of P Sums(ki ) variables for
i = 0, . . . , N  1, together with a set of probabilities P (j) for j = k0 , k1 , k2 , . . . , kN . Note
that P (j) is defined for each switching point only, not for all values from {0, 1, . . . , S}. The
P Sums(ki ) variable represents the sum of all probabilities between ki and ki+1  1.
The complete PSums model is presented in Figure 2. The remainder of this section
provides details of this model.
3.3.1 Probability Constraints
Balance equations are not explicitly stated in this model. However, expressions for P (ki )
and P Sums(ki ) are derived in such a way that the balance equations are satisfied. The
P Sums(ki ) variables are defined in Equation (14). Equation (15) is a recursive formula for
computing P (ki+1 ).

ki+1 1

P Sums(ki ) =

X

P (j)

j=ki

133

fiTerekhov & Beck



ki+1 ki



1



(i + 1)


 P (ki )
if (i+1)
6= 1

=
1

(i + 1)






P (ki )(ki+1  ki )
otherwise.


P (ki+1 ) =


(i + 1)

ki+1 ki

(14)

P (ki ), i  {0, 1, . . . , N  1}.

(15)

PN 1

P Sums(ki ) +

Additionally, the probability variables have to satisfy the constraint
P (kN ) = 1.

i=0

3.3.2 Expected Number of Workers Constraint
F , the expected number of workers in the front room, can be expressed in terms of P (ki )
and P Sums(ki ) as shown in Equation (16):
F

=

N
X

i [P Sums(ki1 )  P (ki1 ) + P (ki )].

(16)

i=1

3.3.3 Expected Number of Customers Constraints
The equation for L is
L =

N
1
X

L(ki ) + kN P (kN )

(17)

i=0

where

L(ki ) = ki P Sums(ki ) + P (ki )
(i + 1)


ki+1 ki 1
ki+1 ki



(ki  ki+1 ) + (i+1)
(ki+1  ki  1) + 1 
 (i+1)

.
2


1  (i+1)
3.3.4 Auxiliary Constraints
The auxiliary constraintsPare exactly the same as in the If-Then model.
However, the
PN
N

has
to
equal
1
is
stated
as
P
(k
)
Sum(k
requirement that P (k0 )  kj=k
0
i ) = 1,
i=0
0 j
rather than as a set of if-then constraints, because this model has an explicit closed-form
expression for the variable P (k0 ).
3.4 Dual Model
The problem can be alternatively formulated using variables wj , which represent the number
of workers in the front room when there are j customers present. The wj variables can be
134

fiA CP Approach for a Queueing Control Problem

minimize Wq
subject to

N
1
X

ki

<

ki+1 i  {0, 1, . . . , N  1};

kN

=

P (ki+1 )

=

S;


P Sums(ki )

=

P Sums(ki )

+

P (k0 )



ki+1 ki

P (ki ),
(i + 1)
i  {0, 1, . . . , N  1};



ki+1 ki




1


(i + 1)


 P (ki )
6= 1
if (i+1)

,
1

(i
+
1)






P (ki )(ki+1  ki )
otherwise
i  {1, 2, . . . , N  1};
P (kN ) = 1;

i=0

F

L

=

=

N
X

i=0
N
X

Sum(ki ) = 1;
i [P Sums(ki1 )  P (ki1 ) + P (ki )] ;

i=1
N
1
X

L(ki ) + kN P (kN ),

i=0

L(ki )

=

ki P Sums(ki ) + P (ki )






(i+1)

ki+1 ki 1


(i + 1)

(ki  ki+1 ) +

1




(i+1)


(i+1)

i  {0, 1, . . . , N  1};

ki+1 ki

2

L
1
 ;
(1  P (kN )) 
 Bl ;

Wq =
N F
auxiliary

constraints.

Figure 2: Complete PSums Model
135

(ki+1  ki  1) + 1

,

fiTerekhov & Beck

referred to as the dual variables because, compared to the ki s, the roles of variables and
values are reversed (Hnich, Smith, & Walsh, 2004; Smith, 2006). As stated by Smith, the
use of dual variables in a constraint programming model is beneficial if some constraints of
the problem are easier to express using these new variables. This is the case for our problem
 in fact, the use of dual variables allows us to significantly reduce the number of if-then
constraints necessary for stating relations between the probabilities.
In our Dual model, there are S + 1 wj variables, each with domain [0, 1, . . . , N ]. These
variables have to satisfy the following equations: w0 = 0, wS = N and wj  wj+1 for all
j from 0 to S  1. Additionally, the complete set of ki variables is included in this model,
since some constraints are easier to express using the ki s rather than the wj s.
The complete Dual model is presented in Figure 3, while the details are discussed below.
3.4.1 Probability Constraints
Given the dual variables, the balance equations can be restated as
P (j) = P (j + 1)wj+1 , j  {0, 1, . . . , S  1}.

(18)

This formulation of the balance equations avoids the inefficient if-then constraints. The rest
of the restrictions on the probability variables are
PSstated in terms of the ki variables, as in the
If-Then model. In particular, the constraints j=0 P (j) = 1 and ((k0 > j)  (P (j) = 0))
j  {0, . . . , S  N  1} are present in this model.
3.4.2 Channelling Constraints
In order to use redundant variables, a set of channelling constraints has to be added to the
model to ensure that an assignment of values to one set of variables will lead to a unique
assignment of variables in the other set. The following channelling constraints are included:
wj < wj+1  kwj = j

j  {0, 1, . . . , S  1},

(19)

wj = wj+1  kwj 6= j

j  {0, 1, . . . , S  1},

(20)

j  {0, 1, . . . , S}, i  {1, . . . , N }.

(21)

wj = i  ki1 + 1  j  ki

Constraints (19) and (20) are redundant given the constraint wj  wj+1 . However, such
redundancy can often lead to increased propagation (Hnich et al., 2004). One direction for
future work is examining the effect that removing one of these constraints may have on the
performance of the program.
3.4.3 Expected Number of Workers Constraint
The expression for the expected number of workers in the front room is F =
3.4.4 Expected Number of Customers Constraint

PS

j=0 wj P (j).

The constraint used to express L is identical to the one used in the If-Then model: L =
P
S
j=0 jP (j). This equation is valid because all P (j) for j < k0 are constrained to be 0.
136

fiA CP Approach for a Queueing Control Problem

minimize Wq
subject to
w0 = 0,

wS = N

wj



wj+1 j  {0, 1, . . . , S  1};

ki

<

ki+1 i  {0, 1, . . . , N  1};

kN

=

S;

wj < wj+1



kwj = jj  {0, 1, . . . , S  1};

wj = wj+1



kwj 6= j

wj = i



ki1 + 1  j  ki

j  {0, 1, . . . , S  1};
i  {1, 2, . . . , N }, j  {0, 1, . . . , S};

P (j)

=

P (j + 1)wj+1  j  {0, 1, . . . , S  1};

(k0 = j)



P (j)

N
X

Sum(ki ) = 1,

j  {0, 1, . . . , S};

i=0

(j < k0 )
S
X



(P (j) = 0),

P (j)

=

1;

F

=

j  {0, 1, . . . , S  N  1};

j=0

S
X

wj P (j);

j=0

L

=

S
X

jP (j);

j=0

Wq

=

N F



auxiliary

L
1
 ;
(1  P (kN )) 
Bl ;
constraints.
Figure 3: Complete Dual Model

137

fiTerekhov & Beck

Statistic/Model
# of decision variables
# of probability variables
# of probability constraints
# of constraints for F
# of constraints for L
# of if-then constraints
total # of constraints

If-Then
N +1
S+1
N (S  1) + 2S + 2
2N (S + 1) + 1
1
3N S + 2N + S + 1
3N S + 4N + 2S + 6

PSums
N +1
2N + 1
2N + 1
1
N +1
0
6N + 5

Dual
N +S +2
S+1
3S  N + 2
1
1
S+1
N S + 3N + 6S + 8

Table 3: Summary of the main characteristics of the three proposed CP models.

3.4.5 Auxiliary Constraints
The auxiliary constraints present in this
are exactly the same as in the If-Then
Pkmodel
N
model. The requirement that P (k0 )  j=k0 j = 1 is also stated as a set of if-then
P
constraints (k0 = j)  P (j) N
i=0 Sum(ki ) = 1 for all j  {0, 1, . . . , S}.
3.5 Summary

Table 3 presents a summary of the number of variables and constraints in each of the three
proposed models. It can be seen that the PSums model has a smaller number of probability
variables and constraints but a slightly larger number of constraints for representing L than
the other two models, and no if-then constraints. The Dual model has a larger number of
decision variables than the If-Then and PSums models. This does not imply that the search
space is bigger in this model because the two sets of variables are linked by channelling
constraints and so are assigned values via propagation. The Dual allows the simplest
representations of F and L, each requiring only one constraint. The number of probability
constraints in the Dual is smaller than or equal to the number of such constraints in the
If-Then model and greater than in the PSums model. However, the actual representation
of these constraints is the most straightforward in the Dual since it neither requires if-then
constraints nor closed-form expressions.
It is hard to determine, simply by looking at Table 3, which of the models will be more
efficient since, in CP, a larger number of constraints and/or variables may actually lead to
more propagation and to a more effective model (Smith, 2006). However, it is known that
if-then constraints do not propagate well, and, since the difference in the number of these
constraints between the PSums model and both the If-Then model and the Dual model is
quite significant, one may expect the PSums model to have some advantage over the other
two models.
In fact, preliminary experiments with all three models showed poor performance (see
Table 4 in Section 6). Due to the complexity of constraints relating the decision variables to
variables representing probabilities, there was little constraint propagation, and, essentially,
search was required to explore the entire branch-and-bound tree. As a consequence, in the
following two sections we examine dominance rules (Beck & Prestwich, 2004; Smith, 2005)
and shaving (Caseau & Laburthe, 1996; Martin & Shmoys, 1996), two stronger inference
138

fiA CP Approach for a Queueing Control Problem

forms used in CP. In Section 8, we investigate why the models without dominance rules
and shaving need to search the whole tree in order to prove optimality, and also discuss
differences in the performance of the models based on our experimental results.

4. Dominance Rules
A dominance rule is a constraint that forbids assignments of values to variables which
are known to be sub-optimal (Beck & Prestwich, 2004; Smith, 2005). For problem P1 , a
dominance rule states that, given a feasible solution, K, all further solutions have to have
at least one switching point assigned a lower value than the value assigned to it in K. In
other words, given two solutions K and K 0 , if the Wq value resulting from policy K 0 is
smaller than or equal to the Wq value resulting from K, then there have to exist switching
points ki0 and ki in K 0 and K, respectively, satisfying the condition ki0 < ki . The following
theorem states the dominance rule more formally.
0 ) be
Theorem 4.1 (Dominance Rule) Let K = (k0 , k1 , . . . , kN ) and K 0 = (k00 , k10 , . . . , kN
0
two policies such that k0 = k00 = 0, k1 = k10 = 1, . . . , kJ1 = kJ1
= J  1 and kJ 6= kJ0
0
(i.e. at least one of kJ , kJ is strictly greater than J) for some J  {0, 1, . . . , N  1}. Let
Wq (K) and Wq (K 0 ) denote the expected waiting times resulting from the two policies K and
K 0 , respectively. If Wq (K 0 ) < Wq (K), then there exists i  {J, J + 1, J + 2, . . . , N  1} for
which ki0 < ki .

Proof: [By Contraposition] We prove the contrapositive of the statement in Theorem
4.1: we assume that there does not exist i  {J, J + 1, . . . , N  1} such that ki0 < ki
and show that, given this assumption, Wq (K 0 ) has to be greater than or equal to
Wq (K).
Assume no i  {J, J + 1, . . . , N  1} exists for which ki0 < ki . Then one of the
following is true:
(a) kn = kn0 for all n  {J, J + 1, . . . , N  1}, or
(b) there exists at least one j  {J, J + 1, . . . , N  1} such that kj0 > kj , and the
values of the rest of the switching points are the same in the two policies.
Case (a) implies that K 0 and K are the same policy, and so Wq (K) = Wq (K 0 ).
To prove (b), suppose there exists exactly one j  {J, J + 1, J + 2, . . . , N  1} such
that kj0 > kj , and kn = kn0 for all n  {J, . . . , N  1} \ {j}. Then K and K 0 are
different in the value of exactly one switching point. Consequently, by Theorem 2.1,
Wq (K 0 )  Wq (K). Similarly, by applying Theorem 2.1 several times, the result
generalizes to cases when there exists more than one j such that kj0 > kj .
Therefore, if no i  {J, J + 1, . . . , N  1} exists for which ki0 < ki , it follows that
Wq (K 0 )  Wq (K). In other words, if Wq (K 0 ) < Wq (K), then there exists
i  {J, J + 1, . . . , N  1} for which ki0 < ki .

2

 , . . . , k
Theorem 4.1 implies that, given a feasible policy (0, 1, . . . , ki , ki+1
N 1 , S) where
all switching points with index i or greater are assigned values that are strictly greater than
their lower bounds, we know that a solution with smaller Wq has to satisfy the constraint
 )  . . .  (k

((ki < ki )  (ki+1 < ki+1
N 1 < kN 1 )). Therefore, in order to implement

139

fiTerekhov & Beck

the dominance rule, we add such a constraint during search every time a feasible policy is
found, which should lead to a reduction in the size of the search space. Section 6 presents
experimental results regarding the usefulness of this technique.

5. Shaving
Shaving is an inference method that temporarily adds constraints to the model, performs
propagation, and then soundly prunes variable domains based on the resulting state of
the problem (Demassey, Artigues, & Michelon, 2005; van Dongen, 2006). For example, a
simple shaving procedure may be based on the assignment of a value a to some variable x.
If propagation following the assignment results in a domain wipe-out for some variable, the
assignment is inconsistent, and the value a can be removed from the domain of x (Demassey
et al., 2005; van Dongen, 2006). In a more general case, both the temporary constraint and
the inferences made based on it can be more complex. Shaving has been particularly useful
in the job-shop scheduling domain, where it is used to reduce the domains of start and end
times of operations (Caseau & Laburthe, 1996; Martin & Shmoys, 1996). For such problems,
shaving is used either as a domain reduction technique before search, or is incorporated into
branch-and-bound search so that variable domains are shaved after each decision (Caseau
& Laburthe, 1996).
In a shaving procedure for our problem, we temporarily assign a particular value to a
switching point variable, while the rest of the variables are assigned either their maximum
or their minimum possible values. Depending on whether the resulting policies are feasible
or infeasible, new bounds for the switching point variables may be derived.
The instance N = 3, S = 6,  = 15,  = 3, Bl = 0.32 is used below for illustration
purposes. Policy K, which always yields the smallest possible Wq , for this instance is

(k0 , k1 , k2 , k3 ) = (0, 1, 2, 6) and policy K, which always yields the greatest possible Wq , is
(3, 4, 5, 6). Thus, the initial domains of the switching points are [0..3], [1..4], [2..5] and [6]
for k0 , k1 , k2 and k3 , respectively. At any step, shaving may be able to reduce the domains
of one or more of these variables.
5.1 Bl -based Shaving Procedure
The initial shaving procedure consists of two cases in which either the upper or the lower
bounds of variables may be modified. In the first case, the constraint ki = min(ki ), where
min(ki ) is the smallest value from the domain of ki , is temporarily added to the problem
for some particular value of i between 0 and N . All other switching points are assigned
their maximum possible values using the function gMax. Given an array of variables, the
function gMax assigns the maximum possible values to all of the variables that do not yet
have a value, returning true if the resulting assignment is feasible, and false otherwise. The
maximum possible values are not necessarily the upper bound values in the domains of the
corresponding variables, rather they are the highest values in these domains that respect
the condition that kn < kn+1 , n  {0, ..., N  1}. In the example, if k1 is assigned the
value 1, while the rest of the variables are unbound, gMax would result in policy (0, 1, 5, 6),
which has a feasible B value of 0.508992, and thus true would be returned.
140

fiA CP Approach for a Queueing Control Problem

Recall that an assignment is infeasible when it yields a B value which is smaller than
Bl . When the policy resulting from the addition of ki = min(ki ) and the use of gMax is
infeasible, and min(ki ) + 1  max(ki ), the constraint ki > min(ki ) can be added to the
problem: if all variables except ki are set to their maximum values, and the problem is
infeasible, then in any feasible policy ki must be greater than min(ki ). Such reasoning is
valid since Theorem 2.1 states that increasing the value of a switching point will increase
B. Note that if the solution is feasible, it should be recorded as the best-so-far solution if
its Wq value is smaller than the Wq value of the previous best policy. For easier reference,
this part of the shaving procedure will be referred to as the gMax case.
In the second case, the constraint ki = max(ki ) is added to the problem for some i
between 0 and N , where max(ki ) is the maximum value from the domain of ki . The rest
of the variables are assigned the minimum values from their domains using the function
gMin. These assignments are made in a way that respects the constraints kn < kn+1 ,
n  {0, ..., N  1}. If the resulting policy is feasible, then the constraint ki < max(ki ) can
be permanently added to the problem, assuming max(ki )  1  min(ki ). Since all variables
except ki are at their minimum values already, and ki is at its maximum, it must be true,
again by Theorem 2.1, that in any better solution the value of ki has to be smaller than
max(ki ). This case will be further referred to as the gMin case.
In both cases, if the inferred constraint violates the current upper or lower bound of
a ki , then the best policy found up to that point is optimal. Whenever the domain of a
switching point is modified as a result of inferences made during the gMax or gMin case, all
of the switching points need to be re-considered. If the domain of one variable is reduced
during a particular shaving iteration, some of the temporary constraints added in the next
round of shaving will be different from the ones used previously, and, consequently, new
inferences may be possible. Thus, the shaving procedure terminates when optimality is
proved or when no more inferences can be made.
Consider the example mentioned above. Suppose the constraint k0 = 0 is added to the
problem, and all the rest of the variables are assigned their maximum possible values (using
gMax ). The resulting policy is (k0 , k1 , k2 , k3 ) = (0, 4, 5, 6). This policy yields a B value
of 0.63171, which implies that this policy is feasible because 0.63171 > 0.32 = Bl , and so
no domain reductions can be inferred. The constraint k0 = 0 is then removed. Since the
domain of k0 has not been modified, the procedure considers the next variable. Thus, the
constraint k1 = 1 is added, and all the other variables are again set to their maximum values.
The resulting policy is (0, 1, 5, 6), which is also feasible since its B value is 0.508992. The
constraint k1 = 1 is then removed, and k2 = 2 is added. When all the other variables are
set to their maximum values, the resulting policy is (0, 1, 2, 6). This policy yields a B value
of 0.1116577, which is smaller than Bl . Thus, this policy is infeasible, and the constraint
k2 > 2 is added to the problem. This changes the domain of k2 to [3..5]. Whenever the
domain of a variable is reduced, the next shaving step considers the same switching point,
and so the next constraint to be added is k2 = 3.
Now, consider the gMin case and assume that all variables have their full initial domains.
Suppose the constraint k0 = 3 is added to the problem. All the rest of the variables are
assigned their smallest possible values consistent with k0 = 3. Thus, the policy (3, 4, 5, 6) is

considered. This policy has a B value of 0.648305 and is feasible (it is in fact K, so if it were
infeasible, the problem would have been infeasible). The value of k0 in any better solution
141

fiTerekhov & Beck

has to be smaller than 3, and so the domains of the variables become [0..2], [1..4], [2..5], and
[6]. The constraint k0 = 3 is removed, and, since the domain of k0 has been modified, the
constraint k0 = 2 is added next. The policy that is considered now is (2, 3, 4, 6). This policy
is also feasible, and so the domains become [0..1], [1..4], [2..5], [6]. The temporary constraint
k0 = 2 is removed, and the next one added is k0 = 1. The corresponding policy assigned by
gMin is infeasible, and no domain reductions are made. Since the addition of k0 = 1 did not
result in any domain reductions, there is no need to reconsider the variable k0 until after all
other switching points have been looked at. Consequently, the next temporary constraint
to be added is k1 = 4.
In the complete Bl -based shaving procedure, we can start either with the gMin or the
gMax case. Since policies considered in the gMin case will generally have smaller waiting
time than ones considered in the gMax case, it may be beneficial to start with the gMin
case. This is the approach we take.
Our complete Bl -based shaving algorithm is presented in Figure 4. It is assumed in all of
the algorithms presented that the functions add(constraint) and remove(constraint)
add and remove constraint to and from the model, respectively.
Upon the completion of this shaving procedure, the constraint Wq  bestWq , where
bestWq is the value of the best solution found up to that point, is added (Wq  bestWq
rather than Wq < bestWq is added because of numerical issues with testing equality of
floating point numbers). However, although such a constraint rules out policies with higher
Wq as infeasible, it results in almost no propagation of the domains of the decision variables
and does little to reduce the size of the search tree. In order to remedy this problem, another
shaving procedure, this time based on the constraint Wq  bestWq is proposed in the next
sub-section. The issue of the lack of propagation of the domains of ki from the addition of
this constraint will be discussed in more detail in Section 8.1.
5.2 Wq -based Shaving Procedure
The Wq -based shaving procedure makes inferences based strictly on the constraint Wq 
bestWq : the constraint B  Bl is removed prior to running this procedure in order to
eliminate the possibility of incorrect inferences. As in Bl -based shaving, a constraint of the
form ki = max(ki ), where max(ki ) is the maximum value in the domain of ki , is added
temporarily, and the function gMin is used to assign values to the rest of the variables.
Because the Bl constraint has been removed, the only reason for the infeasibility of the
policy is that it has a Wq value greater than the best Wq that has been encountered so far.
Since all switching points except ki are assigned their smallest possible values, infeasibility
implies that in any solution with a smaller expected waiting time, the value of ki has to be
strictly smaller than max(ki ). This shaving procedure is stated in Figure 5.
5.3 Combination of Shaving Procedures
Wq -based and Bl -based shaving will result in different domain reductions since they are
based on two different constraints. Moreover, using the two together may cause more
domain modifications than when either is used by itself. Therefore, it makes sense to run the
Bl -based and Wq -based shaving procedures alternately (with Wq and Bl constraints added
142

fiA CP Approach for a Queueing Control Problem

Algorithm 1: Bl -based Shaving
Input: S, N , , , Bl (problem instance parameters); bestSolution (best solution found
so far)
Output: bestSolution with (possibly) modified domains of the variables ki , or bestSolution
with proof of its optimality
while (there are domain changes)
for all i from 0 to N  1
while (shaving successful for Domain(ki ))
add( ki = max(Domain(ki )) )
if (gMin)
if (new best solution found)
bestSolution = currentSolution;
if ( max(Domain(ki ))  1  min(Domain(ki )) )
add( ki < max(Domain(ki )) )
else
return bestSolution; stop, optimality has been proved
remove( ki = max(Domain(ki )) )
while (shaving successful for Domain(ki ))
add( ki = min(Domain(ki )) )
if (gMax )
if (new best solution found)
bestSolution = currentSolution;
else
if ( min(Domain(ki )) + 1  max(Domain(ki )) )
add( ki > min(Domain(ki )) )
else
return bestSolution; stop, optimality has been proved
remove( ki = min(Domain(ki )) )

Figure 4: Bl -based shaving algorithm

143

fiTerekhov & Beck

Algorithm 2: Wq -based Shaving
Input: S, N , , , Bl (problem instance parameters); bestSolution (best solution found
so far)
Output: bestSolution with (possibly) modified domains of the variables ki , or bestSolution
with proof of its optimality
while (there are domain changes)
for all i from 0 to N  1
while (shaving successful for Domain(ki ))
add( ki = max(Domain(ki )) )
if (!gMin)
if ( max(Domain(ki ))  1  min(Domain(ki )) )
add( ki < max(Domain(ki )) )
else
return bestSolution; stop, optimality has been proved
remove( ki = max(Domain(ki )) )

Figure 5: Wq -based shaving algorithm

and removed appropriately) until no more domain pruning is possible. Such a combination
of the two shaving procedures will be referred to as AlternatingShaving.
The AlternatingShaving procedure can be effectively combined with search in the following manner. AlternatingShaving can be run initially, until no further domain modifications
are possible. Search can then be performed until a better solution is found, at which point
AlternatingShaving can be applied again. Subsequently, search and shaving can alternate
until one of them proves optimality of the best solution found. Such an approach may
be successful because if search finds a new best solution, a new constraint on Wq will be
added, and so Wq -based shaving may be able to reduce the upper bounds of the switching
point variables. This way of combining search and shaving will be further referred to as
AlternatingSearchAndShaving.
Other variations of shaving are also possible. In particular, both Bl -based and Wq -based
shaving procedures can be extended to make inferences about values of two switching points.
For example, one can assign maximum values to a pair of switching point variables, while
assigning minimum values to the rest. If the resulting policy is feasible, then a constraint
stating that at least one variable from this pair has to be assigned a smaller value can be
added to the problem. Preliminary experiments indicated that shaving procedures based
on two switching points do not, in general, result in more effective models. Such procedures
do not explicitly reduce the domains of the switching point variables but rather add a set
of constraints to the model which do not appear, in practice, to significantly reduce the size
of the search space. One possible direction for future work may be to further investigate
these variations of shaving.
144

fiA CP Approach for a Queueing Control Problem

6. Experimental Results
Several sets of experiments3 were performed in order to evaluate the efficiency of the proposed models and the effectiveness of dominance rules and shaving procedures, as well as to
compare the performance of the best CP model with the performance of heuristic P1 . All
constraint programming models were implemented in ILOG Solver 6.2, while the heuristic
of Berman et al. was implemented using C++.
We note that numerical results obtained in the experiments are sensitive to the level of
precision that is set. In all constraint programming models, we set the default ILOG Solver
precision to 0.000001. Doing so implies that all floating point variables in our model are
considered bound when the maximum (max) and minimum (min) values of their intervals
are such that ((max  min)/(max{1, |min|})  0.000001 (Solver, 2006). In order to propagate constraints involving floating point variables, such as Equation (15), ILOG Solver uses
standard interval arithmetic and outward rounding.4
6.1 Problem Instances

The information gained from policies K and K is explicitly used in the implementation of

all three models. If K is infeasible, then the program stops as there is no feasible solution
for that instance. Otherwise, if K is feasible, then it is optimal. The two cases when K is

optimal or K is infeasible are therefore trivial and are solved easily both by the CP models

and by Berman et al.s heuristic. Although instances in which K is optimal are very hard to
solve without shaving, using the elementary Bl -based shaving procedure will always result
in a (usually fast) proof of the optimality of this policy. This case is also trivial for Berman
et al.s heuristic. Consequently, the experimental results presented here are based only on

the instances for which the optimal solution is between K and K.
Preliminary experiments indicated that the value of S has a significant impact on the
efficiency of the programs since higher values of S result in larger domains for the ki variables
for all models and a higher number of wj variables for the Dual model. As indicated in
Table 3 in Section 3.5, S also has a big impact on the number of constraints in the IfThen and Dual models. Therefore, we consider instances for each value of S from the
set {10, 20, . . . , 100} in order to gain an accurate understanding of the performance of the
model and the heuristic. We note that for most instances with S greater than 100, neither
our method nor Berman et al.s heuristic P1 may be used due to numerical instability.
Thirty instances were generated for each S in such a way as to ensure that the instance

is feasible and that the optimal policy is neither K nor K. We created random combinations

of parameter values for each S and chose the instances for which policy K was found to
be feasible, but not optimal, and K was determined to be infeasible. In order to check

that K is not optimal, it is sufficient to find a feasible solution that has one switching
point assigned a value lower than its upper bound. When generating combinations of
parameters, the values of N were chosen with uniform probability from the set {2, . . . , 38},
3. Numerical values in some of the results are slightly different from the ones presented in our previous
work (Terekhov & Beck, 2007) due to some minor errors discovered after the publication of that paper.
The main conclusions and analysis of the previous work remain valid, however.
4. Jean-Francois Puget - personal communication.

145

fiTerekhov & Beck

the values of  from {5,. . . , 99}, the values of  from {1, . . . , 49} and the values of Bl
from {1, . . . , 4}. There appears to be no easy way of determining whether a given instance

will have K or K as the optimal solution based only on the parameter values. Moreover,
preliminary experiments indicated that problem difficulty depends on a combination of
problem parameters (especially S, N and Bl ) rather than on one parameter only.
A 10-minute time limit on the overall run-time of the program was enforced in the
experiments. All experiments were performed on a Dual Core AMD 270 CPU with 1 MB
cache, 4 GB of main memory, running Red Hat Enterprise Linux 4.
6.2 Performance Measures
In order to perform comparisons among the CP models, and between the CP models and
Berman et al.s heuristic, we look at mean run-times, the number of instances in which
the optimal solution was found, the number of instances in which optimality was proved,
the number of instances in which the best-known solution was found and the mean relative
error (MRE). MRE is a measure of solution quality that allows one to observe how quickly
a particular algorithm is able to find a good solution. MRE is defined as
M RE(a, M ) =

1 X c(a, m)  c (m)
|M |
c (m)

(22)

mM

where a is the algorithm that is used to solve the problem, M is the set of problem instances
on which the algorithm is being tested, c(a, m) is the cost of a solution found for instance
m by algorithm a, and c (m) is the best-known solution for instance m. As we generated
the instances, c (m) is the best solution we found during our these experiments.
6.3 Comparison of Constraint Programming Models and Techniques
Each CP model was tested with and without shaving and dominance rules. A total of 30
CP-based methods were therefore evaluated. A model with Bl -based shaving is a model
which runs the Bl -based shaving procedure until no more domain changes are possible, adds
a constraint on the value of Wq based on the best solution found during the shaving procedure and runs search for the rest of the time. Similarly, models with Wq -based shaving and
AlternatingShaving are models which run the Wq -based shaving procedure and the AlternatingShaving procedure, respectively, until it is no longer possible to reduce the domains
of the switching point variables, add a constraint requiring Wq to be less than the expected
waiting time of the best solution found by the shaving procedure and use search for the
rest of the time. As described previously, AlternatingSearchAndShaving alternates between
search and the AlternatingShaving procedure. In all models, search assigns switching points
in increasing index order. The smallest value in the domain of each variable is tried first.
6.3.1 Constraint Programming Models
Table 4 presents the number of instances, out of 300, for which the optimal solution was
found and proved by each of the 30 proposed CP-based methods. This table indicates
that the PSums model is the most effective of the three, proving optimality in the largest
number of instances regardless of the use of dominance rules and shaving. With Alter146

fiA CP Approach for a Queueing Control Problem

No Shaving

If-Then
PSums
Dual

D
105
126
105

ND
105
126
105

Bl -based
Shaving
D ND
192 191
202 201
191 191

Wq -based
Shaving
D ND
105 105
126 126
105 105

Alternating
Shaving
D
ND
219
218
225
225
218
218

AlternatingSearch
AndShaving
D
ND
234
234
238
238
232
232

Table 4: Number of instances for which the optimal solution is found and its optimality is
proved within 10 CPU-minutes out of a total of 300 problem instances (D - with
dominance rules, ND - without dominance rules).

natingSearchAndShaving, PSums proves optimality in the largest number of instances: in
79.3% of all instances, 238 out of 239 instances for which optimality has been proved by
any model.
Figure 6 shows how the MRE changes over the first 50 seconds of run-time for If-Then,
PSums and Dual models with AlternatingSearchAndShaving, and for Bermans heuristic
(we comment on the performance of the heuristic in Section 6.4). PSums is, on average,
able to find better solutions than the other two models given the same amount of run-time.
In Table 5, additional statistics regarding the performance of the three models with
AlternatingSearchAndShaving and without dominance rules are presented (we comment on
the same statistics for P1 in Section 6.4). In particular, for each model, the number of
instances in which it finds the best solution (out of 300), the number of instances in which
it finds the optimal solution (out of 239 cases for which optimality has been proved) and
the number of times it proves optimality (out of 300) are presented. It can be seen that
all models find the optimal solution in the 239 instances for which it is known. However,
the PSums model proves optimality in 4 more instances than the If-Then model and in 6
more instances than the Dual. PSums also finds the best-known solution of any algorithm
in 97.6% of all the instances considered. A more detailed discussion of the differences in
the performance of the CP models is presented in Section 8.2.
6.3.2 Shaving Procedures
From Table 4, it can be observed that the CP models without shaving and with the Wq based shaving procedure prove optimality in the fewest number of cases. The similarity
in performance of models without shaving and with Wq -based shaving is not surprising
because the Wq -based procedure is able to start pruning domains only when the value of
the best policy found prior to this procedure is quite good. When the Wq -based procedure

is used alone, it only has one solution to base its inferences on, namely K. Since all policies

will result in a smaller expected waiting time than K, this procedure by itself is useless.
Employing the Bl -based shaving procedure substantially improves the performance of
all models: without dominance rules, the If-Then, the PSums and the Dual models prove
optimality in 86, 75 and 86 more instances, respectively, than the corresponding models
without shaving or with only Wq -based shaving; with dominance rules, the situation is
147

fi0.10

Terekhov & Beck

0.06
0.04
0.00

0.02

Mean Relative Error

0.08

IfThen Model
PSums Model
Dual Model
Berman et al.s Heuristic P1

0

10

20

30

40

50

Run Time (seconds)

Figure 6: Comparison of MRE of three CP models with AlternatingSearchAndShaving with
Bermans heuristic P1 .

equivalent. These results imply that inferences made based on the Bl constraint are effective
in reducing the domains of the decision variables.
Models with AlternatingShaving and AlternatingSearchAndShaving perform even better than models employing only the Bl -based shaving procedure. The real power of Wq based shaving only becomes apparent when it is combined with Bl -based shaving because
Bl -based shaving often finds a good solution and a good value of bestWq , allowing the
Wq -based procedure to infer more domain reductions. This observation explains why AlternatingSearchAndShaving performs better than AlternatingShaving. In particular, in AlternatingSearchAndShaving, the Wq -based procedure is used both after a new best solution
148

fiA CP Approach for a Queueing Control Problem

PSums
If-Then
Dual
P1

# best found (/300)
293
282
279
282

# opt. found (/239)
239
239
239
238

# opt. proved (/300)
238
234
232
0

Table 5: Comparison of three CP models (with AlternatingSearchAndShaving and without
dominance rules) with Bermans heuristic P1 .

is found during shaving and after one is found during search. Therefore, if a higher quality
solution is found during search, it will be used by the Wq -based procedure to further prune
the domains of the switching point variables.
In Figure 7, the average run-times for each value of S from 10 to 100 are presented for the
four shaving procedures with the PSums model. Since a run-time limit of 600 seconds was
used throughout the experiments, we assumed a run-time of 600 seconds for all instances
for which optimality has not been proved within this limit. Therefore, the mean run-times
reported throughout this paper are underestimates of the true means. Figure 7 shows that,
for each value of S, the AlternatingSearchAndShaving procedure gives the best performance.
It can also be seen that, as S increases, it becomes increasingly difficult to prove optimality
and average run-times increase. As stated previously, this is due to larger domains of the
switching point variables. The AlternatingSearchAndShaving procedure, however, is able
to significantly reduce the domains of the ki variables and therefore provides an effective
method for instances with higher values of S as well.
6.3.3 Dominance Rules
Table 4 indicates that there is rarely any difference in the number of instances solved to
optimality between models with and without dominance rules. No difference at all is visible
for any model without shaving, with Wq -based shaving and AlternatingSearchAndShaving.
Recall that dominance rules are implemented by the addition of a constraint on the
values of the switching point variables after a solution is found. Such a constraint will be
more effective when more of the switching point variables are assigned their minimum values
in the current solution. Usually, such policies are also the ones which result in a smaller
expected waiting time. Similarly, Wq -based shaving is useful only when a solution with
small expected waiting time is found. This leads to the conjecture that dominance rules
may be effective only for the same instances for which the Wq -based shaving procedure is
effective. This conjecture is supported by the results of Table 4. In particular, when the
Wq -based shaving procedure is used by itself, it makes inferences based only on the policy

K, a solution that is generally of poorest quality for an instance. The method with a single
run of Wq -based shaving therefore heavily relies on search. Since search takes a long time to
find a feasible solution of good quality, the effectiveness of dominance rule-based constraints
is also not visible within the given time limit.
149

fi200

300

400

500

Wqbased Shaving
Blbased Shaving
AlternatingShaving
AlternatingSearchAndShaving

0

100

Average RunTimes (seconds)

600

Terekhov & Beck

20

40

60

80

100

Values of S

Figure 7: PSums model with various shaving techniques: average run-times for each value
of S. Average run-times for PSums without shaving are not shown in this graph
since the resulting curve would be indistinguishable from the one for Wq -based
shaving.

On the other hand, in AlternatingSearchAndShaving, the Wq -based procedure plays a
key role because it makes domain reductions based on high quality solutions produced by
Bl -based shaving, and later, search. Dominance rules do not play a role in this procedure
since shaving is used after every new solution is found. However, even if dominance rule
constraints were explicitly incorporated in the procedure (i.e. if they were added before
each new run of search), they would be redundant since they serve essentially the same
purpose as the Wq -based shaving procedure.
When shaving is not used, the results are equivalent to those achieved when Wq -based
shaving is employed. The explanation for the absence of a difference between models with
150

fi1 e03
1 e07

1 e05

PSums Model At 10 seconds
PSums Model At 150 seconds
PSums Model At 500 seconds
Berman et al.s Heuristic P1

1 e09

Mean Relative Error

1 e01

A CP Approach for a Queueing Control Problem

20

40

60

80

100

Values of S

Figure 8: MRE for each value of S for P1 and the P Sums model.

and without dominance rules is therefore also the same. In particular, it takes a long time
for a solution to be found whose quality is such that it allows the dominance rule constraint
to effectively reduce the size of the search tree.
When Bl -based shaving and AlternatingShaving are used, dominance rules are sometimes helpful. In both cases, this is because after these two shaving procedures, subsequent
search usually finds a good solution quickly, and, since Wq -based shaving is not used again
at that point, the dominance rule constraint that is added can be effective in reducing the
size of the search tree.
Overall, it can be observed that using AlternatingSearchAndShaving without dominance
rules is more effective than using Bl -based shaving or AlternatingShaving with dominance
151

fiTerekhov & Beck

rules. Therefore, in further comparisons, the focus is only on models with
AlternatingSearchAndShaving without dominance rules.
6.4 Heuristic P1 vs. Best Constraint Programming Approach
Empirical results regarding the performance of heuristic P1 are not presented by Berman
et al. (2005), and so the ability of P1 to find good switching policies has not been explicitly
evaluated in previous work. We wanted to find out how well the heuristic actually performs
by comparing it to our CP methods.
In Table 5, we present several measures of performance for the three proposed models
with AlternatingSearchAndShaving and for the heuristic P1 . The heuristic performs well,
finding the best-known solution in only eleven fewer instances than the PSums model, in
three more instances than the Dual model and in the same number of instances as the
If-Then model. Moreover, the heuristic finds, but, of course, cannot prove, the optimal
solution in 238 out of 239 instances for which the optimal is known. The three CP models
find the optimal solution in all 239 of these. The run-time of the heuristic is negligible,
whereas the mean run-time of the PSums model is approximately 130 seconds (the mean
run-times of the other two models are slightly higher: 141 seconds for the If-Then model
and 149 seconds for the Dual model).
Table 5 also shows that the PSums model is able to find the best-known solution in 11
more instances than the heuristic. Closer examination reveals that there are 275 instances
in which both the PSums model and P1 find the best-known solution, 18 instances in which
only PSums is able to do so and 7 in which only the heuristic finds the best-known.
From Figure 6, it can be observed that the heuristic achieves a very small MRE in a
negligible amount of time. After about 50 seconds of run-time, the MRE over 300 instances
resulting from PSums with AlternatingSearchAndShaving becomes comparable to that of
the heuristic MRE. In Figure 8, the MRE over 30 instances for each value of S is presented
for the heuristic and for PSums with AlternatingSearchAndShaving at 10, 150 and 500
seconds of run-time. After 10 seconds, the performance of PSums is comparable to that
of the heuristic for values of S smaller than or equal to 40, but the heuristic appears to
be quite a bit better for higher values of S. At 150 seconds, the performance of PSums is
comparable to that of the heuristic except at S values of 50 and 80. After 500 seconds,
PSums has a smaller MRE over the 300 instances and also a lower (or equal) MRE for each
value of S except 50 and 100.
Overall, these results indicate that the heuristic performs wellits run-time is negligible,
it finds the optimal solution in all but one of the cases for which it is known, and it finds
the best solution in 94% of all instances. Moreover, it results in very low MRE. Although
PSums with AlternatingSearchAndShaving is able to achieve slightly higher numbers in
most of the performance measures, it is clear that these improvements are small given that
the PSums run-time is so much higher than the run-time of the heuristic.

7. PSums-P1 Hybrid
Naturally, it is desirable to create a method that would be able to find a solution of high
quality in a very short amount of time, as does Bermans heuristic, and that would also
have the same high rate of being able to prove optimality within a reasonable run-time as
152

fiA CP Approach for a Queueing Control Problem

PSums
P1
PSums-P1
Hybrid

# best found (/300)
293
282
300

# opt. found (/239)
239
238
239

# opt. proved (/300)
238
0
238

Table 6: Comparison of PSums model with AlternatingSearchAndShaving and Berman et
al.s heuristic P1 with the Hybrid model.

does PSums with AlternatingSearchAndShaving. It is therefore worthwhile to experiment
with a PSums-P1 Hybrid, which starts off by running P1 and then, assuming the instance is
feasible, uses the PSums model with AlternatingSearchAndShaving to find a better solution
or prove the optimality of the solution found by P1 (infeasibility of an instance is proved if

the heuristic determines that policy K is infeasible).
Since it was shown that heuristic P1 is very fast, running it first incurs almost no
overhead. Throughout the analysis of experimental results, it was also noted that the
performance of the Wq -based shaving procedure depends on the quality of the best solution
found before it is used. We have shown that the heuristic provides solutions of very high
quality. Therefore, the first iteration of the Wq -based procedure may be able to significantly
prune the domains of switching point variables because of the good-quality solution found
by the heuristic. Continuing by alternating the two shaving techniques and search, which
has also been shown to be an effective approach, should lead to good results.
The proposed Hybrid algorithm was tested on the same set of 300 instances that was
used above. Results illustrating the performance of the Hybrid as well as the performance of
P1 and PSums with AlternatingSearchAndShaving are presented in Table 6. The Hybrid is
able to find the best solution in all 300 cases: in the 275 instances in which both the heuristic
and PSums find the best-known solution, in 18 in which only PSums finds the best-known
and in 7 in which only the heuristic does so. The Hybrid finds the optimal solution (for those
instances for which it is known) and proves optimality in as many instances as the PSums
model. The mean run-time for the Hybrid is essentially identical to the mean run-time of
PSums with AlternatingSearchAndShaving, equalling approximately 130 seconds.
Thus, the Hybrid is the best choice for solving this problem: it finds as good a solution
as the heuristic in as little time (close to 0 seconds), it is able to prove optimality in as
many instances as the best constraint programming method, and it finds the best-known
solution in all instances considered. Moreover, all these improvements are achieved without
an increase in the average run-time over the PSums model.

8. Discussion
In this section, we examine some of the reasons for the poor performance of the CP models
without shaving, suggest reasons for the observed differences among the CP models, discuss
the performance of the Hybrid and present some perspectives on our work.
153

fiTerekhov & Beck

8.1 Lack of Back-Propagation
In our experiments, we have some instances for which even the PSums-P1 Hybrid with
AlternatingSearchAndShaving is unable to find and prove the optimal solution within the
10-minute time limit. In fact, in many of these instances, the amount of time spent during
search is higher than the time spent on shaving, and the run-time limit is usually reached
during the search, rather than the shaving, phase. Further analysis of the algorithms
behaviour suggests that this poor performance of search can be explained by the lack of backpropagation. Back-propagation refers to the pruning of the domains of the decision variables
due to the addition of a constraint on the objective function: the objective constraint
propagates back to the decision variables, removing domain values and so reducing search.
In the CP models presented above, there is very little back-propagation.
Consider a model without shaving. Throughout search, if a new best solution is found,
the constraint Wq  bestWq , where bestWq is the new objective value, is added to the
model. However, the domains of the switching point variables are usually not reduced in
any way after the addition of such a constraint. This can be illustrated by observing the
amount of propagation that occurs in the PSums model when Wq is constrained.
For example, consider an instance of the problem with S = 6, N = 3,  = 15,  = 3,
and Bl = 0.32 (this instance is used in Section 5 to illustrate the shaving procedures). The
initial domains of the switching point variables are [0..3], [1..4], [2..5] and [6]. The initial
domains of the probability variables P (ki ) for each i, after the addition of Wq bounds

provided by K and K, are listed in Table 7. The initial domain of Wq , also determined by

the objective function values of K and K, is [0.22225..0.425225]. The initial domains of L
7
and F , are [2.8175e ..6] and [0..2.68], respectively. Upon the addition of the constraint
Wq  0.306323, where 0.306323 is the known optimal value for this instance, the domain
of Wq is reduced to [0.22225..0.306323], the domain of L becomes [1.68024..6] and the
domain of F remains [0..2.68]. The domains of P (ki ) after this addition are listed in
Table 7. The domains of both types of probability variables are reduced by the addition
of the new Wq constraint. However, the domains of the switching point variables remain
unchanged. Therefore, even though all policies with value of Wq less than 0.306323 are
infeasible, constraining Wq to be less than or equal to this value does not result in any
reduction of the search space. It is still necessary to search through all policies in order to
show that no better feasible solution exists.
One of the reasons for the lack of pruning of the domains of the ki variables due to the
Wq constraint is likely the complexity of the expression Wq = (1PL(kN ))  1 . In the example
above, when Wq is constrained to be less than or equal to 0.306323, we get the constraint
1
L
0.306323  15(1P
(kN ))  3 , which implies that 9.594845(1P (kN ))  L. This explains why
the domains of both L and P (kN ) change upon this addition to the model. The domains of
the rest of the P (ki ) variables change because of the relationships between P (ki )s (Equation
(15)) and because of the constraint that the sum of all probability variables has to be 1.
Similarly, the domains of P Sums(ki )s change because these variables are expressed in
terms of P (ki ) (Equation (14)). However, because the actual ki variables mostly occur as
exponents in expressions for P Sums(ki ), P (ki ), and L(ki ), the minor changes in the domains
of P Sums(ki ), P (ki ), or L(ki ) that happen due to the constraint on Wq have no effect on
the domains of the ki . This analysis suggests that it may be interesting to investigate a CP
154

fiA CP Approach for a Queueing Control Problem

j
k0
k1
k2
k3

Before addition of Wq  0.306323
P (j)
P Sums(j)
[4.40235e6 ..0.979592]
[0..1]
7
[1.76094e ..1]
[0..1]
[2.8175e8 ..0.6]
[2.8175e8 ..1]
[4.6958e8 ..1]
N/A

After addition of Wq  0.306323
P (j)
P Sums(j)
[4.40235e6 ..0.979592]
[0..0.683666]
[0.000929106..1]
[0..0.683666]
[0.0362932..0.578224] [0.0362932..0.71996]
[0.28004..0.963707]
N/A

Table 7: Domains of P (j) and P Sums(j) variables for j = k0 , k1 , k2 , k3 , before and after
the addition of the constraint Wq  0.306323.

model based on log-probabilities rather than on the probabilities themselves. Such a model
may lead to stronger propagation.
Likewise, in the If-Then and Dual models, the domains of the decision variables are not
reduced when a bound on the objective function value is added, although the domains of all
probabilities, L and F are modified. In both of these models, the constraints relating F , L
and the probability variables to the variables ki are the balance equations, which are quite
complex. The domains of the probability variables do not seem to be reduced significantly
enough due to the new Wq bound so as to result in the pruning of ki domains because of
these constraints.
These observations served as the motivation for the proposed shaving techniques. In
particular, the Wq -based shaving procedure reduces the domains of switching point variables
when it can be shown that these values will necessarily result in a higher Wq value than the
best one found up to that point. This makes up for some of the lack of back-propagation.
However, even when this procedure is used after each new best solution is found, as in
AlternatingSearchAndShaving, it is not always able to prune enough values from the domains
of the ki s so as to be able to prove optimality within 10 minutes of run-time. It can
therefore be seen that inferences based on the value of Wq are very limited in their power
and, therefore, if the domains of switching point variables are large after shaving, then it
will not be possible to prove optimality in a short period of time.
8.2 Differences in the Constraint Programming Models
Experimental results demonstrate that the PSums model is the best out of the three models
both with and without shaving. In this section, we examine the models in more detail in
an attempt to understand the reasons for such differences.
8.2.1 Comparison of PSums and the other two models
In order to analyze the performance of the models without shaving, we look at the mean
number of choice points statistics, which give an indication of the size of the search space
that is explored. To compare our three models, we look at the mean number of choice
points considered before the first feasible solution is found and the mean total number of
choice points explored within 600 seconds of run-time.
155

fiTerekhov & Beck

If-Then
PSums
Dual

21 Instances Solved by PSums Only
First Solution
Total
8234
137592
6415
464928
7528
102408

105 Instances Solved by All Models
First Solution
Total
1201
37596
1064
36590
1132
36842

Table 8: Mean number of choice points explored before the first solution is found and mean
total number of choice points explored within 600 seconds for the three models
without shaving and without dominance rules. For PSums, the latter statistic
corresponds to the total number of choice points needed to prove optimality.

In Table 4, it is shown that, without shaving, the PSums model proves optimality in
21 more instances than the other two models and that there are 105 instances in which all
three models prove optimality. In Table 8, we present the mean number of choice points
statistics for all three models for both of these sets of instances. It can be seen that the
mean number of choice points that need to be explored by the PSums model in order to
find an initial solution is smaller than for the other two models, both for the 105 instances
that are eventually solved to optimality by all models and for the 21 instances that are only
solved to optimality by PSums. Because the same variable and value ordering heuristics
are used in all models, this observation implies that more propagation occurs during search
in the PSums model than in the other two models. This claim is further supported by the
fact that the mean total number of choice points for the 105 instances that are solved by
all models is smaller for PSums than for the other two models.
Table 8 also shows that, for the 21 instances that are only solved to optimality by
PSums, the mean total number of choice points is the highest for the PSums model. Since
PSums is the only one out of the three models to solve these instances, this implies that
propagation is happening faster in this model. This observation is confirmed by the results
from the 105 instances that are solved by all three models: for these instances, the Dual
explores an average of 713 choice points per second, the If-Then model explores an average
of 895 choice points per second and the PSums model explores an average of 1989 choice
points per second. In other words, it appears that propagation in the PSums model is more
than twice as fast as in the other two models.
A more detailed examination of the results showed that in 82 out of the 105 instances
that were solved by all models, the number of choice points explored, for a given instance, is
the same for all models. Moreover, for instances which have the same value of S and N , the
number of choice points explored is equal. In Figure 9, the run-times of the three models
as the number of choice points increases is illustrated. In order to create this graph, we
averaged the run-times of all instances for which the number of choice points examined is
the same. Some points in the figure are labeled as (S, N ) in order to show the relationship
between the number of choice points, the values of S and N , and the run-times. We note
that there is one instance, when S = 10 and N = 6, for which the number of choice points
is the same as for the instances when S = 10 and N = 4. However, for all other instances
156

fiA CP Approach for a Queueing Control Problem

(70,3)

300

(40,4)
(30,5)

200

(60,3)
(20,9)
(20,8)
100

RunTime (seconds)

400

IfThen Model
PSums Model
Dual Model

(50,3)
(20,6)

0

(70,2)

(20,7)

0

50000

100000

150000

Choice Points

Figure 9: Run-times averaged over all instances with equal number of choice points explored, for 82 instances for which the number of choice points is the same for all
models. The labels in the points indicate the (S, N ) values for the corresponding
instances.

(out of 82), there is a one-to-one correspondence between (S, N ) and the number of choice
points.
Several observations can be made from Figure 9. Firstly, this graph demonstrates that
propagation in the PSums model is faster than in the other models. Secondly, the behaviour
of the PSums model appears to be quite different from that of both the If-Then and the
Dual models. The run-times of the PSums model seem to be significantly influenced by the
value of N . For example, when S = 20, the run-times of this model increase as N increases
157

fiTerekhov & Beck

from 6 to 9. Moreover, given two instances of which one has a high S and a low N , and
the other has a low S and a high N , the PSums model generally needs a longer time to
solve the instance with a low S and a high N (e.g., compare the points (20, 7) and (40, 4),
or (20, 7) and (70, 3)). For the If-Then and the Dual models, there are several cases when
the run-times for instances with a high S and a low N are higher than for instances with a
low S and a high N (e.g., compare the run-time at (70, 3) with that of (20, 7)), although
the opposite happens as well (e.g., compare (50, 3) and (40, 4)). Thus, it appears that N
is the parameter influencing the run-times of PSums the most, while for other two models,
both S and N are influential, with S having a greater effect. Although these characteristics
require additional investigation, one possible reason for such differences in model behaviour
could be the relationship between the number of constraints in the models and the problem
parameters. From Table 3, it is known that the number of constraints is mostly determined
by the value of S in the If-Then and Dual models (since S is typically larger than N ), and
by the value of N in the PSums model. Combining observations from Table 3 and Figure
9, it appears that the effect of N and S on the run-times is due to their influence on the
number of constraints in the models.
Overall, this examination indicates that the superiority of the PSums model without
shaving is caused by its stronger propagation (Table 8) and by the fact that propagation is
faster (Figure 9).
When shaving is employed, the PSums model also performs better than the Dual and
If-Then models, proving optimality in a greater number of instances (Table 4) and finding
good-quality solutions faster (Figure 6). In all models, the shaving procedures make the
same number of domain reductions because shaving is based on the Wq and Bl constraints,
which are present in all models. However, the time that each shaving iteration takes is
different in different models. Our empirical results show that each iteration of shaving
takes a smaller amount of time with the PSums model than with the If-Then and Dual
models. Thus, with shaving, the PSums model performs better than the other two, both
because shaving is faster and because subsequent search, if it is necessary, is faster.
8.2.2 Comparison of the If-Then and the Dual models
A comparison of the If-Then model with AlternatingSearchAndShaving with the Dual with
AlternatingSearchAndShaving using Figure 6 shows that the If-Then model is usually able
to find good solutions in a smaller amount of time. Moreover, as shown in Table 5, the
If-Then model with AlternatingSearchAndShaving finds the best solution in three more
instances, and proves optimality in two more instances, than the Dual model with the same
shaving procedure. With other shaving procedures and without shaving, the same statistics
show almost no difference in the performance of the two models. It was expected that
the Dual would outperform the If-Then model because it uses a simpler representation of
the balance equations and expressions for F and L, and has a smaller number of if-then
constraints. (there are Table 8 shows that the Dual has to explore a smaller number of
choice points to find the initial solution. In the 105 instances that both of these models
solve, the total number of choice points explored by the Dual is also smaller. However, the
If-Then model is faster, exploring, on average, 895 choice points per second compared to
the average of 713 choice points per second explored by the Dual. One possible explanation
158

fiA CP Approach for a Queueing Control Problem

for the Dual being slower is the fact that it has to assign more variables (via propagation)
than the other models. In particular, in order to represent a switching policy, the Dual has
to assign S + 1 wj variables in addition to N + 1 ki variables, with S usually being much
larger than N .
8.3 Performance of the PSums-P1 Hybrid
Experimental results demonstrate that the PSums-P1 Hybrid finds good solutions quickly
and is able to prove optimality in a large number of instances. It should be noted however,
that no synergy results from the combination: the number of instances for which optimality
is proved does not increase and the run-times do not decrease. Moreover, the hybrid model
finds the best-known solution in all test instances simply because there are cases in which
only the PSums model or only the heuristic is able to do so. No new best solutions are
obtained by using the PSums-P1 Hybrid to solve the problem. It appears that starting
the PSums model from the solution found by the heuristic does not lead to a significant
increase in the amount of propagation. Also, the fact that the heuristic finds a good-quality
solution does not improve the overall performance since, if search has to be used, placing a
constraint on Wq that requires all further solutions to be of better quality has little effect
on the domains of the decision variables. These observations imply that in order to create
a more effective model for the problem, one would need to improve back-propagation by
adding new constraints or reformulating the existing ones. If back-propagation is improved,
the good-quality heuristic solution may result in better performance of the hybrid approach.
8.4 Perspectives
The CP methods that we have developed are, in some ways, non-standard. More common
approaches when faced with the poor results of our three basic CP models (without shaving)
would have been to create better models, to develop a global constraint that could represent
and efficiently reason about some relevant sub-structure in the problem, and/or to invent
more sophisticated variable- and value-ordering heuristics. Shaving is more a procedural
technique that must be customized to exploit a particular problem structure. In contrast, a
better model or the creation of a global constraint are more in-line with the declarative goals
of CP. Our decision to investigate shaving arose from the recognition of the need to more
tightly link the optimization function with the decision variables and the clear structure of
the problem that both appeared and proved to be ideal for shaving.
We believe that there is scope for better models and novel global constraints. Modelling
problem P1 using CP was not straightforward because the formulation proposed by Berman
et al. contains expressions such as Equation (6), where the upper and lower limits of a sum
of auxiliary variables are decision variables. Such constraints are not typical for problems
usually modelled and solved by CP and there appear to be no existing global constraints
that could have been used to facilitate our approach. In spite of these issues, our models
demonstrate that CP is flexible enough to support queueing constraints. However, we
believe it is likely that the generalized application of CP to solve a larger class of queueing
control problems will require global constraints specific to expressions commonly occurring
in queueing theory. Given the back-propagation analysis and the fact that the problem is
159

fiTerekhov & Beck

to find and prove optimality, we are doubtful that, for P1 , sophisticated search heuristics
will perform significantly better than our simple heuristics.
While this is both the first time that CP has been used to solve any queueing control
problem and the first time that instances of P1 have been provably solved to optimality,
the work in this paper can be viewed as somewhat narrow: it is a demonstration that a
particular queueing control problem can be solved by constraint programming techniques.
The work does not immediately deliver solutions to more general problems, however, we
believe that it does open up a number of directions of inquiry into such problems.
1. It appears that there is no standard method within queueing theory to address queueing control optimization problems. This first application opens the issue of whether
CP can become an approach of choice for such problems.
2. As noted in Section 1, there is increasing interest in incorporating reasoning about
uncertainty into CP-based problem solving. Queueing theory can provide formulations that allow direct calculation of stochastic quantities based on expectation. The
challenge for CP is to identify the common sub-structures in such formulations and
develop modelling, inference, and search techniques that can exploit them.
3. Challenging scheduling problems, such as staff rostering at call centres (Cezik &
LEcuyer, 2008), consist of queues as well as rich resource and temporal constraints
(e.g., multiple resource requirements, alternative resources of different speeds, task
deadlines, precedence relations between tasks). We believe that a further integration
of CP and queueing theory could prove to be a promising approach to such problems.
4. The ability to reason about resource allocation under uncertainty is an important
component of definitions of intelligent behaviour such as bounded rationality (Simon,
1997). While we cannot claim to have made significant contribution in this direction,
perhaps ideas from queueing theory can serve as the inspiration for such contributions
in the future.

9. Related Work and Possible Extensions
Several papers exist that deal with similar types of problems as the one considered here.
For example, Berman and Larson (2004) study the problem of switching workers between
two rooms in a retail facility where the customers in the front room are divided into two
categories, those shopping in the store and those at the checkout. Palmer and Mitrani
(2004) consider the problem of switching computational servers between different types of
jobs where the randomness of user demand may lead to unequal utilization of resources.
Batta, Berman and Wang (2007) study the problem of assigning cross-trained customer
service representatives to different types of calls in a call centre, depending on estimated
demand patterns for each type of call. These three papers provide examples of problems for
which CP could prove to be a useful approach. Investigating CP solutions to these problems
is therefore one possible direction of future work.
Further work may also include looking at extensions of the problem discussed in this
paper. For example, we may consider a more realistic problem in which there are resource
constraints for one or both of the rooms, or in which workers have varying productivity.
160

fiA CP Approach for a Queueing Control Problem

Another direction for further work is improvement of the proposed models. In particular, in all models, but especially in PSums, there are constraints with variable exponents.
One idea for improving the performance of these constraints is to explicitly represent the
differences between switching points (i.e., ki+1  ki ) as variables.5 Another idea is to investigate a model based on the logarithms of probabilities rather than the probabilities
themselves. Additionally, ways of increasing the amount of back-propagation need to be
examined.
The goal of this paper was to demonstrate the applicability of constraint programming
to solving a particular queueing control problem. The main direction for future work is,
therefore, to explore the possibility of further integrating CP and queueing theory in an
attempt to address stochastic scheduling and resource allocation problems. Such problems
are likely to involve complex constraints, both for encoding the necessary stochastic information and for stating typical scheduling requirements such as task precedences or resource
capacities. Combining queueing theory with CP may help in solving such problems.

10. Conclusions
In this paper, a constraint programming approach is proposed for the problem of finding
the optimal states to switch workers between the front room and the back room of a retail facility under stochastic customer arrival and service times. This is the first work of
which we are aware that examines solving such stochastic queueing control problems using
constraint programming. The best pure CP method proposed is able to prove optimality
in a large proportion of instances within a 10-minute time limit. Previously, there existed
no non-heuristic solution to this problem aside from naive enumeration. As a result of our
experiments, we hybridized the best pure CP model with the heuristic proposed for this
problem in the literature. This hybrid technique is able to achieve performance that is
equivalent to, or better than, that of each of the individual approaches alone: it is able to
find very good solutions in a negligible amount of time due to the use of the heuristic, and is
able to prove optimality in a large proportion of problem instances within 10 CPU-minutes
due to the CP model.
This paper demonstrates that constraint programming can be a good approach for solving a queueing control optimization problem. For queueing problems for which optimality is
important or heuristics do not perform well, CP may prove to be an effective methodology.

Appendix A. Constraint Derivations
In this section, the derivations of constraints in the PSums model and expressions for the
auxiliary variables and constraints are presented.
A.1 Closed-form Expressions in the PSums model
The PSums model has two sets of probability variables, P (ki ), i = 0, 1, . . . , N , the probability of there being ki customers in the front room, and P Sums(ki ), i = 0, 1, . . . , N  1,
the sum of probabilities between two switching point variables. Balance equations are not
5. Thanks to an anonymous reviewer for this suggestion.

161

fiTerekhov & Beck

explicitly stated in this model. However, expressions for P (ki ) and P Sums(ki ) are derived in such a way that the balance equations are satisfied. The technique used for these
derivations is similar to that used by Berman et al. (2005) to simplify the calculation of
probabilities.
Consider the balance equation P (j) = P (j + 1)i, which is true for j = ki1 , ki1 +
1, . . . , ki  1 and any i  {1, 2 . . . , N }. In particular, this subset of the balance equations is
P (ki1 ) = P (ki1 + 1)i
P (ki1 + 1) = P (ki1 + 2)i
..
.
P (ki  1) = P (ki )i.
These equations imply the following expressions:
P (ki1 )
i
P (ki1 + 1)
i

= P (ki1 + 1)

(23)

= P (ki1 + 2)
..
.

P (ki  1)
i

= P (ki ).

Combining these together, we get P (ki ) =
P (ki+1 ) =




(i + 1)

ki+1 ki

 ki ki1

i

P (ki1 ) for all i from 1 to N , or

P (ki ), i  {0, 1, . . . , N  1}.

(24)

This equation is included in the PSums model and has previously been stated as Equation
(15).

Similarly, from Equation (23), we see that P (ki1 + 1) = i
P (ki1 ) for all i from 1 to
N , or
P (ki + 1) =


P (ki ), i  {0, 1, . . . , N }.
(i + 1)

(25)

Using Equation (25), an expression for P Sums(ki ), the sum of probabilities P (j) for j
between ki and ki+1  1, can be derived as follows:
ki+1 1

P Sums(ki ) =

X

P (j)

j=ki

= P (ki ) + P (ki + 1) + P (ki + 2) + . . . + P (ki+1  1)

2


= P (ki ) + P (ki )
+ P (ki )
(i + 1)
(i + 1)
162

fiA CP Approach for a Queueing Control Problem


ki+1 ki 1

+ . . . + P (ki )
(i + 1)
"


2
ki+1 ki 1 #



+
+ ... +
= P (ki ) 1 +
(i + 1)
(i + 1)
(i + 1)

ki+1 ki





1


(i + 1)


 P (ki )
6= 1
if (i+1)

=
1

(i + 1)






P (ki )(ki+1  ki )
otherwise.

(26)


+
The last step in the derivation is based on the observation that the expression [1 + (i+1)

2

ki+1 ki 1



+ . . . + (i+1)
] is a geometric series with the common ratio (i+1)
.
(i+1)

is 1, the expression is simply a sum of ki+1  ki ones. The expression for
When (i+1)
P Sums(ki ) has been previously stated as Equation (14).

A.1.1 Expected Number of Workers Constraint
F can be expressed in terms of P (ki ) and P Sums(ki ) using the following sequence of steps:
F

=

ki
X

N
X

iP (j)

i=1 j=ki1 +1

=

=

N
X

i=1
N
X

i [P (ki1 + 1) + P (ki1 + 2) + . . . + P (ki  1) + P (ki )]
i [P Sums(ki1 )  P (ki1 ) + P (ki )].

(27)

i=1

A.1.2 Expected Number of Customers Constraints
The equation for L can be derived in a similar manner:
L =

kN
X

jP (j)

j=k0

=

kX
1 1
j=k0

jP (j) +

kX
2 1

jP (j) + . . . +

j=k1

kX
N 1

jP (j) + kN P (kN )

j=kN1

= L(k0 ) + L(k1 ) + . . . + L(kN 1 ) + kN P (kN )
=

N
1
X

L(ki ) + kN P (kN )

(28)

i=0

163

fiTerekhov & Beck

where
L(ki ) = ki P (ki ) + (ki + 1)P (ki + 1) + (ki + 2)P (ki + 2) + . . . + (ki+1  1)P (ki+1  1)
= ki P (ki ) + ki P (ki + 1) + ki P (ki + 2) + . . . + ki P (ki+1  1) + P (ki + 1)
+ 2P (ki + 2) + . . . + (ki+1  ki  1)P (ki+1  1)
= ki [P (ki ) + P (ki + 1) + P (ki + 2) + . . . + P (ki+1  1)] + P (ki + 1)
+ 2P (ki + 2) + . . . + (ki+1  ki  1)P (ki+1  1)

2


+ 2P (ki )
= ki P Sums(ki ) + P (ki )
(i + 1)
(i + 1)
ki+1 ki 1


+ . . . + (ki+1  ki + 1)P (ki )
(i + 1)

= ki P Sums(ki ) + P (ki )
(i + 1)
"

2


 1+2
+ ...
+3
(i + 1)
(i + 1)

ki+1 ki 2 #

+ (ki+1  ki  1)P (ki )
(i + 1)

= ki P Sums(ki ) + P (ki )
(i + 1)

ki+1 ki 1

X

n=0




n
(i + 1)

n1

(29)


= ki P Sums(ki ) + P (ki )
(i + 1)


ki+1 ki 1
ki+1 ki



(ki  ki+1 ) + (i+1)
(ki+1  ki  1) + 1 
 (i+1)

.
2


1  (i+1)
A.2 Auxiliary Variables
All constraint programming models contain Equation (13) (restated below as Equation
(30)) for defining the Sum(ki ) variables, which are necessary for expressing an auxiliary
constraint that ensures that the balance equations have a unique solution. The validity of
this equation is proved by the following derivation, which uses the formula for the sum of a
finite geometric series in the last step:
Sum(ki ) =

ki
X

j

j=ki1 +1

 ki1 +1k0  ki1 +1ki1
1

Xi

i
 ki1 +2k0  ki1 +2ki1
 ki k0  ki ki1


1
1
+
Xi + . . . +
Xi

i

i
=

164

fiA CP Approach for a Queueing Control Problem

 ki1 k0 +1  

1
= Xi

i
"
 2  2
 k1 k0 (ki1 k0 +1)  ki ki1 1 #


1
1
1
+
+ ... +
 1+
i

i

i

 ki1 k0 +1   ki k
i1 1 
X
1
 n

= Xi

i
i
n=0



!
 ki ki1






1



 ki1 k0 +1 1 
i !





if i
6= 1
X

i





i

1
i
=




 ki1 k0 +1  




1


Xi
(ki  ki1 ) otherwise.

i

(30)

Acknowledgments This research was supported in part by the Natural Sciences and
Engineering Research Council and ILOG, S.A. Thanks to Nic Wilson and Ken Brown for
discussions and comments on this work, and to Tom Goguen for careful proofreading of
the final copy. A preliminary version of parts of this work has been previously published
(Terekhov & Beck, 2007).

References
Baptiste, P., Le Pape, C., & Nuijten, W. (2001). Constraint-based Scheduling. Kluwer
Academic Publishers.
Batta, R., Berman, O., & Wang, Q. (2007). Balancing staffing and switching costs in a
service center with flexible servers. European Journal of Operational Research, 177,
924938.
Beck, J. C., & Prestwich, S. (2004). Exploiting dominance in three symmetric problems. In
Fourth International Workshop on Symmetry and Constraint Satisfaction Problems.
Beck, J. C., & Wilson, N. (2007). Proactive algorithms for job shop schedulng with probabilistic durations. Journal of Artificial Intelligence Research, 28, 183232.
Berman, O., & Larson, R. (2004). A queueing control model for retail services having back
room operations and cross-trained workers. Computers and Operations Research,
31 (2), 201222.
Berman, O., Wang, J., & Sapna, K. P. (2005). Optimal management of cross-trained workers
in services with negligible switching costs. European Journal of Operational Research,
167 (2), 349369.
165

fiTerekhov & Beck

Brown, K. N., & Miguel, I. (2006). Uncertainty and change. In Rossi, F., van Beek, P.,
& Walsh, T. (Eds.), Handbook of Constraint Programming, chap. 21, pp. 731760.
Elsevier.
Caseau, Y., & Laburthe, F. (1996). Cumulative scheduling with task intervals. In Proceedings of the Joint International Conference and Symposium on Logic Programming,
pp. 363377. MIT Press.
Cezik, M. T., & LEcuyer, P. (2008). Staffing multiskill call centers via linear programming
and simulation. Management Science, 54 (2), 310323.
Demassey, S., Artigues, C., & Michelon, P. (2005). Constraint-propagation-based cutting
planes: An application to the resource-constrained project scheduling problem. INFORMS Journal on Computing, 17 (1), 5265.
Fox, M. S. (1983). Constraint-Directed Search: A Case Study of Job-Shop Scheduling. Ph.D.
thesis, Carnegie Mellon University, Intelligent Systems Laboratory, The Robotics Institute, Pittsburgh, PA. CMU-RI-TR-85-7.
Gross, D., & Harris, C. (1998). Fundamentals of Queueing Theory. John Wiley & Sons,
Inc.
Hnich, B., Smith, B. M., & Walsh, T. (2004). Dual modelling of permutation and injection
problems. Journal of Artificial Intelligence Research, 21, 357391.
Martin, P., & Shmoys, D. B. (1996). A new approach to computing optimal schedules for
the job shop scheduling problem. In Proceedings of the Fifth Conference on Integer
Programming and Combinatorial Optimization, pp. 389403.
Palmer, J., & Mitrani, I. (2004). Optimal server allocation in reconfigurable clusters with
multiple job types. In Proceedings of the International Conference on Computational
Science and Its Applications (ICCSA04), pp. 7686.
Policella, N., Smith, S. F., Cesta, A., & Oddi, A. (2004). Generating robust schedules
through temporal flexibility. In Proceedings of the Fourteenth International Conference
on Automated Planning and Scheduling (ICAPS04), pp. 209218.
Simon, H. A. (1997). Models of Bounded Rationality, Vol. 3. MIT Press.
Smith, B. M. (2005). Modelling for constraint programming. Lecture Notes for the
First International Summer School on Constraint Programming. Available at:
http://www.math.unipd.it/frossi/cp-school/.
Smith, B. M. (2006). Modelling. In Rossi, F., van Beek, P., & Walsh, T. (Eds.), Handbook
of Constraint Programming, chap. 11, pp. 377406. Elsevier.
Solver (2006). ILOG Scheduler 6.2 Users Manual and Reference Manual. ILOG, S.A.
Sutton, A. M., Howe, A. E., & Whitley, L. D. (2007). Using adaptive priority weighting to
direct search in probabilistic scheduling. In Proceedings of the Seventeenth International Conference on Automated Planning and Scheduling, pp. 320327.
Tadj, L., & Choudhury, G. (2005). Optimal design and control of queues. TOP, 13 (2),
359412.
166

fiA CP Approach for a Queueing Control Problem

Tarim, S. A., Manandhar, S., & Walsh, T. (2006). Stochastic constraint programming: A
scenario-based approach. Constraints, 11 (1), 5380.
Tarim, S. A., & Miguel, I. (2005). A hybrid Benders decomposition method for solving
stochastic constraint programs with linear recourse.. In Joint ERCIM/CoLogNET
International Workshop on Constraint Solving and Constraint Logic Programming,
pp. 133148.
Terekhov, D. (2007). Solving queueing design and control problems with constraint programming. Masters thesis, Department of Mechanical and Industrial Engineering,
University of Toronto.
Terekhov, D., & Beck, J. C. (2007). Solving a stochastic queueing control problem with
constraint programming. In Proceedings of the Fourth International Conference on
Integration of AI and OR Techniques in Constraint Programming for Combinatorial
Optimization Problems (CPAIOR07), pp. 303317. Springer-Verlag.
van Dongen, M. R. C. (2006). Beyond singleton arc consistency. In Proceedings of the
Seventeenth European Conference on Artificial Intelligence (ECAI06), pp. 163167.
Walsh, T. (2002). Stochastic constraint programming. In Proceedings of the Fifteenth
European Conference on Artificial Intelligence, pp. 111115.

167

fiJournal of Artificial Intelligence Research 32 (2008) 37-94

Submitted 08/07; published 05/08

Extended RDF as a Semantic Foundation of
Rule Markup Languages
Anastasia Analyti

analyti@ics.forth.gr

Institute of Computer Science, FORTH-ICS, Crete, Greece

Grigoris Antoniou

antoniou@ics.forth.gr

Institute of Computer Science, FORTH-ICS, Crete, Greece
Department of Computer Science, University of Crete, Greece

Carlos Viegas Damasio

cd@di.fct.unl.pt

Centro de Inteligencia Artificial, Universidade Nova de Lisboa,
Caparica, Portugal

Gerd Wagner

G.Wagner@tu-cottbus.de

Institute of Informatics, Brandenburg University
of Technology at Cottbus, Germany

Abstract
Ontologies and automated reasoning are the building blocks of the Semantic Web initiative. Derivation rules can be included in an ontology to define derived concepts, based on
base concepts. For example, rules allow to define the extension of a class or property, based
on a complex relation between the extensions of the same or other classes and properties.
On the other hand, the inclusion of negative information both in the form of negation-asfailure and explicit negative information is also needed to enable various forms of reasoning.
In this paper, we extend RDF graphs with weak and strong negation, as well as derivation
rules. The ERDF stable model semantics of the extended framework (Extended RDF) is
defined, extending RDF(S) semantics. A distinctive feature of our theory, which is based
on Partial Logic, is that both truth and falsity extensions of properties and classes are
considered, allowing for truth value gaps. Our framework supports both closed-world and
open-world reasoning through the explicit representation of the particular closed-world assumptions and the ERDF ontological categories of total properties and total classes.

1. Introduction
The idea of the Semantic Web is to describe the meaning of web data in a way suitable
for automated reasoning. This means that descriptive data (meta-data) in machine readable form are to be stored on the web and used for reasoning. Due to its distributed and
world-wide nature, the Web creates new problems for knowledge representation research.
Berners-Lee (1998) identifies the following fundamental theoretical problems: negation and
contradictions, open-world versus closed-world assumptions, and rule systems for the Semantic Web. For the time being, the first two issues have been circumvented by discarding
the facilities to introduce them, namely negation and closed-world assumptions. Though the
web ontology language OWL (McGuinness & van Harmelen, 2004), which is based on Description Logics (DLs) (Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2003),
includes a form of classical negation through class complements, this form is limited. This
c
2008
AI Access Foundation. All rights reserved.

fiAnalyti, Antoniou, Damasio, & Wagner

is because, to achieve decidability, classes are formed based on specific class constructors
and negation on properties is not fully considered. Rules constitute the next layer over the
ontology languages of the Semantic Web and, in contrast to DL, allow arbitrary interaction
of variables in the body of the rules. The widely recognized need of having rules in the Semantic Web, demonstrated by the Rule Markup Initiative1 , has restarted the discussion of
the fundamentals of closed-world reasoning and the appropriate mechanisms to implement
it in rule systems.
The RDF(S)2 recommendation (Klyne & Carroll, 2004; Hayes, 2004) provides the basic constructs for defining web ontologies and a solid ground to discuss the above issues.
RDF(S) is a special predicate logical language that is restricted to existentially quantified conjunctions of atomic formulas, involving binary predicates only. Due to its purpose,
RDF(S) has a number of special features that distinguish it from traditional logic languages:
1. It uses a special jargon, where the things of the universe of discourse are called resources, types are called classes, and binary predicates are called properties. Like
binary relations in set theory, properties have a domain and a range. Resources are
classified with the help of the property rdf :type (for stating that a resource is of type
c, where c is a class).
2. It distinguishes a special sort of resources, called literal values, which are denotations
of lexical representations of strings, numbers, dates, or other basic datatypes.
3. Properties are resources, that is, properties are also elements of the universe of discourse. Consequently, it is possible to state properties of properties, i.e., make statements about predicates.
4. All resources, except anonymous ones and literal values, are named with the help of
a globally unique reference schema, called Uniform Resource Identifier (URI)3 , that
has been developed for the Web.
5. RDF(S) comes with a non-standard model-theoretic semantics developed by Pat Hayes
on the basis of an idea of Christopher Menzel, which allows self-application without
violating the axiom of foundation. An example of this is the provable sentence stating
that rdfs:Class, the class of all classes, is an instance of itself.
However, RDF(S) does not support negation and rules. Wagner (1991) argues that a
database, as a knowledge representation system, needs two kinds of negation, namely weak
negation  (expressing negation-as-failure or non-truth) and strong negation  (expressing
explicit negative information or falsity) to be able to deal with partial information. In
a subsequent paper, Wagner (2003) makes also this point for the Semantic Web, as a
framework for knowledge representation in general. In the present paper, we make the
same argument for the Semantic Web language RDF and show how it can be extended
to accommodate the two negations of Partial Logic (Herre, Jaspars, & Wagner, 1999), as
well as derivation rules. We call the new language Extended RDF and denote it by ERDF.
1. http://www.ruleml.org/
2. RDF(S) stands for Resource Description Framework (Schema).
3. http://gbiv.com/protocols/uri/rfc/rfc3986.html

38

fiExtended RDF as a Semantic Foundation of Rule Markup Languages

The model-theoretic semantics of ERDF, called ERDF stable model semantics, is developed
based on Partial Logic (Herre et al., 1999).
In Partial Logic, relating strong and weak negation at the interpretation level allows to
distinguish four categories of properties and classes. Partial properties are properties p that
may have truth-value gaps and truth-value clashes, that is p(x, y) is possibly neither true
nor false, or both true and false. Total properties are properties p that satisfy totalness,
that is p(x, y) is true or false (but possibly both). Coherent properties are properties p that
satisfy coherence, that is p(x, y) cannot be both true and false. Classical properties are total
and coherent properties. For classical properties p, the classical logic law applies: p(x, y) is
either true or false. Partial, total, coherent, and classical classes c are defined similarly, by
replacing p(x, y) by rdf :type(x, c).
Partial logic also allows to distinguish between properties (and classes) that are completely represented in a knowledge base and those that are not. The classification if a
property is completely represented or not is up to the owner of the knowledge base: the
owner must know for which properties there is complete information and for which there
is not. Clearly, in the case of a completely represented (closed ) property p, entailment of
p(x, y) allows to derive p(x, y), and the underlying completeness assumption has also
been called Closed-World Assumption (CWA) in the AI literature.
Such a completeness assumption for closing a partial property p by default may be
expressed in ERDF by means of the rule p(?x, ?y)  p(?x, ?y) and for a partial class
c, by means of the rule rdf :type(?x, c)  rdf :type(?x, c). These derivation rules are
called default closure rules. In the case of a total property p, default closure rules are
not applicable. This is because, some of the considered interpretations will satisfy p(x, y)
and the rest p(x, y)4 , preventing the preferential entailment of p(x, y). Thus, on total
properties, an Open-World Assumption (OWA) applies. Similarly to first-order-logic, in
order to infer negated statements about total properties, explicit negative information has
to be supplied, along with ordinary (positive) information.
As an example, consider an ERDF knowledge base KB that contains the facts:
interestedIn(Anastasia, SemanticWeb)

interestedIn(Grigoris, Robotics)

indicating that Anastasia is interested in the SemanticWeb area and Grigoris is interested
in the Robotics area. Then, the statement interestedIn(Anastasia, Robotics) is not satisfied
in the single intended model of KB . Thus, KB entails interestedIn(Anastasia, Robotics).
Assume now that the previous list of areas of interest is not complete for Anastasia or
Grigoris. Then, we should add to knowledge base KB the statement:
rdf :type(interestedIn, erdf :TotalProperty)
indicating that interestedIn is a total property. In this case, an open-world assumption is made for interestedIn and KB does not entail interestedIn(Anastasia, Robotics),
any longer. In particular, there is an intended model of the revised KB that satisfies
interestedIn(Anastasia, Robotics). Of course, if it is known that Anastasia is not interested
in Robotics then interestedIn(Anastasia, Robotics) should be added to KB .
Assume now that we add to KB the following facts:
4. On total properties p, the Law of Excluded Middle p(x, y)p(x, y) applies.

39

fiAnalyti, Antoniou, Damasio, & Wagner

hasCar (Anastasia, Suzuki )

hasCar (Grigoris, Volvo)

and assume that KB has complete knowledge on the property hasCar , as far as it concerns
elements in the Herbrand Universe of KB . Then, the default closure rule hasCar (?x , ?y) 
hasCar (?x , ?y) can be safely added to KB . As a result, hasCar (Anastasia, Volvo) is
satisfied in all intended models of KB . Thus, KB entails hasCar (Anastasia, Volvo).
The previous example shows the need for supporting both closed-world and open-world
reasoning in the same framework. Damasio et al. (2006) and Analyti et al. (2004) provide
further examples and arguments for this need. Unfortunately, classical logic and thus also
OWL support only open-world reasoning.
Specifically, in this paper:
1. We extend RDF graphs to ERDF graphs with the inclusion of strong negation, and
then to ERDF ontologies (or ERDF knowledge bases) with the inclusion of general
derivation rules. ERDF graphs allow to express existential positive and negative
information, whereas general derivation rules allow inferences based on formulas built
using the connectives , , , ,  and the quantifiers , .
2. We extend the vocabulary of RDF(S) with the terms erdf :TotalProperty and
erdf :TotalClass, representing the metaclasses of total properties and total classes, on
which the open-world assumption applies.
3. We extend RDFS interpretations to ERDF interpretations including both truth and
falsity extensions for properties and classes. Particularly, we consider only coherent
ERDF interpretations (imposing coherence on all properties). Thus, in this paper,
total properties and classes become synonymous to classical properties and classes.
4. We extend RDF graphs to ERDF formulas that are built from positive triples, using
the connectives , , , ,  and the quantifiers , . Then, we define ERDF
entailment between two ERDF formulas, extending RDFS entailment between RDF
graphs.
5. We define the ERDF models, the Herbrand interpretations, and the minimal Herbrand
models of an ERDF ontology. Since not all minimal Herbrand models of an ERDF
ontology are intended, we define the stable models of an ERDF ontology. The definition
of a stable model is based on the intuition that:
(a) assertions stating that a property p or class c is total should only be accepted, if
the ontology contains some direct support for them in the form of an acceptable
rule sequence, and
(b) assertions []p(s, o) and []rdf :type(o, c) should only be accepted, if (i) the ontology contains some direct support for them in the form of an acceptable rule
sequence, or (ii) property p and class c are total, respectively.
6. We show that stable model entailment on ERDF ontologies extends ERDF entailment
on ERDF graphs, and thus it also extends RDFS entailment on RDF graphs. Moreover, we show that if all properties are total, (boolean) Herbrand model reasoning and
stable model reasoning coincide. In this case, we make an open-world assumption for
all properties and classes.
40

fiExtended RDF as a Semantic Foundation of Rule Markup Languages

A distinctive feature of the developed framework with respect to Partial Logic (Herre
et al., 1999) is that properties and classes are declared as total on a selective basis, by
extending RDF(S) with new built-in classes and providing support for the respective ontological categories. In contrast, in Partial Logic (Herre et al., 1999), the choice of partial or
total should be taken for the complete set of predicates. Thus, the approach presented here
is, in this respect, more flexible and general.
This work extends our conference paper (Analyti, Antoniou, Damasio, & Wagner, 2005)
by (i) considering the full RDFS model, (ii) providing a detailed characterization of the
properties of ERDF interpretations/models, Herbrand interpretations/models, and finally
ERDF stable models, (iii) discussing decidability issues, and (iv) providing formal proofs of
all lemmas and propositions.
The rest of the paper is organized as follows: In Section 2, we extend RDF graphs to
ERDF graphs and ERDF formulas. Section 3 defines ERDF interpretations and ERDF
entailment. We show that ERDF entailment extends RDFS entailment. In Section 4, we
define ERDF ontologies and the Herbrand models of an ERDF ontology. In Section 5, we
define the stable models of an ERDF ontology. Section 6 defines stable model entailment,
showing that it extends ERDF entailment. In Section 7, we provide a brief sketch of the
ERDF/XML syntax. Decidability issues for the ERDF stable model semantics are discussed
in Section 8. Section 9 shows that the developed ERDF model theory can be seen as a
Tarski-style model theory. Section 10 reviews related work and Section 11 concludes the
paper, including future work. The main definitions of RDF(S) semantics are reviewed in
Appendix A. Appendix B includes the proofs of the lemmas and propositions, presented in
the paper.

2. Extending RDF Graphs with Negative Information
In this section, we extend RDF graphs to ERDF graphs, by adding strong negation. Moreover, we extend RDF graphs to ERDF formulas, which are built from positive ERDF triples,
the connectives , , , , , and the quantifiers , .
According to RDF concepts (Klyne & Carroll, 2004; Hayes, 2004), URI references
are used as globally unique names for web resources. An RDF URI reference is a Unicode string that represents an absolute URI (with an optional fragment identifier). It
may be represented as a qualified name, that is a colon-separated two-part string consisting of a namespace prefix (an abbreviated name for a namespace URI) and a local
name. For example, given the namespace prefix ex defined to stand for the namespace URI http://www.example.org/, the qualified name ex:Riesling (which stands for
http://www.example.org/Riesling) is a URI reference.
A plain literal is a string s, where s is a sequence of Unicode characters, or a pair of
a string s and a language tag t, denoted by s@t. A typed literal is a pair of a string
s and a datatype URI reference d, denoted by sd. For example, 27xsd:integer is
a typed literal.
A (Web) vocabulary V is a set of URI references and/or literals (plain or typed). We
denote the set of all URI references by URI, the set of all plain literals by PL, the set of
all typed literals by T L, and the set of all literals by LIT . It holds: URI  LIT = .
41

fiAnalyti, Antoniou, Damasio, & Wagner

In our formalization, we consider a set Var of variable symbols, such that the sets Var ,
URI, LIT are pairwise disjoint. In the main text, variable symbols are explicitly indicated,
while in our examples, variable symbols are prefixed by a question mark symbol ?.
An RDF triple (Klyne & Carroll, 2004; Hayes, 2004) is a triple s p o., where s 
URI  Var , p  URI, and o  URI  LIT  Var , expressing that the subject s is related
with the object o through the property p. An RDF graph is a set of RDF triples. The
variable symbols appearing in an RDF graph are called blank nodes, and are, intuitively,
existentially quantified variables. In this paper, we denote an RDF triple s p o. by
p(s, o). Below we extend the notion of RDF triple to allow for both positive and negative
information.
Definition 2.1 (ERDF triple) Let V be a vocabulary. A positive ERDF triple over V
(also called ERDF sentence atom) is an expression of the form p(s, o), where s, o  V  Var
are called subject 5 and object, respectively, and p  V  URI is called predicate or property.
A negative ERDF triple over V is the strong negation p(s, o) of a positive ERDF triple
p(s, o) over V . An ERDF triple over V (also called ERDF sentence literal ) is a positive or
negative ERDF triple over V . 
For example, ex:likes(ex:Gerd , ex:Riesling) is a positive ERDF triple, expressing that
Gerd likes Riesling, and ex:likes(ex:Carlos, ex:Riesling) is a negative ERDF triple, expressing that Carlos dislikes Riesling. Note that an RDF triple is a positive ERDF
triple with the constraint that the subject of the triple is not a literal. For example,
ex:denotationOf (Grigoris, ex:Grigoris) is a valid ERDF triple but not a valid RDF triple.
Our choice of allowing literals appearing in the subject position is based on our intuition
that this case can naturally appear in knowledge representation (as in the previous example). Prudhommeaux & Seaborne (2008) and de Bruijn et al. (2005) also consider literals
in the subject position of RDF triples.
Based on the notion of ERDF triple, we define ERDF graphs and ERDF formulas, as
follows:
Definition 2.2 (ERDF graph) An ERDF graph G is a set of ERDF triples over some
vocabulary V . We denote the variables appearing in G by Var (G), and the set of URI
references and literals appearing in G by VG . 
Note that as an RDF graph is a set of RDF triples (Klyne & Carroll, 2004; Hayes, 2004),
an RDF graph is also an ERDF graph.
Definition 2.3 (ERDF formula) Let V be a vocabulary. We consider the logical factors
{, , , , , , }, where , , and  are called strong negation, weak negation, and
material implication, respectively. We denote by L(V ) the smallest set that contains the
positive ERDF triples over V and is closed with respect to the following conditions: if
F, G  L(V ) then {F, F, F G, F G, F  G, xF, xF }  L(V ), where x  Var .
An ERDF formula over V is an element of L(V ). We denote the set of variables appearing
5. Opposed to pure RDF (Klyne & Carroll, 2004), we allow literals in the subject position of an ERDF
triple.

42

fiExtended RDF as a Semantic Foundation of Rule Markup Languages

in F by Var (F ), and the set of free variables6 appearing in F by FVar (F ). Moreover, we
denote the set of URI references and literals appearing in F by VF . 
For example, let:
F = ?x ?y (rdf :type(?x, ex:Person)  ex:hasChild (?y, ?x))  rdf :type(?z, ex:Person)
Then, F is an ERDF formula over the vocabulary V = {rdf :type, ex:Person, ex:hasChild }
with Var (F ) = {?x, ?y, ?z} and FVar (F ) = {?z}.
We will denote the sublanguages of L(V ) formed by means of a subset S of the logical
factors, by L(V |S). For example, L(V |{}) denotes the set of (positive and negative) ERDF
triples over V .

3. ERDF Interpretations
In this section, we extend RDF(S) semantics by allowing for partial properties and classes.
In particular, we define ERDF interpretations and satisfaction of an ERDF formula, based
on the notion of partial interpretation.
3.1 Partial Interpretations
We define a partial interpretation as an extension of a simple interpretation (Hayes, 2004),
where each property is associated not only with a truth extension but also with a falsity
extension allowing for partial properties. The notation P(S), where S is a set, denotes the
powerset of S.
Definition 3.1 (Partial interpretation) A partial interpretation I of a vocabulary V
consists of:
 A non-empty set of resources ResI , called the domain or universe of I.
 A set of properties P ropI .
 A vocabulary interpretation mapping IV 7 : V  URI  ResI  P ropI .
 A property-truth extension mapping PT I : P ropI  P(ResI  ResI ).
 A property-falsity extension mapping PF I : P ropI  P(ResI  ResI ).
 A mapping ILI : V  T L  ResI .
 A set of literal values LV I  ResI , which contains V  PL.

We define the mapping: I : V  ResI  P ropI , called denotation, such that:
 I(x) = IV (x), x  V  URI.
 I(x) = x,  x  V  PL.
 I(x) = ILI (x),  x  V  T L. 
6. Without loss of generality, we assume that a variable cannot have both free and bound occurrences in
F , and more than one bound occurrence.
7. In the symbol IV , V stands for Vocabulary.

43

fiAnalyti, Antoniou, Damasio, & Wagner

Note that the truth and falsity extensions of a property p according to a partial interpretation I, that is P TI (p) and P FI (p), are sets of pairs hsubject, objecti of resources. As
an example, let:
V = {ex:Carlos, ex:Grigoris, ex:Riesling, ex:likes, ex:denotationOf , Grigorisxsd:string}

and consider a structure I that consists of:
 A set of resources ResI = {C, G, R, l, d, Grigoris}.
 A set of properties P ropI = {l, d}.
 A vocabulary interpretation mapping IV : V  URI  ResI  P ropI such that:
IV (ex:Carlos) = C, IV (ex:Grigoris) = G, IV (ex:Riesling) = R, IV (ex:likes) = l, and
IV (ex:denotationOf ) = d.
 A property-truth extension mapping PT I : P ropI  P(ResI  ResI ) such that:
P TI (d) = {hGrigoris, Gi}.
 A property-falsity extension mapping PF I : P ropI  P(ResI  ResI ) such that:
P FI (l) = {hC, Ri}.
 A mapping ILI : V  T L  ResI such that: ILI (Grigorisxsd :string) = Grigoris.
 A set of literal values LV I = {Grigoris}.

It is easy to see that I is a partial interpretation of V , expressing that: (i) Grigoris
is the denotation of Grigoris and (ii) Carlos dislikes Riesling.
Definition 3.2 (Coherent partial interpretation) A partial interpretation I of a vocabulary V is coherent iff for all x  P ropI , PT I (x)  PF I (x) = . 
Coherent partial interpretations enforce the constraint that a pair of resources cannot
belong to both the truth and falsity extensions of a property (i.e., all properties are coherent). Intuitively, this means that an ERDF triple cannot be both true and false.
Continuing our previous example, note that I is a coherent partial interpretation.
Consider now a partial interpretation J which is exactly as I, except that it also holds:
P TJ (l) = {hC, Ri} (expressing that Carlos likes Riesling). Then, hC, Ri belongs to both
the truth and falsity extension of l (i.e., hC, Ri  P TJ (l)  P FJ (l)). Thus, J is not coherent.
To define satisfaction of an ERDF formula w.r.t. a partial interpretation, we need first
the following auxiliary definition.
Definition 3.3 (Composition of a partial interpretation and a valuation) Let I be
a partial interpretation of a vocabulary V and let v be a partial function v : Var  ResI
(called valuation). We define: (i) [I + v](x) = v(x), if x  Var , and (ii) [I + v](x) = I(x),
if x  V . 
Definition 3.4 (Satisfaction of an ERDF formula w.r.t. a partial interpretation
and a valuation) Let F, G be ERDF formulas and let I be a partial interpretation of a
vocabulary V . Additionally, let v be a mapping v : Var (F )  ResI .
 If F = p(s, o) then I, v |= F iff p  V  URI, s, o  V  Var , I(p)  P ropI , and
h[I + v](s), [I + v](o)i  PT I (I(p)).
44

fiExtended RDF as a Semantic Foundation of Rule Markup Languages

 If F = p(s, o) then I, v |= F iff p  V  URI, s, o  V  Var , I(p)  P ropI , and
h[I + v](s), [I + v](o)i  PF I (I(p)).
 If F = G then I, v |= F iff VG  V and I, v 6|= G.
 If F = F1 F2 then I, v |= F iff I, v |= F1 and I, v |= F2 .
 If F = F1 F2 then I, v |= F iff I, v |= F1 or I, v |= F2 .
 If F = F1  F2 then8 I, v |= F iff I, v |= F1 F2 .
 If F = x G then I, v |= F iff there exists mapping u : Var (G)  ResI such that u(y) = v(y),
y  Var (G)  {x}, and I, u |= G.
 If F = x G then I, v |= F iff for all mappings u : Var (G)  ResI such that u(y) = v(y),
y  Var (G)  {x}, it holds I, u |= G.
 All other cases of ERDF formulas are treated by the following DeMorgan-style rewrite rules
expressing the falsification of compound ERDF formulas:
(F  G)  F  G, (F  G)  F  G, (F )  F, ( F )  F 9 ,
(x F )  x F, (x F )  x F, (F  G)  F G. 

Continuing our previous example, let v : {?x, ?y, ?z}  ResI such that v(?x) = C,
v(?y) = R, and v(?z) = G. It holds:
I, v |= ex:likes(?x, ?y)  ex:denotationOf (Grigorisxsd:string, ?z).
Definition 3.5 (Satisfaction of an ERDF formula w.r.t. a partial interpretation)
Let F be an ERDF formula and let I be a partial interpretation of a vocabulary V . We say
that I satisfies F , denoted by I |= F , iff for every mapping v : Var (F )  ResI , it holds
I, v |= F. 
Continuing our previous example, I |= ?x ex:likes(ex:Carlos, ?x).
Below we define ERDF graph satisfaction, extending satisfaction of an RDF graph
(Hayes, 2004) (see also Appendix A).
Definition 3.6 (Satisfaction of an ERDF graph w.r.t. a partial interpretation) Let
G be an ERDF graph and let I be a partial interpretation of a vocabulary V . Let v be a
mapping v : Var (G)  ResI . We define:
 I, v |=GRAPH G iff t  G, I, v |= t.
 I satisfies the ERDF graph G, denoted by I |=GRAPH G, iff there exists a mapping
v : Var (G)  ResI such that I, v |=GRAPH G. 
Intuitively, an ERDF graph G represents an existentially quantified conjunction of
ERDF triples. Specifically, let G = {t1 , ..., tn } be an ERDF graph, and let Var (G) =
{x1 , ..., xk }. Then, G represents the ERDF formula formula(G) = ?x1 , ..., ?xk t1  ...  tn .
This is shown in the following lemma.
8. Material implication is the logical relationship between any two ERDF formulas such that either the first
is non-true or the second is true.
9. This transformation expresses that if it is false that F does not hold then F holds.

45

fiAnalyti, Antoniou, Damasio, & Wagner

Lemma 3.1 Let G be an ERDF graph and let I be a partial interpretation of a vocabulary
V . It holds: I |=GRAPH G iff I |= formula(G).
Following the RDF terminology (Klyne & Carroll, 2004), the variables of an ERDF
graph are also called blank nodes and intuitively denote anonymous web resources. For
example, consider the ERDF graph:
G = {rdf :type(?x, ex:EuropeanCountry), rdf :type(?x, ex:EU member)}.
Then, G represents the ERDF formula formula(G) =
?x (rdf :type(?x, ex:EuropeanCountry)  rdf :type(?x, ex:EU member)),
expressing that there is a European country which is not a European Union member.
Notational Convention: Let G be an ERDF graph, let I be a partial interpretation of
a vocabulary V , and let v be a mapping v : Var (G)  ResI . Due to Lemma 3.1, we will
write (by an abuse of notation) I, v |= G and I |= G instead of I, v |=GRAPH G and
I |=GRAPH G, respectively.
3.2 ERDF Interpretations and Entailment
In this subsection, we define ERDF interpretations and entailment as an extension of RDFS
interpretations and entailment (Hayes, 2004). First, we define the vocabularies of RDF,
RDFS, and ERDF.
The vocabulary of RDF, VRDF , is a set of URI references in the rdf : namespace (Hayes,
2004), as shown in Table 1. The vocabulary of RDFS, VRDF S , is a set of URI references in
the rdfs: namespace (Hayes, 2004), as shown in Table 1. The vocabulary of ERDF , VERDF ,
is a set of URI references in the erdf : namespace. Specifically, the set of ERDF predefined
classes is CERDF = {erdf :TotalClass, erdf :TotalProperty}. We define VERDF = CERDF .
Intuitively, instances of the metaclass erdf :TotalClass are classes c that satisfy totalness,
meaning that each resource belongs to the truth or falsity extension of c. Similarly, instances
of the metaclass erdf :TotalProperty are properties p that satisfy totalness, meaning that
each pair of resources belongs to the truth or falsity extension of p.
We are now ready to define an ERDF interpretation over a vocabulary V as an extension
of an RDFS interpretation (Hayes, 2004) (see also Appendix A), where each property and
class is associated not only with a truth extension but also with a falsity extension, allowing
for both partial properties and partial classes. Additionally, an ERDF interpretation gives
special semantics to terms from the ERDF vocabulary.
Definition 3.7 (ERDF interpretation) An ERDF interpretation I of a vocabulary V
is a partial interpretation of V  VRDF  VRDF S  VERDF , extended by the new ontological
categories ClsI  ResI for classes, TCls I  ClsI for total classes, and TProp I  P ropI for
total properties, as well as the class-truth extension mapping CT I : ClsI  P(ResI ), and
the class-falsity extension mapping CF I : ClsI  P(ResI ), such that:
1. x  CT I (y) iff hx, yi  PT I (I(rdf :type)), and
x  CF I (y) iff hx, yi  PF I (I(rdf :type)).
46

fiExtended RDF as a Semantic Foundation of Rule Markup Languages

VRDF
rdf :type
rdf :Property
rdf :XMLLiteral
rdf :nil
rdf :List
rdf :Statement
rdf :subject
rdf :predicate
rdf :object
rdf :first
rdf :rest
rdf :Seq
rdf :Bag
rdf :Alt
rdf : i, i  {1, 2, ...}
rdf :value

VRDF S
rdfs:domain
rdfs:range
rdfs:Resource
rdfs:Literal
rdfs:Datatype
rdfs:Class
rdfs:subClassOf
rdfs:subPropertyOf
rdfs:member
rdfs:Container
rdfs:ContainerMembershipProperty
rdfs:comment
rdfs:seeAlso
rdfs:isDefinedBy
rdfs:label

Table 1: The vocabulary of RDF and RDFS
2. The ontological categories are defined
P ropI = CT I (I(rdf :Property))
ResI = CT I (I(rdfs:Resource))
TCls I = CT I (I(erdf :TotalClass))

as follows:
ClsI = CT I (I(rdfs:Class))
LV I = CT I (I(rdfs:Literal))
TProp I = CT I (I(erdf :TotalProperty)).

3. If hx, yi  PT I (I(rdfs:domain)) and hz, wi  PT I (x) then z  CT I (y).
4. If hx, yi  PT I (I(rdfs:range)) and hz, wi  PT I (x) then w  CT I (y).
5. If x  ClsI then hx, I(rdfs:Resource)i  PT I (I(rdfs:subClassOf )).
6. If hx, yi  PT I (I(rdfs:subClassOf )) then x, y  ClsI , CT I (x)  CT I (y), and
CF I (y)  CF I (x).
7. PT I (I(rdfs:subClassOf )) is a reflexive and transitive relation on ClsI .
8. If hx, yi  PT I (I(rdfs:subPropertyOf )) then x, y  P ropI , PT I (x)  PT I (y), and
PF I (y)  PF I (x).
9. PT I (I(rdfs:subPropertyOf )) is a reflexive and transitive relation on P ropI .
10. If x  CT I (I(rdfs:Datatype)) then hx, I(rdfs:Literal)i  PT I (I(rdfs:subClassOf )).
11. If x  CT I (I(rdfs:ContainerMembershipProperty)) then
hx, I(rdfs:member)i  PT I (I(rdfs:subPropertyOf )).
12. If x  TCls I then CT I (x)  CF I (x) = ResI .
13. If x  TProp I then PT I (x)  PF I (x) = ResI  ResI .

47

fiAnalyti, Antoniou, Damasio, & Wagner

14. If srdf :XMLLiteral  V and s is a well-typed XML literal string, then
ILI (srdf :XMLLiteral ) is the XML value of s, and
ILI (srdf :XMLLiteral )  CT I (I(rdf :XMLLiteral )).
15. If srdf :XMLLiteral  V and s is an ill-typed XML literal string then
ILI (srdf :XMLLiteral )  ResI  LV I , and
ILI (srdf :XMLLiteral )  CF I (I(rdfs:Literal)).
16. I satisfies the RDF and RDFS axiomatic triples (Hayes, 2004), shown in Table 2 and Table 3
of Appendix A, respectively.
17. I satisfies the following triples, called ERDF axiomatic triples:
rdfs:subClassOf (erdf :TotalClass, rdfs:Class).
rdfs:subClassOf (erdf :TotalProperty, rdfs:Class).

Note that while RDFS intepretations (Hayes, 2004) imply a two-valued interpretation
of the instances of rdf :Property, this is no longer the case with ERDF interpretations.
Specifically, let I be an ERDF interpretation, let p  CTI (I (rdf :Property)), and let hx, yi 
ResI  ResI . It may be the case that neither hx, yi  P TI (p) nor hx, yi  P FI (p). That is
p(x, y) is neither true nor false.
Semantic conditions of ERDF interpretations may impose constraints to both the truth
and falsity extensions of properties and classes. Specifically, consider semantic condition 6 of
Definition 3.7 and assume that hx, yi  PT I (I(rdfs:subClassOf )). Then, I should not only
satisfy CT I (x)  CT I (y) (as an RDFS interpretation I does), but also CF I (y)  CF I (x).
The latter is true because if it is certain that a resource z does not belong to the truth
extension of class y then it is certain that z does not belong to the truth extension of class
x. Thus, the falsity extension of y is contained in the falsity extension of x. Similar is the
case for semantic condition 8. Semantic conditions 12 and 13 represent our definition of
total classes and total properties, respectively. Semantic condition 15 expresses that the
denotation of an ill-typed XML literal is not a literal value. Therefore (see semantic condition 2), it is certain that it is not contained in the truth extension of the class rdfs:Literal.
Thus, it is contained in the falsity extension of the class rdfs:Literal.
Let I be a coherent ERDF interpretation of a vocabulary V . Since I(rdf :type)  P ropI ,
it holds: x  ClsI , CT I (x)  CF I (x) = . Thus, all properties and classes of coherent
ERDF interpretations are coherent.
Convention: In the rest of the document, we consider only coherent ERDF interpretations.
This means that referring to an ERDF interpretation, we implicitly mean a coherent
one. Moreover, to improve the readability of our examples, we will ignore the example
namespace ex:.
According to RDFS semantics (Hayes, 2004), the only source of RDFS-inconsistency is
the appearance of an ill-typed XML literal l in the RDF graph, in combination with the
derivation of the RDF triple x rdf :type rdfs:Literal. by the RDF and RDFS entailment
rules, where x is a blank node allocated to l10 . Such a triple is called XML clash. To
10. In RDF(S), literals are not allowed in the subject position of RDF triples, whereas blank nodes are. For
this reason, before the RDF and RDFS entailment rules are applied on an RDF graph, each literal is
replaced by a unique blank node. This way inferences can be drawn on the literal value denoted by this
literal, without concern for the above restriction (Hayes, 2004).

48

fiExtended RDF as a Semantic Foundation of Rule Markup Languages

understand this, note that from semantic condition 3 of Definition A.3 (RDF interpretation,
Appendix A), it follows that the denotation of an ill-typed XML literal cannot be a literal
value. Now, from semantic conditions 1 and 2 of Definition A.5 (RDFS interpretation,
Appendix A), it follows that the denotation of an ill-typed XML literal cannot be of type
rdfs:Literal. Therefore, the derivation of an XML clash from an RDF graph G through
the application of the RDF and RDFS entailment rules, indicates that there is no RDFS
interpretation that satisfies G.
An ERDF graph can be ERDF-inconsistent11 , not only due to the appearance of an
ill-typed XML literal in the ERDF graph (in combination with semantic condition 15 of
Definition 3.7), but also due to the additional semantic conditions for coherent ERDF
interpretations.
For example, let p, q, s, o  URI and let G = {p(s, o), rdfs:subPropertyOf (p, q), q(s, o)}.
Then, G is ERDF-inconsistent, since there is no (coherent) ERDF interpretation that satisfies G.
The following proposition shows that for total properties and total classes of (coherent)
ERDF interpretations, weak negation and strong negation coincide (boolean truth values).
Proposition 3.1 Let I be an ERDF interpretation of a vocabulary V and let V  = V 
VRDF  VRDF S  VERDF . Then,
1. For all p, s, o  V  such that I(p)  TProp I , it holds:
I |= p(s, o) iff I |= p(s, o) (equivalently, I |= p(s, o)  p(s, o)).
2. For all x, c  V  such that I(c)  TCls I , it holds:
I |= rdf :type(x, c) iff I |= rdf :type(x, c)
(equivalently, I |= rdf :type(x, c)  rdf :type(x, c)).
Below we define ERDF entailment between two ERDF formulas or ERDF graphs.
Definition 3.8 (ERDF entailment) Let F, F  be ERDF formulas or ERDF graphs. We
say that F ERDF-entails F  (F |=ERDF F  ) iff for every ERDF interpretation I, if I |= F
then I |= F  . 
For example, let:
F = ?x ?y (rdf :type(?x, Person)  hasFather (?x, ?y))  rdf :type(John, Person).
Additionally, let F  = ?y hasFather (John, ?y)  rdf :type(hasFather , rdf :Property).
Then F |=ERDF F  .
The following proposition shows that ERDF entailment extends RDFS entailment (Hayes,
2004) (see also Appendix A) from RDF graphs to ERDF formulas. In other words, ERDF
entailment is upward compatible with RDFS entailment.
Proposition 3.2 Let G, G be RDF graphs such that VG VERDF =  and VG VERDF = .
Then, G |=RDF S G iff G |=ERDF G .
It easily follows from Proposition 3.2 that an RDF graph is RDFS satisfiable iff it is
ERDF satisfiable. Thus, an RDF graph can be ERDF-inconsistent only due to an XML
clash.
11. Meaning that there is no (coherent) ERDF interpretation that satisfies the ERDF graph.

49

fiAnalyti, Antoniou, Damasio, & Wagner

4. ERDF Ontologies & Herbrand Interpretations
In this section, we define an ERDF ontology as a pair of an ERDF graph G and a set
P of ERDF rules. ERDF rules should be considered as derivation rules that allow us to
infer more ontological information based on the declarations in G. Moreover, we define the
Herbrand interpretations and the minimal Herbrand models of an ERDF ontology.
Definition 4.1 (ERDF rule, ERDF program) An ERDF rule r over a vocabulary V
is an expression of the form: G  F , where F  L(V )  {true} is called condition and
G  L(V |{})  {false} is called conclusion. We assume that no bound variable in F
appears free in G. We denote the set of variables and the set of free variables of r by Var (r)
and FVar (r)12 , respectively. Additionally, we write Cond(r) = F and Concl(r) = G.
An ERDF program P is a set of ERDF rules over some vocabulary V . We denote the set
of URI references and literals appearing in P by VP . 
Recall that L(V |{}) denotes the set of ERDF triples over V . Therefore, the conclusion
of an ERDF rule, unless it is false, it is either a positive ERDF triple p(s, o) or a negative
ERDF triple p(s, o).
For example, consider the derivation rule r:
allRelated (?P, ?Q)  ?p rdf :type(?p, ?P )  ?q (rdf :type(?q, ?Q)  related (?p, ?q)),

Then, r is an ERDF rule, indicating that between two classes P and Q, it holds allRelated (P,
Q) if for all instances p of the class P , there is an instance q of the class Q such that it
holds related (p, q). Note that Var (r) = {?P, ?Q, ?p, ?q} and FVar (r) = {?P, ?Q}.
When Cond(r) = true and Var (r) = {}, rule r is called ERDF fact. When Concl(r) =
false, rule r is called ERDF constraint. We assume that for every partial interpretation
I and every function v : Var  ResI , it holds I, v |= true, I |= true, I, v 6|= false, and
I 6|= false.
Intuitively, an ERDF ontology is the combination of (i) an ERDF graph G containing
(implicitly existentially quantified) positive and negative information, and (ii) an ERDF
program P containing derivation rules (whose free variables are implicitly universally quantified).
Definition 4.2 (ERDF ontology) An ERDF ontology (or ERDF knowledge base) is a
pair O = hG, P i, where G is an ERDF graph and P is an ERDF program. 
The following definition defines the models of an ERDF ontology.
Definition 4.3 (Satisfaction of an ERDF rule and an ERDF ontology) Let I be an
ERDF interpretation of a vocabulary V .
 We say that I satisfies an ERDF rule r, denoted by I |= r, iff for all mappings
v : Var (r)  ResI such that I, v |= Cond(r), it holds I, v |= Concl(r).
 We say that I satisfies an ERDF ontology O = hG, P i (also, I is a model of O),
denoted by I |= O, iff I |= G and I |= r,  r  P . 
12. FVar (r) = FVar (F )  FVar (G).

50

fiExtended RDF as a Semantic Foundation of Rule Markup Languages

In this paper, existentially quantified variables in ERDF graphs are handled by skolemization, a syntactic transformation commonly used in automatic inference systems for removing existentially quantified variables.
Definition 4.4 (Skolemization of an ERDF graph) Let G be an ERDF graph. The
skolemization function of G is an 1:1 mapping skG : Var (G)  URI, where for each
x  Var (G), skG (x) is an artificial URI, denoted by G:x. The set skG (Var (G)) is called
the Skolem vocabulary of G.
The skolemization of G, denoted by sk(G), is the ground ERDF graph derived from G after
replacing each variable x  Var (G) by skG (x). 
Intuitively, the Skolem vocabulary of G (that is, skG (Var (G))) contains artificial URIs
giving arbitrary names to the anonymous entities whose existence was asserted by the
use of blank nodes in G.
For example, let: G = {rdf :type(?x, EuropeanCountry), rdf :type(?x, EU member)}.
Then,
sk(G) = {rdf :type(skG (?x), EuropeanCountry), rdf :type(skG (?x), EU member)}.

The following proposition expresses that the skolemization of an ERDF graph has the
same entailments as the original graph, provided that these do not contain URIs from the
skolemization vocabulary.
Proposition 4.1 Let G be an ERDF graph and let F be an ERDF formula such that
VF  skG (Var (G)) = . It holds: G |=ERDF F iff sk(G) |=ERDF F .
Below we define the vocabulary of an ERDF ontology O.
Definition 4.5 (Vocabulary of an ERDF ontology) Let O = hG, P i be an ERDF ontology. The vocabulary of O is defined as VO = Vsk(G)  VP  VRDF  VRDF S  VERDF .

Note that the vocabulary of an ontology O = hG, P i contains the skolemization vocabulary of G.
Let O = hG, P i be an ERDF ontology. We denote by ResH
O the union of VO and the set
of XML values of the well-typed XML literals in VO minus the well-typed XML literals.
The following definition defines the Herbrand interpretations and the Herbrand models
of an ERDF ontology.
Definition 4.6 (Herbrand interpretation, Herbrand model of an ERDF ontology)
Let O = hG, P i be an ERDF ontology and let I be an ERDF interpretation of VO . We say
that I is a Herbrand interpretation of O iff:
 ResI = ResH
O.
 IV (x) = x, for all x  VO  URI.
 ILI (x) = x, if x is a typed literal in VO other than a well-typed XML literal, and
ILI (x) is the XML value of x, if x is a well-typed XML literal in VO .
51

fiAnalyti, Antoniou, Damasio, & Wagner

We denote the set of Herbrand interpretations of O by I H (O).
A Herbrand interpretation I of O is a Herbrand model of O iff I |= hsk(G), P i. We denote
the set of Herbrand models of O by MH (O). 
Note that if I is a Herbrand interpretation of an ERDF ontology O then I(x) = x, for
each x  VO other than a well-typed XML literal.
It is easy to see that every Herbrand model of an ERDF ontology O is a model of
O. Moreover, note that every Herbrand interpretation of an ERDF ontology O is uniquely
identified by (i) its set of properties and (ii) its property-truth and property-falsity extension
mappings.
However, not all Herbrand models of an ERDF ontology O are desirable. For example,
let p, s, o  URI, let G = {p(s, o)}, and let O = hG, i. Then, there is a Herbrand model
I of O such that I |= p(o, s), whereas we want p(o, s) to be satisfied by all intended
models of O. This is because p is not a total property and p(o, s) cannot be derived from
O (negation-as-failure)13 .
Before we define the minimal Herbrand interpretations of an ERDF ontology O, we need
to define a partial ordering on the Herbrand interpretations of O.
Definition 4.7 (Herbrand interpretation ordering) Let O = hG, P i be an ERDF ontology. Let I, J  I H (O). We say that J extends I, denoted by I  J (or J  I), iff
P ropI  P ropJ , and for all p  P ropI , it holds PT I (p)  PT J (p) and PF I (p)  PF J (p).

It is easy to verify that the relation  is reflexive, transitive, and antisymmetric. Thus,
it is a partial ordering on I H (O).
The intuition behind Definition 4.7 is that by extending a Herbrand interpretation, we
extend both the truth and falsity extension for all properties, and thus (since rdf :type is a
property), for all classes.
The following proposition expresses that two Herbrand interpretations I, J of an ERDF
ontology O are incomparable, if the property-truth or property-falsity extension of a total
property p w.r.t. I and J are different.
Proposition 4.2 Let O = hG, P i be an ERDF ontology and let I, J  I H (O). Let p 
TProp I  TProp J . If PT I (p) 6= PT J (p) or PF I (p) 6= PF J (p) then I 6 J and J 6 I.
Definition 4.8 (Minimal Herbrand interpretations) Let O be an ERDF ontology and
let I  I H (O). We define minimal(I) = {I  I | 6 J  I : J =
6 I and J  I}. 
We define the minimal Herbrand models of O, as:
Mmin (O) = minimal(MH (O)).
However minimal Herbrand models do not give the intended semantics to all ERDF rules.
This is because ERDF rules are derivation and not implication rules. Derivation rules are
13. On the other hand, if p is a total property then p(o, s)p(o, s) should be satisfied by all intended models.
Therefore, in this case, there should be an intended model of O that satisfies p(o, s).

52

fiExtended RDF as a Semantic Foundation of Rule Markup Languages

often identified with implications. But, in general, these are two different concepts. While
an implication is an expression of a logical formula language, a derivation rule is rather a
meta-logical expression. There are logics, which do not have an implication connective, but
which have a derivation rule concept. In standard logics (such as classical and intuitionistic
logic), there is a close relationship between a derivation rule (also called sequent) and the
corresponding implicational formula: they both have the same models. For non-monotonic
rules (e.g. with negation-as-failure), this is no longer the case: the intended models of such
a rule are, in general, not the same as the intended models of the corresponding implication.
This is easy to see with the help of an example. Consider the rule p  q whose model set,
according to the stable model semantics (Gelfond & Lifschitz, 1988, 1990; Herre & Wagner,
1997; Herre et al., 1999), is {{p}}, that is, it entails p. On the other hand, the model set
of the corresponding implication q  p, which is equivalent to the disjunction p  q, is
{{p}, {q}, {p, q}}; consequently, it does not entail p.
Similarly, let O = h, P i, where P = {p(s, o)  q(s, o)} and p, q, s, o  URI. Not
all minimal Herbrand models of O are intended. In particular, there is I  Mmin (O)
such that I |= q(s, o)  p(s, o), whereas we want q(s, o)  p(s, o) to be satisfied by all
intended models of O, as q is not a total property and q(s, o) cannot be derived by any rule
(negation-as-failure).
To define the intended (stable) models of an ERDF ontology, we need first to define
grounding of ERDF rules.
Definition 4.9 (Grounding of an ERDF program) Let V be a vocabulary and let r
be an ERDF rule. We denote by [r]V the set of rules that result from r if we replace each
variable x  FVar (r) by v(x), for all mappings v : FVar (r)  V .
Let P be an ERDF program. We define [P ]V =

S

rP [r]V .



Note that a rule variable can naturally appear in the subject position of an ERDF triple.
Since variables can be instantiated by a literal, a literal can naturally appear in the subject
position of an ERDF triple in the grounded version of an ERDF program. This case further
supports our choice of allowing literals in the subject position of an ERDF triple.

5. ERDF Stable Models
In this section, we define the intended models of an ERDF ontology O, called stable models
of O, based on minimal Herbrand interpretations. In particular, defining the stable models
of O, only the minimal interpretations from a set of Herbrand interpretations that satisfy
certain criteria are considered.
Below, we define the stable models of an ERDF ontology, based on the coherent stable
models14 of Partial Logic (Herre et al., 1999).
Definition 5.1 (ERDF stable model) Let O = hG, P i be an ERDF ontology and let
M  I H (O). We say that M is an (ERDF) stable model of O iff there is a chain of
Herbrand interpretations of O, I0  ...  Ik+1 such that Ik = Ik+1 = M and:
14. Note that these models on extended logic programs are equivalent (Herre et al., 1999) to Answer Sets of
answer set semantics (Gelfond & Lifschitz, 1990).

53

fiAnalyti, Antoniou, Damasio, & Wagner

1. I0  minimal({I  I H (O) | I |= sk(G)}).
2. For successor ordinals  with 0 <   k + 1:
I  minimal({I  I H (O) | I  I1 and I |= Concl(r), r  P[I1 ,M ] }), where
P[I1 ,M ] = {r  [P ]VO | I |= Cond(r), I  I H (O) s.t. I1  I  M }.
The set of stable models of O is denoted by Mst (O). 
Note that I0 is a minimal Herbrand interpretation of O = hG, P i that satisfies sk(G),
while Herbrand interpretations I1 , ..., Ik+1 correspond to a stratified sequence of rule applications, where all applied rules remain applicable throughout the generation of a stable
model M . In our words, a stable model is generated bottom-up by the iterative application
of the rules in the ERDF program P , starting from the information in the ERDF graph G.
Thus, ERDF stable model semantics, as a refinement of minimal model semantics, captures
the intuition that:
 Assertions rdf :type(p, erdf :TotalProperty) and rdf :type(c, erdf :TotalClass) should only
be accepted if the ontology contains some direct support for them in the form of an
acceptable rule sequence (that corresponds to a proof).
 Assertions p(s, o) and p(s, o) should only be accepted if the ontology contains some
direct support for them in the form of an acceptable rule sequence, or
rdf :type(p, erdf :TotalProperty) is accepted.
 Assertions rdf :type(o, c) and rdf :type(o, c) should only be accepted if the ontology
contains some direct support for them in the form of an acceptable rule sequence, or
rdf :type(c, erdf :TotalClass) is accepted.
Wine Selection Example: Consider a class Wine whose instances are wines, and a
property likes(X, Y ) indicating that person X likes object Y . Assume now that we want
to select wines for a dinner such that, for each guest, there is on the table exactly one wine
that he/she likes. Let the class Guest indicate the persons that will be invited to the dinner
and let the class SelectedWine indicate the wines chosen to be served. An ERDF program
P that describes this wine selection problem is the following (commas , in the body of
the rules indicate conjunction ):
id(?x, ?x)  rdf :type(?x, rdfs:Resource).
rdf :type(?y, SelectedWine)  rdf :type(?x, Guest), rdf :type(?y, Wine), likes(?x, ?y),
?z (rdf :type(?z, SelectedWine), id(?z, ?y)  likes(?x, ?z)).

Consider now the ERDF graph G, containing the factual information:
G = { rdf :type(Carlos, Guest), rdf :type(Gerd , Guest), rdf :type(Riesling, Wine),
rdf :type(Retsina, Wine), rdf :type(Chardonnay, Wine), likes(Gerd , Riesling),
likes(Gerd , Retsina), likes(Carlos, Chardonnay), likes(Carlos, Retsina) }.

Then, according to Definition 5.1, the ERDF ontology O = hG, P i has two stable models,
M1 and M2 , such that:
54

fiExtended RDF as a Semantic Foundation of Rule Markup Languages

M1 |= rdf :type(Riesling, SelectedWine)  rdf :type(Chardonnay, SelectedWine) 
 rdf :type(Retsina, SelectedWine).
M2 |= rdf :type(Retsina, SelectedWine)   rdf :type(Riesling, SelectedWine) 
 rdf :type(Chardonnay, SelectedWine).

Note that, according to stable model M1 , the wines selected for the dinner are Riesling
and Chardonnay. This is because, (i) Gerd likes Riesling but does not like Chardonnay,
and (ii) Carlos likes Chardonnay but does not like Riesling.
According to stable model M2 , only Retsina is selected for the dinner. This is because,
both Gerd and Carlos like Retsina.
Stable model M1 is reached through the chain I0  M1  M1 , where I0 is the single
Herbrand interpretation in minimal({I  I H (O) | I |= sk(G)}). To verify this, note that:
P[I0 ,M1 ] = P[M1 ,M1 ] =
[id(?x, ?x)  rdf :type(?x, rdfs:Resource)]VO 
{rdf :type(Riesling, SelectedWine)  rdf :type(Gerd , Guest),
rdf :type(Riesling, Wine), likes(Gerd , Riesling),
?z (rdf :type(?z, SelectedWine), id(?z , Riesling)  likes(Gerd , ?z))} 
{rdf :type(Chardonnay, SelectedWine)  rdf :type(Carlos, Guest),
rdf :type(Chardonnay, Wine), likes(Carlos, Chardonnay),
?z (rdf :type(?z, SelectedWine), id(?z , Chardonnay)  likes(Carlos, ?z))}.

Similarly, stable model M2 is reached through the chain I0  M2  M2 . To verify this,
note that:
P[I0 ,M2 ] = P[M2 ,M2 ] =
[id(?x, ?x)  rdf :type(?x, rdfs:Resource)]VO 
{rdf :type(Retsina, SelectedWine)  rdf :type(Gerd , Guest),
rdf :type(Retsina, Wine), likes(Gerd , Retsina),
?z (rdf :type(?z, SelectedWine), id(?z, Retsina)  likes(Gerd , ?z))} 
{rdf :type(Retsina, SelectedWine)  rdf :type(Carlos, Guest),
rdf :type(Retsina, Wine), likes(Carlos, Retsina),
?z (rdf :type(?z, SelectedWine), id(?z, Retsina)  likes(Carlos, ?z))}.

Assume now that Retsina should not be one of the selected wines, because it does not
match with the food. To indicate this, we add to P the ERDF constraint:
false  rdf :type(Retsina, SelectedWine).
Then, M1 is the single model of the modified ontology.
It is easy to verify that if O is an ERDF ontology and O is exactly as O, but without
the ERDF constraints appearing in O, then Mst (O)  Mst (O ). In other words, the ERDF
constraints appearing in an ERDF ontology eliminate undesirable stable models.
Paper Assignment Example: Consider a class Paper whose instances are papers submitted to a conference, a class Reviewer whose instances are potential reviewers for the
55

fiAnalyti, Antoniou, Damasio, & Wagner

submitted papers, and a property conflict(R, P ) indicating that there is a conflict of interest between reviewer R and paper P . Assume now that we want to assign papers to
reviewers based on the following criteria: (i) a paper is assigned to at most one reviewer,
(ii) a reviewer is assigned at most one paper, and (iii) a paper is not assigned to a reviewer,
if there is a conflict of interest. The assignment of a paper P to a reviewer R is indicated
through the property assign(P, R). The ERDF triple allAssigned (Paper , Reviewer ) indicates that each paper has been assigned to one reviewer. An ERDF program P describing
the assignment of papers is the following:
id(?x, ?x)  true.
assign(?p, ?r)  rdf :type(?p, Paper ), rdf :type(?p , Paper ), assign(?p , ?r), id(?p, ?p ).
assign(?p, ?r)  rdf :type(?r, Reviewer ), rdf :type(?r , Reviewer ), assign(?p, ?r ), id(?r, ?r ).
assign(?p, ?r)  conflict(?r, ?p).
assign(?p, ?r)
 rdf :type(?r, Reviewer ), rdf :type(?p, Paper ),  assign(?p, ?r).
allAssigned (Paper , Reviewer )  ?p (rdf :type(?p, Paper ) 
?r (rdf :type(?r, Reviewer )  assign(?p, ?r))).

Consider now the ERDF graph G, containing the factual information:
G = { rdf :type(P 1, Paper ), rdf :type(P 2, Paper ), rdf :type(P 3, Paper ), rdf :type(R1, Reviewer ),
rdf :type(R2, Reviewer ), rdf :type(R3, Reviewer ), conflict(P 1, R3), conflict(P 2, R2),
conflict(P 3, R2) }.

Then, according to Definition 5.1, the ERDF ontology O = hG, P i has four stable
models, denoted by M1 , ..., M4 , such that:
M1
M2
M3
M4

|=
|=
|=
|=

assign(P 1, R1)  assign(P 2, R3)  allAssigned (Paper , Reviewer ),
assign(P 1, R1)  assign(P 3, R3)  allAssigned (Paper , Reviewer ),
assign(P 1, R2)  assign(P 2, R1)  assign(P 3, R3)  allAssigned (Paper , Reviewer ),
assign(P 1, R2)  assign(P 2, R3)  assign(P 3, R1)  allAssigned (Paper , Reviewer ).

We would like to note that, in contrast to the previous examples, given an ERDF
ontology O = hG, P i, it is possible that |minimal ({I  I H (O) | I |= sk(G)})| > 1, due
to the declaration of total properties and total classes. Specifically, the number of the
interpretations I0 in item 1 of Definition 5.1 is more than one iff G contains ERDF triples
of the form rdf :type(p, erdf :TotalProperty) or rdf :type(c, erdf :TotalClass). For example, let
O = hG, i, where:
G = {authorOf (John, book1 ), authorOf (Peter , book2 ), rdf :type(authorOf , erdf :TotalProperty)}.

Then, there are I0 , I0  minimal ({I  I H (O) | I |= sk(G)}) such that:
I0 |= authorOf (John, book2 ) and I0 |= authorOf (John, book2 ).
Note that both I0 and I0 are stable models of O. However, I0 satisfies authorOf (John, book2 ),
even though there is no evidence that John is an author of book2 .
The following proposition shows that a stable model of an ERDF ontology O is a Herbrand model of O.
56

fiExtended RDF as a Semantic Foundation of Rule Markup Languages

Proposition 5.1 Let O = hG, P i be an ERDF ontology and let M  Mst (O). It holds
M  MH (O).
On the other hand, if all properties are total, a Herbrand model M of an ERDF ontology
O = hG, P i is a stable model of O15 . Obviously, this is a desirable result since, in this
case, an open-world assumption is made for all properties. Thus, there is no preferential
entailment of weak negation, for any of the properties. Of course, the term stable model
is not very descriptive, for this degenerative case.
Proposition 5.2 Let O = hG, P i be an ERDF ontology such that
rdfs:subClassOf (rdf :Property, erdf :TotalProperty)  G. Then, Mst (O) = MH (O).
A final note is that, similarly to stable models defined by Gelfond & Lifschitz (1988, 1990)
and Herre et al. (1999), ERDF stable models do not preserve Herbrand model satisfiability.
For example, let O = h, P i, where P = {p(s, o)  p(s, o)} and p, s, o  URI. Then,
Mst (O) = , whereas there is a Herbrand model of O that satisfies p(s, o).

6. ERDF Stable Model Entailment & Stable Answers
In this section, we define stable model entailment on ERDF ontologies, showing that it extends ERDF entailment on ERDF graphs. Moreover, we define the skeptical and credulous
answers of an ERDF formula (query) F w.r.t. an ERDF ontology O.
Definition 6.1 (Stable model entailment) Let O = hG, P i be an ERDF ontology and
let F be an ERDF formula or ERDF graph. We say that O entails F under the (ERDF)
stable model semantics, denoted by O |=st F iff for all M  Mst (O), M |= F . 
For example, let O = h, P i, where P = {p(s, o)  q(s, o)} and p, q, s, o  URI. Then,
O |=st q(s, o)  p(s, o).
Now, let G = {rdfs:subClassOf (rdf :Property, erdf :TotalProperty)} and let P be as in
the previous example. Then, hG, P i |=st q(s, o)  p(s, o), but hG, P i 6|=st q(s, o) and
hG, P i 6|=st p(s, o). Note that this is the desirable result, since now q is a total property
(and thus, an open-world assumption is made for q).
As another example, let p, s, o  URI, let G = {p(s, o)}, and let P = {p(?x, ?y) 
p(?x, ?y)}. Then, hG, P i |=st p(o, s)  p(o, s) (note that P contains a CWA on p).
Now, let G = {rdf :type(p, erdf :TotalProperty), p(s, o)} and let P be as in the previous
example. Then, hG, P i |=st ?x ?y (p(?x, ?y) p(?x, ?y)) (see Proposition 3.1), but
hG, P i 6|=st p(o, s) and hG, P i 6|=st p(o, s). Indeed, the CWA in P does not affect the
semantics of p, since p is a total property.
EU Membership Example: Consider the following ERDF program P , specifying some
rules for concluding that a country is not a member state of the European Union (EU).
(r1 )
(r2 )

rdf :type(?x, EUMember) 
rdf :type(?x, EUMember) 

rdf :type(?x, AmericanCountry).
rdf :type(?x, EuropeanCountry),
rdf :type(?x, EUMember).

15. Note that, in this case, M  minimal({I  I H (O) | I |= sk(G)}) and M  minimal({I  I H (O) | I 
M and I |= Concl(r),  r  P[M,M ] }).

57

fiAnalyti, Antoniou, Damasio, & Wagner

A rather incomplete ERDF ontology O = hG, P i is obtained by including the following
information in the ERDF graph G:
rdf :type(Russia, EUMember).
rdf :type(Austria, EUMember).
rdf :type(?x, EuropeanCountry).

rdf :type(Canada, AmericanCountry).
rdf :type(Italy, EuropeanCountry).
rdf :type(?x, EUMember).

Using stable model entailment on O, it can be concluded that Austria is a member of EU,
that Russia and Canada are not members of EU, and that it exists a European Country
which is not a member of EU. However, it is also concluded that Italy is not a member of EU, which is a wrong statement. This is because G does not contain complete
information of the European countries that are EU members (e.g., it does not contain
rdf :type(Italy, EUMember)). Thus, incorrect information is obtained by the closed-world
assumption expressed in rule r2 . In the case that rdf :type(EUMember, erdf :TotalClass)
is added to G (that is, an open-world assumption is made for the class EUMember) then
rdf :type(Italy, EUMember) and thus, rdf :type(Italy, EUMember) are no longer entailed.
This is because, there is a stable model of the extended ERDF ontology O that satisfies
rdf :type(Italy, EUMember). Moreover, if complete information for all European countries
that are members of EU is included in G then the stable model conclusions of O will also
be correct (the closed-world assumption will be correctly applied). Note that, in this case,
G will include the ERDF triple rdf :type(Italy, EUMember).
The following proposition follows directly from the fact that any stable model of an
ERDF ontology O is an ERDF interpretation.
Proposition 6.1 Let O = hG, P i be an ERDF ontology and let F, F  be ERDF formulas.
If O |=st F and F |=ERDF F  then O |=st F  .
For ERDF graphs G, G , it can be proved that hG, i |=st G iff G |=ERDF G (see
below). Now the question arises whether this result can be generalized by replacing the
ERDF graph G by any ERDF formula F . The following example shows that this is not
the case. Let G = {p(s, o)} and let F = p(o, s), where p, s, o  URI. Then hG, i |=st F ,
whereas G 6|=ERDF F . However, G can be replaced by any ERDF d-formula F , defined as
follows:
Definition 6.2 (ERDF d-formula) Let F be an ERDF formula. We say that F is an
ERDF d-formula iff (i) F is the disjunction of existentially quantified conjunctions of ERDF
triples, and (ii) FVar (F ) = . 
For example, let:
F = (?x rdf :type(?x , Vertex )  rdf :type(?x , Red )) 
(?x rdf :type(?x , Vertex )  rdf :type(?x , Blue)).
Then, F is an ERDF d-formula. It is easy to see that if G is an ERDF graph then formula(G)
is an ERDF d-formula.
58

fiExtended RDF as a Semantic Foundation of Rule Markup Languages

Proposition 6.2 Let G be an ERDF graph and let F be an ERDF formula such that
VF  skG (Var (G)) = . It holds:
1. If F is an ERDF d-formula and hG, i |=st F then G |=ERDF F .
2. If G |=ERDF F then hG, i |=st F .
Let G be an ERDF graph and let F be an ERDF d-formula or an ERDF graph such
that VF  skG (Var (G)) = . A direct consequence of Proposition 6.2 is that:
hG, i |=st F iff G |=ERDF F .
The following proposition is a direct consequence of Proposition 3.2 and Proposition
6.2, and shows that stable model entailment extends RDFS entailment from RDF graphs
to ERDF ontologies.
Proposition 6.3 Let G, G be RDF graphs such that VG  VERDF = , VG  VERDF = ,
and VG  skG (Var (G)) = . It holds: G |=RDF S G iff hG, i |=st G .
Recall that the Skolem vocabulary of G (that is, skG (Var (G))) contains artificial URIs
giving arbitrary names to the anonymous entities whose existence was asserted by the
use of blank nodes in G. Thus, the condition VG  skG (Var (G)) =  in Proposition 6.3 is
actually trivial.
Definition 6.3 (ERDF query, ERDF stable answers) Let O = hG, P i be an ERDF
ontology. An (ERDF) query F is an ERDF formula. The (ERDF) stable answers of F
w.r.t. O are defined as follows:

if FVar (F ) =  and M  Mst (O) : M |= F
 yes
st
no
if FVar (F ) =  and M  Mst (O) : M 6|= F
Ans O (F ) =

{v : FVar (F )  VO | M  Mst (O), M |= v(F )}

if FVar (F ) 6= ,

where v(F ) is the formula F after replacing all the free variables x in F by v(x). 
For example, let p, q, c, s, o  URI, let G = {p(s, o), rdf :type(s, c), rdf :type(o, c)}, and
let P = {q(?x, ?y)  rdf :type(?x, c)  rdf :type(?y, c)  p(?x, ?y)}. Then, the stable
answers of F = q(?x, ?y) w.r.t. O = hG, P i are Ans st
O (F ) = {{?x = o, ?y = o}, {?x =
s, ?y = s}, {?x = o, ?y = s}}.
Let O = hG, P i, where q, s, o  URI, G = {rdf :type(p, erdf :TotalProperty), q(s, o)},
st
and P = {p(?x, ?y)  p(?x, ?y)}. Then, it holds Ans st
O (p(?x, ?y))= Ans O (p(?x, ?y)) =
st
Ans O (p(?x, ?y)) = . This is because, in contrast to the above example, p is a total
property. Thus, for all mappings v : {?x, ?y}  VO , there is a stable model M of O
such that M |= v(p(?x, ?y)  p(?x, ?y)), and another stable model M  of O such that
M  |= v(p(?x, ?y)  p(?x, ?y)).
Consider the ERDF ontology O of the paper assignment example, below Definition
st
5.1. Then, Ans st
O (assign(P 1, R2)) =yes and Ans O (assign(P 2, R1)) =no. Though
st
Ans O (assign(P 2, R1)) =no, that is assign(P 2, R1) is not satisfied by all stable models of O, there is a stable model (M3 ) that satisfies assign(P 2, R1). Indeed the answers of
the query assign(?x, ?y) w.r.t. the stable models M3 and M4 are of particular interest since
59

fiAnalyti, Antoniou, Damasio, & Wagner

both M3 and M4 satisfy allAssigned (Paper , Reviewer ), indicating that the desirable paper
assignment has been achieved.
The following definition defines the credulous stable answers of a query F w.r.t. an
ERDF ontology O, that is the answers of F w.r.t. the particular stable models of O.
Definition 6.4 (Credulous ERDF stable answers) Let O = hG, P i be an ERDF ontology. The credulous (ERDF) stable answers of a query F w.r.t. O are defined as follows:

 yes if FVar (F ) =  and M  Mst (O) : M |= F
st
no if FVar (F ) =  and M  Mst (O) : M 6|= F
c-Ans O (F ) =

{ans M (F ) 6=  | M  Mst (O)}
if FVar (F ) 6= ,

where ans M (F ) = {v : FVar (F )  VO | M |= v(F )}. 

Continuing with the paper assignment example, consider the query:
F = allAssigned (Paper , Reviewer ).
st
Then, although Ans st
O (F ) =no, it holds c-Ans O (F ) =yes, indicating that there is at
least one desirable assignment of the papers P 1, P 2, P 3 to reviewers R1, R2, R3.
Consider now the query F = allAssigned (Paper , Reviewer )  assign(?x , ?y). Then,

c-Ans st
O (F ) = {{{?x = P 1, ?y = R2}, {?x = P 2, ?y = R1}, {?x = P 3, ?y = R3}},
{{?x = P 1, ?y = R2}, {?x = P 2, ?y = R3}, {?x = P 3, ?y = R1}}},

indicating all possible desirable assignments of papers. Obviously, the credulous stable
answers of a query F can provide alternative solutions, which can be useful in a range of
applications, where alternative scenarios naturally appear.
Closing this section, we would like to indicate several differences of the ERDF stable
model semantics w.r.t. first-order logic (FOL). First, in our semantics a domain closure assumption is made. This is due to the fact that the domain of every Herbrand interpretation
of an ERDF ontology O is ResH
O , that is the union of the vocabulary of O (VO ) and the
set of XML values of the well-typed XML literals in VO minus the well-typed XML literals.
This implies that quantified variables always range in a closed domain. To understand the
implications of this assumption, consider the ERDF graph:
G = {rdf :type(x, c1) | x  {c1, c2}  V  },

where V  = (VRDF  {rdf : i | i  IN })  VRDF S  VERDF . Additionally, consider the ERDF
program:
P = { rdf :type(?x, c1)  rdf :type(?x, rdfs:ContainerMembershipProperty).
rdf :type(?x, c2)  true.}.

Let F = ?x rdf :type(?x, c2)  rdf :type(?x, c1). It holds that hG, P i |=st F . However,
G  P 6|=F OL F . This is because, there is a FOL model M of G  P with a domain D
and a variable assignment v:{?x}  D such that M, v |= rdf :type(?x, c2) and M, v 6|=
rdf :type(?x, c1).
Another difference is due to the fact that in the definition of the ERDF stable model
semantics, only minimal Herbrand interpretations are considered. Let
60

fiExtended RDF as a Semantic Foundation of Rule Markup Languages

G = {teaches(Anne, CS301 ), teaches(Peter , CS505 ), rdf :type(CS505 , GradCourse)}.

Let F = ?x teaches(Peter , ?x)  rdf :type(?x, GradCourse). Then, hG, i |=st F .
However, G 6|=F OL F . This is because, there is a FOL model M of G with a domain D
and a variable assignment v:{?x}  D such that M, v |= teaches(Peter , ?x) and M, v 6|=
rdf :type(?x, GradCourse). In other words, FOL makes an open-world assumption for
teaches.
Consider now G = G  {rdf :type(teaches, erdf :TotalProperty)}. Then, similarly to
FOL, it holds O = hG , i 6|=st F . This is because now teaches is a total property. Thus,
there is a stable model M of O and a variable assignment v: {?x}  ResH
O such that
M, v |= teaches(Peter , ?x) and M, v 6|= rdf :type(?x, GradCourse). In other worlds, now an
open-world assumption is made for teaches, as in FOL. Thus, there might exist a course
taught by Peter , even if it is not explicitly indicated so in G .
This example also shows that, in contrast to FOL, stable model entailment is nonmonotonic.
Note that the previous ERDF graph G can also be seen as a Description Logic A-Box
A (Baader et al., 2003), where
A = {teaches(Anne, CS301), teaches(Peter , CS505), GradCourse(CS505)}
Consider a T-Box T = . Since Description Logics (DLs) are fragments of first-order
logic, it holds that L = hA, T i 6|=DL teaches.GradCourse(Peter ), meaning that L does not
satisfy that all courses taught by Peter are graduate courses. An interesting approach for
supporting non-monotonic conclusions in DLs is taken by Donini et al. (2002), where DLs of
minimal knowledge and negation as failure (MKNF-DLs) are defined, by extending DLs with
two modal operators K, A. Intuitively, K expresses minimal knowledge and A expresses
weak negation. It holds that L |=MKNF-DL Kteaches.KGradCourse(Peter ), expressing that all
courses known to be taught by Peter are known to be graduate courses. Note that this
conclusion is non-monotonic, and thus it cannot be derived by classical DLs. However,
compared to our theory, MKNF-DLs do not support rules and closed-world assumptions on
properties (i.e., p(?x, ?y)  p(?x, ?y)).

7. An XML-based Syntax for ERDF
A natural approach to define an XML syntax for ERDF is: (i) to follow the RDF/XML
syntax (Beckett, 2004), as much as possible, and (ii) to extend it in a suitable way, where
necessary. Following this approach, we briefly present here an XML syntax for ERDF.
Details are going to be given in a subsequent paper.
Classes and properties are defined with the help of the rdfs:Class and rdf:Property
elements of the RDF/XML syntax. Similarly, total classes and total properties are defined with the help of the erdf:TotalClass and erdf:TotalProperty elements of the
ERDF/XML syntax.
Example 7.1 The following ERDF/XML statements:
<rdf:Property rdf:about="#likes">
<rdfs:domain rdf:resource="#Person"/>
61

fiAnalyti, Antoniou, Damasio, & Wagner

</rdf:Property>
<erdf:TotalProperty rdf:about="#authorOf">
<rdfs:domain rdf:resource="#Person"/>
<rdfs:range rdf:resource="#Book"/>
</erdf:TotalProperty>

correspond to the ERDF graph:
G = { rdf :type(likes, rdf :Property), rdfs:domain(likes, Person),
rdf :type(authorOf , erdf :TotalProperty), rdfs:domain(authorOf , Person),
rdfs:range(authorOf , Book )}.

ERDF triples (and sets of ERDF triples sharing the same subject term) are encoded
by means of the erdf:Description element. Each description contains a non-empty list of
(possibly negated) property-value slots about the subject term.
 URI references, blank node identifiers, and variables that appear in the subject position
of an ERDF triple are expressed as values of the erdf:about attribute, using the
SPARQL syntax (Prudhommeaux & Seaborne, 2008) for blank node identifiers and
variables. On the other hand, literals that appear in the subject position of an ERDF
triple are expressed as the text content of the erdf:about subelement.
 URI references, blank node identifiers, and variables that appear in the object position of an ERDF triple are expressed as values of the attributes rdf:resource,
rdf:nodeID, and erdf:variable, respectively. On the other hand, literals that appear in the object position of an ERDF triple are expressed as the text content of the
corresponding property subelement.
Example 7.2 The following erdf:Description statements:
<erdf:Description erdf:about="#Gerd">
<ex:authorOf rdf:nodeID="x"/>
<ex:likes rdf:resource="#Chicken"/>
<ex:likes erdf:negationMode="Sneg" rdf:resource="#Pork"/>
</erdf:Description>
<erdf:Description>
<erdf:About rdf:datatype="&xsd;string">Grigoris</erdf:About>
<ex:denotationOf rdf:resource="#Grigoris"/>
</erdf:Description>

correspond to the ERDF graph:
G = { authorOf (Gerd , ?x ), likes(Gerd , Chicken), likes(Gerd , Pork ),
denotationOf (Grigorisxsd :string, Grigoris) }.

Now, in order to express ERDF rules with XML, we use the rule markup language R2ML
(REWERSE Rule Markup Language) (Wagner, Giurca, & Lukichev, 2006, 2005), which
is a general XML-based markup language for representing derivation rules and integrity
constraints. This is demonstrated in the following example:
62

fiExtended RDF as a Semantic Foundation of Rule Markup Languages

Example 7.3 The following erdf:DerivationRule statement:
<r2ml:DerivationRule r2ml:ruleID="R1">
<r2ml:conditions>
<erdf:Description erdf:about="?x">
<rdf:type rdf:resource="#MainDish"/>
</erdf:Description>
<erdf:Description erdf:about="?y">
<rdf:type rdf:resource="#Guest"/>
<ex:likes erdf:variable="x"/>
</erdf:Description>
<r2ml:NegationAsFailure>
<r2ml:ExistentiallyQuantifiedFormula>
<r2ml:GenericVariable r2ml:name="z" r2ml:class="#Guest"/>
<erdf:Description erdf:about="?z">
<ex:likes erdf:negationMode="Sneg" erdf:variable="x"/>
</erdf:Description>
</r2ml:ExistentiallyQuantifiedFormula>
</r2ml:NegationAsFailure>
</r2ml:conditions>
<r2ml:conclusion>
<erdf:Description erdf:about="?x">
<rdf:type rdf:resource="#SelectedMainDish"/>
</erdf:Description>
</r2ml:conclusion>
</r2ml:DerivationRule>

expresses that a main dish is selected for dinner, if there is a guest who likes it and no guest
who dislikes it. Specifically, it corresponds to the ERDF rule:
rdf :type(?x, SelectedMainDish)  rdf :type(?x, MainDish), rdf :type(?y, Guest), likes(?y, ?x),
 (?z rdf :type(?z, Guest), likes(?z, ?x)).

8. Undecidability of the ERDF Stable Model Semantics
The main difficulty in the computation of the ERDF stable model semantics is the fact that
VRDF is infinite, and thus the vocabulary of any ERDF ontology O is also infinite (note
that {rdf : i | i  IN }  VRDF  VO ). Due to this fact, satisfiability and entailment under
the ERDF stable model semantics are in general undecidable.
The proof of undecidability exploits a reduction from the unbounded tiling problem. The
unbounded tiling problem consists in placing tiles on an infinite grid, satisfying a given set
of constraints on adjacent tiles. Specifically, the unbounded tiling problem is a structure
D = hT , H, V i, where T = {T1 , ..., Tn } is a finite set of tile types and H, V  T  T specify
which tiles can be adjacent horizontally and vertically, respectively. A solution to D is a
tiling, that is, a total function  : IN  IN  T such that: ( (i, j),  (i + 1, j))  H and
( (i, j),  (i, j + 1))  V , for all i, j  IN . The existence of a solution for a given unbounded
tiling problem is known to be undecidable (Berger, 1966).
Let D = hT , H, V i be an instance of the unbounded tiling problem, where T = {T1 , ..., Tn }.
We will construct an ERDF ontology OD = hG, P i and an ERDF formula FD such that D
has a solution iff OD does not entail FD under the ERDF stable model semantics.
63

fiAnalyti, Antoniou, Damasio, & Wagner

Consider (i) a class Tile whose instances are the tiles placed on the infinite grid, (ii) a
property right(x , y) indicating that tile y is right next to tile x, (iii) a property above(x , y)
indicating that tile y is exactly above tile x, (iv) a class HasRight whose instances are the
tiles for which there exists a tile right next to them, (v) a class HasAbove whose instances
are the tiles for which there exists a tile exactly above them, (vi) a property Type(x, T ),
indicating that the type of tile x is T , (vii) a property HConstraint(T, T  ), indicating that
(T, T  )  H, and (viii) a property VConstraint(T, T  ), indicating that (T, T  )  V .
Let G be the ERDF graph:
G=

{rdfs:subClassOf (rdfs:ContainerMembershipProperty, Tile),
rdfs:subClassOf (Tile, rdfs:ContainerMembershipProperty)} 
{HConstraint(T, T  ) | (T, T  )  H}  {VConstraint(T, T  ) | (T, T  )  V } .

Let P be the ERDF program, containing the following rules (and constraints):
(1)

Type(?x, T1 )  rdf :type(?x, Tile), Type(?x, T2 ), ..., Type(?x, Tn ).
Type(?x, Ti )  rdf :type(?x, Tile), Type(?x, T1 ), ..., Type(?x, Ti1 ),
Type(?x, Ti+1 ), ..., Type(?x, Tn ), for all i = 2, ..., n  1.
Type(?x, Tn )  rdf :type(?x, Tile), Type(?x, T1 ), ..., Type(?x, Tn1 ).

(2)

right(?x, ?y)
right(?x, ?y)




rdf :type(?x, Tile), rdf :type(?y, Tile), right(?x, ?y).
rdf :type(?x, Tile), rdf :type(?y, Tile), right(?x, ?y).

(3)

above(?x, ?y)
above(?x, ?y)




rdf :type(?x, Tile), rdf :type(?y, Tile), above(?x, ?y).
rdf :type(?x, Tile), rdf :type(?y, Tile), above(?x, ?y).

(4)

rdf :type(?x, HasRight)  right(?x, ?y).
rdf :type(?x, HasAbove)  above(?x, ?y).
false  rdf :type(?x, Tile), rdf :type(?x, HasRight).
false  rdf :type(?x, Tile), rdf :type(?x, HasAbove).
id (?x, ?x)  rdf :type(?x, rdfs:Resource).
false  right(?x, ?y), right(?x, ?y  ), id (?y, ?y  ).
false  above(?x, ?y), above(?x, ?y  ), id (?y, ?y  ).

(5)

false  right(?x, ?y), Type(?x, ?T ), Type(?y, ?T  ), HConstraint(?T, ?T  ).
false  above(?x, ?y), Type(?x, ?T ), Type(?y, ?T  ), VConstraint(?T, ?T  ).

Note that in all stable models of OD = hG, P i, the class Tile contains exactly the
(infinite in mumber) rdf : i terms, for i  IN . This is because, computing the stable models
of O, only the minimal models of sk(G) are considered (see Definition 5.1, Step 1). Thus,
each tile on the infinite grid is represented by an rdf : i term, for i  IN .
Intuitively, rule set (1) expresses that each tile should have exactly one associated type
in T . Rule set (2) expresses that two tiles are either horizontally adjacent on the grid or not
64

fiExtended RDF as a Semantic Foundation of Rule Markup Languages

horizontally adjacent. Rule set (3) expresses that two tiles are either vertically adjacent on
the grid or not vertically adjacent. Rule set (4) expresses that each tile should have exactly
one tile right next to it and exactly one tile right above it. Rule set (5) expresses that the
types of horizontally and vertically adjacent tiles should respect the H and V relations of
D, respectively.
To finalize the reduction, we define:
FD = ?x, ?y, ?x , ?y  , ?x right(?x, ?y)  above(?y, ?y  )  right(?x , ?y  )  above(?x , ?x ) 
id (?x, ?x ).

Formula FD expresses that there is a tile x such that, starting from x, if we move:
one step right  one step up  one step left  one step down

then we will meet a tile x different than x.
Proposition 8.1 Let D be an instance of the unbounded tiling problem. It holds:
1. D has a solution iff OD  {false  FD } has a stable model.
2. D has a solution iff OD 6|=st FD .
Since the unbounded tiling problem is undecidable (Berger, 1966), it follows directly from
Proposition 8.1 that satisfiability and entailment under the ERDF stable model semantics
are in general undecidable.
The previous reduction shows that both problems remain undecidable for an ERDF ontology O = hG, P i, even if (i) the body of each rule in P has the form t1 , ..., tk , tk+1 , ..., tn ,
where ti is an ERDF triple and (ii) the terms erdf :TotalClass and erdf :TotalProperty do
not appear in O, that is, (VG  VP )  VERDF = . Note that since each constraint false  F
that appears in an ERDF ontology O can be replaced by the rule t  F , where t is an
RDF, RDFS, or ERDF axiomatic triple, the presence of constraints in O does not affect
decidability.
Future work concerns the identification of syntactic restrictions for an ERDF ontology
O such that ERDF stable model entailment is decidable.

9. ERDF Model Theory as Tarski-style Model Theory
Tarski-style model theory is not limited to classical first-order models, as employed in the
semantics of OWL. It allows various extensions, such as relaxing the bivalence assumption
(e.g., allowing for partial models) or allowing higher-order models. It is also compatible
with the idea of non-monotonic inference, simply by not considering all models of a rule as
being intended, but only those models that satisfy certain criteria. Thus, the stable model
semantics for normal and (generalized) extended logic programs (Gelfond & Lifschitz, 1988,
1990; Herre & Wagner, 1997; Herre et al., 1999) can be viewed as a Tarski-style modeltheoretic semantics for non-monotonic derivation rules.
A Tarski-style model theory is a triple hL, I, |=i such that:
 L is a set of formulas, called language,
65

fiAnalyti, Antoniou, Damasio, & Wagner

 I is a set of interpretations, and
 |= is a relation between interpretations and formulas, called model relation.
For each Tarski-style model theory hL, I, |=i, we can define:
 a notion of derivation rule G  F , where F  L is called condition and G  L is
called conclusion,
 a set of derivation rules DRL = {G  F | F, G  L},
 an extension of the model relation |= to include also pairs of interpretations and
derivation rules, and
 a standard model operator M(KB ) = {I  I | I |= X, X  KB }, where KB 
L  DRL is a set of formulas and/or derivation rules, called a knowledge base.
Notice that in this way we can define rules also for logics which do not contain an
implication connective. This shows that the concept of a rule is independent of the concept
of implication.
Typically, in knowledge representation theories, not all models of a knowledge base are
intended models. Except from the standard model operator M, there are also non-standard
model operators, which do not provide all models of a knowledge base, but only a special
subset that is supposed to capture its intended models according to some semantics.
A particularly important type of such an intended model semantics is obtained on the
basis of some information ordering , which allows to compare the information content of
two interpretations I1 , I2  I. Whenever I1  I2 , we say that I1 is less informative than
I2 . An information model theory hL, I, |=, i is a Tarski-style model theory, extended by
an information ordering .
For any information model theory, we can define a number of natural non-standard
model operators, such as the minimal model operator:
Mmin (KB ) = minimal (M(KB ))
and various refinements of it, like the stable generated models (Gelfond & Lifschitz, 1988,
1990; Herre & Wagner, 1997; Herre et al., 1999).
For any given model operator Mx : P(L  DRL )  P(I), knowledge base KB 
L  DRL , and F  L, we can define an entailment relation:
KB |=x F

iff I  Mx (KB ), I |= F

For non-standard model operators, like minimal and stable models, this entailment
relation is typically non-monotonic, in the sense that for an extension KB   KB it may
be the case that KB entails F , but KB  does not entail F .
Our (ERDF) stable model theory can be seen as a Tarski-style model theory, where
L = L(URI  LIT ), I is the set of ERDF interpretations over any vocabulary V 
URI  LIT , and the model relation |= is as defined in Definitions 3.5 and 4.3. In our
theory, the intended model operator (Mst ) assigns to each ERDF ontology a (possibly
empty) set of stable models (Definition 5.1).
66

fiExtended RDF as a Semantic Foundation of Rule Markup Languages

10. Related Work
In this section, we briefly review extensions of web ontology languages with rules.
Ter Horst (2005b, 2004) generalizes RDF graphs to generalized RDF graphs, by allowing variables in the property position of RDF triples. Additionally, the author extends the
RDFS semantics with datatypes and part of the OWL vocabulary, defining the pD semantics, which extends the if-semantics of RDFS and is weaker than the iff-semantics of
D-entailment (Hayes, 2004) and OWL Full (Patel-Schneider, Hayes, & Horrocks, 2004). A
sound and complete set of entailment rules for pD entailment is also presented.
In a subsequent work, ter Horst (2005a) considers the extension of the previous framework with the inclusion of rules of the form if G then G , where G is an RDF graph without
blank nodes but possibly with variables and G is a generalized RDF graph, possibly with
both blank nodes and variables. Intuitively, rule variables are universally quantified in the
front of the rule (like the free variables of our rules) and blank nodes in the head of the rule
correspond to existentially quantified variables (this feature is not supported in our model).
Based on a set of rules R and a datatype map D, R-entailment16 is defined between two
generalized RDF graphs G and G (G |=R G ), and a set of sound and complete rules for
R-entailment is presented. To relate our work with that of ter Horst (2005a), we state the
following proposition:
Let D be a datatype map, containing only rdf :XMLLiteral , and let R be a set of rules of the form
if G then G  with the constraints: (i) all terms appearing in property position are URIs, (ii) if
G 6= {} then no blank node appears in G , and (iii) VR  (VpOWL  VERDF ) = , where VpOWL
denotes the part of the OWL vocabulary, included in the pD semantics. Let G, G be RDF graphs
such that (VG  VG )  (VpOWL  VERDF ) = . Then based on G and R, we can define, by a simple
transformation, an ERDF ontology O such that G |=R G iff O |=st G .

However, in this work, weak and strong negation are not considered. Thus, closed-world
reasoning is not supported. Additionally, in our theory, the condition of a rule is any ERDF
formula over a vocabulary V , (thus, involving any of the logical factors , , , , , ,
and ), and not just a conjunction of positive triples.
TRIPLE (Sintek & Decker, 2002) is a rule language for the Semantic Web that is
especially designed for querying and transforming RDF models (or contexts), supporting
RDF and a subset of OWL Lite. Its syntax is based on F-Logic (Kifer, Lausen, & Wu,
1995) and supports an important fragment of first-order logic. A triple is represented by
a statement of the form s[p  o] and sets of statements, sharing the same subject s, can
be aggregated using molecules of the form s[p1  o1 ; p2  o2 ; ....]. All variables must be
explicitly quantified, either existentially or universally. Arbitrary formulas can be used in
the body, while the head of the rules is restricted to atoms or conjunctions of molecules.
An interesting and relevant feature of TRIPLE is the use of models to collect sets of related
sentences. In particular, part of the semantics of the RDF(S) vocabulary is represented as
pre-defined rules (and not as semantic conditions on interpretations), which are grouped
together in a module. TRIPLE provides other features like path expressions, skolem model
terms, as well as model intersection and difference. Finally, it should be mentioned that
the queries and models are compiled into XSB Prolog. TRIPLE uses the Lloyd-Topor
transformations (Lloyd & Topor, 1984) to take care of the first-order connectives in the
16. The symbol D does not appear explicitly in the notation of R-entailement, for reasons of simplification.

67

fiAnalyti, Antoniou, Damasio, & Wagner

sentences and supports weak negation under the well-founded semantics (Gelder, Ross, &
Schlipf, 1991). Strong negation is not used.
Flora-2 (Yang, Kifer, & Zhao, 2003) is a rule-based object-oriented knowledge base system for reasoning with semantic information on the Web. It is based on F-logic (Kifer
et al., 1995) and supports metaprogramming, non-monotonic multiple inheritance, logical database updates, encapsulation, dynamic modules, and two kinds of weak negation.
Specifically, it supports Prolog negation and well-founded negation (Gelder et al., 1991),
through invocation of the corresponding operators \+ and tnot of the XSB system (Rao,
Sagonas, Swift, Warren, & Freire, 1997). The formal semantics for non-monotonic multiple inheritance is defined by Yang & Kifer (2003a). In addition, Flora-2 supports reification and anonymous resources (Yang & Kifer, 2003b). In particular, in Flora-2, reified
statements ${s(p  o)}$ are themselves objects. In contrast, in RDF(S), they are referred to by a URI or a blank node x, and are associated with the following RDF triples:
rdf :type(x, rdf :Statement), rdf :subject(x, s), rdf :predicate(x, p), and rdf :object(x, o). In
RDF(S) model theory (and thus, in our theory), no special semantics are given to reified
statements. In Flora-2, anonymous resources are handled through skolemization (similarly
to our theory).
Notation 3 (N3) (Berners-Lee, Connolly, Kagal, Scharf, & Hendler, 2008) provides a
more human readable syntax for RDF and also extends RDF by adding numerous predefined constructs (built-ins) for being able to express rules conveniently. Remarkably,
N3 contains a built-in (log:definitiveDocument) for making restricted completeness assumptions and another built-in (log:notIncludes) for expressing simple negation-as-failure
tests. The addition of these constructs was motivated by use cases. However, N3 does not
provide strong negation and closed-world reasoning is not fully supported. N3 is supported
by the CWM system17 , a forward engine especially designed for the Semantic Web, and
the Euler system18 , a backward engine relying on loop checking techniques to guarantee
termination.
Alferes et al. (2003) propose the paraconsistent well-founded semantics with explicit
negation (WFSXP )19 , as the appropriate semantics for reasoning with (possibly, contradictory) information in the Semantic Web. Supporting arguments include: (i) possible
reasoning, even in the presence of contradiction, (ii) program transformation into WFS,
and (iii) polynomial time inference procedures. No formal model theory has been explicitly
provided for the integrated logic.
DR-Prolog (Antoniou, Bikakis, & Wagner, 2004) and DR-DEVICE (Bassiliades, Antoniou, & Vlahavas, 2004) are two systems that integrate RDFS ontologies with rules (strict or
defeasible), that are partially ordered through a superiority relation, based on the semantics
of defeasible logic (Antoniou, Billington, Governatori, & Maher, 2001; Maher, 2002). Defeasible logic contains only one kind of negation (strong negation) in the object language20 and
allows to reason in the presence of contradiction and incomplete information. It supports
17. http://www.w3.org/2000/10/swap/doc/cwm.html.
18. http://www.agfa.com/w3c/euler/.
19. WFSXP (Alferes, Damasio, & Pereira, 1995) is an extension of the well-founded semantics with explicit
negation (WFSX) on extended logic programs (Pereira & Alferes, 1992) and, thus, also of the wellfounded semantics (WFS) on normal logic programs (Gelder et al., 1991).
20. However, in defeasible logic, negation-as-failure can be easily simulated by other language ingredients.

68

fiExtended RDF as a Semantic Foundation of Rule Markup Languages

monotonic and non-monotonic rules, exceptions, default inheritance, and preferences. No
formal model theory has been explicitly provided for the integrated logic.
OWL-DL (McGuinness & van Harmelen, 2004) is an ontology representation language
for the Semantic Web, that is a syntactic variant of the SHOIN (D) description logic and
a decidable fragment of first-order logic (Horrocks & Patel-Schneider, 2003). However, the
need for extending the expressive power of OWL-DL with rules has initiated several studies,
including the SWRL (Semantic Web Rule Language) proposal (Horrocks, Patel-Schneider,
Boley, Tabet, Grosof, & Dean, 2004). Horrocks & Patel-Schneider (2004) show that this
extension is in general undecidable. AL-log (Donini, Lenzerini, Nardi, & Schaerf, 1998)
was one of the first efforts to integrate Description Logics with (safe) datalog rules, while
achieving decidability. It considers the basic description logic ALC and imposes the constraint that only concept DL-atoms are allowed to appear in the body of the rules, whereas
the heads of the rules are always non DL-atoms. Additionally, each variable appearing in
a concept DL atom in the body of a rule has also to appear in a non DL-atom in the body
or head of the rule. CARIN (Levy & Rousset, 1998) provides a framework for studying the
effects of combining the description logic ALCN R with (safe) datalog rules. In CARIN,
both concept and role DL-atoms are allowed in the body of the rules. It is shown that the
integration is decidable if rules are non-recursive, or certain combinations of constructors
are not allowed in the DL component, or rules are role-safe (imposing a constraint on the
variables of role DL atoms in the body of the rules)21 . Motik et al. (2004) show that the
integration of a SHIQ(D) knowledge base L with a disjunctive datalog program P is decidable, if P is DL-safe, that is, all variables in a rule occur in at least one non DL-atom in the
body of the rule. In this work, in contrast to AL-log and CARIN, no tableaux algorithm
is employed for query answering but L is translated to a disjunctive logic program DD(L)
which is combined with P for answering ground queries.
In this category of works, entailment on the DL, that has been extended with rules, is
based on first-order logic. This means that both the DL component and the logic program
are viewed as a set of first-order logic statements. Thus, negation-as-failure, closed-worldassumptions, and non-monotonic reasoning cannot be supported. In contrast, our work
supports both weak and strong negation, and allows closed-world and open-world reasoning
on a selective basis.
A different kind of integration is achieved by Eiter et al. (2004a). In this work, a
SHOIN (D) knowledge base L communicates with an extended logic program P (possibly
with weak and strong negation), only through DL-query atoms in the body of the rules. In
particular, the description logic component L is used for answering the augmented, with
input from the logic program, queries appearing in the (possibly weakly negated) DL-query
atoms, thus allowing flow of knowledge from P to L and vice-versa. The answer set semantics of hL, P i are defined, as a generalization of the answer set semantics (Gelfond
& Lifschitz, 1990) on ordinary extended logic programs. A similar kind of integration is
achieved by Eiter et al. (2004b). In this work, a SHOIN (D) knowledge base L communicates with a normal logic program P (possibly with weak negation), through DL-query
atoms in the body of the rules. The well-founded semantics of hL, P i are defined, as a
21. A rule is role-safe if at least one of the variables x, y of each role DL atom R(x, y) in the body of the
rule, appears in some body atom of a base predicate, where a base predicate is an ordinary predicate that
appears only in facts or in rule bodies.

69

fiAnalyti, Antoniou, Damasio, & Wagner

generalization of the well-founded semantics (Gelder et al., 1991) of ordinary normal logic
programs. Obviously, in both of these works, derived information concerns only non DLatoms (that can be possibly used as input to DL-query atoms). Thus, rule-based reasoning
is supported only for non DL-atoms. In contrast, in our work, properties and classes appearing in the ERDF graphs can freely appear in the heads and bodies of the rules, allowing
even the derivation of metalevel statements such as subclass and subproperty relationships,
property transitivity, property and class totalness.
Rosati (1999) defines the semantics of a disjunctive AL-log knowledge base, based on
the stable model semantics for disjunctive databases (Gelfond & Lifschitz, 1991), extending
AL-log (Donini et al., 1998). A disjunctive AL-log knowledge base is the integration of an
ALC knowledge base T with a (safe) disjunctive logic program P that allows concept and
role DL-atoms in the body of the rules (along with weak negation on non DL-atoms). The
safety condition enforces that each variable in the head of a rule should also appear in the
body of the rule. Additionally, all constants in P should be DL-individuals. Similarly to
our case, in defining the disjunctive AL-log semantics, only the grounded versions of the
rules are considered (by instantiating variables with DL individuals). However rule-based
reasoning is supported only for non DL-atoms, and DL-atoms in the body of the rules
mainly express constraints.
In a subsequent work, Rosati (2005) defines the r-hybrid knowledge bases. In r-hybrid
knowledge bases, DL-atoms are allowed in the head of the rules and the DL component
T is a SHOIN (D) knowledge base. Additionally, constants in P are not necessarily DLindividuals. However, a stronger safety condition is imposed, as each rule variable should
appear in a (positive) non DL-atom in the body of the rule. Additionally, weak negation is
allowed only for non DL-atoms and rule-based meta-reasoning is not supported. In general,
we can say that for non DL-atoms, a closed-world assumption is made, while DL-atoms
conform to the open-world assumption, as SHOIN (D) is a fragment of first-order logic.

11. Conclusions
In this paper, we have extended RDF graphs to ERDF graphs by allowing negative triples
for representing explicit negative information. Then, we proceeded by defining an ERDF
ontology as an ERDF graph complemented by a set of derivation rules with all connectives
 (weak negation),  (strong negation),  (material implication), , , ,  in the body
of a rule, and with strong negation  in the head of a rule. Moreover, we have extended
the RDF(S) vocabulary by adding the predefined vocabulary elements erdf :TotalProperty
and erdf :TotalClass, for representing the metaclasses of total properties and total classes,
on which the open-world assumption applies.
We have defined ERDF formulas, ERDF interpretations, and ERDF entailment on
ERDF formulas, showing that it conservatively extends RDFS entailment on RDF graphs.
We have developed the model-theoretic semantics of ERDF ontologies, called ERDF stable
model semantics, showing that stable model entailment extends ERDF entailment on ERDF
graphs, and thus it also extends RDFS entailment on RDF graphs. The ERDF stable model
semantics is based on Partial Logic and, in particular, on its generalized definition of stable
models (Herre & Wagner, 1997; Herre et al., 1999) (which extends answer set semantics
on extended logic programs). We have shown that classical (boolean) Herbrand model
70

fiExtended RDF as a Semantic Foundation of Rule Markup Languages

reasoning is a special case of our semantics, when all properties are total. In this case,
similarly to classical logic, an open-world assumption is made for all properties and classes
and the two negations (weak and strong negation) collapse. Allowing (a) the totality of
properties and classes to be declared on a selective basis and (b) the explicit representation
of closed-world assumptions (as derivation rules) enables the combination of open-world
and closed-world reasoning in the same framework.
In particular, for a total property p, the open-world assumption applies, since each
considered Herbrand interpretation I, in the computation of ERDF stable models, satisfies
p(x, y)p(x, y), for each pair (x, y) of ontology vocabulary terms. For a closed property p,
a default closure rule of the form p(?x, ?y)  p(?x, ?y) is added, which allows to infer
the falsity of p(x, y), if there is no evidence that p(x, y) holds. However, this method only
works for partial properties. For a total property p, it may happen that there is a stable
model, where p(x, y) holds, even though there is no evidence for it (see the example in
Section 5, above Proposition 5.1). In fact, if p is a total property, the existence or not of
the corresponding default closure rule does not affect the ontology semantics.
The main advantages of ERDF are summarized as follows:
 It has a Tarski-style model theory, which is a desirable feature for logic languages for
the Semantic Web (Bry & Marchiori, 2005).
 It is based on Partial Logic (Herre et al., 1999), which is the simplest conservative
extension of classical logic that supports both weak and strong negation. Partial
logic also extends Answer Set Programming (ASP)22 (Gelfond & Lifschitz, 1990), by
allowing all logical factors , , , , , ,  in the body of a rule.
 It enables the combination of open-world (monotonic) and closed-world (non-monotonic)
reasoning, in the same framework.
 It extends RDFS ontologies with derivation rules and integrity constraints.
Satisfiability and entailment under the ERDF stable model semantics are in general undecidable. In a subsequent paper, we plan to identify syntactic restrictions for the ERDF
ontologies that guarantee decidability of reasoning and to elaborate on the ERDF computability and complexity issues.
In this work, we consider only coherent ERDF interpretations. However, due to the
Semantic Webs decentralized and distributed nature, contradictory information is frequent
(Schaffert, Bry, Besnard, Decker, Decker, Enguix, & Herzig, 2005). Though Partial Logic
allows for truth-value clashes, handling inconsistency in the Semantic Web is a topic that
deserves extended treatment, which is outside the scope of this paper. It is in our future
plans to consider general ERDF interpretations and extend the vocabulary of ERDF with
the terms erdf :CoherentProperty and erdf :CoherentClass, whose instances are properties
and classes that satisfy coherence. Thus, coherence will be decided on a per property and
22. ASP is a well-known and accepted knowledge representation formalism that allows (through credulous
reasoning) the definition of concepts ranging over a space of choices. This feature enables the compact
representation of search and optimization problems (Eiter, Ianni, Polleres, & Schindlauer, 2006).

71

fiAnalyti, Antoniou, Damasio, & Wagner

per class basis. Admitting incoherent models will only be interesting in combination with
a second preference criterion of minimal incoherence (Herre et al., 1999).
Our future work also concerns the support of datatypes, including XSD datatypes, and
the extension of the predefined ERDF vocabulary by adding other useful constructs, possibly
in accordance with the extensions of ter Horst (2005b). We also plan to formally define the
ERDF/XML syntax, briefly presented in Section 7. Moreover, we plan to implement an
ERDF inference engine.
Finally, we would like to mention that the success of the Semantic Web is impossible without support for modularity, encapsulation, information hiding, and access control.
Modularity mechanisms and syntactic restrictions for merging knowledge bases in the Semantic Web are explored by Damasio et al. (2006). However, in this work, knowledge bases
are expressed by extended logic programs. Our future plans include the extension of ERDF
with mechanisms allowing sharing of knowledge between different ERDF ontologies, along
the lines proposed by Damasio et al. (2006).

Acknowledgments
The authors would like to thank the reviewers for their valuable comments. This research
has been partially funded by the European Commission and by the Swiss Federal Office for Education and Science within the 6th Framework Programme project REWERSE
num. 506779 (www.rewerse.net).

Appendix A: RDF(S) Semantics
For self-containment, in this Appendix, we review the definitions of simple, RDF, and
RDFS interpretations, as well as the definitions of satisfaction of an RDF graph and RDFS
entailment. For details, see the W3C Recommendation of RDF semantics (Hayes, 2004).
Let URI denote the set of URI references, PL denote the set of plain literals, and T L
denote the set of typed literals, respectively. A vocabulary V is a subset of URI  PL  T L.
The vocabulary of RDF, VRDF , and the vocabulary of RDFS, VRDF S , are shown in Table
1 (Section 3).
Definition A.1 (Simple interpretation) A simple interpretation I of a vocabulary V
consists of:
 A non-empty set of resources ResI , called the domain or universe of I.
 A set of properties P ropI .
 A vocabulary interpretation mapping IV : V  URI  ResI  P ropI .
 A property extension mapping PT I : P ropI  P(ResI  ResI ).
 A mapping ILI : V  T L  ResI .
 A set of literal values LV I  ResI , which contains V  PL.

We define the mapping: I : V  ResI  P ropI such that:
 I(x) = IV (x), x  V  URI.
72

fiExtended RDF as a Semantic Foundation of Rule Markup Languages

 I(x) = x,  x  V  PL.
 I(x) = ILI (x),  x  V  T L. 

Definition A.2 (Satisfaction of an RDF graph w.r.t. a simple interpretation) Let
G be an RDF graph and let I be a simple interpretation of a vocabulary V . Let v be a
mapping v : Var (G)  ResI . If x  Var (G), we define [I + v](x) = v(x). If x  V , we
define [I + v](x) = I(x). We define:
 I, v |= G iff  p(s, o)  G, it holds that: p  V, s, o  V  Var , I(p)  P ropI , and
h[I + v](s), [I + v](o)i  PT I (I(p)).
 I satisfies the RDF graph G, denoted by I |= G, iff there exists a mapping v :
Var (G)  ResI such that I, v |= G. 
rdf :type(rdf :type, rdf :Property)
rdf :type(rdf :subject, rdf :Property)
rdf :type(rdf :predicate, rdf :Property)
rdf :type(rdf :object, rdf :Property)
rdf :type(rdf :f irst, rdf :Property)
rdf :type(rdf :rest, rdf :Property)
rdf :type(rdf :value, rdf :Property)
rdf :type(rdf : i, rdf :Property), i  {1, 2, ...}
rdf :type(rdf :nil, rdf :List)

Table 2: The RDF axiomatic triples
Definition A.3 (RDF interpretation) An RDF interpretation I of a vocabulary V is a
simple interpretation of V  VRDF , which satisfies the following semantic conditions:
1. x  P ropI iff hx, I(rdf :Property)i  PT I (I(rdf :type)).
2. If srdf :XMLLiteral  V and s is a well-typed XML literal string, then
ILI (srdf :XMLLiteral ) is the XML value of s,
ILI (srdf :XMLLiteral )  LV I , and
hILI (srdf :XMLLiteral ), I(rdf :XMLLiteral )i  PT I (I(rdf :type)).
3. If srdf :XMLLiteral  V and s is an ill-typed XML literal string then
ILI (srdf :XMLLiteral )  ResI  LV I , and
hILI (srdf :XMLLiteral ), I(rdf :XMLLiteral )i 6 PT I (I(rdf :type)).
4. I satisfies the RDF axiomatic triples, shown in Table 2. 

Definition A.4 (RDF entailment) Let G, G be RDF graphs. We say that G RDFentails G (G |=RDF G ) iff for every RDF interpretation I, if I |= G then I |= G . 
Definition A.5 (RDFS interpretation) An RDFS interpretation I of a vocabulary V
is an RDF interpretation of V  VRDF  VRDF S , extended by the new ontological category
ClsI  ResI for classes, as well as the class extension mapping CT I : ClsI  P(ResI ),
such that:
73

fiAnalyti, Antoniou, Damasio, & Wagner

rdfs:domain(rdf :type, rdfs:Resource)
rdfs:domain(rdfs:domain, rdf :Property)
rdfs:domain(rdfs:range, rdf :Property)
rdfs:domain(rdfs:subPropertyOf , rdf :Property)
rdfs:domain(rdfs:subClassOf , rdfs:Class)
rdfs:domain(rdf :subject, rdf :Statement)
rdfs:domain(rdf :predicate, rdf :Statement)
rdfs:domain(rdf :object, rdf :Statement)
rdfs:domain(rdfs:member, rdfs:Resource)
rdfs:domain(rdf :f irst, rdf :List)
rdfs:domain(rdf :rest, rdf :List)
rdfs:domain(rdfs:seeAlso, rdfs:Resource)
rdfs:domain(rdfs:isDef inedBy, rdfs:Resource)
rdfs:domain(rdfs:comment, rdfs:Resource)
rdfs:domain(rdfs:label, rdfs:Resource)
rdfs:domain(rdfs:value, rdfs:Resource)
rdfs:range(rdf :type, rdfs:Class)
rdfs:range(rdfs:domain, rdfs:Class)
rdfs:range(rdfs:range, rdfs:Class)
rdfs:range(rdfs:subPropertyOf , rdf :Property)
rdfs:range(rdfs:subClassOf , rdfs:Class)
rdfs:range(rdf :subject, rdfs:Resource)
rdfs:range(rdf :predicate, rdfs:Resource)
rdfs:range(rdf :object, rdfs:Resource)
rdfs:range(rdfs:member, rdfs:Resource)
rdfs:range(rdf :f irst, rdfs:Resource)
rdfs:range(rdf :rest, rdf :List)
rdfs:range(rdfs:seeAlso, rdfs:Resource)
rdfs:range(rdfs:isDef inedBy, rdfs:Resource)
rdfs:range(rdfs:comment, rdfs:Literal)
rdfs:range(rdfs:label, rdfs:Literal)
rdfs:range(rdf :value, rdfs:Resource)
rdfs:subClassOf (rdf :Alt, rdfs:Container)
rdfs:subClassOf (rdf :Bag, rdfs:Container)
rdfs:subClassOf (rdf :Seq, rdfs:Container)
rdfs:subClassOf (rdfs:ContainerMembershipProperty, rdf :Property)
rdfs:subPropertyOf (rdfs:isDef inedBy, rdfs:seeAlso)
rdf :type(rdf :XMLLiteral , rdfs:Datatype)
rdfs:subClassOf (rdf :XMLLiteral , rdfs:Literal)
rdfs:subClassOf (rdfs:Datatype, rdfs:Class)
rdf :type(rdf : i, rdfs:ContainerMembershipProperty), i  {1, 2, ...}
rdfs:domain(rdf : i, rdfs:Resource), i  {1, 2, ...}
rdfs:range(rdf : i, rdfs:Resource), i  {1, 2, ...}

Table 3: The RDFS axiomatic triples

74

fiExtended RDF as a Semantic Foundation of Rule Markup Languages

1. x  CT I (y) iff hx, yi  PT I (I(rdf :type)).
2. The ontological categories are defined as follows:
ClsI = CT I (I(rdfs:Class)),
ResI = CT I (I(rdfs:Resource)), and
LV I = CT I (I(rdfs:Literal)).
3. If hx, yi  PT I (I(rdfs:domain)) and hz, wi  PT I (x) then z  CT I (y).
4. If hx, yi  PT I (I(rdfs:range)) and hz, wi  PT I (x) then w  CT I (y).
5. If x  ClsI then hx, I(rdfs:Resource)i  PT I (I(rdfs:subClassOf )).
6. If hx, yi  PT I (I(rdfs:subClassOf )) then x, y  ClsI , CT I (x)  CT I (y).
7. PT I (I(rdfs:subClassOf )) is a reflexive and transitive relation on ClsI .
8. If hx, yi  PT I (I(rdfs:subPropertyOf )) then x, y  P ropI , PT I (x)  PT I (y).
9. PT I (I(rdfs:subPropertyOf )) is a reflexive and transitive relation on P ropI .
10. If x  CT I (I(rdfs:Datatype)) then hx, I(rdfs:Literal)i  PT I (I(rdfs:subClassOf )).
11. If x  CT I (I(rdfs:ContainerM embershipP roperty)) then
hx, I(rdfs:member)i  PT I (I(rdfs:subPropertyOf )).
12. I satisfies the RDFS axiomatic triples, shown in Table 3. 

Definition A.6 (RDFS entailment) Let G, G be RDF graphs. We say that G RDFSentails G (G |=RDF S G ) iff for every RDFS interpretation I, if I |= G then I |= G .


Appendix B: Proofs
In this Appendix, we prove the lemmas and propositions presented in the main paper. In
addition, we provide Lemma B.1, which is used in some of the proofs. To reduce the size of
the proofs, we have eliminated the namespace from the URIs in VRDF  VRDF S  VERDF .
Lemma B.1 Let F be an ERDF formula and let I be a partial interpretation of a
vocabulary V . Let u, u be mappings u, u : Var (F )  ResI such that u(x) = u (x),
x  FVar (F ). It holds: I, u |= F iff I, u |= F .
Proof: We prove the proposition by induction. Without loss of generality, we assume that
 appears only in front of positive ERDF triples. Otherwise we apply the transformation
rules of Definition 3.4, to get an equivalent formula that satisfies the assumption.
Let F = p(s, o). It holds: I, u |= F iff p  V , s, o  V  Var , I(p)  P ropI , and
h[I + u](s), [I + u](o)i  PT I (I(p)) iff p  V , s, o  V  Var , I(p)  P ropI , and h[I +
u ](s), [I + u ](o)i  PT I (I(p)) iff I, u |= p(s, o).
Let F = p(s, o). It holds: I, u |= F iff p  V , s, o  V  Var , I(p)  P ropI ,
and h[I + u](s), [I + u](o)i  PF I (I(p)) iff p  V , s, o  V  Var , I(p)  P ropI , and
h[I + u ](s), [I + u ](o)i  PF I (I(p)) iff I, u |= p(s, o).
Assumption: Assume that the lemma holds for the subformulas of F .
We will show that the lemma holds also for F .
Let F = G. It holds: I, u |= F iff I, u |= G iff VG  V and I, u 6|= G iff VG  V and
I, u 6|= G iff I, u |= G iff I, u |= F .
75

fiAnalyti, Antoniou, Damasio, & Wagner

Let F = F1 F2 . It holds: I, u |= F iff I, u |= F1 F2 iff I, u |= F1 and I, u |= F2 iff
I, u |= F1 and I, u |= F2 iff I, u |= F1 F2 iff I, u |= F .
Let F = x G. We will show that (i) if I, u |= F then I, u |= F and (ii) if I, u |= F
then I, u |= F .
(i) Let I, u |= F . Then, I, u |= xG. Thus, there exists a mapping u1 : Var (G)  ResI
s.t. u1 (y) = u(y), y  Var (G)  {x}, and I, u1 |= G. Let u2 be the mapping u2 :
Var (G)  ResI s.t. u2 (y) = u (y), y  Var (G)  {x}, and u2 (x) = u1 (x). Since u(z) =
u (z), z  FVar (F ) and x  FVar (G), it follows that u1 (z) = u2 (z), z  FVar (G).
Thus, I, u2 |= G. Therefore, there exists a mapping u2 : Var (G)  ResI s.t. u2 (y) = u (y),
y  Var (G)  {x}, and I, u2 |= G. Thus, I, u |= x G, which implies that I, u |= F .
(ii) We prove this statement similarly to (i) by exchanging u and u .
Let F = F1 F2 or F = F1  F2 or F = xG. We can prove, similarly to the above
cases, that I, u |= F iff I, u |= F . 
Lemma 3.1. Let G be an ERDF graph and let I be a partial interpretation of a vocabulary
V . It holds: I |=GRAPH G iff I |= formula(G).
Proof: Let G = {t1 , ..., tn } and F = formula(G).
) Assume that I |=GRAPH G, we will show that I |= F . Since I |=GRAPH G, it follows that
v : Var (G)  ResI such that I, v |= ti , i = 1, ..., n. Thus, v : Var (G)  ResI such
that I, v |= t1 ...tn . This implies that u : Var (G)  ResI such that I, u |= F . Since
FVar (F ) = , it follows from Lemma B.1 that u : Var (G)  ResI , it holds that I, u |= F .
Thus, I |= F .
) Assume that I |= F , we will show that I |=GRAPH G. Since I |= F , it follows that
v : Var (G)  ResI it holds that I, v |= F . Thus, v : Var (G)  ResI such that
I, v |= F . This implies that u : Var (G)  ResI such that I, u |= t1 ...tn . Thus,
u : Var (G)  ResI such that I, u |= ti , i = 1, ..., n. Therefore, I |=GRAPH G. 
Proposition 3.1. Let I be an ERDF interpretation of a vocabulary V and let V  =
V  VRDF  VRDF S  VERDF . Then,
1. For all p, s, o  V  such that I(p)  TProp I , it holds:
I |= p(s, o) iff I |= p(s, o) (equivalently, I |= p(s, o)  p(s, o)).
2. For all x, c  V  such that I(c)  TCls I , it holds:
I |= rdf :type(x, c) iff I |= rdf :type(x, c)
(equivalently, I |= rdf :type(x, c)  rdf :type(x, c)).
Proof:
1) It holds: I |= p(s, o) iff I 6|= p(s, o) iff hI(s), I(o)i 6 PT I (p) iff (since p  TProp I )
hI(s), I(o)i  PF I (p) iff I |= p(s, o). Therefore, I |= p(s, o) iff I |= p(s, o).
We will also show that I |= p(s, o)  p(s, o). It holds I |= p(s, o) or I |= p(s, o). This
implies that I |= p(s, o) or I |= p(s, o), and thus, I |= p(s, o)  p(s, o).
2) The proof is similar to the proof of 1) after replacing p(s, o) by type(x, c) and TProp I by
TCls I . 
Proposition 3.2. Let G, G be RDF graphs such that VG  VERDF =  and VG  VERDF =
. Then, G |=RDF S G iff G |=ERDF G .
76

fiExtended RDF as a Semantic Foundation of Rule Markup Languages

Proof:
) Let G |=ERDF G . We will show that G |=RDF S G . In particular, let I be an RDFS
interpretation of a vocabulary V s.t. I |= G, we will show that I |= G .
Since I |= G, it holds that v : Var (G)  ResI s.t. I, v |= G. Our goal is to construct
an ERDF interpretation J of V s.t. J |= G. We consider an 1-1 mapping res : VERDF  R,
where R is a set disjoint from ResI . Additionally, let V  = V  VRDF  VRDF S  VERDF .
Based on I and the mapping res, we construct a partial interpretation J of V as follows:
 ResJ = ResI  res(VERDF ).
 JV (x) = IV (x), x  (V   VERDF )  URI and JV (x) = res(x), x  VERDF .
 We define the mapping: ILJ : V   T L  ResJ such that: ILJ (x) = ILI (x).
 We define the mapping: J : V   ResJ such that:
 J(x) = JV (x), x  V   URI.
 J(x) = x,  x  V   PL.
 J(x) = ILJ (x),  x  V   T L.
 We define the mapping PT J : ResJ  P(ResJ  ResJ ) as follows:
(PT1) if x, y, z  ResI and hx, yi  PT I (z) then hx, yi  PT J (z).
(PT2) hres(TotalClass), J(Class)i  PT J (J(subClassOf )).
(PT3) hres(TotalProperty), J(Property)i  PT J (J(subClassOf )).
Starting from the derivations of (PT1), (PT2), and (PT3), the following rules are
applied recursively, until a fixpoint is reached:
(PT4) if hx, yi  PT J (J(domain)) and hz, wi  PT J (x) then
hz, yi  PT J (J(type)).
(PT5) if hx, yi  PT J (J(range)) and hz, wi  PT J (x) then
hw, yi  PT J (J(type)).
(PT6) if hx, J(Class)i  PT J (J(type)) then
hx, J(Resource)i  PT J (J(subClassOf )).
(PT7) if hx, yi  PT J (J(subClassOf )) then hx, J(Class)i  PT J (J(type)).
(PT8) if hx, yi  PT J (J(subClassOf )) then hy, J(Class)i  PT J (J(type)).
(PT9) if hx, yi  PT J (J(subClassOf )) and hz, xi  PT J (J(type)) then
hz, yi  PT J (J(type)).
(PT10) if hx, J(Class)i  PT J (J(type)) then hx, xi  PT J (J(subClassOf )).
(PT11) if hx, yi  PT J (J(subClassOf )) and hy, zi  PT J (J(subClassOf )) then
hx, zi  PT J (J(subClassOf )).
(PT12) if hx, yi  PT J (J(subPropertyOf )) then hx, J(Property)i  PT J (J(type)).
(PT13) if hx, yi  PT J (J(subPropertyOf )) then hy, J(Property)i  PT J (J(type)).
(PT14) if hx, yi  PT J (J(subPropertyOf )) and hz, wi  PT J (x) then
hz, wi  PT J (y).
77

fiAnalyti, Antoniou, Damasio, & Wagner

(PT15) if hx, J(Property)i  PT J (J(type)) then hx, xi  PT J (J(subPropertyOf )).
(PT16) if hx, yi  PT J (J(subPropertyOf )) and hy, zi  PT J (J(subPropertyOf ))
then hx, zi  PT J (J(subPropertyOf )).
(PT17) if hx, J(Datatype)i  PT J (J(type)) then
hx, J(Literal)i  PT J (J(subClassOf )).
(PT18) if hx, J(ContainerM embershipP roperty)i  PT J (J(type)) then
hx, J(member)i  PT J (J(subPropertyOf )).
After reaching fixpoint, nothing else is contained in PT J (x), x  ResJ .
 P ropJ = {x  ResJ | hx, J(Property)}  PT J (J(type))}.
 The mapping PT J : P ropJ  P(ResJ  ResJ ) is defined as follows:
PT J (x) = PT J (x), x  P ropJ .
 LV J = {x  ResJ | hx, J(Literal)i  PT J (J(type))}.
 The mapping PF J : P ropJ  P(ResJ  ResJ ) is defined as follows:
(PF1) if srdf :XMLLiteral  V is an ill-typed XML-Literal then
hILJ (srdf :XMLLiteral ), J(Literal)i  PF J (J(type)).
(PF2) if hJ(TotalClass), J(TotalClass)i  PT J (J(type)) then
x  ResJ  {J(TotalClass)}, hx, J(TotalClass)i  PF J (J(type)).
(PF3) if hJ(TotalProperty), J(TotalProperty)i  PT J (J(type)) then
x, y  ResJ , hx, yi  PF J (J(TotalProperty)).
Starting from the derivations of (PF1), (PF2), and (PF3), the following rules are
applied recursively, until a fixpoint is reached:
(PF4) if hx, yi  PT J (J(subClassOf )) and hz, yi  PF J (J(type)) then
hz, xi  PF J (type).
(PF5) if hx, yi  PT J (J(subPropertyOf )) and hz, wi  PF J (y) then
hz, wi  PF J (x).
After reaching fixpoint, nothing else is contained in PF J (x), x  P ropJ .
Before we continue, we prove the following lemma:
Lemma: For all x, y, x  ResJ , hx, yi  PT J (z) iff hx, yi  PT J (z).
Proof :
) if hx, yi  PT J (z), then from the definition of PT J , it follows immediately that
hx, yi  PT J (z).
) Let hx, yi  PT J (z). Then, from the definition of PT J , it follows that it holds (i)
z  P ropI or (ii) w  ResJ , s.t. hw, zi  PT J (J(subPropertyOf )).
(i) Assume that z  P ropI . Then, hz, I(Property)i  PT I (I(type)). This implies that
hz, J(Property)i  PT I (J(type)). From (PT1), it now follows that hz, J(v)i  PT J (J(type)).
Therefore, z  P ropJ . From the definition of PT J , it now follows that hx, yi  PT J (z).
78

fiExtended RDF as a Semantic Foundation of Rule Markup Languages

(ii) Assume that w  ResJ s.t. hw, zi  PT J (J(subPropertyOf )). Then, from (PT13),
it follows that hz, J(Property)i  PT J (J(type)). Therefore, z  P ropJ . From the definition
of PT J , it now follows that hx, yi  PT J (z).
End of Lemma
Though not mentioned explicitly, the above Lemma is used throughout the rest of the
proof.
To show that J is a partial interpretation of V  , it is enough to show that V  PL  LV J .
Let x  V   PL. Then, x  LV I . Thus, hx, I(Literal)i  PT I (I(type)). Due to (PT1),
this implies that hx, J(Literal)i  PT J (J(type)). Thus, x  LV J .
Now, we extend J with the ontological categories:
ClsJ = {x  ResJ | hx, J(Class)i  PT J (J(type))},
TCls J = {x  ResJ | hx, J(TotalClass)i  PT J (J(type))}, and
TProp J = {x  ResJ | hx, J(TotalProperty)i  PT J (J(type))}.
We define CT J , CF J : ClsJ  P(ResJ ) as follows:
x  CT J (y) iff hx, yi  PT J (J(type)), and
x  CF J (y) iff hx, yi  PF J (J(type)).
We will now show that J is an ERDF interpretation of V . Specifically, we will show that
J satisfies the semantic conditions of Definition 3.7 (ERDF interpretation) and Definition
3.2 (Coherent ERDF interpretation).
First, we will show that J satisfies semantic condition 2 of Definition 3.7. We will start
by proving that ResJ = CT J (J(Resource)). Obviously,
CT J (J(Resource))  ResJ . Thus, it is enough to prove that ResJ  CT J (J(Resource)).
Let x  ResJ . Then, we distinguish the following cases:
Case 1) x  ResI . Since I is an RDFS interpretation, it holds that hx, I(Resource)i 
PT I (I(type)). Thus, it holds hx, J(Resource)i  PT J (J(type)), which implies that x 
CT J (J(Resource)).
Case 2) x  res(VERDF ). From the definition of PT J , it follows that
hx, J(Resource)i  PT J (J(type)). Thus, hx, J(Resource)i  PT J (J(type)), which implies
that x  CT J (J(Resource)).
Thus, ResJ = CT J (J(Resource)).
Additionally, it is easy to see that it holds P ropJ = CT J (J(Property)), ClsJ =
CT J (J(Class)), LV J = CT J (J(Literal)), TCls J = CT J (J(TotalClass)), and
TProp J = CT J (J(TotalProperty)).
We will now show that J satisfies semantic condition 3 of Definition 3.7. Let hx, yi 
PT J (J(domain)) and hz, wi  PT J (x). Then, from (PT4) and the definition of CT J , it
follows that z  CT J (y).
We will now show that J satisfies semantic condition 4 of Definition 3.7. Let hx, yi 
PT J (J(range)) and hz, wi  PT J (x). Then, from (PT5) and the definition of CT J , it
follows that w  CT J (y).
We will now show that J satisfies semantic condition 5 of Definition 3.7. Let x 
ClsJ . Thus, it holds: hx, J(Class)i  PT J (J(type)). From (PT6), it now follows that
hx, J(Resource)i  PT J (J(subClassOf )).
79

fiAnalyti, Antoniou, Damasio, & Wagner

We will now show that J satisfies semantic condition 6 of Definition 3.7. Let hx, yi 
PT J (J(subClassOf )). Then, from (PT7), (PT8), and the definition of CT J , it follows that
x, y  ClsJ .
Let hx, yi  PT J (J(subClassOf )). We will show that CT J (x)  CT J (y). In particular,
let z  CT J (x). Then, from (PT9) and the definition of CT J , it follows that z  CT J (y).
Let hx, yi  PT J (J(subClassOf )). We will show that CF J (y)  CF J (x). In particular,
let z  CF J (y). Then, from (PF4) and the definition of CF J , it follows that z  CF J (x).
In a similar manner, we can prove that J also satisfies the semantic conditions 7, 8, 9,
10, and 11 of Definition 3.7.
To continue the rest of the proof, we need to make a few observations.
Consider the mapping h : ResJ  ResI , which is defined as follows:

if x  ResI
 x
I(Class)
if x = res(TotalClass)
h(x) =

I(Property) if x = res(TotalProperty)

Observation 1: If hx, yi  PT J (z) and y  res(VERDF ) then x = y.
Observation 2: If x  res(VERDF ) and x  P ropJ then PT J (x) = .
Observation 3: If hx, yi  PT J (z) then hh(x), h(y)i  PT I (h(z)).
Observation 4: If x, y, z  ResI and hx, yi  PT J (z) then hx, yi  PT I (z)23 .
The proof of these observations is made by induction. It is easy to see that all observations
hold for the derivations of (PT1), (PT2), and (PT3). Assume now that the observations
hold for the derivations obtained at a step k of the application of the fixpoint operator for
PT J . Then, the observations also hold for the derivations obtained at step k + 1.
We will now show that J satisfies semantic condition 12 of Definition 3.7. Let x 
TCls J . Thus, hx, J(TotalClass)i  PT J (J(type)). From Observation 1, it follows that x =
J(TotalClass). From (PF2), it now follows that CT J (J(TotalClass))CF J (J(TotalClass)) =
ResJ . Thus, CT J (x)  CF J (x) = ResJ .
We will now show that J satisfies semantic condition 13 of Definition 3.7. Let x 
TProp J . Thus, hx, J(TotalProperty)i  PT J (J(type)). From Observation 1, it follows
that x = J(TotalProperty). From (PF3), it now follows that PT J (J(TotalProperty)) 
PF J (J(TotalProperty)) = ResJ  ResJ . Thus, PT J (x)  PF J (x) = ResJ  ResJ .
We will now show that J satisfies semantic condition 14 of Definition 3.7.
Let srdf :XMLLiteral be a well-typed XML-Literal in V then ILJ (srdf :XMLLiteral )
= ILI (srdf :XMLLiteral ) is the XML value of s. Additionally, since I is an RDFS
interpretation of V , it holds: hILI (srdf :XMLLiteral ), I(XMLLiteral )i  PT I (I(type)).
Therefore, from (PT1), it follows that hILJ (srdf :XMLLiteral ), J(XMLLiteral )i 
PT J (J(type)).
We will now show that J satisfies semantic condition 15 of Definition 3.7. Let
srdf :XMLLiteral  V s.t. s is not a well-typed XML literal string. Assume now
that ILJ (srdf :XMLLiteral )  LV J . Then, hILJ (srdf :XMLLiteral ), J(Literal)i 
PT J (J(type)). From Observation 4, it follows that hILJ (srdf :XMLLiteral ), J(Literal)i
 PT I (J(type)). Therefore, it follows that hILI (srdf :XMLLiteral ), I(Literal)i 
23. Note that Observation 3 implies Observation 4.

80

fiExtended RDF as a Semantic Foundation of Rule Markup Languages

PT I (I(type)). Thus, ILI (srdf :XMLLiteral )  LV I , which is impossible since I is an
RDFS interpretation of V . Therefore, ILJ (srdf :XMLLiteral )  ResJ  LV J .
Additionally, from (PF1), it follows that hILJ (srdf :XMLLiteral ), J(Literal)i 
PF J (J(type)).
J also satisfies semantic condition 16 of Definition 3.7, due to (PT1). Finally, J satisfies
semantic condition 17, due to (PT2) and (PT3).
Thus, J is an ERDF interpretation of V .
Now, we will show that J is a coherent ERDF interpretation (Definition 3.2). Assume
that this is not the case. Thus, there is z  P ropJ s.t. PT J (z)  PF J (z) 6= . Assume that
hx, yi  PT J (z)  PF J (z), for such a z. We distinguish the following cases:
Case 1) z  res(VERDF ). Then, from Observation 2, it follows that PT J (z) = , which
is a contradiction.
Case 2) y  res(VERDF ) and z  ResI . Then, it holds:
(i) hz, res(TotalProperty)i  PT J (J(subPropertyOf )), or
(ii) hz, J(type)i  PT J (J(subPropertyOf )) and hx, yi  PF J (J(type)).
Now, from Observation 1 and since z  ResI , (i) is impossible. Thus, hz, J(type)i 
PT J (J(subPropertyOf )) and hx, yi  PF J (J(type)). This implies that
y = res(TotalClass). From Observation 1, it follows that x = res(TotalClass), which is
impossible since, due to (PF2), hres(TotalClass), res(TotalClass)i 6 PF J (J(type)).
Case 3) x  res(VERDF ) and y, z  ResI . Then, it holds:
(i) hz, res(TotalProperty)i  PT J (J(subPropertyOf )), or
(ii) hz, J(type)i  PT J (J(subPropertyOf )) and hx, yi  PF J (J(type)).
Now, from Observation 1 and since z  ResI , (i) is impossible. Thus, hz, J(type)i 
PT J (J(subPropertyOf )) and hx, yi  PF J (J(type)). This implies that
y = res(TotalClass), which is impossible, since y  ResI .
Case 4) x, y, z  ResI . Then, x = ILJ (s), where s is an ill-typed XML-Literal in
V , hz, J(type)i  PT J (J(subPropertyOf )) and hy, J(Literal)i  PT J (J(subClassOf )).
Since hx, yi  PT J (z), it follows that hx, yi  PT J (J(type)). Since hy, J(Literal)i 
PT J (J(subClassOf )), it follows that hx, J(Literal)i  PT J (J(type)). From Observation 4,
it follows that hILJ (s), J(Literal)i  PT I (J(type)). Therefore,
hILI (s), I(Literal)i  PT I (I(type)). But this implies that ILI (s)  LV I , which is impossible since I is an RDFS interpretation of V .
Since all cases lead to contradiction, it follows that:
z  P ropJ , PT J (z)  PF J (z) = .
We will now show that J, v |= G. Let p(s, o)  G. Since I, v |= G, it holds that
p  V  , s, o  V   Var . Note that, due to (PT1), it holds P ropI  P ropJ . Since
p 6 VERDF , it holds J(p) = I(p)  P ropI  P ropJ . Since s, o 6 VERDF , it holds that
[I + v](s) = [J + v](s) and [I + v](o) = [J + v](o). Since I, v |= G, it holds h[I + v](s), [I +
v](o)i  PT I (I(p)). Thus, h[J + v](s), [J + v](o)i  PT I (J(p)). From (PT1), it follows
that h[J + v](s), [J + v](o)i  PT J (J(p)). Thus, J, v |= G, which implies that J |= G.
Since J is an ERDF interpretation and G |=ERDF G , it follows that J |= G . Thus,
there is u : Var (G )  ResJ = ResI  res(VERDF ) s.t. J, u |= G . We define a mapping
u : Var (G )  ResI as follows:
81

fiAnalyti, Antoniou, Damasio, & Wagner


if u(x)  ResI
 u(x)
I(Class)
if u(x) = res(TotalClass)
u (x) =

I(Property) if u(x) = res(TotalProperty)

We will show that I, u |= G . Let p(s, o)  G . Since J |= G and VG  VERDF = ,
it follows that p  V  VRDF  VRDF S , s, o  V  VRDF  VRDF S  Var , and J(p) 
P ropJ . Thus, hJ(p), J(type)i  PT J (J(Property)), which implies (since p 6 VERDF ) that
hI(p), I(type)i  PT J (I(Property). Due to Observation 4, it follows that hI(p), I(type)i 
PT I (I(Property). Thus, I(p)  P ropI . Additionally, it holds: h[J + u](s), [J + u](o)i 
PT J (J(p)). We want to show that h[I + u ](s), [I + u ](o)i  PT I (I(p)).
Case 1) It holds: (i) if s  Var (G ) then u(s) 6 res(VERDF ) and (ii) if o  Var (G ) then
u(o) 6 res(VERDF ).
Then, [J + u](s) = [J + u ](s) = [I + u ](s)  ResI , [J + u](o) = [J + u ](o) = [I +
u ](o)  ResI , and J(p) = I(p)  ResI . Thus, h[J + u](s), [J + u](o)i  PT J (J(p)) implies
that h[I + u ](s), [I + u ](o)i  PT J (I(p)). From Observation 4, the latter implies that
h[I + u ](s), [I + u ](o)i  PT I (I(p)).
Case 2) It holds: (i) s  Var (G ) and u(s)  res(VERDF ) and (ii) if o  Var (G ) then
u(o) 6 res(VERDF ).
Assume that u(s) = res(TotalClass), [J + u](o) = y, and J(p) = z. Then y, z  ResI .
Additionally, I(p) = J(p) = z and [I + u ](o) = [J + u](o) = y. Thus, h[I + u ](s), [I +
u ](o)i = hI(Class), yi. It holds hres(TotalClass), yi  PT J (z). Due to Observation 3, it
holds hI(Class), yi  PT I (z). Thus, h[I + u ](s), [I + u ](o)i = hI(Class), yi  PT I (z) =
PT I (I(p)).
Similarly, if u(s) = res(TotalProperty), we prove that h[I+u ](s), [I+u ](o)i  PT I (I(p)).
Case 3) It holds: o  Var (G ) and u(o)  res(VERDF ). Then, Observation 1, it
follows that s  Var (G ) and u(s) = u(o). Assume that u(o) = res(TotalClass), and
J(p) = z. Then, z  ResI and I(p) = J(p) = z. Additionally, h[I + u ](s), [I + u ](o)i =
hI(Class), I(Class)i. It holds hres(TotalClass), res(TotalClass)i  PT J (z). Due to Observation 3, it follows that hI(Class), I(v)i  PT I (z). Thus, h[I + u ](s), [I + u ](o)i =
hI(Class), I(Class)i  PT I (z) = PT I (I(p)).
Similarly, if u(o) = res(TotalProperty), we prove that h[I+u ](s), [I+u ](o)i  PT I (I(p)).
As in all cases, it holds h[I + u ](s), [I + u ](o)i = PT I (I(p)), it follows that I, u |= G ,
which implies that I |= G .
) Let G |=RDF S G . We will show that G |=ERDF G . Let I be an ERDF interpretation
of a vocabulary V , such that I |= G. Thus, there is u : Var (G)  ResI s.t. I, u |= G. We
will show that I |= G .
We define V  = V  VRDF  VRDF S  VERDF . Based on I, we construct an RDFS interpretation J of V  such that: ResJ = ResI , P ropJ = P ropI , LV J = LV I , ClsJ =
ClsI , JV (x) = IV (x), x  V   URI, PT J (x) = PT I (x), x  P ropJ , ILJ (x) =
ILI (x), x  V   T L, CT J (x) = CT I (x), x  ClsJ .
We will now show that J is indeed an RDFS interpretation of V  .
First, we will show that J satisfies semantic condition 1 of Definition A.3 (Appendix
A, RDF interpretation). It holds: x  P ropJ iff x  P ropI iff x  CT I (I(Property)) iff
hx, I(Property)i  PT I (I(type)) iff hx, J(Property)i  PT J (J(type)).
82

fiExtended RDF as a Semantic Foundation of Rule Markup Languages

We will now show that J satisfies semantic condition 2 of Definition A.3.
Let srdf :XMLLiteral  V such that s is a well-typed XML literal string. Then, it
follows from the definition of J and the fact that I is an ERDF interpretation of V
that ILJ (srdf :XMLLiteral ) is the XML value of s, and ILJ (srdf :XMLLiteral ) 
CT J (J(XMLLiteral )). We will show that ILJ (srdf :XMLLiteral )  LV J . Since I is
an ERDF interpretation, ILI (srdf :XMLLiteral )  CT I (I(XMLLiteral )). Additionally,
hI(XMLLiteral ), I(Literal)i  PT I (I(subClassOf )). Therefore, ILI (srdf :XMLLiteral ) 
CT I (I(Literal)), and thus, ILI (srdf :XMLLiteral )  LV I . The last statement implies
that ILJ (srdf :XMLLiteral )  LV J .
We will now show that J satisfies semantic condition 3 of Definition A.3.
Let srdf :XMLLiteral  V such that s is an ill-typed XML literal string. Then, it
follows from the definition of J and the fact that I is an ERDF interpretation of V that
ILJ (srdf :XMLLiteral )  ResJ  LV J . We will show that
hILJ (srdf :XMLLiteral ), J(XMLLiteral )i 6 PT J (J(type)). Assume that
hILJ (srdf :XMLLiteral ), J(XMLLiteral )i  PT J (J(type)). Then,
hILI (srdf :XMLLiteral ), I(XMLLiteral )i  PT I (I(type)). Thus,
ILI (srdf :XMLLiteral )  CT I (I(XMLLiteral )). Since it holds
hI(XMLLiteral ), I(Literal)i  PT I (I(subClassOf )), it follows that
ILI (srdf :XMLLiteral )  CT I (I(Literal)). Thus, ILI (srdf :XMLLiteral )  LV I ,
which is impossible since I is an ERDF interpretation of V . Therefore,
hILJ (srdf :XMLLiteral ), J(XMLLiteral )i 6 PT J (J(type)).
It is easy to see that J satisfies semantic condition 4 of Definition A.3 and all the
semantic conditions of Definition A.5 (Appendix A, RDFS Interpretation). Therefore, J is
an RDFS interpretation of V  .
We will now show that J, u |= G. Let p(s, o)  G. Since I |= G, it holds that p  V  ,
s, o  V   Var , and J(p) = I(p)  P ropI = P ropJ . It holds: h[J + u](s), [J + u](o)i 
PT J (J(p)) iff h[I + u](s)), [I + u](o)i  P ropI (I(p)), which is true, since I, u |= G. Thus,
J, u |= G, which implies that J |= G. Since G |=RDF S G , it follows that J |= G . Thus,
there is v : Var (G )  ResJ s.t. J, v |= G .
We will now show that I |= G . Let p(s, o)  G . Since J, v |= G , it holds that p  V  ,
s, o  V   Var , and I(p) = J(p)  P ropJ = P ropI . It holds: h[I + v](s), [I + v](o)i 
PT I (I(p)) iff h[J + v](s), [J + v](o)i  PT J (J(p)), which is true, since J, v |= G . Thus,
I, v |= G , which implies that I |= G . 
Proposition 4.1. Let G be an ERDF graph and let F be an ERDF formula such that
VF  skG (Var (G)) = . It holds: G |=ERDF F iff sk(G) |=ERDF F .
Proof:
) Let G |=ERDF F . We will show that sk(G) |=ERDF F . Let I be an ERDF interpretation
over a vocabulary V s.t. I |= sk(G). We will show that I |= G. We define V  = V  VRDF 
VRDF S  VERDF . Additionally, we define a total function u : Var (G)  ResI s.t. u(x) =
IV (skG (x)), x  Var (G). Moreover, we define a total function u : V   Var (G)  V  s.t.
u (x) = skG (x), if x  Var (G) and u (x) = x, otherwise.
Let p(s, o)  G. Then, p  V  , s, o  V  Var , and I(p)  P ropI . It holds: h[I+u](s), [I+
u](o)i  PT I (I(p)) iff hI(u (s)), I(u (o))i  PT I (I(p)), which is true, since p(u (s), u (o)) 
sk(G) and I |= sk(G). Thus, I, u |= p(s, o).
83

fiAnalyti, Antoniou, Damasio, & Wagner

Let p(s, o)  G. Then, p  V  , s, o  V   Var , and I(p)  P ropI . It holds:
h[I + u](s), [I + u](o)i  PF I (I(p)) iff hI(u (s)), I(u (o))i  PF I (I(p)), which is true, since
p(u (s), u (o))  sk(G) and I |= sk(G). Thus, I, u |= p(s, o).
Therefore, I |= G. Since G |=ERDF F , it follows that I |= F .
) Let sk(G) |=ERDF F . We will show that G |=ERDF F . Let I be an ERDF interpretation
of a vocabulary V such that I |= G. We will show that I |= F . Since I |= G, there is a total
function u : Var (G)  ResI s.t. I, u |= G. We define V  = V VRDF VRDF S VRDF S . We
construct an ERDF interpretation J of V skG (Var (G)) as follows: ResJ = ResI , P ropJ =
P ropI , LV J = LV I , ClsJ = ClsI . We define JV : (V   skG (Var (G)))  URI  ResJ ,
1
as follows: JV (x) = IV (x), x  V   URI and JV (x) = u(skG
(x)), x  skG (Var (G)).
Moreover, PT J (x) = PT I (x), x  P ropJ , PF J (x) = PF I (x), x  P ropJ , ILJ (x) =
ILI (x), x  V   T L, CT J (x) = CT I (x), x  ClsJ , and CF J (x) = CF I (x), x  ClsJ .
Since I is an ERDF interpretation of V , it is easy to see that J is indeed an ERDF interpretation of V  skG (Var (G)). We will show that J |= sk(G). First, we define a total func1
tion g : V   skG (Var (G))  V   Var (G) as follows: g(x) = skG
(x), x  skG (Var (G))
and g(x) = x, otherwise. Let p(s, o)  sk(G). Since I |= G, it follows that p  V  ,
s, o  V   Var , and J(p) = I(p)  P ropI = P ropJ . It holds J(s) = [I + u](g(s)),
J(o) = [I + u](g(o)), and J(p) = I(p). Therefore, it holds: hJ(s), J(o)i  PT J (J(p)) iff
h[I + u](g(s)), [I + u](g(o))i  PT I (I(p)), which holds since p(g(s), g(o))  G and I, u |= G.
Let v : {}  ResJ . It follows that J, v |= p(s, o). Let p(s, o)  sk(G). We can show that
J, v |= p(s, o), in a similar manner. Therefore, J |= sk(G).
Since sk(G) |=ERDF F , it follows that J |= F . We will show that I |= F . We define
V = V  VRDF  VRDF S  VERDF . Note that ResJ = ResI .


Lemma: For every mapping u : Var (F )  ResJ , it holds J, u |= F iff I, u |= F .
Proof: We will prove the Lemma by induction. Without loss of generality, we assume that
 appears only in front of positive ERDF triples. Otherwise we apply the transformation
rules of Definition 3.4, to get an equivalent formula that satisfies the assumption.
Let F = p(s, o). Assume that J, u |= F . Since VF  skG (Var (G)) = , it follows that
p  V  , s, o  V   Var , and J(p) = I(p)  P ropI = P ropJ . Since h[J + u](s), [J + u](o)i 
PT J (J(p)), it follows that h[I + u](s), [I + u](o)i  PT I (I(p)). Therefore, I, u |= F .
Assume that I, u |= F . It follows that p  V  , s, o  V  Var , and J(p) = I(p)  P ropI =
P ropJ . Since h[I + u](s), [I + u](o)i  PT I (I(p)), it follows that h[J + u](s), [J + u](o)i 
PT J (J(p)). Therefore, J, u |= F .
Let F = p(s, o). Similarly, we prove that J, u |= F iff I, u |= F .
Assumption: Assume that the lemma holds for the subformulas of F .
We will show that the lemma holds also for F .
Let F = G. It holds: I, u |= F iff VG  V  and I, u 6|= G iff VG  V  and J, u 6|= G iff
J, u |= F .
Let F = F1 F2 . It holds: I, u |= F iff I, u |= F1 and I, u |= F2 iff J, u |= F1 and
J, u |= F2 iff J, u |= F .
84

fiExtended RDF as a Semantic Foundation of Rule Markup Languages

Let F = x G. It holds: I, u |= F iff I, u |= x G iff there is v : Var (G)  ResI
s.t. v(y) = u(y), y  Var (G)  {x} and I, v |= G iff there is v : Var (G)  ResJ s.t.
v(y) = u(y), y  Var (G)  {x} and J, v |= G iff J, u |= x G iff J, u |= F .
Let F = F1 F2 or F = F1  F2 or F = xG. We can prove, similarly to the above
cases, that I, u |= F iff J, u |= F .
End of lemma
Since J |= F , it follows that for every mapping u : Var (F )  ResJ ,
J, u |= F .
Therefore, it follows from Lemma and the fact that ResJ = ResI that for every mapping
u : Var (F )  ResI , I, u |= F . Thus, I |= F . 
Proposition 4.2. Let O = hG, P i be an ERDF ontology and let I, J  I H (O). Let
p  TProp I  TProp J . If PT I (p) 6= PT J (p) or PF I (p) 6= PF J (p) then I 6 J and J 6 I.
Proof: Assume PT I (p) 6= PT J (p). Now, assume I  J. Then, PT I (p)  PT J (p)
and PF I (p)  PF J (p). Since I, J  I H (O) and p  TProp I  TProp J , it holds that
H
PF I (p) = ResH
O  PT I (p) and PF J (p) = ResO  PT J (p). Thus, PF I (p)  PF J (p), which
is a contradiction. Thus, I 6 J. Similarly, we can prove that J 6 I.
Assume now that PF I (p) 6= PF J (p). Then, we can prove that I 6 J and J 6 I, in a
similar manner. 
Proposition 5.1. Let O = hG, P i be an ERDF ontology and let M  Mst (O). It holds
M  MH (O).
Proof: Let M  Mst (O). Obviously, M  I H (O) and M |= sk(G). We will show that
M |= r, r  P . Let r  P . Let v be a mapping v : Var (r)  ResH
O s.t. M, v |= Cond(r).
It is enough to show that M, v |= Concl(r).
For any mapping u : X  ResH (O), where X  Var , we define the mapping u : X 
VO as follows:

u(x) if u(x) is not the xml value of a well-typed XML literal in VO
u (x) =
t
if u(x) is the xml value of a well-typed XML literal t in VO




Let x  VO , we define xu = x. Let x  X, we define xu = u (x). Let F  L(VO ) 

{true, f alse} such that FVar (F )  X, we define F u to be the formula that results from F

after replacing each free variable of F by u (x). It is easy to see that it holds: Concl(r)v 

Concl(r)v  [r]VO  [P ]VO .
Lemma: Let F be an ERDF formula over VO and let u be a mapping u : Var (F )  ResH
O.

It holds: M, u |= F iff M, u |= F u .
Proof: We prove the lemma by induction. Without loss of generality, we assume that 
appears only in front of positive ERDF triples. Otherwise we apply the transformation
rules of Definition 3.4, to get an equivalent formula that satisfies the assumption.
Let F = p(s, o). It holds: M, u |= F iff M, u |= p(s, o) iff h[M + u](s), [M + u](o)i 



PT M (M (p)) iff h[M + u](su ), [M + u](ou )i  PT M (M (p)) iff M, u |= p(s, o)u .
Let F = p(s, o). It holds: M, u |= F iff M, u |= p(s, o) iff h[M + u](s), [M + u](o)i 



PF M (M (p)) iff h[M + u](su ), [M + u](ou )i  PF M (M (p)) iff M, u |= (p(s, o))u .
Assumption: Assume that the lemma holds for the subformulas of F .
We will show that the lemma holds also for F .
85

fiAnalyti, Antoniou, Damasio, & Wagner



Let F = G. It holds: M, u |= F iff M, u |= G iff M, u 6|= G iff M, u 6|= Gu iff


M, u |= Gu iff M, u |= F u .
Let F = F1 F2 . It holds: M, u |= F iff M, u |= F1 F2 iff M, u |= F1 and M, u |= F2 iff




M, u |= F1u and M, u |= F2u iff M, u |= (F1 F2 )u iff M, u |= F u .
Let F = xG. It holds: M, u |= F iff there exists a mapping u1 : Var (G)  ResH
O s.t.
u1 (y) = u(y), y  Var (G)  {x} s.t. M, u1 |= G iff there exists a mapping u1 : Var (G) 
u1 iff there exists a mapping
ResH
O s.t. u1 (y) = u(y), y  Var (G)  {x} s.t. M, u1 |= G

u1 : Var (G)  ResH
u1 |= (xG)u1 iff (since
O s.t. u1 (y) = u(y), y  Var(G)  {x} s.t. M,

u1 (y) = u (y), y  FVar (xG)) M, u |= (xG)u iff M, u |= F u .
Let F = F1 F2 or F = F1  F2 or F = xG. We can prove, similarly to the above

cases, that M, u |= F iff M, u |= F u .
End of Lemma
First assume that Cond(r) 6= true. Then, Cond(r)  L(VO ) and thus, Cond(r) is an ERDF

formula over VO . Since M, v |= Cond(r), it follows from Lemma that M, v |= Cond(r)v .


Now since FVar (Cond(r)v ) = , it follows from Lemma B.1 that M |= Cond(r)v . Since

M  Mst (O), it follows that M |= Concl(r)v . Thus, Concl(r) 6= f alse and Concl(r) 

L(VO |{}). Now since FVar (Concl(r)v ) = , it follows from lemma B.1 that M, v |=

Concl(r)v . Since Concl(r) is an ERDF formula over VO , it follows from Lemma that
M, v |= Concl(r).

Assume now that Cond(r) = true. Then, M |= Cond(r)v . Since M  Mst (O), it

follows that M |= Concl(r)v . Therefore, Concl(r) 6= f alse, and we can prove as above
that M, v |= Concl(r).
Therefore, M |= r, r  P . 
Proposition 5.2. Let O = hG, P i be an ERDF ontology, such that
rdfs:subClassOf (rdf :Property, erdf :TotalProperty)  G. Then, Mst (O) = MH (O).
Proof: From Proposition 5.1, it follows that Mst (O)  MH (O). We will show that
MH (O)  Mst (O). Let M  MH (O). It follows that M |= sk(G). We will show that
M  minimal({I  I H (O) | I |= sk(G)}).
Let J  I H (O) s.t. J |= sk(G) and J  M . We will show that J = M . Since J  M ,
it follows that P ropJ  P ropM and for all p  P ropJ , it holds PT J (p)  PT M (p) and
PF J (p)  PF M (p). Let p  P ropJ . Since J |= sk(G), it follows that P ropJ  TProp J .
Thus, p  TProp J . Assume that PT J (p) 6= PT M (p). Then, there is hx, yi  PT M (p)
s.t. hx, yi 6 PT J (p). Then, hx, yi  PF J (p). Thus, hx, yi  PF M (p), which is impossible,
since hx, yi  PT M (p). Thus, PT J (p) = PT M (p). Similarly, we can prove that PF J (p) =
PF M (p). Therefore, for all p  P ropJ , it holds PT J (p) = PT M (p) and PF J (p) = PF M (p).
We will now show that P ropJ = P ropM . It holds P ropJ ={x  ResH
O | hx, Propertyi 
PT J (type)} = {x  ResH
|
hx,
Propertyi

PT
(type)}
=P
rop
.
Based
on these results,
I
M
O
the fact that J, M  I H (O), it follows that J = M . Therefore, M  minimal({I 
I H (O) | I |= sk(G)}).
We will now show that M  minimal({I  I H (O) | I  M and I |= Concl(r), for
all r  P[M,M ] }). Since M  MH (O) it follows that M  {I  I H (O) | I  M and
I |= Concl(r), for all r  P[M,M ] }. Let J  {I  I H (O) | I  M and I |= Concl(r),
for all r  P[M,M ] } and J  M . Since J  M , it follows that P ropM  P ropJ , and
for all p  P ropM , it holds PT M (p)  PT J (p) and PF M (p)  PF J (p). Since J  M ,
86

fiExtended RDF as a Semantic Foundation of Rule Markup Languages

it follows that P ropJ  P ropM , and for all p  P ropJ , it holds PT J (p)  PT M (p) and
PF J (p)  PF M (p). Therefore, it follows that P ropM = P ropJ , and for all p  P ropM ,
it holds PT M (p) = PT J (p) and PF M (p) = PF J (p). Based on this result, the fact that
J, M  I H (O), it follows that J = M .
Thus, M  minimal({I  I H (O) | I  M and I |= Concl(r), for all r  P[M,M ] }).
Since M satisfies the conditions of Definition 5.1 (Stable Model), it follows that M 
st
M (O). Thus, it holds MH (O)  Mst (O).
Therefore, MH (O) = Mst (O). 
Proposition 6.2. Let G be an ERDF graph and let F be an ERDF formula such that
VF  skG (Var (G)) = . It holds:
1. If F is an ERDF d-formula and hG, i |=st F then G |=ERDF F .
2. If G |=ERDF F then hG, i |=st F .
Proof:
1) Let hG, i |=st F . We will show that sk(G) |=ERDF F . Let I be an ERDF interpretation
of a vocabulary V s.t. I |= sk(G). We will show that I |= F . We define V  = V  VRDF 
VRDF S  VERDF .
Let O = hG, i. Based on I, we construct a partial interpretation J of VO as follows:
 ResJ = ResH
O.
 JV (x) = x, for all x  VO  URI.
 We define the mapping: ILJ : VO  T L  ResJ such that:
ILJ (x) = x, if x is a typed literal in VO other than a well-typed XML literal, and
ILI (x) is the XML value of x, if x is a well-typed XML literal in VO .
 We define the mapping: J : VO  ResJ such that:
 J(x) = JV (x), x  VO  URI.
 J(x) = x,  x  VO  PL.
 J(x) = ILJ (x),  x  VO  T L.
 P ropJ = {x  ResJ | x  VO , J(x ) = x and I(x )  P ropI }.
 The mapping PT J : P ropJ  P(ResJ  ResJ ) is defined as follows:
x, y, z  VO , it holds:
hJ(x), J(y)i  PT J (J(z)) iff hI(x), I(y)i  PT I (I(z)).
 We define the mapping PF J : P ropJ  P(ResJ  ResJ ) as follows:
x, y, z  VO , it holds:
hJ(x), J(y)i  PF J (J(z)) iff hI(x), I(y)i  PF I (I(z)).
 LV J = {x  ResJ | hx, J(Literal)i  PT J (J(type))}.
87

fiAnalyti, Antoniou, Damasio, & Wagner

To show that J is a partial interpretation, it is enough to show that VO  PL  LV J .
Let x  VO  PL. Then, x  LV I . Thus, hx, I(Literal)i  PT I (I(type)). This implies that
hx, J(Literal)i  PT J (J(type)). Thus, x  LV J .
Now, we extend J with the ontological categories:
ClsJ = {x  ResJ | hx, J(Class)i  PT J (J(type))},
TCls J = {x  ResJ | hx, J(TotalClass)i  PT J (J(type))}, and
TProp J = {x  ResJ | hx, J(TotalProperty)i  PT J (J(type))}.
We define the mappings CT J , CF J : ClsJ  P(ResJ ) as follows:
x  CT J (y) iff hx, yi  PT J (J(type)), and
x  CF J (y) iff hx, yi  PF J (J(type)).
We will now show that J is an ERDF interpretation of VO . First, we will show that
J satisfies semantic condition 2 of Definition 3.7 (ERDF Interpretation), in a number of
steps:
Step 1: Here, we prove that ResJ = CT J (J(Resource)). Obviously, CT J (J(Resource))
 ResJ . We will show that ResJ  CT J (J(Resource)). Let x  ResJ . Then, there is
x  VO such that J(x ) = x. We want to show that hJ(x ), J(Resource)i  PT J (J(type)).
It holds: hJ(x ), J(Resource)i  PT J (J(type)) iff hI(x ), I(Resource)i  PT I (I(type)),
which is true, since I is an ERDF interpretation that satisfies sk(G) and I(x )  ResI .
Thus, x = J(x )  CT J (J(ResourceResource)).
Therefore, ResJ = CT J (J(Resource)).
Step 2: Here, we prove that P ropJ = CT J (J(Property)). We will show that P ropJ 
CT J (J(Property)). Let x  P ropJ . Then, there is x  VO such that J(x ) = x and
I(x )  P ropI . We want to show that hJ(x ), J(Property)i  PT J (J(type)). It holds:
hJ(x ), J(Property)i  PT J (J(type)) iff hI(x ), I(Property)i  PT I (I(type)),
which is true, since I(x )  P ropI . Thus, x = J(x )  CT J (J(Property)).
Therefore, P ropJ  CT J (J(Property)).
We will now show that CT J (J(Property))  P ropJ . Let x  CT J (J(Property)). Then,
x  VO such that J(x ) = x. It holds hJ(x ), J(Property)i  PT J (J(type)), which implies
that hI(x ), I(Property)i  PT I (I(type)). Thus, I(x )  P ropI and x  P ropJ .
Therefore, CT J (J(Property))  P ropJ .
Step 3: By definition, it holds ClsJ = CT J (J(Class)), LV J = CT J (J(Literal)), TCls J =
CT J (J(TotalClass)) and TProp J = CT J (J(TotalProperty)).
We will now show that J satisfies semantic condition 3 of Definition 3.7 (ERDF Interpretation). Let hx, yi  PT J (J(domain)) and hz, wi  PT J (x). We will show
that z  CT J (y). There are x , y   VO such that J(x ) = x, J(y  ) = y. Thus,
hJ(x ), J(y  )i  PT J (J(domain)). Additionally, there are z  , w  VO such that J(z  ) =
z, J(w ) = w. Thus, hJ(z  ), J(w )i  PT J (J(x )). Then, hI(x ), I(y  )i  PT I (I(domain))
and hI(z  ), I(w )i  PT I (I(x )). Since I is an ERDF interpretation, hI(z  ), I(y  )i 
PT I (I(type)). Thus, hJ(z  ), J(y  )i  PT J (J(type)) and z  CT J (y).
In a similar manner, we can prove that J also satisfies the rest of the semantic conditions
of Definition 3.7. Thus, J is an ERDF interpretation of VO .
Moreover, we will show that J is a coherent ERDF interpretation (Definition 3.2).
Assume that this is not the case. Thus, there is z  P ropJ s.t. PT J (z)  PF J (z) 6= .
Thus, there are x, y  ResJ s.t. hx, yi  PT J (z)  PF J (z), for such a z. Then, there are
88

fiExtended RDF as a Semantic Foundation of Rule Markup Languages

x , y  , z   VO s.t. J(x ) = x, J(y  ) = y, and J(z  ) = z. It holds: hJ(x ), J(y  )i  PT J (J(z  ))
and hJ(x ), J(y  )i  PF J (J(z  )). Thus, hI(x ), I(y  )i  PT I (I(z  )) and hI(x ), I(y  )i 
PF I (I(z  )). But this is impossible, since I is a (coherent) ERDF interpretation. Therefore,
J is also a coherent ERDF interpretation.
Thus, J  I H (O).
We will now show that J |= sk(G). Let p(s, o)  sk(G). It holds p, s, o  VO . Since
I |= sk(G), it holds I(p)  P ropI . Thus, hI(p), I(Property)i  PT I (I(type)), which implies
that hJ(p), J(Property)i  PT J (J(type)). From this, it follows that J(p)  P ropJ . It
holds: hJ(s), J(o)i  PT J (J(p)) iff hI(s), I(o)i  PT I (I(p)). The last statement is true
since I |= sk(G). Let u : {}  ResH
O . Then, J, u |= p(s, o). Let p(s, o)  sk(G). We can
show that J, u |= p(s, o), in a similar manner. Thus, J |= sk(G).
Now, from Definition 5.1 (Stable Model) and the fact that J |= sk(G), it follows that
K  Mst (O) s.t. K  J. From this and the fact that O |=st F , it follows that K |= F .
Since F is an ERDF d-formula, it holds that
F = (?x1 , ..., ?xk1 F1 )  ...  (?x1 , ..., ?xkn Fn ),
where Fi = t1  ...  tmi and tj , for j = 1, ..., mi , is an ERDF triple. Thus, there is an
i  {1, ..., n} and u : Var (Fi )  ResH
O s.t. K, u |= Fi .
We will show that J, u |= Fi .
Let p(s, o)  {t1 , ..., tmi }. Since K is an ERDF interpretation of VO , K, u |= Fi , and
P ropK  P ropJ , it follows that p  VO , s, o  VO  Var , and J(p) = K(p)  P ropK 
P ropJ . Additionally, h[K +u](s), [K +u](o)i  PT K (p). Since h[J +u](s), [J +u](o)i = h[K +
u](s), [K + u](o)i and PT K (p)  PT J (p), it follows that h[J + u](s), [J + u](o)i  PT J (p).
Thus, J, u |= p(s, o).
Let p(s, o)  {t1 , ..., tmi }. Since K is an ERDF interpretation of VO , K, u |= Fi , and
P ropK  P ropJ , it follows that p  VO , s, o  VO  Var , and J(p) = K(p)  P ropK 
P ropJ . Additionally, h[K +u](s), [K +u](o)i  PF K (p). Since h[J +u](s), [J +u](o)i = h[K +
u](s), [K + u](o)i and PF K (p)  PF J (p), it follows that h[J + u](s), [J + u](o)i  PF J (p).
Thus, J, u |= p(s, o).
We now define a total function u : VFi  Var (Fi )  VO , as follows:

u(x) if x  Var (Fi ) and




u(x) is not the xml value of a well-typed XML literal in VO

t
if x  Var (Fi ) and
u (x) =


u(x) is the xml value of a well-typed XML literal t in VO



x
otherwise

Moreover, we define a total function u : Var (Fi )  ResI s.t. u (x) = I(u (x)).
We will show that I, u |= Fi .
Let p(s, o)  {t1 , ..., tmi }. Then, p  VFi and s, o  VFi  Var . Since J, u |= Fi , it follows
that VFi  VO . Therefore, VFi  Vsk(G)  VRDF  VRDF S  VERDF  V  . Thus, p  V  and
s, o  V   Var .
We will now show that I(p)  P ropI . It holds:
hI(p), I(Property)i  PT I (I(type)) iff
hJ(p), J(Property)i  PT J (J(type)), which holds since J, u |= Fi .
89

fiAnalyti, Antoniou, Damasio, & Wagner

We want to show that h[I +v  ](s), [I +v  ](o)i  PT I (I(p)). Note that x  VFi , it holds:
[I + u ](x) = I(u (x)) = I(x) and J(u (x)) = [J + u](x) = J(x). Moreover, x  Var (Fi ),
it holds: [I + u ](x) = I(u (x)) and J(u (x)) = [J + u](x) (recall the definition of J(.)).
Therefore, it holds:
h[I + u ](s), [I + u ](o)i  PT I (I(p)) iff
hI(u (s)), I(u (o))i  PT I (I(p)) iff
hJ(u (s)), J(u (o))i  PT J (J(p)) iff
h[J + u](s), [J + u](o)i  PT J (J(p)), which is true since J, u |= Fi . Thus, I, u |= p(s, o).
Let p(s, o)  {t1 , ..., tmi }. We can show that I, u |= p(s, o), in a similar manner.
Thus, I, u |= Fi , which implies that I, u |=  ?x1 , ..., ?xki Fi . Thus, I, u |= F . Now, it
follows from Lemma B.1 that I |= F .
Thus, sk(G) |=ERDF F . Now, it follows from Proposition 4.1 that G |=ERDF F .
2) Let G |=ERDF F . It follows from Proposition 4.1 that sk(G) |=ERDF F . We will show
that hG, i |=st F . In particular, let O = hG, i and let I  Mst (O). Note that I is an
ERDF interpretation of VO , such that I |= sk(G). Since sk(G) |=ERDF F , it follows that
I |= F . 
Proposition 8.1 Let D be an instance of the unbounded tiling problem. It holds:
1. D has a solution iff OD  {false  FD } has a stable model.
2. D has a solution iff OD 6|=st FD .
Proof:
1) This statement follows easily from statement 2).
2) ) Let  be a solution to D. Since IN  IN is denumerable, there exists a bijective
function  : IN  IN  IN . Consider now a Herbrand interpretation I of OD such that:
1. CTI (Tile) = CTI (HasRight) = CTI (HasAbove) = {rdf : i | i  IN } and
CFI (T ile) = CFI (HasRight) = CFI (HasAbove) = .
2. P TI (id ) = {hx, xi | x  VO } and P FI (id ) = .
3. P TI (HConstraint) = H and P FI (HConstraint) = .
4. P TI (VConstraint) = V and P FI (VConstraint) = .
5. P TI (Type) = {hrdf : (i, j),  (i, j)i | i, j  IN } and P FI (Type) = .
6. P TI (right) = {hrdf : (i, j), rdf : (i + 1, j)i | i, j  IN } and
P FI (right) = {hrdf : i, rdf : ji | i, j  IN and hrdf : i, rdf : ji 6 P TI (right)}.
7. P TI (above) = {hrdf : (i, j), rdf : (i, j + 1)i | i, j  IN } and
P FI (above) = {hrdf : i, rdf : ji | i, j  IN and hrdf : i, rdf : ji 6 P TI (above)}.

It is easy to see that I is a stable model of OD and I 6|= FD . Thus, OD 6|=st FD .
) Let D = hT , H, V i, where T = {T1 , ..., Tn }. Assume that OD 6|=st FD and let I be a
stable model of OD = hG, P i such that I 6|= FD . Obviously, CTI (Tile) = {rdf : i | i  IN }.
Due to rule sets (2)-(4) of P and since OD 6|=st FD , it holds that starting from tile rdf : 0
90

fiExtended RDF as a Semantic Foundation of Rule Markup Languages

and placing tiles according to P TI (right) and P TI (above) relations, a grid is formed. We
define (i, j) = k, for i, j, k  IN , iff the tile rdf : k has been placed on the hi, ji position
of the previous grid. Note that  is a total function. Due to rule set (1) of P , each tile is
assigned a unique type in T = {T1 , ..., Tn }. Due to rule set (5) of P , this type assignment
satisfies the horizontal and vertical adjacency constraints of D. Thus, a solution of D is
 : IN  IN  T , where  (i, j) = T iff hrdf : (i, j), T i  P TI (T ype). Since  is a total
function and, for all k  IN , tile rdf : k is assigned a unique type in T , it follows that  is a
total function.

References
Alferes, J. J., Damasio, C. V., & Pereira, L. M. (1995). A Logic Programming System
for Non-monotonic Reasoning. Special Issue of the Journal of Automated Reasoning,
14 (1), 93147.
Alferes, J. J., Damasio, C. V., & Pereira, L. M. (2003). Semantic Web Logic Programming Tools. In International Workshop on Principles and Practice of Semantic Web
Reasoning (PPSWR03), pp. 1632.
Analyti, A., Antoniou, G., Damasio, C. V., & Wagner, G. (2004). Negation and Negative
Information in the W3C Resource Description Framework. Annals of Mathematics,
Computing & Teleinformatics (AMCT), 1 (2), 2534.
Analyti, A., Antoniou, G., Damasio, C. V., & Wagner, G. (2005). Stable Model Theory for
Extended RDF Ontologies. In 4th International Semantic Web Conference (ISWC2005), pp. 2136.
Antoniou, G., Bikakis, A., & Wagner, G. (2004). A System for Nonmonotonic Rules on the
Web. In 3rd International Workshop on Rules and Rule Markup Languages for the
Semantic Web (RULEML03), pp. 2336.
Antoniou, G., Billington, D., Governatori, G., & Maher, M. J. (2001). Representation
Results for Defeasible Logic. ACM Transactions on Computational Logic (TOCL),
2 (2), 255287.
Baader, F., Calvanese, D., McGuinness, D. L., Nardi, D., & Patel-Schneider, P. F. (Eds.).
(2003). The Description Logic Handbook: Theory, Implementation, and Applications.
Cambridge University Press.
Bassiliades, N., Antoniou, G., & Vlahavas, I. P. (2004). DR-DEVICE: A Defeasible Logic
System for the Semantic Web. In 2nd International Workshop on Principles and
Practice of Semantic Web Reasoning (PPSWR04), pp. 134148.
Beckett, D. (2004). RDF/XML Syntax Specification (Revised). W3C Recommendation.
Available at http://www.w3.org/TR/2004/REC-rdf-syntax-grammar-20040210/.
Berger, R. (1966). The Undecidability of the Dominoe Problem. Memoirs of the American
Mathematical Society, 66, 172.
Berners-Lee, T. (1998). Design Issues - Architectual and Philosophical Points. Personal
notes. Available at http://www.w3.org/DesignIssues.
91

fiAnalyti, Antoniou, Damasio, & Wagner

Berners-Lee, T., Connolly, D., Kagal, L., Scharf, Y., & Hendler, J. (2008). N3Logic: A
Logical Framework For the World Wide Web. to be published by Theory and Practice
of Logic Programming (TPLP), Special Issue on Logic Programming and the Web.
Bry, F., & Marchiori, M. (2005). Ten Theses on Logic Languages for the Semantic Web. In
3rd International Workshop on Principles and Practice of Semantic Web Reasoning
(PPSWR-2005), pp. 4249.
Damasio, C. V., Analyti, A., Antoniou, G., & Wagner, G. (2006). Supporting Open and
Closed World Reasoning on the Web. In 4th Workshop on Principles and Practice of
Semantic Web Reasoning (PPSWR-2006), pp. 149163.
de Bruijn, J., Franconi, E., & Tessaris, S. (2005). Logical Reconstruction of Normative RDF.
In OWL: Experiences and Directions Workshop (OWLED-2005), Galway, Ireland.
Donini, F. M., Lenzerini, M., Nardi, D., & Schaerf, A. (1998). AL-log: Integrating Datalog
and Description Logics. Journal of Intelligent Information Systems, 10 (3), 227252.
Donini, F. M., Nardi, D., & Rosati, R. (2002). Description Logics of Minimal Knowledge
and Negation as Failure. ACM Transactions on Computational Logic, 3 (2), 177225.
Eiter, T., Lukasiewicz, T., Schindlauer, R., & Tompits, H. (2004a). Combining Answer Set
Programming with Description Logics for the Semantic Web. In 9th International
Conference on Principles of Knowledge Representation and Reasoning (KR04), pp.
141151.
Eiter, T., Lukasiewicz, T., Schindlauer, R., & Tompits, H. (2004b). Well-Founded Semantics
for Description Logic Programs in the Semantic Web. In 3rd International Workshop
on Rules and Rule Markup Languages for the Semantic Web (RuleML04), pp. 8197.
Eiter, T., Ianni, G., Polleres, A., & Schindlauer, R. (2006). Answer Set Programming for the
Semantic Web. Tutorial co-located with the 3d European Semantic Web Conference
(ESWC-2006).
Gelder, A. V., Ross, K. A., & Schlipf, J. S. (1991). The Well-Founded Semantics for General
Logic Programs. Journal of the ACM, 38 (3), 620650.
Gelfond, M., & Lifschitz, V. (1988). The Stable Model Semantics for Logic Programming.
In Kowalski, R., & Bowen, K. A. (Eds.), 5th International Conference on Logic Programming, pp. 10701080. MIT Press.
Gelfond, M., & Lifschitz, V. (1990). Logic programs with Classical Negation. In Warren,
& Szeredi (Eds.), 7th International Conference on Logic Programming, pp. 579597.
MIT Press.
Gelfond, M., & Lifschitz, V. (1991). Classical Negation in Logic programs and Disjunctive
Databases. New Generation Computing, 9, 365385.
Hayes, P. (2004). RDF Semantics. W3C Recommendation. Available at http://www.w3.
org/TR/2004/REC-rdf-mt-20040210/.
Herre, H., Jaspars, J., & Wagner, G. (1999). Partial Logics with Two Kinds of Negation
as a Foundation of Knowledge-Based Reasoning. In Gabbay, D. M., & Wansing, H.
(Eds.), What Is Negation? Kluwer Academic Publishers.
92

fiExtended RDF as a Semantic Foundation of Rule Markup Languages

Herre, H., & Wagner, G. (1997). Stable Models are Generated by a Stable Chain. Journal
of Logic Programming, 30 (2), 165177.
Horrocks, I., & Patel-Schneider, P. F. (2003). Reducing OWL Entailment to Description
Logic Satisfiability. In 2nd International Semantic Web Conference (ISWC-2003),
pp. 1729.
Horrocks, I., & Patel-Schneider, P. F. (2004). A Proposal for an OWL Rules Language. In
13th International Conference on World Wide Web (WWW04), pp. 723731. ACM
Press.
Horrocks, I., Patel-Schneider, P. F., Boley, H., Tabet, S., Grosof, B., & Dean, M.
(2004). SWRL: A semantic web rule language combining OWL and RuleML.
W3C Member Submission. Available at http://www.w3.org/Submission/2004/
SUBM-SWRL-20040521/.
Kifer, M., Lausen, G., & Wu, J. (1995). Logical Foundations of Object-Oriented and FrameBased Languages. Journal of the ACM, 42 (4), 741843.
Klyne, G., & Carroll, J. J. (2004). Resource Description Framework (RDF): Concepts
and Abstract Syntax. W3C Recommendation. Available at http://www.w3.org/TR/
2004/REC-rdf-concepts-20040210/.
Levy, A. Y., & Rousset, M. (1998). Combining Horn Rules and Description Logics in
CARIN. Artificial Intelligence, 104 (1-2), 165209.
Lloyd, J. W., & Topor, R. W. (1984). Making Prolog more Expressive. Journal of Logic
Programming, 1 (3), 225240.
Maher, M. J. (2002). A Model-Theoretic Semantics for Defeasible Logic. In ICLP 2002
Workshop on Paraconsistent Computational Logic (PCL-2002), pp. 255287.
McGuinness, D. L., & van Harmelen, F. (2004).
OWL Web Ontology Language
Overview. W3C Recommendation. Available at http://www.w3.org/TR/2004/
REC-owl-features-20040210/.
Motik, B., Sattler, U., & Studer, R. (2004). Query Answering for OWL-DL with Rules. In
3rd International Semantic Web Conference (ISWC-2004), pp. 549563.
Patel-Schneider, P. F., Hayes, P., & Horrocks, I. (2004). OWL Web Ontology Language
Semantics and Abstract Syntax. W3C Recommendation. Available at http://www.
w3.org/TR/2004/REC-owl-semantics-20040210/.
Pereira, L. M., & Alferes, J. J. (1992). Well-Founded Semantics for Logic Programs with
Explicit Negation. In Neumann, B. (Ed.), European Conference on Artificial Intelligence, pp. 102106. John Wiley & Sons.
Prudhommeaux, E., & Seaborne, A. (2008). SPARQL Query Language for RDF. W3C
Recommendation. Available at http://www.w3.org/TR/rdf-sparql-query/.
Rao, P., Sagonas, K. F., Swift, T., Warren, D. S., & Freire, J. (1997). XSB: A System for
Efficiently Computing WFS. In Proceedings of 4th International Conference on Logic
Programming and Nonmonotonic Reasoning (LPNMR97), pp. 10701080.
93

fiAnalyti, Antoniou, Damasio, & Wagner

Rosati, R. (1999). Towards Expressive KR Systems Integrating Datalog and Description
Logics: Preliminary Report. In Proc. of the 1999 Description Logic Workshop (DL99),
pp. 160164.
Rosati, R. (2005). On the Decidability and Complexity of Integrating Ontologies and Rules.
Journal of Web Semantics, 3, 6173.
Schaffert, S., Bry, F., Besnard, P., Decker, H., Decker, S., Enguix, C. F., & Herzig, A.
(2005). Paraconsistent Reasoning for the Semantic Web. In Workshop on Uncertainty
Reasoning for the Semantic Web, co-located with ISWC-2005, pp. 104105.
Sintek, M., & Decker, S. (2002). TRIPLE - A Query, Inference, and Transformation Language for the Semantic Web. In 1st International Semantic Web Conference (ISWC2002), pp. 364378. Springer-Verlag.
ter Horst, H. J. (2004). Extending the RDFS Entailment Lemma. In 3rd International
Semantic Web Conference (ISWC-2004), pp. 7791.
ter Horst, H. J. (2005a). Combining RDF and Part of OWL with Rules: Semantics, Decidability, Complexity. In 4th International Semantic Web Conference (ISWC-2005),
pp. 668684.
ter Horst, H. J. (2005b). Completeness, Decidability and Complexity of Entailment for
RDF Schema and a Semantic Extension Involving the OWL Vocabulary. Journal of
Web Semantics, 3 (2-3), 79115.
Wagner, G. (1991). A Database Needs Two Kinds of Negation. In 3rd Symposium on
Mathematical Fundamentals of Database and Knowledge Base Systems (MFDBS91),
pp. 357371. Springer-Verlag.
Wagner, G. (2003). Web Rules Need Two Kinds of Negation. In 1st International Workshop on Principles and Practice of Semantic Web Reasoning (PPSWR03), pp. 3350.
Springer-Verlag.
Wagner, G., Giurca, A., & Lukichev, S. (2005). A General Markup Framework for Integrity
and Derivation Rules. In Dagstuhl Seminar Proceedings: Principles and Practices of
Semantic Web Reasoning.
Wagner, G., Giurca, A., & Lukichev, S. (2006). A Usable Interchange Format for Rich
Syntax Rules Integrating OCL, RuleML and SWRL. In Workshop on Reasoning on
the Web (RoW-2006), co-located with WWW-2006).
Yang, G., & Kifer, M. (2003a). Inheritance and Rules in Object-Oriented Semantic Web
Languages. In 2nd International Workshop on Rules and Rule Markup Languages for
the Semantic Web (RULEML03), pp. 95110.
Yang, G., & Kifer, M. (2003b). Reasoning about Anonymous Resources and Meta Statements on the Semantic Web. Journal on Data Semantics, 1, 6997.
Yang, G., Kifer, M., & Zhao, C. (2003). Flora-2: A Rule-Based Knowledge Representation
and Inference Infrastructure for the Semantic Web. In 2nd International Conference
on Ontologies, DataBases, and Applications of Semantics for Large Scale Information
Systems (ODBASE03), pp. 671688.

94

fi