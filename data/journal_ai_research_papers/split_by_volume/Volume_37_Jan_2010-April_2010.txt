Journal of Artificial Intelligence Research 37 (2010) 141-188

Submitted 10/09; published 02/10

From Frequency to Meaning:
Vector Space Models of Semantics
Peter D. Turney

peter.turney@nrc-cnrc.gc.ca

National Research Council Canada
Ottawa, Ontario, Canada, K1A 0R6

Patrick Pantel

me@patrickpantel.com

Yahoo! Labs
Sunnyvale, CA, 94089, USA

Abstract
Computers understand very little of the meaning of human language. This profoundly
limits our ability to give instructions to computers, the ability of computers to explain
their actions to us, and the ability of computers to analyse and process text. Vector space
models (VSMs) of semantics are beginning to address these limits. This paper surveys the
use of VSMs for semantic processing of text. We organize the literature on VSMs according
to the structure of the matrix in a VSM. There are currently three broad classes of VSMs,
based on termdocument, wordcontext, and pairpattern matrices, yielding three classes
of applications. We survey a broad range of applications in these three categories and we
take a detailed look at a specific open source project in each category. Our goal in this
survey is to show the breadth of applications of VSMs for semantics, to provide a new
perspective on VSMs for those who are already familiar with the area, and to provide
pointers into the literature for those who are less familiar with the field.

1. Introduction
One of the biggest obstacles to making full use of the power of computers is that they
currently understand very little of the meaning of human language. Recent progress in
search engine technology is only scratching the surface of human language, and yet the
impact on society and the economy is already immense. This hints at the transformative
impact that deeper semantic technologies will have. Vector space models (VSMs), surveyed
in this paper, are likely to be a part of these new semantic technologies.
In this paper, we use the term semantics in a general sense, as the meaning of a word, a
phrase, a sentence, or any text in human language, and the study of such meaning. We are
not concerned with narrower senses of semantics, such as the semantic web or approaches to
semantics based on formal logic. We present a survey of VSMs and their relation with the
distributional hypothesis as an approach to representing some aspects of natural language
semantics.
The VSM was developed for the SMART information retrieval system (Salton, 1971)
by Gerard Salton and his colleagues (Salton, Wong, & Yang, 1975). SMART pioneered
many of the concepts that are used in modern search engines (Manning, Raghavan, &
Schutze, 2008). The idea of the VSM is to represent each document in a collection as a
point in a space (a vector in a vector space). Points that are close together in this space
are semantically similar and points that are far apart are semantically distant. The users
c
2010
AI Access Foundation and National Research Council Canada. Reprinted with permission.

fiTurney & Pantel

query is represented as a point in the same space as the documents (the query is a pseudodocument). The documents are sorted in order of increasing distance (decreasing semantic
similarity) from the query and then presented to the user.
The success of the VSM for information retrieval has inspired researchers to extend the
VSM to other semantic tasks in natural language processing, with impressive results. For
instance, Rapp (2003) used a vector-based representation of word meaning to achieve a
score of 92.5% on multiple-choice synonym questions from the Test of English as a Foreign
Language (TOEFL), whereas the average human score was 64.5%.1 Turney (2006) used a
vector-based representation of semantic relations to attain a score of 56% on multiple-choice
analogy questions from the SAT college entrance test, compared to an average human score
of 57%.2
In this survey, we have organized past work with VSMs according to the type of matrix
involved: termdocument, wordcontext, and pairpattern. We believe that the choice of
a particular matrix type is more fundamental than other choices, such as the particular
linguistic processing or mathematical processing. Although these three matrix types cover
most of the work, there is no reason to believe that these three types exhaust the possibilities.
We expect future work will introduce new types of matrices and higher-order tensors.3
1.1 Motivation for Vector Space Models of Semantics
VSMs have several attractive properties. VSMs extract knowledge automatically from a
given corpus, thus they require much less labour than other approaches to semantics, such
as hand-coded knowledge bases and ontologies. For example, the main resource used in
Rapps (2003) VSM system for measuring word similarity is the British National Corpus
(BNC),4 whereas the main resource used in non-VSM systems for measuring word similarity
(Hirst & St-Onge, 1998; Leacock & Chodrow, 1998; Jarmasz & Szpakowicz, 2003) is a
lexicon, such as WordNet5 or Rogets Thesaurus. Gathering a corpus for a new language
is generally much easier than building a lexicon, and building a lexicon often involves also
gathering a corpus, such as SemCor for WordNet (Miller, Leacock, Tengi, & Bunker, 1993).
VSMs perform well on tasks that involve measuring the similarity of meaning between
words, phrases, and documents. Most search engines use VSMs to measure the similarity
between a query and a document (Manning et al., 2008). The leading algorithms for measuring semantic relatedness use VSMs (Pantel & Lin, 2002a; Rapp, 2003; Turney, Littman,
Bigham, & Shnayder, 2003). The leading algorithms for measuring the similarity of semantic relations also use VSMs (Lin & Pantel, 2001; Turney, 2006; Nakov & Hearst, 2008).
(Section 2.4 discusses the differences between these types of similarity.)
We find VSMs especially interesting due to their relation with the distributional hypothesis and related hypotheses (see Section 2.7). The distributional hypothesis is that
1. Regarding the average score of 64.5% on the TOEFL questions, Landauer and Dumais (1997) note
that, Although we do not know how such a performance would compare, for example, with U.S. school
children of a particular age, we have been told that the average score is adequate for admission to many
universities.
2. This is the average score for highschool students in their senior year, applying to US universities. For
more discussion of this score, see Section 6.3 in Turneys (2006) paper.
3. A vector is a first-order tensor and a matrix is a second-order tensor. See Section 2.5.
4. See http://www.natcorp.ox.ac.uk/.
5. See http://wordnet.princeton.edu/.

142

fiFrom Frequency to Meaning

words that occur in similar contexts tend to have similar meanings (Wittgenstein, 1953;
Harris, 1954; Weaver, 1955; Firth, 1957; Deerwester, Dumais, Landauer, Furnas, & Harshman, 1990). Efforts to apply this abstract hypothesis to concrete algorithms for measuring
the similarity of meaning often lead to vectors, matrices, and higher-order tensors. This
intimate connection between the distributional hypothesis and VSMs is a strong motivation
for taking a close look at VSMs.
Not all uses of vectors and matrices count as vector space models. For the purposes of
this survey, we take it as a defining property of VSMs that the values of the elements in a
VSM must be derived from event frequencies, such as the number of times that a given word
appears in a given context (see Section 2.6). For example, often a lexicon or a knowledge
base may be viewed as a graph, and a graph may be represented using an adjacency matrix,
but this does not imply that a lexicon is a VSM, because, in general, the values of the
elements in an adjacency matrix are not derived from event frequencies. This emphasis
on event frequencies brings unity to the variety of VSMs and explicitly connects them to
the distributional hypothesis; furthermore, it avoids triviality by excluding many possible
matrix representations.
1.2 Vectors in AI and Cognitive Science
Vectors are common in AI and cognitive science; they were common before the VSM was
introduced by Salton et al. (1975). The novelty of the VSM was to use frequencies in a
corpus of text as a clue for discovering semantic information.
In machine learning, a typical problem is to learn to classify or cluster a set of items
(i.e., examples, cases, individuals, entities) represented as feature vectors (Mitchell, 1997;
Witten & Frank, 2005). In general, the features are not derived from event frequencies,
although this is possible (see Section 4.6). For example, a machine learning algorithm can
be applied to classifying or clustering documents (Sebastiani, 2002).
Collaborative filtering and recommender systems also use vectors (Resnick, Iacovou,
Suchak, Bergstrom, & Riedl, 1994; Breese, Heckerman, & Kadie, 1998; Linden, Smith, &
York, 2003). In a typical recommender system, we have a person-item matrix, in which
the rows correspond to people (customers, consumers), the columns correspond to items
(products, purchases), and the value of an element is the rating (poor, fair, excellent) that
the person has given to the item. Many of the mathematical techniques that work well
with termdocument matrices (see Section 4) also work well with person-item matrices, but
ratings are not derived from event frequencies.
In cognitive science, prototype theory often makes use of vectors. The basic idea of
prototype theory is that some members of a category are more central than others (Rosch
& Lloyd, 1978; Lakoff, 1987). For example, robin is a central (prototypical) member of
the category bird, whereas penguin is more peripheral. Concepts have varying degrees of
membership in categories (graded categorization). A natural way to formalize this is to
represent concepts as vectors and categories as sets of vectors (Nosofsky, 1986; Smith, Osherson, Rips, & Keane, 1988). However, these vectors are usually based on numerical scores
that are elicited by questioning human subjects; they are not based on event frequencies.
Another area of psychology that makes extensive use of vectors is psychometrics, which
studies the measurement of psychological abilities and traits. The usual instrument for
143

fiTurney & Pantel

measurement is a test or questionnaire, such as a personality test. The results of a test
are typically represented as a subject-item matrix, in which the rows represent the subjects
(people) in an experiment and the columns represent the items (questions) in the test
(questionnaire). The value of an element in the matrix is the answer that the corresponding
subject gave for the corresponding item. Many techniques for vector analysis, such as factor
analysis (Spearman, 1904), were pioneered in psychometrics.
In cognitive science, Latent Semantic Analysis (LSA) (Deerwester et al., 1990; Landauer & Dumais, 1997), Hyperspace Analogue to Language (HAL) (Lund, Burgess, &
Atchley, 1995; Lund & Burgess, 1996), and related research (Landauer, McNamara, Dennis, & Kintsch, 2007) is entirely within the scope of VSMs, as defined above, since this
research uses vector space models in which the values of the elements are derived from
event frequencies, such as the number of times that a given word appears in a given context. Cognitive scientists have argued that there are empirical and theoretical reasons for
believing that VSMs, such as LSA and HAL, are plausible models of some aspects of human cognition (Landauer et al., 2007). In AI, computational linguistics, and information
retrieval, such plausibility is not essential, but it may be seen as a sign that VSMs are a
promising area for further research.
1.3 Motivation for This Survey
This paper is a survey of vector space models of semantics. There is currently no comprehensive, up-to-date survey of this field. As we show in the survey, vector space models
are a highly successful approach to semantics, with a wide range of potential and actual
applications. There has been much recent growth in research in this area.
This paper should be of interest to all AI researchers who work with natural language,
especially researchers who are interested in semantics. The survey will serve as a general
introduction to this area and it will provide a framework  a unified perspective  for
organizing the diverse literature on the topic. It should encourage new research in the area,
by pointing out open problems and areas for further exploration.
This survey makes the following contributions:
New framework: We provide a new framework for organizing the literature: term
document, wordcontext, and pairpattern matrices (see Section 2). This framework shows
the importance of the structure of the matrix (the choice of rows and columns) in determining the potential applications and may inspire researchers to explore new structures
(different kinds of rows and columns, or higher-order tensors instead of matrices).
New developments: We draw attention to pairpattern matrices. The use of pair
pattern matrices is relatively new and deserves more study. These matrices address some
criticisms that have been directed at wordcontext matrices, regarding lack of sensitivity
to word order.
Breadth of approaches and applications: There is no existing survey that shows
the breadth of potential and actual applications of VSMs for semantics. Existing summaries
omit pairpattern matrices (Landauer et al., 2007).
Focus on NLP and CL: Our focus in this survey is on systems that perform practical
tasks in natural language processing and computational linguistics. Existing overviews focus
on cognitive psychology (Landauer et al., 2007).
144

fiFrom Frequency to Meaning

Success stories: We draw attention to the fact that VSMs are arguably the most
successful approach to semantics, so far.
1.4 Intended Readership
Our goal in writing this paper has been to survey the state of the art in vector space models
of semantics, to introduce the topic to those who are new to the area, and to give a new
perspective to those who are already familiar with the area.
We assume our reader has a basic understanding of vectors, matrices, and linear algebra,
such as one might acquire from an introductory undergraduate course in linear algebra, or
from a text book (Golub & Van Loan, 1996). The basic concepts of vectors and matrices
are more important here than the mathematical details. Widdows (2004) gives a gentle
introduction to vectors from the perspective of semantics.
We also assume our reader has some familiarity with computational linguistics or information retrieval. Manning et al. (2008) provide a good introduction to information retrieval.
For computational linguistics, we recommend Manning and Schutzes (1999) text.
If our reader is familiar with linear algebra and computational linguistics, this survey
should present no barriers to understanding. Beyond this background, it is not necessary
to be familiar with VSMs as they are used in information retrieval, natural language processing, and computational linguistics. However, if the reader would like to do some further
background reading, we recommend Landauer et al.s (2007) collection.
1.5 Highlights and Outline
This article is structured as follows. Section 2 explains our framework for organizing the
literature on VSMs according to the type of matrix involved: termdocument, wordcontext,
and pairpattern. In this section, we present an overview of VSMs, without getting into
the details of how a matrix can be generated from a corpus of raw text.
After the high-level framework is in place, Sections 3 and 4 examine the steps involved
in generating a matrix. Section 3 discusses linguistic processing and Section 4 reviews
mathematical processing. This is the order in which a corpus would be processed in most
VSM systems (first linguistic processing, then mathematical processing).
When VSMs are used for semantics, the input to the model is usually plain text. Some
VSMs work directly with the raw text, but most first apply some linguistic processing to the
text, such as stemming, part-of-speech tagging, word sense tagging, or parsing. Section 3
looks at some of these linguistic tools for semantic VSMs.
In a simple VSM, such as a simple termdocument VSM, the value of an element in a
document vector is the number of times that the corresponding word occurs in the given
document, but most VSMs apply some mathematical processing to the raw frequency values.
Section 4 presents the main mathematical operations: weighting the elements, smoothing
the matrix, and comparing the vectors. This section also describes optimization strategies
for comparing the vectors, such as distributed sparse matrix multiplication and randomized
techniques.
By the end of Section 4, the reader will have a general view of the concepts involved in
vector space models of semantics. We then take a detailed look at three VSM systems in
Section 5. As a representative of termdocument VSMs, we present the Lucene information
145

fiTurney & Pantel

retrieval library.6 For wordcontext VSMs, we explore the Semantic Vectors package, which
builds on Lucene.7 As the representative of pairpattern VSMs, we review the Latent
Relational Analysis module in the S-Space package, which also builds on Lucene.8 The
source code for all three of these systems is available under open source licensing.
We turn to a broad survey of applications for semantic VSMs in Section 6. This section also serves as a short historical view of research with semantic VSMs, beginning with
information retrieval in Section 6.1. Our purpose here is to give the reader an idea of the
breadth of applications for VSMs and also to provide pointers into the literature, if the
reader wishes to examine any of these applications in detail.
In a termdocument matrix, rows correspond to terms and columns correspond to documents (Section 6.1). A document provides a context for understanding the term. If we
generalize the idea of documents to chunks of text of arbitrary size (phrases, sentences,
paragraphs, chapters, books, collections), the result is the wordcontext matrix, which includes the termdocument matrix as a special case. Section 6.2 discusses applications for
wordcontext matrices. Section 6.3 considers pairpattern matrices, in which the rows correspond to pairs of terms and the columns correspond to the patterns in which the pairs
occur.
In Section 7, we discuss alternatives to VSMs for semantics. Section 8 considers the
future of VSMs, raising some questions about their power and their limitations. We conclude
in Section 9.

2. Vector Space Models of Semantics
The theme that unites the various forms of VSMs that we discuss in this paper can be
stated as the statistical semantics hypothesis: statistical patterns of human word usage can
be used to figure out what people mean.9 This general hypothesis underlies several more
specific hypotheses, such as the bag of words hypothesis, the distributional hypothesis, the
extended distributional hypothesis, and the latent relation hypothesis, discussed below.
2.1 Similarity of Documents: The TermDocument Matrix
In this paper, we use the following notational conventions: Matrices are denoted by bold
capital letters, A. Vectors are denoted by bold lowercase letters, b. Scalars are represented
by lowercase italic letters, c.
If we have a large collection of documents, and hence a large number of document
vectors, it is convenient to organize the vectors into a matrix. The row vectors of the matrix
correspond to terms (usually terms are words, but we will discuss some other possibilities)
6.
7.
8.
9.

See http://lucene.apache.org/java/docs/.
See http://code.google.com/p/semanticvectors/.
See http://code.google.com/p/airhead-research/wiki/LatentRelationalAnalysis.
This phrase was taken from the Faculty Profile of George Furnas at the University of Michigan,
http://www.si.umich.edu/people/faculty-detail.htm?sid=41. The full quote is, Statistical Semantics
 Studies of how the statistical patterns of human word usage can be used to figure out what people
mean, at least to a level sufficient for information access. The term statistical semantics appeared in
the work of Furnas, Landauer, Gomez, and Dumais (1983), but it was not defined there.

146

fiFrom Frequency to Meaning

and the column vectors correspond to documents (web pages, for example). This kind of
matrix is called a termdocument matrix.
In mathematics, a bag (also called a multiset) is like a set, except that duplicates are
allowed. For example, {a, a, b, c, c, c} is a bag containing a, b, and c. Order does not matter
in bags and sets; the bags {a, a, b, c, c, c} and {c, a, c, b, a, c} are equivalent. We can represent
the bag {a, a, b, c, c, c} with the vector x = h2, 1, 3i, by stipulating that the first element of
x is the frequency of a in the bag, the second element is the frequency of b in the bag, and
the third element is the frequency of c. A set of bags can be represented as a matrix X, in
which each column x:j corresponds to a bag, each row xi: corresponds to a unique member,
and an element xij is the frequency of the i-th member in the j-th bag.
In a termdocument matrix, a document vector represents the corresponding document
as a bag of words. In information retrieval, the bag of words hypothesis is that we can
estimate the relevance of documents to a query by representing the documents and the
query as bags of words. That is, the frequencies of words in a document tend to indicate
the relevance of the document to a query. The bag of words hypothesis is the basis for
applying the VSM to information retrieval (Salton et al., 1975). The hypothesis expresses
the belief that a column vector in a termdocument matrix captures (to some degree) an
aspect of the meaning of the corresponding document; what the document is about.
Let X be a termdocument matrix. Suppose our document collection contains n documents and m unique terms. The matrix X will then have m rows (one row for each unique
term in the vocabulary) and n columns (one column for each document). Let wi be the i-th
term in the vocabulary and let dj be the j-th document in the collection. The i-th row in
X is the row vector xi: and the j-th column in X is the column vector x:j . The row vector
xi: contains n elements, one element for each document, and the column vector x:j contains
m elements, one element for each term. Suppose X is a simple matrix of frequencies. The
element xij in X is the frequency of the i-th term wi in the j-th document dj .
In general, the value of most of the elements in X will be zero (the matrix is sparse),
since most documents will use only a small fraction of the whole vocabulary. If we randomly
choose a term wi and a document dj , its likely that wi does not occur anywhere in dj , and
therefore xij equals 0.
The pattern of numbers in xi: is a kind of signature of the i-th term wi ; likewise, the
pattern of numbers in x:j is a signature of the j-th document dj . That is, the pattern of
numbers tells us, to some degree, what the term or document is about.
The vector x:j may seem to be a rather crude representation of the document dj . It tells
us how frequently the words appear in the document, but the sequential order of the words
is lost. The vector does not attempt to capture the structure in the phrases, sentences,
paragraphs, and chapters of the document. However, in spite of this crudeness, search
engines work surprisingly well; vectors seem to capture an important aspect of semantics.
The VSM of Salton et al. (1975) was arguably the first practical, useful algorithm for
extracting semantic information from word usage. An intuitive justification for the term
document matrix is that the topic of a document will probabilistically influence the authors
choice of words when writing the document.10 If two documents have similar topics, then
the two corresponding column vectors will tend to have similar patterns of numbers.
10. Newer generative models, such as Latent Dirichlet Allocation (LDA) (Blei, Ng, & Jordan, 2003), directly
model this intuition. See Sections 4.3 and 7.

147

fiTurney & Pantel

2.2 Similarity of Words: The WordContext Matrix
Salton et al. (1975) focused on measuring document similarity, treating a query to a search
engine as a pseudo-document. The relevance of a document to a query is given by the
similarity of their vectors. Deerwester et al. (1990) observed that we can shift the focus to
measuring word similarity, instead of document similarity, by looking at row vectors in the
termdocument matrix, instead of column vectors.
Deerwester et al. (1990) were inspired by the termdocument matrix of Salton et al. (1975),
but a document is not necessarily the optimal length of text for measuring word similarity.
In general, we may have a wordcontext matrix, in which the context is given by words,
phrases, sentences, paragraphs, chapters, documents, or more exotic possibilities, such as
sequences of characters or patterns.
The distributional hypothesis in linguistics is that words that occur in similar contexts
tend to have similar meanings (Harris, 1954). This hypothesis is the justification for applying the VSM to measuring word similarity. A word may be represented by a vector
in which the elements are derived from the occurrences of the word in various contexts,
such as windows of words (Lund & Burgess, 1996), grammatical dependencies (Lin, 1998;
Pado & Lapata, 2007), and richer contexts consisting of dependency links and selectional
preferences on the argument positions (Erk & Pado, 2008); see Sahlgrens (2006) thesis for
a comprehensive study of various contexts. Similar row vectors in the wordcontext matrix
indicate similar word meanings.
The idea that word usage can reveal semantics was implicit in some of the things that
Wittgenstein (1953) said about language-games and family resemblance. Wittgenstein was
primarily interested in the physical activities that form the context of word usage (e.g., the
word brick, spoken in the context of the physical activity of building a house), but the main
context for a word is often other words.11
Weaver (1955) argued that word sense disambiguation for machine translation should
be based on the co-occurrence frequency of the context words near a given target word (the
word that we want to disambiguate). Firth (1957, p. 11) said, You shall know a word
by the company it keeps. Deerwester et al. (1990) showed how the intuitions of Wittgenstein (1953), Harris (1954), Weaver, and Firth could be used in a practical algorithm.
2.3 Similarity of Relations: The PairPattern Matrix
In a pairpattern matrix, row vectors correspond to pairs of words, such as mason : stone
and carpenter : wood, and column vectors correspond to the patterns in which the pairs cooccur, such as X cuts Y  and X works with Y . Lin and Pantel (2001) introduced the
pairpattern matrix for the purpose of measuring the semantic similarity of patterns; that
is, the similarity of column vectors. Given a pattern such as X solves Y , their algorithm
was able to find similar patterns, such as Y is solved by X, Y is resolved in X, and
X resolves Y .
Lin and Pantel (2001) proposed the extended distributional hypothesis, that patterns
that co-occur with similar pairs tend to have similar meanings. The patterns X solves Y 
11. Wittgensteins intuition might be better captured by a matrix that combines words with other modalities,
such as images (Monay & Gatica-Perez, 2003). If the values of the elements are derived from event
frequencies, we would include this as a VSM approach to semantics.

148

fiFrom Frequency to Meaning

and Y is solved by X tend to co-occur with similar X : Y pairs, which suggests that these
patterns have similar meanings. Pattern similarity can be used to infer that one sentence
is a paraphrase of another (Lin & Pantel, 2001).
Turney et al. (2003) introduced the use of the pairpattern matrix for measuring the
semantic similarity of relations between word pairs; that is, the similarity of row vectors.
For example, the pairs mason : stone, carpenter : wood, potter : clay, and glassblower : glass
share the semantic relation artisan : material. In each case, the first member of the pair is
an artisan who makes artifacts from the material that is the second member of the pair.
The pairs tend to co-occur in similar patterns, such as the X used the Y to and the X
shaped the Y into.
The latent relation hypothesis is that pairs of words that co-occur in similar patterns
tend to have similar semantic relations (Turney, 2008a). Word pairs with similar row
vectors in a pairpattern matrix tend to have similar semantic relations. This is the inverse
of the extended distributional hypothesis, that patterns with similar column vectors in the
pairpattern matrix tend to have similar meanings.
2.4 Similarities
Pairpattern matrices are suited to measuring the similarity of semantic relations between
pairs of words; that is, relational similarity. In contrast, wordcontext matrices are suited
to measuring attributional similarity. The distinction between attributional and relational
similarity has been explored in depth by Gentner (1983).
The attributional similarity between two words a and b, sima (a, b)  <, depends on the
degree of correspondence between the properties of a and b. The more correspondence there
is, the greater their attributional similarity. The relational similarity between two pairs of
words a : b and c : d, simr (a : b, c : d)  <, depends on the degree of correspondence between
the relations of a : b and c : d. The more correspondence there is, the greater their relational
similarity. For example, dog and wolf have a relatively high degree of attributional similarity, whereas dog : bark and cat : meow have a relatively high degree of relational similarity
(Turney, 2006).
It is tempting to suppose that relational similarity can be reduced to attributional
similarity. For example, mason and carpenter are similar words and stone and wood are
similar words; therefore, perhaps it follows that mason : stone and carpenter : wood have
similar relations. Perhaps simr (a : b, c : d) can be reduced to sima (a, c) + sima (b, d). However,
mason, carpenter, potter, and glassblower are similar words (they are all artisans), as are
wood, clay, stone, and glass (they are all materials used by artisans), but we cannot infer
from this that mason : glass and carpenter : clay have similar relations. Turney (2006, 2008a)
presented experimental evidence that relational similarity does not reduce to attributional
similarity.
The term semantic relatedness in computational linguistics (Budanitsky & Hirst, 2001)
corresponds to attributional similarity in cognitive science (Gentner, 1983). Two words
are semantically related if they have any kind of semantic relation (Budanitsky & Hirst,
2001); they are semantically related to the degree that they share attributes (Turney, 2006).
Examples are synonyms (bank and trust company), meronyms (car and wheel), antonyms
(hot and cold), and words that are functionally related or frequently associated (pencil and
149

fiTurney & Pantel

paper). We might not usually think that antonyms are similar, but antonyms have a high
degree of attributional similarity (hot and cold are kinds of temperature, black and white
are kinds of colour, loud and quiet are kinds of sound). We prefer the term attributional
similarity to the term semantic relatedness, because attributional similarity emphasizes the
contrast with relational similarity, whereas semantic relatedness could be confused with
relational similarity.
In computational linguistics, the term semantic similarity is applied to words that share
a hypernym (car and bicycle are semantically similar, because they share the hypernym
vehicle) (Resnik, 1995). Semantic similarity is a specific type of attributional similarity. We
prefer the term taxonomical similarity to the term semantic similarity, because the term
semantic similarity is misleading. Intuitively, both attributional and relational similarity
involve meaning, so both deserve to be called semantic similarity.
Words are semantically associated if they tend to co-occur frequently (e.g., bee and
honey) (Chiarello, Burgess, Richards, & Pollock, 1990). Words may be taxonomically similar and semantically associated (doctor and nurse), taxonomically similar but not semantically associated (horse and platypus), semantically associated but not taxonomically similar
(cradle and baby), or neither semantically associated nor taxonomically similar (calculus and
candy).
Schutze and Pedersen (1993) defined two ways that words can be distributed in a corpus of text: If two words tend to be neighbours of each other, then they are syntagmatic
associates. If two words have similar neighbours, then they are paradigmatic parallels. Syntagmatic associates are often different parts of speech, whereas paradigmatic parallels are
usually the same part of speech. Syntagmatic associates tend to be semantically associated (bee and honey are often neighbours); paradigmatic parallels tend to be taxonomically
similar (doctor and nurse have similar neighbours).
2.5 Other Semantic VSMs
The possibilities are not exhausted by termdocument, wordcontext, and pairpattern
matrices. We might want to consider triplepattern matrices, for measuring the semantic
similarity between word triples. Whereas a pairpattern matrix might have a row mason :
stone and a column X works with Y , a triplepattern matrix could have a row mason :
stone : masonry and a column X uses Y to build Z. However, n-tuples of words grow
increasingly rare as n increases. For example, phrases that contain mason, stone, and
masonry together are less frequent than phrases that contain mason and stone together. A
triplepattern matrix will be much more sparse than a pairpattern matrix (ceteris paribus).
The quantity of text that we need, in order to have enough numbers to make our matrices
useful, grows rapidly as n increases. It may be better to break n-tuples into pairs. For
example, a : b : c could be decomposed into a : b, a : c, and b : c (Turney, 2008a). The similarity
of two triples, a : b : c and d : e : f , could be estimated by the similarity of their corresponding
pairs. A relatively dense pairpattern matrix could serve as a surrogate for a relatively
sparse triplepattern matrix.
We may also go beyond matrices. The generalization of a matrix is a tensor (Kolda
& Bader, 2009; Acar & Yener, 2009). A scalar (a single number) is zeroth-order tensor, a
vector is first-order tensor, and a matrix is a second-order tensor. A tensor of order three or
150

fiFrom Frequency to Meaning

higher is called a higher-order tensor. Chew, Bader, Kolda, and Abdelali (2007) use a term
documentlanguage third-order tensor for multilingual information retrieval. Turney (2007)
uses a wordwordpattern tensor to measure similarity of words. Van de Cruys (2009) uses
a verbsubjectobject tensor to learn selectional preferences of verbs.
In Turneys (2007) tensor, for example, rows correspond to words from the TOEFL
multiple-choice synonym questions, columns correspond to words from Basic English (Ogden, 1930),12 and tubes correspond to patterns that join rows and columns (hence we have
a wordwordpattern third-order tensor). A given word from the TOEFL questions is represented by the corresponding wordpattern matrix slice in the tensor. The elements in
this slice correspond to all the patterns that relate the given TOEFL word to any word
in Basic English. The similarity of two TOEFL words is calculated by comparing the two
corresponding matrix slices. The algorithm achieves 83.8% on the TOEFL questions.
2.6 Types and Tokens
A token is a single instance of a symbol, whereas a type is a general class of tokens (Manning
et al., 2008). Consider the following example (from Samuel Beckett):

Ever tried. Ever failed.
No matter. Try again.
Fail again. Fail better.

There are two tokens of the type Ever, two tokens of the type again, and two tokens of
the type Fail. Lets say that each line in this example is a document, so we have three
documents of two sentences each. We can represent this example with a tokendocument
matrix or a typedocument matrix. The tokendocument matrix has twelve rows, one for
each token, and three columns, one for each line (Figure 1). The typedocument matrix
has nine rows, one for each type, and three columns (Figure 2).
A row vector for a token has binary values: an element is 1 if the given token appears in
the given document and 0 otherwise. A row vector for a type has integer values: an element
is the frequency of the given type in the given document. These vectors are related, in that
a type vector is the sum of the corresponding token vectors. For example, the row vector
for the type Ever is the sum of the two token vectors for the two tokens of Ever.
In applications dealing with polysemy, one approach uses vectors that represent word
tokens (Schutze, 1998; Agirre & Edmonds, 2006) and another uses vectors that represent
word types (Pantel & Lin, 2002a). Typical word sense disambiguation (WSD) algorithms
deal with word tokens (instances of words in specific contexts) rather than word types.
We mention both approaches to polysemy in Section 6, due to their similarity and close
relationship, although a defining characteristic of the VSM is that it is concerned with
frequencies (see Section 1.1), and frequency is a property of types, not tokens.
12. Basic English is a highly reduced subset of English, designed to be easy for people to learn. The words
of Basic English are listed at http://ogden.basic-english.org/.

151

fiTurney & Pantel

Ever tried.
Ever failed.
Ever
tried
Ever
failed
No
matter
Try
again
Fail
again
Fail
better

No matter.
Try again.

1
1
1
1
0
0
0
0
0
0
0
0

0
0
0
0
1
1
1
1
0
0
0
0

Fail again.
Fail better.
0
0
0
0
0
0
0
0
1
1
1
1

Figure 1: The tokendocument matrix. Rows are tokens and columns are documents.

Ever tried.
Ever failed.
Ever
tried
failed
No
matter
Try
again
Fail
better

No matter.
Try again.

2
1
1
0
0
0
0
0
0

0
0
0
1
1
1
1
0
0

Fail again.
Fail better.
0
0
0
0
0
0
1
2
1

Figure 2: The typedocument matrix. Rows are types and columns are documents.

152

fiFrom Frequency to Meaning

2.7 Hypotheses
We have mentioned five hypotheses in this section. Here we repeat these hypotheses and
then interpret them in terms of vectors. For each hypothesis, we cite work that explicitly
states something like the hypothesis or implicitly assumes something like the hypothesis.
Statistical semantics hypothesis: Statistical patterns of human word usage can be
used to figure out what people mean (Weaver, 1955; Furnas et al., 1983).  If units of text
have similar vectors in a text frequency matrix,13 then they tend to have similar meanings.
(We take this to be a general hypothesis that subsumes the four more specific hypotheses
that follow.)
Bag of words hypothesis: The frequencies of words in a document tend to indicate
the relevance of the document to a query (Salton et al., 1975).  If documents and pseudodocuments (queries) have similar column vectors in a termdocument matrix, then they
tend to have similar meanings.
Distributional hypothesis: Words that occur in similar contexts tend to have similar
meanings (Harris, 1954; Firth, 1957; Deerwester et al., 1990).  If words have similar row
vectors in a wordcontext matrix, then they tend to have similar meanings.
Extended distributional hypothesis: Patterns that co-occur with similar pairs tend
to have similar meanings (Lin & Pantel, 2001).  If patterns have similar column vectors
in a pairpattern matrix, then they tend to express similar semantic relations.
Latent relation hypothesis: Pairs of words that co-occur in similar patterns tend
to have similar semantic relations (Turney et al., 2003).  If word pairs have similar row
vectors in a pairpattern matrix, then they tend to have similar semantic relations.
We have not yet explained what it means to say that vectors are similar. We discuss
this in Section 4.4.

3. Linguistic Processing for Vector Space Models
We will assume that our raw data is a large corpus of natural language text. Before we
generate a termdocument, wordcontext, or pairpattern matrix, it can be useful to apply
some linguistic processing to the raw text. The types of processing that are used can be
grouped into three classes. First, we need to tokenize the raw text; that is, we need to decide
what constitutes a term and how to extract terms from raw text. Second, we may want to
normalize the raw text, to convert superficially different strings of characters to the same
form (e.g., car, Car, cars, and Cars could all be normalized to car). Third, we may want
to annotate the raw text, to mark identical strings of characters as being different (e.g., fly
as a verb could be annotated as fly/VB and fly as a noun could be annotated as fly/NN).
Grefenstette (1994) presents a good study of linguistic processing for wordcontext
VSMs. He uses a similar three-step decomposition of linguistic processing: tokenization,
surface syntactic analysis, and syntactic attribute extraction.
13. By text frequency matrix, we mean any matrix or higher-order tensor in which the values of the elements
are derived from the frequencies of pieces of text in the context of other pieces of text in some collection
of text. A text frequency matrix is intended to be a general structure, which includes termdocument,
wordcontext, and pairpattern matrices as special cases.

153

fiTurney & Pantel

3.1 Tokenization
Tokenization of English seems simple at first glance: words are separated by spaces. This
assumption is approximately true for English, and it may work sufficiently well for a basic
VSM, but a more advanced VSM requires a more sophisticated approach to tokenization.
An accurate English tokenizer must know how to handle punctuation (e.g., dont, Janes,
and/or), hyphenation (e.g., state-of-the-art versus state of the art), and recognize multi-word
terms (e.g., Barack Obama and ice hockey) (Manning et al., 2008). We may also wish to
ignore stop words, high-frequency words with relatively low information content, such as
function words (e.g., of, the, and) and pronouns (e.g., them, who, that). A popular list of
stop words is the set of 571 common words included in the source code for the SMART
system (Salton, 1971).14
In some languages (e.g., Chinese), words are not separated by spaces. A basic VSM
can break the text into character unigrams or bigrams. A more sophisticated approach
is to match the input text against entries in a lexicon, but the matching often does not
determine a unique tokenization (Sproat & Emerson, 2003). Furthermore, native speakers
often disagree about the correct segmentation. Highly accurate tokenization is a challenging
task for most human languages.
3.2 Normalization
The motivation for normalization is the observation that many different strings of characters often convey essentially identical meanings. Given that we want to get at the meaning
that underlies the words, it seems reasonable to normalize superficial variations by converting them to the same form. The most common types of normalization are case folding
(converting all words to lower case) and stemming (reducing inflected words to their stem
or root form).
Case folding is easy in English, but can be problematic in some languages. In French,
accents are optional for uppercase, and it may be difficult to restore missing accents when
converting the words to lowercase. Some words cannot be distinguished without accents; for
example, PECHE could be either peche (meaning fishing or peach) or peche (meaning sin).
Even in English, case folding can cause problems, because case sometimes has semantic
significance. For example, SMART is an information retrieval system, whereas smart is a
common adjective; Bush may be a surname, whereas bush is a kind of plant.
Morphology is the study of the internal structure of words. Often a word is composed
of a stem (root) with added affixes (inflections), such as plural forms and past tenses (e.g.,
trapped is composed of the stem trap and the affix -ed). Stemming, a kind of morphological
analysis, is the process of reducing inflected words to their stems. In English, affixes are
simpler and more regular than in many other languages, and stemming algorithms based
on heuristics (rules of thumb) work relatively well (Lovins, 1968; Porter, 1980; Minnen,
Carroll, & Pearce, 2001). In an agglutinative language (e.g., Inuktitut), many concepts are
combined into a single word, using various prefixes, infixes, and suffixes, and morphological
analysis is complicated. A single word in an agglutinative language may correspond to a
sentence of half a dozen words in English (Johnson & Martin, 2003).
14. The source code is available at ftp://ftp.cs.cornell.edu/pub/smart/.

154

fiFrom Frequency to Meaning

The performance of an information retrieval system is often measured by precision and
recall (Manning et al., 2008). The precision of a system is an estimate of the conditional
probability that a document is truly relevant to a query, if the system says it is relevant.
The recall of a system is an estimate of the conditional probability that the system will say
that a document is relevant to a query, if it truly is relevant.
In general, normalization increases recall and reduces precision (Kraaij & Pohlmann,
1996). This is natural, given the nature of normalization. When we remove superficial
variations that we believe are irrelevant to meaning, we make it easier to recognize similarities; we find more similar things, and so recall increases. But sometimes these superficial
variations have semantic significance; ignoring the variations causes errors, and so precision
decreases. Normalization can also have a positive effect on precision in cases where variant
tokens are infrequent and smoothing the variations gives more reliable statistics.
If we have a small corpus, we may not be able to afford to be overly selective, and it may
be best to aggressively normalize the text, to increase recall. If we have a very large corpus,
precision may be more important, and we might not want any normalization. Hull (1996)
gives a good analysis of normalization for information retrieval.
3.3 Annotation
Annotation is the inverse of normalization. Just as different strings of characters may have
the same meaning, it also happens that identical strings of characters may have different
meanings, depending on the context. Common forms of annotation include part-of-speech
tagging (marking words according to their parts of speech), word sense tagging (marking
ambiguous words according to their intended meanings), and parsing (analyzing the grammatical structure of sentences and marking the words in the sentences according to their
grammatical roles) (Manning & Schutze, 1999).
Since annotation is the inverse of normalization, we expect it to decrease recall and
increase precision. For example, by tagging program as a noun or a verb, we may be
able to selectively search for documents that are about the act of computer programming
(verb) instead of documents that discuss particular computer programs (noun); hence we
can increase precision. However, a document about computer programs (noun) may have
something useful to say about the act of computer programming (verb), even if the document
never uses the verb form of program; hence we may decrease recall.
Large gains in IR performance have recently been reported as a result of query annotation with syntactic and semantic information. Syntactic annotation includes query
segmentation (Tan & Peng, 2008) and part of speech tagging (Barr, Jones, & Regelson,
2008). Examples of semantic annotation are disambiguating abbreviations in queries (Wei,
Peng, & Dumoulin, 2008) and finding query keyword associations (Lavrenko & Croft, 2001;
Cao, Nie, & Bai, 2005).
Annotation is also useful for measuring the semantic similarity of words and concepts
(wordcontext matrices). For example, Pantel and Lin (2002a) presented an algorithm
that can discover word senses by clustering row vectors in a wordcontext matrix, using
contextual information derived from parsing.
155

fiTurney & Pantel

4. Mathematical Processing for Vector Space Models
After the text has been tokenized and (optionally) normalized and annotated, the first step
is to generate a matrix of frequencies. Second, we may want to adjust the weights of the
elements in the matrix, because common words will have high frequencies, yet they are less
informative than rare words. Third, we may want to smooth the matrix, to reduce the
amount of random noise and to fill in some of the zero elements in a sparse matrix. Fourth,
there are many different ways to measure the similarity of two vectors.
Lowe (2001) gives a good summary of mathematical processing for wordcontext VSMs.
He decomposes VSM construction into a similar four-step process: calculate the frequencies,
transform the raw frequency counts, smooth the space (dimensionality reduction), then
calculate the similarities.
4.1 Building the Frequency Matrix
An element in a frequency matrix corresponds to an event: a certain item (term, word,
word pair) occurred in a certain situation (document, context, pattern) a certain number
of times (frequency). At an abstract level, building a frequency matrix is a simple matter
of counting events. In practice, it can be complicated when the corpus is large.
A typical approach to building a frequency matrix involves two steps. First, scan sequentially through the corpus, recording events and their frequencies in a hash table, a
database, or a search engine index. Second, use the resulting data structure to generate the
frequency matrix, with a sparse matrix representation (Gilbert, Moler, & Schreiber, 1992).
4.2 Weighting the Elements
The idea of weighting is to give more weight to surprising events and less weight to expected
events. The hypothesis is that surprising events, if shared by two vectors, are more discriminative of the similarity between the vectors than less surprising events. For example,
in measuring the semantic similarity between the words mouse and rat, the contexts dissect
and exterminate are more discriminative of their similarity than the contexts have and like.
In information theory, a surprising event has higher information content than an expected
event (Shannon, 1948). The most popular way to formalize this idea for termdocument
matrices is the tf-idf (term frequency  inverse document frequency) family of weighting
functions (Sparck Jones, 1972). An element gets a high weight when the corresponding term
is frequent in the corresponding document (i.e., tf is high), but the term is rare in other
documents in the corpus (i.e., df is low, and thus idf is high). Salton and Buckley (1988)
defined a large family of tf-idf weighting functions and evaluated them on information retrieval tasks, demonstrating that tf-idf weighting can yield significant improvements over
raw frequency.
Another kind of weighting, often combined with tf-idf weighting, is length normalization
(Singhal, Salton, Mitra, & Buckley, 1996). In information retrieval, if document length
is ignored, search engines tend to have a bias in favour of longer documents. Length
normalization corrects for this bias.
Term weighting may also be used to correct for correlated terms. For example, the
terms hostage and hostages tend to be correlated, yet we may not want to normalize them
156

fiFrom Frequency to Meaning

to the same term (as in Section 3.2), because they have slightly different meanings. As
an alternative to normalizing them, we may reduce their weights when they co-occur in a
document (Church, 1995).
Feature selection may be viewed as a form of weighting, in which some terms get a
weight of zero and hence can be removed from the matrix. Forman (2003) provides a good
study of feature selection methods for text classification.
An alternative to tf-idf is Pointwise Mutual Information (PMI) (Church & Hanks, 1989;
Turney, 2001), which works well for both wordcontext matrices (Pantel & Lin, 2002a)
and termdocument matrices (Pantel & Lin, 2002b). A variation of PMI is Positive PMI
(PPMI), in which all PMI values that are less than zero are replaced with zero (Niwa &
Nitta, 1994). Bullinaria and Levy (2007) demonstrated that PPMI performs better than a
wide variety of other weighting approaches when measuring semantic similarity with word
context matrices. Turney (2008a) applied PPMI to pairpattern matrices. We will give the
formal definition of PPMI here, as an example of an effective weighting function.
Let F be a wordcontext frequency matrix with nr rows and nc columns. The i-th row
in F is the row vector fi: and the j-th column in F is the column vector f:j . The row fi:
corresponds to a word wi and the column f:j corresponds to a context cj . The value of the
element fij is the number of times that wi occurs in the context cj . Let X be the matrix
that results when PPMI is applied to F. The new matrix X has the same number of rows
and columns as the raw frequency matrix F. The value of an element xij in X is defined
as follows:
fij
pij = Pnr Pnc

j=1 fij

i=1

(1)

Pnc

j=1 fij
pi = Pnr Pnc

(2)

Pnr
f
Pncij
= Pnr i=1

(3)

i=1

pj

i=1

j=1 fij
j=1 fij



pij
pmiij = log
pi pj

pmiij if pmiij > 0
xij =
0 otherwise

(4)
(5)

In this definition, pij is the estimated probability that the word wi occurs in the context
cj , pi is the estimated probability of the word wi , and pj is the estimated probability of
the context cj . If wi and cj are statistically independent, then pi pj = pij (by the definition
of independence), and thus pmiij is zero (since log(1) = 0). The product pi pj is what we
would expect for pij if wi occurs in cj by pure random chance. On the other hand, if there
is an interesting semantic relation between wi and cj , then we should expect pij to be larger
than it would be if wi and cj were indepedent; hence we should find that pij > pi pj , and
thus pmiij is positive. This follows from the distributional hypothesis (see Section 2). If
the word wi is unrelated to the context cj , we may find that pmiij is negative. PPMI is
designed to give a high value to xij when there is an interesting semantic relation between
157

fiTurney & Pantel

wi and cj ; otherwise, xij should have a value of zero, indicating that the occurrence of wi
in cj is uninformative.
A well-known problem of PMI is that it is biased towards infrequent events. Consider
the case where wi and cj are statistically dependent (i.e., they have maximum association).
Then pij = pi = pj . Hence (4) becomes log (1/pi ) and PMI increases as the probability of
word wi decreases. Several discounting factors have been proposed to alleviate this problem.
An example follows (Pantel & Lin, 2002a):
P c
P r
fik )
fkj , nk=1
min ( nk=1
fij
Pnc
Pnr
ij =

fij + 1 min ( k=1 fkj , k=1 fik ) + 1
newpmiij = ij  pmiij

(6)
(7)

Another way to deal with infrequent events is Laplace smoothing of the probability
estimates, pij , pi , and pj (Turney & Littman, 2003). A constant positive value is added to
the raw frequencies before calculating the probabilities; each fij is replaced with fij + k, for
some k > 0. The larger the constant, the greater the smoothing effect. Laplace smoothing
pushes the pmiij values towards zero. The magnitude of the push (the difference between
pmiij with and without Laplace smoothing) depends on the raw frequency fij . If the
frequency is large, the push is small; if the frequency is small, the push is large. Thus
Laplace smoothing reduces the bias of PMI towards infrequent events.
4.3 Smoothing the Matrix
The simplest way to improve information retrieval performance is to limit the number of
vector components. Keeping only components representing the most frequently occurring
content words is such a way; however, common words, such as the and have, carry little
semantic discrimination power. Simple component smoothing heuristics, based on the properties of the weighting schemes presented in Section 4.2, have been shown to both maintain
semantic discrimination power and improve the performance of similarity computations.
Computing the similarity between all pairs of vectors, described in Section 4.4, is a
computationally intensive task. However, only vectors that share a non-zero coordinate
must be compared (i.e., two vectors that do not share a coordinate are dissimilar). Very
frequent context words, such as the word the, unfortunately result in most vectors matching
a non-zero coordinate. Such words are precisely the contexts that have little semantic
discrimination power. Consider the pointwise mutual information weighting described in
Section 4.2. Highly weighted dimensions co-occur frequently with only very few words and
are by definition highly discriminating contexts (i.e., they have very high association with
the words with which they co-occur). By keeping only the context-word dimensions with
a PMI above a conservative threshold and setting the others to zero, Lin (1998) showed
that the number of comparisons needed to compare vectors greatly decreases while losing
little precision in the similarity score between the top-200 most similar words of every word.
While smoothing the matrix, one computes a reverse index on the non-zero coordinates.
Then, to compare the similarity between a words context vector and all other words context
vectors, only those vectors found to match a non-zero component in the reverse index must
be compared. Section 4.5 proposes further optimizations along these lines.
158

fiFrom Frequency to Meaning

Deerwester et al. (1990) found an elegant way to improve similarity measurements with a
mathematical operation on the termdocument matrix, X, based on linear algebra. The operation is truncated Singular Value Decomposition (SVD), also called thin SVD. Deerwester
et al. briefly mentioned that truncated SVD can be applied to both document similarity
and word similarity, but their focus was document similarity. Landauer and Dumais (1997)
applied truncated SVD to word similarity, achieving human-level scores on multiple-choice
synonym questions from the Test of English as a Foreign Language (TOEFL). Truncated
SVD applied to document similarity is called Latent Semantic Indexing (LSI), but it is
called Latent Semantic Analysis (LSA) when applied to word similarity.
There are several ways of thinking about how truncated SVD works. We will first
present the math behind truncated SVD and then describe four ways of looking at it:
latent meaning, noise reduction, high-order co-occurrence, and sparsity reduction.
SVD decomposes X into the product of three matrices UVT , where U and V are in
column orthonormal form (i.e., the columns are orthogonal and have unit length, UT U =
VT V = I) and  is a diagonal matrix of singular values (Golub & Van Loan, 1996). If X
is of rank r, then  is also of rank r. Let k , where k < r, be the diagonal matrix formed
from the top k singular values, and let Uk and Vk be the matrices produced by selecting the
corresponding columns from U and V. The matrix Uk k VkT is the matrix of rank k that
best approximates the original matrix X, in the sense that it minimizes the approximation
errors. That is, X = Uk k VkT minimizes kX  XkF over all matrices X of rank k, where
k . . . kF denotes the Frobenius norm (Golub & Van Loan, 1996).
Latent meaning: Deerwester et al. (1990) and Landauer and Dumais (1997) describe
truncated SVD as a method for discovering latent meaning. Suppose we have a word
context matrix X. The truncated SVD, X = Uk k VkT , creates a low-dimensional linear
mapping between row space (words) and column space (contexts). This low-dimensional
mapping captures the latent (hidden) meaning in the words and the contexts. Limiting the
number of latent dimensions (k < r) forces a greater correspondence between words and
contexts. This forced correspondence between words and contexts improves the similarity
measurement.
Noise reduction: Rapp (2003) describes truncated SVD as a noise reduction technique.
We may think of the matrix X = Uk k VkT as a smoothed version of the original matrix X.
The matrix Uk maps the row space (the space spanned by the rows of X) into a smaller
k-dimensional space and the matrix Vk maps the column space (the space spanned by
the columns of X) into the same k-dimensional space. The diagonal matrix k specifies
the weights in this reduced k-dimensional space. The singular values in  are ranked in
descending order of the amount of variation in X that they fit. If we think of the matrix
X as being composed of a mixture of signal and noise, with more signal than noise, then
Uk k VkT mostly captures the variation in X that is due to the signal, whereas the remaining
vectors in UVT are mostly fitting the variation in X that is due to the noise.
High-order co-occurrence: Landauer and Dumais (1997) also describe truncated
SVD as a method for discovering high-order co-occurrence. Direct co-occurrence (firstorder co-occurrence) is when two words appear in identical contexts. Indirect co-occurrence
(high-order co-occurrence) is when two words appear in similar contexts. Similarity of
contexts may be defined recursively in terms of lower-order co-occurrence. Lemaire and
Denhiere (2006) demonstrate that truncated SVD can discover high-order co-occurrence.
159

fiTurney & Pantel

Sparsity reduction: In general, the matrix X is very sparse (mostly zeroes), but the
truncated SVD, X = Uk k VkT , is dense. Sparsity may be viewed as a problem of insufficient
data: with more text, the matrix X would have fewer zeroes, and the VSM would perform
better on the chosen task. From this perspective, truncated SVD is a way of simulating the
missing text, compensating for the lack of data (Vozalis & Margaritis, 2003).
These different ways of viewing truncated SVD are compatible with each other; it is
possible for all of these perspectives to be correct. Future work is likely to provide more
views of SVD and perhaps a unifying view.
A good C implementation of SVD for large sparse matrices is Rohdes SVDLIBC.15
Another approach is Brands (2006) incremental truncated SVD algorithm.16 Yet another
approach is Gorrells (2006) Hebbian algorithm for incremental truncated SVD. Brands
and Gorrells algorithms both introduce interesting new ways of handling missing values,
instead of treating them as zero values.
For higher-order tensors, there are operations that are analogous to truncated SVD,
such as parallel factor analysis (PARAFAC) (Harshman, 1970), canonical decomposition
(CANDECOMP) (Carroll & Chang, 1970) (equivalent to PARAFAC but discovered independently), and Tucker decomposition (Tucker, 1966). For an overview of tensor decompositions, see the surveys of Kolda and Bader (2009) or Acar and Yener (2009). Turney (2007)
gives an empirical evaluation of how well four different Tucker decomposition algorithms
scale up for large sparse third-order tensors. A low-RAM algorithm, Multislice Projection,
for large sparse tensors is presented and evaluated.17
Since the work of Deerwester et al. (1990), subsequent research has discovered many
alternative matrix smoothing processes, such as Nonnegative Matrix Factorization (NMF)
(Lee & Seung, 1999), Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999), Iterative Scaling (IS) (Ando, 2000), Kernel Principal Components Analysis (KPCA) (Scholkopf,
Smola, & Muller, 1997), Latent Dirichlet Allocation (LDA) (Blei et al., 2003), and Discrete
Component Analysis (DCA) (Buntine & Jakulin, 2006).
The four perspectives on truncated SVD, presented above, apply equally well to all of
these more recent matrix smoothing algorithms. These newer smoothing algorithms tend
to be more computationally intensive than truncated SVD, but they attempt to model
word frequencies better than SVD. Truncated SVD implicitly assumes that the elements
in X have a Gaussian distribution. Minimizing the the Frobenius norm kX  XkF will
minimize the noise, if the noise has a Gaussian distribution. However, it is known that
word frequencies do not have Gaussian distributions. More recent algorithms are based on
more realistic models of the distribution for word frequencies.18
4.4 Comparing the Vectors
The most popular way to measure the similarity of two frequency vectors (raw or weighted)
is to take their cosine. Let x and y be two vectors, each with n elements.
15.
16.
17.
18.

SVDLIBC is available at http://tedlab.mit.edu/dr/svdlibc/.
MATLAB source code is available at http://web.mit.edu/wingated/www/resources.html.
MATLAB source code is available at http://www.apperceptual.com/multislice/.
In our experience, pmiij appears to be approximately Gaussian, which may explain why PMI works well
with truncated SVD, but then PPMI is puzzling, because it is less Gaussian than PMI, yet it apparently
yields better semantic models than PMI.

160

fiFrom Frequency to Meaning

x = hx1 , x2 , . . . , xn i

(8)

y = hy1 , y2 , . . . , yn i

(9)

The cosine of the angle  between x and y can be calculated as follows:
Pn
cos(x, y) = qP
n

 yi
Pn

i=1 xi

2
2
i=1 xi 
i=1 yi
xy
=

xx yy
x
y
=

kxk kyk

(10)
(11)
(12)

In other words, the cosine of the angle between two vectors is the inner product of the
vectors, after they have been normalized to unit length. If x and y are frequency vectors
for words, a frequent word will have a long vector and a rare word will have a short vector,
yet the words might be synonyms. Cosine captures the idea that the length of the vectors
is irrelevant; the important thing is the angle between the vectors.
The cosine ranges from 1 when the vectors point in opposite directions ( is 180
degrees) to +1 when they point in the same direction ( is 0 degrees). When the vectors
are orthogonal ( is 90 degrees), the cosine is zero. With raw frequency vectors, which
necessarily cannot have negative elements, the cosine cannot be negative, but weighting
and smoothing often introduce negative elements. PPMI weighting does not yield negative
elements, but truncated SVD can generate negative elements, even when the input matrix
has no negative values.
A measure of distance between vectors can easily be converted to a measure of similarity
by inversion (13) or subtraction (14).

sim(x, y) = 1/dist(x, y)

(13)

sim(x, y) = 1  dist(x, y)

(14)

Many similarity measures have been proposed in both IR (Jones & Furnas, 1987) and
lexical semantics circles (Lin, 1998; Dagan, Lee, & Pereira, 1999; Lee, 1999; Weeds, Weir,
& McCarthy, 2004). It is commonly said in IR that, properly normalized, the difference
in retrieval performance using different measures is insignificant (van Rijsbergen, 1979).
Often the vectors are normalized in some way (e.g., unit length or unit probability) before
applying any similarity measure.
Popular geometric measures of vector distance include Euclidean distance and Manhattan distance. Distance measures from information theory include Hellinger, Bhattacharya,
and Kullback-Leibler. Bullinaria and Levy (2007) compared these five distance measures
and the cosine similarity measure on four different tasks involving word similarity. Overall,
the best measure was cosine. Other popular measures are the Dice and Jaccard coefficients
(Manning et al., 2008).
161

fiTurney & Pantel

Lee (1999) proposed that, for finding word similarities, measures that focused more on
overlapping coordinates and less on the importance of negative features (i.e., coordinates
where one word has a nonzero value and the other has a zero value) appear to perform
better. In Lees experiments, the Jaccard, Jensen-Shannon, and L1 measures seemed to
perform best. Weeds et al. (2004) studied the linguistic and statistical properties of the
similar words returned by various similarity measures and found that the measures can be
grouped into three classes:
1. high-frequency sensitive measures (cosine, Jensen-Shannon, -skew, recall),
2. low-frequency sensitive measures (precision), and
3. similar-frequency sensitive methods (Jaccard, Jaccard+MI, Lin, harmonic mean).
Given a word w0 , if we use a high-frequency sensitive measure to score other words wi
according to their similarity with w0 , higher frequency words will tend to get higher scores
than lower frequency words. If we use a low-frequency sensitive measure, there will be a
bias towards lower frequency words. Similar-frequency sensitive methods prefer a word wi
that has approximately the same frequency as w0 . In one experiment on determining the
compositionality of collocations, high-frequency sensitive measures outperformed the other
classes (Weeds et al., 2004). We believe that determining the most appropriate similarity
measure is inherently dependent on the similarity task, the sparsity of the statistics, the
frequency distribution of the elements being compared, and the smoothing method applied
to the matrix.
4.5 Efficient Comparisons
Computing the similarity between all rows (or columns) in a large matrix is a non-trivial
problem, with a worst case cubic running time O(n2r nc ), where nr is the number of rows and
nc is the number of columns (i.e., the dimensionality of the feature space). Optimizations
and parallelization are often necessary.
4.5.1 Sparse-Matrix Multiplication
One optimization strategy is a generalized sparse-matrix multiplication approach (Sarawagi
& Kirpal, 2004), which is based on the observation that a scalar product of two vectors
depends only on the coordinates for which both vectors have nonzero values. Further, we
observe that most commonly used similarity measures for vectors x and y, such as cosine,
overlap, and Dice, can be decomposed into three values: one depending only on the nonzero
values of x, another depending only on the nonzero values of y, and the third depending on
the nonzero coordinates shared both by x and y. More formally, commonly used similarity
scores, sim(x, y), can be expressed as follows:
sim(x, y) = f0 (

Pn

i=1 f1 (xi , yi ), f2 (x), f3 (y))

(15)

For example, the cosine measure, cos(x, y), defined in (10), can be expressed in this model
as follows:
162

fiFrom Frequency to Meaning

P
cos(x, y) = f0 ( ni=1 f1 (xi , yi ), f2 (x), f3 (y))
a
f0 (a, b, c) =
bc
f1 (a, b) = a  b
qP
n
2
f2 (a) = f3 (a) =
i=1 ai

(16)
(17)
(18)
(19)

Let X be a matrix for which we want to compute the pairwise similarity, sim(x, y),
between all rows or all columns x and y. Efficient computation of the similarity matrix S
can be achieved by leveraging the fact that sim(x, y) is determined solely by the nonzero
coordinates shared by x and y (i.e., f1 (0, xi ) = f1 (xi , 0) = 0 for any xi ) and that most of
the vectors are very sparse. In this case, calculating f1 (xi , yi ) is only required when both
vectors have a shared nonzero coordinate, significantly reducing the cost of computation.
Determining which vectors share a nonzero coodinate can easily be achieved by first building
an inverted index for the coordinates. During indexing, we can also precompute f2 (x) and
f3 (y) without changing the algorithm complexity. Then, for each vector x we retrieve in
constant time, from the index, each vector y that shares a nonzero coordinate with x and
weP
apply f1 (xi , yi ) on the shared coordinates i. The computational cost of this algorithm
is i Ni2 where Ni is the number of vectors that have a nonzero i-th coordinate. Its worst
case time complexity is O(ncv) where n is the number of vectors to be compared, c is the
maximum number of nonzero coordinates of any vector, and v is the number of vectors
that have a nonzero i-th coordinate where i is the coordinate which is nonzero for the most
vectors. In other words, the algorithm is efficient only when the density of the coordinates
is low. In our own experiments of computing the semantic similarity between all pairs of
words in a large web crawl, we observed near linear average running time complexity in n.
The computational cost can be reduced further by leveraging the element weighting
techniques described in Section 4.2. By setting to zero all coordinates that have a low
PPMI, PMI or tf-idf score, the coordinate density is dramatically reduced at the cost of
losing little discriminative power. In this vein, Bayardo, Ma, and Srikant (2007) described
a strategy that omits the coordinates with the highest number of nonzero values. Their
algorithm gives a significant advantage only when we are interested in finding solely the
similarity between highly similar vectors.
4.5.2 Distributed Implementation using MapReduce
The algorithm described in Section 4.5.1 assumes that the matrix X can fit into memory,
which for large X may be impossible. Also, as each element of X is processed independently, running parallel processes for non-intersecting subsets of X makes the processing
faster. Elsayed, Lin, and Oard (2008) proposed a MapReduce implementation deployed using Hadoop, an open-source software package implementing the MapReduce framework and
distributed file system.19 Hadoop has been shown to scale to several thousands of machines,
allowing users to write simple code, and to seamlessly manage the sophisticated parallel execution of the code. Dean and Ghemawat (2008) provide a good primer on MapReduce
programming.
19. Hadoop is available for download at http://lucene.apache.org/hadoop/.

163

fiTurney & Pantel

The MapReduce models Map step is used to start m  n Map tasks in parallel, each
caching one m-th part of X as an inverted index and streaming one n-th part of X through
it. The actual inputs are read by the tasks directly from HDFS (Hadoop Distributed
File System). The value of m is determined by the amount of memory dedicated for the
inverted index, and n should be determined by trading off the fact that, as n increases,
more parallelism can be obtained at the increased cost of building the same inverted index
n times.
The similarity algorithm from Section 4.5.1 runs in each task of the Map step of a
MapReduce job. The Reduce step groups the output by the rows (or columns) of X.
4.5.3 Randomized Algorithms
Other optimization strategies use randomized techniques to approximate various similarity measures. The aim of randomized algorithms is to improve computational efficiency
(memory and time) by projecting high-dimensional vectors into a low-dimensional subspace.
Truncated SVD performs such a projection, but SVD can be computationally intensive.20
The insight of randomized techniques is that high-dimensional vectors can be randomly projected into a low-dimensional subspace with relatively little impact on the final similarity
scores. Significant reductions in computational cost have been reported with little average error to computing the true similarity scores, especially in applications such as word
similarity where we are interested in only the top-k most similar vectors to each vector
(Ravichandran, Pantel, & Hovy, 2005; Gorman & Curran, 2006).
Random Indexing, an approximation technique based on Sparse Distributed Memory
(Kanerva, 1993), computes the pairwise similarity between all rows (or vectors) of a matrix
with complexity O(nr nc 1 ), where 1 is a fixed constant representing the length of the index
vectors assigned to each column. The value of 1 controls the tradeoff of accuracy versus
efficiency. The elements of each index vector are mostly zeros with a small number of
randomly assigned +1s and 1s. The cosine measure between two rows r1 and r2 is then
approximated by computing the cosine between two fingerprint vectors, fingerprint(r1 )
and fingerprint(r2 ), where fingerprint(r) is computed by summing the index vectors of
each non-unique coordinate of r. Random Indexing was shown to perform as well as LSA
on a word synonym selection task (Karlgren & Sahlgren, 2001).
Locality sensitive hashing (LSH) (Broder, 1997) is another technique that approximates
the similarity matrix with complexity O(n2r 2 ), where 2 is a constant number of random
projections, which controls the accuracy versus efficiency tradeoff.21 LSH is a general class of
techniques for defining functions that map vectors (rows or columns) into short signatures or
fingerprints, such that two similar vectors are likely to have similar fingerprints. Definitions
of LSH functions include the Min-wise independent function, which preserves the Jaccard
similarity between vectors (Broder, 1997), and functions that preserve the cosine similarity
between vectors (Charikar, 2002). On a word similarity task, Ravichandran et al. (2005)
showed that, on average, over 80% of the top-10 similar words of random words are found
in the top-10 results using Charikars functions, and that the average cosine error is 0.016
20. However, there are efficient forms of SVD (Brand, 2006; Gorrell, 2006).
21. LSH stems from work by Rabin (1981), who proposed the use of hash functions from random irreducible
polynomials to create short fingerprints of collections of documents. Such techniques are useful for many
tasks, such as removing duplicate documents (deduping) in a web crawl.

164

fiFrom Frequency to Meaning

(using 2 = 10,000 random projections). Gorman and Curran (2006) provide a detailed
comparison of Random Indexing and LSH on a distributional similarity task. On the BNC
corpus, LSH outperformed Random Indexing; however, on a larger corpora combining BNC,
the Reuters Corpus, and most of the English news holdings of the LDC in 2003, Random
Indexing outperformed LSH in both efficiency and accuracy.
4.6 Machine Learning
If the intended application for a VSM is clustering or classification, a similarity measure
such as cosine (Section 4.4) may be used. For classification, a nearest-neighbour algorithm
can use cosine as a measure of nearness (Dasarathy, 1991). For clustering, a similaritybased clustering algorithm can use cosine as a measure of similarity (Jain, Murty, & Flynn,
1999). However, there are many machine learning algorithms that can work directly with
the vectors in a VSM, without requiring an external similarity measure, such as cosine.
In effect, such machine learning algorithms implicitly use their own internal approaches to
measuring similarity.
Any machine learning algorithm that works with real-valued vectors can use vectors
from a VSM (Witten & Frank, 2005). Linguistic processing (Section 3) and mathematical
processing (Section 4) may still be necessary, but the machine learning algorithm can handle
vector comparison (Sections 4.4 and 4.5).
In addition to unsupervised (clustering) and supervised (classification) machine learning, vectors from a VSM may also be used in semi-supervised learning (Ando & Zhang,
2005; Collobert & Weston, 2008). In general, there is nothing unique to VSMs that would
compel a choice of one machine learning algorithm over another, aside from the algorithms
performance on the given task. Therefore we refer our readers to the machine learning
literature (Witten & Frank, 2005), since we have no advice that is specific to VSMs.

5. Three Open Source VSM Systems
To illustrate the three types of VSMs discussed in Section 2, this section presents three open
source systems, one for each VSM type. We have chosen to present open source systems so
that interested readers can obtain the source code to find out more about the systems and
to apply the systems in their own projects. All three systems are written in Java and are
designed for portability and ease of use.
5.1 The TermDocument Matrix: Lucene
Lucene22 is an open source full-featured text search engine library supported by the Apache
Software Foundation. It is arguably the most ubiquitous implementation of a termdocument
matrix, powering many search engines such as at CNET, SourceForge, Wikipedia, Disney,
AOL and Comcast. Lucene offers efficient storage, indexing, as well as retrieval and ranking
functionalities. Although it is primarily used as a termdocument matrix, it generalizes to
other VSMs.
22. Apache Lucene is available for download at http://lucene.apache.org/.

165

fiTurney & Pantel

Content, such as webpages, PDF documents, images, and video, are programmatically
decomposed into fields and stored in a database. The database implements the term
document matrix, where content corresponds to documents and fields correspond to terms.
Fields are stored in the database and indices are computed on the field values. Lucene
uses fields as a generalization of content terms, allowing any other string or literal to index
documents. For example, a webpage could be indexed by all the terms it contains, and also
by the anchor texts pointing to it, its host name, and the semantic classes in which it is
classified (e.g., spam, product review, news, etc.). The webpage can then be retrieved by
search terms matching any of these fields.
Columns in the termdocument matrix consist of all the fields of a particular instance
of content (e.g., a webpage). The rows consist of all instances of content in the index.
Various statistics such as frequency and tf-idf are stored in the matrix. The developer
defines the fields in a schema and identifies those to be indexed by Lucene. The developer
also optionally defines a content ranking function for each indexed field.
Once the index is built, Lucene offers functionalities for retrieving content. Users can
issue many query types such as phrase queries, wildcard queries, proximity queries, range
queries (e.g., date range queries), and field-restricted queries. Results can be sorted by any
field and index updates can occur simultaneously during searching. Lucenes index can be
directly loaded into a Tomcat webserver and it offers APIs for common programming languages. Solr,23 a separate Apache Software Foundation project, is an open source enterprise
webserver for searching a Lucene index and presenting search results. It is a full-featured
webserver providing functionalities such as XML/HTTP and JSON APIs, hit highlighting,
faceted search, caching, and replication.
A simple recipe for creating a web search service, using Nutch, Lucene and Solr, consists
of crawling a set of URLs (using Nutch), creating a termdocument matrix index (using
Lucene), and serving search results (using Solr). Nutch,24 the Apache Software Foundation
open source web search software, offers functionality for crawling the web from a seed set
of URLs, for building a link-graph of the web crawl, and for parsing web documents such
as HTML pages. A good set of seed URLs for Nutch can be downloaded freely from the
Open Directory Project.25 Crawled pages are HTML-parsed, and they are then indexed
by Lucene. The resulting indexed collection is then queried and served through a Solr
installation with Tomcat.
For more information on Lucene, we recommend Gospodnetic and Hatchers (2004)
book. Konchady (2008) explains how to integrate Lucene with LingPipe and GATE for
sophisticated semantic processing.26

23.
24.
25.
26.

Apache Solr is available for download at http://lucene.apache.org/solr/.
Apache Nutch is available for download at http://lucene.apache.org/nutch/.
See http://www.dmoz.org/.
Information about LingPipe is available at http://alias-i.com/lingpipe/. The GATE (General Architecture for Text Engineering) home page is at http://gate.ac.uk/.

166

fiFrom Frequency to Meaning

5.2 The WordContext Matrix: Semantic Vectors
Semantic Vectors27 is an open source project implementing the random projection approach
to measuring word similarity (see Section 4.5.3). The package uses Lucene to create a term
document matrix, and it then creates vectors from Lucenes termdocument matrix, using
random projection for dimensionality reduction. The random projection vectors can be
used, for example, to measure the semantic similarity of two words or to find the words
that are most similar to a given word.
The idea of random projection is to take high-dimensional vectors and randomly project
them into a relatively low-dimensional space (Sahlgren, 2005). This can be viewed as a
kind of smoothing operation (Section 4.3), but the developers of the Semantic Vectors
package emphasize the simplicity and efficiency of random projection (Section 4.5), rather
than its smoothing ability. They argue that other matrix smoothing algorithms might
smooth better, but none of them perform as well as random indexing, in terms of the
computational complexity of building a smooth matrix and incrementally updating the
matrix when new data arrives (Widdows & Ferraro, 2008). Their aim is to encourage
research and development with semantic vectors by creating a simple and efficient open
source package.
The Semantic Vectors package is designed to be convenient to use, portable, and easy
to extend and modify. The design of the software incorporates lessons learned from the
earlier Stanford Infomap project.28 Although the default is to generate random projection
vectors, the system has a modular design that allows other kinds of wordcontext matrices
to be used instead of random projection matrices.
The package supports two basic functions: building a wordcontext matrix and searching
through the vectors in the matrix. In addition to generating word vectors, the building
operation can generate document vectors by calculating weighted sums of the word vectors
for the words in each document. The searching operation can be used to search for similar
words or to search for documents that are similar to a query. A query can be a single word or
several words can be combined, using various mathematical operations on the corresponding
vectors. The mathematical operations include vector negation and disjunction, based on
quantum logic (Widdows, 2004). Widdows and Ferraro (2008) provide a good summary of
the Semantic Vectors software.
5.3 The PairPattern Matrix: Latent Relational Analysis in S-Space
Latent Relational Analysis29 (LRA) is an open source project implementing the pairpattern
matrix. It is a component of the S-Space package, a library of tools for building and
comparing different semantic spaces.
LRA takes as input a textual corpus and a set of word pairs. A pairpattern matrix is
built by deriving lexical patterns that link together the word pairs in the corpus. For example, consider the word pair hKorea, Japani and the following retrieved matching sentences:
27. Semantic Vectors is a software package for measuring word similarity, available under the Simplified BSD
License at http://code.google.com/p/semanticvectors/.
28. See http://infomap-nlp.sourceforge.net/.
29. Latent Relational Analysis is part of the S-Space package and is distributed under the GNU General
Public License version 2. It is available at http://code.google.com/p/airhead-research/. At the time of
writing, the LRA module was under development.

167

fiTurney & Pantel

 Korea looks to new Japan prime ministers effect on Korea-Japan relations.
 What channel is the Korea vs. Japan football game?
From these two sentences, LRA extracts two patterns: X looks to new Y  and X vs. Y .
These patterns become two columns in the pairpattern matrix, and the word pair hKorea,
Japani becomes a row. Pattern frequencies are counted and then smoothed using SVD (see
Section 4.3).
In order to mitigate the sparseness of occurrences of word pairs, a thesaurus such as
WordNet is used to expand the seed word pairs to alternatives. For example the pair
hKorea, Japani may be expanded to include hSouth Korea, Japani, hRepublic of Korea,
Japani, hKorea, Nipponi, hSouth Korea, Nipponi, and hRepublic of Korea, Nipponi.
LRA uses Lucene (see Section 5.1) as its backend to store the matrix, index it, and serve
its contents. For a detailed description of the LRA algorithm, we suggest Turneys (2006)
paper.

6. Applications
In this section, we will survey some of the semantic applications for VSMs. We will aim for
breadth, rather than depth; readers who want more depth should consult the references. Our
goal is to give the reader an impression of the scope and flexibility of VSMs for semantics.
The following applications are grouped according to the type of matrix involved: term
document, wordcontext, or pairpattern. Note that this section is not exhaustive; there
are many more references and applications than we have space to list here.
6.1 TermDocument Matrices
Termdocument matrices are most suited to measuring the semantic similarity of documents
and queries (see Section 2.1). The usual measure of similarity is the cosine of column vectors
in a weighted termdocument matrix. There are a variety of applications for measures of
document similarity.
Document retrieval: The termdocument matrix was first developed for document
retrieval (Salton et al., 1975), and there is now a large body of literature on the VSM
for document retrieval (Manning et al., 2008), including several journals and conferences
devoted to the topic. The core idea is, given a query, rank the documents in order of
decreasing cosine of the angles between the query vector and the document vectors (Salton
et al., 1975). One variation on the theme is cross-lingual document retrieval, where a
query in one language is used to retrieve a document in another language (Landauer &
Littman, 1990). An important technical advance was the discovery that smoothing the
termdocument matrix by truncated SVD can improve precision and recall (Deerwester
et al., 1990), although few commercial systems use smoothing, due to the computational
expense when the document collection is large and dynamic. Random indexing (Sahlgren,
2005) or incremental SVD (Brand, 2006) may help to address these scaling issues. Another
important development in document retrieval has been the addition of collaborative filtering,
in the form of PageRank (Brin & Page, 1998).
Document clustering: Given a measure of document similarity, we can cluster the
documents into groups, such that similarity tends to be high within a group, but low across
168

fiFrom Frequency to Meaning

groups (Manning et al., 2008). The clusters may be partitional (flat) (Cutting, Karger,
Pedersen, & Tukey, 1992; Pantel & Lin, 2002b) or they may have a hierarchical structure
(groups of groups) (Zhao & Karypis, 2002); they may be non-overlapping (hard) (Croft,
1977) or overlapping (soft) (Zamir & Etzioni, 1999). Clustering algorithms also differ in how
clusters are compared and abstracted. With single-link clustering, the similarity between
two clusters is the maximum of the similarities between their members. Complete-link
clustering uses the minimum of the similarities and average-link clustering uses the average
of the similarities (Manning et al., 2008).
Document classification: Given a training set of documents with class labels and a
testing set of unlabeled documents, the task of document classification is to learn from the
training set how to assign labels to the testing set (Manning et al., 2008). The labels may
be the topics of the documents (Sebastiani, 2002), the sentiment of the documents (e.g.,
positive versus negative product reviews) (Pang, Lee, & Vaithyanathan, 2002; Kim, Pantel,
Chklovski, & Pennacchiotti, 2006), spam versus non-spam (Sahami, Dumais, Heckerman, &
Horvitz, 1998; Pantel & Lin, 1998), or any other labels that might be inferred from the words
in the documents. When we classify documents, we are implying that the documents in a
class are similar in some way; thus document classification implies some notion of document
similarity, and most machine learning approaches to document classification involve a term
document matrix (Sebastiani, 2002). A measure of document similarity, such as cosine, can
be directly applied to document classification by using a nearest-neighbour algorithm (Yang,
1999).
Essay grading: Student essays may be automatically graded by comparing them to
one or more high-quality reference essays on the given essay topic (Wolfe, Schreiner, Rehder,
Laham, Foltz, Kintsch, & Landauer, 1998; Foltz, Laham, & Landauer, 1999). The student
essays and the reference essays can be compared by their cosines in a termdocument matrix.
The grade that is assigned to a student essay is proportional to its similarity to one of the
reference essays; a student essay that is highly similar to a reference essay gets a high grade.
Document segmentation: The task of document segmentation is to partition a document into sections, where each section focuses on a different subtopic of the document
(Hearst, 1997; Choi, 2000). We may treat the document as a series of blocks, where a block
is a sentence or a paragraph. The problem is to detect a topic shift from one block to the
next. Hearst (1997) and Choi (2000) both use the cosine between columns in a wordblock
frequency matrix to measure the semantic similarity of blocks. A topic shift is signaled by
a drop in the cosine between consecutive blocks. The wordblock matrix can be viewed as
a small termdocument matrix, where the corpus is a single document and the documents
are blocks.
Question answering: Given a simple question, the task in Question Answering (QA)
is to find a short answer to the question by searching in a large corpus. A typical question is, How many calories are there in a Big Mac? Most algorithms for QA have four
components, question analysis, document retrieval, passage retrieval, and answer extraction
(Tellex, Katz, Lin, Fern, & Marton, 2003; Dang, Lin, & Kelly, 2006). Vector-based similarity measurements are often used for both document retrieval and passage retrieval (Tellex
et al., 2003).
Call routing: Chu-carroll and Carpenter (1999) present a vector-based system for
automatically routing telephone calls, based on the callers spoken answer to the question,
169

fiTurney & Pantel

How may I direct your call? If the callers answer is ambiguous, the system automatically
generates a question for the caller, derived from the VSM, that prompts the caller for further
information.
6.2 WordContext Matrices
Wordcontext matrices are most suited to measuring the semantic similarity of words (see
Section 2.2). For example, we can measure the similarity of two words by the cosine of the
angle between their corresponding row vectors in a wordcontext matrix. There are many
applications for measures of word similarity.
Word similarity: Deerwester et al. (1990) discovered that we can measure word similarity by comparing row vectors in a termdocument matrix. Landauer and Dumais (1997)
evaluated this approach with 80 multiple-choice synonym questions from the Test of English as a Foreign Language (TOEFL), achieving human-level performance (64.4% correct
for the wordcontext matrix and 64.5% for the average non-English US college applicant).
The documents used by Landauer and Dumais had an average length of 151 words, which
seems short for a document, but long for the context of a word. Other researchers soon
switched to much shorter lengths, which is why we prefer to call these wordcontext matrices, instead of termdocument matrices. Lund and Burgess (1996) used a context window
of ten words. Schutze (1998) used a fifty-word window (25 words, centered on the target
word). Rapp (2003) achieved 92.5% correct on the 80 TOEFL questions, using a four-word
context window (2 words, centered on the target word, after removing stop words). The
TOEFL results suggest that performance improves as the context window shrinks. It seems
that the immediate context of a word is much more important than the distant context for
determining the meaning of the word.
Word clustering: Pereira, Tishby, and Lee (1993) applied soft hierarchical clustering
to row-vectors in a wordcontext matrix. In one experiment, the words were nouns and the
contexts were verbs for which the given nouns were direct objects. In another experiment,
the words were verbs and the contexts were nouns that were direct objects of the given
verbs. Schutzes (1998) seminal word sense discrimination model used hard flat clustering
for row-vectors in a wordcontext matrix, where the context was given by a window of 25
words, centered on the target word. Pantel and Lin (2002a) applied soft flat clustering to
a wordcontext matrix, where the context was based on parsed text. These algorithms are
able to discover different senses of polysemous words, generating different clusters for each
sense. In effect, the different clusters correspond to the different concepts that underlie the
words.
Word classification: Turney and Littman (2003) used a wordcontext matrix to classify words as positive (honest, intrepid) or negative (disturbing, superfluous). They used the
General Inquirer (GI) lexicon (Stone, Dunphy, Smith, & Ogilvie, 1966) to evaluate their
algorithms. The GI lexicon includes 11,788 words, labeled with 182 categories related to
opinion, affect, and attitude.30 Turney and Littman hypothesize that all 182 categories can
be discriminated with a wordcontext matrix.
Automatic thesaurus generation: WordNet is a popular tool for research in natural
language processing (Fellbaum, 1998), but creating and maintaing such lexical resources
30. The GI lexicon is available at http://www.wjh.harvard.edu/inquirer/spreadsheet guide.htm.

170

fiFrom Frequency to Meaning

is labour intensive, so it is natural to wonder whether the process can be automated to
some degree.31 This task can seen as an instance of word clustering (when the thesaurus
is generated from scratch) or classification (when an existing thesaurus is automatically
extended), but it is worthwhile to consider the task of automatic thesaurus generation
separately from clustering and classification, due to the specific requirements of a thesaurus,
such as the particular kind of similarity that is appropriate for a thesaurus (see Section 2.4).
Several researchers have used wordcontext matrices specifically for the task of assisting or
automating thesaurus generation (Crouch, 1988; Grefenstette, 1994; Ruge, 1997; Pantel &
Lin, 2002a; Curran & Moens, 2002).
Word sense disambiguation: A typical Word Sense Disambiguation (WSD) system
(Agirre & Edmonds, 2006; Pedersen, 2006) uses a feature vector representation in which
each vector corresponds to a token of a word, not a type (see Section 2.6). However, Leacock,
Towell, and Voorhees (1993) used a wordcontext frequency matrix for WSD, in which each
vector corresponds to a type annotated with a sense tag. Yuret and Yatbaz (2009) applied
a wordcontext frequency matrix to unsupervised WSD, achieving results comparable to
the performance of supervised WSD systems.
Context-sensitive spelling correction: People frequently confuse certain sets of
words, such as there, theyre, and their. These confusions cannot be detected by a simple dictionary-based spelling checker; they require context-sensitive spelling correction. A
wordcontext frequency matrix may be used to correct these kinds of spelling errors (Jones
& Martin, 1997).
Semantic role labeling: The task of semantic role labeling is to label parts of a sentence according to the roles they play in the sentence, usually in terms of their connection
to the main verb of the sentence. Erk (2007) presented a system in which a wordcontext
frequency matrix was used to improve the performance of semantic role labeling. Pennacchiotti, Cao, Basili, Croce, and Roth (2008) show that wordcontext matrices can reliably
predict the semantic frame to which an unknown lexical unit refers, with good levels of
accuracy. Such lexical unit induction is important in semantic role labeling, to narrow the
candidate set of roles of any observed lexical unit.
Query expansion: Queries submitted to search engines such as Google and Yahoo!
often do not directly match the terms in the most relevant documents. To alleviate this
problem, the process of query expansion is used for generating new search terms that are
consistent with the intent of the original query. VSMs form the basis of query semantics
models (Cao, Jiang, Pei, He, Liao, Chen, & Li, 2008). Some methods represent queries
by using session contexts, such as query cooccurrences in user sessions (Huang, Chien, &
Oyang, 2003; Jones, Rey, Madani, & Greiner, 2006), and others use click contexts, such as
the urls that were clicked on as a result of a query (Wen, Nie, & Zhang, 2001).
Textual advertising: In pay-per-click advertising models, prevalent in search engines
such as Google and Yahoo!, users pay for keywords, called bidterms, which are then used to
display their ads when relevant queries are issued by users. The scarcity of data makes ad
matching difficult and, in response, several techniques for bidterm expansion using VSMs
have been proposed. The wordcontext matrix consists of rows of bidterms and the columns
31. WordNet is available at http://wordnet.princeton.edu/.

171

fiTurney & Pantel

(contexts) consist of advertiser identifiers (Gleich & Zhukov, 2004) or co-bidded bidterms
(second order co-occurrences) (Chang, Pantel, Popescu, & Gabrilovich, 2009).
Information extraction: The field of information extraction (IE) includes named
entity recognition (NER: recognizing that a chunk of text is the name of an entity, such as
a person or a place), relation extraction, event extraction, and fact extraction. Pasca et
al. (2006) demonstrate that a wordcontext frequency matrix can facilitate fact extraction.
Vyas and Pantel (2009) propose a semi-supervised model using a wordcontext matrix for
building and iteratively refining arbitrary classes of named entities.
6.3 PairPattern Matrices
Pairpattern matrices are most suited to measuring the semantic similarity of word pairs
and patterns (see Section 2.3). For example, we can measure the similarity of two word
pairs by the cosine of the angle between their corresponding row vectors in a pairpattern
matrix. There are many applications for measures of relational similarity.
Relational similarity: Just as we can measure attributional similarity by the cosine
of the angle between row vectors in a wordcontext matrix, we can measure relational
similarity by the cosine of the angle between rows in a pairpattern matrix. This approach
to measuring relational similarity was introduced by Turney et al. (2003) and examined
in more detail by Turney and Littman (2005). Turney (2006) evaluated this approach
to relational similarity with 374 multiple-choice analogy questions from the SAT college
entrance test, achieving human-level performance (56% correct for the pairpattern matrix
and 57% correct for the average US college applicant). This is the highest performance so
far for an algorithm. The best algorithm based on attributional similarity has an accuracy
of only 35% (Turney, 2006). The best non-VSM algorithm achieves 43% (Veale, 2004).
Pattern similarity: Instead of measuring the similarity between row vectors in a pair
pattern matrix, we can measure the similarity between columns; that is, we can measure
pattern similarity. Lin and Pantel (2001) constructed a pairpattern matrix in which the
patterns were derived from parsed text. Pattern similarity can be used to infer that one
phrase is a paraphrase of another phrase, which is useful for natural language generation,
text summarization, information retrieval, and question answering.
Relational clustering: Bicici and Yuret (2006) clustered word pairs by representing
them as row vectors in a pairpattern matrix. Davidov and Rappoport (2008) first clustered
contexts (patterns) and then identified representative pairs for each context cluster. They
used the representative pairs to automatically generate multiple-choice analogy questions,
in the style of SAT analogy questions.
Relational classification: Chklovski and Pantel (2004) used a pairpattern matrix
to classify pairs of verbs into semantic classes. For example, taint : poison is classified as
strength (poisoning is stronger than tainting) and assess : review is classified as enablement
(assessing is enabled by reviewing). Turney (2005) used a pairpattern matrix to classify
noun compounds into semantic classes. For example, flu virus is classified as cause (the
virus causes the flu), home town is classified as location (the home is located in the town),
and weather report is classified as topic (the topic of the report is the weather).
Relational search: Cafarella, Banko, and Etzioni (2006) described relational search
as the task of searching for entities that satisfy given semantic relations. An example of
172

fiFrom Frequency to Meaning

a query for a relational search engine is list all X such that X causes cancer. In this
example, the relation, cause, and one of the terms in the relation, cancer, are given by the
user, and the task of the search engine is to find terms that satisfy the users query. The
organizers of Task 4 in SemEval 2007 (Girju, Nakov, Nastase, Szpakowicz, Turney, & Yuret,
2007) envisioned a two-step approach to relational search: first a conventional search engine
would look for candidate answers, then a relational classification system would filter out
incorrect answers. The first step was manually simulated by the Task 4 organizers and the
goal of Task 4 was to design systems for the second step. This task attracted 14 teams who
submitted 15 systems. Nakov and Hearst (2007) achieved good results using a pairpattern
matrix.
Automatic thesaurus generation: We discussed automatic thesaurus generation in
Section 6.2, with wordcontext matrices, but arguably relational similarity is more relevant
than attributional similarity for thesaurus generation. For example, most of the information in WordNet is in the relations between the words rather than in the words individually.
Snow, Jurafsky, and Ng (2006) used a pairpattern matrix to build a hypernym-hyponym
taxonomy, whereas Pennacchiotti and Pantel (2006) built a meronymy and causation taxonomy. Turney (2008b) showed how a pairpattern matrix can distinguish synonyms from
antonyms, synonyms from non-synonyms, and taxonomically similar words (hair and fur)
from words that are merely semantically associated (cradle and baby).
Analogical mapping: Proportional analogies have the form a : b :: c : d, which means a
is to b as c is to d. For example, mason : stone :: carpenter : wood means mason is to stone
as carpenter is to wood. The 374 multiple-choice analogy questions from the SAT college
entrance test (mentioned above) all involve proportional analogies. With a pairpattern
matrix, we can solve proportional analogies by selecting the choice that maximizes relational
similarity (e.g., simr (mason : stone, carpenter : wood) has a high value). However, we often
encounter analogies that involve more than four terms. The well-known analogy between
the solar system and the Rutherford-Bohr model of the atom contains at least fourteen
terms. For the solar system, we have planet, attracts, revolves, sun, gravity, solar system,
and mass. For the atom, we have revolves, atom, attracts, electromagnetism, nucleus,
charge, and electron. Turney (2008a) demonstrated that we can handle these more complex,
systematic analogies by decomposing them into sets of proportional analogies.

7. Alternative Approaches to Semantics
The applications that we list in Section 6 do not necessarily require a VSM approach. For
each application, there are many other possible approaches. In this section, we briefly
consider a few of the main alternatives.
Underlying the applications for termdocument matrices (Section 6.1) is the task of
measuring the semantic similarity of documents and queries. The main alternatives to
VSMs for this task are probabilistic models, such as the traditional probabilistic retrieval
models in information retrieval (van Rijsbergen, 1979; Baeza-Yates & Ribeiro-Neto, 1999)
and the more recent statistical language models inspired by information theory (Liu &
Croft, 2005). The idea of statistical language models for information retrieval is to measure
the similarity between a query and a document by creating a probabilistic language model
173

fiTurney & Pantel

of the given document and then measuring the probability of the given query according to
the language model.
With progress in information retrieval, the distinction between the VSM approach and
the probabilistic approach is becoming blurred, as each approach borrows ideas from the
other. Language models typically involve multiplying probabilities, but we can view this as
adding logs of probabilities, which makes some language models look similar to VSMs.
The applications for wordcontext matrices (Section 6.2) share the task of measuring the
semantic similarity of words. The main alternatives to VSMs for measuring word similarity
are approaches that use lexicons, such as WordNet (Resnik, 1995; Jiang & Conrath, 1997;
Hirst & St-Onge, 1998; Leacock & Chodrow, 1998; Budanitsky & Hirst, 2001). The idea is
to view the lexicon as a graph, in which nodes correspond to word senses and edges represent
relations between words, such as hypernymy and hyponymy. The similarity between two
words is then proportional to the length of the path in the graph that joins the two words.
Several approaches to measuring the semantic similarity of words combine a VSM with
a lexicon (Turney et al., 2003; Pantel, 2005; Patwardhan & Pedersen, 2006; Mohammad &
Hirst, 2006). Humans use both dictionary definitions and observations of word usage, so it
is natural to expect the best performance from algorithms that use both distributional and
lexical information.
Pairpattern matrices (Section 6.3) have in common the task of measuring the semantic
similarity of relations. As with wordcontext matrices, the main alternatives are approaches
that use lexicons (Rosario & Hearst, 2001; Rosario, Hearst, & Fillmore, 2002; Nastase
& Szpakowicz, 2003; Veale, 2003, 2004). The idea is to reduce relational similarity to
attributional similarity, simr (a : b, c : d)  sima (a, c) + sima (b, d), and then use a lexicon
to measure attributional similarity. As we discuss in Section 2.4, this reduction does not
work in general. However, the reduction is often a good approximation, and there is some
evidence that a hybrid approach, combining a VSM with a lexicon, can be beneficial (Turney
et al., 2003; Nastase, Sayyad-Shirabad, Sokolova, & Szpakowicz, 2006).

8. The Future of Vector Space Models of Semantics
Several authors have criticized VSMs (French & Labiouse, 2002; Pado & Lapata, 2003;
Morris & Hirst, 2004; Budanitsky & Hirst, 2006). Most of the criticism stems from the
fact that termdocument and wordcontext matrices typically ignore word order. In LSA,
for instance, a phrase is commonly represented by the sum of the vectors for the individual
words in the phrase; hence the phrases house boat and boat house will be represented by
the same vector, although they have different meanings. In English, word order expresses
relational information. Both house boat and boat house have a Tool-Purpose relation, but
house boat means Tool-Purpose(boat, house) (a boat that serves as a house), whereas boat
house means Tool-Purpose(house, boat) (a house for sheltering and storing boats).
Landauer (2002) estimates that 80% of the meaning of English text comes from word
choice and the remaining 20% comes from word order. However, VSMs are not inherently
limited to 80% of the meaning of text. Mitchell and Lapata (2008) propose composition
models sensitive to word order. For example, to make a simple additive model become
syntax-aware, they allow for different weightings of the contributions of the vector components. Constituents that are more important to the composition therefore can participate
174

fiFrom Frequency to Meaning

more actively. Clark and Pulman (2007) assigned distributional meaning to sentences using the Hilbert space tensor product. Widdows and Ferraro (2008), inspired by quantum
mechanics, explores several operators for modeling composition of meaning. Pairpattern
matrices are sensitive to the order of the words in a pair (Turney, 2006). Thus there are
several ways to handle word order in VSMs.
This raises the question, what are the limits of VSMs for semantics? Can all semantics
be represented with VSMs? There is much that we do not yet know how to represent with
VSMs. For example, Widdows (2004) and van Rijsbergen (2004) show how disjunction,
conjunction, and negation can be represented with vectors, but we do not yet know how to
represent arbitrary statements in first-order predicate calculus. However, it seems possible
that future work may discover answers to these limitations.
In this survey, we have assumed that VSMs are composed of elements with values that
are derived from event frequencies. This ties VSMs to some form of distributional hypothesis
(see Sections 1.1 and 2.7); therefore the limits of VSMs depend on the limits of the family
of distributional hypotheses. Are statistical patterns of word usage sufficient to figure out
what people mean? This is arguably the major open question of VSMs, and the answer will
determine the future of VSMs. We do not have a strong argument one way or the other,
but we believe that the continuing progress with VSMs suggests we are far from reaching
their limits.

9. Conclusions
When we want information or help from a person, we use words to make a request or
describe a problem, and the person replies with words. Unfortunately, computers do not
understand human language, so we are forced to use artificial languages and unnatural user
interfaces. In science fiction, we dream of computers that understand human language, that
can listen to us and talk with us. To achieve the full potential of computers, we must enable
them to understand the semantics of natural language. VSMs are likely to be part of the
solution to the problem of computing semantics.
Many researchers who have struggled with the problem of semantics have come to the
conclusion that the meaning of words is closely connected to the statistics of word usage
(Section 2.7). When we try to make this intuition precise, we soon find we are working with
vectors of values derived from event frequencies; that is, we are dealing with VSMs.
In this survey, we have organized past work with VSMs according to the structure of
the matrix (termdocument, wordcontext, or pairpattern). We believe that the structure
of the matrix is the most important factor in determining the types of applications that
are possible. The linguistic processing (Section 3) and mathematical processing (Section 4)
play smaller (but important) roles.
Our goal in this survey has been to show the breadth and power of VSMs, to introduce
VSMs to those who less familiar with them, and to provide a new perspective on VSMs to
those who are already familiar with them. We hope that our emphasis on the structure of
the matrix will inspire new research. There is no reason to believe that the three matrix
types we present here exhaust the possibilities. We expect new matrix types and new tensors
will open up more applications for VSMs. It seems possible to us that all of the semantics
of human language might one day be captured in some kind of VSM.
175

fiTurney & Pantel

Acknowledgments
Thanks to Annie Zaenen for prompting this paper. Thanks to Saif Mohammad and Mariana
Soffer for their comments. Thanks to Arkady Borkovsky and Eric Crestan for developing
the distributed sparse-matrix multiplication algorithm, and to Marco Pennacchiotti for his
invaluable comments. Thanks to the anonymous reviewers of JAIR for their very helpful
comments and suggestions.

References
Acar, E., & Yener, B. (2009). Unsupervised multiway data analysis: A literature survey.
IEEE Transactions on Knowledge and Data Engineering, 21 (1), 620.
Agirre, E., & Edmonds, P. G. (2006). Word Sense Disambiguation: Algorithms and Applications. Springer.
Ando, R. K. (2000). Latent semantic space: Iterative scaling improves precision of interdocument similarity measurement. In Proceedings of the 23rd Annual ACM SIGIR
Conference on Research and Development in Information Retrieval (SIGIR-2000), pp.
216223.
Ando, R. K., & Zhang, T. (2005). A framework for learning predictive structures from
multiple tasks and unlabeled data. Journal of Machine Learning Research, 6, 1817
1853.
Baeza-Yates, R., & Ribeiro-Neto, B. (1999). Modern Information Retrieval. Addison Wesley.
Barr, C., Jones, R., & Regelson, M. (2008). The linguistic structure of English websearch queries. In Conference on Empirical Methods in Natural Language Processing
(EMNLP).
Bayardo, R. J., Ma, Y., & Srikant, R. (2007). Scaling up all pairs similarity search. In
Proceedings of the 16th international conference on World Wide Web (WWW 07),
pp. 131140, New York, NY. ACM.
Bicici, E., & Yuret, D. (2006). Clustering word pairs to answer analogy questions. In
Proceedings of the Fifteenth Turkish Symposium on Artificial Intelligence and Neural
Networks (TAINN 2006), Akyaka, Mugla, Turkey.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet Allocation. Journal of
Machine Learning Research, 3, 9931022.
Brand, M. (2006). Fast low-rank modifications of the thin singular value decomposition.
Linear Algebra and Its Applications, 415 (1), 2030.
Breese, J., Heckerman, D., & Kadie, C. (1998). Empirical analysis of predictive algorithms
for collaborative filtering. In Proceedings of the 14th Conference on Uncertainty in
Artificial Intelligence, pp. 4352. Morgan Kaufmann.
Brin, S., & Page, L. (1998). The anatomy of a large-scale hypertextual Web search engine.
In Proceedings of the Seventh World Wide Web Conference (WWW7), pp. 107117.
Broder, A. (1997). On the resemblance and containment of documents. In In Compression
and Complexity of Sequences (SEQUENCES97), pp. 2129. IEEE Computer Society.
176

fiFrom Frequency to Meaning

Budanitsky, A., & Hirst, G. (2001). Semantic distance in WordNet: An experimental,
application-oriented evaluation of five measures. In Proceedings of the Workshop
on WordNet and Other Lexical Resources, Second Meeting of the North American
Chapter of the Association for Computational Linguistics (NAACL-2001), pp. 2924,
Pittsburgh, PA.
Budanitsky, A., & Hirst, G. (2006). Evaluating wordnet-based measures of semantic distance. Computational Linguistics, 32 (1), 1347.
Bullinaria, J., & Levy, J. (2007). Extracting semantic representations from word cooccurrence statistics: A computational study. Behavior Research Methods, 39 (3),
510526.
Buntine, W., & Jakulin, A. (2006). Discrete component analysis. In Subspace, Latent
Structure and Feature Selection: Statistical and Optimization Perspectives Workshop
at SLSFS 2005, pp. 133, Bohinj, Slovenia. Springer.
Cafarella, M. J., Banko, M., & Etzioni, O. (2006). Relational web search. Tech. rep., University of Washington, Department of Computer Science and Engineering. Technical
Report 2006-04-02.
Cao, G., Nie, J.-Y., & Bai, J. (2005). Integrating word relationships into language models.
In Proceedings of the 28th Annual International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR 05), pp. 298305, New York, NY.
ACM.
Cao, H., Jiang, D., Pei, J., He, Q., Liao, Z., Chen, E., & Li, H. (2008). Context-aware query
suggestion by mining click-through and session data. In Proceeding of the 14th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD
08), pp. 875883. ACM.
Carroll, J. D., & Chang, J.-J. (1970). Analysis of individual differences in multidimensional
scaling via an n-way generalization of Eckart-Young decomposition. Psychometrika,
35 (3), 283319.
Chang, W., Pantel, P., Popescu, A.-M., & Gabrilovich, E. (2009). Towards intent-driven
bidterm suggestion. In Proceedings of WWW-09 (Short Paper), Madrid, Spain.
Charikar, M. S. (2002). Similarity estimation techniques from rounding algorithms. In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing (STOC
02), pp. 380388. ACM.
Chew, P., Bader, B., Kolda, T., & Abdelali, A. (2007). Cross-language information retrieval using PARAFAC2. In Proceedings of the 13th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining (KDD07), pp. 143152. ACM
Press.
Chiarello, C., Burgess, C., Richards, L., & Pollock, A. (1990). Semantic and associative
priming in the cerebral hemispheres: Some words do, some words dont . . . sometimes,
some places. Brain and Language, 38, 75104.
Chklovski, T., & Pantel, P. (2004). VerbOcean: Mining the web for fine-grained semantic
verb relations. In Proceedings of Experimental Methods in Natural Language Processing 2004 (EMNLP-04), pp. 3340, Barcelona, Spain.
177

fiTurney & Pantel

Choi, F. Y. Y. (2000). Advances in domain independent linear text segmentation. In
Proceedings of the 1st Meeting of the North American Chapter of the Association for
Computational Linguistics, pp. 2633.
Chu-carroll, J., & Carpenter, B. (1999). Vector-based natural language call routing. Computational Linguistics, 25 (3), 361388.
Church, K. (1995). One term or two?. In Proceedings of the 18th Annual International
ACM SIGIR Conference on Research and Development in Information Retrieval, pp.
310318.
Church, K., & Hanks, P. (1989). Word association norms, mutual information, and lexicography. In Proceedings of the 27th Annual Conference of the Association of Computational Linguistics, pp. 7683, Vancouver, British Columbia.
Clark, S., & Pulman, S. (2007). Combining symbolic and distributional models of meaning.
In Proceedings of AAAI Spring Symposium on Quantum Interaction, pp. 5255.
Collobert, R., & Weston, J. (2008). A unified architecture for natural language processing:
Deep neural networks with multitask learning. In Proceedings of the 25th International
Conference on Machine Learning (ICML-08), pp. 160167.
Croft, W. B. (1977). Clustering large files of documents using the single-link method.
Journal of the American Society for Information Science, 28 (6), 341344.
Crouch, C. J. (1988). A cluster-based approach to thesaurus construction. In Proceedings
of the 11th Annual International ACM SIGIR Conference, pp. 309320, Grenoble,
France.
Curran, J. R., & Moens, M. (2002). Improvements in automatic thesaurus extraction. In
Unsupervised Lexical Acquisition: Proceedings of the Workshop of the ACL Special
Interest Group on the Lexicon (SIGLEX), pp. 5966, Philadelphia, PA.
Cutting, D. R., Karger, D. R., Pedersen, J. O., & Tukey, J. W. (1992). Scatter/gather: a
cluster-based approach to browsing large document collections. In Proceedings of the
15th Annual International ACM SIGIR Conference, pp. 318329.
Dagan, I., Lee, L., & Pereira, F. C. N. (1999). Similarity-based models of word cooccurrence
probabilities. Machine Learning, 34 (13), 4369.
Dang, H. T., Lin, J., & Kelly, D. (2006). Overview of the TREC 2006 question answering
track. In Proceedings of the Fifteenth Text REtrieval Conference (TREC 2006).
Dasarathy, B. (1991). Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques.
IEEE Computer Society Press.
Davidov, D., & Rappoport, A. (2008). Unsupervised discovery of generic relationships using
pattern clusters and its evaluation by automatically generated SAT analogy questions.
In Proceedings of the 46th Annual Meeting of the ACL and HLT (ACL-HLT-08), pp.
692700, Columbus, Ohio.
Dean, J., & Ghemawat, S. (2008). MapReduce: Simplified data processing on large clusters.
Communications of the ACM, 51 (1), 107113.
178

fiFrom Frequency to Meaning

Deerwester, S. C., Dumais, S. T., Landauer, T. K., Furnas, G. W., & Harshman, R. A.
(1990). Indexing by latent semantic analysis. Journal of the American Society for
Information Science (JASIS), 41 (6), 391407.
Elsayed, T., Lin, J., & Oard, D. (2008). Pairwise document similarity in large collections
with mapreduce. In Proceedings of Association for Computational Linguistics and
Human Language Technology Conference 2008 (ACL-08: HLT), Short Papers, pp.
265268, Columbus, Ohio. Association for Computational Linguistics.
Erk, K. (2007). A simple, similarity-based model for selectional preferences. In Proceedings
of the 45th Annual Meeting of the Association of Computational Linguistics, pp. 216
223,, Prague, Czech Republic.
Erk, K., & Pado, S. (2008). A structured vector space model for word meaning in context.
In Proceedings of the 2008 Conference on Empirical Methods in Natural Language
Processing (EMNLP-08), pp. 897906, Honolulu, HI.
Fellbaum, C. (Ed.). (1998). WordNet: An Electronic Lexical Database. MIT Press.
Firth, J. R. (1957). A synopsis of linguistic theory 19301955. In Studies in Linguistic
Analysis, pp. 132. Blackwell, Oxford.
Foltz, P. W., Laham, D., & Landauer, T. K. (1999). The intelligent essay assessor: Applications to educational technology. Interactive Multimedia Electronic Journal of
Computer-Enhanced Learning, 1 (2).
Forman, G. (2003). An extensive empirical study of feature selection metrics for text classification. Journal of Machine Learning Research, 3, 12891305.
French, R. M., & Labiouse, C. (2002). Four problems with extracting human semantics from
large text corpora. In Proceedings of the 24th Annual Conference of the Cognitive
Science Society.
Furnas, G. W., Landauer, T. K., Gomez, L. M., & Dumais, S. T. (1983). Statistical semantics: Analysis of the potential performance of keyword information systems. Bell
System Technical Journal, 62 (6), 17531806.
Gentner, D. (1983). Structure-mapping: A theoretical framework for analogy. Cognitive
Science, 7 (2), 155170.
Gilbert, J. R., Moler, C., & Schreiber, R. (1992). Sparse matrices in MATLAB: Design and
implementation. SIAM Journal on Matrix Analysis and Applications, 13 (1), 333356.
Girju, R., Nakov, P., Nastase, V., Szpakowicz, S., Turney, P., & Yuret, D. (2007). Semeval2007 task 04: Classification of semantic relations between nominals. In Proceedings
of the Fourth International Workshop on Semantic Evaluations (SemEval 2007), pp.
1318, Prague, Czech Republic.
Gleich, D., & Zhukov, L. (2004). SVD based term suggestion and ranking system. In
Proceedings of the Fourth IEEE International Conference on Data Mining (ICDM
04), pp. 391394. IEEE Computer Society.
Golub, G. H., & Van Loan, C. F. (1996). Matrix Computations (Third edition). Johns
Hopkins University Press, Baltimore, MD.
179

fiTurney & Pantel

Gorman, J., & Curran, J. R. (2006). Scaling distributional similarity to large corpora. In
Proceedings of the 21st International Conference on Computational Linguistics and
the 44th annual meeting of the Association for Computational Linguistics (ACL 2006),
pp. 361368. Association for Computational Linguistics.
Gorrell, G. (2006). Generalized Hebbian algorithm for incremental singular value decomposition in natural language processing. In Proceedings of the 11th Conference of the
European Chapter of the Association for Computational Linguistics (EACL-06), pp.
97104.
Gospodnetic, O., & Hatcher, E. (2004). Lucene in Action. Manning Publications.
Grefenstette, G. (1994). Explorations in Automatic Thesaurus Discovery. Kluwer.
Harris, Z. (1954). Distributional structure. Word, 10 (23), 146162.
Harshman, R. (1970). Foundations of the parafac procedure: Models and conditions for an
explanatory multi-modal factor analysis. UCLA Working Papers in Phonetics, 16.
Hearst, M. (1997). Texttiling: Segmenting text into multi-paragraph subtopic passages.
Computational Linguistics, 23 (1), 3364.
Hirst, G., & St-Onge, D. (1998). Lexical chains as representations of context for the detection
and correction of malapropisms. In Fellbaum, C. (Ed.), WordNet: An Electronic
Lexical Database, pp. 305332. MIT Press.
Hofmann, T. (1999). Probabilistic Latent Semantic Indexing. In Proceedings of the 22nd
Annual ACM Conference on Research and Development in Information Retrieval (SIGIR 99), pp. 5057, Berkeley, California.
Huang, C.-K., Chien, L.-F., & Oyang, Y.-J. (2003). Relevant term suggestion in interactive
web search based on contextual information in query session logs. Journal of the
American Society for Information Science and Technology, 54 (7), 638649.
Hull, D. (1996). Stemming algorithms: A case study for detailed evaluation. Journal of the
American Society for Information Science, 47 (1), 7084.
Jain, A., Murty, N., & Flynn, P. (1999). Data clustering: A review. ACM Computing
Surveys, 31 (3), 264323.
Jarmasz, M., & Szpakowicz, S. (2003). Rogets thesaurus and semantic similarity. In
Proceedings of the International Conference on Recent Advances in Natural Language
Processing (RANLP-03), pp. 212219, Borovets, Bulgaria.
Jiang, J. J., & Conrath, D. W. (1997). Semantic similarity based on corpus statistics
and lexical taxonomy. In Proceedings of the International Conference on Research in
Computational Linguistics (ROCLING X), pp. 1933, Tapei, Taiwan.
Johnson, H., & Martin, J. (2003). Unsupervised learning of morphology for English and
Inuktitut. In Proceedings of HLT-NAACL 2003, pp. 4345.
Jones, M. P., & Martin, J. H. (1997). Contextual spelling correction using latent semantic analysis. In Proceedings of the Fifth Conference on Applied Natural Language
Processing, pp. 166173, Washington, DC.
180

fiFrom Frequency to Meaning

Jones, R., Rey, B., Madani, O., & Greiner, W. (2006). Generating query substitutions. In
Proceedings of the 15th international conference on World Wide Web (WWW 06),
pp. 387396, New York, NY. ACM.
Jones, W. P., & Furnas, G. W. (1987). Pictures of relevance: A geometric analysis of
similarity measures. Journal of the American Society for Information Science, 38 (6),
420442.
Kanerva, P. (1993). Sparse distributed memory and related models. In Hassoun, M. H.
(Ed.), Associative neural memories, pp. 5076. Oxford University Press, New York,
NY.
Karlgren, J., & Sahlgren, M. (2001). From words to understanding. In Uesaka, Y., Kanerva,
P., & Asoh, H. (Eds.), Foundations of Real-World Intelligence, pp. 294308. CSLI
Publications.
Kim, S.-M., Pantel, P., Chklovski, T., & Pennacchiotti, M. (2006). Automatically assessing
review helpfulness. In Proceedings of the 2006 Conference on Empirical Methods in
Natural Language Processing, pp. 423430.
Kolda, T., & Bader, B. (2009). Tensor decompositions and applications. SIAM Review,
51 (3), 455500.
Konchady, M. (2008). Building Search Applications: Lucene, LingPipe, and Gate. Mustru
Publishing.
Kraaij, W., & Pohlmann, R. (1996). Viewing stemming as recall enhancement. In Proceedings of the 19th Annual International ACM SIGIR Conference, pp. 4048.
Lakoff, G. (1987). Women, Fire, and Dangerous Things. University Of Chicago Press,
Chicago, IL.
Landauer, T. K. (2002). On the computational basis of learning and cognition: Arguments
from LSA. In Ross, B. H. (Ed.), The Psychology of Learning and Motivation: Advances
in Research and Theory, Vol. 41, pp. 4384. Academic Press.
Landauer, T. K., & Dumais, S. T. (1997). A solution to Platos problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge.
Psychological Review, 104 (2), 211240.
Landauer, T. K., & Littman, M. L. (1990). Fully automatic cross-language document
retrieval using latent semantic indexing. In Proceedings of the Sixth Annual Conference
of the UW Centre for the New Oxford English Dictionary and Text Research, pp. 31
38, Waterloo, Ontario.
Landauer, T. K., McNamara, D. S., Dennis, S., & Kintsch, W. (2007). Handbook of Latent
Semantic Analysis. Lawrence Erlbaum, Mahwah, NJ.
Lavrenko, V., & Croft, W. B. (2001). Relevance based language models. In Proceedings of
the 24th Annual International ACM SIGIR Conference on Research and Development
in Information Retrieval (SIGIR 01), pp. 120127, New York, NY. ACM.
Leacock, C., & Chodrow, M. (1998). Combining local context and WordNet similarity for
word sense identification. In Fellbaum, C. (Ed.), WordNet: An Electronic Lexical
Database. MIT Press.
181

fiTurney & Pantel

Leacock, C., Towell, G., & Voorhees, E. (1993). Corpus-based statistical sense resolution.
In Proceedings of the ARPA Workshop on Human Language Technology, pp. 260265.
Lee, D. D., & Seung, H. S. (1999). Learning the parts of objects by nonnegative matrix
factorization. Nature, 401, 788791.
Lee, L. (1999). Measures of distributional similarity. In Proceedings of the 37th Annual
Meeting of the Association for Computational Linguistics, pp. 2532.
Lemaire, B., & Denhiere, G. (2006). Effects of high-order co-occurrences on word semantic
similarity. Current Psychology Letters: Behaviour, Brain & Cognition, 18 (1).
Lin, D. (1998). Automatic retrieval and clustering of similar words. In roceedings of the
17th international conference on Computational linguistics, pp. 768774. Association
for Computational Linguistics.
Lin, D., & Pantel, P. (2001). DIRT  discovery of inference rules from text. In Proceedings
of ACM SIGKDD Conference on Knowledge Discovery and Data Mining 2001, pp.
323328.
Linden, G., Smith, B., & York, J. (2003). Amazon.com recommendations: Item-to-item
collaborative filtering. IEEE Internet Computing, 7680.
Liu, X., & Croft, W. B. (2005). Statistical language modeling for information retrieval.
Annual Review of Information Science and Technology, 39, 328.
Lovins, J. B. (1968). Development of a stemming algorithm. Mechanical Translation and
Computational Linguistics, 11, 2231.
Lowe, W. (2001). Towards a theory of semantic space. In Proceedings of the Twenty-first
Annual Conference of the Cognitive Science Society, pp. 576581.
Lund, K., & Burgess, C. (1996). Producing high-dimensional semantic spaces from lexical
co-occurrence. Behavior Research Methods, Instruments, and Computers, 28 (2), 203
208.
Lund, K., Burgess, C., & Atchley, R. A. (1995). Semantic and associative priming in highdimensional semantic space. In Proceedings of the 17th Annual Conference of the
Cognitive Science Society, pp. 660665.
Manning, C., & Schutze, H. (1999). Foundations of Statistical Natural Language Processing.
MIT Press, Cambridge, MA.
Manning, C. D., Raghavan, P., & Schutze, H. (2008). Introduction to Information Retrieval.
Cambridge University Press, Cambridge, UK.
Miller, G., Leacock, C., Tengi, R., & Bunker, R. (1993). A semantic concordance. In
Proceedings of the 3rd DARPA Workshop on Human Language Technology, pp. 303
308.
Minnen, G., Carroll, J., & Pearce, D. (2001). Applied morphological processing of English.
Natural Language Engineering, 7 (3), 207223.
Mitchell, J., & Lapata, M. (2008). Vector-based models of semantic composition. In Proceedings of ACL-08: HLT, pp. 236244, Columbus, Ohio. Association for Computational
Linguistics.
182

fiFrom Frequency to Meaning

Mitchell, T. (1997). Machine Learning. McGraw-Hill, Columbus, OH.
Mohammad, S., & Hirst, G. (2006). Distributional measures of concept-distance: A taskoriented evaluation. In Proceedings of the Conference on Empirical Methods in Natural
Language Processing (EMNLP-2006), pp. 3543.
Monay, F., & Gatica-Perez, D. (2003). On image auto-annotation with latent space models.
In Proceedings of the Eleventh ACM International Conference on Multimedia, pp.
275278.
Morris, J., & Hirst, G. (2004). Non-classical lexical semantic relations. In Workshop on
Computational Lexical Semantics, HLT-NAACL-04, Boston, MA.
Nakov, P., & Hearst, M. (2007). UCB: System description for SemEval Task 4. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval 2007),
pp. 366369, Prague, Czech Republic.
Nakov, P., & Hearst, M. (2008). Solving relational similarity problems using theweb as a
corpus. In Proceedings of ACL-08: HLT, pp. 452460, Columbus, Ohio.
Nastase, V., Sayyad-Shirabad, J., Sokolova, M., & Szpakowicz, S. (2006). Learning nounmodifier semantic relations with corpus-based and WordNet-based features. In Proceedings of the 21st National Conference on Artificial Intelligence (AAAI-06), pp.
781786.
Nastase, V., & Szpakowicz, S. (2003). Exploring noun-modifier semantic relations. In
Fifth International Workshop on Computational Semantics (IWCS-5), pp. 285301,
Tilburg, The Netherlands.
Niwa, Y., & Nitta, Y. (1994). Co-occurrence vectors from corpora vs. distance vectors from
dictionaries. In Proceedings of the 15th International Conference On Computational
Linguistics, pp. 304309, Kyoto, Japan.
Nosofsky, R. (1986). Attention, similarity, and the identification-categorization relationship.
Journal of Experimental Psychology: General, 115 (1), 3957.
Ogden, C. K. (1930). Basic English: A General Introduction with Rules and Grammar.
Kegan Paul, Trench, Trubner and Co.
Pado, S., & Lapata, M. (2003). Constructing semantic space models from parsed corpora.
In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pp. 128135, Sapporo, Japan.
Pado, S., & Lapata, M. (2007). Dependency-based construction of semantic space models.
Computational Linguistics, 33 (2), 161199.
Pang, B., Lee, L., & Vaithyanathan, S. (2002). Thumbs up? sentiment classification using
machine learning techniques. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP), pp. 7986, Philadelphia, PA.
Pantel, P. (2005). Inducing ontological co-occurrence vectors. In Proceedings of Association
for Computational Linguistics (ACL-05), pp. 125132.
Pantel, P., & Lin, D. (1998). Spamcop: A spam classification and organization program. In
Learning for Text Categorization: Papers from the AAAI 1998 Workshop, pp. 9598.
183

fiTurney & Pantel

Pantel, P., & Lin, D. (2002a). Discovering word senses from text. In Proceedings of the
Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, pp. 613619, Edmonton, Canada.
Pantel, P., & Lin, D. (2002b). Document clustering with committees. In Proceedings of the
25th Annual International ACM SIGIR Conference, pp. 199206.
Pasca, M., Lin, D., Bigham, J., Lifchits, A., & Jain, A. (2006). Names and similarities on
the Web: Fact extraction in the fast lane. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pp.
809816, Sydney, Australia.
Patwardhan, S., & Pedersen, T. (2006). Using wordnet-based context vectors to estimate
the semantic relatedness of concepts. In Proceedings of the Workshop on Making
Sense of Sense at the 11th Conference of the European Chapter of the Association for
Computational Linguistics (EACL-2006), pp. 18.
Pedersen, T. (2006). Unsupervised corpus-based methods for WSD. In Word Sense Disambiguation: Algorithms and Applications, pp. 133166. Springer.
Pennacchiotti, M., Cao, D. D., Basili, R., Croce, D., & Roth, M. (2008). Automatic induction
of FrameNet lexical units. In Proceedings of the 2008 Conference on Empirical Methods
in Natural Language Processing (EMNLP-08), pp. 457465, Honolulu, Hawaii.
Pennacchiotti, M., & Pantel, P. (2006). Ontologizing semantic relations. In Proceedings of
the 21st International Conference on Computational Linguistics and the 44th annual
meeting of the Association for Computational Linguistics, pp. 793800. Association
for Computational Linguistics.
Pereira, F., Tishby, N., & Lee, L. (1993). Distributional clustering of English words. In
Proceedings of the 31st Annual Meeting on Association for Computational Linguistics,
pp. 183190.
Porter, M. (1980). An algorithm for suffix stripping. Program, 14 (3), 130137.
Rabin, M. O. (1981). Fingerprinting by random polynomials. Tech. rep., Center for research
in Computing technology, Harvard University. Technical Report TR-15-81.
Rapp, R. (2003). Word sense discovery based on sense descriptor dissimilarity. In Proceedings of the Ninth Machine Translation Summit, pp. 315322.
Ravichandran, D., Pantel, P., & Hovy, E. (2005). Randomized algorithms and nlp: using
locality sensitive hash function for high speed noun clustering. In Proceedings of the
43rd Annual Meeting of the Association for Computational Linguistics (ACL 05), pp.
622629, Morristown, NJ. Association for Computational Linguistics.
Resnick, P., Iacovou, N., Suchak, M., Bergstrom, P., & Riedl, J. (1994). Grouplens: An open
architecture for collaborative filtering of netnews. In Proceedings of the ACM 1994
Conference on Computer Supported Cooperative Work, pp. 175186. ACM Press.
Resnik, P. (1995). Using information content to evaluate semantic similarity in a taxonomy.
In Proceedings of the 14th International Joint Conference on Artificial Intelligence
(IJCAI-95), pp. 448453, San Mateo, CA. Morgan Kaufmann.
184

fiFrom Frequency to Meaning

Rosario, B., & Hearst, M. (2001). Classifying the semantic relations in noun-compounds
via a domain-specific lexical hierarchy. In Proceedings of the 2001 Conference on
Empirical Methods in Natural Language Processing (EMNLP-01), pp. 8290.
Rosario, B., Hearst, M., & Fillmore, C. (2002). The descent of hierarchy, and selection in
relational semantics. In Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL-02), pp. 247254.
Rosch, E., & Lloyd, B. (1978). Cognition and Categorization. Lawrence Erlbaum, Hillsdale,
NJ.
Ruge, G. (1997). Automatic detection of thesaurus relations for information retrieval applications. In Freksa, C., Jantzen, M., & Valk, R. (Eds.), Foundations of Computer
Science, pp. 499506. Springer.
Sahami, M., Dumais, S., Heckerman, D., & Horvitz, E. (1998). A Bayesian approach to
filtering junk e-mail. In Proceedings of the AAAI-98 Workshop on Learning for Text
Categorization.
Sahlgren, M. (2005). An introduction to random indexing. In Proceedings of the Methods
and Applications of Semantic Indexing Workshop at the 7th International Conference
on Terminology and Knowledge Engineering (TKE), Copenhagen, Denmark.
Sahlgren, M. (2006). The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in high-dimensional vector spaces.
Ph.D. thesis, Department of Linguistics, Stockholm University.
Salton, G. (1971). The SMART retrieval system: Experiments in automatic document processing. Prentice-Hall, Upper Saddle River, NJ.
Salton, G., & Buckley, C. (1988). Term-weighting approaches in automatic text retrieval.
Information Processing and Management, 24 (5), 513523.
Salton, G., Wong, A., & Yang, C.-S. (1975). A vector space model for automatic indexing.
Communications of the ACM, 18 (11), 613620.
Sarawagi, S., & Kirpal, A. (2004). Efficient set joins on similarity predicates. In Proceedings of the 2004 ACM SIGMOD International Conference on Management of Data
(SIGMOD 04), pp. 743754, New York, NY. ACM.
Scholkopf, B., Smola, A. J., & Muller, K.-R. (1997). Kernel principal component analysis. In
Proceedings of the International Conference on Artificial Neural Networks (ICANN1997), pp. 583588, Berlin.
Schutze, H. (1998). Automatic word sense discrimination. Computational Linguistics, 24 (1),
97124.
Schutze, H., & Pedersen, J. (1993). A vector model for syntagmatic and paradigmatic
relatedness. In Making Sense of Words: Proceedings of the Conference, pp. 104113,
Oxford, England.
Sebastiani, F. (2002). Machine learning in automated text categorization. ACM Computing
Surveys (CSUR), 34 (1), 147.
Shannon, C. (1948). A mathematical theory of communication. Bell System Technical
Journal, 27, 379423, 623656.
185

fiTurney & Pantel

Singhal, A., Salton, G., Mitra, M., & Buckley, C. (1996). Document length normalization.
Information Processing and Management, 32 (5), 619633.
Smith, E., Osherson, D., Rips, L., & Keane, M. (1988). Combining prototypes: A selective
modification model. Cognitive Science, 12 (4), 485527.
Snow, R., Jurafsky, D., & Ng, A. Y. (2006). Semantic taxonomy induction from heterogenous evidence. In Proceedings of the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the ACL, pp. 801808.
Sparck Jones, K. (1972). A statistical interpretation of term specificity and its application
in retrieval. Journal of Documentation, 28 (1), 1121.
Spearman, C. (1904). General intelligence, objectively determined and measured. American Journal of Psychology, 15, 201293.
Sproat, R., & Emerson, T. (2003). The first international Chinese word segmentation bakeoff. In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing,
pp. 133143, Sapporo, Japan.
Stone, P. J., Dunphy, D. C., Smith, M. S., & Ogilvie, D. M. (1966). The General Inquirer:
A Computer Approach to Content Analysis. MIT Press, Cambridge, MA.
Tan, B., & Peng, F. (2008). Unsupervised query segmentation using generative language
models and Wikipedia. In Proceeding of the 17th international conference on World
Wide Web (WWW 08), pp. 347356, New York, NY. ACM.
Tellex, S., Katz, B., Lin, J., Fern, A., & Marton, G. (2003). Quantitative evaluation of
passage retrieval algorithms for question answering. In Proceedings of the 26th Annual
International ACM SIGIR Conference on Research and Development in Information
Retrieval (SIGIR), pp. 4147.
Tucker, L. R. (1966). Some mathematical notes on three-mode factor analysis. Psychometrika, 31 (3), 279311.
Turney, P. D. (2001). Mining the Web for synonyms: PMI-IR versus LSA on TOEFL. In
Proceedings of the Twelfth European Conference on Machine Learning (ECML-01),
pp. 491502, Freiburg, Germany.
Turney, P. D. (2005). Measuring semantic similarity by latent relational analysis. In Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence
(IJCAI-05), pp. 11361141, Edinburgh, Scotland.
Turney, P. D. (2006). Similarity of semantic relations. Computational Linguistics, 32 (3),
379416.
Turney, P. D. (2007). Empirical evaluation of four tensor decomposition algorithms. Tech.
rep., Institute for Information Technology, National Research Council of Canada.
Technical Report ERB-1152.
Turney, P. D. (2008a). The latent relation mapping engine: Algorithm and experiments.
Journal of Artificial Intelligence Research, 33, 615655.
Turney, P. D. (2008b). A uniform approach to analogies, synonyms, antonyms, and associations. In Proceedings of the 22nd International Conference on Computational
Linguistics (Coling 2008), pp. 905912, Manchester, UK.
186

fiFrom Frequency to Meaning

Turney, P. D., & Littman, M. L. (2003). Measuring praise and criticism: Inference of
semantic orientation from association. ACM Transactions on Information Systems,
21 (4), 315346.
Turney, P. D., & Littman, M. L. (2005). Corpus-based learning of analogies and semantic
relations. Machine Learning, 60 (13), 251278.
Turney, P. D., Littman, M. L., Bigham, J., & Shnayder, V. (2003). Combining independent
modules to solve multiple-choice synonym and analogy problems. In Proceedings of
the International Conference on Recent Advances in Natural Language Processing
(RANLP-03), pp. 482489, Borovets, Bulgaria.
Van de Cruys, T. (2009). A non-negative tensor factorization model for selectional preference
induction. In Proceedings of the Workshop on Geometric Models for Natural Language
Semantics (GEMS-09), pp. 8390, Athens, Greece.
van Rijsbergen, C. J. (2004). The Geometry of Information Retrieval. Cambridge University
Press, Cambridge, UK.
van Rijsbergen, C. J. (1979). Information Retrieval. Butterworths.
Veale, T. (2003). The analogical thesaurus. In Proceedings of the 15th Innovative Applications of Artificial Intelligence Conference (IAAI 2003), pp. 137142, Acapulco,
Mexico.
Veale, T. (2004). WordNet sits the SAT: A knowledge-based approach to lexical analogy. In
Proceedings of the 16th European Conference on Artificial Intelligence (ECAI 2004),
pp. 606612, Valencia, Spain.
Vozalis, E., & Margaritis, K. (2003). Analysis of recommender systems algorithms. In
Proceedings of the 6th Hellenic European Conference on Computer Mathematics and
its Applications (HERCMA-2003), Athens, Greece.
Vyas, V., & Pantel, P. (2009). Semi-automatic entity set refinement. In Proceedings of
NAACL-09, Boulder, CO.
Weaver, W. (1955). Translation. In Locke, W., & Booth, D. (Eds.), Machine Translation
of Languages: Fourteen Essays. MIT Press, Cambridge, MA.
Weeds, J., Weir, D., & McCarthy, D. (2004). Characterising measures of lexical distributional similarity. In Proceedings of the 20th International Conference on Computational Linguistics (COLING 04), pp. 10151021. Association for Computational
Linguistics.
Wei, X., Peng, F., & Dumoulin, B. (2008). Analyzing web text association to disambiguate
abbreviation in queries. In Proceedings of the 31st Annual International ACM SIGIR
Conference on Research and Development in Information Retrieval (SIGIR 08), pp.
751752, New York, NY. ACM.
Wen, J.-R., Nie, J.-Y., & Zhang, H.-J. (2001). Clustering user queries of a search engine. In
Proceedings of the 10th International Conference on World Wide Web (WWW 01),
pp. 162168, New York, NY. ACM.
Widdows, D. (2004). Geometry and Meaning. Center for the Study of Language and
Information, Stanford, CA.
187

fiTurney & Pantel

Widdows, D., & Ferraro, K. (2008). Semantic vectors: A scalable open source package and
online technology management application. In Proceedings of the Sixth International
Conference on Language Resources and Evaluation (LREC 2008), pp. 11831190.
Witten, I. H., & Frank, E. (2005). Data Mining: Practical Machine Learning Tools and
Techniques with Java Implementations. Morgan Kaufmann, San Francisco.
Wittgenstein, L. (1953). Philosophical Investigations. Blackwell. Translated by G.E.M.
Anscombe.
Wolfe, M. B. W., Schreiner, M. E., Rehder, B., Laham, D., Foltz, P. W., Kintsch, W., &
Landauer, T. K. (1998). Learning from text: Matching readers and texts by latent
semantic analysis. Discourse Processes, 25, 309336.
Yang, Y. (1999). An evaluation of statistical approaches to text categorization. Information
Retrieval, 1 (1), 6990.
Yuret, D., & Yatbaz, M. A. (2009). The noisy channel model for unsupervised word sense
disambiguation. Computational Linguistics. Under review.
Zamir, O., & Etzioni, O. (1999). Grouper: a dynamic clustering interface to Web search
results. Computer Networks: The International Journal of Computer and Telecommunications Networking, 31 (11), 13611374.
Zhao, Y., & Karypis, G. (2002). Evaluation of hierarchical clustering algorithms for document datasets. In Proceedings of the Eleventh International Conference on Information and Knowledge Management, pp. 515524, McLean, Virginia.

188

fiJournal of Artificial Intelligence Research 37 (2010) 437477

Submitted 8/09; published 3/10

Reasoning about the Transfer of Control
Wiebe van der Hoek
Dirk Walther
Michael Wooldridge

W IEBE .VAN -D ER -H OEK @ LIV. AC . UK
DWALTHER @ LIV. AC . UK
MJW @ LIV. AC . UK

Department of Computer Science
University of Liverpool, UK

Abstract
We present DCL - PC: a logic for reasoning about how the abilities of agents and coalitions
of agents are altered by transferring control from one agent to another. The logical foundation
of DCL - PC is CL - PC, a logic for reasoning about cooperation in which the abilities of agents and
coalitions of agents stem from a distribution of atomic Boolean variables to individual agents  the
choices available to a coalition correspond to assignments to the variables the coalition controls.
The basic modal constructs of CL - PC are of the form coalition C can cooperate to bring about .
DCL - PC extends CL - PC with dynamic logic modalities in which atomic programs are of the form
agent i gives control of variable p to agent j; as usual in dynamic logic, these atomic programs
may be combined using sequence, iteration, choice, and test operators to form complex programs.
By combining such dynamic transfer programs with cooperation modalities, it becomes possible to
reason about how the power of agents and coalitions is affected by the transfer of control. We give
two alternative semantics for the logic: a direct semantics, in which we capture the distributions
of Boolean variables to agents; and a more conventional Kripke semantics. We prove that these
semantics are equivalent, and then present an axiomatization for the logic. We investigate the
computational complexity of model checking and satisfiability for DCL - PC, and show that both
problems are PSPACE-complete (and hence no worse than the underlying logic CL - PC). Finally, we
investigate the characterisation of control in DCL - PC. We distinguish between first-order control 
the ability of an agent or coalition to control some state of affairs through the assignment of values
to the variables under the control of the agent or coalition  and second-order control  the ability
of an agent to exert control over the control that other agents have by transferring variables to other
agents. We give a logical characterisation of second-order control.

1. Introduction
In recent years, there has been much activity in the development of logics for reasoning about the
strategic and cooperative abilities of agents in game-like multi-agent systems. Coalition Logic (Pauly,
2001) and Alternating-time Temporal Logic (ATL) Alur, Henzinger, and Kupferman (2002) are
perhaps the best-known examples of such work. These logics have been widely used as a base
from which to investigate reasoning about cooperation in multi-agent systems (van der Hoek &
Wooldridge, 2003; Jamroga & van der Hoek, 2004; Goranko & Jamroga, 2004).
Although they differ on details, the basic construct in both Coalition Logic and ATL is the
cooperation modality, a construct that is written in ATL as hhCii. The intended meaning of this
expression is that the coalition C can cooperate in such a way as to ensure that, no matter what
the agents outside C do, the property  becomes true. Another way to think about hhCii is as
meaning that coalition C has the collective power to ensure that . It is often assumed that powers
are additive, in the sense that the powers of a coalition derive from the powers of coalition members,
c
2010
AI Access Foundation. All rights reserved.

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

and that adding an agent to a coalition does not reduce the powers of that coalition. However, the
origin of an individual agents powers  that is, where these powers derive from  is rarely discussed
in the cooperation logic literature.
One very natural interpretation for powers or abilities in computational systems arises from
considering which system components have the ability to assign values to the variables making
up the overall system state. Power, in this sense, equates to the ability to choose a value for a
particular variable. Motivated by this observation, van der Hoek and Wooldridge developed CL - PC,
a cooperation logic in which powers are specified by allocating to every agent a set of Boolean
variables: the choices (and hence powers) available to a coalition then correspond to the possible
assignments of truth or falsity that may be made to the variables under their control (van der Hoek
& Wooldridge, 2005b). The CL - PC expression C  means that coalition C can assign values to
the variables under its control in such a way as to make  true. Van der Hoek and Wooldridge
gave a complete axiomatization of CL - PC, and showed that the model checking and satisfiability
problems for the logic are both PSPACE-complete; they also investigated how CL - PC could be used
to characterise the closely related notion of control. However, one drawback of CL - PC is that the
power structure underpinning the logic  the distribution of variables to agents  is assumed to be
fixed, and hence coalitional powers are static in CL - PC.
Ultimately, of course, the assumption that powers are static is not realistic. For example, the
explicit transfer of power and control is a fundamental component of most human organisations,
enabling them to avoid bottlenecks with respect to centralised power and control. Moreover, in
open environments, where agents join and leave a system at run-time, it may not be possible to
know in advance which agents are to fulfill which roles, and so static power allocation schemes are
simply not appropriate for such environments. If software agents are to be deployed in environments
where power structures are dynamic, then it is important to consider the issues of representing and
reasoning about them, and it is to this issue that we address ourselves in the present paper.
We study a variant of CL - PC that allows us to explicitly reason about dynamic power structures.
The logic DCL - PC extends CL - PC with dynamic logic operators (Harel, Kozen, & Tiuryn, 2000), in
which atomic programs are of the form i ;p j, which is read as agent i gives control of variable
p to agent j. The pre-condition of such a program is that variable p is in agent is allocation of
variables, and executing the program has the effect of transferring variable p from agent i to agent
j. Thus the dynamic component of DCL - PC is concerned with transferring power in systems, and
by using the logic, we can reason about how the abilities of agents and coalitions are affected by
such transfers. Note that, as in conventional dynamic logic, atomic programs may be combined
in DCL - PC with the usual sequential composition (;), non-deterministic choice (), test (?),
and iteration ( ) operations, to form complex programs. With these features, DCL - PC provides
a rich framework through which to represent and reason about systems in which power/control is
dynamically allocated.
In the remainder of the paper, following an introduction to the logic, we make four main contributions with respect to DCL - PC:
 First, in Section 2, we give two alternative semantics for the logic: a direct semantics, in
which models directly represent the allocation of propositional variables to the agents that
control them, and a more conventional Kripke semantics. We prove that these two semantics
are equivalent.
438

fiR EASONING A BOUT THE T RANSFER OF C ONTROL

 Second, we give an axiomatization of DCL - PC in Section 3, and show that this axiomatization
is sound and complete (with respect to both semantics).
 Third, we show in Section 4 that, despite the apparently additional expressive power provided
by the dynamic component of DCL - PC, the satisfiability and model checking problems for
DCL - PC are no more complex than the corresponding problems for CL - PC (van der Hoek &
Wooldridge, 2005b): they are both PSPACE-complete.
 Fourth, we distinguish between first-order control and second-order control in Section 5.
While first-order control, as introduced and studied by van der Hoek and Wooldridge (2005b),
is the ability to control some state of affairs by assigning values to variables, second-order
control is the ability of an agent to exert control over the ability of other agents to control
states of affairs. Agents and coalitions can exercise second-order control by transferring variables under their control to other agents. After informally discussing and introducing secondorder control, we develop a logical characterisation of it, in the sense that we characterise the
formulae over which an agent has second-order control.
We conclude in brief with some comments on related work and conclusions. Note that we omit a
detailed introduction to cooperation logics and in particular the motivation behind CL - PC, as this
was done by van der Hoek and Wooldridge (2005b).

2. The Logic DCL - PC
In this section, we define the logic DCL - PC.
2.1 An Informal Introduction
We begin with an informal introduction; readers who are familiar with both CL - PC and dynamic
logic may wish to skim or skip completely this introductory section.
As we noted earlier, DCL - PC extends the logic CL - PC,
"#####$
"#####$
and we begin by briefly reviewing this logic. CL - PC is
'
*
"
!
intended to allow us to reason about domains containing
+
(
a collection of agents, and a collection of propositional
variables; let A = {1, . . . , n} denote the set of agents,
,
and P denote the variables. It is assumed in CL - PC that
"#####$
"#####$
each agent in the system controls some subset of the vari&
%
)
ables P. To keep things simple, it is assumed that agents
exercise unique control: every variable is controlled by
.
exactly one agent, and so the variables P are partitioned
among the agents A. Where i  A is an agent, we denote the variables under the control of i by Pi , so Pi  P.
Figure 1: A typical scenario.
The abilities or powers of an agent in such a scenario correspond to the assignments of truth or falsity that it can make to the variables under its control.
Figure 1 illustrates a typical scenario: we have four agents, A = {1, 2, 3, 4}, and eight variables,
P = {p, q, r, s, t, u, v, w}. Agent 1 controls variables p and q, (P 1 = {p, q}), while agent 2 controls
variable r, (P2 = {r}), and so on. In the scenario illustrated, variables p, q, s, and w have the value
439

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

1 (i.e., true), while all other variables have the value 0 (false). The language of CL - PC is intended to allow us to represent and reason about such scenarios. To represent the values of variables,
we use propositional logic, and so the following formula completely characterises the values of the
variables in this scenario:
p  q  s  w  r  t  u  v
Agents are able to change the value of the variables
under their control, and to represent these abilities in CL PC we use a contingent ability operator (van der Hoek
& Wooldridge, 2005b): the expression C  means that,
under the assumption that the world remains otherwise
unchanged, the set of agents C can modify the value of
their variables so as to make  true. With respect to the
scenario in Figure 1, for example, we have

"-----.
"

"-----.

%

(

&

)

!

*
$

'

+

#

,

1,2 (p  r  q).
"

This is because agent 1 can leave variable p set at true
while making variable q false, while agent 2 makes variable r true: this will result in the formula p  r  q being
true.
The fact that no matter what coalition C do,  will
remain true is expressed by 2C . In the scenario in Figure 1, no matter what agent 1 does, r will remain false
(assuming again that no other agent acts). Thus we have:

%
$

&

!

)
*

$

21 r.

(

"

%
'

+

#

,

As shown elsewhere (van der Hoek & Wooldridge, 2005b),
and as defined below, other types of ability operators may
also be defined.
Figure 2: The effect of executing an
Thus far, the operators we have introduced have all
atomic transfer program.
been part of the CL - PC language. Let us now start to introduce the dynamic aspects of the language, specific to
DCL - PC . First, we have the idea of an atomic transfer program, written i ; p j, meaning agent i
transfers the power to choose a truth value for the variable p to agent j. Now, it will be possible to
execute a program i ;p j iff the variable p is actually under the control of agent i. For example, with
respect to Figure 1, the programs 1 ;p 2 and 2 ;r 1 are executable, while the program 1 ;r 2
is not (since r is not under the control of 1). The (fairly obvious) effect of executing the program
1 ;p 2 is illustrated in Figure 2; note that the actual value of the variable being transferred is
unchanged by the transfer.
In DCL - PC, we allow atomic programs to be combined together to make more complex programs
using constructs from dynamic logic: ; (for sequential composition),  (iteration), and ? (the
test operator). The simplest of these is sequential composition: for example, the program
1 ;p 2; 2 ;r 1
440

fiR EASONING A BOUT THE T RANSFER OF C ONTROL

means that, first, agent 1 gives variable p to 2, and then, agent 2 gives r to 1. The operator  is a
non-deterministic choice operator. If 1 and 2 are transfer programs, then 1  2 means do either
program 1 or 2 . The  operator is used to define iteration: the expression   means execute
the program  zero or more times (it is not defined exactly how many times  will be executed).
Finally, the ? is used to perform tests. The program ? can be executed in a particular scenario
only if the formula  is true of that scenario. To illustrate how these operator work, consider the
following example programs.
p?; 1 ;p 2
This first program says if p is true, then agent 1 gives p to agent 2. Now, since p is true in the
scenario in Figure 1, then this program can be executed from the scenario in Figure 1, and the net
result is the same as the final scenario in Figure 2.
The following program uses non-deterministic choice, and essentially says agent 1 gives either
p or q to 2.
(1 ;p 2)  (1 ;q 2)
As is usual with dynamic logic, we can define the iteration and selection constructs used in conventional imperative programming languages from these basic program constructs. For example, the
conventional programming construct
while  do 
can be defined using the following transfer program construct (see, e.g., Harel et al., 2000):
(?;  ) ; ?
The next step is to see how transfer programs are incorporated with the ability constructs of CL - PC.
To be able to refer to transfer programs and their properties from within the language of DCL - PC,
we use the dynamic operators h i and [ ]. These operators play the same role in DCL - PC
that they play in conventional dynamic logic (Harel et al., 2000). Thus a formula h i asserts that
there exists a computation of program  , starting from the current situation, such that after  has
terminated  holds. Note that h i does not assert that  is guaranteed to terminate, merely that
it has at least one terminating computation. And moreover, it does not state that  is satisfied after
every terminating computation of  ; merely that there is some terminating computation that will
end in a situation satisfying . Thus h i acts as an existential quantifier over the computations of
 . The operator [ ] is a universal quantifier over the computations of  . It asserts that after every
terminating computation of  , the property  holds. Note that it does not assert that  in fact has
any terminating computations.
As an example of the use of these constructs, the following formula asserts that if agent 1 gives
either p or q to 2, then 2 will be able to achieve (p  q)  r.
[(1 ;p 2)  (1 ;q 2)]2 (p  q)  r
It is easy to see that this formula expresses a true property of the scenario in Figure 1: the program
(1 ;p 2)  (1 ;q 2) is executable in this scenario, and after it is executed, agent 2 will control
variable r and either variable p or variable q. Agent 2 will thus be able to make (p  q)  r true.
To conclude this introductory section, consider the following more complex example. The
following DCL - PC formula asserts that it is possible for agent i to give away its variables to agent j,
441

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

non-deterministically choosing one variable at a time, until agent j has the ability to achieve .
[
hwhile j  do
i ;p ji>
pPi

2.2 Syntax
Formally, the language of DCL - PC is formed with respect to a (fixed, finite, non-empty) set A of
agents, and a (fixed, finite, non-empty) set P of propositional variables. Figure 3 defines the syntax
of DCL - PC. We use > as a logical constant for truth,  for negation, and  for disjunction. As
usual, we define the remaining connectives of classical propositional logic as abbreviations:





=

=

=

=


>
(  )
  
(  )  (  ).

Additionally, where  is a set of DCL - PC formulas, we write 5  to mean that exactly one
member of  is true:
_
^
(1  2 ).
5  =




1 6=2 

Where  = {1 , 2 . . . n }, we will also write 1 5 2 5    5 n for 5 .
With respect to transfer programs, other constructs from conventional imperative programs may
be defined as follows (Harel et al., 2000):
if  then 1 else 2
while  do 
repeat  until 
skip
fail

=

=

=

=

=


((?; 1 )  (?; 2 ))
((?;  ) ; ?)
 ; (?;  ) ; ?
>?
?

Where there is no possibility of confusion, we will omit set brackets for cooperation modalities, for
example writing 1,2 rather than {1,2} . A DCL - PC formula containing no modalities is said to be
an objective formula.
Let P() denote the set of propositional variables occurring in DCL - PC formula , and let A()
denote the set of all agents named in  (i.e., A() is the union of all the coalitions occurring in
cooperation modalities in  and all the agents occurring in transfer programs in ).
Although the  operator is useful to define programs succinctly, we will in fact see in Theorem 2
that it is superfluous, which essentially uses the fact that our set of atoms and agents are finite.
2.3 Direct Semantics
We now introduce the first of our two semantics for DCL - PC. We call this semantics a direct
semantics because it is directly based on the intuitive model that we introduced earlier: every agent
has unique control of some set of propositional variables, with every variable being controlled by
some agent.
Given a fixed, finite and non-empty set A of agents, and a fixed, finite and non-empty set P
of propositional variables, we say an allocation of P to A is an indexed tuple  = hP 1 , . . . , Pn i,
442

fiR EASONING A BOUT THE T RANSFER OF C ONTROL

DCL - PC
DCL

formulas:
::= >
| p
| DCL
| DCL  DCL
| C DCL
| h iDCL

Transfer programs:
 ::= i ;p j
| ; 
|  
| 
| DCL?

/* truth constant */
/* propositional variables */
/* negation */
/* disjunction */
/* contingent cooperative ability */
/* existential dynamic operator */

/* i gives p to j */
/* sequential composition */
/* non-deterministic choice */
/* iteration */
/* test */

Figure 3: Syntax of DCL - PC: p  P is a propositional variable, C  A is a set of agents, and i, j  A
are agents.

where there
S is an indexed element Pi for each i  A, such that P1 , . . . , Pn forms a partition of P
(i.e., P = iA Pi and Pi  Pj =  for all i 6= j  A). The intended interpretation of an allocation
 = hP1 , . . . , Pn i is that Pi  P is the set of propositional variables under agent is control. That
is, agent i has freedom to allocate whatever Boolean values it sees fit to the members of P i . Of
course, we could have defined an allocation  as a function  : P  A, so that (p) denotes
the agent controlling propositional variable p; there seems no particular reason for preferring one
representation rather than the other, and so for consistency with the historical record, we will adopt
the partition representation, as used by van der Hoek and Wooldridge (2005b).
Now, we say a model for DCL - PC is a structure:
M = hA, P, 0 , i
where:
 A = {1, . . . , n} is a finite, non-empty set of agents;
 P = {p, q, . . .} is a finite, non-empty set of propositional variables;
 0 = hP1 , . . . , Pn i is the initial allocation of P to A, with the intended interpretation that P i
is the subset of P representing those variables under the control of agent i  A; and finally,
  : P  {tt, ff} is a propositional valuation function, which determines the initial truth value
of every propositional variable.
Some additional notation is convenient in what follows. A coalition C is a subset of A, i.e., C  A.
For any such C  A, we denote the complement of C, (i.e., A \ C) by C. We will write P C for
443

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

Pi . For two valuations  and  0 , and a set of propositional variables   P, we write  =  0
(mod ) if  and  0 differ at most in the propositional variables in , and we then say that 
and  0 are the same modulo . We will sometimes understand the model M to consist of a frame
F = hA, P, 0 i together with a propositional valuation function . Given a model M = hA, P,  0 , i
and a coalition C in M, a C-valuation is a function:
S

iC

C : PC  {tt, ff}.
Thus a C-valuation is a propositional valuation function that assigns truth values to just the propositional variables controlled by the members of the coalition C. If M = hA, P,  0 , i with 0 =
hP1 . . . , Pn i is a model, C a coalition in M, and C a C-valuation, then by M  C we mean the
model hA, P, 0 , 0 i, where  0 is the valuation function defined as follows
(
C (p) if p  PC
0
 (p) =

(p)
otherwise
and all other elements of the model are as in M. Thus M  C denotes the model that is identical
to M except that the values assigned by its valuation function to propositional variables controlled
by members of C are determined by C .
We define the size of a model M = hA, P, 0 , i to be |A| + |P|; we denote the size of M by
size(M).
2.4 Transfer Program Relations
To give a modal semantics to the dynamic logic constructs of DCL - PC, we must define, for every
transfer program  a binary relation R over models such that (M1 , M2 )  R iff M2 is a model
that may result from one possible execution of program  from M 1 . We start by defining the relation
Ri;p j , for atomic transfer programs of the form i ;p j, i.e., agent i gives control of propositional
variable p to agent j. Let M = hA, P, 0 , i and M0 = hA0 , P0 , 00 , 0 i be two models with 0 =
hP1 , . . . , Pn i and 00 = hP01 , . . . , P0n i. Then
(M, M0 )  Ri;p j
iff
1. p  Pi (agent i controls p to begin with)
2. in case i = j:
(a) M = M0 (agent i gives p to herself, with no change in the model)
3. in case i 6= j:
(a) P0i = Pi \ {p} (agent i no longer controls p afterwards);
(b) P0j = Pj  {p} (agent j controls p afterwards); and
(c) all other components of M0 are as in M.
444

fiR EASONING A BOUT THE T RANSFER OF C ONTROL

In order to define M |=d , which means that  is true in M under the direct semantics, we need to
be able to determine what the interpretation of an arbitrary program is, on M; we define this below.
Notice that executing an atomic transfer program has no effect on the valuation function of a model.
Transfer programs only affect the distribution of propositional variables to agents.
For the remaining constructs of transfer programs, we define the program relations inductively,
in terms of the relations for atomic transfer programs, as defined above. Let the composition of
relations R1 and R2 be denoted by R1  R2 , and the reflexive transitive closure (ancestral) of relation
R by R . Then the accessibility relations for complex programs are defined as follows (Harel et al.,
2000):
R1 ;2 =
 R 1  R 2
R1 2 =
 R 1  R 2
 (R )
R  =
R? =
 {(M, M) | M |=d }.
Notice that the last of these definitions refers to the relation |=d , which of course has not yet been
defined. The aim of the next section is to define this relation. We emphasise that, although the
relations R? and |=d mutually refer to one-another, both relations are in fact well-defined (as in
conventional dynamic logic).
2.5 Truth Conditions
We interpret formulas of DCL - PC with respect to models, as introduced above. Given a model
M = hA, P, 0 , i and a formula , we write M |=d  to mean that  is satisfied (or, equivalently,
true) in M, under the direct semantics. The rules defining the satisfaction relation |= d are as
follows:
M |=d >
M |=d p iff (p) = tt

(where p  P)

M |=d  iff M 6|=d 
M |=d    iff M |=d  or M |=d 
M |=d

C  iff there exists a C-valuation C such that M  C |=d 

M |=d h i iff there exists a model M0 such that (M, M0 )  R and M0 |=d .
We say a formula is objective if it contains no modal constructs (i.e., operators  C or h i). Thus
objective formulae are formulae of classical propositional logic.
We assume the conventional definitions of satisfiability and validity: a DCL - PC formula  is
d -satisfiable iff there exists a DCL - PC model M such that M |= d , and  is d -valid iff for every
DCL - PC model M we have M |=d . We write |=d  to indicate that  is d -valid. A valid formula
is also called a tautology. We say  is feasible if it is satisfiable but not valid. Finally, for any set of
formulas  and formula , we define  |=d  as M(  M |=d   M |=d .
Let us define the box 2  to be the dual of the cooperative ability modality   as:

2C  =
 C 
445

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

and the []  to be the dual of the transfer modality hi  as:
[ ] =
 h i.
Where C is a coalition and  is a formula of
choose  to be either true or false:

DCL - PC ,

we write controls(C, ) to mean that C can

controls(C, ) =
 C   C 

(1)

By using the controls(, ) construct, we can capture the distribution of propositional variables
among the agents in a model.
Lemma 1 Let M = hA, P, 0 , i be a model for
and p  P a propositional variable in M. Then

DCL - PC ,

i  A an agent, C  A a set of agents,

1. (van der Hoek & Wooldridge, 2005b) M |=d controls(i, p) iff p  Pi ;
2. M |=d controls(C, p) iff p  PC .
Remark 1 Can we characterize the formulas under control of a coalition C? We have:
If  is feasible and objective, then |=d

^


controls(C, p)  controls(C, )

(2)

pP()

Observe that property (2) is not true for arbitrary DCL - PC formulas. To see this, take for example
the formula hi ;p ji>, no matter whether we define P(hi ;p ji>) to be {p} or . We have |=d
controls(i, hi ;p ji>): independent of i owning p, exactly one of the two formulas hi ; p ji> and
hi ;p ji> is true. That is, p  Pi in M iff M |=d hi ;p ji>.
Also note that the -direction of the right hand side of (2) is not valid for objective : suppose
M = hA, P, 0 , i such that (q) = tt, p  Pi , and q 
/ Pi . Then, we have M |=d controls(i, pq)
(controls(i, p)  controls(i, q)): because q happens to be true in M, i controls the conjunction
p  q, but not each of its conjuncts.
2.6 A Kripke Semantics
Although the direct semantics naturally captures the notions of propositional control and transfer
of control, for some purposes  establishing completeness in particular, and relating it to the main
stream of modal logic  it is more convenient to formulate the semantics for DCL - PC using conventional Kripke structures (Chellas, 1980; Blackburn, de Rijke, & Venema, 2001). The idea is that,
given the (fixed, finite, non-empty) set A of agents and (fixed, finite, non-empty) set P of propositional variables, there will be a possible world for every possible allocation of the variables in
P to the agents in A and every possible propositional valuation function over P. Between those
worlds, there are basically two orthogonal accessibility relations (cf. Figure 4): a horizontal and
a vertical one. First of all, we have a horizontal relation Ri for agent i between two worlds u
and v if agent i is able, given the valuation u in u, to turn it into the valuation v as described by v,
just by choosing appropriate values for her variables. Formally, (u, v)  R i iff u = v (mod Pi ).
That is, Ri is an equivalence relation. In what follows, we drop the symbolic distinction between
worlds and valuations, i.e., we use  for denoting a world and a valuation interchangeably. Notice
446

fiR EASONING A BOUT THE T RANSFER OF C ONTROL

 = hP1 , . . . , Pi , Pj , Pk , . . . , Pn i

M

p, q, r
Ri
p, q, r

v
u

Rj

p, q, r
w

i ;p j
M0

 0 = hP1 , . . . , Pi \ {p}, Pj  {p}, Pk , . . . , Pn i
p, q, r
Rj

p, q, r

v
u

Rj

p, q, r
w

j ;q k
M00

 00 = hP1 , . . . , P0i , P0j \ {q}, Pk  {q}, . . . , Pn i
p, q, r
Rj

p, q, r

v
u

Rk

p, q, r
w

Figure 4: Some Kripke models for DCL - PC.

that the horizontal relation does not affect the allocation : it remains unchanged. Let us therefore
define our Kripke models to be M = h, RiA , i, where  is the set of all valuations  over P. It
is important to realize that the sets A of agents and P of variables are fixed, but the allocations of
variables to agents may vary. We denote the set of all such Kripke models by K(A, P). We will call
a pair (M, ) a pointed Kripke model, and we will sometimes omit the brackets for such a pair.
Secondly, the vertical accessibility relation is between pointed models (M, ) and (M 0 , 0 ),
where M = h, RiA , i, M0 = h, RiA ,  0 i  K(A, P), which indicate a change of the allocation
 to  0 . Since such a change of allocation does not affect the current world, we have for such pairs
that  =  0 . Slightly abusing notation, we define (M, )(i ;p j)(M0 , 0 ) exactly when  =  0 and
p  Pi , and either i = j and M = M0 , or else P0i = Pi \ {p} and P0j = Pj  {p}, and all the other
sets Ph remain the same.
447

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

The truth relation |=K interpreting formulas over Kripke structures holds between pairs of the
form (M, ) and formulas . Its definition is as follows (we omit the Boolean cases and the cases
for complex transfer programs):
M,  |=K C  iff there exists a valuation  0 such that (,  0 )  Ri for each i  C, and
M, 0 |=K 
M,  |=K hi ;p ji iff there exists a Kripke model M0 such that (M, )(i ;p j)(M0 , ) and
M0 ,  |=K 
For a set of formulas  and a formula , we define  |=K  as (M, )(   (M, ) |=K  
M,  |=K ). Figure 4 illustrates the Kripke semantics. Note that for the sets P 0i and P0j in the Kripke
model M0 we have P0i = Pi \{p} and P0j = Pj {p}. Note that in the clause for C , the two pointed
models M,  and M,  0 are the same except for at most the atoms in PC . This is a special case of
two models being similar upto a set of atoms (French, 2006; Ghilardi & Zawadowski, 2000).
Remark 2 Note that in fact, in the Kripke semantics, formulas are not interpreted in a model and a
valuation only, but in the context of other models (which are reached by the atomic program i ; p j).
There are finitely many of them, one for each . Call this collection of models . In fact, this  is
the structure with respect to which formulas are interpreted. In that sense, there is only one Kripke
model for the language (w.r.t. A, P): it is . We will prove completeness with respect to this unique
two-dimensional model, in Section 3.
The following lemma is easily established by induction on :
Lemma 2 For any fixed sets of agents A and propositional variables P, the direct semantics and
the Kripke semantics are equivalent, i.e., for any , any M  K(A, P) with M = h, R iA , i, and
any model M = hA, P, , i:
M |=d  iff M,  |=K .
As usual, we define M |=K  as  : M,  |=K , and |=K  as M : M |=K .

3. A Complete Axiomatization
A sound and complete axiomatization for DCL - PC is presented in Figure 5. For the ease of exposition, we divide the axiomatization into five categories, as follows. While the Propositional
Component and the Rules of Inference are straightforward, the Dynamic Component is an immediate adaptation of Propositional Dynamic Logic (Harel et al., 2000). The Control Axioms are
inherited from CL - PC (van der Hoek & Wooldridge, 2005b). (The occurrence of `(p) refers to a
literal with atomic proposition p: it is either p or p, with the obvious meaning for `(p).) Note
that allocation specifies that every propositional variable is assigned to exactly one agent (i.e., we
have an allocation), while in contrast, for the fixed allocation  that was assumed in CL - PC, one
could explicitly state that controls(i, p), for every p  Pi (van der Hoek & Wooldridge, 2005b).
For the Transfer & Control Axioms, atomic permanence states that no program  changes the
valuation. From this, one easily extends this to arbitrary objective formulas (obtaining objective
permanence, see Theorem 1 below). The axiom persistence 1 (control) says that is control over
p is not affected when we move to another valuation, and axiom persistence 2 (control) specifies
448

fiR EASONING A BOUT THE T RANSFER OF C ONTROL

Propositional Component
Prop

Dynamic Component
K( )
union( )
comp( )
test( )
mix( )
ind( )
Control Axioms
K(i)
T(i)
B(i)
empty
control(i)
allocation
effect(i)
Comp-

where  is any objective tautology


[ ](  )  [ ]  []
[   0 ]  [ ]  [ 0 ]
[ ;  0 ]  [ ][ 0 ]
[?]  ( 
 )
  [ ][  ]  [ ]
  [  ](  [ ])  [  ]


2i (  )  2i   2i 
2i   
  2 i i 
2   


controls(i, p)  i p  i p

V
where
pP controls(1, p)5    5controls(n, p)
 A = {1, . . . , n}


p 6 P(), and
  `(p)  controls(i, p)  i   `(p) where
 is objective
2C1 2C2   2C1 C2 

Transfer & Control Axioms

atomic permanence(;) hi ;p ji>  [i ;p j]q  q
persistence1 (control)
controls(i, p)  2j controls(i, p)
persistence2 (control)
controls(i, p)  [j ;q h]controls(i, p)
precondition(transfer)
hi ;p ji>  controls(i, p)
transfer
controls(i, p)  hi ;p jicontrols(j, p) 
func
controls(i, p)  hi ;p ji  [i ;p j]

Rules of Inference
Modus Ponens
Necessitation

` , ` (  )  ` 
`   ` 2

where i 6= j or p 6= q

2 = [ ], 2i

Figure 5: Axiomatic System for DCL - PC.
how i remains in control over p, even when a transfer program is executed: either the variable
passed in that program is not p, or the delegating agent is not i. The axiom precondition(transfer)
expresses that agents can only give variables away that they possess, and, finally func says that the
transition relation associated with an atomic transfer program is functional: at most one resulting
world emerges.
The following theorem lists some properties of DCL - PC, where controls(C, p) is defined in equation (1) above.
Theorem 1
1. The axioms K(i), T(i), B(i), and effect(i) have coalitional counterparts K(C), T(C), B(C),
and effect(C) that are all derivable for any coalition C.
449

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

at-least(control) : 
`(p)  controls(i, p)  i `(p)
at-most(control) :

`(p)  i `(p)  2j `(p)
(i 6= j)
non-effect(i) :

i `(p)  controls(i, p)  2i `(p)
persistence(non-control) :
controls(i, p)  2j controls(i, p)
objective permanence(;) : 
hi ;p ji>    [i ;p j] where  is objective
objective permanence
 :
h i>    [ ]
where  is objective
inverse :

controls(i, p)    [i ;p j; j ;p i]
reverse :
[i ;p j][k ;q h]  [k ;q h][i ;p j]
where (j 6= k and h 6= i) or p 6= q

Figure 6: Some Theorems of DCL - PC.
2. Moreover, we know (van der Hoek & Wooldridge, 2005b) that the axioms K(i), T(i), B(i),
and effect(i) have the coalitional counterparts K(C), T(C), B(C), and effect(C) that are
derivable for any coalition C.
3. ` controls(C, p) 

W

iC

controls(i, p).

4. ` controls(C, p)  2j controls(C, p), i.e., the property persistence1 (control) is also derivable when we replace agent i by an arbitrary coalition C.

Proof: See Appendix A.

QED

Consider the language without dynamic transfer operators, in which we only have propositional
logic with cooperation modalities C . Models for these are M, M0  K(A, P). In this program-free
language, every formula is equivalent to one without any occurrences of coalition operators van der
Hoek and Wooldridge (2005b). For instance, suppose that P i = {p, q}. Then a formula i (p  r)
is equivalent to (p  r)  (p  r) (we read off the current value of variable r outside is control).
We now establish a similar result for the language including programs. Any world (M, ) is
completely characterized when we know which variables are true in it, and what the allocation of
variables to agents is. In such a case, the truth of all objective formulas, formulas involving abilities
and transfer programs is completely determined.
Lemma 3 Let  be an arbitrary DCL - PC formula and  a conjunction of assertions of the form
controls(j, p) or controls(j, p). Then, in DCL - PC, we can derive
` C (  )  (  C ).
450

fiR EASONING A BOUT THE T RANSFER OF C ONTROL

Proof: Since by Comp-, for C = {a1 , a2 , . . . , aC }, we have C   a1 a2    aC , it is
sufficient to prove the claim for an individual agent i. Moreover, we can move all conjuncts of  out
one by one, once we know that
` i (  controls(j, p))  (controls(j, p)  i ),
where controls(j, p) is either controls(j, p) or controls(j, p). We do the reasoning for the nonnegated case (the other one is similar): i (  controls(j, p)) is equivalent to

controls(j, p)  i (  controls(j, p)) 
 controls(j, p)  i (  controls(j, p)) .

However, by using the theorem persistence(non-control) from Figure 6 (which
we derive below),

we have for the second disjunct that controls(j, p)  i (  controls(j, p))  . That concludes
the proof.
For persistence(non-control), the right-to-left direction follows immediately from T(j). For the
other direction, assume that controls(i, p). From allocation we derive that
controls(1, p)5    5controls(i  1, p)5controls(i + 1, p)5    5controls(n, p),
W
and from this, by persistence1 (control), we get k6=i 2j controls(k, p). For every k 6= i, we have
controls(k, p)  controls(i, p), which follows from allocation. Hence, using Necessitation, we
have 2j (controls(k, p)  controls(i, p)).WFrom Axiom K(j), it now follows that 2 j controls(k, p) 
2j controls(i, p). Combining this with k6=i 2j controls(k, p), we obtain the desired conclusion
2j controls(i, p).
QED
Soundness of the axiom schemes in Figure 5 is readily checked. We now proceed to prove that
the axiomatic system for DCL - PC in Figure 5 is complete. First, we introduce some notation.
Definition 1 Given the set of propositional variables P, a valuation description  is a conjunction
of literals (p or p) over them such that every propositional variable in P occurs in one literal.
Notice that, for each propositional variable p  P, it holds that either   p, or   p. We
denote the set of all valuation descriptions over P with . Notice that, for each valuation , there is
a    such that
V
 = V{p | p  P and (p) = tt}
 {p | p  P and (p) = ff}.
Definition 2 Given the set of propositional variables P and the set of agents A, an allocation description  is a conjunction of formulas of the form controls(i, p) where for every p  P, there is
exactly one i  A such that controls(i, p) appears in .

We denote the set of all allocation descriptions with . Notice that allocations  and conjunctions 
correspond to each other: For each allocation  = hP1 , . . . , Pn i of the variables in P over the agents
in A, there is a    such that
^
controls(i, p).
 =
iA,pPi

451

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

Therefore, we refer to formulas  as allocation descriptions. Given two allocation descriptions
,  0  , we say (i ;p j) 0 if the following three conditions are satisfied:   controls(i, p),
 0  controls(j, p), and  and  0 agree on all other control expressions.
Definition 3 Let, for any allocation description ,    be a set of valuation descriptions. Then,
a formula of the form
_ _

  
(3)


will be called a proposition description.

We will later, in Theorem 2, see that every formula  is equivalent to a proposition description.
The intuition here is that the truth of  requires, for every allocation description , some possible
truth values of atoms to be fixed. To give an example, suppose there are two agents i and j, and
three atoms p, q and r. Consider the formula  = hi ;p ji(q  j (p  r)). In order to find the
equivalent proposition description, we must, for every    make proper choices for   . If 
implies controls(i, p) this allocation would make  false (since i cannot
W transfer control over p),
so for those , we have to choose  to be the empty set, ensuring that (   ) is equivalent to
. If  implies controls(i, p),
W there are basically two cases: either  also implies controls(j, r), and
the constraint on  is that  is equivalent to q, or else  implies controls(j, r), in which case
W
 is equivalent to q  r.
Let us, for two valuation descriptions  and  0 , a coalition C, and an allocation description
, write    0 (mod C, ) if the two conjunctions of literals  and  0 only differ in the variables under control of C, which is determined by . For instance, when C = {1, 2} and  =
controls(1, p1 )controls(2, p2 )controls(3, p3 ), then (p1 p2 p3 )  (p1 p2 p3 ) (mod C, ).
We now first collect some facts about valuation descriptions, allocation descriptions, and proposition
descriptions.
Recall that, for any set  of DCL - PC formulas, 5   is used as shorthand for
W
V
   1 6=2  (1  2 ).

Lemma 4 Given the set  of valuation descriptions , and the set  of allocation descriptions ,
the following six items are satisfied:
1. ` 5 
2. ` 5 
W
3. `    (  )

4. For all    and all   : `   (C  

W

 0  (mod C,) 

0 ).

5. ` (  hi ;p ji 0 )  ((  )  hi ;p ji( 0  )).
6. Let n be the number of agents, and k the number of propositional variables. Then there are
nk
not more than N(n, k) = 22 provably non-equivalent proposition descriptions.
Proof:
1. This follows from Prop and the definition of : the s are mutually exclusive and cannot all
be false.
452

fiR EASONING A BOUT THE T RANSFER OF C ONTROL

2. Item (2) is easily seen to be equivalent to the allocation axiom: allocation implies
and, for every allocation description   , we have that  implies allocation.

W

 ,

3. Item (3) is immediate from Item (2) and axiom Prop. In particular, using Prop we derive from
` A  B that C  (C  A)  (C  B).
4. Assume . For the right-to-left direction, also assume  0 , for some valuation description  0
with  0   (mod C, ). This means that  0 and  only differ in some variables p1 , . . . , pm
for which controls(C, pj ) is implied by  (for j = 1 . . . m). Note that  0 is an objective
formula. We can write  0 as i  `(pi ) (for i = 1 . . . m), where i is as  0 but with the literal
`(pi ) left out. Apparently, we have 1  `(p1 )  controls(C, p1 ). Since 1 is objective, we can
apply effect(C) to conclude C (1 `(p1 )). Using Lemma 3, we derive C (1 `(p1 )).
We can now rewrite 1 to 2  `(p2 ), and obtain C (2  `(p2 )  `(p1 )  ). We use
effect(C) and Lemma 3 again and get C C (2  `(p2 )  `(p1 )  ). By Comp-, this
is the same as C (2  `(p2 )  `(p1 )  ). We can repeat this process until we get
C (j  `(pj )      `(p2 )  `(p1 )  ). But, by definition of  0 , this implies C .
WeWshow the other direction from left to right by contrapositive: fix  and  and assume
 0  (mod C,)  0 . We have to show that C . Let Q(, ) = {`(p) |`   `(p) and 0
  controls(C, p)} be the set of the literals over variables that are not under control of the
agents in C at the allocation . Notice that all valuations  0   (mod C, )
W agree with  on0
the literals
in
Q(,
).
We
can
use
propositional
reasoning
to
derive
from

 0  (mod C,) 
W
V
that  `(p)Q(,) `(p). Using T(C) and K(C), we then conclude that `(p)Q(,) C `(p).
From , it follows, for each literal `(p)  Q(, ), that controls(C,
`(p)), which, by equaW
tion (1), equals C `(p)



`(p).
But
then,
we
can
derive
C
`(p)Q(,) C `(p). Using
V
K(C), we obtain C `(p)Q(,) `(p). Hence, C .

5. First of all, from (  hi ;p ji 0 , follows   controls(i, p). Hence, given , we have that all
formulas hi ;p ji and [i ;p j] are equivalent by func. In particular, we have hi ;p ji>.
Let us first show for literals `(q) that `(q)  hi ;p ji`(q). For negative literals q, q 
hi ;p jiq equals q  [i ;p j]q, which follows from atomic permanence(;). For positive
literals q, we use func to obtain q  [i ;p j]q, which again holds by atomic permanence(;).
Now, given , we have q  hi ;p jiq and q  hi ;p jiq. Then, for any valuation
description , we also have   hi ;p ji. It remains to show that   hi ;p ji 0 .
From left to right, note that we have controls(i, p)  hi ;p jicontrols(j, p) by transfer.
For any other controls expression implied by , this follows from persistence 2 (control) and
func. Finally, consider the direction from right to left: hi ;p ji 0  . We have to show
that, first, hi ;p jicontrols(j, p)  controls(i, p), and, second, that hi ;p jicontrols(h, q) 
controls(h, q), for each controls(h, q) (with q 6= p) implied by  0 . The first part follows immediately from precondition(transfer). For the second part, let h be an agent and q 6= p a variable such that  0 implies
controls(h, q). Suppose hi ;p jicontrols(h, q). By allocation, we
V
controls(k,
q). But then the contrapositive of persistence 2 (control)
have that
hi
;
ji
p
k6=h
V
yields k6=h controls(k, q). It follows by allocation that controls(h, q).
6. Item 6 follows from a straightforward counting argument of proposition description formulas.
The number of proposition descriptions depends on the cardinalities of the sets  and .
Given the number n = |A| of agents, and the number k = |P| of propositional variables,
453

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

it is easy to see that there are 2k valuation descriptions in , and nk allocation descriptions
in  (i.e., the number of ways we can distribute k variables over n agents). Observe that
any proposition description formula is obtained by assigning a set of valuation descriptions
k
k
 to each allocation description . Hence, there are 22 n proposition descriptions. Since
k
k
nk
nk
22 n  22 , we obtain with N(n, k) = 22 an upper bound for the number of different
proposition description formulas.
QED

We now present the main result of this section. We first formulate it, reflect briefly on it, and
then give its proof.
Theorem 2 For every DCL - PC formula , there are sets  ()   of valuation descriptions, one
for each   , such that
_ _

`
 ()   .


According to Theorem 2, we can get rid of all the hi ;p ji. To show that our derivation system
is good enough to establish that, is the main task of its proof. But let us first convince ourselves
semantically that such a normal form makes sense. Remember that every model M comes with
its own allocation . A formula like hi ;p ji is true at M, , if  is true in a model that looks
like M, but in which control over p is transferred from i to j. But this means that some formula  0
must already be true at M, , where i takes the role of j for as far as p is concerned. For instance,
hi ;p ji(q  j (p  r)) is true at M, , if q  j r  controls(i, p) is true under the current allocation
. This formula has no reference to other layers anymore. More precisely, hi ; p ji(q  j (p  r))
is equivalent to

q  ((p  r)  (p  r))  controls(i, p)  controls(j, r)


q  ((p  r)  (p  r)  (p  r)  (p  r))  controls(i, p)  controls(j, r) .
Proof: The proof is by induction on the norm |||| which is defined on DCL - PC formulas as follows:
||>||
||||
||1  2 ||
||C ||
||[i ;p j]||
||[?]||
||[1  2 ]||
||[1 ; 2 ]||
||[  ]||

=
=
=
=
=
=
=
=
=

||p|| = 0, for any p  P
1 + ||||
1 + ||1 || + ||2 ||
1 + ||||, for any C  A
1 + ||||
1 + ||  ||
1 + ||[1 ]  [2 ]||
1 + ||[
V1 ][2 ]||
1 + || i=0..N [ ]i ||

where N = N(n, k) is the number defined in Lemma 4, Item (6).
The induction base for the proof of our theorem has two cases:

W
  = >. We take  (>) = , for
every


.
By
Item
(1)
of
Lemma
4,
we
have
that
 is

W
W
W
 (>)   is equivalent to  , which in turn
an objective tautology. Hence, 
is equivalent to > by (2) of Lemma 4.
454

fiR EASONING A BOUT THE T RANSFER OF C ONTROL

 =
 (p) = {   |`   p}, for every   . Clearly, p
W p, for p  P. Take W
Wis equivalent
to  (p), i.e., ` p   (p). Now, using (3) of Lemma 4, we get ` p   (p  ).
Finally,W
replacing
W the second occurrence of p with its just derived equivalent formula, we get
 (p)   . The other direction follows by simple propositional reasoning.
` p  

Consider the induction step.

  = . We set  () =  \  (), for every   . This works, because of the
following:
  

_



^



_

(



_



(



_





 ()  

_

_

_



(4)

 ())  
 ())  

 ()  







(5)
(6)
(7)

All steps are purely propositional,
between (5) and (6) which we
V except for the equivalence W
explain now. Abbreviate (5) to W  (A  ), then (6) is  (A  ). Note that by
Lemma
V 4, Item (2), we derive  . In other words, oneW must be true, say . But note
that  (A )W implies (A  ) and hence also  (A ), our abbreviation
for (6). Conversely, if  (A  ) holds, we know by Lemma 4, item (2) that 4 ,
i.e., exactly one  must hold, say it is . For this  the formula (A   ) is true, and it is
the only allocation description for which such a formula can be true. For this , we also have
(A  ). Moreover, for anyV 6= , we have , and hence (A   ). So (A  )
holds for all , hence we have  (A  ), our shorthand for (5).

  = 1  2 . We set  (1  2 ) =  (1 )   (2 ), for every   . For the following
equivalences, we only need propositional reasoning:
1   2 

_



_



_



_
(

 _
 (1 )   



_

_

 (1 ) 

_

_

 (2 )  

 (2 ))  

 (1  2 )  







  = C . For every   , we set
 (C ) = {   |    0 (mod C, ) for some  0   ()}
455

(8)
(9)
(10)

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

We can derive the following equivalences:

C 

C



_



_



C



_



C



_





_

_

_

 ()  
 ()  

_





(12)



(14)



 ()  

 (C )  

(11)

(13)

The equivalence in (11) holds by the induction hypothesis. Using K(i), this is equivalent to
(12) (for any diamond we have C (  )  (C   C )). The equivalence of the latter
and (13) is by Lemma 3.
W
W
It remains to show the equivalence of (13) and (14). We have


()
=

C

C
 () .
W
By K(C) and Comp-, this formula is equivalent to  () C . Using Item (4) in
W
W
0
Lemma
4,
we
see
that
this
is
equivalent
to

()
 0 | 0  (mod C,)  . But this equals

W
 (C ) by definition of  (C ).

  = [i ;p j]. We define  ([i ;p j]) as follows: for every   ,


0 () if   controls(i, p)
 ([i ;p j]) =
where (i ;p j) 0



otherwise

To see that this yields a formula of the right form that is equivalent to [i ; p j], let us first
partition  in + (i, p) = {   | `   controls(i, p)} and  (i, p) = {   | `  
controls(i, p)}. Now consider the following derivable equivalences:
_ _
[i ;p j]  [i ;p j]
( 0 ()   0 )
(15)
 0 



controls(i, p)
_ _
( 0 ()  0 ))

(controls(i, p)  hi ;p ji

(16)

0 



_

(

 (i,p)

_

_

  ) 

  hi ;p ji

_



_

(17)

0 

+ (i,p)



_ _

( 0 ()  0 )

 ([i ;p j])  



(18)

The equation in (15) holds by the induction hypothesis. It is equivalent to (16) by propositional reasoning, and changing from [i ;p j] to hi ;p ji is allowed by func. The equivalence of (16) and (17) follows from the definition of + (i, p) and  (i, p) and the fact
456

fiR EASONING A BOUT THE T RANSFER OF C ONTROL

W
that   >. In order to prove the equivalence between (17) and W
(18), it W
is sufficient
+ (i, p), the formula   hi ; ji
to show that, for any fixed



(
0 () 
p
0 
W
0
0 ) is equivalent to   0 (), where (i ;p j) . But this
W from Lemma 4,
W follows
ItemW(5), as follows.W First of all, we can write   hi ;p ji 0  ( 0 ()  0 ) as
  0  hi ;p ji( 0 ()  0 ). By the mentioned lemma,
W we know exactly which  0
0 giving   hi ; ji(  0 ()   0 ). We can rewrite
we need: it is  0 for which
(i
;
j)
p
p

W
this into   hi ;p ji {W  0 |   0 ()}, and then push the diamond hi ;p ji inside
0
0
the disjunction
W to get   {hi ;p ji(   ) |    ()}.
W But then Lemma 4, Item (5)
yields   0 (). The other direction isW
similar: if   0 () and (i ;p j) 0 , then,
by Lemma 4, Item (5), we get   hi ;p ji( 0 ()   0 ), from which the result follows.

  = [ 0 ?]. By axiom test( ), [ 0 ?] is equivalent to  0  , which has an equivalent
formula of the right form by the induction hypothesis.

  = [1 ; 2 ]. By axiom comp( ), [1 ; 2 ] is equivalent to [1 ][2 ], which has an equivalent
formula of the right form by the induction hypothesis.
  = [1  2 ]. By axiom union( ), [1  2 ] is equivalent to [1 ]  [2 ], which has an
equivalent formula of the right form by the induction hypothesis.
  = [  ]. Recall N = N(n, k) as given in Lemma 4, Item (6). Using axiom mix( ) and
K( ), we know that [  ] is equivalent to   [ ][  ]. Doing this N times, we obtain
  [ ]  [ ]2       [ ]N   [ ]N [  ].
By the induction hypothesis, we know that all except the last conjunct have an equivalent
normal form. But since there are only N different such forms, the V
conjunct [ ] N  must be
equivalent to one of the earlier conjuncts [ ]i  (i < N). Define  = i=0..N [ ]i . We claim
  [  ].

(19)

And by the induction hypothesis,  has an equivalent formula of the right form. The  direction of (19) is obvious since  is just the first part of the unraveling [ ][ ][ ] . . .
of [  ] using axioms mix( ) and K( ). To show the other direction it is sufficient to derive
  [  ], because, by the fact that  is just one of the conjuncts in , this immediately
gives   [  ]. To show the derivability of   [  ], we will use ind( ). First of all,
we show   [ ]. To see this, note that [ ]  [ ][ ]0   [ ][ ]1       [ ][ ]N . By
the induction hypothesis, each conjunct [ ]i  (i  N) has a normal form, say, Bi . Then we
obtain from  a sequence B0 , B1 , . . . , BN of N + 1 formulas in normal form. Since there are
at most N provably non-equivalent formulas by Lemma 4, Item (6), we know that there is a B j
that equals a previous Ba with a < j  N. Let Bj be the first such repetition in this sequence.
Notice that we have now [ ]j   Bj = Ba  [ ]a , and thus [ ]k [ ]j   [ ]k [ ]a , for any
k  0. But then, it follows that the last conjunct [ ][ ]N  in [ ] is equivalent to [ ]k [ ]a 
(with k = N  j), which already appears in . Now that we have derived   [ ], we apply
Necessitation for [  ], and obtain ` [  ](  [ ]). By applying ind( ), we get   [  ].
QED

We know from Lemma 4 that there are only finitely many different normal forms: since every
formula has such a normal form, there can be only finitely many non-equivalent formulas. We also
457

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

know from the proof of Theorem 2 above that, for [  ], we only have to consider a finite number
of conjuncts [ ]i  in its unraveling.
Corollary 1 There are only finitely many pairwise non-equivalent formulas of
given the number n of agents, and k of propositional variables, we have:
W
1. i 6= j  M 6` (i  j )  ` iM i , and
V
2. ` [  ]  iN [ ]i ,

DCL - PC .

In fact,

nk

where M = 2nk and N = 22 (as defined in Lemma 4, Item (6)).

Completeness of a derivation system with inference relation ` with respect to a semantics means
that every semantically valid formula is also provable: |=   ` . In order to prove completeness,
often the contrapositive of this is shown: 6`   6|= . That is, every consistent formula  has
a model. A popular technique in modal logic is to construct a dedicated model  the canonical
model, (cf. e.g., Blackburn et al., 2001)  for any consistent formula . That canonical model is a
bridge between syntax and semantics: it consists of all maximal consistent sets  (as worlds), and
is constructed in such a way that membership of a world in  and truth in the corresponding world
coincide.
For DCL - PC this works straightforwardly as follows. Fix the finite sets A and P, and take a
consistent formula . Build a W
maximal
W consistent set  around it. Let the normal form of ,
guaranteed
by
Theorem
2,
be
(
  ). Since  is maximal consistent, for some , we

W
have (   )  . Again, by maximal consistency of , it must contain, for exactly one
   , the formula   . But  uniquely determines a valuation  =   , whereas  determines
an allocation  =  . In other words,  uniquely determines a pointed Kripke model (M, ) with
M = h, RiA , i. All the worlds in  are determined by P, and all RiA (the horizontal layer of
M, in terms of Figure 4) are determined by the Control Axioms. The availability of all the right
models M0 = h, RiA ,  0 i (the vertical layer in Figure 4) is determined by the Delegation and
Control Axioms. As a result, we can directly interpret all subformulas of the form hi ; p ji and
C  in the proper way, in (M, ).
This argument easily extends to strong completeness, which states that for all sets of formulas
 and all formulas , we have  |=K    ` . To see this, suppose  6` , i.e.,   {}
is consistent. Since there are only finitely many pairwise non-equivalent formulas, there must be a
formula  that is equivalent with   {}. By the previous argument, we find a pointed model
(M, ) such that (M, ) |=K . For this model we also have (M, ) |=K   {}. Hence, not
every model for  is one for , i.e.,  6|=K . Strong completeness also follows by an alternative
way of resaoning: note that our language is compact: i.e., if  |= K , there there is a finite set
0   such that 0 |=K . This is seen as follows: we know from Corollary 1 that there are at
most M different formulas that are not provably equivalent, for an M that depends on the number
of agents and the number of atoms. But then, by soundness, there are also at most M semantically
different formulas in . Putting those formulas in 0 gives the desired result. Strong completeness
then follows from (weak) completeness with compactness.
All in all, we obtain the following (also using Lemma 2).
Theorem 3 The language of DCL - PC is compact. Moreover, the axiomatic system DCL - PC is sound
and complete with respect to both the Kripke and the direct semantics. It is also strongly complete.
458

fiR EASONING A BOUT THE T RANSFER OF C ONTROL

1. function program-eval(, M = hA, P, , i, d) returns fail or a model over A, P
2.
if  = ? then
3.
return M if DCL - PC-eval(, M)
4.
or fail otherwise
5.
elsif  = (i ;p j) then
6.
return hA, P,  0 , i if p  Pi
7.
where  0 =  if i = j
8.
otherwise  = hP1 , . . . , Pn i and  0 = hP01 , . . . , P0n i
9.
with P0i = Pi \ {p},
10.
P0j = Pj  {p}, and
11.
P0m = Pm for all m  n, m 6= i, j
12.
or fail otherwise
13. elsif  = (1 ; 2 ) then
14.
return program-eval(2 , program-eval(1 , M, d), d)
15. elsif  = (1  2 ) then non-deterministically choose to either
16.
return program-eval(1 , M, d)
17.
or program-eval(2 , M, d)
18. elsif  =  0 then
19.
return fail if d = 0
20.
or otherwise (if d > 0) non-deterministically choose to either
21.
return M
22.
or program-eval(( 0 ;  0 ), M, d  1)
23. end-function

Figure 7: An algorithm for deciding (M, M0 )  R .
That is, for all sets of

DCL - PC

formulas  and for every DCL - PC formula , we have
 `  iff  |=K  iff  |=d 

4. Complexity
The model checking and satisfiability problems for CL - PC are PSPACE-complete (van der Hoek &
Wooldridge, 2005b), and since DCL - PC subsumes CL - PC, this implies a PSPACE-hardness lower
bound on the corresponding problems for DCL - PC. The obvious question is then whether the additional dynamic constructs of DCL - PC lead to a more complex decision problem  and in particular,
whether DCL - PC satisfiability matches the EXPTIME-completeness of PDL satisfiability (Harel et al.,
2000). In this section, we show that the model checking and satisfiability problems are in fact no
worse than CL - PC: they are both PSPACE-complete. Notice that EXPTIME-completeness is usually
regarded as the characteristic complexity of logics in which there is a modal operator and another
operator representing the transitive closure of this operator (Blackburn et al., 2001).
Note that when we consider the model checking problem in this section, we consider the problem with respect to direct models, not Kripke models. Of course, with respect to satisfiability, it
makes no difference: a formula is satisfiable with respect to direct models iff it is satisfiable w.r.t.
Kripke models.
Before proving PSPACE-completeness for DCL - PC model checking, consider some auxiliary notions first. A program sequence is a transfer program that is composed of atomic transfer programs,
tests, and sequential composition only. A program  admits a program sequence  if  can be un459

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

folded into  by recursively applying the following rules: For any atomic transfer program (i ; p j),
test ?, and transfer programs m (m  0):
(i ;p j)
?
1 ; 2
1   2


(i ;p j)
?
 1 ; 2
1 or 2
1 ; 2 ; . . . ; n , for some n  0,
where m =  , for all m  n

The following two lemmas establish that membership in the accessibility relation R  for a transfer program  can be decided in polynomial space.
Lemma 5 For all transfer programs  0 and all (direct) models M and M0 , (M, M0 )  R 0 implies
that  0 admits a program sequence  of length at most exponential in the length of  0 such that
0 3
(M, M0 )  R . In fact, the length of  can be limited to 2| | .
Proof: Let  0 , M, and M0 be as in the lemma. The proof is by induction on the structure of
 0 . The only interesting case is where  0 =   ; the other cases are straightforward. Suppose
(M, M0 )  R  . Since R  = (R ) , there is a sequence M0 , . . . , Mn , n > 0, of models such
that M0 = M, Mn = M0 , and (Mi1 , Mi )  R , for each i with 1  i  n. By the transitivity
of R  , we can assume that the sequence M0 , . . . , Mn is such that Mi 6= Mj , for all i, j with
1  i < j  n, i.e., the sequence of models contains no loops. The induction hypothesis yields that
3
 admits program sequences 1 , . . . , n such that i is of length at most 2| | and (Mi1 , Mi )  Ri
for each i with 1  i  n. But then  = 1 ; 2 ; . . . ; n is a program sequence admitted by   such
that (M, M0 )  R  . In the following, it is shown that  has the required length. Note that all
models reachable from M = hA, P, , i via R  only differ in the allocation  of propositional
variables in P to the agents in A. More precisely, they differ in the allocation of propositional
variables to agents that occur in  . Thus there are at most `m such reachable models, where ` is the
number of propositional variables occurring in  and m the number of agents occurring in  . Notice
that n does not exceed `m ; otherwise the sequence M0 , . . . , Mn contains loops contradicting the
2
assumption. Together with the fact that `m  | || |  2| | , an upper bound for the length of  can
be given as follows:
|| = |1 ; 2 ; . . . ; n | 

=



n  sup{|i | : 1  i  n} + n
2
3
2
2| |  2| | + 2| |
2
3
2
2| | +| | + 2| |
3
2(| |+1)
 3
2| | .
QED

Lemma 6 For all programs  and all (direct) models M and M0 , the membership problem (M, M0 ) 
R can be decided in PSPACE.
Proof: Let  be a program and let M, M0 be two (direct) models. Consider the following algorithm
that decides (M, M0 )  R by using the function program-eval(   ) in Figure 7:
460

fiR EASONING A BOUT THE T RANSFER OF C ONTROL

3

1. Set d = 2| | .
2. If program-eval(, M, d) = M0 , then return (M, M0 )  R , and No otherwise.
To see that this algorithm is correct, it is shown that program-eval(, M, d) = M 0 iff (M, M0 ) 
R . For the direction from left to right, it is readily checked that program-eval(, M, d) = M 0
implies the existence of a program sequence  admitted by  of length at most | |  d such that
(M, M0 )  R 0 . Clearly, R  R and thus (M, M0 )  R . Consider the direction from right
to left. From (M, M0 )  R , it follows by Lemma 5 that there is a program sequence  admitted
3
by  of length at most 2| | such that (M, M0 )  R . Step 1 ensures that the value of d is such
3
that 2| |  | |  d. Then it is obvious by construction of the algorithm that the non-deterministic
choices in the lines 15 and 20 of Figure 7 yield that program-eval(, M, d) = M 0 . Notice that
the algorithm terminates since the recursive calls in the lines 14, 16, and 17 are applied on strict
subprograms only and the recursive call in Line 22 is followed by the one in Line 14 while the
parameter d limits the recursion depth.
The above algorithm can be run in polynomial space. To see this, notice that the function
DCL - PC-eval(   ), which is called in Line 3, can be computed in polynomial space and that the
parameter d is encoded in binary. Moreover, the stack of an algorithm computing the function
program-eval(   ) can be limited to a size polynomial in the length of  . Note that the stack
only needs to store the currently evaluated program and the programs at the backtracking points,
which are introduced at the nested function call in Line 14. But since this nested function call is
applied on strict subprograms, there are only linearly many backtracking points needed at a time.
Although the algorithm is non-deterministic, it follows from the well-known fact NPSPACE equals
PSPACE (Savitch, 1970) that it runs in PSPACE .
QED
Using the previous two lemmas, we can now prove the following.
Theorem 4 The model checking problem DCL - PC (w.r.t. direct models) is PSPACE-complete.
Proof: Given that DCL - PC subsumes the PSPACE-hard logic CL - PC, we only need to prove the upper
bound. Consider the function DCL - PC-eval(   ) in Figure 8. Soundness is obvious by construction.
First note that the algorithm is strictly analytic: recursion is always on a sub-formula of the input.
That the algorithm is in PSPACE follows from the fact that the loops at lines 1012 and 15-18
involve, in the first case simply binary counting with the variables P C , and in the second simply
looping through all direct models over A and P: we do not need to store these models once they
are checked, and so this can be done in polynomial space. Finally, Lemma 6 yields that the check
(M, M0 )  R on Line 16 can be done in polynomial space.
QED
Now, we make use of the following result, the proof of which is identical to the equivalent result
proved by van der Hoek and Wooldridge (2005b).
Lemma 7 If a DCL - PC formula  is satisfiable, then it is satisfied in a (direct) model M such that
size(M) = |P()| + |Ag()| + 1.
We can now prove the following.
Theorem 5 The satisfiability checking problem for DCL - PC is PSPACE-complete.
461

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

1. function DCL - PC-eval(, M = hA, P, 0 , i) returns tt or ff
2.
if   P then
3.
return ()
4.
elsif  =  then
5.
return not DCL - PC-eval(, M)
6.
elsif  = 1  2 then
7.
return DCL - PC-eval(1 , hA, P, 0 , i)
8.
or DCL - PC-eval(2 , hA, P, 0 , i)
9.
elsif  = C  then
10.
for each C-valuation C
11.
if DCL - PC-eval(, hA, P, 0 , i  C ) then return tt
12.
end-for
13.
return ff
14. elsif  = h i then
15.
for each model M0 over A, P
16.
if (M, M0 )  R then
17.
if DCL - PC-eval(, M0 ) then return tt
18.
end-for
19.
return ff
20. end-function

Figure 8: A model checking algorithm for DCL - PC.

Proof: Given a formula , loop through each model M containing P() and Ag() such that
size(M) = |P()| + |Ag()| + 1, and if M |=d  then return Yes. If we have considered all such
models, return No. By Theorem 4, we can check whether M |= d  in polynomial space.
QED
Notice that the PSPACE complexity for checking satisfiability depends upon the fact that models
for DCL - PC are concise, and that hence we can loop through them all in polynomial space (we do
not need to remember a model after it has been considered).

5. Characterizing Control
One of the main concerns in the original study of CL - PC (van der Hoek & Wooldridge, 2005b) was
to investigate the logical characterization of control: the extent to which we could characterize, in
the logic, what states of affairs agents could reliably control. Control was distinguished from ability
in the sense that, for example, no agent could be said to control a tautology, even if one might be
prepared to concede that an agent would have the ability to bring about a tautology. The starting
point for the study of control (van der Hoek & Wooldridge, 2005b) was the controls(i, p) construct:
as we have already seen, such an expression will be true iff the variable p is under the control of
agent i. This led to an analysis and characterization of the types of formulas that an agent could
be said to control. The type of control studied by van der Hoek and Wooldridge derives from the
ability of agents to choose values for the propositional variables under their control. Let us refer to
this type of control, where an agent is directly able to exert some influence over some state of affairs
by assigning values to its variables, as first-order control. In this section, we undertake a similar
study of control in the richer setting of DCL - PC. Here, however, we have a second type of control,
462

fiR EASONING A BOUT THE T RANSFER OF C ONTROL

which derives from the ability to transfer control of variables to other agents. Thus, for example, if i
controls p, she also has the power to ensure for instance controls(j, p), where j is an agent different
from i. This control is expressed through the transfer modality: hi ; p jicontrols(j, p). We refer
to this type of control as second-order control. We will see that these types of control are indeed
rather orthogonal. For instance, hi ;p jij  (i can give p to j, who then can achieve ) and i,j  (i
and j can cooperate, to achieve ) are logically incomparable. For example, taking  = hj ; p ii>
gives
|=d controls(i, p)  (hi ;p ji  i,j )
while for  = hi ;p ji> and assuming i 6= j, we have
|=d controls(i, p)  (hi ;p ji  i,j ).
However, if the goal is an objective formula, we can relate atomic control and transfer, as we will
shortly see.
To begin our study, consider the transfer program
[
[

i ;p j .
(20)
controls(i, p)?;
givei =

jA

pP

Then hgivei i would express that i has a way to give one of her propositional variables to one
of the agents (possibly herself) in such a way that consequently  holds. Thus, hgive i i means that
i can distribute her variables among the agents in such a way that afterwards  holds. Hence, when
reasoning about is power, the strongest that she can achieve is any  for which  i   hgivei i,
expressing that i can achieve  by either choosing an appropriate value for her variables, or by
distributing her variables over A in an appropriate way. Note that both  i  and hgivei i imply
hgivei ii , and hence any  for which hgivei ii  holds can be seen as what i can achieve on her
own. We will come back to the program givei below.
The program give can be generalized to incorporate coalitions that can give away variables,
and those that can receive: let
[
[[

controls(i, p)?;
hi ;p ji .
(21)
giveC;D =

iC pP

jD{i}

This program giveC;D lets an arbitrary agent i from the coalition C either give any of her
variables p to an arbitrary member of the coalition D, or do nothing (i.e., give them to herself).
Now, for objective formulas , we have the following, where i is a dedicated agent from C:

C   hgiveC;{i} ii .
In words: the agents in the coalition C can choose values for their variables such that , if and only
if they have a way to give all their variables to the dedicated agent i, who then can achieve . Note
that we are in general not able to eliminate all occurrences of  s, since this is the only way to
express first-order control, i.e., to reason about a different valuation.
For some examples in the language without transfer, we refer to a paper by van der Hoek and
Wooldridge (2005b), especially to the example Bach or Stravinsky, (i.e., Example 2.4, van der
Hoek & Wooldridge, 2005b). Before looking at two examples of control in a dynamic setting, note
that A allows the following inference, for any objective formula :
 is consistent  |=d

A 

(22)

This inference says that the grand coalition A can achieve any satisfiable objective formula.
463

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

Example 1 Suppose we have n agents: 1, . . . , n. Each controls a flag r i (i = 1 . . . n) to indicate that
they desire control over a particular resource, modeled with a variable p. It is not that they want p
to be true or false every now and then, (which could be taken care of by a central agent executing
a program making p false and true alternatively), but rather, they want to control p eventually. Let
+n denote addition modulo n, and, similarly, n subtraction modulo n. Let skip denote >?, i.e., a
test on a tautology. Consider the following program:
grant-req(i) =


if
if
...
if

controls(i, p) then skip else
ri+n 1
then (i ;p i +n 1) else
...
ri+n (n1)
then (i ;p i +n (n  1)) else skip

The program grant-req(i) makes agent i pass on the resource p whenever she has it and somebody else needs it, where the need is checked in the order starting with the agent with the next + n
index. Note that the use of this variable p, i.e., making it true or false, is not encoded in our
program constructs. Now consider the program
pass-on(i, j) =
 grant-req(i); . . . ; grant-req(j n 1).
The program pass-on(i, j) will pass control over the variable p to agent j, provided that initially r j
is set and one of the agents in the sequence i, i +n 1, . . . , j n 1 owns it. This can be expressed as
follows:
rj  controls({i, i +n 1, . . . , j n 1}, p)  [pass-on(i, j)]controls(j, p).
Now we have:
ri 

hpass-on(i +n 1, i)icontrols(i, p)
[pass-on(i +n 1, i)]controls(i, p) .

That is: if agent i flags a request ri for resource p, then, after the program pass-on(i +n 1, i) has
been executed, i will be under control of p.
Notice that the previous example freely passes on a variable along chains of agents, thereby
taking for granted that they can control that variable on the fly, and making it true or false at will. In
the following example, control over a variable is not only important, but also the truth of some side
conditions involving them.
Example 2 We have a scenario with three agents: two clients c 1 and c2 , and a server s. The
server always has control over one of the propositional variables p 1 and p2 , in particular s wants
to guarantee that those variables are never true simultaneously. At the same time, c 1 and c2 want
to ensure that at least one of the variables pi (i = 1, 2) is true, where variable pi belongs to client
ci . We can describe the invariant of the system with the formula Inv:
Inv =


_

controls(s, pi ) 

i=1,2

_

controls(ci , pi )

i=1,2

Consider the following transfer program :
=


(controls(s, p1 )? ; s ;p1 c1 ; c2 ;p2 s)

(controls(s, p2 )? ; s ;p2 c2 ; c1 ;p1 s) .
464

fiR EASONING A BOUT THE T RANSFER OF C ONTROL

This says that an arbitrary number of times one variable pi is passed from the server to the client
ci , and another variable pj (i 6= j) from the client cj to the server.
Using Inv and , we can describe the whole scenario as follows:
Inv  []Inv

Inv  [] s (p1  p2 )  {c1 ,c2 } (p1  p2 )

A general characterization of the types of formulas that agents and coalitions could control was
given by van der Hoek and Wooldridge (2005a), and our aim is now to undertake the same study for
DCL - PC . It will appear that this can be done on a local and a global level, but we will also see that
the notion of control that we inherited from CL - PC, has a natural generalization in our context.
The next corollary establishes a result concerning the characterization of control. Its first item
says that of no strict sub-coalition C 6= A of A, is it valid that C controls something. In other
words, control of such a coalition is always a feature of a specific model, in particular, a specific
allocation. According to the second item, the grand coalition A derivably,
W all models, controls
W or in
exactly those formulas  with the property that theirWequivalent form  (  ()  ) is such
that for every allocation description , the formula  () is a contingency,
W i.e., not a tautology
and neither a contradiction. If  is a propositional formula, we have    () and it is easy to
see that  being a contingency is sufficient and necessary to have |= K controls(A, ).
W On the other
hand, if  is for instance controls(i, p), then  () =  if   controls(i, p) and  () equals
>. And indeed, 6|=K controls(A, ). Contrast this with  = p  controls(i, p). If follows easily
from the truth definition of the Kripke semantics defined in Section 2.6, and from Theorem 2 and
Theorem 3.
Corollary 2 Let M be a Kripke model, C any coalition C  A with C 6= A, and let  be ranging
over DCL - PC formulas. Then it follows that:
1. For no , do we have |=K controls(C, ), and
W
W
2. |=K controls(A, ) iff the formula  (  ()  ), to which  is equivalent according
to Theorem 2, is such that for no allocation description ,   () =  or  () = .
Proof:
1. In order for controls(C, ) to be valid under the Kripke semantics, it has to be true at all worlds
in all Kripke models. Take any Kripke model M = h, RiA , i for which the allocation
 = hP1 , P2 , . . . , Pn i is such that Pj = , for all j  C. We then have M,  |=K C 
iff for some world  0   with  0 =  (mod PC ), it holds that M,  0 |=K . But, since
PC = , the only such  0 is  itself, so that we cannot have M,  |=K C   C . Hence,
M,  6|=K controls(C, ).
2. First we
W the left-to-right direction by contraposition. Let  be equivalent to the forW prove
mula  (  ()  ), which we have by Theorem 2. Suppose, for some allocation
description , that  () = . This means, for every Kripke model M = h, RiA ,  i
and for every valuation , that M,  |=K . Consequently, M,  |=K A . However, the
agents in A can only change , but not the current allocation   . Since  () = , A cannot choose a valuation that falsifies , i.e., M,  |=K A . Similarly, if  () = , we
465

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

have M,  |=K C , for each . But, given , A cannot choose a valuation satisfying  on
the current allocation described by , i.e., M,  |=K A . Hence, in either case we have
6|=K controls(A, ).
W
W
Consider the other direction from right to left. Suppose  is equivalent to  (  ())
while, for no , the set  () is either  or . Let M = h, RiA , i be a Kripke model.
Remember that  is the allocation description corresponding to the allocation . By the fact
that  6= , there is a valuation description    such that the corresponding valuation
 satisfies  at (M,  ). But then, A can choose  in order to satisfy , and thus we have
M,  |=K A , for any . Similarly, we have M,  |=K A  which follows by the fact that
 6= . Hence, we have |=K controls(A, ).
QED

One may ask for a more local characterization of what a coalition controls: for which Kripke
models M and valuations  do we have M,  |=K controls(C, )? For this notion of control, the
answer can be immediately read off from Theorem 6, to be given shortly. That theorem is about
a more general notion: to recover a characterization result for the current notion controls(i, ), we
would only need the items (1b) and (2b) of Theorem 6.
The notion of control discussed so far is that taken from CL - PC: we have lifted the characterization results to our richer language. However, as is clear from our discussion earlier in this section,
a more appropriate notion of control of an individual i in our language might be obtained using the
program givei , where givei is defined in (20). Note that i   hgivei ii  is valid, and hence
that hgivei i seems a more general way to reason about is control: it is about what i can achieve
both by toggling its propositional variables and delegating some of them. One can easily discuss
this at the coalitional level, by lifting Definition (20) to the case of coalitions C as it was suggested
in (21) with giveC;D . However, we stick to the individual case here for simplicity. Let us therefore
define
CONTROLS(i, )=
 hgivei ii   hgivei ii 



(23)

This definition says that agent i controls a formula  iff there is a way to distribute her propositional variables over the agents such that after i makes appropriate choices for her remaining variables,  holds, but there is also a way of distributing her variables that enables her to enforce .
From the validity of i   hgivei ii , we infer that controls(i, ) implies CONTROLS(i, ).
Notice that the implication the other way around is not valid since controls(i, ) can never be true
for control of other agents over variables. For example, CONTROLS(i, controls(j, p)) holds iff
p  Pi . From this, we know that controls(i, p)  CONTROLS(i, controls(j, p)) is a theorem,
which basically says that when having control over a variable, you can freely choose to keep it
or pass it on. However, controls(i, p)  controls(i, controls(j, p)) is not valid, we even have
controls(i, p)  controls(i, controls(j, p)): once agent i owns p, she cannot choose to keep p
or to pass it on by only toggling her propositional variables.
Before we state our characterization result, we introduce some more notation. For any two
Kripke models M = h, RiA , i and M0 = h, RiA ,  0 i in K(A, P) and for any agent i, we say
that M i M0 if the allocations  = hP1 , . . . , Pi , . . . , Pn i and  0 = hP01 , . . . , P0i , . . . , P0n i are such
that P0i  Pi and, for all j 6= i, Pj  P0j . That is, M0 is obtained from M by executing givei . In
such a case, we also say  i  0 .
466

fiR EASONING A BOUT THE T RANSFER OF C ONTROL

1



1 ()


givei

	

	 	

	 	

	 	

	 	

	 	

	

	
	
	
	
	
	 fffi 
	
	
	 
	
	
	 
	
	
	 
	
	
	
	

	 	

	 	

	  	

	 	

	 	

	

2





2 ()


2 () 6= 

3

       













































         

3 () 6= 

3 ()



Figure 9: Illustration of  

W



W


 ()   .

For each valuation description   , let  be the valuation that is described by , and, for
each allocation description   , let  be the allocation described by , and let Pi be the set of
propositional variables controlled by agent i in  .

W
W
Theorem 6 Let  be a DCL - PC formula with   
 ()   , as given by Theorem 2.
Let M = h, RiA , i be a Kripke model of K(A, P), and  a world in M. Then, for each agent
i  A,
M,  |=K CONTROLS(i, )
iff the following two conditions are satisfied:
1. There is a    and a    () such that
(a)  i  , and
(b)  =  (mod Pi ).
467

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

2. There is a    and a    \  () such that
(a)  i  , and
(b)  =  (mod Pi ).
We first demonstrate the requirements (1) and (2) of Theorem (6). Suppose that  satisfies (p  q 
r), Agent i = 1 owns p in M, Agent 2 owns q and r, and Agent 3 has no propositional variables in
M. First of all, to see why Item (1b) is needed, we have to guarantee that for some M 0 , 0 , it holds
that M0 , 0 |=K 1 . That means that, even after 1 has given away some of her atoms (resulting in
some allocation  0 ), she still should be able to make  true. This is possible for  = (p  q  r):
Agent 1 could simply stay within the current allocation  and just make p false. However, this is
not possible for  = (p  q  r  controls(3, p)) since, once 1 has delegated control of p to
3, agent 1 cannot make p false anymore. Moreover, an agent i can only give atoms away, so any
model with allocation  that makes it possible for her to satisfy  should be one for which   i  ,
which explains Item (1a). Item (2a) has exactly the same motivation, and requirement (2b) is easily
understood to be similar to (1b), once one realizes that the normal form of  can be expressed in
terms of the normal form of  as follows:
_ _

( \  ())   .
 


For a simple illustrating example, suppose there are only two allocations  1 and 2 , and  is equivalent to
((p  q)  (p  q))  1
 ((p  q)  (p  q))  2 .
Note that s normal form describes the valuations on 1 and 2 where  is satisfied. The normal
form of  is complementary to the one of  in the sense that it describes the valuations on  1 and
2 where  is falsified:
((p  q)  (p  q))  1
 ((p  q)  (p  q))  2 .
Proof: We illustrate our proof with a pictorial story that shows why the requirements
W 1 and 2 of
W
the theorem are both sufficient and necessary. Given that  is equivalent to  (  ()  )
by Theorem 2, semantically this means that  corresponds to a collection of the shaded areas,
as depicted in Figure 9. Now, for CONTROLS(i, ) to be true at a world  in a Kripke model
M = h, RiA , i, Agent i has to be able to move inside such a shaded area, and to move outside
it as well. But moving inside a shaded area, means being able to first go to a model with allocation
 , and then to a world  within that model such that the valuation description  is in  . Notice
that i can move to allocation  only by delegating control over her variables to other agents (hence
the requirement  i  ), and i can move to valuation  only by toggling her remaining variables
in Pi at  (hence the condition  =  (mod Pi )). This shows that Condition 1 is equivalent to
M,  |=K hgivei ii . Accordingly, Condition 2 corresponds to being able to move outside of a
shaded area in Figure 9. Semantically, this means being able to first go to a model with allocation
0 , and then to a world 0 within that model such that the valuation description  0 is not in 0 .
Consequently, Condition 2 is equivalent to M,  |=K hgivei ii , which finishes the proof. QED
468

fiR EASONING A BOUT THE T RANSFER OF C ONTROL

6. Possible Extensions and Refinements
In this section, we consider some possible extensions and refinements to the framework we have
presented in this paper. We do not claim to have substantial results relating to these extensions  the
aim is simply to indicate some possible directions for future research.
6.1 Separating First- and Second-Order Control
In DCL - PC as presented here, an agent can assign a value to a variable (exercising first-order control)
iff it can give this variable away (exercising second-order control). That is, for any pair of agents
i, j and propositional variable p, we have the following.
|=d controls(i, p)  hi ;p ji>

(24)

A moments reflection should confirm that this is not always how things work in human societies. We might empower an individual to make a choice on our behalf, but we might not be happy
with the idea that this individual could in turn transfer this power to somebody else. Sometimes, it
might be acceptable; but certainly not in all cases.
We can straightforwardly distinguish between these situations by extending our models and
modifying the semantics of our language as follows. A model M is now defined to be a structure:
M = hA, P, , 0 , i
where the components A, P, 0 , and  are as originally defined, and  = h1 , . . . , n i is a tuple of
subsets of P, with elements indexed by agents A, such that  1 , . . . , n forms a partition of P.
Now, the intended interpretation of such models is as follows:
 the partition 0 defines who (initially) has the ability to assign values to which variables (i.e.,
who has first-order control of variables); while
 the partition  defines who can transfer control of which variables (i.e., who has second-order
control of variables).
Syntactically, the logic that we define to reason about such structures is identical to DCL - PC,
however the semantics are different. In fact, the only element of the semantics that we need to
change relates to the definition of the accessibility relation for atomic transfer programs.
Let M = hA, P, , 0 , i and M0 = hA, P, , 00 , i be two models with 0 = hP1 , . . . , Pn i and
0
0 = hP01 , . . . , P0n i. Then:
(M, M0 )  Ri;p j
iff
1. p  i (agent i has second-order control of p to begin with)
2. for all k  A, if p  Pk then:
(a) if k = j then l  A, Pl = P0l .
(b) if k 6= j then:
469

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

 P0j = Pj  {p},
 P0k = Pk \ {p},
 l  A \ {j, k}, Pl = P0l .
With this setup, first-order control is dynamic, and can be changed by transfer programs, while
second-order control as defined in  is static. Moreover, the fact that an agent has first-order control
of a variable does not mean it has second-order control: we no longer have the equivalence (24).
6.2 Hierarchies and Networks of Control
Of course, there is no reason why one should stop at second-order control. One could extend the
setup with a finite hierarchy of control levels, with each level u > 1 defining who can transfer
control of variables at level u  1, and level u = 1 defining who can exercise first-order control. We
then need to extend atomic programs to indicate which level of control is being transferred. Atomic
programs then take the form:
i ;up j
to mean that agent i transfers level u control to agent j. The semantics of the language become yet
more involved, but are straightforward to define. Somewhat related ideas were studied by Boella
and van der Torre (2008).
Another direction is to consider multiple agents having write access to propositional variables.
For example, we might consider an authority relation P  A  A, with the intended interpretation
that (i, j)  P means that everything that i is empowered to everything that j is empowered to do.
Propositional variables are then allocated to sink nodes in P (i.e., agents with no outgoing edges
in P). One might then ask, for example, whether structural properties of the graph P characterise
formulae of the object language.

7. Related Work
Although other researchers have begun to develop formal systems for reasoning about delegation
and the transfer of control (e.g., Li, Grosof, & Feigenbaum, 2003), to the best of our knowledge
DCL - PC is the first such system to have a rigorous semantics, and a complete axiomatization. Also,
the emphasis of Li et al. (2003) is on decentralized trust management, in which roles like that of a
requester, credentials and an authorizer are distinguished. In the work presented here, the emphasis
is more on what coalitions can achieve, if they are allowed to hand over control over propositional
variables.
Norman and Reed (2002) consider a logic of delegation, particularly focussing on group delegation. The logic underpinning this work is a STIT (sees to it that) logic, in which the main operator
is of the form Si A, meaning agent i sees to it that A. This extends to delegation by considering
expressions of the form Si Sj (i sees to it that j sees to it that . . . ). For example, an axiom in the
resulting system is:
Si Sj A  Si A.
The work of Norman and Reed represents a serious attempt to develop a philosophically robust logic
for delegation, appropriate for use in computational systems. However, their notion of delegation is
very different to ours, (crudely, agents delegate responsibility, rather than transfer control), and the
470

fiR EASONING A BOUT THE T RANSFER OF C ONTROL

dynamic logic flavour of DCL - PC is absent. Finally, relatively few technical results relating to the
logic are presented.
Jones and Sergot (1996) consider the problem of reasoning about power that an individual obtains by virtue of an organisational role. There, the notion of actions that are carried out in order
to empower agents with certain capabilities is central, and Jones and Sergot also consider the interplay of such actions and ability. However, the logical formalisation is rather different  again a
STIT -like language is used, rather than our dynamic logic framework, and relatively few technical
results relating to the framework are presented. However, the setting of Jones and Sergot (1996) is
much more general than ours: we focus only on propositional control. In somewhat related work,
Boella and van der Torre (2006) present a formalisation of power delegation in the setting of normative multi-agent systems. They consider, for example, the issue of how delegated goals interact
with other goals. The framework provides a rich and compelling setting for investigating questions
relating to delegation. However, no overarching object language is developed for representing this
framework, and relatively few technical results are presented relating to the framework. It would
be interesting to consider whether the dynamic logic approach developed within the present paper
might be adapted to the framework of Boella and van der Torre.
With respect to logics for reasoning about controlled variables, Boutilier (1994) presents a logic
intended to capture notions such as I can achieve my plan using actions that only relate to variables
under my control. In spirit, this logic is very close to the kind of situation we are aiming to model,
although the technical details of Boutiliers logic (the way control is captured in the logic) are
different. Moreover, Boutiliers logic does not consider multi-agent aspects, or the dynamics of
control as we do in the present paper.
We refer the reader to the work of van der Hoek and Wooldridge (2005b) for an extensive discussion and many references for logics of ability. Gerbrandy (2006) generalises the results of van
der Hoek and Wooldridge, by considering situations in which an agent has only partial control of
a variable, or where it shares control with others. Gerbrandy also shows how logics of propositional control are related to cylindrical modal logic (Venema, 1995). Specifically, the generalisation
of CL - PC considered by Gerbrandy can be understood as a cylindrical modal logic, immediately
yielding a complete axiomatization and decidability/undecidability results for various fragments of
the system. A somewhat related formalism is discussed by van Benthem, Girard, and Roy (2009).
This formalism is intended to enable reasoning about ceteris paribus preferences (in the sense of
all other things being equal). Van Benthem et al. develop a logic with a modality hi, where 
is a set of propositional formulae; the intended interpretation of hi in a state u is that there is a
state v agreeing with u on the valuation of formulae  in which  is true. There seems quite a close
connection between DCL - PC and the formalism of van Benthem et al., although we leave the details
for future work.
Our framework explains control in terms of what the agents can change or whcih atoms they can
choose to be true or false. Sauro (2006) addresses the question of how agents can change the world,
where control of coalitions is defined in terms of actions in the agents repertoire. Finally, note that
as discussed by van der Hoek and Wooldridge (2005b), the logic CL - PC is closely related to the
well-known formalism of quantified Boolean formulae, and it is not hard to see that there is also
a close relationship between DCL - PC and quantified Boolean formulae. However, while we may
not ultimately have any gain is formal expressive power when using DCL - PC rather than quantified
Boolean formulae, we do benefit with respect to the naturalness of expression in DCL - PC. Quantified
Boolean formulae have no explicit notion of agency or the dynamics of control, and representing
471

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

these aspects within quantified Boolean formulae leads to formulae that are unintuitive and hard to
understand.

8. Conclusions
In this paper, we have built upon the logic CL - PC of strategic cooperative ability, in which the control
that agents have over their environment is represented by assigning them specific propositional
variables, for which the agents that own them can determine their truth value. We added a dynamic
component to this logic, thus obtaining the language DCL - PC in which one can reason about what
agents (and coalitions of agents) can achieve by setting their assigned variables, or by giving the
control over them to others. We gave two different but equivalent semantics for this language 
a direct and a more conventional Kripke semantics  and provided a complete axiomatization for
them. The key property that establishes the proof of completeness for DCL - PCs axiomatic system is
the fact that every formula in the language is provably equivalent to a normal form: a disjunction of
conjunctions of literals over propositional variables p and assertions of the form controls(i, p). We
also investigated the complexity of the model checking and satisfiability problems for DCL - PC, and
showed that these problems are no worse than for the program-free fragment CL - PC: they are both
PSPACE -complete. We demonstrated that, for the special case where ability in ATL is interpreted as
in ( D ) CL - PC, this implies a simpler satisfiability problem for ATL.
There are several avenues for further development of this work. First of all, it is interesting to
add the assignments that the agents can perform to the transfer actions they can perform, so that the
two dimensions of what agents can achieve become projected in one dimension. Although parallel
execution is not a program construct in our language, and hence one could still not model situations
where an agent chooses some values for its atoms, and at the same time transfers control of some
other atoms, one could at least reason about the effect of programs that do a combination of truth
assignments and transfer of control in sequence, or in a choice. Secondly, in many realistic systems, Property (22) may be too general: often, we want to specify that the overall system satisfies
some constraints. For this, it seems appropriate, however, not only to reason about what agents
can achieve, but also about what they should guarantee. The framework of Social Laws (Moses &
Tennenholtz, 1995; van der Hoek, Roberts, & Wooldridge, 2005) could be set to work in order to
express that under certain conditions, an agent will not set a certain propositional variable to true,
or that she will not pass on control over a certain variable to a specific agent, or that the overall system behaves in such a way that every agent gets a fair chance to trigger a specific variable (i.e., use a
specific resource) infinitely often. Another interesting direction would be to consider how to allow
for the fact that agents outside a transfer program might change their variables while the program is
executing. This might require some consideration of the semantics of parallel action. Relatedly, it
would be interesting to make it possible to capture the temporal properties of the system, outside of
transfer programs. Here, some combination of temporal and dynamic logic might be appropriate.
Similarly, we could weaken the allocation axiom to allow for some propositional not being
under control of any agents, capturing the idea that not all facts are modifiable (by the agents under
consideration). Another extension would be to assign control over atoms to coalitions, rather than
individual agents. This could cater for power in social contexts, with the typical example being
that any coalition bigger than a threshold n can lift a piano. Finally, an implementation of a theorem
prover for the logic would of course be interesting. Finally, an implementation of a theorem prover
for the logic would of course be interesting.
472

fiR EASONING A BOUT THE T RANSFER OF C ONTROL

Acknowledgments The authors wish to thank the JAIR reviewers and the editors for their useful comments. Michael Wooldridge and Dirk Walther were supported by the EPSRC under grant
GR/S62727/01.

Appendix A. Proofs
Theorem 1.
1. The schemes in Figure 6 are derivable in DCL - PC.
2. The axioms K(i), T(i), B(i), and effect(i) have coalitional counterparts K(C), T(C), B(C),
and effect(C) that are all derivable for any coalition C.
W
3. ` controls(C, p)  iC controls(i, p).

4. The property persistence1 (control) is also derivable when we replace agent i by an arbitrary
coalition C.
Proof:
1. We describe how the eight schemes in Figure 6 can be derived in the axiomatic system of
DCL - PC .
 For at-least(control), this follows directly from axiom effect(i), with taking  = >.
 For at-most(control), from `(p) we get, using axiom T(i) and contraposition,  i `(p).
Assuming moreover i `(p), with axiom control, gives controls(i, p). From allocation
we then obtain controls(j, p) for any agent j 6= i, and, using control(j), we get  j p 
j p, i.e., j `(p)  j `(p). Since T(j) gives us j `(p), we obtain j `(p), i.e.,
2j `(p).
 To prove non-effect(i), assume i `(p)  controls(i, p). Then axiom control(i) yields
i `(p), which is equivalent to 2i `(p).
 For persistence(non-control), the right-to-left direction follows immediately from T(j).
For the left-to-right direction, assume that controls(i, p). From allocation we derive
that
controls(1, p)5    5controls(i  1, p)5controls(i + 1, p)5    5controls(n, p),
W
and from this, by persistence1 (control) we get k6=i 2j controls(k, p). For every k 6= i,
we have controls(k, p)  controls(i, p), which follows from allocation. Hence, using
Necessitation, we have 2j (controls(k, p)  controls(i, p)). From W
Axiom K(j), it now
follows 2j controls(k, p)  2j controls(i, p). Combining this with k6=i 2j controls(k, p),
we obtain the desired conclusion 2j controls(i, p).

 Notice that atomic permanence(;) does not place any requirement on p, i or j. Also, for
the program i ;p j, under the condition that hi ;p ji>, by func and preconditions(transfer),
hi ;p ji and [i ;p j] are equivalent. Formally:
hi ;p ji>  (hi ;p ji  [i ;p j])
473

(25)

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

Now, we prove objective permanence(;) by induction on . The induction base, where
 is a propositional variable, follows from atomic permanence(;). Consider the induction step. Suppose the theorem is proven for  and take  = . Assume hi ; p ji>. It
is to show that   [i ;p j] which is equivalent to showing that   hi ;p ji.
This follows from (25) and the induction hypothesis. As a final step in the induction,
suppose objective permanence(;) is proven for 1 and 2 . This means we can assume
that
hi ;p ji>  (1  [i ;p j]1 )  (2  [i ;p j]2 )



(26)

Now take  = 1  2 . Obviously, if we have 1  2 , we have [i ;p j](1  2 ),
which proves hi ;p ji>  (  [i ;p j]). For the other direction, suppose, given
hi ;p ji>, that [i ;p j](1  2 ). We use (25) to conclude hi ;p ji(1  2 ), and
by classical modal reasoning we then obtain hi ;p ji1  hi ;p ji2 . By (25) and the
induction hypothesis we get (1  2 ), which concludes our proof.
 Using K( ) and Necessitation, we can derive the fact h i>  [ ]. Using this fact, it
is possible to show with propositional reasoning that objective permanence is equivalent
to
[ ]  (h i>  )

(27)

The proof is by induction on the structure of the transfer program  . The first case of
the induction base, where  is an atomic program, holds by atomic permanence(;).
For the second case, suppose  is a test ?. Notice that, by the axiom test( ), h?i
is equivalent to   , and thus h?i> is equivalent to . But then, (27) is equivalent
test( ).
Consider the induction step where  = 1 ; 2 . The induction hypothesis tells us hi i> 
([i ]  ), for all objective  and i = 1, 2. Assume h1 ; 2 i>; this implies h1 ih2 i>
and h1 i> by comp( ), and, by the induction hypothesis for  1 , ([1 ]  ). For any
diamond operator, and hence also for h1 i, we have that if an implication    0 is
derivable, we can also derive h1 i  h1 i 0 using Necessitation and K( ). Applying this to the induction hypothesis for 2 , i.e., to h2 i>  ([2 ]  ), we obtain
h1 ih2 i>  h1 i([2 ]  ), and, with Modus Ponens, we arrive at h1 i([2 ] 
). We like to demonstrate that [1 ; 2 ]  . By comp( ), this is equivalent to
[1 ][2 ]  . For the direction from left to right, assume [1 ][2 ]. It is again a modal
principle to conclude h1 i 0 from h1 i(   0 ) and [1 ]. Taking  = [2 ] and
 0 = , we obtain h1 i. To show that  holds, suppose that . This is still an objective formula, so we can apply the induction hypothesis to conclude [ 1 ]. This, of
course, contradicts h1 i, so that indeed we conclude . Conversely, suppose . Then,
by the induction hypothesis for 1 , we also have [1 ]. The induction hypothesis for 2 ,
h2 i  ([2 ]  ) implies   [2 ], and we can apply necessitation and K(1 ) to
this to derive [1 ]  [1 ][2 ].
Now, consider  = 1  2 , and objective permanence proven for 1 and 2 . By axiom
union( ), we have h1  2 i>  (h1 i>  h2 i>). But then, given h1  2 i>, we
have: [1  2 ]  ([1 ]  [2 ]), and the induction hypothesis explains why the
right-hand side of this equivalence is equivalent to .
474

fiR EASONING A BOUT THE T RANSFER OF C ONTROL

Finally, consider  = 1 . By the axiom mix( ), we immediately have h1 i> 
([1 ]  ). For the other direction, recall that from the induction hypothesis we
can derive, as a validity,   [1 ]. Using Necessitation for [1 ] then gives [1 ]( 
[1 ]). But then, using the assumption  and the axiom ind( ) gives us [ 1 ], hence we
also have h1 i>  (  [1 ]).
 For axiom inverse, we rely on the normal form obtained in Theorem 2 (the proof of
which does not involve inverse). So we know that every  is equivalent to a disjunction
of formulas of the form 1  2 , where 1 is an objective formula, and 2 a conjunction of formulas of the form controls(h, q). We now show that both  1 and 2 satisfy
inverse, from which the result follows for arbitrary . So assume controls(i, p). By
precondition(transfer) this entails hi ;p ji>, and hence we can apply objectivepermanence(;
) and func twice to conclude 1  [i ;p j; j ;p i]1 .
Now we consider formulas 2 , starting by their base case controls(h, q). Assume
controls(i, p): we now first show the left to right direction. If p 6= q, we get from
persistence2 (control) that [i ;p j; j ;p i]q. If p = q we consider three subcases: (1)
h = i. We then derive, from controls(i, p), using transfer, that hi ; p jicontrols(j, p) and
controls(j, p)  hj ;p iicontrols(i, p): with func and comp( ), this gives controls(i, p) 
[i ;p j; j ;p i]controls(i, p). (2) h 6= i, h = j. Given controls(i, p), we then have [i ; p
j]   for any , and we are done. (3) h 6= i, h 6= j. We can use persistence 2 (control)
twice to get controls(h, q)  [i ;p j; j ;p i]controls(h, q). Finally, given controls(i, p),
we derive the right to left direction, that is, we derive [i ;p j; j ;p i]controls(h, q) 
controls(h, q). First assume p 6= q and suppose we would have controls(h, q). Then,
by allocation, we have that for some agent k 6= h, we have controls(k, q), and by
persistence2 (control) we get [i ;p j; j ;p i]controls(k, q), which clearly contradicts
[i ;p j; j ;p i]controls(h, q). Now suppose p = q. Again, we have three subcases. (1)
If h = i, the conclusion follows from the overall assumption controls(i, p). (2) Suppose
h 6= i, h 6= j. The same reasoning applies as in case (1). Finally, (3) suppose h 6= i, h = j.
Since controls(i, p) is given, we have hi ;p jicontrols(j, p) (by transfer), and hence
hi ;p j; j ;p iicontrols(i, p). Now, if we would have [i ;p j; j ;p i]controls(j, p),
with i 6= j, this leads to a contradiction (use allocation and the fact that h 6= i, h = j), so
indeed we derive [i ;p j; j ;p i]controls(j, p)  controls(j, p).
 For reverse, similar to inverse.
2. This was proved by van der Hoek and Wooldridge (2005b).
3. The definition of controls(C,
p) is C p  W
C p. Let C = {a1 , a2 , . . . , aC }. By axiom
W
control(i), we have iC controls(i, p)  iC (i p  i p). By the contrapositive of
T(i), we have   i . We can apply this repeatedly for all agents in C, giving  
a1 a2    aC . This is, according to Comp-, the same
W that we have
W as   C . (Note
now proven the contrapositive of T(C).) This gives us iC controls(i, p)  iC (C i p 
C i p). Using Comp- again, we see that the consequent of this implication is equivalent
to C p  C p. For the other direction, we first show at  most(control) of Figure 6. From
`(p) we get, using axiom T(i) and contraposition, i `(p). Assuming moreover i `(p), with
axiom control(i), gives controls(i, p). From allocation we then obtain controls(j, p), for any
475

fiVAN DER

H OEK , WALTHER , & W OOLDRIDGE

agent j 6= i. Using control(j), we get j p  j p, i.e., j `(p)  j `(p). Since T(j)
gives us j `(p), we obtain j `(p), i.e., 2j `(p).
W
W
Now suppose  iC controls(i, p). By allocation, we have xA\C controls(x, p). That
means that for one such x, we have x p  x p. Now we do a case distinction based on
p  p. In the first case, we assume p, and derive, for all i  C, that 2 i p, and thus 2C p.
Hence C p, from which we get controls(C, p). In the case of p, we similarly have, for
all i  C, that 2i p, which gives C p, and again controls(C, p). All in all, no matter
whether p or p, we get controls(C, p).
W
4. This is easy: the previous item showed W
that controls(C, p) means that iC controls(i, p).
Applying persistence1 (control), we get iC 2j controls(i, p). But since controls(i, p) 
controls(C, p) if i  C, we also have, for any i  C, that 2j controls(i, p)  2j controls(C, p)
(use Necessitation and K(j)). This proves 2j controls(C, p).
QED

References
Alur, R., Henzinger, T. A., & Kupferman, O. (2002). Alternating-time temporal logic. Journal of
the ACM, 49(5), 672713.
Blackburn, P., de Rijke, M., & Venema, Y. (2001). Modal Logic. Cambridge University Press:
Cambridge, England.
Boella, G., & van der Torre, L. (2006). Delegation of power in normative multiagent systems.
In Deontic Logic and Artificial Normative Systems, 8th International Workshop on Deontic
Logic in Computer Science, DEON 2006, Utrecht, The Netherlands.
Boella, G., & van der Torre, L. (2008). Institutions with a hierarchy of authorities in distributed
dynamic environments. Artificial Intelligence and Law, 16(1), 5371.
Boutilier, C. (1994). Toward a logic for qualitative decision theory. In Proceedings of Knowledge
Representation and Reasoning (KR&R-94), pp. 7586.
Chellas, B. (1980). Modal Logic: An Introduction. Cambridge University Press: Cambridge, England.
French, T. (2006). Bisimulation Quantifiers for Modal Logic. Ph.D. thesis, The University of
Western Australia, Perth, Australia.
Gerbrandy, J. (2006). Logics of propositional control. In Proceedings of the Fifth International
Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS-2006), pp. 193
200, Hakodate, Japan.
Ghilardi, S., & Zawadowski, M. (2000). From bisimulation quantifiers to classifying toposes. In
Wolter, F., Wansing, H., de Rijke, M., & Zakharyaschev, M. (Eds.), Advances in Modal Logic,
pp. 193220.
Goranko, V., & Jamroga, W. (2004). Comparing semantics of logics for multi-agent systems. Synthese, 139(2), 241280. In section Knowledge, Rationality and Action.
Harel, D., Kozen, D., & Tiuryn, J. (2000). Dynamic Logic. The MIT Press: Cambridge, MA.
476

fiR EASONING A BOUT THE T RANSFER OF C ONTROL

Jamroga, W., & van der Hoek, W. (2004). Agents that know how to play. Fundamenta Informaticae,
63(2-3), 185219.
Jones, A. J. I., & Sergot, M. (1996). A formal characterisation of institutionalised power. Logic
Journal of the IGPL, 3, 427443.
Li, N., Grosof, B. N., & Feigenbaum, J. (2003). Delegation logic: A logic-based approach to distributed authorization. ACM Transactions on Information and System Security, 6(1), 128 
171.
Moses, Y., & Tennenholtz, M. (1995). Artificial social systems. Computers and AI, 14(6), 533562.
Norman, T. J., & Reed, C. (2002). Group delegation and responsibility. In Proceedings of the First
International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS2002), pp. 491498, Bologna, Italy.
Pauly, M. (2001). Logic for Social Software. Ph.D. thesis, University of Amsterdam. ILLC Dissertation Series 2001-10.
Sauro, L. (2006). Formalizing Admissibility Criteria in Coalition Formation among Goal Directed
Agents. Ph.D. thesis, University of Turin, Turin, Italy.
Savitch, W. J. (1970). Relationships between nondeterministic and deterministic tape complexities.
Journal of Computer and Systems Sciences, 4(2), 177192.
van Benthem, J., Girard, P., & Roy, O. (2009). Everything else being equal: A modal logic for
ceteris paribus preferences. Journal of Philosophical Logic, 38, 83125.
van der Hoek, W., Roberts, M., & Wooldridge, M. (2005). Knowledge and social laws. In Dignum,
F., Dignum, V., Koenig, S., Kraus, S., Singh, M., & Wooldridge, M. (Eds.), Proceedings of
the Fourth International Joint Conference on Autonomous Agents and Multi-Agent Systems
(AAMAS 05), pp. 674681, New York, USA. ACM Inc.
van der Hoek, W., & Wooldridge, M. (2003). Time, knowledge, and cooperation: Alternating-time
temporal epistemic logic and its applications. Studia Logica, 75(1), 125157.
van der Hoek, W., & Wooldridge, M. (2005a). On the dynamics of delegation, cooperation, and
control: A logical account. In Dignum, F., Dignum, V., Koenig, S., Kraus, S., Singh, M.,
& Wooldridge, M. (Eds.), Proceedings of the Fourth International Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS 05), pp. 701708, New York, USA. ACM
Inc.
van der Hoek, W., & Wooldridge, M. (2005b). On the logic of cooperation and propositional control.
Artificial Intelligence, 64, 81119.
Venema, Y. (1995). Cylindric modal logic. Journal of Symbolic Logic, 60, 591623.

477

fiJournal of Artificial Intelligence Research 37 (2010) 397-435

Submitted 11/09; published 03/10

Training a Multilingual Sportscaster:
Using Perceptual Context to Learn Language
David L. Chen
Joohyun Kim
Raymond J. Mooney

DLCC @ CS . UTEXAS . EDU
SCIMITAR @ CS . UTEXAS . EDU
MOONEY @ CS . UTEXAS . EDU

Department of Computer Science
The University of Texas at Austin
1 University Station C0500, Austin TX 78712, USA

Abstract
We present a novel framework for learning to interpret and generate language using only perceptual context as supervision. We demonstrate its capabilities by developing a system that learns to
sportscast simulated robot soccer games in both English and Korean without any language-specific
prior knowledge. Training employs only ambiguous supervision consisting of a stream of descriptive textual comments and a sequence of events extracted from the simulation trace. The system
simultaneously establishes correspondences between individual comments and the events that they
describe while building a translation model that supports both parsing and generation. We also
present a novel algorithm for learning which events are worth describing. Human evaluations of
the generated commentaries indicate they are of reasonable quality and in some cases even on par
with those produced by humans for our limited domain.

1. Introduction
Most current natural language processing (NLP) systems are built using statistical learning algorithms trained on large annotated corpora. However, annotating sentences with the requisite parse
trees (Marcus, Santorini, & Marcinkiewicz, 1993), word senses (Ide & Jeronis, 1998) and semantic
roles (Kingsbury, Palmer, & Marcus, 2002) is a difficult and expensive undertaking. By contrast,
children acquire language through exposure to linguistic input in the context of a rich, relevant,
perceptual environment. Also, by connecting words and phrases to objects and events in the world,
the semantics of language is grounded in perceptual experience (Harnad, 1990). Ideally, a machine
learning system would be able to acquire language in a similar manner without explicit human supervision. As a step in this direction, we present a system that can describe events in a simulated
soccer game by learning only from sample language commentaries paired with traces of simulated
activity without any language-specific prior knowledge. A screenshot of our system with generated
commentary is shown in Figure 1.
While there has been a fair amount of research on grounded language learning (Roy, 2002;
Bailey, Feldman, Narayanan, & Lakoff, 1997; Barnard, Duygulu, Forsyth, de Freitas, Blei, & Jordan, 2003; Yu & Ballard, 2004; Gold & Scassellati, 2007), most of the focus has been on dealing
with raw perceptual data rather than language issues. Many of these systems aimed to learn meanings of words and phrases rather than interpreting entire sentences. Some more recent work has dealt
with fairly complex language data (Liang, Jordan, & Klein, 2009; Branavan, Chen, Zettlemoyer, &
c
2010
AI Access Foundation. All rights reserved.

fiC HEN , K IM , & M OONEY

Figure 1: Screenshot of our commentator system
Barzilay, 2009) but do not address all three problems of alignment, semantic parsing, and natural
language generation. In contrast, our work investigates how to build a complete language learning
system using parallel data from the perceptual context. We study the problem in a simulated environment that retains many of the important properties of a dynamic world with multiple agents and
actions while avoiding many of the complexities of robotics and computer vision. Specifically, we
use the RoboCup simulator (Chen, Foroughi, Heintz, Kapetanakis, Kostiadis, Kummeneje, Noda,
Obst, Riley, Steffens, Wang, & Yin, 2003) which provides a fairly detailed physical simulation
of robot soccer. While several groups have constructed RoboCup commentator systems (Andre,
Binsted, Tanaka-Ishii, Luke, Herzog, & Rist, 2000) that provide a textual natural-language (NL)
transcript of the simulated game, their systems use manually-developed templates and are not based
on learning.
Our commentator system learns to semantically interpret and generate language in the RoboCup
soccer domain by observing an on-going commentary of the game paired with the evolving simulator state. By exploiting existing techniques for abstracting a symbolic description of the activity on
the field from the detailed states of the physical simulator (Andre et al., 2000), we obtain a pairing
of natural language with a symbolic description of the perceptual context in which it was uttered.
However, such training data is highly ambiguous because each comment usually co-occurs with several events in the game. We integrate and enhance existing methods for learning semantic parsers
and NL generators (Kate & Mooney, 2007; Wong & Mooney, 2007) in order to learn to understand
and generate language from such ambiguous training data. We also develop a system that, from the
same ambiguous training data, learns which events are worth describing, so that it can also perform
strategic generation, that is, deciding what to say as well as how to say it (tactical generation). 1
1. For conciseness, we use this terminology from early work in generation (e.g., McKeown, 1985). Strategic and tactical
generation are now also commonly referred to as content selection and surface realization, respectively

398

fiT RAINING A M ULTILINGUAL S PORTSCASTER

We evaluate our system and demonstrate its language-independence by training it to generate
commentaries in both English and Korean. Experiments on test data (annotated for evaluation purposes only) demonstrate that the system learns to accurately semantically parse sentences, generate
sentences, and decide which events to describe. Finally, subjective human evaluation of commentated game clips demonstrate that in our limited domain, the system generates sportscasts that are
in some cases similar in quality to those produced by humans.
There are three main contributions we make in this paper. First, we explore the possibility of
learning grounded language models from the perceptual context in the form of ambiguous parallel
data. Second, we investigate several different methods of disambiguating this data and determined
that using a combined score that includes both tactical and strategic generation scores performed the
best overall. Finally, we built a complete system that learns how to sportscast in multiple languages.
We carefully verified through automatic and human evaluations that the system is able to perform
several tasks including disambiguating the training data, semantic parsing, tactical and strategic
generation. While the language involved in this work is restricted compared to handcrafted commercial sportscasting systems, our goal was to demonstrate the feasibility of learning a grounded
language system with no language-specific prior knowledge.
The remainder of the paper is structured as follows. Section 2 provides background on previous work that we utilize and extend to build our system. Section 3 describes the sportscasting
data we collected to train and test our approach. Section 4 and Section 5 present the details of our
basic methods for learning tactical and strategic generation, respectively, and some initial experimental results. Section 6 discusses extensions to the basic system that incorporate information from
strategic generation into the process of disambiguating the training data. Section 7 presents experimental results on initializing our system with data disambiguated by a recent method for aligning
language with facts to which it may refer. Section 8 discusses additions that try to detect superfluous sentences that do not refer to any extracted event. Section 9 presents human evaluation of our
automatically generated sportscasts. Section 10 reviews related work, Section 11 discusses future
work, and Section 12 presents our conclusions.

2. Background
Systems for learning semantic parsers induce a function that maps natural-language (NL) sentences
to meaning representations (MRs) in some formal logical language. Existing work has focused on
learning from a supervised corpus in which each sentence is manually annotated with its correct MR
(Mooney, 2007; Zettlemoyer & Collins, 2007; Lu, Ng, Lee, & Zettlemoyer, 2008; Jurcicek, Gasic,
Keizer, Mairesse, Thomson, & Young, 2009). Such human annotated corpora are expensive and
difficult to produce, limiting the utility of this approach. Kate and Mooney (2007) introduced an
extension to one such system, K RISP (Kate & Mooney, 2006), so that it can learn from ambiguous
training data that requires little or no human annotation effort. However, their system was unable to
generate language which is required for our sportscasting task. Thus, we enhanced another system
called WASP (Wong & Mooney, 2006) that is capable of language generation as well as semantic
parsing in a similar manner to allow it to learn from ambiguous supervision. We briefly describe
the previous systems below. All of these systems assume they have access to a formal deterministic
context-free grammar (CFG) that defines the formal meaning representation language (MRL). Since
MRLs are formal computer-interpretable languages, such a grammar is usually easily available.
399

fiC HEN , K IM , & M OONEY

2.1 KRISP and KRISPER
K RISP (Kernel-based Robust Interpretation for Semantic Parsing) (Kate & Mooney, 2006) uses
support vector machines (SVMs) with string kernels to build semantic parsers. SVMs are state-ofthe-art machine learning methods that learn maximum-margin separators to prevent over-fitting in
very high-dimensional data such as natural language text (Joachims, 1998). They can be extended to
non-linear separators and non-vector data by exploiting kernels that implicitly create an even higher
dimensional space in which complex data is (nearly) linearly separable (Shawe-Taylor & Cristianini,
2004). Recently, kernels over strings and trees have been effectively applied to a variety of problems
in text learning and NLP (Lodhi, Saunders, Shawe-Taylor, Cristianini, & Watkins, 2002; Zelenko,
Aone, & Richardella, 2003; Collins, 2002; Bunescu & Mooney, 2005). In particular, K RISP uses
the string kernel introduced by Lodhi et al. (2002) to classify substrings in an NL sentence.
First, K RISP learns classifiers that recognize when a word or phrase in an NL sentence indicates
that a particular concept in the MRL should be introduced into its MR. It uses production rules
in the MRL grammar to represent semantic concepts, and it learns classifiers for each production
that classify NL substrings as indicative of that production or not. When semantically parsing a
sentence, each classifier estimates the probability of each production covering different substrings
of the sentence. This information is then used to compositionally build a complete MR for the
sentence. Given the partial matching provided by string kernels and the over-fitting prevention
provided by SVMs, K RISP has been experimentally shown to be particularly robust to noisy training
data (Kate & Mooney, 2006).
K RISPER (Kate & Mooney, 2007) is an extension to K RISP that handles ambiguous training
data, in which each sentence is annotated with a set of potential MRs, only one of which is correct.
Psuedocode for the method is shown in Algorithm 1. It employs an iterative approach analogous to
expectation maximization (EM) (Dempster, Laird, & Rubin, 1977) that improves upon the selection
of the correct NLMR pairs in each iteration. In the first iteration (lines 3-9), it assumes that all of
the MRs paired with a sentence are correct and trains K RISP with the resulting noisy supervision.
In subsequent iterations (lines 11-27), K RISPER uses the currently trained parser to score each
potential NLMR pair, selects the most likely MR for each sentence, and retrains the parser on the
resulting disambiguated supervised data. In this manner, K RISPER is able to learn from the type of
weak supervision expected for a grounded language learner exposed only to sentences in ambiguous
contexts. However, the system has previously only been tested on artificially corrupted or generated
data.
2.2 WASP and WASP1
WASP (Word-Alignment-based Semantic Parsing) (Wong & Mooney, 2006) uses state-of-the-art
statistical machine translation (SMT) techniques (Brown, Cocke, Della Pietra, Della Pietra, Jelinek,
Lafferty, Mercer, & Roossin, 1990; Yamada & Knight, 2001; Chiang, 2005) to learn semantic
parsers. SMT methods learn effective machine translators by training on parallel corpora consisting
of human translations of documents into one or more alternative natural languages. The resulting
translators are typically significantly more effective than manually developed systems and SMT has
become the dominant approach to machine translation. Wong and Mooney (2006) adapted such
methods to learn to translate from NL to MRL rather than from one NL to another.
First, an SMT word alignment system, GIZA++ (Och & Ney, 2003; Brown, Della Pietra, Della
Pietra, & Mercer, 1993), is used to acquire a bilingual lexicon consisting of NL substrings coupled
400

fiT RAINING A M ULTILINGUAL S PORTSCASTER

Algorithm 1 K RISPER
input sentences S and their associated sets of meaning representations M R(s)
output BestExamplesSet, a set of NL-MR pairs,
SemanticModel , a K RISP semantic parser
1:
2:
3:
4:
5:
6:
7:
8:
9:

main
//Initial training loop
for sentence si  S do
for meaning representation mj  M R(si ) do
add (si , mj ) to InitialTrainingSet
end for
end for
SemanticModel = Train(InitialTrainingSet)

10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:

//Iterative retraining
repeat
for sentence si  S do
for meaning representation mj  M R(si ) do
mj .score = Evaluate(si , mj , SemanticModel )
end for
end for
BestExampleSet
 The set of consistent examples T = {(s, m)|s  S, m  MR(s)}
P
such that T m.score is maximized
SemanticModel = T rain(BestExamplesSet)
until Convergence or MAX ITER reached
end main

22:

function Train(TrainingExamples)
Train K RISP on the unambiguous TrainingExamples
25:
return The trained K RISP semantic parser
26: end function
23:

24:

27:

function Evaluate(s, m, SemanticModel )
Use the K RISP semantic parser SemanticModel to find a derivation of meaning representation m from sentence s
30:
return The parsing score
31: end function

28:

29:

401

fiC HEN , K IM , & M OONEY

with their translations in the target MRL. As formal languages, MRLs frequently contain many
purely syntactic tokens such as parentheses or brackets, which are difficult to align with words in
NL. Consequently, we found it was much more effective to align words in the NL with productions
of the MRL grammar used in the parse of the corresponding MR. Therefore, GIZA++ is used
to produce an N to 1 alignment between the words in the NL sentence and a sequence of MRL
productions corresponding to a top-down left-most derivation of the corresponding MR.
Complete MRs are then formed by combining these NL substrings and their translations using a
grammatical framework called synchronous CFG (SCFG) (Aho & Ullman, 1972), which forms the
basis of most existing syntax-based SMT (Yamada & Knight, 2001; Chiang, 2005). In an SCFG, the
right hand side of each production rule contains two strings, in our case one in NL and the other in
MRL. Derivations of the SCFG simultaneously produce NL sentences and their corresponding MRs.
The bilingual lexicon acquired from word alignments over the training data is used to construct a set
of SCFG production rules. A probabilistic parser is then produced by training a maximum-entropy
model using EM to learn parameters for each of these SCFG productions, similar to the methods
used by Riezler, Prescher, Kuhn, and Johnson (2000), and Zettlemoyer and Collins (2005). To
translate a novel NL sentence into its MR, a probabilistic chart parser (Stolcke, 1995) is used to find
the most probable synchronous derivation that generates the given NL, and the corresponding MR
generated by this derivation is returned.
Since SCFGs are symmetric, they can be used to generate NL from MR as well as parse NL into
MR (Wong & Mooney, 2007). This allows the same learned grammar to be used for both parsing
and generation, an elegant property that has important advantages (Shieber, 1988). The generation
system, WASP1 , uses a noisy-channel model (Brown et al., 1990):
arg max Pr(e|f ) = arg max Pr(e) Pr(f |e)
e

(1)

e

Where e refers to the NL string generated for a given input MR, f . Pr(e) is the language model,
and Pr(f |e) is the parsing model provided by WASPs learned SCFG. The generation task is to find
a sentence e such that (1) e is a good sentence a priori, and (2) its meaning is the same as the input
MR. For the language model, we use a standard n-gram model, which is useful in ranking candidate
generated sentences (Knight & Hatzivassiloglou, 1995).

3. Sportscasting Data
To train and test our system, we assembled human-commentated soccer games from the RoboCup
simulation league (www.robocup.org). Since our focus is language learning and not computer vision, we chose to use simulated games instead of real game video to simplify the extraction of
perceptual information. Based on the ROCCO RoboCup commentators incremental event recognition module (Andre et al., 2000) we manually developed symbolic representations of game events
and a rule-based system to automatically extract them from the simulator traces. The extracted
events mainly involve actions with the ball, such as kicking and passing, but also include other
game information such as whether the current playmode is kickoff, offside, or corner kick. The
events are represented as atomic formulas in predicate logic with timestamps. These logical facts
constitute the requisite MRs, and we manually developed a simple CFG for this formal semantic
language. Details of the events detected and the complete grammar can be found in Appendix A.
For the NL portion of the data, we had humans commentate games while watching them on the
simulator. We collected commentaries in both English and Korean. The English commentaries were
402

fiT RAINING A M ULTILINGUAL S PORTSCASTER

Total # of comments
Total # of words
Vocabulary size
Avg. words per comment

English dataset
2036
11742
454
5.77

Korean dataset
1999
7941
344
3.97

Table 1: Word statistics for the English and Korean datasets

Number of events

Total

2001 final
2002 final
2003 final
2004 final

4003
2223
2113
2318

722
514
410
390

2001 final
2002 final
2003 final
2004 final

4003
2223
2113
2318

673
454
412
460

Number of comments
Have MRs Have Correct MR
English dataset
671
520
458
376
397
320
342
323
Korean dataset
650
600
444
419
396
369
423
375

Events per comment
Max Average Std. Dev.
9
10
12
9

2.235
2.403
2.849
2.729

1.641
1.653
2.051
1.697

10
12
10
9

2.138
2.489
2.551
2.601

2.076
3.083
3.672
2.593

Table 2: Alignment statistics for the English and Korean datasets. Some comments do not have correct meaning representations associated with them and are essentially noise in the training
data (18% of the English dataset and 8% of the Korean dataset). Moreover, on average
there are more than 2 possible events linked to each comment so over half of these links
are incorrect.

produced by two different people while the Korean commentaries were produced by a single person.
The commentators typed their comments into a text box, which were recorded with a timestamp.
To construct the final ambiguous training data, we paired each comment with all of the events that
occurred five seconds or less before the comment was made. Examples of the ambiguous training
data are shown in Figure 2. The edges connect sentences to events to which they might refer. English
translations of the Korean commentaries have been included in the figure for the readers benefit and
are not part of the actual data. Note that the use of English words for predicates and constants in the
MRs is for human readability only, the system treats these as arbitrary conceptual tokens and must
learn their connection to English or Korean words.
We annotated a total of four games, namely, the finals for the RoboCup simulation league for
each year from 2001 to 2004. Word statistics about the data are shown in Table 1. While the
sentences are fairly short due to the nature of sportscasts, this data provides challenges in the form
of synonyms (e.g. Pink1, PinkG and pink goalie all refer to the same player) and polysemes
(e.g. kick in kicks toward the goal refers to a kick event whereas kicks to Pink3 refers to a
pass event.) Alignment statistics for the datasets are shown in Table 2. The 2001 final has almost
twice the number of events as the other games because it went into double overtime.
403

fiC HEN , K IM , & M OONEY

Natural Language Commentary

Meaning Representation
badPass ( PurplePlayer1 ,
PinkPlayer8 )
turnover ( PurplePlayer1 ,
PinkPlayer8 )
kick ( PinkPlayer8 )
pass ( PinkPlayer8 , PinkPlayer11 )
kick ( PinkPlayer11 )

Purple goalie turns the ball over to
Pink8
Purple team is very sloppy today
Pink8 passes to Pink11
Pink11 looks around for a teammate

kick ( PinkPlayer11 )
ballstopped
kick ( PinkPlayer11 )
pass ( PinkPlayer11 , PinkPlayer8 )
kick ( PinkPlayer8 )
pass ( PinkPlayer8 , PinkPlayer11 )

Pink11 makes a long pass to Pink8

Pink8 passes back to Pink11

(a) Sample trace of ambiguous English training data

Natural Language Commentary

Meaning Representation

10 11 .
(purple10 passes to purple 11)

kick ( PurplePlayer10 )

11 10  .
(purple11 passes again to purple 10)

kick ( PurplePlayer11 )

10  3  .
(pink3 steals the ball from purple 10)

steal ( PinkPlayer3 )

3   .
(pink3 passes to pink goalie)

kick ( PinkPlayer3 )

pass ( PurplePlayer10 , PurplePlayer11 )

pass ( PurplePlayer11 , PurplePlayer10 )

turnover ( PurplePlayer10 , PinkPlayer3 )

playmode ( free_kick_r )

(b) Sample trace of ambiguous Korean training data

Figure 2: Examples of our training data. Each of the outgoing edges from the comments indicate
a possibly associated meaning representations considered by our system. The bold links
indicate correct matches between the comments and the meaning representations.

404

fiT RAINING A M ULTILINGUAL S PORTSCASTER

For evaluation purposes only, a gold-standard matching was produced by examining each comment manually and selecting the correct MR if it exists. The matching is only approximate because
sometimes the comments contain more information than present in the MRs. For example, a comment might describe the location and the length of a pass while the MR captures only the participants
of a pass. The bold lines in Figure 2 indicate the annotated correct matches in our sample data. Notice some sentences do not have correct matches (about one fifth of the English data and one tenth
of the Korean data). For example, the sentence Purple team is very sloppy today in Figure 2(a)
cannot be represented in our MRL and consequently does not have a corresponding correct MR.
For another example, the Korean sentence with the translation pink3 passes to pink goalie in Figure 2(b) can be represented in our MRL, but does not have a correct match due to the incomplete
event detection. A free kick was called while pink3 was passing to the pink goalie so the pass event
was not retrieved. Finally, in the case of the sentence Pink11 makes a long pass to Pink8 in Figure 2(a), the correct MR falls outside of the 5-second window. For each game, Table 2 shows the
total number of NL sentences, the number of these that have at least one recent extracted event to
which it could refer, and the number of these that actually do refer to one of these recent extracted
events. The maximum, average, and standard deviation for the number of recent events paired with
each comment is also given.

4. Learning Tactical Generation from Ambiguous Supervision
While existing systems are capable of solving parts of the sportscasting problem, none of them are
able to perform the whole task. We need a system that can both deal with ambiguous supervision
like K RISPER and generate language like WASP. We introduce three systems below that can do
both. An overview of the differences between the existing systems and the new systems we present
are shown in Table 3.
All three systems introduced here are based on extensions to WASP, our underlying language
learner. The main problem we need to solve is to disambiguate the training data so that we can
train WASP as before to create a language generator. Each of the new system uses a different
disambiguation criteria to determine the best matching between the NL sentences and the MRs.

4.1 WASPER
The first system is an extension of WASP in a manner similar to how K RISP was extended to create
K RISPER. It uses an EM-like retraining to handle ambiguously annotated data, resulting in a system
we call WASPER. In general, any system that learns semantic parsers can be extended to handle
ambiguous data as long as it can produce confidence levels for given NLMR pairs. Given a set of
sentences s  S and the set of MRs associated with each sentence M R(s), we disambiguate the
data by finding pairs (s, m), s  S and m  M R(s) such that m = arg maxm P r(m|s). Although
probability is used here, a ranking of the relative potential parses would suffice. The pseudocode
for WASPER is shown in Algorithm 2. The only difference compared to the K RISPER pseudocode
is that we now use a WASP semantic parser instead of a K RISP parser. Also, we produce a WASP
language generator as well which is the desired final output for our task.
405

fiC HEN , K IM , & M OONEY

Algorithm

Underlying learner

K RISP
K RISPER
WASP

SVM
K RISP
GIZA to align words,
and MR tokens, then
learn probalistic SCFG
WASP
First disambiguate
with K RISPER,
then train WASP
WASP

WASPER
K RISPER -WASP

WASPER -G EN

Generate?

Disambiguation criteria

No
No
Yes

Ambiguous
data?
No
Yes
No

Yes
Yes

Yes
Yes

WASPs parsing score
K RISPs parsing score

Yes

Yes

NIST score of
best NL given MR

n/a
K RISPs parsing score
n/a

Table 3: Overview of the various learning systems presented. The first three algorithms are existing
systems. We introduce the last three systems that are able to both learn from ambiguous
training data and acquire a language generator. They differ in how they disambiguate the
training data.

Algorithm 2 WASPER
input sentences S and their associated sets of meaning representations M R(s)
output BestExamplesSet, a set NL-MR pairs,
SemanticModel , a WASP semantic parser/language generator
1: main
2:
same as Algorithm 1
3: end main
4:

function Train(TrainingExamples)
Train WASP on the unambiguous TrainingExamples
7:
return The trained WASP semantic parser/language generator
8: end function

5:

6:

9:

function Evaluate(s, m, SemanticModel )
11:
Use the WASP semantic parser in SemanticModel to find a derivation of meaning representation m from sentence s
12:
return The parsing score
13: end function
10:

406

fiT RAINING A M ULTILINGUAL S PORTSCASTER

4.2 KRISPER-WASP
K RISP has been shown to be quite robust at handling noisy training data (Kate & Mooney, 2006).
This is important when training on the very noisy training data used to initialize the parser in
K RISPERs first iteration. However, K RISPER cannot learn a language generator, which is necessary for our sportscasting task. As a result, we create a new system called K RISPER-WASP that
is both good at disambiguating the training data and capable of generation. We first use K RISPER
to train on the ambiguous data and produce a disambiguated training set by using its prediction for
the most likely MR for each sentence. This unambiguous training set is then used to train WASP to
produce both a parser and a generator.
4.3 WASPER-GEN
In both K RISPER and WASPER, the criterion for selecting the best NLMR pairs during retraining is based on maximizing the probability of parsing a sentence into a particular MR. However,
since WASPER is capable of both parsing and generation, we could alternatively select the best
NLMR pairs by evaluating how likely it is to generate the sentence from a particular MR. Thus,
we built another version of WASPER called WASPER-G EN that disambiguates the training data in
order to maximize the performance of generation rather than parsing. The pseudocode is shown in
Algorithm 3. The algorithm is the same as WASPER except for the evaluation function. It uses a
generation-based score rather than a parsing-based score to select the best NLMR pairs.
Specifically, an NLMR pair (s, m) is scored by computing the NIST score, a machine translation (MT) metric, between the sentence s and the best generated sentence for m (lines 9-12).2
Formally, given a set of sentences s  S and the set of MRs associated with each sentence
M R(s), we disambiguate the data by finding pairs (s, m), s  S and m  M R(s) such that m =
arg maxm N IST (s, argmaxs P r(s |m)).
NIST measures the precision of a translation in terms of the proportion of n-grams it shares with
a human translation (Doddington, 2002). It has also been used to evaluate NL generation. Another
popular MT metric is BLEU score (Papineni, Roukos, Ward, & Zhu, 2002), but it is inadequate
for our purpose since we are comparing one short sentence to another instead of comparing whole
documents. BLEU score computes the geometric mean of the n-gram precision for each value of n,
which means the score is 0 if a matching n-gram is not found for every value of n. In the common
setting in which the maximum n is 4, any two sentences that do not have a matching 4-gram would
receive a BLEU score of 0. Consequently, BLEU score is unable to distinguish the quality of most
of our generated sentences since they are fairly short. In contrast, NIST uses an additive score and
avoids this problem.
4.4 Experimental Evaluation
This section presents experimental results on the RoboCup data for four systems: K RISPER, WASPER,
K RISPER-WASP, and WASPER-G EN. Since we are not aware of any existing systems that could
learn how to semantically parse and generate language using ambiguous supervision based on perceptual context, we constructed our own lower and upper baselines using unmodified WASP. Since
2. A natural way to use a generation-based score would be to use the probability of an NL given a MR (P r(s|m)).
However, initial experiments using this metric did not produce very good results. We also tried changing WASP to
maximize the joint probability instead of just the parsing probability. However, this also did not improve the results.

407

fiC HEN , K IM , & M OONEY

Algorithm 3 WASPER -G EN
input sentences S and their associated sets of meaning representations M R(s)
output BestExamplesSet, a set of NL-MR pairs,
SemanticModel , a WASP semantic parser/language generator
1: main
2:
same as Algorithm 1
3: end main
4:

function Train(TrainingExamples)
6:
same as Algorithm 2
7: end function

5:

8:

function Evaluate(s, m, SemanticModel )
GeneratedSentence  Use the WASP language generator in SemanticModel to produce a
sentence from the meaning representation m
11:
return The NIST score between GeneratedSentence and s
12: end function
9:

10:

WASP requires unambiguous training data, we randomly pick a meaning for each sentence from its
set of potential MRs to serve as our lower baseline. We use WASP trained on the gold matching
which consists of the correct NLMR pairs annotated by a human as the upper baseline. This represents an upper-bound on what our systems could achieve if they disambiguated the training data
perfectly.
We evaluate each system on three tasks: matching, parsing, and generation. The matching task
measures how well the systems can disambiguate the training data. The parsing and generation tasks
measure how well the systems can translate from NL to MR, and from MR to NL, respectively.
Since there are four games in total, we trained using all possible combinations of one to three
games. For matching, we measured the performance on the training data since our goal is to disambiguate this data. For parsing and generation, we tested on the games not used for training.
Results were averaged over all train/test combinations. We evaluated matching and parsing using
F-measure, the harmonic mean of recall and precision. Precision is the fraction of the systems
annotations that are correct. Recall is the fraction of the annotations from the gold-standard that
the system correctly produces. Generation is evaluated using BLEU scores which roughly estimates how well the produced sentences match with the target sentences. We treat each game as a
whole document to avoid the problem of using BLEU score for sentence-level comparisons mentioned earlier. Also, we increase the number of reference sentences for each MR by using all of
the sentences in the test data corresponding to equivalent MRs, e.g. if pass(PinkPLayer7,
PinkPlayer8) occurs multiple times in the test data, all of the sentences matched to this MR in
the gold matchings are used as reference sentences for this MR.
4.4.1 M ATCHING NL

AND

MR

Since handling ambiguous training data is an important aspect of grounded language learning, we
first evaluate how well the various systems pick the correct NLMR pairs. Figure 3 shows the Fmeasure for identifying the correct set of pairs for the various systems. All of the learning systems
408

fi0.8

0.8

0.75

0.75

0.7

0.7

0.65

0.65
F-measure

F-measure

T RAINING A M ULTILINGUAL S PORTSCASTER

0.6
0.55

0.6
0.55

0.5

0.5

0.45

0.45

WASPER
WASPER-GEN
KRISPER
random matching

0.4

WASPER
WASPER-GEN
KRISPER
random matching

0.4

0.35

0.35
1

2
Number of Training Games

3

1

(a) English

2
Number of Training Games

3

(b) Korean

0.9

0.9

0.8

0.8

0.7

0.7

F-measure

F-measure

Figure 3: Matching results for our basic systems. WASPER -G EN performs the best, outperforming
the existing system K RISPER on both datasets.

0.6

0.5
WASP with gold matching
KRISPER
KRISPER-WASP
WASPER
WASPER-GEN
WASP with random matching

0.4

0.3

0.6

0.5
WASP with gold matching
KRISPER
KRISPER-WASP
WASPER
WASPER-GEN
WASP with random matching

0.4

0.3

0.2

0.2
1

2
Number of Training Games

3

1

(a) English

2
Number of Training Games

3

(b) Korean

Figure 4: Semantic parsing results for our basic systems. The results largely mirrors that of the
matching results with WASPER -G EN performing the best overall.

perform significantly better than random which has a F-measure below 0.5. In both the English and
Korean data, WASPER -G EN is the best system. WASPER also equals or outperforms the previous
system K RISPER as well.
4.4.2 S EMANTIC PARSING
Next, we present results on the accuracy of the learned semantic parsers. Each trained system is
used to parse and produce an MR for each sentence in the test set that has a correct MR in the
gold-standard matching. A parse is considered correct if and only if it matches the gold standard
exactly. Parsing is a fairly difficult task because there is usually more than one way to describe the
same event. For example, Player1 passes to player2 can refer to the same event as Player1 kicks
the ball to player2. Thus, accurate parsing requires learning all the different ways people describe
409

fi0.5

0.6

0.45

0.55

0.4

0.5

0.35

0.45
BLEU

BLEU

C HEN , K IM , & M OONEY

0.3
0.25

0.4
0.35

WASP with gold matching
KRISPER-WASP
WASPER
WASPER-GEN
WASP with random matching

0.2
0.15

WASP with gold matching
KRISPER-WASP
WASPER
WASPER-GEN
WASP with random matching

0.3
0.25

0.1

0.2
1

2
Number of Training Games

3

1

(a) English

2
Number of Training Games

3

(b) Korean

Figure 5: Tactical generation results for our basic systems. While the relative performances of the
various systems change, WASPER -G EN is still the best system.

an event. Synonymy is not limited to verbs. In our data, Pink1, PinkG and pink goalie all
refer to player1 on the pink team. Since we are not providing the systems with any prior knowledge,
they have to learn all these different ways of referring to the same entity.
The parsing results shown in Figure 4 generally correlate well with the matching results. Systems that did better at disambiguating the training data also did better on parsing because their
supervised training data is less noisy. WASPER-G EN again does the best overall on both the English and Korean data. It is interesting to note that K RISPER did relatively well on the English data
compared to its matching performance. This is because K RISP is more robust to noise than WASP
(Kate & Mooney, 2006) so even though it is trained on a noisier set of data than WASPER -G EN it
still produced a comparable parser.
4.4.3 G ENERATION
The third evaluation task is generation. All of the WASP-based systems are given each MR in the test
set that has a gold-standard matching NL sentence and asked to generate an NL description. The
quality of the generated sentence is measured by comparing it to the gold-standard using BLEU
scoring.
This task is more tolerant to noise in the training data than parsing because the system only
needs to learn one way to accurately describe an event. This property is reflected in the results,
shown in Figure 5, where even the baseline system, WASP with random matching, does fairly well,
outperforming K RISPER-WASP on both datasets and WASPER on the Korean data. As the number
of event types is fairly small, only a relatively small number of correct matchings is required to
perform this task well as long as each event type is associated with some correct sentence pattern
more often than any other sentence pattern.
As with the other two tasks, WASPER -G EN is the best system on this task. One possible explanation of WASPER-G ENs superior performance stems from its disambiguation objective function.
Systems like WASPER and K RISPER-WASP that use parsing scores attempt to learn a good translation model for each sentence pattern. On the other hand, WASPER-G EN only tries to learn a good
410

fiT RAINING A M ULTILINGUAL S PORTSCASTER

translation model for each MR pattern. Thus, WASPER-G EN is more likely to converge on a good
model as there are fewer MR patterns than sentence patterns. However, it can be argued that learning
good translation models for each sentence pattern will help in producing more varied commentaries,
a quality that is not captured by the BLEU score. Another possible advantage for WASPER -G EN is
that it uses a softer scoring function. While the probabilities of parsing from a particular sentence
to a MR can be sensitive to noise in the training data, WASPER -G EN only looks at the top generated
sentences for each MR. Even with noise in the data, this top generated sentence remains relatively
constant. Moreover, minor variations of this sentence do not change the results dramatically since
the NIST score allows for partial matching.

5. Learning for Strategic Generation
A language generator alone is not enough to produce a sportscast. In addition to tactical generation
which is deciding how to to say something, a sportscaster must also preform strategic generation
which is choosing what to say (McKeown, 1985).
We developed a novel method for learning which events to describe. For each event type (i.e.
for each predicate like pass, or goal), the system uses the training data to estimate the probability
that it is mentioned by the sportscaster. Given the gold-standard NLMR matches, this probability
is easy to estimate; however, the learner does not know the correct matching. Instead, the system
must estimate the probabilities from the ambiguous training data. We compare two basic methods
for estimating these probabilities.
The first method uses the inferred NLMR matching produced by the language-learning system.
The probability of commenting on each event type, ei , is estimated as the percentage of events of
type ei that have been matched to some NL sentence.
The second method, which we call Iterative Generation Strategy Learning (IGSL), uses a variant of EM, treating the matching assignments as hidden variables, initializing each match with a
prior probability, and iterating to improve the probability estimates of commenting on each event
type. Unlike the first method, IGSL uses information about MRs not explicitly associated with any
sentence in training. Algorithm 4 shows the pseudocode. The main loop alternates between two
steps:
1. Calculating the expected probability of each NLMR matching given the current model of
how likely an event is commented on (line 6)
2. Update the prior probability that an event type is mentioned by a human commentator based
on the matchings (line 9).
In the first iteration, each NLMR match is assigned
a probability inversely proportional to the
P
amount of ambiguity associated with the sentence ( eEvent(s) Pr (e) = |Event(s)|). For example,
a sentence associated with five possible MRs will assign each match a probability of 51 . The prior
probability of mentioning an event type is then estimated as the average probability assigned to
instances of this event type. Notice this process does not always guarantee a proper probability since
a MR can be associated with multiple sentences. Thus, we limit the probability to be at most one. In
the subsequent iterations, the probabilities of the NLMR matchings are updated according to these
new priors. We assign each match the prior probability of its event type normalized across all the
associated MRs of the NL sentence. We then update the priors for each event type as before using
411

fiC HEN , K IM , & M OONEY

Algorithm 4 Iterative Generation Strategy Learning
input event types E = {e1 , ..., en }, the number of occurrences of each event type T otalCount(ei )
in the entire game trace, sentences S and the event types of their associated meaning representations Event(s)
output probabilities of commenting on each event type P r(ei )
1: Initialize all Pr (ei ) = 1
2: repeat
3:
for event type ei  E do
4:
MatchCount = 0
5:
for sentence s  S do
P
Pr (e)
i
P
6:
ProbOfMatch = eEvent(s)e=e
Pr (e)
eEvent(s)

7:
8:
9:
10:
11:

MatchCount = MatchCount + ProbOfMatch
end for
MatchCount
Pr (ei ) = min( TotalCount(e
,1) {Ensure proper probabilities}
i)
end for
until Convergence or MAX ITER reached

!"#$%&&

'()*+*,-,%.&)/&*#,$0& 4)(2+-,5#3&6()*+*,-,%.&
1)22#$%#3&)$&

*+--7%)66#3&

89:;&<&8:=>?&

?9@:&<&8:=>A&

*+3'+77B6C(6-#DE6,$F?G&

:9;D:&

:9?8A&

%C($)"#(B6C(6-#DE6,$F?G&

:9;:;&

:9H@H&

%C($)"#(B6C(6-#DE6,$F?G&

:9;:;&
:9:;8&

IJ#&7#-#1%#3&#"#$%&,7&"#(*+-,5#3&&
K$&#"#$%&,7&7#-#1%#3&*+7#3&&
)$&%J#&$)(2+-,5#3&6()*+*,-,%.& (+$3)2-.&+11)(3,$0&%)&,%7&6()*+*,-,%.&&
)/&*#,$0&1)22#$%#3&)$&

Figure 6: An example of how our strategic generation component works. At every timestep, we
stochastically select an event from all the events occurring at that moment. We then
decide whether to verbalize the selected event based on IGSLs estimated probability of
it being commented upon.

the new estimated probabilities for the matchings. This process is repeated until the probabilities
converge or a pre-specified number of iterations has occurred.
To generate a sportscast, we use the learned probabilities to determine which events to describe.
For each time step, we first determine all the events that are occurring at the time. We then select one
randomly based on their normalized probabilities. To avoid being overly verbose, we do not want to
make a comment every time something is happening, especially if its an event rarely commented on.
Thus, we stochastically decide whether to comment on this selected event based on its probability.
An example of this process is shown in Figure 6.
412

fi0.8

0.8

0.7

0.7

0.6

0.6

F-measure

F-measure

T RAINING A M ULTILINGUAL S PORTSCASTER

0.5
inferred from gold matching
IGSL
inferred from KRISPER
inferred from WASPER
inferred from WASPER-GEN
inferred from random matching

0.4

0.5
inferred from gold matching
IGSL
inferred from KRISPER
inferred from WASPER
inferred from WASPER-GEN
inferred from random matching

0.4

0.3

0.3
1

2
Number of Training Games

3

1

(a) English

2
Number of Training Games

3

(b) Korean

Figure 7: Strategic generation results for our various systems. Our novel algorithm IGSL performs
the best, almost on par with the upper bound which uses gold-annotated matchings.

event
ballstopped
kick
pass
turnover
badPass

# occurrences
5817
2122
1069
566
371

% commented
1.72  104
0.0 33
0.999
0.214
0.429

IGSL
1.09  105
0.018
0.983
0.909
0.970

inferred from WASPER -G EN
0.016
0.117
0.800
0.353
0.493

Table 4: Top 5 most frequent events, the % of times they were commented on, and the probabilities
learned by the top algorithms for the English data

5.1 Experimental Evaluation
The different methods for learning strategic generation were evaluated based on how often the events
they describe in the test data coincide with those the human decided to describe. For the first
approach, results using the inferred matchings produced by K RISPER, WASPER, and WASPER-G EN
as well as the gold and random matching for establishing baselines are all presented in Figure 7.
From the graph, it is clear that IGSL outperforms learning from the inferred matchings and actually
performs at a level close to using the gold matching. However, it is important to note that we are
limiting the potential of learning from the gold matching by using only the predicates to decide
whether to talk about an event.
For the English data, the probabilities learned by IGSL and by inferred matchings from WASPER G EN for the five most frequently occurring events are shown in Table 4. While WASPER -G EN learns
fairly good probabilities in general, it does not do as well as IGSL for the most frequent events. This
is because IGSL uses occurrences of events that are not associated with any possible comments in
its training iterations. Rarely commented events such as ballstopped and kick often occur without
any comments being uttered. Consequently, IGSL assigns low prior probabilities to them which
lowers their chances of being matched to any sentences. On the other hand, WASPER -G EN does
not use these priors and sometimes incorrectly matches comments to them. Thus, using the inferred
413

fiC HEN , K IM , & M OONEY

matches from WASPER -G EN results in learning higher probabilities of commenting on these rarely
commented events.
While all our methods only use the predicates of the MRs to decide whether to comment or not,
they perform quite well on the data we collected. In particular, IGSL performs the best, so we use
it for strategic generation in the rest of the paper.

6. Using Strategic Generation to Improve Matching
In this section, we explore how knowledge learned about strategic generation can be used to improve
the accuracy of matching sentences to MRs. In the previous section, we described several ways to
learn strategic generation, including IGSL which learns directly from the ambiguous training data.
Knowing what events people tend to talk about should also help resolve ambiguities in the training
data. Events that are more likely to be discussed should also be more likely to be matched to an
NL sentence when disambiguating the training data. Therefore, this section describes methods that
integrate strategic generation scores (such as those in Table 4) into the scoring of NLMR pairs used
in the matching process.
6.1 WASPER-GEN-IGSL
WASPER -G EN -IGSL is an extension of WASPER -G EN that also uses strategic generation scores
from IGSL. WASPER -G EN uses NIST score to pick the best MR for a sentence by finding the MR
that generates the sentence closet to the actual NL sentence. WASPER -G EN -IGSL combines tactical
(NIST) and strategic (IGSL) generation scores to pick the best NLMR pairs. It simply multiplies
the NIST score and the IGSL score together to form a composite score. This new score biases
the selection of matching pairs to include events that IGSL determines are, a priori, more likely
to be discussed. This is very helpful, especially in the beginning when WASP does not produce
a particularly good language generator. In many instances, the generated sentences for all of the
possible MRs are equally bad and do not overlap with the target sentence. Even if generation
produces a perfectly good sentence, the generation score can be unreliable because it is comparing
a single sentence to a single reference that is often very short as well. Consequently, it is often
difficult for WASPER-G EN to distinguish among several MRs with equal scores. On the other hand,
if their event types have very different strategic generation scores, then we can default to choosing
the MR with the higher prior probability of being mentioned. Algorithm 5 shows the pseudocode
of WASPER -G EN -IGSL.
6.2 Variant of WASPER-GEN Systems
Although WASPER -G EN uses NIST score to estimate the goodness of NLMR pairs, it could easily
use any MT evaluation metric. We have already discussed the unsuitability of BLEU for comparing short individual sentences since it assigns zero to many pairs. However, NIST score also has
limitations. For example, it is not normalized, which may affect the performance of WASPER -G EN IGSL when it is combined with the IGSL score. Another limitation comes from using higher-order
N-grams. Commentaries in our domain are often short, so there are frequently no higher-order
N-gram matches between generated sentences and target NL sentences.
The METEOR metric (Banerjee & Lavie, 2005) was designed to resolve various weaknesses of
the BLEU and NIST metrics, and it is more focused on word-to-word matches between the reference
414

fiT RAINING A M ULTILINGUAL S PORTSCASTER

Algorithm 5 WASPER -G EN -IGSL
input sentences S and their associated sets of meaning representations M R(s)
output BestExamplesSet, a set of NL-MR pairs,
SemanticModel , a WASP semantic parser/language generator
1: main
2:
same as Algorithm 1
3: end main
4:

function Train(TrainingExamples)
6:
same as Algorithm 2
7: end function

5:

8:
9:
10:
11:
12:
13:
14:
15:

function Evaluate(s, m, SemanticModel )
Call Algorithm 4 to collect IGSL scores
GeneratedSentence  Use the WASP language generator in SemanticModel to produce a
sentence from the meaning representation m
TacticalGenerationScore  The NIST score between GeneratedSentence and s
StrategicGenerationScore  Pr (event type of m) from the result of Algorithm 4
return TacticalGenerationScore  StrategicGenerationScore
end function

sentence and the test sentence. METEOR first evaluates uni-gram matches between the reference
and the test sentence and also determines how well the words are ordered. METEOR seems more
appropriate for our domain because some good generated sentences have missing adjectives or adverbs that are not critical to the meaning of the sentence but prevent higher-order N-gram matches.
In addition, METEOR is normalized to always be between 0 and 1, so it may combine more effectively with IGSL scores (which are also in the range 01).
6.3 Experimental Evaluation
We evaluated the new systems, WASPER-G EN-I GSL with both NIST and METEOR scoring using
the methodology from Section 4.4. The matching results are shown in Figure 8, including results
for WASPER -G EN, the best system from the previous section. Both WASPER-G EN-IGSL with
either NIST or METEOR scoring clearly outperforms WASPER-G EN. This indicates that strategicgeneration information can help disambiguate the data. Using different MT metrics produces a
less noticeable effect. There is no clear winner on the English data; however, METEOR seems to
improve performance on the Korean data.
Parsing results are shown in Figure 9. As previously noted, parsing results generally mirror the
matching results. Both new systems again outperform WASPER -G EN, the previously best system.
And again, the English data does not show a clear advantage of using either NIST or METEOR,
while the Korean data gives a slight edge to using the METEOR metric.
Results for tactical generation are shown in Figure 10. For both the English and the Korean
data, the new systems come close to the performance of WASPER-G EN but do not beat it. However,
the new systems do outperform K RISPER -WASP and WASPER which are not shown in the figure.
415

fi0.9

0.9

0.85

0.85

0.8

0.8

0.75

0.75
F-measure

F-measure

C HEN , K IM , & M OONEY

0.7
0.65
0.6

0.7
0.65
0.6

0.55

0.55
WASPER-GEN
WASPER-GEN-IGSL
WASPER-GEN-IGSL using METEOR

0.5

WASPER-GEN
WASPER-GEN-IGSL
WASPER-GEN-IGSL using METEOR

0.5

0.45

0.45
1

2
Number of Training Games

3

1

(a) English

2
Number of Training Games

3

(b) Korean

0.9

0.9

0.85

0.85

0.8

0.8

0.75

0.75
F-measure

F-measure

Figure 8: Matching results. Integrating strategic information improves the results over the previously best system WASPER -G EN. The choice of the MT metric used, however, makes
less of an impact.

0.7
0.65

0.7
0.65

0.6

0.6
WASP with gold matching
WASPER-GEN
WASPER-GEN-IGSL
WASPER-GEN-IGSL using METEOR

0.55

WASP with gold matching
WASPER-GEN
WASPER-GEN-IGSL
WASPER-GEN-IGSL using METEOR

0.55

0.5

0.5
1

2
Number of Training Games

3

1

(a) English

2
Number of Training Games

3

(b) Korean

Figure 9: Semantic parsing results. The results are similar to the matching results in that integrating
strategic generation information improves the performance.

416

fi0.5

0.6

0.45

0.55

0.4

0.5

0.35

0.45
BLEU

BLEU

T RAINING A M ULTILINGUAL S PORTSCASTER

0.3
0.25

0.4
0.35

0.2

0.3
WASP with gold matching
WASPER-GEN
WASPER-GEN-IGSL
WASPER-GEN-IGSL using METEOR

0.15

WASP with gold matching
WASPER-GEN
WASPER-GEN-IGSL
WASPER-GEN-IGSL using METEOR

0.25

0.1

0.2
1

2
Number of Training Games

3

1

(a) English

2
Number of Training Games

3

(b) Korean

Figure 10: Tactical generation results. While the two new systems come close to the performance
of WASPER -G EN, they do not beat it. However, they do outperform other systems
presented earlier which are not shown in this figure.

Overall, as expected, using strategic information improves performance on the matching and
semantic parsing tasks. For both the English and the Korean datasets, WASPER-G EN-IGSL and
its variant using the METEOR metric clearly outperform WASPER-G EN which does not utilize
strategic information. However, strategic information does not improve tactical generation. This
could be due to the ceiling effect in which WASPER -G EN already performs at a level near the upper
baseline. While the matching performance has improved, the generation performance has little room
to grow.

7. Using a Generative Alignment Model
Recently, Liang et al. (2009) developed a generative model that can be used to match naturallanguage sentences to facts in a corresponding database to which they may refer. As one of their
evaluation domains, they used our English RoboCup sportscasting data. Their method solves the
matching (alignment) problem for our data, but does not address the tasks of semantic parsing
or language generation. However, their generative model elegantly integrates simple strategic and
tactical language generation models in order to find the overall most probable alignment of sentences
and events. They demonstrated improved matching performance on our English data, generating
more accurate NLMR pairs than our best system. Thus, we were curious if their results could be
used to improve our own systems, which also perform semantic parsing and generation. We also
ran their code on our new Korean data but that resulted in much worse matching results compared
to our best system as can be seen in Table 5.
The simplest way of utilizing their results is to use the NLMR pairs produced by their method
as supervised data for WASP. As expected, the improved NLMR pairs for the English data resulted
in improved semantic parsers as can be seen in the results in Table 6. Even for the Korean dataset,
training on matchings produced by their system ended up doing fairly well even though the matching performance was poor. For tactical generation, using their matching only produced marginal
improvement on the English dataset and a surprisingly large improvement on the Korean data as
417

fiC HEN , K IM , & M OONEY

Algorithm
Liang et al. (2009)
WASPER
WASPER-G EN
WASPER-G EN-I GSL
WASPER-G EN-I GSL -M ETEOR

English dataset
No initialization Initialized
75.7
59.7
79.3
68.1
75.8
73.5
73.9
73.1
75.1

Korean dataset
No initialization Initialized
69.4
72.8
76.6
75.3
80.0
81.9
81.6
83.8
84.1

Table 5: Matching results (F1 scores) on 4-fold cross-validation for both the English and the Korean
datasets. Systems run with initialization are initialized with the matchings produced by
Liang et al.s (2009) system.

Algorithm
WASP
WASPER
WASPER-G EN
WASPER-G EN-I GSL
WASPER-G EN-I GSL -M ETEOR

English dataset
No initialization Initialized
n/a
80.3
61.84
79.32
70.15
77.59
73.19
73.04
72.75
74.62

Korean dataset
No initialization Initialized
n/a
74.01
69.12
75.69
72.02
77.49
78.75
75.27
80.65
81.21

Table 6: Semantic parsing results (F1 scores) on 4-fold cross-validation for both the English and
the Korean datasets. Systems run with initialization are initialized with the matchings
produced by Liang et al.s (2009) system.

Algorithm
WASP
WASPER
WASPER-G EN
WASPER-G EN-I GSL
WASPER-G EN-I GSL -M ETEOR

English dataset
No initialization Initialized
n/a
0.4580
0.3471
0.4599
0.4560
0.4414
0.4223
0.4585
0.4062
0.4353

Korean dataset
No initialization Initialized
n/a
0.5828
0.4524
0.6118
0.5575
0.6796
0.5371
0.6710
0.5180
0.6591

Table 7: Tactical generation results (BLEU score) on 4-fold cross-validation for both the English
and the Korean datasets. Systems run with initialization are initialized with the matchings
produced by Liang et al.s (2009) system.

shown in Table 7. Overall, using the alignments produced by Liang et al.s system resulted in good
semantic parsers and tactical generators.
In addition to training WASP with their alignment, we can also utilize their output as a better
starting point for our own systems. Instead of initializing our iterative alignment methods with a
model trained on all of the ambiguous NLMR pairs, they can be initialized with the disambiguated
NLMR pairs produced by Liang et al.s system.
418

fiT RAINING A M ULTILINGUAL S PORTSCASTER

Initializing the systems in this manner almost always improved the performance on all three
tasks (Tables 5, 6, and 7). Moreover, the results from the best systems exceed that of simply training WASP with the alignment in all cases except for semantic parsing on the English data. Thus,
combining Liang et al.s alignment with our disambiguation techniques seems to produce the best
overall results. For the English data, WASPER with initialization performs the best on both matching and generation. It does slightly worse on the semantic parsing task compared to WASP trained
on Liang et al.s alignment. For the Korean data, all the systems do better than just training WASP
on the alignment. WASPER -G EN -I GSL -M ETEOR with initialization performs the best on matching
and semantic parsing while WASPER -G EN with initialization performs the best on generation.
Overall, initializing our systems with the alignment output of Liang et al.s generative model
improved performance as expected. Starting with a cleaner set of data led to better initial semantic
parsers and language generators which led to better end results. Furthermore, by incorporating a
semantic parser and a tactical generator, we were able to improve on the Liang et al.s alignments
and achieve even better results in most cases.

8. Removing Superfluous Comments
So far, we have only discussed how to handle ambiguity in which there are multiple possible MRs
for each NL sentence. During training, all our methods assume that each NL sentence matches
exactly one of the potential MRs. However, some comments are superfluous, in the sense that they
do not refer to any currently extracted event represented in the set of potential MRs. As previously
shown in Tables 2, about one fifth of the English sentences and one tenth of the Korean sentences
are superfluous in this sense.
There are many reasons for superfluous sentences. They occur naturally in language because
people do not always talk about the current environment. In our domain, sportscasters often mention
past events or more general information about particular teams or players. Moreover, depending on
the application, the chosen MRL may not represent all of the things people talk about. For example,
our RoboCup MRL cannot represent information about players who are not actively engaged with
the ball. Finally, even if a sentence can be represented in the chosen MRL, errors in the perceptual
system or an incorrect estimation of when an event occurred can also lead to superfluous sentences.
Such perceptual errors can be alleviated to some degree by increasing the size of the window used
to capture potential MRs (the previous 5 seconds in our experiments). However, this comes at the
cost of increased ambiguity because it associates more MRs with each sentence.
To deal with the problem of superfluous sentences, we can eliminate the lowest-scoring NLMR
pairs (e.g. lowest parsing scores for WASPER or lowest NIST scores for WASPER-G EN). However,
in order to set the pruning threshold, we need to automatically estimate the amount of superfluous
commentary in the absence of supervised data. Notice that while this problem looks similar to the
strategic generation problem (estimating how likely an MR participates in a correct matching as
opposed to how likely an NL sentence participates in a correct matching), the approaches used there
cannot be applied. First, we cannot use the matches inferred by the existing systems to estimate the
fraction of superfluous comments since the current systems match every sentence to some MR. It is
also difficult to develop an algorithm similar to IGSL due to the imbalance between NL sentences
and MRs. Since there are many more MRs, there are more examples of events occurring without
commentaries than vice versa.
419

fiC HEN , K IM , & M OONEY

8.1 Estimating the Superfluous Rate Using Internal Cross Validation
We propose using a form of internal (i.e. within the training set) cross validation to estimate the rate
of superfluous comments. While this algorithm can be used in conjunction with any of our systems,
we chose to implement it for K RISPER which trains much faster than our other systems. This makes
it more tractable to train many different semantic parsers and choose the best one. The basic idea is
to use part of the ambiguous training data to estimate the accuracy of a semantic parser even though
we do not know the correct matchings. Assuming a reasonable superfluous sentence rate, we know
that most of the time the correct MR is contained in the set of MRs associated with an NL sentence.
Thus, we assume that a semantic parser that parses an NL sentence into one of the MRs associated
with it is better than one that parses it into an MR not in the set. With this approach to estimating
accuracy, we can evaluate semantic parsers learned using various pruning thresholds and pick the
best one. The algorithm is briefly summarized in the following steps:
1. Split the training set into an internal training set and an internal validation set.
2. Train K RISPER N times on the internal training set using N different threshold values (eliminating the lowest scoring NLMR pairs below the threshold in each retraining iteration in
Algorithm 1).
3. Test the N semantic parsers on the internal validation set and determine which parser is able
to parse the largest number of sentences into one of their potential MRs.
4. Use the threshold value that produced the best parser in the previous step to train a final parser
on the complete original training set.
8.2 Experiments
We evaluated the effect of removing superfluous sentences on all three tasks: matching, parsing,
and generation. We present results for both K RISPER and K RISPER -WASP. For matching, we
only show results for K RISPER because it is responsible for disambiguating the training data for
both systems (so K RISPER -WASPs results are the same). For generation, we only show results for
K RISPER-WASP, since K RISPER cannot perform generation.
The matching results shown in Figure 11 demonstrate that removing superfluous sentences does
improve the performance for both English and Korean, although the difference is small in absolute
terms. The parsing results shown in Figure 12 indicate that removing superfluous sentences usually
improves the accuracy of both K RISPER and K RISPER -WASP marginally. As we have observed
many times, the parsing results are consistent with the matching results. Finally, the tactical generation results shown in Figure 13 suggest that removing superfluous comments actually decreases
performance somewhat. Once again, a potential explanation is that generation is less sensitive to
noisy training data. While removing superfluous comments improves the purity of the training data,
it also removes potentially useful examples. Consequently, the system does not learn how to generate sentences that were removed from the data. Overall, for generation, the advantage of having
cleaner disambiguated training data is apparently outweighed by the loss of data.
420

fi0.8

0.8

0.75

0.75

0.7

0.7

0.65

0.65
F-measure

F-measure

T RAINING A M ULTILINGUAL S PORTSCASTER

0.6
0.55

0.6
0.55

0.5

0.5

0.45

0.45

0.4

0.4

KRISPER
KRISPER with superfluous comment removal

0.35

KRISPER
KRISPER with superfluous comment removal

0.35
1

2
Number of Training Games

3

1

(a) English

2
Number of Training Games

3

(b) Korean

0.8

0.8

0.75

0.75

0.7

0.7

0.65

0.65
F-measure

F-measure

Figure 11: Matching results comparing the effects of removing superfluous comments.

0.6
0.55

0.6
0.55

0.5

0.5
KRISPER
KRISPER with superfluous comment removal
KRISPER-WASP
KRISPER-WASP with superfluous comment removal

0.45

KRISPER
KRISPER with superfluous comment removal
KRISPER-WASP
KRISPER-WASP with superfluous comment removal

0.45

0.4

0.4
1

2
Number of Training Games

3

1

(a) English

2
Number of Training Games

3

(b) Korean

0.5

0.6

0.45

0.55

0.4

0.5

0.35

0.45
BLEU

BLEU

Figure 12: Semantic parsing results are improved marginally after superfluous comment removal.

0.3

0.4

0.25

0.35

0.2

0.3

0.15

0.25

KRISPER-WASP
KRISPER-WASP with superfluous comment removal

0.1

KRISPER-WASP
KRISPER-WASP with superfluous comment removal

0.2
1

2
Number of Training Games

3

1

(a) English

2
Number of Training Games

3

(b) Korean

Figure 13: Tactical generation performance decreases after removing superfluous comments.

421

fiC HEN , K IM , & M OONEY

9. Human Subjective Evaluation
At best, automatic evaluation of generation is an imperfect approximation of human assessment.
Moreover, automatically evaluating the quality of an entire generated sportscast is even more difficult. Consequently, we used Amazons Mechanical Turk to collect human judgements of the
produced sportscasts. Each human judge was shown three clips of simulated game video in one sitting. There were 8 video clips total. The 8 clips use 4 game segments of 4 minutes each, one from
each of the four games (2001-2004 RoboCup finals). Each of the 4 game segments is commentated
once by a human and once by our system. We use IGSL to determine the events to comment on and
WASPER -G EN (our best performing system for tactical generation) to produce the commentaries.
To make the commentaries more varied, we took the top 5 outputs from WASPER -G EN and chose
one stochastically weighted by their scores. The system was always trained on three games, leaving out the game from which the test segment was extracted. The video clips were accompanied
by commentaries that both appear as subtitles on the screen as well as audio produced by a automated text to speech system 3 The videos are shown in random counter-balanced order to ensure
no consistent bias toward segments being shown earlier or later. We asked the judges to score the
commentaries using the following metrics:

Score
5
4
3
2
1

Fluency
Flawless
Good
Non-native
Disfluent
Gibberish

Semantic
Correctness
Always
Usually
Sometimes
Rarely
Never

Sportscasting
Ability
Excellent
Good
Average
Bad
Terrible

Fluency and semantic correctness, or adequacy, are standard metrics in human evaluations of NL
translations and generations. Fluency measures how well the commentaries are structured, including
syntax and grammar. Semantic correctness indicates whether the commentaries accurately describe
what is happening in the game. Finally, sportscasting ability measures the overall quality of the
sportscast. This includes whether the sportscasts are interesting and flow well. In addition to these
metrics, we also asked them whether they thought the sportscast was composed by a human or a
computer (Human?).
Since Mechanical Turk recruits judges over the Internet, we had to make sure that the judges
were not assigning ratings randomly. Thus, in addition to asking them to rate each video, we also
asked them to count the number of goals in each video. Incorrect responses to this question caused
their ratings to be discarded. This is to ensure that the judges faithfully watched the entire clip before
assigning ratings. After such pruning, there was on average 36 ratings (from 40 original ratings)
for each of the 8 videos for the English data. Since it was more difficult to recruit Korean judges
over the Internet, we recruited them in person and collected 7 ratings on average for each video
in the Korean data. Table 8 and 9 show the results for the English and Korean data, respectively.
Statistically significant results are shown in boldface.
Results are surprisingly good for the English data across all categories with the machine actually
scoring higher than the human on average. However, the differences are not statistically significant
3. Sample video clips with sound are available on the web at http://www.cs.utexas.edu/users/ml/
clamp/sportscasting/.

422

fiT RAINING A M ULTILINGUAL S PORTSCASTER

2001 final
2002 final
2003 final
2004 final
Average

Commentator
Human
Machine
Human
Machine
Human
Machine
Human
Machine
Human
Machine

Fluency
3.74
3.89
4.13
3.97
3.54
3.89
4.03
4.13
3.86
3.94

Semantic
Correctness
3.59
3.81
4.58
3.74
3.73
4.26
4.17
4.38
4.03
4.03

Sportscasting
Ability
3.15
3.61
4.03
3.29
2.61
3.37
3.54
4.00
3.34
3.48

Human?
20.59%
40.00%
42.11%
11.76%
13.51%
19.30%
20.00%
56.25%
24.31%
26.76%

Table 8: Human evaluation of overall sportscasts for English data. Bold numbers indicate statistical
significance.

2001 final
2002 final
2003 final
2004 final
Average

Commentator
Human
Machine
Human
Machine
Human
Machine
Human
Machine
Human
Machine

Fluency
3.75
3.50
4.17
3.25
3.86
2.38
3.00
2.71
3.66
2.93

Semantic
Correctness
4.13
3.67
4.33
3.38
4.29
3.25
3.75
3.43
4.10
3.41

Sportscasting
Ability
4.00
2.83
3.83
3.13
4.00
2.88
3.25
3.00
3.76
2.97

Human?
50.00%
33.33%
83.33%
50.00%
85.71%
25.00%
37.50%
14.29%
62.07%
31.03%

Table 9: Human evaluation of overall sportscasts for Korean data. Bold numbers indicate statistical
significance.

423

fiC HEN , K IM , & M OONEY

based on an unpaired t-test (p > 0.05). Nevertheless, it is encouraging to see the machine being
rated so highly. There is some variance in the humans performance since there were two different
commentators. Most notably, compared to the machine, the humans performance on the 2002 final
is quite good because his commentary included many details such as the position of the players,
the types of passes, and comments about the overall flow of the game. On the other hand, the
humans performance on the 2003 final is quite bad because the human commentator was very
mechanical and used the same sentence pattern repeatedly. The machine performance was more
even throughout although sometimes it gets lucky. For example, the machine serendipitously said
This is the beginning of an exciting match. near the start of the 2004 final clip simply because this
statement was incorrectly learned to correspond to an extracted MR that is actually unrelated.
The results for Korean are not as impressive. The human beats the machine on average for
all categories. However, the largest difference between the scores in any category is only 0.8.
Moreover, the absolute scores indicate that the generated Korean sportscast is at least of acceptable
quality. The judges even mistakenly thought they were produced by humans one third of the time.
Part of the reason for the worse performance compared to the English data is that the Korean commentaries were fairly detailed and included events that were not extracted by our limited perceptual
system. Thus, the machine simply had no way of competing because it is limited to only expressing
information that is present in the extracted MRs.
We also elicited comments from the human judges to get a more qualitative evaluation. Overall,
the judges thought the generated commentaries were good and accurately described the actions on
the field. Picking from the top 5 generated sentences also added variability to the machine-generated
sportscasts that improved the results compared with earlier experiments presented by Chen and
Mooney (2008). However, the machine still sometimes misses significant plays such as scoring
or corner kicks. This is because these plays happen much less frequently and often coincide with
many other events (e.g. shooting the ball and kickoffs co-occur with scoring). Thus, the machine
has a harder time learning about these infrequent events. Another issue concerns our representation.
Many people complain about long gaps in the sportscasts or lack of details. Our event detector only
concentrates on ball possession and not on positions or elapsed time. Thus, a player holding onto a
ball or dribbling for a long time does not produce any events detected by our simulated perceptual
system. Also, a short pass in the backfield is treated exactly the same as a long pass across the
field to near the goal. Finally, people desired more colorful commentary (background information,
statistics, or analysis of the game) to fill in the voids. This is a somewhat orthogonal issue since our
goal was to build a play-by-play commentator that described events that were currently happening.

10. Related Work
In this section we review some of the related work in semantic parsing, natural language generation
as well as grounded language learning.
10.1 Semantic Parsing
As mentioned in Section 2, existing work on semantic parser learners has focused on supervised
learning where each sentence is annotated with its semantic meaning. Some semantic-parser learners additionally require either syntactic annotations (Ge & Mooney, 2005) or prior syntactic knowledge of the target language (Ge & Mooney, 2009; Zettlemoyer & Collins, 2005, 2007). Since the
world never provides any direct feedback on syntactic structure, language-learning methods that
424

fiT RAINING A M ULTILINGUAL S PORTSCASTER

require syntactic annotation are not directly applicable to grounded language learning. Therefore,
methods that learn from only semantic annotation are critical to learning language from perceptual
context.
While we use logic formulas as our MRs, the particular MRL we use contains only atomic
formulas and can be equivalently represented as frames and slots. There are systems that use
transformation-based learning (Jurcicek et al., 2009), or Markov logic (Meza-Ruiz, Riedel, &
Lemon, 2008) to learn semantic parsers using frames and slots. In principle, our framework can
be used with any semantic parser learner as long as it provides confidence scores for its parse results.
10.2 Natural Language Generation
There are several existing systems that sportscast RoboCup games (Andre et al., 2000). Given
game states provided by the RoboCup simulator, they extract game events and generate real-time
commentaries. They consider many practical issues such as timeliness, coherence, variability, and
emotion that are needed to produce good sportscasts. However, these systems are hand-built and
generate language using pre-determined templates and rules. In contrast, we concentrate on the
learning problem and induce the generation components from ambiguous training data. Nevertheless, augmenting our system with some of the other components in these systems could improve the
final sportscasts produced.
There is also prior work on learning a lexicon of elementary semantic expressions and their corresponding natural language realizations (Barzilay & Lee, 2002). This work uses multiple-sequence
alignment on datasets that supply several verbalizations of the corresponding semantics to extract a
dictionary.
Duboue and McKeown (2003) were the first to propose an algorithm for learning strategic generation automatically from data. Using semantics and associated texts, their system learns a classifier
that determines whether a particular piece of information should be included for presentation or not.
There has been some recent work on learning strategic generation using reinforcement learning
(Zaragoza & Li, 2005). This work involves a game setting where the speaker must aid the listener
in reaching a given destination while avoiding obstacles. The game is played repeatedly to find
an optimal strategy that conveys the most pertinent information while minimizing the number of
messages. We consider a different problem setting where such reinforcements are not available to
our strategic generation learner.
In addition, there has also been work on performing strategic generation as a collective task
(Barzilay & Lapata, 2005). By considering all strategic generation decisions jointly, it captures
dependencies between utterances. This creates more consistent overall output and is more consistent
with how humans perform this task. Such an approach could potentially help our system produce
better overall sportscasts.
10.3 Grounded Language Learning
One of the most ambitious end-to-end visually-grounded scene-description system is VITRA (Herzog & Wazinski, 1994) which comments on traffic scenes and soccer matches. The system first
transforms raw visual data into geometrical representations. Next, a set of rules extract spatial relations and interesting motion events from those representations. Presumed intentions, plans, and plan
425

fiC HEN , K IM , & M OONEY

interactions between the agents are also extracted based on domain-specific knowledge. However,
since their system is hand-coded it cannot be adapted easily to new domains.
Srihari and Burhans (1994) used captions accompanying photos to help identify people and
objects. They introduced the idea of visual semantics, a theory of extracting visual information and
constraints from accompanying text. For example, by using caption information, the system can
determine the spatial relationship between the entities mentioned, the likely size and shape of the
object of interest, and whether the entity is natural or artificial. However, their system is also based
on hand-coded knowledge.
Siskind (1996) performed some of the earliest work on learning grounded word meanings. His
learning algorithm addresses the problem of ambiguous training or referential uncertainty for
semantic lexical acquisition, but does not address the larger problems of learning complete semantic
parsers and language generators.
Several robotics and computer vision researchers have worked on inferring grounded meanings
of individual words or short referring expressions from visual perceptual context (e.g., Roy, 2002;
Bailey et al., 1997; Barnard et al., 2003; Yu & Ballard, 2004). However, the complexity of the
natural language used in this existing work is very restrictive, many of the systems use pre-coded
knowledge of the language, and almost all use static images to learn language describing objects and
their relations, and cannot learn language describing actions. The most sophisticated grammatical
formalism used to learn syntax in this work is a finite-state hidden-Markov model. By contrast, our
work exploits the latest techniques in statistical context-free grammars and syntax-based statistical
machine translation that handle more of the complexities of natural language.
More recently, Gold and Scassellati (2007) built a system called TWIG that uses existing language knowledge to help it learn the meaning of new words. The robot uses partial parses to focus
its attention on possible meanings of new words. By playing a game of catch, the robot was able to
learn the meaning of you and me as well as am and are as identity relations.
There has also been a variety of work on learning from captions that accompany pictures or
videos (Satoh, Nakamura, & Kanade, 1997; Berg, Berg, Edwards, & Forsyth, 2004). This area is of
particular interest given the large amount of captioned images and video available on the web and
television. Satoh et al. (1997) built a system to detect faces in newscasts. However, they use fairly
simple manually-written rules to determine the entity in the picture to which the language refers.
Berg et al. (2004) used a more elaborate learning method to cluster faces with names. Using the
data, they estimate the likelihood of an entity appearing in a picture given its context.
Some recent work on video retrieval has focused on learning to recognize events in sports videos
and connecting them to English words appearing in accompanying closed captions (Fleischman
& Roy, 2007; Gupta & Mooney, 2009). However, this work only learns the connection between
individual words and video events and does not learn to describe events using full grammatical
sentences. To avoid difficult problems in computer vision, our work uses a simulated world where
perception of complex events and their participants is much simpler.
In addition to observing events passively, there has also been work on grounded language learning in more interactive environments such as in computer video games (Gorniak & Roy, 2005). In
this work, players cooperate and communicate with each in order to accomplish a certain task. The
system learns to map spoken instructions to specific actions; however, it relies on existing statistical
parsers and does not learn the syntax and semantics of the language from the perceptual environment
alone. Kerr, Cohen, and Chang (2008) developed a system that learns grounded word-meanings for
nouns, adjectives, and spatial prepositions while a human is instructing it to perform tasks in a vir426

fiT RAINING A M ULTILINGUAL S PORTSCASTER

tual world; however, the system assumes an existing syntactic parser and prior knowledge of verb
semantics and is unable to learn these from experience.
Recently, there has been some interest in learning how to interpret English instructions describing how to use a particular website or perform other computer tasks (Branavan et al., 2009; Lau,
Drews, & Nichols, 2009). These systems learn to predict the correct computer action (pressing a
button, choosing a menu item, typing into a text field, etc.) corresponding to each step in the instructions. Instead of using parallel training data from the perceptual context, these systems utilize
direct matches between words in the natural language instructions and English words explicitly occurring in the menu items and computer instructions in order to establish a connection between the
language and the environment.
One of the core subproblems our work addresses is matching sentences to facts in the world to
which they refer. Some recent projects attempt to align text from English summaries of American
football games with database records that contain statistics and events about the game (Snyder
& Barzilay, 2007; Liang et al., 2009). However, Snyder and Barzilay (2007) use a supervised
approach that requires annotating the correct correspondences between the text and the semantic
representations. On the other hand, Liang et al. (2009) have developed an unsupervised approach
using a generative model to solve the alignment problem. They also demonstrated improved results
on matching sentences and events on our RoboCup English sportscasting data. However, their work
does not address semantic parsing or language generation. Section 7 presents results showing how
our methods can improve the NLMR matches produced by this approach as well as use them to
learn parsers and generators.

11. Future Work
As previously discussed, some of the limitations of the current system are due to inadequacies in
the perception of events extracted from the RoboCup simulator. Some of the language commentary,
particularly in the Korean data, refers to information about events that is not currently represented in
the extracted MRs. For example, a player dribbling the ball is not captured by our perceptual system.
The event extractor could be extended to include such information in the output representations.
Commentaries are not always about the immediate actions happening on the field. They can also
refer to statistics about the game, background information, or analysis of the game. While some of
these are difficult to obtain, it would be simple to augment the potential MRs to include events such
as the current score or the number of turnovers, etc. While these may be difficult to learn correctly,
they potentially would make the commentaries much more natural and engaging.
Some statements in the commentaries specifically refer to a pattern of activity across several
recent events rather than to a single event. For example, in one of the English commentaries, the
statement Purple team is very sloppy today. appears after a series of turn-overs to the other team.
The simulated perception could be extended to extract patterns of activity such as sloppiness;
however this assumes that such concepts are predefined, and extracting many such higher-level
predicates would greatly increase ambiguity in the training data. The current system assumes it
already has concepts for the words it needs to learn and can perceive these concepts and represent
them in MRs. However, it would be interesting to include a more Whorfian style of language
learning (Whorf, 1964) in which an unknown word such as sloppiness could actually cause the
creation of a new concept. For content words that do not seem to consistently correlate with any
perceived event, the system could collect examples of recent activity where the word is used and try
427

fiC HEN , K IM , & M OONEY

to learn a new higher-level concept that captures a regularity in these situations. For example, given
examples of situations referred to as sloppy, an inductive logic programming system (Lavrac &
Dzeroski, 1994) should be able to detect the pattern of several recent turnovers.
Another shortcoming of the current system is that each MR is treated independently. This fails
to exploit the fact that many MRs are related to each other. For example, a pass is preceded by a kick,
and a bad pass is followed by a turnover. A more natural way is to use a graphical representation to
represent not only the entities and events but also the relationships between them.
Currently tactical and strategic generation in our system are only loosely coupled. However,
conceptually they are much more closely related, and solving one problem should help solve the
other. Initializing our system with the output from Liang et al. (2009), which uses a generative model
that includes both strategic and tactical components, produced somewhat better results. However,
the interaction between all these components is very loose and a tighter integration of the different
pieces could yield stronger results in all the tasks.
An obvious extension to the current work is to apply it to real RoboCup games rather than
simulated ones. Recent work by Rozinat, Zickler, Veloso, van der Aalst, and McMillen (2008)
analyzes games in the RoboCup Small Size League using video from the overhead camera. By
using the symbolic event trace extracted by such a real perceptual system, our methods could be
applied to real-world games. Using speech recognition to accept spoken language input is another
obvious extension.
We are currently exploring extending our approach to learn to interpret and generate NL instructions for navigating in a virtual environment. The system will observe one person giving English
navigation instructions (e.g. Go down the hall and turn left after you pass the chair.) to another person who follows these directions to get to a chosen destination. By collecting examples of sentences
paired with the actions that were executed together with information about the local environment,
the system will construct an ambiguous supervised dataset for language learning. Such an approach
could eventually lead to virtual agents in games and educational simulations that can automatically
learn to interpret and generate natural language instructions.

12. Conclusion
We have presented an end-to-end system that learns to generate natural-language sportscasts for
simulated RoboCup soccer games by training on sample human commentaries paired with automatically extracted game events. By learning to semantically interpret and generate language without
explicitly annotated training data, we have demonstrated that a system can learn language by simply
observing linguistic descriptions of ongoing events. We also demonstrated the systems language
independence by successfully training it to produce sportscasts in both English and Korean.
Dealing with the ambiguous supervision inherent in the training environment is a critical issue in
learning language from perceptual context. We have evaluated various methods for disambiguating
the training data in order to learn semantic parsers and language generators. Using a generation
evaluation metric as the criterion for selecting the best NLMR pairs produced better results than
using semantic parsing scores when the initial training data is very noisy. Our system also learns
a model for strategic generation from the ambiguous training data by estimating the probability
that each event type evokes human commentary. Moreover, using strategic generation information
to help disambiguate the training data was shown to improve the results. We also demonstrated
that our system can be initialized with alignments produced by a different system to achieve better
428

fiT RAINING A M ULTILINGUAL S PORTSCASTER

results than either system alone. Finally, experimental evaluation verified that the overall system
learns to accurately parse and generate comments and to generate sportscasts that are competitive
with those produced by humans.

Acknowledgments
We thank Adam Bossy for his work on simulating perception for the RoboCup games. We also
thank Percy Liang for sharing his software and experimental results with us. Finally, we thank
the anonymous reviewers of JAIR and the editor, Lillian Lee, for their insightful comments which
helped improve the final presentation of this paper. This work was funded by the NSF grant IIS
0712907X. Most of the experiments were run on the Mastodon Cluster, provided by NSF Grant
EIA-0303609.

Appendix A. Details of the meaning representation language
Table 10 shows brief explanations of the different events we detect with our simulated perception.
Event
Playmode
Ballstopped
Turnover
Kick
Pass
BadPass
Defense
Steal
Block

Description
Signifies the current play mode as defined by the game
The ball speed is below a minimum threshold
The current possessor of the ball and the last possessor are on different teams
A player having possession of the ball in one time interval and not in the next
A player gains possession of the ball from a different player on the same team
A pass in which the player gaining possession of the ball is on a different team
A transfer from one player to an opposing player in their penalty area
A player having possession of the ball in one time interval and another player on
a different team having it in the next time interval
Transfer from one player to the opposing goalie.
Table 10: Description of the different events detected

Below we include the context-free grammar we developed for our meaning representation language. All derivations start at the root symbol *S.

*S
*S
*S
*S
*S
*S
*S
*S
*S

->
->
->
->
->
->
->
->
->

playmode ( *PLAYMODE )
ballstopped
turnover ( *PLAYER , *PLAYER )
kick ( *PLAYER )
pass ( *PLAYER , *PLAYER )
badPass ( *PLAYER , *PLAYER )
defense ( *PLAYER , *PLAYER )
steal ( *PLAYER )
block ( *PLAYER )

429

fiC HEN , K IM , & M OONEY

*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER

->
->
->
->
->
->
->
->
->
->
->
->
->
->
->
->
->
->
->
->
->
->

->
->
->
->
->
->
->
->
->
->
->
->
->
->
->

kick_off_l
kick_off_r
kick_in_l
kick_in_r
play_on
offside_l
offside_r
free_kick_l
free_kick_r
corner_kick_l
corner_kick_r
goal_kick_l
goal_kick_r
goal_l
goal_r

pink1
pink2
pink3
pink4
pink5
pink6
pink7
pink8
pink9
pink10
pink11
purple1
purple2
purple3
purple4
purple5
purple6
purple7
purple8
purple9
purple10
purple11

430

fiT RAINING A M ULTILINGUAL S PORTSCASTER

References
Aho, A. V., & Ullman, J. D. (1972). The Theory of Parsing, Translation, and Compiling. Prentice
Hall, Englewood Cliffs, NJ.
Andre, E., Binsted, K., Tanaka-Ishii, K., Luke, S., Herzog, G., & Rist, T. (2000). Three RoboCup
simulation league commentator systems. AI Magazine, 21(1), 5766.
Bailey, D., Feldman, J., Narayanan, S., & Lakoff, G. (1997). Modeling embodied lexical development. In Proceedings of the Nineteenth Annual Conference of the Cognitive Science Society.
Banerjee, S., & Lavie, A. (2005). METEOR: An automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and
Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pp. 6572
Ann Arbor, Michigan. Association for Computational Linguistics.
Barnard, K., Duygulu, P., Forsyth, D., de Freitas, N., Blei, D. M., & Jordan, M. I. (2003). Matching
words and pictures. Journal of Machine Learning Research, 3, 11071135.
Barzilay, R., & Lapata, M. (2005). Collective content selection for concept-to-text generation. In
Proceedings of the Human Language Technology Conference and Conference on Empirical
Methods in Natural Language Processing (HLT/EMNLP-05).
Barzilay, R., & Lee, L. (2002). Bootstrapping lexical choice via multiple-sequence alignment. In
Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing
(EMNLP-02).
Berg, T. L., Berg, A. C., Edwards, J., & Forsyth, D. A. (2004). Whos in the picture. In Advances
in Neural Information Processing Systems 17 (NIPS 2004).
Branavan, S., Chen, H., Zettlemoyer, L. S., & Barzilay, R. (2009). Reinforcement learning for
mapping instructions to actions. In Proceedings of the Joint conference of the 47th Annual
Meeting of the Association for Computational Linguistics and the 4th International Joint
Conference on Natural Language Processing of the Asian Federation of Natural Language
Processin (ACL-IJCNLP 2009).
Brown, P. F., Cocke, J., Della Pietra, S. A., Della Pietra, V. J., Jelinek, F., Lafferty, J. D., Mercer,
R. L., & Roossin, P. S. (1990). A statistical approach to machine translation. Computational
Linguistics, 16(2), 7985.
Brown, P. F., Della Pietra, V. J., Della Pietra, S. A., & Mercer, R. L. (1993). The mathematics
of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2),
263312.
Bunescu, R. C., & Mooney, R. J. (2005). Subsequence kernels for relation extraction. In Weiss,
Y., Scholkopf, B., & Platt, J. (Eds.), Advances in Neural Information Processing Systems 19
(NIPS 2006) Vancouver, BC.
Chen, D. L., & Mooney, R. J. (2008). Learning to sportscast: A test of grounded language acquisition. In Proceedings of 25th International Conference on Machine Learning (ICML-2008)
Helsinki, Finland.
431

fiC HEN , K IM , & M OONEY

Chen, M., Foroughi, E., Heintz, F., Kapetanakis, S., Kostiadis, K., Kummeneje, J., Noda, I., Obst,
O., Riley, P., Steffens, T., Wang, Y., & Yin, X. (2003). Users manual: RoboCup soccer server
manual for soccer server version 7.07 and later.. Available at http://sourceforge.
net/projects/sserver/.
Chiang, D. (2005). A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL05), pp. 263270 Ann Arbor, MI.
Collins, M. (2002). New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL-2002), pp. 263270 Philadelphia, PA.
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical Society B, 39, 138.
Doddington, G. (2002). Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. In Proceedings of ARPA Workshop on Human Language Technology,
pp. 128132 San Diego, CA.
Duboue, P. A., & McKeown, K. R. (2003). Statistical acquisition of content selection rules for
natural language generation. In Proceedings of the 2003 Conference on Empirical Methods
in Natural Language Processing (EMNLP-03), pp. 121128.
Fleischman, M., & Roy, D. (2007). Situated models of meaning for sports video retrieval. In Proceedings of Human Language Technologies: The Conference of the North American Chapter
of the Association for Computational Linguistics (NAACL-HLT-07) Rochester, NY.
Ge, R., & Mooney, R. J. (2005). A statistical semantic parser that integrates syntax and semantics. In
Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL2005), pp. 916 Ann Arbor, MI.
Ge, R., & Mooney, R. J. (2009). Learning a compositional semantic parser using an existing syntactic parser. In Proceedings of the Joint conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural
Language Processing of the Asian Federation of Natural Language Processin (ACL-IJCNLP
2009).
Gold, K., & Scassellati, B. (2007). A robot that uses existing vocabulary to infer non-visual word
meanings from observation. In Proceedings of the Twenty-Second Conference on Artificial
Intelligence (AAAI-07).
Gorniak, P., & Roy, D. (2005). Speaking with your sidekick: Understanding situated speech in
computer role playing games. In Proceedings of the 4th Conference on Artificial Intelligence
and Interactive Digital Entertainment Stanford, CA.
Gupta, S., & Mooney, R. (2009). Using closed captions to train activity recognizers that improve
video retrieval. In Proceedings of the CVPR-09 Workshop on Visual and Contextual Learning
from Annotated Images and Videos (VCL) Miami, FL.
432

fiT RAINING A M ULTILINGUAL S PORTSCASTER

Harnad, S. (1990). The symbol grounding problem. Physica D, 42, 335346.
Herzog, G., & Wazinski, P. (1994). VIsual TRAnslator: Linking perceptions and natural language
descriptions. Artificial Intelligence Review, 8(2/3), 175187.
Ide, N. A., & Jeronis, J. (1998). Introduction to the special issue on word sense disambiguation:
The state of the art. Computational Linguistics, 24(1), 140.
Joachims, T. (1998). Text categorization with support vector machines: Learning with many relevant
features. In Proceedings of the Tenth European Conference on Machine Learning (ECML98), pp. 137142 Berlin. Springer-Verlag.
Jurcicek, J., Gasic, M., Keizer, S., Mairesse, F., Thomson, B., & Young, S. (2009). Transformationbased learning for semantic parsing. In Interspeech Brighton, UK.
Kate, R. J., & Mooney, R. J. (2006). Using string-kernels for learning semantic parsers. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguistics (COLING/ACL-06), pp. 913920
Sydney, Australia.
Kate, R. J., & Mooney, R. J. (2007). Learning language semantics from ambiguous supervision.
In Proceedings of the Twenty-Second Conference on Artificial Intelligence (AAAI-07), pp.
895900 Vancouver, Canada.
Kerr, W., Cohen, P. R., & Chang, Y.-H. (2008). Learning and playing in wubble world. In Proceedings of the Fourth Artificial Intelligence for Interactive Digital Entertainment Conference
(AIIDE) Palo Alto, CA.
Kingsbury, P., Palmer, M., & Marcus, M. (2002). Adding semantic annotation to the Penn treebank.
In Proceedings of the Human Language Technology Conference San Diego, CA.
Knight, K., & Hatzivassiloglou, V. (1995). Two-level, many-paths generation. In Proceedings
of the 33rd Annual Meeting of the Association for Computational Linguistics (ACL-95), pp.
252260 Cambridge, MA.
Lau, T., Drews, C., & Nichols, J. (2009). Interpreting written how-to instructions. In Proceedings
of the Twenty-first International Joint Conference on Artificial Intelligence (IJCAI-2009).
Lavrac, N., & Dzeroski, S. (1994). Inductive Logic Programming: Techniques and Applications.
Ellis Horwood.
Liang, P., Jordan, M. I., & Klein, D. (2009). Learning semantic correspondences with less supervision. In Proceedings of the Joint conference of the 47th Annual Meeting of the Association for
Computational Linguistics and the 4th International Joint Conference on Natural Language
Processing of the Asian Federation of Natural Language Processin (ACL-IJCNLP 2009).
Lodhi, H., Saunders, C., Shawe-Taylor, J., Cristianini, N., & Watkins, C. (2002). Text classification
using string kernels. Journal of Machine Learning Research, 2, 419444.
433

fiC HEN , K IM , & M OONEY

Lu, W., Ng, H. T., Lee, W. S., & Zettlemoyer, L. S. (2008). A generative model for parsing natural
language to meaning representations. In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing (EMNLP-08) Honolulu, HI.
Marcus, M., Santorini, B., & Marcinkiewicz, M. A. (1993). Building a large annotated corpus of
English: The Penn treebank. Computational Linguistics, 19(2), 313330.
McKeown, K. R. (1985). Discourse strategies for generating natural-language text. Artificial Intelligence, 27(1), 141.
Meza-Ruiz, I. V., Riedel, S., & Lemon, O. (2008). Spoken language understanding in dialogue
systems, using a 2-layer Markov logic network: improving semantic accuracy. In Proceedings
of Londial.
Mooney, R. J. (2007). Learning for semantic parsing. In Gelbukh, A. (Ed.), Computational Linguistics and Intelligent Text Processing: Proceedings of the 8th International Conference,
CICLing 2007, Mexico City, pp. 311324. Springer Verlag, Berlin.
Och, F. J., & Ney, H. (2003). A systematic comparison of various statistical alignment models.
Computational Linguistics, 29(1), 1951.
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). BLEU: a method for automatic evaluation
of machine translation. In Proceedings of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL-2002), pp. 311318 Philadelphia, PA.
Riezler, S., Prescher, D., Kuhn, J., & Johnson, M. (2000). Lexicalized stochastic modeling of
constraint-based grammars using log-linear measures and EM training. In Proceedings of
the 38th Annual Meeting of the Association for Computational Linguistics (ACL-2000), pp.
480487 Hong Kong.
Roy, D. (2002). Learning visually grounded words and syntax for a scene description task. Computer Speech and Language, 16(3), 353385.
Rozinat, A., Zickler, S., Veloso, M., van der Aalst, W., & McMillen, C. (2008). Analyzing multiagent activity logs using process mining techniques. In Proceedings of the 9th International
Symposium on Distributed Autonomous Robotic Systems (DARS-08) Tsukuba, Japan.
Satoh, S., Nakamura, Y., & Kanade, T. (1997). Name-it: Naming and detecting faces in video by
the integration of image and natural language processing. In Proceedings of the Fifteenth
International Joint Conference on Artificial Intelligence (IJCAI-97).
Shawe-Taylor, J., & Cristianini, N. (2004). Kernel Methods for Pattern Analysis. Cambridge University Press.
Shieber, S. M. (1988). A uniform architecture for parsing and generation. In Proceedings of the
12th International Conference on Computational Linguistics (COLING-88), pp. 614619 Budapest, Hungary.
Siskind, J. M. (1996). A computational study of cross-situational techniques for learning word-tomeaning mappings. Cognition, 61(1), 3991.
434

fiT RAINING A M ULTILINGUAL S PORTSCASTER

Snyder, B., & Barzilay, R. (2007). Database-text alignment via structured multilabel classification. In Proceedings of the Twentieth International Joint Conference on Artificial Intelligence
(IJCAI-2007).
Srihari, R. K., & Burhans, D. T. (1994). Visual semantics: Extracting visual information from
text accompanying pictures. In Proceedings of the Twelfth National Conference on Artificial
Intelligence (AAAI-94).
Stolcke, A. (1995). An efficient probabilistic context-free parsing algorithm that computes prefix
probabilities. Computational Linguistics, 21(2), 165201.
Whorf, B. L. (1964). Language, Thought, and Reality: Selected Writings. MIT Press.
Wong, Y., & Mooney, R. J. (2006). Learning for semantic parsing with statistical machine translation. In Proceedings of Human Language Technology Conference / North American Chapter
of the Association for Computational Linguistics Annual Meeting (HLT-NAACL-06), pp. 439
446 New York City, NY.
Wong, Y., & Mooney, R. J. (2007). Generation by inverting a semantic parser that uses statistical
machine translation. In Proceedings of Human Language Technologies: The Conference of
the North American Chapter of the Association for Computational Linguistics (NAACL-HLT07), pp. 172179 Rochester, NY.
Yamada, K., & Knight, K. (2001). A syntax-based statistical translation model. In Proceedings of
the 39th Annual Meeting of the Association for Computational Linguistics (ACL-2001), pp.
523530 Toulouse, France.
Yu, C., & Ballard, D. H. (2004). On the integration of grounding language and learning objects. In
Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI-04), pp.
488493.
Zaragoza, H., & Li, C.-H. (2005). Learning what to talk about in descriptive games. In Proceedings
of the Human Language Technology Conference and Conference on Empirical Methods in
Natural Language Processing (HLT/EMNLP-05), pp. 291298 Vancouver, Canada.
Zelenko, D., Aone, C., & Richardella, A. (2003). Kernel methods for relation extraction. Journal
of Machine Learning Research, 3, 10831106.
Zettlemoyer, L. S., & Collins, M. (2005). Learning to map sentences to logical form: Structured
classification with probabilistic categorial grammars. In Proceedings of 21st Conference on
Uncertainty in Artificial Intelligence (UAI-2005) Edinburgh, Scotland.
Zettlemoyer, L. S., & Collins, M. (2007). Online learning of relaxed CCG grammars for parsing to
logical form. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural Language Learning (EMNLP-CoNLL-07),
pp. 678687 Prague, Czech Republic.

435

fiJournal of Artificial Intelligence Research 37 (2010) 247-277

Submitted 08/09; published 03/10

Context-Based Word Acquisition for Situated Dialogue in a
Virtual World

Shaolin Qu
Joyce Y. Chai

qushaoli@cse.msu.edu
jchai@cse.msu.edu

Department of Computer Science and Engineering
Michigan State University
East Lansing, MI 48824 USA

Abstract
To tackle the vocabulary problem in conversational systems, previous work has applied
unsupervised learning approaches on co-occurring speech and eye gaze during interaction to
automatically acquire new words. Although these approaches have shown promise, several
issues related to human language behavior and human-machine conversation have not been
addressed. First, psycholinguistic studies have shown certain temporal regularities between
human eye movement and language production. While these regularities can potentially
guide the acquisition process, they have not been incorporated in the previous unsupervised approaches. Second, conversational systems generally have an existing knowledge
base about the domain and vocabulary. While the existing knowledge can potentially
help bootstrap and constrain the acquired new words, it has not been incorporated in the
previous models. Third, eye gaze could serve different functions in human-machine conversation. Some gaze streams may not be closely coupled with speech stream, and thus
are potentially detrimental to word acquisition. Automated recognition of closely-coupled
speech-gaze streams based on conversation context is important. To address these issues,
we developed new approaches that incorporate user language behavior, domain knowledge,
and conversation context in word acquisition. We evaluated these approaches in the context of situated dialogue in a virtual world. Our experimental results have shown that
incorporating the above three types of contextual information significantly improves word
acquisition performance.

1. Introduction
One major bottleneck in human machine conversation is robust language interpretation.
When the encountered vocabulary is outside of the systems knowledge, the system tends
to fail. As conversational interfaces have become increasingly important in many applications such as remote interaction with robots (Lemon, Gruenstein, & Peters, 2002; Fong
& Nourbakhsh, 2005) and automated training and education (Traum & Rickel, 2002), the
ability to automatically acquire new words during online conversation becomes essential.
Different from traditional telephony-based spoken dialogue systems, in conversational interfaces users can look at a graphic display or a virtual world while interacting with artificial
agents using natural speech. This unique setting provides an opportunity for automated
vocabulary acquisition. During interaction, users visual perception (e.g., as indicated by
eye gaze) provides a potential channel for the system to automatically learn new words.
c
2010
AI Access Foundation. All rights reserved.

fiQu & Chai

The idea, as shown in previous work (Yu & Ballard, 2004; Liu, Chai, & Jin, 2007), is that
the parallel data of visual perception and spoken utterances can be used by unsupervised
approaches to automatically identify the mappings between words and visual entities and
thus acquire new words. While previous approaches provide a promising direction, they
mainly rely on the co-occurrence between words and visual entities in a completely unsupervised manner. However, in human machine conversation, there are different types of
extra information related to human language behaviors and characteristics of conversation
systems. Although this extra information can potentially provide supervision to guide the
word acquisition process and improve performance, it has not been systematically explored
in previous work.
First, a large body of psycholinguistic studies have shown that eye gaze is tightly linked
to human language processing. This is evident in both language comprehension (Tanenhaus, Spivey-Knowiton, Eberhard, & Sedivy, 1995; Eberhard, Spivey-Knowiton, Sedivy, &
Tanenhaus, 1995; Allopenna, Magnuson, & Tanenhaus, 1998; Dahan & Tanenhaus, 2005;
Spivey, Tanenhaus, Eberhard, & Sedivy, 2002) and language production (Meyer, Sleiderink,
& Levelt, 1998; Rayner, 1998; Griffin & Bock, 2000; Bock, Irwin, Davidson, & Leveltb, 2003;
Brown-Schmidt & Tanenhaus, 2006; Griffin, 2001). Specifically in human language production, which is directly relevant to automated computer interpretation of human language,
studies have found significant temporal regularities between the mentioned objects and the
corresponding words (Meyer et al., 1998; Rayner, 1998; Griffin & Bock, 2000). In object
naming tasks, the onset of a word begins approximately one second after a speaker has
looked at the corresponding visual referent (Griffin, 2004), and gazes are longer the more
difficult the name of the referent is to retrieve (Meyer et al., 1998; Griffin, 2001). About
100-300 ms after the articulation of the object name begins, the eyes move to the next object relevant to the task (Meyer et al., 1998). All these findings suggest that eyes move to
the mentioned objects before the corresponding words are uttered. Although this language
behavior can be used to constrain the mapping between words and visual objects, it has
not been incorporated in the previous approaches.
Second, practical conversational systems always have existing knowledge about their application domains and vocabularies. This knowledge base is acquired at their development
time: either authored by the domain experts or automatically learned from available data.
Although the existing knowledge can be rather limited and is desired to be enhanced automatically online (e.g., through automated vocabulary acquisition), it provides important
information about the structure of the domain and the existing vocabularies, which can
further bootstrap and constrain new word acquisition. This type of domain knowledge has
not been utilized in previous approaches.
Third, although psycholinguistic studies provide us with a sound empirical basis for
assuming that eye movements are predictive of speech, the gaze behavior in an interactive
setting can be much more complex. There are different types of eye movements (Kahneman,
1973). The naturally occurring eye gaze during speech production may serve different
functions, for example, to engage in the conversation or to manage turn taking (Nakano,
Reinstein, Stocky, & Cassell, 2003). Furthermore, while interacting with a graphic display,
a user could be talking about objects that were previously seen on the display or something
completely unrelated to any object the user is looking at. Therefore using all the speechgaze pairs for word acquisition can be detrimental. The type of gaze that is mostly useful
248

fiContext-Based Word Acquisition for Situated Dialogue in a Virtual World

for word acquisition is the kind that reflects the underlying attention and tightly links to the
content of the co-occurring speech. Thus, automatically recognizing closely coupled speech
and gaze streams during online conversation for word acquisition is important. However, it
has not been examined in previous work.
To address the above three issues, we have developed new approaches to automatic word
acquisition that (1) incorporate findings on user language behavior from psycholinguistic
studies, in particular the temporal alignment between spoken words and eye gaze; (2) utilize
the existing domain knowledge, and (3) automatically identify closely-coupled speech and
gaze streams based on conversation context. We evaluated these approaches in the context of
situated dialogue in a virtual world. Our experimental results have shown that incorporating
the above three types of contextual information significantly improves word acquisition
performance. Our simulation studies further demonstrate the effect of automatic online
word acquisition on improving language understanding in human-machine conversation.
In the following sections, we first introduce the domain and data collection in our investigation. We then describe the enhanced models for word acquisition that incorporate
additional contextual information (e.g., language behavior between spoken words and eye
gaze, domain knowledge, conversation context). Finally, we present the empirical evaluation
of our enhanced models and demonstrate the effect of online word acquisition on spoken
language understanding during human-machine conversation.

2. Related Work
Our work is motivated by previous work on grounded language acquisition and eye gaze in
multimodal human-computer interaction.
Grounded language acquisition is to learn the meaning of language by connecting the
language to the perception of the world. Language acquisition by grounding words to the
visual perceptions of objects has been studied in various language learning systems. For
example, given speech paired with video images of single objects, mutual information between audio and visual signals was used to learn words by associating acoustic phoneme
sequences with the visual prototypes (e.g., color, size, shape) of objects (Roy & Pentland,
2002; Roy, 2002). Generative models were developed to learn words by associating words
with image regions given parallel data of pictures and description text (Barnard, Duygulu,
de Freitas, Forsyth, Blei, & Jordan, 2003). Given pairs of spoken instructions containing
object names and the corresponding objects, an utterance-object joint probability model
was used to learn object names by identifying object name phonemes and associating them
with the objects (Taguchi, Iwahashi, Nose, Funakoshi, & Nakano, 2009). Given sequences
of utterances paired with scene representations, an incremental translation model was developed to learn word meaning by associating words with the semantic representations of
the referents in the scene (Fazly, Alishahi, & Stevenson, 2008). In addition to grounding
individual words, previous work has also investigated grounding phrases (referring expressions) to visual objects through semantic decomposition, for example using context free
grammar that connects linguistic structures with underlying visual properties (Gorniak &
Roy, 2004).
Besides visual objects, approaches have also been developed to ground words to meaning representations of events. For example, event logic was applied to ground verbs to
249

fiQu & Chai

motion events that were represented by force dynamics encoding the support, contact, and
attachment relations between objects in video images (Siskind, 2001). In a video game
domain, a translation model was used to ground words to the semantic roles of user actions (Fleischman & Roy, 2005). In a simulated Robocup soccer domain, given textual game
commentaries paired with the symbolic descriptions of game events, approaches based on
statistical parsing and learning were developed to ground the commentary text to game
events (Chen & Mooney, 2008). In a less restricted data setting, generative models were
developed to simultaneously segment the text into utterances and map the utterances to
meaning representations of event states (Liang, Jordan, & Klein, 2009). Different from
the above previous work, in our work, the visual perception is indicated by eye gaze. Eye
gaze, on one hand, is indicative of human attention, which provides opportunities to link
language and perception; on the other hand, is an implicit and subconscious input, which
could bring additional challenge in word acquisition.
Eye gaze has long been explored in human-computer interaction for direct manipulation interfaces as a pointing device (Jacob, 1991; Wang, 1995; Zhai, Morimoto, & Ihde,
1999). Eye gaze as a modality in multimodal interaction goes beyond the function of
pointing. In different speech and eye gaze systems, eye gaze has been explored for the
purpose of mutual disambiguation (Tanaka, 1999; Zhang, Imamiya, Go, & Mao, 2004), as a
complement to the speech channel for reference resolution (Campana, Baldridge, Dowding,
Hockey, Remington, & Stone, 2001; Kaur, Termaine, Huang, Wilder, Gacovski, Flippo,
& Mantravadi, 2003; Prasov & Chai, 2008; Byron, Mampilly, Sharma, & Xu, 2005) and
speech recognition (Cooke, 2006; Qu & Chai, 2007), and for managing human-computer
dialogue (Qvarfordt & Zhai, 2005).
Eye gaze has been explored recently for word acquisition. For example, Yu and Ballard
(2004) proposed an embodied multimodal learning interface for word acquisition, especially
through eye movement. In this work, given speech paired with eye gaze information and
video images, a translation model was applied to acquire words by associating acoustic
phone sequences with visual representations of objects and actions. This work has inspired
our research and is mostly related to our effort here. The difference between our work
and the work by Yu and Ballard lies in two aspects. First, the learning environment is
different. While Yu and Ballard focuses on the narrative descriptions of actions (e.g.,
making a sandwich, pouring some drinks, etc.) from human subjects, our focus is on
interactive conversation. In conversation, a human participant can take both a speaker
role and an addressee role. This represents a new scenario where word acquisition based
on eye movement may have new implications. Second, in the work by Yu and Ballard,
the IBM Translation Model 1 was applied to word acquisition. In our work, we further
incorporate other types of information such as user language behavior, domain knowledge,
and conversation context in the translation models.
In our previous work, we have experimented the application of IBM Translation Model
1 in vocabulary acquisition through gaze modeling in a conversation setting (Liu et al.,
2007). We have reported our initial investigation on incorporating temporal information
and domain knowledge in translation models (Qu & Chai, 2008) as well as automatically
identifying closely coupled speech and gaze streams (Qu & Chai, 2009). This paper extends
our previous work and provides a comprehensive evaluation on incorporating knowledge and
interactivity in word acquisition in a much richer application domain. We further examine
250

fiContext-Based Word Acquisition for Situated Dialogue in a Virtual World

how word acquisition is affected by automated speech recognition and what is the effect of
online word acquisition on language understanding in human-machine conversation.

3. Domain and Data
To facilitate our work on word acquisition, we collected data based on situated dialogue
in a virtual world. This data set is different from the data set used in our previous investigation (Qu & Chai, 2008). The difference lies in two aspects: 1) this dataset was
collected during mixed initiative human-machine conversation whereas the data in our previous investigation was based only on question and answering; 2) user studies in this work
were conducted in the context of situated dialogue, where human users are immersed in a
complex virtual world and can move around in the virtual environment.
3.1 Domain

Figure 1: Treasure hunting domain
The application of our virtual world is for treasure hunting. A human user is immersed
in a virtual world (i.e., a castle) 1 and is able to move around to look for hidden treasures.
The castle contains 115 3D objects. The user needs to consult with a remote expert (i.e.,
the artificial agent) for advice on finding the treasures. The expert has some knowledge
about the environment and the treasures but cannot see where the user is in the castle. So
the user needs to describe to the expert where he is and what he sees in order to get advice
on where to go and what to look for.
Figure 1 shows a snapshot of our virtual world. Figure 2 shows an excerpt of the dialog
between a user and the expert in finding one of the treasures. To focus our investigation on
word acquisition, we conducted wizard-of-oz studies to collect data 2 . More specifically, a set
of response templates corresponding to different types of responses are predefined. At each
1. The virtual world was developed based on the Irrlicht game engine http://irrlicht.sourceforge.net/.
2. WOZ studies allow the observation of a user interacting with a fully functional system, where some
or all of system functions are supplemented by a hidden human wizard. Users are led to believe they
are interacting with a real system (Klemmer, Sinha, Chen, Landay, Aboobaker, & Wang, 2000). This
technology, which permits the experimentally controlled study of human computer interaction behavior
without exposing users to unpredicted responses, has been used widely in earlier research on speech
interfaces and multimodal systems.

251

fiQu & Chai

S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:

Tell me what you see.
I see a golden tomb or sarcophagus.
Describe it.
It appears to be made of gold and there are Egyptian decorations on it.
What else do you see?
I see a carving hanging on the wall appears to be African.
Go back.
Im back by the sarcophagus. There appears to be lava surrounding it or fire.
I couldnt understand you.
The room that the sarcophagus is in is surrounded by lava or fire.
Do you see a statue?
Should i try to open the sarcophagus?
No.
I see a statue.
Describe.
It appears to be a stone statue of an Egyptian.
What else do you see?
I see a small vase a pottery vase decorated with flowers.
The treasure is near.
Should i try to move the vase?
Yes.
I cant move the vase.
Do you see another one?
Yes, theres a wooden table with a pottery jug and a purple vase on it. Should i try to move
the purple vase?
Do you see a statue?
Yes, I see a stone statue.
Do you see another one?
Yes, I see a gold and black statue of an Egyptian sitting.
Compare these objects.
The stone statue is in very bad shape, and the golden statue appears to be in good shape. It
has a golden crown. It is sitting in a wooden chair.
Try moving it.
I move the head off the statue and now i see a scroll. Should i take the scroll?
Describe.
It is a white scroll with a red seal.
Take it.
I took the scroll.
Good.

Figure 2: The dialog between a user and the system in finding one of the treasures

system turn, the system automatically determines what kind of responses (i.e., template)
should be provided; for example, whether to ask for clarification, provide confirmation,
or ask more information about a particular object. Such decision is made based on a
distribution to simulate different types of system acceptance and rejection. This distribution
can be modified according to different needs of the experiments. Based on the chosen
template, the human wizard then serves as a language understanding component and fills
in this template with the specific information related to the user input. The filled template is
252

fiContext-Based Word Acquisition for Situated Dialogue in a Virtual World

used to automatically generate natural language which is further processed by the Microsoft
Text-to-Speech engine to generate speech responses. During the experiments, the users
speech was recorded, and the users eye gaze was captured by a Tobii eye tracker.
3.2 Data Preprocessing
From 20 users experiments, we collected 3709 utterances with accompanying gaze fixations.
We transcribed the collected speech. The vocabulary size of the speech transcript is 1082,
among which 757 are nouns/adjectives. The users speech was also automatically recognized
online by the Microsoft speech recognizer with a word error rate (WER) of 48.1% for the
1-best recognition. The vocabulary size of the 1-best speech recognition is 3041, among
which 1643 are nouns/adjectives. The nouns and adjectives in the transcriptions and the
recognized 1-best hypotheses were automatically identified by the Stanford Part-of-Speech
Tagger (Toutanova & Manning, 2000; Toutanova, Klein, Manning, & Singer, 2003).
Theres

a

purple

vase

in

an

orange

face
speech str eam

gaze fixation

ts
[table_vase] [vase_purple]

gaze str eam

te

[vase_greek3] [vase_greek3]

[vase_greek3]

[vase_greek3]

[fixated entity]

Figure 3: Accompanying gaze fixations and the 1-best recognition of a users utterance
Theres a purple vase and an orange vase. (There are two incorrectly recognized
words in and face in the 1-best recognition)

The collected speech and gaze streams were automatically paired together by the system.
Each time the system detected a sentence boundary of the users speech, it paired the
recognized speech with the gaze fixations that the system had been accumulating since the
previously detected sentence boundary. Figure 3 shows a stream pair of a users speech and
its accompanying gaze fixations. In the speech stream, each spoken word was timestamped
by the speech recognizer. In the gaze stream, each gaze fixation has a starting timestamp
ts and an ending timestamp te provided by the eye tracker. Each gaze fixation results in a
fixated entity (3D object). When multiple entities are fixated by one gaze fixation due to the
overlapping of the entities, the foremost one is chosen. In the gaze stream, the neighboring
gaze fixations that fixate the same entity are merged.
Given the paired speech and gaze streams, we build a set of parallel word sequences and
gaze fixated entity sequences {(w, e)} for the task of word acquisition. The word sequence
w consists of only nouns and adjectives from the 1-best recognition of the spoken utterance.
The entity sequence e contains the entities fixated by the gaze fixations. For the parallel
speech and gaze streams shown in Figure 3, the resulting word sequence is w = [purple vase
orange face] and the resulting entity sequence is e = [table vase vase purple vase greek3 ].
253

fiQu & Chai

4. Translation Models for Word Acquisition
Since we are working on conversational systems where users interact with a visual scene,
we consider the task of word acquisition as associating words with visual entities in the
domain. Given the parallel speech and gaze fixated entities {(w, e)}, we formulate word
acquisition as a translation problem and use translation models to estimate word-entity
association probabilities p(w|e). The words with the highest association probabilities are
chosen as acquired words for entity e.
4.1 Base Model I
Using the translation model I (Brown, Pietra, Pietra, & Mercer, 1993), where each word is
equally likely to be aligned with each entity, we have
m X
l
Y
1
p(w|e) =
p(wj |ei )
(l + 1)m

(1)

j=1 i=0

where l and m are the lengths of the entity and word sequences respectively. We refer to
this model as Model-1.
4.2 Base Model II
Using the translation model II (Brown et al., 1993), where alignments are dependent on
word/entity positions and word/entity sequence lengths, we have
p(w|e) =

m X
l
Y

p(aj = i|j, m, l)p(wj |ei )

(2)

j=1 i=0

where aj = i means that wj is aligned with ei . When aj = 0, wj is not aligned with any
entity (e0 represents a null entity). We refer to this model as Model-2.
Compared to Model-1, Model-2 considers the ordering of words and entities in word
acquisition. EM algorithms are used to estimate the probabilities p(w|e) in the translation
models.

5. Incorporating User Language Behavior in Word Acquisition
In Model-2, word-entity alignments are estimated from co-occurring word and entity sequences in an unsupervised way. The estimated alignments are dependent on where the
words/entities appear in the word/entity sequences, not on when those words and gaze
fixated entities actually occur. Motivated by the findings that users move their eyes to
the mentioned object directly before speaking a word (Griffin & Bock, 2000), we make the
word-entity alignments dependent on their temporal relation in a new model (referred as
Model-2t):
m X
l
Y
p(w|e) =
pt (aj = i|j, e, w)p(wj |ei )
(3)
j=1 i=0

254

fiContext-Based Word Acquisition for Situated Dialogue in a Virtual World

where pt (aj = i|j, e, w) is the temporal alignment probability computed based on the temporal distance between entity ei and word wj .
We define the temporal distance between ei and wj as

ts (ei )  ts (wj )  te (ei )
 0
te (ei )  ts (wj ) ts (wj ) > te (ei )
d(ei , wj ) =
(4)

ts (ei )  ts (wj ) ts (wj ) < ts (ei )
where ts (wj ) is the starting timestamp (ms) of word wj , ts (ei ) and te (ei ) are the starting
and ending timestamps (ms) of gaze fixation on entity ei .
The alignment of word wj and entity ei is decided by their temporal distance d(ei , wj ).
Based on the psycholinguistic finding that eye gaze happens before a spoken word, wj is not
allowed to be aligned with ei when wj happens earlier than ei (i.e., d(ei , wj ) > 0). When wj
happens no earlier than ei (i.e., d(ei , wj )  0), the closer they are, the more likely they are
aligned. Specifically, the temporal alignment probability of wj and ei in each co-occurring
instance (w, e) is computed as

0
d(ei , wj ) > 0



 exp[  d(ei , wj )]
d(ei , wj )  0
l
pt (aj = i|j, e, w) =
(5)
X


exp[  d(ei , wj )]


i=0

where  is a constant for scaling d(ei , wj ).
An EM algorithm is used to estimate probabilities p(w|e) and  in Model-2t. It is
worthwhile to mention that, findings from psycholinguistic studies have provided specific
offsets in terms of how eye gaze corresponds to speech production. For example, it shows
that speakers look at an object about a second before they say it, but about 100-300 ms
after articulation of the object name begins, the eyes move to the next object relevant to
the task (Meyer et al., 1998). Since the conversation setting in our study is much more
complex than the simple settings in psycholinguistic research, we found larger variations on
the offset (Liu et al., 2007) in our data. Therefore we chose not to use any offset in the
alignment model here.

6. Incorporating Domain Knowledge in Word Acquisition
Speech-gaze temporal alignment and occurrence statistics sometimes are not sufficient to
associate words to entities correctly. For example, suppose a user says there is a lamp on the
dresser  while looking at a lamp object on a table object. Due to their co-occurrence with
the lamp object, the words dresser and lamp are both likely to be associated with the lamp
object in the translation models. As a result, the word dresser is likely to be incorrectly
acquired for the lamp object. For the same reason, the word lamp could be acquired
incorrectly for the table object. To solve this type of association problem, the semantic
knowledge about the domain and words can be helpful. For example, the knowledge that
the word lamp is more semantically related to the object lamp can help the system avoid
associating the word dresser to the lamp object. Specifically, we solve this type of wordentity association by utilizing the domain knowledge present in the system and external
lexical semantic resources.
255

fiQu & Chai

On one hand, each conversational system has a domain model, which is the knowledge
representation about its domain such as the types of objects and their properties and relations, the task structures, etc. This domain model is usually acquired at the development
stage before the deployment of the system. The domain model provides an important resource to enable domain reasoning and language interpretation (DeVault & Stone, 2003).
On the other hand, there are available resources about domain independent lexical knowledge (e.g., WordNet, see Fellbaum, 1998). The idea is that if the domain model can be
linked to the external lexical resources either manually or automatically at the development
state, then the external knowledge source can be used to help constrain the acquired words.
In the following sections, we first describe our domain modeling, then define the semantic
relatedness of word and entity based on domain modeling and WordNet semantic lexicon,
and finally describe different ways of using the semantic relatedness of word and entity to
help word acquisition.
6.1 Domain Modeling
We model the treasure hunting domain as shown in Figure 4. The domain model contains
all domain related semantic concepts. For practical conversational systems, this domain
modeling is typically acquired at the development stage either through manual authoring
by the domain experts or automated learning based on annotated data. In our current work,
all properties of the domain entities are represented by domain concepts. The entity properties include: semantic type, color, size, shape, and material. We use WordNet synsets to
represent the domain concepts (i.e., synsets in the format of word#part-of-speech#senseid). The sense-id here represents the specific WordNet sense associated with the word in
representing the concept. For example, the domain concepts SEM PLATE and COLOR of
the entity plate are represented by the synsets plate#n#4 and color#n#1 in WordNet.
Note that the link between domain concepts and WordNet synsets can be automatically acquired given the existing vocabularies. Here, to illustrate the idea, we simplify the problem
and directly connect domain concepts to synsets.
Note that in the domain model, the domain concepts are not specific to a certain entity,
they are general concepts for a certain type of entity. Multiple entities of the same type
have the same properties and share the same set of domain concepts. Therefore, properties
such as color and size of an entity have general concepts color#n#1 and size#n#1
instead of more specific concepts like yellow#a#1 and big#a#1, so their concepts can
be shared by other entities of the same type, but with different colors and sizes.
6.2 Semantic Relatedness of Word and Entity
We compute the semantic relatedness of a word w and an entity e based on the semantic
similarity between w and the properties of e using the domain model as a bridge. Specifically,
semantic relatedness SR(e, w) is defined as
SR(e, w) = max sim(s(cie ), sj (w))
i,j

(6)

where cie is the i-th property of entity e, s(cie ) is the synset of property cie as in domain model,
sj (w) is the j-th synset of word w as defined in WordNet, and sim(, ) is the similarity
score of two synsets.
256

fiContext-Based Word Acquisition for Situated Dialogue in a Virtual World

Do m a i n M o d e l

Entities:

Domain
concepts:

pharaoh

plate

SEM_PLATE

COLOR

SEM_PHARAOH

COLOR

SIZE

color#n#1

WordNet
concepts:

pharaoh#n#1

plate#n#4

picture#n#2

size#n#1

Figure 4: Domain model with domain concepts represented by WordNet synsets

We computed the similarity score of two synsets based on the path length between them.
The similarity score is inversely proportional to the number of nodes along the shortest path
between the synsets as defined in WordNet. When the two synsets are the same, they have
the maximal similarity score of 1. The WordNet-Similarity tool (Pedersen, Patwardhan, &
Michelizzi, 2004) was used for the synset similarity computation.
6.3 Word Acquisition with Word-Entity Semantic Relatedness
We can use the semantic relatedness of word and entity to help the system acquire semantically compatible words for each entity, and therefore improve word acquisition performance.
The semantic relatedness can be applied for word acquisition in two ways: post-process the
learned word-entity association probabilities by rescoring them with semantic relatedness,
or directly affect the learning of word-entity associations by constraining the alignment of
word and entity in the translation models.
6.3.1 Rescoring with Semantic Relatedness
In the acquired word list for an entity ei , each word wj has an association probability p(wj |ei )
that is learned from a translation model. We use the semantic relatedness SR(ei , wj ) to
redistribute the probability mass for each wj . The new association probability is given by:

p(wj |ei )SR(ei , wj )
p0 (wj |ei ) = X
p(wj |ei )SR(ei , wj )
j

257

(7)

fiQu & Chai

6.3.2 Semantic Alignment Constraint in Translation Model
When used to constrain the word-entity alignment in the translation model, semantic relatedness can be used alone or used together with speech-gaze temporal information to decide
the alignment probability of word and entity (Qu & Chai, 2008).
 Using only semantic relatedness to constrain word-entity alignments in Model-2s, we
have
m X
l
Y
p(w|e) =
ps (aj = i|j, e, w)p(wj |ei )
(8)
j=1 i=0

where ps (aj = i|j, e, w) is the alignment probability based on semantic relatedness,
SR(ei , wj )
ps (aj = i|j, e, w) = X
SR(ei , wj )

(9)

i

 Using semantic relatedness and temporal information to constrain word-entity alignments in Model-2ts, we have
p(w|e) =

m X
l
Y

pts (aj = i|j, e, w)p(wj |ei )

(10)

j=1 i=0

where pts (aj = i|j, e, w) is the alignment probability that is decided by both temporal
relation and semantic relatedness of ei and wj ,
ps (aj = i|j, e, w)pt (aj = i|j, e, w)
pts (aj = i|j, e, w) = X
ps (aj = i|j, e, w)pt (aj = i|j, e, w)

(11)

i

where ps (aj = i|j, e, w) is the semantic alignment probability in Equation (9), and
pt (aj = i|j, e, w) is the temporal alignment probability given in Equation (5).
EM algorithms are used to estimate p(w|e) in Model-2s and Model-2ts.

7. Incorporating Conversation Context in Word Acquisition
As mentioned earlier, not all speech-gaze pairs are useful for word acquisition. In a speechgaze pair, if the speech does not have any word that relates to any of the gaze fixated
entities, this instance only adds noise to word acquisition. Therefore, we should identify
the closely coupled speech-gaze pairs and only use them for word acquisition.
In this section, we first describe the feature extraction based on conversation interactivity, then describe the use of a logistic regression classifier to predict whether a speech-gaze
pair is a closely coupled speech-gaze instance  an instance where at least one noun
or adjective in the speech stream is referring to some gaze fixated entity in the gaze stream.
For the training of the classifier for speech-gaze prediction, we manually labeled each instance whether it is a closely coupled speech-gaze instance based on the speech transcript
and gaze fixations.
258

fiContext-Based Word Acquisition for Situated Dialogue in a Virtual World

7.1 Features Extraction
For a parallel speech-gaze instance, the following sets of features are automatically extracted.
7.1.1 Speech Features (S-Feat)
Let cw be the count of nouns and adjectives in the utterance, and ls be the temporal length
of the speech. The following features are extracted from speech:
 cw  count of nouns and adjectives.
More nouns and adjectives are expected in the users utterance describing entities.
 cw /ls  normalized noun/adjective count.
The effect of speech length ls on cw is considered.
7.1.2 Gaze Features (G-Feat)
For each fixated entity ei , let lei be its fixation temporal length. Note that several gaze
fixations may have the same fixated entity, lei is the total length of all the gaze fixations
that fixate on entity ei . We extract the following features from gaze stream:
 ce  count of different gaze fixated entities.
Less fixated entities are expected when the user is describing entities while looking at
them.
 ce /ls  normalized entity count.
The effect of speech temporal length ls on ce is considered.
 maxi (lei )  maximal fixation length.
At least one fixated entitys fixation is expected to be long enough when the user is
describing entities while looking at them.
 mean(lei )  average fixation length.
The average gaze fixation length is expected to be longer when the user is describing
entities while looking at them.
 var(lei )  variance of fixation lengths.
The variance of the fixation lengths is expected to be smaller when the user is describing entities while looking at them.
The number of gaze fixated entities is not only decided by the users eye gaze, it is also
affected by the visual scene. Let cse be the count of all the entities that have been visible
during the length of the gaze stream. We also extract the following scene related feature:
 ce /cse  scene normalized fixated entity count.
The effect of the visual scene on ce is considered.
259

fiQu & Chai

7.1.3 User Activity Features (UA-Feat)
While interacting with the system, the users activity can also be helpful in determining
whether the users eye gaze is tightly linked to the content of the speech. The following
features are extracted from the users activities:
 maximal distance of the users movements  the maximal change of user position (3D
coordinates) during the speech length.
The user is expected to move within a smaller range while looking at entities and
describing them.
 variance of the users positions
The user is expected to move less frequently while looking at entities and describing
them.
7.1.4 Conversation Context Features (CC-Feat)
While talking to the system (i.e., the expert), the users language and gaze behavior
are influenced by the state of the conversation. For each speech-gaze instance, we use the
previous system response type as a nominal feature to predict whether this is a closely
coupled speech-gaze instance.
In our treasure hunting domain, there are eight types of system responses in two categories:
System-Initiative Responses:
 specific-see  the system asks whether the user sees a certain entity, e.g., Do you see
another couch?.
 nonspecific-see  the system asks whether the user sees anything, e.g., Do you see
anything else?, Tell me what you see.
 previous-see  the system asks whether the user previously sees something, e.g., Have
you previously seen a similar object?.
 describe  the system asks the user to describe in detail what the user sees, e.g.,
Describe it, Tell me more about it.
 compare  the system asks the user to compare what the user sees, e.g., Compare
these objects.
 clarify  the system asks the user to make clarification, e.g., I did not understand
that, Please repeat that.
 action-request  the system asks the user to take action, e.g., Go back, Try moving
it.
User-Initiative Responses:
 misc  the system hands the initiative back to the user without specifying further
requirements, e.g., I dont know, Yes.
260

fiContext-Based Word Acquisition for Situated Dialogue in a Virtual World

7.2 Logistic Regression Model
Given the extracted feature x and the closely coupled label y of each instance in the
training set, we train a ridge logistic regression model (Cessie & Houwelingen, 1992) to
predict whether an instance is a closely coupled instance (y = 1) or not (y = 0).
In the logistic regression model, the probability that yi = 1, given the feature xi =
(xi1 , xi2 , . . . , xim ), is modeled by
P
i
exp( m
j=1 j xj )
P
p(yi |xi ) =
i
1 + exp( m
j=1 j xj )
where j are the features weights to be learned.
The log-likelihood l of the data (X, y) is
X
l() =
[yi log p(yi |xi ) + (1  yi ) log(1  p(yi |xi ))]
i

In ridge logistic regression, parameters j are estimated by maximizing a regularized loglikelihood
l () = l()  ||||2
where  is the ridge parameter that is introduced to achieve more stable parameter estimation.
7.3 Evaluation of Speech-gaze Identification
Since the goal of identifying closely coupled speech-gaze instances is to improve word acquisition and we are only interested in acquiring nouns and adjectives, only the instances with
recognized nouns/adjectives are used for training the logistic regression classifier. Among
the 2969 instances with recognized nouns/adjectives and gaze fixations, 2002 (67.4%) instances are labeled as closely coupled. The speech-gaze prediction was evaluated by a 10-fold
cross validation.
Table 1 shows the prediction precision and recall when different sets of features are used.
As seen in the table, as more features are used, the prediction precision goes up and the
recall goes down. It is important to note that prediction precision is more critical than
recall for word acquisition when sufficient amount of data is available. Noisy instances
where the gaze does not link to the speech content will only hurt word acquisition since
they will guide the translation models to ground words to the wrong entities. Although
higher recall can be helpful, its effect is expected to become less when co-occurrences can
already be established.
The results show that speech features (S-Feat) and conversation context features (CCFeat), when used alone, do not improve prediction precision much compared to the baseline
of predicting all instances closely coupled with a precision of 67.4%. When used alone,
gaze features (G-Feat) and user activity features (UA-Feat) are the two most useful feature
sets for increasing prediction precision. When they are used together, the prediction precision is further increased. Adding either speech features or conversation context features
to gaze and user activity features (G-Feat + UA-Feat + S-Feat/CC-Feat) increases the
prediction precision more. Using all four sets of features (G-Feat + UA-Feat + S-Feat +
261

fiQu & Chai

Feature sets
Null (baseline)
S-Feat
G-Feat
UA-Feat
CC-Feat
G-Feat + UA-Feat
G-Feat + UA-Feat + S-Feat
G-Feat + UA-Feat + CC-Feat
G-Feat + UA-Feat + S-Feat + CC-Feat

Precision
0.674
0.686
0.707
0.704
0.688
0.719
0.741
0.731
0.748

Recall
1
0.995
0.958
0.942
0.936
0.948
0.908
0.918
0.899

Table 1: Speech-gaze prediction performances with different feature sets

CC-Feat) achieves the highest prediction precision. McNemar tests have shown that this is a
significant change compared to using G-Feat + UA-Feat + S-Feat (2 = 8.3, p < 0.004) and
all other feature configurations (2 = 45.4  442.7, p < 0.0001). Therefore, we choose to
use all feature sets to identify the closely coupled speech-gaze instances for word acquisition.

8. Evaluation of Word Acquisition
Each practical conversational system starts with an initial knowledge base (vocabulary). We
assume that the system already has one default word for each entity in its default vocabulary.
The default word of an entity indicates the semantic type of the entity. For example, the
word barrel is the default word for the entity barrel. Among the acquired words, we only
evaluate those new words that are not in the systems vocabulary. For example, the word
barrel would be excluded from the candidate words acquired for the entity barrel.
8.1 Grounding Words to Domain Concepts
Based on the translation models for word acquisition (Sections 5 & 6), we can obtain the
word-entity association probability p(w|e). This probability provides a means to ground
words to entities. In conversational systems, one important goal of word acquisition is to
make the system understand the semantic meaning of new words. Word acquisition by
grounding words to objects is not always sufficient for identifying their semantic meanings.
Suppose the word green is grounded to a green chair object, so is the word chair. Although
the system is aware that green is some word describing the green chair, it does not know
that the word green refers to the chairs color while the word chair refers to the chairs
semantic type. Thus, after learning the word-entity associations p(w|e) by the translation
models, we need to further ground words to domain concepts of entity properties.
Based on the domain model discussed earlier (Section 6.1), we apply WordNet to ground
words to domain concepts. For each entity e, based on association probabilities p(w|e), we
can choose the n-best words as acquired words for e. Those n-best words have the n highest
association probabilities. For each word w acquired for e, the grounded concept ce for w is
262

fiContext-Based Word Acquisition for Situated Dialogue in a Virtual World

chosen as the one that has the highest semantic relatedness with w:
ce = arg max[max sim(s(cie ), sj (w))]
i

j

(12)

where sim(s(cie ), sj (w)) is the semantic similarity score defined in Equation (6).
To evaluate the acquired words for the domain concepts, we manually compile a set of
gold standard words from all users speech transcripts and gaze fixations. Those gold
standard words are the words that the users have used to refer to the entities and their
properties (e.g., color, size, shape) during the interaction with the system. The automatically acquired words are evaluated against those gold standard words.
8.2 Evaluation Metrics
Following the standard evaluation on information retrieval, the following metrics are used
to evaluate the words acquired for domain concepts (i.e., entity properties) {ce }.
 Precision

P

ce

 Recall

# words correctly acquired for ce
P
ce # words acquired for ce

P
ce # words correctly acquired for ce
P
ce # gold standard words of ce

 Mean Average Precision (MAP)

MAP =

X PNw P (r)  rel(r)
r=1
Ne
e
#e

where Ne is the number of the gold standard words of all the properties ce of entity
e, Nw is the vocabulary size, P (r) is the acquisition precision at a given cut-off rank r,
rel(r) is a binary function indicating whether the word at rank r is a gold-standard
word for some property ce of entity e.
8.3 Evaluation Results
To investigate the effects of speech-gaze temporal information and domain semantic knowledge on word acquisition, we compare the word acquisition performances of the following
models:
 Model-1  base model I without word-entity alignment (Equation (1)).
 Model-1-r  Model-1 with semantic relatedness rescoring of word-entity association.
 Model-2  base model II with positional alignment (Equation (2)).
 Model-2s  enhanced model with semantic alignment (Equation (8)).
 Model-2t  enhanced model with temporal alignment (Equation (3)).
263

fiQu & Chai

 Model-2ts  enhanced model with both temporal and semantic alignment (Equation (10)).
 Model-2t-r  Model-2t with semantic relatedness rescoring of word-entity association.
8.3.1 Results of Using Speech-Gaze Temporal Information
1

0.4

Model-1
Model-2
Model-2t

0.9

0.38
Mean Average Precision

0.8

Precision

0.7
0.6
0.5
0.4
0.3
0.2

0.34
0.32
0.3
0.28
0.26

0.1
0

0.36

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0.24
M-1

M-2

M-2t

Recall

(a) Precision vs Recall

(b) Mean Average Precision

Figure 5: Word acquisition performance when speech-gaze temporal information is used
Figure 5 shows the interpolated precision-recall curves and Mean Average Precisions
(MAPs) of Model-2t and the baseline models Model-1 and Model-2. As shown in the figure,
Model-2 does not improve word acquisition compared to Model-1. This result shows that
it is not helpful to consider the index-based positional alignment of word and entity for
word acquisition. By incorporating speech-gaze temporal alignment, Model-2t consistently
achieves higher precisions than Model-1 at different recalls. In terms of MAP, Model-2t
significantly increases MAP (t = 3.08, p < 0.002) compared to Model-1. This means that
the use of speech-gaze temporal alignment improves word acquisition.
8.3.2 Results of Using Domain Semantic Relatedness
Figure 6 shows the results of using domain semantic relatedness in word acquisition. As
shown in the figure, compared to the baseline of using no extra knowledge (Model-1),
using domain semantic relatedness improves word acquisition no matter when it is used to
rescore word-entity association (Model-1-r) or to constrain word-entity alignment (Model2s). Compared to Model-1, the MAP is significantly improved by Model-1-r (t = 6.32, p <
0.001) and Model-2s (t = 5.36, p < 0.001).
Domain semantic relatedness can also be used together with speech-gaze temporal information to improve word acquisition. Compared to Model-1, the MAP is significantly increased by Model-2ts (t = 5.59, p < 0.001) that uses semantic relatedness together with temporal information to constrain word-entity alignments and Model-2t-r (t = 6.01, p < 0.001),
where semantic relatedness is used to rescore the word-entity associations learned by Model2t.
264

fiContext-Based Word Acquisition for Situated Dialogue in a Virtual World

1

0.4

Model-1
Model-1-r
Model-2s
Model-2ts
Model-2t-r

0.8

Precision

0.7

0.38
Mean Average Precision

0.9

0.6
0.5
0.4
0.3
0.2

0.34
0.32
0.3
0.28
0.26

0.1
0

0.36

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0.24
M-1

M-1-r

M-2s

M-2ts

M-2t-r

Recall

(a) Precision vs Recall

(b) Mean Average Precision

Figure 6: Word acquisition performance when domain semantic relatedness is used
Comparing the two ways of using semantic relatedness for word acquisition, it is found
that rescoring word-entity association with semantic relatedness works better. Model-2t-r
achieve higher MAP (t = 2.22, p < 0.015) than Model-2ts.
It is also verified that using both speech-gaze temporal alignment and domain semantic
relatedness rescoring works better than using either one alone. With temporal alignment
and semantic relatedness rescoring, Model-2t-r significantly increases MAP when compared
to Model-1-r (t = 2.75, p < 0.004) where only semantic relatedness rescoring is used and
Model-2t (t = 5.38, p < 0.001) where only temporal alignment is used.
8.3.3 Results Based on Identified Closely Coupled Speech-Gaze Streams
We have shown that Model-2t-r, where both speech-gaze temporal alignment and domain
semantic relatedness rescoring are incorporated, achieves the best word acquisition performance. Therefore, Model-2t-r is used to evaluate the word acquisition based on the
identified closely coupled speech-gaze data. Since Model-2t-r requires linking domain models with external knowledge source (e.g., WordNet) which may not be available for some
applications, we also evaluate the effect of the identification of closely coupled speech-gaze
streams on word acquisition with Model-2t, where only speech-gaze temporal alignment is
incorporated.
We evaluate the effect of automatic identification of closely coupled speech-gaze instances
on word acquisition through a 10-fold cross validation. In each fold, 10% of the data set
was used to train the logistic regression classifier for predicting closely coupled speechgaze instances, then all instances, predicted closely coupled instances, and true (manually
labeled) closely coupled instances of the other 90% of the data set were used for word
acquisition respectively. Figures 7 & 8 show the averaged interpolated precision-recall curves
and MAPs achieved by Model-2t and Model-2t-r using all instances (all ), only predicted
closely coupled instances (predicted ), and true closely coupled instances (true).
When words are acquired by Model-2t, as shown in Figure 7, using predicted closely
coupled instances achieves better performance than using all instances. The MAP is significantly increased (t = 2.69, p < 0.005) by acquiring words from predicted closely coupled
265

fiQu & Chai

1

0.42

all
predicted
true

0.9

0.4
Mean Average Precision

0.8

Precision

0.7
0.6
0.5
0.4
0.3
0.2

0.38

0.36

0.34

0.1
0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

0.32

1

all

predicted

true

Recall

(a) Precision vs Recall

(b) Mean Average Precision

Figure 7: Word acquisition performance by Model-2t on different data set

1

0.42

all
predicted
true

0.9

0.4
Mean Average Precision

0.8

Precision

0.7
0.6
0.5
0.4
0.3
0.2

0.38

0.36

0.34

0.1
0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0.32
all

predicted

true

Recall

(a) Precision vs Recall

(b) Mean Average Precision

Figure 8: Word acquisition performance by Model-2t-r on different data set

instances. The result shows that the identification of closely coupled speech-gaze instances
helps word acquisition. When the true closely coupled speech-gaze instances are used for
word acquisition, the acquisition performance is further improved. This means that better
identification of closely coupled speech-gaze instances can lead to better word acquisition
performance.
When words are acquired by Model-2t-r, as shown in Figure 8, using predicted closely
coupled instances improves acquisition performance compared to using all instances. By
acquiring words from predicted closely coupled speech-gaze instances, the MAP is increased
(t = 1.81, p < 0.037) although this improvement is less significant than the one with Model2t.
266

fiContext-Based Word Acquisition for Situated Dialogue in a Virtual World

1

0.56

all 1-best
all transcript
predicted 1-best
predicted transcript

0.8

0.52
Mean Average Precision

0.9

Precision

0.7
0.6
0.5
0.4
0.3
0.2

1-best
transcript

0.48

0.44

0.4

0.36

0.1
0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

0.32

1

all

predicted

Recall

(a) Precision vs Recall

(b) Mean Average Precision

Figure 9: Word acquisition performance by Model-2t on speech recognition and transcript
1

0.56

all 1-best
all transcript
predicted 1-best
predicted transcript

0.8

0.52
Mean Average Precision

0.9

Precision

0.7
0.6
0.5
0.4
0.3
0.2

1-best
transcript

0.48

0.44

0.4

0.36

0.1
0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0.32
all

predicted

Recall

(a) Precision vs Recall

(b) Mean Average Precision

Figure 10: Word acquisition performance by Model-2t-r on speech recognition and transcript

8.3.4 Comparison of Results Based on Speech Recognition and Transcript
To show the effect of speech recognition quality on word acquisition, we also compare the
acquisition performances based on speech transcript and the 1-best recognition. When word
acquisition is based on speech transcript, the word sequence in the parallel speech-gaze data
set contains nouns and adjectives in the speech transcript. Accordingly, the speech feature
used for coupled speech-gaze identification is extracted from the speech transcript.
Figures 9 & 10 show the word acquisition performances of Model-2t and Model-2t-r using
all instances and using only predicted coupled instances based on speech transcript and the
1-best recognition respectively. As shown in the figures, the quality of speech recognition is
critical to word acquisition performance. As expected, word acquisition performance based
on speech transcript is much better than on recognized speech.
267

fiQu & Chai

9. Examples
Table 2 shows the 10-best candidate words acquired for the entity couch by Model-1, Model2t, and Model-2t-r based on all speech-gaze instances and Model-2t-r based on predicted
closely coupled instances. The probabilities of these candidate words are also given in the
table. Across all these models, although the same four words (shown in bold font) are
acquired by each model, the ranking of the acquired words achieves the best by Model-2t-r
based on predicted closely coupled instances.
Table 3 shows another example of the 10-best candidate words acquired for the entity
stool by the four different models. Model-1 acquires four correct words in the 10-best list.
Although Model-2t also acquires four correct words in the 10-best list, the rankings of
these words are higher. With both speech-gaze temporal alignment and domain semantic
relatedness rescoring, Model-2t-r acquires seven correct words in the 10-best list. The
rankings of these correct words are also improved. Compared to using all instances with
Model-2t-r, although using the predicted coupled instances with Model-2t-r results in the
same seven correct words with the same ranks in the 10-best list, the probabilities of these
correctly acquired words are higher. This means that the results based on the predicted
coupled instances are more confident.
Model
Rank 1
Rank 2
Rank 3
Rank 4
Rank 5
Rank 6
Rank 7
Rank 8
Rank 9
Rank 10

Model-1
couch(0.1105)
bedroom(0.1047)
chair(0.1004)
bad(0.0936)
room(0.0539)
wooden(0.0354)
bench(0.0319)
small(0.0289)
yellow(0.0274)
couple(0.0270)

Model-2t
couch(0.1224)
chair(0.0798)
bed(0.0593)
small(0.0536)
room(0.0528)
bad(0.0489)
yellow(0.0333)
bench(0.0332)
lot(0.0331)
wooden(0.0226)

Model-2t-r
couch(0.4743)
chair(0.1668)
bench(0.0949)
bed(0.0311)
small(0.0235)
bad(0.0226)
room(0.0174)
lot(0.0151)
yellow(0.0107)
couple(0.0085)

Model-2t-r(predicted)
couch(0.4667)
chair(0.1557)
bench(0.11129)
bed(0.0368)
small(0.0278)
bad(0.0265)
room(0.0137)
yellow(0.0127)
couple(0.0101)
lot(0.0090)

Table 2: N-best candidate words acquired for the entity couch by different models

Model
Rank 1
Rank 2
Rank 3
Rank 4
Rank 5
Rank 6
Rank 7
Rank 8
Rank 9
Rank 10

Model-1
plant(0.0793)
room(0.0508)
little(0.0471)
flower(0.0424)
stairs(0.0320)
call(0.0319)
square(0.0302)
footstool(0.0301)
brown(0.0300)
short(0.0294)

Model-2t
plant(0.0592)
room(0.0440)
little(0.0410)
flower(0.0409)
square(0.0408)
small(0.0403)
next(0.0308)
stool(0.0307)
brown(0.0300)
stairs(0.0226)

Model-2t-r
stool(0.1457)
little(0.1435)
small(0.1412)
footstool(0.0573)
ottoman(0.0572)
ground(0.0275)
media(0.0263)
chair(0.0257)
plant(0.0253)
square(0.0234)

Model-2t-r(predicted)
stool(0.1532)
little(0.1509)
small(0.1490)
footstool(0.0602)
ottoman(0.0601)
ground(0.0289)
media(0.0276)
chair(0.0272)
plant(0.0270)
square(0.0247)

Table 3: N-best candidate words acquired for the entity stool by different models

268

fiContext-Based Word Acquisition for Situated Dialogue in a Virtual World

10. The Effect of Online Word Acquisition on Language Understanding
One important goal of word acquisition is to use the acquired new words to help language
understanding in subsequent conversation. To demonstrate the effect of online word acquisition on language understanding, we conduct simulation studies based on our collected
data. In these simulations, the system starts with an initial knowledge base  a vocabulary
of words associated to domain concepts. The system continuously enhances its knowledge
base by acquiring words from users with Model-2t-r that incorporates both speech-gaze
temporal information and domain semantic relatedness. The enhanced knowledge base is
used to understand the language of new users.
We evaluate language understanding performance on concept identification rate (CIR):
CIR =

#correctly identified concepts in the 1-best speech recognition
#concepts in the speech transcript

We simulate the process of online word acquisition and evaluate its effect on language
understanding for two situations: 1) the system starts with no training data but with a
small initial vocabulary, and 2) the system starts with some training data.
10.1 Simulation 1: When the System Starts with No Training Data
To build conversational systems, one approach is that domain experts provide domain vocabulary to the system at design time. Our first simulation follows this practice. The
system is provided with a default vocabulary to start without training data. The default
vocabulary contains one seed word for each domain concept.
Using the collected data of 20 users, the simulation process goes through the following
steps:
 For user index i = 1, 2, . . . , 20:
 Evaluate CIR of the i-th users utterances (1-best speech recognition) with the
current system vocabulary.
 Acquire words from all the instances (with 1-best speech recognition) of users
1    i.
 Among the 10-best acquired words, add verified new words to the system vocabulary.
In the above process, the language understanding performance on each individual user
depends on the users own language as well as the users position in the user sequence.
To reduce the effect of user ordering on language understanding performance, the above
simulation process is repeated 1000 times with randomly ordered users. The average of the
CIRs in these simulations is shown in Figure 11.
Figure 11 also shows the CIRs when the system is with a static knowledge base (vocabulary). The curve is drawn in the same way as the curve with a dynamic knowledge
base, except without word acquisition in the random simulation processes. As we can see in
the figure, when the system doest not have word acquisition capability, its language understanding performance does not change after more users have communicated to the system.
With the capability of automatic word acquisition, the systems language understanding
performance becomes better after more users have talked to the system.
269

fiQu & Chai

0.7
static knowledge base
dynamic knowledge base

0.65
0.6

CIR

0.55
0.5
0.45
0.4
0.35
0.3
0.25

1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19 20
User Index

Figure 11: CIR of user language achieved by the system starting with no training data
10.2 Simulation 2: When the System Starts with Training Data
Many conversational systems use real user data to derive domain vocabulary. To follow this
practice, the second simulation provides the system with some training data. The training
data serves two purposes: 1) build an initial vocabulary of the system; 2) train a classifier
to predict the closely coupled speech-gaze instances of new users data.
Using the collected data of 20 users, the simulation process goes through the following
steps:
 Using the first m users data as training data, acquire words from the training instances
(with speech transcript); add the verified 10-best words to the systems vocabulary as
seed words; build a classifier with the training data for prediction of closely coupled
speech-gaze instances.
 Evaluate the effect of incremental word acquisition on CIR of the remaining (20-m)
users data. For user index i = 1, 2, . . . , (20-m):
 Evaluate CIR of the i-th users utterances (1-best speech recognition).
 Predict closely coupled speech-gaze instances of the i-th users data.
 Acquire words from the m training users true coupled instances (with speech
transcript) and the predicted coupled instances (with 1-best speech recognition)
of users 1    i.
 Among the 10-best acquired words, add verified new words to the system vocabulary.
The above simulation process is repeated 1000 times with randomly ordered users to
reduce the effect of user ordering on the language understanding performance. Figure 12
shows the averaged language understanding performance of these random simulations.
The language understanding performance of the system with a static knowledge base
is also shown in Figure 12. The curve is drawn by the same random simulations without
the steps of word acquisition. We can observe a general trend in the figure that, with word
acquisition, the systems language understanding becomes better after more users have
270

fiContext-Based Word Acquisition for Situated Dialogue in a Virtual World

0.6
static knowledge base
dynamic knowledge base
0.59

CIR

0.58

0.57

0.56

0.55

1

2

3

4

5

6

7

8

9

10

User Index

Figure 12: CIR of user language achieved by the system starting with training data of 10
users

communicated to the system. Without word acquisition capability, the systems language
understanding performance does not increase after more users have conversed with the
system.
The simulations show that automatic vocabulary acquisition is beneficial to the systems
language understanding performance when training data is available. When training data
is not available, vocabulary acquisition could be more important and beneficial to robust
language understanding.
10.3 The Effect of Speech Recognition on Online Word Acquisition and
Language Understanding
The simulation results in Figures 11 & 12 are based on the 1-best recognized speech hypotheses with a relatively high WER (48.1%). With better speech recognition, the system
will have better concept identification performance. To show the effect of speech recognition
quality on online word acquisition and language understanding, we also perform Simulation 1 and Simulation 2 based on speech transcript. The simulation processes are the same
as the ones based on the 1-best speech recognition except that word acquisition is based on
speech transcript and CIR is evaluated also on speech transcript in the new simulations.
Figure 13 shows the CIR curves based on speech transcript during online conversation.
With word acquisition, the systems language understanding becomes better after more
users have communicated to the system. This is consistent with the CIR curves based on
the 1-best speech recognition. However, the CIRs based on speech transcript is much higher
the CIRs based on the 1-best speech recognition, which verifies that speech recognition
quality is critical to language understanding performance.

11. Discussion and Future Work
Our experimental results have shown that incorporating extra information improves word
acquisition compared to completely unsupervised approaches. However, our current ap271

fiQu & Chai

1

0.95
static knowledge base
dynamic knowledge base

static knowledge base
dynamic knowledge base

0.8

0.93
CIR

0.94

CIR

0.9

0.7

0.92

0.6

0.91

0.5

1

2

3

4

5

6

7

8

0.9

9 10 11 12 13 14 15 16 17 18 19 20
User Index

(a) Simulation 1: with no training data

1

2

3

4

5

6

7

8

9

10

User Index

(b) Simulation 2: with training data of 10 users

Figure 13: CIR of user language (transcript) achieved by the system during online conversation

proaches have several limitations. First, the incorporation of domain knowledge through
semantic relatedness based on WordNet will restrict the acquired words to those appear in
WordNet. This is certainly not desirable. But this limitation can be readily addressed by
changing the way how the word probability distribution is tailored by semantic relatedness
(in Section 6.3.1 and Section 6.3.2). For example, one simple way is to keep the probability
mass for those words not in WordNet and only tailor the distribution from those words that
occur in WordNet based on their semantic relatedness with the object.
Second, in our current approach, acquired words are limited to the words that are
recognized by the speech recognizer. As shown in Section 8.3.4, the speech recognition
performance is rather poor in our experiments. This is partly due to the lack of language
models specifically trained for this domain. Approaches to improve speech recognition,
for example, based on a referential semantic language model described in (Schuler, Wu, &
Schwartz, 2009) will potentially improve acquisition performance. Furthermore, the set of
acquired words is bounded by the vocabulary of the speech recognizer. Any new words that
are not in the dictionary will not be acquired. To break this barrier, inspired by previous
work (Yu & Ballard, 2004; Taguchi et al., 2009), we are currently extending our approach
to incorporate grounding acoustic phoneme sequences to domain concepts.
Another limitation of our current approaches is that they are incapable of acquiring
multiword expressions. They can only map single words to domain concepts. However,
we did observe multiword expressions (e.g., Rubiks cube) in our data. We will examine
this issue in our future work by incorporating more linguistic knowledge and by modeling
fertility of entities, for example, as in IBM Model 3 and 4.
The simplicity of our current models also limits word acquisition performance. For
example, the alignment model based on temporal information directly incorporates findings
from psycholinguistic studies. These studies were generally conducted in a much simpler
settings without interaction. The recent work by Fang, Chai, and Ferreira (2009) has shown
correlations between intensity of gaze fixations and objects denoted by linguistic centers
272

fiContext-Based Word Acquisition for Situated Dialogue in a Virtual World

(e.g., forward-looking centers based on centering theory, Grosz, Joshi, & Weinstein, 1995).
We plan to incorporate these results to improve alignment modeling in the future.
To further improve performance, another interesting direction is to take into consideration of the interactive nature of conversation, for example, by combining dialog management
to solicit user feedback on the acquired words. However, it will be important to identify
strategies to balance the trade off between explicit feedback solicitation (and thus lengthening the interaction) and the quality of the acquired words. Reinforcement learning can
be a potential approach to address this problem.

12. Conclusions
Motivated by the psycholinguistic findings, we investigate the use of eye gaze for automatic word acquisition in multimodal conversational systems. This paper presents several
enhanced models that incorporate user language behavior, domain knowledge, and conversation context in word acquisition. Our experiments have shown that these enhanced
models significantly improve word acquisition performance.
Recent advancement in eye tracking technology has made available non-intrusive eye
tracking devices that can tolerate head motion and provide high tracking quality. Integrating eye tracking with conversational interfaces is no longer beyond reach. We believe that
incorporating eye gaze with automatic word acquisition provides another potential approach
to improve the robustness of human-machine conversation.

Acknowledgments
This work was supported by IIS-0347548 and IIS-0535112 from the National Science Foundation. We would like to thank anonymous reviewers for their valuable comments and
suggestions.

References
Allopenna, P. D., Magnuson, J. S., & Tanenhaus, M. K. (1998). Tracking the time course
of spoken word recognition using eye movements: Evidence for continuous mapping
models. Journal of Memory & Language, 38, 419439.
Barnard, K., Duygulu, P., de Freitas, N., Forsyth, D., Blei, D., & Jordan, M. (2003). Matching words and pictures. Journal of Machine Learning Research, 3, 11071135.
Bock, K., Irwin, D. E., Davidson, D. J., & Leveltb, W. (2003). Minding the clock. Journal
of Memory and Language, 48, 653685.
Brown, P. F., Pietra, S. D., Pietra, V. J. D., & Mercer, R. L. (1993). The mathematic
of statistical machine translation: Parameter estimation. Computational Linguistics,
19 (2), 263311.
Brown-Schmidt, S., & Tanenhaus, M. K. (2006). Watching the eyes when talking about size:
An investigation of message formulation and utterance planning. Journal of Memory
and Language, 54, 592609.
273

fiQu & Chai

Byron, D., Mampilly, T., Sharma, V., & Xu, T. (2005). Utilizing visual attention for
cross-modal coreference interpretation. In Proceedings of the Fifth International and
Interdisciplinary Conference on Modeling and Using Context (CONTEXT-05), pp.
8396.
Campana, E., Baldridge, J., Dowding, J., Hockey, B., Remington, R., & Stone, L. (2001).
Using eye movements to determine referents in a spoken dialogue system. In Proceedings of the Workshop on Perceptive User Interface.
Cessie, S. L., & Houwelingen, J. V. (1992). Ridge estimators in logistic regression. Applied
Statistics, 41 (1), 191201.
Chen, D. L., & Mooney, R. J. (2008). Learning to sportscast: A test of grounded language
acquisition. In Proceedings of 25th International Conference on Machine Learning
(ICML).
Cooke, N. J. (2006). Gaze-Contigent Automatic Speech Recognition. Ph.D. thesis, University
of Birminham.
Dahan, D., & Tanenhaus, M. K. (2005). Looking at the rope when looking for the snake:
Conceptually mediated eye movements during spoken-word recognition. Psychonomic
Bulletin & Review, 12 (3), 453459.
DeVault, D., & Stone, M. (2003). Domain inference in incremental interpretation. In
Proceedings of ICoS.
Eberhard, K., Spivey-Knowiton, M., Sedivy, J., & Tanenhaus, M. (1995). Eye movements as
a window into real-time spoken language comprehension in natural contexts. Journal
of Psycholinguistic Research, 24, 409436.
Fang, R., Chai, J. Y., & Ferreira, F. (2009). Between linguistic attention and gaze fixations
inmultimodal conversational interfaces. In Proceedings of the International Conference
on Multimodal Interfaces (ICMI), pp. 143150.
Fazly, A., Alishahi, A., & Stevenson, S. (2008). A probabilistic incremental model of word
learning in the presence of referential uncertainty. In Proceedings of the 30th Annual
Conference of the Cognitive Science Society.
Fellbaum, C. (Ed.). (1998). WordNet: An Electronic Lexical Database. MIT Press.
Fleischman, M., & Roy, D. (2005). Intentional context in situated language learning.
In Proceedings of the 9th Conference on Computational Natural Language Learning
(CoNLL).
Fong, T. W., & Nourbakhsh, I. (2005). Interaction challenges in human-robot space exploration. Interactions, 12 (2), 4245.
Gorniak, P., & Roy, D. (2004). Grounded semantic composition for visual scenes. Journal
of Artificial Intelligence Research, 21, 429470.
Griffin, Z., & Bock, K. (2000). What the eyes say about speaking. Psychological Science,
11, 274279.
Griffin, Z. M. (2001). Gaze durations during speech reflect word selection and phonological
encoding. Cognition, 82, B1B14.
274

fiContext-Based Word Acquisition for Situated Dialogue in a Virtual World

Griffin, Z. M. (2004). Why look? Reasons for eye movements related to language production.
In Henderson, J., & Ferreira, F. (Eds.), The Interface of Language, Vision, and Action:
Eye Movements and the Visual World, pp. 213248. Taylor and Francis.
Grosz, B. J., Joshi, A. K., & Weinstein, S. (1995). Centering: A framework for modeling
the local coherence of discourse. Computational Linguistics, 21 (2), 203226.
Jacob, R. J. K. (1991). The use of eye movements in human-computer interaction techniques:
What you look at is what you get. ACM Transactions on Information Systems, 9 (3),
152169.
Kahneman, D. (1973). Attention and Effort. Prentice-Hall, Inc., Englewood Cliffs.
Kaur, M., Termaine, M., Huang, N., Wilder, J., Gacovski, Z., Flippo, F., & Mantravadi,
C. S. (2003). Where is it? event synchronization in gaze-speech input systems. In
Proceedings of the International Conference on Multimodal Interfaces (ICMI).
Klemmer, S., Sinha, A., Chen, J., Landay, J., Aboobaker, N., & Wang, A. (2000). SUEDE:
A wizard of oz prototyping tool for speech user interfaces. In Proceedings of ACM
Symposium on User Interface Software and Technology, pp. 110.
Lemon, O., Gruenstein, A., & Peters, S. (2002). Collaborative activities and multitasking
in dialogue systems. Traitement Automatique des Langues, 43 (2), 131154.
Liang, P., Jordan, M. I., & Klein, D. (2009). Learning semantic correspondences with
less supervision. In Proceedings of the 47th Annual Meeting of the Association for
Computational Linguistics (ACL).
Liu, Y., Chai, J., & Jin, R. (2007). Automated vocabulary acquisition and interpretation
in multimodal conversational systems. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics (ACL).
Meyer, A., Sleiderink, A., & Levelt, W. (1998). Viewing and naming objects: eye movements
during noun phrase production. Cognition, 66 (22), 2533.
Nakano, Y., Reinstein, G., Stocky, T., & Cassell, J. (2003). Towards a model of face-to-face
grounding. In Proceedings of the Annual Meeting of the Association for Computational
Linguistics (ACL).
Pedersen, T., Patwardhan, S., & Michelizzi, J. (2004). WordNet::Similarity - measuring
the relatedness of concepts. In Proceedings of the Nineteenth National Conference on
Artificial Intelligence (AAAI).
Prasov, Z., & Chai, J. Y. (2008). Whats in a gaze? the role of eye-gaze in reference resolution
in multimodal conversational interfaces. In Proceedings of ACM 12th International
Conference on Intelligent User interfaces (IUI).
Qu, S., & Chai, J. Y. (2007). An exploration of eye gaze in spoken language processing for
multimodal conversational interfaces. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational
Linguistics (HLT-NAACL), pp. 284291.
Qu, S., & Chai, J. Y. (2008). Incorporating temporal and semantic information with eye gaze
for automatic word acquisition in multimodal conversational systems. In Proceedings
275

fiQu & Chai

of the Conference on Empirical Methods in Natural Language Processing (EMNLP),
pp. 244253.
Qu, S., & Chai, J. Y. (2009). The role of interactivity in human-machine conversation for
automatic word acquisition. In Proceedings of the 10th Annual Meeting of the Special
Interest Group on Discourse and Dialogue (SIGDIAL), pp. 188195.
Qvarfordt, P., & Zhai, S. (2005). Conversing with the user based on eye-gaze patterns. In
Proceedings of the Conference on Human Factors in Computing Systems (CHI).
Rayner, K. (1998). Eye movements in reading and information processing - 20 years of
research. Psychological Bulletin, 124 (3), 372422.
Roy, D. (2002). Learning visually-grounded words and syntax for a scene description task.
Computer Speech and Language, 16 (3), 353385.
Roy, D., & Pentland, A. (2002). Learning words from sights and sounds, a computational
model. Cognitive Science, 26 (1), 113146.
Schuler, W., Wu, S., & Schwartz, L. (2009). A framework for fast incremental interpretation
during speech decoding. Computational Linguistics, 35 (3), 313343.
Siskind, J. M. (2001). Grounding the lexical semantics of verbs in visual perception using
force dynamics and event logic. Journal of Artificial Intelligence Research, 15, 3190.
Spivey, M. J., Tanenhaus, M. K., Eberhard, K. M., & Sedivy, J. C. (2002). Eye movements
and spoken language comprehension: Effects of visual context on syntactic ambiguity
resolution. Cognitive Psychology, 45, 447481.
Taguchi, R., Iwahashi, N., Nose, T., Funakoshi, K., & Nakano, M. (2009). Learning lexicons from spoken utterances based on statistical model selection. In Proceedings of
Interspeech.
Tanaka, K. (1999). A robust selection system using real-time multi-modal user-agent interactions. In Proceedings of the International Conference on Intelligent User Interfaces
(IUI).
Tanenhaus, M., Spivey-Knowiton, M., Eberhard, K., & Sedivy, J. (1995). Integration of
visual and linguistic information in spoken language comprehension. Science, 268,
16321634.
Toutanova, K., Klein, D., Manning, C., & Singer, Y. (2003). Feature-Rich Part-of-Speech
tagging with a cyclic dependency network. In Proceedings of the Human Language
Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL), pp. 252259.
Toutanova, K., & Manning, C. D. (2000). Enriching the knowledge sources used in a maximum entropy part-of-speech tagger. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora
(EMNLP/VLC), pp. 6370.
Traum, D., & Rickel, J. (2002). Embodied agents for multiparty dialogue in immersive
virtual worlds. In Proceedings of the 1st international joint conference on Autonomous
Agents and Multi-Agent Systems.
276

fiContext-Based Word Acquisition for Situated Dialogue in a Virtual World

Wang, J. (1995). Integration of eye-gaze, voice and manual response in multimodal user
interfaces. In Proceedings of IEEE International Conference on Systems, Man and
Cybernetics, pp. 39383942.
Yu, C., & Ballard, D. (2004). A multimodal learning interface for grounding spoken language
in sensory perceptions. ACM Transactions on Applied Perceptions, 1 (1), 5780.
Zhai, S., Morimoto, C., & Ihde, S. (1999). Manual and gaze input cascaded (MAGIC)
pointing. In Proceedings of the Conference on Human Factors in Computing Systems
(CHI), pp. 246253.
Zhang, Q., Imamiya, A., Go, K., & Mao, X. (2004). Overriding errors in a speech and gaze
multimodal architecture. In Proceedings of the International Conference on Intelligent
User Interfaces (IUI).

277

fiJournal of Artificial Intelligence Research 37 (2010) 1-39

Submitted 07/09; published 01/10

Text Relatedness Based on a Word Thesaurus
George Tsatsaronis

GBT @ IDI . NTNU . NO

Department of Computer and Information Science
Norwegian University of Science and Technology, Norway

Iraklis Varlamis

VARLAMIS @ HUA . GR

Department of Informatics and Telematics
Harokopio University, Greece

Michalis Vazirgiannis

MVAZIRG @ AUEB . GR

Department of Informatics
Athens University of Economics and Business, Greece

Abstract
The computation of relatedness between two fragments of text in an automated manner requires
taking into account a wide range of factors pertaining to the meaning the two fragments convey,
and the pairwise relations between their words. Without doubt, a measure of relatedness between
text segments must take into account both the lexical and the semantic relatedness between words.
Such a measure that captures well both aspects of text relatedness may help in many tasks, such as
text retrieval, classification and clustering. In this paper we present a new approach for measuring
the semantic relatedness between words based on their implicit semantic links. The approach exploits only a word thesaurus in order to devise implicit semantic links between words. Based on
this approach, we introduce Omiotis, a new measure of semantic relatedness between texts which
capitalizes on the word-to-word semantic relatedness measure (SR) and extends it to measure the
relatedness between texts. We gradually validate our method: we first evaluate the performance
of the semantic relatedness measure between individual words, covering word-to-word similarity and relatedness, synonym identification and word analogy; then, we proceed with evaluating
the performance of our method in measuring text-to-text semantic relatedness in two tasks, namely
sentence-to-sentence similarity and paraphrase recognition. Experimental evaluation shows that the
proposed method outperforms every lexicon-based method of semantic relatedness in the selected
tasks and the used data sets, and competes well against corpus-based and hybrid approaches.

1. Introduction
Relatedness between texts can be perceived in several different ways. Primarily, one can think of
lexical relatedness or similarity between texts, which can be easily captured by a vectorial representation of texts (van Rijsbergen, 1979) and a standard similarity measure, like Cosine, Dice (Salton
& McGill, 1983), and Jaccard (1901). Such models have had high impact in information retrieval
over the past decades. Several improvements have been proposed for such techniques towards inventing more sophisticated weighting schemes for the text words, like for example TF-IDF and its
variations (Aizawa, 2003). Other directions explore the need to capture the latent semantic relations between dimensions (words) in the created vector space model, by using techniques of latent
semantic analysis (Landauer, Foltz, & Laham, 1998). Another aspect of text relatedness, probably
of equal importance, is the semantic relatedness between two text segments. For example, the sentences The shares of the company dropped 14 cents, and The business institutions stock slumped
14 cents have an obvious semantic relatedness, which traditional measures of text similarity fail
c
2010
AI Access Foundation. All rights reserved.

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

to recognize. The motivation of this work is to show that a measure of relatedness between texts,
which takes into account both the lexical and the semantic relatedness of word elements, performs
better than the traditional lexical matching models, and can handle cases like the one above.
In this paper we propose Omiotis1 , a new measure of semantic relatedness between texts, which
extends SR, a measure of semantic relatedness between words. The word-to-word relatedness measure, in its turn, is based on the construction of semantic links between individual words, according
to a word thesaurus, which in our case is WordNet (Fellbaum, 1998). Each pair of words is potentially connected via one or more semantic paths, each one comprising one or more semantic
relations (edges) that connect intermediate thesaurus concepts (nodes). For weighting the semantic
path we consider three key factors: (a) the semantic path length, (b) the intermediate nodes specificity denoted by the node depth in the thesaurus hierarchy, and (c) the types of the semantic edges
that compose the path. This triptych allows our measure to perform well in complex linguistic tasks,
that require more than simple similarity, such as the SAT Analogy Test2 that is demonstrated in the
experiments. To the best of our knowledge, Omiotis is the first measure of semantic relatedness
between texts that considers in tandem all three factors for measuring the pairwise word-to-word
semantic relatedness scores. Omiotis integrates the semantic relatedness in word level with words
statistical information in the text level to provide the final semantic relatedness score between texts.
The contributions of this work can be summarized in the following: 1) a new measure for computing semantic relatedness between words, namely SR, which exploits all of the semantic information a thesaurus can offer, including semantic relations crossing parts of speech (POS), while taking
into account the relation weights and the depth of the thesaurus nodes; 2) a new measure for computing semantic relatedness between texts, namely Omiotis, that does not require the use of external
corpora or learning methods, supervised or unsupervised, 3) thorough experimental evaluation on
benchmark data sets for measuring the performance on word-to-word similarity and relatedness
tasks, as well as on word analogy; in addition, experimental evaluation on two text related tasks
(sentence-to-sentence similarity and paraphrase recognition) for measuring the performance of our
text-to-text relatedness measure. Additional contributions of this work are: a) the use of all semantic relations offered by WordNet, which increases the chances of finding a semantic path between
any two words, b) the availability of pre-computed semantic relatedness scores between every pair
of WordNet senses, which accelerates computation of semantic relatedness between texts and facilitates the incorporation of semantic relatedness in several applications (Tsatsaronis, Varlamis,
Nrvag, & Vazirgiannis, 2009; Tsatsaronis & Panagiotopoulou, 2009).
The rest of the paper is organized as follows: Section 2 discusses preliminary concepts regarding
word thesauri, semantic network construction, and semantic relatedness or similarity measures, and
summarizes related work on these fields. Section 3 presents the key contributions of our work.
Section 4 provides the experimental evaluation and the analysis of the results. Finally, Section 5
presents our conclusions and the next steps of our work.

2. Preliminaries and Related Work
Our approach capitalizes on a word thesaurus in order to define a measure of semantic relatedness
between words, and expands this measure to compute text relatedness using both semantic and
1. Omiotis is the Greek word for relatedness or similarity.
2. http://www.aclweb.org/aclwiki/index.php?title=SAT_Analogy_Questions

2

fiT EXT R ELATEDNESS BASED ON A W ORD T HESAURUS

lexical information. In order to facilitate the understanding of our methodology we elaborate on
preliminary concepts in this section and present related research approaches.
2.1 Word Thesauri and their use in Text Applications
Word thesauri, like WordNet (Fellbaum, 1998) or Rogets International Thesaurus (Morris & Hirst,
1991), constitute the knowledge base for several text-related research tasks. WordNet has been
used successfully as a knowledge base in the construction of Generalized Vector Space Models
(GVSM) and semantic kernels for document similarity with application to text classification, such
as the works of Mavroeidis, Tsatsaronis, Vazirgiannis, Theobald and Weikum (2005), and Basili,
Cammisa and Moschitti (2005), and text retrieval, such as the works of Voorhees (1993), Stokoe,
Oakes and Tait (2003), and our previous work regarding the definition of a new GVSM that uses
word-to-word semantic relatedness (Tsatsaronis & Panagiotopoulou, 2009). Furthermore, the idea
of using a thesaurus as a knowledge base in text retrieval has also been proven successful in the case
of cross language information retrieval, like for example in the case of the CLIR system introduced
by Clough and Stevenson (2004). Finally, the exploitation of word thesauri in linguistic tasks,
such as Word Sense Disambiguation (WSD) (Ide & Veronis, 1998) has yielded interesting results
(Mihalcea & Moldovan, 1999; Tsatsaronis, Vazirgiannis, & Androutsopoulos, 2007; Tsatsaronis,
Varlamis, & Vazirgiannis, 2008).
The application of a text relatedness measure to text classification and retrieval tasks should
first consider the impact of lexical ambiguity and WSD in the overall performance in these tasks.
Sanderson (1994, 2008) concludes that ambiguity in words can take many forms, but new test collections are needed to realize the true importance of resolving ambiguity and embedding semantic
relatedness and sense disambiguation in the text retrieval task. In the analysis of Barzilay and Elhadad (1997), and Barzilay, Elhadad and McKeown (2002) the impact of WSD in the performance
of text summarization tasks is addressed by considering all possible interpretations of the lexical
chains created from text. Similar to this methodology, we tackle word ambiguity by taking into account every possible type of semantic information that the thesaurus can offer, for any given sense
of a text word.
From the aforementioned approaches, it is clear that the use of a word thesaurus can offer much
potential in the design of models that capture the semantic relatedness between texts, and consequently, it may improve the performance of existing retrieval and classification models under certain
circumstances that are discussed in the respective research works (Mavroeidis et al., 2005; Basili
et al., 2005; Stokoe et al., 2003; Clough & Stevenson, 2004). The word thesaurus employed in the
development of Omiotis is WordNet. Its lexical database contains English nouns, verbs, adjectives
and adverbs, organized in sets of synonym senses (synsets). Hereafter, the terms senses, synsets and
concepts are used interchangeably. Synsets are connected with various links that represent semantic
relations between them (i.e., hypernymy / hyponymy, meronymy / holonymy, synonymy / antonymy,
entailment / causality, troponymy, domain / domain terms, derivationally related forms, coordinate
terms, attributes, stem adjectives, etc.). Several relations cross parts of speech, like the domain
terms relation, which connects senses pertaining to the same domain (e.g., light, as a noun meaning electromagnetic radiation producing a visual sensation, belongs to the domain of physics). To
the best of our knowledge, the proposed approach is the first that utilizes all of the aforementioned
semantic relations that exist in WordNet for the construction of a semantic relatedness measure.
3

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

2.2 Creating Semantic Networks from Word Thesauri
Omiotis is based on the creation of semantic paths between words in a text using the thesaurus
concepts and relations. Early approaches in this field, used gloss words from the respective word
definitions in order to build semantic networks from text (Veronis & Ide, 1990). The idea of representing text as a semantic network was initially introduced by Quilian (1969). The expansion
of WordNet with semantic relations that cross parts of speech has added more possibilities of semantic network construction from text. More recent approaches to semantic network construction
from word thesauri, by Mihalcea, Tarau and Figa (2004) and Navigli (2008), utilize a wide range
of WordNet semantic relations instead of the gloss words. These methods outperformed previous
methods that used semantic networks in the all words WSD tasks of Senseval 2 and 3 for the English language (Palmer, Fellbaum, & Cotton, 2001; Snyder & Palmer, 2004). In this work we adopt
the semantic network construction method that we introduced in the past (Tsatsaronis et al., 2007).
The method utilizes all of the available semantic relations in WordNet. In the WSD task, the respective method outperformed or matched previous methods that used semantic networks in the all
words WSD tasks of Senseval 2 and 3 for the English language, and this was largely due to the rich
representation that the semantic networks offered. Section 3.1 introduces our semantic relatedness
measure.
2.3 Measures of Semantic Relatedness
Semantic relatedness between words or concepts has been exploited, in the past, in text summarization (Barzilay et al., 2002), text retrieval (Stokoe et al., 2003; Smeaton, Kelledy, & ODonnell,
1995; Richardson & Smeaton, 1995) and WSD (Patwardhan, Banerjee, & Pedersen, 2003) tasks.
Semantic relatedness measures can be widely classified to dictionary-based3 , corpus-based and hybrid.
Among dictionary-based measures, the measure of Agirre and Rigau (1995) was one of the first
measures developed to compute semantic relatedness between two or more concepts (i.e., for a set
of concepts). Their measure was based on the density and depth of concepts in the set and on the
length of the shortest path that connects them. However, they assume that all edges in the path are
equally important.
The measure proposed by Leacock, Miller and Chodorow (1998) for computing the semantic
similarity between a pair of concepts takes into account the length of the shortest path connecting
them, measured as the number of nodes participating in the path, and the maximum depth of the
taxonomy. The measure for two concepts s1 and s2 can be computed as follows:
Sim(s1 , s2 ) = log

length
2D

(1)

where length is the length of the shortest path connecting s1 and s2 and D is the maximum depth of
the taxonomy used.
Regarding hybrid measures, Resniks (1995, 1999) measure for pairs of concepts is based on the
Information Content (IC) of the deepest concept that can subsume both (least common subsumer),
and can be considered as a hybrid measure, since it combines both the hierarchy of the used thesaurus, and statistical information for concepts measured in large corpora. More specifically, the
3. Also found in the bibliography as knowledge-based, thesaurus-based, or lexicon-based.

4

fiT EXT R ELATEDNESS BASED ON A W ORD T HESAURUS

semantic similarity for a given pair of concepts s1 and s2 , which have s0 as their least common
subsumer (i.e., least common ancestor), is defined in the following equation:
Sim(s1 , s2 ) = IC(s0 )

(2)

where the Information Content (IC) of a concept (i.e., s0 ) is defined as:
IC(s0 ) = logP (s0 )

(3)

and P (s0 ) is the probability of occurrence of the concept s0 in a large corpus.
The measure proposed by Jiang and Conrath (1997), is also based on the concept of IC. Given
two concepts s1 and s2 , and their least common subsumer s0 , their semantic similarity is defined as
follows:
1
Sim(s1 , s2 ) =
(4)
IC(s1 ) + IC(s2 )  2  IC(s0 )
The measure of Lin (1998) is also based on IC. Given, again, s1 , s2 , and s0 , as before, the
similarity between s1 and s2 is defined as follows:
Sim(s1 , s2 ) =

2  IC(s0 )
IC(s1 ) + IC(s2 )

(5)

Hirst and St-Onge (1998) reexamine the idea of constructing lexical chains between words,
based on their synsets and the respective semantic edges that connect them in WordNet. The initial
idea of lexical chains was first introduced by Morris and Hirst (1991), who defined the lexical
cohesion of a passage, based on the cohesion of the lexical chains between the passages elements,
which acted as an indicator for the continuity of the passages lexical meaning.
We encourage the reader to consult the analysis of Budanitsky and Hirst (2006) for a detailed
discussion on most of the aforementioned measures, as well as for more measures proposed prior to
the aforementioned. While all these measures use only the noun hierarchy (except from the measure
of Hirst and St-Onge), the implementation of several of those measures provided by Patwardhan,
Banerjee and Pedersen (2003) in the publicly available WordNet::Similarity package can also utilize
the verb hierarchy. Still, the relations that cross parts of speech are not considered, as well as other
factors discussed in detail in Section 3. In contrast, our measure defines the semantic relatedness
between any two concepts, independently of their Part of Speech (POS), utilizing all of the available
semantic links offered by WordNet.
More recent works of interest on semantic relatedness, include: the measures of Jarmasz and
Szpakowicz (2003), who use Rogets thesaurus to compute semantic similarity, by replicating a
number of WordNet-based approaches, the LSA-based measure of Finkelstein et al. (2002), who
perform Latent Semantic Analysis (Landauer et al., 1998) to capture text relatedness and can be
considered as a corpus-based method, the measure of Patwardhan and Pedersen (2006), who utilize the gloss words found from the words definitions to create WordNet-based context vectors,
the methods of Strube and Ponzetto (2006, 2007a), Gabrilovich and Markovitch (2007), and Milne
and Witten (2008) who use Wikipedia to compute semantic relatedness and can also be considered
as corpus-based approaches, and the method of Mihalcea, Corley and Strappavara (2006), which
is a hybrid method that combines knowledge-based and corpus-based measures of text relatedness.
Other recent hybrid measures of semantic similarity are: the measure proposed by Li et al. (2006),
who use information from WordNet and corpus statistics collected from the Brown Corpus (Kucera,
5

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

Francis, & Caroll, 1967) to compute similarity between very short texts, and the measure for text distance proposed by Tsang (2008), that uses both distributional similarity and ontological/knowledge
information to compute the distance between text fragments. Distributional similarity is also used
in a supervised combination with WordNet-based approaches (Agirre, Alfonseca, Hall, Kravalova,
Pasca, & Soroa, 2009), to produce a supervised measure of semantic relatedness. Li et al. (2006)
have created a new data set for their experimental evaluation, which we also use in Section 4 to
evaluate our Omiotis measure and compare against their approach.
In the following section we formally define Omiotis and provide its details, from the creation of
the semantic links to the computation of relatedness between words and texts. We give evidence on
the measures complexity and justify our design choices. Finally, we discuss potential applications
of the measure on text related tasks.

3. Measuring Word-to-Word and Text-to-Text Semantic Relatedness
This section presents the details of Omiotis, our measure of text semantic relatedness. The measure
capitalizes on the idea of semantic relatedness between WordNet senses, extends it to compute
relatedness between words and finally between texts. Since the definition of semantic relatedness
ranges from pairs of keyword senses to pairs of texts, Omiotis is defined in a way that captures
relatedness in every granularity. As a result, it can be applied in a wide range of linguistic and
text related tasks such as WSD, word similarity and word analogy, text similarity, and keyword
ranking. The key points of the proposed measure are: (a) it constructs semantic links between
all word senses in WordNet and pre-computes a relatedness score between every pair of WordNet
senses, (b) it computes the semantic relatedness for a pair of words by taking into account the
relatedness of their corresponding WordNet senses, and (c) it computes a semantic relatedness score
for any two given text segments by extending word-to-word relatedness. Depending on the task, the
computation of semantic relatedness can be modified to take into account all or some of the senses
of each word, all or some of the words in each text, or to apply additional weights depending on
the word importance or sense importance in context. This allows Omiotis to be adapted in various
text related tasks, without modifying the main process of computing relatedness. In Section 3.1
that follows, we formally define our semantic relatedness measure and in Section 3.2 we provide a
detailed justification of our design decisions.
3.1 Construct Semantic Links between Words
The first step in measuring the semantic relatedness between two text fragments, is to find the
implicit semantic links between the words of the two fragments. Thus, we present a definition of
semantic relatedness for a pair of thesaurus concepts, which takes into account the semantic path
connecting the concepts, and expands it to measure the relatedness between words. In order to solve
the problem of constructing semantic paths between words, we base our approach on our previous
method on how to construct semantic networks between words (Tsatsaronis et al., 2007).
3.1.1 S EMANTIC N ETWORK C ONSTRUCTION FROM W ORD T HESAURI
Figure 1 gives an example of the construction of a semantic network for two words ti and tj . For
simplicity reasons, we assume the construction of a semantic path between senses S.i.2 and S.j.1
only (Initial Phase), though we could do the same for every possible combination of the two words
6

fiT EXT R ELATEDNESS BASED ON A W ORD T HESAURUS

ti

S.i.1

S.j.1

S.i.2

S.j.2

...

...

S.i.7

S.j.5

Synonym

...

Holonym

Meronym
S.i.2

tj

...

S.j.1

Hypernym
Antonym

...
Hyponym

Initial Phase
Index:

= Word Node

Network Expansion
= Sense Node

= Semantic Link

Figure 1: Constructing semantic networks from word thesauri.
senses. Initially, the two sense nodes are expanded using all the semantic links offered by WordNet.
The semantic links of the senses, as found in the thesaurus, become the edges and the pointed senses
the nodes of the network (Network Expansion). The expansion process is repeated recursively until
the shortest 4 path between S.i.2 and S.j.1 is found. When no path is found from S.i.2 to S.j.1 then
the senses and consequently the words are not semantically related.
3.1.2 S EMANTIC R ELATEDNESS

BETWEEN A

PAIR OF C ONCEPTS

The semantic relatedness for a pair of concepts is measured over the constructed semantic network.
It considers the path length, captured by compactness, and the path depth, captured by semantic
path elaboration, which are defined in the following. A measure for WSD based on the idea of
compactness was initially proposed by Mavroeidis et al. (2005). The original measure used only
nouns and the hypernym relation, and is extended in the current work to support all of WordNets
relations and the noun, verb and adjective parts of speech. Here we define a new compactness
measure (Definition 1) as the core of the Omiotis measure.
Definition 1 Given a word thesaurus O, a weighting scheme for the edges that assigns a weight
w  (0, 1) for each edge, a pair of senses S = (s1 , s2 ), and a path P of length l connecting
Qthe two
senses, the semantic compactness of S (SCM (S, O, P )) is defined as: SCM (S, O, P ) = li=1 wi ,
where w1 , w2 , ..., wl are the paths edges weights. If s1 = s2 then SCM (S, O, P ) = 1. If there is
no path between s1 and s2 then SCM (S, O, P ) = 0.
Note that compactness takes the path length into account and is bound in [0, 1]. Higher compactness
between senses implies higher semantic relatedness. The intuition behind edge types weighting is
that certain types provide stronger semantic connections than others. Considering that the lexicographers of WordNet tend to use some relation types more often than others (we assume that the most
used relation types are stronger than the types less used), a straightforward solution is to define edge
types weights in proportion to their frequency of occurrence in WordNet 2.0. The weights assigned
to each type using this solution are shown in Table 1 and are in accordance to those found by Song
et al. (2004). The table shows the probability of occurrence in WordNet 2.0 for every possible edge
type in the thesaurus, in descending order of probability values. A detailed analysis of the choices
we made in Definition 1 and in the definitions that follow is performed in Section 3.2.
The depth of nodes that belong to the path also affects term relatedness. A standard means of
measuring depth in a word thesaurus is the hypernym/hyponym hierarchical relation for the noun
and adjective POS and hypernym/troponym for the verb POS. For the adverb POS the related stem
4. The details are presented in Algorithm 1.

7

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

WordNet 2.0 Edge Type

Probability of Occurrence

hypernym/hyponym
nominalization
category domain
part meronym/holonym
region domain
similar
usage domain
member meronym/holonym
antonym
verb group
also see
attribute
entailment
cause
substance meronym/holonym
derived
participle of

0.61
0.147
0.094
0.0367
0.0238
0.02
0.016
0.014
0.0105
0.01
0.0091
0.00414
0.00195
0.00158
0.00089
0.0003
3.4E  06

Table 1: Probability of occurrence for every edge type in WordNet 2.0.
adjective sense can be used to measure its depth. A path with shallow sense nodes is more general
compared to a path with deep nodes. This parameter of semantic relatedness between terms is
captured by the measure of semantic path elaboration introduced in the following definition.
Definition 2 Given a word thesaurus O , a pair of senses S = (s1 , s2 ), where s1 ,s2  O and
s1 6= s2, and a path P =< p1 , p2 , ..., pl > of length l, where either s1 = p1 and s2 = pl or
s1 = pl and s2 = p1 , the semantic path elaboration of the path (SP E(S, O, P )) is defined as:
Q
1
i di+1
SP E(S, O, P ) = li=1 d2di +d
 dmax
, where di is the depth of sense pi according to O, and dmax
i+1
the maximum depth of O. If s1 = s2 , then d1 = d2 = d and SP E(S, O, P ) =
path from s1 to s2 then SP E(S, O, P ) = 0.

d
dmax .

If there is no

It is obvious in Definition 2 that a path of length l comprises l+1 nodes, thus when i = l, di+1 is
the last node in the path. Essentially, SPE is the harmonic mean of the two depths normalized to the
maximum thesaurus depth. The harmonic mean is preferred over the average of depths, since it offers a lower upper bound and gives a more realistic estimation of the paths depth. Compactness and
Semantic Path Elaboration measures capture the two most important parameters of measuring semantic relatedness between terms (Budanitsky & Hirst, 2006), namely path length and senses depth
in the used thesaurus. We combine these two measures following the definition of the Semantic
Relatedness between two terms:
Definition 3 Given a word thesaurus O, and a pair of senses S = (s1 , s2 ) the semantic relatedness
of S (SR(S, O)) is defined as maxP {SCM (S, O, P )  SP E(S, O, P )}.
8

fiT EXT R ELATEDNESS BASED ON A W ORD T HESAURUS

Algorithm 1 Maximum-Semantic-Relatedness(G, u, v, w)
1:
2:

3:
4:
5:
6:
7:
8:
9:
10:
11:

12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:

INPUT: A directed weighted graph G, two nodes u, v and a weighting scheme w : E  (0..1).
OUTPUT: The path from u to v with the maximum product of the edges weights.
Initialize-Single-Source(G, u)
for all vertices v  VG do
d[v] = 
[v] = N U LL
end for
d[u] = 1
Relax(u, v, w)
if d[v] < d[u]  w(u, v) then
d[v] = d[u]  w(u, v)
[v] = u
end if
Maximum-Relatedness(G, u, v, w)
Initialize-Single-Source(G, u)
S=
Q = VG
while v  Q do
s = Extract from Q the vertex with the maximum d
S =Ss
for all vertices k  Adjacency List of s do
Relax(s, k, w)
end for
end while
return the path following all the ancestors  of v back to u

Given a word thesaurus, there can be more than one semantic path connecting two senses. The
senses compactness can take different values for all the different paths. In these cases, we use the
path that maximizes the semantic relatedness. For its computation we introduce Algorithm 1, which
is a modification of Dijkstras algorithm (Cormen, Leiserson, & Rivest, 1990) for finding the shortest path between two nodes in a weighted directed graph. In the algorithm, G is the representation
of the directed weighted graph given as input (e.g., using adjacency lists), and VG is the set of all
the vertices of G. Also, two more sets are used; S, which contains all the vertices for which the
maximum semantic relatedness has been computed from the source vertex (i.e., from u), and Q,
which contains all the vertices for which the algorithm has not computed yet the maximum relatedness from the source vertex. Furthermore, three tables are used; d, which, for any vertex v stores the
maximum semantic relatedness found at any given time of the algorithm execution from the source
vertex, i.e., u in d[v]; , which for any vertex v stores its predecessor in [v]; and w, which stores
the edge weights of the graph (e.g., w[k, m] stores the edge weight of the edge that starts from k
and goes to m).
9

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

The algorithm comprises three functions: (a) Initialize-Single-Source(G, u), which initializes
tables d and , for every vertex v of the graph. More precisely, it sets d[v] = , since the semantic relatedness from the source is unknown at the beginning, and because the algorithm seeks
for the maximum semantic relatedness this is initially set to the minimum value (i.e., ). It
also sets [v] = N U LL, since at the beginning of the algorithm execution we are not aware
yet of the predecessor of any vertex v following the path from the source vertex u to v that results to the maximum semantic relatedness; (b) Relax(u, v, w), which given two vertices, u and v
that are directly connected with an edge of weight w[u, v], it updates the value d[v], in case that
if we follow the edge (u, v) this results to a higher semantic relatedness for vertex v from the
source, compared to the value we have computed up to that time of the algorithm execution; and
(c) Maximum-Relatedness(G, u, v, w), which uses the aforementioned functions and executes the
Dijkstras algorithm. The proof of the algorithms correctness follows in the next theorem.
Theorem 1 Given a word thesaurus O, an edges weighting function w : E  (0, 1), where a
higher value declares a stronger edge, and a pair of senses S(ss , sf ) declaring source (ss ) and destination (sf ) vertices, then the SCM (S, O, P )  SP E(S, O, P ) is maximized for the path returned
2di dj
 =w 

by Algorithm 1, by using the weighting scheme wij
ij dmax (di +dj ) , where wij is the new weight
of the edge connecting senses si and sj .
Proof 1 We will show that for each vertex sf  VG , d[sf ] is the maximum product of edges weight
through the selected path, starting from ss , at the time when sf is inserted into S. From now on,
the notation (ss , sf ) will represent this product. Path p connects a vertex in S, namely ss , to a
vertex in VG  S, namely sf . Consider the first vertex sy along p such that sy  VG  S and let
sx be ys predecessor. Now, path p can be decomposed as ss  sx  sy  sf . We claim that
d[sy ] = (ss , sy ) when sf is inserted into S. Observe that sx  S. Then, because sf is chosen
as the first vertex for which d[sf ] 6= (ss , sf ) when it is inserted into S, we had d[sx ] = (ss , sx )
when sx was inserted into S.
Because sy occurs before sf on the path from ss to sf and all edge weights are nonnegative
and in (0, 1) we have (ss , sy )  (ss , sf ), and thus d[sy ] = (ss , sy )  (ss , sf )  d[sf ]. But
both sy and sf were in V  S when sf was chosen, so we have d[sf ]  d[sy ]. Thus, d[sy ] =
(ss , sy ) = (ss , sf ) = d[sf ]. Consequently, d[sf ] = (ss , sf ) which contradicts our choice of sf .
We conclude that at the time each vertex sf is inserted into S, d[sf ] = (ss , sf ).
Next, to prove that the returned maximum product is the SCM (S, O, P )  SP E(S, O, P ), let
the path between ss and sf with the maximum edge weight product have k edges. Then, Algorithm 1
Q
2dk df
2d2 d3
2ds d2

returns the maximum ki=1 wi(i+1)
= ws2  dmax
(ds +d2 ) w23  dmax (d2 +d3 ) ...wkf  dmax (dk +df ) =
Qk 2di di+1
Qk
1
i=1 di +di+1  dmax = SCM (S, O, P )  SP E(S, O, P ).
i=1 wi(i+1) 
3.1.3 S EMANTIC R ELATEDNESS

FOR A

PAIR OF T ERMS

Based on Definition 3, which measures the semantic relatedness between a pair of senses S, we can
define the semantic relatedness between a pair of terms T (t1 , t2 ) as follows.
Definition 4 Let a word thesaurus O, let T = (t1 , t2 ) be a pair of terms for which there are entries
in O, let X1 be the set of senses of t1 and X2 be the set of senses of t2 in O. Let S1 , S2 , ..., S|X1 ||X2 |
be the set of pairs of senses, Sk = (si , sj ), with si  X1 and sj  X2 . Now the semantic relatedness
of T (SR(T, S, O)) is defined as:
10

fiT EXT R ELATEDNESS BASED ON A W ORD T HESAURUS

maxSk {maxP {SCM (Sk , O, P )  SP E(Sk , O, P )}} = maxSk {SR(Sk , O)}
for all k = 1..|X1 |  |X2 |. Semantic relatedness between two terms t1 , t2 where t1  t2  t and
t
/ O is defined as 1. Semantic relatedness between t1 , t2 when t1  O and t2 
/ O, or vice versa,
is considered 0.
For the remaining of the paper, the SR(T, S, O) for a pair of terms will be denoted as SR(T ), to
ease readability.
3.2 Analysis of the SR Measure
In this section we present the rationale behind the Definitions 1, 2, and 3, by providing theoretical
and/or experimental evidence for the decisions made on the design of the measure. We illustrate
the advantages and disadvantages of the different alternatives using simple examples and argue for
our decisions. Finally, we discuss on the advantages of SR against previous measures of semantic
relatedness.
The list of decisions made for the design of our semantic relatedness measure comprises: a)
use of senses in all POS, instead of noun senses only, b) use of all semantic edge types found in
WordNet, instead of the IS-A relation only, c) use of edge weights, and d) use of senses depth as
a scaling factor. It is important to mention that measures of semantic relatedness differ from the
measures of semantic similarity, which traditionally use hierarchical relations only and ignore all
other type of semantic relations. In addition, both concepts differentiate from semantic distance, in
the sense that the latter is a metric.
3.2.1 U SE ALL POS I NFORMATION
Firstly, we shall argue on the fact that the use of all POS in designing a semantic relatedness measure is important, and can increase the coverage of such a measure. The rationale supporting this
decision is fairly simple. Current data sets for evaluating semantic relatedness or even semantic similarity measures are restricted to nouns, like for example the Rubenstein and Goodenough 65 word
pairs (1965), the Miller and Charles 30 word pairs (1991), and the Word-Similarity-353 collection
(Finkelstein et al., 2002). Thus, the experimental evaluation in those data sets cannot pinpoint the
caveat of omitting the remaining parts of speech. However, text similarity tasks and their benchmark
data sets comprise more than nouns. Throughout the following analysis, the reader must consider
that the resulting measure of semantic relatedness among words is destined to be embedded in a
text-to-text semantic relatedness, as shown in the next section.
The following two sentences are a paraphrase example taken from the Microsoft Paraphrase
Corpus (Dolan, Quirk, & Brockett, 2004) and show the importance of using other POS as well,
such as verbs:
The charges of espionage and aiding the enemy can carry the death penalty.
If convicted of the spying charges he could face the death penalty.

Words that appear in WordNet are written in bold and stopwords have been omitted for simplicity5 .
The two sentences have many nouns in common (charges, death, penalty), but there are also pairs
of words between these two sentences that can contribute the evidence that these two sentences are
5. The stopwords list that we used is available at http://www.db-net.aueb.gr/gbt/resources/stopwords.txt

11

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

a paraphrase. For example espionage and spying have an obvious semantic relatedness, as well
as enemy and spying. Also, convicted and charges, as well as convicted and penalty. This type
of evidence would have been disregarded by any measure of semantic relatedness or similarity that
uses only the noun POS hierarchy of WordNet. Examples of such measures are: the measure of
Sussna (1993), Wu and Palmer (1994), Jiang and Conrath (1997), Resnik (1995, 1999), and the
WordNet-based component of the measure proposed by Finkelstein et al. (2002). From this point
of view, the decision to use all POS information expands the potential matches found by the measure and allows the use of the measure in more complicated tasks, like paraphrase recognition, text
retrieval, and text classification.
3.2.2 U SE E VERY T YPE OF S EMANTIC R ELATIONS
The decision to use all parts of speech in the construction of the semantic graphs, as it was introduced in our previous work (Tsatsaronis et al., 2007), imposes the involvement of all semantic
relations instead of merely taxonomic (IS-A) ones. Moreover, this decision was based on evidence
from related literature. The work of Smeaton et al. (1995) provides experimental evidence that measuring semantic similarity by incorporating non-hierarchical link types (i.e. part meronym/holonym,
member meronym/holonym, substance meronym/holonym) improves much the performance of such
a measure. The experimental evaluation was conducted by adopting a small variation of the Resniks
measure (1995).
Hirst and St-Onge (1998) reported that they have discovered several limitations and missing
connections in the set of WordNet relations during the construction of lexical chains from sentences
for the detection and correction of malapropisms. They provided the following example using the
pair of words in bold to report this caveat:
School administrators say these same taxpayers expect the schools to provide child care and
school lunches, to integrate immigrants into the community, to offer special classes for adult
students,.

The intrinsic connection between the nouns child care and school, which both exist in WordNet,
cannot be discovered by considering only hierarchical edge types. This connection is depicted in
Figure 2, which shows the path in WordNet. Our rich semantic representation is able to detect such
connections and address problems of the aforementioned type.
3.2.3 U SE W EIGHTS

ON

E DGES

The work of Resnik (1999) reports that simple edge counting, which implicitly assumes that links
in the taxonomy represent uniform distances, is problematic and is not the best semantic distance
measure for WordNet. In a similar direction lie the findings of Sussna (1993), who has performed
thorough experimental evaluation by varying edge weights in order to measure semantic distance
between concepts. Sussnas findings, revealed that weights on semantic edges are a non-negligible
factor in the application of his measure for WSD, and that the best results were reported when an
edge weighting scheme was used, instead of assigning each edge the same weight. For all these
reasons, we decided to assign a weight on every edge type, and we chose the simple probability
of occurrence for each edge type in WordNet, as our edge weighting scheme (see Table 1). This
very important factor is absent in several similarity measures proposed in the past, such as in the
measures of Leacock et al. (1998), Jarmasz and Szpakowicz (2003) and Banerjee and Pedersen
(2003), which are outperformed in experimental evaluation by our measure.
12

fiT EXT R ELATEDNESS BASED ON A W ORD T HESAURUS

activity
(Noun)
Hyponym
Hypernym

education
(Noun)

aid
(Noun)

Nominalization

educate
(Verb)

Hyponym
Hypernym

service
(Noun)

school
(Verb)

Nominalization

school
(Noun)

Hypernym

child care
(Noun)

Figure 2: Semantic path from child care to school following WordNet edges.
instrumentality
(Noun)

conveyance
(Noun)

Hyponym
Hypernym

Hypernym
implement
(Noun)
container
(Noun)

vehicle
(Noun)

Hyponym

public
transport
(Noun)

Hyponym
Hypernym
bar
(Noun)

Hypernym

Hyponym

wheeled
vehicle
(Noun)

autobus
(Noun)

Hyponym
wheeled
vehicle
(Noun)

Hypernym
lever
(Noun)

self-propelled
vehicle
(Noun)
Category Domain

Hyponym
Hypernym

car
(Noun)

Hypernym

pedal
(Noun)

motor vehicle
(Noun)
passenger
(Noun)

Part Meronym
Hyponym
Hypernym

accelerator
(Noun)

car
(Noun)

Category Domain

NWPL Path
PR Path

Figure 3: Product Relatedness (PR) and Normalized Weighted Path Length (NWPL) paths for pairs:
car and accelerator (left), car and autobus (right).

3.2.4 U SE D EPTH S CALING FACTOR
Our decision to incorporate the depth scaling factor (SPE in Definition 2) in the edge weighting
mechanism has been inspired by the thorough experimental evaluation conducted by Sussna (1993),
13

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

which has provided evidence on the importance of the edge weighting factor in semantic network
based measures. According to our experiments on the Miller and Charles data set the Spearman
correlation with human judgements was much lower (7 percentage points) when omitting the depth
scaling factor than when adopting the SPE factor (see Definition 3).
3.2.5 J USTIFICATION OF SR D EFINITIONS
According to Definition 1, the semantic compactness for a pair of concepts is the product of depthscaled weights of the edges connecting the two concepts. The use of product instead of sum or
normalized sum of edges weights is explained in the following.
Since there might be several paths connecting the two concepts, Definition 3 clearly selects
the path that maximizes the product of semantic compactness (SC) and semantic path elaboration (SPE). For simplicity, we ignore the effect of the depth scaling
factor (SPE in Definition 2)
Q
and consequently, our aim is to find the path that maximizes li=1 ei , where e1 , e2 , ..., el are the
(non depth-scaled) weights of edges in the path connecting two given concepts. Let us name this
less elaborate version of our semantic relatedness measure after product relatedness (PR), where
P R(S, O) = maxP {SCM (S, O, P )}. An alternative would have beenPto define semantic coml

e

i
pactness as the normalized sum of the weights in the path, which is: i=1
. In this case, the
l
semantic relatedness would be measured on the path that maximizes the latter formula, since by
nature, semantic relatedness always seeks to find the path that maximizes the connectivity between
two concepts. Let us name this alternative after normalized weighted path length (NWPL).
In the example of Figure 3, we show how PR and NWPL compute the semantic relatedness for
the term pair car and accelerator (left) and car and autobus (right). The path that maximizes the
respective formulas of PR and NWPL using Algorithm 1 and edge weights in Table 1, is illustrated in
Figure 3 using black and white arrows respectively. For the pair car and accelerator the sum-based
formula, normalized against the path length, selects a very large path in this example, with a final
computed relatedness of 0.61, which is the weight of the hypernym/hyponym edges. PR finds that
the path maximizing the product is the immediate part meronym relation from car to accelerator,
with a computed relatedness of 0.0367, which is the weight of the part meronym edges. The main
problem arising with NWPL is the fact that it cannot distinguish among the relatedness between
any pair of concepts in the hypernym/hyponym hierarchy of WordNet. In this example, NWPL
computes the same relatedness (0.61) between every possible concept pair shown in the top figure.
In contrast, PR is able to distinguish most of these pairs in terms of relatedness. More precisely, this
behavior of PR is due to the fact that it embeds the notion of the path length, since the computed
relatedness decays by a factor in the range (0, 1) for every hop made following any type of semantic
relation. Another example, that also shows the importance of considering all WordNet relations, is
the one shown on the right part of Figure 3, where NWPL and PR paths have been computed for the
term pair car and autobus. Again, NWPL selects a very large path, and does not incline from the
hypernym/hyponym tree.
Clearly, NWPL would rather traverse through a huge path of hypernym/hyponym edges, than
following any other less important edge type, which would decrease its average path importance.
This behavior creates serious drawbacks: (a) lack of ability to distinguish relatedness among any
pair of concepts in the same hierarchy, and (b) large increase of the actual computational cost of
Algorithm 1, due to the fact that it will tend not to incline from the hypernym/hyponym hierarchy,
even if there is a direct semantic edge (other than hypernym/hyponym) connecting the two concepts,

14

fiT EXT R ELATEDNESS BASED ON A W ORD T HESAURUS

like shown in Figure 3. Furthermore, by conducting experiments with NWPL in the 30 word pairs
of Miller and Charles, we discovered that in almost 40% of the cases, NWPL produces the same
value of semantic relatedness, equal to 0.61, being unable to distinguish them and creating many
ties. Thus, PR is a better option to use in our measure, as the semantic compactness factor.
Last, but not least, regarding the overall design of SR, we should mention that the proposed measure is solely based on the use of WordNet, in contrast to measures of semantic relatedness that use
large corpora, such as Wikipedia. Although, such measures, like the ones proposed by Gabrilovich
and Markovitch (2007), and Ponzetto and Strube (2007a), provide a larger coverage regarding concepts that do not reside in WordNet, they require the processing of a very large corpora (Wikipedia),
which also changes very fast and very frequently. Experimental evaluation in Section 4 shows that
our measure competes well against the aforementioned word-to-word relatedness measures in the
used data sets. In the following section, we introduce Omiotis, the extension of SR for measuring
text-to-text relatedness.
3.3 Omiotis
To quantify the degree to which two text segments semantically relate to each other, we build upon
the SR measure, which we significantly extend in order to account not only for the terms semantic
relatedness but also for their lexical similarity. This is because texts may contain overly-specialized
terms (e.g., an algorithms name) that are not represented in WordNet. Therefore, relying entirely on
the term semantics for identifying the degree to which texts relate to each other would hamper the
performance of our approach. On the other hand, semantics serve as complement to our relevance
estimations given that different text terms might refer to (nearly-) identical concepts.
To quantify the lexical similarity between two texts, e.g., text A and B, we begin with the estimation of the terms importance weights as these are determined by the standard TF-IDF weighting
scheme (Salton, Buckley, & Yu, 1982).
Thereafter, we estimate the lexical relevance, denoted as a,b between terms a  A and b  B
based on the harmonic mean of the respective terms TF-IDF values, given by:
a,b =

2  T F IDF (a, A)  T F IDF (b, B)
T F IDF (a, A) + T F IDF (b, B)

(6)

Harmonic mean is preferred instead of average, since it provides a more tight upper bound (Li,
2008). This decision is based on the fact that T F IDF (a, A) and T F IDF (b, B) are two different
quantities measuring the qualitative strength of a and b in the respective texts.
Having computed the lexical relevance between text terms a and b, we estimate their semantic
relatedness, i.e. SR(a, b) as described previously. Based on the estimated lexical relevance and
semantic relatedness between pairs of text terms, our next step is to find for every word a in text A
the corresponding word b in text B that maximizes the product of semantic relatedness and lexical
similarity values as given by Equation 7.
b = arg max(a,b  SR(a, b))

(7)

bB

Where b corresponds to that term in text B, which entails the maximum lexical similarity and
semantic relatedness with term a from text A.6 In a similar manner, we define a , which corresponds
6. The function argmax selects the case from the examined ones, that maximizes the input formula of the function.

15

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

to that term in text A, which entails the maximum lexical similarity and semantic relatedness with
term b from text B.
a = arg max(a,b  SR(a, b))
(8)
aA

Consequently, we aggregate the lexical and semantic relevance scores for all terms in text A,
with reference to their best match in text B denoted as shown in Equation 9.

!
X
1
(A, B) =
(9)
a,b  SR(a, b )
|A|
aA

We do the same for the opposite direction (i.e. from the words of B to the words of A) to cover
the cases where the two texts do not have an equal number of terms.
Finally, we derive the degree of relevance between texts A and B by combining the values
estimated for their terms that entail the maximum lexical and semantic relevance to one another,
given by:
[(A, B) + (B, A)]
(10)
2
Algorithm 2 summarizes the computation of Omiotis. Its computation entails a series of steps,
the complexity of which is discussed in Section 3.5.
Omiotis(A, B) =

3.4 Applications of Semantic Relatedness
In this section we describe the methodology of incorporating semantic relatedness between pairs of
words or pairs of text segments, into several applications.
3.4.1 W ORD - TO -W ORD S IMILARITY
Rubenstein and Goodenough (1965) obtained synonymy judgements from 51 human subjects on 65
pairs of words, in an effort to investigate the relationship between similarity of context and similarity of meaning (synonymy). Since then, the idea of evaluating computational measures of semantic
relatedness by comparing against human judgments on a given set of word pairs, has been widely
used, and even more data sets were developed. The proposed measure of semantic relatedness
between words (SR), introduced in Definition 4, can be used directly in such a task, in order to
evaluate the basis of Omiotis measure, which is the measurement of word-to-word semantic relatedness. The application is straightforward: Let n be all pairs of words in the used word similarity data
set. Then, the semantic relatedness for every pair is computed, using SR(T, S, O) as defined in 4.
The computed values are sorted in a descending order, and the produced ranking of similarities is
compared against the gold standard ranking of humans, using Spearman correlation. The scores
can be used to compute Pearsons product moment correlation. Additional measures of semantic
relatedness can be compared against each other by examining the respective correlation values with
human judgements.
3.4.2 SAT A NALOGY T ESTS
The problem of identifying similarities in word analogies among pairs of words is a difficult problem
and it has been standardized as a test for assessing the human ability for language understanding,
16

fiT EXT R ELATEDNESS BASED ON A W ORD T HESAURUS

Algorithm 2 Omiotis(A,B, Sem, Lex )
1: INPUT: Two texts A and B, comprising m and n terms each (a and b are terms from A and B
respectively),
a semantic relatedness measure Sem : SR(a, b)  (0..1),
a weighting scheme of term importance in a text Lex : T F IDF (a, A)  (0..1)
2: OUTPUT: Find the pair of terms that maximizes the product of Sem and Lex values.

3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:

Compute-Zeta(A,B)
sum(A) := 0
for all terms a  A do
b := N U LL
T empZeta := 0
for all terms b  B do
a,b = 2Lex(a,A)Lex(b,B)
Lex(a,A)+Lex(b,B)
if T empZeta < a,b  Sem(a, b) then
T empZeta = i,j  Sem(a, b)
b = b
end if
end for
sum(A) := sum(A) + T empZeta
end for
Zeta(A, B) := sum(A)/|A|
Compute-Omiotis(A,B)

17:

Omiotis(A, B) :=

Zeta(A,B)+Zeta(B,A)
2

under the scope of the well known SAT analogy tests (Scholastic Aptitude Tests). SAT tests are
used as admission tests by universities and colleges in the United States. The participants aim is to
locate out of the five given word pairs the one that presents the most similar analogy to the target
pair.
Although it is difficult for machines to model the human cognition of word analogy, several
approaches exist in the bibliography that attempt to tackle this problem. Previous approaches can be
widely categorized into: corpus-based, lexicon-based and hybrid. Some examples of corpus-based
are the approaches of Turney (2008b) and Bicici and Yuret (2006). Examples of lexicon-based
approaches, are those of Veale (2004) and the application of the lexicon-based measure by Hirst
and St-Onge (1998) in SAT, that can be found in the work of Turney (2006). Hybrid approaches are
applied in SAT, through the application of the measures of Resnik (1995) and Lin (1998) that can
also be found in the work of Turney (2006).
In order for the reader to understand the difficulty of answering SAT questions, we must point
out that the average US college applicant scores 57% (Turney & Littman, 2005), while the top
corpus-based approach scores 56.1% (Turney, 2006), the top lexicon-based scores 42% (Veale,
2004) and the top hybrid scores 33.2% (Resnik, 1995).
17

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

Another way of categorizing the approaches that measure semantic similarity in analogy tasks
is to distinguish among attributional and relational similarity measures (Gentner, 1983).7 Representative approaches of the first category are lexicon-based approaches, while paradigms of relational
similarity measures can be found in approaches based on Latent Relational Analysis (LRA) (Turney,
2006). It is of great interest to point out that LRA-based approaches, like the LRME algorithm proposed recently by Turney (2008a), are superior to attributional similarity approaches in discovering
word analogies. This fact is also supported by the experimental findings of Turney (2006). Without
doubt, relational similarity approaches may perform better in the SAT analogy task, but still, as
shown later in the experiments we conducted in other applications, like paraphrase recognition, the
lexicon-based measures can outperform LRA-based approaches in such tasks.
Semantic relatedness (SR) between words, as applied in Omiotis, can be exploited to solve the
word analogy task. The aim of word analogy is, given a pair of words w1 and w2 , to identify the
series of semantic relations that lead from w1 to w2 (semantic path). In the SAT test, the target pair
(w1 ,w2 ) and candidate word pairs (w1k ,w2k ), with k usually being from 1 to 5, are processed in order
to find each pairs analogy. The aim is to locate the pair k, which exposes maximum similarity to w1
and w2 . A straightforward method to choose among the 5 candidate pairs is to employ two criteria:
At first, the k analogies to the analogy of the target pair can be compared, and then the candidate that
shows by far the most similar analogy can be selected. However, when the most similar analogy is
not obvious, all the 6 pairs may be examined together in order for the slightest differences that lead
to the correct answer to be discovered. We attempt to model human cognition of this task using SR
in a two fold manner: (a) we measure SR to capture the horizontal analogy between the given pair
and the possible candidate pairs, and (b) we measure SR to capture the vertical analogy between
the given pair and the possible candidate pairs. These two aspects are covered by the following
Equations 11 to 13. To capture the horizontal analogy between a pair of words and a candidate pair,
we measure the difference of the SR score between the two words respectively as shown:
s1 (w1k , w2k ) = 1  |SR(w1 , w2 )  SR(w1k , w2k )|

(11)

Essentially, s1 expresses the horizontal analogy of the candidate pair (w1k , w2k ) with the given
pair (w1 , w2 ). Similarly, we capture the notion of the vertical analogy between the two pairs by
computing the difference of the SR scores among the two pairs words, as follows:
s2 (w1k , w2k ) = 1  |SR(w1 , w1k )  SR(w2 , w2k )|

(12)

Finally, we rank candidates depending on the combined vertical and horizontal analogy they have
with the given pair, according to the following equation:
s(w1k , w2k ) =

s1 (w1k , w2k ) + s2 (w1k , w2k )
2

(13)

Eventually, we select the candidate pair with the maximum combined score, taking into account
both aspects (horizontal and vertical) of analogy between the given and the candidate pairs.
The intuition behind the selection of the these two scores for handling the SAT test, is the
following. The order of the words in the pairs (both target and candidates) is not random. Usually,
given a pair (w1 , w2 ), and the candidate pairs (w1k , w2k ) the test is solved if one can successfully
7. Two objects, X and Y, are attributionally similar when the attributes of X are similar to the attributes of Y. Two pairs, A:B and C:D,
are relationally similar when the relations between A and B are similar to the relations between C and D.

18

fiT EXT R ELATEDNESS BASED ON A W ORD T HESAURUS

Stem: wallet : money

Choices:

(a)

safe : lock

(b)

suitcase : clothing

S1: 0.2605
S2: 6.75E-04
(c)

camera : film

S1: 0.4795
S2: 0.015
(d)

setting : jewel

S1: 0.1805
S2: 7.87E-05
(e)

car : engine

S1: 0.3764
S2: 8.99E-05

Winner based on S1 (Horizontal Analogy): b
Winner based on S2 (Vertical Analogy): b
Winner based on combined S: b
Correct Answer: b

S1: 0.1506
S2: 0.0029

Figure 4: Example of computing the Semantic Relatedness measure (SR) in a given Scholastic Aptitude Test (SAT) question.

find the analogy: w1k is to w2k what w1 is to w2 . From this perspective, s1 and s2 try to find the
candidate pair that best aligns with the target pair. Figure 4 illustrates these two types of analogies
(horizontal and vertical) for an example SAT question.
In order to motivate more our selection of s1 and s2 for answering SAT questions, we will
discuss in more detail how these two quantities pertain to the concepts of strength and type of
the relations between a pair of SAT words. Turney (2006) describes a method for comparing the
relations between candidate word pairs and the stem word pair, in which he utilizes the type of
the relation connecting the words in each pair and finally selects the pair that best matches the
type of the relation connecting the words in the stem pair. Though we do not explicitly examine
the label of the edges connecting the words in each pair, implicitly we do so by computing SR
between them. Since our weighting of the WordNet edges is fine grained, and distinguishes every
type of semantic relation in WordNet, instead of labels, we are using edge weights. SR definition
can provide a fine grained distinguishment between two pairs of words, depending on the types of
the edges connecting the words respectively, which is expressed by their weights, and also taking
into account other factors, like the depth of the nodes comprising their connecting path inside the
thesaurus. Besides s1 , which attempts to capture the aforementioned properties between word pairs,
s2 attempts the same between the words of the same order among two word pairs (i.e., the first word
from the first pair, with the second word from the second pair). This forms an attempt to capture
how aligned are two word pairs, according to their SR values between their words.
3.4.3 PARAPHRASE R ECOGNITION

AND

S ENTENCE - TO -S ENTENCE S IMILARITY

Performance of applications relying on natural language processing may suffer from the fact that
the processed documents might contain lexically different, yet semantically related, text segments.
The task of recognizing synonym text segments, which is better known as paraphrase recognition,
or detection, is challenging and difficult to solve, as shown in the work of Pasca (2005). The task
itself is important for many text related applications, like summarization (Hirao, Fukusima, Oku19

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

mura, Nobata, & Nanba, 2005), information extraction (Shinyama & Sekine, 2003) and question
answering (Pasca, 2003). We experimentally evaluate the application of Omiotis in the paraphrasing
detection task (Section 4.2), using the Microsoft Research Paraphrase Corpus (Dolan et al., 2004).
The application of Omiotis in paraphrase detection is straightforward: given a pair of text segments,
we compute the Omiotis score between them, using Equation 10 and Algorithm 2. Higher values
of Omiotis for a given pair denote stronger semantic relation between the examined text segments.
The task is now reduced to define a threshold, above which an Omiotis value can be considered as
a determining sign of a paraphrasing pair. In the experimental evaluation of Omiotis, we explain in
detail how we have selected this threshold for the paraphrase recognition task.
In a similar manner, by using Equation 10 and Algorithm 2, the semantic relatedness scores for
pairs of sentences can be computed. For this task, we are using the data set of Li et al. (2006) to
evaluate Omiotis, comprising 30 sentence pairs, for which human scores are provided. In Section 4
we describe in detail the experimental set up.
3.5 Complexity and Implementation Issues
The computation of Omiotis entails a series of steps, the complexity of which is strongly related to its
base measure of Semantic Relatedness (SR). Primarily, given two words, w1 and w2 the construction
time of the semantic network used to compute SR according to Algorithm 1, has been proven to
be O(2  k l+1 ) (Tsatsaronis et al., 2007), where k is the maximum branching factor of the used
thesaurus nodes and l is the maximum semantic path length in the thesaurus. Once the semantic
network is constructed, the complexity of Algorithm 1 is reduced to the standard time complexity
cost of Dijkstras algorithm. Using Fibonacci heaps, it is possible to alleviate the computational
burden of Dijkstra and further improve time complexity. In the semantic network, Dijkstra takes
O(nL + mD + nE), where n is the number of nodes in the network, m the number of edges, L is
the time for insert, D the time for decrease-key and E the time for extract-min. If Fibonacci heaps
are used then L = D = O(1) and the cost of extract-min is O(logn), thus significantly reducing
the cost of execution. This whole procedure is repeated 2  n1  n2 times for the computation of
Omiotis between two documents d1 and d2 having in total n1 and n2 distinct words respectively.
From the aforementioned, it is obvious that the computation of Omiotis is not cheap in general.
For this purpose, and in order to improve the systems scalability, we have pre-computed and stored
all SR values between every possible pair of synsets in a RDBMS. This is a one-time computation
cost, which dramatically decreases the computational complexity of Omiotis. The database schema
has three entities, namely Node, Edge and Paths. Node contains all WordNet synsets. Edge indexes
all edges of the WordNet graph adding weight information for each edge computed using the SR
measure. Finally, Paths contains all pairs of WordNet synsets that are directly or indirectly connected in the WordNet graph and the computed relatedness. These pairs were found by running a
Breadth First Search (BFS) starting from all WordNet roots for all POS. Table 2 provides statistical
information for the RDBMS which exceeds 220 Gbytes in size. Column 1 indicates the number of
distinct synsets examined, column 2 shows the total number of the edges, and column 3 depicts the
number of the connected synsets (by at least one path following the offered WordNet edges). The
current implementation takes advantage of the database structures (indices, stored procedures etc)
in order to decrease the computational complexity of Omiotis. The following example is indicative
of the complexity of SR computation. The average number of senses per term is between 5 and 7
2
(depending on the POS). For a pair of terms of known POS, we perform n2 (n  6) combinations
20

fiT EXT R ELATEDNESS BASED ON A W ORD T HESAURUS

Distinct Synsets
115,424

Total Edges
324,268

Connected Synset Pairs
5,591,162,361

Table 2: Statistics of the WordNet graph in the implemented database.

and for each pair of synsets we compute the similarity as presented in Definition 3. When these
similarities are pre-computed, the time required for processing 100 pairs of terms is  1 sec, which
makes the computation of Omiotis feasible and scalable. As a proof of concept, we have developed
an on-line version of the SR and the Omiotis measures8 , where the user can test the term-to-term
and sentence-to-sentence semantic relatedness measures (Tsatsaronis et al., 2009).

4. Experimental Evaluation
The experimental evaluation of Omiotis is two-fold. First, we test the performance of the wordto-word semantic relatedness measure (SR), in which Omiotis is based, in three types of tasks: (a)
word-to-word similarity and relatedness, (b) synonym identification, and (c) Scholastic Aptitude
Test (SAT). Second, we evaluate the performance of Omiotis in two tasks: (a) sentence-to-sentence
similarity, and (b) the paraphrase recognition task.
4.1 Evaluation of the Semantic Relatedness (SR) Measure
For the evaluation of the proposed semantic relatedness measure between two terms we experimented on three different categories of tests. The first category comprises data sets that contain
word pairs, for which human subjects have provided similarity scores or relatedness scores. The
provided scores create a ranking of the word pairs, from the most similar to the most irrelevant. We
evaluate the performance of measures, by computing the correlation between the list of the human
rankings and the list produced by the measures. In this task, we evaluate the performance of SR in
three benchmark data sets, namely the Rubenstein and Goodenough 65 word pairs (1965) (R&G),
and the Miller and Charles 30 word pairs (1991) (M&C), for which humans have provided similarity scores, and, also, in the Word-Similarity-353 collection (Finkelstein et al., 2002) (353-C), which
comprises 353 word pairs, for which humans have provided relatedness scores.
The second category of experiments comprises synonym identification tests. In these tests, given
an initial word, the most appropriate synonym word must be identified among the given options.
In this task we evaluate the performance of SR in the TOEFL data set, comprising 80 multiple
choice synonym questions, and the ESL data set, comprising 50 multiple choice synonym questions
questions.9
The third category of experiments is based on the Scholastic Aptitude Test (SAT) questions. In
SAT, given a pair of words, the most relevant pair among five other given pairs must be selected. This
task is based on word analogy identification. The evaluation data set comprises 374 test questions.
8. Publicly available at http://omiotis.hua.gr
9. http://www.aclweb.org/aclwiki/index.php?title=TOEFL_Synonym_Questions
http://www.aclweb.org/aclwiki/index.php?title=ESL_Synonym_Questions_(State_of_the_art)

21

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

Category
Lexicon-based

Corpus-based

Hybrid

Method
HS
LC
JS
GM
WLM
SP
IS-A SP
JC
L
R
HR
SR

R&G
Spearmans  Pearsons r
0.745
0.786

0.785
0.838
N/A
0.818

0.816
N/A

0.64
N/A
N/A
0.52
N/A
0.70

0.709
0.781

0.77
0.818
0.7485
0.778
0.817
N/A
0.8614
0.876

M&C
Spearmans  Pearsons r
0.653
0.744

0.748
0.816
N/A
0.878
0.723
N/A

0.70
N/A
N/A
0.47
N/A
0.69
0.805
0.85
0.767
0.829
0.737
0.774
0.904
N/A
0.856
0.864

Table 3: Spearmans and Pearsons correlations for the Rubenstein and Goodenough (R&G) and
Miller and Charles (M&C) data sets. Confidence levels:  =0.90,  =0.95,  =0.99

4.1.1 E VALUATION

ON

S EMANTIC S IMILARITY

AND

R ELATEDNESS

For the first category of experiments, we compared our measure against ten known measures of
semantic relatedness: Hirst and St-Onge (1998) (HS), Jiang and Conrath (1997) (JC), Leacock
et al. (1998) (LC), Lin (1998) (L), Resnik (1995, 1999) (R), Jarmasz and Szpakowicz (2003) (JS),
Gabrilovich and Markovitch (2007, 2009) (GM), Milne and Witten (2008) (WLM), Finkelstein et al.
(2002) (LSA), Hughes and Ramage (2007) (HR), and Strube and Ponzetto (2006, 2007a) (SP). For
the measure of Strube and Ponzetto we have also included the results of a version of the measure that
is only based on IS-A relations (Ponzetto & Strube, 2007b) (IS-A SP). For each measure, including
our own measure (SR), we have computed both the Spearman rank order correlation coefficient
() and the Pearson product-moment correlation coefficient (r), with  being derived from r, since
for the computation of  the relatedness scores are transformed into rankings. Both correlation
coefficients are computed based on the relatedness scores and rankings provided by humans in
all three data sets (the relatedness scores create a ranking of the pairs of words, based on their
similarity). For the measures HS, JC, LC, L and R, the rankings and the relatedness scores of the
word pairs for the R&G and the M&C data sets, are given in the work of Budanitsky and Hirst
(2006). For the JS measure, the r value is given in the work of Jarmasz and Szpakowicz (2003)
for the R&G and the M&C data sets, and the  value is given in the work of Gabrilovich and
Markovitch (2007). For the GM measure the  values are given in the work of Gabrilovich and
Markovitch (2007). For the WLM measure the  values are given in the work of Milne and Witten
(2008). For the LSA method the  value is given in the work of Gabrilovich and Markovitch (2007),
only for the 353-C data set. For the HR measure the  values are given in the work of Hughes and
Ramage (2007). Finally, for the SP measure the r values are given in the work of Ponzetto and
Strube (2007a), and for the IS-A SP are given in the work of Ponzetto and Strube (2007b).
22

fiT EXT R ELATEDNESS BASED ON A W ORD T HESAURUS

In Table 3 we show the values of  and r for the R&G and the M&C data sets and for SR
and the compared measures. The human scores for all pairs of words for the two data sets can
be found in the analysis of Budanitsky and Hirst (2006). Note that the M&C data set is a subset
of the R&G data set. In some cases, the computation of  or r was not feasible, due to missing
information regarding the detailed rankings or relatedness scores for the respective measures. In
these cases the table has the entry N/A. Also the LSA measure is omitted in this table because 
and r were not reported in the literature for these two data sets. We have also conducted a statistical
significance test on the difference between SR correlations and the respective correlations of the
compared measures, using Fishers z-transformation (Fisher, 1915). For each reported number, the
symbol  indicates that the difference between the correlation produced by SR and the respective
measure is statistically significant at the 0.99 confidence level (p < 0.01). The symbol  indicates
the same at the 0.95 confidence level (p < 0.05) and, finally, the symbol  indicates statistical
significance of the correlations difference at the 0.90 confidence level (p < 0.10). In cases when
the difference is not statistically significant in any of those confidence levels, there is no indicating
symbol.
In Table 4 we show the values of  and r for the 353-C data set. The reason we present the results
of the experiments in the 353-C data set in another table than the respective results of the R&B and
M&C data sets is that this collection focuses on the concept of semantic relatedness, rather than on
the concept of semantic similarity (Gabrilovich & Markovitch, 2007). Relatedness is more general
concept than similarity, as argued in the analysis of Budanitsky and Hirst (2006). Thus, it can be
argued that the humans in the 353-C thought differently when scoring, compared to the case of the
R&B and M&C data sets. The detailed human scores for the 353-C data set are made available with
the collection10 . The measures L, JC and HS are omitted, because no information was available for
computing  or r values. As a further remark regarding the 353-C collection, we need to add the fact
that there are cases where the inter-judge correlations may fall below 65%, while R&B and M&C
data sets have inter-judge correlations between 0.88 and 0.95. Again, statistical significance tests
have been conducted using the Fishers z-transformation, regarding the difference of SR correlations
and the correlations of the compared measures. The used symbols that indicate the level of the
statistical significance are the same as previously. With regards to the reported correlations for the
R&G and M&C data sets, it is shown that SR performs very well, since in the majority of the cases
SR has higher correlation compared to the other measures of semantic relatedness or similarity
of any category (knowledge-based, corpus-based or hybrid). In the R&G data set SR reports the
highest  and r correlations. In the M&C data set SR has the second highest  correlation. The
HR measure has the highest  correlation, but in the R&G and 353-C SR outperforms HR. The
differences between SR and HR are not statistically significant in any of the two examined data sets.
Also, in the M&C data set SR has the second r correlation with the JS reporting the highest, but
JS is outperformed by SR in the R&G and 353-C data sets. In the case of the M&C data set, the
difference between SR and JS is not statistically significant, but SR outperforms JS in the R&G and
the 353-C data sets, with statistically significant difference in the reported correlations. Another
important conclusion from the results, is the fact that the IS-A SP measure performs better than the
SP measure. This is mainly due to the fact that for the computation of the similarity values in such
data sets, the inclusion of only IS-A relations is much more reasonable (Ponzetto & Strube, 2007b).
The differences in their results (SP and IS-A SP) motivate even more our SR measure, since we
10. http://www.cs.technion.ac.il/gabr/resources/data/wordsim353/

23

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

Category
Lexicon-based

Corpus-based

Hybrid

Method
LC
JS
GM
WLM
LSA
SP
R
HR
SR

353-C
Spearmans  Pearsons r
N/A
0.34

0.55
N/A

0.75
N/A
0.69
N/A

0.56
N/A
N/A
0.49
N/A
0.34

0.552
N/A
0.61
0.628

Table 4: Spearmans and Pearsons correlations for the 353 word pairs (353-C) data set. Confidence
levels:  =0.90,  =0.95,  =0.99

take the best of both worlds, i.e., we weigh IS-A relations high, and fall back to other relations if
necessary.
Regarding the 353-C data set, the results in Table 4 show that SR again performs well, with
the top performers being the Wikipedia-based approaches (Gabrilovich & Markovitch, 2009; Milne
& Witten, 2008). The difference between them is statistically significant, but we should note that
SR outperforms both GM and WLM in the R&G and M&C data sets, with statistically significant
difference as well. Partly, this difference in the performance of SR compared to GM and WLM can
be explained as follows: the GM measure considers words in context (Gabrilovich & Markovitch,
2009), and thus inherently performs word sense disambiguation; in contrast, SR takes as input a pair
of words, lacks context, and is based only on the information existing in WordNet, which, especially
for several of the cases in the 353-C data set, creates a disadvantage (e.g., in the word pair Arafat
and Jackson, there are 11 different entries for the second word in WordNet). The same holds for
the WLM measure. Another reason for this difference in performance is the coverage of WordNet.
In several cases, one or both of the two words in the 353-C data set comprising a pair, do not exist
in WordNet (e.g., the football player Maradona). However, as expected, and also shown in the
experimental analysis of Omiotis that follows, when context is considered, the proposed semantic
relatedness measure performs better (the reader may wish to consult Table 9, where for a subset of
the R&G data set that contains the full definitions of the words, the correlations of Omiotis with the
human judgements are the top found among the compared approaches).
To visualize the performance of our measure in a more comprehensible manner, we also present
in Figure 5 the relatedness values given by humans for all pairs in the R&G and M&C data sets,
in increasing order of value (left side) and the respective values for these pairs produced using SR
(right side). Note that the x-axis in both charts begins from the least related pair of terms, according
to humans, and continues up to the most related pair of terms. The y-axis in the left chart is the
respective humans rating for each pair of terms. The right figure shows SR for each pair. A closer
look on Figure 5 reveals that the values produced by SR (right figure) follow a pattern similar to that
of the human ratings (left figure).
24

fiT EXT R ELATEDNESS BASED ON A W ORD T HESAURUS

HUMAN RATINGS AGAINST HUMAN RANKINGS - R&G Data Set

SEMANTIC RELATEDNESS AGAINST HUMAN RANKINGS - R&G Data Set

4
0.9
Semantic Relatedness

Human Rating

3.5
3
2.5
2
1.5
1
0.5

20

30

40

50

0.7
0.6
0.5
0.4
0.3
0.2

correlation of human pairs ranking and human ratings

correlation of human pairs ranking and semantic relatedness

0.1

0
10

0.8

60

65

10

20

30

40

50

60

Pair Number

Pair Number

HUMAN RATINGS AGAINST HUMAN RANKINGS - M&C Data Set

SEMANTIC RELATEDNESS AGAINST HUMAN RANKINGS - M&C Data Set

65

4
Semantic Relatedness

Human Rating

3.5
3
2.5
2
1.5
1
0.5

correlation of human pairs ranking and human ratings

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1

correlation of human pairs ranking and semantic relatedness

0
5

10

15

20

25

30

5

Pair Number

10

15

20

25

30

Pair Number

Figure 5: Correlation between human ratings and Semantic Relatedness measure (SR) in the Rubenstein and Goodenough (R&G) and Miller and Charles (M&C) data sets.

4.1.2 E VALUATION

ON

S YNONYM I DENTIFICATION

For the synonym identification task we are using the TOEFL 80 questions data set and the ESL 50
questions data set. For the TOEFL data set we compare with several other methods. More specifically, we examine: the lexicon-based measures of Leacock et al. (1998) (LC), Hirst and St-Onge
(1998) (HS), and Jarmasz and Szpakowicz (2003) (JS); the corpus-based measures of Landauer and
Dumais (1997) (LD), Pado and Lapata (2007) (PL), Turney (2008b) (T), Terra and Clarke (2003)
(TC), and Matveeva et al. (2005) (M); the hybrid measures of Resnik (1995) (R), Lin (1998) (L),
Jiang and Conrath (1997) (JC), and Turney et al. (2003) (PR); and a Web-based method by RuizCasado et al. (2005) (RC). We also report the results of random guessing (RG) and the performance
of the average college applicant (H). Table 5 shows the results on the 80 TOEFL questions. The
table reports the number of the correct and the respective percentage given by all measures. In order
to test the statistical significance of the differences in the measures performance, we conducted
Fishers Exact Test (Agresti, 1990). As in the previous tables, the symbol  indicates statistically
significant difference at the 0.99 confidence level,  at the 0.95 confidence level, and  at the 0.90
confidence level. The results of Table 5 show that SR ranks second among all reported methods,
with the best method being the hybrid PR (Turney et al., 2003). With regards to its comparison with
the lexicon-based methods, SR reports better results, statistically significant at the confidence levels
indicated.
In a similar manner, we have conducted experiments in the ESL 50 questions data set, and
compare our results with: the lexicon-based measures of Leacock et al. (1998) (LC), Hirst and StOnge (1998) (HS), and Jarmasz and Szpakowicz (2003) (JS); the corpus-based measures of Turney
(2001) (PMI-IR), and Terra and Clarke (2003) (TC); and the hybrid measures of Resnik (1995) (R),
25

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

Category
Lexicon-Based

Corpus-Based

Hybrid
Web-Based
Other

Method
LC
HS
JS
LD
PL
T
TC
M
R
L
JC
PR
RC
RG
H
SR

#Correct Answers
17
62
63
52
58
61
65
69
16
19
20
78
66
20
52
70

Percentage of Correct Answers
0.212
0.775
0.787
0.65
0.725
0.762
0.812
0.862
0.2
0.237
0.25
0.975
0.825
0.25
0.65
0.875

Table 5: Number and percentage of correct answers in the TOEFL 80 questions test. Confidence
levels:  =0.90,  =0.95,  =0.99

Category
Lexicon-Based
Corpus-Based
Hybrid
Other

Method
LC
HS
JS
PMI-IR
TC
R
L
JC
RG
SR

#Correct Answers
18
31
41
37
40
16
18
18
20
41

Percentage of Correct Answers
0.36
0.62
0.82
0.74
0.8
0.32
0.36
0.36
0.25
0.82

Table 6: Number and percentage of correct answers in the ESL 50 questions test. Confidence levels:
 =0.95,  =0.99

Lin (1998) (L), and Jiang and Conrath (1997) (JC). We report the results, together with random
guessing, in Table 6. The results of Table 6 show that SR ranks first, having the same performance
with JS in this data set, both outperforming all of the compared corpus-based methods. These
26

fiT EXT R ELATEDNESS BASED ON A W ORD T HESAURUS

Category
Lexicon-Based
Corpus-Based
Hybrid
Web-Based
Other

Method
LC
HS
V
LRA
PMI-IR
R
L
JC
B
RG
S1
S2
S
NB
UB

#Correct Answers
117
120
161
210
131
124
102
102
150
75
106
114
128
142
196

Percentage of Correct Answers
0.313
0.321
0.43
0.561
0.35
0.332
0.273
0.273
0.4
0.2
0.283
0.304
0.342
0.381
0.524

Table 7: Number and percentage of correct answers in the 374 Scholastic Aptitude Test (SAT)
questions. Confidence levels:  =0.90,  =0.95,  =0.99

results are very interesting, since they indicate that lexicon-based methods are very promising in the
synonym identification tasks.
4.1.3 E VALUATION

ON

SAT A NALOGY Q UESTIONS

The approach that we choose to evaluate SR in the analogy task is to use the typical benchmark test
set employed in the related bibliography, namely the Scholastic Aptitude Test (SAT).11 It comprises
of 374 words pairs and for each target pair 5 supplementary pairs of words. The average US college
applicant answered correctly only the 57 percent of the questions, and no machine-based approach
has yet surpassed the performance of the average college applicant.
In Table 7, we present the number of correct answers and the respective percentage (recall) on
the 374 SAT questions, of the following methods: random guessing (RG), Jiang and Conrath (1997)
(JC), Lin (1998) (L), Leacock et al. (1998) (LC), Hirst and St-Onge (1998) (HS), Resnik (1995)
(R), Bollegala et al. (2008) (B), Veale (2004) (V), PMI-IR (Turney, 2001) and LRA (Turney, 2006).
Furthermore, we present the results of s1 (Equation 11), s2 (Equation 12) and s (Equation 13). We
also present, as before, the statistical significance of the differences in performance, conducting
Fishers exact test.
Towards the direction of combining the answers of s1 and s2 in a different manner than the
naive average, we also report the upper bound performance of such an attempt. This is computed
by simply finding the union of the correct answers that s1 and s2 may provide. This is reported
in the table as (UB). In an effort to design a learning mechanism that would learn when to select
11. Many thanks to Peter Turney, for providing us with a standard set for experimentation, comprising of 374 SAT questions.

27

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

s1 or s2 answers for each SAT question, with the goal to reach our upper-bound, we designed
and implemented a simple representation of the SAT questions as training instances. For each
SAT question, we created a training instance that has 6 features: the minimum s1 value found for
this question (among the five computed values for all the possible pairs), the maximum s1 value,
and their difference. We also added the same features regarding s2 . We then trained and tested a
Naive Bayes classifier using ten-fold cross validation in the 374 SAT questions. The results of this
experiment are shown in the table as NB (Naive Bayes). Finally, we also present the top results ever
reported in the literature for the specific data set, which is the LRA method by Turney (2006). This
is reported in the table as (LRA).
The results presented in Table 7 show that S ranks second among the compared lexicon-based
measures with the first being the measure of Veale (2004) (V). The method of Bollegala et al. (2008)
(B) achieves higher score than SR, but needs training in SAT questions. At this point we have to
note that the LRA method needs almost 8 days to process the 374 SAT questions (Turney, 2006),
(B) needs around 6 hours (Bollegala et al., 2008), while S needs less than 3 minutes.
Furthermore, the fact that combining s1 and s2 can reach 52.4% shows that S can produce very
promising results, if a classifier learns successfully how to combine them. The N B results, which
are a simple attempt to construct such a learner with few features, shows an important boost in
performance of 4.1%. A proper feature engineering in the task, and more training SAT questions
can potentially yield more promising results, as the gap between 38.1% and the upper bound of
52.4% is still large. In all, these results prove that our lexicon-based relatedness measure has a
comparable performance to the state of the art measures for the SAT task, while it has smaller
execution time than the majority of the methods which outperform it in recall.
4.2 Evaluation of the Omiotis Measure
In order to evaluate the performance of the Omiotis measure, we performed two experiments which
test the ability of the measure to capture the similarity between sentences. The first experiment is
based on the data set produced by Li et al. (2006). The second experiment is based on the paraphrase
recognition task, using the Microsoft Research Paraphrase Corpus (Dolan et al., 2004).
4.2.1 E VALUATION

ON

S ENTENCE S IMILARITY

The data set produced by Li et al. (2006) comprises 65 sentence pairs (each pair consists of two
sentences that are the respective dictionary word definitions of the R&G 65 word pairs data set).
The used dictionary was the Collins Cobuild dictionary (Sinclair, 2001). For each sentence pair,
similarity scores have been provided by 32 human participants, ranging from 0.0 (the sentences are
unrelated in meaning), to 4.0 (the sentences are identical in meaning).12 .
From the 65 sentence pairs, Li et al. (2006) decided to keep a subset of 30 sentence pairs,
similarly to the process applied by Miller and Charles (1991), in order to retain the sentences whose
human ratings create a more even distribution across the similarity range. Thus, we apply Omiotis
in this same subset of the 65 sentence pairs, described by Li et al. (2006). In this data set, we
compare Omiotis against the STASIS measure of semantic similarity, proposed by Li et al. (2006),
an LSA-based approach described by OShea et al. (2008), and the STS measure proposed by Islam
and Inkpen (2008). To the best of our knowledge, this data set has only been used by these three
12. The data set is publicly available at http://www.docm.mmu.ac.uk/STAFF/J.Oshea/

28

fiT EXT R ELATEDNESS BASED ON A W ORD T HESAURUS

previous works. In Table 8 we present the sentence pairs used, and the respective scores by humans,
STASIS, LSA, STS, and Omiotis.
Sentence Pair
1.cord:smile
5.autograph:shore
9.asylum:fruit
13.boy:rooster
17.coast:forest
21.boy:sage
25.forest:graveyard
29.bird:woodland
33.hill:woodland
37.magician:oracle
41.oracle:sage
47.furnace:stove
48.magician:wizard
49.hill:mound
50.cord:string
51.glass:tumbler
52.grin:smile
53.serf:slave
54.journey:voyage
55.autograph:signature
56.coast:shore
57.forest:woodland
58.implement:tool
59.cock:rooster
60.boy:lad
61.cushion:pillow
62.cemetery:graveyard
63.automobile:car
64.midday:noon
65.gem: jewel

Human
0.01
0.005
0.005
0.108
0.063
0.043
0.065
0.013
0.145
0.13
0.283
0.348
0.355
0.293
0.47
0.138
0.485
0.483
0.36
0.405
0.588
0.628
0.59
0.863
0.58
0.523
0.773
0.558
0.955
0.653

STASIS
0.329
0.287
0.209
0.53
0.356
0.512
0.546
0.335
0.59
0.438
0.428
0.721
0.641
0.739
0.685
0.649
0.493
0.394
0.517
0.55
0.759
0.7
0.753
1
0.663
0.662
0.729
0.639
0.998
0.831

LSA
0.51
0.53
0.505
0.535
0.575
0.53
0.595
0.505
0.81
0.58
0.575
0.715
0.615
0.54
0.675
0.725
0.695
0.83
0.61
0.7
0.78
0.75
0.83
0.985
0.83
0.63
0.74
0.87
1
0.86

STS
0.06
0.11
0.07
0.16
0.26
0.16
0.33
0.12
0.29
0.20
0.09
0.30
0.34
0.15
0.49
0.28
0.32
0.44
0.41
0.19
0.47
0.26
0.51
0.94
0.60
0.29
0.51
0.52
0.93
0.65

Omiotis
0.1062
0.1048
0.1046
0.3028
0.2988
0.243
0.2995
0.1074
0.4946
0.1085
0.1082
0.2164
0.5295
0.5701
0.5502
0.5206
0.5987
0.4965
0.4255
0.4287
0.9308
0.612
0.7392
0.9982
0.9309
0.3466
0.7343
0.7889
0.9291
0.8194

Table 8: Human, STASIS, LSA, STS and Omiotis scores for the 30 sentence pairs.
In Table 9 we present the results of the comparison, comprising the reporting of the Spearmans
rank order correlation coefficient  and the Pearsons product moment correlation coefficient r for
STASIS, LSA, STS, and Omiotis. We have also included in the results, a version of Omiotis that
does not take into account the inter-POS relations (i.e., relations that cross parts of speech). This
version of Omiotis is indicated in the table as SimpleOmiotis. The objective of this experiment was
to measure the contribution of the relations that cross parts of speech in the computation of text-to29

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

STASIS
LSA
STS
Simple Omiotis
Omiotis
Average Participant
Worst Participant
Best Participant

Spearmans 
0.8126
0.8714
0.838
0.6889
0.8905
N/A
N/A
N/A

Pearsons r
0.8162
0.8384
0.853
0.7277
0.856
0.825
0.594
0.921

Table 9: Spearmans and Pearsons correlations with human similarity ratings. Confidence levels:
 =0.95,  =0.99

text semantic relatedness values, though these types of relations have been reported in the previous
bibliography as advantageous (Jarmasz, 2003; Jarmasz & Szpakowicz, 2003), but their individual
contribution had never been measured.
We also show the r correlation between the average participant (mean of individuals with group;
n = 32, leave-one-out resampling and standard deviation 0.072), the worst participant (worst participant with group; n = 32, leave-one-out resampling) and the best participant (best participant
with group; n = 32, leave-one-out resampling), taken from the work of OShea et al. (2008). In
addition, we have also conducted a z-test regarding the difference between Omiotis correlations and
the compared measures correlations. The symbols used in the previous tables indicate the confidence level of the statistical significance. Note, also, that the reported correlations (STASIS, LSA,
STS, and Omiotis) individually constitute statistically significant positive correlations with the human scores (r) and rankings (). As the results indicate, Omiotis has the best correlation, according
to  and r values, compared to STASIS, LSA, and STS. Furthermore, the contribution of the semantic relations that cross parts of speech is obvious, since the difference between the simple version of
Omiotis that omits them and the defined Omiotis measure is large and statistically significant at the
0.99 confidence level. Overall, the results indicate that Omiotis can be applied successfully to the
computation of similarities between small text segments, like sentences.
4.2.2 E VALUATION

ON

PARAPHRASE R ECOGNITION

In order to further evaluate the performance of Omiotis in measuring the semantic relatedness between small text segments, we conducted additional experiments on the paraphrase recognition task
using the test pairs of the Microsoft Research Paraphrase Corpus (Dolan et al., 2004). From the
original data set, containing both training and test pairs, we run experiments only on the 1725 test
pairs of text segments, which have been collected from news sources on the Web over a period of
18 months. For each pair, human subjects have determined whether any of the two texts in the pair
consists a paraphrase of the other (direction is not an issue). The reported inter-judge agreement
between annotators is 83%. The paraphrase recognition task has been widely studied in the past,
since it is very important in many natural language applications, like question answering (Harabagiu
30

fiT EXT R ELATEDNESS BASED ON A W ORD T HESAURUS

Category
Baselines
Corpus-based

Lexicon-based

Machine learning-based

Method
Random
VSM and Cosine
PMI-IR
LSA
STS
JC
LC
Lesk
L
WP
R
Comb.
Wan et al.
Zhang and Patrick
Qiu et al.
Finch et al.
Omiotis

Accuracy
51.3
65.4
69.9
68.4
72.6
69.3
69.5
69.3
69.3
69
69
70.3
75
71.9
72
74.96
69.97

Precision
68.3
71.6
70.2
69.7
74.7
72.2
72.4
72.4
71.6
70.2
69
69.6
77
74.3
72.5
76.58
70.78

Recall
50
79.5
95.2
95.2
89.1
87.1
87
86.6
88.7
92.1
96.4
97.7
90
88.2
93.4
89.8
93.4

F-Measure
57.8
75.3
81
80.5
81.3
79
79
78.9
79.2
80
80.4
81.3
83
80.7
81.6
82.66
80.52

Table 10: Omiotis and competitive methods performance on the Microsoft Research Paraphrase
Corpus (MSR).

& Hickl, 2006), and text summarization (Madnani, Zajic, Dorr, Fazil Ayan, & Lin, 2007). For this
task we computed Omiotis between the sentences of every pair and marked as paraphrases only
those pairs with Omiotis value greater than a threshold. The threshold was set to 0.2, after tuning in
the training set. We used a simple approach for the tuning, namely forward hill-climbing and beam
search (Guyon, Gunn, Nikravesh, & Zadeh, 2006).
We compare the performance of Omiotis against several other methods of various categories;
more precisely, against: (a) two baseline methods, a random selection method that marks randomly
each pair as being paraphrase of not (Random), and a vector-based similarity measure, using the
cosine similarity measure and TF-IDF weighting for the features (VSM and Cosine) 13 , (b) corpusbased methods; the PMI-IR proposed by Turney (2001), an LSA-based approach introduced by
Mihalcea et al. (2006), and the STS measure proposed by Islam and Inkpen (2008), (c) lexiconbased methods; Jiang and Conrath (1997) (JC), Leacock et al. (1998) (LC), Lin (1998) (L), Resnik
(1995, 1999) (R), Lesk (1986) (Lesk), Wu and Palmer (1994) (WP), and a metric that combines the
measures of this category, proposed by Mihalcea et al. (2006) (Comb.), and (d) machine-learning
based techniques, which also constitute the state of the art in paraphrase recognition, like the method
of Wan et al. (2006), which trains a classifier with lexical and dependency similarity measures,
the method of Zhang and Patrick (2005), who also build a feature vector with lexical similarities
between the sentence pairs (e.g., edit distance, number of common words), the method of Qiu et al.
13. The features are all words of the used data set.

31

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

(2006), who use an SVM classifier (Vapnik, 1995) to decide whether or not a set of features for each
sentence that has been created by parsing and semantic role labelling matches or not the respective
set of the second sentence in the pair, and with what importance, and, finally, the method of Finch
et al. (2005), who also train an SVM classifier based on machine translation evaluation metrics.
The results of the evaluation are shown in Table 10. The results indicate that Omiotis surpasses
all the lexicon-based methods, and matches the combined method of Mihalcea et al. (2006). At
this point we must mention that we also tuned Omiotis with a goal to maximize F-Measure in the
test set, at the cost of dropping precision in favor of recall. This type of tuning reported an FMeasure of 81.7, which is larger than the F-Measures of the lexicon-based, the corpus-based and
two of the machine learning-based approaches. Even though the reported results used a different
and simpler tuning explained previously, still the results indicate that Omiotis manages very well
in the paraphrase recognition task and produces comparable results with the state of the art. We
believe that it can be used as part of a machine learning-based method, since it is one of the best
choices in lexicon-based methods for paraphrase recognition, and this also constitutes part of our
plan for future work in this application.

5. Conclusions and Future Work
In this paper we presented a new measure of text semantic relatedness. The major strength of this
measure lies in the formulation of the semantic relatedness between words. Experimental evaluation, proved that our measure approximates human understanding of semantic relatedness between
words better than previous related measures. The combination of path length, nodes depth and
edges type in a single formula allowed us to apply our semantic relatedness measure to different
text-based tasks with very good performance. More specifically, the SR measure outperformed overall in the used data sets all state of the art measures in word-to-word tasks and the Omiotis measure
performed very well in the sentence similarity and the paraphrase recognition tasks. Although, the
results in the word analogy task are satisfactory, since no special tuning has been performed, we
are confident that there is still place for improvement. The extensive evaluation of SR and Omiotis
in several applications shows the capabilities of our measures and proves that both can be applied
to several text related tasks. It is on our next plans to apply our relatedness measures to more applications, such as text classification and clustering, keyword and sentence extraction, and query
expansion, and compare with state of the art techniques in each field. Finally, we are improving our
supporting infrastructure in order to facilitate large scale tasks such as document clustering and text
retrieval.

Acknowledgments
Part of this work was done while George Tsatsaronis was at the Department of Informatics of Athens
University of Economics and Business. We would like to thank Kjetil Nrvag for his constructive
comments, and Ion Androutsopoulos for his feedback on the early stage of this work. We would
also like to thank the anonymous reviewers for their detailed feedback.
32

fiT EXT R ELATEDNESS BASED ON A W ORD T HESAURUS

References
Agirre, E., Alfonseca, E., Hall, K., Kravalova, J., Pasca, M., & Soroa, A. (2009). A study on
similarity and relatedness using distributional and wordnet-based approaches.. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American
Chapter of the Association for Computational Linguistics (NAACL), pp. 1927.
Agirre, E., & Rigau, G. (1995). A proposal for word sense disambiguation using conceptual distance. In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP).
Agresti, A. (1990). Categorical Data Analysis. Wiley, Hoboken, NJ.
Aizawa, A. (2003). An information-theoretic perspective of TF-IDF measures. Information Processing and Management, 39(1), 4565.
Banerjee, S., & Pedersen, T. (2003). Extended gloss overlaps as a measure of semantic relatedness.
In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence
(IJCAI), pp. 805810.
Barzilay, R., & Elhadad, M. (1997). Using lexical chains for text summarization. In Proceedings of
the ACL 97/EACL 97 Workshop on Intelligent Scalable Text Summarization, pp. 1017.
Barzilay, R., Elhadad, M., & McKeown, K. (2002). Inferring strategies for sentence ordering in
multidocument news summarization. Journal of Artificial Intelligence Research, 17, 3555.
Basili, R., Cammisa, M., & Moschitti, A. (2005). A semantic kernel to exploit linguistic knowledge. In Proceedings of Advances in Artificial Intelligence, Ninth Congress of the Italian
Association for Artificial Intelligence (AI*IA), pp. 290302.
Bicici, E., & Yuret, D. (2006). Clustering word pairs to answer analogy questions. In Proceedings
of the Fifteenth Turkish Symposium on Artificial Intelligence and Neural Networks.
Bollegala, D., Matsuo, Y., & Ishizuka, M. (2008). WWW sits the sat: Measuring relational similarity from the web. In Proceedings of the Eighteenth European Conference on Artificial
Intelligence (ECAI), pp. 333337.
Budanitsky, A., & Hirst, G. (2006). Evaluating wordnet-based measures of lexical semantic relatedness. Computational Linguistics, 32(1), 1347.
Clough, P., & Stevenson, M. (2004). Cross-language information retrieval using EuroWordNet and
word sense disambiguation. In Proceedings of the Twenty Sixth European Conference on
Information Retrieval (ECIR), pp. 327337.
Cormen, T., Leiserson, C., & Rivest, R. (1990). Introduction to Algorithms. The MIT Press.
Dolan, W., Quirk, C., & Brockett, C. (2004). Unsupervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Proceedings of the Twentieth International
Conference on Computational Linguistics (COLING).
Fellbaum, C. (1998). WordNet  an electronic lexical database. MIT Press.
33

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

Finch, A., Hwang, Y., & Sumita, E. (2005). Using machine translation evaluation techniques to determine sentence-level semantic equivalence. In Proceedings of the 3rd International Workshop on Paraphrasing,.
Finkelstein, L., Gabrilovich, E., Matias, Y., Rivlin, E., Solan, Z., Wolfman, G., & Ruppin, E. (2002).
Placing search in context: The concept revisited. ACM Transactions on Information Systems,
20(1), 116131.
Fisher, R. (1915). Frequency distribution of the values of the correlation coefficient in samples of
an indefinitely large population. Biometrika, 10, 507521.
Gabrilovich, E., & Markovitch, R. (2007). Computing semantic relatedness using Wikipedia-based
explicit semantic analysis. In Proceedings of the Twentieth International Joint Conference on
Artificial Intelligence (IJCAI), pp. 16061611.
Gabrilovich, E., & Markovitch, R. (2009). Wikipedia-based semantic interpretation for natural
language processing. Journal of Artificial Intelligence Research, 34, 443498.
Gentner, D. (1983). Structure-mapping: A theoretical framework for analogy. Cognitive Science,
7(2), 155170.
Guyon, I., Gunn, S., Nikravesh, M., & Zadeh, L. (2006). Feature Extraction, Foundations and
Applications. Springer.
Harabagiu, S., & Hickl, A. (2006). Methods for using textual entailment in open-domain question
answering.. In Proceedings of the Joint Conference of the International Committee on Computational Linguistics and the Association for Computational Linguistics (COLING-ACL),
pp. 905912.
Hirao, T., Fukusima, T., Okumura, M., Nobata, C., & Nanba, H. (2005). Corpus and evaluation
measures for multiple document summarization with multiple sources. In Proceedings of the
Twentieth International Conference on Computational Linguistics (COLING), pp. 535541.
Hirst, G., & St-Onge, D. (1998). Lexical chains as representations of context for the detection and
correction of malapropisms. In WordNet: An Electronic Lexical Database, chapter 13, pp.
305332 Cambridge. The MIT Press.
Hughes, T., & Ramage, D. (2007). Lexical semantic relatedness with random graph walks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),
pp. 581589.
Ide, N., & Veronis, J. (1998). Word Sense Disambiguation: The State of the Art. Computational
Linguistics, 24(1), 140.
Islam, A., & Inkpen, D. (2008). Semantic text similarity using corpus-based word similarity and
string similarity. ACM Transactions on Knowledge Discovery from Data, 2(2), 125.
Jaccard, P. (1901). Etude comparative de la distribution florale dans une portion des alpes et des
jura.. Bulletin del la Societe Vaudoise des Sciences Naturelles, 37, 547579.
34

fiT EXT R ELATEDNESS BASED ON A W ORD T HESAURUS

Jarmasz, M. (2003). Rogets thesaurus and semantic similarity. Masters Thesis, University of
Ottawa.
Jarmasz, M., & Szpakowicz, S. (2003). Rogets thesaurus and semantic similarity. In Proceedings of
the International Conference on Recent Advances in Natural Language Processing (RANLP),
pp. 212219.
Jiang, J., & Conrath, D. (1997). Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of the International Conference Research on Computational Linguistics
(ROCLING X), pp. 1933.
Kucera, H., Francis, W., & Caroll, J. (1967). Computational Analysis of Present Day American
English. Brown University Press.
Landauer, T., & Dumais, S. (1997). A solution to Platos problem: The latent semantic analysis
theory of the acquisition, induction, and representation of knowledge. Psychological Review,
104(2), 211240.
Landauer, T., Foltz, P., & Laham, D. (1998). Introduction to latent semantc analysis. Discourse
Processes, 25, 259284.
Leacock, C., Miller, G., & Chodorow, M. (1998). Using corpus statistics and WordNet relations for
sense identification. Computational Linguistics, 24(1), 147165.
Lesk, M. (1986). Automated sense disambiguation using machine-readable dictionaries: How to
tell a pine cone from an ice cream cone. In Proceedings of the Fifth Annual International
Conference on Systems Documentation (SIGDOC), pp. 2426.
Li, P. (2008). Estimators and tail bounds for dimension reduction in l (0 <   2) using stable
random projections. In Proceedings of the Nineteenth Annual ACM-SIAM Symposium on
Discrete Algorithms (SODA), pp. 1019.
Li, Y., McLean, D., Bandar, Z., OShea, J., & Crockett, K. (2006). Sentence similarity based on
semantic nets and corpus statistics. IEEE Transactions on Knowledge and Data Engineering,
18(8), 11381150.
Lin, D. (1998). An information-theoretic definition of similarity. In Proceedings of the Fifteenth
International Conference on Machine Learning (ICML), pp. 296304.
Madnani, N., Zajic, D., Dorr, B., Fazil Ayan, N., & Lin, J. (2007). Multiple alternative sentence
compressions for automatic text summarization. In Proceedings of the HLT/NAACL Document Understanding Conference (DUC).
Matveeva, I., Levow, G., Farahat, A., & Royer, C. (2005). Generalized latent semantic analysis for
term representation. In Proceedings of the International Conference on Recent Advances in
Natural Language Processing (RANLP).
Mavroeidis, D., Tsatsaronis, G., Vazirgiannis, M., Theobald, M., & Weikum, G. (2005). Word sense
disambiguation for exploiting hierarchical thesauri in text classification. In Proceedings of the
Ninth European Conference on Principles and Practice of Knowledge Discovery in Databases
(PKDD), pp. 181192.
35

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

Mihalcea, R., Corley, C., & Strapparava, C. (2006). Corpus-based and knowledge-based measures
of text semantic similarity. In Proceedings of the Twenty First Conference on Artificial Intelligence (AAAI), pp. 775780.
Mihalcea, R., & Moldovan, D. (1999). A method for word sense disambiguation of unrestricted text.
In Proceedings of the 37th annual meeting of the Association for Computational Linguistics
(ACL), pp. 152158.
Mihalcea, R., Tarau, P., & Figa, E. (2004). PageRank on semantic networks with application to
word sense disambiguation. In Proceedings of the Twentieth International Conference on
Computational Linguistics (COLING).
Miller, G., & Charles, W. (1991). Contextual correlates of semantic similarity. Language and
Cognitive Processes, 6(1), 128.
Milne, D., & Witten, I. (2008). An effective, low-cost measure of semantic relatedness obtained
from Wikipedia links. In Proceedings of the first AAAI Workshop on Wikipedia and Artificial
Intelligence (WIKIAI).
Morris, J., & Hirst, G. (1991). Lexical cohesion computed by thesaural relations as an indicator of
the structure of text. Computational Linguistics, 17, 2148.
Navigli, R. (2008). A structural approach to the automatic adjudication of word sense disagreements. Natural Language Engineering, 14(4), 547573.
OShea, J., Bandar, Z., Crocket, K., & McLean, D. (2008). A comparative study of two short
text semantic similarity measures. In Proceedings of the Agent and Multi-Agent Systems:
Technologies and Applications, Second KES International Symposium (KES-AMSTA), pp.
172181.
Pado, S., & Lapata, M. (2007). Dependency-based construction of semantic space models. Computational Linguistics, 33(2), 161199.
Palmer, M., Fellbaum, C., & Cotton, S. (2001). English tasks: All-words and verb lexical sample.
In Proceedings of Senseval-2, pp. 2124.
Pasca, M. (2003). Open-domain question answering from large text collections. In CSLI Studies
in Computational Linguistics. CSLI Publications, Distributed by the University of Chicago
Press.
Pasca, M. (2005). Mining paraphrases from self-anchored web sentence fragments. In Proceedings
of the Ninth European Conference on Principles and Practice of Knowledge Discovery in
Databases (PKDD), pp. 193204.
Patwardhan, S., Banerjee, S., & Pedersen, T. (2003). Using measures of semantic relatedness for
word sense disambiguation. In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics (CICLing), pp. 241257.
Patwardhan, S., & Pedersen, T. (2006). Using WordNet based context vectors to estimate the semantic relatedness of concepts. In Proceedings of the EACL 2006 Workshop Making Sense of
Sense - Bringing Computational Linguistics and Psycholinguistics Together, pp. 18.
36

fiT EXT R ELATEDNESS BASED ON A W ORD T HESAURUS

Ponzetto, S., & Strube, M. (2007a). Knowledge derived from Wikipedia for computing semantic
relatedness. Journal of Artificial Intelligence Research, 30, 181212.
Ponzetto, S., & Strube, M. (2007b). Deriving a large-scale taxonomy from Wikipedia. In Proceedings of the Twenty Second Conference on Artificial Intelligence (AAAI), pp. 14401445.
Qiu, L., Kan, M., & Chua, T. (2006). Paraphrase recognition via dissimilarity significance classification. In Proceedings of the Conference on Empirical Methods in Natural Language
Processing (EMNLP), pp. 1826.
Quilian, R. (1969). The teachable language comprehender: a simulation program and theory of
language. Communications of ACM, 12(8), 459476.
Resnik, P. (1995). Using information content to evaluate semantic similarity. In Proceedings of the
Fourteenth International Joint Conference on Artificial Intelligence (IJCAI), pp. 448453.
Resnik, P. (1999). Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language. Journal of Artificial Intelligence
Research, 11, 95130.
Richardson, R., & Smeaton, A. (1995). Using WordNet in a knowledge-based approach to information retrieval. In Proceedings of the BCS-IRSG Colloquium.
Rubenstein, H., & Goodenough, J. (1965). Contextual correlates of synonymy. Communications of
the ACM, 8(10), 627633.
Ruiz-Casado, M., Alfonseca, E., & Castells, P. (2005). Using context-window overlapping in synonym discovery and ontology extension. In Proceedings of the International Conference on
Recent Advances in Natural Language Processing (RANLP).
Salton, G., Buckley, C., & Yu, C. (1982). An evaluation of term dependence models in information retrieval. In Proceedings of the Fifth Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval, pp. 151173.
Salton, G., & McGill, M. (1983). Introduction to Modern Information Retrieval. McGraw-Hill.
Sanderson, M. (1994). Word sense disambiguation and information retrieval. In Proceedings of the
Seventeenth Annual International ACM SIGIR Conference on Research and Development in
Information Retrieval, pp. 142151.
Sanderson, M. (2008). Ambiguous queries: Test collections need more sense. In Proceedings of
the Thirty First Annual International ACM SIGIR Conference on Research and Development
in Information Retrieval, pp. 499506.
Shinyama, Y., & Sekine, S. (2003). Paraphrase acquisition for information extraction. In Proceedings of the ACL 2nd Workshop on Paraphrasing: Paraphrase Acquisition and Applications,
pp. 6571.
Sinclair, J. (2001). Collins Cobuild English Dictionary for Advanced Learners, 3rd edn. Harper
Collins, New York.
37

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

Smeaton, A., Kelledy, F., & ODonnell, R. (1995). TREC-4 experiments at Dublin City University:
Thresholding posting lists, query expansion with WordNet and POS tagging of Spanish. In
Proceedings of the Fourth Text REtrieval Conference (TREC).
Snyder, B., & Palmer, M. (2004). The English All-words task. In Proceedings of Senseval-3, pp.
4143.
Song, Y., Han, K., & Rim, H. (2004). A term weighting method based on lexical chain for automatic summarization. In Proceedings of the Fifth International Conference on Intelligent Text
Processing and Computational Linguistics (CICLing), pp. 636639.
Stokoe, C., Oakes, M., & Tait, J. (2003). Word sense disambiguation in information retrieval revisited. In Proceedings of the Twenty Sixth Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval, pp. 159166.
Strube, M., & Ponzetto, S. (2006). WikiRelate! Computing semantic relatedness using Wikipedia.
In Proceedings of the Twenty First Conference on Artificial Intelligence (AAAI), pp. 1419
1424.
Sussna, M. (1993). Word sense disambiguation for free-text indexing using a massive semantic network. In Proceedings of the Second International Conference on Information and Knowledge
Management (CIKM), pp. 6774.
Terra, E., & Clarke, C. (2003). Frequency estimates for statistical word similarity measures. In
Proceedings of the North American Chapter of the Association for Computational Linguistics
- Human Language Technologies Conference (HLT/NAACL)., pp. 244251.
Tsang, V. (2008). A Graph Approach to Measuring Text Distance. PhD Thesis, University of
Toronto.
Tsatsaronis, G., & Panagiotopoulou, V. (2009). A generalized vector space model for text retrieval
based on semantic relatedness. In Proceedings of the the 12th Conference of the European
Chapter of the Association for Computational Linguistics (EACL - Student Research Workshop), pp. 7078.
Tsatsaronis, G., Varlamis, I., Nrvag, K., & Vazirgiannis, M. (2009). Omiotis: A thesaurus-based
measure of text relatedness. In Proceedings of the European Conference on Machine Learning
and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD), pp. 742
745.
Tsatsaronis, G., Varlamis, I., & Vazirgiannis, M. (2008). Word sense disambiguation with semantic
networks. In Proceedings of the 11th International Conference on Text, Speech and Dialogue
(TSD), pp. 219226.
Tsatsaronis, G., Vazirgiannis, M., & Androutsopoulos, I. (2007). Word sense disambiguation with
spreading activation networks generated from thesauri. In Proceedings of the Twentieth International Joint Conference on Artificial Intelligence (IJCAI), pp. 17251730.
Turney, P. (2001). Mining the Web for synonyms: PMI-IR versus LSA on TOEFL. In Proceedings
of the Twelfth European Conference on Machine Learning (ECML), pp. 491502.
38

fiT EXT R ELATEDNESS BASED ON A W ORD T HESAURUS

Turney, P. (2006). Similarity of semantic relations. Computational Linguistics, 32(3), 379416.
Turney, P. (2008a). The latent relation mapping engine: Algorithm and experiments. Journal of
Artificial Intelligence Research, 33, 615655.
Turney, P. (2008b). A uniform approach to analogies, synonyms, antonyms, and associations. In
Proceedings of the Twenty Second International Conference on Computational Linguistics
(COLING), pp. 905912.
Turney, P., & Littman, M. (2005). Corpus-based learning of analogies and semantic relations.
Machine Learning, 60(1-3), 251278.
Turney, P., Littman, M., Bigham, J., & Shnayder, V. (2003). Combining independent modules to
solve multiple-choice synonym and analogy problems. In Proceedings of the International
Conference on Recent Advances in Natural Language Processing (RANLP), pp. 482489.
van Rijsbergen, C. (1979). Information Retrieval. Butterworth.
Vapnik, V. (1995). The nature of statistical learning theory. Springer.
Veale, T. (2004). WordNet sits the SAT: A knowledge-based approach to lexical analogy. In Proceedings of the Sixteenth European Conference on Artificial Intelligence (ECAI), pp. 606
612.
Veronis, J., & Ide, N. (1990). Word sense disambiguation with very large neural networks extracted
from machine readable dictionaries. In Proceedings of the Thirteenth International Conference on Computational Linguistics (COLING), pp. 389394.
Voorhees, E. (1993). Using WordNet to disambiguate word sense for text retrieval. In Proceedings
of the Sixteenth Annual International ACM SIGIR Conference on Research and Development
in Information Retrieval, pp. 171180.
Wan, S., Dras, M., Dale, R., & Paris, C. (2006). Using dependency-based features to take the parafarce out of paraphrase. In Proceedings of the Australasian Language Technology Workshop,
pp. 131138.
Wu, Z., & Palmer, M. (1994). Verb semantics and lexical selection. In Proceedings of the Thirty
Second Annual Meeting of the Association for Computational Linguistics (ACL), pp. 133
138.
Zhang, Y., & Patrick, J. (2005). Paraphrase identification by text canonicalization. In Proceedings
of the Australasian Language Technology Workshop, pp. 160166.

39

fiJournal of Artificial Intelligence Research 37 (2010) 329-396

Submitted 08/09; published 3/10

An Investigation into Mathematical Programming
for Finite Horizon Decentralized POMDPs
Raghav Aras

raghav.aras@gmail.com

IMS, Suplec Metz
2 rue Edouard Belin, Metz Technopole
57070 Metz - France

Alain Dutech

alain.dutech@loria.fr

MAIA - LORIA/INRIA
Campus Scientifique - BP 239
54506 Vandoeuvre les Nancy - France

Abstract
Decentralized planning in uncertain environments is a complex task generally dealt with
by using a decision-theoretic approach, mainly through the framework of Decentralized Partially Observable Markov Decision Processes (DEC-POMDPs). Although DEC-POMDPS
are a general and powerful modeling tool, solving them is a task with an overwhelming
complexity that can be doubly exponential. In this paper, we study an alternate formulation of DEC-POMDPs relying on a sequence-form representation of policies. From this
formulation, we show how to derive Mixed Integer Linear Programming (MILP) problems
that, once solved, give exact optimal solutions to the DEC-POMDPs. We show that these
MILPs can be derived either by using some combinatorial characteristics of the optimal
solutions of the DEC-POMDPs or by using concepts borrowed from game theory. Through
an experimental validation on classical test problems from the DEC-POMDP literature, we
compare our approach to existing algorithms. Results show that mathematical programming outperforms dynamic programming but is less efficient than forward search, except
for some particular problems.
The main contributions of this work are the use of mathematical programming for DECPOMDPs and a better understanding of DEC-POMDPs and of their solutions. Besides,
we argue that our alternate representation of DEC-POMDPs could be helpful for designing
novel algorithms looking for approximate solutions to DEC-POMDPs.

1. Introduction
The framework of Decentralized Partially Observable Markov Decision Processes (DECPOMDPs) can be used to model the problem of designing a system made of autonomous
agents that need to coordinate in order to achieve a joint goal. Solving DEC-POMDPs is an
untractable task as they belong to the class of NEXP-complete problems (see Section 1.1).
In this paper, DEC-POMDPs are reformulated into sequence-form DEC-POMDPs so as
to derive Mixed Integer Linear Programs that can be solved using very efficient solvers
in order to design exact optimal solutions to finite-horizon DEC-POMDPs. Our main
motivation is to investigate the benefits and limits of this novel approach and to get a
better understanding of DEC-POMDPs (see Section 1.2). On a practical level, we provide
new algorithms and heuristics for solving DEC-POMDPs and evaluate them on classical
problems (see Section 1.3).
c
2010
AI Access Foundation. All rights reserved.

fiAras & Dutech

1.1 Context
One of the main goals of Artificial Intelligence is to build artificial agents that exhibit
intelligent behavior. An agent is an entity situated in an environment which it can perceive
through sensors and act upon using actuators. The concept of planning, i.e., to select
a sequence of actions in order to reach a goal, has been central to the field of Artificial
Intelligence for years. While the notion of intelligent behavior is difficult to assess and to
measure, we prefer to refer to the concept of rational behavior as formulated by Russell
and Norvig (1995). As a consequence, the work presented here uses a decision-theoretic
approach in order to build agents that take optimal actions in an uncertain and partially
unknown environment.
We are more particularly interested in cooperative multi-agent systems where multiple
independent agents with limited perception of their environment must interact and coordinate in order to achieve a joint task. No central process with a full knowledge of the state
of the system is there to control the agents. On the contrary, each agent is an autonomous
entity that must execute its actions by itself. This setting is both a blessing, as each agent
should ideally deal with a small part of the problem, and a curse, as coordination and
cooperation are harder to develop and to enforce.
The decision-theoretic approach to rational behavior relies mostly on the framework of
Markov Decision Processes (MDP) (Puterman, 1994). A system is seen as a sequence of
discrete states with stochastic dynamics, some particular states giving a positive or negative
reward. The process is divided into discrete decision periods; the number of such periods
is called the horizon of the MDP. At each of these periods, an action is chosen which
will influence the transition of the process to its next state. By using the right actions to
influence the transition probabilities between states, the objective of the controller of the
system is to maximize its long term return, which is often an additive function of the reward
earned for the given horizon. If the controller knows the dynamics of the system, which is
made of a transition function and of a reward function, algorithms derived from the field
of Dynamic Programming (see Bellman, 1957) allow the controller to compute an optimal
deterministic policy, i.e., a decision function which associates an optimal action to every
state so that the expected long term return is optimal. This process is called planning in
the MDP community.
In fact, using the MDP framework, it is quite straightforward to model a problem with
one agent which has a full and complete knowledge of the state of the system. But agents,
and especially in a multi-agent setting, are generally not able to determine the complete
and exact state of the system because of noisy, faulty or limited sensors or because of the
nature of the problem itself. As a consequence, different states of the system are observed
as similar by the agent which is a problem when different optimal actions should be taken in
these states; one speaks then of perceptual aliasing. An extension of MDPs called Partially
Observable Markov Decisions Processes (POMDPs) deals explicitly with this phenomenon
and allows a single agent to compute plans in such a setting provided it knows the conditional
probabilities of observations given the state of the environment (Cassandra, Kaelbling, &
Littman, 1994).
As pointed out by Boutilier (1996), multi-agent problems could be solved as MDPs
if considered from a centralized point of view for planning and control. Here, although
330

fiMathematical Programming for DEC-POMDPs

planning is a centralized process, we are interested in decentralized settings where every
agent executes its own policy. Even if the agents could instantly communicate their observation, we consider problems where the joint observation resulting from such communications would still not be enough to identify the state of the system. The framework of
Decentralized Partially Observable Markov Decision Processes (DEC-POMDP) proposed
by Bernstein, Givan, Immerman, and Zilberstein (2002) takes into account decentralization
of control and partial observability. In a DEC-POMDP, we are looking for optimal joint
policies which are composed of one policy for each agent, these individual policies being
computed in a centralized way but then independently executed by the agents.
The main limitation of DEC-POMDPs is that they are provably untractable as they
belong to the class of NEXP-complete problems (Bernstein et al., 2002). Concretely, this
complexity result implies that, in the worst case, finding an optimal joint policy of a finite
horizon DEC-POMDP requires time that is exponential in the horizon if one always make
good choices. Because of this complexity, there are very few algorithms for finding exact
optimal solutions for DEC-POMDPs (they all have a doubly exponential complexity) and
only a few more that look for approximate solutions. As discussed and detailed in the
work of Oliehoek, Spaan, and Vlassis (2008), these algorithms follow either a dynamic
programming approach or a forward search approach by adapting concepts and algorithms
that were designed for POMDPs.
Yet, the concept of decentralized planning has been the focus of quite a large body of
previous work in other fields of research. For example, the Team Decision Problem (Radner,
1959), later formulated as a Markov system in the field of control theory by Anderson and
Moore (1980), led to the Markov Team Decision Problem (Pynadath & Tambe, 2002). In
the field of mathematics, the abundant literature on Game Theory brings a new way for
looking at multi-agent planning. In particular, a DEC-POMDP with finite horizon can
be thought as a game in extensive form with imperfect information and identical interests
(Osborne & Rubinstein, 1994).
Taking inspiration from the field of game theory and mathematical programming to design exact algorithms for solving DEC-POMDPs is precisely the subject of our contribution
to the field of decentralized multi-agent planning.
1.2 Motivations
The main objective of our work is to investigate the use of mathematical programming,
more especially mixed-integer linear programs (MILP) (Diwekar, 2008), for solving DECPOMDPs. Our motivation relies on the fact that the field of linear programming is quite
mature and of great interest to the industry. As a consequence, there exist many efficient
solvers for mixed-integer linear programs and we want to see how these efficient solvers
perform in the framework of DEC-POMDPs.
Therefore, we have to reformulate a DEC-POMDP to solve it as a mixed-integer linear
program. As shown in this article, two paths lead to such mathematical programs, one
grounded on the work from Koller, Megiddo, and von Stengel (1994), Koller and Megiddo
(1996) and von Stengel (2002), and another one grounded on combinatorial considerations.
Both methods rely on a special reformulation of DEC-POMDPs in what we have called
331

fiAras & Dutech

sequence-form DEC-POMDPs where a policy is defined by the histories (i.e., sequences of
observations and actions) it can generate when applied to the DEC-POMDP.
The basic idea of our work is to select, among all the histories of the DEC-POMDP,
the histories that will be part of the optimal policy. To that end, an optimal solution to
the MILP presented in this article will assign a positive weight to each history of the DECPOMDP and every history with a non-negative weight will be part of the optimal policy
to the DEC-POMDP. As the number of possible histories is exponential in the horizon of
the problem, the complexity of a naive search for the optimal set of histories is doubly
exponential. Therefore, our idea appears untractable and useless.
Nevertheless, we will show that combining the efficiency of MILP solvers with some quite
simple heuristics leads to exact algorithms that compare quite well to some existing exact
algorithms. In fact, sequence-form DEC-POMDPs only need a memory space exponential
in the size of the problem. Even if solving MILPs can also be exponential in the size of the
MILP and thus leads to doubly exponential complexity for sequence-form based algorithms,
we argue that sequence-form MILPs compare quite well to dynamic programming thanks
to optimized industrial MILP solvers like Cplex.
Still, our investigations and experiments with Mathematical Programming for DECPOMDPs do not solely aim at finding exact solutions to DEC-POMDPs. Our main motivation is to have a better understanding of DEC-POMDPs and of the limits and benefits of
the mathematical programming approach. We hope that this knowledge will help deciding
to what extent mathematical programming and sequence-form DEC-POMDPs can be used
to design novel algorithms that look for approximate solutions to DEC-POMDPs.
1.3 Contributions
In this paper we develop new algorithms in order to find exact optimal joint policies for
DEC-POMDPs. Our main inspiration comes from the work of Koller, von Stegel and
Megiddo that shows how to solve games in extensive form with imperfect information and
identical interests, that is how to find a Nash equilibrium for this kind of game (Koller et al.,
1994; Koller & Megiddo, 1996; von Stengel, 2002). Their algorithms caused a breakthrough
as the memory space requirement of their approach is linear in the size of the game whereas
more canonical algorithms required space that is exponential in the size of the game. This
breakthrough is mostly due to the use of a new formulation of a policy in what they call a
sequence-form.
Our main contribution, as detailed in Section 3.3, is then to adapt the sequence-form
introduced by Koller, von Stegel and Megiddo to the framework of DEC-POMDPs (Koller
et al., 1994; Koller & Megiddo, 1996; von Stengel, 2002). As a result, it is possible to
formulate the resolution of a DEC-POMDP as a special kind of mathematical program
that can still be solved quite efficiently: a mixed linear program where some variables
are required to be either 0 or 1. The adaptation and the resulting mixed-integer linear
program is not straightforward. In fact, Koller, von Stegel and Megiddo could only find
one Nash equilibrium in a 2-agent game. What is needed for DEC-POMDPs is to find
the set of policies, called a joint policy, that corresponds to the Nash equilibrium with
the highest value, finding only one Nash equilibrium  already a complex task  is not
enough. Besides, whereas Koller, von Stegel and Megiddo algorithms could only be applied
332

fiMathematical Programming for DEC-POMDPs

to 2-agent games, we extend the approach so as to solve DEC-POMDPs with an arbitrary
number of agents, which constitutes an important contribution.
In order to formulate DEC-POMDPs as MILPs, we analyze in detail the structure of
an optimal joint policy for a DEC-POMDP. A joint policy in sequence-form is expressed as
a set of individual policies that are themselves described as a set of possible trajectories for
each of the agents of the DEC-POMDP. Combinatorial considerations on these individual
histories, as well as constraints that ensure these histories do define a valid joint policy are
at the heart of the formulation of a DEC-POMDP as a mixed linear program, as developped
in Sections 4 and 5. Thus, another contribution of our work is a better understanding of
the properties of optimal solutions to DEC-POMDPs, a knowledge that might lead to the
formulation of new approximate algorithms for DEC-POMDPs.
Another important contribution of this work is that we introduce heuristics for boosting the performance of the mathematical programs we propose (see Section 6). These
heuristics take advantage of the succinctness of the DEC-POMDP model and of the knowledge acquired regarding the structure of optimal policies. Consequently, we are able to
reduce the size of the mathematical programs (resulting also in reducing the time taken to
solve them). These heuristics constitute an important pre-processing step in solving the
programs. We present two types of heuristics: the elimination of extraneous histories which
reduces the size of the mixed integer linear programs and the introduction of cuts in the
mixed integer linear programs which reduces the time taken to solve a program.
On a more practical level, this article presents three different mixed integer linear
programs, two are more directly derived from the work of Koller, von Stegel and Megiddo
(see Table 4 and 5) and a third one is based solely on combinatorial considerations on the
individual policies and histories (see Table 3). The theoretical validity of these formulations is backed by several theorems. We also conducted experimental evaluations of our
algorithms and of our heuristics on several classical DEC-POMDP problems. We were thus
able to confirm that our algorithms are quite comparable to dynamic programming exact
algorithms but outperformed by forward search algorithms like GMAA* (Oliehoek et al.,
2008). On some problems, though, MILPs are indeed faster by one order of magnitude or
two than GMAA*.
1.4 Overview of this Article
The remainder of this article is organized as follows. Section 2 introduces the formalism of
DEC-POMDP and some background on the classical algorithms, usually based on dynamic
programing. Then we expose our reformulation of the DEC-POMDP in sequence-form in
Section 3 where we also define various notions needed by the sequence-form. In Section 4,
we show how to use combinatorial properties of the sequence-form policies to derive a first
mixed integer linear program (MILP, in Table 3) for solving DEC-POMDP. By using game
theoretic concepts like Nash equilibrium, we take inspiration from previous work on games
in extensive form to design two other MILPs for solving DEC-POMDP (Tables 4, 5). These
MILPs are smaller in size and their detailed derivation is presented in Section 5. Our
contributed heuristics to speed up the practical resolutions of the various MILPs make
up the core of Section 6. Section 7 presents experimental validations of our MILP-based
algorithms on classical benchmarks of the DEC-POMDP literature as well as on randomly
333

fiAras & Dutech

built problems. Finally, Section 8 analyzes and discusses our work and we conclude this
paper with Section 9.

2. Dec-POMDP
This section gives a formal definition of Decentralized Partially Observed Markov Decision
Processes as introduced by Bernstein et al. (2002). As described, a solution of a DECPOMDP is a policy defined on the space of information sets that has an optimal value.
This sections ends with a quick overview of the classical methods that have been developed
to solve DEC-POMDPs.
2.1 Formal Definition
A DEC-POMDP is defined as a tuple D = h I, S, {Ai }, P, {Oi }, G, R, T ,  i where:
 I = {1, 2,    , n} is a set of agents.
 S is a finite set of states. The set of probability distributions over S shall be denoted
by (S). Members of (S) shall be called belief states.
 For each agent i  I, Ai is a set of actions. A = iI Ai denotes the set of joint
actions.
 P : S  A  S  [0, 1] is a state transition function. For each s, s  S and for each
a  A, P(s, a, s ) is the probability that the state of the problem in a period t is s if,
in period t  1, its state was s and the agents performed the joint action a. Thus, for
any time period t  2, for each pair of states s, s  S and for each joint action a  A,
there holds:
P(s, a, s ) = Pr(st = s |st1 = s, at = a).
Thus, (S, A, P) defines a discrete-state, discrete-time controlled Markov process.
 For each agent i  I, Oi is a set of observations. O = iI Oi denotes the set of joint
observations.
 G : A  S  O  [0, 1] is a joint observation function. For each a  A, for each
o  O and for each s  S, G(a, s, o) is the probability that the agents receive the
joint observation o (that is, each agent i receives the observation oi ) if the state of
the problem in that period is s and if in the previous period the agents took the joint
action a. Thus, for any time period t  2, for each joint action a  A, for each state
s  S and for each joint observation o  O, there holds:
G(a, s, o) = Pr(ot = o|st = s, at1 = a).
 R : S  A  R is a reward function. For each s  S and for each a  A, R(s, a)  R
is the reward obtained by the agents if they take the joint action a when the state of
the process is s.
334

fiMathematical Programming for DEC-POMDPs

 T is the horizon of the problem. The agents are allowed T joint-actions before the
process halts.
   (S) is the initial state of the DEC-POMDP. For each s  S, (s) denotes the
probability that the state of the problem in the first period is s.
As said, S, A and P define a controlled Markov Process where the next state depends
only on the previous state and on the joint action chosen by the agents. But the agents
do not have access to the state of the process and can only rely on observations, generally
partial and noisy, of this state, as specified by the observation function G. From time to
time, agents receive a non-zero reward according to the reward function R.

a n0

o n1

a 10

o 11

s0

s1

o nt

a n1

o 1t

a 11

s2

a nt

a 1t

st

Figure 1: DEC-POMDP. At every period t of the process, the environment is in state
st , every agent i receives observations oti and decides of its action ati . The joint
action hat1 , at2 ,    , atn i alters the state of the process.
More specifically, as illustrated in Figure 1, the control of a DEC-POMDP by the n
agents unfolds over discrete time periods, t = 1, 2,    ,T as follows. In each period t, the
process is in a state denoted by st from S. In the first period t = 1, the state s1 is chosen
according to  and the agents take actions a1i . In each period t > 1 afterward, each agent
i  I takes an action denoted by ati from Ai according to the agents policy. When the
agents take the joint action at = hat1 , at2 ,    , atn i, the following events occur:
1. The agents all obtain the same reward R(st , at ).
2. The state st+1 is determined according to the function P with arguments st and at .
3. Each agent i  I receives an observation ot+1
from Oi . The joint observation ot+1 =
i
t+1
t+1
t+1 and at .
hot+1
1 , o2 ,    , on i is determined by the function G with arguments s
4. The period changes from t to t + 1.
In this paper, the DEC-POMDP we are interested in have the following properties:
335

fiAras & Dutech

 the horizon T is finite and known by the agents;
 agents cannot infer the exact state of the system from their joint observations (this is
the more general setting of DEC-POMDPs);
 agents do not observe actions and observations of the other agents. They are only
aware of their own observations and reward;
 agents have a perfect memory of their past; they can base their choice of action on
the sequence of past actions and observations. We speak of perfect recall setting;
 transition and observation functions are stationary, meaning that they do not depend
on the period t.
Solving a DEC-POMDP means finding the agents policies (i.e., their decision functions)
to optimize a given criterion based on the rewards received. The criterion we will work with
is called the cumulative reward and defined by:
" T
#
X
t
t
t
t
E
R(s , ha1 , a2 , . . . , an i)
(1)
t=1

where E is the mathematical expectation.
2.2 Example of DEC-POMDP
The problem known as the Decentralized Tiger Problem (hereby denoted MA-Tiger),
introduced by Nair, Tambe, Yokoo, Pynadath, and Marsella (2003), has been widely used
to test DEC-POMDPs algorithms. It is a variation of a problem previously introduced
for POMDPs (i.e., DEC-POMDPs with one agent) by Kaelbling, Littman, and Cassandra
(1998).
In this problem, we are given two agents confronted with two closed doors. Behind one
door is a tiger, behind the other an escape route. The agents do not know which door
leads to what. Each agent, independently of the other, can open one of the two doors or
listen carefully in order to detect the tiger. If either of them opens the wrong door, the
lives of both will be imperiled. If they both open the escape door, they will be free. The
agents have a limited time in which to decide which door to open. They can use this time
to gather information about the precise location of the tiger by listening carefully to detect
the location of the tiger. This problem can be formalized as a DEC-POMDP with:
 two states as the tiger is either behind the left door (sl ) or the right door (sr );
 two agents, that must decide and act;
 three actions for each agent: open the left door (al ), open the right door (ar ) and
listen (ao );
 two observations, as the only thing the agent can observe is that they hear the tiger
on the left (ol ) or on the right (or ).
336

fiMathematical Programming for DEC-POMDPs

The initial state is chosen according to a uniform distribution over S. As long as the door
remains closed, the state does not change but, when one door is opened, the state is reset
to either sl or sr with equal probability. The observations are noisy, reflecting the difficulty
of detecting the tiger. For example, when the tiger is on the left, the action ao produces
an observation ol only 85% of the time. So if both agents perform ao , the joint observation
(ol ,ol ) occurs with a probability of 0.85  0.85 = 0.72. The reward function encourages the
agents to coordinate their actions as, for example, the reward when both open the escape
door (+20) is bigger than when one listens while the other opens the good door (+9). The
full state transition function, joint observation function and reward function are described
in the work of Nair et al. (2003).
2.3 Information Sets and Histories
An information set  of agent i is a sequence (a1 .o2 .a2 .o3    .ot ) of even length in which
the elements in odd positions are actions of the agent (members of Ai ) and those in even
positions are observations of the agent (members of Oi ). An information set of length 0
shall be called the null information set, denoted by . An information set of length T  1
shall be called a terminal information set. The set of information sets of lengths less
than or equal to T  1 shall be denoted by i .
We define a history of agent i  I to be a sequence (a1 .o2 . a2 . o3    .ot .at ) of odd
length in which the elements in odd positions are actions of the agent (members of Ai ) and
those in even positions are observations of the agent (members of Oi ). We define the length
of a history to be the number of actions in the history (t in our example). A history of
length T shall be called a terminal history. Histories of lengths less than T shall be called
non-terminal histories. The history of null length shall be denoted . The information
set associated to an history h, denoted (h), is the information set composed by removing
from h its last action. If h is a history and o an observation, then h.o is an information set.
We shall denote by Hit the set of all possible histories of length t of agent i. Thus, Hi1 is
just the set of actions Ai . We shall denote by Hi the set of histories of agent i of lengths
less than or equal to T . The size ni of Hi is thus:
ni = |Hi | =

PT

t
t1
t=1 |Ai | |Oi |

= |Ai |

(|Ai ||Oi |)T  1
.
|Ai ||Oi |  1

(2)

The set HiT of terminal histories of agent i shall be denoted by Ei . The set Hi \HiT of
non-terminal histories of agent i shall be denoted by Ni .
A tuple hh1 , h2 , . . . , hn i made of one history for each agent is called a joint history. The
tuple obtained by removing the history hi from the joint history h is noted hi and called
an i-reduced joint history.
Example Coming back to the MA-Tiger example, a set of valid histories could be: , (ao ),
(ao .ol .ao ), (ao .or .ao ), (ao .ol .ao .ol .ao ), (ao .ol .ao .or .ar ), (ao .or .ao .ol .ao ) and (ao .or .ao .or .ar ).
Incidently, this set of histories corresponds to the support of the policy (i.e., the histories
generated by using this policy) of the Figure 2, as explained in the next section.
337

fiAras & Dutech

2.4 Policies
At each period of time, a policy must tell an agent what action to choose. This choice
can be based on whatever past and present knowledge the agent has about the process at
time t. One possibility is to define an individual policy i of agent i as a mapping from
information sets to actions. More formally:
i : i  (Ai )

(3)

Among the set  of policies, three families are usually distinguished:
 Pure policies. A pure or deterministic policy maps a given information set to one
unique action. The set of pure policies for the agent i is denoted . Pure policies
could also be defined using trajectories of past observations only since actions, which
are chosen deterministically, can be reconstructed from the observations.
 Mixed policies. A mixed policy is a probability distribution over the set of pure
policies. Thus, an agent using a mixed policy will control the DEC-POMDP by using
a pure policy randomly chosen from a set of pure policies.
 Stochastic policies. A stochastic policy is the more general formulation as it associates
a probability distribution over actions to each history.
If we come back to the MA-Tiger problem (Section 2.2), Figure 2 gives a possible policy
for a horizon 2. As shown, a policy is classically represented by an action-observation tree.
In that kind of tree, each branch is labelled by an observation. For a given sequence of past
observations, one starts from the root node and follows the branches down to an action
node. This node contains the action to be executed by the agent when it has seen this
sequence of observations.
Observation sequence
Chosen action

ol
ao


ao

or
ao

ol .ol
al

ol .or
ao

or .ol
ao

or .or
ar

ao
ol

or

ao

ao

ol

or

ol

or

al

ao

ao

ar

Figure 2: Pure policy for MA-Tiger. A pure policy maps sequences of observations to
actions. This can be represented by an action-observation tree.

A joint policy  = h1 , 2 ,    , n i is an n-tuple where each i is a policy for agent i.
Each of the individual policies must have the same horizon. For an agent i, we also define
the notion of an i-reduced joint policy i = h1 ,    , i1 , i+1 ,    , n i composed of
the policies of all the other agents. We thus have that  = hi , i i.
338

fiMathematical Programming for DEC-POMDPs

2.5 Value Function
When executed by the agents, every T -horizon joint policy generates a probability distribution over the possible sequences of reward from which one can compute the value of the
policy according to Equation 1. Thus the value of the joint policy  is formally defined as:
V (, ) = E

" T
X

R(st , at )|, 

t=1

#

(4)

given that the state in the first period is chosen according to  and that actions are chosen
according to .
There is a recursive definition of the value function of a policy  that is also a way to
compute it when the horizon T is finite. This definition requires some concepts that we
shall now introduce.
Given a belief state   (S), a joint action a  A and a joint observation o  O,
let T (o|, a) denote the probability that the agents receive joint observation o if they take
joint action a in a period t in which the state is chosen according to . This probability is
defined as
X
X
T (o|, a) =
(s)
P(s, a, s )G(a, s , o)
(5)
s S

sS

Given a belief state   (S), a joint action a  A and a joint observation o  O , the
updated belief state  ao  (S) of  with respect to a and o is defined as (for each
s  S),
 ao (s ) =

P
G(a,s ,o)[ sS (s)P(s,a,s )]
T (o|,a)

if T (o|, a) > 0

(6)

 ao (s ) =

0

if T (o|, a) = 0

(7)

P
Given a belief state   (S) and a joint action a  A, R(, a) denotes sS (s)R(s, a).
Using the above definitions and notations, the value V (, ) of  is defined as follows:
V (, ) = V (, , )

(8)

where V (, , ) is defined by recursion using equations (9), (10) and (11), given below.
These equations are a straight reformulation of the classical Bellman equations for finite
horizon problems.
 For histories of null length
V (, , ) = R(, ()) +

X

T (o|, ())V (()o , , o)

(9)

oO

() denotes the joint action h1 (), 2 (),    , n ()i and ()o denotes the
updated state of  given () and the joint observation o.
339

fiAras & Dutech

 For non-terminal histories. For any   (S), for each t of {1, . . . , T  2}, for each
1:T   O
1:T
1:T
tuple of sequences of t observations o1:T = ho1:T
T i
1 , o2 ,    , on i where oi
is a sequence of t observations of agent i  I:
V ( , , o1:T ) = R( , (o1:T )) +

X

1:T )o

T (o| , (o1:T ))V ((o

, , o1:T .o) (10)

oO
1:T

(o )o is the updated state of  given the joint action (o1:T ) and joint observation
o = ho1 , o2 ,    , on i and o1:T .o is the tuple of sequences of (t + 1) observations ho1:T
1 .o1 ,
1:T .o i.
o1:T
.o
,



,
o
n
2
n
2
 For terminal histories. For any   (S), for each tuple of sequences of (T - 1)
1
1
1 i:
observations o1:T 1 = ho1:T
, o1:T
,    , o1:T
n
1
2
V ( , , o1:T 1 ) = R(, (o1:T 1 )) =

X

 (s)R(s, (o1:T 1 ))

(11)

sS

An optimal policy   is a policy with the best possible value, verifying:
V (,   )  V (, )

  .

(12)

An important fact about DEC-POMDPs, based on the following theorem, is that we can
restrict ourselves to the set of pure policies when looking for a solution to a DEC-POMDP.
Theorem 2.1. A DEC-POMDP has at least one optimal pure joint policy.
Proof: See proof in the work of Nair et al. (2003).



2.6 Overview of DEC-POMDPs Solutions and Limitations
As detailed in the work of Oliehoek et al. (2008), existing methods for solving DECPOMDPs with finite-horizon belong to several broad families: brute force, alternating
maximization, search algorithms and dynamic programming.
Brute Force The simplest approach for solving a DEC-POMDP is to enumerate all
possible joint policies and to evaluate them in order to find the optimal one. However, such
a method becomes quickly untractable as the number of joint policies is doubly exponential
in the horizon of the problem.
Alternating Maximization Following Chades, Scherrer, and Charpillet (2002) and Nair
et al. (2003), one possible way to solve DEC-POMDPs is for each agent (or each small group
of agents) to alternatively search for a better policy while all the other agents freeze their
own policy. Called alternating maximization by Oliehoek and alternated co-evolution by
Chades this method guarantees only to find a Nash equilibria, that is a locally optimal joint
policy.
340

fiMathematical Programming for DEC-POMDPs

Heuristic Search Algorithms The concept was introduced by Szer, Charpillet, and
Zilberstein (2005) and relies on heuristic search for looking for an optimal joint policy,
using an admissible approximation of the value of the optimal joint policy. As the search
progresses, joint policies that will provably by worse that the current admissible solution are
pruned. Szer et al. used underlying MDPs or POMDPs to compute the admissible heuristic,
Oliehoek et al. (2008) introduced a better heuristic based on the resolution of a Bayesian
Game with a carefully crafted cost function. Currently, Oliehoeks method called GMAA*
(for Generic Multi-Agent A*) is the quickest exact method on a large set of benchmarks.
But, as every exact method, it is limited to quite simple problems.
Dynamic Programming The work from Hansen, Bernstein, and Zilberstein (2004)
adapts solutions designed for POMDPs to the domain of DEC-POMDPs. The general
idea is to start with policies for 1 time step that are used to build 2 time step policies
and so on. But the process is clearly less efficient that the heuristic search approach as an
exponential number of policies must be constructed and evaluated at each iteration of the
algorithm. Some of these policies can be pruned but, once again, pruning is less efficient.
As exposed in more details in the paper by Oliehoek et al. (2008), several others approaches have been developed for subclasses of DEC-POMDPs. For example, special settings where agents are allowed to communicate and exchange informations or settings where
the transition function can be split into independant transition functions for each agent have
been studied and found easier to solve than generic DEC-POMDPs.

3. Sequence-Form of DEC-POMDPs
This section introduces the fundamental concept of policies in sequence-form. A new
formulation of a DEC-POMDP is thus possible and this leads to a Non-Linear Program
(NLP) the solution of which defines an optimal solution to the DEC-POMDP.
3.1 Policies in Sequence-Form
A history function p of an agent i is a mapping from the set of histories to the interval
[0, 1]. The value p(h) is the weight of the history h for the history function p. A policy i
defines a probability function over the set of histories of the agent i by saying that, for each
history hi of Hi , p(hi ) is the conditional probability of hi given an observation sequence
(o0i .o1i .    .oti ) and i .
If every policy defines a policy function, not every policy function can be associated to
a valid policy. Some constraints must be met. In fact, a history function p is a sequenceform policy for agent i when the following constraints are met:
X

p(a) = 1,

(13)

aAi

p(h) +

X

p(h.o.a) = 0,

h  Ni , o  Oi ,

(14)

aAi

where h.o.a denotes the history obtained on concatenating o and a to h. This definition
appears in a slightly different form as Lemma 5.1 in the work of Koller et al. (1994).
341

fiAras & Dutech

Variables: x(h), h  Hi ,
X

x(a) = 1

(15)

aAi

x(h) +

X

x(h.o.a) = 0,

h  Ni , o  Oi

(16)

h  Hi

(17)

aAi

x(h)  0,

Table 1: Policy Constraints. This set of linear inequalities, once solved, provide a valid
sequence-form policy for the agent i. That is, from the weights x(h), it is possible
to define a policy for the agent i.

A sequence-form policy can be stochastic as the probability of choosing action a in the
information set h.o is p(h.o.a)/p(h). The support S(p) of a sequence-form policy is made
of the set of histories that have a non-negative weight, i.e. S(p) = {h  Hi | p(h) > 0}. As
a sequence-form policy p defines a unique policy  for an agent, a sequence-form policy will
be called a policy in the rest of this paper when no ambiguity is present.
The set of policies in the sequence-form of agent i shall be denoted by Xi . The set of
pure policies in the sequence-form shall be denoted by Xi  Xi .
In a way similar to the definitions of Section 2.4, we define a sequence-form joint
policy as a tuple of sequence-form policies, one for each agent. The weight Q
of the joint
history h = hhi i of a sequence-form joint policy hp1 , p2 ,    , pn i is the product iI pi (hi ).
The set of joint policies in the sequence-form iI Xi shall be denoted by X and the set of
i-reduced sequence-form joint policy is called Xi .
3.2 Policy Constraints
A policy of agent i in the sequence-form can be found by solving a set of linear inequalities
(LI) found in Table 1. These LI merely implement the definition of a policy in the sequenceform. The LI contains one variable x(h) for each history h  Hi to represent the weight of
h in the policy. A solution x to these LI constitutes a policy in the sequence-form.
Example In the Section E.1 of the Appendices, the policy constraints for the decentralized
Tiger problem are given for 2 agents and a horizon of 2.
Notice that in the policy constraints of an agent, each variable is only constrained to be
non-negative whereas by the definition of a policy in sequence-form, the weight of a history
must be in the interval [0, 1]. Does it mean that a variable in a solution to the policy
constraints can assume a value higher than 1? Actually, the policy constraints are such
that they prevent any variable from assuming a value higher than 1 as the following lemma
shows.
Lemma 3.1. In every solution x to (15)-(17), for each h  Hi , x (h) belongs to the [0, 1]
interval.
342

fiMathematical Programming for DEC-POMDPs

Proof: This can be shown by forward induction.
Every x(h) being non-negative (see Eq. (17)), it is also the case for every action a of
Ai . Then, no x(a) can be greater than 1 otherwise constraint (15) would be violated. So,
h  Hi1 , (i.e. a  Ai ), we have x(h) belong to [0, 1].
If every h of Hit is such that x(h)  [0, 1], the previous reasoning applied using constraint
(16) leads evidently to the fact that x(h)  [0, 1] for every h of Hit+1 .
Thereby, by induction this holds for all t.

Later in this article, in order to simplify the task of looking for joint policies, the policy
constraints LI will be used to find pure policies. Looking for pure policies is not a limitation
as finite-horizon DEC-POMDPs admit deterministic policies when the policies are defined
on information set. In fact, pure policies are needed in two of the three MILPs we build
in order to solve DEC-POMDPs, otherwise their derivation would not be possible (see
Sections 4 and 5.4).
Looking for pure policies, an obvious solution would be to impose that every variable
x(h) belongs to the set {0, 1}. But, when solving a mixed integer linear program, it is
generally a good idea to limit the number of integer variables as each integer variable is
a possible node for the branch and bound method used to assign integer values to the
variables. A more efficient implementation of a mixed integer linear program is to take
advantage of the following lemma to impose that only the weights of the terminal histories
take 0 or 1 as possible values.
Lemma 3.2. If in (15)-(17), (17) is replaced by,
x(h)  0,

h  Ni

x(h)  {0, 1},

h  Ei

(18)
(19)

then in every solution x to the resulting LI, for each h  Hi , x (h) = 0 or 1. We will
speak of a 0-1 LI.
Proof: We can prove this by backward induction. Let h be a history of length T - 1.
Due to (16), for each o  Oi , there holds,
X
x (h) =
x (h.o.a).
(20)
aAi

Since h is a history of length T - 1, each history h.o.a is a terminal history. Due to Lemma
3.1, x (h)  [0, 1]. Therefore, the sum on the right hand side of the above equation is also
in [0, 1]. But due to (19), each x (h.o.a)  {0, 1}. Hence the sum on the right hand side
is either 0 or 1, and not any value in between. Ergo, x (h)  {0, 1} and not any value in
between. By this same reasoning, we can show that x (h)  {0, 1} for every non-terminal
history h of length T - 2, T - 3,    , 1.

To formulate the linear inequalities of Table 1 in memory, we require
P space that is only
exponential in the horizon. For each agent i  I, the size of Hi is Tt=1 |Ai |t |Oi |t1 . It is
then exponential in T and the number of variables
LP is also exponential in T . The
PT 1in the
t
number of constraints in the LI of Table 1 is t=0 |Ai | |Oi |t , meaning that the number of
constraints of the LI is also exponential in T .
343

fiAras & Dutech

3.3 Sequence-Form of a DEC-POMDP
We are now able to give a formulation of a DEC-POMDP based on the use of sequence-form
policies. We want to stress that this is only a re-formulation, but as such will provide us
with new ways of solving DEC-POMDPs with mathematical programming.
Given a classical formulation of a DEC-POMDP (see Section 2.1), the equivalent
sequence-form DEC-POMDP is a tuple hI, {Hi }, , Ri where:
 I = {1, 2,    , n} is a set of agents.
 For each agent i  I, Hi is the set of histories of length less than or equal to T for
the agent i, as defined in the previous section. Each set Hi is derived using the sets
Ai and Oi .
  is the joint history conditional probability function. For each joint history j  H,
(, j) is the probability of j occurring conditional on the agents taking joint actions
according to it and given that the initial state of the DEC-POMDP is . This function
is derived using the set of states S, the state transition function P and the joint
observation function G.
 R is the joint history value. For each joint history j  H, R(, j) is the value of
the expected reward the agents obtain if the joint history j occurs. This function is
derived using the set of states S, the state transition function P, the joint observation
function G and the reward function R. Alternatively, R can be described as a function
of  and R.
This formulation folds S, P and G into  and R by relying on the set of histories. We
will now give more details about the computation of  and R.
(, j) is the conditional probability that the sequence of joint observations received
by the agents till period t is (o1 (j).o2 (j).   . ot1 (j)) if the sequence of joint actions
taken by them till period t - 1 is (a1 (j). a2 (j).    . at1 (j)) and the initial state of the
DEC-POMDP is . That is,
(, j) = Prob.(o1 (j).o2 (j).    .ot1 (j)|, a1 (j).a2 (j).    .at1 (j))

(21)

This probability is the product of the probabilities of seeing observation ok (j) given the
appropriate belief state and action chosen at time k, that is:
(, j) =

t1
Y

T (ok (j)|jk1 , ak (j))

(22)

k=1

where jk is the probability distribution on S given that the agents have followed the joint
history j up to time k, that is:
jk (s) = Prob.(s|o1 (j).a1 (j).    .ok (j)).
344

(23)

fiMathematical Programming for DEC-POMDPs

Variables: xi (h), i  I, h  Hi
Maximize

X

R(, j)

jE

Y

xi (ji )

(27)

iI

subject to
X

xi (a) = 1,

i  I

(28)

i  I, h  Ni , o  Oi

(29)

i  I, h  Hi

(30)

aAi

xi (h) +

X

xi (h.o.a) = 0,

aAi

xi (h)  0,

Table 2: NLP. This non-linear program expresses the constraints for finding a sequenceform joint policy that is an optimal solution to a DEC-POMDP.

Regarding the value of a joint history, it is defined by:
R(, j) = R(, j)(, j)

(24)

where
R(, j) =

t X
X

jk1 (s)R(s, ak (j)).

(25)

k=1 sS

Thus, V(, p), the value of a sequence-form joint policy p, is the weighted sum of
the value of the histories in its support:
X
V(, p) =
p(j)R(, j)
(26)
jH

with p(j) =

Q

iI

pi (ji ).

3.4 Non-Linear Program for Solving DEC-POMDPs.
By using the sequence-form formulation of a DEC-POMDP, we are able to express joint
policies as sets of linear constraints and to assess the value of every policy. Solving a DECPOMDP amounts to finding the policy with the maximal value, which can be done with the
non-linear program (NLP) of Table 2 where, once again, the xi variables are the weights of
the histories for the agent i.
Example An example of the formulation of such an NLP can be found in the Appendices,
in Section E.2. It is given for the decentralized Tiger problem with 2 agents and an horizon
of 2.
The constraints of the program form a convex set, but the objective function is not
concave (as explained in appendix A). In the general case, solving non-linear program is
345

fiAras & Dutech

very difficult and there are no generalized method that guarantee finding a global maximum
point. However, this particular NLP is in fact a Multilinear Mathematical Program (see
Drenick, 1992) and this kind of programs are still very difficult to solve. When only two
agents are considered, one speaks of bilinear programs, that can be solved more easily
(Petrik & Zilberstein, 2009; Horst & Tuy, 2003).
An evident, but inefficient, method to find a global maximum point is to evaluate all the
extreme points of the set of feasible solutions of the program since it is known that every
global as well as local maximum point of a non-concave function lies at an extreme point of
such a set (Fletcher, 1987). This is an inefficient method because there is no test that tells
if an extreme point is a local maximum point or a global maximum point. Hence, unless
all extreme points are evaluated, we cannot be sure of having obtained a global maximum
point. The set of feasible solutions to the NLP is X, the set of T -step joint policies. The
set of extreme points of this set is X, the set of pure T -step joint policies, whose number is
doubly exponential in T and exponential in n. So enumerating the extreme points for this
NLP is untractable.
Our approach, developed in the next sections, is to linearize the objective function of
this NLP in order to deal only with linear programs. We will describe two ways for doing
this: one is based on combinatorial consideration (Section 4) and the other is based on
game theory concepts (Section 5). In both cases, this shall mean adding more variables and
constraints to the NLP, but upon doing so, we shall derive mixed integer linear programs
for which it is possible to find a global maximum point and hence an optimal joint policy
of the DEC-POMDP.

4. From Combinatorial Considerations to Mathematical Programming
This section explains how it is possible to use combinatorial properties of DEC-POMDPs
to transform the previous NLP into a mixed integer linear program. As shown, this mathematical program belongs to the family of 0-1 Mixed Integer Linear Programs, meaning that
some variables of this linear program must take integer values in the set {0, 1}.
4.1 Linearization of the Objective Function
Borrowing ideas from the field of Quadratic Assignment Problems (Papadimitriou & Steiglitz, 1982), we turn the non-linear objective function of the previous NLP into a linear
objective function and linear constraints involving new variables z that must take integer
values. The variable z(j) represents the product of the xi (ji ) variables.
Thus, the objective function that was:
X
Y
maximize
R(, j)
xi (ji )
(31)
jE

iI

can now be rewritten as
maximize

X

R(, j)z(j)

jE

where j = hj1 , j2 ,    , jn i.
346

(32)

fiMathematical Programming for DEC-POMDPs

We must ensure that there is a two way mapping between the value of the new variables
z and the x variables for any solution of the mathematical program, that is:
Y
z  (j) =
xi (ji ).
(33)
iI

For this, we will restrict ourself to pure policies where the x variables can only be 0 or 1.
In that case, the previous constraint (33) becomes:
z  (j) = 1  xi (ji ) = 1,

i  I

(34)

There, we take advantage on the fact that the support of a pure policy for an agent i is
composed of |Oi |T 1 terminal histories to express these new constraints. On the one hand,
to guarantee that z(j) is equal to 1 only when enough x variables are also equal to 1, we
write:
n
X

xi (ji )  nz(j)  0,

j  E.

(35)

i=1

On the other hand, to limit the number of z(j) variables that can take a value of 1, we will
enumerate the number of joint terminal histories to end up with:
Y
X
|Oi |T 1 .
(36)
z(j) =
iI

jE

The constraints (35) would weight heavily on any mathematical program as there would
be one constraint for each terminal joint history, a number which is exponential in n and
T . Our idea to reduce this number of constraints is not to reason about joint histories
but with individual histories. An history h of agent i is part of the support of the solution
ofPthe problem (i.e., xQ
i (h) = 1) if and only if the number of joint histories
Q it belongs to
( j  Ei z(hh, j  i)) is kI\{i} |Ok |T 1 . Then, we suggest to replace the |Ei | constraints
(35)
n
X

xi (ji )  nz(j)  0,

j  E.

(35)

i=1

by the

P

|Ei | constraints
X



|Ok |T 1
xi (h)
|Oi |T 1
Y
= xi (h)
|Ok |T 1 ,

z(hh, j i) =

j  Ei

Q

kI

i  I, h  Ei .

(37)

kI\{i}

4.2 Fewer Integer Variables
The linearization of the objective function rely on the fact that we are dealing with pure
policies, meaning that every x and z variable is supposed to value either 0 or 1. As solving
linear programs with integer variables is usually based on the branch and bound technique
347

fiAras & Dutech

Variables:
xi (h), i  I, h  Hi
z(j), j  E
X

Maximize

R(, j)z(j)

(38)

jE

subject to:
X

xi (a) = 1,

i  I

(39)

i  I, h  Ni , o  Oi

(40)

aAi

xi (h) +

X

xi (h.o.a) = 0,

aAi

X

Y

z(hh, j  i) = xi (h)

T
j  Hi

|Ok |T 1 ,

i  I, h  Ei

(41)

kI\{i}

X

z(j) =

jE

Y

|Oi |T 1

(42)

iI

xi (h)  0,

i  I, h  Ni

xi (h)  {0, 1},

(43)

i  I, h  Ei

z(j)  [0, 1],

(44)

j  E

(45)

Table 3: MILP. This 0-1 mixed integer linear program finds a sequence-form joint policy
that is an optimal solution to a DEC-POMDP.

(Fletcher, 1987), for efficiency reasons, it is important to reduce the number of integer
variables in our mathematical programs.
As done in Section 3.2, we can relax most x variables and allow them to take non-negative
values provided that the x values for terminal histories are constrained to integer values.
Furthermore, as proved by the following lemma, these constraints on x also guarantee that
z variables only take their value in {0, 1}.
We eventually end up with the following linear program with real and integer variables,
thus called an 0-1 mixed integer linear program (MILP). The MILP is shown in Table 3.
Example In Section E.3 of the Appendices, an example if such MILP is given for the
problem of the decentralized Tiger for 2 agents and an horizon of 2.
Lemma 4.1. In every solution (x , z  ) to the MILP of Table 3, for each j  E, z  (j) is
either 0 or 1.
Proof: Let (x , z  ) be a solution of MILP. Let,
S(z) = {j  E|z  (j) > 0}
Si (xi ) = {h 


Ei |xi (h)

= 1},




(46)
i  I

Si (z, j ) = {j  E|ji = j , z (j) > 0},
348

(47)
i  I,

j 

 Ei

(48)

fiMathematical Programming for DEC-POMDPs

Q
Q
T 1 . By showing that |S(z)| 
T 1 ,
Now, due to (42) and (45), |S(z)|
i|
iI |Oi |
Q iIT|O
1
we shall establish that |S(z)| = iI |Oi |
. Then due to the upper bound of 1 on each z

variable, the implication will be that z (j) is 0 or 1 for each terminal joint history j thus
proving the statement of the lemma.
Note that by Lemma (3.2), for each agent i, xi is a pure policy. Therefore, we have that
|Si (x)| = |Oi |T 1 . This means that in the set of constraints (41), an i-reduced terminal
joint history j   Ei will appear on the right hand side not more than |Oi |T 1 times when
in the left hand side, we have xi (h) = 1. Thus, j   Ei ,
|Si (z, j  )|  |Oi |T 1 .

(49)

(h) is either 0 or 1 since
Now, we know that for each agent i and for each history h  Hi , xi Q
xi is a pure policy. So, given an i-reduced terminal joint history j  , kI\{i} xk (jk ) is either
0 or 1. Secondly, due to (41), the following implication clearly holds for each terminal joint
history j,
z  (j) > 0  xi (ji ) = 1,

i  I.

(50)

Therefore, we obtain
|Si (z, j  )|  |Oi |T 1

(51)
Y

T 1

= |Oi |

xk (jk ).

(52)

kI\{i}

As a consequence,
X

X

|Si (z, j  )| 

j  Ei

Y

|Oi |T 1

j  Ei

xk (jk )

(53)

xk (jk )

(54)

xk (h )

(55)

kI\{i}

= |Oi |T 1

X

Y

j  Ei kI\{i}

= |Oi |T 1

Y

X

kI\{i} h Ek

= |Oi |T 1

Y

|Ok |T 1

(56)

kI\{i}

=

Y

T 1

|Oj |

.

(57)

jI

Since

S

j  Ei

Si (z, j  ) = S(z), there holds that

P

Y

|S(z)| 

j  Ei

|Si (z, j  )| = |S(z)|. Hence,

|Oj |T 1 .

(58)

jI

Thus the statement of the lemma is proved.
349



fiAras & Dutech

4.3 Summary
By using combinatorial considerations, it is possible to design a 0-1 MILP for solving a given
DEC-POMDP. As proved by theorem 4.1, the solution of this MILP defines an optimal joint
policy
Nevertheless, this MILP is quite large, with O(kT ) constraints
Pfor the DEC-POMDP.
Q
nT
and i |Hi | + i |Ei | = O(k ) variables, O(kT ) of these variables must take integer values.
The next section details another method for the linearization of NLP which leads to a
smaller mathematical program for the 2-agent case.
Theorem 4.1. Given a solution (x , z  ) to MILP, x = hx1 , x2 ,    , xn i is a pure
T -period optimal joint policy in sequence-form.
Proof: Due to the policy constraints and the domain constraints of each agent, each xi

is a pure sequence-form policy
Q of agent i. Due to the constraints (41)-(42), each z values 1
if and only if the product iI xi (ji ) values 1. Then, by maximizing the objective function
we are effectively maximizing the value of the sequence-form policy hx1 , x2 ,    , xn i. Thus,

hx1 , x2 ,    , xn i is an optimal joint policy of the original DEC-POMDP.

5. From Game-Theoretical Considerations to Mathematical
Programming
This section borrows concepts like Nash equilibrium and regret from game theory in
order to design yet another 0-1 Mixed Integer Linear Program for solving DEC-POMDPs.
In fact, two MILPs are designed, one that can only be applied for 2 agents and the other
one for any number of agents. The main objective of this part is to derive a smaller
mathematical program for the 2 agent case. Indeed, MILP-2 agents (see Table 4) has
slightly less variables and constraints than MILP (see Table 3) and thus might prove easier
to solve. On the other hand, when more than 2 agents are considered, the new derivation
leads to a MILP which is only given for completeness as it is bigger than MILP.
Links between the fields of multiagent systems and game theory are numerous in the
literature (see, for example, Sandholm, 1999; Parsons & Wooldridge, 2002). We will elaborate on the fact that the optimal policy of a DEC-POMDP is a Nash Equilibrium. It is in
fact the Nash Equilibrium with the highest utility as the agents all share the same reward.
For the 2-agent case, the derivation we make in order to build the MILP is similar to
the first derivation of Sandholm, Gilpin, and Conitzer (2005). We give more details of this
derivation and adapt it to DEC-POMDP by adding an objective function to it. For more
than 2 agents, our derivation can still be use to find Nash equilibriae with pure strategies.
For the rest of this article, we will make no distinction between a policy, a sequence-form
policy or a strategy of an agent as, in our context, these concepts are equivalent. Borrowing
from game theory, a joint policy will be denoted p or q, an individual policy pi or qi and a
i-reduced policy pi or qi .
5.1 Nash Equilibrium
A Nash Equilibrium is a joint policy in which each policy is a best response to the reduced
joint policy formed by the other policies of the joint policy. In the context of a sequence-form
350

fiMathematical Programming for DEC-POMDPs

DEC-POMDP, a policy pi  Xi of agent i is said to be a best response to an i-reduced
joint policy qi  Xi if there holds that
V(, hpi , qi i)  V(, hpi , qi i)  0,

pi  Xi .

(59)

A joint policy p  X is a Nash Equilibrium if there holds that
V(, p)  V(, hpi , pi i)  0,
That is,
X X

hEi j  Ei

R(, hh, j  i)

Y

kI\{i}

i  I, pi  Xi .


	
 0,
pk (jk ) pi (h)  pi (h)

i  I, pi  Xi .

(60)

(61)

The derivation of the necessary conditions for a Nash equilibrium consists of deriving
the necessary conditions for a policy to be a best response to a reduced joint policy. The
following program finds a policy for an agent i that is a best response to an i-reduced joint
policy qi  Xi . Constraints (63)-(64) ensure that the policy defines a valid joint policy
(see Section 3.2) and the objective function is a traduction of the concept of best response.
Variables: xi (h), i  I, h  Hi



X X
Y
Maximize
R(, hh, j  i)
qk (jk ) xi (h)


hEi

j Ei

(62)

kI\{i}

subject to:

X

xi (a) = 1

(63)

aAi

xi (h) +

X

xi (h.o.a) = 0,

h  Ni , o  Oi

(64)

h  Hi .

(65)

aAi

xi (h)  0,

This linear program (LP) must still be refined so that its solution is not only a best
response for agent i but a global best response, i.e., the policy of each agent is a best
response to all the other agents. This will mean introducing new variables (a set of variable
for each agent). The main point will be to adapt the objective function as the current
objective function, when applied to find global best response, would lead to a non-linear
objective function where product of weights of policies would appear. To do this, we will
make use of the dual of the program (LP).
The linear program (LP) has one variable xi (h) for each history h  Hi representing
the weight of h. It has one constraint per information set of agent i. In other words, each
constraint of the linear program (LP) is uniquely labeled by an information set. For instance,
the constraint (63) is labeled by the null information set , and for each nonterminal
history h and for each observation o, the corresponding constraint in (64) is labeled by the
information set h.o. Thus, (LP) has ni variables and mi constraints.
As described in the appendix (see appendix B), the dual of (LP) is expressed as:
351

fiAras & Dutech

Variables: yi (),   i
Minimize

yi ()

(66)

subject to:
yi ((h)) 

X

yi (h.o)  0,

h  Ni

(67)

qk (jk )  0,

h  Ei

(68)

oOi

yi ((h)) 

X

R(, hh, j  i)

j  Ei

Y

kI\{i}

yi ()  (, +),

  i

(69)

where (h) denotes the information set to which h belongs. The dual has one free variable
yi () for every information set of agent i. This is why the function (h) (defined in Section 2.3) appears as a mapping from histories to information sets1 . The dual program has
one constraint per history of the agent. Thus, the dual has mi variables and ni constraints.
Note that the objective of the dual is to minimize only yi () because in the primal (LP),
the right hand side of all the constraints, except the very first one, is a 0.
The theorem of duality (see the appendix B), applied to the primal (LP) (62)-(65) and the
transformed dual (66)-(69), says that their solutions have the same value. Mathematically,
that means that:



X X
Y
R(, hh, j  i)
qk (jk ) xi (h) = yi ().
(70)


hEi

j Ei

kI\{i}

Thus, the value of the joint policy hxi , qi i can be expressed either as
X X
Y
	
V(, hxi , qi i) =
R(, hh, j  i)
qk (jk ) xi (h)
hEi

j  Ei

(71)

kI\{i}

or as

V(, hxi , qi i) = yi ().

(72)

Due to the constraints (63) and (64) of the primal LP, there holds that
X

	
X  	 X X 
xi (a) +
yi (h.o)  xi (h) +
xi (h.o.a)
yi () = yi ()
aAi

hNi oOi

(73)

aAi

as constraint (63) guarantees that the first term in the braces is 1 and constraints (65)
guarantee that each of the remaining terms inside the braces is 0. The right hand side of
(73) can be rewritten as
X
X
X

	

	
P
xi (a) yi ()
 oOi yi (a.o)
+
xi (h) yi ((h)) 
yi (h.o)
aAi

oOi

hNi \Ai

+

X

xi (h)yi ((h))

hEi

=

P


hNi xi (h)



yi ((h))

X

oOi

	 X 
yi (h.o) +
xi (h)yi ((h))
hEi

1. As h.o is an information set, yi (h.o) is a shortcut in writing for yi ((h.o)).

352

(74)

fiMathematical Programming for DEC-POMDPs

So, combining equations (70) and (74), we get
X
X
 
	
xi (h)
yi ((h)) 
yi (h.o)
oOi

hNi

+

X

xi (h)



hEi

yi ((h))

R(, hh, j  i)

X



j  Ei

Y

kI\{i}

	
qk (jk ) = 0

(75)

It is time to introduce supplementary variables w for each information set. These variables, usually called slack variables, are defined as:
X
yi ((h)) 
yi (h.o) = wi (h), h  Ni
(76)
oOi

yi ((h)) 

X



R(, hh, j i)

j  Ei

Y

qk (jk ) = wi (h),

h  Ei .

(77)

kI\{i}

As shown is Section C of the appendix, these slack variables correspond to the concept of
regret as defined in game theory. The regret of an history expresses the loss in accumulated
reward the agent incurs when he acts according to this history rather than according to a
history which would belong to the optimal joint policy.
Thanks to the slack variables, we can furthermore rewrite (75) as simply
X
X
xi (h)wi (h) +
xi (h)wi (h) = 0
(78)
hNi

hEi

X

xi (h)wi (h) = 0.

(79)

hHi

Now, (79) is a sum of ni products, ni being the size of Hi . Each product in this sum is
necessarily 0 because both xi (h) and wi (h) are constrained to be nonnegative in the primal
and the dual respectively. This property is strongly linked to the complementary slackness
optimality criterion in linear programs (see, for example, Vanderbei, 2008). Hence, (79) is
equivalent to
xi (h)wi (h) = 0,

h  Hi .

(80)

Back to the framework of DEC-POMDPs, these constraints are written:
pi (h)i (hh, qi i) = 0,

h  Hi .

(81)

To sum up, solving the following mathematical program would give an optimal joint
policy for the DEC-POMDP. But constraints (87) are non-linear and thus prevent us from
solving this program directly. The linearization of these constraints, called complementarity
constraints, is the subject of the next section.
Variables:
xi (h), wi (h) i  I and h  Hi
yi () i  I and   i
Maximize

y1 ()

353

(82)

fiAras & Dutech

subject to:
X

xi (a) = 1

(83)

aAi

xi (h) +

X

i  I, h  Ni , o  Oi

(84)

yi (h.o) = wi (h),

i  I, h  Ni

(85)

xk (jk ) = wi (h),

i  I, h  Ei

(86)

xi (h.o.a) = 0,

aAi

yi ((h)) 

X

oOi

yi ((h)) 

X

j  Ei

R(, hh, j  i)

Y

kI\{i}

xi (h)wi (h) = 0,

i  I, h  Hi

(87)

xi (h)  0,

i  I, h  Hi

(88)

wi (h)  0,

i  I, h  Hi

(89)

yi ()  (, +),

i  I,   i

(90)

5.2 Dealing with Complementarity Constraints
This section explains how the non-linear constraints xi (h)wi (h) = 0 in the previous mathematical program can be turned into sets of linear constraints and thus lead to a mixed
integer linear programming formulation of the solution of a DEC-POMDP.
Consider a complementarity constraint ab = 0 in variables a and b. Assume that the
lower bound on the values of a and b is 0. Let the upper bounds on the values of a and b be
respectively ua and ub . Now let c be a 0-1 variable. Then, the complementarity constraint
ab = 0 can be separated into the following equivalent pair of linear constraints,
a  ua c

(91)

b  ub (1  c).

(92)

In other words, if this pair of constraints is satisfied, then it is surely the case that ab = 0.
This is easily verified. c can either be 0 or 1. If c = 0, then a will be set to 0 because a is
constrained to be no more than ua c (and not less than 0); if c = 1, then b will be set to 0
since b is constrained to be not more than ub (1  c) (and not less than 0). In either case,
ab = 0.
Now consider each complementarity constraint xi (h)wi (h) = 0 from the non-linear program (82)-(90) above. We wish to separate each constraint into a pair of linear constraints.
We recall that xi (h) represents the weight of h and wi (h) represents the regret of h. The
first requirement to convert this constraint to a pair of linear constraints is that the lower
bound on the values of the two terms be 0. This is indeed the case since xi (h) and wi (h)
are both constrained to be non-negative in the NLP. Next, we require upper bounds on the
weights of histories and regrets of histories. We have shown in Lemma 3.1 that the upper
bound on the value of xi (h) for each h is 1. For the upper bounds on the regrets of histories,
we require some calculus.
354

fiMathematical Programming for DEC-POMDPs

In any policy pi of agent i there holds that
X
pi (h) = |Oi |T 1 .

(93)

hEi

Therefore, in every i-reduced joint policy hq1 , q2 ,    , qn i  Xi , there holds
Y
X Y
|Ok |T 1
qk (jk ) =
j  Ei kI\{i}

(94)

kI\{i}

Since the regret of a terminal history h of agent i given hq1 , q2 ,    , qn i is defined as
X Y

	
(95)
i (h, q) = max
qk (jk ) R(, hh , j  i)  R(, hh, j  i) ,
h (h)

j  Ei kI\{i}

we can conclude that an upper bound Ui (h) on the regret of a terminal history h  Ei
of agent i is,


Y
T 1
 

Ui (h) =
|Ok |
max max
R(, hh , j i)  min R(, hh, j i) . (96)

kI\{i}

h (h) j Ei

j Ei

Now let us consider the upper bounds on the regrets of non-terminal histories. Let  be
an information set of length t of agent i. Let Ei ()  Ei denote the set of terminal histories
of agent i such the first 2t elements of each history in the set are identical to . Let h be a
history of length t  T of agent i. Let Ei (h)  Ei denote the set of terminal histories such
that the first 2t - 1 elements of each history in the set are identical to h. Since in any policy
pi of agent i, there holds
X
pi (h )  |Oi |T t
(97)
h Ei (h)

we can conclude that an upper bound Ui (h) on the regret of a nonterminal history
h  Ni of length t agent i is


 

max
max
R(,
hh
,
j
i)

min
min
R(,
hg,
j
i)
(98)
Ui (h) = Li


h Ei ((h)) j Ei

gEi (h) j Ei

where
Li = |Oi |T t

Y

|Ok |T 1 .

(99)

kI\{i}

Notice that if t = T (that is, h is terminal) (98) reduces to (96).
So, the complementarity constraint xi (h)wi (h) = 0 can be separated into a pair of linear
constraints by using a 0-1 variable bi (h) as follows,
xi (h)  1  bi (h)
wi  Ui (h)bi (h)
bi (h)  {0, 1}
355

(100)
(101)
(102)

fiAras & Dutech

Variables:
xi (h), wi (h) and bi (h) for i  {1, 2} and h  Hi
yi () for i  {1, 2} and   i
Maximize

y1 ()

(103)

subject to:
X

xi (a) = 1

(104)

aAi

xi (h) +

X

i = 1, 2, h  Ni , o  Oi

(105)

i = 1, 2, h  Ni

(106)

R(, hh, h i)x2 (h ) = w1 (h),

h  E1

(107)

R(, hh , hi)x1 (h ) = w2 (h),

h  E2

(108)

xi (h.o.a) = 0,

aAi

yi ((h)) 

X

yi (h.o) = wi (h),

oOi

y1 ((h)) 

X

h E2

y2 ((h)) 

X

h E1

xi (h)  1  bi (h),
wi (h)  Ui (h)bi (h),

i = 1, 2, h  Hi
i = 1, 2, h  Hi

(109)
(110)

xi (h)  0,

i = 1, 2, h  Hi

(111)

wi (h)  0,

i = 1, 2, h  Hi

(112)

bi (h)  {0, 1},

i = 1, 2, h  Hi

yi ()  (, +),

(113)

i = 1, 2,   i (114)

Table 4: MILP-2 agents. This 0-1 mixed integer linear program, derived from game
theoretic considerations, finds optimal stochastic joint policies for DEC-POMDPs
with 2 agents.

5.3 Program for 2 Agents
When we combine the policy constraints (Section 3.2), the constraints we have just seen
for a policy to be a best response (Sections 5.1, 5.2) and a maximization of the value of the
joint policy, we can derive a 0-1 mixed integer linear program the solution of which is an
optimal joint policy for a DEC-POMDP for 2 agents. Table 4 details this program that we
will call MILP-2 agents.
Example The formulation of the decentralized Tiger problem for 2 agents and for an
horizon of 2 can be found in the appendices, in Section E.4
The variables of the program are the vectors xi , wi , bi and yi for each agent i. Note that
for each agent i  I and for each history h of agent i, Ui (h) denotes the upper bound on
the regret of history h.
356

fiMathematical Programming for DEC-POMDPs

A solution (x , y  , w , b ) to MILP-2 agents consists of the following quantities: (i)
an optimal joint policy x = hx1 , x2 i which may be stochastic; (ii) for each agent i = 1,
2, for each history h  Hi , wi (h), the regret of h given the policy xi of the other agent;
(iii) for each agent i = 1, 2, for each information set   i , yi (), the value of  given
the policy xi of the other agent; (iv) for each agent i = 1, 2, the vector bi simply tells us
which histories are not in the support of xi ; each history h of agent i such that bi (h) = 1 is
not in the support of xi . Note that we can replace y1 () by y2 () in the objective function
without affecting the program. We have the following result.
Theorem 5.1. Given a solution (x , w , y  , b ) to MILP-2 agents, x = hx1 , x2 i is an
optimal joint policy in sequence-form.
Proof: Due to the policy constraints of each agent, each xi is a sequence-form policy of
agent i. Due to the constraints (106)-(108), yi contains the values of the information sets
of agent i given xi . Due to the complementarity constraints (109)-(110), each xi is a best
response to xi . Thus hx1 , x2 i is a Nash equilibrium. Finally, by maximizing the value
of the null information set of agent 1, we are effectively maximizing the value of hx1 , x2 i.
Thus hx1 , x2 i is an optimal joint policy.

In comparison with the MILP presented before in Table 3, MILP-2 agents should
constitutes a particularly effective program in term of computation time for finding a 2agent optimal T -period joint policy because it is a much smaller program. While the number
of variables required by MILP is exponential in T and in n, the number of variables required
by MILP-2 agents is exponential only in T . This represents a major reduction in size
that should lead to an improvement in term of computation time.
5.4 Program for 3 or More Agents
When the number of agents is more than 2, the constraint (86) of the non-linear program
(82)-(90) is no longer a complementarity constraint
between 2 variables that could be linQ
earized as before. In particular, the term kI\{i} xk (jk ) of the constraint (86) involves as
many variables as there are different agents. To linearize this term, we will restrict ourselves once again to pure joint policies and exploit some combinatorial facts on the number
of histories involved. This leads to the 0-1 mixed linear program called MILP-n agents
and depicted in Table 5.
The variables of the program MILP-n agents are the vectors xi , wi , bi and yi for each
agent i and the vector z. We have the following result.
Theorem 5.2. Given a solution (x , w , y  , b , z  ) to MILP-n agents, x = hx1 , x2 ,
   , xn i is a pure T -period optimal joint policy in sequence-form.
Proof: Due to the policy constraints and the domain constraints of each agent, each
is a pure sequence-form policy of agent i. Due to the constraints (118)-(119), each yi
contains the values of the information sets of agent i given xi . Due to the complementarity
constraints (122)-(123), each xi is a best response to xi . Thus x is a Nash equilibrium.
Finally, by maximizing the value of the null information set of agent 1, we are effectively
maximizing the value of x . Thus x is an optimal joint policy.

xi

357

fiAras & Dutech

Variables:
xi (h), wi (h) and bi (h) i  I and h  Hi
yi () i  I,   i
z(j) j  E
Maximize

y1 ()

(115)

xi (a) = 1

(116)

xi (h.o.a) = 0,

i  I, h  Ni , o  Oi (117)

subject to:
X

aAi

xi (h) +

X

aAi

yi ((h)) 

X

yi (h.o) = wi (h),

i  I, h  Ni

(118)

oOi

yi ((h)) 

X
1
R(, hh, ji i)z(j) = wi (h), i  I, h  Ei
T
1
|Oi |
jE
X
Y
z(hh, j  i) = xi (h)
|Ok |T 1 ,
j  Ei

(119)

kI\{i}

i  I, h  Ei
X
Y
z(j) =
|Oi |T 1
jE

(120)
(121)

iI

xi (h)  1  bi (h),

i  I, h  Hi (122)

wi (h)  Ui (h)bi (h),

i  I, h  Hi (123)

xi (h)  0,

i  I, h  Ni

xi (h)  {0, 1}
wi (h)  0,

i  I, h  Ei

i  I, h  Hi

bi (h)  {0, 1},

h  Hi

yi ()  (, +),
z(j)  [0, 1],

j  E

(124)
(125)
(126)
(127)

i  I,   i(128)
(129)

Table 5: MILP-n agents. This 0-1 mixed integer linear program, derived from game
theoretic considerations, finds pure optimal joint policies for DEC-POMDPs with
3 or more agents.

358

fiMathematical Programming for DEC-POMDPs

Compared to the MILP of Table 3, MILP-n agents has roughly the same size but
with more real valued variables and more 0-1 variables. To be precise, P
MILP has a 0-1
variable for every terminal history of every agent (that is approximatively iI |Ai |T |Oi |T 1
integer variables) while MILP-n agents has two 0-1 variables
for every terminal as well
P
as nonterminal history of each agent (approximatively 2 iI (|Ai ||Oi |)T integer variables).

5.5 Summary

The formulation of the solution of a DEC-POMDP and the application of the Duality
Theorem for Linear Programs allow us to formulate the solution of a DEC-POMDP as the
solution of a new kind of 0-1 MILP. For 2 agents, this MILP has only O(kT ) variables
and constraints and is thus smaller than MILP of the previous section. Still, all these
MILPS are quite large and the next section investigates heuristic ways to speed up their
resolution.

6. Heuristics for Speeding up the Mathematical Programs
This section focusses on ways to speed up the resolution of the various MILPs presented so
far. Two ideas are exploited. First, we show how to prune the set of sequence-form policies
by removing histories that will provably not be part of the optimal joint policy. These
histories are called locally extraneous. Then, we give some lower and uppers bounds to
the objective function of the MILPs, these bounds can sometimes be used in the branch and
bound method often used by MILP solvers to finalize the values of the integer variables.
6.1 Locally Extraneous Histories
A locally extraneous history is a history that is not required to find an optimal joint policy
when the initial state of the DEC-POMDP is  because it could be replaced by a co-history
without affecting the value of the joint policy. A co-history of a history h of an agent is
defined to be a history of that agent that is identical to h in all aspects except for its last
action. If Ai = {b, c}, the only co-history of c.u.b.v.b is the history c.u.b.v.c. The set of
co-histories of a history h shall be denoted by C(h).
Formally, a history h  Hit of length t of agent i is said to be locally extraneous if,
t of i-reduced joint histories of length t,
for every probability distribution  over the set Hi
there exists a history h  C(h) such that
X

	
(j  ) R(, hh , j  i)  R(, hh, j  i)
 0
(130)
j  Hti

where (j  ) denotes the probability of j  in .
An alternative definition is as follows. A history h  Hit of length t of agent i is said to be
locally extraneous if there exists a probability distribution  over the set of co-histories
of h such that for each i-reduced joint history j  of length t, there holds
X
(h )R(, hh , j  i)  R(, hh, j  i)
(131)
h C(h)

359

fiAras & Dutech

where (h ) denotes the probability of the co-history h in .
The following theorem justifies our incremental pruning of locally extraneous histories
so that the search for optimal joint policies is faster because it is performed on a smaller
set of possible support histories.
Theorem 6.1. For every optimal T -period joint policy p such that for some agent i  I
and for a terminal history h of agent i that is locally extraneous at , pi (h) > 0, there exists
another T -period joint policy p that is optimal at  and that is identical to p in all respects
except that pi (h) = 0.
Proof: Let p be a T -period joint policy that is optimal at . Assume that for some
agent i  I and for a terminal history h of agent i that is locally extraneous at , pi (h) > 0.
By (130), there exists at least one co-history h of h such that,
X

	
pi (j  ) R(, hh , j  i)  R(, hh, j  i)
 0.
(132)
j  HT
i

Let q be a T -period policy of agent i that is identical to pi in all respects except that q(h )
= pi (h) + pi (h ) and q(h) = 0. We shall show that q is also optimal at . There holds,
X

j  HT
i

V(, hq, pi i)  V(, hpi , pi i) =

	
pi (j  ) R(, hh , j  i)q(h )  R(, hh , j  i)pi (h )  R(, hh, j  i)pi (h)
=
X

j  HT
i


	
pi (j  ) R(, hh , j  i)(q(h )  pi (h ))  R(, hh, j  i)pi (h)
=
X

j  HT
i


	
pi (j  ) R(, hh , j  i)pi (h)  R(, hh, j  i)pi (h)

since q(h ) = pi (h) + pi (h ). Therefore,
X

j  HT
i

V(, hq, pi i)  V(, hpi , pi i) =

	
pi (j  ) R(, hh , j  i)  R(, hh, j  i)
 0 (due to (132)).

Hence, p = hq, pi i is also an optimal T -period joint policy at .



One could also wonder if the order with which extraneous histories are pruned is important
or not. To answer this question, the following theorem shows that if many co-histories are
extraneous, they can be pruned in any order as:
 either they all have the same value, so any one of them can be pruned ;
 or pruning one of them does not change the fact that the others are still extraneous.
Theorem 6.2. If two co-histories h1 and h2 are both locally extraneous, either their values
t are equal or h is also locally extraneous
R(, hh1 , j  i) and R(, hh2 , j  i)for all j   Hi
1
relatively to C(h) \ {h2 }.
360

fiMathematical Programming for DEC-POMDPs

Proof: Let C + denotes the union C(h1 )  C(h2 ). We have immediately that C(h1 ) =
C + \ {h1 } and C(h2 ) = C + \ {h2 }. h1 (resp. h2 ) being locally extraneous means that there
exists a probability distribution 1 on C(h1 ) (resp. 2 on C(h2 )) such that, for all j  of
t :
Hi
X
1 (h )R(, hh , j  i)  R(, hh1 , j  i)
(133)
h C + \{h1 }

X

2 (h )R(, hh , j  i)  R(, hh2 , j  i)

(134)

h C + \{h2 }

(135)
Eq. (133) can be expanded in:
X

1 (h2 )R(, hh2 , j  i) +

h C + \{h

1 (h )R(, hh , j  i)  R(, hh1 , j  i).
1 ,h2 }

Using (134) in (136) gives
X
1 (h2 )
2 (h )R(, hh , j  i) +
h C + \{h2 }

leading to
X

(136)

X

1 (h )R(, hh , j  i)  R(, hh1 , j  i)

h C + \{h1 ,h2 }

(137)

(1 (h2 )2 (h ) + 1 (h ))R(, hh , j  i)  (1  1 (h2 )2 (h1 ))R(, hh1 , j  i) (138)

h C + \{h1 ,h2 }

So, two cases are possible:
 1 (h2 ) = 2 (h1 ) = 1. In that case, as R(, hh2 , j  i)  R(, hh1 , j  i) and R(, hh1 , j  i) 
t .
R(, hh2 , j  i), we have that R(, hh1 , j  i) = R(, hh2 , j  i) for all j  of Hi
 1 (h2 )2 (h1 ) < 1. In that case we have:
X

h C + \{h1 ,h2 }

1 (h2 )2 (h ) + 1 (h )
R(, hh , j  i)  R(, hh1 , j  i)
1  1 (h2 )2 (h1 )

(139)

meaning that even without using h2 , h1 is still locally extraneous because
1 (h2 )2 (h )+1 (h )
is a probability distribution over C + \ {h1 , h2 }
11 (h2 )2 (h1 )
X

h C + \{h

1 ,h2 }

1 (h2 )(1  2 (h1 )) + (1  1 (h2 ))
1 (h2 )2 (h ) + 1 (h )
=
1  1 (h2 )2 (h1 )
1  1 (h2 )2 (h1 )
1  1 (h2 )2 (h1 )
1  1 (h2 )2 (h1 )
= 1.
=

(140)
(141)
(142)


361

fiAras & Dutech

In order to prune locally extraneous histories, one must be able to identify these histories.
There are indeed two complementary ways for doing this.
The first method relies on the definition of the value of a history (see Section 3.3), that
is
R(, hh, j  i) = (, hh, j  i)R(, hh, j  i).

(143)

Therefore, if
(, hh, j  i) = 0,

t
j   Hi

(144)

is true for a history h, then that means that every joint history of length t occurring from
 of which the given history is a part of has an a priori probability of 0. thus, h is clearly
extraneous. Besides, every co-history of h will also be locally extraneous as they share the
same probabilities.
A second test is needed because some locally extraneous histories do not verify (144).
Once again, we turn to linear programing and in particular to the following linear program
t
Variables: y(j), j  Hi
Minimize



(145)

subject to:
X

j  Hti


	
y(j  ) R(, hh , j  i)  R(, hh, j  i)
 ,
X

h  C(h)

y(j  ) = 1

(146)
(147)

j  Hti

because of the following Lemma.

y(j  )  0,

t
j   Hi

(148)

Lemma 6.1. If, it exists a solution ( , y  ) to the linear program (145)-(148) where   0,
then h is locally extraneous.
Proof : Let ( , y  ) be a solution to the LP (145)-(148). y  is a probability distribution
t due to constraints (147)-(148). If   0, since we are minimizing , due to
over Hi
t ), and for every co-history h of h
constraints (146), we have that for every y  (Hi
X

	
y(j  ) R(, hh , j  i)  R(, hh, j  i)
  .
(149)
j  Hti

Therefore, by definition, h is locally extraneous.



The following procedure identifies all locally extraneous terminal histories of all the agents
and proceed to their iterative pruning. This is mainly motivated by Theorems 6.1 and 6.2
for effectively removing extraneous histories. The procedure is similar to the procedure of
iterated elimination of dominated strategies in a game (Osborne & Rubinstein, 1994). The
concept is also quite similar to the process of policy elimination in the backward step of the
dynamic programming for partially observable stochastic games (Hansen et al., 2004).
362

fiMathematical Programming for DEC-POMDPs

 Step 1: For each agent i  I, set HiT to Ei . Let H T denote the set iI HiT . For
each joint history j  H T , compute and store the value R(, j) of j and the joint
observation sequence probability (, j) of j.
 Step 2: For each agent i  I, for each history h  HiT , if for each i-reduced joint
T , (, hh, j  i) = 0, remove h from H T .
history j   Hi
i
 Step 3: For each agent i  I, for each history h  HiT do as follows: If C(h)  HiT
is non-empty, check whether h is locally extraneous or not by setting up and solving
t by the set H T and the set C(h)
LP (145)-(148). When setting the LP, replace Hi
i
by the set C(h)  HiT . If upon solving the LP, h is found to be locally extraneous at
, remove h from HiT .
 Step 4: If in Step 3 a history (of any agent) is found to be locally extraneous, go to
Step 3. Otherwise, terminate the procedure.
The procedure builds the set HiT for each agent i. This set contains every terminal
history of agent i that is required for finding an optimal joint policy at , that is every
terminal history that is not locally extraneous at . For each agent i, every history that
is in HiT but not in HiT is locally extraneous. The reason for reiterating Step 3 is that if
a history h of some agent i is found to be locally extraneous and consequently removed
from HiT , it is possible that a history of some other agent that was previously not locally
extraneous now becomes so, due to the removal of h from HiT . Hence, in order to verify if
this is the case for any history or not, we reiterate Step 3.
Besides, Step 2 of the procedure below also prunes histories that are impossible given
the model of the DEC-POMDP because their observation sequence can not be observed.
A last pruning step can be taken in order to remove non-terminal histories that can only
lead to extraneous terminal histories. This last step is recursive, starting from histories of
horizon T  1, we remove histories hi that have no non-extraneous terminal histories, that
is, histories hi such that all h.o.a are extraneous for a  Ai and o  Oi .
Complexity The algorithm for pruning locally extraneous histories has an exponential
complexity. Each joint history must be examined to compute its value and its occurence
probability. Then, in the worst case, a Linear Program can be run for every local history in
order to check it is extraneous or not. Experimentations are needed to see if the prunning
is really interesting.
6.2 Cutting Planes
Previous heuristics were aimed at reducing the search space of the linear programs, which
incidentally has a good impact on the time needed to solve these programs. Another option
which directly aims at reducing the computation time is to use cutting planes (Cornuejols,
2008). A cut (Dantzig, 1960) is a special constraint that identifies a portion of the set
of feasible solutions in which the optimal solution provably does not lie. Cuts are used in
conjunction with various branch and bounds mechanism to reduce the number of possibles
combination of integer variables that are examined by a solver.
We will present two kinds of cuts.
363

fiAras & Dutech

Variables: y(j), j  H
Maximize

X

R(, j)y(j)

(153)

jE

subject to,
X

y(a) = 1

(154)

aA

y(j) +

X

y(j.o.a) = 0,

j  N , o  O

(155)

j  H

(156)

aA

y(j)  0,

Table 6: POMDP. This linear program finds an optimal policy for a POMDP.
6.2.1 Upper Bound for the Objective Function
The first cut we propose is the upper bound POMDP cut. The value of an optimal
T -period joint policy at  for a given DEC-POMDP is bounded from above by the value
VP of an optimal T -period policy at  for the POMDP derived from the DEC-POMDP.
This derived POMDP is the DEC-POMDP but assuming a centralized controller (i.e. with
only one agent using joint-actions).
A sequence-form representation of the POMDP is quite straightforward. Calling H the
set Tt=1 Ht of joint histories of lengths less than or equal to T and N the set H\E of nonterminal joint histories, a policy for POMDP with horizon T in sequence-form is a function
q from H to [0, 1] such that:
X
q(a) = 1
(150)
aA

q(j) +

X

q(j.o.a) = 0,

j  N , o  O

(151)

aA

The value VP (, q) of a sequence-form policy q is then given by:
X
VP (, q) =
R(, j)q(j)

(152)

jE

Thereby, the solution y  of the linear program of Table 6 is P
an optimal policy for the
POMDP of horizon T and the optimal value of the POMDP is jE R(, j)y  (j). So, the
value V(, p ) of the optimal joint policy p = hp1 , p2 ,    , pn i of the DEC-POMDP is
bounded by above by the value VP (, q  ) of the associated POMDP.
Complexity The complexity of finding an upper bound is linked to the complexity of
solving a POMDP which, as showed by Papadimitriou and Tsitsiklis (1987), can be PSPACE
(i.e. require a memory that is polynomial in the size of the problem, leading to a possible
exponential complexity in time). Once again, only experimentation can help us decide in
which cases the upper bound cut is efficient.
364

fiMathematical Programming for DEC-POMDPs

6.2.2 Lower Bound for the Objective Function
In the case of DEC-POMDPs with non-negative reward, it is trivial to show that the value
of a T -period optimal policy is bounded from below by the value of the T 1 horizon optimal
value. So, in the general case, we have to take into account the lowest reward possible to
compute this lower bound and we can say that:
X
R(, j)z(j)  V T 1 () + min min R(s, a)
(157)
aA sS

jE

where V T 1 is the value of the optimal policy with horizon T  1. The reasoning leads to
an iterated computation of DEC-POMDPs of longer and longer horizon, reminiscent of the
MAA* algorithm (Szer et al., 2005). Experiments will tell if it is worthwhile to solve bigger
and bigger DEC-POMDPs to take advantage of a lower bound or if it is better to directly
tackle the T horizon problem without using any lower bound.
Complexity To compute the lower bound, one is required to solve a DEC-POMDP whith
an horizon that is one step shorter than the current horizon. The complexity is clearly at
least exponential. In our experiments, the value of a DEC-POMDP has been used for the
same DEC-POMDP with a bigger horizon. In such case, the computation time has been
augmented by the best time to solve the smaller DEC-POMDP.
6.3 Summary
Pruning locally extraneous histories and using the bounds of the objective function can be
of practical use for software solving the MILPs presented in this paper. Pruning histories
means that the space of policies used by the MILP is reduced and, because the formulation
of the MILP depends on combinatorial characteristics of the DEC-POMDP, these MILP
must be altered as show in Appendix D.
Validity As far as cuts are concerned, they do not alter the solution found by the MILPs,
so a solution to these MILPs is still an optimal solution to the DEC-POMDP. When extraneous histories are pruned, at least one valid policy is left as a solution because, in step
3 of the algorithm, an history is pruned only if it has other co-histories left. Besides, this
reduced set of histories can still be used to build an optimal policy because of Theroem 6.1.
As a consequence, the MILP build on this reduced set of histories admit a solution and this
solution is one optimal joint policy.
In the next section, experimental results will allow us to understand in which cases the
heuristics introduced can be useful.

7. Experiments
The mathematical programs and the heuristics designed in this paper are tested on four
classical problems found in the literature. For these problems, involving two agents, we have
mainly compared the computation time required to solve a DEC-POMDP using Mixed
Integer Linear Programming methods to computation time reported for methods found
in the literature. Then we have tested our programs on three-agent problems randomly
designed.
365

fiAras & Dutech

Problem
MABC
MA-Tiger
Fire Fighting
Grid Meeting
Random Pbs

|Ai |
2
3
3
5
2

|Oi |
2
2
2
2
2

|S|
4
2
27
16
50

n
2
2
2
2
3

Table 7: Complexity of the various problems used as test beds.

MILP and MILP-2 are solved using the iLog Cplex 10 solver  a commercial set of
Java packages  that relies on a combination of the Simplex and Branch and Bounds
methods (Fletcher, 1987). The software is run on an Intel P4 at 3.4 GHz with 2Gb of
RAM using default configuration parameters. For the mathematical programs, different
combination of heuristics have been evaluated: pruning of locally extraneous histories, using
a lower bound cut and using an upper bound cut, respectively denoted LOC, Low and
Up in the result tables to come.
The Non-Linear Program (NLP) of Section 3.4 has been evaluated by using various solvers from the NEOS website (http://www-neos.mcs.anl.gov ), even thought this
method does not guarantee an optimal solution to the DEC-POMDP. Three solvers have
been used: LANCELOT (abbreviated as LANC.), LOQO and SNOPT.
The result tables also report results found in the literature for the following algorithms:
DP stands for Dynamic Programming from Hansen et al. (2004); DP-LPC is an improved
version of Dynamic Programming where policies are compressed in order to fit more of them
in memory and speed up their evaluation as proposed by Boularias and Chaib-draa (2008);
PBDP is an extension of Dynamic Programming where pruning is guided by the knowledge
of reachable belief-states as detailed in the work of Szer and Charpillet (2006); MAA* is
a heuristically guided forward search proposed by Szer et al. (2005) and a generalized and
improved version of this algorithm called GMAA* developed by Oliehoek et al. (2008).
The problems selected to evaluate the algorithms are detailed in the coming subsections.
They have been widely used to evaluate DEC-POMDPs algorithms in the literature and
their complexity, in term of space size, is summarized in Table 7.
7.1 Multi-Access Broadcast Channel Problem
Several versions of the Multi-Access Broadcast Channel (MABC) problem can be found in
the literature. We will use the description given by Hansen et al. (2004) that allows this
problem to be formalized as a DEC-POMDP.
In the MABC, we are given two nodes (computers) which are required to send messages
to each other over a common channel for a given duration of time. Time is imagined to
be split into discrete periods. Each node has a buffer with a capacity of one message. A
buffer that is empty in a period is refilled with a certain probability in the next period. In
a period, only one node can send a message. If both nodes send a message in the same
period, a collision of the messages occurs and neither message is transmitted. In case of
a collision, each node is intimated about it through a collision signal. But the collision
366

fiMathematical Programming for DEC-POMDPs

signaling mechanism is faulty. In case of a collision, with a certain probability, it does not
send a signal to either one or both nodes.
We are interested in pre-allocating the channel amongst the two nodes for a given number
of periods. The pre-allocation consists of giving the channel to one or both nodes in a period
as a function of the nodes information in that period. A nodes information in a period
consists only of the sequence of collision signals it has received till that period.
In modeling this problem as a DEC-POMDP, we obtain a 2-agent, 4-state, 2-actionsper-agent, 2-observations-per-agent DEC-POMDP whose components are as follows.
 Each node is an agent.
 The state of the problem is described by the states of the buffers of the two nodes.
The state of a buffer is either Empty or Full. Hence, the problem has four states:
(Empty, Empty), (Empty, Full), (Full, Empty) and (Full, Full).
 Each node has two possible actions, Use Channel and Dont Use Channel.
 In a period, a node may either receive a collision signal or it may not. So each node
has two possible observations, Collision and No Collision.
The initial state of the problem  is (Full, Full). The state transition function P, the
joint observation function G and the reward function R have been taken from Hansen et al.
(2004). If both agents have full buffers in a period, and both use the channel in that period,
the state of the problem is unchanged in the next period; both agents have full buffers in
the next period. If an agent has a full buffer in a period and only he uses the channel in
that period, then his buffer is refilled with a certain probability in the next period. For
agent 1, this probability is 0.9 and for agent 2, this probability is 0.1. If both agents have
empty buffers in a period, irrespective of the actions they take in that period, their buffers
get refilled with probabilities 0.9 (for agent 1) and 0.1 (for agent 2).
The observation function G is as follows. If the state in a period is (Full, Full) and
the joint action taken by the agents in the previous period is (Use Channel, Use Channel),
the probability that both receive a collision signal is 0.81, the probability that only one of
them receives a collision signal is 0.09 and the probability that neither of them receives a
collision signal is 0.01. For any other state the problem may be in a period and for any other
joint action the agents may have taken in the previous period, the agents do not receive a
collision signal.
The reward function R is quite simple. If the state in a period is (Full, Empty) and
the joint action taken is (Use Channel, Dont Use Channel) or if the state in a period is
(Empty, Full) and the joint action taken is (Dont Use Channel, Use Channel), the reward
is 1; for any other combination of state and joint action, the reward is 0.
We have evaluated the various algorithms on this problem for three different horizons (3,
4 and 5) and the respective optimal policies have a value of 2.99, 3.89 and 4.79. Results are
detailed in Table 8 where, for each horizon and algorithm, the value and the computation
time for the best policy found are given.
The results show that the MILP compares favorably to more classical algorithms except
for GMAA* that is always far better for horizon 4 and, for horizon 5, roughly within the
367

fiAras & Dutech

Resolution method
Program
Solver
Heuristics
MILP
Cplex
MILP
Cplex
Low
MILP
Cplex
Up
MILP
Cplex
LOC
MILP
Cplex
LOC, Low
MILP
Cplex
LOC, Up
MILP-2
Cplex
NLP
SNOPT
NLP
LANC.
NLP
LOQO
Algorithm
Family
DP
Dyn. Prog.
DP-LPC
Dyn. Prog.
PBDP
Dyn. Prog.
MAA*
Fw. Search
GMAA*
Fw. Search

Horizon 3
Value
Time
2.99
0.86
2.99 0.10 / 0.93
2.99 0.28 / 1.03
2.99 0.34 / 0.84
2.99 0.44 / 0.84
2.99 0.62 / 0.93
2.99
0.39
2.90
0.01
2.99
0.02
2.90
0.01
Value
Time
2.99
5
2.99
0.36
2.99
< 1s
2.99
< 1s
?
?

Horizon 4
Value
Time
3.89
900
3.89
0.39 / 900
3.89
0.56 / 907
3.89
1.05 / 80
3.89
1.44 / 120
3.89 1.61 / 10.2
3.89
3.53
3.17
0.01
3.79
0.95
3.79
0.05
Value
Time
3.89
17.59
3.89
4.59
3.89
2
3.89
5400
3.89
0.03

Horizon 5
Value
Time
-m
3.5 / -m
4.73 / -m
2.27 / -t
5.77 / -t
4.79
7.00 / 25
-m
4.70
0.21
4.69
20
4.69
0.18
Value
Time
-m
-m
4.79
105
-t
4.79
5.68

Table 8: MABC Problem. Value and computation time (in seconds) for the solution of
the problem as computed by several methods, best results are highlighted. When
appropriate, time shows first the time used to run the heuristics then the global
time, in the format heuristic/total time. -t means a timeout of 10,000s;
-m indicates that the problem does not fit into memory and ? indicates that
the algorithm was not tested on that problem.

same order of magnitude as MILP with the more pertinent heuristics. As expected, apart
for the simplest setting (horizon of 3), NLP based resolution can not find the optimal policy
of the DEC-POMDP, but the computation time is lower than the other methods. Among
MILP methods, MILP-2 is better than MILP even with the best heuristics for horizon 3
and 4. When the size of the problem increases, heuristics are the only way for MILPs to
be able to cope with the size of the problem. The table also shows that, for the MABC
problem, pruning extraneous histories using the LOC heuristic is always a good method and
further investigation revealed that 62% of the heuristics proved to be locally extraneous. As
far are cutting bounds are concerned, they dont seem to be very useful at first (for horizon
3 and 4) but are necessary for MILP to find a solution for horizon 5. For this problem,
one must also have in mind that there is only one optimal policy for each horizon.
7.2 Multi-Agent Tiger Problem
As explained in section 2.2, the Multi-Agent Tiger problem (MA-Tiger) has been introduced
in the paper from Nair et al. (2003). From the general description of the problem, we ob368

fiMathematical Programming for DEC-POMDPs

Joint Action
(Listen, Listen)
(Listen, Listen)
(Listen, Listen)
(Listen, Listen)
(Listen, Listen)
(Listen, Listen)
(Listen, Listen)
(Listen, Listen)
(*, *)

State
Left
Left
Left
Left
Right
Right
Right
Right
*

Joint Observation
(Noise Left, Noise Left)
(Noise Left, Noise Right)
(Noise Right, Noise Left)
(Noise Right, Noise Right)
(Noise Left, Noise Left)
(Noise Left, Noise Right)
(Noise Right, Noise Left)
(Noise Right, Noise Right)
(*, *)

Probability
0.7225
0.1275
0.1275
0.0225
0.0225
0.1275
0.1275
0.7225
0.25

Table 9: Joint Observation Function G for the MA-Tiger Problem.
tain a 2-agent, 2-state, 3-actions-per-agent, 2-observations-per agent DEC-POMDP whose
elements are as follows.
 Each person is an agent. So, we have a 2-agent DEC-POMDP.
 The state of the problem is described by the location of the tiger. Thus, S consists
of two states Left (tiger is behind the left door) and Right (tiger is behind the right
door).
 Each agents set of actions consists of three actions: Open Left (open the left door),
Open Right (open the right door) and Listen (listen).
 Each agents set of observations consists of two observations: Noise Left (noise coming
from the left door) and Noise Right (noise coming from the right door).
The initial state is an equi-probability distribution over S. The state transition function P,
joint observation function G and the reward function R are taken from the paper by Nair
et al. (2003). P is quite simple. If one or both agents opens a door in a period, the state of
the problem in the next period is set back to . If both agents listen in a period, the state
of the process in unchanged in the next period. G, given in Table (9), is also quite simple.
Nair et al. (2003) describes two reward functions called A and B for this problem, here
we report only results for reward function A, given in Table 10, as the behavior of the
algorithm are similar for both reward functions. The optimal value of this problem for
horizons 3 and 4 are respectively 5.19 and 4.80.
For horizon 3, dynamic programming or forward search methods are generally better
than mathematical programs. But this is the contrary for horizon 4 were the computation time of MILP with the Low heuristic is significatively better than any other, even
GMAA*. Unlike MABC, the pruning of extraneous histories does not improve methods
based on MILP, this is quite understandable as deeper investigations showed that there are
no extraneous histories. Using lower cutting bounds proves to be very efficient and can be
seen as a kind of heuristic search for the best policy ; not directly in the set of policies (like
369

fiAras & Dutech

Joint Action
(Open Right, Open Right)
(Open Left, Open Left)
(Open Right, Open Left)
(Open Left, Open Right)
(Listen, Listen)
(Listen, Open Right)
(Open Right, Listen)
(Listen, Open Left)
(Open Left, Listen)

Left
20
-50
-100
-100
-2
9
9
-101
-101

Right
-50
20
-100
-100
-2
-101
-101
9
9

Table 10: Reward Function A for the MA-Tiger Problem.

GMAA*) but in the set of combination of histories, which may explain the good behavior
of MILP+Low.
It must also be noted that for this problem, approximate methods like NLP but also
other algorithms not depicted here like the Memory Bound Dynamic Programming of
Seuken and Zilberstein (2007) are able to find the optimal solution. And, once again,
methods based on a NLP are quite fast and sometimes very accurate.
7.3 Fire Fighters Problem
The problem of the Fire Fighters (FF) has been introduced as a new benchmark by Oliehoek
et al. (2008). It models a team of n fire fighters that have to extinguish fires in a row of nh
houses.
The state of each house is given by an integer parameter, called the fire level f , that
takes discrete value between 0 (no fire) and nf (fire of maximum severity). At every time
step, each agent can move to any one house. If two agents are at the same house, they
extinguish any existing fire in that house. If an agent is alone, the fire level is lowered with
a 0.6 probability if a neighbor house is also burning or with a 1 probability otherwise. A
burning house with no fireman present will increase its fire level f by one point with a 0.8
probability if a neighbor house is also burning or with a probability of 0.4 otherwise. An
unattended non-burning house can catch fire with a probability of 0.8 if a neighbor house
is burning. After an action, the agents receive a reward of f for each house that is still
burning. Each agent can only observe if there are flames at its location with a probability
that depends on the fire level: 0.2 if f = 0, 0.5 if f = 1 and 0.8 otherwise. At start,
the agents are outside any of the houses and the fire level of the houses is sampled from a
uniform distribution.
The model has the following characteristics:
 na agents, each with nh actions and nf possible informations.

h 1
states as there are nnf h possible states for the burning houses
 There are nnf h . na +n
n
a

h 1
different ways to distribute the na fire fighters in the houses. For
and na +n
na
example, 2 agents with 3 houses and 3 levels of fire lead to 9  6 = 54 states. But, it
370

fiMathematical Programming for DEC-POMDPs

Resolution method
Program
Solver
Heuristics
MILP
Cplex
MILP
Cplex
Low
MILP
Cplex
Up
MILP
Cplex
LOC
MILP
Cplex
LOC, Low
MILP
Cplex
LOC, Up
MILP-2
Cplex
NLP
SNOPT
NLP
LANC.
NLP
LOQO
Algorithm
Family
DP
Dyn. Prog.
DP-LPC
Dyn. Prog.
PBDP
Dyn. Prog.
MAA*
Fw. Search
GMAA*
Fw. Search

Horizon 3
Value
Time
5.19
3.17
5.19 0.46 / 4.9
5.19 0.42 / 3.5
5.19 1.41 / 6.4
5.19 1.88 / 7.6
5.19 1.83 / 6.2
5.19
11.16
-45
0.03
5.19
0.47
5.19
0.01
Value
Time
5.19
2.29
5.19
1.79
?
?
5.19
0.02
5.19
0.04

Horizon 4
Value
Time
-t
4.80
3.5 / 72
0.75 / -t
16.0 / -t
4.80 19.5 / 175
16.75 / -t
-t
-9.80
4.62
4.80
514
4.78
91
Value
Time
-m
4.80
534
?
?
4.80
5961
4.80
3208

Table 11: MA-Tiger Problem. Value and computation time (in seconds) for the solution
of the problem as computed by several methods, best results are highlighted.
When appropriate, time shows first the time used to run the heuristics then
the global time, in the format heuristic/total time.-t means a timeout of
10.000s; -m indicates that the problem does not fit into memory and ? indicates that the algorithm was not tested on that problem.

371

fiAras & Dutech

Resolution method
Program
Solver
Heuristics
MILP
Cplex
MILP-2
Cplex
NLP
SNOPT
NLP
LANC.
NLP
LOQO
Algorithm
Family
MAA*
Fw. Search
GMAA*
Fw. Search

Horizon 3
Value
Time
-t
-5.98
38
-5.98
0.05
-5.98
2.49
-6.08
0.24
Value
Time
(-5.73) 0.29
(-5.73)
0.41

Horizon 4
Value
Time
-t
-t
-7.08
4.61
-7.13
1637
-7.14
83
Value
Time
(-6.57) 5737
(-6.57) 5510

Table 12: Fire Fighting Problem. Value and computation time (in seconds) for the solution of the problem as computed by several methods, best results are highlighted.
-t means a timeout of 10.000s. For MAA* and GMAA*, value in parenthesis
are taken from the work of Oliehoek et al. (2008) and should be optimal but are
different from our optimal values.

is possible to use the information from the joint action to reduce the number of state
needed in the transition function to simply nnf h , meaning only 27 states for 2 agents
with 3 houses and 3 levels of fire.
 Transition, observation and reward functions are easily derived from the above description.
For this problem, dynamic programming based methods are not tested as the problem
formulation is quite new. For horizon 3, the value of the optimal policy given by Oliehoek
et al. (2008) (5.73) differs from the value found by the MILP algorithms (5.98) whereas
both methods are supposed to be exact. This might come from slight differences in our
respective formulation of the problems. For horizon 4, Oliehoek et al. (2008) report an
optimal value of (6.57).
For this problem, MILP methods are clearly outperformed by MAA* and GMAA*.
Only NLP methods, which give an optimal solution for horizon 3, are better in term of
computation time. It might be that NLP are also able to find optimal policies for horizon 4
but as our setting differs from the work of Oliehoek et al. (2008), we are not able to check
if the policy found is really the optimal. The main reason for the superiority of forward
search method lies in the fact that this problem admits many many optimal policies with
the same value. In fact, for horizon 4, MILP-based methods find an optimal policy quite
quickly (around 82s for MILP-2) but then, using branch-and-bound, must evaluate all the
other potential policies before knowing that it indeed found an optimal policy. Forward
search methods stop nearly as soon as they hit one optimal solution.
Heuristics are not reported as, not only do they not improve the performance of MILP
but they take away some computation time and thus the results are worse.
372

fiMathematical Programming for DEC-POMDPs

7.4 Meeting on a Grid
The problem called Meeting on a grid deals with two agents that want to meet and stay
together in a grid world. It has been introduced in the work of Bernstein, Hansen, and
Zilberstein (2005).
In this problem, we have two robots navigating on a two-by-two grid world with no
obstacles. Each robot can only sense whether there are walls to its left or right, and the
goal is for the robots to spend as much time as possible on the same square. The actions
are to move up, down, left or right, or to stay on the same square. When a robot attempts
to move to an open square, it only goes in the intended direction with probability 0.6,
otherwise it randomly either goes in another direction or stays in the same square. Any
move into a wall results in staying in the same square. The robots do not interfere with
each other and cannot sense each other. The reward is 1 when the agents share a square,
and 0 otherwise. The initial state distribution is deterministic, placing both robots in the
upper left corner of the grid.
The problem is modelled as a DEC-POMDP where:
 There are 2 agents, each one with 5 actions and observations (wall on left, wall on
right).
 There are 16 states, since each robot can be in any of 4 squares at any time.
 Transition, observation and reward functions are easily derived from the above description.
For this problem, dynamic programming based methods are not tested as the problem
formulation is quite new. This problem is intrinsically more complex that FF and as such
is only solved for horizon 2 and 3. Again, optimal value found by our method differ from
the value reported by Oliehoek et al. (2008). Whereas we found that the optimal values are
1.12 and 1.87 for horizon 2 and 3, they report optimal values of 0.91 and 1.55.
Results for this problem have roughly the same pattern that the results for the FF
problem. MAA* and GMAA* are quicker than MILP, but this time MILP is able to find
an optimal solution for horizon 3. NLP methods give quite good results but they are slower
than GMAA*. As for the FF, there are numerous optimal policies and MILP methods are
not able to detect that the policy found quickly is indeed optimal.
Again, heuristics are not reported as, not only do they not improve the performance of
MILP but they take away some computation time and thus the results are worse.
7.5 Random 3-Agent Problems
To test our approach on problems with 3 agents, we have used randomly generated DECPOMDPs where the state transition function, the joint observation function and the reward
functions are randomly generated. The DEC-POMDPs have 2 actions and 2 observations
per agent and 50 states. Rewards are randomly generated integers in the range 1 to 5.
The complexity of this family of problem is quite similar to the complexity of the MABC
problem (see Section 7.1).
373

fiAras & Dutech

Resolution method
Program
Solver
Heuristics
MILP
Cplex
MILP-2
Cplex
NLP
SNOPT
NLP
LANC.
NLP
LOQO
Algorithm
Family
MAA*
Fw. Search
GMAA*
Fw. Search

Horizon 2
Value Time
1.12
0.65
1.12
0.61
0.91
0.01
1.12
0.06
1.12
0.07
Value Time
(0.91)
0s
(0.91)
0s

Horizon 3
Value Time
1.87
1624
-t
1.26
1.05
1.87
257
0.48
81
Value Time
(1.55)
10.8
(1.55) 5.81

Table 13: Meeting on a Grid Problem. Value and computation time (in seconds) for
the solution of the problem as computed by several methods, best results are
highlighted. -t means a timeout of 10.000s. For MAA* and GMAA*, value
in parenthesis are taken from the work of Oliehoek et al. (2008) and should be
optimal but are different from our optimal values...

Program
MILP
MILP-2

Least Time (secs)
2.45
6.85

Most Time (secs)
455
356

Average
120.6
86.88

Std. Deviation
183.48
111.56

Table 14: Times taken by MILP and MILP-2 on the 2-agent Random Problem for horizon 4.

In order to assess the real complexity of this Random problem, we have first tested a
two-agent version of the problem for a horizon of 4. Results averaged over 10 runs of the
programs are given in Table 14. When compared to the MABC problem which seemed of
comparable complexity, the Random problem proves easier to solve (120s vs 900s). For this
problem, the number of 0-1 variable is relatively small, as such it does not weight too much
on the resolution time of MILP-2 which is thus faster.
Results for a three-agent problem with horizon 3 are given in Table 15, once again
averaged over 10 runs. Even though the size of the search space is smaller in that case
(for 3 agents and a horizon of 3, there are 9  1021 policies whereas the problem with 2
agents and horizon 4, there are 1.5  1051 possible policies), the 3 agent problems seems
more difficult to solve, demonstrating that one of the big issue is policy coordination. Here,
heuristics bring a significative improvement on the resolution time of MILP. As predicted,
MILP-n is not very efficient and is only given for completeness.
374

fiMathematical Programming for DEC-POMDPs

Program
MILP
MILP-Low
MILP-n

Least Time (secs)
21
26
754

Most Time (secs)
173
90
2013

Average
70.6
53.2
1173

Std. Deviation
64.02
24.2
715

Table 15: Times taken by MILP and MILP-n on the 3-agent Random problem for horizon 3.

8. Discussion
We have organized the discussion in two parts. In the first part, we analyze our results and
offer explanations on the behavior of our algorithms and the usefulness of heuristics. Then,
in a second part, we explicitely address some important questions.
8.1 Analysis of the Results
From the results, it appears that MILP methods are a better alternative to Dynamic Programming methods for solving DEC-POMDPs but are globally and generally clearly outperformed by forward search methods. The structure and thus the characteristics of the
problem have a big influence on the efficiency of the MILP methods. Whereas it seems
that the behavior of GMAA* in terms of computation time is quite correlated with the
complexity of the problem (size of the action and observation spaces), MILP methods seem
sometimes less correlated to this complexity. It is the case for the MABC problem (many
extraneous histories can be pruned) and the MA-Tiger problem (special structure) where
they outperform GMAA*. On the contrary, when many optimal policies exists, forward
search methods like GMAA* are clearly a better choice. Finally, Non-Linear Programs,
even though they can not guarantee an optimal solution, are generally a good alternative
as they are sometimes able to find a very good solution and their computation time is often
better than GMAA*. This might prove useful for approximate heuristic-driven forward
searches.
The computational record of the two 2-agent programs shows that MILP-2 agents is
slower than MILP when the horizon grows. There are two reasons to which the sluggishness
of MILP-2 agents may be attributed. The time taken by the branch and bound (BB)
method to solve a 0-1 MILP is inversely proportional to the number of 0-1 variables in
the MILP. MILP-2 agents has many more 0-1 variables than MILP event hough the
total number of variables in it is exponentially less than in MILP. This is the first reason.
Secondly, MILP-2 agents is a more complicated program than MILP; it has many more
constraints than MILP. MILP is a simple program, concerned only with finding a subset
of a given set. In addition to finding weights of histories, MILP also finds weights of
terminal joint histories. This is the only extra or superfluous quantity it is forced to find.
On the other hand, MILP-2 agents takes a much more circuitous route, finding many more
superfluous quantities than MILP. In addition to weights of histories, MILP-2 agents
also finds supports of policies, regrets of histories and values of information sets. Thus, the
375

fiAras & Dutech

Problem

Heuristic

MABC

LOC
Low
Up
LOC
Low
Up
LOC

MA-Tiger

Meeting

Horizon 2
Time #pruned

0.41

0/18

1.36

15/50*

Horizon 3
Time #pruned
0.34
14/32
0.10
0.28
1.41
0/108
0.46
0.42
74.721 191/500*

Horizon 4
Time #pruned
1.047
74/128
0.39
3.89
16.0
0/648
3.5
0.75

Horizon 5
Time #pruned
2.27
350/512
3.5
4.73

Table 16: Computation time of heuristics. For the LOC heuristics, we give the computation time in seconds and the number of locally extraneous histories pruned
over the total number of histories (for an agent). A * denotes cases where one
additional history is prunned for the second agent. For the Low and Up heuristic,
only computation time is given.

relaxation of MILP-2 agents takes longer to solve than the relaxation of MILP. This is
the second reason for the slowness with which the BB method solves MILP-2 agents.
For bigger problems, namely Fire-Fighters and Meeting on a Grid, when the horizon
stays small, MILP-2 agents can compete with MILP because of its slightly lower size. Its
complexity grows like O((|Ai ||Oi |)T ) whereas it grows like O((|Ai ||Oi |)2T ) for MILP. But
that small difference does not hold long as the number of integer variables quickly lessens
the efficiency of MILP-2 agents.
As far as heuristic are concerned, they proved to be invaluable for some problems (MABC
and MA-Tiger) and useless for others. In the case of MABC, heuristics are very helpful to
prune a large number of extraneous heuristics but ultimately, it is the combination with
the upper bound cut that it the more efficient when the horizon grows. In the case of
MA-Tiger, although no extraneous histories are found, using the lower bound cut heuristic
with MILP leads to the quickest algorithm for solving the problem with a horizon of 4.
For other problems, heuristics are more of a burden as they are too greedy in computation
time to speed up the resolution. For example, for the Grid Meeting problem, the time
taken to prune extraneous histories is bigger than the time saved for solving the problem.
As a result, the added value of using heuristics depends on the nature of the problem
(as depicted in Table 16) but, right now, we are not able to predict their usefulness without
trying them.
We also emphasize that the results given here lie at the limit of what is possible to solve
in an exact manner given the memory of the computer used for the resolution, especially
in terms of the horizon. Furthermore, as the number of agent increases, the length of the
horizon must be decreased for the problems to still be solvable.
376

fiMathematical Programming for DEC-POMDPs

8.2 Questions
The mathematical programing approach presented in this paper raises different questions.
We have explicitly addressed some of the questions that appears important to us.
Q1: Why is the sequence-form approach not entirely doomed by its exponential
complexity?
As the number of sequence-form joint policies grows doubly exponentially with the horizon and the number of agents, the sequence-form approach seems doomed, even compared
to dynamic programming which is doubly exponential in the worst cases only. But, indeed,
some arguments must be taken into consideration.
Only an exponential number of individual histories need to be evaluated. The joint
part of the sequence-form is left to the MILP solver. And every computation done on a
particular history, like computing its value or checking if it is extraneous, has a greater
reusability than computations done on entire policies. An history is shared by many
more joint policies than an individual policy. In some way, sequence-form allows us to work
on reusable part of policies without having to work directly in the world of distributions on
the set of joint-policies.
Then, the MILPs derived from the sequence-form DEC-POMDPs need a memory size
which grows only exponentially with the horizon and the number of agents. Obviously,
such a complexity is quickly overwhelming but it is also the case of every other exact method
so far. As shown by the experiments, the MILP approach derived from the sequence-form
compares quite well with dynamic programming, even if outperformed by forward methods
like GMAA*.
Q2: Why does MILP sometimes take so little time to find an optimal joint
policy when compared to existing algorithms?
Despite the complexity of our MILP approach, three factors contribute to the relative
efficiency of MILP.
1. First, the efficiency of linear programming tools themselves. In solving MILP, the
BB method solves a sequence of linear programs using the simplex algorithm. Each of
these LPs is a relaxation of MILP. In theory, the simplex algorithm requires in the
worst case an exponential number of steps (in the size of the LP) in solving a LP2 ,
but it is well known that, in practice, it usually solves a LP in a polynomial number
of steps (in the size of the LP). Since the size of a relaxation of MILP is exponential
in the horizon, this means that, roughly speaking, the time taken to solve a relaxation
of MILP is only exponential in the horizon whereas it can be doubly exponential
for other methods.
2. The second factor is the sparsity of the matrix of coefficients of the constraints of
MILP. The sparsity of the matrix formed by the coefficients of the constraints of
2. This statement must be qualified: this worst case time requirement has not been demonstrated for all
variants of the simplex algorithm. It has been demonstrated only for the basic version of the simplex
algorithm.

377

fiAras & Dutech

an LP determines in practice the rate with which a pivoting algorithm such as the
simplex solves the LP (this also applies to Lemkes algorithm in the context of an
LCP). The sparser this matrix, the lesser the time required to perform elementary
pivoting (row) operations involved in the simplex algorithm and the lesser the space
required to model the LP.
3. The third factor is the fact that we supplement MILP with cuts; the computational
experience clearly shows how this speeds up the computations. While the first two
factors were related to solving a relaxation of MILP (i.e., an LP), this third factor
has an impact on the BB method itself. The upper bound cut identifies an additional
terminating condition for the BB method, thus enabling it to terminate earlier than
in the absence of this condition. The lower bound cut attempts to shorten the list
of active subproblems (LPs) which the BB method solves sequentially. Due to this
cut, the BB method has potentially a lesser number of LPs to solve. Note that in
inserting the lower bound cut, we are emulating the forward search properties of the
A* algorithm.
Q3: How do we know that the MILP-solver (iLogs Cplex in our experiments)
is not the only reason for the speedup?
Clearly, our approach would be slower, even sometime slower than a classical dynamic
programming approach if we had used another program for solving our MILPs as we experimented also our MILPs with solvers from the NEOS website that were indeed very very
slow. It is true that Cplex, the solver we have used in our experiments, is quite optimized.
Nevertheless, it is exactly one of the points we wanted to experiment with in this paper:
one of the advantages of formulating a DEC-POMDP as a MILP is the possibility to use
the fact that, as mixed integer linear programs are very important for the industrial world,
optimized solvers do exist.
Then, we had to formulate a DEC-POMDP as a MILP and this is mostly what this paper
is about.
Q4: What is the main contribution of this paper?
As stated earlier in the paper, current algorithms for DEC-POMDPs were largely inspired by POMDPs algorithms. Our main contribution was to pursue an entirely different
approach, i.e., mixed integer linear programming. As such, we have learned a lot about
DEC-POMDPs and about the pro & con of this mathematical programming approach. This
has lead to the formulation of new algorithms.
In designing these algorithms, we have, first of all, drawn attention to a new representation of a policy, namely the sequence form of a policy, introduced by Koller, Megiddo
and von Stengel. The sequence form of a policy is not a compact representation of the
policy of an agent, but it does afford a compact representation of the set of policies of the
agent.
The algorithms we have proposed for finite horizon DEC-POMDPs are mathematical
programming algorithms. To be precise, they are 0-1 MILPs. In the MDP domain,
378

fiMathematical Programming for DEC-POMDPs

mathematical programming has been long used for solving the infinite horizon case. For
instance, an infinite horizon MDP can be solved by a linear program (dEpenoux, 1963).
More recently, mathematical programming has been directed at infinite horizon POMDPs
and DEC-POMDPs. Thus, an infinite horizon DEC-MDP (with state transition independence) can be solved by a 0-1 MILP (Petrik & Zilberstein, 2007) and an infinite horizon
POMDP or DEC-POMDP can be solved (for local optima) by a nonlinear program (Amato, Bernstein, & Zilberstein, 2007b, 2007a). The finite horizon case  much different in
character than the infinite horizon case  has been dealt with using dynamic programming.
As stated earlier, whereas dynamic programming has been quite successful for finite horizon
MDPs and POMDPs, it has been less so for finite horizon DEC-POMDPs.
In contrast, in game theory, mathematical programming has been successfully directed
at games of finite horizon. Lemkes algorithm (1965) for two-player normal form games, the
Govindan-Wilson algorithm (2001) for n-player normal form games and the Koller, Megiddo
and von Stengel approach (which internally uses Lemkes algorithm) for two-player extensive
form games are all for finite-horizon games.
What remained then was to a find way to appropriate mathematical programming for
solving the finite horizon case of the POMDP/DEC-POMDP domain. Our work has done
precisely this (incidently, we now have an algorithm for solving some kind of n-player normal form games). Throughout the paper, we have shown how mathematical programming
(in particular, 0-1 integer programming) can be applied for solving finite horizon DECPOMDPs (it is easy to see that the approach we have presented yields a linear program
for solving a finite horizon POMDP). Additionally, the computational experience of our
approach indicates that for finite horizon DEC-POMDPs, mathematical programming may
be better (faster) than dynamic programming. We have also shown how the well-entrenched
dynamic programming heuristic of the pruning of redundant or extraneous objects (in our
case, histories) can be integrated into this mathematical programming approach.
Hence, the main contribution of this paper is that it presents, for the first time, an alternative approach for solving finite horizon POMDPs/DEC-POMDPs based on MILPs.
Q5: Is the mathematical programming approach presented in this paper something of a dead end?
This question is bit controversial and a very short answer to this question could be
a small yes. But this is true for every approach that looks for exact optimal solutions
to DEC-POMDPs, whether it is grounded on dynamic programming or forward search or
mathematical programming. Because of the complexity of the problem, an exact solution
will always be untractable but our algorithms can still be improved.
A longer answer is more mitigated, especially in the light of the recent advances made
for dynamic programming and forward search algorithms. One crucial point in sequenceform DEC-POMDPs is the pruning of extraneous histories. A recent work from Oliehoek,
Whiteson, and Spaan (2009) has shown how to clusters histories that are equivalent in a
way that could also reduce the nomber of constraints in MILPs. The approach of Amato,
Dibangoye, and Zilberstein (2009) that improves and speed up the dynamic programming
operator could help in finding extraneous histories. So, at the very least, some work is
379

fiAras & Dutech

still required before stating that every aspect of sequence-form DEC-POMDPs have been
studied.
We now turn to an even longer answer. Consider the long horizon case. Given that exact
algorithms (including the ones presented in this paper) can only tackle horizons less than 6,
by long horizon, we mean anything upwards of 6 time periods. For the long horizon case,
we are required to conceive a possibly sub-optimal joint policy for the given horizon and
determine an upper bound on the loss of value incurred by using the joint policy instead of
using an optimal joint policy.
The current trend for the long horizon case is a memory-bounded approach. The memory
bounded dynamic programming (MBDP) algorithm (Seuken & Zilberstein, 2007) is the
main exponent of this approach. This algorithm is based on the backward induction DP
algorithm (Hansen et al., 2004). The algorithm attempts to run in a limited amount of
space. In order to do so, unlike the DP algorithm, it prunes even non-extraneous (i.e., nondominated) policy trees at each iteration. Thus, at each iteration, the algorithm retains
a pre-determined number of trees. This algorithm and its variants have been used to find
a joint policy for the MABC, the MA-tiger and the Box pushing problems for very long
horizons (of the order of thousands of time periods).
MBDP does not provide an upper bound on the loss of value. The bounded DP (BDP)
algorithm presented in the paper by Amato, Carlin, and Zilberstein (2007c) does give an
upper bound. However, on more interesting DEC-POMDP problems (such as MA-tiger),
MBDP finds a much better joint policy than BDP.
A meaningful way to introduce the notion of memory boundedness into our approach is
to fix an a priori upper bound on the size of the concerned mathematical program. This
presents all sorts of difficulties but the main difficulty seems to be the need to represent
a policy for a long horizon in limited space. The MBDP algorithm solves this problem
by using what may be termed as a recursive representation. The recursive representation
causes the MBDP algorithm to take a long time to evaluate a joint policy, but it does allow
the algorithm to represent a long horizon joint policy in limited space. In the context of
our mathematical programming approach, we would have to change the policy constraints
in some way so that a long horizon policy is represented by a system consisting of a limited
number of linear equations and linear inequalities. Besides the policy constraints, other
constraints of the presented programs would also have to be accordingly transfigured. It is
not evident (to us) if such a transfiguration of the constraints is possible.
On the other hand, the infinite horizon case seems to be a promising candidate to adapt
our approach to. Mathematical programming has already been applied, with some success,
to solving infinite horizon DEC-POMDPs (Amato et al., 2007a). The computational experience of this mathematical programming approach shows that it is better (finds higher
quality solutions in lesser time) than a dynamic programming approach (Bernstein et al.,
2005; Szer & Charpillet, 2006).
Nevertheless, this approach has two inter-related shortcomings. First, the approach
finds a joint controller (i.e., an infinite horizon joint policy) of a fixed size and not of the
optimal size. Second, much graver than the first, for the fixed size, it finds a locally optimal
joint controller. The approach does not guarantee finding an optimal joint controller. This
is because the program presented in the work of Amato et al. (2007a) is a (non-convex)
380

fiMathematical Programming for DEC-POMDPs

nonlinear program (NLP). The NLP finds a fixed size joint controller in the canonical form
(i.e., in the form of a finite state machine). We believe that both these shortcomings can
be removed by conceiving a mathematical program (specifically, a 0-1 mixed integer linear
program) that finds a joint controller in the sequence-form. As stated earlier, the main
challenge in this regard is therefore an identification of the sequence-form of an infinite
horizon policy. In fact, it may be that if such sequence-form characterization of an infinite
horizon policy is obtained, it could be used in conceiving a program for the long horizon
(undiscounted reward) case as well.
Q6: How does this help achieve designing artificial autonomous agents ?
At first sight, our work does not have any direct and immediate applied benefits for the
purpose of building artificial intelligent agents or understanding how intelligence works.
Even in the limited field of multi-agent planning, our contributions are more on a theoretical
level than on a practical one.
Real artificial multi-agent systems can indeed be modeled as DEC-POMDPs, even if they
make use of communication, of common knowledge, of common social law. Then, such real
systems would likely be made of a large number of states, actions or observations and require
solutions over a large horizon. Our mathematical programming approach is practically
useless in that setting as limited to DEC-POMDPs of very small size. Other models that
are simpler  but far from trivial  to solve because they explicitly take into account some
characteristics of the real systems do exist. Some works take advantage of communications
(Xuan, Lesser, & Zilberstein, 2000; Ghavamzadeh & Mahadevan, 2004), some of the existing
independencies in the system (Wu & Durfee, 2006; Becker, Zilberstein, Lesser, & Goldman,
2004), some do focus on interaction between agents (Thomas, Bourjot, & Chevrier, 2004),
some, as said while answering the previous questions, rely on approximate solutions, etc...
It is our intention to facilitate the re-use and the adaptation to these other models of the
concepts used in our work and of the knowledge about the structure of an optimal solution of
a DEC-POMDP. To that end, we decided not only to describe the MILP programs but also,
and most importantly, how we derived these programs by making use of some properties of
optimal DEC-POMDP solutions.
Truly autonomous agents will also require to adapt to new and unforeseen situations.
Our work being dedicated to planning, it seems easy to argue that it does not contribute
very much to that end either. On the other hand, learning in DEC-POMDPs has never
really been addressed except for some fringe work in particular settings (Scherrer & Charpillet, 2002; Ghavamzadeh & Mahadevan, 2004; Buffet, Dutech, & Charpillet, 2007). In fact,
even for simple POMDPs, learning is a very difficult task (Singh, Jaakkola, & Jordan,
1994). Currently, the more promising research deals with learning the Predictive State
Representation (PSR) of a POMDP (Singh, Littman, Jong, Pardoe, & Stone, 2003; James
& Singh, 2004; McCracken & Bowling, 2005). Making due allowance to the fundamental
differences between the functional role of PSR and histories, we notice that PSR and histories are quite similar in structure. While it is too early to say, it might be that trying to
learn the useful histories of a DEC-POMDP could take some inspiration from the way the
right PSRs are learned for POMDPs.

381

fiAras & Dutech

9. Conclusion
We designed and investigated new exact algorithms for solving Decentralized Partially Observable Markov Decision Processes with finite horizon (DEC-POMDPs). The main contribution of our paper is the use of sequence-form policies, based on a sets of histories, in
order to reformulate a DEC-POMDP as a non-linear programming problem (NLP). We
have then presented two different approaches to linearize the NLP in order to find global
and optimal solutions to DEC-POMDPs. The first approach is based on the combinatorial
properties of the optimal policies of DEC-POMDPs and the second one relies on concepts
borrowed from the field of game theory. Both lead to formulating DEC-POMDPs as 0-1
Mixed Integer Linear Programming problems (MILPs). Several heuristics for speeding up
the resolution of these MILPs make another important contribution of our work.
Experimental validation of the mathematical programming problems designed in this
work was conducted on classical DEC-POMDP problems found in the literature. These
experiments show that, as expected, our MILP methods outperform classical Dynamic
Programming algorithms. But, in general, they are less efficient and more costly than
forward search methods like GMAA*, especially in the case where the DEC-POMDP admits
many optimal policies. Nevertheless, according to the nature of the problem, MILP methods
can sometimes greatly outperform GMAA* (as in the MA-Tiger problem).
While it is clear that exact resolution of DEC-POMDPs can not scale up with the size
of the problems or the length of the horizon, designing exact methods is useful in order
to develop or improve approximate methods. We see at least three research directions
where our work can contribute. One direction could be to take advantage of the large
literature on algorithms for finding approximate solutions to MILPs and to adapt them to
the MILPs formulated for DEC-POMDPs. Another direction would be to use the knowledge
gained from our work to derive improved heuristics for guiding existing approximate existing
methods for DEC-POMDPs. For example, the work of Seuken and Zilberstein (2007), in
order to limit the memory resources used by the resolution algorithm, prune the space of
policies to only consider some of them; our work could help using a better estimation of
the policies that are important to be kept in the search space. Then, the one direction we
are currently investigating is to adapt our approach to DEC-POMDPs of infinite length by
looking for yet another representation that would allow such problems to be seen as MILPs.
More importantly, our work participates to a better understanding of DEC-POMDPs.
We analyzed and understood key characteristics of the nature of optimal policies in order
to design the MILPs presented in this paper. This knowledge can be useful for other work
dealing with DEC-POMDPs and even POMDPs. The experimentations have also given
some interesting insights on the nature of the various problems tested, in term of existence
of extraneous histories or on the number of optimal policies. These insights might be a first
step toward a taxonomy of DEC-POMDPs.

Appendix A. Non-Convex Non-Linear Program
Using the simplest example, this section aims at showing that the Non-Linear Program
(NLP) expressed in Table 2 can be non-convex.
382

fiMathematical Programming for DEC-POMDPs

Let us consider an example with two agents, each one with 2 possible actions (a and b)
that want to solve a horizon-1 decision problem. The set of possible joint-histories is then:
ha, ai, ha, bi, hb, ai and hb, bi. Then the NLP to solve is:
Variables: x1 (a), x1 (b), x2 (a), x2 (a)
Maximize

R(, ha, ai)x1 (a)x2 (a) + R(, ha, bi)x1 (a)x2 (b)

(158)

+R(, hb, ai)x1 (b)x2 (a) + R(, hb, bi)x1 (b)x2 (b)
subject to
x1 (a) + x1 (b) = 1
x2 (a) + x2 (b) = 1
x1 (a)  0,

x1 (b)  0

x2 (a)  0,

x2 (b)  0

A matrix formulation of the objective
x of the following kind:

0 0 c
 0 0 e
C=
 c e 0
d f 0

function of eq. (158) would be xT .C.x with C and

d
f 

0 
0




x1 (a)
 x1 (b) 

x=
 x2 (a)  .
x2 (b)

(159)

If  is the eigen value of vector v = [v1 v2 v3 v4 ]T then it is straightforward to show that
 is also an eigen value: [v1  v2 v3 v4 ]T = C.[v1 v2  v3  v4 ]T . As a result, the
matrix C, hessian of the objective function, is not positive-definite and thus the objective
function is not convex.

Appendix B. Linear Program Duality
Every linear program (LP) has a converse linear program called its dual. The first LP is
called the primal to distinguish it from its dual. If the primal maximizes a quantity, the
dual minimizes the quantity. If there are n variables and m constraints in the primal, there
are m variables and n constraints in the dual. Consider the following (primal) LP.
Variables: x(i), i  {1, 2,    , n}
Maximize

n
X

c(i)x(i)

i=1

subject to:
n
X

a(i, j)x(i) = b(j),

j = 1, 2,    , m

i=1

x(i)  0,

i = 1, 2,    , n

383

fiAras & Dutech

This primal LP has one variable x(i) for each i = 1 to n. The data of the LP consists
of numbers c(i) for each i = 1 to n, the numbers b(j) for each j = 1 to m and the numbers
a(i, j) for each i = 1 to n and for each j = 1 to m. The LP thus has n variables and m
constraints. The dual of this LP is the following LP.
Variables: y(j), j  {1, 2,    , m }


Minimize

m
X

b(j)y(j)

j=1

subject To:


m
X

a(i, j)y(j)  c(i),

i = 1, 2,    , n

j=1

y(j)  (, +),

j = 1, 2,    , m

The dual LP has one variable y(j) for each j = 1 to m. Each y(j) variable is a free
variable. That is, it is allowed to take any value in R. The dual LP has m variables and n
constraints.
The theorem of linear programming duality is as follows.
Theorem B.1. (Luenberger, 1984) If either a primal LP or its dual LP has a finite optimal
solution, then so does the other, and the corresponding values of the objective functions are
equal.
Applying this theorem to the primal-dual pair given above, there holds,
n
X

c(i)x (i) =

m
X

b(j)y  (j)

j=1

i=1

where x denotes an optimal solution to the primal and y  denotes an optimal solution to
the dual.
The theorem of complementary slackness is as follows.
Theorem B.2. (Vanderbei, 2008) Suppose that x is feasible for a primal linear program and
y is feasible for its dual. Let (w1 ,   ,wm ) denote the corresponding primal slack variables,
and let (z1 ,   ,zn ) denote the corresponding dual slack variables. Then x and y are optimal
for their respective problems if and only if
xj zj = 0

for j = 1,    , n,

wi yi = 0

for i = 1,    , m.
384

fiMathematical Programming for DEC-POMDPs

Appendix C. Regret for DEC-POMDPs
The value of an information set   Ii of an agent i for a i-reduced joint policy q,
denoted i (, q), is defined by:
X
i (, q) = max
R(, hh, j  i)q(j  )
(160)
h

j  Ei

for any terminal information set and, if  is non-terminal, by:
X
i (h.o, q)
i (, q) = max
h

(161)

oOi

Then, the regret of a history h for an agent i and for a i-reduced joint policy q,
denoted i (h, q), it is defined by:
X
i (h, q) = i ((h), q) 
R(, hh, j  i)q(j  )
(162)
T
j  Hi

if h is terminal and, if h is non-terminal, by:
i (h, q) = i ((h), q) 

X

i (h.o, q)

(163)

oOi

The concept of regret of the agent i, which is independant of the policy of the agent i, is
very useful when looking for optimal policy because its optimal value is known: it is 0. It
is thus easier to manipulate than the optimal value of a policy.

Appendix D. Program Changes Due to Optimizations
Pruning locally or globally extraneous histories reduces the size of the search space of the
mathematical programs. Now, some constraints of the programs depend on the size of the
search space, we must then alter some of these constraints.
Let denote by a  superscript the sets actually used in our program. For example, Ei
will be the actual set of terminal histories of agent i, be it pruned of extraneous histories
or not.
Programs MILP (Table 3) and MILP-n agents (Table 5) rely on the fact that the
number of histories of a given length t in the support of a pure policy of each agent is fixed
and equal to |Oi |t1 . As it may not be the case with pruned sets, the following changes
have to be made:
 The constraint (42) of MILP or (121) MILP-n agents, that is
X
Y
z(j) =
|Oi |T 1
jE

iI

must be replaced by
X

z(j) 

Y
iI

jE

385

|Oi |T 1 .

(164)

fiAras & Dutech

 The set of constraints (41) of MILP or (120) of MILP-n agents, that is
X

z(hh, j  i) =

j  Ei

Y

|Ok |T 1 xi (h),

i  I, h  Ei

Y

|Ok |T 1 xi (h),

i  I, h  Ei .

kI\{i}

must be replaced by
X

z(hh, j  i) 

(165)

kI\{i}

j  Ei

 The set of constraints (119) of MILP-n agents, that is
yi ((h)) 

X
1
R(, hh, ji i)z(j) = wi (h),
|Oi |T 1

h  Ei

jE

must be replaced by
yi ((h)) 

X
1
R(, hh, ji i)z(j) = wi (h),
|Oi |T 1

h  Ei .

(166)

jE

Appendix E. Example using MA-Tiger
All these example are derived using the Decentralized Tiger Problem (MA-Tiger) described
in Section 2.2. We have two agents, with 3 actions (al , ar , ao ) and 2 observations (ol , or ).
We will only consider problem with an horizon of 2.
There are 18 (32  2) terminal histories for an agent: ao .ol .ao , ao .ol .al , ao .ol .ar , ao .or .ao ,
ao .or .al , ao .or .ar , al .ol .ao , al .ol .al , al .ol .ar , al .or .ao , al .or .al , al .or .ar , ar .ol .ao , ar .ol .al ,
ar .ol .ar , ar .or .ao , ar .or .al , ar .or .ar .
And thus 324 (182 = 322 22 ) joint histories for the agents: hao .ol .ao ,ao .ol .ao i,hao .ol .ao ,ao .ol .al i,
hao .ol .ao ,ao .ol .ar i,    , har .or .ar ,ar .or .ar i.
E.1 Policy Constraints
The policy constraints with horizon 2 for one agent in the MA-Tiger problem would be:
Variables: x for every history
x(ao ) + x(al ) + x(ar ) = 0
x(ao ) + x(ao .ol .ao ) + x(ao .ol .al ) + x(ao .ol .ar ) = 0
x(ao ) + x(ao .or .ao ) + x(ao .or .al ) + x(ao .or .ar ) = 0
x(al ) + x(al .ol .ao ) + x(al .ol .al ) + x(al .ol .ar ) = 0
x(al ) + x(al .or .ao ) + x(al .or .al ) + x(al .or .ar ) = 0
x(ar ) + x(ar .ol .ao ) + x(ar .ol .al ) + x(ar .ol .ar ) = 0
x(ar ) + x(ar .or .ao ) + x(ar .or .al ) + x(ar .or .ar ) = 0
386

fiMathematical Programming for DEC-POMDPs

x(ao )  0

x(al )  0

x(ar )  0

x(ao .ol .ao )  0 x(ao .ol .al )  0 x(ao .ol .ar )  0
x(ao .or .ao )  0 x(ao .or .al )  0 x(ao .or .ar )  0
x(al .ol .ao )  0

x(al .ol .al )  0

x(al .ol .ar )  0

x(al .or .ao )  0 x(al .or .al )  0 x(al .or .ar )  0
x(ar .ol .ao )  0 x(ar .ol .al )  0 x(ar .ol .ar )  0
x(ar .or .ao )  0 x(ar .or .al )  0 x(ar .or .ar )  0
E.2 Non-Linear Program for MA-Tiger
The Non-Linear Program for finding an optimal sequence-form policy for the MA-Tiger
with horizon 2 would be:
Variables: xi for every history for each agent

Maximize

R(, hao .ol .ao , ao .ol .ao i)x1 (ao .ol .ao )x2 (ao .ol .ao )
+ R(, hao .ol .ao , ao .ol .al i)x1 (ao .ol .ao )x2 (ao .ol .al )
+ R(, hao .ol .ao , ao .ol .ar i)x1 (ao .ol .ao )x2 (ao .ol .ar )
+ 

subject to:
x1 (ao ) + x1 (al ) + x1 (ar ) = 0
x1 (ao ) + x1 (ao .ol .ao ) + x1 (ao .ol .al ) + x1 (ao .ol .ar ) = 0
x1 (ao ) + x1 (ao .or .ao ) + x1 (ao .or .al ) + x1 (ao .or .ar ) = 0
x1 (al ) + x1 (al .ol .ao ) + x1 (al .ol .al ) + x1 (al .ol .ar ) = 0
x1 (al ) + x1 (al .or .ao ) + x1 (al .or .al ) + x1 (al .or .ar ) = 0
x1 (ar ) + x1 (ar .ol .ao ) + x1 (ar .ol .al ) + x1 (ar .ol .ar ) = 0
x1 (ar ) + x1 (ar .or .ao ) + x1 (ar .or .al ) + x1 (ar .or .ar ) = 0

x2 (ao ) + x2 (al ) + x2 (ar ) = 0
x2 (ao ) + x2 (ao .ol .ao ) + x2 (ao .ol .al ) + x2 (ao .ol .ar ) = 0
x2 (ao ) + x2 (ao .or .ao ) + x2 (ao .or .al ) + x2 (ao .or .ar ) = 0
x2 (al ) + x2 (al .ol .ao ) + x2 (al .ol .al ) + x2 (al .ol .ar ) = 0
x2 (al ) + x2 (al .or .ao ) + x2 (al .or .al ) + x2 (al .or .ar ) = 0
x2 (ar ) + x2 (ar .ol .ao ) + x2 (ar .ol .al ) + x2 (ar .ol .ar ) = 0
x2 (ar ) + x2 (ar .or .ao ) + x2 (ar .or .al ) + x2 (ar .or .ar ) = 0
387

fiAras & Dutech

x1 (ao )  0

x1 (al )  0

x1 (ar )  0

x1 (ao .ol .ao )  0 x1 (ao .ol .al )  0 x1 (ao .ol .ar )  0
x1 (ao .or .ao )  0 x1 (ao .or .al )  0 x1 (ao .or .ar )  0
x1 (al .ol .ao )  0

x1 (al .ol .al )  0

x1 (al .ol .ar )  0

x1 (al .or .ao )  0 x1 (al .or .al )  0 x1 (al .or .ar )  0
x1 (ar .ol .ao )  0 x1 (ar .ol .al )  0 x1 (ar .ol .ar )  0
x1 (ar .or .ao )  0 x1 (ar .or .al )  0 x1 (ar .or .ar )  0

x2 (ao )  0

x2 (al )  0

x2 (ar )  0

x2 (ao .ol .ao )  0 x2 (ao .ol .al )  0 x2 (ao .ol .ar )  0
x2 (ao .or .ao )  0 x2 (ao .or .al )  0 x2 (ao .or .ar )  0
x2 (al .ol .ao )  0

x2 (al .ol .al )  0

x2 (al .ol .ar )  0

x2 (al .or .ao )  0 x2 (al .or .al )  0 x2 (al .or .ar )  0
x2 (ar .ol .ao )  0 x2 (ar .ol .al )  0 x2 (ar .ol .ar )  0
x2 (ar .or .ao )  0 x2 (ar .or .al )  0 x2 (ar .or .ar )  0

E.3 MILP for MA-Tiger
The MILP with horizon 2 for the agents in the MA-Tiger problem would be:
Variables:
xi (h) for every history of agent i
z(j) for every terminal joint history

Maximize

R(, hao .ol .ao , ao .ol .ao i)z(hao .ol .ao , ao .ol .ao i)
+ R(, hao .ol .ao , ao .ol .al i)z(hao .ol .ao , ao .ol .al i)
+ R(, hao .ol .ao , ao .ol .ar i)z(hao .ol .ao , ao .ol .ar i)
+ 
388

fiMathematical Programming for DEC-POMDPs

subject to:
x1 (ao ) + x1 (al ) + x1 (ar ) = 0
x1 (ao ) + x1 (ao .ol .ao ) + x1 (ao .ol .al ) + x1 (ao .ol .ar ) = 0
x1 (ao ) + x1 (ao .or .ao ) + x1 (ao .or .al ) + x1 (ao .or .ar ) = 0

x2 (ao ) + x2 (al ) + x2 (ar ) = 0
x2 (ao ) + x2 (ao .ol .ao ) + x2 (ao .ol .al ) + x2 (ao .ol .ar ) = 0
x2 (ao ) + x2 (ao .or .ao ) + x2 (ao .or .al ) + x2 (ao .or .ar ) = 0

z(hao .ol .ao , ao .ol .ao i) + z(hao .ol .ao , ao .ol .al i) + z(hao .ol .ao , ao .ol .ar i) = 2  x1 (ao .ol .ao )
z(hao .ol .ao , ao .ol .ao i) + z(hao .ol .al , ao .ol .ao i) + z(hao .ol .ar , ao .ol .ao i) = 2  x2 (ao .ol .ao )
z(hao .ol .al , ao .ol .ao i) + z(hao .ol .al , ao .ol .al i) + z(hao .ol .al , ao .ol .ar i) = 2  x1 (ao .ol .al )
z(hao .ol .ao , ao .ol .al i) + z(hao .ol .al , ao .ol .al i) + z(hao .ol .ar , ao .ol .al i) = 2  x2 (ao .ol .al )


x1 (ao )  0

x1 (al )  0

x1 (ar )  0

x1 (ao .ol .ao )  {0, 1}

x1 (ao .ol .al )  {0, 1}

x1 (ao .ol .ar )  {0, 1}

x1 (ao .or .ao )  {0, 1}

x1 (ao .or .al )  {0, 1}

x1 (ao .or .ar )  {0, 1}


x2 (ao )  0

x2 (al )  0

x2 (ar )  0

x2 (ao .ol .ao )  {0, 1}

x2 (ao .ol .al )  {0, 1}

x2 (ao .ol .ar )  {0, 1}

x2 (ao .or .ao )  {0, 1}

x2 (ao .or .al )  {0, 1}

x2 (ao .or .ar )  {0, 1}


z(hao .ol .ao , ao .ol .ao i)  {0, 1} z(hao .ol .ao , ao .ol .al i)  {0, 1} z(hao .ol .ao , ao .ol .ar i)  {0, 1}
z(hao .ol .al , ao .ol .ao i)  {0, 1} z(hao .ol .al , ao .ol .al i)  {0, 1} z(hao .ol .al , ao .ol .ar i)  {0, 1}

E.4 MILP-2 Agents for MA-Tiger
The MILP-2 agents with horizon 2 for the agents in the MA-Tiger problem would be:
Variables:
xi (h), wi (h) and bi (h) for every history of agent i
yi ()) for each agent and for every information set
Maximize

y1 ()

389

fiAras & Dutech

subject to:

x1 (ao ) + x1 (al ) + x1 (ar ) = 0
x1 (ao ) + x1 (ao .ol .ao ) + x1 (ao .ol .al ) + x1 (ao .ol .ar ) = 0
x1 (ao ) + x1 (ao .or .ao ) + x1 (ao .or .al ) + x1 (ao .or .ar ) = 0

x2 (ao ) + x2 (al ) + x2 (ar ) = 0
x2 (ao ) + x2 (ao .ol .ao ) + x2 (ao .ol .al ) + x2 (ao .ol .ar ) = 0
x2 (ao ) + x2 (ao .or .ao ) + x2 (ao .or .al ) + x2 (ao .or .ar ) = 0

y1 ()  y1 (ao .ol )  y1 (ao .or ) = w1 (ao )
y1 ()  y1 (al .ol )  y1 (al .or ) = w1 (al )
y1 ()  y1 (ar .ol )  y1 (ar .or ) = w1 (ar )
y2 ()  y2 (ao .ol )  y2 (ao .or ) = w2 (ao )
y2 ()  y2 (al .ol )  y2 (al .or ) = w2 (al )
y2 ()  y2 (ar .ol )  y2 (ar .or ) = w2 (ar )

y1 (ao .ol )  R(, hao .ol .ao , ao .ol .ao i)x2 (ao .ol .ao )
R(, hao .ol .ao , ao .ol .al i)x2 (ao .ol .al )
R(, hao .ol .ao , ao .ol .ar i)x2 (ao .ol .ar )
R(, hao .ol .ao , al .ol .ao i)x2 (al .ol .ao )
R(, hao .ol .ao , al .ol .al i)x2 (al .ol .al )
R(, hao .ol .ao , al .ol .ar i)x2 (al .ol .ar )
   = w1 (ao .ol .ao )

y1 (ao .ol )  R(, hao .ol .al , ao .ol .ao i)x2 (ao .ol .ao )
R(, hao .ol .al , ao .ol .al i)x2 (ao .ol .al )
R(, hao .ol .al , ao .ol .ar i)x2 (ao .ol .ar )
R(, hao .ol .al , al .ol .ao i)x2 (al .ol .ao )
R(, hao .ol .al , al .ol .al i)x2 (al .ol .al )
R(, hao .ol .al , al .ol .ar i)x2 (al .ol .ar )
   = w1 (ao .ol .al )
390

fiMathematical Programming for DEC-POMDPs


y1 (ar .or )  R(, har .or .ar , ao .ol .ao i)x2 (ao .ol .ao )
R(, har .or .ar , ao .ol .al i)x2 (ao .ol .al )
R(, har .or .ar , ao .ol .ar i)x2 (ao .ol .ar )
R(, har .or .ar , al .ol .ao i)x2 (al .ol .ao )
R(, har .or .ar , al .ol .al i)x2 (al .ol .al )
R(, har .or .ar , al .ol .ar i)x2 (al .ol .ar )
   = w1 (ar .or .ar )
y2 (ao .ol )  R(, hao .ol .ao , ao .ol .ao i)x1 (ao .ol .ao )
R(, hao .ol .al , ao .ol .ao i)x1 (ao .ol .al )
R(, hao .ol .ar , ao .ol .ao i)x1 (ao .ol .ar )
R(, hal .ol .ao , ao .ol .ao i)x1 (al .ol .ao )
R(, hal .ol .al , ao .ol .ao i)x1 (al .ol .al )
R(, hal .ol .ar , ao .ol .ao i)x1 (al .ol .ar )
   = w2 (ao .ol .ao )
y2 (ao .ol )  R(, hao .ol .ao , ao .ol .al i)x1 (ao .ol .ao )
R(, hao .ol .al , ao .ol .al i)x1 (ao .ol .al )
R(, hao .ol .ar , ao .ol .al i)x1 (ao .ol .ar )
R(, hal .ol .ao , ao .ol .al i)x1 (al .ol .ao )
R(, hal .ol .al , ao .ol .al i)x1 (al .ol .al )
R(, hal .ol .ar , ao .ol .al i)x1 (al .ol .ar )
   = w2 (ao .ol .al )


x1 (ao )  1  b1 (ao )

x1 (al )  1  b1 (al )

x1 (ar )  1  b1 (ar )

x1 (ao .ol .ao )  1  b1 (ao .ol .ao )

x1 (ao .ol .al )  1  b1 (ao .ol .al )

x1 (ao .ol .ar )  1  b1 (ao .ol .ar )



w1 (ao )  U1 (ao )b1 (ao )

w1 (al )  U1 (al )b1 (al )

w1 (ar )  U1 (ar )b1 (ar )

w1 (ao .ol .ao )  U1 (ao .ol .ao )b1 (ao .ol .ao )

w1 (ao .ol .al )  U1 (ao .ol .al )b1 (ao .ol .al )

w1 (ao .ol .ar )  U1 (ao .ol .ar )b1 (ao .ol .ar )


391

fiAras & Dutech

x1 (ao )  0
x1 (ao .ol .ao )  0

x1 (al )  0
x1 (ao .ol .al )  0

x1 (ar )  0
x1 (ao .ol .ar )  0


w1 (ao )  0
w1 (ao .ol .ao )  0

w1 (al )  0
w1 (ao .ol .al )  0

w1 (ar )  0
w1 (ao .ol .ar )  0


b1 (ao )  {0, 1}
b1 (ao .ol .ao )  {0, 1}

b1 (al )  {0, 1}
b1 (ao .ol .al )  {0, 1}


y1 ()  (, +)
y1 (ao .ol )  (, +) y1 (ao .or )  (, +)

... and the same for agent 2

392

b1 (ar )  {0, 1}
b1 (ao .ol .ar )  {0, 1}

fiMathematical Programming for DEC-POMDPs

References
Amato, C., Bernstein, D. S., & Zilberstein, S. (2007a). Optimizing memory-bounded controllers for decentralized POMDPs. In Proc. of the Twenty-Third Conf. on Uncertainty
in Artificial Intelligence (UAI-07).
Amato, C., Bernstein, D. S., & Zilberstein, S. (2007b). Solving POMDPs using quadratically
constrained linear programs. In Proc. of the Twentieth Int. Joint Conf. on Artificial
Intelligence (IJCAI07).
Amato, C., Carlin, A., & Zilberstein, S. (2007c). Bounded dynamic programming for
decentralized POMDPs. In Proc. of the Workshop on Multi-Agent Sequential Decision
Making in Uncertain Domains (MSDM) in AAMAS07.
Amato, C., Dibangoye, J., & Zilberstein, S. (2009). Incremental policy generation for finitehorizon DEC-POMDPs. In Proc. of the Nineteenth Int. Conf. on Automated Planning
and Scheduling (ICAPS-09).
Anderson, B., & Moore, J. (1980). Time-varying feedback laws for decentralized control.
Nineteenth IEEE Conference on Decision and Control including the Symposium on
Adaptive Processes, 19(1), 519524.
Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. (2004). Solving transition independent
decentralized Markov decision processes. Journal of Artificial Intelligence Research,
22, 423455.
Bellman, R. (1957). Dynamic programming. Princeton University Press, Princeton, NewJersey.
Bernstein, D., Givan, R., Immerman, N., & Zilberstein, S. (2002). The complexity of decentralized control of Markov decision processes. Mathematics of Operations Research,
27 (4), 819840.
Bernstein, D. S., Hansen, E. A., & Zilberstein, S. (2005). Bounded policy iteration for
decentralized POMDPs. In Proc. of the Nineteenth Int. Joint Conf. on Artificial
Intelligence (IJCAI), pp. 12871292.
Boularias, A., & Chaib-draa, B. (2008). Exact dynamic programming for decentralized
pomdps with lossless policy compression. In Proc. of the Int. Conf. on Automated
Planning and Scheduling (ICAPS08).
Boutilier, C. (1996). Planning, learning and coordination in multiagent decision processes.
In Proceedings of the 6th Conference on Theoretical Aspects of Rationality and Knowledge (TARK 96), De Zeeuwse Stromen, Nederlands.
Buffet, O., Dutech, A., & Charpillet, F. (2007). Shaping multi-agent systems with gradient reinforcement learning. Autonomous Agent and Multi-Agent System Journal
(AAMASJ), 15 (2), 197220.
393

fiAras & Dutech

Cassandra, A., Kaelbling, L., & Littman, M. (1994). Acting optimally in partially observable
stochastic domains. In Proc. of the 12th Nat. Conf. on Artificial Intelligence (AAAI).
Chades, I., Scherrer, B., & Charpillet, F. (2002). A heuristic approach for solving
decentralized-POMDP: assessment on the pursuit problem. In Proc. of the 2002 ACM
Symposium on Applied Computing, pp. 5762.
Cornuejols, G. (2008). Valid inequalities for mixed integer linear programs. Mathematical
Programming B, 112, 344.
Dantzig, G. B. (1960). On the significance of solving linear programming problems with
some integer variables. Econometrica, 28(1), 3044.
dEpenoux, F. (1963). A probabilistic production and inventory problem. Management
Science, 10(1), 98108.
Diwekar, U. (2008). Introduction to Applied Optimization (2 edition). Springer.
Drenick, R. (1992). Multilinear programming: Duality theories. Journal of Optimization
Theory and Applications, 72(3), 459486.
Fletcher, R. (1987). Practical Methods of Optimization. John Wiley & Sons, New York.
Ghavamzadeh, M., & Mahadevan, S. (2004). Learning to communicate and act in cooperative multiagent systems using hierarchical reinforcement learning. In Proc. of the 3rd
Int. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS04).
Govindan, S., & Wilson, R. (2001). A global newton method to compute Nash equilibria.
Journal of Economic Theory, 110, 6586.
Hansen, E., Bernstein, D., & Zilberstein, S. (2004). Dynamic programming for partially
observable stochastic games. In Proc. of the Nineteenth National Conference on Artificial Intelligence (AAAI-04).
Horst, R., & Tuy, H. (2003). Global Optimization: Deterministic Approaches (3rd edition).
Springer.
James, M., & Singh, S. (2004). Learning and discovery of predictive state representations
in dynamical systems with reset. In Proc. of the Twenty-first Int. Conf. of Machine
Learning (ICML04).
Kaelbling, L., Littman, M., & Cassandra, A. (1998). Planning and acting in partially
observable stochastic domains. Artificial Intelligence, 101, 99134.
Koller, D., Megiddo, N., & von Stengel, B. (1994). Fast algorithms for finding randomized
strategies in game trees. In Proceedings of the 26th ACM Symposium on Theory of
Computing (STOC 94), pp. 750759.
Koller, D., & Megiddo, N. (1996). Finding mixed strategies with small supports in extensive
form games. International Journal of Game Theory, 25(1), 7392.
394

fiMathematical Programming for DEC-POMDPs

Lemke, C. (1965). Bimatrix Equilibrium Points and Mathematical Programming. Management Science, 11(7), 681689.
Luenberger, D. (1984). Linear and Nonlinear Programming. Addison-Wesley Publishing
Company, Reading, Massachussetts.
McCracken, P., & Bowling, M. H. (2005). Online discovery and learning of predictive state
representations. In Advances in Neural Information Processing Systems 18 (NIPS05).
Nair, R., Tambe, M., Yokoo, M., Pynadath, D., & Marsella, S. (2003). Taming decentralized
POMDPs: towards efficient policy computation for multiagent setting. In Proc. of
Int. Joint Conference on Artificial Intelligence, IJCAI03.
Oliehoek, F., Spaan, M., & Vlassis, N. (2008). Optimal and approximate Q-value functions
for decentralized POMDPs. Journal of Artificial Intelligence Research (JAIR), 32,
289353.
Oliehoek, F., Whiteson, S., & Spaan, M. (2009). Lossless clustering of histories in decentralized POMDPs. In Proc. of The International Joint Conference on Autonomous
Agents and Multi Agent Systems, pp. 577584.
Osborne, M. J., & Rubinstein, A. (1994). A Course in Game Theory. The MIT Press,
Cambridge, Mass.
Papadimitriou, C. H., & Steiglitz, K. (1982). Combinatorial Optimization: Algorithms and
Complexity. Dover Publications.
Papadimitriou, C. H., & Tsitsiklis, J. (1987). The Complexity Of Markov Decision Processes. Mathematics of Operations Research, 12 (3), 441  450.
Parsons, S., & Wooldridge, M. (2002). Game theory and decision theory in multi-agent
systems. Autonomous Agents and Multi-Agent Systems (JAAMAS), 5(3), 243254.
Petrik, M., & Zilberstein, S. (2007). Average-reward decentralized Markov decision processes. In Proc. of the Twentieth Int. Joint Conf. on Artificial Intelligence (IJCAI
2007).
Petrik, M., & Zilberstein, S. (2009). A bilinear programming approach for multiagent
planning. Journal of Artificial Intelligence Research (JAIR), 35, 235274.
Puterman, M. (1994). Markov Decision Processes: discrete stochastic dynamic programming. John Wiley & Sons, Inc. New York, NY.
Pynadath, D., & Tambe, M. (2002). The Communicative Multiagent Team Decision Problem: Analyzing Teamwork Theories And Models. Journal of Artificial Intelligence
Research, 16, 389423.
Radner, R. (1959). The application of linear programming to team decision problems.
Management Science, 5, 143150.
Russell, S., & Norvig, P. (1995). Artificial Intelligence: A modern approach. Prentice Hall.
395

fiAras & Dutech

Sandholm, T. (1999). Multiagent systems, chap. Distributed rational decision making, pp.
201258. The MIT Press. Ed. by G. Weiss.
Sandholm, T., Gilpin, A., & Conitzer, V. (2005). Mixed-integer programming methods for
finding nash equilibria. In Proc. of the National Conference on Artificial Intelligence
(AAAI).
Scherrer, B., & Charpillet, F. (2002). Cooperative co-learning: A model based approach
for solving multi agent reinforcement problems. In Proc. of the IEEE Int. Conf. on
Tools with Artificial Intelligence (ICTAI02).
Seuken, S., & Zilberstein, S. (2007). Memory-bounded dynamic programming for DECPOMDPs. In Proc. of the Twentieth Int. Joint Conf. on Artificial Intelligence (IJCAI07).
Singh, S., Jaakkola, T., & Jordan, M. (1994). Learning without state estimation in partially
observable markovian decision processes.. In Proceedings of the Eleventh International
Conference on Machine Learning.
Singh, S., Littman, M., Jong, N., Pardoe, D., & Stone, P. (2003). Learning predictive state
representations. In Proc. of the Twentieth Int. Conf. of Machine Learning (ICML03).
Szer, D., & Charpillet, F. (2006). Point-based Dynamic Programming for DEC-POMDPs.
In Proc. of the Twenty-First National Conf. on Artificial Intelligence (AAAI 2006).
Szer, D., Charpillet, F., & Zilberstein, S. (2005). MAA*: A heuristic search algorithm for
solving decentralized POMDPs. In Proc. of the Twenty-First Conf. on Uncertainty
in Artificial Intelligence (UAI05), pp. 576 583.
Thomas, V., Bourjot, C., & Chevrier, V. (2004). Interac-DEC-MDP : Towards the use of
interactions in DEC-MDP. In Proc. of the Third Int. Joint Conf. on Autonomous
Agents and Multi-Agent Systems (AAMAS04), New York, USA, pp. 14501451.
Vanderbei, R. J. (2008). Linear Programming: Foundations and Extensions (3rd edition).
Springer.
von Stengel, B. (2002). Handbook of Game Theory, Vol. 3, chap. 45-Computing equilibria
for two-person games, pp. 17231759. North-Holland, Amsterdam.
Wu, J., & Durfee, E. H. (2006). Mixed-integer linear programming for transitionindependent decentralized MDPs. In Proc. of the fifth Int. Joint Conf. on Autonomous
Agents and Multiagent Systems (AAMAS06), pp. 10581060 New York, NY, USA.
ACM.
Xuan, P., Lesser, V., & Zilberstein, S. (2000). Communication in multi-agent Markov
decision processes. In Proc. of ICMAS Workshop on Game Theoretic and Decision
Theoretics Agents Boston, MA.

396

fiJournal of Artificial Intelligence Research 37 (2010) 85-98

Submitted 10/09; published 2/10

Mechanisms for Multi-Unit Auctions
Shahar Dobzinski

shahar@cs.cornell.edu

Computer Science Department, Cornell University
Ithaca, NY 14853

Noam Nisan

noam@cs.huji.ac.il

The School of Computer Science and Engineering
The Hebrew University of Jerusalem
Jerusalem, Israel

Abstract
We present an incentive-compatible polynomial-time approximation scheme for multiunit auctions with general k-minded player valuations. The mechanism fully optimizes over
an appropriately chosen sub-range of possible allocations and then uses VCG payments
over this sub-range. We show that obtaining a fully polynomial-time incentive-compatible
approximation scheme, at least using VCG payments, is NP-hard. For the case of valuations
given by black boxes, we give a polynomial-time incentive-compatible 2-approximation
mechanism and show that no better is possible, at least using VCG payments.

1. Introduction
In Algorithmic Mechanism Design our goal is to construct efficient mechanisms that will
handle the selfish behavior of the players. In particular, we are interested in designing
truthful mechanisms, that is, mechanisms in which the dominant strategy of each player is
to simply reveal his true valuation.
Typical problems in the field involve allocating goods to players. One key problem is
the problem of multi-unit auctions. Here we are given m identical items and n bidders.
In our setting we view the number of items m as large and desire mechanisms whose
computational complexity is polynomial in the number of bits needed to represent m. Every
bidder i has a valuation function vi : {1, ..., m}  <, where vi (q) denotes his value for
obtaining q items. We assume that vi is weakly monotone increasing (free disposal), and
normalized (vi (0) = 0). Our goal is the usual one of maximizing the social welfare i vi (qi )
where i qi  m.
In the general case, each vi is represented by m real numbers, and in abstract settings
may be accessed as a black box. In a concrete setting, we will assume that vi is represented
as a k-minded bid, i.e. given by a list: (q1 , p1 ), ..., (qk , pk ), where vi (q) = max{j|qj q} pj .
This corresponds to a XOR bidding language (Nisan, 2006). Observe that a k-minded
valuation corresponds to a step function with at most k steps (step i is located at qi and
has height pi ). In general, k can be as large as m, and the case k = 1 is the single-minded
case.
This problem has received much attention, starting from Vickreys seminal paper (1961)
that described a truthful mechanism for the case of downward sloping valuations in which
the items can be optimally allocated greedily. The general case, however, is NP-hard, as the
single-minded case is just a re-formulation of the knapsack problem. Luckily, the knapsack
c
2010
AI Access Foundation. All rights reserved.

fiDobzinski & Nisan

problem has a fully-polynomial time approximation scheme (i.e. can be approximated to
within a factor of 1 +  in time polynomial in n, log m, 1 ), and it is not hard to see that
the algorithm directly extends to the general case of multi-unit auctions.
1.1 VCG-Based Mechanisms
The key positive technique of mechanism design is the VCG payment scheme. In this
payment scheme each bidder i pays hi (vi )  j6=i vj (a), where a is the algorithmic output,
and hi is some arbitrary function that does not depend on vi . Unfortunately, while VCG
works perfectly well from a game-theoretic perspective, it is not so useful in computational
settings, since in multi-unit auctions, and in most interesting combinatorial optimization
problems, calculating the exact optimum is intractable.
One naive idea is to use an approximation algorithm to find an approximate solution
a, and then let each bidder i pay hi (vi )  j6=i vj (a). Applying this idea to the case of
multi-unit auction is tempting in particular in light of the known (1 + )-approximation
algorithm for this problem. Unfortunately, it turns out that in general using an approximation algorithm together with VCG payments does not result in a truthful mechanism.
This phenomenon was studied by Nisan and Ronen (2007). It was observed there that the
following family of allocation algorithms do yield truthful VCG-based mechanisms:
Definition: An allocation algorithm (that produces an output a  A for each input
v1 , ..., vn , where A is the set of possible alternatives) is called maximal-in-range (henceforth MIR) if it completely optimizes the social welfare over some subrange R  A. I.e.,
for some R  A, we have that for all v1 , ..., vn , a  arg maxaR i vi (a).
I.e., MIR algorithms use the following natural and simple strategy to find an approximately
optimal solution: they just optimally search within a pre-specified sub-range of feasible
solutions  a subrange over which optimal search is algorithmically feasible.
The main result of Nisan and Ronen (2007) states that this is essentially it  no other
VCG-based mechanisms are incentive compatible.
Theorem (Nisan & Ronen, 2007): The allocation algorithm of any incentive-compatible
VCG-based mechanism for combinatorial auctions is equivalent to a maximal-in-range algorithm.
Equivalent here means that the social utilities are identical for all inputs, i.e. if a and b
are the outputs of the two allocation algorithms for input v1 , . . . , vn then i vi (a) = i vi (b).
In particular, the outputs must coincide generically  except perhaps in case of ties. Nisan
and Ronen (2007) prove this for two specific types of mechanism design problems, but the
result is more general (Dobzinski & Nisan, 2007).
Following Nisan and Ronen (2007), Dobzinski and Nisan (2007) view this result as
a negative one. Namely, they show that in the setting of combinatorial auctions with
submodular bidders, MIR algorithms do not have much power. This might imply that in
the above setting truthful deterministic mechanisms do not have much power, since Lavi
et al. (2003) give a partial evidence that all truthful mechanisms that provide a good
approximation ratio must be MIR.
86

fiMechanisms for Multi-Unit Auctions

In contrast, this paper views this result as a positive result. We observe that MIR
algorithms provide us with a constructive way of obtaining truthful mechanisms1 . Indeed,
this paper suggests that there are some settings where the power of MIR algorithms is not
trivial at all. We note that several previous papers already obtained upper bounds using
MIR techniques (Holzman, Kfir-Dahav, Monderer, & Tennenholtz, 2004; Dobzinski, Nisan,
& Schapira, 2005; Blumrosen & Dobzinski, 2007). Yet, this paper initiates the systematic
study of MIR algorithms. In particular, the techniques used are more sophisticated then
those obtained in previous work.
1.2 Previous Work and Our Results
For the case of multi-unit auctions with single-minded bidders, the paper by Briest, Krysta,
and Vocking (2005) presents a truthful fully polynomial time approximation scheme (FPTAS), improving upon a previous result of Mualem and Nisan (2002). The only result
known for k-minded bidders is a randomized 12 -approximation mechanism that is truthful in expectation (Lavi & Swamy, 2005). The current paper presents a polynomial time
approximation scheme (PTAS) for the general case.
Theorem: For every fixed  > 0, there exists a truthful (1  )-approximation mechanism
for multi-unit auctions with k-minded bidders whose running time is polynomial in n, log m,
and k. The dependence of the running time on  is exponential.
However, we prove two ways in which the mechanism can not be improved upon. First,
we show that the dependence on  cannot be made polynomial without destroying the
truthfulness, as long as MIR techniques are used. In contrast, from a pure algorithmic point
of view it is possible to obtain a fully polynomial time approximation scheme2 . Furthermore,
there exists a truthful FPTAS if all bidders are known to be single minded (Briest et al.,
2005).
Theorem: No fully polynomial time truthful approximation mechanism that uses VCG
payments exists, unless P=NP.
Then we show that the dependence on k is necessary, and that no approximation scheme
is possible in a general black-box model. This is shown in a general communication model,
and even for two bidders.
Theorem: Every approximation mechanism among two bidders with general valuations
that uses VCG payments requires exponentially many queries to obtain an approximation
factor that is better than 12 .
We do present a truthful approximation mechanism in the general black box model that
does obtain a factor of 12 . This improves upon the randomized one of Lavi and Swamy
(2005) that is only truthful in expectation.
1. Note that the payments of an efficient MIR mechanism can be computed efficiently: take the output
allocation and pay each bidder the sum of the values of the other bidders in the output allocation.
2. Of course, ignoring computational issues, the standard VCG mechanism is MIR and provides an approximation ratio of 1.

87

fiDobzinski & Nisan

Theorem: There exists a truthful 12 -approximation mechanism for multi-unit auctions
among general valuations whose running time is polynomial in n and log m. The access to
bidders valuations is through value queries: what is vi (q)?3 . The mechanism uses VCG
payments.
In Section 5 we present a fairly general construction that takes any -approximation MIR
1
algorithm A for t bidders, for some   1, and converts it into an (  t+1
)-approximation
algorithm for n bidders, in time polynomial in n, log m, and the running time of A. The
construction works as long as the model allows us to answer value queries. We present
four applications of the construction: the first two applications provide another proof of the
upper bounds discussed above (the PTAS for k-minded bidders, and the 12 -approximation for
the black-box model). Then we present two new applications: a PTAS for stronger bidding
languages, such as the one used by Kothari et al. (2005), and a ( 43 + )-approximation
mechanism for multi-unit auctions with subadditive valuations. Prior to our paper, nothing
was known about the latter setting.
The construction provides us with an interesting example of a truthful reduction among
problems: any MIR approximation algorithm for a fixed number of bidders can be automatically translated into a truthful approximation algorithm for any number of bidders, while
losing only a small factor in the approximation ratio.
1.3 Discussion and Open Questions
The main open problem remains to determine the best approximation ratio that can be obtained in a truthful way. The only general method known for constructing such mechanisms
is VCG, and our lower bounds state that VCG cannot take us any further. Furthermore,
Lavi et al. (2003) show that for 2 bidders in the black-box model, if all items are allocated,
then MIR algorithms are the only truthful way to obtain a reasonable approximation ratio.
Combined with our lower bounds, we get that no better-than- 12 truthful approximation is
possible in polynomial time, for 2 bidders and if all items are allocated. An intriguing
open question is to understand whether the condition that all items are allocated is indeed
necessary.
Another issue that the paper highlights is the inherent difference between obtaining
approximation algorithms in single-parameter environments and multi-parameter environments. In a single parameter environment the private information of each bidder consists of
only one number, while in multi-parameter environments the private information consists
of more than one number. Recall that in the single parameter variant of multi-unit auctions, where we assume that all bidders are single-minded, there exists a truthful FPTAS
(Briest et al., 2005). However, the variants we discuss in this paper are multi-parameter,
and indeed our lower bounds suggest that obtaining FPTAS is impossible (if one can prove
that all mechanisms that give a good approximation ratio must be MIR).
Paper Organization
In Section 3 we present the PTAS for k-minded bidders, and the 21 -approximation in the
black-box model. Section 4 considers lower bounds for MIR algorithms in both models.
3. This is yet another improvement upon the mechanism of Lavi and swamy (2005) which requires the
stronger demand queries.

88

fiMechanisms for Multi-Unit Auctions

In Section 5 we describe a general construction, and its algorithmic applications: truthful
mechanisms for other models and more powerful bidding languages.

2. Preliminaries
In this section we provide the basic definitions and notations used in this paper.
2.1 The Setting
In a multi-unit auction there is a set of m identical items, and a set N = {1, 2, . . . , n}
of bidders. Each bidder i has a valuation function vi : [m]  R+ , which is normalized
(vi (0) = 0) and non-decreasing. Denote by V the set of possible valuations. An allocation
of the items ~s = (s1 , . . . , sn ) to N is a vector of non-negative integers with i si  m.
Denote the set of allocations by S. The goal is to find an allocation that maximizes the
welfare: i vi (si ).
We consider two settings that differ in how the valuations are given to us. In the concrete
setting (a bidding language model) we will assume that vi is represented as a k-minded
bid, i.e. given by a list: (q1 , p1 ), ..., (qk , pk ), where vi (q) = max{j|qj q} pj .
Otherwise, the valuations are given to us as black boxes. For algorithms, the black
box v will only answer the weak value queries: given s, what is the value of v(s). For the
impossibility result, we assume that the black box v can answer any query that is based
on v (the communication model). Our algorithms run in time poly(n, log m), while our
impossibility result gives a lower bound on the number of bits transferred, and holds even
if the mechanism is computationally unbounded.
2.2 Truthfulness
A deterministic n-bidder mechanism for multi-unit auctions is a pair (f, p) where f : V n  S
and p = (p1 ,    , pn ), where pi : V n  R.
Definition 2.1 Let (f, p) be a deterministic mechanism. (f, p) is truthful if for all i, all
vi , vi0 and all vi we have that vi (f (vi , vi )i )  pi (vi , vi )  vi (f (vi0 , vi )i )  p(vi0 , vi ).
Definition 2.2 f is an affine maximizer if there exist a set of allocations R, a constant
i  0 for i  N , and a constant ~s  < for each ~s  S, such that f (v1 , ..., vn ) 
arg max~s=(s1 ,...,sn )R (i (i vi (si )) + s ). f is called maximal-in-range (MIR) if i = 1
for i  N , and s = 0 for each ~s  R.
The following proposition is standard:
Proposition 2.3 Let f be an affine maximizer (in particular, maximal in range). There
are payments p such that (f, p) is a truthful mechanism.

3. The Basic Mechanisms
This section provides the basic mechanisms for multi-unit auctions: a PTAS for k-minded
bidders, and a 12 approximation for the black-box model.
89

fiDobzinski & Nisan

3.1 A Truthful PTAS for k-Minded Bidders
We design an MIR algorithm for this problem, which directly yields an incentive-compatible
VCG-based mechanism. We define a range R of allocations, and prove that if all bidders
are k-minded then the algorithm outputs in polynomial time the best allocation in R. We
start with defining the subrange R.
Definition 3.1 We say that an allocation (s1 , ..., sn ) is t-round if there exists a set T of
bidders, |T |  t, such that the following two conditions hold:
 Let l = jT sj .
j
k
ml
 For each bidder i 
/ T , si is a multiple of max( (nt)
, 1).
2
k
j
ml
, 1)  (n  t)2 items are assigned to bidders outside T : iT
 At most max( (nt)
2
/ si 
j
k
ml
max( (nt)
, 1)  (n  t)2
2
We let R be the set of all t-round allocations for some fixed t (that will depend only on
the approximation guarantee). Next we prove that the value of the best allocation in R is
close to the optimum:
Lemma 3.2 Let (a1 , ..., an ) be an optimal t-round allocation, and (o1 , ..., on ) an optimal
1
unrestricted allocation, then i vi (ai )  (1  t+1
)i vi (oi ).
Proof:
Let us start with an optimal unrestricted allocation (o1 , ..., on ), and use it to
construct a t-round allocation with high value. Assume that all items are allocated in the
optimal allocation (without loss of generality due to the monotonicity of the valuations), and
that v1 (o1 )  ...  vn (on ). Let T = {1, ..., t} be the set of t bidders required in Definition
3.1, and assign each bidder i  T a bundle of size oi . As in Definition 3.1, let l = jT oj .
Let j 
/ T be the bidder who got the largest number of items oj  ml
. For each i 
/ T,
j
knt
ml
i 6= j, round up each oi to the nearest multiple of b = max( (nt)2 , 1) and assign this
many items to bidder i. Assign bidder
jkno items. This is a valid t-round allocation since if
j
ml
b 6= 1 we added at most (n  t)  (nt)2  ml
nt items by rounding up, but deleted at least
ml
nt

items by removing oj . Notice that the second condition also holds. If b = 1, observe
that even the optimal allocation is t-round. As for the value of the solution, observe that
vi (oi )
each bidder i 6= j gets a bundle no smaller than oi . In addition, vj (oj )  it+1
, which
gives the required approximation.
Our MIR approximation algorithm will try each subset of at most t bidders to be the
set T of bidders. For each possible selection of T , the algorithm will consider all possible
allocations to bidders in T according to the k bids each bidder submitted. That is, we
will consider the allocation that assigns each bidder i  T exactly si items, if and only if
iT si  m, and for each si there is a bid (si , pi ) in the k bids of bidder i (for some pi > 0).
For each selection of T and allocation to the bidders in T according to their bids, the
algorithm splits the remaining m  l items into at most (n  t)2 equi-sized bundles of size
90

fiMechanisms for Multi-Unit Auctions

k
j
ml
, 1), where l is the total number of items that bidders in T get. The maximalmax( (nt)
2
in-range algorithm will optimally allocate these equi-size bundles among the bidders that
are not in T . Finally, the algorithm outputs the best allocation among all allocations
considered. All that is left is to show the following two lemmas:
Lemma 3.3 For every fixed t the above algorithm runs in time polynomial in n and log m.
Proof:
There are at most nt possible selections of sets T . For each selection of T
there are at most k t allocations to bidders in T that are considered. Finding the optimal
allocation to bidders not in T is by dynamic programming. Let b be the size of the equi-size
bundles. Without loss of generality, we assume that T = {n  t + 1, ..., n}. We calculate
the following information for every 1  i  n  t and 1  q  (n  t)2 : M (i, q) is the
maximum value that can be obtained by allocating at most q equi-size bundles among
bidders 1...i. Each entry can be filled in polynomial time using the relations: M (i, q) =
maxq0 q (vi (q 0 b) + M (i  1, q  q 0 )). In particular notice that if b = 1 then the number of
equi-size bundles is polynomial in the number of bidders, thus the number of entries in
the table is polynomial also in this case. Overall we get that the algorithm runs in time
polynomial in n and log m, for every fixed t.
Lemma 3.4 The above algorithm finds an optimal t-round allocation.
Proof: First, notice that the algorithm outputs a t-round allocation. Let us prove that
it outputs an optimal one. Let O = (o1 , ..., on ) be an optimal t-round allocation, let T be
the set of bidders from Definition 3.1, and let l = iT oi . For each bidder i  T remove
the maximal number qi (possibly zero) of items from oi such that vi (oi ) = vi (oi  qi ).
Observe that there exists a pair (qj0 , p0j ) in is XOR bids such that qj0 = oi  qi . We now
handle the bidders
that
/ T holds a bundle that is a multiple
k are not in T . Each bidder i 
j
ml
,
1)
in
O,
while
in
order
for
the
allocation that we construct to be
of b = max( (nt)
2
j
k
ml0
t-round we need the bidders not in T to receive multiples of b0 = max( (nt)
, 1), for
2
l0 = iT (oi  qi ). However, notice that b0  b, and that the number of equi-size bundles
is at least the same. Hence, by assigning each bidder i 
/ T the same number of equi-size
bundles as in O, bidder i holds at least the same value as in O. The lemma follows since
the algorithm considers the newly constructed allocation.
We therefore have the following theorem:
1
Theorem 3.5 There exists a truthful VCG-based mechanism that provides a (1  t+1
)approximation for multi-unit auctions with k-minded bidders in time polynomial in n, log m,
k, for every constant t.

3.2 A 21 -Approximation for Multi-Unit Auctions with Black-Box Access
Let us consider the multi-unit auction problem with general valuations given by black boxes.
We will assume in our algorithm an oracle access to it that may be queried for vi (q), where
q is the given bundle size4 .
4. This is analogous to the weakest value query in a combinatorial auction setting. Our lower bounds
presented later will apply to all other query types as well.

91

fiDobzinski & Nisan

We will design a 12 -approximation MIR algorithm for this problem, which again yields
an incentive-compatible VCG-based mechanism. Our MIR approximation
algorithm will

m
2
first split the items into n equi-sized bundles of size b = n2 as well as a single extra
bundle of size r that holds the remaining elements (thus n2 b + r = m). The maximum in
range algorithm will optimally allocate these whole bundles among the n bidders. What we
need to show are the following two simple facts:
Lemma 3.6 An optimal allocation of the bundles can be found in time polynomial in n and
log m.
Lemma 3.7 Let (a1 , ..., an ) be an optimal allocation of the bundles that was found by the
algorithm, and (o1 , ..., on ) an optimal unrestricted allocation, then i vi (oi )  2i vi (ai ).
The proofs are simple:
Proof: (of Lemma 3.6): The algorithm is by dynamic programming. We calculate the
following information for every 1  i  n and 1  q  n2 : M (i, q) is the maximum
value that can be obtained by allocating at most q regular bundles among bidders 1...i, and
M + (i, q) is the maximum value that can be obtained by allocating at most q regular bundles
and the remainder bundle among bidders 1...i. Each entry can be filled in polynomial
time using the relations: M (i, q) = maxq0 q vi (q 0 b) + M (i  1, q  q 0 ) and M + (i, q) =
max(maxq0 q (vi (q 0 b) + M + (i  1, q  q 0 )), maxq0 q (vi (q 0 b + r) + M (i  1, q  q 0 ))).
Proof: (of Lemma 3.7): Let us start with an optimal unrestricted allocation o1 ...on where
all items are allocated (without loss of generality since the valuations are monotone), and
look at the bidder j that got the largest number of items oj  m/n. There are now two
possibilities: if vj (oj )  i6=j vi (oi ) then by allocating all items to j (i.e. all regular-sized
bundles as well as the remainder bundle) we get the required 2-approximation. Otherwise,
round up each oi to the nearest multiple of b (i.e. to full bundles), except for bidder j
that gets nothing. This is a valid allocation since we added at most nb  m/n items by
rounding up, but deleted at least m/n items by removing oj , and its value is certainly at
least i6=j vi (oi ) which gives the required approximation.
We have thus proved:
Theorem 3.8 There exists a truthful polynomial time VCG-based mechanism that gives a
1
2 -approximation for multi-unit auctions with general valuations.

4. Lower Bounds for VCG-Based Mechanisms
We now move on to show that both mechanisms essentially achieve the best approximation
ratios possible. We say that an allocation (s1 , ..., sn ) is complete if all items are allocated:
i si = m. Consider an MIR algorithm for n bidders that does not have full range of
complete allocations. I.e., for some 0  s1 , ..., sn1  m, i<n si  m it never outputs the
allocation (s1 , ..., sn1 , m  i<n si ). Now consider the set of valuations where for every
bidder i vi (q) = 1 if and only if q  si (and 0 otherwise). The only allocation with value
n is (s1 , ..., sn1 , m  i<n si ) which is not in the range, while all other allocations have a
value of at most n  1.
92

fiMechanisms for Multi-Unit Auctions

From this we can easily get a lower bound for any computationally efficient MIR algorithm in the models considered in this paper. We start with a lower bound in the black-box
model. The lower bound is on the number of queries that the bidders must be queried, and
holds for any type of query  i.e., in a general communication setting.
Proposition 4.1 Let A be a MIR algorithm for multi-unit auctions that achieves an approximation ratio better than 12 . Then, the communication complexity of A is (m).
Proof: In the case of two bidders, an optimal algorithm is known to have a communication
complexity of (m):
Lemma 4.2 (Nisan & Segal, 2006) Finding the optimal allocation in multi-unit auctions requires (m) bits of communication, even if there are only two bidders and even for
just finding the value of the allocation. This lower bound also applies to nondeterministic
settings.
Thus, any MIR algorithm for 2 bidders that uses o(m) bits of communication will be
non-optimal and thus, as argued above, gives no better than a 12 -approximation. The case
of more than 2 bidders follows by setting all valuations but the first two to 0, and then
considering all allocations in which all items are allocated to the first two bidders.
The second result rules out the existence of FPTAS for k-minded bidders. In other
words, the dependence of the running time in 1 cannot be made polynomial. The result
essentially applies to all models that allow single-minded bidders, e.g., XOR bids, and the
bidding language used by Kothari et al. (2005) (see the next section for a description).
Proposition 4.3 Let A be a MIR algorithm that achieves an approximation ratio better
than (1  n1 ). Then, A does not run in polynomial time, unless P = N P .
Proof: Similarly to the previous proposition, and by the standard reduction from knapsack
to multi-unit auctions, it follows that for every polynomial-time MIR algorithm there exist
large enough n and m for which the range of complete allocations is not full, unless P = N P .
The lemma follows by the discussion above.
This concludes the proof of the lower bounds for MIR mechanisms. To draw the same
conclusion for VCG-based mechanisms, one technical detail that should be explicitly mentioned. Our lower bounds were for MIR algorithms, while VCG-based mechanisms are only
proved to give algorithms that are equivalent to MIR algorithms. See Dobzinski and Nisan
(2007). However, both proofs hold even for finding the value of the optimal allocation and
thus directly apply also to algorithms that are equivalent to MIR algorithms.

5. A General Construction and Applications
We present a general construction that takes a maximal-in-range algorithm for a constant
number of bidders in some bidding language or model5 , and extends it to a truthful mechanism for an unbounded number of bidders. Yet, the extension loses only an arbitrarily
small constant in the approximation ratio.
5. By a model we mean, e.g., some restriction on the valuations or on how they can be accessed.

93

fiDobzinski & Nisan

We describe four applications of the construction. First, we reprove the PTAS for kminded bidders and the 12 -approximation algorithm for general bidders of Section 3. Then,
we study the bidding language considered by Kothari et al. (2005). Kothari et al. describe
an approximately truthful FPTAS for this bidding language, while we present a truthful
VCG-based PTAS (this is the best possible since Section 4 essentially rules out the possibility of a VCG-based FPTAS). Finally, we present a truthful ( 34 +)-approximation mechanism
for the case the valuations are sub-additive (a.k.a. complement free) and are accessed via
a black box.
5.1 The Setting
Fix some bidding language or a model for multi-unit auction in which the bidders can
answer value queries. Let A be a maximal-in-range algorithm for t bidders and at most m
items in this model. Denote the complexity of A by A(t, m), its range by RA,t,m , and its
approximation guarantee by   1.
The Construction
1. Build the set Q of allocations as follows:
(a) Let u = (1 +

1
2n ).

Let L = {0, 1, buc, bu2 c, . . . , ublogu mc , m}.
(b) For every set T of bidders, |T |  t, and l  L:
i. Run A with m  l items and the set T of bidders. Denote by si the number
of items A allocates to each bidder i  T .
ii. Split the remaining l items into at most 2n2 bundles, each consisting of
max( 2nl 2 , 1) items.
iii. Find the optimal allocation of the equal-size bundles among the bidders that
are not in T . Denote by si the allocation to each bidder i 
/ T.
iv. Add (s1 , . . . , sn ) to Q.
2. Output the allocation with the highest welfare in Q.
Theorem 5.1 There exists some range of allocations R such that the construction is maximal in R. The construction runs in time poly(log m, n, A(t, m)) for every constant t. It
1
outputs an allocation with value of (  t+1
) of the optimal allocation.
Proof:

We will make use of the following definition:

Definition 5.2 An allocation is (R, t, l)-round if:
 R is a set of allocations, and in each R  R at most t bidders are allocated non-empty
bundles. The bidders are allocated together up to m  l items.
 There exists a set T of bidders, |T |  t, such that the bidders in T are allocated
according to some allocation in R.
94

fiMechanisms for Multi-Unit Auctions

 
 Each bidder i 
/ T receives an exact multiple of max( 2nl 2 , 1) units, and iT
/ si 
 l 
2
max( 2n2 , 1)  n .
It is not hard to verify that the construction always outputs allocations that are in the
following range:

R = {S|S is a (RA,k,ml , k, l)-round allocation where l  L and k  t}
Lemma 5.3 Let (o1 , . . . , on ) be an optimal unrestricted allocation. There exists an alloca1
tion (s1 , . . . , sn )  R for which i vi (si )  (  t+1
)  i vi (oi ).
Proof:
Denote the value of the optimal solution by OPT. Without loss of generality,
assume that all items are allocated and that v1 (o1 )  . . .  vn (on ). Let l  L be the largest
such that m  l  ti=1 oi . Let (s1 , . . . , st ) be the allocation that A outputs when run on
bidders 1, . . . , t and m  l items, and assign each bidder 1  i  t si items. Observe that
ti=1 vi (si )    ti=1 vi (oi ). To finish the proof we show that there is an allocation in the
range that recovers the value of all bidders t + 1, . . . , n but at most one. The lemma will
T
then follow since for each such bidder i, vi (oi )  OP
t+1 .
Claim 5.4 Step 1(b)iii returns an allocation (st+1 , . . . , st ) of the l items to bidders t +
1, . . . , n such that for each bidder i in this set, but at most one, we have that vi (si )  vi (oi ).
Proof: Let r = ni=t+1 oi . Let j  t + 1 be some bidder with oj  nr (observe that the
existence of this bidder is guaranteed). Let l  L be the largest such that m  l  ti=1 oi
r
(observe that l  r). We also have that l  r  2n
: we chose the largest possible value for
r
r
l, and therefore l  (1+ 1 )  r  2n .
2n
 
Now, for each i 6= j round up each oi to the nearest multiple of max( 2nl 2 , 1), and
allocate no items to bidder j. Observe that for all bidders but bidder j the bundle size they
get increases. Also observe that the number of additional items we allocate to bidders in
l
r
l
l
{t + 1, . . . n} is at most 2nl 2  n = 2n
. Thus, we have that l  r  2n
+ 2n
 r  oj + 2n
.
All that is left is to show that the construction runs in polynomial time:
Lemma 5.5 The optimal allocation in R can be found in time poly(log m, n, A(t, m)), for
every constant t.
Proof: Step 1(b)iii of the construction can be implemented using a dynamic programming
similarly to Lemma 3.3; optimality of the allocation in R is clear. The running time is
poly(log1+ 1 m  nk  A(t, m)), which is polynomial in the relevant parameters for every
2n+1

constant t.

5.2 Applications of the Construction
We now provide several applications of our construction.
95

fiDobzinski & Nisan

5.2.1 A PTAS for k-Minded Valuations
We reprove the PTAS for k-minded bidders of Section 3: a multi-unit auction problem
with m items and t k-minded bidders can be optimally solved by exhaustive search in
time poly((tk)t , log m), which is polynomial in log m and k for every constant t. By the
construction (and since an optimal algorithm is in particular maximal in range), we get a
1
PTAS for k-minded bidders: for every constant t, we get a (1  t+1
)-approximation in time
polynomial in n and log m.
5.2.2 A 12 -Approximation for General Valuations
Here we observe that a multi-unit auction with one bidder can be optimally solved by
allocating all items to the single bidder. We let t = 1 in the statement of Theorem 5.1, and
get a 12 -approximation algorithm for general valuations.
5.2.3 A PTAS for the Marginal Piecewise Bidding Language
The following marginal piecewise bidding language was used by Kothari et al. (2005): a
valuation v is determined by a list of at most k tuples denoted by (u1 , m1 ),. . . ,(uk , mk ). We
assume that the mi s are non-negative and that uk > . . . > u1 = 1. The tuples determine
the marginal utility of the jth item. In other words, to determine the value of a set of s
items, we sum over all the marginal utilities. I.e., for each item j, ul  j < ul+1 , let his
marginal utility be rj = ml , and for every s  m, let v(s) = sj=1 rj . (In fact, the above
bidding language is more powerful than the one described by Kothari et al. (2005), which
allows only marginal-decreasing piecewise valuations.)
We now show how to optimally solve a multi-unit auction problem in this setting with
a constant number of bidders. A PTAS follows, just as in the k-minded case.
We say that bidder i is precisely assigned if he is allocated si items, and for some ui > 0
there exists a tuple (si , ui ) in his k bids. The main observation here is that there is an
optimal solution (o1 , . . . , on ) in which at most one bidder is not precisely assigned: suppose
there are two bidders i and i0 that are not precisely assigned. Then, move items to the bidder
with the higher (or equal) marginal utility. The value of the allocation cannot decrease.
Continue this process until all bidders but at most one are precisely assigned.
Now optimally solving a multi-unit auction problem with a constant number of bidders
is obvious: select each of the n bidders in his turn to be the bidder that is not precisely
assigned. In each iteration, let i be the bidder that is not precisely assigned, and go over
all allocations in which all other bidders are precisely assigned. Then, assign bidder i the
remaining items. Since there are at most k possible sets that make a bidder precisely
assigned, the algorithm runs in time poly(n  (n  k)n1 , log m), which is polynomial in log m
and k for every constant n.
5.2.4 A ( 43 + )-Approximation for Subadditive Valuations
In this model we assume that the valuations are given as black boxes (as in Section 3.2),
and that for each valuation v and bundles 0  s, t  m we have that v(s) + v(t)  v(s + t).
Such valuations are called subadditive valuations.
96

fiMechanisms for Multi-Unit Auctions

Let us now describe an algorithm that provides an approximation ratio of 34 in this
setting when the number of bidders is constant. By the construction, we get a ( 34 + )approximation VCG-based mechanism for unbounded number of bidders, for every constant
. The algorithm is quite simple: Fix a small enough constant  > 1 ( = 43 suffices). All
bidders, but at most one, can only receive a bundle s that is a power of  (including the
empty bundle). The bidder that does not get a bundle of size that is a power of  receives
the remaining items. We use exhaustive search to find the optimal allocation in this range.
To see that the algorithm indeed provides an approximation ratio of 34 , consider an
optimal solution (o1 , . . . , on ). Without loss of generality, assume that o1  . . .  on (notice
that unlike before now the bidders are ordered by their bundle size). Let O be the set of
bidders with odd indices, and E be the set of bidders with even indices.
The analysis is divided into two cases. First suppose that iO vi (oi )  iE vi (oi ).
Consider the following allocation: the bundles of bidders in E are rounded up to the power
of  that is near o2i , the bundles of bidders in O \ {1} are rounded up to the nearest power
of , and bidder 1 gets the remaining items. Notice that the above allocation is valid since
for a small enough choice of  we assign bidders in O no more items than what we removed
from bidders in E. Also notice that this allocation is in the range. As for the approximation
ratio, observe that bidders in O hold at least the same value as in the optimal solution,
since each bidder in O is allocated at least the same number of items as in the optimal
solution. In addition, each bidder in E holds at least half of the items allocated to him in
the optimal solution. Thus, by subadditivity, bidders in E hold together at least half of the
value they hold in the optimal solution. In total, the value of the allocation obtained by
the algorithm is at least 34 of the value of the optimal solution.
Let us now handle the case where iO vi (oi ) < iE vi (oi ). Consider the allocation
where all bidders in O are rounded up to the power of  that is near o2i , and all bidders
in E are rounded up to the nearest power of  (except for one arbitrary bidder in E who
gets the remaining items). This allocation is in the range, and the analysis is similar to the
previous case, leaving us with an approximation ratio of 34 also in the current case.
The running time of the algorithm is poly(n  (log m)n1 ), which is polynomial in log m
for constant n and .
Notice that the approximation ratio achieved is almost the best possible, as every MIR
approximation algorithm that guarantees a factor better than 43 requires (m) communication: by Lemma 4.2 finding the optimum solution of a multi-unit auction with two bidders
requires (m) bits of communication. We make the valuations sub-additive by defining
for each v a new valuation: v 0 (s) = v(s) + v(m), for all s 6= 0. Thus, as in Section 4, the
range of every polynomial-time MIR mechanism for two bidders with subadditive valuations
cannot contain all complete allocations. Fix some MIR algorithm, and let (s1 , m  s1 ) be
a complete allocation that is not in the range. Consider the following instance: bidder 1
values at least s1 items with a value of 2, and smaller bundles with a value of 1, and bidder
2 values at least m  s2 items with a value of 2, and smaller bundles with a value of 1 (and
0 is the value of the empty bundle). Notice that the valuations of the bidders are indeed
subadditive. Also observe that the optimal welfare is 4, but the mechanism can achieve
welfare of at most 3.
97

fiDobzinski & Nisan

Acknowledgments
A preliminary version of this paper appeared in EC07. We are grateful to Liad Blumrosen
and Ahuva Mualem for helpful discussions. The second author is supported by a grant
from the Israeli Science Foundation.

References
Blumrosen, L., & Dobzinski, S. (2007). Welfare maximization in congestion games. IEEE
Journal on Selected Areas in Communications, 25 (6), 12241236.
Briest, P., Krysta, P., & Vocking, B. (2005). Approximation techniques for utilitarian
mechanism design. In STOC, pp. 3948.
Dobzinski, S., & Nisan, N. (2007). Limitations of vcg-based mechanisms. In STOC, pp.
338344.
Dobzinski, S., Nisan, N., & Schapira, M. (2005). Approximation algorithms for combinatorial auctions with complement-free bidders. In STOC, pp. 610618.
Holzman, R., Kfir-Dahav, N., Monderer, D., & Tennenholtz, M. (2004). Bundling equilibrium in combinatrial auctions. Games and Economic Behavior, 47, 104123.
Kothari, A., Parkes, D. C., & Suri, S. (2005). Approximately-strategyproof and tractable
multi-unit auctions. Decision Support Systems, 39, 105121. Special issue dedicated
to the Fourth ACMConference on Electronic Commerce.
Lavi, R., Mualem, A., & Nisan, N. (2003). Towards a characterization of truthful combinatorial auctions. In The 44th Annual IEEE Symposium on Foundations of Computer
Science (FOCS).
Lavi, R., & Swamy, C. (2005). Truthful and near-optimal mechanism design via linear
programming. In FOCS, pp. 595604.
Mualem, A., & Nisan, N. (2002). Truthful approximation mechanisms for restricted combinatorial auctions. In AAAI-02.
Nisan, N. (2006). In P. Cramton and Y. Shoham and R. Steinberg (Editors), Combinatorial
Auctions. Chapter 1. Bidding Languages. MIT Press.
Nisan, N., & Ronen, A. (2007). Computationally feasible vcg mechanisms. J. Artif. Intell.
Res. (JAIR), 29, 1947.
Nisan, N., & Segal, I. (2006). The communication requirements of efficient allocations and
supporting prices.. Journal of Economic Theory.
Vickrey, W. (1961). Counterspeculation, auctions and competitive sealed tenders. Journal
of Finance, 837.

98

fiJournal of Artificial Intelligence Research 37 (2010) 189-246

Submitted 10/2009; published 02/2010

On Action Theory Change
Ivan Jose Varzinczak

ivan.varzinczak@meraka.org.za

Meraka Institute, CSIR
Pretoria, South Africa

Abstract
As historically acknowledged in the Reasoning about Actions and Change community,
intuitiveness of a logical domain description cannot be fully automated. Moreover, like any
other logical theory, action theories may also evolve, and thus knowledge engineers need
revision methods to help in accommodating new incoming information about the behavior
of actions in an adequate manner. The present work is about changing action domain
descriptions in multimodal logic. Its contribution is threefold: first we revisit the semantics
of action theory contraction proposed in previous work, giving more robust operators that
express minimal change based on a notion of distance between Kripke-models. Second
we give algorithms for syntactical action theory contraction and establish their correctness
with respect to our semantics for those action theories that satisfy a principle of modularity
investigated in previous work. Since modularity can be ensured for every action theory
and, as we show here, needs to be computed at most once during the evolution of a domain
description, it does not represent a limitation at all to the method here studied. Finally
we state AGM-like postulates for action theory contraction and assess the behavior of our
operators with respect to them. Moreover, we also address the revision counterpart of
action theory change, showing that it benefits from our semantics for contraction.

1. Introduction
Consider an intelligent agent designed to perform rationally in a dynamic world, and suppose
that she should reason about the dynamics of an automatic coffee machine (Figure 1).

NiceCaf
$

Figure 1: The coffee deliverer agent.
Suppose, for example, that the agent believes that coffee is always a hot beverage.
Suppose now that some day she gets a coffee at the machine and observes that it is cold.
In such a case, the agent must change her beliefs about the relationship between the two
propositions I hold a coffee and I hold a hot drink. This example is an instance of
the problem of changing propositional belief bases and has been largely addressed in the
c
2010
AI Access Foundation. All rights reserved.

fiVarzinczak

literature about belief revision (Alchourron, Gardenfors, & Makinson, 1985; Gardenfors,
1988; Hansson, 1999) and belief update (Katsuno & Mendelzon, 1992).
Next, let our agent believe that whenever she buys a coffee from the machine, she gets a
hot drink. This means that in every state of the world that follows the execution of buying
a coffee, the agent ends up with a hot drink. Now, in a situation where the machine is
running out of cups, after buying, the coffee runs through the shelf and the agent, contrary
to what she was expecting, does not hold a hot drink in her hands.
Imagine now that the agent never considered any relation between buying a coffee on the
machine and its service availability, in the sense that she always believed (quite reasonably)
that buying does not prevent other users from using the machine. Nevertheless, someday
our agent is queuing to buy a coffee and observes that just after the agent before her has
bought, the machine went out of order (maybe due to a lack of coffee powder).
Completing our agents struggle in discovering the intricacies of operating a coffee machine, let us suppose now that she always believed that if she has a token, then it is possible
to buy coffee, provided that some other preconditions like being close enough to the button,
having a free hand, etc, are satisfied. Eventually, due to a blackout, the agent realizes that
she does not manage to buy her coffee, even with a token.
The last three examples illustrate cases in which changing the beliefs about the behavior
of the action of buying coffee is mandatory. In the first one, buying coffee, once believed to
have a deterministic outcome, namely always a hot drink, has now to be seen as nondeterministic or, alternatively, to have a different effect in a more specific context (e.g. if there
is no cup in the machine). In the second example, buying a coffee is now known to have
side-effects (ramifications) which one was not aware of. Finally, in the last example, the
feasibility of the action under concern is questioned in the light of new information showing
a context that was not known to preclude its execution.
Such cases of theory change are very important when one deals with logical descriptions
of dynamic domains: it may always happen that one discovers that an action actually has
a behavior that is different from that one has always believed it had.
Up to now, theory change has been studied mainly for knowledge bases in classical logics,
both in terms of revision and update. Since the work by Fuhrmann (1989), only in a few recent studies has it been considered in the realm of modal logics, viz. in epistemic logic (Hansson, 1999) and in dynamic logics (Herzig, Perrussel, & Varzinczak, 2006). Recently some
studies have investigated revision of beliefs about facts of the world (Shapiro, Pagnucco,
Lesperance, & Levesque, 2000; Jin & Thielscher, 2005) or the agents goals (Shapiro,
Lesperance, & Levesque, 2005). In our scenario, this would concern for instance the truth
of token in a given state: the agent believes that she has a token, but is actually wrong
about that. Then she might subsequently be forced to revise her beliefs about the current
state of affairs or change her goals according to what she can perform in that state. Such
belief revision operations do not modify the agents beliefs about the action laws. On the
other hand, here we are interested exactly in such modifications. Starting with Baral and
Lobos work (1997), some recent studies have been done on that issue (Eiter, Erdem, Fink,
& Senko, 2005) for domain descriptions in action languages (Gelfond & Lifschitz, 1993).
We here take a step further in this direction and propose a method which is more robust
by integrating a notion of minimal change and complying with postulates of theory change.
190

fiOn Action Theory Change

The present text is structured as follows: in Section 2 we establish the formal background that will be used throughout this article. Sections 36 are the core of the work: in
Section 3 we present the central definitions for a semantics of action theory change, providing justifications for the design choices here made (Section 4). Section 5 is devoted to
the syntactical counterpart of our operators while Section 6 to the proof of its correspondence with the semantics under certain acceptable conditions. In Section 7 we discuss some
postulates for contraction/erasure and then present a semantics for action theory revision
(Section 8). After a discussion on and comparison with existing work in the field (Section 9),
we conclude with an overview and future directions of research.

2. Logical Preliminaries
Following the tradition in the Reasoning about Actions and Change (RAC) community,
we consider action theories to be finite collections of statements that have the particular
form (Shanahan, 1997):
 if context, then effect after every execution of action (effect laws);
 if precondition, then action executable (executability laws).
Statements mentioning no action at all represent laws about the underlying structure of the
world, i.e., its possible states (static laws).
Several logical frameworks have been proposed to formalize such statements (Shanahan,
1997). Among the most prominent ones are the first-order based Situation Calculus (McCarthy & Hayes, 1969; Reiter, 2001), the family of Action Languages (Gelfond & Lifschitz,
1993; Giunchiglia, Kartha, & Lifschitz, 1997), the Fluent Calculus (Thielscher, 1997), and
Propositional Dynamic Logic (PDL) (Harel, Tiuryn, & Kozen, 2000) with different specific extensions thereof (De Giacomo & Lenzerini, 1995; Castilho, Gasquet, & Herzig, 1999;
Zhang & Foo, 2001; Castilho, Herzig, & Varzinczak, 2002).
Here we opt to formalize action theories using the multimodal logic Kn (Popkorn, 1994).
Among the main reasons for such a choice are:
 We benefit from the well defined semantics for multimodal logics which, as we are
going to see in the sequel, provides simple and intuitive foundations on which to build
the meaning of changing action domain descriptions.
 Kn syntax allows us to express all the afore mentioned types of laws without requiring
the full expressiveness of PDL or the machinery of a first-order language.
 Since Kn is the core of all above mentioned PDL-based action formalisms, all we shall
say in the sequel should smoothly transfer to them.
 Contrary to first-order based approaches, Kn is decidable and has several implemented
theorem provers for it available in the literature.
191

fiVarzinczak

2.1 Action Theories in Multimodal Logic
Let Act = {a1 , a2 , . . . , an } be the set of all atomic action constants of a given dynamic
domain. An example of atomic action is buy. To each atomic action a there is associated
a modal operator [a]. We here suppose that our multimodal logic is independently axiomatized (Kracht & Wolter, 1991), i.e., the logic is a fusion and there is no interaction between
the different modal operators.1
Prop = {p1 , p2 , . . . , pn } denotes a finite set of propositional constants, also called fluents
or elementary atoms. Examples of those are token (the agent has a token) and coffee
(the agent holds a coffee). Lit = {p, p : p  Prop} is the set of literals. We use ` to
denote a literal. If ` = p, then we identify ` with p. By |`| we denote the atom in `.
We use small Greek letters , , . . . to denote Boolean (propositional) formulas. They
are recursively defined in the usual way:
 ::= p | > |  |  |    |    |    |   
(   denotes (  )  (  ).) Fml is the set of all Boolean formulas. An example of a
Boolean formula is coffee  hot. A propositional valuation v is a maximal consistent set of
literals. We denote by v   the fact that v satisfies a propositional formula . By val()
we denote the set of all valuations satisfying . By CPL we denote Classical Propositional
Logic and |=CPL is its respective consequence relation. Cn() denotes all logical consequences
of  in CPL, i.e., Cn() = { :  |=
}.
CPL
If  is a propositional formula, atm() denotes the set of elementary atoms actually
occurring in . For example, atm(p1  (p1  p2 )) = {p1 , p2 }.
For  a Boolean formula, IP() denotes the set of its prime implicants (Quine, 1952),
i.e., the weakest terms (conjunctions of literals) that imply . As an example, IP(p1  p2 ) =
{p1  p2 , p1  p2 }. For more on prime implicants, their properties and how to compute
them, see the chapter by Marquis (2000). With  we denote a prime implicant, and given `
and , `   abbreviates ` is a literal of . For a given set X, X denotes its complement.
Hence atm() denotes Prop \ atm().
We denote complex formulas (possibly with modal operators) by , , . . . They are
recursively defined in the following way:
 ::=  | [a] |  |    |    |    |   
hai is the dual operator of [a], defined by hai =def [a]. An instance of a complex
formula in our scenario example is coffee  [buy]coffee.
Given a complex formula , with act() we denote the action names occurring in ,
i.e., the modalities of . For example, act([a2 ]p1  ([a1 ]p2  [a2 ]p3 )) = {a1 , a2 }.
The semantics here is the standard semantics of multimodal logic Kn (Popkorn, 1994).
Definition 2.1 (Kn -Model) A Kn -model is a tuple M = hW, Ri where W is a set of valuations (also called possible worlds), and R maps action constants a to accessibility relations
Ra  W  W.
1. Later on we will see that this is a requirement to ensure that an action theory is modular.

192

fiOn Action Theory Change

As an example, for Act = {a1 , a2 } and Prop = {p1 , p2 }, we have the Kn -model M =
hW, Ri, where
W = {{p1 , p2 }, {p1 , p2 }, {p1 , p2 }},


({p1 , p2 }, {p1 , p2 }), ({p1 , p2 }, {p1 , p2 }),
R(a1 ) =
({p1 , p2 }, {p1 , p2 }), ({p1 , p2 }, {p1 , p2 })
R(a2 ) = {({p1 , p2 }, {p1 , p2 }), ({p1 , p2 }, {p1 , p2 })}
Figure 2 gives a graphical representation of the model M .
a1
w1
p1 , p2

w2

a1

p1 , p2

a2

M :

a1

a1

w3
p1 , p2
a2

Figure 2: Example of a Kn -model for Act = {a1 , a2 }, and Prop = {p1 , p2 }.
Notice that our definition of Kn -model does not follow the traditional notion from modal
logics: here no two worlds satisfy the same valuation. This is a pragmatic choice, as we
will see in Section 5. Nevertheless, all we shall say in the sequel can be straightforwardly
formulated for standard Kn models as well.
Definition 2.2 (Truth Conditions) Given a Kn -model M = hW, Ri,
M

 |=
p (p is true at world w of M ) iff w  p (valuation w satisfies p, i.e., p  w);
w
M

M

 |=
[a] iff |= 0  for every w0 such that (w, w0 )  Ra ;
w
w

M

M

M

 |=
   iff |=
 and |=
;
w
w
w
M

M

M

 |=
 iff 6|=w , i.e., not |=w ;
w
 truth conditions for the other connectives are as usual.
By M we will denote a (possibly empty) set of Kn -models.
M

M

A Kn -model M is a model of  (denoted |= ) if and only if for all w  W, |=
. In the
w
M

M

model depicted in Figure 2, we have |= p1  [a2 ]p2 and |= p1  p2 . M is a model of a set
M
M
of formulas  (noted |= ) if and only if |=  for every   . If  is the set of formulas
we start off with (our non-logical theory), then each    is called a global axiom.
193

fiVarzinczak

Definition 2.3 (Global Consequence) A formula  is a global consequence of a set
of global axioms  in the class of all Kn -models (noted  |=
) if and only if for every
K
M

n

M

Kn -model M , if |= , then |= .
With Kn we can state laws describing the behavior of actions. One way of doing this is by
stating some formulas as global axioms.2 As usually done in the RAC community (Shanahan, 1997), we here distinguish three types of laws. The first kind of statements are static
laws, which are constraints on the allowed states of a dynamic domain.
Definition 2.4 (Static Law) A static law is a global axiom   Fml.
An example of a static law is coffee  hot, saying that if the agent holds a coffee, then she
holds a hot drink. In the Situation Calculus formalism (Reiter, 2001) one would write the
first-order formula s.[coffee(s)  hot(s)]. The set of all static laws of a scenario is denoted
by S  Fml. In our example we will have S = {coffee  hot}.
The second kind of action law we consider is given by the effect laws. These are formulas
relating an action to its effects, which can be conditional.
Definition 2.5 (Effect Law) Let ,   Fml. An effect law for action a is a global axiom
of the form   [a].
The consequent  is the effect which always obtains in accessible states (which need not
exist in general) when action a is executed in a state where the antecedent  holds. In our
Kripke semantics, this means that in every possible world where  holds, every transition
by an a-labeled arrow (if any) leads to a possible world where  holds. If a is a nondeterministic action, then the consequent  is typically a disjunction. An example of an effect
law is coffee  [buy]coffee, saying that in a situation where the agent has no coffee, after
buying, the agent has a coffee. If  is inconsistent, then we have a special kind of effect
law that we call an inexecutability law. For example, we could also have token  [buy],
expressing that buy cannot be executed if the agent has no token. In the Situation Calculus our examples of effect and inexecutability laws would be expressed respectively as
s.[coffee(s)  coffee(do(buy, s))] and s.[token(s)  Poss(buy, s)].
The set of effect laws of a given scenario is denoted by E . In our coffee machine scenario,
we could have for example:


 coffee  [buy]coffee, 
token  [buy]token,
E =


token  [buy]
Finally, we also define executability laws, which stipulate the context where an action is
guaranteed to be executable. In Kn , the operator hai is used to express executability. hai>
thus reads the execution of a is possible. Formally, hai> being true in a world w means
that there is at least one world w0 accessible from w via Ra (cf. Definition 2.2).
2. An alternative to that is given by Castilho et al. (1999, 2002), with laws being stated with the aid of an
extra universal modality and local consequence being thus considered.

194

fiOn Action Theory Change

Definition 2.6 (Executability Law) Let   Fml. An executability law for action a is
a global axiom of the form   hai>.
For instance, token  hbuyi> says that buying can be executed whenever the agent has a
token. The set of all executability laws of a given domain is denoted by X . In our scenario
example we will have X = {token  hbuyi>}.
Note that in principle one needs to know nothing about the accessible world w0 . However,
a common (albeit tacit) assumption in the RAC community is that we state executability
laws only for actions of which we know the effects, in other words act(X )  act(E ).
In the Situation Calculus our example would be stated as s.[token(s)  Poss(buy, s)].
However, we point out that, traditionally, in Reiter basic action theories (Reiter, 2001)
executability laws and inexecutability laws are mixed together in the form of bi-conditionals
like s.[token(s)  Poss(buy, s)], called precondition axioms. For a critique of such a
practice and its implications in formalizing dynamic domains, see the work by Herzig and
Varzinczak (2007).
With our three basic types of laws, we are able to define action theories:
Definition 2.7 (Action Theory) Given any (possibly empty) sets of laws S , E , and X ,
T = S  E  X is an action theory.
Given an action theory T and an action a, Ea (resp. Xa ) will denote the set of only those
effect (resp. executability) laws about a in E (resp. X ). Ta = S  Ea  Xa is then the action
theory for a.
It is worth noting that for a1 , a2  Act, a1 6= a2 , the intuition is indeed that Ta1 and Ta2
overlap only on S , i.e., the only laws that are common to both Ta1 and Ta2 are the laws
about the structure of the world. This requirement is somehow related with the underlying
modal logic being independently axiomatized (see note above).
2.2 The Frame, Ramification and Qualification Problems
During the last 40 years, most of the effort in the reasoning about actions community has
been devoted to searching for satisfactory solutions to the frame problem, the ramification
problem and the qualification problem.
Roughly speaking, the frame problem (McCarthy & Hayes, 1969) relates to the need for
inferring the persistence of some facts of the world after the execution of an action known
not to affect them, without having to state that explicitly in the form of frame axioms.
(Frame axioms are a special type of effect law, having the form `  [a]`, for `  Lit.)
In our example, buying a coffee in a context where the agent has already got one does
not make it lose the coffee: coffee  [buy]coffee should be a consequence of our theory.
The ramification problem (Finger, 1987) comes from the observation that an action may
have several possibly interdependent effects and stating all of them explicitly is a huge
task. In our scenario, we want to be able to infer [buy]hot without saying it in the theory,
and in such a way some intrinsic causal connection between coffee and hot is taken into
account. Finally, the qualification problem (McCarthy, 1977) amounts to addressing the
issue of ensuring that an action is executable in a given context. Specifying all the sufficient
195

fiVarzinczak

conditions for an action to be executable is an incredibly hard task. In our example, one
may state token  hbuyi>, but it may well be the case that buying fails due to some
condition unforeseen at design time, like the agents arm being rusty and stuck.
For more on these core problems of the RAC community, the reader is referred to the
book by Shanahan (1997).
For the sake of clarity, here we abstract from the frame and ramification problems, and
suppose that the agents theory already entails all the relevant frame axioms. We point out
however that all we shall say could have been defined within a formalism with a solution to
the frame and ramification problems. For instance, we could have used any suitable solution
to the frame problem, like e.g. the dependence relation (Castilho et al., 1999), which is used
in the work of Herzig et al. (2006), or a kind of successor state axioms in a slightly modified
setting (Demolombe, Herzig, & Varzinczak, 2003). To make the presentation more clear
to the reader, here we do not bother with a particular solution to the frame problem and
just assume that all frame axioms can be inferred from the action theory. Actually we
can suppose that all intended frame axioms are automatically recovered and stated in the
theory, more specifically, in the set of effect laws.
Given the largely acknowledged difficulty of the qualification problem in the literature (Shanahan, 1997), we do not assume here any a priori solution to it. Instead, as tacitly
assumed in many approaches to reasoning about actions (Castilho et al., 1999; Zhang &
Foo, 2001; Reiter, 2001), we suppose that the knowledge engineer may want to state some
(not necessarily fully specified) executability laws for some actions. These may be incorrect
at the starting point (and in all probability they will be), but revising wrong executability
laws is an approach towards its solution and one of the aims of this work. With further
information the knowledge engineer will have the chance to change them so that eventually
they will correspond to the intuition (cf. Sections 3 and 8).
Having agreed on these points, the action theory of our example will be:


coffee

hot,
token

hbuyi>,






coffee  [buy]coffee,
T=
token  [buy]token, token  [buy], 





coffee  [buy]coffee, hot  [buy]hot
(We have not stated the frame axiom token  [buy]token because it can be trivially
deduced from the inexecutability law token  [buy].)
Figure 3 below shows a Kn -model for the action theory T above.
We are going to see in the sequel that the finite base T formalizing the action theory
plays a role in the contraction of laws. In particular, the base representing the static laws
turns out to be quite important. So given an action theory T, it will be useful to consider
models of T whose possible worlds are all the possible valuations allowed by S :
Definition 2.8 (Canonical Frame) Let T = S  E  X be an action theory. Then the
tuple Mcan = hWcan , Rcan i is the canonical frame of T if and only if:
 Wcan = val(S ); and
S
M
M
 Rcan = aAct Ra s.t. Ra = {(w, w0 ) : for all   [a]  Ea , if |=w , then |= 0 }.
w

196

fiOn Action Theory Change

w1
t, c, h
b

b
w3

w2

M :

t, c, h

t, c, h

b

w4
t, c, h

Figure 3: A model for our coffee machine scenario: b, t, c, and h stand for, respectively,
buy, token, coffee, and hot.

The canonical frame of an action theory need not be one of its models. To witness why,
let Prop = {p}, Act = {a}, and consider the simple action theory {p  [a], p  hai>}.
Then in the associated canonical frame we have Wcan = {{p}, {p}}. Clearly the world {p}
does not satisfy this theory.
Definition 2.9 (Canonical Model) M is a canonical model of T if and only if M is a
M
canonical frame of T and |= T.
Figure 4 below shows the canonical model of our action theory example T.
w1
t, c, h
b

M :

b
w3

w2
t, c, h

w5

b

t, c, h

w6

w4
t, c, h

t, c, h

t, c, h

Figure 4: The canonical model for the coffee machine scenario.

2.3 Prime Valuations
We say that an atom p is essential to a formula  if and only if p  atm(0 ) for every 0
such that |=
  0 . For instance, p1 is essential to p1  (p1  p2 ). Given , atm!()
CPL
denotes the set of essential atoms of . (If  is not contingent, i.e.,  is a tautology or a
contradiction, then atm!() = .)
Given  a Boolean formula,  is the set of all formulas 0 such that  |=
0 and
CPL
0
atm( )  atm!(). ForV instance, p1 V
 p2 
/ p1 , as p1 |=
p  p2 but atm(p1  p2 ) 6
CPL 1
atm!(p1 ). Clearly, atm( ) = atm!( ), moreover whenever |=CPL   0 is the case,
then atm!() = atm!(0 ) and also  = 0 .
197

fiVarzinczak

Theorem 2.1 (LeastVAtom-Set Theorem, Parikh, 1999) Let  be a propositional formula. Then |=CPL   , and for every 0 such that |=CPL   0 , atm()  atm(0 ).
A proof of this theorem is given by Makinson (2007) and we do not state it here.
Essentially, the theorem establishes that for every Boolean formula , there is a unique
least set of elementary atoms such that  may equivalently be expressed using only atoms
from that set. Hence, Cn() = Cn().
Given a valuation v, v0  v is a subvaluation. Given a set of valuations W, a subvaluation
v0 satisfies a propositional formula  modulo W (noted v0 W ) if and only if v   for all
v  W such that v0  v. We say that a subvaluation v essentially satisfies  (modulo W),
!
!
noted v W , if and only if v W  and {|`| : `  v}  atm!(). If v W , we call v an
essential subvaluation of  (modulo W).
Definition 2.10 (Prime Subvaluation) Let  be a Boolean formula and W a set of val!
uations. A subvaluation v is a prime subvaluation of  (modulo W) if and only if v 

W
!

and there is no v0  v such that v0 W .
A prime subvaluation of a formula  is thus one of the weakest states of truth in which 
is true. Hence, prime subvaluations are just another way of seeing prime implicants (Quine,
1952) of . By base(, W) we will denote the set of all prime subvaluations of  modulo W.
Proposition 2.1 W
Let   FmlVand W be a set of valuations. Then for all w  W, w  
if and only if w  vbase(,W) `v `.
Proof: Right to left direction is straightforward. For the left to right direction, if w  ,
0
then w  . Let w0  w be the least subset of w still
Vsatisfying . Clearly, w is a prime
subvaluation of  modulo W, and then because w  `w0 `, the result follows.
2
2.4 Closeness between Models
When contracting a formula from a model, we will perform a change in its structure. Because
there can be several different ways of modifying a model (not all of them minimal), we need
a notion of distance between models to identify those that are closest to the original one.
As we are going to see in more depth in the next section, changing a model amounts
to modifying its possible worlds or its accessibility relation. Hence, the distance between
two Kn -models will depend upon the distance between their sets of worlds and accessibility
relations. These here will be based on the symmetric difference between sets, defined as
X Y = (X \ Y )  (Y \ X).
Definition 2.11 (Closeness between Kn -Models) Let M = hW, Ri be a model. Then
M 0 = hW0 , R0 i is at least as close to M as M 00 = hW00 , R00 i, noted M 0 M M 00 , if and only if
 either WW0  WW00 ;
 or WW0 = WW00 and RR0  RR00 .
198

fiOn Action Theory Change

This is an extension of Burger and Heidemas relation (Burger & Heidema, 2002) to our
modal case. It defines a lexicographic order on the set of all Kn -models. Although simple,
this notion of closeness turns out to be sufficient for our purposes here, as we shall see in the
sequel. Notice that other notions of distance between models could have been considered as
well, namely the cardinality of symmetric differences or Hamming distance. (See Section 4
for a discussion on this.)

3. Semantics of Action Theory Change
When admitting the possibility of a law  failing, one must ensure that  becomes invalid,
i.e., not true in at least one model of the dynamic domain that is formalized. Because there
can be lots of such models, we may have a set M of models in which  is (potentially) valid.
Thus contracting  amounts to making it no longer valid in this set of models. What are
the operations that must be carried out to achieve that? Throwing models out of M does
not work, since  will keep on being valid in all models of the remaining set. Thus one
should add new models to M. Which models? Well, models in which  is not true. But
not any of such models: taking models falsifying  that are too different from our original
models will certainly violate the principle of minimal change.
Hence, we shall take some model M  M as basis and manipulate it to get a new model
0
M in which  is not true. In our modal semantics, the removal of a law  from a model
M = hW, Ri means modifying the possible worlds or the accessibility relation in M so that
 becomes false. Such an operation gives as result a set M of models each of which is no
longer a model of . But if there are several candidates, which ones should we choose? We
shall take those models that are minimal modifications of the original M , i.e., those which
are minimal with respect to our distance M between models. Of course, there can be more
than one such an M 0 that is minimal with respect to M . In that case, because adding
just one of these new models is enough to invalidate , we take all possible combinations
M  {M 0 } of expanding our original set of models M by one of these minimal models.
(Observe that this approach relates to orderly maxichoice contraction Hansson, 1999.) The
result will be a set of sets of models. In each set of models there will be precisely one model
M 0 falsifying .
It might be claimed that, as such, our contraction method described above does not
respect the so-called principle of categorical matching: the input and output are different
sorts of objects, namely a set of models and a set of sets of models. It is easy to see,
however, that the reasoning above can be stated in such a way that each output set of
models corresponds precisely to the result of one contraction operator, satisfying then the
referred principle. The choice for defining the result of an operation as a set of possible
outputs will become more clear in Section 5, where we are going to present algorithms that
correspond exactly to our semantic constructions.
3.1 Model Contraction of Executability Laws
To contract an executability law   hai> from one model, intuitively we should remove
transitions leaving -worlds. In order to succeed in the operation, we have to guarantee
that in the resulting model there will be at least one -world with no departing a-arrow.

199

fiVarzinczak


Definition 3.1 Let M = hW, Ri. M 0 = hW0 , R0 i  Mhai>
if and only if

 W0 = W;
 R0  R;
M

; and
 if (w, w0 )  R \ R0 , then |=
w
M0

 there is w  W0 such that 6|=w   hai>.


Observe that Mhai>
6=  if and only if  is satisfiable in W. Moreover, M  Mhai>
M

if and only if 6|=   hai>.
Just to provide the reader with an insight on how this operation would be carried out
in the Situation Calculus, there one should look at a given situation s in which  holds
and then modify the interpretation of the predicate Poss(a) so that it becomes false in s.
Like in our case, there may be many of such situations and then all of them must be taken
into account. An essential difference here is that our Kripke structures are always finite,
whereas the space of situations is possibly infinite (Reiter, 2001).
To get minimal change, we want such an operation of removing transitions to be minimal
with respect to the original model: one should remove a minimum set of transitions which
is sufficient to get the desired result.
Definition 3.2 contract(M ,   hai>) =

S


min{Mhai>
, M }

And now we define the sets of possible models resulting from the contraction of an
executability law in a set of models:
Definition 3.3 Let M be a set of models, and   hai> an executability law. Then
0
0
0
0
M
hai> = {M : M = M  {M }, M  contract(M ,   hai>), M  M}

In our running example, consider M = {M }, where M is the model in Figure 4. When
the agent discovers that even with a token she does not manage to buy a coffee any more,
she has to change her models in order to admit (new) models with states where token is
the case but from which there is no buy-transition at all. Because having just one such
a world in each new model is enough, taking those resulting models whose accessibility
relations are maximal guarantees minimal change. Hence we will have M
tokenhbuyi> =
0
0
0
0
{M  {M1 }, M  {M2 }, M  {M3 }}, where each Mi is depicted in Figure 5.
M

Clearly, if  is not satisfied in M, i.e., |=  for all M  M, then the contraction
of   hai> does not succeed. This is in line with the expectations and it relates to the
Success Postulate (cf. Section 7.2).
200

fiOn Action Theory Change

w1

w1
t, c, h

t, c, h
b

M10 :

b
w3

w2
t, c, h

w5

w6

w4
t, c, h

t, c, h

b

t, c, h

w6

w4

w5

t, c, h

t, c, h

w3

w2

M20 :

t, c, h

b

t, c, h

t, c, h

t, c, h

w1
t, c, h
b

M30 :

b
w3

w2
t, c, h

t, c, h

w5

w6

w4
t, c, h

t, c, h

t, c, h

Figure 5: Models resulting from contracting token  hbuyi> in the model M of Figure 4.
3.2 Model Contraction of Effect Laws
When our agent discovers that there may be some cases where after buying she gets no
hot drink, she must e.g. give up the belief in the effect law token  [buy]hot in her set of
models. This means that token  hbuyihot shall now be admitted in at least one world of
some of the new models of her set of beliefs. Therefore, to contract an effect law   [a]
from a given model, intuitively we have to add new transitions from -worlds to worlds not
satisfying . As we shall see, the great challenge in such an operation is precisely how to
guarantee minimal change.
In our example, when contracting token  [buy]hot from the model of Figure 4 we shall
add transitions from token-worlds to hot-worlds. Because coffee  hot is a static law and
so is hot  coffee, this should also give us hbuyicoffee in some token-world (coffee is
causally relevant to hot, i.e., to have hot we must also have coffee). This means that if
we allow for hbuyihot in some token-world, we also have to allow for hbuyicoffee in that
same world. The same argument does not necessarily hold for token: allowing for hbuyihot
does not necessarily oblige us to allow for hbuyitoken in the respective world. This is because
token is not relevant to hot (as coffee is). This means that we have the freedom either
to allow for it or not.
Hence, in our running example we can add transitions from token-worlds to hot 
coffee  token-worlds, as well as to hot  coffee  token. This situation is depicted in
Figure 6. For instance, we can add a new buy-arrow from the world {token, coffee, hot}
to one of these candidates (Figure 7).
In the Situation Calculus, such a modification would be slightly different, but with the
same intuition behind: one should look at a given situation s in which  holds and then
modify the interpretation of the fluents (atoms) in do(a, s), the situation resulting from
performing a in s. Alternatively, new -situations should lead to at least one -situation.
201

fiVarzinczak

w1
t, c, h
b

M :

b
w3

w2
t, c, h

w5

t, c, h

b

w6

w4
t, c, h

t, c, h

t, c, h

Figure 6: Candidate worlds to receive transitions coming from token-worlds.
w1
t, c, h
b

M :

b
w3

w2
t, c, h

w5

b

t, c, h

w6

w4
t, c, h

b

t, c, h

t, c, h

b

Figure 7: Two candidate new buy-arrows to falsify token  [buy]hot in M .
Notice however that this would require the addition of new whole branches to the tree-like
first-order model induced by Reiter basic action theories (Reiter, 2001).
Back to our example, observe that adding the new transition to {token, coffee, hot}
itself would make us lose the effect token, true after every execution of buy in the original
M
model (|= token  [buy]token). How do we preserve this law while allowing for the new
transition to a hot-world? That is, how do we get rid of the effect hot without losing effects
that are not relevant for that? We here develop an approach for this issue.
When adding a new transition leaving a world w we intuitively want to preserve as many
effects as we had before doing so. To achieve this, it is enough to preserve old effects only
in w (because the remaining structure of the model remains unchanged after adding the
new transition). Of course, we cannot preserve effects that are inconsistent with  (those
will all be lost). Hence it suffices to preserve only the effects that are consistent with .
To achieve that we must observe what is true in w and in the target world w0 :
 The proper effects of the action in world w0 , i.e., what changes from w to w0 (w0 \ w)
through the new execution of a must be what is obliged to be so: either because those
literals that now change from w to w0 are necessary to having  in w0 (like coffee
in our example) or because they are necessary to have another effect (independent of
, like token) in world w0 .
202

fiOn Action Theory Change

 The non-effects of action a in world w0 , i.e., what does not change from w to w0
(w  w0 ) through as new execution should be only what is allowed to be so: certain
literals are never preserved (like token in our example), then when pointing the new
transition towards a world where it does not change with respect to the leaving world
(hot  coffee  token in our example), we may lose effects that held in w before
adding the transition.
This means that the only things allowed to change in the candidate target world must
be those that are forced to change, either by some non-related law or because of having
 modulo a set of states W. In other words, we want the literals that (now) change from
w to w0 to be at most those that are sufficient to get  modulo W, while preserving the
maximum of other effects. Every change beyond that is not an intended one. Similarly,
we want the literals from w that are (now) preserved in the target world w0 to be at most
those that are usually preserved in a given set of models. Every preservation beyond those
may make us lose some law. This looks like prime implicants, and that is where prime
subvaluations play their role: the worlds to which the new transition will point are those
whose difference with respect to the departing world are literals that are relevant and whose
similarity with respect to it are literals that we know do not change.
Definition 3.4 (Relevant Target Worlds) Let M = hW, Ri be a model, w, w0  W, M
a set of models such that M  M, and   [a] an effect law. Then w0 is a relevant target
world of w with respect to   [a] for M in M if and only if
M

M

 |=
 and 6|= 0 ;
w
w

 for all `  w0 \ w:
 either there is v  base(, W) such that v  w0 and `  v;
 or there is  0  Fml such that there is v0  base( 0 , W) such that v0  w0 , `  v0 ,
M
and for every Mi  M, |=w i [a] 0
 for all `  w  w0 :
 either there is v  base(, W) such that v  w0 and `  v;
M

i
 or there is Mi  M such that 6|=
[a]`;
w

By RelTarget(w,   [a], M , M) we denote the set of all relevant target worlds of w with
respect to   [a] for M in M.
Note that we need the set of models M (and here we can suppose it contains all models
of the theory we want to change) because preserving effects depends on what other effects
hold in the other models that interest us. We need to take them into account in the
local operation of changing one model. (The reason we do not need M in the definition

of the local, one model contraction of executability laws Mhai>
is that when removing
transitions there is no way of losing effects, as every effect law that held in the world from
which a transition has been removed remains true in the same world in the resulting model.)
203

fiVarzinczak

Definition 3.5 Let M = hW, Ri, and M be such that M  M. Then M 0 = hW0 , R0 i 

M[a]
if and only if
 W0 = W;
 R  R0 ;
 If (w, w0 )  R0 \ R, then w0  RelTarget(w,   [a], M , M); and
M0

 there is w  W0 such that 6|=w   [a].

Observe that M[a]
6=  if and only if  and  are both satisfiable in W. Moreover,
M


M  M[a]
if and only if 6|=   [a].

Because having just one world where the law is no longer true in each model is enough,
taking those resulting models whose accessibility relations are minimal with respect to the
original one guarantees minimal change.
Definition 3.6 contract(M ,   [a]) =

S


min{M[a]
, M }

Now we can define the possible sets of models resulting from contracting an effect law
from a set of models:
Definition 3.7 Let M be a set of models, and   [a] an effect law. Then
0
0
0
0
M
[a] = {M : M = M  {M }, M  contract(M ,   [a]), M  M}

Taking again M = {M }, where M is the model in Figure 4, after contracting token 
0
0
0
[buy]hot from M we get M
token[buy]hot = {M  {M1 }, M  {M2 }, M  {M3 }}, where all
Mi0 s are as depicted in Figure 8.
In both cases where  is not satisfiable in M or  is valid in M , of course our operator
does not succeed in falsifying   [a] (cf. end of Section 3.1). Again, this works as
expected and it has to do with the Success Postulate (see also Section 7.2).
3.3 Model Contraction of Static Laws
When contracting a static law from a model, we want to admit the existence of at least
one (new) possible state falsifying it. This means that intuitively we should add new worlds
to the original model. (In a Situation Calculus setting that would correspond to allowing
for situations not satisfying some of the domain constraints.) This is quite easy. A very
delicate issue however is what to do with the accessibility relation: should new transitions
leave/arrive at the new world? If no transition leaves the new added world, we may lose
some executability law. If some transition leaves it, then we may lose some effect law, the
same holding if we add a transition pointing to the new world. On the other hand, if no
transition arrives at the new world, what about the intuition? Is it intuitive to have an
unreachable state? (Similar issues would also arise in Situation Calculus interpretations,
which means that they are independent of the underlying formalism.)
204

fiOn Action Theory Change

w1

w1
t, c, h

b

M10 :

t, c, h
b

b
w3

w2
t, c, h

w3

w2

M20 :

t, c, h

b

b

t, c, h

b

t, c, h

b
w5

w6

w4
t, c, h

w5

t, c, h

t, c, h

b

t, c, h

w6

w4
t, c, h

t, c, h

w1
t, c, h
b

M30 :

b
w3

w2
t, c, h

b

t, c, h

b
w5

w6

w4
t, c, h

t, c, h

t, c, h

Figure 8: Models resulting from contracting token  [buy]hot in the model M of Figure 4.

All this discussion shows how drastic a change in the static laws might be: it is a change
in the underlying structure (possible states) of the world! Changing them may have as an
indirect, unexpected (and in all probability unwanted) consequence the loss of some effect
law(s) or some executability law(s). What we can do is choose which type(s) of laws we
may accept to lose in this process and then postpone their change (by the other operators).
Following the tradition in the RAC community, which states that executability laws are
in general more difficult to formalize than effect laws, and therefore they are more likely
to be incorrect (Shanahan, 1997), here we prefer not to change the accessibility relation,
which means that we preserve effect laws and postpone the correction of executability laws,
if required. (Remember that this is an approach towards a solution to the qualification
problem  cf. Section 2.2 above.)
One may argue that doing things this way makes our three operators incoherent in the
sense that for effect and executability laws we adopt a minimal change approach, giving
stronger theories, whereas for static laws we adopt a more cautious approach, giving weaker
theories (see the next section). It is worth noting however that as largely recognized by the
RAC community, the different laws of a domain description do not have the same status:
a minimal change approach for static law contraction that preserves as many executability
laws as possible, even if coherent, would definitely fail to cope with the qualification problem.
Moreover, by propagating wrong executability laws, such a coherent method would definitely
be less elaboration tolerant (McCarthy, 1998) than the one we are defining with regards to
further modifications of the theory.
For those reasons, our contention here is that static law contraction should be cautious.
(For a detailed discussion on this, see Section 4.2 below and the end of Section 5.3.)

205

fiVarzinczak

Definition 3.8 Let M = hW, Ri. M 0 = hW0 , R0 i  M if and only if
 W  W0 ;
 R = R0 ; and
M0

 there is w  W0 such that 6|=w .
M

Note that M =  if and only if |= . Moreover, M  M if and only if 6|= .
The minimal modifications of one model are defined as usual:
S
Definition 3.9 contract(M , ) = min{M , M }
And now we define the sets of models resulting from contracting a static law from a
given set of models:
Definition 3.10 Let M be a set of models, and  a static law. Then
0
0
0
0
M
 = {M : M = M  {M }, M  contract(M , ), M  M}

In our scenario example, if the initial set of models is M = {M }, where M is the model
in Figure 4, then contracting the static law coffee  hot from M would give us the resulting
0
0
0
new set of models M
coffeehot = {M  {M1 }, M  {M2 }}, where each Mi is as depicted
in Figure 9 below.
w1

w7
t, c, h

b

M10 :

w5

t, c, h
b

w3
b

M20 :

t, c, h

w6

w4
t, c, h

t, c, h

b

w2
t, c, h

w1

w7

t, c, h

t, c, h

t, c, h

b
w3

w2
t, c, h

b

t, c, h

w6

w4

w5
t, c, h

t, c, h

t, c, h

Figure 9: Models resulting from contracting coffee  hot in the model M of Figure 4.
Notice that by not modifying the accessibility relation all the effect laws which are true
in the original model M are preserved in the resulting models. This is ensured by [buy]
being true in the new world w7 .
It is only some executability laws that are potentially lost, due to the cautiousness of
our approach. For instance, in M10 above, it is no longer the case that token  hbuyi> is
true, since now there is a world, namely w7 , which does not satisfy it anymore. (In M20 this
executability law is still true in every possible world.)
It is worth point out, however, how our approach is indeed in line with intuition: when
learning that a new state is now possible, we do not necessarily know all the behavior of the
actions in the new added state. We may expect some action laws to hold in the new world
(see end of Section 5.3), but, with the information we dispose, not touching the accessibility
relation is the safest way of contracting static laws (cf. Section 4.2 below).
206

fiOn Action Theory Change

4. Interlude
Before presenting the algorithmic counterpart of our action theory change operators, in this
section we discuss alternatives to some of our technical constructions. We point out the
issues that such alternatives would raise. We also provide more justifications for some of
the design choices that have been made in the previous sections.
4.1 Other Distance Notions
Here we have defined and used a model distance which is based on the symmetric difference between sets (Definition 2.11). This distance is an extension to Kripke structures of
Winsletts (1988) notion of closeness between propositional interpretations in the Possible
Models Approach (PMA). Instead of it, however, we could have considered other distance
notions as well, like Dalals (1988) distance, Hamming distance (1950), or weighted distance. Due to space limitations, we do not develop a through comparison among all these
distances here. (For more details, the reader may want to refer to Schlechtas 2004 book.)
We nevertheless do show that with a cardinality-based distance, for example, we may not
always get the intended result.
Let card(X) denote the number of elements in set X. Then suppose that our closeness
between Kn -models was defined as follows:
Definition 4.1 (Cardinality-based Closeness between Kn -Models) Let M = hW, Ri
be a model. Then M 0 = hW0 , R0 i is at least as close to M as M 00 = hW00 , R00 i, noted
M 0 M M 00 , if and only if
 either card(WW0 )  card(WW00 );
 or card(WW0 ) = card(WW00 ) and card(RR0 )  card(RR00 ).
Such a notion of distance is closely related to Dalals (1988) closeness.
Because when contracting a static law  from a model M we usually add one new
possible world, it is easy to see that with this cardinality-based distance we get the same
result in contract(M , ) as with the distance from Definition 2.11.
When it comes to the contraction of action laws, and then changing the accessibility
relations, however, this cardinality-based distance does not seem to fit with the intuitions.
To witness, consider the model M in Figure 10, in which the law p1  hai> is true.
a

w1

w2
p1 , p2

p1 , p2
a

M :

a
w3
p1 , p2

Figure 10: A model of the executability law p1  hai>.
207

fiVarzinczak

Then, the models resulting from contraction of p1  hai> in the model M will be
Mphai> = {M 0 , M 00 }, where M 0 and M 00 are as depicted in Figure 11.
1

w2

w1

a

w1

p1 , p2

p1 , p2

w2
p1 , p2

p1 , p2

a

M0 :

a

M 00 :

w3

w3

p1 , p2

p1 , p2

Figure 11: Models resulting from contracting p1  hai> in the model M of Figure 10.
M 00

Note that M 00 is an intended contracted model: 6|= p1  hai>. However, with the
0
cardinality-based distance above we will get {M }
p1 hai> = {{M , M }}. We do not have
00
0
00
0
{M , M } in the result since M M M : in M only one transition has been removed,
while in M 00 two.
4.2 Minimal Change v. Cautiousness
As usually done in the literature on classical belief revision, when defining a (traditional)
theory change operator one must always make the fundamental decision which of two opposing principles should be the guiding one: that of minimizing change, which leads to
strong modified theories, versus that of cautious change, which leads to weak theories. In
this regard, one might argue that our action theory change operators are incoherent. That
is because we adopt the first principle for the contraction of effect and executability laws,
but then the latter principle for contraction of static laws.3
It turns out, however, that this view is debatable. From a different perspective one can
think of our three operators as being coherent in the following sense: all of them perform a
version of maxichoice, namely the addition of precisely a single model to the original models
of the theory.4
In any case, in the sequel we give a justification for the behavior of our operators and
show that there can be no such an operator for contraction of static laws that is not cautious
while being coherent with the operators for contraction of effect and executability laws. (We
say that an operator for static law contraction is coherent with respect to our operators for
contraction of effect and executability laws if it also performs minimal change with respect
to the other types of laws, i.e., if it preserves effect and executability laws.)
Where does the claimed incoherence come from? Here our contention is that it is
inherent to the problem of action theory change itself, and not a flaw of our definitions.
The justification is as follows. Remembering the intuitions for our semantic constructions,
it is easy to see that for the contraction of executability laws knowledge about some actions
feasibility (the transitions) is removed and only that. For the contraction of effect laws, a
3. We thank an anonymous referee for having pointed this out.
4. We thank another anonymous referee for having pointed this out.

208

fiOn Action Theory Change

piece of knowledge is also added (the new transition), but notice that this one is guided
by some given concrete extra information, namely the  effect that we want to allow.
Now, for the contraction of static laws, notice that no extra information whatsoever is
given about the new possible state which could guide the addition of some knowledge about
the feasibility of an action. The only thing that we know is that the new world should exist.
Nothing more is said about whether there should be any transition leaving it or arriving at
it at all. This is a property of the problem per se: the problem of removing a static law
does not mention executabilities, and it is just reflected by our operator.
Therefore, such an incoherence is already in the problem, and as such it is not surprising
to find it again in the proposed operators. These are designed to do what they are allowed
to do given the constraints of the problem. Should we have more information in our hands
regarding the new added state, a coherent version of the corresponding operator would
have been defined. (See the discussion in Section 9 for a comparison with Eiter et al.s 2005
constraint-based method for update of action theories.)
Proposition 4.1 There is no minimal change operator for static law contraction that is
coherent with our operators for contraction of effect and executability laws.
Proof: Suppose that we have a minimal change based (non-cautious) contraction operator
for static laws that is coherent with the other operators. This operator must be such that
when contracting   Fml only formulas of the type of  are removed (otherwise it is
not coherent with the other operators). This means that both effect and executability laws
should be preserved. In particular, this operator is coherent in this respect when contracting
the formula p1  p2 from model M in Figure 12 below.
w1

w1
p1 , p2

M :

a

p1 , p2

w3

M0 :

w2

a

w2
p1 , p2

p1 , p2

a

p1 , p2

Figure 12: Adding a transition from a new added world in the alternative semantics to
static law contraction. M denotes the original model, while M 0 shows the new
added world and a candidate transition to add to Ra .
Following the intuition about contraction of Boolean formulas, a new world, viz. the
valuation {p1 , p2 }, is added to W in M . Because the operator in question is non-cautious,
a transition should also be added from the new added world {p1 , p2 } in M , in order to
preserve the executability law p1  hai>. Also because the operator is non-cautious, the
effect law p1  [a]p1 should be preserved. Hence, such a new transition should point
neither to world {p1 , p2 } nor to {p1 , p2 } itself. Now, if we direct the new transition to
{p1 , p2 } (the only world that is left), we get the model M 0 in Figure 12.
209

fiVarzinczak

M

M0

Observe that |= (p1  p2 )  [a]p1 . However, 6|= (p1  p2 )  [a]p1 : the operator
makes us lose an effect law! This means that it is not coherent. In order for us to keep this
effect law, the only option is not to direct the new transition to {p1 , p2 }. But then, no
transition is added at all: the operator is cautious! Hence there is no such an operator for
static law contraction that is based on minimal change and is coherent with the operators
for the other laws.
2
The result above supports our contention that we cannot have a coherent set of minimal
change operators for action theory contraction. This is a general result and it holds not
only for modal-based approaches like ours, but it applies to any framework for reasoning
about actions which is based on transition systems and which also allows for the three types
of laws that we consider here.
Furthermore, the result also illustrates well the difference between action theory change
and classical belief change. To witness, even though contraction of static laws amounts to
propositional contraction of Boolean formulas, it remains a special case of the latter. The
reason is that when contracting static laws one always asks what happens to the laws of
other types?, a question that is not asked in classical propositional contraction for the
obvious reason that there simply there are no other types of formulas.

5. Syntactic Operators for Contraction of Laws
Once having given a semantic construction for action theory change, we now turn our
attention to the definition of syntactic operators for changing sets of formulas describing a
dynamic domain.
As Nebel (1989) says, [. . . ] finite bases usually represent [. . . ] laws, and when we
are forced to change the theory we would like to stay as close as possible to the original
[. . . ] base. Hence, besides the definition of syntactical operators, we should also guarantee
that they perform minimal change at the theory level. By that we mean that the resulting
theory should of course not entail the law we want to contract the theory with, and it should
also preserve as much of the previous knowledge as possible when performing syntactical
manipulations on the laws in the original theory. Ideally, from the knowledge engineers
perspective, the modified theory should also keep a certain degree of resemblance with the
original one: the resulting laws should be slight modifications of the relevant ones in the
original action theory.
By T 
 we denote in the sequel the result of contracting a law  from the set of laws T.
5.1 Contracting Executability Laws
For the case of contracting an executability law   hai> from an action theory, first we
have to ensure that action a keeps its executability state in all those contexts where the
antecedent  holds, if that is the case. We achieve that by strengthening the antecedents
of the relevant executability laws. Second, in order to get minimality, we must make a
executable in some contexts where  is true, viz. all -worlds but one. Since there are
possibly many different alternatives for that, this means that we can have several action
theories as outcome. Algorithm 1 gives a syntactical operator to achieve this.
210

fiOn Action Theory Change

It can be easily checked that Algorithm 1 always terminates: the input action theory
T is always finite; from finiteness of Prop follows that of atm(), for any   IP(S  ).
Moreover, the entailment problem of multimodal K is decidable (Harel et al., 2000), as is
that of classical propositional logic. Therefore contracting executability laws is decidable.
Algorithm 1: Contraction of an Executability Law
Input: T,   hai>
Output: T 
hai> /* set of theories output to the knowledge engineer */
1

begin

2

:= 
T
hai>

3

if T |=
  hai> and S 6|=
 then
K
CPL
n

foreach   IP(S  ) do

4

forall A  atm() do
V
V
A := pi atm() pi  pi atm() pi /* extend  to a valuation */

5
6

pi A

if S 6|=
(  A )   then
CPL

7

9

else
:= {T}
T
hai>

11
12
13

/* it is an allowed state */

/* construct a theory that is weaker for that state */
T 0 := (T \ Xa )  {(i  (  A ))  hai> : i  hai>  Xa }
0
:= T 
T
hai>
hai>  {T }

8

10

pi A
/

return T 
hai>
end

It is straightforward to see that Algorithm 1 above can be adapted to Situation Calculus
action theories as well. The crucial point however would be its termination, since entailment
in the Situation Calculus is in general undecidable.
In our running example, contracting the executability law token  hbuyi> from the
0
0
0
action theory T would give us T 
tokenhbuyi> = {T1 , T2 , T3 }, where:


coffee  hot, coffee  [buy]coffee, 





token  [buy]token, token  [buy],
0
T1 =
coffee  [buy]coffee, hot  [buy]hot, 





(token  (coffee  hot))  hbuyi>


coffee

hot,
coffee

[buy]coffee,






token  [buy]token, token  [buy],
0
T2 =
coffee  [buy]coffee, hot  [buy]hot, 





(token  (coffee  hot))  hbuyi>
211

fiVarzinczak






coffee  hot, coffee  [buy]coffee,
token
 [buy]token, token  [buy],
T30 =
coffee
 [buy]coffee, hot  [buy]hot,



(token  (coffee  hot))  hbuyi>









Now all the knowledge engineer has to do is choose which theory is more in line with
her intuitions and implement the required changes (cf. Figure 5).
5.2 Contracting Effect Laws
When contracting an effect law   [a] from an action theory T, intuitively we should contract those effect laws that preclude  in target worlds. In order to cope with minimality,
we must change only those laws that are relevant to the unwanted   [a].
Let (Ea, )1 , . . . , (Ea, )n denote minimal subsets (with respect to set inclusion) of Ea such
that S , (Ea, )i |=
  [a], for 1  i  n. In other words, each (Ea, )i is a support set for
Kn
the effect law   [a] in T. To make a parallel with the terminology usually adopted in
the belief change community, we shall see each (Ea, )i as a special type of kernel (Hansson,
1994) for the formula   [a].
According to Herzig and Varzinczak (2007), given any action theory one can always
ensure that at least one support set for   [a] exists. Now let
[
Ea =
(Ea, )i
1in

The laws in Ea will serve as guidelines to get rid of [a] in each -world allowed by the
theory T: they are the effect laws to be weakened to allow for hai in some -contexts.
This resembles classical kernel contraction (Hansson, 1994): finding minimal sets implying
a formula and changing them. A crucial difference, however, is that instead of completely
removing a formula from each kernel, what we do here is weaken the laws.
When modifying the support sets, the first thing we must do is to ensure that action
a still has effect  in all those contexts in which  does not hold, if that is the case. This
means we shall weaken the laws in Ea, specializing them to . Now, we need to preserve
all old effects in all -worlds but one. To achieve that we specialize the above laws to
each possible valuation (maximal consistent conjunction of literals) satisfying  but one.
Then, in the left -valuation, we must ensure that action a has either its old effects or 
as outcome. We achieve that by weakening the consequent of the laws in Ea . Finally, in
order to get minimal change, we must ensure that all literals in this -valuation that are
not forced to change in -worlds should be preserved. We do this by stating an effect law
of the form (k  `)  [a](  `), where k is the above -valuation. The reason this is
needed is clear: there can be several -valuations, and as far as we want at most one to
be reachable from the k -world, we should force it to be the one whose difference to this
k -valuation is minimal.
In Situation Calculus terms, all these syntactical operations would correspond to strengthening the right-hand side of the relevant successor state axioms and/or weakening their
212

fiOn Action Theory Change

left-hand side. Alternatively, the same can be done with the original effect axioms, then
recompiling them again into new successor state axioms afterwards.
The output of the operations described above will be a set of action theories which will
be output to the knowledge engineer. Algorithm 2 below gives the operator.
Algorithm 2: Contraction of an Effect Law
Input: T,   [a]
Output: T 
[a] /* set of theories output to the knowledge engineer */
1

begin

2

:= 
T
[a]

3

if T |=K   [a] and S 6|=
 then
CPL
n

foreach   IP(S  ) do

4

forall A  atm() do
V
V
A := pi atm() pi  pi atm() pi /* extend  to a valuation */

5
6

pi A

pi A
/

if S 6|=
(  A )   then
CPL

7

/* it is an allowed state */

0

foreach  IP(S  ) do
T 0 := T \ Ea /* the support sets will be weakened */
T 0 := T 0  {(i  (  A ))  [a]i : i  [a]i  Ea }

8
9
10

11

/* allow for  after a in this state */
T 0 := T 0  {(i    A )  [a](i   0 ) : i  [a]i  Ea }

12

forall L  Lit do
if S |=
(  A ) 
CPL

13

V

`L `)

  then

n

T 0 := T 0  {(  A  `)  [a](  `)}

16

0
:= T 
T
[a]
[a]  {T }

17

else
:= {T}
T
[a]

19

21

and S 6|=
( 0 
CPL

if T 6|=
(  A  `)  [a]` or `   0 then
K

15

20

`L `

foreach `  L do

14

18

V

return T 
[a]
end

Again, from the finiteness of the action theory T and that of atm(), for any  
IP(S  ), and from the decidability of multimodal K (Harel et al., 2000) as well as that
of classical propositional logic, it can be easily verified that Algorithm 2 always terminates.
213

fiVarzinczak

Therefore, contracting effect laws is decidable. Of course, the complexity of computing all
the support sets as well as the prime implicants is quite high (see Section 5.4 later on for a
discussion on this matter).
For an example of execution of Algorithm 2, let us suppose that we want to contract
the effect law token  [buy]hot from the action theory T of our running example. First we
have to compute the support sets for token  [buy]hot in T (i.e., the minimal subsets of
Ebuy which together with S entail token  [buy]hot). These are the following:
token,hot
(Ebuy
)1

token,hot
(Ebuy
)2



coffee  [buy]coffee,
coffee  [buy]coffee





hot  [buy]hot,
coffee  [buy]coffee



=

=

Now for each possible context in which the antecedent token is the case, we have to
token,hot
token,hot

)2 . Since S = {coffee  hot}, such
)1  (Ebuy
= (Ebuy
weaken the effect laws in Ebuy
contexts are token  coffee  hot, token  coffee  hot and token  coffee  hot.

with
For token  coffee  hot: Algorithm 2 replaces in T the laws from Ebuy



 (coffee  (token  coffee  hot))  [buy]coffee, 
(hot  (token  coffee  hot))  [buy]hot,


(coffee  (token  coffee  hot))  [buy]coffee
so that we preserve their effects in all possible contexts but token  coffee  hot. Now, in
order to preserve some effects in token  coffee  hot-contexts while allowing for reachable
hot-worlds, the algorithm adds the laws:


(token  coffee  hot)  [buy](coffee  hot),
(token  coffee  hot)  [buy](hot  coffee)
Now, we search all possible combinations of laws from Ebuy that apply on token  coffee  hot
contexts and find token  [buy]token. Because token must be true after every execution
of action buy, we do not state the law (token  coffee  hot)  [buy](hot  token), and end
up with the following theory:


coffee  hot, token  hbuyi>,








token

[buy]token,
token

[buy],







 (coffee  (token  coffee  hot))  [buy]coffee, 

(hot  (token  coffee  hot))  [buy]hot,
T10 =



 (coffee  (token  coffee  hot))  [buy]coffee, 









(token

coffee

hot)

[buy](coffee

hot),




(token  coffee  hot)  [buy](hot  coffee)
On the other hand, if in our language we also had an atom p with the same theory T,
then we should have added a law (token  coffee  hot  p)  [buy](hot  p) to meet minimal
change by preserving effects that are not relevant to  (cf. Definition 3.4).
214

fiOn Action Theory Change

The execution for contexts token  coffee  hot and token  coffee  hot are analogous
0
0
0
and the algorithm ends with T 
token[buy]hot = {T1 , T2 , T3 }, where:

T20 =

















coffee  hot, token  hbuyi>,
token  [buy]token, token  [buy],
(coffee  (token  coffee  hot))  [buy]coffee,
(hot  (token  coffee  hot))  [buy]hot,
(coffee  (token  coffee  hot))  [buy]coffee,
(token  coffee  hot)  [buy](coffee  hot)


coffee  hot, token  hbuyi>,




token  [buy]token, token  [buy],




 (coffee  (token  coffee  hot))  [buy]coffee,
(hot  (token  coffee  hot))  [buy]hot,
T30 =


(coffee
 (token  coffee  hot))  [buy]coffee,





(token
 coffee  hot)  [buy](hot  coffee),


(token  coffee  hot)  [buy](coffee  hot)




































Looking at Figure 8, we can see the correspondence between these theories and their
respective models. It is now up to the knowledge engineer to look at these action theories
and pick up the one corresponding to her expectations.
5.3 Contracting Static Laws
Finally, in order to contract a static law from a theory, we can use any contraction/erasure
operator 	 for classical logic that is available in the literature. Because contracting static
laws means admitting new possible states (cf. the semantics), just modifying the set S of
static laws may not be enough for the multimodal logic case. However, since in general
we do not necessarily know the behavior of the actions in a new discovered state of the
world, a careful approach is to change the theory so that all action laws remain the same
in the contexts where the contracted law is the case. (The reader is invited to see that in
the Situation Calculus by allowing a new situation to exist one may need to change the
precondition axioms as well, which means that the problem here described is independent
of the logical formalism chosen.)
In our scenario example, if in contracting the static law coffee  hot the knowledge
engineer is not really sure whether action buy is still executable or not, then she should
weaken the set of executability laws specializing them to the context coffee  hot, and
make buy a priori inexecutable in all (coffee  hot)-contexts. It is worth noting that
this is in line with the assumption commonly made in the RAC community according to
which executability laws are by and large much more likely to be incorrect right from the
beginning (Shanahan, 1997). Therefore extrapolating them to previously unknown states
might (and in all probability will) result in the propagation of errors and, even worse, the
loss of effect laws (remember the discussion in Sections 3.3 and 4.2). The operator given in
Algorithm 3 formalizes this.
215

fiVarzinczak

Algorithm 3: Contraction of a Static Law
Input: T, 
Output: T 
 /* set of theories output to the knowledge engineer */
1

begin

2

T
 := 

3

if S |=CPL  then
/* call classical contraction S 	  of S with  */
foreach S   S 	  do

4

/* build a theory preserving executability in old states */
T 0 := ((T \ S )  S  ) \ Xa

5

T 0 := T 0  {(i  )  hai> : i  hai>  Xa }  {  [a]}

0
T
 := T   {T }

6
7
8

else
T
 := {T}

9
10
11

return T 

end

In our running coffee example, contracting the static law coffee  hot from the action
0
0
theory T produces T 
coffeehot = {T1 , T2 }, where

T10 =













T20 =













(token  coffee  hot),
(token  coffee  hot)  hbuyi>,
coffee  [buy]coffee, token  [buy]token,
token  [buy], coffee  [buy]coffee,
hot  [buy]hot, (coffee  hot)  [buy]








(token  coffee  hot),
(token  coffee  hot)  hbuyi>,
coffee  [buy]coffee, token  [buy]token,
token  [buy], coffee  [buy]coffee,
hot  [buy]hot, (coffee  hot)  [buy]




















Observe that the effect laws are not affected at all by the change: as far as we do not
pronounce ourselves about the executability of some action in the new added world, all the
effect laws remain true in it.
If the knowledge engineer is not happy with (coffee  hot)  [buy], she can contract
this formula from the theory using Algorithm 2. Ideally, besides stating that buy is executable in the context coffee  hot, we should want to specify its outcome in this context
as well. For example, we could want (coffee  hot)  hbuyihot to be true in the result.
This requires theory revision. See Section 8 for the semantics of such an operation.
216

fiOn Action Theory Change

5.4 Complexity Issues
While terminating, our algorithms come with a considerable computational cost: the Kn entailment tests with global axioms in the beginning of the algorithms and inside the loops
are known to be exptime-complete (Harel
V et al., 2000).
V The computation of all possible
contexts allowed by the theory, namely pi atm() pi  pi atm() pi , for all A  atm() and
pi A

pi A
/

all   IP(S  ), is clearly exponential. Moreover, the computation of prime implicants
IP(.) might result in exponential growth (Marquis, 2000).
Given that theory change can be carried out offline, from the perspective of the knowledge engineer what is more important is the complexity of the size of the computed contracted theories: the number of formulas as well as the length of the modified ones. This
plays an important role when deciding among several output theories which one corresponds
to the knowledge engineers expectations. In that matter, whereas the length of new added
formulas may increase exponentially, with respect to the number of laws our results are
positive: the size of the computed contracted theories is linear in the size of the original
action theory. (Remember that card(X) denotes the number of elements in set X.)
Proposition 5.1 Let T be an action theory,   hai> an executability law, and T 0 
0
T
hai> . Then card(T ) = card(T).
0
Proof: If T 6|=
  hai>, then T 
hai> = {T}, and then T = T, from which the result
Kn
0
follows. Suppose T |=
  hai> is the case. Then T is such that T 0 = (T \ Xa )  Xa 0 ,
Kn
where Xa 0 is obtained from Xa in such a way that (i  0 )  hai>  Xa 0 if and only
if i  hai>  Xa , for a fixed 0 . From this it follows card(Xa 0 ) = card(Xa ). Now,
card((T \ Xa )  Xa 0 ) = card(T \ Xa )+card(Xa 0 )card((T \ Xa )  Xa 0 ) = card(T)card(Xa )+
card(Xa 0 )  card() = card(T)  card(Xa ) + card(Xa )  0 = card(T).
2

Proposition 5.2 Let T be an action theory,   [a] an effect law, and T 0  T 
[a] .
Then card(T 0 )  card(T) + card(Ea ) + card(Lit).
0
Proof: If T 6|=
  [a], then T 
[a] = {T}, and then T = T, from what we get
Kn
card(T 0 ) = card(T). Since card(T)  card(T) + card(Ea ) + card(Lit), the result follows.
Suppose that T |=K   [a] is the case. Then T 0 = (T \ Ea )  Ea 0  Ea 00  Fa , where:
n

0

00

 Ea and Ea are both obtained from Ea in such a way that (i  0 )  [a]i  Ea 0
and (i  0 )  [a](i   0 )  Ea 00 if and only if i  [a]i  Ea , for fixed 0 ,  0 ;
 Fa  {(0  `)  [a](  `) : `  Lit}, for a fixed 0 ; and
 T, Ea 0 , Ea 00 , Fa are pairwise disjoint.
Hence card(Ea 0 ) = card(Ea 00 ) = card(Ea ), and card(Fa )  card(Lit). Then card(T 0 ) =
card(T \ Ea ) + card(Ea 0 ) + card(Ea 00 ) + card(Fa ) = card(T \ Ea ) + card(Ea ) + card(Ea ) +
card(Fa ) = card(T)  card(Ea ) + card(Ea ) + card(Ea ) + card(Fa ) = card(T) + card(Ea ) +
card(Fa )  card(T) + card(Ea ) + card(Lit).
2
217

fiVarzinczak

Given the arbitrary choice of the contraction operator for static laws, without loss of
generality we can resort to a slightly modified version of it, viz. one that always gives us
as result a set of static laws with the same cardinality as the original S . (This is possible
since, contrary to E and X , a conjunction of static laws is still a static law, with no further
rewriting.) By agreeing on that, the following proposition is straightforward:
0
Proposition 5.3 Let T be an action theory,  a static law, and T 0  T 
 . Then card(T ) =
card(T) + 1.

Propositions 5.15.3 are positive results: if the knowledge engineer can deal with the
original action theory, then she will be able to deal with the output of the algorithms.
(Observe that for a given T 0 all the conditional frame axioms added to Fa in the contraction
of an effect law can be factored into a single law, so that the resulting theory has a
cardinality of at most card(T) + card(Ea ) + 1.)
We finish this section by observing that the size of T 
 , the set of resulting contracted
theories, depends solely on the set of static laws plus the law we contract T with:
Proposition 5.4 Let T be an action theory, and let  be a law such that T |=
. Then
K
n

 card(T 
 ) = card(S 	 ), if  is 
 card(T 
 ) = card(val(S  {})), if  is either   hai> or   [a].
Proof: The proof follows straightforwardly from the outermost loops in Algorithms 13. 2

6. Correctness of the Operators
We now address the correctness of our algorithms with respect to our semantics for contraction. Correctness here is understood as completeness and adequacy. Adequacy means that
the algorithms output only theories whose models result from our semantic modifications
of models of the original theory. Conversely, completeness says that every model resulting
from the semantic modifications of models of the original theory is indeed a model of some
theory output by the algorithm.
6.1 Challenges to Completeness and Adequacy
Let the theory T = {p1  hai>, (p1  p2 )  [a], [a]p2 } and consider its model M
depicted in Figure 13. (Notice that T |=
(p1  p2 ).) When contracting p1  [a]p2 in M ,
Kn
0
we get M in Figure 13.
0
Now contracting p1  [a]p2 from T using Algorithm 2 gives T 
p [a]p = {T }, where
1

2



 p1  hai>, (p1  p2 )  [a], 
(p1  p2 )  [a](p2  p2 ),
T0=


(p1  p2 )  [a](p2  p1 )
Notice that the formula (p1  p2 )  [a](p2  p1 ) is put in T 0 by Algorithm 2 because
there is {p1 }  Lit such that S 6|=
(p  p2 )   and T 6|=K (p1  p2 )  [a]p1 . It is
CPL 1
n

218

fiOn Action Theory Change

a

a

w1

a

p1 , p2

w2

w1

p1 , p2

M :

M0 :
w3

w2
p1 , p2

a

p1 , p2

a
w3

p1 , p2

p1 , p2

Figure 13: A model M of T and the result M 0 of contracting p1  [a]p2 in it.
M0

0
clearly the case that 6|= T 0 and no theory in T 
p1 [a]p2 has M as model. This means that
there are theories for which the contraction operators are not complete.
This issue arises because Algorithm 2 tries to allow a transition from the p1  p2 -world
to a p2 -world that is closest to it, viz. {p1 , p2 }, but has no way of knowing
V that such a
0 
world does not exist. A remedy for that is replacing the test T 6`
(
`L `)   for
Kn
V
0
S 6`CPL (  `L `)  , but that would increase even more the complexity of the algorithm.
A better option would be to have S complete enough to allow the algorithm to determine
the worlds to which a new transition could exist.

The other way round, it does not hold in general that the models of each T 0  T 
 result
from the semantic contraction of models of T by . To see why suppose that there is only
one atom p and one action a, and consider the action theory T = {p  [a], hai>}. The
only model of T is M = h{{p}}, {({p}, {p})}i in Figure 14.
a

a

w2

w1

w1

M :

p

M0 :

p

p

Figure 14: Inadequacy of contraction: a model M of T and a model M 0 of the theory
resulting from contracting p  hai> from T.
From our definitions, contract(M , p  hai>) = {M }. (There is no p-world in M from
0
which to remove an arrow.) On the other hand, T 
phai> is the singleton {T } such that
T 0 = {p  [a], p  hai>}. Then M 0 = h{{p}, {p}}, ({p}, {p})i in Figure 14 is a
model of the resulting contracted theory. Clearly, M 0 does not result from the semantic
contraction of p  hai> from M : while p is valid in the contraction of the models of T, it
is not valid in the models of T 0 . This means that there are theories for which the operators
are not adequate.
This problem occurs because, in our example, the worlds that are forbidden by T, e.g.
{p}, are not preserved as such in T 0 . When contracting an executability or an effect law,
we are not supposed to change the possible worlds of a theory (cf. Section 3).
Fortunately correctness of the algorithms with respect to our semantics can be established for those action theories whose S is maximal, i.e., the set of static laws in S alone
219

fiVarzinczak

characterize what worlds are possible in the models of the theory. This is the principle of
modularity (Herzig & Varzinczak, 2005b) and we briefly review it in the next section.
6.2 Modular Action Theories
A quite useful, albeit simple, property of domain descriptions in reasoning about actions is
that of action theory modularity (Herzig & Varzinczak, 2005b).
Definition 6.1 (Modularity) An action theory T is modular if and only if for every
Boolean formula   Fml, if T |=
, then S |=CPL .
K
n

For an example of a non-modular theory, let us suppose that the action theory T of our
coffee machine scenario were stated as


coffee

hot,
hbuyi>,






coffee  [buy]coffee,
T=

token  [buy]token, token  [buy], 




coffee  [buy]coffee, hot  [buy]hot
The modified law is underlined: we have (in this case wrongly) stated that the agent can
always buy at the machine. Then T |=K token, but S 6|=
token.
CPL
n

Since the underlying multimodal logic is independently axiomatized (see Section 2.1), we
can use the algorithms given by Herzig and Varzinczak (2005b) to check whether an action
theory satisfies the principle of modularity. Whenever this is not the case, the algorithms
return the Boolean formulas entailed by the theory which are not consequences of S alone.
For the theory T above, they would return {token}: as we have stated hbuyi>, from this and
the inexecutability law token  [buy] we have that T |=
token. Because S 6|=
token,
Kn
CPL
token is what is called an implicit static law (Herzig & Varzinczak, 2004) of the action
theory T.5
Modular action theories have several interesting computational properties. For example, consistency can be checked by just checking consistency of the static laws in S : if
T is modular, then T |=
 if and only if S |=
. Deduction of effect laws does not
Kn
CPL
need the executability ones and vice versa. Deduction of an effect of a sequence of actions a1 ; . . . ; an (prediction) does not need to take into account the effect laws for actions
other than a1 , . . . , an . This applies in particular to plan validation when deciding whether
ha1 ; . . . ; an i is the case.
Modularity is not an exclusive property of action theories formalized in Kn : similar
notions have also been investigated for different contexts in other formalisms, like regulation consistency in deontic logic (Cholvy, 1999), Situation Calculus (Herzig & Varzinczak,
2005a), DL ontologies (Herzig & Varzinczak, 2006), dynamic logic (Zhang, Chopra, & Foo,
2002) and also in the Fluent Calculus (Thielscher, 2010). For more details on modularity
in Kn action theories, as well as its role in the presence of a solution to the frame and
ramification problems, see the work by Varzinczak (2006).
5. Implicit static laws are closely related to veridical paradoxes (Quine, 1962). It turns out that sometimes
they are intuitive, but sometimes they are not. For a deep discussion on implicit static laws, see the
work by Varzinczak (2006).

220

fiOn Action Theory Change

Another interesting property of modular action theories is the following:
Theorem 6.1 T is modular if and only if T has a canonical model.
Proof: Let Mcan = hWcan , Rcan i be the canonical frame of T.
Mcan

Mcan

(): By definition, Mcan is such that |= S  E . It remains to show that |= X . Let
Mcan
i  hai>  Xa , and let w  Wcan be such that |=w i . Therefore for all j  Fml such
Mcan

that T |=K j  [a], we must have 6|=
w
n

Mcan

S |=
(i  j ), and hence |=
CPL

Mcan

some w0  Wcan such that |= 0
Mcan

and |=

w

j , because T |=
(i  j ), and as T is modular,
K
n

(i  j ). Then by the construction of Mcan , there is
Mcan

 for all   [a]  Ea such that |=
w

. Thus Ra (w) 6= 

i  hai>.

(): Suppose T is not modular. Then there must be some   Fml such that T |=
 and
Kn
S 6|=
. This means that there is v  val(S ) such that v 6 . As v  Wcan (because Wcan
CPL
contains all possible valuations of S ), Mcan is not a model of T.
2
6.3 Correctness Under Modularity
As shown by Herzig and Varzinczak (2007), given an action theory formalized with any
framework available in the literature allowing for the expression of our three basic types of
laws, it is always possible to ensure modularity. Moreover, as we are going to see in the
sequel (cf. Section 7.2), it has to be computed at most once during the evolution of the
action theory. Hence, relying on modular theories is not a limitation at all to our approach.
The following theorem establishes that under the assumption that the action theory T
is modular, the semantic contraction of a formula  from the set of models of T produces
models of some contracted theory in T 
.
M

Theorem 6.2 Let T be modular, and  be a law. For all M0  M
 such that |= T for
M0

0
0
0
every M  M, there is T 0  T 
 such that |= T for every M  M .

2

Proof: See Appendix A.

The next theorem establishes the other way round: under modularity models of theories
in T 
 are all models of the semantic contraction of  from models of T.
M0

0
0
Theorem 6.3 Let T be modular,  a law, and T 0  T 
 . For all M such that |= T , there
M

0
0
is M0  M
 such that M  M and |= T for every M  M.

2

Proof: See Appendix B.
With these two theorems we get the correctness of our operators:
M0

0
Corollary 6.1 Let T be modular,  a law, and T 0  T 
 if and only if |= 
 . Then T |=
K
M

n

for every M 0  M0 such that M0  M
 for some M such that |= T for all M  M.
221

fiVarzinczak

Proof:
M0
0
0
(): Let M 0 be such that |= T 0 . By Theorem 6.3, there is M0  M
 such that M  M
M

M0

for some M such that |= T for all M  M. From this and T 0 |=
 , we have |=  .
K
n

(): Suppose T 0 6|=K  . (We show that there is some model M 0  M0 such that M0  M

n

M

M0

for some M with |= T for all M  M, and 6|=  .)
Given that T is modular, by Lemma B.1 T 0 is modular, too. Then, by Lemma B.3,
M0
M0
there is some M 0 = hval(S 0 ), R0 i such that 6|=  . Clearly |= T 0 , and from Lemma B.4 the
result follows.
2

7. Assessment of Postulates for Change
Do our action theory change operators satisfy the classical postulates for change? Before
answering this question, one should ask: do our operators behave like revision or update
operators? We here address this issue and then show which postulates for theory change
are satisfied by our definitions.
7.1 Contraction or Erasure?
The distinction between revision/contraction and update/erasure for classical theories is
historically controversial in the literature. The same is true for the case of modal theories
describing actions and their effects. We here rephrase Katsuno and Mendelzons definitions (1992) in our terms so that we can see to which one our method is closer.
In Katsuno and Mendelzons view, contracting a law  from an action theory T intuitively means that the description of the possible behavior of the dynamic world T must be
adjusted to the possibility of  being false. This amounts to selecting from the models of
 those that are closest to models of T and allow them as models of the result.
In contrast, update methods select, for each model M of T, the set of models of  that
are closest to M . Erasing  from T means adding models to T; for each model M , we add
all those models closest to M in which  is false. Hence, from our constructions so far it
seems that our operators are closer to update than to revision.
Moreover, according to Katsuno and Mendelzons view (1992), our change operators
would also be classified as update because we make modifications in each model independently, i.e., without changing other models.6 Besides that, in our setting a different
ordering on the resulting models is induced by each model of the theory T (see Definitions 3.3, 3.7 and 3.10), which according to Katsuno and Mendelzon is a typical property
of an update/erasure method.
Nevertheless, things get quite different when it comes to the postulates for theory change.
7.2 The Postulates
In this section we analyze the behavior of our action theory change operators with respect
to AGM-like postulates. Here we follow Katsuno and Mendelzons presentation of the
6. Even if when contracting an effect law from one particular model we need to check the other models of
the theory, those are not modified.

222

fiOn Action Theory Change

postulates to assess both contraction and erasure. Let T = S  E  X denote an action
theory and  denote a law.
T 0 , for all T 0  T 
Monotonicity Postulate: T |=
.
K
n

This postulate is our version of Katsuno and Mendelzons (C1) and (E1) postulates
for contraction and erasure, respectively, and it is satisfied by our change operators. The
proof is in Lemma A.1. Such a postulate is not satisfied by the operators proposed by
Herzig et al. (2006): there when removing e.g. an executability law   hai> one may
make   [a] valid in all models of the resulting theory.
Preservation Postulate: If T 6|=
, then |=
T  T 0 , for all T 0  T 
.
K
K
n

n

This is Katsuno and Mendelzons (C2) postulate. Our operators satisfy it as far as
whenever T 6|=
, then the models of the resulting theory are exactly the models of T,
Kn
because these are the minimal models falsifying .
The corresponding version of Katsuno and Mendelzons (E2) postulate about erasure,
i.e., if T |=
, then |=K T  T 0 , for all T 0  T 
 , is clearly also satisfied by our operators
Kn
n
as a special case of the postulate above. Satisfaction of (C2) indicates that our operators
are closer to contraction than to erasure.
If T 6|=K  and 6|=K , then T 0 6|=K , for all T 0  T 
.

Success Postulate:

n

n

n

This postulate is our version of Katsuno and Mendelzons (C3) and (E3) postulates. If
 is a propositional   Fml, our operators satisfy it, as long as the classical propositional
change operator satisfies it as well. For the general case, however, as stated the postulate is
not always satisfied. This is shown by the following example: let T = {p, hai>, p  [a]}.
Note that T is modular and consistent. Now, contracting the (contingent) formula p  hai>
from T gives us T 0 = T. Clearly T 0 |=K p  hai>. This happens because, despite not being a
n
tautology, p  hai> is a trivial formula with respect to T: since p is valid in all T-models,
p  hai> is trivially true in these models (cf. end of Section 3.1).
Fortunately, for all those formulas that are non-trivial consequences of the theory, our
operators guarantee success of contraction:
Theorem 7.1 Let T be consistent, and  be an executability or an effect law such that
S 6|=
. If T is modular, then T 0 6|=K  for every T 0  T 
.
K
n

n

0
Proof: Let us suppose that there is some T 0  T 
. Since T is modular,
 such that T |=
K
n

M0

Corollary 6.1 tells us that |=  for every M 0  M0 such that M0  M
 , where M = {M :
M

|= T and M = hval(S ), Ri}.
M0

M 00

If |=  for every M 0  M0 , then even for M 00  M0 \M we have |= . But M 00  M
M 00

for some M  M, and by definition 6|= . Hence M = , and then the truth of  in M
does not depend on the accessibility relation Ra . Hence, whether  has the form   hai>
or   [a], for ,   Fml, this holds only if S |=CPL  (see Definitions 3.1 and 3.5), and
therefore we get S |=K .
2
n

223

fiVarzinczak

Equivalences Postulate: If |=K T1  T2 and |=
1  2 , then |=K T10  T20 , for some
K

0
T10  (T1 )
2 and T2  (T2 )1 .

n

n

n

This postulate corresponds to Katsuno and Mendelzons (C4) and (E4) postulates. It
is worth noting that equivalence here is considered always modulo action laws, i.e., the
formulas are assumed to be either static laws, effect laws or executability laws, as well as
their equivalents. Moreover we remember that the theories here must be action theories,
i.e., sets of action laws of our three basic types. Under modularity and the assumption that
the propositional change operator satisfies (C4)/(E4), our operations satisfy this postulate:
T1  T2 and |=K 1  2 , then for each
Theorem 7.2 Let T1 and T2 be modular. If |=
K
n

n


0
T 0  T20 , and vice-versa.
T10  (T1 )
2 there is T2  (T2 )1 such that |=
K 1
n

Proof: The proof follows straight from our results: since |=
T  T2 and |=K 1  2 , they
K 1
n

M

M

n

have pairwise the same models. Hence, given M such that |= T1 and |= T2 , the semantic
contraction of 1 and that of 2 from M have the same operations on M . As T1 and T2
are modular, Corollary 6.1 guarantees we get the same syntactical results. Moreover, as the
classical operator 	 satisfies (C4)/(E4), if follows that |=K T10  T20 .
2
n

Recovery Postulate: T 0  {} |=
T, for all T 0  T 
.
K
n

This is the action theory counterpart of Katsuno and Mendelzons (C5) and (E5) postulates. Again we rely on modularity in order to satisfy it.
Theorem 7.3 Let T be modular. T 0  {} |=
T, for all T 0  T 
.
K
n

Proof: If T 6|=K , because our operators satisfy the preservation postulate, T 0 = T, and
n
then the result follows by monotonicity.
Let T |=
, and let M0 denote the set of all models of T 0 . As T is modular, by CorolK
n

M0

M0

lary 6.1 every M 0  M0 is such that either |= T (and then |= ) or M 0  contract(M , )
M
(and then M 0  M ) for some M such that |= T.
Let M00 denote the set of all models of T 0  {}. Clearly M00  M0 , by monotonicity.
M 00

Moreover, every M 00  M00 is such that |= , hence M 00 
/ M for every M such that
M

|= T, and then M 00 
/ contract(M , ), for any M model of T. Thus M 00 is a model of T
0
and then T  {} |=K T.
2
n
W

0
Let T 
 denote the disjunction of all T in T  .
W
W
Disjunctive Rule: (T1  T2 )
(T1 )
(T2 )
 is equivalent to
 
.
This is our version of (E8) erasure postulate by Katsuno and Mendelzon. Clearly our
syntactical operators do not manage to contract a law from a disjunction of theories: T1 T2
is not an action theory and cannot in general be rewritten as one. Nevertheless, by proving
that it holds in the semantics, from the correctness of our operators, we get an equivalent
operation. Again the fact that the theories under concern are modular gives us the result.
224

fiOn Action Theory Change

Theorem 7.4 Let T1 and T2 be modular, and  be a law. Then
_
_
_
(T2 )
(T1 )
|=K
(T1  T2 )
)
 
 (
n

Proof:
W
M 0W
M 0W
M 0W

(T2 )
(T1 )
(): Let M 0 be such that |=
(T1 )
 . Suppose
 or |=
  (T2 ) . Then |=
0
M0
M W


|=
(T1 ) (the other case is analogous). Then there is (T1 )0  (T1 ) such that |= (T1 )0 .
0
0
Then by Corollary 6.1, there is M0  M
 such that M  M , for M a set of models of
0
T1 . Then M is a model resulting from contracting  from models of T1 , and then M 0 also
results from contracting  in models of T1  T2 , viz. those models of T1 . Then by CorolM0
M 0W
0 , and then |=
such
that
|=
(T

T
)
(T1  T2 )
lary 6.1, there is (T1  T2 )0  (T1  T2 )
1
2
.

M 0W

(): Let M 0 be such that |=
M0


0
(T1  T2 )
 . Then there is (T1  T2 )  (T1  T2 ) such that

0
0
|= (T1  T2 )0 . By Corollary 6.1, there is M0  M
 such that M  M , for M a set of
0
models of T1  T2 . Then M is a model resulting from contracting  from models of T1  T2 .
Hence M 0 results from contracting  from models of T1 or from models of T2 . Suppose the
former is the case (the second is analogous). Then by Corollary 6.1 there is (T1 )0  (T1 )

M0
M 0W
such that |= (T1 )0 , and then |=
(T1 )
.
2


We have thus shown that our constructions satisfy the (E8) postulate. Nevertheless,
as far as we see, it is not immediate whether it is really expected here. This supports our
position that our operators behavior is closer to contraction than to erasure.
As we have seen from the results above, modularity is a sufficient condition for the
satisfaction of the AGM-like postulates for action theory contraction. To finish up we state
a new postulate:
Preservation of Modularity: If T is modular, then every T 0  T 
 is modular.
Changing a modular theory should not make it non-modular. This is not a standard
postulate, but we think that since it is a good property modularity should be preserved
across changing an action theory. If so, this means that whether a theory is modular or
not can be checked once for all and one does not need to care about it during the future
evolution of the action theory, i.e., when other changes will be made on it. Our operators
satisfy this postulate and the proof is given in Appendix B.
Now one may naturally asks whether we can get a characterization result in the traditional AGM sense, i.e., whether any contraction operator satisfying all our versions of
the postulates is one of our three contraction operations. Unfortunately, good sense points
towards a negative answer: there might well be an operator satisfying all the above postulates that, by not complying with all the assumptions in the RAC community (Shanahan,
1997), is not necessarily one of the operators defined in Section 3 (cf. the discussion on
general formula contraction in Section 10). To witness, consider for example an operator
that also modifies worlds when contracting effect laws. This supports one of the contentions
of the present work, viz. that classical belief change cannot be fully transposed to action
theories and expected to give exactly the same kind of outcome. Similar negative results
have also been found for revision in DL ontologies (Flouris, Plexousakis, & Antoniou, 2004)
and contraction of Horn theories (Booth, Meyer, & Varzinczak, 2009).
225

fiVarzinczak

8. A Semantics for Action Theory Revision
So far we have analyzed the case of contraction: the knowledge engineer realizes that the
theory is too strong and therefore it has to be weakened. Let us now take a look at the
other way round, i.e., the theory is (possibly) too liberal and the agent discovers new laws
about the world that should be added to her beliefs, which amounts to strengthening them.
Suppose that the action theory of our scenario example were initially stated as follows:


coffee  hot, token  hbuyi>,


coffee  [buy]coffee, token  [buy],
T=


coffee  [buy]coffee, hot  [buy]hot
Then the canonical model of theory T is as shown in Figure 15.
w1
t, c, h
b

b

w2

M :

b

w3

b

t, c, h

t, c, h

b
b

w5
t, c, h

w6

w4
t, c, h

t, c, h

Figure 15: Canonical model of the new initial action domain description.
Looking at model M in Figure 15 we can see that, for example, the agent does not know
that she loses her token every time she buys coffee at the machine. This is a new law that
she should incorporate to her knowledge base at some stage of her action theory evolution.
Contrary to contraction, where we want the negation of some law to become satisfiable,
in revision we want to make a new law valid. This means that one has to eliminate all cases
satisfying its negation. This depicts the duality between revision and contraction: whereas
in the latter one invalidates a formula by making its negation satisfiable, in the former one
makes a formula valid by forcing its negation to be unsatisfiable prior to adding the new
law to the theory.
The idea behind our semantics for revision is as follows: we initially have a set of models
M in which a given formula  is (potentially) not valid, i.e.,  is (possibly) not true in
every model in M. In the result we want to have only models of . Adding -models to
M is of no help. Moreover, adding models makes us lose laws: the corresponding resulting
theory would be more liberal.
One solution amounts to deleting from M those models that are not -models. Of
course removing only some of them does not solve the problem, we must delete every such a
model. By doing that, all resulting models will be models of . (This corresponds to theory
expansion, when the resulting theory is satisfiable.) However, if M contains no model of
, we will end up with . Consequence: the resulting theory is inconsistent. (This is the
main revision problem.) In this case the solution is to substitute each model M in M by
226

fiOn Action Theory Change

its nearest modification M? that makes  true. This lets us to keep as close as possible to
the original models we had. But, what if for one model in M there are several minimal
(incomparable) modifications of it validating ? In that case we shall consider all of them.
The result will also be a list of models M? , all being models of .
Before defining the revision of sets of models, we present what modifications of (individual) models are.
8.1 Revising a Model by a Static Law
Suppose that our coffee deliverer agent discovers that the only hot drink that is served on
the machine is coffee. In this case, she might want to revise her beliefs with the new static
law coffee  hot: she cannot hold a hot drink that is not a coffee.
Considering the model depicted in Figure 15, one can see that the Boolean formula
coffee  hot is satisfiable (there is a world of the model in which it holds). Since we do
not want this to be the case, the first step is to remove all worlds in which coffee  hot is
true. The second step is to guarantee that all the remaining worlds (if any) satisfy the new
static law. Such an issue has been largely addressed in the literature on propositional belief
base revision and update (Gardenfors, 1988; Winslett, 1988; Katsuno & Mendelzon, 1992;
Herzig & Rifi, 1999). Here we can achieve that with a semantics similar to that of classical
revision operators: basically one can change the set of possible valuations, by removing or
adding worlds.
In our example, removing the possible worlds {t, c, h} and {t, c, h} would do the job
(there is no need to add new valuations since the new incoming law is already satisfied in at
least one world of the original model, and therefore the resulting set of worlds is non-empty).
The delicate point in removing worlds is that this may have as consequence the loss of
some executability laws: in the example, if there were some transition from some world w
to say {t, c, h}, then removing the latter from the model would make the action under
concern no longer executable in w, if it was the only transition labeled by that action leaving
it. From a semantic point of view, this is intuitive: if the state of the world to which we
could move is no longer possible, then we do not have a transition to that state anymore.
Therefore, if that transition was the only one we had, it is natural to lose it.
Similarly, one could ask what to do with the accessibility relation if new worlds are
added, i.e., when expansion is not possible. Following the discussion in Section 3.3, we here
prefer not to add new transitions systematically to the accessibility relation. Hence we shall
postpone correction of executability laws, if needed. This approach may be debatable, but
with the information we have at hand, this is the safest way of changing static laws. (See
also the discussion in Sections 3.3 and 4.2.)
The semantics for revision of one model by a static law is as follows:
Definition 8.1 Let M = hW, Ri. M 0 = hW0 , R0 i  M? if and only if:
 W0 = (W \ val())  W , where W  val(); and
 R0  R.
227

fiVarzinczak

M0

Clearly unless  |=
, we have that |=  for each M 0  M? . The minimal models
CPL
resulting from revising a model M by  are those closest to M with respect to M :
Definition 8.2 Let M be a model and  a static law. revise(M , ) =

S

min{M? , M }.

In the example of model M in Figure 15, revise(M , coffee  hot) is the singleton
{M 0 }, where M 0 is as shown in Figure 16.
w1
t, c, h
b
w2

M0 :

b

t, c, h
b
b

w5
t, c, h

w4
t, c, h

Figure 16: Model resulting from revising the model M in Figure 15 with coffee  hot.

8.2 Revising a Model by an Effect Law
Let us suppose now that our agent eventually discovers that after buying coffee she does
not keep her token anymore. (That was a design mistake that the agent still possesses a
token even after ordering a coffee at the machine). This means that her theory should now
be revised in such a way that the new effect law token  [buy]token holds. Looking at
model M in Figure 15, this amounts to guaranteeing that the formula token  hbuyitoken
is satisfiable in none of its worlds. To do that, we have to look at all the worlds satisfying
this formula (if any) and either (i) make token false in each of these worlds; or (ii) make
hbuyitoken false in all of them. If we chose the first option, we will essentially flip the truth
value of literal token in the respective worlds, which changes the set of valuations of the
model. If we chose the latter, we will basically remove buy-arrows leading to token-worlds.
In that case, a change in the accessibility relation will be made.
In our example, we have that the possible worlds {token, coffee, hot}, {token, coffee, hot}
and {token, coffee, hot} satisfy token  hbuyitoken and all they have to change.
Flipping token in all these worlds to token would do the job, but would also have
as consequence the introduction of a new static law: token would now be valid, i.e., the
agent never has a token! Another issue with this approach is that by making token true
everywhere, the new incoming law token  [buy]token will be trivially true in the resulting
model, which does not mean that there is an execution of action buy from a token-world
to a token one. This defeats the purpose of changing the action theory on the basis that
it has been observed that every execution of the action under consideration should lead to
token-contexts.
One of our contentions in the present work is that changing action laws should never
have as a side effect a change in the static laws (cf. Sections 3 and 4). Given their special status (Shanahan, 1997), these should change only if explicitly required. In this case, each world
228

fiOn Action Theory Change

satisfying token  hbuyitoken has to be changed so that hbuyitoken is no longer true in it.
In our example, we should remove the transitions ({token, coffee, hot}, {token, coffee, hot}),
({token, coffee, hot}, {token, coffee, hot}) and ({token, coffee, hot}, {token, coffee, hot}).
The semantics of one model revision for the case of a new effect law is:
?
Definition 8.3 Let M = hW, Ri. M 0 = hW0 , R0 i  M[a]
if and only if:

 W0 = W;
 R0  R;
M

 If (w, w0 )  R \ R0 , then |=w ; and
M0

 |=   [a].
The minimal models resulting from the revision of a model M by a new effect law are
those that are closest to M with respect to our order on the models M :
Definition 8.4 Let M be a model and   [a] an effect law. revise(M ,   [a]) =
S
?
min{M[a]
, M }.
Taking once again M as shown in Figure 15, revise(M , token  [buy]token) will be
the singleton {M 0 } (Figure 17).
w1
t, c, h
b

b
w3

w2

M0 :

t, c, h

t, c, h
b

w5

w6

w4
t, c, h

t, c, h

t, c, h

Figure 17: Model resulting from revising the model M in Figure 15 with the new effect law
token  [buy]token.

8.3 Revising a Model by an Executability Law
Let us now suppose that in some stage it has been decided to grant free coffee to everybody.
Faced with this information, the agent will now revise her laws to reflect the fact that buy
can also be executed in token-contexts: token  hbuyi> is a new executability law (and
therefore we will have hbuyi> in all new models of the agents beliefs).
Considering again the model in Figure 15, we observe that (token  hbuyi>) is
satisfiable in M . This means that we must throw token  [buy] away to ensure that the
new formula becomes true in the new model, i.e., satisfied by all of its worlds.
229

fiVarzinczak

To remove token  [buy] we have to look at all worlds satisfying it and modify M
so that they no longer satisfy that formula. Given worlds {token, coffee, hot} and
{token, coffee, hot}, we have two options: change the interpretation of token or add new
transitions leaving these worlds. A question that arises is what choice is more drastic:
change a world or a transition ? Again, here we think that changing the worlds content
(the valuation) is more drastic, as the existence of such a world was foreseen by some static
law and is hence assumed to be as it is, unless we have enough information supporting
the contrary, in which case we explicitly change the static laws (see above). Moreover,
changing the truth value of token in these worlds would trivialize the new incoming law
token  hbuyi> in the new model, defeating the purpose of guaranteeing the existence of
a buy-transition from a token-context. Therefore we shall add a new buy-arrow from each
of {token, coffee, hot} and {token, coffee, hot}.
Having agreed on that, the issue now is: which worlds should the new transitions be
directed to? Recalling the reasoning developed in Section 3.2, in order to comply with
minimal change, the new transitions shall be directed to worlds that are relevant targets
of each of the token-worlds in question. In our example, {token, coffee, hot} is the only
relevant target world here: the two other token-worlds violate the effect coffee of buy, while
the three token-worlds would make us violate the frame axiom token  [buy]token.
The semantics for one model revision by a new executability law is as follows:
?
Definition 8.5 Let M = hW, Ri. M 0 = hW0 , R0 i  Mhai>
if and only if:

 W0 = W;
 R  R0 ;
 If (w, w0 )  R0 \ R, then w0  RelTarget(w,   [a], M , M); and
M0

 |=   hai>.
The minimal models resulting from revising a model M by a new executability law are
those closest to M with respect to M :
Definition
M be a model and   hai> be an executability law. revise(M ,  
S 8.6 Let
?
hai>) = min{Mhai>
, M }.
In our running example, revise(M , token  hbuyi>) is the singleton {M 0 }, where M 0
is as depicted in Figure 18.
In this example, observe that because we have a single relevant target world we get a
single model in the result of revision.
8.4 Revising Sets of Models
Up until now we have seen what the revision of single models means. This is needed when
expansion by the new law is not possible due to inconsistency. We here give a unified
definition of revision of a set of models M by a new law :
230

fiOn Action Theory Change

b
w1
t, c, h
b

b

b

w2

M0 :

b

t, c, h
b

t, c, h

t, c, h

b
b

w5

b
w3

w6

w4
t, c, h

t, c, h

Figure 18: Result of revising M in Figure 15 by the new executability law token  hbuyi>.

Definition 8.7 Let M be a set of models and  be a law. Then
(
M
M
M \ {M : 6|= }, if there is M  M such that |= ;
?
M =
S
M M revise(M , ), otherwise.
Observe that Definition 8.7 comprises both expansion and revision: in the first one, simple
addition of the new law gives a satisfiable theory; in the latter a deeper change is needed
to get rid of inconsistency.

9. Related Work
To the best of our knowledge, the first work on updating an action domain description is
that by Li and Pereira (1996) in a narrative-based action description language (Gelfond &
Lifschitz, 1993). Contrary to us, however, they mainly investigate the problem of updating
the narrative with new observed facts and (possibly) with occurrences of actions that explain
those facts. This amounts to updating a given state/configuration of the world (in our terms,
what is true in a possible world) and focusing on the models of the narrative in which some
actions took place (in our terms, the models of the action theory with a particular sequence
of action executions). Clearly the models of the action laws remain the same.
Baral and Lobo (1997) introduce extensions of action languages that allow for some
causal laws to be stated as defeasible. Their work is similar to ours in that they also allow
for weakening of laws: in their setting, effect propositions can be replaced by what they call
defeasible (weakened versions of) effect propositions. Our approach is different from theirs
in the way executability laws are dealt with. Here executability laws are explicit and we
are also able to contract them. This feature is important when the qualification problem is
considered: we may always discover contexts that preclude the execution of a given action
(cf. the Introduction).
Liberatore (2000) proposes a framework for reasoning about actions in which it is possible to express a given semantics of belief update, like Winsletts (1988) and Katsuno and
Mendelzons (1992). This means it is the formalism, essentially an action description lan231

fiVarzinczak

guage, that is used to describe updates (the change of propositions from one state of the
world to another) by expressing them as laws in the action theory.
The main difference between Liberatores work (2000) and Li and Pereiras (1996) is
that, despite not being concerned, at least a priori, with changing action laws, Liberatores
framework allows for abductively introducing in the action theory new effect propositions
(effect laws, in our terms) that consistently explain the occurrence of an event.
The work by Eiter et al. (2005) is similar to ours in that they also propose a framework
which is oriented to updating action laws. They mainly investigate the case where e.g.
a new effect law is added to the description (and then has to be true in all models of the
modified theory). This problem is the dual of contraction and is then closer to our definition
of revision (cf. Section 8).
In Eiter et al.s framework (2005), action theories are described in a variant of a narrativebased action description language. Like in the present work, the semantics is also in terms
of transition systems, with transitions (action occurrences) linking states (configurations
of the world). Contrary to us, however, the minimality condition on the outcome of the
update is in terms of inclusion of sets of laws, which means that the approach is more syntax
oriented to some extent.
In their setting, during an update an action theory T is seen as composed of two pieces,
Tu and Tm , where Tu stands for the part of T that is not supposed to change and Tm contains
the laws which may be modified. In our terms, when contracting a static law we would
have Tm = S  Xa ; when contracting an executability law Tm = Xa ; and when contracting
effects laws Tm = Ea . The difference here is that in our approach it is always clear what
laws should not change in a given type of contraction, and therefore Tu and Tm do not need
to be explicitly specified prior to the update.
Their approach and ours can both be described as constraint-based update, in that the
theory change is carried out relative to some constraints (a set of laws that we want to hold
in the result). In our framework, for example, all changes in the action laws are relative to
the set of static laws S (and that is why we concentrate on models of T having val(S ) as
worlds). When changing a law, we want to keep the same set of states. The difference with
respect to Eiter et al.s (2005) approach is that there it is also possible to update a theory
relatively to e.g. executability laws: when expanding T with a new effect law, one may want
to constrain the change so that the action under concern is guaranteed to be executable in
the result.7 As shown in the referred work, this may require the withdrawal of some static
law. Hence, in Eiter et al.s framework, static laws do not have the same status as in ours.
Herzig et al. (2006) define a method for action theory contraction that, despite the
similarity with the current work and the common underlying motivations, is more limited
than the present constructions.
First, with the referred approach we do not get minimal change. For example, in the
referred work the operator for contracting executability laws is such that in the resulting
theory the modified set of executability laws is given by
Xa = {(i  )  hai> : i  hai>  Xa }
7. We could simulate that in our approach with two successive modifications of T: first adding the effect
law and then an executability law (cf. Section 8).

232

fiOn Action Theory Change

which, according to its semantics, gives theories among whose models are those resulting
from removing transitions from all -worlds. A similar comment can be made with respect
to contraction of effect laws.
Second, Herzig et al.s (2006) contraction method does not satisfy most of the postulates
for action theory change that we have addressed in Section 7. Besides not satisfying the
monotonicity postulate, it does not satisfy the preservation one. To witness, suppose that
we have a language with only one atom p, and the model M depicted in Figure 19.
a
w1

M :

a

a

w2
p

p

a
a

w1

M :
0

a

w2
p

p
a

Figure 19: Counter-example to preservation in the method by Herzig et al. (2006).
M

M

Then |= p  [a]p and 6|= [a]p. Now the contraction operator defined there is such
that when removing [a]p from M yields the model M 0 in Figure 19 such that R0a = WW.
M0

Then 6|= p  [a]p, i.e., the effect law p  [a]p is not preserved.
Finally, another work that is related to ours is that by Zhang and Ding (2008). Like
ours, their approach is also about giving a semantic characterization of the basic operations
for changing Kripke models. Contrary to us however, their focus is on model checking, not
on entailment. Despite the definition and use of operations that in essence are similar to
ours (modifications of the set of possible worlds or of the accessibility relation), their work
is concerned mainly with modifications of a single model, not with that of sets of models
as we do, and hence it does not provide operations for changing action laws. Because of
that, their approach is not directly comparable to ours, since here we are interested in
entailment-based revision.

10. Concluding Remarks
In this work we have addressed the problem of changing an action domain description for
reasoning about actions, a problem not sufficiently investigated in the literature so far.
We have seen the intuitions behind such a kind of theory modification and have given a
semantics for action theory change in terms of distances between models that captures the
notion of minimal change. We have given algorithms to contract a formula from a theory
that terminate and that are correct with respect to our semantics (Corollary 6.1). We have
shown the importance that our modularity notion has in this result and in others.
We have also extended Varzinczaks investigations (2008) by defining a semantics for
action theory revision based on minimal modifications of models. For the corresponding
revision algorithms, the reader is referred to the work by Varzinczak (2009). One of our
ongoing research topics is on assessing our revision operators behavior with respect to
appropriate versions of the AGM postulates for revision (Alchourron et al., 1985) and its
links with the contraction counterpart.
With our algorithms we provide a set of tools to be used by the knowledge engineer in an
interactive and possibly iterative way to modify an action theory. These tools are guaranteed
233

fiVarzinczak

to perform minimal change when assisting the knowledge engineer in implementing her
desired modifications. They give her a set of options and it is up to the knowledge engineer
to decide which one is more in line with her intuitions.
Given that action theory change is not a single step operation, the knowledge engineer is
expected to make use of the contraction/revision operators to make a series of modifications
that eventually will give a fine-grained theory not entailing the contracted laws and entailing
the new learned laws about the domain.
For the sake of presentation, here we have abstracted from the frame and ramification
problems. However our definitions could have been stated in a formalism with a suitable solution to them, like Castilho et al.s approaches (1999, 2002). With regards to the
qualification problem, this is not ignored here: contracting wrong executability laws is an
approach towards its solution. Indeed, given the difficulty of stating all sufficient conditions
for executability of an action, the knowledge engineer writes down some of them and lets
the theory evolve via subsequent revisions.
A possible criticism to the approach here developed concerns the cautiousness of our
operator for contracting static laws: we prefer to lose some executability laws rather than
induce them and lose effect laws. This behavior could make our operators to be interpreted
as incoherent. We have pointed out nevertheless that this is in line with largely accepted
assumptions in the RAC community, and moreover we have shown the impossibility of a
non-cautious static law contraction operator that complies with all that and is coherent
with the other operators.
Indeed one of the purposes of the present work is to shed some light on the fundamental
differences between belief change in action domain descriptions and in logical theories in
general. Classical belief change cannot be fully transplanted to action theories, and here we
have shown why (cf. Sections 3.2, 4.2, 5.3 and 8.3).
In particular, looking at the postulates of classical belief change (or our versions thereof)
one sees that they are not enough to fully characterize operators for action theory change.
For that to be achieved the fundamental assumptions in reasoning about actions that we
have extensively used throughout this work should somehow be compiled into postulates
supplementing the classical ones. It is not immediately clear what these new postulates
would look like, but this is an interesting thread of investigation worth pursuing.
It might also be argued that our semantic operations do not respect the principle of
categorical matching, given that the input and output are different sorts of objects, viz. a
set of models and a set of sets of models (cf. Definitions 3.3, 3.7 and 3.10). It is easy to see,
however, that our semantic constructions could have been defined in such a way that each
M0  M
 corresponds to the result of one contraction operator. The choice for defining
the result of an operation as a set of possible outputs was driven by the definition of the
algorithms, where a theory (corresponding to a set of models) is given as input and the
output is a set of theories (hence corresponding to a set of set of models).
Although the semantic operators can be redefined to satisfy the principle of categorical matching, the same is not immediate about the algorithms (they would be nondeterministic). Therefore we preferred to keep a balance between the semantic and the
syntactic definitions so that we see more clearly their direct correspondence.
234

fiOn Action Theory Change

One of our contentions here is that sticking to modular theories (and hence to canonical models) is not a big deal: we can use existent algorithms in the literature (Herzig &
Varzinczak, 2007) to ensure that an action theory T is characterized by its canonical models.
We have seen that under modularity, our operators satisfy all the postulates for contraction: Modularity is one of the sufficient conditions for Success in Theorem 7.1. It is
also a sufficient condition in Theorem 7.2, and, as shown in Theorem 7.3, it is a sufficient
condition for Recovery. Finally it is also a sufficient condition for the Disjunctive Rule
to hold, and is shown to be preserved by the contraction operators (cf. last paragraph of
Section 7.2, proof in Appendix B). Preservation of modularity is an important result since
it means that it has to be checked/ensured at most once during the lifetime of the action
theory. All these results support the thesis that our modularity notion is fruitful.
By forcing formulas to be explicitly stated in their respective modules (and thus possibly
making them inferable in independently different ways), modularity intuitively could be
seen to diminish elaboration tolerance (McCarthy, 1998). For instance, when contracting a
Boolean formula  in a non-modular theory, it seems reasonable to expect not to change the
set of static laws S , while the theory being modular surely forces changing such a module. It
is not difficult, however, to conceive non-modular theories in which contraction of a formula
 may demand a change in S as well. As an example, suppose S = {1  2 } in an action
theory from whose dynamic part we (implicitly) infer 2 . In this case, contracting 1
while keeping 2 would necessarily ask for a change in S .
We point out nevertheless that in both cases (modular and non-modular) the extra work
in changing other modules stays in the mechanical level, i.e., in the algorithms that carry
out the modification, and does not augment in a significant way the amount of work the
knowledge engineer is expected to do.
Contrary to the trend in the belief change community, where the focus is either on belief
bases or belief sets (Hansson, 1999), the method here proposed is a hybrid one (Delgrande,
2009). On one hand, semantics plays a crucial role in the notion of minimal change here
studied. On the other hand, we deal only with domain descriptions in reasoning about
actions, which are sets of laws of specific types. On top of that, the modularity property (a
syntactical one) is fundamental to our main results.
Following those lines, another issue that drives our future research on the subject is how
to contract not only laws but any Kn -formula. As defined, the order of application of our
operators matter in the final result: if we contract  and then   [a] from a theory T,
the result may not be the same as contracting   [a] first and then removing . This
problem would not appear in a more general framework in which any formula could be
contracted: removing   (  [a]) should give the same result as (  [a])  . This
is the principle of syntax independence (Dalal, 1988).
Related to that is the question on how our revision definitions relate to our contraction
operators. What is known is that the Levi identity (1977), T ? = T 
  {}, in general
does not hold for action laws (effect and executability ones). The reason is that up to now
there is no contraction operator for  where  is an effect or an executability law. Indeed
this is the general contraction problem for non-classical logics: contraction of a general
formula (like  above) is still an open problem in the belief change area. Some insights in
this direction are given by our revision definitions, with which we make  false in every
possible world of a Kripke model.
235

fiVarzinczak

Definitions 3.1, 3.5 and 3.8 appear to be important for better understanding the problem
of contracting general formulas: basically the set of modifications to perform in a given
model in order to force it to falsify a general formula will comprise removal/addition of
transitions/worlds. The definition of a general revision/contraction method will then benefit
from our constructions.
Furthermore, given the well-known connection between multimodal logics and Description Logics (Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2003), we believe
that our definitions may also contribute to ontology evolution and debugging in some specific
families of DLs.

Acknowledgments
Parts of this work have been done during the authors stay at the Institut de Recherche en
Informatique de Toulouse (IRIT), France, and during his visit to the National ICT Australia
(NICTA), Sydney.
The author is grateful to the anonymous referees for their constructive and useful remarks, which helped improving the quality of the work. The paper has also benefited from
discussions with Andreas Herzig and Laurent Perrussel.
Special thanks to my colleagues at the Meraka Institute Arina Britz, Ken Halland,
Johannes Heidema and Tommie Meyer for their invaluable comments and suggestions on
earlier versions of this article.

Appendix A. Proof of Theorem 6.2
M

Let T be modular, and  be a law. For all M0  M
 such that |= T for every M  M,
M0

0
0
0
there is T 0  T 
 such that |= T for every M  M .

Before we give the proof of this theorem, we will need the following lemma (cf. the
Monotonicity Postulate in Section 7.2):
Lemma A.1 T |=
T 0.
K
n

Proof: Let T be an action theory, and let T 0  T 
 , for a given law . We are going to
analyze each case.
Let  be of the form   hai>, for some   Fml. Then T 0 is such that
T 0 = (T \ Xa )  {(i  (  A ))  hai> : i  hai>  Xa }
V
V
where   IP(S  ) and A = pi atm() pi  pi atm() pi , for some A  atm().
pi A

pi A
/

M

Let M = hW, Ri be such that |= T. It is enough to show that M is a model of the new
M
M
laws. For every (i  (  A ))  hai>, for every w  W, if |=w i  (  A ), then |=
.
w i
M

Because T |=K i  hai>, |= i  hai>, and then Ra (w) 6= .
n

M

Therefore we have that |= T 0 .
236

fiOn Action Theory Change

Let now  have the form   [a], for ,   Fml. Then T 0 is such that
(T \ Ea ) 
{(i  (  A ))  [a]i : i  [a]i  Ea } 
0

T 0 = {(i    A )  [a](i   ) : i  [a]i  Ea } 


`  L, forVsome L  Lit s.t.


(  A  `)  [a](  `) : S 6` ( 0  `L `)  , and `   0


or T 6`Kn (  A  `)  [a]`

where Ea =

S

,
1in (Ea )i ,

  IP(S  ), A =

V
pi atm()
pi A

pi 

V
pi atm()
pi A
/

pi , for some

A  atm(), and  0  IP(S  ).
M
Let M = hW, Ri be such that |= T. It is enough to show that M is a model of the
M
added laws. Given (i  (  A ))  [a]i , for every w  W, if |=w i  (  A ), then
M

M

M

|=w i . Because T |=K i  [a]i , |= i  [a]i , and then |= 0 i for every w0  W such that
w
n
(w, w0 )  Ra .
M
M
For (i    A )  [a](i   0 ), for every w  W, if |=w i    A , then again |= 0 i
w
for every w0  W such that (w, w0 )  Ra .
M
M
Now, given (  A  `)  [a](  `), for every w  W, if |=w   A  `, then |=
, and
w
M

M

M

then |=
. Since T |=
  [a], we have |=   [a], and then |= 0  for every w0  W
Kn
w
w
0
such that (w, w )  Ra .
M
Therefore |= T 0 .
Let  be a propositional . Then T 0 is such that
((T \ S )  S  ) \ Xa 
T 0 = {(i  )  hai> : i  hai>  Xa } 
{  [a]}
for some S   S 	 .
M
Let M = hW, Ri be such that |= T. It suffices to show that M satisfies the added laws.
Since we assume 	 behaves like a classical contraction operator, like e.g. Katsuno and
M
M
Mendelzons (1992), we have |=
S  S  , and then, because |= S , we have |= S  .
CPL
M

M

M

Now given (i  )  hai>, for every w  W, if |=w i  , then |=
i , and because
w

|= i  hai>, we have Ra (w) 6= .
M
Finally, for   [a], because |= , M trivially satisfies   [a].
M
Therefore |= T 0 .

2

Proof of Theorem 6.2
M

M0


0
0
Let M = {M : |= T}, and M0  M
 . We show that there is T  T  such that |= T
0
0
for every M  M .

237

fiVarzinczak

M0

M0

By definition, each M 0  M0 is such that either |= T or 6|= . Because T 
 6= , there
M0

M0

0
must be T 0  T 
 . If |= T, by Lemma A.1 |= T and we are done. Let us then suppose
M0

that 6|= . We analyze each case.
Let  have the form   hai> for some   Fml. Then M 0 = hW0 , R0 i, where W0 = W,
M

0
 and (w, w0 )  Ra }, for some M  M.
R0 = R \ R
a , with Ra = {(w, w ) : |=
w
M0

M0

Let u  W0 be such that 6|=
  hai>, i.e., |=
 and R0a (u) = .
u
u
V
0
Because u  , there must be v  base(, WV
) such that v  u. Let  = `v `. Clearly
 is a prime implicant of S  . Let also A = `u\v `, and consider
T 0 = (T \ Xa )  {(i  (  A ))  hai> : i  hai>  Xa }
(Clearly, T 0 is a theory produced by Algorithm 1.)
It is enough to show that M 0 is a model of the new added laws. Given (i (A )) 
M

M0

M0

i , from what it follows |=w i .
i  (  A ), then |=
hai>  T 0 , for every w  W0 , if |=
w
w
M

Because |= i  hai>, there is w0  W such that w0  Ra (w). We need to show that
M
M
0
0
(w, w0 )  R0a . If 6|=
, then R
, either w = u, and then from
a = , and (w, w )  Ra . If |=
w
w
M0

M0

|=u   A we conclude |=u (i  (  A ))  hai>, or w 6= u and then we must have



(w, w0 )  R0a , otherwise there is S
a  Ra such that R(R \ Sa )  R(R \ Ra ), and then
M 00

00
0
M 00 = hW0 , R \ S
a i is such that 6|=   hai> and M M M , a contradiction because
M0

M0

M 0 is minimal with respect to M . Thus (w, w0 )  R0a , and then |=
hai>. Hence |= T 0 .
w
Now let  be of the form   [a], for ,  both Boolean. Then M 0 = hW0 , R0 i, where
W0 = W, R0 = R  R,
, with
a
R,
= {(w, w0 ) : w0  RelTarget(w,   [a], M , M)}
a
for some M = hW, Ri  M.
M0

M0

  [a]. Then there is u0  W0 such that (u, u0 )  R0a and
Let u  W0 be such that 6|=
u

6|=0 . Because u  , there is v  base(, W0 ) such that v  u, and as u0  , there must
u
V
V
V
be v 0  base(, W0 ) such that v 0  u0 . Let  = `v `, A = `u\v `, and  0 = `v0 `.
Clearly  (resp.  0 ) is a prime implicant of S   (resp. S  ).
S
Now let Ea = 1in (Ea, )i and let the theory
(T \ Ea ) 
{(i  (  A ))  [a]i : i  [a]i  Ea } 
0

T 0 = {(i    A )  [a](i   ) : i  [a]i  Ea } 


`  L, forVsome L  Lit s.t.


(  A  `)  [a](  `) : S 6` ( 0  `L `)  , and `   0


or T 6`Kn (  A  `)  [a]`

(Clearly, T 0 is a theory produced by Algorithm 2.)
238

fiOn Action Theory Change

In order to show that M 0 is a model of T 0 , it is enough to show that it is a model of the
M0
added laws. Given (i (A ))  [a]i  T 0 , for every w  W0 , if |=w i (A ), then
M0

M

M

M

|=
i , and then |=w i . Because |= i  [a]i , |= 0 i for all w0  W such that (w, w0 )  Ra .
w
M

w

= , and then R0a (w) = Ra (w).
We need to show that R0a (w) = Ra (w). If 6|=w , then R,
a
M

M0

M0

If |=
, then either w = u, and from |=u   A we conclude |=
(i  (  A ))  [a]i ,
w
u
,
or w 6= u, and then we must have Ra
= , otherwise there would be S,
 R,
such
a
a
,
,
0
,
00
that R(R  Sa )  R(R  Ra ), and then M = hW , R  Sa i would be such that
M 00

6|=   [a] and M 00 M M 0 , a contradiction since M 0 is minimal with respect to M .
M0

Hence R0a (w) = Ra (w), and |= 0 i for all w0 such that (w, w0 )  R0a .
w

M0

M0

Now, given (i    A )  [a](i   0 ), for every w  W0 , if |=w i    A , then
M

M

M

|=w i , and then |=
i . Because, |= i  [a]i , we have |= 0 i for all w0  W such that
w
w

M0

(w, w0 )  Ra , and then |= 0 i for every w0  W0 such that (w, w0 )  R0a \ Ra, . Now, given
w

M0

, |= 0  0 , and the result follows.
(w, w0 )  R,
a
w

M0

M0

Now, for each (  A  `)  [a](  `), for every w  W0 , if |=
  A  `, then
w
M

M

M

|=w , and then |=
. Because |=   [a], we have |= 0  for every w0  W such that
w
(w, w0 )

w

M0

 Ra , and then |= 0  for all
w

M0

w0

0

 W such that (w, w0 )  R0a \ R,
. It remains to
a

show that |= 0 ` for every w0  W0 such that (w, w0 )  R,
. Since M 0 is minimal, it is
a
w

M0

M0

enough to show that |=0 ` for every `  Lit such that |=
  A  `. If `   0 , the result
u
u

M0

follows. Otherwise, suppose 6|=0 `. Then
u

 either `   0 , then  0 and ` are unsatisfiable, and in this case Algorithm 2 has not
put the law (  A  `)  [a](  `) in T 0 , a contradiction;
 or `  u0 \ v 0 . In this case, there is a valuation u00 = (u0 \ {`})  {`} such that
0
00
0
u00 6 . We
: `i  u00 } such that
iV
V must have u  W , otherwise there will be L = {`
0
0
T |=K (  `i L0 `i )  , and, because T is modular, S |=CPL (  `i L0 `i )  , and
n
then Algorithm 2 has not put the law (  A  `)  [a](  `) in T 0 , a contradiction.
Then u00  W0 , and moreover u00 
/ R,
(u), otherwise M 0 is not minimal. As
a
,
u00 \ u  u0 \ u, the only reason why u00 
/ Ra (u) is that there is `0  u  u00 such that
Mi V
0
|= `j u `j  [a]` for every Mi  M if and only if `0 
/ v0 for any v0  base(, W 0 )
M V
such that v0  u00 . Clearly `0 = `, and because ` 
/  0 , we have |= i `j u `j  [a]`
for every Mi  M. Then T |=
(  A  `)  [a]`, and then Algorithm 2 has not
Kn
put the law (  A  `)  [a](  `) in T 0 , a contradiction.
M0

Hence we have |= 0   ` for every w0  W0 such that (w, w0 )  R0a .
w

M0

Putting the above results together, we get |= T 0 .
Let now  be some propositional . Then M 0 = hW0 , R0 i, where W  W0 , R0 = R, is
minimal with respect to M , i.e., W0 is a minimal superset of W such that there is u  W0
239

fiVarzinczak

with u 6 . Because we have assumed the syntactical classical contraction operator is
sound and complete with respect to its semantics and is moreover minimal, then there must
M0
be S   S 	  such that W0 = val(S  ). Therefore |= S  .
Because R0 = R, every effect law of T remains true in M 0 .
Now, let
((T \ S )  S  ) \ Xa 
T 0 = {(i  )  hai> : i  hai>  Xa } 
{  [a]}
(Clearly, T 0 is a theory produced by Algorithm 3.)
M0

For every (i  )  hai>  T 0 and every w  W0 , if |=
i  , then Ra (w) 6= ,
w
M0

M

i  hai>. Given   [a], for every w  W0 , if |=
, then w = u, and
because |=
w
w
Ra (w) = .
M0

2

Putting all these results together, we have |= T 0 .

Appendix B. Proof of Theorem 6.3
M0


0
0
0
Let T be modular,  a law, and T 0  T 
 . For all M such that |= T , there is M  M
M

such that M 0  M0 and |= T for every M  M.
In order to prove this result, first we need to show four important lemmas.
Lemma B.1 Let  be a law. If T is modular, then every T 0  T 
 is modular.
0
Proof: Let  be nonclassical, and suppose there is T 0  T 
 such that T is not modular.
0
0
0
0
0
0
Then there is some   Fml such that T |=K  and S 6|=
 , where S is the set of static
CPL
n
0
0
laws in T . By Lemma A.1, T |=
T , and then we have T |=
0 . Because  is nonclassical,
Kn
Kn
0
S = S . Thus S 6|=CPL 0 , and therefore T is not modular.

Let now  be some   Fml. Then
((T \ S )  S  ) \ Xa 
T 0 = {(i  )  hai> : i  hai>  Xa } 
{  [a]}
for some S   S 	 .
Assume that T is modular, and let 0  Fml be such that T 0 |=
0 and S  6|=
0 .
Kn
CPL
As S  6|=
0 , there is v  val(S  ) such that v 6 0 . If v  val(S ), then S 6|=CPL 0 , and
CPL
as T is modular, T 6|=K 0 . By Lemma A.1, T |=K T 0 , and we have T 0 6|=K 0 , a contradiction.
n
n
n
Hence v 
/ val(S ). Moreover, we must have v 6 , otherwise 	 has not worked as expected.
M
Let M = hW, Ri be such that |= T 0 . (We extend M to another model of T 0 .) Let
M 0 = hW0 , R0 i be such that W0 = W  {v} and R0 = R. To show that M 0 is a model
M0

of T 0 , it suffices to show that v satisfies every law in T 0 . As v  val(S  ), |=
S  . Given
v
240

fiOn Action Theory Change

M0

  [a]  T 0 , as v 6  and R0a (v) = , |=v   [a]. Now, for every i  [a]i  T 0 ,
M0

M0

if |=v i , then we trivially have |=0 i for every v0 such that (v, v0 )  R0a . Finally, given
v

M0

(i  )  hai>  T 0 , as v 6 , the formula trivially holds in v. Hence |= T 0 , and because
M0

there is v  W0 such that 6|=
0 , we have T 0 6|=K 0 , a contradiction. Hence for all 0  Fml
v
n
0 , and then T 0 is modular.
such that T 0 |=K 0 , S  |=

2
CPL
n

Lemma B.2 If Mcan = hWcan , Rcan i is a model of T, then for every M = hW, Ri such
M
that |= T there is a minimal (with respect to set inclusion) extension R0  Rcan \ R such
that M 0 = hval(S ), R  R0 i is a model of T.
M

Proof: Let Mcan = hWcan , Rcan i be a model of T, and let M = hW, Ri be such that |= T.
M0

Consider M 0 = hval(S ), Ri. If |= T, we have R0 =   Rcan \ R that is minimal. Suppose
M0

M0

then 6|= T. We extend M 0 to a model of T that is a minimal extension of M . As 6|= T,
M0

M0

there is v  val(S ) \ W such that 6|=
T. Then there is   T such that 6|=v . If  is
v
some   Fml, as v  Wcan , Mcan is not a model of T. If  is of the form   [a], for
,   Fml, there is v0  val(S ) such that (v, v0 )  Ra and v0 6 , a contradiction since
M0

Ra (v) = . Let now  have the form   hai> for some   Fml. Then |=
. As v  Wcan ,
v
Mcan

Mcan

if 6|=v   hai>, then 6|= T. Hence, Rcana (v) 6= . Thus taking any (v, v0 )  Rcana gives
us a minimal R0 = {(v, v0 )} such that M 00 = hval(S ), R  R0 i is a model of T.
2
Lemma B.3 Let T be modular, and  be a law. Then T |=
 if and only if every M 0 =
K
0

hW,Ri

hval(S ), R i such that |=

n

0

T and R  R is a model of .

Proof:
M
M
(): Straightforward, since T |=
 implies |=  for every M such that |= T, in particular
Kn
for those which are extensions of some model of T.
M

M

(): Suppose T 6|=
. Then there is M = hW, Ri such that |= T and 6|= . As T is modular,
Kn
the canonical frame Mcan = hWcan , Rcan i of T is a model of T. Then by Lemma B.2 there is
a minimal extension R0 of R with respect to Rcan such that M 0 = hval(S ), R  R0 i is a model
M
M
of T. Because 6|= , there is w  W such that 6|=w . If  is some propositional   Fml or
M0

an effect law, any extension M 0 of M is such that 6|=w . If  is of the form   hai>,
M

then |=
 and Ra (w) = . As any extension of M is such that (u, v)  R0 if and only if
w
u  val(S ) \ W, only worlds other than those in W get a new departing transition. Thus
M0

2

(R  R0 )a (w) = , and then 6|=w .

0
0
0
Lemma B.4 Let T be modular,  a law, and T 0  T 
 . If M = hval(S ), R i is a model
M

of T 0 , then there is M = {M : M = hval(S ), Ri and |= T} such that M 0  M0 for some
M0  M
.
241

fiVarzinczak

M0

M0

Proof: Let M 0 = hval(S 0 ), R0 i be such that |= T 0 . If |= T, the result follows. Let us
M0

suppose then 6|= T. We analyze each case.
Let  be of the form   hai>, for some   Fml. Let M = {M : M = hval(S ), Ri}.
Since by hypothesis T is modular, from Lemmas B.2 and B.3 it follows that M is non-empty
and contains only models of T.
Suppose M 0 is not a minimal model of T 0 , i.e., there is M 00 such that M 00 M M 0 for
some M  M. Then M 0 and M 00 differ only in the executability
of a inVa given -world,
V
viz. a   A -context, for some   IP(S  ) and A = pi atm() pi  pi atm() pi such
pi A

M0

M 00

pi A
/

that A  atm(). Because 6|= (  A )  hai>, we must have |= (  A )  hai> and
M 00

then |= T. Hence M 0 is minimal with respect to M .
When contracting executability laws, S 0 = S . Hence taking the right R and a minimal
M
0


0
R
 and (w, w0 ) 
a such that M = hval(S ), Ri and R = R \ Ra , for some Ra  {(w, w ) :|=
w

Ra }, we construct M0 = M  {M 0 }  Mhai> .
Let  now be of the form   [a], for ,   Fml. Let M = {M : M = hval(S ), Ri}.
Since by hypothesis T is modular, from Lemmas B.2 and B.3 it follows that M is non-empty
and contains only models of T.
We claim that M 0 has only oneVtransition linking
V a -world, viz. a context i    A
for some   IP(S  ) and A = pi atm() pi  pi atm() pi , such that A  atm(), to a
pi A

pi A
/

 0 -world, where  0  IP(S  ). The proof is as follows: given `  Lit such that ` holds
in this i    A -world
 if (  A  `)  [a](  `) 
/ T 0 , then ` 
/  0 and T |=
(  A  `)  [a]`. Then
Kn
this world has only `-successors.
 if (  A  `)  [a](  `)  T 0 , then every  0 -successor is an `-world.
By successively applying this reasoning to each ` that holds in this i    A -world, we
will end up with only one  0 -successor.
M 00
Suppose now that M 0 is not a minimal model of T 0 , i.e., there is M 00 such that |= T 0
and M 00 M M 0 for some M  M. Then M 0 and M 00 differ only in the effects on
that i    A -world: M 00 has no transition linking it to a  0 -world. Then we have
M 00
M 00
|= (i    A )  [a]i , and then |= T. Therefore M 0 is a minimal model of T 0 with
respect to M .
When contracting effect laws, S 0 = S . Thus taking the right R and a minimal R,
a
M
0
,
,
0
such that M = hval(S ), Ri and R = R  Ra , for some Ra  {(w, w ) :|=
 and w0 
w

RelTarget(w,   [a], M , M)}, we construct M0 = M  {M 0 }  M[a] .
Let now  be  for some   Fml. Since T is modular, by Lemmas B.2 and B.3 there
M
is M = hval(S ), Ri such that |= T. We know val(S )  val(S  ). Because   [a]  T 0 ,
R0a (v) =  for every -world v added in M 0 . Hence, because 	 is minimal, taking M =
{M } gives us the result.
2
242

fiOn Action Theory Change

Proof of Theorem 6.3
From the hypothesis that T is modular and Lemma B.1, it follows that T 0 is modular,
too. Then M 0 = hval(S 0 ), Ri is a model of T 0 , by Lemma B.3. From this and Lemma B.4
the result follows.
2

References
Alchourron, C., Gardenfors, P., & Makinson, D. (1985). On the logic of theory change:
Partial meet contraction and revision functions. Journal of Symbolic Logic, 50, 510
530.
Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. (Eds.). (2003).
Description Logic Handbook. Cambridge University Press.
Baral, C., & Lobo, J. (1997). Defeasible specifications in action theories. In Pollack, M.
(Ed.), Proceedings of the 15th International Joint Conference on Artificial Intelligence
(IJCAI), pp. 14411446. Morgan Kaufmann Publishers.
Booth, R., Meyer, T., & Varzinczak, I. (2009). Next steps in propositional Horn contraction. In Boutilier, C. (Ed.), Proceedings of the 21st International Joint Conference on
Artificial Intelligence (IJCAI), pp. 702707. AAAI Press.
Burger, I., & Heidema, J. (2002). Merging inference and conjecture by information. Synthese, 131 (2), 223258.
Castilho, M., Gasquet, O., & Herzig, A. (1999). Formalizing action and change in modal
logic I: the frame problem. Journal of Logic and Computation, 9 (5), 701735.
Castilho, M., Herzig, A., & Varzinczak, I. (2002). It depends on the context! A decidable
logic of actions and plans based on a ternary dependence relation. In 9th International
Workshop on Nonmonotonic Reasoning (NMR).
Cholvy, L. (1999). Checking regulation consistency by using SOL-resolution. In Proceedings
of the 7th International Conference on AI and Law, pp. 7379.
Dalal, M. (1988). Investigations into a theory of knowledge base revision: preliminary report.
In Smith, R., & Mitchell, T. (Eds.), Proceedings of the 7th National Conference on
Artificial Intelligence (AAAI), pp. 475479. Morgan Kaufmann Publishers.
De Giacomo, G., & Lenzerini, M. (1995). PDL-based framework for reasoning about actions. In Gori, M., & Soda, G. (Eds.), Proceedings of the 4th Congress of the Italian Association for Artificial Intelligence (IA*AI), No. 992 in LNAI, pp. 103114.
Springer-Verlag.
Delgrande, J. (2009). Personal communication. Commonsense09, Toronto.
Demolombe, R., Herzig, A., & Varzinczak, I. (2003). Regression in modal logic. Journal of
Applied Non-Classical Logic, 13 (2), 165185.
Eiter, T., Erdem, E., Fink, M., & Senko, J. (2005). Updating action domain descriptions.
In Kaelbling, L., & Saffiotti, A. (Eds.), Proceedings of the 19th International Joint
Conference on Artificial Intelligence (IJCAI), pp. 418423. Morgan Kaufmann Publishers.
243

fiVarzinczak

Finger, J. (1987). Exploiting constraints in design synthesis. Ph.D. thesis, Stanford University.
Flouris, G., Plexousakis, D., & Antoniou, G. (2004). Generalizing the AGM postulates. In
10th International Workshop on Nonmonotonic Reasoning (NMR).
Fuhrmann, A. (1989). On the modal logic of theory change. In The Logic of Theory Change,
pp. 259281.
Gardenfors, P. (1988). Knowledge in Flux: Modeling the Dynamics of Epistemic States.
MIT Press.
Gelfond, M., & Lifschitz, V. (1993). Representing action and change by logic programs.
Journal of Logic Programming, 17 (2/3&4), 301321.
Giunchiglia, E., Kartha, G., & Lifschitz, V. (1997). Representing action: indeterminacy and
ramifications. Artificial Intelligence, 95 (2), 409438.
Hamming, R. (1950). Error detecting and error correcting codes. Bell System Technical
Journal, 26 (2), 147160.
Hansson, S. (1994). Kernel contraction. Journal of Symbolic Logic, 59 (3), 845859.
Hansson, S. (1999). A Textbook of Belief Dynamics: Theory Change and Database Updating.
Kluwer Academic Publishers.
Harel, D., Tiuryn, J., & Kozen, D. (2000). Dynamic Logic. MIT Press.
Herzig, A., Perrussel, L., & Varzinczak, I. (2006). Elaborating domain descriptions. In
Brewka, G., Coradeschi, S., Perini, A., & Traverso, P. (Eds.), Proceedings of the 17th
European Conference on Artificial Intelligence (ECAI), pp. 397401. IOS Press.
Herzig, A., & Rifi, O. (1999). Propositional belief base update and minimal change. Artificial
Intelligence, 115 (1), 107138.
Herzig, A., & Varzinczak, I. (2004). Domain descriptions should be modular. In Lopez de
Mantaras, R., & Saitta, L. (Eds.), Proceedings of the 16th European Conference on
Artificial Intelligence (ECAI), pp. 348352. IOS Press.
Herzig, A., & Varzinczak, I. (2005a). Cohesion, coupling and the meta-theory of actions.
In Kaelbling, L., & Saffiotti, A. (Eds.), Proceedings of the 19th International Joint
Conference on Artificial Intelligence (IJCAI), pp. 442447. Morgan Kaufmann Publishers.
Herzig, A., & Varzinczak, I. (2005b). On the modularity of theories. In Schmidt, R., PrattHartmann, I., Reynolds, M., & Wansing, H. (Eds.), Advances in Modal Logic, Vol. 5,
pp. 93109. Kings College Publications.
Herzig, A., & Varzinczak, I. (2006). A modularity approach for a fragment of ALC. In
Fisher, M., van der Hoek, W., Konev, B., & Lisitsa, A. (Eds.), Proceedings of the 10th
European Conference on Logics in Artificial Intelligence (JELIA), No. 4160 in LNAI,
pp. 216228. Springer-Verlag.
Herzig, A., & Varzinczak, I. (2007). Metatheory of actions: beyond consistency. Artificial
Intelligence, 171, 951984.
244

fiOn Action Theory Change

Jin, Y., & Thielscher, M. (2005). Iterated belief revision, revised. In Kaelbling, L., & Saffiotti, A. (Eds.), Proceedings of the 19th International Joint Conference on Artificial
Intelligence (IJCAI), pp. 478483. Morgan Kaufmann Publishers.
Katsuno, H., & Mendelzon, A. (1992). On the difference between updating a knowledge
base and revising it. In Gardenfors, P. (Ed.), Belief revision, pp. 183203. Cambridge
University Press.
Kracht, M., & Wolter, F. (1991). Properties of independently axiomatizable bimodal logics.
Journal of Symbolic Logic, 56 (4), 14691485.
Levi, I. (1977). Subjunctives, dispositions and chances. Synthese, 34, 423455.
Li, R., & Pereira, L. (1996). What is believed is what is explained. In Shrobe, H., & Senator, T. (Eds.), Proceedings of the 13th National Conference on Artificial Intelligence
(AAAI), pp. 550555. AAAI Press/MIT Press.
Liberatore, P. (2000). A framework for belief update. In Proceedings of the 7th European
Conference on Logics in Artificial Intelligence (JELIA), pp. 361375.
Makinson, D. (2007). Friendliness and sympathy in logic. In Beziau, J.-Y. (Ed.), Logica
Universalis, Vol. 2, pp. 195224. Springer-Verlag.
Marquis, P. (2000). Consequence finding algorithms. In Gabbay, D., & Smets, P. (Eds.),
Handbook on Defeasible Reasoning and Uncertainty Management Systems, Vol. 5:
Algorithms for Uncertainty and Defeasible Reasoning, edited by J. Kohlas and S.
Moral, chap. 2, pp. 41145. Kluwer Academic Publishers.
McCarthy, J. (1977). Epistemological problems of artificial intelligence. In Sridharan, N.
(Ed.), Proceedings of the 5th International Joint Conference on Artificial Intelligence
(IJCAI), pp. 10381044. Morgan Kaufmann Publishers.
McCarthy, J. (1998). Elaboration tolerance. In Proceedings of the 4th International Symposium on Logical Formalizations of Commonsense Reasoning.
McCarthy, J., & Hayes, P. (1969). Some philosophical problems from the standpoint of
artificial intelligence. In Meltzer, B., & Mitchie, D. (Eds.), Machine Intelligence,
Vol. 4, pp. 463502. Edinburgh University Press.
Nebel, B. (1989). A knowledge level analysis of belief revision. In Brachman, R., Levesque,
H., & Reiter, R. (Eds.), Proceedings of the 1st International Conference on Principles
of Knowledge Representation and Reasoning (KR), pp. 301311. Morgan Kaufmann
Publishers.
Parikh, R. (1999). Beliefs, belief revision, and splitting languages. In Moss, L. (Ed.),
Logic, Language and Computation, No. 96 in CSLI Lecture Notes, pp. 266278. CSLI
Publications.
Popkorn, S. (1994). First Steps in Modal Logic. Cambridge University Press.
Quine, W. (1952). The problem of simplifying truth functions. American Mathematical
Monthly, 59, 521531.
Quine, W. (1962). Paradox. Scientific American, 1, 8496.
245

fiVarzinczak

Reiter, R. (2001). Knowledge in Action: Logical Foundations for Specifying and Implementing Dynamical Systems. MIT Press.
Schlechta, K. (2004). Coherent Systems. Studies in Logic and Practical Reasoning 2.
Elsevier.
Shanahan, M. (1997). Solving the frame problem: a mathematical investigation of the common sense law of inertia. MIT Press.
Shapiro, S., Lesperance, Y., & Levesque, H. (2005). Goal change. In Kaelbling, L., & Saffiotti, A. (Eds.), Proceedings of the 19th International Joint Conference on Artificial
Intelligence (IJCAI), pp. 582588. Morgan Kaufmann Publishers.
Shapiro, S., Pagnucco, M., Lesperance, Y., & Levesque, H. (2000). Iterated belief change in
the situation calculus. In Cohn, T., Giunchiglia, F., & Selman, B. (Eds.), Proceedings
of the 7th International Conference on Principles of Knowledge Representation and
Reasoning (KR), pp. 527538. Morgan Kaufmann Publishers.
Thielscher, M. (1997). Ramification and causality. Artificial Intelligence, 89 (12), 317364.
Thielscher, M. (2010). A unifying action calculus. To appear in Artificial Intelligence.
Varzinczak, I. (2006). What is a good domain description? Evaluating and revising action
theories in dynamic logic. Ph.D. thesis, Universite Paul Sabatier, Toulouse.
Varzinczak, I. (2008). Action theory contraction and minimal change. In Lang, J., &
Brewka, G. (Eds.), Proceedings of the 11th International Conference on Principles of
Knowledge Representation and Reasoning (KR), pp. 651661. AAAI Press.
Varzinczak, I. (2009). On the revision of action laws: an algorithmic approach. In IJCAI
Workshop on Nonmonotonic Reasoning, Action and Change (NRAC).
Winslett, M.-A. (1988). Reasoning about action using a possible models approach. In Smith,
R., & Mitchell, T. (Eds.), Proceedings of the 7th National Conference on Artificial
Intelligence (AAAI), pp. 8993. Morgan Kaufmann Publishers.
Zhang, D., Chopra, S., & Foo, N. (2002). Consistency of action descriptions. In Ishizuka,
M., & Sattar, A. (Eds.), Proceedings of the 7th Pacific Rim International Conference
on Artificial Intelligence, No. 2417 in LNCS, pp. 7079. Springer-Verlag.
Zhang, D., & Foo, N. (2001). EPDL: A logic for causal reasoning. In Nebel, B. (Ed.), Proceedings of the 17th International Joint Conference on Artificial Intelligence (IJCAI),
pp. 131138. Morgan Kaufmann Publishers.
Zhang, Y., & Ding, Y. (2008). CTL model update for system modifications. Journal of
Artificial Intelligence Research, 31, 113155.

246

fiJournal of Artificial Intelligence Research 37 (2010) 479-525

Submitted 12/09; published 03/10

Multiattribute Auctions Based on Generalized Additive
Independence
Yagil Engel

yagile@ie.technion.ac.il

Technion - Israel Institute of Technology
Faculty of Industrial Engineering & Management
Technion City, Haifa 32000, Israel

Michael P. Wellman

wellman@umich.edu

University of Michigan
Division of Computer Science & Engineering
2260 Hayward St, Ann Arbor, MI 48109-2121, USA

Abstract
We develop multiattribute auctions that accommodate generalized additive independent
(GAI) preferences. We propose an iterative auction mechanism that maintains prices on
potentially overlapping GAI clusters of attributes, thus decreases elicitation and computational burden, and creates an open competition among suppliers over a multidimensional
domain. Most significantly, the auction is guaranteed to achieve surplus which approximates optimal welfare up to a small additive factor, under reasonable equilibrium strategies
of traders. The main departure of GAI auctions from previous literature is to accommodate non-additive trader preferences, hence allowing traders to condition their evaluation
of specific attributes on the value of other attributes. At the same time, the GAI structure
supports a compact representation of prices, enabling a tractable auction process. We perform a simulation study, demonstrating and quantifying the significant efficiency advantage
of more expressive preference modeling. We draw random GAI-structured utility functions
with various internal structures, generate additive functions that approximate the GAI
utility, and compare the performance of the auctions using the two representations. We
find that allowing traders to express existing dependencies among attributes improves the
economic efficiency of multiattribute auctions.

1. Introduction
Multiattribute trading mechanisms extend traditional, price-only mechanisms by facilitating negotiation over a set of predefined attributes representing various non-price aspects
of a deal. Rather than negotiate over a fully specified good or service, a multiattribute
mechanism delays commitment to particular configurations until it extracts sufficient information on traders preferences. For example, a companys procurement department may
run a multiattribute auction to select a supplier of hard drives. Supplier offers may be
evaluated not only over the price they offer, but also over features such as volume, RPM,
access time, latency, transfer rate, and so on. In addition, suppliers may offer contracts
differing in terms such as warranty, delivery time, and service.
In order to account for traders preferences, the auction mechanism must extract evaluative information over a complex domain of multidimensional configurations. Constructing
and communicating a complete preference specification can pose a severe burden for even a

c
!2010
AI Access Foundation. All rights reserved.

fiEngel & Wellman

moderate number of attributes, hence practical multiattribute auctions must either accommodate partial specifications, or support compact expression of preferences assuming some
simplified form. By far the most popular multiattribute form to adopt is the simplest: an
additive representation where overall value is a linear combination of values associated with
each attribute. For example, several recent proposals for iterative multiattribute auctions
(Beil & Wein, 2003; Bichler, 2001; David, Azoulay-Schwartz, & Kraus, 2002; Parkes &
Kalagnanam, 2005) require additive preference representations.
Such additivity reduces the complexity of preference specification exponentially (compared to the general discrete case), but precludes expression of any interdependencies among
the attributes. In practice, however, interdependencies among natural attributes are quite
common. For example, the hard-drive buyer may exhibit complementary preferences for
volume and access time (since the performance effect is more salient if much data is involved), or may view a strong warranty as a good substitute for high reliability ratings.
Similarly, a sellers production characteristics can easily violate additivity, for example if
decreasing access time is technically more difficult for higher-capacity drives. In such cases
an additive value function may not be able to provide an adequate approximation of real
preferences.
On the other hand, fully general models are intractable, and multiattribute preferences
typically exhibit some structure. Our goal, therefore, is to identify the subtler yet more
widely applicable structured representations, and exploit these properties of preferences in
trading mechanisms.
We propose an iterative auction mechanism based on just such a flexible preference
structure. Our approach is inspired by the design of an iterative multiattribute procurement
auction for additive preferences, due to Parkes and Kalagnanam (2005) (PK). PK present
two auction designs: the first (NLD) makes no assumptions about traders preferences, and
lets sellers bid on the full multidimensional attribute space. Because NLD maintains an
exponential price structure, it is suitable only for small domains. The other auction (AD)
assumes additive buyer valuation and seller cost functions. It collects sell bids per attribute
level and for a single discount term. The price of a configuration is the sum of the prices of
the chosen attribute levels minus the discount.
The auction we propose also supports compact price spaces, albeit for levels of clusters of attributes rather than singletons. We employ a preference decomposition based on
generalized additive independence (GAI), a model flexible enough to accommodate interdependencies to the exact degree of accuracy desired, yet providing a compact functional
form to the extent that interdependence can be limited.
First, we build a direct, formally justified link from preference statements over priced
outcomes to a generalized additive decomposition of the willingness-to-pay (wtp) function.
After laying out this infrastructure, we employ this representation tool for the development
of a multiattribute iterative auction mechanism that allows traders to express their complex
preferences in GAI format. We then study the auctions allocational, computational, and
practical properties. Next, we present a simulation study of our proposed auction mechanism, in order to practically evaluate the economic and computational properties of GAI
auctions. We simulate the auctions using random GAI utility functions, including some
based on constrained preference structures often exhibited in applications. The simulations
let us quantify the benefits of modeling preferences accurately using GAI, in comparison to
480

fiGAI Auctions

using an additive approximation. We show that under most circumstances, a GAI auction
achieves significantly higher surplus than an auction that uses an additive approximation
of preferences.
After providing background on multiattribute preferences and multiattribute auctions
(Section 2), we develop new multiattribute structures for wtp functions, supporting generalized additive decompositions (Section 3). We describe our auction mechanism in Section 4,
followed by a detailed example in Section 5, and study the mechanisms allocational, computational, and practical properties in Section 6. We present our simulation framework in
Section 7, and discuss the experimental results in Section 8.

2. Background
In this section we provide essential background on multiattribute preferences (Sections 2.1
and 2.2) and on multiattribute auctions (Section 2.3).
2.1 Multiattribute Preferences and Utility
Let  denote the space of possible outcomes, with ! a preference relation (weak total order)
over . Let A = {a0 , . . . , an }!be a set of attributes describing . Each attribute a  A has
a domain D(a), so that   ni=1 D(ai ). Capital letters denote subsets of attributes, small
latin letters (with or without numeric subscripts) denote specific attributes, and X = A\X.
 (and variations such as  " or ) indicate specific outcome in . An instantiation to a subset
of attributes Y is denoted using prime signs (as in Y " ) or numerical superscript (as in Y 1 ).
In particular, Y " is a projection on Y of some instantiations   . To represent an
instantiation of subsets X, Y at the same time we use a sequence of instantiation symbols,
as in X 1 Y 2 .
The preference relation ! over outcomes is usually represented numerically by a value
function v() (Keeney & Raiffa, 1976).
Definition 1 (Value Function). v :   % is a value function representing ! if for any
,  "  , v()  v( " ) iff  !  " .
Clearly, any monotonic transformation of v() is also a value function for !.
In many cases it is useful to represent, beyond a simple preference order over outcomes,
a notion of strength of preferences. A value function that expresses strength of preferences is
called a cardinal value function. A measurable value function is a well-established cardinal
 over pairs of outcomes.
value framework which posits the existence of a preference order !
"
"
"
"

For some  !  and  !  , the statement (,  ) ! (,  ) means that the strength of
preference of  " over  is greater than or equal to that of  " over . Krantz, Luce, Suppes,
and Tversky (1971) establish the set of axioms ensuring the existence of a utility function

representing !.
Definition 2 (Measurable Value Function). A measurable value function (MVF) is a
value function u :   %, such that for any ,  " , ,  "  , for which  !  " and  !  " , the
following holds:
 (,  " ).
u( " )  u()  u( " )  u()  (,  " ) !
(1)

481

fiEngel & Wellman

Hence the order over differences in values of u() correspond exactly to the order over
preference differences. Note that an MVF can also be used as a value function representing
 ( "" , ) iff  " !  "" , for any .
!, because ( " , ) !
In auction theory and mechanism design, traders preferences are usually represented
using a quasi-linear value function, such as v(, p) = u()+p, where p represents a monetary
outcome.1 The cardinal value function u() expresses strength of preference, in that the
difference u( " )  u( "" ) corresponds to the additional amount a trader is willing to pay
for  " relative to  "" . For example, if  " represents a red Mercedes with a sunroof, and  ""
denotes a blue Subaru with no sunroof, then u( " )  u( "" ) is the strength of preference for
the Mercedes configuration over the Subaru. If the Mercedes costs p" and the Subaru p"" ,
then according to v(, p) the trader prefers the Mercedes deal iff u( " )  u( "" )  p"  p"" .
In fact, u() can be easily shown to be an MVF, where the preference differences correspond to differences in willingness-to-pay (Engel & Wellman, 2007). For this reason, we
use MVF as the basis for utility in this work, and assume that traders willingness-to-pay
(wtp) functions constitute an MVF.
Reasoning over full outcomes is hard in several ways. Most notably, it is difficult for
humans to compare outcomes over many dimensions, and complex for machines to store
and analyze preferences over a number of outcomes that is exponential in the number of
attributes. It is therefore useful to consider preferences over the joint product of some
Y  A, considering the rest of the attributes Y fixed on some predefined values. Such an
order is also often referred to as a ceteris paribus preference orderone partial outcome is
preferred to another all else being equal.
Definition 3 (Conditional Preference). Partial outcome Y 2 is conditionally preferred
"
"
"
to partial outcome Y 1 given Y , if Y 1 Y ! Y 2 Y . The conditional preference order over Y
"
"
"
given Y is denoted by !Y ! , hence Y 1 Y ! Y 2 Y is abbreviated Y 1 !Y ! Y 2 .
In general, conditional preferences may depend on the particular assignment chosen for
the rest of the attributes. More precisely, if Y 1 Y ! Y 2 , we could still find that Y 2 Y !! Y 1
""
"
for some Y ,= Y . When this is the case, one needs to maintain both conditional preference
orders !Y ! and !Y !! , and hence in general this scheme might not yield any computational
benefits. Fortunately, in many cases one can identify subsets Y for which this preference
reversal does not occur, that is the preference order over Y is invariant to the instantiation
of Y .
Definition 4 (Preferential Independence). Y is preferential independent (PI) of Y ,
"
""
written PI(Y, Y ), if for any Y 1 and Y 2 , and for any Y , Y , we have Y 1 !Y ! Y 2 iff Y 1 !Y !!
Y 2.
First-order preferential independence (FOPI), independence of a single attribute from
the rest, is a natural assumption in many domains. For example, in typical purchase decisions greater quantity or higher quality is more desirable regardless of the assignments
to other attributes. Preferential independence of higher order, however, requires invariance
of the tradeoffs among some attributes with respect to variation in others, a more stringent independence condition. The MPI condition, defined below, is over the global set of
attributes A, and requires all possible subsets to be PI.
1. We use the term trader when referring to either buyers or sellers.

482

fiGAI Auctions

Definition 5 (Mutual Preferential Independence). Attributes A are mutually preferential independent (MPI) iff for all Y  A, P I(Y, Y ).
Preferential independence can greatly simplify the form of v.
Theorem 1 (Debreu, 1959). A preference order over set of attributes A can be represented
by an additive value function
v(a1 , . . . , an ) =

n
"

vi (ai ),

i=1

iff A is mutually preferential independent.
Dyer and Sarin (1979) extend additivity theory to MVF, and specify the conditions
under which u() as well has an additive structure as above. Effectively, additive forms
used in trading mechanisms assume MPI over the full set of attributes, including the money
attribute. Intuitively that means willingness-to-pay for levels of an attribute or attributes
cannot be affected by the instantiation of other attributes. This sweeping condition rarely
holds in practice (Von Winterfeldt & Edwards, 1986). Therefore, recent AI literature often
relaxes the MPI assumption by imposing additivity only with respect to subsets of attributes
which may overlap.
Definition 6 (Generalized Additive
Independence). Let I1 , . . . , Ig be (not necessarily
#
disjoint) subsets of A, such that gi=1 Ii = A. The elements I1 , . . . , Ig are called generalized
additive independent (GAI) if there exist functions f1 , . . . , fg such that,
u(a1 , . . . , an ) =

g
"

fr (Ir ).

(2)

r=1

2.2 Related Work on Generalized Independence
Our definition of GAI is somewhat nonstandard. Most literature defines a GAI condition
for the expected utility function (von Neumann & Morgenstern, 1944). In that well-known
model, a particular choice results in a lottery, that is a probability distribution over outcomes. The expected utility function represents a complete preference order over lotteries.
Informally, the GAI definition requires preferences on lotteries over  to depend only on
the margins over the subsets I1 , . . . , Ig . The form of Eq. (2) is a result of that definition,
obtained by Fishburn (1967). Fishburn not only introduces the functional decomposition,
but also provides a well-defined form for the functional constituents f1 , . . . , fg . Graphical
models and elicitation procedures for GAI decomposable utility were developed within the
expected utility framework (Bacchus & Grove, 1995; Boutilier, Bacchus, & Brafman, 2001;
Gonzales & Perny, 2004; Braziunas & Boutilier, 2005). In addition, generalized additive
utility models have been employed by Hyafil and Boutilier (2006) as an aid in direct revelation mechanisms, and by Robu, Somefun, and La Poutre (2005) for opponent modeling
in bilateral multi-item negotiation.
Bacchus and Grove (1995), who in fact coined the term GAI, show that the decomposition can also be obtained as a result of a collection of local, easier to detect, binary
483

fiEngel & Wellman

independence conditions. More specifically, they rely on a form called conditional additive
independence, which, informally, corresponds to a GAI decomposition limited to two (overlapping) subsets X  A and Y  A. They prove that this condition can be expressed as
a separation criterion on a graph whose nodes correspond to A, by the means of a perfect
map (Pearl, 1988). Crucially, the utility function decomposes to GAI form over lower dimensional functions, each defined on a maximal clique of the graph. When combined with
Fishburns work, their result provides a well-defined functional form that can be obtained
from a collection of conditional additive independence conditions. This result relies on the
form of lotteries as a basis for the utility function and the independence conditions.
The expression of willingness-to-pay requires a cardinal measure of preferences, yet
without uncertainty, there is no need for an expected utility representation. We therefore
invoke the MVF framework, and in Section 3, build on the additive decompositions for MVF
developed by Dyer and Sarin (1979) to develop multiattribute preference structures for wtp.
This development enables us to follow the footsteps of Fishburn (1967) and Bacchus and
Grove (1995) and show that a well-defined GAI form for MVF can also be obtained using
a collection of easy-to-detect binary independence conditions.
2.3 Multiattribute Auctions
The distinguishing feature of a multiattribute auction is that the goods are defined by
vectors of attributes. As above, we use A to denote a set of attributes describing the domain
. A configuration is a particular attribute vector,   . Multiattribute auctions are used
primarily for procurement, as part of strategic sourcing processes (Sandholm, 2007). In
the procurement model there is a single buyer, who has a utility function (representing
willingness-to-pay) ub () for purchasing   . There are m sellers s1 , . . . , sm with utility
functions ci :   %, representing the cost for si to supply configurations in  to the buyer.
Definition 7 (Multiattribute Allocation Problem). The multiattribute allocation problem (Parkes & Kalagnanam, 2005) is:
MAP =

max

i{1,...,m},

ub ()  ci ().

(3)

An allocation (si ,   ) solving MAP is said to maximize the surplus of the procurement
problem.
MAP can be decomposed to two subproblems: first find the most efficient configuration
for each trader, and then find the trader whose efficient configuration yields the highest
surplus. We call the first part the multiattribute matching problem (Engel, Wellman, &
Lochner, 2006).
Definition 8 (Multiattribute Matching Problem). The multiattribute matching problem (MMP) for a buyer b and a seller si is:
MMP(b, si ) = arg max ub ()  ci ().


We also call a configuration   selected by MMP(b, si ) a bilaterally efficient configuration
for si .
484

fiGAI Auctions

Most of the theoretical work on surplus-maximizing multiattribute auctions relates in
some way to the foundational work by Che (1993). In Ches model, the good or service
is characterized by a single quality attribute, and each seller has an independent private
cost function over quality. The buyer announces a scoring rule to the sellers, by which
price-quality offers will be evaluated. Che suggests several types of auctions, including
the second-score auction, where the seller bidding highest score wins, and must provide a
combination of price and quality that achieves the second-best score. In the second-score
mechanism, bidding truthfully is an equilibrium in dominant strategies. In particular, Che
shows that sellers bid on the quality that maximizes the difference between the buyers
scoring rule and their own cost function; in other words, on the respective MMP solution.
Branco (1997) generalizes Ches model and some of his results to correlated costs.
This basic model was later generalized by several authors to account explicitly for multiple quality attributes, and usually restricting the scoring rule to be additive over the
attributes (Bichler, 2001; David et al., 2002). Vulkan and Jennings (2000) suggest a modified version of English auctions (iterative auctions that require new bids to increment
over current bid price) under which bidders are required to improve current score, rather
than price. Sandholm and Suri (2006) consider the incorporation of non-price attributes in
multi-item (combinatorial) auctions.
The literature surveyed above emphasizes that auctions require the buyer to reveal a
scoring function prior to bidding. In order to achieve economic efficiency, this scoring
function must convey the buyers full utility function ub (). This is a major obstacle to
practical adaption of these mechanisms. Procurement auctions are rarely an isolated event,
and the buyer-supplier relationships usually evolve and change over time, during which
suppliers may retain some market power, and take advantage of the information revealed
by the buyer. Events are sometimes conducted on a recurrent basis, and several events may
be conducted for related goods with correlated valuations. In addition, the buyer may wish
to keep secret the way her utility may be discriminating for or against particular suppliers
(Koppius, 2002).
As noted in Section 1, Parkes and Kalagnanam (2005) suggest an alternative approach,
which employs prices over the space of configurations to drive traders to the efficient configurations. Auction NLD maintains a price for each   , and sellers bid for such full
configuration in each round. Auction AD maintains a price for each level a"i  D(ai ). Prices
are initially set to be high. In each round, sellers bid on a particular value for each attribute,
and the auction selects a set of levels (again, per attribute) which are myopically buyer preferred in that round, that is, approximately maximize the buyers utility with respect to
current prices. In addition, the auction maintains a discount factor that is applied to ensure
that a single seller is eventually selected. The price of a configuration is defined as the sum
of the prices of the chosen attribute levels minus the discount. After each round, prices of
particular levels of particular attributes are decremented by a constant ", according to a set
of price change rules, ensuring that the auction ultimately converges to an efficient solution.
Both auctions are shown to obtain optimal surplus (up to "-proportional error), when
all sellers bid myopically rather than strategically (we define this concept formally in Section 6.1). The myopic behavior is shown to be an ex-post Nash equilibrium. Auction NLD
is fully expressive but not tractable when the number of attributes is large. Auction AD
is computationally efficient, but its expressiveness is limited to additive preferences (see
485

fiEngel & Wellman

discussion following Theorem 1). When traders preferences are not additive, the welfare
achieved by the auction is not necessarily optimal; that is, it does solve MAP optimally,
but with respect to inaccurate utility functions. Moreover, it is not clear how this lack of
expressiveness may affect the incentives of traders to act strategically.
Theoretically, one could also use the well-known Vickrey-Clake-Grove (VCG) mechanism. Parkes and Kalagnanam define the sell-side VCG mechanism: all traders submit
their full utility or cost functions, MAP is solved by the auction engine, and the winning
seller pays according to her VCG price (definition of this pricing is provided in Section 6.1).
In such an auction, traders can be allowed to use any compact preference structure, including GAI. However, this scheme suffers from the same disadvantages as any of the proposals
that require full revelation of utility.
To summarize, no previously suggested surplus-maximizing multiattribute procurement
auction is at the same time expressive (accommodates interdependencies between attributes),
tractable (its computations do not depend on the fully exponential domain), and preserving
of the buyers private information, meaning (minimally) that it does not require the buyer
to reveal a full utility function before extracting any bids from sellers. Our proposed mechanism, as we show theoretically and using simulations, possesses attractive properties on
all these criteria.

3. Detection of GAI Structure for Measurable Value Functions
In this section we provide the basis for the application of GAI decomposition in procurement
problems. In Section 3.1 we show how GAI can be obtained as a collection of local, weaker
conditions which are based on invariance of willingness-to-pay. In Section 3.2 we use an
example to demonstrate how this process can be used in procurement problems.
3.1 Difference Independence and GAI
Dyer and Sarin (1979) introduce for measurable value an analog to additive independence,
called difference independence. Our first step is to introduce a conditional generalization of
their definition.
Definition 9 (Conditional Difference Independence). Let X, Y  A and X  Y = ,
and define Z = A \ X  Y . X is conditionally difference independent of Y , denoted as
CDI(X, Y ), if for any Z "  D(Z), and for any X 1 , X 2  D(X), Y 1 , Y 2  D(Y ),
(X 1 Y 1 Z " , X 2 Y 1 Z " ) 
 (X 1 Y 2 Z " , X 2 Y 2 Z " ),

(4)

 and 1 both hold.
where the symbol 
 indicates that !
By the definition of MVFs (1), the CDI condition (4) can be expressed equivalently in
terms of measurable value:
u(X 1 Y 1 Z " )  u(X 2 Y 1 Z " ) = u(X 1 Y 2 Z " )  u(X 2 Y 2 Z " )
This condition states that the value, or willingness-to-pay, for a change in the assignment
to X does not depend on the current assignment of Y , for any fixed value of Z.
A CDI condition leads to a convenient decomposition of the MVF.
486

fiGAI Auctions

Lemma 2. Let u(A) be an MVF representing preference differences, with X, Y, Z as specified in Definition 9. Then CDI(X, Y ) iff
u(A) = u(X 0 , Y, Z) + u(X, Y 0 , Z)  u(X 0 , Y 0 , Z),
for any arbitrary instantiations X 0 , Y 0 .
With a single CDI condition, we can therefore replace the n-ary function u(X, Y, Z) with
two lower-dimensional functions u(X 0 , Y, Z) and u(X, Y 0 , Z). It is reasonable to assume
that one can apply more CDI conditions to further decompose the resulting functions. In
order to take full advantage of all existing CDI conditions, we introduce the notion of a
dependency graph, which is a simplification of the concept of perfect map mentioned in
Section 2.2.
Definition 10 (Dependency Graph). Let S denote a set, and R a binary relation over
2S . Then a graph G = (S, E) is a dependency graph for R if for any S1 , S2  S, it holds
that (S1 , S2 )  R iff for any a1  S1 and a2  S2 , (a1 , a2 ) 
/ E.
Hence the dependency graph expresses R as a separation criterion; two subsets have a
direct connection iff they are not in R. A dependency graph for CDI can be constructed
simply by removing any edge (a1 , a2 ) for which CDI({a1 }, {a2 }); this is because CDI(S1 , S2 )
holds iff CDI({a1 }, {a2 }) holds for any a1  S1 and a2  S2 . We use the term CDI map for
a dependency graph induced by a CDI relation.
The next theorem links the CDI condition, the CDI map, and a GAI decomposition
over A. In fact, it establishes that the functional constituents of GAI decomposition for
MVF are the same as the functional constituents of GAI decomposition for the expected
utility model, as defined by Fishburn (1967). We adopt the following conventional notation.
Let (a01 , . . . , a0n ) be a predefined vector called the reference outcome. For any I  A, the
function u([I]) stands for the projection of u(A) to I where the rest of the attributes are
fixed at their reference levels.
Theorem 3 (CDI-GAI Theorem). Let G = (A, E) be a CDI map for A, and {I1 , . . . , Ig }
a set of (overlapping) maximal cliques of G. Then
u(A) =

g
"

fr (Ir ),

(5)

r=1

where
f1 = u([I1 ]), and
for r = 2, . . . , g,

fr = u([Ir ]) +

r1
"

(6)
(1)j

j=1

"
1i1 <<ij <r

u([

j
$

Iis  Ir ]).

s=1

As a small example, Table 1 exhibits a utility function u(x1 , x2 , x3 ). Each of the three
attributes has a boolean domain, that is D(xi ) = {0, 1}. Let x0i and x1i denote the assignments 0 and 1 (respectively) to xi . We first observe that CDI({x1 }, {x3 }) holds because:2
2. Note that x02 and x12 correspond to Z ! in Definition 9.

487

fiEngel & Wellman

x1
0
1
0
1
0
1
0
1

x2
0
0
1
1
0
0
1
1

x3
0
0
0
0
1
1
1
1

u(x1 , x2 , x3 )
0
5
2
6
3
8
7
11

u(x1 , x2 , x03 )
0
5
2
6
0
5
2
6

u(x01 , x2 , x3 )
0
0
2
2
3
3
7
7

u(x01 , x2 , x03 )
0
0
2
2
0
0
2
2

u1 (I1 )
0
5
2
6
0
5
2
6

u2 (I2 )
0
0
0
0
3
3
5
5

Table 1: A utility function over three attributes, decomposable via GAI into the sum of two
functions of two attributes each. u1 () depends only on {x1 , x2 } and u2 () depends only on
{x2 , x3 }.

1. The utility difference on values of x1 given x02 is 5, for both x03 and x13 . More explicitly,
u(x11 , x02 , x03 )  u(x01 , x02 , x03 ) = 5  0 = 5, and u(x11 , x02 , x13 )  u(x01 , x02 , x13 ) = 8  3 = 5.
2. Similarly, the difference on x1 given x12 is 4, for both x03 and x13 .
Though x1 and x3 are CDI of each other, we see that both depend on x2 . For example, the
differences mentioned above for x1 are 5 and 4 given x02 and x12 (respectively), hence the
difference on x1 given fixed x3 depends on the value of x2 . The CDI map for this example
therefore includes an edge (x1 , x2 ) and an edge (x2 , x3 ). The maximal cliques of this graph
are I1 = {x1 , x2 } and I2 = {x2 , x3 }.
To obtain the numeric decomposition, we first define (x01 , x02 , x03 ) as reference values.
Next, from (6), we get u1 (I1 ) = u([I1 ]) = u(x1 , x2 , x03 ) and u2 (I2 ) = u([I2 ])  u([I1  I2 ]) =
u(x01 , x2 , x3 )  u(x01 , x2 , x03 ). The functions involved are given in Table 1. Note that the
fifth and sixth columns are obtained from the appropriate values of the fourth column; for
example, u(x01 , x2 , x3 ) for the line x1 = 1, x2 = 1, x3 = 0 is the value u(x1 , x2 , x3 ) in the line
x1 = 0, x2 = 1, x3 = 0. It is easy to verify that indeed u(x, y, z) = u1 (I1 ) + u2 (I2 ).
The CDI-GAI Theorem provides an operational form of GAI, by establishing a GAI
decomposition that can be obtained from a collection of simple CDI conditions. The assumption or detection of CDI conditions can be performed incrementally, until the MVF is
decomposed to a reasonable dimension. The CDI conditions, in turn, based as they are on
invariance of preference differences, are relatively intuitive to detect. This is particularly
true when the differences carry a direct interpretation, as in the case of willingness-to-pay:
we can check invariance of the monetary amount a buyer is willing to pay to get one outcome
over the other.
The GAI decomposition can be depicted graphically using a clique graph of the CDI
map, that is, a graph whose nodes correspond to maximal cliques of the CDI map. For our
purposes it is convenient to use a particular clique graph called a tree decomposition (or
junction tree). We introduce this well-known concept, and discuss its implications for GAI
representation.
Definition 11 (Tree Decomposition). A tree decomposition for a graph G = (N, E) is
a pair (T, I), where T = (, E) is an acyclic graph, I = {Ii | i  } is a collection of
488

fiGAI Auctions

term
MAP
MMP
MVF
PI
FOPI
MPI
GAI
CDI
CDI map
GAI tree

Meaning
Multiattribute Allocation Problem
Multiattribute Matching Problem
Measurable Value Function
Preferential Independence
First-Order Preferential Independence
Mutual Preferential Independence
Generalized Additive Independence
Conditional Difference Independence
graph whose separation criterion is CDI
tree decomposition of a CDI map

Reference
(Parkes & Kalagnanam, 2005)
(Engel et al., 2006)
(Dyer & Sarin, 1979)
(Keeney & Raiffa, 1976)
(Keeney & Raiffa, 1976)
(Bacchus & Grove, 1995)
cf. (Bacchus & Grove, 1995)
cf. (Gonzales & Perny, 2004)

Table 2: Acronym terms, with references to related literature. Empty references indicate
terms introduced in this work. The terms are arranged according to topics: (i) multiattribute economic problems, (ii) independence relations, (iii) graphical concepts.
#
subsets of N , each corresponding to a node in T , and (i) iI Ii = N , (ii) for each edge
(n1 , n2 )  E, there exists Ii such that n1 , n2  Ii , and (iii) (running intersection) for any
i, j, k  , if j is on the path from i to k in T then Ii  Ik  Ij .
Any graph can be tree-decomposed, typically in more than one way. For example, there
can be a single node in I. The width of a tree decomposition is maxiI |Ii |  1, and the
treewidth of a graph is the minimum width among all its possible tree decompositions.
It is easy to show that any maximal clique of G is contained within some i  I. Therefore, by Theorem 3, a utility function decomposes additively over the subsets I = {Ii | i 
}, where T = (, E) is a tree decomposition of the CDI map. The notion of GAI tree is
adapted from the work of Gonzales and Perny (2004), who introduce GAI graphical models
for the expected utility framework.
Definition 12 (GAI Tree). A GAI tree for u() is a tree decomposition of the CDI map
of u().
We therefore refer to the elements I1 , . . . , Ig of a GAI decomposition as the set I of a
tree decomposition. The next subsection provides a qualitative example of the CDI concept,
its dependency graph, and corresponding GAI tree.
The results of this section lay out the foundations for using GAI decomposition in
multiattribute trading mechanisms. The results generalize additive MVF theory, and justify
the application of methods developed under the expected utility framework (Bacchus &
Grove, 1995; Boutilier et al., 2001; Gonzales & Perny, 2004; Braziunas & Boutilier, 2005)
to representation of monetary value under certainty. Table 2 summarizes the acronym
terminology introduced up to this point.
3.2 Employing GAI in Procurement
In this section we demonstrate the process of obtaining a GAI decomposition from the
collection of CDI conditions. In addition, this example is used to motivate our approach
489

fiEngel & Wellman

in comparison to the work of Parkes and Kalagnanam (2005). Consider a procurement
department that wishes to purchase new hard drives (HD) for the desktops of a large
number of employees. The buyer cares about several characteristics (attributes) of the hard
drives and the particular terms of the procurement contract. Each attribute is listed with a
designated attribute name (the first letter), and its domain. In some cases (e.g., attribute
I) we use arbitrary symbols to represent domain elements, abstracting from the meaningful
interpretation they are assumed to have in context.
RPM (R) 3600, 4200, 5400 RPM
Transfer rate (T) 3.4, 4.3, 5.7 MBS
Volume (V) 60, 80, 120, 160 GB
Supplier ranking (S) 1, 2, 3, 4, 5
Quality rating (Q) (of the HD brand) 1, 2, 3, 4, 5
Delivery time (D) 10, 15, 20, 25, 30, 35 days
Warranty (W) 1, 2, 3 years
Insurance (I) (for the case the deal is signed but not implemented) 1 , 2 , 3
Payment timeline (P) 10, 30, 90 days
Compatibility (C) (with existing hardware and software) 1 , 2 , 3
Consider, for example, the pair of attributes Quality and Warranty. The value of warranty is different for different values of quality; it is higher when the quality is known to
be low, and lower when the quality is known to be high. The two attributes therefore depend on each other. Furthermore, we might expect that Volume complements both Quality
and Warranty. Larger hard drives are more prone to failures, making the quality and warranty more valuable. Similarly, there is interdependence between Supplier ranking and the
contract insurance we buy, and between Supplier ranking and the warranty the supplier provides. Other reasonable dependencies are among Delivery, Insurance, and Payment timeline
(e.g., later delivery requires better insurance, later payment reduces the need for insurance),
and between Volume to the RPM and Transfer rate. Preferences over compatibility may
not depend on any other attribute. The corresponding CDI map is depicted in Figure 1a.
As described in Section 2.1, the utility function decomposes over the elements of a tree
decomposition of the CDI map. Such a tree decomposition is depicted in Figure 1b. In this
example the set of elements of the tree decomposition correspond exactly to the maximal
cliques of the CDI map. In general the tree decomposition might include supersets of the
maximal cliques, but the decomposition can obviously be maintained over the supersets as
well.
Non-additive traders, if required to deal with an additive price space as in auction
AD (Parkes & Kalagnanam, 2005), face an exposure problem, somewhat analogous to traders
with combinatorial preferences that participate in simultaneous auctions (Wellman, Osepayshvili, MacKie-Mason, & Reeves, 2008). Essentially, the problem can manifest itself
490

fiGAI Auctions

(a)

(b)

Figure 1: HD procurement problem: (a) CDI map, (b) GAI tree.
in two ways. One type of exposure occurs from one auction round to another, as in the
following two-attribute example. A sellers conditional preference order over an attribute
a may be optimized at an assignment a1 given that the other attribute b is at b1 , but if
the assignment of b changes, a1 may become arbitrarily suboptimal. Therefore bidding a1
and b1 may result in a poor allocation if the seller is outbid on b1 (and thus must resort to
another assignment) but left winning a1 . The second exposure occurs in any single round
of the auction, if a trader bids on multiple configurations. For example, suppose configurations (a1 , b1 ) and (a2 , b2 ) are both optimal at the current prices. Because bids are collected
independently for each attribute, a trader bidding on both may end up with configuration
(a1 , b2 ), which again, may be arbitrarily suboptimal.
We can prevent exposure on the sellers part by taking simple measures in the auction
design. First, bids are collected anew each round, independently of previous rounds, hence
the first problem is avoided. Sellers can likewise avoid the second problem by limiting
themselves to bid on one configuration per round.
On the buyers side, this solution does not work because we require the buyer to bid
a full set of optimal configurations in each round, in order to ensure the auctions convergence (this becomes clearer in Section 6.1). To prevent buyer exposure, our auction design
structures prices according to the buyers preferences, and the traders bid on clusters of
interdependent attributes. In terms of the example above, if a and b are interdependent
(meaning CDI({a}, {b}) does not hold), we should be able to bid on the cluster ab. If b in
turn depends on c, we need another cluster bc. This is still simpler than a general pricing
structure that solicits bids for the cluster abc. More generally, we find all reasonable CDI
conditions which are correct for the buyer, obtain the corresponding GAI tree decomposition, and solicit bids for clusters of attributes corresponding to these GAI elements. In
Section 4, we describe our auction design in detail, along with an example in Section 5. In
Section 6.1, we prove that the auction terminates with an (approximately) optimal solution
to MAP.

4. GAI Auctions
Before introducing our auction design, we reiterate our model and notation, and provide a
definition that facilitates the auction presentation.

491

fiEngel & Wellman

4.1 Notations and Definitions
In the procurement setting, a single buyer wishes to procure a single good, in some configuration    from one of the candidate sellers s1 , . . . , sm . The buyer has some private
valuation function ub :   %+ , and similarly each seller si has a private cost function, ci .
Both ub () and ci () are MVFs, for which utility differences express differences in willingnessto-pay, as explained in Section 2.1. Assume that the buyers preferences are reflected in a
GAI structure I1 , . . . , Ig . We call an assignment to GAI element Ir a sub-configuration. We
use r to denote the sub-configuration formed by projecting configuration  to element Ir .
Definition 13 (Consistent Cover). A collection of sub-configurations {1 , . . . , g }, where
for each r  {1, . . . , g}, r is an instantiation of Ir , is a consistent cover if for any
r, r "  {1, . . . , g}, and any attribute aj  Ir  Ir! , r and r! agree on the assignment
to aj .
In words, a consistent cover is a collection of sub-configurations from which we can
compose a valid configuration. A collection {1 , . . . , g } which is a consistent cover can
equivalently be considered a configuration, which we denote by (1 , . . . , g ). For example,
consider a good with three attributes: a, b, c. Each attributes domain has two possible
assignments (e.g., {a1 , a2 } is the domain of a). Let the GAI structure be I1 = {a, b}, I2 =
{b, c}. Here, sub-configurations are assignments of the form a1 b1 , a1 b2 , b1 c1 , and so on. The
set of sub-configurations {a1 b1 , b1 c1 } is a consistent cover, corresponding to the configuration
a1 b1 c1 . In contrast, the set {a1 b1 , b2 c1 } is inconsistent.
4.2 The GAI Auction
We define an iterative, descending-price multiattribute auction that maintains a GAI pricing structure: that is, in any round t, there is a price pt (), corresponding to each subconfiguration of each GAI element. The price pt () of a configuration  at round t is
defined in terms of the sub-configuration prices and a global discount term ,
t

p () =

g
"

pt (r )  .

(7)

r=1

Importantly, the elements r may refer to overlapping attributes. Bidders submit subbids on sub-configurations and on the global discount .3 Sub-bids are submitted in each
round and they expire in the next round. A sub-bid in round t for configuration r is
automatically assigned the price pt (r ). The set of full bids of a seller contains all consistent
covers that can be generated from that sellers current set of sub-bids. The existence of a
full bid over a configuration  represents the sellers willingness to accept the price pt () for
supplying .
At the start of the auction, the buyer reports (to the auction, not to sellers) a complete
valuation function ub (). Under GAI, this can be expressed
in decomposed form (6) with
%
local functions (fb,1 , . . . , fb,g ), such that ub () =
r fb,r (r ). The initial prices of subconfigurations are set at some level above the buyers valuations, that is, p1 (r ) > fb,r (r ) for
all r . The discount  is initialized to zero. The auction has the dynamics of a descending
3. The discount term could be replaced with a uniform price reduction across all sub-configurations.

492

fiGAI Auctions

clock auction: at each round t, bids are collected for current prices and then prices are
reduced according to price rules. A seller is considered active in a round if the set of subbids she submitted contains at least one full bid. In round t > 1, only sellers who were
active in round t  1 are allowed to participate, and the auction terminates when no more
than a single seller is active. We denote the set of sub-bids submitted by si by Bit , and the
corresponding set of full bids is
Bit = { = (1 , . . . , g )   | {1 , . . . , g }  Bit }.
In the example of Section 4.1, a seller could submit sub-bids on a set of sub-configurations
such as {a1 b1 , b1 c1 }, and that combines to a full bid on a1 b1 c1 .
The auction proceeds in two phases. In the first phase (A), at each round t the auction
computes a set of buyer-preferred sub-configurations Mt : those sub-configurations that are
part of a configuration which is within " of being profit-maximizing for the buyer at the
current prices. Formally, we first define the buyer profit from a configuration  as4
bt () = ub ()  pt ().
The buyer-preferred set of sub-configurations is then defined by:
Mt = {r | bt ()  max
bt ( " )  ", r = 1, . . . , g}.
!
 

In Section 6.2 we show how Mt can be computed efficiently. We stress that though Mt
is a set of sub-configurations, the criterion for selecting them is based on the profit over
full configurations. Profits over individual sub-configurations are meaningless outside the
context of configurations.
In Phase A, the auction adjusts prices after each round, reducing the price of every
sub-configuration that has received a bid but is not in the buyers preferred set. Let " be
the prespecified price
parameter. Specifically, the Phase A price change rule is
#mdecrement
t
t
applied to all r  i=1 Bi \ M :
"
pt+1 (r )  pt (r )  .
g

[A]

Let M t denote the set of configurations that are consistent covers in Mt :
M t = { = (1 , . . . , g )   | {1 , . . . , g }  Mt }.
The auction switches to Phase B when all active sellers have at least one full bid in the
buyers preferred set:
i. Bit =   Bit  M t ,= .
[SWITCH]
Let T be the round at which [SWITCH] becomes true. At this point, the auction selects
the buyer-optimal full bid i for each seller si .
i = arg max (bT ()).
BiT

(8)

4. We drop the t superscript in generic statements involving price and profit functions, understanding that
all usage is with respect to the (currently) applicable prices.

493

fiEngel & Wellman

In Phase B, si may bid only on i . Sub-configuration prices are fixed at pT () during this
phase. The only adjustment is to , which is increased in every round by ". By (7), any
increase of  decreases the current price of each of the configurations i . The auction
terminates when at most one seller (if exactly one, designate it si ) is active. The allocation
is determined according to four distinct cases:
1. All sellers drop out in Phase A (i.e., before rule [SWITCH] holds). The auction
terminates with no allocation.
2. All active sellers drop out in the same round in Phase B. Of all the sellers that
dropped in the last round, the auction selects a seller si for which ub (i )  pT (i )
is maximal, and designates that seller as the winner si . With a single winner, the
appropriate case 3 or 4 is applied.
3. The auction terminates in Phase B with a final price above the buyers valuation,
pT (i )   > ub (i ). It is still possible that there is exactly one seller (the winning
seller) whose cost is below the buyers valuation, in which case a trade with positive
surplus is possible. Therefore, the auction offers the winner si an opportunity to
supply i at price ub (i ).
4. The auction terminates in Phase B with a final price pT (i )    ub (i ). This is the
ideal situation, where the auction allocates the chosen configuration and seller at this
resulting price.
Collect a reported valuation, ub () from the buyer;
Set high initial prices, p1 (r ) on each sub-configuration r , and set  = 0;
while not [SWITCH], and at least one active seller do
Collect sub-bids from sellers;
Compute Mt ;
Apply price change by [A];
end
Compute i ;
while more than one active seller do
Increase  by ";
Collect bids on (i , ) from sellers;
end
Implement allocation and payment to winning seller;
Algorithm 1: GAI-based multiattribute auction.
The overall auction is described by high-level pseudocode in Algorithm 1. The role
of Phase A is to guide the traders to their efficient configurations (MMP solutions), by
reducing prices on configurations that are chosen by at least one seller but not preferred by
the buyer. The price reduction makes such configurations slightly less attractive to the seller
and slightly more attractive to the buyer. Phase B is a one-dimensional competition over
the profit that remaining seller candidates can provide to the buyer. In the next section
we formalize these notions, and prove that Phase A indeed converges and that Phase B
494

fiGAI Auctions

I1
fb
f1
f2

a1 b1
65
35
35

a2 b1
50
20
20

I2
a1 b2
55
30
25

a2 b2
70
70
25

b1 c1
50
65
55

b2 c1
85
65
110

b1 c2
60
70
70

b2 c2
75
61
95

Table 3: GAI utility functions for the example domain. fb represents the buyers valuation,
and f1 and f2 costs of sellers s1 and s2 .

selects a seller whose efficient configuration yields (approximately) the highest surplus. In
Section 6.2 we discuss the computational tasks associated with the auction.

5. GAI Auction Example
We illustrate the auction with a simple three-attribute scenario, employing the two-element
GAI structure I1 = {a, b}, I2 = {b, c}. Table 3 shows the GAI utilities for the buyer
and the two sellers s1 , s2 . The efficient allocation is (s1 , a1 b2 c1 ): the buyers valuation is
55+ 85 = 140 and the cost of s1 for this configuration (boldface in the table) is 30+ 65 = 95,
hence the surplus is 45. The maximal surplus of the second-best seller, s2 , is 25, achieved
by a1 b1 c1 , a2 b1 c1 , and a2 b2 c2 . We set all initial prices over I1 to 75, all initial prices over
I2 to 90, and " = 8, meaning that price reduction for sub-configurations ("/g) is 4.
For the sake of the example we assume that each seller bids in each round on the configuration that maximizes her profit (price minus cost), with respect to prices of the current
round. In the next section we provide formal definitions and prove incentive properties for
this strategy.
Table 4 shows the progress of Phase A. Initially all configuration have the same price
(165), so sellers bid on their lowest-cost configurationa2 b1 c1 for both (with profit 80 to s1
and 90 to s2 )realized by sub-bids on a2 b1 and b1 c1 . M1 contains the sub-configurations
a2 b2 and b2 c1 of the highest value configuration a2 b2 c1 , which yields buyer profit of 10.
As we show in the next section (Lemma 7), this maximum does not change throughout
Phase A. Price is therefore decreased on a2 b1 and b1 c1 . After the price change, the profit
of s1 for a2 b1 c1 is 72, and because she has higher profit (74) on a1 b2 c2 she bids on a1 b2 and
b2 c2 . Now (round 2) their prices go down, reducing the profit on a1 b2 c2 to 66 and therefore
in round 3 s1 prefers a2 b1 c2 (profit 67). Note that at this point the configuration a2 b2 c2
yields profit of 16 to the buyer, which is within " of the maximal buyers profit (-10),
hence b2 c2 is marked to be in M3 .
After the next price change, the configurations a1 b2 c1 and a1 b2 c2 both become optimal
for s1 (profit 66), and the sub-bids a1 b2 , b2 c1 and b2 c2 capture the two. These configurations
stay optimal for another round (5), with profit 62. In round 5 the profit for configuration
a1 b2 c1 is 140  157 = 17, which is within " of maximizing the buyers profit, therefore
the sub-configuration a1 b2 is added to M5 . At this point s1 has a full bid (in fact two full
bids: a1 b2 c2 and a1 b2 c1 ) in M 5 , and she no longer changes her bids because the price of
her optimal configurations does not decrease. Seller s2 however sticks to a2 b1 c1 during the

495

fiEngel & Wellman

t
1

a1 b 1
75

2

75

3

75

4

75

5

75
s2
71

6
7
8
9

I1
a2 b 1
75
s 1 , s2
71
s2
67
s 1 , s2
63
s2
59

71
s2
67

67
, s2

59
s2
55
55
s2
51

a1 b 2
75
75
s1
71
71
s1
67
, s1
67
, s1
67
, s1
67
, s1
67
, s1

a2 b 2
75

75

75

75

75

75

75

75

75


b 1 c1
90
s 1 , s2
86
s2
82
s2
78
s2
74
s2
70
70
s2
66

66
, s2

I2
b 2 c1
90

90

90

90
, s1
90
, s1
90
, s1
90
, s1
90
, s1
90
, s1

b 1 c2
90

b 2 c2
90

90

90
s1
86

86
, s1
86
, s1
86
, s1
86
, s1
86
, s1
86
, s1

90
s1
86
86
86
s2
82
82
s2
78

Table 4: Auction progression in Phase A. Sell bids and designation of Mt (using ) are
shown below the price of each sub-configuration.

first four rounds, switching to a1 b1 c1 in round 5. It takes four more rounds for s2 and Mt
to converge (M9  B29 = {a1 b1 c1 }).
After round 9, the auction sets 1 = a1 b2 c1 (which yields more buyer profit than a1 b2 c2 )
and 2 = a1 b1 c1 . In the second phase, which starts at this point, the sellers compete on the
amount of surplus they transfer to the buyer, whose profit consequently becomes positive.
For the next round (10)  = 8, increased by 8 for each subsequent round. Note that
p9 (a1 b1 c1 ) = 133, and c2 (a1 b1 c1 ) = 90, therefore the profit of s2 at this point is 43. In
round 15,  = 48 meaning p15 (a1 b1 c1 ) = 85 and that causes s2 to drop out because his
profit becomes negative. This ends the auction, and sets the final allocation to (s1 , a1 b2 c1 )
and pT (a1 b2 c1 ) = 157  48 = 109. That leaves the buyer with a profit of 31 and s1 with a
profit of 14.

6. Analysis
We analyze the economic properties of the auction in Section 6.1, and address practical and
computational issues in Section 6.2.
6.1 Economic Properties
We adopt the following assumptions for this discussion:
A1 The optimal (seller, configuration) pair provides non-negative surplus.
A2 ub () is the real utility function of the buyer.
496

fiGAI Auctions

When the optimal solution to MAP (3) provides negative surplus and sellers do not bid
below their cost, the auction terminates in Phase A, no trade occurs, and the auction is
trivially efficient. Therefore Assumption A1 does not cause loss of generality. A2 can be
interpreted as follows: given non-truthful buyer report, our efficiency results below apply
to the face value of the buyers report rather than to the true utility.
6.1.1 Properties of the buyers profit function
For   {1, . . . , g}, we define the (partial) profit from a set of of sub-configurations 
corresponding to  as
"
b ( ) =
(fb,r (r )  p(r )).
r

The functions f come from the GAI breakdown of ub as in (6).
Lemma 4. For any  and its complement ,
b () = b ( ) + b ( )
Proof. From (6) and from the definition of b ( ) we get
"
"
b () =
(fb,r (r )  p(r )) +
(fb,r (r )  p(r )) = b ( ) + b ( ).
r

r

In round 5 of the example in the previous section, the sub-configuration a1 b2 is placed in
M because the configuration a1 b2 c1 is within " of the maximal buyer profit 10. Actually,
at that point not only a1 b2 c1 is added to M t , but also a1 b2 c2 whose buyer profit (23) is
not within " of the maximum. If a1 b2 c2 is later selected as i for some si this could lead to
additional efficiency loss, beyond ". The following lemma bounds this potential loss.
Lemma 5. Let  be a set of configurations, all within " of maximizing profit for a trader 
(buyer or seller) at given prices. Let  = {r |   , r  {1, . . . , g}}. Then any consistent
cover in  is within g" of maximizing profit for  under the same prices.
In particular, if  includes only exactly optimal configurations, any consistent cover will
be exactly optimal as well. The proof (in Appendix B.1) relies on our definition of the GAI
decomposition as a tree decomposition, and uses the partial profit function defined above
along with Lemma 4.
The bound above is tight, in that for any GAI tree and nontrivial domain we can
construct an example set  as above in which there exists a consistent cover whose utility
is exactly g" below the maximal.
As a result we get the following corollary.
Corollary 6.
  M t . bt ()  max
bt ( " )  g"
!
 

497

fiEngel & Wellman

Proof. Apply Lemma 5 for bt : define  as the set of configurations within " of max!  bt ( " ).
Mt , by its definition, serves as  in the lemma. M t is then exactly the set of consistent
covers over , and hence each   M t must be within g" of the optimum max!  bt ( " ).
We now show that, as noted in the example, the maximal profit of the buyer does not
change during Phase A.
Lemma 7. max bt () = max b1 () for any round t of Phase A.
Proof. Assume there exists some  " for which bt+1 ( " ) > bt ( " ). Then necessarily pt+1 ( " ) =
pt ( " )   for some  > 0. The only price change is by Rule [A], meaning that some w  g
t
sub-configurations of  " are not in Mt , and  = w"
g . In that case, by definition of M ,
bt ( " ) < max bt ()  ".


Therefore,
bt+1 ( " ) =  t ( " ) +  =  t ( " ) +

w"
g"
  t ( " ) +
< max bt ().

g
g

This is true for any  " whose profit improves, therefore max bt () does not change
during Phase A, and hence equals its value in round 1.
6.1.2 Straightforward bidding sellers
We now turn our attention to the sellers behavior. We first define the profit function of
seller si by it () = pt ()  ci ().
Definition 14 (Straightforward Bidder). A seller is called a straightforward bidder (SB)
if at each round t she bids on Bit as follows: if max it () < 0, then Bit = . Otherwise
select bti  arg max it (), and set
Bit = {r |   bti , r  {1, . . . , g}}.
Intuitively, SB sellers follow a myopic best response strategy, optimizing profit with
respect to current prices. This approach was termed straightforward by Milgrom (2000)
in the sense that agents bid myopically, rather than strategically anticipating subsequent
price responses.
SB sellers can choose any optimal configuration to bid on; none of the results proved
below is affected by this choice. It is also important to note that SB sellers find their optimal
full configuration bti , rather than optimize each GAI element separately. The configuration bti
is translated to its set of sub-configurations Bit . In order to calculate bti , seller si needs to find
the optimum of her current profit function. In Section 6.2 we show that this optimization
problem is tractable under the assumption that ui (), too, has a compact GAI structure.
The following is an immediate corollary of the definition of SB.
Corollary 8. For SB seller si ,
t,   Bit . it () = max
it ( " ).
!
 

498

fiGAI Auctions

In general, sellers preference structure may not coincide with the auctions price structure. Nevertheless, Corollary 8 holds by definition of SB, because Bit (defined in Section 4.2)
contains a single configuration which is the submitted bid bti . Alternatively, the definition
of SB can be modified, so sellers with GAI preferences consistent with the auctions price
structure can bid on multiple optimal configurations (if such exist). If sellers bid on multiple
configurations, this can speed up convergence. In that case bti denotes a set of submitted
configurations, Bit denotes the respective collection of sub-configurations, and Bit is the set
of consistent covers over Bit . Lemma 5 (with " = 0) entails that Corollary 8 still holds.
However, for simplicity of the analysis we retain Definition 14.
6.1.3 Efficiency given SB
Lemma 9 states that through the price system and price change rules, Phase A leads the
buyer and each of the sellers to their mutually efficient configuration. Formally, we are
interested in maximizing the function i :   %, which represents the surplus ub ()  ci ().
For any prices pt ,
i () = bt () + it ().
Lemma 9. For SB seller si , i is g"-efficient:
i (i )  max i ()  g".


Proof. Configuration i is chosen to maximize the buyers profit out of Bit at the end of
Phase A. Because Bit  M t ,= , a configuration i  M t is available in Bit , hence one
must be chosen to maximize buyers utility. For any , and for any i  Bit , we get from
Corollary 8,
iT (i )  iT (),
and from Corollary 6, we get for any i  M t ,
bT (i )  bT ()  g".
Because i  Bit  M t we can add up the two inequalities and get i (i )  i ()  g", which
is the desired result.
Based on Phase Bs simple role as a single-dimensional bidding competition over the
discount, we next assert that the overall result is efficient under SB, which in turn (Section 6.1.4) proves to be an approximately ex-post equilibrium strategy in the two phases.
Theorem 10. Given a truthful buyer and SB sellers, the surplus of the final allocation is
within (g + 1)" of the maximal surplus.
Proof Sketch: we first establish that the auction must reach Phase B. To do that, we
show that in each round of Phase A, a price of at least one sub-configuration is reduced,
whereas by Lemma 7, max bt () does not change. The latter enforces a lower bound
on how far prices can be reduced within Phase A, hence Phase A must terminate. Because
initial prices are above the buyers valuation, a seller whose surplus (MMP solution) is
positive cannot drop during that phase, so using Assumption A1 we show that the only way
499

fiEngel & Wellman

for Phase A to terminate is by reaching condition [SWITCH]. Next, we show that for any
two sellers, the surplus of the first to drop from the auction cannot be significantly higher
than that of the one who stayed longer. This ensures that the winning seller is the efficient
one, or one whose MMP surplus is almost maximal, and from Lemma 9 the auction must
obtain (almost) all of that surplus. The full proof is given in Appendix B.2.
The bound guaranteed by Theorem 10 is a worst-case bound, and as shown experimentally in the following sections the auction typically achieves efficiency closer to the optimum.
In the example of Section 5, the difference in the efficiencies of the two sellers is lower than
the potential efficiency loss (as (g + 1)" = 24). However, for that instance it is still guaranteed that s1 wins, either with the efficient allocation, or with a1 b2 c2 which provides a
surplus of 39. The reason is that these are the only two configurations of s1 with surplus
within g" = 16 of the solution to MMP(b, s1 ), hence by Lemma 9 one of them must be chosen as 1 . Both of these configurations provide more than " surplus over s2 s most efficient
configuration, and this is sufficient in order to win in Phase B.
The bound of Theorem 10 can be improved when the CDI map contains disconnected
components. For example, when a fully additive decomposition (as assumed in previous
literature) does exist, the CDI map contains a disconnected component for each attribute.
To take advantage of this disconnectedness we create a separate tree decomposition for
each disconnected components. The definition of M has to be adapted to apportion "
proportionally across the disconnected trees. Formally, we redefine Mt as follows.
Definition 15 (Buyers Preferred Set). Let G be comprised of trees G1 , . . . , Gh . Let
j denote the projection of a configuration  on the tree Gj , and gj the number of GAI
elements in Gj . Similarly, j denotes the projection of  on Gj . Define
"
t "
, r  Gj }.
Mtj = {r | bt (j )  max

(
)

g
j
j
b
g
j! j
The buyers preferred set is given by Mt =

#h

t
j=1 Mj .

Let ej = gj  1 denote the number of edges in Gj . We define a connectivity parameter,
e = maxj=1,...,h ej . It turns out that e + 1 can replace g in the approximation results. The
first step is to replace Corollary 6 with this tighter bound on the optimality of configurations
in M t .
Corollary 11.
  M t . bt ()  max
bt ( " )  (e + 1)"
!
 

Proof. We apply Lemma 5 for each Gj , but with gj g" instead of ", hence any consistent cover
over Mtj is within gj g" gj of maxj! j bt (j" ). From Lemma 4, we get that any consistent
%
cover over Mt (meaning any configuration in M t ) is within hr=1 gj g" gj of max!  bt ( " ).
%
As e + 1 = maxj=1,...,h gj , this is bounded by g" hr=1 gj (e + 1) = "(e + 1).
We can now obtain a tighter efficiency result.
Theorem 12. Given a truthful buyer and SB sellers, the surplus of the final allocation is
within (e + 2)" of the maximal surplus.
500

fiGAI Auctions

In the fully additive case this loss of efficiency reduces to 2". On the other extreme,
if the CDI map is connected then e + 1 = g, reducing Theorem 12 to Theorem 10. If we
do not assume any preference structure for the buyer, meaning that the CDI map is fully
connected, then e = 0 and the efficiency loss is again proportional to ".
6.1.4 Sellers incentives to use SB
Following Parkes and Kalagnanam (2005), we relate our auction to the Vickrey-ClarkeGroves (VCG) mechanism to establish incentive properties for the sellers. In the one-sided
multiattribute VCG auction, the buyer reports valuation ub , the sellers report cost functions
ci , and the buyer pays the sell-side VCG payment to the winning seller.
Definition 16 (Sell-Side VCG Payment). Let (  , i ) be an optimal solution to MAP.
Let (, i) be the best solution to MAP when i does not participate. The sell-side VCG
payment is
VCG(ub , ci ) = ub (  )  max(0, ub ()  ci ()).
It is well known that truthful bidding is a dominant strategy for sellers in the one-sided
VCG auction. Parkes and Kalagnanam (2005) showed that the maximal regret for buyers
from bidding truthfully in this mechanism is ub (  )  ci (  )  (ub ()  ci ()), that is, the
marginal product of the efficient seller.
As typical for iterative auctions, the VCG outcome is not exactly achieved, but the
deviation is bounded by the minimal price change.
Definition 17 (-VCG Payment). A sell-side -VCG payment for MAP is a payment p
such that
VCG(ub , ci )    p  VCG(ub , ci ) + .
When payment is guaranteed to be -VCG, sellers can affect their payment only within
that range, hence their gain from falsely reporting cost is bounded by 2.
Lemma 13. When sellers are SB, the GAI auction payment is sell-side (e + 2)"-VCG.
In the example of Section 5, the profit of the winner (14) is less than " below his VCG
profit 20. The proof (in Appendix B.4) also covers Case 3 in the allocation options of
Section 4.2, in which we force the payment to equal ub (i ).
We are now ready for our final result of this section, showing that the approximately
efficient outcome guaranteed by Theorem 12 is achieved in an (approximate) ex-post Nash
equilibrium.
Theorem 14. SB is a (3e + 5)" ex-post Nash equilibrium for sellers in the GAI auction.
That is, sellers cannot gain more than (3e + 5)" by deviating from SB, given that other
sellers follow SB.
In order to exploit even this bounded potential gain, sellers need to know, for a given
configuration in M t , whether it was explicitly selected as approximately optimal for the
buyer, or it is a combination of sub-configurations from approximately optimal configurations. It seems highly unlikely for sellers to have such information. They are more likely to
lose if they do not bid on their myopically optimal configurations.
501

fiEngel & Wellman

6.2 Computation and Complexity
The advantage of GAI auctions over an additive auction such as AD (Parkes & Kalagnanam,
2005) is in economic efficiency: by accommodating expressive bidding, the efficiency results
are with respect to a more accurate utility function. In contrast, the key advantage with respect to an auction that does not employ preference structures, such as auction NLD (Parkes
& Kalagnanam, 2005), is in computational efficiency. The property we show in this section
is that all computations are exponential only in the size of the largest GAI element, rather
than in |A|. In particular, the size of the price space the auction maintains is equal to the
total number of sub-configurations. This number is exponential in the treewidth (plus one)
of the original CDI map.5 To ensure computational tractability, one can define a priori a
constant C, and force the treewidth of the CDI map to be bounded by C by ignoring some of
the interdependencies. This is still much better than using an additive representation that
ignores all interdependencies. The constant represents a tradeoff between economic and
computational efficiency; a larger C supports a more accurate preference representation,
but the GAI elements may be larger.
!
#
For the purpose of computational analysis, let I = gr=1 aj Ir D(aj ), that is the collection of all sub-configurations. Since M t grows monotonically with t, nave generation
of the best outcomes sequentially might end up enumerating significant portions of the
domain. Fortunately, this enumeration can be avoided, and the complexity of this computation (as well as the optimization performed by the seller) grows only with |I|, that is, no
computation depends on the size of the exponential domain.
Theorem 15. The computation of Mt can be performed in time O(g|I|2 ). Moreover, the
total time spent on this task throughout the auction is O(g|I|(|I| + T )).
We obtain a bound on T , the number of rounds of Phase A, by comparing the sum of
prices of all sub-configurations in rounds 1 and T .
Theorem 16. The number of rounds required by the auction is bounded by
"
g
T 
p1 (r ) .
"
r I

%
i
Proof. Let i =
r I p (r ) (the sum of prices of all sub-configurations in round i).
i
Assume that  < 0 for some 1  i  T . Then because ub ()  0, there must exist   
for which bi () > 0. Because we chose initial prices such that for all   , b1 () < 0,
this contradicts Lemma
T  0, hence the sum of prices cannot be reduced
% 7. Therefore,
1
1
by more than  = r I p (r ) throughout the auction. Also, in each round at least one
price is reduced by g" . This leads to the required result.
This bound is rather looseits purpose is to ensure that the number of rounds does
not depend on the size of the non-factored domain. It depends on the number of subconfigurations, and on the result of dividing the initial price by the minimum price decrement. Usually Phase A converges much faster. Let the initial negative profit chosen by
the auctioneer be m = max b1 (). In the worst case, Phase A needs to run until
5. The use of the term treewidth is subject to using an optimal tree decomposition.

502

fiGAI Auctions

  . b () = m. This happens for example when r  I. pt (r ) = fb,r (r ) + m
g . That
implies that the closer the initial prices reflect the buyers valuation, the faster Phase A
converges. One extreme choice is to set p1 (r ) = fb,r (r ) + m
g . That would make Phase A
redundant, at the cost of full initial revelation of the buyers valuation (Section 2.3). Between this option and the other extreme, which is ,   I. p1 () = p1 (), the auctioneer
has a range of choices to determine the right tradeoff between convergence time and information revelation. In the example of Section 5, the choice of a lower initial price for the
domain of I1 provides some speedup by revealing a harmless amount of information. In our
simulations below, we also set constant initial prices within each GAI element.
Furthermore, many domains have natural dependencies that are mutual to traders, in
which case the price structure used by the auction may also accommodate sellers preference
structures. If so, sellers can bid on multiple equally profitable configurations in each round,
thus speeding up convergence, as discussed above in Section 6.1.
We also consider computational complexity of the SB strategy for sellers.
Theorem 17. Let b denote the treewidth of the CDI map of ub (), and let i denote the
treewidth of the CDI map of ui (). The optimization of ui ()  p() takes time exponential
in b + i in the worst case.
Proof. Consider the graph G which includes the union of the edges of the two CDI maps.
The treewidth of G is b + i in the worst case. By definition, the price function p() is
decomposed according to ub (), hence ui ()  p() decomposes according to the additive GAI
factors of ui () and ub (). Therefore, for any pair of attributes x and y which have a mutual
factor in ui ()  p(), there is an edge x, y in G. It is well known that the complexity of
combinatorial optimization is exponential only in the treewidth of such graphfor example,
using cost networks (Dechter, 1997).
Of potential concern may be the communication cost associated with the descending
auction style. The sellers need to send their bids over and over again at each round. A
simple change can be made to avoid much of the redundant communication: the auction
can retain sub-bids from previous rounds on sub-configurations whose price did not change.
Because combinations of sub-bids from different rounds can yield suboptimal configurations,
each sub-bid should be tagged with the number of the latest round in which it was submitted,
and only consistent combinations from the same round are considered to be full bids. With
this implementation sellers need not resubmit their bid until a price of at least one of its
sub-configurations has changed.
To summarize, GAI auctions are shown to perform well on the criteria mentioned in Section 2.3: they achieve approximate efficiency given reasonable incentive properties, they are
expressive enough to accommodate preferences with interdependencies among attributes,
they are tractable when the maximal size of GAI clusters is reasonably bounded, and they
do not require full revelation of utility. Performance on this last criterion is quantified in
the experimental part of the paper.

7. Experimental Design
The main idea behind GAI auctions is to improve efficiency over auctions that assume
additivity, when preferences are not additive. However, (given a fixed ") the theoretical
503

fiEngel & Wellman

efficiency guarantee of GAI auctions depends on e, the connectivity parameter of the GAI
tree. This suggests a tradeoff: complex models more accurately represent true utility, but
can increase approximation error due to higher connectivity. An obvious question is whether
more accurate preference modeling is indeed more efficient, and in particular, whether
GAI auctions are more efficient than additive auctions, given that the preferences are not
additive. To address the question experimentally, we assume that the buyers preferences
have some GAI structure, and compare the performance of GAI auctions that model this
structure with the performance of auctions that are restricted to an additive representation.
For the latter, we use an instance of GAI auction in which the pricing structure is additive,
and name it the additive approximating auction (AP). This auction is similar in principle
to auction AD (Parkes & Kalagnanam, 2005).6 To the best of our knowledge, AD is the
only proposed instance of a surplus-maximizing multiattribute auction based on additive
preferences, besides those that require full revelation of the buyers utility. In all of the
experiments, sellers employ SB strategies.
In Section 7.1 we describe how random GAI utilities are drawn, and in Section 7.2 we
extend the scheme to generate GAI utility functions that exhibit additional structure. In
Section 7.3 we show how we obtain an additive approximation of these random functions,
allowing us to simulate auction AD. The results of the simulations are presented in Section 8.
7.1 GAI Random Utility
We performed simulations using randomly generated utility functions representing the
buyers value function and sellers cost functions. Our random utility generation procedure follows the utility elicitation procedure suggested by Braziunas and Boutilier (2005),
and uses a two-step process: first we create local utility functions over each GAI element,
normalized to the range [0, 1]. Next, we draw scaling constants that represent the relative
weight of each local function in the overall utility.
More formally, let ur (Ir ) = u([Ir ]) denote a local utility function over Ir , each normalized
to [0, 1]. Next, let fr (Ir ) be defined according to the GAI functional form of Eq. (6), with
u([Ir ]) replaced with ur (Ir ), hence
f1 = u1 (I1 ), and
for r = 2, . . . , g,

fr = ur (Ir ) +

r1
"
(1)j
j=1

"
1i1 <<ij <r

ur ([

j
$

Iis  Ir ]).

(9)

s=1

Braziunas and Boutilier (2005) show that for GAI-structured utility, there exist scaling
constants r  [0, 1] such that
g
"
r fr (Ir ).
(10)
u(A) =
r=1

6. Both auctions employ an additive price space that drives bidders to their efficient configurations. AD is
efficient up to ! when the buyer and all the sellers have additive preferences. GAI auctions are !-efficient
given additive buyers preferences, and make no assumption regarding sellers preference. There are
some more structural differences: (i) AD employs more complicated price change rules, in order to allow
sellers to ignore some of the attributes, (ii) discounts can be used in any stage of AD, and the auction
selects a provisional winner at each iteration.

504

fiGAI Auctions

We refer to the functions ur (Ir ) as subutility functions. Note that values of the form
ur ([Iir  Iir! ]) are drawn only once and used in both ur (Ir ) and ur (Iir! ). This representation
lets us draw random GAI functions, for a given GAI tree structure, using the following
steps:
1. Draw random subutility functions ur (Ir ), r = 1, . . . , g in the range [0,1].
2. Compute fr (), r = 1, . . . , g using (9).
3. Draw random scaling constants r , such that
(10).

%g

r=1 r

= 1, and compute u(A) by

The scaling constants represent the importance the decision maker accords to corresponding
GAI elements in the overall decision. The procedure results in utilities that are normalized
in [0, 1]. Finally, for each particular trader we draw mean  and variance , and scale u()
to the range [  ,  + ], resulting in the utility functions ub () and ui () for i = 1, . . . , m.
7.2 Structured Subutility
A subutility function in the model above may represent any valuation over the subspace.
In practice we may often find additional structure within each GAI element. We introduce
two structures which we consider most typical and generally applicable, and we use them
for the simulations, along with completely random local functions.
As we argue in Section 2.1, typical purchase and sale decisions exhibit FOPI (first
order preferential independence), meaning that most or all single attributes have a natural
ordering of quality. For example, hard-drive buyers always prefer more memory, higher
RPM, longer warranty, and so on. To implement FOPI, we let the integer values of each
attribute represent its quality. For example, if a belongs to some GAI element Ir = {a, b},
we make sure that ur (ai , b" )  ur (aj , b" ) for any ai > aj , ai , aj  D(a), and any b"  D(b).
This must of course hold for any attribute a that is FOPI, and any GAI element Ir that
includes a. We enforce the condition after all the values for that GAI element have been
drawn, through a special-purpose sorting procedure, applied between steps 1 and 2 above.
The FOPI condition makes the random utility function more realistic, and in particular
more appropriate to the target application. Once attributes exhibit FOPI, the dependencies
among different attributes are likely to be framed as complements or substitutes. These
concepts are known primarily in the context of combinatorial preferences, that is, preferences
over combinations of distinct items. In the multiattribute framework, two attributes are
complements if an improvement in the value of both is worth more than the sum of the
same improvement in each separately. Two attributes are substitutes if it is the other way
around. These concepts are meaningful only with respect to attributes that are FOPI,
otherwise the notion of improvement is conditional on the value of other attributes.
Definition 18 (Complements and Substitutes). Let u() be a measurable value function over S " . Let a, b  S " , and Z = S " \ {a, b}, and assume that a and b are each FOPI
of the rest of the attributes. Attributes a and b are called complements if for any ai > ai
(ai , ai  D(a)) and any bj > bj (bj , bj  D(b)), and any Z "  D(Z),
u(ai , bj , Z " )  u(ai , bj , Z " ) > u(ai , bj , Z " )  u(ai , bj , Z " ) + u(ai , bj , Z " )  u(ai , bj , Z " ).
505

fiEngel & Wellman

Attributes a and b are substitutes if the inequality sign is (always) reversed.
This relationship between attributes is ruled out under an additive utility function, but
admitted under a weaker independence condition, called mutual utility independence (MUI)
(Keeney & Raiffa, 1976), which implies that the utility function can be either multiplicative
or additive. If it is multiplicative, the utility function can be represented by n singledimensional functions, n scaling constants, and a single parameter k (the MUI-factor) that
controls the strength of complementarity (k > 0) or substitutivity (k < 0) between all
pairs of attributes within a GAI element (for k = 0 the set of attributes is additive).7 For
experimental purposes, we assume that each attribute cluster (GAI element) exhibits MUI,
and that the value of k is the same for all.
In an elicitation procedure, one would normally extract the MUI scaling constants from
a user, and then compute k (Keeney & Raiffa, 1976). For our purposes, we first determine
k according to the relationship we wish to impose on the attributes, and then draw MUI
scaling constants that are consistent with this value. More explicitly, we draw random
scaling constants, and then iteratively modify all the constants, until a set of constants is
found that is consistent with k. The next step is to compute ur (Ir ) according to the MUI
formula (Keeney & Raiffa, 1976). The ur (Ir ) (for all r) are in the range [0, 1], hence at
this point we can proceed with steps 2 and 3 above. Note that in this procedure several
distinct sets of scaling constants are used: the g constants used in step 3 scale the different
GAI elements, whereas the MUI constants, per GAI element, scale the attributes within
the element.
7.3 Additive Approximation
Another issue for experiment design is how the additive auction (AP) behaves in the face
of non-additive buyer preferences, specifically how would it select the approximately buyerpreferred sets of configurations. The approach we took is to come up with an additive
function that approximates the buyers true utility function, and use it throughout the
auction. We are not aware of a better strategy, but do not rule out the possibility that one
exists.
%
A natural approach to generate a linear approximation i fi () for an arbitrary function
ub () is to use linear regression. We define an indicator variable xij for every aij  D(ai ),
and consider any value of an assignment as a data point. For example, the assignment
a1j(1) , . . . , amj(m) creates the following data point:
m
"

"

cij xij = u(a1j(1) , . . . , amj(m) ),

i=1 aij D(ai )

in which the value of the variable xij is 1 if j = j(i) and 0 otherwise. The coefficients cij
result from the regression and represent the values to be used as fi (aij ).
When the problem includes many attributes, it is not possible to consider all the points
in . Under the assumption that a compact GAI representation exists, it is sensible to
expect that we could use fewer data points for the regression. We indeed found that a small
7. We formalize this notion in Appendix D.

506

fiGAI Auctions

random sample from the joint utility yields an approximation as effective as one based on all
the points. More precisely, for the largest domain we tested (25 attributes, each with domain
of size 4) we found that the efficiency of AP does not improve when increasing the number
of sampled points beyond 200. We show a chart supporting this claim in Appendix E. Our
experiments use 300 points for all instances.
This method of comparison probably overestimates the quality of an additive approximation. In general, we would not have the true utility function explicitly when we generate
the approximation. Extraction or elicitation of the utility function is usually the most
serious bottleneck of a multiattribute mechanism. Therefore, the major reason to use an
additive approximation is to reduce the burden of elicitation. Hence in practice we would
try to obtain the additive function directly, rather than obtain the full utility and then approximate it. The result of such process is somewhat unpredictable, because the elicitation
queries may not be coherent: if willingness to pay for a1 depends on the value of b, then
what is the willingness to pay for a1 when we do not know b? We therefore consider our
experimental generation method biased in favor of the additive approximation.

8. Simulation Results
We provide detailed results of our simulation study. Section 8.1 provides and analyses economic efficiency results. Section 8.2 covers the computational study, and results regarding
revelation of private information are provided in Section 8.3.
8.1 Efficiency and GAI Structure
We measure efficiency in terms of percentage of the MAP solution, which is the surplus
achieved by the optimal seller-configuration pair. To evaluate the effect of preference modeling on efficiency, we vary structural parameters of the buyers GAI preferences: the connectivity factor e, and the size  of the largest GAI element. Performance depends on
many additional factors, such as the size of attribute domains, number of sellers, amount of
price decrement ("), and the distribution from which utility functions are drawn. To isolate
the primary structural parameters, we first tested how efficiency varies according to the
choices of these side factors, for several fixed GAI structures with fully random subutility
functions. As a result of these tests, we picked the following parameter values for the rest
of the simulations: all valuations are drawn from a uniform distribution, with buyer mean
set at 500. A mean for each seller is drawn uniformly from [500, 700]. The variance is set
at 200 for all traders. We use the same domain size of 3 or 4 for all attributes, and number
of sellers m = 5. Further explanation of the process leading to these choices is provided in
the full report (Engel, 2008).
In the following experiment we used a roughly fixed GAI structure, with g = 6 elements
and e = 5, (that is, the GAI structure is a tree, not a forest), and " = 24 (meaning reduction
of  = 4 per sub-configuration). We vary the number of attributes by varying the size of each
element. Figure 2a shows the efficiency obtained with respect to , the size of the largest
GAI element. As expected, the size of the GAI elements has negligible, or no effect on the
efficiency of GAI auctions. It has a dramatic effect on the efficiency of AP. When  = 1,
the decomposition is in fact additive and hence AP performs optimally. The performance
then deteriorates as  increases.
507

fiEngel & Wellman

(a)

(b)

Figure 2: Efficiency as a function of: (a) the size of largest GAI element (), given e = 5,
(b) the number of GAI elements (e + 1), given  = 5.
We performed the same test when using utility in which all attributes are FOPI. With
the FOPI restriction, the additive approximation is much more efficient relative to unconstrained random utility. When FOPI applies to a strict subset of the attributes, we would
expect the efficiency of AP to fall somewhere between its efficiency under FOPI and the
unrestricted case. Somewhat surprisingly, imposing FOPI renders the GAI auctions slightly
less efficient. Nevertheless, the additive approximation achieves lower efficiency compared
to the accurate preference modeling, with differences that pass the statistical significance
test (P < 0.01), for   3. Further, note that the performance of GAI auctions can always
be improved using a smaller value of " and  = g" , whereas this hardly improves performance
of AP. With  = 2, a statistically significant difference (with the same confidence level) is
already detected for   2. We used  = 2 hereafter.
The next experiment (Figure 2b) measures efficiency as a function of e, for a given fixed
. We assume connected GAI trees, so e is the number of GAI elements minus one. We
tested structures with e varying from 1 to 10, all elements of size 3 to 5, and  = 5 for
all the structures.8 On a single element, the GAI auction is similar to NLD (Parkes &
Kalagnanam, 2005), which is an auction that assigns a price to every point in the joint
domain. Here e = 0, hence the efficiency of GAI is close to perfect. This structure is on the
other extreme compared to an additive representation, and indeed the performance of AP
is particularly inferior (only 70% efficient).
With more GAI elements, the efficiency of GAI auctions declines at a very slow pace. The
theoretical potential error (e + 2)", is mostly a result of efficiency loss of i for the winning
seller, based on Lemma 5. Such efficiency loss may occur only if each sub-configuration
in i belongs to a configuration that yields the lowest profit allowed in the buyer-preferred
seta particularly rare case. In practice, the loss is closer to e, which is a much smaller
error.
The performance of AP improves as the number of elements grows while their maximal
and average sizes are fixed. Intuitively, changing the structure that way takes it closer to
8. We did not find the particular tree structure to be influential on the results; the final structure used in
the reported results has a maximum of three children per node.

508

fiGAI Auctions

(a)

(b)

Figure 3: (a) Efficiency as a function of k  0 (complements). (b) Efficiency as a function
of k  0 (substitutes).
an additive representation. Under FOPI, we see a similar phenomenon as before. However,
the difference between GAI FOPI and AP FOPI, even for ten elements, is substantial and
statistically significant.
Figures 3a and 3b present efficiency as a function of the MUI-factor k, for complements
and substitutes, respectively. We used a fixed GAI structure with four elements, the largest
of which has four attributes, and imposed the same k on all elements. As expected, the
stronger the complementarity among the attributes, the lower the efficiency of AP, whereas
this relationship does not affect the efficiency of GAI auctions. For the case of substitutes,
in contrast, the additive approximation performs well, as efficiency starts to deteriorate only
for extreme values of k. Very roughly, we can say that when relationship among attributes
(within each GAI element) is limited to (mild) substitutions, it could be a good idea to
use an additive approximation. Unfortunately, our interpretation of the parameter k lacks
quantitative scaling: there is no clear intuition of what the actual numbers mean, beyond
the qualitative classification mentioned above.
To summarize this part, the experimental results show that GAI auctions yield significant efficiency improvement in comparison to an additive auction, on almost all classes of
evaluations. Though the efficiency of an additive auction may come across as relatively
high and perhaps sufficient, such an observation is misleading in several respects. (i) In
large procurement events, 510% efficiency differences translate to large amounts of money.
(ii) The wider efficiency loss of an additive auction (with no theoretical bound) may have
an impact on incentives; SB may no longer be an approximate ex-post Nash equilibrium.
(iii) Efficiency is expected to deteriorate for larger problems with larger GAI elements, and
in particular if FOPI does not hold for many of the attributes. (iv) As argued in Section 7.3, we expect practical additive auctions to perform worse than AP with our tailored
approximation.
8.2 Computational Analysis
The computational tasks required by auction simulations were performed using the algorithms described in Appendix C. These algorithms have been suggested and applied for
509

fiEngel & Wellman

(a)

(b)

Figure 4: Number of rounds as a function of: (a) the size of largest GAI element (), given
e = 5, (b) the number of GAI elements (e + 1), given  = 5 and  = 2.
combinatorial optimization problems before (Dechter, 1997; Nilsson, 1998), therefore the
computational runtime to process a round is not of particular interest to this work. Instead,
we focus on the number of rounds the auction requires. We tested the number of rounds
required by auctions GAI and AP, under fully random and FOPI preferences, varying three
of the parameters:  (size of largest GAI element), e (connectivity), and .
The complexity in terms of number of rounds is shown in Figure 4a (with respect to )
and Figure 4b (with respect to the number of elements). We observe that under FOPI the
GAI auction takes much longer to converge, compared to the case of random preferences.
The reason is that under FOPI, the sellers and the buyers preferences can in general be seen
as opposites: at the same price, and for a specific attribute, the buyer prefers higher quality,
whereas the sellers prefer lower quality (given fixed values for the rest of the attributes),
and everyone agrees on the relative quality of attribute values. The apparent difference in
the growth rate (the FOPI case seems to have a steeper curve) is somewhat misleading: for
 = 8 (not shown) GAI under random preferences is already caught up with the same curve
we see for the FOPI case. The number of rounds, as expected, grows exponentially with
the size of the largest element. However, as observed from Figure 4b, this number does not
grow quickly as a function of the number of elements, supporting the theoretical arguments
of Section 6.2. Note also that the variance chosen for traders preferences is fixed, thus for
a small number of elements the variance over them is wider, resulting in the large number
of rounds required by GAI FOPI in that case.
For AP, the only implication of increasing  is the respective increase in the number
of attributes. As a result, the complexity of AP (not shown) grows very slowly with the
increase in . For the FOPI case, with  = 2, AP takes an average of 481 rounds for  = 1
(6 attributes) and 546 rounds for  = 6 (19 attributes). The numbers are slightly higher for
random preferences (523 to 628).
For high-dimensional multiattribute auctions, we expect that participation would typically be automated through software bidding agents (Wellman, Greenwald, & Stone, 2007).
Under these circumstances, an auction taking up to thousands of rounds should not cause
a concern. However, if for some reason rounds are expensive, we might reconsider adopt-

510

fiGAI Auctions

Figure 5: Efficiency as a function of the number of rounds.
ing additive auctions, and sacrifice efficiency in order to decrease the number of rounds.
Alternatively, we could keep using GAI auctions and increase " (and with it ). The final
experiment compares these two alternatives. We vary the level of ", in order to view efficiency as a function of the number of rounds (Figure 5). The GAI structure used for this
experiment has e = 5 and  = 5.
As evident from the chart, in most cases GAI achieves better efficiency even for a fixed
number of rounds. The only exception is when the budget of rounds is very small (under
200), and FOPI holds. In such case we need to pay with more rounds in order to get the
higher efficiency.
The total computation time, carried out by a GAI auction with 10 elements,  = 5,
d = 3,  = 2, and the rest of the parameters fixed as above, is around 11 seconds on
average, using an Intel Dual Core (2.00 Ghz) CPU, with 2048 MB RAM.
8.3 Information Revelation
A key difference between the mechanism proposed here and most previous literature is in
the extent to which the buyer is required to reveal preference information. In GAI auctions,
the buyer does not need to reveal all of its private preference information up front. Of
course, the price changes do reveal some of the buyers information. Another experimental
question is therefore whether this mechanism significantly reduces the overall amount of
information revealed by the buyer.
PK study information revelation by both the buyer and the seller, under an additivity
assumption. When the utility function is additive the amount of information revealed can
be measured in terms of constraints on the linear weights. Sellers can infer bounds on the
buyers set of weights, and the amount of information hidden from them is represented by
the fraction of the simplex that satisfies those constraints. This simplex analysis is not
possible for GAI utilities. We suggest an alternative geared towards the kind of information
revealed by the GAI auctions.
In GAI auctions, the buyers private information is partially revealed through the selection of the buyers preferred set Mt . The auction does not need to announce this directly;
in general the sellers can infer that a sub-configuration is in Mt only if it received a bid
(usually sellers will observe this only for their own bids), yet its price does not change in the

511

fiEngel & Wellman

next round. We therefore measure exactly thatfor how many sub-configurations  r there
was at least one round t such that  r  Mt  Bit for some i. More specifically, we define such
a sub-configuration as revealed, and within each GAI element we measure the fraction of
sub-configurations that are revealed by the end of the auction. This measurement overestimates the information that is actually revealed, as sellers can infer some bounds on relative
preferences but not the precise values of the functions fb (). Moreover, it assumes that each
seller observes all bids (meaning that sellers share bid information with each other), an
unrealistic event in practice.
Based on this criterion, GAI auctions reveal on average 15%25% of the buyers preferences when preferences exhibit FOPI, and 10%15% when the subutilities are completely
random. It does not seem to systematically depend on any other parameter we tested. This
validates our claim as to the advantage that GAI auctions promise over second-score types
of auctions.

9. Conclusions
We propose a novel exploitation of preference structure in multiattribute auctions. Rather
than assuming full additivity, or no structure at all, we model preferences using the generalized additive independence (GAI) decomposition. We show how a GAI representation
can be constructed from relatively simple statements of willingness-to-pay, and develop an
iterative auction mechanism directly relying on the decomposition. Our auction mechanism
generalizes the preference modeling employed by Parkes and Kalagnanam (2005), while in
essence retaining their information revelation properties. It allows for a range of tradeoffs
between the accuracy of preference representation and computational complexity of the
auction, as well as the tradeoff between buyer information revelation and the number of
rounds required for convergence.
We performed a simulation study of our proposed multiattribute auctions, compared
to a mechanism that assumes additive preferences. The study validated the usefulness
of GAI auctions when preferences are non-additive but GAI, and allowed us to quantify
the advantages for specific classes of preferences. In general, we found that our design
yields significantly higher economic efficiency in comparison to additive auctions. When
the GAI subutilities exhibit internal structures, such as FOPI, the efficiency loss of additive
approximation is less severe, but in most cases the benefit of an accurate GAI model is still
significant. Using an additive approximation may be a reasonable approach when the GAI
structure is fairly similar to an additive one, or when the auction must terminate within a
small number of rounds.
The tradeoff between expressive and compactness of preference representation is ubiquitous in applications involving preferences. On one hand, we would like to ask users for as
little as possible information; on the other, users preference statements may not be accurate or even meaningful if they cannot express important dependencies. In such problems
it could be useful to experimentally compare the accuracy of GAI and additive representations. The experimental methodologies used in this study, in particular the generation of
random structured utility functions, and finding an additive approximation to GAI functions, may therefore prove applicable to a broader class of preference research problems in
which this tradeoff exists.
512

fiGAI Auctions

Acknowledgments
This work was supported in part by NSF grants IIS-0205435 and IIS-0414710, and the
STIET program under NSF IGERT grant 0114368. Yagil Engel was supported in part by
the Aly Kaufman fellowship at the Technion. We thank anonymous reviewers for many
useful comments and suggestions.

Appendix A. Proofs of Section 3.1
Lemma 2.
Let u(A) be an MVF representing preference differences, and let X, Y, Z
define a partition of A. Then CDI(X, Y | Z) iff
u(A) = u(X 0 , Y, Z) + u(X, Y 0 , Z)  u(X 0 , Y 0 , Z),
for arbitrary instantiations X 0 , Y 0 .
Proof. Let X 0 , Y 0 be arbitrary instantiations.
u(X, Y, Z) = u(X, Y, Z)u(X 0 , Y, Z)+u(X 0 , Y, Z) = u(X, Y 0 , Z)u(X 0 , Y 0 , Z)+u(X 0 , Y, Z)
The second equality holds iff for any X 0 , Y 0 , CDI(X, Y | Z).
Theorem 3 (CDI-GAI Theorem).
Let G = (A, E) be a CDI map for A, and
{I1 , . . . , Ig } a set of overlapping maximal cliques. Then
u(A) =

g
"

fr (Ir ),

(A.1)

r=1

where
f1 = u([I1 ]), and
for r = 2, . . . , g,

fr = u([Ir ]) +

(A.2)

r1
"

"

(1)j

j=1

u([

1i1 <<ij <r

j
$

Iis  Ir ]).

s=1

Proof. We actually prove a somewhat stronger result.
Claim. Let G be a CDI map for utility function u(). Let Q = {C1 , . . . , Cw } denote the set
of maximal cliques of G. Then,
u(A) =

w
"
(1)k+1
k=1

"
1i1 <<ik w

u([

k
$

Cis ]).

(A.3)

s=1

Let G0 = (A, E 0 ) be the complete graph over the nodes of G. By definition of CDI
map, each edge (x, y)  E 0 \ E implies CDI(x, y). We use induction on a series of edge
removals. starting from the graph G0 , at each step i we remove an edge in E 0 \ E to get
0
graph Gi . After the last step i = |E 0 |  |E| and G|E ||E| = G. We prove that the claim
513

fiEngel & Wellman

holds for each graph Gi . Since A is the only clique in G0 , in step 0, Q0 = {A} and the
claim trivially hold. Following the process for step 1 provides intuition as for how the final
decomposition is obtained. We pick a pair of nodes (x, y) such that CDI(x, y). We use the
notation S a = S \ {a} for any S  A and a  A. By Lemma 2 ,
u(A) = u(x, y, Ax,y )

(A.4)

= u(x0 , y, Ax,y ) + u(x, y 0 , Ax,y )  u(x0 , y 0 , Ax,y )
= u([Ax ]) + u([Ay ])  u([Ax  Ay ]).
The set of maximal cliques of G1 is Q1 = {Ax , Ay }. Equation (A.4) shows that (A.3)
holds for Q1 .
For proving the induction step, we assume (A.3) holds at step i, and show they carry
over to step i + 1. Let (x, y) denote the edge removed in step i + 1. Let C1 , . . . , Cd (WLOG)
indicate all the sets in Qi that include both x and y. Similar to (A.4), we observe that
u([C1 ]) = u([C1x ]) + u([C1y ])  u([C1x  C1y ]).

(A.5)

Similarly for any k = 1, . . . , wi  1, and 1 < i1 <    < ik  wi ,
u([

k
$

s=1

Cis  C1 ]) = u([

k
$

s=1

Cis  C1x ]) + u([

k
$

Cis  C1y ])  u([

s=1

k
$

Cis  C1x  C1y ]). (A.6)

s=1

In (A.3) (assumed to hold before this step) each term that includes C1 can be substituted
according to (A.5) or (A.6). Doing so will result in (A.3) holding for the set (Qi \ {C1 }) 
{C1x , C1y }.
We repeat the same operation for C2 , . . . , Cd , and define the resulting collection
Qi+1 = (Qi \ {C1 , . . . , Cd })  {C1x , C1y , . . . , Cdx , Cdy }.
All elements in Qi+1 are subsets of elements in Qi , which are all maximal cliques of Gi . We
now verify the induction property:
 Any element in Qi+1 is a clique in Gi+1 , because the only difference between Gi and
Gi+1 is the removed edge (x, y), and no set in Qi+1 includes both x and y.
 Any such clique in C  Qi+1 is maximal, because it is a subset of a maximal clique
of C  Gi , and either: (i) y  C and C = C \ {x} or (ii) x  C and C = C \ {y}, or
(iii)C = C. x and y are no longer connected so C remains maximal in all cases.
 If M is a maximal clique in Gi+1 , then M  C for some C  Qi . Again either M = C,
or M = C \ {x}, or M = C \ {y}, and in all three cases M is an element in Qi+1 .
This proves the induction step.
As a result, in the last step the decomposition (A.3) holds for the set Q = Q|E0 ||E| ,
which is the set of maximal cliques of G, and hence the claim is proved.
Now define f1 , . . . , fg according to (A.2). By the claim, we get that (A.1) holds.

514

fiGAI Auctions

Appendix B. Proofs of Section 6.1
B.1 Proving Lemma 5
Lemma 5. Let  be a set of configurations, all are within " of maximizing profit for a
trader  at the a given price vector. Let  = {r |   , r  {1, . . . , g}}. Then any
consistent cover in  is within g" of maximizing profit for  at these prices.
We show that given a suboptimal consistent cover  over , we can find a suboptimal
member in , contradicting the premise of the lemma. We do that by traversing the GAI
tree in a depth-first manner, at each step we flip the sub-configurations corresponding to the
elements of the subtree to a set of sub-configurations that have the same source configuration
in  as the parent of that subtree (thus trimming that subtree). This, as we show, results
in another consistent cover that is also sub-optimal. Eventually we obtain a configuration
in  which is still suboptimal.
For that purpose we introduce the following notions:
 The operator  turns a set of sub-configurations, which is a consistent cover, into a
configuration:
{1 , . . . , g } = (1 , . . . , g ).
 Let  be a consistent cover over . The -source of an element r is a configuration
   from which it originated (meaning, r = r ).
 The operation trim replaces some of the sub-configurations of a given configuration 
with a corresponding set of sub-configurations of a different configuration , according
to the following rules. Let i denote the indices of the GAI elements, corresponding to
a subtree in the GAI-tree, whose root is the GAI element Ii . Let  denote a consistent
cover over . The operation -trim over  and i is defined if all the elements in 
corresponding to i have the same -source. Formally, there exists   , such that
r , if r  i then i = i . Now, Let  be the parent of Ii , or an arbitrary element
outside i if i is disconnected from the rest of the graph. Let    be the source of
 . Then
-trim(i , ) = {r | r 
/ i }  {r |r  i }
That is we replace each of r for r  i by the corresponding sub-configuration in ,
so that in the resulting configuration all the elements corresponding to i have the
same -source as the parent of Ii .
Lemma B.1.  " = -trim(, ) is a consistent cover.
Proof. We need to show that any pair of sub-configurations in the set {r | r 
/ i }  {r |r 
i } are consistent, that is they assign the same value to any attribute that appear in both
corresponding GAI elements.
The sub-configurations {r |r  i } are internally consistent because they have a mutual
-source . The sub-configurations {r | r 
/ i } are internally consistent because they
i
are all sub-configurations of . Let r1   and r2 
/ i denote indices of GAI elements,
such that Ir1  Ir2 ,= . Now, Ir1 is in a subtree whose root is Ii , whereas Ir2 is outside
the subtree, so the path between them must go through Ii and its parent . Due to the
515

fiEngel & Wellman

running intersection property of the GAI tree, Ir1  Ir2  Ii  . The corresponding subconfigurations i and  must be consistent because  is also the -source of , hence r1
and r2 must also be consistent.
Lemma B.2. Let  and  be defined as in Lemma 5, and let  denote a consistent cover
in . Then if  " = -trim(i , ) (for some i), then  ( " )   () + ".
Proof. Let    denote the single -source of {r | r  i }. Let  = i and  =
{1, . . . , g} \ . If  ( " ) >  () + ", then (using Lemma 4)
 ( " ) =  (" ) +  (" ) >  ( ) +  ( ) + ",
and because  = " ,

 (" ) >  ( ) + ".

Define the following cover:
 = {r" | r  }  {r | r  }
 is a consistent coveragain (as in Lemma B.1) the only possible intersection between
an element from  " and an element from  is between i (the root of  = i ) and its parent
. The corresponding sub-configurations i" and  must be consistent for the following
argument: i is consistent with  because they appear together in . i" is consistent with
 because they have the same -source by definition of -trim. Hence i and i" assign the
same values to the attributes in Ii  I . Now because i is consistent with  , so must be
i" . We get
 () =  (" ) +  ( ) >  ( ) +  ( ) + " =  () + ".
The last equation follows from the fact that all sub-configurations of  are from . This
contradicts "-optimality of   .
Proof of Lemma 5. Let  1 be a consistent cover over  contradicting the lemma, meaning
 ( 1 )  max  ()  g". We first reorder the GAI elements as 1, . . . , g, according to
the order corresponding to backtracking in Depth-First-Search: that is, starting from the
leftmost leaf, next move to its siblings, next their parent, and in general once all children
of a node Ii are visited, the next element to be visited is Ii . We perform a series of g  1
-trim operations, resulting in a series  1 , . . . ,  g . To do that, we must show that at each
step i the operation -trim(i ,  i ) is valid, that is the sub-configurations corresponding to
i have a mutual -source. If Ii is a leaf, then |i | = 1 hence the elements of i have a
single source. Otherwise,  i is a result of trimming the subtrees of all children of Ii , hence
by definition of -trim they all have the same -source as ii .
Now, consider the resulting  g . We assumed  ( 1 ) < max  ()  g", hence by
applications of Lemma B.2 in each of the g  1 -trim operations, we get  ( g ) <
max  ()  ". The last element  g is such that all its elements have a mutual -source,
meaning  g  . Therefore, we got a contradiction to the "-optimality of .

516

fiGAI Auctions

B.2 Proving Theorem 10
In order to prove Theorem 10 we need several additional claims.
Lemma B.3. The price of at least one sub-configuration must be reduced in every round of
phase A.
Proof. In each round t < T of phase A there exists an active seller i for whom Bit  M t = .
However to be active in round t, Bit ,= . Let   Bit . If r.r  Mt , then   M t by
definition of M t . Therefore there must be r , Mt .
Lemma B.4. The auction must reach phase B.
Proof. By Lemma B.3 some prices must go down in every round of phase A. Lemma 7
ensures a lower bound on how much prices can be reduced during phase A, therefore the
auction either terminates in phase A or must reach condition [SWITCH].
We set the initial prices high such that max b1 () < 0, and then max bt () < 0
during phase A by Lemma 7. By Assumption A2 the efficient allocation (  , i ) provides
positive welfare, that is i (  ) = bt (  ) + it (  ) > 0. si is SB therefore she will leave the
auction only when it (  ) < 0. This can happen only when bt (  ) > 0, therefore si does
not drop in phase A. Because Phase A continues as long as at least one seller is active, the
auction cannot terminate before reaching condition [SWITCH].
Finally, the following lemma states that for any two sellers, the potential surplus of the
first one to drop from the auction cannot be significantly higher than the potential surplus
of the one that stayed longer.
Lemma B.5. If sellers si and sj are SB, and si is active at least as long as sj is active in
phase B, then
i (i )  max j ()  (g + 1)".


Proof. From SB and the definition of phase B, sj drops when  > jT (j ). If si did not
drop before that point then iT (i )    " > jT (j )  ". Because i  M t , we get from
Corollary 6 that,
bT (i ) + iT (i )  max bT () + jT (j )  (g + 1)".


From Corollary 8, jT (j ) = max jT (). Therefore
i (i ) = bT (i ) + iT (i )  max bT () + max jT ()  (g + 1)"  max j ()  (g + 1)".






Theorem 10. Given a truthful buyer and SB sellers, the surplus of the final allocation
is within (g + 1)" of the maximal surplus.
Proof. From Lemma B.4 the auction terminates with an allocation (si , i ). From Lemma
9, the theorem is immediate in case the winning seller si is the efficient seller. Otherwise
the efficient seller is sj who dropped before or with si . The result is now immediate from
Lemma B.5.
517

fiEngel & Wellman

B.3 Proving Theorem 12
We first adapt Lemma 7, Lemma 9, and Lemma B.5 to use e + 1 instead of g.
Lemma B.6. max bt () does not change in any round t of phase A.
Proof. Let G be comprised of trees G1 , . . . , Gh , let j" denote the projection of a configuration
 " on the tree Gj , and let gj denote the number of GAI elements in Gj .
Assume there exists some j" for which bt+1 (j" ) > bt (j" ). Then necessarily pt+1 (j" ) =
t
p (j" )  . For that to happen it must be the case that some w  gj sub-configurations of
t
j" are not in Mtj , and  = w"
g . In that case, by definition of Mj ,
"
bt (j" ) < max bt (j )  gj .
j j
g
Therefore,
bt+1 (j" ) =  t (j" ) +  =  t (j" ) +

w"
gj "
  t (j" ) +
< max bt (j ).
j j
g
g

This is true for any j" whose profit improves, therefore maxj j bt (j ) does not change
during phase A. Now
max bt () = max




h
"

bt (j ) =

j=1

h
"
j=1

max bt (j ).

j j

The last equality holds because the optimal values for disconnected components of the GAI
tree are independent of each other. As a result, max bt () as well does not change during
phase A.
Lemma B.7. For SB seller si , i is (e + 1)"-efficient:
i (i )  max i ()  (e + 1)".


The proof is identical to the proof of Lemma 9, replacing g by e+1 and using Corollary 11
instead of Corollary 6.
Lemma B.8. If sellers si and sj are SB, and si is active at least as long as sj is active in
phase B, then
i (i )  max j ()  (e + 2)".


The proof here too is identical to the proof of Lemma B.5, using Corollary 11 instead
of Corollary 6.
Theorem 12. Given a truthful buyer and SB sellers, the surplus of the final allocation
is within (e + 2)" of the maximal surplus.
Proof. The proof is identical to the proof of Theorem 10, replacing Lemmas 9 and B.5 with
lemmas B.7 and B.8, respectively.

518

fiGAI Auctions

B.4 Lemma 13 and Theorem 14
Lemma 13. When sellers are SB, the GAI auction payment is sell-side (e + 2)"-VCG.
Proof. Trivially, we consider only the winning seller si . In the case that the final price is
above buyers valuation the payment ub (i ) is exactly the VCG payment. We can therefore
assume that the final price is not above the buyers valuation, and the payment to the
winning seller is pT (i )  . Let sj be the second best seller. sj drops before si , when the
discount is   ", hence,
 + " > jT (j ) = max jT ().
(B.1)


From Corollary 6,
ub (i )  pT (i )  max bT ()  (e + 1)".


Therefore (using (B.1) for the second inequality)
pT (i )    ub (i )  max b () + (e + 1)"   <


ub (i )  max bT () + (e + 2)"  max jT ()  ub (i )  max j () + (e + 2)". (B.2)






Bow because sj s survived in the auction until the discount was   ",
  jT (j ) + ".
Meaning:
pT (j )    cj (j )  ".

(B.3)

From Corollary 6
ub (i )  pT (i )  ub (j )  pT (j ) + (e + 1)".
Therefore (using (B.3) for the second inequality)
pT (i )    ub (i )  ub (j ) + pT (j )  (e + 1)"   
ub (i )  (ub (j )  cj (j ))  "  (e + 1)"  ub (i )  max j ()  (e + 2)". (B.4)


Equations (B.2) and (B.4) place the payment pT (i )   within (e + 2)" from si s VCG
payment.
Theorem 14. SB is a (3e + 5)" ex-post Nash equilibrium for sellers in the GAI auction.
That is, sellers cannot gain more than (3e+5)" by deviating from SB, given that other sellers
follow SB.
Let s1 play some arbitrary strategy 1 against SB sellers s2 , . . . , sn . If s1 does not win
she would clearly have done no worse using SB, therefore we assume s1 wins 1 in final price
p and that she gains at least (3e + 5)" from the trade. Let i  2, . . . n. The calculation of
(B.2) assumed nothing on the winning traders strategy, therefore it applies here as well:
p = pT (1 )    ub (1 )  max i () + (e + 2)".


(B.5)

Next, define the following cost function: c1 (1 ) = p  (2e + 3)" and c1 ( " ) = ,  " ,= 1 .
Assume s1 plays SB for c1 .
519

fiEngel & Wellman

Claim. By playing SB assuming cost c1 , s1 is still the winner, and her profit (wrt to c1 ())
is within (2e + 3)" of her profit playing 1 .
Proof. Clearly, s1 bids only on 1 . Let p() denote prices in the end of phase A in the new
 be the final discount.
instance of the auction, let b () denote the buyers profit, and let 
 = 1 (1 ) = p(1 ) c1 (1 ) =
Now assume for a moment that prices reach s1 s limit, that is 
p(1 )  (p  (2e + 3)").
Now (for the inequality, use pT (1 ) = ub (1 )  bT (1 ) and also (B.5)),
 = pT (1 )  p + (2e + 3)" > ub (1 )   T (1 )  (ub (1 )  max i () + (e + 2)") + (2e + 3)"

b


= max i () 


bT (1 )

+ (e + 1)".
(B.6)

Let i denote the configuration chosen for some seller si at the end of phase A in the
new instance. Since i  M T in that instance, we get that bT (i )  bT (1 )  (e + 1)".
Therefore we can modify (B.6) to state,
 > i (i )   T (i ) = pT (i )  ci (i ),

b

(B.7)

meaning that if prices reached the limit of s1 , all the other sellers dropped off. That shows
that s1 wins in the new instance as well. Furthermore, the lowest possible price paid to s1
 = p(1 )  (p  (2e + 3)"), hence that price is at least p  (2e + 3)".
is determined by 
Proof of Theorem 14. From Lemma 13:
p  V CG(c1 , c2 , . . . , cn ) + (e + 2)".
Truthful reporting is a dominant strategy for sellers in one-sided VCG auctions. Therefore
V CG(c1 , c2 , . . . , cn )  V CG(c1 , c2 , . . . , cn ).
With the result of the claim we get
p  p + (2e + 3)"  V CG(c1 , c2 , . . . , cn ) + (3e + 5)".
Therefore by playing 1 , s1 could not have gained more than (3e + 5)" above her worstcase payoff for playing SB with respect to her true cost c1 .

Appendix C. Proofs of Section 6.2
Theorem 15. The computation of Mt can be performed in time O(g|I|2 ). Moreover, the
total time spent on this task throughout the auction is O(g|I|(|I| + T )).
Proof. For simplicity of notations we assume that there is a single (connected) GAI-tree.
The extension to multiple connected components is immediate because each Mjt is computed
separately.
The functions ub and pt have the same GAI form, hence the function bt = ub  pt
has the same GAI form. As have been noted before (Boutilier et al., 2001), functions in
520

fiGAI Auctions

GAI form can be optimized using variable elimination schemes in cost networks (Dechter,
1997). In fact, our GAI structure is already a tree, in which case the optimization is linear
in the size of the domain which is |I|. However, Mt includes sub-configurations of all
configurations within " of max b (). To find it, we must find the maximum of bt , add its
sub-configurations to Mt , then find the best configuration which is not already in M t (that
is, maximal in  \ M t ) and so on. This can be done by the following procedure, adapted
from the work of Nilsson (1998):
1. For i=1,. . . , g:
 Define i = {   | 1 , . . . , i1  Mt and i 
/ Mt }.
 Find  i = arg maxi bt ().
2. The best configuration in  \ M t is   = arg maxi=1,...,g bt ( i ) (which means, a configuration which has at least one sub-configuration not in Mt ).
If bt (  )  max bt ()  ", then each sub-configuration of   that is not already in Mt is
added to Mt . Otherwise, Mt is ready.
The procedure itself performs g optimizations, each takes linear in the size of the domain. This amounts to O(g|I|). Each time this procedure is done, either at least one
sub-configuration is added to Mt , or Mt is ready. Therefore the number of times the
procedure is done per round is bounded by the number of sub-configurations |I| plus one,
giving the O(g|I|2 ) bound. Moreover, Mt is monotonically increasing in the auction. In
each round, we start from the Mt computed in the previous round. Throughout the auction,
each application of the procedure either yields a new sub-configuration in Mt , or terminates
the round, so the total number of times the procedure is performed throughout the auction
is bounded by |I| + T , leading to the overall bound of O(g|I|(|I| + T )).

Appendix D. Relating the MUI condition to Complements and
Substitutes
The definitions for the utility independence (UI) condition and MUI can be found elsewhere
(Keeney & Raiffa, 1976).
Definition 19. A MUI-factor of a set A of MUI attributes is a solution to
1+k =

n
&
(1 + kki ).
i=1

Keeney and Raiffa (1976) (KR) show that there is at most one MUI-factor in addition
to zero (Appendix 6B of their text). This ensures the soundness of the following adaptation
to their MUI representation theorem:9
Theorem D.1. Let A be a set of MUI attributes.
1. If the only MUI-factor of A is zero, then u(A) =

%n

i=1 ki ui (ai ).

9. The theorem is adapted from the book of Keeney and Raiffa (1976), Theorem 6.1, page 289.

521

fiEngel & Wellman

2. Otherwise, let k ,= 0 be a MUI-factor. Then
!n
[kki ui (ai ) + 1]  1
u(A) = i=1
.
k

(D.1)

KR go on to point out that if k > 0 we can define u" (A) = 1 + ku(A), a strategically
equivalent function to u(), and turn (D.1) into a multiplicative representation. This can
be done in a similar fashion for k < 0. Further, they show that if MUI is known to exist,
one elicitation query is sufficient in order to determine whether the form of the function is
additive or multiplicative.
The following relationship allow us to interpret the MUI factor with respect to complements and substitutes. The result generalizes and formalizes an intuition given by KR for
the case of MUI between two attributes.
Theorem D.2. Let A be a set of MUI attributes, such that there is a MUI-factor k ,= 0.
Then k > 0 iff all pairs of attributes in A are complements, and k < 0 iff all pairs of
attributes in A are substitutes.
Proof. The proof is based on the work of Keeney and Raiffa (1976), Theorem 6.1, as explained below.
Assume that u() is normalized such that u(A0 ) = 0. For each attribute a  A, let
a = {a}, and we know U I(a, a). Utility independence of such form leads to the following
functional form: there exist functions f and g such that,
u(A) = f (a) + g(a)u(a, a0 )
We instantiate this form with the assignment a0 and get
u(a0 , a) = f (a) + g(a)u(a0 , a0 ) = f (a)
Hence f (a) = u(a0 , a), and g(a) =
u(A0 ) = 0, we get

u(A)u(a0 ,a)
u(a,a0 )

g(a) =

(this development is done by KR). With

u(A)  u(a0 , a)
.
u(a, a0 )  u(a0 , a0 )

(D.2)

In proof of Theorem 6.1, KR define the MUI-factor as follows:
k=

g(a)  1
u(a0 , a)

The denominator is always positive. Furthermore, as shown by (D.2), when g(a) > 1,
u(A)  u(a0 , a) > u(a, a0 )  u(a0 , a0 ). In particular it means that for any b  a, a and b
are complements, because the inequality holds when holding fixed all attributes in a but b.
Similarly, when g(a) < 1, a and any b  a are substitutes. Putting these pieces together,
we get the desired result.

522

fiGAI Auctions

Figure 6: Efficiency of AP as a function of the number of sampling points used to devise
the additive approximation.

Appendix E. Optimal Regression Using a Small Sample
We show an experiment supporting the claim in Section 7.3: a larger set of sampling
points than the one we used for the linear regression of the utility function cannot improve
the efficiency of AP. Figure 6 shows the efficiency of AP as a function of the number of
sampling points used, for the largest domain we used in the experiments: 25 attributes with
d = 4 (e = 9 and  = 5). Similar results were shown for other distributions and for FOPI
preferences. This chart is a result of 150 experiments for each of 10 points on the x-axis,
the largest number of tests we used.

References
Bacchus, F., & Grove, A. (1995). Graphical models for preference and utility. In 11th
Conference on Uncertainty in Artificial Intelligence, pp. 310, Montreal.
Beil, D. R., & Wein, L. M. (2003). An inverse-optimization-based auction for multiattribute
RFQs. Management Science, 49, 15291545.
Bichler, M. (2001). The Future of e-Markets: Multi-Dimensional Market Mechanisms. Cambridge University Press.
Boutilier, C., Bacchus, F., & Brafman, R. I. (2001). UCP-networks: A directed graphical
representation of conditional utilities. In 17th Conference on Uncertainty in Artificial
Intelligence, pp. 5664, Seattle.
Branco, F. (1997). The design of multidimensional auctions. RAND Journal of Economics,
28 (1), 6381.
Braziunas, D., & Boutilier, C. (2005). Local utility elicitation in GAI models. In 21st
Conference on Uncertainty in Artificial Intelligence, pp. 4249, Edinburgh.
Che, Y.-K. (1993). Design competition through multidimensional auctions. RAND Journal
of Economics, 24 (4), 668680.
David, E., Azoulay-Schwartz, R., & Kraus, S. (2002). An English auction protocol for multiattribute items. In Agent Mediated Electronic Commerce IV: Designing Mechanisms
and Systems, Vol. 2531 of Lecture Notes in Artificial Intelligence, pp. 5268. Springer.
523

fiEngel & Wellman

Debreu, G. (1959). Topological methods in cardinal utility theory. In Arrow, K. J., Karlin,
S., & Suppes, P. (Eds.), Mathematical Methods in the Social Sciences, pp. 1626.
Stanford University Press.
Dechter, R. (1997). Mini-buckets: A general scheme for generating approximations in automated reasoning. In 15th International Joint Conference on Artificial Intelligence,
pp. 12971303, Nagoya.
Dyer, J. S., & Sarin, R. K. (1979). Measurable multiattribute value functions. Operations
Research, 27, 810822.
Engel, Y. (2008). Structured Preference Representation and Multiattribute Auctions. Ph.D.
thesis, University of Michigan, Ann Arbor, MI.
Engel, Y., & Wellman, M. P. (2007). Generalized value decomposition and structured
multiattribute auctions. In 8th ACM Conference on Electronic Commerce, pp. 227
236, San Diego.
Engel, Y., Wellman, M. P., & Lochner, K. M. (2006). Bid expressiveness and clearing
algorithms in multiattribute double auctions. In 7th ACM Conference on Electronic
Commerce, pp. 110119, Ann Arbor.
Fishburn, P. C. (1967). Interdependence and additivity in multivariate, unidimensional
expected utility theory. International Economic Review, 8, 335342.
Gonzales, C., & Perny, P. (2004). GAI networks for utility elicitation. In 9th International
Conference on Principles of Knowledge Representation and Reasoning, pp. 224234,
Whistler, BC.
Hyafil, N., & Boutilier, C. (2006). Regret-based incremental partial revelation mechanisms.
In 21st National Conference on Artificial Intelligence, pp. 672678, Boston, MA.
Keeney, R. L., & Raiffa, H. (1976). Decisions with Multiple Objectives: Preferences and
Value Tradeoffs. Wiley.
Koppius, O. (2002). Information Architecture and Electronic Market Performance. Ph.D.
thesis, Erasmus University, Rotterdam, The Netherlands.
Krantz, D. H., Luce, R. D., Suppes, P., & Tversky, A. (1971). Foundations of Measurement,
Vol. 1. Academic Press, New York.
Milgrom, P. (2000). Putting auction theory to work: The simultaneous ascending auction.
Journal of Political Economy, 108, 245272.
Nilsson, D. (1998). An efficient algorithm for finding the M most probable configurations
in probabilistic expert systems. Statistics and Computing, 8 (2), 159173.
Parkes, D. C., & Kalagnanam, J. (2005). Models for iterative multiattribute procurement
auctions. Management Science, 51, 435451.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann.
Robu, V., Somefun, D. J. A., & La Poutre, J. A. (2005). Modeling complex multi-issue negotiations using utility graphs. In 4th International Joint Conference on Autonomous
Agents and Multi-Agent Systems, pp. 280287, Utrecht.
524

fiGAI Auctions

Sandholm, T. (2007). Expressive commerce and its application to sourcing: How we conducted $35 billion of generalized combinatorial auctions. AI Magazine, 28 (3), 4558.
Sandholm, T., & Suri, S. (2006). Side constraints and non-price attributes in markets.
Games and Economic Behavior, 55, 321330.
von Neumann, J., & Morgenstern, O. (1944). Theory of Games and Economic Behavior.
Princeton University Press.
Von Winterfeldt, D., & Edwards, W. (1986). Decision Analysis and Behavioral Research.
Cambridge University Press.
Vulkan, N., & Jennings, N. R. (2000). Efficient mechanisms for the supply of services in
multi-agent environments. Decision Support Systems, 28, 519.
Wellman, M. P., Greenwald, A., & Stone, P. (2007). Autonomous Bidding Agents: Strategies
and Lessons from the Trading Agent Competition. MIT Press.
Wellman, M. P., Osepayshvili, A., MacKie-Mason, J. K., & Reeves, D. M. (2008). Bidding strategies for simultaneous ascending auctions. B. E. Journal of Theoretical
Economics (Topics), 8 (1).

525

fiJournal of Artificial Intelligence Research 37 (2010) 279-328

Submitted 05/09; published 03/10

Join-Graph Propagation Algorithms
Robert Mateescu

ROMATEES @ MICROSOFT. COM

Microsoft Research
7 J J Thomson Avenue
Cambridge CB3 0FB, UK

Kalev Kask

KKASK @ ICS . UCI . EDU

Donald Bren School of Information and Computer Science
University of California Irvine
Irvine, CA 92697, USA

Vibhav Gogate

VGOGATE @ CS . WASHINGTON . EDU

Computer Science & Engineering
University of Washington, Seattle
Seattle, WA 98195, USA

Rina Dechter

DECHTER @ ICS . UCI . EDU

Donald Bren School of Information and Computer Science
University of California Irvine
Irvine, CA 92697, USA

Abstract
The paper investigates parameterized approximate message-passing schemes that are based on
bounded inference and are inspired by Pearls belief propagation algorithm (BP). We start with the
bounded inference mini-clustering algorithm and then move to the iterative scheme called Iterative
Join-Graph Propagation (IJGP), that combines both iteration and bounded inference. Algorithm
IJGP belongs to the class of Generalized Belief Propagation algorithms, a framework that allowed
connections with approximate algorithms from statistical physics and is shown empirically to surpass the performance of mini-clustering and belief propagation, as well as a number of other stateof-the-art algorithms on several classes of networks. We also provide insight into the accuracy of
iterative BP and IJGP by relating these algorithms to well known classes of constraint propagation
schemes.

1. Introduction
Probabilistic inference is the principal task in Bayesian networks and is known to be an NP-hard
problem (Cooper, 1990; Roth, 1996). Most of the commonly used exact algorithms such as jointree clustering (Lauritzen & Spiegelhalter, 1988; Jensen, Lauritzen, & Olesen, 1990) or variableelimination (Dechter, 1996, 1999; Zhang, Qi, & Poole, 1994), and more recently search schemes
(Darwiche, 2001; Bacchus, Dalmao, & Pitassi, 2003; Dechter & Mateescu, 2007) exploit the network structure. While significant advances were made in the last decade in exact algorithms, many
real-life problems are too big and too hard, especially when their structure is dense, since they are
time and space exponential in the treewidth of the graph. Approximate algorithms are therefore
necessary for many practical problems, although approximation within given error bounds is also
NP-hard (Dagum & Luby, 1993; Roth, 1996).

c
2010
AI Access Foundation. All rights reserved.

fiM ATEESCU , K ASK , G OGATE & D ECHTER

The paper focuses on two classes of approximation algorithms for the task of belief updating.
Both are inspired by Pearls belief propagation algorithm (Pearl, 1988), which is known to be exact
for trees. As a distributed algorithm, Pearls belief propagation can also be applied iteratively to
networks that contain cycles, yielding Iterative Belief Propagation (IBP), also known as loopy belief
propagation. When the networks contain cycles, IBP is no longer guaranteed to be exact, but in
many cases it provides very good approximations upon convergence. Some notable success cases
are those of IBP for coding networks (McEliece, MacKay, & Cheng, 1998; McEliece & Yildirim,
2002), and a version of IBP called survey propagation for some classes of satisfiability problems
(Mezard, Parisi, & Zecchina, 2002; Braunstein, Mezard, & Zecchina, 2005).
Although the performance of belief propagation is far from being well understood in general,
one of the more promising avenues towards characterizing its behavior came from analogies with
statistical physics. It was shown by Yedidia, Freeman, and Weiss (2000, 2001) that belief propagation can only converge to a stationary point of an approximate free energy of the system, called
Bethe free energy. Moreover, the Bethe approximation is computed over pairs of variables as terms,
and is therefore the simplest version of the more general Kikuchi (1951) cluster variational method,
which is computed over clusters of variables. This observation inspired the class of Generalized
Belief Propagation (GBP) algorithms, that work by passing messages between clusters of variables.
As mentioned by Yedidia et al. (2000), there are many GBP algorithms that correspond to the same
Kikuchi approximation. A version based on region graphs, called canonical by the authors, was
presented by Yedidia et al. (2000, 2001, 2005). Our algorithm Iterative Join-Graph Propagation is
a member of the GBP class, although it will not be described in the language of region graphs. Our
approach is very similar to and was independently developed from that of McEliece and Yildirim
(2002). For more information on BP state of the art research see the recent survey by Koller (2010).
We will first present the mini-clustering scheme which is an anytime bounded inference scheme
that generalizes the mini-bucket idea. It can be viewed as a belief propagation algorithm over a tree
obtained by a relaxation of the networks structure (using the technique of variable duplication). We
will subsequently present Iterative Join-Graph Propagation (IJGP) that sends messages between
clusters that are allowed to form a cyclic structure.
Through these two schemes we investigate: (1) the quality of bounded inference as an anytime
scheme (using mini-clustering); (2) the virtues of iterating messages in belief propagation type
algorithms, and the result of combining bounded inference with iterative message-passing (in IJGP).
In the background section 2, we overview the Tree-Decomposition scheme that forms the basis
for the rest of the paper. By relaxing two requirements of the tree-decomposition, that of connectedness (via mini-clustering) and that of tree structure (by allowing cycles in the underlying graph),
we combine bounded inference and iterative message-passing with the basic tree-decomposition
scheme, as elaborated in subsequent sections.
In Section 3 we present the partitioning-based anytime algorithm called Mini-Clustering (MC),
which is a generalization of the Mini-Buckets algorithm (Dechter & Rish, 2003). It is a messagepassing algorithm guided by a user adjustable parameter called i-bound, offering a flexible tradeoff
between accuracy and efficiency in anytime style (in general the higher the i-bound, the better the
accuracy). MC algorithm operates on a tree-decomposition, and similar to Pearls belief propagation algorithm (Pearl, 1988) it converges in two passes, up and down the tree. Our contribution
beyond other works in this area (Dechter & Rish, 1997; Dechter, Kask, & Larrosa, 2001) is in: (1)
Extending the partition-based approximation for belief updating from mini-buckets to general treedecompositions, thus allowing the computation of the updated beliefs for all the variables at once.
280

fiJ OIN -G RAPH P ROPAGATION A LGORITHMS

This extension is similar to the one proposed by Dechter et al. (2001), but replaces optimization
with probabilistic inference. (2) Providing empirical evaluation that demonstrates the effectiveness
of the idea of tree-decomposition combined with partition-based approximation for belief updating.
Section 4 introduces the Iterative Join-Graph Propagation (IJGP) algorithm. It operates on a
general join-graph decomposition that may contain cycles. It also provides a user adjustable i-bound
parameter that defines the maximum cluster size of the graph (and hence bounds the complexity),
therefore it is both anytime and iterative. While the algorithm IBP is typically presented as a generalization of Pearls Belief Propagation algorithm, we show that IBP can be viewed as IJGP with the
smallest i-bound.
We also provide insight into IJGPs behavior in Section 4. Zero-beliefs are variable-value pairs
that have zero conditional probability given the evidence. We show that: (1) if a value of a variable
is assessed as having zero-belief in any iteration of IJGP, it remains a zero-belief in all subsequent
iterations; (2) IJGP converges in a finite number of iterations relative to its set of zero-beliefs; and,
most importantly (3) that the set of zero-beliefs decided by any of the iterative belief propagation
methods is sound. Namely any zero-belief determined by IJGP corresponds to a true zero conditional probability relative to the given probability distribution expressed by the Bayesian network.
Empirical results on various classes of problems are included in Section 5, shedding light on
the performance of IJGP(i). We see that it is often superior, or otherwise comparable, to other
state-of-the-art algorithms.
The paper is based in part on earlier conference papers by Dechter, Kask, and Mateescu (2002),
Mateescu, Dechter, and Kask (2002) and Dechter and Mateescu (2003).

2. Background
In this section we provide background for exact and approximate probabilistic inference algorithms
that form the basis of our work. While we present our algorithms in the context of directed probabilistic networks, they are applicable to any graphical model, including Markov networks.
2.1 Preliminaries
Notations: A reasoning problem is defined in terms of a set of variables taking values on finite
domains and a set of functions defined over these variables. We denote variables or subsets of
variables by uppercase letters (e.g., X, Y, Z, S, R . . .) and values of variables by lower case letters
(e.g., x, y, z, s). An assignment (X1 = x1 , . . . , Xn = xn ) can be abbreviated as x = (x1 , . . . , xn ).
For a subset of variables S, DS denotes the Cartesian product of the domains of variables in S. xS
is the projection of x = (x1 , . . . , xn ) over a subset S. We denote functions by letters f , g, h, etc.,
and the scope (set of arguments) of the function f by scope(f ).
D EFINITION 1 (graphical model) (Kask, Dechter, Larrosa, & Dechter, 2005) A graphical model
M is a 3-tuple, M = hX, D, Fi, where: X = {X1 , . . . , Xn } is a finite set of variables; D =
{D1 , . . . , Dn } is the set of their respective finite domains of values; F = {f1 , . . . , fr } is a set
of positive real-valued discrete functions, each defined over a subset of variables Si  X, called
its scope, and denoted by scope(f
P i ). A graphical model typically has an associated combination
1
operator , (e.g.,   {, } - product, sum). The graphical model represents the combination
1. The combination operator can also be defined axiomatically (Shenoy, 1992).

281

fiM ATEESCU , K ASK , G OGATE & D ECHTER

of all its functions: ri=1 fi . A graphical model has an associated primal graph that captures the
structural information of the model:
D EFINITION 2 (primal graph, dual graph) The primal graph of a graphical model is an undirected graph that has variables as its vertices and an edge connects any two vertices whose corresponding variables appear in the scope of the same function. A dual graph of a graphical model has
a one-to-one mapping between its vertices and functions of the graphical model. Two vertices in the
dual graph are connected if the corresponding functions in the graphical model share a variable.
We denote the primal graph by G = (X, E), where X is the set of variables and E is the set of
edges.
D EFINITION 3 (belief networks) A belief (or Bayesian) network is a graphical model B =
hX, D, G, P i, where G = (X, E) is a directed acyclic graph over variables X and P = {pi },
where pi = {p(Xi | pa (Xi ) ) } are conditional probability tables (CPTs) associated with each variable Xi and pa(Xi ) = scope(pi ){Xi } is the set of parents of Xi in G. Given a subset of variables
S, we will write P (s) as the probability P (S = s), where s  DS . A belief network represents
a probability distribution over X, P (x1 , . . . ., xn ) = ni=1 P (xi |xpa(Xi ) ). An evidence set e is an
instantiated subset of variables. The primal graph of a belief network is called a moral graph. It
can be obtained by connecting the parents of each vertex in G and removing the directionality of
the edges. Equivalently, it connects any two variables appearing in the same family (a variable and
its parents in the CPT).
Two common queries in Bayesian networks are Belief Updating (BU) and Most Probable Explanation (MPE).
D EFINITION 4 (belief network queries) The Belief Updating (BU) task is to find the posterior
probability of each single variable given some evidence e, that is to compute P (Xi |e). The Most
Probable Explanation (MPE) task is to find a complete assignment to all the variables having maximum probability given the evidence, that is to compute argmaxX i pi .
2.2 Tree-Decomposition Schemes
Tree-decomposition is at the heart of most general schemes for solving a wide range of automated
reasoning problems, such as constraint satisfaction and probabilistic inference. It is the basis for
many well-known algorithms, such as join-tree clustering and bucket elimination. In our presentation we will follow the terminology of Gottlob, Leone, and Scarcello (2000) and Kask et al. (2005).
D EFINITION 5 (tree-decomposition, cluster-tree) Let B = hX, D, G, P i be a belief network. A
tree-decomposition for B is a triple hT, , i, where T = (V, E) is a tree, and  and  are labeling
functions which associate with each vertex v  V two sets, (v)  X and (v)  P satisfying:
1. For each function pi  P , there is exactly one vertex v  V such that pi  (v), and
scope(pi )  (v).
2. For each variable Xi  X, the set {v  V |Xi  (v)} induces a connected subtree of T .
This is also called the running intersection (or connectedness) property.
We will often refer to a node and its functions as a cluster and use the term tree-decomposition and
cluster-tree interchangeably.
282

fiJ OIN -G RAPH P ROPAGATION A LGORITHMS

D EFINITION 6 (treewidth, separator, eliminator) Let D = hT, , i be a tree-decomposition of
a belief network B. The treewidth (Arnborg, 1985) of D is maxvV |(v)|  1. The treewidth of
B is the minimum treewidth over all its tree-decompositions. Given two adjacent vertices u and
v of a tree-decomposition, the separator of u and v is defined as sep(u, v) = (u)  (v), and
the eliminator of u with respect to v is elim(u, v) = (u)  (v). The separator-width of D is
max(u,v) |sep(u, v)|. The minimum treewidth of a graph G can be shown to be identical to a related
parameter called induced-width (Dechter & Pearl, 1987).
Join-tree and cluster-tree elimination (CTE) In both Bayesian network and constraint satisfaction communities, the most used tree-decomposition method is join-tree decomposition (Lauritzen
& Spiegelhalter, 1988; Dechter & Pearl, 1989), introduced based on relational database concepts
(Maier, 1983). Such decompositions can be generated by embedding the networks moral graph G
into a chordal graph, often using a triangulation algorithm and using its maximal cliques as nodes in
the join-tree. The triangulation algorithm assembles a join-tree by connecting the maximal cliques in
the chordal graph in a tree. Subsequently, every CPT pi is placed in one clique containing its scope.
Using the previous terminology, a join-tree decomposition of a belief network B = hX, D, G, P i is
0
a tree T = (V, E), where V is the set of cliques of a chordal graph G that contains G, and E is a set
of edges that form a tree between cliques, satisfying the running intersection property (Maier, 1983).
Such a join-tree satisfies the properties of tree-decomposition and is therefore a cluster-tree (Kask
et al., 2005). In this paper, we will use the terms tree-decomposition and join-tree decomposition
interchangeably.
There are a few variants for processing join-trees for belief updating (e.g., Jensen et al., 1990;
Shafer & Shenoy, 1990). We adopt here the version from Kask et al. (2005), called cluster-treeelimination (CTE), that is applicable to tree-decompositions in general and is geared towards space
savings. It is a message-passing algorithm; for the task of belief updating, messages are computed
by summation over the eliminator between the two clusters of the product of functions in the originating cluster. The algorithm, denoted CTE-BU (see Figure 1), pays a special attention to the
processing of observed variables since the presence of evidence is a central component in belief
updating. When a cluster sends a message to a neighbor, the algorithm operates on all the functions
in the cluster except the message from that particular neighbor. The message contains a single combined function and individual functions that do not share variables with the relevant eliminator. All
the non-individual functions are combined in a product and summed over the eliminator.
Example 1 Figure 2a describes a belief network and Figure 2b a join-tree decomposition for it.
Figure 2c shows the trace of running CTE-BU with evidence G = ge , where h(u,v) is a message that
cluster u sends to cluster v.
T HEOREM 1 (complexity of CTE-BU) (Dechter et al., 2001; Kask et al., 2005) Given a Bayesian
network B = hX, D, G, P i and a tree-decomposition hT, , i of B, the time complexity of CTE
BU is O(deg  (n + N )  dw +1 ) and the space complexity is O(N  dsep ), where deg is the maximum
degree of a node in the tree-decomposition, n is the number of variables, N is the number of nodes
in the tree-decomposition, d is the maximum domain size of a variable, w is the treewidth and sep
is the maximum separator size.

283

fiM ATEESCU , K ASK , G OGATE & D ECHTER

Algorithm CTE for Belief-Updating (CTE-BU)
Input: A tree-decomposition hT, , i, T = (V, E) for B = hX, D, G, P i. Evidence variables
var(e).
Output: An augmented tree whose nodes are clusters containing the original CPTs and the
messages received from neighbors. P (Xi , e), Xi  X.
Denote by H(u,v) the message from vertex u to v, nev (u) the neighbors of u in T excluding v,
cluster(u) = (u)  {H(v,u) |(v, u)  E},
clusterv (u) = cluster(u) excluding message from v to u.

 Compute messages:
For every node u in T , once u has received messages from all nev (u), compute message to node
v:
1. Process observed variables:
Assign relevant evidence to all pi  (u)
2. Compute the combined function:
X

h(u,v) =

Y

f

elim(u,v) f A

where A is the set of functions in clusterv (u) whose scope intersects elim(u, v).
Add h(u,v) to H(u,v) and add all the individual functions in clusterv (u)  A
Send H(u,v) to node v.
 Compute P (Xi , e):
For every Xi  X let u be a vertex in T such that Xi  (u). Compute P (Xi , e) =
P
Q
(u){Xi } ( f cluster(u) f )

Figure 1: Algorithm Cluster-Tree-Elimination for Belief Updating (CTE-BU).
A

1

 (1) = { A, B, C}
 (1) = { p(a ), p(b | a ), p(c | a, b)}

1

ABC
BC

B

2
C

D

 ( 2) = { B , C , D , F }
 (2) = { p(d | b), p( f | c, d }

2 BCDF

E

BF

3

 (3) = {B, E , F }
 (3) = { p (e | b, f )}

4

 (4) = {E , F , G}
 (4) = { p( g | e, f )}

3

F
G

(a)

BEF
EF

4

(b)

EFG

h (1 , 2 ) ( b , c ) =


(b , c ) = 
a

h ( 2 ,1 )

p ( a )  p (b | a )  p (c | a , b )
p ( d | b )  p ( f | c , d )  h( 3, 2 ) (b , f )

d,f

h( 2 ,3 ) (b , f ) =


h( 3 , 2 ) ( b , f ) = 
e

p (d | b)

h( 3 , 4 ) ( e , f ) =

p ( e | b , f )  h( 2 ,3 ) (b , f )

c ,d



p ( f | c, d )

h (1 , 2 ) ( b , c )

p ( e | b , f )  h( 4 ,3 ) ( e , f )

b

h( 4 ,3 ) ( e , f ) = p ( G = g e | e , f )

(c)

Figure 2: (a) A belief network; (b) A join-tree decomposition; (c) Execution of CTE-BU.

3. Partition-Based Mini-Clustering
The time, and especially the space complexity, of CTE-BU renders the algorithm infeasible for problems with large treewidth. We now introduce Mini-Clustering, a partition-based anytime algorithm
which computes bounds or approximate values on P (Xi , e) for every variable Xi .
284

fiJ OIN -G RAPH P ROPAGATION A LGORITHMS

Procedure MC for Belief Updating (MC-BU(i))
2. Compute the combined mini-functions:
Make an (i)-size mini-cluster partitioning of clusterv (u), {mc(1), . . . , mc(p)};
P
Q
h1(u,v) = elim(u,v) f mc(1) f
Q
hi(u,v) = maxelim(u,v) f mc(i) f i = 2, . . . , p
add {hi(u,v) |i = 1, . . . , p} to H(u,v) . Send H(u,v) to v.
Compute upper bounds P (Xi , e) on P (Xi , e):
For every Xi  X let u  V be a cluster such that Xi  (u). Make (i) mini-clusters from
cluster(u), {mc(1), . . . , mc(p)}; Compute P (Xi , e) =
P
Q
Qp
Q
( (u)Xi f mc(1) f )  ( k=2 max(u)Xi f mc(k) f ).

Figure 3: Procedure Mini-Clustering for Belief Updating (MC-BU).
3.1 Mini-Clustering Algorithm
Combining all the functions of a cluster into a product has a complexity exponential in its number
of variables, which is upper bounded by the induced width. Similar to the mini-bucket scheme
(Dechter, 1999), rather than performing this expensive exact computation, we partition the cluster into p mini-clusters mc(1), . . . , mc(p), each having at most
Pi variables,
Q where i is an accuracy parameter. Instead of computing by CTE-BU h(u,v) =
elim(u,v)
f (u) f , we can divide the functions
of (u)
mc(k), k  {1, . . . , p}, and rewrite h(u,v) =
P into p mini-clusters
Qp Q
P
Q
f
=
mc(k) f . By migrating the summation operator into
elim(u,v)
elim(u,v)
f (u)
Q
P k=1 fQ
p
each mini-cluster, yielding k=1 elim(u,v) f mc(k) f , we get an upper bound on h(u,v) . The
resulting algorithm is called MC-BU(i).
Consequently, the combined functions are approximated via mini-clusters, as follows. Suppose
u  V has received messages from all its neighbors other than v (the message from v is ignored even
if received). The functions in clusterv (u) that are to be combined are partitioned into mini-clusters
{mc(1), . . . , mc(p)}, each one containing at most i variables. Each mini-cluster is processed by
summation over the eliminator, and the resulting combined functions as well as all the individual
functions are sent to v. It was shown by Dechter and Rish (2003) that the upper bound can be
improved by using the maximization operator max rather than the summation operator sum on some
mini-buckets. Similarly, lower bounds can be generated by replacing sum with min (minimization)
for some mini-buckets. Alternatively, we can replace sum by a mean operator (taking the sum and
dividing by the number of elements in the sum), in this case deriving an approximation of the joint
belief instead of a strict upper bound.
Algorithm MC-BU for upper bounds can be obtained from CTE-BU by replacing step 2 of the
main loop and the final part of computing the upper bounds on the joint belief by the procedure given
in Figure 3. In the implementation we used for the experiments reported here, the partitioning was
done in a greedy brute-force manner. We ordered the functions according to their sizes (number of
variables), breaking ties arbitrarily. The largest function was placed in a mini-cluster by itself. Then,
we picked the largest remaining function and probed the mini-clusters in the order of their creation,
285

fiM ATEESCU , K ASK , G OGATE & D ECHTER

1

ABC

BC

H (1, 2)

h(11, 2 ) (b, c) :=  p (a )  p (b | a )  p(c | a, b)
a
1
( 2 ,1)

h

H ( 2,1)

(b) :=  p (d | b)  h(13, 2 ) (b, f )
d, f

h(22,1) (c) := max p ( f | c, d )
d, f

2

BCDF
1
( 2 , 3)

h

H ( 2 , 3)
BF

3

BEF

EF

c ,d

h(22,3) ( f ) := max p( f | c, d )
c,d

1
( 3, 2 )

(b, f ) :=  p(e | b, f )  h(14,3) (e, f )

H ( 3, 2 )

h

H ( 3, 4 )

h(13, 4 ) (e, f ) :=  p (e | b, f )  h(12,3) (b)  h(22,3) ( f )

H ( 4 , 3)

4

(b) :=  p (d | b)  h(11, 2 ) (b, c)

e

b

1
( 4 , 3)

h

(e, f ) := p(G = g e | e, f )

EFG

Figure 4: Execution of MC-BU for i = 3.
trying to find one that together with the new function would have no more than i variables. A new
mini-cluster was created whenever the existing ones could not accommodate the new function.
Example 2 Figure 4 shows the trace of running MC-BU(3) on the problem in Figure 2. First, evidence G = ge is assigned in all CPTs. There are no individual functions to be sent from cluster 1
to cluster 2. Cluster 1 contains only 3 variables,
(1) = {A, B, C}, therefore it is not partitioned.
P
p(a)

p(b|a)  p(c|a, b) is computed and the message
The combined function h1(1,2) (b, c) =
a
1
H(1,2) = {h(1,2) (b, c)} is sent to node 2. Now, node 2 can send its message to node 3. Again, there
are no individual functions. Cluster 2 contains 4 variables, (2) = {B, C, D, F }, and a partitioning is necessary: MC-BU(3) can choose
P mc(1) = {p(d|b), h(1,2) (b,2c)} and mc(2) = {p(f |c, d)}.
1
The combined functions h(2,3) (b) = c,d p(d|b)  h(1,2) (b, c) and h(2,3) (f ) = maxc,d p(f |c, d) are
computed and the message H(2,3) = {h1(2,3) (b), h2(2,3) (f )} is sent to node 3. The algorithm continues until every node has received messages from all its neighbors. An upper bound on p(a, G = ge )
can now be computed by choosing cluster
1, which contains variable A. It doesnt need partitionP
ing, so the algorithm just computes b,c p(a)  p(b|a)  p(c|a, b)  h1(2,1) (b)  h2(2,1) (c). Notice that
unlike CTE-BU which processes 4 variables in cluster 2, MC-BU(3) never processes more than 3
variables at a time.
It was already shown that:
T HEOREM 2 (Dechter & Rish, 2003) Given a Bayesian network B = hX, D, G, P i and the evidence e, the algorithm MC-BU(i) computes an upper bound on the joint probability P (Xi , e) of
each variable Xi (and each of its values) and the evidence e.
T HEOREM 3 (complexity of MC-BU(i)) (Dechter et al., 2001) Given a Bayesian network B =
hX, D, G, P i and a tree-decomposition hT, , i of B, the time and space complexity of MC-BU(i)
is O(n  hw  di ), where n is the number of variables, d is the maximum domain size of a variable
and hw = maxuT |{f  P |scope(f )  (u) 6= }|, which bounds the number of mini-clusters.
286

fiJ OIN -G RAPH P ROPAGATION A LGORITHMS








ffff
ff





ffff


fffifffi
	



ff"ff"
#$%& 'ff" & 



! ! !



ff""ff""
()* $ && '%&&
!! !! !!


ffff
!

fffifffi
!	



+

Figure 5: Node duplication semantics of MC: (a) trace of MC-BU(3); (b) trace of CTE-BU.
Semantics of Mini-Clustering The mini-bucket scheme was shown to have the semantics of relaxation via node duplication (Kask & Dechter, 2001; Choi, Chavira, & Darwiche, 2007). We
extend it to mini-clustering by showing how it can apply as is to messages that flow in one direction
(inward, from leaves to root), as follows. Given a tree-decomposition D, where CTE-BU computes
a function h(u,v) (the message that cluster u sends to cluster v), MC-BU(i) partitions cluster u into p
mini-clusters u1 , . . . , up , which are processed independently and then the resulting functions h(ui ,v)
are sent to v. Instead consider a different decomposition D0 , which is just like D, with the exception that (a) instead of u, it has clusters u1 , . . . , up , all of which are children of v, and each variable
appearing in more than a single mini-cluster becomes a new variable, (b) each child w of u (in D) is
a child of uk (in D0 ), such that h(w,u) (in D) is assigned to uk (in D0 ) during the partitioning. Note
that D0 is not a legal tree-decomposition relative to the original variables since it violates the connectedness property: the mini-clusters u1 , . . . , up contain variables elim(u, v) but the path between
the nodes u1 , . . . , up (this path goes through v) does not. However, it is a legal tree-decomposition
relative to the new variables. It is straightforward to see that H(u,v) computed by MC-BU(i) on D
is the same as {h(ui ,v) |i = 1, . . . , p} computed by CTE-BU on D0 in the direction from leaves to
root.
If we want to capture the semantics of the outward messages from root to leaves, we need to generate a different relaxed decomposition (D00 ) because MC, as defined, allows a different partitioning
in the up and down streams of the same cluster. We could of course stick with the decomposition in
D0 and use CTE in both directions which would lead to another variant of mini-clustering.
Example 3 Figure 5(a) shows a trace of the bottom-up phase of MC-BU(3) on the network in Figure
4. Figure 5(b) shows a trace of the bottom-up phase of CTE-BU algorithm on a problem obtained
from the problem in Figure 4 by splitting nodes D (into D0 and D00 ) and F (into F 0 and F 00 ).
The MC-BU algorithm computes an upper bound P (Xi , e) on the joint probability P (Xi , e).
However, deriving a bound on the conditional probability P (Xi |e) is not easy when the exact
287

fiM ATEESCU , K ASK , G OGATE & D ECHTER

Random Bayesian N=50 K=2 P=2 C=48
0.20
0.18
0.16

Avg abs error

0.14
0.12
0.10
0.08
#ev=0
#ev=10
#ev=20
#ev=30

0.06
0.04
0.02
0.00
0

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

Number of iterations

Figure 6: Convergence of IBP (50 variables, evidence from 0-30 variables).
value of P (e) is not available. If we just try to divide (multiply) P (Xi , e) by a constant, the
result is not
P necessarily an upper bound on P (Xi |e). It is easy to show that normalization,
P (xi , e)/ xi Di P (xi , e), with the mean operator is identical to normalization of MC-BU output
when applying the summation operator in all the mini-clusters.
MC-BU(i) is an improvement over the Mini-Bucket algorithm MB(i), in that it allows the computation of P (Xi , e) for all variables with a single run, whereas MB(i) computes P (Xi , e) for just
one variable, with a single run. When computing P (Xi , e) for each variable, MB(i) has to be run
n times, once for each variable, an algorithm we call nMB(i). It was demonstrated by Mateescu
et al. (2002) that MC-BU(i) has up to linear speed-up over nMB(i). For a given i, the accuracy of
MC-BU(i) can be shown to be not worse than that of nMB(i).
3.2 Experimental Evaluation of Mini-Clustering
The work of Mateescu et al. (2002) and Kask (2001) provides an empirical evaluation of MC-BU
that reveals the impact of the accuracy parameter on its quality of approximation and compares with
Iterative Belief Propagation and a Gibbs sampling scheme. We will include here only a subset of
these experiments which will provide the essence of our results. Additional empirical evaluation of
MC-BU will be given when comparing against IJGP later in this paper.
We tested the performance of MC-BU(i) on random Noisy-OR networks, random coding networks, general random networks, grid networks, and three benchmark CPCS files with 54, 360 and
422 variables respectively (these are belief networks for medicine, derived from the Computer based
Patient Case Simulation system, known to be hard for belief updating). On each type of network we
ran Iterative Belief Propagation (IBP) - set to run at most 30 iterations, Gibbs Sampling (GS) and
MC-BU(i), with i from 2 to the treewidth w to capture the anytime behavior of MC-BU(i).
The random networks were generated using parameters (N,K,C,P), where N is the number of
variables, K is their domain size (we used only K=2), C is the number of conditional probability
tables and P is the number of parents in each conditional probability table. The parents in each table
are picked randomly given a topological ordering, and the conditional probability tables are filled
288

fiJ OIN -G RAPH P ROPAGATION A LGORITHMS

0
|e| 10
20

NHD
max

IBP

MC-BU(2)

MC-BU(5)

MC-BU(8)

0
0
0
0
0
0
0
0
0

mean
0
0
0
0
0
0
0
0
0
0
0
0

N=50, P=2, 50 instances
Abs. Error
max

1.6E-03
1.1E-03
5.7E-04
1.1E-03
7.7E-04
2.8E-04
3.6E-04
1.7E-04
3.5E-05

mean
9.0E-09
3.4E-04
9.6E-04
1.1E-03
8.4E-04
4.8E-04
9.4E-04
6.9E-04
2.7E-04
3.2E-04
1.5E-04
3.5E-05

Rel. Error

max

1.9E+00
1.4E+00
7.1E-01
1.4E+00
9.3E-01
3.5E-01
4.4E-01
2.0E-01
4.3E-02

mean
1.1E-05
4.2E-01
1.2E+00
1.3E+00
1.0E+00
5.9E-01
1.2E+00
8.4E-01
3.3E-01
3.9E-01
1.9E-01
4.3E-02

Time
max

0.056
0.048
0.039
0.070
0.063
0.058
0.214
0.184
0.123

mean
0.102
0.081
0.062
0.057
0.049
0.039
0.072
0.066
0.057
0.221
0.190
0.127

Table 1: Performance on Noisy-OR networks, w = 10: Normalized Hamming Distance, absolute
error, relative error and time.

randomly. The grid networks have the structure of a square, with edges directed to form a diagonal
flow (all parallel edges have the same direction). They were generated by specifying N (a square
integer) and K (we used K=2). We also varied the number of evidence nodes, denoted by |e| in
the tables. The parameter values are reported in each table. For all the problems, Gibbs sampling
performed consistently poorly so we only include part of its results here.
In our experiments we focused on the approximation power of MC-BU(i). We compared two
versions of the algorithm. In the first version, for every cluster, we used the max operator in all its
mini-clusters, except for one of them that was processed by summation. In the second version, we
used the operator mean in all the mini-clusters. We investigated this second version of the algorithm
for two reasons: (1) we compare MC-BU(i) with IBP and Gibbs sampling, both of which are also
approximation algorithms, so it would not be possible to compare with a bounding scheme; (2) we
observed in our experiments that, although the bounds improve as the i-bound increases, the quality
of bounds computed by MC-BU(i) was still poor, with upper bounds being greater than 1 in many
cases.2 Notice that we need to maintain the sum operator for at least one of the mini-clusters. The
mean operator simply performs summation and divides by the number of elements in the sum. For
example, if A, B, C are binary variables (taking values 0 and 1), and f (A, B, C) is the aggregated
function of one mini-cluster,
and elim = {A, B}, then computing the message h(C) by the mean
P
operator gives: 1/4 A,B{0,1} f (A, B, C).
We computed the exact solution and used three different measures of accuracy: 1) Normalized
Hamming Distance (NHD) - we picked the most likely value for each variable for the approximate
and for the exact, took the ratio between the number of disagreements and the total number of variables, and averaged over the number of problems that we ran for each class; 2) Absolute Error (Abs.
Error) - is the absolute value of the difference between the approximate and the exact, averaged
over all values (for each variable), all variables and all problems; 3) Relative Error (Rel. Error) - is
the absolute value of the difference between the approximate and the exact, divided by the exact,
averaged over all values (for each variable), all variables and all problems. For coding networks,
2. Wexler and Meek (2008) compared the upper/lower bounding properties of the mini-bucket on computing probability
of evidence. Rollon and Dechter (2010) further investigated heuristic schemes for mini-bucket partitioning.

289

fiM ATEESCU , K ASK , G OGATE & D ECHTER

10
|e| 20
30

N=50, P=3, 25 instances
Abs. Error

NHD
max

mean
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

IBP
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

MC-BU(2)

MC-BU(5)

MC-BU(8)

MC-BU(11)

MC-BU(14)

max

1.3E-03
5.3E-04
2.3E-04
1.0E-03
4.6E-04
2.0E-04
6.6E-04
1.8E-04
3.4E-05
2.6E-04
3.8E-05
6.4E-07
4.2E-05
0
0

mean
1.3E-04
3.6E-04
6.8E-04
9.6E-04
4.0E-04
1.9E-04
8.3E-04
4.1E-04
1.9E-04
5.7E-04
1.8E-04
3.4E-05
2.4E-04
3.8E-05
6.4E-07
4.1E-05
0
0

Rel. Error
max

Time

mean
7.9E-01
2.2E+00
4.2E+00
5.8E+00
2.4E+00
1.2E+00
5.1E+00
2.4E+00
1.2E+00
3.5E+00
1.0E+00
2.1E-01
1.5E+00
2.3E-01
4.0E-03
2.4E-01
0
0

8.2E+00
3.1E+00
1.4E+00
6.4E+00
2.7E+00
1.2E+00
4.0E+00
1.1E+00
2.1E-01
1.6E+00
2.3E-01
4.0E-03
2.5E-01
0
0

max

mean
0.242
0.184
0.121
0.108
0.077
0.064
0.133
0.105
0.095
0.509
0.406
0.308
2.378
1.439
0.624
7.875
2.093
0.638

0.107
0.077
0.064
0.133
0.104
0.098
0.498
0.394
0.300
2.339
1.421
0.613
7.805
2.075
0.630

Table 2: Performance on Noisy-OR networks, w = 16: Normalized Hamming Distance, absolute
error, relative error and time.
Noisy-OR networks, N=50, P=3, evid=20, w*=16, 25 instances

Noisy-OR networks, N=50, P=3, evid=10, w*=16, 25 instances

1e+0

1e+0

MC
IBP
Gibbs Sampling

MC
IBP
Gibbs Sampling

1e-1

Absolute error

Absolute error

1e-1

1e-2

1e-3

1e-2

1e-3

1e-4

1e-4

1e-5

1e-5
0

2

4

6

8

10

12

14

16

0

2

4

6

8

10

12

14

16

i-bound

i-bound

Figure 7: Absolute error for Noisy-OR networks.
we report only one measure, Bit Error Rate (BER). In terms of the measures defined above, BER
is the normalized Hamming distance between the approximate (computed by an algorithm) and the
actual input (which in the case of coding networks may be different from the solution given by
exact algorithms), so we denote them differently to make this semantic distinction. We also report
the time taken by each algorithm. For reported metrics (time, error, etc.) provided in the Tables, we
give both mean and max values.
In Figure 6 we show that IBP converges after about 5 iterations. So, while in our experiments
we report its time for 30 iterations, its time is even better when sophisticated termination is used.
These results are typical of all runs.

290

fiJ OIN -G RAPH P ROPAGATION A LGORITHMS

Random networks, N=50, P=2, k=2, evid=0, w*=10, 50 instances

Random networks, N=50, P=2, k=2, evid=10, w*=10, 50 instances

0.16

0.16

MC
Gibbs Sampling
IBP

0.14

0.14

MC
Gibbs Sampling
IBP

0.12

0.10

Absolute error

Absolute error

0.12

0.08
0.06
0.04

0.10
0.08
0.06
0.04

0.02

0.02

0.00

0.00

0

2

4

6

8

10

0

2

4

i-bound

6

8

10

i-bound

Figure 8: Absolute error for random networks.
BER

 = .22
max
mean

IBP
GS
MC-BU(2)
MC-BU(4)
MC-BU(6)
MC-BU(8)

0.000
0.483
0.002
0.001
0.000
0.000

0.000
0.483
0.002
0.001
0.000
0.000

IBP
GS
MC-BU(2)
MC-BU(4)
MC-BU(6)
MC-BU(8)
MC-BU(10)

0.000
0.506
0.006
0.006
0.005
0.002
0.001

0.000
0.506
0.006
0.006
0.005
0.002
0.001

 = .26
 = .32
 = .40
max
mean
max
mean
max
mean
N=100, P=3, 50 instances, w*=7
0.000 0.000 0.002 0.002 0.022 0.022
0.483 0.483 0.483 0.483 0.483 0.483
0.004 0.004 0.024 0.024 0.068 0.068
0.002 0.002 0.018 0.018 0.046 0.045
0.000 0.000 0.004 0.004 0.038 0.038
0.000 0.000 0.002 0.002 0.023 0.023
N=100, P=4, 50 instances, w*=11
0.000 0.000 0.002 0.002 0.013 0.013
0.506 0.506 0.506 0.506 0.506 0.506
0.015 0.015 0.043 0.043 0.093 0.094
0.017 0.017 0.049 0.049 0.104 0.102
0.011 0.011 0.035 0.034 0.071 0.074
0.004 0.004 0.022 0.022 0.059 0.059
0.001 0.001 0.008 0.008 0.033 0.032

 = .51
max
mean

Time

0.088
0.483
0.132
0.110
0.106
0.091

0.088
0.483
0.131
0.110
0.106
0.091

0.00
31.36
0.08
0.08
0.12
0.19

0.075
0.506
0.157
0.158
0.151
0.121
0.101

0.075
0.506
0.157
0.158
0.150
0.122
0.102

0.00
39.85
0.19
0.19
0.29
0.71
1.87

Table 3: Bit Error Rate (BER) for coding networks.

Random Noisy-OR networks results are summarized in Tables 1 and 2, and Figure 7. For NHD,
both IBP and MC-BU gave perfect results. For the other measures, we noticed that IBP is more
accurate when there is no evidence by about an order of magnitude. However, as evidence is added,
IBPs accuracy decreases, while MC-BUs increases and they give similar results. We see that
MC-BU gets better as the accuracy parameter i increases, which shows its anytime behavior.
General random networks results are summarized in Figure 8. They are similar to those for
random Noisy-OR networks. Again, IBP has the best result only when the number of evidence
variables is small. It is remarkable how quickly MC-BU surpasses the performance of IBP as
evidence is added (for more, see the results of Mateescu et al., 2002).
Random coding networks results are given in Table 3 and Figure 9. The instances fall within the
class of linear block codes, ( is the channel noise level). It is known that IBP is very accurate for
this class. Indeed, these are the only problems that we experimented with where IBP outperformed
MC-BU throughout. The anytime behavior of MC-BU can again be seen in the variation of numbers
in each column and more vividly in Figure 9.
291

fiM ATEESCU , K ASK , G OGATE & D ECHTER

Coding networks, N=100, P=4, sigma=.51, w*=12, 50 instances

Coding networks, N=100, P=4, sigma=.22, w*=12, 50 instances

0.18

0.007
MC
IBP

0.006

MC
IBP

0.16

0.005

Bit Error Rate

Bit Error Rate

0.14
0.004
0.003
0.002

0.12

0.10

0.001

0.08
0.000

0.06
0

2

4

6

8

10

0

12

2

4

6

8

10

12

i-bound

i-bound

Figure 9: Bit Error Rate (BER) for coding networks.
Grid 15x15, evid=10, w*=22, 10 instances

Grid 15x15, evid=10, w*=22, 10 instances

0.06

12
MC
IBP

0.05

MC
IBP

10

Time (seconds)

Absolute error

8
0.04

0.03

0.02

6

4

2
0.01

0

0.00
0

2

4

6

8

10

12

14

16

18

0

2

4

6

i-bound

8

10

12

14

16

18

i-bound

Figure 10: Grid 15x15: absolute error and time.
Grid networks results are given in Figure 10. We notice that IBP is more accurate for no evidence
and MC-BU is better as more evidence is added. The same behavior was consistently manifested
for smaller grid networks that we experimented with (from 7x7 up to 14x14).
CPCS networks results We also tested on three CPCS benchmark files. The results are given
in Figure 11. It is interesting to notice that the MC-BU scheme scales up to fairly large networks,
like the real life example of CPCS422 (induced width 23). IBP is again more accurate when there
is no evidence, but is surpassed by MC-BU when evidence is added. However, whereas MC-BU
is competitive with IBP time-wise when i-bound is small, its runtime grows rapidly as i-bound
increases. For more details on all these benchmarks see the results of Mateescu et al. (2002).
Summary Our results show that, as expected, IBP is superior to all other approximations for
coding networks. However, for random Noisy-OR, general random, grid networks and the CPCS
networks, in the presence of evidence, the mini-clustering scheme is often superior even in its weakest form. The empirical results are particularly encouraging as we use an un-optimized scheme that
exploits a universal principle applicable to many reasoning tasks.

292

fiJ OIN -G RAPH P ROPAGATION A LGORITHMS

CPCS 422, evid=0, w*=23, 1 instance

CPCS 422, evid=10, w*=23, 1 instance

0.05

0.05
MC
IBP

MC
IBP

0.04

Absolute error

Absolute error

0.04

0.03

0.02

0.01

0.03

0.02

0.01

0.00

0.00
2

4

6

8

10

12

14

16

18

2

4

6

i-bound

8

10

12

14

16

18

i-bound

Figure 11: Absolute error for CPCS422.

4. Join-Graph Decomposition and Propagation
In this section we introduce algorithm Iterative Join-Graph Propagation (IJGP) which, like miniclustering, is designed to benefit from bounded inference, but also exploit iterative message-passing
as used by IBP. Algorithm IJGP can be viewed as an iterative version of mini-clustering, improving
the quality of approximation, especially for low i-bounds. Given a cluster of the decomposition,
mini-clustering can potentially create a different partitioning for every message sent to a neighbor.
This dynamic partitioning can happen because the incoming message from each neighbor has to be
excluded when realizing the partitioning, so a different set of functions are split into mini-clusters for
every message to a neighbor. We can define a version of mini-clustering where for every cluster we
create a unique static partition into mini-clusters such that every incoming message can be included
into one of the mini-clusters. This version of MC can be extended into IJGP by introducing some
links between mini-clusters of the same cluster, and carefully limiting the interaction between the
resulting nodes in order to eliminate over-counting.
Algorithm IJGP works on a general join-graph that may contain cycles. The cluster size of the
graph is user adjustable via the i-bound (providing the anytime nature), and the cycles in the graph
allow the iterative application of message-passing. In Subsection 4.1 we introduce join-graphs and
discuss their properties. In Subsection 4.2 we describe the IJGP algorithm itself.
4.1 Join-Graphs
D EFINITION 7 (join-graph decomposition) A join-graph decomposition for a belief network B =
hX, D, G, P i is a triple D = hJG, , i, where JG = (V, E) is a graph, and  and  are labeling
functions which associate with each vertex v  V two sets, (v)  X and (v)  P such that:
1. For each pi  P , there is exactly one vertex v  V such that pi  (v), and scope(pi ) 
(v).
2. (connectedness) For each variable Xi  X, the set {v  V |Xi  (v)} induces a connected
subgraph of JG. The connectedness requirement is also called the running intersection property.

293

fiM ATEESCU , K ASK , G OGATE & D ECHTER

2,4

1,2,4

2,3,4

A

C
1,4

2,3,4

A

3,4

B

2,4

1,2,4

C
1,4

1,3,4

B

a)

3
1,3,4

b)

Figure 12: An edge-labeled decomposition.
We will often refer to a node in V and its CPT functions as a cluster3 and use the term joingraph decomposition and cluster-graph interchangeably. Clearly, a join-tree decomposition or a
cluster-tree is the special case when the join-graph D is a tree.
It is clear that one of the problems of message propagation over cyclic join-graphs is overcounting. To reduce this problem we devise a scheme, which avoids cyclicity with respect to any
single variable. The algorithm works on edge-labeled join-graphs.
D EFINITION 8 (minimal edge-labeled join-graph decompositions) An edge-labeled join-graph
decomposition for B = hX, D, G, P i is a four-tuple D = hJG, , , i, where JG = (V, E)
is a graph,  and  associate with each vertex v  V the sets (v)  X and (v)  P and 
associates with each edge (v, u)  E the set ((v, u))  X such that:
1. For each function pi  P , there is exactly one vertex v  V such that pi  (v), and
scope(pi )  (v).
2. (edge-connectedness) For each edge (u, v), ((u, v))  (u)  (v), such that Xi  X,
any two clusters containing Xi can be connected by a path whose every edge label includes
Xi .
Finally, an edge-labeled join-graph is minimal if no variable can be deleted from any label while
still satisfying the edge-connectedness property.
D EFINITION 9 (separator, eliminator of edge-labeled join-graphs) Given two adjacent vertices
u and v of JG, the separator of u and v is defined as sep(u, v) = ((u, v)), and the eliminator of u
with respect to v is elim(u, v) = (u)  ((u, v)). The separator width is max(u,v) |sep(u, v)|.
Edge-labeled join-graphs can be made label minimal by deleting variables from the labels while
maintaining connectedness (if an edge label becomes empty, the edge can be deleted). It is easy to
see that,
Proposition 1 A minimal edge-labeled join-graph does not contain any cycle relative to any single
variable. That is, any two clusters containing the same variable are connected by exactly one path
labeled with that variable.
Notice that every minimal edge-labeled join-graph is edge-minimal (no edge can be deleted), but
not vice-versa.
3. Note that a node may be associated with an empty set of CPTs.

294

fiJ OIN -G RAPH P ROPAGATION A LGORITHMS

Example 4 The example in Figure 12a shows an edge minimal join-graph which contains a cycle
relative to variable 4, with edges labeled with separators. Notice however that if we remove variable 4 from the label of one edge we will have no cycles (relative to single variables) while the
connectedness property is still maintained.
The mini-clustering approximation presented in the previous section works by relaxing the jointree requirement of exact inference into a collection of join-trees having smaller cluster size. It
introduces some independencies in the original problem via node duplication and applies exact inference on the relaxed model requiring only two message passings. For the class of IJGP algorithms
we take a different route. We choose to relax the tree-structure requirement and use join-graphs
which do not introduce any new independencies, and apply iterative message-passing on the resulting cyclic structure.
Indeed, it can be shown that any join-graph of a belief network is an I-map (independency map,
Pearl, 1988) of the underlying probability distribution relative to node-separation. Since we plan
to use minimally edge-labeled join-graphs to address over-counting problems, the question is what
kind of independencies are captured by such graphs.
D EFINITION 10 (edge-separation in edge-labeled join-graphs) Let D = hJG, , , i, JG =
(V, E) be an edge-labeled decomposition of a Bayesian network B = hX, D, G, P i. Let NW , NY 
V be two sets of nodes, and EZ  E be a set of edges in JG. Let W, Y, Z be their corresponding
sets of variables (W = vNW (v), Z = eEZ (e)). We say that EZ edge-separates NW and
NY in D if there is no path between NW and NY in the JG graph whose edges in EZ are removed.
In this case we also say that W is separated from Y given Z in D, and write hW |Z|Y iD . Edgeseparation in a regular join-graph is defined relative to its separators.
T HEOREM 4 Any edge-labeled join-graph decomposition D = hJG, , , i of a belief network
B = hX, D, G, P i is an I-map of P relative to edge-separation. Namely, any edge separation in D
corresponds to conditional independence in P .
Proof: Let M G be the moral graph of BN . Since M G is an I-map of P , it is enough to prove that
JG is an I-map of M G. Let NW and NY be disjoint sets of nodes and NZ be a set of edges in JG,
and W, Z, Y be their corresponding sets of variables in M G. We will prove:
hNW |NZ |NY iJG = hW |Z|Y iM G
by contradiction. Since the sets W, Z, Y may not be disjoint, we will actually prove that hW 
Z|Z|Y  ZiM G holds, this being equivalent to hW |Z|Y iM G .
Supposing hW  Z|Z|Y  ZiM G is false, then there exists a path  = 1 , 2 , . . . , n1 ,  = n
in M G that goes from some variable  = 1  W  Z to some variable  = n  Y  Z without
intersecting variables in Z. Let Nv be the set of all nodes in JG that contain variable v  X, and
let us consider the set of nodes:
S = ni=1 Ni  NZ
We argue that S forms a connected sub-graph in JG. First, the running intersection property
ensures that every Ni , i = 1, . . . , n, remains connected in JG after removing the nodes in NZ
(otherwise, it must be that there was a path between the two disconnected parts in the original JG,
which implies that a i is part of Z, which is a contradiction). Second, the fact that (i , i+1 ), i =
295

fiM ATEESCU , K ASK , G OGATE & D ECHTER

1, . . . , n  1, is an edge in the moral graph M G implies that there is a conditional probability table
(CPT) on both i and i+1 , i = 1, . . . , n  1 (and perhaps other variables). From property 1 of the
definition of the join-graph, it follows that for all i = 1, . . . , n  1 there exists a node in JG that
contains both i and i+1 . This proves the existence of a path in the mutilated join-graph (JG with
NZ pulled out) from a node in NW containing  = 1 to the node containing both 1 and 2 (N1
is connected), then from that node to the one containing both 2 and 3 (N2 is connected), and
so on until we reach a node in NY containing  = n . This shows that hNW |NZ |NY iJG is false,
concluding the proof by contradiction. 2
Interestingly however, deleting variables from edge labels or removing edges from edge-labeled
join-graphs whose clusters are fixed will not increase the independencies captured by edge-labeled
join-graphs. That is,
Proposition 2 Any two (edge-labeled) join-graphs defined on the same set of clusters, sharing (V ,
, ), express exactly the same set of independencies relative to edge-separation, and this set of
independencies is identical to the one expressed by node separation in the primal graph of the
join-graph.
Proof: This follows by looking at the primal graph of the join-graph (obtained by connecting any
two nodes in a cluster by an arc over the original variables as nodes) and observing that any edgeseparation in a join-graph corresponds to a node separation in the primal graph and vice-versa. 2
Hence, the issue of minimizing computational over-counting due to cycles appears to be unrelated to the problem of maximizing independencies via minimal I-mapness. Nevertheless, to avoid
over-counting as much as possible, we still prefer join-graphs that minimize cycles relative to each
variable. That is, we prefer minimal edge-labeled join-graphs.
Relationship with region graphs There is a strong relationship between our join-graphs and the
region graphs of Yedidia et al. (2000, 2001, 2005). Their approach was inspired by advances in
statistical physics, when it was realized that computing the partition function is essentially the same
combinatorial problem that expresses probabilistic reasoning. As a result, variational methods from
physics could have counterparts in reasoning algorithms. It was proved by Yedidia et al. (2000,
2001) that belief propagation on loopy networks can only converge (when it does so) to stationary
points of the Bethe free energy. The Bethe approximation is only the simplest case of the more
general Kikuchi (1951) cluster variational method. The idea is to group the variables together in
clusters and perform exact computation in each cluster. One key question is then how to aggregate
the results, and how to account for the variables that are shared between clusters. Again, the idea
that everything should be counted exactly once is very important. This led to the proposal of region
graphs (Yedidia et al., 2001, 2005) and the associated counting numbers for regions. They are
given as a possible canonical version of graphs that can support Generalized Belief Propagation
(GBP) algorithms. The join-graphs accomplish the same thing. The edge-labeled join-graphs can
be described as region graphs where the regions are the clusters and the labels on the edges. The
tree-ness condition with respect to every variable ensures that there is no over-counting.
A very similar approach to ours, which is also based on join-graphs appeared independently by
McEliece and Yildirim (2002), and it is based on an information theoretic perspective.

296

fiJ OIN -G RAPH P ROPAGATION A LGORITHMS

Algorithm Iterative Join-Graph Propagation (IJGP)
Input An arc-labeled join-graph decomposition hJG, , , i, JG = (V, E) for B = hX, D, G, P i. Evidence variables var(e).
Output An augmented graph whose nodes are clusters containing the original CPTs and the messages received
from neighbors. Approximations of P (Xi |e), Xi  X.
Denote by h(u,v) the message from vertex u to v, nev (u) the neighbors of u in JG excluding v.
cluster(u) = (u)  {h(v,u) |(v, u)  E}.
clusterv (u) = cluster(u) excluding message from v to u.
 One iteration of IJGP:
For every node u in JG in some topological order d and back, do
1. Process observed variables:
Assign relevant evidence to all pi  (u) (u) := (u)  var(e), u  V
2. Compute individual functions:
Include in H(u,v) each function in clusterv (u) whose scope does not contain variables in elim(u, v).
Denote by A the remaining functions.
P
Q
3. Compute and send to v the combined function: h(u,v) = elim(u,v) f A f .
Send h(u,v) and the individual functions H(u,v) to node v.
Endfor
 Compute an approximation of P (Xi |e):
For every Xi  X let u be
P a vertex in JG
Q such that Xi  (u).
Compute P (Xi , e) =  (u){Xi } ( f cluster(u) f )

Figure 13: Algorithm Iterative Join-Graph Propagation (IJGP).
4.2 Algorithm IJGP
Applying CTE iteratively to minimal edge-labeled join-graphs yields our algorithm Iterative JoinGraph Propagation (IJGP) described in Figure 13. One iteration of the algorithm applies messagepassing in a topological order over the join-graph, forward and back. When node u sends a message
(or messages) to a neighbor node v it operates on all the CPTs in its cluster and on all the messages
sent from its neighbors excluding the ones received from v. First, all individual functions that share
no variables with the eliminator are collected and sent to v. All the rest of the functions are combined
in a product and summed over the eliminator between u and v.
Based on the results by Lauritzen and Spiegelhalter (1988) and Larrosa, Kask, and Dechter
(2001) it can be shown that:
T HEOREM 5
1. If IJGP is applied to a join-tree decomposition it reduces to join-tree clustering, and therefore it is guaranteed to compute the exact beliefs in one iteration.

2. The time complexity of one iteration of IJGP is O(deg  (n + N )  dw +1 ) and its space
complexity is O(N  d ), where deg is the maximum degree of a node in the join-graph, n
is the number of variables, N is the number of nodes in the graph decomposition, d is the
maximum domain size, w is the maximum cluster size and  is the maximum label size.
For proof, see the properties of CTE presented by Kask et al. (2005).

297

fiM ATEESCU , K ASK , G OGATE & D ECHTER

3

A
2
B

C

a)

A

AB

3

A
A

B

b)

1

ABC

2
AB

A

A

1
AB

ABC

c)

Figure 14: a) A belief network; b) A dual join-graph with singleton labels; c) A dual join-graph
which is a join-tree.

The special case of Iterative Belief Propagation Iterative belief propagation (IBP) is an iterative application of Pearls algorithm that was defined for poly-trees (Pearl, 1988), to any Bayesian
network. We will describe IBP as an instance of join-graph propagation over a dual join-graph.
D EFINITION 11 (dual graphs, dual join-graphs) Given a set of functions F = {f1 , . . . , fl } over
scopes S1 , . . . , Sl , the dual graph of F is a graph DG = (V, E, L) that associates a node with
each function, namely V = F and an edge connects any two nodes whose functions scope share a
variable, E = {(fi , fj )|Si  Sj 6= }. L is a set of labels for the edges, each edge being labeled
by the shared variables of its nodes, L = {lij = Si  Sj |(i, j)  E}. A dual join-graph is an edgelabeled edge subgraph of DG that satisfies the connectedness property. A minimal dual join-graph
is a dual join-graph for which none of the edge labels can be further reduced while maintaining the
connectedness property.
Interestingly, there may be many minimal dual join-graphs of the same dual graph. We will
define Iterative Belief Propagation on any dual join-graph. Each node sends a message over an edge
whose scope is identical to the label on that edge. Since Pearls algorithm sends messages whose
scopes are singleton variables only, we highlight minimal singleton-label dual join-graphs.
Proposition 3 Any Bayesian network has a minimal dual join-graph where each edge is labeled by
a single variable.
Proof: Consider a topological ordering of the nodes in the acyclic directed graph of the Bayesian
network d = X1 , . . . , Xn . We define the following dual join-graph. Every node in the dual graph
D, associated with pi is connected to node pj , j < i if Xj  pa(Xi ). We label the edge between pj
and pi by variable Xj , namely lij = {Xj }. It is easy to see that the resulting edge-labeled subgraph
of the dual graph satisfies connectedness. (Take the original acyclic graph G and add to each node
its CPT family, namely all the other parents that precede it in the ordering. Since G already satisfies
connectedness so is the minimal graph generated.) The resulting labeled graph is a dual graph with
singleton labels. 2

Example 5 Consider the belief network on 3 variables A, B, C with CPTs 1.P (C|A, B),
2.P (B|A) and 3.P (A), given in Figure 14a. Figure 14b shows a dual graph with singleton labels on the edges. Figure 14c shows a dual graph which is a join-tree, on which belief propagation
can solve the problem exactly in one iteration (two passes up and down the tree).

298

fiJ OIN -G RAPH P ROPAGATION A LGORITHMS

Algorithm Iterative Belief Propagation (IBP)
Input: An edge-labeled dual join-graph DG = (V, E, L) for a Bayesian network B = hX, D, G, P i. Evidence e.
Output: An augmented graph whose nodes include the original CPTs and the messages received from neighbors. Approximations of P (Xi |e), Xi  X. Approximations of P (Fi |e), Fi  B.
Denote by: hvu the message from u to v; ne(u) the neighbors of u in V ; nev (u) = ne(u)  {v}; luv the
label of (u, v)  E; elim(u, v) = scope(u)  scope(v).
 One iteration of IBP
For every node u in DJ in a topological order and back, do:
1. Process observed variables
Assign evidence variables to the each pi and remove them from the labeled edges.
2. Compute and send to v the function:
X
Y
hvu =
(pu 
hui )
elim(u,v)

{hu
i ,inev (u)}

Endfor
 Compute approximations of P (Fi |e), P (Xi |e):
For every Xi QX let u be the vertex of family Fi in DJ,
P (Fi , e) = ( hu ,une(i) hui )  pu ;
P i
P (Xi , e) =  scope(u){Xi } P (Fi , e).

Figure 15: Algorithm Iterative Belief Propagation (IBP).
For completeness, we present algorithm IBP, which is a special case of IJGP, in Figure 15. It
is easy to see that one iteration of IBP is time and space linear in the size of the belief network. It
can be shown that when IBP is applied to a minimal singleton-labeled dual graph it coincides with
Pearls belief propagation applied directly to the acyclic graph representation. Also, when the dual
join-graph is a tree IBP converges after one iteration (two passes, up and down the tree) to the exact
beliefs.
4.3 Bounded Join-Graph Decompositions
Since we want to control the complexity of join-graph algorithms, we will define it on decompositions having bounded cluster size. If the number of variables in a cluster is bounded by i, the
time and space complexity of processing one cluster is exponential in i. Given a join-graph decomposition D = hJG, , , i, the accuracy and complexity of the (iterative) join-graph propagation
algorithm depends on two different width parameters, defined next.
D EFINITION 12 (external and internal widths) Given an edge-labeled join-graph decomposition
D = hJG, , , i of a network B = hX, D, G, P i, the internal width of D is maxvV |(v)|, while
the external width of D is the treewidth of JG as a graph.
Using this terminology we can now state our target decomposition more clearly. Given a graph
G, and a bounding parameter i we wish to find a join-graph decomposition D of G whose internal
width is bounded by i and whose external width is minimized. The bound i controls the complexity
of join-graph processing while the external width provides some measure of its accuracy and speed
of convergence, because it measures how close the join-graph is to a join-tree.
299

fiM ATEESCU , K ASK , G OGATE & D ECHTER

Algorithm Join-Graph Structuring(i)
1. Apply procedure schematic mini-bucket(i).
2. Associate each resulting mini-bucket with a node in the join-graph, the variables of the
nodes are those appearing in the mini-bucket, the original functions are those in the minibucket.
3. Keep the edges created by the procedure (called out-edges) and label them by the regular
separator.
4. Connect the mini-bucket clusters belonging to the same bucket in a chain by in-edges
labeled by the single variable of the bucket.

Figure 16: Algorithm Join-Graph Structuring(i).
Procedure Schematic Mini-Bucket(i)
1. Order the variables from X1 to Xn minimizing (heuristically) induced-width, and associate a bucket for each variable.
2. Place each CPT in the bucket of the highest index variable in its scope.
3. For j = n to 1 do:
Partition the functions in bucket(Xj ) into mini-buckets having at most i variables.
For each mini-bucket mb create a new scope-function (message) f where scope(f ) =
{X|X  mb}  {Xi } and place scope(f) in the bucket of its highest variable. Maintain
an edge between mb and the mini-bucket (created later) of f .

Figure 17: Procedure Schematic Mini-Bucket(i).
We can consider two classes of algorithms. One class is partition-based. It starts from a given
tree-decomposition and then partitions the clusters until the decomposition has clusters bounded by
i. An alternative approach is grouping-based. It starts from a minimal dual-graph-based join-graph
decomposition (where each cluster contains a single CPT) and groups clusters into larger clusters
as long as the resulting clusters do not exceed the given bound. In both methods one should attempt
to reduce the external width of the generated graph-decomposition. Our partition-based approach
inspired by the mini-bucket idea (Dechter & Rish, 1997) is as follows.
Given a bound i, algorithm Join-Graph Structuring(i) applies the procedure Schematic MiniBucket(i), described in Figure 17. The procedure only traces the scopes of the functions that would
be generated by the full mini-bucket procedure, avoiding actual computation. The procedure ends
with a collection of mini-bucket trees, each rooted in the mini-bucket of the first variable. Each of
these trees is minimally edge-labeled. Then, in-edges labeled with only one variable are introduced,
and they are added only to obtain the running intersection property between branches of these trees.
Proposition 4 Algorithm Join-Graph Structuring(i) generates a minimal edge-labeled join-graph
decomposition having bound i.
Proof: The construction of the join-graph specifies the vertices and edges of the join-graph, as well
as the variable and function labels of each vertex. We need to demonstrate that 1) the connectedness
property holds, and 2) that edge-labels are minimal.

300

fiJ OIN -G RAPH P ROPAGATION A LGORITHMS

G: (GFE)

GFE

P(G|F,E)
EF

E: (EBF)

(EF)

EBF

P(E|B,F)

P(F|C,D)

F: (FCD)

(BF)

BF
F

FCD

BF

CD

D: (DB)

(CD)

P(D|B)

CDB
CB

C: (CAB) (CB)

P(C|A,B)

B
CAB
BA

B: (BA)

(AB)

A: (A)

(A)

(B)

P(B|A)

BA
A
P(A)

A

(b)

(a)

Figure 18: Join-graph decompositions.
Connectedness property specifies that for any 2 vertices u and v, if vertices u and v contain
variable X, then there must be a path u, w1 , . . . , wm , v between u and v such that every vertex on
this path contains variable X. There are two cases here. 1) u and v correspond to 2 mini-buckets
in the same bucket, or 2) u and v correspond to mini-buckets in different buckets. In case 1 we
have 2 further cases, 1a) variable X is being eliminated in this bucket, or 1b) variable X is not
eliminated in this bucket. In case 1a, each mini-bucket must contain X and all mini-buckets of the
bucket are connected as a chain, so the connectedness property holds. In case 1b, vertexes u and v
connect to their (respectively) parents, who in turn connect to their parents, etc. until a bucket in
the scheme where variable X is eliminated. All nodes along this chain connect variable X, so the
connectedness property holds. Case 2 resolves like case 1b.
To show that edge labels are minimal, we need to prove that there are no cycles with respect to
edge labels. If there is a cycle with respect to variable X, then it must involve at least one in-edge
(edge connecting two mini-buckets in the same bucket). This means variable X must be the variable
being eliminated in the bucket of this in-edge. That means variable X is not contained in any of the
parents of the mini-buckets of this bucket. Therefore, in order for the cycle to exist, another in-edge
down the bucket-tree from this bucket must contain X. However, this is impossible as this would
imply that variable X is eliminated twice. 2

Example 6 Figure 18a shows the trace of procedure schematic mini-bucket(3) applied to the problem described in Figure 2a. The decomposition in Figure 18b is created by the algorithm graph
structuring. The only cluster partitioned is that of F into two scopes (FCD) and (BF), connected by
an in-edge labeled with F.
A range of edge-labeled join-graphs is shown in Figure 19. On the left side we have a graph
with smaller clusters, but more cycles. This is the type of graph IBP works on. On the right side
we have a tree decomposition, which has no cycles at the expense of bigger clusters. In between,
there could be a number of join-graphs where maximum cluster size can be traded for number of
cycles. Intuitively, the graphs on the left present less complexity for join-graph algorithms because
the cluster size is smaller, but they are also likely to be less accurate. The graphs on the right side
301

fiM ATEESCU , K ASK , G OGATE & D ECHTER

A

A

A

C

ABC

AB
ABDE

BC

BE

C

A

A

ABC

AB

C

BCE

C
BC

BC

ABDE

ABCDE

DE

CE

CDE

C
DE

CE

CE

CDEF

CDEF

CDEF

CDEF
F

FGH

H

FGH

H

FGI

GH

GI

F

H

GHIJ

H

FGH
H
F

F

F
FG

ABCDE

BCE

C
DE

BCE

FGI

GI

F
F

GH

GHIJ

FGI

GH

GI

GHI
GHIJ

FGHI

GHIJ

more accuracy
less complexity

Figure 19: Join-graphs.
are computationally more complex, because of the larger cluster size, but they are likely to be more
accurate.
4.4 The Inference Power of IJGP
The question we address in this subsection is why propagating the messages iteratively should help.
Why is IJGP upon convergence superior to IJGP with one iteration and superior to MC? One clue
can be provided when considering deterministic constraint networks which can be viewed as extreme probabilistic networks. It is known that constraint propagation algorithms, which are analogous to the messages sent by belief propagation, are guaranteed to converge and are guaranteed
to improve with iteration. The propagation scheme of IJGP works similar to constraint propagation
relative to the flat network abstraction of the probability distribution (where all non-zero entries
are normalized to a positive constant), and propagation is guaranteed to be more accurate for that
abstraction at least.
In the following we will shed some light on the IJGPs behavior by making connections with
the well-known concept of arc-consistency from constraint networks (Dechter, 2003). We show
that: (a) if a variable-value pair is assessed as having a zero-belief, it remains as zero-belief in
subsequent iterations; (b) that any variable-value zero-beliefs computed by IJGP are correct; (c) in
terms of zero/non-zero beliefs, IJGP converges in finite time. We have also empirically investigated
the hypothesis that if a variable-value pair is assessed by IBP or IJGP as having a positive but very
close to zero belief, then it is very likely to be correct. Although the experimental results shown in
this paper do not contradict this hypothesis, some examples in more recent experiments by Dechter,
Bidyuk, Mateescu, and Rollon (2010) invalidate it.

302

fiJ OIN -G RAPH P ROPAGATION A LGORITHMS

4.4.1 IJGP

AND

A RC -C ONSISTENCY

For any belief network we can define a constraint network that captures the assignments having
strictly positive probability. We will show a correspondence between IJGP applied to the belief
network and an arc-consistency algorithm applied to the constraint network. Since arc-consistency
algorithms are well understood, this correspondence not only proves the target claims, but may
provide additional insight into the behavior of IJGP. It justifies the iterative application of belief
propagation, and it also illuminates its distance from being complete.
D EFINITION 13 (constraint satisfaction problem) A Constraint Satisfaction Problem (CSP) is a
triple hX, D, Ci, where X = {X1 , . . . , Xn } is a set of variables associated with a set of discretevalued domains D = {D1 , . . . , Dn } and a set of constraints C = {C1 , . . . , Cm }. Each constraint
Ci is a pair hSi , Ri i where Ri is a relation Ri  DSi defined on a subset of variables Si  X and
DSi is a Cartesian product of the domains of variables Si . The relation Ri denotes all compatible
tuples of DSi allowed by the constraint. A projection operator  creates a new relation, Sj (Ri ) =
{x|x  DSj and y, y  DSi \Sj and xy  Ri }, where Sj  Si . Constraints can be combined with
the join operator 1, resulting in a new relation, Ri 1 Rj = {x|Si (x)  Ri and Sj (x)  Rj }.
A solution is an assignment of values to all the variables x = (x1 , . . . , xn ), xi  Di , such that
Ci  C, xSi  Ri . The constraint network represents its set of solutions, 1i Ci .
Given a belief network B, we define a flattening of the Bayesian network into a constraint
network called f lat(B), where all the zero entries in a probability table are removed from the corresponding relation. The network f lat(B) is defined over the same set of variables and has the same
set of domain values as B.
D EFINITION 14 (flat network) Given a Bayesian network B = hX, D, G, P i, the flat network
f lat(B) is a constraint network, where the set of variables is X, and for every Xi  X and its
CPT P (Xi |pa(Xi ))  B we define a constraint RFi over the family of Xi , Fi = {Xi }  pa(Xi ) as
follows: for every assignment x = (xi , xpa(Xi ) ) to Fi , (xi , xpa(Xi ) )  RFi iff P (xi |xpa(Xi ) ) > 0.
T HEOREM 6 Given a belief network B = hX, D, G, P i, where X = {X1 , . . . , Xn }, for any tuple
x = (x1 , . . . , xn ): PB (x) > 0  x  sol(f lat(B)), where sol(f lat(B)) is the set of solutions of
the flat constraint network.
Proof: PB (x) > 0  ni=1 P (xi |xpa(Xi ) ) > 0  i  {1, . . . , n}, P (xi |xpa(Xi ) ) > 0  i 
{1, . . . , n}, (xi , xpa(Xi ) )  RFi  x  sol(f lat(B)). 2
Constraint propagation is a class of polynomial time algorithms that are at the center of constraint processing techniques. They were investigated extensively in the past three decades and the
most well known versions are arc-, path-, and i-consistency (Dechter, 1992, 2003).
D EFINITION 15 (arc-consistency) (Mackworth, 1977) Given a binary constraint network
(X, D, C), the network is arc-consistent iff for every binary constraint Rij  C, every value v  Di
has a value u  Dj s.t. (v, u)  Rij .

303

fiM ATEESCU , K ASK , G OGATE & D ECHTER

Note that arc-consistency is defined for binary networks, namely the relations involve at most
two variables. When a binary constraint network is not arc-consistent, there are algorithms that
can process it and enforce arc-consistency. The algorithms remove values from the domains of
the variables that violate arc-consistency until an arc-consistent network is generated. There are
several versions of improved performance arc-consistency algorithms, however we will consider a
non-optimal distributed version, which we call distributed arc-consistency.
D EFINITION 16 (distributed arc-consistency algorithm) The
algorithm
distributed
arcconsistency is a message-passing algorithm over a constraint network. Each node is a variable,
and maintains a current set of viable values Di . Let ne(i) be the set of neighbors of Xi in the
constraint graph. Every node Xi sends a message to any node Xj  ne(i), which consists of the
values in Xj s domain that are consistent with the current Di , relative to the constraint Rji that
they share. Namely, the message that Xi sends to Xj , denoted by Dij , is:
Dij  j (Rji 1 Di )

(1)

Di  Di  (1kne(i) Dki )

(2)

and in addition node i computes:

Clearly the algorithm can be synchronized into iterations, where in each iteration every node
computes its current domain based on all the messages received so far from its neighbors (Eq. 2), and
sends a new message to each neighbor (Eq. 1). Alternatively, Equations 1 and 2 can be combined.
The message Xi sends to Xj is:
Dij  j (Rji 1 Di 1kne(i) Dki )

(3)

Next we will define a join-graph decomposition for the flat constraint network so that we can
establish a correspondence between the join-graph decomposition of a Bayesian network B and the
join-graph decomposition of its flat network f lat(B). Note that for constraint networks, the edge
labeling  can be ignored.
D EFINITION 17 (join-graph decomposition of the flat network) Given a join-graph decomposition D = hJG, , , i of a Bayesian network B, the join-graph decomposition Df lat =
hJG, , f lat i of the flat constraint network f lat(B) has the same underlying graph structure
JG = (V, E) as D, the same variable-labeling of the clusters , and the mapping f lat maps
each cluster to relations corresponding to CPTs, namely Ri  f lat (v) iff CPT pi  (v).
The distributed arc-consistency algorithm of Definition 16 can be applied to the join-graph decomposition of the flat network. In this case, the nodes that exchange messages are the clusters
(namely the elements of the set V of JG). The domain of a cluster of V is the set of tuples of the
join of the original relations in the cluster (namely the domain of cluster u is ./Rf lat (u) R). The
constraints are binary, and involve clusters of V that are neighbors. For two clusters u and v, their
corresponding values tu and tv (which are tuples representing full assignments to the variables in
the cluster) belong to the relation Ruv (i.e., (tu , tv )  Ru,v ) if the projections over the separator (or
labeling ) between u and v are identical, namely ((u,v)) tu = ((u,v)) tv .
304

fiJ OIN -G RAPH P ROPAGATION A LGORITHMS

We define below the algorithm relational distributed arc-consistency (RDAC), that applies distributed arc-consistency to any join-graph decomposition of a constraint network. We call it relational to emphasize that the nodes exchanging messages are in fact relations over the original
problem variables, rather than simple variables as is the case for arc-consistency algorithms.
D EFINITION 18 (relational distributed arc-consistency algorithm: RDAC over a join-graph)
Given a join-graph decomposition of a constraint network, let Ri and Rj be the relations of two
clusters (Ri and Rj are the joins of the respective constraints in each cluster), having the scopes Si
and Sj , such that Si  Sj 6= . The message Ri sends to Rj denoted h(i,j) is defined by:
h(i,j)  Si Sj (Ri )

(4)

where ne(i) = {j|Si  Sj 6= } is the set of relations (clusters) that share a variable with Ri . Each
cluster updates its current relation according to:
Ri  Ri 1 (1kne(i) h(k,i) )

(5)

Algorithm RDAC iterates until there is no change.
Equations 4 and 5 can be combined, just like in Equation 3. The message that Ri sends to Rj
becomes:
h(i,j)  Si Sj (Ri 1 (1kne(i) h(k,i) ))

(6)

To establish the correspondence with IJGP, we define the algorithm IJGP-RDAC that applies
RDAC in the same order of computation (schedule of processing) as IJGP.
D EFINITION 19 (IJGP-RDAC algorithm) Given the Bayesian network B = hX, D, G, P i, let
Df lat = hJG, , f lat , i be any join-graph decomposition of the flat network f lat(B). The algorithm IJGP-RDAC is applied to the decomposition Df lat of f lat(B), and can be described as
IJGP applied to D, with the following modifications:
Q
1. Instead of , we use 1.
P
2. Instead of , we use .
3. At end end, we update the domains of variables by:
Di  Di  Xi ((1vne(u) h(v,u) ) 1 (1R(u) R))

(7)

where u is the cluster containing Xi .
Note that in algorithm IJGP-RDAC, we could first merge all constraints in each cluster u into
a single constraint Ru =1R(u) R. From our construction, IJGP-RDAC enforces arc-consistency
over the join-graph decomposition of the flat network. When the join-graph Df lat is a join-tree,
IJGP-RDAC solves the problem namely it finds all the solutions of the constraint network.

305

fiM ATEESCU , K ASK , G OGATE & D ECHTER

Proposition 5 Given the join-graph decomposition Df lat = hJG, , f lat , i, JG = (V, E), of
the flat constraint network f lat(B), corresponding to a given join-graph decomposition D of a
Bayesian network B = hX, D, G, P i, the algorithm IJGP-RDAC applied to Df lat enforces arcconsistency over the join-graph Df lat .
Proof: IJGP-RDAC applied to the join-graph decomposition Df lat = hJG, , f lat , i, JG =
(V, E), is equivalent to applying RDAC of Definition 18 to a constraint network that has vertices V
as its variables and {1R(u) R|u  V } as its relations. 2
Following the properties of convergence of arc-consistency, we can show that:
Proposition 6 Algorithm IJGP-RDAC converges in O(m  r) iterations, where m is the number of
edges in the join-graph and r is the maximum size of a separator Dsep(u,v) between two clusters.
Proof: This follows from the fact messages (which are relations) between clusters in IJGP-RDAC
change monotonically, as tuples are only successively removed from relations on separators. Since
the size of each relation on a separator is bounded by r and there are m edges, no more than O(mr)
iterations will be needed. 2
In the following we will establish an equivalence between IJGP and IJGP-RDAC in terms of
zero probabilities.
Proposition 7 When IJGP and IJGP-RDAC are applied in the same order of computation, the
messages computed by IJGP are identical to those computed by IJGP-RDAC in terms of zero / nonzero probabilities. That is, h(u,v) (x) 6= 0 in IJGP iff x  h(u,v) in IJGP-RDAC.
Proof: The proof is by induction. The base case is trivially true since messages h in IJGP are initialized to a uniform distribution and messages h in IJGP-RDAC are initialized to complete relations.
The induction step. Suppose that hIJGP
(u,v) is the message sent from u to v by IJGP. We will show
IJGP RDAC
IJGP RDAC
that if hIJGP
where h(u,v)
is the message sent by IJGP(u,v) (x) 6= 0, then x  h(u,v)
RDAC from u to v. Assume that the claim holds for all messages received by u from its neighbors.
Let f  clusterv (u) in IJGP and Rf be the corresponding relation in IJGP-RDAC,
and
P
Q t be an asIJGP
signment of values to variables in elim(u, v). We have h(u,v) (x) 6= 0  elim(u,v) f f (x) 6= 0
Q
 t, f f (x, t) 6= 0  t, f, f (x, t) 6= 0  t, f, scope(Rf ) (x, t)  Rf  t, elim(u,v) (1Rf
IJGP RDAC
IJGP RDAC
. 2
 x  h(u,v)
scope(Rf ) (x, t))  h(u,v)

Next we will show that IJGP computing marginal probability P (Xi = xi ) = 0 is equivalent to
IJGP-RDAC removing xi from the domain of variable Xi .
Proposition 8 IJGP computes P (Xi = xi ) = 0 iff IJGP-RDAC decides that xi 6 Di .
Proof: According to Proposition 7 messages computed by IJGP and IJGP-RDAC are identical in
terms of zero probabilities. Let f  cluster(u) in IJGP and Rf be the corresponding relation in
IJGP-RDAC, and t be an assignment of values to variables in (u)\Xi . We will show that when
IJGP computes P (Xi = xi ) = 0 (upon convergence), then IJGP-RDAC computes xi 6 Di . We
306

fiJ OIN -G RAPH P ROPAGATION A LGORITHMS

Q
Q
P
have P (Xi = xi ) =
f f (xi , t) = 0  t, f, f (xi , t) = 0 
f f (xi ) = 0  t,
X\Xi
t, Rf , scope(Rf ) (xi , t) 6 Rf  t, (xi , t) 6 (1Rf Rf (xi , t))  xi 6 Di  Xi (1Rf Rf (xi , t))
 xi 6 Di . Since arc-consistency is sound, so is the decision of zero probabilities. 2
Next we will show that P (Xi = xi ) = 0 computed by IJGP is sound.
T HEOREM 7 Whenever IJGP finds P (Xi = xi ) = 0, then the probability P (Xi ) expressed by the
Bayesian network conditioned on the evidence is 0 as well.
Proof: According to Proposition 8, whenever IJGP finds P (Xi = xi ) = 0, the value xi is removed
from the domain Di by IJGP-RDAC, therefore value xi  Di is a no-good of the network f lat(B),
and from Theorem 6 it follows that PB (Xi = xi ) = 0. 2
In the following we will show that the time it takes IJGP to find all P (Xi = xi ) = 0 is bounded.
Proposition 9 IJGP finds all P (Xi = xi ) = 0 in finite time, that is, there exists a number k, such
that no P (Xi = xi ) = 0 will be found after k iterations.
Proof: This follows from the fact that the number of iterations it takes for IJGP to compute P (Xi =
xi ) = 0 is exactly the same number of iterations IJGP-RDAC takes to remove xi from the domain
Di (Proposition 7 and Proposition 8), and the fact the IJGP-RDAC runtime is bounded (Proposition
6). 2
Previous results also imply that IJGP is monotonic with respect to zeros.
Proposition 10 Whenever IJGP finds P (Xi = xi ) = 0, it stays 0 during all subsequent iterations.
Proof: Since we know that relations in IJGP-RDAC are monotonically decreasing as the algorithm
progresses, it follows from the equivalence of IJGP-RDAC and IJGP (Proposition 7) that IJGP is
monotonic with respect to zeros. 2

4.4.2 A F INITE P RECISION P ROBLEM
On finite precision machines there is the danger that an underflow can be interpreted as a zero
value. We provide here a warning that an implementation of belief propagation should not allow
the creation of zero values by underflow. We show an example in Figure 20 where IBPs messages
converge in the limit (i.e., in an infinite number of iterations), but they do not stabilize in any finite
number of iterations. If all the nodes Hk are set to value 1, the belief for any of the Xi variables as a
function of iteration is given in the table in Figure 20. After about 300 iterations, the finite precision
of our computer is not able to represent the value for Bel(Xi = 3), and this appears to be zero,
yielding the final updated belief (.5, .5, 0), when in fact the true updated belief should be (0, 0, 1).
Notice that (.5, .5, 0) cannot be regarded as a legitimate fixed point for IBP. Namely, if we would
initialize IBP with the values (.5, .5, 0), then the algorithm would maintain them, appearing to have
a fixed point, but initializing IBP with zero values cannot be expected to be correct. When we

307

fiM ATEESCU , K ASK , G OGATE & D ECHTER

X1

Prior for Xi

H3

H1

X3

X2

Xi

P ( Xi )

1

.45

2

.45

3

.1

H2
Hk Xi

CPT for Hk

Xj

P ( Hk | Xi , Xj )

1

1

2

1

1

2

1

1

1

3

3

1

1

 

0

#iter

Bel(Xi = 1) Bel(Xi = 2) Bel(Xi = 3)

1

.45

.45

.1

2

.49721

.49721

.00545

3

.49986

.49986

.00027

100





1e-129

200





1e-260

300

.5

.5

0

True
belief

0

0

1

Figure 20: Example of a finite precision problem.
initialize with zeros we forcibly introduce determinism in the model, and IBP will always maintain
it afterwards.
However, this example does not contradict our theory because, mathematically, Bel(Xi = 3)
never becomes a true zero, and IBP never reaches a quiescent state. The example shows that a close
to zero belief network can be arbitrarily inaccurate. In this case the inaccuracy seems to be due to
the initial prior belief which are so different from the posterior ones.
4.4.3 ACCURACY OF IBP ACROSS B ELIEF D ISTRIBUTION
We present an empirical evaluation of the accuracy of IBPs prediction for the range of belief distribution from 0 to 1. These results also extend to IJGP. In the previous section, we proved that zero
values inferred by IBP are correct, and we wanted to test the hypothesis that this property extends
to  small beliefs (namely, that are very close to zero). That is, if IBP infers a posterior belief close
to zero, then it is likely to be correct. The results presented in this paper seem to support the hypothesis, however new experiments by Dechter et al. (2010) show that it is not true in general. We
do not have yet a good characterization of the cases when the hypothesis is confirmed.
To test this hypothesis, we computed the absolute error of IBP per intervals of [0, 1]. For a given
interval [a, b], where 0  a < b  1, we use measures inspired from information retrieval: Recall
Absolute Error and Precision Absolute Error.
Recall is the absolute error averaged over all the exact posterior beliefs that fall into the interval
[a, b]. For Precision, the average is taken over all the approximate posterior belief values computed
by IBP to be in the interval [a, b]. Intuitively, Recall([a,b]) indicates how far the belief computed
by IBP is from the exact, when the exact is in [a, b]; Precision([a,b]) indicates how far the exact is
from IBPs prediction, when the value computed by IBP is in [a, b].
Our experiments show that the two measures are strongly correlated. We also show the histograms of distribution of belief for each interval, for the exact and for IBP, which are also strongly
correlated. The results are given in Figures 21 and 22. The left Y axis corresponds to the histograms
(the bars), the right Y axis corresponds to the absolute error (the lines).
We present results for two classes of problems: coding networks and grid network. All problems
have binary variables, so the graphs are symmetric about 0.5 and we only show the interval [0, 0.5].
The number of variables, number of iterations and induced width w* are reported for each graph.

308

fiJ OIN -G RAPH P ROPAGATION A LGORITHMS

Recall Abs. Error

noise = 0.20

noise = 0.40

0.4

Absolute Error

0.45

0.3

0
0.35

0.35

0.01

0.2

0%

0.02

0.25

0%

0.03

0.1

0%

0.04

0.15

5%

Precision Abs. Error
0.05

0

10%

5%
0.4

10%

5%

0.45

10%

0.3

15%

0.2

20%

15%

0.25

20%

15%

0.1

20%

0.15

25%

0

30%

25%

0.05

30%

25%

0.4

30%

0.45

35%

0.3

40%

35%

0.35

40%

35%

0.2

40%

0.25

45%

0.1

45%

0.15

45%

0

50%

0.05

IBP Histogram
50%

0.05

Percentage

Exact Histogram
50%

noise = 0.60

Figure 21: Coding, N=200, 1000 instances, w*=15.
Recall Abs. Error

evidence = 0

evidence = 10

0.004
0.003
0.002
0.001

Absolute Error

0.005

0.4

0.45

0.3

0.35

0.2

0.25

0.1

0.15

0
0

0.4

0.45

0.3

0.35

0%
0.2

0%
0.25

5%
0.1

10%

5%
0.15

10%

0

15%

0.05

20%

15%

0.4

20%

0.45

25%

0.3

30%

25%

0.35

30%

0.2

35%

0.25

40%

35%

0.1

40%

0.15

45%

0

45%

Precision Abs. Error

0.05

IBP Histogram

50%

0.05

Percentage

Exact Histogram
50%

evidence = 20

Figure 22: 10x10 grids, 100 instances, w*=15.
Coding networks IBP is famously known to have impressive performance on coding networks.
We tested on linear block codes, with 50 nodes per layer and 3 parent nodes. Figure 21 shows the
results for three different values of channel noise: 0.2, 0.4 and 0.6. For noise 0.2, all the beliefs
computed by IBP are extreme. The Recall and Precision are very small, of the order of 1011 . So,
in this case, all the beliefs are very small ( small) and IBP is able to infer them correctly, resulting
in almost perfect accuracy (IBP is indeed perfect in this case for the bit error rate). When the noise
is increased, the Recall and Precision tend to get closer to a bell shape, indicating higher error for
values close to 0.5 and smaller error for extreme values. The histograms also show that less belief
values are extreme as the noise is increased, so all these factors account for an overall decrease
in accuracy as the channel noise increases. These networks are examples with a large number of
-small probabilities and IBP is able to infer them correctly (absolute error is small).
Grid networks We present results for grid networks in Figure 22. Contrary to the case of coding
networks, the histograms show higher concentration of beliefs around 0.5. However, the accuracy is
still very good for beliefs close to zero. The absolute error peaks close to 0 and maintains a plateau,
as evidence is increased, indicating less accuracy for IBP.

5. Experimental Evaluation
As we anticipated in the summary of Section 3, and as can be clearly seen now by the structuring
of a bounded join-graph, there is a close relationship between the mini-clustering algorithm MC(i)
309

fiM ATEESCU , K ASK , G OGATE & D ECHTER

IBP
#it
1

5

10

MC

#evid
0
5
10
0
5
10
0
5
10
0
5
10

0.02988
0.06178
0.08762
0.00829
0.05182
0.08039
0.00828
0.05182
0.08040

Absolute error
IJGP
i=2
i=5
0.03055 0.02623
0.04434 0.04201
0.05777 0.05409
0.00636 0.00592
0.00886 0.00886
0.01155 0.01073
0.00584 0.00514
0.00774 0.00732
0.00892 0.00808

IBP
i=8
0.02940
0.04554
0.05910
0.00669
0.01123
0.01399
0.00495
0.00708
0.00855

0.04044 0.04287 0.03748
0.05303 0.05171 0.04250
0.06033 0.05489 0.04266

0.06388
0.15005
0.23777
0.01726
0.12589
0.21781
0.01725
0.12590
0.21782

Relative error
IJGP
i=5
0.05677
0.12056
0.14278
0.01239
0.01965
0.02553
0.01069
0.01628
0.01907

i=2
0.15694
0.12340
0.18071
0.01326
0.01967
0.03014
0.01216
0.01727
0.02101

IBP
i=8
0.07153
0.11154
0.15686
0.01398
0.02494
0.03279
0.01030
0.01575
0.02005

0.08811 0.09342 0.08117
0.12375 0.11775 0.09596
0.14702 0.13219 0.10074

0.00213
0.00812
0.01547
0.00021
0.00658
0.01382
0.00021
0.00658
0.01382

KL distance
IJGP
i=5
0.00208
0.00478
0.00768
0.00015
0.00026
0.00042
0.00010
0.00017
0.00024

i=2
0.00391
0.00582
0.00915
0.00014
0.00024
0.00055
0.00012
0.00018
0.00028

IBP
i=8
0.00277
0.00558
0.00899
0.00018
0.00044
0.00073
0.00010
0.00016
0.00029

0.00403 0.00435 0.00369
0.00659 0.00636 0.00477
0.00841 0.00729 0.00503

0.0017
0.0013
0.0013
0.0066
0.0060
0.0048
0.0130
0.0121
0.0109

Time
IJGP
i=5
0.0058
0.0052
0.0036
0.0226
0.0185
0.0138
0.0436
0.0355
0.0271

i=2
0.0036
0.0040
0.0040
0.0145
0.0120
0.0100
0.0254
0.0223
0.0191

i=8
0.0295
0.0200
0.0121
0.1219
0.0840
0.0536
0.2383
0.1639
0.1062

0.0159 0.0173 0.0552
0.0146 0.0158 0.0532
0.0119 0.0143 0.0470

Table 4: Random networks: N=50, K=2, C=45, P=3, 100 instances, w*=16.

and IJGP(i). In particular, one iteration of IJGP(i) is similar to MC(i). MC sends messages up and
down along the clusters that form a set of trees. IJGP has additional connections that allow more
interaction between the mini-clusters of the same cluster. Since this is a cyclic structure, iterating is
facilitated, with its virtues and drawbacks.s
In our evaluation of IJGP(i), we focus on two different aspects: (a) the sensitivity of parametric
IJGP(i) to its i-bound and to the number of iterations; (b) a comparison of IJGP(i) with publicly
available state-of-the-art approximation schemes.
5.1 Effect of i-bound and Number of Iterations
We tested the performance of IJGP(i) on random networks, on M-by-M grids, on the two benchmark CPCS files with 54 and 360 variables, respectively and on coding networks. On each type
of networks, we ran IBP, MC(i) and IJGP(i), while giving IBP and IJGP(i) the same number of
iterations.
We use the partitioning method described in Section 4.3 to construct a join-graph. To determine
the order of message computation, we recursively pick an edge (u,v), such that node u has the fewest
incoming messages missing.
For each network except coding, we compute the exact solution and compare the accuracy
using the absolute and relative error, as before, as well as the KL (Kullback-Leibler) distance Pexact (X = a)  log(Pexact (X = a)/Papproximation (X = a)) averaged over all values, all variables
and all problems. For coding networks we report the Bit Error Rate (BER) computed as described
in Section 3.2. We also report the time taken by each algorithm.
The random networks were generated using parameters (N,K,C,P), where N is the number of
variables, K is their domain size, C is the number of conditional probability tables (CPTs) and P
is the number of parents in each CPT. Parents in each CPT are picked randomly and each CPT
is filled randomly. In grid networks, N is a square number and each CPT is filled randomly. In
each problem class, we also tested different numbers of evidence variables. As before, the coding
networks are from the class of linear block codes, where  is the channel noise level. Note that we
are limited to relatively small and sparse problem instances because our evaluation measures are
based on comparing against exact figures.
Random networks results for networks having N=50, K=2, C=45 and P=3 are given in Table 4
and in Figures 23 and 24. For IJGP(i) and MC(i) we report 3 different values of i-bound: 2, 5, 8. For
IBP and IJGP(i) we report results for 3 different numbers of iterations: 1, 5, 10. We report results
310

fiJ OIN -G RAPH P ROPAGATION A LGORITHMS

Random networks, N=50, K=2, P=3, evid=5, w*=16
0.010

IJGP 1 it
IJGP 2 it
IJGP 3 it
IJGP 5 it
IJGP 10 it
IJGP 15 it
IJGP 20 it
MC
IBP 1 it
IBP 2 it
IBP 3 it
IBP 5 it
IBP 10 it

0.008

KL distance

0.006

0.004

0.002

0.000

0

1

2

3

4

5

6

7

8

9

10

11

i-bound

(a) Performance vs. i-bound.
Random networks, N=50, K=2, P=3, evid=5, w*=16
0.010

IBP
IJGP(2)
IJGP(10)

0.008

KL distance

0.006

0.004

0.002

0.000

0

5

10

15

20

25

30

35

Number of iterations

(b) Convergence with iterations.

Figure 23: Random networks: KL distance.
for 3 different numbers of evidence: 0, 5, 10. From Table 4 and Figure 23a we see that IJGP(i)
is always better than IBP (except when i=2 and number of iterations is 1), sometimes by an order
of magnitude, in terms of absolute error, relative error and KL distance. IBP rarely changes after 5
iterations, whereas IJGP(i)s solution can be improved with more iterations (up to 15-20). As theory
predicted, the accuracy of IJGP(i) for one iteration is about the same as that of MC(i). But IJGP(i)
improves as the number of iterations increases, and is eventually better than MC(i) by as much as
an order of magnitude, although it clearly takes more time, especially when the i-bound is large.

311

fiM ATEESCU , K ASK , G OGATE & D ECHTER

Random networks, N=50, K=2, P=3, evid=5, w*=16
1.0
IJPG 1 it
IJGP 2 it
IJGP 3 it
IJGP 5 it
IJGP 10 it
IJGP 15 it
IJGP 20 it
MC
IBP 1 it
IBP 20 it

Time (seconds)

0.8

0.6

0.4

0.2

0.0

0

1

2

3

4

5

6

7

8

9

10

11

i-bound

Figure 24: Random networks: Time.
IBP
#it
1

5

10

MC

#evid
0
5
10
0
5
10
0
5
10
0
5
10

0.03524
0.05375
0.07094
0.00358
0.03224
0.05503
0.00352
0.03222
0.05503

Absolute error
IJGP
i=2
i=5
0.05550 0.04292
0.05284 0.04012
0.05453 0.04304
0.00393 0.00325
0.00379 0.00319
0.00364 0.00316
0.00352 0.00232
0.00357 0.00248
0.00347 0.00239

IBP
i=8
0.03318
0.03661
0.03966
0.00284
0.00296
0.00314
0.00136
0.00149
0.00141

0.05827 0.04036 0.01579
0.05973 0.03692 0.01355
0.05866 0.03416 0.01075

0.08075
0.16380
0.23624
0.00775
0.11299
0.19403
0.00760
0.11295
0.19401

Relative error
IJGP
i=5
0.10252
0.09889
0.12492
0.00702
0.00710
0.00756
0.00502
0.00549
0.00556

i=2
0.13533
0.13225
0.14588
0.00849
0.00844
0.00841
0.00760
0.00796
0.00804

IBP
i=8
0.07904
0.09116
0.12202
0.00634
0.00669
0.01313
0.00293
0.00330
0.00328

0.13204 0.08833 0.03440
0.13831 0.08213 0.03001
0.14120 0.07791 0.02488

0.00289
0.00725
0.01232
0.00005
0.00483
0.00994
0.00005
0.00483
0.00994

KL distance
IJGP
i=5
0.00602
0.00570
0.00681
0.00007
0.00007
0.00009
0.00003
0.00003
0.00003

i=2
0.00859
0.00802
0.00905
0.00006
0.00006
0.00006
0.00005
0.00005
0.00005

IBP
i=8
0.00454
0.00549
0.00653
0.00010
0.00010
0.00019
0.00001
0.00002
0.00001

0.00650 0.00387 0.00105
0.00696 0.00348 0.00099
0.00694 0.00326 0.00075

0.0010
0.0016
0.0013
0.0049
0.0053
0.0036
0.0090
0.0096
0.0090

Time
IJGP
i=5
0.0106
0.0092
0.0072
0.0347
0.0309
0.0271
0.0671
0.0558
0.0495

i=2
0.0053
0.0041
0.0038
0.0152
0.0131
0.0127
0.0277
0.0246
0.0223

i=8
0.0426
0.0315
0.0256
0.1462
0.1127
0.0913
0.2776
0.2149
0.1716

0.0106 0.0142 0.0382
0.0102 0.0130 0.0342
0.0099 0.0116 0.0321

Table 5: 9x9 grid, K=2, 100 instances, w*=12.

Figure 23a shows a comparison of all algorithms with different numbers of iterations, using the
KL distance. Because the network structure changes with different i-bounds, we do not necessarily
see monotonic improvement of IJGP with i-bound for a given number of iterations (as is the case
with MC). Figure 23b shows how IJGP converges with more iterations to a smaller KL distance
than IBP. As expected, the time taken by IJGP (and MC) varies exponentially with the i-bound (see
Figure 24).
Grid networks results with networks of N=81, K=2, 100 instances are very similar to those of
random networks. They are reported in Table 5 and in Figure 25, where we can see the impact
of having evidence (0 and 5 evidence variables) on the algorithms. IJGP at convergence gives the
best performance in both cases, while IBPs performance deteriorates with more evidence and is
surpassed by MC with i-bound 5 or larger.
CPCS networks results with CPCS54 and CPCS360 are given in Table 6 and Figure 26, and are
even more pronounced than those of random and grid networks. When evidence is added, IJGP(i)
is more accurate than MC(i), which is more accurate than IBP, as can be seen in Figure 26a.
Coding networks results are given in Table 7. We tested on large networks of 400 variables, with
treewidth w*=43, with IJGP and IBP set to run 30 iterations (this is more than enough to ensure
312

fiJ OIN -G RAPH P ROPAGATION A LGORITHMS

Grid network, N=81, K=2, evid=5, w*=12
0.010
IJGP 1 it
IJGP 2 it
IJGP 3 it
IJGP 5 it
IJGP 10 it
MC
IBP 1 it
IBP 2 it
IBP 3 it
IBP 5 it
IBP 10 it

0.008

KL distance

0.006

0.004

0.002

0.000

0

1

2

3

4

5

6

7

8

9

10

11

i-bound

(a) Performance vs. i-bound.
Grid network, N=81, K=2, evid=5, w*=12
7e-5
IJGP 20 iterations
(at convergence)
6e-5

KL distance

5e-5

4e-5

3e-5

2e-5

1e-5

0
1

2

3

4

5

6

7

8

9

10

11

i-bound

(b) Fine granularity for KL.

Figure 25: Grid 9x9: KL distance.
convergence). IBP is known to be very accurate for this class of problems and it is indeed better
than MC. However we notice that IJGP converges to slightly smaller BER than IBP even for small
values of the i-bound. Both the coding network and CPCS360 show the scalability of IJGP for large
size problems. Notice that here the anytime behavior of IJGP is not clear.
In summary, we see that IJGP is almost always superior to both IBP and MC(i) and is sometimes
more accurate by several orders of magnitude. One should note that IBP cannot be improved with
more time, while MC(i) requires a large i-bound for many hard and large networks to achieve
reasonable accuracy. There is no question that the iterative application of IJGP is instrumental to its
success. In fact, IJGP(2) in isolation appears to be the most cost-effective variant.

313

fiM ATEESCU , K ASK , G OGATE & D ECHTER

IBP
#it

1

5

10

MC

1
10
20
MC

#evid

Absolute error
IJGP
i=2
i=5

Relative error
IJGP
i=5

IBP
i=8

i=2
0.02716
0.05736
0.08475
0.00064
0.04067
0.07302
0.00064
0.04067
0.07302

0.08966
0.09007
0.09156
0.00033
0.00124
0.00215
0.00018
0.00078
0.00123
0.05648
0.05687
0.06002

KL distance
IJGP
i=5

IBP
i=8

CPCS54
0.07761 0.05616
0.07676 0.05856
0.08246 0.06687
0.00255 0.00225
0.00194 0.00203
0.00298 0.00302
0.00029 0.00031
0.00071 0.00080
0.00109 0.00122
0.05128 0.03047
0.05314 0.03713
0.05318 0.03409

Time
IBP

i=2
0.00041
0.00199
0.00357
7.75e-7
0.00161
0.00321
7.75e-7
0.00161
0.00321

0.00583
0.00573
0.00567
0.00000
0.00000
0.00001
0.0000
0.00000
4.0e-6
0.00218
0.00201
0.00216

0.00512
0.00493
0.00506
0.00002
0.00001
0.00003
0.00000
0.00000
3.0e-6
0.00171
0.00186
0.00177

i=8
0.00378
0.00366
0.00390
0.00001
0.00001
0.00002
0.00000
0.00000
4.0e-6
0.00076
0.00098
0.00091

0.0097
0.0072
0.005
0.0371
0.0337
0.0290
0.0736
0.0633
0.0575

i=2

IJGP
i=5

i=8

0.0137
0.0094
0.0047
0.0334
0.0215
0.0144
0.0587
0.0389
0.0251
0.0144
0.0103
0.0094

0.0146
0.0087
0.0052
0.0384
0.0260
0.0178
0.0667
0.0471
0.0297
0.0125
0.0126
0.0090

0.0275
0.0169
0.0115
0.0912
0.0631
0.0378
0.1720
0.1178
0.0723
0.0333
0.0346
0.0295

0
5
10
0
5
10
0
5
10
0
5
10

0.01324
0.02684
0.03915
0.00031
0.01874
0.03348
0.00031
0.01874
0.03348

0.03747
0.03739
0.03843
0.00016
0.00058
0.00101
0.00009
0.00037
0.00058
0.02721
0.02702
0.02825

0.03183
0.03124
0.03426
0.00123
0.00092
0.00139
0.00014
0.00034
0.00051
0.02487
0.02522
0.02504

0.02233
0.02337
0.02747
0.00110
0.00098
0.00144
0.00015
0.00038
0.00057
0.01486
0.01760
0.01600

10
20
10
20
10
20
10
20

0.26421
0.26326
0.01772
0.02413
0.01772
0.02413

0.14222
0.12867
0.00694
0.00466
0.00003
0.00001
0.03389
0.02715

0.13907
0.12937
0.00121
0.00115
3.0e-6
9.0e-6
0.01984
0.01543

CPCS360
0.14334 7.78167 2119.20 2132.78 2133.84 0.17974 0.09297 0.09151 0.09255 0.7172 0.5486 0.5282 0.4593
0.13665 370.444 28720.38 30704.93 31689.59 0.17845 0.08212 0.08269 0.08568 0.6794 0.5547 0.5250 0.4578
0.00258 1.06933 6.07399 0.01005 0.04330 0.017718 0.00203 0.00019 0.00116 7.2205 4.7781 4.5191 3.7906
0.00138 62.99310 26.04308 0.00886 0.01353 0.02027 0.00118 0.00015 0.00036 7.0830 4.8705 4.6468 3.8392
3.0e-6
1.06933 0.00044
8.0e-6
7.0e-6
0.01771
5.0e-6
0.0
0.0
14.4379 9.5783 9.0770 7.6017
9.0e-6
62.9931 0.00014 0.00013 0.00004 0.02027
0.0
0.0
0.0
13.6064 9.4582 9.0423 7.4453
0.01402
0.65600 0.20023 0.11990
0.01299 0.00590 0.00390
2.8077 2.7112 2.5188
0.00957
0.81401 0.17345 0.09113
0.01007 0.00444 0.00234
2.8532 2.7032 2.5297

Table 6: CPCS54 50 instances, w*=15; CPCS360 10 instances, w*=20.


0.22 IJGP
MC
0.28 IJGP
MC
0.32 IJGP
MC
0.40 IJGP
MC
0.51 IJGP
MC
0.65 IJGP
MC

2
0.00005
0.00501
0.00062
0.02170
0.00238
0.04018
0.01202
0.08726
0.07664
0.15396
0.19070
0.21890

IJGP 0.36262
MC 0.25281

Bit Error Rate
i-bound
4
6
8
0.00005 0.00005 0.00005
0.00800 0.00586 0.00462
0.00062 0.00062 0.00062
0.02968 0.02492 0.02048
0.00238 0.00238 0.00238
0.05004 0.04480 0.03878
0.01188 0.01194 0.01210
0.09762 0.09272 0.08766
0.07498 0.07524 0.07578
0.16048 0.15710 0.15452
0.19056 0.19016 0.19030
0.22056 0.21928 0.21904
Time
0.41695 0.86213 2.62307
0.21816 0.31094 0.74851

10
0.00005
0.00392
0.00062
0.01840
0.00238
0.03558
0.01192
0.08334
0.07554
0.15180
0.19056
0.21830

IBP
0.00005
0.00064
0.00242
0.01220
0.07816
0.19142

9.23610 0.019752
2.33257

Table 7: Coding networks: N=400, P=4, 500 instances, 30 iterations, w*=43.

5.2 Comparing IJGP with Other Algorithms
In this section we provide a comparison of IJGP with state-of-the-art publicly available schemes.
The comparison is based on a recent evaluation of algorithms performed at the Uncertainty in AI
2008 conference4 . We will present results on solving the belief updating task (also called the task
of computing posterior node marginals - MAR). We first give a brief overview of the schemes that
we experimented and compared with.
1. EDBP - Edge Deletion for Belief Propagation
4. Complete results are available at http://graphmod.ics.uci.edu/uai08/Evaluation/Report.

314

fiJ OIN -G RAPH P ROPAGATION A LGORITHMS

CPCS360, evid=10, w*=20
0.20
IJGP 1 it
IJGP 10 it
IJGP 20 it
MC
IBP 1 it
IBP 10 it
IBP 20 it

0.18
0.16

KL distance

0.14
0.12
0.10
0.08
0.06
0.04
0.02
0.00

0

1

2

3

4

5

6

7

8

9

10

11

i-bound

(a) Performance vs. i-bound.
CPCS360, evid=10, w*=20
6e-6
IJGP 20 iterations
(at convergence)
5e-6

KL distance

4e-6

3e-6

2e-6

1e-6

0

1

2

3

4

5

6

7

8

9

10

11

i-bound

(b) Fine granularity for KL.

Figure 26: CPCS360: KL distance.
EDBP (Choi & Darwiche, 2006a, 2006b) is an approximation algorithm for Belief Updating.
It solves exactly a simplified version of the original problem, obtained by deleting some of
the edges of the problem graph. Edges to be deleted are selected based on two criteria:
quality of approximation and complexity of computation (tree-width reduction). Information
loss from lost dependencies is compensated for by introducing auxiliary network parameters.
This method corresponds to Iterative Belief Propagation (IBP) when enough edges are deleted
to yield a poly-tree, and corresponds to generalized BP otherwise.
2. TLSBP - A truncated Loop series Belief propagation algorithm

315

fiM ATEESCU , K ASK , G OGATE & D ECHTER

TLSBP is based on the loop series expansion formula of Chertkov and Chernyak (2006) which
specifies a series of terms that need to be added to the solution output by BP so that the exact
solution can be recovered. This series is basically a sum over all so-called generalized loops in
the graph. Unfortunately, because the number of these generalized loops can be prohibitively
large, the series is of little value. The idea in TLSBP is to truncate the series by decomposing
all generalized loops into simple and smaller loops, thus limiting the number of loops to be
summed. In our evaluation, we used an implementation of TLSBP available from the work
of Gomez, Mooji, and Kappen (2007). The implementation can handle binary networks only.
3. EPIS - Evidence Pre-propagation Importance Sampling
EPIS (Yuan & Druzdzel, 2003) is an importance sampling algorithm for Belief Updating. It is
well known that sampling algorithms perform poorly when presented with unlikely evidence.
However, when samples are weighted by an importance function, good approximation can be
obtained. This algorithm computes an approximate importance function using loopy belief
propagation and -cutoff heuristic. We used an implementation of EPIS available from the
authors. The implementation works on Bayesian networks only.
4. IJGP - Iterative Join-Graph Propagation
In the evaluation, IJGP(i) was first run with i=2, until convergence, then with i=3, until convergence, etc. until i= treewidth (when i-bound=treewidth, the join-graph becomes a join-tree
and IJGP becomes exact). As preprocessing, the algorithm performed SAT-based variable domain pruning by converting zero probabilities in the problem to a SAT problem and performing singleton-consistency enforcement. Because the problem size may reduce substantially,
in some cases, this preprocessing step may have a significant impact on the time-complexity
of IJGP, amortized over the increasing i-bound. However, for a given i-bound, this step improves the accuracy of IJGP only marginally.
5. SampleSearch
SampleSearch (Gogate & Dechter, 2007) is a specialized importance sampling scheme for
graphical models that contain zero probabilities in their CPTs. On such graphical models,
importance sampling suffers from the rejection problem in that it generates a large number
of samples which have zero weight. SampleSearch circumvents the rejection problem by
sampling from the backtrack-free search space in which every assignment (sample) is guaranteed to have non-zero weight. The backtrack-free search space is constructed on the fly by
interleaving sampling with backtracking style search. Namely, when a sample is supposed
to be rejected because its weight is zero, the algorithm continues instead with systematic
backtracking search, until a non zero weight sample is found. For the evaluation version,
the importance distribution of SampleSearch was constructed from the output of IJGP with
i-bound of 3. For more information on how the importance distribution is constructed from
the output of IJGP, see the work by Gogate (2009).
The evaluation was conducted on the following benchmarks (see footnote 4 for details):
1. UAI06-MPE - from UAI-06, 57 instances, Bayesian networks (40 instances were used).
2. UAI06-PE - from UAI-06, 78 instances, Bayesian networks (58 instances were used).
316

fiJ OIN -G RAPH P ROPAGATION A LGORITHMS

IJGP
EDBP
TLSBP
EPIS
SampleSearch

WCSPs BN2O Grids Linkage Promedas UAI06-MPE UAI06-PE Relational

































Table 8: Scope of our experimental study.
Score vs KL distance
1
Score vs KL distance
0.9
0.8

Score

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

0.2

0.4

0.6

0.8

1

KL distance

Figure 27: Score as a function of KL distance.
3. Relational Bayesian networks - constructed from the Primula tool, 251 instances, binary variables, large networks with large tree-width, but with high levels of determinism (30 instances
were used).
4. Linkage networks - 22 instances, tree-width 20-35, Markov networks (5 instances were used).
5. Grids - from 12x12 to 50x50, 320 instances, treewidth 12-50.
6. BN2O networks - Two-layer Noisy-OR Bayesian networks, 18 instances, binary variables, up
to 55 variables, treewidth 24-27.
7. WCSPs - Weighted CSPs, 97 instances, Markov networks (18 instances were used).
8. Promedas - real-world medical diagnosis, 238 instances, tree-width 1-60, Markov networks
(46 instances were used).

Table 8 shows the scope of our experimental study. A indicates that the solver was able to

handle the benchmark type and therefore evaluated on it while a lack of a indicates otherwise.
We measure the performance of the algorithms in terms of a KL-distance based score. Formally,
the score of a solver on a problem instance is equal to 10avgkld where avgkld is the average KL
distance between the exact marginal (which was computed using the UCLA Ace solver, see Chavira
& Darwiche, 2008) and the approximate marginal output by the solver. If a solver does not output a
solution, we consider its KL-distance to be . A score lies between 0 and 1, with 1 indicating that
the solver outputs exact solution while 0 indicating that the solver either does not output a solution
or has infinite average KL distance. Figure 27 shows the score as a function of KL distance.
317

fiM ATEESCU , K ASK , G OGATE & D ECHTER

In Figures 28-35 we report the results of experiments with each of the problem sets. Each
solver has a timeout of 20 minutes on each problem instance; when solving a problem, each solver
periodically outputs the best solution found so far. Using this, we can compute, for each solver, at
any point in time, the total sum of its scores over all problem instances in a particular set, called
SumScore(t). On the horizontal axis, we have the time and on the vertical axis, the SumScore(t).
The higher the curve of a solver is, the better (the higher the score).
In summary, we see that IJGP shows the best performance on the first four classes of networks
(UAI-MPE, UAI-PE, Relational and Linkage), it is tied with other algorithms on two classes (Grid
and BN2O), and is surpassed by EDBP on the last two classes (WCSPs and Promedas). EPIS and
SampleSearch, which are importance sampling schemes, are often inferior to IJGP and EDBP. In
theory, the accuracy of these importance sampling schemes should improve with time. However,
the rate of improvement is often unknown in practice. On the hard benchmarks that we evaluated
on, we found that this rate is quite small and therefore the improvement cannot be discerned from
the Figures. We discuss the results in detail below.
As mentioned earlier, TLSBP works only on binary networks (i.e., two variables per function)
and therefore it was not evaluated on WCSPs, Linkage, UAI06-MPE and UAI06-PE benchmarks.
The UAI-MPE and UAI-PE instances were used in the UAI 2006 evaluation of exact solvers (for
details see the report by Bilmes & Dechter, 2006). Exact marginals are available on 40 UAI-MPE
instances and 58 UAI-PE instances. The results for UAI-MPE and UAI-PE instances are shown
in Figures 28 and 29 respectively. IJGP is the best performing scheme on both benchmark sets
reaching a SumScore very close to the maximum possible value in both cases after about 2 minutes
of CPU time. EDBP and SampleSearch are second best in both cases.
Relational network instances are generated by grounding the relational Bayesian networks using
the Primula tool (Chavira, Darwiche, & Jaeger, 2006). Exact marginals are available only on 30
out of the submitted 251 instances. From Figure 30, we observe that IJGPs SumScore steadily
increases with time and reaches a value very close to the maximum possible score of 30 after about
16 minutes of CPU time. SampleSearch is the second best performing scheme. EDBP, TLSBP and
EPIS perform quite poorly on these instances reaching the SumScore of 10, 13 and 13 respectively
after 20 minutes of CPU time.
The Linkage instances are generated by converting linkage analysis data into a Markov network
using the Superlink tool (Fishelson & Geiger, 2003). Exact marginals are available only on 5 out of
the 22 instances. The results are shown in Figure 31. After about one minute of CPU time, IJGPs
SumScore is close to 5 which remains steady thereafter while EDBP only reaches a SumScore of 2
in 20 minutes. SampleSearch is the second best performing scheme while EDBP is third best.
The results on Grid networks are shown in Figure 32. The sink node of the grid is the evidence
node. The deterministic ratio p is a parameter specifying the fraction of nodes that are deterministic,
that is, whose values are determined given the values of their parents. The evaluation benchmark
set consists of 30 instances having p = 50%,75% and 90% with exact marginals available on 27
instances only. EPIS, IJGP, SampleSearch and EDBP are in a close tie on this network, while
TLSBP has the lowest performance. While hard to see, EPIS is just slightly the best performing
scheme, IJGP is the second best followed by SampleSearch and EDBP. On this instances IJGPs
SumScore increases steadily with time.
The results on BN2O instances appear in Figure 33. This is again a very close tie, in this case
of all five algorithms. IJGP has a minuscule decrease of SumScore with time from 17.85 to 17.7.
Although in general an improvement in accuracy is expected for IJGP with higher i-bound, it is not
318

fiJ OIN -G RAPH P ROPAGATION A LGORITHMS

Approximate Mar Problem Set uai06-mpe

40
35

Sum Score

30
25
20
15
10
5
0
0

2

4

6

8

10

12

14

16

18

20

Time in minutes
SampleSearch

IJGP

EDBP

EPIS

Figure 28: Results on UAI-MPE networks. TLSBP is not plotted because it cannot handle UAIMPE benchmarks.

Approximate Mar Problem Set uai06-pe

50

Sum Score

40

30

20

10

0
0

2

4

6

8

10

12

14

16

18

20

Time in minutes
SampleSearch

IJGP

EDBP

EPIS

Figure 29: Results on UAI-PE networks. TLSBP is not plotted because it cannot handle UAI-PE
benchmarks.

319

fiM ATEESCU , K ASK , G OGATE & D ECHTER

Approximate Mar Problem Set Relational
35

30

Sum Score

25

20

15

10

5

0
0

2

4

6

8

10

12

14

16

18

20

18

20

Time in minutes
SampleSearch
IJGP

EDBP
TLSBP

EPIS

Figure 30: Results on relational networks.

Approximate Mar Problem Set Linkage
6

5

Sum Score

4

3

2

1

0
0

2

4

6

8

10

12

14

16

Time in minutes
SampleSearch

IJGP

EDBP

Figure 31: Results on Linkage networks. EPIS and TLSBP are not plotted because they cannot
handle Linkage networks.

320

fiJ OIN -G RAPH P ROPAGATION A LGORITHMS

Approximate Mar Problem Set Grids

25

Sum Score

20

15

10

5

0
0

2

4

6

8

10

12

14

16

18

20

16

18

20

Time in minutes
SampleSearch
IJGP

EDBP
TLSBP

EPIS

Figure 32: Results on Grid networks.

Approximate Mar Problem Set bn2o
18
16

Sum Score

14
12
10
8
6
4
2
0
0

2

4

6

8

10

12

14

Time in minutes
SampleSearch
IJGP

EDBP
TLSBP

EPIS

Figure 33: Results on BN2O networks. All solvers except IJGP quickly converge to the maximum
possible score of 18 and are therefore indistinguishable in the Figure.

321

fiM ATEESCU , K ASK , G OGATE & D ECHTER

Approximate Mar Problem Set WCSPs
18
16
14

Sum Score

12
10
8
6
4
2
0
0

2

4

6

8

10

12

14

16

18

20

Time in minutes
SampleSearch

IJGP

EDBP

Figure 34: Results on WCSPs networks. EPIS and TLSBP are not plotted because they cannot
handle WCSPs.

Approximate Mar Problem Set Promedas
45
40
35

Sum Score

30
25
20
15
10
5
0
0

2

4

6

8

10

12

14

16

18

20

Time in minutes
SampleSearch

IJGP

EDBP

TLSBP

Figure 35: Results on Promedas networks. EPIS is not plotted because it cannot handle Promedas
benchmarks, which are Markov networks.

322

fiJ OIN -G RAPH P ROPAGATION A LGORITHMS

guaranteed, and this is an example when it does not happen. The other solvers reach the maximum
possible SumScore of 18 (or very close to it) after about 6 minutes of CPU time.
The WCSP benchmark set has 97 instances. However we used only the 18 instances for which
exact marginals are available. Therefore the maximum SumScore that an algorithm can reach is
18. The results are shown in Figure 34. EDBP reaches a SumScore of 17 after almost 3 minutes
of CPU time while IJGP reaches a SumScore of 13 after about 3 minutes. The SumScores of
both IJGP and EDBP remain unchanged in the interval from 3 to 20 minutes. After looking at the
raw results, we found that IJGPs score was zero on 5 instances out of 18. This was because the
singleton consistency component implemented via the SAT solver did not finish in 20 minutes on
these instances. Although the singleton consistency step generally helps to reduce the practical time
complexity of IJGP on most instances, it adversely affects it on these WCSP instances.
The Promedas instances are Noisy-OR binary Bayesian networks (Pearl, 1988). These instances
are characterized by extreme marginals. Namely, for a given variable, the marginals are of the form
(1  , ) where  is a very small positive constant. Exact marginals are available only on 46 out of
the submitted 238 instances. On these structured problems (see Figure 35), we see that EDBP is the
best performing scheme reaching a SumScore very close to 46 after about 7 minutes of CPU time
while TLSBP and IJGP are able to reach a SumScore of about 40 in 20 minutes.

6. Related Work
There are numerous lines of research devoted to the study of belief propagation algorithms, or
message-passing schemes in general. Throughout the paper we have mentioned and compared with
other related work, especially in the experimental evaluation section. We give here a short summary
of the developments in belief propagation and present some related schemes that were not mentioned
before. For additional information see also the recent review by Koller (2010).
About a decade ago, Iterative Belief Propagation (Pearl, 1988) received a lot of interest from
the information theory and coding community. It was realized that two of the best error-correcting
decoding algorithms were actually performing belief propagation in networks with cycles. The
LDPC code (low-density parity-check) introduced long time ago by Gallager (1963), is now considered one of the most powerful and promising schemes that often performs impressively close to
Shannons limit. Turbo codes (Berrou, Glavieux, & Thitimajshima, 1993) are also very efficient in
practice and can be understood as an instance of belief propagation (McEliece et al., 1998).
A considerable progress towards understanding the behavior and performance of BP was made
through concepts from statistical physics. Yedidia et al. (2001) showed that IBP is strongly related
to the Bethe-Peierls approximation of variational (Gibbs) free energy in factor graphs. The Bethe
approximation is a particular case of the more general Kikuchi (1951) approximation. Generalized
Belief Propagation (Yedidia et al., 2005) is an application of the Kikuchi approximation that works
with clusters of variables, on structures called region graphs. Another algorithm that employs the
region-based approach is Cluster Variation Method (CVM) (Pelizzola, 2005). These algorithms
focus on selecting a good region-graph structure to account for the over-counting (and over-overcounting, etc.) of evidence. We view generalized belief propagation more broadly as any belief
propagation over nodes which are clusters of functions. Within this view IJGP, and GBP as defined
by Yedidia et al. (2001), as well as CVM, are special realizations of generalized belief propagation.
Belief Propagation on Partially Ordered Sets (PBP) (McEliece & Yildirim, 2002) is also a generalized form of Belief Propagation that minimizes the Bethe-Kikuchi variational free energy, and

323

fiM ATEESCU , K ASK , G OGATE & D ECHTER

that works as a message-passing algorithm on data structures called partially ordered sets, which
has junction graphs and factor graphs as examples. There is one-to-one correspondence between
fixed points of PBP and stationary points of the free energy. PBP includes as special cases many
other variants of belief propagation. As we noted before, IJGP is basically the same as PBP.
Expectation Propagation (EP) (Minka, 2001) is a an iterative approximation algorithm for computing posterior belief in Bayesian networks. It combines assumed-density filtering (ADF), an
extension of the Kalman filter (used to approximate belief states using expectations, such as mean
and variance), with IBP, and iterates until these expectations are consistent throughout the network.
TreeEP (Minka & Qi, 2004) deals with cyclic problem by reducing the problem graph to a tree subgraph and approximating the remaining edges. The relationship between EP and GBP is discussed
by Welling, Minka, and Teh (2005).
Survey Propagation (SP) (Braunstein et al., 2005) solves hard satisfiable (SAT) problems using a
message-passing algorithm on a factor graph consisting of variable and clause nodes. SP is inspired
by an algorithm called Warning Propagation (WP) and by BP. WP can determine if a tree-problem is
SAT, and if it is then it can provide a solution. BP can compute the number of satisfying assignments
for a tree-problem, as well as the fraction of the assignments where a variable is true. These two
algorithms are used as heuristics to define the SP algorithm, that is shown to be more efficient
than either of them on arbitrary networks. SP is still a heuristic algorithm with no guarantee of
convergence. SP was inspired by the new concept of cavity method in statistical physics, and can
be interpreted as BP where variables can not only take the values true or false, but also the extra
dont care value. For a more detailed treatment see the book by Mezard and Montanari (2009).

7. Conclusion
In this paper we investigated a family of approximation algorithms for Bayesian networks, that
could also be extended to general graphical models. We started with bounded inference algorithms
and proposed Mini-Clustering (MC) scheme as a generalization of Mini-Buckets to arbitrary tree
decompositions. Its power lies in being an anytime algorithm governed by a user adjustable i-bound
parameter. MC can start with small i-bound and keep increasing it as long as it is given more time,
and its accuracy usually improves with more time. If enough time is given to it, it is guaranteed to
become exact. One of its virtues is that it can also produce upper and lower bounds, a route not
explored in this paper.
Inspired by the success of iterative belief propagation (IBP), we extended MC into an iterative
message-passing algorithm called Iterative Join-Graph Propagation (IJGP). IJGP operates on general join-graphs that can contain cycles, but it is sill governed by an i-bound parameter. Unlike IBP,
IJGP is guaranteed to become exact if given enough time.
We also make connections with well understood consistency enforcing algorithms for constraint
satisfaction, giving strong support for iterating messages, and giving insight into the performance
of IJGP (IBP). We show that: (1) if a value of a variable is assessed as having zero-belief in any
iteration of IJGP, then it remains a zero-belief in all subsequent iterations; (2) IJGP converges in a
finite number of iterations relative to its set of zero-beliefs; and, most importantly (3) that the set
of zero-beliefs decided by any of the iterative belief propagation methods is sound. Namely any
zero-belief determined by IJGP corresponds to a true zero conditional probability relative to the
given probability distribution expressed by the Bayesian network.

324

fiJ OIN -G RAPH P ROPAGATION A LGORITHMS

Our experimental evaluation of IJGP, IBP and MC is provided, and IJGP emerges as one of the
most powerful approximate algorithms for belief updating in Bayesian networks.

References
Arnborg, S. A. (1985). Efficient algorithms for combinatorial problems on graphs with bounded
decomposability - a survey. BIT, 25, 223.
Bacchus, F., Dalmao, S., & Pitassi, T. (2003). Value elimination: Bayesian inference via backtracking search. In Proceedings of the Nineteenth Conference on Uncertainty in Artificial
Intelligence (UAI03), pp. 2028.
Berrou, C., Glavieux, A., & Thitimajshima, P. (1993). Near Shannon limit error-correcting coding:
Turbo codes. In Proceedings of the 1993 International Conference on Communications, pp.
10641070.
Bilmes, J., & Dechter, R. (2006). Evaluation of probabilistic inference systems in UAI06.
http://ssli.ee.washington.edu/ bilmes/uai06InferenceEvaluation/.
Braunstein, A., Mezard, M., & Zecchina, R. (2005). Survey propagation: An algorithm for satisfiability. Random Struct. Algorithms, 27(2), 201226.
Chavira, M., & Darwiche, A. (2008). On probabilistic inference by weighted model counting.
Artificial Intelligence, 172(67), 772799.
Chavira, M. D., Darwiche, A., & Jaeger, M. (2006). Compiling relational bayesian networks for
exact inference. International Journal of Approximate Reasoning, 42(1-2), 420.
Chertkov, M., & Chernyak, V. Y. (2006). Loop series for discrete statistical models on graphs.
Journal of Statistical Mechanics: Theory and Experiment, P6009.
Choi, A., Chavira, M., & Darwiche, A. (2007). Node splitting: A scheme for generating upper
bounds in bayesian networks. In Proceedings of the Twenty Third Conference on Uncertainty
in Artificial Intelligence (UAI07), pp. 5766.
Choi, A., & Darwiche, A. (2006a). An edge deletion semantics for belief propagation and its
practical impact on approximation quality. In Proceedings of the The Twenty-First National
Conference on Artificial Intelligence (AAAI06), pp. 11071114.
Choi, A., & Darwiche, A. (2006b). A variational approach for approximating bayesian networks by
edge deletion. In Proceedings of the Twenty Second Conference on Uncertainty in Artificial
Intelligence (UAI06), pp. 8089.
Cooper, G. F. (1990). The computational complexity of probabistic inferences. Artificial Intelligence, 42, 393405.
Dagum, P., & Luby, M. (1993). Approximating probabilistic inference in bayesian belief networks
is NP-hard. Artificial Intelligence, 60(1), 141153.
Darwiche, A. (2001). Recursive conditioning. Artificial Intelligence, 125(1-2), 541.
325

fiM ATEESCU , K ASK , G OGATE & D ECHTER

Dechter, R. (1992). Constraint networks. Encyclopedia of Artificial Intelligence, 276285.
Dechter, R. (1996). Bucket elimination: A unifying framework for probabilistic inference algorithms. In Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence
(UAI96), pp. 211219.
Dechter, R. (1999). Bucket elimination: A unifying framework for reasoning. Artificial Intelligence,
113, 4185.
Dechter, R. (2003). Constraint Processing. Morgan Kaufmann Publishers.
Dechter, R., Bidyuk, B., Mateescu, R., & Rollon, E. (2010). The power of belief propagation: A
constraint propagation perspective. In Dechter, R., Geffner, H., & Halpern, J. (Eds.), Heuristics, Probabilities and Causality: A Tribute to Judea Pearl.
Dechter, R., Kask, K., & Larrosa, J. (2001). A general scheme for multiple lower bound computation in constraint optimization. In Proceedings of the Seventh International Conference on
Principles and Practice of Constraint Programming (CP01), pp. 346360.
Dechter, R., Kask, K., & Mateescu, R. (2002). Iterative join-graph propagation. In Proceedings of
the Eighteenth Conference on Uncertainty in Artificial Intelligence (UAI02), pp. 128136.
Dechter, R., & Mateescu, R. (2003). A simple insight into iterative belief propagations success. In
Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence (UAI03),
pp. 175183.
Dechter, R., & Mateescu, R. (2007). AND/OR search spaces for graphical models. Artificial Intelligence, 171(2-3), 73106.
Dechter, R., & Pearl, J. (1987). Network-based heuristics for constraint satisfaction problems.
Artificial Intelligence, 34, 138.
Dechter, R., & Pearl, J. (1989). Tree clustering for constraint networks. Artificial Intelligence, 38,
353366.
Dechter, R., & Rish, I. (1997). A scheme for approximating probabilistic inference. In Proceedings
of the Thirteenth Conference on Uncertainty in Artificial Intelligence (UAI97), pp. 132141.
Dechter, R., & Rish, I. (2003). Mini-buckets: A general scheme for approximating inference.
Journal of ACM, 50(2), 107153.
Fishelson, M., & Geiger, D. (2003). Optimizing exact genetic linkage computations. In Proceedings
of the Seventh Annual International Conference on Computational Biology (RECOMB03),
pp. 114121.
Gallager, R. G. (1963). Low-Density Parity-Check Codes. MIT Press, Cambridge, MA.
Gogate, V., & Dechter, R. (2007). SampleSearch: A scheme that searches for consistent samples. In
Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics
(AISTATS07), pp. 147154.
326

fiJ OIN -G RAPH P ROPAGATION A LGORITHMS

Gogate, V. (2009). Sampling Algorithms for Probabilistic Graphical models with Determinism.
Ph.D. thesis, School of Information and Computer Sciences, University of California, Irvine.
Gomez, V., Mooji, J. M., & Kappen, H. J. (2007). Truncating the loop series expansion for belief
propagation. Journal of Machine Learning, 8, 19872016.
Gottlob, G., Leone, N., & Scarcello, F. (2000). A comparison of structural CSP decomposition
methods. Artificial Intelligence, 243282.
Jensen, F. V., Lauritzen, S. L., & Olesen, K. G. (1990). Bayesian updating in causal probabilistic
networks by local computation. Computational Statistics Quarterly, 4, 269282.
Kask, K. (2001). Approximation algorithms for graphical models. Ph.D. thesis, Information and
Computer Science, University of California, Irvine.
Kask, K., & Dechter, R. (2001). A general scheme for automatic search heuristics from specification
dependencies. Artificial Intelligence, 129(1-2), 91131.
Kask, K., Dechter, R., Larrosa, J., & Dechter, A. (2005). Unifying cluster-tree decompositions for
reasoning in graphical models. Artificial Intelligence, 166 (1-2), 165193.
Kikuchi, R. (1951). A theory of cooperative phenomena. Phys. Rev., 81(6), 9881003.
Koller, D. (2010). Belief propagation in loopy graphs. In Dechter, R., Geffner, H., & Halpern, J.
(Eds.), Heuristics, Probabilities and Causality: A Tribute to Judea Pearl.
Larrosa, J., Kask, K., & Dechter, R. (2001). Up and down mini-bucket: a scheme for approximating
combinatorial optimization tasks. Tech. rep., University of California Irvine.
Lauritzen, S. L., & Spiegelhalter, D. J. (1988). Local computation with probabilities on graphical
structures and their application to expert systems. Journal of the Royal Statistical Society,
Series B, 50(2), 157224.
Mackworth, A. K. (1977). Consistency in networks of relations. Artificial Intelligence, 8(1), 99
118.
Mezard, M., & Montanari, A. (2009). Information, Physics and Computation. Oxford University
Press.
Mezard, M., Parisi, G., & Zecchina, R. (2002). Analytic and algorithmic solution of random satisfiability problems. Science, 297, 812815.
Maier, D. (1983). The Theory of Relational Databases. Computer Science Press, Rockville, MD.
Mateescu, R., Dechter, R., & Kask, K. (2002). Tree approximation for belief updating. In Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI02), pp. 553559.
McEliece, R. J., MacKay, D. J. C., & Cheng, J. F. (1998). Turbo decoding as an instance of Pearls
belief propagation algorithm. IEEE J. Selected Areas in Communication, 16(2), 140152.
McEliece, R. J., & Yildirim, M. (2002). Belief propagation on partially ordered sets. In Mathematical Systems Theory in Biology, Communications, Computation, and Finance, pp. 275300.
327

fiM ATEESCU , K ASK , G OGATE & D ECHTER

Minka, T. (2001). Expectation propagation for approximate bayesian inference. In Proceedings
of the Seventeenth Annual Conference on Uncertainty in Artificial Intelligence (UAI01), pp.
362369.
Minka, T., & Qi, Y. (2004). Tree-structured approximations by expectation propagation. In Advances in Neural Information Processing Systems 16 (NIPS03).
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann.
Pelizzola, A. (2005). Cluster variation method in statistical physics and probabilistic graphical
models. Journal of Physics A: Mathematical and General, 38(33), R309R339.
Rollon, E., & Dechter, R. (2010). Evaluating partition strategies for mini-bucket elimination. In The
Eleventh International Symposium on Artificial Intelligence and Mathematics (ISAIM10).
Roth, D. (1996). On the hardness of approximate reasoning. Artificial Intelligence, 82(1-2), 273
302.
Shafer, G. R., & Shenoy, P. P. (1990). Probability propagation. Annals of Mathematics and Artificial
Intelligence, 2, 327352.
Shenoy, P. P. (1992). Valuation-based systems for bayesian decision analysis. Operations Research,
40, 463484.
Welling, M., Minka, T. P., & Teh, Y. W. (2005). Structured region graphs: Morphing ep into gbp. In
Proceedings of the Twenty First Conference on Uncertainty in Artificial Intelligence (UAI05),
pp. 609614.
Wexler, Y., & Meek, C. (2008). MAS: A multiplicative approximation scheme for probabilistic inference. In Proceedings of Advances in Neural Information Processing Systems 21 (NIPS08),
pp. 17611768.
Yedidia, J. S., Freeman, W. T., & Weiss, Y. (2000). Generalized belief propagation. Tech. rep.
TR2000-26, Mitsubishi Electric Research Laboratories.
Yedidia, J. S., Freeman, W. T., & Weiss, Y. (2001). Generalized belief propagation. In Advances in
Neural Information Processing Systems 13 (NIPS00), pp. 689695.
Yedidia, J. S., Freeman, W. T., & Weiss, Y. (2005). Constructing free energy approximations and
generalized belief propagation algorithms. IEEE Transactions on Information Theory, 51,
22822312.
Yuan, C., & Druzdzel, M. J. (2003). An importance sampling algorithm based on evidence prepropagation. In Proceedings of the Nineteenth Conference in Uncertainty in Artificial Intelligence (UAI03), pp. 624631.
Zhang, N. L., Qi, R., & Poole, D. (1994). A computational theory of decision networks. International Journal of Approximate Reasoning, 11, 83158.

328

fiJournal of Artificial Intelligence Research 37 (2010) 99-139

Submitted 08/09; published 02/10

Interactive Cost Configuration Over Decision Diagrams
Henrik Reif Andersen

hra@configit.com

Configit A/S
DK-2100 Copenhagen, Denmark

Tarik Hadzic

t.hadzic@4c.ucc.ie

Cork Constraint Computation Centre
University College Cork
Cork, Ireland

David Pisinger

pisinger@man.dtu.dk

DTU Management
Technical University of Denmark
DK-2800 Kgs. Lyngby, Denmark

Abstract
In many AI domains such as product configuration, a user should interactively specify
a solution that must satisfy a set of constraints. In such scenarios, offline compilation
of feasible solutions into a tractable representation is an important approach to delivering efficient backtrack-free user interaction online. In particular, binary decision diagrams
(BDDs) have been successfully used as a compilation target for product and service configuration. In this paper we discuss how to extend BDD-based configuration to scenarios
involving cost functions which express user preferences.
We first show that an efficient, robust and easy to implement extension is possible if
the cost function is additive, and feasible solutions are represented using multi-valued decision diagrams (MDDs). We also discuss the effect on MDD size if the cost function is
non-additive or if it is encoded explicitly into MDD. We then discuss interactive configuration in the presence of multiple cost functions. We prove that even in its simplest form,
multiple-cost configuration is NP-hard in the input MDD. However, for solving two-cost
configuration we develop a pseudo-polynomial scheme and a fully polynomial approximation scheme. The applicability of our approach is demonstrated through experiments over
real-world configuration models and product-catalogue datasets. Response times are generally within a fraction of a second even for very large instances.

1. Introduction
Interactively specifying a solution that must satisfy a number of combinatorial restrictions
is an important problem in many AI domains related to decision making: from buying a
product online, selling an insurance policy to setting up a piece of equipment. Solutions
are often modeled as assignments to variables over which constraints are imposed. When
assigning variables without sufficient guidance, a user might be forced to backtrack, since
some of the choices he made cannot be extended in a way that would satisfy all of the
succeeding constraints. To improve the usability of interaction it is therefore important
to indicate to a user all values that participate in at least one remaining solution. If a
c
2010
AI Access Foundation. All rights reserved.

fiAndersen, Hadzic, & Pisinger

user is assigning only such values he is guaranteed to be able to reach any feasible solution
while never being forced to backtrack. We refer to the task of computing such values
as calculating valid domains (CVD). Since this is a computationally challenging (NP-hard)
problem, and short execution times are important in an interactive setting, it has been
suggested to compile offline (prior to user interaction) the set of all feasible solutions into a
representation form that supports efficient execution of CVD during online interaction.
Mller, Andersen, and Hulgaard (2002) and Hadzic, Subbarayan, Jensen, Andersen,
Mller, and Hulgaard (2004) investigated such an approach by using binary decision diagrams (BDDs) as a compilation target. BDDs are one of the data-structures investigated in
the knowledge compilation community which preprocess original problem formulations into
more tractable representations to enhance solving the subsequent tasks. CVD is just one of
such tasks occurring in the configuration domain. Knowledge compilation has been successfully applied to a number of other areas such as planning, diagnosis, model checking etc.
Beside BDDs, a number of other structures, such as various sublanguages of negation normal forms (NNFs) (Darwiche & Marquis, 2002), AND/OR diagrams (Mateescu, Dechter, &
Marinescu, 2008), finite state automata (Vempaty, 1992; Amilhastre, Fargier, & Marquis,
2002) and various extensions of decision diagrams (Drechsler, 2001; Wegener, 2000; Meinel
& Theobald, 1998) are used as compilation targets. Some of them are suitable for interactive configuration as well. In particular, Vempaty (1992) suggested compiling constraints
into an automaton. However, BDDs are the most investigated data structures with a tool
support unrivaled by other emerging representations. There are many highly optimized
open-source BDD packages (e.g., Somenzi, 1996; Lind-Nielsen, 2001) that allow easy and
efficient manipulation of BDDs. In contrast, publicly available, open-source compilers are
still being developed for many newer representations. In particular, the application of BDDs
to configuration resulted in a patent approval (Lichtenberg, Andersen, Hulgaard, Mller, &
Rasmussen, 2001) and the establishment of the spinoff company Configit A/S1 .
The work in this paper is motivated by decision making scenarios where solutions are
associated with a cost function, expressing implicitly properties such as price, quality, failure probability etc. A user might prefer one solution over another given the value of such
properties. A natural way in which a user expresses his cost preferences in a configuration
setting is to bound the minimal or maximal cost of any solution he is willing to accept.
We therefore study the problem of calculating weighted valid domains (wCVD), where we
eliminate those values that in every valid solution are more expensive than a user-provided
maximal cost. We present a configurator that supports efficient cost bounding for a wide
class of additive cost functions. Our approach is easily implementable and scales well for
all the instances that were previously compiled into BDDs for standard interactive configuration. The cornerstone of our approach is to reuse the robust compilation of constraints
into a BDD, and then extract a corresponding multi-valued decision diagram (MDD). The
resulting MDD allows us to label edges with weights and utilize efficient shortest path algorithms to label nodes and filter expensive values on MDD edges. While our MDD extraction
technique is novel, labeling edges in a decision diagram is suggested in other works as well.
In its most generic interpretation (Wilson, 2005), edges of a decision diagram can be labeled
with elements of a semiring to support algebraic computations relevant for probabilistic rea1. http://www.configit.com

100

fiInteractive Cost Configuration Over Decision Diagrams

soning, optimization etc. Amilhastre et al. (2002) suggest labeling edges of an automaton to
reason abut optimal restorations and explanations. In general, many knowledge compilation
structures have their weighted counterparts, many of which are captured in the framework
of valued negation normal forms (VNNFs) (Fargier & Marquis, 2007). These structures
are utilized for probabilistic reasoning, diagnosis, and other tasks involving reasoning about
real-valued rather than Boolean functions. Some of them can in principle be used for wCVD
queries, but the public tool support for weighted variants is less available or is tailored for
tasks outside the configuration domain.
We further extend our approach to support valid domains computation in the presence
of multiple cost functions. A user often has multiple conflicting objectives, that should be
satisfied simultaneously. Traditional approaches in multi-criteria optimization (Figueira,
Greco, & Ehrgott, 2005; Ehrgott & Gandibleux, 2000) typically interact with a user in a
way that is unsuitable in a configuration setting  cost functions are combined in a single
objective and in each interaction step few non-dominated solutions are sampled and displayed to a user. Based on user selections a more adequate aggregation of costs is performed
before the next interaction step. We suggest a more configuration-oriented interaction approach where domains are bounded with respect to multiple costs. We prove that this is a
particularly challenging problem. Computing valid domains over an MDD in the presence of
two cost functions (2-wCVD) is NP-hard, even in the simplest extension of linear inequalities
with positive coefficients and Boolean variables. Despite this negative result, we provide
an implementation of 2-wCVD queries in pseudo-polynomial time and space and develop a
fully polynomial time approximation scheme (FPTAS). We prove that no pseudo-polynomial
algorithm and hence no fully polynomial approximation scheme exists for computing domains in the presence of arbitrarily many cost functions since that is an NP-hard problem
in the strong sense. Finally, we demonstrate through experimental evaluation the applicability of both the wCVD and 2-wCVD query over large real-world configuration models and
product-catalogue datasets. To the best of our knowledge, we present the first interactive configurator supporting configuration wrt. cost restrictions in a backtrack-free and
complete manner. This constitutes a novel addition to both existing product-configuration
approaches as well as to approaches within multi-criteria decision making (Figueira et al.,
2005).
The remainder of the paper is organized as follows. In Section 2 we describe background
work and notation. In Section 3 we describe our approach to implementing wCVD query over
an MDD while in Section 4 we show how to compile such an MDD. In Section 5 we discuss
configuring in the presence of multiple costs. In Section 6 we present empirical evaluation
of our approach. In Section 7 we describe related work and finally we conclude in Section 8.

2. Preliminaries
We will briefly review the most important concepts and background.
2.1 Constraint Satisfaction Problems
Constraint satisfaction problems (CSPs) form a framework for modeling and solving combinatorial problems, where a solution to a problem can be formulated as an assignment to
101

fiAndersen, Hadzic, & Pisinger

variables that satisfy certain constraints. In its standard form, CSP involves only a finite
number of variables, defined over finite domains.
Definition 1 (CSP) A constraint satisfaction problem (CSP) is a triple (X, D, F ) where
X is a set of variables {x1 , . . . , xn }, D = D1 . . .Dn is the Cartesian product of their finite
domains D1 , . . . , Dn and F = {f1 , ..., fm } is a set of constraints defined over variables X.
Each constraint f is a function defined over a subset of variables Xf  X called the scope
of f . It maps each assignment to the Xf variables into {0, 1} where 1 indicates that f is
satisfied and 0 indicates that f is violated by the assignment. The solution is an assignment
to all variables X that satisfies all constraints simultaneously.
Formally, an assignment of values a1 , . . . , an to variables x1 , . . . , xn is denoted as a set
of pairs  = {(x1 , a1 ), . . . , (xn , an )}. The domain of an assignment dom() is the set of
variables which are assigned: dom() = {xi | a  Di .(xi , a)  } and if all variables
are assigned, i.e. dom() = X, we refer to  as a total assignment. We say that a total
assignment  is valid if it satisfies all the rules, which is denoted as  |= F . A partial
assignment , dom()  X is valid if it can be extended to a total assignment 0   that
is valid 0 |= F . We define the solution space Sol as the set of all valid total assignments,
i.e. Sol = { |  |= F, dom() = X}.
2.2 Interactive Configuration
Interactive configuration is an important application domain where a user is assisted in
specifying a valid configuration (of a product, a service or something else) by interactively
providing feedback on valid options for unspecified attributes. Such a problem arises in a
number of domains. For example, when buying a product, a user should specify a number of
product attributes. Some attribute combinations might not be feasible and if no guidance
is provided, the user might reach a dead-end when interacting with the system. He will be
forced to backtrack, which might seriously decrease the user satisfaction.
In many cases, valid configurations can be implicitly described by specifying restrictions
on combining product attributes. We use a CSP model to represent such restrictions, and
each CSP solution corresponds to a valid configuration. Each configurable attribute is
represented with a variable, so that each attribute option corresponds to a value in the
variable domain. In Example 1 we illustrate a simple configuration problem and its CSP
model.
Example 1 To specify a T-shirt we have to choose the color (black, white, red, or blue),
the size (small, medium, or large) and the print (Men In Black - MIB or Save The
Whales - STW). If we choose the MIB print then the color black has to be chosen as well,
and if we choose the small size then the STW print (including a large picture of a whale)
cannot be selected as the picture of a whale does not fit on the small shirt. The configuration
problem (X, D, F ) of the T-shirt example consists of variables X = {x1 , x2 , x3 } representing
color, size and print. Variable domains are D1 = {0, 1, 2, 3} (black , white, red , blue), D2 =
{0, 1, 2} (small , medium, large), and D3 = {0, 1} (MIB , STW ). The two rules translate to
F = {f1 , f2 }, where f1 is x3 = 0  x1 = 0 (MIB  black ) and f2 is (x2 = 0  x3 6= 1)
(small  not STW ). There are |D1 ||D2 ||D3 | = 24 possible assignments. Eleven of these
assignments are valid configurations and they form the solution space shown in Fig. 1. 
102

fiInteractive Cost Configuration Over Decision Diagrams

(black , small , MIB )
(black , medium, MIB )
(black , medium, STW )
(black , large, MIB )

(black , large, STW )
(white, medium, STW )
(white, large, STW )
(red , medium, STW )

(red , large, STW )
(blue, medium, STW )
(blue, large, STW )

Figure 1: Solution space for the T-shirt example.
The fundamental task that we are concerned with in this paper is calculating valid
domains (CVD) query. For a partial assignment  representing previously made user assignments, the configurator calculates and displays a valid domain VD i []  Di for each
unassigned variable xi  X \ dom(). A domain is valid if it contains those and only those
values with which  can be extended to a total valid assignment 0 . In our example, if a
user selects a small T-shirt (x2 = 0), valid domains should be restricted to a MIB print
V D3 = {0} and black color V D1 = {0}.
Definition 2 (CVD) Given a CSP model (X, D, F ), for a given partial assignment  compute valid domains:
VDi [] = {a  Di | 0 .(0 |= F and   {(xi , a)}  0 )}
This task is of main interest since it delivers important interaction requirements: backtrackfreeness (user should never be forced to backtrack) and completeness (all valid configurations
should be reachable) (Hadzic et al., 2004). There are other queries relevant for supporting
user interaction such as explanations and restorations from a failure, recommendations of
relevant products, etc., but CVD is an essential operation in our mode of interaction and is
of primary importance in this paper.
2.3 Decision Diagrams
Decision diagrams form a family of rooted directed acyclic graphs (DAGs) where each node
u is labeled with a variable xi and each of its outgoing edges e is labeled with a value a  Di .
No node may have more than one outgoing edge with the same label. The decision diagram
contains one or more terminal nodes, each labeled with a constant and having no outgoing
edges. The most well known member of this family are binary decision diagrams (BDDs)
(Bryant, 1986) which are used for manipulating Boolean functions in many areas, such
as verification, model checking, VLSI design (Meinel & Theobald, 1998; Wegener, 2000;
Drechsler, 2001) etc. In this paper we will primarily operate with the following variant of
multi-valued decision diagrams:
Definition 3 (MDD) An MDD denoted M is a rooted directed acyclic graph (V, E), where
V is a set of vertices containing the special terminal vertex 1 and a root r  V . Further,
var : V  {1, . . . , n + 1} is a labeling of all nodes with a variable index such that var(1) =
n + 1. Each edge e  E is denoted with a triple (u, u0 , a) of its start node u, its end node u0
and an associated value a.
We work only with ordered MDDs. A total ordering < of the variables is assumed such
that for all edges (u, u0 , a), var(u) < var(u0 ). For convenience we assume that the variables
103

fiAndersen, Hadzic, & Pisinger

in X are ordered according to their indices. Ordered MDDs can be considered as being
arranged in n layers of vertices, each layer being labeled with the same variable index. We
will denote with Vi the set of all nodes labeled with xi , Vi = {u  V | var(u) = i}. Similarly,
we will denote with Ei the set of all edges originating in Vi , i.e. Ei = {e(u, u0 , a)  E |
var(u) = i}. Unless otherwise specified, we assume that on each path from the root to the
terminal, every variable labels exactly one node.
An MDD encodes a CSP solution set Sol  D1  . . .  Dn , defined over variables
{x1 , . . . , xn }. To check whether an assignment a = (a1 , . . . , an )  D1  . . .  Dn is in Sol we
traverse M from the root, and at every node u labeled with variable xi , we follow an edge
labeled with ai . If there is no such edge then a is not a solution, i.e., a 6 Sol. Otherwise, if
the traversal eventually ends in terminal 1 then a  Sol. We will denote with p : u1
u2
any path in MDD from u1 to u2 . Also, edges between u and u0 will be sometimes denoted
as e : u  u0 . A value a of an edge e(u, u0 , a) will be sometimes denoted as v(e). We will not
make distinction between paths and assignments. Hence, the set of all solutions represented
by the MDD is Sol = {p | p : r
1}. In fact, every node u  Vi can be associated with a
subset of solutions Sol(u) = {p | p : u
1}  Di  . . .  Dn .
x1
0

1

x2
0
x3

1

x3
0

2

1
x3

0

x3

1 0

1

1

1

x1
2

3

0 1

x2

x2

2

1

x3

x3

x3

x3

1

1

1

1

x2
2

1

1

x2
2

x2

0 2 1
x3

x3

2 3

1 2

x3
0 0 1

x3
1

1

(a) An MDD before merging.

(b) A merged MDD.

Figure 2: An uncompressed and merged MDD for the T-Shirt example.
Decision diagrams can be exponentially smaller than the size of the solution set they
encode by merging isomorphic subgraphs. Two nodes u1 , u2 are isomorphic if they encode
the same solution set Sol(u1 ) = Sol(u2 ). In Figure 2 we show a fully expanded MDD 2(a)
and an equivalent merged MDD 2(b) for the T-shirt solution space. In addition to merging
isomorphic subgraphs, another compression rule is usually utilized: removing redundant
nodes. A node u  Vi is redundant if it has Di outgoing edges, each pointing to the same
node u0 . Such nodes are eliminated by redirecting incoming edges from u to u0 and deleting u
from V . This introduces long edges that skip layers. An edge e(u, u0 , a) is long if var(u)+1 <
var(u0 ). In this case, e encodes the set of solutions: {a}  Dvar(u)+1  . . .  Dvar(u0 )1 . We
will refer to an MDD where both merging of isomorphic nodes and removal of redundant
nodes have taken place as a reduced MDD, which constitutes a multi-valued generalization
of BDDs which are typically reduced and ordered. A reduced MDD for the T-shirt CSP
is shown in Figure 3. In this paper, unless emphasized otherwise, by MDD we always
assume an ordered merged but not reduced MDD, since exposition is simpler, and removal
of redundant nodes can have at most a linear effect on size. Given a variable ordering
104

fiInteractive Cost Configuration Over Decision Diagrams

there is a unique merged MDD for a given CSP (X, D, F ) and its solution set Sol. The
size of MDD depends critically on the ordering, and could vary exponentially. It can grow
exponentially with the number of variables, but in practice, for many interesting problems
the size is surprisingly small.
x1
0 1
x2

2 3
x2

0
x3

12
2 1

0

x3
1

1

Figure 3: A reduced MDD for the T-shirt example.
Interactive Configuration over Decision Diagrams. A particularly attractive property of decision diagrams is that they support efficient execution of a number of important
queries, such as checking for consistency, validity, equivalence, counting, optimization etc.
This is utilized in a number of application domains where most of the problem description is
known offline (diagnosis, verification,etc.). In particular, calculating valid domains is linear
in the size of the MDD. Since calculating valid domains is an NP-hard problem in the size
of the input CSP model, it is not possible to guarantee interactive response in real-time.
In fact, the unacceptably long worst-case response times have been empirically observed
in a purely search-based approach to computing valid domains (Subbarayan et al., 2004).
Therefore, by compiling CSP solutions off-line (prior to user interaction) into a decision
diagram, we can efficiently (in the size of the MDD) compute valid domains during online
interaction with a user. It is important to note that the order in which the user decides
variables is completely unconstrained, i.e. it does not depend on the ordering of MDD variables. In our previous work we utilized Binary Decision Diagrams (BDDs) to represent all
valid configurations so that CVD queries can be executed efficiently (Hadzic et al., 2004).
Of course, BDDs might be exponentially large in the input CSP, but for many classes of
constraints they are surprisingly compact.

3. Interactive Cost Processing over MDDs
The main motivation for this work is extending the interactive configuration approach of
Mller et al. (2002), Hadzic et al. (2004), Subbarayan et al. (2004) to situations where in
addition to a CSP model (X, D, F ) involving only hard constraints, there is also a cost
function:
c : D1  . . .  Dn  R.
In product configuration setting, this could be a product price. In uncertainty setting, the
cost function might indicate a probability of an occurrence of an event represented by a
105

fiAndersen, Hadzic, & Pisinger

solution (failure of a hardware component, withdrawal of a bid in an auction etc.). In
any decision support context, the cost function might indicate user preferences. There is a
number of cost-related queries in which a user might be interested, e.g. finding an optimal
solution, or computing a most probable explanation. We, however, assume that a user is
interested in tight control of both the variable values as well as the cost of selected solutions.
For example, a user might desire a specific option xi = a, but he would also care about how
would such an assignment affect the cost of the remaining optimal solutions. We should
communicate this information to the user, and allow him to strike the right balance between
the cost and variable values by allowing him to interactively limit the maximal cost of the
product in addition to assigning variable values. Therefore, in this paper we are primarily
concerned with implementing a weighted CVD (wCVD) query: for a user-specified maximum
cost K, we should indicate which values in the unassigned variable domains can be extended
to a total assignment that is valid and costs less than K. From now on, we assume that a
user is interested in bounding the maximal cost (limiting the minimal cost is symmetric).
Definition 4 (wCVD) Given a CSP model (X, D, F ), a cost function c : D  R and a
maximal cost K, for a given partial assignment  a weighted CVD (wCVD) query requires
computation of the valid domains:
VDi [, K] = {a  Di | 0 .(0 |= F and   {(xi , a)}  0 and c(0 )  K)}
In this section we assume that an MDD representation of all CSP solutions is already
generated in an offline compilation step. We postpone discussion of MDD compilation to
Section 4 and discuss only delivering efficient online interaction on top of such MDD. We
will first discuss the practicability of implementing wCVD queries through explicit encoding of
costs into an MDD. We will then provide a practical and efficient approach to implementing
wCVD over an MDD when the cost function is additive. Finally, we will discuss further
extensions to handling more expressive cost functions.
3.1 Handling Costs Explicitly
An immediate approach to interactively handling a cost function is to treat the cost as any
other solution attribute, i.e. to add a variable y to variables X and add the constraint
y = c(x1 , . . . , xn )

(1)

to formulas F to enforce that y is equal to the total cost. The resulting configuration model
is compiled into an MDD M 0 and a user is able to bound the cost by restricting the domain
of y.
Assuming the variable ordering x1 < . . . < xn in the original CSP model (X, D, F ),
and assuming we inserted a cost variable into the i-th position, the new variable set X 0
has a variable ordering x01 < . . . < x0n+1 s.t. x01 = x1 , . . . , x0i1 = xi1 , x0i = y and x0i+1 =
xi , . . . , x0n+1 = xn . The domain Di0 of variable x0i is the set of all feasible costs C(Sol) =
{c(s) | s  Sol}. We will now demonstrate that the MDD M 0 may be exponentially larger
than M .
Lemma 1 |Ei0 |  |C(Sol)|.
106

fiInteractive Cost Configuration Over Decision Diagrams

Proof 1 For the i-th layer of MDD M 0 corresponding to variable y, for each cost c  C(Sol)
there must be at least one path p : r
1 with c(p) = c, and for such a path, an edge e  Ei0
at the i-th layer must be labeled with v(e) = c. Hence, for each cost there must be at least
one edge in Ei0 . This proves the lemma.
0
Furthermore, at least one of the layers of nodes Vi0 , Vi+1
has a number of nodes greater
p 0
than |Ei |. This follows from the following lemma:
0 |  |E 0 |.
Lemma 2 For the i-th layer of MDD M 0 , |Vi0 |  |Vi+1
i

0 | pairs of nodes (u , u )  V 0 V 0 , the statement
Proof 2 Since there are at most |Vi0 ||Vi+1
1 2
i+1
i
follows from the fact that for each pair (u1 , u2 ) there can be at most one edge e : u1  u2 .
Namely, every solution p3 formed by concatenating paths p1 : r
u1 and p2 : u2
1 has
a unique cost c(p3 ). However, if there were two edges e1 , e2 : u1  u2 , they would have to
have different values v(e1 ) 6= v(e2 ). But then, the same solution c(p3 ) would correspond to
two different costs v(e1 ), v(e2 ).

From the above considerations we see that whenever the range of possible costs C(Sol)
is exponential, the resulting MDD M 0 would be exponentially large as well. This would
result in a significantly increased size |V 0 |/|V |, particularly when there is a large number of
isomorphic nodes in M that would become non-isomorphic once the variable y is introduced
(since they root paths with different costs). An extreme instance of such a behavior is
presented in Example 2. Furthermore, even if C(Sol) is not large, there could be orders of
magnitude of increase in the size of M 0 due to breaking of isomorphic nodes in the MDD
as will be empirically demonstrated in Section 6, Table 3, for a number of configuration
instances. This is a major disadvantage as otherwise efficient CVD algorithms become
unusable since they operate over a significantly larger structure.
Example 2 Consider a model C(X, D, F ) with no constraints F = {}, and Boolean variables Dj = {0, 1}, j = 1 . . . , n. The solution space includes all assignments Sol = D1 
. . .  Dn and a corresponding MDD M (V, E) has one vertex and two
at each layer,
Pnedgesj1
 xj , there is
|V | = n + 1, |E| = 2  n. If we use the cost function: c(x1 , . . . , xn ) = j=1 2
an exponential number of feasible costs C(Sol) = {0, . . . , 2n  1}. Hence, |Ei0 |  2n and for
0 | is greater
the i-th layer corresponding to variable y, at least one of the layers |Vi0 |, |Vi+1

n/2
than 2n = 2 .
However, if there was no significant node isomorphism in M , adding a y variable does
not necessarily lead to a significant increase in size. An extreme instance of this is an MDD
with no isomorphic nodes, for example when every edge is labeled with a unique value. For
such an MDD, the number of non-terminal nodes is n  |Sol|. By adding a cost variable
y, the resulting MDD would add at most one node per path, leading to an MDD with at
most (n + 1)  |Sol| nodes. This translates to a minor increase in size: |V 0 |/|V | = (n + 1)/n.
This property will be empirically demonstrated in Section 6, Table 3, for product-catalogue
datasets. In the remainder of this paper we develop techniques tailored for instances where
a large increase in size occurs. We avoid explicit cost encoding and aim to exploit the
structure of the cost function to implement wCVD.
107

fiAndersen, Hadzic, & Pisinger

3.2 Processing Additive Cost Functions
One of the main contributions of this paper is a practical and efficient approach to deliver
wCVD queries if the cost function is additive. An additive cost function has the form
c(x1 , . . . , xn ) =

n
X

ci (xi )

i=1

where a cost ci (ai )  R is assigned for every variable xi and every value in its domain
ai  Di .
Additive functions are one of the most important and frequently used modeling constructs. A number of important combinatorial problems are modeled as integer linear programs where often both the constraints and the objective function are linear, i.e. represent
special cases of additive cost functions. In multi-attribute utility theory user preferences
are under certain assumptions aggregated into a single additive function through weighted
summation of utilities of individual attributes. In a product configuration context, many
properties are additive such as the memory capacity of a computer or the total weight. In
particular, based on our experience in commercially applying configuration technology, the
price of a product can often be modeled as the (weighted) sum of prices of individual parts.
3.2.1 The Labeling Approach
Assuming that we are given an MDD representation of the solution space Sol and a cost
function c, our approach to answering wCVD queries is based on three steps: 1) restricting
MDD wrt. the latest user assignment, 2) labeling remaining nodes by executing shortest
path algorithms and 3) filtering too expensive values by using node labels.
Restricting MDD. We are given a user assignment xi = ai , where xi can be any of the
unassigned variables, regardless of its position in the MDD variable ordering. We initialize
MDD pruning by removing all edges e(u, u0 , a), that are not in agreement with the latest
assignment, i.e. where var(u) = i and a 6= ai . This might cause a number of other edges
and nodes to become unreachable from the terminal or the root if we removed the last edge
in the set of children edges Ch(u) or parent edges P (u0 ). Any unreachable edge must be
removed as well. The pruning is repeated until a fixpoint is reached, i.e. until no more
nodes or edges can be removed. Algorithm 1 implements this scheme in O(|V | + |E|) time
and space by using a queue Q to maintain the set of edges that are yet to be removed.
Note that unassigning a user assignment xi = ai can be easily implemented in linear
time as well. It suffices to restore a copy of the initial MDD M , and perform restriction wrt.
a partial assignment  \ {(xi , ai )} where  is a current assignment. Algorithm 1 is easily
extended for this purpose by initializing the edge removal list Q with edges incompatible
wrt. any of the assignments in .
Computing Node Labels. Remaining edges e(u, u0 , a) in each layer Ei are implicitly
labeled with c(e) = ci (a). In the second step we compute for each MDD node u  V an
upstream cost of the shortest path from the root r to u, denoted as U [u], and a downstream
cost of the shortest path from u to the terminal 1, denoted as D[u]:
(
(
)
)
X
X
U [u] = min
c(e)
(2)
c(e) , D[u] = min
p:r

u

p:u

ep

108

1

ep

fiInteractive Cost Configuration Over Decision Diagrams

Algorithm 1: Restrict MDD.
Data: MDD M (V, E), variable xi , value ai
foreach e  Ei , v(e) 6= ai do
Q.push(e);
while Q 6=  do
e(u, u0 , a)  Q.pop();
delete e from M ;
if Ch(u) =  then
foreach e : u00  u do
Q.push(e);
if P (u0 ) =  then
foreach e : u0  u00 do
Q.push(e);

Algorithm 2 computes U [u] and D[u] labels in (|V | + |E|) time and space.
Algorithm 2: Update U, D labels.
Data: MDD M (V, E), Cost function c
D[] = , D[1] = 0;
foreach i = n, . . . , 1 do
foreach u  Vi do
foreach e : u  u0 do
D[u] = min{D[u], c(e) + D[u0 ]}
U [] = , U [r] = 0;
foreach i = 1, . . . , n do
foreach u  Vi do
foreach e : u  u0 do
U [u0 ] = min{U [u0 ], c(e) + U [u]}

Computing Valid Domains. Once the upstream and downstream costs U, D are computed, we can efficiently compute valid domains VDi wrt. any maximal cost bound K
since:
VDi [K] = {v(e) | U [u] + c(e) + D[u0 ]  K, e : u  u0 , u  Vi }
(3)
This can be achieved in a linear-time traversal (|V | + |E|) as shown in Algorithm 3.
Algorithm 3: Compute valid domains.
Data: MDD M (V, E), Cost function c, Maximal cost K
foreach i = 1, . . . , n do
V Di = ;
foreach u  Vi do
foreach e : u  u0 do
if U [u] + c[e] + D[u0 ]  K then
V Di  V Di  {v(e)};

Hence the overall interaction is as follows. Given a current partial assignment , MDD is
restricted wrt.  through Algorithm 1. Labels U, D are then computed through Algorithm 2
and valid domains are computed using Algorithm 3. The execution of all of these algorithms
109

fiAndersen, Hadzic, & Pisinger

requires (|V | + |E|) time and space. Hence, when an MDD representation of the solution
space is available, we can interactively enforce additive cost restrictions in linear time and
space.
3.3 Processing Additive Costs Over Long Edges
Our scheme can be extended to MDDs containing long edges. While for multivalued CSP
models with large domains space savings due to long edges might not be significant, for
binary models and binary decision diagrams (BDDs) more significant savings are possible.
Furthermore, in a similar fashion, our scheme might be adopted over other versions of
decision diagrams that contain long edges (with different semantics) such as zero-suppressed
BDDs where a long edge implies that all skipped variables are assigned 0.
Recall that in reduced MDDs, redundant nodes u  Vi which have Di outgoing edges,
each pointing to the same node u0 , are eliminated. An edge e(u, u0 , a) with var(u) =
k and var(u0 ) = l is long if k + 1 < l, and in this case, e encodes a set of solutions:
{a}  Dk+1  . . .  Dl1 . The labeling of edges can be generalized to accommodate such
edges as well. Let domains Dj0 , j = 1, . . . , n represent variable domains updated wrt. the
current assignment, i.e. Dj0 = Dj if xj is unassigned, and Dj0 = {[xj ]} otherwise. An edge
e(u, u0 , a), (var(u) = k, var(u0 ) = l) is removed if a 6 Dk0 in an analogous way to the MDD
pruning in the previous subsection. Otherwise, it is labeled with
c(e) = ck (a) +

l1
X

j=k+1

min cj (a0 )

a0 Dj0

(4)

which is the cost of the cheapest assignment to xk , . . . , xl1 consistent with the edge and
the partial assignment . Once the edges are labeled, the upstream and downstream costs
U, D are computed in (|V | + |E|) time, in the same manner as in the previous subsection.
However, computing valid domains has to be extended. As before, a sufficient condition
for a  VD i is the existence of an edge e : u  u0 , originating in the i-th layer u  Vi such
that v(e) = a and
U [u] + c[e] + D[u0 ]  K.
(5)
However, this is no longer a necessary condition, as even if there is no edge satisfying (5),
there could exist a long edge skipping the i-th layer that still allows a  VD i . We therefore,
for each layer i, have to compute the cost of the cheapest path skipping the layer:
P [i] = min{U [u] + c(e) + D[u0 ] | e : u  u0  E, var(u) < i < var(u0 )}

(6)

If there is no edge skipping the i-th layer, we set P [i] = . Let cmin [i] denote the cheapest
value in Di0 , i.e. cmin [i] = minaDi0 ci (a). To determine if there is a long edge allowing
a  VD i , for an unassigned variable xi , the following must hold:
P [i] + ci (a)  cmin [i]  K

(7)

Finally, a sufficient and necessary condition for a  VD i is that one of the conditions (5) and
(7) holds. If variable xi is assigned with a value drawn from a valid domain in a previous
step, we are guaranteed that V Di = {[xi ]} and no calculations are necessary. Labels P [i]
110

fiInteractive Cost Configuration Over Decision Diagrams

Algorithm 4: Update P labels.
Data: MDD M (V, E), Cost function c
P [] = ;
foreach i = 1, . . . , n do
foreach u  Vi do
foreach e : u  u0 do
foreach j  {var(u) + 1, . . . , var(u0 )  1} do
P [j] = min{P [j], U [u] + c(e) + D[u0 ]};

can be computed by Algorithm 4 in worst-case O(|E|  n) time. Note that this bound is
over-pessimistic as it assumes that every edge in |E| is skipping every variable in X.

Once the auxiliary structures U, D, P are computed, valid domains can be efficiently
extracted using Algorithm 5. For each unassigned variable xi , value a  Di is in a valid
domain VDi [K] iff the following holds: condition (7) is satisfied or for an edge e(u, u0 , a)  E
condition (5) is satisfied. For each non-assigned variable i, the algorithm first checks for
each value a  Di whether it is supported by a skipping edge P [i]. Afterwards, it scans the
i-th layer and extracts
P values supported by edges Ei . This is achieved in (|D| + |V | + |E|)
time, where |D| = ni=1 |Di |.
Algorithm 5: Computing valid domains V Di .
Data: MDD M (V, E), cost function C, maximal cost K
foreach i = 1, . . . , n do
V Di = ;
if xi assigned to ai then
V Di  {ai };
continue;
foreach a  Di do
if P [i] + ci (a)  cmin [i]  K then
V Di  V Di  {a};
foreach u  Vi do
foreach e : u  u0 do
if U [u] + c[e] + D[u0 ]  K then
V Di  V Di  {v(e)};

Again, the overall interaction remains the same. Labels P can be incrementally updated
in worst case O(|E|  n) time. Valid domains are then extracted in (|D| + |V | + |E|) time.
In response to changing a cost restriction K, auxiliary labels need not be updated. Valid
domains are extracted directly using Algorithm 5 in (|D| + |V | + |E|) time.
3.4 Handling Non-Additive Cost Functions
In certain interaction settings, the cost function is not additive. For example, user preferences might depend on an entire package of features rather than a selection of each individual
feature. Similarly, the price of a product need not be a simple sum of costs of individual
parts, but might depend on combinations of parts that are selected. In general, our cost
111

fiAndersen, Hadzic, & Pisinger

function c(x1 , . . . , xn ) might be a sum of non-unary cost functions ci , i = 1, . . . , k,
c(x1 , . . . , xn ) =

k
X

ci (Xi )

i=1

where each cost function ci expresses a unique contribution of combination of features within
a subset of variables Xi  X,
Y
Dj  R.
ci :
jXi

3.4.1 Non-Unary Labeling
Our approach can be extended to handle non-unary costs by adopting labeling techniques
that are used with other graphical representations (e.g., Wilson,
2005; Mateescu et al.,
Pk
2008). Assume we are given a cost function c(x1 , . . . , xn ) = i=1 ci (Xi ). Let A(i) denote
the set of all cost functions cj such that xi is the last variable in the scope of cj :
A(i) = {cj | xi  Xj and xi0 6 Xj , i0 > i}.
Given assignment a(a1 , . . . , ai ) to variables x1 , . . . , xi , we can evaluate every function cj 
Ai . If the scope of cj is a strict subset of {x1 , . . . , xi }, we set cj (a) to be the value of
u, u 
cj (Xj (a)) where Xj (a) is a projection of a onto Xj . Now, for every path p : r
Vi+1 , and its last edge (in the i-th layer) e  Ei , we label e with the sum of all cost functions
that have become completely instantiated after assigning xi = ai :
X
cj (p).
(8)
c(e, p) =
cj A(i)

With respect to such labeling, aP
cost of a solution represented by a path p would indeed
be the sum of costs of its edges: ep c(e, p). In order to apply our approach developed for
additive cost functions in Section 3.2, each edge should be labeled with a cost that is the
same for any incoming path. However, this is not possible in general. We therefore have
to expand the original MDD, by creating multiple copies of e and splitting incoming paths
to ensure that any two paths p1 , p2 sharing a copy e0 of an edge e induce the same edge
cost c(e0 , p1 ) = c(e0 , p2 ). Such an MDD, denoted as Mc , can be generated using for example
search with caching isomorphic nodes as suggested by Wilson (2005), or by extending the
standard apply operator to handle weights as suggested by Mateescu et al. (2008).
3.4.2 Impact on the Size
The increase in size of Mc relatively to the cost-oblivious version M depends on the additivity of the cost function c. For example, for fully additive cost functions (each scope
Xi contains a single variable) Mc = M , since a label on c(e) is the same regardless of the
incoming path. However, if the entire cost function c is a single non-additive component
c1 (X1 ) with global scope (X1 = X), then only the edges in the last MDD layer are labeled,
as in the case of explicit cost encoding into MDD from Section 3.1. There must be at least
C(Sol) edges in the last layer, one for each feasible cost. Hence, if the range of costs C(Sol)
112

fiInteractive Cost Configuration Over Decision Diagrams

is exponential, so is the size of Mc . Furthermore, even if C(Sol) is of limited size, an increase in Mc might be significant due to breakup of node isomorphisms in previous layers.
In case of explicit cost encoding (Section 3.1) such an effect is demonstrated empirically in
Section 6. A similar effect on the size would occur in other graphical-representations. For
example, in representations exploiting global CSP structure - such as weighted cluster trees
(Pargamin, 2003) - adding non-additive cost functions increases the size of the clusters, as
it is required that for each non-additive component ci (Xi ) at least one cluster contains the
entire scope Xi . Furthermore, criteria for node merging of Wilson (2005) and Mateescu
et al. (2008) are more refined, since nodes are no longer isomorphic if they root the same
set of feasible paths, but the paths must be of the same cost as well.
3.4.3 Semiring Costs and Probabilistic Queries
Note that our approach can be further generalized to accommodate more general aggregation of costs as discussed by Wilson (2005). Cost functions ci need not map assignments
of Xi variables into the set of real numbers R but to any set A equipped with operators
,  such that A = (A, 0, 1, , ) is a semiring. The MDD property that is computed is
p:r 1 ep c(e). Operator  aggregates edge costs while operator  aggregates path costs.
In a semiring  distributes over , and the global computation can be done efficiently by local node-based aggregations, much as a shortest path is computed. Our framework is based
on reasoning about paths of minimal cost which corresponds to using A = (R+ , 0, 1, min, +)
but different semirings could be used. In particular, by taking A = (R+ , 0, 1, +, ) we can
handle probabilistic reasoning. Each cost function ci corresponds to a conditional probability table, the cost of an edge c(e), e : u  u0  Ei corresponds to the probability
of
Q
P (xi = v(e)) given any of the assignments p : r
u. The cost of a path c(p) = ep c(e)
is a probability of an event represented by the path, and
Pfor a given value a  Di we 0can
get the marginal probability of P (xi = a) by computing e(u,u0 ,a)Ei (U [u]  c(e)  D[u ]).

4. Compiling MDDs
In the previous section we showed how to implement cost queries once the solution space
is represented as an MDD. In this section, we discuss how to generate such MDDs from a
CSP model description (X, D, F ). Our goal is to develop an efficient and easy to implement
approach that can handle all instances handled previously through BDD-based configuration
(Hadzic et al., 2004).
Variable Ordering. The first step is to choose an ordering for CSP variables X. This
is critical since different variable orders could lead to exponential differences in MDD size.
This is a well investigated problem, especially for binary decision diagrams. For a fixed
formula, deciding if there is an ordering such that the resulting BDD would have at most T
nodes (for some threshold T ) is an NP-hard problem (Bollig & Wegener, 1996). However,
there are well developed heuristics, that either exploit the structure of the input model or
use variable swapping in existing BDD to improve the ordering in a local-search manner
(Meinel & Theobald, 1998). For example, fan-in and weight heuristics are popular when the
input is in the form of a combinational circuits. If the input is a CSP, a reasonable heuristic
is to choose an ordering that minimizes the path-width of the corresponding constraint graph,
113

fiAndersen, Hadzic, & Pisinger

as an MDD is in worst case exponential in the path-width (Bodlaender, 1993; Wilson, 2005;
Mateescu et al., 2008). Investigating heuristics for variable ordering is out of the scope of
our work, and in the remainder of this paper we assume that the ordering is already given.
In all experiments we use default orderings provided for the instances.
Compilation Technique. Our approach is to first compile a CSP model into a binary
decision diagrams (BDD) by exploiting highly optimized and stable BDD packages (e.g.,
Somenzi, 1996) and afterwards extract the corresponding MDD. Dedicated MDD packages
are rare, provide limited functionality and their implementations are not as optimized as
BDD packages to offer competitive performance (Miller & Drechsler, 2002). An interesting
recent alternative is to generate BDDs through search with caching isomorphic nodes. Such
an approach was suggested by Huang and Darwiche (2004) to compile BDDs from CNF
formulas, and it proved to be a valuable addition to standard compilation based on pairwise
BDD conjunctions. However, such compilation technology is still in the early stages of
development and an open-source implementation is not publicly available.
4.1 BDD Encoding
Regardless of the BDD compilation method, the finite domain CSP variables X first have
to be encoded by Boolean variables. Choosing a proper encoding is important since the
intermediate BDD might be too large or inadequate for subsequent extraction. In general,
each CSP variable xi would be encoded with ki Boolean variables {xi1 , . . . , xiki }. Each a  Di
has to be mapped into a bit vector enci (a) = (a1 , . . . , aki )  {0, 1}ki such that for different
values a 6= a0 we get different vectors enci (a) 6= enci (a0 ). There are several standard Boolean
encodings of multi-valued variables (Walsh, 2000). In the log encoding scheme each xi is
encoded with ki = dlog|Di |e Boolean variables, each representing a digit in binary notation.
A multivalued assignment xi = a is translated into a set of assignments xij = aj such
P i j1 i
Pki j1
2 xj < |Di | is added
aj . Additionally, a domain constraint kj=1
that a =
j=1 2
i
i
to forbid those bit assignments (a1 , . . . , aki ) that encode values outside domain Di . The
direct encoding (or 1-hot encoding) is also common, and especially well suited for efficient
propagation when searching for a single solution. In this scheme, each multi-valued variable
xi is encoded with |Di | Boolean variables {xi1 , . . . , xiki }, where each variable xij indicates
whether the j-th value in domain aj  Di is assigned. For each variable xi , exactly one
value from Di has to be assigned. Therefore, we enforce a domain constraint xi1 +. . .+xiki = 1
for each i = 1, . . . , n. Hadzic, Hansen, and OSullivan (2008) have empirically demonstrated
that using log encoding rather than direct encoding yields smaller BDDs.
Sn Thei set of i Boolean variables is fixed as the union of all encoding variables, Xb =
i=1 {x1 , . . . , xki } but we still have to specify the ordering. A common ordering that
is well suited for efficiently answering configuration queries is clustered ordering. Here,
Boolean variables {xi1 , . . . , xiki } are grouped into blocks that respect the ordering among
finite-domain variables x1 < . . . < xn . That is,
xij11 < xij22  i1 < i2  (i1 = i2  j1 < j2 ).
There might be other orderings that yield smaller BDDs for specific classes of constraints.
Bartzis and Bultan (2003) have shown that linear arithmetic constraints can be represented
114

fiInteractive Cost Configuration Over Decision Diagrams

more compactly if Boolean variables xij are grouped wrt. bit-position j rather than the
finite-domain variable xi , i.e. xij11 < xij22  j1 < j2  (j1 = j2  i1 < i2 ). However,
configuration constraints involve not only linear arithmetic constraints, and space savings
reported by Bartzis and Bultan (2003) are significant only when all the variable domains
have a size that is a power of two. Furthermore, clustered orderings yield BDDs that
preserve essentially the same combinatorial structure which allows us to extract MDDs
efficiently as will be seen in Section 4.2.
Example 3 Recall that in the T-shirt example D1 = {0, 1, 2, 3}, D2 = {0, 1, 2}, D3 =
{0, 1}. The log encoding variables are x11 < x12 < x21 < x22 < x31 , inducing a variable set
Xb = {1, 2, 3, 4, 5}. The log-BDD with clustered variable ordering is shown in Figure 4(a).

x1

x1
x1

0 1 2 3

x2

x2

x2

x2

x2

x2

0
x2

x3
x3

x3

2 1
0

x3
1

1

1

(a) A log-BDD.

12

x2

(b) An extracted MDD.

Figure 4: A log-BDD with clustered ordering, and an extracted MDD for the T-shirt example. For BDD, we draw only the terminal node 1 while terminal node 0 and
its incoming edges are omitted for clarity. Each node corresponding to a Boolean
encoding variable xij is labeled with the corresponding CSP variable xi . Edges
labeled with 0 and 1 are drawn as dashed and full lines, respectively.

4.2 MDD Extraction
Once the BDD is generated using clustered variable ordering we can extract a corresponding
MDD using a method which was originally suggested by Hadzic and Andersen (2006) and
that was subsequently expanded by Hadzic et al. (2008). In the following considerations,
we will use a mapping cvar(xij ) = i to denote the CSP variable xi of an encoding variable
xij and, with a slight abuse of notation, we will apply cvar also to BDD nodes u labeled
with xij . For terminal nodes, we define cvar(0) = cvar(1) = n + 1 (recall that BDD has two
terminal nodes 0 and 1 indicating false and true respectively). Analogously, we will use a
mapping pos(xij ) = j to denote the position of a bit that the variable is encoding.
Our method is based on recognizing a subset of BDD nodes that captures the core of
the MDD structure, and that can be used directly to construct the corresponding MDD.
115

fiAndersen, Hadzic, & Pisinger

In each block of BDD layers corresponding to a CSP variable xi , Li = Vxi  . . .  Vxi , it
1
ki
suffices to consider only those nodes that are reachable by an edge from a previous block of
layers:
Ini = {u  Li | (u0 ,u)E cvar(u0 ) < cvar(u)}.

0
0
For the first layer
Sn+1we take In1 = {r}. The resulting MDD M (V , E ) M contains only nodes
0
in Ini , V = i=1 Ini and is constructed using extraction Algorithm 6. An edge e(u, u0 , a)
is added to E 0 whenever traversing BDD B from u wrt. encoding of a ends in u0 6= 0.
Traversals are executed using Algorithm 7. Starting from u, in each step the algorithm
traverses BDD by taking the low branch when corresponding bit ai = 0 or high branch
when ai = 1. Traversal takes at most ki steps, terminating as soon as it reaches a node
labeled with a different CSP variable. The MDD extracted from a log-BDD in Figure 4(a)
is shown in Figure 4(b).

Algorithm 6: Extract MDD.
Data: BDD B(V, E)
E 0  {},V 0  {r};
foreach i = 1, . . . , n do
foreach u  Ini do
foreach a  Di do
1
u0  Traverse(u, a);
if u0 6= 0 then
E 0  E 0  {(u, u0 , a)};
V 0  V 0  {u0 }
return (V 0 , E 0 );

Algorithm 7: Traverse BDD.
Data: BDD B(V, E), u, a
i  cvar(u);
(a1 , . . . , aki )  enci (v);
repeat
s  pos(u);
if as = 0 then
u  low(u);
else
u  high(u);
until cvar(u) 6= i ;
return u;

Since each traversal (in lineP
1 of Algorithm 6) takes O(dlog|Di |e) steps, the running time
for the MDD extraction
is O( ni=1 |Ini |  |Di |  dlog|Di |e). The resulting MDD M (V 0 , E 0 )
Pn
has at most O( i=1 |Ini |  |Di |) edges because we
Pnadd at most |Di | edges for every node
0
u  Ini . Since we keep only nodes in Ini , |V | = i=1 |Ini |  |V |.
4.3 Input Model and Implementation Details

An important factor for usability of our approach is the easiness of specifying the input
CSP model. BDD packages are callable libraries with no default support for CSP-like input
language. To the best of our knowledge, the only open-source BDD-compilation tool that
116

fiInteractive Cost Configuration Over Decision Diagrams

accepts as an input a CSP-like model is CLab (Jensen, 2007). It is a configuration interface
on top of a BDD package BuDDy (Lind-Nielsen, 2001). CLab constructs a BDD for each
input constraint and conjoins them to get the final BDD. Furthermore CLab generates a
BDD using log-encoding with clustered ordering which suits well our extraction approach.
Therefore, our compilation approach is based on using CLab to specify the input model and
generate a BDD that will be used by our extraction Algorithm 6.
Note that after extracting the MDD, we preprocess it for efficient online querying.
We expand the long edges and merge isomorphic nodes to get a merged MDD. We then
translate it into a more efficient form for online processing. We rename BDD node names
to indexes from 0, . . . , |V |, where root has index 0 and terminal 1 has index |V |. This
allows for subsequent efficient implementation of U and D labels, as well as an efficient
access to children and parent edges of each node. In our initial experiments we got an order
of magnitude speed-up of wCVD queries after we switched from BDD node names (which
required using less efficient mapping for U , D, Ch and P structures).

5. Interactive Configuration With Multiple Costs
In a number of domains, a user should configure in the presence of multiple cost functions
which express often conflicting objectives that a user wants to achieve simultaneously. For
example, when configuring a product, a user wants to minimize the price, while maximizing the quality, reducing the ecological impact, shortening delivery time etc. We assume
therefore that in addition to the CSP model (X, D, F ) whose solution space is represented
by a merged MDD M , we are given k additive cost functions
ci (x1 , . . . , xn ) =

n
X

cij (xi ), i = 1 . . . , k

j=1

expressing multiple objectives. Multi-cost scenarios are often considered within the multicriteria optimization framework (Figueira et al., 2005; Ehrgott & Gandibleux, 2000). It
is usually assumed that there is an optimal (but unknown) way to aggregate multiple
objectives into a single objective function that would lead to a solution that achieves the
best balance in satisfying various objectives. The algorithms sample few efficient solutions
(nondominated wrt. objective criteria) and display them to the user. Through user input,
the algorithms learn how to aggregate objectives more adequately which is then used for
the next sampling of efficient solutions etc. In some approaches a user is asked to explicitly
assign
Pkweights wi to objectives ci which are then aggregated through weighted summation
c = i=1 wi ci .
While adopting these techniques to run over a compiled representation of solution space
would immediately improve their complexity guarantees and would be useful in many scenarios where multi-criteria techniques are traditionally used, we believe that in a configuration
setting, a more explicit control over variable values is needed. A user should easily explore
the effect of assigning various variable values on other variables as well as cost functions.
We therefore suggest to directly extend our wCVD query so that a user could explore the
effect of cost restrictions in the same way he explores interactions between regular variables. The key query that we want to deliver is computing valid domains wrt. multiple cost
restrictions:
117

fiAndersen, Hadzic, & Pisinger

Definition 5 (k-wCVD) Given a CSP model (X, D, F ), additive cost functions cj : D  R,
and maximal costs Kj , j = 1, . . . , k, for a given partial assignment , compute:
VD i [, {Kj }kj=1 ] = {a  Di | 0 .(0 |= F and   {(xi , a)}  0 and

k
^

j=1

cj (0 )  Kj )}

We are particularly interested in two-cost configuration as it is more likely to occur
in practice and has strong connections to existing research in solving Knapsack problems
and multi-criteria optimization. In the reminder of the section we will first discuss the
complexity of 2-wCVD queries and then develop a practical implementation approach. We
will then discuss the general k-wCVD query.
5.1 Complexity of 2-wCVD query
We assume that as an input to the problem we have a merged MDD M , additive cost
functions c1 , c2 and cost bounds K1 , K2 . The first question is whether it is possible for
some restricted forms of additive cost functions c1 , c2 to implement 2-wCVD in polynomial
time. For this purpose we formulate a decision-version of the 2-wCVD problem:
Problem 1 (2-wCVD-SAT) Given CSP (X, D, F ) and MDD
PnM representation of its solution space, and given two additive cost functions ci (x) = j=1 cij (xj ), i = 1, 2 with cost
restrictions K1 , K2 , decide whether F  c1 (x)  K1  c2 (x)  K2 is satisfiable.
Unfortunately, the answer is no even if both constraints involve only positive coefficients,
and have binary domains. To show this we reduce from the well-known Two-Partition
Problem (TPP) which is NP-hard (Garey & Johnson, 1979). For a given set of positive
integers S = {s1 , . . . , sn }, the TPP asks to decide whether it is possible to split a set of
indexes
I =P{1, . . . , n} into two sets A and I \ A such that the sum in each set is the same:
P
s
iI\A si .
iA i =

Proposition 3 The 2-wCVD-SAT problem defined over Boolean variables and involving only
linear cost functions with positive coefficients is NP-hard.

Proof 3 We show the stated by reduction from TPP. In order to reduce TPP to two-cost
configuration we introduce 2n binary variables x1 , . . . , x2n such that i  A if and only
if x2i1 = 1 and i  A \ I if and only if x2i = 1. We construct an MDD for F =
{x1 6= x2P
, . . . , x2n1 6= x2n } and introduce
P two linear cost functions with positive coefficients,
c1 (x) = ni=1 si P
x2i1 and c2 (x) = ni=1 si  x2i . The overall capacity constraints are set
to K1 = K2 =
iI si /2. By setting A = {i  I | x2i1 = 1} it is easily seen that
F  c1 (x)  K1  c2 (x)  K2 is satisfiable if and only if the TPP has a feasible solution.
Hence, if we were able to solve 2-wCVD-SAT with Boolean variables and positive linear cost
functions in polynomial time, we would also be able to solve the TPP problem polynomially.
5.2 Pseudo-Polynomial Scheme for 2-wCVD
In the previous subsection we demonstrated that answering 2-wCVD queries is NP-hard even
for the simplest class of positive linear cost functions over Boolean domains. Hence, there
118

fiInteractive Cost Configuration Over Decision Diagrams

is no hope of solving 2-wCVD with guaranteed polynomial execution time unless P = N P .
However, we still want to provide a practical solution to the 2-wCVD problem. We hope
to avoid worst-case performance by exploiting the specific nature of the cost-functions we
are processing. In this subsection we therefore show that 2-wCVD can be solved in pseudopolynomial time by extending our labeling approach from Section 3.2. Furthermore, we
show how to adopt advanced techniques used for the Knapsack problem (Kellerer, Pferschy,
& Pisinger, 2004).
5.2.1 Overall Approach
Our algorithm runs analogous to the single-cost approach developed in Section 3.2. After
restricting the MDD wrt. a current assignment, we calculate upstream and downstream
costs U, D (which are no longer constants but lists of tuples), and use them to check for
each edge e, whether v(e) is in a valid domain.
For a given edge e : u  u0 , labeled with costs c1 (e), c2 (e), it follows v(e)  V Di iff
there are paths p : r
u, and p0 : u0
1 such that c1 (p) + c1 (e) + c1 (p0 )  K1 and
0
c2 (p) + c2 (e) + c2 (p )  K2 . At each node u it suffices to store two sets of labels:
U [u] = {(c1 (p), c2 (p)) | p : r

u}

D[u] = {(c1 (p), c2 (p)) | p : u

1}

Then, for given cost restrictions K1 , K2 , and an edge e : u  u0 , u  Vi , domain V Di [K1 , K2 ]
contains v(e) if for some (a1 , a2 )  U [u] and (b1 , b2 )  D[u] it holds
a1 + c1 (e) + b1  K1  a2 + c2 (e) + b2  K2

(9)

5.2.2 Exploiting Pareto Optimality
While in the single-cost case it was sufficient to store at U [u], D[u] only the minimal value
(the cost of the shortest path to root/terminal), in multi-cost case we need to store multiple
tuples. The immediate extension would require storing at most K1  K2 tuples at each node.
However, we need to store only non-dominated tuples in U and D lists. If there are two
tuples (a1 , a2 ) and (a01 , a02 ) in the same list such that
a1  a01 and a2  a02
then we may delete (a01 , a02 ) as if test (9) succeeds for (a01 , a02 ) it will also succeed for (a1 , a2 ).
The remaining entries are the costs of pareto-optimal solutions. A solution is pareto-optimal
wrt. solution set S and cost functions c1 , c2 if it is not possible to find a cheaper solution
in S with respect to one cost without increasing the other. Path p : r
1 represents a
pareto-optimal solution in Sol iff for each node u on the path, both sub-paths p1 : r
u
and p2 : u
1 are pareto-optimal wrt. the sets of paths {p : r
u} and {p : u
1}
respectively. Hence, for each node u it suffices to store:
U [u] = {(c1 (p), c2 (p)) | p : r

u, p0 :r

u (c1 (p)

 c1 (p0 )  (c2 (p)  c2 (p0 ))}

D[u] = {(c1 (p), c2 (p)) | p : u

1, p0 :u

1 (c1 (p)

 c1 (p0 )  (c2 (p)  c2 (p0 ))}

119

fiAndersen, Hadzic, & Pisinger

Note that due to pareto-optimality, for each a1  {0, . . . , K1 } and each a2  {0, . . . , K2 }
there can be at most one tuple in U or D where the first coordinate is a1 or the second
coordinate is a2 . Therefore, for each node u, U [u] and D[u] can have at most min{K1 , K2 }
entries. Hence, the space requirements of our algorithmic scheme are in worst case O(|V |K)
where K = min{K1 , K2 }.
5.2.3 Computing U and D Sets
We will now discuss how to compute the U and D sets efficiently by utilizing advanced
techniques for solving Knapsack problems (Kellerer et al., 2004). We recursively update U
and D sets in a layer by layer manner as shown in Algorithm 8. The critical component of
each recursion step in the algorithm is merging lists in lines 2 and 4. In this operation a
new list is formed such that all dominated tuples are detected and eliminated. In order to
do this efficiently, it is critical to keep both U and D lists sorted wrt. the first coordinate,
i.e.
(a1 , a2 )  (a01 , a02 )  a1 < a2 .
If U and D are sorted, they can be merged in O(K) time using the list-merging algorithm
for Knapsack optimization from (Kellerer et al., 2004, Section 3.4).
Algorithm 8: Update U, D labels.

1
2

3
4

Data: MDD M , Cost functions c1 , c2 , Bounds K1 , K2
U [] = {(, )}, U [r] = {(0, 0)};
foreach i = 1, . . . , n do
foreach u  Vi do
foreach e : u  u0 do
S  ;
foreach (a1 , a2 )  U [u] do
if a1 + c1 (e)  K1  a2 + c2 (e)  K2 then
S  S  (a1 + c1 (e), a2 + c2 (e));
U [u0 ]  M ergeLists(S, U [u0 ]);
D[] = {(, )}, D[1] = {(0, 0)};
foreach i = n, . . . , 1 do
foreach u  Vi do
foreach e : u  u0 do
S  ;
foreach (a1 , a2 )  D[u0 ] do
if a1 + c1 (e)  K1  a2 + c2 (e)  K2 then
S  S  (a1 + c1 (e), a2 + c2 (e));
D[u]  M ergeLists(S, D[u]);

The time complexity is determined by populating list S (in lines 1 and 3) and merging
(in lines 2 and 4). Each of these updates takes O(K) in worst case. Since we perform these
updates for each edge e  E, the total time complexity of Algorithm 8 is O(|E|  K) in the
worst case.
120

fiInteractive Cost Configuration Over Decision Diagrams

5.2.4 Valid Domains Computation
Once the U, D sets are updated we can extract valid domains in a straightforward manner
using Algorithm 9. For each edge e : u  u0 the algorithm evaluates whether v(e)  V Di in
worst case O(|U [u]|  |D[u0 ]|) = O(K 2 ) steps. Hence, valid domain extraction takes in worst
case O(|E|  K 2 ) steps.
Algorithm 9: Compute valid domains.
Data: MDD M , Cost functions c1 , c2 , Cost bounds K1 , K2 , Labels U ,D
foreach i = 1, . . . , n do
VDi  ;
foreach u  Vi do
foreach e : u  u0 do
foreach (a1 , a2 )  U [u], (b1 , b2 )  D[u0 ] do
if a1 + c1 (e) + b1  K1  a2 + c2 (e) + b2  K2 then
VDi  VDi  {v(e)};
break;

However, we can improve the running time of valid domains computation by exploiting
(1) pareto-optimality and (2) the fact that the sets U, D are sorted. It is critical to observe
that given an edge e : u  u0 , for each (a1 , a2 )  U [u] it suffices to perform the validity
test (9) only for a tuple (b1 , b2 )  D[u0 ], where b1 is a maximal first coordinate satisfying
a1 + c1 (e) + b1  K1 , i.e.
b1 = max{b1 | (b1 , b2 )  D[u0 ], a1 + c1 (e) + b1  K1 }.

Namely, if the test succeeds for some (b01 , b02 ) where b01 < b1 , it will also succeed for (b1 , b2 )
since due to pareto-optimality, b01 < b1  b2 < b02 and hence a2 +c2 (e)+b2 < a2 +c2 (e)+b02 
K2 . Since the lists are sorted, comparing all relevant tuples can be performed efficiently by
traversing U [u] in increasing order, while traversing D[u0 ] in decreasing order. Algorithm
10 implements the procedure.
Algorithm 10: Extract edge value.

1
2

Data: MDD M , Cost constraints c1 , c2 , Bounds K1 , K2 , Edge e : u  u0 in Ei
a(a1 , a2 ) = U [u].begin();
b(b1 , b2 ) = D[u0 ].end();
while a 6= >  b 6=  do
if a1 + c1 (e) + b1 > K1 then
b(b1 , b2 )  D[u0 ].previous();
continue;
else if a1 + c1 (e) + b1  K1  a2 + c2 (e) + b2  K2 then
VDi  VDi  {v(e)};
return;
a(a1 , a2 )  U [u].next();

The algorithm relies on several list operations. Given list L of sorted tuples, operations
L.begin() and L.end() return the first and the last tuple respectively wrt. the list ordering.
121

fiAndersen, Hadzic, & Pisinger

Operations L.next() and L.previous() return the next and the previous element in the
list wrt. the ordering. Elements > and  indicate two special elements that appear after
the last and before the first element in the list respectively. They indicate that we have
passed beyond the boundary of the list. The algorithm terminates (line 2) as soon as the
test succeeds. Otherwise, it keeps iterating over tuples until we have processed either the
last tuple in U [u] or the first tuple in D[u0 ]. In that case the algorithm terminates as
it is guaranteed that v(e) 6 V Di . In each step, we traverse at least one element from
U [u] or D[u0 ]. Hence, in total we can execute at most U [u] + D[u0 ]  2K operations.
Therefore, the time complexity of single edge traversal is O(K) and the complexity of valid
domains computation of Algorithm 9 (after replacing the quadratic loop with Algorithm
10) is O(|E|  K) where K = min{K1 , K2 }.
In conclusion, we have developed a pseudo-polynomial scheme for computing valid domains wrt. two cost functions (2-wCVD). The space complexity is dominated by storing U
and D sets at each node. In worst case we have to store O(|V |  K) entries. The time
complexity to compute U and D labels and extract valid domains takes O(|E|  K) steps.
The overall interaction is similar to the single-cost approach. After assigning a variable,
we have to recompute the labels as well as extract domains. If we tighten cost restrictions
K1 , K2 to K10  K1 , K20  K2 we only need to extract domains. However, if we relax either
of the cost restrictions, such as K10 > K1 we need to recompute the labels as well. More
precisely, labels U, D need to be recomputed only if K1 > K1max where K1max was the initial
cost restriction after the last assignment.
5.2.5 Further Extensions
Note that our approach can, in principle, be extended to handle general k-wCVD query for a
fixed k. Lists U and D would contain the set of non-dominated k-tuples, ordered such that:
(a1 , . . . , ak )  (a01 , . . . , a0k ) iff for the smallest coordinate j for which aj 6= a0j it holds aj < a0j .
Both the list merging as well as valid domains extraction would be directly generalized to
operate over such ordered sets, although the time complexity for testing dominans will
increase. The worst-case complexity would depend on the size of an efficient frontier, which
for k cost functions with cost bounds K is bounded by O(K k1 ). In practice however, we
could expect that the number of non-dominated tuples be much smaller, especially for cost
functions over smaller scopes and with smaller coefficients. Note that our approach can
also be extended to accommodate non-additive cost functions by expanding the MDD to
accommodate non-unary labels in the same fashion as discussed in Section 3.4.
5.3 Approximation Scheme for 2-wCVD
In this subsection we analyze the complexity of answering 2-wCVD queries in approximative
manner, i.e. how can we improve running time guarantees by settling for an approximate
solution. Assume that one of the constraints K2 is fixed while the second constraint may be
exceeded with a small tolerance (1+)K1 . For example, a user might be willing to tolerate a
small increase in price as long as strict quality restrictions are met. In this section we present
a fully polynomial time approximation scheme (FPTAS) for calculating valid domains in
time O(En 1 ) for this problem. The FPTAS should satisfy that no feasible solution with
respect to the original costs should be fathomed, and that any feasible configuration found
122

fiInteractive Cost Configuration Over Decision Diagrams

by use of the FPTAS in the domain restriction should satisfy the cost constraint within
(1 + )K1 . Finally, the FPTAS should have running time polynomial in 1/ and the input
size.
In order to develop the FPTAS we use a standard scaling technique (Schuurman &
Woeginger, 2005) originally presented by Ibarra and Kim (1975). Given an , let n be
the number of decision variables. Set T = K1 /(n + 1) and determine new costs c1 (e) =
bc1 (e)/T c and new bounds K1 = dK1 /T e. We then perform the valid domains computation
(label updating and domain extraction) as described in Section 5.2, using the scaled weights.
The following propositions prove that we obtained a FPTAS scheme.
Proposition 4 The running time of valid domains computation is O( 1 En)
Proof 4 We may assume that K1 < K2 as otherwise we may interchange the two costs.
The running time becomes
1
n+1
) = O(En )
O(E K1 ) = O(EK1 /T ) = O(EK1
K1

since n  V this is polynomial in the input size O(V + E) and the precision 1 .
Proposition 5 If a solution was feasible with respect to the original costs, then it is also
feasible with respect to the scaled costs.
P
Proof 5 Assume that ep c1 (e)  K1 . Then
X
X
1
1
1X
c1 (e)  K1  d K1 e = K1
c1 (e) =
bc1 (e)/T c 
T ep
T
T
ep
ep
Proposition 6 Any solution that was feasible with respect to the scaled costs c1 (e) satisfies
original constraints within (1 + )K1 .
P
Proof 6 Assume that ep c1 (e)  C1 . Then
P
P
P
P
ep c1 (e) + T n
ep (bc1 (e)/T c + 1)  T
ep c1 (e)/T  T
ep c1 (e) = T
 T K1 + T n = T dK1 /T e + T n  T (K1 /T + 1) + T n

= K1 + T (n + 1)

Since T = K1 /(n + 1) we get
X
c1 (e)  K1 + (n + 1)K1 /(n + 1) = (1 + )K1
ep

which shows the stated.

The time complexity can be further improved using techniques from Kellerer et al. (2004)
for the Knapsack Problem, but we are here only interested in showing the existence of a
FPTAS.
By the considerations in previous subsections we have fully analyzed the complexity
of answering 2-wCVD queries. We first showed that this is an NP-hard problem. We then
developed a pseudo-polynomial scheme for solving it, and finally we devised a fully polynomial time approximation scheme. Even though we cannot provide polynomial running-time
guarantees, based on these considerations, we can hope to provide a reasonable performance
for practical instances, as it will be demonstrated in Section 6.
123

fiAndersen, Hadzic, & Pisinger

5.4 Complexity of k-wCVD Query
We conclude this section by discussing complexity of general k-wCVD queries. While our
practical implementation efforts are focused on implementing 2-wCVD queries, or other wCVD
queries where the number of cost constraints is known in advance, for completeness we
consider a generic problem of delivering k-wCVD for arbitrary k, i.e. where k is part of the
input to the problem.
We will prove now that for such a problem there is no pseudo-polynomial scheme unless
NP=P. We will show that decision version of such problem k-wCVD-SAT is NP-hard in the
strong sense (Garey & Johnson, 1979) by reduction from the bin-packing problem (BPP)
which is strongly NP-hard (Garey & Johnson, 1979). In the decision form the BPP asks
whether a given set of numbers s1 , . . . , sn can be placed into k bins of size K each. Notice,
that we cannot use reduction below for showing NP-hardness of 2-wCVD-SAT, since k is a
part of the input in BPP.
Theorem 7 The k-wCVD-SAT problem with variable k, is strongly NP-hard.
Proof 7 For a given instance of BPP we reduce it to a k-wCVD-SAT instance as follows:
We construct an MDD for a CSP (X, D, F ) over n variables X = {x1 , . . . , xn } each with
a domain of size k, Di = {1, . . . , k}, i = 1, . . . , n. We set F = , so that resulting MDD
allows all assignments. It has n nonterminal nodes u1 , . . . , un corresponding to the numbers
s1 , . . . , sn . Between two nodes ui , ui+1 we have k edges with costs (c1 (e), c2 (e), . . . , ck (e))
set to
(si , 0, . . . , 0), (0, si , 0, . . . , 0), (0, 0, si , 0, . . . , 0), . . . , (0, . . . , si ),
The first node u1 is the root u1 = r while the last node un is connected to the terminal
un+1 = 1. The overall capacity constraints are (K1 , . . . , Kk ) = (K, . . . , K).
It is easily seen that we may find a path from r to 1 if and only if the BPP has a
feasible solution. Since the BPP is strongly NP-hard we have shown that k-wCVD-SAT also
is strongly NP-hard.

6. Experimental Evaluation
We implemented our compilation scheme and the algorithms for wCVD and 2-wCVD queries.
We performed a number of experiments to evaluate the applicability of our approach as
well as to confirm various hypotheses made throughout the paper. We used two sets of
instances whose properties are presented in Table 1. The first set corresponds to real-world
configuration problems available at configuration benchmarks library CLib2 . These are CSP
models with configuration constraints. They correspond to highly structured configuration
problems with a huge number of similar solutions. The second set of instances represents
product-catalogue datasets used by Nicholson, Bridge, and Wilson (2006). These catalogues
are defined explicitly, as tables of solutions. They represent a much smaller and sparser set
of solutions.
2. http://www.itu.dk/research/cla/externals/clib/

124

fiInteractive Cost Configuration Over Decision Diagrams

Instance
ESVS
FS
Bike2
PC2
PC
Big-PC
Renault
Travel
Laptops
Cameras
Lettings

Sol
231
224
226
220
220
283
241
1461
683
210
751

X
26
23
34
41
45
124
99
7
14
9
6

dmin
2
2
2
4
2
2
2
4
2
5
2

dmax
61
51
38
34
33
66
42
839
438
165
174

davg
5
5
6
9
8
12
4
134
42
40
45

Table 1: First seven instances are real-world configuration problems available at configuration benchmarks library CLib. Remaining four instances are product catalogues
used by Nicholson et al. (2006). For each instance we provide the number of solutions Sol, number of variables X, the minimal, maximal and average domain
size.

6.1 MDD Size
In the first set of experiments, for each instance we generated a log-encoded BDD B using
CLab (Jensen, 2007). We then extracted a corresponding MDD M from B. Finally, we
expanded long edges in M and merged isomorphic nodes to generate a merged MDD M 0 .
We compare the sizes of B, M and M 0 in Table 2. For each structure we provide the number
of nodes V and edges E. We also provide the size of the BDD B. We conclude from the
table that both BDDs and MDDs are exponentially smaller than the size of the solution
space for configuration instances while not as significantly smaller for more diverse product
configuration catalogues. Furthermore, we can see that the number of edges in merged
MDDs M 0 is not significantly larger in comparison to extracted MDDs M . Hence, due to
simpler online algorithms, using merged MDDs seems well suited for online reasoning. We
can also see that multi-valued encoding in many cases reduces the number of nodes and
edges in comparison to BDDs. Even though compilation times are less important since the
generation of the MDD is performed offline, it is worth noting that for the largest instance,
Renault, it took around 2min and 30sec to compile the instance into a BDD and extract an
MDD.
6.1.1 Encoding Cost Explicitly
We also investigated the impact of encoding cost information explicitly into an MDD.
For each instance we compared the size of the MDD without and with cost variables (M
and M c respectively). For configuration
benchmarks we introduce an additional variable
P
y  [0, 10000] such that y = ni=1 ai xi where coefficients ai are randomly drawn from the
interval [0, 50]. We put variable y as the last in the ordering since for other positions we
get MDDs of similar size, and putting y at the end allows easier theoretical analysis. Since
125

fiAndersen, Hadzic, & Pisinger

Instance
ESVS
FS
Bike2
PC2
PC
Big-PC
Renault
Travel
Laptops
Cameras
Lettings

VB
306
3,044
3,129
13,332
16,494
356,696
455,796
8479
9528
4274
2122

EB
612
6,088
6,258
26,664
32,988
713,392
911,592
16,958
19,056
8,548
4,244

KB
5
41
56
237
298
7,945
9,891
154
172
71
36

VM
87
753
853
3,907
4875
100,193
283,033
1469
2033
791
351

EM
202
1,989
1,726
6,136
7989
132,595
334,008
2928
2713
999
1099

VM0
96
767
933
3907
4875
100,272
329,135
1469
2033
791
351

EM0
220
2017
1886
6136
7989
132,889
426,212
2928
2713
999
1099

Table 2: Comparison between BDDs and MDDs for instances from Table 1. The second,
third and fourth column give the number of non-terminal BDD nodes VB , the
number of edges EB and the size on disk of the BDD in kilobytes KB. The fifth
and the sixth column give the number of vertices VM and edges EM in an MDD
M extracted from the BDD using Algorithm 6 on page 116. The final two columns
provide the number of nodes and edges in a merged MDD (M 0 ) where all long
edges from extracted MDD M have been expanded.

product catalogues already contain the cost variable y (price), we produce a cost-oblivious
version M by existentially quantifying y, M = y M c .
In Table 3 we compare the MDDs M and M c . For both structures we provide the number
of edges as well as the representation size in kilobytes. We also show the size of cost range
C(Sol). We observe that for configuration instances that have a high level of sharing and
compression, introducing cost information explicitly induces an order of magnitude increase
in size even when the cost range C(Sol) is limited (400 times increase for Bike2 instance).
MDDs for the two largest instances could not be generated. However, for product catalogues
which have much less sharing, removing cost information does not have a dramatic effect.
In the worst case, the number of edges in M c is two times larger than in M . Hence, the
experimental results confirm that introducing cost explicitly could have a dramatic effect
for MDD representations of highly compressed solution spaces, usually implicitly defined
through conjunction of combinatorial constraints. However, the effect of adding explicit cost
information might be modest when the solution space is defined explicitly, as a (sparse) list
of database entries, such as the case for product catalogues. Furthermore, the size of the
cost range C(Sol) needs not be significant for a large increase in size to take place.
6.2 Response Times for wCVD Queries
In the second set of experiments, we evaluated the performance of wCVD queries over merged
MDD representations of configuration instances. We report the running times for both
computing U and D labels (Algorithm 2) as well as computing valid domains (Algorithm 3).
In Table 4 we report both average and worst-case running times over initial merged MDDs
126

fiInteractive Cost Configuration Over Decision Diagrams

Instance
ESVS
FS
Bike2
PC2
PC
Big-PC
Renault
Travel
Laptops
Cameras
Lettings

E
202
1,989
1,726
6,136
7,989
132,595
334,008
1640
1592
725
496

KB
5
41
56
237
298
7,945
9,891
45
80
44
9

Ec
129,514
407,662
693,824
1,099,842
1,479,306
2928
2713
999
1099

KB
4,408
12,767
31,467
57,909
70,900
154
172
71
36

C(Sol)
1,966
1,497
3,008
2,000
2,072
839
438
165
174

Table 3: Effect of explicitly encoding cost information. The second and third column indicate the number of edges and the representation size in kilobytes for cost-oblivious
MDD, while the fourth and fifth column show the same for the MDD containing
cost information. Column C(Sol) indicates the range of available costs over all
solutions.

from Table 2. We also report the time necessary to restrict the MDD wrt. an assignment
(Algorithm 1). We randomly create an additive cost function c by assigning for each variable
xi and each value a  Di a cost ci (a) from [0, 50]. Valid domains are computed wrt. the
maximal cost restriction K that is set to a value larger than the the length of the longest
MDD path wrt. cost function c. This ensures the longest execution time of Algorithm 3.
Each data-point in the table is an average or a maximum over 1000 executions on a Fedora
9 operating system, using dual Quad core Intel Xeon processor running at 2.66 GHz. Only
one core is used for each instance. Empirical evaluation demonstrates that response times
are easily within acceptable interaction bounds even for the largest instances, where in worst
case the MDD nodes are labeled within 0.13 seconds, valid domains are computed within
0.07 seconds and MDD is restricted wrt. an assignment within 0.28 seconds.
6.3 Response Times for 2-wCVD Query
We generated analogous statistics for 2-wCVD in Table 5. We tested the performance of
our algorithms under the computationally most demanding circumstances: we operate over
the original (fully-sized) MDD, even though during interaction it would be reduced due
to user assignments. Furthermore, both cost functions c1 , c2 have a global scope, and we
use no cost restrictions when computing U and D labels (i.e. we ignore the condition in
line 1 of Algorithm 10 and hence, U [1] and D[r] correspond to an entire efficient frontier).
Normally, cost functions would involve only a subset of variables and only a fraction of the
labels on the efficient frontier (within restrictions K1 , K2 ) would be relevant for the user. We
generate cost functions c1 , c2 by drawing costs ci (a) randomly from [0, 50]. For computing
valid domains, we use restrictions K1 , K2 larger than the lengths of corresponding longest
127

fiAndersen, Hadzic, & Pisinger

Instance
ESVS
FS
Bike2
PC2
PC
Big-PC
Renault

Labeling U, D
avg
max
0.0001
0.01
0.0001
0.01
0.0002
0.01
0.0002
0.01
0.0003
0.01
0.0210
0.04
0.0590
0.13

Valid domain
avg
max
0.0001
0.01
0.0001
0.01
0.0001
0.01
0.0002
0.01
0.0003
0.01
0.0110
0.03
0.0310
0.07

Restrict
avg
max
0.0001 0.01
0.0002 0.01
0.0010 0.01
0.0010 0.02
0.0010 0.01
0.0400 0.08
0.1600 0.28

Table 4: Interaction time in seconds for wCVD queries. We report time required for computing U and D labels, valid domain computation and restriction wrt. a single
assignment.

paths, so that all possible solutions in the efficient frontier are allowed. This would lead to
the longest execution time of Algorithm 9.
Our algorithms can easily handle the first five instances. For the largest two instances,
if U and D labels are known, calculating valid domains can be done within a fraction of a
second. Hence, a user can efficiently explore the effect of various cost restrictions K1 , K2 wrt.
a fixed partial assignment. After a user assigns a variable, recomputing U and D labels
takes in total on average less than 0.85 seconds, or in worst case less than 1.4 seconds.
While this is already within acceptable interaction times, the usability of the system can be
further enhanced, e.g. by using a layered display of information: always reacting with the
information that is fastest to compute (such as CVD or wCVD), and while the user is analyzing
it, execute more time consuming operations. In particular, the entire efficient frontier is
known as soon as U labels are generated  in worst case within 0.64 seconds. At this
stage, a user can explore the cost-space while D labels are computed (on average within
the next 0.79 seconds). Note that the running times can be reduced through a number of
additional schemes, e.g. by computing U and D labels in parallel, if two or more processors
are present.

Instance
ESVS
FS
Bike2
PC2
PC
Big-PC
Renault

Labeling U
avg
max
0.0001 0.01
0.0010 0.01
0.0010 0.02
0.0030 0.02
0.0050 0.02
0.2070 0.45
0.3470 0.64

Labeling D
avg
max
0.0002 0.01
0.0020 0.02
0.0020 0.01
0.0030 0.02
0.0040 0.02
0.3160 0.60
0.4700 0.79

Valid domain
avg
max
0.0001
0.01
0.0001
0.01
0.0001
0.01
0.0005
0.01
0.0008
0.01
0.0300
0.04
0.0700
0.08

Table 5: Interaction time in seconds for 2-wCVD query.

128

fiInteractive Cost Configuration Over Decision Diagrams

Our empirical evaluation demonstrates the practical value of our approach. Even the
NP-hard 2-wCVD query can be implemented with response times suitable for interactive use,
when applied to huge configuration instances. Note, however, that in order to achieve such
performance it is critical to optimize MDD implementation as well as to utilize advanced
list operation techniques. Our initial implementation efforts that failed to do so, led to
response times measured in tens of seconds for the largest instances.

7. Related Work
There are several directions of related work. There is a large variety of representations
investigated in the area of knowledge compilation that might be suitable for supporting
interactive decision making with cost restrictions. There are also a number of approaches
to handle multiple cost functions in multi-criteria optimization.
7.1 Compiled Knowledge Representation Forms
In this paper we used binary decision diagrams (BDDs) and multi-valued decision diagrams
(MDDs) as compiled representations of our CSP model. However, there might be other compiled representations that might be more suitable for supporting interactive configuration.
Any compiled representation that supports efficient consistency checking and conditioning
would in theory support polytime interactive configuration. To calculate valid domains it
suffices for each value to restrict the representation and check if it is consistent. Any representation that supports efficient optimization and conditioning would support polytime
cost restrictions. It would suffice to restrict the representation with a value and check if the
minimum is smaller than a threshold value. We will therefore briefly survey some of the
related compiled representations and evaluate their suitability for our framework.
Knowledge-Compilation Structures. Probably the most well known framework for
comparing various compiled forms of propositional theories is based on viewing them as
special classes of negation normal form (NNF) languages (Darwiche & Marquis, 2002).
NNFs are directed acyclic graphs where internal nodes are associated with conjunctions ()
or disjunctions (), while leaf nodes are labeled with literals (x,x) or constants true or
false. By imposing various restrictions we get subclasses of NNF languages that support
efficient execution of various queries and transformations. More restrictive representations
are less succinct i.e. they can be exponentially larger for some instances, but they support
a larger number of queries and transformations in polytime. A comprehensive overview of
such representations is presented by Darwiche and Marquis (2002).
The critical restriction that makes NNF languages more tractable is decomposability.
It exploits variable independencies by enforcing that children of an -node have nonoverlapping
T variable scopes. Hence, for a propositional formula F = F1  F2 such that
var(F1 ) var(F2 ) = , to evaluate satisfiability of F it suffices to independently evaluate
F1 and F2 . A resulting language is decomposable negation normal form (DNNF) which
already supports in polytime two operations critical for calculating valid domains: consistency checking and conditioning. However, no general DNNF compiler exists. Current
compilation approach based on exhaustive DPLL search with caching isomorphic nodes
(Huang & Darwiche, 2005) constructs subsets of DNNF that satisfy an additional property
129

fiAndersen, Hadzic, & Pisinger

of determinism. Any two children of an -node are mutually exclusive. The resulting structure is called deterministic decomposable negation normal form (d-DNNF). This structure
would be an interesting target for cost-configuration. For Boolean CSP models, additive
cost functions could be efficiently optimized over d-DNNFs. For multi-valued models however, more research is necessary on how to encode finite-domain values in a way that allows
efficient cost processing. The tool support for compiling d-DNNFs so far takes as an input
only CNF formulas, and we are unaware of extensions allowing direct compilation of general
CSP models.
Other known knowledge representation forms can be retrieved by enforcing additional
properties. For example, by further enforcing that all nodes are decision nodes and that
each variable is encountered at most once on each path (read-once property) we get free
BDDs (FBDDs). After enforcing that all decision nodes appear wrt. fixed ordering we get
ordered BDDs (OBDDs). In fact, the d-DNNF compiler of Huang and Darwiche (2005) can
be specialized to compile OBDDs, which proved to be a valuable alternative way to BDD
compilation.
Weighted and Multi-Valued Knowledge Compilation Structures. Most of the compiled representations for propositional theories have valued counterparts. Many of them can
be seen as special cases of valued NNFs (VNNF) (Fargier & Marquis, 2007). Roughly, every
valued counterpart is obtained by changing the semantics of nodes, from logical operators
(such as , ) to general operators  (that could be arithmetic, such as + and ). Values
of functions represented by these structures are no longer in {0, 1} but in R. Furthermore,
functions need not be defined over Boolean domains, but could take finite-domain values. In
general, subsets of VNNF that satisfy decomposability and operator distributivity support
efficient optimization (Fargier & Marquis, 2007) and could, in principle, be used to support
cost configuration.
Construction of MDDs based on encoding into BDDs has been discussed by Srinivasan,
Kam, Malik, and Brayton (1990). Amilhastre et al. (2002) augmented automata of Vempaty (1992) with edge weights to reason about optimal restorations and explanations. These
weighted extensions correspond closely to our weighted MDDs since the variant of automata
used by Vempaty (1992) is equivalent to merged MDDs (Hadzic et al., 2008). However, the
weights are used to compute different queries and while we generate MDDs based on widely
available BDD-packages, Vempaty (1992) does not report compilation tools used. Semiring
labeled decision diagrams (SLDDs) (Wilson, 2005) label edges of an (unordered) MDD with
values from a semiring and allow computation of a number of queries relevant for reasoning
under uncertainty. Due to relaxed ordering, SLDDs are more succinct than our weighted
MDDs and are therefore an attractive target for cost-based configuration. However, the
proposal for now seems to be theoretic, and does not seem to be implemented. Arithmetic
circuits are directed acyclic graphs where internal nodes are labeled with summation and
multiplication operators while leaf nodes are labeled with constants or variables (Darwiche,
2002). They could be seen as a valued extension of d-DNNFs and hence are more succinct
than SLDDs. Furthermore, they support efficient optimization when all coefficients are
positive (in Bayesian context - they support efficient computing of most probable explanations). Compilation technology for ACs is not directly applicable to general CSP models, as
it is used primarily for representing Bayesian networks. It is based on compiling d-DNNFs
or tree clustering approaches (Darwiche, 2002, 2003). In our context, ACs might be use130

fiInteractive Cost Configuration Over Decision Diagrams

ful when optimizing non-additive objective functions with multiplicative coefficients such
as multi-linear functions induced by Bayesian networks. However, for purely propositional
constraints over which an additive cost function should be optimized, a purely propositional
representation form (such as d-DNNF) would be more adequate. Furthermore, efficient optimization queries based on ACs implicitly assume that all constants (at leaf nodes) are
positive, which is the case when modeling Bayesian networks, but does not hold for general
cost functions.
Global Structure Approaches. A number of techniques based on tree-clustering (Dechter
& Pearl, 1989) and variable-elimination (Dechter, 1999) exploit variable independencies that
are present globally in a CSP model. Both time and space complexity of these techniques
turn out to be bounded exponentially in the size of an important graph-connectivity notion
of tree-width (Bodlaender, 1993). While most of these techniques are geared towards enhancing search for a single (optimal) solution (adaptive consistency, bucket elimination etc),
the same concepts can be utilized for compiling representations of all solutions. AND/OR
MDDs (Mateescu et al., 2008) when restricted to Boolean variables are a subset of d-DNNF
formulas, where variable labeling respects a pseudo-tree obtained by a variable elimination
order. Due to utilization of variable independencies through -nodes, they are more succinct than MDDs and are therefore an attractive compilation target for cost-configuration.
Furthermore, they are already extended to handle weighted graphical models to support
Bayesian reasoning. However, publicly available tool support is limited and does not allow
processing weighted CVD queries. Tree-driven-automata (Fargier & Vilarem, 2004) utilize
tree clustering (Dechter & Pearl, 1989), to generate a partial variable ordering that is used
to generate an automaton. Tree-driven-automata are equivalent to AND/OR MDDs and
when restricted to the Boolean case they represent a subset of d-DNNF languages called
strongly ordered decomposable decision graphs (SO-DDG) (Fargier & Marquis, 2006). Like
AND/OR MDDs they are more succinct than MDDs and therefore are an interesting target
for cost-configuration. However, tools for compiling tree-driven-automata are yet to become publicly available, and so far they have not been extended to handle costs. Weighted
cluster trees of Pargamin (2003) are a weighted extension of cluster trees used to support
interactive configuration with preferences. However, there is no publicly available compilation tool (an internal company-based implementation was presented), and the clusters are
represented explicitly without utilizing compressions based on local structure through decision diagrams or other compiled representations. Tree-of-BDDs (ToB) (Subbarayan, 2008)
directly exploit tree clustering by representing each cluster as a BDD. However, they do not
support conditioning in polytime which is a fundamental transformation in supporting user
interaction (assigning variables). However, they can be compiled for instances for which
d-DNNF compilation fails, and empirical evaluation shows that on average conditioning
times are short.
BDD Extensions. There is a large variety of weighted extensions of binary decision diagrams, that represent real-valued functions f : {0, 1}n  R rather than Boolean functions
f : {0, 1}n  {0, 1}. These extensions are limited to Boolean variables and their adoption in
future would have to consider encoding techniques of multi-valued variables that avoid explosion in size and support cost processing. Comprehensive overviews of these extensions are
presented by Drechsler (2001), Wegener (2000), and Meinel and Theobald (1998). An immediate extension is in the form of algebraic decision diagrams (ADDs) (Bahar, Frohm, Gaona,
131

fiAndersen, Hadzic, & Pisinger

Hachtel, Macii, Pardo, & Somenzi, 1993), also known as multi-terminal BDDs (MTBDDs),
that are essentially BDDs with multiple terminal nodes - one for each cost value. This is a
structure-oblivious approach to encoding cost, much as our approach of explicitly encoding
cost as a variable. The size grows quickly with increase of the number of terminals. Therefore a number of BDD extensions are introduced based on labeling edges with weights. They
differ mostly on cost operators and decomposition types associated with nodes. Edge-valued
BDDs (EVBDDs) (Lai & Sastry, 1992) label every edge with an additive cost value c(e) so
that for an edge e : u  u0 , the value val(u) = c(e) + val(u0 ) when v(e) = 1 (otherwise
val(u) = val(u0 )). Factored EVBDDs (FEVBDDs) (Tafertshofer & Pedram, 1997) introduce multiplicative weights, so that when v(e) = 1, value val(u) = c(e) + w(e)  val(u0 )
(otherwise val(u) = val(u0 )). Affine ADDs (AADDs) of Sanner and McAllester (2005)
further introduce additive and multiplicative edge weights for any edge (regardless of v(e)).
Then val(u) = c(e) + w(e)  val(u0 ) for every edge. It has been shown that AADDs are a
special case of valued NNFs (Fargier & Marquis, 2007).
An orthogonal extension of BDDs is to change decomposition type of nodes. OBDDs are
based on Shannon decomposition fu = xi fu0  xi fu1 . We can change this decomposition
type to positive Davio (pD) decomposition fu = f0  xi f1 or negative Davio(nD) decomposition fu = f0  xi f1 . By using pD decomposition we get ordered functional decision
diagrams (OFDDs) (Kebschull & Rosenstiel, 1993). These structures are incomparable to
OBDDs, i.e. they might be exponentially larger or smaller than OBDDs depending on the
instance. However, ordered Kronecker functional decision diagrams (OKFDDs)(Drechsler,
Sarabi, Theobald, Becker, & Perkowski, 1994) allow all three decomposition types, thus generalizing both OBDDs and OFDDs. Extending OFDDs with additive edge weights leads
to binary moment diagrams (BMDs) (Bryant & Chen, 1995), adding also multiplicative
edge weights leads to multiplicative binary moment diagrams ( BM Ds). Analogously, by
extending OKFDDs with additive and multiplicative edge weights we get Kronecker binary
moment diagrams (KBMDs) and K  BM Ds respectively (Drechsler, Becker, & Ruppertz,
1996).
It is unclear whether Boolean structures with advanced cost labeling schemes can be
used directly to represent multi-valued CSP models with cost functions. However, we could
compare the generalizations of such labeling schemes to multi-valued structures. A multivalued generalization of EVBDDs would correspond roughly to our weighted MDDs. However, introducing both additive and multiplicative weights as in AADDs would correspond
to a generalization of our labeling scheme that could prove to be useful for labeling multilinear cost functions. Namely, through introduction of multiplicative weights there would
be more subgraph sharing, and not as many nodes would have to be refined to accommodate non-additive costs. However, due to multiplicative factors, it is not obvious if our
cashing technique based on computing U, D can be directly extended, especially if some of
the coefficients are negative. In case of additive cost functions though, all of these schemes
would correspond to our labeling scheme. Most of these structures pay the price in less
efficient operators (such as apply operator) and larger memory requirements as they maintain more information. Therefore, for compiling Boolean functions, using these structures
would pose an unnecessary overhead in comparison to OBDDs. Hence, for models with
a large number of propositional (configuration) constraints, and an additive cost function,
we would not gain from compiling using these structures even in the Boolean case. When
132

fiInteractive Cost Configuration Over Decision Diagrams

the cost function is non-additive, introducing more elaborate cost representations might
prove beneficial for reducing memory requirements, but might make our label computing
technique unapplicable. From a practical point of view, while there are implementations
supporting Boolean versions of these structures, we are not aware of any tool supporting
multi-valued generalizations of these structures nor input language format that can be used
for specifying general propositional constraints.
7.2 Multi-Objective Cost Processing
Our multiple-cost configuration is close to approaches within a framework of multi-criteria
optimization where a decision maker should find a solution subject to multiple (often conflicting) objectives (Figueira et al., 2005; Ehrgott & Gandibleux, 2000). In particular, our
MDD-based algorithms are very close to the approaches for solving multiobjective shortest
path problem, where for a given graph (V, E) each arc is labeled with multiple costs, and the
goal is typically to compute the set of Pareto-optimal (efficient, non-dominated) solutions
(Ehrgott & Gandibleux, 2000; Muller-Hannemann & Weihe, 2001; Tarapata, 2007; Reinhardt & Pisinger, 2009). It has been shown that the multi-objective shortest path problem
is intractable. In particular, the number of Pareto-optimal solutions can grow exponentially with the number of vertices |V |, but a FPTAS (fully polynomial time approximation
scheme) has been developed for approximating the set of Pareto-optimal solutions. However, the way in which the solution space of multi-criteria optimization problems is explored
is significantly different from our approach. Typically, in each interaction step a subset of
Pareto-optimal solutions is computed and afterwards a decision maker interactively navigates through the set in order to reach the satisfying compromising solution. Interactive
methods in multi-criteria optimization usually compute a subset of solutions on the efficient
frontier, suggest it to the user for evaluation, and based on his input compute a new set of
solutions (Figueira et al., 2005, Chapter 16). These techniques would use the user input to
better estimate the way to aggregate multiple objectives, and some of them would require
the user to explicitly assign weights of importance to objectives. In contrast, instead of being primarily driven by the costs of solutions, our interactive approach supports reasoning
about the variable assignments in the solutions themselves through valid domains computation. It is an inherently different way of exploring the solution space which is more adequate
for users that want explicit control over variable assignments and not just indicating the
importance of cost functions.
Most of the approaches in the CSP community model preferences as soft constraints
(Meseguer, Rossi, & Shiex, 2006) that can be partially satisfied or violated, with a goal to
find the most satisfying or the least violating solution. This usually presupposes that preferences can be aggregated via algebraic operators, and as such is more related to single-cost
optimization problems. However, the approach by Rollon and Larrosa (2006) deals with
multiple costs explicitly. It utilizes global structure (i.e. variable independencies) of the
weighted CSP model to compute an efficient frontier through bucket-based variable elimination. A highly related approach that utilizes global structure of the generalized additive
independence (GAI) network is presented by Dubus, Gonzales, and Perny (2009). In order
to compute the efficient frontier, the authors use a message passing computation mechanism which is analogous to computing buckets. In addition, the authors develop a fully
133

fiAndersen, Hadzic, & Pisinger

polynomial approximation scheme to approximate the efficient frontier and demonstrate
the significant improvement in performance. However, neither of these methods can exploit
the fact that the solution space of hard constraints is available in a compiled representation.
Instead, these methods operate only over an unprocessed model specification (whether it is
a weighted CSP or a GAI network) treating both the hard and soft constraints uniformly
and hence allowing the scope of hard constraints to decrease the variable independencies in
the model (and thus decrease the performance of the algorithms). Furthermore, the result
of computation of these methods does not allow a full exploration of efficient solutions. For
each value on the frontier only a single supporting efficient solution is maintained while we
maintain for each efficient value the set of all supporting efficient solutions. Hence, it is not
possible to efficiently retrieve valid domains even after the algorithms terminate. It would
be interesting to see however, whether these methods could be adopted to work over MDD
representations of a solution space.
Knapsack constraints are special case of two-cost configuration problems over a universally true MDD. Trick (2001) used dynamic programming to propagate Knapsack constraints during CSP search. Fahle and Sellmann (2002) presented an approximated filtering
algorithm, based on various integer programming bounds for the Knapsack problem. Sellmann (2004) presented a fully polynomial time approximation algorithm for approximated
filtering. However, these techniques were considered in constraint propagation context and
none of them considered processing over existing MDD structure.

8. Conclusions and Future Work
In this paper we presented an extension of BDD-based interactive configuration to configuring in the presence of cost restrictions. We guarantee polynomial-time cost configuration
when the cost function is additive and feasible solutions are represented using multi-valued
decision diagram. We process cost restrictions over an MDD which is extracted from an
underlying BDD. We therefore strictly extend BDD-based configuration of Hadzic et al.
(2004) to support cost-bounding of additive cost functions without incurring exponential
increase in complexity. Our implementation delivers running times that easily satisfy interactive response-time requirements. Furthermore, our approach can be extended to support
bounding in the presence of non-additive and semiring-based costs.
We further extended our approach by considering cost bounding wrt. multiple costs.
We proved that this is an NP-hard problem in the input MDD size even when processing
only two linear inequalities with positive coefficients and Boolean variables. However, we
provided a pseudo-polynomial scheme and fully polynomial approximation scheme for twocost configuration (which, in principle, can be extended to any k-cost configuration for a
fixed k). Our empirical evaluation demonstrated that despite inherent hardness of this
problem we can still provide satisfying performance in interactive setting. Our interaction
based on computing valid domains wrt. multiple cost restrictions is a novel addition to
interaction modes within multiple-criteria decision making (Figueira et al., 2005). We
provide an explicit control over variable assignments as well as cost functions.
In the future we plan to investigate other compiled representations over which delivering
cost configuration might be efficient and to investigate practical approaches to processing
non-unary cost functions. In particular, we plan to examine whether existing methods to
134

fiInteractive Cost Configuration Over Decision Diagrams

multiobjective non-unary optimization (e.g., Rollon & Larrosa, 2006; Dubus et al., 2009)
can be adopted to operate over MDD representation of a solution space.

Acknowledgments
We would like to thank the anonymous reviewers for their extensive comments that helped
us improve the paper. We would also like to thank Erik van der Meer for providing the
T-shirt example. The first version of this paper was created while Tarik Hadzic was at the
IT University of Copenhagen, while the updated version was made at the Cork Constraint
Computation Centre with a support from an IRCSET/Embark Initiative Postdoctoral Fellowship Scheme.

References
Amilhastre, J., Fargier, H., & Marquis, P. (2002). Consistency Restoration and Explanations
in Dynamic CSPs-Application to Configuration. Artificial Intelligence, 135 (1-2), 199
234.
Bahar, R., Frohm, E., Gaona, C., Hachtel, E., Macii, A., Pardo, A., & Somenzi, F. (1993).
Algebraic decision diagrams and their applications. In IEEE/ACM International Conference on CAD, pp. 188191.
Bartzis, C., & Bultan, T. (2003). Construction of efficient BDDs for bounded arithmetic
constraints. In Garavel, H., & Hatcliff, J. (Eds.), TACAS, Vol. 2619 of Lecture Notes
in Computer Science, pp. 394408. Springer.
Bodlaender, H. L. (1993). A tourist guide through treewidth. Acta Cybernetica, 11, 123.
Bollig, B., & Wegener, I. (1996). Improving the variable ordering of OBDDs is NP-complete.
Computers, IEEE Transactions on, 45 (9), 9931002.
Bryant, R. E. (1986). Graph-Based Algorithms for Boolean Function Manipulation. IEEE
Transactions on Computers, 35, 677691.
Bryant, R. E., & Chen, Y.-A. (1995). Verification of Arithmetic Circuits with Binary
Moment Diagrams. In In Proceedings of the 32nd ACM/IEEE Design Automation
Conference, pp. 535541.
Darwiche, A., & Marquis, P. (2002). A Knowledge Compilation Map. Journal of Artificial
Intelligence Research, 17, 229264.
Darwiche, A. (2002). A Logical Approach to Factoring Belief Networks. In Fensel, D.,
Giunchiglia, F., McGuinness, D., & Williams, M.-A. (Eds.), KR2002: Principles
of Knowledge Representation and Reasoning, pp. 409420 San Francisco, California.
Morgan Kaufmann.
Darwiche, A. (2003). A differential approach to inference in Bayesian networks. Journal of
the ACM, 50 (3), 280305.
135

fiAndersen, Hadzic, & Pisinger

Dechter, R. (1999). Bucket Elimination: A Unifying Framework for Reasoning. Artificial
Intelligence, 113 (1-2), 4185.
Dechter, R., & Pearl, J. (1989). Tree-Clustering for Constraint Networks. Artificial Intelligence, 38 (3), 353366.
Drechsler, R., Sarabi, A., Theobald, M., Becker, B., & Perkowski, M. A. (1994). Efficient
representation and manipulation of switching functions based on ordered Kronecker
functional decision diagrams. In DAC 94: Proceedings of the 31st annual conference
on Design automation, pp. 415419 New York, NY, USA. ACM.
Drechsler, R. (2001). Binary decision diagrams in theory and practice. International Journal
on Software Tools for Technology Transfer (STTT), 3 (2), 112136.
Drechsler, R., Becker, B., & Ruppertz, S. (1996). K*BMDs: A New Data Structure for
Verification. In EDTC 96: Proceedings of the 1996 European conference on Design
and Test, p. 2 Washington, DC, USA. IEEE Computer Society.
Dubus, J.-P., Gonzales, C., & Perny, P. (2009). Multiobjective Optimization using GAI
Models. In Boutilier, C. (Ed.), IJCAI, pp. 19021907.
Ehrgott, M., & Gandibleux, X. (2000). A Survey and Annotated Bibliography of Multiobjective Combinatorial Optimization. OR Spektrum, 22, 425460.
Fahle, T., & Sellmann, M. (2002). Cost Based Filtering for the Constrained Knapsack
Problem. Annals of Operations Research, 115, 7393.
Fargier, H., & Marquis, P. (2006). On the Use of Partially Ordered Decision Graphs in
Knowledge Compilation and Quantified Boolean Formulae. In Proceedings of AAAI
2006, pp. 4247.
Fargier, H., & Marquis, P. (2007). On Valued Negation Normal Form Formulas. In Proceedings of IJCAI 2007, pp. 360365.
Fargier, H., & Vilarem, M.-C. (2004). Compiling CSPs into Tree-Driven Automata for
Interactive Solving. Constraints, 9 (4), 263287.
Figueira, J. R., Greco, S., & Ehrgott, M. (2005). Multiple Criteria Decision Analysis: State
of the Art Surveys. Springer Verlag, Boston, Dordrecht, London.
Garey, M. R., & Johnson, D. S. (1979). Computers and Intractability-A Guide to the Theory
of NP-Completeness. W H Freeman & Co.
Hadzic, T., Subbarayan, S., Jensen, R. M., Andersen, H. R., Mller, J., & Hulgaard, H.
(2004). Fast Backtrack-Free Product Configuration using a Precompiled Solution
Space Representation. In In Proceedings of PETO Conference, pp. 131138. DTUtryk.
Hadzic, T., & Andersen, H. R. (2006). A BDD-based Polytime Algorithm for Cost-Bounded
Interactive Configuration. In Proceedings of AAAI 2006, pp. 6267.
136

fiInteractive Cost Configuration Over Decision Diagrams

Hadzic, T., Hansen, E. R., & OSullivan, B. (2008). On Automata, MDDs and BDDs in
Constraint Satisfaction. In Proceedings of the ECAI 2008 Workshop on Inference
Methods based on Graphical Structures of Knowledge.
Huang, J., & Darwiche, A. (2004). Using DPLL for efficient OBDD construction. In
Proceedings of SAT 2004, pp. 127136.
Huang, J., & Darwiche, A. (2005). DPLL with a trace: From SAT to knowledge compilation.
In Kaelbling, L. P., & Saffiotti, A. (Eds.), IJCAI, pp. 156162. Professional Book
Center.
Ibarra, O., & Kim, C. (1975). Fast approximation algorithms for the knapsack and sum of
subset problem. Journal of the ACM, 22, 463468.
Jensen, R. M. (2007). CLab: A C++ library for fast backtrack-free interactive product
configuration. http://www.itu.dk/people/rmj/clab/.
Kebschull, U., & Rosenstiel, W. (1993). Efficient graph-based computation and manipulation of functional decision diagrams. Design Automation, 1993, with the European
Event in ASIC Design. Proceedings. [4th] European Conference on, 278282.
Kellerer, H., Pferschy, U., & Pisinger, D. (2004). Knapsack Problems. Springer, Berlin,
Germany.
Lai, Y.-T., & Sastry, S. (1992). Edge-valued binary decision diagrams for multi-level hierarchical verification. In DAC 92: Proceedings of the 29th ACM/IEEE conference
on Design automation, pp. 608613 Los Alamitos, CA, USA. IEEE Computer Society
Press.
Lichtenberg, J., Andersen, H. R., Hulgaard, H., Mller, J., & Rasmussen, A. S. (2001).
Method of configuring a product. US Patent No: 7,584,079.
Lind-Nielsen, J. (2001).
BuDDy - A Binary
http://sourceforge.net/projects/buddy.

Decision

Diagram

Package.

Mateescu, R., Dechter, R., & Marinescu, R. (2008). AND/OR Multi-Valued Decision Diagrams (AOMDDs) for Graphical Models. Journal of Artificial Intelligence Research,
33, 465519.
Meinel, C., & Theobald, T. (1998). Algorithms and Data Structures in VLSI Design.
Springer.
Meseguer, P., Rossi, F., & Shiex, T. (2006). Soft constraints. In Rossi, F., van Beek,
P., & Walsh, T. (Eds.), Handbook of Constraint Programming, Foundations of Artificial Intelligence, chap. 9, pp. 281328. Elsevier Science Publishers, Amsterdam, The
Netherlands.
Miller, D. M., & Drechsler, R. (2002). On the Construction of Multiple-Valued Decision
Diagrams. In Proceedings of the 32nd International Symposium on Multiple-Valued
Logic (ISMVL02), p. 245 Washington, DC, USA. IEEE Computer Society.
137

fiAndersen, Hadzic, & Pisinger

Mller, J., Andersen, H. R., & Hulgaard, H. (2002). Product configuration over the internet.
In INFORMS Conference on Information Systems and Technology.
Muller-Hannemann, M., & Weihe, K. (2001). Pareto Shortest Paths is Often Feasible in
Practice. In WAE 01: Proceedings of the 5th International Workshop on Algorithm
Engineering, pp. 185198 London, UK. Springer-Verlag.
Nicholson, R., Bridge, D. G., & Wilson, N. (2006). Decision Diagrams: Fast and Flexible
Support for Case Retrieval and Recommendation. In Proceedings of ECCBR 2006,
pp. 136150.
Pargamin, B. (2003). Extending Cluster Tree Compilation with non-Boolean variables in
Product Configuration: a Tractable Approach to Preference-based Configuration. In
IJCAI03 Workshop on Configuration.
Reinhardt, L. B., & Pisinger, D. (2009). Multi-Objective and Multi-Constrained NonAdditive Shortest Path Problems. Computers and Operations Research. Submitted.
Technical report version available at: http://man.dtu.dk/upload/institutter/
ipl/publ/publikationer%202009/rapport%2016.pdf.
Rollon, E., & Larrosa, J. (2006). Bucket elimination for multiobjective optimization problems. Journal of Heuristics, 12 (4-5), 307328.
Sanner, S., & McAllester, D. A. (2005). Affine Algebraic Decision Diagrams (AADDs) and
their Application to Structured Probabilistic Inference. In Proceedings of IJCAI 2005,
pp. 13841390.
Schuurman, P., & Woeginger, G. J. (2005). Approximation schemes  a tutorial. In
Moehring, R., Potts, C., Schulz, A., Woeginger, G., & Wolsey, L. (Eds.), Lectures on
Scheduling. Forthcoming.
Sellmann, M. (2004). The Practice of Approximated Consistency for Knapsack Constraints.
In McGuinness, D. L., & Ferguson, G. (Eds.), AAAI, pp. 179184. AAAI Press / The
MIT Press.
Somenzi, F. (1996). CUDD: Colorado university decision diagram package. ftp://vlsi
.colorado.edu/pub/.
Srinivasan, A., Kam, T., Malik, S., & Brayton, R. K. (1990). Algorithms for discrete
function manipulation. In International Conference on CAD, pp. 9295.
Subbarayan, S., Jensen, R. M., Hadzic, T., Andersen, H. R., Hulgaard, H., & Mller, J.
(2004). Comparing two implementations of a complete and backtrack-free interactive
configurator. In Proceedings of CP04 CSPIA Workshop, pp. 97111.
Subbarayan, S. M. (2008). On Exploiting Structures for Constraint Solving. Ph.D. thesis,
IT University of Copenhagen, Copenhagen.
Tafertshofer, P., & Pedram, M. (1997). Factored edge-valued binary decision diagrams. In
Formal Methods in System Design, Vol. 10, pp. 243270. Kluwer.
138

fiInteractive Cost Configuration Over Decision Diagrams

Tarapata, Z. (2007). Selected multicriteria shortest path problems: An analysis of complexity, models and adaptation of standard algorithms. International Journal of Applied
Mathematics and Computer Science, 17 (2), 269287.
Trick, M. (2001). A dynamic programming approach for consistency and propagation for
knapsack constraints. In 3rd international workshop on integration of AI and OR
techniques in constraint programming for combinatorial optimization problems CPAI-OR, pp. 113124.
Vempaty, N. R. (1992). Solving constraint satisfaction problems using finite state automata.
In Proceedings of the Tenth National Conference on Artificial Intelligence, pp. 453
458.
Walsh, T. (2000). SAT v CSP. In Dechter, R. (Ed.), Proceedings of CP 2000, Lecture Notes
in Computer Science, pp. 441456.
Wegener, I. (2000). Branching Programs and Binary Decision Diagrams. Society for Industrial and Applied Mathematics (SIAM).
Wilson, N. (2005). Decision diagrams for the computation of semiring valuations. In
Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence
(IJCAI-05), pp. 331336.

139

fiJournal of Artificial Intelligence Research 37 (2010) 41-83

Submitted 07/09; published 02/10

Predicting the Performance of IDA* using
Conditional Distributions
Uzi Zahavi

zahaviu@cs.biu.ac.il

Computer Science Department
Bar-Ilan University, Israel

Ariel Felner

felner@bgu.ac.il

Department of Information Systems Engineering
Deutsche Telekom Labs
Ben-Gurion University, Israel

Neil Burch

burch@cs.ualberta.ca

Computing Science Department
University of Alberta, Canada

Robert C. Holte

holte@cs.ualberta.ca

Computing Science Department
University of Alberta, Canada

Abstract
Korf, Reid, and Edelkamp introduced a formula to predict the number of nodes IDA*
will expand on a single iteration for a given consistent heuristic, and experimentally demonstrated that it could make very accurate predictions. In this paper we show that, in addition to requiring the heuristic to be consistent, their formulas predictions are accurate
only at levels of the brute-force search tree where the heuristic values obey the unconditional distribution that they defined and then used in their formula. We then propose a
new formula that works well without these requirements, i.e., it can make accurate predictions of IDA*s performance for inconsistent heuristics and if the heuristic values in any
level do not obey the unconditional distribution. In order to achieve this we introduce the
conditional distribution of heuristic values which is a generalization of their unconditional
heuristic distribution. We also provide extensions of our formula that handle individual
start states and the augmentation of IDA* with bidirectional pathmax (BPMX), a technique for propagating heuristic values when inconsistent heuristics are used. Experimental
results demonstrate the accuracy of our new method and all its variations.

1. Introduction and Overview
Heuristic search algorithms such as A* (Hart, Nilsson, & Raphael, 1968) and IDA* (Korf,
1985) are guided by the cost function f (n) = g(n) + h(n), where g(n) is the actual distance
from the start state to state n and h(n) is a heuristic function estimating the cost from n to
the nearest goal state. A heuristic h is admissible if h(n)  dist(n, goal) for every state n
and goal state goal, where dist(n, m) is the cost of a least-cost path from n to m. If h(n) is
admissible, i.e. always returns a lower bound estimate of the optimal cost, these algorithms
are guaranteed to find an optimal path from the start state to a goal state if one exists.
c
2010
AI Access Foundation. All rights reserved.

fiZahavi, Felner, Burch, & Holte

An important question to ask is how many nodes will be expanded by these algorithms
to solve a given problem. A major advance in answering this question was the work done by
Korf, Reid, and Edelkamp which introduced a formula to predict the number of nodes IDA*
will expand (Korf & Reid, 1998; Korf, Reid, & Edelkamp, 2001). These papers, the formula
they present, and the predictions it makes, will all be referred to as KRE in this paper. Prior
to KRE, the standard method for comparing two heuristic functions was to compare their
average values, with preference being given to the heuristic with the larger average (Korf,
1997; Korf & Felner, 2002; Felner, Korf, Meshulam, & Holte, 2007). KRE made a substantial
improvement on this by characterizing the quality of a heuristic function by the distribution
of its values. They then developed the KRE formula based on the heuristic distribution to
predict the number of nodes expanded by IDA* when it is searching with a specific heuristic
and cost threshold. Finally, they compared the predictions of their formula to the actual
number of nodes expanded by IDA* for different thresholds on several benchmark search
spaces and showed that it gave virtually perfect predictions. This was a major advance in
the analysis of search algorithms and heuristics.
Despite its impressive results, the KRE formula has two main shortcomings. The first is
that KRE assumes that in addition to being admissible the given heuristic is also consistent.
A heuristic h is consistent if for every pair of states, m and n, h(m)  h(n)  dist(m, n).1
When the heuristic is consistent, the heuristic values of a nodes children are thus constrained to be similar to the heuristic value of the node. A heuristic is inconsistent if it is not
consistent, i.e. if for some pair of nodes m and n, h(m)  h(n) > dist(m, n). Inconsistency
allows a nodes children to have heuristic values that are arbitrarily larger or smaller than
the nodes own heuristic value. While the term inconsistency has a negative connotation as
something to be avoided, recent studies have shown that inconsistent heuristics are easy to
define in many search applications and can produce substantial performance improvements
(Felner, Zahavi, Schaeffer, & Holte, 2005; Zahavi, Felner, Schaeffer, & Sturtevant, 2007;
Zahavi, Felner, Holte, & Schaeffer, 2008). For this reason, it is important to extend the
KRE formula to accurately predict IDA*s performance on inconsistent heuristics, as such
heuristics are likely to become increasingly important in future applications.
The second shortcoming of the KRE formula is that it works well only at levels of the
search tree where the heuristic distribution follows the equilibrium distribution (defined
below in Section 3.1.2). This always holds at sufficiently deep levels of the search tree,
where the heuristic values converge to the equilibrium distribution. In addition, it will hold
at all levels when the heuristic values of the set of start states is distributed according to
the equilibrium distribution. However, as will be shown below (in Section 3.2.2) the KRE
formula can be very inaccurate at depths of practical interest on single start states and
on large sets of start states whose values are not distributed according to the equilibrium
distribution. In such cases, the heuristic values at the levels of the search tree that are
actually examined by IDA* will not obey the equilibrium distribution and applying KRE to
these cases will result in inaccurate predictions.
The main objective of this paper is to develop a formula to accurately predict the number
of nodes IDA* will expand, for a given cost threshold, for any given heuristic and set of start
states, including those not currently covered by KRE. To do this we first extend KREs idea
1. This is a general definition for any graph. In the case of undirected graphs we can write the consistency
definition as |h(m)  h(n)|  dist(m, n).

42

fiPredicting the Performance of IDA* using Conditional Distributions

of a heuristic distribution, which is unconditional, to a conditional distribution, in which
the probability of a specific heuristic value is not constant, as in KRE, but is conditioned
on certain local properties of the search space. Our conditional distribution provides more
insights about the behavior of the heuristic values during search because it is informed
about when (in what context in the search tree) a specific heuristic value will be produced.
This allows for a better study of heuristic behavior.
Based on the conditional distribution we develop a new formula, CDP (Conditional Distribution Prediction), that predicts IDA*s performance on any set of start states (regardless of
how their heuristic values are distributed) and for any desired depth (not necessarily large)
whether the heuristic is consistent or not. CDP has a recursive structure and information
about the number of nodes is propagated from the root to the leaves of the search tree. In
all of our experiments CDPs predictions are at least as accurate as KREs, and CDP is much
more accurate for inconsistent heuristics or sets of start states that have non-equilibrium
heuristic distributions. In its basic form, CDP is not particularly accurate on single start
states. We describe a simple extension that improves its accuracy in this setting. Finally,
we adapt CDP to make predictions when IDA* is augmented with the bidirectional pathmax
method (BPMX) (Felner et al., 2005). When inconsistent heuristics are being used, BPMX
is a useful addition to IDA*. It prunes many subtrees that would otherwise be explored,
thereby substantially reducing the number of nodes IDA* expands.
Throughout the paper we provide experimental results demonstrating the accuracy of
CDP in all of the above scenarios using the same two benchmark domains used in KRE  the
sliding-tile puzzle and Rubiks Cube.
For simplicity of discussion, we assume in this paper that all edges cost 1. This is
true for many problem domains. The generalization of the ideas to the case of variable edge
costs is straightforward, although their practical implementation introduces some additional
challenges (briefly described in Section 11.2).
The paper is organized as follows. Section 2 presents background material. Section 3
derives the KRE formula from first principles and discusses its limitations. In Section 4, our
notion of the conditional distribution of heuristic values is presented. Our new formula, CDP,
is presented in Section 4.2. Section 5 discusses a subtle but important way in which our
experiments differ from KREs. Experimental results are presented in Sections 6 and 7. The
extension of the CDP formula to better handle single start states is presented in Section 8.
Section 9 proposes a technique, based on CDP, for estimating upper and lower bounds on
the number of nodes IDA* can expand for a given unconditional distribution. Section 10
presents an extension of CDP for predicting the performance of IDA* when BPMX is applied.
Related work is discussed in Section 11, and conclusions and suggestions for future work are
given in Section 12. A preliminary version of this paper appeared (Zahavi, Felner, Burch,
& Holte, 2008).

2. Background
Two application domains were used by KRE to demonstrate the accuracy of their formula.
In our experiments we use exactly the same domains. In this section we describe them
as well as the search algorithm and the different heuristic functions that are used in our
experiments.
43

fiZahavi, Felner, Burch, & Holte

2.1 Problem Domains
Two of the classic examples in the AI literature of a single-agent pathfinding problems are
Rubiks Cube and the sliding-tile puzzle.
2.1.1 Rubiks Cube

Figure 1: 3  3  3 Rubiks Cube
Rubiks Cube was invented in 1974 by Erno Rubik of Hungary. The standard version
consists of a 3  3  3 cube (Figure 1), with different colored stickers on each of the exposed
squares of the sub-cubes, or cubies. There are 20 movable cubies and 6 stable cubies in
the center of each face. The movable cubies can be divided into eight corner cubies, with
three faces each, and twelve edge cubies, with two faces each. Corner cubies can only move
among corner positions, and edge cubies can only move among edge positions.
Each one of the 6 faces of the cube can be rotated 90, 180, or 270 degrees relative to the
rest of the cube. This results in 18 possible moves for each state. Since twisting the same
face twice in a row is redundant, the branching factor after the first move can be reduced
to 15. In addition, movements of opposite faces are independent. For example, twisting the
left face and then the right face leads to the same state as performing the same moves in
the opposite order. Pruning redundant moves results in a search tree with an asymptotic
branching factor of about 13.34847 (Korf, 1997).
In the goal state, all the squares on each side of the cube are the same color. The puzzle
is scrambled by making a number of random moves, and the task is to restore the cube to
its original unscrambled state. There are about 4  1019 different reachable states.
2.1.2 The Sliding-tile Puzzles
The sliding-tile puzzle consists of a square frame containing a set of numbered square tiles,
and an empty position called the blank. The legal operators are to slide any tile that is
horizontally or vertically adjacent to the blank into the blank position. The problem is to
rearrange the tiles from some random initial configuration into a particular desired goal
configuration. The state space grows exponentially in size as the number of tiles increases,
and it has been shown that finding optimal solutions to the sliding-tile problem is NPcomplete (Ratner & Warmuth, 1986). The two most common versions of the sliding-tile
puzzle are the 3  3 8-puzzle, and the 4  4 15-puzzle. The 8-puzzle contains 9!/2 (181,440)
44

fiPredicting the Performance of IDA* using Conditional Distributions

1

2

3
7

1

2

4

5

6

3

4

5

8

9

10 11

6

7

8

12 13 14 15

Figure 2: The 8-puzzle and 15-puzzle goal states
reachable states, and the 15-puzzle contains about 1013 reachable states. The goal states of
these puzzles are shown in Figure 2.
The classic heuristic function for the sliding-tile puzzles is called the Manhattan Distance. It is computed by counting the number of grid units that each tile is displaced from
its goal position, and summing these values over all tiles, excluding the blank. Since each
tile must move at least its Manhattan Distance to its goal position, and each move changes
the location of one tile by just one grid unit, the Manhattan Distance is a lower bound of
the minimum number of moves needed to solve a problem instance.
2.2 Iterative Deepening A*
Iterative deepening A* (IDA*) (Korf, 1985) performs a series of depth-first searches, increasing a cost threshold d each time. In the depth-first search, all nodes n with f (n)  d
are expanded. Threshold d is initially set to h(s), where s is the start node. If a goal is
found using the current threshold, the search ends successfully. Otherwise, IDA* proceeds
to the next iteration by increasing d to the minimum f value that exceeded d in the previous
iteration.
2.3 Pattern Databases (PDBs)
A powerful approach for obtaining admissible heuristics is to create a simplified version, or
abstraction, of the given state space and then to use exact distances in the abstract space
as estimates of the distances in the original state space. The type of abstractions we use
in this paper for the sliding-tile puzzles are illustrated in Figure 3. The left side of the
figure shows a 15-puzzle state S and the goal state. The right side shows the corresponding
abstract states, which are defined by erasing the numbers on all the tiles except for 2, 3, 6
and 7. To estimate the distance from S to the goal state in the 15-puzzle, we calculate the
exact distance from the abstract state corresponding to S to the abstract goal state.
A pattern database (PDB) is a lookup table that stores the distance to the abstract goal
of every abstract state (or pattern) (Culberson & Schaeffer, 1994, 1998). A PDB is built
by running a breadth-first search2 backwards from the abstract goal until the whole abstract
space is spanned. To compute h(s) for a state s in the original space, s is mapped to the
corresponding abstract state p and the distance-to-goal for p is looked up in the PDB.
2. This description assumes all operators have the same cost. This technique can be easily extended to
cases where operators have different costs.

45

fiZahavi, Felner, Burch, & Holte

The PDB lookup

State S
11

9

5

10

1

15 12

2

4

14 13

2

3

6

7

8

3

1

2

3

2

3

4

5

6

7

6

7

8

9

10 11

13
6

7

12 13 14 15

Goal State

Goal Pattern

(a)

(b)

Figure 3: Example of regular lookups

For example, a PDB for the 15-puzzle based on tiles 2, 3, 6, and 7 would contain an entry
for every possible way of placing those four tiles and the blank in the 16 puzzle positions.
Such a PDB could be implemented as a 5-dimensional array, P DB, with the array indexes
being the locations of the blank and tiles 2, 3, 6, and 7 respectively. The lookup for state
S shown in Figure 3 would then be P DB[0][8][12][13][14] (the blank is in position 0, tile 2
is in position 8, tile 3 is in position 12, etc.). In this paper, accessing the PDB for a state
S as just described will be referred to as a regular lookup, and the heuristic value returned
by a regular lookup will be referred to as a regular heuristic value.
Pattern databases have proven very useful for finding lower bounds for combinatorial
puzzles (Korf, 1997; Culberson & Schaeffer, 1998; Korf & Felner, 2002; Felner, Korf, &
Hanan, 2004; Felner et al., 2007). Furthermore, they have also proven to be useful for other
search problems, e.g., multiple sequence alignment (McNaughton, Lu, Schaeffer, & Szafron,
2002; Zhou & Hansen, 2004) and planning (Edelkamp, 2001a).
2.4 Geometric Symmetries
It is common practice to exploit special properties of a state space to enable additional
heuristic evaluations. In particular, additional PDB lookups can be performed given a
single PDB. For example, consider Rubiks Cube and suppose we had a PDB based on
the positions of the cubies that have a yellow face (the positions of the other cubies dont
matter). Reflecting and rotating the puzzle will enable similar lookups for cubies with a
different color (e.g., green, red, etc.) since the puzzle is perfectly symmetric with respect
to color. Thus, there are 24 symmetric lookups for such a PDB and different heuristic
values are obtained for each of these lookups in the same PDB. All these heuristic values
are admissible for any given state of the puzzle.
46

fiPredicting the Performance of IDA* using Conditional Distributions

As another example, consider the sliding-tile puzzle. A line of symmetry is the main
diagonal (assuming the goal location of the blank is in the upper left corner). Any configuration of tiles can be reflected about the main diagonal and the reflected configuration
shares the same attributes as the original one. Such reflections are usually used when using
PDBs for the sliding-tile puzzle (Culberson & Schaeffer, 1998; Korf & Felner, 2002; Felner
et al., 2004, 2007) and can be looked up from the same PDB.
2.5 Methods for Creating Inconsistent Heuristics
With consistent heuristics, the difference between the heuristic value of neighboring nodes
is constrained to be less than or equal to the cost of the connecting edge. For inconsistent
heuristics, there is no constraint on the difference between heuristic values of neighboring
nodes and it can be much larger than the cost of the edge connecting them.
The KRE formula is designed to work with consistent heuristics and therefore the KRE
papers report on experiments done with consistent heuristics only. By contrast, our new
formula, CDP, works for all types of heuristics including inconsistent heuristics. Therefore,
in this paper, in addition to the usual consistent heuristics such as regular PDB lookups or
Manhattan Distance we also experiment with inconsistent heuristics. We have previously
described several methods for producing inconsistent heuristics (Zahavi et al., 2007). Two
inconsistent heuristics that are used in the experiments below are the Random selection of
heuristics and Dual evaluations.
 Random selection of heuristics: A well-known method for overcoming the pitfalls
of a given heuristic is to employ several heuristics and use their maximum value (Holte,
Felner, Newton, Meshulam, & Furcy, 2006). For example, multiple heuristics can be
based on domain-specific geometric symmetries such as the ones described above.
When using geometric symmetries there are no additional storage costs associated
with these extra evaluations, even when these evaluations are based on PDBs.
Although using multiple heuristics results in an improved heuristic value, and therefore
is likely to reduce the number of nodes expanded in finding a solution, it also increases
the time required to calculate the heuristic values of the nodes, which might increase
the overall running time of the search. Instead of using all the available heuristics
for every heuristic calculation, one could instead choose to consult only one of them,
with the selection being made either randomly or systematically. Because only one
heuristic is consulted at each node, the time-per-node is virtually the same as if only
one heuristic was available. Even if each of the individual heuristics is consistent, the
heuristic values that are actually used are inconsistent because different heuristics are
consulted at different nodes. We showed (Zahavi et al., 2007) that this inconsistency
generally reduces the number of expanded nodes compared to using the same heuristic
for all the nodes and it is almost as low as if the maximum over all the heuristics had
been computed at every node. For Rubiks Cube, we randomly chose one of the 24
different lookups of the same PDB that arise from the 24 lines of symmetry of this
cube.
 Dual evaluation: In permutation state spaces such as Rubiks Cube, for each state
s there exists a dual state sd located the same distance from the goal as s (Felner
47

fiZahavi, Felner, Burch, & Holte

et al., 2005; Zahavi, Felner, Holte, & Schaeffer, 2006; Zahavi et al., 2008). Therefore,
any admissible heuristic applied to sd is also admissible for s. The puzzles studied
in this paper are permutation state spaces, and the dual of a state in these puzzles
is calculated by reversing the role of locations and objects: the regular state uses
a set of objects indexed by their current location, while the dual state has a set
of locations indexed by the objects they contain. When using PDBs, a dual lookup
is to look up sd in the PDB. Performing only regular PDB lookups for the states
generated during the search produces consistent values. However, the values produced
by performing the dual lookup can be inconsistent because the identity of the objects
being queried can change dramatically between two consecutive lookups. Due to its
diversity, the dual heuristic was shown to be preferable to a regular heuristic (Zahavi
et al., 2007). An exact definition and explanations about the dual lookup is provided
in the original papers (Felner et al., 2005; Zahavi et al., 2006, 2008).
It is important to note that all three PDB lookups (regular, dual, and random) consult
the same PDB. Thus, they need the same amount of memory and share the same overall
distribution of heuristic values (Zahavi et al., 2007).

3. The KRE Formula and its Limitations
This section begins with a short derivation of the KRE formula for state spaces in which all
state transitions have a cost of 1. KRE describe how this can be generalized to account for
variable edge costs (Korf et al., 2001).
3.1 The KRE formula
For a given state s and IDA* threshold d, KRE aims to predict N (s, d), the number of nodes
that IDA* will expand if it uses s as its start state and does a complete search with an
IDA* threshold of d (i.e., searches to depth d and does not terminate its search if the goal
is encountered). This can be written as
N (s, d) =

d
X

Ni (s, d)

(1)

i=0

where Ni (s, d) is the number of nodes expanded by IDA* at level i when its threshold is d.
One way to decompose Ni (s, d) is as the product of two terms
Ni (s, d) = Ni (s)  Pex (s, d, i)

(2)

where Ni (s) is the number of nodes in level i of BF Ssd , the brute-force search tree (i.e.,
the tree created by breadth first search without any heuristic pruning) of depth d rooted
at start state s, and Pex (s, d, i) is the percentage of the nodes in level i of BF Ssd that are
expanded by IDA* when its threshold is d.
In KRE, Ni (s) is written as Ni , i.e., without the dependence on the start state s. This is
perfectly correct for state spaces with a uniform branching factor b, because Ni (s) in such
cases is simply bi . For state spaces with a non-uniform but regular branching structure,
48

fiPredicting the Performance of IDA* using Conditional Distributions

KRE showed how Ni could be computed exactly using recurrence equations that are independent of s. However, the base cases of the recurrences in KRE do depend on s so their
using Ni instead of Ni (s) is reasonable but not strictly correct.
3.1.1 Conditions for Node Expansion in IDA*
To understand how Pex (s, d, i) is treated in KRE, it is necessary to reflect on the conditions
required for node expansion. A node n in level i of BF Ssd will be expanded by IDA* if it
satisfies two conditions:
1. f (n) = g(n) + h(n) must be less than or equal to d. When all edges have a unit cost,
g(n) = i and this condition is equivalent to h(n)  d  i. We call nodes that satisfy
this condition potential nodes because they have the potential to be expanded.
2. n must be generated by IDA*, i.e., its parent (at level i  1) must be expanded by
IDA*.
KRE restricted its analysis to heuristics that are consistent and proved that in this case
the second condition is implied by the first condition. In other words, when the given
heuristic is consistent, the nodes expanded by IDA* at level i of BF Ssd for threshold d are
exactly the set of potential nodes at level i.3 This observation allows Equation 2 to be
rewritten as
Ni (s, d) = Ni (s)  PP OT EN T IAL (s, i, d  i)

(3)

where PP OT EN T IAL (s, i, v) is defined as the percentage of nodes at level i of BF Ssd whose
heuristic value is less than or equal to v.
Note that although PP OT EN T IAL(s, i, di) = Pex (s, d, i) when the given heuristic is consistent, PP OT EN T IAL (s, i, d  i) overestimates Pex (s, d, i) when the heuristic is inconsistent,
sometimes by a very large amount (see Section 3.2.1).
3.1.2 Approximating PP OT EN T IAL (s, i, v)
KRE use three different approximations to PP OT EN T IAL(s, i, v). In KREs theoretical analysis
PP OT EN T IAL(s, i, v) is approximated by the equilibrium distribution, which we denote
as PEQ (v). It is defined to be the probability that a node chosen randomly and uniformly
among all nodes at a given depth of the brute-force search tree has heuristic value less than
or equal to v, in the limit of large depth (Korf et al. 2001, p. 208). KRE proved that, in
the limit of large d,
d
X
Ni (s)  PEQ (d  i)
i=0

would converge to N (s, d) if the given heuristic is consistent. Their final formula (the KRE
formula) is therefore:
3. See section 3.2.1 below for more discussion of the KRE formula with consistent heuristics.

49

fiZahavi, Felner, Burch, & Holte

N (s, d) =

d
X

Ni (s)  PEQ (d  i)

(4)

i=0

KRE contrasted the equilibrium distribution with the overall distribution, which is
defined as the probability that a state chosen randomly and uniformly from all states in
the problem has heuristic value less than or equal to v (p. 207). Unlike the equilibrium
distribution, which is defined over a search tree, the overall distribution is a property of the
state space. The overall distribution can be directly computed from a pattern database, if
just one pattern database is used and each of its entries corresponds to the same number
of states in the original state space, or can be approximated, in more complex settings,
by computing the heuristic values of a large random sample of states. KRE argued that in
Rubiks Cube the overall distribution for a heuristic defined by a single pattern database
is the same as the equilibrium distribution, but that for the sliding-tile puzzles, the two
distributions are different.
The heuristic used in KREs experiments with Rubiks Cube was defined as the maximum
over three pattern databases. For each individual pattern database, the overall distribution
was computed exactly. In KREs experiments these distributions were combined to approximate PP OT EN T IAL(s, i, v) by assuming that the values from the three pattern databases
were independent.
For the experiments with the sliding-tile puzzles, KRE defined three types of states based
on the whether the blank was located in a corner position, an edge position, or an interior
position, and approximated PP OT EN T IAL(s, i, v) by a weighted combination of the overall distributions of the states of each type. The weights used at level i were the exact
percentages of states of the different types at that level.
In our experiments we followed KRE precisely and use the overall distribution for individual Rubiks Cube pattern databases and the weighted overall distribution just described
for the sliding-tile puzzles. For simplicity, in the reminder of this paper we use the phrase
unconditional heuristic distribution4 and the notation P (v) to refer to the probability that
a node has a heuristic less than or equal to v. We let the exact context determine which
distribution P (v) actually denotes, whether it is the equilibrium distribution, the overall
distribution, or any other approximation of PP OT EN T IAL and Pex . Likewise we will use
p(v) (lower case p) to denote P (v)  P (v  1) (with p(0) = P (0)). p(v) is the probability
that a state will have a heuristic value of exactly v according to the distribution P .
3.2 Limitations of the KRE Formula
The KRE formula (Equation 4) has two main shortcomings: (1) its predictions are not
accurate if the given heuristic is inconsistent, and (2) even with consistent heuristics its
predictions can be inaccurate for individual start states or sets of start states whose heuristic
values are not distributed according to the unconditional heuristic distribution, P (v). We
now turn to examine each of these in detail.
50

fiPredicting the Performance of IDA* using Conditional Distributions

Consistent
4
3
3

3 3

expanded
generated
not generated

5

Inconsistent
4 R
3

3

4 6

3

5 m
6

4 3 n

Figure 4: Consistent versus inconsistent heuristics
3.2.1 Inconsistent Heuristics
As specifically mentioned in the KRE papers one property required for the KRE analysis
is that the heuristic be consistent. This is necessary because the KRE formula aims to
count the number of potential nodes at each level in BF Ssd . With consistent heuristics, the
heuristic value of neighboring states never changes by more than the change in the g-value,
as illustrated in the left side of Figure 4 (where the number inside a node is its heuristic
value). This implies that the f -value of a nodes ancestor is always less than or equal to
the f -value of the node (i.e., f is monotone non-decreasing along a path in the search tree).
Therefore, it is easy to prove that with consistent heuristics all the ancestors of a potential
node are also potential nodes (Korf et al., 2001). Consequently IDA* will expand all and
only the potential nodes in BF Ssd . Hence, a formula such as KRE that aims to count the
number of potential nodes in BF Ssd can be used to predict the number of nodes IDA* will
expand when given a consistent heuristic.
For inconsistent heuristics this reasoning does not apply. The heuristic values of neighboring states can differ by much more than the cost of the edge that connects them, and
thus the f -values along a path in the search tree are not guaranteed to be monotonically
non-decreasing. Therefore, the ancestors of a potential node are not guaranteed to be
potential nodes themselves, with the consequence that a potential node might never be
generated. For example, consider the search tree in the right side of Figure 4. The numbers
inside each node show the nodes heuristic value. Assume that the start node is R and
that the IDA* threshold is 5 (a node is a potential node if its f -value is less than or equal
to 5). There are 3 potential nodes at depth 2 (all with heuristic value 3). Consider the
potential node n. The path to it is through node m but node m is not a potential node
(f (m) = 1 + 5 = 6 > 5), so it will be generated but not expanded. Therefore, node n
will never be generated, preventing IDA* from expanding it. Since the KRE formula counts
the number of potential nodes, it will count node n and thus overestimate the number of
expanded nodes when an inconsistent heuristic is used.
The amount by which KRE overestimates the number of nodes expanded by IDA* with
an inconsistent heuristic can be very large. To illustrate this, consider the state space for
Rubiks Cube and a PDB heuristic defined by the locations of 6 (out of 12) of the edge
cubies. The regular method for looking up a heuristic value in a PDB produces a consistent
heuristic. As discussed in Section 2.5 two alternative PDB lookups that produce inconsistent
4. unconditional to distinguish it from the conditional distribution we introduce in Section 4.1

51

fiZahavi, Felner, Burch, & Holte

d
8
9
10
11
12
13

KRE
257
3,431
45,801
611,385
8,161,064
108,937,712

Regular
277
3,624
47,546
626,792
8,298,262
110,087,215

Dual
36
518
6,809
92,094
1,225,538
16,333,931

Random Symmetry
26
346
4,608
61,617
823,003
10,907,276

Table 1: Rubiks Cube - Number of nodes expanded by IDA* using regular, dual, and
random-symmetry PDB lookups for different IDA* threshold d and the corresponding KRE predictions.

heuristics are the dual evaluation and the random selection from multiple heuristics. In
Rubiks Cube there are 24 symmetries and each can be applied to any state to create a new
way to perform a PDB lookup for it. Thus, there are 24 heuristics for Rubiks Cube based
on the same PDB and the random-symmetry lookup chooses one of them randomly.
Because all three lookups (regular, dual, and random-symmetry) consult the same PDB
they have the same distribution of heuristic values, P (v), and therefore KRE will predict
that IDA* will expand the same number of nodes regardless of whether a regular, dual,
or random-symmetry lookup is done. The experimental results in Table 1 show that a
substantially different number of nodes are actually expanded in practice for each of these
methods.
Each row of Table 1 presents results for a specific IDA* threshold (d). Each result is an
average over 1, 000 random initial states, each of which is generated by making 180 random
moves from the goal state. The KRE column shows the KRE prediction based on the
unconditional heuristic distribution. The last three columns of Table 1 show the number of
nodes IDA* expands when it performs either a regular, dual, or random-symmetry lookup
in the PDB. The KRE prediction is within 8% of the actual number of nodes expanded
when IDA* uses the regular (consistent) PDB lookup (third column) but it substantially
overestimates the number of nodes expanded when IDA* uses the dual or random-symmetry
inconsistent lookups in the same PDB (fourth and fifth columns).
3.2.2 Sets of Start States Whose Heuristics Values do not Obey the
unconditional heuristic distribution
As explained above, KRE used the unconditional heuristic distribution P (v) and, in their
theoretical analysis, proved that its use in the KRE formula would give accurate predictions in the limit of large depth. In fact, accurate predictions will occur as soon as the
heuristic distribution at the depth of interest d closely approximates P (v). This happens
at large depths by definition but this can happen even at very shallow levels under certain
circumstances. The reason that KRE was able to produce extremely accurate predictions
in its experiments using the unconditional heuristic distribution P (v) for all depths and
all start states is that its experiments report average predictions and performances over
52

fiPredicting the Performance of IDA* using Conditional Distributions

a large number of randomly drawn start states. In the spaces used in KREs experiments,
the heuristic distribution of a large random set of start states very closely approximated
the P (v) distribution they used. This caused heuristic distributions at all levels to closely
approximate P (v).
However, if the set of start states does not have its heuristic values distributed according
to P (v), as is the case for most non-random sets of start states or a single start state,
KRE should not be expected to make good predictions for small depths. In other words,
in such cases the unconditional heuristic distribution P (v) is not expected to be a good
approximation of Pex (s, d, i).
Consider the case of a single start state and a consistent heuristic. The distribution of
heuristic values in the search tree close to the start state will be highly correlated with the
heuristic value of the start state, and therefore will not be the same in search trees with
start states having different heuristic values. For example, a great deal of pruning is likely
to occur near the top of the search tree for a start state with a large heuristic value, resulting
in fewer nodes expanded than for a start state with a small heuristic value. Applying KRE
to these two states will produce the same prediction, and therefore be inaccurate for at
least one of them, because it uses the same unconditional heuristic distribution P (v) in
both cases.
h
5
6
7
8
9

IDA*
30,363,829
18,533,503
10,065,838
6,002,025
3,538,964

KRE
8,161,064
8,161,064
8,161,064
8,161,064
8,161,064

Table 2: Results for a set of 1,000 start states all with the h-value shown in the first column
(regular PDB lookup, IDA* threshold of d = 12)

Table 2 demonstrates this phenomenon on Rubiks Cube with one regular 6-edge PDB
lookup for IDA* threshold d = 12. The IDA* column shows the average number of nodes
expanded for 1, 000 start states, all with the same heuristic value h for any given row. KRE
ignores the heuristic values of the start states and predicts that 8,161,064 nodes will be
expanded by IDA* for every start state. The row for d = 12 in Table 1 shows that this is
an accurate prediction when performance is averaged over a large random sample of start
states, but in Table 2 we see that it is too low for start states with small heuristic values
and too high for ones with large heuristic values.
3.2.3 Convergence of the Heuristic Distributions at Large Depths
As described above, KRE will make accurate predictions for level i if the nodes at that level
actually obey the unconditional heuristic distribution P (v). As i increases, the distribution
of heuristic values will start to converge to P (v). The rate of convergence depends upon
the state space. It is believed to be fairly slow for the sliding-tile puzzles, but faster for
53

fiZahavi, Felner, Burch, & Holte

Rubiks Cube. If the convergence occurs before the IDA* threshold is reached KRE will
provide accurate predictions for any set of start states (including single start states).
In order to experimentally test this we repeated the KRE Rubiks Cube experiment but,
in addition to using a large set of random start states, we also looked at the individual
performance of two start states, s6 , which has a low heuristic value (6), and s11 , which has
the maximum value for the heuristic used in this experiment (11). As in KRE we used the
8-6-6 heuristic which takes the maximum of 3 different PDBs (one based on all 8 corner
cubies and two based on 6 edge cubies each). This heuristic is admissible and consistent.
Over the billion random states we sampled to estimate P (v) the maximum value was 11
and the average value was 8.898.

KRE
d
10
11
12
13
14
15
16
17

1,510
20,169
269,229
3,593,800
47,971,732
640,349,193
8,547,681,506
114,098,463,567

Multiple start states
IDA*
Ratio
1,501
0.99
20,151
1.00
270,396
1.00
3,564,495
0.99
47,961,699
1.00
642,403,155
1.00
8,599,849,255
1.01
114,773,120,996
1.01

s6
53,262
422,256
3,413,547
29,114,115
259,577,913
2,451,954,240
24,484,797,237
258,031,139,364

Single start state
Ratio
s11
0.03
0.05
8,526
0.08
162,627
0.12
2,602,029
0.18
38,169,381
0.26
542,241,315
0.35
7,551,612,957
0.44 103,934,322,960

Ratio
2.37
1.66
1.38
1.26
1.18
1.13
1.10

Table 3: Rubiks Cube - Max of (8,6,6) PDBs
Table 3 presents the results. The KRE column presents the KRE prediction and the
Multiple start states columns presents the actual number of states generated (averaged over
a set of random start states) for each IDA* threshold. Both columns are copied from the KRE
journal paper (Korf et al., 2001). The Ratio columns of Table 3 shows the value predicted
by the KRE formula divided by the actual number of nodes generated. This ratio was found
to be very close 1.0 for multiple start states, indicating that KREs predictions were very
accurate.
The results for the two individual start states we tested are shown in the Single start
state part of the table. Note that both states are optimally solved at depth 17, but, as in
KRE, the search at that depth was run to completion. In both cases the KRE formula was not
accurate for small thresholds but the accuracy of the prediction increased as the threshold
increased. At threshold d = 17 the KRE prediction was roughly a factor of 2 too small for s6
and about 10% too large for s11 . This is a large improvement over the smaller thresholds.
These predictions will become even more accurate as depth continues to increase.
The reason the predictions improve for larger values of d is that at deeper depths the
heuristic distribution within a single level converges to the unconditional heuristic distribution. Using dashed and dotted lines of various types, Figure 5(a) shows the distribution of
heuristic values seen in states that are 0, 1, 2 and 4 moves away from s6 . The solid line in
Figure 5(a) is the unconditional heuristic distribution. The x-axis corresponds to different
heuristic values and the y-axis shows the percentage of states at the specified depth with
heuristic values less than or equal to each x value. For example for depth 0 (which includes
54

fiPredicting the Performance of IDA* using Conditional Distributions

120

120
Unconditional heuristic distribution
Depth = 4
Depth = 2
Depth = 1
Depth = 0

80

100
cumulative percentage

cumulative percentage

100

Unconditional heuristic distribution
Depth = 4
Depth = 2
Depth = 1
Depth = 0

60
40
20

80
60
40
20

0

0
0

2

4

6

8

10

12

0

2

Heuristic value

4

6

8

10

Heuristic value

(a) Heuristic Distributions for s6

(b) Heuristic Distributions for s11

Figure 5: Convergence of heuristic distributions

the start state only) only a heuristic value of 6 was seen (leftmost curve). For depth 1,
heuristic values of 5, 6 and 7 were seen (second curve from the left), and so on. The figure
shows that the heuristic distribution at successive depths converges to the unconditional
heuristic distribution (rightmost curve in Figure 5(a)). At depth 17 (not shown), the heuristic distribution is probably quite close to the unconditional heuristic distribution, making
the KRE prediction quite accurate even for this single start state.
Figure 5(b) shows the heuristic distributions for nodes that are 0, 1, 2, and 4 moves
away from s11 . In this case the unconditional heuristic distribution is to the left of the
heuristic distributions for the shallow depths, with the heuristic distribution for depth 0
being the rightmost curve in this figure. Comparing parts (a) and (b) of Figure 5 we see
that the convergence to the unconditional heuristic distribution is faster for s11 than for s6 ,
which explains why the KRE prediction in Table 3 is more accurate for s11 .

4. Conditional Distribution and the CDP Formula
We now present our new formula CDP (Conditional Distribution Prediction), which overcomes the two shortcomings of KRE described in the previous section. An important feature
of CDP is that it extends the unconditional heuristic distribution of heuristic values P (v)
used in KRE to be a conditional distribution.
4.1 Conditional Distribution of Heuristic Values
The conditional distribution of heuristic values is denoted P (v|context), where the context
represents local properties of the search tree in the neighborhood of a node that influence the
distribution of heuristic values in the nodes children. Specifically, if Pn (v) is the percentage
of node ns children that have a heuristic value less than or equal to v, then we define
P (v|context) to be the average of Pn (v) over all nodes n that satisfy the conditions defined
55

12

fiZahavi, Felner, Burch, & Holte

by the context. P (v|context) can be interpreted as the probability that a node with heuristic
value less than or equal to v will be produced when a node satisfying the conditions specified
by context is expanded. When the context is empty it is denoted P (v) as in Section 3. We
use p(v|context) (lower case p) to denote the probability that a node with heuristic value
equal to v will be produced when a node
P satisfying the conditions specified by context is
expanded. Obviously, P (v|context) = vi=0 p(i|context).
4.1.1 The Basic 1-Step Model
The conditioning context can be any combination of local properties of the search tree,
including properties of the node itself (e.g. its heuristic value), the operator that was applied
to generate the node, properties of the nodes ancestors in the search tree, etc. The simplest
conditional distribution is p(v|vp ), the probability of a node with a heuristic value equal to v
being produced when a node with value vp is expanded. We call this a 1-step model because
each value is conditioned by nodes that are one step away only. In special circumstances,
p(v|vp ) can be determined exactly by analysis of the state space and the heuristic, but in
general it must be approximated empirically by sampling the state space.
In our sampling method p(v|vp ) is represented by the entry M [v][vp ] in a two-dimensional
matrix M [0..hmax ][0..hmax ], where hmax is the maximum possible heuristic value. To build
the matrix we first set all values in the matrix to 0. We then randomly generate a state
and calculate its heuristic value vp . After that, we generate each child of this state one at
a time, calculate the childs heuristic value (v), and increment M [v][vp ]. We repeat this
process a large number of times in order to generate a large sample. Finally, we divide the
value of each cell of the matrix by the sum of the column the cell belongs to, so that entry
M [v][vp ] represents the percentage of children generated with value v when a state with
value vp is expanded.

vp

vp
6

v

7

8

9

10

6

7

8

9

10

6

0.17 0.11 0.06 0.03 0.02

7

0.36 0.38 0.33 0.25 0.19

0.00 0.44 0.70 0.67 0.00

8

0.37 0.44 0.53 0.60 0.62

9

0.00 0.00 0.09 0.32 0.89

9

0.02 0.03 0.05 0.08 0.14

10

0.00 0.00 0.00 0.01 0.11

10

0.00 0.00 0.00 0.01 0.01

6

0.30 0.11 0.00 0.00 0.00

7

0.60 0.45 0.21 0.00 0.00

8

v

(a) Consistent heuristic

(b) Inconsistent heuristic

Figure 6: A portion of the Conditional Distribution matrix for Rubiks Cube for consistent
and for inconsistent heuristics

56

fiPredicting the Performance of IDA* using Conditional Distributions

Figure 6 shows the bottom right corner of two such matrices for the 6-edge PDB of
Rubiks Cube. The left matrix (a) shows p(v|vp ) for the regular (consistent) lookup in
this PDB and the right matrix (b) shows p(v|vp ) for the inconsistent heuristic created by
the dual lookup in this PDB. The matrix in (a) is tridiagonal because neighboring values
cannot differ by more than 1. For example, states with a heuristic value of 8 can only have
children with heuristics of 7, 8 and 9; these occur with probabilities of 0.21, 0.70 and 0.09
respectively (see column 8). By contrast, the matrix in (b) is not tridiagonal. In column 8,
for example, we see that 6% of the time states with heuristic value of 8 have children with
heuristic values of 6.
4.1.2 Richer Models
When IDA* expands a node, it eliminates some children because of operator pruning. For
example, in state spaces with undirected operators, such as we are using in our studies, the
parent of a node would be generated among the nodes children but IDA* would immediately
prune it away. Distribution p(v|vp ) does not take this into account. In order to take this into
consideration it is necessary to extend the context of the conditional probability to include
the heuristic value of the parent of the node being expanded (we refer to the parent node
as gp). We denote this by p(v|vp ,vgp ) and call this a 2-step model because it conditions
on information from ancestors up to two steps away. p(v|vp ,vgp ) gives the probability of a
node with a heuristic value equal to v being generated when the node being expanded has
a heuristic value of vp and the parent of the node being expanded has a heuristic value of
vgp . It is estimated by sampling in the same way as was done to estimate p(v|vp ), except
that each sample generates a random state, gp, then all its neighbors, and then all of
their neighbors except those eliminated by operator pruning. Naturally, the results of the
sampling for this 2-step model are stored in a three-dimensional array.
The context of the conditional distribution can be extended in other ways as well. For
the sliding-tile puzzles, KRE conditions the overall distribution on the type of the state
being expanded, where the type indicates if the blank is in a corner, edge, or interior
location. In our experiments with the sliding-tile puzzle below, we extend p(v|vp ,vgp ) with
this type information: p(v, t|vp , tp ,vgp , tgp ) gives the probability of a node of type t with
heuristic value equal to v being generated when the node being expanded has heuristic
value vp and type tp and the expanded nodes parent has heuristic value vgp and type tgp .
4.2 A New Prediction Formula, CDP (Conditional Distribution Prediction)
In this section we use the conditional distributions just described to develop CDP, an alternative to the KRE formula for predicting the number of nodes IDA* will expand for a
given heuristic, IDA* threshold, and set of start states. As will be shown below experimentally, the new formula CDP overcomes the limitations of KRE and works well for inconsistent
heuristics and for any set of start states with arbitrary IDA* threshold.
Our overall approach is as follows. Define Ni (s, d, v) to be the number of nodes that
IDA* will generate at level i with a heuristic value equal to v when s is the start state and
d is the IDA* threshold.
Pdi Given Ni (s, d, v), the number of nodes IDA* will expand at level
i for threshold d is v=0 Ni (s, d, v), and, N (s, d), the total number of nodes expanded in a
complete iteration of IDA* with threshold d over all levels, the quantity we are ultimately
57

fiZahavi, Felner, Burch, & Holte

P P
interested in, is di=0 di
v=0 Ni (s, d, v). In these summations v only runs up to d  i because
only nodes with heuristic values in the range [0 . . . d  i] will be expanded at level i.
If Ni (s, d, v) could be calculated exactly, this formula would calculate N (s, d) exactly
whether the given heuristic is consistent or not. However, there is no general method for efficiently calculating Ni (s, d, v) exactly. Instead, Ni (s, d, v) will be estimated recursively from
Ni1 (s, d, v) and the conditional distribution; the exact details depend on the conditional
model being used and are given in the subsections that follow. We will use Ni (s, d, v) to
denote an approximation of Ni (s, d, v). In Section 4.5.1 we will describe conditions in which
this calculation is, in fact, exact, and therefore produces perfect predictions of N (s, d).
But in the general case these predictions may not be perfect and are only estimates. At
the present time we have no analytical tools for estimating their accuracy but as we show
experimentally, these estimates are often very accurate.
4.3 Prediction Using the Basic 1-Step Model
If the basic 1-step conditional distribution p(v|vp ) is being used, Ni (s, d, v) can be estimated
recursively as follows:
d(i1)

Ni (s, d, v)  Ni (s, d, v) =

X

Ni1 (s, d, vp )  bvp  p(v|vp )

(5)

vp =0

where bvp is the average branching factor of nodes with heuristic value vp , which is estimated
during the sampling process that estimates the conditional distribution.5 The reasoning
behind this equation is that Ni1 (s, d, vp )bvp is the total number of children IDA* generates
via the nodes it expands at level i  1 with heuristic value equal to vp . This is multiplied by
p(v|vp ) to get the expected number of these children that have heuristic value v. Nodes at
level i1 are expanded if and only if their heuristic value is less than or equal to d  (i  1),
hence the summation only includes vp values in the range of [0 . . . d  (i  1)]. By restricting
vp to be less than or equal to d  (i  1) in every recursive application of this formula,
we ensure (even for inconsistent heuristics) that a node is only counted at level i if all
its ancestors are expanded by IDA*. The base case of this recursion, N0 (s, d, v), is 1 for
v = h(s) and 0 for all other values of v.
Based on this, the number of nodes expanded by IDA* given start state s, threshold d,
and a particular heuristic can be predicted as follows:
CDP1 (s, d) =

d X
di
X

Ni (s, d, v)

(6)

i=0 v=0

If a set, S, of start states is given instead of just one start state, the calculation is
identical except that the base case of the recursion is defined using all the start states in
S. That is, we define N0 (S, d, v) to be equal to k if there are k states in S with a heuristic
value of v. The rest of the formula remains the same (with S substituted for s everywhere).
5. In the general case of our equation the branching factor depends on the context that defines the conditional distribution. Since in the 1-step model, the context is just the heuristic value v, we formally
allow the branching factor to depend on it. In practice, the branching factor is usually the same for all
heuristic values.

58

fiPredicting the Performance of IDA* using Conditional Distributions

4.4 Prediction Using Richer Models
If the 2-step conditional distribution p(v|vp , vgp ) is being used, we define Ni (s, d, v, vp ) to
be the number of nodes that IDA* will generate at level i with a heuristic value equal to v
from nodes at level i  1 with heuristic value vp when s is the start state and d is the IDA*
threshold. Ni (s, d, v, vp ) can be estimated recursively as follows:
d(i2)

Ni (s, d, v, vp )  Ni (s, d, v, vp ) =

X

Ni1 (s, d, vp , vgp )  bvp ,vgp  p(v|vp , vgp )

(7)

vgp =0

where bvp ,vgp is the average branching factor of nodes with heuristic value vp and a parent
with heuristic value vgp . The base case for this 2-step model is at level 1, not level 0.
N1 (s, d, v, vp ) is 0 for vp 6= h(s), and is the number of children of the start state s with
heuristic value v for vp = h(s). Based on this 2-step model the number of nodes expanded
by IDA* given start state s, threshold d, and a particular heuristic can be predicted as
follows:
CDP2 (s, d) =

d X
di d(i1)
X
X
i=0 v=0

Ni (s, d, v, vp )

(8)

vp =0

If there is a set S of start states instead of just one, the base case is N1 (S, d, v, vp ), the
number of children with heuristic value v of the states in S with heuristic value vp .
Analogous definitions of Ni and CDP can be used with any definition of context. For
example, if using a 1-step model with a set T of state types, one would define Ni (s, d, v, t)
as the number of nodes of type t that IDA* will generate at level i with a heuristic value
equal to v, and estimate it recursively as follows:
d(i1)

Ni (s, d, v, t)  Ni (s, d, v, t) =

X

X

vp =0

tp T

Ni1 (s, d, vp , tp )  bvp ,tp  p(v, t|vp , tp )

(9)

Based on this model the number of nodes expanded by IDA* given start state s, threshold
d, and a particular heuristic can be predicted as follows:
CDP(s, d) =

d X
di X
X

Ni (s, d, v, t)

(10)

i=0 v=0 tT

4.5 Prediction Accuracy
The accuracy of our predictions can be arbitrarily good or arbitrarily bad depending on
the accuracy of the conditional model being used. In the following subsections we examine
each of these extreme cases.
In principle, extending the context should never decrease the accuracy of the predictions
because additional information is taken into account. However, when the conditional model
is being estimated by sampling, an extended context can result in poorer predictions because
there are fewer samples in each context. This is our explanation of why the 1-step model
is more accurate than the 2-step model in rows h = 6 and h = 9 in Table 7 in Section 6.2
below.
59

fiZahavi, Felner, Burch, & Holte

4.5.1 Perfect Predictions
Consider any definition of context that includes the heuristic value of the node being expanded (vp in the contexts defined above) and contains sufficient information to allow
operator pruning to be correctly accounted for. We will use the notation (v, x) to refer to a
specific instance of such a context, where v is the heuristic value of the node being expanded
and x is an instantiation of the other information in the the context (e.g., the state type
information in the last model above). The general form of our predictive model with such
a context is
CDP(s, d) =

d X
di
X
i=0 v=0

X

Ni (s, d, v, x)

(11)

all x
such that (v, x)
is an instance
of the context

with
d(i1)

Ni (s, d, v, x) =

X

X

vp =0

all xp
such that (vp , xp )
is an instance
of the context

Ni1 (s, d, vp , xp )  bvp ,xp  p(v, x|vp , xp )

(12)

where bvp ,xp is the average branching factor, after operator pruning, of all nodes satisfying
the conditions of context (vp , xp ), and p(v, x|vp , xp ) is the average over all nodes n satisfying
the conditions of context (vp , xp ) of pn (v, x), the percentage of ns children, after operator
pruning, that satisfy the conditions of context (v, x).
If, for every context (vp , xp ), all nodes n satisfying the conditions defined by (vp , xp )
have exactly the same branching factor bvp ,xp and exactly the same value of pn (v, x) for all
contexts (v, x), a simple proof by induction starting from the correctness of the base cases,
N1 (s, d, v, x), shows that Ni (s, d, v, x) = Ni (s, d, v, x) for all i, i.e., that our prediction
method correctly calculates exactly how many nodes will satisfy the conditions of each
context at every level of the search tree. From this it follows that CDP(s, d) will be exactly
the number of nodes IDA* will expand given start state s and IDA* threshold d.
A practical setting in which the predictions of our 2-step model are guaranteed to be
perfect by this reasoning is when the following conditions hold:
1. the heuristic is defined to be the exact distance to the goal in an abstract state space,
as is the case when a single pattern database is used.
2. any two states, s1 , s2 , that map to the same abstract state x have the same set of
operators {op1 , ..., opk } that apply to them, and
3. if states s1 and s2 map to abstract state x, then for all operators op  {op1 , ..., opk }
that apply to s1 and s2 , s1 s child op(s1 ) and s2 s child op(s2 ) map to the same abstract
state, op(x).
60

fiPredicting the Performance of IDA* using Conditional Distributions

Define the context of a node to be its heuristic value and the abstract state to which it maps.
Condition (2) guarantees that for every context (v, x), all nodes satisfying the conditions
of (v, x) have exactly the same branching factor bv,x . This is true because if nodes n1
and n2 satisfy the conditions of context (v, x), they both map to the same abstract state
x, and condition (2) then requires that exactly the same set of operators apply to them
both. Conditions (2) and (3) together guarantee that for every context (vp , xp ), all nodes
satisfying the conditions of (vp , xp ) have exactly the same value of pn (v, x) for all v and x.
This is true because if nodes n1 and n2 satisfy the conditions of context (vp , xp ), they both
map to the abstract state xp , the same set of operators applies to both, and each operator
op creates a child that, in both cases, maps to a specific abstract state, op(xp ). Therefore
the percentage of children that map to any particular abstract state is the same for both
n1 and n2 .
A straightforward implementation of the prediction method in this setting associates a
counter with each abstract state, which is initialized to the number of start states that map
to the abstract state. The counter for abstract state x is updated once for each value of i
(1  i  d) by adding to it, for each operator op, the current value of the counter of each
abstract state y such that op(y) = x. This algorithm has a computational complexity that
is O(d  |A|   2 ) where |A| is the number of abstract states and  is the effective branching
factor in the abstract space. Because the complexity depends only linearly on d, in contrast
to the typically exponential dependency on d for the number of nodes IDA* will expand,
for sufficiently large d the prediction will be arbitrarily faster to compute than the search
itself. For example, for a PDB for the 15-puzzle based on the positions of 8 tiles and the
blank (roughly 4 billion abstract states), the prediction for 1000 start states with d = 52
takes only 6% of the time required to execute the search.
An exact prediction for this setting has two potential uses. The first is to determine if
searching with a single PDB is feasible or not. For example, the calculation might show
that even the first iteration of IDA* (with a threshold of h(start)) will take more than a
year to complete. The second is to use the prediction to compare the actual performance
of an alternative method executed on a set of start states (e.g. taking the maximum over
a set of PDBs) to the performance using a single PDB without actually having to execute
the IDA* search with the single PDB.
4.5.2 Very Poor Predictions
The predictions made by a conditional model will be extremely inaccurate if the distribution
of heuristic values is independent of the information supplied by the context. We illustrate
this with an example based on the 4x3 sliding-tile puzzle and two heuristics, a PDB based
on the locations of tiles 1-7 and the blank, and the heuristic that returns 0 for every state.
If the given state has the blank in its goal position, or in a position that is an even number
of moves from the goal position, the heuristic value for that state is taken from the PDB.
The other states have a heuristic value of 0. In the search tree, the heuristic used at level i
will therefore be the opposite of the one used at level i  1.
A 1-step model in this situation will clearly be hopeless for predicting the heuristic
distribution in levels where the PDB is being used until i is sufficiently large that the
distribution at level i converges to the unconditional distribution.
61

fiZahavi, Felner, Burch, & Holte

There is some hope that a 2-step model could make reasonably accurate predictions
because the PDB, considered by itself, defines a consistent heuristic and therefore the distribution of heuristic values of a nodes children are somewhat correlated with the heuristic
value of the nodes parent.
We tested this using the 4x3 sliding-tile puzzle, which is small enough that we could
build our 2-step model using all the states in the state space so that no error is introduced
through a sampling process. To test the prediction accuracy of the model we generated
50,000 solvable states at random and, as will be explained in detail in the next section, used
state s as a start state in combination with IDA* threshold d if IDA* would actually have
executed an iteration with threshold d when given state s as a start state. This means that
a different number of start states might be used for each value of d. The Num column in
Table 4 indicates how many start states were used for each value of d (first column) and we
have only included in this table results for which more than 5,000 start states were used.
The IDA* column shows the average number of nodes expanded by IDA* on the start
states used for each d and the Prediction column shows the number predicted by our
2-step model. The Ratio column is Prediction divided by IDA*. One can clearly see
the improvement of the predictions as d increases. But even at the deepest depth at which
our sample provided more than 5,000 start states, the prediction is a factor of 6 smaller than
the true value. Of course, using the constant heuristic value of 0 in alternate levels is not
something one would do in practice, but we obtained similar results, for essentially the same
reason, with the 15-puzzle when switching, from one level to the next, between a pattern
database based on tiles 1-7 and a pattern database based on tiles 9-15 (see Section 7.1).

d
27
28
29
30
31
32
33
34
35
36
37
38
39

IDA*
1,212
1,529
2,340
3,072
4,818
6,607
10,748
15,184
24,613
36,726
60,779
96,077
152,079

CDP2
Prediction Ratio
48
0.04
63
0.04
90
0.04
131
0.04
208
0.04
338
0.05
585
0.05
1,027
0.07
1,896
0.08
3,513
0.10
6,737
0.11
12,941
0.13
25,119
0.17

Num
5,754
7,780
9,086
11,561
12,397
14,109
14,109
14,545
13,492
12,261
10,405
8,355
6,505

Table 4: 4x3 sliding-tile puzzle, alternating between a good heuristic and 0.

5. Experimental Setup
The next two sections describe the experimental results that we obtained by running IDA*
and comparing the number of nodes it expanded to the number predicted by KRE and by
62

fiPredicting the Performance of IDA* using Conditional Distributions

CDP. We experimented on the same two application domains used by KRE, namely, Rubiks
Cube (Section 6) and the sliding-tile puzzle (Section 7). In each domain we evaluated the
accuracy of the two formulas, for both consistent and inconsistent heuristics, on a set of
solvable start states that were generated at random.
In all the experiments reported here, the start states used for a given IDA* threshold
d were subject to a special condition. State s is only used as a start state in combination
with threshold d if IDA* actually performs an iteration with threshold d when s is the start
state. For example, we would not use s as a start state for d = 17 if s is only distance 11
from the goal or if h(s) > 17. In addition, for the sliding-tile puzzle, start state s would
not be used with IDA* threshold d if h(s) and d were of different parity. By contrast, the
experiments in the KRE paper did not restrict the choice of start states in this way, the
same start states were used with every IDA* threshold .
This difference in how start states are chosen can have a large impact on the number
of nodes IDA* expands. Table 5 illustrates this for the 15-Puzzle using the Manhattan
Distance heuristic for IDA* threshold d (first column) between 43 and 50. The nodes
column under Unrestricted shows the number of nodes IDA* expanded on average for
50,000 randomly generated solvable start states. The values in this column are in close
agreement with the corresponding results of Table 5 in the KRE paper (Korf et al., 2001).
The number column shows how many of these start states satisfy our additional condition.
If we remove the start states that violate our condition, IDA* expands substantially fewer
nodes on average, as shown in the nodes column under Restricted, and the difference
increases as d increases. At d = 50 there is almost an order of magnitude difference between
the number of nodes expanded in the two settings. This difference needs to be kept in
mind when making comparisons with the experimental results reported here and in the KRE
papers.

d
43
44
45
46
47
48
49
50

Unrestricted
nodes
439,942
1,014,941
1,985,565
4,542,249
8,963,747
20,355,110
40,479,725
91,329,281

Restricted
number
nodes
22,525
219,001
22,484
393,406
22,937
688,119
22,266 1,182,522
22,243 2,108,766
21,028 3,508,482
20,389 6,037,064
18,758 9,904,973

Table 5: 15-Puzzle with Manhattan Distance. The effect on nodes expanded if start states
are randomly chosen or subject to our condition.

63

fiZahavi, Felner, Burch, & Holte

6. Experimental Results for Rubiks Cube
We begin with Rubiks Cube experiments. The heuristic used here is the 6-edge PDB
heuristic described above (Section 3.2.1). We experimented with the (consistent) regular
lookup and the (inconsistent) random-symmetry and dual lookups on this PDB. For the
CDP formula, two models were used, CDP1 and CDP2 , which denote the 1-step and 2-step
models, respectively.
As outlined in Section 4.1.1, the conditional distribution tables were built by generating one billion states (each is generated by applying 180 random moves to the goal state),
computing all their neighbors, and incorporating their heuristic information into the matrix representing the one-step model. For the two-step model we also generated all the
grandchildren and used their heuristic information.
In addition, in order to get reliable samples we added the following two procedures:
 While generating children and grandchildren for sampling we used the same pruning
techniques based on operator ordering that were used in the main search (see the
description in Section 2.1.1). That is, we did not use a sequence of operators that
would not be generated by the main search. This is done by looking at the random
walk that led to the initial state and using the last operator in the random walk as
the basis for operator pruning.
 In order to get a reliable sample we need each entry in the table to be sufficiently
sampled. Some entries in the table have very low frequency. For example, states
with heuristic value of 0 are very rare even in a sample of a billion states causing our
table for the 0 row to be generated by a very small sample. Therefore, we enriched
such entries by artificially creating random states with heuristic value of 0. Other
under-sampled entries were sampled in a similar way. One technique, for example,
for creating (with high probability) a random state with a heuristic value of x, is to
perform a random walk of length x on a random state with heuristic value of 0.
6.1 Rubiks Cube with Consistent Heuristics
Table 6 compares KRE to CDP1 and to CDP2 . The accuracy of the three prediction methods
was compared while using regular lookups on the 6-edge PDB. Results in each row are
averages over a set of 1000 random states. Each row presents the results of an IDA* iteration

d
8
9
10
11
12
13

IDA*
277
3,624
47,546
626,792
8,298,262
110,087,215

KRE
Prediction
257
3,431
45,801
611,385
8,161,064
108,937,712

Ratio
0.93
0.95
0.96
0.98
0.98
0.99

CDP1
Prediction Ratio
235
0.85
3,151
0.87
41,599
0.87
546,808
0.87
7,188,863
0.87
94,711,234
0.86

CDP2
Prediction Ratio
257
0.93
3,446
0.95
45,985
0.97
613,332
0.98
8,180,676
0.99
109,133,021
0.99

Table 6: Rubiks Cube with a consistent heuristic.
64

fiPredicting the Performance of IDA* using Conditional Distributions

with different threshold (d), given in the first column. The second column (IDA*) presents
the actual number of nodes expanded for each IDA* threshold. The next columns report
the predictions and the accuracy (Ratio) of each prediction defined as the ratio between
the predicted number and the actual number of expanded nodes. As was reported in the
KRE paper, the KRE formula was found to be very accurate for a consistent heuristic when
averaged over a large set of random start states. The table shows that CDP1 is reasonably
accurate but systematically underestimates because the one-step model does not consider
that a nodes parent will not be included among its children. We elaborate on this below.
CDP2 s predictions are very accurate, slightly more accurate than KREs.
6.2 Rubiks Cube with Start States Having Specific Heuristic Values
Table 2, presented above (Section 3.2.2), and the related discussion, show that KRE might
not make accurate predictions when start states are restricted to have a specific heuristic
value h. For the particular example shown (IDA* threshold 12) KRE will always predict
a value of 8, 161, 064, but the exact value depends on the specific set of start states used
because the IDA* threshold of 12 is not sufficiently large for the number of nodes to be
independent of the start states. Table 7 extends Table 2 to include the predictions of CDP.
It shows that both versions of CDP substantially outperform KRE on any particular set of
start states.

h
5
6
7
8
9

IDA*
30,363,829
18,533,503
10,065,838
6,002,025
3,538,964

KRE
Prediction Ratio
8,161,064
0.27
8,161,064
0.44
8,161,064
0.81
8,161,064
1.36
8,161,064
2.31

CDP1
Prediction Ratio
48,972,619
1.61
17,300,476
0.93
7,918,821
0.79
5,094,018
0.85
3,946,146
1.12

CDP2
Prediction Ratio
20,771,895
0.68
13,525,425
0.73
9,131,303
0.91
6,743,686
1.12
5,240,425
1.48

Table 7: Results for different start state heuristic values (h) for a regular PDB with an
IDA* threshold of d = 12.

6.3 Rubiks Cube with Inconsistent Heuristics
The same experiments were repeated for inconsistent heuristics. The dual and randomsymmetry lookups were performed on the 6-edge PDB instead of the regular lookup, thereby
creating an inconsistent heuristic. As discussed in Section 3.2.1, KRE produces the same
prediction for all heuristics (consistent and inconsistent) derived from a single PDB and
overestimates for the inconsistent heuristics. Table 8 shows that CDP2 is extremely accurate.
Its prediction is always within 2% of the actual number of nodes expanded.
The 1-step model used by CDP1 systematically underestimates the actual number of
nodes expanded for regular and dual lookups (see the regular lookup in Table 6 and the
dual lookup in Table 8). To understand why, consider what happens when the node m in
the right side of Figure 7 is expanded. It generates two children, node n and (assuming
65

fiZahavi, Felner, Burch, & Holte

KRE
Prediction

d

IDA*

8
9
10
11
12
13

36
518
6,809
92,094
1,225,538
16,333,931

8
9
10
11
12
13

26
346
4,608
61,617
823,003
10,907,276

CDP1
Ratio Prediction Ratio
Dual
257
7.14
31
0.86
3,431
6.62
418
0.81
45,801
6.73
5,556
0.82
611,385
6.64
74,037
0.80
8,161,064
6.66
987,666
0.81
108,937,712
6.67 13,180,960
0.81
Random Symmetry
257
9.88
26
1.00
3,431
9.92
353
1.02
45,801
9.94
4,718
1.02
611,385
9.92
62,990
1.02
8,161,064
9.92
840,849
1.02
108,937,712
9.99 11,224,108
1.03

CDP2
Prediction Ratio
36
508
6,792
90,664
1,210,225
16,154,640

1.00
0.98
1.00
0.98
0.99
0.99

26
346
4,601
61,174
815,444
10,878,227

1.00
1.00
1.00
0.99
0.99
1.00

Table 8: Rubiks Cube with dual, and random-symmetry (inconsistent) heuristics
operators have inverses as is the case in Rubiks Cube) a copy of its parent R (shown as ms
left child in Figure 7). This child is 2 levels deeper than R and therefore has an f -value that
is 2 greater than Rs. With an IDA* threshold of 5, this child will not be a potential node
and the 1-step model will conclude that m will generate a potential child with a probability
of 0.5, whereas in fact all of the children that remain after operator pruning are potential
nodes.

4 R
3 m
4

3 n

Figure 7: The 1-step model may underestimate
The reason the 1-step model does not underestimate the number of nodes expanded
when random-symmetry lookups are done is because the child copy of R is not constrained
to have the same heuristic value as R itself  different symmetries could be chosen for
different occurrences R. The childs f -value has no correlation with the f -value of R and
the above explanation of why CDP1 underestimates does not apply.
In fact, if different copies of a state have uncorrelated h-values the only effect of operator
pruning that needs to be taken into account is that it reduces the number of children, and
this can be done as well within a 1-step model when calculating the branching factor. There
may be other advantages of using the wider context of the 2-step model but the results for
the random-symmetry heuristic show that they are minor in this case.
66

fiPredicting the Performance of IDA* using Conditional Distributions

7. Experimental Results - Sliding-Tile Puzzle
In the KRE experiments on the sliding-tile puzzle, three state types are used, based on
whether the blank is in a corner, edge, or interior location. We used the same state types in
our experiments and used exact recurrence equations for N (s, v, d, t) in the type-dependent
version of the KRE formula. The heuristic used was Manhattan Distance (MD). We experimented with the 2-step CDP that includes the type system in the recurrence equations.
Results for the 1-step CDP are not included here because it performed poorly in early versions of these experiments.
For the 8-puzzle the conditional distribution P (v, t|vp , tp ,vgp , tgp ) needed by CDP2 and
the typed unconditional distribution P (v, t) needed by the type-dependent KRE formula
were computed by enumerating all the states in the 8-puzzle reachable from the goal.
For the 15-puzzle, it was not possible to do exhaustive enumeration of the entire state
space so the conditional distributions were estimated by generating ten billion reachable
states at random. This uniform random sample was used to estimate P (v, t) for KRE, and
each state in the sample was used as gp in the sampling method described in Section 4.1.2 for
P (v, t|vp , tp ,vgp , tgp ). For the latter, however, the basic sampling method had to be extended
because even after processing ten billion gp states some of the entries in the 6-dimensional
matrix were missing or were not sampled sufficiently. To correct this, after we generate
gp, its children, and its grandchildren and update the matrix accordingly, we check if the
matrix already contains data for gps great-grandchildren. If it does not then we generate
gps great-grandchildren and update the corresponding entries in the matrix. This continues
as long as we encounter contexts that have never been seen before. This introduces a small
statistical bias into the sample, but it guarantees that the sample contains the required
data.

h

#States

12
14
16
18
20

11,454
19,426
18,528
10,099
2,719

34
36
38
40
42
44

1,331
2,330
2,999
3,028
2,454
1,507

KRE
IDA*
Prediction
Ratio
8-puzzle depth 22
1,499
1,391
0.93
1,042
1,404
1.35
660
1,419
2.15
377
1,447
3.84
168
1,503
8.95
15-puzzle depth 52
77,028,888 420,858,250
5.46
38,206,986 424,113,561
11.10
16,226,330 428,883,700
26.43
6,310,724 433,096,514
68.63
2,137,488 438,475,079 205.14
620,322 444,543,678 716.63

CDP2
Prediction Ratio
1,809
1,051
544
246
91

1.21
1.01
0.82
0.65
0.54

172,845,559
64,247,275
21,505,426
6,477,903
1,749,231
409,341

2.24
1.68
1.33
1.03
0.82
0.66

Table 9: sliding-tile puzzles with a consistent heuristic (MD).
Prediction results for KRE and CDP2 for the 8- and 15-puzzles are shown in Table 9 in the
same format as above. For the 8-puzzle the predictions were made for an IDA* threshold
67

fiZahavi, Felner, Burch, & Holte

of 22 and each row corresponds to the group of all 8-puzzle states with the same heuristic
value h (shown in the first column) for which IDA* would actually have used threshold
22. The second column gives the number of states in each group. Clearly, as shown in
the IDA* column, for states with higher initial heuristic values IDA* expanded a smaller
number of nodes. This trend is not reflected in the KRE predictions since KRE does not take
h into account. For KRE the only difference between the attributes of different rows is the
different type distribution for the given group. Thus, the predicted number of expanded
nodes of KRE is very similar for all rows (around 1,400). The CDP formula takes the heuristic
value of the start state into account and was able to predict the number of expanded nodes
much better than KRE. The bottom part of Table 9 show results for the 15-puzzle for an
IDA* threshold of 52. Similar tendencies are observed.
7.1 Inconsistent Heuristics for the Sliding-tile Puzzle
Our next experiment is for an inconsistent heuristic on the 8-puzzle. We defined two PDBs,
one based on the location of the blank and tiles 14, the other based on the location of the
blank and tiles 58. To create an inconsistent heuristic, only one of the PDBs was consulted
by a regular lookup. The choice of PDB was made systematically, not randomly, based on
the position of the blank. Different occurrences of the same state were guaranteed to do
the same lookup but neighboring states were guaranteed to consult different PDBs and this
causes inconsistency. The results are presented in Table 10 for a variety of IDA* thresholds.
For each threshold the Num column indicates how many start states were used. The
results show that CDPs predictions are reasonably accurate, and very much more accurate
than KREs which overestimate by up to a factor of 26.

d
18
19
20
21
22
23
24
25
26
27
28
29

Num
44,243
40,773
60,944
48,888
60,345
40,894
42,031
22,494
18,668
7,036
4,131
762

IDA*
14.5
22.2
27.4
43.3
58.5
95.4
135.7
226.7
327.8
562.0
818.4
1,431.7

KRE
Prediction
80.4
151.5
244.2
459.0
734.4
1,383.6
2,200.6
4,155.3
6,569.9
12,475.0
19,515.7
37,424.6

Ratio
5.56
6.82
8.91
10.59
12.55
14.50
16.21
18.33
20.04
22.20
23.85
26.14

CDP2
Prediction Ratio
10.4
0.72
16.1
0.73
20.2
0.74
32.1
0.74
44.0
0.75
72.5
0.76
103.4
0.76
174.2
0.77
251.0
0.77
432.2
0.77
618.8
0.76
1,074.8
0.75

Table 10: Inconsistent heuristic for the 8-puzzle.
Similar experiments were conducted for the 15-puzzle. Here, the first PDB was based
on the location of the blank and tiles 17, the other was based on the location of the
blank and tiles 915. Table 11 shows the results for IDA* thresholds from 48 to 55 (recall
that the median solution length for this puzzle is 52). The numbers shown are averages
68

fiPredicting the Performance of IDA* using Conditional Distributions

over 50,000 start states. The CDP predictions for the 15-puzzle are considerably worse
than for the 8-puzzle, but the KRE predictions have degraded much more. The reason for
the inaccuracy of these predictions was discussed in Section 4.5.2. Much more accurate
predictions are produced if the context is extended to include the heuristic value of both
pattern databases, not just the one that the search algorithm actually consults.

d
48
49
50
51
52
53
54
55

IDA*
231,939.6
388,201.1
644,350.1
1,062,597.5
1,746,025.1
2,773,611.6
4,539,767.0
7,546,286.9

KRE
Prediction
311,462,527.1
664,920,142.2
1,413,202,357.9
3,014,405,997.5
6,404,191,951.4
13,639,455,787.3
29,035,096,650.9
61,899,533,064.7

Ratio
1,342.9
1,712.8
2,193.2
2,836.8
3,667.9
4,917.6
6,395.7
8,202.6

CDP2
Prediction
71,550.2
149,257.5
313,132.4
663,004.4
1,402,898.2
2,985,321.1
6,361,011.5
13,627,941.8

Ratio
0.308
0.384
0.486
0.624
0.803
1.076
1.401
1.806

Table 11: Inconsistent heuristic for the 15-puzzle.

8. Accurate Predictions for Single Start States
We have seen that CDP works well when the base cases of the recursive calculation of
Ni (s, d, v) is seeded by a large set of start states, no matter how their heuristic values
are distributed. However, the actual number of expanded nodes for a specific single start
state can deviate from the number predicted by CDP. The conditional distribution reflects
the expected values over all nodes that share the same context, and the single start state
of interest might behave differently than the average state that has the same context.
Consider a Rubiks Cube state with a heuristic value of 8. CDP2 predicts that IDA* will
expand 6, 743, 686 for such a state with IDA* threshold 12. Table 2 shows that on the
average (over 1, 000 start states with a heuristic value of 8) 6, 002, 025 states are expanded.
Examining the results for the individual start states showed that the actual number of
expanded nodes ranged between 2, 398, 072 to 15, 290, 697 nodes.
In order to predict the number of expanded nodes for a single start state we propose the
following enhancement to CDP. Suppose that we want to predict the number of expanded
nodes for IDA* threshold d and start state s. First, we perform a small initial search from
s to depth r. We then use all the states at depth r to seed the base cases of the CDP formula
and compute the formula with IDA* threshold d  r. This will cause a larger set of nodes
to be used in calculating Ni (s, d, v), thereby improving the accuracy of CDPs predictions.
8.1 Rubiks Cube, 6-edge PDB Heuristic
Table 12 shows results for four specific Rubiks Cube states with a heuristic value of 8 (of
the regular 6-edge PDB lookup) when the IDA* threshold was set to 12. We chose the
states with the least and greatest number of expanded nodes and two states around the
median. The first column shows the actual number of nodes IDA* expands for each state.
69

fiZahavi, Felner, Burch, & Holte

The next columns show the number of expanded nodes predicted by our enhanced CDP2
formula where the initial search was performed to depths (r) of 0, 2, 5 and 6. Clearly, these
initial searches give much better predictions than the original CDP2 (with r = 0), which
predicts 6, 743, 686 for all these states. With an initial search to depth 6, the predictions
are very accurate.
h
8
8
8
8

IDA*
2,398,072
4,826,154
9,892,376
15,290,697

CDP2 (r=0)
6,743,686
6,743,686
6,743,686
6,743,686

CDP2 (r=2)
4,854,485
7,072,952
8,555,170
9,432,008

CDP2 (r=5)
3,047,836
5,495,475
9,611,325
13,384,290

CDP2 (r=6)
2,696,532
5,184,453
9,763,455
14,482,001

Table 12: Single state (d = 12).

8.2 Rubiks Cube, 8-6-6 Heuristic
Section 3.2.3 presented KRE predictions for two start states, s6 , with a heuristic value of
6, and s11 , with a heuristic value of 11, for Rubiks Cube with the 8-6-6 heuristic. Here
we repeat these experiments with CDP1 . Tables 13 and 14 show the results with an initial
search of depth (r) 0 and 4. The tables show that CDP1 was able to achieve substantially
better predictions than KRE in most cases, and that an initial search to depth 4 usually
improved CDP1 s predictions.
d
10
11
12
13
14
15
16
17

IDA*
53,262
422,256
3,413,547
29,114,115
259,577,913
2,451,954,240
24,484,797,237
258,031,139,364

KRE
1,510
20,169
269,229
3,593,800
47,971,732
640,349,193
8,547,681,506
114,098,463,567

Ratio
0.03
0.05
0.08
0.12
0.18
0.26
0.35
0.44

CDP1 (r=0)
32,207
246,158
1,979,417
16,690,055
149,319,061
1,435,177,445
14,925,206,678
167,181,670,892

Ratio
0.60
0.58
0.58
0.57
0.58
0.59
0.61
0.65

CDP1 (r=4)
69,770
690,556
5,422,001
42,650,077
345,370,148
2,934,134,125
26,380,507,927
254,622,231,216

Ratio
1.31
1.64
1.59
1.46
1.33
1.20
1.08
0.99

Table 13: 8-6-6 PDB, single start state s6
8.3 Experiments on the 8-Puzzle - Single Start States
We performed experiments with the enhanced CDP2 formula on all the states of the 8-puzzle
with the (consistent) MD heuristic. We use the term trial to refer to each pair of a single
start state and a given IDA* threshold d. The trials included all possible values of d and for
each d all start states for which IDA* would actually perform a search with IDA* threshold
d. Predictions were made for each trial separately, and the relative error, predicted/actual,
for the trial was calculated. The results are shown in Figure 8. There are four curves in
the figure, for KRE, for CDP, and for the enhanced CDP with initial search depths (r) of 5
70

fiPredicting the Performance of IDA* using Conditional Distributions

d
11
12
13
14
15
16
17

IDA*
8,526
162,627
2,602,029
38,169,381
542,241,315
7,551,612,957
103,934,322,960

KRE
20,169
269,229
3,593,800
47,971,732
640,349,193
8,547,681,506
114,098,463,567

Ratio
2.37
1.66
1.38
1.26
1.18
1.13
1.10

CDP1 (r=0)
8,246
191,077
3,188,470
47,281,091
665,292,864
9,125,863,883
123,571,401,411

Ratio
0.97
1.17
1.23
1.24
1.23
1.21
1.19

CDP1 (r=4)
8,904
139,422
2,834,542
45,690,554
614,042,865
8,544,807,943
120,978,148,822

Ratio
1.04
0.86
1.09
1.20
1.13
1.13
1.16

Table 14: 8-6-6 PDB, single start state s11

cumulative percentage

100
80
60
40
KRE
CDP2
CDP2 (radius=5)
CDP2 (radius=10)

20
0
0

1

2

3

4

5

6

7

8

9

10

predicted / actual

Figure 8: Relative error for the 8-puzzle
and 10. The x-axis is relative error. The y-axis is the percentage of trials for which the
prediction had a relative error of x or less. For example, the y-value of 20% for the KRE
curve at x = 0.5 means that KRE underestimated by a factor of 2 or more on 20% of the
trials. The rightmost point of the KRE plot (x = 10, y = 94%) indicates that on 6% of the
trials KREs prediction was more than 10 times the actual number of nodes expanded. By
contrast CDP has a much larger percentage of highly accurate predictions, with over 99% of
its predictions within a factor of two of the actual number of nodes expanded. The figure
clearly shows the advantage of using the enhanced CDP. With an initial search to a depth
of 10, 90% of the trials had predictions within 10% of the correct number.

9. Performance Range for a Given Unconditional Distribution
The experiments in this paper that have used the 6-edge PDB for Rubiks Cube have illustrated the fact that the number of nodes IDA* expands given a PDB can vary tremendously
depending on how the PDB is used (Zahavi et al., 2007). To see this clearly, the middle
three columns of Table 15 show data that has already been seen in Tables 6 and 8, namely,
the number of nodes IDA* expands when the 6-edge PDB is used in the regular manner,
71

fiZahavi, Felner, Burch, & Holte

with dual lookups, and with random-symmetry lookups. IDA* expands ten times fewer
nodes when the 6-edge PDB is consulted with random-symmetry lookups than when it is
consulted in the normal way.
This raises the intriguing question of what range of performance can be achieved by
varying the conditional distribution when the unconditional distribution is fixed.
d
8
9
10
11
12
13
Correlation

CDP
257
3,431
45,801
611,385
8,161,064
108,937,712

Regular
277
3,624
47,546
626,792
8,298,262
110,087,215
0.591

Dual
36
518
6,809
92,094
1,225,538
16,333,931
0.359

Random Symmetry
26
346
4,608
61,617
823,003
10,907,276
0.187

CDP
16
210
2,813
37,553
501,273
6,691,215

Table 15: Range of IDA* Performace for the 6-edge Rubiks Cube PDB

9.1 Upper Limit
The upper extreme, which results in the most nodes expanded, occurs when a consistent
heuristic is used. This is because IDA* only expands potential nodes, so the maximum
number of nodes is expanded when the conditional distribution is such that the parent of
every potential node at level i is a potential node at level i  1. An exact calculation of
the number of potential nodes in the brute-force tree is therefore a theoretical upper bound
on the number of nodes IDA* will expand for a given unconditional distribution. As we
have already discussed, one way to estimate the number of potential nodes is to use the
KRE formula. This estimate of the upper bound of the number of nodes that IDA* could
expand is denoted as CDP in Table 15.
Alternatively, the number of potential nodes can be approximated with the CDP formula
given the conditional distribution. Consider Equation 6. In the summation we consider all
possible vp values in [0, d(i1)] as only these nodes are potential nodes at level i1. Thus
only these nodes are expanded by IDA* at level i  1 and only these nodes can generate
children at level i.6 Now, lets substitute this with vp  [0, hmax ]. We now consider all
the nodes at level i  1, even the ones that are not potential nodes. Using this in the
summation will calculate the number of all nodes with heuristic v at level i even ones that
are not actually generated be IDA* (because their parents were not potential nodes, i.e.
with vp > d  (i  1). This is shown in Equation 13.
Ni (s, v) =

hX
max

Ni1 (s, vp )  bvp  p(v|vp )

(13)

vp =0

6. Note that if the heuristic is consistent then only vp values in {v  1, v,v + 1} need to be considered in
the summation because nodes with other values of vp (smaller than v  1 or larger than v + 1) cannot
generate children with a heuristic value of v.

72

fiPredicting the Performance of IDA* using Conditional Distributions

Using this in the general prediction equation we get:

CDP =

d X
di
X

Ni (s, v)

(14)

i=0 v=0

This gives an alternative method to approximate the number of potential nodes. Both
these methods approximate this upper bound. In practice, however, it is possible that the
number of expanded nodes will slightly exceed this approximate bound due to noise and
small errors in the sampling or the calculations.
9.2 Lower Limit
With consistent heuristics values of neighboring states are highly correlated. At the other
extreme are cases where there is no correlation between heuristic values of neighboring
nodes. That is, the heuristic value of a child node is statistically independent of the heuristic
value of its parent. This means that regardless of the parents heuristic value vp , the heuristic
values of the children are distributed according to the unconditional heuristic distribution,
i.e., p(v|vp ) = p(v).
Our motivation for using this as an estimated lower bound on the number of nodes IDA*
could expand for a given unconditional distribution is the empirical observation that the
number of nodes IDA* expands decreases as the correlation between a parents heuristic
value and its childrens heuristic values decreases.
This is illustrated in the last row of the three middle columns of Table 15, which shows
the correlation between the heuristic values of neighboring states when different types of
lookups are done in the 6-edge PDB. It was calculated using Pearsons correlation coefficient,
defined over n pairs of x, y values according to the following equation

Correlationxy

Pn
Pn
xi yi  i=1 xi i=1 yi
p Pn
= p Pn
Pn
Pn
n i=1 x2i  ( i=1 xi )2 n i=1 yi2  ( i=1 yi )2
n

Pn

i=1

(15)

In order to calculate the correlation, 60, 000 random pairs of (xi ,yi ) neighboring states
were generated. Their heuristic values were computed and used in Equation 15. The bottom
row of Table 15 shows that the number of nodes expanded decreases as the correlation
between neighboring heuristic values decreases. This leads us to suggest that the number
of nodes expanded will reach a minimum when the correlation is zero.7
This estimated lower bound can be calculated using the CDP formula with p(v|vp ) = p(v).
We denote this by CDP. For the 1-step model this would be calculated using the following
equations:
7. In theory, it is possible for a heuristic to have a negative correlation between the parents heuristic value
and its childrens heuristic values, i.e., parents with low heuristic values could tend to have children with
large heuristic values and vice versa. We believe this is unlikely to occur in practice.

73

fiZahavi, Felner, Burch, & Holte

d(i1)

Ni (s, d, v) =

X

Ni1 (s, d, vp )  bvp  p(v)

(16)

vp =0

CDP =

di
d X
X

Ni (s, d, v)

(17)

i=0 v=0

As can be seen by comparing the rightmost two columns in Table 15, the randomsymmetry use of the 6-edge PDB is within a factor of two of our estimated minimum
possible number of nodes expanded with this PDB, which suggests that to substantially
improve upon its performance one would have to use a different PDB.
Table 16 shows the estimated upper and lower bounds of IDA*s performance, for a range
of IDA* thresholds, for three different PDBs for Rubiks Cube. The bounds are calculated
using 1, 000 random start states. The table shows that, according to these estimates, inconsistent heuristics based on the 5-edge PDB can outperform consistent heuristics based on
the 6-edge PDB but probably cannot outperform consistent heuristics based on the 7-edge
PDB since the estimated lower bound of the 5-edge PDB is larger than the estimated upper
bound of the 7-edge PDB.

d
8
9
10
11
12
13

5-edge PDB
CDP
CDP
2,869
134
38,355
2,278
511,982
30,623
6,834,185
408,775
91,225,920
5,456,512
1,217,726,395 72,836,079

6-edge PDB
CDP
CDP
257
16
3,431
210
45,801
2,813
611,385
37,553
8,161,064
501,273
108,937,712 6,691,215

7-edge PDB
CDP
CDP
42
10
348
42
4,535
291
60,535
3,829
808,051
51,116
10,786,252 682,311

Table 16: Estimated Bounds on Performance for three Rubiks Cube PDBs.

10. Predicting the Performance of IDA* with BPMX
With an inconsistent heuristic, the heuristic value of a child can be much larger than that
of the parent. If this happens in a state space with undirected edges, the childs heuristic
value can be propagated back to the parent. If this causes the parents f -value to exceed
the IDA* threshold the entire search subtree rooted at the parent can be pruned without
generating any of the remaining children. This propagation technique is called bidirectional
pathmax (BPMX) (Felner et al., 2005; Zahavi et al., 2007). It was shown to be very effective
in reducing the search effort by pruning subtrees that would otherwise be explored. We
now show how to modify CDP to handle BPMX propagation. Since BPMX only applies to
state spaces with undirected edges, the discussion in this section is limited to such spaces.
74

fiPredicting the Performance of IDA* using Conditional Distributions

10.1 Bidirectional Pathmax (BPMX)
Traditional pathmax (Mero, 1984) propagates heuristic values from a parent to its children,
and can be applied in any state space. Admissibility is preserved by subtracting the cost
of the connecting edge from the heuristic value. The basic insight of bidirectional pathmax
(BPMX) is that when edges are undirected heuristic values can propagate to all neighbors,
which includes from a child node to its parent. This process can continue to any distance
in any direction. BPMX is illustrated in Figure 9. The left side of the figure shows the
inconsistent heuristic values for a node and its two children. Consider the left child with a
heuristic value of 5. Since this value is admissible and all edges in this example have a cost
of one, all its immediate neighbors are at least 4 moves away from the goal, their neighbors
are at least 3 moves away, and so on. When the left child is generated, its heuristic value
(h = 5) can propagate up to the parent and then down again to the right child. To preserve
admissibility, each propagation along a path reduces h by the cost of traversing the path.
This results in h = 4 for the root and h = 3 for the right child. When using IDA*, this
bidirectional propagation may cause many nodes to be pruned that would otherwise be
expanded. For example, suppose the current IDA* threshold is 2. Without the propagation
of h from the left child, both the root node (f = g + h = 0 + 2 = 2) and the right child
(f = g + h = 1 + 1 = 2) would be expanded. Using the above propagation, the left child
will increase the parents h value to 4, resulting in search at this node being abandoned
without even generating the right child.

4

2
5

5

1

3

Figure 9: Propagation of values with inconsistent heuristics

10.2 CDP Overestimates When BPMX is Applied
When an inconsistent heuristic is being used and BPMX is applied, CDP will overestimate
the number of expanded nodes because it will count the nodes in subtrees that BPMX
prunes. In Section 4.2, we defined Ni (s, d, v) to be the number of nodes that IDA* will
generate at level i with a heuristic value exactly equal to v when s is the start state and d
is the IDA* threshold. The formula given for estimating Ni (s, d, v) (Equation 5) was:
d(i1)

Ni (s, d, v) =

X

Ni1 (s, d, vp )  bvp  p(v|vp )

vp =0

In calculating Ni (s, d, v) from Ni1 (s, d, vp ) this formula assumes that when a node is
expanded all its children are generated. This is why Ni1 (s, d, vp ) is multiplied by the
branching factor bvp . When BPMX is applied, a child may prune the parent before the
rest of the children are generated. If this happens, the assumption that all the children
of expanded nodes are generated would be wrong. For example, without BPMX, while
75

fiZahavi, Felner, Burch, & Holte

expanding the root of the left tree in Figure 9 both children are generated and the child on
the right is also expanded. Indeed CDP will count two nodes in this case. When BPMX is
applied the root is expanded but the child on the right will not be generated (and therefore
not expanded). Thus, CDP, which counts the two nodes, is overestimating the number of
nodes expanded. In the following section we modify our equation to correct this.
10.3 New Formula for Estimating Ni (s, d, v)
Let n be the node that is currently being expanded. Assume that n has b children and
consider the order in which they are generated. We call this order the generation order.
Note that when BPMX is applied, the probability that a child will be generated decreases
as we move through the generation order. Children that appear late in the order will have
a larger chance of not being generated since there are more previous children that might
cause a BPMX cutoff. Let pbx (l) be the probability that the child in location l in the order
will be generated even if BPMX is applied. With this definition we can extend Equation 5
as follows:
d(i1) bvp

Ni (s, d, v) =

X X
{Ni1 (s, d, vp )  pbx (l)  p(v|vp )}
vp =0

(18)

l=1

Ni (s, d, v) is being calculated in a similar way to Equation 5, except for the way we count
the total number of children IDA* generates via the nodes it expands at level i  1 with
heuristic value equal to vp . The idea here is to iterate over all the possible locations in the
generation order and calculate the probability that a node in location l will be generated.
In practice, however, the actual context for pbx has other variables besides the location l.
It also includes the IDA* threshold (d), the depth of the parent (i  1) and the heuristic
value of the parent (vp ), We thus get our final formula:
d(i1) bvp

Ni (s, d, v) =

X X
{Ni1 (s, d, vp )  pbx (l, d, i  1, vp )  p(v|vp )}
vp =0

(19)

l=1

This is exactly equal to Equation 5 in the special case when pbx (l) = 1 for all l, which
happens when BPMX is not used or when it is used with a consistent heuristic.
10.4 Calculating pbx
For simplicity, our model assumes that a heuristic value can only be propagated by BPMX
one level up the tree. This means that a state can be pruned only from its immediate
children and not by descendants at deeper levels. We make this assumption for another
reason besides simplicity of description. Our experiments with Rubiks Cube and other
domains showed that indeed almost all the pruning of BPMX was caused by a 1-level BPMX
propagation. A generalized formula with deeper BPMX propagations can be similarly
developed but it will include complicated recursive terms with very low practical value, at
least for the state spaces and heuristics we have studied.
Assume that c is a child of n in location l of the generation order. Child c will be
generated only if n is not pruned by any of its l  1 children that appear before c in the
76

fiPredicting the Performance of IDA* using Conditional Distributions

generation order. Assume that n is at level i and that the threshold is d. Since n is
expanded, h(n)  d  i. With BPMX h(n) can be increased (and cause BPMX pruning)
from a child k if h(k) > d  i + 1. In this case, h(k)  1 is larger than d  i, so when it
is used instead of h(n) IDA* will decide not to expand n and no additional children will
be generated. Therefore, in order for a child c in location l of the generation order to be
generated, all its l  1 predecessors in the generation order must have heuristics less than
or equal to d  i + 1. Assuming that the heuristic value of the parent is v the probability of
this will be
pbx (l, d, i, v) = {

di+1
X

p(h|v)}l1

(20)

h=0

We sum up the probability of each relevant heuristic value and raise the sum to the
power of l  1 since l  1 children appear before c.
10.5 Experiments on Rubiks Cube with BPMX
We repeated the experiments on Rubiks Cube with the 6-edge PDB but with BPMX
activated. Since BPMX affects only inconsistent heuristics, only the Dual and Random
Symmetry heuristics were tested. Each heuristic was tested for IDA* thresholds 8 through
13. The results, averaged over the same set of 1, 000 random states, are presented in
Table 17. The No BPMX columns are repeated from Table 8. The additional columns
show our results with BPMX. The column IDA* + BPMX presents the actual number of
expanded nodes when using BPMX. BPMX reduces the number of nodes expanded by more
than 30% for the Dual and by more than 25% reduction for the Random Symmetry, making
the unmodified CDP2 s predictions high by about the same amount. The CDPbx
2  column
shows that the modifications introduced in this section greatly improve the accuracy.

d

IDA*

8
9
10
11
12
13

36
518
6,809
92,094
1,225,538
16,333,931

8
9
10
11
12
13

26
346
4,608
61,617
823,003
10,907,276

No BPMX
CDP2

With BPMX
Ratio IDA* + BPMX
CDPbx
2
Dual
36
1.00
26
24
508
0.98
353
328
6,792
1.00
4,700
4,387
90,664
0.98
62,405
58,562
1,210,225
0.99
831,362
781,704
16,154,640
0.99
11,091,676 10,434,547
Random Symmetry
26
1.00
19
18
346
1.00
256
240
4,601
1.00
3,432
3,207
61,174
0.99
45,881
42,818
815,444
0.99
608,816
571,556
10,878,227
1.00
8,125,962
7,629,396

Ratio
0.92
0.93
0.93
0.94
0.94
0.94
0.95
0.94
0.93
0.93
0.94
0.94

Table 17: BPMX on Rubiks Cube - Dual & Random Symmetry

77

fiZahavi, Felner, Burch, & Holte

11. Related Work
Previous work on predicting A* or IDA*s performance from properties of a heuristic falls
into two main camps. The first bases its analysis on the accuracy of the heuristic, while
the second bases its analysis, as we have done, on the distribution of heuristic values. The
next two subsections survey these approaches.
11.1 Analysis Based on a Heuristics Accuracy
One common approach is to characterize a heuristic by focusing on the error in the heuristic
value (deviation from the optimal cost). The first analysis in this line, focusing on the effect
of errors on the performance of search algorithms, was done by Pohl (1970). Many other
papers in this line have appeared since (Pohl, 1977; Gaschnig, 1979; Huyn, Dechter, &
Pearl, 1980; Karp & Pearl, 1983; Pearl, 1984; Chenoweth & Davis, 1991; McDiarmid &
Provan, 1991; Sen, Bagchi, & Zhang, 2004; Dinh, Russell, & Su, 2007; Helmert & Roger,
2008).
These works usually assume an abstract model space of a tree where every node has
exactly b children and aim to provide the asymptotic estimation for the number of expanded
nodes. They mainly differ by the model assumptions (e.g. binary or non-binary trees) and
for what case the results are derived (worst case or average case). Worst case analysis
showed that there is a correlation between the
heuristic errors and the search complexity.
|h(n)h (n)|
They found that if the relative error,
, is constant, the search complexity will
h (n)
be exponential (in the length of solution path) but if the absolute error, |h(n)  h (n)|, is
bounded by a constant the search complexity is linear (Pohl, 1977; Gaschnig, 1979). Three
main assumptions used by Pohl (1977) are that the branching factor is assumed to be
constant across inputs, that there is a single goal state and that there are no transpositions
in the search space. When these assumptions do not hold, as is the case for many standard
benchmark domains in planning, general search algorithms such as A* explore exponential
number of states even under the assumption of an almost perfect heuristic (i.e., a heuristic
whose error is bounded by a small additive constant) (Helmert & Roger, 2008).
Since it is difficult to guarantee precise bounds on the magnitude of errors produced by
a given heuristic, a probabilistic characterization of these magnitudes was suggested (Huyn
et al., 1980; Pearl, 1984). Heuristics are modeled as random variables (RVs), and the relative
errors are assumed to be independent and identically distributed (IID model). In this model,
attaining an average polynomial A* complexity was proved to be essentially equivalent to
requiring that values of h(n) be clustered near h (n) where the allowed deviation is a
logarithmic function of h (n) itself.
Additional research in this line was conducted by Chenoweth and Davis (1991). Instead
of using the IID model, they suggested using the NC model, which places no constraints
on the errors of h. With this model the heuristic is defined according to how the heuristic values grow with respect to the distance to the goal, and not according to the error.
They predicted that A* complexity will be polynomial whenever the values of h(n) are
logarithmicaly clustered near h (n) + (h (n)), where  is an arbitrary, non-negative, and
non-decreasing function. Heuristics whose values grow slower than the distance to the
goal cause exponential complexity. Studies with the NC model showed that replacing a
78

fiPredicting the Performance of IDA* using Conditional Distributions

heuristic h with wh for some w  0 can often change A* complexity from exponential to
polynomial.
Most of these works focused on tree searches. By contrast, Sen et al. (2004) presented
a general technique for extending the analysis of the average case performance of A* from
search spaces that are trees to search spaces that are directed acyclic graphs. Their analytical results show that the expected complexity can change from exponential to polynomial
as the heuristic estimates of nodes become more accurate and restrictions are placed on the
cost matrix. Recent research in this line, analyzing the complexity of the A* algorithm was
presented by Dinh et al. (2007). This research presented both worst and average case analysis for the performance of A* for approximately accurate heuristics8 for search problems
with multiple solutions. Bounds presented in that paper have been proved to be dependent
on the heuristic accuracy and distribution of solutions.
11.2 Analysis Based on the Heuristic Distribution
As discussed at the outset of this paper, KRE suggested an alternative approach for calculating the time complexity of IDA* on multiple-goal spaces (Korf & Reid, 1998; Korf
et al., 2001). Arguing that the heuristic accuracy is very difficult to obtain, they suggested
deriving the analysis from the unconditional distribution of heuristic values, which is easy
to determine at least approximately. They also came up with a method for deriving a
closed-form formula for Ni , the number of nodes at level i of the brute-force search tree.
That method was later formalized (Edelkamp, 2001b). Unlike the work described in the
previous subsection, which provides a big-O complexity analysis, KREs aim (and ours) is
to exactly predict the number of nodes IDA* will expand.
KRE correctly point out that, when operators do not all have the same cost, Ni must be
defined as the number of nodes that can be reached by a path of cost i, as opposed to the
number of nodes that are i edges from the start state. The calculation of Ni in this more
general setting has been studied in detail by Ruml, in a slightly different context (Ruml,
2002). His solution involves using a conditional distribution for edge costs that bears a
strong resemblance to our conditional distribution on heuristic values.
Based on the work of KRE and on the insight that for PDB heuristics there is a correlation
between the size of the PDB and its heuristic value distribution, a new analysis limited to
PDB heuristics has been done (Korf, 2007; Breyer & Korf, 2008). The prediction is achieved
based on the branching factor of the problem and the size of the PDB without knowing
the actual heuristic distribution. In order to derive the heuristic distribution from the
size of the PDB it was assumed that the forward and backward branching factors of the
abstract space are equal and that the abstract space has a negligible number of cycles.
Since the second assumption is usually not realistic this model underestimates the number
of expanded nodes.
The KRE formula was developed to predict the performance of the IDA* algorithm. The
general approach can also be applied to A* as long as appropriate modifications are made
to the computations of Ni and P (v) (Korf et al., 2001; Holte & Hernadvolgyi, 2004; Breyer
& Korf, 2008). The challenge is accounting for the effect of A*s pruning of the search
tree when it generates a state that it has previously reached by a path of smaller or equal
8. A heuristic is an -approximation if (1  )h (s)  h(s)  (1 + )h (s) for all states in the search space.

79

fiZahavi, Felner, Burch, & Holte

cost. This is particularly challenging when the heuristic is inconsistent, because in that case
the first time A* generates a state it is not guaranteed to have reached it via a least-cost
path, so the state will occur more than once in A*s search tree. Indeed, in the worst case,
for every state A* will enumerate all the paths to the state in decreasing order of cost,
thereby generating exactly the same search tree as IDA* (Martelli, 1977). But in general,
A*s pruning will reduce Ni , especially for large i, in ways that may be hard to capture
in a small set of recurrence equations. The heuristic distribution over A*s entire search
tree, taken to its maximum depth, is, for consistent heuristics, the overall distribution (Korf
et al., 2001) since each state occurs exactly once in A*s search tree (as just observed, this
is not true for inconsistent heuristics). This does not imply that the overall distribution can
be used to good effect on a level-by-level basis, but its use in the KRE formula did result
in accurate predictions of A*s performance on the 15-puzzle for two different consistent
heuristics when used together with an exact calculation of Ni for A*s search tree (Breyer
& Korf, 2008).

12. Conclusions and Future Work
Historically, heuristics were characterized by their average. KRE introduced the idea of
characterizing heuristics by their unconditional heuristic distribution and presented their
formula to predict the number of nodes expanded on one iteration of IDA* based on the
unconditional heuristic distribution. The work we have presented in this paper takes another
step along this line. The conditional distribution we have introduced, and the prediction
formula CDP based on it, advance our understanding of how properties of a heuristic affect
the performance of IDA*.
Our CDP method advances KRE by improving its predictions at shallow depths, on a
wider range of sets of start states, and for inconsistent heuristics. We have also shown how
to use it to make an accurate prediction for a single start state and for an IDA* search that
uses BPMX heuristic value propagation.
Of course, with the more sophisticated methods, more preprocessing is needed and
special care must be taken when gathering the data in order to get a reliable sample. It
is much easier to calculate the average of the heuristic than to calculate a 3-dimensional
matrix. On the other hand, the latter approach better characterizes the heuristic and
enables generating accurate predictions for a larger variety of circumstances.
Future work will address a number of issues. It is not yet clear what attributes make
the best context for prediction, and how this is influenced by the choice of the heuristic
and by the attributes of the specific domain. Larger contexts (more parameters) will probably provide better prediction at a cost of more pre-processing. This tradeoff needs to be
further studied. Another direction will aim to extend this analysis approach to predict the
performance of other search algorithms such as A*.

13. Acknowledgments
This research was supported by grant number 728/06 and 305/09 from the Israeli Science
Foundation (ISF) to Ariel Felner. Robert Holte and Neil Burch gratefully acknowledge the
ongoing support for this work from Canadas Natural Sciences and Engineering Research
80

fiPredicting the Performance of IDA* using Conditional Distributions

Council (NSERC) and Albertas Informatics Circle of Research Excellence (iCORE). The
code for Rubiks Cube in this paper is based on the implementation of Richard E. Korf
used in his seminal work on this domain(Korf, 1997). We thank the anonymous reviewer
who encouraged us to widen our experimental results and to better explain the results of
KRE and their relation to our results. His/her comments clearly improved the strength of
this paper. Thanks also to Sandra Zilles for her careful checking of the details in Section 4.

References
Breyer, T., & Korf, R. (2008). Recent results in analyzing the performance of heuristic
search. In Proceedings of the First International Workshop on Search in Artificial
Intelligence and Robotics (held in conjunction with AAAI), pp. 2431.
Chenoweth, S. V., & Davis, H. W. (1991). High-performance A* search using rapidly
growing heuristics. In Proceedings of the Twelfth International Joint Conference on
Artificial Intelligence (IJCAI-91), pp. 198203.
Culberson, J. C., & Schaeffer, J. (1994). Efficiently searching the 15-puzzle. Tech. rep.
94-08, Department of Computer Science, University of Alberta.
Culberson, J. C., & Schaeffer, J. (1998). Pattern databases. Computational Intelligence,
14 (3), 318334.
Dinh, H. T., Russell, A., & Su, Y. (2007). On the value of good advice: The complexity of
A* search with accurate heuristics. In Proceedings of the Twenty-Second Conference
on Artificial Intelligence (AAAI-07), pp. 11401145.
Edelkamp, S. (2001a). Planning with pattern databases. In Proceedings of the 6th European
Conference on Planning (ECP-01), pp. 1334.
Edelkamp, S. (2001b). Prediction of regular search tree growth by spectral analysis.
In Advances in Artificial Intelligence, Joint German/Austrian Conference on AI,
(KI/OGAI-2001), pp. 154168.
Felner, A., Korf, R. E., & Hanan, S. (2004). Additive pattern database heuristics. Journal
of Artificial Intelligence Research, 22, 279318.
Felner, A., Korf, R. E., Meshulam, R., & Holte, R. C. (2007). Compressed pattern databases.
Journal of Artificial Intelligence Research, 30, 213247.
Felner, A., Zahavi, U., Schaeffer, J., & Holte, R. C. (2005). Dual lookups in pattern
databases. In Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence (IJCAI-05), pp. 103108.
Gaschnig, J. (1979). Performance Measurement and Analysis of Certain Search Algorithms.
Ph.D. thesis, Carnegie-Mellon University.
Hart, P. E., Nilsson, N. J., & Raphael, B. (1968). A formal basis for the heuristic determination of minimum cost paths. IEEE Transactions on Systems Science and Cybernetics,
SCC-4(2), 100107.
Helmert, M., & Roger, G. (2008). How good is almost perfect?. In Proceedings of the
Twenty-Third Conference on Artificial Intelligence (AAAI-08), pp. 944949.
81

fiZahavi, Felner, Burch, & Holte

Holte, R. C., Felner, A., Newton, J., Meshulam, R., & Furcy, D. (2006). Maximizing over
multiple pattern databases speeds up heuristic search. Artificial Intelligence, 170 (1617), 11231136.
Holte, R. C., & Hernadvolgyi, I. T. (2004). Steps towards the automatic creation of search
heuristics. Tech. rep. TR04-02, Computing Science Department, University of Alberta.
Huyn, N., Dechter, R., & Pearl, J. (1980). Probabilistic analysis of the complexity of A*.
Artificial Intelligence, 15 (3), 241254.
Karp, R. M., & Pearl, J. (1983). Searching for an optimal path in a tree with random costs.
Artificial Intelligence, 21 (1-2), 99116.
Korf, R. E. (1985). Depth-first iterative-deepening: An optimal admissible tree search.
Artificial Intelligence, 27 (1), 97109.
Korf, R. E. (1997). Finding optimal solutions to Rubiks Cube using pattern databases.
In Proceedings of the Fourteenth Conference on Artificial Intelligence (AAAI-97), pp.
700705.
Korf, R. E. (2007). Analyzing the performance of pattern database heuristics. In Proceedings
of the Twenty-Second Conference on Artificial Intelligence (AAAI-07), pp. 11641170.
Korf, R. E., & Felner, A. (2002). Disjoint pattern database heuristics. Artificial Intelligence,
134 (1-2), 922.
Korf, R. E., & Reid, M. (1998). Complexity analysis of admissible heuristic search. In
Proceedings of the Fifteenth Conference on Artificial Intelligence (AAAI-98), pp. 305
310.
Korf, R. E., Reid, M., & Edelkamp, S. (2001). Time complexity of Iterative-Deepening-A* .
Artificial Intelligence, 129 (1-2), 199218.
Martelli, A. (1977). On the complexity of admissible search algorithms. Artificial Intelligence, 8, 113.
McDiarmid, C. J. H., & Provan, G. M. (1991). An expected-cost analysis of backtracking
and non-backtracking algorithms. In Proceedings of the Twelfth International Joint
Conference on Artificial Intelligence (IJCAI-91), pp. 172177.
McNaughton, M., Lu, P., Schaeffer, J., & Szafron, D. (2002). Memory-efficient A* heuristics
for multiple sequence alignment. In Proceedings of the Eighteenth Conference on
Artificial Intelligence (AAAI-02), pp. 737743.
Mero, L. (1984). A heuristic search algorithm with modifiable estimate. Artificial Intelligence, 23 (1), 1327.
Pearl, J. (1984). Heuristics: Intelligent Search Strategies for Computer Problem Solving.
Addison & Wesley.
Pohl, I. (1970). Heuristic search viewed as path finding in a graph. Artificial Intelligence,
1 (3), 193204.
Pohl, I. (1977). Practical and theoretical considerations in heuristic search algorithms.
Machine Intelligence, 8, 5572.
82

fiPredicting the Performance of IDA* using Conditional Distributions

Ratner, D., & Warmuth, M. K. (1986). Finding a shortest solution for the n  n extension
of the 15-puzzle is intractable. In Proceedings of the Fifth Conference on Artificial
Intelligence (AAAI-86), pp. 168172.
Ruml, W. (2002). Adaptive Tree Search. Ph.D. thesis, Harvard University.
Sen, A. K., Bagchi, A., & Zhang, W. (2004). Average-case analysis of best-first search in
two representative directed acyclic graphs. Artificial Intelligence, 155 (1-2), 183206.
Zahavi, U., Felner, A., Burch, N., & Holte, R. C. (2008). Predicting the performance of
IDA* with conditional distributions. In Proceedings of the Twenty-Third Conference
on Artificial Intelligence (AAAI-08), pp. 381386.
Zahavi, U., Felner, A., Holte, R., & Schaeffer, J. (2006). Dual search in permutation
state spaces. In Proceedings of the Twenty-First Conference on Artificial Intelligence
(AAAI-06), pp. 10761081.
Zahavi, U., Felner, A., Holte, R. C., & Schaeffer, J. (2008). Duality in permutation state
spaces and the dual search algorithm. Artificial Intelligence, 172 (4-5), 514540.
Zahavi, U., Felner, A., Schaeffer, J., & Sturtevant, N. R. (2007). Inconsistent heuristics.
In Proceedings of the Twenty-Second Conference on Artificial Intelligence (AAAI-07),
pp. 12111216.
Zhou, R., & Hansen, E. A. (2004). Space-efficient memory-based heuristics. In Proceedings
of the Nineteenth Conference on Artificial Intelligence (AAAI-04), pp. 677682.

83

fi