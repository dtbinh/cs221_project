Journal of Artificial Intelligence Research 11 (1999) 199-240

Submitted 7/98; published 9/99

Unifying Class-Based Representation Formalisms

calvanese@dis.uniroma1.it
lenzerini@dis.uniroma1.it
nardi@dis.uniroma1.it

Diego Calvanese
Maurizio Lenzerini
Daniele Nardi

Dipartimento di Informatica e Sistemistica
Universita di Roma \La Sapienza"
Via Salaria 113, I-00198 Roma, Italy
Abstract

The notion of class is ubiquitous in computer science and is central in many formalisms
for the representation of structured knowledge used both in knowledge representation and
in databases. In this paper we study the basic issues underlying such representation formalisms and single out both their common characteristics and their distinguishing features.
Such investigation leads us to propose a unifying framework in which we are able to capture the fundamental aspects of several representation languages used in different contexts.
The proposed formalism is expressed in the style of description logics, which have been
introduced in knowledge representation as a means to provide a semantically well-founded
basis for the structural aspects of knowledge representation systems. The description logic
considered in this paper is a subset of first order logic with nice computational characteristics. It is quite expressive and features a novel combination of constructs that has not been
studied before. The distinguishing constructs are number restrictions, which generalize existence and functional dependencies, inverse roles, which allow one to refer to the inverse of
a relationship, and possibly cyclic assertions, which are necessary for capturing real world
domains. We are able to show that it is precisely such combination of constructs that makes
our logic powerful enough to model the essential set of features for defining class structures
that are common to frame systems, object-oriented database languages, and semantic data
models. As a consequence of the established correspondences, several significant extensions
of each of the above formalisms become available. The high expressiveness of the logic we
propose and the need for capturing the reasoning in different contexts forces us to distinguish between unrestricted and finite model reasoning. A notable feature of our proposal is
that reasoning in both cases is decidable. We argue that, by virtue of the high expressive
power and of the associated reasoning capabilities on both unrestricted and finite models,
our logic provides a common core for class-based representation formalisms.
1. Introduction

In many fields of computer science we find formalisms for the representation of objects and
classes (Motschnig-Pitrik & Mylopoulous, 1992). Generally speaking, an object denotes an
element of the domain of interest, and a class denotes a set of objects with common characteristics. We use the term \class-based representation formalism" to refer to a formalism
that allows one to express several kinds of relationships and constraints (e.g., subclass constraints) holding among the classes that are meaningful in a set of applications. Moreover,
class-based formalisms aim at taking advantage of the class structure in order to provide
various information, such as whether a class is consistent, i.e., it admits at least one object,
whether a class is a subclass of another class, and more generally, whether a given constraint
c 1999 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiCalvanese, Lenzerini, & Nardi

holds between a given set of classes. From the above characterization, it should be clear
that the formalisms referred to in this paper deal only with the structural aspects of objects
and classes, and do not include any features for the specification of behavioral properties of
objects.
Three main families of class-based formalisms are identified in this paper. The first one
comes from knowledge representation and in particular from the work on semantic networks
and frames (see for example Lehmann, 1992; Sowa, 1991). The second one originates in
the field of databases and in particular from the work on semantic data models (see for
example Hull & King, 1987). The third one arises from the work on types in programming
languages and object-oriented systems (see for example Kim & Lochovsky, 1989).
In the past there have been several attempts to establish relationships among the various
families of class-based formalisms (see Section 6 for a brief survey). The proposed solutions
are not fully general and a formalism capturing both the modeling constructs and the
reasoning techniques for all the above families is still missing. In this paper we address this
problem by proposing a class-based representation formalism, based on description logics
(Brachman & Levesque, 1984; Schmidt-Schau & Smolka, 1991; Donini, Lenzerini, Nardi,
& Schaerf, 1996), and by using it for comparing other formalisms.
In description logics, structured knowledge is described by means of so called concepts
and roles, which denote unary and binary predicates, respectively. Starting from a set of
atomic symbols one can build complex concept and role expressions by applying suitable
constructors which characterize a description logic. Formally, concepts are interpreted as
subsets of a domain and roles as binary relations over that domain, and all constructs
are equipped with a precise set-theoretic semantics. The most common constructs include
boolean operations on concepts, and quantification over roles. For example, the concept
Person u 8child.Male, denotes the set of individuals that are instances of the concept
Person and are connected through the role child only to instances of the concept Male,
while the concept 9child denotes all individuals that are connected through the role child
to some individual. Further constructs that have been considered important include more
general forms of quantification, number restrictions, which allow one to state limits on the
number of connections that an individual may have via a certain role, and constructs on
roles, such as intersection, concatenation and inverse. A description logic knowledge base,
expressing the intensional knowledge about the modeled domain, is built by stating inclusion
assertions between concepts, which have to be satisfied by the models of the knowledge base.
The assertions are used to specify necessary and/or necessary and sucient conditions for
individuals to be instances of certain concepts. Reasoning on such knowledge bases includes
the detection of inconsistencies in the knowledge base itself, determining whether a concept
can be populated in a model of the knowledge base, and checking subsumption, i.e., whether
all instances of a concept are necessarily also instances of another concept in all models of
the knowledge base.
In this paper we propose a description logic called aluni, which is quite expressive
and includes a novel combination of constructs, including number restrictions, inverse roles,
and inclusion assertions with no restrictions on cycles. Such features make aluni powerful
enough to provide a unified framework for frame systems, object-oriented languages, and
semantic data models. We show this by establishing a precise correspondence with a framebased language in the style of the one proposed by Fikes and Kehler (1985), with the
200

fiUnifying Class-Based Representation Formalisms

Entity-Relationship model (Chen, 1976), and with an object-oriented language in the style
of the one introduced by Abiteboul and Kanellakis (1989). More specifically, we identify
the most relevant features to model classes in each of the cited settings and show that
a specification in any of those class-based formalisms can be equivalently expressed as a
knowledge base in aluni. In this way, we are able to identify which are the commonalities
among the families and which are the specificities of each family. Therefore, even though
there are specific features of every family that are not addressed by aluni, we are able
to show that the formalism proposed in this paper provides important features that are
currently missing in each family, although their relevance has often been stressed. In this
sense, our unifying framework points out possible developments for the languages belonging
to all the three families.
One fundamental reason for regarding aluni as a unifying framework for class-based
representation formalisms is that reasoning in aluni is hard, but nonetheless decidable, as
shown by Calvanese, Lenzerini, and Nardi (1994), Calvanese (1996c). Consequently, the
language features arising from different frameworks to build class-based representations are
not just given a common semantic account, but are combined in a more expressive setting
where one retains the capability of solving reasoning tasks. The combination of constructs
included in the language makes it necessary to distinguish between reasoning with respect to
finite models, i.e., models with a finite domain, and reasoning with respect to unrestricted
models. Calvanese (1996c) devises suitable techniques for both unrestricted and finite model
reasoning, that enable for reasoning in the different contexts arising from assuming a finite
domain, as it is often the case in the field of databases, or assuming that a domain can also
be infinite. In the paper, we discuss the results on reasoning in aluni, and compare them
with other results on reasoning in class-based representation formalisms.
Summarizing, our framework provides an adequate expressive power to account for
the most significant features of the major families of class-based formalisms. Moreover, it
is equipped with suitable techniques for reasoning in both finite and unrestricted models.
Therefore, we believe that aluni captures the essential core of the class-based representation
formalisms belonging to all three families mentioned above.
The paper is organized as follows. In the next section we present our formalism and
in Sections 3, 4, and 5 we discuss three families of class-based formalisms, namely, frame
languages, semantic data models, and object-oriented data models, showing that their basic
features are captured by knowledge bases in aluni. The final sections contain a review of
related work, including a discussion of reasoning in aluni and class-based formalism, and
some concluding remarks.
2. A Unifying Class-Based Representation Language

In this section, we present aluni, a class-based formalism in the style of description logics
(DLs) (Brachman & Levesque, 1984; Schmidt-Schau & Smolka, 1991; Donini et al., 1996;
Donini, Lenzerini, Nardi, & Nutt, 1997). In DLs the domain of interest is modeled by means
of concepts and roles, which denote classes and binary relations, respectively. Generally
speaking, a DL is formed by three basic components:
 A description language, which specifies how to construct complex concept and role
expressions (also called simply concepts and roles), by starting from a set of atomic
201

fiCalvanese, Lenzerini, & Nardi
Construct

atomic concept
atomic negation
conjunction
disjunction
universal quantification
number restrictions
atomic role
inverse role

Syntax
A
:A
C1 u C2
C1 t C2
8R.C
9nR
9nR
P
P

Semantics
AI  I
I n AI
C1I \ C2I
C1I [ C2I
fo j 8o0 . (o; o0 ) 2 RI ! o0 2 C I g
fo j ]fo0 j (o; o0 ) 2 RI g  ng1
fo j ]fo0 j (o; o0 ) 2 RI g  ng
P I  I  I
f(o; o0 ) j (o0 ; o) 2 P I g

Table 1: Syntax and semantics of ALUNI
symbols and by applying suitable constructors. It is the set of allowed constructs that
characterizes the description language.
 A knowledge specification mechanism, which specifies how to construct a DL knowledge base, in which properties of concepts and roles are asserted.
 A set of basic reasoning tasks provided by the DL.
In the rest of the section we describe the specific form that these three components assume
in aluni.
2.1 The Description Language of

aluni

In the description language of aluni, called ALUNI , concepts and roles are formed according to the syntax shown in Table 1, where A denotes an atomic concept, P an atomic
role, C an arbitrary concept expression, R an arbitrary role expression, and n a nonnegative integer. To increase readability of concept expressions, we also introduce the following
abbreviations:
>  A t :A; for some atomic concept A
?  A u :A; for some atomic concept A
9R  91R
9=nR  9nR u 9nR

Concepts are interpreted as subsets of a domain and roles as binary relations over that
domain. Intuitively, :A represents the negation of an atomic concept, and is interpreted
as the complement with respect to the domain of interpretation. C1 u C2 represents the
conjunction of two concepts and is interpreted as set intersection, while C1 t C2 represents
disjunction and is interpreted as set union. Consequently, > represents the whole domain,
1. ]S denotes the cardinality of a set S .

202

fiUnifying Class-Based Representation Formalisms

and ? the empty set. 8R.C is called universal quantification over roles and is used to
denote those elements of the interpretation domain that are connected through role R only
to instances of the concept C . 9nR and 9nR are called number restrictions, and impose
on their instances restrictions on the minimum and maximum number of objects they are
connected to through role R. P , called the inverse of role P , represents the inverse of the
binary relation denoted by P .
More formally, an interpretation I = (I ; I ) consists of an interpretation domain I
and an interpretation function I that maps every concept C to a subset C I of I and
every role R to a subset RI of I  I according to the semantic rules specified in Table 1.
The sets C I and RI are called the extensions of C and R respectively.
Example 2.1 Consider the concept expression
8enrolls.Student u 92enrolls u 930 enrolls u
8teaches .(Professor t GradStudent) u 9=1teaches u
:AdvCourse

specifying the constraints for an object to be a university course. The expression reects the
fact that each course enrolls only students, and restrictions on the minimum and maximum
number of enrolled students. By using the role teaches and the inverse constructor we
can state the property that each course is taught by exactly one individual, who is either a
professor or a graduate student. Finally, negation is used to express disjointness from the
concept denoting advanced courses.
2.2 Knowledge Bases in

aluni

An aluni knowledge base, which expresses the knowledge about classes and relations of the
modeled domain, is formally defined as a triple K = (A; P ; T ), where A is a finite set of
atomic concepts, P is a finite set of atomic roles, and T is a finite set of so called inclusion
assertions. Each such assertion has the form
A _ C
where A is an atomic concept and C an arbitrary concept expression. Such an inclusion
assertion states by means of the concept C necessary properties for an element of the domain
in order to be an instance of the atomic concept A. Formally, an interpretation I satisfies
the inclusion assertion A _ C if AI  C I . An interpretation I is a model of a knowledge
base K if it satisfies all inclusion assertions in K. A finite model is a model with finite
domain.
Example 2.1 (cont.) The assertion
_ 8enrolls.Student u 92enrolls u 930enrolls u
Course 
8teaches .(Professor t GradStudent) u 9=1teaches
makes use of a complex concept expression to state necessary conditions for an object to
be an instance of the concept Course.
203

fiCalvanese, Lenzerini, & Nardi

In aluni no restrictions are imposed on the form that the inclusion assertions may
assume. In particular we do not rule out cyclic assertions, i.e., assertions in which the
concept expression on the right hand side refers, either directly or indirectly via other
assertions, to the atomic concept on the left hand side. In the presence of cyclic assertions
different semantics may be adopted (Nebel, 1991). The one defined above, called descriptive
semantics, accepts all interpretations that satisfy the assertions in the knowledge base, and
hence interprets assertions as constraints on the domain to be modeled. For inclusion
assertions, descriptive semantics has been claimed to provide the most intuitive results
(Buchheit, Donini, Nutt, & Schaerf, 1998). Alternative semantics which have been proposed
are based on fixpoint constructions (Nebel, 1991; Schild, 1994; De Giacomo & Lenzerini,
1994b), and hence allow to define in a unique way the interpretation of concepts.
In general, cycles in the knowledge base increase the complexity of reasoning (Nebel,
1991; Baader, 1996; Calvanese, 1996b) and require a special treatment by reasoning procedures (Baader, 1991; Buchheit, Donini, & Schaerf, 1993). For this reason, many DL based
systems assume the knowledge base to be acyclic (Brachman, McGuinness, Patel-Schneider,
Alperin Resnick, & Borgida, 1991; Bresciani, Franconi, & Tessaris, 1995). However, this assumption is unrealistic in practice, and cycles are definitely necessary for a correct modeling
in many application domains. Indeed, the use of cycles is allowed in all data models used
in databases, and, as shown in the following sections, in order to capture their semantics in
aluni the possibility of using cyclic assertions is fundamental.
Besides inclusion assertions, some DL based systems also make use of equivalence assertions (Buchheit et al., 1993), which express both necessary and sucient conditions for
an object to be an instance of a concept. Although this possibility is ruled out in aluni,
this does not limit its ability of capturing both frame based systems and database models,
where the constraints that can be expressed correspond naturally to inclusion assertions.
2.3 Reasoning in

aluni

The basic tasks we consider when reasoning over an aluni knowledge base are concept
consistency and concept subsumption:
 Concept consistency is the problem of deciding whether a concept C is consistent in
a knowledge base K (written as K 6j= C  ?), i.e., whether K admits a model I such
that C I 6= ;.
 Concept subsumption is the problem of deciding whether a concept C1 is subsumed by
a concept C2 in a knowledge base K (written as K j= C1  C2), i.e., whether C1I  C2I
for each model I of K.
The inclusion of number restrictions and inverse roles in ALUNI and the ability in
aluni of using arbitrary, possibly cyclic inclusion assertions allows one to construct a knowledge base in which a certain concept is consistent but has necessarily an empty extension
in all finite models of the knowledge base. Similarly, a subsumption relation between two
concepts may hold only if infinite models of the knowledge base are ruled out and only finite
models are considered.
204

fiUnifying Class-Based Representation Formalisms
Keven = (A P T ), where
A = fNumber Eveng,
P = fdoublesg,
and the set T of assertions consists of:
;

;

;

Number
Even

_ 9doubles u 8doubles .Even
_ Number u 91 doubles u 8doubles.Number

Figure 1: An aluni knowledge base with two concepts that are equivalent in all finite
models
Let Keven be the knowledge base shown in Figure 1. Intuitively, the assertions in Keven state that for each number there is an even number which doubles it, and
that all numbers which double it are even. Each even number is a number, doubles at most
one number, and doubles only numbers. Observe that for any model I of Keven , the universal quantifications together with the functionality of doubles in the assertions imply that
]EvenI  ]NumberI , while the direct inclusion assertion between Even and Number implies
that ]EvenI  ]NumberI . Therefore, the two concepts have the same cardinality, and since
one is a sub-concept of the other, if the domain is finite, their extensions coincide. This
does not necessarily hold for infinite domains. In fact, the names we have chosen suggest
already an infinite model of the knowledge base in which Number and Even are interpreted
differently. The model is obtained by taking the natural numbers as domain, and interpreting Number as the whole domain, Even as the even numbers, and doubles as the set
f(2n; n) j n  0g.
The example above shows that aluni does not have the finite model property, which
states that if a concept is consistent in a knowledge base then the knowledge base admits
a finite model in which the concept has a nonempty extension. Therefore, it is important
to distinguish between reasoning with respect to unrestricted models and reasoning with
respect to finite models. We call (unrestricted) concept consistency (written as K 6j=u C 
?) and (unrestricted) concept subsumption (written as K j=u A  C ) the reasoning tasks
as described above, i.e., carried out without restricting the attention to finite models. The
corresponding reasoning tasks carried out by considering finite models only, are called finite
concept consistency (written as K 6j=f C  ?) and finite concept subsumption (written as
K j=f A  C ).
Example 2.2 (cont.) Summing up the previous considerations, we can say that Number is
not subsumed by Even in Keven , i.e., Keven 6j=u Number  Even, but is finitely subsumed, i.e.,
Keven j=f Number  Even. Equivalently Numberu:Even is consistent in Keven , i.e., Keven 6j=u
Number u:Even  ?, but is not finitely consistent, i.e., Keven j=f Number u:Even  ?.
A distinguishing feature of aluni is that reasoning both in the finite and in the unrestricted case is decidable. In particular, unrestricted concept satisfiability and concept
subsumption are decidable in deterministic exponential time (De Giacomo & Lenzerini,
Example 2.2

205

fiCalvanese, Lenzerini, & Nardi

1994a; Calvanese et al., 1994), and since reasoning in strict sublanguages of aluni is already EXPTIME-hard (Calvanese, 1996c), the known algorithms are computationally optimal. Finite concept consistency in aluni is also decidable in deterministic exponential time
while finite concept subsumption (in the general case) is decidable in deterministic double
exponential time (Calvanese, 1996c). A more precise discussion on the methods for reasoning in aluni is provided in Section 6.2, while a detailed account of the adopted algorithms
and an analysis of their computational complexity is presented by Calvanese (1996c).
In the next sections we show how the two forms of reasoning with respect to unrestricted
and finite models, capture the reasoning tasks that are typically considered in different
formalisms for the structured representation of knowledge.
3. Frame Based Systems

Frame languages are based on the idea of expressing knowledge by means of frames, which
are structures representing classes of objects in terms of the properties that their instances
must satisfy. Such properties are defined by the frame slots, that constitute the items of a
frame definition. Since the 70s a large number of frame systems have been developed, with
different goals and different features. DLs bear a close relationship with the kl-one family
of frame systems (Woods & Schmolze, 1992). However, here we would like to consider frame
systems from a more general perspective, as discussed for example by Karp (1992), Karp,
Myers, and Gruber (1995), and establish the correspondence with aluni knowledge bases
in this context.
We remark that we are restricting our attention to those aspects that are related to
the taxonomic structure. Moreover, as discussed below, we consider assertional knowledge
bases, where intensional knowledge is characterized in terms of inclusion assertions rather
than definitions. In addition, we do not consider those features that cannot be captured in
a first-order framework, such as default values in the slots, attached procedures, and the
specification of overriding inheritance policies. Some of the issues concerning the modeling
of these aspects in DLs are addressed by Donini, Lenzerini, Nardi, Nutt, and Schaerf (1994),
Donini, Nardi, and Rosati (1995), within a modal nonmonotonic extension of DLs.
3.1 Syntax of Frame Based Systems

To make the correspondence precise, we need to fix syntax and semantics for the frame
systems we consider. Unfortunately, there is no accepted standard and we have chosen to
use here basically the notation adopted by Fikes and Kehler (1985), which is used also in
the KEE2 system.
A frame knowledge base, denoted by F , is formed by a set of frame and
slot names, and is constituted by a set of frame definitions of the following form:
Definition 3.1

Frame : F in KB F E;
2. KEE is a trademark of Intellicorp. Note that a KEE user does not directly specify her knowledge base
in this notation, but is allowed to define frames interactively via the graphical system interface.

206

fiUnifying Class-Based Representation Formalisms
: Course in KB University
: enrolls
ValueClass: Student
Cardinality.Min: 2
Cardinality.Max: 30
MemberSlot: taughtby
ValueClass: (UNION GradStudent
Professor)
Cardinality.Min: 1
Cardinality.Max: 1

Frame

: BasCourse in KB University
: Course
MemberSlot: taughtby
ValueClass: Professor

Frame

MemberSlot

SuperClasses

: Professor in

Frame

: Student in

Frame

KB

University

University

: GradStudent in KB University
: Student
MemberSlot: degree
ValueClass: String
Cardinality.Min: 1
Cardinality.Max: 1

Frame

SuperClasses

: AdvCourse in KB University
: Course
MemberSlot: enrolls
ValueClass: (INTERSECTION

Frame

SuperClasses

GradStudent
(NOT Undergrad))

KB

: Undergrad in KB University
: Student

Frame

SuperClasses

: 20

Cardinality.Max

Figure 2: A KEE knowledge base
where E is a frame expression, i.e., an expression formed according to the following syntax:
E ! SuperClasses : F1 ; : : : ; Fh
MemberSlot : S1
ValueClass : H1
Cardinality.Min : m1
Cardinality.Max : n1

MemberSlot : Sk
ValueClass : Hk
Cardinality.Min : mk
Cardinality.Max : nk
F and S denote frame and slot names, respectively, m and n denote positive integers, and
H denotes slot constraint, which can be specified as follows:
H

! Fj

(INTERSECTION H1 H2) j
(UNION H1 H2) j
(NOT H )

For readers that are familiar with the KEE system, we point out that we omit the
specification of the sub-classes for a frame present in KEE, since it can be directly derived
from the specification of the super-classes.
Example 3.2 Figure 2 shows a simple example of a knowledge base modeling the situation
at an university expressed in the frame language we have presented. The frame Course
207

fiCalvanese, Lenzerini, & Nardi

represents courses which enroll students and are taught either by graduate students or
professors. Cardinality restrictions are used to impose a minimum and maximum number
of students that may be enrolled in a course, and to express that each course is taught by
exactly one individual. The frame AdvCourse represents courses which enroll only graduate
students, i.e., students who already have a degree. Basic courses, on the other hand, may
be taught only by professors.
3.2 Semantics of Frame Based Systems

To give semantics to a set of frame definitions we resort to their interpretation in terms of
first-order predicate calculus (Hayes, 1979). According to such interpretation, frame names
are treated as unary predicates, while slots are considered binary predicates.
A frame expression E is interpreted as a predicate logic formula E (x), which has one
free variable, and consists of the conjunction of sentences, obtained from the super-class
specification and from each slot specification. In particular, for the super-classes F1 ; : : : ; Fh
we have:
F1 (x) ^ : : : ^ Fh (x)
and for a slot specification
MemberSlot : S
ValueClass : H
Cardinality.Min : m
Cardinality.Max : n
we have
8y. (S (x; y) !VH (y)) ^
9y1; : : : ; ym. (( i6=j yi 6= yj ) ^ S (x; y1 ) ^    ^ S (x; ym )) ^
8y1; : : : ; yn+1. ((S (x; y1 ) ^    ^ S (x; yn+1)) ! Wi6=j yi = yj );
under the assumption that within one frame definition the occurrences of x refer to the same
free variable. Finally the constraints on the slots are interpreted as conjunction, disjunction
and negation, respectively, i.e.:
(INTERSECTION H1 H2) is interpreted as H1(x) ^ H2(x)
(UNION H1 H2)
is interpreted as H1(x) _ H2(x)
(NOT H )
is interpreted as :H (x)
A frame definition
Frame : F in KB F E
is then considered as the universally quantified sentence of the form
8x.(F (x) ! E (x)):
The whole frame knowledge base F is considered as the conjunction of all first-order sentences corresponding to the frame definitions in F .
Here we regard frame definitions as necessary conditions, which is commonplace in the
frame systems known as assertional frame systems, as opposed to definitional systems,
where frame definitions are interpreted as necessary and sucient conditions.
208

fiUnifying Class-Based Representation Formalisms

In order to enable the comparison with our formalisms for representing structured knowledge we restrict our attention to the reasoning tasks that involve the frame knowledge base,
independently of the assertional knowledge, i.e., the frames instances. Fikes and Kehler
(1985) mention several reasoning services associated with frames, such as:
 Consistency checking, which amounts to verifying whether a frame F is satisfiable
in a knowledge base. In particular, this involves both reasoning on cardinalities and
checking whether the filler of a given slot belongs to a certain frame.
 Inheritance, which, in our case, amounts to the ability of identifying which of the
frames are more general than a given frame, sometimes called all-super-of (Karp
et al., 1995). All the properties of the more general frames are then inherited by the
more specific one. Such a reasoning is therefore based on the more general ability to
check the mutual relationhips between frame descriptions in the knowledge base.
These reasoning services are formalized in the first-order semantics as follows.
Definition 3.3 Let F be a frame knowledge base and F a frame in F . We say that F is
consistent in F if the first-order sentence F ^ 9x.F (x) is satisfiable. Moreover, we say that
a frame description E is more general than F in F if F j= 8x.(F (x) ! E (x)).
3.3 Relationship between Frame Based Systems and

aluni

The first-order semantics given above allows us to establish a straightforward relationship
between frame languages and aluni. Indeed, we now present a translation from frame
knowledge bases to aluni knowledge bases.
We first define the function  that maps each frame expression into an ALUNI concept
expression as follows:
 Every frame name F is mapped into an atomic concept (F ).
 Every slot name S is mapped into an atomic role (S ).
 Every slot constraint is mapped as follows
(UNION H1 H2)
is mapped into (H1) t (H2):
(INTERSECTION H1 H2) is mapped into (H1) u (H2):
(NOT H )
is mapped into :(H ):
 Every frame expression of the form
SuperClasses : F1 ; : : : ; Fh
MemberSlot : S1
ValueClass : H1
Cardinality.Min : m1
Cardinality.Max : n1

MemberSlot : Sk
ValueClass : Hk
Cardinality.Min : mk
Cardinality.Max : nk
209

fiCalvanese, Lenzerini, & Nardi
K = (A P T ), where
A = fCourse AdvCourse BasCourse Professor Student GradStudent Undergrad Stringg,
P = fenrolls taughtby degreeg,
and the set T of assertions consists of:
;

;

;

;

;

Course
AdvCourse
BasCourse
GradStudent
Undergrad

;

;

;

;

;

;

_ 8enrolls.Student u 92 enrolls u 930 enrolls u
8taughtby.(Professor t GradStudent) u 9=1 taughtby
_ Course u 8enrolls.(GradStudent u :Undergrad) u 920 enrolls
_ Course u 8taughtby.Professor
_ Student u 8degree.String u 9=1 degree
_ Student

Figure 3: The aluni knowledge base corresponding to the KEE knowledge base in Figure 2
is mapped into the class expression
(F1 ) u    u (Fh ) u
8(S1).(H1) u 9m (S1) u 9n (S1) u

8(Sk ).(Hk ) u 9mk (Sk ) u 9nk (Sk):
1

1

This mapping allows us to translate a frame knowledge base into an aluni knowledge base,
as specified in the following definition.
The aluni knowledge base (F ) = (A; P ; T ) corresponding to a frame
knowledge base F is obtained as follows:
 A consists of one atomic concept (F ) for each frame name F in F .
 P consists of one atomic role (S ) for each slot name S in F .
 T consists of an inclusion assertion
(F ) _ (E )
for each frame definition
Frame : F in KB F E
in F .
Definition 3.4

Example 3.2 (cont.) We illustrate the translation on the frame knowledge base in Figure 2. The corresponding aluni knowledge base is shown in Figure 3.
210

fiUnifying Class-Based Representation Formalisms

The correctness of the translation follows from the correspondence between the settheoretic semantics of aluni and the first-order interpretation of frames (see for example
Hayes, 1979; Borgida, 1996; Donini et al., 1996). We can observe that inverse roles are in
fact not necessary for the formalization of frames. Indeed, the possibility of referring to the
inverse of a slot has been rarely considered in frame knowledge representation systems (Some
exceptions are reported in Karp, 1992). Due to the absence of inverse roles the distinction
between reasoning in finite and unrestricted models is not necessary3 . Consequently, all
the above mentioned forms of reasoning are captured by unrestricted concept consistency
and concept subsumption in aluni knowledge bases. This is summarized in the following
theorem.
Theorem 3.5 Let F be a frame knowledge-base, F be a frame in F , E be a frame description, and (F ), (F ), and (E ) be their translations in aluni. Then the following
hold:

 F is consistent in F if and only if (F ) 6j=u (F )  ?.
 E is more general than F in F if and only if (F ) j=u (F )  (E ).

The claim directly follows from the semantics of frame knowledge bases and the
translation into DLs that we have adopted.
By Theorem 3.5 it becomes possible to exploit the methods for unrestricted reasoning
on aluni knowledge bases in order to reason on frame knowledge bases. Since the problem
of reasoning, e.g., in KEE is already EXPTIME-complete, we do not pay in terms of computational complexity for the expressiveness added by the constructs of aluni. In fact, by
resorting to the correspondence with aluni it becomes possible to add to frame systems
useful features, such as the possibility of specifying the inverse of a slot (Karp, 1992), and
still retain the ability to reason in EXPTIME.
Proof.

4. Semantic Data Models

Semantic data models were introduced primarily as formalisms for database schema design.
They provide a means to model databases in a much richer way than traditional data
models supported by Database Management Systems, and are becoming more and more
important because they are adopted in most of the recent database design methodologies
and Computer Aided Software Engineering tools.
The most widespread semantic data model is the Entity-Relationship (ER) model introduced by Chen (1976). It has by now become a standard, extensively used in the design
phase of commercial applications. In the commonly accepted ER notation, classes are called
entities and are represented as boxes, whereas relationships between entities are represented
as diamonds. Arrows between entities, called ISA relationships, represent inclusion assertions. The links between entities and relationships represent the ER-roles, to which number
restrictions are associated. Dashed links are used whenever such restrictions are refined for
more specific entities. Finally, elementary properties of entities are modeled by attributes,
3. If we eliminate from

ALUNI inverse roles, then the resulting DL has the finite model property.
211

fiCalvanese, Lenzerini, & Nardi

whose values belong to one of several predefined domains, such as Integer, String, or
Boolean.
The ER model does not provide constructs for expressing explicit disjointness or disjunction of entities, but extensions of the model allow for the use of generalization hierarchies
which represent a combination of these two constructs. In order to keep the presentation simple, we do not consider generalization hierarchies in the formalization we provide,
although their addition would be straightforward. Similarly, we omit attributes of relations.
We now show that all relevant aspects of the ER model can be captured in aluni, and
thus that reasoning on an ER schema can be reduced to reasoning on the corresponding
aluni knowledge base. Since aluni is equipped with capabilities to reason on knowledge
bases, both with respect to finite and unrestricted models (see Section 6.2), the reduction
shows that reasoning on the ER model, and more generally on semantic data models, is
decidable.
As in the case of frame-based systems, we restrict our attention to those aspects that
constitute the core of the ER model. For this reason we do not consider some features,
such as keys and weak entities, that have been introduced in the literature (Chen, 1976),
but appear only in some of the formalizations of the ER model and the methodologies for
conceptual modeling based on the model. A proposal for the treatment of keys in description
logics is presented by Borgida and Weddell (1997).
In order to establish the correspondence between the ER model and aluni, we present
formal syntax and semantics of ER schemata.
4.1 Syntax of the Entity-Relationship Model

Although the ER model has by now become an industrial standard, several variants and
extensions have been introduced, which differ in minor aspects in expressiveness and in
notation (Chen, 1976; Teorey, 1989; Batini, Ceri, & Navathe, 1992; Thalheim, 1992, 1993).
Also, ER schemata are usually defined using a graphical notation which is particularly
useful for an easy visualization of the data dependencies, but which is not well suited for our
purposes. Therefore we have chosen a formalization of the ER model which abstracts with
respect to the most important characteristics and allows us to develop the correspondence
to aluni.
In the following, for two finite sets X and Y we call a function from a subset of X
to Y an X -labeled tuple over Y . The labeled tuple T that maps xi 2 X to yi 2 Y , for
i 2 f1; : : : ; kg, is denoted [x1 : y1 ; : : : ; xk : yk ]. We also write T [xi ] to denote yi .
An ER schema is a tuple S = (LS ; S ; att S ; rel S ; card S ), where
 LS is a finite alphabet partitioned into a set ES of entity symbols, a set AS of attribute
symbols, a set US of role symbols, a set RS of relationship symbols, and a set DS of
domain symbols; each domain symbol D has an associated predefined basic domain
DBD , and we assume the various basic domains to be pairwise disjoint.
 S  ES  ES is a binary relation over ES .
 att S is a function that maps each entity symbol in ES to an AS -labeled tuple over DS .

Definition 4.1

212

fiUnifying Class-Based Representation Formalisms

is a function that maps each relationship symbol in RS to an US -labeled tuple
over ES . We assume without loss of generality that:
{ Each role is specific to exactly one relationship, i.e., for two relationships
R; R0 2 RS with R 6= R0 , if rel S (R) = [U1 : E1 ; : : : ; Uk : Ek ] and rel S (R0 ) =
[U10 : E10 ; : : : ; Uk0 0 : Ek0 0 ], then fU1; : : : ; Uk g and fU10 ; : : : ; Uk0 0 g are disjoint.
{ For each role U 2 US there is a relationship R and an entity E such that
rel S (R) = [: : : ; U : E; : : :].
 card S is a function from ES  RS  US to IN0  (IN0 [ f1g) that satisfies the following condition: for a relationship R 2 RS such that rel S (R) = [U1 : E1 ; : : : ; Uk : Ek ],
card S (E; R; U ) is defined only if U = Ui for some i 2 f1; : : : ; kg, and if E S Ei
(where S denotes the reexive transitive closure of S ). The first component
of card S (E; R; U ) is denoted with cmin S (E; R; U ) and the second component with
cmax S (E; R; U ). If not stated otherwise, cmin S (E; R; U ) is assumed to be 0 and
cmax S (E; R; U ) is assumed to be 1.
Before specifying the formal semantics of ER schemata we give an intuitive description of
the components of a schema. The relation S models the ISA-relationship between entities.
We do not need to make any special assumption on the form of S such as acyclicity
or injectivity. The function att S is used to model attributes of entities. If for example
att S associates the AS -labeled tuple [A1 : Integer; A2 : String] to an entity E , then E has
two attributes A1 ; A2 whose values are integers and strings respectively. For simplicity we
assume attributes to be single-valued and mandatory, but we could easily handle also multivalued attributes with associated cardinalities. The function rel S associates a set of roles
to each relationship symbol R, determining implicitly also the arity of R, and for each role
U in such set a distinguished entity, called the primary entity for U in R. In a database
satisfying the schema only instances of the primary entity are allowed to participate in
the relationship via the role U . The function card S specifies cardinality constraints, i.e.,
constraints on the minimum and maximum number of times an instance of an entity may
participate in a relationship via some role. Since such constraints are meaningful only if
the entity can effectively participate in the relationship, the function is defined only for
the sub-entities of the primary entity. The special value 1 is used when no restriction is
posed on the maximum cardinality. Such constraints can be used to specify both existence
dependencies and functionality of relations (Cosmadakis & Kanellakis, 1986). They are
often used only in a restricted form, where the minimum cardinality is either 0 or 1 and
the maximum cardinality is either 1 or 1. Cardinality constraints in the form considered
here have been introduced already by Abrial (1974) and subsequently studied by Grant
and Minker (1984), Lenzerini and Nobili (1990), Ferg (1991), Ye, Parent, and Spaccapietra
(1994), Thalheim (1992).
Example 4.2 Figure 4 shows a simple ER schema modeling a state of affairs similar to the
one represented by the KEE knowledge base in Figure 2. We have used the standard graphic
notation for ER schemata, except for the dashed link, which represents the refinement of
a cardinality constraint for the participation of a sub-entity (in our case AdvCourse) in a
relationship (in our case ENROLLING).


rel S

213

fiCalvanese, Lenzerini, & Nardi

Tof

(1,1)

Course

6

Ein

(2,30)

TEACHING

ENROLLING

Tby

(0,1)

Eof

(4,6)

Teacher

Student

6

(2,20)
AdvCourse

degree/String

GradStudent

Figure 4: An ER schema
4.2 Semantics of the Entity-Relationship Model

The semantics of an ER schema can be given by specifying which database states are
consistent with the information structure represented by the schema. Formally, a database
state B corresponding to an ER schema S = (LS ; S ; att S ; rel S ; card S ) is constituted by a
nonempty finite set B , assumed to be disjoint from all basic domains, and a function B
that maps
 every domain symbol D 2 DS to the corresponding basic domain DBD ,
 every entity E 2 ES to a subset E B of B ,
 every attribute A 2 AS to a set AB  B  SD2DS DBD , and
 every relationship R 2 RS to a set RB of US -labeled tuples over B .
The elements of E B , AB , and RB are called instances of E , A, and R respectively.
A database state is considered acceptable if it satisfies all integrity constraints that are
part of the schema. This is captured by the definition of legal database state.
Definition 4.3 A database state B is said to be legal for an ER schema S =
(LS ; S ; att S ; rel S ; card S ), if it satisfies the following conditions:
 For each pair of entities E1; E2 2 ES such that E1 S E2, it holds that E1B  E2B .
 For each entity E 2 ES , if att S (E ) = [A1 : D1 ; : : : ; Ah : Dh], then for each instance
e 2 E B and for each i 2 f1; : : : ; hg the following holds:
{ there is exactly one element ai 2 ABi whose first component is e, and
{ the second component of ai is an element of DiBD .
 For each relationship R 2 RS such that rel S (R) = [U1 : E1 ; : : : ; Uk : Ek ], all instances
of R are of the form [U1: e1 ; : : : ; Uk : ek ], where ei 2 EiB , i 2 f1; : : : ; kg.
214

fiUnifying Class-Based Representation Formalisms
Number

(1,1)

6

Even

DOUBLES

(0,1)

Figure 5: The ER schema corresponding to Example 2.2
 For each relationship R 2 RS such that rel S (R) = [U1 : E1 ; : : : ; Uk : Ek ], for each
i 2 f1; : : : ; kg, for each entity E 2 ES such that E S Ei and for each instance e of E
in I , it holds that
cmin S (E; R; Ui )  ]fr 2 RB j r[Ui ] = eg  cmax S (E; R; Ui ):

Notice that the definition of database state reects the usual assumption in databases
that database states are finite structures (see also Cosmadakis, Kanellakis, & Vardi, 1990).
In fact, the basic domains are not required to be finite, but for each legal database state
for a schema, only a finite set of values from the basic domains are actually of interest. We
define the active domain Bact of a database state B as the set of all elements of the basic
domains DBD , D 2 DS , that effectively appear as values of attributes in B. More formally:
Bact = fd 2 DBD j D 2 DS ^ 9A 2 AS ; e 2 B . (e; d) 2 AB g:
Since B is finite and AS contains only a finite number of attributes, which are functional
by definition, also Bact is finite.
Reasoning in the ER model includes verifying entity satisfiability and deducing inheritance. Entity satisfiability amounts to checking whether a given entity can be populated in
some legal database state (Atzeni & Parker Jr., 1986; Lenzerini & Nobili, 1990; Di Battista
& Lenzerini, 1993), and corresponds to the notion of concept consistency in DLs. Deducing
inheritance amounts to verifying whether in all databases that are legal for the schema,
every instance of an entity is also an instance of another entity. Such implied ISA relationships can arise for different reasons. Either trivially, through the transitive closure of the
explicit ISA relationships present in the schema, or in more subtle ways, through specific
patterns of cardinality restrictions along cycles in the schema and the requirement of the
database state to be finite (Lenzerini & Nobili, 1990; Cosmadakis et al., 1990).
Figure 5 shows an ER schema modeling the same situation as the knowledge
base of Example 2.2. Arguing exactly as in that example we can conclude that the two
entities Number and Even denote the same set of elements in every finite database legal for
the schema, although the ISA relation from Number to Even is not stated explicitly. It is
implied, however, due to the cycle involving the relationship and the two entities and due
to the particular form of cardinality constraints.
Example 4.4

215

fiCalvanese, Lenzerini, & Nardi
4.3 Relationship between Entity-Relationship Schemata and

aluni

We now show that the different forms of reasoning on ER schemata are captured by finite
concept consistency and finite concept subsumption in aluni. The correspondence between
the two formalisms is established by defining a translation  from ER schemata to aluni
knowledge bases, and then proving that there is a correspondence between legal database
states and finite models of the derived knowledge base.
Definition 4.5 Let S = (LS ; S ; att S ; rel S ; card S ) be an ER schema. The aluni knowledge base (S ) = (A; P ; T ) is defined as follows:
The set A of atomic concepts of (S ) contains the following elements:
 for each domain symbol D 2 DS , an atomic concept (D);
 for each entity E 2 ES , an atomic concept (E );
 for each relationship R 2 RS , an atomic concept (R).
The set P of atomic roles of (S ) contains the following elements:
 for each attribute A 2 AS , an atomic role (A);
 for each relationship R 2 RS such that rel S (R) = [U1 : E1 ; : : : ; Uk : Ek ], k atomic roles
(U1 ); : : : ; (Uk ).
The set T of assertions of (S ) contains the following elements:
 for each pair of entities E1; E2 2 ES such that E1 S E2, the assertion
(E1 ) _ (E2 )
(1)
 for each entity E 2 ES such that att S (E ) = [A1 : D1 ; : : : ; Ah: Dh ], the assertion
(E ) _ 8(A1 ).(D1 ) u    u 8(Ah ).(Dh ) u 9=1(A1 ) u    u 9=1 (Ah ) (2)
 for each relationship R 2 RS such that rel S (R) = [U1 : E1 ; : : : ; Uk : Ek ], the assertions
(R) _ 8(U1 ).(E1 ) u    u 8(Uk ).(Ek ) u 9=1 (U1 ) u    u 9=1(Uk ) (3)
(Ei ) _ 8((Ui )) .(R);
i 2 f1; : : : ; kg
(4)
 for each relationship R 2 RS such that rel S (R) = [U1 : E1 ; : : : ; Uk : Ek ], for i 2
f1; : : : ; kg, and for each entity E 2 ES such that E S Ei,
{ if m = cmin S (E; R; Ui ) 6= 0, the assertion
(E ) _ 9m ((Ui )) :
(5)
{ if n = cmax S (E; R; Ui ) 6= 1, the assertion
(E ) _ 9n ((Ui )) :
(6)
 for each pair of symbols X1 ; X2 2 ES [RS [DS such that X1 6= X2 and X1 2 RS [DS ,
the assertion
(X1 ) _ :(X2 ):
(7)
216

fiUnifying Class-Based Representation Formalisms
K = (A P T ), where
A = fCourse AdvCourse Teacher Student GradStudent TEACHING ENROLLING Stringg,
P = fTof Tby Ein Eof degreeg,
and the set T of assertions consists of:
;

;

;

;

;

;

;

;

;

;

;

;

;

TEACHING
ENROLLING
Course
AdvCourse
Teacher
Student
GradStudent

_ 8Tof.Course u 9=1 Tof u
8Tby.Teacher u 9=1 Tby
_ 8Ein.Course u 9=1 Ein u
8Eof.Student u 9=1 Eof
_ 8Tof .TEACHING u 9=1 Tof u
8Ein .ENROLLING u 92 Ein u 930 Ein
_ Course u 920 Ein
_ 8Tby .TEACHING
_ 8Eof .ENROLLING u 94 Eof u 96 Eof
_ Student u 8degree.String u 9=1 degree

:

Figure 6: The aluni knowledge base corresponding to the ER schema in Figure 4
We illustrate the translation on the ER schema of Figure 4. The
knowledge base that captures exactly its semantics is shown in Figure 6, where for
brevity the disjointness assertions (7) are omitted, and assertions with the same concept on
the left hand side are collapsed.
The translation makes use of both inverse attributes and number restrictions to capture
the semantics of ER schemata. We observe that, by means of the inverse constructor, a
binary relationship could be treated in a simpler way by choosing a traversal direction and
mapping the relationship directly to a role. Notice also that the assumption of acyclicity
of the resulting knowledge base is unrealistic in this case, and in order to exploit the correspondence for reasoning in the ER model, we need techniques that can deal with inverse
attributes, number restrictions, and cycles together. As shown in Example 2.2, the combination of these factors causes the finite model property to fail to hold, and we need to
resort to reasoning methods for finite models.
In fact, we can reduce reasoning in the ER model to finite model reasoning in aluni
knowledge bases. For this purpose we define a mapping between database states corresponding to an ER schema and finite interpretations of the knowledge base derived from it.
Due to the possible presence of relations with arity greater than 2, this mapping is however
not one-to-one and we first need to characterize those interpretations of the knowledge base
that directly correspond to database states.
Definition 4.6 Let S = (LS ; S ; att S ; rel S ; card S ) be an ER schema and (S ) be defined
as above. An interpretation I of (S ) is relation-descriptive, if for every relationship R 2
RS , with rel S (R) = [U1 : E1 ; : : : ; Uk : Ek ], for every d; d0 2 ((R))I , we have that
^
(8)
( 8d00 2 I . ((d; d00 ) 2 ((Ui ))I $ (d0 ; d00 ) 2 ((Ui ))I )) ! d = d0 :
Example 4.2 (cont.)
aluni

1ik

217

fiCalvanese, Lenzerini, & Nardi

Intuitively, the extension of a relationship in a database state is a set of labeled tuples,
and such a set does not contain the same element twice. Therefore it is implicit in the
semantics of the ER model that there cannot be two labeled tuples connected through all
roles of the relationship to exactly the same elements of the domain. In a model of the
aluni knowledge base corresponding to the ER schema, on the other hand, each tuple is
represented by a new individual, and the above condition is not implicit anymore. It also
cannot be imposed in aluni by suitable assertions. The following lemma, however, shows
that we do not need such an explicit condition, when we are interested in reasoning on an
aluni knowledge base corresponding to an ER schema. This is due to the fact that we can
always restrict ourselves to considering only relation-descriptive models.
S be an ER schema, (S ) be the aluni knowledge base obtained from S
according to Definition 4.5, and C be a concept expression of (S ). If C is finitely consistent
in (S ), then there is a finite relation-descriptive model I of (S ) such that C I 6= ;.
Lemma 4.7 Let

Let I0 be a finite model of (S ) such that C I 6= ;. We can build a finite relationdescriptive model I 0 by starting from I0 and applying the following construction once for
each relationship in RS .
Let I be the model obtained in the previous step and let R 2 RS with rel S (R) =
[U1 : E1 ; : : : ; Uk : Ek ] be the next relationship to which we apply the construction. We construct from I a model IR such that condition 8 is satisfied for relationship R.
Given an individual r 2 ((R))I , we denote by Ui(d), i 2 f1; : : : ; kg the (unique)
individual e such that (r; e) 2 ((Ui ))I . For ei 2 ((Ei ))I , i 2 f1; : : : ; kg we define
X(U :e ;:::;Uk :ek ) = fr 2 ((R))I j Ui (d) = ei ; for i 2 f1; : : : ; kgg. We call conict-set
a set X(U :e ;:::;Uk:ek ) with more than one element. From each conict-set X(U :e ;:::;Uk:ek)
we randomly choose one individual r, and we say that the others induce a conict on
(U1 : e1 ; : : : ; Uk : ek ). We call Conf the (finite) set of all objects inducing a conict on some
(U1 : e1 ; : : : ; Uk : ek ).
We define an interpretation I2Conf as the disjoint union of 2]Conf copies of I , one copy,
denoted by IZ , for every set Z 2 2Conf . We denote by dZ the copy in IZ of the individual
d in I . Since the disjoint union of two models of an aluni knowledge base is again a
model, I2Conf is a model of (S ). Let IZ and IZ 0 be two copies of I in I2Conf . We call
exchanging Uk (rZ ) with Uk (rZ 0 ) the operation on I2Conf consisting of replacing in ((Uk ))IZ
the pair (rZ ; Uk (rZ )) with (rZ ; Uk (rZ 0 )) and, at the same time, replacing in ((Uk ))IZ0 the
pair (rZ 0 ; Uk (rZ 0 )) with (rZ 0 ; Uk (rZ )). Intuitively, by exchanging Uk (rZ ) with Uk (rZ 0 ), the
individuals rZ and rZ 0 do not induce conicts anymore.
We construct now from I2Conf an interpretation IR as follows: For each r 2 Conf and
for each Z 2 2Conf such that r 2 Z , we exchange Uk (rZ ) with Uk (rZnfrg ). It is possible
to show that all conicts are thus eliminated while no new conict is created. Hence, in
IR, condition 8 for R is satisfied. We still have to show that IR is a model of (S ) in
which C IR 6= ;. Indeed, it is straightforward to check by induction that for every concept
expression C 0 appearing in (S ), for all Z 2 2Conf , d 2 C 0I if and only if dZ 2 C 0IR . Thus
all assertions of (S ) are still satisfied in IR and C IR 6= ;.
Proof.

1 1

1 1

1 1

218

fiUnifying Class-Based Representation Formalisms

With this result, the following correspondence between legal database states for an
ER schema and relation-descriptive models of the resulting aluni knowledge base can be
established.
Proposition 4.8 For every ER schema S = (LS ; S ; att S ; rel S ; card S ) there exist two
mappings ffS , from database states corresponding to S to finite interpretations of its translation (S ), and fiS , from finite relation-descriptive interpretations of (S ) to database states
corresponding to S , such that:

1. For each legal database state B for S , ffS (B) is a finite model of (S ), and for each
symbol X 2 ES [ AS [ RS [ DS , X B = ((X ))ffS (B) .

2. For each finite relation-descriptive model I of (S ), fiS (I ) is a legal database state for
S , for each entity E 2 ES , ((E ))I = E fiS (I ) , and for each symbol X 2 AS [RS [DS ,
](X )I = ]X fiS (I ) .

Proof.

(1) Given a database state B, we define the interpretation I = ffS (B) of (S ) as

follows:
 I = B [ Bact [ SR2RS RB .
 For each symbol X 2 ES [ AS [ RS [ DS ,
((X ))I = X B :

 For each relationship R 2 RS such that rel S (R) = [U1 : E1 ; : : : ; Uk : Ek ],
((Ui ))I = f(r; e) 2 I  I j r 2 RB ; and r[Ui] = eg; i 2 f1; : : : ; kg:

(9)
(10)

Let B be a legal database state. To prove claim (1) it is sucient to show that I satisfies
every assertion in (S ). Assertions 1 are satisfied since B satisfies the set inclusion between
the extensions of the corresponding entities. With respect to assertions 2, let E 2 ES be an
entity such that att S (E ) = [A1 : D1 ; : : : ; Ah : Dh], and consider an instance e 2 ((E ))I . We
have to show that for each i 2 f1; : : : ; hg, there is exactly one element ei 2 I such that
(e; ei ) 2 ((Ai ))I , and moreover that ei 2 ((Di ))I . By 9, e 2 E B , and by definition of legal
database state there is exactly one element ai 2 ABi = ((Ai ))BI whose first component is e.
Moreover, the second component ei of ai is an element of Di D = ((Di ))I . With respect
to assertions 3, let R 2 RS be a relationship such that rel S (R) = [U1 : E1 ; : : : ; Uk : Ek ],
and consider an instance r 2 ((R))I . We have to show that for each i 2 f1; : : : ; kg
there is exactly one element ei 2 I such that (r; ei ) 2 ((Ui ))I , and that moreover
ei 2 ((Ei ))I . By 9, r 2 RB , and by definition of legal database state, r is a labeled tuple
of the form [U1 : e01 ; : : : ; Uk : e0k ], where e0i 2 EiB , i 2 f1; : : : ; kg. Therefore r is a function
defined on fU1 ; : : : ; Uk g, and by 10, ei is unique and equal to e0i. Moreover, again by 9,
ei 2 ((Ei ))I = EiB . Assertions 4 are satisfied, since by 10 the first component of each
element of ((Ui ))I is always an element of RB = ((R))I . With respect to assertions 5,
let R 2 RS be a relationship such that rel S (R) = [U1 : E1 ; : : : ; Uk : Ek ], let E 2 ES be an
entity such that E S Ei, for some i 2 f1; : : : ; kg, and such that m = cmin S (E; R; Ui ) 6= 0.
219

fiCalvanese, Lenzerini, & Nardi

Consider an instance e 2 ((E ))I . We have to show that there are at least m pairs in
((Ui ))I that have e as their second component. Since assertions 4 are satisfied we know
that the first component of all such pairs is an instance of (R). By 9 and by definition
of legal database state, there are at least m labeled tuples in RB whose Ui component is
equal to e. By 10, ((Ui ))I contains at least m pairs whose second component is equal to
e. With respect to assertions 6 we can proceed in a similar way. Finally, assertions 7 are
satisfied since first, by definition the basic domains are pairwise disjoint and disjoint from
B and from the set of labeled tuples, second, no element of B is a labeled tuple, and
third, labeled tuples corresponding to different relationships cannot be equal since they are
defined over different sets of roles.
(2) Let I be a finite relation-descriptive interpretation of (S ). For each basic domain
D 2 DS , let fiD be a function from I to DBD that is one-to-one and onto. Since I
is finite and each basic domain contains a countable number of elements, such a function
always exists. In order to define fiS (I ) we first specify a mapping fi that associates to
each individual d 2 I an element as follows:
 If d 2 ((E ))I for some entity E 2 ES , then fi (d) = d.
 If d 2 ((R))I for some relationship R 2 RS with rel S (R) = [U1 : E1 ; : : : ; Uk : Ek ], and
there are individuals d1; : : : ; dk 2 I such that (d; di ) 2 ((Ui ))I , for i 2 f1; : : : ; kg,

then fi (d) = [U1 : d1 ; : : : ; Uk : dk ].

 If d 2 ((D))I for some basic domain D 2 DS , then fi (d) = fiD (d).
 Otherwise fi (d) = d.

For a pair of individuals (d1 ; d2 ) 2 I  I , fi((d1 ; d2 )) = (fi (d1 ); fi (d2 )), and for a set
X , fi (X ) = ffi (x) j x 2 X g.
If I is a model of (S ) the above rules define fi(d) for every d 2 I . Indeed, by
assertions 7, each d 2 I can be an instance of at most one atomic concept corresponding
to a relationship or basic domain, and if this is the case it is not an instance of any atomic
concept corresponding to an entity. Moreover, if d 2 ((R))I for some relationship R 2 RS
with rel S (R) = [U1: E1 ; : : : ; Uk : Ek ], then by assertions 3, for each i 2 f1; : : : ; kg there is
exactly one element di 2 I such that (d; di ) 2 ((Ui ))I . If I is not a model of (S ) and
for some d 2 I , fi(d) is not uniquely determined, then we choose nondeterministically
one possible value.
We can now define the database state B = fiS (I ) corresponding to I :
 B = I n

S



I S
I
R2RS ((R)) [ D2DS ((D )) .

 For each symbol X 2 ES [ AS [ RS [ DS , X B = fi(((X ))I ).

It is not dicult to see, that if I is a model of (S ), then B defined in such a way is a legal
database state for S with active domain SD2DS ((D))I .
220

fiUnifying Class-Based Representation Formalisms

The following theorem allows us to reduce reasoning on ER schemata to finite model
reasoning on aluni knowledge bases.
Theorem 4.9 Let S be an ER schema, E; E 0 be two entities in S , and (S ) be the translation of S . Then the following holds:

S if and only if (S ) 6j=f (E )  ?.
2. E inherits from E 0 in S if and only if (S ) j=f (E )  (E 0 ).
B 6 ;. By part 1 of Proposition 4.8,
Proof. (1) \)" Let B be a legal database state with E =
ff
(
B
)
S
ffS (B) is a finite model of (S ) in which ((E ))
=6 ;.
\(" Let (E ) be finitely consistent in (S ). By Lemma 4.7 there is a finite relationdescriptive model I of (S ) with (E )I =6 ;. By part 2 of Proposition 4.8, fiS (I ) is a
database state legal for S in which E B =6 ;.
(2) \)" Let (S ) 6j=f (E )  (E 0 ). Then (E ) u:(E 0 ) is finitely consistent in (S ).
By Lemma 4.7 there is a finite relation-descriptive model I of (S ) with d 2 ((E ))I and
d 62 ((E 0 ))I , for some d 2 I . By part 2 of Proposition 4.8, fiS (I ) is a database state legal
for S in which d 2 E B and d 62 E 0B . Therefore E does not inherit from E 0.
\(" Assume E does not inherit from E 0. Then there is a database state B legal
for S where for an instance e 2 E B we have e 62 E 0B . By part 1 of Proposition 4.8,
ffS (B) is a finite model of (S ) in which e 2 ((E ))ffS (B) and e 62 ((E 0 ))ffS (B) . Therefore
(S ) 6j=f (E )  (E 0 ).
1. E is satisfiable in

Theorem 4.9 allows us to effectively exploit the reasoning methods that have been developed for aluni in order to reason on ER schemas. The complexity of the resulting method
for reasoning on ER schemata is exponential. Observe however, that the known algorithms
for reasoning on ER schemata are also exponential (Calvanese & Lenzerini, 1994b), and
that the precise computational complexity of the problem is still open.
Moreover, by exploiting the correspondence with aluni, it becomes possible to add to
the ER model (and more in general to semantic data models) several features and modeling
primitives that are currently missing, and which have been considered important, and fully
take them into account when reasoning over schemata. Such additional features include for
example the possibility to specify and use arbitrary boolean combinations of entities, and
to refine properties of entities along ISA hierarchies.
5. Object-Oriented Data Models

Object-oriented data models have been proposed with the goal of devising database formalisms that could be integrated with object-oriented programming systems (Kim, 1990).
They are the subject of an active area of research in the database field, and are based on
the following features:
 They rely on the notion of object identifier at the extensional level (as opposed to
traditional data models which are value-oriented) and on the notion of class at the
intensional level.
221

fiCalvanese, Lenzerini, & Nardi

 The structure of the classes is specified by means of typing and inheritance.

As in the previous section, we present the common basis of object-oriented data models
with other class-based formalisms by introducing a language for specifying object-oriented
schemata and show that such schemata can be correctly represented as aluni knowledge
bases. In our analysis, we concentrate our attention on the structural aspects of objectoriented data models. One of the characteristics of the object-oriented approach is to provide
mechanisms for specifying also the dynamic properties of classes and objects, typically
through the definition of methods associated to the classes. Those aspects are outside the
scope of our investigations. Nevertheless, we argue that general techniques for schema level
reasoning, in particular, type consistency and type inference, can be profitably exploited for
restricted forms of reasoning on methods (Abiteboul, Kanellakis, Ramaswamy, & Waller,
1992).
5.1 Syntax of an Object-Oriented Model

Below we define a simple object-oriented language in the style of most popular models
featuring complex objects and object identity. Although we do not refer to any specific
formalism, our model is inspired by the ones presented by Abiteboul and Kanellakis (1989),
Hull and King (1987).
Definition 5.1 An object-oriented schema is a tuple S = (CS ; AS ; DS ), where:
 CS is a finite set of class names, denoted by the letter C .
 AS is a finite set of attribute names, denoted by the letter A.
 DS is a finite set of class declarations of the form
Class C is-a C1 ; : : : ; Ck type-is T;
in which T denotes a type expression built according to the following syntax:
T

! Cj

Union T1 ; : : : ; Tk End j
Set-of T j
Record A1: T1 ; : : : ; Ak : Tk End:
DS contains exactly one such declaration for each class C 2 CS .
Figure 7 shows a fragment of the object-oriented schema corresponding to
the KEE knowledge base of Figure 2.
Each class declaration imposes constraints on the instances of the class it refers to. The
is-a part of a class declaration allows one to specify inclusion between the sets of instances of
the involved classes, while the type-is part specifies through a type expression the structure
assigned to the objects that are instances of the class.
Example 5.2

222

fiUnifying Class-Based Representation Formalisms
Class Teacher type-is
Union Professor, GradStudent
End
Class GradStudent is-a Student type-is
Record
degree: String
End

Class Course type-is
Record
enrolls: Set-of Student,
taughtby: Teacher
End

Figure 7: An object-oriented schema
5.2 Semantics of an Object-Oriented Model

The meaning of an object-oriented schema is given by specifying the characteristics of an
instance of the schema. The definition of instance makes use of the notions of object
identifier and value.
Let us first characterize the set of values that can be constructed from a set of symbols,
called object identifiers. Given a finite set O of symbols denoting real world objects, the set
VO of values over O is inductively defined as follows:
 O  VO .
 If v1 ; : : : ; vk 2 VO then fjv1 ; : : : ; vk jg 2 VO .
 If v1 ; : : : ; vk 2 VO then [ A1 : v1 ; : : : ; Ak : vk ] 2 VO .
 Nothing else is in VO .
A database instance J of a schema S = (CS ; AS ; DS ) is constituted by
 a finite set OJ of object identifiers;
 a mapping J assigning to each class in CS a subset of OJ ;
 a mapping J assigning a value in VOJ to each object in OJ .
Although the set VOJ of values that can be constructed from a set OJ of object identifiers
is infinite, for a database instance one needs only to consider a finite subset of VOJ .
Definition 5.3 Given an object-oriented schema S and an instance J of S , the set VJ of
active values with respect to J is constituted by:
 the set OJ of object identifiers.
 the set of values assigned by J to the elements of OJ , including those values that
are not explicitly associated with object identifiers, but are used to form other values.
The interpretation of type expressions in J is defined through an interpretation function J that assigns to each type expression a subset of VOJ such that the following conditions are satisfied:
C J =  J (C )
223

fiCalvanese, Lenzerini, & Nardi

(Union T1 ; : : : ; Tk End)J = T1J [    [ TkJ
(Set-of T )J = ffjv1 ; : : : ; vk jg j k  0; vi 2 T J ; for i 2 f1; : : : ; kgg
(Record A1 : T1 ; : : : ; Ak : Tk End)J = f[ A1 : v1 ; : : : ; Ah : vh] j h  k;
vi 2 TiJ ; for i 2 f1; : : : ; kg;
vj 2 VOJ ; for j 2 fk + 1; : : : ; hgg:
Notice that the instances of type record may have more components than those specified in
the type of the class. Thus we are using an open semantics for records, which is typical of
object-oriented data models (Abiteboul & Kanellakis, 1989).
In order to characterize object-oriented data models we consider the instances that are
admissible for the schema.
Definition 5.4 Let S = (CS ; AS ; DS ) be an object-oriented schema. A database instance
J of S is said to be legal (with respect to S ) if for each declaration
Class C is-a C1 ; : : : ; Cn type-is T
in DS , it holds that C J  CiJ for each i 2 f1; : : : ; ng, and that J (C J )  T J .
Therefore, for a legal database instance, the type expressions that are present in the
schema determine the (finite) set of active values that must be considered. The construction
of such values is limited by the depth of type expressions.
5.3 Relationship between Object-Oriented Schemata and

aluni

We establish now a relationship between aluni and the object-oriented language presented
above. This is done by providing a mapping from object-oriented schemata into aluni
knowledge bases. Since the interpretation domain for aluni knowledge bases consists of
atomic objects, whereas each instance of an object-oriented schema is assigned a possibly
structured value (see the definition of VO ), we need to explicitly represent some of the
notions that underlie the object-oriented language. In particular, while there is a correspondence between concepts and classes, one must explicitly account for the type structure
of each class. This can be accomplished by introducing in aluni concepts AbstractClass,
to represent the classes, and RecType and SetType to represent the corresponding types.
The associations between classes and types induced by the class declarations, as well as the
basic characteristics of types, are modeled by means of roles: the (functional) role value
models the association between classes and types, and the role member is used for specifying
the type of the elements of a set. Moreover, the concepts representing types are assumed to
be mutually disjoint, and disjoint from the concepts representing classes. These constraints
are expressed by adequate inclusion assertions that will be part of the knowledge base we
are going to define.
We first define the function that maps each type expression into an ALUNI concept
expression as follows:
 Every class C is mapped into an atomic concept (C ).
 Every type expression Union T1; : : : ; Tk End is mapped into (T1 ) t    t (Tk ).
224

fiUnifying Class-Based Representation Formalisms

 Every type expression Set-of T is mapped into SetType u 8member. (T ).
 Every attribute A is mapped into an atomic role (A), and every type expression

Record A1: T1 ; : : : ; Ak : Tk End is mapped into

(A1 ). (T1 ) u 9=1 (A1 ) u    u
8 (Ak ). (Tk ) u 9=1 (Ak ):

RecType u 8

Using we define the aluni knowledge base corresponding to an object-oriented schema.
The aluni knowledge base (S ) = (A; P ; T ) corresponding to the objectoriented schema S = (CS ; AS ; DS ) is obtained as follows:
Definition 5.5

 A = fAbstractClass; RecType; SetTypeg [ f (C ) j C 2 CS g.
 P = fvalue; memberg [ f (A) j A 2 AS g.
 T consists of the following assertions:
AbstractClass
RecType
SetType

_
_
_

9=1value
8value.?
8value.? u :RecType

and for each class declaration
Class C is-a C1 ; : : : ; Cn type-is T
in DS , an inclusion assertion
(C ) _ AbstractClass u (C1 ) u    u (Cn) u 8value. (T ):
From the above translation we can observe that inverse roles are not necessary for the
formalization of object-oriented data models. Indeed, the possibility of referring to the
inverse of an attribute is generally ruled out in such models. However, this strongly limits
the expressive power of the data model, as pointed out in recent papers (see for example
Albano, Ghelli, & Orsini, 1991; Cattell, 1994). Note also that the use of number restrictions
is limited to the value 1, which corresponds to existence constraints and functionality,
whereas union is used in a more general form than for example in the KEE system.
We illustrate the translation on the fragment of object-oriented
schema in Figure 7. The corresponding aluni knowledge base is shown in Figure 8.
Example 5.2 (cont.)

225

fiCalvanese, Lenzerini, & Nardi
K = (A P T ), where
A = fAbstractClass RecType SetType String
;

;

;

P

;

;

;

Course; Teacher; Professor; Student; GradStudentg,
= fvalue; member; enrolls; taughtby; degreeg,

and the set T of assertions consists of:
Course

_

Teacher
GradStudent

_
_

AbstractClass
RecType
SetType

AbstractClass u
8value.(RecType u 9=1 enrolls u 9=1 taughtby u
8enrolls.(SetType u 8member.Student) u 8taughtby.Teacher)
AbstractClass u 8value.(GradStudent t Professor)
AbstractClass u Student u
8value.(RecType u 8degree.String u 9=1 degree)

_ 9=1 value
_ 8value.?
_ 8value.? u :RecType

Figure 8: The aluni knowledge base corresponding to the object-oriented schema in Figure 7
Below we discuss the effectiveness of the translation . First of all observe that the
knowledge base (S ) resulting from the translation of an object-oriented schema S
may admit models that do not have a direct counterpart among legal database instances
of S . More precisely, both an interpretation of (S ) and a database instance of S can be
viewed as a directed labeled graph: In the case of an interpretation, the nodes are domain
individuals and the arcs are labeled with roles. In the case of a database instance, the
nodes are either object identifiers or active values, and an arc either connects an object
identifier to its associated value (in which case it is labeled with value), or is part of the
sub-structure representing a set or record value (in which case it is labeled with member or
with an attribute, in accordance with the type of the value). In a legal database instance
of S , a value v is represented by a sub-structure that has the form of a finite tree with v as
root, set and record values as intermediate nodes, and objects identifiers as leaves. Clearly,
such a substructure does not contain cycles. Conversely, in a model of (S ), there may
be cycles involving only nodes that are instances of SetType and RecType and in which
all roles are different from value. We call such cycles bad. A model containing bad cycles
cannot be put directly in correspondence with a legal database instance. Also, due to the
open semantics of records one cannot adopt a different translation for which bad cycles in
the model are ruled out.
aluni

Example 5.6

Consider the object-oriented schema S , containing a single class declaration

Class C type-is Record a1 : Record a2 : Record a3 : C End End End
226

fiUnifying Class-Based Representation Formalisms

o1
C

value

v1

RecType
a3

a1

a2

o2
C

v2

RecType
value

v3

a1

RecType

v4

a2

RecType

v5

RecType

a3

Figure 9: A model containing cycles
which is translated to
C _ AbstractClass u
8value.(RecType u 9=1a1 u 8a1.(RecType u 9=1a2 u 8a2.(RecType u 9=1a3 u 8a3.C ))):
Figure 9 shows a model of (S ) represented as a graph. For clarity, we have named the
instances of C , and hence of AbstractClass, with o and the instances of RecType with
v. Observe the two different types of cycles in the graph. The cycle involving individuals
o2 ; v3 ; v4 , and v5 does not cause any problems since it contains an arc labeled with value,
which is not part of the structure constituting a complex value. In fact, v3 represents the
record value [ a1 :[[a2:[[a3: o2 ] ] ] . On the other hand, due to the bad cycle involving v1 and
v2 , individual v1 represents (together with o2 connected via a3 to v1 ) a record of infinite
depth.
We can nevertheless establish a correspondence from finite models of (S ) possibly
containing bad cycles to legal instances of the object-oriented schema S . This can be
achieved by unfolding the bad cycles in a model of (S ) to infinite trees. Obviously, the
unfolding of a cycle into an infinite tree, generates an infinite number of nodes, which
would correspond to an infinite database state. However, we can restrict the duplication of
individuals to those that represent set and record values, and thus are instances of SetType
and RecType. The instances of AbstractClass, instead, are not duplicated in the process
of unfolding, and therefore their number remains finite. Moreover, since the set of possible
active values associated with each object identifier is bound by the depth of the schema, we
can in fact block the unfolding of bad cycles to the finite tree of depth equal to the depth
of the schema.
Let us first formally define the depth of an object-oriented schema S .
Definition 5.7 For a type expression T we define depth (T ) inductively as follows:
8 0;
if T = C .
>
>
< max
if T = Union T1 ; : : : ; Tk End.
1ik (depth (Ti ));
depth (T ) =
0 );
>
1
+
depth
(
T
if T = Set-of T 0.
>
:
1 + max1ik (depth (Ti )); if T = Record A1: T1 ; : : : ; Ak : Tk End.
The depth of an object-oriented schema S is defined as the maximum of depth (T ) for a type
expression T in S .
227

fiCalvanese, Lenzerini, & Nardi

o1
C

value

a1

0

v1

RecType

a2

0

v2

RecType

a3

a3

C

RecType

1

v2

a2

RecType

2

v1

a1

RecType

:::

a3

value
o2

a1

1

v1

a1

v3

RecType

v4

RecType

a2

v5

RecType

a3

Figure 10: The unfolded version of the model in Figure 9
We can now introduce the notion of unfolding of an aluni interpretation.
Definition 5.8 Let S be an object-oriented schema, (S ) its translation in aluni and I
a finite interpretation of (S ). We call unfolded version of I the interpretation obtained
from I as follows: For each individual v that is part of a bad cycle, unfold the bad cycle
into an (infinite) tree having v as root, by generating new individuals only for the instances
of RecType and SetType. For a nonnegative integer m, we call m-unfolded version of I ,
denoted as Ijm, the interpretation obtained by truncating at depth m each infinite tree
generated in the process of unfolding.
Example 5.6 (cont.) Figure 10 shows the unfolded version of the model in Figure 9.
Notice that only the bad cycle has been unfolded to an infinite tree, and that all arcs labeled
with a3 lead to o2, which is an instance of AbstractClass and has not been duplicated.

The correctness of (S ) is sanctioned by the following proposition.
Proposition 5.9 For every object-oriented schema S of depth m, there exist mappings:

1. ffS from instances of S into finite interpretations of (S ) and ffV from active values
of instances of S into domain elements of the finite interpretations of (S ) such that:
For each legal instance J of S , ffS (J ) is a finite model of (S ), and for each type
expression T of S and each v 2 VJ , v 2 T J if and only if ffV (v) 2 ( (T ))ffS (J ) .

2. fiS from finite interpretations of (S ) into instances of S and fiV from domain elements of the m-unfolded versions of the finite interpretations of (S ) into active
values of instances of S , such that: For each finite model I of (S ), fiS (I ) is a legal
instance of S , and for each concept (T ), which is the translation of a type expression
T of S and each d 2 Ijm , d 2 ( (T ))Ijm if and only if fiV (d) 2 T fiS (I ) .

Proof.

follows:

(1) Given a database instance J we define an interpretation ffS (J ) of (S ) as
228

fiUnifying Class-Based Representation Formalisms

 ffV is a function mapping every element of VJ into a distinct element of ffS (J ) .
Therefore ffS (J ) is defined as the set of elements ffV (v) such that v 2 VJ . Moreover

we denote with id, rec, and set the elements of ffS (J ) corresponding to object
identifiers, record and set values, respectively.
 The interpretation of atomic concepts is defined as follows:
( (C ))ffS (J ) = fffV (o) j o 2 J (C )g;
for every (C ) corresponding to a class name C in S
AbstractClassffS (J ) = id
RecTypeffS (J ) = rec
SetTypeffS (J ) = set
 The interpretation of atomic roles is defined as follows:
( (A))ffS (J ) = f(d1 ; d2 ) j d1 2 rec and ffV 1 (d1 ) = [ : : : ; A: ffV 1 (d2 ); : : :] g;
for every (A) corresponding to an attribute name A in S
ff
(
J
)
S
member
= f(d1 ; d2 ) j d1 2 set and ffV 1(d1 ) = fj: : : ; ffV 1 (d2 ); : : :jgg
ff
(
J
)
value S
= f(d1 ; d2 ) j (ffV 1 (d1 ); ffV 1 (d2 )) 2 J g

We prove that for each type T and each v 2 VJ , v 2 T J if and only if ffV (v) 2
( (T ))ffS (J ) . The first part of the thesis then follows from the definition of ffS (J ). The
proof is by induction on the structure of the type expression.
Base case: T = C (i.e., T is a class1 name). If o 2 C J then ffV (o) 2 ( (C ))ffS (J ) , and
vice-versa if d 2 ( (C ))ffS (J ) then ffV (d) 2 C J .
Inductive case: T = Record A1 : T1 ; : : : ; Ak : Tk End and (T ) = RecType u
8 (A1 ). (T1 ) u 9=1 (A1 ) u    u 8 (Ak ). (Tk ) u 9=1 (Ak ). We assume that v 2 TiJ
iff ffV (v) 2 ( (Ti ))ffS (J ), for i 2 f1; : : : ; kg, and show that v 2 T J iff ffV (v) 2 ( (T ))ffJS (J ).
Suppose that v 2 T J , i.e., v = [ A1 : v1 ; : : : ; Ah: vh] with h  k and vi 2 Ti for
i 2 f1; : : : ; kg. By induction hypothesis ffV (vi ) 2 ( (Ti ))ffS (J ) , for i 2 f1; : : : ; kg, and by
definition of ffS , ffV (v) 2 RecTypeffS (J ) , (ffV (v); ffV (vi )) 2 ( (Ai ))ffS (J ) for i 2 f1; : : : ; kg,
and all roles (A) corresponding to attribute names are functional. Therefore, ffV (v) 2
( (T ))ffS (J ) .
Conversely, suppose that d = ffV (v) 2 ( (T ))ffS (J ) . Then, for each i 2 f1; : : : ; kg there is
exactly one di 2 ffS (J ) such that (d; di ) 2 ( (Ai ))ffS (J ), and moreover di 2 ( (T1i ))ffS (J ) .
By definition of ffS we have v = [ A1: v1 ; : : : ; AhJ: vh] , with h  k and vi = ffV (di ), for
i 2 f1; : : : ; kg. By induction hypothesis vi 2 Ti , for i 2 f1; : : : ; kg, and therefore v 2
(Record A1 : T1 ; : : : ; Ak : Tk End)J .
The cases for T = Union T1; : : : ; Tk End and T = Set-of T 0 can be treated analogously.
(2) Given a finite model I of (S ) of depth m, we define a legal database instance fiS (I )
as follows:
 fiV is a function mapping every element of Ijm into a distinct element of VfiS (I ) such
that the following conditions are satisfied:
{ OfiS (I )  VfiS (I ) is the set of elements fiV (d) such that d 2 AbstractClassIjm .
229

fiCalvanese, Lenzerini, & Nardi

If d 2 RecTypeIjm , (d;Idi ) 2 ( (Ai ))Ijm , for i 2 f1; : : : ; kg, and there
is no
other individual d0 2  jm and attribute A0 such that (d; d0 ) 2 ( (A0 ))Ijm , then
fiV (d) = [ A1 : fiV (d1 ); : : : ; Ak : fiV (dk )]].
{ If d 2 SetTypeIjm , (d; di ) 2 memberIjm , for i 2 f1; : : : ; kg, and there is no
other individual d0 2 Ijm such that (d; d0 ) 2 (member)Ijm , then fiV (d) =
ffiV (d1 ); : : : ; fiV (dk )g.
 For every class name C , fiS (I ) (C ) = ffiV (d) j d 2 ( (C ))Ijm g.
 fiS (I ) = f(o; v) j fiV (d1 ) = o; fiV (d2 ) = v; and (d1 ; d2 ) 2 valueIjm g.
We first prove that for each concept (T ), which is the translation of a type expression
T of S , and each d 2 Ijm , d 2 ( (T ))Ijm if and only if fiV (d) 2 T fiS (I ) . The proof is
by induction on the structure of the concept expression. Again for the inductive part we
restrict our attention to the case of record types.
Base case: T = C (i.e., (T ) is an atomic
concept).
If
d 2 ( (C ))Ijm then fiV (d) 2
C fiS (I ) , and vice-versa if o 2 C fiS (I ) then fiV 1 (o) 2 ( (C ))Ijm .
Inductive case: (T ) = RecType u 8 (A1 ). (T1 ) u 9=1 (A1 ) u    u 8 (Ak ). (ITk ) u
9=1 (Ak ) and T = Record A1: T1 ; : : : ; Ak : Tk End. We assume that d 2 ( (Ti )) jm iff
fiV (d) 2 TifiS (I ) , for i 2 f1; : : : ; kg, and show that d 2 ( (T ))Ijm iff fiV (d) 2 T fiS (I ) .
Suppose that d 2 ( (T ))Ijm . Then d 2I RecTypeIjm and for eachI i 2 f1; : : : ; kg there
is an individual di such that di 2 ( (Ti )) jm and (d; di ) 2 ( (Ai )) jm . By construction
fiV (d) = [ A1 : v1 ; : : : ; Ah : vh ] for some h  k. Moreover, by induction hypothesis fiV (di ) 2
TifiS (I ) and therefore fiV (d) 2 T fiS (I ) .
Conversely, suppose that fiV (d) 2 T fiS (I ) , i.e., fiV (d) = [ A1: v1 ; : : : ; Ah: vh ] with h  k
and vi 2 TifiS (I ) for i 2 f1; : : : ; kg. By induction hypothesis di = fiV 1(vi ) 2 ( (Ti ))Ijm ,
for i 2 f1; : : : ; kg, and by definition of fiV , d 2 RecTypeIjm and (d; di ) 2 ( (Ai ))Ijm ,
for i 2 f1; : : : ; kg. Since all roles (A) corresponding to attribute names are functional,
d 2 ( (T ))Ijm .
It remains to show that for each declaration
Class C is-a C1 ; : : : ; Cn type-is T
{

in DS , (a) C fiS (I )  CifiS (I ) for each i 2 f1; : : : ; ng, and (b) fiS (I )(C fiS (I ) )  T fiS (I ) .
(a) follows from the fact that (S ) contains the assertion (C ) _ (C1 ) u    u (Cn )
and from the definition of fiS (I ) .
(b) follows from what we have shown above and from the fact that Ijm still satisfies the
assertion (C ) _ AbstractClass u 8value. (T ). In fact, for some d 2 ( (C ))I let d0 be
the unique individual such that (d;I d0 ) 2 valueI . Since I is a model of (S ), d0 2 ( (T ))I .
We argue that also d0 2 ( (T )) jm . If d0 is not part of a bad cycle in I , then I and
Ijm coincide on the sub-structure rooted at d0 and formed by the individuals reached via
member and roles corresponding to attributes, and we are done. Otherwise, in Ijm such
sub-structure is expanded into a finite tree. Since by construction the depth of this tree
is at least depth (T ), and the connections between individuals in I are preserved in Ijm, it
follows that d0 2 ( (T ))Ijm .
230

fiUnifying Class-Based Representation Formalisms

The basic reasoning services considered in object-oriented databases are subtyping
(check whether a type denotes a subset of another type in every legal instance) and type
consistency (check whether a type is consistent in a legal instance). Based on Proposition 5.9, we can show that these forms of reasoning are fully captured by finite concept
consistency and finite concept subsumption in aluni knowledge bases.
be an object-oriented schema, T; T 0 two type expressions in S , and
the translation of S . Then the following holds:

Theorem 5.10 Let

(S )

S

S if and only if (S ) 6j=f (T )  ?.
T is a subtype of T 0 in S if and only if (S ) j=f (T )  (T 0 ).

1. T is consistent in
2.

The proof is analogous to the proof of Theorem 4.9, but it makes use of Proposition 5.9 instead of Proposition 4.8.
Again, the correspondence with aluni established by Theorem 5.10 allows us to make
use of the reasoning techniques developed for aluni to reason on object-oriented schemas.
Observe that reasoning in object-oriented models is already PSPACE-hard (Bergamaschi
& Nebel, 1994) and thus the known algorithms are exponential. However, by resorting
to aluni, it becomes possible to take into account for reasoning also various extensions
of the object-oriented formalism. Such extensions are useful for conceptual modeling and
have already been proposed in the literature (Cattell & Barry, 1997). First of all, the same
considerations developed for the ER model with regard to the use of arbitrary boolean
constructs on classes can be applied also in the object-oriented setting, which provides
disjunction but does not admit any form of negation. Additional features that can be added
to object oriented models are inverses of attributes, cardinality constraints on set-valued
attributes, and more general forms of restrictions on the values of attributes.
Proof.

6. Related Work

In this section we briey discuss recent results on the correspondence between class-based
formalisms and on techniques for reasoning in aluni and in class-based representation
formalisms.
6.1 Relationships among Class-Based Formalisms

In the past there have been several attempts to establish relationships among class-based
formalisms. Blasius, Hedstuck, and Rollinger (1990), Lenzerini, Nardi, and Simi (1991)
carry out a comparative analysis of class-based languages and attempt to provide a unified
view. The analysis makes it clear that several diculties arise in identifying a common
framework for the formalisms developed in different areas. Some recent papers address this
problem. For example, an analysis of the relationships between frame-based languages and
types in programming languages has been carried out by Borgida (1992), while Bergamaschi
and Sartori (1992), Piza, Schewe, and Schmidt (1992) use frame-based languages to enrich
the deductive capabilities of semantic and object-oriented data models.
231

fiCalvanese, Lenzerini, & Nardi

Artale, Cesarini, and Soda (1996) study reasoning in object-oriented data models by
presenting a translation to DLs in the style of the one discussed in Section 5. However, the
proposed translation is applicable only in the case where the shema contains no recursive
class declarations. This limitation is not present in the work by Bergamaschi and Nebel
(1994), where a formalism derived from DLs is used to model complex objects and an
algorithm for computing subsumption between classes is provided.
A recent survey on the application of DLs to the problem of data management has been
presented by Borgida (1995) . The application to the task of data modeling of reasoning
techniques derived from the correspondences presented in Sections 4 and 5 is discussed in
more detail by Calvanese, Lenzerini, and Nardi (1998).
Recently, there have also been proposals to integrate the object-oriented and the logic
programming paradigms (Kifer & Wu, 1993; Kifer, Lausen, & Wu, 1995). These proposals
are however not directly related to the present work, since they aim at providing mechanisms
for computing with structured objects, rather than means for reasoning over a conceptual
(object-oriented) representation of the domain of interest.
6.2 Reasoning in

aluni

and in Class-Based Representation Formalisms

is equipped with techniques to reason both with respect to unrestricted and with
respect to finite models. We briey sketch the main ideas underlying reasoning in both
contexts. A detailed account of the reasoning techniques has been carried out by Calvanese
(1996c).
aluni

6.2.1 Unrestricted Model Reasoning

We remind that reasoning on a knowledge base with respect to unrestricted models amounts
to check either concept consistency, i.e., determine whether the knowledge base admits a
(possibly infinite) model in which a given concept has a nonempty extension, or concept
subsumption, i.e., determine whether the extension of one concept is contained in the extension of another concept in every model (including the infinite ones) of the knowledge
base.
The method to reason in aluni with respect to unrestricted models exploits a well known
correspondence between DLs and Propositional Dynamic Logics (PDLs) (Kozen & Tiuryn,
1990), which are a class of logics specifically designed to reason about programs. The
correspondence, which has first been pointed out by Schild (1991), relies on a substantial
similarity of the interpretative structures of both formalisms, and allows one to exploit the
reasoning techniques developed for PDLs to reason in the corresponding DLs. In particular,
since ALUNI , the description language of aluni, includes the construct for inverse roles,
for the correspondence one has to resort to converse-PDL, a variant of PDL that includes
converse programs (Kozen & Tiuryn, 1990). However, because of the presence of number
restrictions in ALUNI which have no direct correspondence in PDLs, we cannot rely on
traditional techniques for reasoning in PDLs. Recently, encoding techniques have been
developed, which allow one to eliminate number restrictions from a knowledge base while
preserving concept consistency and concept subsumption (De Giacomo & Lenzerini, 1994a).
The encoding is applicable to knowledge bases formulated in expressive variants of DLs, and
in particular it can be used to reduce unrestricted model reasoning on aluni knowledge
232

fiUnifying Class-Based Representation Formalisms

bases (both concept consistency and concept subsumption) to deciding satisfiability of a
formula of converse-PDL. Reasoning in converse-PDL is decidable in EXPTIME (Kozen &
Tiuryn, 1990), and since the encoding is polynomial (De Giacomo & Lenzerini, 1994a) we
obtain an EXPTIME decision procedure for unrestricted concept consistency and concept
subsumption in aluni knowledge bases. A simplified form of the encoding, which can be
applied to decide unrestricted concept consistency in aluni has also been presented by
Calvanese et al. (1994).
6.2.2 Finite Model Reasoning

We remind that reasoning on a knowledge base with respect to finite models amounts to
check either finite concept consistency or finite concept subsumption, for which only the
finite models of the knowledge base must be considered.
For finite model reasoning, the techniques based on a reduction to reasoning in PDLs
are not applicable. Indeed, the PDL formula corresponding to an aluni knowledge base
contains constructs both for converse programs (corresponding to inverse roles) and for
functionality of direct and inverse programs, and thus is a formula of a variant of PDL
which does not have the finite model property (Vardi, 1985). However, after encoding
functionality, one obtains a converse-PDL formula, and since converse-PDL has the finite
model property (Fischer & Ladner, 1979), this formula is satisfiable if and only if it is
finitely satisfiable. This shows that the encoding of number restrictions (and in particular
the encoding of functionality), while preserving unrestricted satisfiability does not preserve
finite satisfiability (De Giacomo & Lenzerini, 1994a).
For finite model reasoning in aluni one can adopt a different technique, which is based
on the idea of separating the reasoning process in two distinct phases (see Calvanese, 1996c,
for full details). The first phase deals with all constructs except number restrictions, and
builds an \expanded knowledge base" in which these constructs are embedded implicitly
in the concepts and roles. In the second phase the assertions involving number restrictions
are used to derive from this expanded knowledge base a system of linear inequalities. The
system is defined in such a way that its solutions of a certain type (acceptable solutions) are
directly related to the finite models of the original knowledge base. In particular, from each
acceptable solution one can directly deduce the cardinalities of the extensions of all concepts
and roles in a possible finite model. The proposed method allows one to establish for aluni
EXPTIME decidability for finite concept consistency and for special cases of finite concept
subsumption. By resorting to a more complicated encoding one can obtain a 2EXPTIME
decision procedure for finite concept subsumption in aluni in general (Calvanese, 1996a,
1996c).
Reasoning with respect to finite models has also been investigated in the context of dependency theory in databases. As shown by Casanova, Fagin, and Papadimitriou (1984) for
the relational model, when functional and inclusion dependencies interact, the dependency
implication problem in the finite case differs from the one in the unrestricted case. While
the implication problem for arbitrary functional and inclusion dependencies is undecidable
(Chandra & Vardi, 1985; Mitchell, 1983), for functional and unary inclusion dependencies
it is solvable in polynomial time, both in the finite and the unrestricted case (Cosmadakis
et al., 1990).
233

fiCalvanese, Lenzerini, & Nardi

Consistency with respect to finite models of schemata expressed in an enriched EntityRelationship model with cardinality constraints has been shown decidable in polynomial
time by Lenzerini and Nobili (1990). Calvanese and Lenzerini (1994b) extend the decidability result to include also ISA relationships, and Calvanese and Lenzerini (1994a) show
EXPTIME decidability of reasoning in an expressive object-oriented model. An algorithm
for computing a refinement ordering for types (the analogue to a concept hierarchy) in the
framework of the O2 object oriented model in discussed by Lecluse and Richard (1989).
Reasoning in the strict sublanguage of aluni obtained by omitting inverse roles and
number restrictions is already EXPTIME-hard (Calvanese, 1996b). Therefore, the known
algorithms for deciding unrestricted concept consistency and subsumption and finite concept
consistency are essentially optimal.
7. Conclusions

We have presented a unified framework for representing information about class structures
and reasoning about them. We have pursued this goal by looking at various class-based
formalisms proposed in different fields of computer science, namely frame based systems
used in knowledge representation, and semantic and object-oriented data models used in
databases, and rephrasing them in the framework of description logics. The resulting description logic, called aluni includes a combination of constructs that was not addressed
before, although all of the constructs had previously been considered separately.
The major achievement of the paper is the demonstration that class-based formalisms
can be given a precise characterization by means of a powerful fragment of first-order logic,
which thus can be regarded as the essential core of the class-based representation formalisms
belonging to all three families mentioned above. This has several consequences.
First of all, any of the formalisms considered in the paper can be enriched with constructs
originating from other formalisms and treated in the general framework. In this sense, the
work reported here not only provides a common powerful representation formalism, but
may also contribute to significant developments for the languages belonging to all the three
families. For example, the usage of inverse roles in concept languages greatly enhances the
expressivity of roles, while the combination of ISA, number restrictions, and union enriches
the reasoning capabilities available in semantic data models.
Secondly, the comparison of class-based formalisms from the fields of knowledge representation and conceptual data modeling makes it feasible to address the development of
reasoning tools to support conceptual modeling (Calvanese et al., 1998). In fact, reasoning capabilities become especially important in complex scenarios such as those arising in
heterogenous database applications and Data Warehousing. This line of work was among
the motivations for developing systems based on expressive description logics (Horrocks,
1998; Horrocks & Patel-Schneider, 1999), and has lead to further extending the language of
description logics to support Information Integration and, more specifically, the conceptual
modeling of Data Warehouses (Calvanese, De Giacomo, Lenzerini, Nardi, & Rosati, 1998).
234

fiUnifying Class-Based Representation Formalisms
References

Abiteboul, S., Kanellakis, P., Ramaswamy, S., & Waller, E. (1992). Method schemas. Tech.
rep. CS-92-33, Brown University. An earlier version appeared in
.
Abiteboul, S., & Kanellakis, P. (1989). Object identity as a query language primitive. In
Proceedings of the ACM SIGMOD International Conference on Management of Data,
pp. 159{173.
Abrial, J. R. (1974). Data semantics. In Klimbie, J. W., & Koffeman, K. L. (Eds.), Data
Base Management, pp. 1{59. North-Holland Publ. Co., Amsterdam.
Albano, A., Ghelli, G., & Orsini, R. (1991). A relationship mechanism for strongly typed
Object-Oriented database programming languages. In Proceedings of the Seventeenth International Conference on Very Large Data Bases (VLDB'91), pp. 565{575
Barcelona.
Artale, A., Cesarini, F., & Soda, G. (1996). Describing database objects in a concept
language environment. IEEE Transactions on Knowledge and Data Engineering, 8 (2),
345{351.
Atzeni, P., & Parker Jr., D. S. (1986). Formal properties of net-based knowledge representation schemes. In Proceedings of the Second IEEE International Conference on Data
Engineering (ICDE'86), pp. 700{706 Los Angeles.
Baader, F. (1991). Augmenting concept languages by transitive closure of roles: An alternative to terminological cycles. In Proceedings of the Twelfth International Joint
Conference on Artificial Intelligence (IJCAI'91) Sydney, Australia.
Baader, F. (1996). Using automata theory for characterizing the semantics of terminological
cycles. Annals of Mathematics and Artificial Intelligence, 18, 175{219.
Batini, C., Ceri, S., & Navathe, S. B. (1992). Conceptual Database Design, an EntityRelationship Approach. Benjamin and Cummings Publ. Co., Menlo Park, California.
Bergamaschi, S., & Nebel, B. (1994). Acquisition and validation of complex object database
schemata supporting multiple inheritance. Applied Intelligence, 4 (2), 185{203.
Bergamaschi, S., & Sartori, C. (1992). On taxonomic reasoning in conceptual design. ACM
Transactions on Database Systems, 17 (3), 385{422.
Blasius, K. H., Hedstuck, U., & Rollinger, C.-R. (Eds.). (1990). Sorts and Types in Artificial
Intelligence, Vol. 418 of Lecture Notes in Artificial Intelligence. Springer-Verlag.
Borgida, A. (1992). From type systems to knowledge representation: Natural semantics
specifications for description logics. Journal of Intelligent and Cooperative Information
Systems, 1 (1), 93{126.
Borgida, A. (1995). Description logics in data management. IEEE Transactions on Knowledge and Data Engineering, 7 (5), 671{682.
Proc. of the 9th

Symp. on Principles of Database Systems PODS-90

235

fiCalvanese, Lenzerini, & Nardi

Borgida, A. (1996). On the relative expressiveness of description logics and predicate logics.
Artificial Intelligence, 82, 353{367.
Borgida, A., & Weddell, G. E. (1997). Adding functional dependencies to description logics.
In Proceedings of the Fifth International Conference on Deductive and Object-Oriented
Databases (DOOD'97).
Brachman, R. J., & Levesque, H. J. (1984). The tractability of subsumption in frame-based
description languages. In Proceedings of the Fourth National Conference on Artificial
Intelligence (AAAI'84), pp. 34{37.
Brachman, R. J., & Levesque, H. J. (Eds.). (1985). Readings in Knowledge Representation.
Morgan Kaufmann, Los Altos.
Brachman, R. J., McGuinness, D. L., Patel-Schneider, P. F., Alperin Resnick, L., & Borgida,
A. (1991). Living with CLASSIC: When and how to use a KL-ONE-like language. In
Sowa, J. F. (Ed.), Principles of Semantic Networks, pp. 401{456. Morgan Kaufmann,
Los Altos.
Bresciani, P., Franconi, E., & Tessaris, S. (1995). Implementing and testing expressive
description logics: Preliminary report. In Borgida, A., Lenzerini, M., Nardi, D., &
Nebel, B. (Eds.), Working Notes of the 1995 Description Logics Workshop, Technical
Report, RAP 07.95, Dipartimento di Informatica e Sistemistica, Universita di Roma
\La Sapienza", pp. 131{139 Rome (Italy).
Buchheit, M., Donini, F. M., Nutt, W., & Schaerf, A. (1998). A refined architecture for
terminological systems: Terminology = schema + views. Artificial Intelligence, 99 (2),
209{260.
Buchheit, M., Donini, F. M., & Schaerf, A. (1993). Decidable reasoning in terminological
knowledge representation systems. Journal of Artificial Intelligence Research, 1, 109{
138.
Calvanese, D. (1996a). Finite model reasoning in description logics. In Aiello, L. C., Doyle,
J., & Shapiro, S. C. (Eds.), Proceedings of the Fifth International Conference on the
Principles of Knowledge Representation and Reasoning (KR'96), pp. 292{303. Morgan
Kaufmann, Los Altos.
Calvanese, D. (1996b). Reasoning with inclusion axioms in description logics: Algorithms
and complexity. In Wahlster, W. (Ed.), Proceedings of the Twelfth European Conference on Artificial Intelligence (ECAI'96), pp. 303{307. John Wiley & Sons.
Calvanese, D. (1996c). Unrestricted and Finite Model Reasoning in ClassBased Representation Formalisms.
Ph.D. thesis, Dipartimento di Informatica e Sistemistica, Universita di Roma \La Sapienza". Available at
http://www.dis.uniroma1.it/pub/calvanes/thesis.ps.gz.
Calvanese, D., De Giacomo, G., Lenzerini, M., Nardi, D., & Rosati, R. (1998). Description
logic framework for information integration. In Proceedings of the Sixth International
236

fiUnifying Class-Based Representation Formalisms
Conference on Principles of Knowledge Representation and Reasoning (KR'98),

pp.
2{13.
Calvanese, D., & Lenzerini, M. (1994a). Making object-oriented schemas more expressive.
In Proceedings of the Thirteenth ACM SIGACT SIGMOD SIGART Symposium on
Principles of Database Systems (PODS'94), pp. 243{254 Minneapolis. ACM Press
and Addison Wesley.
Calvanese, D., & Lenzerini, M. (1994b). On the interaction between ISA and cardinality
constraints. In Proceedings of the Tenth IEEE International Conference on Data
Engineering (ICDE'94), pp. 204{213 Houston (Texas). IEEE Computer Society Press.
Calvanese, D., Lenzerini, M., & Nardi, D. (1994). A unified framework for class based representation formalisms. In Doyle, J., Sandewall, E., & Torasso, P. (Eds.), Proceedings
of the Fourth International Conference on the Principles of Knowledge Representation
and Reasoning (KR'94), pp. 109{120 Bonn. Morgan Kaufmann, Los Altos.
Calvanese, D., Lenzerini, M., & Nardi, D. (1998). Description logics for conceptual data
modeling. In Chomicki, J., & Saake, G. (Eds.), Logics for Databases and Information
Systems, pp. 229{264. Kluwer Academic Publisher.
Casanova, M. A., Fagin, R., & Papadimitriou, C. H. (1984). Inclusion dependencies and
their interaction with functional dependencies. Journal of Computer and System
Sciences, 28 (1), 29{59.
Cattell, R. G. G. (Ed.). (1994). The Object Database Standard: ODMG-93. Morgan Kaufmann, Los Altos. Release 1.1.
Cattell, R. G. G., & Barry, D. K. (Eds.). (1997). The Object Database Standard: ODMG
2.0. Morgan Kaufmann, Los Altos.
Chandra, A. K., & Vardi, M. Y. (1985). The implication problem for functional and inclusion
dependencies is undecidable. SIAM Journal on Computing, 14 (3), 671{677.
Chen, P. P. (1976). The Entity-Relationship model: Toward a unified view of data. ACM
Transactions on Database Systems, 1 (1), 9{36.
Cosmadakis, S. S., & Kanellakis, P. C. (1986). Functional and inclusion dependencies - A
graph theoretical approach. In Kanellakis, P. C., & Preparata, F. P. (Eds.), Advances
in Computing Research, Vol. 3, pp. 163{184. JAI Press.
Cosmadakis, S. S., Kanellakis, P. C., & Vardi, M. (1990). Polynomial-time implication
problems for unary inclusion dependencies. Journal of the ACM, 37 (1), 15{46.
De Giacomo, G., & Lenzerini, M. (1994a). Boosting the correspondence between description logics and propositional dynamic logics. In Proceedings of the Twelfth National
Conference on Artificial Intelligence (AAAI'94), pp. 205{212. AAAI Press/The MIT
Press.
237

fiCalvanese, Lenzerini, & Nardi

De Giacomo, G., & Lenzerini, M. (1994b). Concept language with number restrictions and
fixpoints, and its relationship with -calculus. In Proceedings of the Eleventh European
Conference on Artificial Intelligence (ECAI'94), pp. 411{415.
Di Battista, G., & Lenzerini, M. (1993). Deductive entity-relationship modeling. IEEE
Transactions on Knowledge and Data Engineering, 5 (3), 439{450.
Donini, F. M., Lenzerini, M., Nardi, D., & Nutt, W. (1997). The complexity of concept
languages. Information and Computation, 134, 1{58.
Donini, F. M., Lenzerini, M., Nardi, D., Nutt, W., & Schaerf, A. (1994). Queries, rules and
definitions. In Foundations of Knowledge Representation and Reasoning. SpringerVerlag.
Donini, F. M., Lenzerini, M., Nardi, D., & Schaerf, A. (1996). Reasoning in description
logics. In Brewka, G. (Ed.), Principles of Knowledge Representation, Studies in Logic,
Language and Information, pp. 193{238. CSLI Publications.
Donini, F. M., Nardi, D., & Rosati, R. (1995). Non-first-order features in concept languages. In Gori, M., & Soda, G. (Eds.), Proceedings of the Fourth Conference of the
Italian Association for Artificial Intelligence (AI*IA'95), Vol. 992 of Lecture Notes in
Artificial Intelligence, pp. 91{102. Springer-Verlag.
Ferg, S. (1991). Cardinality concepts in entity-relationship modeling. In Proceedings of the
Tenth International Conference on the Entity-Relationship Approach (ER'91), pp.
1{30.
Fikes, R., & Kehler, T. (1985). The role of frame-based representation in reasoning. Communications of the ACM, 28 (9), 904{920.
Fischer, M. J., & Ladner, R. E. (1979). Propositional dynamic logic of regular programs.
Journal of Computer and System Sciences, 18, 194{211.
Grant, J., & Minker, J. (1984). Numerical dependencies. In Gallaire, H., Minker, J., &
Nicolas, J.-M. (Eds.), Advances in Database Theory II. Plenum Publ. Co., New York.
Hayes, P. J. (1979). The logic of frames. In Metzing, D. (Ed.), Frame Conceptions and Text
Understanding, pp. 46{61. Walter de Gruyter and Co. Republished in (Brachman &
Levesque, 1985).
Horrocks, I. (1998). Using an expressive description logic: FaCT or fiction?. In Proceedings
of the Sixth International Conference on Principles of Knowledge Representation and
Reasoning (KR'98), pp. 636{647.
Horrocks, I., & Patel-Schneider, P. F. (1999). Optimizing description logic subsumption.
Journal of Logic and Computation, 9 (3), 267{293.
Hull, R. B., & King, R. (1987). Semantic database modelling: Survey, applications and
research issues. ACM Computing Surveys, 19 (3), 201{260.
238

fiUnifying Class-Based Representation Formalisms

Karp, P. D. (1992). The design space of knowledge representation systems. Tech. rep. SRI
AI Technical Note 520, SRI International, Menlo Park, CA.
Karp, P. D., Myers, K. L., & Gruber, T. (1995). The generic frame protocol. In Proceedings
of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI'95),
Vol. A, pp. 768{774 Montreal, Canada.
Kifer, M., Lausen, G., & Wu, J. (1995). Logical foundations of Object-Oriented and framebased languages. Journal of the ACM, 42 (4), 741{843.
Kifer, M., & Wu, J. (1993). A logic for programming with complex objects. Journal of
Computer and System Sciences, 47, 77{120.
Kim, W. (1990). Introduction to Object-Oriented Databases. The MIT Press.
Kim, W., & Lochovsky, F. H. (Eds.). (1989). Object-Oriented Concepts, Databases, and
Applications. ACM Press and Addison Wesley, New York.
Kozen, D., & Tiuryn, J. (1990). Logics of programs. In van Leeuwen, J. (Ed.), Handbook of
Theoretical Computer Science { Formal Models and Semantics, pp. 789{840. Elsevier
Science Publishers (North-Holland), Amsterdam.
Lecluse, C., & Richard, P. (1989). Modeling complex structures in object-oriented databases.
In Proceedings of the Eighth ACM SIGACT SIGMOD SIGART Symposium on Principles of Database Systems (PODS'89), pp. 362{369.
Lehmann, F. (Ed.). (1992). Semantic Networks in Artificial Intelligence. Pergamon Press,
Oxford.
Lenzerini, M., Nardi, D., & Simi, M. (Eds.). (1991). Inheritance Hierarchies in Knowledge
Representation and Programming Languages. John Wiley & Sons, Chichester.
Lenzerini, M., & Nobili, P. (1990). On the satisfiability of dependency constraints in entityrelationship schemata. Information Systems, 15 (4), 453{461.
Mitchell, J. C. (1983). The implication problem for functional and inclusion dependencies.
Information and Control, 56, 154{173.
Motschnig-Pitrik, R., & Mylopoulous, J. (1992). Classes and instances. Journal of Intelligent and Cooperative Information Systems, 1 (1).
Nebel, B. (1991). Terminological cycles: Semantics and computational properties. In Sowa,
J. F. (Ed.), Principles of Semantic Networks, pp. 331{361. Morgan Kaufmann, Los
Altos.
Piza, B., Schewe, K.-D., & Schmidt, J. W. (1992). Term subsumption with type constructors. In Yesha, Y. (Ed.), Proceedings of the International Conference on Information
and Knowledge Management (CIKM'92), pp. 449{456 Baltimore.
239

fiCalvanese, Lenzerini, & Nardi

Schild, K. (1991). A correspondence theory for terminological logics: Preliminary report.
In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence
(IJCAI'91), pp. 466{471 Sydney, Australia.
Schild, K. (1994). Terminological cycles and the propositional -calculus. In Doyle, J.,
Sandewall, E., & Torasso, P. (Eds.), Proceedings of the Fourth International Conference on the Principles of Knowledge Representation and Reasoning (KR'94), pp.
509{520 Bonn. Morgan Kaufmann, Los Altos.
Schmidt-Schau, M., & Smolka, G. (1991). Attributive concept descriptions with complements. Artificial Intelligence, 48 (1), 1{26.
Sowa, J. F. (Ed.). (1991). Principles of Semantic Networks. Morgan Kaufmann, Los Altos.
Teorey, T. J. (1989). Database Modeling and Design: The Entity-Relationship Approach.
Morgan Kaufmann, Los Altos.
Thalheim, B. (1992). Fundamentals of cardinality constraints. In Pernoul, G., & Tjoa,
A. M. (Eds.), Proceedings of the Eleventh International Conference on the EntityRelationship Approach (ER'92), pp. 7{23. Springer-Verlag.
Thalheim, B. (1993). Fundamentals of the Entity Relationship Model. Springer-Verlag.
Vardi, M. Y. (1985). The taming of converse: Reasoning about two-way computations.
In Parikh, R. (Ed.), Proc. of the 4th Workshop on Logics of Programs, Vol. 193 of
Lecture Notes in Computer Science, pp. 413{424. Springer-Verlag.
Woods, W. A., & Schmolze, J. G. (1992). The KL-ONE family. In Lehmann, F. W. (Ed.),
Semantic Networks in Artificial Intelligence, pp. 133{178. Pergamon Press. Published
as a special issue of Computers & Mathematics with Applications, Volume 23, Number
2{9.
Ye, X., Parent, C., & Spaccapietra, S. (1994). Cardinality consistency of derived objects in
DOOD systems. In Loucopoulos, P. (Ed.), Proceedings of the Thirteenth International
Conference on the Entity-Relationship Approach (ER'94), Vol. 881 of Lecture Notes
in Computer Science, pp. 278{295 Manchester (UK). Springer-Verlag.

240

fiJournal of Artificial Intelligence Research 11 (1999) 391-427

Submitted 1/99; published 11/99

Markov Localization for Mobile Robots
in Dynamic Environments
dfox@cs.cmu.edu

Dieter Fox
Computer Science Department and Robotics Institute
Carnegie Mellon University
Pittsburgh, PA 15213-3891

burgard@informatik.uni-freiburg.de

Wolfram Burgard
Department of Computer Science
University of Freiburg
D-79110 Freiburg, Germany

thrun@cs.cmu.edu

Sebastian Thrun
Computer Science Department and Robotics Institute
Carnegie Mellon University
Pittsburgh, PA 15213-3891

Abstract

Localization, that is the estimation of a robot's location from sensor data, is a fundamental problem in mobile robotics. This papers presents a version of Markov localization
which provides accurate position estimates and which is tailored towards dynamic environments. The key idea of Markov localization is to maintain a probability density over the
space of all locations of a robot in its environment. Our approach represents this space
metrically, using a fine-grained grid to approximate densities. It is able to globally localize
the robot from scratch and to recover from localization failures. It is robust to approximate models of the environment (such as occupancy grid maps) and noisy sensors (such
as ultrasound sensors). Our approach also includes a filtering technique which allows a
mobile robot to reliably estimate its position even in densely populated environments in
which crowds of people block the robot's sensors for extended periods of time. The method
described here has been implemented and tested in several real-world applications of mobile
robots, including the deployments of two mobile robots as interactive museum tour-guides.
1. Introduction

Robot localization has been recognized as one of the most fundamental problems in mobile
robotics (Cox & Wilfong, 1990; Borenstein et al., 1996). The aim of localization is to
estimate the postition of a robot in its environment, given a map of the environment and
sensor data. Most successful mobile robot systems to date utilize localization, as knowledge
of the robot's position is essential for a broad range of mobile robot tasks.
Localization|often referred to as position estimation or position control|is currently a
highly active field of research, as a recent book by Borenstein and colleagues (1996) suggests.
The localization techniques developed so far can be distinguished according to the type of
c 1999 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiFox, Burgard & Thrun

problem they attack. Tracking or local techniques aim at compensating odometric errors
occurring during robot navigation. They require, however, that the initial location of the
robot is (approximately) known and they typically cannot recover if they lose track of the
robot's position (within certain bounds). Another family of approaches is called global
techniques. These are designed to estimate the position of the robot even under global
uncertainty. Techniques of this type solve the so-called wake-up robot problem, in that they
can localize a robot without any prior knowledge about its position. They furthermore can
handle the kidnapped robot problem, in which a robot is carried to an arbitrary location
during it's operation1. Global localization techniques are more powerful than local ones.
They typically can cope with situations in which the robot is likely to experience serious
positioning errors.
In this paper we present a metric variant of Markov localization, a technique to globally
estimate the position of a robot in its environment. Markov localization uses a probabilistic
framework to maintain a position probability density over the whole set of possible robot
poses. Such a density can have arbitrary forms representing various kinds of information
about the robot's position. For example, the robot can start with a uniform distribution
representing that it is completely uncertain about its position. It furthermore can contain
multiple modes in the case of ambiguous situations. In the usual case, in which the robot
is highly certain about its position, it consists of a unimodal distribution centered around
the true position of the robot. Based on the probabilistic nature of the approach and the
representation, Markov localization can globally estimate the position of the robot, it can
deal with ambiguous situations, and it can re-localize the robot in the case of localization
failures. These properties are basic preconditions for truly autonomous robots designed to
operate over long periods of time.
Our method uses a fine-grained and metric discretization of the state space. This approach has several advantages over previous ones, which predominately used Gaussians or
coarse-grained, topological representations for approximating a robot's belief. First, it provides more accurate position estimates, which are required in many mobile robot tasks (e.g.,
tasks involving mobile manipulation). Second, it can incorporate raw sensory input such as
a single beam of an ultrasound sensor. Most previous approaches to Markov localization, in
contrast, screen sensor data for the presence or absence of landmarks, and they are prone
to fail if the environment does not align well with the underlying assumptions (e.g., if it
does not contain any of the required landmarks).
Most importantly, however, previous Markov localization techniques assumed that the
environment is static. Therefore, they typically fail in highly dynamic environments, such
as public places where crowds of people may cover the robot's sensors for extended periods
of time. To deal with such situations, our method applies a filtering technique that, in
essence, updates the position probability density using only those measurements which are
with high likelihood produced by known objects contained in the map. As a result, it
permits accurate localization even in densely crowded, non-static environments.
Our Markov localization approach has been implemented and evaluated in various environments, using different kinds of robots and sensor modalities. Among these applications
are the deployments of the mobile robots Rhino and Minerva (see Figure 1) as interac1. Please note that the wake-up problem is the special case of the kidnapped robot problem in which the
robot is told that it has been carried away.
392

fiMarkov Localization for Mobile Robots in Dynamic Environments

(a)

(b)

Fig. 1. The mobile robots Rhino (a) and Minerva (b) acting as interactive museum tour-guides.

tive museum tour-guide robots (Burgard et al., 1998a, 2000; Thrun et al., 1999) in the
Deutsches Museum Bonn and the National Museum of American History in Washington,
DC, respectively. Experiments described in this paper illustrate the ability of our Markov
localization technique to deal with approximate models of the environment, such as occupancy grid maps and noisy sensors such as ultrasound sensors, and they demonstrate that
our approach is well-suited to localize robots in densely crowded environments, such as
museums full of people.
The paper is organized as follows. The next section describes the mathematical framework of Markov localization. We introduce our metric version of Markov localization in
Section 3. This section also presents a probabilistic model of proximity sensors and a filtering scheme to deal with highly dynamic environments. Thereafter, we describe experimental
results illustrating different aspects of our approach. Related work is discussed in Section 5
followed by concluding remarks.
2. Markov Localization

To introduce the major concepts, we will begin with an intuitive description of Markov
localization, followed by a mathematical derivation of the algorithm. The reader may
notice that Markov localization is a special case of probabilistic state estimation, applied
to mobile robot localization (see also Russell & Norvig, 1995; Fox, 1998 and Koenig &
Simmons, 1998).
For clarity of the presentation, we will initially make the restrictive assumption that the
environment is static. This assumption, called Markov assumption, is commonly made in
the robotics literature. It postulates that the robot's location is the only state in the environment which systematically affects sensor readings. The Markov assumption is violated
if robots share the same environment with people. Further below, in Section 3.3, we will
side-step this assumption and present a Markov localization algorithm that works well even
in highly dynamic environments, e.g., museums full of people.
2.1 The Basic Idea

Markov localization addresses the problem of state estimation from sensor data. Markov
localization is a probabilistic algorithm: Instead of maintaining a single hypothesis as to
393

fiFox, Burgard & Thrun

Fig. 2. The basic idea of Markov localization: A mobile robot during global localization.

where in the world a robot might be, Markov localization maintains a probability distribution
over the space of all such hypotheses. The probabilistic representation allows it to weigh
these different hypotheses in a mathematically sound way.
Before we delve into mathematical detail, let us illustrate the basic concepts with a
simple example. Consider the environment depicted in Figure 2. For the sake of simplicity,
let us assume that the space of robot positions is one-dimensional, that is, the robot can
only move horizontally (it may not rotate). Now suppose the robot is placed somewhere in
this environment, but it is not told its location. Markov localization represents this state
of uncertainty by a uniform distribution over all positions, as shown by the graph in the
first diagram in Figure 2. Now let us assume the robot queries its sensors and finds out
that it is next to a door. Markov localization modifies the belief by raising the probability
for places next to doors, and lowering it anywhere else. This is illustrated in the second
diagram in Figure 2. Notice that the resulting belief is multi-modal, reecting the fact that
the available information is insucient for global localization. Notice also that places not
next to a door still possess non-zero probability. This is because sensor readings are noisy,
and a single sight of a door is typically insucient to exclude the possibility of not being
next to a door.
Now let us assume the robot moves a meter forward. Markov localization incorporates
this information by shifting the belief distribution accordingly, as visualized in the third
diagram in Figure 2. To account for the inherent noise in robot motion, which inevitably
leads to a loss of information, the new belief is smoother (and less certain) than the previous
one. Finally, let us assume the robot senses a second time, and again it finds itself next to a
door. Now this observation is multiplied into the current (non-uniform) belief, which leads
to the final belief shown at the last diagram in Figure 2. At this point in time, most of the
probability is centered around a single location. The robot is now quite certain about its
position.
394

fiMarkov Localization for Mobile Robots in Dynamic Environments
2.2 Basic Notation

To make this more formal, let us denote the position (or: location) of a mobile robot by a
three-dimensional variable l = hx; y; i, comprising its x-y coordinates (in some Cartesian
coordinate system) and its heading direction . Let lt denote the robot's true location at
time t, and Lt denote the corresponding random variable. Throughout this paper, we will
use the terms position and location interchangeably.
Typically, the robot does not know its exact position. Instead, it carries a belief as
to where it might be. Let Bel(Lt ) denote the robot's position belief at time t. Bel(Lt )
is a probability distribution over the space of positions. For example, Bel(Lt = l) is the
probability (density) that the robot assigns to the possibility that its location at time t is
l. The belief is updated in response to two different types of events: The arrival of a measurement through the robot's environment sensors (e.g., a camera image, a sonar scan), and
the arrival of an odometry reading (e.g., wheel revolution count). Let us denote environment sensor measurements by s and odometry measurements by a, and the corresponding
random variables by S and A, respectively.
The robot perceives a stream of measurements, sensor measurements s and odometry
readings a. Let
d = fd0 ; d1 ; : : : ; dT g
(1)
denote the stream of measurements, where each dt (with 0  t  T ) either is a sensor
measurement or an odometry reading. The variable t indexes the data, and T is the most
recently collected data item (one might think of t as \time"). The set d, which comprises
all available sensor data, will be referred to as the data.
2.3 Recursive Localization

Markov localization estimates the posterior distribution over LT conditioned on all available
data, that is
P (LT = l j d) = P (LT = l j d0 ; : : : ; dT ):
(2)
Before deriving incremental update equations for this posterior, let us briey make explicit
the key assumption underlying our derivation, called the Markov assumption. The Markov
assumption, sometimes referred to as static world assumption, specifies that if one knows
the robot's location lt , future measurements are independent of past ones (and vice versa):
P (dt+1 ; dt+2 ; : : : j Lt = l; d0 ; : : : ; dt ) = P (dt+1 ; dt+2 ; : : : j Lt = l) 8t
(3)
In other words, we assume that the robot's location is the only state in the environment, and
knowing it is all one needs to know about the past to predict future data. This assumption
is clearly inaccurate if the environment contains moving (and measurable) objects other
than the robot itself. Further below, in Section 3.3, we will extend the basic paradigm to
non-Markovian environments, effectively devising a localization algorithm that works well
in a broad range of dynamic environments. For now, however, we will adhere to the Markov
assumption, to facilitate the derivation of the basic algorithm.
395

fiFox, Burgard & Thrun

When computing P (LT = l j d), we distinguish two cases, depending on whether the
most recent data item dT is a sensor measurement or an odometry reading.
Case 1: The most recent data item is a sensor measurement dT = sT .
Here
P (LT = l j d) = P (LT = l j d0 ; : : : ; dT 1 ; sT ):
(4)
Bayes rule suggests that this term can be transformed to
P (sT j d0 ; : : : ; dT 1 ; LT = l) P (LT = l j d0 ; : : : ; dT 1 )
;
(5)
P (s j d ; : : : ; d )
T

T

0

1

which, because of our Markov assumption, can be simplified to:
P (sT j LT = l) P (LT = l j d0 ; : : : ; dT 1 )
:
(6)
P (sT j d0 ; : : : ; dT 1 )
We also observe that the denominator can be replaced by a constant ffT , since it does not
depend on LT . Thus, we have
P (LT = l j d) = ffT P (sT j LT = l) P (LT = l j d0 ; : : : ; dT 1 ):
(7)
The reader may notice the incremental nature of Equation (7): If we write
Bel(LT = l) = P (LT = l j d0 ; : : : ; dT );
(8)
to denote the robot's belief Equation (7) becomes
Bel(LT = l) = ffT P (sT j l) Bel(LT 1 = l):
(9)
In this equation we replaced the term P (sT j LT = l) by P (sT j l) based on the assumption
that it is independent of the time.
Case 2: The most recent data item is an odometry reading: dT = aT .
Here we compute P (LT = l j d) using the Theorem of Total Probability:
Z
P (LT = l j d) =
P (LT = l j d; LT 1 = l0 ) P (LT 1 = l0 j d) dl0 :
(10)
Consider the first term on the right-hand side. Our Markov assumption suggests that
P (LT = l j d; LT 1 = l0 ) = P (LT = l j d0 ; : : : ; dT 1 ; aT ; LT 1 = l0 )
(11)
0
= P (LT = l j aT ; LT 1 = l )
(12)
The second term on the right-hand side of Equation (10) can also be simplified by observing
that aT does not carry any information about the position LT 1:
P (LT 1 = l0 j d) = P (LT 1 = l0 j d0 ; : : : ; dT 1 ; aT )
(13)
0
= P (LT 1 = l j d0 ; : : : ; dT 1 )
(14)
396

fiMarkov Localization for Mobile Robots in Dynamic Environments

Substituting 12 and 14 back into Equation (10) gives us the desired result
P (LT

= l j d) =

Z

P (LT

= l j aT ; LT 1 = l0) P (LT 1 = l0 j d0 ; : : : ; dT 1) dl0 : (15)

Notice that Equation (15) is, too, of an incremental form. With our definition of belief
above, we have
Bel(LT

= l) =

Z

P (l j aT ; l0 ) Bel(LT

1

= l0) dl0:

(16)

Please note that we used P (l j aT ; l0 ) instead of P (LT = l j aT ; LT 1 = l0 ) since we assume
that it does not change over time.
2.4 The Markov Localization Algorithm

Update Equations (9) and (16) form the core of the Markov localization algorithm. The full
algorithm is shown in Table 1. Following Basye et al. (1992) and Russell & Norvig (1995),
we denote P (l j a; l0 ) as the robot's motion model, since it models how motion effect the
robot's position. The conditional probability P (s j l) is called perceptual model, because it
models the outcome of the robot's sensors.
In the Markov localization algorithm P (L0 = l), which initializes the belief Bel(L0),
reects the prior knowledge about the starting position of the robot. This distribution
can be initialized arbitrarily, but in practice two cases prevail: If the position of the robot
relative to its map is entirely unknown, P (L0 ) is usually uniformly distributed. If the initial
position of the robot is approximately known, then P (L0 ) is typically a narrow Gaussian
distribution centered at the robot's position.
2.5 Implementations of Markov Localization

The reader may notice that the principle of Markov localization leaves open
1. how the robot's belief Bel(L) is represented and
2. how the conditional probabilities P (l j a; l0 ) and P (s j l) are computed.
Accordingly, existing approaches to Markov localization mainly differ in the representation
of the state space and the computation of the perceptual model. In this section we will
briey discuss different implementations of Markov localization focusing on these two topics
(see Section 5 for a more detailed discussion of related work).
1. State Space Representations: A very common approach for the representation of
the robots belief Bel(L) is based on Kalman filtering (Kalman, 1960; Smith et al.,
1990) which rests on the restrictive assumption that the position of the robot can be
modeled by a unimodal Gaussian distribution. Existing implementations (Leonard
& Durrant-Whyte, 1992; Schiele & Crowley, 1994; Gutmann & Schlegel, 1996; Arras & Vestli, 1998) have proven to be robust and accurate for keeping track of the
robot's position. Because of the restrictive assumption of a Gaussian distribution these
techniques lack the ability to represent situations in which the position of the robot
397

fiFox, Burgard & Thrun

for each

location l do
Bel(L0 = l)

P (L0 = l)

/* initialize the belief */
(17)

end for

forever do
if

new sensory input sT is received do
ffT
0
for each location l do
/* apply the perception model */
d (LT = l)
Bel
P (sT j l)  Bel(LT 1 = l)
(18)
d (LT = l)
ffT
ffT + Bel
(19)
end for
for each

location l do
Bel(LT = l)

ffT

/* normalize the belief */
1  Bel
d (LT = l)
(20)

end for
end if
if

an odometry reading aT is received do
for each location l do
Bel(LT

= l)

Z

/* apply the motion model */

P (l j l0 ; aT )  Bel(LT

1

= l0 ) dl0

(21)

end for
end if
end forever

Tab. 1. The Markov localization algorithm
maintains multiple, distinct beliefs (c.f. 2). As a result, localization approaches using
Kalman filters typically require that the starting position of the robot is known and
are not able to re-localize the robot in the case of localization failures. Additionally,
Kalman filters rely on sensor models that generate estimates with Gaussian uncertainty. This assumption, unfortunately, is not met in all situations (see for example
Dellaert et al. 1999).
398

fiMarkov Localization for Mobile Robots in Dynamic Environments

To overcome these limitations, different approaches have used increasingly richer
schemes to represent uncertainty in the robot's position, moving beyond the Gaussian
density assumption inherent in the vanilla Kalman filter. Nourbakhsh et al. (1995),
Simmons & Koenig (1995), and Kaelbling et al. (1996) use Markov localization for
landmark-based corridor navigation and the state space is organized according to the
coarse, topological structure of the environment and with generally only four possible
orientations of the robot. These approaches can, in principle, solve the problem of
global localization. However, due to the coarse resolution of the state representation,
the accuracy of the position estimates is limited. Topological approaches typically give
only a rough sense as to where the robot is. Furthermore, these techniques require
that the environment satisfies an orthogonality assumption and that there are certain
landmarks or abstract features that can be extracted from the sensor data. These
assumptions make it dicult to apply the topological approaches in unstructured
environments.
2.

In addition to the different representations of the state space various
perception models have been developed for different types of sensors (see for example
Moravec, 1988; Kortenkamp & Weymouth, 1994; Simmons & Koenig, 1995; Burgard
et al., 1996; Dellaert et al., 1999; and Konolige, 1999). These sensor models differ
in the way how they compute the probability of the current measurement. Whereas
topological approaches such as (Kortenkamp & Weymouth, 1994; Simmons & Koenig,
1995; Kaelbling et al., 1996) first extract landmark information out of a sensor scan,
the approaches in (Moravec, 1988; Burgard et al., 1996; Dellaert et al., 1999; Konolige,
1999) operate on the raw sensor measurements. The techniques for proximity sensors
described in (Moravec, 1988; Burgard et al., 1996; Konolige, 1999) mainly differ in
their eciency and how they model the characteristics of the sensors and the map of
the environment.

Sensor Models:

In order to combine the strengths of the previous representations, our approach relies on
a fine and less restrictive representation of the state space (Burgard et al., 1996, 1998b;
Fox, 1998). Here the robot's belief is approximated by a fine-grained, regularly spaced grid,
where the spatial resolution is usually between 10 and 40 cm and the angular resolution is
usually 2 or 5 degrees. The advantage of this approach compared to the Kalman-filter based
techniques is its ability to represent multi-modal distributions, a prerequisite for global
localization from scratch. In contrast to the topological approaches to Markov localization,
our approach allows accurate position estimates in a much broader range of environments,
including environments that might not even possess identifiable landmarks. Since it does
not depend on abstract features, it can incorporate raw sensor data into the robot's belief.
And it typically yields results that are an order of magnitude more accurate. An obvious
shortcoming of the grid-based representation, however, is the size of the state space that
has to be maintained. Section 3.4 addresses this issue directly by introducing techniques
that make it possible to update extremely large grids in real-time.
399

fiFox, Burgard & Thrun

(a)

(b)

Fig. 3. Typical \banana-shaped" distributions resulting from different motion actions.
3. Metric Markov Localization for Dynamic Environments

In this section we will describe our metric variant of Markov localization. This includes
appropriate motion and sensor models. We also describe a filtering technique which is
designed to overcome the assumption of a static world model generally made in Markov
localization and allows to localize a mobile robot even in densely crowded environments.
We then describe our fine-grained grid-based representation of the state space and present
techniques to eciently update even large state spaces.
3.1 The Action Model

To update the belief when the robot moves, we have to specify the action model P (l j l0; at ).
Based on the assumption of normally distributed errors in translation and rotation, we
use a mixture of two independent, zero-centered Gaussian distributions whose tails are cut
off (Burgard et al., 1996). The variances of these distributions are proportional to the length
of the measured motion.
Figure 3 illustrates the resulting densities for two example paths if the robot's belief
starts with a Dirac distribution. Both distributions are three-dimensional (in hx; y; i-space)
and Figure 3 shows their 2D projections into hx; yi-space.
3.2 The Perception Model for Proximity Sensors

As mentioned above, the likelihood P (s j l) that a sensor reading s is measured at position l has to be computed for all positions l in each update of the Markov localization
algorithm (see Table 1). Therefore, it is crucial for on-line position estimation that this
quantity can be computed very eciently. Moravec (1988) proposed a method to compute
a generally non-Gaussian probability density function P (s j l) over a discrete set of possible
distances measured by an ultrasound sensor at location l. In a first implementation of our
approach (Burgard et al., 1996) we used a similar method, which unfortunately turned out
to be computationally too expensive for localization in real-time.
To overcome this disadvantage, we developed a sensor-model which allows to compute
P (s j l) solely based on the distance ol to the closest obstacle in the map along the direction
of the sensor. This distance can be computed by ray-tracing in occupancy grid maps or
400

fiMarkov Localization for Mobile Robots in Dynamic Environments

0.125

ol

0.1

Sonar
Laser
probability Pu (di )

probability Pm (di | l)

0.125

0.075

0.05

0.025

0

Sonar
Laser

0.1

0.075

0.05

0.025

100

200

300

measured distance di [cm]

400

0

500

100

200

(a)

300

measured distance di [cm]

400

500

(b)

Fig. 4. Probability of measuring a distance di (a) if obstacle in distance ol is detected and (b) due
to unknown obstacles.

CAD-models of the environment. In particular, we consider a discretization d1 ; : : : ; dn of
possible distances measured by a proximity sensor. In our discretization, the size of the
ranges d = di+1 di is the same for all i, and dn corresponds to the maximal range of
the proximity sensor2. Let P (di j l) denote the probability of measuring a distance di if the
robot is at location l. In order to derive this probability we first consider the following two
cases (see also Hennig 1997 and Fox 1998):
a.) Known obstacles: If the sensor detects an obstacle the resulting distribution is
modeled by a Gaussian distribution with mean at the distance to this obstacle. Let
Pm (d j l) denote the probability of measuring distance d if the robot is at location l,
assuming that the sensor beam is reected by the closest obstacle in the map (along the
sensor beam). We denote the distance to this specific obstacle by ol . The probability
Pm (d j l) is then given by a Gaussian distribution with mean at ol :
d ol
1
(22)
Pm (d j l) = p e 
 2
The standard deviation  of this distribution models the uncertainty of the measured
distance, based on
 the granularity of the discretization of L, which represents the robot's position,
 the accuracy of the world model, and
 the accuracy of the sensor.
Figure 4(a) gives examples of such Gaussian distributions for ultrasound sensors and
laser range-finders. Here the distance ol to the closest obstacle is 230cm. Observe here
that the laser sensor has a higher accuracy than the ultrasound sensor, as indicated
by the smaller variance.
b.) Unknown obstacles: In Markov localization, the world model generally is assumed
to be static and complete. However, mobile robot environments are often populated
and therefore contain objects that are not included in the map. Consequently, there is
(

2 2

)2

2. Typical values for n are between 64 and 256 and the maximal range dn is typically 500cm or 1000cm.
401

fiFox, Burgard & Thrun

a non-zero probability that the sensor is reected by an obstacle not represented in the
world model. Assuming that these objects are equally distributed in the environment,
the probability Pu (di ) of detecting an unknown obstacle at distance di is independent
of the location of the robot and can be modeled by a geometric distribution. This
distribution results from the following observation. A distance di is measured if the
sensor is not reected by an obstacle at a shorter distance dj<i and is reected at
distance di . The resulting probability is
(
0
i=0
Pu (di ) =
(23)
P
cr (1
j<i Pu (dj )) otherwise:
In this equation the constant cr is the probability that the sensor is reected by an
unknown obstacle at any range given by the discretization.
A typical distribution for sonar and laser measurements is depicted in Figure 4(b). In
this example, the relatively large probability of measuring 500cm is due to the fact
that the maximum range of the proximity sensors is set to 500cm. Thus, this distance
represents the probability of measuring at least 500cm.
Obviously, only one of these two cases can occur at a certain point in time, i.e., the
sensor beam is either reected by a known or an unknown object. Thus, P (di j l) is a
a mixture of the two distributions Pm and Pu. To determine the combined probability
P (di j l) of measuring a distance di if the robot is at location l we consider the following
two situations: A distance di is measured, if
a.) the sensor beam is
1.) not reected by an unknown obstacle before reaching distance di
X
a1 = 1
Pu (dj );
(24)
j<i

2.)

and

reected by the known obstacle at distance di
a2 = cd Pm (di j l)

(25)

b.) OR the beam is
1.) reected neither by an unknown obstacle nor by the known obstacle before
reaching distance di
X
b1 = 1
P (dj j l)
(26)
j<i

2.)

and

reected by an unknown obstacle at distance di
b2 = cr :
402

(27)

fiMarkov Localization for Mobile Robots in Dynamic Environments

0.125

Approximated
Measured

0.1

probability p(d i | l)

probability p(d i | l)

0.125

ol

0.075

0.05

0.025

0

Approximated
Measured

0.1

ol

0.075

0.05

0.025

100

200

300

400

0

500

(a)

measured distance di [cm]

100

200

300

measured distance di [cm]

400

500

(b)

Fig. 5. Measured and approximated probabilities of (a) sonar and (b) laser measurements given
the distance ol to the closest obstacle along the sensing direction.

The parameter cd in Equation (25) denotes the probability that the sensor detects the closest
obstacle in the map. These considerations for the combined probability are summarized in
Equation (28). By double negation and insertion of the Equations (24) to (27), we finally
get Equation (31).


P (di j l) =
p
(a1 ^ a2 ) _ (b1 ^ b2)
(28)


= :p :(a1 ^ a2 ) ^ :(b1 ^ b2)
(29)


= 1 [1 P (a1a2 )]  [1 P (b1 b2 )]
(30)


X
X
= 1 1 (1
Pu (dj )) cd Pm (di j l)))  (1 (1
P (dj )) cr (31)
j<i

j<i

To obtain the probability of measuring dn, the maximal range of the sensor, we exploit the
following equivalence: The probability of measuring a distance larger than or equal to the
maximal sensor range is equivalent to the probability of not measuring a distance shorter
than dn. In our incremental scheme, this probability can easily be determined:
X
P (dn j l) = 1
P (dj j l)
(32)
j<n

To summarize, the probability of sensor measurements is computed incrementally for the
different distances starting at distance d1 = 0cm. For each distance we consider the probability that the sensor beam reaches the corresponding distance and is reected either by
the closest obstacle in the map (along the sensor beam), or by an unknown obstacle.
In order to adjust the parameters , cr and cd of our perception model we collected
eleven million data pairs consisting of the expected distance ol and the measured distance
di during the typical operation of the robot. From these data we were able to estimate the
probability of measuring a certain distance di if the distance ol to the closest obstacle in
the map along the sensing direction is given. The dotted line in Figure 5(a) depicts this
probability for sonar measurements if the distance ol to the next obstacle is 230cm. Again,
the high probability of measuring 500cm is due to the fact that this distance represents
the probability of measuring at least 500cm. The solid line in the figure represents the
distribution obtained by adapting the parameters of our sensor model so as to best fit the
403

fiFox, Burgard & Thrun

probability

probability

0.4

0.4

0.3

0.3

0.2

0.2

0.1

0.1
500

0

500

0

400

400

300
measured distance [cm]
200

100
300
expected distance [cm]

300
measured distance [cm]
200

100

200

200
100

300
expected distance [cm]

400

100
400

(a)

(b)

probability

probability

0.4

0.4

0.3

0.3

0.2

0.2

0.1

0.1
500

0

500

0

400

400

300
measured distance [cm]
200

100
300
expected distance [cm]

300
measured distance [cm]
200

100

200

200
100

300
expected distance [cm]

400

(c)

100
400

(d)

Fig. 6. Measured and approximated probability of sonar (a,b) and laser (c,d) measurements, respectively. Each table contains the probabilities of distance measurements given the expected distance
ol extracted from a map of the environment.

measured data. The corresponding measured and approximated probabilities for the laser
sensor are plotted in Figure 5(b).
The observed densities for all possible distances ol to an obstacle for ultrasound sensors
and laser range-finder are depicted in Figure 6(a) and Figure 6(c), respectively. The approximated densities are shown in Figure 6(b) and Figure 6(d). In all figures, the distance ol is
labeled \expected distance". The similarity between the measured and the approximated
distributions shows that our sensor model yields a good approximation of the data.
Please note that there are further well-known types of sensor noise which are not explicitly represented in our sensor model. Among them are specular reections or cross-talk
which are often regarded as serious sources of noise in the context of ultra-sound sensors.
However, these sources of sensor noise are modeled implicitly by the geometric distribution
resulting from unknown obstacles.
3.3 Filtering Techniques for Dynamic Environments

Markov localization has been shown to be robust to occasional changes of an environment
such as opened / closed doors or people walking by. Unfortunately, it fails to localize a
robot if too many aspects of the environment are not covered by the world model. This
is the case, for example, in densely crowded environments, where groups of people cover
the robots sensors and thus lead to many unexpected measurements. The mobile robots
Rhino and Minerva, which were deployed as interactive museum tour-guides (Burgard et al.,
1998a, 2000; Thrun et al., 1999), were permanently faced with such a situation. Figure 7
404

fiMarkov Localization for Mobile Robots in Dynamic Environments

RHINO

(a)

(b)

Fig. 7. Rhino surrounded by visitors in the Deutsches

Museum Bonn

.

(a)

(b)

Fig. 8. Typical laser scans obtained when Rhino is surrounded by visitors.

shows two cases in which the robot Rhino is surrounded by many visitors while giving a
tour in the Deutsches Museum Bonn, Germany.
The reason why Markov localization fails in such situations is the violation of the Markov
assumption, an independence assumption on which virtually all localization techniques are
based. As discussed in Section 2.3, this assumption states that the sensor measurements
observed at time t are independent of all other measurements, given that the current state
Lt of the world is known. In the case of localization in densely populated environments,
this independence assumption is clearly violated when using a static model of the world.
To illustrate this point, Figure 8 depicts two typical laser scans obtained during the
museum projects (maximal range measurements are omitted). The figure also shows the
obstacles contained in the map. Obviously, the readings are, to a large extent, corrupted,
since people in the museum are not represented in the static world model. The different
shading of the beams indicates the two classes they belong to: the black lines correspond
to the static obstacles in the map and are independent of each other if the position of the
robot is known. The grey-shaded lines are beams reected by visitors in the Museum. These
sensor beams cannot be predicted by the world model and therefore are not independent
of each other. Since the vicinity of people usually increases the robot's belief of being close
to modeled obstacles, the robot quickly loses track of its position when incorporating all
405

fiFox, Burgard & Thrun

sensor measurements. To reestablish the independence of sensor measurements we could
include the position of the robot and the position of people into the state variable L.
Unfortunately, this is infeasible since the computational complexity of state estimation
increases exponentially in the number of dependent state variables to be estimated.
A closely related solution to this problem could be to adapt the map according to the
changes of the environment. Techniques for concurrent map-building and localization such
as (Lu & Milios, 1997a; Gutmann & Schlegel, 1996; Shatkey & Kaelbling, 1997; Thrun et
al., 1998b), however, also assume that the environment is almost static and therefore are
unable to deal with such environments. Another approach would be to adapt the perception
model to correctly reect such situations. Note that our perceptual model already assigns
a certain probability to events where the sensor beam is reected by an unknown obstacle.
Unfortunately, such approaches are only capable to model such noise on average. While such
approaches turn out to work reliably with occasional sensor blockage, they are not sucient
in situations where more than fifty percent of the sensor measurements are corrupted. Our
localization system therefore includes filters which are designed to detect whether a certain
sensor reading is corrupted or not. Compared to a modification of the static sensor model
described above, these filters have the advantage that they do not average over all possible
situations and that their decision is based on the current belief of the robot.
The filters are designed to select those readings of a complete scan which do not come
from objects contained in the map. In this section we introduce two different kinds of filters.
The first one is called entropy filter. Since it filters a reading based solely on its effect on
the belief Bel(L), it can be applied to arbitrary sensors. The second filter is the distance
filter which selects the readings according to how much shorter they are than the expected
value. It therefore is especially designed for proximity sensors.
3.3.1 The Entropy Filter

The entropy H (L) of the belief over L is defined as
X
H (L) =
Bel(L = l) log Bel(L = l)
l

(33)

and is a measure of uncertainty about the outcome of the random variable L (Cover &
Thomas, 1991). The higher the entropy, the higher the robot's uncertainty as to where it
is. The entropy filter measures the relative change of entropy upon incorporating a sensor
reading into the belief Bel(L). More specifically, let s denote the measurement of a sensor
(in our case a single range measurement). The change of the entropy of Bel(L) given s is
defined as:
H (L j s) := H (L j s) H (L)
(34)
The term H (L j s) is the entropy of the belief Bel(L) after incorporating the sensor measurement s (see Equations (18) { (20)). While a positive change of entropy indicates that
after incorporating s, the robot is less certain about its position, a negative change indicates
an increase in certainty. The selection scheme of the entropy filter is to exclude all sensor
measurements s with H (L j s) < 0. In other words, it only uses those sensor readings
confirming the robot's current belief.
406

fiMarkov Localization for Mobile Robots in Dynamic Environments

1

ol

probability

0.8

Pshort (di j l)
Pm (di j l)

0.6

0.4

0.2

0

100

200

300

400

500

measured distance [cm]

Fig. 9. Probability Pm (di j l) of expected measurement and probability Pshort (di j l) that a distance
di is shorter than the expected measurement given the location l.

Entropy filters work well when the robot's belief is focused on the correct hypothesis.
However, they may fail in situations in which the robot's belief state is incorrect. This topic
will be analyzed systematically in the experiments described in Section 4.1. The advantage
of the entropy filter is that it makes no assumptions about the nature of the sensor data
and the kind of disturbances occurring in dynamic environments.
3.3.2 The Distance Filter

The distance filter has specifically been designed for proximity sensors such as laser rangefinders. Distance filters are based on a simple observation: In proximity sensing, unmodeled
obstacles typically produce readings that are shorter than the distance expected from the
map. In essence, the distance filter selects sensor readings based on their distance relative
to the distance to the closest obstacle in the map.
To be more specific, this filter removes those sensor measurements s which with probability higher than  (this threshold is set to 0:99 in all experiments) are shorter than
expected, and which therefore are caused by an unmodeled object (e.g. a person).
To see, let d1 ; : : : ; dn be a discrete set of possible distances measured by a proximity
sensor. As in Section 3.2, we denote by Pm (di j l) the probability of measuring distance di
if the robot is at position l and the sensor detects the closest obstacle in the map along the
sensing direction. The distribution Pm describes the sensor measurement expected from the
map. As described above, this distribution is assumed to be Gaussian with mean at the
distance ol to the closest obstacle along the sensing direction. The dashed line in Figure 9
represents Pm , for a laser range-finder and a distance ol of 230cm. We now can define the
probability P (di j l) that a measured distance di is shorter than the expected one given
the robot is at position l. This probability is obviously equivalent to the probability that
the expected measurement ol is longer than di given the robot is at location l and thus can
be computed as follows:
X
P (di j l) =
Pm (dj j l):
(35)
short

short

j>i

407

fiFox, Burgard & Thrun
Bel(Lt = l)

y



x

(0; 0; 0)

Fig. 10. Grid-based representation of the state space

In practice, however, we are interested in the probability P (di ) that di is shorter
than expected, given the complete current belief of the robot. Thus, we have to average
over all possible positions of the robot:
X
P (di ) =
P (di j l)Bel(L = l)
(36)
short

short

l

short

Given the distribution P (di ), we now can implement the distance filter by excluding all
sensor measurements di with P (di ) >  . Whereas the entropy filter filters measurements
according to their effect on the belief state of the robot the distance filter selects measurements solely based on their value and regardless of their effect on the robot's certainty.
It should be noted that Fox (1998) additionally developed a blockage filter for proximity
sensors, which is based on a probabilistic description of situations in which a sensor is
blocked by an unknown obstacle. We omit this filter here since its derivation is quite complex
and the resulting filter is not significantly different from the distance filter described here.
short

short

3.4 Grid-based Representation of the State Space

We will now return to the issue of how to represent and compute the belief distribution
of the robot eciently, describing what one might think of as the \nut and bolts" of gridbased Markov localization. Recall that to obtain accurate metric position estimates, our
approach to Markov localization uses a fine-grained discretization of the state space. Here
L is represented by a three-dimensional, regularly spaced grid, where the spatial resolution
is usually between 10cm and 40cm and the angular resolution is usually 2 or 5 degrees.
Figure 10 illustrates the structure of a position probability grid. Each layer of such a grid
corresponds to all possible poses of the robot with the same orientation.
While such a fine-grained approximation makes it possible to estimate the robot's position with high accuracy, an obvious disadvantage of such a fine-grained discretization lies
408

fiMarkov Localization for Mobile Robots in Dynamic Environments

in the huge state space which has to be maintained. For a mid-size environment of size
30  30m2 , an angular grid resolution of 2 , and a cell size of 15  15cm2 the state space
consists of 7; 200; 000 states. The basic Markov localization algorithm updates each of these
states for each sensory input and each atomic movement of the robot. Current computer
speed, thus, makes it impossible to update matrices of this size in real-time.
To update such state spaces eciently, we have developed two techniques, which are
described in the remainder of this section. The first method, introduced in Section 3.4.1,
pre-computes the sensor model. It allows us to determine the likelihood P (s j l) of sensor
measurements by two look-up operations|instead of expensive ray tracing operations. The
second optimization, described in Section 3.4.2, is a selective update strategy. This strategy
focuses the computation, by only updating the relevant part of the state space. Based on
these two techniques, grid-based Markov localization can be applied on-line to estimate the
position of a mobile robot during its operation, using a low-cost PC.
3.4.1 Pre-Computation of the Sensor Model

As described in Section 3.2, the perception model P (s j l) for proximity sensors only depends
on the distance ol to the closest obstacle in the map along the sensor beam. Based on the
assumption that the map of the environment is static, our approach pre-computes and stores
these distances ol for each possible robot location l in the environment. Following our sensor
model, we use a discretization d1 ; : : : ; dn of the possible distances ol . This discretization
is exactly the same for the expected and the measured distances. We then store for each
location l only the index of the expected distance ol in a three-dimensional table. Please
note that this table only needs one byte per value if 256 different values for the discretization
of ol are used. The probability P (di j ol ) of measuring a distance di if the closest obstacle
is at distance ol (see Figure 6) can also be pre-computed and stored in a two-dimensional
lookup-table.
As a result, the probability P (s j l) of measuring s given a location l can quickly be
computed by two nested lookups. The first look-up retrieves the distance ol to the closest
obstacle in the sensing direction given the robot is at location l. The second lookup is then
used to get the probability P (s j ol ). The ecient computation based on table look-ups
enabled our implementation to quickly incorporate even laser-range scans that consist of
up to 180 values in the overall belief state of the robot. In our experiments, the use of
the look-up tables led to a speed-up-factor of 10, when compared to a computation of the
distance to the closest obstacle at run-time.
3.4.2 Selective Update

The selective update scheme is based on the observation that during global localization,
the certainty of the position estimation permanently increases and the density quickly concentrates on the grid cells representing the true position of the robot. The probability of
the other grid cells decreases during localization and the key idea of our optimization is to
exclude unlikely cells from being updated.
For this purpose, we introduce a threshold3 " and update only those grid cells l with
Bel(Lt = l) > ". To allow for such a selective update while still maintaining a density over
3. In our current implementation " is set to 1% of the a priori position probability.
409

fiFox, Burgard & Thrun

the entire state space, we approximate P (st j l) for cells with Bel(Lt = l)  " by the a
priori probability of measuring st . This quantity, which we call Pe (st ), is determined by

averaging over all possible locations of the robot:
X
Pe (st ) = P (st j l) P (l)
l

(37)

Please note that Pe (st) is independent of the current belief state of the robot and can
be determined beforehand. The incremental update rule for a new sensor measurement st
is changed as follows (compare Equation (9)):
(
fft  P (st j l)  Bel(Lt 1 = l) if Bel(Lt 1 = l) > "
Bel(Lt = l)
(38)
fft  P~ (st )  Bel(Lt 1 = l) otherwise
By multiplying Pe (st) into the normalization factor fft , we can rewrite this equation as
8
< ff~ t  P (st jl)  Bel(Lt 1 = l) if Bel(Lt 1 = l) > "
Pe(st )
(39)
Bel(Lt = l)
: ff~ t
 Bel(Lt 1 = l) otherwise
where ff~t = fft  Pe (st ).
The key advantage of the selective update scheme given in Equation (39) is that all cells
with Bel(Lt 1 = l)  " are updated with the same value ff~t . In order to obtain smooth
transitions between global localization and position tracking and to focus the computation
on the important regions of the state space L, for example, in the case of ambiguities we use
a partitioning of the state space. Suppose the state space L is partitioned into n segments
or parts 1; : : : ; n. A segment i is called active at time t if it contains locations with probability above the threshold "; otherwise we call such a part passive because the probabilities
of all cells are below the threshold. Obviously, we can keep track of the individual probabilities within a passive part i by accumulating the normalization factors ff~t into a value
fii . Whenever a segment i becomes passive, i.e. the probabilities of all locations within
i no longer exceed ", the normalizer fii (t) is initialized to 1 and subsequently updated as
follows: fii(t + 1) = ff~t  fii (t). As soon as a part becomes active again, we can restore the
probabilities of the individual grid cells by multiplying the probabilities of each cell with the
accumulated normalizer fii (t). By keeping track of the robot motion since a part became
passive, it suces to incorporate the accumulated motion whenever the part becomes active
again. In order to eciently detect whether a passive part has to be activated again, we
store the maximal probability Pimax of all cells in the part at the time it becomes passive.
Whenever Pimax  fii (t) exceeds ", the part i is activated again because it contains at least
one position with probability above the threshold. In our current implementation we partition the state space L such that each part i consists of all locations with equal orientation
relative to the robot's start location.
To illustrate the effect of this selective update scheme, let us compare the update of
active and passive cells on incoming sensor data. According to Equation (39), the difference
lies in the ratio P (st j l)=P~ (st ). An example of this ratio for our model of proximity sensors
is depicted in Figure 11 (here, we replaced st by a proximity measurement di ). In the
beginning of the localization process, all cells are active and updated according to the ratio
410

fiMarkov Localization for Mobile Robots in Dynamic Environments

ol

9

Laser
Sonar

8

likelihood ratio

7
6
5
4
3
2
1
0

100

200

300

400

500

measured distance di [cm]

Fig. 11. Ratio PP~((ddiijl)) for sonar and laser measurements for expected distance ol of 230cm.

depicted in Figure 11. The measured and expected distances for cells that do not represent
the true location of the robot usually deviate significantly. Thus, the probabilities of these
cells quickly fall below the threshold ".
Now the effect of the selective update scheme becomes obvious: Those parts of the state
space that do not align well with the orientation of the environment, quickly become passive
as the robot localizes itself. Consequently, only a small fraction of the state space has to
be updated as soon as the robot has correctly determined its position. If, however, the
position of the robot is lost, then the likelihood ratios for the distances measured at the
active locations become smaller than one on average. Thus the probabilities of the active
locations decrease while the normalizers fii of the passive parts increase until these segments
are activated again. Once the true position of the robot is among the active locations, the
robot is able to re-establish the correct belief.
In extensive experimental tests we did not observe evidence that the selective update
scheme has a noticably negative impact on the robot's behavior. In contrast, it turned
out to be highly effective, since in practice only a small fraction (generally less than 5%)
of the state space has to be updated once the position of the robot has been determined
correctly, and the probabilities of the active locations generally sum up to at least 0.99.
Thus, the selective update scheme automatically adapts the computation time required to
update the belief to the certainty of the robot. This way, our system is able to eciently
track the position of a robot once its position has been determined. Additionally, Markov
localization keeps the ability to detect localization failures and to relocalize the robot. The
only disadvantage lies in the fixed representation of the grid which has the undesirable
effect that the memory requirement in our current implementation stays constant even if
only a minor part of the state space is updated. In this context we would like to mention
that recently promising techniques have been presented to overcome this disadvantage by
applying alternative and dynamic representations of the state space (Burgard et al., 1998b;
Fox et al., 1999).
4. Experimental Results

Our metric Markov localization technique, including both sensor filters, has been implemented and evaluated extensively in various environments. In this section we present some
411

fiFox, Burgard & Thrun

of the experiments carried out with the mobile robots Rhino and Minerva (see Figure 1).
Rhino has a ring of 24 ultrasound sensors each with an opening angle of 15 degrees. Both,
Rhino and Minerva are equipped with two laser range-finders covering a 360 degrees field
of view.
The first set of experiments demonstrates the robustness of Markov localization in two
real-world scenarios. In particular, it systematically evaluates the effect of the filtering
techniques on the localization performance in highly dynamic environments. An additional
experiment illustrates a further advantage of the filtering technique, which enables a mobile
robot to reliably estimate its position even if only an outline of an oce environment is
given as a map.
In further experiments described in this section, we will illustrate the ability of our
Markov localization technique to globally localize a mobile robot in approximate world
models such as occupancy grid maps, even when using inaccurate sensors such as ultrasound
sensors. Finally, we present experiments analyzing the accuracy and eciency of grid-based
Markov localization with respect to the size of the grid cells.
The experiments reported here demonstrate that Markov localization is able to globally
estimate the position of a mobile robot, and to reliably keep track of it even if only an
approximate model of a possibly dynamic environment is given, if the robot has a weak
odometry, and if noisy sensors such as ultrasound sensors are used.
4.1 Long-term Experiments in Dynamic Environments

For our mobile robots Rhino and Minerva, which operated in the Deutsches Museum Bonn
and the US-Smithsonian's National Museum of American History, the robustness and reliability of our Markov localization system was of utmost importance. Accurate position
estimation was a crucial component, as many of the obstacles were \invisible" to the robots'
sensors (such as glass cages, metal bars, staircases, and the alike). Given the estimate of
the robot's position (Fox et al., 1998b) integrated map information into the collision avoidance system in order to prevent the robot from colliding with obstacles that could not be
detected.
Figure 12(a) shows a typical trajectory of the robot Rhino, recorded in the museum
in Bonn, along with the map used for localization. The reader may notice that only the
obstacles shown in black were actually used for localization; the others were either invisible
or could not be detected reliably. Rhino used the entropy filter to identify sensor readings
that were corrupted by the presence of people. Rhino's localization module was able to (1)
globally localize the robot in the morning when the robot was switched on and (2) to reliably
and accurately keep track of the robot's position. In the entire six-day deployment period, in
which Rhino traveled over 18km, our approach led only to a single software-related collision,
which involved an \invisible" obstacle and which was caused by a localization error that
was slightly larger than a 30cm safety margin.
Figure 12(b) shows a 2km long trajectory of the robot Minerva in the National Museum
of American History. Minerva used the distance filter to identify readings reected by
unmodeled objects. This filter was developed after Rhino's deployment in the museum in
Bonn, based on an analysis of the localization failure reported above and in an attempt to
prevent similar effects in future installations. Based on the distance filter, Minerva was able
412

fiMarkov Localization for Mobile Robots in Dynamic Environments

Duration: 4.8 hours
Distance: 1540 meters

Duration: 1 hour
Distance: 2000 meters

(a)

(b)

Fig. 12. Typical trajectories of (a) Rhino in the
National Museum of American History.

Deutsches Museum Bonn

and (b) Minerva in the

to operate reliably over a period of 13 days. During that time Minerva traveled a total of
44km with a maximum speed of 1.63m/sec.
Unfortunately, the evidence from the museum projects is anecdotal. Based on sensor
data collected during Rhino's deployment in the museum in Bonn, we also investigated the
effect of our filter techniques more systematically, and under even more extreme conditions.
In particular, we were interested in the localization results
a.) when the environment is densely populated (more than 50% of the sensor reading are
corrupted), and
b.) when the robot suffers extreme dead-reckoning errors (e.g. induced by a person carrying the robot somewhere else). Since such cases are rare, we manually inicted such
errors into the original data to analyze their effect.
4.1.1 Datasets

During the experiments, we used two different datasets. These sets differ mainly in the
amount of sensor noise.
a.) The first dataset was collected during 2.0 hours of robot motion, in which the robot
traveled approximately 1,000 meters. This dataset was collected when the museum
was closed, and the robot guided only remote Internet-visitors through the museum.
The robot's top speed was 50cm/sec. Thus, this dataset was \ideal" in that the
environment was only sparsely populated, and the robot moved slowly.
b.) The second dataset was recorded during a period of 4.8 hours, during which Rhino
traveled approximately 1,540 meters. The path of this dataset is shown in Figure 12(a). When collecting this data, the robot operated during peak trac hours.
It was frequently faced with situations such as the one illustrated in Figure 7. The
robot's top speed was 80cm/sec.
Both datasets consist of logs of odometry and laser range-finder scans, collected while the
robot moved through the museum. Using the time stamps in the logs, all tests have been
413

fiFox, Burgard & Thrun

Denesely populated
Sparcely populated

Noise [%]

60

40

20

1000

4000

7000

10000

13000

16000

Time [sec]

Fig. 13. Percentage of noisy sensor measurements averaged over time intervals of five minutes.

conducted in real-time simulation on a SUN-Ultra-Sparc 1 (177-MHz). The first dataset
contained more than 32,000, and the second dataset more than 73,000 laser scans. To
evaluate the different localization methods, we generated two reference paths, by averaging
over the estimates of nine independent runs for each filter on the datasets (with small
random noise added to the input data). We verified the correctness of both reference paths
by visual inspection; hence, they can be taken as \ground truth."
Figure 13 shows the estimated percentage of corrupted sensor readings over time for both
datasets. The dashed line corresponds to the first data set, while the solid line illustrates
the corruption of the second (longer) data set. In the second dataset, more than half of
all measurements were corrupted for extended durations of time, as estimated by analyzing
each laser reading post-facto as to whether it was significantly shorter than the distance to
the next obstacle.
4.1.2 Tracking the Robot's Position

In our first series of experiments, we were interested in comparing the ability of all three
approaches|plain Markov localization without filtering, localization with the entropy filter,
and localization with the distance filter|to keep track of the robot's position under normal
working conditions. All three approaches tracked the robot's position in the empty museum
well (first dataset), exhibiting only negligible errors in localization. The results obtained
for the second, more challenging dataset, however, were quite different. In a nutshell,
both filter-based approaches tracked the robot's position accurately, whereas conventional
Markov localization failed frequently. Thus, had we used the latter in the museum exhibit,
it would inevitably have led to a large number of collisions and other failures.
Filter
None
Entropy Distance
failures [%] 1:6  0:4 0:9  0:4 0:0  0:0
failures [%] 26:8  2:4 1:1  0:3 1:2  0:7
I

II

Table 2: Ability to track the robot's position.
Table 2 summarizes the results obtained for the different approaches in this tracking
experiment. The first row of Table 2 provides the percentage of failures for the different
414

fiMarkov Localization for Mobile Robots in Dynamic Environments

final position

Distance at final position: 19 cm
Certainty at final position: 0.003

final position

(a)

Distance at final position: 1 cm
Certainty at final position: 0.987

final position

(b)

Distance at final position: 1 cm
Certainty at final position: 0.998

(c)

Fig. 14. Estimated and real paths of the robot along with endpoints of incorporated sensor measurements using (a) no filter, (b) entropy filter, and (c) distance filter.

filters on the first dataset (error values represent 95% confidence intervals). Position estimates were considered a \failure" if the estimated location of the robot deviated from the
reference path by more than 45cm for at least 20 seconds. The percentage is measured in
time during which the position was lost, relative to the total time of the dataset.
As can be seen here, all three approaches work well, and the distance filter provides the
best performance. The second row provides the failures on the second dataset. While plain
Markov localization failed in 26.8% of the overall time, both filter techniques show almost
equal results with a failure of less than 2%. Thus, the two filter techniques are robust in
highly dynamic environments, plain Markov localization is prone to fail.
To shed light onto the question as to why Markov localization performs so poorly when
compared to the filter algorithms, we analyzed the sensor readings that each method used
during the localization task. Figure 14 shows, for a a small fraction of the data, the measurements incorporated into the robot's belief by the three different approaches. Shown there
are the end points of the sensor measurements used for localization relative to the positions
on the reference path. Obviously, both filter approaches manage to focus their attention on
the \correct" sensor measurements, whereas plain Markov localization incorporates massive
amounts of corrupted (misleading) measurements. As also illustrated by Figure 14, both
filter-based approaches produce more accurate results with a higher certainty in the correct
position.
4.1.3 Recovery from Extreme Localization Failures

We conjecture that a key advantage of the original Markov localization technique lies in its
ability to recover from extreme localization failures. Re-localization after a failure is often
more dicult than global localization from scratch, since the robot starts with a belief that
is centered at a completely wrong position. Since the filtering techniques use the current
belief to select the readings that are incorporated, it is not clear that they still maintain
the ability to recover from global localization failures.
To analyze the behavior of the filters under such extreme conditions, we carried out a
series of experiments during which we manually introduced such failures into the data to
test the robustness of these methods in the extreme. More specifically, we \tele-ported" the
robot at random points in time to other locations. Technically, this was done by changing
the robot's orientation by 18090 degree and shifting it by 0100cm, without letting the
robot know. These perturbations were introduced randomly, with a probability of 0:005 per
415

fiFox, Burgard & Thrun

Filter
trec [sec]

failures [%]
trec [sec]

failures [%]

None
Entropy
Dataset I
237  27 1779  548
10:2  1:8 45:6  7:1
Dataset II
269  60 1310  904
39:5  5:1 72:8  7:3

Distance
188
6:8

 30
 1:6

235
7:8

 46
 1:9

Table 3: Summary of recovery experiments.
meter of robot motion. Obviously, such incidents make the robot lose track of its position.
Each method was tested on 20 differently corrupted versions of both datasets. This resulted
in a total of more than 50 position failures in each dataset. For each of these failures we
measured the time until the methods re-localized the robot correctly. Re-Localization was
assumed to have succeeded if the distance between the estimated position and the reference
path was smaller than 45cm for more than 10 seconds.
Table 3 provides re-localization results for the various methods, based on the two different datasets. Here trec represents the average time in seconds needed to recover from
a localization error. The results are remarkably different from the results obtained under
normal operational conditions. Both conventional Markov localization and the technique
using distance filters are relatively ecient in recovering from extreme positioning errors in
the first dataset, whereas the entropy filter-based approach is an order of magnitude less
ecient (see first row in Table 3). The unsatisfactory performance of the entropy filter in
this experiment is due to the fact that it disregards all sensor measurements that do not
confirm the belief of the robot. While this procedure is reasonable when the belief is correct,
it prevents the robot from detecting localization failures. The percentage of time when the
position of the robot was lost in the entire run is given in the second row of the table. Please
note that this percentage includes both, failures due to manually introduced perturbations
and tracking failures. Again, the distance filter is slightly better than the approach without filter, while the entropy filter performs poorly. The average times trec to recover from
failures on the second dataset are similar to those in the first dataset. The bottom row in
Table 3 provides the percentage of failures for this more dicult dataset. Here the distance
filter-based approach performs significantly better than both other approaches, since it is
able to quickly recover from localization failures and to reliably track the robot's position.
The results illustrate that despite the fact that sensor readings are processed selectively,
the distance filter-based technique recovers as eciently from extreme localization errors as
the conventional Markov approach.
4.2 Localization in Incomplete Maps

A further advantage of the filtering techniques is that Markov localization does not require
a detailed map of the environment. Instead, it suces to provide only an outline which
416

fiMarkov Localization for Mobile Robots in Dynamic Environments

(b)

(a)

(c)

Fig. 15. (a) Outline of the oce environment and (b,c) examples of filtered (grey) and incorporated
(black) sensor readings using the distance filter.
C

22m

3m

A
B
31m

(a)

20m

(b)

Fig. 16. (a) Occupancy grid map of the 1994 AAAI mobile robot competition arena. (b) Trajectory
of the robot and ultrasound measurements used to globally localize the robot in this map.

merely includes the aspects of the world which are static. Figure 15(a) shows a ground plan
of our department building, which contains only the walls of the university building. The
complete map, including all movable objects such as tables and chairs, is shown in Figure 19.
The two Figures 15(b) and 15(c) illustrate how the distance filter typically behaves when
tracking the robot's position in such a sparse map of the environment. Filtered readings
are shown in grey, and the incorporated sensor readings are shown in black. Obviously,
the filter focuses on the known aspects of the map and ignores all objects (such as desks,
chairs, doors and tables) which are not contained in the outline. Fox (1998) describes more
systematic experiments supporting our belief that Markov localization in combination with
the distance filter is able to accurately localize mobile robots even when relying only on an
outline of the environment.
4.3 Localization in Occupancy Grid Maps Using Sonar

The next experiment described here is carried out based on data collected with the mobile
robot Rhino during the 1994 AAAI mobile robot competition (Simmons, 1995). Figure 16(a)
shows an occupancy grid map (Moravec & Elfes, 1985; Moravec, 1988) of the environment,
constructed with the techniques described in (Thrun et al., 1998a; Thrun, 1998b). The size
of the map is 31  22m2 , and the grid resolution is 15cm.
417

fiFox, Burgard & Thrun
Robot position (A)
Robot position (B)

(a)

Robot position (C)

(b)

(c)

Fig. 17. Density plots after incorporating 5, 18, and 24 sonar scans (darker positions are more
likely).

(a)

(b)

Fig. 18. Odometry information and corrected path of the robot.

Figure 16(b) shows a trajectory of the robot along with measurements of the 24 ultrasound sensors obtained as the robot moved through the competition arena. Here we use
this sensor information to globally localize the robot from scratch. The time required to
process this data on a 400MHz Pentium II is 80 seconds, using a position probability grid
with an angular resolution of 3 degrees. Please note that this is exactly the time needed by
the robot to traverse this trajectory; thus, our approach works in real-time. Figure 16(b)
also marks positions of the robot after perceiving 5 (A), 18 (B), and 24 (C) sensor sweeps.
The belief states during global localization at these three points in time are illustrated in
Figure 17.
The figures show the belief of the robot projected onto the hx; yi-plane by plotting for
each hx; yi-position the maximum probability over all possible orientations. More likely
positions are darker and for illustration purposes, Figures 17(a) and 17(b) use a logarithmic
scale in intensity. Figure 17(a) shows the belief state after integrating 5 sensor sweeps (see
also position A in Figure 16(b)). At this point in time, all the robot knows is that it is in one
of the corridors of the environment. After integrating 18 sweeps of the ultrasound sensors,
the robot is almost certain that it is at the end of a corridor (compare position B in Figures 16(b) and 17(b)). A short time later, after turning left and integrating six more sweeps
of the ultrasound ring, the robot has determined its position uniquely. This is represented
by the unique peak containing 99% of the whole probability mass in Figure 17(c).
Figure 18 illustrates the ability of Markov localization to correct accumulated deadreckoning errors by matching ultrasound data with occupancy grid maps. Figure 18(a)
shows a typical 240m long trajectory, measured by Rhino's wheel-encoders in the 1994
418

fiMarkov Localization for Mobile Robots in Dynamic Environments

1

22
8

10
13

2

16
7

20

3 21

5

19
9

12

11

4
6

15 18

17
14

Fig. 19. Path of the robot and reference positions

AAAI mobile robot competition arena. Obviously, the rotational error of the odometry
quickly increases. Already after traveling 40m, the accumulated error in the orientation
(raw odometry) is about 50 degrees. Figure 18(b) shows the path of the robot estimated
by Markov localization, which is significantly more correct.
4.4 Precision and Performance

We will now describe experiments aimed at characterizing the precision of position estimates. Our experiments also characterize the time needed for global localization in relation
to the size of the grid cells. Figure 19 shows a path of the robot Rhino in the Computer
Science Department's building at the University of Bonn. This path includes 22 reference
positions, where the true position of the robot was determined using the scan matching
technique presented in (Gutmann & Schlegel, 1996; Lu & Milios, 1994). All data recorded
during this run were split into four disjoint traces of the sensor data. Each of these different
traces contained the full length of the path, but only every fourth sensor reading which was
sucient to test the localization performance.
Figure 20(a) shows the localization error averaged over the four runs and all reference
positions. The error was determined for different sizes of grid cells, using a laser rangefinder or ultrasound sensors. These results demonstrate (1) that the average localization
error for both sensors is generally below the cell size and (2) that laser range-finders provide
a significantly higher accuracy than ultrasound sensors. When using the laser range-finder
at a spatial resolution of 4cm, the average positioning error can even be reduced to 3.5cm.
Figure 20(b) shows the average CPU-time needed to globally localize the robot as a
function of the size of the grid cells. The values represent the computation time needed
on a 266MHz Pentium II for global localization on the path between the starting point
and position 1. In this experiment, we used a fixed angular resolution of four degrees.
In the case of 64cm cell size, the average localization time is approximately 2.2 seconds.
419

fiFox, Burgard & Thrun

Average localization time [sec]

Average estimation error [cm]

120

Ultrasound sensor
Laser-range finder

30
25
20
15
10
5
0

Ultrasound sensor
Laser-range finder

100
80
60
40
20
0

0

10

20

30
40
Grid cell size [cm]

50

60

70

0

(a)

10

20

30
40
50
Grid cell size [cm]

60

70

(b)

Fig. 20. (a) Average localization error and (b) average CPU-time needed for global localization time
both for ultrasound sensors and laser range-finder depending on the grid resolution.

Of course, the effective time needed for global localization in practice highly depends on
the structure of the environment and the amount of information gathered on the path of
the robot. For example, due to the symmetry of the corridor of this oce environment,
the robot is not able to localize itself unless it enters a room. The reader may notice
that recently, we developed a decision-theoretic method for actively guiding the robot to
places which allow it to resolve ambiguities during global localization (Fox et al., 1998a;
Fox, 1998). Based on this method, the localization process becomes more ecient, especially
in oce environments with a lot of indistinguishable places as, for example, long corridors.
The experiments described above demonstrate that our metric variant of Markov localization is able to eciently estimate the position of a mobile robot in dynamic environments.
It furthermore can deal with approximate models of the environment such as occupancy
grid maps or rough outline maps. Finally, it is able to eciently and accurately estimate
the position of a mobile robot even if ultrasound sensors are used.
5. Related Work

Most of the techniques for mobile robot localization in the literature belong to the class of
local approaches or tracking techniques, which are designed to compensate odometric error
occurring during navigation. They assume that the initial position of the robot is known
(see Borenstein et al. 1996 for a comprehensive overview). For example, Wei et al. (1994)
store angle histograms constructed out of laser range-finder scans taken at different locations
in the environment. The position and orientation of the robot are calculated by maximizing
the correlation between the stored histograms and laser range-scans obtained while the
robot moves through the environment. The estimated position, together with the odometry
information, is then used to predict the position of the robot and to select the histogram
used for the next match. Yamauchi (1996) and Schulz et al. (1999) apply a similar technique,
but they use hill-climbing to match local maps built from ultrasound sensors into a global
occupancy grid map. As in the approach by Wei et al. (1994), the location of the robot
is represented by the position yielding the best match. These techniques, in contrast to
Markov localization, do not represent the uncertainty of the robot in its current belief and
therefore cannot deal appropriately with globally ambiguous situations.
420

fiMarkov Localization for Mobile Robots in Dynamic Environments

A popular probabilistic framework for position tracking are Kalman filters (Maybeck,
1990; Smith et al., 1990), a signal processing technique introduced by Kalman (1960). As
mentioned in Section 2.4, Kalman filter-based methods represent their belief of the robot's
position by a unimodal Gaussian distribution over the three-dimensional state-space of the
robot. The mode of this distribution yields the current position of the robot, and the
variance represents the robot's uncertainty. Whenever the robot moves, the Gaussian is
shifted according to the distance measured by the robot's odometry. Simultaneously, the
variance of the Gaussian is increased according to the model of the robot's odometry. New
sensory input is incorporated into the position estimation by matching the percepts with
the world model.
Existing applications of Kalman filtering to position estimation for mobile robots are
similar in how they model the motion of the robot. They differ mostly in how they update
the Gaussian according to new sensory input. Leonard and Durrant-Whyte (1991) match
beacons extracted from sonar scans with beacons predicted from a geometric map of the
environment. These beacons consist of planes, cylinders, and corners. To update the current estimate of the robot's position, Cox (1991) matches distances measured by infrared
sensors with a line segment description of the environment. Schiele and Crowley (1994)
compare different strategies to track the robot's position based on occupancy grid maps
and ultrasonic sensors. They show that matching local occupancy grid maps with a global
grid map results in a similar localization performance as if the matching is based on features that are extracted from both maps. Shaffer et al. (1992) compare the robustness of
two different matching techniques with different sources of noise. They suggest a combination of map-matching and feature-based techniques in order to inherit the benefits of
both. Lu and Milios (1994,1997b) and Gutmann and Schlegel (1996) use a scan-matching
technique to precisely estimate the position of the robot based on laser range-finder scans
and learned models of the environment. Arras and Vestli (1998) use a similar technique to
compute the position of the robot with a very high accuracy. All these variants, however,
rest on the assumption that the position of the robot can be represented by a single Gaussian distribution. The advantage of Kalman filter-based techniques lies in their eciency
and in the high accuracy that can be obtained. The restriction to a unimodal Gaussian
distribution, however, is prone to fail if the position of a robot has to be estimated from
scratch, i.e. without knowledge about the starting position of the robot. Furthermore,
these technique are typically unable to recover from localization failures. Recently, Jensfelt and Kristensen (1999) introduced an approach based on multiple hypothesis tracking,
which allows to model multi-modal probability distributions as they occur during global
localization.
Markov localization, which has been employed successfully in several variants (Nourbakhsh et al., 1995; Simmons & Koenig, 1995; Kaelbling et al., 1996; Burgard et al., 1996;
Hertzberg & Kirchner, 1996; Koenig & Simmons, 1998; Oore et al., 1997; Thrun, 1998a),
overcomes the disadvantage of Kalman filter based techniques. The different variants of
this technique can be roughly distinguished by the type of discretization used for the representation of the state space. Nourbakhsh et al. (1995), Simmons and Koenig (1995),
and Kaelbling et al. (1996) use Markov localization for landmark-based navigation, and the
state space is organized according to the topological structure of the environment. Here
nodes of the topological graph correspond to distinctive places in hallways such as openings
421

fiFox, Burgard & Thrun

or junctions and the connections between these places. Possible observations of the robot
are, for example, hallway intersections. The advantage of these approaches is that they can
represent ambiguous situations and thus are in principle able to globally localize a robot.
Furthermore, the coarse discretization of the environment results in relatively small state
spaces that can be maintained eciently. The topological representations have the disadvantage that they provide only coarse information about the robot's position and that they
rely on the definition of abstract features that can be extracted from the sensor information.
The approaches typically make strong assumptions about the nature of the environments.
Nourbakhsh et al. (1995), Simmons and Koenig (1995), and Kaelbling et al. (1996), for
example, only consider four possible headings for the robot position assuming that the
corridors in the environment are orthogonal to each other.
Our method uses instead a fine-grained, grid-based discretization of the state space.
The advantage of this approach compared to the Kalman filter based techniques comes
from the ability to represent more complex probability distributions. In a recent experimental comparison to the technique introduced by Lu and Milios (1994) and Gutmann and
Schlegel (1996), we found that Kalman filter based tracking techniques provide highly accurate position estimates but are less robust, since they lack the ability to globally localize the
robot and to recover from localization errors (Gutmann et al., 1998). In contrast to the topological implementations of Markov localization, our approach provides accurate position estimates and can be applied even in highly unstructured environments (Burgard et al., 1998a;
Thrun et al., 1999). Using the selective update scheme, our technique is able to eciently
keep track of the robot's position once it has been determined. It also allows the robot to
recover from localization failures.
Finally, the vast majority of existing approaches to localization differ from ours in that
they address localization in static environments. Therefore, these methods are prone to fail
in highly dynamic environments in which, for example, large crowds of people surround the
robot (Fox et al., 1998c). However, dynamic approaches have great practical importance,
and many envisioned application domains of service robots involve people and populated
environments.
6. Discussion

In this paper we presented a metric variant of Markov localization, as a robust technique
for estimating the position of a mobile robot in dynamic environments. The key idea of
Markov localization is to maintain a probability density over the whole state space of the
robot relative to its environment. This density is updated whenever new sensory input is
received and whenever the robot moves. Metric Markov localization represents the state
space using fine-grained, metric grids. Our approach employs ecient, selective update
algorithms to update the robot's belief in real-time. It uses filtering to cope with dynamic
environments, making our approach applicable to a wide range of target applications.
In contrast to previous approaches to Markov localization, our method uses a finegrained discretization of the state space. This allows us to compute accurate position
estimates and to incorporate raw sensory input into the belief. As a result, our system can
exploit arbitrary features of the environment. Additionally, our approach can be applied
in arbitrary unstructured environments and does not rely on an orthogonality assumption
422

fiMarkov Localization for Mobile Robots in Dynamic Environments

or similar assumptions of the existence of certain landmarks, as most other approaches to
Markov localization do.
The majority of the localization approaches developed so far assume that the world is
static and that the state of the robot is the only changing aspect of the world. To be able to
localize a mobile robot even in dynamic and densely populated environments, we developed
a technique for filtering sensor measurements which are corrupted due to the presence of
people or other objects not contained in the robot's model of the environment.
To eciently update the huge state spaces resulting from the grid-based discretization,
we developed two different techniques. First, we use look-up operations to eciently compute the quantities necessary to update the belief of the robot given new sensory input.
Second, we apply the selective update scheme which focuses the computation on the relevant parts of the state space. As a result, even large belief states can be updated in
real-time.
Our technique has been implemented and evaluated in several real-world experiments
at different sites. Recently we deployed the mobile robots Rhino in the Deutsches Museum Bonn, Germany, and Minerva in the Smithsonian's National Museum of American
History, Washington, DC, as interactive museum tour-guides. During these deployments,
our Markov localization technique reliably estimated the position of the robots over long
periods of time, despite the fact that both robots were permanently surrounded by visitors
which produced large amounts of false readings for the proximity sensors of the robots.
The accuracy of grid-based Markov localization turned out to be crucial to avoid even such
obstacles that could not be sensed by the robot's sensors. This has been accomplished by
integrating map information into the collision avoidance system (Fox et al., 1998b).
Despite these encouraging results, several aspects warrant future research. A key disadvantage of our current implementation of Markov localization lies in the fixed discretization
of the state space, which is always kept in main memory. To scale up to truly large environments, it seems inevitable that one needs variable-resolution representations of the
state space, such as as the one suggested in (Burgard et al., 1997; 1998b; Gutmann et al.,
1998). Alternatively, one could use Monte-Carlo based representations of the state space
as described in (Fox et al., 1999). Here, the robot's belief is represented by samples that
concentrate on the most likely parts of the state space.
Acknowledgment

The authors would like to thank the research group for autonomous intelligent systems at
the University of Bonn for fruitful discussions, useful suggestions and comments, especially
Daniel Hennig and Andreas Derr. We would also like to thank the members of CMU's
Robot Learning Lab for many inspiring discussions. Finally, we would like to thank the
staff of the Deutsches Museum Bonn and the National Museum of American History for
their enthusiasm and their willingness to expose their visitors to one of our mobile robots.
This research is sponsored in part by NSF (CAREER Award IIS-9876136) and DARPA
via TACOM (contract number DAAE07-98-C-L032), and Rome Labs (contract number
F30602-98-2-0137), which is gratefully acknowledged. The views and conclusions contained
in this document are those of the authors and should not be interpreted as necessarily
423

fiFox, Burgard & Thrun

representing ocial policies or endorsements, either expressed or implied, of NSF, DARPA,
TACOM, Rome Labs, or the United States Government.
References

[Arras & Vestli, 1998] K.O. Arras and S.J. Vestli. Hybrid, high-precision localization for
the mail distributing mobile robot system MOPS. In Proc. of the IEEE International
Conference on Robotics & Automation (ICRA), 1998.
[Basye et al., 1992] K. Basye, T. Dean, J. Kirman, and M. Lejter. A decision-theoretic
approach to planning, perception, and control. IEEE Expert, 7(4), 1992.
[Borenstein et al., 1996] J. Borenstein, B. Everett, and L. Feng. Navigating Mobile Robots:
Systems and Techniques. A. K. Peters, Ltd., Wellesley, MA, 1996.
[Burgard et al., 1996] W. Burgard, D. Fox, D. Hennig, and T. Schmidt. Estimating the
absolute position of a mobile robot using position probability grids. In Proc. of the
National Conference on Artificial Intelligence (AAAI), 1996.
[Burgard et al., 1997] W. Burgard, D. Fox, and D. Hennig. Fast grid-based position tracking for mobile robots. In Proc. of the German Conference on Artificial Intelligence (KI),
Germany. Springer Verlag, 1997.
[Burgard et al., 1998a] W. Burgard, A.B. Cremers, D. Fox, D. Hahnel, G. Lakemeyer,
D. Schulz, W. Steiner, and S. Thrun. The interactive museum tour-guide robot. In
Proc. of the National Conference on Artificial Intelligence (AAAI), 1998.
[Burgard et al., 1998b] W. Burgard, A. Derr, D. Fox, and A.B. Cremers. Integrating global
position estimation and position tracking for mobile robots: the Dynamic Markov Localization approach. In Proc. of the IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), 1998.
[Burgard et al., 2000] W. Burgard, A.B. Cremers, D. Fox, D. Hahnel, G. Lakemeyer,
D. Schulz, W. Steiner, and S. Thrun. Experiences with an interactive museum tourguide robot. Artificial Intelligence, 114(1-2), 2000. To appear.
[Cover & Thomas, 1991] T.M. Cover and J.A. Thomas. Elements of Information Theory.
Wiley Series in Telecommunications. Wiley, New York, 1991.
[Cox & Wilfong, 1990] I.J. Cox and G.T. Wilfong, editors. Autonomous Robot Vehicles.
Springer Verlag, 1990.
[Cox, 1991] I.J. Cox. Blanche|an experiment in guidance and navigation of an autonomous
robot vehicle. IEEE Transactions on Robotics and Automation, 7(2):193{204, 1991.
[Dellaert et al., 1999] F. Dellaert, W. Burgard, D. Fox, and S. Thrun. Using the condensation algorithm for robust, vision-based mobile robot localization. In Proc. of the IEEE
Computer Society Conference on Computer Vision and Pattern Recognition (CVPR),
1999.
424

fiMarkov Localization for Mobile Robots in Dynamic Environments

[Fox et al., 1998a] D. Fox, W. Burgard, and S. Thrun. Active Markov localization for
mobile robots. Robotics and Autonomous Systems, 25:195{207, 1998.
[Fox et al., 1998b] D. Fox, W. Burgard, S. Thrun, and A.B. Cremers. A hybrid collision
avoidance method for mobile robots. In Proc. of the IEEE International Conference on
Robotics & Automation (ICRA), 1998.
[Fox et al., 1998c] D. Fox, W. Burgard, S. Thrun, and A.B. Cremers. Position estimation
for mobile robots in dynamic environments. In Proc. of the National Conference on
Artificial Intelligence (AAAI), 1998.
[Fox et al., 1999] D. Fox, W. Burgard, F. Dellaert, and S. Thrun. Monte Carlo Localization:
Ecient position estimation for mobile robots. In Proc. of the National Conference on
Artificial Intelligence (AAAI), 1999.
[Fox, 1998] D. Fox. Markov Localization: A Probabilistic Framework for Mobile Robot Localization and Naviagation. PhD thesis, Dept. of Computer Science, University of Bonn,
Germany, December 1998.
[Gutmann & Schlegel, 1996] J.-S. Gutmann and C. Schlegel. AMOS: Comparison of scan
matching approaches for self-localization in indoor environments. In Proc. of the 1st
Euromicro Workshop on Advanced Mobile Robots. IEEE Computer Society Press, 1996.
[Gutmann et al., 1998] J.-S. Gutmann, W. Burgard, D. Fox, and K. Konolige. An experimental comparison of localization methods. In Proc. of the IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS), 1998.
[Hennig, 1997] D. Hennig. Globale und lokale Positionierung mobiler Roboter mittels
Wahrscheinlichkeitsgittern. Master's thesis, Department of Computer Science, University
of Bonn, Germany, 1997. In German.
[Hertzberg & Kirchner, 1996] J. Hertzberg and F. Kirchner. Landmark-based autonomous
navigation in sewerage pipes. In Proc. of the First Euromicro Workshop on Advanced
Mobile Robots. IEEE Computer Society Press, 1996.
[Jensfelt & Kristensen, 1999] P. Jensfelt and S. Kristensen. Active global localisation for a
mobile robot using multiple hypothesis tracking. In Proc. of the IJCAI-99 Workshop on
Reasoning with Uncertainty in Robot Navigation, 1999.
[Kaelbling et al., 1996] L.P. Kaelbling, A.R. Cassandra, and J.A. Kurien. Acting under
uncertainty: Discrete Bayesian models for mobile-robot navigation. In Proc. of the
IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 1996.
[Kalman, 1960] R.E. Kalman. A new approach to linear filtering and prediction problems.
Trans. of the ASME, Journal of basic engineering, 82:35{45, March 1960.
[Koenig & Simmons, 1998] S. Koenig and R. Simmons. A robot navigation architecture
based on partially observable Markov decision process models. In Kortenkamp et al.
(1998).
425

fiFox, Burgard & Thrun

[Konolige, 1999] K. Konolige. Markov localization using correlation. In Proc. of the International Joint Conference on Artificial Intelligence (IJCAI), 1999.
[Kortenkamp & Weymouth, 1994] D. Kortenkamp and T. Weymouth. Topological mapping
for mobile robots using a combination of sonar and vision sensing. In Proc. of the National
Conference on Artificial Intelligence (AAAI), 1994.
[Kortenkamp et al., 1998] D. Kortenkamp, R. P. Bonasso, and R. Murphy, editors.
MIT/AAAI Press, Cambridge, MA, 1998.
[Leonard & Durrant-Whyte, 1991] J.J. Leonard and H.F. Durrant-Whyte. Mobile robot
localization by tracking geometric beacons. IEEE Transactions on Robotics and Automation, 7(3):376{382, 1991.
[Leonard & Durrant-Whyte, 1992] J.J. Leonard and H.F. Durrant-Whyte. Directed Sonar
Sensing for Mobile Robot Navigation. Kluwer Academic, Boston, MA, 1992.
[Lu & Milios, 1994] F. Lu and E. Milios. Robot pose estimation in unknown environments
by matching 2d range scans. In IEEE Computer Vision and Pattern Recognition Conference (CVPR), 1994.
[Lu & Milios, 1997a] F. Lu and E. Milios. Globally consistent range scan alignment for
environment mapping. Autonomous Robots, 4:333{349, 1997.
[Lu & Milios, 1997b] F. Lu and E. Milios. Robot pose estimation in unknown environments
by matching 2d range scans. Journal of Intelligent and Robotic Systems, 18, 1997.
[Maybeck, 1990] P.S. Maybeck. The Kalman filter: An introduction to concepts. In Cox &
Wilfong (1990).
[Moravec & Elfes, 1985] H.P. Moravec and A.E. Elfes. High resolution maps from wide
angle sonar. In Proc. of the IEEE International Conference on Robotics & Automation
(ICRA), 1985.
[Moravec, 1988] H.P. Moravec. Sensor fusion in certainty grids for mobile robots. AI Magazine, Summer 1988.
[Nourbakhsh et al., 1995] I. Nourbakhsh, R. Powers, and S. Birchfield. DERVISH an ocenavigating robot. AI Magazine, 16(2), Summer 1995.
[Oore et al., 1997] S. Oore, G.E. Hinton, and G. Dudek. A mobile robot that learns its
place. Neural Computation, 1997.
[Russell & Norvig, 1995] Stuart J. Russell and Peter Norvig. Artificial Intelligence: A Modern Approach, chapter 17. Number 0-13-103805-2 in Series in Artificial Intelligence. Prentice Hall, 1995.
[Schiele & Crowley, 1994] B. Schiele and J.L. Crowley. A comparison of position estimation
techniques using occupancy grids. In Proc. of the IEEE International Conference on
Robotics & Automation (ICRA), 1994.
426

fiMarkov Localization for Mobile Robots in Dynamic Environments

[Schultz et al., 1999] A. Schultz, W. Adams, and B. Yamauchi. Integrating exploration,
localization, navigation and planning through a common representation. Autonomous
Robots, 6(3), 1999.
[Shaffer et al., 1992] G. Shaffer, J. Gonzalez, and A. Stentz. Comparison of two range-based
estimators for a mobile robot. In SPIE Conf. on Mobile Robots VII, pages 661{667, 1992.
[Shatkey & Kaelbling, 1997] H. Shatkey and L.P. Kaelbling. Learning topological maps
with weak local odometric information. In Proc. of the International Joint Conference
on Artificial Intelligence (IJCAI), 1997.
[Simmons & Koenig, 1995] R. Simmons and S. Koenig. Probabilistic robot navigation in
partially observable environments. In Proc. of the International Joint Conference on
Artificial Intelligence (IJCAI), 1995.
[Simmons, 1995] R. Simmons. The 1994 AAAI robot competition and exhibition. AI Magazine, 16(2), Summer 1995.
[Smith et al., 1990] R. Smith, M. Self, and P. Cheeseman. Estimating uncertain spatial
relationships in robotics. In I. Cox and G. Wilfong, editors, Autonomous Robot Vehicles.
Springer Verlag, 1990.
[Thrun et al., 1998a] S. Thrun, A. Bucken, W. Burgard, D. Fox, T. Frohlinghaus, D. Hennig, T. Hofmann, M. Krell, and T. Schimdt. Map learning and high-speed navigation in
RHINO. In Kortenkamp et al. (1998).
[Thrun et al., 1998b] S. Thrun, D. Fox, and W. Burgard. A probabilistic approach to
concurrent mapping and localization for mobile robots. Machine Learning, 31:29{53,
1998. Also appeared in Autonomous Robots 5, pp. 253{271, joint issue.
[Thrun et al., 1999] S. Thrun, M. Bennewitz, W. Burgard, A.B. Cremers, F. Dellaert,
D. Fox, D. Hahnel, C. Rosenberg, N. Roy, J. Schulte, and D. Schulz. MINERVA: A
second generation mobile tour-guide robot. In Proc. of the IEEE International Conference on Robotics & Automation (ICRA), 1999.
[Thrun, 1998a] S. Thrun. Bayesian landmark learning for mobile robot localization. Machine Learning, 33(1), 1998.
[Thrun, 1998b] S. Thrun. Learning metric-topological maps for indoor mobile robot navigation. Artificial Intelligence, 99(1):27{71, 1998.
[Wei et al., 1994] G. Wei, C. Wetzler, and E. von Puttkamer. Keeping track of position
and orientation of moving indoor systems by correlation of range-finder scans. In Proc. of
the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 1994.
[Yamauchi, 1996] B. Yamauchi. Mobile robot localization in dynamic environments using
dead reckoning and evidence grids. In Proc. of the IEEE International Conference on
Robotics & Automation (ICRA), 1996.
427

fiJournal of Artificial Intelligence Research 11 (1999) 361-390

Submitted 6/99; published 11/99

The Complexity of Reasoning about Spatial Congruence
Matteo Cristani

Dipartimento Scientifico e Tecnologico,
Universita di Verona,
Ca Vignal 2, strada Le Grazie, I-37134 Verona

cristani@sci.univr.it

Abstract

In the recent literature of Artificial Intelligence, an intensive research effort has been
spent, for various algebras of qualitative relations used in the representation of temporal and
spatial knowledge, on the problem of classifying the computational complexity of reasoning
problems for subsets of algebras. The main purpose of these researches is to describe
a restricted set of maximal tractable subalgebras, ideally in an exhaustive fashion with
respect to the hosting algebras.
In this paper we introduce a novel algebra for reasoning about Spatial Congruence, show
that the satisfiability problem in the spatial algebra MC-4 is NP-complete, and present a
complete classification of tractability in the algebra, based on the individuation of three
maximal tractable subclasses, one containing the basic relations. The three algebras are
formed by 14, 10 and 9 relations out of 16 which form the full algebra.

1. Introduction
Qualitative spatial reasoning has received an increasing amount of interest in the recent
literature. The main reason for this, as already observed by Jonsson and Drakengren (1997),
is probably that spatial reasoning has proved to be applicable to real-world problems, as in
Geographical Information Systems (Egenhofer, 1991; Grigni, Papadias, & Papadimitriou,
1995), and Molecular Biology (Cui, 1994).
The specific stress on qualitative reasoning about space, as observed by Zimmermann
(1995), is justified by the fact that qualitative spatial relations can be treated as eciently
as their quantitative counterparts, but they seem to be closer to the model of relations
humans adopt for spatial reasoning.
Even though qualitative spatial reasoning has an extended literature, in spite of its
relatively short history, certain aspects of the discipline have been neglected. In particular,
no exhaustive computational perspective has been developed on qualitative morphological
reasoning about space. The term morphological reasoning is intended to suggest reasoning
about the internal structure of the objects. In the case of spatial reasoning this includes
reasoning about the size, shape and internal topology of spatial regions.
The purpose of the present work is to analyse the complexity of reasoning about relations
of congruence, either actual or partial, between spatial regions, using the spatial algebra
MC-4 which has been preliminarly analysed by Cristani (1997).
The algebra MC-4 is a Constraint Algebra (Ladkin & Maddux, 1994) for qualitative
reasoning about the morphological relation of congruence. Two spatial regions, in the
models documented in literature, are considered to be equivalent if and only if they share
interior and boundaries, namely if and only if they are the same spatial region. In particular,
c 1999 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiCristani

the relation EQ, as defined by Randell, Cui and Cohn (1989), and analogously by Egenhofer
and Franzosa (1991), becomes identity under the unique name assumption. Geometry and
topology, conversely, allow other kinds of equivalence relations. These relations are weaker
than the identity, namely the equivalence classes they induce are larger than singletons.
The simplest weakening we can define is the congruence relation. Even though, the
relation has been studied (Borgo, Guarino, & Masolo, 1996, 1997; Cristani, 1997; Mutinelli,
1998), the complexity of reasoning in subalgebras has not yet been deeply investigated.
The reason to introduce this new relation, and to provide an algebraic structure to host
relations based on it (in particular relations in which we compare regions, which, even if
they are not congruent, can be overlapped by roto-translating one into the interior of the
other), is that, in many cases, the knowledge we have to represent in our systems necessarily
includes internal structures. Consider the following example.

Example 1
A GIS is dedicated to the representation of geographical data about industrial sites.
In the system, the structure of factories can be described by means of various attributes,
including size and shape. One of the users wants to query the system about the opportunity
of moving the factory, where he works, from the present site to a new one. In the new site
prefabricated facilities already exist, and the problem consists of deciding how to preserve
topological layout of the factory and minimise the costs of buying new engines, to substitute
those which cannot fit with the new facilities.
In order to reason about these problems the system can be made able to represent the
qualitative relations between the old and new site parts, establishing, in particular whether
the spatial regions occupied by the engines are \congruent", or even \congruent to a part"
of the ones which can be chosen for hosting them in the new factory's site.
Clearly we cannot use topological information, since a spatial region presently occupied
by an engine is surely disjoint from the region that this engine is going to occupy in the new
site, but this information is insucient for deciding whether the new site is able to host the
engine itself.
Though metric information can be involved in final decision about moving the factory,
qualitative reasoning can be used in the initial design phase.
In Figure 1 we give a pictorial representation of one possible situation in which, to make
decisions, we need to represent spatial information which is not topological.


The algebra MC-4 can be used to represent the four basic relations which can be built on
the equivalence relation of congruence: if we consider the roto-translation of a region x with
respect to a region y, only four possible situations arise.
 There is at least one roto-translation in which x coincides with y. The two regions
are congruent.
 There is at least one roto-translation in which x coincides with a proper part of y.
The region x is congruent to a proper part of y.
362

fiThe Complexity of Reasoning about Spatial Congruence

Figure 1: A pictorial representation of the relations to be analysed in a GIS for moving
a factory from an old to a new site. Note that the production lines of the new
facility may not be suitable for hosting certain engines.

 There is at least one roto-translation in which y coincides with a proper part of x.
The region x has a proper part congruent to y.
 None of the above. The region x and y cannot be perfectly overlapped.
We would like to stress here three main aspects of these relations.
1. The relations can be established between two spatial regions in any position of space.
There is no specific need for the regions to be close, or to be in any particular topological relation, provided that the relation is compatible; so these congruence relations
are purely morphological.
2. The equality of two regions implies congruence. The relation of proper part excludes
morphological relations other than \congruent to a part". This is not the case for the
morphological side. A region can be congruent to a part of another region, even if
they are disjoint, and the same can happen to congruence, as already stated in point
1.
3. Even if two regions are of the same size, there may be the case that the regions are
not congruent. This holds also for partial congruence, being possible that a region is
smaller than another one but they cannot be overlapped.
The practical relevance of the algebra can be proved by exhibiting many other examples,
especially coming from GIS. In this paper we deal with the problem of reasoning about
congruence in two-dimensional space domain. The kind of congruence we assume is the
weakest one: roto-translability. Three-dimensional congruence is not of the same type. The
\natural" notion of three-dimensional congruence is isometry, which is much more general
than roto-translability. For example left and right hands are congruent (under the simplified
assumption that they have the same shape and size), but they cannot be roto-translated
each into the other one. We concentrate here in two-dimensional reasoning which may be
363

fiCristani

analysed in terms of roto-translation and topological relations. The analysis of isometries
is left for further work.
The paper is organised as follows. Section 2 describes related work which has been
developed in the area. Section 3 presents the spatial algebra MC-4, and Section 4 discusses
the classification of tractability we found for this algebra. Finally, in Section 5, some
conclusions are given.

2. Previous Work
From the perspective of individuating primitives to describe space, a significant effort has
been lately spent, in the direction of defining binary relations between spatial regions, which
may be used as a model of space in qualitative terms. Moreover, it was natural, in the AI
community, to use Constraint Processing for reasoning about such binary relations. A
specific attention to Constraint Processing emerged in the Spatial Database community,
and in the community of Geographical Information Science as well.
Two apparently independently developed models, which can be shown to be essentially
equivalent, exist in the Artificial Intelligence (Randell & Cohn, 1989; Randell, Cui, &
Cohn, 1992) and Spatial Database literatures (Egenhofer & Herring, 1990; Egenhofer &
Franzosa, 1991; Franzosa & Egenhofer, 1992). The Artificial Intelligence model, (Randell &
Cohn, 1989; Randell et al., 1992) is known as RCC, where the acronym stands for Region
Connection Calculus.
The RCC model is centered on the primitive of \connection" as originally suggested
by Clarke (1981). Gotts (1994) and further Gotts, Gooday and Cohn (1996) obtained
connection to be the only primitive. In the original framework (Randell & Cohn, 1989;
Randell et al., 1992) the model was formulated in two versions, now called RCC-5 and
RCC-8. The RCC-5 model is a model in which we cannot distinguish between interior
and boundary of a spatial region (so that the external connection may be tangency or
overlapping, for example), while in the RCC-8 model this distinction is possible.
Bennett (1994, 1995), used propositional logic to represent RCC reasoning problems. He
observed that, given a propositional logic, and interpreting the truth values of each formula
as a spatial region, the language of RCC-5 is sucient to express a satisfaction problem at
the semantical level. This fact, however, is not due to the spatial interpretation, because
a non-spatial interpretation in which the RCC-5 relations are still sucient to represent
truth condition at semantical level can be found: set theory. Therefore, he applied the
result of Cook (1971) on NP-completeness of classical propositional logic to the restricted
model RCC-5, proving that deciding the consistency of a Constraint Satisfaction Problem
(CSP) on RCC-5 is an NP-complete problem too.
The complete model of RCC-8, instead, can not be analysed in terms of pure set theory,
because the distinction between boundary-connection and interior-connection is possible.
This distinction means that the minimal interpretation in which RCC-8 still provides a
correct and complete representation of truth conditions at semantical level requires topology.
Statman (1979) proved that intuitionistic logic along with interpretations in set theory forces
a topology in the models of a sound theory. Bennett proved, thus, that RCC-8 can be used
to define the truth conditions of formulas in an intuitionistic propositional logic. Statman
(1979) proved that intuitionistic propositional logic is PSPACE-complete. However, since
364

fiThe Complexity of Reasoning about Spatial Congruence

we do not need a complete truth verification procedure, but only a procedure for constraint
processing, Bennett could reduce the result, proving that also RCC-8 is NP-complete.
These results, even if they are in general negative for practical purposes, encouraged
researches in the direction of restricted models, in such a way that at least for certain
cases we may process a finite set of RCC constraints in polynomial fashion on deterministic
machines. In particular, Nebel (1995) showed that reasoning with the basic relations of
RCC-5 and RCC-8 are tractable problems. Renz and Nebel (1999) improved the results
above, by showing that there exists a maximal tractable subclass of RCC-5, denoted by
Hb5, formed by 28 relations out of 32, which includes all the basic relations, and a maximal
tractable subclass of RCC-8, denoted by Hb8 , formed by 148 relations out of 256 including
the basic relations. A maximal tractable subclass A, is a subset of a constraint algebra, such
that problems defined on A are tractable, while problems defined on proper supersets of A
are not. These results have been obtained in a similar fashion to the work of Nebel and
Burckert (1995) on temporal reasoning. The result of Renz and Nebel is anyway incomplete,
since he simply proved that there exists one maximal tractable subclass, and he did not
classify every maximal tractable subclasses of RCC-5. Jonsson and Drakengren (1997)
showed that there exist four maximal tractable subclasses of RCC-5, one including the
basic relations. The result is obtained in a similar fashion to (Drakengren & Jonsson, 1997;
Jonsson, Drakengren, & Backstrom, 1999). A complete analysis of the RCC-8 maximal
tractable subclasses including the basic relations has been provided by Renz in (1999).
Our result is the analogous in MC-4 of Jonsson and Drakengren result for RCC-5. The
MC-4 algebra, we describe in this paper, is structured in the same way as the Algebra of
Partially Ordered Time (PO-time algebra), studied by Anger, Mitra and Rodriguez. We
would like to stress two main aspects here:
 The MC-4 algebra has the PO-time algebra are the same algebraic structure: the
discussion of this paper stands a different interpretation of the PO-time algebra, even
different from the spatial interpretation provided by Anger, Mitra and Rodriguez in
their papers.
 The computational results we present here can be applied to the PO-time algebra as
well, and they extend the previous findings obtained by Anger, Mitra and Rodriguez.
In particular Anger, Mitra and Rodriguez (1998, 1999), proved that path-consistency
is insucient to ensure consistency for relations in this algebra, and that there exists a
tractable subalgebra of it which can be treated by an O(n3 ) algorithm.
Anger, Mitra and Rodriguez (1999), showed that deciding the consistency of a PO-time
network is an NP-complete problem by reducing to it the analogous decision problem on
RCC-5.
Some important observations are needed, with respect to the results of Anger, Mitra
and Rodriguez:
 The results on the complexity of PO-time algebra can be applied to MC-4, only if
we can show that when a MC-4 network is consistent then we can exhibit a consistent scenario in which vertices of the network are substituted with spatial regions.
Therefore, even if we can show this correspondence for PO-time algebra too, our
NP-completeness result is independent. Moreover, the method we used to prove the
365

fiCristani

intractability of MC-4 is independent as well, and the main difference is in the way
we used to exhibit problematic algebraic structure. Through this proof we derived a
simple way to solve the problems whenever possible in polynomial time.

 The discovery of the tractable subalgebras we indicate by M99 , M81 and M72 in

the present paper, deserves acknowledgement of priority to Anger, Mitra and Rodriguez. However, Anger, Mitra and Rodriguez also classified one more algebra, which
is tractable, but not maximal, since it is contained in M99 . Moreover, the algorithms
we found for M99 and M81 are substantially different from the one Anger, Mitra and
Rodriguez present and more ecient, being O(n2 ) instead of O(n3 ). Therefore the
two algorithmic solutions can be considered as independent results as well.

 The classification presented here is complete, since we classify all the maximal tractable
subalgebras of MC-4, and this is a result which may be applied to temporal reasoning
as well, since it is obtained by means of algorithms which are completely independent
from the interpretation we give to the relations (either spatial or temporal).

 The introduction of morphological relations in spatial reasoning is novel too, and the

study of spatial congruence deserves, in our opinion, deep investigations henceforth.
The fact that two algebras of spatial and temporal reasoning present substantial similarities is not a novelty. The RCC model corresponds to subalgebras of Interval
Calculus, as stated by Bennett (1994). Also Anger, Mitra and Rodriguez stated this
property of PO-time algebra with respect to RCC-5, but their interpretation is completely different from our own, the equality of PO-time being interpreted as EQ in
RCC-5, while we interpret it as congruence.

Thus, even though a similar algebraic structure has been partially investigated before, the
present paper presents substantial methodological differences and we have obtained results
which are independent of or extensions to the ones obtained by Anger, Mitra and Rodriguez.
In the remainder of the paper, when a result can be attributed to Anger, Mitra and
Rodriguez we note it in the text.

3. The Spatial Algebra MC-4
In this section we present the spatial algebra MC-4, which has been previously presented
by Cristani (1997) and largely analysed in by Mutinelli (1998).
MC-4 is a Binary Constraint Algebra (henceforth indicated as a Constraint Algebra). In
a Constraint Algebra we have a Constraint Domain and an Algebra Base, which is formed
by mutually exclusive relations among elements of the Constraint Domain, whose union
form the universal relation. The converse of a basic relation is a basic relation too, and
the composition of basic relations is the union of some basic relations. A Constraint is the
establishment of one of all possible unions of basic relations between two variables which
vary on the domain. A constraint is satisfied by an assignment of one pair of values of the
domain to the variables, so that the pair of values is in one of the relations of the constraint
itself. Given a finite set of constraints between two variables, the problem of deciding
whether there exists an assignment to the variables such that all the constraints can be
366

fiThe Complexity of Reasoning about Spatial Congruence

y

x

x CG y
y

x

x CGPP y
y

x

x CNO y

Figure 2: A pictorial representation of the basic relations of MC-4
CG
CGPP

CG
CG
CGPP

CGPP
CGPP
CGPP

CGPP,1
CGPP,1

CGPP,1

CGPP,1

>

CGPP,1

CNO

CNO

CGPP
CNO

CGPP,1
CNO




>

CNO
CNO
CGPP
CNO
CGPP,1
CNO

>

Table 1: The composition table of MC-4. The symbol > represents the universal relation
fCG; CGPP; CGPP,1 ; CNOg.
simultaneously satisfied is referred to as Constraint Satisfaction Problem, and henceforth
indicated as a CSP.
The MC-4 algebra is formed by all the unions of the four basic relations, which can be
established between two-dimensional spatial regions, from a morphological point of view,
with respect to the equivalence relation of congruence.
The congruence relation is variously interpreted in geometry. Our interpretation is the
most restrictive one: two regions are congruent iff they can be rigidly roto-translated into
each other.
A region x, in this interpretation, may be congruent (x CG y) to a region y, or congruent
to a part of y (x CGPP y) or they cannot be perfectly overlapped (x CNO y). The relation
congruent to a part of may also be inverted to having a part congruent to (y CGPP,1 x iff
x CGPP y).
In Figure 2 a pictorial representation of the three basic relations CG, CGPP and CNO is
given. In Table 1 we present the composition table of MC-4 showing how the basic relations
compose with each other.
367

fiCristani

x DR y
x

x PO y
y

x

y

-1

x PP y

x PP y

x EQ y

x y

x y

x y

Figure 3: A pictorial representation of the relations of RCC-5.
A CSP can be represented by a Network of Constraints. A network of constraints is a
labelled graph G = hV ; ; Ei, where V is a finite set of vertices, E is a binary relation on
V whose elements are called edges, and  is a labelling function which maps each vertex
of G to a variable, and each edge to a relation of a Constraint Algebra on a given domain
D. Given a network of constraints G , the problem of deciding the consistency of G is the
problem of establishing whether is it possible to instantiate each vertex label (the variables)
with elements of D, in such a way that all the relations represented in G as labels of the
edges are simultaneously satisfied. The CSP represented by a network G is often referred to,
for the algebra A, as A-SAT(G ). For the sake of simplicity we refer to the CSP represented
by a network G on MC-4 as MSAT(G ).
The main result on the MC-4 algebra, with respect to MSAT, is unfortunately a negative
computational account. In general, deciding the consistency is hard to solve for networks
of constraints between variables representing spatial regions, as stated in Theorem 2. This
result has been already proved by Cristani (1997).
Anger, Mitra and Rodriguez (1998), showed that path-consistency cannot be applied
successfully to the Algebra of Partially Ordered Time, which is isomorphic to MC-4 at the
syntactic level. This is insucient to prove that the CSP on this algebra is NP-complete.
They showed (Anger et al., 1999) that PO-time algebra is NP-complete. Their proof lies on
the  translation, which can be shown to be analogous, but not identical, to the 
 translation
we introduced here, at the syntactical level, being completely different at the semantical
one. However, their proof is insucient to show that we can arrange spatial solutions,
since the map they defined is purely syntactical. As already observed by Lemon (1996),
the representation of space by means of relation algebra is not pure, and we can obtain
satisfiable networks which are not realizable in space. Our proof, instead, can be applied
to spatial interpretations. It can be applied to nonlinear time temporal interpretations as
well, since the syntactic level is shown to be sucient for space, and nonlinear time can be
interpreted as space too (Anger et al., 1999).
Before getting into the proof of this negative result we need to describe a relevant
correspondences of the MC-4 algebra to the RCC-5 model, which is also used in the proof
of Theorem 2. The well known RCC-5 algebra is a Constraint Algebra with 5 basic relations:
EQ , DR , PO , PP , PP,1 . The five relations correspond to the pictorial representation of
Figure 3.
If two spatial regions are in one of the relations of MC-4, then only certain relations
of RCC-5 can be established between them. Conversely, if certain relations of RCC-5 are
established only certain corresponding relations of MC-4 are. This correspondence, however,
is not one-to-one. Consider, for instance, a region x and a region y, such that x DR y, in
368

fiThe Complexity of Reasoning about Spatial Congruence

Rel. of MC-4 Rel. of RCC-5
CG
CGPP
CGPP,1
CNO

fEQ; DR; POg
fPP; DR; POg
fPP,1; DR; POg
fDR; POg

Table 2: The basic relations of MC-4 and their counterparts in RCC-5. The meaning of the
Table is that when a relation of MC-4 is established, then one of the relations of
RCC-5 in second column is established as well.

Rel. of RCC-5 Rel. of MC-4
EQ
PP
PP,1
PO
DR

CG
CGPP
CGPP,1
fCG; CGPP; CGPP,1; CNOg
fCG; CGPP; CGPP,1; CNOg

Table 3: The basic relations of RCC-5 and their counterparts in MC-4.
RCC-5, namely x is disjoint from y. Then, each relation of MC-4 can be established between
x and y. However, if x PP y, namely x is a proper part of y, then only the CGPP relation
can be established between x and y. On the other hand, if two regions are congruent, only
the relations DR , PO and EQ can exist between x and y.
In Table 3 we set the correspondences between MC-4 basic relations and RCC-5, while
in Table 2 we set the correspondences between RCC-5 and MC-4.
The correspondences of the above Tables are not suciently strict, to use them in proving that MSAT(MC-4) is an NP-complete problem by a direct polynomial reduction. If
we consider a CSP on MC-4, the corresponding RCC-5 CSP is not intractable, since the
relations obtained from Table 2 do not define an intractable subset of RCC-5, by means
of the complete classification established by Jonsson and Drakengren (1997). Therefore,
MSAT(MC-4) is not reducible to RSAT(RCC-5) (the RSAT symbol represents the satisfiability in RCC models) by the corresponding relation mapping of Table 2.
Conversely, we can establish, by means of the above Tables, that among all the possible
regions satisfying the relation CG, there exists at least one pair in which the regions are
EQ, that when CGPP is established, there exists one pair in a PP relation, and finally that
when CNO is established, we have two regions in a PO relation. The summary of this
correspondence is reported in Table 4.
This correspondence is obtained from the definition of basic relations. Two regions a
and b are congruent iff we can roto-translate a by a T in such a way that T (a) EQ b or
conversely by T 0 so that T 0 (b) EQ a. Analogously a is congruent to a part of b iff we can
369

fiCristani

Rel. of MC-4 Rel. of RCC-5
CG
CGPP
CGPP,1
CNO

EQ
PP
PP,1
PO

Table 4: The basic relations of MC-4 and their counterparts in RCC-5 in the special mapping 
.
roto-translate a by T so that T (a) PP b. Finally two regions are in a CNO relation iff they
can only be in a PO relation or disjoint.
The correspondence of Table 4 is called 
, and a Network of Constraints G on MC-4
translated in RCC-5 by it is denoted by 
(G ). The networks which are labelled on all edges
by basic relations of MC-4 are henceforth called scenarios of MC-4.
Consider a consistent scenario S of MC-4. Applying the composition tables of MC-4 and
RCC-5, we clearly derive that the scenario of RCC-5 
(S ) is consistent, when the scenario
of MC-4 does so. The consequence of above reasoning is the next lemma.

Lemma 1 If a scenario on MC-4 is consistent, then 
(G ) is consistent.
Ladkin and Maddux (1994) proved that a network of constraints is consistent iff it has a
consistent scenario. Therefore, an immediate consequence of Lemma 1 is that if a network
of constraints G on MC-4 is consistent, then 
(G ) is consistent. We are now able to prove
a first theorem.

Theorem 1 MSAT(MC-4) is NP-hard.
Proof

By the observation about existence of consistent scenarios in a Constraint Algebra due
to Mackworth and Freuder (1985), we obtain a polynomial reduction of RSAT(RCC-5) to
MSAT(MC-4).
It suces to note that if we can solve MSAT then we can solve RCC-5 problems obtained
by the 
 translation as well. Now, the problems mapped from MC-4 into RCC-5 by means
of 
 can be trivially inverted by 
,1 , since 
 is trivially one-to-one. This means that each
problem in the set of relations obtained in RCC-5 by 
 corresponds to a problem in MC-4,
and vice versa.
The set of relations of RCC-5 translated by 
 contains fPP; PP,1 g and PO . Nebel and
Renz (1999), proved that each set of relations of RCC-5 containing these two relations is
intractable. Therefore the set 
(MC , 4) is intractable.
This proves that if we are able to solve a problem in MC-4 we can solve a problem of a
subset of RCC-5 which is intractable. Therefore MSAT(MC-4) is NP-hard.


370

fiThe Complexity of Reasoning about Spatial Congruence

Mackworth and Freuder (1985) also proved that backtracking can be applied to CSPs. The
backtracking algorithm is usually implemented by a linear non-deterministic technique,
being therefore a polynomial algorithm on non-deterministic machines.
The backtracking technique is applicable to MC-4 as well, so we can perform a polynomial solution of MSAT on nondeterministic machines. This shows that MSAT is in NP,
and allows us to claim:

Theorem 2 MSAT(MC-4) is NP-complete.
Because of this negative result a deep investigation is needed to define tractable subclasses of the set of 16 relations which allow us to perform polynomial analysis for at least
a subset of the networks of constraints definable on MC-4.
In this paper we give the definition of the three maximal tractable subclasses of MC4, exhibiting therefore a complete classification of tractability for the algebra. The three
maximal tractable subclasses have already been studied by Anger, Mitra and Rodriguez in
(1999). They obtain maximality of the algebras, and exhibited O(n3 ) algorithms. We have
three main differences here with respect to their paper:
1. The number of classes we individuated is lower than theirs, because they found four
maximal tractable subclasses. They failed to note that one of the four subalgebras is
a subset of another one. In Table 10 the subalgebra M88 corresponds to the fourth
algebra of Anger, Mitra and Rodriguez. This subalgebra is tractable, but not maximal.
2. The algorithms we exhibit are all O(n2 ) while Anger, Mitra and Rodriguez exhibited
only an O(n3 ) algorithm for one of the three maximal subsets.
3. Our classification is complete. Thus any subset of MC-4 which is not a subset of one of
the three maximal tractable subalgebras we individuated is intractable. Anger, Mitra
and Rodriguez did not find this result, since they did not analyse all the subalgebras
of PO-time algebra, as we did for MC-4.

4. Maximal Tractable Subclasses of MC-4

Given a subset S of a constraint algebra A, we indicate by Sb the set formed by all the
relations of A which can be written as expressions of the algebra involving only elements
of S and the operations of composition, intersection and converse of relations. This set is
often called the transitive closure of S with respect to the operations above. We refer to it
as the closure of S . A set S which coincides with its closure is called a subalgebra.
In the previous section we recall the result on NP-completeness for MC-4. The first
important observation on the complexity of subalgebras is that, when a subalgebra B does
not contain the empty relation, then a network of constraints on B cannot entail a strict
contradiction: then it is consistent. So far, the problem is necessarily polynomially solvable,
because it is O(1). This is stated in the next lemma.

Lemma 2 Given an algebra A, if a subalgebra B of A does not contain the empty relation,
then SAT(B) is polynomial.

371

fiCristani

The main consequence of Lemma 2 is that we can limit ourselves in the analysis of
subalgebras in MC-4 to these subalgebras which contain the empty relation.
Moreover, since a network of constraints represents the relations in an implicit way,
when an edge of a network is not labelled we should interpret it as representing the universal
relation. Therefore, the universal relation should be a member of the subalgebras to which
we are interested in. We call algebras which contain both the empty and the universal
relations expressive algebras.
There are 102 expressive subalgebras, we denominate Mi where i varies between 0 to
101. In Tables 5, 6, 7, 8, 10, 9 of Appendix A, the 102 expressive subalgebras of MC-4 are
listed.

Lemma 3 Given a subset A of MC-4, A is an expressive subalgebra iff A is one of the
subalgebras Mi with i between 0 and 101 .

Proof

Consider a subset A of MC-4. We test the closure of A by a LISP program which computes
the closure by composition, intersection and converse of a subset of MC-4, and test the
presence of empty and universal relation in A by the obvious membership test. The LISP
procedure TRANSITIVE-CLOSURE is listed in Online Appendix 1, which accompanies this
article. The test succeeds for the subalgebras Mi with i between 0 and 101 and fails for all
the other 216 , 102 = 65434 subsets of MC-4. The claim is therefore proved.


The following technical lemma shows that some of the 102 subalgebras individuated
above are NP-hard. There are 20 subalgebras of the 102 which are intractable by Lemma
4. The algebras are presented in Table 5 of Appendix A. The proof of Lemma 5 is a trivial
consequence of the proof of Theorem 2.

Lemma 4 Given a subalgebra A of MC-4, if A contains the relations CNO and fCGPP;
CGPP,1 g then MSAT(A) is NP-hard.
In the next three subsections we show that three maximal tractable subclasses of MC-4 can
be found, so that the only intractable algebras are the 20 listed in Table 5 of Appendix A.

4.1 The CG -complete Subalgebra M72

5 is tractable, Jonsson and Drakengren (1997) observed
When proving that the class R28
that any subalgebra of RCC-5 containing only relations including EQ is tractable. This
result applies also to MC-4 with respect to the relation CG, and also to the relations CNO,
fCGPP; CGPP,1g, fCG; CNOg, fCG; CGPP; CGPP,1g.
The only relevant cases are CG , CNO and fCGPP; CGPP,1 g , since the other two
cases are included in two of the former three. An algebra formed by relations which all
contain CG and by the relation ? is tractable, because we can obtain an inconsistency
iff we explicitly have an edge labelled by ? in the network. Deciding the consistency is
therefore an O(n2 ) problem.

372

fiThe Complexity of Reasoning about Spatial Congruence

ALGORITHM M72-CONSISTENCY
A constraint network T on M72

INPUT:

OUTPUT:

1.

2.

Yes if T has a solution formed by spatial regions of R2 , no if not.

For

each edge in T , hx; yi
If the label on hx; y i is ?

then return inconsistency

Loop

Return consistency
Figure 4: Algorithm M72-consistency.

M72 = f?, CG, fCG; CGPPg, fCG; CGPP,1g, fCG; CNOg, fCG; CGPP; CGPP,1g,
fCG; CGPP; CNOg, fCG; CGPP,1; CNOg, >g.
Since no contradiction derives from a relation in M72 except for ?, Algorithm M72 -consistency
(see Figure 4) solves the consistency checking problem for a network of constraints on M72 .
Thus we can claim the following theorem:

Theorem 3 Algorithm M72 -consistency correctly decides the consistency of a network of
constraints T on M72 in O(n2 ) time where n is the number of vertices in T .
The immediate consequence of Theorem 3 is the following theorem:

Theorem 4 MSAT(M72 ) is polynomial.
The subalgebras included in M72 are in Table 6 of Appendix A.
The 13 subalgebras of Table 6 are not the only subalgebras which can be theoretically
obtained based on the method incorporated in Algorithm M72 -consistency. The same algorithm can be applied to subalgebras formed only with relations containing a symmetrical
relation in MC-4 and the empty relation. Then we can also prove the polynomiality for
subalgebras of relations all containing CNO, or all containing fCGPP; CGPP,1 g, if such subalgebras exist. The subalgebra formed by relations containing CNO and the empty relation
is M78 , the subalgebra formed only by relations containing fCGPP; CGPP,1 g is M31 . In
Table 7 and in Table 8 of Appendix A the subalgebras of M78 and of M31 are shown.
In the next subsection we introduce a maximal tractable subclass which includes M78 ,
and in subsection 4.3 we introduce an algebra containing M31 , so that the proof of tractability for subalgebras in Tables 7 and 8 can be derived from these tables as well. Conversely,
the subalgebra M72 is neither a subalgebra of M99 nor a subalgebra of M81 , so Theorem 3
is an independent result.
373

fiCristani

4.2 The Maximal Tractable Subclass M99

We look for a maximal tractable subalgebra containing all the basic relations. The best
candidate, based on Table 5, is M99 , which is the only algebra formed by more than 13
relations which can be polynomial, because we did not show that it is NP-hard by reducing
it to an intractable problem over RCC-5.
M99 = f?, CG, CGPP, CGPP,1, CNO, fCG; CGPPg, fCG; CGPP,1g, fCG; CNOg,
fCGPP; CNOg, fCGPP,1 ; CNOg, fCG; CGPP; CNOg, fCG; CGPP,1; CNOg, >g.
Fortunately, we can prove the tractability of M99 , so it is maximal based on the fact
that all algebras containing the basic relations are either intractable by Table 5 or subsets
of M99 .
Consider the subset of M99 ,
G99 = ffCG; CGPPg, fCG; CNOg, fCGPP; CGPP,1; CNOgg.
The following claim holds.

Lemma 5 Gd99 = M99 .
Proof

The following expressions represent valid implementations of relations in M99 using only
elements of G99 and the operators of composition, intersection and converse.
t.1.

?

= fCG; CGPPg

t.2.

CG

 fCG; CNOg
fCGPP; CGPP,1; CNOg
= fCG; CGPPg  fCG; CGPPg ^



t.3.

CGPP

= fCG; CGPPg  fCGPP; CGPP,1 ; CNOg

t.4.

CGPP,1

= fCG; CGPPg ^  fCGPP; CGPP,1 ; CNOg

t.5.

CNO

= fCG; CNOg  fCGPP; CGPP,1 ; CNOg

t.6.

>

= fCG; CGPPg 
 fCGPP; CGPP,1 ; CNOg

t.7.

fCG; CGPP,1g

= fCG; CGPPg ^

t.8.

fCGPP; CNOg

= (fCG; CGPPg

t.9.

fCGPP,1; CNOg

t.10. fCG; CGPP; CNOg


 fCG; CNOg )
 fCGPP; CGPP,1; CNOg
= (fCG; CGPPg ^ 
 fCG; CNOg )
 fCGPP; CGPP,1; CNOg
= fCG; CGPPg 
 fCG; CNOg

t.11. fCG; CGPP,1 ; CNOg

= fCG; CGPPg ^ 
 fCG; CNOg
374

fiThe Complexity of Reasoning about Spatial Congruence


A network of constraints T on M99 , implemented by means of Lemma 5 is denoted henceforth by 	99 (T ).
We can derive a contradiction from a network of constraints iff the network derives two
relations R1 and R2 , between one pair of vertices such that R1 \ R2 = ?. The ways in
which a contradiction can be derived in networks labelled by relations of M99 , based on the
intersections of relations (except the empty relation which obviously generates a contradiction by itself) are:
a)
b)
c)
d)
e)
f)
g)
h)
i)
l)
m)
n)

CG
CG
CG
CG
CGPP
CGPP
CGPP
CGPP
CGPP
CGPP
CNO
fCG; CGPPg














CGPP
CNO
fCGPP; CNOg
fCGPP; CGPP,1; CNOg
CGPP,1
CNO
fCG; CGPP,1g
fCG; CNOg
fCGPP,1 ; CNOg
fCG; CGPP,1; CNOg
fCG; CGPPg
fCGPP,1 ; CNOg

This is simply obtained by considering all pairs of relations in M99 whose intersection is
empty.
In G99 the contradictions are only:
fCG; CGPPg  fCG; CGPPg ^  fCGPP; CGPP,1; CNOg and
fCG; CGPPg  fCG; CNOg  fCGPP; CGPP,1 ; CNOg .
Henceforth we represent the relation fCG; CGPPg by -, the relation fCG; CNOg by ./
and the relation fCGPP; CGPP,1 ; CNOg by 6. Since any path in which labels are all corresponds to the representation of - between each pair of vertices in the path, and 
 ./ = -, the possible expressions for contradictions in G99 are (-n 
 -,n)  6 and (-n

 -,(n,k) 
 ./ 
 -,k )  6. Hence, a cycle (-n 
 -,n) is called a --cycle, while a cycle
(-n 
 -,(n,k) 
 ./ 
 -,k ) is called a quasi --cycle. The graph representation of these
two different contradictory situations in G99 is showed in Figure 5. A --cycle and a quasi
--cycle, in M99 force the elements involved in the cycle to be all in the relation CG .
We can now show that the contradictions represented in G99 are the only contradictions
which can be obtained by the implementation suggested in Lemma 5. This is very important,
because we may perform consistency checking by simply checking all cycles. This result is
stated in the next lemma.

Lemma 6 Given a network of constraints T on M99 , T is inconsistent iff 	99(T ) contains
either a --cycle of a quasi --cycle, and two vertices of one cycle are connected by an edge
labelled by 6.
375

fiCristani

Proof (Sketch)

Case by case, the contradictions from a) to n), as in the table above, generate one of the
two situations of the claim.
For example, the contradiction a) CG  CGPP , generates
(fCG; CGPPg 
 fCG; CGPPg ^ )  (fCG; CGPPg  fCGPP; CGPP,1 ; CNOg )
which is a --cycle in which two vertices are connected by an edge labelled by 6, as stated
in the claim. All the other cases behave in the same way as can be easily checked by the
reader.
Conversely, if a contradiction derives from one of the possible implicit ways of representing relations in M99 , the implementation also produces one of the cases of the claim.
In particular we have that, in M99 , CGn = CG , CGPPn = CGPP , (CGPP,1 )n = CGPP,1 ,
fCG; CGPPgn = fCG; CGPPg and fCG; CGPP,1 gn = fCG; CGPP,1g are the only idempotent relations. The other cases of implicitness can be obtained by considering all the
14  14 = 196 pairs of relations in M99 , composing and intersecting them. The implicit cases
arising thus are listed in below.
i.1.

CG

= fCG; CGPPgn  fCG; CGPP,1 gn

i.2.

CG

= fCG; CGPPgn  fCG; CNOg

i.3.

CGPP

= fCG; CGPPgn  fCGPP; CNOg

i.4.

CNO

= fCG; CNOg  fCGPP; CNOg

i.5.

CNO

= fCGPP; CNOg  fCGPP,1 ; CNOg

i.6.

CNO

= fCGPP; CNOg  fCG; CGPP,1 ; CNOg

i.7.

fCG; CNOg

= fCG; CGPP; CNOg

i.8.

fCGPP; CNOg

= CGPPn 
 CNO

i.9.

fCGPP; CNOg

= CGPPn 
 fCG; CNOg

fCG; CGPP,1; CNOg



i.10. fCGPP; CNOg

= CGPPn 
 fCG; CGPP; CNOg

i.11. fCGPP; CNOg

= fCG; CGPPgn 
 CNO

i.12. fCGPP; CNOg

= fCG; CGPP; CNOg

fCGPP; CGPP,1; CNOg
i.13. fCG; CGPP; CNOg = fCG; CGPPgn 
 fCG; CNOg



Readers may directly express in G99 each of these relations along with the corresponding
376

fiThe Complexity of Reasoning about Spatial Congruence

Figure 5: Contradictions in G99 : fg indicates ? and R, S, T respectively fCG; CGPPg,
fCG; CNOg and fCGPP; CGPP,1; CNOg.
relation producing a contradiction in M99 and they immediately verify the existence of -cycles or quasi --cycle where one pair of vertices is connected by an edge labelled by 6 for
these generated graphs.
For example if we express CG as implicit relation by i.1. as in table above (formed by
relations in G99 ), CGPP as implicit relation by i.3. and fCGPP; CNOg by 	99 as in t.8.,
and consider the contradiction CG  CGPP we obtain a quasi --cycle where two vertices
are connected by an edge labelled by 6. Similarly we can derive the other cases.



We can now exhibit an algorithm which looks for --cycles and quasi --cycles, and
checks about pairs of a cycle being connected by an edge labelled by 6.
In Figure 7 an algorithm able to solve the Consistency Checking Problem for networks
of constraints labelled by relations in M99 is presented. Based on Lemma 6 we can prove
the following theorem.
Theorem 5 Algorithm M99-CONSISTENCY correctly decides the consistency of a network of constraints T on M99 in O(n2 ) steps where n is the number of vertices of T .

Proof

By Lemma 6 we can ensure the correctness of Algorithm M99-CONSISTENCY. The complexity of the algorithm can be derived from the fact that the computation of strongly
377

fiCristani

Figure 6: Contradictions in M99 : R, S, T respectively indicate fCG; CGPPg,
fCG; CNOg and fCGPP; CGPP,1; CNOg. Letters from a) to n) refer to the
table of page 375.
378

fiThe Complexity of Reasoning about Spatial Congruence

ALGORITHM M99-CONSISTENCY
A constraint network T on M99

INPUT:

OUTPUT:

1.
2.
3.
4.
2.

Yes if T has a solution formed by spatial regions of R2 , no if not.

Translate T into the generator set as in Lemma 5
Look for cycles labelled by - and at most one ./
Check whether edges between two vertices in one cycle are not labelled by 6
otherwise return no.
Substitute vertices of the cycle with one vertex.
If there are more cycles go to Step 2., otherwise return yes.
Figure 7: Algorithm M99-consistency

connected components is a O(e) problem where e is the number of edges in the network.
The 	99 implementation adds, in the worst case, O(e0 ) edges, where e0 is the number of
edges in T , and therefore the number of edges in 	99 (T ) is O(2  e0 ), which is O(n2 ).



An immediate consequence of Theorem 5 is the following theorem:

Theorem 6 MSAT(M99 ) is polynomial.
The subalgebras of MC-4 included in M99 are in Table 10 of Appendix A.

4.3 The Maximal Tractable Subclass M81

The problem of deciding the consistency of a network of constraints on MC-4 is tractable,
by means of Tables 6, 7, 8 and 10 for 85 subalgebras. The remainder is formed by 17
subalgebras, each of these can be either tractable or intractable. The set
M81 = f ?, CG, CGPP, CGPP,1, fCG; CGPPg, fCG; CGPP,1g, fCGPP; CGPP,1g,
fCG; CGPP; CGPP,1g, fCGPP; CGPP,1 ; CNOg, >g
contains all these subalgebras, so that if M81 is tractable all these algebras are as well.
By analogy with the schema of the previous section we look for a small generator set
for M81 . This is G81 = ffCG; CGPPg, fCG; CGPP; CGPP,1 g, fCGPP; CGPP,1 ; CNOgg.
The following lemma states the properties of M81 with respect to G81 .
Lemma 7 Gd81 = M81 .

Proof

The following expressions represent valid implementations of relations in M81 using only
elements of G81 and the operators of composition, intersection and converse.
379

fiCristani

r.1. ?

= fCG; CGPPg  fCG; CGPPg

r.2. CG

= fCG; CGPPg  fCG; CGPPg ^

r.3. CGPP

= fCG; CGPPg  fCGPP; CGPP,1 ; CNOg

r.4. CGPP,1

= fCG; CGPPg ^  fCGPP; CGPP,1 ; CNOg

r.5. fCG; CGPP,1 g

= fCG; CGPPg ^

 fCGPP; CGPP,1 ; CNOg

r.6. fCGPP; CGPP,1 g = fCG; CGPP; CGPP,1 g

fCGPP; CGPP,1 ; CNOg

^



= fCG; CGPPg 
 fCGPP; CGPP,1 ; CNOg

r.7. >


The implementation of relations in M81 as described in Lemma 7 is denoted by 	81 . The
contradictions in M81 are as in the next table.
a)
b)
c)
d)
e)

CG
CG
CG
CGPP
CGPP







CGPP
fCGPP; CGPP,1g
fCGPP; CGPP,1; CNOg
CGPP,1
fCG; CGPP,1g

The only possible contradiction in G81 is

fCG; CGPPg  fCG; CGPPg ^  fCGPP; CGPP,1; CNOg
and it corresponds to a --cycle where two vertices are connected by an edge labelled by 6.
Lemma 8 Given a network of constraints T on M81 , T is inconsistent iff 	81(T ) contains
a --cycle and two vertices of one cycle are connected by an edge labelled by 6.
Proof (Sketch)

Case by case, the contradictions from a) to e), as in the table above, generate one of the
two cases claimed here.
For example, the contradiction CG  fCGPP; CGPP,1 g , is implemented
(fCG; CGPPg  fCG; CGPPg ^ )  (fCG; CGPP; CGPP,1 g
 fCGPP; CGPP,1; CNOg )
which is a --cycle in which two vertices are connected by an edge labelled by 6, as stated
in the claim. All the other cases behave in the same way as can be easily checked by the
reader.
The only possible ways of representing implicit relations are provided by the schema of
	81 . Therefore the claim is proved.
380

fiThe Complexity of Reasoning about Spatial Congruence

Figure 8: Contradictions in M81 .
R, S, T respectively indicate fCG; CGPPg,
,
1
fCG; CGPP; CGPP g, fCGPP; CGPP,1; CNOg. Letters from a) to e) refer to
the table of page 380.

ALGORITHM M81-CONSISTENCY
A constraint network T on M81

INPUT:

OUTPUT:

1.
2.
3.
4.
2.

Yes if T has a solution formed by spatial regions of R2 , no if not.

Translate T into the generator set as in Lemma 7
Look for cycles labelled by Check whether edges between two vertices in one cycle are not labelled by 6
otherwise return no.
Substitute vertices of the cycle with one vertex.
If there are more cycles go to Step 2., otherwise return yes.
Figure 9: Algorithm M81-consistency
381

fiCristani


In Figure 9 an algorithm is presented which solves the problem of consistency checking for
the subalgebra M81 . We can show, in particular, the following claim.

Theorem 7 Algorithm M81-CONSISTENCY correctly decides the consistency of a network of constraints T on M81 in O(n2 ) steps where n is the number of vertices of T .
Proof

By Lemma 8 we can ensure the correctness of Algorithm M81-CONSISTENCY. The complexity of the algorithm can be derived from the fact that the computation of strongly
connected components is a O(e) problem where e is the number of edges in the network.
The 	81 implementation adds, in the worst case, O(e0 ) edges, where e0 is the number of
edges in T , and therefore the number of edges in 	81 (T ) is O(2  e0 ), which is clearly O(n2 ).


An immediate consequence of Theorem 3 is the following

Theorem 8 MSAT(M81 ) is polynomial.
The subalgebras included in M81 are in Table 9 of Appendix A.

5. Conclusions

We presented a classification of tractability which is complete for the spatial algebra MC-4.
This classification states that there exist three maximal tractable subalgebras M72 , M99
and M81 which include 92 out of 102 expressive subalgebras of MC-4.
The interest in a complete classifications of tractability, as already observed by Jonsson
and Drakengren (1997), is determined by the need for a definition of the boundary between
tractable and intractable problems. Nebl (1999) has suggested that the knowledge of this
boundary can be used either as preprocessing step and as a way to structure the backtracking
search on such algebras.
The provision of a complete classification is one step in researching about constraint
algebras. A next step is the individuation of useful heuristics which give improvements in the
performances of various techniques. We are currently exploiting the use of these techniques
in association with techniques based on the classification presented in this paper to obtain
ecient reasoning algorithms which can be used in practice. Preliminary results on pathconsistency are encouraging, but we cannot yet guarantee the percentage of improvement,
since the algebra MC-4 is so simply structured that for networks randomly chosen it is very
hard to obtain a case where inconsistency is not detectable by path-consistency.

Acknowledgements

I would like to thank Elena Mutinelli who developed her Laurea Thesis (Mutinelli, 1998)
on the theme of complexity of reasoning about congruence and first discussed preliminary
382

fiThe Complexity of Reasoning about Spatial Congruence

results which I used for developing the classification of this paper. Her work has been very
relevant in reaching my results.
I would also like to thank Bernhard Nebel for some important observations he made to
a student of mine, Alessandro Fin, which improved this work. Further thanks go to Jochen
Renz for discussions in the early stages of this work.
I would also like to thank the anonymous referees of the Journal of Artificial Intelligence
Research for their very careful comments and suggestions which allowed me to make the
presentation simpler and more systematical. Some of the proofs in the paper have been
rewritten thanks to their suggestions.
Finally I would like to thank Tony Cohn for reading a near final version. His comments
have been useful in enhancing both scientific and literary quality of the paper.
This work has been developed in the context of the National Project, MURST ex 40%
\Metodologie e tecnologie per la gestione di dati e processi su reti internet ed intranet"
(Methods and technologies for data and process management on internet and intranet)
directed by L. Tanca.

383

fiCristani

Appendix A. Tables of the 102 subalgebras of MC-4

In this section we present the subalgebras of MC-4 organized in separated tables depending
on their characteristics. In particular Table 5 shows those subalgebras which are intractable
by Lemma 4, Table 6 those which are tractable by Theorem 3, and in Tables 7, 8 those
to which Algorithm used in Theorem 3 can be applied (which are all including CNO or
fCGPP; CGPP,1g ). In Table 10 the subalgebras included in M99 are displyed, while in
Table 9 we present the subalgebras included in M81 .
Rel.

Alg.

M30
M43
M44
M46
M54
M56
M67
M77
M83
M84
M85
M89
M90
M92
M93
M95
M97
M98
M100
M101



CG
CGPP
CGPP,1
CNO






















  
  


    

     

     





































































 
 





  
  



























































 















































 
 
  
Table 5: The subalgebras of MC-4 containing the relations CNO and fCGPP; CGPP,1 g

384

fiThe Complexity of Reasoning about Spatial Congruence



Rel. CG

Alg.

M0
M1
M3
M5
M9
M12
M18
M22
M25
M34
M38
M63
M72

CGPP
CGPP,1
CNO















M2
M6
M10
M15
M24
M28
M37
M39
M47
M58
M64
M78













 






 











  
  



CGPP
CGPP,1
CNO














 

  
  


    

     

     















 


 


 
 
 





















 
  
Table 6: The subalgebras of MC-4 contained in M72 .

Rel. CG

Alg.

  
  


    

     

     





















 





 

  
Table 7: The subalgebras of MC-4 contained in M78 and not contained in M72 .
385

fiCristani



Rel. CG

Alg.

CGPP
CGPP,1
CNO



  
  

    

     

     








 

 
Table 8: The subalgebras of MC-4 contained in M31 and not contained in M72 or M78 .
M4
M13
M16
M31

Rel. CG

Alg.

M11
M20
M21
M23
M29
M32
M33
M35
M42
M45
M55
M59
M60
M66
M70
M74
M81

CGPP
CGPP,1
CNO

































  
  


    

     

     





























 
 



  
  
 
 

 
  
  
  
  
  
  





 
 
























  
  

  


Table 9: The subalgebras of MC-4 contained in M81 and not contained in M99 or M72 or
M78 or M31 .

386

fiThe Complexity of Reasoning about Spatial Congruence



Rel. CG

Alg.

CGPP
CGPP,1
CNO




 




 

 




 

 


 

 

 

 

 
 

 
 
 

  
  

    

     

     









 






 



 


 


 
  

 
 


  



  

 
 
 
 

 
  

  
 
 
 
 

  
 
 



     
 
 
 
 
  
 
   
 

 
     
 
  
 

      
 
     
   
 
 
 
      
 
      
      
 
          
Table 10: The subalgebras of MC-4 contained in M99 and not contained in M72 or M78 or
M31 .
M7
M8
M14
M17
M19
M26
M27
M36
M40
M41
M48
M50
M51
M52
M57
M61
M62
M65
M68
M69
M71
M73
M75
M76
M79
M80
M82
M86
M87
M88
M91
M94
M96
M99








































387

fiCristani

References
Anger, F., Mitra, D., & Rodriguez, R. (1998). Temporal Constraint Networks in Nonlinear
Time. In Workshop Notes of the Spatial and Temporal Reasoning at ECAI98 Brighton,
UK.
Anger, F., Mitra, D., & Rodriguez, R. (1999). Satisfiability in Nonlinear Time: Algorithms
and Complexity. In Proceedings of the Florida Artificial Intelligence Research Society
conference Orlando (USA).
Bennett, B. (1994). Spatial reasoning with propositional logic. In Doyle, J., Sandewall, E.,
& Torasso, P. (Eds.), Proceedings of the 4th International Conference on Principles of
Knowledge Representation and Reasoning (KR-94), pp. 165{176. Morgan Kaufmann,
San Francisco, CA, USA.
Bennett, B. (1995). Carving Up Space: Existential Axioms for a Formal Theory of Spatial
Regions. In Proceedings of the IJCAI 95 Workshop on Spatial and Temporal Reasoning
Montreal.
Borgo, S., Guarino, N., & Masolo, C. (1996). A Pointless Theory of Space Based On
Strong Connection and Congruence. In Aiello, L. C., & Doyle, J. (Eds.), Proceedings
of the 6th International Conference on Principles of Knowledge Representation and
Reasoning (KR-96). Morgan Kaufmann, San Francisco, CA, USA.
Borgo, S., Guarino, N., & Masolo, C. (1997). An Ontological Theory of Physical Objects.
In Ironi, L. (Ed.), Proceedings of Eleventh International Workshop on Qualitative
Reasoning (QR 1997), pp. 223{231 Cortona, Italy.
Clarke, B. L. (1981). A Calculus of Individuals Based on \Connection". Notre Dame
Journal of Formal Logic, 22, 204{218.
Cook, S. A. (1971). The complexity of theorem-proving procedures. In Proceedings of the
3rd Symposium on Theory of Computation, pp. 151{158.
Cristani, M. (1997). Morphological Spatial Reasoning: Preliminary Report. Tech. rep.
08/97, LADSEB-CNR Padova (Italy).
Cui, Z. (1994). Using interval logic for order assembly. In Proceedings of the Second International Conference on Intelligent Systems in Molecular Biology, pp. 103{111. AAAI
Press.
Drakengren, T., & Jonsson, P. (1997). Twenty-one large tractable subclasses of Allen's
algebra. Artificial Intelligence, 93 (1-2), 297{319.
Egenhofer, M. J. (1991). Reasoning about binary topological relations. In Gunther, O., &
Schek, H. J. (Eds.), Advances in Spatial Databases-Second Symposium SSD '91, No.
525 in Lecture Notes in Computer Science, pp. 143{160. Springer-Verlag, New York,
NY, USA.
388

fiThe Complexity of Reasoning about Spatial Congruence

Egenhofer, M. J., & Franzosa, R. (1991). Point-Set Topological Spatial Relations. International Journal of Geographical Information Systems, 5 (2), 161{174.
Egenhofer, M. J., & Herring, J. (1990). A Mathematical Framework for the Definition
of Topological Relationships. In Fourth International Symposium on Spatial Data
Handling, pp. 803{813 Zurich, Switzerland.
Franzosa, R., & Egenhofer, M. J. (1992). Topological Spatial Relations Based on Components and Dimensions of Set Intersections. In SPIE's OE/Technology '92-Vision
Geometry Boston, MA ,USA.
Gotts, N. M. (1994). How Far Can We \C"? defining a \Doughnut" Using Connection
Alone. In Doyle, J., Sandewall, E., & Torasso, P. (Eds.), Proceedings of the 4th
International Conference on Principles o f Knowledge Representation and Reasoning
(KR-94), pp. 246{257. Morgan Kaufmann, San Francisco, CA, USA.
Gotts, N. M., Gooday, J. M., & Cohn, A. G. (1996). A Connection Based Approach to
Commonsense Topological Description and Reasoning. The Monist: An International
Journal of General Philosofical Inquiry, 79 (1).
Grigni, M., Papadias, D., & Papadimitriou, C. (1995). Topological Inference. In Mellis, C.
(Ed.), Proceeding of the 14th International Joint Conference on Artificial Intelligence
(IJCAI-95), pp. 901{906 Montreal, PQ, Canada. Morgan Kaufmann.
Jonsson, P., & Drakengren, T. (1997). A Complete Classification of Tractability in the Spatial Theory RCC-5. Journal of Artificial Intelligence Research, 6, 211{221. Research
Note.
Jonsson, P., Drakengren, T., & Backstrom, C. (1999). Computational complexity of relating
time points with intervals. Artificial Intelligence, 109 (1-2), 273{295.
Ladkin, P., & Maddux, R. (1994). On Binary Constraint Problems. Journal of the ACM,
41 (3), 435{469.
Lemon, O. J. (1996). Semantical Foundations of Spatial Logics. In Aiello, L. C., & Doyle, J.
(Eds.), Proceedings of the 6th International Conference on Principles o f Knowledge
Representation and Reasoning (KR-96). Morgan Kaufmann, San Francisco, CA, USA.
Mackworth, A., & Freuder, C. (1985). The complexity of some polynomial network consistency algorithms for constraint satisfation problems. Artificial Intelligence, 25 (1),
65{74.
Mutinelli, E. (1998). Sviluppo ed Analisi di Algoritmi per il Ragionamento Spaziale Qualitativo con Reti di Vincoli. Laurea thesis, Universita di Verona. in Italian.
Nebel, B. (1995). Computational properties of qualitative spatial reasoning: First results. In
Advances in artificial intelligence (KI-95), pp. 233{244 Bielefeld, Germany. SpringerVerlag, New York, NY, USA.
389

fiCristani

Nebel, B. (1999). Observations on the complexity of reasoning in constraint algebras.
Personal communication to A. Fin and M. Cristani.
Nebel, B., & Burckert, H. J. (1995). Reasoning about temporal relations: A maximal
tractable subclass of Allen's interval algebra. Journal of the ACM, 42 (1), 43{66.
Randell, D. A., & Cohn, A. G. (1989). Modelling topological and metrical properties
of physical processes. In Brachman, R. J., Levesque, H. J., & Reiter, R. (Eds.),
Proceedings of the 1st International Conference on Knowledge Representation and
Reasoning (KR-89), pp. 55{66 Toronto, ON, Canada. Morgan Kaufmann.
Randell, D. A., Cui, Z., & Cohn, A. G. (1992). A spatial logic based on regions and
connection. In Swartout, B., & Nebel, B. (Eds.), Proceedings of the 3rd International
Conference on Principles of Knowledge Representation and Reasoning (KR-92), pp.
165{176 Cambridge, MA, USA. Morgan Kaufmann.
Renz, J. (1999). Maximal Tractable Fragments of the Region Connection Calculus: A
Complete Analysis. In Proccedings of the 17th International Conference on Artificial
Intelligence (IJCAI 99).
Renz, J., & Nebel, B. (1999). On the complexity of qualitative spatial reasoning: A maximal
tractable fragment of the Region Connection Calculus. Artificial Intelligence, 108 (12), 69{123.
Statman, R. (1979). Intuitionistic logic is polynomial-space complete. Theoretical Computer
Science, 9 (1), 67{72.
Zimmermann, K. (1995). Measuring without Measures: The -Calculus. In Proceedings of
the International Conference on Spatial Information, pp. 59{67.

390

fiJournal of Artificial Intelligence Research 11 (1999) 277-300

Submitted 5/99; published 10/99

Reasoning about Minimal Belief and Negation as Failure
Riccardo Rosati

rosati@dis.uniroma1.it

Dipartimento di Informatica e Sistemistica
Universita di Roma \La Sapienza"
Via Salaria 113, 00198 Roma, Italy

Abstract
We investigate the problem of reasoning in the propositional fragment of MBNF, the
logic of minimal belief and negation as failure introduced by Lifschitz, which can be considered as a unifying framework for several nonmonotonic formalisms, including default
logic, autoepistemic logic, circumscription, epistemic queries, and logic programming. We
characterize the complexity and provide algorithms for reasoning in propositional MBNF.
In particular, we show that skeptical entailment in propositional MBNF is p3 -complete,
hence it is harder than reasoning in all the above mentioned propositional formalisms for
nonmonotonic reasoning. We also prove the exact correspondence between negation as
failure in MBNF and negative introspection in Moore's autoepistemic logic.

1. Introduction
Research in the formalization of commonsense reasoning has pointed out the need of formalizing agents able to reason introspectively about their own knowledge and ignorance
(Moore, 1985; Levesque, 1990). Modal epistemic logics have thus been proposed, in which
modalities are interpreted in terms of knowledge or belief. Generally speaking, the conclusions an introspective agent is able to draw depend on both what she knows and what she
does not know. Hence, any such conclusion may be retracted when new facts are added to
the agent's knowledge. For this reason, many nonmonotonic modal formalisms have been
proposed in order to characterize the reasoning abilities of an introspective agent.
Among the nonmonotonic modal logics proposed in the literature, the logic of minimal
belief and negation as failure MBNF (Lifschitz, 1991, 1994) is one of the most studied formalisms (Chen, 1994; Bochman, 1995; Beringer & Schaub, 1993). Roughly speaking, such
a logic is built by adding to first-order logic two distinct modalities, a \minimal belief"
modality B and a \negation as failure" modality not . The logic thus obtained is characterized in terms of a nice model-theoretic semantics. MBNF has been used in order to
give a declarative semantics to very general classes of logic programs (Lifschitz & Woo,
1992; Schwarz & Lifschitz, 1993; Inoue & Sakama, 1994), which generalize the stable model
semantics of negation as failure in logic programming (Gelfond & Lifschitz, 1988, 1990,
1991). Also, MBNF can be viewed as an extension of the theory of epistemic queries to
databases (Reiter, 1990), which deals with the problem of querying a first-order database
about its own knowledge. Due to its ability of expressing many features of nonmonotonic
logics (Lifschitz, 1994; Schwarz & Lifschitz, 1993), MBNF is generally considered as a unifying framework for several nonmonotonic formalisms, including default logic, autoepistemic
logic, circumscription, epistemic queries, and logic programming.

c 1999 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiRosati

Although several aspects of the logic MBNF have been thoroughly investigated (Schwarz
& Lifschitz, 1993; Chen, 1994; Bochman, 1995), the existing studies concerning the computational properties of MBNF are limited to subclasses of propositional MBNF theories
(Inoue & Sakama, 1994) or to a very restricted subset of the first-order case (Beringer &
Schaub, 1993).
In this paper we present a computational characterization of deduction in the propositional fragment of MBNF. In particular, we show that logical implication in the propositional fragment of MBNF is a p3 -complete problem: hence, it is harder (unless the polynomial hierarchy collapses) than reasoning in all the best known propositional formalisms
for nonmonotonic reasoning, like autoepistemic logic (Niemela, 1992; Gottlob, 1992), default logic (Gottlob, 1992), circumscription (Eiter & Gottlob, 1993), (disjunctive) logic
programming (Eiter & Gottlob, 1995), and several McDermott and Doyle's logics (Marek
& Truszczynski, 1993). As shown in the following, this result also implies that minimal
knowledge is computationally harder than negation as failure.
Moreover, we study the subclass of at MBNF theories, i.e. MBNF theories without nested occurrences of modalities, showing that in this case logical implication is p2 complete. This case is the most interesting one from the logic programming viewpoint.
Indeed, it implies that, under the stable model semantics, increasing the syntax of program
rules, by allowing propositional formulas as goals in the rules, does not affect the worst-case
complexity of query answering for disjunctive logic programs with negation as failure.
Furthermore, we provide algorithms for reasoning both in MBNF and in its at fragment,
which are optimal with respect to worst-case complexity. Notably, such deductive methods
can be considered as generalizations of known methods for reasoning in nonmonotonic
formalisms such as default logic, autoepistemic logic, and logic programming under stable
model semantics.
We also show that the \negation as failure" modality in MBNF exactly corresponds
to negative introspection in autoepistemic logic (Moore, 1985). This result implies that
the logic MBNF can be considered as the \composition" of two epistemic modalities: the
\minimal knowledge" operator due to Halpern and Moses (1985) and Moore's autoepistemic
operator.
Besides its theoretical interest, we believe that such a computational and epistemological
analysis of MBNF has interesting implications for the development of knowledge representation systems with nonmonotonic abilities, since it allows for a better understanding and
comparison of the different nonmonotonic formalisms captured by MBNF. The interest in
defining deductive methods for MBNF also arises from the fact that such a logic, originally
developed as a framework for the comparison of different logical approaches to nonmonotonic reasoning, has recently been considered as an attractive knowledge representation
formalism. In particular, it has been shown (Donini, Nardi, & Rosati, 1997a) that the full
power of MBNF is necessary in order to logically formalize several features of implemented
frame-based knowledge representation systems.
In the following, we first briey recall the logic MBNF. In Section 3 we address the
relationship between MBNF and Moore's autoepistemic logic. Then, in Section 4 we study
the problem of reasoning in propositional MBNF: we first consider the case of general MBNF
theories, then we deal with at MBNF theories. In Section 5 we present the computational
278

fiReasoning about minimal belief and negation as failure

characterization of reasoning in MBNF. We conclude in Section 6. This paper is an extended
and thoroughly revised version of (Rosati, 1997).

2. The Logic MBNF

In this section we briey recall the logic MBNF (Lifschitz, 1994), which is a modal logic
with two epistemic operators: a \minimal belief" modality B and a \negation as failure"
(also called \negation by default") modality not . We use L to denote a fixed propositional
language built in the usual way from: (i) an alphabet A of propositional symbols; (ii) the
symbols true, false; (iii) the propositional connectives _; ^; :; . We denote as LM the
modal extension of L with the modalities B and not . We say that a formula ' 2 LM
has (modal) depth i (with i  0) if each subformula in ' lies within the scope of at most
i modalities, and there exists a subformula in ' which lies within the scope of exactly i
modalities.
We denote as LSM the set of subjective MBNF formulas, i.e. the subset of formulas from
LM in which each occurrence of a propositional symbol lies within the scope of at least one
modality, and with L1M the set of at MBNF formulas, that is the set of formulas from LM
in which each propositional symbol lies within the scope of exactly one modality. We call a
modal formula ' from LM positive (resp. negative ) if the modality not (resp. B ) does not
occur in '. LB denotes the set of positive formulas from LM , while LSB denotes the set of
positive formulas from LSM .
We now recall the notion of MBNF model. An interpretation is a set of propositional
symbols. An MBNF structure is a triple (I; Mb ; Mn ), where I is an interpretation (also called
initial world) and Mb ; Mn are non-empty sets of interpretations (worlds). Satisfiability of a
formula in an MBNF structure is defined inductively as follows:
1. if ' is a propositional symbol, ' is satisfied by (I; Mb ; Mn ) iff ' 2 I ;
2. :' is satisfied by (I; Mb ; Mn ) iff ' is not satisfied by (I; Mb ; Mn );

3. '1 ^ '2 is satisfied by (I; Mb ; Mn ) iff '1 is satisfied by (I; Mb ; Mn ) and '2 is satisfied
by (I; Mb ; Mn );

4. '1 _ '2 is satisfied by (I; Mb ; Mn ) iff either '1 is satisfied by (I; Mb ; Mn ) or '2 is
satisfied by (I; Mb ; Mn );
5. '1  '2 is satisfied by (I; Mb ; Mn ) iff either '1 is not satisfied by (I; Mb ; Mn ) or '2
is satisfied by (I; Mb ; Mn );
6. B' is satisfied by (I; Mb ; Mn ) iff, for every J 2 Mb , ' is satisfied by (J; Mb ; Mn );

7. not ' is satisfied by (I; Mb ; Mn ) iff there exists J 2 Mn such that ' is not satisfied by
(J; Mb ; Mn ).
We write (I; Mb ; Mn ) j= ' to indicate that ' is satisfied by (I; Mb ; Mn ). We say that
a theory   LM is satisfied by (I; Mb ; Mn ) (and write (I; Mb ; Mn ) j= ) iff each formula
from  is satisfied by (I; Mb ; Mn ). If ' 2 LSM , then the evaluation of ' is insensitive to
the initial interpretation I : thus, in this case we also write (Mb ; Mn ) j= '. Analogously, if
279

fiRosati

' 2 LSB , then the evaluation of ' is insensitive both to the initial interpretation I and to
the set Mn , and we also write Mb j= '. If ' 2 L then the evaluation of ' does not depend
on the sets Mb ; Mn , and in this case we write I j= '.

In order to relate MBNF structures to standard interpretation structures in modal
logic (i.e. Kripke structures), we remark that, due to the above notion of satisfiability,
we can consider the sets Mb , Mn in an MBNF interpretation structure as two distinct
universal Kripke structures, i.e. possible-world structures in which each world is connected
to all worlds of the structure. In fact, since the accessibility relation in such a structure is
universal, without loss of generality it is possible to identify a universal Kripke structure
with the set of interpretations contained in it. We recall that the class of universal Kripke
structures characterizes the logic S5 (Marek & Truszczynski, 1993, Theorem 7.52).
The nonmonotonic character of MBNF is obtained by imposing the following preference
semantics over the interpretation structures satisfying a given theory.

Definition 2.1 A structure (I; M; M ), where M 6= ;, is an MBNF model of a theory
  LM iff (I; M; M ) j=  and, for each interpretation J and for each set of interpretations
M 0, if M 0  M then (J; M 0 ; M ) 6j= .
We say that a formula ' is entailed (or logically implied ) by  in MBNF (and write
 j=MBNF ') iff ' is satisfied by every MBNF model of . In order to simplify notation,
we denote the MBNF model (I; M; M ) with the pair (I; M ), and, if  2 LSM , we denote
(I; M; M ) with M , since in this case the evaluation of  is insensitive to the initial world I ,
namely, if (I; M ) is a model for , then, for each interpretation J , (J; M ) is a model for .

Example 2.2 Let  = fBpg. The only MBNF models for  are of the form (I; M ), with
M = fI : I j= pg. Hence,  j=MBNF Bp, and  j=MBNF :B for each 2 L such that the
propositional formula p  is not valid. Therefore, the agent modeled by  has minimal
belief, in the sense that she only believes p and the objective facts logically implied by p.

Example 2.3 Let  = fnot married  B hasNoChildren g. It is easy to see that the only
models for  are of the form (I; M ) such that M = fI : I j= hasNoChildren g, since married
can be assumed not to hold by the agent modeled by , which is then able to conclude
::married in
B hasNoChildren . Notably, the meaning of  is analogous to the default rule hasNoChildren
Reiter's default logic (Lifschitz, 1994). Also, let  = fB bird ^ not : ies  B ies ; B bird g.

In a way analogous to the previous case, it can be shown that the only MBNF models for
 are of the form (I; M ), with M = fI : I j= bird ^ ies g. Therefore,  j=MBNF B ies . As
:ies g; bird ).
shown by Lifschitz (1994),  corresponds to the default theory (f birdies

Given a set of interpretations M , Th(M ) denotes the set of formulas B' such that
B' 2 LB and M j= B'. Let M1; M2 be sets of interpretations. We say that M1 is
equivalent to M2 iff Th(M1 ) = Th(M2 ).

Definition 2.4 A set of interpretations M is maximal iff, for each set of interpretations
M 0, if M 0 is equivalent to M then M 0  M .
280

fiReasoning about minimal belief and negation as failure

It turns out that, when restricting to theories composed of subjective positive formulas,
MBNF corresponds to the modal logic of minimal knowledge due to Halpern and Moses
(1985), also known as ground nonmonotonic modal logic S5G (Kaminski, 1991; Donini,
Nardi, & Rosati, 1997b). In fact, S5G is obtained from modal logic S5 by imposing the
following preference order over the universal Kripke structures satisfying a theory  2 LB :
M is a model for  iff M j=  and, for each M 0 , if M 0 j=  then M 0 6 M (Shoham,
1987). In fact, it is immediate to see that the MBNF semantics of theories composed of
subjective positive formulas corresponds to the above semantics according to S5G . Hence,
the following property holds.

Proposition 2.5 Let   LB . Then, M is an S5G model for  iff, for each I , (I; M ) is
an MBNF model for fB' : ' 2 g.
The previous proposition implies that, when   LSB , a set of interpretations M satisfying  is compared with all other sets of interpretations satisfying , while, in the case
  LM , M is only compared with the sets M 0 such that (M 0 ; M ) satisfies .

Hence, the main difference between MBNF and S5G lies in the fact that in S5G all models
are maximal with respect to set containment (or minimal with respect to the objective
knowledge which holds in the model), while in MBNF this property is not generally true.
E.g., the theory  = fnot married _ B married g has two types of models, for each possible
choice of the initial world J : (J; M1 ), where M1 corresponds to the set of all interpretations
(which represents the case in which married is not assumed to hold); and (J; M2 ), where
M2 = fI : I j= married g. Namely, if married is assumed to hold, then  forces one to
conclude B married : that is, the initial assumption is justified by the knowledge derived on
the basis of such an assumption (Lin & Shoham, 1992). We remark that, by Proposition 2.5,
the interpretation of the MBNF operator B exactly corresponds to the interpretation of the
modality B in S5G .

3. Relating MBNF to Autoepistemic Logic

In this section we study the relationship between autoepistemic logic and MBNF. First, we
briey recall Moore's autoepistemic logic (AEL). In order to keep notation to a minimum,
we change the language of AEL, using the modality B instead of L. Thus, in the following
a formula of AEL is a formula from LB .

Definition 3.1 A propositionally consistent set of formulas T  LB is a stable expansion
for a set of initial knowledge   LB if T satisfies the following equation:
T = Cn( [ fB' j ' 2 T g [ f:B' j ' 62 T g)
(1)
where Cn(S ) denotes the propositional deductive closure of the modal theory S  LB .
Given a theory   LB and a formula ' 2 LB , we write  j=AEL ' iff ' belongs to
all the stable expansions of . Each stable expansion T is a stable set according to the
following definition (Stalnaker, 1993).

Definition 3.2 A modal theory T  LB is a stable set if
281

fiRosati

1. T = Cn(T );
2. for every ' 2 LB , if ' 2 T then B' 2 T ;
3. for every ' 2 LB , if ' 62 T then :B' 2 T .

We recall that a stable set T corresponds to a maximal universal Kripke structure MT
such that T is the set of formulas satisfied by MT (Marek & Truszczynski, 1993). With the
term AEL model for  we will thus refer to a set of interpretations M whose set of theorems
Th(M ) corresponds to a stable expansion for  in AEL.
Finally, notice that we have adopted the notion of consistent autoepistemic logic (Marek
& Truszczynski, 1993), i.e. in (1) we do not allow the inconsistent theory T = LB composed
of all modal formulas to be a (possible) stable expansion. The results presented in this
section can be easily extended to this case (corresponding to Moore's original proposal):
however, this requires to slightly change the semantics of MBNF, allowing in Definition 2.1
the empty set of interpretations to be a possible component of MBNF structures.
In the following, we use the term embedding (or translation) to indicate a transformation
function  () for modal theories. We are interested in finding a faithful embedding (Gottlob,
1995; Schwarz, 1996; Janhunen, 1998), in the following sense:  () is a faithful embedding
of AEL into MBNF if, for each theory   LB and for each model M , M is an AEL model
for  iff M is an MBNF model for  ().
It is already known that AEL theories can be embedded into MBNF theories. In particular, it has been proven (Lin & Shoham, 1992; Schwarz & Truszczynski, 1994) that AEL
theories with no nested occurrences of B (called at theories) can be embedded into MBNF;
now, since in AEL any theory can be transformed into an equivalent at theory (which has
in general size exponential in the size of the initial theory), it follows that any AEL theory
can be embedded into MBNF.
However, we now prove a much stronger result: negation as failure in MBNF exactly
corresponds to negative introspection in AEL, i.e. AEL's modality :B and MBNF's modality not are semantically equivalent. Hence, such a correspondence is not only limited to
modal theories without nested modalities, and induces a polynomial-time embedding of any
AEL theory into MBNF.
We first define the translation  () of modal theories from AEL to MBNF theories.

Definition 3.3 Let ' 2 LB . Then,  (') is the MBNF formula obtained from ' by substituting each occurrence of B with :not. Moreover, if   LB , then  () denotes the MBNF
theory fB (')j' 2 g.
We now show that the translation  () embeds AEL theories into MBNF. To this aim, we

exploit the semantic characterization of AEL defined by Schwarz (1992). Roughly speaking,
according to such a preference semantics over possible-world structures, an AEL model for
 is a set of interpretations M satisfying  such that, for any interpretation J not contained
in M , the pair (J; M ) does not satisfy . Formally:

Proposition 3.4 (Schwarz, 1992, Proposition 4.1) Let   LB . Then, M is an AEL
model for  iff, for each interpretation I 2 M , (I; M ) j=  and, for each interpretation
J 62 M , (J; M ) 6j= .
282

fiReasoning about minimal belief and negation as failure

In the following, we say that an occurrence of a subformula in a formula ' 2 LM is
strict if it does not lie within the scope of a modal operator. E.g., let  = B' ^ not (B _  ).
The occurrence of B' in  is strict, while the occurrence of B is not strict.

Theorem 3.5 Let   LB . Then, M is an AEL model for  iff, for each I , (I; M ) is an
MBNF model of  ().

If part. Suppose (I; M ) is an MBNF model of  (). Then, for each M 0  M ,
(M 0 ; M ) 6j=  (). Since  () is a set of formulas of the form B', where ' does not contain
any occurrence of the operator B , it follows that, for each subformula of the form not '
occurring in  (), (M 0 ; M ) j= not ' iff (M; M ) j= not '. Now let B' 2  (), let '0 denote
the propositional formula obtained from ' by replacing each strict occurrence in ' of a
formula of the form not with true if (M; M ) j= not and with false otherwise, and let
0 = f'0 : B' 2  ()g. Now suppose there exists an interpretation J such that J j= 0
and J 62 M . Then, from the definition of satisfiability in MBNF structures it follows that
(M [fJ g; M ) j=  (), thus contradicting the hypothesis that (I; M ) is an MBNF model for
 (). Hence, M = fI : I j= 0g. Now consider a pair (J; M ): again, from the definition of
satisfiability in MBNF structures it follows immediately that (J; M ) j=  iff J j= 0 . And
since M contains all the interpretations satisfying 0 , it follows that, for each interpretation
J 62 M , (J; M ) 6j= , therefore by Proposition 3.4 it follows that M is an AEL model for .
Only-if part. Suppose M is an AEL model for . Then, by Proposition 3.4, for each
interpretation I 2 M , (I; M ) j=  and, for each interpretation J 62 M , (J; M ) 6j= . For
each ' 2 , let '00 denote the propositional formula obtained from ' by replacing each strict
occurrence of a formula of the form B with true if M j= B and with false otherwise, and
let 00 = f'00 : ' 2 g. Then, suppose there exists an interpretation J such that J j= 00
and J 62 M . Then, from the definition of satisfiability in MBNF structures it follows that
(J; M ) j= , thus contradicting the hypothesis that M is an AEL model for . Hence,
M = fI : I j= 00g. Now suppose that, for some interpretation I , (I; M ) is not an MBNF
model for  (). Then, there exists M 0  M such that (M 0 ; M ) j=  (). From the definition
of  (), it follows that each interpretation in M 0 satisfies 00 , and, since M 0  M , there exists
J 62 M such that J j= 00. Contradiction. Therefore, (I; M ) is an MBNF model for .
We remark that the above theorem could alternatively be proved from the fact that
the K -free fragment of the logic MKNF (Lifschitz, 1991) is equivalent to AEL, which is
stated (although without proof) by Schwarz and Truszczynski (1994, page 123), and from
the correspondence between MBNF and MKNF (Lifschitz, 1994).
The previous theorem implies that the interpretation of the modality not in MBNF and
of the modal operator in autoepistemic logic are the same. This property extends previous
results relating MBNF with AEL (Lin & Shoham, 1992; Schwarz & Lifschitz, 1993; Chen,
1994), and has interesting consequences both in the logic programming framework and in
nonmonotonic reasoning. In particular, since MBNF generalizes the stable model semantics
for logic programs (Gelfond & Lifschitz, 1988), the above result strengthens the idea that
AEL is the true logic of negation as failure (as interpreted according to the stable model
semantics). Moreover, positive theories have the same interpretation both in MBNF and
in the logic of minimal knowledge S5G (Halpern & Moses, 1985): consequently, the logic
MBNF generalizes both Halpern and Moses' S5G and Moore's AEL.
Proof.

283

fiRosati

4. Reasoning in MBNF

In this section we present algorithms for reasoning in propositional MBNF: in particular, we
study the entailment problem in MBNF. From now on, we assume to deal with finite MBNF
theories , therefore we refer to a single formula  (which corresponds to the conjunction
of all the formulas contained in the finite theory ).

4.1 Characterizing MBNF Models

We now present a finite characterization of the MBNF models of a formula  2 LM . As
in several methods for reasoning in nonmonotonic modal logics (Gottlob, 1992; Marek
& Truszczynski, 1993; Eiter & Gottlob, 1992; Niemela, 1992; Donini et al., 1997b), the
technique we employ is based on the definition of a correspondence between the preferred
models of a theory and the partitions of the set of modal subformulas of the theory. In
fact, such partitions can be used in order to provide a finite characterization of a universal
Kripke structure: specifically, a partition satisfying certain properties identifies a particular
universal Kripke structure M , by uniquely determining a propositional theory such that M
is the set of all interpretations satisfying such a theory.
We extend such known techniques in order to deal with the preference semantics of
MBNF. In particular, we characterize the properties that a partition of modal subformulas
of a formula  2 LM must satisfy in order to identify an MBNF model for . In this way,
we provide a method that does not rely on a modal logic theorem prover, but reduces the
problem of reasoning in a bimodal logic to a number of reasoning problems in propositional
logic.
First, we introduce some preliminary definitions. We call a formula of the form B' or
not ', with ' 2 LM , a modal atom.

Definition 4.1 Let  2 LM . We call the set of modal atoms occurring in  the modal
atoms of  (and denote such a set as MA()).

Definition 4.2 Let  2 LM and let (P; N ) be a partition of a set of modal atoms. We
denote as (P; N ) the formula obtained from  by substituting each strict occurrence in 
of a formula in P with true, and each strict occurrence in  of a formula in N with false.

Observe that only the occurrences in  of modal subformulas which are not within the
scope of another modality are replaced; notice also that, if P [ N contains MA(), then
(P; N ) is a propositional formula. In this case, the pair (P; N ) identifies a guess on the
modal subformulas from , i.e. P contains the modal subformulas of  assumed to hold,
while N contains the modal subformulas of  assumed not to hold.

Definition 4.3 Let  2 LM and let (P; N ) be a partition of MA(). We denote as ob (P; N )
the propositional formula

^
'(P; N )
B'2P
Roughly speaking, the propositional formula ob (P; N ) represents the \objective knowledge" implied by the guess (P; N ) on the formulas of the form B' belonging to P . From the
ob (P; N ) =

284

fiReasoning about minimal belief and negation as failure

semantic viewpoint, in each structure (I; M; M 0 ) satisfying the guess on the modal atoms
given by (P; N ), the propositional formula ob (P; N ) constrains the interpretations of M ,
since in each such structure the propositional formula ob (P; N ) must be satisfied by each
interpretation J 2 M , i.e. J j= ob (P; N ).

Example 4.4 Let

 = (Ba _ not (b ^ c)) ^ d ^ :B (:f _ g)
Then, MA() = fBa; not (b ^ c); B (:f _ g)g. Now suppose that
P = fBa; not (b ^ c)g
N = fB (:f _ g)g
Then, (P; N ) = (true _ true) ^ d ^ :false (which is equivalent to d), and ob (P; N ) = a.

Definition 4.5 We say that a pair of sets of interpretations (M; M 0 ) induces the partition
(P; N ) of MA() if, for each modal atom  2 MA(),  2 P iff (M; M 0 ) j=  .
Lemma 4.6 Let ' 2 LM , let I be an interpretation, let M; M 0 be sets of interpretations,
and let (P; N ) be the partition induced by (M; M 0 ) on a set of modal atoms S . Then,
(I; M; M 0 ) j= ' iff (I; M; M 0 ) j= '(P; N ).
Follows immediately from Definitions 4.2 and 4.5, and from the definition of
satisfiability in MBNF structures.
We now show that, if (I; M ) is an MBNF model for  which induces the partition (P; N )
of MA(), then the formula ob (P; N ) completely characterizes the set of interpretations
M.
Proof.

Theorem 4.7 Let  2 LM , let (I; M ) be an MBNF model for , and let (P; N ) be the
partition of MA() induced by (M; M ). Then, M = fJ : J j= ob (P; N )g.
Let M 0 = fJ : J j= ob (P; N )g. Since (M; M ) induces the partition (P; N ), by
Definition 4.5 it follows that each interpretation in M must satisfy ob (P; N ), hence M  M 0 .
Now suppose M  M 0 , and consider the structure (I; M 0 ; M ). We prove that each modal
atom  2 MA() belongs to P iff (I; M 0 ; M ) j=  . The proof is by induction on the depth
Proof.

of formulas in MA().
First, consider a modal atom not such that 2 L: from the definition of satisfiability
of a formula in an MBNF structure, it follows immediately that not 2 P iff (I; M 0 ; M ) j=
not . Then, consider a modal atom B such that 2 L: if B 2 P , then, by definition of
ob (P; N ), the propositional formula ob (P; N )  is valid, therefore (I; M 0 ; M ) j= B . If
B 2 N , then there exists an interpretation J in M such that J 6j= , and since M 0  M ,
it follows that (I; M 0 ; M ) 6j= B . Hence, each modal atom  2 MA() of depth 1 belongs
to P iff (I; M 0 ; M ) j=  .
Suppose now that  2 P iff (I; M 0 ; M ) j=  for each modal atom  in MA() of depth
less or equal to i. Consider a modal atom B of MA() of depth i + 1: by the induction
hypothesis, and by Lemma 4.6, (I; M 0 ; M ) j= B iff M 0 j= B ( (P; N )). Now, if B 2 P ,
285

fiRosati

then, by definition of ob (P; N ), the propositional formula ob (P; N )  (P; N ) is valid, and
since M 0 = fJ : J j= ob (P; N )g, it follows that M 0 j= B ( (P; N )), which in turn implies
(I; M 0 ; M ) j= B ; on the other hand, if B 2 N , then there exists an interpretation J in M
such that (J; M; M ) 6j= , hence, by the induction hypothesis and Lemma 4.6, J 6j= (P; N ).
Now, since M 0  M , it follows that M 0 6j= B ( (P; N )), hence (I; M 0 ; M ) 6j= B . In the
same way it is possible to show that a modal atom of the form not of depth i + 1 belongs
to P iff (I; M 0 ; M ) j= not .
We have thus proved that each modal atom  2 MA() belongs to P iff (I; M 0 ; M ) j=  :
this in turn implies that (I; M 0 ; M ) j=  iff I j= (P; N ), and since by hypothesis (I; M; M )
satisfies  and (P; N ) is the partition of MA() induced by (M; M ), by Lemma 4.6 it
follows that I j= (P; N ). Therefore, (I; M 0 ; M ) j= , which contradicts the hypothesis
that (I; M ) is an MBNF model for . Consequently, M 0 = M , which proves the thesis.
Informally, the above theorem states that each MBNF model for  can be associated with
a partition (P; N ) of the modal atoms of ; moreover, the propositional formula ob (P; N )
exactly characterizes the set of interpretations M of an MBNF model (I; M ), in the sense
that M is the set of all interpretations satisfying ob (P; N ). This provides a finite way to
describe all MBNF models for .
We now define the notion of a partition of a set of modal atoms induced by a pair of
propositional formulas.

Definition 4.8 Let  2 LM , '1 ; '2 2 L. We denote as Prt (; '1 ; '2 ) the partition of
MA() induced by (M1 ; M2 ), where M1 = fI : I j= '1 g, M2 = fI : I j= '2 g.
In order to simplify notation, we denote as Prt (; ') the partition Prt (; '; '). The
following theorem provides a constructive way to build the partition Prt (; '; ).

Theorem 4.9 Let  2 LM , '; 2 L. Let (P; N ) be the partition of MA() built as follows:
1. start from P = N = ;;
2. for each modal atom B in MA() such that  (P; N ) 2 L, if the propositional formula
'  (P; N ) is valid, then add B to P , otherwise add B to N ;
3. for each modal atom not  in MA() such that  (P; N ) 2 L, if the propositional
formula   (P; N ) is not valid, then add not  to P , otherwise add not  to N ;
4. iteratively apply the above rules until P [ N = MA().
Then, (P; N ) = Prt (; '; ).

The proof is by induction on the structure of the formulas in MA(). First,
from the fact that Prt (; '; ) is the partition induced by (M; M 0 ), with M = fI : I j=
'g, M 0 = fI : I j= g, and from the definition of satisfiability in MBNF structures, it
follows that, if  2 L, then (M; M 0 ) j= B if and only if '   is a valid propositional
formula, and (M; M 0 ) j= not  if and only if   is not a valid propositional formula.
Therefore, (P; N ) agrees with Prt (; '; ) on all modal atoms of modal depth 1. Suppose
now that (P; N ) and Prt (; '; ) agree on all modal atoms of modal depth less or equal
Proof.

286

fiReasoning about minimal belief and negation as failure

to i. Consider a modal atom B of MA() of modal depth i + 1. From Lemma 4.6 and
from the definition of satisfiability in MBNF structures, it follows that (M; M 0 ) j= B if
and only if '   (Prt (; '; )) is a valid propositional formula, and since by Definition 4.2
the value of the formula  (Prt (; '; )) only depends on the guess of the modal atoms of
modal depth less or equal to i in Prt (; '; ), by the induction hypothesis it follows that
(Prt (; '; )) = (P; N ), hence B belongs to P if and only if (M; M 0 ) j= B. Analogously,
it can be proven that any modal atom of depth i + 1 of the form not  belongs to P if and
only if (M; M 0 ) j= not  . Therefore, (P; N ) and Prt (; '; ) agree on all modal atoms of
modal depth i + 1.
The algorithms we present in the following for reasoning in MBNF use the above shown
properties of partitions of modal subformulas of a formula , together with additional
conditions on such partitions (that vary according to the different classes of theories accepted
as inputs), in order to identify all the MBNF models for .
As for the entailment problem  j=MBNF ', we point out that the occurrences of not in
' are equivalent to occurrences of :B , since in each MBNF model for  both modalities in
' are evaluated on the same set of interpretations. Therefore, as in the original formulation
of MBNF (Lifschitz, 1994), we restrict query answering in MBNF to positive formulas.
Let ' 2 LB , 2 L, and M = fJ : J j= g. We denote as '( ) the propositional
formula obtained from ' by substituting each strict occurrence of a modal atom B of
' with true if M j= B, and with false otherwise. It can be immediately verified that
'( ) = '(Prt ('; )).

Theorem 4.10 Let ; ' 2 LM . Let (I; M ) be an MBNF model for  and let (P; N ) be
the partition of MA() induced by (M; M ). Then, ' is satisfied by (I; M; M ) iff I j=

'(ob (P; N )).

The proof follows immediately from the fact that, by Theorem 4.9, '(ob (P; N )) =
'(Prt ('; ob (P; N ))), and from Lemma 4.6.
We now show that the entailment problem in MBNF is related to the membership
problem for stable sets (Gottlob, 1995), which in turn is related to the notion of (objective)
kernel that has been used to characterize stable expansions of autoepistemic theories (Marek
& Truszczynski, 1993).
Proof.

Definition 4.11 Let 2 L. We denote as ST ( ) the (unique) stable set T  LB such
that

T \ L = f' 2 Lj  ' is validg

Theorem 4.12 Let  2 LM , ' 2 LSB . Then,  6j=MBNF ' iff there exists an MBNF model
(I; M ) for  such that ' 62 ST (ob (P; N )), where (P; N ) is the partition of MA() induced

by (M; M ).

Let M = fI : I j= ob (P; N )g: from the above definition and Definition 3.2, it follows
immediately that ST (ob (P; N )) = Th(M ). Therefore, if ' 2 LSB then (I; M; M ) j= ' iff
' 2 ST (ob (P; N )).
Proof.

287

fiRosati

Algorithm MBNF-Not-Entails(; ')
Input: formula  2 LM , formula ' 2 LB ;
Output: true if  6j=MBNF ', false otherwise.
begin
if there exists partition (P; N ) of MA()
such that
(a) (P; N ) = Prt (; ob (P; N )) and
(b) (P; N ) ^ :'(ob (P; N )) is satisfiable and
(c) for each partition (P 0 ; N 0 ) 6= (P; N ) of MA(),
(c1) (P 0 ; N 0 ) is not satisfiable or
(c2) (P 0 ; N 0 ) =
6 Prt (; ob (P 0; N 0 ); ob (P; N )) or
(c3) ob (P; N ) ^ :ob (P 0 ; N 0 ) is satisfiable
then return true
else return false
end
Figure 1: Algorithm MBNF-Not-Entails.

4.2 Reasoning in Propositional MBNF

We now define a deductive method for reasoning in general propositional MBNF theories.
Specifically, we present the algorithm MBNF-Not-Entails, reported in Figure 1, for computing entailment in MBNF.
The algorithm exploits the finite characterization of MBNF models given by Theorem 4.7
and an analogous finite characterization, in terms of partitions of MA(), of all the models
relevant for establishing whether a partition (P; N ) of MA() identifies an MBNF model.
The algorithm checks whether there exists a partition (P; N ) of MA() satisfying the
three conditions (a), (b), (c). Intuitively, the partition cannot be self-contradictory (condition (a)): in particular, the condition (P; N ) = Prt (; ob (P; N )) establishes that the
objective knowledge implied by the partition (P; N ) (that is, the formula ob (P; N )) identifies a set of interpretations M = fI : I j= ob (P; N )g such that (M; M ) induces the same
partition (P; N ) on MA(). Moreover, the partition must be consistent with  and :'
(condition (b)): such a condition implies that there exists an interpretation I such that
both  is satisfied in (I; M; M ) and ' is not satisfied in the structure (I; M; M ). Finally,
condition (c) corresponds to check whether such a structure (I; M; M ) identifies an MBNF
model for  according to Definition 2.1, i.e. whether there is no pair (J; M 0 ) such that
M 0  M and (J; M 0 ; M ) satisfies . Again, the search of such a structure is performed by
examining whether there exists a partition of MA(), different from (P; N ), which does not
satisfy any of the conditions (c1), (c2), (c3).
We illustrate the algorithm through the following simple example.

Example 4.13 Suppose
 = B (a _ Bb) ^ (not (c _ :d) _ B :not b) ^ c
288

fiReasoning about minimal belief and negation as failure

' = :Bb _ (:b ^ B (a ^ b))
Then, MA() = fB (a _ Bb); Bb; not (c _ :d); B :not b; not bg. Now suppose that (P; N ) =
(P1 ; N1 ), where

P1 = fB (a _ Bb); not (c _ :d); not bg
N1 = fBb; B :not bg
Then, (P; N ) = true ^ (true _ false) ^ c (which is equivalent to c), and ob (P; N ) = a _ false
(which is equivalent to a). Now, let M = fI : I j= ag: it is easy to see that (M; M )
satisfies the modal atoms in P , while it does not satisfy the modal atoms in N , hence
(P; N ) = Prt (; ob (P; N )), thus satisfying condition (a) of the algorithm. Then, since
a  a ^ b is not a valid propositional formula, M 6j= B (a ^ b), hence :'(ob (P; N )) =
:(true _ (:b ^ false)), which is equivalent to false. Therefore, (P; N ) ^ :'(ob (P; N )) is not
satisfiable, thus condition (b) does not hold.
Suppose now that (P; N ) = (P2 ; N2 ), where

P2 = fB (a _ Bb); not (c _ :d); Bb; B :not bg
N2 = fnot bg
Then, (P; N ) = true ^ (true _ true) ^ c (which is equivalent to c), and ob (P; N ) = (a _ true) ^
b^true, which is equivalent to b. Again, it is easy to see that (P; N ) = Prt (; ob (P; N )), thus
satisfying condition (a) of the algorithm. Then, since b  a ^ b is not a valid propositional
formula, :'(ob (P; N )) = :(false _ (:b ^ false)), which is equivalent to true. Hence, (P; N ) ^
:'(ob (P; N )) is equivalent to c, which implies that condition (b) holds. Finally, it is easy
to verify that either condition (c1) or condition (c2) holds for each partition of MA()
different from (P2 ; N2 ), with the exception of (P1 ; N1 ). So let (P 0 ; N 0 ) = (P1 ; N1 ): as shown
before, ob (P 0 ; N 0 ) is equivalent to a, hence ob (P; N ) ^ :ob (P 0 ; N 0 ) is equivalent to b ^ :a,
therefore condition (c3) holds for (P 0 ; N 0 ) = (P1 ; N1 ), which implies that condition (c)
holds for (P; N ) = (P2 ; N2 ). Consequently, MBNF-Not-Entails(; ') returns true. In fact,
the partition (P2 ; N2 ) identifies the set of MBNF models for  (I; M ) such that I is an
interpretation satisfying c and M = fI : I j= bg. Each such model does not satisfy the
query ': indeed, it can immediately be verified that, for each interpretation I , (I; M; M ) 6j=
:Bb _ (:b ^ B (a ^ b)), since M 6j= B (a ^ b) and M j= Bb.
To prove correctness of the algorithm MBNF-Not-Entails we need the following preliminary lemma.

Lemma 4.14 Let  2 LM , and let (P; N ) be the partition of MA() induced by (M 0; M ).
Let M 00 = fI : I j= ob (P; N )g. Then, (P; N ) is the partition induced by (M 00 ; M ).
The proof is by induction on the depth of the modal atoms of MA(). Let
not 2 MA() such that 2 L: then, (M 0 ; M ) j= not iff there exists an interpretation
I 2 M such that I 6j= , therefore (M 0 ; M ) j= not iff (M 00 ; M ) j= not . Now let B 2
MA() such that 2 L: by Definition 4.3, (M 0 ; M ) j= B iff the propositional formula
Proof.

289

fiRosati

ob (P; N )  is valid, and since M 00 = fI : I j= ob (P; N )g, it follows that (M 0 ; M ) j= B
iff (M 00 ; M ) j= B .
Now suppose that, for each modal atom  of depth i, (M 0 ; M ) j=  iff (M 00 ; M ) j=  ,
and let (P 0 ; N 0 ) denote the partition of the modal atoms in MA() of depth less or equal
to i induced by (M 0 ; M ). First, consider a modal atom not of depth i + 1. Then,
by Lemma 4.6, (M 0 ; M ) j= not iff (M 0 ; M ) j= not ( (P 0 ; N 0 )) and, by the inductive
hypothesis and Lemma 4.6, (M 00 ; M ) j= not iff (M 00 ; M ) j= not ( (P 0 ; N 0 )). Then, since
has depth i, (P 0 ; N 0 ) is a propositional formula, hence (M 0 ; M ) j= not ( (P 0 ; N 0 )) iff
there exists an interpretation I 2 M such that I 6j= (P 0 ; N 0 ), which immediately implies
that (M 0 ; M ) j= not iff (M 00 ; M ) j= not . Now consider a modal atom B of depth
i + 1. Then, by Lemma 4.6, (M 0 ; M ) j= B iff (M 0; M ) j= B ( (P 0 ; N 0 )) and, by the
inductive hypothesis and Lemma 4.6, (M 00 ; M ) j= B iff (M 00 ; M ) j= B ( (P 0 ; N 0 )). By
Definition 4.3, (M 0 ; M ) j= B iff the propositional formula ob (P; N )  (P 0 ; N 0 ) is valid,
and since M 00 = fI : I j= ob (P; N )g, it follows that (M 0 ; M ) j= B iff (M 00 ; M ) j= B ,
which proves the thesis.
We are now ready to prove correctness of the algorithm MBNF-Not-Entails.

Theorem 4.15 Let  2 LM , ' 2 LB . Then, MBNF-Not-Entails(; ') returns true iff
 6j=MBNF '.
If part. Suppose  6j=MBNF '. Then, there exists a pair (I; M ) such that (I; M ) is
an MBNF model for  and (I; M; M ) 6j= '. Let (P; N ) be the partition of MA() induced
by (M; M ). By Theorem 4.7, M = fI : I j= ob (P; N )g. Therefore, by Definition 4.8,
(P; N ) = Prt (; ob (P; N )). Then, since (I; M; M ) 6j= ', by Theorem 4.10 it follows that
I 6j= '(ob (P; N )), and since (I; M; M ) j= , by Lemma 4.6 I j= (P; N ), therefore I j=
(P; N ) ^:'(ob (P; N )). Now suppose there exists a partition (P 0 ; N 0) of MA() such that
(P 0 ; N 0 ) =
6 (P; N ) and none of conditions (c1), (c2), and (c3) holds. Then, since (P 0 ; N 0 )
is satisfiable, there exists an interpretation J such that J j= (P 0 ; N 0 ), and since (P 0 ; N 0 ) =
Prt (; ob (P 0 ; N 0 ); ob (P; N )), from Lemma 4.6 it follows that there exists an interpretation
J such that (J; M 0 ; M ) j= , where M 0 = fI : I j= ob (P 0 ; N 0)g. Then, since condition (c3)
does not hold, the propositional formula ob (P; N )  ob (P 0 ; N 0 ) is valid, which implies that
M 0  M . Now, if M 0 = M , then (P 0 ; N 0 ) would be the partition induced by (M; M ), thus
contradicting the hypothesis (P 0 ; N 0 ) =
6 (P; N ). Hence, M 0  M , and since (J; M 0 ; M ) j= ,
Proof.

it follows that (I; M ) is not an MBNF model for . Contradiction. Consequently, condition
(c) in the algorithm holds, therefore MBNF-Not-Entails(; ') returns true.
Only-if part. Suppose MBNF-Not-Entails(; ') returns true. Then, there exists a partition (P; N ) of MA() such that conditions (a), (b), and (c) hold. Let M = fI : I j=
ob (P; N )g. Since (P; N ) = Prt (; ob (P; N )), by Definition 4.8 (P; N ) is the partition induced by (M; M ). And since (P; N )^:'(ob (P; N )) is satisfiable, it follows that there exists
an interpretation I such that I j= (P; N ) and I 6j= '(ob (P; N )), hence, by Lemma 4.6,
(I; M; M ) j=  and (I; M; M ) 6j= '. Now suppose (I; M ) is not an MBNF model for .
Then, there exists a set M 0 and an interpretation J such that M 0  M and (J; M 0 ; M ) j= .
Let (P 0 ; N 0 ) be the partition of MA() induced by (M 0 ; M ). Since M = fI : I j= ob (P; N )g,
it follows that M 0 contains
V at least one interpretation J which does not satisfy ob (P; N ),
and since ob (P; N ) = B 2P (P; N ), J does not satisfy at least one formula of the form
290

fiReasoning about minimal belief and negation as failure

(P; N ) such that B 2 P . Therefore, P 0 6= P , which implies that (P 0 ; N 0 ) 6= (P; N ).
Then, since (J; M 0 ; M ) j= , by Lemma 4.6 J j= (P 0 ; N 0 ), hence (P 0 ; N 0 ) is satisfiable.
Now let M 00 = fI : I j= ob (P 0 ; N 0 )g. By Lemma 4.14, it follows that (P 0 ; N 0 ) is the partition
induced by (M 00 ; M ), therefore, by Definition 4.8, (P 0 ; N 0 ) = Prt (; ob (P 0 ; N 0 ); ob (P; N )).
Moreover, since M 0  M , it follows that the propositional formula ob (P; N )  ob (P 0 ; N 0 )
is valid, hence the formula ob (P; N ) ^ :ob (P 0 ; N 0 ) is unsatisfiable. Consequently, (P 0 ; N 0 )
does not satisfy condition (c) in the algorithm, thus contradicting the hypothesis. Therefore, (I; M ) is an MBNF model for , and since (I; M; M ) 6j= ', it follows that  6j=MBNF ',
thus proving the thesis.
We point out the fact that the algorithm MBNF-Not-Entails does not rely on a theorem
prover for a modal logic: thus, \modal reasoning" is not actually needed for reasoning in
MBNF. This is an interesting peculiarity that MBNF shares with other nonmonotonic
modal formalisms, like autoepistemic logic (Moore, 1985) or the autoepistemic logic of
knowledge (Schwarz, 1991).

4.3 Reasoning in Flat MBNF

We now study reasoning in at MBNF theories. The main reason for taking into account the
at fragment of MBNF is the fact that reasoning in many of the best known nonmonotonic
formalisms like default logic, circumscription, and logic programming, can be reduced to
reasoning in at MBNF theories (Lifschitz, 1994).
It is known that, if  2 L1M and ' 2 LSB , then it is possible to reduce the entailment
 j=MBNF ' to reasoning in logic S4FMDD , by translating MBNF formulas into unimodal
formulas of S4FMDD (Schwarz & Truszczynski, 1994). Thus, the procedure for deciding
entailment in the logic S4FMDD presented by Marek and Truszczynski (1993) can be employed for computing the entailment  j=MBNF '. In the following we study a more general
problem, that is entailment  j=MBNF ' in the case  2 L1M and ' 2 LB , and present a
specialized algorithm for this problem, which is simpler than the more general reasoning
method for S4FMDD .
In Figure 2 we report the algorithm Flat-Not-Entails for computing such an entailment.
In the algorithm, Pn denotes the subset of modal atoms from P prefixed by the modality
not , i.e. Pn = fnot : not 2 P g.
Informally, correctness of the algorithm Flat-Not-Entails is established by the fact that,
if  2 L1M , then (a), (b), and (c) are necessary and sucient conditions on a partition (P; N )
in order to establish whether it is induced by a pair (M; M ) such that there exists an MBNF
model for  of the form (I; M ). In particular, condition (c) states that B (ob (P; N )) must
be a consequence of (Pn ; N ) in modal logic S5,1 since it can be shown that if (Pn ; N ) 
B (ob (P; N )) is not valid in S5, then the guess on the modal atoms of the form B' in P is
not minimal. We illustrate this fact through the following example.

Example 4.16 Let
 = (Ba ^ not (c _ d)) _ (B (a ^ b) ^ :Bc) _ Bc
1. We denote as B the modal operator used in S5.

291

fiRosati

Algorithm Flat-Not-Entails(; ')
Input: formula  2 L1M , formula ' 2 LB ;
Output: true if  6j=MBNF ', false otherwise.
begin
if there exists partition (P; N ) of MA()
such that
(a) (P; N ) = Prt (; ob (P; N )) and
(b) (P; N ) ^ :'(ob (P; N )) is satisfiable and
(c) (Pn ; N )  B (ob (P; N )) is valid in S5
then return true
else return false
end
Figure 2: Algorithm Flat-Not-Entails.
and suppose

P = fBa; B (a ^ b); not (c _ d)g
N = fBcg
Then,

(Pn ; N ) = (Ba ^ true) _ (B (a ^ b) ^ :false) _ false;
which is propositionally equivalent to Ba _ B (a ^ b), and ob (P; N ) = a ^ (a ^ b), which is
equivalent to a^b. Now, Ba_B (a^b)  B (a^b) is not valid in S5, which is proved by the fact
that the set of interpretations M 0 = fI : I j= ag is such that M 0 j= (Ba_B (a^b))^:B (a^b).
Indeed, the set of interpretations M 0 can be immediately used in order to prove that (P; N )
does not identify any MBNF model for . In fact, let M = fJ : J j= a ^ bg: it is immediate
to see that, for each interpretation I , (I; M 0 ; M ) j= , and since M 0  M , (I; M ) is not an
MBNF model for .
Finally, condition (b) corresponds to check whether there exists an interpretation I
satisfying :'(ob (P; N )): in fact, if such an interpretation exists, then (I; M ) is an MBNF
model for  which does not satisfy '.
Therefore, the algorithms MBNF-Not-Entails and Flat-Not-Entails only differ in the
way in which it is verified whether the MBNF structure associated with a partition (P; N )
satisfies the preference semantics provided by Definition 2.1, which is implemented through
condition (c) in both algorithms. In the algorithm MBNF-Not-Entails, a partition is checked
against all other partitions of MA(), while in the algorithm Flat-Not-Entails it is sucient
to verify that the partition (P; N ) satisfies a \local" property. As shown in the next section,
such a difference reects the different computational properties of the entailment problem
in the two cases.
In order to establish correctness of the algorithm, we need a preliminary lemma.
292

fiReasoning about minimal belief and negation as failure

Lemma 4.17 Let  2 L1M and let (P; N ) be the partition induced by a structure (M; M ).
Then, (I; M ) is an MBNF model for  iff for each M 0  M the positive formula (Pn ; N )
is not satisfied by M 0 .
Suppose (I; M ) is an MBNF model for , and let (P; N ) be the partition induced
by (M; M ). Let M 0 be any set of interpretations such that M 0  M . Then, (M 0 ; M ) 6j= .
Since  2 L1M and M 0  M , this implies that for each modal atom  in N , (M 0 ; M ) 6j=  .
Moreover, for each modal atom not 2 P , (M 0 ; M ) j= not . Therefore, by Lemma 4.6,
(M 0 ; M ) 6j= (Pn ; N ). Now, since  2 L1M , (Pn ; N ) is a at positive formula, hence its
satisfiability only depends on the structure M 0 , therefore M 0 6j= (Pn ; N ).
Conversely, suppose (I; M ) is not an MBNF model for , and let (P; N ) be the partition
induced by (M; M ). Then, there exists a set of interpretations M 0 such that M 0  M and
(M 0 ; M ) j= . As shown before, this implies that the positive formula (Pn ; N ) is satisfied
by M 0 .
As observed in Section 2, the class of universal Kripke structures characterizes modal
logic S5. This immediately implies the following property.
Proof.

Lemma 4.18 A formula ' 2 LSB is valid in S5 iff, for each set of interpretations M , the
formula :' is not satisfied by M .
Based on the above property, we are now able to prove correctness of the algorithm
Flat-Not-Entails.

Theorem 4.19 Let  2 L1M and ' 2 LB . Then, Flat-Not-Entails(; ') returns true iff
 6j=MBNF '.
If-part. If  6j=MBNF ', then there exists an MBNF model (I; M ) for  such
that (I; M; M ) 6j= '. Let (P; N ) be the partition of MA() induced by (M; M ). From
Theorem 4.7 it follows that M = fJ : J j= ob (P; N )g. Therefore, by Definition 4.8,
(P; N ) = Prt (; ob (P; N )), hence condition (a) in the algorithm holds.
Now let 0 = (Pn ; N ), and suppose the formula 0  B (ob (P; N )) is not valid in S5.
Then, since the formula 0  B (ob (P; N )) belongs to LSB , by Lemma 4.18 it follows that
there exists a set of interpretations M 0 satisfying 0 ^ :B (ob (P; N )). Let (P 0 ; N 0 ) be the
partition of VMA(0 ) induced by (M 0 ; M 0 ), and let M 00 = fI : I j= ob (P 0 ; N 0 )g. Since
ob (P; N ) = B'2MA( ) ', by Definition 4.3 it follows that ob (P; N )  ob (P 0 ; N 0 ) is a valid
propositional formula, hence M 00  M . Now, since by hypothesis M 0 j= :B (ob (P; N )), it
follows that M 00  M . Moreover, since 0 2 LB , by Lemma 4.14 it follows that (P 0 ; N 0 ) is
the partition induced by (M 00 ; M 00 ), and since M 0 j= 0 and 0 is at, 0 (P 0 ; N 0 ) is equivalent
to true, therefore M 00 j= 0 (P 0 ; N 0 ) and, by Lemma 4.6, M 00 j= 0 . On the other hand, since
M 00  M , by Lemma 4.17 it follows that M 00 6j= 0. Contradiction. Hence, 0  B (ob (P; N ))
Proof.

0

is valid in S5, consequently condition (c) of the algorithm holds.
Finally, since (I; M; M ) 6j= ' and M = fJ : J j= ob (P; N )g, by Theorem 4.10 it follows
that I 6j= '(ob (P; N )). Moreover, since (I; M; M ) j= , from Lemma 4.6 it follows that
I j= (P; N ), consequently I j= (P; N ) ^ :'(ob (P; N )), hence the propositional formula
(P; N )^:'(ob (P; N )) is satisfiable. Therefore, conditions (a), (b), and (c) in the algorithm
hold, which implies that Flat-Not-Entails(; ') returns true.
293

fiRosati

Only-if-part. If Flat-Not-Entails(; ') returns true, then there exists a partition (P; N )
of MA() for which conditions (a), (b), and (c) of the algorithm hold. Let M = fJ : J j=
ob (P; N )g. By Definition 4.8, (P; N ) is the partition of MA() induced by (M; M ). Now,
since (P; N ) ^ :'(ob (P; N )) is satisfiable, there exists an interpretation I such that I j=
(P; N ) and I j= :'(ob (P; N )), hence by Lemma 4.6 (I; M; M ) j= , and by Theorem 4.10
(I; M; M ) 6j= ', therefore we only have to show that (I; M ) is an MBNF model for . So,
let us suppose (I; M ) is not an MBNF model for . Then, by Lemma 4.17 there exists
M 0  M such that (Pn ; N ) is satisfied in M 0 . Now, condition (c) in the algorithm implies
that B (ob (P; N )) is a consequence of (Pn ; N ) in S5, therefore ob (P; N ) is satisfied by each
interpretation in M 0 , that is, M 0  fJ : J j= ob (P; N )g, which contradicts the hypothesis
M 0  M = fJ : J j= ob (P; N )g. Consequently, (I; M ) is an MBNF model for .

We remark the fact that the algorithm Flat-Not-Entails can be seen as a generalization
of known methods for query answering in Reiter's default logic, Moore's autoepistemic logic,
and (disjunctive) logic programming under the stable model (and answer set) semantics. In
particular, condition (c) in the algorithm can be seen as a generalization of the minimality
check used in (disjunctive) logic programming for verifying stability of a model of a logic
program (Gelfond & Lifschitz, 1990, 1991).

5. Complexity Results
In this section we provide a computational characterization of reasoning in MBNF.
We first briey recall the complexity classes in the polynomial hierarchy, and refer
to (Johnson, 1990; Papadimitriou, 1994) for further details about the complexity classes
mentioned in the paper. PA (NPA ) is the class of problems that are solved in polynomial
time by deterministic (nondeterministic) Turing machines using an oracle for A (i.e. that
solves in constant time any problem in A). The classes pk , pk and pk pof the polynomial
hierarchy are defined by p0 = p0 = p0 = P, and for k  0, pk+1 = NPk , pk+1 = copk+1
p
and pk+1 = Pk . In particular, the complexity class p2 is the class of problems that are
solved in polynomial time by a nondeterministic Turing machine that uses an NP-oracle,
and p2 is the class of problems that are complement of a problem in p2 , while p3 is the
class of problems that are solved in polynomial time by a nondeterministic Turing machine
that uses an p2 -oracle, and p3 is the class of problems that are complement of a problem
in p3 . It is generally assumed that the polynomial hierarchy does not collapse: hence,
a problem in the class p2 or p2 is considered computationally easier than a p3 -hard or
p3 -hard problem.
As for the complexity of entailment in MBNF, we start by establishing a lower bound
for reasoning in propositional MBNF theories. To this end, we exploit the correspondence
between MBNF and the logic of minimal knowledge S5G (Halpern & Moses, 1985). Indeed,
as stated by Proposition 2.5, there is a one-to-one correspondence between MBNF models
and S5G models of positive subjective theories.

Lemma
5.1 Let  2 LSM and let ' 2 LB . Then, the problem of deciding whether  j=MBNF
p
' is 3-hard.

294

fiReasoning about minimal belief and negation as failure

As shown by (Donini et al., 1997b), entailment in S5G is p3 -complete. Therefore,
by Proposition 2.5, for subjective (and hence for general) MBNF theories, entailment is
p3 -hard.
Then, we show that the entailment problem in propositional MBNF is complete with
respect to the class p3 .
Proof.

Theorem 5.2 Let
 2 LM and let ' 2 LB . Then, the problem of deciding whether
 j=MBNF ' is p3 -complete.

Hardness with respect to p3 follows from Lemma 5.1. As for membership in p3 ,
we analyze the complexity of the algorithm MBNF-Not-Entails reported in Figure 1. In
particular, observe that:
 given (P; N ), the formula ob (P; N ) can be computed in polynomial time with respect
to the size of P . Moreover, by Lemma 4.9 it follows that, since MA() has size linear
with respect to the size of , construction of the partition Prt (; ob (P; N )) can be
performed through a linear number (with respect to the size of ) of calls to an NPoracle for propositional satisfiability. Therefore, condition (a) can be checked through
a linear number (in the size of the input) of calls to an NP-oracle;
 since '(ob (P; N )) = '(Prt ('; ob (P; N ))), the formula :'(ob (P; N )) can be computed
in time linear with respect to the size of ' ^ ob (P; N ) using an NP-oracle. And since,
given  and (P; N ), (P; N ) can be computed in polynomial time with respect to
the size of the input, it follows that condition (b) can be computed through a linear
number (in the size of the input) of calls to an NP-oracle;
 given a partition (P 0 ; N 0), each of the conditions (c1), (c2) and (c3) (analogous to
conditions (a) and (b)) can be checked in polynomial time, with respect to the size of
, using an NP-oracle. Therefore, since the guess of the partition (P 0 ; N 0 ) ofpMA()
requires a nondeterministic choice, falsity of condition (c) can be decided in 2 , which
implies that verifying whether condition (c) holds can be decided in p2 .
Since the guess of the partition (P; N ) of MA() requires a nondeterministic choice, it
follows that the algorithm MBNF-Not-Entails, if considered as a nondeterministic procedure, decides  6j=MBNF ' in nondeterministic polynomial time (with respect to the size of
 ^ '), using a p2-oracle. Thus, we obtain an upper bound of p3 for the non-entailment
problem, which implies that entailment in MBNF is in p3 .
The previous analysis also allows for a computational characterization of the logic MKNF
(Lifschitz, 1991), which is a slight modification of MBNF. Indeed, it is known (Lifschitz,
1994) that, for each theory   LM , M is an MKNF model of  iff, for each interpretation
I , (I; M ) is an MBNF model of the subjective theory 0 = fB' : ' 2 g. Therefore, from
Proposition 2.5 and from p3 -hardness of entailment in S5G (Donini et al., 1997b), it follows
that entailment in MKNF is p3 -hard. Then, since  j=MKNF ' iff 0 j=MBNF B' (Lifschitz,
1994), it follows that entailment in MKNF can be polynomially reduced to entailment in
MBNF, hence such a problem belongs to p3 . Therefore, the following property holds.
Proof.

Theorem 5.3 Entailment in propositional MKNF is p3-complete.
295

fiRosati

Finally, the previous theorem provides a computational characterization of the logic of
grounded knowledge and justified assumptions GK (Lin & Shoham, 1992). In fact, the
logic GK can be considered as a syntactic variant of the propositional fragment of MKNF.
Therefore, skeptical entailment in GK is p3 -complete.

Remark. The computational properties of MBNF and its variants relate such formalisms

to ground nonmonotonic modal logics (Eiter & Gottlob, 1992; Donini et al., 1997b; Rosati,
1999). Notably, ground nonmonotonic modal logics share with MBNF the interpretation in
terms of minimal knowledge (or minimal belief) of the modality B ; specifically, as already
mentioned, the propositional fragment of MBNF can be considered as built upon S5G by
adding a second modality not . Therefore, it turns out that, in the propositional case, adding
a \negation by default" modality to the S5 logic of minimal knowledge does not increase
the computational complexity of reasoning, while adding a minimal knowledge modality to
AEL does increase the complexity of deduction. We can thus summarize as follows: minimal
knowledge is computationally harder than negation as failure.
We now study the complexity of entailment for at MBNF theories. First, it is known
that, in the case of at MBNF theories and subjective queries, entailment is p2 -complete:
membership in the class p2 is a consequence of the fact that at MKNF theories can
be polynomially embedded into McDermott and Doyle's nonmonotonic modal logic S4F
(Schwarz & Truszczynski, 1994, Proposition 3.2), whose entailment problem is p2 -complete
(Marek & Truszczynski, 1993), while p2 -hardness follows from the existence of a polynomialtime embedding of propositional default theories into at MBNF theories (Lifschitz, 1994).
Therefore, the following property holds.

Proposition 5.4p Let  2 L1M and let ' 2 LSB . Then, the problem of deciding whether
 j=MBNF ' is 2 -complete.
As for complexity of entailment of generic queries with respect to at MBNF theories,
we analyze the complexity of the algorithm Flat-Not-Entails reported in Figure 2. As shown
before, both condition (a) and condition (b) can be checked through a linear number (with
respect to the size of the input) of calls to an NP-oracle. Moreover, validity in modal logic
S5 is a coNP-complete problem (Halpern & Moses, 1992). Hence, each of the conditions in
the algorithm can be computed through a number of calls to an oracle for the propositional
validity problem which is polynomial in the size of the input, and since the guess of the
partition (P; N ) of MA() requires a nondeterministic choice, it follows that the algorithm
runs in p2 . Therefore, the following property holds.

Theorem 5.5 Let
 2 L1M and let ' 2 LB . Then, the problem of deciding whether
p
 j=MBNF ' is 2 -complete.
Membership of the problem to the class p2 is implied by the algorithm Flat-notentails, whereas p2 -hardness is implied by Proposition 5.4.
Hence, the algorithm Flat-Not-Entails is \optimal" in the sense that it matches the
lower bound for the entailment problem.

Proof.

296

fiReasoning about minimal belief and negation as failure

Finally, we remark that the subset of at MBNF theories in conjunctive normal form
can be seen as a further extension of the framework of generalized logic programming introduced by Inoue and Sakama (1994), which in turn is an extension of the disjunctive logic
programming framework under the stable model semantics (Gelfond & Lifschitz, 1991).
Roughly speaking, at MBNF theories in conjunctive normal form correspond to rules of
generalized logic programs in which propositional formulas (instead of literals) are allowed
as goals. The above computational characterization implies that such an extension of the
framework of logic programming under the stable model semantics does not affect the worstcase complexity of the entailment problem, which is p2 -complete just like entailment in logic
programs with disjunction under the stable model semantics (Eiter & Gottlob, 1995). Such
a result extends analogous properties (Marek, Truszczynski, & Rajasekar, 1995) to the case
of disjunctive logic programs.

6. Conclusions
In this paper we have investigated the problem of reasoning in the propositional fragment
of MBNF. The main results presented can be summarized as follows:

 the negation as failure modality not of MBNF exactly corresponds to negative introspection in AEL. This implies that the logic MBNF can be viewed as the conservative
extension of two different nonmonotonic modal logics: Halpern and Moses' logic of
minimal knowledge S5G and Moore's AEL;

 reasoning in the propositional fragment of MBNF lies at the third level of the polyno-

mial hierarchy, hence (unless the polynomial hierarchy does not collapse) reasoning in
MBNF is harder than reasoning in the best known propositional nonmonotonic logics,
like default logic, autoepistemic logic, and circumscription;

 we have defined methods for reasoning in MBNF, which subsume and generalize wellknown nonmonotonic reasoning algorithms used in logic programming (Gelfond &
Lifschitz, 1991), default logic (Gottlob, 1992), and autoepistemic logic (Marek &
Truszczynski, 1993);

 we have studied the at fragment of MBNF and its relationship with the logic programming paradigm.

As for the computational aspects of reasoning in MBNF, the results presented in Section 5 prove that one source of complexity is due to the presence of nested occurrences of
modalities in the theory, since reasoning in at MBNF is computationally easier than in
the general case.
It can be proven that another source of complexity lies in the underlying objective
language. In fact, if we consider L0 to be a tractable fragment of propositional logic, then
the complexity of reasoning in the modal language L0M built upon L0 is lower than in the
general case. In particular, it is easy to see that, under the assumption that entailment
in L0 can be computed in polynomial time, the algorithm MBNF-Not-Entails provides an
upper bound of p2 for MBNF-entailment in the fragment L0M .
297

fiRosati

One possible development of the present work is towards the analysis of reasoning about
minimal belief and negation as failure in a first-order setting: in particular, it should be
interesting to see whether it is possible to extend the techniques developed for the propositional case to a more expressive language. A first attempt in this direction is reported by
Donini et al. (1997a).

Acknowledgments

This research has been partially supported by Consiglio Nazionale delle Ricerche, grant
203.15.10.

References

Beringer, A., & Schaub, T. (1993). Minimal belief and negation as failure: a feasible
approach. In Proc. of the 11th Nat. Conf. on Artificial Intelligence (AAAI'93), pp.
400{405.
Bochman, A. (1995). On bimodal nonmonotonic modal logics and their unimodal and
nonmodal equivalents. In Proc. of the 14th Int. Joint Conf. on Artificial Intelligence
(IJCAI'95), pp. 1518{1524.
Chen, J. (1994). The logic of only knowing as a unified framework for non-monotonic
reasoning. Fundamenta Informaticae, 21, 205{220.
Donini, F. M., Nardi, D., & Rosati, R. (1997a). Autoepistemic description logics. In Proc.
of the 15th Int. Joint Conf. on Artificial Intelligence (IJCAI'97), pp. 136{141.
Donini, F. M., Nardi, D., & Rosati, R. (1997b). Ground nonmonotonic modal logics. J. of
Logic and Computation, 7 (4), 523{548.
Eiter, T., & Gottlob, G. (1992). Reasoning with parsimonious and moderately grounded
expansions. Fundamenta Informaticae, 17 (1,2), 31{54.
Eiter, T., & Gottlob, G. (1993). Propositional circumscription and extended closed world
reasoning are p2 -complete. Theoretical Computer Science, 114, 231{245.
Eiter, T., & Gottlob, G. (1995). On the computational cost of disjunctive logic programming: propositional case. Annals of Mathematics and Artificial Intelligence, 15 (3,4).
Gelfond, M., & Lifschitz, V. (1988). The stable model semantics for logic programming.
In Proceedings of the Fifth Logic Programming Symposium, pp. 1070{1080. The MIT
Press.
Gelfond, M., & Lifschitz, V. (1990). Logic programs with classical negation. In Proceedings
of the Seventh International Conference on Logic Programming, pp. 579{597. The
MIT Press.
Gelfond, M., & Lifschitz, V. (1991). Classical negation in logic programs and disjunctive
databases. New Generation Computing, 9, 365{385.
298

fiReasoning about minimal belief and negation as failure

Gottlob, G. (1992). Complexity results for nonmonotonic logics. J. of Logic and Computation, 2, 397{425.
Gottlob, G. (1995). NP trees and Carnap's modal logic. J. of the ACM, 42 (2), 421{457.
Halpern, J. Y., & Moses, Y. (1985). Towards a theory of knowledge and ignorance: Preliminary report. In Apt, K. (Ed.), Logic and models of concurrent systems. SpringerVerlag.
Halpern, J. Y., & Moses, Y. (1992). A guide to completeness and complexity for modal
logics of knowledge and belief. Artificial Intelligence, 54, 319{379.
Inoue, K., & Sakama, C. (1994). On positive occurrences of negation as failure. In Proc.
of the 4th Int. Conf. on the Principles of Knowledge Representation and Reasoning
(KR'94), pp. 293{304. Morgan Kaufmann, Los Altos.
Janhunen, T. (1998). On the intertranslatability of autoepistemic, default and priority
logics. In Proc. of the 6th European Workshop on Logics in Artificial Intelligence
(JELIA'98), pp. 216{232.
Johnson, D. S. (1990). A catalog of complexity classes. In van Leuven, J. (Ed.), Handbook
of Theoretical Computer Science, Vol. A, chap. 2. Elsevier Science Publishers (NorthHolland), Amsterdam.
Kaminski, M. (1991). Embedding a default system into nonmonotonic logics. Fundamenta
Informaticae, 14, 345{354.
Levesque, H. J. (1990). All I know: a study in autoepistemic logic. Artificial Intelligence,
42, 263{310.
Lifschitz, V., & Woo, T. (1992). Answer sets in general nonmonotonic reasoning (preliminary report). In Proc. of the 3rd Int. Conf. on the Principles of Knowledge Representation and Reasoning (KR'92), pp. 603{614. Morgan Kaufmann, Los Altos.
Lifschitz, V. (1991). Nonmonotonic databases and epistemic queries. In Proc. of the 12th
Int. Joint Conf. on Artificial Intelligence (IJCAI'91), pp. 381{386.
Lifschitz, V. (1994). Minimal belief and negation as failure. Artificial Intelligence, 70,
53{72.
Lin, F., & Shoham, Y. (1992). Epistemic semantics for fixed-point non-monotonic logics.
Artificial Intelligence, 57, 271{289.
Marek, W., & Truszczynski, M. (1993). Nonmonotonic Logics { Context-Dependent Reasoning. Springer-Verlag.
Marek, W., Truszczynski, M., & Rajasekar, A. (1995). Complexity of computing with
extended propositional logic programs. Annals of Mathematics and Artificial intelligence, 15 (3,4).
299

fiRosati

Moore, R. C. (1985). Semantical considerations on nonmonotonic logic. Artificial Intelligence, 25, 75{94.
Niemela, I. (1992). On the decidability and complexity of autoepistemic reasoning. Fundamenta Informaticae, 17 (1,2), 117{156.
Papadimitriou, C. H. (1994). Computational Complexity. Addison Wesley Publ. Co., Reading, Massachussetts.
Reiter, R. (1990). What should a database know?. J. of Logic Programming, 14, 127{153.
Rosati, R. (1997). Reasoning with minimal belief and negation as failure: Algorithms and
complexity. In Proc. of the 14th Nat. Conf. on Artificial Intelligence (AAAI'97), pp.
430{435. AAAI Press/The MIT Press.
Rosati, R. (1999). Reasoning about minimal knowledge in nonmonotonic modal logics. J.
of Logic, Language and Information, 8 (2), 187{203.
Schwarz, G. (1991). Autoepistemic logic of knowledge. In Proc. of the 1st Int. Workshop on
Logic Programming and Non-monotonic Reasoning (LPNMR'91), pp. 260{274. The
MIT Press.
Schwarz, G. (1992). Minimal model semantics for nonmonotonic modal logics. In Proc.
of the 7th IEEE Sym. on Logic in Computer Science (LICS'92), pp. 34{43. IEEE
Computer Society Press.
Schwarz, G. (1996). On embedding default logic into Moore's autoepistemic logic. Artificial
Intelligence, 80, 388{392.
Schwarz, G., & Lifschitz, V. (1993). Extended logic programs as autoepistemic theories. In
Proc. of the 2nd Int. Workshop on Logic Programming and Non-monotonic Reasoning
(LPNMR'93), pp. 101{114. The MIT Press.
Schwarz, G., & Truszczynski, M. (1994). Minimal knowledge problem: a new approach.
Artificial Intelligence, 67, 113{141.
Shoham, Y. (1987). Nonmonotonic logics: Meaning and utility. In Proc. of the 10th Int.
Joint Conf. on Artificial Intelligence (IJCAI'87), pp. 388{392.
Stalnaker, R. (1993). A note on non-monotonic modal logic. Artificial Intelligence, 64 (2),
183{196.

300

fiJournal of Artificial Intelligence Research 11 (1999) 1{94

Submitted 09/98; published 07/99

Decision-Theoretic Planning: Structural Assumptions and
Computational Leverage
Craig Boutilier

cebly@cs.ubc.ca

Department of Computer Science, University of British Columbia
Vancouver, BC, V6T 1Z4, Canada

Thomas Dean

tld@cs.brown.edu

Department of Computer Science, Brown University
Box 1910, Providence, RI, 02912, USA

Steve Hanks

hanks@cs.washington.edu

Department of Computer Science and Engineering, University of Washington
Seattle, WA, 98195, USA

Abstract

Planning under uncertainty is a central problem in the study of automated sequential
decision making, and has been addressed by researchers in many different fields, including
AI planning, decision analysis, operations research, control theory and economics. While
the assumptions and perspectives adopted in these areas often differ in substantial ways,
many planning problems of interest to researchers in these fields can be modeled as Markov
decision processes (MDPs) and analyzed using the techniques of decision theory.
This paper presents an overview and synthesis of MDP-related methods, showing how
they provide a unifying framework for modeling many classes of planning problems studied
in AI. It also describes structural properties of MDPs that, when exhibited by particular classes of problems, can be exploited in the construction of optimal or approximately
optimal policies or plans. Planning problems commonly possess structure in the reward
and value functions used to describe performance criteria, in the functions used to describe
state transitions and observations, and in the relationships among features used to describe
states, actions, rewards, and observations.
Specialized representations, and algorithms employing these representations, can achieve
computational leverage by exploiting these various forms of structure. Certain AI techniques|
in particular those based on the use of structured, intensional representations|can be
viewed in this way. This paper surveys several types of representations for both classical
and decision-theoretic planning problems, and planning algorithms that exploit these representations in a number of different ways to ease the computational burden of constructing
policies or plans. It focuses primarily on abstraction, aggregation and decomposition techniques based on AI-style representations.

1. Introduction

Planning using decision-theoretic notions to represent domain uncertainty and plan quality
has recently drawn considerable attention in artificial intelligence (AI).1 Decision-theoretic
planning (DTP) is an attractive extension of the classical AI planning paradigm because it
allows one to model problems in which actions have uncertain effects, the decision maker has
1. See, for example, the recent texts (Dean, Allen, & Aloimonos, 1995; Dean & Wellman, 1991; Russell &
Norvig, 1995) and the research reported in (Hanks, Russell, & Wellman, 1994).

c 1999 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiBoutilier, Dean, & Hanks

incomplete information about the world, where factors such as resource consumption lead to
solutions of varying quality, and where there may not be an absolute or well-defined \goal"
state. Roughly, the aim of DTP is to form courses of action (plans or policies) that have
high expected utility rather than plans that are guaranteed to achieve certain goals. When
AI planning is viewed as a particular approach to solving sequential decision problems of
this type, the connections between DTP and models used in other fields of research|such
as decision analysis, economics and operations research (OR)|become more apparent. At
a conceptual level, most sequential decision problems can be viewed as instances of Markov
decision processes (MDPs), and we will use the MDP framework to make the connections
explicit.
Much recent research on DTP has explicitly adopted the MDP framework as an underlying model (Barto, Bradtke, & Singh, 1995; Boutilier & Dearden, 1994; Boutilier, Dearden,
& Goldszmidt, 1995; Dean, Kaelbling, Kirman, & Nicholson, 1993; Koenig, 1991; Simmons
& Koenig, 1995; Tash & Russell, 1994), allowing the adaptation of existing results and algorithms for solving MDPs (e.g., from the field of OR) to be applied to planning problems. In
doing so, however, this work has departed from the traditional definition of the \planning
problem" in the AI planning community|one goal of this paper is to make explicit the
connection between these two lines of work.
Adopting the MDP framework as a model for posing and solving planning problems
has illuminated a number of interesting connections among techniques for solving decision
problems, drawing on work from AI planning, reasoning under uncertainty, decision analysis
and OR. One of the most interesting insights to emerge from this body of work is that many
DTP problems exhibit considerable structure, and thus can be solved using special-purpose
methods that recognize and exploit that structure. In particular, the use of feature-based
representations to describe problems, as is the typical practice in AI, often highlights the
problem's special structure and allows it to be exploited computationally with little effort.
There are two general impediments to the more widespread acceptance of MDPs within
AI as a general model of planning. The first is the absence of explanations of the MDP model
that make the connections to current planning research explicit, at either the conceptual
or computational level. This may be due in large part to the fact that MDPs have been
developed and studied primarily in OR, where the dominant concerns are, naturally, rather
different. One aim of this paper is to make the connections clear: we provide a brief
description of MDPs as a conceptual model for planning that emphasizes the connection
to AI planning, and explore the relationship between MDP solution algorithms and AI
planning algorithms. In particular, we emphasize that most AI planning models can be
viewed as special cases of MDPs, and that classical planning algorithms have been designed
to exploit the problem characteristics associated with these cases.
The second impediment is skepticism among AI researchers regarding the computational
adequacy of MDPs as a planning model: can the techniques scale to solve planning problems
of reasonable size? One diculty with solution techniques for MDPs is the tendency to rely
on explicit, state-based problem formulations. This can be problematic in AI planning
since state spaces grow exponentially with the number of problem features. State space size
and dimensionality are of somewhat lesser concern in OR and decision analysis. In these
fields, an operations researcher or decision analyst will often hand-craft a model that ignores
certain problem features deemed irrelevant, or will define other features that summarize a
2

fiDecision-Theoretic Planning: Structural Assumptions

wide class of problem states. In AI, the emphasis is on the automatic solution of problems
posed by users who lack the expertise of a decision analyst. Thus, assuming a well-crafted,
compact state space is often not appropriate.
In this paper we show how specialized representations and algorithms from AI planning
and problem solving can be used to design ecient MDP solution techniques. In particular,
AI planning methods assume a certain structure in the state space, in the actions (or
operators), and in the specification of a goal or other success criteria. Representations and
algorithms have been designed that make the problem structure explicit and exploit that
structure to solve problems effectively. We demonstrate how this same process of identifying
structure, making it explicit, and exploiting it algorithmically can be brought to bear in
the solution of MDPs.
This paper has several objectives. First, it provides an overview of DTP and MDPs
suitable for readers familiar with traditional AI planning methods and makes connections
with this work. Second, it describes the types of structure that can be exploited and how
AI representations and methods facilitate computationally effective planning with MDPs.
As such, it is a suitable introduction to AI methods for those familiar with the classical
presentation of MDPs. Finally, it surveys recent work on the use of MDPs in AI and
suggests directions for further research in this regard, and should therefore be of interest to
researchers in DTP.

1.1 General Problem Definition
Roughly speaking, the class of problems we consider are those involving systems whose
dynamics can be modeled as stochastic processes, where the actions of decision maker,
referred to here as the agent , can inuence the system's behavior. The system's current
state and the choice of action jointly determine a probability distribution over the system's
possible next states. The agent prefers to be in certain system states (e.g., goal states) over
others, and therefore must determine a course of action|also called a \plan" or \policy" in
this paper|that is likely to lead to these target states, possibly avoiding undesirable states
along the way. The agent may not know the system's state exactly in making its decision
on how to act, however|it may have to rely on incomplete and noisy sensors and be forced
to base its choice of action on a probabilistic estimate of the state.
To help illustrate the types of problems in which we are interested, consider the following
example. Imagine that we have a robot agent designed to help someone (the \user") in an
oce environment (see Figure 1). There are three activities it might undertake: picking up
the user's mail, getting coffee, or tidying up the user's research lab. The robot can move
from location to location and perform various actions that tend to achieve certain target
states (e.g., bringing coffee to the user on demand, or maintaining a minimal level of tidiness
in the lab).
We might associate a certain level of uncertainty with the effects of the robot's actions
(e.g., when it tries to move to an adjacent location it might succeed 90% of the time and fail
to move at all the other 10% of the time). The robot might have incomplete access to the
true state of the system in that its sensors might supply it with incomplete information (it
cannot tell whether mail is available for pickup if it is not in the mail room) and incorrect
3

fiBoutilier, Dean, & Hanks

My Office

Hallway

Lab
MailRoom

Coffee

Figure 1: A decision-theoretic planning problem
information (even when in the mail room its sensors occasionally fail to detect the presence
of mail).
Finally, the performance of the robot might be measured in various ways: do its actions
guarantee that a goal will be achieved? Do they maximize some objective function defined
over possible effects of its actions? Do they achieve a goal state with sucient probability while avoiding \disastrous" states with near certainty? The stipulation of optimal or
acceptable behavior is an important part of the problem specification.
The types of problems that can be captured using this general framework include classical (goal-oriented, deterministic, complete knowledge) planning problems and extensions
such as conditional and probabilistic planning problems, as well as other more general
problem formulations.
The discussion to this point has assumed an extensional representation of the system's
states|one in which each state is explicitly named. In AI research, intensional representations are more common. An intensional representation is one in which states or sets of
states are described using sets of multi-valued features. The choice of an appropriate set
of features is an important part of the problem design. These features might include the
current location of the robot, the presence or absence of mail, and so on. The performance
metric is also typically expressed intensionally. Figure 2 serves as a reference for our example problem, which we use throughout the paper. It lists the basic features used to describe
the states of the system, the actions available to the robot and the exogenous events that
might occur, together with an intuitive description of the features, actions and events.
The remainder of the paper is organized as follows. In Section 2, we present the MDP
framework in the abstract, introducing basic concepts and terminology and noting the relationship between this abstract model and the classical AI planning problem. Section 3 surveys common solution techniques|algorithms based on dynamic programming for general
MDP problems and search algorithms for planning problems|and points out the relationship between problem assumptions and solution techniques. Section 4 turns from algorithms
to representations, showing various ways in which the structured representations commonly
used by AI algorithms can be used to represent MDPs compactly as well. Section 5 surveys
4

fiDecision-Theoretic Planning: Structural Assumptions

Features
Location

Denoted
Description
Loc(M ), etc. Location of robot. Five possible locations: mailroom (M), coffee room
(C), user's oce (O), hallway (H), laboratory (L)
Tidiness
T (0), etc.
Degree of lab tidiness. Five possible values: from 0 (messiest) to 4
(tidiest)
Mail present
M; M
Is there mail is user's mail box? True (M ) or False (M )
Robot has mail
RHM; RHM Does the robot have mail in its possession?
Coffee request
CR; CR
Is there an outstanding (unfulfilled) request for coffee by the user?
Robot has coffee RHC; RHC Does the robot have coffee in its possession?
Actions
Denoted
Description
Move clockwise
Clk
Move to adjacent location (clockwise direction)
Counterclockwise CClk
Move to adjacent location (counterclockwise direction)
Tidy lab
Tidy
If the robot is in the lab, the degree of tidiness is increased by 1
Pickup mail
PUM
If the robot is in the mailroom and there is mail present, the robot
takes the mail (RHM becomes true and M becomes false)
Get coffee
GetC
If the robot is in the coffee room, it gets coffee (RHC becomes true)
Deliver mail
DelM
If the robot is in the oce and has mail, it hands the mail to the user
(RHM becomes false)
Deliver coffee
DelC
If the robot is in the oce and has coffee, it hands the coffee to the
user (RHC and CR both become false)
Events
Denoted
Description
Mail arrival
ArrM
Mail arrives causing M to become true
Request coffee
ReqC
User issues coffee request causing CR to become true
Untidy the lab
Mess
The lab becomes messier (one degree less tidy)

Figure 2: Elements of the robot domain.
some recent work on abstraction, aggregation and problem decomposition methods, and
shows the connection to more traditional AI methods such as goal regression. This last
section demonstrates that representational and computational methods from AI planning
can be used in the solution of general MDPs. Section 5 also points out additional ways in
which this type of computational leverage might be developed in the future.

2. Markov Decision Processes: Basic Problem Formulation
In this section we introduce the MDP framework and make explicit the relationship between
this model and classical AI planning models. We are interested in controlling a stochastic
dynamical system: a system that at any point in time can be in one of a number of distinct
states, and in which the system's state changes over time in response to events. An action
is a particular kind of event instigated by an agent in order to change the system's state.
We assume that the agent has control over what actions are taken and when, though the
effects of taking an action might not be perfectly predictable. In contrast, exogenous events
are not under the agent's control, and their occurrence may be only partially predictable.
This abstract view of an agent is consistent both with the \AI" view where the agent is an
autonomous decision maker and the \control" view where a policy is determined ahead of
time, programmed into a device, and executed without further deliberation.
5

fiBoutilier, Dean, & Hanks

2.1 States and State Transitions

We define a state to be a description of the system at a particular point in time. How one
defines states can vary with particular applications, some notions being more natural than
others. However, it is common to assume that the state captures all information relevant
to the agent's decision-making process. We assume a finite state space S = fs1 ; : : : ; sN g
of possible system states.2 In most cases the agent will not have complete information
about the current state; this uncertainty or incomplete information can be captured using
a probability distribution over the states in S .
A discrete-time stochastic dynamical system consists of a state space and probability
distributions governing possible state transitions|how the next state of the system depends
on past states. These distributions constitute a model of how the system evolves over time
in response to actions and exogenous events, reecting the fact that the effects of actions
and events may not be perfectly predictable even if the prevailing state is known.
Although we are generally concerned with how the agent chooses an appropriate course
of action, for the remainder of this section we assume that the agent's course of action is
fixed, concentrating on the problem of predicting the system's state after the occurrence of
a predetermined sequence of actions. We discuss the action selection problem in the next
section.
We assume the system evolves in stages, where the occurrence of an event marks the
transition from one stage t to the next stage t + 1. Since events define changes in stage,
and since events often (but not necessarily) cause state transitions, we often equate stage
transitions with state transitions. Of course, it is possible for an event to occur but leave
the system in the same state.
The system's progression through stages is roughly analogous to the passage of time.
The two are identical if we assume that some action (possibly a no-op) is taken at each
stage, and that every action takes unit time to complete. We can thus speak loosely as if
stages correspond to units of time, and we refer to T interchangeably as the set of all stages
and the set of all time points.3
We can model uncertainty by regarding the system's state at some stage t as a random
variable S t that takes values from S . An assumption of \forward causality" requires that the
variable S t does not depend directly on the value of future variable S k (k > t). Roughly,
it requires that we model our system such that the past history \directly" determines
the distribution over current states, whereas knowledge of future states can inuence the
estimate of the current state only indirectly by providing evidence on what the current state
may have been so as to lead to these future states. Figure 3(a) shows a graphical perspective
on a discrete-time, stochastic dynamical system. The nodes are random variables denoting
the state at a particular time, and the arcs indicate the direct probabilistic dependence
of states on previous states. To describe this system completely we must also supply the
conditional distributions Pr(S t jS 0 ; S 1 ;    S t,1 ) for all times t.
States should be thought of as descriptions of the system being modeled, so the question arises of how much detail about the system is captured in a state description. More
2. Most of the discussion in this paper also applies to cases where the state space is countably infinite. See
(Puterman, 1994) for a discussion of infinite and continuous-state problems.
3. While we do not deal with such topics here, there is a considerable literature in the OR community on
continuous-time Markov decision processes (Puterman, 1994).

6

fiDecision-Theoretic Planning: Structural Assumptions

(a)

S

S

S

(b)

S

S

S

(c)

0

0

1

1

S

2

S

2

S

t-1

t-1

S

t

S

t

S

t-1

t

Figure 3: A general stochastic process (a), a Markov chain (b), and a stationary Markov
chain (c).
detail implies more information about the system, which in turn often translates into better
predictions of future behavior. Of course, more detail also implies a larger set S , which can
increase the computational cost of decision making.
It is commonly assumed that a state contains enough information to predict the next
state. In other words, any information about the history of the system relevant to predicting
its future is captured explicitly in the state itself. Formally, this assumption, the Markov
assumption, says that knowledge of the present state renders information about the past
irrelevant to making predictions about the future:
Pr(S t+1 jS t ; S t,1 ; : : : ; S 0 ) = Pr(S t+1 jS t )
Markovian models can be represented graphically using a structure like that in Figure 3(b),
reecting the fact that the present state is sucient to predict future state evolution.4
Finally, it is common to assume that the effects of an event depend only on the prevailing
state, and not the stage or time at which the event occurs.5 If the distribution predicting
the next state is the same regardless of stage, the model is said to be stationary and can
be represented schematically using just two stages, as in Figure 3(c). In this case only a
single conditional distribution is required. In this paper we generally restrict our attention
to discrete-time, finite-state, stochastic dynamical systems with the Markov property, commonly called Markov chains. Furthermore, most of our discussion is restricted to stationary
chains.
To complete the model we must provide a probability distribution over initial states,
reecting the probability of being in any state at stage 0. This distribution can be repre4. It is worth mentioning that the Markov property applies to the particular model and not to the system
itself. Indeed, any non-Markovian model of a system (of finite order, i.e., whose dynamics depend on at
most the k previous states for some k) can be converted to an equivalent though larger Markov model.
In control theory, this is called conversion to state form (Luenberger, 1979).
5. Of course, this is also a statement about model detail, saying that the state carries enough information
to make the stage irrelevant to predicting transitions.

7

fiBoutilier, Dean, & Hanks

.3

6

.7

.9
.1

.5

1
.5

3

.1

4

1.0

.2

2

.8

5

.4

7
1.0

.5

Figure 4: A state-transition diagram.
sented as a real-valued (row) vector of size N = jS j (one entry for each state). We denote
this vector P 0 and use p0i to denote its ith entry, that is, the probability of starting in state
si .
We can represent a T -stage nonstationary Markov chain with T transition matrices,
each of size N  N , where matrix P t captures the transition probabilities governing the
system as it moves from stage t to stage t + 1. Each matrix consists of probabilities ptij ,
where ptij = Pr(S t+1 = sj jS t = si ). If the process is stationary, the transition matrix is
the same at all stages and one matrix (whose entries are denoted pij ) will suce. Given an
initial
over states P 0 , the probability distribution over states after n stages is
Q0 Pdistribution
i.
i=n
A stationary Markov process can also be represented using a state-transition diagram
as in Figure 4. Here nodes correspond to particular states and the stage is not represented
explicitly. Arcs denote possible transitions (those with non-zero probability) and are labeled
with the transition probabilities pij = Pr(S t+1 = sj jS t = si ). The arc from node i to node
j is labeled with pij if pij > 0.6 The size of such a diagram is at least O(N ) and at most
O(N 2 ), depending on the number of arcs. This is a useful representation when the transition
graph is relatively sparse, for example, when most states have immediate transitions to only
few neighbors.

Example 2.1 To illustrate these notions, imagine that the robot in Figure 1 is executing

the policy of moving counterclockwise repeatedly. We restrict our attention to two
variables, location Loc and presence of mail M , giving a state space of size 10. We
suppose that the robot always moves to the adjacent location with probability 1:0.
In addition, mail can arrive at the mailroom with probability 0:2 at any time (independent of the robot's location), causing the variable M to become true. Once M
becomes true, the robot cannot move to a state where M is false, since the action of
moving does not inuence the presence of mail. The state-transition diagram for this
example is illustrated in Figure 5. The transition matrix is also shown. 2

The structure of a Markov chain is occasionally of interest to us in planning. A subset

C  S is closed if pij = 0 for all i 2 C and j 62 C . It is a proper closed set if no proper
subset of C enjoys this property. We sometimes refer to proper closed sets as recurrent
classes of states. If a closed set consists of a single state, then that state is called an
absorbing state. Once an agent enters a closed set or absorbing state, it remains there
6. It is important to note that the nodes here do not represent random variables as in the earlier figures.

8

fiDecision-Theoretic Planning: Structural Assumptions

s1
s2

0.2

OM

0.8

0.8
0.2

OM

s7

HM

LM

0.8

1.0

1.0

s10
0.2

0.8
CM

MM

MM

0.2

s3

HM

s9 1.0

s4

0.8

s1
s2
s3
s4
s5
s6
s7
s8
s9
s10

s6

1.0

s5

LM

0.2

s8

CM

1.0

s1
0:0
0:0
0:0
0:0
0:8
0:0
0:0
0:0
0:0
0:0

s2
0:8
0 :0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0 :0

s3
0:0
0:8
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0 :0

s4
0:0
0:0
0:8
0:0
0:0
0:0
0:0
0:0
0:0
0:0

s5
0:0
0:0
0:0
0:8
0:0
0:0
0:0
0:0
0:0
0:0

s6
0:0
0 :0
0:0
0 :0
0:2
0 :0
0 :0
0 :0
0 :0
1:0

s7
0:2
0:0
0:0
0:0
0:0
1:0
0:0
0:0
0:0
0:0

s8
0:0
0:2
0:0
0:0
0:0
0:0
1:0
0:0
0:0
0:0

s9
0:0
0:0
0:2
0:0
0:0
0:0
0:0
1:0
0:0
0:0

s10
0:0
0:0
0:0
0:2
0:0
0:0
0:0
0:0
1:0
0:0

Figure 5: The state-transition diagram and transition matrix for a moving robot.
forever with probability 1. In the example above (Figure 5), the set of states where M
holds forms a recurrent class. There are no absorbing states in the example, but should we
program the robot to stay put whenever it is in the state hM; Loc(O)i, then this would be
an absorbing state in the altered chain. Finally, we say a state is transient if it does not
belong to a recurrent class. In Figure 5, each state where M holds is transient|eventually
(with probability 1), the agent leaves the state and never returns, since there is no way to
remove mail once it arrives.

2.2 Actions
Markov chains can be used to describe the evolution of a stochastic system, but they do
not capture the fact that an agent can choose to perform actions that alter the state of the
system. A key element of MDPs is the set of actions available to the decision maker. When
an action is performed in a particular state, the state changes stochastically in response to
the action. We assume that the agent takes some action at each stage of the process, and
then the system changes state accordingly.
At each stage t of the process and each state s, the agent has available a set of actions
Ats. This is called the feasible set for s at stage t. To describe the effects of a 2 Ats, we must
supply the state-transition distribution Pr(S t+1 jS t = s; At = a) for all actions a, states s,
and stages t. Unlike the case of a Markov chain, the terms Pr(S t+1 jS t = s; At = a) are not
true conditional distributions, but rather a family of distributions parameterized by S t and
At , since the probability of At is not part of the model. We retain this notation, however,
for its suggestive nature.
We often assume that the feasible set of actions is the same for all stages and states, in
which case the set of actions is A = fa1 ; : : : ; aK g and each can be executed at any time.
This contrasts with the AI planning practice of assigning preconditions to actions defining
the states in which they can meaningfully be executed. Our model takes the view that any
action can be executed (or \attempted") in any state. If the action has no effect when
executed in some state, or its execution leads to disastrous effects, this can be noted in
the action's transition matrix. Action preconditions are often a computational convenience
rather than a representational necessity: they can make the planning process more ecient
by identifying states in which the planner should not even consider selecting that action.
Preconditions can be represented in MDPs by relaxing the assumption that the set of
9

fiBoutilier, Dean, & Hanks

s1
s2
s3
s4
s5
s6
s7
s8
s9
s10

s1
0:0
0:0
0:0
0:0
0:8
0:0
0:0
0:0
0:0
0:0

s2
0:8
0:0
0:8
0:0
0:0
0:0
0:0
0:0
0:0
0:0

s3
0:0
0:8
0:0
0:8
0:0
0:0
0:0
0:0
0:0
0:0

s4
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0

s5
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0

s6
0:0
0:0
0:0
0:0
0:2
0:0
0:0
0:0
0:0
1:0

s7
0:2
0:0
0:2
0:0
0:0
1:0
0:0
1:0
0:0
0:0

s8
0:0
0:2
0:0
0:2
0:0
0:0
1:0
0:0
1:0
0:0

s9
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0

s10
0:2
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0

s1
0.8

0.2

OM

0.8

s2

LM

s3

s7

s10

1.0

s9

1.0
0.8

CM

MM

MM

s8

s4

0.2
0.2

1.0

s6
HM

s5

0.8
0.8

1.0

HM

0.2

LM

OM

0.2

CM

1.0

Figure 6: The transition matrix for Clk and the induced transition diagram for a two-action
policy.
feasible actions is the same for all states. To illustrate planning concepts below, however,
we sometimes assume actions do have preconditions.
We again restrict our attention to stationary processes, which in this case means that
the effects of each action depends only on the state and not on the stage. Our transition
matrices thus take the form pkij = Pr(S t+1 = sj jS t = si ; At = ak ), capturing the probability
that the system moves to state sj when ak is executed in state si . In stationary models an
action is fully described by a single N  N transition matrix P k . It is important to note
that the transition matrix for an action includes not only the direct effects of executing the
action but also the effects of any exogenous events that might occur at the same stage.7

Example 2.2 The example in Figure 5 can be extended so the agent has two available

actions: moving clockwise and moving counterclockwise. The transition matrix for
CClk (with the assumption that mail arrives with probability 0:2) is shown in Figure 5.
The matrix for Clk appears on the left in Figure 6. Suppose the agent fixes its behavior
so that it moves clockwise in locations M and C and counterclockwise in locations H ,
O and L (we address below how the agent might come to know its location so that it
can actually implement this behavior). This defines the Markov chain illustrated in
the transition diagram on the right in Figure 6. 2

2.3 Exogenous Events

Exogenous events are those events that stochastically cause state transitions, much like
actions, but beyond the control of the decision maker. These might correspond to the
evolution of a natural process or the action of another agent. Notice that the effect of
the action CClk in Figure 5 \combines" the effects of the robot's action with that of the
exogenous event of mail arrival: state-transition probabilities incorporate both the motion
of the robot (causing a change in location) and the possible change in mail status due
to mail arrival. For the purposes of decision making, it is precisely this combined effect
7. It is possible to assess the effects of actions and exogenous events separately, then combine them into
a single transition matrix in certain cases (Boutilier & Puterman, 1995). We discuss this later in this
section.

10

fiDecision-Theoretic Planning: Structural Assumptions

that is important when predicting the distribution over possible states resulting when an
action is taken. We call such models of actions implicit-event models, since the effects of
the exogenous event are folded into the transition probabilities associated with the action.
However, it is often natural to view these transitions as comprised of these two separate
events, each having its own effect on the state. More generally, we often think of transitions
as determined by the effects of the agent's chosen action and those of certain exogenous
events beyond the agent's control, each of which may occur with a certain probability.
When the effects of actions are decomposed in this fashion, we call the action model an
explicit-event model.
Specifying a transition function for an action and zero or more exogenous events is not
generally easy, for actions and events can interact in complex ways. For instance, consider
specifying the effect of action PUM (pickup mail) at a state where no mail is present but
there is the possibility of \simultaneous" mail arrival (i.e., during the \same unit" of discrete
time). If the event ArrM occurs, does the robot obtain the newly arrived mail, or does the
mail remain in the mailbox? Intuitively, this depends on whether the mail arrived before or
after the pickup was completed (albeit within the same time quantum). The state transition
in this case can be viewed as the composition of two transitions where the precise description
of the composition depends on the ordering of the agent's action and the exogenous event.
If mail arrives first, the transition might be s ! s0 ! s00 , where s0 is a state where mail
is waiting and s00 is a state where no mail is waiting and the robot is holding mail; but if
the pickup action is completed first, the transition would be s ! s ! s0 (i.e., PUM has no
effect, then mail arrives and remains in the box).
The picture is more complicated if the actions and events can truly occur simultaneously
over some interval|in this case the resulting transition need not be a composition of the
individual transitions. As an example, if the robot lifts the side of a table on which a glass
of water is situated, the water will spill; similarly if an exogenous event causes the other side
to be raised. But if the action and event occur simultaneously, the result is qualitatively
different (the water is not spilled). Thus, the \interleaving" semantics described above is
not always appropriate.
Because of such complications, modeling exogenous events and their combination with
actions or other events can be approached in many ways, depending on the modeling assumptions one is willing to make. Generally, we specify three types of information. First,
we provide transition probabilities for all actions and events under the assumption that
these occur in isolation|these are standard transition matrices. The transition matrix in
Figure 5 can be decomposed into the two matrices shown in Figure 7, one for Clk and one
for ArrM.8 Second, for each exogenous event, we must specify its probability of occurrence.
Since this can vary with the state, we generally require a vector of length N indicating the
probability of occurrence at each state. The occurrence vector for ArrM would be
[0:2 0:2 0:2 0:2 0:2 0:0 0:0 0:0 0:0 0:0]
8. The fact that these individual matrices are deterministic is an artifact of the example. In general, the
actions and events will each be represented using genuinely stochastic matrices.

11

fiBoutilier, Dean, & Hanks

s1
s2
s3
s4
s5
s6
s7
s8
s9
s10

s1

0:0
0:0
0:0
0:0
1:0
0:0
0:0
0:0
0:0
0:0

s2

1:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0

s3

0:0
1:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0

s4

0:0
0:0
1:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0

s5

0:0
0:0
0:0
1:0
0:0
0:0
0:0
0:0
0:0
0:0

s6

0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
1:0

s7

0:0
0:0
0:0
0:0
0:0
1:0
0:0
0:0
0:0
0:0

s8

0:0
0:0
0:0
0:0
0:0
0:0
1:0
0:0
0:0
0:0

s9

0:0
0:0
0:0
0:0
0:0
0:0
0:0
1:0
0:0
0:0

s10
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
1:0
0:0

s1
s2
s3
s4
s5
s6
s7
s8
s9
s10

Action Clk

s1

0 :0
0 :0
0 :0
0 :0
0 :0
0 :0
0 :0
0 :0
0 :0
0 :0

s2

0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0

s3

0 :0
0 :0
0 :0
0 :0
0 :0
0 :0
0 :0
0 :0
0 :0
0 :0

s4

0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0

s5

0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0

s6

1:0
0:0
0:0
0:0
0:0
1:0
0:0
0:0
0:0
0:0

s7

0:0
1:0
0:0
0:0
0:0
0:0
1:0
0:0
0:0
0:0

s8

0:0
0:0
1:0
0:0
0:0
0:0
0:0
1:0
0:0
0:0

s9

0:0
0:0
0:0
1:0
0:0
0:0
0:0
0:0
1:0
0:0

s10
0:0
0:0
0:0
0:0
1:0
0:0
0:0
0:0
0:0
1:0

Event ArrM

Figure 7: The transition matrices for an action and exogenous event in an explicit-event
model.
where we assume, for illustration, that mail arrives only when none is present.9 The final
requirement is a combination function that describes how to \compose" the transitions of an
action with any subset of event transitions. As indicated above, this can be very complex,
sometimes almost unrelated to the individual action and event transitions. However, under
certain assumptions combination functions can be specified reasonably concisely.
One way of modeling the composition of transitions is to assume an interleaving semantics of the type alluded to above. In this case, one needs to specify the probability that
the action and events that take place occur in a specific order. For instance, one might
assume that each event occurs at a time|within the discrete time unit|according to some
continuous distribution (e.g., an exponential distribution with a given rate). With this information, the probability of any particular ordering of transitions, given that certain events
occur, can be computed, as can the resulting distribution over possible next states. In the
example above, the probability of (composed) transitions s1 ! s2 ! s3 and s1 ! s1 ! s2
would be given by the probabilities with which mail arrived first or last, respectively.
In certain cases, the probability of this ordering is not needed. To illustrate another
combination function, assume that the action always occurs before the exogenous events.
Furthermore, assume that events are commutative: (a) for any initial state s and any pair
of events e1 and e2 , the distribution that results from applying event sequence e1  e2 to s is
identical to that obtained from the sequence e2  e1 ; and (b) the occurrence probabilities at
intermediate states are identical. Intuitively, the set of events in our domain, ArrM, ReqC
and Mess, has this property. Under these conditions the combined transition distribution
for any action a is computed by considering the probability of any subset of events and
applying that subset in any order to the distribution associated with a.
Generally, we can construct an implicit-event model from the various components of the
explicit-event model; thus, the \natural" specification can be converted to the form usually
used by MDP solution algorithms. Under the two assumptions above, for instance, we can
form an implicit event transition matrix Pr(si ; a; sj ) for any action a, given the matrix
Pcra (si ; sj ) for a (which assumes no event occurrences), the matrices Pre (si ; sj ) for events
e, and the occurrence vector Pre(si ) for each event e. The effective transition matrix for
9. The probability of different events may be correlated (possibly at particular states). If this is the case,
then it is necessary to specify occurrence probabilities for subsets of events. We will treat event occurrence
probabilities as independent for ease of exposition.

12

fiDecision-Theoretic Planning: Structural Assumptions

event e is defined as follows:
Pcre (si ; sj ) = Pre (si )Pre (si ; sj ) +

(

1 , Pre (si ) : i = j
0 : i 6= j

This equation captures the event transition probabilities with the probability of event occurrence factored in. If we let E; E 0 denote the diagonal matrices with entries Ekk = Pre (sk )
0 = 1 , Pre (sk ), then Pr
c e(si; sj ) = E Pre +E 0. Under the assumptions above, the
and Ekk
implicit-event matrix Pr(si ; a; sj ) for action a is then given by Pr = Pcre1    Pcren Pra for
any ordering of the n possible events.
Naturally, different procedures for constructing implicit-event matrices will be required
given different assumptions about action and event interaction. Whether such implicit models are constructed or specified directly without explicit mention of the exogenous events, we
will always assume unless stated otherwise that action transition matrices take into account
the effects of exogenous events as well, and thus represent the agent's best information
about what will happen if it takes a particular action.

2.4 Observations

Although the effects of an action can depend on any aspect of the prevailing state, the
choice of action can depend only on what the agent can observe about the current state
and remember about its prior observations. We model the agent's observational or sensing
capabilities by introducing a finite set of observations O = fo1 ; : : : ; oH g. The agent receives
an observation from this set at each stage prior to choosing its action at that stage. We
can model this observation as a random variable Ot whose value is taken from O. The
probability that a particular Ot is generated can depend on:

 the state of the system at t , 1
 the action taken at t , 1
 the state of the system at t after taking the action at t , 1 and after the effects of any
exogenous events at t , 1 are realized, but before the action at t is taken.
We let Pr(Ot = oh jS t,1 = si ; At,1 = ak ; S t = sj ) be the probability that the agent observes

oh at stage t given that it performs ak in state si and ends up in state sj . As with actions,

we assume that observational distributions are stationary (independent of the stage), using

phi;j;k = Pr(ohjsi; ak ; sj ) to denote this quantity. We can view the probabilistic dependencies

among state, action and observation variables as a graph in which the time-indexed variables
are shown as nodes and one variable is directly probabilistically dependent on another if
there is an edge from the latter to the former; see Figure 8.
This model allows a wide variety of assumptions about the agent's sensing capabilities.
At one extreme are fully observable MDPs (FOMDPs), in which the agent knows exactly
what state it is in at each stage t. We model this case by letting O = S and setting
Pr(oh jsi ; ak ; sj ) =
13

(

1 iff oh = sj
0 otherwise

fiBoutilier, Dean, & Hanks

A

t-1

S

S

t-1

t

O

t

Figure 8: Graph showing the dependency relationships among states, actions and observations at different times.
In the example above, this means the robot always knows its exact location and whether or
not mail is waiting in the mailbox, even if it is not in the mailroom when the mail arrives.
The agent thus receives perfect feedback about the results of its actions and the effects
of exogenous events|it has noisy effectors but complete, noise-free, and \instantaneous"
sensors. Most recent AI research that adopts the MDP framework explicitly assumes full
observability.
At the other extreme we might consider non-observable systems (NOMDPs) in which
the agent receives no information about the system's state during execution. We can model
this case by letting O = fog. Here the same observation is reported at each stage, revealing
no information about the state, so that Pr(sj jsi ; ak ; o) = Pr(sj jsi ; ak ). In these open-loop
systems, the agent receives no useful feedback about the results of its actions: the agent has
noisy effectors and no sensors. In this case an agent chooses its actions according to a plan
consisting of a sequence of actions executed unconditionally. In effect, the agent is relying
on its predictive model to determine good action choices before execution time.
Traditionally, AI planning work has implicitly made the assumption of non-observability,
often coupled with an omniscience assumption|that the agent knows the initial state with
certainty, can predict the effects of its actions perfectly, and can precisely predict the occurrence of any exogenous events and their effects. Under these circumstances, the agent
can predict the exact outcome of any plan, thus obviating the need for observation. Such
an agent can build a straight-line plan|a sequence of actions to be performed without
feedback|that is as good as any plan whose execution might depend on information gathered at execution time.
These two extremes are special cases of the general observation model described above,
which allows the agent to receive incomplete or noisy information about the system state
(i.e., partially observable MDPs, or POMDPs). For example, the robot might be able to
determine its location exactly, but might not be able to determine whether mail arrives
unless it is in the mailroom. Furthermore, its \mail" sensors might occasionally report
inaccurately, leading to an incorrect belief as to whether there is mail waiting.

Example 2.3 Suppose the robot has a \checkmail" action that does not change the system

state but generates an observation that is inuenced by the presence of mail, provided
14

fiDecision-Theoretic Planning: Structural Assumptions

Loc(M ); M
Loc(M ); M
Loc(M ); M
Loc(M ); M

Pr(Obs = mail) Pr(Obs = nomail)
0:92
0:08
0:05
0:95
0:00
1:00
0:00
1:00

Figure 9: Observation probabilities for checking mailbox.
the robot is in the mailroom at the time the action is performed. If the robot is not
in the mailroom, the sensor always reports \no mail." A noisy \checkmail" sensor
can be described by a probability distribution like the one shown in Figure 9. We can
view these error probabilities as the probability of \false positives" (0:05) and \false
negatives" (0:08). 2

2.5 System Trajectories and Observable Histories

We use the terms trajectory and history interchangeably to describe the system's behavior
during the course of a problem-solving episode, or perhaps some initial segment thereof.
The complete system history is the sequence of states, actions, and observations generated
from stage 0 to some time point of interest, and can be of finite or infinite length. Complete
histories can be represented by a (possibly infinite) sequence of tuples of the form

hhS 0 ; O0 ; A0 i; hS 1 ; O1 ; A1 i; : : : hS T ; OT ; AT ii
We can define two alternative notions of history that contain less complete information.
For some arbitrary stage t we define the observable history as the sequence

hhO0 ; A0 i; : : : ; hOt,1 ; At,1 ii
where O0 is the observation of the initial state. The observable history at stage t comprises
all information available to the agent about its history when it chooses its action at stage t.
A third type of trajectory is the system trajectory, which is the sequence

hhS 0 ; A0 i; : : : ; hS t,1 ; At,1 i; S t i
describing the system's behavior in \objective" terms, independent of the agent's particular
view of the system.
In evaluating an agent's performance, we will generally be interested in the system
trajectory. An agent's policy must be defined in terms of the observable history, since the
agent does not have access to the system trajectory, except in the fully observable case,
when the two are equivalent.

2.6 Reward and Value

The problem facing the decision maker is to select an action to be performed at each stage
of the decision problem, making this decision on the basis of the observable history. The
agent still needs some way to judge the quality of a course of action. This is done by defining
15

fiBoutilier, Dean, & Hanks

A

t-1

S

S

t-1

t

C

t

R

t

Figure 10: Decision process with rewards and action costs.
a value function V() as a function mapping the set of system histories HS into the reals;
that is, V : HS ! R.10 The agent prefers system history h to h0 just in case V(h) > V(h0 ).
Thus, the agent judges its behavior to be good or bad depending on its effect on the
underlying system trajectory. Generally, the agent cannot predict with certainty which
system trajectory will occur, and can at best generate a probability distribution over the
possible trajectories caused by its actions. In that case, it computes the expected value of
each candidate course of action and chooses a policy that maximizes that quantity.
Just as with system dynamics, specifying a value function over arbitrary trajectories
can be cumbersome and unintuitive. It is therefore important to identify structure in the
value function that can lead to a more parsimonious representation.
Two assumptions about value functions commonly made in the MDP literature are
time-separability and additivity. A time-separable value function is defined in terms of
more primitive functions that can be applied to component states and actions. The reward
function R : S ! R associates a reward with being in a state s. Costs can be assigned
to taking actions by defining a cost function C : S  A ! R that associates a cost with
performing an action a in state s. Rewards are added to the value function, while costs are
subtracted.11
A value function is time-separable if it is a \simple combination" of the rewards and costs
accrued at each stage. \Simple combination" means that value is taken to be a function of
costs and rewards at each stage, where the costs and rewards can depend on the stage t, but
the function that combines these must be independent of the stage, most commonly a linear
combination or a product.12 A value function is additive if the combination function is a
sum of the reward and cost function values accrued over the history's stages. The addition
of rewards and action costs in a system with time-separable value can be viewed graphically
as shown in Figure 10.
The assumption of time-separability is restrictive. In our example, there might be
certain goals involving temporal deadlines (have the workplace tidy as soon as possible
after 9:00 tomorrow morning) and maintenance (do not allow mail to sit in the mailroom
10. Technically, the set of histories of interest also depends on the horizon chosen, as described below.
11. The term \reward" is somewhat of a misnomer in that the reward could be negative, in which case
\penalty" might be a better word. Likewise, \costs" can be either positive (punitive) or negative (beneficial). Thus, they admit great exibility in defining value functions.
12. See (Luenberger, 1973) for a more precise definition of time-separability.

16

fiDecision-Theoretic Planning: Structural Assumptions

undelivered for more than 10 minutes) that require value functions that are non-separable
given our current representation of the state. Note, however, that separability|like the
Markov property|is a property of a particular representation. We could add additional
information to the state in our example: the clock time, the interval of time between 9:00
and the time at which tidiness is achieved, the length of time mail sits in the mail room
before the robot picks it up, and so on. With this additional information we could reestablish a time-separable value function, but at the expense of an increase in the number
of states and a more ad hoc and cumbersome action representation.13

2.7 Horizons and Success Criteria

In order to evaluate a particular course of action, we need to specify how long (in how
many stages) it will be executed. This is known as the problem's horizon. In finite-horizon
problems, the agent's performance is evaluated over a fixed, finite number of stages T .
Commonly, our aim is to maximize the total expected reward associated with a course of
action; we therefore define the (finite-horizon) value of any length T history h as (Bellman,
1957):

V (h) =

TX
,1
t=0

fR(st ) , C (st; at )g + R(sT )

An infinite-horizon problem, on the other hand, requires that the agent's performance
be evaluated over an infinite trajectory. In this case the total reward may be unbounded,
meaning that any policy could be arbitrarily good or bad if it were executed for long enough.
In this case it may be necessary to adopt a different means of evaluating a trajectory. The
most common is to introduce a discount factor, ensuring that rewards or costs accrued at
later stages are counted less than those accrued at earlier stages. The value function for
an expected total discounted reward problem is defined as follows (Bellman, 1957; Howard,
1960):
1
X
V (h) =  t (R(st) , C (st; at ))
t=0

where  is a fixed discount rate (0   < 1). This formulation is a particularly simple
and elegant way to ensure a bounded measure of value over an infinite horizon, though it is
important to verify that discounting is in fact appropriate. Economic justifications are often
provided for discounted models|a reward earned sooner is worth more than one earned
later provided the reward can somehow be invested. Discounting can also be suitable for
modeling a process that terminates with probability 1 ,  at at any point in time (e.g., a
robot that can break down), in which case discounted models correspond to expected total
reward over a finite but uncertain horizon. For these reasons, discounting is sometimes used
for finite-horizon problems as well.
Another technique for dealing with infinite-horizon problems is to evaluate a trajectory
based on the average reward accrued per stage, or gain. The gain of a history is defined as
n
X
g(h) = lim 1 fR(st ) , C (st ; at )g
n!1 n t=0

13. See (Bacchus, Boutilier, & Grove, 1996, 1997), however, for a systematic approach to handling certain
types of history-dependent reward functions.

17

fiBoutilier, Dean, & Hanks

Refinements of this criterion have also been proposed (Puterman, 1994).
Sometimes the problem itself ensures that total reward over any infinite trajectory is
bounded, and thus the expected total reward criterion is well-defined. Consider the case
common in AI planners in which the agent's task is to bring the system to a goal state. A
positive reward is received only when the goal is reached, all actions incur a non-negative
cost, and when a goal is reached the system enters an absorbing state in which no further
rewards or costs are accrued. As long as the goal can be reached with certainty, this
situation can be formulated as an infinite-horizon problem where total reward is bounded
for any desired trajectory (Bertsekas, 1987; Puterman, 1994). In general, such problems
cannot be formulated as (fixed) finite-horizon problems unless an a priori bound on the
number of steps needed to reach the goal can be established. These problems are sometimes
called indefinite-horizon problems: from a practical point of view, the agent will continue to
execute actions for some finite number of stages, but the exact number cannot be determined
ahead of time.

2.8 Solution Criteria

To complete our definition of the planning problem we need to specify what constitutes
a solution to the problem. Here again we see a split between explicit MDP formulations
and work in the AI planning community. Classical MDP problems are generally stated as
optimization problems: given a value function, a horizon, and an evaluation metric (e.g.,
expected total reward, expected total discounted reward, expected average reward per stage)
the agent seeks a behavioral policy that maximizes the objective function.
Work in AI often seeks satisficing solutions to such problems. In the planning literature,
it is generally taken that any plan that satisfies the goal is equally preferred to any other
plan that satisfies the goal, and that any plan that satisfies the goal is preferable to any
plan that does not.14 In a probabilistic framework, we might seek the plan that satisfies
the goal with maximum probability (an optimization), but this can lead to situations in
which the optimal plan has infinite length if the system state is not fully observable. The
satisficing alternative (Kushmerick, Hanks, & Weld, 1995) is to seek any plan that satisfies
the goal with a probability exceeding a given threshold.

Example 2.4 We extend our running example to demonstrate an infinite-horizon, fully
observable, discounted reward situation. We begin by adding one new dimension to
the state description, the boolean variable RHM (does the robot have mail), giving us
a system with 20 states. We also provide the agent with two additional actions: PUM
(pickup mail) and DelM (deliver mail) as described in Figure 2. We can now reward
the agent in such a way that mail delivery is encouraged: we associate a reward of
10 with each state in which RHM and M are both false and 0 with all other states.
If actions have no cost, the agent gets a total reward of 20 for this six-stage system
trajectory:
hLoc(M ); M; RHMi; Stay; hLoc(M ); M; RHMi; PUM; hLoc(M ); M; RHMi;
Clk; hLoc(H ); M; RHMi; Clk; hLoc(O); M; RHMi; DelM; hLoc(O); M; RHMi

14. Though see (Haddawy & Hanks, 1998; Williamson & Hanks, 1994) for a restatement of planning as an
optimization problem.

18

fiDecision-Theoretic Planning: Structural Assumptions

If we assign an action cost of ,1 for each action except Stay (which has 0 cost),
the total reward becomes 16. If we use a discount rate of 0:9 to discount future
rewards and costs, this initial segment of an infinite-horizon history would contribute
10 + :9(,1) + :81(,1) + :729(,1) + :6561(,1) + :59054(,1 + 10) = 12:2 to the total
value of the trajectory (as subsequently extended). Furthermore, we can establish a
bound on the total expected value of this trajectory. In the best case, all subsequent
stages will yield a reward of 10, so the expected total discounted reward is bounded
by
1
X
12:2 + :96 (10) + :97 (10) + : : : = 12:2 + 10  :96 0:9i < 66
i=0

A similar effect on behavior can be achieved by penalizing states (i.e., having negative
rewards) in which either M or RHM is true. 2

2.9 Policies

We have mentioned policies (or courses of action, or plans) informally to this point, and
now provide a precise definition. The decision problem facing an agent can be viewed most
generally as deciding which action to perform given the current observable history. We
define a policy  to be a mapping from the set of observable histories HO to actions, that
is,  : HO ! A. Intuitively, the agent executes action

at = (hho0 ; a0 i; : : : ; hot,1 ; at,1 i; ot i)
at stage t if it has performed the actions a0 ;    at,1 and made observations o0 ;    ot,1 at
earlier stages, and has just made observation ot at the current stage.
A policy induces a distribution Pr(hj) over the set of system histories HS ; this probability distribution depends on the initial distribution P 0 . We define the expected value of a
policy to be:
X
EV() =
V(h) Pr(hj)
h2HS

We would like the agent to adopt a policy that either maximizes this expected value or, in
a satisficing context, has an acceptably high expected value.
The general form of a policy, depending as it does on an arbitrary observation history,
can lead to very complicated policies and policy-construction algorithms. In special cases,
however, assumptions about observability and the structure of the value function can result
in optimal policies that have a much simpler form.
In the case of a fully observable MDP with a time-separable value function, the optimal
action at any stage can be computed using only information about the current state and
the stage: that is, we can restrict policies to have the simpler form  : S  T ! A without
danger of acting suboptimally. This is due to the fact that full observability allows the state
to be observed completely, and the Markov assumption renders prior history irrelevant.
In the non-observable case, the observational history contains only vacuous observations
and the agent must choose its actions using only knowledge of its previous actions and the
stage; however, since  incorporates previous actions, it takes the form  : T ! A. This
19

fiBoutilier, Dean, & Hanks

form of policy corresponds to a linear, unconditional sequence of actions ha1 ; a2 ; : : : ; aT i, or
a straight-line plan in AI nomenclature.15

2.10 Model Summary: Assumptions, Problems, and Computational
Complexity

This concludes our exposition of the MDP model for planning under uncertainty. Its generality allows us to capture a wide variety of the problem classes that are currently being
studied in the literature. In this section we review the basic components of the model,
describe problems commonly studied in the DTP literature with respect to this model, and
summarize known complexity results for each. In Section 3, we describe some of the specialized computational techniques used to solve problems in each of these problem classes.
2.10.1 Model Summary and Assumptions

The MDP model consists of the following components:
 The state space S , a finite or countable set of states. We generally make the Markov
assumption, which requires that each state convey all information necessary to predict
the effects of all actions and events independent of any further information about
system history.
 The set of actions A. Each action ak is represented by a transition matrix of size
jS jjS j representing the probability pkij that performing action ak in state si will move
the system into state sj . We assume throughout that the action model is stationary,
meaning that transition probabilities do not vary with time. The transition matrix
for an action is generally assumed to account for any exogenous events that might
occur at the stage at which the action is executed.
 The set of observation variables O. This is the set of \messages" sent to the agent after
an action is performed, that provide execution-time information about the current
system state. With each action ak and pair of states si , sj , such that pkij > 0,
we associate a distribution over possible observations: pkm
ij denotes the probability
of obtaining observation om given that action ak was taken in si and resulted in a
transition to state sj .
 The value function V . The value function maps a state history into a real number
such that V (h1 )  V (h2 ) just in case the agent considers history h1 at least as good
as h2 . A state history records the progression of states the system assumes along
with the actions performed. Assumptions such as time-separability and additivity are
common for V . In particular, we generally use a reward function R and cost function
C when defining value.
 The horizon T . This is the number of stages over which the state histories should be
evaluated using V .
15. Many algorithms in the AI literature produce a partially ordered sequence of actions. These plans do
not, however, involve conditional or nondeterministic execution. Rather, they represent the fact that
any linear sequence consistent with the partial order will solve the problem. Thus, a partially ordered
plan is a concise representation for a particular set of straight-line plans.

20

fiDecision-Theoretic Planning: Structural Assumptions

 An optimality criterion. This provides a criterion for evaluating potential solutions
to planning problems.

2.10.2 Common Planning Problems

We can use this general framework to classify various problems commonly studied in the
planning and decision-making literature. In each case below, we note the modeling assumptions that define the problem class.

Planning Problems in the OR/Decision Sciences Tradition
 Fully Observable Markov Decision Processes (FOMDPs) | There is an ex-

tremely large body of research studying FOMDPs, and we present the basic algorithmic techniques in some detail in the next section. The most commonly used formulation of FOMDPs assumes full observability and stationarity, and uses as its optimality
criterion the maximization of expected total reward over a finite horizon, maximization of expected total discounted reward over an infinite horizon, or minimization of
the expected cost to a goal state.
FOMDPs were introduced by Bellman (1957) and have been studied in depth in the
fields of decision analysis and OR, including the seminal work of Howard (1960). Recent texts on FOMDPs include (Bertsekas, 1987) and (Puterman, 1994). Average reward optimality has also received attention in this literature (Blackwell, 1962; Howard,
1960; Puterman, 1994). In the AI literature, discounted or total reward models have
been most popular as well (Barto et al., 1995; Dearden & Boutilier, 1997; Dean, Kaelbling, Kirman, & Nicholson, 1995; Koenig, 1991), though the average-reward criterion
has been proposed as more suitable for modeling AI planning problems (Boutilier &
Puterman, 1995; Mahadevan, 1994; Schwartz, 1993).

 Partially Observable Markov Decision Processes (POMDPs) | POMDPs

are closer than FOMDPs to the general model of decision processes we have described.
POMDPs have generally been studied with the assumption of stationarity and optimality criteria identical to those of FOMDPs, though the average-reward criterion
has not been widely considered. As we discuss below, a POMDP can be viewed as
a FOMDP with a state space consisting of the set of probability distributions over
S . These probability distributions represent states of belief: the agent can \observe"
its state of belief about the system although it does not have exact knowledge of the
system state itself.
POMDPs have been widely studied in OR and control theory (Astrom, 1965; Lovejoy, 1991b; Smallwood & Sondik, 1973; Sondik, 1978), and have drawn increasing
attention in AI circles (Cassandra, Kaelbling, & Littman, 1994; Hauskrecht, 1998;
Littman, 1996; Parr & Russell, 1995; Simmons & Koenig, 1995; Thrun, Fox, & Burgard, 1998; Zhang & Liu, 1997). Inuence diagrams (Howard & Matheson, 1984;
Shachter, 1986) are a popular model for decision making in AI and are, in fact, a
structured representational method for POMDPs (see Section 4.3).

Planning Problems in the AI Tradition
21

fiBoutilier, Dean, & Hanks

 Classical Deterministic Planning | The classical AI planning model assumes

deterministic actions: any action ak taken at any state si has at most one successor sj .
The other important assumptions are non-observability and that value is determined
by reaching a goal state: any plan that leads to a goal state is preferred to any
that does not. Often there is a preference for shorter plans; this can be represented
by using a discount factor to \encourage" faster goal achievement or by assigning a
cost to actions. Reward is associated only with transitions to goal states, which are
absorbing. Action costs are typically ignored, except as noted above.
In classical models it is usually assumed that the initial state is known with certainty.
This contrasts with the general specification of MDPs above, which does not assume
knowledge of or even distributional information about the initial state. Policies are
defined to be applicable no matter what state (or distribution over states) one finds
oneself in|action choices are defined for every possible state or history. Knowledge of
the initial state and determinism allow optimal straight-line plans to be constructed,
with no loss in value associated with non-observability, but unpredictable exogenous
events and uncertain action effects cannot be modeled consistently if these assumptions are adopted.
For an overview of early classical planning research and the variety of approaches
adopted, see (Allen, Hendler, & Tate, 1990) as well as Yang's (1998) recent text.

 Optimal Deterministic Planning | A separate body of work retains the classical

assumptions of complete information and determinism, but tries to recast the planning
problem as an optimization that relaxes the implicit assumption of \achieve the goal
at all costs." At the same time, these methods use the same sort of representations
and algorithms applied to satisficing planning.
Haddawy and Hanks (1998) present a multi-attribute utility model for planners that
keeps the explicit information about the initial state and goals, but allows preferences to be stated about the partial satisfaction of the goals as well as the cost of the
resources consumed in satisfying them. The model also allows the expression of preferences over phenomena like temporal deadlines and maintenance intervals that are
dicult to capture using a time-separable additive value function. Williamson (1996)
(see also Williamson & Hanks, 1994). implements this model by extending a classical planning algorithm to solve the resulting optimization problem. Haddawy and
Suwandi (1994) also implement this model in a complete decision-theoretic framework.
Their model of planning, refinement planning, differs somewhat from the generative
model discussed in this paper. In their model the set of all possible plans is pre-stored
in an abstraction hierarchy, and the problem solver's job is to find in the hierarchy
the optimal choice of concrete actions for a particular problem.
Perez and Carbonell's (1994) work also incorporates cost information into the classical
planning framework, but maintains the split between a classical satisficing planner
and additional cost information provided in the utility model. The cost information is
used to learn search-control rules that allow the classical planner to generate low-cost
goal-satisfying plans.
22

fiDecision-Theoretic Planning: Structural Assumptions

 Conditional Deterministic Planning | The classical planning assumption of

omniscience can be relaxed somewhat by allowing the state of some aspects of the
world to be unknown. The agent is thus in a situation where it is certain that the
system is one of a particular set of states, but does not know which one. Unknown
truth values can be included in the initial state specification, and taking actions can
cause a proposition to become unknown as well.
Actions can provide the agent with information while the plan is being executed: conditional planners introduce the idea of actions providing runtime information about
the prevailing state, distinguishing between an action that makes proposition P true
and an action that will tell the agent whether P is true when the action is executed.
An action can have both causal and informational effects, simultaneously changing
the world and reporting on the value of one or more propositions. This second sort
of information is not useful at planning time except that it allows steps in the plan
to be executed conditionally, depending on the runtime information provided by prior
information-producing steps. The value of such actions lies in the fact that different
courses of action may be appropriate under different conditions|these informational
effects allow runtime selection of actions based on the observations produced, much
like the general POMDP model.
Examples of conditional planners in the classical framework include early work by
Warren (1976) and the more recent CNLP (Peot & Smith, 1992), Cassandra (Pryor
& Collins, 1993), Plynth (Goldman & Boddy, 1994), and UWL (Etzioni, Hanks,
Weld, Draper, Lesh, & Williamson, 1992) systems.

 Probabilistic Planning Without Feedback | A direct probabilistic extension

of the classical planning problem can be stated as follows (Kushmerick et al., 1995):
take as input (a) a probability distribution over initial states, (b) stochastic actions
(explicit or implicit transition matrices), (c) a set of goal states, and (d) a probability
success threshold  . The objective is to produce a plan that reaches any goal state
with probability at least  , given the initial state distribution. No provision is made
for execution-time observation, thus straight-line plans are the only form of policy
possible. This is a restricted case of the infinite-horizon NOMDP problem, one in
which actions incur no cost and goal states offer positive reward and are absorbing.
It is also a special case in that the objective is to find a satisficing policy rather than
an optimal one.

 Probabilistic Planning With Feedback | Draper et al. (1994a) have proposed

an extension of the probabilistic planning problem in which actions provide feedback,
using exactly the observation model described in Section 2.4. Again, the problem is
posed as that of building a plan that leaves the system in a goal state with sucient
probability. But a plan is no longer a simple sequence of actions|it can contain conditionals and loops whose execution depends on the observations generated by sensing
actions. This problem is a restricted case of the general POMDP problem: absorbing goal states and cost-free actions are used, and the objective is to find any policy
(conditional plan) that leaves the system in a goal state with sucient probability.
23

fiBoutilier, Dean, & Hanks

Comparing the Frameworks: Task-oriented Versus Process-oriented Problems

It is useful at this point to pause and contrast the types of problems considered in the classical planning literature with those typically studied within the MDP framework. Although
problems in the AI planning literature have emphasized a goal-pursuit or \one-shot" view of
problem solving, in some cases viewing the problem as an infinite-horizon decision problem
results in a more satisfying formulation. Consider our running example involving the oce
robot. It is simply not possible to model the problem of responding to coffee requests, mail
arrival and keeping the lab tidy as a strict goal-satisfaction problem while capturing the
possible nuances of intuitively optimal behavior.
The primary diculty is that no explicit and persistent goal states exist. If we were
simply to require that the robot attain a state where the lab is tidy, no mail awaits, and no
unfilled coffee requests exist, no \successful" plan could anticipate possible system behavior
after a goal state was reached. The possible occurrence of exogenous events after goal
achievement requires that the robot bias its methods for achieving its goals in a way that
best suits the expected course of subsequent events. For instance, if coffee requests are
very likely at any point in time and unmet requests are highly penalized, the robot should
situate itself in the coffee room in order to satisfy an anticipated future request quickly.
Most realistic decision scenarios involve both task-oriented and process-oriented behavior,
and problem formulations that take both into account will provide more satisfying models
for a wider range of situations.
2.10.3 The Complexity of Policy Construction

We have now defined the planning problem in several different ways, each having a different
set of assumptions about the state space, system dynamics and actions (deterministic or
stochastic), observability (full, partial, or none), value function (time-separable, goal only,
goal rewards and action costs, partially satisfiable goals with temporal deadlines), planning
horizon (finite, infinite, or indefinite), and optimality criterion (optimal or satisficing solutions). Each set of assumptions puts the corresponding problem in a particular complexity
class, which defines worst-case time and space bounds on any representation and algorithm
for solving that problem. Here we summarize known complexity results for each of the
problem classes defined above.
Fully Observable Markov Decision Processes Fully observable MDPs (FOMDPs)
with time-separable, additive value functions can be solved in time polynomial in the size
of the state space, the number of actions, and the size of the inputs.16 The most common algorithms for solving FOMDPs are value iteration and policy iteration, which are
described in the next section. Both finite-horizon and discounted infinite-horizon problems
require a polynomial amount of computation per iteration|O(jS j2 jAj) and O(jS j2 jAj+jS j3 ),
respectively|and converge in a polynomial number of iterations (with factor 1,1  in the
discounted case). On the other hand, these problems have been shown to be P-complete
(Papadimitriou & Tsitsiklis, 1987), which means that an ecient parallel solution algorithm
is unlikely.17 The space required to store the policy for an n-stage finite-horizon problem
16. More precisely, the maximum number of bits required to represent any of the transition probabilities or
costs.
17. See (Littman, Dean, & Kaelbling, 1995) for a summary of these complexity results.

24

fiDecision-Theoretic Planning: Structural Assumptions

is O(jS jn). For most interesting classes of infinite-horizon problems, specifically those involving discounted models with time-separable additive reward, the optimal policy can be
shown to be stationary, and the policy can be stored in O(jS j) space.
Bear in mind that these are worst-case bounds. In many cases, better time bounds and
more compact representations can be found. Sections 4 and 5 explore ways to represent
and solve these problems more eciently.
Partially Observable Markov Decision Processes POMDPs are notorious for their
computational diculty. As mentioned above, a POMDP can be viewed as a FOMDP
with an infinite state space consisting of probability distributions over S , each distribution
representing the agent's state of belief at a point in time (Astrom, 1965; Smallwood &
Sondik, 1973). The problem of finding an optimal policy for a POMDP with the objective
of maximizing expected total reward or expected total discounted reward over a finite
horizon T has been shown to be exponentially hard both in jS j and in T (Papadimitriou
& Tsitsiklis, 1987). The problem of finding a policy that maximizes or approximately
maximizes the expected discounted total reward over an infinite horizon is shown to be
undecidable (Madani, Condon, & Hanks, 1999).
Even restricted cases of the POMDP problem are computationally dicult in the worst
case. Littman (1996) considers the special case of boolean rewards: determining whether
there is an infinite-horizon policy with nonzero total reward given that the rewards associated with all states are non-negative. He shows that the problem is EXPTIME-complete if
the transitions are stochastic, and PSPACE-hard if the transitions are deterministic.
Deterministic Planning Recall that the classical planning problem is defined quite
differently from the MDP problems above: the agent has no ability to observe the state but
has perfect predictive powers, knowing the initial state and the effects of all actions with
certainty. In addition, rewards come only from reaching a goal state, and any plan that
achieves the goal suces.
Planning problems are typically defined in terms of a set P of boolean features or
propositions: a complete assignment of truth values to features describes exactly one state,
and a partial assignment of truth values describes a set of states. A set of propositions P
induces a state space of size 2jPj. Thus, the space required to represent a planning problem
using a feature-based representation can be exponentially smaller than that required by a
at representation for the same problem (see Section 4).
The ability to represent planning problems compactly has a dramatic impact on worstcase complexity. Bylander (1994) shows that the deterministic planning problem without
observation is PSPACE-complete. Roughly speaking, this means that at worst planning
time will increase exponentially with P and A, and further, that the size of a solution plan
can grow exponentially with the problem size. These results hold even when the action
space A is severely restricted. For example, the planning problem is NP-complete even
in cases where each action is restricted to one precondition feature and one postcondition
feature. Conditional and optimal planning are PSPACE-complete as well. These results
are for inputs that are generally more compact (generally exponentially so) than those in
terms of which the complexity of the FOMDP and POMDP problems are phrased.
Probabilistic Planning In probabilistic goal-oriented planning, as for POMDPs, we
typically search for a solution in a space of probability distributions over states (or over
25

fiBoutilier, Dean, & Hanks

formulas that describe states). Even the simplest problem in probabilistic planning|one
that admits no observability|is undecidable at worst (Madani et al., 1999). The intuition
is that even though the set of states is finite, the set of distributions over those states is not,
and at worst the agent may have to search an infinite number of plans before being able
to determine whether or not a solution exists. An algorithm can be guaranteed to find a
solution plan eventually if one exists, but cannot be guaranteed to terminate in finite time
if there is no solution plan. Conditional probabilistic planning is a generalization of the
non-observable probabilistic planning problem, and thus is undecidable as well.
It is interesting to note a connection between conditional probabilistic planning and
POMDPs. The actions and observations of the two problems have equivalent expressive
power, but the reward structure of the conditional probabilistic planning problem is quite
restrictive: goal states have positive rewards, all other states have no reward, and goal states
are absorbing. Since we cannot put an a priori bound on the length of a solution plan,
conditional probabilistic planning must be viewed as an infinite-horizon problem where the
objective is to maximize total expected undiscounted reward. Note, however, that since goal
states are absorbing, we can guarantee that total expected reward will be non-negative and
bounded, even over an infinite horizon. Technically this means that the conditional probabilistic planning problem is a restricted case of an infinite-horizon positive-bounded problem
(Puterman, 1994, Section 7.2). We can therefore conclude that the problem of solving an
arbitrary infinite-horizon undiscounted positive-bounded POMDP is also undecidable. The
more commonly studied problem is the infinite-horizon POMDP with a criterion of maximizing expected discounted total reward; finding optimal or near-optimal solutions to that
problem is also undecidable, as noted above.
2.10.4 Conclusion

We end this section by noting again that these results are algorithm-independent and describe worst-case behavior. In effect, they indicate how badly any algorithm can be made to
perform on an \arbitrarily unfortunate" problem instance. The more interesting question
is whether we can build representations, techniques, and algorithms that typically perform
well on problem instances that typically arise in practice. This concern leads us to examine
the problem characteristics with an eye toward exploiting the restrictions placed on the
states and actions, on observability, and on the value function and optimality criterion. We
begin with algorithmic techniques that focus on the value function|particularly those that
take advantage of time-separability and goal orientation. Then in the following section we
explore complementary techniques for building compact problem representations.

3. Solution Algorithms: Dynamic Programming and Search
In this section we review standard algorithms for solving the problems described above in
terms of the \unstructured" or \at" problem representations. As noted in the analysis
above, fully observable Markov decision processes (FOMDPs) are by far the most widely
studied models in this general class of stochastic sequential decision problems. We begin by
describing techniques for solving FOMDPs, focusing on techniques that exploit structure in
the value function like time-separability and additivity.
26

fiDecision-Theoretic Planning: Structural Assumptions

3.1 Dynamic Programming Approaches

Suppose we are given a fully-observable MDP with a time-separable, additive value function.
In other words, we are given the state space S , action space A, a transition matrix Pr(s0 js; a)
for each action a, a reward function R, and a cost function C . We start with the problem
of finding the policy that maximizes expected total reward for some fixed, finite-horizon T .
Suppose we are given a policy  such that (s; t) is the action to be performed by the agent
in state s with t stages remaining to act (for 0  t  T ).18 Bellman (1957) shows that the
expected value of such a policy at any state can be computed using the set of t-stage-to-go
value functions Vt . We define V0 (s) to be R(s), then define:

Vt (s) = R(s) + C ((s; t)) +

X

s 2S
0

fPr(s0j(s; t); s)Vt,1 (s0)g

(1)

This definition of the value function for  makes its0 dependence on the initial state clear.
We say a policy  is optimal if VT (s)  VT (s) for all policies 0 and all s 2 S .
The optimal T -stage-to-go value function, denoted VT , is simply the value function of any
optimal T -horizon policy. Bellman's principle of optimality (Bellman, 1957) forms the basis
of the stochastic dynamic programming algorithms used to solve MDPs, establishing the
following relationship between the optimal value function at tth stage and the optimal value
function at the previous stage:

Vt (s) = R(s) + max
fC (a) +
a2A

X

s0 2S

Pr(s0 ja; s)Vt,1 (s0 )g

(2)

3.1.1 Value Iteration

Equation 2 forms the basis of the value iteration algorithm for finite-horizon problems.
Value iteration begins with the value function V0 = R, and uses Equation 2 to compute in
sequence the value functions for longer time intervals, up to the horizon T . Any action that
maximizes the right-hand side of Equation 2 can be chosen as the policy element (s; t).
The resulting policy is optimal for the T -stage, fully observable MDP, and indeed for any
shorter horizon t < T .
It is important to note that a policy describes what should be done at every stage and
for every state of the system, even if the agent cannot reach certain states given the system's
initial configuration and its available actions. We return to this point below.

Example 3.1 Consider a simplified version of the robot example, in which we have four

state variables M , CR, RHC, and RHM (movement to various locations is ignored),
and four actions GetC, PUM, DelC, and DelM. Actions GetC and PUM make RHC
and RHM, respectively, true with certainty. Action DelM, when RHM holds, makes
both M and RHM false with probability 1.0; DelC makes both CR and RHC false with
probability 0.3, leaving the state unchanged with probability 0.7. A reward of 3 is
associated with CR and a reward of 1 is associated with M . The reward for any state
is the sum of the rewards for each objective satisfied in that state. Figure 11 shows
the optimal 0-stage, 1-stage and 2-stage value functions for various states, along with

18. Recall that for FOMDPs other aspects of history are not relevant.

27

fiBoutilier, Dean, & Hanks

State
s0 = hM; RHM; CR; RHCi
s1 = hM; RHM; CR; RHCi
s2 = hM; RHM; CR; RHCi
s3 = hM; RHM; CR; RHCi
s4 = hM; CR; RHCi
s5 = hM; CR; RHCi
s6 = hM; RHM; CRi
s7 = hM; RHM; CRi
s8 = hM; CRi

V0

0
0
0
0
1
1
3
3
4

V1

0
1
0.9
1
2.9
2
7
6
8

 (1)

 (2)
any
1 PUM
DelM 2 DelM
DelC 2.43 DelC
DelM 2.9 DelM
DelC 5.43 DelC
any 3.9 GetC
DelM 11 DelM
any 10 PUM
any 12 any
V2

Figure 11: Finite-horizon optimal value and policy.
the optimal choice of action at each state-stage pairing (the values for any \state"
with missing variables hold for all instantiations of those variables). Note that V0 (s)
is simply R(s) for each state s.
To illustrate the application of Equation 2, first consider the calculation of V1 (s3 ).
The robot has the choice of delivering coffee or delivering mail, and the expected value
of each option, with one stage remaining, is given by:
EV1 (s3; DelC) = 0:3V0(s6) + 0:7V0 (s3) = 0:9
EV1(s3; DelM) =
1:0V0 (s4 )
= 1:0
Thus  (s3 ; 1) = DelM and V1 (s3 ) is the value of this maximizing choice. Notice that
the robot with one action to perform will aim for the \lesser" objective M due to the
risk of failure inherent in delivering coffee. With two stages remaining at the same
state, the robot will again deliver mail, but with certainty will move to s4 with one
stage to go, where it will attempt to deliver coffee ( (s4 ; 1) = DelC).
To illustrate the effects a fixed finite horizon can have on policy choice, note that
 (s0; 2) = PUM. With two stages remaining and the choice of getting mail or coffee,
the robot will get mail because subsequent delivery (in the last stage) is guaranteed
to succeed, whereas subsequent coffee delivery may fail. However, if we compute
 (s0; 3), we see:
EV3(s0 ; GetC) = 1:0V2(s2 ) = 2:43
EV3 (s0; PUM) = 1:0V2(s1 ) = 2:0
With three stages to go, the robot will instead retrieve coffee at s0 . Once it has
coffee, it has two chances at successful delivery. The expected value of this course
of action is greater than that of (guaranteed) mail delivery. Note that three stages
does not allow sucient time to try to achieve both objectives at s0 . In fact, the
larger reward associated with coffee delivery ensures that with any greater number of
stages remaining, the robot should focus first on coffee retrieval and delivery, and then
attempt mail retrieval and delivery once coffee delivery is successfully completed. 2
Often we are faced with tasks that do not have a fixed finite horizon. For example, we
may want our robot to perform the tasks of keeping the lab tidy, picking up mail whenever it
28

fiDecision-Theoretic Planning: Structural Assumptions

arrives, responding to coffee requests, and so on. There is no fixed time horizon associated
with these tasks|they are to be performed as the need arises. Such problems are best
modeled as infinite-horizon problems.
We consider here the problem of building a policy that maximizes the discounted sum
of expected rewards over an infinite horizon.19 Howard (1960) showed that there always
exists an optimal stationary policy for such problems. Intuitively, this is the case because
no matter what stage the process is in, there are still an infinite number of stages remaining;
so the optimal action at any state is independent of the stage. We can therefore restrict
our attention to policies that choose the same action for a state regardless of the stage of
the process. Under this restriction, the policy will have the same size jSj regardless of the
number of stages over which the policy is executed|the policy  has the form  : S ! A.
In contrast, optimal policies for finite-horizon problems are generally nonstationary, as
illustrated in Example 3.1.
Howard also shows that the value of policy  satisfies the following recurrence:

V  (s) = R(s) + fC ((s)) + 

X

s0 2S

and that the optimal value function V  satisfies:

V (s) = R(s) + max
fC (a) + 
a2A

Pr(s0 j(s); s)V  (s0 )g

X
s 2S
0

Pr(s0 ja; s)V  (s0 )g

(3)
(4)

The value of a fixed policy  can be evaluated using the method of successive approximation, which is almost identical to the procedure described in Equation 1 above. We begin
with an arbitrary assignment of values to V0 (s), then define:

Vt (s) = R(s) + C ((s; t)) + 

X

s 2S
0

fPr(s0j(s; t); s)Vt,1 (s0)g

(5)

The sequence of functions Vt converges linearly to the true value function V  .
One can also alter the value-iteration algorithm slightly so it builds optimal policies for
infinite-horizon discounted problems. The algorithm starts with a value function V0 that
assigns an arbitrary value to each s 2 S . Given value estimate Vt (s) for each state s, Vt+1 (s)
is calculated as:

Vt+1 (s) = R(s) + max
fC (a) + 
a2A

X

s0 2S

Pr(s0 ja; s)  Vt (s0 )g

(6)

The sequence of functions Vt converges linearly to the optimal value function V  (s). After
some finite number of iterations n, the choice of maximizing action for each s forms an
optimal policy  and Vn approximates its value.20
19. This is by far the most commonly studied problem in the literature, though it is argued in (Boutilier &
Puterman, 1995; Mahadevan, 1994; Schwartz, 1993) that such problems are often best modeled using
average reward per stage as the optimality criterion. For a discussion of average reward optimality and
its many variants and refinements, see (Puterman, 1994).
20. The number of iterations n is based on a stopping criterion that generally involves measuring the difference between Vt and Vt+1 . For a discussion of stopping criteria and convergence of the algorithm, see
(Puterman, 1994).

29

fiBoutilier, Dean, & Hanks

3.1.2 Policy Iteration

Howard's (1960) policy-iteration algorithm is an alternative to value iteration for infinitehorizon problems. Rather than iteratively improving the estimated value function, it instead
modifies the policies directly. It begins with an arbitrary policy 0 , then iterates, computing
i+1 from i.
Each iteration of the algorithm comprises two steps, policy evaluation and policy improvement:
1. (Policy evaluation) For each s 2 S , compute the value function V i (s) based on the
current policy i .
2. (Policy improvement) For each s 2 S , find the action a that maximizes

Qi+1(a; s) = R(s) + C (a) + 

X

s0 2S

Pr(s0 ja; s)  V i (s0 )

(7)

If Qi+1 (a ; s) > V i (s), let i+1 = a ; otherwise i+1 (s) = i (s).21
The algorithm iterates until i+1 (s) = i (s) for all states s. Step 1 evaluates the current
policy by solving the N  N linear system represented by Equation 3 (one equation for
each s 2 S ), and can be computationally expensive. However, the algorithm converges to
an optimal policy at least linearly and under certain conditions converges superlinearly or
quadratically (Puterman, 1994). In practice, policy iteration tends to converge in many
fewer iterations than does value iteration. Policy iteration thus spends more computational
time at each individual stage, with the result that fewer stages need be computed.22
Modified policy iteration (Puterman & Shin, 1978) provides a middle ground between
policy iteration and value iteration. The structure of the algorithm is exactly the same as
that of policy iteration, alternating evaluation and improvement phases. The key insight is
that one need not evaluate a policy exactly in order to improve it. Therefore, the evaluation
phase involves some (usually small) number of iterations of successive approximation (i.e.,
setting V  = Vt for some small t, using Equation 6). With some tuning of the value
of t used at each iteration, modified policy iteration can work extremely well in practice
(Puterman, 1994). Both value iteration and policy iteration are special cases of modified
policy iteration, corresponding to setting t = 0 and t = 1, respectively.
A number of other variants of both value and policy iteration have been proposed. For
instance, asynchronous versions of these algorithms do not require that the value function
be constructed, or policy improved, at each state in lockstep. In the case of value iteration
for infinite-horizon problems, as long as each state is updated suciently often, convergence
can be assured. Similar guarantees can be provided for asynchronous forms of policy iteration. Such variants are important tools for understanding various online approaches to
solving MDPs (Bertsekas & Tsitsiklis, 1996). For a nice discussion of asynchronous dynamic
programming, see (Bertsekas, 1987; Bertsekas & Tsitsiklis, 1996).
21. The Q-function defined by Equation 7, and so called because of its use in Q-learning (Watkins & Dayan,
1992), gives the value of performing action a at state s assuming the value function V  accurately reects
future value.
22. See (Littman et al., 1995) for a discussion of the complexity of the algorithm.

30

fiDecision-Theoretic Planning: Structural Assumptions

3.1.3 Undiscounted Infinite-Horizon Problems

The diculty with finding optimal solutions to infinite-horizon problems is that total reward
can grow without limit over time. Thus, the problem definition must provide some way to
ensure that the value metric is bounded over arbitrarily long horizons. The use of expected
total discounted reward as the optimality criterion offers a particularly elegant way to
guarantee a bound, since the infinite sum of discounted rewards is finite. However, although
discounting is appropriate for certain classes of problems (e.g., economic problems, or those
where the system may terminate at any point with a certain probability), for many realistic
AI domains it is dicult to justify counting future rewards less than present rewards, and
the discounted-reward criterion is not appropriate.
There are a variety of ways to bound total reward in undiscounted problems. In some
cases the problem itself is structured so that reward is bounded. In planning problems, for
example, the goal reward can be collected at most once, and all actions incur a cost. In
that case total reward is bounded from above and the problem can legitimately be posed
in terms of maximizing total expected undiscounted reward in many cases (e.g., if the goal
can be reached with certainty).
In cases where discounting is inappropriate and total reward is unbounded, different
success criteria can be employed. For example, the problem can instead be posed as one
in which we wish to maximize expected average reward per stage, or gain. Computational
techniques for constructing gain-optimal policies are similar to the dynamic-programming
algorithms described above, but are generally more complicated, and the convergence rate
tends to be quite sensitive to the communicating structure and periodicity of the MDP.
Refinements to gain optimality have also been studied. For example, bias optimality can
be used to distinguish two gain-optimal polices by giving preference to the policy whose total
reward over some initial segment of policy execution is larger. Again, while the algorithms
are more complicated than those for discounted problems, they are variants of standard
policy or value iteration. See (Puterman, 1994) for details.
3.1.4 Dynamic Programming and POMDPs

Dynamic programming techniques can be applied in partially observable settings as well
(Smallwood & Sondik, 1973). The main diculty in building policies for situations in which
the state is not fully observable is that, since past observations can provide information
about the system's current state, decisions must be based on information gleaned in the
past. As a result, the optimal policy can depend on all observations the agent has made since
the beginning of execution. These history-dependent policies can grow in size exponential
in the length of the horizon. While history-dependence precludes dynamic programming,
the observable history can be summarized adequately with a probability distribution over S
(Astrom, 1965), and policies can be computed as a function of these distributions, or belief
states.
A key observation of Sondik (Smallwood & Sondik, 1973; Sondik, 1978) is that when
one views a POMDP with a time-separable value function by taking the state space to be
the set of probability distributions over S , one obtains a fully observable MDP that can
be solved by dynamic programming. The (computational) problem with this approach is
31

fiBoutilier, Dean, & Hanks

that the state space for this FOMDP is an N -dimensional continuous space,23 and special
techniques must be used to solve it (Smallwood & Sondik, 1973; Sondik, 1978).
We do not explore these techniques here, but note that they are currently practical
only for very small problems (Cassandra et al., 1994; Cassandra, Littman, & Zhang, 1997;
Littman, 1996; Lovejoy, 1991b). A number of approximation methods, developed both in
OR (Lovejoy, 1991a; White III & Scherer, 1989) and AI (Brafman, 1997; Hauskrecht, 1997;
Parr & Russell, 1995; Zhang & Liu, 1997), can be used to increase the range of solvable
problems, but even these techniques are presently of limited practical value.
POMDPs play a key role in reinforcement learning as well, where the \natural state
space" consisting of agent observations provides incomplete information about the underlying system state (see, e.g., McCallum, 1995).

3.2 AI Planning and State-Based Search
We noted in Section 2.7 that the classical AI planning problem can be formulated as an
infinite-horizon MDP and can therefore be solved using an algorithm like value iteration.
Recall that two assumptions in classical planning specialize the general MDP model, namely
determinism of actions and the use of goal states instead of a more general reward function.
A third assumption|that we want to construct an optimal course of action starting from a
known initial state|does not have a counterpart in the FOMDP model as presented above,
since the policy dictates the optimal action from any state at any stage of the plan. As we
will see below, the interest in online algorithms within AI has led to revised formulations
of FOMDPs that do take initial and current states into account.
Though we defined the classical planning problem earlier as a non-observable process
(NOMDP), it can be solved as if it were fully observable. We let G be the set of goal states
and sinit be the initial state. Applying value iteration to this type of problem is equivalent
to determining the reachability of goal states from all system states. For instance, if we
make goal states absorbing, assign a reward of 1 to all transitions from any s 2 S , G
to some g 2 G and 0 to all others, then the set of all states where Vk (s) > 0 is exactly
the set of states that can lead to a goal state.24 In particular, if Vk (sinit ) > 0, then a
successful plan can be constructed by extracting actions from the k-stage (finite-horizon)
policy produced by value iteration. The determinism assumption means that the agent can
predict the state perfectly at every stage of execution; the fact that it cannot observe the
state is unimportant.
The assumptions commonly made in classical planning can be exploited computationally in value iteration. First, we can terminate the process at the first iteration k where
Vk (sinit) > 0, because we are interested only in plans that begin at sinit, not in acting
optimally from every possible start state. Second, we can terminate value iteration after jS j
iterations: if VjS j(sinit ) = 0 at that point, the algorithm will have searched every possible
state and can guarantee that no solution plan exists. Therefore, we can view classical planning as a finite-horizon decision problem with a horizon of jS j. This use of value iteration
23. More accurately, it is an N -dimensional simplex, or (N , 1)-dimensional space.
24. Specifically, Vk (s) indicates the probability with which one reaches the goal region under the optimal
policy from s 2 S , G in stochastic settings. In the deterministic case being discussed, this value must
be 1 or 0.

32

fiDecision-Theoretic Planning: Structural Assumptions

is equivalent to using the Floyd-Warshall algorithm to find a minimum-cost path through
a weighted graph (Floyd, 1962).
3.2.1 Planning and Search

While value iteration can, in theory, be used for classical planning, it does not take advantage
of the fact that the goal and initial states are known. In particular, it computes the value
and policy assignment for all states at all stages. This can be very wasteful since optimal
actions will be computed for states that cannot be reached from sinit or that cannot possibly
lead to any state g 2 G. It is also problematic when jS j is large, since each iteration of
value iteration requires O(jS jjAj) computations. For this reason dynamic programming
approaches have not been used extensively in AI planning.
The restricted form of the value function, especially the fact that initial and goal states
are given, makes it more advantageous to view planning as a graph-search problem. Unlike
general FOMDPs, where it is generally not known a priori which states are most desirable
with respect to (long-term) value, the well-defined set of target states in a classical planning
problem makes search-based algorithms appropriate. This is the approach taken by most
AI planning algorithms.
One way to formulate the problem as a graph search is to make each node of the graph
correspond to a state in S . The initial state and goal states can then be identified, and
the search can proceed either forward or backward through the graph, or in both directions
simultaneously.
In forward search, the initial state is the root of the search tree. A node is then chosen
from the tree's fringe (the set of all leaf nodes), and all feasible actions are applied. Each
action application extends the plan by one step (or one stage) and generates a unique new
successor state, which is a new leaf node in the tree. This node can be pruned if the state it
defines is already in the tree. The search ends when a state is identified as a member of the
goal set (in which case a solution plan can be extracted from the tree), or when all branches
have been pruned (in which case no solution plan exists). Forward search attempts to build
a plan from beginning to end, adding actions to the end of the current sequence of actions.
Forward search never considers states that cannot be reached from the sinit .
Backward search can be viewed in several different ways. We could arbitrarily select
some g 2 G as the root of the search tree, and expand the search tree at the fringe by
selecting a state on the fringe and adding to the tree all states such that some action would
cause the system to enter the chosen state. In general, an action can give rise to more than
one predecessor vertex, even if actions are deterministic. A state can again be pruned if it
appears in the search tree already. The search terminates when the initial state is added to
the tree, and a solution plan can again be extracted from the tree. This search is similar
to dynamic-programming-based algorithms for finding the shortest path through a graph.
The difference is that backward search considers only those states at a depth k in the search
tree that can reach the chosen goal state within k steps. Dynamic programming algorithms,
in contrast, visit every state at every stage of the search.
One diculty with the backward approach as described above is the commitment to
a particular goal state. Of course, this assumption can be relaxed, as an algorithm could
\simultaneously" search for paths to all goal states by adding at the first level of the search
33

fiBoutilier, Dean, & Hanks

tree any vertex that can reach some g 2 G. We will see in Section 5 that goal regression
can be viewed as doing this, at least implicitly.
It is generally thought that regression (or backward) techniques are more effective in
practice than progression (or forward) methods. The reasoning is that the branching factor
in the forward graph, which is the number of actions that can feasibly be applied in a given
state, is substantially larger than the branching factor in the reverse graph, which is the
number of operators that could bring the system into a given state.25 This is especially true
when goal sets are represented by a small set of propositional literals (Section 5). The two
approaches are not mutually exclusive, however: one can mix forward and backward expansions of the underlying problem graph and terminate when a forward path and backward
path meet.
The important thing to observe about these algorithms is that they restrict their attention to the relevant and reachable states. In forward search, only those states that can be
reached from sinit are ever considered: this can provide benefit over dynamic programming
methods if few states are reachable, since unreachable states cannot play a role in constructing a successful plan. In backward approaches, similarly, only states lying on some path
to the goal region G are considered, and this can have significant advantages over dynamic
programming if only a fraction of the state space is connected to the goal region.
In contrast, dynamic programming methods (with the exception of asynchronous methods) must examine the entire state space at every iteration. Of course, the ability to ignore
parts of the state space comes from planning's stringent definition of what is relevant: states
in G have positive reward, no other states matter except to the extent they move the agent
closer to the goal, and the choice of action at states unreachable from sinit is not of interest.
While state-based search techniques use knowledge of a specific initial state and a specific
goal set to constrain the search process, forward search does not exploit knowledge of the
goal set, nor does backward search exploit knowledge of the initial state. The GraphPlan
algorithm (Blum & Furst, 1995) can be viewed as a planning method that integrates the
propagation of forward reachability constraints with backward goal-informed search. We
describe this approach in Section 5. Furthermore, work on partial order planning (POP)
can be viewed as a slightly different approach to this form of search. It too is described
in Section 5, after we discuss feature-based or intensional representations for MDPs and
planning problems.
3.2.2 Decision Trees and Real-time Dynamic Programming

State-based search techniques are not limited to deterministic, goal-oriented domains. Knowledge of the initial state can be exploited in more general MDPs as well, forming the basis of
decision tree search algorithms. Assume we have been given a finite-horizon FOMDP with
horizon T and initial state sinit . A decision tree rooted at sinit is constructed in much the
same way as a search tree for a deterministic planning problem (French, 1986). Each action
applicable at sinit forms level 1 of the tree. The states s0 that result with positive probability when any of those actions occur are applied at sinit are placed at level 2, with an arc
25. See Bacchus et al. (1995, 1998) for some recent work that makes the case for progression with good
search control, and Bonet et al. (1997) who argue that progression in deterministic planning is useful
when integrating planning and execution.

34

fiDecision-Theoretic Planning: Structural Assumptions

s init
V = max(V1 , V2 )
a1
p1

a2
p2

s1
a1

p3

s2
a2

a1

V1
p4

s3
a2

a1

V2 = p V 3 + p V4
3
4

s4
a2

a1

a2

V3

V4

Figure 12: The initial stages of a decision tree for evaluating action choices at sinit . The
value of an action is the expected value of its successor states, while the value
of a state is the maximum of the values of its successor actions (as indicated by
dashed arrows at selected nodes).
labeled with probability Pr(s0 ja; sinit ) relating s0 with a. Level 3 has the actions applicable
at the states at level 2, and so on, until the tree is grown to depth 2T , at which point each
branch of the tree is a path consisting of a positive-probability length-T trajectory rooted
at sinit (see Figure 12).
The relevant part of the optimal T -stage value function and the optimal policy can easily
be computed using this tree. We say that value of any node in the tree labeled with an
action is the expected value of its successor states in the tree (using the probabilities labeling
the arcs), while the value of any node in the tree labeled with state s is the sum of R(s) and
the maximum value of its successor actions.26 The rollback procedure, whereby value at the
leaves of the tree are first computed and then values at successively higher levels of the tree
are determined using the preceding values, is, in fact, a form of value iteration. The value
of any state s at level 2t is precisely VT,t (s) and the maximizing actions form the optimal
finite-horizon policy. This form of value iteration is directed: (T , t)-stage-to-go values
are computed only for states that are reachable from sinit within t steps. Infinite-horizon
problems can be solved in an analogous fashion if one can determine a priori the depth
required (i.e., the number of iterations of value iteration needed) to ensure convergence to
an optimal policy.
Unfortunately, the branching factor for stochastic problems is generally much greater
than that for deterministic problems. More troublesome still is the fact that one must
construct the entire decision tree to be sure that the proper values are computed, and hence
the optimal policy constructed. This stands in contrast with classical planning search,
where attention can be focused on a single branch: if a goal state is reached, the path
constructed determines a satisfactory plan. While worst-case behavior for planning may
require searching the whole tree, decision-tree evaluation is especially problematic because
26. States at level 2T are given value R(s).

35

fiBoutilier, Dean, & Hanks

the entire tree must be generated in general to ensure optimal behavior. Furthermore,
infinite-horizon problems pose the diculty of determining a suciently deep tree.
One way around this diculty is the use of real time search (Korf, 1990). In particular,
real-time dynamic programming, or RTDP, has been proposed in (Barto et al., 1995) as a
way of approximately solving large MDPs in an online fashion. One can interleave search
with execution of an approximately optimal policy using a form of RTDP similar to decisiontree evaluation as follows. Imagine the agent finds itself in a particular state sinit . It can
then build a partial search tree to some depth, perhaps uniformly or perhaps with some
branches expanded more deeply than others. Partial tree construction may be halted due to
time pressure or due to an assessment by the agent that further expansion of the tree may
not be fruitful. When a decision to act must be made, the rollback procedure is applied to
this partial, possibly unevenly expanded decision tree.
Reward values can be used to evaluate the leaves of the tree, but this may offer an
inaccurate picture of the value of nodes higher in the tree. Heuristic information can be
used to estimate the long-term value of states labeling leaves. As with value iteration, the
deeper the tree, the more accurate the estimated value at the root (generally speaking)
for a fixed heuristic. We will see in Section 5 that structured representations of MDPs can
provide a means to construct such heuristics (Dearden & Boutilier, 1994, 1997). Specifically,
with admissible heuristics or upper and lower bounds on the true values of leaf nodes in the
tree, methods such as A* or branch-and-bound search can be used.
A key advantage of integrating search with execution is that the actual outcome of the
action taken can be used to prune from the tree the branches rooted at the unrealized
outcomes. The subtree rooted at the realized state can then be expanded further to make
the next action choice. The algorithm of Hansen and Zilberstein (1998) can be viewed as
a variant of these methods in which stationary policies (i.e., state-action mappings) can be
extracted during the search process.
RTDP is formulated by Barto et al. (1995) more generally as a form of online, asynchronous value iteration. Specifically, the values \rolled backed" can be cached and used
as improved heuristic estimates of the value function at the states in question. This technique is also investigated in (Bonet et al., 1997; Dearden & Boutilier, 1994, 1997; Koenig
& Simmons, 1995), and is closely tied to Korf's (1990) LRTA* algorithm. These value
updates also need not proceed strictly using a decision tree to determine the states; the key
requirement of RTDP is simply that the actual state sinit be one of the states whose value
is updated at each decision-action iteration.
A second way to avoid some of the computational diculties that arise in large search
spaces is to use sampling methods. These methods sample from the space of possible trajectories and use this sampled information to provide estimates of the values of specific courses
of action. This approach is quite common in reinforcement learning (Sutton & Barto, 1998),
where simulation models are often used to generate experience from which a value function
can be learned. In the present context, Kearns, Mansour and Ng (Kearns, Mansour, &
Ng, 1999) have investigated search methods for infinite-horizon MDPs where the successor
states of any specific action are randomly sampled according to the transition distribution.
Thus, rather than expand all successor states, only sampled states are searched. Though
this method is exponential in the \effective" horizon (or mixing rate) of the MDP and is
required to expand all actions, the number of states expanded can be less than that required
36

fiDecision-Theoretic Planning: Structural Assumptions

by full search, even if the underlying transition graph is not sparse. They are able to provide polynomial bounds (ignoring action branching and horizon effects) on the number of
trajectories that need to be sampled in order to generate approximately optimal behavior
with high probability.

3.3 Summary

We have seen that dynamic programming methods and state-based search methods can
both be used for fully observable and non-observable MDPs, with forward search methods interpretable as \directed" forms of value iteration. Dynamic programming algorithms
generally require explicit enumeration of the state space at each iteration, while search
techniques enumerate only reachable states; but the branching factor may require that,
at sucient depth in the search tree, search methods enumerate individual states multiple
times, whereas they are considered only once per stage in dynamic programming. Overcoming this diculty in search requires the use of cycle-checking and multiple-path-checking
methods.
We note that search techniques can be applied to partially observable problems as well.
One way to do this is to search through the space of belief states (just as dynamic programming can be applied to the belief space MDP|see Section 2.10.2). Specifically, belief
states play the role of system states and the stochastic effects of actions on belief states are
induced by specific observation probabilities, since each observation has a distinct, but fixed
effect on any belief state. This type of approach has been pursued in (Bonet & Geffner,
1998; Koenig & Simmons, 1995).

4. Factored Representations

To this point our discussion of MDPs has used an explicit or extensional representation for
the set of states (and actions) in which states are enumerated directly. In many cases it
is advantageous, from both the representational and computational point of view, to talk
about properties of states or sets of states: the set of possible initial states, the set of
states in which action a can be executed, and so on. It is generally more convenient and
compact to describe sets of states based on certain properties or features than to enumerate
them explicitly. Representations in which descriptions of objects substitute for the objects
themselves are called intensional and are the technique of choice in AI systems.
An intensional representation for planning systems is often built by defining a set of
features that are sucient to describe the state of the dynamic system of interest. In the
example in Figure 2, the state was described by a set of six features: the robot's location, the
lab's tidiness, whether or not mail is present, whether or not the robot has mail, whether or
not there is a pending coffee request, and whether or not the robot has coffee. The first and
second features can each take one of five values, and the last four can each take one of two
values (true or false). An assignment of values to the six features completely defines a state;
the state space thus comprises all possible combinations of feature values, with jSj = 400.
Each feature, or factor, is typically assigned a unique symbolic name, as indicated in the
second column of Figure 2. The fundamental tradeoff between extensional and intensional
representations becomes clear in this example. An extensional representation of the coffee
example views the space of possible states as a single variable that takes on 400 possible
37

fiBoutilier, Dean, & Hanks

values, whereas the intensional or factored representation views a state as the cross product
of six variables, each of which takes on substantially fewer values. Generally, the state space
grows exponentially in the number of features required to describe a system.
The fact that the state of a system can be described using a set of features allows one
to adopt factored representations of actions, rewards and other components of an MDP. In
a factored action representation, for instance, one generally describes the effect of an action
on specific state features rather than on entire states. This often provides considerable representational economy. For instance, in the Strips action representation (Fikes & Nilsson,
1971), the state transitions induced by actions are represented implicitly by describing the
effects of actions on only those features that change value when the action is executed.
Factored representations can be very compact when individual actions affect relatively few
features, or when their effects exhibit certain regularities. Similar remarks apply to the
representation of reward functions, observation models, and so on. The regularities that
make factored representations suitable for many planning problems can often be exploited
by planning and decision-making algorithms.
While factored representations have long been used in classical AI planning, similar
representations have also been adopted in the recent use of MDP models within AI. In
this section (Section 4), we focus on the economy of representation afforded by exploiting
the structure inherent in many planning domains. In the following section (Section 5), we
describe how this structure|when made explicit by the factored representations|can be
exploited computationally in plan and policy construction.

4.1 Factored State Spaces and Markov Chains

We begin by examining structured states, or systems whose state can be described using a
finite set of state variables whose values change over time.27 To simplify our illustration of
the potential space savings, we assume that these state variables are boolean. If there are
M such variables, then the size of the state space is jSj = N = 2M . For large M , specifying
or representing the dynamics explicitly using state-transition diagrams or N  N matrices
is impractical. Furthermore, representing a reward function as an N -vector, and specifying
the observational probabilities, is similarly infeasible. In Section 4.2, we define a class of
problems in which the dynamics can be represented in O(M ) space in many cases. We begin
by considering how to represent Markov chains compactly and then consider incorporating
actions, observations and rewards.
We let a state variable X take on a finite number of values and let 
X stand for the
set of possible values. We assume that 
X is finite, though much of what follows can be
applied to countable state and action spaces as well. We say the state space is at if it is
specified using one state variable (this variable is denoted S as in the general model, taking
values from S ). The state space is factored if there is more than one state variable. A state
is any possible assignment of values to these variables. Letting Xi represent the ith state
variable, the state space is the cross product of the value spaces for the individual state
t
variables; that is, S = M
i=1 
Xi . Just as S denotes the state of the process at stage t, we
let Xit be the random variable representing the value of the ith state variable at stage t.
27. These variables are often called uents in the AI literature (McCarthy & Hayes, 1969). In classical
planning, these are the atomic propositions used to describe the domain.

38

fiDecision-Theoretic Planning: Structural Assumptions

A Bayesian network (Pearl, 1988) is a representational framework for compactly representing a probability distribution in factored form. Although these networks have most typically been used to represent atemporal problem domains, we can apply the same techniques
to represent Markov chains, encoding the chain's transition probabilities in the network
structure (Dean & Kanazawa, 1989).
Formally, a Bayes net is a directed acyclic graph in which vertices correspond to random
variables and an edge between two variables indicates a direct probabilistic dependency
between them. A network so constructed also reects implicit independencies among the
variables. The network must be quantified by specifying a probability for each variable
(vertex) conditioned on all possible values of its immediate parents in the graph. In addition,
the network must include a marginal distribution: an unconditional probability for each
vertex that has no parents. This quantification is captured by associating a conditional
probability table (CPT) with each variable in the network. Together with the independence
assumptions defined by the graph, this quantification defines a unique joint distribution
over the variables in the network. The probability of any event over this space can then be
computed using algorithms that exploit the independencies represented within the graphical
structure. We refer to Pearl (1988) for details.
Figures 3(a)-(c) (page 7) are special cases of Bayes nets called \temporal" Bayesian
networks. In these networks, vertices in the graph represent the system's state at different
time points and arcs represent dependencies across time points. In these temporal networks,
each vertex's parent is its temporal predecessor, the conditional distributions are transition
probability distributions, and the marginal distributions are distributions over initial states.
The networks in Figure 3 reect an extensional representation scheme in which states are
explicitly enumerated, but techniques for building and performing inference in probabilistic temporal networks are designed especially for application to factored representations.
Figure 13 illustrates a two-stage temporal Bayes net (2TBN) describing the state-transition
probabilities associated with the Markov chain induced by the fixed policy of executing
the action CClk (repeatedly moving counterclockwise). In a 2TBN, the set of variables is
partitioned into those corresponding to state variables at a given time (or stage) t and those
corresponding to state variables at time t + 1. Directed arcs indicate probabilistic dependencies between those variables in the Markov chain. Diachronic arcs are those directed
from time t variables to time t + 1 variables, while synchronic arcs are directed between
variables at time t + 1. Figure 13 contains only diachronic arcs; synchronic arcs will be
discussed later in this section.
Given any state at time t, the network induces a unique distribution over states at t +1.
The quantification of the network describes how the state of any particular variable changes
as a function of certain state variables. The lack of a direct arc (or more generally a directed
path if there are synchronic arcs among the t + 1 variables) from a variable Xt to another
variable Yt+1 means that knowledge of Xt is irrelevant to the prediction of the (immediate,
or one-stage) evolution of variable Y in the Markov process.
Figure 13 shows how compact this representation can be in the best of circumstances, as
many of the potential links between one stage and the next can be omitted. The graphical
representation makes explicit the fact that the policy (i.e., the action CClk) can affect only
the state variable Loc, and the exogenous events ArrM, ReqC, and Mess can affect only
39

fiBoutilier, Dean, & Hanks

Loc

Loc

T

T

CR

CR

P(Loc t+1 )
Loc t O L C M H
O 0.1 0.9 0 0 0
0 0.1 0.9 0 0
L
0 0 0.1 0.9 0
C
M 0 0 0 0.1 0.9
H 0.9 0 0 0 0.1
P(CR t+1)
CR t
t
f

RHC

t f
1.0 0
0.2 0.8

RHC

RHM

RHM

M

M

Time t

Time t+1

P(RHC t+1)
RHC t t f
t 1.0 0
f
0 1.0

Figure 13: A factored 2TBN for the Markov chain induced by moving counterclockwise
(with selected CPTs shown).
the variables M , CR, and Tidy, respectively.28 Furthermore, the dynamics of Loc (and
the other variables) can be described using only knowledge of the state of their parent
variables; for instance, the distribution over Loc at t +1 depends only on the value of Loc at
the previous stage (e.g., if Loct = O, then Loct+1 = M with probability 0:9 and Loct+1 = O
with probability 0:1). Similarly, CR can become true with probability 0:2 (due to a ReqC
event), but once true, cannot become false (under this simple policy); and RHC remains
true (or false) with certainty if it was true (or false) at the previous stage. Finally, the
effects on the relevant variables are independent. For any instantiation of the variables at
time t, the distribution over next states can be computed by multiplying the conditional
probabilities of relevant t + 1 variables.
The ability to omit arcs from the graph based on the locality and independence of action
effects has a strong effect on the number of parameters that must be supplied to complete
the model. Although the full transition matrix for CClk would be of size 4002 = 160000,
the transition model in Figure 13 requires only 66 parameters.29
The example above shows how 2TBNs exploit independence to represent Markov chains
compactly, but the example is extreme in that there is effectively no relationship between the
variables|the chain can be viewed as the product of six independently evolving processes.
28. We show only some of the CPTs for brevity.
29. In fact, we can exploit the fact that probabilities sum to one and leave one entry unspecified per row of
any CPT or explicit transition matrix. In this case, the 2TBN requires only 48 explicit parameters, while
the transition matrix requires 400  300 = 159; 600 entries. We generally ignore this fact when comparing
the sizes of representations.

40

fiDecision-Theoretic Planning: Structural Assumptions

Loc

Loc

T

T

CR

CR

RHC

RHC

RHM

RHM

M

M

Time t

Time t+1

Loc t RHC t
O
t
L
t
C
t
M
t
H
t
O
f
L
f
C
f
M
f
H
f

P(Loc t+1 )
O L C M H
1.0 0 0 0 0
0 0.1 0.9 0 0
0 0 0.1 0.9 0
0 0 0 0.1 0.9
0.9 0 0 0 0.1
0.1 0.9 0 0 0
0 0.1 0.9 0 0
0 0 0.1 0.9 0
0 0 0 0.1 0.9
P(CR t+1)
0.9 0 0 0 0.1
Loc t RHC t CR t t f

P(RHC t+1)
Loc t RHC t
t f
t 0.0 1.0
O
f 0.0 1.0
O
t 1.0 0.0
L
f 0.0 1.0
L
t 1.0 0.0
C
f 0.0 1.0
C
etc.
etc.

O
O
O
O
L
L
L
L

t
t
f
f
t
t
f
f
etc.

t
f
t
f
t
f
t
f

.05 .95
0.2 0.8
1.0 0.0
0.2 0.8
1.0 0.0
0.2 0.8
1.0 0.0
0.2 0.8
etc.

Figure 14: A 2TBN for the Markov chain induced by moving counterclockwise and delivering coffee.

41

fiBoutilier, Dean, & Hanks

In general, these \subprocesses" will interact, but still exhibit certain independencies and
regularities that can be exploited by a 2TBN representation. We consider two distinct
Markov chains that exhibit different types of dependencies.
Figure 14 illustrates a 2TBN representing the Markov chain induced by the following
policy: the robot consistently moves counterclockwise unless it is in the oce and has
coffee, in which case it delivers coffee to the user. Notice that different variables are now
dependent: for instance, predicting the value of RHC at t + 1 requires knowing the values
of Loc and RHC at t. The CPT for RHC shows that the robot retains coffee at stage t + 1
with certainty, if it has it at stage t, in all locations except O (where it executes DelC,
thus losing the coffee). The variable Loc also depends on the value of RHC. The location
will change as in Figure 13 with one exception: if the robot is in the oce with coffee, the
location remains the same (since the robot does not move, but executes DelC). The effect
on the variable CR is explained as follows: if the robot is in the oce and delivers coffee in
its possession, it will fulfill any outstanding coffee request. However, the 0:05 chance of CR
remaining true under these conditions indicates a 5% chance of spilling the coffee.
Even though there are more dependencies (i.e., additional diachronic arcs) in this 2TBN,
it still requires only 118 parameters. Again, the distribution over resulting states is determined by multiplying the conditional distributions for the individual variables. Even though
the variables are \related," when the state S t is known, the variables at time t + 1 (Loct+1 ,
RHCt+1 , etc.) are independent. In other words,
Pr(Loct+1 ; T t+1 ; CRt+1 ; RHCt+1 ; RHMt+1 ; M t+1 jS t ) =
t)
Pr(Loct+1 jS t ) Pr(T t+1 jS t ) Pr(CRt+1 jS t ) Pr(RHCt+1 jS t ) Pr(RHMt+1 jS t ) Pr(M t+1 jS(8)
Figure 15 illustrates a 2TBN representing the Markov chain induced by the same policy
as above, but where we assume that the act of moving counterclockwise has a slightly
different effect. We now suppose that, when the robot moves from the hallway into some
adjacent location, it has a 0:3 chance of spilling any coffee it has in its possession: the
fragment of the CPT for RHC in Figure 15 illustrates this possibility. Furthermore, should
the robot be carrying mail whenever it loses coffee (whether \accidentally" or \intentionally"
via the DelC action), there is a 0:5 chance it will lose the mail. Notice that the effects of this
policy on the variables RHC and RHM are correlated: one cannot accurately predict the
probability of RHMt+1 without determining the probability of RHCt+1 . This correlation is
modeled by the synchronic arc between RHC and RHM at the t + 1 slice of the network.
The independence of the t +1 variables given S t does not hold in 2TBNs with synchronic
arcs. Determining the probability of a resulting state requires some simple probabilistic
reasoning, for example, application of the chain rule. In this example, we can write
Pr(RHCt+1 ; RHMt+1 jS t ) = Pr(RHMt+1 jRHCt+1 ; S t ) Pr(RHCt+1 jS t )
The joint distribution over t + 1 variables given S t can then be computed as in Equation 8 above, with this term replacing the Pr(RHCt+1 jS t ) Pr(RHMt+1 jS t )|while these two
variables are correlated, the remaining variables are independent.
We refer to 2TBNs with no synchronic arcs, like the one in Figure 14, as simple 2TBNs.
General 2TBNs allow synchronic as well as diachronic arcs, as in Figure 15.
42

fiDecision-Theoretic Planning: Structural Assumptions

Loc

Loc

T

T

CR

CR

RHC

RHC

RHM

RHM

M

M

Time t

Time t+1

Pr(RHC t+1 )
Loc t RHC t t f
t 1.0 1.0
O
f 0.0 1.0
O
t 0.7 0.3
H
f 0.0 1.0
H
t 1.0 0.0
C
f 0.0 1.0
C
etc.
etc.
Pr(RHMt+1 )
RHC t RHC t+1 RHMt t f
1.0 0.0
t
t
t
0.0 1.0
f
t
t
0.5 0.5
t
f
t
0.0 1.0
f
f
t
1.0 0.0
t
t
f
0.0 1.0
f
t
f
1.0 0.0
t
f
f
0.0 1.0
f
f
f

Figure 15: A 2TBN for the Markov chain induced by moving counterclockwise and delivering coffee with correlated effects.

4.2 Factored Action Representations

Just as we extended Markov chains to account for different actions, we must extend the
2TBN representation to account for the fact that the state transitions are inuenced by
the agent's choice of action. We discuss a variety of techniques for specifying the transition
matrices that exploit the factored state representation to produce representations that are
more natural and compact than explicit transition matrices.
4.2.1 Implicit-Event Models

We begin with the implicit-event model from Section 2.3 in which the effects of actions
and exogenous events are combined in a single transition matrix. We will consider explicitevent models in Section 4.2.4. As we saw in the previous section, algorithms such as value
and policy iteration require the use of transition models that reect the ultimate transition
probabilities, including the effects of any exogenous events.
One way to model the dynamics of a fully observable MDP is to represent each action
by a separate 2TBN. The 2TBN shown above in Figure 13 can be seen as a representation
of the action CClk (since the policy inducing the Markov chain in that example consists
of the repeated application of that action alone). The network fragment in Figure 16(a)
illustrates the interesting aspects of the 2TBN for the DelC action including the effects of
exogenous events. As above, the robot satisfies an outstanding coffee request if it delivers
coffee while it is in the oce and has coffee (with a 0:05 chance of spillage), as shown in the
conditional probability table for CR. The effect on RHC can be explained as follows: the
43

fiBoutilier, Dean, & Hanks

Loc

RHC

CR
Time t

Loc

RHC

CR
Time t+1

(a)

Pr(RHC t+1 )
Loc t RHC t t f
O
t 0.0 1.0
O
f 0.0 1.0
L
t 0.3 0.7
f 0.0 1.0
L
t 0.3 0.7
C
f 0.0 1.0
C
etc.
etc.
Pr(CR t+1 )
Loc t RHC t CR t t f
t .05 .95
t
O
f 0.2 0.8
t
O
t 1.0 0.0
f
O
f 0.2 0.8
f
O
t 1.0 0.0
t
L
f 0.2 0.8
t
L
t 1.0 0.0
f
L
f 0.2 0.8
f
L
etc.
etc.

Off
t
t

CR

0.05

RHC

Loc

else

f

f

t
t

CR

0.2 1.0

CR

1.0

f

f

0.2

0.2

(b)
Off
t
t

CR

0.05

RHC

f

Loc
else

f
f

0.2

CR

t

1.0

(c)

Figure 16: A factored 2TBN for action DelC (a) and structured CPT representations (b,c).
robot loses the coffee (to the user or to spillage) if it delivers it in the oce; if it attempts
delivery elsewhere, there is a 0:7 chance that a random passerby will take the coffee from
the robot.
As in the case of Markov chains, the effects of actions on different variables can be
correlated, in which case we must introduce synchronic arcs. Such correlations can be
thought of as ramifications (Baker, 1991; Finger, 1986; Lin & Reiter, 1994).
4.2.2 Structured CPTs

The conditional probability table (CPT) for the node CR in Figure 16(a) has 20 rows, one
for each assignment to its parents. However, the CPT contains a number of regularities.
Intuitively, this reects the fact that the coffee request will be met successfully (i.e., the
variable becomes false) 95% of the time when DelC is executed, if the robot has coffee and
is in the right location (the user's oce). Otherwise, CR remains true if it was true and
becomes true with probability 0:2 if it was not. In other words, there are three distinct cases
to be considered, corresponding to three \rules" governing the (stochastic) effect of DelC
on CR. This can be represented more compactly by using a decision tree representation
(with \else" branches to summarize groups of cases involving multivalued variables such
as Loc) like that shown in Figure 16(b), or more compactly still using a decision graph
(Figure 16(c)). In tree- and graph-based representations of CPTs, interior nodes are labeled
by parent variables, edges by values of the variables, and leaves or terminals by distributions
over the child variable's values.30
Decision-tree and decision-graph representations are used to represent actions in fully
observable MDPs in (Boutilier et al., 1995; Hoey, St-Aubin, Hu, & Boutilier, 1999) and
30. When the child is boolean, we label the leaves with only the probability of that variable being true (the
probability of the variable being false is one minus this value).

44

fiDecision-Theoretic Planning: Structural Assumptions

are described in detail in (Poole, 1995; Boutilier & Goldszmidt, 1996).31 Intuitively, trees
and graphs embody the rule-like structure present in the family of conditional distributions
represented by the CPT, and in the settings we consider often yield considerable representational compactness. Rule-based representations have been used directly by Poole (1995,
1997a) in the context of decision processes and can often be more compact than trees (Poole,
1997b). We generically refer to representations of this type as 2TBNs with structured CPTs.
4.2.3 Probabilistic STRIPS Operators

The 2TBN representation can be viewed as oriented toward describing the effects of actions
on distinct variables. The CPT for each variable expresses how it (stochastically) changes
(or persists), perhaps as a function of the state of certain other variables. However, it
has long been noted in AI research on planning and reasoning about action that most
actions change the state in limited ways; that is, they affect a relatively small number of
variables. One diculty with variable-oriented representations such as 2TBNs is that one
must explicitly assert that variables unaffected by a specific action persist in value (e.g.,
see the CPT for RHC in Figure 13)|this is an instance of the infamous frame problem
(McCarthy & Hayes, 1969).
Another form of representation for actions might be called an outcome-oriented representation: one explicitly describes the possible outcomes of an action or the possible joint
effects over all variables. This was the idea underlying the Strips representation from
classical planning (Fikes & Nilsson, 1971).
A classical Strips operator is described by a precondition and a set of effects. The
former identifies the set of states in which the action can be executed, and the latter
describes how the input state changes as a result of taking the action. A probabilistic
Strips operator (PSO) (Hanks, 1990; Hanks & McDermott, 1994; Kushmerick et al., 1995)
extends the Strips representation in two ways. First, it allows actions to have different
effects depending on context, and second, it recognizes that the effects of actions are not
always known with certainty.32
Formally, a PSO consists of a set of mutually exclusive and exhaustive logical formulae,
called contexts, and a stochastic effect associated with each context. Intuitively, a context discriminates situations under which an action can have differing stochastic effects.
A stochastic effect is itself a set of change sets|a simple list of variable values|with a
probability attached to each change set, with the requirement that these probabilities sum
to one. The semantics of a stochastic effect can be described as follows: when the stochastic
effect of an action is applied at state s, the possible resulting states are determined by the
change sets, each occurring with the corresponding probability; the resulting state associated with a change set is constructed by changing variable values at state s to match those
in the change set, while all unmentioned variables persist in value. Note that since only one
31. The fact that certain direct dependencies among variables in a Bayes net are rendered irrelevant under
specific variable assignments has been studied more generally in the guise of context-specific independence
(Boutilier, Friedman, Goldszmidt, & Koller, 1996); see (Geiger & Heckerman, 1991; Shimony, 1993) for
related notions.
32. The conditional nature of effects is also a feature of a deterministic extension of Strips known as ADL
(Pednault, 1989).

45

fiBoutilier, Dean, & Hanks

RHC
t

f

Loc
O

-CR -RHC +M
-CR -RHC
-RHC +M
-RHC

+CR +M
+CR
+M
nil

else

-RHC +CR +M
-RHC +CR
-RHC +M
-RHC
+CR +M
+CR
+M
nil

0.19
0.76
0.01
0.04

0.04
0.16
0.16
0.64

0.028
0.112
0.112
0.448
0.012
0.048
0.048
0.192

Figure 17: A PSO representation for the DelC action.
Loc

O
L

+Loc(L) 0.9
nil
0.1

+Loc(C) 0.9
nil
0.1

H

C

M

+Loc(M) 0.9
nil
0.1

+Loc(H) 0.9
nil
0.1

+Loc(O) -RHC -RHM
+Loc(O) -RHC
+Loc(O)
-RHC -RHM
-RHC
nil

0.135
0.135
0.63
0.015
0.015
0.07

Figure 18: A PSO representation of a simplified CClk action.
context can hold in any state s, the transition distribution for the action at any state s is
easily determined.
Figure 17 gives a graphical depiction of the PSO for the DelC action (shown as a 2TBN
in Figure 16). The three contexts :RHC, RHC ^ Loc(O) and RHC ^:Loc(O) are represented
using a decision tree. At the leaf of each branch in the decision tree is the stochastic effect
(set of change sets and associated probabilities) determined by the corresponding context.
For example, when RHC ^ Loc(O) holds, the action has four possible effects: the robot loses
the coffee; it may or may not satisfy the coffee request (due to the 0:05 chance of spillage);
and mail may or may not arrive. Notice that each outcome is spelled out completely. The
number of outcomes in the other two contexts is rather large due to possible exogenous
events (we discuss this further in Section 4.2.4).33
A key difference between PSOs and 2TBNs lies in their treatment of persistence. All
variables that are unaffected by an action must be given CPTs in the 2TBN model, while
such variables are not mentioned at all in the PSO model (e.g., compare the variable Loc in
both representations of DelC). In this way, PSOs can be said to \solve" the frame problem,
since unaffected variables need not be mentioned in an action's description.34
33. To keep Figure 17 manageable, we ignore the effect of the exogenous event Mess on variable T .
34. For a discussion of the frame problem in 2TBNs, see (Boutilier & Goldszmidt, 1996).

46

fiDecision-Theoretic Planning: Structural Assumptions

ArrM

Mess

Loc

Loc

Loc

Loc

RHC

RHC

RHC

RHC

RHM

RHM

RHM

RHM

CR

CR

CR

CR

M

M

M

M

T

T

T

T

t

t+ 1

t+ 2

t+1

Figure 19: An simplified explicit-event model for DelC.
PSOs can provide an effective means for representing actions with correlated effects.
Recall the description of the CClk action captured in Figure 15, where the robot may
drop its coffee as it moves from the hallway, and may drop its mail only if it drops the
coffee. In the 2TBN representation of CClk, one must have both RHCt and RHCt+1 as
parents of RHMt+1 : we must model the dependence of RHM on a change in value in the
variable RHC. Figure 18 shows the CClk action in PSO format (for simplicity, we ignore
the occurrence of exogenous events). The PSO representation can offer an economical
representation of correlated effects such as this since the possible outcomes of moving in the
hallway are spelled out explicitly. Specifically, the (possible) simultaneous change in values
of the variables in question is made clear.
4.2.4 Explicit-Event Models

Explicit-event models can also be represented using 2TBNs in a somewhat different form.
As in our discussion in Section 2.3, the form taken by explicit-event models depends crucially on one's assumptions about the interplay between the effects of the action itself and
exogenous events. However, under certain assumptions even explicit-event models can be
rather concise.
To illustrate, Figure 19 shows the deliver-coffee action represented as a 2TBN with
exogenous events explicitly represented. The first \slice" of the network shows the effects of
the action DelC without the presence of exogenous events. The subsequent slices describe
the effects of the events ArrM and Mess (we use only two events for illustration). Notice the
presence of the extra random variables representing the occurrence of the events in question.
The CPTs for these nodes reect the occurrence probabilities for the events under various
47

fiBoutilier, Dean, & Hanks

conditions, while the directed arcs from the event variables to state variables indicate the
effects of these events. These probabilities do not depend on all state variables in general;
thus, this 2TBN represents the occurrence vectors (see Section 2.3) in a compact form. Also
notice that, in contrast to the event occurrence variables, we do not explicitly represent the
action occurrence as a variable in the network, since we are modeling the effect on the
system given that the action was taken.35
This example reects the assumptions described in Section 2.3, namely, that events
occur after the action takes place and that event effects are commutative, and for this
reason the ordering of the events ArrM and Mess in the network is irrelevant. Under this
model, the system actually passes through two intermediate though not necessarily distinct
states as it goes from stage t to stage t + 1; we use subscripts "1 and "2 to suggest this
process. Of course, as described earlier, not all actions and events can be combined in such a
decomposable way; more complex combination functions can also be modeled using 2TBNs
(for one example, see Boutilier & Puterman, 1995).
4.2.5 Equivalence of Representations

An obvious question one might ask concerns the extent to which certain representations are
inherently more concise than others. Here we focus on the standard implicit-event models,
describing some of the domain features that make the different representations more or less
suitable.
Both 2TBN and PSO representations are oriented toward representing the changes in
the values of the state variables induced by an action; a key distinction lies in the fact that
2TBNs model the inuence on each variable separately, while the PSO model explicitly
represents complete outcomes. A simple 2TBN|a network with no synchronic arcs|can
be used to represent an action in cases where there are no correlations among the action's
effect on different state variables. In the worst case, when the effect on each variable
differs at each state, each time t + 1 variable must have all time t variables as parents.
If there are no regularities that can be exploited in structured CPT representations, then
such an action requires the specification of O(n2n ) parameters (assuming boolean variables),
compared with the 22n entries required by an explicit transition matrix. When the number of
parents of any variable is bounded by k, then we need specify no more than n2k conditional
probabilities. This can be further reduced if the CPTs exhibit structure (e.g., can be
represented concisely in a decision tree). For instance, if the CPT can be captured by the
representation of choice with no more than f (k) entries, where f is a polynomial function of
the number of parents of a variable, then the representation size, O(n  f (k)), is polynomial
in the number of state variables. This is often the case, for instance, in actions where one
of its (stochastic) effects on a variable requires that some number of (pre-) conditions hold;
if any of them do not, a different effect comes into play.
A PSO representation may not be as concise as a 2TBN when an action has multiple
independent stochastic effects. A PSO requires that each possible change list be enumerated with its corresponding probability of occurrence. The number of such changes grows
exponentially with the number of variables affected by the action. This fact is evident in
35. Sections 4.2.7 and 4.3 discuss representations that model the choice of action explicitly as a variable in
the network.

48

fiDecision-Theoretic Planning: Structural Assumptions

RHC
t

f

Loc
O

-RHC -CR
-RHC

nil

+M 0.2
nil 0.8
1.0

else
0.95
0.05

-RHC
nil

0.7
0.3

Figure 20: A \factored" PSO representation for the DelC action.
Figure 17, where the impact of exogenous events affects a number of variables stochastically and independently. The problem can arise with respect to \direct" action effects, as
well. Consider an action in which a set of 10 unpainted parts is spray painted; each part is
successfully painted with probability 0:9, and these successes are uncorrelated. Ignoring the
complexity of representing different conditions under which the action could take place, a
simple 2TBN can represent such an action with 10 parameters (one success probability per
part). In contrast, a PSO representation might require one to list all 210 distinct change
lists and their associated probabilities. Thus, a PSO representation can be exponentially
larger (in the number of affected variables) than a simple 2TBN representation.
Fortunately, if certain variables are affected deterministically, these do not cause the
PSO representation to blow up. Furthermore, PSO representations can also be modified
to exploit the independence of an action's effects on different state variables (Boutilier &
Dearden, 1994; Dearden & Boutilier, 1997), thus escaping this combinatorial diculty. For
instance, we might represent the DelC action shown in Figure 17 in the more \factored
form" illustrated in Figure 20 (for simplicity, we show only the effect of the action and
the exogenous event ArrM). Much like a 2TBN, we can determine an overall effect by
combining the change sets (in the appropriate contexts) and multiplying the corresponding
probabilities.
Simple 2TBNs defined over the original set of state variables are not sucient to represent all actions.36 Correlated action effects require the presence of synchronic arcs. In
the worst case, this means that time t + 1 variables can have up to 2n , 1 parents. In
fact,
P the acyclicity condition assures that in the worst case, the total number of parents
is nk=1 2k , 1; thus, we end up specifying O(22n ) entries, the same as required by an
explicit transition matrix. However, if the number of parents (whether occurring within
the time slice t or t + 1) can be bounded, or if regularities in the CPTs allow a compact
representation, then 2TBNs can still be profitably used.
PSO representations compare more favorably to 2TBNs in cases in which most of an
action's effects on different variables are correlated. In this case, PSOs can provide a
somewhat more economical representation of action effects, primarily because one needn't
worry about frame conditions. The main advantage of PSOs is that one need not enlist the
aid of probabilistic reasoning procedures to determine the transitions induced by actions
with correlated effects. Contrast the explicit specification of outcomes in PSOs with the
type of reasoning required to determine the joint effects of an action represented in 2TBN
36. However, Section 4.2.6 discusses certain problem transformations that do render simple 2TBNs sucient
for any MDP.

49

fiBoutilier, Dean, & Hanks

form with synchronic arcs, as described in Section 4.1. Essentially, correlated effects are
\compiled" into explicit outcomes in PSOs.
Recent results by Littman (1997) have shown that simple 2TBNs and PSOs can both
be used to represent any action represented as a 2TBN without an exponential blowup
in representation size. This is effected by a clever problem transformation in which new
sets of actions and propositional variables are introduced (using either a simple 2TBN or
PSO representation). The structure of the original 2TBN is reected in the new planning
problem, incurring no more than a polynomial increase in the size of the input action
descriptions and the description of any policy. Though the resulting policy consists of actions
that do not exist in the underlying domain, extracting the true policy is not dicult. It
should be noted, however, that while such a representation can automatically be constructed
from a general 2TBN specification, it is unlikely that it could be provided directly, since
the actions and variables in the transformed problem have no \physical" meaning in the
original MDP.
4.2.6 Transformations to Eliminate Synchronic Constraints

The discussion above has assumed that the variables or propositions used in the 2TBN or
PSO action descriptions are the original state variables. However, certain problem transformations can be used to ensure that one can represent any action using simple 2TBNs, as
long as one does not require the original state variables to be used. One such transformation
simply clusters all variables on which some action has a correlated effect. A new compound
variable|which takes as values assignments to the clustered variables|can then be used
in the 2TBN, removing the need for synchronic arcs. Of course, this variable will have a
domain size exponential in the number of clustered variables.
Some of the intuitions underlying PSOs can be used to convert general 2TBN action descriptions to simple 2TBN descriptions with explicit \events" dictating the precise outcome
of the action. Intuitively, this event can occur in k different forms, each corresponding to a
different change list induced by the action (or a change list with respect to the variables in
question). As an example, we can convert the \action" description for CClk in Figure 15
into the explicit-event model shown in Figure 21.37 Notice that the \event" takes on values
corresponding to the possible effects on the correlated variables RHC and RHM. Specifically, a denotes the event of the robot escaping the hallway successfully without losing its
cargo, b denotes the event of the robot losing only its coffee, and c denotes the event of losing
both the coffee and the mail. In effect, the event space represents all possible \combined"
effects, obviating the need for synchronic arcs in the network.
4.2.7 Actions as Explicit Nodes in the Network

One diculty with the 2TBN and PSO approach to action description is that each action
is represented separately, offering no opportunity to exploit patterns across actions. For
instance, the fact that location persists in all actions except moving clockwise or counterclockwise means that the \frame axiom" is duplicated in the 2TBN for all other actions
(this is not the case for PSOs, of course). In addition, ramifications (or correlated action
37. While Figure 15 describes a Markov chain induced by a policy, the representation of CClk can easily be
extracted from it.

50

fiDecision-Theoretic Planning: Structural Assumptions

Loc

Hall

a: 1.0
b: 0.0
c: 0.0

Event

t

RHC

RHM
t

Loc

else

f

a:0.7 a:0.7
b:0.15 b:0.3
c:0.15 c:0.0

Loc

f

a: 1.0
b: 0.0
c: 0.0

t

a

RHC

RHC
1.0
t
Off

RHM
Time t

RHM
Time t+1

Loc

1.0
0.0

a

RHM

b
0.0

b
0.0

f

0.0

c
0.0

f

0.0

else

Event

Event

RHC

c
1.0

Figure 21: An explicit-event model that removes correlations.
effects) are duplicated across actions as well. For instance, if a coffee request occurs (with
probability 0:2) only when the robot ends up in the oce, then this correlation is duplicated
across all actions. A more compelling example might be one in which the robot can move
a briefcase to a new location in one of a number of ways. We'd like to capture the fact (or
ramification) that the contents of the briefcase move to the same location as the briefcase
regardless of the action that moves the briefcase.
To circumvent this diculty, we can introduce the choice of action as a \random variable" in the network, conditioning the distribution over state variable transitions on the
value of this variable. Unlike state variables (or event variables in explicit event models),
we do not generally require a distribution over this action variable|the intent is simply
to model schematically the conditional state-transition distributions given any particular
choice of action. This is because the choice of action will be dictated by the decision maker
once a policy is determined. For this reason, anticipating terminology used for inuence
diagrams (see Section 4.3), we call these nodes decision nodes and depict them in our network diagrams with boxes. Such a variable can take as its value any action available to the
agent.
A 2TBN with an explicit decision node is shown in Figure 22. In this restricted example,
we might imagine the decision node can take one of two values, Clk or CClk. The fact that
the issuance of a coffee request at t+1 depends on whether the robot successfully moved from
(or remained in) the oce is now represented \once" by the arc between Loct+1 and CRt+1 ,
rather than repeated across multiple action networks. Furthermore, the noisy persistence
of M under both actions is also represented only once (adding the action PUM, however,
undercuts this advantage as we will see when we try to combine actions).
One diculty with this straightforward use of decision nodes (which is the standard
representation in the inuence diagram literature) is that adding candidate actions can
cause an explosion in the network's dependency structure. For example, consider the two
51

fiBoutilier, Dean, & Hanks

Act

Loc

Loc

CR

CR

M

M

Time t

Time t+1

Figure 22: An inuence diagram for a restricted process.

Act
X

X

X

X

X

Act

X

else

a1
a2

Y

Y

Y

Y

Y

Y

t
1.0

Y

f
t

X

0.9

Z

Z
(a) action a1

Z

Z

Z

(b) action a2

1.0

f
0

Y

f
t

0.9

t
Z

1.0

f
0

Z

(c) influence diagram

Figure 23: Unwanted dependencies in inuence diagrams.

52

t

(d) CPT for Y

Y

f
0

fiDecision-Theoretic Planning: Structural Assumptions

action networks shown in Figure 23(a) and (b). Action a1 makes Y true with probability
0:9 if X is true (having no effect otherwise), while a2 makes Y true if Z is true.
Combining these actions in a single network in the obvious way produces the inuence
diagram shown in Figure 23(c). Notice that Y now has four parent nodes, inheriting the
union of all its parents in the individual networks (plus the action node) and requiring a
CPT with 16 entries for actions a1 and a2 together with eight additional entries for each
action that does not affect Y . The individual networks reect the fact that Y depends
on X only when a1 is performed and on Z only when a2 is performed. This fact is lost
in the naively constructed inuence diagram. However, structured CPTs can be used to
recapture this independence and compactness of representation: the tree of Figure 23(d)
captures the distribution much more concisely, requiring only eight entries. This structured
representation also allows us concisely to express that Y persists under all other actions. In
large domains, we expect variables to generally be unaffected by a substantial number of
(perhaps most) actions, thus requiring representations such as this for inuence diagrams.
See (Boutilier & Goldszmidt, 1996) for a deeper discussion of this issue and its relationship
to the frame problem.
While we provide no distributional information over the action choice, it is not hard to
see that a 2TBN with an explicit decision node can be used to represent the Markov chain
induced by a particular policy in a very natural way. Specifically, by adding arcs from state
variables at time t to the decision node, the value of the decision node (i.e., the choice of
action at that point) can be dictated by the prevailing state.38

4.3 Inuence Diagrams
Inuence diagrams (Howard & Matheson, 1984; Shachter, 1986) extend Bayesian networks
to include special decision nodes to represent action choices, and value nodes to represent
the effect of action choice on a value function. The presence of decision nodes means that
action choice is treated as a variable under the decision maker's control. Value nodes treat
reward as a variable inuenced (usually deterministically) by certain state variables.
Inuence diagrams have not typically been associated with the schematic representation
of stationary systems, instead being used as a tool for decision analysts where the sequential
decision problem is carefully handcrafted. This more generic use of inuence diagrams has
been discussed by Tatman and Shachter (1990). In any event, there is no theory of plan
construction associated with inuence diagrams: the choice of all possible actions at each
stage must be explicitly encoded in the model. Inuence diagrams are, therefore, usually
used to model finite-horizon decision problems by explicitly describing the evolution of the
process at each stage in terms of state variables.
As in Section 4.2.7, decision nodes take as values specific actions, though the set of
possible actions can be tailored to the particular stage. In addition, an analyst will generally
include at each stage only state variables that are thought relevant to the decision at that
or subsequent stages. Value nodes are also a key feature of inuence diagrams and are
discussed Section 4.5. Usually, a single value node is specified, with arcs indicating the
38. More generally, a randomized policy can be represented by specifying a distribution over possible actions
conditioned on the state.

53

fiBoutilier, Dean, & Hanks

T
RHM
M

RHM
Rew

CR

M

etc.

T
0

CR

etc.

1 2

T
3

4

-7 -6.5 -6 -5.5 -5

0

1 2

3

4

-4 -3.5 -3 -2.5 -2

Figure 24: The representation of a reward function in an inuence diagram.
inuence of particular state and decision variables (often over multiple stages) on the overall
value function.
Inuence diagrams are typically used to model partially observable problems. An arc
from a state variable to a decision node reects the fact that the value of that state variable
is available to the decision maker at the time the action is to be chosen. In other words,
this variable's value forms part of the observation made at time t prior to the action being
selected at time t +1, and the policy constructed can refer to this variable. Once again, this
allows a compact specification of the observation probabilities associated with a system. The
fact that the probability of a given observation depends directly only on certain variables
and not on others can mean that far fewer model parameters are required.

4.4 Factored Reward Representation

We have already noted that it is very common in formulating MDP problems to adopt a
simplified value function: assigning rewards to states and costs to actions, and evaluating histories by combining these factors according to some simple function like addition.
This simplification alone allows a representation for the value function significantly more
parsimonious than one based on a more complex comparison of complete histories. Even
this representation requires an explicit enumeration of the state and action space, however,
motivating the need for more compact representations for these parameters. Factored representations for rewards and action costs can often obviate the need to enumerate state and
action parameters explicitly.
Like an action's effect on a particular variable, the reward associated with a state often
depends only on the values of certain features of the state. For example, in our robot
domain, we can associate rewards or penalties with undelivered mail, with unfulfilled coffee
requests and with untidiness in the lab. This reward or penalty is independent of other
variables, and individual rewards can be associated with the groups of states that differ on
the values of the relevant variables. The relationship between rewards and state variables is
represented in value nodes in inuence diagrams, represented by the diamond in Figure 24.
The conditional reward table (CRT) for such a node is a table that associates a reward with
every combination of values for its parents in the graph. This table, not shown in Figure 24,
is locally exponential in the number of relevant variables. Although Figure 24 shows the
case of a stationary Markovian reward function, inuence diagrams can be used to represent
54

fiDecision-Theoretic Planning: Structural Assumptions

nonstationary or history-dependent rewards and are often used to represent value functions
for finite-horizon problems.
Although in the worst case the CRT will take exponential space to store, in many
cases the reward function exhibits structure, allowing it to be represented compactly using
decision trees or graphs (Boutilier et al., 1995), Strips-like tables (Boutilier & Dearden,
1994), or logical rules (Poole, 1995, 1997a). Figure 24 shows a fragment of one possible
decision-tree representation for the reward function used in the running example.
The independence assumptions studied in multiattribute utility theory (Keeney & Raiffa,
1976) provide yet another way in which reward functions can be represented compactly. If
we assume that the component attributes of the reward function make independent contributions to a state's total reward, the individual contributions can be combined functionally.
For instance, we might imagine penalizing states where CR holds with a (partial) reward
of ,3, penalizing situations where there is undelivered mail (M _ RHM) with ,2, and
penalizing untidiness T (i) with i , 4 (i.e., in proportion to how untidy things are). The
reward for any state can then be determined simply by adding the individual penalties associated with each feature. The individual component rewards along with the combination
function constitute a compact representation of the reward function. The tree fragment in
Figure 24, which reects the additive independent structure just described, is considerably
more complex than a representation that defines the (independent) rewards for individual
propositions separately. The use of additive reward functions for MDPs is considered in
(Boutilier, Brafman, & Geib, 1997; Meuleau, Hauskrecht, Kim, Peshkin, Kaelbling, Dean,
& Boutilier, 1998; Singh & Cohn, 1998).
Another example of structured rewards is the goal structure studied in classical planning.
Goals are generally specified by a single proposition (or a set of literals) to be achieved.
As such, they can generally be represented very compactly. Haddawy and Hanks (1998)
explore generalizations of goal-oriented models that permit extensions such as partial goal
satisfaction, yet still admit compact representations.

4.5 Factored Policy and Value Function Representation

The techniques studied so far have been concerned with the input specification of the MDP:
the states, actions, and reward function. The components of a problem's solution|the policy and optimal value function|are also candidates for compact structured representation.
In the simplest case, that of a stationary policy for a fully observable problem, a policy
must associate an action with every state, nominally requiring a representation of size
O(jSj). The problem is exacerbated for nonstationary policies and POMDPs. For example,
the policy for a finite-horizon FOMDP with T stages generates a policy of size O(T jSj).
For a finite-horizon POMDP, each possible
P observable history of length t < T might require
a different action choice; as many as Tk=1 bk such histories can be generated by a fixed
policy, where b is the maximum number of possible observations one can make following an
action.39
The fact that policies require too much space motivates the need to find compact functional representations, and standard techniques like the tree structures discussed above for
39. Other methods of dealing with POMDPs, by conversion to FOMDPs over belief space (see Section 2.10.2),
are more complex still.

55

fiBoutilier, Dean, & Hanks

CR
RHC

etc.

Loc
O

L C

Loc
M

H

H

O L

C

M

DelC Clk Clk M Cclk Cclk HRM Clk GetC M
PUM Cclk

DelM Cclk

PUM Cclk

Figure 25: A tree representation of a policy.
actions and reward functions can be used to represent policies and value functions as well.
Here we focus on stationary policies and value functions for FOMDPs, for which any logical
function representation may be used. For example, Schoppers (1987) uses a Strips-style
representation for universal plans, which are deterministic, plan-like policies. Decision trees
have also been used for policies and value functions (Boutilier et al., 1995; Chapman &
Kaelbling, 1991). An example policy for the robot domain specified with a decision tree is
given in Figure 25. This policy dictates that, for instance, if CR and RHC are true: (a) the
robot deliver the coffee to the user if it is in the oce, and (b) it move toward the oce
if it is not in the oce, unless (c) there is mail and it is in the mailroom, in which case it
should pickup the mail on its way.

4.6 Summary

In this section we discussed a number of compact factored representations for components of
an MDP. We began by discussing intensional state representations, then temporal Bayesian
networks as a device for representing the system dynamics. Tree-structured conditional
probability tables (CPTs) and probabilistic Strips operators (PSOs) were introduced as an
alternative to transition matrices. Similar tree structures and other logical representations
were introduced for representing reward functions, value functions, and policies.
While these representations can often be used to describe a problem compactly, by
themselves they offer no guarantee that the problem can be solved effectively. In the next
section we explore algorithms that use these factored representations to avoid iterating
explicitly over the entire set of states and actions.

5. Abstraction, Aggregation, and Decomposition Methods

The greatest challenge in using MDPs as the basis for DTP lies in discovering computationally feasible methods for the construction of optimal, approximately optimal or satisficing
policies. Of course, arbitrary decision problems are intractable|even producing satisficing
or approximately optimal policies is generally infeasible. However, the previous sections
suggest that many realistic application domains may exhibit considerable structure, and
furthermore that the structure can be modeled explicitly and exploited so that typical
problems can be solved effectively. For instance, structure of this type can lead to compact
56

fiDecision-Theoretic Planning: Structural Assumptions

factored representations of both input data and output policies, often polynomial-sized with
respect to the number of variables and actions describing the problem. This suggests that
for these compact problem representations, policy construction techniques can be developed that exploit this structure and are tractable for many commonly occurring problem
instances.
Both the dynamic programming and state-based search techniques described in Section 3 exploit structure of a different kind. Value functions that can be decomposed into
state-dependent reward functions, or state-based goal functions, can be tackled by dynamic
programming and regression search, respectively. These algorithms exploit the structure
in decomposable value functions to prevent having to search explicitly through all possible
policies. However, while these algorithms are polynomial in the size of the state space,
the curse of dimensionality makes even these algorithms infeasible for practical problems.
Though compact problem representations aid in the specification of large problems, it is
clear that a large system can be specified compactly only if the representation exploits
\regularities" found in the domain. Recent AI research on DTP has stressed using the
regularities implicit in compact representations to speed up the planning process. These
techniques focus on both optimal and approximately optimal policy construction.
In the following subsection we focus on abstraction and aggregation techniques, especially those that manipulate factored representations. Roughly, these techniques allow the
explicit or implicit grouping of states that are indistinguishable with respect to certain characteristics (e.g., value or optimal action choice). We refer to a set of states grouped in this
manner as an aggregate or abstract state , or sometimes as a cluster, and assume that the
set of abstract states constitutes a partition of the state space; that is to say, every state is
in exactly one abstract state and the union of all abstract states comprises the entire state
space.40 By grouping similar states, each abstract state can be treated as a single state, thus
alleviating the need to perform computations for each state individually. These techniques
can be used for approximation if the elements of an abstract state are only approximately
indistinguishable (e.g., if the values of those states lie within some small interval).
We then look at the use of problem decomposition techniques in which an MDP is
broken into various pieces, each of which is solved independently; the solutions are then
pieced together or used to guide the search for a global solution. If subprocesses whose
solutions interact minimally are treated as independent, we might expect an approximately
optimal global solution. Furthermore, if the structure of the problem requires a solution
to a particular subproblem only, then the solutions to other subproblems can be ignored
altogether.
Related is the use of reachability analysis to restrict attention to \relevant" regions of
state space. Indeed, reachability analysis and the communicating structure of an MDP
can be used to form certain types of decompositions. Specifically, we distinguish serial
decompositions from parallel decompositions.
The result of a serial decomposition can be viewed as a partitioning of the state space
into blocks, each representing a (more or less) independent subprocess to be solved. In
serial decomposition, the relationship between blocks is generally more complicated than
in the case of abstraction or aggregation. In a partition resulting from decomposition, the
40. We might also group states into non-disjoint sets that cover the entire state space. We do not consider
such soft-state aggregation here, but see (Singh, Jaakkola, & Jordan, 1994).

57

fiBoutilier, Dean, & Hanks

states within a particular block may behave quite differently with respect to (say) value or
dynamics. The important consideration in choosing a decomposition is that it is possible to
represent each block compactly and to compute eciently the consequences of moving from
one block to another and, further, that the subproblems corresponding to the subprocesses
can themselves be solved eciently.
A parallel decomposition is somewhat more closely related to an abstract MDP. An
MDP is divided into \parallel sub-MDPs" such that each decision or action causes the
state to change within each sub-MDP. Thus, the MDP is the cross product or join of the
sub-MDPs (in contrast to the union, as in serial decomposition). We briey discuss several
methods that are based on parallel MDP decomposition.

5.1 Abstraction and Aggregation
One way problem structure can be exploited in policy construction relies on the notion
of aggregation|grouping states that are indistinguishable with respect to certain problem
characteristics. For example, we might group together all states that have the same optimal
action, or that have the same value with respect to the k-stage-to-go value function. These
aggregates can be constructed during the solution of the problem.
In AI, emphasis has generally been placed on a particular form of aggregation, namely
abstraction methods, in which states are aggregated by ignoring certain problem features.
The policy in Figure 25 illustrates this type of abstraction: those states in which CR,
RHC and Loc(O) are true are grouped, and the same action is selected for each such
state. Intuitively, when these three propositions hold, other problem features are ignored
and abstracted away (i.e., they are deemed irrelevant). A decision-tree representation of a
policy or a value function partitions the state space into a distinct cluster for each leaf of
the tree. Other representations (e.g., Strips-like rules) abstract the state space similarly.
It is precisely this type of abstraction that is used in the compact, factored representations of actions and goals discussed in Section 4. In the 2TBN shown in Figure 16, the
effect of the action DelC on the variable CR is given by the CPT for CRt+1 ; however,
this (stochastic) effect is the same at any state for which the parent variables have the
same value. This representation abstracts away other variables, combining states that have
distinct values for the irrelevant (non-parent) variables. Intensional representations often
make it easy to decide which features to ignore at a certain stage of problem solving, and
thus (implicitly) how to aggregate the state space.
There are at least three dimensions along which abstractions of this type can be compared. The first is uniformity: a uniform abstraction is one in which variables are deemed
relevant or irrelevant uniformly across the state space, while a nonuniform abstraction allows certain variables to be ignored under certain conditions and not under others. The
distinction is illustrated schematically in Figure 26. The tabular representation of a CPT
can be viewed as a form of uniform abstraction|the effect of an action on a variable is
distinguished for all clusters of states that differ on the value of a parent variable, and is
not distinguished for states that agree on parent variables but disagree on others|while a
decision tree representation of a CPT embodies a nonuniform abstraction.
A second dimension of comparison is accuracy. States are grouped together on the
basis of certain characteristics, and the abstraction is called exact if all states within a
58

fiDecision-Theoretic Planning: Structural Assumptions

Uniform
ABC
ABC

ABC
ABC

ABC
ABC

ABC
ABC

Nonuniform
A
B

5.3
5.3

AB

=

ABC

C

ABC

Exact
5.3
5.3

A

Approximate
5.3
5.2

2.9
2.9

2.9
2.7

5.5

9.3
9.3

9.3
9.0

5.3

Adaptive

Fixed

Figure 26: Different forms of state space abstraction.
cluster agree on this characteristic. A non-exact abstraction is called approximate. This
is illustrated schematically in Figure 26: the exact abstraction groups together states that
agree on the value assigned to them by a value function, while the approximate abstraction
allows states to be grouped together that differ in value. The extent to which these states
differ is often used as a measure of the quality of an approximate abstraction.
A third dimension is adaptivity. Technically, this is a property not of an abstraction
itself, but of how abstractions are used by a particular algorithm. An adaptive abstraction
technique is one in which the abstraction can change during the course of computation, while
a fixed abstraction scheme groups together states once and for all (again, see Figure 26).
For example, one can imagine using an abstraction in the representation of a value function
V k , then revising this abstraction to represent V k+1 more accurately.
Abstraction and aggregation techniques have been studied in the OR literature on
MDPs. Bertsekas and Castanon (1989) develop an adaptive aggregation (as opposed to
abstraction) technique. The proposed method operates on at state spaces, however, and
therefore does not exploit implicit structure in the state space itself. An adaptive, uniform
abstraction method is proposed by Schweitzer et al. (1985) for solving stochastic queuing models. These methods, often referred to as aggregation-disaggregation procedures, are
typically used to accelerate the calculation of the value function for a fixed policy. Valuefunction calculation requires computational effort at least quadratic in the size of the state
space, which is impractical for very large state spaces. In aggregation-disaggregation procedures, the states are first aggregated into clusters. A system of equations is then solved,
or a series of summations performed, requiring effort no more than cubic in the number of
clusters. Next, a disaggregation step is performed for each cluster, requiring effort at least
linear in the size of the cluster. The net result is that the total work, while at least linear
in the total number of states, is at worst cubic in the size of the largest cluster.
In DTP it is generally assumed that computations even linear in the size of the full
state space are infeasible. Therefore it is important to develop methods that perform
59

fiBoutilier, Dean, & Hanks

work polynomial in the log of the size of the state space. Not all problems are amenable
to such reductions without some (perhaps unacceptable) sacrifice in solution quality. In
the following section, we review some recent techniques for DTP aimed at achieving such
reductions.
5.1.1 Goal Regression and Classical Planning

In Section 3.2 we introduced the general technique of regression (or backward) search
through state space to solve classical planning problems, those involving deterministic actions and performance criteria specified in terms of reaching a goal-satisfying state. One
diculty is that such a search requires that any branch of the search tree lead to a particular
goal state. This commitment to a goal state may have to be retracted (by backtracking
the search process) if no sequence of actions can lead to that particular goal state from the
initial state. However, a goal is usually specified as a set of literals G representing a set of
states, where reaching any state in G is equally suitable|it may, therefore, be wasteful to
restrict the search to finding a plan that reaches a particular element of G.
Goal regression is an abstraction technique that avoids the problem of choosing a particular goal state to pursue. A regression planner works by searching for a sequence of actions
as follows: the current set of subgoals SG0 is initialized as G. At each iteration an action
ff is selected that achieves one or more of the current subgoals of SGi without deleting
the others, and whose preconditions do not conict with the \unachieved subgoals." The
subgoals so achieved are removed from the current subgoal set and replaced by a formula
representing the context under which ff will achieve the current subgoals, forming SGi+1 .
This process is known as regressing SGi through ff. The process is repeated until one of
two conditions holds: (a) the current subgoal set is satisfied by the initial state, in which
case the current sequence of actions so selected is a successful plan; or (b) no action can be
applied, in which case the current sequence cannot be extended into a successful plan and
some earlier action choice must be reconsidered.
Example 5.1 As an example, consider the simplified version of the robot planning example used in Section 3.1 to illustrate value iteration: the robot has only four actions
PUM, GetC, DelC and DelM, which we make deterministic in the obvious way. The
initial state sinit is hCR; M; RHC; RHMi and the goal set G is fCR; M g. Regressing G through DelM results in SG1 = fCR; M; RHMg. Regressing SG1 through
DelC results in SG2 = fRHC; M; RHMg. Regressing SG2 through PUM results in
SG3 = fRHC; M g. Regressing SG3 through GetC results in SG4 = fM g. Note that
sinit 2 SG4, so the sequence of actions GetC, PUM, DelC, DelM will successfully reach
a goal state. 2
To see how this algorithm implements a form of abstraction, first note that the goal
itself provides an initial partition of the state space, dividing it into one set of states in
which the goal is satisfied (G) and a second set in which it is not (G). Viewed as a partition
of a zero-stage-to-go value function, G represents those states whose value is positive while
G represents those states whose value is zero.
Every regression step can be thought of as revising this partition. When the planning
algorithm attempts to satisfy the current subgoal set SGi by applying action ff, it uses
60

fiDecision-Theoretic Planning: Structural Assumptions

GetC

RHC

M

S

4

PUM

RHC

DelC

CR

DelM

CR

M

M

M

RHM

RHM

M

S

S

S

Goal

3

2

1

Figure 27: An example of goal regression.
regression to compute the (largest) set of states such that, after executing ff, all subgoals
are satisfied. In particular, the state space is repartitioned into two abstract states: SGi+1
and SGi+1 . In this way, the abstraction mechanism implemented by goal regression should
be considered adaptive. This can be viewed as an (i + 1)-stage value function: any state
satisfying SGi+1 can reach a goal state in i +1 steps using the action sequence that produced
SGi+1 .41 The regression process can be stopped when the initial state is a member of the
abstract state SGi+1 . Figure 27 illustrates the repartitioning of the state space into the
different regions SGi+1 for each of the steps in the example above.
While regression produces a compact representation of something like a value function
(as in our discussion of deterministic, goal-based dynamic programming in Section 3.2), the
analogy is not exact in that the regions produced by regression record only the property of
goal reachability contingent on a particular choice of action or action sequence.
Standard dynamic programming methods can be implemented in a structured way by
simply noticing that a number of different regions can be produced at the ith iteration
by considering all actions that can be regressed at that stage. The union of all of these
regressions form the states that have positive values in Vi , thus making the representation of
the i-stage-to-go value function exact. Notice that each iteration is now more costly, since
regression through all actions must be attempted, but this approach obviates the need for
backtracking and can ensure that a shortest plan is found. Standard regression does not
provide such guarantees without commitment to a particular search strategy (e.g., breadthfirst). This use of dynamic programming using Strips action descriptions forms the basic
idea of Schoppers's universal planning method (Schoppers, 1987).
Another general technique for solving classical planning problems is partial order planning (POP) (Chapman, 1987; Sacerdoti, 1975), embodied in such popular planning algorithms as SNLP (McAllester & Rosenblitt, 1991) and UCPOP (Penberthy & Weld, 1992).42
The main motivation for the least-commitment approach comes from the realization that
regression techniques are incrementally building a plan from the end to the beginning (in
the temporal dimension). Thus, each iteration must commit to inserting a step last in the
plan.
In many cases it can be determined that a particular step must appear somewhere in the
plan, but not necessarily as the last step in the plan; and, indeed, in many cases the step
41. It is not the case, however, that states in SGi+1 cannot reach the goal region in i + 1 steps. It is only
the case that they cannot do so using the specific sequence of actions chosen so far.
42. This type of planning is also sometimes called nonlinear or least-commitment planning. See Weld's
(1994) survey for a nice overview.

61

fiBoutilier, Dean, & Hanks

under consideration cannot appear last, but this fact cannot be recognized until later choices
reveal an inconsistency. In these cases, a regression algorithm will prematurely commit to
the incorrect ordering and will eventually have to backtrack over that choice. For example,
suppose in the problem scenario above that the robot can hold only one item at a time,
coffee or mail. Picking up mail causes the robot to spill any coffee in its possession, and
similarly grasping the coffee makes it drop the mail. The plan generated by regression would
no longer be valid: once the first two actions (DelC and DelM) have been inserted into the
plan, no action can be added to achieve RHC or RHM without making the other one false;
the search for a plan would have to backtrack. Ultimately it would be discovered that no
successful plan can end with these two actions performed in sequence.
Partial-order planning algorithms proceed much like regression algorithms, choosing
actions to achieve unachieved subgoals and using regression to determine new subgoals,
but leaving actions unordered to whatever extent possible. Strictly speaking, subgoal sets
aren't regressed; rather, each unachieved goal or action precondition is addressed separately,
and actions are ordered relative to one another only if one action threatens to negate the
desired effect of another. In the example above, the algorithm might first place actions
DelC and DelM into the plan, but leave them unordered. PUM can be added to the plan
to achieve the requirement RHM of DelM; it is ordered before DelM but is still unordered
with respect to DelC. When GetC is finally added to the plan so as to achieve RHC for
action DelC, two threats arise. First, GetC threatens the desired effect RHM of PUM. This
can be resolved by ordering GetC before PUM or after DelM. Assume the former ordering
is chosen. Second, PUM threatens the desired effect RHC of GetC. This threat can also
be resolved by placing PUM before GetC or after DelC; since the first threat was resolved
by ordering GetC before PUM, the latter ordering is the only consistent one. The result
is the plan GetC, DelC, PUM, DelM. No backtracking was required to generate the plan,
because the actions were initially unordered, and orderings were introduced only when the
discovery of threats required them.
In terms of abstraction, any incomplete, partially ordered plan that is threat-free,
but perhaps has certain \open conditions" (unachieved preconditions or subgoals), can be
viewed in much the same way as a partially completed regression plan: any state satisfying
the open conditions can reach a goal state by executing any total ordering of the plan's
actions consistent with current set of ordering constraints. See (Kambhampati, 1997) for a
framework that unifies various approaches to solving classical plan-generation problems.
While techniques relying on regression have been studied extensively in the deterministic
setting, they have only recently been applied to probabilistic unobservable (Kushmerick
et al., 1995) and partially observable (Draper, Hanks, & Weld, 1994b) domains. For the
most part, these techniques assume a goal-based performance criterion and attempt to
construct plans whose probability of reaching a goal state exceeds some threshold. These
augment standard POP methods with techniques for evaluating a plan's probability of
achieving the goal, and techniques for improving this probability by adding further structure
to the plan. In the next section, we consider how to use regression-related techniques to
solve MDPs with performance criteria more general than goals.
62

fiDecision-Theoretic Planning: Structural Assumptions

5.1.2 Stochastic Dynamic Programming with Structured Representations

A key idea underlying propositional goal regression|that one need only regress the relevant propositions through an action|can be extended to stochastic dynamic programming
methods, like value iteration and policy iteration, and used to solve general MDPs. There
are, however, two key diculties to overcome: the lack of a specific goal region and the
uncertainty associated with action effects.
Instead of viewing the state space as partitioned into goal and non-goal clusters, we
consider grouping states according to their expected values. Ideally, we might want to
group states according to their value with respect to the optimal policy. Here we consider
a somewhat less dicult task, that of grouping states according to their value with respect
to a fixed policy. This is essentially the task performed by the policy evaluation step in
policy iteration, and the same insights can be used to construct optimal policies.
For a fixed policy, we want to group states that have the same value under that policy.
Generalizing the goal versus non-goal distinction, we begin with a partition that groups
states according their immediate rewards. Then, using an analogue of regression developed
for the stochastic case, we reason backward to construct a new partition in which states
are grouped according to their value with respect to the one-stage-to-go value function. We
iterate in this manner so that on the kth iteration we produce a new partition that groups
states according the k-stage-to-go value function.
On each iteration, we perform work polynomial in the number of abstract states (and
the size of the MDP representation) and, if we are lucky, the total number of abstract states
will be bounded by some logarithmic factor of the size of the state space. To implement this
scheme effectively, we have to perform operations like regression without ever enumerating
the set of all states, and this is where the structured representations for state-transition,
value, and policy functions play a role.
For FOMDPs, approaches of this type are taken in (Boutilier, 1997; Boutilier & Dearden, 1996; Boutilier et al., 1995; Boutilier, Dearden, & Goldszmidt, 1999; Dietterich &
Flann, 1995; Hoey et al., 1999). We illustrate the basic intuitions behind this approach
by describing how value iteration for discounted infinite-horizon FOMDPs might work. We
assume that the MDP is specified using a compact representation of the reward function
(such as a decision tree) and actions (such as 2TBNs).
In value iteration, we produce a sequence of value functions V0 ; V1 ;    ; Vn , each Vk
representing the utility of the optimal k-stage policy. Our aim is to produce a compact
representation of each value function and, using Vn for some suitable n, produce a compact
representation of the optimal stationary policy. Given a compact representation of the
reward function R, it is clear that this constitutes a compact representation of V0 . As
usual, we think of each leaf of the tree as a cluster of states having identical utility. To
produce V1 in compact form, we can proceed in two phases.
Each branch of the tree for V0 provides an intensional description|namely, the conjunction of variable values labeling the branch|of an abstract state, or region, comprising
states with identical value with respect to the initial value function V0 . For any deterministic action ff, we can perform a regression step using this description to determine the
conditions under which, should we perform ff, we would end up in this cluster. This would,
furthermore, determine a region of the state space containing states of identical future value
63

fiBoutilier, Dean, & Hanks

X

X

X

1.0 0.0
X

Y

Y

0.9

Y

1.0 0.0

Z

Z

Y
0.9

Time t

Z

Time t+1
1.0 0.0

Figure 28: An example action.
with respect to the execution of ff with one stage to go.43 Unfortunately, nondeterministic
actions cannot be handled in quite this way: at any given state, the action might lead to
several different regions of V0 with non-zero probability. However, for each leaf in the tree
representing V0 (i.e., for each region of V0 ), we can regress the conjunction X describing
that region through action ff to produce the conditions under which X becomes true or
false with a specified probability. In other words, instead of regressing in the standard fashion to determine the conditions under which X becomes true, we produce a set of distinct
conditions under which X becomes true with different probabilities. By piecing together
the regions produced for the different labels in the description of V0 , we can construct a
set of regions such that each state in a given region: (a) transitions (under action ff) to a
particular part of V0 with identical probability; and hence (b) has identical expected future
value (Boutilier et al., 1995). We can view this as a generalization of propositional goal
regression suitable for decision-theoretic problems.
Example 5.2 To illustrate, consider the example action a shown in Figure 28 and the value
function V 0 shown to the left of Figure 29. In order to generate the set of regions
consisting of states whose future value (w.r.t. V 0 ) under a is identical, we proceed in
two steps (see Figure 29). We first determine the conditions under which a has a fixed
probability of making Y true (hence we have a fixed probability of moving to the left
or right subtree of V 0 ). These conditions are given by the tree representing the CPT
for node Y , which makes up the first portion of the tree representing V 1 |see Step 1
of Figure 29. Notice that this tree has leaves labeled with the probability of making
Y true or (implicitly) false.
If a makes Y true, then we know that its future value (i.e., value with zero stages
to go) is 8.1; but if Y becomes false, we need to know whether a makes Z true (to
43. We ignore immediate reward and cost distinctions within the region so produced in our description;
recall that the value of performing ff at any state s is given by R(s), C (ff; s) and expected future value.
We simply focus on abstract states whose elements have identical future expected value. Differences in
immediate reward and cost can be added after the fact.

64

fiDecision-Theoretic Planning: Structural Assumptions

Y

X

8.1

Z

9.0

Y 0.9

0.0

X

Y

Y 1.0

Y

Y 0.0

Y 0.9
Z 0.9

Z

Y 0.9
Z 1.0
0

V

Step 1

Y

Y 1.0

Y 0.9
Z 0.0

Z

Y 0.0
Z 1.0

Y 0.0
Z 0.0

Step 2

Figure 29: An iteration of decision-theoretic regression. Step 1 produces the portion of the
tree with dashed lines, while Step 2 produces the portion with dotted lines.
determine whether the future value is 0 or 9:0). The probability with which Z becomes
true is given by the tree representing the CPT for node Z . In Step 2 in Figure 29,
the conditions in that CPT are conjoined to the conditions required for predicting
Y 's probability (by \grafting" the tree for Z to the tree for Y given by the first step).
This grafting is slightly different at each of the three leaves of the tree for Y : (a) the
full tree for Z is attached to the leaf X = t; (b) the tree for Z is simplified where it is
attached to to the leaf X = f ^ Y = f by removal of the redundant test on variable
Y ; (c) notice that there is no need to attach the tree for Z to the leaf X = f ^ Y = t,
since a makes Y true with probability 1 under those conditions (and Z is relevant to
the determination of V 0 only when Y is false).
At each of the leaves of the newly formed tree we have both Pr(Y ) and Pr(Z ). Each of
these joint distributions over Y and Z (the effect of a and these variables is independent by the semantics of the network) tells us the probability of having Y and Z true
with zero stages to go given that the conditions labeling the appropriate branch of the
tree hold with one stage to go. In other words, the new tree uniquely determines, for
any state with one stage remaining, the probability of making any of the conditions
labeling the branches of V 0 true. The computation of expected future value obtained
by performing a with one stage to go can then be placed at the leaves of this tree by
taking expectation over the values at the leaves of V 0 . 2
The new set of regions produced this way describes the function Qff1 , where Qff1 (s) is the
value associated with performing ff at state s with one stage to go and acting optimally
thereafter. These functions (for each action ff) can be pieced together (i.e., \maxed"|see
Section 3.1) to determine V1 . Of course, the process can be repeated some number of times
to produce Vn for some suitable n, as well as the optimal policy with respect to Vn .
This basic technique can be used in a number of different ways. Dietterich and Flann
(1995) propose ideas similar to these, but restrict attention to MDPs with goal regions
65

fiBoutilier, Dean, & Hanks

and deterministic actions (represented using Strips operators), thus rendering true goalregression techniques directly applicable.44 Boutilier et al. (1995) develop a version of
modified policy iteration to produce tree-structured policies and value functions, while
Boutilier and Dearden (1996) develop the version of value iteration described above. These
algorithms are extended to deal with correlations in action effects (i.e., synchronic arcs in the
2TBNs) in (Boutilier, 1997). These abstraction schemes can be categorized as nonuniform,
exact and adaptive.
The utility of such exact abstraction techniques has not been tested on real-world problems to date. In (Boutilier et al., 1999), results on a series of abstract process-planning
examples are reported, and the scheme is shown to be very useful, especially for larger
problems. For example, in one specific problem with 1.7 million states, the tree representation of the value function has only 40,000 leaves, indicating a tremendous amount of
regularity in the value function. Schemes like this exploit such regularity to solve problems
more quickly (in this example, in much less than half the time required by modified policy iteration) and with much lower memory demands. However, these schemes do involve
substantial overhead in tree construction, and for smaller problems with little regularity,
the overhead is not repaid in time savings (simple vector-matrix representations methods
are faster), though they still generally provide substantial memory savings. What might be
viewed as best- and worst-case behavior is also described in (Boutilier et al., 1999). In a
series of \linear" examples (i.e., problems with value functions that can be represented with
trees whose size is linear in the number of problem variables), the tree-based scheme solves
problems many orders of magnitude faster than classical state-based techniques. In contrast, problems with exponentially-many distinct values are also tested (i.e., with a distinct
value at each state): here tree-construction methods are required to construct a complete
decision tree in addition to performing the same number of expected value and maximization
computations as classical methods. In this worst case, tree-construction overhead makes
the algorithm run about 100 times slower than standard modified policy iteration.
In (Hoey et al., 1999), a similar algorithm is described that uses algebraic decision
diagrams (ADDs) (Bahar, Frohm, Gaona, Hachtel, Macii, Pardo, & Somenzi, 1993) rather
than trees. ADDs are a simple generalization of boolean decision diagrams (BDDs) (Bryant,
1986) that allow terminal nodes to be labeled with real values instead of just boolean values.
Essentially, ADD-based algorithms are similar to the tree-based algorithms except that
isomorphic subtrees can be shared. This lets ADDs provide more compact representations
of certain types of value functions. Highly optimized ADD manipulation and evaluation
software developed in the verification community can also be applied to solving MDPs.
Initial results provided in (Hoey et al., 1999) are encouraging, showing considerable savings
over tree-based algorithms on the same problems. For example, the ADD algorithm applied
to the 1.7-million-state example described above revealed the value function to have only
178 distinct values (cf. the 40,000 tree leaves required) and produced an ADD description
of the value function with less than 2200 internal nodes. It also solved the same problem
in seven minutes, about 40 times faster than earlier reported timing results using decision
trees (though some of this improvement was due to the use of optimized ADD software
packages). Similar results obtain with other problems (problems of up to 268 million states
44. Dietterich and Flann (1995) also describe their work in the context of reinforcement learning rather than
as a method for solving MDPs directly.

66

fiDecision-Theoretic Planning: Structural Assumptions

were solved in about four hours). Most encouraging is the fact that on the worst-case
(exponential) examples, the overhead associated with using ADDs|compared to classical,
vector-based methods|is much less than with trees (about a factor of 20 compared to \at"
modified policy iteration with 12 state variables), and lessens as problems become larger.
Like tree-based algorithms, these methods have yet to be applied to real-world problems.
With these exact abstraction schemes it is clear that, while in some examples the resulting policies and value functions may be compact, in others the set of regions may get very
large (even reaching the level of individual states Boutilier et al., 1995), thus precluding
any computational savings. Boutilier and Dearden (1996) develop an approximation scheme
that exploits the tree-structured nature of the value functions produced. At each stage k,
the value function Vk can be pruned to produce a smaller, less accurate tree that approximates Vk . Specifically, approximate value functions are represented using trees whose leaves
are labeled with an upper and lower bound on the value function in that region; decisiontheoretic regression is performed on these bounds. Certain subtrees of the value tree can
be pruned when leaves of the subtree are very close in value or when the tree is too large
given computational constraints. This scheme is nonuniform, approximate and adaptive.
This approximation scheme can be tailored to provide (roughly) the most accurate value
function of a given maximum tree size, or the smallest value function (with respect to tree
size) of some given minimum accuracy. Results reported in (Boutilier & Dearden, 1996)
show that approximation on a small set of examples (including the worst-case examples for
tree-based algorithms) allows substantial reduction in computational cost. For instance, in
a 10-variable worst-case example, a small amount of pruning introduced an average error of
only 0.5% but reduced computation time by a factor of 50. More aggressive pruning tends
to increase error and decrease computation time very rapidly; making appropriate tradeoffs
in these two dimensions is still to be addressed. This method too remains to be tested and
evaluated on realistic problems.
Structured representations and solution algorithms can be applied to problems other
than FOMDPs. Methods for solving inuence diagrams (Shachter, 1986) exploit structure
in a natural way; Tatman and Shachter (1990) explore the connection between inuence diagrams and FOMDPs and the relationship between inuence diagram solution techniques and
dynamic programming. Boutilier and Poole (1996) show how classic history-independent
methods for solving POMDPs, based on conversion to a FOMDP with belief states, can exploit the types of structured representations described here. However, exploiting structured
representations of POMDPs remains to be explored in depth.
5.1.3 Abstract Plans

One of the diculties with the adaptive abstraction schemes suggested above is the fact
that different abstractions must be constructed repeatedly, incurring substantial computational overhead. If this overhead is compensated by the savings obtained during policy
construction|e.g., by reducing the number of backups|then it is not problematic. But in
many cases the savings can be dominated by the time and space required to generate the
abstractions, and thus motivates the development of cheaper but less accurate approximate
clustering schemes.
67

fiBoutilier, Dean, & Hanks

Another way to reduce this overhead is to adopt a fixed abstraction scheme so that
only one abstraction is ever produced. This approach has been adopted in classical planning in hierarchical or abstraction-based planners, pioneered by Sacerdoti's AbStrips system (Sacerdoti, 1974). A similar form of abstraction is studied by Knoblock (1993) (see also
Knoblock, Tenenberg, & Yang, 1991). In this work, variables (in this case propositional) are
ranked according to criticality (roughly, how important such variables are to the solution
of the planning problem) and an abstraction is constructed by deleting from the problem
description a set of propositions of low criticality. A solution to this abstract problem is a
plan that achieves the elements of the original goal that have not been deleted. However,
preconditions and effects of actions that have been deleted are not accounted for in this solution, so it might not be a solution to the original problem. Even so, the abstract solution
can be used to restrict search for a solution in the underlying concrete space. Very often
hierarchies of more and more refined abstractions are used and propositions are introduced
back into the domain in stages.
This form of abstraction is uniform (propositions are deleted uniformly) and fixed. Since
the abstract solution need not be a solution to the problem, we might be tempted to view
it as an approximate abstraction method. However, it is best not to think of the abstract
plan as a solution at all, rather as a form of heuristic information that can help solve the
true problem more quickly.
The intuitions underlying Knoblock's scheme are applied to DTP by Boutilier and Dearden (1994, 1997): variables are ranked according to their degree of inuence on the reward
function and a subset of the most important variables is deemed relevant. Once this subset
is determined, those variables that inuence the relevant variables through the effects of
actions (which can be determined easily using Strips or 2TBN action descriptions) are
also deemed relevant, and so on. All remaining variables are deemed irrelevant and are
deleted from the description of the problem (both action and reward descriptions). This
leaves an abstract MDP with a smaller state space (i.e., fewer variables) that can be solved
by standard methods. Recall that the state space reduction is exponential in the number of
variables removed. We can view this method as an uniform fixed approximate abstraction
scheme. Unlike the output of classical abstraction methods, the abstract policy produced
can be implemented and has a value. The degree to which the optimal abstract policy and
the true optimal policy differ in value can be bounded a priori once the abstraction is fixed.

Example 5.3 As a simple illustration, suppose that the reward for satisfying coffee requests

(or penalty for not satisfying them) is substantially greater than that for keeping the
lab tidy or for delivering mail. Suppose that time pressure requires our agent to focus
on a specific subset of objectives in order to produce a small abstract state space. In
this case, of the four reward-laden variables in our problem (see Figure 24), only CR
will be judged to be important. When the action descriptions are used to determine
the variables that can (directly or indirectly) affect the probability of achieving CR,
only CR, RHC and Loc will be deemed relevant, allowing T , M , and RHM to be
ignored. The state space is thus reduced from size 400 to size 20. In addition, several
of the action descriptions (e.g., Tidy) become trivial and can be deleted. 2
68

fiDecision-Theoretic Planning: Structural Assumptions

The advantage of these abstractions is that they are easily computed and incur little
overhead. The disadvantages are that the uniform nature of such abstractions is restrictive,
and the relevant \reward variables" are determined before the policy is constructed and
without knowledge of the agent's ability to control these variables. As a result, important
variables|those that have a large impact on reward|but over which the agent has no
control, may be taken into account, while less important variables that the agent can actually
inuence are ignored. However, a series of such abstractions can be used that take into
account objectives of decreasing importance, and the a posteriori most valuable objectives
can be dealt with once risk and controllability are taken into account (Boutilier et al.,
1997). The policies generated at more abstract levels can also be used to \seed" value or
policy iteration at less abstract levels, in certain cases reducing the time to convergence
(Dearden & Boutilier, 1997). It has also been suggested (Dearden & Boutilier, 1994, 1997)
that the abstract value function be used as a heuristic in an online search for policies that
improve the abstract policy so constructed, as discussed in Section 3.2.2. Thus, the error
in the approximate value function is overcome to some extent by search, and the heuristic
function can be improved by asynchronous updates.
A different use of abstraction is adopted in the DRIPS planner (Haddawy & Suwandi,
1994; Haddawy & Doan, 1994). Actions can be abstracted by collapsing \branches," or possible outcomes, and maintaining probabilistic intervals over the abstract, disjunctive effects.
Actions are also combined in an decomposition hierarchy, much like those in hierarchical
task networks. Planning is done by evaluating abstract plans in the decomposition network, producing ranges of utility for the possible instantiations of those plans, and refining
only those plans that are possibly optimal. The use of task networks means that search is
restricted to finite-horizon, open-loop plans with action choice restricted to possible refinements of the network. Such task networks offer a useful way to encode a priori heuristic
knowledge about the structure of good plans.
5.1.4 Model Minimization and Reduction Methods

The abstraction techniques defined above can be recast in terms of minimizing a stochastic
automaton, providing a unifying view of the different methods and offering new insights
into the abstraction process (Dean & Givan, 1997). From automata theory we know that
for any given finite-state machine M recognizing a language L there exists a unique minimal
finite-state machine M 0 that also recognizes L. It could be that M = M 0 , but it might also
be that M 0 is exponentially smaller than M . This minimal machine, called the minimal
model for the language L, captures every relevant aspect of M and so the machines are
said to be equivalent. We can define similar notions of equivalence for MDPs. Since we are
primarily concerned with planning, it is important that equivalent MDPs agree on the value
functions for all policies. From a practical standpoint, it may not be necessary to find the
minimal model if we can find a reduced model that is suciently small but still equivalent.
We apply the idea of model minimization (or model reduction) to planning as follows:
we begin by using an algorithm that takes as input an implicit MDP model in factored form
and produces (if we are lucky) an explicit, reduced model whose size is within a polynomial
factor of the size of the factored representation. We then use our favorite state-based
dynamic programming algorithms to solve the explicit model.
69

fiBoutilier, Dean, & Hanks

We can think of the dynamic programming techniques that rely on structured representations discussed earlier as operating on a reduced model without ever explicitly constructing
that model. In some cases, building the reduced model once and for all may be appropriate;
in other cases, one might save considerable effort by explicitly constructing only those parts
of the reduced model that are absolutely necessary.
There are some potential computational problems with the model-minimization techniques sketched above. A small minimal model may exist, but it may be hard to find.
Instead, we might look for a reduced model that is easier to find but not necessarily minimal. This too could fail, in which case we might look for a model small enough to be useful
but only approximately equivalent to the original factored model. We have to be careful
what we mean by \approximate," but intuitively two MDPs are approximately equivalent
if the corresponding optimal value functions are within some small factor of one another.
In order to be practical, MDP model reduction schemes operate directly on the implicit
or factored representation of the original MDP. Lee and Yannakakis (1992) call this online
model minimization. Online model minimization starts with an initial partition of the states.
Minimization then iteratively refines the partition by splitting clusters into smaller clusters.
A cluster is split if and only if the states in the cluster behave differently with respect to
transitions to states in the same or other clusters. If this local property is satisfied by all
clusters in a given partition, then the model consisting of aggregate states that correspond
to the clusters of this partition is equivalent to the original model. In addition, if the
initial partition and the method of splitting clusters satisfy certain properties,45 then we
are guaranteed to find the minimal model. In the case of MDP reduction, the initial partition
groups together states that have the same reward, or nearly the same reward in the case of
approximation methods.
The clusters of the partitions manipulated by online model reduction methods are represented intensionally as formulas involving the state variables. For instance, the formula
RHC ^ Loc(M ) represents the set of all states such that the robot has coffee and is located
in the mail room. The operations performed on these clusters require conjoining, complementing, simplifying, and checking for satisfiability. In the worst case, these operations are
intractable, and so the successful application of these methods depends critically on the
problem and the way in which it is represented. We illustrate the basic idea on a simple
example.

Example 5.4 Figure 30 depicts a simple version of our running example with a single
action. There are three boolean state variables corresponding to RHC|the robot has
coffee (or not, RHC), CR|there is an outstanding request for coffee (or not, CR),
and, considering only two location possibilities, Loc(C )|the robot is in the coffee
room (or not, Loc(C )). Whether there is an outstanding coffee request depends on
whether there was a request in the previous stage and whether the robot was in the
coffee room. Location depends only on the location at the previous stage, and the
reward depends only on whether or not there is an outstanding coffee request.

45. The property required of the initial partition is that, if two states are in the same cluster of the partition
defining the minimal model (recall that the minimal model is unique), then they must be in same cluster
in the initial partition.

70

fiDecision-Theoretic Planning: Structural Assumptions

St  1

St

CR

CR

Pr(CR S t  1)
CR
CR
Loc(C)
Loc(C)
0.8
0.7
0.9

Loc

Loc

Pr(Loc(C) S t  1) = 0.7

RHC

Pr(RHC S t  1)
Loc(C)
Loc(C)
RHC
RHC
0.5
0.7
1.0

R


R(S t) =  1 if CR
 0 else

RHC

Figure 30: Factored model illustrating model-reduction techniques.
CR  Loc(C)

CR
CR

CR
CR  Loc(C)
(a)

(b)

Figure 31: Models involving aggregate states: (a) the model corresponding to the initial
partition and (b) the minimal model.
The initial partition shown in Figure 31(a) is defined in terms of immediate rewards.
We say that all the states in a particular starting cluster behave the same with respect
to a particular destination cluster if the probability of ending up in the destination
cluster is the same for all states in the starting cluster. This property is not satisfied
for starting cluster CR and destination cluster CR in Figure 31(a), and so we split the
cluster labeled CR to obtain the model in Figure 31(b). Now the property is satisfied
for all pairs of clusters and the model in Figure 31(b) is the minimal model. 2
The Lee and Yannakakis algorithm for non-deterministic finite-state machines has been
extended by Givan and Dean to handle classical Strips planning problems (Givan & Dean,
1997) and MDPs (Dean & Givan, 1997). The basic step of splitting a cluster is closely
related to goal regression, a relationship explored in (Givan & Dean, 1997). Variants of
the model reduction approach apply when the action space is large and represented in a
factored form (Dean, Givan, & Kim, 1998); for example, when each action is specified
by a set of parameters such as those corresponding to the allocations of several different
resources in an optimization problem. There also exist algorithms for computing approxi71

fiBoutilier, Dean, & Hanks

A

A

A

R

G

C

G
P

B

B

D

E

B

(a)

(b)

(c)

Figure 32: Reachability and serial problem decomposition.
mate models (Dean, Givan, & Leach, 1997) and ecient planning algorithms that use these
approximate models (Givan, Leach, & Dean, 1997).

5.2 Reachability Analysis and Serial Problem Decomposition
5.2.1 Reachability Analysis

The existence of goal states can be exploited in different settings. For instance, in deterministic classical planning problems, regression can be viewed as a form of directed dynamic
programming. Without uncertainty, a certain policy either reaches a goal state or does not,
and the dynamic programming backups need be performed only from goal states, not from
all possible states. Regression, therefore, implicitly exploits certain reachability characteristics of the domain along with the special structure of the value function.
Reachability analysis applied much more broadly forms the basis for various types of
problem decomposition. In decomposition problem solving, the MDP is broken into several
subprocesses that can be solved independently, or roughly independently, and the solutions
can be pieced together. If subprocesses whose solutions interact marginally are treated as
independent, we might expect a good but nonoptimal global solution to result. Furthermore,
if the structure of the problem requires that only a solution to a particular subproblem is
needed, then the solutions to other subproblems can be ignored or need not be computed
at all. For instance, in regression analysis, the optimal action for states that cannot reach
a goal region is irrelevant to the solution of a classical AI planning problem. This is shown
schematically in Figure 32(a), where regions A and B are never explored in the backward
search through state space: only states that can reach the goal within the search horizon
are ever deemed relevant. While regions A and B may be reachable from the start state, the
fact that they do not reach the goal state means they are known to be irrelevant. Should
the system dynamics be stochastic, such a scheme can form the basis of an approximately
optimal solution method: regions A and B can be ignored if they are unlikely to transition
to the regression of the goal region (region R). Similar remarks using progression or forward
search from the start state apply, as illustrated in Figure 32(b).
72

fiDecision-Theoretic Planning: Structural Assumptions

Several schemes have been proposed in the AI literature for exploiting such reachability
constraints, apart from the usual forward- or backward-search approaches. Peot and Smith
(1993) introduce the operator graph, a structure computed prior to problem solving that
caches reachability relationships among propositions. The graph can be consulted during
the planning process in deciding which actions to insert into the plan and how to resolve
threats.
The GraphPlan algorithm of Blum and Furst (1995) attempts to blend considerations
of both forward and backward reachability in a deterministic planning context. One of the
diculties with regression is that we may regress the goal region through a sequence of
operators only to find ourselves in a region that cannot be reached from the initial state.
In Figure 32(a), for example, not all states in region R may be reachable from the initial
state. GraphPlan constructs a variant of the operator graph called the planning graph, in
which certain forward reachability constraints are posted. Regression is then implemented
as usual, but if the current subgoal set violates the forward reachability constraints at any
point, this subgoal set is abandoned and the regression search backtracks.
Conceptually, one might think of GraphPlan as constructing a forward search tree
through state space with the initial state as the root, then doing a backward search from
the goal region backward through this tree. Of course, the process is not state-based:
instead, constraints on the possible variable values that can hold simultaneously at different
planning stages are recorded, and regression is used to search backward through the planning
graph. In a sense, GraphPlan can be viewed as constructing an abstraction in which
forward-reachable states are distinguished from unreachable states at each planning stage,
and using this distinction among abstract states quickly to identify infeasible regression
paths. Note, however, the GraphPlan approximates this distinction by overestimating
the set of reachable states. Overestimation (as opposed to underestimation) ensures that
the regression search space contains all legitimate plans.
Reachability has also been exploited in the solution of more general MDPs. Dean
et al. (1995) propose an envelope method for solving \goal-based" MDPs approximately.
Assuming some path can be generated quickly from a given start state to the goal region,
an MDP consisting of the states on this path and perhaps neighboring states is solved. To
deal with transitions that lead out of this envelope, a heuristic method estimates a value for
these states.46 As time permits, the set of neighboring states can be expanded, increasing
solution quality by more accurately evaluating the quality of alternative actions.
Some of the ideas underlying GraphPlan have been applied to more general MDPs in
(Boutilier, Brafman, & Geib, 1998), where the construction of a planning graph is generalized to deal with the stochastic, conditional action representation offered by 2TBNs. Given
an initial state (or set of initial states), this algorithm discovers reachability constraints
that have a form like those in GraphPlan | for instance, that two variable values X = x1
and Y = y3 cannot both obtain simultaneously; that is, no action sequence starting at the
given initial state can lead to a state in which these values both hold.47 The reachability
constraints discovered by this process are then used to simplify the action and reward representation of an MDP so that it refers only to reachable states. In this case, any action that
46. The approximate abstraction techniques described in Section 5.1.3 might be used to generate such
heuristic information.
47. General k-ary constraints of this type are considered in (Boutilier et al., 1998).

73

fiBoutilier, Dean, & Hanks

requires an unreachable set of values to hold is effectively deleted. In some cases, certain
variables are discovered to be immutable given the initial conditions and can themselves be
deleted, leading to much smaller MDPs. This simplified representation retains the original
propositional structure so standard abstraction methods can be applied to the reachable
MDP. It is also suggested that a strong synergy exists between abstraction and reachability analysis such that together these techniques reduce the size of the \effective" MDP to
be solved much more dramatically than either does in isolation. Just as reachability constraints can be used to prune regression paths in deterministic domains, they can be used
to prune value function and policy estimates generated by decision-theoretic regression and
abstraction algorithms (Boutilier et al., 1998).
The results reported in (Boutilier et al., 1998) are limited to a single process-planning
domain, but show that reachability analysis together with abstraction can provide substantial reductions in the size of the effective MDP that must be solved, at least in some domains.
In a domain with 31 binary variables, reachability considerations generally eliminated on
the order of 10 to 15 variables (depending on the initial state and the arity|binary or
ternary|of the constraints considered), reducing the state space from size 231 to anywhere
from 222 to 215 . Incorporating abstraction on the reachable MDP provided considerably
more reduction, reducing the MDP to sizes ranging from 28 to effectively zero states. The
latter case would occur if it is discovered that no values of variables that impact reward
can be altered|in which case every course of action has the same expected utility and the
MDP needn't be solved (or can be solved by applying null actions with zero cost).
5.2.2 Serial Problem Decomposition and Communicating Structure

The communicating or reachability structure of an MDP provides a way to formalize different types of problem decomposition. We can classify an MDP according to the Markov
chains induced by the stationary policies it admits. For a fixed Markov chain, we can group
states into maximal recurrent classes and transient states, as described in Section 2.1. An
MDP is recurrent if each policy induces a Markov chain with a single recurrent class. An
MDP is unichain if each policy induces a single recurrent class with (possibly) some transient states. An MDP is communicating if for any pair of states s; t, there is some policy
under which s can reach t. An MDP is weakly communicating if there exists a closed set
of states that is communicating plus (possibly) a set of states transient under every policy.
We call other MDPs noncommunicating.
These notions are crucial in the construction of optimal average-reward policies, but can
also be exploited in problem decomposition. Suppose an MDP is discovered to consist of a
set of recurrent classes C1 ;    Cn (i.e., no matter what policy is adopted, the agent cannot
leave any such class once it enters that class) and a set of transient states.48 It is clear that
optimal policy restricted to any class Ci can be constructed without reference to the policy
decisions made at any states outside of Ci or even their values. Essentially, each Ci can be
viewed as an independent subprocess.
48. A simple way to view these classes is to think of the agent adopting a randomized policy where each action
is adopted at any state with positive probability. The classes of the induced Markov chain correspond
to the classes of the MDP.

74

fiDecision-Theoretic Planning: Structural Assumptions

This observation leads to the following suggestion for optimal policy construction:49 we
solve the subprocesses consisting of the recurrent classes for the MDPs; we then remove
these states from the MDP, forming a reduced MDP consisting only of the transient states.
We then break the reduced MDP into its recurrent classes and solve these independently.
The key to doing this effectively is to use the value function for the original recurrent
states (computed in solving the independent subproblems in Step 1) to take into account
transitions out of the recurrent classes in the reduced MDP. Figure 32(c) shows an MDP
broken into the classes that might be constructed this way. In the original MDP, classes C
and E are recurrent and can be solved independently. Once removed from the MDP, class
D is recurrent in the reduced MDP. It can, of course, be solved without reference to classes
A and B , but does rely on the value of the states that it transitions to in class E . However,
the value function for E is available for this purpose, and can be used to solve for D as
if D consisted only of jDj states. With this in hand, B can then be solved, and finally A
can be solved. Lin and Dean (1995) provide a version of this type of decomposition that
also employs a factored representation. The factored representation allows dimensionality
reduction in different state subspaces by aggregating states that differ only in the values of
the irrelevant variables in their subspaces.
A key to such a decomposition is the discovery of the recurrent classes of an MDP.
Puterman (1994) suggests an adaptation of the Fox-Landi algorithm (Fox & Landi, 1968)
for discovering the structure of Markov chains that is O(N 2 ) (recall N = jSj).50 To alleviate
the diculties of algorithms that work with an explicit state-based representation, Boutilier
and Puterman (1995) propose a variant of the algorithm that works with a factored 2TBN
representation.
One diculty with this form of decomposition is its reliance on strongly independent
subproblems (i.e., recurrent classes) within the MDP. Others have explored exact and approximate techniques that work under less restrictive assumptions. One simple method of
approximation is to construct \approximately recurrent classes." In Figure 32(c) we might
imagine that C and E are nearly independent in the sense that all transitions between them
are very low-probability or high-cost. Treating them as independent might lead to approximately optimal policies whose error can be bounded. If the solutions to C and E interact
strongly enough that the solutions should not be constructed completely independently, a
different approach to solving the decomposed problem can be taken.
If we have the optimal value function for E then, as pointed out, we can calculate the
optimal value function for D. The first thing to note is that we don't need to know the
value function for all of the states in E , just the value of every state in E that is reachable
from some state in D in a single step. The set of all states outside D reachable in a single
step from a state inside D is referred to as the states in the periphery of D. The values of
the states in the intersection of E and the periphery of D summarize the value of exiting D
and ending up in E . We refer to the set of all states that are in the periphery of some block
as the kernel of the MDP. All of the different blocks interact with one another through
states in the kernel.
49. Ross and Varadarajan (1991) make a related suggestion for solving average-reward problems.
50. A slight correction is made to the suggested algorithm in (Boutilier & Puterman, 1995).

75

fiBoutilier, Dean, & Hanks

Loc(C)

Loc(L)

Loc(M )

Loc(O)

Figure 33: Decomposition based on location.

Loc(C)

Loc(L)
Kernel

Loc(M )

Loc(O)

Figure 34: Kernel-based decomposition depicting the kernel states.

76

fiDecision-Theoretic Planning: Structural Assumptions

Example 5.5 Spatial features often provide a natural dimension along which to decom-

pose a domain. In our running example, the location of the robot might be used to
decompose the state space into blocks of states, one block for each of the possible locations. Figure 33 shows such a decomposition superimposed over the state-transition
diagram for the MDP. States in the kernel are shaded and might correspond to the
entrances and exits of locations. The star-shaped topology, induced by the kernel
decomposition used in (Kushner & Chen, 1974) and (Dean & Lin, 1995), is illustrated
in Figure 34. In Figure 33, the hallway location is not explicitly represented. This
simplification may be reasonable if the hallway is only a conduit for moving from
one room to another; in this case the function of the hallway is accounted for in the
dynamics governing states in the kernel. Figures 33 and 34 are idealized in that, given
the full set of features in our running example, the kernel would contain many more
states. 2

One technique for computing the optimal policy for the entire MDP involves repeatedly
solving the MDPs corresponding to the individual blocks. The techniques works as follows:
initially, we guess the value of every state in the kernel.51 Given a current estimate for the
values of the kernel states, we solve the component MDPs; this solution produces a new
estimate for the states in the kernel. We adjust the values of the states in the kernel by
considering the difference between the current and the new estimates and iterate until this
difference is negligible.
This iterative method for solving a decomposed MDP is a special case of the Lagrangian
method for finding the extrema of a function. The OR literature is replete with such
methods for both linear and nonlinear systems of equations (Winston, 1992). It is possible
to formulate an MDP as a linear program (D'Epenoux, 1963; Puterman, 1994). Dantzig and
Wolfe (1960) developed a method of decomposing a system of equations involving a very
large number of variables into a set of smaller systems of equations interacting through a set
of coupling variables (variables shared by more two or more blocks). In the Dantzig-Wolfe
decomposition method, the original, very large system of equations is solved by iteratively
solving the smaller systems and adjusting the coupling variables on each iteration until no
further adjustment is required. In the linear programming formulation of an MDP, the
values of the states are encoded as variables.
Kushner and Chen (1974) exploit the fact that MDPs can be modeled as linear programs
by using the Dantzig-Wolfe decomposition method to solve MDPs involving a large number
of states. Dean and Lin (1995) describe a general framework for solving decomposed MDPs
pointing to the work of Kushner and Chen as a special case, but neither work addresses
the issue of where the decompositions come from. Dean et al. (1995) investigate methods
for decomposing the state space into two blocks: those reachable in k steps or fewer and
those not reachable in k steps (see the discussion of reachability above). The set of states
reachable in k or fewer steps is used to construct an MDP that is the basis for a policy that
approximates the optimal policy. As k increases, the size of the block of states reachable in
k steps increases, ensuring a better solution; but the amount of time required to compute a
51. Ideally we would aggregate kernel states with the same value so as to provide a compact representation.
In the remainder of this section, however, we won't consider this or any other opportunities for combining
aggregation and decomposition methods.

77

fiBoutilier, Dean, & Hanks

solution also increases. Dean et al. (1995) discuss methods for solving MDPs in time-critical
problems by trading off quality against time.
We have ignored the issue of how to obtain decompositions that expedite our calculations. Ideally, each component of the decomposition would yield to simplification via
aggregation and abstraction, reducing the dimensionality in each component and thereby
avoiding explicit enumeration of all the states. Lin (1997) presents methods for exploiting
structure for certain special cases in which the communicating structure is revealed by a
domain expert. In general, however, finding a decomposition so as to minimize the effort
spent in solving the component MDPs is quite hard (at least as hard as finding the smallest circuit consistent with a given input-output behavior) and so the best we can hope for
are good heuristic methods. Unfortunately, we are not aware of any particularly useful
heuristics for finding serial decompositions for Markov decision processes. Developing such
heuristics is clearly an area for investigation.
Related to this form of decomposition is the development of macro operators for MDPs
(Sutton, 1995). Macros have a long history in classical planning and problem solving (Fikes,
Hart, & Nilsson, 1972; Korf, 1985), but only recently have they been generalized to MDPs
(Hauskrecht, Meuleau, Kaelbling, Dean, & Boutilier, 1998; Parr, 1998; Parr & Russell, 1998;
Precup, Sutton, & Singh, 1998; Stone & Veloso, 1999; Sutton, 1995; Thrun & Schwartz,
1995). In most of this work, a macro is taken to be a local policy over a region of state
space (or block in the above terminology). Given an MDP comprising these blocks and a
set of macros defined for each block, the MDP can be solved by selecting a macro action
for each block such that the global policy induced by the set of macros so picked is close
to optimal, or at the very least is the best combination of macros from the set available.
In (Sutton, 1995; Precup et al., 1998), macros are treated as temporally-abstract actions
and models are defined by which a macro can be treated as if it were a single action and
used in policy or value iteration (along with concrete actions). In (Hauskrecht et al., 1998;
Parr, 1998; Parr & Russell, 1998), these models are exploited in a hierarchical fashion, with
a high-level MDP consisting only of states lying on the boundaries of blocks, and macros
the only \actions" that can be chosen at these states. The issue of macro generation|
constructing a set of macros guaranteed to provide the exibility to select close to optimal
global behavior|is addressed in (Hauskrecht et al., 1998; Parr, 1998). The relationship
to serial decomposition techniques is quite close; thus, the problems of discovering good
decompositions, constructing good sets of macros, and exploiting intensional representations
are areas in which clearer, compelling solutions are required. To date, work in this area has
not provided much computational utility in the solution of MDPs|except in cases where
good, hand-crafted, region-based decompositions and macros can be provided|and little
of this work has taken into account the factored nature of many MDPs. For this reason, we
do not discuss it in detail. However, the general notion of serial decomposition continues to
develop and shows great promise.

5.3 Multiattribute Reward and Parallel Decomposition
Another form of decomposition is parallel decomposition, in which an MDP is broken into
a set of sub-MDPs that are \run in parallel." Specifically, at each stage of the (global)
decision process, the state of each subprocess in affected. For instance, in Figure 35, action
78

fiDecision-Theoretic Planning: Structural Assumptions

a

MDP1

a

MDP2

a

MDP3

Figure 35: Parallel problem decomposition.

a affects the state of each subprocess. Intuitively, an action is suitable for execution in the

original MDP at some state if it is reasonably good in each of the sub-MDPs.
Generally, the sub-MDPs form either a product or join decomposition of the original
state space (contrast this with the union decompositions of state space determined by serial
decompositions): the state space is formed by taking the cross product of the sub-MDP state
spaces, or the join if certain states in the subprocesses cannot be linked. The subprocesses
may have identical action spaces (as in Figure 35), or each may have its own action space,
with the global action choice being factored into a choice for each subprocess. In the latter
case, the sub-MDPs may be completely independent, in which case the (global) MDP can be
solved exponentially faster. A more challenging problem arises when there are constraints
on the legal action combinations. For example, if the actions in the subprocesses each
require certain shared resources, interactions in the global choice may arise.
In a parallel MDP decomposition, we wish to solve the sub-MDPs and use the policies
or value functions generated to help construct an optimal or approximately optimal solution
to the original MDP, highlighting the need to find appropriate decompositions for MDPs
and to develop suitable merging techniques. Recent parallel decomposition methods have
all involved decomposing an MDP into subprocesses suitable for distinct objectives. Since
reward functions often deal with multiple objectives, each associated with an independent
reward, and whose rewards can be summed to determine a global reward, this is often a
very natural way to decompose MDPs. Thus, ideas from multiattribute utility theory can
be seen to play a role in the solution of MDPs.
Boutilier et al. (1997) decompose an MDP specified using 2TBNs and an additive reward
function using the abstraction technique described in Section 5.1.3. For each component
of the reward function, abstraction is used to generate an MDP referring only to variables
relevant to that component.52 Since certain state variables may be present in multiple
sub-MDPs (i.e., relevant to more than one objective), the original state space in the join of
the subspaces. Thus, decomposition is tackled automatically. Merging is tackled in several
ways. One involves using the sum of the value functions obtained by solving the sub-MDPs
as a heuristic estimate of the true value function. This heuristic is then used to guide online,
state-based search (see Section 3.2.1). If the sub-MDPs do not interact, then this heuristic
is perfect and leads to backtrack-free optimal action selection; if they interact, search is
52. Note that the existence of a factored MDP representation is crucial for this abstraction method.

79

fiBoutilier, Dean, & Hanks

required to detect conicts. Note that each sub-MDP has identical sets of actions. If the
action space is large, the branching factor of the search process may be prohibitive.
Singh and Cohn (1998) also deal with parallel decomposition, though they assume the
global MDP is specified explicitly as a set of parallel MDPs, thus generating decompositions
of a global MDP is not at issue. The global MDP is given by the cross product of the state
and action spaces of these sub-MDPs and the reward functions are summed. However,
constraints on the feasible action combinations couple the solutions of these sub-MDPs. To
solve the global MDP, the sum of the sub-MDP value functions is used as an upper bound
on the optimal global value function, while the maximum of these (at any global state) is
used as a lower bound. These bounds then form the basis of an action-elimination procedure
in a value-iteration algorithm for solving the global MDP.53 Unfortunately, value iteration
is run over the explicit state space of the global MDP. Since the action space is also a cross
product, this is a potential computational bottleneck for value iteration, as well.
Meuleau et al. (1998) use parallel decomposition to approximate the solution of stochastic resource allocation problems with very large state and action spaces. Much like Singh
and Cohn (1998), an MDP is specified in terms of a number of independent MDPs, each
involving a distinct objective, whose action choices are linked through shared resource constraints. The value functions for the individual MDPs are constructed oine and then used
in set of online action-selection procedures. Unlike many of the approximation procedures
we have discussed, this approach makes no attempt to construct a policy explicitly (and
is similar to real-time search or RTDP in this respect) nor to construct the value function
explicitly. This method has been applied to very large MDPs, with state spaces of size
21000 and actions spaces that are even larger, and can solve such problems in roughly half
an hour. The solutions produced are approximate, but the size of the problem precludes
exact solution; so good estimates of solution quality are hard to derive. However, when the
same method is applied to smaller problems of the same nature whose exact solution can
be computed, the approximations have very high quality (Meuleau et al., 1998). While able
to solve very large MDPs (with large, but factored, state and action spaces), the model
relies on somewhat restrictive assumptions about the nature of the local value functions
that ensure good solution quality. However, the basic approach appears to be generalizable,
and offers great promise for solving very large factored MDPs.
The algorithms in both (Singh & Cohn, 1998) and (Meuleau et al., 1998) can be seen
to rely at least implicitly on structured MDP representations involving almost independent
subprocesses. It seems likely that such approaches could take further advantage of automatic
MDP decomposition algorithms such as that of (Boutilier et al., 1997), where factored
representations explicitly play a part.

5.4 Summary

We have seen a number of ways in which intensional representations can be exploited to
solve MDPs effectively without enumeration of the state space. These include techniques
for abstraction of MDPs, including those based on relevance analysis, goal regression and
decision-theoretic regression; techniques relying on reachability analysis and serial decomposition; and methods for parallel MDP decomposition exploiting the multiattribute nature
53. Singh and Cohn (1998) also incorporate methods for removing unreachable states during value iteration.

80

fiDecision-Theoretic Planning: Structural Assumptions

of reward functions. Many of these methods can, in fortunate circumstances, offer exponential reduction is solution time and space required to represent a policy and value function;
but none come with guarantees of such reductions except in certain special cases. While
most of the methods described provide approximate solutions (often with error bounds provided), some of them offer optimality guarantees in general, and most can provide optimal
solutions under suitable assumptions.
One avenue that has not been explored in detail is the relationship between the structured solution methods developed for MDPs described above and techniques used for solving
Bayesian networks. Since many of the algorithms discussed in this section rely on the structure inherent in the 2TBN representation of the MDP, it is natural to ask whether they
embody some of the intuitions that underlie solution algorithms for Bayes nets, and thus
whether the solution techniques for Bayes nets can be (directly or indirectly) applied to
MDPs in ways that give rise to algorithms similar to those discussed here. This remains an
open question at this point, but undoubtedly some strong ties exist. Tatman and Shachter
(1990) have explored the connections between inuence diagrams and MDPs. Kjaerulff
(1992) has investigated computational considerations involved in applying join tree methods
for reasoning tasks such as monitoring and prediction in temporal Bayes nets. The abstraction methods discussed in Section 5.1.2 can be interpreted as a form of variable elimination
(Dechter, 1996; Zhang & Poole, 1996). Elimination of variables occurs in temporal order,
but good orderings within a time slice must also exploit the tree or graph structure of the
CPTs. Approximation schemes based on variable elimination (Dechter, 1997; Poole, 1998)
may also be related to certain of the approximation methods developed for MDPs. The
independence-based decompositions of MDPs discussed in Section 5.3 can clearly be viewed
as exploiting the independence relations made explicit by \unrolling" a 2TBN. The development of these and other connections to Bayes net inference algorithms will no doubt prove
very useful in enhancing our understanding of existing methods, increasing their range of
applicability and pointing to new algorithms.

6. Concluding Remarks
The search for effective algorithms for controlling automated agents has a long and important history, and the problem will only continue to grow in importance as more decisionmaking functionality is automated. Work in several disciplines, among them AI, decision
analysis, and OR, has addressed the problem, but each has carried with it different problem definitions, different sets of simplifying assumptions, different viewpoints, and hence
different representations and algorithms for problem solving. More often than not, the assumptions seem to have been made for historical reasons or reasons of convenience, and it
is often dicult to separate the essential assumptions from the accidental. It is important
to clarify the relationships among problem definitions, crucial assumptions, and solution
techniques, because only then can a meaningful synthesis take place.
In this paper we analyzed various approaches to a particular class of sequential decision problems that have been studied in the OR, decision analysis, and AI literature. We
started with a general, reasonably neutral statement of the problem, couched, for convenience, in the language of Markov decision processes. From there we demonstrated how
various disciplines define the problem (i.e., what assumptions they make), and the effect
81

fiBoutilier, Dean, & Hanks

of these assumptions on the worst-case time complexity of solving the problem so defined.
Assumptions regarding two main factors seem to distinguish the most commonly studied
classes of decision problems:

 observation or sensing: does sensing tend to be fast, cheap, and accurate or laborious,
costly and noisy?

 the incentive structure for the agent: is its behavior evaluated on its ability to perform
a particular task, or on its ability to control a system over an interval of time?

Moving beyond the worst-case analysis, it is generally assumed that, although pathological cases are inevitably dicult, the agent should be able to solve \typical" or \easy"
cases effectively. To do so, the agent needs to be able to identify structure in the problem
and to exploit that structure algorithmically.
We identified three ways in which structural regularities can be recognized, represented,
and exploited computationally. The first is structure induced by domain-level simplifying
assumptions like full observability, goal satisfaction or time-separable value functions, and
so on. The second is structure exploited by compact domain-specific encodings of states,
actions, and rewards. The designer can use these techniques to make structure explicit, and
decision-making algorithms can then exploit the structural regularities as they apply to the
particular problem at hand. The third involves aggregation, abstraction and decomposition techniques, whereby structural regularities can be discovered and exploited during the
problem-solving process itself. In developing this framework|one that allows comparison
of domains, assumptions, problems, and techniques drawn from different disciplines|we
discover the essential problem structure required for specific representations and algorithms
to prove effective; and we do so in such a way that the insights and techniques developed
for certain problems, or within certain disciplines, can be evaluated and potentially applied
to new problems, or within other disciplines.
A main focus of this work has been the elucidation of various forms of structure in
decision problems and of how each can be exploited representationally or computationally.
For the most part, we have focused on propositional structure, which is most commonly associated with planning in AI circles. A more complete treatment would also have included
other compact representations of dynamics, rewards, policies, and value functions often
considered in continuous, real-valued domains. For instance, we have not discussed linear
dynamics and quadratic cost functions, often used in control theory (Caines, 1988), or the
use of neural-network representations of value functions, as frequently adopted within the
reinforcement learning community (Bertsekas & Tsitsiklis, 1996; Tesauro, 1994),54 nor have
we discussed the partitioning of continuous state spaces often addressed in reinforcement
learning (Moore & Atkeson, 1995). Neither have we addressed the relational or quantificational structure used in first-order planning representations. However, even these techniques
can be cast within the framework described here; for example, the use of piecewise-linear
value functions can be seen as a form of abstraction in which different linear components
are applied to different regions or clusters of state space.
54. Bertsekas and Tsitsiklis (1996) provide an in-depth treatment of neural network and linear function
approximators for MDPs and reinforcement learning.

82

fiDecision-Theoretic Planning: Structural Assumptions

Although in certain cases we have indicated how to devise methods that exploit several
types of structure at once, research along these lines has been limited. To some extent,
many of the representations and algorithms described in this paper are complementary and
should pose few obstacles to combination. It remains to be seen how they interact with
techniques developed for other forms of structure, such as those used for continuous state
and action spaces.
So our analysis raises opportunities and challenges: by understanding the assumptions,
the techniques, and their relationships, a designer of decision-making agents has many more
tools with which to build effective problem solvers; and the challenges lie in the development
of additional tools and the integration of existing ones.

Acknowledgments

Many thanks to the careful comments of the referees. Thanks to Ron Parr and Robert
St-Aubin for their comments on an earlier draft of this paper. The students taking CS3710
(Spring 1999) taught by Martha Pollack at the University of Pittsburgh and CPSC522
(Winter 1999) at the University of British Columbia also deserve thanks for their detailed
comments.
Boutilier was supported by NSERC Research Grant OGP0121843, and the NCE IRISII program Project IC-7. Dean was supported in part by a National Science Foundation
Presidential Young Investigator Award IRI-8957601 and by the Air Force and the Advanced
Research Projects Agency of the Department of Defense under Contract No. F30602-91-C0041. Hanks was supported in part by ARPA / Rome Labs Grant F30602{95{1{0024 and
in part by NSF grant IRI{9523649.

References

Allen, J., Hendler, J., & Tate, A. (Eds.). (1990). Readings in Planning. Morgan-Kaufmann,
San Mateo.
Astrom, K. J. (1965). Optimal control of Markov decision processes with incomplete state
estimation. J. Math. Anal. Appl., 10, 174{205.
Bacchus, F., Boutilier, C., & Grove, A. (1996). Rewarding behaviors. In Proceedings of
the Thirteenth National Conference on Artificial Intelligence, pp. 1160{1167 Portland,
OR.
Bacchus, F., Boutilier, C., & Grove, A. (1997). Structured solution methods for nonMarkovian decision processes. In Proceedings of the Fourteenth National Conference
on Artificial Intelligence, pp. 112{117 Providence, RI.
Bacchus, F., & Kabanza, F. (1995). Using temporal logic to control search
in a forward chaining planner.
In Proceedings of the Third European
Workshop on Planning (EWSP'95) Assisi, Italy. Available via the URL
ftp://logos.uwaterloo.ca:/pub/tlplan/tlplan.ps.Z.
Bacchus, F., & Teh, Y. W. (1998). Making forward chaining relevant. In Proceedings of the
Fourth International Conference on AI Planning Systems, pp. 54{61 Pittsburgh, PA.
83

fiBoutilier, Dean, & Hanks

Bahar, R. I., Frohm, E. A., Gaona, C. M., Hachtel, G. D., Macii, E., Pardo, A., & Somenzi,
F. (1993). Algebraic decision diagrams and their applications. In International Conference on Computer-Aided Design, pp. 188{191. IEEE.
Baker, A. B. (1991). Nonmonotonic reasoning in the framework of the situation calculus.
Artificial Intelligence, 49, 5{23.
Barto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning to act using real-time dynamic
programming. Artificial Intelligence, 72 (1{2), 81{138.
Bellman, R. (1957). Dynamic Programming. Princeton University Press, Princeton, NJ.
Bertsekas, D. P., & Castanon, D. A. (1989). Adaptive aggregation for infinite horizon
dynamic programming. IEEE Transactions on Automatic Control, 34 (6), 589{598.
Bertsekas, D. P. (1987). Dynamic Programming. Prentice-Hall, Englewood Cliffs, NJ.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-dynamic Programming. Athena, Belmont,
MA.
Blackwell, D. (1962). Discrete dynamic programming. Annals of Mathematical Statistics,
33, 719{726.
Blum, A. L., & Furst, M. L. (1995). Fast planning through graph analysis. In Proceedings
of the Fourteenth International Joint Conference on Artificial Intelligence, pp. 1636{
1642 Montreal, Canada.
Bonet, B., & Geffner, H. (1998). Learning sorting and decision trees with POMDPs. In
Proceedings of the Fifteenth International Conference on Machine Learning, pp. 73{81
Madison, WI.
Bonet, B., Loerincs, G., & Geffner, H. (1997). A robust and fast action selection mechanism.
In Proceedings of the Fourteenth National Conference on Artificial Intelligence, pp.
714{719 Providence, RI.
Boutilier, C. (1997). Correlated action effects in decision theoretic regression. In Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence, pp. 30{37
Providence, RI.
Boutilier, C., Brafman, R. I., & Geib, C. (1997). Prioritized goal decomposition of Markov
decision processes: Toward a synthesis of classical and decision theoretic planning. In
Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence,
pp. 1156{1162 Nagoya, Japan.
Boutilier, C., Brafman, R. I., & Geib, C. (1998). Structured reachability analysis for Markov
decision processes. In Proceedings of the Fourteenth Conference on Uncertainty in
Artificial Intelligence, pp. 24{32 Madison, WI.
Boutilier, C., & Dearden, R. (1994). Using abstractions for decision-theoretic planning
with time constraints. In Proceedings of the Twelfth National Conference on Artificial
Intelligence, pp. 1016{1022 Seattle, WA.
84

fiDecision-Theoretic Planning: Structural Assumptions

Boutilier, C., & Dearden, R. (1996). Approximating value trees in structured dynamic
programming. In Proceedings of the Thirteenth International Conference on Machine
Learning, pp. 54{62 Bari, Italy.
Boutilier, C., Dearden, R., & Goldszmidt, M. (1995). Exploiting structure in policy construction. In Proceedings of the Fourteenth International Joint Conference on Artificial
Intelligence, pp. 1104{1111 Montreal, Canada.
Boutilier, C., Dearden, R., & Goldszmidt, M. (1999). Stochastic dynamic programming
with factored representations. (manuscript).
Boutilier, C., Friedman, N., Goldszmidt, M., & Koller, D. (1996). Context-specific independence in Bayesian networks. In Proceedings of the Twelfth Conference on Uncertainty
in Artificial Intelligence, pp. 115{123 Portland, OR.
Boutilier, C., & Goldszmidt, M. (1996). The frame problem and Bayesian network action
representations. In Proceedings of the Eleventh Biennial Canadian Conference on
Artificial Intelligence, pp. 69{83 Toronto.
Boutilier, C., & Poole, D. (1996). Computing optimal policies for partially observable
decision processes using compact representations. In Proceedings of the Thirteenth
National Conference on Artificial Intelligence, pp. 1168{1175 Portland, OR.
Boutilier, C., & Puterman, M. L. (1995). Process-oriented planning and average-reward optimality. In Proceedings of the Fourteenth International Joint Conference on Artificial
Intelligence, pp. 1096{1103 Montreal, Canada.
Brafman, R. I. (1997). A heuristic variable-grid solution method for POMDPs. In Proceedings of the Fourteenth National Conference on Artificial Intelligence, pp. 727{733
Providence, RI.
Bryant, R. E. (1986). Graph-based algorithms for boolean function manipulation. IEEE
Transactions on Computers, C-35 (8), 677{691.
Bylander, T. (1994). The computational complexity of propositional STRIPS planning.
Artificial Intelligence, 69, 161{204.
Caines, P. E. (1988). Linear stochastic systems. Wiley, New York.
Cassandra, A. R., Kaelbling, L. P., & Littman, M. L. (1994). Acting optimally in partially
observable stochastic domains. In Proceedings of the Twelfth National Conference on
Artificial Intelligence, pp. 1023{1028 Seattle, WA.
Cassandra, A. R., Littman, M. L., & Zhang, N. L. (1997). Incremental pruning: A simple, fast, exact method for pomdps. In Proceedings of the Thirteenth Conference on
Uncertainty in Artificial Intelligence, pp. 54{61 Providence, RI.
Chapman, D. (1987). Planning for conjunctive goals. Artificial Intelligence, 32 (3), 333{377.
85

fiBoutilier, Dean, & Hanks

Chapman, D., & Kaelbling, L. P. (1991). Input generalization in delayed reinforcement
learning: An algorithm and performance comparisons. In Proceedings of the Twelfth
International Joint Conference on Artificial Intelligence, pp. 726{731 Sydney, Australia.
Dantzig, G., & Wolfe, P. (1960). Decomposition principle for dynamic programs. Operations
Research, 8 (1), 101{111.
Dean, T., Allen, J., & Aloimonos, Y. (1995). Artificial Intelligence: Theory and Practice.
Benjamin Cummings.
Dean, T., & Givan, R. (1997). Model minimization in Markov decision processes. In
Proceedings of the Fourteenth National Conference on Artificial Intelligence, pp. 106{
111 Providence, RI. AAAI.
Dean, T., Givan, R., & Kim, K.-E. (1998). Solving planning problems with large state and
action spaces. In Proceedings of the Fourth International Conference on AI Planning
Systems, pp. 102{110 Pittsburgh, PA.
Dean, T., Givan, R., & Leach, S. (1997). Model reduction techniques for computing approximately optimal solutions for Markov decision processes. In Proceedings of the
Thirteenth Conference on Uncertainty in Artificial Intelligence, pp. 124{131 Providence, RI.
Dean, T., Kaelbling, L., Kirman, J., & Nicholson, A. (1993). Planning with deadlines in
stochastic domains. In Proceedings of the Eleventh National Conference on Artificial
Intelligence, pp. 574{579.
Dean, T., Kaelbling, L., Kirman, J., & Nicholson, A. (1995). Planning under time constraints in stochastic domains. Artificial Intelligence, 76 (1-2), 3{74.
Dean, T., & Kanazawa, K. (1989). A model for reasoning about persistence and causation.
Computational Intelligence, 5 (3), 142{150.
Dean, T., & Lin, S.-H. (1995). Decomposition techniques for planning in stochastic domains. In Proceedings of the Fourteenth International Joint Conference on Artificial
Intelligence, pp. 1121{1127.
Dean, T., & Wellman, M. (1991). Planning and Control. Morgan Kaufmann, San Mateo,
California.
Dearden, R., & Boutilier, C. (1994). Integrating planning and execution in stochastic
domains. In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, pp. 162{169 Washington, DC.
Dearden, R., & Boutilier, C. (1997). Abstraction and approximate decision theoretic planning. Artificial Intelligence, 89, 219{283.
Dechter, R. (1996). Bucket elimination: A unifying framework for probabilistic inference.
In Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence, pp.
211{219 Portland, OR.
86

fiDecision-Theoretic Planning: Structural Assumptions

Dechter, R. (1997). Mini-buckets: A general scheme for generating approximations in
automated reasoning in probabilistic inference. In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, pp. 1297{1302 Nagoya, Japan.
D'Epenoux, F. (1963). Sur un probleme de production et de stockage dans l'aleatoire.
Management Science, 10, 98{108.
Dietterich, T. G., & Flann, N. S. (1995). Explanation-based learning and reinforcement
learning: A unified approach. In Proceedings of the Twelfth International Conference
on Machine Learning, pp. 176{184 Lake Tahoe, NV.
Draper, D., Hanks, S., & Weld, D. (1994a). A probabilistic model of action for leastcommitment planning with information gathering. In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, pp. 178{186 Washington, DC.
Draper, D., Hanks, S., & Weld, D. (1994b). Probabilistic planning with information gathering and contingent execution. In Proceedings of the Second International Conference
on AI Planning Systems, pp. 31{36.
Etzioni, O., Hanks, S., Weld, D., Draper, D., Lesh, N., & Williamson, M. (1992). An
approach to planning with incomplete information. In Proceedings of the Third International Conference on Principles of Knowledge Representation and Reasoning, pp.
115{125 Boston, MA.
Fikes, R., Hart, P., & Nilsson, N. (1972). Learning and executing generalized robot plans.
Artificial Intelligence, 3, 251{288.
Fikes, R., & Nilsson, N. J. (1971). STRIPS: A new approach to the application of theorem
proving to problem solving. Artificial Intelligence, 2, 189{208.
Finger, J. (1986). Exploiting Constraints in Design Synthesis. Ph.D. thesis, Stanford University, Stanford.
Floyd, R. W. (1962). Algorithm 97 (shortest path). Communications of the ACM, 5 (6),
345.
Fox, B. L., & Landi, D. M. (1968). An algorithm for identifying the ergodic subchains and
transient states of a stochastic matrix. Communications of the ACM, 2, 619{621.
French, S. (1986). Decision Theory. Halsted Press, New York.
Geiger, D., & Heckerman, D. (1991). Advances in probabilistic reasoning. In Proceedings
of the Seventh Conference on Uncertainty in Artificial Intelligence, pp. 118{126 Los
Angeles, CA.
Givan, R., & Dean, T. (1997). Model minimization, regression, and propositional STRIPS
planning. In Proceedings of the Fifteenth International Joint Conference on Artificial
Intelligence, pp. 1163{1168 Nagoya, Japan.
87

fiBoutilier, Dean, & Hanks

Givan, R., Leach, S., & Dean, T. (1997). Bounded-parameter Markov decision processes. In
Proceedings of the Fourth European Conference on Planning (ECP'97), pp. 234|246
Toulouse, France.
Goldman, R. P., & Boddy, M. S. (1994). Representing uncertainty in simple planners.
In Proceedings of the Fourth International Conference on Principles of Knowledge
Representation and Reasoning, pp. 238{245 Bonn, Germany.
Haddawy, P., & Doan, A. (1994). Abstracting probabilistic actions. In Proceedings of the
Tenth Conference on Uncertainty in Artificial Intelligence, pp. 270{277 Washington,
DC.
Haddawy, P., & Hanks, S. (1998). Utility Models for Goal-Directed Decision-Theoretic
Planners. Computational Intelligence, 14 (3).
Haddawy, P., & Suwandi, M. (1994). Decision-theoretic refinement planning using inheritence abstraction. In Proceedings of the Second International Conference on AI Planning Systems, pp. 266{271 Chicago, IL.
Hanks, S. (1990). Projecting plans for uncertain worlds. Ph.D. thesis 756, Yale University,
Department of Computer Science, New Haven, CT.
Hanks, S., & McDermott, D. V. (1994). Modeling a dynamic and uncertain world I: Symbolic
and probabilistic reasoning about change. Artificial Intelligence, 66 (1), 1{55.
Hanks, S., Russell, S., & Wellman, M. (Eds.). (1994). Decision Theoretic Planning: Proceedings of the AAAI Spring Symposium. AAAI Press, Menlo Park.
Hansen, E. A., & Zilberstein, S. (1998). Heuristic search in cyclic AND/OR graphs. In
Proceedings of the Fifteenth National Conference on Artificial Intelligence, pp. 412{
418 Madison, WI.
Hauskrecht, M. (1997). A heuristic variable-grid solution method for POMDPs. In Proceedings of the Fourteenth National Conference on Artificial Intelligence, pp. 734{739
Providence, RI.
Hauskrecht, M. (1998). Planning and Control in Stochastic Domains with Imperfect Information. Ph.D. thesis, Massachusetts Institute of Technology, Cambridge.
Hauskrecht, M., Meuleau, N., Kaelbling, L. P., Dean, T., & Boutilier, C. (1998). Hierarchical
solution of Markov decision processes using macro-actions. In Proceedings of the
Fourteenth Conference on Uncertainty in Artificial Intelligence, pp. 220{229 Madison,
WI.
Hoey, J., St-Aubin, R., Hu, A., & Boutilier, C. (1999). SPUDD: Stochastic planning
using decision diagrams. In Proceedings of the Fifteenth Conference on Uncertainty
in Artificial Intelligence Stockholm. To appear.
Howard, R. A. (1960). Dynamic Programming and Markov Processes. MIT Press, Cambridge, Massachusetts.
88

fiDecision-Theoretic Planning: Structural Assumptions

Howard, R. A., & Matheson, J. E. (1984). Inuence diagrams. In Howard, R. A., & Matheson, J. E. (Eds.), The Principles and Applications of Decision Analysis. Strategic
Decisions Group, Menlo Park, CA.
Kambhampati, S. (1997). Refinement planning as a unifying framework for plan synthesis.
AI Magazine, Summer 1997, 67{97.
Kearns, M., Mansour, Y., & Ng, A. Y. (1999). A sparse sampling algorithm for nearoptimal planning in large markov decision processes. In Proceedings of the Sixteenth
International Joint Conference on Artificial Intelligence Stockholm. To appear.
Keeney, R. L., & Raiffa, H. (1976). Decisions with Multiple Objectives: Preferences and
Value Tradeoffs. John Wiley and Sons, New York.
Kjaerulff, U. (1992). A computational scheme for reasoning in dynamic probabilistic networks. In Proceedings of the Eighth Conference on Uncertainty in AI, pp. 121{129
Stanford.
Knoblock, C. A. (1993). Generating Abstraction Hierarchies: An Automated Approach to
Reducing Search in Planning. Kluwer, Boston.
Knoblock, C. A., Tenenberg, J. D., & Yang, Q. (1991). Characterizing abstraction hierarchies for planning. In Proceedings of the Ninth National Conference on Artificial
Intelligence, pp. 692{697 Anaheim, CA.
Koenig, S. (1991). Optimal probabilistic and decision-theoretic planning using Markovian
decision theory. M.sc. thesis UCB/CSD-92-685, University of California at Berkeley,
Computer Science Department.
Koenig, S., & Simmons, R. (1995). Real-time search in nondeterministic domains. In
Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence,
pp. 1660{1667 Montreal, Canada.
Korf, R. (1985). Macro-operators: A weak method for learning. Artificial Intelligence, 26,
35{77.
Korf, R. E. (1990). Real-time heuristic search. Artificial Intelligence, 42, 189{211.
Kushmerick, N., Hanks, S., & Weld, D. (1995). An Algorithm for Probabilistic Planning.
Artificial Intelligence, 76, 239{286.
Kushner, H. J., & Chen, C.-H. (1974). Decomposition of systems governed by Markov
chains. IEEE Transactions on Automatic Control, 19 (5), 501{507.
Lee, D., & Yannakakis, M. (1992). Online minimization of transition systems. In Proceedings
of 24th Annual ACM Symposium on the Theory of Computing, pp. 264{274 Victoria,
BC.
Lin, F., & Reiter, R. (1994). State constraints revisited. Journal of Logic and Computation,
4 (5), 655{678.
89

fiBoutilier, Dean, & Hanks

Lin, S.-H. (1997). Exploiting Structure for Planning and Control. Ph.D. thesis, Department
of Computer Science, Brown University.
Lin, S.-H., & Dean, T. (1995). Generating optimal policies for high-level plans with conditional branches and loops. In Proceedings of the Third European Workshop on
Planning (EWSP'95), pp. 187{200.
Littman, M. L. (1997). Probabilistic propositional planning: Representations and complexity. In Proceedings of the Fourteenth National Conference on Artificial Intelligence,
pp. 748{754 Providence, RI.
Littman, M. L., Dean, T. L., & Kaelbling, L. P. (1995). On the complexity of solving
Markov decision problems. In Proceedings of the Eleventh Conference on Uncertainty
in Artificial Intelligence, pp. 394{402 Montreal, Canada.
Littman, M. L. (1996). Algorithms for sequential decision making. Ph.D. thesis CS{96{09,
Brown University, Department of Computer Science, Providence, RI.
Lovejoy, W. S. (1991a). Computationally feasible bounds for partially observed Markov
decision processes. Operations Research, 39 (1), 162{175.
Lovejoy, W. S. (1991b). A survey of algorithmic methods for partially observed Markov
decision processes. Annals of Operations Research, 28, 47{66.
Luenberger, D. G. (1973). Introduction to Linear and Nonlinear Programming. AddisonWesley, Reading, Massachusetts.
Luenberger, D. G. (1979). Introduction to Dynamic Systems: Theory, Models and Applications. Wiley, New York.
Madani, O., Condon, A., & Hanks, S. (1999). On the undecidability of probabilistic planning
and infinite-horizon partially observable Markov decision problems. In Proceedings of
the Sixteenth National Conference on Artificial Intelligence Orlando, FL. To appear.
Mahadevan, S. (1994). To discount or not to discount in reinforcement learning: A case
study in comparing R-learning and Q-learning. In Proceedings of the Eleventh International Conference on Machine Learning, pp. 164{172 New Brunswick, NJ.
McAllester, D., & Rosenblitt, D. (1991). Systematic nonlinear planning. In Proceedings of
the Ninth National Conference on Artificial Intelligence, pp. 634{639 Anaheim, CA.
McCallum, R. A. (1995). Instance-based utile distinctions for reinforcement learning with
hidden state. In Proceedings of the Twelfth International Conference on Machine
Learning, pp. 387{395 Lake Tahoe, Nevada.
McCarthy, J., & Hayes, P. J. (1969). Some philosophical problems from the standpoint of
artificial intelligence. Machine Intelligence, 4, 463{502.
90

fiDecision-Theoretic Planning: Structural Assumptions

Meuleau, N., Hauskrecht, M., Kim, K., Peshkin, L., Kaelbling, L., Dean, T., & Boutilier, C.
(1998). Solving very large weakly coupled Markov decision processes. In Proceedings
of the Fifteenth National Conference on Artificial Intelligence, pp. 165{172 Madison,
WI.
Moore, A. W., & Atkeson, C. G. (1995). The parti-game algorithm for variable resolution
reinforcement learning in multidimensional state spaces. Machine Learning, 21, 199{
234.
Papadimitriou, C. H., & Tsitsiklis, J. N. (1987). The complexity of Markov chain decision
processes. Mathematics of Operations Research, 12 (3), 441{450.
Parr, R. (1998). Flexible decomposition algorithms for weakly coupled Markov decision
processes. In Proceedings of the Fourteenth Conference on Uncertainty in Artificial
Intelligence, pp. 422{430 Madison, WI.
Parr, R., & Russell, S. (1995). Approximating optimal policies for partially observable
stochastic domains. In Proceedings of the Fourteenth International Joint Conference
on Artificial Intelligence, pp. 1088{1094 Montreal.
Parr, R., & Russell, S. (1998). Reinforcement learning with hierarchies of machines. In
Jordan, M., Kearns, M., & Solla, S. (Eds.), Advances in Neural Information Processing
Systems 10, pp. 1043{1049. MIT Press, Cambridge.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible
Inference. Morgan Kaufmann, San Mateo.
Pednault, E. (1989). ADL: Exploring the middle ground between STRIPS and the situation calculus. In Proceedings of the First International Conference on Principles of
Knowledge Representation and Reasoning, pp. 324{332 Toronto, Canada.
Penberthy, J. S., & Weld, D. S. (1992). UCPOP: A sound, complete, partial order planner for
ADL. In Proceedings of the Third International Conference on Principles of Knowledge
Representation and Reasoning, pp. 103{114 Boston, MA.
Peot, M., & Smith, D. (1992). Conditional Nonlinear Planning. In Proceedings of the First
International Conference on AI Planning Systems, pp. 189{197 College Park, MD.
Perez, M. A., & Carbonell, J. G. (1994). Control knowledge to improve plan quality. In
Proceedings of the Second International Conference on AI Planning Systems, pp. 323{
328 Chicago, IL.
Poole, D. (1995). Exploiting the rule structure for decision making within the independent
choice logic. In Proceedings of the Eleventh Conference on Uncertainty in Artificial
Intelligence, pp. 454{463 Montreal, Canada.
Poole, D. (1997a). The independent choice logic for modelling multiple agents under uncertainty. Artificial Intelligence, 94 (1{2), 7{56.
91

fiBoutilier, Dean, & Hanks

Poole, D. (1997b). Probabilistic partial evaluation: Exploiting rule structure in probabilistic
inference. In Proceedings of the Fifteenth International Joint Conference on Artificial
Intelligence, pp. 1284{1291 Nagoya, Japan.
Poole, D. (1998). Context-specific approximation in probabilistic inference. In Proceedings
of the Fourteenth Conference on Uncertainty in Artificial Intelligence, pp. 447{454
Madison, WI.
Precup, D., Sutton, R. S., & Singh, S. (1998). Theoretical results on reinforcement learning
with temporally abstract behaviors. In Proceedings of the Tenth European Conference
on Machine Learning, pp. 382{393 Chemnitz, Germany.
Pryor, L., & Collins, G. (1993). CASSANDRA: Planning for contingencies. Technical
report 41, Northwestern University, The Institute for the Learning Sciences.
Puterman, M. L. (1994). Markov Decision Processes. John Wiley & Sons, New York.
Puterman, M. L., & Shin, M. (1978). Modified policy iteration algorithms for discounted
Markov decision problems. Management Science, 24, 1127{1137.
Ross, K. W., & Varadarajan, R. (1991). Multichain Markov decision processes with a
sample-path constraint: A decomposition approach. Mathematics of Operations Research, 16 (1), 195{207.
Russell, S., & Norvig, P. (1995). Artificial Intelligence: A Modern Approach. Prentice Hall,
Englewood Cliffs, NJ.
Sacerdoti, E. D. (1974). Planning in a hierarchy of abstraction spaces. Artificial Intelligence,
5, 115{135.
Sacerdoti, E. D. (1975). The nonlinear nature of plans. In Proceedings of the Fourth
International Joint Conference on Artificial Intelligence, pp. 206{214.
Schoppers, M. J. (1987). Universal plans for reactive robots in unpredictable environments.
In Proceedings of the Tenth International Joint Conference on Artificial Intelligence,
pp. 1039{1046 Milan, Italy.
Schwartz, A. (1993). A reinforcement learning method for maximizing undiscounted rewards. In Proceedings of the Tenth International Conference on Machine Learning,
pp. 298{305 Amherst, MA.
Schweitzer, P. L., Puterman, M. L., & Kindle, K. W. (1985). Iterative aggregationdisaggregation procedures for discounted semi-Markov reward processes. Operations
Research, 33, 589{605.
Shachter, R. D. (1986). Evaluating inuence diagrams. Operations Research, 33 (6), 871{
882.
Shimony, S. E. (1993). The role of relevance in explanation I: Irrelevance as statistical
independence. International Journal of Approximate Reasoning, 8 (4), 281{324.
92

fiDecision-Theoretic Planning: Structural Assumptions

Simmons, R., & Koenig, S. (1995). Probabilistic robot navigation in partially observable
environments. In Proceedings of the Fourteenth International Joint Conference on
Artificial Intelligence, pp. 1080{1087 Montreal, Canada.
Singh, S. P., & Cohn, D. (1998). How to dynamically merge Markov decision processes. In
Advances in Neural Information Processing Systems 10, pp. 1057{1063. MIT Press,
Cambridge.
Singh, S. P., Jaakkola, T., & Jordan, M. I. (1994). Reinforcement learning with soft state
aggregation. In Hanson, S. J., Cowan, J. D., & Giles, C. L. (Eds.), Advances in Neural
Information Processing Systems 7. Morgan-Kaufmann, San Mateo.
Smallwood, R. D., & Sondik, E. J. (1973). The optimal control of partially observable
Markov processes over a finite horizon. Operations Research, 21, 1071{1088.
Smith, D., & Peot, M. (1993). Postponing threats in partial-order planning. In Proceedings
of the Eleventh National Conference on Artificial Intelligence, pp. 500{506 Washington, DC.
Sondik, E. J. (1978). The optimal control of partially observable Markov processes over the
infinite horizon: Discounted costs. Operations Research, 26, 282{304.
Stone, P., & Veloso, M. (1999). Team-partitioned, opaque-transition reinforcement learning.
In Asada, M. (Ed.), RoboCup-98: Robot Soccer World Cup II. Springer Verlag, Berlin.
Sutton, R. S. (1995). TD models: Modeling the world at a mixture of time scales. In
Proceedings of the Twelfth International Conference on Machine Learning, pp. 531{
539 Lake Tahoe, NV.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press,
Cambridge, MA.
Tash, J., & Russell, S. (1994). Control strategies for a stochastic planner. In Proceedings
of the Twelfth National Conference on Artificial Intelligence, pp. 1079{1085 Seattle,
WA.
Tatman, J. A., & Shachter, R. D. (1990). Dynamic programming and inuence diagrams.
IEEE Transactions on Systems, Man, and Cybernetics, 20 (2), 365{379.
Tesauro, G. J. (1994). TD-Gammon, a self-teaching backgammon program, achieves masterlevel play. Neural Computation, 6, 215{219.
Thrun, S., Fox, D., & Burgard, W. (1998). A probabilistic approach to concurrent mapping
and localization for mobile robots. Machine Learning, 31, 29{53.
Thrun, S., & Schwartz, A. (1995). Finding structure in reinforcement learning. In Tesauro,
G., Touretzky, D., & Leen, T. (Eds.), Advances in Neural Information Processing
Systems 7 Cambridge, MA. MIT Press.
Warren, D. (1976). Generating conditional plans and programs. In Proceedings of AISB
Summer Conference, pp. 344{354 University of Edinburgh.
93

fiBoutilier, Dean, & Hanks

Watkins, C. J. C. H., & Dayan, P. (1992). Q-learning. Machine Learning, 8, 279{292.
Weld, D. S. (1994). An introduction to least commitment planning. AI Magazine, Winter
1994, 27{61.
White III, C. C., & Scherer, W. T. (1989). Solutions procedures for partially observed
Markov decision processes. Operations Research, 37 (5), 791{797.
Williamson, M. (1996). A value-directed approach to planning. Ph.D. thesis 96{06{03,
University of Washington, Department of Computer Science and Engineering.
Williamson, M., & Hanks, S. (1994). Optimal planning with a goal-directed utility model.
In Proceedings of the Second International Conference on AI Planning Systems, pp.
176{180 Chicago, IL.
Winston, P. H. (1992). Artificial Intelligence, Third Edition. Addison-Wesley, Reading,
Massachusetts.
Yang, Q. (1998). Intelligent Planning : A Decomposition and Abstraction Based Approach.
Springer Verlag.
Zhang, N. L., & Liu, W. (1997). A model approximation scheme for planning in partially
observable stochastic domains. Journal of Artificial Intelligence Research, 7, 199{230.
Zhang, N. L., & Poole, D. (1996). Exploiting causal independence in Bayesian network
inference. Journal of Artificial Intelligence Research, 5, 301{328.

94

fiJournal of Artificial Intelligence Research 11 (1999) 335{360

Submitted 8/98; published 11/99

Committee-Based Sample Selection
For Probabilistic Classifiers
Shlomo Argamon-Engelson

Department of Computer Science
Jerusalem College of Technology, Machon Lev
P.O.B. 16031
Jerusalem 91160, Israel

argamon@mail.jct.ac.il

Ido Dagan

Department of Mathematics and Computer Science
Bar-Ilan University
52900 Ramat Gan, Israel

dagan@cs.biu.ac.il

Abstract
In many real-world learning tasks it is expensive to acquire a sucient number of labeled
examples for training. This paper investigates methods for reducing annotation cost by
sample selection. In this approach, during training the learning program examines many
unlabeled examples and selects for labeling only those that are most informative at each
stage. This avoids redundantly labeling examples that contribute little new information.
Our work follows on previous research on Query By Committee, and extends the
committee-based paradigm to the context of probabilistic classification. We describe a
family of empirical methods for committee-based sample selection in probabilistic classification models, which evaluate the informativeness of an example by measuring the degree
of disagreement between several model variants. These variants (the committee) are drawn
randomly from a probability distribution conditioned by the training set labeled so far.
The method was applied to the real-world natural language processing task of stochastic part-of-speech tagging. We find that all variants of the method achieve a significant
reduction in annotation cost, although their computational eciency differs. In particular,
the simplest variant, a two member committee with no parameters to tune, gives excellent
results. We also show that sample selection yields a significant reduction in the size of the
model used by the tagger.

1. Introduction
Algorithms for supervised concept learning build classifiers for a concept based on a given
set of labeled examples. For many real-world concept learning tasks, however, acquiring
such labeled training examples is expensive. Hence, our objective is to develop automated
methods that reduce training cost within the framework of active learning, in which the
learner has some control over the choice of examples which will be labeled and used for
training.
c 1999 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiArgamon & Dagan

There are two main types of active learning. The first uses membership queries, in which
the learner constructs examples and asks a teacher to label them (Angluin, 1988; MacKay,
1992b; Plutowski & White, 1993). While this approach provides proven computational
advantages (Angluin, 1987), it is not always applicable since it is not always possible to
construct meaningful and informative unlabeled examples for training. This diculty may
be overcome when a large set of unlabeled training data is available. In this case the second
type of active learning, sample selection, can often be applied: The learner examines many
unlabeled examples, and selects only the most informative ones for learning (Seung, Opper,
& Sompolinsky, 1992; Freund, Seung, Shamir, & Tishby, 1997; Cohn, Atlas, & Ladner,
1994; Lewis & Catlett, 1994; Lewis & Gale, 1994).
In this paper, we address the problem of sample selection for training a probabilistic
classifier. Classification in this framework is performed by a probability-based model which,
given an input example, assigns a score to each possible classification and selects that with
the highest score.
Our research follows theoretical work on sample selection in the Query By Committee
(QBC) paradigm (Seung et al., 1992; Freund et al., 1997). We propose a novel empirical
scheme for applying the QBC paradigm to probabilistic classification models (allowing label
noise), which were not addressed in the original QBC framework (see Section 2.2). In this
committee-based selection scheme, the learner receives a stream of unlabeled examples as
input and decides for each of them whether to ask for its label or not. To that end, the
learner constructs a `committee' of (two or more) classifiers based on the statistics of the
current training set. Each committee member then classifies the candidate example, and the
learner measures the degree of disagreement among the committee members. The example
is selected for labeling depending on this degree of disagreement, according to some selection
protocol.
In previous work (Dagan & Engelson, 1995; Engelson & Dagan, 1996b) we presented
a particular selection protocol for probabilistic concepts. This paper extends our previous
work mainly by generalizing the selection scheme and by comparing a variety of different
selection protocols (a preliminary version appeared as Engelson & Dagan, 1996a).

1.1 Application To Natural Language Processing
Much of the early work in sample selection has either been theoretical in nature, or has
been tested on toy problems. We, however, are motivated by complex, real-world problems
in the area of statistical natural language and text processing. Our work here addresses
the task of part-of-speech tagging, a core task for statistical natural language processing
(NLP). Other work on sample selection for natural language tasks has mainly focused on
text categorization problems, such as the works of Lewis and Catlett (1994), Liere and
Tadepalli (1997), and McCallum and Nigam (1998).
In statistical NLP, probabilistic classifiers are often used to select a preferred analysis
of the linguistic structure of a text, such as its syntactic structure (Black, Jelinek, Lafferty,
Magerman, Mercer, & Roukos, 1993), word categories (Church, 1988), or word senses (Gale,
336

fiCommittee-Based Sample Selection for Probabilistic Classifiers

Church, & Yarowsky, 1993). The parameters of such a classification model are estimated
from a training corpus (a collection of text).
In the common case of supervised training, the learner uses a corpus in which each
sentence is manually annotated with the correct analysis. Manual annotation is typically
very expensive. As a consequence, few large annotated corpora exist, mainly for the English language, covering only a few genres of text. This situation makes it dicult to apply
supervised learning methods to languages other than English, or to adapt systems to different genres of text. Furthermore, it is infeasible in many cases to develop new supervised
methods that require annotations different from those which are currently available.
In some cases, manual annotation can be avoided altogether, using self-organized methods, such as was shown for part-of-speech tagging of English by Kupiec (1992). Even in
Kupiec's tagger, though, manual (and somewhat unprincipled) biasing of the initial model
was necessary to achieve satisfactory convergence. Elworthy (1994) and Merialdo (1991)
have investigated the effect of self-converging re-estimation for part-of-speech tagging and
found that some initial manual training is needed. More generally, the more supervised
training is provided, the better the results. In fact, fully unsupervised methods are not
applicable for many NLP tasks, and perhaps not even for part-of-speech tagging in some
languages. Sample selection is an appropriate way to reduce the cost of annotating corpora,
as it is easy to obtain large volumes of raw text from which smaller subsets will be selected
for annotation.
We have applied committee-based selection to learning Hidden Markov Models (HMMs)
for part-of-speech tagging of English sentences. Part-of-speech tagging is the task of labeling
each word in the sentence with its appropriate part of speech (for example, labeling an
occurrence of the word `hand' as a noun or a verb). This task is non-trivial since determining
a word's part of speech depends on its linguistic context. HMMs have been used extensively
for this task (e.g., Church, 1988; Merialdo, 1991), in most cases trained from corpora
which have been manually annotated with the correct part of speech for each word. Our
experiments on part-of-speech tagging, described in Section 6.5, show that using committeebased selection results in substantially faster learning rates, enabling the learner to achieve
a given level of accuracy using far fewer training examples than by sequential training using
all of the text.

2. Background
The objective of sample selection is to select those examples which will be most informative in the future. How might we determine the informativeness of an example? One
approach is to derive an explicit measure of the expected amount of information gained
by using the example (Cohn, Ghahramani, & Jordan, 1995; MacKay, 1992b, 1992a). For
example, MacKay (1992b) assesses the informativeness of an example, in a neural network
learning task, by the expected decrease in the overall variance of the model's prediction,
after training on the example. Explicit measures can be appealing, since they attempt to
give a precise characterization of the information content of an example. Also, for membership querying, an explicit formulation of information content sometimes enables finding
337

fiArgamon & Dagan

the most informative examples analytically, saving the cost of searching the example space.
The use of explicit methods may be limited, however, since explicit measures are generally
(a) model-specific, (b) complex, often requiring various approximations to be practical, and
(c) depend on the accuracy of the current hypothesis at any given step.
The alternative to measuring the informativeness of an example explicitly is to measure
it implicitly, by quantifying the amount of uncertainty in the classification of the example
given the current training data. The informativeness of an example is evaluated with respect
to models derived from the training data at each stage of learning. One approach is to use
a single model based on the training data seen so far. This approach is taken by Lewis
and Gale (1994), for training a binary classifier. They select for training those examples
whose classification probability is closest to 0.5, i.e, those examples for which the current
best model is most uncertain.
In order to better evaluate classification uncertainty with respect to the entire space of
possible models, one may instead measure the classification disagreement among a sample
set of possible models (a committee). Using the entire model space enables measuring the
degree to which the training entails a single (best) classification for the example. On the
other hand, referring to a single model measures only the degree to which that model is
certain of its classification. For example, a classifier with sucient training for predicting ips of a coin with heads probability 0.55 will always predict heads, and hence will
make mistakes 45% of the time. However, although this classifier is quite uncertain of the
correctness of its classification, additional training will not improve its accuracy.
There are two main approaches for generating a committee in order to evaluate example
uncertainty: the version space approach and the random sampling approach. The version
space approach, pursued by Cohn et al. (1994) seeks to choose committee members on
the border of the space of all the models allowed by the training data (the version space,
Mitchell, 1982). Thus models are chosen for the committee which are as far from each other
as possible while being consistent with the training data. This ensures that the models will
disagree on an example whenever training on the example would restrict the version space.
The version space approach can be dicult to apply since finding models on the edge
of the version space is non-trivial in general. Furthermore, the approach is not directly
applicable in the case of probabilistic classification models, where almost all models are
possible, though not equally probable, given the training. The alternative is random sampling, as exemplified by the Query By Committee algorithm (Seung et al., 1992; Freund
et al., 1997), which inspired this paper. In this approach, models are sampled randomly
from the set of all possible models, according to the probability of the models given the
training data. Our work applies the random sampling approach to probabilistic classifiers
by computing an approximation to the posterior model distribution given the training data,
and generating committee members from that distribution. McCallum and Nigam (1998)
use a similar approach for sample selection on text categorization using a naive Bayes classifier. The primary difference is that they skew example selection using density-weighted
sampling, such that documents that are similar to many other documents in the training
set will be selected for labeling with a higher probability.

338

fiCommittee-Based Sample Selection for Probabilistic Classifiers

Matan (1995) presents two other methods for random sampling. In the first method, he
trains committee members on different subsets of the training data. In his second method,
for neural network models, Matan generates committee members by backpropagation training using different initial weights in the networks so that they reach different local minima.
A similar approach is taken by Liere and Tadepalli (1997), who applied a committee-based
selection approach to text categorization using the Winnow learning algorithm (Littlestone,
1988) which learns linear classifiers. They represented the model space by a set of classifiers (the model set). Each classifier in the model set learns independently from labeled
examples, having been initialized with a different initial hypothesis (thus at any point the
set gives a selection of the possible hypotheses given the training data). Labeling decisions
are performed based on two models chosen at random from the model set. If the models
disagree on a document's class, the document's label is requested, and all models in the
space are updated.

2.1 Query By Committee
As mentioned above, this paper follows theoretical work on sample selection in the Query
By Committee (QBC) paradigm (Seung et al., 1992; Freund et al., 1997). This method was
proposed for learning binary (non-probabilistic) concepts in cases where there exists a prior
probability distribution measure over the concept class. QBC selects `informative' training
examples out of a stream of unlabeled examples. When an example is selected the learner
queries the teacher for its correct label and adds it to the training set. As examples are
selected for training, they restrict the set of consistent concepts, i.e, the set of concepts that
label all the training examples correctly (the version space).
A simple version of QBC, which was analyzed by Freund et al. (1997) (see also the
summary in Freund, 1994), uses the following selection algorithm:
1. Draw an unlabeled input example at random from the probability distribution of the example space.
2. Select at random two hypotheses according to the prior probability distribution of the concept class, restricted to the set of consistent concepts.
3. Select the example for training if the two hypotheses disagree on its classification.
Freund et al. prove that, under some assumptions, this algorithm achieves an exponential
reduction in the number of labeled examples required to achieve a desired classification
accuracy, compared with random selection of training examples. This speedup is achieved
because the algorithm tends to select examples that split the version space into two parts
of similar size. One of these parts is eliminated from the version space after the example
and its correct label are added to the training set.

2.2 Selection For Probabilistic Classifiers
We address here the problem of sample selection for training a probabilistic classifier. Classification in this framework is performed by a probabilistic model which, given an input
339

fiArgamon & Dagan

example, assigns a probability (or a probability-based score) to each possible classification
and selects the best classification. Probabilistic classifiers do not fall within the framework
addressed in the theoretical QBC work. Training a probabilistic classifier involves estimating the values of model parameters which determine a probability estimate for each possible
classification of an example. While we expect that in most cases the optimal classifier will
assign the highest probability to the correct class, this is not guaranteed to always occur.
Accordingly, the notion of a consistent hypothesis is generally not applicable to probabilistic
classifiers. Thus, the posterior distribution over classifiers given the training data cannot
be defined as the restriction of the prior to the set of consistent hypotheses. Rather, within
a Bayesian framework, the posterior distribution is defined by the statistics of the training
set, assigning higher probability to those classifiers which are more likely given the statistics.
We now discuss some desired properties of examples that are selected for training. Generally speaking, a training example contributes data to several statistics, which in turn
determine the estimates of several parameter values. An informative example is therefore
one whose contribution to the statistics leads to a useful improvement of parameter estimates. Assuming the existence of an optimal classification model for the given concept
(such as a maximum likelihood model), we identify three properties of parameters for which
acquiring additional statistics is most beneficial:
1. The current estimate of the parameter is uncertain due to insucient statistics in
the training set. An uncertain estimate is likely to be far from the true value of the
parameter and can cause incorrect classification. Additional statistics would bring the
estimate closer to the true value.
2. Classification is sensitive to changes in the current estimate of the parameter. Otherwise, acquiring additional statistics is unlikely to affect classification and is therefore
not beneficial.
3. The parameter takes part in calculating class probabilities for a large proportion of
examples. Parameters that are only relevant for classifying few examples, as determined by the probability distribution of the input examples, have low utility for future
estimation.
The committee-based selection scheme, as we describe further below, tends to select
examples that affect parameters with the above three properties. Property 1 is addressed by
randomly picking parameter values for committee members from the posterior distribution
of parameter estimates (given the current statistics). When the statistics for a parameter
are insucient the variance of the posterior distribution of the estimates is large, and hence
there will be large differences in the values of the parameter picked for different committee
members. Note that property 1 is not addressed when uncertainty in classification is only
judged relative to a single model (as in, e.g., Lewis & Gale, 1994). Such an approach
captures uncertainty with respect to given parameter values, in the sense of property 2, but
it does not model uncertainty about the choice of these values in the first place (the use of
a single model is criticized by Cohn et al., 1994).
Property 2 is addressed by selecting examples for which committee members highly disagree in classification. Thus, the algorithm tends to acquire statistics where uncertainty in
340

fiCommittee-Based Sample Selection for Probabilistic Classifiers

parameter estimates entails uncertainty in actual classification (this is analogous to splitting
the version space in QBC). Finally, property 3 is addressed by independently examining
input examples which are drawn from the input distribution. In this way, we implicitly
model the expected utility of the statistics in classifying future examples.

2.3 Paper Outline
The following section defines the basic concepts and notation that we will use in the rest
of the paper. Section 4 presents a general selection scheme along with variant selection
algorithms. The next two sections demonstrate the effectiveness of the sample selection
scheme. Section 5 presents results on an artificial \colorful coin ipper" problem, providing
a simple illustration of the operation of the proposed system. Section 6 presents results for
the task of stochastic part-of-speech tagging, demonstrating the usefulness of committeebased sample selection in the real world.

3. Definitions
The concern of this paper is how to minimize the number of labeled examples needed to
learn a classifier which accurately classifies input examples e by classes c 2 C , where C
is a known set of possible classes. During learning, a stream of unlabeled examples is
supplied for free, with examples drawn from an unknown probability distribution. There is
a cost, however, for the learning algorithm to obtain the true label of any given example.
Our objective is to reduce this cost as much as possible, while still learning an accurate
classifier.
We address the specific case of probabilistic classifiers, where classification is done on
the basis of a score function, FM (c; e), which assigns a score to each possible class of an
input example. The classifier assigns the input example to the class with the highest score.
FM is determined by a probabilistic model M . In many applications, FM is the conditional
probability function, PM (cje), specifying the probability of each class given the example.
Alternatively, other score functions that denote the likelihood of the class may be used
(such as an odds ratio). The particular type of model used for classification determines the
specific form of the score, as a function of features of the example.
A probabilistic model M , and thus the score function FM , is defined by a set of parameters, fffi g, giving the probabilities of various possible events. For example, a model
for part-of-speech tagging contains parameters such as the probability of a particular word
being a verb or a noun. During training, the values of the parameters are estimated from
a set of statistics, S , extracted from a training set of labeled examples. A particular model
is denoted by M = faig, where each ai is a specific value for the corresponding ffi .

4. Committee-Based Sample Selection
This section describes the algorithms which apply the committee-based approach for evaluating classification uncertainty of each input example. The learning algorithm evaluates
341

fiArgamon & Dagan

an example by giving it to a committee containing several versions, or copies, of the classifier, all `consistent' with the training data seen so far. The greater the agreement of the
committee members on the classification of the example, the greater our certainty in its
classification. This is because if the training data entails a specific classification with high
certainty, then most (in a probabilistic sense) versions of the classifier consistent with the
data will produce that classification. An example is selected for labeling, therefore, when
the committee members disagree on its appropriate classification.

4.1 Generating A Committee
To generate a committee with k members, we randomly choose k models according to
the posterior distribution P (M jS ) of possible models given the current training statistics.
How this sampling is performed depends on the form of this distribution, which in turn
depends on the form of the model. Thus when implementing committee-based selection for
a particular problem, an appropriate sampling procedure must be devised. As an illustration
of committee generation, the rest of this section describes the sampling process for models
consisting of independent binomial parameters or multinomial parameter groups.
Consider first a model containing a single binomial parameter ff (the probability of a
success), with estimated value a. The statistics S for such a model are given by N , the
number of trials, and x, the number of successes in those trials.
Given N and x, the `best' model parameter value can be estimated by any of several
estimation methods. For example, the maximum likelihood estimate (MLE) for ff is a = Nx ,
giving the model M = fff = Nx g. When generating a committee of models, however, we are
not interested in the `best' model, but rather in sampling the distribution of models given
the statistics. For our example, we need to sample the posterior density of estimates for
ff, namely p(ff = ajS ). In the binomial case, this density is the beta distribution (Johnson,
1970). Sampling this distribution yields a set of estimates scattered around Nx (assuming a
uniform prior), where the variance of these estimates gets smaller as N gets larger. Each
such estimate participates in a different member of the committee. Thus, the more statistics
there are for estimating the parameter, the closer are the estimates used by different models
in the committee.
Now consider a model consisting of a single group of interdependent parameters defining a multinomial. In this case, the posterior is a Dirichlet distribution (Johnson, 1972).
Committee members are generated by sampling from this joint distribution, giving values
for all the model parameters.
For models consisting of a set of independent binomials or multinomials, sampling
P (M jS ) amounts to sampling each of the parameters independently. For models with
more complex dependencies among parameters sampling may be more dicult. In practice,
though, it may be possible to make enough independence assumptions to make sampling
feasible.
Sampling the posterior generates committee members whose parameter estimates differ
most when they are based on low training counts and tend to agree when based on high
counts. If the classification of an example relies on parameters whose estimates by com342

fiCommittee-Based Sample Selection for Probabilistic Classifiers

For each unlabeled input example e:
1. Draw 2 models randomly from P (M jS ), where S are statistics acquired
from previously labeled examples;
2. Classify e by each model, giving classifications c1 and c2;
3. If c1 6= c2, select e for annotation;
4. If e is selected, get its correct label and update S accordingly.
Figure 1: The two member sequential selection algorithm.
mittee members differ, and these differences affect classification, then the example would
be selected for learning. This leads to selecting examples which contribute statistics to
currently unreliable estimates that also have an effect on classification. Thus we address
Properties 1 and 2 discussed in Section 2.2.

4.2 Selection Algorithms
Within the committee-based paradigm there exist different methods for selecting informative examples. Previous research in sample selection has used either sequential selection
(Seung et al., 1992; Freund et al., 1997; Dagan & Engelson, 1995), or batch selection (Lewis
& Catlett, 1994; Lewis & Gale, 1994). We present here general algorithms for both sequential and batch committee-based selection. In all cases, we assume that before any
selection algorithm is applied a small amount of labeled initial training is supplied, in order
to initialize the training statistics.
4.2.1 Two Member Sequential Selection

Sequential selection examines unlabeled examples as they are supplied, one by one, and
estimates their expected information gain. Those examples determined to be suciently
informative are selected for training. Most simply, we can choose a committee of size two
from the posterior distribution of the models, and select an example when the two models
disagree on its classification. This gives the parameter-free, two member sequential selection
algorithm, shown in Figure 1. This basic algorithm has no parameters.
4.2.2 General Sequential Selection

A more general selection algorithm results from:

 Using a larger number k of committee members, in order to evaluate example informativeness more precisely,

 More refined example selection criteria, and
343

fiArgamon & Dagan

For each unlabeled input example e:
1. Draw k models fMi g randomly from P (M jS ) (possibly using a temperature t);
2. Classify e by each model Mi giving classifications fci g;
3. Measure the disagreement D(e) based on fci g;
4. Decide whether or not to select e for annotation, based on the value
of D(e);
5. If e is selected, get its correct label and update S accordingly.
Figure 2: The general sequential selection algorithm.

 Tuning the frequency of selection by replacing P (M jS ) with a distribution with a
different variance. This has the effect of adjusting the variability among the committee
members chosen. In many cases (eg., HMMs, as described in Section 6 below) this
can be implemented by a parameter t (called the temperature), used as a multiplier
of the variance of the posterior parameter distribution.

This gives the general sequential selection algorithm, shown in Figure 2.
It is easy to see that two member sequential selection is a special case of general sequential selection. In order to instantiate the general algorithm for larger committees, we need
to fix a general measure D(e) for disagreement (step 3), and a decision method for selecting
examples according to this disagreement (step 4).
We measure disagreement by the entropy of the distribution of classifications `voted for'
by the committee members. This vote entropy is a natural measure for quantifying the
uniformity of classes assigned to an example by the different committee members1 . We
further normalize this entropy by a bound on its maximum possible value (log min(k; jcj)),
giving a value between 0 and 1. Denoting the number of committee members assigning a
class c for input example e by V (c; e), the normalized vote entropy is:
X V (c; e) V (c; e)
1
D(e) = ,
log min(k; jC j) c k log k
Normalized vote entropy has the value one when all committee members disagree, and the
value zero when they all agree, taking on intermediate values in cases with partial agreement.
We consider here two alternatives for the selection criterion (step 4). The simplest is
thresholded selection, in which an example is selected for annotation if its normalized vote
entropy exceeds some threshold . Another alternative is randomized selection, in which
an example is selected for annotation based on the ip of a coin biased according to the
vote entropy|a higher vote entropy corresponding to a higher probability of selection. We
1. McCallum and Nigam (1998) have suggested an alternative measure, the KL-divergence to the mean
(Pereira, Tishby, & Lee, 1993). It is not clear whether that measure has an advantage over the simpler
entropy function.

344

fiCommittee-Based Sample Selection for Probabilistic Classifiers

For a batch B of N examples:
1. For each example e in B :
(a) Draw k models randomly from P (M jS );
(b) Classify e by each model, giving classifications fcig;
(c) Measure the disagreement D(e) for e based on fcig;
2. Select for annotation the m examples with the highest D(e);
3. Update S by the statistics of the selected examples.
Figure 3: The batch selection algorithm.
use a simple model where the selection probability is a linear function of normalized vote
entropy: P (e) = gD(e), calling g the entropy gain2 .
4.2.3 Batch Selection

An alternative to sequential selection is batch selection. Rather than evaluating examples
individually for their informativeness a large batch of N examples is examined, and the m
best are selected for annotation. The batch selection algorithm is given in Figure 3.
This procedure is repeated sequentially for successive batches of N examples, returning
to the start of the corpus at the end. If N is equal to the size of the corpus, batch selection
selects the m globally best examples in the corpus at each stage (as in Lewis & Catlett, 1994).
Batch selection has certain theoretical drawbacks (Freund et al., 1997), particularly that
it does not consider the distribution of input examples. However, as shown by McCallum
and Nigam (1998), the distribution of the input examples can be modeled and taken into
account during selection. They do this by combining their disagreement measure with a
measure of example density, which produces good results with batch selection (this work is
discussed in more detail below in Section 7.2). A separate diculty with batch selection is
that it has the computational disadvantage that it must look at a large number of examples
before selecting any. As the batch size is decreased, batch selection behaves similarly to
sequential selection.

5. Example: Colorful Coin Flipper
As an illustrative example of a learning task, we define a colorful coin-ipper (CCF) as a
machine which contains an infinite number of coins of various colors. The machine chooses
coins to ip, one by one, where each color of coin has a fixed (unknown) probability of being
chosen. When a coin is ipped, it comes up heads with probability determined solely by its
color. Before it ips a coin, the machine tells the learner which color of coin it has chosen to
2. The selection method used in (Dagan & Engelson, 1995) is randomized sequential selection using this
linear selection probability model, with parameters k, t and g.

345

fiArgamon & Dagan

ip. In order to know the outcome of the ip, however, the learner must pay the machine.
In training, the learner may choose the colors of coins whose outcomes it will examine. The
objective of selective sampling is to choose so as to minimize the training cost (number of
ips examined) required to attain a given prediction accuracy for ip outcomes.
For the case of the CCF, an example e is a coin ip, characterized by its color, and its
class c is either heads or tails. Note that we do not require that ips of a given color always
have the same class. Therefore the best that we can hope to do is classify according to the
most likely class for each color.
For a CCF, we can define a model whose parameters are the heads probabilities for the
coins of each particular color. So, for a CCF with three colors, one possible model would be
m = fr = 0:8; g = 0:66; b = 0:2g, giving the probabilities of heads for red, green, and blue
coins, respectively. A coin of a given color will then be classified `heads' if its score (given
directly by the appropriate model parameter) is > 12 , and `tails' otherwise.

5.1 Implementation Of Sample Selection
Training a model for a CCF amounts to counting the proportion of heads for each color,
providing estimates of heads probabilities. In complete training every coin ip in the training
sequence is examined and added to the counts. In sample selection we seek to label and
count only training ips of those colors for which additional counts are likely to improve
the model's accuracy. Useful colors to train on are either those for which few training
examples have so far been seen, or those whose current probability estimates are near 0.5
(cf. Section 2.2).
Recall that for sample selection we build a committee by sampling models from P (M jS ).
In the case of a CCF, all of the model parameters ffi (the heads probabilities for different
colors) are independent, and so sampling from P (M jS ) amounts to sampling independently
for each of the parameters.
While the form of the posterior distribution P (ffi = ai jS ) is given by the beta distribution, we found it technically easier to use a normal approximation, which was found
satisfactory in practice. Let Ni be the number of coin ips of color i seen so far, and ni be
the number of those ips which came up heads. We approximate P (ffi = ai jS ) as a truncated normal distribution (restricted to [0,1]), with estimated mean i = Nn and variance
i2 =  (1N, ). This approximation made it easy to also incorporate a `temperature' parameter t (as in Section 4.2.2), which is used as a multiplier for the variance estimate i2 . Thus,
we actually approximate P (ffi = aijS ) as a truncated normal distribution with mean i and
variance i2 t. Sampling from this distribution was done using the algorithm given by Press,
Flannery, Teukolsky, and Vetterling (1988) for sampling from a normal distribution.
i

i

i

i

i

5.2 Vote Entropy
The CCF is useful to illustrate the importance of determining classification uncertainty
using the vote entropy over a committee of models rather than using the entropy of the
class distribution given by a single model (as discussed in Section 2). Consider a CCF with
346

fiCommittee-Based Sample Selection for Probabilistic Classifiers

Model
0
1
2
3

Red
0.55 (heads)
0.55 (heads)
0.60 (heads)
0.60 (heads)

Blue
0.45 (tail)
0.45 (tail)
0.55 (heads)
0.55 (heads)
(a)

Green
0.48 (heads)
0.75 (tail)
0.85 (tail)
0.95 (tail)

Color D(e) ACDE
Red
0.0 0.98
Blue
1.0 0.99
Green 0.81 0.68
(b)

Figure 4: (a) A committee of CCF models. (b) The resultant vote entropy for each color.
CCF results for 50 colors

CCF results for 100 colors

200
PTM
Committee-Based Sampling
Complete Training

PTM
Committee-Based Sampling
Complete Training

200

150

Selected training

Selected training

150

100

100

50
50

0
0.5

0
0.55

0.6

0.65
Desired accuracy

0.7

0.75

0.8

0.5

(a)

0.55

0.6

0.65
Desired accuracy

0.7

0.75

0.8

(b)

Figure 5: CCF results for random CCFs with 50 and 100 different coin colors. Results
are averaged over 4 different such CCFs, comparing complete training with two
member sample selection. The figures show the amount of training required for
a desired classification accuracy: (a) for 50 colors, (b) for 100 colors.

three coin colors, red, blue, and green. Suppose the 4-member committee in Figure 4(a) is
generated. From this committee, we estimate for each color its vote entropy D(e), as well
as the average of the class distribution entropies given by each of the individual models
(ACDE), given in Figure 4(b).
If we compare the entropies of red and blue, for example, we see that their entropies
over the expected class probability distribution are both quite high (since both estimated
class probabilities are near 0.5). However, when we consider their vote entropies (over the
assigned classes), blue has maximal entropy, since the range of possible models straddles a
class boundary (0.5), while red has minimal entropy, since the range of possible models does
not straddle a class boundary. That is, it is quite certain that the optimal classification
for red is \heads". We also see how green has a higher vote entropy than red, although its
average class distribution entropy is lower. This shows the importance of using vote entropy
for selection.
347

fiArgamon & Dagan

CCF frequency of selection
1
50 color CCF
100 color CCF

0.9
0.8

Selection frequency

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0

20

40

60

80

100
120
Selected training

140

160

180

200

Figure 6: Frequency of selection vs. amount of selected training for CCFs with 50 and 100
colors, averaged for 4 different CCFs.

5.3 Results
We simulated sample selection for the simple CCF model in order to illustrate some of its
properties. In the following, we generated random CCFs with a fixed number of coins by
randomly generating occurrence probabilities and heads probabilities for each coin color.
We then generated learning curves for complete training, on all input examples, and for two
member sample selection, using 50 coin-ips for initial training. Both complete training
and sample selection were run on the same coin-ip sequences. Accuracy was measured
by computing the expected accuracy (assuming an infinite test set) of the MLE model
generated by the selected training. The figures also show the accuracy for the theoretical
perfectly trained model (PTM) which knows all of the parameters perfectly.
Figure 5 summarizes the average results for 4 comparison runs of complete vs. sample
selection for CCFs of 50 and 100 coins. In Figures 5(a) and (b), we compare the amount
of selected training required to reach a given desired accuracy. We see in both cases that
as soon as sample selection starts operating, its eciency is higher than complete training,
and the gap increases in size as greater accuracy is desired. In Figure 6, we examine the
cumulative frequency of selection (ratio between the number of selected examples and the
total number of examples seen) as learning progresses. We see here an exponential decrease
in the frequency of selection, as expected in the case of QBC for non-probabilistic models
(analyzed in Seung et al., 1992; Freund et al., 1997).

6. Application: Stochastic Part-Of-Speech Tagging
We have applied committee-based selection to the real-world task of learning Hidden Markov
Models (HMMs) for part-of-speech tagging of English sentences. Part-of-speech tagging is
the task of labeling each word in the sentence with its appropriate part of speech (for
example, labeling an occurrence of the word `hand' as a noun or a verb). This task is nontrivial since determining a word's part of speech depends on its linguistic context. HMMs
348

fiCommittee-Based Sample Selection for Probabilistic Classifiers

have been used extensively for this task (e.g., Church, 1988; Merialdo, 1991), in most cases
trained from corpora which have been manually annotated with the correct part of speech
for each word.

6.1 HMMs And Part-Of-Speech Tagging
A first-order Hidden Markov Model (HMM) is a probabilistic finite-state string generator
(Rabiner, 1989), defined as a set of states Q = fqi g, a set of output symbols , a set of
transition probabilities P (qi !qj ) of each possible transition between states qi and qj , a set of
output probabilities P (ajq) for each state q to output each symbol a 2 , and a distinguished
start state q0 . The probability of a string s = a1a2    an being generated by an HMM is
given by
!
n
X
Y
P (qi,1 !qi )P (aijqi ) ;
q1 qn 2Qn i=1

the sum, for all paths through the HMM, of the joint probability that the path was traversed
and that it output the given string. In contrast with ordinary Markov Models, in an HMM
it is not known which sequence of states generated a given string (hence the term `hidden').
HMMs have been used widely in speech and language processing. In particular, an HMM
can be used to provide a classification model for sequence elements: If we need to classify
each element in a sequence, we encode each possible class by a state in an HMM. Training
the HMM amounts to estimating the values of the transition and output probabilities.
Then, given a sequence for classification, we assume that it was generated by the HMM and
compute the most likely state sequence for the string, using the Viterbi algorithm3 (Viterbi,
1967).
An HMM can be used for part-of-speech tagging of words by encoding each possible partof-speech tag, t (noun, verb, adjective, etc.), as an HMM state. The output probabilities,
P (wjt), give the probability of producing each word w in the language conditioned on the
current tag t. The transition probabilities, P (t1!t2), give the probability of generating
a word with the tag t2 given that the previous word's tag is t1 . This constitutes a weak
syntactic model of the language. This model is often termed the tag-bigram model4 .
Given an input word sequence W = w1    wn , we seek the most likely tag sequence
T = t1    tn :
)
arg maxT P (T jW ) = arg maxT PP(T;W
(W )
= arg maxT P (T; W )
3. An alternative classification scheme is to compute the most likely state for each individual element
(instead of the most likely state sequence) by the Forward-Backward algorithm (Rabiner, 1989) (also
called the Baum-Welch algorithm Baum, 1972). We do not address here this alternative, which is
computationally more expensive and is typically not used for part-of-speech tagging. It is possible,
however, to apply the committee-based selection method also for this type of classification.
4. It should be noted that practical implementations of part-of-speech tagging often employ a tag-trigram
model, in which the probability of a tag depends on the last two tags rather than just the last one. The
committee-based selection method which we apply here to the bigram model can easily be applied also
to the trigram case.

349

fiArgamon & Dagan

since P (W ) is a constant. Thus we seek the T which maximizes

P (T; W ) =

n
Y
i=1

P (ti,1!ti )P (wijti )

For technical convenience, we use Bayes' theorem to replace each P (wijti ) term by the term
P (t jw )P (w )
, noting that P (wi) does not effect the maximization over tag sequences and can
P (t )
therefore be omitted (following Church, 1988). The parameters of a part-of-speech model,
then, are: tag probabilities P (ti), transition probabilities P (ti,1!ti ), and lexical probabilities
P (tjw).
Supervised training of the tagger is performed using a tagged corpus (text collection),
which was manually labeled with the correct part-of-speech for each word. Maximum likelihood estimates (MLEs) for the parameters are easily computed from word and tag counts
from the corpus. For example, the MLE of P (t) is the fraction of tag occurrences in the corpus that were the tag t, whereas P (tjw) is the ratio between the count for the word w being
labeled with the tag t and the total count for w. In our committee-based selection scheme,
the counts are used also to compute the posterior distributions for parameter estimates, as
discussed below in Section 6.2.
We next describe the application of our committee-based selection scheme to the HMM
classification framework. First we will discuss how to sample from the posterior distributions over the HMM parameters P (ti !tj ) and P (tjw), given training statistics.5 We then
discuss the question of how to define an example for training|an HMM deals with (in
principle) infinite strings; on what substrings do we make decisions about labeling? Finally,
we describe how to measure the amount of disagreement between committee members.
i

i

i

i

6.2 Posterior Distributions For Multinomial Parameters
In this section, we consider how to select committee members based on the posterior parameter distributions P (ffi = aijS ) for an HMM, assuming a uniform prior. First note that the
parameters of an HMM define a set of multinomial probability distributions. Each multinomial corresponds to a conditioning event and its values are given by the corresponding
set of conditioned events. For example, a transition probability parameter P (ti !tj ) has
conditioning event ti and conditioned event tj .
Let fui g denote the set of possible values of a given multinomial variable (e.g., the
possible tags for a given word), and let S = fni g denote a set of statistics extracted from
the training set, where ni is the number of times that the value ui appears in the training
P
set. We denote the total number of appearances of the multinomial variable as N = i ni .
The parameters whose distributions we wish to estimate are ffi = P (ui ).
The maximum likelihood estimate for each of the multinomial's distribution parameters,
ffi , is ff^i = nN . In practice, this estimator is usually smoothed in some way to compensate
for data sparseness. Such smoothing typically reduces the estimates for values with positive
i

5. We do not sample the model space over the tag probability parameters, since the amount of data for tag
frequencies is large enough to make their MLEs quite definite.

350

fiCommittee-Based Sample Selection for Probabilistic Classifiers

counts and gives small positive estimates for values with a zero count. For simplicity, we
first describe here the approximation of P (ffi = aijS ) for the unsmoothed estimator6 .
The posterior P (ffi = ai jS ) is a Dirichlet distribution (Johnson, 1972); for ease of
implementation, we used a generalization of the normal approximation described above
(Section 5.1) for binomial parameters. We assume first that a multinomial is a collection of
independent binomials, each of which corresponds to a single value ui of the multinomial; we
then separately apply the constraint that the parameters of all these binomials should sum
to 1. For each such binomial, we sample from the approximate distribution (possibly with
a temperature t). Then, to generate a particular multinomial distribution, we renormalize
the sampled parameters so they sum to 1.
To sample for the smoothed estimator, we first note that the estimator for the smoothed
model (interpolating with the uniform) is

 ;
ff^Si = (1(1,,))Nni++
where   1 is a smoothing parameter controlling the amount of smoothing (in our experiments  = 0:05), and  is the number of possible values for the given multinomial. We
then sample for each i from the truncated normal approximation (as in Section 5) for the
smoothed estimate, i.e, with mean  = ff^Si and variance 2 = (1N,) . Normalization for the
multinomial is then applied as above.
Finally, to generate a random HMM given statistics S , we note that all of its parameters
P (ti !tj ) and P (tjw) are independent of each other. We thus independently choose values
for the HMM's parameters from each multinomial distribution.

6.3 Examples For HMM Training
Typically, concept learning problems are formulated such that there is a set of training
examples that are independent of each other. When training an HMM, however, each
state/output pair is dependent on the previous state, so we are presented (in principle)
with a single infinite input string for training. In order to perform sample selection, we
must divide this infinite string into (short) finite strings.
For part-of-speech tagging, this problem may be solved by considering each sentence as
an individual example. More generally, we can break the text at any point where tagging
is unambiguous. In particular, it is common to have a lexicon which specifies which partsof-speech are possible for each word (i.e, which of the parameters P (tjw) are positive). In
bigram tagging, we can use unambiguous words (those with only one possible part of speech)
as example boundaries. Similar natural breakpoints occur in other HMM applications; for
example, in speech recognition we can consider different utterances separately. In other
cases of HMM learning, where such natural breakpoints do not occur, some heuristic will
have to be applied, preferring to break at `almost unambiguous' points in the input.
6. In the implementation we smooth the MLE by interpolation with a uniform probability distribution,
following Merialdo (1991). Adaptation of P (ffi = ai S ) to the smoothed version of the estimator is given
below.
j

351

fiArgamon & Dagan

6.4 Quantifying Disagreement
Recall that our selection algorithms decide whether or not to select an example based on
how much the committee members disagree on its labeling. As discussed in Section 4.2.2,
we suggest the use of vote entropy for measuring classification disagreement between committee members. This idea is supported by the fact that we found empirically that the
average normalized vote entropy for words which the tagger (after some training) classified correctly was 0.25, whereas the average entropy for incorrectly classified words was
0.66. This demonstrates that vote entropy is a useful measure of classification uncertainty
(likelihood of error) based on the training data.
In bigram tagging, each example consists of a sequence of several words. In our implementation, we measured vote entropy separately for each word in the sequence, and use
the average vote entropy over the sequence as our measurement of disagreement for the
example. We use the average entropy rather than the entropy over the entire sequence,
because the number of committee members is small with respect to the total number of
possible tag sequences.

6.5 Results
We now present our results on applying committee-based sample selection to bigram part-ofspeech tagging, comparing it with complete training on all examples in the corpus. Evaluation was performed using the University of Pennsylvania tagged corpus from the ACL/DCI
CD-ROM I. For ease of implementation, we used a complete (closed) lexicon which contains
all the words in the corpus.7 Approximately 63% of the word occurrences in the corpus are
ambiguous in the lexicon (have more than one possible part-of-speech).
Each committee-based selection algorithm was initialized using the first 1,000 words
from the corpus, and then examined the following examples in the corpus for possible
labeling. The training set consisted of the first million words in the corpus, with sentence
ordering randomized to compensate for inhomogeneity in corpus composition. The test set
was a separate portion of the corpus consisting of 20,000 words, starting just after the first
1,000,000.
We compared the amount of training required by different selection methods to achieve
a given tagging accuracy on the test set, where both the amount of training and tagging
accuracy are measured over ambiguous words.8
6.5.1 Labeling Efficiency
7. We use the lexicon provided with Brill's part-of-speech tagger (Brill, 1992). While in an actual application
a complete lexicon would not be available, our results using a complete lexicon are valid, as the evaluation
of complete training and committee-based selection is comparative.
8. Most other work on tagging has measured accuracy over all words, not just ambiguous ones. Complete
training of our system on 1,000,000 words gave us an accuracy of 93.5% over ambiguous words, which
corresponds to an accuracy of 95.9% over all words in the test set, comparable to other published results
on bigram tagging.

352

fiCommittee-Based Sample Selection for Probabilistic Classifiers

40000
Batch selection (m=5; N=100)
Thresholded selection (th=0.3)
Randomized selection (g=0.5)
Two member selection
Complete training

35000

Selected training

30000
25000
20000
15000
10000
5000
0
0.85

0.86

0.87

0.88

0.89
Accuracy

0.9

0.91

0.92

0.93

Figure 7: Labeled training versus classification accuracy. In batch, random, and thresholded runs, k = 5 and t = 50.

Figure 7 presents a comparison of the results of several selection methods. The reported parameter settings are the best found for each selection method by manual tuning.
Figure 7 shows the advantage that sample selection gives with regard to annotation cost.
For example, complete training requires annotated examples containing 98,000 ambiguous
words to achieve a 92.6% accuracy, while the selection methods require only 18,000{25,000
ambiguous words to achieve this accuracy. We also find that, to a first approximation, all
of the methods considered give similar results. Thus, it seems that a refined choice of the
selection method is not crucial for achieving large reductions in annotation cost.
6.5.2 Computational Efficiency

Figure 8 plots classification accuracy versus number of words examined, instead of those
selected. Complete training is clearly the most ecient in these terms, as it learns from all
examples examined. The selective methods are similar, though two member selection seems
to require somewhat fewer examples for examination than the other methods. Furthermore,
since only two committee members are used this method is computationally more ecient
in evaluating each examined example.
6.5.3 Model Size

The ability of committee-based selection to focus on the more informative parts of the
training corpus is analyzed in Figure 9. Here we examined the number of lexical and bigram
353

fiArgamon & Dagan

400000
Batch selection (m=5; N=100)
Thresholded selection (th=0.3)
Randomized selection (g=0.5)
Two member selection
Complete training

350000

Examined training

300000
250000
200000
150000
100000
50000
0
0.85

0.86

0.87

0.88

0.89
Accuracy

0.9

0.91

0.92

0.93

Figure 8: Examined training (both labeled and unlabeled) versus classification accuracy.
In batch, random, and thresholded runs, k = 5 and t = 50.

20000

1600
Two member selection
Complete training

18000

Two member selection
Complete training

1400

14000

Bigram model size

Lexical model size

16000

12000
10000
8000
6000

1200
1000
800
600

4000
400

2000
0
0.85

0.86

0.87

0.88

0.89
Accuracy

0.9

0.91

0.92

200
0.85

0.93

(a)

0.86

0.87

0.88

0.89
0.9
Accuracy

0.91

0.92

0.93

0.94

(b)

Figure 9: Numbers of frequency counts > 0, plotted (y-axis) versus classification accuracy
(x-axis). (a) Lexical counts (freq(t; w)) (b) Bigram counts (freq(t1 !t2 )).

354

fiCommittee-Based Sample Selection for Probabilistic Classifiers

1
Two member selection
Batch selection (m=5; N=50)
Batch selection (m=5; N=100)
Batch selection (m=5; N=500)
Batch selection (m=5; N=1000)

0.98

Accuracy

0.96
0.94
0.92
0.9
0.88
0.86
0

100000

200000

300000 400000
Examined training

500000

600000

Figure 10: Evaluating batch selection, for m = 5. Classification accuracy versus number of
words examined from the corpus for different batch sizes.
counts that were stored (i.e, were non-zero) during training, using the two member selection
algorithm and complete training. As the graphs show, committee-based selection achieves
the same accuracy as complete training with fewer lexical and bigram counts. To achieve
92% accuracy, two member selection requires just 6200 lexical counts and 750 bigram counts,
as compared with 15,800 lexical counts and 1100 bigram counts for complete training. This
implies that many counts in the data are not needed for correct tagging, since smoothing
estimates the probabilities equally well.9 Committee-based selection ignores these counts,
focusing its efforts on parameters which improve the model's performance. This behavior
has an additional practical advantage of reducing the size of the model significantly. Also,
the average count is lower in a model constructed by selective training than in a fully trained
model, suggesting that the selection method tends to avoid using examples which increase
the counts for already known parameters.
6.5.4 Batch Selection

We investigated the properties of batch selection, varying batch size from 50 to 1000
examples, fixing the number of examples selected from each batch at 5. We found that
in terms of the number of labeled examples required to attain a given accuracy, selection
for these different batch sizes performed similarly. This means that increased batch size
9. As mentioned above, in the tagging phase we smooth the MLE estimates by interpolation with a uniform
probability distribution, following Merialdo (1994).

355

fiArgamon & Dagan

does not seem to improve the effectiveness of selection. On the other hand, we did not see
a decrease in performance with increased batch size, which we might have expected due
to poorer modeling of the input distribution (as noted in Section 2.2). This may indicate
that even a batch size of 1000 (selecting just 1/200 of the examples seen) is small enough
to let us model the input distribution with reasonable accuracy. However, the similarity
in performance of the different batch sizes to each other and to sequential selection does
not hold with respect to the amount of unlabeled training used. Figure 10 shows accuracy
attained as a function of the amount of unlabeled training used. We see quite clearly that,
as expected, using larger batch sizes required examining a far larger number of unlabeled
training examples in order to obtain the same accuracy.

7. Discussion
7.1 Committee-Based Selection As A Monte-Carlo Technique
We can view committee-based selection as a Monte-Carlo method for estimating the probability distribution of classes assigned to an example over all possible models, given the
training data. The proportion of votes among committee members for a class c on an example e is a sample-based estimate of the probability, for a model chosen randomly from
the posterior model distribution, of assigning c to e. That is, the the proportion of votes
for c given e, V (kc;e) , is a Monte-Carlo estimate of
Z

P (cje; S ) = TM (cje)P (M jS )dM
M

where M ranges over possible models (vectors of parameter values) in the model space M,
P (M jS ) is the posterior probability density of model M given statistics S , and TM (cje) = 1
if c is the highest probability class for e based on M (i.e, if c = arg maxc PM (ci je), where
PM (cje) is the class probability distribution for e given by model M ), and 0 otherwise. Vote
entropy, as defined in Section 4.2.2, is thus an approximation of the entropy of P . This
entropy is a direct measure of uncertainty in example classification over the possible models.
Note that we measure entropy over the final classes assigned to an example by possible
models (i.e, TM ), not over the class probabilities given by a single model (i.e, PM ), as
illustrated by the CCF example of Section 5.2. Measuring entropy over PM (say, by looking
at the expected probability over all models) would not properly address properties 1 and 2
discussed in Section 2.2.
i

7.2 Batch Selection
Property 3 discussed in Section 2.2 states that parameters that affect only few examples have
low overall utility, and so atypical examples are not very useful for learning. In sequential
selection, this property is addressed by independently examining input examples which are
drawn from the input distribution. In this way, we implicitly model the distribution of model
parameters used for classifying input examples. Such modeling, however, is not inherent in
the basic form of batch selection, which can lead to it being less effective (Freund et al.,
1997).
356

fiCommittee-Based Sample Selection for Probabilistic Classifiers

This diculty of batch selection is addressed directly by McCallum and Nigam (1998),
who describe a version of batch selection (called pool-based sampling), which differs from
the basic batch selection scheme presented in Section 4.2.3 in two ways. First, they quantify disagreement between committee members by the KL-divergence to the mean (Pereira
et al., 1993), rather than vote entropy. More significantly, their disagreement measure is
combined with an explicit density measure in density-weighted sampling, such that documents that are similar to many other documents in the training set will be more probably
selected for labeling. This is intended to address property 3 in Section 2.2. The authors
found empirically that for text classification using naive Bayes, their density-weighted poolbased selection method using KL-divergence to the mean improved learning eciency over
complete training. They also found that sequential selection using vote entropy was worse
than complete training for their problem.
We hypothesize that this is due to the high degree of sparseness of the example space
(text documents), which leads to a large proportion of the examples being atypical (even
though documents similar to a given atypical document are rare, many different atypical
documents occur.) Since this is the case, the sequential variant may tend to select many
atypical documents for labeling, which would degrade learner performance by skewing the
statistics. This problem can be remedied by adding density-weighting to sequential selection
in future research. This may yield an ecient sequential selection algorithm that also works
well in highly sparse domains.

8. Conclusions
Labeling large training sets for supervised classification is often a costly process, especially
for complicated domain areas such as natural language processing. We have presented an
approach for reducing this cost significantly using committee-based sample selection, which
reduces redundant annotation of examples that contribute little new information. The
method is applicable whenever it is possible to estimate a posterior distribution over the
model space given the training data. We have shown how to apply it to training Hidden
Markov Models, and demonstrated its effectiveness for the complex task of part of speech
tagging. Implicit modeling of uncertainty makes the selection system generally applicable
and relatively simple to implement. In practical settings, the method may be applied in a
semi-interactive process, in which the system selects several new examples for annotation
at a time and updates its statistics after receiving their labels from the user.
The committee-based sampling method addresses the three factors which relate the
informativeness of a training example to the model parameters that it affects. These factors
are: (1) the statistical significance of the parameter's estimate, (2) the parameter's effect
on classification, and (3) the probability that the parameter will be used for classification
in the future. The use of a committee models the uncertainty in classification relative to
the entire model space, while sequential selection implicitly models the distribution of the
examples.
Our experimental study of variants of the selection method suggests several practical
conclusions. First, it was found that the simplest version of the committee-based method,
357

fiArgamon & Dagan

using a two-member committee, yields reduction in annotation cost comparable to that
of the multi-member committee. The two-member version is simpler to implement, has
no parameters to tune and is computationally more ecient. Second, we generalized the
selection scheme giving several alternatives for optimizing the method for a specific task.
For bigram tagging, comparative evaluation of the different variants of the method showed
similar large reductions in annotation cost, suggesting the robustness of the committeebased approach. Third, sequential selection, which implicitly models the expected utility
of an example relative to the example distribution, worked in general better than batch
selection. Recent results on improving batch selection by modeling explicitly the `typicality'
of examples suggest further comparison of the two approaches (as discussed in the previous
section). Finally, we studied the effect of sample selection on the size of the trained model,
showing a significant reduction in model size for selectively trained models.
In future research we propose to investigate the applicability and effectiveness of committeebased sample selection for additional probabilistic classification tasks. Furthermore, the
generality obtained by implicitly modeling information gain suggests using variants of
committee-based sampling also in non-probabilistic contexts, where explicit modeling of
information gain may be impossible. In such contexts, committee members might be generated by randomly varying some of the decisions made in the learning algorithm.

Acknowledgments
Discussions with Yoav Freund, Yishai Mansour, and Wray Buntine greatly enhanced this
work. The first author was at Bar-Ilan University while this work was performed, and was
supported by the Fulbright Foundation during part of the work.

References
Angluin, D. (1987). Learning regular sets from queries and counterexamples. Information
and Computation, 75 (2), 87{106.
Angluin, D. (1988). Queries and concept learning. Machine Learning, 2, 319{342.
Baum, L. E. (1972). An inequality and an associated maximization technique in statistical
estimation of probabilistic functions of a markov process. Inequalities, 3:1-8.
Black, E., Jelinek, F., Lafferty, J., Magerman, D., Mercer, R., & Roukos, S. (1993). Towards
history-based grammars: using richer models for probabilistic parsing. In Proc. of the
Annual Meeting of the ACL, pp. 31{37.
Brill, E. (1992). A simple rule-based part of speech tagger. In Proc. of ACL Conference on
Applied Natural Language Processing.
Church, K. W. (1988). A stochastic parts program and noun phrase parser for unrestricted
text. In Proc. of ACL Conference on Applied Natural Language Processing.
Cohn, D., Atlas, L., & Ladner, R. (1994). Improving generalization with active learning.
Machine Learning, 15, 201{221.
358

fiCommittee-Based Sample Selection for Probabilistic Classifiers

Cohn, D. A., Ghahramani, Z., & Jordan, M. I. (1995). Active learning with statistical
models. In Tesauro, G., Touretzky, D., & Alspector, J. (Eds.), Advances in Neural
Information Processing, Vol. 7. Morgan Kaufmann, San Mateo, CA.
Dagan, I., & Engelson, S. (1995). Committee-based sampling for training probabilistic
classifiers. In Proceedings of the International Conference on Machine Learning.
Elworthy, D. (1994). Does Baum-Welch re-estimation improve taggers?. In Proc. of ACL
Conference on Applied Natural Language Processing, pp. 53{58.
Engelson, S., & Dagan, I. (1996a). Minimizing manual annotation cost in supervised learning from corpora. In Proceedings of the 34th Annual Meeting of the Association for
Computational Linguistics.
Engelson, S., & Dagan, I. (1996b). Sample selection in natural language learning. In
Wermter, S., Riloff, E., & Scheler, G. (Eds.), Symbolic, Connectionist, and Statistical
Approaches To Learning For Natural Language Processing. Springer-Verlag.
Freund, Y., Seung, H. S., Shamir, E., & Tishby, N. (1997). Selective sampling using the
Query By Committee algorithm. Machine Learning, 28, 133{168.
Freund, Y. (1994). Sifting informative examples from a random source. In Working Notes
of the Workshop on Relevance, AAAI Fall Symposium Series, pp. 85{89.
Gale, W., Church, K., & Yarowsky, D. (1993). A method for disambiguating word senses
in a large corpus. Computers and the Humanities, 26, 415{439.
Johnson, N. L. (1970). Continuous Univariate Distributions { 2. John Wiley & Sons, New
York.
Johnson, N. L. (1972). Continuous Multivariate Distributions. John Wiley & Sons, New
York.
Kupiec, J. (1992). Robust part-of-speech tagging using a hidden makov model. Computer
Speech and Language, 6, 225{242.
Lewis, D. D., & Catlett, J. (1994). Heterogeneous uncertainty sampling for supervised
learning. In Proceedings of the International Conference on Machine Learning.
Lewis, D. D., & Gale, W. A. (1994). A sequential algorithm for training text classifiers. In
Proceedings of the of the ACM SIGIR Conference.
Liere, R., & Tadepalli, P. (1997). Active learning with committees for text categorization.
In Proceedings of the National Conference on Artificial Intelligence.
Littlestone, N. (1988). Learning quickly when irrelevant features abound: A new linearthreshold algorithm. Machine Learning, 2.
MacKay, D. J. C. (1992a). The evidence framework applied to classification networks.
Neural Computation, 4.
359

fiArgamon & Dagan

MacKay, D. J. C. (1992b). Information-based objective functions for active data selection.
Neural Computation, 4.
Matan, O. (1995). On-site learning. Tech. rep. LOGIC-95-4, Stanford University.
McCallum, A. K., & Nigam, K. (1998). Employing EM and pool-based active learning
for text classification. In Proceedings of the International Conference on Machine
Learning.
Merialdo, B. (1991). Tagging text with a probabilistic model. In Proc. Int'l Conf. on
Acoustics, Speech, and Signal Processing.
Merialdo, B. (1994). Tagging text with a probabilistic model. Computational Linguistics,
20 (2), 155{172.
Mitchell, T. (1982). Generalization as search. Artificial Intelligence, 18.
Pereira, F., Tishby, N., & Lee, L. (1993). Distributional clustering of english words. In
Proceedings of the Annual Meeting of the Association for Computational Linguistics
(ACL).
Plutowski, M., & White, H. (1993). Selecting concise training sets from clean data. IEEE
Trans. on Neural Networks, 4 (2).
Press, W. H., Flannery, B. P., Teukolsky, S. A., & Vetterling, W. T. (1988). Numerical
Recipes in C. Cambridge University Press.
Rabiner, L. R. (1989). A tutorial on Hidden Markov Models and selected applications in
speech recognition. Proc. of the IEEE, 77 (2).
Seung, H. S., Opper, M., & Sompolinsky, H. (1992). Query by committee. In Proceedings
of the ACM Workshop on Computational Learning Theory.
Viterbi, A. J. (1967). Error bounds for convolutional codes and an asymptotically optimal
decoding algorithm. IEEE Trans. Informat. Theory, T-13.

360

fi	
fffi 	


	"!$#%#'&#(((*)+#,-#"."#/0

=+>@?$ACBEDFGHDIAKJML

1*23456#78(9:;2
!5<98((

DNEOPCQR?6O?S>UTWVXPCD*AHD*AHJZY[P\B]P

^H_`ba_dcHe4f@`hgXi'ajlk
zl{|}*}~E}6<~{{~hX}h4+b*64h*
 bhbE*
 %]%*z

mln+o+pqrbsXtCruv'w xy+n+py+rEw rlpy

_`be'<`bj+i'a

bn+rpqtun%'w mly'w rpy

 *<}4}|RX*E}h}zl*b
 %RX}}hX~|$*
 }}hE*
 }}hb]@-z

6hE
4Xl4%d-bbHHl-4H-l%%-
hCl4Hh-X$-bh-l*h
-Sb%b-H-hHbC$"]Cl-
 '-%S%'-lCh+C%-4l'lEhl%
hCH@l-@W%lhH-Rl@h"@-Wl%
hH$l4l6-C$-l%*  SblHb4-6l<
hl-Hl--h%@6d-lb-
-+%lH-XCh-X$-@46lS$l%C
h%-$4**@hbWl--$bl@4RlWl
@"4l%X'4E$l-SE4bl44\*4*6hbl-
4ShClSl-64Kl"-'l-h%\l
44@"h4l%6$-4h'l<4H@l$
l-
Xh 4S6 

 	ff
fi	
fi
 !"
fi#	ff$#&%'(#)*	+	ff',
 
fi##-
 #	ff.$	',
)/	0fi!
1fi#

& 
2!3!) &
)4)&%5&%
2fi
2) ) 76
 #	ff8
 
9:;	ff=<>ff#	ff) fi#9?	1) $8!) &
)4)
'@
A!'B#-0CEDE%='@
A!'B'F

9G
2H%#I
1fiJ<)	ffK&%L
fi##$9M	NO%=
&
B
:	ff
&%
<<$	ff<>
2 ) )P	0&%E%	) Jfi#
 !"
fi#	ff>O%'Q;	ffR&%
&
SCNDT%EUP	ff V4) $!13%$
;	ff) )P	ffJ!'@<$	J&%WL
fi##$9"	0&
!J
&
=19@# 7+9!G
Kfi#!'B!
 !G')fi
1fi#
!) &
)=<>	E 	K
<<fi#9ff!M&%4H%	)Mfi#
 !X
2fi	ff$#&%'MYO%$19M!$
2) !Mfi
2) ) 76
 #	ff

 
29C
Z 
1fi#![ >	ffB
\	J+	ff)  
fi$
)	ff)@!fi!S[)&1^]_ ##_9Y
O
a`bO$9?$	ffY
	ff!
L
95	E&%J;	ff 'J
 #	ff5) c 	dfi!
1Sfi/
%3	ff1]$C@e1]$ #$9?'J
93
$#) @U%
	ff1) >I
 #	ff)/J 	f1SE 
VB!4) 	ff'BPUE
a9W)&%B
)R#) 
) P) $#_94	ffgUW%4&%P;	ff 'J
 #	ff
) 8 	Mfi!
1fiE
h	ff1]$=#)Bji0$b$	ff'k&%@!;	ff '@
	5 	MU%%c&%"fi#
5
fi#	$#&%'
UE#fi#fiW%
aM
) )C:l	ff4A
'@<fi#YRUW%8fi!
1Sfi#m<#Afffi#)K!8'J
K
&
Yn&%d

2fi9) B$9^<#
fi#fi#9
) )E#)&
fi0!<W
&%o&%
K&%='B$#
fi!)W	n&%+
2&$ 	ffo	ff >)&<	ff!K 	B&%
	ff1) >I
 #	ffpC4q	ff'@
2)4!3UW%#H%hA^<S$ )#)&
ff>"
$@
&
fiN<fi!
)r+	)&1]$ #@fi!
1fi#!
 $	$)ts;e'49&%pYuavvwxCEyzM	O%E	ff'J
!)Yp&%t'	) Pb$L$9<=	2/ $	ffP#)='B#) O
V)'@

 #((%('fi+fi|5	X	5o}$%	R~E	*ff34	R2
!bfi

b!*l5
{

fi

n+o+pqrbsm,Xn+rpq

$!@
O
a`bO$9C0O%>"
)W	0fi!
1Sfi!K $	ffo
$#) )PUW%J&%;	ff 'J
 #	ffK) "	=fi!
1fi

%@	1) $I
2 #	ffK#)E!
L^
2 CRl	/A
'@<fi#Y!K&%'B#
fi0	'@
!@#W'J
9@	E1S=<	) )1fi#
 	B<z;	ff '(&%W)  )*))&
$9@	=ff

*O%
*
4!
ff	))P#)u
 
Cl	ffR	'@
!)
!BUW%#%=fi!
1Sfi#B >	ff$)n	ffY
B
 	ff'J
 t'&%	ffB	fi#!'B!
 !B	ffg	ff $=')fi
1fi#
	ff1) >I
 #	ff)TUE#fifin!'@<$	&%=<>t
 
29"	R&%fi!
) )j6;	ff 'BKb$	ff'&%&
!

&
SC
zy 8&%#)"
$fi#MUPd
$))@&%M<$	ff1fi#'	2W# 7;9ff!8& 
!!8)&
)@&%
2
$G')>`
fi!
1fi#0C8fi!
Qs$uvwx'B	ff) O 
 5O%
t
)@	#) Kfi#fiP!$
) )Yo$'B	ff!5	#) Kb$	ff'

 O$!1 P;	ff 'J
 #	ffB$
2) )/&%<$# #f
2 
9B	O%W$)&fi# !Bfi!
) ) 76R7O%*)O
'B

 O$!1 =	#) =#)W<>) T&%=
2&
4 	"1fi!
)) 760CEybMO%
) 	/'B#) fi!
1fi#3& 
2!K!`
) &
)fsfi!
) )E	#) xo&%T	ff<<	)W#)P& #&E&%& 
2!B
&
UTfi#fi>)&fi#/!M
fi!
) )j6
UE#&%)j6
 fi#9%#ff%:<$# #m
 
9Cl	ffKA
'@<fi#YP$	fi9Q
l$#fiGs>uvvw
SY
uvvw1xn#fifi!) O 
 K&%
n;	ffnfi!
) )P	#) Wfi#fi#)P	0fi#) )/O%
BffY$'B	ff!B'B#) fi!
1fi#M!) &
)
b$	ff'&%& 
!!@
&
B$)OfiMM%#ff%<$# #=

9G>fi
2 # 	Bfi!
) ) 76
 #	ffM
2`
 
#)
%dUE#&%	ffJ fi#
!"O%& 
!!"
2&
C
 t!&$	@
M'B&%	d;	ffT# 7;9ff!5'B#) fi!
1Sfim!) O
)&%
#)t	)&<76K 	M
9
<
$fi!
fi
 !"
fi#	ff$#&%'Y^1E 
2&%o) $)E
)P
= 
fi0'B&%	"O%
P
"1S=
<<fi##d 	


2&
) P1O;	ff$T+!B#*	t
fi#
@
fi#	$#&%'MC/DE%1
) ##
=#)P 	B) W
4) /	2fi#


fi#	ff>O%'B)@ 	3$
Mfi
2) ) 76$)K&%
@
@
)"
J*#;a+	B&%M& 
2!5
2&
C?DT%d'B&%	m)
'B	
 [195&%M %#L^M	2f$'B	\ff;#=!m$ff$)) #	ffm

fi#9) #)Xs  #)&1S$YuvxC
3	ff fi##4)B
M
)Xs;
:!) &
x4&%
2	ff)B	+	fifi#	U,O%@)&
'BK'B	fffiP
)&%">) 	E&%

&
J
X
<<S
$)W
2)E&%	ffff%#)W	ff')P.>	ff'
B7i0$f<$	1
1#fi#$9d#) O$!1 #	ffpCWE

2 )

$
2) )WUTO%d
Bfi!
$t$) #
fin $	C #  #)&1S$?s$uvxo)&) )W1#fi!3
@'B	fiN) !M
fi#fi
	R&%
&
4A<P;	ffP&%)&)O< :	ff fi##W
M)  !@UW%&%T	ff)	ffo	ff)	W1fi#	ff
 	@O%='B	fffin) !"&%A  
2fifi#9M) O #-m7`b ) aC
 $YgU*K
<<fi#9XO%)B#
d195) !5
K) =	Efi!
) )j6>)=;	ff 'db$	ff'<
$	P&%@&
!

&
	W )RUW%&%R!) O
)/!B&%E$'J
!@<
>	p&%E&
!4
&

$E'B#) fi!
1Sfi0CP
!'@<	$&
Pji0$=1S$U*J	ffoUP	ff VB
M<$	)W
<<$	ff
2H%)o 	B	ff fi##/ #	ffK#)E&%

	ff
<<$	ff
2H%:
) )&'B)O%
E&%B $	$)W!:&%Bfi!
) )fi!
1Sfi)B
$4<SB	/O%t<
$fi!

'B	fiN1S!M6 	"O%=
&
SCWyb:) )Yp	ffW'&%	ff5
 '@< )E	"# 7+9:
&
K<	! )&%

UP	fffi#X1S	ff fi##$)W!c'B	fi;C
 W
fi!
 	ffP
<<$	
H%J	ff6
&
) )*O%
*
>f<$	W 	Bfi!
1Sfi!K $	ff>)
"UPT6
&%
46fi# $!8)&1) &
 !
fi#fi#98'J<$	)"<_+	ff'@
KUW%8fi!
1fi#)d
$M	#) 9Cmybm
2	pY/UP
	ff'@<
$6fi# $!M 	K'@
]$	ff$#$9"	 ) 'B1fi#Bfi
2) ) 76$) 	K#fifi!) O 
 B&%

fi#&%	ffff%:'@
]$	ff$#$9
	=fi!
) ) 76$)=<>	I#B) 	ff'BB<$	  #	ff3

!) f	)9M
&
YS6fi# $!d$)&fi# )!:) #ff76
 fi#9
%#ff%=
2 
9C@&%!$8A^<S$!'B4I
fi!
)tO%<>)	5	E	ff'&%	ffch 7;9ff!
	fffi#9?'B#) fi!
1Sfi#[
&
M<S	!)C  K	fffi!MUE#&%5
)) ) #	ff8	Pb&$M$) 
$%3!$ #	ff)

!'BM
*'!'B#-!K&%W<$	ff1
1fi##$9"	2#) 
$!t!) O
)P&%
*
$EA< #	ff)E 
&%&%

	#) C

OOn b_jI0_O;$bP jzbR$_I;PO$_b=;E/Pz!jb&P$OjOj Pp$z/pjBI
 b_OzI$Eab$;b;$IROzIzbOIPab  a=j;;aIjff


fi

'@3



 
 

prv+hsvc%q+mrqhrpm6n++v+v5SIb

X

DE%"<>	ff1fi#'	W%
fi#!5	#) "%
)B1c&%B;	ff)	W'B%X
2   #	ff3!5'@
%!"fi#


"'	) R! #fi#
 !@
fi#	ff$#&%')/%
W
'B%
#)&'(+	ffR%
fi!@	#) !B&%E&
!

&
Kfi!
1Sfi#)C@l	ffA^
'@<fi#Y< !3!3#) #	ff5O$)#)=) #ff8 	d$@O%@H%
@&%

&%4&$=#)	z6 !@ 	"	#) B!:&%=O 
!!M
&
CP)f<S	!X	W19d4
'B1$Y Z 
a 
 

mqK- $	)&Vfs$uavvwxYR$'	I!?	#) .$	'O%M
&
51SO+	ff>d%9<	O%) #)@;	ff '@
2 #	ff?%
2)@&%


O
&%
E	#) 9MA
'@<fi#)	"	E!@%9^<S	&%) #)	ff) & 	pC
DT%
K	2Nfi#!'B!
 !d!) O
)W	@!'@<$	&%B<z;	ff '@
=	/
$) %ff1S	ffEfi!
)>`
) 76$)E%
)E1M
r;	ff)P	$)
$%@!K1	&%K<
  @$		M
"!) O
O`_1
) Mfi#
 !C
 #fi)	ff,s$uvx@) 
\`_
$) d#ff%1	ff5s_ `_WxKfi!
) )j68s!\A^<S$!'B)5dUE
)K) M 	
&%$xo 	B) fi#E!) O
)E&%
oUP$WO%M) "	=;	ff '
KuO`_Wfi!
) ) 76S	fffi#9"!) &
)E&%

&%K `_fi!
) ) 765	ff$ fi#93U*>>&
!:+	ffO%GuH`_W4C0D0	'BV?s$uavwxPA 5O%)B
<`
<$	ff
2H%3UTO%?
:<$	ff>"&%
2=
fi#fi#)  #fi#) 	ffp)
2fi	ff$#&%'k+	>
) !5I
2fi)J	IC  fi#) 	

5
$ !-ms$uvvSYWuvvvx@%
a:	 <	ff
 &%#)G
<<$	ff
%m! 	[
5)OX	2=)&
3) O`
fi# #	ff3 %#L^);	ffTAff'J<fi
z`_1
) 5fi#
d
2fi	ff$#&%'B)CTf%
SY01fi#4
dfi!1$Ms$uvvuax
'B	ff)& 
 4&%
R) fi# !4)&
)*1
2) B	ffB$	ff$)R	O%	ff&$!1 #	ff4 	fi
2) ) 76
 #	ff

 
29X!8
c)&
O`_1
) 8fi!
) ) 76B!'@<>	I)=&%"
2 
95	E&%J&%"$)&fi# !5fi!
)) 7`
6CPeV
fi!
V5s>uvvffx$
 3
K!) &
4) fi# #	ff3'B%
#)&';	ffE
$) W#ff%1	ffTfi!
) ) 76$)
UE#&%J&%	ff
2fi	N$!@&%!P	'@<&
 #	ff
2fi	) aYffU%%K<)T	ff@O%f'B1P	0) 	ff$
!) &
)aCdDE%K) fi# #	ff8	W
K;U)&
)ds) #ff

)B<$	 	_9<)Ox195
:3	ffT
$fi#	
)&
'J<fi!@
fi#	ff>O%''B	ff)& 
 J&%
/
 
29@UE
)/'@
!&
!G
@K 
#) B;	ffR) 
fi

&
T)  )aC  #fi#) 	ffds>uvvvxg<$	ff#)/
	ff'@<>%) #E	$#Um	!) &
E)fi	@ %#L^)
;	ffPA'@<fi!
z`_1
) :fi#
 !G
2fi	ff$#&%'B)C
D %E#
=	2) fi# !5 	ff	!) &
)E%
)P
fi#) 	t1S"
<<fi##M 		O%R$9^<S)/	pfi!
) ) 76$)C
T
 !)  	ffMs$uvxp'B	) & 
tO%fi##$9B	) fi# !3&
n'B#) ) ) UW%fi
 !=) && 
fi
) $!< #	ff)aC?eV
2fi
V?
mP#) ) fi!
s$uvvx) $!13
8
<<$	ff
%c 	d) fi# !8!) &
)B;	ff

:#) #	ffm&>G
fi#	$#&%') !?
:
) O`_1
) m$&>
fiW
fi#	$#&%'M)4&
A	ff	ff'493	W
) )ds;	ff
A
'@<fi# &%X'	) $`b	ff`_<S	!M
2) ) ffxC Z UE#)d
E
 fi# 5s$uvv2ffxB#fi#fi)& 
 3&%
2")&
'@<fi#
!) &
)d)[
[) !'@
 :	=fi!
) ) 76
 #	ff$&
!_9m
)  #
fi#fi#9$)M&%5
'B	ff@	

&
Jd 	@fi#
M
B	ff<C


 	ff'@
2 #
fi#fi9$'	I![!) &
)d&%
M
	d15	 $ fi#9fi!
) ) 76Q)
&%
MO%9'B#ff%X1S5Aff<	)X	?&%c 
fi= fi#C  %
\)&
5#)X
Aff<	
 	:&%M 
fio
) YR#B
m
<<S
B
)B&%	ffff%hJ)B!	ff > fi#9mfi
1fi#0C5FV295L^) #	ff8!
!'@<$	8
&
hL^
fi##$9[#)M%	IU 	?#) )O%Aff<	)@b$	ff'	)C=9	ffpY/5
3

 
<!V)ts$uvvwxR
<<$	
H%J) )E
@!;	ff '@
2 #	ff@>$#	ffK 	t'
)&$
J)&
)E$9<#
fi#$9p

$9<
2fi*!) O
)G
$"O%[<$)  m 	5
3%'@
8A<$@	5  'B!MUW%O%B&%9m
$
'B#) fi!
1Sfi	@A< #	ff)C  	U*YE&%9	 3&%
M1
) 5&%!d'&%	ff#)X
[	`bfi!
'B&%	=#n)&i0$)g.$	'	ff$$!Oi0 )CnV

=R	)&%#
@s$uvvSYuvvwxp$
 B
E'B&%	B&%

fi#
 )R
fi#-
2 #	ff)/
=A< #	ff)R) <
 
 fi#9t19='@
!&
!!=
W>	ff$=	2^&%o	ff $fi9B

!	ff $fi9Kfi!
) ) 76:<)E!K&%!t$#	ffK	2
2H%K) 	ff$@A
'@<fi#CPDE%f'H%
#)&'
;	ff0#) )O%@	#) ob$	ff'A< #	ff)R#)/1
) B	ff4
W)z`b)&<76M<
 
' YUW%#%B#)N)
 	)&>E&%
R
%4)  	ff>t)O
'@<fi#)/fi!
)) 76
 #	ffK 
 P#)/)&K fi#9"%#ff%pCRDg	=	ffRV	UEfi#Y
&%=
<<>	ff
%M%
)E	fffi#9d1 )  M	M
$ 76!
fin
&
) )C


fi

n+o+pqrbsm,Xn+rpq

e>#
)&
pY5fi	ff@
MP
!5s$uvvxR) f
B!+	ff'@
 #	ffJ&%	ff$
<<$	
H%J 	4 
A< #	ff).>	ff'	#) B$!"&%4	ff) & #	ffM	/
Bfi#	#
fin&%	ff>9C/DE%!'B	 #
 #	ffM#)&%

&%$#)W	@'B%
#)&'19@UW%#H%M
B	ff`_'	ff	 	ff#fi#
 !")& 
 9t
"$fi#!
1fi#9M#)  !ff#)&%
& A< #	ff)b$	ff'	#) Co3&%	ff)1
) d	Kfi	) 3U*	$")O<!
fi##-
 #	ff[s;P
!:5fi#O`
 	ffpYuvvSuxp	z6n&%/
2&
CDg	) fi#R&%EAffnfi!
) P 	
B 	E&%P $n&%	ff$9Y&%9)fi
&%	ff&%
o	ff $)*O%='B	) o $	ff$)=sO%9B+	"'@<!$#
fi#fi#9dO%
P&%f'	ff$$	ff1) E'B&%	
&%
=
2fi)	M	ff) #$)=O%@	ff'@<fi#A$93	P&%Bfi
) "	)=	!'@<
B$)&fi# )&xCtDg	d
$) )4&%
<$	ff1fi#'&%
P&%=1S) Efi!
) B'@
a9"	W<$	=
K!'@'B!
=!$
) 4!M	ff'@<>) ) #	ffpY&%9
	ff !ff4 	@
dfi!
))YUE
# !@	"'@
V&%B<
 r6
fi #fi0&%9K	ff1&
!:
B	ff'@<$)) #	ffpC
DE%#)P
J	ffE
I+ )  
2fifi!
) )E%
a1M
0C/y+	ff'@<>) ) #	ffKo	ff'B)Y&%J&%
fi!
) 5s;
5)O1) L^Bfi!
) )&xB
$K	t
8 	:&%@&%	ff>9CGDT%B'BO%	ff5#)@

fi#		ff)
 	M<$O`_< !3	/#) #	ffc&$)C=yz3&%!A<$!'B )Yn&%9:a]$ ?
	ff'fi
2) ) 76
 #	ff
	#) dsfi!!5 Z 
!$0YRuvxP!	"&%4
&
CDE%#)#) #
fiR 	K	ff'B&%	:+	ffT!a]_
	#) T+	E;RK#OOC0l	ff/'Bfi# #fi!
) )E
))Y	ffRA<$!'B&
fi0'BO%	ffK!a]_ )E	#) !K&%
'@
&%
E#EUP	fffi#X
2& 
fi#fi#9M	ff&%=	'@
!5s) =e #	ffffxC
4
'B1$/
 Z 
a 
t
 s$uvvwx/
M=
't1S$Y Z 
a 
W
 
Kq"- $	)OVRs$uvvwx/%
afi7`
	ff<S"
'B&%	B;	ff%
fi!@	#) &%
g6$) /$'	I)R	) #)  /A
'@<fi#)Rb$	ff'Q&%P&
!

&
SCtyz	ff) #)  =A
'@<fi#)t
$B&%	) B&%
=%
aB&%J)&
'BBI
2fi)r+	&%B;
&$)=17`
;$fi!
) )4fi
1fi#)CBDE%9d&%:&
)$;	ff '&%4;
&$)!	d
"1!
$9:;
&$4) CTAff&%9
A
'B!MUW%#H%8)t	2A
'@<fi#)YoUW%8$'B	0YR$)@O%M 	&
fi/'B14fi##  
fi#)M
 	3$&
!c&%G<>	ff<$$93&%
4&%@ >B) 4	E)&
)@#)@	=!	ff)) aCXDE%98%
"

) z`b)@&%$)&%	fimO%
@'B	ff# 	ff$)@%	U1#5&%#)@A
'@<fi#M) @)&%	fi[1SCm#8$UP	d)  )B	
A
'@<fi#)E&%
P$)&fi#E!M
JL^
2fi> #	ffK!K&%f
'B	ff/	fi##  
2fi)aYUPWUP	fffi#Kfi!V4 	=)fi
&%)&'J
fi#fi1
) :	ffK&%=%$#) B&%
2*##)f'B	$Wfi#!Vfi#9d	"1B	#) C
 %
	=
"T#)&%#
Gs>uvvxg
fiUE#&%@
$fi!
 J))&EB&%W<$	1fi'(		)W!+
O$E'B
a`
)&$' )aCNDT%o
<<$	ff
2H%4A )Rb--9@fi#	#)P
<<$	ff
%4 	=$<$)&
2 #	ff@
B
fi#fi!
 #	ff
	E!
 
@
&
CKDE%93# 7;9?!

 @
O
M	ff3O%"1
) #)=	2L
fi##&
 #K	ff >fi
2 #	ff)

'B	f$fi!
@
&
1
) @	=O%*	1) $I
2 #	ff4&%
R) 	ff'R;
&$)/
>*L
fi##&
 #fi#9B<
)&%"
2)/) 9'@< 	ff'J
 #E
&
4$O !"
4<
 #)*#) 
2) CRl	RA
'@<fi#W7/=3uT	ffo	M) 9'@<`
 	ff')!#
 &%
R
<
 #/%
)/
<
$ #fi!
n#) 
)Y&%BUPE'B#ff%/1fi##&%
n&%PI
fi!E	
J) 9^'J< 	ff'U
2)E!	ff $ fi#95'B
)O$d	ffT $0CDE%!'B&%	39^
'
2fifi#93  ')
b--9@! >I
fi#)P;	ffo!
 
 
&
B
M$L!$)E&%
o&%9"%
a	ff'@
!KVff	UEfi#	=#ff#
&%P;
&>)/!	t) )/UW%	) W'B'B1>)*
$TL^
fi##&
fi9K<C  %"	4	ff'@
!KVff	UEfi7`
4)=
a
#fi
1fiYpO%9M)&) ) !d
Bb--9Mfi#	#B) 9ff) '&%
%
)W<>  'B!5! $I
2fi)
;	ffP&%;
&$)aC
e 
fi$fi#	ff<' )R%
aRff$
2 fi#9f%fi!<BUE#&%rfi#
 !WA< #	ff)R!4&%nb

	*	#) 9:
&
CPq# $#H%3
5P
V>s$uavvxofi#	ff<S?
"'&%	ff:;	ffTfi
 !Mfi
2) ) 76$);	ff
'Bfi# !<fifi!
) ) )R!BU%%B >	ffz`b	ff > !	ff&<R	)/
$P'@<fi#	9@
)/
#) &$!1 @	&<
$<$)&
2 #	ff?s
2H%Kfi!
) )#)f
2) ) #ffX
JL@1!
$9M) &>K	Rfi#&%dpxCPDE%9M#fi#fi!) & 

&%
Kfi!
) ) 76
	
1SX#UP
)d
8	ff'@'B#
 #	ff<$	1fi'!UW%#H%&%3 #$9[	
&%K	ff $B	ffO<@fi!
) )4+	t
3UA
'@<fi#M#)"1S & 
)O'B#  3	=
3	#) 98H%
fi+C
d'J<>
2fi/I
2fi
2 #	ff3'B	ff)& 
 :&%
 $	ff_`b	ff $ !K	ff)
X1S") 3 	K!'@<$	
<_+	ff'@
Cnf	&%P$o!	I
 #	ffK#)E1		)  !ds;e%
<!$YpuvvSff!fi!
pY0uvvwxYUW%#%
;	ff 'B)=
K) 4	*fi!
)) 76$)BUW%	) "<>	)
$@	ff'B1!19d	 !C"o		)  !d
] )  )4&%
UP#ff%)/	p&%WO 
!!=)&
'J<fi)E
R
2H%J#  
 #	ffpY<
a9ff!t'	ff$W
 	B 	)&
'J<fi)P&%
2/
$


fi

prv+hsvc%q+mrqhrpm6n++v+v5SIb

 7"fi#=	"fi#
 pCK<	  !
fiR<$	ff1fi#'UE#&%:&%) @'BO%	ff)#)&%
&%9d'@
a9M
 
fi!
) ) 76$)o&%
/%
aE1t6/ 	) 9)  '@
E	#) CX	$	Ya&%#)/) #&
	t'J
9B1E7"fi#P 	
 CDo9^<#
fi#fi#9Y0 	KI
2fi#

K<
$fi!
fi!
)) 76	ffE'&%	ff0Yp	=	)=
@$	) )$`bI
2fi#
	
	
@) 	/fi!
1Sfic
&
Cy
2fifin	P&%=
O
@UE
)Efi!
1Sfi#19M&%B)&
'BB'B%
#)&'&%:&%
 !$W
2&
) o	ffO
!)*O%E)&
'B) 9)  '@
2 #E $	ff>)CnW%#ff!"%#ff%K
 
9J	ff@O%)P$9<E	
 !< : ) E) aY'@
a9d'B
M&%
T&%t'BO%	ff5%
)	fft
MAfifi#E]$	ff1d
2P6  !M 	
&%) 9)  'J
 #=	#) C
yz8&%d<
) J
Y/O%d	ff'@<O
 #	ff
fiE'@
%!dfi#
 !8	ff'@'B#$9%
)J) 
 
I
$!
 #	ff)E	
 [fi#
K&%
P'B	fi0&%_9<	2N	#) &%
E'B#ff%P	ffM
B$
2fifi#

ff!$	ff'"sfi!!5 Z 
2>0YRuv0efi#	ff
pYRuvq
&YRuvvwxCE3	ff$B$fi9Yn'B	fi)
	W	ff`_7;	ff 'fi!
) )j6
2 #	ffm	#) 5s;efi#	ff
pYPuvvx=
?<
$ !
fi*	`_j;	ff ' 	#) hs+q
&Y
uvvxP%
f1SM!O$	ff0CPDE%) B'B	fi)	"	E
))&'B&%
P
%K!) &
B%
)E&%)O
'B
'B#) fi!
) )j6
2 #	ff3 
 
K&%$O;	ff$=
$'B	ff>f$
2fi#) B'B	fi)	O%_9<)P	/	#) 	ff1)$
!m$
fi7`bUP	ff$fi#m
<<fi#
2 #	ff)C3$t!	I
2 #	ff8#)t	X
fi# 4&%@fi#
8<$	ff>K+	ffr&%
V	IU"	#) =
 )=sq
&YuavvxC  	UPo!M'B	) E$
2fij`bUP	ff>fiM)
$#	)P	ffUE#fi#fi	E%
a

) )P 	B&%& 	#) = 
2 )/	n&%W
$#	ff)Efi!
)) )C/yz@&%)
) )Yq
&s$uvvxR)&)  )
) 
>H%!B;	ffo&%	#) = 
 ) !"
4$	))$`bI
fi##
 #	ff) 
$%pY1E&%#)E
<<$	ff
2H%K
) )&')E&%

	ff=%
)	#) b$=
2&
4UE#&%MUW%#%M 	B
fi!
 =O%t$)&fi# )T	NO%) 
$%pC

4

   




p 

	





DE%#)P)  #	ffK) >1S)W
4 
fip<$	$+	R#j;9K'B#) fi!
1fi#3!) &
)P!M
&
!
) CRDE%6>) P)  <K#)E 	B#j;9M
#
 !) O
)W19G) !ff
fi#
 !M
fi#	ff$#&%'B)Bs
fi#fi#
*#;aM ffOfi= xB 	c&
5!) &
)d
)@	ff > fi#9m	ffB!	ff $fi9\fi!
1fi#0CD0	c&%#)M0YE
?S`
;	fi#?$	) )$`bI
2fi#
	5#)@<z;	ff '5	&%J& 
!!3
&
CJl	ffr
H%h	E&%"[<
$ )Yg&%ff


fi#	ff>O%'B)*
>E& 
!K	ffJ&%E	&%o=3u<
$ )CRDE%
F$)OfiBfi!
) )j6>)W
$EO%") K 	
&
P
%!) O
/!O%/Afi@<
>
)0#&%g	ff $g	ff0'B#) fi!
1Sfi0Cfr##
fifi!
) )j6
&
)f
h)&
"
)=')fi
1fi#m7#4fi
2) ) 76)=&%J)&
"
)=1Sfi#	ff!3 	d
K7i>4fi
2) )
&%
c&%
4#8195# )B&
!cfi
1fi;C?T	 @O%
BUW%? #)@L^
2fi/ 	3&%K 	&
2fi/ff'B14	
& 
2!4)&
)Y&%#)/'B&%	t7i0$)nb$	ff'  #)&1>)fs>uvx	ff fi##R  #	ffB'BO%	ffB	fffi#9
!M&%) W) : 	B  'B!BUW%&%E

) #)f
K	ff fi##C
T=&%Km	W&%MS`;+	fim>	) )$`b
fi#
2 #	ffm
%c)&
M!8&%K& 
!!5
&
3%
)@1S
&
0
C T) !M&%#)!+	ff'@
 #	ffpY&%B) 	ff:)  <:#) 	B;	ff '
@fi!
) )j64) !d
"U$) #	ff
	&%dO 
!!?
O
:+	BUW%#H%\
fi#fiW	2f&%M!) O
)M# 76
)M'B#) fi!
1fi#
$d$'B	0C
lR#fi# $!?
?1Sd1
) m	ff8	ffK	ffB'B	ff>"	&%
 
 1
2) Mfi#fiPfi
2) ) 76$)o&
)aCXDE%K6fi# >
) E	2N&
!K!) &
)#)W<>	I#5
)E!<W	@&%EPp#O&" & fiBCPDE%t$)Ofi
fi!
) ) 76n#)n&%/@<$		2^&%P
<<$	ff
2H%pCglR$WuR<# )RO%/ 
2fiff<$	$C/e<76
!'@<fi#'B&
 #	ff)B	P&%#)t 
fi/<$	$Kji0rc%	IU&%B6fi# $!3#)t<Sz+	 'B0Y
5!
&%=$fi!
	)&%!<X1S$U*&%6fi# 
fi#	ff$#&%'ds+)&xP
d&%6
2fi0fi#
 !G
2fi	ff$#&%'ds)OxC

 e X]ajdWagE` "!

#

Ca j+`

$

 d
<<$	
H%h)B 	3) K&%")O
'BKfi
 !?
fi#	ff$#&%'k 	d	) & @1	&%5O%@6fi# B
8&%

6
fifi
2) ) 76CPDE%#)*
<<$	
H%)E'B	) /) !'B#fi!
o 	B$'B	ff!4	ff fi##$)PK$ff>) ) #	ffK

fi#9ff))aY;	ff
UW%#%K&%)&
'B'B	fip)) M 	B ) o;	ffo	ff fi##$)W
K;	ff6  !B&%6
fi0'B	fip 	B&%
&

	ff&%E	fffi#$)E%
aW1S">'B	0C0>fi
2 "'B&%	K)o&%
P<$	ff<	) "19&%	ff%Xs>uvvxg;	ff
('

fi

)

Training
Instances

Filter

n+o+pqrbsm,Xn+rpq

Correctly
Labeled
Training
Instances

Learning
Algorithm

Classifier

lR#ff$Ku+*RDE%= 
fi0<$	$4+	ffofi#!'B!
 !X')fi
1fi#3)&
)C
$'B	!"&%4& 
!!M!) &
)O%
W
$B< 519dCjds;!fi!
pYouvvxC*e<Sj6
2fifi#9Y0;	ff

%@fi#
a/	KO%f< MO$E;	ffoUW%#%K& 
!!@!) &
)
$	ff1) $Jb$	ff','	ff$&%

	ff@fi!
)),
Y %	%p)='B&%	5fi#!'B!
 )B&%	) B!) &
)tO%
f
$"	b$	ff'&%"'@
2]_	$#_9Mfi!
))C
DE%@O$"#)4&%?>1fi#4b$	ff'O%"$m) 4	*&
!3!) &
)CdDE%#)@<$	) )=#  
2 )
 #fi/	@b$&%< !d
:1B	ffCDE%@V9M7i0$"1S_UP	ff'B&%	X
 %	%p)
#)P&%
R	ffo'B&%	G) )P
$	) )$`bI
2fi#
	K	IR&%EO 
!!t
2&
UE#&%J	ffE#  
	@UW%>
)
%	ff%p)W'B&%	d
fi#)UE#&%K&%&
!KA
'@<fi#)W!$fi93
d<_+	ff'B)W'Bfi# !<fiB#  
2 #	ff)C
Q) 	ff:UE
9K 	@!'@<fi#'B6fi# $!M#)W	@	ff) &W
B6fi# ) !K	fft
fi#	$#&%'F
d 	
	ff) O =&%J6
fiofi
2) ) 76@) !5
:ji0$t
2fi	ff$#&%'MCdDT%G
))&'@< #	ff8$fi#9ff!8&%#)

<<$	
H%#)=&%
) 	ff't
fi#	ff>O%'B)=

)		ff6fi$)+		&%
fi#	ff$#&%'B)aY0't%Mfi#!V@)	ff'B

fi#	ff>O%'B)f
2f
)T	ff	M;
&>=) fi# #	ff5'&%	ff)T;	ffP	&%>)s;T
$#YuavvxCPDE%B
<<$	ff
%
) $!1Sh19  #fi#) 	ff8s$uvxR 	46fi# $!K
&
r;	ffE
KuO`_) !M
K `_)
KA^
'@<fi#=	R&%#)

<<$	
H%pC
c 0$-j1!-2'aj
 #e . /



Ca j `

$

3

)'t1fi#Pfi!
) ) 76$)R	't1!P&%o	ff&< )0	2
P) n	S1
2) O`bfi#fifi!
) ) 76$)Ws  
) 4[e
fi!
'	ffpY
u vvoV ) )	ffMeUE
!pYuvv  	fi!<>YpuvvxCp'@
]$	ff$#$9=	 P) 'B1fi#fi
2) ) 76oUE#fifi
	ff&<Sz;	ff '
2H%K1
) O`bfi#fipfi!
) ) 76P	@
4
&
) R7$UP	=		)%	fi,*s$uxRO%f<$	1
1#fi#$9
	/
4	ff $Efi!
)) 76
 #	ff319K
%K!#
fiRfi!
) ) 76T#)Wff>
 o&%
MC7t
?s;xP&%$	ff$)
!d<$# #	ff)	2N&%=1
2) O`bfi#fifi!
)) 76$)f
$!<"s  
) de
fi!
'	ffpY0uvvxC
yzK6fi# $!Y0
M) 'B1fi#Bfi
2) ) 76  )='B#) fi!
1Sfi#5!) &
)=19M	ff) & !G
J) E	
4H6 5_#( 7a, 8;O;sfi!
)) 76$)&x=
XO%X):&%!fi!
) )j6
2 #	ff3 $	$)W	M 7;9h')fi
I`
1fi#\)&
)C5DE%K 
2fi
<<$	ff
%5#)B 	3&
3
c)&
d
)B'B#) fi!
1fi#7:
 98	W&%;
 

1
) O`bfi#fiTfi
2) ) 76$)K
	Bfi!
) ) 7;9mJ	ff $ fi#9C8yb8&%#)@UP	ff VhU*KA
'B!X1S	&%8'@
]$	ff$#$9

8	ff) )&)46fi# $)aCd'@
2]_	ff>$93	B6fi# 4&
)=
c!) &
M
)@'B#) fi!
1Sfi\7f'B	ff$J&%

%
fi7R	&%<
 
1
)fifi0fi!
) ) 76$)fi!
) ) 7;9"#E!	ff > fi#9C/Q	ff) )&)6fiW$L!$)E&%
2f
1
) O`bfi#fin  	$)W'B) Pb
#fi	@fi!
) ) 7;9d
M!) &
t
)EO%=fi!
) )#d19K)W&
!Kfi!
1fi
;	ffP#E 	"1Sfi#'
2 3b$	ff'&%& 
2!K
&
C
y.=#)B!'@<	ff>&
t	X	 J&%
=O%G$fi#9!?<$'B#) M	W
c) 'B1fi#K6fi# 4ji0$)4b$	ff'
'B&%	)Bfi#	ff<m!8$ff$)) #	ff?

fi#9ff) #)Y8U%%c	ff fi##$)@
$KO6[$fi!
 #K 	X
:<
z`
 #fi!
'B	fi;C  >tUPB
) )&'B4&%
) 	ff'B4!) &
)!3&%4
&
K%
at1S5'B#) fi!
1fi#m

&%
T&%=fi!
1Sfi >	ff$)W
><S4	/&%B<
$ #fi!
'B	fffin1!K6W	@&%=
2&
CPDE%$O`
;	ff$E	fi#fi# !K;	ff 'J
 #	ff.$	'7i0$'B	fffi#)PUE#fi#fi<$	
B1 *'&%	ffJ;	ffR 
'B#) fi!
1Sfic)&
)&%
K	fi#fi# !M!+	ff'@
 #	ffJb$	ff'
4) !fi#@'B	fi;C
e
$#_9:	/& 
2!:
&
J)=
M<>	ff1fi#'!3'@
9dfi!
) ) 76
	5
3fi#
 !X<$	1fi'k	`
'@
!)Xs+CC!YR'B#
fiP!
ff	))OxC5l	ff4)&H%8
2&
)  )aYUPKUE
4 	5'B!!'B#-d&%d<$	1
1#fi#$9
(=

fi

prv+hsvc%q+mrqhrpm6n++v+v5SIb

>

? >

? >

Discarded

Mislabeled

>

? >
>

E1

? >

? >

D M

? >

E2

@ Training
Instances

lR#ff>tA*RDo9^<S)W	2  #	ff $	ff$)
	P)
$!X
:!) &
@&%
#)=
:Aff< #	ffc 
&%&%
X
M $	C=yb0Ynq=
9fiV:

/>	I	) ts$uavvx	 B&%
fi#
 !:.>	ff'F	)9X
2&
@#)4jKfi@1S
) @##)t%
>X	"#)  !`
ff#)&%M1S_UPM	#) =
MAff< #	ff)aY)&<!
fi#fi#9d7R&%	#) #)E) 9)  '@
Coy;
fi#fi#9YO%f1!
) )
	*
fi#
)	ffB	/&%Bfi#
 !X
fi#	$#&%'B)UE#fi#fi/
1fi#@# 	Kfi#
 :&%BA< #	ffpCtDT%$O+	$Y
	ffE	R'B	ff$E	2&%Efi!
)) 76$)P&%
/	'@<$#) E&%1
) O`bfi#fip) R	0  	$)R
@%
aE7"fi#$9

<&>B
W<
$ #fi!
RA< #	ffBUE#&%	R
) !=&%PA< #	ffB 	=1SP $	ff	ff)fi9Bfi#!'B!
 
b$	ff'O%W&
!B
O
CRybJ&%#)*
2) Y&%W	) )&)E'B&%	"UTfi#fi'J
VP;UPR  #	ffK$	ff$)
&%
4
'@
]$	ff$#$9	ff0)fiE
fi#	$#&%''B&%	0CRD
V!=
E	) )&)/ 
O%0&%
=
'@
2]_	ff>$9	 
#)*
4'B	ff$P	ff) >I
 #
<<$	
H%B
JUE#fifi0$)Ofio!B;UPn)&
)E1!Bfi#'
2 K.>	ff'Q&%
& 
2!K
&
CoDE%= 
aUW1
2HVJ	/
B	ff) $
 #B
<<$	ff
%M#)&%=
3$#)&VK	/$&
!!d1


&
SCGyz8&%"AB)  #	ffcU*M

fi#9-"O%G<$	1
1#fi# #)@	2f'@
V!d#j6
2 #	ffm >	ff$);	ff
1	&%M$&
!!M1
d
O
t
d&%$	UE!@
aUE
9J	ff	M
&
t+	ffP'@
2]_	$#_9K
M	ff) )&)T6fi# $)C

 e
 B ij1CEDGFb_Ag,

c`b`gE`

$

yb 7;9ff!@'B#) fi!
1fi#K!) &
)o&%$E
$P$UP	E$9^<S)R	 >	ff0O%
R
B1W'J
=s) PlR#ff$
xC:DE%@6>) =$9<5sHKux	ff>)tU%m
3!) &
M#)B!	ff $fi98&
?
)B'B#) fi!
1fi#

#)")O1) L^ fi#9m#) 
$sq=xCRDE%M) 	[$9^<SM	 $	ffMsHBx4	$)@UW%\
X')fi
1fi#
!) &
5s5x#)B&
5
)4	ff $fi93fi!
1Sfi0C3ybc&%#)=) #	ff5UP"

fi#9ff-K&%"<$	1
1#fi#$95	

%K	R&%) $9^<S)W	2 $	ff>)P+	/O%=	ff) )O)f
d'J
]$	ff$#$9B6fi# E'B&%	)C

IKJIKJLNMPORQTSRUVWXY<SZW0[
DE%E	R!	ff $fi9MO
!"
B	 $P!) &
B
)W'B#) fi!
1Sfi8%
<<)UW%M'B	ff>&%

%
fi7=	=&% 
 1
)O`bfifiW 	ff$)J.
#fiE	?fi!
) )j;9m&%:)&
3	ff $fi9C Z \@sHKu^]x@1S
&%@<$	ff1
1fi##$93&%
fi
2) ) 76<_P'@
V2)f
-HKu >	ffW
d;	ffT&%B)&
V4	/fi!
$#$9X
))&'BB&%


fi#`
fi 
1
)O`bfifi/fi!
) ) 76$)@%
a@O%t)O
'B"<$	1
1#fi#$95	E'@
V!X
-HKuB $	ffO%
#)=L
fi

	 \@s HK^u ];xCKyEUP"
) )&'@&%
4&%@ $	$)	E&%"1
)O`bfifi*fi!
)) 76$)@
$@!<Yo&%
<$	ff1
1fi##$9M&%
W
J'@
]$	ff$#$9@	E6fi# TUE#fi#fi&%>	IU	ffT	ff	M
&
4#)#d19K*

\@sHKuxa

bdf c1e \@sHKu ] x
bTgAe`h

7

b

s>u*P\@siH"u ] x x
(p

ekjb:l


 mon

fin+o+pqrbsm,Xn+rpq



%$q\@siH"u6]x s$uB	\@sHKu^]x x


b


 mrn

ekjb l

><$)  )"O%d%
3	

m

 >	ff$)M
'B	ff5O%q


1
) O`bfi#fiTfi
2) ) 76$)Cmy&%d<$	ff1
1#fi##$9?	'@
V8
sHKuJ $	ff4#)@fi#) )@&%
?C7Yn&%8&%
'@
2]_	ff>$9B6fi# UTfi#fiR'@
V;UPE$	ff$)EO%
M
@)fiO`_
2fi	ff$#&%'6fi# +	ff'BMb$	ff'	=	R&%
1
) O`bfi#finfi!
) ) 76$)aC
DT%f<$	1
1#fi#$9@	R'B#) &
V!
4'B#) fi!
1fi#:)&
;	ff/
4	ff > fi#9tfi!
1Sfi#d!) &
"is Htx
	$)U%t'	ff$R&%
4%
fi7	&%P1
) O`bfi#fifi!
) ) 76$)Rfi!
)) 7+94&%/!) &

2)0&%*')fi
1fi#
fi!
) )C 7 Z &
 \@is Htt ]x1K&%"<$	1
1#fi#$95&%
2=
M1
) O`bfi#fiP  	ffu
 _'@
V2)=
: $	ffr	*$9<
HBCP) )&'B!K&%
&%= >	ff$)
$!<@
d&%
2W&%B<$	ff1
1#fi## #)=	/O%t1
) O`bfi#fi
fi!
) ) 76$)K'@
V5
v
 Ht $	ff4
$@O%")&
'BYn&%5O%G<$	1
1#fi#$98&%
=O%"'@
]$	ff$#$93	 
6fi# '@
V)E
B$9<<
 HBB $	/#)#M19

\@sHBxa

db f c1e \@sHB ];x
bTgAe`h

7

b

s>u*P\@siHtt]+x x

ekjb:l


 mon

DE%$O;	ff$Yp
@'@
2]_	$#_9K	6fiEUE#fi#fi/'@
V2;U*wHt $	ff$)T&%
d
J) !fi#O`_
fi#	ff>O%'6fiE7
\@s HB ]x4#)Bfi#) )B&%
8CjC  %?
2fifiE1
)O`bfifi*fi!
)) 76$)GHBM$	ff$)B
$"'@
2"	ffh&%")O
'B
)&1) o	0&%)&
)Y&%W<>	ff1
1#fi#$9@O%
*
4'@
]$	ff$#$9=	 Pfi
2) ) 76PUE#fi#fi0'@
VE
x
 HB $	ff
#)W# #
fin 	B&%t<$	1
1#fi#$9M&%

=)fiO`_
2fi	ff$#&%'6fi# TUE#fifiR'@
V2f
 $	ffC

IKJIKJy{zS"|,}~[A|,}(,}xVTW0[AU}
l	/
	) )&)P6fi# Y
xK
H uP $	ff	ff>)*U%"
fi#fip	0&%1
) O`bfi#fip  	$)o.
#fi 	=fi!
) )j;9

B!) O
W	 $ fi#9C Z k@
\ sK
H u^] x/1&%W<>	ff1
1#fi#$9K&%
/1
)O`bfifi 	ffGn_ '@
V2)*
K
H u
 $	/
;

 1&%ff'B1S/	2g1
2) O`bfi#fipfi
2) ) 76$)YO%@&% 
2fi;	ff '	2O%f<$	1
1#fi#$9
	/'@
V!"
;K
H u $	ffP#)#M19K*
\@sHKuxa\@siH"u # dx \@siH"u 
x #\@sHKu^  HKu 0
7  HKu # ^
# v + HKu

ekj

#

x

y R&%1
) O`bfi#fin  	ff>)'J
V $	ff$)P	ffJ&%)&
'BW!) O
)E&%K&%=<>	ff1
1#fi#$9M	R
ffHKu
 $	#)BL^
fio 	M&%M<$	ff1
1#fi##$95&%
B
M) !fi#d1
) O`bfi#fiP 	ff='@
V)t
h $	ff\@sHKu^]xC
 H"uT $	ff>)/	O%f1
)O`bfifi0  	ff>)
$W!<SY&%M
4	ff) )O)/6fi# T%
)
 %K&%u

K)&'@
2fifi#B<$	ff1
1fi##$95	E'@
V3
 HKuB $	ff&%
3
%d	2# )=1
2) O`bfi#fiP  	ff$)=
5&%
<$	ff1
1fi##$9M	/'@
VM
;
 HKu $	ffo#)#d1K
9 *

\@siH"uaxRa  e \@siH"u6]x
] #
c

y P&%@
) )&'J< #	ff3	/!<aM	/&%@$	ff$)	/&%"1
2) O`bfi#fiR  	ff>)=%	fi#)Y0&%:UP
UP	fffi#MA^<SW
B	ff))&)P6fi# P 	@%
a=
4)&'@
2fifi#<$	ff1
1#fi##$9"	/'@
V!"
xHKuW$	ffo&%
K

) !fi#O`_
fi#	ff>O%'6fiC

1 RIoRjjzjO;/$O;&T*; ;jW$  $zRI= R     IR&;j$&zzj&;zb$Gp_$I;o a
  jjo $/o
 z;b;o2RIzfPbRII    nIO;nj$&zzj&;zbR*R <Pz;b  j

jp EaIabb$;jPO;  jjzjO;0$&;On&;gIO$0$o;j$ IjjpjIO;_0b$aj;
  z;_j$R$ ;b$R&;j$&zbjO;Izb$Inab  a=   &oz;b  IRp*zjO;R$&;
 IRjjzj&;o$&;RbR$&o;$IR;O*Rp O
(

fi

prv+hsvc%q+mrqhrpm6n++v+v5SIb

DT%f<$	1
1#fi#$9@	R'B#) &
V!
4'B#) fi!
1fi#:)&
;	ff/
4	ff > fi#9tfi!
1Sfi#d!) &
"siHtx
	$)EUW%:
K!) &
4)'B#) fi!
1fi#?
M		ffE'B	ff>W	R&%=1
)O`bfififi!
) )j6>)f<$# )
&%@'B#) fi!
1Sfi8fi!
) )C Z <\@sHB ]x1B&%@<$	ff1
1#fi##$9XO%
=
"1
2) O`bfi#fi/ 	ff_T'@
V)

 $	P	$9<&
 HBCg	ff) )&)T6fi# E'@
V)W
B$9<<HB4 $	ffP7R	ff=	*'	ff$	R&%=1
) O`bfi#fi
fi!
) ) 76$)K'@
V)=
_9<;
 HBK $	ffCGDT%)@<$	1
1#fi#$95#)BL^
2fi/ 	:	ff"'B!)tO%G<$	1
1#fi#$9
&%
=	t	2*&%@1
) O`bfi#fio  	ff>)='@
V)=
 HBJ $	ffCByus
 \@s HB ];x#)&%"<$	1
1#fi#$9
&%
Bfi!
) )j6x
 _	)"	K'@
VM

 HB: $	ffY&%m&%d<>	ff1
1#fi#$9m&%
2
h	ff) )&)B6fi# 
'@
V2)W
;
 HB4 $	ffo#)Wd1K
9 *

\@siHtxRauTs$uPs\@siHt
#

x xs$uP\@sHB
7

 HB
#

x x^#s$u/P\@siHt

e

 Ht #  ## HB

ewj

#

xx

% 5O%G<$	1
1#fi#$9m	W
31
) O`bfi#fiPfi!
) ) 76@'J
V!?
PHBM $	=#)B!<"	2W&%

<$	ff1
1fi##$9M	R&%=	O%*1
2) O`bfi#fifi!
)) 76$)f'J
V!M
;HB4 $	ffPO%)1	ff'B)~*


\@sHBxa(uE  e s$uEP\@sHB ]xx
] #
DE%$O;	ff$Y!B!$P	ffO 
) n 	<K
H uP $	ff>)Y!c <=	2&%HB ] $	ff>)R
Bfi#
@ 	=%#ff%
	 
fi#fit
H r $	ffN+	/O%W	ff))&)o6fiC/ybK)OH%J
) )Y
4) !fi#O`_
fi#	ff>O%'6fi# oUP	fffi#G'J
V
;U*xB
H 3 $	$)@&%
\
5	) )&)K6fi# J&%
K	ffO
!)"&%:) !fi#?
fi#	ff>O%' 
)K	ff:	=# )
1
) O`bfi#finfi!
) ) 76$)aC

 Ee 


$ 12

 a_ <j aj+i

B 0$6_0Fj$Ej ` $ ,$HcwRFbjAZgZ0$

 O+	$?'B	m	 	[

fi!
 #	ff	tO%?
<<$	ff
2H%pY&%3#) )&m'B) d1?
2$) ) Q&%

o
!) &
)T&
K
)E'B#) fi!
1fi#X19@&%
1S	If
<<>	ff
%B	fffi#d1A< #	ff)E 	B
4 
fip fi#

4&%$O;	ff$/UP	fffi#@=)O<!
fi&$
2&'BC  %=
!) &
P#)/
rAff< #	ff4 	T&%/
fi

) Y#/
B
<<S
/
)RO%	ffff%B#R#)/!	ff > fi#9@fi!
1fi#0C  %J
<<fi#9ff!B %L)P 	#j;9

3fi#!'B!
 M	#) 93)&
)Yg	ffBU
 ) 	M
a	#3)
$!3	ff $ fi#9:fi
1fi#?A< #	ff)C
yb:e #	ffdU*#) ))Eb&$=<fi!
)T+	Pfi#
 !"	@#)  !ff#)&%3	#) b$	ff'A< #	ff)C
) 	m) #&
 #	ffc!8UW%#H%8
c)&
d'B#ff%t1SM)
$m $		ff) fi#9?195	ff6fi# 

<<$	
H%K#)E7*
M
2fi	ff$#&%'UE#&%M
K
<<$	ff<$!
 4fi#
 !G1!
)T;	ffo&%
&
B)*#)) 0C/yz
)&%:
) )Y&%@
fi#	ff$#&%')f><$) &
 #	ff:fi!
ff
'J
9d	f<S 'B#=
3
 
t$<$)`
&
	d	2/&%B	ff<C=DE%#)B<$	ff1fi#'#)t

fi#		ff)W	M) #&
 #	ff)!3UW%#%3$'B	ff!M	fffi#$)
	)/fi##  fi#W	=!'@<$	P&%P6o	
f6$) $`b	$nfi#
*>ff$) ) #	ffj0&%T	ff $P'B	fffi	2O%E
&

#)=L
 
CrlR
2fifi#9Yn) !@&%46fi# 4
fi#	ff$#&%'ds+)&xE	) &  )=
Kfi!
)) 76=) !dO%t	$#!
fi
	#) 93
&
J) Yg&%B# 76
	5	E'B#) fi!
1fi#m!) &
)=#)t1S	ff5 	K!fiM$	ff$)0)

4fi!
) ) 76+	ff'B@b$	ff''B#) fi!
1Sfi#X!) O
)W	t 'B!7/	&%o!) &
)W
$f')fi
1fi#
UE#fi#fifi#
[ 	c) 	ff'B: $	ff$)aC  #&%mO%) d
a
)@!'0YPUP3	IU<$	ff 	8
m'@<!$#
fi
I
2fi
2 #	ffM	R&%=
<<>	ff
%pC

4

+






6 

D 	X
fi!
 K&%M
1#fi#$9m	E&%KI
>	)B6fi# $!?
<<$	
H%)4 	3# 7;9')fi
1fi#[&
!
g
!) &
)PUPE%	) E	'@
!)o+	ffnU%%Jfi
1fi#!" >	ffn	ff>)
2& 
fi#fi#9C/Dg	=) !'Bfi!
 &%E$9^<S)
	0 $	R&%
R	o!"< 
2 #YffUPE	)&fi# M	ff'@
!BA<> )o+	ff
%B
&
)/ 	# 7+9K&%
<
!$)R	pfi!
) ) )Rfi#!Vfi#9@ 	1E	.) 0CRDg	 ) R&%o6fi# $!B
<<$	ff
%UPE
$j6!
fi#fi#9@!&$	
(

fi

u





w


v

u
u u

n+o+pqrbsm,Xn+rpq

Pfi!
))WW
'B
1$	
fi#
aN$ff$J;	ff$) 
	ff7;$	ff)E$ff$G+	ff>) EUP	ff	fi!

%#ff%fi
2 #&=#	ff);	ff$) E(U*		fi

&

#	ff)$`b$ff$;	ff$) EUP		fffi!

UP		ffMff
) ) fi!

ff
) ) fi!

1
>ff$	ff
fi# #I
2 
1$	
fi#
aN#	ff);	ff$)(UP	ff	fi!

)&%1)W
X1
$ff$	ff

yb) &
)
w

uu


u
ff
vu

u
u

Dn
1fi#"u+* Z 
d	Pfi!
) ))
	#) M! 	3&%"O 
!!Xfi!
1Sfi#)G1S_UPc&%) M<
!$)B	Wfi!
) ))C  K[	=!O$	ffd	)
1$UPd
2fifin<
!$)W	2Nfi!
) ))f
)T&%#)WUP	fffi#X	W'B	fi0&%=$9<)E	2Nfi!
1Sfi!: $	ff$)T&%
E	
!3< 
 #CWTA<$!'B )=
$4) #ff3 	M
) ) ) )T&%=7i0$_9<)	n6fi# $)p
1#fi#$93 	
#j;9['B#) fi!
1Sfi\!) &
)M
8&%MOi0tO%
=fi#!'B!
 !['B#) fi!
1Sfi!) &
)"%
)B	ff
<$# #@
 
29C  ) $!1St	PA^<S$!'B&
fiR'B&%	M:e #	ffMCjC

'e{RgZ!

i0$

_

DE%#)W$)
$%@	ff>
2 K.>	ff'Oi	$ )*
2$) ) !K&%WO
)&VB	R
 	ff'@
2 @fi!
`b	E'@
<<
b$	ff')&
2 fi#fi# 4
&
CRyzd
<<fi#9d'J
H%!4fi
 !" %#L^)	@&%#)W<$	ff1fi#'UPfi#	ff<
&%P#
	)4	ff) )&)6fi# $)R 	$'B	E'B#) fi!
1SfiK& 
2!4)&
)CRP)&fi# ).>	ff'Q&%#)
UP	ff V
J1P;	ff@!ds;P$	fi#9@l$#fi;Ypuvvw
Yffuavvw1xC0Dg	=A<fi#	ff$EO%)oL^) #	ff4b$&%YUP
%	) P;	ffR
# #	ff
fi
&
) )n=	ffR%	#EUE
)/1
)@	ff@
o] ff'B/	pUW%&%&%Efi!
1fi#!
<$	) )E!fi!5)&1) O

2fifi#fi#)	n)&1]$ #ff#$9M	ffP	#) CPybMO%)T)  #	ffpYUP#j;9d%	U
fi!
1fi#!M $	$)W'@
a9"
>)=!M
%K	R&%6	ff'J
!)C

,JLAJL	<0W0S"wOAW0[A,O"|,zSR[AUMPORAAV|Z
DE%:6$) M
2&
) MUP3A^
'B!	ff) #)  )M	2
8 !'B5)$#)d	=fi#	ff1
2fifi#9#) O$!1 )O
 fi#fi#
	ff1) >I
 #	ff)	/&% 3 
$&%p)E)&zb
CEDE%4
&
) TU
2)E	ff'@<#fi#?19"qlp$#)=
XDg	UW)&%
s$uvvffxY
:fi!)@vBfi#	ff
	)W&%
2W	ff'J<
) )
fi#fiN'@
2]_	/ $) &>
2fiN1#	ff'B) , 
:fi

	P$9<)ts)=D
1fi#KuxC
DT%>'B	 o) ) !=	ff1)$I
	)*
$P'B
)&>'B)R	
<

'B g
fi#fi#"&%E	ff'@
fi##-
7i0$EO
 #	ff4!As.Eq  yzxCDE%#)RAJ)R	'@'B	fffi#9t)@ 	;RO%
'B	ff	pfi#
O
 #	ff@<$)PUE#&%!K
t<#Afi0
/&%T !'BW	2
2&
=
L#) # #	ffpCPDE%fTq  yn
O
) d%$
UP$B	fi#fi# 193&%BW
  $9  #ff%5) 	fi! #	ff5E
2	'B 	51	ff
$3&%"
 #	ff
fi

#t
"EO'B	)&<%$#'B!#) & 
	K) $#)E	N' 	ff$	fi#	
2fi)O
 fi#fi#)C/DE%
O
=%
a

a1PdE^GjIpjbO_;pa2jOj= 
  b$;;0Ia_;zb;$

I0;z;b$;;j _$O;;;$*$O*gOoj$0  jOP$b0b&;jIa$
 

fi

prv+hsvc%q+mrqhrpm6n++v+v5SIb

lR#ff>=A*/Dn 
!!G
M)  !@) # )C
1 \$O`b)&
'J<fi\b$	ff'&%3 
aU )O
 fi#fi#d
&
h 	?%
ad
37+	ff')&<
2 !
fiW$) 	fi	m		ff

ff$M	2Wfi!
 #&X
?fi#	ff#&C 3 
%5	Mff$d<#AfffiT)J) $!119m
: !'BM) $#)@	
$UPfi=Eq  yo
fi!)f
'B	ff&%fi#9M !'B=!$'B )T.>	ff'uvY
d19M# )Tfi
2 #&YU%%:

1E) Obfi+	ff)$!'B!
 !t
'	ffWfi!
)) )UTO%	&%>UE#) /) !'B#fi!
)&<&
fiff<$	ff<S$ #)WsqOb$#)
Dg	IU)&%0YpuvvffxCDE%='B	&%fi#9@'@<	ff
fip)&
'@<fi#!"<$	$=
fi#fi#	UE)P&%	ff'@<#fi!
 #	ffK	
fi#	ff`;b$Bff#UE)	R&% 3 
>&%p)P)&zb
Y
3
fi#) 	@
<O$)W)
) 	ff
fip9
'B#)!M&
 #	ffpC
DE%: '@<	 
fiE!;	ff '@
	m#)d<
$ #fi!
$fi#9) O.fiT;	ff@fi!
)) 76
 #	ff\	=O
 #	ffm
\fi

	Y
)E)
) 	ff
fip%
)E!&
	M
$	ff	R&%=1S) W!#
 	$)W	2N&
 #	ffJ$9^<SC
\)&'@'@
$9B	&%)E
&
r)P<$	ff#K!@D
1fiBuCRDE%Efi!
))/fi!
1fi#)EUP$P) fi# K 	B$O
b
!$fi#9d1$	ff
:fi
2) ) )UE#&%MA ) #4	ff 
<%4	 
Ch
<)Efi#	ff<h.$	',O%)fi!
)) 76`

 #	ff) H%'f'@
a94&%@1) K 	=$fi!
 Tfi
"	nfi!
) ))*	) & &
fi#fi9K
Bb #	ff
fi#fi#9
) #ff76
W	fi	#
fiN<>	ff<$)aC/lp$	ff'
@>'B	 ) )M<>)&< #Y&%#)fi
2) ) 76
 #	ff:) 9)$`
 '$O )@
	ff'@<$	ff')G1S_UPcfi!
) )Bfi!
1fi#)K&%
t
$@) <

1fi#K.$	'	ff
>) "$) 	fi	
$'B	 @) ) !3
&
SY0
?fi!
))=fi!
1fi#)@O%
=
$")O.fio 	d`_) >)t)OH%5
2)=	fi#	#) )CKl	ff
b$&%B&
2fi#)M$ff
>8&%M)&<S76h<>	ff$)M) \ 	3	ff'@<#fi#XO%d
&
SYR&%d$
)
$O; $M 	Jql$#)W
3D0	UW)O%?s$uvvx*
 Z 	)A
Y %)  #t
3DV=s>uvvffxC
Z 
1fi#!3 $	ffT	ff>)=!3fi!
`b	O 
!!d
&
;	ff'@
9X$
2) 	ff)C=J) 	ff$"
$#) )
1
) d#) $Mfi
2) ) )M
[1	ff
$#)G
>G) \ 	3#)  !ff#)&%
'B	ff3fi!
) ))@&%
@%
a
)&1 fi#@1	
$#)!d)O<
t
:&%
W%
ar.
2>fi9M)O'@
fi#fi7i0$)!M 'B)	NO%<%9) #
fi

 O$!1 )CRl	ffoA^
'J<fiY&%#)  ! #	ff31S_UPM
B 
) ) fi!
d
dUP	ff	Mff 
2) ) fi!
M
M1S
L"7"fi#B 	M#)  pCd	ff) L fi#9YR<#Afffi#)Bfi!
1fi#[
)4ff 
)) fi!
?'@
a9d!h.
2=$<$) 
	ff<SUP	ff	fi!

$
2)G
ff#5>)&
C,DE%#):) 	ff$3	B $	ffK#)d)O<!
fi#fi#9<$	ff1fi#'@
m

&%:	ffdff>d)&<

2fif$)	fi! #	ffm	=O%d
&
c) %$Cf	&%@)	ff$d	2f >	ffB#)M&%
7i0$@1$UPd<	 
2fiN
3
&
fig&
	p`
C n	 !
fig&
	M$O+>)W 	J&%=$9<
	&
 #	ffKO%
P	ff$)
& 
fi#fi#9M!M
@$#	ffM1
2) M	ffK) 	#fi;Yfi#!'@
 B
M	fi#	4	ff&$	fi#)C
&
fin&
	X>O+$) 	K&%B&
2 #	ffd<$)!3&%@$	pCq7i>)=
$#) @1
)
%'@
)@%
a")O1) &
 !
fi#fi#9'B	76[&% 3 
$&%p)4)&zb
@b$	ff'k# )"
&
fi/) O
 C Z 
1fi#!
 $	K	+[	$)d1S
) ?<S	  !
fi&
2 #	ff\fi
1fi#)5
$5) !&%5
1)5	t	&%
!+	 '@
 #	ffpC
	&%) 	ff$4	N$	ffE
>))f1S
) =	2/fi
`b	IE%
CPyb:
$
)W>	!G
<#
	ff	ff'cfi#	ff<'BsCC!YP&%5%ff')&1&$	ff<#)&xK!;	ff '@
2 #	ffLVfi91S	ff'B)d	"	
 ff

fi

n+o+pqrbsm,Xn+rpq


 CRDE%)f<$	1fi')W
$W1S) P#fifi!) O 
 d19@&%) 	ff$	0	ffo
&
SYUW%#%@	ff'Eb$	ff'(&%$
A) !"'@
<)P	fi	1
fifi!
M	s5
 O%UE)Yuvfffi#) 	ffpY  
  )aYffWfi#fi##) 	ffpYnuv  fi#) 	
  $) 	`_efifi#$)aYRuvxCN	ff'@<
$#) 	M	Rfi!
d	Pfi!
1Sfi#)=
'B	ffB&%4&%$='@
<))&%	UE)
&%
P&%9d
2ff$T+	ff	fffi#9d
<<$	Aff!'@
2 fi#9G,	2N&% 3 
$&%p)*fi!
:)&zb
)CWs.elR$t4;	ff
fi#	ff
2 #	ff)=UW%>@&%@&%>"'@
<)UP$B!?
2ff$'BCjx8DE%"<$	1fi'k	):
&
K	fi#fi# 
b$	ff''Bfi<fi#BA<$ )%
)W1SM	'B d!M	&%P	ff'@
2)
)EUPfi#fi*s;e'49&%pYpuvvwxC
l	ffP&%#)UP	ff VY1
) :	ffM	ffTA<$)W)&)  #	ff)YSU*4!&>	ff5 
	' $	ff1$UP
&%J+	fifi#	UE!?<
!$)B	fi
2) ) () *Ka`bYRa`_YRwa`_Yoa`&uuYRa`&u8s) MD
1fi5uJ;	ffO%G
')t	2W&%
fi!
) ) )Ox=s;P$	fi9d(l$#fi;Ynuvvw
xC

,JLAJy{zU[A,VW&AAUCSOR
DE%:	ff
fiP	=$#M
<<$	I
2fiE#)" 	c  'B!3UW%&%J 	5#d
[
<<fi##
d
3$#"
$0C
T
&
) Tfi!)=wvB!) &
)fi!
1fi#?
)<	) # #4	ffEff
2 #C*DT%$t
>=B#) $

 O$!1 )RUE#&%4$U*	 	;	ff$ 4
fi!)Y
B) #At	 !	ff)/
 O$!1 )CRE	ffn'B	ff$E
&$!1 
I
2fi)K
$"'B#) ):b$	ff'M!) &
)CXDT%"fi!
) )4#) &$!1 #	ff )4b
!$fi#95UPfi#fi*1
fi!
0YoUE#&%
4!) &
)Wfi!
1Sfi "
d=!) O
)Wfi!
1Sfi#$`bC
DT%)B	ff'J
!8U
2)=%	) 81
) MO%"%	#M	WUW%O%=	d#d
8
<<fi##
@$#B)
)&1]$ #!M
&$C 3  $	ff#)*!&$	X1S
) =
@
2) ) ) )&'P	pb&$=1%
a#	ffo)1
) 
	ff\<
) "1S%
#	ffCyz[
  #
fi=
<<fi##
 #	ffs;1M) !m
57i0$"
O
) &xYf'B>

3 A<$) )>` W+	@&%
ofi	
@	2"$)/UP$Tfi))*O%
@Q	 $*
2*<$# !@UW%O%&1S	ffz`
$fi#!@
<<fi#
 )PUP	fffi#@Ob
fi#/	ff4&%!fi	
)fs3#%YpuvvxC0DE%#)E'B
)RO%
/Q	p&%
fi!
1fi#)4U*>t!3$	ffT	ffEO%
2 &$!1 )UP$@	f
2L^
t	"#)  !ff#)&%c		ff:b$	ff'1
5
<`
<fi##
)CWs;y.*#)P!$)  !B 	t	 E&%
P
#) #	ffK&>WUE
)/
1fi# 	4fi!
) ) 7;9"(	1	ff$>fi!

<<fi##
 )W	 $ fi#9M!M&%<
 Q'B$#
 3 A^<>) )W	'@
!5s3#H%#YnuvvxxC

,JLAJI{1[A|,[1[A"[A|0WOAW,VSR|
l	0&%#)n
&
T) Ya&%/	ff
fi#)R 	Efi#
=	E) ff'BR
!'@
/!	&%/) fi!
) ) )~*)OV9Ya'BY
UE!	U=Y1>VY0ff 
2) )YS+	fi#!
K
?<
&%pC 3 
2H%:	P&%Bfi
2) ) )t%
2)=K	ff1) >I
 #	ff)aY09fi#!
uaE 	&
fi	ff1) $
 #	ff)C 3 
%=!) O
E#)/&%
 
o	
4=UE!	Um	<#Afi)E$<>) 
19ufi#	IU`bfifi+YE>
fi7`bI
fi!!'@
J+
2&$)C8DE%M!) &
)@UP$M
U? 
	ff'Bfi#93b$	ff'


&
1
) B	E) 3	 	ff	'J
)b$	ff'1#fi#!)M
$	ff5O%
 #$) #$95	P5
) )&
%)   )

E'@%$)E
'@<)C
DT%tfi!
1Sfi#)=;	ffT&%#)=
O
) U*><>	ffm196$)  !3&%B'J
)&%$	ff%3
K	fi#	ff
) ff'&
2 #	ff
fi#	ff$#&%'s&%/W4X
2fi	ff$#&%'sq= 
<SYP	fi#fi#!)YP$	fi##	Y  
) 	ffpYmP#) '@
pY
uvvx x=
[&%m'@
ff
fi#fi#9mfi!
1fi#!m
%m$#	ff 	ffm
:	ff'@< @'	ff# 	ffCmDE%#)M<$	ff>
<$	)"$UP	3$9^<S)@	fi
1fi#![ $	$() *K	ff1]$ )B&%
"1fi#!	5	ffd
	O%Y/)OH% &%
@

$#	ff &%
@#)M<$	ff'B!
 fi#9[	M_9<:		ff1]$"%
)M<Afi#)K.>	ff'
	O%B	ff1]$@fi!
) )K!
#E
m$#	ff)B;	ff4UW%#%8&%d1	ff
$9?#)"fi#
Y/c;	ff4)O
fiP)O< #	ff19?%ff'J
)
sq= 
<YpuvvxC0l	ffPA
'@<fi#Y1S
) =)OV9K ) 	3&<	ffV2&%$	ffff%4;	fi#!
Y)&V9M
K+	fi!


m1SM	ff.)m8O%M& 
!!5
&
SC5yb8&%KA^<S$!'B )"O%
;	fi#fi#	IUU*K!O$	ff\&%
;	fi#fi	UE!K	ffb) #	ff() *o)&V9`;;	fi#
2<
&%`b 
) )ff
) )$`;;	fi#!
C 

a1pI$;R$

 IjIRpzbRIO&$;;$;$0bb Ioj0IRzb$O; R  IoO; &;O
 I

fi

prv+hsvc%q+mrqhrpm6n++v+v5SIb



u



w




v

Pfi!
) )W
'B
P	ff

P	ff
fi#!
= 
afi
= 
2) )
q!$
l	fi#!

DV
eVA
9 Dn$
eV9

yz)  )

u
ff
w
v
u
u



D
1fiBA*RP	ff
:) ff'B&
	Kfi
2) ) )

lR#ff$41*RP	ff
M
O
) (*n	ff$#!
fiR
M) 'BM& 
!!"!'@
2C

,JLAJEkSOR	C[AR[A|WOAW0VS"|
DE%#)B
&
:	ff'B)r.>	ff'
:) 4	W!'@
)B	E	ff&$95$	
)Bc5
) )&
2H%)   )aC 3 
%3!) &

$<$) )@
:x
 ?Mff>8	W<#Afffi#)@)$!1193&%$K	fi#	ffB
3;	ff4 A&$B;
&$)aCdDE%
fi!
) ) )R
$P$	ff
0Y$	ff
2fi!Yff
fi+Yaff 
2) )Ya>Y;	fi#!
Y&VYa)&V
9 O$*
4)&V9CDE%$E
>/w
!) &
)!M&%#)
&
B)W
5uJ
 &$!1 
fi!)f
$='B#) ) !CEDE%b$L9M#) &$!1 #	ff
	Wfi!
)) )B)J)&%	UW5!mDn
1fi#dC3DE%Mfi!
1fi#!<>	ff$K;	ff4&%#)B	ff'@
!8UE
)4&%")O
'BM
)
;	ff&%K) @) ff'&
2 #	ff3	ff'@
2pCKlR#ff$Jd)&%	UE)B
3	ff$#!
fio!'@
@	3&%@fi#O;
5# )
	ff >)&<	ff!4) ff'B&
 #	ff419O%P
fi#	ff>O%'	ff4&%E$#ff%YaUW%#H%)/)19t
%'@

 	M$
2 @& 
2!3fi!
1fi#)CMy.=#)Bfi
b$	ff'&%#)BA^
'@<fi#YO%
&%"$	)t<$	193&%
) ff'&
2 #	ffM
fi#	ff$#&%'
$B	#) 9M!3
&$K+	PA^
'@<fi#tfi#	
 !M
fi#fi	/O%=&$O VK	ff
&%=$#ff%E	R&%!'@
t) !K&%=$#	ff)W<>	ff519"&%==
fi#	ff$#&%' )!'@<	) ) !1fi#C
yz@	RA<$!'B )PUPE!&>	ffK&%T+	fi#fi#	UE!B	ff.)	() *n;	fi#
2O`b)&V9s.wa`_vxff 
afi7`b!$
s;a`_xff 
) )>`b>?s`_x4$	ff
`bff
fits$uO`_xff 
))$`;+	fi!
 s+`_wx)&V9`b)OVA
9 &$[s;vI`_xT+	fi!
O`
)&V
9 2&$ffs;wa`_x*
K+	fi#!
O`bO Vs.wa`_xC 

1pI$;R$

 IjIRpzbRIO&$;;$;$0bb Ioj0IRzb$O; R  IoO; &;O
 

fi

n+o+pqrbsm,Xn+rpq

,JLAJ{VU[uOR|ZR[AUU[A,VCW,VS"|
DE%:	ff
fiP	&%#)"
2&
) @#)K 	? 
V36$)K!m 'B)@	&%!@)$#$9m	ffm
3) 
fi#:	uH`_C[
fi!
1fiR	2u4!
2 )t	J6$=	 $3	ff&%

a9G
3fi!
1Sfi)=I`_Y><$) E6$)Tb$	ff''B!	ff
s;x 	")$Gs.xCEDE%@ 
V!"	2*
9M19d%'@
)#)=) )&
>fi#93)&1]$ #43
O$C
l>&% 'B	ff>Yff)6>)W
$ )P&%
P	ff	IR !'BYO%W%
	!	ff))  
V!
!$
) )UE#&%&%H%
=&%
E
B7i0$W<>) 	ffd'J
9J	"&%=
V!C
DT%)
&
2) PU
2)E	ff'@<#fi#3	ffd1)&%G6$t
2 #ff#$9"!:&%='@
2fifi#BO
 #	ffM
$
		$&%`
UP)   # 	ff$!
Yp) & 
2fi!
3sq	UP= ) fi;YRuvv
SYuavv1pY0uavvffxCPDE%B
&
2) E	ff&
!)
	ff1) >I
 #	ff)ds
a9)&x
2H%c) $!119c 3;
&>)tO%
t'B
2)&$K&%G'J
Aff!'B' '@<S 
a`
&$Y&%'t1SP	0
a9ff)P) !#Efi!
) E 
20Y
$	ff%P!AYS&% '@<S 
&$
P<'MY&%
UP$`_1fi!13 '@<S 
&>Y&%BUE!5)O<0Y0O%t$fi!
 #K%ff'B##$9Y0&%4;	ff$) 6$B
!AY
&%
!P<$) )&$Y
K&%W 
) )6$W
CRybJ	ffoA^<S$!'B )PU*&$#K 	t<>P&%A

6$:) $#$9Y/U%$
)"%#)  	$#
fi#fi9\&%d'B	)@	ff'@'B	m) M	&%#)"
O
3#)" 	3&8K!	?

1!
$9"<>	d<$	ff1fi#'&%
o#)  !ff#)&%)1$UP"	46$@sfi!
) )uxo
@6>sfi!
)) )Ea`_xC
o
) 4T) 'B)%%fi9dfi#!Vfi#93&%
E	U*	fi: $	ff	) fi#9"fi!
1SfiN
46>b$
a9G
)%
!

6>f
Jff#$)O
YUPE!&>	ffMfi!
) )P	ffb) #	ff)E
'B	&%f<
2>() *Ra`_YffI`bY`_SYffa`_wYwa`_Y

da`_SC

'e#. w
c R<j+`i!j11_a$j ]gXi
)W) >1S
1	Y 	") WO%=) !fi#O`_
fi#	ff>O%'MY'@
2]_	$#_9K	t
:	ff) )&)=<$	$)Y
UPB&$	? 
	ff'F	)@! 	MO%tO 
!!d
&
K1$UP3<
!$)=	2/fi
2) ) )=O%
f
$@'B	) 
fi#!Vfi#9[ 	?1Sd	ffb) \ &%d	$#!
fiEfi!
1fi#)Cyzm&%#)KUE
9YU*d%
aM)'Bfi!
 \&%d$9<M	
fi!
1fi#!m $	B&%
B#)"	'@'B	ff8 	3
%8	ff'@
2pCDE%d<
!$)J	ffi!
)) )"	XU%%m$	ffBUE
)
!O$	ff+	P
%K	ff'@
!MUP$)$!13!M&%=<$#	ff))  #	ffpC
l 	ffP
%M	2N : )Y
%K
&
) TU
2)W 
	ff'fi9M##5! 	M
B&
!?s.v@xo) 



B ) !hs$ua@xo) CoE; PO%
&
BUE
)E)O<fi#! 	@!<=&
!d
M )E)  )YSU*4&%
	ff < 3&%B& 
!!d
O
M19:&$	!dfi!
1Sfi!3 $	$)=) !d	#) @fi#fi#)@ 
!Kb$	ff'
K 	MffF	#) CBl	ff
M	#) @fi#
fi 90Y0
h##
fiP	ff1) $
 #	ff3UW%	) Bfi
2) )=UE
)	ff@	P&%
#j6M<$	ff1fi#'@
2 #W<
!$)/%
@

 9Q%
E	21!B	ff  < 0Cnl	ff0A
'@<fi#Y&%Pfi
`
	R	'@
!"
@!) &
.$	'fi!
) )E"s.1
$ff$	ffxo%
)E

 9%
	1!"%
K 	
fi!
) )uuts)O% 1)*
K1
$Eff>	ffxYff
"
B!) &
Eb$	ff'Qfi!
) )uuW%
2)*

 9%
E	1S
%
5 	:fi!
) )tSff
C T) !XO%)B'B&%	5&%M<$&
@	*O%" !$@&
!3) O%
=UE
)
	ff < d'@
a9G1Sfi#) )WO%
;
 9;	ffP'Bfi# 7`bfi
2) )f<>	ff1fi#'B)f1S
) 4	fffi#9M) 	ff'B<
!$)	fi!
) ) )

$4	ff) #$5<$	ff1fi#'@
2 #CfDT%t
&
2fi<$&
=	*	#) B!:&%=	  < :& 
!!M
&

#)f><	ff$M&
1fi#)&%
E<$) E&%4A^<S$!'BO
fi$)&fi# )aC
l 	ffP
%d	)=fi#fi;YpUP=	ff'@<
$d&%B
a 
=<>
2 
9M	Rfi!
) ) 76$)&
!

) !B6fi# $M$)&)E6fi# $M
&
SCl	ffR
2H%J	&% K )P&%
E'@
V2W<@O%=
 
YUP
) M
;	ffz`;;	fi#@$	))$`bI
fi##
 #	ffK	6fi# RO%W	ff < @!) &
)/b$	ff'QO%W&
!B
O
CDg	

) )) )P&%=
1#fi#$9M	n&%) !fi#O`_
fi#	ff$#&%'Y'@
2]_	$#_9@
M	) )&)T6fiE'B&%	)E 	B#j;9
&%B	ff < 3)&
)=UP4&%3 
:
%d	2/&%Bfi
 !X
fi#	ff>O%'B)$UE#+ *E6>) f):&%
6fi# $3
&
2) P&%d) !@O%6fi# $:
&
) aC
 

fi

'e 

 

jl_` X



prv+hsvc%q+mrqhrpm6n++v+v5SIb

# "!$

Wa gE`b

 H%	) "O%$MUPfifi7`_V	UWm
fi#	ff$#&%')=b$	ff'k&%M'@
%Mfi#
m
8) &
) #
fi*<
  
 K
$		M	ff'@'B# #) 	B;	ff '&%r6fi# $)(*o#) #	ff:&$)Y
$) %ff1S	ffEfi!
)) 76$)=

fi#!
='@
2H%!)C  @$)&$#&%@<$) &
 #	ff:	P	ff'@<!$#
fiP$)&fi# ) 	M&%)tO%$@
fi#	`
$#&%'B)R	%
E&%Pfi!
$#$9=	p	ffR<$) &
	=	2&%E'B&%	@
B 	=$EO%'B1n	
&
1fi#)/<$) 0C  	U*Y!B
# #	ffB 	EO%*A<>' )/><	ff$0YU*P
fi#) 	 
A<>' )
UE#&%6B1
) Bfi#fi/
fi#	ff$#&%')sO%=&%$B!:&%#)=<
<<fi!)=a`_,
 Z 3q=Ds;P$	fi#9X
T 	i/Y/uavvx xC"=>)&fi# )4+	ffr&%K>
) 8) B	Efi
 !h
2fi	ff$#&%'B)B)&%	UP5&%@)O
'B
&$)
)E&%	)=$<	ff> M!M&%#)
$ #fi#C

RX+_`_Aj iXjAF$g,{`hjjdxd$`hjljC #)"#&%J
3fi#
ar	d	&
2!m
:fi
2) ) 76
 #	ff
	ff
c
 &$!1 B )YpUE#&%:;	ff
%3I
2fiJ	E&%@
 &$!1 Yn
M1 
%3 	d
K#) #	ffc&$C@Dg	
fi!
) ) 7;9X
K!) O
@) !G
J#) #	ff:&$Y	ff4) &
> )W
EO%t$		E	ff@
6)WO%t1 
%
	ff >)&<	ff!m 	c&%d
fi!X	2=&%d )"
 &>1d	ff1) >[!mO%d!) &
CQDE%#)d<$	) )
$<S
 )"
B&%M)O1&$d$		 m
B&%
@1 
%m #fiW
:fi
It	ffM#)"$
2H%0CmDE%M!) &

#)"O%m
) ) #ff\&%Mfi!
) )@fi!
1SfiW	&%Mfi#
a>C\dUPfi#fij`_V	UW[
<<$	ff
%8 	3	ff) & !?

#) #	ff8&$J)4 	Mff$	U,
&$" #fi/
%3	P&%@ 'B!
fi*	)dsfi#
)&x	ff&
!3!) &
)
b$	ff'
@) !fi#@fi!
))t
:&%5< @1
VM&%4&$BUE#&%:&%B	ff1]$ #B	R6!dO%t)O1&$
UE#&%\&%3fi#	U*)G'B#) fi!
)) 76
 #	ffQ 
 CK!'@<fi#'B  
2fi	ff$#&%' ) )dPCj)"<
'B&%	MUE#&%:
B	ff6Bfi#fi0	/C!uMs;!fi!
pYnuvvxC /
Dg	) fi#/
E )0;	ffn
W	P&%/&>YU*oH%		) o&%P ) n&%
R'@
A!'B#-)R&%P;	ff 'J
 #	ff`
ff
!8 
 #	3'B&$#5s;!fi
pYEuvwxC"4!'@<fi#'B&
 #	ff8) )=&%M'B!!'t''B14	E!`
) &
)E 	4+	 '
B )	=	1S=L
fi0 	B$UP	
C W#
$!
 4)	dO$=
fi#	ff$#&%')W$L>
&%
o
%@ )*%
a
#) $ ='B1o		 	ff'B)CRDg	t'BP&%#)E$L!$'BY
%@	$$
;
&$u
 k]n#)f'J
<<M 	@
B)*	2N	ff$$+
2&$)E19J6!G
4) T	No	ff	fi
K )  )E	2&%
;	ff 
' k]"YUW%$<
 T#)*!&%W	1) $d 
f
"#)W
4W<S	!P	
 w] CRE
fi#	ff>O%'6)
&%4I
fi!B	G
 &%
W'@
2Aff!'B#-)&%4;	ff 'J
 #	ff`bff
!: 
 #	CPD0	K&%#)0Y&%4	ff1) $dI
2fi)
;	ff
 k]p
$P) 	ff> 0Y
B&%E'B#<	! )E1$UP4fi
2) )*1S	ff
$#)*
$PI
fi!
 3s;!fi
pYuavw
l
a99ff
M,yz 
;Y0uvvxC
ZT]j_`hj$6-]j+#ZK2<g]`d-ffFa_1$($hED4j ` s+q=
"
 
$aYuvxT)B
K) 	8!) &
)Y

%Bb$	ff'	ff	N'fi!
) ))Y&%
P
$) M 	4fi
2) ) 7+9d
"fi!
1Sfi3!) &
=
	ff$!B 	B&%
'@
2]_	ff>$9fi!
) )j6
2 #	ff@	2O%*!) O
)kB
$) P%ff1S	ff$)C/yzB&%#)/$) #	ffB	p&%W
fi#	$#&%'

%8!) &
M!8&%M&
!c
&
3<$)  m 	3&%M
fi#	ff$#&%' )K$&
!0CmD0	3 'B!
&%K#) &
M1$UP?
:<
!	E!) &
)@UP"
<<fi#95&% 3 fi##
8#) &
M'B&>C:ybc	ff
A<$!'B u
) KU
2)E) E 	B	ffC

 )
J) W	2(fi!
#) >'
r. #	ff)O%
W
$B) d	fifi#O`
 #fi#9d	G
)) #ff3
:)&
B 	K	ffB	/&%fi!
) ))"s;E#fi#) ) 	pYRuvwxC Z u1K
M!) &

) $!< #	ff3s;
<
   	ff xn	ff)) !=	0
	ff) &
EuE
B&%W;
&$)R&%
2R) $!1&%
!) &
CoDE%@
2H%J#) $!'B!
P.	
 t] s 3xR%
)P&%E;	ff 
' ] ?YU%$
 o]0#)E
4 	ff
	WxuB	ffOK )C fi#!
@'J
H%!K!+$)B!) &
ff
  1fi#	ff)B 	Mfi!
)
) _T7W
?	fi937




-!_1F+Xi]j  



a Ej_`

a1=0.IO;0n   + 7  $_I;gbzjoIbEOz;j*z;Ij$&;$IOj Ipz   bP$o2$;&zbO;
 0a&PI$<a IabEp =&oIIobzjOj jM$$ztP;at$(zI$Tj_Oz0 @I
j$&z   j;o=IRa&;a

 '

fi

T	#)  Z fi
&
fiT	#) 
uO`_
E	
el
3l
Pl
E	
Z 
el
3l
Pl
q`_Dn$
E	
el
3l
Pl


Cj
CjuCj
CjuCj
Cj
 uCj
Cj
 uCj
Cj
w Cj
vC!u
u uCj
vCj
 uCjv
Cj
 C!u
Cj
w uCj
Cj
 uCj
Cj
 uCjw
Cj
 Cjw

n+o+pqrbsm,Xn+rpq


Cj
CjuCj
SCj,Cj
SCj
 uCjw
SC
 uCjw
SC
 ,Cj
vSCj
 uCjw
SCj
 uCj
vSCj
 ,Cj
SC!u
u uCj
SCj
 uCj
SCj
 ,Cj
SCj
 uCjv

u
wSCj
uCjCj
wSC!u
u C!u
wSCj
w uCj
wSC!u
u Cj
SCj
 C
SCj
 Cj
vSCj
 C
vSCj
w Cj
SCj
 Cj
Cj
 uCjw
SCj
v uCj
SCj
 uCjw


uCj
wCjuC7
C
 uC7
Cj
 uC7
Cj
 uC7v
wCj
 C7
Cj
 Cu
Cj
w C7
Cj
 C7
Cj
v uC7
Cj
 C7
Cj
 Cu
Cj
 uC


uCj
SuCjv,C7
Cj
w ,C7
Cj
 uC7v
Cj
 ,C7
wCj
v ,vC7
2Cj
 ,C7
wC
 ,C7v
2Cj
 C7
wvCj
v uC7
vCj
w ,C7
vC
 ,C7
C!u
u ,C7

ff
vSC
wCjCj
Cj
 Cj
Cj
 Cjv
Cj
v Cj
wCj
w Cj
SuCj
 QCjw
SuCj
 C
Cj
 Cjv
wCj
 Cj
SuCj
v Cjw
Cj
 C
SuCj
 C!u

D
1fi#BA*/Pfi!
) )j6
2 #	ffd

9@Kfi
d	P
O


mm;
s
aN_bxZ+]>sXx s3xCnl	ffo&%=
$
) )TUW%#H%+]_s3xa s3x*
m 
d
 1#&
$9@#) #	ff
#)f'J
+R
* 	ffP!'@<fi#'Bb &
	M	/
 Z %	ff	))E&%=)O'@
fi#fi#E	_/
b  !M&%)
) )C

Dg	:6[&%MUP#ff% )@	&%Mfi#
M'@
%MUPd) :&%M&% 'J
fiP& 
!!? fi#s.*>	fffi#9
T 	i/YEuvvxCc $"'B	76
 #	ff  	5&%#)"<$	$ms;P$	fffi#9YuvvxB
$) ))@&%
<$	ff1fi#'O%
/&%UP#ff%)R;	ffG19@&%#)Wfi<:	ffJ&%W	$R!KUW%#%K&%W!) O
)E
$
<$)  0
E<S	ff	0	ff>$!W
fi#
4 	W
r!
 
 ofi!
) ) 76CRDg	W'B!!'B#-E&%#)N<>	ff1fi#'MY&%
&% 'J
fi&
!K<$	ff>)
<<fi#: J !'B)Y) !"
r7i>E	$$!4+	ffo&%!) &
)

%8 !'BCmDE%#)"<$	)" Z m)aY/
%cUE#&%m
37i0$J) B	fUP#ff% )C8DE% Z  &%

'@
A!'B#-)WO%;	ff 'J
 #	ff`bff
!: 
 #	"'BO$#=#)E&%H%	)pC

'eE 
c jAF gCCa#j+`bg,

1$($hEDGFb_Ag, ;
 F+F `h_1Fbk

^a_

by MDn
1fi#=UP)&%	U[&%
 
29@;	ffRO%Wfi!
M	
&
4	0&%fi
2) ) 76$)T+	ff'B"19@
%@	
&%P&%$
fi#	ff$#&%'B)R)  @) !t	r6fiWs;T	ffxY
) !fi#O`_
fi#	$#&%'(6fiEs;el/x 0 Y
'@
]$	ff$#$9
	6fi# "s+Xl/xY
5
K	ff) )&)r6fi# @s;Pl/xCpDE%46$) =>	IU$<S	ff$ )&%@	#) K 
 B) 5 	
	ff <W&%B
O
CWT	 4&%
T+	ffT&%#)
&
2) WO%<S$&
4	/&%B>tO 
!!M) &%

#)=	ff < 3;	ff=
M	)G 
2 B	
 9cUE#fi#fi1S@fi#) )B&%
 91
)"	fffi#93) 	ff'"<
!$)=	2*fi!
) ) )

$B	ff)><$	1fi'J
 #C@DE%"
O
fiN<S$&
@	2*	ff < 3& 
!!d
O
K)B$<	$ 
!M&%)	ffX$	U	R&%&
1fi#C
 %5	3	#) @#)=!&$	0Yn6fi# $!3#	='J
V@
M) #ff76
B7i>@;	ff
9d	
&%='BO%	ff)	ffK&%#)
&
) aCNe!B&%	ff>
2fi
O
=#)	Eff
 
 M 	@1B	#) b$YUP
%
a"	3UE
a9d 	:I
fi!
 MU%&%6fi# $!5!'@<>	I)tO%d;&Bfi!
) ) 76
	[
 
29h)
&% )E
&
B
aI
#fi!
1fi#@%$C

 K #=

 IR;j$$6@b  zb0;PI/ jPj&;jIj;zNIz=Io;&Pnj$ ;aIj&;jIj0;_;
  b8O=I"j;zR W" zjO;Iz 
 =

fi

prv+hsvc%q+mrqhrpm6n++v+v5SIb

l	ffn	#) Efi#fi#)E<B 	MYUW%#B
&
b$	ff'
'@
2]_	ff>$96fi# Y
fi#fi'B&%	)RUP$E
1fi#
@
	 $&
2fi#	)  	B&%!4 ^5$#&aff$aYUW%#H%UPO6= 	@14&%
P	ff1&
2+	/O%
) 
	*	#) @
3	K6fi$!Cl	ff	#) Bfi#fi#)=	/
3ffMY^6fi$!d!'@<$	)
 
9
;	ffR
fi#fi&%>W
fi#	ff$#&%')YUE#&%J'@
]$	ff$#$96fi# $!@<_+	ff'B!=) fi##ff% fi#9"1 R&%
	ff) )&)o	ff
) !fi#O`_
fi#	ff>O%'6fi# $!Cl	P&%#)
&
)Y&%B1) fi!
) )j6
2 #	ff3'B&%	)U*>"uO`_

#) #	ff8&$)aYN
2fiO%	ffff%5
B
d	#) Kfi#fi/	2
fi#fio&%$"
fi#	$#&%'B)t
2H%#m	ff'J<
 
1fi#

 
2)aCdyb8D
1fi3u8s) @<<#Ax@UP@)&%	U&%M$)&fi# )B	W
:<
!$7`b) =	ff'J<
$!
	T6fi# $!?s;T	ffxP 	K
%M	R&%6fi# >d'&%	ff)aCEDE%B&
1fi#t$<S	ff$ )&%B<`bI
fi!YpUW%#%
#)B&%"<$	ff1
1fi##$9?O%
=&%Jji0$M!c&%K_UP	M)&
'@<fi#G'B
)=#)B" 	:H%
C 9 lR#ff$
M)O%	IUT)=
Mff 
<%d	2O%"
 
93
fi!))3
5uO`_
)=O%B6
fiRfi!
) ) 764	E&%B6fi# >

&
SCdE	 @&%
B&%J$Kfi
1fi#el$O;$)B 	d&%M$)&fi# )4b$	ff') !h
:) !fi#d
fi#	$#&%'
6fi# T	ff) & Mff!
J&%)&
'Bfi#
M
fi#	ff>O%'
)E&%6
2fi0fi!
) ) 76C
100

None
SF
MF
CF

Accuracy

90
80
70

60
0

20

10

30

40

Noise Level

lR#ff$=A*R 
9K	R&%fi!
d	
&
r+	E
MuO`_=C
l	ffp&%>/	&%*>'@
!!+	ffg
&
2)  )0UPR)&%	U5ff
<%)N><	ff$&%P
 
9	
%	
&%r+	ff*6fi$!d'B&%	)s;	Y) !fi#O`_
fi#	ff$#&%'Y0'@
]$	ff$#$9G
M	ff) )&)&xT!:	ffa]>	
UE#&%:
B6
finfi!
) ) 76Yp)fih19M%	ff	)J;	ffP
%M
O
) E&%@'B	) W
 
2 	/&%4&%$
fi#
 !
2fi	ff$#&%'B)MUW% \UE#&%	d
c6fiM
UE#&%	ffM!a]_	#) Cl	ffJ&%:6$

&
2) YU*B%	) @	")&%	U&%@$)Ofi);	ffW
K#) #	ffc&$@1S
) K&%) @$)&fi# )B<	) )) )&%
fi!
$)ji0$M!5
2 
951$UP:6fi# $!5
?	r6fi# $!CMDE%Bbfi#fi/&
1fi#@	2T>)&fi# )
;	ffP
%K
&
)*
d1;	ff::D
1fi)Bua`_SuW!&%f<<S#AC
l	ffP&%B$#
O
XslR#ff>twxY&%4fi!
4'@
%!t#)=
K1 Efi#
 !31
2)&%
M#&%
&%uO`_m	ffg&%P#) #	ffJ&$*
2)Rff#M19# )/%#ff%o1
) O`bfi#fi
 
29Gs;SCjE$)&)RC!u

MCjwxCyz@&%#)E
2) f
<<fi9!"
4) !fi#O`_
fi#	ff$#&%' 	ff/'@
2]_	$#_946fi# ofi#
)E 	4) fi##ff%fi9M1 
$)&fi# )T&%
M
4	ff) )&)6fio+	*	)=fi#fi#)f
1	MCnEE	#) =fi#fi#)	N 
d%%Y
6fi# $!d
2) ) 	K'J<$	Bfi
2) ) 76
 #	ff5
2 
9CEF	)Yp#)Bfi!Vfi#95O%
f
9M	
&%P6fi# >@'BO%	ff)o	fffi#"!'@<$	
 
9@1
) W!)&KW%#ff%BL
fi##$9@& 
!!B
&

#)t
aI
2fi!
1fi#@	d1fi#m
3
 
=6fi# Ctyz0YN
2Wff	#) Y0&%@) B	E
K	ff) )&)6fi# 
9fi#)fi#	UP*
2 
9M$fi!
 #=		/6fi# >Y
)T&%=1!
) )E	R#) #	ffMO$)W
5uO`_fi
2

a1pI$;j_I$4b$j;IjK$ jzb$dIjoj;jB&gj$;;b;j.;ToIOPI;bO;$d&E
Ib$ 	;$;nIO/tzj$ O;$p
 O20 z;b ff
 b$ Oz ffr;aIB;zt;jj$$O;@;j 0pj@Tb O
Ibjj 4;Eb= ;_;g;_;$an O/Ojj&;$=R&;oIjtIO0R;_;g;_;g bnzbz$
 p

fi

n+o+pqrbsm,Xn+rpq

100

None
Self
Majority
Consensus

Accuracy

90
80
70

60
0

10

20
Noise Level

30

40

lR#ff$=wA*W 
29"	R&%$#
&
4;	ffP
Bfi#!
'@
%!C

100

None
SF
MF
CF

Accuracy

90
80
70

60
0

20

10

30

40

Noise Level

lR#ff$=A*W 
29"	R&%=$	
M) ff'B&
	K
&
r+	E
B#) #	ff:&$C

100

Accuracy

90
80
70

60
0

None
SF
MF
CF

10

20
Noise Level

30

40

lR#ff$= R
* W
9M	&%4) =)ff'B&
 #	ffJ
&
4;	ffP
MuO`_4C
 

fi

prv+hsvc%q+mrqhrpm6n++v+v5SIb

&%B'B&%	X	"fi#!'B!
 "'J
9M	ff	d!) &
)"sUP=UE#fi#fiRA^<
d&%#)=<	UW%:UP=#) ) )
&% >	ffE 
)*	2N&%6fi# >)W!de	MCjSCjx
lR#ff>)dm
5)&%	U&%5$)Ofi)M	t&%5$	
) ff'B&
 #	ff\
) 3) 'BO
 #	ff
$)&<S #fi#9C*DT%) W$UP	
&
2)  )*
$E) !'B#fi!
P!J&%
/O%9@UP$Efi!
1fi#3) !tO%W)&
'BE)O
fi
fi!
1fi#!M<$	ff))fs) >1SM!"e #	ffJC!uCjxY1oji0o!@O%P+
2&$)*'
)&$"
"&%T) 
	fi!
)) )CRl	ffo&%=$	ff
K) ff'&
2 #	ffK
&
Y
fi#fi0&%$6fi# >M'B&%	)E<z;	ff ' 	ff'@<
 
1fi9

3!:
H%
) B)&1) O

2fifi#9:'J<$	@
 
9d$fi!
t	G	T6fi# $!Cl	ffP&%#)
O
) Y
) !t
#) #	ffJ&$E
)R
T6
fifi!
) )j6o9#fiK) fi##ff% fi#91S  R<Sz+	 '@
EO%
B
BuO`_W\

UPB)&%	U,$)&fi# );	ff&%#)='B&%	0C=l	ff&%B) @)ff'B&
 #	ff:
&
SY0
duO`_UE
)f
K)fi#ff% fi#9
1 6
fifi!
) )j6T
)P)&%	UWB!MD
1fiBuCnl	ffR&%#)P
&
2) Yff
2*	#) fi#fi#)E	N,
Kff
&%W'J
]$	ff$#$96fi# P<z;	ff ')*1 R&%
@
%B	0&%T	&%$)P
G>&
!)*1
)O`bfi!=

9"

	#) CPDE%!'@<$	'E!:
 
9J.>	ff'(6fi$!@;	ffP
%K	O%) =$UP	=) 'BO
 #	ff

&
2)  )=
?1SG
&$!1 8 	d&%!4fi!
) )B) <

1#fi#$9C5e<76
fi#fi#9ffYP#B#)@$fi!
 #fi#9?
2) 95 	
)&<S	W	 fi#>)f1S
) B&%$4	/&%;
&$)!d
2H%
&
) 'B
)&$	fi	Y
X'@
9K	/&%
fi!
) ) )
$UPfi#fi7`b) <
 
2 d!M)O<& 
2fi0)&<
C

Accuracy

80

None
SF
MF
CF

fi70
60
0

20

10

30

40

Noise Level

lR#ff$tv 
*  
9M	2N&%6>) $#$9"
2&
4;	ffP
B#) #	ff:&$C
DT%W$)&fi# )o;	ffR&%T6$) $#$9B
&
)/7i0/)O1) &
 !
fi#fi#9B.>	ff'&%	&%N+	R
&
2)  )C
lR#ff$"v)&%	UE)&%">)&fi# );	ff&%@#) #	ff8O$"
fi#	ff>O%'
$	) )4&%@) #A5	#) @fi#fi#)Cl	ff
&%#)=
2&
) YS6fi# $!d!'@<>	I)fi
2) ) 76
 #	ff5
2 
9:+	W	)CtP
2fifio&%
	#) @UE
)
!O$	ff[
'B	ff3fi!
) ) )@a`_Yo1	t!8fi!
) )MuC58fi#	)=!)  #ff
2 #	ffpYRUP"#) 	>
&%
KUW%
<<fi#986fi# >\ 	8&%d	ff>
2fi
&
) 3s; 
	#) xM
fi!'B	) M
fi#fi	=&%
!) &
)b$	ff'fi!
) ) )=I`_@UP$4&
3
)	#) 9X19d
fi#fiR&%$r6fi'B&%	)CD
1fi#BM)&%	UE)

M	'@<
$#) 	ffh	E&%"#) O$!1 #	ff8	E!) &
)B!5O%@	ff$#!
fiP
&
2) =
8	E&%@!) &
)
fi#O+=
I+ 
K'@
]$	ff$#$9@6fi# %
2)f1S5
<<fi##0C  BA^
'B!XO%')fi
2) ) 76
 #	ff5'J
&$#AM;	ff
&%	ff$#!
fip
&
B
J;	ff"O%
*
1S	ffE%
fi7N	2O%W >	ff$)E$)&fi# J.>	ff'fi
2) ) 7+9!@!) &
)
fi!
1 fi#a`_5
2)@fi!
 ) )dud
m&%d	O%t%
2fij=UP$Kb$	ff'fi!
) )j;9mfi!
)ff
) 9
)Bfi
2) ) YoUW%$
9 a M
5 9 SCET+ 6fi# $!Yp&%B
O
) E	ff&
!)=
K<$<	 
B	/!) &
)
1fi#	ffm	5fi!
) )5uCeH%\
[m#) &$!1 #	ff	ffi!
)) )G>)&fi# )M!mfi!
) ) 76$)d1!
)
 	UE
$fi!
) ) 7;9ff!$9!) O
*
)nfi!
) )PuYUW%#%4r&%/	$#!
fiff#) O$!1 #	ff4	^O%/!) &
)
s;'@
2&
!K!@&%	ff  < K ) o!) &
)&x#)>	ffff%fi#9@MCRyb&$	!@'B	ff$W	)W! 	
fi!
) ) )=I`_K8	W%
B&%#)=1S%
#	ff=
XO%$O+	$YO%
2 
9d>)"s) BlR#ff$@vx









 

fi

 $#!
finq=
&
) 

5
]$	ff$#$9tlR#fi# 

u
ffuCj
vSCj

n+o+pqrbsm,Xn+rpq


Cj
Cj


C7
uwCj


uCj
Cj


uSuCj
C

w
Cj
uCjv


vCj
Cj


Cj
C!u

Dn
1fi#=1*EPfi!
))E#) &$!1 #	ffG+	ffo&%6>) $#$9@
&
2) PUE#&%M
MUE#&%	ffo6fi# $!M
a 

	o : )

T	#) 



au 

 

 


Z 
XP	
E	ff
Pl
uCj
uuC
SuC
uvC7
SuCj
uC7v
ffSuCjw
uvCu
C!u
C
ffwCj
wC7

E$#
T	ff
Pl
wSC!u ffSCj
vSCj
SCj
vvSCj
SCj
uuCj
SCj
uSC
vSCjv
uwSCjw
SC!u

P	ff
2
T	ff
Pl
uCjv
vC7w
Cj
vC7
uC!u
vC7w
Cj
uuuC7v
vCj
uuC7
ffCjw
uC7

e
E	
Pl
ffSCj
ffCj
vSC!u
ffCj
uaCj
uCjv
uaCj
wCj
Cj
vC!u
C
uuwCj

lR!$Be$#$9
E	ff
Pl
C7
uaC
wC7v
uaCj
uC7v
uaCj
wC7
uawCj
vCu
uawCj
vC7
uaC!u

Dn
1fi#tA*/Dn$) #-rGff'B1SE	Rfi#
a)
$'@
2r
R
$	) )N
2fifi	#) Pfi#fi#)C/yz=O%*O 
# #	ff
fi) /	2&%#)R
&
) Es;<$# !r6$/>)&)
	M6>x=
 
2)B	ECjwU*>@	ff1) $3+	$ff$) )	5<$# #	ffpYRUW%#fi#K;	ff&%@&
2)&VM	
<$# !3fi#	IUFs>uO`_x$)O)=%%5$#)&V:
a9ff)"s2`_xEO%
2 
93UE
)fCjs;) fi/q	IUPY
uvvxC/yzd)O'@'@
$9Y^+	ff&%#)
&
) aYUP	ffa]_O$=&%
2W#&%T&%;
&>)f
>
2L^

 	#) $!'B!
fi
2) ) )Ea`_Y	ff&%Efi!
1Sfi)TJ&%E	$#!
fi
O
	ffO
!"
ff>W	p)&1]$ #ff#$9
&%
E'@
V)E#W!'@<S	) ) !1fi#B 	@$
2 =
M
 
2 6fi# C

'e 
c jAF gCCa#j+`bg,$`hjljv



 j

<<fi9!:6fi$) 	K&%B& 
!!d
O
@fi#
)4 	")O1) &
 !
fi#fi#9X)O'@
fi#fi##) #	ffc&$)C=Dn
1fi#@
$<S	ff$ )P&%ff'B1SP	fi#
a)P!K)	M&$)E<$	Kb$	ff'&%	ff) )&)P6fi# >d
M&%
6fi# $M
O
C ( l	ffoa`_	#) Y&%T6fi$M
&
r$
 )P&$)PUE#&%;U*fi#
)*O%
@O$)
)  !'@
2 :.$	'&%B	ff$#!
fin
&
)CEl	ffEO%t$	ff
2d) ff'&
2 #	ffd
6$B) $#$9M
&
2)  )Y
m
4ff	#) Yo&%K&$)@<$	mb$	ff'&%J6fi$m
&
3%
aJ+UP4fi#
)@&%
c&%
	ff=<$	Mb$	ff'&%4	ff$#!
fi0
2&
) E
E	)C/DE%#)WOi0PUE
)W
fi#) 	B	ff1)$d1ff
9 %	%
s$uvvx
m
 &>1? 	 *
 4
=
1#fi#$98 	X$'	I5 	ffb) !3!) &
)Bb$	ff'k&%
& 
2!5
2&
Yn&%$19?$!8&%")-M	2W&%Kfi
 [#) #	ff &$)C3
 )@

 %)
s$uvvx4)&%	UPm'@<!$#
fi#fi9&%
2B+	@'@
9?
2&
)  )J&%$M#)G
cfi#!
">fi
2 #	ff)&%!<1$UP
&$T) #-W
@&%ff'B1	0& 
!!=!) O
)R@ 
	ff'Bfi#9B!$
) !B&%W'B1	&
!
!) &
)/%
2)R&%POin	!$
)4&$P) #-E4UW%B< !4)/
<<fi#0C # DE%!R

fi#9ff))
	 P
 4H
4)&%	UE)WO%
WuCjw 	*&%B$
2) Bh&$=)-J)B
 &>1O
1fi#t	G$	



ff

 "!

$#

% 

a1pIb$j;/   b$4 	;$;$O/ ;IoIb/  j$ O$RpjtIjIIaj;z;Ia^ 0nIORI
'& zbzI$o=_O;$&;j0jI$/O0P$&ab_O0n)(    j$Oz!
  1pjgbzj&jIa@O  I;o j;P bOI$b_$  ,IO/'& bbzg;aIP$I$
'

fi

Training
Instances

Training
Instances

Training
Instances

prv+hsvc%q+mrqhrpm6n++v+v5SIb

* * *
* * ,*
* * Filter*
* * *

Correctly
Labeled
Training
Instances

-

1 1
1 1
1 14
1 31

Filter

+

Learning
Algorithm

Classifier

/
/

C1

-.

Algorithm 1

C2

0

Algorithm N

2

Correctly
Labeled
Training
Instances

5

54

Algorithm 1
Algorithm N

Maj
Vote

CN

6
6

C1
C2

7

Maj
Vote

CN

lR#ff$@u 
* 3	fi#)W	ff'J<
$
&%) # -W	2&%E&
!J) CRDE%$'@
!o#)EW	=&%$'B	I
2fi	!+	 '@
 #!) &
)
s;	#)  xC
Q) 	ffMO$d!MO$) #-=#)
<<
$P&%t>)&fi# )W<$)  ::D
1fiBA*R
)E&%=	)
fi#fi!$
2) )P&%E) #-	0&%E&$)+	 'B4.>	ff',6fi# $K
&
$	UE)/'B	ff$TL^#Vfi#9@&%
B&%
) #-	&%T&$)n;	ff 'B4b$	ff'6fi# $K
&
CnDE%#)*$!;	ff$)P&%PUPfi#fij`_V	UWK<%	ff'B	ffJ&%

	#) @!:&%@fi!
))fi
1fi#)=!$
))=&%B) #-@	E
@#) #	ffc&$Crf3A< #	ff3	"&%#)
fi
&$K#)E	ff1) >@;	ff&%6$) $#$9@
&
2) CT	 &%
o;	ffRO%)T
&
) aYff
 
29"
K&$
) #-M
$"
<<>	IA!'@
 fi#93	ff) O
=
2$	) )
$#	ff)@	#) @fi#fi#)CdDT%)B$)&fi# )@1
) @;	ff
%
fi#fin	N	#) 4&%='B&%	d&%>	IUT)*	W
fi!'B	) T&%)&
'B)O1) E	R&%!) &
)aC

'e8:9g1iKEj ` $ ,$Haj `biK
%9<	O%) #)	N! $) T)UW%&%
@'@
2]_	$#_9J	 ) 'B1fi#tfi!
)) 76
M1B) d!)  
2
	R6fi# $!CBDg	" )W&%#)=%9^<S	&%) #)4U*r+	 'Bd$UP	M'@
]$	ff$#$9"	 =)'t1fi#@fi!
)) 76$)(*	ff
b$	ff'&%46fi# $?
5	ff4.>	ff'F6fi# >?
O
CtDT%'J
]$	ff$#$9M	 4) 'B1fiK) $)=
)&%
*a#& *n
"	P
)/O%E6fi# s;
)o)&%	UWB!J&%f1S	  	'Q_UP	4) H%')/<# M!KlR#ff$
uxCNDT%=$)&fi# !Mfi!
) ) 76$)UP$&%M) : 	@fi!
) ) 7;9M&%=	ff < d )E
&
C
DT%"$)&fi# )4;	ff&%@fi!
8	
O
d
$B)&%	UWh8Dn
1fi#GwCKl	ff
%5	E&%>"'B&%	)
;s T	ffY5
]$	ff$#$9W
@P	ff))&)&xgU*o	ff'@<
>/&%P
 
94	S
'@
2]_	ff>$9	 ofi!
) ) 76n 	&%
uO`_\fi
2) ) 76YUW%#H%J#)/&%'B	) /

 E	p&%E&%>W1
) O`bfi#fifi
2) ) 76$)o+	R&%#)/	'@
!pC
DE%&
1fi!fi!)&%<`bI
fi!)	R
t<
2>M$`b ) P 	@
) )) )P&%) #ff76
B!K&%7i>
;	ff1S$U*c&%M'@
]$	ff$#$9dfi!
)) 76@
uO`_W fi
2) ) 76C5DE%M'@
]$	ff$#$9d	 @fi!
) )j6)
'@
K<3	
cuO`_=Y0
)	5&>"
?
Kfi#!
B'@
%!C  @#	=)"
9XUP#ff% !
) %'BT+	ff	ff'B1!J&%!/	 )Cnl	R
%B6fi# >B) %'BY&%'@
]$	ff$#$9=	 Efi!
) )j6T%
)
L
fi	ffR1S  o
 
9B&%
&%tuH`_W\fi!
) ) 76oUE#&%BO%EAff<	K		#) fi#fi#)P	a`_
;	ffP&%B'@
]$	ff$#$9B6fi# CoEEfi#	UPE	#) =fi#fi#)"s;I`&u@x6fi# $!M	)f	%
=
Bfi!
$4'J<

	ff3O%
2 
93	*O%"'@
]$	ff$#$9d	 Bfi!
) ) 76C  	UPY0
=%#ff%B	#) Jfifi)ds;a`_@xY
1	&%3'@
]$	ff$#$9d
5	ff))&)6fi# $!3'J<$	B&%@'@
]$	ff$#$9M	 4fi!
) ) 76)@

9d	
&%
P	ff1O
!dUW%:	B6fi# $!G'&%	ff:UE
)E
<<fi#0C
'

fi

E	#) 
Z fi



u 

 
 
ff

E	@lR#fi# 
uO`_
SCj
Cj
uCj
wSCj
uCjv
wSCj

3%
Cj
wC!u
Cjw
C!u
Cjv
SuCj

<
SCj
SCj
SCj
SCj
SCjw
SCj

n+o+pqrbsm,Xn+rpq

h
2]_	$#_9BlR#fi# 
3%
uO`_
<
wCj
Cj
Cj
Cj
Cj
Cj
Cj
wCjw
Cff
Cjw
Cj
C!uu
Cj
Cj
Cju
Cjv
Cj
Cjw

	ff)
3%
C!u
Cj
Cj
C
Cj
Cjw

)&)lR#fi
uO`_
<
Cj
Cff
C
Cff2
wC!u
Cj2
Cj
CjSu
Cj
Cj
Cjv
Cj

Dn
1fi#twA*oP	ff'@<
>)	ffK	06fi# $!M 	B	 !B"fi!
d	
&

DT%f$)Ofi)P+	/O%=$'@
!!@;	ffo
&
2)  )*
$)&%	UWJMDn
1fi#)a`_r&%f<<SApC
yb8	M
) ?s$#&xY*
8!#
2fi='B&%	sfi#
"'@
2H%!xBUE
)@'B	ff>G

 M&%
pY	ff

<<$	A!'@
 fi#9@L#I
fi#W 	Y&%'@
]$	ff$#$9=	 Efi!
) )j6*+	ff$9B6fi# >K'B&%	0CRl	ff&%
	&%o&%>W
O
)  )Y&%'@
2]_	ff>$9B	Wfi!
) )j6U
2)*	"
a 
2W1 o&%
K&%) !fi#t1S) 
fi!
) ) 76C 3 Afi!3&%J) @) ff'&
2 #	ff:
&
Y0
<<fi#9!X
d'J
]$	ff$#$9"	ff	) )&)6fi# 

M&%M1#fi#!d
4fi
2) ) 76W)K&%) !fi#t1S) W
fi#	ff>O%'	ff&<Sz+	 'BM
@'@
]$	ff$#$9B	 
fi!
) ) 76UE#&%	P6fi# $!K+	E	#) =fi#fi#)	Eu
X%#ff%C 3 Aff<T;	ffP&%=	#) 4
) 
;	ff4&%d$	ff
[
m) M) ff'B&
	8
&
Y/
m&%d
) K;	ff4&%M) :
&
Yn6fi# >
!'@<$	\&%X

9m	&%X'@
2]_	$#_98	Mfi
2) ) 76K	@	J6fi# $!Cyz[
# #	ffpYT!
'@
93
) )ds;<
> #fi
$fi#9
B%#ff%@	#) Mfi#fi#)&x46fi# >8
<<fi#mUTO%8&%G1S) B!#
fi
fi!
) ) 76T	ff1&
!31  T
 
#)&%
M
<<fi#9MO%='@
]$	ff$#$9B	 fi!
) )j6T 	@6fi# $

&
SC=DE%) Bfi!
) $UP	G>)&fi# )'B	ff) O 
 B&%
T;	ffEO%) @
&
2)  )Yp'@
]$	ff$#$9K	 Bfi!
) ) 76$)

	$<fi!
6fi# $!C

'e ;



<

Ca j `

AF $ ,

`j  g

Dg	G
)) ) )&%46fi$)p
1#fi#$93 	@# 7+95'B#) fi!
1Sfi8!) &
)aYpU*4A^
'B!5&%=! $) 	
1$UP4&%P) n	!) &
)o&%
nU*>/	ff < @
=&%P) R	2)&
)R&%
nUP$/O
B
)
'B#) fi!
1Sfi0CPyblR$EE&%#)o)RO%
$

 CRDE%E$)Ofi)R	&%#)/

2fi9) #)n;	ffn&%Pfi!
`b	

&
J
$=)O%	IUM!dDn
1fi#@C 3 
%d>	IU&%=O
1fi#><	ff$)E&%t
a
	P&%4 d)W	
&%ff'B1SP	0!) &
)E#) 
$d19J
H%J6fi# 
Y
Y
Y&%'t1SP	0!) &
)
	ff < ?!8&%M
2&
 Y/
8;	ff
2H%h6fi# B&%Mff'B1SB	W!) &
)@!8&%"! $) 	
	&%M) K		ff  < m
2&
5
m&%:) @	#) 
$
O
Cmy;
fi#fi#9&%M) @	!) &
)
#) 
$c)&%	fffi#3	ff'@<fi# fi#93 $)&%=) 	*	#) 9:!) &
)C4eJU*B'@
a9G%
aB	#) 9
!) &
)P	o
G
1	E&%'t1So	
$ 76!
fi#fi#9"	ff < K)&
)PUPE
	EV	IU\&%
A
=ff'B1SC@DE%$O;	ff$B!3&%#)t

fi#9ff) #)4U*@
<<>	IA!'@
 B	ff
2fifi!
	)t	2<$#) #	ff819

) )O'B!"&%
2E&%	fffi#9d	#) 9M!) &
)f
>=&%	)&%
EUPA<fi## fi#93	ff  <0C*yzM&%#)
) 
UPWUP	fffi#Mfi#!V&%! $)  #	ffM1S$U*KO%W!) &
)E#) 
$3
"O%)&
)E	ff < 
 	313uMCMyz?< 
2 #"UP@) K&%
4&%#)B)@	4&%@
)CdP)&fi# )4;	ff&%M$'@
2!:;	ff

&
2)  )E
$#!dDn
1fi#)fwI`_v4	NO%f<<SApC
yz@Dn
1fi#)I`&uUPW$<	$/) 'J
 )/	2O%W<$	ff1
1#fi## #)E&%
o
%46fi# /'@
V`
) HKuE
ff
 HB
 $	$)
C \@is H"uaxE$<$)  )&%B<$	ff1
1#fi##$9X	2/&%$	UE!"	W		X
2&
@
3
31B) 'J
 

)(*

>= ff?

?A@)B ?CAB ?ADEB

=

'

fi

E	)
Z fi


u


 
ff

prv+hsvc%q+mrqhrpm6n++v+v5SIb

yb)&
)q)
$

?F@GB

wCj
C!u
vwCj
uuvCj
uCj

?wCFvCjB  ?FwDC7B 

wwCj
vCj
uavCj
uuvCjw

C7
ffwC
uC7
C7w

yb) O
)
P	ff < ?s5x
uwCj
C!u
ffCj
wwCj
vvCjv

yz) &
)E!:yb $)   #	ff
 
 
vC
uCjw
uaCjv
uCj
Cj
wCj
Cj
ffC!u
Cj
wCj

H?A@GB


I?CAB

H?AvDEuCjB v
uCj
C
vCjw
Cj

Dn
1fi#tA*EDE%) #-	0&%! >)  #	ffK	0#) 
$d
G'B#) fi!
1Sfi#X
2&
)  )`Nfi!
K	o
&

s;e
l auH`_Wx

T	#) 
Z fi


u 

 
ff

efi7/lR#fiPXuH`_W
\@s HKux
\@s HBx
C!ua
C7
Cj
Cu
Cj2
C7
Cj
C7
CjSu
Cff

5
]$	ff$#$9tlR#fi# 
\@is Htx
SC!uw
SCj
SC!u
SCjv
SCj
SC!u
SCjw
SCj
SCjv
SCj

	ff) )&)ElR#fi# 
\@is H"uax \@siHtx
SCjw
SC!u
SCjw
SCj
SCj
SCff
SCj
SCj
SC!u
SCjww

\@sHKux

Dn
1fi#tA*lRfi# <$#) #	ffG`*fi!
d	o
O


\@siH"uaxRa

? _KJMLNPOGQSRTQVUffEWRTOGJTRMLXW a ? = ?
Y>Z W[NS\p^] Z OGOG_a`bWRTQ
>Y Z W[NS\p=

\@sHBxP$<$) )T&%=<$	ff1
1#fi##$9d	/V<!G1
2M
&
@
M
M1=) !'@
 3
)(*
\@sHBxa

] Z OGOG_a`bWRTQ4cUffEWRTOGJTRMLXW
] Z OGOG_a`dW[RTQ

a

= c= ?
=

l 	E&%4fi!
`b	
&
Y&%$B
$@wds;v 	*vxP 	&
2fi0& 
!!M!) &
)aCfDT%$O+	$Y

;	ffE
B	#) =fi#fin	/
dO%	ff) )&)T6fi# Y

\@sHKuxa

 wAj5vu+7v

wmuwA7

a 7w

\@siHtxRa

uawAj5vu+7v
uA
w j

a u




Dn
1fi#)a`&uB)&%	U)'fi!
O$)CPl	P&%) 4
&
) )Y&%B$)&fi# )T+	ff&%=	ff))&)E6fi# 
)&%	U,O%
&%"<$	1
1#fi#$95	E&%>	IUT:	ff=	ff	5
&
M>'@
!)B)&'@
fi#fiRh+	%%t	)
fi#fi#)Y#fi#fi!) & 
K&%
P&%	ff) )O)P6fi# o)T	ff) $
 #!K#) 
$!@
O
CRK&%W	&%
'

fi

T	#) 
Z fi


ua


 


n+o+pqrbsm,Xn+rpq

efi7/lR#fi#  P Z 
\@s HKux \@sHBx
Cu
7C 
Cu
7C 
C7
7C 
C7w
C ff
C7
C ff

h
2]_	$#_9BlR#fi# 
\@s HBx
C7
C7
C7
C7
C7
C7
C7u
Cffw
Cff
C7u

\@sHKux

P	) )&)lRfi# 
\@s HKux \@sHBx
C7
Cff
C7
C7
C7
C7v
C7v
C7
Cu
C7w

Dn
1fi#tvA*lRfi# T<$#) #	ff`*$#E
O

T	#) 
Z fi


ua


 


efi7/lR#fi# PKq&$
\@s HKux
\@sHBx
Cj
Cuw
Cj
C7u
Cj
C7v
Cff
C7v
Cj
Cffw

 
2]_	$#_9BlR#fi# 
h
\@sHKux \@sHBx
Cj
Cu
Cj
C7
Cj
C7
Cj
C7
Cffv
C

P	) )&)lRfi# 
\@s HKux \@sHBx
Cuu
C7
Cuu
C7
Cu
Cu
Cuw
C7
C7
C7v

Dn
1fi#"uaA*nlRfi# <$#) #	ff`*$	ff
2M) ff'B&
 #	ffK
2&

T	#) 
Z fi


u 

 
ff

efi7/lR#fiPXuH`_W
\@s HKux
\@s HBx
Cjw
C7
Cjv
Cu
C!u
C7
C!ua
C7
Cj
Cff

5
]$	ff$#$9tlR#fi# 
\@is Htx
SCj
SCj
SCjw
SCjv
SCj
SC!u
SC!uu
SCj
SC!u
SCj

\@sHKux

	ff) )&)ElR#fi# 
\@is H"uax \@siHtx
SCju
SC!u
SCju
SCju
SCju
SCj
SCj
SC
SCj
SCj

Dn
1fi#"uu*RlR#fiE<$#) #	ff`*) )ff'B&
 #	ffK
O

T	#) 
Z fi


ua


 


efi7/lR#fi# PKq&$
\@s HKux
\@sHBx
Cjw
Cu
Cjw
Cu
Cj
C7
Cj
C7
Cj
Cff

h
2]_	$#_9BlR#fi# 
\@s HBx
Cj
C7
Cj
Cu
Cjw
C7u
Cj
C7
Cj
Cff

\@sHKux

P	) )&)lRfi# 
\@s HKux \@sHBx
C7
C7
C7
C7w
C7u
C7w
C7
Cu
Cuv
C7

D
1fi#KuA*nlR#fiW<$#) #	ffG`/6$)$#$9"
O

'

fi

prv+hsvc%q+mrqhrpm6n++v+v5SIb

%
0Yn&%M$)&fi# )4fi#fi!) &
 M&%
O%G<$	1
1#fi#$95	W
	ff) )&)46fi# 4fi# !X	XV<81

 
&
4#)Pfi!
$o&%
J&%='@
2]_	$#_94	 T6fi# )o;	ffR
2H%M	#) fi#fi0
>	) )E
fi#fip
&
) )CRyb

;	ff=
3	#) Kfi#fiP	EffMYn&%MPl%
)B
3ww s+fi
?	 xYw s$#&xYov s.$	ff
xYn
s) xYff
"Fs6>xnH%
	$&
2!@'B#) fi!
1SfiM!) O
)CRl	ffn&%'@
]$	ff$#$96fi# Y&%
%
	N'@
V!
 HKuf
 HB$	ff$)P#)W'B	ff>WL
fi;C 3 Afi!!"&%>T
&
=
2*	#) Y
&%M<$	ff1
1#fi##$9?	E&%)" >	ff$)tt>
H%)B
1	MCdP	ff) #$!3&%
B'@
]$	ff$#$93	 
<_+	ff'B)t1S  O%
3	ff) )O)=6fi# >)=;	ff%#ff%B	#) M 
 )aY0&%#)=)O%	IUT)=&%
=
	ff) )&)
6fi# )E<$	<) #$9@ 	UE
$@'@
V!
 Htr $	ff>)fs;$O
!@1
K
&
x/%!$)W<Sz+	 '@
W'	ff$
&%
4'@
]$	ff$#$9T6fi)nfi)) R
1#fi#$9B 	W>&
!4	ff	=
2&
ts++CC!YI'J
]$	ff$#$9f'@
V)'B	ff>`
 HKuR $	$)&xC
DT%6$
&
)%
2)
$9@7i0$W<$	6fi#CPDE%=<$	ff1
1#fi##$9M	&%$	UE!@	*	ff	M
&

$'@
2)B
fi!'B	) 	) &
=
$	))W&%Jji0$t	)@fi#fi#)C"DE%#)4)B1
)@;	ff&%#)=
2&
) 
 		"!) &
)=
<<
T 	"%
aB
"%#ff%:fi#fiR	/	#) C:&%=	&%W%
0Y&%@<$	1
1#fi#$9
&%
E
46fi# TUE#fi#fiN>&
!M1
d
2&
B$#) )f
2)E&%=	#) 4fifi0!$
) )C
+y t	ff3%
)d
8fi#	K	t
O
YP&%
\fi#I
  H"u: $	@#)d<$	ff1
1fi9\fi#) )d	t
8%! 

&%
:
Mfi#
 PHB=$	ff;CC!Y&%>	IUTK	ffE	ff	d
O
YUW%9	ffd%
aB
Bfi#	)fi#) )	) fi9
&%
:$&
!!X1
2d
&
SCWT*	$) Y	ffBU*	fi:fi#V2@ 	K)O$B&%
	ff=#)=	&%$	UE!K	ff
A< #	ff)C

e f
'

  



SI    



4S







  

DE%#)E
$ #fi#<$)  )*
<$	$T+	R#j;9K'B#) fi!
1fi#M!) &
)aC/DE%$)&fi# )/	2g
B'4`
<!$#
fiRI
2fi
2 #	ff3'B	ff)& 
 :&%
T6fi# $!d!'@<$	)fi!
) ) 76
	5
 
9+	E
O
)  )
&%
<	) )) )fi!
1fi#!d >	ff$)CPlR#fi# $!X
2fifi#	UPX

#)=
E	"&%B1
) O`bfi#!"
 
29d 	
13$&
! +	ffJ	#) dfi#fi#)M<m 	?k+	ffJ
fi#fiE
&
) )YP
[<m 	8k+	B$U*	3
O
)  )
s&%B$	ff
d
d) 4) ff'B&
 #	ff
&
) )&xC/TA<$!'B ))&%	U&%
2W
)E&%B	#) Bfi#fi
!$
) )aY&%
1#fi#$9M	0&%='BO%	ffK 	@$&
!J&%=1
) fi#!t
2 
9@$
2) )CR3	ff$	Y
)
#fi#fi)& 
 819M&%r6$=)$#$9M
&
) aY7*&%B'B&%	X)&
$ )TUE#&%M
O
@&%
2E#)	I$fi#9d	#) 9Y
#=
	;	ff '
5
2 
 46fi# CK,	'@<
$#) 	ffh	E	 !M 	6fi$!X#fi#fi!) & 
2 ?O%
&%
'@
2]_	ff>$9@	 =fi!
)) 76<z;	ff 'B31 E&%
M&%4##
fi/fi!
) )j6>)Y01WO%
E#W
	
$<fi!
T6fi# $!=UW%
&

$E	#) 9Co/$)&fi# )o)&%	Um&%
R&%E1S) *
<<>	ff
%=#)o 		ff'B1!
6fi# $!5
8	 !CK3I
fi!
	8	E&%"<$#) #	ff8	P6fi# $!3#fi#fi)& 
 mO%
	ff) )&)
6fi# $)@
$J	ff) $
 #Kc&%$	UE!3
UE
a9d	ff	5
&
M
2=&%KA^<S) K	WV<!h')fi
1fi#

&
SYpUW%$
);	ff'@
2]_	ff>$9M	 6fi# $)4&%"<$	1
1#fi#$93	*O%$	UE!M	ff		ff5
2&
"
5&%
<$	ff1
1fi##$9"	2N$&
!!"1
K
&
4
$'B	ff$TpC/o
) ='@
2]_	$#_94	 6fi$)W<_+	ff',1>`
 g	ffB
a 
o&%
4	ff) )O)n6fi$)R&%#)/)&%	UE)n&%
R$&
2!B1
B
&
%>)*<_+	ff'@

'B	ff>W&%
K&%$	UE!@	ffP		ffM
2&
r+	ffo&%)=
&
2)  )CnDE%#)W&>M)<
$ #fi!
$fi#9M!'@<	$&

UW%	ff=%
)W
M
1
=	R
&
SC
DT%=#) )&B	/ 'B!MU%&%	ffE	 	M
<<fi#9M6fi# $!M 	M
J#M
O
@) 'B) W1S
	ff) #$0Col	n&%WUP	ffV) $!1Sd%$Y&%E
&
UP$
$ 76!
fi#fi#9"	ff < 0C/DE%>O+	ff>E&%

&>
t'@
OE	&%Pfi!
1fi#!B $	ff>)0UP$EVff	UW4
<$#	ff$;CW;	ff$&
2 fi#9Y&%#)R$9^<S/	
!+	 '@
 #	ff#)f 
$fi#9GV	UWJ+	E'B	) J&$
fipUP	ff$fi#"
<<fi##
 #	ff)aCEybM)	ff'B) #&
	)Y#'@
a9
1<	) )1fi# 	=)E	ff'@
!@V	UEfi#E 	) !'@
 P&%W
'B	ffR	fi!
1Sfi	#) !@

&
)Cnl	ff
) #&
	)=UW%$B&%#)BVff	UEfi#B#)t	=
a
#fi
1fiYn&%B	ff)$I
"
&$B	P&%B	ff) )&)
6fi# P#&
)E&%
E$fi!
 #fi#9K;U)&
)UE#fifin1=#) 
$K+	/
2&
B)  )PUE#&%Kfi#	Ufi#fi#)W	
'('

fi

n+o+pqrbsm,Xn+rpq

fi!
1fi#!8 $	ffCKDE%$O;	ff$Yn&%M
<<fi##
 #	ff8	E&%#)t'&%	ff8 	3$fi!
 #fi#9?	#) J.>"
O
)  )
)&%	fffi#3	E) #ff76
 fi#9d!'@<
T&%t<Sz+	 '@
	R&%6
figfi
2) ) 76
 #	ff3<$	$C
8b&$P!$ #	ff	&%#)/$) 
>H%4UE#fi#fi1P 	AfftO%/6fi# R
<<$	ff
%4 	tOO$&afi!
1fi#!
 $	$)/!J& 
!!@
&
SC0l	RA
'@<fi#Y	ffEUE
a9B 	=	4&%#)E'B#ff%E1SW 	B$fi!
1fip!) &
)E7&%
	ff) )O)Efi!
) )P#)E7i>P&%
J&%	ff1) $Mfi
2) )CRyb) O
)P;	ffoUW%#H%J&%	ff) )O)/6fi# 
<$# )n$U*	P	ffg'B	ff$Rfi!
) ))UP	fffi#)  #fi#fi1/#) 
$0CRDE%#)!$ #	ff4#)R<
$ #fi!
$fi#9!'@<	$&

1
) =	R&%=<
#$9M	/%#ff%ML
fi##_9K& 
2!K
&
@
a
#fi
1fi4;	ffP'@
9G
<<fi#
	)C

/!M
	ff'@
 #
fi#fi#9"$'	I!=!) &
)E&%
o
	E1	ff $fi9Jfi
2) ) 76M#)E&%

&%9'B#ff%N1S/A< #	ff)	E&%R 
2fiff fi#C  %4
!) &
o#)N
A< #	ffr 	E&%R
fi

) Y#/
B
<<S
/
)RO%	ffff%B#R#)/!	ff > fi#9@fi!
1fi#0C  %J
<<fi#9ff!B %L)P 	#j;9

3fi#!'B!
 M	#) 93)&
)Yg	ffBU
 ) 	M
a	#3)
$!3	ff $ fi#9:fi
1fi#?A< #	ff)C
DE%$O;	ff$@
MV93L)  #	ff3!3!'@<>	I!d
O
KL^
fi##$93)B%	U 	M#)  !ff#)&%cAff< #	ff)4b$	ff'
	#) CE4) 	fi! #	ffM	@&%#)f<>	ff1fi#'F'B#ff%1=	@$
 !
	)  #)&%
Efi#		ffVM
E&%@R!
UW%#%M
B!) &
#)E'B#) fi!
) )j6:!@	ff>/	=  '7#P#)E
@A< #	ffJ	ffP
B >	ffC  
<fi!
: 	K) 
 =U%&%UE#&%:fi#'3+1
2HVYp	ff=
dfi#
d	@#)  !ff#)&%cAff< #	ff)
b$	ff'	#) t1
2) d	ffKO%Tfi!
) ) 76
	d1%
a#	ffW
M<;
&>I
fi!)C
DT%@A^<S$!'B )=) >1Sm!5&%#)B<
<4%
1S5	6? 	M!&$	!?	#) @! 	
&%E
2&
!"
4'@
RO%
R#)*
&
fi+	R&%W<
$ #fi!
R	ff'J
!pC/DE%#)/UE
)P) )&
$9Y1
)
;	ff4&%M
&
) )@) mUPd%
[	5UE
a95	W)&>m
3	#) O`;b$d
fi#
2 #	ffm )@) Cc V9
;	ff)P	.&>WUP	ff VrUE#fi#fi1SW	= 
2 W	#) T.>EI
fi##
 #	ffK
2&
 	) /	ffo'B&%	"	B&%
	ff$#!
fin
&
) C  t
>= $ fi#9MUP	ff V!@	ffM	1&
!!d	#) b$=
fi##
 #	ff3
O
4+	P&%
fi!
d	fi
2) ) 76
 #	ff&
)&VC

dg

  

b


R



 UP	fffi#Bfi!VT 	&%
VEO%Bql$#)n;	ffg)&<<fi#9ff!B&%EEq  yp
&
)YI
P Tq= 
<g;	ff
 P
<$	!:&%=fi!
) )	.) #	ff);	ffT&%B) @
5$	ff
:) ff'B&
	M
&
SC  B&%
VM
2 #
 #fi)	ff +	ffJ%B	ff'@'B )C  :&%
Vc&%X
	9'B	ff)">ff#UP$)M
[	ff	ffB;	ffBO%

$Obfi$
2M
:Afffi#fi#)O)  #	ff)aC*E
$fi!
@P$	fffi#9p)$) 
$%MUE
)P)&<<S	ff$ 519Gel
y$ybe`_vK
3Ee,ff 
 E=a`_wvuCR5
 VJl>fi;)=>) 
$%MUE
)E)O<<	ff$
19GTfe,Tff 
 TW=I`_uC

>h

ffh

;"<j1Xi'


 Z

ji

e<Wi'i< g ]_a

A$ Xa($

j

Dn
1fi#fuaW$<S	ff$ )n&%E$)&fi# )R	2
W<
!$:7`b ) g+	ffg&%Pfi!
`b	gfi
2) ) 76
 #	ffB
2&
) CnD
1fi)
u4 	tu$<	$/&%fi!
) ) 76
	M
 
9Y&%)&
'@<fi#) &

$@
2 #	ffM
K&%W$)&fi# )P	

K<
!$3$`b ) T+	P&%B$#=$#)&VY$	
X
:) B) ff'B&
	pY
:6$4) $#$9M
&
2)  )C
Dn
1fi#)W4 	@4)&%	U&%=$)&fi# )E	2N
4	ff'@<
$#) 	K	N'@
2]_	ff>$9B	Wfi!
) )j6
2 #	ffM 	46fi# $!Y
;	ffo&%W>$#)&VY$	ff
M
M) ) 'BO
 #	ffpYff
K6$) $#$9@
O
)  )CRDn
1fi#)Ww4 	@v
)&%	U[O%W<$#) #	ffK	0&%P6fi# >K'B&%	)R;	ffR&%$#E$#)&VYff$	ff
2"
K) W)ff'B&
 #	ffpY

K6$4) $#$9K
&
) )C

'(=

fi

prv+hsvc%q+mrqhrpm6n++v+v5SIb

T	#)  Z  fi
uH`_W
e l

3l
Pl
el
Z 
3l
Pl
q`_Dn$
el
3l
Pl


Cffw
Cj
Cffw
Cjwu
Cjw
Cjffw
Cj
C!u
Cjv


Cj
Cj
Cj
C!u2
Cjuav
Cj
Cj
Cj
Cj

u
SCj
SCj
SCj
SCjw
SCju
SC!u
SCj
SCju
SCju


Cj
Cj
Cj
Cj
Cj
Cju
Cj
Cj
Cj


Cj
Cj
Cj
C!u
Cj2
Cjwv
Cj
Cj
Cj

ff
SCj
SCj
SCj
SCjuu
SCj
SCju
SCj
SCj
SCj

Dn
1fi#"uA*E	ff'@<
$#) 	B	06fi# $!@	t	o6fi# $!Xs)&
 #)  #
fi) #ff76
xtfi!
M	o
&


 	#)  Z fi
T
&
fiT	#) 
uO`_
E	
el
3l
Pl
E	
Z 
el
3l
Pl
q`_Dn$
E	
el
3l
Pl




C!uuCj
Cj
v QCj
uCj
 Cj
uCj
 QCj
Cj
 QCj
C
 Cj
Cj
w Cj
Cj
 Cj
Cj
w wCj
Cj
w Cj
Cj
v Cjv
uCj
 QCj


 Cj

SC!uu,wC!u
SC
 ,Cj
SC
 ,wCj
SCj
w ,Cj
uCj
 ,Cj
SC
 ,Cjw
SCj
 ,Cj
uCj
w Cj
SCj
v ,wCj
SCj
 ,Cjv
vSCj
 ,C
SC!u
u Cj

u 
uC7
uCjvQCjv
SCj
w QCjw
SC
 QCj
SCj
w C!u
uCj
w QCj
Cj
w C!u
SCj
 Cj
SCj
 QCj
SCj
 wCj
wSCj
 QCjv
SCj
 QCjv
SCj
 Cj

 
uvCjw
wCwC7v
uCj
v vC7
Cj
 wC7
Cj
 C7
C
 QC7
Cj
 C7
Cj
 QC7
uCj
 QC7w
wCj
 wC7v
C
 QC7
Cj
 QC7
C!u
u wC7

D
1fiKu1*/fi
2) ) 76
 #	ff:
 
9J"$#
&


'(p

 

SC
wCjv,wC7
wCj
v ,C7
SuCj
 ,wCu
wvCj
 ,C7w
Cj
 ,C7
wCj
 ,C
Cj
 ,C7
Cj
v ,wC7
w2C!u
u ,wC
wwCj
 ,wC7
wvCj
v ,wC7v
wCj
w ,C7

 
ff
ffSCjv
vCjvC
wCj
w Cj
wCj
 QCj
wCj
 C
Cj
w Cj
Cj
 QCj
2Cj
 Cj
Cj
 C
Cj
 Cj
Cj
 Cjv
w2Cj
w Cj
wSuCj
v wC

fi

T	#)  Z  fi
uH`_W
e l

3l
Pl
el
Z 
3l
Pl
q`_Dn$
el
3l
Pl


Cjv
Cj
Cj
Cffw
Cj
C!uw
Cju
C!u
Cjuw

n+o+pqrbsm,Xn+rpq


Cj
Cj
Cj
C!u
C!uSu
Cjvv
Cjv
C!u
Cj

u 
SCj
SCj
SCj
SC!u
SCju
SCjw
SCj
SCj
SCju

 
Cjw
Cj
Cj
Cj
Cju
Cj
C!uu
Cju
Cj

 

Cj
Cj
CjSu
C!u
Cjua
Cjw
Cj
Cjw
Cff

 
ff
SCjww
SC!u
SCj
SCj
SCffvv
SCff
SCj
SCjv
SC!u

D
1fi#KuA*/P	'@<
$#) 	ffK	206fi# $!M 	@	P6fi# $!5s) &
2 #)  #
fi0) #ff76
x"$#
&


 	#)  Z fi
T
&
fiT	#) 
uO`_
E	
el
3l
Pl
E	
Z 
el
3l
Pl
q`_Dn$
E	
el
3l
Pl


 Cj

vCjwuCj
CjuCj
Cj
w uCj
uCj
v uC
wCj
w QCj
wCj
v Cj
Cj
 Cj
Cj
 Cjw
Cj
 uC
uCj
 uCj
uCj
 uCj
Cj
 uCj


 Cu

wC7&uCj
C7&uCj
uC7&
w uCj
uC7&
w uCj
wvSCj
 uuC7
wC7&
 wSCj
C7&
 SC
wC:
u SCj
C7&
 SCj
uC7&
 uC
C7&
 uCj
uC:
u uCj

u 
uCjw
uC7&Cj
C7&
v uCj
C7&
 uCjv
uC7&
 Cj
wC7&
 wCj
C:
u Cj
wC7&
 Cj
C7&
 (Cj
C7&
 Cj
C7&
 Cj
uC7&
 uC
uC7&
w Cj

 

SCjw
wwCjw,C7
vC
 uC7
Cj
v uC7
Cj
v uC7w
wuC7&
w uuC!u
Cj
 C7
Cj
w uC7v
2Cj
 ,wCu
wCj
 ,C7
vCj
 ,C7
Cj
 uC
vCj
 uC7v

 

wCj
wCjC7
Cj
 uC7w
vCj
 uC7
wCj
 uCu
wCj
v C
Cj
v C7v
Cj
 C7w
Cj
v wC7
wwCj
 C7
Cj
v Cu
Cj
 uC7
Cj
 uC7

D
1fiKuwA*RPfi!
) )j6
2 #	ffd

9K"$	ff
M) 'BO
 #	ffK
&


'(

 

SCj
Cj,C7
2Cj
 ,C7
Cj
 ,C7v
2Cj
 ,C7
vC
 ,C7
2Cj
v ,C7
wC
 ,C7
SuCj
w C7
vCj
 C7
wCj
 uC7
C
 ,C7
Cj
 ,C7

fi

prv+hsvc%q+mrqhrpm6n++v+v5SIb

T	#)  Z  fi
uH`_W
e l

3l
Pl

el
Z
3l
Pl
q`_Dn$
el
3l
Pl


Cj
Cju
Cju
Cjw
Cjff
Cju
Cj
Cj
Cj


Cj
Cj
Cj
C!uv
C!u
C!u
Cj
CjSu
Cj

u
SCj
SCj
SCj
SCju
SCjv
SCju
SCj
SCj
SCj


Cj
Cj
Cj
Cj
Cju
Cj
Cj
Cj
Cj


Cj
Cj
Cj
Cj2
CjSu
Cj
Cj
Cj
Cj

ff
SCj
SCj
SCj
SCj
SCj
SCj
SCj
SCj
SCj

Dn
1fi#"uA*E	ff'@<
$#) 	@	g6fi$!@ 	@	o6fi# $!Xs) O
 #)  #
fip) #ff76
x"$	ff
K) ff'B&
a`
	K
&


T	#)  Z fi
&
fiT	#) 
uO`_
E	
el
3l
Pl
E	
Z 
el
3l
Pl
q`_Dn$
E	
el
3l
Pl


Cj
vwCjuC!u
vCuCj
vCj
w uC
vCj
 uC!u
vCj
 C
vCj
 uCjw
vCj
 C!u
vuCj
 uCj
vCj
 uCjw
vCj
 C
vCj
 uCjw
vCj
 uC!u


Cj
vSCjuC!u
vCuC
vCj
v uC!u
vSCj
 uC!u
vuCj
 uCj
vSCj
 ,Cj
vSCj
 uC
vSCj
 uCj
vCj
 uC
vSCj
v uCjw
vC
 uCjv
vCj
v uCj

u
wSCj
vSCjvuCj
vSCj
 uCjw
vCj
 uCj
vSCj
 uCjw
vSC!u
u Cj
vSCj
 uCjv
vSCj
 uCj
vuCj
 C!u
vuC
 uCjw
vC!u
u Cj
vCj
 uCj
vCj
 uCj


uCj
wC!uuC
vuCj
 uC7
vCj
 uC7
vCj
w uC7w
vCj
 C7
vCj
w C7
vCj
w uC7v
vuCj
 C7
wCj
 C7
vCj
 C7
vCj
 C7
vC
 C7


uvSCj
Cj,C7
Cj
 uC7
vCj
 ,C7
vCj
 ,C7
Cj
 ,C7
vCj
 uC7
vCj
 uC7
vSuCj
 ,C7
SuCj
v uC7w
vCj
 ,Cu
vCj
w ,C7
vCj
 ,Cu

D
1fi#KuA*RPfi!
) ) 76
	X
2 
9J") =)ff'B&
 #	ffK
O


'(

ff
SCj
wCjCj
Cj
w Cj
vCj
 C!u
Cj
 Cj
wCj
 Cj
vCj
 uCj
vCj
 Cj
vCj
 Cjw
Cj
w C
C
 QCj
vCj
 Cj
wCj
v C!u

fi

T	#)  Z  fi
uH`_W
e l

3l
Pl

el
Z
3l
Pl
q`_Dn$
el
3l
Pl


Cj
Cj
Cjff
Cj
Cjw
C!u
Cj
Cjffw
Cjff

n+o+pqrbsm,Xn+rpq


C!uu
Cj
Cj
Cj
Cj
Cj
C!uwv
Cjua
C!uv

u
SCj
SCj
SCj
SCffvw
SCju
SCju
SCj
SCju
SCj


Cj
Cj
Cj
Cjv
Cff
Cjffw
Cju
Cj
Cj


Cj
Cj
Cj
Cju
Cjv
Cjuu
Cj
Cj
Cj

ff
SCj
SCj
SCj
SCj
SCju
SCj
SCj
SCj
SCj

Dn
1fi#"uvA*E	ff'@<
$#) 	M	o6fi$!M 	G	6fi$!s) O
 #)  #
fi0)j6
xM) @)ff'B`
O
 #	ffK
&


T	#)  Z fi
&
fiT	#) 
uO`_
E	
el
3l
Pl
E	
Z 
el
3l
Pl
q`_Dn$
E	
el
3l
Pl


wCuCj
wC!u
u Cj
wvCj
v Cjv
wwCj
w uC!u
wCj
 Cj
wCj
 QCj
wCj
 C!u
wwC!u
u Cj
wCj
 uCjw
wvC!u
u uCj
Cj
w Cjv
wCj
 uCj


uC
wSCuCj
wSCj
 uCj
SC
 ,Cj
wSC!u
u uC
wSCj
 ,C
wSCj
 C
wSCj
 ,Cj
wwSCj
 ,Cjw
wSCj
w uCj
wSCj
v uCj
uCj
 uC!u
wSCj
 uCj

u
SC!u
wSCjuCjw
wSCj
 uCjw
SCj
 Cj
wSCj
 uC!u
wSCj
 Cjv
wSCj
 Cj
wwSCj
v Cjw
wwSCj
 uCj
wuC
 uCj
wvSCj
 uC!u
SCj
 uC!u
wSCj
 uCjw


wCjv
wCjC7
wCjuC7
Cj
 uC7
wC
 uC7
wCj
 C7
wCj
w C7
wC
 C7
wC!u
u C7
wC!u
u uC7
wvCj
 uC7
uC
 C7v
wC!u
u uC7w

Dn
1fi#t N
* fi
2) ) 76
 #	ff:
 
9KB6$4
&


=


uuCjw
vCjuC7
wCj
 ,C7v
Cj
v ,C7
wCj
 uC7
wwCj
 uC7w
wCj
 C7
wCj
 ,C7w
wCj
 ,C
wCj
 uC7
wvCj
w uC7
Cj
 uC7
wCj
 uC7

ff
uSCjv
vCjvCj
wvC!u
u uCj
Cj
v uCj
wCj
v uCj
wCj
 uCjv
w2C!u
u Cj
wCj
v Cj
wC!u
u Cjv
wCj
v C
wvCj
 uCj
SuC
 Cjw
wCj
w uCj

fi

prv+hsvc%q+mrqhrpm6n++v+v5SIb

T	#)  Z  fi
uH`_W
e l

3l
Pl
el
Z 
3l
Pl
q`_Dn$
el
3l
Pl


Cj
Cj
Cj
Cjff
Cju
Cj
Cj
Cj
Cj


Cj
Cj
Cj
Cju
Cjv
Cj
Cj
Cj
Cj

u 
SCj
SCj
SCj
SCjw
SC!u
SCj
SCj
SCj
SCj

 
Cj
Cj
Cj
Cj
Cff
Cj
Cj
Cj
Cj

 

Cj
Cj
Cj
Cjvw
Cjua
C!uw
Cj
Cj
Cj

 
ff
SCj
SCj
SCj
SCffv
SC!u
SC!uw
SCj
SCj
SCj

Dn
1fi#tu+*RP	'@<
$#) 	ffK	26fi# $!K 	@	P6fi# $!?s)&
 #)  #
fip) #ff76
xTJ6$
&


E	)
Z fi

X%
SCj
SCj
vSCj
SCj
SCj
wSCj




u 

 
 
ff

T	B6fi#  
<
Z 
SCj
 Cj

uCj
 Cjw

uCjw
 Cj

SC
 C!uu

SCj
 C!u

SCjw
 Cjw


5
]$	ff$#$9B6fi# 
3%
<
Z 
Cj
2Cjw
Cj
SuCj
Cj
Cj
Cjv
Cj
C!uua
Cjv
Cj
Cj
Cjv
Cj
Cjvv
Cj
2Cj
Cj

P	ff))&)P6fi# 
3%
<
Z 
C!u
Cj
C!uau
C!u
uCjw
C
Cj
Cj
C!uu
Cj
uCj
Cj
Cj
Cjv
Cjw
wCj
Cj
Cj

D
1fiBA*RP	ff'J<
$#) 	ffK	n6fi# $!K 	B	 !B">
&


E	)
Z fi



u


 
ff

3%
Cj
uCj
vCj
Cj
Cjw
wCj

E	6fi
q`_D$
Cj
Cj
Cj
wCj
wwCj
vCj

<
SCju
SCj
SCj
SCj
SCj
SCj

X%
SC!u
SCj
SCj
uCj
SCj
vSCj

5
]$	ff$#$9B6fi#
q`_Dn$
SuCj
Cj
SuCj
Cj
Cj
C



<
Cju
Cjw
Cj
CjSu
Cju
Cju

	ff) )&)P6fi#
3%
q`_D>
Cj
C7
Cj
uCu
Cj
uC7w
uCj
vC7
vCj
C7
Cj
C7

Dn
1fi#tA*RP	ff'@<
>)	ffK	n6fi$!" 	J	 !B"$	ff
2M) ff'B&
 #	ffK
2&


=



<
Cju
Cju
Cj
Cj
Cj
Cj

fiE	#) 
Z fi

3%
vwCj
vwCj
vCj
vC!u
vCjv
wCj




u 

 
 
ff

T	B6fi# 
uO`_
vwSCj
vSCj
vSCjv
wSC!u
SCj
wSCj



n+o+pqrbsm,Xn+rpq

<
SCj
SCj
SCj
SCj
SCj
SCj

5
]$	ff$#$9B6fi#
3%
uO`_
vC
v2Cjw
vCj
v2Cjv
vCj
v2Cj
vCjv
vCj
vCj
vCj
vC!u
vCj



<
C!uw
C!uu
Cj
Cju
Cj
Cj

P	ff))&)P6fi# 
3%
uO`_
<
vwCj
vCj
Cj
vwCj
vCj
C!u
vwCj
vCj
Cj
vCj
vCjw
Cj
vCj
vCj
Cj
vuCj
Cj
Cj

Dn
1fi#tC*N	ff'@<
$#) 	K	6fi# >K 	@	 !B") 4) ff'B&
	@
O


E	)
Z fi



u


 
ff

3%
wwCj
wCjv
wCjw
wCj
wCjv
wCjv

E	6fi
q`_D$
wCj
wCjw
wuC
wC!u
wCj
wCjv

<
SCj
SCj
SCj
SCj
SCj
SCj

5
]$	ff$#$9B6fi#
X%
q`_Dn$
uCj
Cjw
uCj
SuCj
uC
Cj
uCj
SuC
uC!u
Cj
uCj
SuC



<
C
Cj
Cjw
C!uav
Cjv
Cjw2ff

	ff)  )&)P6fi#
3%
q`_D>
wvC!u
wC7
wvCjv
wC7
wvCj
wC7
wvCjv
wCu
wvCjv
wC7
Cj
wC7w



<
Cj
Cju
Cj
Cju
Cj
Cju

D
1fiBA*RP	ff'J<
$#) 	ffK	n6fi# $!K 	@	t6>=
&


T	#) 
Z fi


u


 
ff

yb)&
)q)
$

?A@GB ?uffCACjB  ?AwDECB 

uC
uC7
uvC7
Cu
wC7

uauCj
SuCjv
2uCj
C!u

Cj
Cjv
uuCj
uwSCj

yb)&
)
P	  < ?s5x
Cj
wwCj
uavCj
SuCj
vCj


yb)&
):yb $)  	
 
 
wC!u
wSCjw
SuC!u
ffvSCj
vSuC!u
SCj
uC
uuwSCjw
uCj
uffSC!u

H?A@GB

H?CAB

k?FuDCjB 

C!u
Cj
wCj
wCj

Dn
1fi#twA*EDT%") # -:	E&%M! >)  #	ff8	W#) 
$[
['B#) fi!
1Sfi
O
)  )r`=$#@
&

s.ela Z x

=

fi

T	#) 
Z fi


u


 
ff

prv+hsvc%q+mrqhrpm6n++v+v5SIb

yb)&
)q)
$

?A@GB ?ffSCAuCjB  ?AwDEuCjB 

vCjw
wCj
vCj
vuaCj
uaCjv

Cj
Cj
wSuCj
vwCj

C
Cjv
SuCj
Cjv

yz) &
)
P	ff  <?s5x
v2Cj
uvCj
uaCj
wwCj
v2Cj

yb) O
)E!dyz >)   #	ff
 
 
vSCj
Cjw
uCj
uC7
vSCj
C7
uSCj
ffC7
SCj
C7v

H?A@)B


k?lCAB

H?ASDEuCjB w
uwCj
Cjv
wCjw
ffCj

Dn
1fi#tA*EDT%=) #-=	2N&%4! >)  #	ffM	2N#) 
>h
d'B#) fi!
1fi#8
&
) )/`E$	ff
M)ff'B`
O
 #	ffK
&
Ms;elaQq`_Dn$x

T	#) 
Z fi


u 

 
 
ff

yb)&
)q)
$

?A@GB ?uaCACjB  ?ADECjB 

u C7
wC7w
ffC
uC7
wwC7

 Cj

wCj
vCj
wCjw

uSC!u
uvSCj
Cj
vSCj

yb)&
)
P	  < ?s5x
Cj
uaCj
vCj
Cj
Cj

yb)&
):yb $)  	
 
 
Cj
Cj
uCj
uCj
uuSC!u
vvCj
uvCjw
uSCjw
u C7
Cjw
SCjv
 C7
vC!u
wSCj
w C7

H?A@GB


H?CAB

k?FDB

Dn
1fi#tA*EDT%W) #-	n&%! >)  #	ffK	0#) 
$3
M'B#) fi!
1fi#3
&
) )R`/) )ff'B`
O
 #	ffK
&
Ms;elauH`_Wx

E	)
Z fi


u


 
ff

yb)&
)q)
$

? @GB

uwvCj
uCj
uCj
uCj
uC!u

? CFB

uauCj
uaCj
uauC!u
uauCj
uauCj

? DB

vC7
uCu
C7
C7
vC7

yb) O
)
P	ff < ?s5x
Cj
uCj
ffCj
ffC!u
wuCjv


yz) &
)E!:yb $)   #	ff
 
 
uC!u
ffC7
Cjv
vCj
vC7
vCjw
uavC
uvCj
u Cj
wwCj
Cj
 Cj
Cjv
Cj
u Cjw

H?

@GB

I? CAB

H? DEB

Dn
1fi#tvA*EDT%=) #-B	/&%4 $) #	ff:	/#) 
$5
3'B#) fi!
1fi#8
&
) )P`P6$=
O
ds;el
aQq`_Dn$x

=

fin+o+pqrbsm,Xn+rpq



Sm

3 &

 
  




^n

%
Ynq@C!YR1fi#Yoq@C!YRWfi!1>Y/mCPs$uvvuaxCMyb) &
O`_1
) mfi#
 !?
fi#	$#&%'B)C 3^fi
OOY ns$uxYawwC

qp

o

n

fi!pYnq@C!Y Z 
!$0YRnCRs$uvxC Z 
 !Mb$	ff'	#) 93A
'@<fi#)C 3^fi
ffa^C

jn

o

"r

&OY RsffxY

ts

uGrYnua

P
!pYmC!Y(hfi# 	ffpYepCRs$uvvSuxCRE	`_'B	ff 	ff#=fi#
 !C 3^fi O;#j&aY
uC

^swvxvyv

o!V )) 	ffpY%CYNeUE
!pYCEs$uavvxCKP	ff) )O)tO%	ff$ #Kfi
2) ) 76
 #	ff\'B&%	)C
_; aH;~4 X /18 /+4O;Y NsxYwwa^C

|{

z

}~n d} |

rr

Sn

P$	fi9YrC 3 Cffs>uvvxCIP>) #E
 	ff'@
2 #/1
2)) fi# #	ffr;	ffpfi!
) )j6n	) &  #	ffpC 3^fi
OOY Ywa^vC

ra

o

P$	fi9YoC 3 C!Y0lp$#fi;YRmC0BC*s$uvvw
xC@y.j;98
?fi#!'B!
 !?')fi
1fi#[&
!
)&
)CEyz P$O&T8 tfiff
fiffO;&afi f; o $& O; P &;5
ff&aYff<<pCpvva<n	$ fi!
0YCWfyo$) )C

c





z

y 



s

P$	fi9YC 3 C!Yol>fi;YPmCRBCWs$uvvw1xCmyb'J<$	ff!m
 	ff'@
2 ?fi!
\	@'@
<<!m19
 7;9ff!t
tfi#!'B!
 !@'B#) fi!
1SfiM	1) $I
2 #	ff)n.>	ff'O 
!!
&
Cyb P_a&OT8
 " fiff &;O; E&2HaaO@18 P(B; a   ^2HBY  	fi;Cy$y YR<<pC/uaa
u Z !	fi!pY0WC

 s



P$	fi9Y0rC 3 C!Y{T 	i/YAnC
ffa^SC

3

|

{

F

{ 

n

Cs$uvvxCn5fi
$!
 B#) #	ff3O$)C X6fiff

o

uY

OOY

ln



o

E
>YrCffs$uvvxC(T) !W#) #	ff&$)0 	T'J<$	/
)O`_1
) =fi#
Cffyb X6fiff O&
*$O&T 8 tfiff fi &;O; o $&aYff<<pCa^4'@%$) YSXBC3	ff$ff


.'J
pC





z

As

 

q=
9fffi!VYBC!Yp{o$		) YlECRs$uvvxC/e'@
2fifiR#);]  )!3
 #	ff* Z 
 !M 	@!
ff	)t$	ff$)
:&%4 fi#<%	ff"$UP	ff VJfi#	ff
fiRfi#		ff<pCyb X^fiff OO P$O&T8 @fiff afi
&;a&; / Ha$OY<<pCpuO'@%$) aY3tC3	ff>ff
K
.'J
pC

s

n

~ 



o



z

q
&Y*epCs$uvvwxC Z 
 !8!%91$#	#) :!$	ff'B )M) !m) &
 #) 
2fi*L$#)Cyz
lR)O%Yq@C!Y Z -Y  C0s 3 )CjxY O& H$+ ; O; P &;#jO18 ;;T5
; nY<<pC0uauSCffe<$!  $fi!
YTUQR	ff VC

  P

o



s

F{

q
&YepC*s>uvvxCffR
@fi#
3UE#&%c	ff) O
>`_<
$ # #	ff3fi!
)) 76
 #	ffm	#) M
?
<<fi##
a`
 #	ff)E 	@#) #	ff:&>=! #	ffpCEyb X6fiff &O P$O&T8 tfiff ffO;&afi
&;a&; / Ha$OY<<pCpavuW
)&%ff#fi#fi#Y0DE4C3	ff$ff
K
b'@
pC

s

n

~ 

b

o

 l

qOb$#)YCepCYD0	UW)O%0YA%CC@Cs$uavvffxCnEq  y `b>dfi!
d	ofi!
) )j6
2 #	ff)=
E

fi#	ff1
fip) 
fi#C &;a&; ffO0 P( B; affY s>uxYwa^wC

s

E

t

{

ua

q#  $#%pYDrC^JCs$uvvxCgf<<$	A!'@
 ) O
 # )  #
fi0)  )P;	ffo	ff'@<
>K)&<S$ff#) 3fi!
) )j6
I`
 #	ffKfi#
 !G
2fi	ff$#&%'B)C faff$ o+ ff;;Y Ns;xYpuvapuvC



 

u)

=

fiprv+hsvc%q+mrqhrpm6n++v+v5SIb



q#  $#%pYWDCP@CY/*
V!$;YWJCWs$uvvxCe	fi#!['tfi# #fi!
) ):fi#
 ![<$	ff1fi#'B)M!
8 $	ffz`
	ff $ !@	&<E	)C ffO r&; *a &;#jO *HO$^fiY YwawC



>

Es

j

qr

F

q	UPYq@C Z C!Y ) fi;Y=Cs$uavv
xC[#) #	ffJ&$E'B	fi	1)&%6$W
2 #ff#$9Cyb P_a&OT8
 Efiff  Ifi H;$#  / $aO* &; *a &;#jOYa<<pCffI^vC  	ff$fi#
e 76C

 j{  t



x 

A

as

q	UPYq@C Z C!Yp ) fi;Y4C/s$uvv1xCE#) #	ff5O$t'B	fiR	*1)&%6>"
 #$9C=Dg%pC0$<pC
D0%#
fiW$<S	ff$"vuvYg3	ff
)&%#$) #$9Y/3fi!1	 YPf)& 
fi#!
C?DT%)J H%#
fi
$<	ff>M	ff $ )K&%?
$ #fi#?
<<
$![!&%?e#A&%) & 
2fi!
%	!d	ff+>5	ff
f$j6!
finyb fi#fi#C

Esa#&;SHY

q	UPYq@C Z C!Y ) fi;Y=Cs$uavvffxCq#) #	ffB&$E'B	fi#)/	1)O%6$W
 ##_9C
s;xYuO^SC



q= 
<SYCs$uavvxC"n$) 	ff
2fi	'@'B
2 #	ffpC!C
q= 
<SYffC!YP	fi#fi#!)YrC!YffP$	fi##	Y%C!Y  
) 	ffpYtCYffQP#) '@
pY 3 Cs$uavvxCDE%fe%'@
4e9ff) 'MC
&;a&; ffO0 /+ ; Y YvI^C

s

E

q=
YCBCY  
$YnC
e	ff)YTtC

A  | r
C0s$uvxC~*;;a&V/#O P&;MC8A{p&aff4#a!C,%	%

3

#fi#9


l
a99ff
0Y14CCYyb 
;YnBCCRs$uvvxCE:&%t%
fi#!d	2/	ff	ff)$`b
fi!
2 &$!1 )!
)	dO$ 
	pC X6fiff OOffY s>uxYIuC

n

o



=
't1S$Yq@C!Y Z 
 
 Y4C!YqK- $	)&V;YepCNs>uvvwxCEE	#) Bfi#!'B!
 #	ff3!:! #@	ff<
fi
 !1
 *,
2) B) &93!5'B#
fiR!
ff	))aC@yb p(7afi &;O; M afiff 
 ffO fit &O
fiff&&Y<<pC0uavva^uaCffe<$!C



o

V{

z

s

>



=9	ffpY*yC!YP5
 #YE4C!YE  
<!VY  CWs$uvvwxCmq#) 	>8!+	 '@
 #3<
  )G
\
&

fi
Cyzml
a99ff
0YG4CRmC!YR!
 )&V9`_e%
<!$	YEJC!Y/e'49&%pY`nC!YP T&% 
)&
'=9YoC
s 3 )C7xY 8+7&* =P#T 8ff !O+7a&418 ; XY<<pCuSuO^SCWy2hyzD
/$) )aC

a



j

 n

^svyvyv z

 
) pY Z CBCYe
fi!
'B	ffpY"CPs$uvvxC"T 
fiP$U*	 VM) 'B1fi#)C
*;;a& #W18 3^fi &;7ffaOY Ns>uxYvvIuuC





n

s

%	ff%pYRJC

uGr



$a;St

V
A n

 CPs$uvvxCt	ff1) B#) #	ff5O$)(*BP'B	ff!:	ff fi##$)4.$	'
&
CKyb P_a&OT8
 =fiff /  &;O; / Ha_aO =P#T8ff !&7O@C8 ; 3 ffY<<pC
uuav3	ff&$
fi;Y1Cpfy /$) )aC

  ~s

 

|



 ) fi;Y=C!YPq	UPYoq@C Z Cs$uvvxCo$# !1)&%6>h
2 #ff#$9m![O%d5
fi#fi#h>#	ffm	
&%=	ff$&%UP)   # 	$!
") !M
B#) #	ff:&$='B	fi;C*yz P_a&OT8W tfiff &SH;;ff;B
;$# EO ff$ Cfiff / Ha_aO/3fi!1	 Y) &
fi#
SC



c

X

^

A 

 s



Z UE#)Yq@C!Y\E
 fi# Y~%Cs$uvv2ffxC   $		)N$O
!$9)&
'J<fi!r+	ff)&<$#) @fi#
 !C
yb X6 fiff O& *$a&O8* =fiff P~7afi &;a&; / Ha_aOY<<pCpuffa
uwBEUQ*) UE#HVYpk%CSX	$ff
K
b'@
pC

n

o



 v
=('

s

 

fi

n+o+pqrbsm,Xn+rpq

Z 	)aYffepCffBCY+%ff) YCBC!YffQDnHVYffC%Cs$uavvffxCfi	1
fi0uEff>f19MuEff$Eq  yn
&

) R;	ffRfi#!'@
) &#)P$#K.>	ff'&%!'@')/	ff !O
fi0Eq  yn
&
C &;O;
^O0 *( B; SHY ns$uxYffvauvC





l{

s

qua

5
 &%UT)Y 3 CRs$uvxCPfi#	ff1
fig&
	d
:fi
h)+*WTU %#ff%3$) 
	 fi	d
2&
@1
) );	ff
fi!'@
2 =) O)aC ^ffOn /#B;B18
#T8 3;O$# IY YffffC



A

 An

rar

3#H%#Yq@Cs$uvvxCo$	ff1fi#'B)o		ff'J< z`_
#K	ff<R;	ff '@
2 # 	ffpC!Cyb@!fi!
pYA%CffCs 3 0
 CjxY
pO;
^O a;( 4HY  	fi;CSCff#) 	ff`  )fi9Y  	ffV!ff%
'MY1BC

a

vy x{
Tfi#) )	ffpY4CA%Cns$uvwxC o O&B^fiffHCX4 
aUP` 

fi#fi;YTUR	 VC


 )aY^DrC!Y %) pYq@C0s$uvvxC0DE%Oi0 )P	n& 
 !!@) E)-	K)	M&$	ff'@<fi#A#_9C
yb 3^fi OO P_a&OT8" dfiff 0&;Ofi &;O; / $&aY/<<pC
^w=
)&%ff#fi#fi#YDT=C3	ff>ff
K
.'J
pC

n



o

 ^

s

ff 

V
SY*4C!YRR	)&%#
YEBCEs$uvvxC Z  
 !$fi
@
m! $fffi!
A^
'J<fi)K) <

 fi#9C?yz
*$O&T 8 d fiff
&;O;  / Ha$OM faff$ ;RY
<<pC0uuOpuCy 3G3 3 o$) )C



 uaGVswvyvxvs



 



x

V
SY=C!Ya[R	)&%#
YBC^s>uvvwxC[	#) O`b	fi# 
 N%9^1>@'B	fi	
Pfi#	ff1
fi
B
Efi#	
fifffi#

'B	fffi;Cyz *$a&O 8R * fiff
5 MTfiff &;aff$; 3ff; # O&8 X(8
H T p_
 7M1
 8 O# 3^ fi OO
   ff&fi4HY<<pCpvauaCfy/>) )C



 js ff

F{

 x>js apF
An
o

Xs
t

ffn  o

n

fi#) 	ffpYA%C!Y  
  )aY+%C!YWfi#fi##) 	ffpY Z C0s$uvxC0E
1	ffJ!Kfi#&
 #	ffJ	N'J
]$	ffRUP	ff$fi#K	) 9)$`
 'B)CPD0%pCp$<pCDg%
2fiNP<S	ff$  `_ffa` 3 `_wSYeMq<
$&'BE	 3 $9Y
V
P#t
 #	ff
fi Z 
1	 
 	ff>9C

xn

!fi!
pY%CCns$uvwxCyz #	ff:	N#) #	ff:&>)C X6fiff

o

qu

OOY ns$uxYuOpuwC

EaP$ ff$+4B6fiff3#&OCm3	ff$ff
m
.'J
pYEe


!fi!
pY%C*rCWs$uvvxC
h
2 	YfftC





j

z

!fi!
pY+%CaC^s>uvvwxCP
!Y1		)  !W
tPC7CIyz P_a&OT8 *fiff fiffO;Ofi ;
/ $aO &; *a O;#j&aY<<pCa^n	ff$ fi!
0YCWy/$) )aC

 

e%
<!$YrC

3



s

jn

Cs>uvvxCDE%) O$&%K	RUP
VBfi#
 
1fi##$9C X6fiff

o

q

&OY Yguva^C

eV
2fi
VYpq@Cs>uvvffxC`o$	 	$9<=
+
2&$=)fi	X19M)&
'J<fi!d
5 
	ff''B&
 #	ff:%#fifi
fi!'B1!G
2fi	ff$#&%'B)CRyz X6fiff O& P$O&T8E =fi P#(7afi &;O;
/ $aOY<<pCpva^uWTUP ) UE#VYpw
 %C3	ff$ff
"
b'@
pC

|n

 

x

o

 tv

s

eV
2fi
VYPq@C!YoP#) ) fi!
0Y 3 CWs>uvvxCyz #3fi#
 !?!
5'B#A<

#ff')  !C[yz
*$O&T 8W =fiff Pjfifffi ; o $& r&; *a &;a#7ff&aY<<pC2ffa^
/	)  	ffpY3tC3	ff$
M
b'@
pC



 v



 



s





efi#	ff
pY=rCs>uvxCD/9<):	"	#) 8
&
+	K	ff<3fi
 !Cyb *$a&O8d ?fi
/H MT fiff 3 o+  ff;; && fiffOOIY<<pCvSuO^vwET
'B1$#Y3tCW
/$) )aC

 

X  

o

z

=(=

fi

prv+hsvc%q+mrqhrpm6n++v+v5SIb

s 

V

efi#	ff
pY/rC*s$uvvxCKl	$9^<S)B	W	#) M!5
O
K;	ff<ffi#
 !C & TB; *$OOH
a;;HY Ps;xY0uaauwC



o

~

e'49ffO%pYnCs$uavvwxCpo	ff)P	ffB&%'B
fi
2) ) 76
 #	ffK >	ff/ 
2 E	'Bfi# !<fiA<> )C *;;O
*O&ff; ;;Y Y0uauC



ud

o

e$!#I
)O
pYptCY5fi# 	ffpYpepC!Y*
2pYpmCs>uvvxCRq#)  !ff#)&%!dA< #	ff)b$	ff'	#) 4!
	ff`_'B	ff		ff#tfi#
C"yb P_a&OT8= Kfiff p&OC8 &18;7   P$ ff$+B
M a fi Y<<pCpvaua=Dg	ffV9	
Y %ff
<
pCybEDs;DgH%#
fiR$<	$EDP:`&uuxC





X

 A{

s

o 

xswvyvxv z

Dg	ff'BVY^yC0s$uvwxCJA^<S$!'BUE#&%J# d
$) >`_%ff1S	ffE fi#C
aH;~ 4 X3C
 8 ot 4Oa;HY s.wxYffaffC


p
#)&1S$YpepCs$uvxCa
 #8K#&W$zff$OHC,%	ff%



{

}~n

$;S

fi#9de	ff)aC


swvxvyv



#fi)	ffpYq@Cs$uavxCW)9^'@<	 #W<$	ff<S$ #)/	2
$)/%ff1S	ff/ fi#)P) !=# @
O
C
_ p a;( 4 XX18 /+4 a&;Y YaffffSuC


#fi)	ffpYq@CCffs>uvvxC 8+7&/B;O^5d4 2HT8E#&&W ff&fi4HC/%pCq@CO&%))aY*>%
'
R	ffff
 $) #$9YKo$		YDC


#fi)	ffpYq@CIrC!Ym5
> !-YDCCs$uvvxCyb)&
*<H%#L)Cyb X6fiff O&
*$O&T 8= " fiff 0&;Ofi &;O; o $&aY<<pCgffuuBW
)O%#fifi#YRDE4C
X	$ff
K
b'@
pC


#fi)	ffpYq@CC!Y5
$-YDCCps$uvvvxCP #	ffJ H%#L)o+	ffnA'@<fi!
z`_1
2) "fi#


fi#	ff$#&%'B)aC 3^ fi OOffY $&C


#fi)	ffpYCalEC!Y  $) 	`_efifi#$)aYtCs>uvxCfffi#	ff1
2fi
$%E	fi!
J	IR
@) 	#fi#)P
&

+	ffP) 4!M 
2fi!$fi!
	X'	fffi#)C ^ffOg /#B;# IY YguuvauffC


!)  ff
	 pY1nC  Cs$uavxC Z 
 !M) O 
 & 
 fip) ><	).$	',A
'@<fi#)CPyb 
s 3 0CjxY fiff /I6fiff# B / pff
 ;a n!C34 
aUP`  #fi#fi;YTUR	ffVC

z

T |{

T



}~n

 

|

r

s

yn

o

y 

o



F



z 

Fn



 ff 

ff



"

) 	ffpY1nC  C

"

 	fi!<S$Yq@C  Cs$uavvxCne&
VM 
2fi#-
	pC $ ;RY YuH^vC
 %
	YBC!Y0T#)&%#
YDCRs$uvvxCE) !ML
fi##&
 #@%9^<S	&%) )4 	"# 7+93!
 
2 B
&
C
^O0 &; *a &;a#7ffaO P&$6fiffY Y0uuvIuffC



>

s

ff



=(p

fiJournal of Artificial Intelligence Research 11 (1999) 241-276

Submitted 1/99; published 9/99

Evolutionary Algorithms for Reinforcement Learning
David E. Moriarty

moriarty@isi.edu

University of Southern California, Information Sciences Institute
4676 Admiralty Way, Marina del Rey, CA 90292

Alan C. Schultz

Navy Center for Applied Research in Artificial Intelligence
Naval Research Laboratory, Washington DC 20375-5337

schultz@aic.nrl.navy.mil

John J. Grefenstette

Institute for Biosciences, Bioinformatics and Biotechnology
George Mason University, Manassas, VA 20110

gref@ib3.gmu.edu

Abstract

There are two distinct approaches to solving reinforcement learning problems, namely,
searching in value function space and searching in policy space. Temporal difference methods and evolutionary algorithms are well-known examples of these approaches. Kaelbling,
Littman and Moore recently provided an informative survey of temporal difference methods. This article focuses on the application of evolutionary algorithms to the reinforcement
learning problem, emphasizing alternative policy representations, credit assignment methods, and problem-specific genetic operators. Strengths and weaknesses of the evolutionary
approach to reinforcement learning are presented, along with a survey of representative
applications.

1. Introduction
Kaelbling, Littman, and Moore (1996) and more recently Sutton and Barto (1998) provide informative surveys of the field of reinforcement learning (RL). They characterize two
classes of methods for reinforcement learning: methods that search the space of value functions and methods that search the space of policies. The former class is exemplified by
the temporal difference (TD) method and the latter by the evolutionary algorithm (EA)
approach. Kaelbling et al. focus entirely on the first set of methods and they provide an
excellent account of the state of the art in TD learning. This article is intended to round
out the picture by addressing evolutionary methods for solving the reinforcement learning
problem.
As Kaelbling et al. clearly illustrate, reinforcement learning presents a challenging array
of diculties in the process of scaling up to realistic tasks, including problems associated
with very large state spaces, partially observable states, rarely occurring states, and nonstationary environments. At this point, which approach is best remains an open question, so
it is sensible to pursue parallel lines of research on alternative methods. While it is beyond
the scope of this article to address whether it is better in general to search value function
space or policy space, we do hope to highlight some of the strengths of the evolutionary
approach to the reinforcement learning problem. The reader is advised not to view this
c 1999


AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiMoriarty, Schultz, & Grefenstette

article as an EA vs. TD discussion. In some cases, the two methods provide complementary
strengths, so hybrid approaches are advisable; in fact, our survey of implemented systems
illustrates that many EA-based reinforcement learning systems include elements of TDlearning as well.
The next section spells out the reinforcement learning problem. In order to provide a
specific anchor for the later discussion, Section 3 presents a particular TD method. Section 4 outlines the approach we call Evolutionary Algorithms for Reinforcement Learning
(EARL), and provides a simple example of a particular EARL system. The following three
sections focus on features that distinguish EAs for RL from EAs for general function optimization, including alternative policy representations, credit assignment methods, and
RL-specific genetic operators. Sections 8 and 9 highlight some strengths and weaknesses
of the EA approach. Section 10 briey surveys some successful applications of EA systems
on challenging RL tasks. The final section summarizes our presentation and points out
directions for further research.

2. Reinforcement Learning

All reinforcement learning methods share the same goal: to solve sequential decision tasks
through trial and error interactions with the environment (Barto, Sutton, & Watkins, 1990;
Grefenstette, Ramsey, & Schultz, 1990). In a sequential decision task, an agent interacts
with a dynamic system by selecting actions that affect state transitions to optimize some
reward function. More formally, at any given time step t, an agent perceives its state
st and selects an action at. The system responds by giving the agent some (possibly zero)
numerical reward r(st) and changing into state st+1 =  (st ; at). The state transition may be
determined solely by the current state and the agent's action or may also involve stochastic
processes.
The agent's goal is to learn a policy,  : S ! A, which maps states to actions. The
optimal policy,  , can be defined in many ways, but is typically defined as the policy that
produces the greatest cumulative reward over all states s:

 = argmax
V  (s); (8s)


(1)

where V  (s) is the cumulative reward received from state s using policy  . There are also
many ways to compute V  (s). One approach uses a discount rate  to discount rewards
over time. The sum is then computed over an infinite horizon:

V

(

1
X
s ) =  ir

t

i=0

t+i

(2)

where rt is the reward received at time step t. Alternatively, V  (s) could be computed by
summing the rewards over a finite horizon h:

V  (st) =

Xh r
i=0

t+i

(3)

The agent's state descriptions are usually identified with the values returned by its
sensors, which provide a description of both the agent's current state and the state of the
242

fiEvolutionary Algorithms for Reinforcement Learning

world. Often the sensors do not give the agent complete state information and thus the
state is only partially observable.
Besides reinforcement learning, intelligent agents can be designed by other paradigms,
notably planning and supervised learning. We briey note some of the major differences
among these approaches. In general, planning methods require an explicit model of the
state transition function  (s; a). Given such a model, a planning algorithm can search
through possible action choices to find an action sequence that will guide the agent from
an initial state to a goal state. Since planning algorithms operate using a model of the
environment, they can backtrack or \undo" state transitions that enter undesirable states.
In contrast, RL is intended to apply to situations in which a suciently tractable action
model does not exist. Consequently, an agent in the RL paradigm must actively explore
its environment in order to observe the effects of its actions. Unlike planning, RL agents
cannot normally undo state transitions. Of course, in some cases it may be possible to
build up an action model through experience (Sutton, 1990), enabling more planning as
experience accumulates. However, RL research focuses on the behavior of an agent when it
has insucient knowledge to perform planning.
Agents can also be trained through supervised learning. In supervised learning, the agent
is presented with examples of state-action pairs, along with an indication that the action
was either correct or incorrect. The goal in supervised learning is to induce a general policy
from the training examples. Thus, supervised learning requires an oracle that can supply
correctly labeled examples. In contrast, RL does not require prior knowledge of correct
and incorrect decisions. RL can be applied to situations in which rewards are sparse; for
example, rewards may be associated only with certain states. In such cases, it may be
impossible to associate a label of \correct" or \incorrect" on particular decisions without
reference to the agent's subsequent decisions, making supervised learning infeasible.
In summary, RL provides a exible approach to the design of intelligent agents in situations for which both planning and supervised learning are impractical. RL can be applied
to problems for which significant domain knowledge is either unavailable or costly to obtain.
For example, a common RL task is robot control. Designers of autonomous robots often
lack sucient knowledge of the intended operational environment to use either the planning
or the supervised learning regime to design a control policy for the robot. In this case, the
goal of RL would be to enable the robot to generate effective decision policies as it explores
its environment.
Figure 1 shows a simple sequential decision task that will be used as an example later
in this paper. The task of the agent in this grid world is to move from state to state by
selecting among two actions: right (R) or down (D). The sensor of the agent returns the
identity of the current state. The agent always starts in state a1 and receives the reward
indicated upon visiting each state. The task continues until the agent moves off the grid
world (e.g., by taking action D from state a5). The goal is to learn a policy that returns
the highest cumulative rewards. For example, a policy which results in the sequences of
actions R; D; R; D; D; R; R; D starting from from state a1 gives the optimal score of 17.
243

fiMoriarty, Schultz, & Grefenstette

a

b

c

d

e

1

0

2

1

-1

1

2

1

1

2

0

2

3

3

-5

4

3

1

4

1

-2

4

1

2

5

1

1

2

1

1

Figure 1: A simple grid-world sequential decision task. The agent starts in state a1 and
receives the row and column of the current box as sensory input. The agent moves
from one box to another by selecting between two moves (right or down), and the
agent's score is increased by the payoff indicated in each box. The goal is to find
a policy that maximizes the cumulative score.

2.1 Policy Space vs. Value-Function Space
Given the reinforcement learning problem as described in the previous section, we now
address the main topic: how to find an optimal policy,   . We consider two main approaches,
one involves search in policy space and the other involves search in value function space.
Policy-space search methods maintain explicit representations of policies and modify
them through a variety of search operators. Many search methods have been considered,
including dynamic programming, value iteration, simulated annealing, and evolutionary
algorithms. This paper focuses on evolutionary algorithms that have been specialized for
the reinforcement learning task.
In contrast, value function methods do not maintain an explicit representation of a
policy. Instead, they attempt learn the value function V  , which returns the expected
cumulative reward for the optimal policy from any state. The focus of research on value
function approaches to RL is to design algorithms that learn these value functions through
experience. The most common approach to learning value functions is the temporal difference (TD) method, which is described in the next section.

3. Temporal Difference Algorithms for Reinforcement Learning
As stated in the Introduction, a comprehensive comparison of value function search and
direct policy-space search is beyond the scope of this paper. Nevertheless, it will be useful
to point out key conceptual differences between typical value function methods and typical
evolutionary algorithms for searching policy space. The most common approach for learning
a value function V for RL problems is the temporal difference (TD) method (Sutton, 1988).
244

fiEvolutionary Algorithms for Reinforcement Learning

The TD learning algorithm uses observations of prediction differences from consecutive
states to update value predictions. For example, if two consecutive states i and j return
payoff prediction values of 5 and 2, respectively, then the difference suggests that the payoff
from state i may be overestimated and should be reduced to agree with predictions from
state j . Updates to the value function V are achieved using the following update rule:

V (st) = V (st) + ff(V (st+1) , V (st) + rt)
(4)
where ff represents the learning rate and rt any immediate reward. Thus, the difference in
predictions (V (st+1 ),V (st )) from consecutive states is used as a measure of prediction error.
Consider a chain of value predictions V (s0 )::V (sn ) from consecutive state transitions with
the last prediction V (sn ) containing the only non-zero reward from the environment. Over

many iterations of this sequence, the update rule will adjust the values of each state so that
they agree with their successors and eventually with the reward received in V (sn ). In other
words, the single reward is propagated backwards through the chain of value predictions.
The net result is an accurate value function that can be used to predict the expected reward
from any state of the system.
As mentioned earlier,
the goal
of TD methods is to learn the value function for the




optimal policy, V . Given V , the optimal action,  (s), can be computed using the
following equation:
 ( (s; a))
(s) = argmax
V
a

(5)

Of course, we have already stated that in RL the state transition function  (s; a) is unknown
to the agent. Without this knowledge, we have no way of evaluating (5). An alternative
value function that can be used to compute  (s) is called a Q-function, Q(s; a) (Watkins,
1989; Watkins & Dayan, 1992). The Q-function is a value function that represents the
expected value of taking action a in state s and acting optimally thereafter:

Q(s; a) = r(s) + V  ((s; a))

(6)
where r(s) represents any immediate reward received in state s. Given the Q-function,
actions from the optimal policy can be directly computed using the following equation:

(s) = argmax
Q(s; a)
a

(7)

Q(st; at) = Q(st; at) + ff(max
Q(st+1; at+1) , Q(st; at) + r(st))
a +1

(8)

Table 1 shows the Q-function for the grid world problem of Figure 1. This table-based
representation of the Q-function associates cumulative future payoffs for each state-action
pair in the system. (The letter-number pairs at the top represent the state given by the row
and column in Figure 1, and R and D represent the actions right and down, respectively.)
The TD method adjusts the Q-values after each decision. When selecting the next action,
the agent considers the effect of that action by examining the expected value of the state
transition caused by the action.
The Q-function is learned through the following TD update equation:
t

245

fiMoriarty, Schultz, & Grefenstette

a1 a2 a3 a4 a5 b1 b2 b3 b4 b5 c1 c2 c3 c4 c5 d1 d2 d3 d4 d5 e1 e2 e3 e4 e5
R 17 16 10 7 6 17 15 7 6 5 7 9 11 8 4 6 6 7 4 2 1 2 1 2 1
D 16 11 10 7 1 17 8 1 3 1 15 14 12 8 2 6 7 7 3 1 7 6 4 3 1

Table 1: A Q-function for the simple grid world. A value is associated with each state-action
pair.
Essentially, this equation updates Q(st ; at) based on the current reward and the predicted
reward if all future actions are selected optimally. Watkins and Dayan (1992) proved that
if updates are performed in this fashion and if every Q-value is explicitly represented,
the estimates will asymptotically converge to the correct values. A reinforcement learning
system can thus use the Q values to select the optimal action in any state. Because Qlearning is the most widely known implementation of temporal difference learning, we will
use it in our qualitative comparisons with evolutionary approaches in later sections.

4. Evolutionary Algorithms for Reinforcement Learning (EARL)
The policy-space approach to RL searches for policies that optimize an appropriate objective
function. While many search algorithms might be used, this survey focuses on evolutionary
algorithms. We begin with a brief overview of a simple EA for RL, followed by a detailed
discussion of features that characterize the general class of EAs for RL.

4.1 Design Considerations for Evolutionary Algorithms

Evolutionary algorithms (EAs) are global search techniques derived from Darwin's theory
of evolution by natural selection. An EA iteratively updates a population of potential
solutions, which are often encoded in structures called chromosomes. During each iteration,
called a generation, the EA evaluates solutions and generates offspring based on the fitness
of each solution in the task environment. Substructures, or genes, of the solutions are then
modified through genetic operators such as mutation and recombination. The idea is that
structures that are associated with good solutions can be mutated or combined to form
even better solutions in subsequent generations. The canonical evolutionary algorithm is
shown in Figure 2. There have been a wide variety of EAs developed, including genetic
algorithms (Holland, 1975; Goldberg, 1989), evolutionary programming (Fogel, Owens, &
Walsh, 1966), genetic programming (Koza, 1992), and evolutionary strategies (Rechenberg,
1964).
EAs are general purpose search methods and have been applied in a variety of domains
including numerical function optimization, combinatorial optimization, adaptive control,
adaptive testing, and machine learning. One reason for the widespread success of EAs is
that there are relatively few requirements for their application, namely,
1. An appropriate mapping between the search space and the space of chromosomes, and
2. An appropriate fitness function.
246

fiEvolutionary Algorithms for Reinforcement Learning

procedure EA
begin
t = 0;
initialize P(t);
evaluate structures in P(t);
while termination condition not satisfied do
begin
t = t + 1;
select P(t) from P(t-1);
alter structures in P(t);
evaluate structures in P(t);
end
end.
Figure 2: Pseudo-code Evolutionary Algorithm.
For example, in the case of parameter optimization, it is common to represent the list of
parameters as either a vector of real numbers or a bit string that encodes the parameters.
With either of these representations, the \standard" genetic operators of mutation and
cut-and-splice crossover can be applied in a straightforward manner to produce the genetic
variations required (see Figure 3). The user must still decide on a (rather large) number
of control parameters for the EA, including population size, mutation rates, recombination
rates, parent selection rules, but there is an extensive literature of studies which suggest
that EAs are relatively robust over a wide range of control parameter settings (Grefenstette,
1986; Schaffer, Caruana, Eshelman, & Das, 1989). Thus, for many problems, EAs can be
applied in a relatively straightforward manner.
However, for many other applications, EAs need to be specialized for the problem domain (Grefenstette, 1987). The most critical design choice facing the user is the representation, that is, the mapping between the search space of knowledge structures (or, the
phenotype space) and the space of chromosomes (the genotype space). Many studies have
shown that the effectiveness of EAs is sensitive to the choice of representations. It is not
sucient, for example, to choose an arbitrary mapping from the search space into the space
of chromosomes, apply the standard genetic operators and hope for the best. What makes a
good mapping is a subject for continuing research, but the general consensus is that candidate solutions that share important phenotypic similarities must also exhibit similar forms
of \building blocks" when represented as chromosomes (Holland, 1975). It follows that the
user of an EA must carefully consider the most natural way to represent the elements of
the search space as chromosomes. Moreover, it is often necessary to design appropriate
mutation and recombination operators that are specific to the chosen representation. The
end result of this design process is that the representation and genetic operators selected
for the EA comprise a form of search bias similar to biases in other machine learning meth247

fiMoriarty, Schultz, & Grefenstette

Parent 1:

A

B

C

D

E

F

G

Parent 2:

a

b

c

d

e

f

g

Offspring 1:

A

B

C

d

e

f

g

Offspring 2:

a

b

c

D

E

F

G

Figure 3: Genetic operators on fixed-position representation. The two offspring are generated by crossing over the selected parents. The operation shown is called one-point
crossover. The first offspring inherits the initial segment of one parent and the
final segment of the other parent. The second offspring inherits the same pattern
of genes from the opposite parents. The crossover point is position 3, chosen at
random. The second offspring has also incurred a mutation in the shaded gene.
ods. Given the proper bias, the EA can quickly identify useful \building blocks" within the
population, and converge on the most promising areas of the search space.1
In the case of RL, the user needs to make two major design decisions. First, how will the
space of policies be represented by chromosomes in the EA? Second, how will the fitness of
population elements be assessed? The answers to these questions depend on how the user
chooses to bias the EA. The next section presents a simple EARL that adopts the most
straightforward set of design decisions. This example is meant only to provide a baseline
for comparison with more elaborate designs.

4.2 A Simple EARL

As the remainder of this paper shows, there are many ways to use EAs to search the space
of RL policies. This section provides a concrete example of a simple EARL, which we call
Earl1 . The pseudo-code is shown in Figure 4. This system provides the EA counterpart
to the simple table-based TD system described in Section 3.
The most straightforward way to represent a policy in an EA is to use a single chromosome per policy with a single gene associated with each observed state. In Earl1 , each
gene's value (or allele in biological terminology) represents the action value associated with
the corresponding state, as shown in Figure 5. Table 2 shows part of an Earl1 population
of policies for the sample grid world problem. The number of policies in a population is
usually on the order of 100 to 1000.
The fitness of each policy in the population must reect the expected accumulated fitness
for an agent that uses the given policy. There are no fixed constraints on how the fitness of
an individual policy is evaluated. If the world is deterministic, like the sample grid-world,
1. Other ways to exploit problem specific knowledge in EAs include the use of heuristics to initialize the
population and the hybridization with problem specific search algorithms. See (Grefenstette, 1987) for
further discussions of these methods.

248

fiEvolutionary Algorithms for Reinforcement Learning

procedure EARL-1
begin
t = 0;
initialize a population of policies, P(t);
evaluate policies in P(t);
while termination condition not satisfied do
begin
t = t + 1;
select high-payoff policies, P(t), from policies in P(t-1);
update policies in P(t);
evaluate policies in P(t);
end
end.
Figure 4: Pseudo-code for Evolutionary Algorithm Reinforcement Learning system.
Policy i:

s1
a1

s1
a1

s3
a3

...

sN
aN

Figure 5: Table-based policy representation. Each observed state has a gene which indicates
the preferred action for that state. With this representation, standard genetic
operators such as mutation and crossover can be applied.
the fitness of a policy can be evaluated during a single trial that starts with the agent in the
initial state and terminates when the agent reaches a terminal state (e.g., falls off the grid
in the grid-world). In non-deterministic worlds, the fitness of a policy is usually averaged
over a sample of trials. Other options include measuring the total payoff achieved by the
agent after a fixed number of steps, or measuring the number of steps required to achieve
a fixed level of payoff.
Once the fitness of all policies in the population has been determined, a new population
is generated according to the steps in the usual EA (Figure 2). First, parents are selected
for reproduction. A typical selection method is to probabilistically select individuals based
on relative fitness:
(pi )
Pr(pi ) = PnFitness
j =1 Fitness(pj )

(9)

where pi represents individual i and n is the total number of individuals. Using this selection
rule, the expected number of offspring for a given policy is proportional to that policy's
fitness. For example, a policy with average fitness might have a single offspring, whereas
249

fiMoriarty, Schultz, & Grefenstette

Policy
1
2
3
4
5

a1
D
D
R
D
R

a2
R
D
D
D
D

a3
D
D
D
D
D

a4
D
D
R
D
D

a5
R
R
R
R
R

b1
R
R
D
D
D

b2
R
R
R
R
R

b3
R
R
D
R
R

b4
R
R
R
R
D

b5
R
R
R
R
R

c1
D
D
D
R
R

c2
R
D
D
D
D

c3
D
R
D
R
R

c4
D
R
R
R
R

c5
R
D
D
R
D

d1
R
R
R
D
R

d2
D
D
D
R
D

d3
R
R
R
R
R

d4
R
R
R
D
R

d5
R
R
R
R
D

e1
D
D
D
D
D

e2
R
R
R
R
R

e3
R
D
D
D
D

e4
D
D
D
D
D

e5 Fitness
R 8
R 9
D 17
R 11
D 16

Table 2: An EA population of five decision policies for the sample grid world. This simple
policy representation specifies an action for each state of the world. The fitness
corresponds to the payoffs that are accumulated using each policy in the grid
world.
a policy with twice the average fitness would have two offspring.2 Offspring are formed
by cloning the selected parents. Then new policies are generated by applying the standard
genetic operators of crossover and mutation to the clones, as shown in Figure 3. The process
of generating new populations of strategies can continue indefinitely or can be terminated
after a fixed number of generations or once an acceptable level of performance is achieved.
For simple RL problems such as the grid-world, Earl1 may provide an adequate approach. In later sections, we will point out some ways in which even Earl1 exhibits
strengths that are complementary to TD methods for RL. However, as in the case of TD
methods, EARL methods have been extended to handle the many challenges inherent in
more realistic RL problems. The following sections survey some of these extensions, organized around three specific biases that distinguish EAs for Reinforcement Learning (EARL)
from more generic EAs: policy representations, fitness/credit-assignment models, and RLspecific genetic operators.

5. Policy Representations in EARL

Perhaps the most critical feature that distinguishes classes of EAs from one another is the
representation used. For example, EAs for function optimization use a simple string or
vector representation, whereas EAs for combinatorial optimization use distinctive representations for permutations, trees or other graph structures. Likewise, EAs for RL use a
distinctive set of representations for policies. While the range of potential policy representations is unlimited, the representations used in most EARL systems to date can be
largely categorized along two discrete dimensions. First, policies may be represented either by condition-action rules or by neural networks. Second, policies may be represented
by a single chromosome or the representation may be distributed through one or more
populations.

5.1 Single-Chromosome Representation of Policies
5.1.1 Rule-based Policies

For most RL problems of practical interest, the number of observable states is very large,
and the simple table-based representation in Earl1 is impractical. For large scale state
2. Many other parent selection rules have been explored (Grefenstette, 1997a, 1997b).

250

fiEvolutionary Algorithms for Reinforcement Learning

Policy i:

c i1  ai1

c i2  ai2

c i3  ai3

...

c ik  aik

Figure 6: Rule-based policy representation. Each gene represents a condition-action rule
that maps a set of states to an action. In general, such rules are independent
of the position along the chromosome. Conict resolution mechanisms may be
needed if the conditions of rules are allowed to intersect.
w1
w k1
Policy i:

w1

w2

w3

...

wk

=>
...

wk
wj

Figure 7: A simple parameter representation of weights for a neural network. The fitness
of the policy is the payoff when the agent uses the corresponding neural net as
its decision policy.
spaces, it is more reasonable to represent a policy as a set of condition-action rules in which
the condition expresses a predicate that matches a set of states, as shown in Figure 6. Early
examples of this representation include the systems LS-1 (Smith, 1983) and LS-2 (Schaffer
& Grefenstette, 1985), followed later by Samuel (Grefenstette et al., 1990).
5.1.2 Neural Net Representation of Policies

As in TD-based RL systems, EARL systems often employ neural net representations as
function approximators. In the simplest case (see Figure 7), a neural network for the
agent's decision policy is represented as a sequence of real-valued connection weights. A
straightforward EA for parameter optimization can be used to optimize the weights of
the neural network (Belew, McInerney, & Schraudolph, 1991; Whitley, Dominic, Das, &
Anderson, 1993; Yamauchi & Beer, 1993). This representation thus requires the least
modification of the standard EA. We now turn to distributed representations of policies in
EARL systems.

5.2 Distributed Representation of Policies

In the previous section we outlined EARL approaches that treat the agent's decision policy
as a single genetic structure that evolves over time. This section addresses EARL approaches
that decompose a decision policy into smaller components. Such approaches have two
potential advantages. First, they allow evolution to work at a more detailed level of the task,
e.g., on specific subtasks. Presumably, evolving a solution to a restricted subtask should be
251

fiMoriarty, Schultz, & Grefenstette

Sensors

Message List

Rewards

Classifiers

Decision

Evolutionary
Algorithm

Figure 8: Holland's Learning Classifier System.
easier than evolving a monolithic policy for a complex task. Second, decomposition permits
the user to exploit background knowledge. The user might base the decomposition into
subtasks on a prior analysis of the overall performance task; for example, it might be known
that certain subtasks are mutually exclusive and can therefore be learned independently.
The user might also decompose a complex task into subtasks such that certain components
can be explicitly programmed while other components are learned.
In terms of knowledge representation in EARL, the alternative to the single chromosome
representation is to distribute the policy over several population elements. By assigning a
fitness to these individual elements of the policy, evolutionary selection pressure can be
brought to bear on more detailed aspects of the learning task. That is, fitness is now a
function of individual subpolicies or individual rules or even individual neurons. This general
approach is analogous to the classic TD methods that take this approach to the extreme of
learning statistics concerning each state-action pair. As in the case of single-chromosome
representations, we can partition distributed EARL representations into rule-based and
neural-net-based classes.
5.2.1 Distributed Rule-based Policies

The most well-known example of a distributed rule-based approach to EARL is the Learning Classifier Systems (LCS) model (Holland & Reitman, 1978; Holland, 1987; Wilson,
1994). An LCS uses an evolutionary algorithm to evolve if-then rules called classifiers that
map sensory input to an appropriate action. Figure 8 outlines Holland's LCS framework
(Holland, 1986). When sensory input is received, it is posted on the message list. If the left
hand side of a classifier matches a message on the message list, its right hand side is posted
on the message list. These new messages may subsequently trigger other classifiers to post
messages or invoke a decision from the LCS, as in the traditional forward-chaining model
of rule-based systems.
In an LCS, each chromosome represents a single decision rule and the entire population
represents the agent's policy. In general, classifiers map a set of observed states to a set of
messages, which may be interpreted as either internal state changes or actions. For example,
252

fiEvolutionary Algorithms for Reinforcement Learning

condition
action strength
a#
! R
0.75
#2
! D
0.25
d3

:::
!

D

0.50

Table 3: LCS population for grid world. The # is a don't care symbol which allows for
generality in conditions. For example, the first rule says \Turn right in column
a." The strength of a rule is used for conict resolution and for parent selection in
the genetic algorithm.

LCS
LCS

LCS

Environment

Figure 9: A two-level hierarchical Alecsys system. Each LCS learns a specific behavior.
The interactions among the rule sets are pre-programmed.
if the learning agent for the grid world in Figure 1 has two sensors, one for the column and
one for the row, then the population in an LCS might appear as shown in Table 3. The
first classifier matches any state in the column a and recommends action R. Each classifier
has a statistic called strength that estimates the utility of the rule. The strength statistics
are used in both conict resolution (when more than one action is recommended) and as
fitness for the genetic algorithm. Genetic operators are applied to highly fit classifiers to
generate new rules. Generally, the population size (i.e., the number of rules in the policy)
is kept constant. Thus classifiers compete for space in the policy.
Another way that EARL systems distribute the representation of policies is to partition
the policy into separate modules, with each module updated by its own EA. Dorigo and
Colombetti (1998) describe an architecture called Alecsys in which a complex reinforcement learning task is decomposed into subtasks, each of which is learned via a separate
LCS, as shown in Figure 9. They provide a method called behavior analysis and training
(BAT) to manage the incremental training of agents using the distributed LCS architecture.
The single-chromosome representation can also be extended by partitioning the policy across multiple co-evolving populations. For example, in the cooperative co-evolution
model (Potter, 1997), the agent's policy is formed by combining chromosomes from several independently evolving populations. Each chromosome represents a set of rules, as
in Figure 6, but these rules address only a subset of the performance task. For example,
separate populations might evolve policies for different components of a complex task, or
253

fiMoriarty, Schultz, & Grefenstette

EA i
EA 1

Domain
Model
collaboration

fitness

Evolutionary
Algorithm

Population

representative

Merge

representative

individual
to be
evaluated

representative

EA 2

EA n

representative

Figure 10: Cooperative coevolutionary architecture from the perspective of the ith EA instance. Each EA contributes a representative, which is merged with the others'
representatives to form a collaboration, or policy for the agent. The fitness of
each representative reects the average fitness of its collaborations.

might address mutually exclusive sets of observed states. The fitness of each chromosome is
computed based on the overall fitness of the agents that employ that chromosome as part of
its combined chromosomes. The combined chromosomes represent the decision policy and
are called a collaboration (Figure 10).
5.2.2 Distributed Network-based Policies

Distributed EARL systems using neural net representations have also been designed. In
(Potter & De Jong, 1995), separate populations of neurons evolve, with the evaluation of
each neuron based on the fitness of a collaboration of neurons selected from each population.
In SANE (Moriarty & Miikkulainen, 1996a, 1998), two separate populations are maintained
and evolved: a population of neurons and a population of network blueprints. The motivation for SANE comes from our a priori knowledge that individual neurons are fundamental
building blocks in neural networks. SANE explicitly decomposes the neural network search
problem into several parallel searches for effective single neurons. The neuron-level evolution provides evaluation and recombination of the neural network building blocks, while the
population of blueprints search for effective combinations of these building blocks. Figure 11
gives an overview of the interaction of the two populations.
Each individual in the blueprint population consists of a set of pointers to individuals
in the neuron population. During each generation, neural networks are constructed by
combining the hidden neurons specified in each blueprint. Each blueprint receives a fitness
according to how well the corresponding network performs in the task. Each neuron receives
a fitness according to how well the top networks in which it participates perform in the
task. An aggressive genetic selection and recombination strategy is used to quickly build
and propagate highly fit structures in both the neuron and blueprint populations.
254

fiEvolutionary Algorithms for Reinforcement Learning

Network Blueprint Population

Neuron Population

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

Figure 11: An overview of the two populations in SANE. Each member of the neuron population specifies a series of connections (connection labels and weights) to be
made within a neural network. Each member of the network blueprint population specifies a series of pointers to specific neurons which are used to build a
neural network.

6. Fitness and Credit Assignment in EARL

Evolutionary algorithms are all driven by the concept of natural selection: population
elements that have higher fitness leave more offspring to later generations, thus inuencing
the direction of search in favor of high performance regions of the search space. The concept
of fitness is central to any EA. In this section, we discuss features of the fitness model that
are common across most EARL systems. We specifically focus on ways in which the fitness
function reects the distinctive structure of the RL problem.

6.1 The Agent Model

The first common features of all EARL fitness models is that fitness is computed with
respect to an RL agent. That is, however the policy is represented in the EA, it must be
converted to a decision policy for an agent operating in a RL environment. The agent is
assumed to observe a description of the current state, select its next action by consulting
its current policy, and collect whatever reward is provided by the environment. In EARL
systems, as in TD systems, the agent is generally assumed to perform very little additional
computation when selecting its next action. While neither approach limits the agent to
strict stimulus-response behavior, it is usually assumed that the agent does not perform
extensive planning or other reasoning before acting. This assumption reects the fact that
RL tasks involve some sort of control activity in which the agent must respond to a dynamic
environment within a limited time frame.
255

fiMoriarty, Schultz, & Grefenstette

6.2 Policy Level Credit Assignment

As shown in the previous section, the meaning of fitness in EARL systems may vary depending on what the population elements represent. In a single-chromosome representation,
fitness is associated with entire policies; in a distributed representation, fitness may be associated with individual decision rules. In any case, fitness always reects accumulated
rewards received by the agent during the course of interaction with the environment, as
specified in the RL model. Fitness may also reect effort expended, or amount of delay.
It is worthwhile considering the different approaches to credit assignment in the TD
and EA methods. In a reinforcement learning problem, payoffs may be sparse, that is,
associated only with certain states. Consequently, a payoff may reect the quality of an
extended sequence of decisions, rather than any individual decision. For example, a robot
may receive a reward after a movement that places it in a \goal" position within a room.
The robot's reward, however, depends on many of its previous movements leading it to
that point. A dicult credit assignment problem therefore exists in how to apportion the
rewards of a sequence of decisions to individual decisions.
In general, EA and TD methods address the credit assignment problem in very different ways. In TD approaches, credit from the reward signal is explicitly propagated to
each decision made by the agent. Over many iterations, payoffs are distributed across a
sequence of decisions so that an appropriately discounted reward value is associated with
each individual state and decision pair.
In simple EARL systems such as Earl1 , rewards are associated only with sequences
of decisions and are not distributed to the individual decisions. Credit assignment for an
individual decision is made implicitly, since policies that prescribe poor individual decisions
will have fewer offspring in future generations. By selecting against poor policies, evolution
automatically selects against poor individual decisions. That is, building blocks consisting
of particular state-action pairs that are highly correlated with good policies are propagated
through the population, replacing state-action pairs associated with poorer policies.
Figure 12 illustrates the differences in credit assignment between TD and Earl1 in the
grid world of Figure 1. The Q-learning TD method explicitly assigns credit or blame to each
individual state-action pair by passing back the immediate reward and the estimated payoff
from the new state. Thus, an error term becomes associated with each action performed by
the agent. The EA approach does not explicitly propagate credit to each action but rather
associates an overall fitness with the entire policy. Credit is assigned implicitly, based on the
fitness evaluations of entire sequences of decisions. Consequently, the EA will tend to select
against policies that generate the first and third sequences because they achieve lower fitness
scores. The EA thus implicitly selects against action D in state b2, for example, which is
present in the bad sequences but not present in the good sequences.

6.3 Subpolicy Credit Assignment

Besides the implicit credit assignment performed on building blocks, EARL systems have
also addressed the credit assignment problem more directly. As shown in Section 4, the
individuals in an EARL system might represent either entire policies or components of
a policy (e.g., component rule-sets, individual decision rules, or individual neurons). For
distributed-representation EARLs, fitness is explicitly assigned to individual components.
256

fiEvolutionary Algorithms for Reinforcement Learning

TD Explicit Credit Assignment
2+Max(Q(b1,a))

a1,R

b1,D

2+Max(Q(b1,a))

a1,R

2+Max(Q(a2,a))

a1,D

1+Max(Q(b2,a))

a2,D

a1,R

b1,D

b2,D

b3,D

c3

2

c3

a1,R

b1,D

b2,R

c2,D

c3

9

c3

a1,D

a2,R

b2,D

b3,D

c3

1

d2

a1,D

a2,D

b2,R

c2,D

d2

8

4+Max(Q(c3,a))

4+Max(Q(c3,a))

b3,D

-5+Max(Q(c2,a))

b2,R

c3

c2,D

-5+Max(Q(b3,a))

b2,D

Fitness

4+Max(Q(c3,a))

b3,D

-5+Max(Q(c2,a))

b2,R

1+Max(Q(b2,a))

a2,R

-5+Max(Q(b3,a))

b2,D

1+Max(Q(b2,a))

b1,D

2+Max(Q(a2,a))

a1,D

1+Max(Q(b2,a))

EA Implicit Credit Assignment

4+Max(Q(d2,a))

c2,D

Figure 12: Explicit vs. implicit credit assignment. The Q-learning TD method assigns credit
to each state-action pair based on the immediate reward and the predicted future
rewards. The EA method assigns credit implicitly by associating fitness values
with entire sequences of decisions.
In cases in which a policy is represented by explicit components, different fitness functions
can be associated with different evolving populations, allowing the implementer to \shape"
the overall policy by evolving subpolicies for specific subtasks (Dorigo & Colombetti, 1998;
Potter, De Jong, & Grefenstette, 1995). The most ambitious goal is to allow the system to
manage the number of co-evolving species as well as the form of interactions (Potter, 1997).
This exciting research is still at an early stage.
For example, in the LCS model, each classifier (decision rule) has a strength which is
updated using a TD-like method called the bucket brigade algorithm (Holland, 1986). In the
bucket brigade algorithm, the strength of a classifier is used to bid against other classifiers
for the right to post messages. Bids are subtracted from winning classifiers and passed back
to the classifiers that posted the enabling message on the previous step. Classifier strengths
are thus reinforced if the classifier posts a message that triggers another classifier. The
classifier that invokes a decision from the LCS receives a strength reinforcement directly
from the environment. The bucket brigade bid passing mechanism clearly bears a strong
relation to the method of temporal differences (Sutton, 1988). The bucket brigade updates
a given classifier's strength based on the strength of the classifiers that fire as a direct result
of its activation. The TD methods differ slightly in this respect because they assign credit
based strictly on temporal succession and do not take into account causal relations of steps.
It remains unclear which is more appropriate for distributing credit.
Even for single chromosome representations, TD-like methods have been adopted in
some EARL systems. In Samuel, each gene (decision rule) also maintains a quantity called
strength that is used to resolve conict when more than one rule matches the agent's current
sensor readings. When payoff is obtained (thereby terminating the trial), the strengths of
257

fiMoriarty, Schultz, & Grefenstette

all rules that fired during the trial are updated (Grefenstette, 1988). In addition to resolving
conicts, a rule's strength also plays a role in triggering mutation operations, as described
in the next section.

7. RL-Specific Genetic Operators

The creation of special genetic operators provides another avenue for imposing an RLspecific bias on EAs. Specialized operators in EARL systems first appeared in (Holland,
1986), in which so-called triggered operators were responsible for creating new classifiers
when the learning agent found that no classifier in its existing population matched the
agent's current sensor readings. In this case, a high-strength rule was explicitly generalized
to cover the new set of sensor readings. A similar rule-creation operator was included in
early versions of Samuel (Grefenstette et al., 1990). Later versions of Samuel included
a number of mutation operators which created altered rules based on an agent's early
experiences. For example, Samuel's Specialization mutation operator is triggered when
a low-strength, general rule fires during an episode that results in high payoff. In such a
case, the rule's conditions are reduced in generality to more closely match the agent's sensor
readings. For example, if the agent has a sensor readings (range = 40; bearing = 100)
and the original rule is:
IF range = [25; 55] AND bearing = [0; 180] THEN SET turn = 24 (strength
0.1)
then the new rule would be:
IF range = [35; 45] AND bearing = [50; 140] THEN SET turn = 24 (strength
0.8)
Since the episode triggering the operator resulted in high payoff, one might suspect that
the original rule was over-generalized, and that the new, more specific version might lead
to better results. (The strength of the new rule is initialized to the payoff received during
the triggering episode.) This is considered a Lamarckian operator because the agent's
experience is causing a genetic change which is passed on to later offspring.3
Samuel also uses an RL-specific crossover operator to recombine policies. In particular,
crossover in Samuel attempts to cluster decision rules before assigning them to offspring.
For example, suppose that the traces of the most previous evaluations of the parent strategies are as follows (Ri;j denotes the j th decision rule in policy i):
Trace for parent #1:
Episode:
..
.
8. R1;3 ! R1;1 ! R1;7 ! R1;5 High Payoff
9. R1;2 ! R1;8 ! R1;4
Low Payoff
3. Jean Baptiste Lamarck developed an evolutionary theory that stressed the inheritance of acquired characteristics, in particular acquired characteristics that are well adapted to the surrounding environment.
Of course, Lamarck's theory was superseded by Darwin's emphasis on two-stage adaptation: undirected
variation followed by selection. Research has generally failed to substantiate any Lamarckian mechanisms
in biological systems (Gould, 1980).

258

fiEvolutionary Algorithms for Reinforcement Learning

..
.
Trace for parent #2:
..
.
4. R2;7 ! R2;5
5. R2;6 ! R2;2 ! R2;4
..
.
Then one possible offspring would be:

Low Payoff
High Payoff

fR1;8 ; : : :; R1;3 ; R1;1 ; R1;7 ; R1;5 ; : : :; R2;6 ; R2;2 ; R2;4 ; : : :; R2;7g
The motivation here is that rules that fire in sequence to achieve a high payoff should be
treated as a group during recombination, in order to increase the likelihood that the offspring
policy will inherit some of the better behavior patterns of its parents. Rules that do not
fire in successful episodes (e.g., R1;8) are randomly assigned to one of the two offspring.
This form of crossover is not only Lamarckian (since it is triggered by the experiences
of the agent), but is directly related to the structure of the RL problem, since it groups
components of policies according to the temporal association among the decision rules.

8. Strengths of EARL

The EA approach represents an interesting alternative for solving RL problems, offering
several potential advantages for scaling up to realistic applications. In particular, EARL
systems have been developed that address dicult challenges in RL problems, including:
 Large state spaces;
 Incomplete state information; and
 Non-stationary environments.
This section focuses on ways that EARL address these challenges.

8.1 Scaling Up to Large State Spaces

Many early papers in the RL literature analyze the eciency of alternative learning methods
on toy problems similar to the grid world shown in Figure 1. While such studies are useful
as academic exercises, the number of observed states in realistic applications of RL is likely
to preclude any approach that requires the explicit storage and manipulation of statistics
associated with each observable state-action pair. There are two ways that EARL policy
representations help address the problem of large state spaces: generalization and selectivity.
8.1.1 Policy Generalization

Most EARL policy representations specify the policy at a level of abstraction higher than an
explicit mapping from observed states to actions. In the case of rule-based representations,
the rule language allows conditions to match sets of states, thus greatly reducing the storage
259

fiMoriarty, Schultz, & Grefenstette

a1 a2 a3 a4 a5 b1 b2 b3 b4 b5 c1 c2 c3 c4 c5 d1 d2 d3 d4 d5 e1 e2 e3 e4 e5
R 16 7 ? 17 12 8 12 11 11 12 14 7 12 13 9 12 11 12 12 11 ? 12 7 ? 9
L 9 13 12 11 ? 15 ? 17 16 ? 11 13 12 7 14 11 12 ? 11 16 12 ? 13 12 16

Table 4: An approximated value function from the population in Table 2. The table displays the average fitness for policies that select each state-action pair and reects
the estimated impact each action has on overall fitness. Given the tiny population
size in this example, the estimates are not particularly accurate. Note the question
marks in states where actions have converged. Since no policies select the alternative action, the population has no statistics on the impact of these actions on
fitness. This is different from simple TD methods, where statistics on all actions
are maintained.

required to specify a policy. It should be noted, however, that the generality of the rules
within a policy may vary considerably, from the level of rules that specify an action for
a single observed state all the way to completely general rules that recommend an action
regardless of the current state. Likewise, in neural net representations, the mapping function
is stored implicitly in the weights on the connections of the neural net. In either case, a
generalized policy representation facilitates the search for good policies by grouping together
states for which the same action is required.
8.1.2 Policy Selectivity

Most EARL systems have selective representations of policies. That is, the EA learns mappings from observed states to recommended actions, usually eliminating explicit information
concerning less desirable actions. Knowledge about bad decisions is not explicitly preserved,
since policies that make such decisions are selected against by the evolutionary algorithm
and are eventually eliminated from the population. The advantage of selective representations is that attention is focused on profitable actions only, reducing space requirements for
policies.
Consider our example of the simple EARL operating on the grid world. As the population evolves, policies normally converge to the best actions from a specific state, because of
the selective pressure to achieve high fitness levels. For example, the population shown in
Table 2 has converged alleles (actions) in states a3; a5; b2; b5; d3; e1; and e2. Each of these
converged state-action pairs is highly correlated with fitness. For example, all policies have
converged to action R in state b2. Taking action R in state b2 achieves a much higher
expected return than action D (15 vs. 8 from Table 1). Policies that select action D from
state b2 achieve lower fitness scores and are selected against. For this simple EARL, a snapshot of the population (Table 2) provides an implicit estimate of a corresponding TD value
function (Table 4), but the distribution is biased toward the more profitable state-actions
pairs.
260

fiEvolutionary Algorithms for Reinforcement Learning

.5
L

3.0
L

Red

R

Blue

R
1.0

Green

L

Blue

L

- 4.0

R
R
1.0
.75

Figure 13: An environment with incomplete state information. The circles represent the
states of the world and the colors represent the agent's sensory input. The agent
is equally likely to start in the red state or the green state

8.2 Dealing with Incomplete State Information

Clearly, the most favorable condition for reinforcement learning occurs when the agent can
observe the true state of the dynamic system with which it interacts. When complete state
information is available, TD methods make ecient use of available feedback by associating
reward directly with individual decisions. In real world situations, however, the agent's
sensors are more likely to provide only a partial view that may fail to disambiguate many
states. Consequently, the agent will often be unable to completely distinguish its current
state. This problem has been termed perceptual aliasing or the hidden state problem. In
the case of limited sensory information, it may be more useful to associate rewards with
larger blocks of decisions. Consider the situation in Figure 13, in which the agent must
act without complete state information. Circles represent the specific states of the world,
and the colors represent the sensor information the agent receives within the state. Square
nodes represent goal states with the corresponding reward shown inside. In each state, the
agent has a choice of two actions (L or R). We further assume that the state transitions
are deterministic and that the agent is equally likely to start in either the state with the
red or green sensor readings.
In this example, there are two different states that return a sensor reading of blue,
and the agent is unable to distinguish between them. Moreover, the actions for each blue
state return very different rewards. A Q function applied to this problem treats the sensor
reading of blue as one observable state, and the rewards for each action are averaged over
both blue states. Thus, Q(blue; L) and Q(blue; R) will converge to -0.5 and 1, respectively.
Since the reward from Q(blue; R) is higher than the alternatives from observable states red
and green, the agent's policy under Q-learning will choose to enter observable state blue
each time. The final decision policy under Q-learning is shown in Table 5. This table also
shows the optimal policy with respect to the agent's limited view of its world. In other
261

fiMoriarty, Schultz, & Grefenstette

Value Function Policy Optimal Policy
R
R
L
R
R
L
Expected Reward
1.0
1.875

Red
Green
Blue

Table 5: The policy and expected reward returned by a converged Q function compared to
the optimal policy given the same sensory information.
words, the policy reects the optimal choices if the agent cannot distinguish the two blue
states.
By associating values with individual observable states, the simple TD methods are
vulnerable to hidden state problems. In this example, the ambiguous state information
misleads the TD method, and it mistakenly combines the rewards from two different states
of the system. By confounding information from multiple states, TD cannot recognize that
advantages might be associated with specific actions from specific states, for example, that
action L from the top blue state achieves a very high reward.
In contrast, since EA methods associate credit with entire policies, they rely more on
the net results of decision sequences than on sensor information, that may, after all, be
ambiguous. In this example, the evolutionary algorithm exploits the disparity in rewards
from the different blue states and evolves policies that enter the good blue state and avoid
the bad one. The agent itself remains unable to distinguish the two blue states, but the evolutionary algorithm implicitly distinguishes among ambiguous states by rewarding policies
that avoid the bad states.
For example, an EA method can be expected to evolve an optimal policy in the current
example given the existing, ambiguous state information. Policies that choose the action
sequence R,L when starting in the red state will achieve the highest levels of fitness, and
will therefore be selected for reproduction by the EA. If agents using these policies are
placed in the green state and select action L, they receive the lowest fitness score, since
their subsequent action, L from the blue sensors, returns a negative reward. Thus, many of
the policies that achieve high fitness when started in the red state will be selected against if
they choose L from the green state. Over the course of many generations, the policies must
choose action R from the green state to maximize their fitness and ensure their survival.
We confirmed these hypotheses in empirical tests. A Q-learner using single-step updates
and a table-based representation converged to the values in Table 5 in every run. An
evolutionary algorithm4 consistently converged 80% of its population on the optimal policy.
Figure 14 shows the average percentage of the optimal policy in the population as a function
of time, averaged over 100 independent runs.
Thus even simple EA methods such as Earl1 appear to be more robust in the presence
of hidden states than simple TD methods. However, more refined sensor information could
still be helpful. In the previous example, although the EA policies achieve a better average
reward than the TD policy, the evolved policy remains unable to procure both the 3.0
4. We used a binary tournament selection, a 50 policy population, 0.8 crossover probability, and 0.01
mutation rate.

262

fiEvolutionary Algorithms for Reinforcement Learning

100

Percentage Optimal

80

60

40

20

0
0

10

20

30

40

50
Generation

60

70

80

90

100

Figure 14: The optimal policy distribution in the hidden state problem for an evolutionary
algorithm. The graph plots the percentage of optimal policies in the population,
averaged over 100 runs.
and 1.0 rewards from the two blue states. These rewards could be realized, however, if
the agent could separate the two blue states. Thus, any method that generates additional
features to disambiguate states presents an important asset to EA methods. Kaelbling
et al. (1996) describe several promising solutions to the hidden state problem, in which
additional features such as the agent's previous decisions and observations are automatically
generated and included in the agent's sensory information (Chrisman, 1992; Lin & Mitchell,
1992; McCallum, 1995; Ring, 1994). These methods have been effective at disambiguating
states for TD methods in initial studies, but further research is required to determine the
extent to which similar methods can resolve significant hidden state information in realistic
applications. It would be useful to develop ways to use such methods to augment the sensory
data available in EA methods as well.

8.3 Non-Stationary Environments

If the agent's environment changes over time, the RL problem becomes even more dicult,
since the optimal policy becomes a moving target. The classic trade-off between exploration
and exploitation becomes even more pronounced. Techniques for encouraging exploration
in TD-based RL include adding an exploration bonus to the estimated value of state-action
pairs that reects how long it has been since the agent has tried that action (Sutton, 1990),
and building a statistical model of the agent's uncertainty (Dayan & Sejnowski, 1996).
Simple modifications of standard evolutionary algorithms offer an ability to track nonstationary environments, and thus provide a promising approach to RL for these dicult
cases.
The fact that evolutionary search is based on competition within a population of policies
suggest some immediate benefits for tracking non-stationary environments. To the extent
that the population maintains a diverse set of policies, changes in the environment will bias
263

fiMoriarty, Schultz, & Grefenstette

selective pressure in favor of the policies that are most fit for the current environment. As
long as the environment changes slowly with respect to the time required to evaluate a
population of policies, the population should be able to track a changing fitness landscape
without any alteration of the algorithm. Empirical studies show that maintaining the
diversity within the population may require a higher mutation rate than those usually
adopted for stationary environments (Cobb & Grefenstette, 1993).
In addition, special mechanisms have been explored in order to make EAs more responsive to rapidly changing environments. For example, (Grefenstette, 1992) suggests
maintaining a random search within a restricted portion of the population. The random
population elements are analogous to immigrants from other populations with uncorrelated
fitness landscapes. Maintaining this source of diversity permits the EA to respond rapidly
to large, sudden changes in the fitness landscape. By keeping the randomized portion of
the population to less than about 30% of the population, the impact on search eciency in
stationary environments is minimized. This is a general approach that can easily be applied
in EARL systems.
Other useful algorithms that have been developed to ensure diversity in evolving popultions include fitness sharing (Goldberg & Richardson, 1987), crowding (De Jong, 1975),
and local mating (Collins & Jefferson, 1991). In Goldberg's fitness sharing model, for example, similar individuals are forced to share a large portion of a single fitness value from
the shared solution point. Sharing decreases the fitness of similar individuals and causes
evolution to select against individuals in overpopulated niches.
EARL methods that employ distributed policy representations achieve diversity automatically and are well-suited for adaptation in dynamic environments. In a distributed
representation, each individual represents only a partial solution. Complete solutions are
built by combining individuals. Because no individual can solve the task on its own, the
evolutionary algorithm will search for several complementary individuals that together can
solve the task. Evolutionary pressures are therefore present to prevent convergence of the
population. Moriarty and Miikkulainen (1998) showed how the inherent diversity and specialization in SANE allow it to adapt much more quickly to changes in the environment
than standard, convergent evolutionary algorithms.
Finally, if the learning system can detect changes in the environment, even more direct
response is possible. In the anytime learning model (Grefenstette & Ramsey, 1992), an
EARL system maintains a case-base of policies, indexed by the values of the environmental
detectors corresponding to the environment in which a given policy was evolved. When
an environmental change is detected, the population of policies is partially reinitialized,
using previously learned policies selected on the basis of similarity between the previously
encountered environment and the current environment. As a result, if the environment
changes are cyclic, then the population can be immediately seeded with those policies in
effect during the last occurrence of the current environment. By having a population of
policies, this approach is protected against some kinds of errors in detecting environmental
changes. For example, even if a spurious environmental change is mistakenly detected,
learning is not unduly affected, since only a part of the current population of policies is
replaced by previously learned policies. Zhou (1990) explored a similar approach based on
LCS.
264

fiEvolutionary Algorithms for Reinforcement Learning

In summary, EARL systems can respond to non-stationary environments, both by techniques that are generic to evolutionary algorithms and by techniques that have been specifically designed with RL in mind.

9. Limitations of EARL
Although the EA approach to RL is promising and has a growing list of successful applications (as outlined in the following section), a number of challenges remain.

9.1 Online Learning
We can distinguish two broad approaches to reinforcement learning |online learning and
oine learning. In online learning, an agent learns directly from its experiences in its
operational environment. For example, a robot might learn to navigate in a warehouse by
actually moving about its physical environment. There are two problems with using EARL
in this situation. First, it is likely to require a large number of experiences in order to
evaluate a large population of policies. Depending on how quickly the agent performs tasks
that result in some environmental feedback, it may take an unacceptable amount of time
to run hundreds of generations of an EA that evaluates hundreds or thousands of policies.
Second, it may be dangerous or expensive to permit an agent to perform some actions in
its actual operational environment that might cause harm to itself or its environment. Yet
it is very likely that at least some policies that the EA generates will be very bad policies.
Both of these objections apply to TD methods as well. For example, the theoretical results
that prove the optimality of Q-learning require that every state be visited infinitely often,
which is obviously impossible in practice. Likewise, TD methods may explore some very
undesirable states before an acceptable value-function is found.
For both TD and EARL, practical considerations point toward the use of oine learning,
in which the RL system performs its exploration on simulation models of the environment.
Simulation models provide a number of advantages for EARL, including the ability to
perform parallel evaluations of all the policies in a population simultaneously (Grefenstette,
1995).

9.2 Rare States
The memory or record of observed states and rewards differs greatly between EA and TD
methods. Temporal difference methods normally maintain statistics concerning every stateaction pair. As states are revisited, the new reinforcement is combined with the previous
value. New information thus supplements previous information, and the information content of the agent's reinforcement model increases during exploration. In this manner, TD
methods sustain knowledge of both good and bad state-action pairs.
As pointed out previously, EA methods normally maintain information only about good
policies or policy components. Knowledge of bad decisions is not explicitly preserved, since
policies that make such decisions are selected against by the evolutionary algorithm and
are eventually eliminated from the population. For example, refer once again to Table 4,
which shows the implicit statistics of the population from Table 2. Note the question
265

fiMoriarty, Schultz, & Grefenstette

marks in states where actions have converged. Since no policies in the population select the
alternative action, the EA has no statistics on the impact of these actions on fitness.
This reduction in information content within the evolving population can be a disadvantage with respect to states that are rarely visited. In any evolutionary algorithm, the value
of genes that have no real impact on the fitness of the individual tends to drift to random
values, since mutations tend to accumulate in these genes. If a state is rarely encountered,
mutations may freely accumulate in the gene that describes the best action for that state.
As a result, even if the evolutionary algorithm learns the correct action for a rare state, that
information may eventually be lost due to mutations. In contrast, since table-based TD
methods permanently record information about all state-action pairs, they may be more
robust when the learning agent does encounter a rare state. Of course, if a TD method
uses a function approximator such as a neural network as its value function, then it too
can suffer from memory loss concerning rare states, since many updates from frequently
occurring states can dominate the few updates from the rare states.

9.3 Proofs of Optimality

One of the attractive features of TD methods is that the Q-learning algorithm has a proof
of optimality (Watkins & Dayan, 1992). However, the practical importance of this result is
limited, since the assumptions underlying the proof (e.g., no hidden states, all state visited
infinitely often) are not satisfied in realistic applications. The current theory of evolutionary
algorithms provide a similar level of optimality proofs for restricted classes of search spaces
(Vose & Wright, 1995). However, no general theoretical tools are available that can be
applied to realistic RL problems. In any case, ultimate convergence to an optimal policy
may be less important in practice than eciently finding a reasonable approximation.
A more pragmatic approach may be to ask how ecient alternative RL algorithms are,
in terms of the number of reinforcements received before developing a policy that is within
some tolerance level of an optimal policy. In the model of probably approximately correct
(PAC) learning (Valiant, 1984), the performance of a learner is measured by how many
learning experiences (e.g., samples in supervised learning) are required before converging
to a correct hypothesis within specified error bounds. Although developed initially for
supervised learning, the PAC approach has been extended recently to both TD methods
(Fiechter, 1994) and to general EA methods (Ros, 1997). These analytic methods are
still in an early stage of development, but further research along these lines may one day
provide useful tools for understanding the theoretical and practical advantages of alternative
approaches to RL. Until that time, experimental studies will provide valuable evidence for
the utility of an approach.

10. Examples of EARL Methods

Finally, we take a look at a few significant examples of the EARL approach and results
on RL problems. Rather than attempt an exhaustive survey, we have selected four EARL
systems that are representative of the diverse policies representations outlined in Section 5.
Samuel represents the class of single-chromosome rule-based EARL systems. Alecsys is
an example of a distributed rule-based EARL method. Genitor is a single chromosome
neural-net system, and Sane is a distributed neural net system. This brief survey should
266

fiEvolutionary Algorithms for Reinforcement Learning

provide a starting point for those interested in investigating the evolutionary approach to
reinforcement learning.

10.1

Samuel
Samuel (Grefenstette et al., 1990) is an EARL system that combines Darwinian and Lamarckian evolution with aspects of temporal difference reinforcement learning. Samuel has

been used to learn behaviors such as navigation and collision avoidance, tracking, and herding, for robots and other autonomous vehicles.
Samuel uses a single-chromosome, rule-based representation for policies, that is, each
member of the population is a policy represented as a rule set and each gene is a rule that
maps the state of the world to actions to be performed. An example rule might be:
IF range = [35; 45] AND bearing = [0; 45] THEN SET turn = 16 (strength
0.8)
The use of a high-level language for rules offers several advantages over low-level binary
pattern languages typically adopted in genetic learning systems. First, it makes it easier to
incorporate existing knowledge, whether acquired from experts or by symbolic learning programs. Second, it is easier to transfer the knowledge learned to human operators. Samuel
also includes mechanisms to allow coevolution of multiple behaviors simultaneously. In
addition to the usual genetic operators of crossover and mutation, Samuel uses more traditional machine learning techniques in the form of Lamarckian operators. Samuel keeps a
record of recent experiences and will allow operators such as generalization, specialization,
covering, and deletion to make informed changes to the individual genes (rules) based on
these experiences.
Samuel has been used successfully in many reinforcement learning applications. Here
we will briey describe three examples of learning complex behaviors for real robots. In
these applications of Samuel, learning is performed under simulation, reecting the fact
that during the initial phases of learning, controlling a real system can be expensive or
dangerous. Learned behaviors are then tested on the on-line system.
In (Schultz & Grefenstette, 1992; Schultz, 1994; Schultz & Grefenstette, 1996), Samuel
is used to learn collision avoidance and local navigation behaviors for a Nomad 200 mobile
robot. The sensors available to the learning task were five sonars, five infrared sensors,
and the range and bearing to the goal, and the current speed of the vehicle. Samuel
learned a mapping from those sensors to the controllable actions { a turning rate and a
translation rate for the wheels. Samuel took a human-written rule set that could reach
the goal within a limited time without hitting an obstacle only 70 percent of the time, and
after 50 generations was able to obtain a 93.5 percent success rate.
In (Schultz & Grefenstette, 1996), the robot learned to herd a second robot to a \pasture". In this task, the learning system used the range and bearing to the second robot, the
heading of the second robot, and the range and bearing to the goal, as its input sensors.
The system learned a mapping from these sensors to a turning rate and steering rate. In
these experiments, success was measured as the percentage of times that the robot could
maneuver the second robot to the goal within a limited amount of time. The second robot
implemented a random walk, plus a behavior that made it avoid any nearby obstacles. The
first robot learned to exploit this to achieve its goal of moving the second robot to the goal.
267

fiMoriarty, Schultz, & Grefenstette

Samuel was given an initial, human-designed rule set with a performance of 27 percent,
and after 250 generations was able to move the second robot to the goal 86 percent of the
time.
In (Grefenstette, 1996) the Samuel EA system is combined with case-based learning to
address the adaptation problem. In this approach, called anytime learning (Grefenstette &
Ramsey, 1992), the learning agent interacts both with the external environment and with
an internal simulation. The anytime learning approach involves two continuously running
and interacting modules: an execution module and a learning module. The execution
module controls the agent's interaction with the environment and includes a monitor that
dynamically modifies the internal simulation model based on observations of the actual agent
and the environment. The learning module continuously tests new strategies for the agent
against the simulation model, using a genetic algorithm to evolve improved strategies, and
updates the knowledge base used by the execution module with the best available results.
Whenever the simulation model is modified due to some observed change in the agent or the
environment, the genetic algorithm is restarted on the modified model. The learning system
operates indefinitely, and the execution system uses the results of learning as they become
available. The work with Samuel shows that the EA method is particularly well-suited
for anytime learning. Previously learned strategies can be treated as cases, indexed by the
set of conditions under which they were learned. When a new situation is encountered, a
nearest neighbor algorithm is used to find the most similar previously learned cases. These
nearest neighbors are used to re-initialize the genetic population of policies for the new case.
Grefenstette (1996) reports on experiments in which a mobile robot learns to track another
robot, and dynamically adapts its policies using anytime learning as its encounters a series
of partial system failures. This approach blurs the line between online and oine learning,
since the online system is being updated whenever the oine learning system develops an
improved policy. In fact, the oine learning system can even be executed on-board the
operating mobile robot.

10.2

Alecsys

As described previously, Alecsys (Dorigo & Colombetti, 1998) is a distributed rule-based
EA that supports an approach to the design of autonomous systems called behavioral engineering. In this approach, the tasks to be performed by a complex autonomous systems are
decomposed into individual behaviors, each of which is learned via a learning classifier systems module, as shown in Figure 9. The decomposition is performed by the human designer,
so the fitness function associated with each LCS can be carefully designed to reect the role
of the associated component behavior within the overall autonomous system. Furthermore,
the interactions among the modules is also preprogrammed. For example, the designer may
decide that the robot should learn to approach a goal except when a threatening predator
is near, in which case the robot should evade the predator. The overall architecture of the
set of behaviors can then be set such that the evasion behavior has higher priority than
the goal-seeking behavior, but the individual LCS modules can evolve decision rules for
optimally performing the subtasks.
Alecsys has been used to develop behavioral rules for a number of behaviors for
autonomous robots, including complex behavior groups such as Chase/Feed/Escape
268

fiEvolutionary Algorithms for Reinforcement Learning

(Dorigo & Colombetti, 1998). The approach has been implemented and tested on both
simulated robots and on real robots. Because it exploits both human design and EARL
methods to optimize system performance, this method shows much promise for scaling up
to realistic tasks.

10.3

Genitor

Genitor (Whitley & Kauth, 1988; Whitley, 1989) is an aggressive, general purpose genetic

algorithm that has been shown effective when specialized for use on reinforcement-learning
problems. Whitley et al. (1993) demonstrated how Genitor can eciently evolve decision
policies represented as neural networks using only limited reinforcement from the domain.
Genitor relies solely on its evolutionary algorithm to adjust the weights in neural
networks. In solving RL problems, each member of the population in Genitor represents a
neural network as a sequence of connection weights. The weights are concatenated in a realvalued chromosome along with a gene that represents a crossover probability. The crossover
gene determines whether the network is to be mutated (randomly perturbed) or whether a
crossover operation (recombination with another network) is to be performed. The crossover
gene is modified and passed to the offspring based on the offspring's performance compared
to the parent. If the offspring outperforms the parent, the crossover probability is decreased.
Otherwise, it is increased. Whitley et al. refer to this technique as adaptive mutation,
which tends to increase the mutation rate as populations converge. Essentially, this method
promotes diversity within the population to encourage continual exploration of the solution
space.
Genitor also uses a so-called \steady-state" genetic algorithm in which new parents are
selected and genetic operators are applied after each individual is evaluated. This approach
contrasts with \generational" GAs in which the entire population is evaluated and replaced
during each generation. In a steady-state GA, each policy is evaluated just once and retains
this same fitness value indefinitely. Since policies with lower fitness are more likely to be
replaced, it is possible that a fitness based on a noisy evaluation function may have an
undesirable inuence on the direction of the search. In the case of the pole-balancing RL
application, the fitness value depends on the length of time that the policy can maintain
a good balance, given a randomly chosen initial state. The fitness is therefore a random
variable that depends on the initial state. The authors believe that noise in the fitness
function had little negative impact on learning good policies, perhaps because it was more
dicult for poor networks to obtain a good fitness than for good networks (of which there
were many copies in the population) to survive an occasional bad fitness evaluation. This
is an interesting general issue in EARL that needs further analysis.
Genitor adopts some specific modification for its RL applications. First, the representation uses a real-valued chromosome rather than a bit-string representation for the weights.
Consequently, Genitor always recombines policies between weight definitions, thus reducing potentially random disruption of neural network weights that might result if crossover
operations occurred in the middle of a weight definition. The second modification is a very
high mutation rate which helps to maintain diversity and promote rapid exploration of the
policy space. Finally, Genitor uses unusually small populations in order to discourage
different, competing neural network \species" from forming within the population. Whit269

fiMoriarty, Schultz, & Grefenstette

ley et al. (1993) argue that speciation leads to competing conventions and produces poor
offspring when two dissimilar networks are recombined.
Whitley et al. (1993) compare Genitor to the Adaptive Heuristic Critic (Anderson,
1989, AHC), which uses the TD method of reinforcement learning. In several different
versions of the common pole-balancing benchmark task, Genitor was found to be comparable to the AHC in both learning rate and generalization. One interesting difference
Whitley et al. found was that Genitor was more consistent than the AHC in solving the
pole-balancing problem when the failure signals occurs at wider pole bounds (make the
problem much harder). For AHC, the preponderance of failures appears to cause all states
to overpredict failure. In contrast, the EA method appears more effective in finding policies
that obtain better overall performance, even if success is uncommon. The difference seems
to be that the EA tends to ignore those cases where the pole cannot be balanced, and concentrate on successful cases. This serves as another example of the advantages associated
with search in policy space, based on overall policy performance, rather than paying too
much attention to the value associated with individual states.

10.4

Sane

The Sane (Symbiotic, Adaptive Neuro-Evolution) system was designed as a ecient method
for building artificial neural networks in RL domains where it is not possible to generate
training data for normal supervised learning (Moriarty & Miikkulainen, 1996a, 1998). The
Sane system uses an evolutionary algorithm to form the hidden layer connections and
weights in a neural network. The neural network forms a direct mapping from sensors to
actions and provides effective generalization over the state space. Sane's only method of
credit assignment is through the EA, which allows it to apply to many problems where
reinforcement is sparse and covers a sequence of decisions. As described previously, Sane
uses a distributed representation for policies.
Sane offers two important advantages for reinforcement learning that are normally not
present in other implementations of neuro-evolution. First, it maintains diverse populations.
Unlike the canonical function optimization EA that converge the population on a single solution, Sane forms solutions in an unconverged population. Because several different types
of neurons are necessary to build an effective neural network, there is inherent evolutionary
pressure to develop neurons that perform different functions and thus maintain several different types of individuals within the population. Diversity allows recombination operators
such as crossover to continue to generate new neural structures even in prolonged evolution.
This feature helps ensure that the solution space will be explored eciently throughout the
learning process. Sane is therefore more resilient to suboptimal convergence and more
adaptive to changes in the domain.
The second feature of Sane is that it explicitly decomposes the search for complete solutions into a search for partial solutions. Instead of searching for complete neural networks
all at once, solutions to smaller problems (good neurons) are evolved, which can be combined to form an effective full solution (a neural network). In other words, Sane effectively
performs a problem reduction search on the space of neural networks.
Sane has been shown effective in several different large scale problems. In one problem,
Sane evolved neural networks to direct or focus a minimax game-tree search (Moriarty
270

fiEvolutionary Algorithms for Reinforcement Learning

& Miikkulainen, 1994). By selecting which moves should be evaluated from a given game
situation, Sane guides the search away from misinformation in the search tree and towards
the most effective moves. Sane was tested in a game tree search in Othello using the
evaluation function from the former world champion program Bill (Lee & Mahajan, 1990).
Tested against a full-width minimax search, Sane significantly improved the play of Bill,
while examining only a subset of the board positions.
In a second application, SANE was used to learn obstacle avoidance behaviors in a
robot arm (Moriarty & Miikkulainen, 1996b). Most approaches for learning robot arm
control learn hand-eye coordination through supervised training methods where examples
of correct behavior are explicitly given. Unfortunately in domains with obstacles where the
arm must make several intermediate joint rotations before reaching the target, generating
training examples is extremely dicult. A reinforcement learning approach, however, does
not require examples of correct behavior and can learn the intermediate movements from
general reinforcements. Sane was implemented to form neuro-control networks capable of
maneuvering the OSCAR-6 robot arm among obstacles to reach random target locations.
Given both camera-based visual and infrared sensory input, the neural networks learned to
effectively combine both target reaching and obstacle avoidance strategies.
For further related examples of evolutionary methods for learning neural-net control
systems for robotics, the reader should see (Cliff, Harvey, & Husbands, 1993; Husbands,
Harvey, & Cliff, 1995; Yamauchi & Beer, 1993).

11. Summary
This article began by suggesting two distinct approaches to solving reinforcement learning
problems; one can search in value function space or one can search in policy space. TD
and EARL are examples of these two complementary approaches. Both approaches assume
limited knowledge of the underlying system and learn by experimenting with different policies and using reinforcement to alter those policies. Neither approach requires a precise
mathematical model of the domain, and both may learn through direct interactions with
the operational environment.
Unlike TD methods, EARL methods generally base fitness on the overall performance
of a policy. In this sense, EA methods pay less attention to individual decisions than TD
methods do. While at first glance, this approach appears to make less ecient use of
information, it may in fact provide a robust path toward learning good policies, especially
in situations where the sensors are inadequate to observe the true state of the world.
It is not useful to view the path toward practical RL systems as a choice between EA
and TD methods. We have tried to highlight some of the strengths of the evolutionary
approach, but we have also shown that EARL and TD, while complementary approaches,
are by no means mutually exclusive. We have cited examples of successful EARL systems
such as Samuel and Alecsys that explicitly incorporate TD elements into their multilevel credit assignment methods. It is likely that many practical applications will depend
on these kinds of multi-strategy approaches to machine learning.
We have also listed a number of areas that need further work, particularly on the theoretical side. In RL, it would be highly desirable to have a better tools for predicting the
amount of experience needed by a learning agent before reaching a specified level of per271

fiMoriarty, Schultz, & Grefenstette

formance. The existing proofs of optimality for both Q-learning and EA are of extremely
limited practical use in predicting how well either approach will perform on realistic problems. Preliminary results have shown that the tools of PAC analysis can be applied to both
EA an TD methods, but much more effort is needed in this direction.
Many serious challenges remain in scaling up reinforcement learning methods to realistic applications. By pointing out the shared goals and concerns of two complementary
approaches, we hope to motivate further collaboration and progress in this field.

References

Anderson, C. W. (1989). Learning to control an inverted pendulum using neural networks.
IEEE Control Systems Magazine, 9, 31{37.
Barto, A. G., Sutton, R. S., & Watkins, C. J. C. H. (1990). Learning and sequential
decision making. In Gabriel, M., & Moore, J. W. (Eds.), Learning and Computational
Neuroscience. MIT Press, Cambridge, MA.
Belew, R. K., McInerney, J., & Schraudolph, N. N. (1991). Evolving networks: Using
the genetic algorithm with connectionist learning. In Farmer, J. D., Langton, C.,
Rasmussen, S., & Taylor, C. (Eds.), Artificial Life II Reading, MA. Addison-Wesley.
Chrisman, L. (1992). Reinforcement learning with perceptual aliasing: The perceptual
distinctions approach. In Proceedings of the Tenth National Conference on Artificial
Intelligence, pp. 183{188 San Jose, CA.
Cliff, D., Harvey, I., & Husbands, P. (1993). Explorations in evolutionary robotics. Adaptive
Behavior, 2, 73{110.
Cobb, H. G., & Grefenstette, J. J. (1993). Genetic algorithms for tracking changing environments. In Proc. Fifth International Conference on Genetic Algorithms, pp. 523{530.
Collins, R. J., & Jefferson, D. R. (1991). Selection in massively parallel genetic algorithms.
In Proceedings of the Fourth International Conference on Genetic Algorithms, pp.
249{256 San Mateo, CA. Morgan Kaufmann.
Dayan, P., & Sejnowski, T. J. (1996). Exploration bonuses and dual control. Machine
Learning, 25 (1), 5{22.
De Jong, K. A. (1975). An Analysis of the Behavior of a Class of Genetic Adaptive Systems.
Ph.D. thesis, The University of Michigan, Ann Arbor, MI.
Dorigo, M., & Colombetti, M. (1998). Robot Shaping: An Experiment in Behavioral Engineering. MIT Press, Cambridge, MA.
Fiechter, C.-N. (1994). Ecient reinforcement learning. In Proceedings of the Seventh
Annual ACM Conference on Computational Learning Theory, pp. 88{97. Association
for Computing Machinery.
Fogel, L. J., Owens, A. J., & Walsh, M. J. (1966). Artificial Intelligence through Simulated
Evolution. Wiley Publishing, New York.
272

fiEvolutionary Algorithms for Reinforcement Learning

Goldberg, D. E. (1989). Genetic Algorithms in Search, Optimization, and Machine Learning. Addison-Wesley, Reading, MA.
Goldberg, D. E., & Richardson, J. (1987). Genetic algorithms with sharing for multimodal
function optimization. In Proceedings of the Second International Conference on Genetic Algorithms, pp. 148{154 San Mateo, CA. Morgan Kaufmann.
Grefenstette, J. J. (1986). Optimization of control parameters for genetic algorithms. IEEE
Transactions on Systems, Man & Cybernetics, SMC-16 (1), 122{128.
Grefenstette, J. J. (1987). Incorporating problem specific knowledge into genetic algorithms.
In Davis, L. (Ed.), Genetic Algorithms and Simulated Annealing, pp. 42{60 San Mateo,
CA. Morgan Kaufmann.
Grefenstette, J. J. (1988). Credit assignment in rule discovery system based on genetic
algorithms. Machine Learning, 3 (2/3), 225{245.
Grefenstette, J. J. (1992). Genetic algorithms for changing environments. In Manner, R.,
& Manderick, B. (Eds.), Parallel Problem Solving from Nature, 2, pp. 137{144.
Grefenstette, J. J. (1995). Robot learning with parallel genetic algorithms on networked
computers. In Proceedings of the 1995 Summer Computer Simulation Conference
(SCSC '95), pp. 352{257.
Grefenstette, J. J. (1996). Genetic learning for adaptation in autonomous robots. In Robotics
and Manufacturing: Recent Trends in Research and Applications, Volume 6, pp. 265{
270. ASME Press, New York.
Grefenstette, J. J. (1997a). Proportional selection and sampling algorithms. In Handbook of
Evolutionary Computation, chap. C2.2. IOP Publishing and Oxford University Press.
Grefenstette, J. J. (1997b). Rank-based selection. In Handbook of Evolutionary Computation, chap. C2.4. IOP Publishing and Oxford University Press.
Grefenstette, J. J., & Ramsey, C. L. (1992). An approach to anytime learning. In Proc.
Ninth International Conference on Machine Learning, pp. 189{195 San Mateo, CA.
Morgan Kaufmann.
Grefenstette, J. J., Ramsey, C. L., & Schultz, A. C. (1990). Learning sequential decision
rules using simulation models and competition. Machine Learning, 5, 355{381.
Holland, J. H. (1975). Adaptation in Natural and Artificial Systems: An Introductory
Analysis with Applications to Biology, Control and Artificial Intelligence. University
of Michigan Press, Ann Arbor, MI.
Holland, J. H. (1986). Escaping brittleness: The possibilities of general-purpose learning
algorithms applied to parallel rule-based systems. In Machine Learning: An Artificial
Intelligence Approach, Vol. 2. Morgan Kaufmann, Los Altos, CA.
273

fiMoriarty, Schultz, & Grefenstette

Holland, J. H. (1987). Genetic algorithms and classifier systems: Foundations and future
directions. In Proceedings of the Second International Conference on Genetic Algorithms, pp. 82{89 Hillsdale, New Jersey.
Holland, J. H., & Reitman, J. S. (1978). Cognitive systems based on adaptive algorithms.
In Pattern-Directed Inference Systems. Academic Press, New York.
Husbands, P., Harvey, I., & Cliff, D. (1995). Circle in the round: state space attractors for
evolved sighted robots. Robot. Autonomous Systems, 15, 83{106.
Kaelbling, L. P., Littman, M. L., & Moore, A. W. (1996). Reinforcement learning: A survey.
Journal of Artificial Intelligence Research, 4, 237{285.
Koza, J. R. (1992). Genetic Programming: On the Programming of Computers by Means
of Natural Selection. MIT Press, Cambridge, MA.
Lee, K.-F., & Mahajan, S. (1990). The development of a world class Othello program.
Artificial Intelligence, 43, 21{36.
Lin, L.-J., & Mitchell, T. M. (1992). Memory approaches to reinforcement learning in nonMarkovian domains. Tech. rep. CMU-CS-92-138, Carnegie Mellon University, School
of Computer Science.
McCallum, A. K. (1995). Reinforcement Learning with Selective Perception and Hidden
State. Ph.D. thesis, The University of Rochester.
Moriarty, D. E., & Miikkulainen, R. (1994). Evolving neural networks to focus minimax
search. In Proceedings of the Twelfth National Conference on Artificial Intelligence
(AAAI-94), pp. 1371{1377 Seattle, WA. MIT Press.
Moriarty, D. E., & Miikkulainen, R. (1996a). Ecient reinforcement learning through
symbiotic evolution. Machine Learning, 22, 11{32.
Moriarty, D. E., & Miikkulainen, R. (1996b). Evolving obstacle avoidance behavior in a
robot arm. In From Animals to Animats: Proceedings of the Fourth International
Conference on Simulation of Adaptive Behavior (SAB-96), pp. 468{475 Cape Cod,
MA.
Moriarty, D. E., & Miikkulainen, R. (1998). Forming neural networks through ecient and
adaptive co-evolution. Evolutionary Computation, 5 (4), 373{399.
Potter, M. A. (1997). The Design and Analysis of a Computational Model of Cooperative
Coevolution. Ph.D. thesis, George Mason University.
Potter, M. A., & De Jong, K. A. (1995). Evolving neural networks with collaborative
species. In Proceedings of the 1995 Summer Computer Simulation Conference Ottawa,
Canada.
Potter, M. A., De Jong, K. A., & Grefenstette, J. (1995). A coevolutionary approach to
learning sequential decision rules. In Eshelman, L. (Ed.), Proceedings of the Sixth
International Conference on Genetic Algorithms Pittsburgh, PA.
274

fiEvolutionary Algorithms for Reinforcement Learning

Rechenberg, I. (1964). Cybernetic solution path of an experimental problem. In Library
Translation 1122. Royal Aircraft Establishment, Farnborough, Hants, Aug. 1965.
Ring, M. B. (1994). Continual Learning in Reinforcement Environments. Ph.D. thesis, The
University of Texas at Austin.
Ros, J. P. (1997). Probably approximately correct (PAC) learning analysis. In Handbook of
Evolutionary Computation, chap. B2.8. IOP Publishing and Oxford University Press.
Schaffer, J. D., Caruana, R. A., Eshelman, L. J., & Das, R. (1989). A study of control
parameters affecting online performance of genetic algorithms for function optimization. In Proceedings of the Third International Conference on Genetic Algorithms,
pp. 51{60. Morgan Kaufmann.
Schaffer, J. D., & Grefenstette, J. J. (1985). Multi-objective learning via genetic algorithms.
In Proceedings of the Ninth International Joint Conference on Artificial Intelligence,
pp. 593{595. Morgan Kaufmann.
Schultz, A. C. (1994). Learning robot behaviors using genetic algorithms. In Intelligent
Automation and Soft Computing: Trends in Research, Development, and Applications,
pp. 607{612. TSI Press, Albuquerque.
Schultz, A. C., & Grefenstette, J. J. (1992). Using a genetic algorithm to learn behaviors for
autonomous vehicles. In Proceedings of the AiAA Guidance, Navigation, and Control
Conference Hilton Head, SC.
Schultz, A. C., & Grefenstette, J. J. (1996). Robo-shepherd: Learning complex robotic behaviors. In Robotics and Manufacturing: Recent Trends in Research and Applications,
Volume 6, pp. 763{768. ASME Press, New York.
Smith, S. F. (1983). Flexible learning of problem solving heuristics through adaptive search.
In Proceedings of the Eighth International Joint Conference on Artificial Intelligence,
pp. 422{425. Morgan Kaufmann.
Sutton, R. (1990). Integrated architectures for learning, planning, and reacting based on
approximate dynamic programming. In Machine Learning: Proceedings of the Seventh
International Conference, pp. 216{224.
Sutton, R. S. (1988). Learning to predict by the methods of temporal differences. Machine
Learning, 3, 9{44.
Sutton, R. S., & Barto, A. (1998). Reinforcement Learning: An Introduction. MIT Press,
Cambridge, MA.
Valiant, L. G. (1984). A theory of the learnable. Communications of the ACM, 27, 1134{
1142.
Vose, M. D., & Wright, A. H. (1995). Simple genetic algorithms with linear fitness. Evolutionary Computation, 2, 347{368.
275

fiMoriarty, Schultz, & Grefenstette

Watkins, C. J. C. H. (1989). Learning from Delayed Rewards. Ph.D. thesis, University of
Cambridge, England.
Watkins, C. J. C. H., & Dayan, P. (1992). Q-learning. Machine Learning, 8 (3), 279{292.
Whitley, D. (1989). The GENITOR algorithm and selective pressure. In Proceedings of the
Third International Conference on Genetic Algorithms, pp. 116{121 San Mateo, CA.
Morgan Kaufman.
Whitley, D., & Kauth, J. (1988). GENITOR: A different genetic algorithm. In Proceedings
of the Rocky Mountain Conference on Artificial Intelligence, pp. 118{130 Denver, CO.
Whitley, D., Dominic, S., Das, R., & Anderson, C. W. (1993). Genetic reinforcement
learning for neurocontrol problems. Machine Learning, 13, 259{284.
Wilson, S. W. (1994). ZCS: A zeroth level classifier system. Evolutionary Computation,
2 (1), 1{18.
Yamauchi, B. M., & Beer, R. D. (1993). Sequential behavior and learning in evolved
dynamical neural networks. Adaptive Behavior, 2, 219{246.
Zhou, H. (1990). CSM: A computational model of cumulative learning. Machine Learning,
5 (4), 383{406.

276

fiJournal of Artificial Intelligence Research 11 (1999) 429{435

Submitted 6/99; published 12/99

Technical Addendum
Cox's Theorem Revisited
Joseph Y. Halpern

Cornell University, Computer Science Department
Ithaca, NY 14853
http://www.cs.cornell.edu/home/halpern

halpern@cs.cornell.edu

Abstract

The assumptions needed to prove Cox's Theorem are discussed and examined. Various
sets of assumptions under which a Cox-style theorem can be proved are provided, although
all are rather strong and, arguably, not natural.

I recently wrote a paper (Halpern, 1999) casting doubt on how compelling a justification
for probability is provided by Cox's celebrated theorem (Cox, 1946). I have received (what
seems to me, at least) a surprising amount of response to that article. Here I attempt to
clarify the degree to which I think Cox's theorem can be salvaged and respond to a glaring
inaccuracy on my part pointed out by Snow (1998). (Fortunately, it is an inaccuracy that
has no affect on either the correctness or the interpretation of the results of my paper.) I
have tried to write this note with enough detail so that it can be read independently of
my earlier paper, but I encourage the reader to consult the earlier paper as well as the two
major sources it is based on (Cox, 1946; Paris, 1994), for further details and discussion.
Here is the basic situation. Cox's goal is to \try to show that . . . it is possible to derive
the rules of probability from two quite primitive notions which are independent of the notion
of ensemble and which . . . appeal rather immediately to common sense" (Cox, 1946). To
that end, he starts with a function Bel that associates a real number with each pair (U; V )
of subsets of a domain W such that U 6= ;. We write Bel(V jU ) rather than Bel(U; V ), since
we think of Bel(V jU ) as the belief, credibility, or likelihood of V given U . Cox's Theorem
as informally understood, states that if Bel satisfies two very reasonable restrictions, then
Bel must be isomorphic to a probability measure. The first one says that the belief in V
complement (denoted V ) given U is a function of the belief in V given U ; the second says
that the belief in V \ V 0 given U is a function of the belief in V 0 given V \ U and the belief
in V given U . Formally, we assume that there are functions S : IR ! IR and F : IR2 ! IR
such that
A1. Bel(V jU ) = S (Bel(V jU )) if U 6= ;, for all U; V  W .
A2. Bel(V \ V 0 jU ) = F (Bel(V 0 jV \ U ); Bel(V jU )) if V \ U 6= ;, for all U; V; V 0  W .
If Bel is a probability measure, then we can take S (x) = 1 , x and F (x; y ) = xy .
Before going on, notice that Cox's result does not claim that Bel is a probability measure,
just that it is isomorphic to a probability measure. Formally, this means that there is a
continuous one-to-one onto function g : IR ! IR such that g  Bel is a probability measure
on W , and
g (Bel(V jU ))  g (Bel(U )) = g (Bel(V \ U )) if U 6= ;,
(1)

c 1999 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiHalpern

where Bel(U ) is an abbreviation for Bel(U jW ).
If we are willing to accept that belief is real valued (this is a strong assumption since,
among other things, it commits us to the assumption that beliefs cannot be incomparable|
for any two events U and V , we must have either Bel(U )  Bel(V ) or Bel(V )  Bel(U )),
then A1 and A2 are very reasonable. If this were all it took to prove Cox's Theorem, then
it indeed would be a very compelling argument for the use of probability.
Unfortunately, it is well known that A1 and A2 by themselves do not suce to prove
Cox's Theorem. Dubois and Prade (1990) give an example of a function Bel, defined on a
finite domain, that satisfies A1 and A2 with F (x; y ) = min(x; y ) and S (x) = 1 , x but is
not isomorphic to a probability measure. Thus, if we are to prove Cox's Theorem, we need
to have additional assumptions.
It is hard to dig out of Cox's papers (1946, 1978) exactly what additional assumptions
his proofs need. I show in my paper that the result is false under some quite strong
assumptions (see below). My result also suggests that most of the other proofs given of
Cox-style theorems are at best incomplete (that is, they require additional assumptions
beyond those stated by the authors); see my previous paper for discussion. The goal of
this note is to clarify what it takes to prove a Cox-style theorem, by giving a number of
hypotheses under which the result can be proved. All of the positive versions of the theorem
that I state can be proved in a straightforward way by adapting the proof given by Paris
(1994). (This is the one correct, rigorous proof of the result of which I am aware, with
all the hypotheses stated clearly.) Nevertheless, I believe it is worth identifying all these
variants, since they are philosophically quite different.
Paris (1994) proves Cox's Theorem under the following additional assumptions:
Par1. The range of Bel is [0; 1].
Par2. Bel(;jU ) = 0 and Bel(U jU ) = 1 if U 6= ;.
Par3. The S in A1 is decreasing.
Par4. The F is A2 is strictly increasing (in each coordinate) in (0; 1]2 and continuous.
Par5. For all 0  ff; fi;   1 and  > 0, there are sets U1  U2  U3  U4 such that U3 6= ;,
and each of jBel(U4jU3 ) , ffj, jBel(U3jU2 ) , fi j, and jBel(U2jU1) ,  j is less than .

Theorem 1: (Paris, 1994) If Par1-5 hold, then Bel is isomorphic to a probability measure.
There is nothing special about 0 and 1 in Par1 and Par2; all we need to assume is that
there is some interval [e; E ] with e < E such that Bel(V jU ) 2 [e; E ] for all V; U  W ,
Bel(;jU ) = e, and Bel(U jU ) = E . These assumptions certainly seem reasonable, provided
we accept that beliefs should be linearly ordered. Nor is it hard too hard to justify Par3
and Par4 (indeed, Cox justifies them in his original paper). The problematic assumption
here is Par5 (called A4 in my earlier paper and Co5 by Paris (1994)). Par5 can be thought
of as a density requirement; among other things, it says that for each fixed V , the set of
values that Bel(U jV ) takes on is dense in [0; 1]. It follows that, in particular, to satisfy
Par5, W must be infinite; Par5 cannot be satisfied in finite domains. While \natural" and
\reasonable" are, of course, in the eye of the beholder, it does not strike me as a natural
430

fiCox's Theorem Revisited

or reasonable assumption in any obvious sense of the words. This is particularly true since
many domains of interest in AI (and other application areas) are finite; any version of Cox's
Theorem that uses Par5 is simply not applicable in these domains. Can we weaken Par5?
Cox does not require anything like Par5 in his paper. He does require at various times
that F be twice differentiable, with a continuous second derivative, and that S be twice
differentiable.1 While differentiability assumptions are perhaps not as compelling as continuity assumptions, they do seem like reasonable technical restrictions. Unfortunately, the
counterexample I give in my earlier paper shows that these assumptions do not suce to
prove Cox's theorem. What I show is the following.

Theorem 2: (Halpern, 1999) There is a function Bel0, a finite domain W , and functions

S and F satisfying A1 and A2, respectively, such that

 Bel0(V jU ) 2 [0; 1] for U 6= ;,
 S (x) = 1 , x (so that S is strictly decreasing and infinitely differentiable),
 F is infinitely differentiable, nondecreasing in each argument in [0; 1]2, and strictly increasing in each argument in (0; 1]2. Moreover, F is commutative, F (x; 0) = F (0; x) =
0, and F (x; 1) = F (1; x) = x.

However, Bel0 is not isomorphic to a probability measure.

To understand what the makes counterexample tick and the role of Par5, it is useful to
review part of Cox's argument. In the course of his proof, Cox shows that A2 forces F to
be an associative function, that is, that
F (x; F (y; z )) = F (F (x; y ); z ):

(2)

Here is Cox's argument.
Suppose U1  U2  U3  U4 . Let x = Bel(U4jU3), y = Bel(U3jU2), z = Bel(U2jU1 ),
u1 = Bel(U4 jU2), u2 = Bel(U3 jU1), and u3 = Bel(U4 jU1). By A2, we have that u1 = F (x; y ),
u2 = F (y; z ), and u3 = F (x; u2) = F (u1 ; z ). It follows that F (x; F (y; z )) = F (F (x; y ); z ).
Note that this argument does not show that F (x; F (y; z )) = F (F (x; y ); z ) for all x; y; z .
It shows only that the equality holds for those x; y; z for which there exist U1  U2  U3 
U4 such that x = Bel(U1 jU2), y = Bel(U2jU3 ), and z = Bel(U3jU4). Par5 guarantees that
the set of such x; y; z is dense in [0; 1]3. Combined with the continuity of F assumed in
Par4, this tells us that (2) holds for all x; y; z .
I had claimed in my earlier paper that none of the authors who had proved variants
of Cox's Theorem, including Cox himself, Aczel, and Reichenbach, seemed to be aware of
the need to make (2) hold for all x; y; z .2 I was wrong in including Cox in this list. (This
is the glaring inaccuracy I referred to above.) As Snow (1998) points out, Cox actually
does realize that F must satisfy (2) for all x; y; z , and explicitly makes this assumption at
1. Cox never collects his assumptions in any one place, so it is somewhat dicult to tell exactly what he
thinks he needs for his proof. More on this later.
2. As I pointed out in in my earlier paper, Aczel recognized this problem in later work (Aczel & Daroczy,
1975).

431

fiHalpern

a certain point in his first paper (Cox, 1946), although he does not make this assumption
explicitly in his (more informal) later paper (Cox, 1978).
Unfortunately, although Cox escapes from my criticism by recognizing the need to make
this assumption, it does not make his theorem any less palatable. Indeed, if anything, it
makes matters worse. Associativity is a rather strong assumption, as Cox himself shows.
In fact, Cox shows that if F is associative and has continuous second derivatives, then
F is isomorphic to multiplication, that is, there exists a function f and constant C such
that Cf [F (x; y )] = f (x)f (y ). Let me stress that the conclusion that F is isomorphic to
multiplication just follows from the fact that it is associative and has continuous second
derivatives, and has nothing to do with A2. Of course, by the time we are willing to assume
that there is a function F that is isomorphic to multiplication that satisfies A2, then we
are well on the way to showing that Bel is isomorphic to a probability measure. For future
reference, I remark that Paris shows (in his Lemma 3.7) that Par1, Par2, Par4, and Par5
suce to show that F is isomorphic to multiplication (and that we can take C = 1).
In any case, suppose we are willing to strengthen Par4 so as to require F to be associative
as well as continuous and strictly increasing. Does this suce to get rid of Par5 altogether?
Unfortunately, it does not seem to.
Later in his argument, Cox shows that S must satisfy the following two functional
equations for all sets U1  U2  U3 :
S [S (Bel(U2jU1 ))] = Bel(U2 jU1)

(3)

and
Bel(U2 jU1) S (Bel(U3 jU1)=Bel(U2jU1)) = S [S (Bel(U2jU1))=S (Bel(U3jU1 ))]S (Bel(U3jU1 ))
(4)
This means that for all x and y > 0 for which there exist sets U1, U2 , and U3 such that
x = Bel(U3jU1) and y = Bel(U2 jU1), we have
S (S (y )) = y

(5)

and

yS (x=y ) = S (x)S [S (y )=S (x)]:
(6)
Cox actually wants these equations to hold for all x and y . Paris shows that this follows from
Par1{5. (Here is Paris's argument. Using Par3, it can be shown that S is continuous (see

(Paris, 1994, Lemma 3.8)). This combined with Par5 easily gives us that (5) holds for all
y 2 [0; 1]. (6) follows from Par5 and the fact that F must be isomorphic to multiplication;
as I mentioned above, the latter fact is shown by Paris to follow from Par1, Par2, Par4, and
Par5.) Without Par5, we need to assume that (5) and (6) both hold for all x and y , and
that is what Cox does.3
In the proof given by Paris for Theorem 1, the only use made of Par5 is in deriving the
associativity of F and the fact that S satisfies (5) and (6). Thus, we immediately get the
following variant of Cox's Theorem.
3. Actually, Cox starts with (4) and derives the more symmetric functional equation yS [S (x)=y] =
xS [S (y)=x], rather than (6). It is this latter functional equation that he assumes holds for all x and y.
If we replace x by S (x) everywhere and use (5), then we get (6).

432

fiCox's Theorem Revisited

Theorem 3: If Par1-4 hold and, in addition, the F in A2 is associative and the S in A1
satisfies both (5) and (6) for all x; y 2 [0; 1], then Bel is isomorphic to a probability measure.
I stress here that A1 and A2 place constraints only on how F and S act on the range of
Bel (that is, on elements x of the form Bel(U ) for some subset U of W ), while associativity,
(5), and (6) place constraints on the global behavior of F and S , that is, on how F and S
act even on arguments not in the range of Bel. The example I give in my earlier paper can
be viewed as giving a Bel for which it is possible to find F and S satisfying A1 and A2, but
there is no F satisfying A2 which is associative on [0; 1].
We can get a variant even closer to what Cox (1946) shows by replacing Par4 by the
assumption that F is twice differentiable. Note that we need to make some continuity,
monotonicity, or differentiability assumptions on F . As I mentioned earlier, Dubois and
Prade show there is a Bel that is not isomorphic to a probability function for which S (x) =
1 , x and F (x; y ) = min(x; y ). The min function is differentiable (and a fortiori continuous),
but is not twice differentiable, nor is it strictly increasing in each coordinate in (0; 1]2
(although it is nondecreasing).
The advantage of replacing Par5 by the requirement that F be associative and that S
satisfy (5) and (6) is that this variant of Cox's Theorem now applies even if W is finite. On
the other hand, it is hard (at least for me) to view (6) as a \natural" requirement. While
assumptions like associativity for F and idempotency for S (i.e., (5)) are certainly natural
mathematical assumptions, the only justification for requiring them on all of [0; 1] seems to
be that they provably follow from the other assumptions for certain tuples in the range of
Bel. Is this reasonable or compelling? Of course, that is up to the reader to judge. In any
case, these are assumptions that needed to be highlighted by anyone using Cox's Theorem
as a justification for probability, rather than being swept under the carpet. The requirement
that S must satisfy (6) is not even mentioned by Snow (1998), let alone discussed. Snow is
not alone; it does not seem to be mentioned in any other discussion of Cox's results either
(other than by Paris). Of course, we can avoid mentioning (5) and (6) by just requiring
that S (x) = 1 , x (as Cox (1978) does). However, this makes the result less compelling.
A number of other variants of Cox's Theorem which are correct are discussed in (Halpern,
1999, Section 5). Let me conclude by formalizing two of them that apply to finite domains,
but use Par5 (or slight variants of it), rather than assuming that F must be associative and
that S must satisfy (5) and (6) for all pairs x; y 2 [0; 1].
The first essentially assumes that we can extend any finite domain to an infinite domain
by adding a suciently many \irrelevant" propositions, such as the tosses of fair coin. As
I observed in my earlier paper, this type of extendability argument is fairly standard. For
example, it is made by Savage (1954) in the course of justifying one of his axioms for
preference. Snow (1998) essentially uses it as well. Formally, this gives us the following
variant of Cox's Theorem, whose proof is a trivial variant of that of Theorem 1.

Theorem 4: Given a function Bel on a domain W , suppose there exists a domain W +  W
and a function Bel+ extending Bel defined on all subsets of W + such that A1 and A2 hold
for Bel+ and all subsets U; V; V 0 of W + and Par1-5 hold for Bel+ . Then Bel+ (and hence
Bel) is isomorphic to a probability measure.

433

fiHalpern

The problem with this approach is that it requires us to extend Bel to events we were never
interested in considering in the first place, and to do so in a way that is guaranteed to
continue to satisfy Par1-5.
The second variant assumes that Bel is defined not just on one domain W , but on all
domains (or at least, a large family of domains); the functions F and S then have to be
uniform across all domains. More precisely, we would get the following.

Theorem 5: Suppose we have a function Bel defined on all domains W in some set W of

domains, there exist functions F and S such that F and S satisfy A1 and A2 for all the
domains W 2 W , Par1{4 hold for F and S , and the following variant of Par5 holds:

Par50 . for all 0  ff; fi;   1 and  > 0, there exists W 2 W and sets U1 ; U2; U3; U4  W
such that U1  U2  U3  U4 , U3 6= ;, and each of jBel(U4 jU3) , ffj, jBel(U3jU2 ) , fi j,
and jBel(U2jU1 ) ,  j is less than .
Then Bel is uniformly isomorphic to a probability measure, in that there exists a function
g : IR ! IR such that for all W 2 W , we have that g  Bel is a probability measure on each
W and for all U; V  W , we have
g (Bel(V jU ))  g (Bel(U )) = g (Bel(V

\ U )) if U 6= ;.

The advantage of this formulation is that W can consist of only finite domains; we never
have to venture into the infinite (although then W would have to include infinitely many
finite domains). This conception of one function Bel defined uniformly over a family of
domains seems consistent with the philosophy of both Cox and Jaynes (see, in particular,
(Jaynes, 1996)).
While the hypotheses of Theorems 4 and 5 may seem more reasonable than some others
(at least, to some readers!), note that they still both essentially require Par5 and, like all
the other variants of Cox's Theorem that I am aware of, disallow a notion of belief that has
only finitely many gradations. One can justify a notion of belief that takes on all values
in [0; 1] by continuity considerations (again, assuming that one accepts a linearly-ordered
notion of belief), but it is still a nontrivial requirement.4
I will stop at this point and leave it to the reader to form his or her own beliefs.

Acknowledgments

I'd like to thank Paul Snow for some useful email exchanges on this topic (and for pointing
out that Cox had in fact realized the need to assume that F was associative for all (x; y; z )).
This work was supported in part by the NSF, under grant IRI-96-25901.
4. Snow (1998) quotes the conference version of (Halpern, 1999) (which appeared in AAAI '96, pp. 1313{
1319) as saying `Cox's Theorem \disallows a notion of belief that takes on only finitely many or countably
many gradations",' but what I say disallows a notion of belief is not Cox's Theorem, but the viewpoint
that assumed that Bel varies continuously from 0 to 1. In fact, Co5 is compatible with a notion of
belief that takes on countably many (although not finitely many) values. (Essentially the same sentence
appears in the journal version of the paper, where it does refer to Cox's Theorem, but without the phrase
\or countably".)

434

fiCox's Theorem Revisited

References

Aczel, J., & Daroczy, Z. (1975). On Measures of Information and Their Characterizations.
Academic Press, New York.
Cox, R. (1946). Probability, frequency, and reasonable expectation. American Journal of
Physics, 14 (1), 1{13.
Cox, R. (1978). Of inference and inquiry: An essay in inductive logic. In Levine, R. D.,
& Tribus, M. (Eds.), The Maximum Entropy Formalism, pp. 119{167. MIT Press,
Cambridge, Mass.
Dubois, D., & Prade, H. (1990). The logical view of conditioning and its application to
possibility and evidence theories. International Journal of Approximate Reasoning,
4 (1), 23{46.
Halpern, J. Y. (1999). A counterexample to theorems of Cox and Fine. Journal of A.I.
Research, 10, 76{85.
Jaynes, E. T. (1996). Probability Theory|The Logic of Science. Unpublished; available at
http://bayes.wustl.edu.
Paris, J. B. (1994). The Uncertain Reasoner's Companion. Cambridge University Press,
Cambridge, U.K.
Savage, L. J. (1954). Foundations of Statistics. John Wiley & Sons, New York.
Snow, P. (1998). On the correctness and reasonableness of Cox's Theorem for finite domains.
Computational Intelligence, 14 (3), 452{459.

435

fiJournal of Artificial Intelligence Research 11 (1999) 301-333

Submitted 3/99; published 10/99

Decentralized Markets versus Central Control:
A Comparative Study
Fredrik Ygge

ygge@enersearch.se

EnerSearch AB and Uppsala University
Chalmers Science Park
S-412 88 Gothenburg, Sweden
www.enersearch.se/ygge

Hans Akkermans

HansAkkermans@cs.vu.nl

AKMC and Free University Amsterdam
Department of Information Management and Software Engineering
Computer Science Division
De Boelelaan 1081a, NL-1081 HV Amsterdam, The Netherlands

Abstract
Multi-Agent Systems (MAS) promise to offer solutions to problems where established,
older paradigms fall short. In order to validate such claims that are repeatedly made
in software agent publications, empirical in-depth studies of advantages and weaknesses of
multi-agent solutions versus conventional ones in practical applications are needed. Climate
control in large buildings is one application area where multi-agent systems, and marketoriented programming in particular, have been reported to be very successful, although
central control solutions are still the standard practice. We have therefore constructed and
implemented a variety of market designs for this problem, as well as different standard
control engineering solutions. This article gives a detailed analysis and comparison, so as
to learn about differences between standard versus agent approaches, and yielding new
insights about benefits and limitations of computational markets. An important outcome
is that local information plus market communication produces global control.

1. Introduction
When new paradigms arise on the scientific horizon, they must prove their value in comparison and competition with existing, more established ones. The multi-agent systems
(MAS) paradigm is no exception. In a recent book on software agents (Bradshaw, 1997),
Norman observes that perhaps the most relevant predecessors to todays intelligent agents
are servomechanisms and other control devices. And indeed, a number of applications for
which multi-agent systems have recently claimed success, are close to the realm of what is
traditionally called control engineering. One clear example is the climate control of large
buildings with many office rooms. Here, Huberman & Clearwater (1994, 1995) have constructed and tested a working MAS solution based on a market approach, that they reported
to outperform existing conventional control.
The key question studied in this article is: in what respect and to what extent are
multi-agent solutions better than their (conventional) alternatives? We believe that the
c
1999
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiYgge & Akkermans

above-mentioned application provides a nice opportunity to study this question in a detailed empirical way. It is practically very relevant, it lends itself to alternative solutions,
and it is quite prototypical for a wide range of industrial applications in distributed resource allocation (including energy management applications (Ygge & Akkermans, 1996;
Akkermans & Ygge, 1997; Ygge & Akkermans, 1998), telecoms applications, the file allocation problem of Kurose and Simha (1989), and the flow problems investigated by Wellman
(1993)).
This article gives a detailed analysis of a published MAS solution to building climate
control, and it compares multi-agent markets with traditional control approaches. We also
introduce an improved and novel multi-agent solution to this problem based on an equilibrium market. From our comparative analysis we are able to draw general conclusions about
the suitability of the various approaches. Briefly, we show how computational markets can
be designed that perform as well as centralized controllers having global information about
the total system. However, a major advantage of the market framework is that it achieves
this in a fully decentralized fashion using only locally available data. We finally outline that
it should be possible to come to a more general theory concerning the connections between
markets and conventional control concepts. Here, we show that for the considered type
of applications the quasi-equation local data + market communication = global control
holds.
The structure of the article is as follows. After an introduction (Section 2) to marketoriented programming and available theory, Section 3 gives the problem definition. Section 4
introduces the application domain: it describes the office environment and gives the physical model for cooling power and the various temperature and outside weather influences.
We then discuss the results of a standard control engineering solution, based on local and
independent integral controllers regulating the building climate (Section 5). Next, we review the market-based approach as put forward by Huberman & Clearwater (1994, 1995)
(Section 6), and validate their claim that this market approach performs better than conventional independent controllers. We subsequently analyze this market protocol in detail
and show that its success is related to the fact that the agents possess global information
before the auction is started (Section 7). In Section 8 we then develop an improved standard control engineering scheme that also exploits global data. Such a control scheme turns
out to perform much better than the Huberman-Clearwater market. Finally, we propose a
market design of our own based on general equilibrium theory. It performs as well as the
controller having access to global data, but operates on local data only and thus represents
a really decentralized solution (Section 9). Section 10 puts our results into perspective and
summarizes the general conclusions in comparing different approaches.

2. Market-oriented Programming
The use of market mechanisms for resource allocation by computer systems has a rather
long history in computer science, e.g. (Sutherland, 1968), and more recently markets have
been used in a number of different application areas.

302

fiDecentralized Market Control

There is rather extensive theory available for the relations between optimization and
markets. For example:
 Jennergren (1973) has shown how a price schedule decomposition algorithm can be
used for solving linear programming problems.
 Kurose & Simha (1989) showed how an equilibrium constitutes an optimal solution
to a file allocation problem. A similar result was shown by Bertsekas (1992) for the
assignment problem.
 Bikhchandani & Mamer (1997) showed that a Pareto efficient outcome1 maximizes
the sum of the utilities in an exchange market with agents holding quasi-linear utility
functions2 This is a significant extension to the above, as it also applies to non-linear
problems.3
 The results of Bikhchandani and Mamer were extended to markets that include production and uncertainty by Ygge (1998, Chapter 3). The relations between these
types of markets and traditional optimization problems were also made more explicit
in the latter work.
Markets approaches have been applied to a number of different computerized applications, and some of them are briefly discussed here. Already in 1968 Sutherland proposed an
auction mechanism for allocation of computational resources on a PDP-1 computer (Sutherland, 1968). Different amounts of money were assigned to different users in accordance with
the importance of their projects. It was reported that the users then over time learn how
to bid properly for computational resources. This basic approach has been further refined
by, for example, Gagliano et al. (Gagliano et al., 1995).
Kurose & Simha (1989) investigate a file allocation problem. In this work the agents
report their marginal utility (for having a certain amount of storage) and its derivative to
an auctioneer, which reallocates the resource using a resource-oriented Newton-Raphson
algorithm (cf. Ygge & Akkermans, 1998) until an equilibrium has been reached in which
the marginal utilities of all agents are the same. Although the paper is based on a number of
microeconomic abstractions, it does not for example utilize prices and there is no trade-off
between different commodities. So, it is debatable whether this approach can be referred
to as really market-based.4
The assigning problem of allocating n objects to n users has been investigated by Bertsekas (1992). Each user has a valuation of each object, and cannot be assigned more than
one object. It is reported that each user can be seen as an economic agent, and it is shown
how an auction (which essentially is an English auction) results in an equilibrium. The
1. An allocation is Pareto efficient or Pareto optimal if there is no alternative allocation that makes any
agent better off without making the outcome worse for some other agent (Varian, 1996, p. 15).
2. An example of a quasi-linear utility function is given in Eq. (19).
3. However, one should remember that this theory does not provide any computational advantages. If it is
hard to find the allocation that maximizes the sum of the utilities, then it is also hard to find a Pareto
optimal one (as they are the same in this case).
4. One should also note that the formulation of the Newton-Raphson scheme of this approach is overly
simplistic, and there are much better standard methods available (Press et al., 1994; Ygge & Akkermans,
1998).

303

fiYgge & Akkermans

Trading Agent

Auctioneer
Initiate Auction H
H
Supply = Demand?

HH
j Express demand

1





No
XX
XXX
Yes
z Register and implement
X

Figure 1: High-level view of an equilibrium market mechanism.
assumed agent behavior is that an agent bids v  w for its most highly valued object (i.e.
the object with the highest difference, v, between the valuation and the price). w is the
difference between the valuation and the price of the second most preferred object. There
is however no motivation given for the assumed agent behavior, and it is not clear why any
agent would use this strategy unless it can be externally imposed. Though there is some
economical interpretation of the prices, the approach is not very market-like due to the
rather unrealistic assumptions of agent behavior.
The equilibria in the applications by Kurose and Simha, and Bertsekas are proven to be
optimal. It is not hard to see that the problem of Kurose and Simha can be reformulated
as a proper market with quasi-linear utility functions, and that the competitive equilibrium then is equivalent to the equilibrium described by Kurose and Simha, cf. (Ygge, 1998,
Corollary 3.3.1). Similarly, the price vector obtained by Bertsekas clearly constitutes a
competitive equilibrium. At the same time, it is shown by Ygge (Ygge, 1998, Theorem 3.2),
that all separable optimization problems can be formulated in market terms and the (competitive) general equilibrium (if existing) is identical to the optimal solution to the original
optimization problem. Consequently, this recent theory generalizes the earlier theory by for
example Kurose and Simha, and Bertsekas.
Wellman et al. have contributed significantly in developing market-based approaches to
resource allocation into a programming paradigm, which they have given the name marketoriented programming, e.g. (Wellman, 1993; Mullen & Wellman, 1995; Wellman, 1995, 1996;
Yamaki, Wellman, & Ishida, 1996; Hu & Wellman, 1996; Cheng & Wellman, 1998; Wellman
& Hu, 1998; Walsh, Wellman, Wurman, & MacKie-Mason, 1998). Particularly, the microeconomic framework of general equilibrium theory has been successfully used as a resource
allocation mechanism. In such a market which we call an equilibrium market agents send
demand functions telling how much they like to consume or produce at different prices.
The auctioneer then tries to establish an equilibrium price vector such that supply meets
demand for all commodities, cf. Figure 1.
The process of submitting parts of the demand function may be iterated if an equilibrium
price is outside the region captured by the submitted demand functions. One such process
is the basic price tatonnement process, cf. e.g. (Cheng & Wellman, 1998), in which demand
functions for the respective commodities are sent to an auctioneer. Each of those demands is
based on expected prices of the other commodities. That is, if those other prices change, a set
of new demand functions may need to be submitted. Once the auctioneer has established
an equilibrium price, the agents will exchange the resources as stated by their bid and
304

fiDecentralized Market Control

the equilibrium price. (For example, if an agent states that it wants to buy 1/p units of
a commoditywhere p is the price in the commodity moneyand the equilibrium price
becomes 1, the agent will buy one unit of resource for one unit of money.)
Equilibrium markets have many attractive theoretical properties. For example, if all
agents act competitively5 , the outcome is Pareto efficient. Furthermore, if all utility functions are quasi-linear, the outcome is globally optimal (Ygge, 1998, Theorem 3.2), and
in the presence of uncertainty, the outcome maximizes the expected global utility (Ygge,
1998, Theorem 3.5). We note that equilibrium markets be computationally implemented in
a computationally very efficient manner (Ygge, 1998, Chapter 4).
Wellman et al. have applied equilibrium markets to a number of applications, such
as multi-commodity flow problems, design problems, and bandwidth allocation problems.
They have also introduced a market-based approach to scheduling which has many similarities with the assignment problem of Bertsekas described above, but relies on more realistic
assumptions on agent behavior.
The present authors have introduced a market-oriented approach to power load management (Ygge & Akkermans, 1996; Ygge, 1998; Ygge et al., 1999).
We note that the aim of market-oriented programming in computer science is fundamentally different from the aim of economic theory. This is visualized in Figure 2. In
market-oriented programming, microeconomic theory is taken as given and serves as the
theory for implementation of computational agents. Whether or not the microeconomic
theory actually reflects human behavior is not the critical issue. The important question
is instead how microeconomic theory can be utilized for the implementation of successful
resource allocation mechanisms in computer systems. For example, even though no (or at
least very few) people believe that humans use explicit utility functions when making their
decisions, such functions do appear very useful to concisely represent human preferences for
use in computational agents.
It is obviously very interesting to investigate the use of computational markets for automating trades between different self-interested parties, where information is private and
is revealed only if there is an expected gain from doing so. However, the use of markets has
also been proposed for standard resource allocation where the true utility/costs for all nodes
that consume and produce resource is assumed to be available (though possibly uncertain
and/or distributed in the system). The main arguments found in the literature for applying
market to these types of problems are:
 The numerous similarities between economic systems and distributed computer systems suggest that models and methods previously developed within the field of mathematical economics can serve as blueprints for engineering similar mechanisms in
distributed computer systems (Kurose & Simha, 1989).
 Auction algorithms are highly intuitive and easy to understand; they can be explained
in terms of economic competition concepts, which are couched on everyday experience (Bertsekas, 1992).
5. An agent that acts competitively treats prices as exogenous, that is, the impact on the prices due to its
own behavior is negligible (Varian, 1996, p. 516). This is a very reasonable assumption if the market is
of at least moderate size and/or if there is uncertainty about the behavior of the other agents (Sandholm
& Ygge, 1997).

305

fiYgge & Akkermans

Micro-economic theory
Study
Generate
Computer
scientist

Economist
Study
Generate
Real World

Human agents on
human markets

Computational agents on
computational markets

Figure 2: A simplified view on the relation between economics and computer science with respect
to microeconomic theory. Economists study humans and generate theory that is used
for the explanation of human economic behavior. Computer scientists doing marketoriented programming use this theory as a basis for building working computational market
systems.

 Market approaches enable a natural decomposition, both from a software engineering
perspective as well as from a computational perspective (Schreiber et al., 1999; Ygge,
1998, Chapter 15).
 Market approaches are very flexible in that they allow for ongoing addition and deletion of agents. No global changes are requiredmerely the demand/supply relation
is altered (Ygge, 1998, Chapter 15).
 Markets are informationally efficient in terms of information dimensionality (Jordan,
1982), and the abstractions used are the most natural ones for the user (Ygge, 1998,
Chapter 15).
 The introduction of trading resources for some sort of money enables evaluation of
local performance and valuation of resources, so that it becomes apparent which resources are the most valuable and which agents are using the most of these (Ygge,
1998, Chapter 15).
The above arguments are mainly conceptual and related to software/system design and
engineering issues, and must prove their value from acceptance by software and system
designers. But there are also different and in some respects stronger claims. For example, Huberman and Clearwater state (Huberman & Clearwater, 1995) for the application
of building control that: While in principle an omniscient central controller with access
to all the environmental and thermal parameters of a building (i.e. a perfect model) could
306

fiDecentralized Market Control

optimally control it, in practice such knowledge is seldom available to the system. Instead,
partial information about local changes in the variables (such as instantaneous office occupancy, external temperature, and computer use) is the only reliable source that can be used
for controlling the building. As an alternative to such an omniscient controller Huberman
and Clearwater propose a market-based multi-agent approach to this problem. This raises
a very interesting question: Are there applications in which the information structure is
such that market-based approaches do better than traditional approaches? In this paper
we will carefully examine this issue for a building climate control problem. We will investigate what information different alternatives (traditional and new market-based) require,
and what the performance of these different approaches are. We will use exactly the same
problem formulation as Huberman and Clearwater in order to reproduce their results and
evaluate other new approaches with their original formulation. This problem formulation
is given in the next section.

3. Problem Definition
The task is to allocate a resource (cold air) in an office building, given the setpoint temperatures of the respective offices. As a measure of the success of the allocation, the standard
deviation of the deviation from the setpoint is used (Huberman & Clearwater, 1995), i.e.
v
u
N
u1 X
setp
StdDev(Ti  T
)=t
[(Tio  Tosetp )  (hTi i  hT setp i)]2 ,

N

(1)

o=1

where hi denotes the average value of a variable, Tio is the actual temperature of an office,
and Tosetp is the setpoint temperature. The index i denotes a time interval under observation
and the index o denotes the office under observation. This naming convention is used
throughout the article. (In addition the index k will also be used to denote time periods.)
It may be debated whether or not this is the best measure, but we stick to it in this
article in order to evaluate the approach taken by Huberman and Clearwater using their
own measure.

4. The Office Environment
The case study we consider for the comparison of decentralized markets versus central
control solutions is the building climate control of a large office environment.
In this section, we present a mathematical-physical model of the office environment.
We first give a conceptual summary so that it is possible to understand the basic ideas of
the model without studying the equations. The offices are attached to a pipe in which the
resource (cold air) is transported as in Figure 3. The characteristics of this system are similar
to the characteristics of a district heating system, but with offices instead of households.
We assume that there are 100 offices in total, and that they are equally distributed towards
East, South, West, and North.
The thermodynamics of the office environment is actually quite simple. Every office is
seen as a storage place for heat, but heat may dissipate to its environment. In the model,
the thermodynamic behavior of an office is equivalent to a basic electrical RC-circuit. Here,
307

fiYgge & Akkermans

Noon sun

Morning sun

Afternoon sun

Air out
Air in
Figure 3: Offices and air transportation.
voltage is analogous to temperature, and electrical current is analogous to heat flow. C and
R then respectively denote heat capacitance and thermal resistance.
A good general reference on thermodynamic models as the one we use here is the book
by Incropera & Witt (1990). The heat equations are continuous in time, but are discretized
according to standard procedures from control engineering, cf. (Ogata, 1990). The ontology and reusability aspects involved in thermodynamics model construction are discussed
extensively by Borst, Akkermans, & Top (1997).
4.1 Thermodynamic Properties
The resource treated is cooling power. Each office can make use only of a fraction, , of the
available resource at that office, Pioavail , so that
Piocons    Pioavail ,

(2)

where Piocons is the consumed power. The available resource at one office is equal to the
available resource at the previous office minus the consumed resource at the previous office.
Throughout this article we assume an  of 0.5.
We treat everything in discrete time. The time interval we use in the calculations is
one minute. For all offices the temperature, Tio , is obtained by integrating a differential
equation in discretized form:
Tio = T0,o +

i
X

heat
cons
(Pko
 Pko
)/Co ,

(3)

k=1
heat
Pko

where
is the heating power and Co is the thermal capacitance. The heating power is
described by
Pioheat = (Tiovirt  Tio )/Ro ,
(4)
308

fiDecentralized Market Control

where Ro is the thermal resistance and Tiovirt is a virtual outdoor temperature, described in
more detail below.
From Eqs. (3) and (4) we see that there is a feedback loop between the office temperature
and the heating power. Solving for the temperature we obtain


Tio =

1
1+

1
Ro Co

Ti1,o +

virt
Tio
Ro

 Piocons
Co


 , i > 0.

(5)

This is the equation for the dynamics of the system that can be directly computed. At the
right-hand side we have known quantities, where Co and Ro are externally given building
parameters, Piocons is the output of the utilized controller (as will be described in detail later
in the article for various different controllers), and Tiovirt is obtained from the weather model
below.
4.2 Weather Model
All external weather influences on the office environment are modeled by a virtual temperature, representing the outdoor temperature, sun radiation, etc. We assume that there
is sunshine every day and that the outdoor temperature, T outd , varies from 22 to 35 C
according to
2
Tioutd = 22 + 13  e((is4) mod 2412) /20 ,
(6)
where s is the length of each time interval expressed in hours, so here s = 1/60.
The virtual temperature, Tiovirt , is described by
Tiovirt = Tioutd + Tosun + Tiof luct,

(7)

where T f luct is a random disturbance, thought to represent small fluctuations caused by for
example the wind. T f luct is Gaussian distributed with zero mean and a standard deviation
equal to unity. T sun is the sun radiation component. For the offices located at the East
side T sun is described by
2 /5

Tisun,East = 8  e((is+4) mod 2412)

,

(8)

and correspondingly for the South and the West offices
2 /5

Tisun,South = 15  e(is mod 2412)
and

,

2 /5

Tisun,W est = 8  e((is4) mod 2412)

(9)

.

(10)

The various temperatures are plotted in Figure 4.
4.3 Office Temperatures without Control
In Figure 5, the temperature for a South-oriented office is plotted with different thermal
resistances, Ro , and thermal capacitances, Co . For simplicity we assume all Ro to be equal
and all Co to be equal. From this figure we observe two things: first, the higher Ro Co
309

fiYgge & Akkermans

35

Time of day, h

Time of day, h

24

21

18

15

9

12

6

0

24

21

18

15

-5

24

21

18

15

9

12

6

3

0

9

0

0

50
40
30
20
10
0

3

5

12

5

6

10

3

15

10

0

20

Temperature C

15

25

Temperature C

Temeprature C

30

Time of day, h

Figure 4: The plot at the left shows the outdoor temperature, Tioutd. The middle plot shows the sun
radiation components, T sun , (with peaks at time 8, 12, and 16 hours for offices located
at the East, South, and West sides, respectively). Finally, the outdoor temperature plus
the sun radiation components are plotted at the right.

45

Temeperature C

40

R=0 or C=0

35

R= 10, C= 10,
30
R=10 and C= 20 or R=20 and
C=10
25

R=20 and C=20

20

22

20

18

16

14

12

10

8

6

4

2

0

15
Time of day, h

Figure 5: The indoor temperature for an uncontrolled office is plotted for different values of the
thermal resistance and the heat capacitance. Small values of the thermal resistance and
capacitance give higher peaks, while higher values give smoother curves.

the bigger the lag in temperature changes, and second, the higher Ro Co the smaller the
fluctuations in temperature. For the simulation experiments in this article we took Ro = 10
and Co = 10. An Ro or a Co equal to zero implies that Tio = Tiovirt , as can be seen by
letting Ro or Co approach zero in Eq. (5). Clearly, without control the office temperatures
strongly fluctuate and reach unacceptably high values in all cases.
310

fiDecentralized Market Control

5. Control-A: Conventional Independent Controllers
5.1 Integral Control for the Offices
The application of regulating office temperature has a long tradition in control theory. The
most widely known controllers are different variants of the PID controller. The letters PID
denote that the control signal is proportional (P) to the error (that is, to the difference
between the setpoint and the actual value); proportional to the integral (I) of the error; or
proportional to the derivative (D) of the error. Here, we use a variant of an integrating
controller6 of the form
Fio = Fi1,o + (Tio  Tosetp ),
(11)
where F is the output signal from the controller, and  is a so-called gain parameter (that
can be set externally in the design of the controller). For the simulations it is assumed
that Fio is limited to a value between zero and three. This is in order to model that the
valves can be closed but that cooling resources are not delivered from one room to another
(the lower bound), and that there is a maximum to the amount of resources that can be
obtained by opening the valve fully (the upper bound). The control signal Fio is sent to the
actuator and the actual Piocons is obtained from
(

Piocons

=

Fio ,
Fio    Pioavail
.
avail
  Pio , Fio >   Pioavail

(12)

Plots of the office temperatures with different gains are shown in Figure 6. The gain of
the controller is not critical for this application. Too high a gain will result in the controller
overreacting when the temperature exceeds the setpoint, after which it will be under the
setpoint for quite some time. This leads to a larger error than if smaller adjustments are
made. Also, the amplitude of the control signal then gets unnecessarily high, but the system
does not get dangerously unstable. We note that the maximum deviation here is 0.06 C.
Thus, controllers using any of the three gains perform very well. In the further calculations
of this article a gain equal to 10 has been adopted.
5.2 The Implications of Limited Resources
So far, we have assumed that the total amount of available resources is unlimited. Now,
we suppose that there is a maximum value for the cooling power that is inserted into the
system. In such a situation, offices that are situated close to the air input will obtain a
sufficient amount of cool air, but those near the end will suffer if totally uncoordinated
controllers are used. Thus, the smaller the total amount of available resources, the larger
the standard deviation will be. This is visualized in Figure 7. As a reasonable figure we
have chosen an upper limit for the total resource amount of 140.
We conclude, as shown by the example, that independent integrating controllers perform
very well when the amount of cooling resources is unlimited. On the other hand, when there
6. The main reason for choosing an integrating controller is that we want to have a setting that is as close
as possible to the setting described by Huberman and Clearwater (1995). Other controllers for this
application may well be considered, but as long as the performance is as good as shown in Figure 6, this
is not at all crucial for the central argument of this article.

311

fiYgge & Akkermans

20,06

Temperature, C

20,04

20,02
Gain = 100
Gain = 10

20

Gain = 1
19,98

19,96

19,94
1

11

21

Minutes in an observed interval

Figure 6: The indoor temperature for an office, utilizing an integral controller, is plotted for different
controller gains. The setpoint temperature is 20 C.

6

Standard deviation

5
4
3
2
1

24

23

22

21

20

19

18

17

16

15

14

13

12

11

9

10

8

7

6

5

4

3

2

1

0

0
Time of day, h

Figure 7: The standard deviation (in  C) , as defined in Eq. (1) for each interval, i, is plotted
for different amounts of available cold air resources (130, 140, 150, and 160) with 100
offices. The lower the amount of available resources, the higher the standard deviation.

312

fiDecentralized Market Control

is a shortage of available resources, the standard deviation increases dramatically, yielding
poor performance.

6. Market-A: The Approach by Huberman and Clearwater
A multi-agent systems solution to the problem of building control has been presented by
Huberman and Clearwater (1994, 1995). The approach taken is to model the resource
allocation problem of the building environment as a computational market where agents
buy and sell cooling power resources. The non-separability in terms of agents is ignored.7
The basic idea is that every office is represented by an agent that is responsible for
making good use of its resources, and to this end trades resources with other agents. The
agents send bids to an auctioneer, which calculates a clearing price, and makes sure that
no agent has to buy for a price higher than its bid nor has to sell for a price lower than its
bid.
In this section we give the market protocol proposed by Huberman and Clearwater, and
we reproduce their results. In order to make the paper self-contained, the interested reader
can find the original equations underlying the Huberman-Clearwater market protocol in
Appendix A.
6.1 Market protocol
A bid is constructed as the following tuple:
bid = [sellio , vio , Bio ],

(13)

where sell is a boolean variable indicating whether the current bid is a sell bid (true) or
buy bid (f alse), v is the volume traded for, and B is the price that the agent demands (sell
bid) or is prepared to pay (buy bid).
Each of the variables in Eq. (13) is a function of other variables according to
sellio = sellio (Tio , Tosetp , hTi i, hTosetp i),
vio = vio (Ti , Tsetp , ), and
Bio = Bio (Tio , Tosetp , hTi i, hTosetp i, mio ),

(14)

setp
where Ti = [Ti1 , Ti2 , . . . , Ti100 ], Tsetp = [T1setp , T2setp , . . . , T100
],  is a strength parameter of
the auction, and mio is called a money parameter. The trade volume is directly proportional
to  and, hence, doubling  doubles the traded volume in an auction. Thus,  is a free
parameter that can be externally set to tune the trade volumes to proper levels. The
money parameter does not correspond to any real currency; it is merely a control variable,
varying between 100 and 200, and it does not have any direct connection to, for example,
the value of the cooling resource. The money cannot be saved between auction rounds

7. A resource allocation problem is separable if the total utility of the system of the system can be expressed
as the sum of the utilities of each node (producer/consumer), and the only constraints on how much
resource that can be assigned to each node is only restricted by that their sum must not exceed the total
available resource (Ibaraki & Katoh, 1988). Hence, the problem treated in this paper is not separable as
there are other constraints on how the resource can be distributed. For example, if we assign sufficiently
much to the penultimate agent, the ultimate will have a significant amount of resource available.

313

fiYgge & Akkermans

in the Huberman-Clearwater protocol, another indication that it simply functions as an
adjustment parameter in this protocol.
An auction consists of the following three phases:
1. All agents send their bids, [sellio (Tio , Tosetp , hTi i, hTosetp i,
Bio (Tio , Tosetp , hTi i, hTosetp i, mio )], to the auctioneer.

vio (Ti , Tsetp , ),

2. The auctioneer then computes a market clearing price by searching for the following
minimum:
fi
fi
fi
fi
X
X
fi
fi
fi
min fi
vio 
vio fifi .
pi fi
fi
o|sell=true,Bio pi
o|sell=f alse,Bio pi

(15)

This yields a price such that supply matches demand as closely as possible.
3. Finally, the resources are allocated as dictated by the above bids and the obtained
market price, according to the rule: all sellers that are offering vio for a price, Bio ,
that is lower than or equal to the market price will sell vio ; and correspondingly for
buyers.8
As an example of the computation of the equilibrium price, assume that the auctioneer
has received the following bids [true, 2, 4], [true, 1, 3], [true, 2, 2], [f alse, 1, 3], [f alse, 2, 2],
[f alse, 2, 1]. Then the market clearing price is 3, leading to acceptance of bid 2, 3, 4, and 5.
(Bid 1 is too high a sell price and bid 6 is too low a buy price.)
6.2 Simulations
Figure 8 shows two plots of a simulation for the period between 3 p.m. and 7 p.m.9 The
initial temperatures for all offices were set to 20 C. The upper plot is the standard deviation when independent integrating controllers are used, and the lower one shows the
agent-based control scheme as defined above. We found that for the adjustment parameter
 (see Eq. (23)) a value of 64 led to the smallest overall standard deviation. The key observation from the figure is that the agent approach offers at least one order of magnitude
of improvement.
Compared to independent conventional controllers this is indeed a major advance, and
this is the central claim made by Huberman and Clearwater. The market approach given
above leads to a reduced control error and thus to increased comfort compared to standard controllers. Our simulations thus reproduce and validate the results of Huberman &
Clearwater (1994, 1995).
8. Since the bids are given using discrete volumes, supply will seldomly match demand exactly at the
clearing price. Normally, there will be a very small excess demand or supply. If there is an excess supply,
all buyers that are willing to pay at most the clearing price will buy, but not all sellers that are willing
to accept at least the clearing price can sell. In this situation, a seller is selected randomly from the
valid candidates does deliver only a fraction of its bid. A corresponding procedure is used when there is
a small excess demand.
9. All solutions discussed in this article have been implemented in and simulated by the authors using
C++ on a PC running Windows95. Furthermore, all simulations have been independently recreated and
verified by Bengt Johannesson from various relevant papers (Ygge & Akkermans, 1997; Huberman &
Clearwater, 1995; Clearwater & Huberman, 1994)), as part of a masters thesis project, using Python
on a PC running Linux.

314

fiDecentralized Market Control

4,5
4

Standard deviation

3,5
3
2,5
2
Control-A

1,5

Market-A

1
0,5

19

18

17

16

15

0
Time of day, h

Figure 8: Standard deviation for independent controllers (top) and agent-based control (bottom).
However, this cannot be the last word in a successful system study. A first question to
be raised is whether we can find out in more detail what the actual reason is for the success
of the above market-based approach. A second relevant question is whether there are
alternative, market-based and/or central control-based, solutions to the building control
problem that are even better. The answer to both questions is yes, as we set out to
demonstrate now.

7. Market-A0: A Suite of Variations
In this section we present a suite of variations on the scheme presented in Section 6. The
main aim is to understand which of many factors involved are actually responsible for the
good performance of the Huberman-Clearwater market approach.
7.1 Deleting the Money Dependency
A first simplification is to remove the adjustment parameter called money. This is done by
setting mio to a very high value, C, regardless of the resource allocation.10 The simplified
market protocol then is:
1. All agents send their bids, [sellio (Tio , Tosetp , hTi i, hTosetp i,
Bio (Tio , Tosetp , hTi i, hTosetp i, mio = C)], to the auctioneer.

vio (Ti , Tsetp , ),

2. The auctioneer computes a market price from
fi
fi
fi
fi
X
X
fi
fi
min fifi
vio 
vio fifi .
pi fi
fi
o|sell=true,Bio pi
o|sell=f alse,Bio pi

3. Allocate the resource as dictated by the bids and the market clearing price.
315

(16)

fiYgge & Akkermans

0,25

Standard deviation

0,2

0,15

Market-A
Market-A',
No Money

0,1

0,05

19

18

17

16

15

0
Time of day, h

Figure 9: Standard deviation for the original Huberman-Clearwater scheme and a scheme without
any money dependencies.

Plots of the standard deviation with the original scheme as well as with a scheme where
all money dependencies have been removed are shown in Figure 9. For the scheme without
money, setting the  parameter to 66 turned out to be optimal. The scheme without
money dependencies performs as well as the original scheme. The reason for this is that
Bio s dependency on mio is completely negligible.11 Hence, the money parameter does not
play any significant role in the success of the Huberman-Clearwater market scheme.
7.2 Deleting the Temperature Dependency
Another factor of interest is the impact of the temperature on the bid prices. We remove
this dependency by setting the bid price to 10 for all selling agents and to 100 for all buying
agents. That is, we now use the following market scheme:
1. All agents send their bids,
[sellio (Tio , Tosetp , hTi i, hTosetp i),

(

vio (Ti

, Tsetp , ),

Bio =

10, sell = true
],
100, sell = f alse

to the auctioneer.
2. Now, all prices larger than 10 and smaller than 100 are equally good candidates. If
there is a mismatch between supply and demand, say, supply exceeds demand, the
agents that will sell are picked randomly among the valid candidates, and the resources
are allocated accordingly.
10. More precisely, this is done by setting U (0, m) in Eq. (28) (Appendix A) to a constant value, 2000. This
corresponds to letting mio approach infinity.
11. In actual fact, U (0, m) in Eq. (28) of Appendix A will vary only between 1999.86 and 2000 for all possible
mio .

316

fiDecentralized Market Control

0,25

Standard deviation

0,2

0,15
Market-A',
No Money
0,1
Market A',
No Money,
No Temp

0,05

0
15

16

17
Time of day, h

18

19

Figure 10: Standard deviation for the scheme with the money dependencies removed, compared to
a scheme with both the money and temperature dependencies removed.

In Figure 10 the standard deviation is plotted for the scheme without money, mentioned
above, and for a scheme where the dependency on the temperature has been removed as
well. Here, setting the fit parameter  to 65 turned out to be optimal. We see that also
here the performance is as good as that of the original scheme. Note that the temperature
is still used to determine both vio and the boolean variable sell. But the conclusion is that
the temperature dependency does not affect the quality of the market-based solution to the
building control problem.
7.3 Deleting the Auction
Next, we let the agents assign their bids to themselves without any auction whatsoever. This
means that the sum total of the controller outputs, Fio , might sometimes exceed the total
resource amount and sometimes be below that. The physical model is of course still obeyed,
so that the total resource amount actually used, the total consumed cooling power Piocons ,
will never exceed the totally available resource amount, as described by Eq. (2). Hence, the
resource updates are described by:
(

+vio (Ti , Tsetp , ), sell = f alse
vio (Ti , Tsetp , ), sell = true
0,
Fio0 < 0
Fio0 , 0  Fio0  3
3,
Fio0 > 3

Fio0 = Fi1,o




Fio =




(17)

The result of this simulation is shown in Figure 11. Here, an  of 17 turned out to be
optimal. The surprising conclusion is that the multi-agent scheme without performing any
auction is roughly ten times better than the auction-based Market-A scheme.12
12. From a practical point of view this is a pathological solution, revealing a loophole in the problem
definition of Clearwater and Huberman, cf. Section 3. The revealed loophole of the problem definition is

317

fiYgge & Akkermans

0,25

0,2

Standard deviation

Market-A

0,15
Market-A', No
Money, No
Temp, No
Auction

0,1

0,05

0
15

16

17

18

19

Time of day, h

Figure 11: Standard deviation with the auction mechanisms completely removed, compared to the
original Huberman-Clearwater Market-A scheme.

7.4 Discussion
At first glance, it might seem counterintuitive that performance actually improves significantly when the core mechanisms of a market are removed. First we showed that introducing
the market improves performance considerably compared to conventional independent control, and then we showed that market mechanisms are superfluous. What, then, is the big
difference between the uncoordinated integrating controller, the Control-A scheme, and
the multi-agent scheme without any auction that we ended up with? The simple answer, in
our view, is the access to global data that each agent has, in terms of the average temperature and the average setpoint temperature (as follows from Eq. (14)), in the Market-A
and Market-A0 schemes. Knowing average quantities means that an agent has some information about the other agents c.q. offices. This information is not available in the case of
independent conventional controllers. The exploitation of this non-local information makes
the big difference.
that it does not take absolute temperature into account, but only the differences between temperature
(i.e. minimizing standard deviation while potentially sacrificing average temperature). In the proper
approaches introduced in the remainder of this article we do not allow ourselves to take advantage
of this loophole, but minimize differences while using all available resource. That is, all approaches
that are introduced from now on solve a problem which is strictly harder than the problem defined
in Section3. What happens in the Market-A scheme without an auction can be seen from observing the
definition of tio in Eq. (22). It is based on a quotient between the setpoint temperature and the actual
temperature. Then the request volume, Eq. (24), is proportional to this quotient. This always results in
a significant excess supply. (Hint, think of a small example with two offices, both with setpoint 20, and
with temperatures 19 and 21. Now, 20
is farther away from 1 than 20
, and hence the selling volume will
19
21
exceed the buying volume.) This implies that at each trade occasion, many offices are prevented from
selling (because of poor definitions of the volumes). The offices that are prevented to sell will deviate
significantly, and hence this results in a high value of the measure. When the auction is deleted, all
agents that are below the average distance to the setpoints are now able to get rid of the resource and
hence the differences in temperature will decrease.

318

fiDecentralized Market Control

Thus, the upshot of our factor analysis of the success of the Huberman-Clearwater
market scheme of Section 6 is that the agents have to share crucial office and control
information first, before the auction takes place. This is why one can remove the auction
without any deterioration of the results: a major function of an auction is to share necessary
information through the bidding procedure, but in the Huberman-Clearwater protocol the
needed information has already been shared. A more appropriate description of this market
scheme would therefore be to say that it consists of not three (as described in Section 6),
but four phases, in which the first phase is that all agents send their current temperatures
Tio as well as their setpoint temperatures Tosetp to all other agents.
In sum, we have now answered the first question raised at the end of the previous section:
What is the reason for the success of the Huberman-Clearwater market scheme, compared
to conventional independent controllers? The single most important factor is contained in
the first step above: before the market procedure is carried out, the agents communicate key
information about their current situation to all other agents. If that is done neither money
nor auction mechanisms are needed. For conventional independent controllers, with which
Huberman and Clearwater compare their own approach, this globally shared information is
not available, however. This explains why these two solutions perform so very differently.
Now, we turn to the second question raised in the previous section: are there alternative
and perhaps better solutions? The above analysis suggests that we look into two different
directions. If one wants to persuade control engineers of the value of agent-based approaches,
it will be more convincing to compare the Huberman-Clearwater scheme not to strictly local
controllers, but to controllers that have access to the same global information as the agents
have. Secondly, this dependency on global information by the agents is a highly undesirable
feature that clearly runs counter to a decentralized computational market philosophy. So,
one should investigate whether it is possible to design a market protocol that exploits only
strictly local information in the agents bid construction. These issues we investigate in
the following two sections, first treating central control and subsequently devising a strictly
local market protocol. It will be seen that both yield significantly improved performance.

8. Control-B: A Standard Controller Using Global Data
Having concluded that the access to global data is crucial for performance, it is of course
of interest to analyze what the performance would be of an integrating controller, like the
one introduced in Eq. (11), but now incorporating global data.
We would like the controller to take into account not only its own deviation from its setpoint, but to consider also the deviations of the other offices from their setpoints. Therefore,
the controller in Eq. (11) is extended to
Fio = Fi1,o + [(Tio  Tosetp )  (hTi i  hT setp i)],

(18)

where  is set to 10 as previously. Piocons is, as before, obtained from Eq. (12).
The plot from the simulation with this controller, compared to the Market-A simulation, and to the Market-A0 simulation where the auction was removed, is shown in
Figure 12.
We see that the standard deviation is approximately the same for the Control-B and
the Market-A0 schemes. Thus, the Control-B scheme also performs roughly ten times
319

fiYgge & Akkermans

0,25

Standard deviation

0,2
Market-A
0,15
Market-A',
No Money,
No Temp,
No Auction
Control-B

0,1

0,05

19

18

17

16

15

0
Time of day, h

Figure 12: Standard deviation with an integrating controller that utilizes global data compared to

the Huberman-Clearwater Market-A scheme, and to the Market-A0 scheme with the
auction removed.

better than Market-A. An important difference, though, is that Control-B employs a
well-known integrating controller for which there are well-understood methods and theories
for e.g. stability analysis.13 In contrast, the Market-A scheme is not easily analyzable
from a formal theory perspective, since it does not rely on such well established concepts.
While Control-B is relatively clear, see Eq. (18), it is much harder to conceptually grasp
the principles behind Market-A, as it consists of a large number of complicated and not
easily justified equations (see Eqs. (21)(33) in Appendix A). The main conclusion from
the present section is that, if we enable conventional controllers to act upon the same global
information as the Huberman-Clearwater agents do, the central control approach performs
best and is easiest to understand.

9. Market-B: A Market with Local Data Only
Thus, the computational market Market-A is outperformed by the global control scheme
Control-B. Therefore, an interesting follow-up question is whether a simple and well
performing computational market approach can be devised that does not depend on having available global information, in contrast to both these schemes. In this section we
show that this is indeed the case, such that a decentralized market performs comparably
to fully centralized conventional control. The approach derived in this section relies on
more general theory for the relations between optimization problems and markets(Ygge,
1998, Chapter 3) and (Bikhchandani & Mamer, 1997)  applicable to this and many other
applications.
13. This holds under the assumption that the characteristic time scale of the variations of the average
temperature is much larger than that of the fluctuation of the temperature. This is a very reasonable
assumption in the present case.

320

fiDecentralized Market Control

We use a market-oriented programming approach based on an equilibrium market. The
basic protocol for this is (cf. Figure 1):
1. Each agent submits a net demand function zio (p) to the auctioneer, describing what
change in allocation is desired by agent o at price p.
2. The auctioneer computes an equilibrium, by calculating a price pi such that
P
P
o zio (pi ) = 0 (or, strictly speaking, |
o zio (pi )|  , where  is a small (positive)
numerical tolerance).
3. Each agent receives its demanded resource zio (pi ) as calculated at the obtained market
equilibrium price.
The computation of zio (p) and the process of computing the equilibrium are described
further below. Note that this is a truly distributed approach, as it does not rely on any
global data, or communication of temperatures between the agents before the market communication starts.
9.1 The Relation Between Markets and Conventional Controllers utilizing
Global Data
The performance measure for the system is given by Eq. (1). The best system is therefore
one that minimizes this equation. Hence, the most straightforward move one can think of
to come to a market model, is to take this measure as representing the utility function for
the overall system. So, the utility functions for the individual agents are ideally related to
[(Tio  Tosetp )  (hTi i  hT setp i)]2 . 14 However, this is still a formulation containing global
information.
Thus, we want to obtain a purely local reformulation, by getting rid of the terms with
hT i containing the global information. We might replace them, though, by terms relating
to the changes in the local resource. In doing so, we take inspiration from the standard
controller equations Eqs. (11) and (18), indicating that we get good results (for unconstrained resources) with the update equation Fio = Fi1,o + io , when io has the form
io = (Tio  Tosetp ). The intended interpretation of io is to represent the output that the
local controller would have delivered if it acted independently with unconstrained resources.
In the market setting each agent updates its control signal Fio through Fio = Fi1,o + Fio ,
where Fio is determined by the outcome of the market. Since the resource is only reP
distributed among the agents, we have that N
o=1 Fio = 0. Accordingly, as a step in the
design of a Market-B scheme, based on local data only, we employ the following definition.
Definition 9.1 Let the utility function for the individual office agents be defined by
u(Fio , m) = 2o (Fio  io )2 + m,

(19)

where o is a strength parameter for each office representing its physical properties such as
Ro and Co . (The proper choice of o is discussed in Theorem 9.3 later on.)
Furthermore, let all agents be price-takers.
14. Since utilities are expressions of preference orderings, they are invariant under monotonic transformations.

321

fiYgge & Akkermans

Due to the simple quadratic form of the utility function in Eq. (19), the net demand,
zio (p), is easily computed for price-taking agents by analytical means. Note that the money
m here are fundamentally different from the money of Market-A, c.f. Section 6. Here the
money represents a commodity and the agent needs to decide how much money to trade
for the commodity cold air. As described above, the money in Section 6 is just a control
variable without an intuitive interpretation.
Theorem 9.1 With the utility functions and behavior given by Definition 9.1, there exists
P
a unique equilibrium price pi | o zio (pi ) = 0, and the allocation obtained by the equilibrium
market is Pareto optimal.
Proof. See (Mas-Colell, Whinston, & Green, 1995; Varian, 1992; Takayama, 1985). 2
In this setting, there is a wide variety of different algorithms that are guaranteed to
P
converge to the clearing price pi | o zio (pi ) = 0, in the second step of our equilibrium
market protocol given above. Examples of such algorithms are price tatonnement, Walras (Cheng & Wellman, 1998), and various Newton-Raphson type methods. A detailed
discussion of communication and computation efficiency aspects of such algorithms is given
by Ygge (1998, Chapter 4).
Theorem 9.2 Any Pareto optimal outcome in a market in which the agents hold the utility
function of Eq. (19) is equivalent to an integrating controller that exploits global information as described by the following resource update equation :
Fio = io 

2o

1
hi i,
 h1/2 i

(20)

given that no agent is at its boundary. Here, h1/2 i and hi i are the average 1/2o , and i0
respectively.
Proof.
At a Pareto-optimal allocation where no agent is at its bounds, all
u(Fio )/Fio are equal. (Assume u(Fip )/Fip < u(Fiq )/Fiq , then at any
price u(Fip )/Fip < p < u(Fiq )/Fiq , there exists a (sufficiently small) amount
of Fi , say , such that up (Fip  , mip + p) > up (Fip , mip ) and uq (Fiq + , miq 
p) > uq (Fiq , miq ). Correspondingly for u(Fip )/Fip > u(Fiq )/Fiq . Hence,
u(Fip )/Fip = u(Fiq )/Fiq ).

2N
2o (FiN  iN ). Summing the
PN 1
PN 1
P 1 1
2
equations yields o=1 Fio  o=1 io = N (FiN iN ) N
o=1 2o . Adding FiN iN to
P
both sides and dividing both sides by N together
with the resource constraint ( N
o=1 Fio =
P

Thus, it will hold for every office that Fio  io =



PN

io

N
1
o=1 2

o=1
0) yields
= 2N (FiN  iN ) N o which is equivalent to FiN = iN 
N
1
h i. For reasons of symmetry, this equation holds for all offices. 2
2 h1/2 i i
N

Corollary 9.1 For the special case where all o are equal, and io = (Tio Tosetp ), Eq. (20)
becomes exactly the Control-B scheme captured by Eq. (18).
322

fiDecentralized Market Control

Consequently, it also follows that simulation results for an equilibrium Market-B
scheme, based on the utility function of Eq. (19) with the provisions given in the above corollary, are identical to those of the centralized Control-B scheme, cf. Figure 12. This fully
decentralized market scheme thus performs better than the original Huberman-Clearwater
market protocol.
In sum, we see that (under suitable conditions which are fulfilled here) local data plus
market communication is equivalent to conventional central control utilizing global data.
Even though our proof was based on the assumption that the agents are never at their
boundary values, it will be a close approximation in many practical applications. It should
also be noted that managing the boundaries is not required for a successful implementation.
As seen above, omitting the management of boundaries in the current application leads to
Control-B, which was shown to have a very high performance.
9.2 Finding an Optimal Utility Function
In this section we show how an optimal utility function is constructed in the constrained
case, from an optimal controller for the unconstrained case.15 Earlier we noted that it is
debatable whether or not the measure in Eq. (1) is a good one. One argument against it is
that it allows for pathological solutions, as it does not take the average indoor temperature
into consideration. If we for example increase the indoor temperature in every office by
10 C, all of them could be unbearably hot, while the measure might be minimized. A
reasonable thing to do here, to prevent these kind of solutions, is to treat the average
temperature as given (the result of using all available resources). If we do so, the following
theorem shows how an optimal utility function is constructed in the constrained case, from
an optimal controller for the unconstrained case.
Theorem 9.3 If Tio is a linear function of Fio ,16 and if io minimizes Eq. (1) in the
unconstrained case, then in a market where the utility functions are described by Eq. (19),
Tio
with o = F
, any associated Pareto optimal allocation minimizes Eq. (1) in the conio
strained case, if the resource can be independently allocated among the agents (i.e., when
P
the only constraint on Fio is the upper and lower bound, and N
o=1 Fio = 0), and the
average temperature is considered as given.
P

Proof. Minimizing Eq. (1) boils down to minimizing f (Ti1 , Ti2 , . . . , Tin ) = N
o=1 [(Tio 
Tosetp )(hTi ihT setp i)]2 , since this is just a monotonic transformation. Due to the fact that
f
io minimizes Eq. (1), it holds that F
= 0 for Fio = io . Since the average temperature
io

f
Tio
is considered as given, we have that F
= 2[(Tio (io )  Tosetp )  (hTi i  hT setp i)]  
io
io
for Fio = io . This gives that Tio (io ) = Tosetp + (hTi i  hT setp i). Thus, f can be
P
2
rewritten as N
o=1 [Tio (Fio )Tio (io )] . Since Tio is a linear function of Fio , we have that

P



2

Tio
Tio
Tio (Fio ) = Tio (io ) + F
 (Fio  io ), and hence f becomes N
(Fio 
o=1 Fio
io
2
io ) . As the reallocation of m does not effect the total (summed) utility, minimizing

15. The resource is constrained when the action of any agent is limited by Pio , see Eq. (12) and Figure 7. It
is unconstrained when such a limitation is not present, cf. Figure 6.
16. From the thermal model discussed in Section 4 (especially Eqs. (5) and (12)) we note that this is indeed
the case for a reasonably wide range.

323

fiYgge & Akkermans

P

Tio
f is equivalent to minimizing  N
o=1 uo , with o = Fio . Furthermore, we have that
any Pareto optimal allocation in a market where the utility functions are described by
P
a
Eq. (19) maximizes N
o=1 uo . (Suppose that there is an allocation Fio that does not
PN
b
b
maximize o=1 uo , but that Fio does. Then reallocating Fio to Fio and letting mbio =

PN

uo (F b ,mb )

PN

uo (F a ,ma )

io
io
io
io
o=1
maio + uo (Fioa , maio )  uo (Fiob , mbio ) + o=1
is always a Pareto
N
improvement. Hence, a non-maximized sum implies a non-Pareto optimal allocation, and
a Pareto optimal allocation implies a maximized sum.) Then (Ygge, 1998, Theorem 3.1,
p. 44) implies that any Pareto-optimal allocation in a market where the utility functions
Tio
are described by Eq. (19) with o = F
, minimizes f . 2
io
Thus, we have constructed a fully distributed market design that yields an optimal
outcome. Particularly note that Theorem 9.3 was not based on the assumption that an
integral controller was used, rather it said that if io (the desired resource by the controller)
is optimal, then the proposed utility function generates a globally optimal outcome.

9.3 Discussion
Previously, we saw that the independent controller Control-B that incorporates global
data, viz. the average temperatures, performs very well. In the present section we positively
answered the question if one can construct a market, Market-B, that is based on local
data only and that performs as well.
For this result we have employed a market approach based on general equilibrium theory.
This is of course not the only available mechanism for resource allocation in multi-agent
systems. It seems interesting to try out other mechanisms, like the contract net protocol
(Davis & Smith, 1983), and see if they perform better. However, Theorem 9.3 tells us
that, if we treat the problem of building control as being separable in terms of agents (as
also done by Huberman and Clearwater), there is no better scheme.17 For example, if
we assigned all the resources to an auctioneer, that on its turn would iteratively assign the
total resource in small portions to bidders bidding with their true marginal utility, we would
end up with something close to the competitive equilibrium, but we cannot do better than
Market-B. Furthermore, this would be a computationally extremely inefficient way to
arrive at equilibrium compared to other available methods (Ygge, 1998, Chapter 4). That
is, we can use different mechanisms for achieving the competitive equilibrium, but we can
never hope to find a mechanism that would do better than the Market-B scheme.

17. We note that, as mentioned earlier, the problem is actually not fully separable in terms of agents and
therefore better solutions may exist if this is taken into account. Another observation is that in this article
we have investigated only the case of using the currently available resource as the interesting commodity,
in accordance with the work of Huberman and Clearwater, and we found an optimal mechanism for that.
We note however that extending the negotiations to future resources as well could potentially increase
performance. But this is a different problem setting with different demands on available local and/or
global information items, such as predictions. We have given a solution to this problem in recent work,
see Ygge et al. (1999).

324

fiDecentralized Market Control

10. Conclusions
We believe that both the approach and the results, as presented in this article, pose an
interesting challenge to the software agent community. Multi-agent systems offer a new
way of looking at information systems development, with a potentially large future impact. However, new approaches must prove their value in comparison and competition with
existing, more established ones. The agent paradigm is no exception.
We have therefore deliberately played the role of the devils advocate in this article. In
our view, a key question not yet satisfactorily answered by the software agent community
is: in what respect and to what extent are multi-agent solutions better than their more
conventional alternatives? This article has shown that arguing in favor of the multi-agent
systems approach does require careful analysis beyond the disciplinary boundaries of computer science. Empirical and comparative studies in the multi-agent literature of the kind
carried out in the present article are all too rare as yet. But as we have shown in technical detail, established paradigms such as conventional central control cannot be that easily
dismissed. A similar argument holds, by the way, regarding mathematical optimization
techniques in distributed resource allocation problems, cf. our previous discussions (Ygge
& Akkermans, 1996; Ygge, 1998). Many of them are, in a distributed guise, better than
many newly proposed market protocols.
Abstract considerations alone, concerning the general nature of agenthood, autonomy,
rationality, or cooperation, are not sufficient to prove the value of the agent paradigm. Such
theoretical reflections are worthwhile, but do not diminish the need for thorough analysis of
(agent and market) failures and successes in real-life applications. Therefore, we have taken
a different approach, aimed at obtaining experimental data points on the basis of which
convincing software agent claims can be established.
The data point considered in this article is climate control within a large building consisting of many offices. Given a measure, Eq. (1), a local controller approach, Eq. (11),
current temperatures, and the setpoint temperature, we have investigated the problem on
how to properly distribute the resource among the offices. This is a rather prototypical
application relating to the general problem of optimal resource allocation in a highly distributed environment. This class of problems has already received much attention in the
multi-agent area. Reportedly, this type of application is very suitable for market-oriented
programming (Huberman & Clearwater, 1995). On the other hand, we have devised some
better conventional control engineering solutions, as well as alternative and better market
designs.
The main conclusions of our investigation are:
 The market approach by Huberman & Clearwater (1994, 1995) indeed outperforms
a standard control solution based on local, independent controllers. So, the marketbased multi-agent approach indeed yields a working solution to this type of problem.
Our analysis has shown that the success of this market approach depends on the
agents communicating their local information to all other agents before the auction
starts.
 However, if conventional central control schemes are allowed to exploit the same global
information, they perform even better.
325

fiYgge & Akkermans

 We have proposed an alternative market design based on general equilibrium theory
that uses local data only. It performs better than the Huberman-Clearwater market
and as well as a centralized control scheme having access to global information.
 Our general conclusion can be formulated as a quasi-equation: local information
+ market communication = global control. This holds under suitable conditions
(existence of market equilibrium, price-taking agents) and its validity has been specifically shown here for the case of building climate control (compare particularly the
Control-B and the Market-B schemes we developed). However, as we intend to
show in forthcoming work, this is a much more generally valid result.
 The important difference is that in computational markets this global information is
an emergent property rather than a presupposed concept, as it is in central control.
In our analysis we have focused on the market approach. It is tempting to ask whether
things are different when a non-market multi-agent approach is followed, say, using the
contract net (Davis & Smith, 1983). As we argued, the answer in our opinion is a straightforward no. The goal in the considered class of problems is to find the optimal distributed
solution. Alternative agent approaches, market as well as non-market ones, only change the
multi-agent dynamics on the way to this goal. This might be done in a better or poorer way,
but it is not possible to change this goal itself. The goal state in any multi-agent approach
is, however formulated, equivalent to market equilibrium, the yardstick for having achieved
it is given by some quantitative performance measure as we discussed, and both are stable
across different agent approaches.
Thus, one of the main conclusions of this article is that one must be very careful when
promoting multi-agent approaches to resource allocation problems for which traditional
approaches are available. In particular, one should be cautious using arguments related
to distribution of information (as was done by Huberman and Clearwater), or other computational aspects. Still, we argue that there are conceptual advantages of market-based
approaches to this type of problems, as well as other advantages such as evaluation of local
performance (cf. Section 2). These advantages do not show off very clearly in the present
application setting  for example, it was assumed that all offices have the same thermal
characteristics and office agents on the market are not added and deleted continuously on
the fly  but they are clearly visible in other more general settings.
A final note is that in this article we have devised an optimal market design given the
problem formulation rather than devised an optimal approach to the general problem of
building control. This is in line with the objective of this paper to give a comparative study
of different possible approaches. That is, we have focused on a published and well-known
problem formulation in order to focus on the subject of markets for resource allocation
alone. As stated earlier in the article, several aspects can be improved, for example:
 The local controllers. I-controllers are seldom used, rather PI- or PID-controllers are
preferred. Furthermore, for these kind of computerized settings, modern digital controllers, based on e.g. pole-placement methods are preferable (Astrom & Wittenmark,
1990).
326

fiDecentralized Market Control

 The measure. The average value is at least as important as the standard deviation.
(We have as a consequence excluded pathological solutions from our analysis by taking
the average temperature as given in Theorem 9.3).
 The incentives for office agents to reveal their true preferences in order to avoid speculation. Reasonably, personnel in the offices should have the opportunity to make
trade-offs between comfort and economical value. (It has been shown that generally
speaking it is very difficult to benefit from speculation if the number of agents on the
market is sufficiently large, and/or if uncertainty about the other agents behavior
exists (Sandholm & Ygge, 1997).)
 Taking several time periods into account. As each office serves as a storage for heat,
agents can for example gain by using relatively much resource during certain hours
when the total resource need is small.
That is, a more realistic approach to the building control problem is to use minimized
cost as the measure, give the users real incentives to reveal their energy preferences (by
letting them pay their actual energy costs so that attempted speculation will generally
cost money), and take future time periods into account. Elsewhere we have solved the
problem of dealing with simultaneous optimization over different time periods, by means of
a multi-commodity market (Ygge et al., 1999).
In such a more realistic and large-scale setting, the market-based approach is attractive
compared to its alternatives. The abstractions used (prices and demands) are natural and
easily understood by everyone, and they are uniform over all types of agents (even ones
that do not use the resource for controlling a temperature).
A major qualitative conclusion of this article is that local data plus market communication yields global control. This conclusion is based on the discussed application setting
of building control, as an instance of a distributed resource allocation problem. It suggests
that generally for this type of problem both central control engineering and multi-agent
market solutions can be devised that give comparable optimal control quality. A next step
is to prove this more generally in a rigorous mathematical fashion, delineating the preconditions in more detail. Such a proof can be based upon the (continuous and matrix-algebraic)
dynamic systems theory available from control engineering, and especially upon results concerning what is called optimal control. We believe that this indeed can be done in a formal
way, and as a conjecture we state the following general result. Multi-agent equilibrium markets yield optimal decentralized solutions to distributed resource allocation problems that
are of the same quality, in terms of a given overall systems performance index, as solutions
given by the optimal central (multi-input, multi-output) systems controller that has access
to all relevant local data. These local data involve the total system state and control vectors
(in building control these are the vectors formed from the difference between the actual and
setpoint temperatures for each office, and the cooling power for each office, respectively).
Preconditions for this to be true are: (i) agents act competitively; (ii) the equilibrium exists; (iii) the systems performance index can be written as a linear combination of local
contributions (implying diagonality of certain matrices; if not, agents are not independent).
These local contributions are directly related to the agents utility functions. The agent
approach will be more readily generalizable to large-scale systems and non-linear control
327

fiYgge & Akkermans

solutions (linearity is the main case where control engineering can get analytical mathematical expressions). This is indeed a strong and general statement about the relationship
(and even outcome equivalence) between decentralized markets and central control.

Acknowledgments
The present work was mainly carried out at the University of Karlskrona/Ronneby. We
thank Rune Gustavsson and Hans Ottosson for all their support. A special acknowledgment
goes to Olle Lindeberg whose very detailed comments on the draft papers led to significant
improvements in the simulations and whose ideas also helped us in the design of the market
in Section 9. We benefit significantly from several discussions with Michael Wellman. We
also thank Bengt Johannesson, who, as a part of his masters thesis, went through all details
of this article and independently recreated and verified all the simulations. We thank Arne
Andersson, Eric Astor, and the SoC team for useful comments and discussions of draft
material. This work was partially sponsored by NUTEK and EnerSearch AB.
An earlier version of this paper was presented at the MAAMAW97 workshop (Ygge &
Akkermans, 1997).

328

fiDecentralized Market Control

Appendix A. Original Formulation of the Huberman-Clearwater Market
All formulae in this section were directly taken from the papers by Huberman & Clearwater (1994, 1995).
Trade volumes

First, the decision for an agent to buy or sell is based on
(

T setp
hTi i
tio = o
 setp
Tio hT
i

tio > 1, seller
.
tio < 1, buyer

(21)

Then, the total trade volume, V , is calculated from
Vi =

N
X

|1  tio | ,

(22)

o=1

where N is the number of offices.
Every agent calculates its request volume, v, according to
vio = 

|1  tio |
.
Vi

(23)

When an agent buys or sells its v the actual movement of a valve, called V AV , is
computed from
V AVio = f (f lowio , vio , V AVio ),
(24)
after which the actual V AV position for each interval is updated according to
V AVi+1,o = V AVio + V AVio .

(25)

The function f in Eq. (24) stems from the physics of the office control, but it is not
easy to derive in the general case and it is neither explicitly given in the original papers
(Clearwater & Huberman, 1994; Huberman & Clearwater, 1995). A reasonable and simple
choice is to assume a linear relationship (the cited papers suggest that they do so in practice
as well), replacing Eq. (24) and Eq. (25) by
Fi+1,o = Fio  vio ,

(26)

where plus or minus depend on whether it refers to an accepted buy or sell bid. Pcio is
obtained from Eq. (12). We have employed Eq. (26) in our simulations. The nature of this
assumption is not crucial to our central line of reasoning.
Bids

The bids are based on a marginal utility 18 of the form described by19
setp

U (tio /Tosetp , mio ) = [U (0, mio )](1tio /To

)

(1

= [U (0, mio )]

hTi i
)
Tio hT setp i

,

(27)

18. This is called utility in the work of Huberman and Clearwater. We prefer to use the term marginal utility
instead, because it is directly related to price. This terminology conforms better to that of microeconomic
theory.
19. This notion of a utility function used by Clearwater and Huberman is very much based on the work of
Steiglitz & Honig (1992)

329

fiYgge & Akkermans

with

U (0, mio ) = u3  (u3  u1 )emio ,


and
 = ln

(28)



u3  u 1
,
u3  u 2

(29)

where u1 = 20, u2 = 200, and u3 = 2000, and m is the amount of money that an agent has,
given by
mio = 100(2  V AVio ).
(30)
With the above relation between Fio and V AV , we rewrite Eq. (30) as
mio = 100(2 

Fomax  Fio
).
Fomax

(31)

Observing Eqs. (28) and (29), we note that these equations can be simplified to
U (0, mio ) = u3  (u3  u1 )emio = u3  (u3  u1 )(e )mio =
u3  (u3  u1 )



u3 u1
u3 u2

mio

.

(32)

The bids are calculated from multiplying the marginal utility with the previous price,
price, according to
Bi,o = Uio (tio /Tosetp , mio )  pricei .
(33)
This is the equation given by Huberman and Clearwater. Straightforward application
of Eq. (33) turns out to produce major problems in simulations, however. Equation (27)
ii
shows that U (tio /Tosetp , mio ) is minimized for minimized Tio and maximized hThTsetp
i . As we
can expect Tio to be well above 10 C and

hTi i
hT setp i

to be well below 2, and U (0, m)  2000,
2

we can be sure that U (tio /Tosetp , mio ) will be well above 20001 10  437. Thus, the bidding
price will never be below 437. Then, Eq. (33) tells us that the market price will be at
least price0  437i . This leads to numerical overflow after a few iterations. We note however
that, since all agents multiply their bids with the previous price, this has no effect on the
reallocation itself: it affects only the price level. Therefore, we omit multiplying by the
previous price in our simulations, so that the agents bid prices in Eq. (33) equal their
marginal utilities of Eq. (27). Huberman and Clearwater state that they burn all money
after each auction round in order to avoid inflation and overflow, indicating that they may
adopt a similar procedure. In any case, this procedure leads to exactly the same allocations
but avoids numerical overflow.

330

fiDecentralized Market Control

References
Akkermans, J. M., & Ygge, F. (1997). Smart software as customer assistant in large-scale
distributed load management. In Proceedings of Distribution Automation/Demand
Side Management (DA/DSM) 97. PenWell Conferences and Exhibitions.
Bertsekas, D. (1992). Auction algorithms for network flow problems: A tutorial introduction. Computational Optimization and Applications, pp. 766.
Bikhchandani, S., & Mamer, J. W. (1997). Competitive equilibrium in an exchange economy
with indivisibilities. Journal of Economic Theory, 74, 385413.
Borst, W. N., Akkermans, J. M., & Top, J. L. (1997). Engineering ontologies. International
Journal of Human-Computer Studies, 46, 365406. ISSN 1071-5819.
Bradshaw, J. M. (Ed.). (1997). Software Agents. AAAI Press/The MIT Press, Menlo Park,
CA.
Cheng, J., & Wellman, M. P. (1998). The walras algorithma convergent distributed
implementation of general equilibrium outcomes. Computational Economics, 12, 1
24.
Clearwater, S., & Huberman, B. A. (1994). Thermal markets for controlling building environments. Energy Engineering, 91 (3), 2556.
Davis, R., & Smith, R. G. (1983). Negotiation as a metaphor for distributed problem
solving. Artificial Intelligence, 20 (1), 63109.
Gagliano, R. A., Fraser, M. D., & Schaefer, M. E. (1995). Allocation of compting resources.
Communications of the ACM, 38, 88103.
Hu, J., & Wellman, M. P. (1996). Self-fulfilling bias in multiagent learning. In Tokoro, M.
(Ed.), Proceedings of the Second International Conference on Multi-Agent Systems,
ICMAS96, pp. 118125. AAAI Press, Menlo Park, CA.
Huberman, B. A., & Clearwater, S. (1995). A multi-agent system for controlling building
environments. In Lesser, V. (Ed.), Proceedings of the First International Conference
on Multi-Agent Systems, ICMAS95, pp. 171176. AAAI Press / The MIT Press,
Menlo Park, CA.
Ibaraki, T., & Katoh, N. (1988). Resource Allocation ProblemsAlgorithmic Approaches.
The MIT Press, Cambridge, MA.
Incropera, F. P., & Witt, D. P. D. (1990). Fundamentals of Heat and Mass Transfer. Wiley
and Sons, New York. Third Edition, ISBN 0-471-51729-1.
Jennergren, P. (1973). A price schedules decomposition algorithm for linear programming
problems. Econometrica, 41 (5), 965980.
Jordan, J. S. (1982). The competitive allocation process is informationally efficient uniquely.
Journal of Economic Theory, 28, 118.
331

fiYgge & Akkermans

Kurose, J. F., & Simha, R. (1989). A microeconomic approach to optimal resource allocation
in distributed computer systems. IEEE Transactions on Computers, 38 (5), 705717.
Mas-Colell, A., Whinston, M., & Green, J. R. (1995). Microeconomic Theory. Oxford
University Press.
Mullen, T., & Wellman, M. P. (1995). A simple computational market for network information services. In Lesser, V. (Ed.), Proceedings of the First International Conference
on Multi-Agent Systems, ICMAS95, pp. 283289 San Francisco, CA.
Ogata, K. (1990). Modern Control Engineering. Prentice-Hall, Englewood Cliffs, NJ. Second
Edition, ISBN 0-13-589128-0.
Press, W., Teukolsky, S., Vetterling, W., & Flannery, B. (1994). Numerical Recipies in C.
Cambridge University Press. Second Edition.
Sandholm, T. W., & Ygge, F. (1997). On the gains and losses of speculation in equilibrium
markets. In Proceeding of the Fifteenth International Joint Conference on Artificial
Intelligence, IJCAI 97, pp. 632638.
Schreiber, A. T., Akkermans, J. M., & al. (1999). Knowledge Engineering and Management.
The MIT Press, Cambridge, MA. In Press.
Steiglitz, K., & Honig, M. L. (1992). Chaotic behavior in an auction-based microeconomic
model. Unpublished Manuscript.
Sutherland, I. E. (1968). A futures market in computer time. Communications of the ACM,
11 (6), 449451.
Takayama, A. (1985). Mathematical Economics. Cambridge University Press.
Varian, H. R. (1992). Microeconomic Analysis. New York: W. W. Norton. Third Edition.
Varian, H. R. (1996). Intermediate MicroeconomicsA Modern Approach. W. W. Norton
and Company, New York. Fourth Edition.
Walsh, W. E., Wellman, M. P., Wurman, P. R., & MacKie-Mason, J. K. (1998). Some
economics of market-based distributed scheduling. In Proceedings of the Eighteenth
International Conference on Distributed Computing Systems, pp. 612621.
Wellman, M. P. (1993). A market-oriented programming environment and its application to
distributed multicommodity flow problems. Journal of Artificial Intelligence Research,
pp. 123.
Wellman, M. P. (1996). Market-oriented programming: Some early lessons. In Clearwater,
S. (Ed.), Market-Based Control: A Paradigm for Distributed Resource Allocation,
chap. 4. World Scientific.
Wellman, M. P. (1995). A computational market model for distributed configuration design.
AI EDAM, 9, 125133.
332

fiDecentralized Market Control

Wellman, M. P., & Hu, J. (1998). Conjectural equilibrium in multiagent learning. Machine
Learning, 33, 179200.
Yamaki, H., Wellman, M. P., & Ishida, T. (1996). A market-based approach to allocating
QoS for multimedia applications. In Tokoro, M. (Ed.), Proceedings of the Second
International Conference on Multi-Agent Systems, ICMAS96, pp. 385392. AAAI
Press, Menlo Park, CA.
Ygge, F. (1998). Market-Oriented Programming and its Application to Power Load Management. Ph.D. thesis, Department of Computer Science, Lund University. ISBN
91-628-3055-4, CODEN LUNFD6/(NFCS-1012)/1-224/(1998).
Ygge, F., & Akkermans, J. M. (1996). Power load management as a computational market.
In Tokoro, M. (Ed.), Proceedings of the Second International Conference on MultiAgent Systems, ICMAS96, pp. 393400. AAAI Press, Menlo Park, CA.
Ygge, F., & Akkermans, J. M. (1997). Making a case for multi-agent systems. In Boman,
M., & de Velde, W. V. (Eds.), Proceedings of MAAMAW 97, pp. 156176. Springer
Verlag, Berlin. ISBN-3-540-63077-5.
Ygge, F., & Akkermans, J. M. (1998). On resource-oriented multi-commodity market computations. In Demazeau, Y. (Ed.), Proceedings of the Third International Conference
on Multi-Agent Systems ICMAS98, pp. 365371. IEEE Computer Society.
Ygge, F., Akkermans, J. M., Andersson, A., Krejic, M., & Boertjes, E. (1999). The HomeBots system and field tests: A multi-commodity market for predictive load management. In Proceedings of the Fourth International Conference and Exhibition on The
Practical Application of Intelligent Agents and Multi-Agents (PAAM99), pp. 363382.
Astrom, K.-J., & Wittenmark, B. (1990). Computer-Controlled Systems - Theory and
Design. Prentice Hall, Englewood Cliffs, NJ. ISBN 0-13-168600-3.

333

fi
	fffi	ffffff
!"	#%$$'&$(((*)+$,(-$(.

8:9<;>=>?A@CBEDGF>H+IKJML>?AION

/012'34$5((*671ff"#3!.*5((

IQPSR>9UT>HWVYXZF[DGJ\;>]B^]A_W@C?Y`aPS=>T>b

cedgf^hikjmlnhoqp

r+sutvgwx4y{zfi|~}Qv|uS}

!*<
un%+K{
Cu

g
4*u
~*n*{:g
hdidh

Q{y{tx4'|}n|uS}

+*QgC
ua*<
u
g
4u*
!~*g:{u

fi
%AA<W*^*g!Q u* <
*AaA*0^00u<Au0C*fi
CA
AAn*4AC*%AAmA*0 {A^
*uAA>
*AA*>!U
40a* A%*<AAu0
W***
%u*
0uAU%AA<A'**'0%^***Q'fi*
uAumA
**nu*A**C*>A {A g*u%
uu*0fiuA>AA
AQ<Q*gACC
AC%
uuK*%
4K4gA*SC*fi

uAA0A'*W'A*0A
**** 
%C'00u WAA* u
%figAW
**U
g%'***%*% C%*a*A
K
aA*0
 U!0%
%AuA!A0
'*'u*C%%
uA!*u^fiAA


CAu0A0AK*An
>a*CA*0AA* UuA>u4A*
0fiQu* QAA
Q<gA*%**g'fi*
U%
fi!!A
40
0
A*+
u4>0>*%uAn
uAA 
C
'fi*
U0A%m{A<mu
0
uAA04 
!00%*uAA 
*
 
*'
A*uSC*4*A
+
a
u!**AA<
	
gA*%*QA%A
!CkA*0A*<uAC{AuA {A >**Q**A
gC0A<
4fiff<0uASuA'uauAA0A

fi

fiQ



 "!$#&%$#&!$'
()#&!$%*(+-,+#/.0,+#&%21.0341#&561(+#71#&'
()).08:9)#/;<='&;>?).0).0+3@1()#/A)!$#&5).0'B1.0;+%C;<=>9)D 1. E
A)D0#'&D0%$%$.0F)#&!G%H1;IA)!G;J5)9+'&#7@%$.0)3D0#K'&D0%$%$.0F)#&!*LNMO!$#&.0>*+PRQSST'&UVWD0#&>*#&)PQSXSUZY#&!$!G;)#&PQSS[U
\];D0A^#&!21P_QSS`a-b@cW()#!$#&%$9)D 1.0)3@'&D0%$%$.0F+#&!dLN()#&!$#&<e1#&!!$#&<N#&!$!$#&5d1;I%gf$h)ijf$kml-nofpaq.0%3#&)#&!$D0D 
>*;!$#r'&'&9)!$41#1(+6 g;<s1()#*.0+5). ,^.05+9)D_'&D0%$%$.0F)#&!G%7>*t:.0)3m9)A@1(+#*#&)%$#&>?)D0#&buMO;41(m1()#&;4E
!$#B1.0'&DWLNvw)%$#&xzy)D0>r;)PsQSS{UH|}!$;3(ux~=#&5)#&D0%$?P_QSSaq)5#&>*A).0!$.0'&DLNv%G()#&>*PRQSSU
 A). 1/xy+()-,D0.0tJPwQSSTP=QSST?+ar!$#&%$#&!G'G(g()%r5)#&>*;)%21!G41#&51()413;:;J5g#&+%$#&>?+D0#/.0%*;)#
 ()#&!G#H1(+#.0)5). ,.05)9)D'&D0%$%$.0F)#&!$%=.01()##&)%$#&>?)D0#!$#?^;41(I'&'&9)!$41#}+5/>*t#O1()#&.0!#&!$!G;!$%;
5).0#&!$#&1dA)!21%;<1(+#*.0)A)91r%GA)'&#&buc  ;mA^;A)9)D0!>r#B1();J5+%7<N;!}'&!$#&41.0)3'&'&9)!$41#@#&)%$#&>?)D0#&%
!$#/MO33.0)3LNM!G#&.0>*)P=QSST'&a@)5MO;:;%21.0)3]LN)!$#&9+)5gxy)'
()A).0!$#&PQSSTU}y)'G(+A).0!$#&PwQSS{a-b
cW()#&%G#>*#B1(+;J5)%O!$#&D ;4!$#&%$>*A+D0.0)31#&'G()).08:9)#&%R1;C;?1.0/5+.0#&!$#&1O1!$.0).0)3K%$#B1%<N;!O#&'G(m;<
1()#*'&D0%G%$.0F)#&!$%$bu@1().0%7A)A^#&!  #CA+!$#&%$#&1dm'&;>*A)!G#&()#&)%$. ,+#C#B,D09)41.0;;<?^;41(MO33.0)3)5
MO;J;%o1.0)3I;`[@5)41I%$#B1%q9)%$.0)3}1  ;d?+%$.0''&D0%G%$.0F)'&41.0;6>*#B1(+;J5)%Gw5+#&'&.0%$.0;d1!$#&#&%q)5+#&9)!$D
)#B1  ;!$t:%$b


$((('g"" 
3	C	3	^	 2'	q*1
ff"#"pfiffff*#A"S"3

fi

stv{wz{y{t

Y!$#B,^.0;9+%  ;!Gtd()%=5)#&>*;)%21!G41#&5K1()417MO33.0)3r+5/M;:;%21.0)3r!$#O,+#&!2#&#&'B1. ,+#<;!=5)#&'&. E
%$.0;1!$#&#&%7LNMO9)#&!x|7;()-,.0PQSSSUq!$9)'
t#&!xVW;!21#&%$P^QSSTUMO!$#&.0>*+PQSST'&PQSST?)U)!G#&9))5
xy+'G()A).0!G#&PQSSTU79).0)D0)PqQSSTa-UK();  #B,+#&!$PO1(+#&!$#u()%d?^#&#&]D0. 1-1D0##&>*A).0!$.0'&DH1#&%21.0)3  . 1(
)#&9)!GD_)#B1  ;!$t:%@LN#&%$A^#&'&.0D0D   . 1(I1(+#*)#  MO;J;%o1.0)3uD03;!$. 1(+>*a-buq.0%$'&9)%G%$.0;)%  . 1(A)!$#B,.0;9)%
!$#&%$#&!G'G()#&!$%R!$#B,+#&D1()41>r r91(+;!$%'&;)'&#&1!$41#&5K;5)#&'&.0%$.0;=1!$#&#&%s5+9)#^1;_1(+#&.0!s<N%211!G.0).0)3
%$A^#&#&5u+5  #&D0D EG#&%21?)D0.0%$()#&55)#&<N9)D 1KA)!$>*#B1#&!w%$#B1-1.0)3%$bw#&9)!$Ds)#B1  ;!$t:%qA)!$#&%$#&15).0*'&9+D 1.0#&%
<N;!Z1#&%21.0+3r?^;41(u.0r1#&!$>*%w;<1()#7%G.03).0F)'&1A)!$;:'&#&%$%$.0+31.0>*#!$#&8J9+.0!$#&5u)5.0%G#&D0#&'B1.0)31!$.0E
.0)3A)!$>*#B1#&!G%$U^();  #B,+#&!$P  #w<N#&#&D1(+#&!$#=!$#w5).0%21.0+'B1q5,13#&%1;.0+'&D09)5).0)3)#&9)!$D)#B1  ;!$t:%
.0;9)!%o19)5b.0!$%o1PA)!$#B,.0;9)%#&>rA).0!$.0'&D%219)5).0#&%()-,+#I5)#&>r;)%21!$41#&5u1()41/.0)5). ,.05)9)DH+#&9)!$D
)#B1  ;!$t:%wA)!$;:5)9)'&#().03()D '&'&9+!$41#'&D0%G%$.0F)#&!$%Z1()41!$#%$;>*#B1.0>*#&%q>*;!G#}'&'&9)!$41#=1()u'&;!G!$#BE
%$A^;)5).0+35)#&'&.0%G.0;w1!$#&#&%LN.0%$()#&!_x"'&|}9)%$.0'
tJPQSXSU;:;)#BP+y)(+-,D0.0tJPcR;  #&D0D0Px"7;-,+#&PQSXSa-b
y)#&'&;)5+P+#&9)!$DZ)#B1  ;!GtJ%K()$,+#d?^#&#&#B:1#&)%$. ,+#&D A)A)D0.0#&5'&!$;%$%K:9)>*#&!$;9)%5);>*.0)%ILN!G?).0?)P
QSSa-bm.0)D0D P? %219)5^.0)3I)#&9+!$D)#B1  ;!$t:%7.05)5). 1.0;/1;m5)#&'&.0%$.0;I1!G#&#&%  #C'&6#B^>r.0)#
();  MO33.0)3@)5mMO;J;%o1.0)3r!$#.0))9)#&+'&#&5/? I1()#}D0#&!G).0)3rD03;!G. 1()>*P3. ,.0)3d<9)!o1()#&!.0)%$.03(1
.01;K1()#*3#&)#&!GDZ'G()!G'B1#&!$.0%21.0'&%K;<1(+#&%$#*A)A)!G;'G()#&%$buM9+#&!7)56|7;()-,.LNQSSSaD0%$;m%219)5
MO33.0)3u+56MO;J;%21.0+3mA)A)D0.0#&5I1;C1  ;mD0#&!$).0)3u>*#B1();:5)%$PZ.0I1(+#&.0!}'&%$#*5)#&'&.0%G.0;/1!$#&#&%}9+%$.0)3
K,!$.01@;<VWb0m+5). ,+#BEGMO-+#&%C'&D0%G%$.0F)#&!$%$P?)9171()#&.0!7%219+5>r.0)D g'&;)'&#& 1!G41#&5;m1()#
5)#&'&.0%$.0;*1!$#&#}!$#&%G9)D 1%$b
 9+!)#&9+!$Ds+#B1  ;!$tu)55)#&'&.0%G.0;@1!$#&#K!$#&%$9)D 1%qD0#&59)%1;Id:9)>?#&!w;<H.01#&!$#&%21.0+3@'&;+'&D09E
%$.0;)%GbcW()#uF)!$%21u.0%}1()41MO33.0)3#&)%$#&>?)D0#u3#&+#&!$D0D A)!$;:5)9)'&#&%*'&D0%$%$.0F)#&!}1()41.0%d>*;!$#
'&'&9)!$41#1()/C%21)5)!G5/'&D0%$%$.0F)#&!$b=cW(J9+%;)#q%$();9)D05/<N#&#&D'&;>*<N;!21?)D0#}D  -%=MO33.0)3}1()#&.0!
5)#&'&.0%$.0;r1!$#&#&%;!=)#&9)!GD)#B1  ;!$t:%$bw);!MO;:;%21.0)3PR();  #B,+#&!$P  #);41#}>*;!$#  .05)#&D I,!2^.0+3*!$#BE
%$9)D 1%Gb=);!r<N#  5)41r%G#B1%=MO;J;%o1.0)3rA)!G;J5)9+'&#&5/5)!$>*41.0'!$#&5)9)'B1.0;)%=.0u#&!$!$;!LN#B,+#&'&;>*A+!$#&5
1;6MO33.0)3a-Pq?)91m<N;!*;41()#&!*5+41%$#B1%r. 1u'B19)D0D .0)'&!G#&%$#&%r.0#&!$!G;!*;-,+#&!d%$.0)3D0#/'&D0%$%$.0F+#&!
LNA)!21.0'&9+D0!$D   . 1(+#&9)!$DR)#B1  ;!$t:%$a-brB<9)!o1()#&!1#&%21%  #K#B>*.0)#&5@1()#K#&#&'B1%};<H+;.0%$#C)5
%$9)A+A;!o1K)!G#&9))56)5y+'G()A).0!G#&0%@LNQSSTa7'&;p#&'B19)!G#q1()41dMO;:;%21.0)30%7%G#&)%$. 1. ,. 11;m);.0%G#C>*-
?^#A)!21D !$#&%$A^;)%$.0?+D0#<;!W. 1%;J'&'&%G.0;)D.0)'&!G#&%$#}.0m#&!$!G;!$b
wID 1#&!$)41#}?)%G#&D0.0)#qA)A)!$;'
(  #q.0 ,+#&%o1.0341#&5  %R1()#w'&!$#&41.0;m;<K%$.0>rA)D0#+#&9)!$D)#B1-E
;

$
!
tK#&)%$#&>?)D0#  ()#&!G##&'G(@)#B1  ;!$tC9)%$#&5w1(+#W<9+D0D1!$.0).0)37%$#B1)5*5+.0#&!$#&5r;)D m.0r. 1%H!G)5);>

.0). 1.0D  #&.03(1%$#B1-1.0+3%$b  9)!*!$#&%$9+D 1%*.0)5).0'&41#*1()41*1().0%d#&)%$#&>?)D0#K1#&'G(+).08J9+#u.0%*%$9+!$A)!$.0%$.0+3D 
#&#&'B1. ,+#&P_;<1#&A)!$;:5)9)'&.0)3C!G#&%$9)D 1%w%3;:;J5%MO33.0)3b=#&%$#&!G'G(? wD0.)5Y&&).OLNQSSTa
5)#&>*;+%21!$41#&5m%$.0>r.0D0!=!$#&%$9+D 1%W9)%$.0)3C!G)5);>*.0&#&5m5+#&'&.0%$.0;C1!$#&#7D03;!$. 1()>*%$b
 9+!!$#&%G9)D 1%qD0%$;/%$();  1(+41w1(+##&)%G#&>?)D0#K>*#B1(+;J5)%q!$#K3#&)#&!GD0D '&;)%$.0%o1#& 1LN.0d1#&!$>*%;<
1()#&.0!s#&#&'B1O;}'&'&9)!$'Ba  (+#&}A)A)D0.0#&5}#&. 1(+#&!)1;W+#&9)!$D)#B1  ;!$t:%;!)1;W5)#&'&.0%$.0;=1!$#&#&%$U();  #B,+#&!GP
1()#&!$#@.0%KD0. 1-1D0#I.0 1#&!2EG'&;!G!$#&D041.0;]?^#B1  #&#&)#&9)!$D)#B1  ;!$t:%C)55)#&'&.0%$.0;1!$#&#&%C#B'&#&A1I<N;!1()#
MO;J;%o1.0)3>r#B1();J5+%$bcW().0%d%$9)33#&%21%1()41%$;>*#;<1()#.0)'&!$#&%$#&%IA)!$;:5)9)'&#&5]? MO;:;%21.0)3!$#
5)#&A^#&)5)#&1C;I1()#KA)!21.0'&9)D0!7'G()!$'B1#&!G.0%21.0'&%;<1()#C5)41/%$#B1r!G41()#&!1();@1()#C'&;>*A^;)#&1
'&D0%$%$.0F+#&!$bzB]<N9)!21(+#&!1#&%21%  #5+#&>*;)%21!$41#r1()41MO33.0)3.0%@>r;!$#u!$#&%G.0D0.0#& 1I1;);.0%$#r1()
MO;J;%o1.0)3b
.0)D0D P  #q.0 ,+#&%o1.0341#&5*1()#q8:9)#&%21.0;/;<s();  >*'&;>*A^;)#&17'&D0%$%$.0F+#&!$%%$();9)D05I?^#w9)%$#&5
.0]#&)%$#&>?)D0#&bV=;)%$.0%21#&1  . 1(]A)!$#B,.0;9)%d!$#&%$#&!$'
(LN+!$#&9))5xy)'
()A).0!$#&PQSSTUC79).0)D0)P
QSSTa-PW;9)!!$#&%$9+D 1%7%$();  1(+41I>*;%o1@;<R1()#d!$#&5)9)'B1.0;.0#&!G!$;!<N;!7#&+%$#&>?+D0#d>*#B1(+;J5)%;J'&'&9+!$%
 . 1(m1()#*F)!$%o1r<#  5)5). 1.0;)D'&D0%$%$.0F)#&!$%Gb*\. 1(MO;:;%21.0)35)#&'&.0%$.0;/1!G#&#&%$PO();  #B,+#&!$PO!$#&D041. ,+#&D 
D0!$3#}3.0+%=>*$?^#%$#&#&u9)Am9)1.0D?^;91`*'&D0%$%G.0F)#&!$%$b
*

fi

r+su}SmQz
r

vr++z

cW(+.0%A)A^#&!w.0%q;!$3).0&#&5%q<;D0D0;  %$bKr1()#)#B:1K%$#&'B1.0;  #A)!$#&%$#&1K;-,+#&!o,^.0#  ;<H'&D0%2E
%$.0F)#&!#&)%$#&>?)D0#&%w)5m5).0%$'&9+%$%=MO33.0)3@)5uMO;:;%21.0)3d.0u5)#B1.0D0bw#BJ1  #A)!$#&%$#&1#BJ1#&)%G. ,+#
#&>*A).0!G.0'&D)D ^%G.0%=;<MO33.0)3d)5/MO;J;%o1.0)3b=);D0D0;  .0)3}1()41  #A+!$#&%$#&17<919+!$#!G#&%$#&!$'G()5
5)5). 1.0;+D!$#&D041#&5  ;!$t@?^#&<;!G#'&;)'&D09)5).0)3b
m

fi

 )Y  S
 

.039)!$#CQ/.0D0D09)%o1!$41#&%1()#K?)%G.0'C<!$>r#  ;!$t<N;!/'&D0%$%$.0F)#&!}#&+%$#&>?+D0#&b@Bd1().0%}#B>*A)D0#&PZ+#&9)!$D
)#B1  ;!$t:%H!G#_1()#W?+%$.0'W'&D0%$%$.0F+'&41.0;@>r#B1();J5+P1();9)3(*'&;+'&#&A19)D0D m'&D0%$%$.0F)'&41.0;@>*#B1();:5
LN#&b03b0P5)#&'&.0%$.0;q1!$#&#&%GaZ'&*?^#O%$9)?)%21. 191#&5*.0CA)D0'&#W;<1()#)#B1  ;!$t:%$bOH'
(r)#B1  ;!Gt.0C.039)!G#Q0%
#&)%$#&>?)D0#KLN)#B1  ;!$tCQO1()!G;9)3(*)#B1  ;!$t*.0}1().0%H'&%$#&a.0%1!$.0+#&5r9)%$.0+3O1()#1!$.0+.0)3}.0)%21+'&#&%
<N;!H1(+41C+#B1  ;!$tJbKcW(+#&)Ps<N;!q#&'G(#B>*A)D0#&P1()#A)!$#&5+.0'B1#&5;91A)91C;<Z#&'G(;<1()#&%$#)#B1  ;!$t:%
L24.0.039)!G#7Qaw.0%'&;>?).0)#&5r1;rA)!G;J5)9+'&#O1()#7;91A)91;<1()##&)%$#&>?)D0#@Lr
 .0u.039)!$#Qa-b} 
!$#&%$#&!G'G()#&!$%wLND0A)$^5+.0)P+QSS[U^MO!$#&.0>*)P+QSST'&U|7!$;3(*x~=#&5)#&D0%$?PQSSUs.0)'&;D0*xy)t:!$BA^#&tJP
QSXSaq()-,+#5)#&>*;+%21!$41#&5C1(+41K#&#&'B1. ,+#K'&;>?).0).0)3d%$'G()#&>r#.0%H1;r%$.0>rA)D 6-,+#&!$3#1()#A)!$#BE
5).0'B1.0;)%;<)1(+#})#B1  ;!$t:b

V ;>?).0).0+3W1()#w;91A)91q;<%G#B,+#&!$D^'&D0%$%$.0F)#&!$%.0%O9)%G#&<9)D;)D .0<1(+#&!$#.0%O5).0%G3!$#&#&>*#&1>r;)3
W
1()#&>*b  ?,^.0;9+%$D P}'&;>?).0).0)3%$#B,+#&!$Dq.05)#&1.0'&Dq'&D0%$%$.0F)#&!G%@A+!$;J5+9)'&#&%d+;3.0)bvw)%$#&)5
y)D0>*;gLNQSS{a}A+!$;-,+#&5d1()41*.0<^1()#-,+#&!G3#*#&!$!$;!q!$41#K<N;!w#B>*A)D0#.0%D0#&%$%1(){)5
1()#w'&;>*A^;)#&1w'&D0%$%$.0F+#&!$%O.0}1()#w#&)%$#&>?)D0#w!$#w.0)5)#&A^#&)5)#&1.071()#A+!$;J5+9)'B1.0;r;<1(+#&.0!#&!$!G;!$%$P
1()#/#BA^#&'B1#&5]#&!$!$;!C<N;!q1()41u#B>*A)D0#u'&g?^#/!$#&5)9)'&#&51;6&#&!G;%1()#/:9)>?#&!K;<q'&D0%$%$.0F+#&!$%
'&;>?).0)#&53;:#&%1;u.0)F)). 1^U();  #B,+#&!$PO%$9)'
(6%$%$9+>*A1.0;)%}!G!$#&D (+;D05.06A)!G'B1.0'&#&bu|}!$;3()5
~=#&5)#&D0%$?LNQSSa=D041#&!WA)!$;$,+#&571(+411()##&+%$#&>?+D0#w#&!G!$;!O'&/?#w5). ,.05)#&5@.0 1;C1#&!$>>*#&%$9+!$.0)3
1()#6$,+#&!$3#3#&)#&!$D0.0&41.0;#&!$!$;!m;<K#&'
(.0)5+. ,^.05)9+D'&D0%$%$.0F)#&!)51#&!$>>*#&%$9)!$.0+31()#
5).0%$3!G#&#&>*#& 1>*;)3=1()#'&D0%$%$.0F)#&!$%GbR\()411()#Bu<N;!$>*D0D u%$(+;  #&5  %1()41q@.05)#&D#&)%$#&>?)D0#
'&;)%$.0%o1%7;<().03()D '&;!$!$#&'B1@'&D0%G%$.0F)#&!$%O1(+41d5).0%$3!$#&#r%7>9)'G(6%A^;%$%$.0?)D0#&b  A+. 1C)56y)()$,^D0.0t
LNQSSTPRQSST?)a=#&>*A+.0!$.0'&D0D /,+#&!$.0F)#&5K1(+41%$9)'
(m#&)%$#&>?)D0#&%=3#&+#&!$D0.0&#  #&D0D0b
 %H}!G#&%$9)D 1P+>r#B1();J5+%H<N;!Z'&!$#&41.0)3#&)%$#&>?)D0#&%H'&#&1#&!!$;9))5*A+!$;J5+9)'&.0)3q'&D0%$%$.0F)#&!G%1()41w5).0%2E
w
3!$#&#O;=1()#&.0!sA+!$#&5).0'B1.0;)%$b}#&)#&!GD0D P1()#&%$#>*#B1();:5)%R<;:'&9)%;KD 1#&!$.0+3Z1()#1!$.0).0+3=A)!$;:'&#&%$%R.0

o
ensemble output
combine network outputs
o
o
1

2

network 1 network 2

o

N

network N

input
.039)!G#Q='&D0%$%$.0F)#&!#&)%$#&>?)D0#};<R)#&9)!$D)#B1  ;!GtJ%$b
*

fi

stv{wz{y{t

1()#}(+;A#1()41=1()#}!$#&%$9+D 1.0)3*'&D0%$%$.0F+#&!$%  .0D0DA+!$;J5+9)'&#5).0#&!$#& 1A)!$#&5).0'B1.0;)%GbW);!=#B>*A)D0#&P)#&9E
!$D)#B1  ;!$tw1#&'G(+).08J9+#&%1()41q()-,+#?^#&#&r#&>*A)D0;$+#&5d.0+'&D09)5)#W>*#B1(+;J5)%<;!1!$.0).0)3  . 1(r5).0#&!$#&1
1;A^;D0;3.0#&%$PR5).0#&!G#& 1.0+. 1.0D  #&.03( 1%GP5).0#&!$#&1A)!$>*#B1#&!G%$P)5C1!$.0).0)3*;+D ;*A^;!21.0;u;<
1()#O1!G.0).0)3*%$#B1/LND0A)$^5+.0)PQSS[URw!G9)'Gt#&!$PVW;!21#&%$PJ'Gt#&D0PR#&VW9+)Px~=A)).0t:PQSSURv+%$#&
xy)D0>*;+PZQSS{U'&D0.06xy)(+-,D0.0tJPZQSSa-bIB@1().0%A)A^#&!  #C'&;+'&#& 1!$41#r;I1  ;/A^;A)9)D0!
>*#B1();:5)%dLNMO33.0)3)5M;:;%21.0)3a=1()411!21;m3#&)#&!$41#@5).0%$3!$#&#&>r#& 1/>*;)3K1(+#r'&D0%$%$.0F+#&!$%
?D 1#&!$.0+31()#1!$.0+.0)3*%$#B1#&'
('&D0%G%$.0F)#&!W%$#&#&%Gb
d Shs
+$ )

d) *
 hNRgfi

MO33.0)3LNM!G#&.0>*)POQSST'&aK.0%4?;:;41%21!$A+LNH<N!$;xcW.0?)%G().0!$).0PQSS[a#&)%G#&>?)D0#d>*#B1();:5
1()41/'&!$#&41#&%C.0)5). ,.05)9)D0%<N;!. 1%K#&)%G#&>?)D0#@?1!$.0).0)3#&'G('&D0%$%G.0F)#&!K;!$)5+;>!$#&5+.0%21!$. E
?)91.0;;<1()#1!$.0+.0)3*%$#B1bqH'
('&D0%G%$.0F)#&!$0%H1!$.0).0)3d%$#B1.0%w3#&)#&!$41#&5? 6!$+5);>*D 65)!$  .0)3P
 . 1(!$#&A+D0'&#&>*#& 1P#B>*A)D0#&%  (+#&!$#.0%1()#C%$.0&#C;<1()#K;!$.03.0)D1!$.0).0+3/%$#B1UH>*;<
1()#@;!$.03.0)DO#B>*A)D0#&%C>*$?^#r!$#&A^#&41#&5.01()#@!$#&%$9)D 1.0)3r1!$.0).0+3%$#B1  (+.0D0#d;41()#&!G%>r-?^#
D0#&<e1/;91b]H'
(.0)5). ,.05)9)D'&D0%$%$.0F)#&!r.0u1(+#@#&+%$#&>?+D0#@.0%C3#&+#&!$41#&5  . 1(5).0#&!$#& 1u!G)5);>
%$>*A+D0.0)3C;<1()#1!$.0).0)3r%$#B1b
.039)!$#`@3. ,+#&%}d%G>*A)D0#;<_();  MO33.0)3/>*.03(1  ;!$tm;@.0>*3.0)!2%$#B1*;<_5)41bKy).0)'&#
MO33.0)3q!$#&%$>*A+D0#&%)1()#1!$.0).0)3w%$#B1  . 1(7!$#&A)D0'&#&>r#& 1P)%G;>*#H.0)%o1)'&#H!$#!$#&A)!$#&%G#& 1#&5>9+D 1.0A)D0#
1.0>*#&%  (+.0D0#;41()#&!G%w!G#D0#&<e1*;91bKy);@M33.0+30%1!$.0).0+34EG%$#B1-EGQ/>*.03(1C'&;1.0#B>*A)D0#&%[I)5
1  .0'&#&PH?)91*5);:#&%);41*'&;1.0#&. 1()#&!}#B>*A)D0#C/;!b@w%}/!$#&%$9)D 1P1(+#C'&D0%$%$.0F)#&!O1!G.0)#&5;
1!$.0).0+34EG%$#B1-EGQ7>*.03(1=;?1.0*(+.03()#&!1#&%21-EG%$#B1#&!G!$;!1()q1()#O'&D0%$%G.0F)#&!Z9)%$.0+3wD0D+;<1()#5)41bB
<N'B1P)D0DJ<N;9)!R;<)MO33.0)30%Z'&;>*A^;)#&1'&D0%$%$.0F)#&!$%_'&;9+D05!G#&%$9)D 1.0K().03(+#&!)1#&%21-EG%G#B1=#&!$!$;!$U();  #B,+#&!GP
 ()#&'&;>?).0)#&5)P+1()#&%$#<;9+!'&D0%$%G.0F)#&!$%q'&gLN)5;<e1#&5);awA)!$;:5)9)'&#1#&%o1-EG%$#B1*#&!$!$;!wD0;  #&!1()
1()41K;<1()#}%$.0)3D0#'&D0%$%$.0F)#&!CLe1()#5). ,+#&!$%$. 1>*;)3}1()#&%$#7'&D0%G%$.0F)#&!$%w3#&)#&!$D0D '&;>*A^#&)%G41#&%<N;!
1()#}.0+'&!$#&%$#}.0m#&!G!$;!W!$41#};<R.0+5). ,^.05+9)D'&D0%$%$.0F)#&!Ga-b
MO!$#&.0>*LNQSST'&a/%$();  #&561()41M33.0+3.0%@#&#&'B1. ,+#;49))%21?+D0#&D0#&!$).0+3D03;!$. 1()>r%
 ()#&!G#/%$>*D0DW'
())3#&%@.01()#K1!$.0+.0)3%$#B1!$#&%G9)D 1.0gD0!$3#'G()+3#&%r.0gA)!G#&5).0'B1.0;)%$bM!G#&.0>*
LNQSST'&a'&D0.0>*#&5d1()41K)#&9)!$Ds)#B1  ;!$t:%)55)#&'&.0%$.0;d1!$#&#&%w!$#7#B>*A)D0#&%q;<_9)+%21?)D0#7D0#&!G).0)3
D03;!$. 1()>r%$b\]#C%219)5/1()##&#&'B1. ,+#&)#&%$%};<ZMO33.0)3/;?^;41(r1()#&%$#KD0#&!$).0)3@>*#B1();:5)%w.0d1().0%
!21.0'&D0#&b
+2K *oqhs

d) q
 h2Rj

MO;J;%o1.0)3ILN+!$#&9))5@xy)'
()A).0!$#&PQSSTUy)'
()A).0!$#&PQSS{aO#&)'&;>*A)%$%G#&%<>*.0D ;<>*#B1();:5)%$bcW()#
<N;J'&9)%q;<^1(+#&%$#K>*#B1();:5)%.0%1;IA)!G;J5)9+'&#7Iijf-4-f-i;<Z'&D0%G%$.0F)#&!$%$bCc=()#W1!G.0).0)3@%$#B1*9)%$#&5<;!q#&'
(
>*#&>?^#&!;<1()#C%G#&!$.0#&%}.0%}'
();%$#&6?)%G#&56;I1()#CA^#&!$<N;!$>*)'&#K;<1()#C#&!$D0.0#&!}'&D0%G%$.0F)#&!fiLN%$a7.0I1()#
%$#&!$.0#&%Gb6MO;:;%21.0)3P#B>*A)D0#&%=1()41@!G#r.0)'&;!$!G#&'B1D A)!$#&5).0'B1#&5? ]A+!$#B,^.0;9+%7'&D0%$%$.0F+#&!$%.0m1()#
%$#&!$.0#&%7!$#K'G();%$#&>*;!$#K;<1#&/1()#B^>rA)D0#&%1()41  #&!$#C'&;!$!$#&'B1D gA)!$#&5+.0'B1#&5)brcW(:9)%qMO;J;%o1.0)3
41-1#&>*A1%1;@A)!$;:5)9)'&#}+#  '&D0%$%$.0F+#&!$%Z1(+41K!$#?^#B1-1#&!?)D0#=1;@A)!$#&5).0'B1K#B^>*A+D0#&%<N;!  ().0'G(r1()#
'&9)!$!G#& 16#&)%$#&>?)D0#&0%/A^#&!$<N;!$>*)'&#.0%/A^;:;!$bLN;41#/1()416.0"M33.0+3Pw1()#!$#&%$>*A)D0.0+3;<W1()#
1!$.0).0+3*%$#B1.0%W+;415)#&A^#&)5)#&1};K1()#}A^#&!$<N;!$>*)'&#;<+1()#}#&!GD0.0#&!='&D0%$%$.0F+#&!$%$b0a
B/1().0%  ;!$t  #d#B^>*.0+#}1  ;)#  +5A^;  #&!$<N9)DZ<N;!$>*%7;<MO;:;%21.0)3Iw!$'&.0)3LNMO!$#&.0>r)P
QSST?)am+5"5)4EGMO;:;%21.0)3LN+!$#&9))5"xy)'
()A).0!$#&PQSSTa-b.0t#6MO33.0)3PCw!$'&.0)3'
();J;%G#&%u
1!$.0).0+3d%$#B1*;<Z%G.0&#K<;!q'&D0%$%$.0F+#&!q]QI? A)!$;?)?).0D0.0%o1.0'&D0D %$#&D0#&'B1.0)3L  . 1(!G#&A)D0'&#&>*#&1a
#B>*A)D0#&%W<N!$;>1()#;!G.03.0)D1!$.0).0)3C#B>*A)D0#&%$b=)D0.0t#7M33.0+3Ps();  #B,+#&!GP1()#A)!$;?+?).0D0. 1
*

fi

r+su}SmQz
r

vr++z

%$>*A+D0#};<sr%$.0)3D0#}'&D0%$%G.0F)#&!=;m.0>*3.0)!26%$#B1;<s5)41b
L  !G.03.0)D0aWcR!$.0).0)3*y+#B1
cR!$.0).0+34EG%$#B1-EGQ
QP`P[PPPTPPX
%$>*A)D0#};<RMO33.0)3@;K1()#%$>*#75)41b
LN#&%$>*A)D0#&5+aWcs!G.0).0)3Cy)#B1
cs!$.0+.0)34EG%$#B1-EGQ
`PPXP[PPTP[PQ
cs!$.0+.0)34EG%$#B1-EG`
PXPPTPP`PPQ
cs!$.0+.0)34EG%$#B1-EG[
[PTP`PPPTP`P`
cs!$.0+.0)34EG%$#B1-EG
PPQPPTPP[PX
%$>*A)D0#7;<sMO;:;%21.0)3r;C1()#}%$>*#5+41b
LN#&%$>*A)D0#&5+aWcs!G.0).0)3Cy)#B1
cs!$.0+.0)34EG%$#B1-EGQ
`PPXP[PPTP[PQ
cs!$.0+.0)34EG%$#B1-EG`
QPPPPQPPTP
cs!$.0+.0)34EG%$#B1-EG[
PQPPXPQPXPQP
cs!$.0+.0)34EG%$#B1-EG
QPQPTPQPQP[PQP
.039)!$#}`7vZA^;41()#B1.0'&Dq!$9))%d;<M33.0+3+5MO;:;%21.0)3b%$%G9)>*#C1(+#&!$#u!$##&.03(1@1!G.0).0)3
#B^>*A+D0#&%$bw%$%$9+>*##B^>*A+D0#Q.0%/4;91D0.0#&!$g)5".0%/()!G5<N;!1()#'&;>*A^;)#&1
D0#&!$).0)3D03;!$. 1()>1;'&D0%$%$.0<e'&;!$!G#&'B1D b6\. 1(gMO33.0)3Pw#&'G(1!$.0+.0)3%$#B1u.0%C
.0)5)#&A^#&)5)#&1%$>rA)D0#u;<O1(+#5)41Uw1(:9)%$P7%$;>*##B^>*A+D0#&%@!G#>*.0%G%$.0)3)5;41(+#&!$%
;J'&'&9)!=>9)D 1.0A+D0#O1.0>*#&%$bWc=()#qM;:;%21.0)3}1!$.0).0)3C%G#B1%=!$#D0%$;r%$>*A)D0#&%W;<+1()#};!G.03.0)D
5)41d%G#B1Ps?)91W1()#@4(+!$5)*#B>*A)D0#/LN#B^>rA)D0#Qa;J'&'&9+!$%>r;!$#7.0D041#&!Z1!G.0).0)3@%$#B1%
%$.0)'&#}MO;:;%21.0)3*'&;)'&#&1!$41#&%q;m'&;!$!G#&'B1D A)!$#&5).0'B1.0)3C. 1b

;<)%$#&D0#&'B1.0+3#B>*A)D0#O.0%);41W#&8:9)DJ'&!G;%$%1()#1!$.0).0)3q%$#B1bOcW().0%RA)!$;?+?).0D0. 1I5)#&A^#&)5)%;K(); 
;<e1#&I1()41d#B>*A)D0#  %}>*.0%$'&D0%G%$.0F)#&5?u1(+#CA)!$#B,.0;9)%7'&D0%$%$.0F)#&!$%Gb/5)4EGMO;:;%21.0)3m'&9)%$#
1()#A)A+!$;'G(C;<_LNa_%$#&D0#&'B1.0)3}%G#B1=;<+#B>*A)D0#&%Z?)%G#&5C;w1()#A)!$;?)?+.0D0. 1.0#&%Z;<1()##B>*A)D0#&%GP);!
LN?)a_%$.0>rA)D I9+%$.0)3qD0D);<J1()##B>*A)D0#&%H+5  #&.03(1H1()##&!G!$;!Z;<#&'G(r#B>*A)D0#=? 1()#A)!$;?+?).0D0. 1
<N;!W1()41/#B>*A)D0#LN.0b0#&b0P=#B^>*A+D0#&%  . 1(().03(+#&!7A)!$;?+?).0D0. 1.0#&%K()-,+#@>*;!$#@#&#&'B1/;u1()#d#&!G!$;!$a-b
cW().0%D041-1#&!WA)A+!$;'G(/()%s1()#w'&D0#&!5, 13#1()41q#&'
(m#B>*A)D0#q.0%O.0)'&;!$A^;!$41#&56LN417D0#&%21}.0
A)!21a.01()#1!$.0).0)3%$#B1bW+9)!21()#&!$>r;!$#&P^)!$.0#&5)>*I#B1}D0bLNQSSXa()$,+#5)#&>*;)%o1!$41#&51()41O1().0%
<N;!$>;<^5+4EGM;:;%21.0)3'&r?^#R,.0#  #&5@%H}<N;!$>;<^5)5+. 1. ,+#=>*;:5)#&D0.0)3<N;!H;A1.0>r.0&.0)37}D0;3.0%o1.0'
D0;%$%Z<N9))'B1.0;+bBw1().0%  ;!$t:P)();  #B,+#&!$P  #(+-,+#W'
();%$#&q1;}9+%$#s1()#A)A)!$;'
(*;<%$9)?)%$>rA)D0.0)31()#
5)411;I#&)%$9)!G#@<.0!#&>rA).0!$.0'&D'&;>*A)!$.0%G;LN.0A+!21K5+9)#W1;1()#!$#&%21!o1.0)3I!$#&%$;5).0%$'&9+%$%$#&5
?^#&D0;  a-b
MO;41(!G'&.0)3I)5w5)4EGMO;:;%21.0)3I.0). 1.0D0D %G#B11()#A)!$;?+?).0D0. 1;<_A).0'Gt:.0)3I#&'G(#B>*A)D0#=1;
?^#7Q&bCcW()#&%$#>*#B1();:5)%H1()#&!$#&'&D0'&9)D041#w1()#&%G#A+!$;?)?).0D0. 1.0#&%q<e1#&!#&'G(@1!$.0)#&5'&D0%$%G.0F)#&!.0%
5)5)#&51;@1(+#@#&+%$#&>?+D0#&b);!r5)4EGMO;:;%21.0)3PqD0#B1?^#}1()#/%G9)>;<Z1()#IA)!G;?)?).0D0. 1.0#&%*;<H1()#


fi

stv{wz{y{t

>*.0%$'&D0%G%$.0F)#&5/.0)%21+'&#&%<;!s1()#q'&9)!$!G#& 1D *1!G.0)#&5/'&D0%$%$.0F)#&!W:bcW()#wA)!$;?+?).0D0. 1.0#&%<;!s1()#q)#B:1
1!$.0DZ!G#*3#&)#&!$41#&5? >9+D 1.0A)D ^.0+3K1()#CA)!$;?+?).0D0. 1.0#&%7;<OJ0%7.0)'&;!$!$#&'B1D ]'&D0%$%G.0F)#&56.0)%21+'&#&%
?1(+#*<'B1;!=+uLNQ}Ja$+5I1()#&6!$#&+;!$>*D0.0&.0)3D0DHA)!$;?+?).0D0. 1.0#&%7%$;C1()41}1()#&.0!%$9)>
#&8:9)D0%rQb"w5)4EGMO;J;%o1.0)3'&;>?).0)#&%}1()#/'&D0%$%$.0F)#&!G%r $&&&&  9)%$.0)3  #&.03( 1#&5,+;41.0)3  ()#&!$#
()%  #&.03(1uD0;3+L +a-bcW()#&%$#  #&.03(1%CD0D0;  w5)4EGMO;J;%o1.0)3d1;5+.0%$'&;9)171()#@A)!$#&5).0'B1.0;+%;<
'&D0%$%$.0F+#&!$%1()41I!$#*);417,+#&!2g'&'&9)!$41#@;I1()#r;$,+#&!$D0DA)!$;?)D0#&>*bu)!$.0#&5)>*#B1@DLNQSSXaK()$,+#
D0%$;*%G9)33#&%21#&5uuD 1#&!$)41. ,+#>*#&'G()+.0%$>1()417F1%1;3#B1()#&!H1()#A)!$#&5).0'B1.0;+%=;<+1()#}'&D0%$%$.0F+#&!$%
%=u5)5). 1. ,+#}>*;:5)#&D9+%$.0)3K*>*4.0>9+>D0.0t#&D0.0();:;J5'&!$. 1#&!$.0;)b
B1().0%  ;!$tJP  #9+%$#d1()#!$#B,^.0%G.0;5)#&%$'&!$.0?^#&5?MO!$#&.0>*LNQSST?)a  ()#&!$#  #!G#&%$#B1D0D
1()#  #&.03( 1%C1;?##&8:9)D)5!$#&%21!o1.0<#&. 1()#&!m].0%m+;41D0#&%G%K1()"{b0;!m?^#&'&;>*#&%m{b $
Msg!$#&%$#B1-1.0)3K1(+#  #&.03(1%  #C5);m+;41r5).0%$5, 13#}1()#Cw5)4EGMO;J;%21.0+3uD0#&!$)#&!7.0I1();%$#*'&%G#&%
 ()#&!G#w. 17!G#&'G()#&%Z1()#&%$#,D09)#&%;<RJU:1(+#ww5)4EGMO;:;%21.0)3*D0#&!$+#&!WD  $^%.0)'&;!$A^;!$41#&%1(+#%$>*#
:9)>?^#&!_;<'&D0%$%$.0F)#&!G%H%Z;41()#&!>*#B1();:5)%  #R1#&%21#&5)bcR;}>*t#1(+.0%Z<#&%G.0?)D0#&P  #!$#W<N;!$'&#&5q1;79)%$#
1()#CA)A+!$;'G(6;<%$#&D0#&'B1.0)3u/5)41m%$#B1rA)!G;?)?).0D0.0%21.0'&D0D g!$41()#&!1(+  #&.03(1.0)3K1(+#C#B^>*A+D0#&%$P
;41()#&!  .0%$#}5)#B1#&!$>r.0).0%21.0'>*#B1();:5*%$9+'G(C%HV=b0  ;9)D05r'B^'&D0#W+5*3#&)#&!$41#W5+9)A)D0.0'&41#W>*#&>?^#&!$%
;<1()#C#&)%$#&>?)D0#&b/cW()41r.0%GPH!$#&%$#B1-1.0+3K1()#  #&.03( 1%W1;mQ&  ;9)D05'&9+%$#q1()#CD0#&!$+#&!O1;m!$#&A^#&41
1()#65+#&'&.0%$.0;1!$#&#D0#&!$)#&5%*1()#6F)!$%o1>*#&>?^#&!/;<1()#6#&+%$#&>?+D0#&PC)51().0%  ;9)D05D0#&5g1;
!$#  #&.03( 1.0+3*1()#r5)41u%$#B11()#*%$>r#r%7<N;!1(+#*%$#&'&;)5>*#&>?^#&!;<1(+#r#&)%$#&>?)D0#&P)56%$;;)b
=+5);>*D %$#&D0#&'B1.0+3C#B^>*A+D0#&%<;!R1()#w5)41K%$#B1}?)%$#&5I;1()#w#B>*A)D0#qA)!$;?)?+.0D0. 1.0#&%D0D0#B,^.041#&%
1().0%WA+!$;?)D0#&>*b
w!$'&.0)34Eo6LNM!G#&.0>*)PQSST?)a*L  ().0'
(  #  .0D0Ds!$#&<N#&!H1;d%$.0>rA)D %q!$'&.0+3a%21!o1#&5;91K%q
%$.0>*A+D0#C>*#&'G()+.0%$> <N;!}#B,D09+41.0)3K1()#C#&#&'B1@;<MO;J;%21.0+3m>*#B1();:5)%  ()#&!G#w1()#C!$#&%$9)D 1.0+3/'&D0%2E
%$.0F)#&!G%  #&!$#u'&;>?).0+#&5  . 1();91  #&.03(1.0)3I1()#,+;41#&%$b!$'&.0+39)%$#&%*%$.0>rA)D0#/>*#&'G(+).0%$><N;!
5)#B1#&!$>r.0).0)3O1()#A)!$;?)?+.0D0. 1.0#&%;<.0)'&D09+5).0)3}#B>*A)D0#&%.01()#_1!$.0).0)3%$#B1b);!1()#  1(d#B^>rA)D0#
.0K1()#1!$.0).0)3*%G#B1P1()#O,D09)
#   !$#&<N#&!$%1;1(+#J9)>?^#&!W;<+1.0>*#&%_1()41K#B^>rA)D0#  %=>*.0%$'&D0%G%$. E
F)#&5? u1()#A+!$#B,^.0;9+%q'&D0%$%$.0F)#&!$%Gbc=()#7A)!G;?)?).0D0. 1
 W<N;!%G#&D0#&'B1.0)3/#B^>rA)D0#  1;I?^#7A+!21K;<
'&D0%$%$.0F+#&!gQ0%_1!G.0).0)3C%$#B1.0%5)#&F))#&5/%


	


QWm

ff
$

LNQW



a

LNQa

MO!$#&.0>*'G();%G#1()#},D09)#r;<R1()#*A^;  #&!ILNa#&>*A).0!$.0'&D0D <1#&!=1!2.0)3m%$#B,+#&!$D5).0#&!$#&17,D09)#&%
LNMO!$#&.0>*)PQSST?+a-bD 1();9+3(u1(+.0%>*#&'
()).0%$>5);:#&%K);41/()-,+#1()#  #&.03(1#&5,+;41.0)3;<Ww5)4E
MO;J;%o1.0)3I. 1K%21.0D0DA)!G;J5)9+'&#&%='&'&9)!$41#K#&)%G#&>?)D0#&%q)5.0%q%$.0>*A)D0#=1;@.0>*A)D0#&>*#&1U1(J9+%  #.0+'&D09)5)#
1().0%W>r#B1();J5LND0;)3  . 1(u5)4EGMO;:;%21.0)3a.0m;9)!W#&>*A+.0!$.0'&D#B,D09)41.0;)b
.039)!$#C`/%G();  %7/( A^;41()#B1.0'&DZ!$9);<MO;J;%21.0+3b/;41#q1(+411()#CF)!$%21q1!$.0+.0)3/%$#B1  ;9)D05
?^#w1()#r%G>*#*%7MO33.0)3U=();  #B,+#&!$POD041#&!W1!G.0).0)3m%$#B1%'&'&#&19)41#d#B>*A)D0#&%=1()41  #&!$#d>*.0%2E
'&D0%$%$.0F+#&5g? 1()#I#&!$D0.0#&!*>*#&>?^#&!K;<Z1()#I#&)%G#&>?)D0#&%GbB1().0%CF)39)!$#&P=#B^>*A+D0#/Q.0%C]4(+!$5)
#B>*A)D0#H1()41qA+!$#B,^.0;9+%'&D0%$%G.0F)#&!$%1#&)5}1;>*.0%$'&D0%G%$.0<b_\. 1(1()#w%$#&'&;)571!$.0).0)3%$#B1P#B^>rA)D0#
Q7;:'&'&9)!$%>9)D 1.0A)D0#H1.0>*#&%$P^%5+;7#B>*A)D0#&%O7)5@7%G.0)'&#Z1(+#B  #&!$#D0#&<1;91;<:1()#=F+!$%21H1!G.0).0)3
%$#B1+5)P^.0K1().0%W'&%$#&P>*.0%$'&D0%$%$.0F+#&5u?d1()#F+!$%21}D0#&!$)#&!$b=);!_1()#F+)D:1!$.0).0)3C%G#B1P#B^>rA)D0#}Q

fi "! #$%& '(*)+ !$,- .0/213 *4#.3fi56789 913;:<<=3,->13?@94#A13#BC>+=3=3DE
D;#139GFIHKJLM/KN ( :PORQEST.FU< =3BGC4 WV XYZ;A[,-W1\=3YE$1\?]! #$&$0' ( =313#B C#V^.3_
/2#>4>a`G=3,-# 13?b_6cd94 913;:d<@!e4@fX$=\BGg>4 h=3! =3V)G?i#jD ?i,-#1\1k>+=\=3DEgD#1\94
/.3..fi:l# P#Vj94=\V4Bg#@V4 B#=3DE-0.C< =3BG*2#! g/2=3*>4mh9!$ hj13=3BG13?])+$ * 913P#V]4
#13 V4#*#>>#!n k=3V@>4=\13o94h=3 ;:
Zp



fi

r+su}SmQz
r

vr++z

?^#&'&;>*#&%H1()#7A)!G#&5);>*.0)1#B>*A)D0#'G();%$#&gL  (+#&!$#&%+;*%$.0)3D0##B^>rA)D0#7.0%w'&'&#& 19+41#&5  . 1(
MO33.0)3a-U1(:9)%$PJ1()#w;-,+#&!$D0D:1#&%o1-EG%$#B17#&!$!$;!<;!s1().0%O'&D0%$%G.0F)#&!O>*.03( 1}?^#&'&;>*#H,+#&!2().03(+bw#&%GA). 1#
1().0%$P();  #B,+#&!GPZMO;J;%21.0+3  .0D0DA)!$;?)?)D ;?1.0@D0;  #&!7#&!$!$;!q!$41#  ()#&. 1*'&;>?+.0)#&%1()#;91-E
A)91K;<^1()#&%$#<N;9)!q'&D0%$%$.0F)#&!$%q%$.0+'&#. 1C<;:'&9)%$#&%q;'&;!$!$#&'B1D A)!$#&5).0'B1.0+3dA)!G#B,^.0;9)%GD >*.0%$'&D0%$%G.0F)#&5
#B>*A)D0#&%W)5  #&.03( 1%Z1()#qA)!$#&5).0'B1.0;+%;<1()#5).0#&!$#&1}'&D0%$%$.0F)#&!$%=?)%$#&5/;1()#&.0!'&'&9)!G'B<N;!
1()#1!$.0).0+3@%G#B1b*MO91*M;:;%21.0)3/'&D0%$;/;$,+#&!$F1r.0d1()#KA)!$#&%G#&)'&#K;<Z);.0%$#uLN%  #K#&>*A).0!$.0'&D0D 
%$();  .0my)#&'B1.0;[a-b
  
+rqts s

vuRxw4dhd

hd+l

c/g

<y
l

 qhoqh 


=#&'&#&1D P^%$#B,+#&!$D91();!$%LNMO!$#&.0>r)P+QSST?)U^)!G.0#&5)>*)PQSSTU|7;()-,.)x\];D0A^#&!21PQSSTU^|};)3
xq.0#B1-1#&!$.0'
()PQSSaK()-,+#dA)!$;A^;%$#&5I1(+#&;!$.0#&%<N;!1(+#r#&#&'B1. ,+#&)#&%$%K;<MO33.0)3+5MO;J;%o1.0)3
?)%$#&5;7#&>*#B1KD0b00%*LNQSS`aw?+.0%A)D09+%,!G.0)'&#5+#&'&;>*A^;%$. 1.0;;<_'&D0%G%$.0F)'&41.0;#&!$!$;!GbwB
1().0%w5)#&'&;>*A^;%$. 1.0;  #'&r,^.0#  1()#7#BA^#&'B1#&5#&!G!$;!;<_dD0#&!$).0)3@D03;!$. 1()>;dA)!o1.0'&9)D0!
z;{ $ |:f zo}~ dh  z  h/+5 z  { h)Nh |*if z iGv jfW%W()-,.0)3q1(+!$#&#}'&;>*A^;)#&1%$
Qb}l- { s
i 1#&!$>>*#&%$9)!$.0+3();  '&D0;%G#H1(+#$,+#&!$3#'&D0%$%$.0F)#&!OA)!G;J5)9+'&#&5r?C1()#D0#&!G).0)3D03;4E
!G. 1()>  .0D0D?^#1;q1(+#1!$3#B1<N9))'B1.0;)U
`b} { 4  { hdGfW1#&!$> >*#&%$9+!$.0)3();  >9+'G(#&'G(;<R1()#@D0#&!$).0)3D03;!$. 1()>r0%39+#&%$%$#&%  .0D0D
,!2  . 1(m!G#&%$A^#&'B11;*#&'G(;41()#&!LN();  ;<e1#&C1()#B5).0%$3!G#&#&a-U)5
[b}1#&!$>>*#&%$9)!$.0+31()#W>*.0).0>9)>'&D0%G%$.0F)'&41.0;@#&!$!$;!Z%$%G;J'&.041#&5  . 1(q1()#=M$+#&%;A1.0>*D
'&D0%G%$.0F)#&!K<;!=1()#}1!$3#B1m<9)+'B1.0;Le1().0%1#&!$> .0%%$;>*#B1.0>*#&%K!$#&<N#&!$!$#&5u1;%=1()#@.0 1!G.0)%$.0'
1!G3#B1);.0%$#&a-b
w%$.0)31().0%_<N!$>*#  ;!$t7. 1W()%?^#&#&K%$9)33#&%21#&5LNMO!$#&.0>r)P)QSST?)a1()41=?^;41(KMO33.0)3)5CMO;:;%21-E
.0)3!$#&5)9+'&#@#&!G!$;!K? !$#&5)9)'&.0)3r1()#,!$.0)'&#1#&!$>*b])!$#&9))5)5y)'
()A).0!$#6LNQSSTa*!G39)#1()41
MO;J;%o1.0)3CD0%$;K41-1#&>*A1%R1;C!$#&5+9)'&#Z1(+#w#&!G!$;!O.071(+#?).0%R1#&!G>%G.0)'&#q. 1<N;:'&9)%$#&%O;/>*.0%$'&D0%$%G.0F)#&5
#B>*A)D0#&%$bWy+9)'G(IK<;:'&9)%>*$'&9)%G#1()#qD0#&!$)#&!R1;KA)!$;:5)9)'&#w/#&)%$#&>?)D0#q<9+)'B1.0;1()41}5).0#&!$%
%$.03).0F+'& 1D <!$;>1()#d%$.0)3D0#@D0#&!$).0)3D03;!$. 1()>*bB<N'B1PMO;J;%21.0+3>r-'&;)%o1!$9)'B1/<N9))'BE
1.0;C1(+417.0%=);417#B,+#&A)!$;:5)9)'&.0?)D0#q?. 1%W'&;>rA;+#& 1D0#&!G).0)3CD03;!$. 1()>LN#&b03b0P'
())3.0)3dD0.0)#&!
A)!$#&5+.0'B1.0;)%q.0 1;II'&D0%$%$.0F)#&!1()41*'&;1.0)%);EGD0.0+#&!wA+!$#&5).0'B1.0;)%$a-bK 1K.0%1(+.0%'&A)?+.0D0. 11()41
>*t#&%MO;:;%21.0)3CIA)A)!$;A+!$.041#D03;!G. 1()><;!O'&;>?).0).0)31()#qA)!G#&5).0'B1.0;)%O;<  #&tJ*D0#&!G).0)3
D03;!$. 1()>r%LN.0b0#&b0PRD03;!$. 1()>*%Z1()41()$,+#7*%$.0>rA)D0#D0#&!$).0)3r?).0%$a-bWBK1()#&.0!W!G#&'&#& 1KA)A^#&!$PMO9)#&!
)5@|};()$,^._LNQSSSa5)#&>*;)%o1!$41#&51()41qMO;:;%21.0)3K5);:#&%.0+5)#&#&5d%G#&#&>1;K!$#&5+9)'&#=?).0%O<N;!'&#&!o1.0
!$#&D  ;!$D05/A)!$;?+D0#&>*%$b;!$#%$9)!$A+!$.0%$.0)3D PJ1()#BD0%$;*%$();  #&51(+417M33.0+3r'&/D0%$;*!G#&5)9)'&#1()#
?).0%WA^;!21.0;u;<+1(+##&!$!$;!$P;<e1#&m<N;!_1()#%G>*#}5)41*%G#B1%=<N;!  ().0'
(mMO;J;%21.0+3r!$#&5)9+'&#&%1()#?).0%$b
cW(+;9)3(d1()#K?).0%2Eo,!$.0)'&#C5)#&'&;>*A^;%$. 1.0;.0%q.01#&!$#&%21.0)3P1()#&!$#K!$#K'&#&!21.06D0.0>*. 141.0;+%O1;
A)A)D .0)3/. 1}1;!$#&D E  ;!$D055)41u%$#B1%$bucR;m?^#K?)D0#q1;#&%21.0>*41#1(+#*?).0%$P,!$.0+'&#&PH)5I1!G3#B1
);.0%$#7<;!rA)!21.0'&9+D0!=A)!$;?+D0#&>*P  #})#&#&5C1;dtJ+;  1(+#}'B19)DR<9)+'B1.0;m?^#&.0)3*D0#&!$)#&5+bcW(+.0%W.0%
9))$,.0D0?+D0#<N;!q>*;%21C!$#&D E  ;!$D05A)!$;?)D0#&>r%$bcR;I5)#&D  . 1(d1().0%qA)!$;?)D0#&>|};()$,^.)5r\];D0A^#&!21
LNQSSTam%$9+33#&%216();D05).0)3;916%$;>*#;<1()#5)41P=1()#A)A)!$;'
("9)%$#&5? M9+#&!@+5|7;()-,.
LNQSSSa.0@1()#&.0!7%219+5bIcW()#C>*.06A+!$;?)D0#&>  . 1(/1().0%O1#&'G(+).08J9+#*.0%O1()41}1(+#w1!$.0).0)3/%$#B1@%$.0&#
.0%w3!$#&41D !$#&5+9)'&#&5u.0;!$5)#&!Z1;d3#B1C3;J;:5#&%21.0>*41#&%;<1()#?).0%+5*,!$.0)'&#=1#&!$>*%$b\]#K()$,+#
'G(+;%$#&K1;K%21!$.0'B1D <;:'&9)%;/3#&)#&!GD0.0&41.0;u'&'&9)!$'B.0/;9)!%219+5P^.0@A+!21}?#&'&9+%$#qM9+#&!W)5
|};()$,^.00%  ;!Gtd()%=)%  #&!$#&5K1()#}8:9)#&%21.0;u?;91  ()#B1()#&!WMO;:;%21.0)3*+5mMO33.0)3r!$#&5+9)'&#1()#
ff

fi

stv{wz{y{t

?).0%<;!!$#&D  ;!GD05A+!$;?)D0#&>*%ILe1()#B?^;41(5+;a-P)5?^#&'&9)%$#71()#&.0!#B^A^#&!$.0>r#& 1%5)#&>*;)%o1!$41#
1()41  ().0D0#O1().0%w5)#&'&;>*A^;%$. 1.0;3. ,+#&%w%$;>*#7.0+%$.03( 1K.0 1;@#&)%$#&>?)D0#>*#B1();:5)%$P. 1K.0%=;)D *%G>*D0D
A)!21=;<1()#W#&8:9)41.0;)bO);!5).0#&!$#& 1w5)41}%$#B1%^1(+#B/;?+%$#&!2,+#'&%$#&%  ()#&!$#OMO;:;%21.0)3})5rM33.0+3
?^;41(r5)#&'&!G#&%$#q>*;%21D C1()#H,!G.0)'&#A^;!21.0;d;<1()##&!$!$;!$P)5r;41(+#&!'&%G#&%  ()#&!$#WMO;:;%21.0)3K)5
MO33.0)3m?^;41(!$#&5)9)'&#1()#K?).0%q)5@,!$.0)'&#C;<1()#K#&!$!$;!$bCc=()#&.0!1#&%21%D0%$;/%$#&#&>1;/.0+5).0'&41#
1()417MO;:;%21.0)30%W3#&)#&!GD0.0&41.0;#&!$!G;!.0+'&!$#&%$#&%W;1()#q5);>r.0)%  ()#&!$#qMO;:;%21.0)3C.0)'&!$#&%G#&%1()#
,!$.0)'&#7A^;!21.0;;<1()#}#&!$!$;!GU?)91P. 1.0%=5).0r'&9)D 1=1;*5+#B1#&!$>*.0)#  ()41%$A^#&'B1%;<+1(+#75)41d%$#B1%
D0#&5K1;}1()#&%$#!$#&%$9+D 1%$b

j

 



c ().0%=%$#&'B1.0;5+#&%$'&!$.0?^#&%W;9)!W#&>rA).0!$.0'&D%o19)5;<RMO33.0)3PRw5)4EGMO;J;%21.0+3Ps)5u!$'&.0+3b=H'
(;<
W
1()#&%$#1()!$#&#>*#B1();:5)%  %_1#&%o1#&5  . 1(m?^;41(m5+#&'&.0%$.0;C1!$#&#&%)5/)#&9)!GD)#B1  ;!$t:%$b

q+$

"sgo

cedo*d



cR;r#B,D09)41#1()#7A^#&!$<N;!$>*)'&#7;<_M33.0+3@+5MO;:;%21.0)3P  #;?1.0)#&5r:9)>?#&!;<_5)41@%$#B1%
<N!$;>1()#u). ,+#&!G%$. 1;<\.0%$'&;)%$.0'G().0)##&!G).0)36!$#&A^;%$. 1;!o"%  #&D0D=%71()#mwVWK5)41
%$#B1*!$#&A^;%$. 1;!2LN9)!$A+( xw()PQSSa-b*cW()#&%$#5)41@%$#B1%  #&!$#(+)5%G#&D0#&'B1#&56%$9)'
(*1()41q1()#B
LNaZ'&>*#=<!$;>!G#&D E  ;!$D05dA)!$;?)D0#&>r%$PLN?)a,!$.0#&5*.0C'G(+!$'B1#&!$.0%21.0'&%$P^)5uLN'&a  #&!$#5+#&#&>*#&5*9)%$#&<N9)D
?A)!G#B,^.0;9)%7!$#&%$#&!$'
()#&!$%$b/cR?)D0#*Q/3. ,+#&%=1()#C'G(+!$'B1#&!$.0%21.0'&%K;<;9)!}5)41m%G#B1%$b/cW()#C5)41u%$#B1%
'G(+;%$#&d,!2'&!G;%$%qr:9)>?^#&!;<Z5).0>*#&)%G.0;)%=.0)'&D09+5).0)31()#=1A#;<^1()#<#&419)!G#&%w.0r1()#5)41
%$#B1]LN.0b0#&b0P7'&;1.0J9+;9)%$P75+.0%$'&!$#B1#&P};!@>r. ;<1()#r1  ;a-Uw1()#:9)>?^#&!r;<};91A+91'&D0%$%G#&%$UC)5
1()#K:9)>?^#&!w;<Z#B>*A)D0#&%.0d1()#K5)41I%$#B1bCcR?)D0#KQ@D0%G;@%G();  %1()#K!$'
(). 1#&'B19)!$#C+5r1!G.0).0)3
A)!$>r#B1#&!$%=9)%G#&5/.0m;9)!W)#&9+!$D)#B1  ;!$t:%=#BA^#&!$.0>*#&1%$b

q+2

goq


i

   8

=#&%G9)D 1%$Pq9))D0#&%$%r;41()#&!  .0%$#u);41#&5)Pw!$#m-,+#&!G3#&5;$,+#&!dF,+#m%21)5)!G5gQ{4EG<;D05'&!$;%$%},D0.05+41.0;
#BA#&!G.0>*#& 1%Gb+;!#&'
(dQ{4EG<N;D05I'&!$;%$%,D0.05)41.0;71()#=5+41}%$#B1w.0%F)!$%21=A+!21. 1.0;)#&5@.0 1;Q{7#&8:9)D E
%$.0&#&5%$#B1%GP_1()#&#&'
(g%$#B1m.0%C.019)!$9)%$#&5%w1()#1#&%21u%G#B1  ().0D0#1()#/'&D0%$%$.0F)#&!w1!G.0)%C;1()#
;41()#&!O).0)#w%$#B1%$b+;!#&'G(/<N;D05dI#&)%$#&>?)D0#q;<`'&D0%$%$.0F)#&!G%.0%'&!G#&41#&5)bWVW!$;%G%,D0.05)41.0;/<N;D05)%
 #&!$#rA^#&!$<N;!$>*#&56.0+5)#&A^#&)5)#&1D <N;!#&'
(D03;!G. 1()>*b@\]#71!$.0)#&5/1(+#r)#&9)!GDZ)#B1  ;!$t:%9+%$.0)3
%21)5+!$5?+'Gt:A)!$;A)341.0;D0#&!G).0)3"LN=9)>*#&D0(+!21P7vw.0 1;+P7x\.0D0D0.0>*%GPQSXTa-bY!$>r#B1#&!
%$#B1-1.0)3%7<;!1()#K)#&9)!GDs)#B1  ;!GtJ%.0)'&D09+5)#K@D0#&!G).0)3@!$41#C;<H{b0QPZI>*;>r#& 19)>z1#&!$>;<{b0SP
)5  #&.03(1%K!$#@.0). 1.0D0.0&#&5!$)5);>rD 1;?^#r?^#B1  #&#&EG{b0)5{b0bcW()#dJ9+>?^#&!7;<(+.05)5)#&
9)). 1%q)5#&A^;:'G()%q9)%$#&5<N;!1!$.0).0)3I!$#3. ,+#&6.0d1()#K)#B:1K%$#&'B1.0;+b\]#K'G();%G#w1()#KJ9)>?^#&!;<
().05)5+#&d9)+. 1%?)%G#&5@;1()#w:9)>?^#&!;<.0)A)91q)5@;91A)91q9)+. 1%$bOcW().0%O'G();.0'&#  %?)%$#&5I;1()#
'&!$. 1#&!$.0;<O()-,.0)341@D0#&%21/;)#r(+.05)5)#&69)+. 1dA^#&!};91A)91P41@D0#&%o1@;)#d().05)5)#&69+). 1d<N;!7#B,+#&!2
1#&/.0)A)91%$P+5/F,+#q().05)5)#&@9)). 1%O?^#&.0)3KK>*.0).0>9)>*bcW()#w:9)>?^#&!;<#&A^;:'G()%  %?)%$#&5/?^;41(
;d1()#J9)>?^#&!=;<Z#B>*A)D0#&%q)5d1()#:9)>?^#&!;<ZA)!$>*#B1#&!G%CLN.0b0#&b0P1;A^;D0;34a;<^1()#)#B1  ;!$t:b
y)A^#&'&.0F)'&D0D P  #9)%G#&5rT{=1;X{K#&A^;J'
()%<;!%$>*D0DA)!$;?)D0#&>*%.0,+;D ,^.0)3K<N#  #&!1()I`{#B^>*A+D0#&%$U
{/#&A^;J'
()%<N;!O1()#K>*.05EG%$.0&#&5A)!$;?)D0#&>*%'&;1.0).0)3u?#B1  #&#&`{K1;/{{m#B>*A)D0#&%$U)5`{1;
{#&A;:'G(+%<N;!D0!G3#&!A)!$;?)D0#&>*%Gb+;!1(+#=5)#&'&.0%$.0;1!$#&#&%  #9+%$#&5}1(+#VWb0=1;:;DLN}9+.0)D0)PQSS[a
)5dA)!$9))#&5w1!G#&#&%qL  (+.0'G(r#&>rA).0!$.0'&D0D uA)!$;:5)9)'&#?^#B1-1#&!A#&!G<;!$>r)'&#&aZ%%$9)33#&%21#&5@.0r}9+.0)D0)0%
 ;!$tJb
*

fi

q41ry)#B1
?)!$#&%o1-EG'&)'&#&!2E 
'&!$#&5). 1-EG
'&!$#&5). 1-EG3
5).0?^#B1#&%
3D0%$%
()#&!21-EG'&D0#B,+#&D0+5
()#&A)41. 1.0%
();9)%G#BEo,+;41#&%2EGX
(^A^;
.0;);%$A+()#&!$#
.0!$.0%
t:!2Eo,^%oEGtJA
D0?^;!
D0#B1-1#&!
A)!$;>r;41#&!$%2EGS[T
!$.0?^;%$;>*#BEG?+.0)5
%$41#&D0D0. 1#
%$#&3>*#&141.0;
%$.0'
t
%$;)!
%$;$^?^#&
%$A)D0.0'&#
,+#&().0'&D0#

V=%$#&%
TSS
TS{
Q{{{
TX
`Q
[{[
Q
[
[`
[Q
QS
[QST

`{{{{
S[T
QX
T[
`[Q{
[`
`{X
TX[
[QS{
XT

r+su}SmQz
r

VWD0%$%
`
`
`
`
T
`
`
`

`
[
`
`
`T
`
`
T

`
`
QS
[


)#&419)!$#&%
VW;1
q.0%$'
S
E
T
S

Q[
S
E
S
E
X

T
Q[
E
QT

``
[
E

E
E
[T
X
X
QT
E
E

E
S
[T
E
QS
E

``
T{
E
E
[
E
T{
QX
E

vr++z

B)A)91%
S

T[
X
S
Q[
[`
QT

[


`S
QT
``X
QST
[T
QS

T{
Q[
`{
QX

#&9)!GD#B1  ;!Gt
 91A)91%
vw.05)5)#&)%
Q

Q
Q{
Q
Q{
Q

T
Q{
Q

Q
Q{
Q


Q
Q
Q{
[

Q
Q
Q
Q{
`T
{
Q
`{
Q
`{
T
Q

Q
Q
Q{
Q
Q{
QS
`
`
`

Q{

HA^;J'
()%
`{
[
[{
[{
X{
{
T{
{
{
{
X{
`{
X{
[{
[{
[
[{
`{
{
T{
{
[{
{

cR?)D0#}Q}y)9+>*>*!2r;<1(+#5)41q%$#B1%9)%G#&5.01().0%A)A^#&!$by)();  !$#s1()#:9)>?^#&!R;<)#B>*A)D0#&%.0
1()#w5)41K%$#B1U:1()#w:9)>?^#&!;<;91A)91q'&D0%$%G#&%$U:1()#qJ9+>?^#&!;<'&; 1.0:9);9)%+5d5).0%G'&!$#B1#
.0)A+91<N#&419)!$#&%$U+1()#J9)>?^#&!W;<_.0)A)91P;91A)91P)5().05)5)#&9)). 1%=9+%$#&5.0*1()#7+#&9)!$D
)#B1  ;!GtJ%Z1#&%21#&5)U+5/();  >* #&A^;J'
()%W#&'
()#&9+!$D)#B1  ;!$t  %_1!$.0)#&5+b

q+rq

"sgox%

cedo*d



do

:

cR?)D0#Z`W%$(+;  %)1#&%21-EG%$#B1#&!G!$;!!$41#&%s<;!)1()#_5)41=%G#B1%5)#&%$'&!G.0?#&5.07cs?)D0#QW<N;!F,+#Z)#&9+!$D)#B1  ;!$t
>*#B1();:5)%q)5<N;9)!q5)#&'&.0%$.0;@1!$#&#K>*#B1();:5)%$bmLNBcs?)D0#&%I)5  #%G();  1()#&%$#K#&!$!G;!!$41#&%%
 #&D0Ds%1()#%21)5)!G5u5)#B,.041.0;<;!w#&'G(;<^1(+#&%$#W,D09)#&%$b0aD0;+3  . 1(d1()#W1#&%21-EG%$#B1*#&!$!$;!G%<N;!
MO33.0)3P!$'&.0+3PZ)5w5)4EG?^;J;%21.0+3P  #C.0+'&D09)5)#1()#w1#&%21-EG%$#B1@#&!$!$;!!$41#*<N;!/%$.0)3D0#C+#&9)!$D E
)#B1  ;!$t)5d%$.0+3D0#5)#&'&.0%G.0;Eo1!$#&#C'&D0%$%$.0F+#&!$bW\]#KD0%$;I!$#&A^;!21K!$#&%$9)D 1%w<N;!w@%$.0>*A+D0#ILN?)%G#&D0.0)#&a
)#&9)!GD EG)#B1  ;!$t@#&)%$#&>?)D0#A)A)!$;'
(Ku'&!$#&41.0)3r/#&)%$#&>?)D0#;<s+#B1  ;!$tJ%  ()#&!$#q#&'G()#B1  ;!$t
,!$.0#&%7;)D g?!G)5);>*D .0). 1.0D0.0&.0)3r1()#  #&.03( 1%;<s1()#*)#B1  ;!GtJb*\]#r.0)'&D09)5+#w1()#&%$#*!$#&%G9)D 1%
.0/'&#&!21.0'&;>*A)!$.0%$;+%R1;C5)#&>*;)%21!G41#1()#&.0!W%$.0>*.0D0!G. 1I1;CMO33.0)3b  +#w;?,.0;9)%W'&;)'&D09)%G.0;
5)!$  <!$;>z1()#!$#&%$9+D 1%w.0%1()41*#&'
(6#&)%$#&>?)D0#>*#B1();:5A+A#&!G%Z1;I!$#&5)9+'&#W1()##&!$!$;!q!$41#K<N;!
D0>*;%21uD0D=;<_1()#/5)41%G#B1%$P=)5.0>*"'&%$#&%q1().0%C!$#&5+9)'B1.0;g.0%CD0!$3#&b<N'B1PH1()#1  ;4E
1.0D0#&5%$.03r1#&%21K.0+5).0'&41#&%1()41#B,+#&!2#&)%$#&>?)D0#7>*#B1(+;J5.0%%$.03+.0F)'& 1D ?#B1-1#&!H1(). 1%%G.0)3D0#


fi

stv{wz{y{t

#&9+!$Dw#B1  ;!$t
q41ry)#B1
?)!$#&%o1-EG'&)'&#&!2E 
'&!$#&5). 1-EG
'&!$#&5). 1-EG3
5).0?^#B1#&%
3D0%$%
()#&!21-EG'&D0#B,+#&D0+5
()#&A)41. 1.0%
();9)%G#BEo,+;41#&%2EGX
(^A^;
.0;);%$A+()#&!$#
.0!$.0%
t:!2Eo,^%oEGtJA
D0?^;!
D0#B1-1#&!
A)!$;>r;41#&!$%2EGS[T
!$.0?^;%$;>*#BEG?+.0)5
%$41#&D0D0. 1#
%$#&3>*#&141.0;
%$.0'
t
%$;)!
%$;$^?^#&
%$A)D0.0'&#
,+#&().0'&D0#

y1
[b0
Qb0X
`b0S
`[b0S
[Xb0T
QXb0T
`{b0Q
b0S
Tb0
Sb0
b0[
`b0[
Tb0Q
QXb0{
b0[
Sb0[
Q[b0{
Tb0T
b0S
QTb0T
Sb0`
b0
`b0S

y).0>rA
[b0
Q[b0
`b0
`[b0{
[b0`
Qb0
QSb0
b0X
Tb0`
b0
[b0S
{b0X
[b0`
Q`b0X
b0X
Xb0
Q{b0S
b0[
b0
Qb0S
Tb0
b0{
`Qb0`

M ;J;%21.0+3
O
w!$'
w5)
[b0X
b0{
Qb0X
Qb0
`b0`
`b0[
`b0
`[b0[
[`b0{
[Qb0Q
`{b0
`Qb0Q
QSb0{
QSb0
b0Q
b0[
Tb0`
Tb0`
b0T
Xb0[
[b0
[b0S
{b0
{b0[
[b0`
[b0`
b0
b0T
b0
b0T
Xb0Q
Xb0`
Sb0S
Q{b0{
[b0
[b0[
b0
b0
Q`b0S
Q[b0{
Tb0
Tb0[
b0{
b0`
QSb0Q
QSb0

MO3
[b0
Q[b0X
`b0`
``b0X
[[b0Q
Qb0{
Qb0X
b0Q
Tb0`
Sb0`
b0{
{b0X
b0`
Q{b0
b0{
Xb0
Q{b0T
b0
b0
QTb0X
Tb0S
[b0S
`{b0

V=b0
y1
b0{
Qb0S
`Sb0T
`b0X
[Qb0[
`b0[
`Qb0`
[b0T
{b0
Xb0Q
b0`
{b0T
QTb0
Qb0{
Q`b0X
QQb0`
Q[b0X
[b0
Qb0[
`Sb0
Xb0{
b0S
`Sb0

MO3
[b0
Q[b0
`b0`
`b0
`b0X
QSb0
Qb0[
[b0T
{b0
Tb0
b0S
{b0T
Q[b0
b0{
Q{b0T
Q{b0`
Sb0S
[b0{
Qb0`
`b0[
b0S
b0
`b0Q

M ;:;%21.0)3

w!$'
w5)
[b0
[b0
Qb0{
Q[b0
`b0S
`Tb0
`Tb0{
`b0
`b0
`[b0[
`Qb0
`{b0X
QTb0S
Qb0`
b0{
b0X
{b0
{b0
Tb0{
Tb0Q
b0Q
b0T
{b0[
{b0
Q[b0{
QQb0T
b0Q
[b0S
Tb0X
Tb0
Sb0[
Sb0T
Xb0T
Xb0
Qb0
Qb0
Qb0Q
Qb0{
`Qb0
`Qb0
b0`
Tb0
b0Q
b0[
``b0
``b0S

cR?)D0#}`}cR#&%21%$#B1=#&!G!$;!Z!$41#&%<;!1()#=5)41%$#B1%H9+%$.0)3*LNQaH%$.0+3D0#W)#&9)!$D)#B1  ;!Gt'&D0%G%$.0F)#&!$U_LN`a
#&)%G#&>?)D0#  (+#&!$#K#&'G(6.0)5+. ,^.05)9+Ds)#B1  ;!Gt.0%1!$.0+#&59)%$.0)371()#C;!$.03.0)D1!G.0).0)3
%$#B1*)5@1(:9)%q;)D 5).0#&!G%w<N!$;>1()#K;41()#&!+#B1  ;!$tJ%.0@1(+##&+%$#&>?+D0#?. 1%!G)5);>
.0). 1.0D  #&.03( 1%GUKLN[a#&+%$#&>?+D0#  (+#&!$#1()#@)#B1  ;!$t:%K!$#71!$.0)#&59)%$.0)3u!$)5);>*D 
!$#&%G>*A)D0#&51!$.0).0)3%$#B1%uLNMO33.0)3a-U#&)%G#&>?)D0#  (+#&!$#}1(+#@+#B1  ;!$tJ%C!G#71!G.0)#&5
9)%G.0)3  #&.03( 1#&5!G#&%$>*A)D0#&5r1!$.0).0)3@%$#B1%*LNMO;J;%o1.0)3a  ()#&!$#W1()#7!$#&%G>*A)D0.0)3@.0%?+%$#&5
;}1()#LNa!$'&.0+37>*#B1();:5*)5LNaHw5)}>*#B1(+;J5)U_LNTaH%$.0)3D0#W5+#&'&.0%$.0;}1!G#&#='&D0%$%$.0F+#&!$U
LNaKuMO33.0)3#&+%$#&>?+D0#d;<5)#&'&.0%G.0;u1!G#&#&%$U=)5"LNXa!$'&.0+3)5"LNSa5)MO;J;%o1.0)3
#&)%G#&>?)D0#&%;<s5+#&'&.0%$.0;C1!$#&#&%Gb

'&;>*A^;)#&1r'&D0%$%$.0F)#&!41}1()#CS'&;)F)5)#&)'&#CD0#B,+#&D0UO();  #B,+#&!GP);+#C;<1()#C#&+%$#&>?+D0#*>*#B1();:5)%
!$#}%G.03).0F)'&1D ?^#B1-1#&!Z1()m;41(+#&!W#&)%$#&>?)D0#}A)A+!$;'G(41W1()#S'&;+F)5)#&)'&#}D0#B,+#&D0b
cR;?^#B1-1#&!)D &#@cR?)D0#@`0%K!$#&%$9)D 1%$PO.039)!$#&%[)5A)D0;411()#rA^#&!$'&#&13#I!$#&5)9)'B1.0;.0
#&!$!$;!O<N;!R1()#w5)4EGMO;:;%21.0)3Pw!$'&.0)3P+5@MO33.0)3C>r#B1();J5I%K<N9))'B1.0;I;<1()#q;!$.03.0+D#&!G!$;!
!$41#&b >*.0).0)371()#&%$#F)39)!$#&%  #+;41#W1()41*>*;<^1(+#3.0)%qA)!G;J5)9+'&#&5?/1()##&)%$#&>?)D0#
>*#B1();:5)%@!$#>9)'
(D0!$3#&!1()1()#%o1)5)!$55)#B,.041.0;,D09)#&%GbzB61#&!G>*%I;<}'&;>*A)!$.0%G;)%
;<q5).0#&!$#& 1u>r#B1();J5+%$P. 1.0%*A)A)!$#&1<N!$;>?^;41(gF)39+!$#&%w1()41*1()#uM;:;%21.0)3>*#B1();:5)%LNw5)4E

Z



fi

r+su}SmQz
r

vr++z

kr-vs-kp
letter
segmentation
labor
soybean
satellite
sick
sonar
vehicle
glass
ionosphere
promoters-936
ribosome-bind
iris
splice
credit-g
diabetes
Ada-Boosting
hypo
Arcing
hepatitis

Bagging

credit-a
house-votes-84
heart-cleveland
breast-cancer-w
-40

-20

0

20

40

60

80

100

Percent Reduction in Error

.039)!$#}[7=#&5)9)'B1.0;].0#&!$!$;!*<N;!C5+4EGM;:;%21.0)3Pqw!$'&.0)3P)5MO33.0)3)#&9+!$DO)#B1  ;!$t#&E
%$#&>?+D0#&%=%=rA#&!G'&#& 13#;<+1(+#};!$.03.0)D#&!G!$;!W!$41#ILN.0b0#&b0P*!$#&5+9)'B1.0;u<N!$;>m#&!G!$;!
!$41#;<`b01;Qb0`  ;9)D05@?#{!G#&5)9)'B1.0;@.0@#&!G!$;!!G41#&P-9)%21q%O!$#&5+9)'B1.0;
<!$;>Q{b0{1;rb0{  ;9)D05uD0%G;r?^#*{!$#&5)9)'B1.0;)a-bD0%G;r%$();  L  (+. 1#}A^;!21.0;
;<Z#&'G(?)!$a.0%q;)#%o1)5)!$55)#B,^.041.0;<N;!H1()#&%$#!$#&%$9)D 1%$b7cW()#7%o1)5)!$55)#B,.041.0;
.0%W%$();  m%=u5)5). 1.0;C1;1()##&!$!$;!W!G#&5)9)'B1.0;)b

*

fi

stv{wz{y{t

letter
segmentation
promoters-936
kr-vs-kp
satellite
labor
breast-cancer-w
hypo
sonar
glass
ionosphere
vehicle
sick
hepatitis
soybean
heart-cleveland
ribosome-bind
Ada-Boosting
splice
Arcing
credit-g

Bagging

credit-a
diabetes
iris
house-votes-84
-80

-60

-40

-20

0

20

40

60

80

Percent Reduction in Error

.039)!$#}7=#&5)9)'B1.0;K.0#&!$!$;!R<;!Rw5)4EGMO;J;%21.0+3P)!G'&.0)3P)5M33.0+3w5+#&'&.0%$.0;1!$#&#O#&)%$#&>?)D0#&%
%A^#&!$'&#&13#;<1(+#;!$.03.0+D#&!$!G;!!$41#&b=D0%$;%$();  L  (). 1#wA;!o1.0;d;<#&'
(/?)!$a
.0%W;)#}%21+5)!$5m5)#B,.041.0;u<N;!1()#&%G#}!$#&%$9)D 1%Gb

Z

 *

fi

r+su}SmQz
r

vr++z

MO;J;%o1.0)36)5w!$'&.0)3aC!$#/%$.0>r.0D0!*.01()#&.0!C!$#&%$9+D 1%$P=?^;41(<;!r)#&9)!$DO)#B1  ;!$t:%*)55)#&'&.0%G.0;
1!$#&#&%$b"+9)!21()#&!$>r;!$#&P_1()#/w5)4EGMO;:;%21.0)3+5!$'&.0+3>*#B1();:5)%CA)!$;:5)9)'&#I%$;>*#/;<1()#/D0!G3#&%21
!$#&5)9+'B1.0;)%}.06#&!$!G;!$b  I1(+#C;41()#&!7()+5)P  ().0D0#w1(+#*MO33.0)3u>*#B1(+;J56'&;+%$.0%21#&1D ]A)!$;:5)9)'&#&%
!$#&5)9+'B1.0;)%=.0#&!$!$;!<;!D0>*;%21CD0D;<1()#'&%$#&%$P  . 1(u)#&9+!$D)#B1  ;!GtJ%H1()#7MO;:;%21.0)3@>*#B1();:5)%
'&u%G;>*#B1.0>*#&%=!G#&%$9)D 17.0m.0)'&!$#&%$#}.0u#&!$!$;!$b
s;J;t:.0)3411()#@;!$5)#&!G.0)3u;<R1()#@5)41%$#B1%K.0m1(+#1  ;F)39)!$#&%/Le1()#@!$#&%$9)D 1%!$#@%$;!21#&5? 
1()#*A^#&!$'&#&13#@;<!G#&5)9)'B1.0;9+%$.0)3K1()#r5)4EGMO;:;%21.0)3>*#B1();:5)a-P  #r+;41#1()41}1(+#r5)41%$#B1%
<N;!  ().0'G(u1()#@#&)%$#&>?)D0#I>*#B1();:5)%%G#&#&>1;  ;!$t  #&D0D!G#@%G;>*#  ()41/'&;)%$.0%21#&1m'&!$;%$%C?^;41(
)#&9)!GD)#B1  ;!$t:%q)5u5+#&'&.0%$.0;d1!$#&#&%$b});!1()#<#  5);>*.0+%  (+.0'G(%$#&#7.0)'&!G#&%$#&%q.0#&!$!G;!$P. 1C.0%
5).0*'&9+D 11;K!$#&'
(m%21!$;+3'&;+'&D09)%$.0;)%%$.0+'&#H1(+#w#&+%$#&>?+D0#>*#B1();:5)%O%$#&#&>1;C5);  #&D0D<N;!KD0!$3#
:9)>?^#&!;<O5);>*.0+%$b  )#C5+;>*.0;  ().0'G(m1()#CMO;J;%21.0+3u>*#B1();:5)%}5+;/9)).0<N;!$>*D A;:;!$D .0%
1()#q();9)%G#BEo,+;41#&%2EGX*5);>*.0+bW%  #5).0%$'&9)%G%D041#&!GP:1()#&!$#q>*-);.0%$#q.01().0%5);>r.0)0%O#B^>*A+D0#&%
1()41'&9+%$#&%_1()#MO;:;%21.0)3d>*#B1();:5)%W%$.03).0F+'& 1A+!$;?)D0#&>*%$b

q+r %R  
y  2hp
H!GD   ;!$tLNv+%$#&uxy+D0>*;)PQSS{a;u#&)%$#&>?)D0#&%W%G9)33#&%21#&5r1()41#&)%$#&>?)D0#&%  . 1(m%=<N# 
%1#&@>*#&>?^#&!$%  #&!$#=5)#&8:9)41#Z1;7%$9)r'&.0#& 1D u!$#&5+9)'&#1#&%21-EG%$#B1#&!$!$;!Gb\().0D0#Z1().0%H'&D0.0>>*-u?^#
1!$9)#=<;!1()#=#&!$D0.0#&!A)!$;A^;%$#&5r#&)%$#&>?)D0#&%$P1()#WMO;J;%o1.0)3D0. 1#&!G419)!$#LNy+'G()A).0!G#&P)!G#&9))5)PM!o1D0#B1-1P
x#&#&PQSSa*()%C!$#&'&#&1D "%$9)33#&%21#&5LN?+%$#&5;<#  5)41%G#B1%  . 1(5)#&'&.0%$.0;1!$#&#&%Gaw1()41
. 1@.0%A^;%$%$.0?)D0#1;u<N9)!21()#&!}!G#&5)9)'&#1#&%o1-EG%$#B1I#&!G!$;!7#B,+#&<1#&!=1#&>*#&>?^#&!$%()-,+#@?^#&#&65)5)#&5m1;
#&+%$#&>?+D0#LN)5/1(+#B+;41#1()411().0%7!$#&%G9)D 1dD0%$;A)A)D0.0#&%1;uM33.0+3a-bm1().0%7%$#&'B1.0;+P  #
A^#&!$<N;!$>5)5+. 1.0;)D#BA^#&!$.0>*#&1%Z1;@<9+!21()#&!=.0,+#&%21.0341#w1()#A)A)!$;A+!$.041#7%$.0&#;<_#&)%$#&>?)D0#&b
.039)!$#du%$();  %=1()#r'&;>*A^;%$. 1#d#&!$!$;!!$41#r;$,+#&!KD0D;<O;9)!5)41u%G#B1%<N;!+#&9)!$DZ)#B1  ;!Gt6)5
5)#&'&.0%$.0;d1!$#&#K#&)%G#&>?)D0#&%q9)%G.0)3r9)Ar1;IQ{{I'&D0%$%$.0F)#&!$%Gb  9+!#BA^#&!$.0>*#&1%.0)5+.0'&41#=1(+41C>r;%21K;<
1()#>*#B1(+;J5)%A+!$;J5+9)'&#q%$.0>*.0D0!$D 6%$()A^#&5I'&9)!2,+#&%GbW%=#B^A^#&'B1#&5)P>9)'
(m;<)1(+#!$#&5)9)'B1.0;m.0/#&!G!$;!
5)9)#1;d5)5).0)3r'&D0%$%$.0F)#&!$%Z1;rm#&+%$#&>?+D0#7'&;>*#&%  . 1(K1()#7F)!$%217<N#  '&D0%$%G.0F)#&!$%$Us();  #B,+#&!$P+1()#&!$#
.0%W%$;>r#O,!G.041.0;  . 1(u!$#&%$A^#&'B11;  ()#&!G#1()#}#&!$!$;!=!$#&5)9)'B1.0;mF+)D0D %o^>*A1;41#&%$b
);!?^;41(6MO33.0)3)56MO;J;%o1.0)3A+A)D0.0#&5I1;u+#&9)!$DZ)#B1  ;!GtJ%$P>9)'G(;<1(+#*!$#&5)9)'B1.0;.0
#&!$!$;!A+A#&!G%H1;I()$,+#*;:'&'&9)!$!$#&5<e1#&!1#&@1;/F)<1#&#&'&D0%G%$.0F)#&!$%$bd%$.0>r.0D0!'&;)'&D09)%$.0;'&6?^#
!$#&'
()#&5*<N;!_M33.0+3})5C5)#&'&.0%$.0;1!$#&#&%$P  (+.0'G(C.0%_'&;)%$.0%o1#& 1  . 1(CM!G#&.0>*uLNQSSTa-b=MO91=w5)4E
?^;J;%o1.0)3u)5!G'&.0)3'&;1.0:9)#}1;>*#&%$9+!$?)D ].0>*A)!G;-,+#71()#&.0!W1#&%o1-EG%$#B1m#&!$!$;!9) 1.0D!$;9))5`
'&D0%$%$.0F+#&!$%=<N;!W5)#&'&.0%$.0;*1!$#&#&%$bWR1`*'&D0%$%$.0F)#&!G%_1()##&!$!G;!W!$#&5)9)'B1.0;u<;!?;41(/>r#B1();J5+%WA)A^#&!$%
1;()-,+#w)#&!$D u%2>*A1;41#&51;A)D041#&9+bcW()#&!$#&<N;!$#&P1()#=!$#&%G9)D 1%!G#&A;!o1#&5r.071().0%HA)A^#&!!$#;<
@#&)%$#&>?)D0#%G.0&#w;<`ILN.0b0#&b0P%$9)*'&.0#&1+#B1q>*+3#&?)D0#q%$.0&#q<;!8J9+D0. 141. ,+#)D ^%G.0%$a-bW 1  %
1!$5). 1.0;+D0D ?^#&D0.0#B,+#&5LN)!G#&9))5*xy)'G()A+.0!$#&PQSSTa1()41q%$>*D0D+!G#&5)9)'B1.0;)%.0q1#&%21-EG%$#B1}#&!$!$;!H>*-
'&;1.0J9)#.0+5)#&F)). 1#&D d<N;!?^;J;%21.0+3U();  #B,+#&!$P+7!$;-,+#O)5Ky)'
(J9)9+!$>*)%LNQSSXaZ5)#&>*;+%21!$41#1()41
w5)4EG?^;J;%21.0+3'&.0)5)#&#&5?^#&3.0u1;;$,+#&!$F1  . 1
( :f$ uD0!$3#I#&)%$#&>?)D0#I%$.0&#&%mLNQ{P0{{{6;!K>*;!$#
>*#&>?^#&!$%$a-b

q+r

 

jgdSoqh



c y


R:goq

i 
 R

w%%$9)33#&%21#&5m?^;$,+#&P. 17A)A^#&!$%R1()41O1()#A^#&!$<N;!$>*)'&#w;<R>* ;<)1(+#w#&+%$#&>?+D0#>*#B1();:5)%!$#
().03()D '&;!$!$#&D041#&5  . 1(;)#C);41()#&!GbdcR;/()#&D0A.05)#&1.0<1()#&%$#C'&;+%$.0%21#&)'&.0#&%$Pcs?+D0#C[/A)!$#&%$#&1%
1()#W'&;!G!$#&D041.0;d'&;:#&*'&.0#&1%;<J1()#=A#&!G<;!$>r)'&#;<^D0D)%$#B,+#&@#&)%$#&>?)D0#W>*#B1();:5)%$b);!#&'G(@5)41
%$#B1PA#&!G<;!$>r)'&#.0%Z>*#&%$9+!$#&5*%1(+#W#&)%$#&>?)D0##&!$!$;!H!$41#=5). ,.05)#&5C?K1()#%$.0)3D0#BEG'&D0%$%G.0F)#&!#&!G!$;!

Z

 *

fi

stv{wz{y{t

0.18

DT-Ada
DT-Arc
DT-Bag
NN-Ada
NN-Arc
NN-Bag

Composite Error Rate

0.16

0.14

0.12

0.10
0

10

20

30

40

50

60

70

80

90

100

Number of Networks in Ensemble

.039)!$#}7Z,+#&!$3#@1#&%o1-EG%$#B1#&!$!$;!I;-,+#&!/D0Dq`[5)41%$#B1%I9)%$#&5.0;9)!I%219)5).0#&%I<N;!I#&)%$#&>?)D0#&%
.0)'&;!$A^;!$41.0)3@<!G;>;+#=1;@Q{{/5)#&'&.0%$.0;@1!G#&#&%w;!q)#&9+!$Ds+#B1  ;!$tJ%GbCcW()#7#&!G!$;!q!$41#
3!$A)()#&5}.0%s%$.0>*A)D q1()#H-,+#&!$3#;<fi1()#H#&!$!G;!!$41#&%R;<1(+#Z`[=5)41=%$#B1%$bcW()#ZD 1#&!$+41. ,+#
;<_-,+#&!$3.0)31(+#7#&!$!$;!w;-,+#&!qD0DR5)41@A;.01%CLN.0b0#&b0P  #&.03(1.0)3Ir5)41@%$#B10%q#&!$!$;!!$41#
? . 1%=%$>rA)D0#}%$.0&#&aWA+!$;J5+9)'&#&%%$.0>*.0D0!$D 6%$(+A#&5/'&9+!2,+#&%$b

!$41#&b}c=(J9)%=r().03('&;!$!$#&D041.0;gLN.0b0#&b0P;)#7+#&!=Qb0{aq%$9)33#&%21%H1()411  ;d>*#B1(+;J5)%!$#'&;)%$.0%21#&1
.0K1()#75);>*.0)%W.0  ().0'G(C1()#B(+-,+#1(+#}3!$#&41#&%21K.0>*A)'B1;K1#&%o1-EG%$#B1#&!$!G;!W!$#&5)9)'B1.0;+b
cR?)D0#m[6A+!$;-,.05)#&%r:9)>*#&!G;9)%*.01#&!$#&%21.0)3.0)%$.03(1%$bcW()#uF)!$%21m.0%1(+41*1()#m)#&9+!$D EG)#B1  ;!$t
#&)%$#&>?)D0#>*#B1(+;J5)%=!$#%21!$;)3D '&;!$!$#&D041#&5  . 1(/;)#+;41()#&!W)5K1(+#5)#&'&.0%$.0;Eo1!$#&#7#&)%$#&>?)D0#
>*#B1();:5)%*!G#m%21!$;)3D "'&;!$!$#&D041#&5  . 1(g;+#m);41()#&!$U7();  #B,+#&!$P1()#&!$#m.0%rD0#&%$%r'&;!$!G#&D041.0;?^#BE
1  #&#&)#&9)!$D EG)#B1  ;!Gt#&+%$#&>?+D0#C>*#B1();:5)5 5)#&'&.0%$.0;Eo1!$#&#r#&)%G#&>?)D0#C>r#B1();J5+bdw;41
%$9)!GA)!$.0%$.0)3D P5+4EG?;:;%21.0)3)5@!G'&.0)3!G#%21!G;)3D '&;!$!$#&D041#&5+P#B,+#&@'&!G;%$%5).0#&!$#& 1'&;>*A^;4E
)#&1D0#&!$+.0)3rD03;!$. 1(+>*%$bcW().0%=%$9)33#&%21%H1()41MO;:;%21.0)30%q#&#&'B1. ,+#&)#&%$%5)#&A^#&)5)%=>*;!$#};r1()#
5)41%$#B11()  ()#B1()#&!1(+#d'&;>*A^;)#&1@D0#&!G).0)3D03;!$. 1()>.0%)#&9)!GDH)#B1  ;!$t;!5)#&'&.0%G.0;
1!$#&#&bdM33.0+3m;@1()#K;41()#&!}()+5)P.0%);41*'&;!$!$#&D041#&5'&!G;%$%'&;>*A^;)#&1CD0#&!G).0)3/D03;!$. 1()>r%$b
cW()#&%G#W!$#&%$9)D 1%!$#='&;+%$.0%21#&1  . 1(d;9)!HD041#&!'&D0.0>1(+41  ().0D0#=M;:;%21.0)3.0%HA^;  #&!$<9+D#&)%$#&>?)D0#
>*#B1();:5)P. 1.0%=>*;!$#}%$9+%$'&#&A1.0?)D0#O1;rC);.0%265)41*%$#B1=1()mMO33.0)3b

Z

 *

fi

y).0>*A+D0#BEG
MO33.0)34EGw
w!$'&.0)34EGw
w5)4EG
MO33.0)34EGqc
w!$'&.0)34EGqc
w5)4EGqc

y).0>rA)D0#
Qb0{{
{b0XX
{b0X
{b0X
EG{b0Q{
{b0[X
{b0[

r+su}SmQz
r

w#&9)!$Dw#B1  ;!$t
MO33.0)3
 !$'&.0)3
w
{b0XX
{ b0X

Qb0{{
{ b0X

{b0X
Q b0{{

{b0X
{ b0SS

EG{b0QQ
{ b0Q

{b0[
{ b0TQ

{b0[
{ b0T{


vr++z

5)
{b0X
{b0X
{b0SS
Qb0{{
{b0Q
{b0T`
{b0T[

MO33.0)3
EG{b0Q{
EG{b0QQ
{b0Q
{b0Q
Qb0{{
{b0TX
{b0TS

w#&'&.0%G.0;cR!$#&#
!$'&.0+3
{b0[X
{b0[
{b0TQ
{b0T`
{b0TX
Qb0{{
{b0ST

w5)
{b0[
{b0[
{b0T{
{b0T[
{b0TS
{b0ST
Qb0{{

cR?)D0#}[}Y#&!$<N;!$>*)'&#'&;!$!$#&D041.0;'&;J#&*'&.0#&1%/'&!$;%$%I#&)%G#&>?)D0#D0#&!$+.0)3>r#B1();J5+%$bY#&!$<N;!2E
>*+'&#.0%>*#&%$9)!G#&5r?C1()#=!$41.0;K;<:1()##&)%$#&>?)D0#w>*#B1();:5)0%1#&%21-EG%G#B1#&!G!$;!5+. ,^.05)#&5d? 
1()#7%$.0)3D0#}'&;>*A^;)#&17'&D0%$%$.0F+#&!$0%_1#&%21-EG%G#B1K#&!$!G;!$b

q+r

Gusnh y

d)ShsGf+gj

l2

  R  
y 

sgo

2:

.039)!$#T6%$();  %1()#uM33.0+3)5y).0>*A)D0#u)#B1  ;!$t#&)%$#&>?)D0#m!G#&%$9)D 1%r<N!$;>cs?)D0#`bc=()#&%$#
!$#&%$9+D 1%*.0)5).0'&41#*1()41u;<1#&6y).0>*A+D0#/H)%$#&>?)D0#/A)A+!$;'G(  .0D0DWA)!$;:5)9)'&#I!$#&%G9)D 1%q1()41!G#m%
'&'&9)!$41#@%M33.0+3LN'&;!$!G#&D041.0;!$#&%$9)D 1%<!$;>cs?+D0#r[uD0%G;u%$9)A+A;!o11().0%7%2141#&>r#& 1a-bcW().0%
%$9)33#&%o1%1(+41q>*#&'
()).0%$>  ().0'
(d'&9)%G#&%D0#&!$).0+37>*#B1();:571;A)!G;J5)9+'&#W%$;>*#w!$)5);>r)#&%$%
.0d1()#<;!$>r41.0;;<H. 1%q'&D0%$%$.0F)#&!G%'&?#9)%$#&5d1;I<;!G>'&'&9)!G41#C#&)%$#&>?)D0#&%$P_)5.0)5)#&#&5+PswD0.
)5mY&&).OLNQSSTa(+-,+#}5)#&>r;)%21!$41#&5%$.0>*.0D0!=!G#&%$9)D 1%W<N;!W!$)5+;>*.0&#&5m5)#&'&.0%$.0;*1!$#&#&%$b

q+rt I udS  o  4f+gfiGuRc/gh2qh 


s

jJ:

w);41()#&!W.01#&!$#&%21.0)3d8J9+#&%21.0;u.0%W(+;  #&#&'B1. ,+#W1(+#}5).0#&!$#& 1>*#B1();:5)%!$#}<N;!W)#&9)!GD)#B1  ;!$t:%
)5*5+#&'&.0%$.0;}1!G#&#&%$b.039)!$#&%HP+XP+)5*S'&;>*A)!$#_1()#W#&!$!$;!!$41#&%H+5r!$#&5)9+'B1.0;*.0r#&!$!G;!,D09)#&%
<N;!w5)4EGMO;J;%o1.0)3P!$'&.0+3PR+5uMO33.0)3I!$#&%$A^#&'B1. ,+#&D b;41#=1()41  #3!$A)(#&!$!$;!w!$41#!G41()#&!
1()*A^#&!$'&#&1=!$#&5)9+'B1.0;*.0*#&!$!$;!H!$41#W?^#&'&9)%$#R1()#?+%$#&D0.0)#W<N;!Z#&'G(d>*#B1();:5uLN5)#&'&.0%$.0;}1!$#&#&%H<N;!
w5)4EGMO;J;%21.0+3;5)#&'&.0%$.0;1!G#&#&%),+#&!$%G9)%)#&9)!GD)#B1  ;!$t:%s<N;!w5)4EGMO;:;%21.0)3=;)#&9)!$D)#B1  ;!$t:%$a
>*$A)!21.0D0D #BA)D0.0C1(+#75).0#&!$#&)'&#&%w.0uA^#&!$'&#&1!$#&5)9+'B1.0;)bq);!#B>*A)D0#&PR.0C1(+#7A)!$;>r;41#&!$%2E
S[TmA)!G;?)D0#&>9)%$.0)3/w5)4EGMO;:;%21.0)3P1()#C>9+'G(6D0!$3#&!7!$#&5)9)'B1.0;6.0#&!$!$;!<N;!O1()#C5)#&'&.0%$.0;/1!$#&#
A)A)!G;'G(>*-6?^#}5)9)#1;1()#}<N'B1=1(+415)#&'&.0%$.0;r1!$#&#&%w5);*);41%$#&#&>1;d?^#}%=#&#&'B1. ,+#K<;!Z1().0%
A)!$;?+D0#&>*P_)5w5)4EGMO;J;%21.0+3K1()#&!$#&<N;!$#CA)!$;:5)9)'&#&%q/D0!$3#&!7A#&!G'&#& 1*!$#&5)9+'B1.0;.0I1()#K#&!$!$;!<N;!
5)#&'&.0%$.0;*1!$#&#&%$b
cW(+#!$#&%G9)D 1%I%$();  1(+41.0>*'&%G#&%/.0<}%$.0)3D0#5)#&'&.0%$.0;1!$#&#(+5D0;  #&!LN;!/(+.03()#&!$a
#&!$!$;!H1()d%$.0+3D0#7)#&9)!GD)#B1  ;!$tu;d5)41d%G#B1P)1()#&*1()#5)#&'&.0%$.0;Eo1!$#&##&)%$#&>?)D0#>*#B1();:5)%
D0%$;@()5D0;  #&!CLN;!q().03(+#&!$a=#&!$!G;!Z1()r1()#&.0!w)#&9)!$Ds)#B1  ;!$tm'&;9+ 1#&!$A+!21b7cW(+#7#B'&#&A1.0;)%1;
1().0%*!G9)D0#/3#&)#&!$D0D ()A)A^#&)#&5;1(+#/%$>*#/5)416%G#B1<;!CD0D_1()!$#&#/#&)%$#&>?)D0#m>r#B1();J5+%uLN#&b03b0P
()#&A)41. 1.0%GPZ%$;-?^#&)P%$41#&D0D0. 1#&P'&!$#&5). 1-EGP_)5()#&!21-EG'&D0#B,+#&D0+5)a-bdc=()#&%$#K!$#&%$9)D 1%q%$9+33#&%211()41LNa
1()#A#&!G<;!$>r)'&#};<1()##&)%$#&>?)D0#>r#B1();J5+%=.0%w5)#&A^#&)5)#&17;?^;41(C1(+#75)41@%$#B1+5u'&D0%$%$.0F+#&!
>*#B1();:5)P)5]LN?)aq#&)%G#&>?)D0#&%'&+P_41rD0#&%21r.0%$;>*#K'&%$#&%$PH;-,+#&!$'&;>*#1()#K.0)5)9)'B1. ,+#C?).0%q;<. 1%
'&;>*A^;)#&1D0#&!$).0+3CD03;!$. 1()>*b

Z


fi

stv{wz{y{t

kr-vs-kp
letter
labor
soybean
promoters-936
segmentation
satellite
splice
vehicle
house-votes-84
glass
credit-g
hepatitis
ribosome-bind
heart-cleveland
iris
credit-a
Bagging

ionosphere

Simple
diabetes
hypo
sick
breast-cancer-w
sonar
-20

0

20

40

60

80

Percent Reduction in Error

.039)!$#}T7=#&5)9)'B1.0;.0#&!G!$;!7<N;!7MO33.0)3)5y+.0>*A)D0#*)#&9+!$DZ)#B1  ;!$t#&+%$#&>?+D0#&%%uA^#&!2E
'&#& 13#K;<1()#;!$.03.0)D#&!G!$;!=!$41#&b7D0%$;d%$();  L  (+. 1#7A^;!21.0;u;<_#&'
(?)!$aW.0%w;)#
%21)5)!$5u5)#B,^.041.0;<;!_1()#&%$#}!$#&%G9)D 1%$b

ZZp


fi

r+su}SmQz
r

vr++z

kr-vs-kp
letter
segmentation
labor
soybean
satellite
sick
sonar
vehicle
glass
ionosphere
promoters-936
ribosome-bind
iris
splice
credit-g
diabetes
Neural Network

hypo

Decision Tree
hepatitis
credit-a
house-votes-84
heart-cleveland
breast-cancer-w
0

10

20

30

40

Error (%)

.039)!$#}7H!$!$;!=!$41#&%<N;!=w5)4EGMO;:;%21.0)3@#&)%$#&>?)D0#&%$bqcW()#  (+. 1#7A^;!21.0;u%G();  %Z1()#}!$#&5+9)'B1.0;
.06#&!$!$;!;<5+4EGM;:;%21.0)3'&;>*A)!$#&5/1;//%$.0)3D0#*'&D0%$%G.0F)#&!  ().0D0#C.0+'&!$#&%$#&%7.0#&!G!$;!
!$#%$();  *.0C?)D0'
tJbcW(+#5)41}%$#B1%Z!G#%$;!21#&5r?71(+#W!$41.0;};<!$#&5)9)'B1.0;r.0C#&)%$#&>?)D0#
#&!$!$;!1;*;$,+#&!$D0DR#&!$!$;!W<N;!W)#&9)!GD)#B1  ;!$t:%$b

Zff


fi

stv{wz{y{t

kr-vs-kp
letter
labor
segmentation
soybean
satellite
vehicle
sonar
ionosphere
sick
glass
promoters-936
iris
splice
ribosome-bind
credit-g
hepatitis
Neural Network

hypo

Decision Tree
diabetes
house-votes-84
credit-a
heart-cleveland
breast-cancer-w
0

10

20

30

40

Error (%)

.039)!$#}X7H!$!$;!!$41#&%O<;!!G'&.0)37#&)%G#&>?)D0#&%GbcW()#  (). 1#=A^;!21.0;d%$();  %1()#!$#&5)9)'B1.0;@.0r#&!G!$;!
;<w!$'&.0)36'&;>rA)!$#&51;6%$.0)3D0#m'&D0%$%G.0F)#&!  (+.0D0#/.0)'&!$#&%$#&%d.0g#&!$!$;!*!G#m%$();  .0
?)D0'Gt:bcW()#d5)41%$#B1%K!$#I%$;!21#&5? 1()#d!G41.0;;<=!$#&5)9)'B1.0;.0#&)%$#&>?)D0#I#&!$!$;!=1;
;-,+#&!$D0DR#&!$!$;!W<N;!W)#&9+!$D)#B1  ;!$t:%$b

Z

 *

fi

r+su}SmQz
r

vr++z

kr-vs-kp
letter
labor
soybean
promoters-936
segmentation
satellite
splice
vehicle
house-votes-84
glass
credit-g
hepatitis
ribosome-bind
heart-cleveland
iris
credit-a
Neural Network

ionosphere

Decision Tree
diabetes
hypo
sick
breast-cancer-w
sonar
0

10

20

30

40

Error (%)

.039)!$#}S7H!$!$;!C!G41#&%r<N;!*MO33.0)3#&)%G#&>?)D0#&%Gb"cW()#  (). 1#/A^;!21.0;]%$();  %q1()#/!$#&5)9+'B1.0;g.0
#&!$!$;!;<M33.0+3'&;>rA)!$#&51;7%G.0)3D0#'&D0%G%$.0F)#&!  ().0D0#W.0+'&!$#&%$#&%O.0d#&!G!$;!H!$#w%$();  
.0u?)D0'
tJbcW(+#}5)41r%G#B1%=!$#%$;!21#&5u?/1()#}!G41.0;d;<!$#&5)9+'B1.0;u.0u#&+%$#&>?+D0#7#&!$!$;!Z1;
;-,+#&!$D0DR#&!$!$;!W<N;!W)#&9+!$D)#B1  ;!$t:%$b

Z


fi

q+rK

*oqhsGdi



h2

stv{wz{y{t



)!$#&9+)5g)5gy+()A).0!$#6LNQSSTa@%$9)33#&%21#&561()41r1()#/%G;>*#B1.0>*#&%@A;:;!CA^#&!$<N;!$>*)'&#/;<qMO;J;%o1.0)3
!$#&%$9+D 1%}<!G;> ;-,+#&!$F1-1.0)3C1()#1!$.0).0)3/%$#B1@%$.0)'&#*D041#&!W1!$.0).0)3m%G#B1%}>*-g?^#C;-,+#&!oEG#&>*A)()%$.0&.0+3
#B>*A)D0#&%w1()41m!$#@);.0%$#6Le1(:9)%K'&!$#&41.0)3#BJ1!G#&>*#&D A^;:;!'&D0%G%$.0F)#&!$%$a-bc=().0%!G39)>*#&1/%G#&#&>*%
#&%$A^#&'&.0D0D A^#&!21.0+#& 1@1;M;:;%21.0)3<N;!1  ;!$#&%$;)%$bcW(+#F)!G%21)5>*;%o1;?,^.0;9+%/!$#&%$;.0%
1()41W1(+#&.0!W>*#B1();:5m<N;!W9)A^5)41.0)31()#A)!G;?)?).0D0. 1.0#&%=>r-6?^#;-,+#&!2EG#&>*A+()%$.0&.0)3d);.0%2#B^>*A+D0#&%$b
cW()#u%$#&'&;)5!$#&%$;.0%}1()41d1()#u'&D0%G%$.0F)#&!$%@!$#u'&;>?).0)#&59)%$.0+3  #&.03( 1#&5,+;41.0)3bY!$#B,.0;9)%
 ;!$tLNy);D0D0.0'
(@x|}!$;3(+PQSSTa()%%$();  71()41q;A1.0>*.0&.0)3=1()#'&;>?).0).0+3  #&.03( 1%'&@D0#&51;
;-,+#&!GF1-1.0)3  ().0D0#Wd9)  #&.03(1#&5,+;41.0)3%$'G(+#&>*#=.0%3#&)#&!$D0D u!$#&%$.0D0.0#&1H1;;-,+#&!$F1-1.0)3bW)!$.0#&5+>*
#B1D0b@LNQSSXaw(A;41(+#&%$.0&#1()41CMO;J;%o1.0)3d>r#B1();J5+%$P%w5)5). 1. ,+#>*;J5+#&D0%$P>*$%$#&#.0)'&!$#&%$#&%w.0
#&!$!$;!.01(+;%$#q%$. 19)41.0;)%  ()#&!$#1()#?).0%;<)1()#?)%$#q'&D0%$%$.0F+#&!W.0%A)A)!$;A+!$.041#q<;!R1()#A+!$;?)D0#&>
?^#&.0)37D0#&!G)#&5)bR\]#H1#&%21O1().0%(A;41(+#&%$.0%.0@;9)!%$#&'&;)5@%$#B1;<!$#&%$9)D 1%A)!$#&%G#& 1#&5@.0}1(+.0%%G#&'B1.0;)b
cR;q#B,D09)41#_1()#O( A^;41()#&%$.0%1()41WMO;J;%o1.0)3>*-/?^#A+!$;)#1;;-,+#&!$F1-1.0+3  #A^#&!$<N;!$>*#&5q%$#B1
;<#B^A^#&!$.0>r#& 1%9)%$.0)3q1(+#}<;9+!=#&)%$#&>?)D0#}+#&9)!$D)#B1  ;!Gt/>*#B1();:5)%$b_\]#.01!$;J5+9)'&#&5m*PsQ{*P
`{*P)56[{ );.0%$#GK.01;u<N;9)!}5).0#&!$#&1d5)41%$#B1%$buR1I#&'G(D0#B,+#&D  #r'&!$#&41#&5F,+#*5).0#&!$#&1
);.0%25)41m%G#B1%$PHA^#&!$<N;!$>*#&5mQ{4EG<N;D05'&!G;%$%,D0.05)41.0;;#&'G()P1()#&6$,+#&!$3#&5;-,+#&!1(+#*F,+#
!$#&%$9+D 1%$b/.039)!G#}Q{  #%G();  1(+#!$#&5)9)'B1.0;m.0/#&!G!$;!W!$41#<N;!W#&'G(u;<)1()##&+%$#&>?+D0#}>*#B1();:5)%
'&;>*A)!G#&51;9)%$.0)3%G.0)3D0#/)#&9)!$D+#B1  ;!$t'&D0%G%$.0F)#&!$b"cW(+#&%$#I!$#&%$9)D 1%*5+#&>*;)%21!$41#1(+41%
1()#}+;.0%$#}D0#B,+#&D3!G;  %$P)1()#}#&*'&'B;<+1()#}y+.0>*A)D0#})5uM33.0+3r#&)%$#&>?)D0#&%3#&)#&!$D0D .0)'&!$#&%G#&%
 ().0D0#w1(+#C!$'&.0+3/)55+4EGM;:;%21.0)3u#&)%$#&>?)D0#&%}3.0)%}.0A#&!G<;!$>r)'&#K!$#C>9+'G(%$>*D0D0#&!ILN;!
>*$'B19)D0D 5)#&'&!$#&%$#&a-b;41#@1(+41@1(+.0%@#&#&'B1.0%/>*;!$##B:1!$#&>*#<N;!@w5)4EGMO;:;%21.0)3  ().0'
(
%$9)A+A;!o1%;9)!}(^A^;41()#&%$.0%1()41dw5)4EGMO;J;%21.0+3u.0%7>r;!$#C#&'B1#&5?+;.0%$#&bmcW().0%7%$9)33#&%21%W1()41
MO;J;%o1.0)30%A^;J;!A^#&!$<;!G>*)'&#w<;!'&#&!21.0/5)41K%$#B1%O>*$?^#=A)!o1.0D0D #BA)D0.0)#&5I? ;-,+#&!$F1-1.0+3
);.0%$#&b
cR;q<9)!o1()#&!s5+#&>*;)%21!$41#s1()#O#&#&'B1=;<));.0%$#O;KMO;J;%o1.0)3  #O'&!$#&41#&5C%G#B,+#&!$D%$#B1%_;<)!21.0F+'&.0D
5)41%$A^#&'&.0F)'&D0D 5)#&%$.03)#&51;>*.0%GD0#&5gMO;J;%21.0+3>*#B1();:5)%$b);!C#&'
(g5)41%$#B1  #/'&!$#&41#&5g
%$.0>*A+D0#=(^A^#&!$A)D0+#='&;)'&#&A1?)%$#&5@;@%$#B1;<:1()#w<#&419+!$#&%qLN+5dD0%$;.0)'&D09)5+#&5d%$;>r#.0!$!G#&D0#B,1
<N#&419)!$#&%$a-b%$#B1I;<!$)5);> A^;.0 1%  #&!$#71()#&3#&)#&!$41#&5g+5D0?^#&D0#&5?)%$#&5;  ().0'
(%$.05)#
;<H1()#/( A^#&!$A)D0)#1(+#B<N#&D0D0b"cW()#&6'&#&!o1.0gA^#&!$'&#& 13#;<Z1(+#/A;.01%*;g;+#m%$.05)#/;<H1()#
(^A^#&!$A)D0+#  #&!G#d>*.0%GD0?#&D0#&5%K?^#&.0)3mA)!21I;<1()#d;41()#&!'&D0%G%$b);!1()#d#B^A^#&!$.0>r#& 1%K%$();  
?^#&D0;  #}3#&)#&!G41#&5uF,+#}5+41*%$#B1%  ()#&!$#1()#}'&;)'&#&A1  %=?)%$#&5m;*1  ;*D0.0)#&!=<N#&419)!$#&%GP()5
<N;9)!W.0!$!$#&D0#B,1<N#&419)!$#&%$Ps)5m`{;<1()#75)41  %=>*.0%GD0?#&D0#&5+b_\]#=1!$.0)#&5mF,+##&)%$#&>?)D0#&%=;<
)#&9)!GDZ)#B1  ;!$t:%ILNA^#&!$'&#&A1!$;)%$a}<N;!7#&'
(5+41u%$#B1I)5$,+#&!$3#&51()#r#&)%G#&>?)D0#&%GHA)!$#&5+.0'B1.0;)%$b
cW(:9)%s1()#&%$##BA^#&!$.0>*#&1%.0 ,+;D ,+#}D0#&!G).0)3C.0/%$. 19)41.0;+%  ()#&!$#H1()#;!$.03.0+D?).0%;<1()#D0#&!G)#&!
LN%$.0)3D0#( A^#&!$A)D0)#uA)!$;:5)9)'&#&5]?A^#&!$'&#&A1!$;)ad.0%IA)A)!$;A)!G.041#u<N;!71(+#uA)!$;?+D0#&>*P})5%
)!$.0#&5+>*I#B1D0bLNQSSXa%$9)33#&%21P9)%G.0)37I5)5). 1. ,+#q>*;:5)#&D>*-()!$>A^#&!$<;!G>*)'&#&b.039)!$#wQQ
%$();  %1(+#r!$#&%$9+D 1.0)3u#&!$!G;!7!$41#&%K<N;!7w5)4EGMO;J;%21.0+3P!$'&.0+3P+5MO33.0)3? 1()#dJ9+>?^#&!7;<
)#B1  ;!$t:%H?^#&.0)3q'&;>?).0)#&5*.0w1(+##&)%$#&>?)D0#&bcW()#&%$#O!$#&%G9)D 1%Z.0)5).0'&41#='&D0#&!$D K1()41=.0*'&%$#&%  ()#&!$#
1()#&!$#.0%q);.0%$#7MO33.0)30%7#&!$!$;!=!G41#  .0D0DR);41K.0)'&!$#&%$#%1()#7#&)%G#&>?)D0#%$.0&#.0)'&!$#&%$#&%  (+#&!$#&%
1()##&!$!$;!@!$41#;<1(+#uMO;J;%o1.0)3>r#B1();J5+%r>*$.0)5)#&#&5.0)'&!$#&%$#%I#&)%$#&>?)D0#u%$.0&#.0)'&!$#&%G#&%$b

56V=\*=3V4h=3! #$&#&$#!ng2$#94#=3V4=\V4B- `G#,>13 ;Fm)+A=\V4>94l#V4hg94>94l2 #9 ;Fm#h0-6
!n#V4! [8)+ =3VBA#Vh4,-13?@>+$9)+ hW#V4$%2 #9CD#1\94C%4#d2 #9U/2%!$VG=3VZ9494%2 #9 ;F
 k>+=\)413[ kD;#139$&X#o!n$V@)G?A$`Z#,-=3V=3VB#131I%#=3V=3VB-$`Z#,->13 ;:
ZZ


fi

diabetes

4

Reduction in error rate (% pts)

r+su}SmQz
r

soybean-large

9

3
6
2
3
1

0

Bagging Ensemble
Boosting (Arcing) Ensemble
Boosting (Ada) Ensemble

0
0

5

10

15

20

25

30

0

promoters-936

4

Reduction in error rate (% pts)

vr++z

5

3

2

2

1

1

0

15

20

25

30

25

30

segmentation

4

3

10

0
0

5

10

15

20

25

30

0

Noise rate (%)

5

10

15

20

Noise rate (%)

.039)!$#}Q{7y).0>*A)D0#&P_MO33.0)3P)5MO;J;%21.0+3LN!$'&.0+3/)55)a+#&9)!$DR)#B1  ;!$t#&)%$#&>?)D0#C!$#BE
5)9)'B1.0;.0#&!$!$;!q%q'&;>*A)!$#&5d1;@9)%$.0)3@d%G.0)3D0#K)#&9)!$Ds)#B1  ;!$t:bC}!$A)()#&5.0%1()#
A#&!G'&#& 13A
# lNh z !$#&5+9)'B1.0;m.0/#&!$!$;!LN#&b03b0P<N;!W);.0%$#7.0K1()#%$#&3>*#&141.0;5)41
%$#B1P.0<s1()#r%$.0)3D0#*)#B1  ;!Gt>r#B1();J5()56#&!G!$;!}!$41#d;<Qb0S )5I1()#rM33.0+3
>*#B1();:5()5#&!$!$;!C!$41#/;<wQb0*PH1()#&1().0%*.0%C3!GA)()#&5%CQb0`6A^#&!$'&#&13#
A;.01!$#&5+9)'B1.0;m.0K1()#7#&!$!$;!W!$41#&a-b

w5)5). 1.0;)D1#&%o1%wLN);41q%$();  C()#&!$#&aZ%$(+; 
!$#&%21!o1.0)3*.0%=+;417#&>*A)D0;-+#&5+b

1(+415+4EGM;:;%21.0)30%#&!$!$;!H!$41#W?^#&'&;>*#&%  ;!$%$#  ()#&

cW(+.0%'&;+'&D09)%$.0;g5);$,+#B1.0D0%d+.0'&#&D   . 1(y)'
()A).0!$#I#B1uD0b00%LNQSSad!$#&'&#&1u5).0%$'&9)%$%G.0;  ()#&!$#
1()#B);41#*1()41*1()##&#&'B1. ,+#&+#&%$%/;<q/,+;41.0)3>*#B1();:5g'&?#u>*#&%$9)!$#&5? #B>*.0).0)3m1()#
k { $ |Nh)i=;<+1()##B>*A)D0#&%$bLNcW()#>*!$3.0u.0%R1()#5).0#&!$#&)'&#}?^#B1  #&#&K1()#:9)>?^#&!;<s'&;!$!G#&'B1)5
.0)'&;!$!G#&'B1r,+;41#&%I<;!r]#B>*A)D0#&b0a6%$.0>*A+D0#m!$#&%$>*A+D0.0)36>*#B1();:5]%G9)'G(%rMO33.0)3P}#&'
(
!$#&%$9+D 1.0)3}'&D0%$%$.0F+#&!H<N;J'&9)%G#&%H;r.0+'&!$#&%$.0)3O1(+#W>*!$3.0d<;!%H>*m;<1(+#W#B^>rA)D0#&%H%A;%G%$.0?)D0#&b
MO91/.0MO;:;%21.0)3>*#B1();:5)PD041#&!C'&D0%$%$.0F+#&!$%K<;:'&9)%K;.0)'&!$#&%$.0)3r1()#I>*!$3.0+%<N;!#B^>*A+D0#&%
 . 1(A;:;!W'&9)!G!$#& 1>*!$3.0)%$bww%=y)'
()A).0!$#}#B1KD0b*LNQSSa);41#&P)1().0%w.0%=q,+#&!2#&#&'B1. ,+#K%21!$41#&34
.0<1()#};$,+#&!$D0D'&'&9)!$'B;<1()#!$#&%$9)D 1.0+3r'&D0%$%$.0F+#&!5);:#&%=+;415)!$;Au%$.03).0F)'&1D b});!qrA+!$;?)D0#&>
 . 1(6+;.0%$#&PH<N;J'&9+%$.0)3I;6>*.0%$'&D0%G%$.0F)#&56#B>*A)D0#&%>*-g'&9)%$#C/'&D0%G%$.0F)#&!1;u<;:'&9)%;6?^;:;%21.0)3
1()#}>r!$3.0)%W;<LN);.0%2aW#B>*A)D0#&%_1(+41  ;9+D05m.0m<N'B1?^#q>*.0%$D0#&5).0)3r.0u;-,+#&!GD0D'&D0%$%$.0F+'&41.0;)b

Z

 *

fi

stv{wz{y{t

20

Error rate

18
Arc
Ada

16
14

Bag

12
10
0

5

10

15

20

25

30

20

Error rate

18
16
Arc
14

Ada

12

Bag

10
0

5

10

15

20

25

30

20

Error rate

18
16
Arc
14
Ada
12
Bag

10
0

5

10

15

20

25

30

20

Error rate

18
Arc
Ada

16
14

Bag
12
10
0

5

10

15

20

25

30

20

Error rate

18
16
Arc
14

Ada

12

Bag

10
0

5

10

15

20

25

30

Networks in Ensemble

.039)!$#}QQ7H!$!$;!w!$41#&%q?m1()#%$.0&#;<H#&)%$#&>?)D0#<N;!w5)4EGMO;:;%21.0)3P_w!$'&.0)3P)5MO33.0)3/#&E
%$#&>?+D0#&%=<N;!=F,+#5).0#&!$#&1!21.0F)'&.0Ds5)41*%$#B1%'&; 1.0+.0)3r;)#BEG%G.05)#&5u);.0%G#dLN%$#&#O1#B:1
<;!5)#&%$'&!$.0A1.0;+a-b

**

fi

j

r+su}SmQz
r

vr++z

Q

 )
 #7.0 1#&!$#&%o1.0)3d8:9)#&%21.0;  #A)D0C1;d.0 ,+#&%o1.0341#.0%w();  #&#&'B1. ,+#d%$.0)3D0#'&D0%$%$.0F)#&!wA)A)!$;'
(
>*.03(1*?#.0<H. 1  %D0D0;  #&5I1;/9)%$#=1()#1.0>*#K. 1q1t#&%O1()#K#&)%$#&>?)D0#K>*#B1();:5d1;1!$.0>9+D 1.0A)D0#
'&D0%$%$.0F+#&!$%1;#B^A)D0;!G#. 1%/'&;+'&#&A16%$A)'&#&b);!m#B>*A)D0#&P)#&9)!$Dw)#B1  ;!$tA)A)!$;'
("'&;9)D05
A^#&!$<N;!$>A).0D0;41%219)5+.0#&%9)%$.0)31()#1!$.0+.0)3C%$#B11;*%$#&D0#&'B1A)A)!$;A)!G.041#H,D09)#&%W;<RA)!$>r#B1#&!$%W%$9)'
(
%=(+.05)5)#&/9)). 1%GPD0#&!$).0)3C!$41#&Ps#B1'&b
\ #@A)D0/1;'&;>*A)!$#dMO33.0)3+5MO;J;%o1.0)3>r#B1();J5+%O1;u;41()#&!>*#B1();:5)%.0 1!$;:5)9)'&#&5!$#BE
]
'&#&1D bW@A)!21.0'&9+D0!  #=.01#&)51;#B^>*.0+#Z1()#9)%$#;<y1'Gt:.0)3/Le\];D0A^#&!21PQSS`aO%>*#B1();:5
;< z  { h+
h |I'&;>?+.0).0)3<9)+'B1.0;)P%G;%=1;u$,+;.051()#r#&#&'B1m;<()-,.0)3C1;  #&.03( 1/'&D0%$%$.0F)#&!G%$b
\]#}D0%$;rA)D01;*'&;>*A)!G#M33.0+3r)5/MO;:;%21.0)3q1;C;41()#&!=>*#B1();:5)%%$9)'
(m%  A). 1+5/y)()-,:E
D0.0t:0%CLNQSST?+aA)A)!$;'
(r1;I'&!$#&41.0+3@#&)%G#&>?)D0#&bKcW(+.0%A)A+!$;'G(9)%$#&%q3#&)#B1.0'K%$#&!G'G(@1;@F))5
'&D0%$%$.0F+#&!$%_1()41!$#'&'&9)!$41#)5m5).0#&!W.0K1()#&.0!A)!$#&5).0'B1.0;+%$b
.0)D0D P%G.0)'&#Z1(+#wMO;:;%21.0)3C>*#B1(+;J5)%!$#q#B:1!$#&>*#&D %$9)'&'&#&%$%$<N9)D.0I>* 5);>*.0)%GP  #qA)D0
1;.0,+#&%21.0341#);$,+#&DA+A)!$;'G(+#&%71(+41  .0D0D!$#B1.061(+#u?^#&)#&F1%d;<wMO;:;%21.0)3bc=()#m3;D  .0D0D
?^#1;'&!G#&41#mD0#&!$)#&!  (+#&!$#+;9'&g#&%$%$#&1.0D0D A)9)%$(%21!21u?)91-1;+5D0#B1u. 1u!$9))bcR;
5);*1().0%  #  ;9)D05u1!21;A)!G#&%$#&!2,+#q1()#d?^#&)#&F1%7;<MO;:;%21.0)3  ().0D0#rA+!$#B,+#& 1.0+3;$,+#&!$F1-1.0)3;
);.0%2g5)41%$#B1%$b  )#CA^;%$%G.0?)D0#*A)A)!G;'G(  ;9)D05?#1;u9)%$#rm();D05);9171!$.0).0)3%$#B1LNK19+).0)3
%$#B1as1;C#B,D09)41#1()#wA#&!G<;!$>r)'&#;<)1()#wM;:;%21.0)3C#&+%$#&>?+D0#H1;K5)#B1#&!G>*.0)#  ()#&71()#q'&'&9)!$'B
.0%7+;uD0;)3#&!.0)'&!$#&%$.0+3bw);41()#&!A)A)!$;'
(  ;9)D05?#1;u9)%$#rA).0D0;41@%219+5).0#&%1;5)#B1#&!$>*.0+#*
4;A1.0>*D0dJ9+>?^#&!;<'&D0%$%$.0F)#&!G%_1;*9)%G#.0mm#&)%$#&>?)D0#&b



 H 
 


g

 

M

)

w%>r#& 1.0;)#&5I?^#&<N;!$#&P1(+#=.05)#&K;<9)%$.0+37I#&)%$#&>?)D0#w;<'&D0%$%$.0F)#&!G%!G41()#&!1(+}1()#w%$.0)3D0#w?^#&%21
'&D0%$%$.0F+#&!s()%R?^#&#&7A)!G;A;%G#&5}? r%G#B,+#&!$DJA^#&;A)D0#&bB7y)#&'B1.0;`P  #A)!$#&%$#&1<!$>r#  ;!$t<N;!1(+#&%$#
%2%21#&>*%$P%$;>*#}1()#&;!$.0#&%7;<  (+41d>*t#&%7#&#&'B1. ,+#@#&)%$#&>?)D0#&P#B:1#&)%$. ,+#d'&;-,+#&!$.0)3;<s1()#
MO33.0)3})5KMO;:;%21.0)3D03;!$. 1()>r%$P+)5Kq5).0%$'&9+%$%$.0;K;w1()#O?).0%_A)D09)%,!$.0)'&#5)#&'&;>rA;%G. 1.0;)b
y)#&'B1.0;[!$#&<#&!G!$#&5m1;#&>*A).0!G.0'&D%o19)5).0#&%%$.0>*.0D0!1;;9+!$%$U1()#&%G#d>*#B1(+;J5)%5).0#&!<N!$;> ;9)!$%.0
1()41Z1(+#B  #&!$#D0.0>*. 1#&51;5)#&'&.0%$.0;1!G#&#&%$P3#&+#&!$D0D   . 1(*<N#  #&!5)417%G#B1%$bs\]#='&;-,+#&!O5)5+. 1.0;)D
!$#&D041#&5  ;!$tI.0K1(+.0%W%$#&'B1.0;)b

s.0)'&;D0r)5ry)tJ!GB^A^#&t/LNQSXSa-P).sLNQSSQaO)51()#W<N;!$#&'&%21.0)3D0. 1#&!$419)!$#KLNVWD0#&>*#&)PQSXSU
}!$+3#&!$P=QSXSa*.0+5).0'&41#1()41m%$.0>*A)D0#I$,+#&!$3.0)36;<Z1()#IA)!$#&5).0'B1;!$%C3#&+#&!$41#&%*@,+#&!23;J;:5
'&;>*A^;%$. 1#>*;J5+#&D0U+();  #B,+#&!$P>*mD041#&!!$#&%G#&!$'G()#&!G%wLNwD0A)-5).0)PQSS[U^%$t#&!x'&D0.0)PQSSP
QSS?)UMO!$#&.0>*+PQSST'&Uv%G()#&>*P^QSSU'&D0.0)PQSSXUY#&!$!$;)#&P^QSS`U\];D0A^#&!21P^QSS`k
U ())3P
#&%$.0!$;-,P^x\]D 1&P^QSS`aO()-,+#<9+!21()#&!Z.0>*A+!$;-,+#&5@3#&)#&!$D0.0&41.0;  . 1(},+;41.0)3%$'G()#&>r#&%1()41q!$#
'&;>*A)D0#Bg'&;>?).0)41.0;)%;<#&'
(A)!G#&5).0'B1;!$0%};91A+91b  )#C>9+%21r?^#'&!G#&<9)D_.0/1().0%'&%$#&P%$.0)'&#
;A1.0>*.0&.0+3W1()#='&;>?).0+.0)3  #&.03( 1%O'&@#&%$.0D uD0#&571;1()#A)!$;?)D0#&>z;<;-,+#&!$F1-1.0+3  (+.0'G(r%G.0>*A)D0#
-,+#&!G3.0)3d%G#&#&>*%1;r$,+;.05LNy);D0D0.0'
(x|}!$;3()PQSSTa-b
 ;%21A)A+!$;'G()#&%;)D mhd&&f; z n1!21;}3#&)#&!G41#=().03()D /'&;!$!$#&'B1w'&D0%$%$.0F)#&!$%1()415).0%$3!G#&#W%

>9)'G(@%A^;%$%G.0?)D0#&bOcW()#&%$#>*#B1();:5)%1!o1;'&!$#&41#q5). ,+#&!$%G#='&D0%$%$.0F+#&!$%O? C1!$.0+.0)37'&D0%$%G.0F)#&!$%  . 1(
5).0%$%G.0>*.0D0!=D0#&!$+.0)3rA)!G>*#B1#&!$%LNwD0A)$^5).0+PQSS[a-PR5).0#&!G#& 1K'&D0%$%$.0F)#&!w!$'G(+. 1#&'B19)!$#&%CLNv%$(+#&>*P
QSSa-P,!$.0;9+%_.0). 1.0D))#&9)!GD EG)#B1  ;!$t  #&.03(1=%$#B1-1.0)3%qLN'&D0.0*x  A). 1&P)QSSU^'&D0.0rx"y)()-,:E
D0.0t:PQSSa-P;!%$#&A)!$41#dA)!21. 1.0;)%;<R1()#q1!G.0).0)3u%G#B1LNMO!$#&.0>*)POQSSTUW|7!$;3(x~=#&5)#&D0%$?P
QSSa-b@M;:;%21.0)3/;d1()#K;41()#&!(+)5.0%'B1. ,+#r.0r1!o^.0)31;/3#&)#&!G41#C().03()D '&;!$!G#&'B1r)#B1  ;!$t:%
**

fi

stv{wz{y{t

%$.0)'&#@. 1m'&'&#& 19+41#&%*#B^>rA)D0#&%K'&9)!$!$#&1D '&D0%$%$.0F+#&5.0)'&;!$!$#&'B1D ? ]A)!G#B,^.0;9)%K>*#&>?^#&!$%;<1()#
#&)%$#&>?)D0#&b
iodgk L  A). 1uxy)()$,^D0.0t:PqQSSTPqQSST?)ad.0%d);41()#&!d#B^>*A+D0#u;<q]A)A+!$;'G(61()41
5).0!$#&'B1D 1!$.0#&%1;6'&!$#&41#m65+. ,+#&!$%$#/#&)%$#&>?)D0#&b iodgk 9)%$#&%C3#&)#B1.0'D03;!$. 1()>*%}1;%$#&!$'
(
#BA)D0.0'&. 1D <N;!7m().03(+D 5+. ,+#&!$%$#r%G#B1@;<O'&'&9)!$41#71!$.0)#&5)#B1  ;!$t:%$b jdogk  ;!$t:%?gF)!$%21
'&!$#&41.0)3.0). 1.0DA^;A)9)D041.0;+PO1()#&9)%$#&%I3#&)#B1.0';A^#&!$41;!$%1;'&!$#&41#)#  )#B1  ;!$t:%I'&;E
1.0:9)D0D Pt#&#&A+.0)31()#C%$#B1d;<)#B1  ;!$t:%O1()41@!$#C().03()D '&'&9)!$41#  ().0D0#C5).0%$3!$#&#&.0+3  . 1(6#&'
(
;41()#&!q%q>9+'G(%qA;%G%$.0?)D0#&b idogk .0%D0%$;I#&#&'B1. ,+#*41C.0)'&;!$A^;!$41.0)3@A)!$.0;!wtJ+;  D0#&5)3#&P_.0<
-,.0D0?)D0#&P+1;*.0>*A)!G;-,+#O1()#78J9)D0. 16;<R. 1%#&)%$#&>?)D0#&b
wD 1#&!$)41#/A)A+!$;'G(1;@1()#I#&)%G#&>?)D0#/<N!$>*#  ;!$t.0%w1;r1!G.0.0)5). ,.05)9)D)#B1  ;!$t:%*;
%$9)?1%$t:P)5m1;r1()#&'&;>?+.0)#}1(+#&%$#dA+!$#&5).0'B1.0;)%  . 1(4341.0)3<N9))'B1.0;u1()41/5)#&A^#&)5)%
;1()#.0+A)91b'&;?)%u#B1D0b00%LNQSSQa5)A1. ,+#>r. J19)!G#&%/;<D0;:'&D7#BA^#&!21%$PM4:10%LNQSS`a
>*#B1();:5g<N;!*.05)#&1.0<.0)36>=+;:'&!$5).0D=.0+<!$'B1.0;+Pw+5;  D0+5gy)#B-);  %GtJ.00%LNQSS`a7,.0%$9)D
>*;:5)#&DD0D1!$.0])#B1  ;!$t:%1;6D0#&!$]%$A^#&'&.0F)'/%$9)?1%GtJ%$bcW(+#/t#B".05+#&;<H1(+#&%$#1#&'G()+.08J9)#&%d.0%
1()415)#&'&;>rA;%G. 1.0;;<1()#A)!$;?+D0#&>.01;%$A^#&'&.0F)'u%$9)?1%$t:%@>*.03( 1D0#&51;>*;!$##&*'&.0#&1
!$#&A)!G#&%$#& 141.0;+%=)5K1!G.0).0)3LNvw>*A)%$(+.0!$#x\].0?^#&D0PQSXSa-b
 +'&#*mA)!$;?+D0#&> .0%7?)!G;t#&6.0 1;%$9)?1%GtJ%$P1()#*!$#&%G9)D 1.0)3/%$;D091.0;+%)#&#&5/1;m?^#C'&;>?).0+#&5)b
'&;?+%#B1ID0bqLNQSSQaKA)!$;A^;%$#r()-,.0)3C1(+#r341.0)3<9+)'B1.0;?^#*)#B1  ;!$t@1()41mn2f { 4h)iK();  1;
D0D0;:'&41#r#B>*A)D0#&%O1;1()#K#BA^#&!21%$b@cW(J9+%H1(+#341.0+3m)#B1  ;!$tD0D0;J'&41#&%#&'G(#B^>rA)D0#w1;/;)#
;!/>*;!$##BA^#&!21%$P7+51(+#?+'Gt:A)!$;A)341#&5#&!G!$;!$%I)5!$#&%$9)D 1.0+3  #&.03(1'G())3#&%u!$#d1()#&
!$#&%21!G.0'B1#&571;W1()#&%$#)#B1  ;!$t:%LN+51()#341.0)3K<9)+'B1.0;)a-bOcs!$#&%GAd)5dcs+.039)'G().LNQSSaA)!$;A^;%$#
*>*#B1(+;J5m<N;!W5)#B1#&!G>*.0).0)3q1(+#341.0)3r<N9))'B1.0; {$} z f$1()#A)!$;?)D0#&>()%=?^#&#&/5)#&'&;>*A^;%$#&5)5
1()#q#BA#&!o1%1!G.0)#&5)bcW()#&.0!341.0)3C<N9))'B1.0;I.0%I.0)A)91-EG5+#&A#&+5)#& 1PD0.0)#&!2E  #&.03( 1.0+3*<9+)'B1.0;
1()41r.0%}5+#B1#&!$>*.0)#&56?/'&;>?).0)41.0;;<1()#C)#B1  ;!$t:%$_5). ,+#&!$%G. 1g;I1()#C'&9)!G!$#& 1*.0)A+91  . 1(
1()#}D0.0t#&D0.0(+;J;:5C1()41=1()#&%$#})#B1  ;!GtJ%()-,+#}%G#&#&u5)414+#&!$q1()41.0+A)91b
wD 1();9)3(w1()#>r. J19)!G#&%R;<#B^A^#&!21%_+5#&+%$#&>?+D0#A)!$5).03>*%_%G#&#&>,+#&!2I%G.0>*.0D0!$P1()#BI!$#O.0
<N'B18:9). 1#O5).0%21.0+'B1<N!$;>q%2141.0%21.0'&D)A^;.01O;<,^.0#  bc=()#>r. J19)!G#&%2EG;<EG#BA^#&!21%>*;:5)#&DJ>rt#&%^1()#
%$%$9+>*A1.0;71()41q%$.0)3D0#w#BA#&!o1w.0%!$#&%$A^;)%$.0?)D0#<;!#&'G(I#B>*A)D0#&bWB71().0%'&%G#&P#&'
(/#B^A^#&!21q.0%
C>*;:5)#&D^;<sC!G#&3.0;m;<)1(+#.0)A)91%$A)'&#&P+51()#p;?/;<+1()#q341.0)3*<N9))'B1.0;u.0%R1;C5)#&'&.05)#<N!$;>
 ().0'
(d>r;J5)#&D1()#=5)41A^;.0 1w;!$.03.0)41#&%$by).0)'&#W#&'
(@+#B1  ;!$t*.071()#=#&)%G#&>?)D0#A)A)!$;'
(dD0#&!G)%
1()#  ();D0#w1%Gt!$41(+#&!1()@p9+%21*%$;>*#K%$9)?1%Gtu)5@1(:9)%>*t#&%}+;@%G9)'G(>919)D#B'&D09)%$. ,. 1
%$%$9+>*A1.0;)P^#&)%$#&>?)D0#&%O!$#qA)A)!G;A)!$.041#  ()#&I);;)#q>*;:5)#&D.0%(+.03()D D0.0t#&D @1;C?#'&;!$!$#&'B17<N;!
;+#A;.01.0K1(+#.0)A)917%$A+'&#&b



 


 j 


cW().0%IA)A^#&!IA)!$#&%G#& 1%/'&;>*A)!G#&()#&)%$. ,+##&>*A+.0!$.0'&Dq#B,D09)41.0;;<7MO33.0)3g+5MO;:;%21.0)3<N;!
)#&9)!GDw+#B1  ;!$tJ%)5"5)#&'&.0%$.0;1!$#&#&%$b  9)!I!$#&%G9)D 1%m5)#&>*;+%21!$41#@1()41MO33.0)3]#&)%$#&>?)D0#
)#&!$D rD  -%_;91A^#&!$<N;!$>*%s%$.0+3D0#'&D0%$%G.0F)#&!$b  9)!!G#&%$9)D 1%RD0%$;%G();  1()41OMO;:;%21.0)3w#&)%$#&>?)D0#
'&63!$#&41D g;91A^#&!$<N;!$>?^;41(MO33.0)3m)5/%$.0)3D0#C'&D0%$%$.0F+#&!$bIv;  #B,+#&!$P<N;!}%$;>*#C5)41u%$#B1%
MO;J;%o1.0)3>*-"%$();  &#&!$;3.0g;!*#B,+#&5+#&'&!$#&%$#/.0A^#&!$<N;!$>*)'&#I<N!$;>%$.0)3D0#/'&D0%$%$.0F+#&!$b
)9)!o1()#&!w1#&%21%*.0)5+.0'&41#1()41mM;:;%21.0)3>*-%$9)#&!C<!$;>;$,+#&!$F1-1.0)36.01(+#@A+!$#&%$#&)'&#I;<w);.0%$#
 ().0'
(m>*$#BA)D0.0%$;>*#};<1()#5)#&'&!G#&%$#&%.0uA#&!G<;!$>r)'&#<;!=M;:;%21.0)3bZ\]#7D0%$;r<;9)+51()41
}%$.0>rA)D0#=#&)%G#&>?)D0#=A)A)!$;'
(r;<9)%$.0+3)#&9)!$D)+#B1  ;!$tJ%1()415+.0#&!;)D m.01()#&.0!!$)5);>z.0). 1.0D
 #&.03( 1K%$#B1-1.0)3%=A^#&!$<N;!$>*#&5/%G9)!$A)!$.0%G.0)3D   #&D0D0P;<e1#&m5);.0)3r%  #&D0Ds%1()#}MO33.0)3b
**

fi

r+su}SmQz
r

vr++z

w)D %$.0%;<_;9+!=!$#&%$9+D 1%=%$9)33#&%o1%H1(+41=1()#A#&!G<;!$>r)'&#};<_?^;41(mMO;J;%o1.0)3d>r#B1();J5+%LNw5)4E
MO;J;%o1.0)3/)5!G'&.0)3aq.0%41CD0#&%o1rA)!21D 5)#&A^#&)5+#& 1C;d1()#K5)41I%$#B1*?#&.0+3@#B>*.0)#&5+P  ()#&!$#
MO33.0)3%$();  %H>9)'G(dD0#&%$%'&;!G!$#&D041.0;)bWcW(+#W%21!$;)37'&;!$!$#&D041.0;)%O<N;!HMO;J;%o1.0)37>*$u?^#WA)!21.0D0D 
#BA)D0.0)#&5]?. 1%r%$#&+%$. 1. ,^. 1g1;6);.0%G#&Pw'&D0.0>%G9)A)A^;!21#&5?5)5+. 1.0;)D_1#&%21%Gb.0)D0D P  #
%$();  1()41>9)'
(u;<+1()#}A^#&!$<N;!$>*)'&##&)(+)'&#&>*#&1<;!=m#&)%$#&>?)D0#}'&;>*#&%  . 1(K1(+#F)!$%217<N# 
'&D0%$%$.0F+#&!$%'&;>?).0)#&5+P?+91Z1()41MO;J;%21.0+35)#&'&.0%G.0;}1!$#&#&%O>*$'&;1.0J9)#H1;<9)!o1()#&!Z.0>*A)!G;-,+#  . 1(
D0!$3#&!#&)%$#&>?)D0#}%$.0&#&%$b
B/'&;)'&D09)%$.0;+P%WC3#&)#&!$D1#&'G()).08:9)#}<N;!W5)#&'&.0%G.0;K1!$#&#&%W)5u)#&9)!$D^)#B1  ;!$t:%$PMO33.0)3d.0%
A)!$;?+?)D A)A)!G;A)!$.041#w<;!>*;%o1A+!$;?)D0#&>*%$P?)91  ()#&IA)A+!$;A)!$.041#&P^MO;J;%o1.0)3uLN#&. 1()#&!w!$'&.0)3K;!
w5)aW>*$A+!$;J5+9)'&#qD0!$3#&!=3.0)%.0m'&'&9)!$'Bb

  <



 )[ 

fi

c ().0%_!G#&%$#&!$'G(  %_A)!o1.0D0D /%G9)A)A^;!21#&5? I). ,+#&!G%$. 1m;<+.0))#&%G;41}}!$1%2EG.0EGw.05q1;?^;41(C9E
W
1();!$%Gb}q-,+#  A). 1  %qD0%$;@%$9)A)A^;!21#&5u? w41.0;)Dy)'&.0#&)'&#);9))5+41.0;3!G 1C EGS[QSP
1()#;1)  [HYy)VW;Y#B1!$;D0#&9)> =#&%G#&!2,+;.0!V=()!$'B1#&!$.0&41.0;Y!$;jp#&'B1Pd  wcWy
3!$1%G9)A)A^;!21#&5g?1()#uw). ,+#&!$%G. 1;<;1)P7)5;1)y+'&.0#&)'&#cR#&'G(+);D0;34wD E
D0.0)'&#K3!$1bCcW().0%q.0%q#BJ1#&)5+#&5r,+#&!G%$.0;;<ZIA)A^#&!A+9)?)D0.0%$()#&5.0d1()
#  ~  z fGf-h z{Ez  h { n
 h } f-&f-h G
f "
h W z \ -- { &
n -h z f$nn  |f-d
h 
Zf 

  p

 
 

wD0.0P}|}b0PwxY&&).0PbKLNQSSTa-b!$!$;!d!$#&5)9)'B1.0;1()!$;9+3(]D0#&!$).0+3>9)D 1.0A)D0#5)#&%$'&!$.0A1.0;)%$b
 {   hsg
f f { 4h)N
h | 
P MPQ[4+`{`b
wD0A)-5).0)PHb=LNQSS[a-b9)D 1.0A+D0#*)#B1  ;!$t:%}<N;!}<9+)'B1.0;6D0#&!$).0+3bmEZGfGfNh|i }]z fEMM
C[-h z f-4h {Ez  h { n  h } f-&f-hGfh  f ~  { n  f z fii$P~=;D0bP^A)A)b^`4+[`dy)/)!$)'&.0%G'&;b
w!$?).0?)PbZLN5)b0a-bHLNQSSa-b]

f  { 
h l;Gmb }W  { Nh  ;f  { 
h   f ~  { n  f z  fii$bOcYH!G#&%$%$b
 A
w%$t#&!$P=sb0Px'&D0.0)Pw=b}LNQSSa-b")%$#&>?)D0#&%C%*%$#&8:9)#&)'&#/;<='&D0%G%$.0F)#&!$%$bR
 EZG fGf Nh| i
 }z f }z fGf$h z -h z f-4h {Ez  h { nk& Nh z   h } f$f$hdG fj hW
  z \--  { nl- h z f-nNn | f-hd
 fpPA)A)b+XT{4+XT
w3;-+PJA))b

w%$t#&!$Pb0PJx'&D0.0)P)bLNQSS?)a-b+#&419)!$#O#&)3.0)#&#&!$.0+3)5'&D0%$%$.0F)#&!%$#&D0#&'B1.0;+'&%G#%o19)5*.0
~C#&:9)%$.0=,+;D0'&);5+#B1#&'B1.0;)bB]
 E ZGfGf N
h |U
i  }*z *
f % ~  z f
f-h z -h z f-4h {Ez  h { n  &h } f$f$d
h Gf
&
h  {   hsg
f f { 4h)N
h | PsA)A)b^[4+QQrw%$(,^.0D0D0#&PRcWwb
MO9)#&!$Pb0Px|};()$,^.0PbLNQSSSa-bw#&>rA).0!$.0'&D:'&;>*A)!$.0%$;K;<j,+;41.0)3q'&D0%$%G.0F)'&41.0;*D03;!$. 1()>r%$
MO33.0)3PR?^;J;%o1.0)3P)5K,!$.01%$A
b  {   Nhsg
f Rf { 4h)8
h | d
P X4PQ{4EGQ[Sb
MO4J1P\b7LNQSS`a-bB>*A)!$;$,^.0+3@1()#/'&'&9)!$'B;<wg!21.0F)'&.0D=)#&9)!$D)#B1  ;!Gt9+%$.0)3>9+D 1.0A)D0#
5+.0#&!$#&1D I1!$.0)#&5)#B1  ;!$t:%$b  f ~  { n  0
k  ~z;{Ez  h%
P P`4+X{b
MO!$#&.0>*)PbHLNQSSTa-bqMO33.0)3rA)!G#&5).0'B1;!$%$bA

{ 

hsf0f { 4 h)Nh| PMLN`a-PRQ`[4+Q{b

MO!$#&.0>*)PbqLNQSST?)a-bMO.0%$Ps,!G.0)'&#&PO)56!$'&.0)3'&D0%$%$.0F)#&!G%$bcs#&'
()bH!$#&A+bZT{POVEGMO#&!$t#&D0#BP
MO#&!$t#&D0#BPRVWwb
*

fi

stv{wz{y{t

MO!$#&.0>*)PbHLNQSST'&a-bqy1'Gt#&5!$#&3!$#&%G%$.0;)%$bW

{ 

Nhsfgf { 4 h)Nh| PMLNQa-PRS4+Tb

VWD0#&>*#&+PbZLNQSXSa-bWVW;>?).0).0)3K<N;!$#&'&%21%$W!$#B,.0# 
%&;f  { i z Nh | W
P 4PS4+X[b

)5/));4141#&5/?).0?+D0.0;3!$A)(b-&

~

{ Cn  }

4h

q!$9)'
t#&!$Pvwb0PxVW;!o1#&%$PVWbLNQSSTa-bwMO;J;%o1.0)3r5)#&'&.0%G.0;C1!$#&#&%$bucs;9)!G#B1%$t Pwb0Ps;&#&!GPb0Px
vw%$%$#&D0>r;Pb_LNH5)%$b0a-l
P UE { 
h Gf-ih  f ~  { X
n -h } &
k {Ez  
h Hm GGf$iGiGN
h |]i z f-kKiGP~=;D0b^XPA)A)b
S4+XIVW>?)!$.05+3#&PwbcYH!$#&%$%Gb
q!$9)'
t#&!$PHvwb0PZVW;!21#&%GPZVWb0P_'
t#&D0Psb0PZ#&V=9))P<Kb0PHx~=A)).0t:P^~Kb=LNQSSa-b/MO;:;%21.0)3m+5;41()#&!
>r'G().0)#wD0#&!$).0+3}D03;!$. 1()>*%GbZ
 Hm GGf
;f N
h |fii
i  }-z f n2af :f-h z -h z f-4h {Ez  h { n  &h } f$f$d
h Gf
&
h  {   hsg
f f { 4h)N
h | PsA)A)b^[4+TQ@#  MO!$9+)%  .0'
tJPwb
H<N!$;)PMb0PRxc=.0?)%$().0!$+.0P=bOLNQSS[a-b]Wh-h z m G
w#  =;!GtJb

~  z h z  z f  G z i z  { +bV=()A)>*)5uvwD0D0P

.0%$()#&!$Pqb0Px'&|}9)%$.0'
tJP|}bRLNQSXSa-bwd#&>*A+.0!$.0'&D'&;>rA)!$.0%$;@;<q[)5@?)'GtEGA)!$;A)341.0;+b
B
 E ZGfGf N
h |
i  }z 
f n2af :f-h z -h z f-4h {Ez  h { g
n l&h z  h } f-&f-
h G"
f 
h W z 3 - { W
n -h z f$nn  
|f-dh 
fpPA+A)b^XX4+S[@q#B1!$;. 1PsBb
)!$#&9+)5)PXCb0PZxy)'G(+A).0!$#&PZb=LNQSSTa-bmA^#&!$.0>*#&1%  . 1(6/)# 

fGf 8h |U
i  }%z A
f   N z f
f-h z -h z f-4h {Ez  h { n  h } -f &f-hd
fAhx
MO!$.0Ps 1D b

?^;:;%21.0)3/D03;!$. 1()>rb/EE
hsP
f Rf { 
h+
h |PA)A)bQX4+QT

{ 

)!$.0#&5+>*)PRbLNQSSTa-b  ?).0%GP),!$.0)'&#&P_{+Q4EGD0;%$%GP_)5*1(+#'&9)!G%$#BEG;<EG5+.0>*#&)%$.0;)D0. 1bl
 }0x{Ez;{ uNh)8h | { dh jhd  nof;E|f  i;Ef-fiP-4b

~

4h

{
n

)!$.0#&5+>*)Pb0Pv%21.0#&P^cWb0PxcW.0?)%$().0!G).0P+bRLNQSSXa-bHw5)5). 1. ,+#wD0;3.0%21.0'!$#&3!G#&%$%$.0;)O"%2141.0%21.0'&D
,.0#  ;<R?^;J;%o1.0)3bLN(1-1A) + WW EG%o141b0%21)<N;!$5)b0#&5)
9 Bp(+<-a-b
}#&>*+Py)b0PMO.0#&)#&)%21;:'Gt:PRHb0Pxq;9)!$%G41P=bHLNQSS`a-bww#&9)!$D)#B1  ;!GtJ%)5K1()#?).0%;4,!$.0+'&#
5+.0D0#&>*>*b  f ~  { n  0
k  ~z;{Ez  h%
P PQ4+Xb
}!$+3#&!$PV=bLNQSXSa-bVW;>?).0).0)3/<N;!$#&'&%21%$@c  #& 1+#&!$%D041#&!$b&
QT4+Q[b

~

4h

n  } &f; { i z Nh|PWP
{g

}!$;$,+#&Pqb0Pxy)'
(J9+9)!$>*)%GPWqb7LNQSSXa-b"M;:;%21.0)3.01()#/D0.0>*. 14.0>*.0&.0)3m1()#I>*!$3.0g;<
D0#&!G)#&5#&)%$#&>?)D0#&%$bK
 Hm G
fG;f &
h |fi
i  }z U
f  } z f
f-h zi{Ez  h { n  &h } f$f$d
h G
f 
h W z \ -$ { n
-h z f-nNn v |:f$dh Gf-PA)A)b^TS`4+TSSI5+.0%$;)P\b
vw>*A)%$().0!G#&PJb0Px]\].0?^#&D0Pb_LNQSXSa-bOcW()#=>r#B14EGA).^)#B1  ;!$t:M9+.0D05).0)35).0%21!$.0?)91#&5dt:);  D0#&5)3#
!G#&A)!$#&%$#&141.0;)%I<;!@!$;?)9+%21A)41-1#&!$!$#&'&;3). 1.0;+bcs#&'
()bw!$#&A)bWVWHEGVWyEGXS4EGQTTPVWwP
Y. 1-1%$?)9)!G3()PY^b
vw)%$#&)Pb0Pqxy)D0>*;)PY^bCLNQSS{a-b#&9+!$Dq)#B1  ;!$t#&)%$#&>?)D0#&%$b[C+ { +
h i
 {Ezz f-4"
h Wh { n iGNi { d
h ] {   Nhsg
f -h z f$nn v |:f-
h GfpPCmPSS[4+Q{{Qb
vw%$()#&>*Py)bLNQSSa-b  A1.0>*DD0.0)#&!@'&;>?).0+41.0;)%d;<+#&9)!$DW)#B1  ;!GtJ%$b
LNa-PRSS4+TQb

Zp

*

{  z  h+i&h

 f ~  { n  f z  iGP

fi

r+su}SmQz
r

vr++z

'&;?+%$Pb0P7J;!$5))P}b0P;  D0)Py)b0P7xvw.0 1;+P}b*LNQSSQa-b5+A1. ,+#>*. J19+!$#&%/;<7D0;:'&D
#BA^#&!21%$b  f ~  { n  0
k  ~z;{Ez hP%PS4+Xb
|};()$,^.0PKb0Px\];D0A#&!o1PqbrLNQSSTa-bMO.0%mA)D09+%,!$.0+'&#5)#&'&;>*A^;%$. 1.0;<N;!m&#&!$;4EG;)#D0;%$%
<N9))'B1.0;+%$b
 E G
fGf 8
h |c
i  }z 
f   N z f
f-h z -h z f$
h {mz  h { n  h } f$f$d
h G
f 
h  {   hRf
f { 4h)Nh | PsA)A)b`4+`X[@MO!$.0P 1D b
|};)3P:Hb0PJxgq.0#B1-1#&!$.0'G(+P)cWbLNQSSa-bH!$!G;!2EG'&;!$!$#&'B1.0)3=;91A)91'&;:5).0)3='&;!$!$#&'B1%R?).0%)5W,!G.0)'&#&b
B
 Hm GGf
;f N
h |fif
i  }Pz 0
f   f$n }z -h z f-4h {Ez  h { n  h } f-&f-d
h 
g
f &
h  {   Nhs[
f Rf { 
h)N
h |P)A)A)bJ[Q[4
[`Qdcs(+;J#}V=. 1PVWwb
|}!$;3(+P_b0P_x~=#&5)#&D0%G? Pb=LNQSSa-b/w#&9)!$D)#B1  ;!$t#&)%$#&>?)D0#&%$PZ'&!G;%$%O,D0.05)41.0;)PH+5'B1. ,+#
D0#&!G).0)3b"cR#&%$9)!G;P7b0PcR;9)!$#B1&tPCqb0PKx#&#&)PKcWb*LNH5+%$b0a-P@E { hd
f-ih  f ~  { n
-h } 4k {Ez  
h Hm G
f-iGi
8
h |xi z f$kiGP~=;D0bPA)A)b`[Q4+`[X@VW>?+!$.05)3#&PbBcYH!G#&%$%$b
.0+'&;D0)P\b0Pxy)t:!$B^A^#&t:PJbOLNQSXSa-b*y^)#&!G34;<H'&D09)%21#&!G.0)3d>9)D 1.0A)D0#K?)'
tuA)!G;A)341.0;)#B1-E
P UE { d
h Gf$iNh  f ~  { d
n -h } 4k {Ez  
h Hm GGf$iGiGN
h |i z f$kiGP
 ;!$t:%$bcR;9)!$#B1&tP_wbLNH5+b0a-%
~=;D0bs`PA)A)bT{4+TS@y)m41#&;PRVWb;!$3|}9)<N>*))b
'&D0.0)P=bLNQSSXa-bKMO;:;%21.0)3@'&D0%$%$.0F)#&!$%q!$#&3.0;+D0D b7BEG
fGfh8|i^ }@z f@ }z fGf$h
 &h } f$f$dh G
f 
h W z \ -$ { o
n -h z f-nNn v |:f$d
h Gf-PA)A)b{{4+{@5+.0%$;)P\b

zb{Ez h {

'&D0.0)P7b0Pwx  A). 1&P7wb*LNQSSa-bw#&>*A).0!$.0'&Dw#B,D09+41.0;;<}?)33.0)3)5?^;J;%o1.0)3bzB
E ZGfGf Nh|ix }z f% ~  z f
f-h z{Ez  &h { n  h } f-&f-hd
f]&hW z 3-$ { n<-h z f$nn v|:f-hGfpPA)A)b+T4
QdYH!$;$,^.05+#&)'&#&P=Bb
'&D0.0)PZ=b0Pxy)(+-,D0.0tJP_JbLNQSSa-b@VW;>?).0).0)31()#KA)!G#&5).0'B1.0;)%;<H>9)D 1.0A)D0#C'&D0%G%$.0F)#&!$%$w%$.0)3
'&;>rA#B1. 1. ,+#=D0#&!$).0)31;.0). 1.0D0.0&#W+#&9)!$D:)#B1  ;!$t:%$bBi
 E ZGfGf N
h |f
i  }[z [
f % ~  z f
f-h z -
h 
f 
h W z 3 -$ { I
n -h z f$nn v |:f-
h GfpPA)A+b`4+[{;1!$#&D0P)V=)5)b
z f$
h {Ez  &h { n &h z  h } f-&f-dh 
A
).0P)}bLNQSSQa-bs;  #&!$.0)3,!$.0)'&#;<5)#&'&.0%$.0;)%_?@9)%G.0)3!o1.0F)'&.0D)#&9)!$D)#B1  ;!Gt7A^;!21<N;D0.0;%$b
k  ~z;{Ez hP%PX4+XTb
 f ~  { n  0
;J;)#BP=b0Py)()$,^D0.0t:Pb0P7cR;  #&D0D0PK}b0Px};-,+#&PKwbrLNQSXSa-bw#B^A^#&!$.0>*#&1D'&;>*A)!G.0%$;
;<%o^>?^;D0.0')5'&;))#&'B1.0;).0%o1D0#&!$+.0)3D03;!G. 1()>*%$bBEZGfGfNh|i }z fn2fa:f$h z
-h z f-4h {Ez  h { n l&h z  h } f-&f-h Gx
f 
h W z \ -- { &
n -h z f$nn  |f-d
h 
fpPA+A)b^4+X{@q#B1!$;. 1PsBb
9)!$A)(PY^b}b0P}xw()PKqbO\brLNQSSa-bwVWr!G#&A;%G. 1;!2;<K>*'
().0)#6D0#&!G).0)35)41?)%G#&%
LN>r'G().0)#BEG!G#&5)?)D0#m5)41!G#&A;%G. 1;!2^a-b]). ,+#&!G%$. 1";<=VWD0.0<N;!$).04EGB!2,.0)#&P=q#&A)!o1>*#& 1u;<
B)<N;!$>*41.0;)5/VW;>*A)91#&!Wy)'&.0#&)'&#&b
w;  D0)Py)b0Pxy)#Bp+;  %$t:.0PcWb}LNQSS`a-b].0D 1#&!*%G#&D0#&'B1.0;]>*;:5)#&D<;!K3#&)#&!$41.0+3@,^.0%$9+D>r;41.0;
%G.03)D0%$brv)%G;)P)y)b0PVW;  )P+Jb0P+x7.0D0#&%$PV=bsLNH5)%Gb0a-P<UE { hGf-i=h  f ~  { n8-h } 4k {mz  h
E ZGf-i
iGNh |xi z f-kKi$P)~=;D0bPA)A)b[TS4+[TIy)/41#&;PRVWb;!$3|}9)<N>*))b
 A). 1&P:qb0Pxgy+()-,D0.0tJP:b+LNQSSTa-bw'B1. ,+#&D d%$#&!$'
().0)3W<N;!}#&#&'B1. ,+#O)#&9)!$D EG+#B1  ;!$tq#&)%$#&>?)D0#&b
 &h)hs;f  z  &"
h d--f-d
h 
fpk
P OLN+
[ a-P[[4+[[b
*ff

n

fi

stv{wz{y{t

 A). 1&POqb0PHxy)()$,^D0.0t:PObwLNQSST?+a-b6}#&)#&!$41.0+3'&'&9)!G41#d)55). ,+#&!$%$#d>*#&>?#&!G%7;<u+#&9)!$D E
+#B1  ;!$t6#&)%$#&>?)D0#&bcs;9)!G#B1%$t Pqb0PO;&#&!$PWb0Pxvw%$%$#&D0>*;PbqLN5)%$b0a-
P UE { 
h Gf-i
Nh  f ~  { l
n -h } &
k {Ez  
h Hm GGf$iGi

h |ji z f$kKi$P:~=;D0b^XP^A)A)b+[4+QrVW>?+!$.05)3#&P^wbBc
Y!$#&%$%$b
Y#&!$!$;)#&P=bLNQSS`a-b]%G;<1-EG'&;>*A^#B1. 1. ,+#m%GA)D0. 1-1.0)3!$9)D0#@<;!K5)A1. ,+#71!G#&#BEG%21!$9)'B19)!G#&5+#&9)!$D
+#B1  ;!$tJ%GbB
 Hm GGf
;f N
h |^
i  }Uz f
f -h z f-4h {Ez  h { *
n &Nh z  h } f-&f-
h G
f &h  f ~  { n  f z  iGP
A+A)bTXS4+TS[dMOD 1.0>*;!G#&Psqb
Y#&!$!$;)#&PqbLNQSS[a-b-k0EmNh|"f|fif$iGi
 hi z Nk {Ez  &hdW:f$ { |Nh|f z Gi }  { 4 { hdGf
w;f  ~  z  h   z [ z f$h)iG h z f-hsf$ { n  h:faf { i ~ f"o z kKv {Ez  hbYH()b0qb1(+#&%$.0%$P
MO!$;  uw). ,+#&!$%G. 1PY!$;-,.05)#&)'&#&P=Bb
}9).0+D0)P+JbRLNQSS[a-b
V=b

 XMCE |fi {

ki

} A {  

hsfWRf { 
 h+h|bZ;!$3d|79)<>r))P+y+r41#&;P

}9).0+D0)PJb=brLNQSSTa-bM33.0+3PC?;:;%21.0)3PK)5'&b0bEG
fGfh8|i }z 
f 
f 
h W z 3 - { &
n -h z f-nNn  |f-d
h 
fpPA)A+b`4+[{bY;!21D0)5+P  =b
{Ez  h { n  h } f-&f-dh 
x

 N z f
f-h z

=9+>*#&D0()!21POqb0Pv.01;)PO}b0Px\.0D0D0.0>r%$P=bLNQSXTa-bs#&!$).0)3.0 1#&!G)D!G#&A)!$#&%$#&141.0;)%K? 
#&!G!$;!A+!$;A)341.0;)b6B9)>*#&D0()!o1Pqb0Px'&VWD0#&D0D0)5)PJbwLNH5+%$b0a-P {  { nNn2f$n  i z 
-l ~z f
E ZGf-i
iGNh |gCn  {Ez  h)iKh z fkK $E &i z  ~  z~ 
f  } ; |fih) z  d
h l&n ~ kmfMA ~ h {Ez h)i$P
A+A)b[QX4+[T[bBcYH!G#&%$%$PVW>?)!$.05)3#&Pwb
y)'
()A).0!$#&PbHLNQSS{a-bc=()#%21!$#&)341(u;<  #&tID0#&!$)?).0D0. 1bg

{ 

hsfgf { 4 h)Nh| P-LN`a-PRQS4+``b

y)'
()A).0!$#&P=b0P=+!$#&9))5)PPKb0PMO!21D0#B1-1PqYb0Pwx#&#&PH\b7LNQSSa-b"M;:;%21.0)3/1()#/>*!$3.0))# 
#BA)D0)41.0;<N;!_1()#}#&#&'B1. ,+#&)#&%$%q;<,+;41.0)3d>*#B1();:5)%$bb
 E ZGfGf N
h |
i  }gz g
f % ~  z f
f-h z
-h z f-4h {Ez  h { n  h } f-&f-dh 
x
f 
h  {   Nhs0
f Rf { 
h+
h |PA)A+b[``4+[[{dw%$(,^.0D0D0#&PscWb
y);D0D0.0'
()PYb0Px|}!$;3()PRwbOLNQSSTa-bCs#&!$).0)3  . 1(#&)%$#&>?)D0#&%$}vw;  ;-,+#&!oEGF1-1.0)3/'&?#9)%$#&<N9)D0b
B*cR;9)!$#B1%$tPwb0P;&#&!$Pb0Pxv%G%$#&D0>*;PbLNH5)%$b0a-PX@m { hd
f-iWNh  f ~  { n8-h } 4k {mz  h
E ZGf-i
iGNh |xi z f-kKi$P)~=;D0bXPA)A)bQS{4+QSTIVW>?)!$.05+3#&PwbcY!$#&%$%$b
cR!$#&%$A)P~Cb0PxcR).039)'G(+.0P=b7LNQSSa-bV=;>?).0+.0)3#&%21.0>*41;!$%*9+%$.0)3);EG'&;)%o1 1  #&.03( 1.0+3
<N9))'B1.0;+%$bcs#&%G9)!$;P}b0P}cR;9)!$#B1&tP}qb0P}x#&#&)PcWbKLNH5)%Gb0a-
P @E { d
h 
f-imNh  f ~  { n
-h } 4k {Ez  
h Hm G
f-iGi
8
h |xi z f$kiGP~=;D0bPA)A)bQS4+`T@VW>?+!$.05)3#&PbBcYH!G#&%$%$b
\];D0A^#&!21PqbLNQSS`a-bwy1'
t#&5u3#&)#&!$D0.0&41.0;+b

 f ~  { n  f z  iGP4P`Q4+`Sb

()+3PCb0P^#&%G.0!$;-,Pb0P^x\]D 1&PqbZLNQSS`a-bWvH^?)!G.05d%2%21#&><N;!OA)!$;41#&.0/%$#&'&;)5+!2%21!$9+'B19)!$#
A+!$#&5).0'B1.0;)bA& ~ 4h { n }  nof; ~ n {    n |Zj
 PM<4 PsQ{S4+Q{T[b

**

fi

"[


r+su}SmQz
r

vr++z

 Hn

cR?)D0#&%O)5@%$(); 

q41ry)#B1
?)!$#&%o1-EG'&)'&#&!2E 
'&!$#&5). 1-EG
'&!$#&5). 1-EG3
5).0?^#B1#&%
3D0%$%
()#&!21-EG'&D0#B,+#&D0+5
()#&A)41. 1.0%
();9)%G#BEo,+;41#&%2EGX
(^A^;
.0;);%$A+()#&!$#
.0!$.0%
t:!2Eo,^%oEGtJA
D0?^;!
D0#B1-1#&!
A)!$;>r;41#&!$%2EGS[T
!$.0?^;%$;>*#BEG?+.0)5
%$41#&D0D0. 1#
%$#&3>*#&141.0;
%$.0'
t
%$;)!
%$;$^?^#&
%$A)D0.0'&#
,+#&().0'&D0#

1()#'&;>*A)D0#B1#q!$#&%$9+D 1%<N;!1()#F+!$%21w%G#B1q;<#BA^#&!$.0>*#&1%9+%$#&5d.071().0%OA)A^#&!$b

H!$!
[b0
Qb0X
`b0S
`[b0S
[Xb0T
QXb0T
`{b0Q
b0S
Tb0
Sb0
b0[
`b0[
Tb0Q
QXb0{
b0[
Sb0[
Q[b0{
Tb0T
b0S
QTb0T
Sb0`
b0
`b0S

y+.0)3D0#
y)
{b0[
{b0
{b0X
{b0S
Qb0
Qb0{
Qb0T
{b0T
{b0`
Qb0[
Qb0
{b0
Qb0
{b0[
{b0T
{b0
{b0[
{b0
{b0
Qb0
Qb0Q
{b0`
Qb0`

MO#&%21
`b0S
Q[b0T
`Tb0`
``b0T
[Tb0S
QTb0X
QSb0Q
b0Q
Tb0`
b0
`b0{
Qb0
[b0
Qb0T
b0
Xb0S
Q`b0T
b0
b0`
Qb0S
b0{
b0
``b0S

y .0>rA)D0#
)
H!G!
y)
[b0
{b0`
Q[b0
{b0
`b0
{b0`
`[b0{
{b0
[b0`
Qb0Q
Qb0
Qb0Q
QSb0
Qb0
b0X
{b0`
Tb0`
{b0Q
b0
{b0
[b0S
{b0[
{b0X
{b0Q
[b0`
{b0X
Q`b0X
{b0`
b0X
{b0[
Xb0
{b0[
Q{b0S
{b0`
b0[
{b0[
b0
{b0`
Qb0S
Qb0`
Tb0
{b0
b0{
{b0`
`Qb0`
{b0X

M 33.0)3
O
!$!
y)
[b0
{b0`
Q[b0X
{b0T
`b0`
{b0
``b0X
{b0
[[b0Q
Qb0S
Qb0{
{b0T
Qb0X
{b0
b0Q
{b0`
Tb0`
{b0Q
Sb0`
Qb0`
b0{
{b0
{b0X
{b0`
b0`
Qb0{
Q{b0
{b0[
b0{
{b0[
Xb0
{b0
Q{b0T
{b0[
b0
{b0`
b0
{b0Q
QTb0X
Qb0Q
Tb0S
{b0
[b0S
{b0Q
`{b0
{b0T

 !$'&.0)3
w
H!$!
y)
[b0X
{b0
Qb0X
{b0T
`b0`
{b0X
`b0
{b0`
[`b0{
`b0
`{b0
Qb0T
QSb0{
Qb0[
b0Q
{b0
Tb0`
{b0Q
b0T
{b0T
[b0
{b0T
{b0
{b0Q
[b0`
{b0X
b0
{b0
b0
{b0`
Xb0Q
{b0`
Sb0S
{b0`
[b0
{b0`
b0
{b0`
Q`b0S
Qb0
Tb0
{b0
b0{
{b0Q
QSb0Q
Qb0{

M ;J;%o1.0)3
O
H!G!
y)
b0{
{b0
Qb0
{b0T
`b0[
{b0Q
`[b0[
Qb0`
[Qb0Q
{b0S
`Qb0Q
{b0S
QSb0
{b0S
b0[
{b0
Tb0`
{b0Q
Xb0[
{b0
[b0S
Qb0{
{b0[
{b0Q
[b0`
{b0X
b0T
{b0Q
b0T
{b0[
Xb0`
{b0[
Q{b0{
{b0[
[b0[
{b0`
b0
{b0[
Q[b0{
Qb0
Tb0[
{b0T
b0`
{b0Q
QSb0
Qb0{

cR?)D0#}}w#&9)!$D)#B1  ;!$t*1#&%21*%$#B1C#&!$!$;!q!$41#&%)5%21)5+!$55+#B,^.041.0;@,D09)#&%<N;!H1(+;%$##&!G!$;!
!$41#&%<N;!7LNQaK%$.0)3D0#q)#&9)!GD+#B1  ;!$tr'&D0%$%G.0F)#&!$UHLN`aK%$.0>*A)D0#q)#&9+!$D+#B1  ;!$td#&+%$#&>qE
?)D0#&UwLN[a}IM33.0+3m#&)%$#&>?)D0#&UwLNaw!$'&.0)3I#&)%G#&>?)D0#&U)5]LNa)5w5)4EGMO;J;%21.0+3
#&)%G#&>?)D0#&b=D0%$;K%$(+;  6LN!$#&%$9+D 1%'&;D09)>*I[a.0%R1()#C4?#&%o1!G#&%$9)D 1}A)!$;:5)9)'&#&5@<!G;>D0D;<
1()#7%$.0)3D0#})#B1  ;!Gt@!G#&%$9)D 1%W!$9+/9)%$.0)3CD0D;<1()#1!G.0).0)3*5)41b

*

fi

q41ry)#B1
?)!$#&%o1-EG'&)'&#&!2E 
'&!$#&5). 1-EG
'&!$#&5). 1-EG3
5).0?^#B1#&%
3D0%$%
()#&!21-EG'&D0#B,+#&D0+5
()#&A)41. 1.0%
();9)%G#BEo,+;41#&%2EGX
(^A^;
.0;);%$A+()#&!$#
.0!$.0%
t:!2Eo,^%oEGtJA
D0?^;!
D0#B1-1#&!
A)!$;>r;41#&!$%2EGS[T
!$.0?^;%$;>*#BEG?+.0)5
%$41#&D0D0. 1#
%$#&3>*#&141.0;
%$.0'
t
%$;)!
%$;$^?^#&
%$A)D0.0'&#
,+#&().0'&D0#

!$!
b0{
Qb0S
`Sb0T
`b0X
[Qb0[
`b0[
`Qb0`
[b0T
{b0
Xb0Q
b0`
{b0T
QTb0
Qb0{
Q`b0X
QQb0`
Q[b0X
[b0
Qb0[
`Sb0
Xb0{
b0S
`Sb0

y+.0)3D0#
y)
{b0
{b0X
Qb0{
Qb0{
`b0Q
Qb0[
Qb0`
{b0[
{b0Q
{b0
{b0
{b0Q
[b0
{b0X
{b0
{b0T
{b0
{b0`
{b0S
Qb0S
{b0
{b0[
{b0

stv{wz{y{t

M 33.0)3
O
H!$!
y+
[b0
{b0
Q[b0
{b0
`b0`
{b0
`b0
{b0X
`b0X
{b0
QSb0
{b0
Qb0[
`b0{
[b0T
{b0`
{b0
{b0{
Tb0
{b0T
b0S
{b0X
{b0T
{b0Q
Q[b0
{b0X
b0{
{b0Q
Q{b0T
{b0T
Q{b0`
{b0Q
Sb0S
{b0`
[b0{
{b0`
Qb0`
{b0Q
`b0[
Qb0[
b0S
{b0
b0
{b0`
`b0Q
{b0S

MO#&%21
b0{
Qb0`
`Xb0
`Tb0
`Xb0
``b0
`{b0{
[b0`
{b0
b0Q
b0[
{b0
Q`b0
Q`b0`
Q`b0
Q{b0X
Q[b0
[b0
Qb0Q
`Tb0S
b0
b0
`Xb0T

 !$'&.0)3
w
!$!
y)
[b0
{b0T
Qb0{
{b0S
`b0S
Qb0{
`Tb0{
{b0T
`b0
Qb0
`Qb0
Qb0T
QTb0S
Qb0Q
b0{
Qb0Q
{b0
{b0Q
Tb0{
{b0
b0Q
{b0T
{b0[
{b0Q
Q[b0{
`b0S
b0Q
{b0Q
Tb0X
{b0
Sb0[
{b0`
Xb0T
{b0Q
Qb0
{b0`
Qb0Q
{b0Q
`Qb0
[b0{
b0`
{b0`
b0Q
{b0Q
``b0
{b0X

M ;:;%21.0)3
O
H!$!
y)
[b0
{b0[
Q[b0
{b0
`Tb0
{b0
`b0
{b0T
`[b0[
Qb0[
`{b0X
Qb0{
Qb0`
Qb0[
b0X
Qb0{
{b0
{b0{
Tb0Q
{b0
b0T
Qb0Q
{b0
{b0{
QQb0T
`b0{
[b0S
{b0Q
Tb0
{b0[
Sb0T
{b0
Xb0
{b0`
Qb0
{b0`
Qb0{
{b0Q
`Qb0
`b0X
Tb0
{b0S
b0[
{b0`
``b0S
Qb0S

cR?)D0#}}q#&'&.0%$.0;1!$#&#*1#&%21%$#B1#&!$!$;!d!$41#&%d+5g%21)5)!G5]5)#B,.041.0;6,D09)#&%@<;!}1();%$#m#&!G!$;!
!$41#&%O<N;!qLNQa7%G.0)3D0#5+#&'&.0%$.0;}1!G#&#'&D0%$%G.0F)#&!$UZLN`aMO33.0)3K#&)%$#&>?)D0#&UZLN[a@w!$'&.0)3
#&)%G#&>?)D0#&U)5LNaq)55)4EGMO;:;%21.0)3I#&)%$#&>?)D0#&bD0%G;d%$(+;  LN!$#&%$9)D 1%w'&;D09)>*[aw.0%
1()#m4?^#&%21/!$#&%G9)D 1*A)!$;:5)9)'&#&5<!$;>D0D_;<1()#K%$.0)3D0#w1!$#&#C!G#&%$9)D 1%q!$9)9)%$.0+3@D0D_;<1()#
1!$.0+.0)3*5)41b

Z

*

fiJournal of Artificial Intelligence Research 11 (1999) 95-130

Submitted 3/98; published 7/99

Semantic Similarity in a Taxonomy: An Information-Based
Measure and its Application to Problems of Ambiguity in
Natural Language
Philip Resnik

resnik@umiacs.umd.edu

Department of Linguistics and
Institute for Advanced Computer Studies
University of Maryland
College Park, MD 20742 USA

Abstract

This article presents a measure of semantic similarity in an is-a taxonomy based on
the notion of shared information content. Experimental evaluation against a benchmark
set of human similarity judgments demonstrates that the measure performs better than
the traditional edge-counting approach. The article presents algorithms that take advantage of taxonomic similarity in resolving syntactic and semantic ambiguity, along with
experimental results demonstrating their effectiveness.

1. Introduction
Evaluating semantic relatedness using network representations is a problem with a long
history in artificial intelligence and psychology, dating back to the spreading activation
approach of Quillian (1968) and Collins and Loftus (1975). Semantic similarity represents a
special case of semantic relatedness: for example, cars and gasoline would seem to be more
closely related than, say, cars and bicycles, but the latter pair are certainly more similar.
Rada et al. (Rada, Mili, Bicknell, & Blettner, 1989) suggest that the assessment of similarity
in semantic networks can in fact be thought of as involving just taxonomic (is-a) links, to
the exclusion of other link types; that view will also be taken here, although admittedly
links such as part-of can also be viewed as attributes that contribute to similarity (cf.
Richardson, Smeaton, & Murphy, 1994; Sussna, 1993).
Although many measures of similarity are defined in the literature, they are seldom
accompanied by an independent characterization of the phenomenon they are measuring,
particularly when the measure is proposed in service of a computational application (e.g.,
similarity of documents in information retrieval, similarity of cases in case-based reasoning).
Rather, the worth of a similarity measure is in its utility for the given task. In the cognitive
domain, similarity is treated as a property characterized by human perception and intuition,
in much the same way as notions like \plausibility" and \typicality." As such, the worth
of a similarity measure is in its fidelity to human behavior, as measured by predictions
of human performance on experimental tasks. The latter view underlies the work in this
article, although the results presented comprise not only direct comparison with human
performance but also practical application to problems in natural language processing.
A natural, time-honored way to evaluate semantic similarity in a taxonomy is to measure
the distance between the nodes corresponding to the items being compared | the shorter

c 1999 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiResnik

the path from one node to another, the more similar they are. Given multiple paths, one
takes the length of the shortest one (Lee, Kim, & Lee, 1993; Rada & Bicknell, 1989; Rada
et al., 1989).
A widely acknowledged problem with this approach, however, is that it relies on the
notion that links in the taxonomy represent uniform distances. Unfortunately, uniform
link distance is dicult to define, much less to control. In real taxonomies, there is wide
variability in the \distance" covered by a single taxonomic link, particularly when certain
sub-taxonomies (e.g., biological categories) are much denser than others. For example, in
WordNet (Miller, 1990; Fellbaum, 1998), a widely used, broad-coverage semantic network
for English, it is not at all dicult to find links that cover an intuitively narrow distance
(rabbit ears is-a television antenna) or an intuitively wide one (phytoplankton
is-a living thing). The same kinds of examples can be found in the Collins COBUILD
Dictionary (Sinclair, ed., 1987), which identifies superordinate terms for many words (e.g.,
safety valve is-a valve seems much narrower than knitting machine is-a machine).
In the first part of this article, I describe an alternative way to evaluate semantic similarity in a taxonomy, based on the notion of information content. Like the edge-counting
method, it is conceptually quite simple. However, it is not sensitive to the problem of
varying link distances. In addition, by combining a taxonomic structure with empirical
probability estimates, it provides a way of adapting a static knowledge structure to multiple contexts. Section 2 sets up the probabilistic framework and defines the measure of
semantic similarity in information-theoretic terms, and Section 3 presents an evaluation of
the similarity measure against human similarity judgments, using the simple edge-counting
method as a baseline.
In the second part of the article, Sections 4 and 5, I describe two applications of semantic
similarity to problems of ambiguity in natural language. The first concerns a particular
case of syntactic ambiguity that involves both coordination and nominal compounds, each
of which is a pernicious source of structural ambiguity in English. Consider the phrase food
handling and storage procedures: does it represent a conjunction of food handling and storage
procedures, or does it refer to the handling and storage of food? The second application
concerns the resolution of word sense ambiguity | not for words in running text, which is a
large open problem (though cf. Wilks & Stevenson, 1996), but for groups of related words
as are often discovered by distributional analysis of text corpora or found in dictionaries
and thesauri. Finally, Section 6 discusses related work.

2. Similarity and Information Content

Let C be the set of concepts in an is-a taxonomy, permitting multiple inheritance. Intuitively,
one key to the similarity of two concepts is the extent to which they share information, indicated in an is-a taxonomy by a highly specific concept that subsumes them both. The
edge-counting method captures this indirectly, since if the minimal path of is-a links between two nodes is long, that means it is necessary to go high in the taxonomy, to more
abstract concepts, in order to find a least upper bound. For example, in WordNet, nickel
and dime are both subsumed by coin, whereas the most specific superclass that nickel
and credit card share is medium of exchange (see Figure 1). In a feature-based setting
(e.g., Tversky, 1977), this would be reected by explicit shared features: nickels and dimes
96

fiInformation-Based Semantic Similarity

MEDIUM OF EXCHANGE
MONEY
CASH

CREDIT

COIN
NICKEL

DIME

CREDIT CARD

Figure 1: Fragment of the WordNet taxonomy. Solid lines represent is-a links; dashed lines
indicate that some intervening nodes were omitted to save space.
are both small, round, metallic, and so on. These features are captured implicitly by the
taxonomy in categorizing nickel and dime as subordinates of coin.
By associating probabilities with concepts in the taxonomy, it is possible to capture
the same idea as edge-counting, but avoiding the unreliability of edge distances. Let the
taxonomy be augmented with a function p : C ! [0; 1], such that for any c 2 C , p(c) is the
probability of encountering an instance of concept c. This implies that p is monotonically
nondecreasing as one moves up the taxonomy: if c1 is-a c2, then p(c1 )  p(c2). Moreover,
if the taxonomy has a unique top node then its probability is 1.
Following the standard argumentation of information theory (Ross, 1976), the information content of a concept c can be quantified as negative the log likelihood, , log p(c).
Notice that quantifying information content in this way makes intuitive sense in this setting: as probability increases, informativeness decreases; so the more abstract a concept,
the lower its information content. Moreover, if there is a unique top concept, its information
content is 0.
This quantitative characterization of information provides a new way to measure semantic similarity. The more information two concepts share, the more similar they are, and the
information shared by two concepts is indicated by the information content of the concepts
that subsume them in the taxonomy. Formally, define
max [, log p(c)] ;
sim(c1; c2) =
(1)
c 2 S (c1 ; c2)

where S (c1; c2) is the set of concepts that subsume both c1 and c2. A class that achieves the
maximum value in Equation 1 will be termed a most informative subsumer; most often there
is a unique most informative subsumer, although this need not be true in the general case.
Taking the maximum with respect to information content is analogous to taking the first
intersection in semantic network marker-passing or the shortest path with respect to edge
distance (cf. Quillian, 1968; Rada et al., 1989); a generalization from taking the maximum
to taking a weighted average is introduced in Section 3.4.
Notice that although similarity is computed by considering all upper bounds for the two
concepts, the information measure has the effect of identifying minimal upper bounds, since
no class is less informative than its superordinates. For example, in Figure 1, coin, cash,
etc. are all members of S (nickel; dime), but the concept that is structurally the minimal
97

fiResnik

PERSON
p=.2491
info=2.005

ADULT
p=.0208
info=5.584

FEMALE_PERSON
p=.0188
info=5.736

PROFESSIONAL
p=.0079
info=6.993

ACTOR1
p=.0027
info=8.522

INTELLECTUAL
p=.0113
info=6.471

DOCTOR2
p=.0005
info=10.84

NURSE2
p=.0001
info=13.17

HEALTH_PROFESSIONAL
p=.0022
info=8.844

DOCTOR1
p=.0018
info=9.093

GUARDIAN
p=.0058
info=7.434

LAWYER
p=.0007
info=10.39

NURSE1
p=.0001
info=12.94

Figure 2: Another fragment of the WordNet taxonomy
upper bound, coin, will also be the most informative. This can make a difference in cases
of multiple inheritance: two distinct ancestor nodes may both be minimal upper bounds,
as measured using distance in the graph, but those two nodes might have very different
values for information content. Also notice that in is-a taxonomies such as WordNet,
where there are multiple sub-taxonomies but no unique top node, asserting zero similarity
for concepts in separate sub-taxonomies (e.g., liberty, aorta) is equivalent to unifying
the sub-taxonomies by creating a virtual topmost concept.
In practice, one often needs to measure word similarity , rather than concept similarity.
Using s(w) to represent the set of concepts in the taxonomy that are senses of word w,
define
wsim(w1; w2) = cmax
(2)
1; c2 [sim(c1; c2)] ;

where c1 ranges over s(w1) and c2 ranges over s(w2). This is consistent with Rada et al.'s
(1989) treatment of \disjunctive concepts" using edge-counting: they define the distance
between two disjunctive sets of concepts as the minimum path length from any element of
the first set to any element of the second. Here, the word similarity is judged by taking
the maximal information content over all concepts of which both words could be an instance. To take an example, consider how the word similarity wsim(doctor, nurse) would
be computed, using the taxonomic information in Figure 2. (Note that only noun senses
are considered here.) By Equation 2, we must consider all pairs of concepts hc1; c2i, where
c1 2 fdoctor1; doctor2g and c2 2 fnurse1; nurse2g, and for each such pair we must
compute the semantic similarity sim(c1 ,c2) according to Equation 1. Table 1 illustrates the
computation.
98

fiInformation-Based Semantic Similarity

c1 (description)

c2 (description)

sim(c1 ,c2)
doctor1 (medical) nurse1 (medical) health professional
8.844
doctor1 (medical) nurse2 (nanny)
person
2.005
doctor2 (Ph.D.) nurse1 (medical)
person
2.005
doctor2 (Ph.D.) nurse2 (nanny)
person
2.005
subsumer

Table 1: Computation of similarity for doctor and nurse
As the table shows, when all the senses for doctor are considered against all the senses
for nurse, the maximum value is 8.844, via health professional as a most informative
subsumer; this is, therefore, the value of word similarity for doctor and nurse.1

3. Evaluation

This section describes a simple, direct method for evaluating semantic similarity, using
human judgments as the basis for comparison.

3.1 Implementation

The work reported here used WordNet's taxonomy of concepts represented by nouns (and
compound nominals) in English.2 Frequencies of concepts in the taxonomy were estimated
using noun frequencies from the Brown Corpus of American English (Francis & Kucera,
1982), a large (1,000,000 word) collection of text across genres ranging from news articles
to science fiction. Each noun that occurred in the corpus was counted as an occurrence of
each taxonomic class containing it.3 For example, in Figure 1, an occurrence of the noun
dime would be counted toward the frequency of dime, coin, cash, and so forth. Formally,
X
freq(c) =
count(n);
(3)
n2words(c)
where words(c) is the set of words subsumed by concept c. Concept probabilities were
computed simply as relative frequency:
(4)
p^(c) = freq(c) ;

N

where N was the total number of nouns observed (excluding those not subsumed by any
WordNet class, of course). Naturally the frequency estimates in Equation 3 would be
1. The taxonomy in Figure 2 is a fragment of WordNet version 1.6, showing real quantitative information
computed using the method described below. The \nanny" sense of nurse (nursemaid, a woman who is
the custodian of children) is primarily a British usage. The example omits two other senses of doctor in
WordNet: a theologian in the Roman Catholic Church, and a game played by children. WordNet does
not use node labels like doctor1, but I have created such labels here for the sake of readability.
2. Concept as used here refers to what Miller et al. (1990) call a synset, essentially a node in the taxonomy.
The experiment reported in this section used the noun taxonomy from WordNet version 1.4, which has
approximately 50,000 nodes.
3. Plural nouns counted as instances of their singular forms.

99

fiResnik

improved by taking into account the intended sense of each noun in the corpus | for
example, an instance of crane can be a bird or a machine, but not both. Sense-tagged
corpora are generally not available, however, and so the frequency estimates are done using
this weaker but more generally applicable technique.
It should be noted that the present method of associating probabilities with concepts in
a taxonomy is not based on the notion of a single random variable ranging over all concepts
| were that the case, the \credit" for each noun occurrence would be distributed over all
concepts for the noun, and the counts normalized across the entire taxonomy to sum to 1.
(That is the approach taken in Resnik, 1993a, also see Resnik, 1998b for discussion.) In
assigning taxonomic probabilities for purposes of measuring semantic similarity, the present
model associates a separate, binomially distributed random variable with each concept.4
That is, from the perspective of any given concept c, an observed noun either is or is
not an instance of that concept, with probabilities p(c) and 1 , p(c), respectively. Unlike a
model in which there is a single multinomial variable ranging over the entire set of concepts,
this formulation assigns probability 1 to the top concept of the taxonomy, leading to the
desirable consequence that its information content is zero.

3.2 Task

Although there is no standard way to evaluate computational measures of semantic similarity, one reasonable way to judge would seem to be agreement with human similarity ratings.
This can be assessed by using a computational similarity measure to rate the similarity of
a set of word pairs, and looking at how well its ratings correlate with human ratings of the
same pairs.
An experiment by Miller and Charles (1991) provided appropriate human subject data
for the task. In their study, 38 undergraduate subjects were given 30 pairs of nouns that
were chosen to cover high, intermediate, and low levels of similarity (as determined using
a previous study, Rubenstein & Goodenough, 1965), and those subjects were asked to
rate \similarity of meaning" for each pair on a scale from 0 (no similarity) to 4 (perfect
synonymy). The average rating for each pair thus represents a good estimate of how similar
the two words are, according to human judgments.5
In order to get a baseline for comparison, I replicated Miller and Charles's experiment,
giving ten subjects the same 30 noun pairs. The subjects were all computer science graduate
students or postdoctoral researchers at the University of Pennsylvania, and the instructions
were exactly the same as used by Miller and Charles, the main difference being that in
this replication the subjects completed the questionnaire by electronic mail (though they
were instructed to complete the whole task in a single uninterrupted sitting). Five subjects
received the list of word pairs in a random order, and the other five received the list in the
reverse order. The correlation between the Miller and Charles mean ratings and the mean
ratings in my replication was .96, quite close to the .97 correlation that Miller and Charles
obtained between their results and the ratings determined by the earlier study.
4. This is similar in spirit to the way probabilities are used in a Bayesian network.
5. An anonymous reviewer points out that human judgments on this task may be inuenced by prototypicality, e.g., the pair bird/robin would likely yield higher ratings than bird/crane. Issues of this kind are
briey touched on in Section 6, but for the most part they are ignored here since prototypicality, like
topical relatedness, is not captured in most is-a taxonomies.

100

fiInformation-Based Semantic Similarity

For each subject in my replication, I computed how well his or her ratings correlated
with the Miller and Charles ratings. The average correlation over the 10 subjects was
r = 0:88, with a standard deviation of 0.08.6 This value represents an upper bound on
what one should expect from a computational attempt to perform the same task.
For purposes of evaluation, three computational similarity measures were used. The
first is the similarity measurement using information content proposed in the previous section. The second is a variant on the edge-counting method, converting it from distance to
similarity by subtracting the path length from the maximum possible path length:


wsimedge (w1; w2) = (2  max) , cmin
len(c1; c2)
;c
1

2



(5)

where c1 ranges over s(w1), c2 ranges over s(w2), max is the maximum depth of the taxonomy, and len(c1; c2) is the length of the shortest path from c1 to c2 . (Recall that s(w)
denotes the set of concepts in the taxonomy that represent senses of word w.) If all senses
of w1 and w2 are in separate sub-taxonomies of WordNet their similarity is taken to be
zero. Note that because correlation is used as the evaluation metric, the conversion from a
distance to a similarity can be viewed as an expository convenience, and does not affect the
results: although the sign of the correlation coecient changes from positive to negative,
its magnitude turns out to be just the same regardless of whether or not the minimum path
length is subtracted from (2  max).
The third point of comparison is a measure that simply uses the probability of a concept,
rather than the information content, to define semantic similarity of concepts
max [1 , p(c)]
simp(c)(c1; c2) =
(6)
c 2 S (c1 ; c2)

and the corresponding measure of word similarity:

h

i

wsimp(c)(w1; w2) = cmax
simp(c)(c1; c2) ;
;c
1

2

(7)

where c1 ranges over s(w1) and c2 ranges over s(w2) in Equation 7. The probability-based
similarity score is included in order to assess the extent to which similarity judgments might
be sensitive to frequency per se rather than information content. Again, the difference
between maximizing 1 , p(c) and minimizing p(c) turns out not to affect the magnitude of
the correlation. It simply ensures that the value can be interpreted as a similarity value,
with high values indicating similar words.

3.3 Results

Table 2 summarizes the experimental results, giving the correlation between the similarity
ratings and the mean ratings reported by Miller and Charles. Note that, owing to a noun
missing from the WordNet 1.4 taxonomy, it was only possible to obtain computational
similarity ratings for 28 of the 30 noun pairs; hence the proper point of comparison for
human judgments is not the correlation over all 30 items (r = :88), but rather the correlation
over the 28 included pairs (r = :90). The similarity ratings by item are given in Table 3.
6. Inter-subject correlation in the replication, estimated using leaving-one-out resampling (Weiss & Kulikowski, 1991), was r = :90; stdev = 0:07.

101

fiResnik

Similarity method
Correlation
Human judgments (replication) r = :9015
Information content
r = :7911
Probability
r = :6671
Edge-counting
r = :6645
Table 2: Summary of experimental results.
Word Pair
car
gem
journey
boy
coast
asylum
magician
midday
furnace
food
bird
bird
tool
brother
crane
lad
journey
monk
food
coast
forest
monk
coast
lad
chord
glass
noon
rooster

automobile
jewel
voyage
lad
shore
madhouse
wizard
noon
stove
fruit
cock
crane
implement
monk
implement
brother
car
oracle
rooster
hill
graveyard
slave
forest
wizard
smile
magician
string
voyage

Miller and Charles Replication
wsim wsimedge wsimp(c)
means
means
3.92
3.9 8.0411
30 0.9962
3.84
3.5 14.9286
30 1.0000
3.84
3.5 6.7537
29 0.9907
3.76
3.5 8.4240
29 0.9971
3.70
3.5 10.8076
29 0.9994
3.61
3.6 15.6656
29 1.0000
3.50
3.5 13.6656
30 0.9999
3.42
3.6 12.3925
30 0.9998
3.11
2.6 1.7135
23 0.6951
3.08
2.1 5.0076
27 0.9689
3.05
2.2 9.3139
29 0.9984
2.97
2.1 9.3139
27 0.9984
2.95
3.4 6.0787
29 0.9852
2.82
2.4 2.9683
24 0.8722
1.68
0.3 2.9683
24 0.8722
1.66
1.2 2.9355
26 0.8693
1.16
0.7 0.0000
0 0.0000
1.10
0.8 2.9683
24 0.8722
0.89
1.1 1.0105
18 0.5036
0.87
0.7 6.2344
26 0.9867
0.84
0.6 0.0000
0 0.0000
0.55
0.7 2.9683
27 0.8722
0.42
0.6 0.0000
0 0.0000
0.42
0.7 2.9683
26 0.8722
0.13
0.1 2.3544
20 0.8044
0.11
0.1 1.0105
22 0.5036
0.08
0.0 0.0000
0 0.0000
0.08
0.0 0.0000
0 0.0000
Table 3: Semantic similarity by item.
102

fiInformation-Based Semantic Similarity

n1
tobacco
tobacco
tobacco

n2
wsim(n1 ,n2) subsumer
alcohol
7.63
drug
sugar
3.56 substance
horse
8.26 narcotic

Table 4: Similarity with tobacco computed by maximizing information content

3.4 Discussion

The experimental results in the previous section suggest that measuring semantic similarity
using information content provides results that are better than the traditional method of
simply counting the number of intervening is-a links.
The measure is not without its problems, however. Like simple edge-counting, the
measure sometimes produces spuriously high similarity measures for words on the basis of
inappropriate word senses. For example, Table 4 shows the word similarity for several words
with tobacco. Tobacco and alcohol are similar, both being drugs, and tobacco and sugar are
less similar, though not entirely dissimilar, since both can be classified as substances. The
problem arises, however, in the similarity rating for tobacco with horse: the word horse can
be used as a slang term for heroin, and as a result information-based similarity is maximized,
and path length minimized, when the two words are both categorized as narcotics. This is
contrary to intuition.
Cases like this are probably relatively rare. However, the example illustrates a more
general concern: in measuring similarity between words, it is really the relationship among
word senses that matters, and a similarity measure should be able to take this into account.
In the absence of a reliable algorithm for choosing the appropriate word senses, the most
straightforward way to do so in the information-based setting is to consider all concepts
to which both nouns belong rather than taking just the single maximally informative class.
This suggests defining a measure of weighted word similarity as follows:
wsimff(w1; w2) =

X

i

ff(ci)[, log p(ci)];

(8)

where fcig is the set of concepts dominating both w1 and w2 in any sense of either word, and
ff is a weighting function over concepts such that Pi ff(ci) = 1. This measure of similarity

takes more information into account than the previous one: rather than relying on the
single concept with maximum information content, it allows each class representing shared
properties to contribute information content according to the value of ff(ci). Intuitively,
these ff values measure relevance. For example, in computing wsimff (tobacco,horse), the
ci would range over all concepts of which tobacco and horse are both instances, including
narcotic, drug, artifact, life form, etc. In an everyday context one might expect low
values for ff(narcotic) and ff(drug), but in the context of, say, a newspaper article about
drug dealers, the weights of these concepts might be quite high. Although it is not possible
to include weighted word similarity in the comparison of Section 3, since the noun pairs are
judged without context, Section 4 provides further discussion and a weighting function ff
designed for a particular natural language processing task.
103

fiResnik

4. Using Taxonomic Similarity in Resolving Syntactic Ambiguity

Having considered a direct evaluation of the information-based semantic similarity measure,
I now turn to the application of the measure in resolving syntactic ambiguity.

4.1 Clues for Resolving Coordination Ambiguity

Syntactic ambiguity is a pervasive problem in natural language. As Church and Patil
(1982) point out, the class of \every way ambiguous" syntactic constructions | those for
which the number of analyses is the number of binary trees over the terminal elements |
includes such frequent constructions as prepositional phrases, coordination, and nominal
compounds. In the last several years, researchers in natural language have made a great
deal of progress in using quantitative information from text corpora to provide the needed
constraints. Progress on broad-coverage prepositional phrase attachment ambiguity has
been particularly notable, now that the dominant approach has shifted from structural
strategies to quantitative analysis of lexical relationships (Whittemore, Ferrara, & Brunner,
1990; Hindle & Rooth, 1993; Brill & Resnik, 1994; Ratnaparkhi & Roukos, 1994; Li & Abe,
1995; Collins & Brooks, 1995; Merlo, Crocker, & Berthouzoz, 1997). Noun compounds have
received comparatively less attention (Kobayasi, Takunaga, & Tanaka, 1994; Lauer, 1994,
1995), as has the problem of coordination ambiguity (Agarwal & Boggess, 1992; Kurohashi
& Nagao, 1992).
In this section, I investigate the role of semantic similarity in resolving coordination
ambiguities involving nominal compounds. I began with noun phrase coordinations of the
form n1 and n2 n3, which admit two structural analyses, one in which n1 and n2 are the
two noun phrase heads being conjoined (1a) and one in which the conjoined heads are n1
and n3 (1b).
(1) a. a (bank and warehouse) guard
b. a (policeman) and (park guard)
Identifying which two head nouns are conjoined is necessary in order to arrive at a correct
interpretation of the phrase's content. For example, analyzing (1b) according to the structure of (1a) could lead a machine translation system to produce a noun phrase describing
somebody who guards both policemen and parks. Analyzing (1a) according to the structure of (1b) could lead an information retrieval system to miss this phrase when looking for
queries involving the term bank guard.
Kurohashi and Nagao (1992) point out that similarity of form and similarity of meaning
are important cues to conjoinability. In English, similarity of form is to a great extent
captured by agreement in number (singular vs. plural):
(2)

a. several business and university groups
b. several businesses and university groups
Similarity of form between candidate conjoined heads can thus be thought of as a Boolean
variable: number agreement is either satisfied by the candidate heads or it is not.
Similarity of meaning of the conjoined heads also appears to play an important role:
(3)

a. a television and radio personality
104

fiInformation-Based Semantic Similarity

b. a psychologist and sex researcher
Clearly television and radio are more similar than television and personality; correspondingly for psychologist and researcher. This similarity of meaning is captured well by semantic
similarity in a taxonomy, and thus a second variable to consider when evaluating a coordination structure is semantic similarity as measured by overlap in information content between
the two head nouns.
In addition, for the constructions considered here, the appropriateness of noun-noun
modification is relevant:
(4)

a. mail and securities fraud
b. corn and peanut butter

One reason we prefer to conjoin mail and securities is that mail fraud is a salient compound
nominal phrase. On the other hand, corn butter is not a familiar concept; compare to the
change in perceived structure if the phrase were corn and peanut crops. In order to measure
the appropriateness of noun-noun modification, I use a quantitative measure of selectional
fit called selectional association (Resnik, 1996), which takes into account both lexical cooccurrence frequencies and semantic class membership in the WordNet taxonomy. Briey,
the selectional association of a word w with a WordNet class c is given by
p(cjw) log p(cjw)
A(w; c) = D(p(C jw) k p(p(Cc) ))

(9)

where D(p1 k p2) is the Kullback-Leibler distance (relative entropy) between probability
distributions p1 and p2. Intuitively, A(w, c) is measuring the extent to which class c is
predicted by word w; for example, A(wool, clothing) would have a higher value than,
say, A(wool, person). The selectional association A(w1; w2) of two words is defined as
the maximum of A(w1; c) taken over all classes c to which w2 belongs. For example,
A(wool, glove) would most likely be equal to A(wool, clothing), as compared to, say,
A(wool, sports equipment) | the latter value corresponding to the sense of glove as
something used in baseball or in boxing. (See Li & Abe, 1995, for an approach in which
selectional relationships are modeled using conditional probability.) A simple way to treat
selectional association as a variable in resolving coordination ambiguities is to prefer analyses that include noun-noun modifications with very strong anities (e.g., bank as a modifier
of guard) and to disprefer very weak noun-noun relationships (e.g., corn as a modifier of
butter). Thresholds defining \strong" and \weak" are parameters of the algorithm, defined
below.

4.2 Resolving Coordination Ambiguity: First Experiment

I investigated the roles of these sources of evidence by conducting a straightforward disambiguation experiment using naturally occurring linguistic data. Two sets of 100 noun
phrases of the form [NP n1 and n2 n3] were extracted from the parsed Wall Street Journal
(WSJ) corpus, as found in the Penn Treebank (Marcus, Santorini, & Marcinkiewicz, 1993).
These were disambiguated by hand, with one set used for development and the other for
105

fiResnik

Source of evidence Conjoined Condition
Number agreement n1 and n2 number(n1) = number(n2) and number(n1) 6= number(n3)
n1 and n3 number(n1) = number(n3) and number(n1) 6= number(n2)
undecided otherwise
Semantic similarity n1 and n2 wsim(n1,n2) > wsim(n1,n3)
n1 and n3 wsim(n1,n3) > wsim(n1,n2)
undecided otherwise
Noun-noun
n1 and n2 A(n1,n3) >  or A(n3,n1) > 
modification
n1 and n3 A(n1,n3) <  or A(n3,n1) < 
undecided otherwise

Table 5: Rules for number agreement, semantic similarity, and noun-noun modification in
resolving syntactic ambiguity of noun phrases n1 and n2 n3
testing.7 A set of simple transformations were applied to all WSJ data, including the mapping of all proper names to the token someone, the expansion of month abbreviations, and
the reduction of all nouns to their root forms.
Number agreement was determined using a simple analysis of suxes in combination
with WordNet's lists of root nouns and irregular plurals.8 Semantic similarity was determined using the information-based measure of Equation (2) | the noun class probabilities
of Equation (1) were estimated using a sample of approximately 800,000 noun occurrences
in Associated Press newswire stories.9 For the purpose of determining semantic similarity,
nouns not in WordNet were treated as instances of the class hthingi. Appropriateness of
noun-noun modification was determined by computing selectional association (Equation 9),
using co-occurrence frequencies taken from a sample of approximately 15,000 noun-noun
compounds extracted from the WSJ corpus. (This sample did not include the test data.)
Both selection of the modifier for the head and selection of the head for the modifier were
considered by the disambiguation algorithm. Table 5 provides details of the decision rule
for each source of evidence when used independently.10
In addition, I investigated several methods for combining the three sources of information. These included: (a) a simple form of \backing off" (specifically, given the number
agreement, noun-noun modification, and semantic similarity strategies in that order, use
the choice given by the first strategy that isn't undecided); (b) taking a vote among the
three strategies and choosing the majority; (c) classifying using the results of a linear re7. Hand disambiguation was necessary because the Penn Treebank does not encode NP-internal structure.
These phrases were disambiguated using the full sentence in which they occurred, plus the previous and
following sentence, as context.
8. The experiments in this section used WordNet version 1.2.
9. I am grateful to Donald Hindle for making these data available.
10. Thresholds  = 2:0 and  = 0:0 were fixed manually based on experience with the development set
before evaluating the test data.

106

fiInformation-Based Semantic Similarity

Strategy

Default
Number agreement
Noun-noun modification
Semantic similarity
Backing off
Voting
Number agreement + default
Noun-noun modification + default
Semantic similarity + default
Backing off + default
Voting + default
Regression
ID3 Tree

Coverage (%) Accuracy (%)

100.0
53.0
75.0
66.0
95.0
89.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0

66.0
90.6
69.3
71.2
81.1
78.7
82.0
65.0
72.0
81.0
76.0
79.0
80.0

Table 6: Syntactic disambiguation for items of the form n1 and n2 n3
gression; and (d) constructing a decision tree classifier. The latter two methods are forms of
supervised learning; in this experiment the development set was used as the training data.11
The results are shown in Table 6. The development set contained a bias in favor of
conjoining n1 and n2; therefore a \default" strategy, always choosing that bracketing, was
used as a baseline for comparison. The default was also used for resolving undecided cases
in order to make comparisons of individual strategies at 100% coverage. For example,
\Number agreement + default" shows the figures obtained when number agreement is
used to make the choice and the default is selected if that choice is undecided.
Not surprisingly, the individual strategies perform reasonably well on the instances they
can classify, but coverage is poor; the strategy based on similarity of form is the most highly
accurate, but arrives at an answer only half the time. However, the heavy a priori bias makes
up the difference | to such an extent that even though the other forms of evidence have
some value, no combination beats the number-agreement-plus-default combination. On the
positive side, this shows that the ambiguity can be resolved reasonably well using a very
simple algorithm: viewed in terms of how many errors are made, number agreement makes
it possible to cut the baseline 34% error rate nearly in half to 18% incorrect analyses (a
44% reduction). On the negative side, the results fail to make a strong case that semantic
similarity can add something useful.
Before taking up this issue, let us assess the contributions of the individual strategies to
the results when evidence is combined, by further analyzing the behavior of the unsupervised
evidence combination strategies. When combining evidence by voting, a choice was made
in 89 cases. The number agreement strategy agreed with the majority vote in 57 cases,
of which 43 (75.4%) were correct; the noun-noun modification strategy agreed with the
majority in 73 cases, of which 50 (68.5%) were correct; and the semantic similarity strategy
11. What I am calling \backing off" is related in spirit to Katz's well known smoothing technique (Katz,
1987), but the \backing off" strategy used here is not quantitative. I retain the double quotes in order
to highlight the distinction.

107

fiResnik

agreed with the majority in 58 cases, of which 43 (74.1%) were correct. In the \backing off"
form of evidence combination, number agreement makes a choice for 53 cases and is correct
for 48 (90.6%); then, of those remaining undecided, noun-noun modification makes a choice
for 35 cases and is correct for 24 (68.6%); then, of those still undecided, semantic similarity
makes a choice for 7 cases of which 5 are correct (71.4%); and the remaining 5 cases are
undecided.
This analysis and the above-baseline performance of the semantic-similarity-plus-default
strategy show that semantic similarity does contain information about the correct answer:
it agrees with the majority vote a substantial portion of the time, and it selects correct
answers more often than one would expect by default for the cases it receives through
\backing off." However, because the default is correct two thirds of the time, and because
the number agreement strategy is correct nine out of ten times for the cases it can decide,
the potential contribution of semantic similarity remains suggestive rather than conclusive.
In a second experiment, therefore, I investigated a more dicult formulation of the problem
in order to obtain a better assessment.

4.3 Resolving Coordination Ambiguity: Second Experiment

In the second experiment using the same data sources, I investigated a more complex set
of coordinations, looking at noun phrases of the form n0 n1 and n2 n3. The syntactic
analyses of such phrases are characterized by the same top-level binary choice as the data
in the previous experiment, either conjoining heads n1 and n2 as in (5) or conjoining n1
and n3 as in (6).12
(5) a. freshman ((business and marketing) major)
b. (food (handling and storage)) procedures
c. ((mail fraud) and bribery) charges
(6)

a. Clorets (gum and (breath mints))
b. (baby food) and (puppy chow)

For this experiment, one set of 89 items was extracted from the Penn Treebank WSJ data
for development, and another set of 89 items was set aside for testing. The development
set showed significantly less bias than the data in the previous experiment, with 53.9% of
items conjoining n1 and n2.
The disambiguation strategies in this experiment were a more refined version of those
used in the previous experiment, as illustrated in Table 7. Number agreement was used just
as before. However, rather than employing semantic similarity and noun-noun modification
as independent strategies | something not clearly warranted given the lackluster performance of the modification strategy | the two were combined in a measure of weighted
semantic similarity as defined in Equation (8). Selectional association was used as the basis
for ff. In particular, ff1;2(c) was the greater of A(n0,c) and A(n3,c), capturing the fact that
when n1 and n2 are conjoined, the combined phrase potentially stands in a head-modifier
relationship with n0 and a modifier-head relationship with n3 . Correspondingly, ff1;3(c)
was the greater of A(n0,c) and A(n2,c), capturing the fact that the coordination of n1
12. The full 5-way classification problem for the structures in (5) and (6) was not investigated.

108

fiInformation-Based Semantic Similarity

Source of evidence Conjoined Condition
Number agreement n1 and n2 number(n1) = number(n2) and number(n1) 6= number(n3)
n1 and n3 number(n1) = number(n3) and number(n1) 6= number(n2)
undecided otherwise
Weighted semantic n1 and n2 wsimff1 2 (n1,n2) > wsimff1 3 (n1,n3)
similarity
n1 and n3 wsimff1 3 (n1,n3) > wsimff1 2 (n1,n2)
undecided otherwise
;

;

;

;

Table 7: Rules for number agreement and weighted semantic similarity in resolving syntactic ambiguity of noun phrases n0 n1 and n2 n3
Strategy

Default
Number agreement
Weighted semantic similarity
Backing off

Coverage (%) Accuracy (%)

100.0
40.4
69.7
85.4

44.9
80.6
77.4
81.6

Table 8: Syntactic disambiguation for items of the form n0 n1 and n2 n3
and n3 takes place in the context of n2 modifying n3 and of n1 (or a coordinated phrase
containing it) being modified by n0.
For example, consider an instance of the ambiguous phrase:
(7)

telecommunications products and services units.

It so happens that a high-information-content connection exists between product in its
sense as \a quantity obtained by multiplication" and unit in its sense as \a single undivided
whole." As a result, although neither of these senses is relevant for this example, nouns n1
and n3 would be assigned a high value for (unweighted) semantic similarity and be chosen
incorrectly as the conjoined heads for this example. However, the unweighted similarity computation misses an important piece of context: in any syntactic analysis conjoining product
and unit (cf. examples 6a and 6b), the word telecommunications is necessarily a modifier
of the concept identified by products. But the selectional association between telecommunications and products in its \multiplication" sense is weak or nonexistent. Weighting by
selection association, therefore, provides a way to reduce the impact of the spurious senses
on the similarity computation.
In order to combine sources of evidence, I used \backing off" (from number agreement
to weighted semantic similarity) to combine the two individual strategies. As a baseline,
results were evaluated against a simple default strategy of always choosing the group that
was more common in the development set. The results are shown in Table 8.
In this case, the default strategy defined using the development set was misleading,
yielding worse than chance accuracy. For this reason, strategy-plus-default figures are not
reported. However, even if default choices were made using the bias found in the test set,
109

fiResnik

accuracy would be only 55.1%. In contrast to the equivocal results in the first experiment,
this experiment demonstrates a clear contribution of semantic similarity: by employing
semantic similarity in those cases where the more accurate number-agreement strategy
cannot apply, it is possible to obtain equivalent or even somewhat better accuracy than
number agreement alone while at the same time more than doubling the coverage.
Comparison with previous algorithms is unfortunately not possible, since researchers on
coordination ambiguity have not established a common data set for evaluation or even a
common characterization of the problem, in contrast to the now-standard (v, n1, prep, n2)
contexts used in work on propositional phrase attachment. With that crucial caveat, it
is nonetheless interesting to note that the results obtained here are broadly consistent
with Kurohashi and Nagao (1992), who report accuracy results in the range of 80-83% at
100% coverage when analyzing a broad range of conjunctive structures in Japanese using
a combination of string matching, syntactic similarity, and thesaurus-based similarity, and
with Agarwal and Boggess (1992), who use syntactic types and structure, along with partly
domain-dependent semantic labels, to obtain accuracies in a similar range for identifying
conjuncts in English.

5. Using Taxonomic Similarity in Word Sense Selection
This section considers the application of the semantic similarity measure in resolving another
form of ambiguity: selecting the appropriate sense of a noun when it appears in the context
of other nouns that are related in meaning.

5.1 Associating Word Senses with Noun Groupings
Knowledge about groups of related words plays a role in many natural language applications.
As examples, query expansion using related words is a well studied technique in information
retrieval (e.g., Harman, 1992; Grefenstette, 1992), clusters of similar words can play a
role in smoothing stochastic language models for speech recognition (Brown, Della Pietra,
deSouza, Lai, & Mercer, 1992), classes of verbs that share semantic structure form the
basis for an approach to interlingual machine translation (Dorr, 1997), and clusterings of
related words can be used in characterizing subgroupings of retrieved documents in largescale Web searches (e.g., Digital Equipment Corporation, 1998). There is a wide body
of research on the use of distributional methods for measuring word similarity in order to
obtain groups of related words (e.g., Bensch & Savitch, 1992; Brill, 1991; Brown et al., 1992;
Grefenstette, 1992, 1994; McKeown & Hatzivassiloglou, 1993; Pereira, Tishby, & Lee, 1993;
Schutze, 1993), and thesauri such as WordNet are another source of word relationships (e.g.,
Voorhees, 1994).
Distributional techniques can sometimes do a good job of identifying groups of related
words (see Resnik, 1998b, for an overview and critical discussion), but for some tasks the
relevant relationships are not among words, but among word senses. For example, Brown
et al. (1992) illustrate the notion of a distributionally derived, \semantically sticky" cluster
using an automatically derived word group containing attorney, counsel, trial, court, and
judge. Although the semantic coherence of this cluster \pops out" for a human reader, a
naive computational system has no defense against word sense ambiguity: using this cluster
110

fiInformation-Based Semantic Similarity

for query expansion could result in retrieving documents involving advice (one sense of
counsel) and royalty (as one sense of court).13
Resnik (1998a) introduces an algorithm that uses taxonomically-defined semantic similarity in order to derive grouping relationships among word senses from grouping relationships among words. Formally, the problem can be stated as follows. Consider a set of words
W = fw1; : : :; wng, with each word wi having an associated
set Si = fsi;1 ; : : :; si;mg of posS
sible senses. Assume that there exists some set W 0  Si , representing the set of word
senses that an ideal human judge would conclude belong to the group of senses corresponding to the word grouping W . (It follows that W 0 must contain at least one representative
from each Si .) The goal is then to define a membership function ' that takes si;j , wi , and
W as its arguments and computes a value in [0; 1], representing the confidence with which
one can state that sense si;j belongs in sense grouping W 0 . Note that, in principle, nothing
precludes the possibility that multiple senses of a word are included in W 0.
For example, consider again the group
attorney, counsel, trial, court, judge.
Restricting attention to noun senses in WordNet, every word but attorney is polysemous.
Treating this word group as W , a good algorithm for computing ' should assign a value
of 1 to the unique sense of attorney, and it should assign a high value to the sense of counsel
as
a lawyer who pleads cases in court.
Similarly, it should assign high values to the senses of trial as
legal proceedings consisting of the judicial examination of issues by a competent tribunal
the determination of a person's innocence or guilt by due process of law.
It should also assign high values to the senses of court as
an assembly to conduct judicial business
a room in which a law court sits.
And it should assign a high value to the sense of judge as
a public ocial authorized to decide questions brought before a court of justice.
It should assign low values of ' to the various word senses of words in this cluster that are
associated with the group to a lesser extent or not at all. These would include the sense of
counsel as
direction or advice as to a decision or course of action;
similarly, a low value of ' should be assigned to other senses of court such as
13. See Krovetz and Kroft, 1992 and Voorhees, 1993 for experimentation and discussion of the effects of
word sense ambiguity in information retrieval.

111

fiResnik

Algorithm (Resnik, 1998a). Given W = fw1 ; : : : ; w g, a set of nouns:
n

for i and j = 1 to n, with i < j
f

v = wsim(w , w )
c = the most informative subsumer for w and w
i;j

i

j

i;j

i

j

for k = 1 to num senses(w )
if c is an ancestor of sense
increment support[i, k] by v
i

i;j

i;k

i;j

for k = 1 to num senses(w )
if c is an ancestor of sense
increment support[j, k'] by v
0

j

j;k 0

i;j

i;j

increment normalization[i] by v
increment normalization[j] by v

i;j

g

i;j

for i = 1 to n
for k = 1 to num senses(w )
f

i

if (normalization[i] > 0.0)
' = support[i, k] / normalization[i]
else
' = 1 / num senses(w )
i;k

g

i;k

i

Figure 3: Disambiguation algorithm for noun groupings
a yard wholly or partly surrounded by walls or buildings.
The disambiguation algorithm for noun groups is given in Figure 3. Intuitively, when two
polysemous words are similar, their most informative subsumer provides information about
which sense of each word is the relevant one. This observation is similar in spirit to other
approaches to word sense disambiguation based on maximizing relatedness of meaning (e.g.,
Lesk, 1986; Sussna, 1993). The key idea behind the algorithm is to consider the nouns in a
word group pairwise. For each pair the algorithm goes through all possible combinations of
the words' senses, and assigns \credit" to senses on the basis of shared information content,
as measured using the information content of the most informative subsumer.14
As an example, WordNet lists doctor as meaning either a medical doctor or someone
holding a Ph.D., and lists nurse as meaning either a health professional or a nanny, but
when the two words are considered together, the medical sense of each word is obvious to
the human reader. This effect finds its parallel in the operation of the algorithm. Given a
taxonomy like that of Figure 2, consider a case in which the set W of words contains w1 =
doctor, w2 = nurse, and w3 = actor. In the first pairwise comparison, for doctor and nurse,
14. In Figure 3, the square bracket notation highlights the fact that support is a matrix and normalization
is an array. Conceptually v and c are (triangular) matrices also; however, I use subscripts rather than
square brackets because at implementation time there is no need to implement them as such since the
values v and c are used and discarded on each pass through the double loop.
i;j

i;j

112

fiInformation-Based Semantic Similarity

the most informative subsumer is c1;2 = health professional, which has information
content v1;2 = 8.844. Therefore the support for doctor1 and nurse1 is incremented
by 8.844. Neither doctor2 nor nurse2 receives any increment in support based on this
comparison, since neither has health professional as an ancestor. In the second pairwise
comparison, the most informative subsumer for doctor and actor is c1;3 = person, with
information content v1;3 = 2.005, and so there is an increment by that amount to the
support for doctor1, doctor2, and actor1, all of which have person as an ancestor.
Similarly, in the third pairwise comparison, the most informative subsumer for nurse and
actor is also person, so nurse1, nurse2, and actor1 all have their support incremented
by 2.005. In the end, therefore, doctor1 has received support 8:884 + 2:005 out of a
possible 8:884 + 2:005 for all the pairwise comparisons in which it participated, so for that
word sense ' = 1. In contrast, doctor2 received support in the amount of 2.005 out of a
possible 8:884 + 2:005 for the comparisons in which it was involved, so the value of ' for
2:005 = 0:185.
doctor2 is 8:884+2
:005
Resnik (1998a) illustrates the algorithm of Figure 3 using word groupings from a variety
of sources, including several of the sources on distributional clustering cited above, and
evaluates the algorithm more rigorously on the task of associating WordNet senses with
nouns in Roget's thesaurus, based on their thesaurus category membership. On average, the
algorithm achieved approximately 89% of the performance of human annotators performing
the same task.15 In the remainder of this section I describe a new application of the
algorithm, and evaluate its performance.

5.2 Linking to WordNet using a Bilingual Dictionary
Multilingual resources for natural language processing can be dicult to obtain, although
some promising efforts are underway in projects like EuroWordNet (Vossen, 1998). For
many languages, however, such large-scale resources are unlikely to be available in the
near future, and individual research efforts will have to continue to build from scratch or
to adapt existing resources such as bilingual dictionaries (e.g., Klavans & Tzoukermann,
1995). In this section I describe an application of the algorithm of Figure 3 to the English
definitions in the CETA Chinese-English dictionary (CETA, 1982). The ultimate task,
being undertaken in the context of a Chinese-English machine translation project, will be
to associate Chinese vocabulary items with nodes in WordNet, much in the same way that
vocabulary in Spanish, Dutch, and Italian are associated with interlingual taxonomy nodes
derived from the American WordNet, in the EuroWordNet project; the task is also similar
to attempts to relate dictionaries and thesauri monolingually (e.g., see Section 5.3 and
Ji, Gong, & Huang, 1998). The present study investigates the extent to which semantic
similarity might be useful in partially automating the process.
15. The task was performed independently by two human judges. Treating Judge 1 as the benchmark the
accuracies achieved by Judge 2, the algorithm, and random selection were respectively 65.7%, 58.6%,
and 34.8%; treating Judge 2 as the benchmark the accuracies achieved by Judge 1, the algorithm, and
random selection were respectively 68.6%, 60.5%, and 33.3%. As the relatively low accuracies for human
judges demonstrate, disambiguation using WordNet's fine-grained senses is quite a bit more dicult than
disambiguation to the level of homographs (Hearst, 1991; Cowie, Guthrie, & Guthrie, 1992). Resnik and
Yarowsky (1997, 1999) discuss the implications of WordNet's fine-grainedness for evaluation of word
sense disambiguation, and consider alternative evaluation methods.

113

fiResnik

For example, consider the following dictionary entries:
(a)
: 1. hliti brother-in-law (husband's elder brother) 2. hregi father 3.
hregi uncle (father's elder brother) 4. uncle (form of address to an older
man)
: actress, player of female roles.
(b)
In order to associate Chinese terms such as these with the WordNet noun taxonomy, it
is important to avoid associations with inappropriate senses | for example, the word in
, should clearly not be associated with father in its WordNet senses as Church
entry (a),
Father, priest, God-the-Father, or founding father.16
Although one traditional approach to using dictionary entries has been to compute word
overlap with respect to dictionary definitions (e.g., Lesk, 1986), the English glosses in the
CETA dictionary are generally too short to take advantage of word overlap in this fashion.
However, many of the definitions do have a useful property: they possess multiple subdefinitions that are similar in meaning, as in the cases illustrated above. Although one
cannot always assume that this is so, e.g.,
(c)
: 1. case (i.e., upper case or lower case) 2. dial (of a watch, etc.),
inspection of the dictionary confirms that when multiple definitions are present they tend
more toward polysemy than homonymy.
Based on this observation, I conducted an experiment to assess the extent to which the
word sense disambiguation algorithm of Figure 3 can be used to identify relevant noun senses
in WordNet for Chinese words in the CETA dictionary, using the English definitions as the
source of similar nouns to disambiguate. Nouns heading definitional noun phrases were
extracted automatically via simple heuristic methods, for a randomly-selected sample of
100 dictionary entries containing multiple definitions to be used as a test set. For example,
the noun groups associated with the definitions above would be
(a') uncle, brother-in-law, father
(b') actress, player.
WordNet's noun database was used to automatically identify compound nominals where
possible. So, for example, a word defined as \record player" would have the compound
record player rather than player as its head noun because record player is a compound
noun known to WordNet.17
It should be noted that no attempt was made to exclude dictionary entries like (c) when
creating the test set. Since in general there is no way to automatically identify alternative
definitions distinguished by synonymy from those distinguished by homonymy, such entries
must be faced by any disambiguation algorithm for this task.
Two independent judges were recruited for assistance in annotating the test set, one a
native Chinese speaker, and the second a Chinese language expert for the United States
government. These judges independently annotated the 100 test items. For each item,
16. Annotations within the dictionary entries such as
ignored by the algorithm described in this section.
17. WordNet version 1.5 was used for this experiment.

<lit>

114

(literary),

<reg>

(regional), and the like are

fiInformation-Based Semantic Similarity

For each WordNet definition, you will see 6 boxes: 1, 2, 3, 4, 5, and is-a. For each definition:
 if you think the Chinese word can have that meaning, select the number corresponding to your
confidence in that choice, where 1 is lowest confidence and 5 is highest confidence.
 If the Chinese word cannot have that meaning, but can have a more specific meaning, select is-a.
For example, if the Chinese word means \truck" and the WordNet definition is \automotive vehicle:
self-propelled wheeled vehicle", you would select this option. (That is, it makes sense to say that
this Chinese word describes a concept that IS A KIND OF automotive vehicle.) Then pick 1,
2, 3, 4, or 5 as your confidence in this decision, again with 1 as lowest confidence and 5 as highest
confidence.
 If neither of the above cases apply for this WordNet definition, don't check off anything for this
definition.

Figure 4: Instructions for human judges selecting senses associated with Chinese words
the judge was given the Chinese word, its full CETA dictionary definition (as in examples
a{c), and a list of all the WordNet sense descriptions associated with any sense of any head
noun in the associated noun group. For example, the list corresponding to the following
dictionary definition
(d)
: urgent message, urgent dispatch
would contain the following WordNet sense descriptions, as generated via the head nouns
message and dispatch:
 message, content, subject matter, substance: what a communication that
is about something is about
 dispatch, expedition, expeditiousness, fastness: subconcept of celerity, quickness, rapidity
 dispatch, despatch, communique: an ocial report (usually sent in haste)
 message: a communication (usually brief) that is written or spoken or
signaled; \he sent a three-word message"
 dispatch, despatch, shipment: the act of sending off something
 dispatch, despatch: the murder or execution of someone
For each item, the judge was first asked whether he knew that Chinese word in that meaning;
if the response was negative, he was instructed to proceed to the next item. For items with
known words, the judges were instructed as in Figure 4.
Although the use of the is-a selection was not used in the analysis of the results, it
was important to include it because it provided the judges with a way to indicate where a
Chinese word could best be classified in the WordNet noun taxonomy, without having to
assert translational equivalence between the Chinese concept and a close WordNet (English)
concept. So, for example, a judge could classify the word
(the spring festival, lunar
new year, Chinese new year) as belonging under the WordNet sense glossed as
festival: a day or period of time set aside for feasting and celebration,
115

fiResnik

the most sensible choice given that \Chinese New Year" does not appear as a WordNet
concept. Annotating the is-a relationship for the set was also important because the algorithm being evaluated was working on groups of head nouns, thereby potentially losing
information pointing to a more specific concept reading. For example, the definition
: steel tube, steel pipe
(e)
would be given to the algorithm as a group containing head nouns tube and pipe.
Once the test set was annotated, evaluation was done according to two paradigms:
selection and filtering. In both paradigms we assume that for each entry in the test set, an
annotator has correctly specified which WordNet senses are to be considered correct, and
which are incorrect. An algorithm being tested against this set must identify, for each listed
sense, whether that sense should be included for that item or whether it should be excluded.
For example, the WordNet sense corresponding to \the murder or execution of someone"
would be identified by an annotator as incorrect for (d), and so an algorithm marking it as
\included" should be penalized.
For the selection paradigm, the goal is to identify WordNet senses to include. We can
therefore define precision in that paradigm as
of correctly included senses
Pselection = number
(10)
number of included senses
and recall as
of correctly included senses :
Rselection = number
(11)
number of correct senses
These correspond directly to the use of precision and recall in information retrieval. Precision begins with the set of senses included by some method, and computes the proportion of
these that are correct. Recall begins with the set of senses that should have been included,
and computes the proportion of these that the method actually managed to choose.
Since the number of potential WordNet senses for an item can be quite large, an equally
valid alternative to the selection paradigm is what I will call the filtering paradigm, according
to which the goal is to identify WordNet senses to exclude. One can easily imagine this
being the more relevant paradigm | for example, in a semi-automated setting where one
wishes to reduce the burden of a user selecting among alternatives. In the filtering paradigm
one can define filtering precision as
of correctly excluded senses
Pfiltering = number
number of excluded senses

(12)

of correctly excluded senses :
Rfiltering = number
number of senses labeled incorrect

(13)

and filtering recall as

In the filtering paradigm, precision begins with the set of senses that the method filtered
out and computes the proportion that were correctly filtered out. And recall in filtering
begins with the set of senses that should have been excluded (i.e. the incorrect ones) and
computes the proportion of these that the method actually managed to exclude.
116

fiInformation-Based Semantic Similarity

Sense Selection
Sense Filtering
Precision (%) Recall (%) Precision (%) Recall (%)
Random
29.5
31.2
88.0
87.1
Algorithm
36.9
69.9
93.8
79.3
Judge 2
54.8
55.6
91.9
91.7
Table 9: Evaluation using Judge 1 as the reference standard, considering items selected
with confidence 3 and above.
Judge 2
Algorithm
Random
Include Exclude Include Exclude Include Exclude
Judge 1 Include
40
32
58
25
26
57
Exclude
33
363
99
380
61
418
Table 10: Agreement and disagreement with Judge 1
Table 9 shows the precision/recall figures using the judgments of Judge 1, the native
Chinese speaker, as a reference standard, considering only known items selected with confidence 3 and above.18 The algorithm recorded all 100 items as known, and its confidence
values were scaled linearly from continuous values in range [0,1] to discrete values from 1
to 5. The table shows the algorithm's results with its choice thresholded at confidence 3,
and Figure 5 shows how recall and precision vary as the confidence threshold changes. As
a lower bound for comparison, an algorithm was implemented that considered each word
sense for each item, selecting that sense probabilistically (with complete confidence) in such
a way as to make the average number of senses per item as close as possible to the average
number of senses per item in the reference standard (1.3 senses). Figures for the random
baseline are the average over 10 runs. Table 10 illustrates the choices underlying those
figures; for example, there were 26 senses that the random procedure chose to include that
were also included by Judge 1.
The fact that Judge 2 has such low precision and recall for selection indicates that
matching the choices of an independent judge is indeed a dicult task. This is unsurprising, given previous experience with the problem of selecting among WordNet's fine-grained
senses (Resnik, 1998a; Resnik & Yarowsky, 1997). The results clearly show that the algorithm is better than the baseline, but also indicate that it is overgenerating senses, which
hurts selection precision. In terms of filtering, when the algorithm chooses to filter out a
sense it tends to do so reliably (filtering precision). However, its propensity toward overgeneration is reected in its below-baseline performance on filtering recall; that is, the algorithm
is choosing to allow in senses that it should be filtering out.
18. Judge 1, the native speaker of Chinese, identified 65 of the words as known to him; Judge 2 identified
69. This on-line dictionary was constructed from a large variety of lexical resources, and includes a great
many uncommon words, archaic usages, regionalisms, and the like.

117

fiResnik

Filtering: Algorithm
Human
Random
Selection: Algorithm
Human
Random

Precision

1
0.8
0.6
0.4
0.2

0.2

0.4

0.6

0.8
Recall

1

Figure 5: Precision/recall curves using Judge 1 as the reference standard, varying the confidence threshold
This pattern of results suggests that the best use of this algorithm at its present level
of performance would be as a filter for a lexical acquisition process with a human in the
loop, dividing candidate WordNet senses for dictionary entries according to higher and
lower priority. For Chinese-English dictionary entries that serve as appropriate input to the
algorithm (of which there are approximately 37000 in the CETA dictionary), if a WordNet
sense is not selected by the algorithm with a confidence at least equal to 3 it should be
demoted to the lower priority group in the presentation of alternatives, since the algorithm's
choice to exclude a sense is correct approximately 93% of the time. Those senses that are
selected by the algorithm are not necessarily to be included | the human judge is still
needed to make the selection, since selection precision is low | but the algorithm tends to
err on the side of caution, and so correct senses will be found in the higher priority group
some 70% of the time.

5.3 Linking to WordNet from an English Dictionary/Thesaurus

The results on WordNet sense selection using a bilingual dictionary demonstrate that the
algorithm of Figure 3 does a good job of assigning low scores to WordNet senses that should
be filtered out, even if it should probably not be trusted to make categorical decisions. One
application proposed as suitable, therefore, was helping to identify which senses should be
filtered out within a semi-automated process of lexical acquisition. Here I describe a closely
related, real-world application for which the algorithm has been deployed: adding pointers
into WordNet from an on-line dictionary/thesaurus on the Web.
The context of this application is the Wordsmyth English Dictionary-Thesaurus (WEDT,
http://www.wordsmyth.net/), an on-line educational dictionary aliated with the ARTFL
text database project (http://humanities.uchicago.edu/ARTFL/; Morrissey, 1993). It
has been designed to be useful in educational contexts, and, as part of that design, it
integrates a thesaurus within the structure of the dictionary. As illustrated in Figure 6,
118

fiInformation-Based Semantic Similarity

bar

SYL:
PRO:
POS:
DEF:
EXA:
EXA:
EXA:
SYN:
SIM:
DEF:
SYN:
SIM:
..
.

bar1
bar
noun
1. a length of solid material, usu. rectangular or cylindrical:
a bar of soap;
a candy bar;
an iron bar.
rod (1), stick1 (1,2,3)
pole1 , shaft, stake1 , ingot, block, rail1 , railing,
crowbar, jimmy, lever
2. anything that acts as a restraint or hindrance.
block (10), hindrance (1), obstruction (1), impediment (1),
obstacle, barrier (1,3), stop (5)
barricade, blockade, deterrent, hurdle, curb, stumbling
block, snag, jam1 , shoal1 , reef1 , sandbar

Figure 6: Example from the Wordsmyth English Dictionary-Thesaurus (WEDT)
WEDT contains traditional dictionary information, such as part of speech, pronunciation,
and definitional information, but in many cases also includes pointers to synonyms (SYN)
or similar words (SIM). Within the on-line dictionary, these thesaurus items are hyperlinks
| for example, stake1 is a link to the first WEDT entry for stake | and parenthetical
numbers refer to specific definitions within an entry.
The thesaurus-like grouping of similar words provides an opportunity to exploit the
algorithm for disambiguating noun groupings by automatically linking WEDT entries to
WordNet. The value in linking these two resources comes from their compatability, in that
both have properties of both a thesaurus and a dictionary, as well as from their complementarity: beyond being an alternative source of definitional information and lists of synonyms,
WordNet provides ordering of word senses by frequency, estimates of word familiarity, partof relationships, and of course the overall taxonomic organization illustrated in Figures 1
and 2. Figure 7 shows how taxonomic information is presented using the WordNet Web
server (http://www.cogsci.princeton.edu/cgi-bin/webwn/).
In a collaboration with WEDT and ARTFL, I have taken the noun entries from the
WEDT dictionary and, for each grouping of similar words, added a set of experimental
hyperlinks to WordNet entries on the WordNet Web server. Figure 8 shows how the experimental WordNet links (XWN) look to the WEDT user. Links to WordNet senses, such as
pole1, appear together with the confidence level assigned by the sense disambiguation algorithm; senses with confidence less than a threshold are not presented.19 When an XWN
hyperlink is selected by the user, WordNet taxonomic information for the selected sense
appears in a parallel browser window, as in Figure 7.
From this window, the user has an entry point into the other capabilities of the WordNet
web server. For example, one might choose to look at all the WordNet senses for pole as
19. The current threshold, 0.1, was chosen manually. It may be sub-optimal but I have found that it works
well in practice.

119

fiResnik

Sense 1
pole
(a long (usually round) rod of wood or metal or plastic)
=> rod
(a long thin implement made of metal or wood)
=> implement
(a piece of equipment or tool used to effect an end)
=> instrumentality, instrumentation
(an artifact (or system of artifacts) that is
instrumental in accomplishing some end)
=> artifact, artefact
(a man-made object)
=> object, physical object
(a physical (tangible and visible) entity; ``it was
full of rackets, balls and other objects'')
=> entity, something
(anything having existence (living or nonliving))

Figure 7: WordNet entry (hypernyms) for pole1
bar

SYL:
PRO:
POS:
DEF:
EXA:
EXA:
EXA:
SYN:
SIM:
XWN:
DEF:
SYN:
SIM:
XWN:

..
.

bar1
bar
noun
1. a length of solid material, usu. rectangular or cylindrical:
a bar of soap;
a candy bar;
an iron bar.
rod (1), stick1 (1,2,3)
pole1 , shaft, stake1 , ingot, block, rail1 , railing,
crowbar, jimmy, lever
pole1 (0.82) ingot1 (1.00) block1 (0.16) rail1 (0.39)
railing1 (1.00) crowbar1 (1.00)
jimmy1 (1.00) lever1 (0.67) lever2(0.23) lever3(0.15)
2. anything that acts as a restraint or hindrance.
block (10), hindrance (1), obstruction (1), impediment (1),
obstacle, barrier (1,3), stop (5)
barricade, blockade, deterrent, hurdle, curb, stumbling
block, snag, jam1 , shoal1 , reef1 , sandbar
barricade1 (1.00) barricade2(1.00) blockade1 (0.25)
blockade2 (0.75) deterrent1 (1.00) hurdle1 (0.50)
hurdle2 (0.43) curb1 (0.56) curb2 (0.56)
curb3 (0.29) curb4 (0.44) stumbling block1 (1.00)
snag1 (1.00) jam1 (0.27) shoal1 (0.23) shoal2(0.91)
reef1 (1.00) sandbar1 (1.00)

Figure 8: Example from WEDT with experimental WordNet links
120

fiInformation-Based Semantic Similarity

1. pole { (a long (usually round) rod of wood or metal or plastic)
2. Pole { (a native or inhabitant of Poland)
3. pole { (one of two divergent or mutually exclusive opinions; \they are at opposite poles" or \they are
poles apart")
4. perch, rod, pole { ((British) a linear measure of 16.5 feet)
5. perch, rod, pole { (a square rod of land)
6. pole, celestial pole { (one of two points of intersection of the Earth's axis and the celestial sphere)
7. pole { (one of two antipodal points where the Earth's axis of rotation intersects the Earth's surface)
8. terminal, pole { (a point on an electrical device (such as a battery) at which electric current enters
or leaves)
9. pole { (a long fiberglass implement used for pole vaulting)
10. pole, magnetic pole { (one of the two ends of a magnet where the magnetism seems to be concentrated)

Figure 9: List of WordNet senses for pole
a noun, displayed as in Figure 9. Notice that if a user of WEDT had simply gone directly
to the WordNet server to look up pole, the full list of 10 senses would have appeared
with no indication of which are most potentially related to the WEDT dictionary entry
under consideration. In contrast, the WEDT hyperlinks, introduced via the sense selection
algorithm, filter out the majority of the irrelevant senses and provide the user a measure of
confidence in selecting among those that remain.
Although no formal evaluation of the WEDT/WordNet connection has been attempted,
the results of the bilingual dictionary experiment suggest that this application of word
sense disambiguation | filtering out the least relevant senses, and then leaving the user
in the loop | is a task for which the sense disambiguation algorithm is well suited. This
is supported by user feedback on the XWN feature of WEDT, which has been favorable
(Robert Parks, personal communication). The site has been growing in popularity, with a
current estimate of 1000-1500 hits per day.

6. Related Work
There is an extensive literature on measuring similarity in general, and on word similarity
in particular; for a classic paper see Tversky (1977). Recent work in information retrieval
and computational linguistics has emphasized a distributional approach, in which words
are represented as vectors in a space of features and similarity measures are defined in
terms of those vectors; see Resnik (1998b) for discussion, and Lee (1997) for a good recent
example. Common to the traditional and the distributional approaches is the idea that word
or concept representations include explicit features, whether those features are specified in
a knowledge-based fashion (e.g., dog might have features like mammal, loyal) or defined
in terms of distributional context (e.g., dog might have features like \observed within 5
words of howl). This representational assumption contrasts with the assumptions embodied
in a taxonomic representation, where most often the is-a relation stands between nondecomposed concepts. The two are not inconsistent, of course, since concepts in a taxonomy
121

fiResnik

sometimes can be decomposed into explicit features, and the is-a relation, as it is usually
interpreted, implies inheritance of features whether they are explicit or implicit. In that
respect, the traditional approach of counting edges can be viewed as a particularly simple
approximation to a similarity measure based on counting feature differences, under the
assumption that an edge exists to indicate a difference of at least one feature.
Information-theoretic concepts and techniques have, in recent years, emerged from the
speech recognition community to find wide application in natural language processing; e.g.,
see Church and Mercer (1993). The information of an event is a fundamental notion in
stochastic language modeling for speech recognition, where the contribution of a correct
word prediction based on its conditional probability, p(wordjcontext), is measured as the
information conveyed by that prediction, , log p(wordjcontext). This forms the basis for
standard measures of language model performance, such as cross entropy. Frequency of
shared and unshared features has also long been a factor in computing similarity over vector representations. The inverse document frequency (idf) for term weighting in information
retrieval makes use of logarithmic scaling, and serves to identify terms that do not discriminate well among different documents, a concept very similar in spirit to the idea that such
terms have low information content (Salton, 1989).
Although the counting of edges in is-a taxonomies seems to be something many people
have tried, there seem to be few published descriptions of attempts to directly evaluate
the effectiveness of this method. A number of researchers have attempted to make use of
conceptual distance in information retrieval. For example, Rada et al. (1989, 1989) and Lee
et al. (1993) report experiments using conceptual distance, implemented using the edgecounting metric, as the basis for ranking documents by their similarity to a query. Sussna
(1993) uses semantic relatedness measured with WordNet in word sense disambiguation,
defining a measure of distance that weights different types of links and also explicitly takes
depth in the taxonomy into account.
Following the original proposal to measure semantic similarity in a taxonomy using
information content (Resnik, 1993b, 1993a), a number of related proposals have been explored. Leacock and Chodorow (1994) define a measure resembling information content,
but using the normalized path length between the two concepts being compared rather than
the probability of a subsuming concept. Specifically, they define
2
min
len(c1 ; c2) 3
c
;
c
(14)
wsimndist (w1; w2) = , log 4 1 (22 max) 5 :
(The notation above is the same as for Equation (5).) In addition to this definition, they
also include several special cases, most notably to avoid infinite similarity when c1 and
c2 are exact synonyms and thus have a path length of 0. Leacock and Chodorow have
experimented with this measure and the information content measure described here in the
context of word sense disambiguation, and found that they yield roughly similar results.
Implementing their method and testing it on the task reported in Section 3, I found that
it actually outperformed the information-based measure slightly on that data set; however,
in a follow-up experiment using a different and larger set of noun pairs (100 items), the
information-based measure performed significantly better (Table 11).
Analyzing the differences between the two studies is illuminating. In the follow-up experiment, I used netnews archives to gather highly frequent nouns within related topic areas
122

fiInformation-Based Semantic Similarity

Similarity method
Correlation
Information content
r = :6894
Leacock and Chodorow r = :4320
Edge-counting
r = :4101
Table 11: Summary of experimental results in follow-up study.
(to ensure that similar noun pairs occurred) and then selected noun pairings at random (in
order to avoid biasing the follow-up study in favor of either algorithm). There is, therefore,
a predominance of low-similarity noun pairs in the test data. Looking at the distribution
of ratings for the noun pairs, as given by the two measures, it is evident that the Leacock
and Chodorow measure is overestimating semantic similarity for many of the predominantly
non-similar pairs. This stands to reason since the measure is identical whenever the edge
distance is identical, regardless of whether the pair is high or low in the taxonomy (e.g., the
distance between plant and animal is the same as the distance between white oak and red
oak). In contrast, the information-based measure is sensitive to the difference, and better
at avoiding spuriously high similarity values for non-similar pairs. On a related note, the
edge-counting measure used in the follow-up study was a variant that computes path length
through a virtual top node, rather than asserting zero similarity between words with no path
connecting them in the existing WordNet taxonomy, as was done previously. Using the data
set in the follow-up study, the information-based measure, at r = :6894, does significantly
better than either of the edge-counting variants (r = :4101 and r = :2777); but going back
to the original Miller and Charles data, the virtual-top-node variant does significantly better
than the assert-zero edge distance measure, with its correlation of r = :7786 approaching
that of the measure based on information content. This comparison between the follow-up
study and the original Miller and Charles data illustrates quite clearly how the utility of a
similarity measure can depend upon the distribution of items given by the task.
Lin (1997, 1998) has recently proposed an alternative information-theoretic similarity
measure, derived from a set of basic assumptions about similarity in a style reminiscent of
the way in which entropy/information itself has a formal definition derivable from a set of
basic properties (Khinchin, 1957). Formally, Lin defines similarity in a taxonomy as:
T

 log p( i Ci)
simLin(c1; c2) = log2 p(
c1) + log p(c2)

(15)

where the Ci are the \maximally specific superclasses" ofT both c1 and c2 . Although the
possibility of multiple inheritance makes the intersection i Ci necessary in principle, multiple inheritance is in fact so rare in WordNet that in practice one computes Equation (15)
separately for each common ancestor Ci , using p(Ci) in the numerator, and then takes
the maximum (Dekang Lin, p.c.). Other than the multiplicative constant of 2, therefore,
Lin's method for determining similarity in a taxonomy is essentially the information-based
similarity measure of Equation 1, but normalized by the combined information content of
the two concepts assuming their independence. Put another way, Lin's measure is taking
123

fiResnik

Similarity method Correlation
Information content r = :7947
simWu&Palmer
r = :8027
simLin
r = :8339
Table 12: Summary of Lin's results comparing alternative similarity measures
into account not only commonalities but differences between the items being compared,
expressing both in information-theoretic terms.
Lin's measure is theoretically well motivated and elegantly derived. Moreover, Lin points
out that his measure will by definition yield the same value for simLin(x; x) regardless of
the identity of x | unlike information content, which has been criticized on the grounds
that the value of self-similarity depends on how specific a concept x is, and that two nonidentical items x and y can be rated more similar to each other than a third item z is to itself
(Richardson et al., 1994). From a cognitive perspective, however, similarity comparisons
involving self-similarity (\Robins are similar to robins"), as well as subclass relationships
(\Robins are similar to birds"), have themselves been criticized by psychologists as anomalous (Medin, Goldstone, & Gentner, 1993). Moreover, experimental evidence with human
judgments suggests that not all identical objects are judged equally similar, consistent with
the information-content measure proposed here but contrary to Lin's measure. For example, objects that are identical and complex, such as twins, can seem more similar to each
other than objects that are identical and simple, such as two instances of a simple geometric shape (Goldstone, 1999; Tversky, 1977). It would appear, therefore, that insofar as
fidelity to human judgments is relevant, further experimentation is needed to evaluate the
competing predictions of alternative similarity measures.
Wu and Palmer (1994) propose a similarity measure that is based on edge distances, but
related to Lin's measure in the way it takes into account the most specific node dominating
c1 and c2, characterizing their commonalities, while normalizing in a way that accounts for
their differences. Revising Wu and Palmer's notation slightly, their measure is:
c3)
(16)
simWu&Palmer(c1; c2) = d(2c ) +d(d(
c2)
1
where c3 is the maximally specific superclass of c1 and c2 , d(c3) is its depth, i.e. distance
from the root of the taxonomy, and d(c1) and d(c2) are the depths of c1 and c2 on the path
through c3.
Lin (1998) repeats the experiment of Section 3 for the information content measure,
simLin, and simWu&Palmer, reporting the results that appear in Table 12. Lin uses a sensetagged corpus to estimate frequencies, and smoothed probabilities rather than simple relative frequency. His results show a somewhat higher correlation for simLin than the other
measures. Further experimentation is needed in order to assess the alternative measures,
particularly with respect to their competing predictions and the variability of performance
across data sets. What seems clear, however, is that all these measures perform better than
the traditional edge-counting measure.
124

fiInformation-Based Semantic Similarity

7. Conclusions
This article has presented a measure of semantic similarity in an is-a taxonomy, based on
the notion of information content. Experimental evaluation was performed using a large,
independently constructed corpus, an independently constructed taxonomy, and previously
existing and new human subject data, and the results suggest that the measure performs
encouragingly well and can be significantly better than the traditional edge-counting approach. Semantic similarity, as measured using information content, was shown to be useful
in resolving cases of two pervasive kinds of linguistic ambiguity. In resolving coordination
ambiguity, the measure was employed to capture the intuition that similarity of meaning is
one indicator that two words are being conjoined; suggestive results of a first experiment
were bolstered by unequivocal results in a second study, demonstrating significant improvements over a disambiguation strategy based only on syntactic agreement. In resolving word
sense ambiguity, the semantic similarity measure was used to assign confidence values to
word senses of nouns within thesaurus-like groupings. A formal evaluation provided evidence that the technique can produce useful results but is better suited for semi-automated
sense filtering than categorical sense selection. Application of the technique to a dictionary/thesaurus on the World Wide Web provides a demonstration of the method in action
in a real-world setting.

Acknowledgements
Sections 1-3 of this article comprise a revised and extended version of Resnik (1995).
Section 4 describes previously presented algorithms and data (Resnik, 1993b, 1993a), extended by further discussion and analysis. Section 5 summarizes an algorithm described
in Resnik (1998a), and then extends previous results by presenting new applications of the
algorithm, with Section 5.2 containing a formal evaluation in a new setting and Section 5.3
giving a real-world illustration where the approach has been put into practice. Section 6
adds a substantial discussion of related work by other authors that has taken place since
the information-based similarity measure was originally proposed.
Parts of this research were done at the University of Pennsylvania with the partial
support of an IBM Graduate Fellowship and grants ARO DAAL 03-89-C-0031, DARPA
N00014-90-J-1863, NSF IRI 90-16592, and Ben Franklin 91S.3078C-1; parts of this research
were also done at Sun Microsystems Laboratories in Chelmsford, Massachusetts; and parts
of this work were supported at the University of Maryland by Department of Defense
contract MDA90496C1250, DARPA/ITO Contract N66001-97-C-8540, Army Research
Laboratory contract DAAL03-91-C-0034 through Battelle, and a research grant from Sun
Microsystems Laboratories. The author gratefully acknowledges the comments of three
anonymous JAIR reviewers and helpful discussions with John Kovarik, Claudia Leacock,
Dekang Lin, Johanna Moore, Mari Broman Olsen, and Jin Tong, as well as comments and
criticism received during various presentations of this work.

References
Agarwal, R., & Boggess, L. (1992). A simple but useful approach to conjunct identifica125

fiResnik

tion. In Proceedings of the 30th Annual Meeting of the Association for Computational
Linguistics, pp. 15{21. Association for Computational Linguistics.
Bensch, P. A., & Savitch, W. J. (1992). An occurrence-based model of word categorization.
Presented at 3rd Meeting on Mathematics of Language (MOL3).
Brill, E. (1991). Discovering the lexical features of a language. In Proceedings of the 29th
Annual Meeting of the Association for Computational Linguistics, Berkeley, CA.
Brill, E., & Resnik, P. (1994). A rule-based approach to prepositional phrase attachment disambiguation. In Proceedings of the 15th International Conference on Computational
Linguistics (COLING-94).
Brown, P. F., Della Pietra, V. J., deSouza, P. V., Lai, J. C., & Mercer, R. L. (1992).
Class-based n-gram models of natural language. Computational Linguistics, 18 (4),
467{480.
CETA (1982). Chinese Dictionaries: an Extensive Bibliography of Dictionaries in Chinese
and Other Languages. Chinese-English Translation Assistance Group, Greenwood
Publishing.
Church, K. W., & Mercer, R. (1993). Introduction to the special issue on computational
linguistics using large corpora. Computational Linguistics, 19 (1), 1{24.
Church, K. W., & Patil, R. (1982). Coping with syntactic ambiguity or how to put the
block in the box on the table. American Journal of Computational Linguistics, 8 (3-4),
139{149.
Collins, A., & Loftus, E. (1975). A spreading activation theory of semantic processing.
Psychological Review, 82, 407{428.
Collins, M., & Brooks, J. (1995). Prepositional phrase attachment through a backed-off
model. In Third Workshop on Very Large Corpora. Association for Computational
Linguistics. cmp-lg/9506021.
Cowie, J., Guthrie, J., & Guthrie, L. (1992). Lexical disambiguation using simulated annealing. In Proceedings of the 14th International Conference on Computational Linguistics
(COLING-92), pp. 359{365 Nantes, France.
Digital Equipment Corporation (1998).

AltaVista web page: Refine, or cow9?..
http://altavista.digital.com/av/content/about_our_technology_cow9.htm.

Dorr, B. J. (1997). Large-Scale Dictionary Construction for Foreign Language Tutoring and
Interlingual Machine Translation. Machine Translation, 12 (4), 271{322.
Fellbaum, C. (Ed.). (1998). WordNet: An Electronic Lexical Database. MIT Press.
Francis, W. N., & Kucera, H. (1982). Frequency Analysis of English Usage: Lexicon and
Grammar. Houghton Miin, Boston.
126

fiInformation-Based Semantic Similarity

Goldstone, R. L. (1999). Similarity. In MIT Encyclopedia of the Cognitive Sciences. MIT
Press, Cambridge, MA.
Grefenstette, G. (1992). Use of syntactic context to produce term association lists for text
retrieval. In Proceedings of the Fifteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 89{97.
Grefenstette, G. (1994). Explorations in Automatic Thesaurus Discovery. Kluwer, Boston.
Harman, D. (1992). Relevance feedback revisited. In Proceedings of the Fifteenth Annual
International ACM SIGIR Conference on Research and Development in Information
Retrieval, pp. 1{10.
Hearst, M. (1991). Noun homograph disambiguation using local context in large corpora.
In Proceedings of the 7th Annual Conference of the University of Waterloo Centre for
the New OED and Text Research Oxford.
Hindle, D., & Rooth, M. (1993). Structural ambiguity and lexical relations. Computational
Linguistics, 19 (1), 103{120.
Ji, D., Gong, J., & Huang, C. (1998). Combining a Chinese thesaurus with a Chinese
dictionary. In COLING-ACL '98, pp. 600{606. Universite de Montreal.
Katz, S. M. (1987). Estimation of probabilities from sparse data for the language model
component of a speech recognizer. IEEE Transactions on Acoustics, Speech and Signal
Processing, ASSP-35 (3), 400{401.
Khinchin, A. I. (1957). Mathematical Foundations of Information Theory. New York: Dover
Publications. Translated by R. A. Silverman and M. D. Friedman.
Klavans, J. L., & Tzoukermann, E. (1995). Dictionaries and Corpora: Combining Corpus
and Machine-Readable Dictionary Data for Building Bilingual Lexicons. Machine
Translation, 10, 185{218.
Kobayasi, Y., Takunaga, T., & Tanaka, H. (1994). Analysis of Japanese compound nouns
using collocational information. In Proceedings of the 15th International Conference
on Computational Linguistics (COLING-94).
Krovetz, R., & Croft, W. B. (1992). Lexical ambiguity and information retrieval. ACM
Transactions on Information Systems, 10 (2), 115{141.
Kurohashi, S., & Nagao, M. (1992). Dynamic programming method for analyzing conjunctive structures in Japanese. In Proceedings of the 14th International Conference on
Computational Linguistics (COLING-92) Nantes, France.
Lauer, M. (1994). Conceptual association for compound noun analysis. In Proceedings of the
32nd Annual Meeting of the Association for Computational Linguistics Las Cruces,
New Mexico. Student Session.
Lauer, M. (1995). Designing Statistical Language Learners: Experiments on Noun Compounds. Ph.D. thesis, Macquarie University, Sydney, Australia.
127

fiResnik

Leacock, C., & Chodorow, M. (1994). Filling in a sparse training space for word sense
identification. ms.
Lee, J. H., Kim, M. H., & Lee, Y. J. (1993). Information retrieval based on conceptual
distance in IS-A hierarchies. Journal of Documentation, 49 (2), 188{207.
Lee, L. (1997). Similarity-based approaches to natural language processing. Tech. rep.
TR-11-97, Harvard University. Doctoral dissertation. cmp-lg/9708011.
Lesk, M. (1986). Automatic sense disambiguation using machine readable dictionaries:
how to tell a pine cone from an ice cream cone. In Proceedings of the 1986 SIGDOC
Conference, pp. 24{26.
Li, H., & Abe, N. (1995). Generalizing case frames using a thesaurus and the MDL principle.
In Proceedings of the International Conference on Recent Advances in NLP Velingrad,
Bulgaria.
Lin, D. (1997). Using syntactic dependency as local context to resolve word sense ambiguity. In Proceedings of the 35th Annual Meeting of the Association for Computational
Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics Madrid, Spain.
Lin, D. (1998). An information-theoretic definition of similarity. In Proceedings of the
Fifteenth International Conference on Machine Learning (ICML-98) Madison, Wisconsin.
Marcus, M. P., Santorini, B., & Marcinkiewicz, M. (1993). Building a large annotated
corpus of English: the Penn Treebank. Computational Linguistics, 19, 313{330.
McKeown, K., & Hatzivassiloglou, V. (1993). Augmenting lexicons automatically: Clustering semantically related adjectives. In Bates, M. (Ed.), ARPA Workshop on Human
Language Technology. Morgan Kaufmann.
Medin, D., Goldstone, R., & Gentner, D. (1993). Respects for similarity. Psychological
Review, 100 (2), 254{278.
Merlo, P., Crocker, M., & Berthouzoz, C. (1997). Attaching multiple prepositional phrases:
Generalized backed-off estimation. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing (EMNLP-2). cmp-lg/9710005.
Miller, G. (1990). WordNet: An on-line lexical database. International Journal of Lexicography, 3 (4). (Special Issue).
Miller, G. A., & Charles, W. G. (1991). Contextual correlates of semantic similarity. Language and Cognitive Processes, 6 (1), 1{28.
Morrissey,
R.
(1993). Texts and contexts: The ARTFL database in French studies. Profession 93,
27{33. http://humanities.uchicago.edu/homes/publications/romoart.html.
128

fiInformation-Based Semantic Similarity

Pereira, F., Tishby, N., & Lee, L. (1993). Distributional clustering of English words. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics
(ACL-93) Morristown, New Jersey. Association for Computational Linguistics.
Quillian, M. R. (1968). Semantic memory. In Minsky, M. (Ed.), Semantic Information
Processing. MIT Press, Cambridge, MA.
Rada, R., & Bicknell, E. (1989). Ranking documents with a thesaurus. JASIS, 40 (5),
304{310.
Rada, R., Mili, H., Bicknell, E., & Blettner, M. (1989). Development and application of
a metric on semantic nets. IEEE Transaction on Systems, Man, and Cybernetics,
19 (1), 17{30.
Ratnaparkhi, A., & Roukos, S. (1994). A maximum entropy model for prepositional phrase
attachment. In Proceddings of the ARPA Workshop on Human Language Technology
Plainsboro, NJ.
Resnik, P. (1993a). Selection and Information: A Class-Based Approach to Lexical Relationships.
Ph.D.
thesis,
University
of
Pennsylvania.
(ftp://ftp.cis.upenn.edu/pub/ircs/tr/93-42.ps.Z).
Resnik, P. (1993b). Semantic classes and syntactic ambiguity. In Proceedings of the 1993
ARPA Human Language Technology Workshop. Morgan Kaufmann.
Resnik, P. (1995). Using information content to evaluate semantic similarity in a taxonomy.
In Proceedings of the 14th International Joint Conference on Artificial Intelligence
(IJCAI-95). (cmp-lg/9511007).
Resnik, P. (1996). Selectional constraints: An information-theoretic model and its computational realization. Cognition, 61, 127{159.
Resnik, P. (1998a). Disambiguating noun groupings with respect to Wordnet senses. In
Armstrong, S., Church, K., Isabelle, P., Tzoukermann, E., & Yarowsky, D. (Eds.),
Natural Language Processing Using Very Large Corpora. Kluwer.
Resnik, P. (1998b). WordNet and class-based probabilities. In Fellbaum, C. (Ed.), WordNet:
An Electronic Lexical Database. MIT Press.
Resnik, P., & Yarowsky, D. (1997). A perspective on word sense disambiguation methods
and their evaluation. In ANLP Workshop on Tagging Text with Lexical Semantics
Washington, D.C.
Resnik, P., & Yarowsky, D. (1999). Distinguishing systems and distinguishing senses: New
evaluation methods for word sense disambiguation. Natural Language Engineering.
(to appear).
Richardson, R., Smeaton, A. F., & Murphy, J. (1994). Using WordNet as a knowledge base for measuring semantic similarity between words. Working paper CA1294, Dublin City University, School of Computer Applications, Dublin, Ireland.
ftp://ftp.compapp.dcu.ie/pub/w-papers/1994/CA1294.ps.Z.
129

fiResnik

Ross, S. (1976). A First Course in Probability. Macmillan.
Rubenstein, H., & Goodenough, J. (1965). Contextual correlates of synonymy. CACM,
8 (10), 627{633.
Salton, G. (1989). Automatic Text Processing. Addison-Wesley.
Schutze, H. (1993). Word space. In Hanson, S. J., Cowan, J. D., & Giles, C. L. (Eds.), Advances in Neural Information Processing Systems 5, pp. 895{902. Morgan Kaufmann
Publishers, San Mateo CA.
Sinclair (ed.), J. (1987). Collins COBUILD English Language Dictionary. Collins: London.
Sussna, M. (1993). Word sense disambiguation for free-text indexing using a massive semantic network. In Proceedings of the Second International Conference on Information
and Knowledge Management (CIKM-93) Arlington, Virginia.
Tversky, A. (1977). Features of similarity. Psychological Review, 84, 327{352.
Voorhees, E. M. (1993). Using WordNet to disambiguate word senses for text retrieval. In
Korfhage, R., Rasmussen, E., & Willett, P. (Eds.), Proceedings of the Sixteenth Annual
International ACM SIGIR Conference on Research and Development in Information
Retrieval, pp. 171{180.
Voorhees, E. M. (1994). Query expansion using lexical-semantic relations. In 17th International Conference on Research and Development in Information Retrieval (SIGIR
'94) Dublin, Ireland.
Vossen, P. (1998). Special issue on EuroWordNet. Computers and the Humanities, 32 (2/3).
Weiss, S. M., & Kulikowski, C. A. (1991). Computer systems that learn: classification and
prediction methods from statistics, neural nets, machine learning, and expert systems.
Morgan Kaufmann, San Mateo, CA.
Whittemore, G., Ferrara, K., & Brunner, H. (1990). Empirical study of predictive powers of
simple attachment schemes for post-modifier prepositional phrases. In Proceedings of
the 28th Annual Meeting of the Association for Computational Linguistics, pp. 23{30.
Pittsburgh, Pennsylvania.
Wilks, Y., & Stevenson, M. (1996). The grammar of sense: Is word-sense tagging much
more than part-of-speech tagging?.. Technical Report CS-96-05, cmp-lg/9607028.
Wu, Z., & Palmer, M. (1994). Verb Semantics and Lexical Selection. In Proceedings of the
32nd Annual Meeting of the Association for Computational Linguistics Las Cruces,
New Mexico.

130

fi