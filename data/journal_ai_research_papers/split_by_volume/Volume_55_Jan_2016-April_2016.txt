Journal of Artificial Intelligence Research 55 (2016) 1059-1090

Submitted 10/15; published 04/16

Learning Concept Graphs from Online Educational Data
Hanxiao Liu
Wanli Ma
Yiming Yang
Jaime Carbonell

hanxiaol@cs.cmu.edu
mawanli@cs.cmu.edu
yiming@cs.cmu.edu
jgc@cs.cmu.edu

School of Computer Science
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213 USA

Abstract
This paper addresses an open challenge in educational data mining, i.e., the problem of
automatically mapping online courses from different providers (universities, MOOCs, etc.)
onto a universal space of concepts, and predicting latent prerequisite dependencies (directed
links) among both concepts and courses. We propose a novel approach for inference within
and across course-level and concept-level directed graphs. In the training phase, our system
projects partially observed course-level prerequisite links onto directed concept-level links;
in the testing phase, the induced concept-level links are used to infer the unknown courselevel prerequisite links. Whereas courses may be specific to one institution, concepts are
shared across different providers. The bi-directional mappings enable our system to perform
interlingua-style transfer learning, e.g. treating the concept graph as the interlingua and
transferring the prerequisite relations across universities via the interlingua. Experiments
on our newly collected datasets of courses from MIT, Caltech, Princeton and CMU show
promising results.

1. Introduction
The large and growing amounts of online education data present both open challenges and
significant opportunities for machine learning research to enrich educational offerings. One
of the most important challenges is to automatically detect the prerequisite dependencies
among massive quantities of online courses, and to support decision making such as curricula planning for students, and to support course and curriculum design by teachers based
on existing course offerings. One example is to find a coherent sequence of courses among
MOOC offerings from different providers that respect implicit prerequisite relations. A
more specific example would be a new student who just enters a university for a MS or
PhD degree. She is interested in machine learning and data mining courses, but finds it
difficult to choose among many courses which look similar or with ambiguous course titles
to her, such as Machine Learning, Statistical Machine Learning, Applied Machine Learning, Machine Learning with Large Datasets, Scalable Analytics, Advanced Data Analysis,
Statistics: Data Mining, Intermediate Statistics, Statistical Computing, and so on. Completing all the courses would imply taking forever to graduate, and possibly waste a big
portion of her time due to the overlapping content. Alternately, if she wants to choose a
small subset, which courses should she include? How should she order the included courses
c
2016
AI Access Foundation. All rights reserved.

fiLiu, Ma, Yang, & Carbonell

Courses in University 2

Courses in University 1
E&M

Mechanics
Calculus

Differential Eq

Matrix A

Matrix A

Quantum

Algorithms

Topology

Java Prog
Scalable Algs

Num Analysis

Universal Concepts (e.g. Wikipedia Topics)

Figure 1: The framework of two-level directed graphs: The higher-level graphs have courses
(nodes) with prerequisite relations (links). The lower-level graph consists of universal concepts (nodes) and pairwise preference in learning or teaching concepts.
The links between the two levels are system-assigned weights of concepts to each
course.

without sufficient understanding about the prerequisite dependencies? Often prerequisites
are explicit within an academic department but implicit across departments. Moreover, if
she already took several courses in machine learning or data mining through Coursera or in
her undergraduate education, how much do those courses overlap with the new ones? Without an accurate representation of content overlap between courses and how the overlapped
content reflects the prerequisite relations, it is difficult to help her find the most suitable
courses in a correct order. Universities solve this problem in the old-fashioned way, via
academic advisors, but it is not clear how to address this problem in the context of MOOCs
or cross-university offerings where courses do not have unique IDs and are not described in
a universally controlled or consistent vocabulary.
Ideally, we would like to have a universal graph whose nodes are canonical and discriminant concepts (e.g. convexity or eigenvalues) being taught in a broad range of courses,
and whose links indicate pairwise preferences in sequencing the teaching of these concepts.
For example, to learn the concepts of PageRank and HITS, students should have already
learned the concepts of eigenvectors, Markov matrices and irreducibility of matrices. This
means directed links from eigenvectors, Markov matrices and irreducibility to PageRank
and HITS in the concept graph. To generalize this further, if there are many directed links
from the concepts in one course (say Matrix Algebra) to the concepts in another course (say
Web Mining with Link Analysis as a sub-topic), we may infer a prerequisite relation between the two courses. Clearly, having a directed graph with a broad coverage of universal
1060

fiLearning Concept Graphs from Online Educational Data

concepts is crucial for reasoning about course content overlap and prerequisite relationships, and hence important for educational decision making, such as curriculum planning
by students and modularization in course syllabus design by instructors.
How can we obtain such a knowledge-rich concept graph? Manual specification is obviously not scalable when the number of concepts reaches tens of thousands or larger. Using
machine learning to automatically induce such a graph based on massive online course materials is an attractive alternative; however, no statistical learning techniques have been
developed for this problem, to our knowledge. Addressing this open challenge with principled algorithmic solutions is the novel contribution we aim to accomplish in this paper. We
call our new method Concept Graph Learning (CGL). Specifically, we propose a multi-level
inference framework as illustrated in Figure 1, which consists of two levels of graphs and
cross-level links. Generally, a course would cover multiple concepts, and a concept may
be covered by more than one course. Notice that the course-level graphs do not overlap
because different universities do not have universal course IDs. However, the semantic concepts taught in different universities do overlap, and we want to learn the mappings between
the non-universal courses and the universal concept space based on online course materials.
In this paper we investigate the problem of concept graph learning (CGL) with our new
collections of course syllabi (including course names, descriptions, listed lectures, prerequisite relations, etc.) from Massachusetts Institute of Technology (MIT), California Institute
of Technology (Caltech), Carnegie Mellon University (CMU) and Princeton. The syllabus
data allow us to construct an initial course-level graph for each university, which may be
further enriched by discovering latent prerequisite links. As for representing the universal
concept space, we study four representation schemes (Section 2.1), including 1) using the
English words in course descriptions, 2) using sparse coding of English words, 3) using
distributed word embedding of English words, and 4) using a large subset of Wikipedia
categories. For each of these representation schemes, we provide algorithmic solutions to
establish a mapping from courses to concepts, and to learn the concept-level dependencies based on observed prerequisite relations at the course level. The second part, i.e., the
explicit learning of the directed graph for universal concepts, is the most unique part of
our proposed framework. Once the concept graph is learned, we can predict unobserved
prerequisite relations among any courses, including those not in the training set and by different universities. In other words, CGL enables an interlingua-style transfer learning as to
train the models on the course materials of some universities and to predict the prerequisite
relations for the courses in other universities. The universal transferability is particularly
desirable in MOOC environments where courses are offered by different instructors in many
universities. As we mentioned before, the course-level sub-graphs of different universities do
not overlap with each other, and the prerequisite links are only local within each sub-graph.
Thus to enable cross-university transfer, it is crucial to project course-level prerequisite
links in different universities onto the directed links among universal concepts.
The bi-directional inference between the two directed graphs makes our CGL framework
fundamentally different from existing approaches in graph-based link detection (Kunegis &
Lommatzsch, 2009; Liben-Nowell & Kleinberg, 2007; Lichtenwalter, Lussier, & Chawla,
2010), matrix completion (Candes & Recht, 2009; Fazel, 2002; Johnson, 1990) and collaborative filtering (Su & Khoshgoftaar, 2009). That is, our approach requires explicit learning
1061

fiLiu, Ma, Yang, & Carbonell

of the concept-level directed graph and the optimal mapping between the two levels of links
while other methods do not (see Section 7 for more discussion).
Our main contributions in this paper1 can be summarized as:
1. A novel framework for within- and cross-level inference of prerequisite relations both
at the course-level and at the concept-level;
2. New algorithmic solutions for scalable concept graph learning under various (dense,
sparse and transductive) settings;
3. New data collections from multiple universities with syllabus descriptions, prerequisite
links and lecture materials;
4. The first evaluation for prerequisite link prediction in within- and cross-university
settings.
The rest of the paper is organized as follows: Section 2 introduces the formal definitions
of our framework and optimization objectives; Section 3 provides scalable algorithms for
learning large concept graphs; Section 4 extends our new method to learn a sparse, parsimonious concept graph for better interpretability; Section 5 explores how unlabeled course
pairs can be leveraged to significantly improve the prediction performance of the learned
concept graph; Section 6 describes the new datasets we collected for this study and future
benchmark evaluations, and reports our empirical findings; Section 7 discusses related work
and how concept graphs can be deployed to benefit future educational applications; and
Section 8 summarizes the main findings in this study.

2. Framework & Algorithms
Let us formally define our methods with the following notation.
 n is the number of courses in a training set;
 p is the dimension of the universal concept space (Section 2.1);
 X = [x1 , x2 , . . . , xn ]>  Rnp is a collection of n courses, where xi  Rp is the bag-ofconcepts representation of the i-th course;
 Y  {1, +1}nn is a collection of n2 binary indicators of the observed prerequisite
relations between courses, i.e., yij = 1 means that course j is a prerequisite of course
i, and yij = 1 otherwise.
 A  Rpp is the adjacency matrix of the concept graph, whose elements are the weights
of directed links among concepts. That is, A is the matrix of model parameters we
want to optimize given the training data in X and Y .
1. This journal paper is a substantially extended version of a previous paper (Yang, Liu, Carbonell, & Ma,
2015).

1062

fiLearning Concept Graphs from Online Educational Data

2.1 Representation Schemes
What is the best way to represent the contents of courses to learn the universal concept
space? We explore different answers with four alternate choices as follows:
1. Word-based Representation (Word): This method uses the vocabulary of course
descriptions plus any listed keywords by the course providers (MIT, Caltech, CMU
and Princeton) as the entire concept (feature) space. We applied standard procedures
for text preprocessing, including stop-word removal, term-frequency (TF) based term
weighting, and the removal of the rare words whose training-set frequency is one.
We did not use TF-IDF weighting because the relative small number of documents
(courses) in our datasets do not allow reliable estimates of the IDF part.
2. Sparse Coding of Words (SCW): This method projects the original n-dimensional
vector representations of words (the columns in the course-by-word matrix X) onto
sparse vectors in a smaller k-dimensional space using Non-negative Matrix Factorization (Lee & Seung, 1999), where k is much smaller than n. One can view the lower
dimensional components as the system-discovered latent concepts. Intrigued by the
successful application of sparse coding in image processing (Hoyer, 2004), we explored
its application to our graph-based inference problem. By applying an existing sparse
coding algorithm (Kim & Park, 2008) to our training sets we obtained a k-dimensional
vector for each word; by taking the average of word vectors in each course we obtained
the bag-of-concepts representation of the course. This resulted in an n-by-k matrix X,
representing all the training-set courses in the k-dimensional space of latent concepts.
We set k = 100 in our experiments based on cross validation.
3. Distributed Word Embedding (DWE): This method also uses dimension-reduced
vectors to represent words in the courses, similar to SCW. However, the lower dimensional vectors (continuous vector representations) for words are discovered by neural
networks based on word usage w.r.t. contextual, syntactic and semantic information
(Le & Mikolov, 2014). Intrigued by the popularity of DWE in recent research in
Natural Language Processing and other domains (Collobert, Weston, Bottou, Karlen,
Kavukcuoglu, & Kuksa, 2011; Chen, Perozzi, Al-Rfou, & Skiena, 2013), we explored
its application to our graph-based inference problem. Specifically, we deploy English
word embeddings trained on Wikipedia articles (Al-Rfou, Perozzi, & Skiena, 2013), a
domain which is believed to be semantically close to that of academic courses. The
vector representation for each course is obtained by aggregating the vector representations of words it contains.
4. Category-based Representation (Cat): This method used a large subset of
Wikipedia categories as the concept space. We selected the subset via a pooling strategy as follows: We used the words in our training-set courses to form 3509 queries (one
query per course), and retrieved the top 100 documents per query based on cosine
similarity. We then took the union of the Wikipedia category labels of these retrieved
documents, and removed the categories which were retrieved by only three queries or
less. This process resulted in a total of 10,051 categories in the concept space. The
categorization of courses was based on an earlier highly scalable very-large category
1063

fiLiu, Ma, Yang, & Carbonell

space work (Gopal & Yang, 2013): the classifiers were trained on labeled Wikipedia
articles and then applied to the word-based vector representation of each course for
(weighted) category assignments.
Each of the above representation schemes may have its own strengths and weaknesses.
Word is simple and natural but rather noisy, because semantically equivalent lexical variants
are not unified into canonical concepts and there could be systematic vocabulary variation
across universities. Also, this scheme will not work in cross-language settings, e.g., if course
descriptions are in English and Chinese. Cat would be less noisy and better in cross-language
settings, but the automated classification step will unavoidably introduce errors in category
assignments. SCW (sparse coding of words) reduces the total number of model parameters
via dimensionality reduction, which may lead to robust training (avoiding overfitting) and
efficient computation, but at the risk of losing useful information in the projection from
the original high-dimensional space to a lower dimensional space. DWE (distributed word
embedding) deploys recent advances in representation learning of word meanings in context.
However, reliable word embedding requires the availability of large volumes of training text
(e.g., Wikipedia articles); the potential mismatch between the training domain (for which
large volumes of data can be obtained easily) and the test domain (for which large volumes
of data are hard or costly to obtain) could be a serious issue. Yet another distinction among
these representation schemes is that Word and Cat produce human-understandable concepts
and links, while SCW and DWE produce latent factors which are harder to interpret by
humans, although methods such as L1 regularization help with interpretability (sec 6.5).
By exploring all the four representation schemes in our unified framework for two-level
graph based inference, and by examining their effectiveness in the task of link prediction
of prerequisite relations among courses, we aim to obtain a deeper understanding of the
strengths and weaknesses of those representational choices.
2.2 The Optimization Methods
We define the problem of concept graph learning as a key part of learning-to-predict prerequisite relations among courses, i.e., for the two-level statistical inference we introduced in
Section 1 with Figure 1. Given a training set of courses with a bag-of-concepts representation per course as a row in matrix X, and a list of known prerequisite links per course as a row
in matrix Y, we optimize matrix A whose elements specify both the direction (sign) and the
strength (magnitudes) of each link between concepts. We propose two new approaches to
this problem: a classification approach and a learning-to-rank approach. Both approaches
deploy the same extended versions of SVM algorithms with squared hinge loss, but the
objective functions for optimization are different. We also propose a nearest-neighbor approach for comparison, which predicts the course-level links (prerequisites) without learning
the concept-level links.
2.2.1 The Classification Approach (CGL.Class)
In this method, we predict the score of the prerequisite link from course i to course j as:
Fij = x>
i Axj
1064

(1)

fiLearning Concept Graphs from Online Educational Data

Figure 2: The weighted connections from course i to course j via matrix A which encodes
the directed links between concepts

The intuition behind this formula is shown in Figure 2. It can be easily verified that
the quantity x>
i Axj is the summation of the weights of all the paths from node i to node
j in this graph, where each path is weighted using the product of the corresponding xik ,
Akk0 and xjk0 . In other words, we assume the prerequisite strength between two courses is
a cumulative effect of the prerequisite strengths of all concept pairs.
The criterion for optimizing matrix A given training data xi for i = 1, 2, . . . , n and true
labels yij for all course pairs is defined as:
min

ARpp


i2
Xh

1  yij x>
Ax
+ kAk2F
j
i
2
+

(2)

i,j

where (1  v)+ = max(0, 1  v) denotes the hinge function, and k  kF denotes the matrix
Frobenius norm. The 1st term in formula (2) is the empirical loss; the 2nd term is the regularization term, controlling the model complexity based on the large margin principle. We
choose to use the squared hinge loss (1  v)2+ as the first term to gain first-order continuity
of our objective function, enabling efficient computation using accelerated gradient descent
(Nesterov, 1983, 1988) (Section 3). This efficiency improvement is crucial because we operate on pairs of courses, and thus have a much larger space than in normal classification
(e.g. classifying individual courses).
2.2.2 The Learning-to-Rank Approach (CGL.Rank)
Inspired by the learning-to-rank literature (Joachims, Li, Liu, & Zhai, 2007), we explored
going beyond the binary classifier in the previous approach to one that essentially learns to
rank prerequisite preferences. Let i be the set of course pairs with the true labels yij = 1
for different js, and i the pairs of courses with the true labels yik = 1 for different ks,
we want our system to give all the pairs in i higher scores than that of any pair in i . We
call this the partial-order preference over links conditioned on course i.
1065

fiLiu, Ma, Yang, & Carbonell

Let T be the union of tuple sets {(i, j, k)|(i, j)  i , (i, k)  i } for all i in 1, 2, . . . , n.
We formulate our optimization problem as:
X

min

ARpp

(i,j,k)T

h

i2

>
1  x>
+ kAk2F
i Axj  xi Axk
2
+

(3)

Or equivalently, the objective can be rewritten as:
min

ARpp



X



1  XAX

(i,j,k)T

>




ij

+ XAX

>

 2
ik +

+


kAk2F
2

(4)

Solving this optimization problem requires us to extend standard packages of SVM algorithms in order to improve computational efficiency because the number of model parameters
(p2 ) in our formulation is very large. For example, with the vocabulary size of 15,396 words
in the MIT dataset, the number of model parameters in A is over 237 million. When p
(the number of concepts) is much larger than n (the number of courses), one may consider
solving this optimization problem in the dual space instead of the primal space. However,
even in the dual space, the number of dual variables is still O(n3 ), corresponding to all the
triplets in T , and the kernel matrix is in the order of O(n6 ). We are going to address these
computational challenges in Section 3.
2.2.3 The Nearest-Neighbor Approach (kNN)
Different from the two approaches above where matrix A plays a central role, as a different
baseline, we propose to predict the prerequisite relationship for any pair of courses based
on matrices X and Y without A. Let (i0 , j 0 ) be a new pair of courses in the test set. We
score each course pair (i, j) in the training set with respect to the new test pair as:


ff 

ff
xi0 , xi  xj 0 , xj
i, j = 1, 2, . . . , n
(5)
where h, i stands for the inner product between two vectors. By taking the top-scored pairs
in the training set and by aggregating the corresponding yij s, we perform the kNN-based
prediction of yi0 j 0 for the new test pair. If we normalize the vectors, the dot-products in (5)
become the cosine similarity. This approach requires nearest-neighbor search on-demand;
when the number of course pairs in the test set is large, the online computation would be
substantial. Via cross validation, we have found k = 1 (1NN) works best for this problem
on the current datasets.
2.2.4 The Support Vector Machine (SVM)
As another baseline for comparision we also include an SVM with the following objective:
minp

wR

X h

1  yij (xi  xj )> w

i,j

i
+

+


kwk22
2

(6)

Similar to the kNN approach above and unlike CGL, the SVM optimization in (6) does not
involve learning the matrix A (the directed graph of universal concepts). The feature vector
1066

fiLearning Concept Graphs from Online Educational Data

for each course pair is simply the pairwise difference in the two vector representations (the
two bags of words) of these courses. Once the model parameter vector w is optimized on
a labeled training set, it can be used to predict the prerequisite relation among any pair of
courses by computing w> (xi  xj ) whose sign indicates the direction of the relationship,
and whose magnitude indicates the strength of the relationship. By sorting the scores of
the course pairs for each fixed i in the test set, a ranked list of candidate prerequisites is
obtained for course i.

3. Scalable Algorithms
Though it appears that any gradient-based method is directly applicable to CGL.Rank
according to (4), the optimization is computationally challenging due to the following facts:
(a) The large number of course-level triplets (i, j, k) in T , which can be n3 in the worst
case. This makes the gradient computation w.r.t. the loss term in (4) expensive.
(b) The large size of matrix A in Rpp . As an example, the number of entries in A for Word
representation on MIT dataset goes to 237 million, making the matrix manipulations
costly both in time and space.
To tackle challenge (a), it is natural for one to consider Stochastic Gradient Descent
(SGD). SGD avoids the expensive summation over all the O(n3 ) triplets by taking a noisy
(instead of exact) gradient step in each iteration, where each noisy gradient step is computed
solely based on one individual triplet randomly sampled from the training data. Stochastic
optimization has been recently successfully applied to triplet-based loss functions, such as in
collaborative filtering with implicit feedback (Rendle, Freudenthaler, Gantner, & SchmidtThieme, 2009). However, the success of SGD crucially relies on the assumption that each
noisy gradient step is sufficiently cheap, which is not true in our case due to challenge (b).
To tackle challenge (b), one would consider solving the dual problem of CGL.Rank
because the dual space may have a smaller number of coefficients to learnin our case, the
p2 entries in A are folded in the kernel matrix. However, as the number of dual variables for
CGL.Rank is equal to the number of triplets, i.e., n3 in the worst case, scalable optimization
in the dual space is still hard.
In the following sections, we first address (b) by reformulating the CGL.Rank problem
in (4) in the way that the optimization objective remains to be equivalent but the number
of variables is substantially reduced (from p2 to n2 ). Then, we address the problem of (a)
with two specific algorithms which have a substantially reduced number of iterations during
the optimization.
3.1 Reduce the Number of Variables
Theorem 3.1 (Variable Reduction). Let the kernel matrix K = XX > , and let h, i be the
matrix inner product. If A is the minimizer for CGL.Rank in optimization (4) and B  is
the minimizer for the following optimization problem
min

BRnn

X
(i,j,k)T

h
i2
ff


1  (KBK)ij + (KBK)ik + KBK, B
2
+
1067

(7)

fiLiu, Ma, Yang, & Carbonell

then we have A = X > B  X.
Proof. First, let us introduce a dummy matrix variable F = XAX >  Rnn , where each
element in F , denoted by Fij , corresponds to our estimated strength of the prerequisite
dependency from course i to course j.
With F we rewrite the unconstrained optimization (4) as a constrained optimization
X

min

ARpp ,F Rnn

(1  Fij + Fik )2+ +

(i,j,k)T

subject to F = XAX


kAk2F
2

(8)

>

Now we are going to show that in optimization (8), the degree of freedom of the optimal
A is actually much smaller than p2 .
To achieve this, we introduce a matrix dual variable M  Rnn corresponding to the n2
equality constraints in F = XAX > . The Lagrangian of (8) can be written as
L (A, F, M ) =

X

(1  Fij + Fik )2+ +

(i,j,k)T

D
E

kAk2F + F  XAX > , M
2

(9)

It is not hard to verity that (8) is a convex optimization with Slaters condition satisfied,
hence strong duality holds. According to the stationarity condition, the derivative of the
Lagrangian w.r.t. A should vanish to zero in the optimal. That is


D
E

kAk2F + F  XAX > , M
2

  > >
= A 
tr X M XA
A
= A  X > M X


L (A, F, M )
=
A
A



(10)

=0
= A = 1 X > M  X. It is worth noticing that while A  Rpp contains p2 variables, A
is completely determined by M   Rnn which only involves n2 variables (n  p).
Now let us define B  1 M  Rnn . Combining A = 1 X > M  X = X > B  X with
the constraint F = XAX > , we have F  = KB  K where K = XX > . Plugging back the
expressions of A and F  to (8) yields optimization (7).
The substantially reduced number of variables in optimization (7) allows us to efficiently
compute and store the gradients in concept graph learning.
3.2 Reduce the Number of Iterations
In this section we introduce two algorithms for optimization (7) which leads to further speed
ups by reducing the total number of iterations.
1068

fiLearning Concept Graphs from Online Educational Data

3.2.1 Accelerated Gradient Descent
Although gradient descent is readily applicable to (7), the convergence can become slow
as we approach the optimal. The smoothness of our objective function (with the squared
hinge loss) enables us to deploy Nestrerovs accelerated gradient descent (Nesterov, 1983,
1988), ensuring a faster convergence rate of O(t2 ) against the rate of O(t1 ) for gradient
descent, where t is the number of gradient steps.
Recall that in Section 3 we have F = XAX > = KBK. Denote by ijk = (1  Fij + Fik )+
and by ei the i-th unit vector in Rn .
The gradient of the objective in (7) w.r.t. B is
ff
 

 F, B
2
(i,j,k)T


X

= 2
ijk  (Fij  Fik ) + tr BKB > K
2
(i,j,k)T
h



i
X
>
= 2
K

tr
BKe
e
K
+ KBK
ijk tr BKej e>
k
i
i

B =

X

 (1  Fij + Fik )2+ +

(11)

(i,j,k)T


= 2K 


X





> 
K + F
ijk ei e>
j  ei ek

(i,j,k)T



P
>
Despite the large number of course-level triplets in T , matrix (i,j,k)T ijk ei e>

e
e
i
j
k
of size n  n can still be computed efficiently since the majority of those triplets are inactive (i.e. ijk = 0 for a large number of triplet (i, j, k)) during the optimization. In fact,
the number of operations required for evaluating the ijk s can be substantially reduced by
maintaining specialized data structures such as the order statistics tree (Cormen, Leiserson, Rivest, & Stein, 2001), which has recently been exploited to speed up the gradient
computation of rankSVM (Lee & Lin, 2014; Airola, Pahikkala, & Salakoski, 2011).
Detailed implementation of CGL.Rank with accelerated gradient descent is summarized
in Algorithm 1.
3.2.2 Inexact Newton Method
It often turns out that the bottleneck of gradient computation, after variable reduction, is a
dense matrix-matrix multiplication in the complexity around O(n2.373 ) (Davie & Stothers,
2013). The multiplication is affordable in our case since the number of courses n we are
dealing with is up to few thousands. However, to scale to substantially larger data collections, one may need to either consider further pruning the per-iteration complexity through
techniques such as low-rank kernel approximation (Williams & Seeger, 2001), or further reducing the total number of iterations. In this section, we focus on the latter by incorporating
second-order information using Newtons method.
The Newtons method we are going to derive is inexact (Dembo, Eisenstat, & Steihaug,
1982). That is, we are about to approximate the Newton direction in each iteration by approximately solving a linear system via preconditioned Conjugate Gradient method (PCG)
without inverting the Hessian. In fact, we are going to avoid explicitly writing the Hessian
1069

fiLiu, Ma, Yang, & Carbonell

Algorithm 1 CGL.Rank with Nestrerovs Accelerated Gradient Descent
1: procedure CGL.Rank.Nestrerov(X, T, , )
2:
K  XX > , B  0, Q  0
3:
t1
4:
while not converge do
5:
0
6:
F  KBK
7:
for (i, j, k) in T do
8:
ijk  1  Fij + Fik
9:
if ijk > 0 then
10:
ij  ij + ijk
11:
ik  ik  ijk
12:
13:
14:
15:
16:
17:

P  B   (F  2KK)
B  P + t1
t+2 (P  Q)
QP
tt+1
A  X > BX
return A
2

2

during the entire optimization process, since our Hessian H  Rn n (corresponding to the
n2 model parameters in B  Rnn ) is extremely large.
Denote by  the Tensor (Kronecker) product operator between matrices, and by eij =
2
ei  ej the (i  n + j)-th unit vector in Rn . The Hessian for (7) can be explicitly derived
H = 2 (K  K)  (K  K) + K  K
(12)
P
where  is a shorthand for (i,j,k)T (eij  eik ) (eij  eik )> .
Denote by vec the vectorization operator which concatenates the columns of a matrix
into a single vector. Based
P on (11) and (12) one can verify it is always true that vec (B ) 
Hvec (B)  2 (K  K) (i,j,k)T (eij  eik ). Therefore, the Newton update is
vec (B)  vec (B)  H 1 vec (B )



X

= vec (B)  H 1 Hvec (B)  2 (K  K)

(eij  eik )

(i,j,k)T

= 2H 1 (K  K)

X

(eij  eik )

(13)

(i,j,k)T

= 2 [ (K  K) + In2 ]1

X

(eij  eik )

(i,j,k)T

Though it is even intractable to compute the n2  n2 matrix inside the inverse operation
in (13), the updated vec(B) (or matrix B) can be well approximated by solving the following
linear system via PCG iterative method
X
2
(eij  eik ) =  (K  K) vec (B) + vec (B)
(14)
(i,j,k)T

1070

fiLearning Concept Graphs from Online Educational Data

The computation bottleneck of PCG lies in matrix-vector multiplication  (K  K) vec (B),
2
2
which seems expensive and requires a huge dense matrix K K  Rn n to be stored in the
memory. Interestingly, we can equivalently write the expression as vec (KBK) by playing
the vec trick for Tensor (Kronecker) product (Van Loan, 2000). After reformulation, the
aforementioned matrix-vector multiplication becomes affordable since vec (KBK) can be
computed in O(n2.373 ) and  is highly sparse.
The above suggests the complexity in each iteration of PCG is as cheap as that in each
gradient step. We also empirically observed that the inexact Newton method only requires
a small constant number (typically 3-5) of Newton updates to reach the optimal, and on
average for each Newton update only 10 PCG iterations suffice to yield good results.

4. Learning a Sparse Concept Graph
Notice the CGL algorithm we studied so far produces a fully dense concept graph A. However, it is commonly believed that dependencies among the knowledge concepts should be
highly sparse. A sparse concept graph is also desirable for visualization purposes thus allowing more intuitive user-exploration. For these reasons, in this section we modify our
CGL algorithm to produce sparse graphs (sparse-CGL).
It is straightforward to enforce sparsity by replacing the `2 -norm overP
concept graph A
in the original CGL formulation to be the `1 -norm defined as kAk1 := i,j |aij |. In this
case, the sparse CGL optimization objective can be cast as
2
X 
>
min
1  x>
Ax
+
x
Ax
+ kAk1
(15)
j
k
i
i
ARpp

+

(i,j,k)T

Optimization (15) can be viewed as a generalization of LASSO (which is known to produce
sparse solutions), except that we are using a pairwise squared hinge loss and that the coefficients are in matrix forms. Unlike in LASSO where one optimizes over a vector coefficient,
the parameter space for (15) is a high-dimensional matrix of extremely large size.
Due to the presence of the `1 -norm in the objective function, our previous parameter
reduction techniques for CGL can no longer be applied to sparse CGL. In the following, we
are going to focus on directly carrying out optimization w.r.t. A. This is feasible because
storing a large but highly sparse concept graph is much cheaper than in the dense case.
4.1 Efficient Optimization for Sparse-CGL
Although sub-gradient methods can be directly applied to minimizing the non-smooth objective in (15), they suffer from slow convergence rate through both theoretical and empirical
perspectives. Commonly used optimization solvers for `1 regularization such as the coordinate descent (CD) (Tseng & Yun, 2009; Chang, Hsieh, & Lin, 2008) no longer has the
closed-form solution in our case for each sub-step, and needs as many as p2 steps to go over
even a single cycle of all the model parameters in A.
4.1.1 Proximal Gradient Descent
Among the family of first-order methods, proximal gradient descent (PGD) has been widely
applied to objective functions involving non-smooth components. It enjoys several desir1071

fiLiu, Ma, Yang, & Carbonell

able computational properties, including having the same order of convergence rate as the
gradient descent even when applied to non-smooth objective functions. Each updating step
of PGD can be efficiently performed as long as the proximal operation is efficient.
The proximal operator for sparse CGL in (15) is defined as
1
kA  Zk22 + kZk1
2tk

proxtk (A) := argmin
ZRpp

(16)

where tk is the step size for the k-th iteration. The solution for optimization (16) can
be expressed concisely in a closed-form:
proxtk (A) = Stk (A)

(17)

where S : Rpp 7 Rpp is known as the soft-thresholding operator (parameterized by  )
applied to each element in A. That is, i, j


Aij   Aij > 
[S (A)]ij = 0
(18)
  Aij  


Aij +  Aij < 
With the proximal operator proxtk (A)  Stk (A), PGD iteratively applies




A(k) = proxtk A(k1)  tk g(A(k1) ) = Stk A(k1)  tk g(A(k1) )

(19)

until the model converges. In the above expression g(A) denotes the gradient of the first
term in optimization (15), which can be further recast as


X
ijk xi (xj  xk )>
(20)
g A(k1) = 2
(i,j,k)T


= 2X > 


X

ijk ei (ej  ek )>  X

(21)

(i,j,k)T
>

= 2X X

(22)

For sparse CGL, PGD is guaranteed to reach the global optimal since both the smooth and
non-smooth components in the objective function (15) are convex over A.
4.1.2 PGD with Nesterovs Acceleration
Similar to the gradient descent for CGL discussed in previous section 3.2.1, the convergence
rate of PGD for sparse-CGL can be further accelerated by applying the Nesterovs method.
In this case, we replace (19) in PGD with



P (k) = Stk A(k1)  tk g A(k1)
(23)


k1
A(k) = P (k) +
P (k)  P (k1)
(24)
k+2
1072

fiLearning Concept Graphs from Online Educational Data

Algorithm 2 Sparse CGL.Rank with Accelerated PGD
1: procedure Sparse.CGL.Rank(X, T, , )
2:
A  0pp , P  0pp , Q  0pp
3:
k1
4:
while not converge do
5:
  0nn
6:
F  XAX >
7:
for (i, j, k) in T do
8:
ijk  1  Fij + Fik
9:
if ijk > 0 then
10:
ij  ij + ijk
11:
ik  ik  ijk
12:
13:
14:
15:
16:
17:

for j = 1, 2 . . . p do

Pj = Stk Aj + 2tk X > Xj
A  P + k1
k+2 (P  Q)
QP
k k+1
return A

where we use P  Rpp to capture the momentum information from historical iterations.
As gradient descent and PGD, the convergence of accelerated PGD is guaranteed for sparse
CGL but with a substantially faster rate.

One might be concerned that in both (19) and (23), the gradient g A(k1) is a dense
matrix by definition (20) and therefore can be expensive in memory consumption throughout
the optimization. To address this issue, recall the soft-thresholding Stk operator is applied
(k)
element-wisely to its input. Let Pi be the i-th column of P (k) , we have
i
h

(k)
(k1)
(25)
Pi = Stk Ai
 tk g A(k1)

 i
(k1)
= Stk Ai
+ 2tk X > N Xi
(26)
The above suggests sequentially threshold A(k1)  tk g A(k1)



via Stk column-by-

(k)
Pi s)

column, and
without storing
 only store the resulting sparse column vectors (the
g A(k1) . Details of accelerated PGD for sparse CGL.Rank is summarized in Alg. 2.

5. Transductive Concept Graph Learning
In real scenarios, the observed course-level prerequisite links are highly sparse. For example,
only 1,173 out of 2,694,681 (0.043%) all possible links has been observed in the MIT data
collection. Meanwhile, we notice that the features of unlabeled course-level links are already
given, and are massively available during the training phase. We argue that transductive
learning should be particular effective in this case for the following reasons:
1. It helps better leverage the unlabeled data by allowing the information to propagate
through both the labeled and unlabeled course pairs.
1073

fiLiu, Ma, Yang, & Carbonell

2. It makes weaker assumptions about the unlabeled (missing) links. This is in contrast
to our previous CGL formulation where all unobserved links are implicitly treated as
negative examples.
To derive transductive CGL, we start with the following equivalent form of CGL, which can
be derived by eliminating A in the original CGL optimization via the constraint F = XAX > .
X

min

F Rnn

(1  Fij + Fik )2+ +

(i,j,k)T


vec(F )> (K  K)1 vec(F )
2

(27)

By viewing the second term in (27) as the negative log-likelihood, we see that the original
CGL formulation essentially assumes vec(F ) is sampled from a Gaussian prior distribution
with covariance matrix K  K where K = XX > .
To allow transduction over both the labeled and unlabeled course-level links, we propose
to replace the inverse pairwise kernel matrix K  K in (27) with its associated graph Lapla2
2
cian matrix L in Rn n (Chung, 1997), following the previous work on label propagation
(Zhu, 2005) and spectral kernel design (Zhang & Ando, 2006). Formally, define
1

1

L := D 2 (D  K  K) D 2

(28)

where D is an n2  n2 diagonal matrix with each of its diagonal elements equal to the corresponding row-sum (degree) of K  K. When the kernel matrix K has been symmetrically
normalized, it is not hard to show that the above definition (28) reduces to
L = I  K  K

(29)

In this case, vec(F )L vec(F ) becomes the (normalized) manifold regularizer (Zhu, Ghahramani, & Lafferty, 2003) which enforces our predictions in F to be smooth over the graph of
course-level links with adjacency matrix K  K. In particular, we have
X
2
vec(F )> L vec(F ) 
kii0 kjj 0 Fi,j  Fi0 ,j 0
(30)
(i,j),(i0 ,j 0 )

Given two course-level links (i, j) and (i0 , j 0 ), recall that kii0 defines the similarity between
i and i0 , kjj 0 denotes the similarity between j and j 0 . Hence kii0 kjj 0 denotes the similarity
between (i, j) and (i0 , j 0 ), and minimizing (30) essentially enforces similar course-level links
to share similar prerequisite strength.
With the intuitions above, we cast the optimization for transductive CGL as
min

F Rnn

X

(1  Fij + Fik )2+ +

(i,j,k)T


vec(F )> L vec(F )
2

(31)

5.1 Efficient Optimization over the Course-Level Links
It is worth mentioning that though the graph Laplacian matrix L of course-level links is
extremely large, it is also highly structured. As a result, we are able to carry out operations
involving L fairly efficiently. Moreover, we are going to show that the explicit storage of
the full graph Laplacian can be avoided during the entire optimization.
1074

fiLearning Concept Graphs from Online Educational Data

Algorithm 3 trans-CGL.Rank with accelerated GD
1: procedure CGL.Rank.Nestrerov(X, T, , )
2:
K  XX > , F  rand(n, n), Q  0nn
3:
t1
4:
while not converge do
5:
  0nn
6:
for (i, j, k) in T do
7:
ijk  1  Fij + Fik
8:
if ijk > 0 then
9:
ij  ij + ijk
10:
ik  ik  ijk
11:
12:
13:
14:
15:
16:

P  F   (F  KF K  2)
F  P + t1
t+2 (P  Q)
QP
tt+1
A  X > K 1 F K 1 X
return A

In the following, we describe how gradient computation can be carried out for transCGL. The gradient of the transductive CGL objective in (31) w.r.t. F is
X
F =
 (1  Fij + Fik )2+ + vec1 (L vec(F ))
(32)
(i,j,k)T

X

= 2

ijk  (Fij  Fik ) + vec1 [(I  K  K) vec(F )]

(33)

(i,j,k)T


= 2 


X

ijk ei (ej  ek )>  + vec1 (vec(F ))  vec1 [(K  K) vec(F )] (34)

(i,j,k)T

= 2 + F  KF K
(35)
hP
i
>
where  :=

e
(e

e
)
. In order to obtain the last equality, we have again
k
(i,j,k)T ijk i j
applied the vectorization trick (Van Loan, 2000) for Tensor (Kronecker) product to the third
term. Note that the expression of gradient in (35) doesnt involve any tensor-type operation,
2
2
despite the huge Laplacian matrix L  Rn n in the original objective function.
Second-order methods, such as the inexact Newton method are applicable to trans-CGL.
We omit the details since the derivations are similar to that for CGL.
5.2 Projecting Back to the Concept-Level Links
The objective for transductive CGL (31) only involves the course-level prerequisite strength
F instead of the concept-level graph A. In order to recover A from the optimal solution
F  for (31), we consider solving the following optimization problem
min

ARpp

kAk22

subject to F  = XAX >
1075

(36)

fiLiu, Ma, Yang, & Carbonell

University
MIT
Caltech
CMU
Princeton

# Courses
2322
1048
83
56

# Prerequisites
1173
761
150
90

# Words
15396
5617
1955
454

Table 1: Datasets Statistics
where the bilinear system F  = XAX > is under-determined as the total number of concepts
p is assumed to be greater than the number of courses n. While there can be multiple feasible
concept graphs associated with F  , (36) aims to pick the concept graph with minimum norm
(which usually indicates strong generalization ability).
Solution for optimization (36) can be derived from its stationarity condition, and can
be written in the closed-form as follows
A = X > K 1 F  K 1 X

(37)

Details of the accelerated gradient descent for trans-CGL.Rank, including the recovery
step for the concept graph, are summarized in Alg. 3.

6. Experiments
We collected course listings, including course descriptions and available prerequisite structure from MIT OpenCourseWare, Caltech, CMU and Princeton2 . The first two were complete course catalogs, and the latter two required spidering and scraping, and hence we
collected only Computer Science and Statistics for CMU, and Mathematics for Princeton.
This implies that we can test within-university prerequisite discovery for all fourthough
MIT and Caltech will be most comprehensiveand cross-university only for university pairs
where the training university contains the disciplines in the test university.
Table 1 summarizes the datasets statistics.
To evaluate performance, we use the Mean Average Precision (MAP) which has been the
preferred metric in information retrieval for evaluating ranked lists, and the Area Under the
Curve of ROC (ROC/AUC or simply AUC) which is popular in link detection evaluations.
6.1 Within-University Prerequisite Prediction
We tested all the methods on the dataset from each university. We used one third of the
data for testing, and the remaining two thirds for training and validation. We conducted
5-fold cross validation on the training two-thirds, i.e., trained the model on 80% of the
training/validation dataset, and tuned extra parameters on the remaining 20%. We repeated
this process 5 times with a different 80-20% spit in each run. The results of the 5 runs were
averaged in reporting results. Figure 3 and Table 2 summarize the results of CGL.Rank,
CGL.Class, 1NN and SVM. All the methods used the English words as the representation
scheme in this first set of experiments.
2. The datasets are available at http://nyc.lti.cs.cmu.edu/teacher/dataset/

1076

fiLearning Concept Graphs from Online Educational Data

on MIT

on Caltech

on CMU

on Princeton

1.00

AUC
MAP

0.80

0.60

0.40

0.20

Cl

Ra

L.

L.

CG

CG

as
s
1N
N
SV
M

nk

N
SV
M

as
s

1N

Cl
L.

CG

CG

L.

Ra

nk

as
s
1N
N
SV
M

Cl
L.

CG

CG

L.

Ra

nk

as
s
1N
N
SV
M

Cl
L.

CG

CG

L.

Ra

nk

0.00

Figure 3: Different methods in within-university prerequisite prediction: All the methods
used words as concepts.

Algorithm
CGL.Rank
CGL.Class
1NN
SVM
CGL.Rank
CGL.Class
1NN
SVM
CGL.Rank
CGL.Class
1NN
SVM
CGL.Rank
CGL.Class
1NN
SVM

Data
MIT
MIT
MIT
MIT
Caltech
Caltech
Caltech
Caltech
CMU
CMU
CMU
CMU
Princeton
Princeton
Princeton
Princeton

AUC
0.96
0.86
0.76
0.78
0.95
0.86
0.60
0.74
0.79
0.70
0.75
0.64
0.92
0.89
0.82
0.71

MAP
0.46
0.34
0.30
0.04
0.33
0.27
0.16
0.03
0.55
0.38
0.43
0.30
0.69
0.61
0.58
0.31

Table 2: Results of within-university prerequisite prediction using words as concepts.

1077

fiLiu, Ma, Yang, & Carbonell

Notice that the AUC scores for all methods are much higher than the MAP scores.
The high AUC scores derive in large part from the fact that AUC gives an equal weight to
the system-predicted true positives, regardless of their positions in system-produced ranked
lists. On the other hand, MAP weighs more heavily the true positives in higher positions of
ranked lists. In other words, MAP measures the performance of a system in a harder task:
Not only the system needs to find the true positives (along with false positives), it also needs
to rank them higher than false positives as possible in order to obtain a high MAP score.
Using a more concrete example, a totally useless system which makes positive or negative
predictions at random with 50% of the chances will have an AUC score of 50%. But this
system will have an extremely low score in MAP because the chance for a true positive to
randomly appear in the top of a ranked list will be low when true negatives dominate in
the domain. Our datasets are from such a domain because each course only requires a very
small number of other courses as prerequisites. Back to our original point, regardless the
popularity of AUC in link detection evaluations, its limitation should be recognized: the
relative performance among methods is more informative than the absolute values of AUC.
As we can see in Figure 3, the relative ordering of the methods in AUC and MAP are
indeed highly correlated across all the datasets. MAP emphasizes positive instances in the
top portion of each ranked list, and hence is more sensible for measuring the usefulness of the
system where the user interacts with the system-recommended ranked list of prerequisites
per query (a course in the test set).
Comparing the results of all the methods, we see that CGL.Rank clearly dominates the
others in both AUC and MAP on most datasets. CGL.Class is the second best, outperforming 1NN and SVM. Comparing 1NN and SVM, these two methods are comparable in
AUC on average; however, 1NN is better than SVM in MAP on all the four datasets.
One may wonder why SVM has the relatively poor performance in course-level prerequisite prediction and in particular under the MAP metric, given that it works well for
text classification and learning to rank for retrieval in general. We argue that SVM is
suffering from the major difficulty in prerequisite prediction due to the extremely sparse
labeled positive training instances (prerequisite pairs). Recall that in text classification
SVM optimizes one w for each category; however, in prerequisite prediction SVM uses the
same w to generate the ranked lists for all possible queries (i.e., test-set courses). Thus the
labeled training instances per query is extremely sparse on average, resulting in the poor
performance we observed. In other words, SVM generalized too much from the extremely
small set of labeled training instances when optimizing the global w. kNN (or 1NN) on
the other hand, suffers less than SVM because the inference in kNN is based on the local
training instances in the neighborhood of each query, instead of a global generalization for
all queries.3 Nevertheless, neither kNN nor SVM is competitive in comparison with our
proposed CGL methods, which is evident in this set of evaluation results.
3. Notice that SVM, as described in (6), is essentially going to produce identical ranked list of prerequisite
courses for any given course. To see this, consider any given course i, SVM scores the remaining courses
>
>
by scorej := (xi  xj )> w = x>
i w  xj w for different js. Since i is given, the first term xi w can be
ignored during the ranking as it is shared across all the scores. As the result, the ranked list becomes
irrelevant to xi itself. The above analysis holds for any course i hence the ranked list of prerequisites
will be identical for all the courses, leading to a low MAP score. In contrast, CGL scores course j by
>
>
scorej = x>
i Axj = (Axi ) xj := wi xj where the ranking coefficient wi is personalized for the i-th
course, and hence is able to produce diverse ranked lists for different courses.

1078

fiLearning Concept Graphs from Online Educational Data

1.00

1.00

AUC
MAP

0.80
0.60

0.60

0.40

0.40

0.20

0.20

0.00

Word

Cat.

SCW

0.00

DWE

(a) CGL.Rank on MIT Data
1.00

0.40

0.40

0.20

0.20
Cat.

SCW

SCW

DWE

AUC
MAP

0.80
0.60

Word

Cat.

1.00

0.60

0.00

Word

(b) CGL.Rank on Caltech Data

AUC
MAP

0.80

AUC
MAP

0.80

0.00

DWE

(c) CGL.Rank on CMU Data

Word

Cat.

SCW

DWE

(d) CGL.Rank on Princeton Data

Figure 4: CGL.Rank with different representation schemes in within-university prerequisite
prediction

6.2 Effects of Representation Schemes
Figure 4 and Table 3 summarize the results of CGL.Rank with the four representation
schemes described in Section 2.1, i.e., Word, Cat, SCW (sparse coding of words) and DWE
(distributed word embedding), respectively. Again the scores of AUC and MAP are not on
the same scale, but the relative performance suggest that Word and Cat are competitive
with each other (or Word is slightly better on some datasets), followed by SCW and then
DWE. In the rest of the empirical results reported in this paper, we focus more on the
performance of CGL.Rank with Word as the representation scheme because it performs
better, and the space limit does not allow us to present all the results for every possible
permutation of method, scheme, dataset and metric.
6.3 Cross-University Prerequisite Prediction
In this set of experiments, we fixed the same test sets which were used in within-university
evaluations, but we alter the training sets across universities, yielding transfer learning results where the models were trained with the data from a different university than those
where they were tested. By fixing the test sets in both within- and cross-university evaluations we can compare the results on a common basis. The competitive performance of Cat
in comparison with Word is encouraging, given that Wikipedia categories are defined as
general knowledge, and the classifiers (SVMs) we used for category assignment to courses
were trained on Wikipedia articles instead of the course materials (because we do not have
1079

fiLiu, Ma, Yang, & Carbonell

Concept
Word
Cat.
SCW
DWE
Word
Cat.
SCW
DWE
Word
Cat.
SCW
DWE
Word
Cat.
SCW
DWE

Data
MIT
MIT
MIT
MIT
Caltech
Caltech
Caltech
Caltech
CMU
CMU
CMU
CMU
Princeton
Princeton
Princeton
Princeton

AUC
0.96
0.93
0.93
0.83
0.95
0.93
0.91
0.76
0.79
0.77
0.73
0.67
0.92
0.84
0.82
0.77

MAP
0.46
0.36
0.33
0.09
0.33
0.32
0.22
0.12
0.55
0.55
0.43
0.35
0.69
0.68
0.60
0.50

Table 3: CGL.Rank with four representations
human assigned Wikipedia category labels for courses). This means that the Wikipedia
categories indeed have a good coverage on the concepts being taught in universities (and
probably MOOC courses), and that our pooling strategy for selecting a relevant subset of
Wikipedia categories is reasonably successful.
Table 4 and Figure 5 show the results of CGL.Rank (using words as concepts). Recall
that the MIT and Caltech data cover the complete course catalogs, while the CMU data
only cover the Computer Science and Statistics, and the Princeton data only over the
Mathematics. This implies that we can only measure transfer learning on pairs where the
training university contains the disciplines in the test university. By comparing the red bars
(when the training university and the test university is the same) and the blue bars (when
the training university is not the same as the test university), we see some performance
loss in the transfer learning, which is expected. Nevertheless, we do get transfer, and this
is the first report on successful transfer learning of educational knowledge, especially the
prerequisite structures in disjoint graphs, across different universities through a unified
concept graph. The results are therefore highly encouraging and suggest continued efforts
to improve. Those results also suggest some interesting points, e.g., MIT might have a
better coverage of the topics taught in Caltech, compared to the inverse. And, MIT courses
seem to be closer to those in Princeton (Math) compared with those of CMU.
6.4 CGL vs Sparse-CGL
In this subsection we evaluate the performance of CGL (CGL.Rank) and sparse-CGL (Section 4) in course-level prerequisite prediction. The two methods were compared under budget
constraints, by which we mean that the number of allowed links in the system-induced concept graph was controlled as the condition for comparison. In other words, we control the
1080

fiLearning Concept Graphs from Online Educational Data

Training
MIT
Caltech
Caltech
MIT
CMU
MIT
Caltech
Princeton
MIT
Caltech

Test
MIT
MIT
Caltech
Caltech
CMU
CMU
CMU
Princeton
Princeton
Princeton

MAP
0.46
0.13
0.33
0.25
0.55
0.34
0.28
0.69
0.46
0.43

AUC
0.96
0.88
0.95
0.86
0.79
0.70
0.62
0.92
0.72
0.58

Table 4: CGL.Rank in within-university and cross-university settings
1.00

MAP

0.80
0.60
0.40
0.20
U
.
Ca MI CM
Pr
lte T.C U
in
ch M
ce
.C U
to
M
n.
U
Ca MI Pri
lte T.P nce
ch rin to
.P ce n
rin to
ce n
to
n

CM

Ca MI
lte T.M
ch IT
.M
Ca
IT
lte
ch
M .Ca
IT lt
.C ec
al h
te
ch

0.00

<Traning Set>.<Test Set>

Figure 5: Results of CGL.Rank based on words in the tasks of within-university (red) and
cross-university (blue) prerequisite prediction.

graph sparsity by varying the number of allowed non-zero elements in A 4 , and compare
the performance of the two methods conditioned on each fixed degree of graph sparsity.
Figure 6 compares the performance of the two methods on the datasets of MIT, Caltech,
CMU and Princeton, in the task of within-university prediction of course-level prerequisite
relations. The horizontal axis of each graph specifies the number of non-zero elements
allowed in matrix A, ranging from 27 to 212 . The vertical axis in each graph is MAP on
4. The concept graph induced by CGL is typically dense, which was sparsified (given the desired sparsity)
by keeping the most dominating elements of A and setting the remaining elements to zero. With sparseCGL, on the other hand, sparse graphs were directly induced by the optimization algorithm by adjusting
the values of `1 -regularization strength .

1081

fiLiu, Ma, Yang, & Carbonell

the left and AUC on the right for each data set. The CGL curves are in blue, and the
sparse-CGL curves are in red.
Clearly, sparse-CGL consistently outperformed CGL in these experiments over most
budget regions on all the datasets, both in MAP and AUC. These results are highly encouraging for effective and efficient interaction or navigation by users over the system-induced
concept graph, where an interpretable and relatively sparse graph is often preferable than
a densely connected graph. The budget constrained graphs would also lead to scalable
curriculum planning and fast computation for on-demand recommendation.
6.5 CGL vs Trans-CGL
In this subsection we compare the course-level prerequisite prediction performance of CGL
(in particular, CGL.Rank) against its transductive extension. The two methods are tested
over the aforementioned four datasets under the words representation scheme. All experiments are conducted under the same 5-fold cross-validation setting as described in 6.1. We
use MAP and AUC as our evaluation metrics and summarize the results in Table 5.
From Table 5 we see that the link prediction performance of trans-CGL dominates that
of CGL. This justifies our previous arguments that making a good use of massive unlabeled
course-level links provided in the training set can help us get better prediction performance.
Meanwhile, we notice that the performance gain obtained by trans-CGL over MIT is
not as large as that over other three institutions. Since the MIT dataset has substantially larger amount of course-level labels, we conjecture that trans-CGL, as many other
transductive/semi-supervised learning approaches in general, is more advantageous when
the available supervision is insufficient (compared to the amount of hidden information in
the unlabeled data). To validate this thought, we repeat the experiments of the two methods over the down-sampled MIT dataset. That is, we only use a random subset of available
course-level prerequisite links for training, and gradually vary the size of the training subset
to get multiple set of results. The results are summarized in Table 6.
Institution

MIT

Caltech

CMU

Princeton

MAP

CGL
trans-CGL

0.482
0.485

0.477
0.499

0.482
0.539

0.436
0.445

AUC

CGL
trans-CGL

0.956
0.957

0.929
0.941

0.801
0.818

0.634
0.67

Table 5: Comparison of the course prerequisite prediction performance between CGL and
trans-CGL over MIT, Caltech, CMU and Princeton. Results of the significance
tests (paired t-tests) between the best method against the other method on each
dataset is denoted by a  for significance at 1% level.

6.6 Experiment Details
We tested the efficiency of our proposed algorithms (based on the optimization formulation
after variable reduction) on a single machine with an Intel i7 8-core processor and 32GB
1082

fiLearning Concept Graphs from Online Educational Data

MAP over MIT

AUC over MIT

0.45

0.96
0.94
0.92
0.9
0.88
0.86
0.84
0.82
0.8

0.4
0.35
0.3
0.25
0.2

CGL
sparse-CGL

0.15
0.1

CGL
sparse-CGL
96

40

48

20

24

10

2

51

6

25

8

12

96

40

48

20

24

10

2

51

6

25

8

12

MAP over Caltech

AUC over Caltech

0.22

0.85

0.2

0.8

0.18

0.75

0.16

0.7

0.14

CGL
sparse-CGL

0.12
0.1

CGL
sparse-CGL

0.65
0.6

6

9
40

8

4
20

4

2
10

0.44
0.43
0.42
0.41
0.4
0.39
0.38
0.37
0.36
0.35
0.34
0.33

2
51

6
25

8
12

6

9
40

8

4
20

4

2
10

2
51

6
25

8
12

MAP over CMU

AUC over CMU
0.82
0.8
0.78
0.76
0.74
0.72

CGL
sparse-CGL

CGL
sparse-CGL

0.7
0.68

6

9
40

8

4
20

4

2
10

2
51

6
25

8
12

6

9
40

8

4
20

4

2
10

2
51

6
25

8
12

MAP over Princeton

AUC over Princeton

0.44

0.67

0.42

0.66

0.4

0.65

0.38

0.64

0.36
0.34

0.63

CGL
sparse-CGL

0.32
0.3

CGL
sparse-CGL

0.62
0.61

96

40

48

20

24

10

2

51

6

25

8

12

96

40

48

20

24

10

2

51

6

25

8

12

Figure 6: Comparison of CGL (blue curves) and sparse-CGL (red curves) in the prediction
of course-level prerequisite relations within MIT, Caltech, CMU and Princeton,
respectively. The x-axis of each graph specifies the budget constraint in logscale, which is the number of allowed non-zero elements in matrix A. The y-axis
measures the performance in MAP on the left, and in AUC on the right.

1083

fiLiu, Ma, Yang, & Carbonell

Training Data
Down-sampling Rate

1
10

1
9

1
8

1
7

1
6

1
5

1
4

1
3

1
2

MAP

CGL
trans-CGL

0.183
0.202

0.149
0.153

0.192
0.213

0.231
0.249

0.245
0.26

0.264
0.285

0.254
0.262

0.258
0.274

0.289
0.307

AUC

CGL
trans-CGL

0.843
0.874

0.832
0.838

0.842
0.845

0.872
0.875

0.873
0.872

0.876
0.892

0.873
0.868

0.897
0.91

0.905
0.914

Table 6: Comparison of the course prerequisite prediction performance between CGL and
trans-CGL over the MIT dataset, as the training size varies from 10% to 50%.

RAM. On the largest MIT dataset with 981,009 training triplets and 490,505 test triplets,
CGL.Rank with gradient descent took 37.3 minutes and 1490 iterations to reach the convergence rate of 103 . To achieve the same objective value, the accelerated gradient descent
took 3.08 minutes with 401MB memory at 103 iterations, and the inexact Newton method
took only 43.4 seconds with 587MB memory. CGL.Class is equally efficient as CGL.Rank
in terms of run time, though the latter is superior in terms of result quality. As for our 1NN
baseline, it took 2.88 hours since a huge number (2  981, 009  490, 505) of dot-products
need to be computed on the fly.
The CPU time consumed by trans-CGL.Rank is similar to that by CGL.Rank. As for
sparse-CGL, it took the accelerated proximal gradient method 2.07 minutes to reach the
convergence rate of 103 on MIT with 3.9GB peak memory consumption. The resulting
concept graph has 1519 nonzero entries.

7. Discussion and Related Work
Whereas the task of inferring prerequisite relations among courses and concepts, and the
task of inferring a concept network from a course network in order to transfer learned
relations are both new, as are the extensions to the SVM algorithms presented, our work
was inspired by methods in related fields, primarily:
In collaborative filtering via matrix completion the literature has focused on only one
graph, such as a bipartite graph of u users and m preferences (e.g. for movies or products).
Given some known values in the u-by-m matrix, the task is to estimate the remaining values
(e.g. which other unseen movies or products would each user like) (Su et al., 2009). This
is done via methods such as affine rank minimization (Fazel, 2002) that reduce to convex
optimization (Boyd & Vandenberghe, 2004).
Another line of related research is transfer learning (Do & Ng, 2005; Yang, Hanneke,
& Carbonell, 2013; Zhang, Ghahramani, & Yang, 2008). We seek to transfer prerequisite
relations between pairs of courses within universities to other pairs also within universities,
and to pairs that span universities. This is inspired by but different from the transfer
learning literature. Transfer learning traditionally seeks to transfer informative features,
priors, latent structures and more recently regularization penalties (Kshirsagar, Carbonell,
& Klein-Seetharaman, 1990). Instead, we transfer shared concepts in the mappings between
course-space and concept-space to induce prerequisite relations.
Although our evaluations primarily focus on detecting prerequisite relations among
courses, such a task is only one direct application of the automatically induced univer1084

fiLearning Concept Graphs from Online Educational Data

Linear_algebra
Numerical_analysis
Probability_theory
Applied_mathematics_stubs

Functional_analysis

Integral_calculus

Cybernetics

Mathematical_analysis_stubs

Randomness

Mathematical_optimization

Operations_research

Differential_geometry

Fourier_analysis

Computational_science Signal_processing
Dynamical_systems

Figure 7: A visualization of concept graph produced by CGL.Rank based on 10,051 academic Wikipedia categories, 2322 courses and 1173 prerequisite relations in MIT
OpenCourseWare. Each node denotes a concept (i.e. a Wikipedia category), and
the strength of each link encodes the prerequisite strength between a pair of concepts. Concepts of small degrees and links with weak strength are removed via
thresholding for visualization purposes.

sal concept graph. Other important applications include automated or semi-automated
curriculum planning for personal education goals based on different backgrounds of students, and modularization in course syllabus design by instructors. Both tasks require the
interaction between humans (students or teachers) and the system-induced concept graph as
well as the system-recommended options and optimal sequences of ordered courses. Figure
7 visualizes a sub-graph in the system-induced concept space based on the course materials
(including partially observed prerequisite structure) from MIT OpenCourseWare: the nodes
are Wikipedia categories (concepts), and the links are system-predicted partial orders for
instructors to follow in teaching those concepts, or for students to follow in learning those
concepts. As one can see, Linear Algebra, Probability Theory and Functional Analysis are
the hubs (with a high degree of out-links) in the graph, indicating that those concepts
are more fundamental for one to acquire before pursuing more advanced topics such as
Differential Geometry, Cybernetics and Fourier Analysis.
Let us further illustrate the potential use of CGL.Rank and its further development
for the problem of inferring prerequisite relationships when observed prerequisites among
courses are (almost) not available, e.g., in the context of MOOC. Recall that MOOC courses
1085

fiLiu, Ma, Yang, & Carbonell

weights from X

bigdataanalytics

bioinfomethods2

patterndiscovery

Target courses

weights from A

weights from X

Data_warehousing

Relational_database_management_systems

Data_security

Distributed_computing_architecture

Computer_science_stubs

Project_management_software

Business_intelligence

Information

Information_technology_management

Information_science

Data_modeling

Project_management

Design

Data_management

Software_design_patterns

Urban_studies_and_planning

Data_management

Information_technology

Database_management_systems

Database_management_systems

Target concepts

Prerequisite concepts

statinference
getdata
exdata
compmethods
introstats
dataanalysis
predmachlearn

Prerequisite courses

Figure 8: An example of prerequisite course recommendation on Coursera using the concept graph learned from the MIT OpenCourseWare dataset. We map the courses
(red, left) that the student wants to learn to the concept space of Wikipedia categories, find the prerequisite concepts and then map back to Coursera courses
(red, right). The sizes of the concept nodes in the middle (green) are proportional to aggregated weights of the corresponding links, and the strengths of
course-concept mapping and concept-concept prerequisite relations are shown by
the color intensity of edges (purple).

are offered by different institutions across the world, where cross-provider prerequisite links
among courses are not explicitly available. For instance, among the 900+ courses that
Coursera currently offers, about a half of them do not mention anything about the required
background; as for the remaining half, the mentioned background requirements are often
vague, e.g., Undergraduate-level networking know-how is recommended for the course of
Cloud Networking. Such overly generic and vague notions are not sufficiently informative for
students to understand the true prerequisite relationship among courses, and also not very
useful for intelligent systems to learn the prediction of latent prerequisites automatically. 5
The example in Figure 8 illustrates the key idea of applying the CGL.Rank algorithm
to address the problems above. The first column on the left consists of three Coursera
courses that a student wants to take, i.e., Big Data Analytics for Healthcare (bigdataanalytics), Bioinformatic Methods II(bioinfomethods2 ) and Pattern Discovery in Data
Mining(patterndiscovery), respectively. The second column of nodes are the top-10 universal concepts (Wikipedia categories) assigned by our classifiers (Section 2.1 Cat) to these
courses, and the color intensity of the edges between the 1st and the 2nd columns reflects
5. Coursera does offer so-called specializations, where each specialization consists of a sequence of courses
on the same topic. It is unclear whether those specializations may serve as prerequisite relations.

1086

fiLearning Concept Graphs from Online Educational Data

the confidence scores (matrix X) of category assignments. The size of each concept node is
proportional to the aggregated confidence scores of the corresponding edges, indicating the
relative importance of that concept. The third column of nodes are the top-10 prerequisite
concepts (also from Wikipedia categories) of the concepts shown in the 2nd column, and
the color intensity of the edges between the 2st and 3nd columns reflects the automatically
induced strengths of concept-level links (Matrix A) by our CGL.Rank algorithm, which
used the MIT OpenCourseWare data as the training set. The nodes in the 4th column are
the Coursera courses that best cover the prerequisite concepts in the 3rd column; the edges
from the 3rd column are similar to that between the 1st and 2nd columns. Together, the
chained network allows us to make an inference about the connections between the target
courses (the left-most column) and the prerequisite courses (the right-most column).
As the reader can see in Figure 8, many of the courses and prerequisites are highly
relevant and focus on the primary dimension of data analytics, but still constitute an incomplete set as the second dimension of biotechnology is not represented. We are in the
process of refining our methods before thorough testing can be performedwe offer this
example as work-in-progress indicating current challenges and future directions.

8. Concluding Remarks
We conducted a new investigation on automatically inferring directed graphs at both the
course level and the concept level, to enable prerequisite prediction for within-university
and cross-university settings.
We proposed three approaches: a classification approach (CGL.Class), a learning to rank
approach (CGL.Rank), and a nearest-neighbor search approach (kNN). Both CGL.Class and
CGL.Rank (deploying adapted versions of SVM algorithms) explicitly model concept-level
dependencies through a directed graph, and support an interlingua-style transfer learning
across universities, while kNN makes simpler prediction without learning concept dependencies. To tackle the extremely high-dimensional optimization in our problems (e.g., 2  108
links in the concept graph for the MIT courses), our novel reformulation of CGL.Rank
enables the deployment of fast numerical solutions. On our newly collected datasets from
MIT, Caltech, CMU and Princeton, CGL.Rank proved best under MAP and ROC/AUC,
and computationally much more efficient than kNN.
We extended the aforementioned CGL algorithm to further produce a sparse concept
graph based on `1 -regularization (sparse-CGL), and to leverage the information in massive
unlabeled course pairs based on graph-regularization (trans-CGL). We developed scalable
optimization strategies in support of these new formulations, and conducted experiments
which empirically showed that sparse-CGL was able to give a more interpretable concept
graph, and that trans-CGL significantly and consistently improved the performance of the
ordinary CGL in course prerequisite prediction.
We also tested four representation schemes for course content: using the original words,
using Wikipedia categories as concepts, using a distributed word representation, and using
sparse word encoding. The first two: original words and Wikipedia-derived concepts proved
best. Our results in both the within- and cross-university settings are highly encouraging.
1087

fiLiu, Ma, Yang, & Carbonell

We envision that the cross-university transfer learning of our approaches is particularly
important for MOOCs where courses come from different providers and across institutions,
where there are seldom any explicit prerequisite links. A rich suite of future work includes:
 Testing on cross-institution or cross-course-provider prerequisite links. We have tested
cross-university transfer learning, but the inferred links are within each target university, rather than cross-institutional links. A natural extension of the current work is
to predict cross-institutional prerequisites. For evaluation we will need labeled ground
truth of cross-institutional prerequisites.
 Cross-language transfer. Using the Wikipedia categories and entries in different languages, it would be an interesting challenge to infer prerequisite relations for courses
in different languages by mapping to the Wikipedia category/concept interlingua.
 Extensions of the inference from single source to multiple sources, from single media
(text) to multiple media (including videos), and from single granularity level (courses)
to multiple levels (including lectures).
 Deploying the induced concept graph for personalized curriculum planning for students (as in Section 7) and for syllabus design and course modularization by teachers.

Acknowledgments
We thank the reviewers for their helpful comments. This work is supported in part by the
National Science Foundation under grants IIS-1216282, IIS-1350364 and IIS-1546329.

References
MIT OpenCourseWare. http://ocw.mit.edu/index.htm. Accessed: 2016-03-31.
Airola, A., Pahikkala, T., & Salakoski, T. (2011). Training linear ranking SVMs in linearithmic time using redblack trees. Pattern Recognition Letters, 32 (9), 13281336.
Al-Rfou, R., Perozzi, B., & Skiena, S. (2013). Polyglot: Distributed word representations
for multilingual NLP. CoNLL-2013, 183.
Boyd, S., & Vandenberghe, L. (2004). Convex optimization. Cambridge university press.
Candes, E. J., & Recht, B. (2009). Exact matrix completion via convex optimization.
Foundations of Computational mathematics, 9 (6), 717772.
Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2008). Coordinate descent method for large-scale
l2-loss linear support vector machines. The Journal of Machine Learning Research, 9,
13691398.
Chen, Y., Perozzi, B., Al-Rfou, R., & Skiena, S. (2013). The expressive power of word embeddings. ICML 2013 Workshop on Deep Learning for Audio, Speech, and Language
Processing.
Chung, F. R. K. (1997). Spectral graph theory, Vol. 92. American Mathematical Soc.
1088

fiLearning Concept Graphs from Online Educational Data

Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011).
Natural Language Processing (almost) from Scratch. The Journal of Machine Learning Research, 12, 24932537.
Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2001). Introduction to algorithms.
MIT Press and McGraw-Hill.
Davie, A. M., & Stothers, A. J. (2013). Improved bound for complexity of matrix multiplication. Proceedings of the Royal Society of Edinburgh: Section A Mathematics,
143 (02), 351369.
Dembo, R. S., Eisenstat, S. C., & Steihaug, T. (1982). Inexact Newton methods. SIAM
Journal on Numerical analysis, 19 (2), 400408.
Do, C., & Ng, A. Y. (2005). Transfer learning for text classification. In Proceedings of
NIPS-05.
Fazel, M. (2002). Matrix rank minimization with applications. Ph.D. thesis, Stanford University.
Gopal, S., & Yang, Y. (2013). Recursive regularization for large-scale classification with
hierarchical and graphical dependencies. In Proceedings of the 19th ACM SIGKDD
international conference on Knowledge discovery and data mining, pp. 257265. ACM.
Hoyer, P. O. (2004). Non-negative matrix factorization with sparseness constraints. The
Journal of Machine Learning Research, 5, 14571469.
Joachims, T., Li, H., Liu, T.-Y., & Zhai, C. (2007). Learning to rank for information
retrieval (LR4IR 2007). In SIGIR Forum, Vol. 41, pp. 5862.
Johnson, C. R. (1990). Matrix completion problems: a survey. In Proceedings of Symposia
in Applied Mathematics, Vol. 40, pp. 171198.
Kim, H., & Park, H. (2008). Nonnegative matrix factorization based on alternating nonnegativity constrained least squares and active set method. SIAM Journal on Matrix
Analysis and Applications, 30 (2), 713730.
Kshirsagar, M., Carbonell, J., & Klein-Seetharaman, J. (1990). Transfer learning based
methods towards the discovery of host-pathogen protein-protein interactions. In Proc
of ISMB, Vol. 40, pp. 171198.
Kunegis, J., & Lommatzsch, A. (2009). Learning spectral graph transformations for link
prediction. In Proceedings of the 26th Annual International Conference on Machine
Learning, pp. 561568. ACM.
Le, Q., & Mikolov, T. (2014). Distributed representations of sentences and documents. In
Proceedings of the 31st International Conference on Machine Learning (ICML-14),
pp. 11881196.
Lee, C.-P., & Lin, C.-J. (2014). Large-scale linear rankSVM. Neural computation, 26 (4),
781817.
Lee, D. D., & Seung, H. S. (1999). Learning the parts of objects by non-negative matrix
factorization. Nature, 401 (6755), 788791.
1089

fiLiu, Ma, Yang, & Carbonell

Liben-Nowell, D., & Kleinberg, J. (2007). The link-prediction problem for social networks.
Journal of the American society for information science and technology, 58 (7), 1019
1031.
Lichtenwalter, R. N., Lussier, J. T., & Chawla, N. V. (2010). New perspectives and methods
in link prediction. In Proceedings of the 16th ACM SIGKDD international conference
on Knowledge discovery and data mining, pp. 243252. ACM.
Nesterov, Y. (1983). A method of solving a convex programming problem with convergence
rate o (1/k2). In Soviet Mathematics Doklady, Vol. 27, pp. 372376.
Nesterov, Y. (1988). On an approach to the construction of optimal methods of minimization
of smooth convex functions. Ekonomika i Mateaticheskie Metody, 24, 509517.
Rendle, S., Freudenthaler, C., Gantner, Z., & Schmidt-Thieme, L. (2009). BPR: Bayesian
personalized ranking from implicit feedback. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pp. 452461. AUAI Press.
Su, X., & Khoshgoftaar, T. M. (2009). A survey of collaborative filtering techniques. Advances in artificial intelligence, 2009, 4.
Tseng, P., & Yun, S. (2009). A coordinate gradient descent method for nonsmooth separable
minimization. Mathematical Programming, 117 (1-2), 387423.
Van Loan, C. F. (2000). The ubiquitous Kronecker product. Journal of computational and
applied mathematics, 123 (1), 85100.
Williams, C., & Seeger, M. (2001). Using the Nystrom method to speed up kernel machines.
In Proceedings of the 14th Annual Conference on Neural Information Processing Systems, No. EPFL-CONF-161322, pp. 682688.
Yang, L., Hanneke, S., & Carbonell, J. (2013). A theory of transfer learning with applications
to active learning. Machine learning, 90 (2), 161189.
Yang, Y., Liu, H., Carbonell, J., & Ma, W. (2015). Concept graph learning from educational
data. In Proceedings of the Eighth ACM International Conference on Web Search and
Data Mining, pp. 159168. ACM.
Zhang, J., Ghahramani, Z., & Yang, Y. (2008). Flexible latent variable models for multi-task
learning. Machine Learning, 73 (3), 221242.
Zhang, T., & Ando, R. (2006). Analysis of spectral kernel design based semi-supervised
learning. Advances in neural information processing systems, 18, 1601.
Zhu, X. (2005). Semi-supervised learning literature survey. Tech. rep. TR 1530, University
of Wisconsin - Madison.
Zhu, X., Ghahramani, Z., & Lafferty, J. (2003). Semi-supervised learning using Gaussian
fields and harmonic functions. In Proceedings of ICML-03, Vol. 3, pp. 912919.

1090

fiJournal of Artificial Intelligence Research 55 (2016) 443-497

Submitted 11/14; published 02/16

Optimally Solving Dec-POMDPs as Continuous-State MDPs
Jilles Steeve Dibangoye

jilles-steeve.dibangoye@insa-lyon.fr

Univ de Lyon
INSA-Lyon, CITI-Inria, F-69621, France

Christopher Amato

camato@cs.unh.edu

University of New Hampshire
Durham, NH, USA

Olivier Buffet
Francois Charpillet

olivier.buffet@inria.fr
francois.charpillet@inria.fr

Inria  Universite de Lorraine  CNRS
Villers-les-Nancy, F-54600, France

Abstract
Decentralized partially observable Markov decision processes (Dec-POMDPs) provide a general model for decision-making under uncertainty in decentralized settings, but are difficult to solve
optimally (NEXP-Complete). As a new way of solving these problems, we introduce the idea of
transforming a Dec-POMDP into a continuous-state deterministic MDP with a piecewise-linear and
convex value function. This approach makes use of the fact that planning can be accomplished in a
centralized offline manner, while execution can still be decentralized. This new Dec-POMDP formulation, which we call an occupancy MDP, allows powerful POMDP and continuous-state MDP
methods to be used for the first time. To provide scalability, we refine this approach by combining heuristic search and compact representations that exploit the structure present in multi-agent
domains, without losing the ability to converge to an optimal solution. In particular, we introduce
a feature-based heuristic search value iteration (FB-HSVI) algorithm that relies on feature-based
compact representations, point-based updates and efficient action selection. A theoretical analysis
demonstrates that FB-HSVI terminates in finite time with an optimal solution. We include an extensive empirical analysis using well-known benchmarks, thereby demonstrating that our approach
provides significant scalability improvements compared to the state of the art.

1. Introduction
Many significant real-world problems involve decision making in sequential multiagent environments. Examples include: exploration robots that must coordinate to perform experiments on an
unknown planet (Zilberstein, Washington, Bernstein, & Mouaddib, 2002); rescue robots that, after
a disaster, must safely find victims as quickly as possible (Paquet, Chaib-draa, Dallaire, & Bergeron, 2010); optimized distributed congestion control in a noisy computer network (Winstein &
Balakrishnan, 2013); sensor networks where multiple sensors work jointly to perform a large-scale
sensing task under strict power constraints (Jain, Taylor, Tambe, & Yokoo, 2009); or robot logistics
problems with communication limitations and sensor uncertainty (Amato, Konidaris, Anders, Cruz,
How, & Kaelbling, 2015). All these tasks require multiple decision makers, or agents, to coordinate
their actions in order to achieve common long-term goals. Additionally, uncertainty is ubiquitous
in these domains, both in the effects of actions and in the information received by the agents.

c
2016
AI Access Foundation. All rights reserved.

fiDibangoye, Amato, Buffet & Charpillet

Markov decision processes (MDPs) address uncertainty in system dynamics, but assume centralization. Standard methods for solving MDPs, e.g., linear and dynamic programming (Bellman,
1957; Puterman, 1994) and heuristic search (Barto, Bradtke, & Singh, 1995; Hansen & Zilberstein, 2001), are centralized during both the planning and the execution phases. Partially observable
Markov decision processes (POMDPs) extend MDPs to situations in which there is uncertainty over
the state of the system (Kaelbling, Littman, & Cassandra, 1998; Smith & Simmons, 2006; Shani,
Pineau, & Kaplow, 2013), but similarly assume centralized planning and execution.
To use the MDP and POMDP models when multiple agents are present, a centralized coordinator agent must have a global view of the underlying state (or belief state in the case of a POMDP)
of the entire system, and plan on behalf of its teammates. Every time step, this agent would transmit the appropriate action that each agent must perform and then observe the resulting state (or
observations of each agent in a POMDP). These methods, collectively referred to as centralized
planning for centralized control, assume agents communicate at each step with no delay or cost,
either explicitly through messages or implicitly through observations. Unfortunately, in many practical applications, agents are not permitted to share their information with no delay or cost; rather,
each agent possesses only local, unshared observations, and acts without full knowledge of what
others observe or plan to do. These characteristics have led to the development of a rich body of
research on decentralized decision-making under uncertainty.
The decentralized partially observable Markov decision process (Dec-POMDP) is a standard
formulation for cooperative decision-making in these sequential settings without instantaneous, free
and noiseless communication (Bernstein, Givan, Immerman, & Zilberstein, 2002). Over the past
decade, there has been extensive research on solution methods for Dec-POMDPs, using methods
such as dynamic programming (Hansen, Bernstein, & Zilberstein, 2004; Boularias & Chaib-draa,
2008; Amato, Dibangoye, & Zilberstein, 2009), optimization (Aras & Dutech, 2010; Amato, Bernstein, & Zilberstein, 2010) and heuristic search (Szer, Charpillet, & Zilberstein, 2005; Oliehoek,
Spaan, & Vlassis, 2008; Oliehoek, Spaan, Amato, & Whiteson, 2013). These approaches directly
search for an optimal solution in the space of possible solutions (or policies) but become intractable
for larger problems. This is not unexpected given the worst-case NEXP complexity of finite-horizon
Dec-POMDPs (Bernstein et al., 2002).
A key assumption in many Dec-POMDP algorithms is that planning can be centralized as long
as execution remains decentralized (Hansen et al., 2004; Boularias & Chaib-draa, 2008; Amato
et al., 2009; Szer et al., 2005; Oliehoek et al., 2008, 2013). That is, these methods represent centralized planning for decentralized control  a centralized planner generates a tuple of individual
solutions, one individual solution for each agent. While the use of the Dec-POMDP model does not
require centralized planning (e.g., Velagapudi, Varakantham, Sycara, & Scerri, 2011; Wu, Zilberstein, & Chen, 2011; Banerjee, Lyle, Kraemer, & Yellamraju, 2012), because Dec-POMDPs are a
cooperative framework it is common to assume that centralized planning is possible. Unfortunately,
current algorithms do not take full advantage of this centralized planning assumption.
This article extends a conference paper published at IJCAI13 (Dibangoye, Amato, Buffet, &
Charpillet, 2013), which includes the introduction of a centralized solution method that recasts a
Dec-POMDP as a continuous-state MDP with more detailed background, new theorems and proofs,
as well as more concise representations of policies and value functions. Our novel method is also
able to produce decentralized solutions, but leverages work on centralized planning methods to significantly increase scalability. Furthermore, we show that the optimal value function of the aforementioned MDP is a piecewise-linear and convex function. In this form, theory from POMDPs
444

fiOptimally Solving Finite-Horizon Dec-POMDPs

(Kaelbling et al., 1998) applies, allowing POMDP algorithms to produce optimal solutions for DecPOMDPs. A wide range of POMDP algorithms, which have demonstrated significant scalability
(Shani et al., 2013), can now be applied. We extend one such heuristic search algorithm (Smith &
Simmons, 2004) to the Dec-POMDP case, but because the number of states and actions in this MDP
grows exponentially with the planning horizon, scalability remains limited.
To increase scalability, we introduce a novel mechanism that refines this centralized solution
methodology and present ways to combine classical heuristic search and compact representations,
without losing the ability to converge to an optimal solution. To incorporate compact representations, we build on feature-based dynamic programming (Tsitsiklis & van Roy, 1996), which includes
feature extraction and value prediction with approximation methods. We introduce the feature-based
heuristic search value iteration (FB-HSVI) algorithm that relies on compact representations, pointbased updates and efficient action selection. A theoretical analysis demonstrates that FB-HSVI terminates in finite time with an optimal solution. This combination of POMDP theory and compact
representations can greatly reduce the problem size and solution efficiency while retaining optimal
solutions for Dec-POMDPs.

Action2

Observation2

Observation2

oa,z ( s)

start

Action2

Observation2

oa,z ( s)

State

ra (s)

oa,z ( s)

State

pa (s, s)

ra (s)

Agent 2s World

State

pa (s, s)

Hidden
oa,z ( s)

oa,z ( s)

Observation1

Time

oa,z ( s)

Observation1

Agent 1s World

Observation1

Action1

Action1

t

t+1

t+2

Figure 1: A graphical model of the two-agent Dec-POMDP model.

445

fiDibangoye, Amato, Buffet & Charpillet

2. Background on Dec-POMDPs
This section introduces the basic components of decentralized partially observable Markov decision
processes (Dec-POMDPs).
2.1 Problem Definition and Notations
Consider multiple agents that are faced with the task of influencing a stochastic system as it evolves
over time (see the two agent case in Figure 1). At every time step, each agent receives a private
observation that gives (possibly) incomplete and noisy information about the current state of the
system. Since the states are not observable, an agent cannot choose its actions based on the states.
Instead, it can consider a complete history of its past actions and observations to choose an action.
Actions produce a common immediate reward, and the system evolves to a new state at the next
time step according to a probability distribution conditioned on actions. At the next time step,
agents face a similar problem again, but now the system may be in a different state. The goal
of agents is to choose the actions based on these local action-observation sequences which cause
the system to perform optimally with respect to a shared performance criterion (which we discuss
below). The Dec-POMDP model formalizes these interactions between agents and the system. This
paper formulates and solves this general decentralized stochastic control problem for a process that
operates for a finite planning horizon.
Definition 1. A Dec-POMDP is represented as a tuple M  (I, S , A, Z, p, o, r, b0 , T ), where:
 I is a finite set of agents i  {1, 2, . . . , |I|};
 S is a finite set of n states;
 Ai is the finite set of agent is actions; A  i Ai is the finite set of joint actions;
 Z i is the finite set of agent is observations; Z  i Z i is the finite set of joint observations;
 p = {pa | a  A} denotes the transition model. pa is an n  n stochastic matrix, where pa (s, s)
is the probability of transitioning to state s if the agents choose joint action a in state s;
 o = {oa,z | a  A, z  Z} is the observation model. oa,z is an n  1 vector1 , where oa,z ( s) is the
probability of observing z if joint action a is performed and the resulting state is s;
 r = {ra | a  A} is the reward function; ra is a 1  n reward vector, where ra (s) is the bounded
reward obtained by executing joint action a in state s;
 b0 is the initial probability distribution over states; and
 T is the number of decision steps t  {0, 1, . . . , T  1} (the problem horizon).
def

Remark 1. We often use shorthand notation pa,z (s, s) = oa,z ( s)pa (s, s), for all s, s  S , a  A, and
z  Z, combining the transition and observation models. That is, the probability of transitioning to
state s after observing z if joint action a is performed and the resulting state is s.
1. The observation vector is not a stochastic vector.

446

fiOptimally Solving Finite-Horizon Dec-POMDPs

To make the representation more concrete, we discuss a very simple Dec-POMDP, namely the
multi-agent tiger problem (Nair, Tambe, Yokoo, Pynadath, & Marsella, 2003). We will revisit this
problem in later sections to clarify the ideas presented in the paper.
Example 1 (Multi-agent tiger  problem description). In the multi-agent tiger problem, two agents
stand in front of two closed doors. Behind one of the doors there is a hungry tiger, and behind the
other door there is valuable treasure. The agents do not know the position of either. By listening,
rather than simply opening one of the doors, the agents can gain information about the position
of the tiger. But listening has a cost and is not entirely accurate (i.e., it only reveals the correct
information about the location of the tiger with some probability). Moreover, agents cannot communicate their observations to each other. At each step, each agent can independently either listen
or open one of the doors. If one of the agents opens the door with the treasure behind it (while the
other agent listens or also opens the correct door), they both get the reward. If either agent opens
the door with the tiger, a large penalty is incurred. However, if they both open the tiger door at
the same time, they receive a smaller penalty. The agents must make decisions about listening and
opening doors based on the local observations. After a door is opened and the agents receive a
reward or penalty, the problem starts over again and the tiger is randomly repositioned.
We refer to the state of the multi-agent tiger world when the tiger is on the left as stl (tiger left)
and when it is on the right as str (tiger right). The actions for each agent are aol (open left), aor
(open right), and al (listen). There are only two possible observations for each agent (even after
opening a door): to hear the tiger on the left zhl (hear left) or to hear the tiger on the right zhr (hear
right). The reward function is defined as shown on Table 1.
actions of
both agents
listens
opens good door
opens bad door

listens
2
+9
101

opens
good door
+9
+20
100

opens
bad door
101
100
50

Table 1: Reward function definition for the multi-agent tiger problem
The transition and observation models can be described in detail as follows. The joint action
(al , al ) does not change the state of the world. Any other joint action causes a transition to state
stl with probability 0.5 and to state str with probability 0.5  essentially resetting the problem.
When the world is in state stl , the joint action (al , al ) results in observation zhl for either agent with
probability 0.85 and observation zhr with probability 0.15; conversely for state str . No matter what
state the world is in, the other joint actions result in either observation with probability 0.5.
This example illustrates a small Dec-POMDP. Even in this small Dec-POMDP, coordination is
difficult due to uncertainty about the location of the tiger and the actions of the other agent.
2.2 Preliminaries
Given a T -step Dec-POMDP M, we would like agents to act in such a way as to maximize some
common measure of long-term return in M. The challenge in Dec-POMDPs is that each agents
strategy typically must take the other agents strategies into account. To this end, we discuss agent
447

fiDibangoye, Amato, Buffet & Charpillet

and team decision rules and policies that allow the agents to act based on local information, while
attempting to maximize a joint objective.
2.2.1 Private Decision Rules and Policies
At every time step, each agent chooses an action to be executed based on the actions the agent has
previously executed and the observations that it has received. This is called a policy. To better
understand this concept, we introduce the notions of private histories and decision rules.
Definition 2. A step-t private history of agent i  I is a length-t sequence of actions and observadef
tions, ti = (ai0 , zi1 , . . . , zit1 , ait1 , zit ), where ait and zit denote actions and observations of agent i  I
at time step t  {0, 1, . . . , T  1}.
i , ai , zi ) and the initial private
Any step-t private history ti follows the recursion ti = (t1
t1 t
i
i
history 0 is empty. Let t be the set of all step-t private histories of agent i  I, namely the step-t
private history set. A private policy specifies private decision rules an agent can use at all time steps,
one private decision rule for each time step.

Definition 3. A step-t private decision rule dti : it 7 Ai prescribes agent i  I the private action to
be executed in each private history ti  it at a specified time step t  {0, 1, . . . , T  1}.
We further denote Dit to be the set of all step-t private decision rules for agent i  I at time step
t  {0, 1, . . . , T  1}, namely the step-t private decision rule set of agent i  I. Decision rules may
be randomized or deterministic. In Dec-POMDPs, as in MDPs, there always exists a deterministic
decision rule that is as good as any randomized decision rule (Oliehoek et al., 2008; Puterman,
1994). For this reason, we focus on deterministic (private) decision rules. Hence, private policies
provide each agent with a mapping for action selection for any possible private history.
def

i ) is a sequence of private decision
Definition 4. A (t + 1)-step private policy it:t+t = (dti , . . . , dt+t

rules for agent i  I from time step t to time step t + t , where t  {0, 1, . . . , T  1} and t  N.

Example 2 (Multi-agent tiger  private decision rule and policy descriptions). Figure 2 shows a
pair of 2-step private policies 10:1 and 20:1 as trees, for agent 1 and 2, respectively.
Agent 1

Agent 2




 agent 2s decision rule d02 depends only upon 02  
aol

al

zhl

zhr

zhl

zhr
 agent 2s decision rule d12 depends only upon 12

al

aol

aor

aor

Figure 2: A pair of private policies in the form of trees and decomposed as decision rules for two
steps of the multi-agent tiger problem.

448

fiOptimally Solving Finite-Horizon Dec-POMDPs

For agent 2 for example, step-2 private policy 20:1 consists of two private decision rules d02 and
for time steps t = 0 and t = 1, respectively. The step-0 private decision rule d02 maps empty
private history 02 =  to private action al . In addition, the step-1 private decision rule d12 maps
private histories (aol , zhl ) and (aol , zhr ) to private action aor in both cases. It is worth noticing that
private decision rules only maintain private histories that are reachable from past actions executed
and observations received.

d12 ,

So far, we focused on the information each agent has at the execution phase including: private
histories, decision rules and policies. Nevertheless, the goal of Dec-POMDP planning is to find
a separable joint policy. For this reason, we will next discuss joint histories, decision rules and
policies.
2.2.2 Separable Joint Decision Rules and Policies
In this section, we extend private information available to a given agent, e.g., private histories,
decision rules and policies, to joint information that consists of a collection of private data. Let
joint histories t , separable joint decision rules dt and separable joint policies t:t be |I|-tuples
of private histories (t1 , t2 , . . . , t|I| ), decision rules (dt1 , dt2 , . . . , dt|I| ) and policies (1t:t , 2t:t , . . . , |I|
t:t ),

respectively for all time steps t  {0, 1, . . . , T  1} and t  N. Note that each of these concepts
is separable in the sense that it is expressed as |I|-tuple using only private information, one private
concept for each agent.
Example 3 (Multi-agent tiger  separable joint decision rule and joint policy descriptions). Figure
3 depicts a 2-step separable joint policy 0:1  (10:1 , 20:1 ) as a tuple of private policies, one for each
agent i  {1, 2}. If we group together private decision rules of agents at a given time step, then we
have a separable joint decision rule. For example, at the initial time step t = 0, the tuple of private
decision rules d0  (d01 , d02 ) is a separable joint decision rule. In addition, separable joint decision
rule d0 prescribes to agents 1 and 2 actions al and aol , respectively.
Agent 1

Agent 2




 separable joint decision rule d0  (d01 , d02 ) depends on 0  (01 , 02 )
aol

al

zhl

zhr

zhl

zhr
 separable joint decision rule d1  (d11 , d12 ) depends on 1  (11 , 12 )

al

aol

aor

aor

Figure 3: A 2-step separable joint policy for the multi-agent tiger problem.
2.3 Acting Optimally
In this section, we discuss the criterion used throughout the paper to compute an optimal separable
joint policy starting at the initial belief-state. Before proceeding any further, we first cast DecPOMDPs into an MDP, namely the information-state MDP.
449

fiDibangoye, Amato, Buffet & Charpillet

2.3.1 Information-state Markov Decision Processes
As mentioned above, a common assumption in solving Dec-POMDPs is that planning takes place in
a centralized (offline) manner even though agents execute actions in a decentralized fashion (online).
In such a planning paradigm, a centralized algorithm maintains, at each step, the total available
information about the process to be controlled. This centralized algorithm essentially performs a
policy search in the space of separable joint policies. Thus, the separable joint decision rule choices
are based only on the exhaustive information available to the centralized algorithm or on statistics
derived from that information. This is illustrated in the influence diagram in Figure 4, where the
separable joint decision rule at time step t depends only on previous separable joint decision rules
and initial belief state, not on hidden states. The statistics summarizing the exhaustive information
available to the centralized algorithm are called information states (Hauskrecht, 2000). As defined
below, complete information states represent a trivial case, i.e., the exhaustive data.
start

0

t

t+1

d0

dt

dt+1

r0

rt

rt+1

Figure 4: Influence diagram for an information-state MDP. Information states (t and t+1 ) are represented by cycles. Joint-decision-rule choices (dt and dt+1 ) are represented by rectangles,
and depend only on the current information state, not on the underlying hidden states. Diamonds represent expected immediate rewards r0 , . . . , rt and rt+1 . Dashed lines represent
indirect influence over time.
def

Definition 5. In Dec-POMDPs, a step-t complete information state Ct = (b0 , 0:t1 ) is a length-t
sequence of separable joint decision rules starting with the initial belief state b0 , for all time steps
t  {0, 1, . . . , T  1}. It satisfies the recursion: C0 = (b0 ), and Ct+1 = (Ct , dt ).
Example 4 (Multi-agent tiger  complete information state description). Figure 5 depicts a step-2
complete information state as a sequence of separable joint decision rules 2  (b0 , d0 , d1 ) starting
at initial belief state b0 . Alternatively, the complete information state consists of a separable joint
policy represented as a separable joint policy tree together with the initial distribution b0 . It is
worth noting that, in a complete information state, each private history of each agent occurs in more
than one joint history. It is this interdependence between joint histories that makes Dec-POMDPs
significantly different from centralized problems (e.g., MDPs and POMDPs) since policies must
remain decentralized. This interdependence also explains why joint histories, which are sufficient
for optimally planning in MDPs and POMDPs, are no longer sufficient for optimally planning in
Dec-POMDPs. Instead, we rely on complete information states.

450

fiOptimally Solving Finite-Horizon Dec-POMDPs

b0

 separable joint decision rule d0

al , aol

zhl , zhl

zhr , zhr

zhr , zhl

zhl , zhr
 separable joint decision rule d1

al , aor

aol , aor

aol , aor

al , aor

Figure 5: A step-2 complete information state C2  (b0 , d0 , d1 ) for the multi-agent tiger problem.
Separable joint decision rules are groups of private decision rules depicted in Figure 3.

There is no need to retain the complete information states; instead, one can rely on more compact information states. Recall that information states are quantities summarizing the complete
information states. The collection of random variables {t : t  {0, 1, . . . , T }} taking values in the
information state space S defines an information-state Markov decision process (-MDP). In such
an MDP, states are information states and actions are separable joint decision rules as illustrated
in the influence diagram in Figure 4. Our -MDP is deterministic since the next-step information
state t+1 is a deterministic function of the previous information state t and the joint-decision-rule
choice dt  i.e., t+1 = P(t , dt ). Furthermore, after taking a separable joint decision rule dt at an
information state t , the expected reward is R(t , dt ).
Definition 6. A -MDP M  (S, A, P, R, 0 , T ) w.r.t. Dec-POMDP M is given by:
 S is the information-state set, which defines the set of all information states t , at every time
step t  {0, 1, . . . , T  1};
 A is the set of separable joint decision rules, which defines the set of all separable joint
decision rules dt , at every time step t  {0, 1, . . . , T  1};
 P specifies the next-step information state t+1 = P(t , dt ) after taking separable joint decision
rule dt at information state t , at every time step t  {0, 1, . . . , T  1};
P
 R specifies the immediate expected reward R(t , dt ) = s, Pr(s, |t )  rdt () (s) to be gained
by executing a separable joint decision rule dt at information state t , at every time step
t  {0, 1, . . . , T  1};
 0 is the initial information state; and T is the problems temporal horizon.

451

fiDibangoye, Amato, Buffet & Charpillet

The information-state MDP M differs from the original Dec-POMDP M because the state
space is implicit. That is, the information-state space S is much too large to be generated and
stored in memory. Instead, information states are generated as they are explored during the state
space search, and typically discarded thereafter. Generating the separable joint decision rules and
their corresponding expected rewards are also sources of the complexity for current methods as discussed later. All these methods build upon the assumption that one can always convert the original
Dec-POMDP into an information-state MDP by using complete information states without losing
optimality (Szer et al., 2005; Oliehoek et al., 2008, 2013). Below, we provide a formal proof of this
property for the sake of completeness.
Lemma 1. Any optimal separable joint policy  for complete-information-state MDP M is an
M
optimal separable joint policy M for the original Dec-POMDP M.
Proof. In demonstrating the proof, we need to show that optimal joint policies M and M are
separable joint policies with the same expected value. Throughout the proof, we will use notations
AT to denote T -steps separable joint policies and Pb0 , () to denote a joint probability distribution
with parameter b0 and . The proof starts with the definition of an optimal separable joint policy for
information-state MDP M:
def

M = arg max AT

T 1
X

R(Ct , dt ).

t=0

Next, we replace R(Ct , dt ) by the immediate expectation of rewards received after taking joint (separable) decision rule dt at complete information state Ct :
M = arg max AT

T
1
X
t=0

n
o
E(st ,t )Pb0 ,0:t () rdt (t ) (st ) ,

(Def. of R(Ct , dt )).

The following holds because the sum of expectations is equal to expectation of sums:
T 1



X



 def 
at
M = arg max AT E(s0 ,a0 ,...,sT 1 ,aT 1 )Pb0 , () 
r
(s
)
= M .

t




t=0

Hence, it is sufficient to search for an optimal separable joint policy using M to find an optimal
separable joint policy for M (and vice versa).

This lemma allows us to interchangeably use either complete-information-state MDPs M or the
original Dec-POMDP counterpart M with no loss in optimality.
2.3.2 Optimality Criterion
In this paper, we consider the finite-horizon Dec-POMDP (and therefore the finite-horizon -MDP
M), where the optimality criterion is to find a separable joint policy that maximizes the expected sum
of rewards over the planning horizon starting at a given belief state. To find an optimal separable
joint policy, we first characterize the expected value to be gained from executing any arbitrary separable joint policy t:T 1 starting from any arbitrary step-t information state. This characterization
represents the Dec-POMDP value function using the -MDP notation.
452

fiOptimally Solving Finite-Horizon Dec-POMDPs

Definition 7. Let t:T 1 be a separable joint policy with respect to M. The value function V M,t:T 1
denotes the expected cumulative reward obtained if the team of agents executes t:T 1 from time
def PT t1
R(t+k , dt+k ), where
step t onward. For any arbitrary information state t , V M,t:T 1 (t ) = k=0
t+k+1 = (t , dt , . . . , dt+k ), t  {0, 1, . . . , T  1} and k  {0, 1, . . . , T  t  1}.
Example 5 (Multi-agent tiger  expected values given a separable joint policy and an information
state). Figure 6 depicts a mapping from step-1 private histories to private policies 11:T 1 and 21:T 1
for agents 1 and 2, respectively. These private histories result from agents taking one action and
receiving an observation, i.e., agent 1 took action al and received either observation zhl or zhr . The
mapping ensures decentralized control since private histories map to private policies. For example,
agent 1s private history (al , zhl ) maps to private policy x. However, the expected value of one
agents private policy depends on the other agents private policies. For this reason, we rely on
joint histories induced by information state 1 as illustrated in Figure 7. Figure 7 depicts a mapping
from joint histories to future separable joint policies. Each joint history is a pair of private histories
from Figure 6, one for each agent. For example, joint history {(al , aol ), (zhl , zhl )} maps to future
separable joint policy (x, ) as a separable joint policy tree. The contribution of separable joint
policy (x, ) to the expected value depends on the probability of joint history {(al , aol ), (zhl , zhl )}
and the initial belief.




al

aol
 private histories generated by
d01 and d02

zhl

zhr

zhl

zhr

?

?

?

?

x

x





 future separable joint policy
1:T 1 = (11:T 1 , 21:T 1 )

Figure 6: Mappings from private histories to future private policies for each agent.
Value function V M,t:T 1 satisfies the following recursion:V M ,t:T 1 (t ) = R(t , dt )+V M,t+1:T 1 (P(t , dt ))
where V M,t+1:T 1 (P(t , dt )) describes the future value of executing separable joint policy t+1:T 1
from time step t + 1 onward starting at information state t+1 = P(t , dt ).
2.3.3 Bellmans Optimality Equations
The standard definitions of optimality equations in a T -step -MDP follow. We first describe the
optimal value at a given information state as the highest value of any separable joint policy for that
information state. Let t:T 1 be the set of all separable joint policies with respect to M. For all
 ( ) at information state  is V  ( ) def
t  {0, 1, . . . , T  1}, the optimal step-t value function V M,t
=
t
t
M,t t
453

fiDibangoye, Amato, Buffet & Charpillet

b0


al , aol
 joint histories
induced by 1 = (b0 , d0 )

zhl , zhl

zhr , zhr

zhr , zhl

zhl , zhr

?, ?

?, ?

?, ?

?, ?

x 

x 

x 

x 

 future separable joint
policy 1:T 1

Figure 7: Mappings from joint histories to future separable joint policies.

0
d0



1

|A0 |

d0

1

|A1 |
2



2



2



2

|A2 |






Figure 8: The information-state search tree where search nodes are information states and arcs of
the search tree are labeled by separable joint decision rules.

454

fiOptimally Solving Finite-Horizon Dec-POMDPs

maxt:T 1 V M, (t ). The optimality equations (or Bellmans optimality equations, see Bellman,
1957; Puterman, 1994) are:


def


(P(t , dt )) ,
t  St , t  {0, 1, . . . , T  1},
(1)
V M,t
(t ) = max R(t , dt ) + V M,t+1
dt  At

def

 () = 0 for time step t = T .
with an added boundary condition V M,T
Note that the optimal step-t value function is written in terms of the optimal step-(t + 1) value
function. This recursion implies an efficient procedure for computing step-t value functions which
we will discuss below. Moreover, an optimal separable joint policy can be directly extracted from
 )
the optimal value functions. Suppose (V M,t
t{0,1,...,T 1} are solutions of the optimality equations (1)
subject to the boundary condition, then it is clear that an optimal separable joint policy 0:T 1 =
(dt )t{0,1,...,T 1} satisfies:



(P(t , dt )) ,
t  {0, 1, . . . , T  1}.
(2)
dt  arg maxdt  At R(t , dt ) + V M,t+1

This property implies that an optimal separable joint policy is found by first solving the optimality equations, and then for each time step choosing a separable joint decision rule that attains the
maximum of the right hand side of (2) for t  {0, 1, . . . , T  1}.
2.4 Optimally Solving Dec-POMDPs
We provide an overview of dynamic programming and heuristic search principles exact methods
build upon. For a thorough introduction to solution methods in Dec-POMDPs, the reader can refer
to surveys (e.g., Oliehoek, 2012; Amato, Chowdhary, Geramifard, Ure, & Kochenderfer, 2013).
Notice, however, that most dynamic programming methods do not explicitly consider information
states (instead considering value functions over underlying system states). They construct separable joint policies from the last step in the horizon to the first by evaluating the possible separable
joint policies at each step and pruning those that have provably lower value over the full state
space (Hansen et al., 2004; Boularias & Chaib-draa, 2008; Amato et al., 2009). Heuristic search
techniques implicitly use complete information states in developing Dec-POMDP solution methods
(Szer et al., 2005; Oliehoek et al., 2008; Oliehoek, Whiteson, & Spaan, 2009; Spaan, Oliehoek, &
Amato, 2011), but do not explicitly use the C -MDP representation.
2.4.1 Dynamic Programming Methods
One class of Dec-POMDP solution methods is based on dynamic programming (Howard, 1960).
Here, a set of T -step separable joint policies is generated from the bottom up (Hansen et al., 2004).
At each step, all step-t separable joint policies are generated that build off separable joint policies
from step t+1. Any separable joint policy that has lower value than some other separable joint policy
for all states and possible separable joint policies of the other agents is then pruned (with linear
programming). These generation and pruning steps continue until the desired horizon is reached and
a separable joint policy with the highest value at the initial state is chosen. Given that the number
of separable joint policies grows doubly exponentially every generation step, the importance of
pruning away unnecessary separable joint policies is crucial. More efficient dynamic programming
methods have been developed, reducing the number of separable joint policies generated (Amato
et al., 2009) or compressing separable joint policy representations (Boularias & Chaib-draa, 2008).
455

fiDibangoye, Amato, Buffet & Charpillet

2.4.2 Heuristic Search Methods
Another class of Dec-POMDP solution methods is based on heuristic search techniques. Unlike dynamic programming methods, heuristic search algorithms can take advantage of the initial complete
information state. Separable joint policies can be built from the top down using centralized heuristic
search methods over the search tree shown on Figure 8 (Szer et al., 2005).
In this case, a search node is a complete information state at a given horizon, t. These complete
information states can be evaluated up to that horizon and then a heuristic value can be added. The
resulting heuristic values are over-estimates of the true value, allowing an A*-style search through
the space of possible complete information states, expanding promising search nodes to horizon
t + 1 from horizon t. While in principle, A*-style search methods can find an optimal separable
joint policy, in practice the doubly exponential growth of the search tree makes it difficult. Recent
work has included clustering probabilistically equivalent complete information states (Boularias &
Chaib-draa, 2008) and histories (Oliehoek et al., 2009), and incrementally expanding nodes in the
search tree (Spaan et al., 2011; Oliehoek et al., 2013), greatly improving scalability of the original
algorithms.
2.4.3 Limitations of Current Methods
While current methods attempt to reduce the number of separable joint policies or information
states considered, they rely on explicit representations that consider all possible joint histories (even
though many of them may be unreachable). Moreover, existing techniques fail to generalize value
functions from one information state to other information states, which slows down the convergence
to an optimal joint policy. Finally, even though most solution methods use an offline centralized
planning phase, no concise representation of the information state has been identified (until now)
that allows for greater scalability. Simultaneous to this work one exception developed concise representations based on observation histories, but did not show the value function over the resulting
MDP was piecewise linear and convex (Oliehoek, 2013). We are able to show this piecewise linear
and convex property and develop a novel algorithm to exploit the resulting structure. To do so, we
draw inspiration from advances in MDP and POMDP algorithms as discussed below.
Significant progress has been made in solving large MDPs and POMDPs. One reason for
progress in MDPs has been the use of approximate dynamic programming and function approximation (Tsitsiklis & van Roy, 1996; De Farias & Van Roy, 2003; Powell, 2007) to represent the
state of the system and value functions more concisely. For POMDPs, efficient algorithms have been
developed by recasting problems as belief MDPs that utilize probability distributions over states of
the system, namely belief states (Smallwood & Sondik, 1973). This belief MDP is a continuousstate MDP with a piecewise linear and convex value function, allowing algorithms to scale to large
problems while sometimes retaining performance bounds (Smith & Simmons, 2004; Shani et al.,
2013). We will take advantage of such advances by recasting a Dec-POMDP as a continuous-state
MDP with a piecewise linear and convex optimal value function. The resulting formulation opens
the door for direct application of POMDP methods and opens research directions on utilizing DecPOMDP structure in centralized planning representations. We discuss this formulation and some
progress in using this structure in the remaining sections.

456

fiOptimally Solving Finite-Horizon Dec-POMDPs

3. Solving Dec-POMDPs as Continuous-State MDPs
The contribution of this section is threefold. Section 3.1 introduces a statistic (i.e., the occupancy
state) which summarizes the information state. Section 3.1.4 demonstrates occupancy states are
sufficient for optimally solving Dec-POMDPs, i.e., occupancy states are sufficient statistics. Occupancy states further allow transforming information-state MDPs into occupancy-state MDPs. Section 3.1.6 establishes a fundamental property of the resulting MDP, namely the piecewise linearity and convexity of the optimal value function over the occupancy states. Remember that these
methods assume centralized offline planning and actions as separable joint decision rules to ensure decentralized execution. These contributions enable the application of a vast collection of
MDP and POMDP solution methods to Dec-POMDP problems. Finally, Section 3.2 introduces the
occupancy-based heuristic search value iteration (OHSVI) algorithm for solving occupancy-state
MDPs that builds upon the HSVI algorithm for POMDPs (Smith, 2007).
3.1 Summarizing Complete Information States
Before providing a formal definition of occupancy states, we start with a brief motivation. Then,
we demonstrate that the occupancy state induces a deterministic process that is Markov, namely the
occupancy-state Markov decision process. Finally, we prove that the occupancy state is a sufficient
statistic for optimal decision-making in Dec-POMDPs.
3.1.1 Occupancy State
As discussed in Section 2.4.2, standard heuristic search methods for solving Dec-POMDPs rely on
complete information states (Szer et al., 2005; Oliehoek et al., 2008, 2009; Spaan et al., 2011).
While complete information states preserve the ability to find an optimal separable joint policy (see
Lemma 1), heuristic search methods using them can only solve small toy problems. One reason for
this poor behavior is that complete information states result in redundant and useless computations.
In particular, every time they estimate the immediate rewards R(C , d) the entire multivariate probability distribution Pr(s, |C ) needs to be computed over all states and joint histories (see Definition
6). This operation is time-consuming because it involves exponentially many joint histories, including unreachable ones. Since this operation occurs at every time step, it is important to reduce the
time required. To this end, we introduce a statistic called the occupancy state that we can maintain
in place of the complete information state.
Definition 8. The step-t occupancy state, denoted t , is defined as the posterior probability distribudef
tion of state st and joint history t given complete information state Ct , i.e., t (st , t ) = Pr(st , t |Ct ),
t  {0, 1, . . . , T }. We denote t the step-t occupancy simplex, that is, the set of all possible step-t
occupancy states.
Example 6 (Multi-agent tiger  from complete information states to occupancy states). Figure 9
depicts a complete information state (left-hand side) and a corresponding occupancy state (righthand side) over joint histories and states of the system. We illustrate an occupancy state as a tree,
where branches are joint histories of the complete information state and leaves are state-probability
pairs. Note that the same initial belief is assumed in both state types.
The occupancy state represents a predictive model of the state that the system may end up in and
joint history the agents may experience given a complete information state. As such, in occupancy
457

fiDibangoye, Amato, Buffet & Charpillet

Complete Information State C1

zhl

Corresponding Occupancy State 1







al

aol

al , aol

zhr

zhl

zhr

zhl , zhl
stl

?

?

?

?

str

.125 .125

zhr , zhr
stl

str

.125 .125

zhr , zhl
stl

str

.125 .125

zhl , zhr
stl

str

.125 .125

Figure 9: A step-1 occupancy state 1 that corresponds to complete information state 1 .
states, we need just to maintain state and joint history pairs that are reachable given the complete
information state.
3.1.2 Markov Property
This section proves that occupancy states induce a process that is Markov. In other words, the future
occupancy states of the process depend only upon the present occupancy state and the next-step
separable joint decision rule.
Theorem 1. Occupancy state t+1 depends on current occupancy state t and separable joint decision rule dt , i.e., for any arbitrary s  S , at  A, zt+1  Z and t  t ,
X
t+1 ( s, (t , at , zt+1 )) = 1{at } (dt (t ))
t (s, t )  pat ,zt+1 (s, s),
(3)
sS

where 1{} () is an indicator function which returns 1 when the actions at are chosen by dt , and
returns 0 otherwise.
Proof. In demonstrating this theorem we also derive a procedure for updating the occupancy states.
Let t be our step-t information state, which we will decompose as t = (t1 , dt1 ) i.e., the information state t1 prior to time-step t plus the known separable joint decision rule dt1 . By Definition
8, we can relate the occupancy state and the information state as follows: for any arbitrary state st
and joint history t ,
def

t (st , t ) = Pr(st , t |t ).

(4)

The substitution of t = (t1 , dt ) into (4) yields
t (st , t ) = Pr(st , t |t1 , dt1 ).

(5)

The expansion of the right-hand side of (5) over all states of the system at the end of time-step t  1
produces
X
t (st , t ) =
Pr(st1 , st , t |t1 , dt1 ).
(6)
st1 S

458

fiOptimally Solving Finite-Horizon Dec-POMDPs

The expansion of the joint probability in (6) as the product of conditional probabilities results in
X
t (st , t ) =
Pr(at1 |t1 , dt1 )  Pr(st , zt |st1 , t1 , t1 , dt1 )  Pr(st1 , t1 |t1 , dt1 ).
(7)
st1 S

The first factor denotes the joint action at1 that separable joint decision rule dt1 prescribes at
t1 . Since we assume that separable joint decision rules are deterministic, Pr(at1 |t1 , dt1 ) 
{0, 1}. In fact, Pr(at1 |t1 , dt1 ) = 1 if dt1 (t1 ) = at1 , otherwise Pr(at1 |t1 , dt1 ) = 0. So,
Pr(at1 |t1 , dt1 ) = 1{at1 } (dt1 (t1 )), where 1F is an indicator function.
The second factor on the right-hand side of (7) is the transition probability
X
pat1 ,zt (st1 , st )  Pr(st1 , t1 |t1 , dt1 ).
(8)
t (st , t ) = 1{at1 } (dt1 (t1 ))
st1 S

The last factor defines the prior occupancy state t1 at state st1 and joint history t1 , which does
not depend on the current separable joint decision rule dt1 . Overall (6) becomes
X
t (st , t ) = 1{at1 } (dt1 (t1 ))
pat1 ,zt (st1 , st )  t1 (st1 , t1 ).
st1 S

Therefore, the calculation of the occupancy state after time-step t requires only the occupancy
state of the previous time-step t  1 and the current separable joint decision rule.

Equation (3) describes the transitions of a continuous-state MDP in which states are occupancy
states and actions are separable joint decision rules. For this process, the transitions are deterministic
but the state space is continuous. Next, we formally define the process occupancy states induce.
3.1.3 Occupancy-State Markov Decision Processes
We consider the MDP described by the occupancy states; we call it an occupancy-state Markov
decision process.
Definition 9. Let M  (, A, R, P, b0 , T ) be the occupancy-state Markov decision process with
respect to Dec-POMDP M, where:
  = t{0,1,...,T } t is the occupancy simplex, where 0 = b0 is the initial occupancy state;
 A = t{0,1,...,T } At is the separable joint decision rule set;
def

 R :   A 7 R is a reward function: the reward at (t , dt ) is R(t , dt ) =
def

P

s, t (s, )  r

dt () (s);

 P :   A 7  is a transition function: next occupancy state t+1 = P(t , dt ) as described in
Equation (3) given (t , dt );
 b0 is the initial belief state; and
 T denotes the planning horizon.

459

fiDibangoye, Amato, Buffet & Charpillet

Here, the states of the system represent the centralized knowledge of the planner while the
actions represent separable joint decision rules to ensure decentralized execution. The (occupancy)
state can be updated by using the known transition and observation functions of the Dec-POMDP
given the current (occupancy) state and the chosen separable joint decision rule. The rewards are
also calculated (as an expectation) using the known reward model of the Dec-POMDP. The optimal
value functions of M are solutions of the optimality equations:


def


V M,t
(t ) = max R(t , dt ) + V M,t+1
(P(t , dt )) , t  {0, 1,    , T  1},
(9)
dt  A

def

with an added boundary condition V  () = 0 for t = T .
M,T
 )
Notice that once a solution (V M,t
t{0,1,...,T 1} of the optimality equations Eq. (9) has been found,
one can always retrieve an optimal separable joint policy starting at the initial occupancy state.
This is achieved by iteratively retrieving optimal separable joint decision rules for decision steps
t  {0, 1, . . . , T  1}. At each decision step t, the procedure selects the current occupancy state t
(starting with initial occupancy state 0 ). It uses the max operator to retrieve an optimal separable
joint decision rule dt at current occupancy state t :


def

(P(t , dt )) .
(10)
dt = arg maxdt  A R(t , dt ) + V M,t+1
Thereafter, it moves to the next occupancy state t+1 = P(t , dt ) and makes it the current one. The
procedure then repeats until final decision epoch T has been reached. The sequence of optimal
separable joint decision rules (d0 , d1 , . . . , dT 1 ) defines an optimal separable joint policy  for an
occupancy-state MDP M. Furthermore, Theorem 2 proves that an optimal separable joint policy for
an occupancy-state MDP M is an optimal separable joint policy for the original Dec-POMDP M.
Optimally solving a continuous-state MDP, such as the occupancy-state MDP, is a nontrivial
task. In general, there is no exact solution method for solving general continuous-state MDPs.
Methods often rely on structural assumptions about the shape of the optimal value function (Tsitsiklis & van Roy, 1996; De Farias & Van Roy, 2003; Powell, 2007). We next demonstrate that a
useful structure does indeed exist for occupancy MDPs in the form of optimal value functions that
are piecewise linear and convex functions over the occupancy states.
3.1.4 Sufficiency of Occupancy States
We first show that the occupancy state is a sufficient statistic for optimal decision-making in DecPOMDPs. Throughout the remainder of this paper, we call a statistic a sufficient statistic when the
statistic of the information state is sufficient for optimal decision making in occupancy-state MDPs.
Theorem 2. Occupancy state t = Pr(s, t |Ct )sS ,t t is a sufficient statistic of complete information state Ct , i.e., it is sufficient for optimally solving occupancy-state MDPs. Furthermore, an
optimal joint policy for the occupancy-state MDP M together with the correct estimation of the
occupancy states, is also optimal for information-state MDP M (respectively Dec-POMDP M).
Proof. In demonstrating the sufficiency of the occupancy state with respect to its corresponding
information state, we need to demonstrate that (a) the optimal value function at an occupancy state
is identical to that of its corresponding information state and (b) the future occupancy states depend
only upon the current occupancy states (and next-step separable joint decision rule). We proved (b)
in Theorem 1, so it only remains to prove statement (a). We show this by induction.
460

fiOptimally Solving Finite-Horizon Dec-POMDPs

The sufficiency of the occupancy state with respect to its corresponding information state trivially holds at the last time step of the problem. In fact, V  (CT ) = V  (T ) = 0 for any arbitrary
M,T

M,T

complete information state CT and its corresponding occupancy state T (since the horizon has been
reached).
If we assume that statement (a) holds for time-step t + 1, we can now show it holds for time-step
t. For any arbitrary step-t information state, Bellmans optimality criterion prescribes the following:


V M,t
(Ct ) = max R(t , dt ) + V M,t+1
(Ct+1 ),
dt  A

(11)



(t+1 ), if t+1
(Ct+1 ) = V M,t+1
where Ct+1 = (Ct , dt ). By the induction hypothesis, we have V M,t+1

corresponds to the occupancy state associated to Ct+1 . Hence, Equation (11) becomes


V M,t
(Ct ) = max R(Ct , dt ) + V M,t+1
(t+1 ).
dt  A

Moreover, given that R(Ct , dt ) =
since t = (Pr(s, |Ct )) s, :

P

s,

(12)

rdt () (s)  Pr(s, |Ct ) = R(t , dt ), the following expression holds



(t+1 ),
(Ct ) = max R(t , dt ) + V M,t+1
V M,t
dt  A

(13)

 ( ) = max

which ends the proof of statement (a) at time-step t, since V M,t
t
dt  A R(t , dt )+V M,t+1 (t+1 ).

As a consequence, statement (a) holds for any arbitrary complete information state Ct  St and at
any arbitrary time-step t  {0, 1, . . . , T  1}. Combining statements (a) and (b), we are guaranteed
to find the optimal value function for M by using occupancy states instead of information states.
As such, an optimal joint policy for the occupancy-state MDP M, together with the correct estimation of the occupancy states, is also optimal for information-state MDP M (or the original DecPOMDP M). Given the optimal value function of M, an optimal joint policy  = (dt )t{0,1,...,T 1} is
given by:

dt = arg maxdt  A R(t , dt ) + V M,t+1
(t+1 ),

(14)

for any arbitrary information state t , and all t  {0, 1, . . . , T  1}, where t and t+1 correspond to Ct

and Ct+1 = (Ct , dt ), respectively.
This theorem demonstrates that by optimally solving any of M, M or M, we are guaranteed to
find an optimal separable joint policy to the others.
3.1.5 Belief States versus Occupancy States
Note that there is a similarity between the occupancy state in Dec-POMDPs and the belief state
in POMDPs. Formally, a step-t belief state bt = (P(s|t , b0 )) sS is a probability distribution over
states conditioned on step-t history t . It is also a sufficient statistic for the total data available
to the centralized agent (i.e., the action-observation history) an algorithm can rely on to find an
optimal solution in POMDPs. Similarly, an occupancy state is a sufficient statistic for the total data
available to a centralized planner (i.e., the history of separable joint decision rules) an algorithm
can rely on to find an optimal separable joint policy in Dec-POMDPs. However, the occupancy
state remains fundamentally different from the belief state. First, the belief state is not sufficient for
461

fiDibangoye, Amato, Buffet & Charpillet

optimal decision making in Dec-POMDPs (because it is not geared to ensure the separability of the
joint policy). Second, a belief state defines a time-invariant statistic, i.e., the dimension of belief
states is bounded by the number of states. In contrast, the dimension of the occupancy states grows
exponentially with the horizon. Also, unlike the belief state, the occupancy state is only a plan
time sufficient statistic which is not used during execution time. Instead, agents still condition their
actions on local action-observation histories in Dec-POMDPs. These differences make algorithmic
and theoretic transfers from belief-state MDPs to occupancy-state MDPs nontrivial.
3.1.6 Piecewise-Linearity and Convexity Property
We now present one of the main results of this paper  the piecewise-linearity and convexity of the
optimal value function of the occupancy-state MDP.
For this discussion, we use vector (resp. matrix) representation for operators R(, dt ) and P(, dt ).
Vector rdt = (rdt () (s))s, denotes the immediate reward of executing dt starting from any state and
joint history (i.e., R(t , dt ) is the inner product of t and rdt for any occupancy state t  t ).
Moreover, operator P(, dt ) transforms any step-t occupancy state to a step-(t + 1) occupancy state,
that is, P(, dt ) describes a transition matrix pdt such that P(t , dt ) = t pdt for every step-t occupancy
state t  t . With these linear transformations as a background, the following holds.
 )
Theorem 3. The optimal value functions (V M,t
t{0,1, ,T } (solutions to Equations in (9)) are piecewiselinear and convex functions of the occupancy states. Hence, for all t  {0, 1,    , T  1}, there exists
a finite set of length-n|t | vector values t such that, for any arbitrary occupancy state t  t , we
have:

V M,t
(t ) = max ht , t i,

(15)

t t

where ht , t i denotes the inner product

P P
s

 t (s, )

 t (s, ).

Proof. We show that (15) holds by induction. Since V  (T ) = 0 for all T   (since the horizon
M,T
 ( ) = max
has been reached), we have that V M,T
T
T T hT , T i, where T () = 0 and T = {T }.
 ( ) =
Hence, the property holds for k = T . Assume that the property holds for k  t + 1, that is, V M,k
k
maxk k hk , k i for k  t + 1. Now we want to prove the property for k = t i.e.,



V M,t
(t ) = max R(t , dt ) + V M,t+1 (P(t , dt )) ,
t  t .
dt  A

Using linear transformations rdt and pdt , the following holds:


def

V M,t
(t ) = max R(t , dt ) + V M,t+1 (P(t , dt )) ,
dt  A
!
dt
= max ht , r i + max hP(t , dt ), t+1 i ,
t+1 t+1
dt  A
E

D
= max max ht , rdt i + t  pdt , t+1 ,
dt  A t+1 t+1


= max max ht , rdt + t+1  (pdt ) i .

(Inductive Hypothesis)
(Rearranging Terms)

dt  A t+1 t+1

def

Finally, if we let t be the set of all length-n|t | vectors t = rdt + t+1  (pdt ) for all separable joint
 ( ) = max
decision rules dt  A and all vectors t+1  t+1 , then V M,t
t
t t ht , t i. As a consequence,
the proof holds for every time step t  {0, 1, . . . , T  1}.

462

fiOptimally Solving Finite-Horizon Dec-POMDPs

We demonstrated that the information states and value functions can be represented in a vector
space, the occupancy-state space, without loosing optimality. Next, we provide an approach for
extending MDP and POMDP solution methods to occupancy-state Markov decision processes.
3.2 Heuristic Search Solution Methods
This section presents the occupancy-based heuristic search value iteration (OHSVI) algorithm for
solving occupancy-state MDPs. This algorithm extends the heuristic search value iteration (HSVI)
algorithm for POMDPs (Smith & Simmons, 2004) as well as other heuristic search algorithms such
as A* (Hart, Nilsson, & Raphael, 1968) and LRTA* (Korf, 1990).
3.2.1 Heuristic Search Value Iteration for Occupancy-State MDPs
HSVI is a state-of-the-art algorithm for solving POMDPs (Smith & Simmons, 2004). It produces
solutions by maintaining two-sided bounds on the optimal value function and updating them over
a number of sample trajectories. The upper bounds, (U M,t )t{0,1, ,T } , are represented as (belief)
state-value mappings, and the lower bounds, (L M,t )t{0,1, ,T } , are represented by vector sets. Each
trajectory begins at the initial belief state and continues until the time horizon is reached. Once a
trajectory is finished, the upper and lower bounds are updated at each belief state, in the reverse order
of visit. The trajectories can also be interrupted once they have reached a belief state where the upper
and lower bounds are equal, since there is no reason to expand a belief state whose optimal value
is provably known. Finally, it is often useful to prune lower and upper bounds to maintain concise
representations, by removing either dominated vectors or points, respectively (Pineau, Gordon, &
Thrun, 2006; Smith, 2007).
Algorithm 1: The OHSVI algorithm
function OHSVI((L M,t )t{0,1, ,T } , (U M,t )t{0,1, ,T } )
while Stop(0 ) do Explore (0 )
function Stop(t )
return U M,t (t ) = L M,t (t )
function Explore (t )
if Stop(t ) then
dt  arg maxdt R(t , dt ) + U M,t+1 (P(t , dt ))
Explore(P(t , dt ))
update U M,t and L M,t at t

OHSVI (outlined in Algorithm 1) operates in a similar manner as described above, but remains
fundamentally different from HSVI. HSVI generates trajectories of belief states while OHSVI generates trajectories of occupancy states. Also, HSVI generates trajectories by (i) picking a greedy
action with respect to the upper bound (optimistic exploration), and (ii) performing the transition
corresponding to the largest gap in error. Due to the deterministic nature of occupancy-state MDPs,
OHSVI does not need HSVIs gap-based heuristic to guide the state exploration (Smith, 2007).
Instead, OHSVI always executes a greedy separable joint decision rule with respect to the upper
bounds, and then selects the next occupancy state based on this greedy separable joint decision rule.
463

fiDibangoye, Amato, Buffet & Charpillet

OHSVI can be also thought of as an extension of learning real-time A* (LRTA*) (Korf, 1990) that
takes advantage of the piecewise-linearity and convexity of the optimal value function.
V  (t )  0.2  v1t + 0.8  v2t

0t

2t

1t

(b) upper bound U M,t

(t1 , v1 )

(t2 , v2 )

M,t

M,t

V  (t )  ht , kt i

(a) lower bound L M,t

t

t = 0.2  t1 + 0.8  t2

Figure 10: (a) Lower bounds are represented using sets of vectors, where vectors are dashed lines,
solid lines represent the upper surface of these vectors (lower-bound value function), and
the circle is the projection of the target occupancy onto the lower-bound value function.
(b) Upper bounds are represented using occupancy-value mappings, where dashed lines
denotes the convex hull formed by these points.

OHSVI relies on standard approaches to represent lower and upper bounds for piece-wise linear
and convex value functions: vector sets and occupancy-value mappings, which we detail in the next
section.
3.2.2 Vector Sets : Lower Bounds
As in HSVI or other algorithms (and depicted in Figure 10(a)), the lower bound L M,t can be represented as a finite collection t of n|t |-dimensional vectors, for every time-step t  {0, 1, . . . , T }
(Smith, 2007; Kaelbling et al., 1998; Hauskrecht, 2000; Smallwood & Sondik, 1973). Lower
bounds can be iteratively updated using point-based backup steps as follows: t  t ,
[
t = t
{backup(t+1 , t )} , where
(16)
backup(t+1 , t ) = arg maxgt : dt  A ht , gdtt i,

(17)

dt



gdtt = rdt + arg maxgt+1 : 
dt



t+1 t+1

ht , gdtt+1 i,



gdtt+1 = t+1  (pdt ) ,

(sum vectors in Rn|t | )

(18)

(projection Rn|t+1 | 7 Rn|t | )

(19)

t being the vector set prior to backup and t the vector set after the backup. Lower bounds
(L M,t )t{0,1,...,T 1} initializes (t )t{0,1,...,T 1} with a single vector t () = min sS ,aA (T  t)  ra (s), for
all t  {0, 1, . . . , T }. Notice that the vector representation is suitable only for lower bounds.
3.2.3 Occupancy-Value Mappings : Upper Bounds
Upper bounds (U M,t )t{0,1,...,T 1} can be represented using mappings from occupancy states to reals,
see e.g., Figure 10(b). The upper bound is then the convex hull of the current point set. It is
464

fiOptimally Solving Finite-Horizon Dec-POMDPs

possible to interpolate the value for occupancy states whose mapping is not currently maintained
or is outdated. This can be achieved using linear approximation methods (e.g., Hauskrecht, 2000;
Smith, 2007). Of this family, the sawtooth linear interpolation maps every occupancy state    t
and point set t to upper-bound value
U M,t (  ) = min {v , v | ( 7 v )  t }, where


v = v + (v  v )  D(,   ), and
X
v =
(s, )  v s, .

(20)
(21)
(22)

s,

We refer to D(,   ) = min s, : (s,)>0   (s, )/(s, ) as the sawtooth measure. To update the upper
bound U M,t at a specific occupancy state  using sawtooth, we need to compute a new value for ,
and add it to occupancy-value mapping U M,t , as follows:
[n
o
t = t
( 7 v ) ,
(23)
v = max R(, d) + U M,t+1 (P(, d)),
d At

(24)

where t is the point set prior update and t is the point set after the update. The upper bound
U M,t initializes t = { s, 7 vts, | s  S } using the optimal value of the underlying MDP for corner
points, for every t  {0, 1, . . . , T }.
Clearly, one can eventually find an optimal solution to the occupancy-state MDP using OHSVI
with the full lower and upper bounds. However, it quickly becomes intractable to maintain these
bounds in the full occupancy-state space since the number of state and joint-history pairs grows
as the horizon increases. This highlights the necessity for compact representations of occupancy
states, decision rules and vector values.

4. Solving Dec-POMDPs as Lossless Compact Occupancy-State MDPs
The previous sections show that every Dec-POMDP can be represented as an occupancy-state MDP
without losing optimality. The difficulty with using this representation is that common algorithms
for solving such MDPs with piecewise linear convex value functions quickly run out of time and/or
memory since state and action spaces become intractably large for most real-world problems. This
is not surprising given the NEXP-Complete worst-case complexity of general Dec-POMDPs, but
realistic Dec-POMDP applications often have significant structure.
In this section, we discuss optimally solving occupancy-state MDPs while potentially reducing
the dimensionality of the occupancy states, decision rules and value functions. In Subsection 4.1,
we reduce the dimensionality of occupancy states and decision rules by constructing clusters of
equivalent histories. Next, we define compact representations for occupancy states and decision
rules based upon clusters of histories rather than single histories. While the resulting compact
MDP may have exponentially fewer states and actions than the original model, the optimal value
function of the compact model may no longer be piecewise linear and convex. In Subsection 4.2,
we overcome this limitation, allowing values from one compact occupancy state to generalize to
another one using parametric value functions. Finally, Subsection 4.3 presents the feature-based
heuristic search value iteration algorithm and theoretical guarantees.

465

fiDibangoye, Amato, Buffet & Charpillet

4.1 Lossless Compact Occupancy-State MDPs
The dimension of occupancy states and decision rules typically grows exponentially with the horizon. Because of this, it is often impractical to compute and store every component of occupancy
state and decision rule representations. We overcome this limitation by using compact representations of occupancy states and decision rules based on notions of equivalence between histories.
These notions of equivalence are fundamental to designing and analyzing algorithms for reducing
the dimensionality of the occupancy-state MDP, thereby improving scalability. Equivalence relations permit us to aggregate histories that convey the same information about the process. We target
equivalence relations that, upon replacing each group of aggregated histories by one element of the
group, allow us to produce compact representations for all occupancy states and decision rules while
still preserving the ability to find an optimal solution.
4.1.1 Probabilistic Equivalence for History Space Aggregation
We first present the equivalence notions that we build upon before defining compact representations
and proving that they preserve optimality. The definitions here are inspired by work on concise
information states (Boularias & Chaib-draa, 2008; Oliehoek et al., 2013). Here, we connect this
research to occupancy-state MDPs, and later provide natural algorithms for constructing and effectively solving them.
Definition 10. Private histories i , i  it of agent i  I are locally probabilistically equivalent
with respect to occupancy state   t denoted -LPE if, and only if, for any state s  S and
i i
i i
history i  i
t of the other agents I\{i}: Pr(s,  | , ) = Pr(s,  | , ).
It is worth noticing that -LPE can be used to partition private history set it of any agent i  I,
for all time steps t  {0, 1, . . . , T }. This partition is a set of nonempty subsets (called clusters)
Bi1 , Bi2 , . . . , Bik of private histories, such that Bi1  Bi2  . . .  Bik = it . We distinguish between two
sets of private histories for each agent i  I and any occupancy state . The first set, denoted it (),
consists of private histories with non-zero probability with respect to . The second set, denoted
it \it (), consists of private histories with zero probability with respect to . This difference is
particularly important, as we will show later. In fact, only nonzero private histories play a part in
demonstrating that -LPE preserves optimality. In addition, it is useful to note that, for each agent,
-LPE groups together all zero private histories w.r.t.  into the same cluster.
Given that all private histories are clustered that convey the same information, representations
of compact occupancy states, decision rules and value functions should depend only upon these
clusters. Unfortunately, maintaining clusters still requires a large amount of memory, which explains the impetus for labeled clusters. A labeled cluster is a cluster along with a label; all private
histories in a cluster match the corresponding label. Throughout the paper, we use the following
convention: each cluster of private histories maps to the minimum private history (of that cluster)
using lexicographical ordering. Specifically, the label of a cluster is chosen among private histories
in the cluster, which have non-zero probability w.r.t. the occupancy state. Therefore, the label of a
cluster is also a private history in that cluster, which leads to a clear relationship between a private
history, a cluster and its corresponding label. Thus, the representation of compact occupancy states,
decision rules and value functions should depend only upon labels instead of clusters.
Nonetheless, these compact representations make it hard to generalize the value function from
one compact occupancy state to another. As we will see later, this generalization requires the ability
466

fiOptimally Solving Finite-Horizon Dec-POMDPs

to quickly check whether a given private history belongs to a specified cluster. If we group histories
using -LPE, checking whether private history i belongs to cluster Bi whose label is another private
history i  Bi can now be replaced by checking whether private histories i and i are -LPE, for
any arbitrary agent i  I. Unfortunately, checking whether two private histories of an agent are
-LPE requires enumerating all states and other agents nonzero histories w.r.t. , which results in a
subroutine with complexity O(n|i
t ()|)  see Algorithm 3 in Appendix B. Given that we call this
procedure for exponentially many private histories, the importance of replacing local probabilistic
equivalence by a cheaper equivalence relation is clear. To this end, we introduce truncation probabilistic equivalence w.r.t.  (denoted -TPE). Before providing the formal definition of -TPE, we
start with a motivation.
In many practical situations, all important information about the process to be controlled lies
in a history window (of a fixed length) of actions and observations the agents have experienced.
Based on this insight, we want truncation probabilistic equivalence to group private histories of
each agent that share: (i) the same model about states and other agent histories (i.e., be -LPE)
and (ii) a common suffix of a fixed length m. In such a setting, once the private histories have
been clustered, checking if any particular private history belongs to a cluster can now be replaced
by checking whether both that private history and the label (another private history) for the cluster
share the same suffix of a specified length m  which is significantly faster than checking whether
two private histories are -LPE: O(m) versus O(n|i
t ()|). That is, once the private histories are
clustered, any two private histories with the same suffixes of length m are already known to be LPE though the clustering process and choice of m. It turns out that in defining the probabilistic
truncation equivalence w.r.t. , we need to determine the suffix length (i.e., history window) m that
is sufficient  called the local truncation parameter w.r.t. .
The local truncation parameter m with respect to occupancy state   t has to be: (i) large
enough to ensure all nonzero private histories w.r.t.  that share the same suffix of length m are also
-LPE; and (ii) small enough to group the maximum number of private histories. A straightforward
method (Algorithm 4 in Appendix B) to compute m starts with parameter m = 0 and proceeds
as follows: (step 1) If for any agent, nonzero private histories w.r.t.  that share a common suffix
of length m are also -LPE with one another, then set m = m and terminate; (step 2) Otherwise,
increment m = m + 1 and go back to step 1. This algorithm is guaranteed to terminate after at most
t (the current time step) iterations, i.e., m = t in the worst case. The later case corresponds to the
boundary case where no clustering is done  i.e., each cluster corresponds to a single joint history.
We are now ready to formally define the truncation probabilistic equivalence relation.
Definition 11. Private histories i , i  it of agent i  I are truncation probabilistically equivalent
with respect to occupancy state   t  we denote -TPE  if, and only if, they hold the same last
m private actions and observations given m is found for the set of histories as discussed above.
Not surprisingly, -TPE also partitions the private history sets. However, it often produces more
clusters than -LPE since it further constrains -LPE non-zero private histories w.r.t.  (since it also
requires the suffixes to be the same). In contrast to -LPE, it spreads zero private histories w.r.t. 
over different clusters.
Recall that the advantage of -TPE over -LPE is that finding the appropriate cluster for a
private history (i.e., checking whether two private histories are equivalent) is cheaper using -TPE
than using -LPE. The former requires the comparison of their suffix for a fixed length, whereas the
latter needs the comparison of large distributions over states and joint histories.
467

fiDibangoye, Amato, Buffet & Charpillet

It is worth noticing that, to the best of our knowledge, TPE and LPE are the weakest assumptions
to date that can reduce the dimensionality of full occupancy states. Nevertheless, not all DecPOMDPs will benefit from these data reduction approaches. More precisely, it is unlikely that
these assumptions provide concise occupancy states in totally unstructured domains. Fortunately,
real-world domains are often very structured, as demonstrated in Section 5. Next, we address the
problem of using equivalence relations between private histories to find compact representations for
all occupancy states, decision rules and real vectors.
4.1.2 Compact States, Actions, Vectors and MDPs
We define the compact representations for occupancy states, decision rules and real vectors based
upon the aforementioned equivalence relations. Notice that the following definitions depend on
either local or truncation probabilistic equivalence relations only through the labeled clusters they
generate. Intuitively, compact occupancy states are distributions over states and joint labels, compact decision rules are mappings from labels to private actions, and compact real vectors are mappings from pairs of states and joint labels to reals. Notationally, let Li be the label set of agent i  I
that occupancy state  generates (i.e., the set of labels generated from the histories which make up
), and Bi be the labeled cluster of agent i  I with label i  Li and let L = iI Li .
Definition 12. A compact occupancy state of , denoted , is a distribution over states and joint
labels: for all s  S and (i )iI  L ,
X
X X
def
(s, 1 , 2 , |I| ).
(25)
...
(s, 1 , 2 , . . . , |I| ) =
1 B1 2 B2

|I| B|I|

Example 7 (Multi-agent tiger  from full occupancy states to compact occupancy states). Figure
11 depicts a full occupancy state (left-hand side) and a corresponding compact occupancy state
(right-hand side) over joint histories and hidden states. To obtain this compact occupancy state, we
show that (al , zhl ) and (al , zhr ) are 1 -LPE for agent green, and (al , zhl ) and (aol , zhr ) are 1 -LPE
for agent red. As a consequence, one can group histories Bgreen  {(al , zhl ), (al , zhr )} for agent
green, and Bred  {(aol , zhl ), (aol , zhr )} for agent red, and replace each cluster by a single label.
A compact decision rule w.r.t.  of agent i  I, denoted di : Li 7 Ai , is a mapping from label
set to private action set. In addition, a compact private policy w.r.t. (0 , 1 , . . . , T 1 ) of agent i  I,
denoted i = (di t )t{0,1,...,T 1} , is a sequence of compact decision rules of agent i  I. A similar
definition follows for compact separable joint policies  = (i )iI .
A compact real vector w.r.t. , denoted   Rn|L | , is a mapping from pairs of state s  S and
joint label   L to reals.
Intuitively, distribution  reassigns the probability mass of each joint cluster (Bi )iI to the corresponding joint label (i )iI . Algorithms 3 and 4 (see Appendix B) present a straightforward way to
compute a compact occupancy state using local and truncation probabilistic equivalence relations,
respectively. It is worth noticing that, for a given occupancy state   , -LPE always produces
a number of joint labels |L | less than or equal to the number of labels that -TPE produces. This
is because -TPE is a stricter form of local probabilistic equivalence, as discussed above. Hence,
-LPE produces compact occupancy states, decision rules and real vectors that are more concise
than those from -TPE.
468

fiOptimally Solving Finite-Horizon Dec-POMDPs

Full Occupancy State 1

Compact Occupancy State 1





al , aol

al , aol

zhl , zhl
stl

str

.125 .125

zhr , zhr
stl

str

.125 .125

zhr , zhl
stl

str

.125 .125

zhl , zhr
stl

zhl , zhl

str

stl

.125 .125

str

0.5 0.5

Figure 11: A step-1 compact occupancy state 1 that corresponds to full occupancy state 1 .
In demonstrating that compact occupancy states are sufficient for optimal decision making in
occupancy-state MDPs, we rely on the notion of compact occupancy-state MDPs.
Definition 13. The compact occupancy-state MDP w.r.t. occupancy-state MDP M is given by tuple
 A, R, P, 0 , T ):
M  (,
  = t{0,1,...,T }  t is the space of compact occupancy states, with 0 = 0 the initial compact
occupancy state;
 A = t{0,1,...,T } At is the space of compact joint (decentralized) decision rules;
P
 R :   A 7 R is a compact reward function where R(, d ) = s, (s, )  rd () (s);

def
 P :   A 7  is a compact transition function where P(, d ) =   , with   = P(, d ); and

 T denotes the planning horizon.
Analogous to occupancy-state MDPs, compact occupancy states can be updated using known
transition and observation functions of the Dec-POMDP given the current compact occupancy state
and the chosen compact separable joint decision rule. The rewards are also calculated (as expectations) using the known reward model of the Dec-POMDP. Finally, the optimal value function of M
is solution of the optimality equations:


def


V M,t
(t ) = max R(t , dt ) + V M,t+1
( P(t , dt )) ,
t  {0, 1, . . . , T  1},
(26)
dt  A

 () = 0.
with an added boundary condition V M,T

4.1.3 Sufficiency of Compact Occupancy States
Below, we prove that, when planning over compact occupancy states instead of full occupancy
states, the optimal separable joint policy for the compact model immediately induces a corresponding optimal separable joint policy for the original model. Before proceeding any further, we first
demonstrate the optimality of compact policies.
469

fiDibangoye, Amato, Buffet & Charpillet

Note that this proof mirrors a similar proof showing that histories for an agent can be clustered
without any change to the optimal policy or loss in value by using probabilistic equivalence in
the traditional Dec-POMDP representation (Oliehoek et al., 2013). We extend these ideas to the
occupancy-state MDP case and incorporate truncation probabilistic equivalence here to show that
the compact policies preserve optimality.
Theorem 4 (Optimality of compact policies). In occupancy-state MDPs, there exists optimal policies of an agent that depend only upon labels of that agent produced based on either local or
truncation probabilistic equivalence with respect to occupancy states these optimal policies induce,
and not on private histories.
Proof. We construct the proof by induction. We first show that in the last step of the problem, an
agent policy depends on labels, but not on its private histories.
i
Given occupancy state T 1  T 1 and policies i
T 1  T 1 of the other agents I\{i}, the policy
iT 1  iT 1 of agent i  I is a best response to i
T 1 . That is, it chooses for every private history
i
i
T 1  T 1 a private action so as to maximize value based on model (Pr(s, Ti1 |Ti 1 , T 1 )) s,i
T 1
occupancy state T 1 and private history Ti 1 induce over possible histories of the other agents and
resulting states of the system:
X
i i
i
(27)
Pr(s, Ti1 |Ti 1 , T 1 )  ra ,T 1 (T 1 ) (s).
iT 1 (Ti 1 )  arg maxai Ai
s,Ti1

In assigning private actions to private histories given T 1 , only nonzero private histories w.r.t. T 1
affect the outcome. That is, zero probability private histories w.r.t. T 1 can be prescribed any private action without losing optimality (e.g., a private action identical to that of the associated labels
generated based on either local or truncation probabilistic equivalence relations). Hence, policy
iT 1 depends on zero probability private histories w.r.t. T 1 only through corresponding labels.
Next, we restrict our attention to nonzero probability private histories w.r.t. T 1 . Recall that private
histories of this family that are -TPE with one another are also -LPE with one another. Therefore,
if we demonstrate the property for private histories of this family that are -LPE with one another,
the proof follows immediately for those that are -TPE.
Assume Ti 1 is a nonzero probability private history w.r.t. T 1 . If Ti 1 is in the cluster of
T 1 -LPE private histories with label iT 1 (another nonzero probability private history w.r.t. T 1 ),
then equality (Pr(s, Ti1 |Ti 1 , T 1 ))s,i = (Pr(s, Ti1 |iT 1 , T 1 ))s,i holds. As a consequence,
T 1
T 1
policy iT 1 depends on nonzero probability private history w.r.t. T 1 only through corresponding
labels:
X
i i
i
(28)
Pr(s, Ti1 |iT 1 , T 1 )  ra ,T 1(T 1 ) (s).
iT 1 (Ti 1 )  arg maxai Ai
s,Ti1

Therefore, the property holds at the last step of the problem, for any arbitrary agent i  I. This
allows us to define policies on the last step as mappings from labels to private actions, i.e., compact
policies.
Next, we rely on the concept of private policy tree, which is a tree that represents actions in
nodes and observations in edges with a depth that is the number of stages to go. The root node
determines the first private action to be taken. Then depending on private observation received,
the agent executes another private action; and so on until a leaf node is reached. A policy tree
470

fiOptimally Solving Finite-Horizon Dec-POMDPs

it:T 1 is a portion of a private policy it:T 1 , which prescribes private actions to be taken by agent
i  I in the remaining stages starting at a given private history ti . By an abuse of the notation, let
it:T 1 = it:T 1 (ti ), for all agents i  I and all time steps t  {0, 1, . . . , T }.
For the induction step, we can show that, if an agent policy-tree from step t + 1 onward depends
on private histories only through the corresponding labels for either equivalence relations, then
that agents policy tree from step t onward also depends on its private histories only through the
corresponding labels for either equivalence relations. Given occupancy state t  t and policy
i
i
i
i
t:T 1  t:T 1 of the other agents I\{i}, policy t:T 1  t:T 1 of agent i  I is as follows:
X
i
i
i
it:T 1 (ti )  arg maxi t:T 1
Pr(s, ti |ti , t )  t:T 1 ,t:T 1(t ) (s),
(29)
t:T 1

s,ti

i

i

i

t:T 1 ,t:T 1 (t )
i
is the associated vector
where (it:T 1 , i
t:T 1 (t )) is a separable joint policy-tree and 
value. Again, in assigning private actions to private histories given t , only nonzero probability
private histories w.r.t. t matter. In fact, the property trivially holds for zero probability private
histories w.r.t. t . For nonzero probability private histories w.r.t. t , the property holds for private
histories that belong to a cluster of t -TPE private histories since those histories would belong to a
cluster of t -LPE private histories, as previously discussed.
For this reason, we restrict our attention to nonzero probability private histories ti w.r.t. t that
belong to a cluster of t -TPE private histories with label it . Then, equality (Pr(s, ti |ti , t ))s,ti =
(Pr(s, ti |it , t )) s,ti holds. Hence,

it:T 1 (ti )  arg maxi

t:T 1 t:T 1

X

i

i

i

Pr(s, ti |it , t )  t:T 1 ,t:T 1(t ) (s).

(30)

s,ti

Consequently, an agent policy depends on private history for any step of the problem only through
corresponding labels for either equivalence relations. This demonstrates that a compact policy does
not lose information.

Theorem 5. Compact occupancy state  based on either local or truncation probabilistic equivalence relations is a sufficient statistic of occupancy state   t . Furthermore, an optimal separable
joint policy for compact occupancy-state MDP M, together with the correct estimation of the compact occupancy states, immediately induces an optimal separable joint policy for occupancy-state
MDP M.
Proof. The proof proceeds similarly to that of Theorem 2. That is, in proving the sufficiency of
the compact occupancy state with respect to its corresponding full occupancy state, we need to
demonstrate: (a) the optimal value function at a compact occupancy state is identical to that of its
corresponding occupancy state and (b) the next-step compact occupancy states depend only upon
the current compact occupancy states (and next-step compact separable joint decision rules). We
stated (b) in Definition 13, so only statement (a) remains to be proved. We show this by induction.
The sufficiency of the compact occupancy state with respect to its corresponding occupancy
 ( ) = V  ( ) = 0 for any
state trivially holds at the last step of the problem. In fact, V M,T
T
T
M,T

arbitrary occupancy state T and its corresponding compact occupancy state T (since the horizon
has been reached). If we assume the statement (a) holds for time-step t + 1, we can now show

471

fiDibangoye, Amato, Buffet & Charpillet

that it holds for time-step t. For any arbitrary step-t occupancy state, Bellmans optimality criterion
produces the following equality:
def



V M,t
(t ) = max R(t , dt ) + V M,t+1
(t+1 ),

(31)

dt  A



(t+1 ) = V M,t+1
(t+1 ). This
where t+1 = P(t , dt ). By the inductive hypothesis, we have that V M,t+1
results in equality:


V M,t
(t ) = max R(t , dt ) + V M,t+1
( P(t , dt )).

(32)

dt  A

In addition, Theorem 4 demonstrated that by restricting our attention to compact (joint) decision
rules A, we preserve optimality. Thus, we obtain:
X
X
R(t , dt ) =
rdt () ( s)
 s, (s, )  t (s, ),
(Theorem 4)
 s, t

=

X

s,

r

dt ()

( s)  t ( s, ),

(Definition of t )

 s, t

= R(t , dt ),

(dt () = dt ()).

Hence,


( P(t , dt )),
V M,t
(t ) = max R(t , dt ) + V M,t+1

(33)

dt  A


(t ).
= V M,t

(34)

Therefore, statement (a) holds for any arbitrary occupancy state t  t and any arbitrary time
step t  {0, 1, . . . , T  1}. Combining statements (a) and (b), we are guaranteed to find the optimal
value function for M by using compact occupancy states instead of occupancy states. In addition,
def
 )
given the optimal value function (V M,t
t{0,1,...,T } , the optimal compact policy  = (dt )t{0,1,...,T 1} is
obtained by successive one-step lookaheads: for all t  {0, 1, . . . , T  1},

( P(t , dt )),
dt  max R(t , dt ) + V M,t+1

(35)

dt  A

def

where 0 = 0 and t+1 = P(t , dt ). This immediately induces a separable joint policy  =
(dt )t{0,1,...,T 1} for the original occupancy-state MDP M such that, for every agent i  I and every private history ti , we set dti (ti ) = dti (it ), where ti is in a cluster with label it . Since both  and
 yield the same expected value starting at the initial occupancy state, separable joint policy  is
optimal for the original occupancy-state MDP M.

By solving the compact problem instead of the original one, we circumvent the exhaustive enumeration of occupancy states and decision rules and preserve ability to find an optimal solution
for the original problem. It is worth noticing that the optimal value function in the compact occupancy space is not PWLC. This is because compact occupancy states are expressed using different
label sets. However, by exploiting the PWLC property of the optimal value function in the full
occupancy-state space, we develop methods that can generalize value from one compact occupancy
472

fiOptimally Solving Finite-Horizon Dec-POMDPs

state to another. Next, we propose a method for incrementally improving lower and upper bounds
that narrow the range of the optimal value function. But, in contrast to the OHSVI algorithm, we do
this using compact occupancy states and compact real vectors. More precisely, our compact upper
and lower bounds are defined on full occupancy states, but only through their compact representations.
4.2 Feature-Based Compact Bounds
In our algorithm, upper and lower bounds are of crucial importance. They can narrow the range
of the optimal value function, determine suboptimal regions of the search space, and speed up the
convergence towards an exact solution. Our approach approximates the full lower- and upper-bound
heuristic functions with heuristic functions defined over the same full occupancy space (Tsitsiklis &
van Roy, 1996; Hauskrecht, 2000; Roy, Gordon, & Thrun, 2005). The new heuristic functions are
typically more compact (with respect to traditional high-dimensional vector or point sets discussed
in Section 3.2.2 and Section 3.2.3), and are easier to compute than the full bounds. Our heuristic
functions can be formulated as feature-based compact functions which combine dimensionality
reduction (using the clustering methods discussed above) and function approximation (Tsitsiklis &
van Roy, 1996). Thus, to demonstrate that the new heuristic functions are valid bounds, we will
analyze our dimension reduction and approximation methods.
4.2.1 Feature-Based Compact States and Vectors
One can think of feature-based compact representations as a dimension reduction model for representing high-dimensional bounds using bounds of lower dimensionality. However, this approach
requires a set of basis functions (or feature set), in which lower-dimensional bounds can be expressed effectively. A basis function (or feature) is a function which maps high-dimensional data to
one salient feature of the problem at hand. In our setting for example, a feature can be an indicator
function of whether a private history matches one specified label. Features are at the core of the
feature-based compact representations of full occupancy states and real vectors, which ultimately
serve to represent upper and lower bounds using either feature-based compact vector or point sets
in a similar way as standard vector and point set representations.
Definition 14. A feature is an indicator function  s,(i )iI : S   7 {0, 1} for one specified state
s  S and one specified joint label (i )iI  L that occupancy state  induces  i.e., for all s  S
and (i )iI  ,
(
1, if s = s and, for all i  I, i belongs to cluster with label i ,
i
 s,(i )iI ( s, ( )iI ) =
0, otherwise.
def

The feature set w.r.t. , denoted  , is given by  = { s, |s  S ,   iI Li }.
The feature set w.r.t.  represents the partition of the state and joint history space that equivalence
relation -LPE or -TPE induces. This partition is a set of nonempty subsets (B s, ) sS ,L (called
labeled joint clusters) such that sS ,L B s, = S  . A labeled joint cluster B s, consists of
the cross-product between the singleton {s} and labeled clusters B1 , B2 , . . . , B|I| that equivalence
relation -LPE or -TPE generates. Therefore, a feature can be interpreted as a way to check whether
a state and joint history pair belongs to a specified labeled joint cluster. Thus, features provide
473

fiDibangoye, Amato, Buffet & Charpillet

an alternative (possibly lossy) representation of compact occupancy states. One can express the
compact representation  of full occupancy state  using its feature set  , as follows


X X

 = 
 s, (s, )  (s, )
.
sS ()

 s, 

Specifically, for all state s  S and all labels i  Li and all agent i  I,
def

( s, 1 , 2 , . . . , |I| ) =

X

X

...

1 B1 2 B2

=

X

X

( s, 1 , 2 , |I| ),

|I| B|I|
def

( = (1 , 2 , . . . , |I| ))

( s, ),

( s,)B s,

=

X X

 s, (s, )  (s, ),

sS ()

An analogous property holds for feature-based compact real vectors w.r.t. . That is, a feature-based
compact real vector  is a | |-dimensional real vector that is expressed using  .
It is worth noticing that standard feature-based approaches assume a unique feature set and data
are all expressed based on that unique feature set, which eases bound generalization (Tsitsiklis &
van Roy, 1996). In our setting, however, different occupancy states yield different (possibly disjoint)
feature sets. Hence, feature-based compact occupancy states (or real vectors) are expressed based
on different feature sets, making it hard to transfer value from one feature-based compact occupancy
state to another one. Bounds generalize naturally among feature-based compact occupancy states
only if they share the same feature set. To remedy this, we introduce basis change operations for
both feature-based compact occupancy states and real vectors.
4.2.2 Change of Feature Set
In enabling the bound generalization, it is necessary to work with more than one feature set. Hence,
it is important to be able to easily transform feature vectors calculated with respect to one feature set
to their corresponding (possibly lossy) representations with respect to another feature set. To ease
the change of feature set, it is necessary to match features from the original feature set to those from
the destination feature set. We introduce two heuristic methods to match features from different
feature sets, thereby allowing the change of feature sets.
Definition 15. Let  and  be feature sets w.r.t. occupancy states  and   , respectively. The
projection of feature-based compact occupancy state  onto feature set  , denoted F (), is
given by probability distribution:


 X

def 
F () = 
,
(36)
 s, (s, )  (s, )
 s, 

 s, 

where F () reassigns the probability mass (s, ) of each pair (s, ), such that  s,   , to pair
( s, ), such that  s,   , if and only if they match, i.e.,  s, (s, ) = 1.
474

fiOptimally Solving Finite-Horizon Dec-POMDPs

This change of feature set describes a function from original feature set  to destination feature
set  . In particular, it is a surjective function (i.e., every feature  s, in the destination set has at
least one corresponding feature  s, in the original set  those such that  s, (s, ) = 1). The
transformation assigns to each feature in the destination set the probability mass of all corresponding
features in the original set. This heuristic method provides no guarantee that both F () and  share
the same optimal value. By replacing a range of features in the original set by a single feature in the
destination set, we produce compact but possibly lossy representations, which ultimately precludes
ability to preserve the value of the original compact occupancy state. Fortunately, since we will use
these representations to provide bounds, even lossy representations can generate useful bounds.
Definition 16. Let  and  be feature sets w.r.t. occupancy states  and   , respectively. The
projection of feature-based compact real vector  using feature set  , denoted G ( ), is given
by feature vector:



 X
def
(37)
 s, ( s, )   (s, )
G ( ) = 
 s, 

 s, 

This change of feature set applies to real vectors, and describes a function from destination
feature set  to original feature set  . Specifically, every pair ( s, ) along with feature  s, in
the destination set has a corresponding pair (s, ) along with a feature  s, in the original set 
i.e., the only one such that  s, ( s, ) = 1. The transformation assigns to each pair ( s, ) in the
destination set the value  (s, ) from their corresponding pair (s, ) in the original set. Since not all
pairs in the original set can have their values represented in G ( ), the resulting feature vector is
a lossy representation of the original vector  . The loss in the resulting feature vector depends on
original and destination feature sets, and the choice of the equivalence relation between histories.
As previously mentioned, we will make use of either -LPE or -TPE relations.
Using -LPE, we distinguish between state and joint-history pairs that involve only non-zero
probability private history sets w.r.t.  (i.e., non-zero probability pairs), and state and joint-history
pairs that involve zero probability private history sets w.r.t.  (i.e., zero probability pairs). For the
sake of conciseness, compact real vectors maintain only values associated to non-zero probability
pairs. All zero probability pairs have the same default value, for example (T  t) min sS ,aA ra (s).
Hence, the change of feature set is such that all pairs ( s, ) in the destination set, with a corresponding non-zero probability pair (s, ) in the original set, inherits the value  (s, ). However, all
pairs ( s, ) in the destination set, with a corresponding zero probability pair (s, ) in the original set,
inherits the default (and loose) value. Because the number of zero probability pairs in occupancy
state  is far larger than the number of non-zero probability pairs, feature vectors that result from
the change of basis via -LPE have multiple components with a loose value.
Using -TPE, we distinguish between state and joint-history pairs only through joint-history
suffixes of length m  see Definition 10. There are many pairs in the destination set, that would
have been associated to a corresponding zero probability pair in the original set using -LPE. Using -TPE, however, these pairs are associated to a non-zero probability pair in the original set.
Specifically, pair ( s, ) in the destination set, whose probability is zero with respect to , has a corresponding zero probability pair in the original set using -LPE. Yet, if pair ( s, ) in the destination
set and non-zero probability pair (s, ) in the original set share a common length m history suffix,
then ( s, ) is associated to (s, ) using -TPE. Thus, feature vectors that result from the change of
475

fiDibangoye, Amato, Buffet & Charpillet

feature set using -TPE involve fewer components with a default, loose value than those from using
-LPE.
Although heuristic methods for the change of feature set are not guaranteed to produce representations that retain all information of their compact counterparts, in many cases the resulting
(possibly lossy) representation is sufficient to generalize bounds over the entire compact occupancy
space. The bound property (i.e., the ability to overestimate or underestimate the optimal value) of
the resulting representation can be determined by examining methods for the change of feature set.
The following theorem (proved in the Appendix) establishes that the F and G mappings we use
preserve the bound property.
Theorem 6. For any arbitrary feature set , the change of feature set based on mappings G and
F preserve the bound property  i.e.,    t ,
 (), then   V  (F ());
a.   R, if   V M,t
M,t 


 ()  h, G ()i, then V  ()  h, G (G ())i.
b.   R| | , if V M,t



M,t

Theorem 6 shows that bound properties for a given occupancy state are preserved for the occupancy state obtained upon the change of feature set using heuristic methods F or G. Next, we
discuss how to approximate full upper and lower bounds over the entire occupancy space.
4.2.3 Compact Point-Value Mappings: Upper Bounds
The full upper-bound value function can be approximated by a finite set of points and the sawtooth
interpolation rule that estimates the value of an arbitrary point of the compact occupancy space by
relying on the points already experienced and their associated values. A key aspect of this heuristic
approximation is the sawtooth interpolation rule over compact occupancy states.
In the full upper-bound value function, the sawtooth interpolation can only approximate points
expressed within the same basis set. Here, we demonstrate that it can apply even when points are
expressed in different feature sets by means of the change of feature set. Let  = {(1 7 v1 ), (2 7
v2 ), . . . , (k 7 vk )} be a set of point-value pairs that represents approximate function U M defined
over the compact occupancy space, such that each point satisfies Theorem 6a. Then the approximate
value for an arbitrary compact occupancy state   based on point-value pair ( 7 v ) can be obtained
using the sawtooth interpolation in a way that is similar to the calculation in the full upper-bound
value function:


v = v + (v  v )  D(F (),   ),
where v =

P

s,

(38)

(s, )  v s, . From Theorem 6a, we know that feature vector F () shares the

same upper bound v with . In addition, F () and   are expressed using the same feature set
 . Hence the sawtooth interpolation can apply and produce upper-bound value v for compact
occupancy state   based on point-value pair ( 7 v ). The optimization with respect to compact
occupancy state   is then acquired by choosing the best overall upper-bound value from all pointvalue pairs in :


U M (  ) = min {v , v | ( 7 v )  t }.
476

(39)

fiOptimally Solving Finite-Horizon Dec-POMDPs

This heuristic approximation differs from the full upper-bound value function only because it requires the change of feature set F and compact occupancy states instead of full occupancy states.
Hence, this sawtooth interpolation is computationally more efficient than that of the full upperbound value function, because compact occupancy states are of lower dimensionality. As for the accuracy of the resulting upper-bound values, it is not clear how this sawtooth interpolation compares
with that from the full upper-bound value function (i.e., whether or not the sawtooth interpolation
weakens the upper bounds). Yet, a collection of point-value pairs obtained for a selection of compact occupancy states can be combined to define an approximate function of the upper-bound value
function as discussed next.
A feature-based compact value function (U M,t )t{0,1,...,T } over compact occupancy space 
is represented using sets (t )t{0,1,...,T } of point-value pairs that estimate values of any arbitrary
compact occupancy state. Initially, each set t contains |S | point-value pairs { s, 7 vts, | s  S }
that represent the step-t optimal value function of the underlying MDP. The sawtooth interpolation
estimates U M,t at any compact occupancy state     t as follows:


U M,t (  ) = min {v , v | ( 7 v )  t },

t  {0, 1, . . . , T }.

Set t is updated for every compact occupancy state     t , using a point-based backup step as
follows:
[n
 o
t = t
(  7 v ) ,
t  {0, 1, . . . , T }


where v = maxd  A R(  , d ) + U M,t+1 ( P(  , d )). Approximate function (U M,t )t{0,1,...,T } is an
upper-bound value function similarly to the full upper-bound value function as stated below and
proven in Appendix A.
Theorem 7. Feature-based compact value function (U M,t )t{0,1,...,T } , as iteratively updated, upper
bounds the optimal value function over the entire compact occupancy space.
4.2.4 Compact Vector Sets: Lower Bounds
A full lower bound over the full occupancy space can be approximated by a finite set of compact
real vectors along with their associated feature sets and linear function updates. We take inspiration
from initialization, evaluation and update routines from the full lower-bound value function. One
]
important aspect of our approximation lies in the definition of the update operation, denoted backup.
Let  be a set of compact real vectors that represents the approximate value function, such that
each compact real vector satisfies Theorem 6b. Then, a new compact real vector for any compact
occupancy state  and compact decision rule d can be computed efficiently as in the full lowerbound value function (Section 3.2.2):



gd = rd + arg maxg :  , gd ,
(40)


d



where gd = G P(,d ) ()  (pd ) is the expression of the projection of G P(,d ) () onto feature set



 , and G P(,d ) () is the expression of  in feature set  P(,d ) . The optimization with respect to


compact occupancy state  is then acquired by choosing the compact vector with the best overall
477

fiDibangoye, Amato, Buffet & Charpillet

value from all vectors gd :


] , ) = arg max 
backup(
g

d

: d  A




, gd .

(41)



The principal difference with respect to the full lower-bound value function lies in the use of transformation G and compact real vectors instead of high-dimensional real vectors. Hence, this backup
operation is more efficient than that of the full lower-bound value function, since operations are the
same but we now use only lower dimensional vectors, which is likely to save significant time. But
the change of feature set may produce weaker bounds. Nonetheless, a collection of compact vectors
obtained for a selection of compact occupancy states can be combined to define an approximate
function of the lower-bound value function as discussed next.
A feature-based compact value function (L M,t )t{0,1,...,T 1} over the compact occupancy space
 is represented using sets (t )t{0,1,...,T 1} of compact real vectors along with their associated feature
sets that estimate values of any arbitrary compact occupancy state. Initially, each set t contains a
single compact real vector t given by
t () = (T  t) min ra (s),

t  {0, 1, . . . , T }.

sS ,aA

(42)

The max-vector rule estimates L M,t at any compact occupancy t   t as follows:
L M,t (t ) =

max
(7t )t

ht , Gt (t )i,

t  {0, 1, . . . , T }.

(43)

Set t is updated for every compact occupancy state t   t , using point-based backup steps as
follows:
[n
o
] t+1 , t )) .
t = t
(t 7 backup(
(44)
Approximate function (L M,t )t{0,1,...,T 1} is a lower-bound value function since its main difference
with respect to the full lower-bound value function lies in the use of G , which preserves the bound
property. For a complete proof, the reader can refer to Appendix A.
Theorem 8. Feature-based compact value function (L M,t )t{0,1,...,T } , lower bounds the optimal value
function over the entire compact occupancy space.
4.3 Feature-Based Heuristic Search Value Iteration Algorithm
This section presents the feature-based heuristic search value iteration algorithm (FB-HSVI) that
iteratively updates feature-based compact lower- and upper-bound representations. We also discuss
FB-HSVIs theoretical guarantees.
4.3.1 Algorithm Description
Similar to OHSVI (Algorithm 1), FB-HSVI (Algorithm 2) solves occupancy-state MDPs by generating trajectories of occupancy states and iteratively updating lower and upper bounds, but in
the case of FB-HSVI, these are now compact occupancy states and feature-based compact lower
(L M,t )t{0,1,...,T } and upper bounds (U M,t )t{0,1,...,T } . FB-HSVI improves the scalability of OHSVI in
478

fiOptimally Solving Finite-Horizon Dec-POMDPs

several ways. First, FB-HSVI replaces full exact representations by compact representations for all:
occupancy states, decision rules, lower and upper bounds. In addition, it combines stopping criteria
from HSVI (Smith, 2007) and optimal classical heuristic search methods (e.g., Hart et al., 1968;
Korf, 1990), which may result in more efficient pruning of unnecessary subspaces.
Algorithm 2: The FB-HSVI Algorithm.
function FB-HSVI((L M,t )t{0,1, ,T } , (U M,t )t{0,1, ,T } )
initialize L M,t and U M,t for all t  {0,    , T  1}.
while  Stop (0 , 0) do Explore(0, 0)
function Explore(t, gt )
if  Stop (t , gt ) then
dt  arg maxdt  A R(t , dt ) + U M,t+1 ( P(t , dt ))
Update U M,t
Explore( P(t , dt ), R(t , dt ) + gt )
Update L M,t
return gt
function Stop(t , gt )
return U M,t (t )  L M,t (t )  L M,0 (0 )  gt + U M,t (t )
FB-HSVI differs from OHSVI in four main ways:
1. compact representation of occupancy states, which significantly reduces the search space;
2. compact representation of decision rules, which speeds up the decision-rule selection;
3. compact representation of lower and upper bounds, which speeds up the convergence;
4. enhanced value function generalization and combination of stopping criteria, which results
in more efficient pruning of unnecessary subspaces.
4.3.2 Stopping Criteria
The stopping criteria of FB-HSVI build upon those from optimal classical heuristic search methods
(e.g., Hart et al., 1968; Korf, 1990; Smith, 2007). They determine when to stop the current trajectory
of compact occupancy states in the algorithm. Ideally, an optimal criterion would measure the distance from the current trajectory to an optimal trajectory, but this is not known. Instead, we use two
criteria based on the upper and lower bound values of trajectories and compact occupancy states.
The upper bound of the current trajectory, which we denote f (t ), is the sum of two functions: (1)
the past trajectory-reward function g(0 , t ), which is the sum of rewards from the starting compact occupancy state 0 to the current occupancy state t ; and (2) the future trajectory-reward from
compact occupancy state t , which is an admissible heuristic estimate, e.g., upper-bound U M,t (t ) at
compact occupancy state t .
The first criterion relies on the fact that there is no reason to expand an occupancy state t that
has f (t ) less than or equal to L M,0 (0 ), since it cannot lead to a solution better than the current best
solution; a criterion previously used in optimal classical heuristic search methods (e.g., Hart et al.,
1968; Korf, 1990).

479

fiDibangoye, Amato, Buffet & Charpillet

Criterion 1. A trajectory of occupancy states (0 , . . . , t ) is interrupted whenever heuristic value
f (0 ) is less than or equal to L M,0 (0 )  i.e., L M,0 (0 )  f (0 ). The best solution found so far
is optimal if there is no expanded occupancy state t on the frontier of the search space2 with a
heuristic-value f (t ) higher than L M,0 (0 ).
The second criterion builds upon the fact that there is no reason to expand an occupancy state t
that has upper bound less than or equal to its lower bound (Smith, 2007).
Criterion 2. A trajectory of occupancy states (0 , . . . , t ) is interrupted whenever upper bound
U M,t (t ) is less than or equal to lower bound L M,t (t )  i.e., L M,t (t )  U M,t (t ). The best solution
found so far is optimal if upper and lower bounds at the initial occupancy state are equal.
By interrupting any trajectory that satisfies either of criterion 1 or 2, FB-HSVI preserves the
ability to find an optimal separable joint policy, as shown below.
4.3.3 Convergence Guarantees
Theorems 7 and 8 show the feature-based compact functions (L M,t )t{0,1, ,T } and (U M,t )t{0,1, ,T } as
iteratively updated by FB-HSVI (Algorithm 2) upper and lower-bounds the optimal value function.
Next, we prove that upon the update of each trajectory no compact occupancy state has its bounds
depreciated, and at least one compact occupancy state improves its bounds. Since there is a finite
number of compact occupancy states, the bounds ultimately converge to the optimal value at the
initial occupancy state. Here, we use this argument to show that FB-HSVI converges to an optimal
separable joint policy after a finite number of iterations. To this end, a compact occupancy state is
said to be finished if either criterion 1 or 2 is satisfied; otherwise it is not finished. Moreover, all
compact occupancy states T at the last time step T are finished since criterion 2 is satisfied at the
last time step T .
Theorem 9. The FB-HSVI algorithm always terminates after a finite number of trials and the solution found at termination  the separable joint policy the lower bound induces  is optimal.
Proof. First, we show by contradiction that the algorithm cannot terminate before an optimal solution is found. Suppose the algorithm terminates before finding an optimal solution which has value
f  (0 ). Then, the sequence of f (0 ) values generated while planning is f 0 (0 ), f 1 (0 ), . . . , f k (0 ),
where f 0 (0 ) is the initial lower bound before any solution is found, f 1 (0 ) is the value of the first
solution found, and f k (0 ) that of the last solution found. In addition, we know by hypothesis that
f 0 (0 ) < f 1 (0 ) < . . . < f k (0 )  f  (0 ), where the last inequality holds under the assumption that
the algorithm may terminate with a suboptimal solution.
Now consider an optimal path 0 , 1 , . . . , T leading from the initial occupancy state to a terminal
occupancy state. Under the assumption that this optimal path was not found, there must be some
occupancy state t along this path that was generated but not expanded. This is only possible if
f (t )  f k (0 ). But by the admissibility of f , we know that f (t )  f  (0 ) and therefore f (t ) 
f  (0 ) > f m (0 ) for any m  {0, 1, . . . , k}. From this contradiction, it follows that the algorithm
cannot terminate before an optimal solution is found.
Next, we show that each trial turns at least one not finished occupancy state into a finished one.
Suppose the algorithm has not yet terminated and a trial is executed. Let the last two occupancy
2. Typically, search algorithms involves expanding nodes by adding all unexpanded neighboring nodes into a priority
queue, called a frontier of the search space.

480

fiOptimally Solving Finite-Horizon Dec-POMDPs

states encountered during the forward expansion be t and t+1 . Given that the trial terminated at
t+1 , we know that before the trial, t was not finished but t+1 was finished. Because t+1 results
from a greedy separable joint decision rule selection at t , we know t will be finished after being
updated. This is because only two scenarios are possible, each of which corresponds to a stopping
condition:
 either t+1 yields its optimal value, then t will also yield its optimal value after being updated,
making it a finished occupancy state after the update;
 or t+1 has f (t+1 ) lower than or equal to L M,0 (0 ), then t will also have an f (t ) lower than
or equal to L M,0 (0 ) after being updated.
Thus, executing a trial causes occupancy state t , which was not finished, to become finished.
Finally, we show that the algorithm terminates after a finite number of trials. To this end, we
note that the search graph of the algorithm is a tree similar to Figure 8, with a bounded branching
factor | At | at depth t  {0, 1, . . . , T }. By hypothesis, only occupancy states that appear at depth t < T
are not finished. Thus, the total number of occupancy states at all depths up to time step T is upper
bounded by the total number of information states at all depths up to time step T :
1
| Tt=0
St | = |A |

|I||Z  |T |A |T  1
,
|Z  ||A |  1

(45)

where A = maxiI Ai and Z  = maxiI Z i . Given that at least one occupancy state becomes finished
1 S | trials, causing
after each trial, the initial occupancy state must become finished after at most |Tt=0
t
the algorithm to terminate.

Another important property of FB-HSVI is that it refines both upper and lower bounds throughout planning. Since compact occupancy state expansions are interleaved with updates, FB-HSVI
offers an anytime solution. Furthermore, cutting off FB-HSVI trials at any time, we know that the
difference between the current best solution and the optimal one is bounded.
Theorem 10. At any iteration of FB-HSVI, the current solution  and the separable joint policy
induced by the current lower bound  is within  = U M,0 (0 )  L M,0 (0 ) of the optimal solution.
Proof. Formally, the difference in value between executing the separable joint policy lb induced
by the current lower bound instead of an optimal separable joint policy  is written as follows:
V M, (0 )  V M,lb (0 ) = V M, (0 )  L M,0 (0 ),

(V M,lb (0 ) = L M,0 (0 ))

(46)

 U M,0 (0 )  L M,0 (0 ).

(V M, (0 )  U M,0 (0 ))

(47)

Consequently, whenever FB-HSVI is interrupted, its current solution is within the  given above of
the optimal solution.


5. Experiments
This section empirically demonstrates and validates the importance of our feature-based heuristic
search value iteration (FB-HSVI) algorithm. We show that FB-HSVI outperforms all existing exact
algorithms on all tested domains from the literature and that FB-HSVI can solve those problems
over unprecedented time horizons.
481

fiDibangoye, Amato, Buffet & Charpillet

5.1 Experimental Setup
As discussed throughout this paper, there are many key components that can affect the performance
of FB-HSVI. These key components include the (upper and lower) bound representations, the bound
update methods, the history compression, the value generalization, and the initial upper bound.
We present three variants of FB-HSVI, denoted OHSVI, FB-HSVI-LPE and FB-HSVI-TPE. The
two latter differ only on the notion of history equivalence they use in the feature-based compact
representations (see Table 2). When no equivalence relation is given, FB-HSVI refers to its default
(and better performing) implementation, FB-HSVI-TPE.
Algorithm
OHSVI
FB-HSVI-LPE
FB-HSVI-TPE

Bound Representations
full
feature-based compact
feature-based compact

Compression
none
lpe
tpe

Table 2: A review of the selected algorithmic components we use.
We selected benchmarks with the goal of spanning the range of properties that may affect the
performance of a Dec-POMDP solver. In Table 3, we review the selected domains and their properties. These domains can be downloaded at http://masplan.org.

Dec-Tiger
Mabc
Grid-Small
Recycling-Robots
Box Pushing
Mars Rovers

domain M parameters
N |S | |Ai | |Z i |
2
2
3
2
2
4
2
2
2 16
5
2
2
4
3
2
2 100 4
5
2 256 6
8

T =2
6561
256
390625
6561
3.34  107
1.69  1014

|0:t | for different T
T =5
T = 10
3.43  1030
1.39  10977
1.84  1019
3.23  10616
5.42  1044
3.09  101431
30
3.43  10
1.39  10977
5.23  10940
1.25  102939746
7285
1.88  10
2.57  10238723869

Table 3: Domain parameters and maximum number of separable joint policies per horizons.

5.2 Empirical Analysis of our Algorithms
In this section, we compare FB-HSVI to other exact solvers. The exact Dec-POMDP solvers considered are the state-of-the-art methods including: GMAA*-ICE (Oliehoek et al., 2013), IPG (Amato
et al., 2009), MILP (Aras & Dutech, 2010), and LPC (Boularias & Chaib-draa, 2008). IPG and
LPC perform dynamic programming, GMAA*-ICE performs heuristic search and MILP is a mixed
integer linear programming method. Results for GMAA*-ICE (provided by Matthijs Spaan), IPG,
MILP, LPC were conducted on different machines. Because of this, the timing results are not directly comparable, but are likely to only differ by a small constant factor. Our three FB-HSVI
variants (Table 2) were implemented in the same framework, using identical basic operations, such
as occupancy state and value function updates, and separable joint decision rule selection. We terminate FB-HSVI whenever the distance between lower and upper bounds is within  = 0.01. A
time limit was set to 1000ms.

482

fiOptimally Solving Finite-Horizon Dec-POMDPs

5.2.1 Comparing to other Exact Planners
T

MILP

LPC

IPG ICE OHSVI
multi-agent tiger
0.32 0.01
0.161
55.4 0.01 28.567
2286 108
347

2
3
4
5
6
7
8
9
10


4.9
72

0.17
1.79
534

2
3
4
5
10
30
70
100











recycling robot
0.30
36
0.04
1.07
36
0.555
42.0
72
696.8
1812
72

2
3
4
5
6
7
8
9
10
20
30











meeting in a 3x3 grid


93.029







FB-HSVI

L0, M (0 )

0.03
0.40
1.36
9.65
24.42
33.11
41.21
58.51
65.57

4.00
5.1908
4.8027
7.0264
10.381
9.9935
12.217
15.572
15.184

0.01
0.10
0.30
0.34
0.52
1.13
2.13
2.93

7.000
10.660
13.380
16.486
31.863
93.402
216.47
308.78

0.03
0.04
0.79
1.30
1.88
2.55
18.06
24.39
34.42
291.1
456.6

0.0
0.133
0.432
0.894
1.491
2.19
2.96
3.80
4.68
14.35
24.33

Table 4: Experiments comparing the computation times (in seconds) of all exact solvers (part 1).
Time limit violations are indicated by  ,  indicate unknown values. Bold entries
correspond to the best known results for these benchmarks, both in terms of computational
time and expected value.

Tables 4 and 5 show performance results for the exact algorithms. For each algorithm, we reported
the computation time, which includes the time to compute heuristic values when appropriate (since
all algorithms do not use the same heuristics). We also reported the best expected cumulative reward
L M,0 (0 ) at the initial occupancy state. Tables 4 and 5 clearly show that FB-HSVI allows for significant improvement over the state-of-the-art solvers: for all tested benchmarks we provide results
for longer horizons than have been solved previously (the bold entries). In many cases, an (epsilon)
optimal solution can be found for horizons that are an order of magnitude larger than was previously
solvable. There are two main reasons for FB-HSVIs performance. First, it searches in the space
483

fiDibangoye, Amato, Buffet & Charpillet

of policies mapping lower-dimensional features to actions, whereas the other exact solvers search
in the space of policies mapping full histories to actions. In addition, it uses a value function mapping occupancy states to reals allowing it to generalize the value function over unvisited occupancy
states whereas all other solvers use value functions mapping partial policies to reals. FB-HSVI performs best when the domain possesses structure that results in a compact value function, as in the
recycling robot and mabc domains.
T

MILP

LPC

2
3
4
5
10
30
50
100











IPG
ICE
OHSVI
broadcast channel


0.036


3.446





2
3
4
5
6
7
8
9
10

0
0.65
1624






0
0.18
4.09
77.4

2
3
4
5
6
7
8
9
10









cooperative box-pushing
1.07
36
0.294
6.43
540
1138
2596

2
3
4
5
6
7
8
9
10







83
389

grid small
36
0.911
36
1512
242605

Mars rovers
1.0
0.027
1.0
1.881
103

FB-HSVI

L0, M (0 )

0.02
0.22
0.32
0.33
0.78
14.0
41.7
473.3

2
2.99
3.89
4.79
9.29
27.42
45.50
90.76

0
0.1
0.73
1.39
3.51
8.30
42.2
69.03
581.2

0.37
0.91
1.55
2.24
2.97
3.71
4.47
5.23
6.03

0.1
0.457
0.622
5.854
10.7
24.96
28.97
184.3
293.7

17.600
66.081
98.593
107.72
120.67
156.42
191.22
210.27
223.74

0.10
0.23
0.47
0.82
3.97
5.81
22.8
26.5
62.7

5.80
9.38
10.18
13.26
18.62
20.90
22.47
24.31
26.31

Table 5: Experiments comparing the computation times (in seconds) of all exact solvers (part 2).

484

fiOptimally Solving Finite-Horizon Dec-POMDPs

5.2.2 Choosing a Method to Keep Information Concise
We now compare the local and truncation probabilistic equivalence notions we introduced to maintain concise the representations of the occupancy states, decision rules, and value functions.
Box-Pushing

Mars Rover

400

TPE
LPE

103

|H|

|H|

300
200
100

102

101

0
2

3

4
Horizon T

5

2

6

Recycling Robot

3

4

5 6 7
Horizon T

8

9

10

Grid-Small

40
102
|H|

|H|

30
20
10

101

0
2

3

4

5 6 7
Horizon T

8

9

10

2

3

4
5
Horizon T

Dec-Tiger

7

6

mabc
20

100
|H|

|H|

15
10

50
5
0

0
2

3

4
5
Horizon T

6

2

7

3

4

5 6 7
Horizon T

8

9

10

Figure 12: Comparison of compression methods to maintain concise data through FB-HSVI-LPE
and FB-HSVI-TPE. All graphs shows the memory requirements until convergence or
time exceeds in the y-axis given the various number of planning horizons in x-axis.

Clearly, algorithms that use feature-based compact representations provide significant savings
in the number of maintained histories over those that do not (e.g., OHSVI). Using OHSVI, the
number of generated histories grows (in the worst case) exponentially with the planning horizon.
It is this exponential growth that explains why OHSVI, which does not use history aggregation,

485

fiDibangoye, Amato, Buffet & Charpillet

cannot scale beyond planning horizon T = 4 over all tested domains (see Tables 4 and 5). For
Recycling Robot at horizon T = 5, experimental results together with Table 3 show that algorithms
that use our compression methods maintain up to 30 orders of magnitude less separable joint policies
than algorithms that do not. The number of histories retained is important as all occupancy states,
decision rules, and value functions are mappings from reachable histories (or corresponding labels).
To this end, we compare LPE and TPE over our selection of benchmarks and various planning
horizons.
As previously discussed, though LPE yields compact occupancy states that are more concise
than those that result from TPE, the latter eases the generalization of bounds, which speeds up the
convergence to an optimal solution. Figure 12 reports the total number of joint labels  denoted
|L|  that are explicitly maintained for FB-HSVI-LPE and FB-HSVI-TPE over various planning
horizons. We observe that TPE yields more concise bound representations than LPE over most
benchmarks and planning horizons (i.e., using TPE number |L| is lower than that using LPE). In
particular, we notice that, in all tested domains, there is a bounded number of labels that is sufficient
for representing optimal or near-optimal value functions. TPE often succeeds in identifying this
memory-bounded parametric space, resulting in more concise value functions, whereas LPE often
fails.
In the Recycling Robot problem for example, TPE yields no more than 6 joint labels (i.e.,
histories) for all horizons whereas LPE maintains up to 38 different joint labels, a number that
keeps growing as the planning horizon increases. In fact, (Dibangoye, Amato, & Doniec, 2012;
Becker, Zilberstein, Lesser, & Goldman, 2004) demonstrated that in the Recycling-Robot problem,
the most recent private observation is sufficient to summarize all past private histories of each agent
(i.e., only the four joint observations are necessary). Here, TPE yields 6 joint labels because it
relies on joint action-observation histories rather than joint observation histories. In the BroadcastChannel domain, TPE yields no more than 4 joint labels for all horizons whereas LPE produces up
to 20 different joint labels. Again, these results are due to the underlying structure of the BroadcastChannel domain. In such a scenario, the future states of the world are conditionally independent of
joint histories. Hence, TPE can forget about joint histories, and reason only about states. Another
domain of interest is the Dec-Tiger problem. In this problem, for T = 6, TPE produces no more
than 30 joint labels for all horizons whereas LPE maintains up to 126 different joint labels. Our
assumption is that there always exists an optimal separable joint policy that is periodic (with period
3) for the Dec-Tiger domain. In other words, there exists an optimal separable joint policy that
depends on histories only upon the most recent three action-observation pairs. Also, there are many
scenarios in which both equivalence relations would fail to identify a memory-bounded space of
histories, even if such a space exists. For example, important information in a history may be
spread over a few time steps, but not necessarily the last ones.

6. Discussion
While we have demonstrated that our method can solve Dec-POMDPs which are larger than those
previously solved, many practical applications are much larger than domains considered in this
paper. As a result, additional methods may be necessary to solve very large problems which do not
permit the construction of a concise feature space while preserving optimality. This is of concern
since the numbers of states and histories impact all occupancy states, separable joint decision rules,
and value functions. Maintaining these objects for large feature spaces is prohibitive. This highlights

486

fiOptimally Solving Finite-Horizon Dec-POMDPs

the necessity of addressing the scalability issue of FB-HSVI through more concise (and possibly
lossy) feature spaces. In that direction, we have already extended the general methodology presented
in this paper along two lines: error-bounded approximations and tractable subclasses.
6.1 Error-Bounded Approximations
FB-HSVI can find an optimal solution because it maintains concise representations that preserve
optimality. This is both an advantage and a liability. On the one hand, for problems of a reasonable
size, the algorithm can find an optimal solution. On the other hand, in many realistic applications,
it will run out of time or memory. These scalability limitations are because FB-HSVI maintains
accurate estimates of (compact) occupancy states, value functions and decision rules. To improve
the scalability of Dec-POMDP solvers, many researchers have investigated approximate solutions.
A notable example of this family includes the memory-bounded dynamic programming (MBDP)
algorithm for finite-horizon Dec-POMDPs (Seuken & Zilberstein, 2007; Carlin & Zilberstein, 2008;
Dibangoye, Mouaddib, & Chaib-draa, 2009; Kumar & Zilberstein, 2010; Wu, Zilberstein, & Chen,
2010). These are dynamic programming methods that require bounded computational resources
to produce heuristic solutions that empirically perform well in standard Dec-POMDP benchmarks.
However, these methods do not possess any theoretical guarantees concerning the quality of their
solutions.
Recently, we introduced a framework for monitoring the error in FB-HSVI by replacing an exact
estimate of (compact) occupancy states, decision rules and value functions, by their approximate
counterparts (Dibangoye, Mouaddib, & Chaib-draa, 2011; Dibangoye, Buffet, & Charpillet, 2014).
The resulting algorithm can solve Dec-POMDP instances with larger planning horizon while still
providing strong theoretical guarantees.
It is also worth noting that, because FB-HSVI is trial-based, it can be used as an anytime algorithm. That is, it alternates between the generation of an occupancy-state trajectory and the update
of the current best value function. As the algorithm proceeds, the current (best) value function is
improved at the expense of increased computational time. The algorithm can be terminated either
when a satisfactory value function is attained, or when allocated planning time is exceeded. In either
case, this algorithm can always provide online performance bounds on the returned value function
illustrating how far from the optimal value function the current one is.
In the future, we also would like to explore using occupancy states over observation histories
(rather than action-observation histories), which were shown to be sufficient (along with actionobservation histories) simultaneous to this work (Oliehoek, 2013). The inclusion of observation
histories could lead to further scalability gains by reducing the dimensionality of the feature space.
6.2 Tractable Subclasses
Many attempts to address the scalability issues in Dec-POMDPs rely on the use of tractable subclasses. These subclasses have additional assumptions that allow more concise representations for
all occupancy states, decision rules and value functions; and therefore speed up the convergence
towards an optimal solution.
For instance, we have already shown that occupancy states over just states (and not agent histories) can be used in transition- and observation-independent Dec-MDPs (Becker et al., 2004) (where
the state is fully determined by the joint observation) to greatly increase scalability while preserving
optimality (Dibangoye et al., 2012; Dibangoye, Amato, Doniec, & Charpillet, 2013). By restrict487

fiDibangoye, Amato, Buffet & Charpillet

ing attention to decentralized Markov policies (i.e., mappings from private states to private actions)
we reduce the complexity significantly (NP versus NEXP), and make it possible to optimally solve
larger problems. We plan to investigate other forms of tractable structures including temporal dependencies or constraints that induce structured domains in both single and multi-agent settings
(Dibangoye, Chaib-draa, & Mouaddib, 2008; Dibangoye, Shani, Chaib-Draa, & Mouaddib, 2009;
Pajarinen, Hottinen, & Peltonen, 2013). In that line of research, we introduced a novel approach
called structural analysis as a means of discovering the underlying structural properties embedded
in certain decentralized decision-making problems (Dibangoye, Buffet, & Simonin, 2015).
We also applied the general methodology presented in this paper to scale up the number of
agents involved in the process. To this end, we consider domains that exhibit the locality of interactions (Dibangoye, Amato, Buffet, & Charpillet, 2015, 2014). Examples include the networked
distributed partially observable Markov decision processes (ND-POMDPs) (Nair, Varakantham,
Tambe, & Yokoo, 2005). We plan to explore applying our methodology and FB-HSVI to DecPOMDPs where agents have joint dynamics or rewards, as well as domains with delayed communication (Ooi & Wornell, 1996; Grizzle, Marcus, & Hsu, 1981; Oliehoek & Spaan, 2012), as a means
of reducing the memory burden.
A secondary (but no less important) issue concerning scalability in Dec-POMDPs pertains to
efficient methods to update occupancy states and value functions at the planning stage. The locality
of interaction among agents may be exploited statically (e.g., Nair et al., 2005; Kumar & Zilberstein,
2009; Amato, Konidaris, & Kaelbling, 2014) or dynamically (e.g., Canu & Mouaddib, 2011) by
considering factorization and graphical models in our representation and hence improve scalability.
This is a critical issue as the number of occupancy states necessary to obtain a good solution may
be exponential in the planning horizon. So, techniques that can efficiently update both occupancy
states and value functions are of great importance.

7. Conclusion
This paper describes a novel way of representing Dec-POMDPs, as continuous-state MDPs with
piecewise-linear convex value functions, and a scalable algorithm for generating -optimal solutions. We summarize the key contributions below.
By exploiting the assumption of centralized planning for decentralized execution, our method
recasts the Dec-POMDP problem into an equivalent deterministic and centralized fully observable
MDP (using information that is available to all agents). Next, we identify a concise statistic  the
occupancy state  that represents the state of the resulting fully observable MDP, which we call
the occupancy-state MDP. We demonstrate that the optimal value functions of occupancy MDPs
are piecewise linear and convex functions of the occupancy states. We also prove that an optimal
solution of the occupancy-state MDP is an optimal solution to the corresponding Dec-POMDP.
We also present the feature-based heuristic search value iteration (FB-HSVI) algorithm to find
an optimal solution to the occupancy-state MDP. This algorithm builds off the theory for solving
POMDPs and MDPs, as our occupancy-state MDP allows these methods to be directly applied to
Dec-POMDPs for the first time. We believe FB-HSVI is a major step forward in scalable exact
solutions for Dec-POMDPs. This scalability is achieved by defining feature-based compact occupancy states and decision rules through the use of equivalence relations between private histories.
These concise representations permit us to circumvent the exhaustive enumeration of an otherwise
intractable number of occupancy states and decision rules.

488

fiOptimally Solving Finite-Horizon Dec-POMDPs

Another aspect of the improved scalability stems from generalization of the value function.
This is achieved through the piecewise linear and convex functions in the occupancy-state MDP.
We show that, although feature-based compact lower and upper bounds are no longer piecewiselinear and convex, they can still generalize value functions over the entire feature-based compact
occupancy-state space.
Experimentally, we show that FB-HSVI is able to outperform all current state-of-the-art exact
Dec-POMDP solvers in common benchmark domains. These results show that -optimal solutions
can be found for larger horizons in all problems and for horizons that are sometimes an order of
magnitude larger than those that have previously been solved.

8. Acknowledgements
We would like to thank Matthijs Spaan for providing results for GMAA*-ICE as well as Frans
Oliehoek, Akshat Kumar and the anonymous reviewers for their helpful comments. Research supported in part by AFOSR MURI project #FA9550-09-1-0538.

Appendix A. Correctness of Feature-Based Compact Bounds
A.1 Proof of Theorem 6
 (). Hence, we obtain successively:
(a) By hypothesis, we have    t , such that v  V M,t

()
v  V M,t

(48)


= V M,t
()
X
def
= max
(s, )  (s, )
t

 max
t

= max
t

( = F () by Theorem 5),

(49)


(by definition of V M,t
()),

(50)

( projected onto  ),

(51)

s,



X X

  (s, )  (s, )  ( s, )
s,



 s, 

X

s,

( s, )  ( s, ),

(52)

 s, 




X  X



 s, ( s, )  ( s, )  (s, )
 max


t

( projected onto ),

(53)

 s,   s, 

def


= V M,t
(F ()),

(54)

which ends the proof of Theorem 6.a.

 ()  h, G ()i for any arbitrary   R| | . To
(b) By hypothesis, we have    t such that V M,t


 ()  h, G (G ())i for any arbitrary feature set , we successively show:
prove V M,t


X
X
X
def
 s, (s, )
 s, ( s, )  ( s, ),
(s, )
h, G (G ())i =
 s, 

 s, 

 s,

(55)





 X

X
X


( s, )  
(s, )
=
 s, ( s, )   s, (s, ) , (Re-ordering). (56)



 s, 

 s, 

 s, 

489

fiDibangoye, Amato, Buffet & Charpillet

Before proceeding any further, we need to prove quantity  s, (s, ) is greater or equal to quantity
 s,   s, ( s, )   s, (s, ) (in the bracket of the last expression above). To this end, we start with
the interpretations of each expression. The first expression asks whether state-history pair (s, )
belongs to cluster along with feature  s, , an affirmative answer results in value  s, (s, ) = 1
otherwise 0. Let  s , be the feature in  whose cluster includes state-history pair (s, ). Then, the
second expression asks whether both state-history pairs ( s ,  ) and (s, ) belong to cluster along
P
with feature  s,   , an affirmative answer will result in value s,   s, ( s, )   s, (s, ) = 1
otherwise 0. Clearly, the second expression is a stricter form of the first expression, hence  s, (s, )
P
P
is greater or equal to s,   s, ( s, )   s, (s, ). Thus, by replacing s,   s, ( s, )   s, (s, )
by  s, (s, ), we obtain:


 X

X
X


(s, )
( s, )  
h, G (G ())i =
 s, ( s, )   s, (s, )
(57)



 s, 
 s, 
 s, 
X
X
(s, )
 s, (s, )  ( s, ),
(58)

P

 s, 

 s, 

def

= h, G ()i

(59)


(),
V M,t

(60)

which ends the proof Theorem 6.b.





A.2 Proof of Theorem 7
The proof proceeds by induction. Heuristic function (U M,t )t{0,1,...,T } , initially upper bounds the
optimal value function, since it is initialized using the underlying MDP value function.
For the induction step, we assume heuristic function (U M,t )t{0,1,...,T } represented using point sets
(t )t{0,1,...,T } upper bounds the optimal value function.
 , after the
Next, we show that, at any arbitrary time step t  {0, 1, . . . , T }, heuristic function U M,t
 over
update of U M,t resulting in upper bound v at occupancy state , is also an upper bound on V M,t
the entire compact occupancy space  t . That is,

    t :



(  ).
(  )  V M,t
U M,t (  )  U M,t

(61)

 (  ). Using the sawtooth interpolation approach3 ,
We first show that     t : U M,t (  )  U M,t
we obtain successively:



def


(  ) = min v , v | (  7 v )  t  { 7 v } ,
U M,t
(see sawtooth definition)
(62)


= min v , U M,t (  ) ,
(63)

 U M,t (  ),

(64)

which proves the first part of expression (Eq. 61).
 (  )  V  (  ). To this end, we distinguish between before and
Now, we show     t : U M,t
M,t
after the update of the U M,t .
3. Here, we adapted the sawtooth interpolation to replace full occupancy states by compact occupancy states.

490

fiOptimally Solving Finite-Horizon Dec-POMDPs

Before the update,

the following holds:


(  ),
(  )  V M,t
U M,t

    t :

(65)

by the inductive hypothesis.
After the update, we obtain two important results. On the one hand, we have that the resulting
value v is an upper bound at :
def

v = max R(, d ) + U M,t+1 ( P(, d )),

(66)


( P(, d )),
 max R(, d ) + V M,t+1

(67)

d  A

d  A

def


().
= V M,t

(68)


On the other hand, we show that from v one can extrapolate an upper bound value v for any other
compact occupancy state   . This is mainly thanks to Theorem 6.a, in which we demonstrate: if
 ()  v holds, then for any arbitrary    
 t , expression V  (F ())  v holds
expression V M,t
M,t
as well. The sawtooth interpolation method concludes this argument as follows:




v = v + (v  v )  D(F (),   ),
def


(  ).
 V M,t

(69)
(70)

In fact, the sawtooth interpolation can always generate an upper-bound for one compact occupancy
state from the upper bound of any compact occupancy state as long as both are expressed into the
same feature set.

A.3 Proof of Theorem 8
The proof proceeds by induction as well. Heuristic function (L M,t )t{0,1,...,T } initially lower bounds
the optimal value function since we initialize it using the value function associated with the worst
separable joint policy. The one that prescribes the agents the joint action that yields the minimum
def
reward, and this over all time steps, i.e., L M,t () = (T  t) min s,a ra (s), for all t  {0, 1, . . . , T }.
For the induction step, we assume heuristic function (L M,t )t{0,1,...,T } represented using compact
vector sets (t )t{0,1,...,T } lower bounds the optimal value function. Next, we show that for any
arbitrary time step t  {0, 1, . . . , T }, heuristic function LM,t , which results from the update of lower
 over
bound L M,t and produces compact vector  along feature set  , is also a lower bound on V M,t
the entire compact occupancy space  t . That is,

(  ).
L M,t (  )  LM,t (  )  V M,t

    t :

(71)

We first show that     t : L M,t (  )  LM,t (  ). In fact,
def

L M,t (  ) = max h  , G ()i,

(72)

t

 max
t { }
def

= LM,t (  ),
491

h  , G ()i,

(73)
(74)

fiDibangoye, Amato, Buffet & Charpillet

which proves the first part.
 (  ). To this end, we distinguish between before and
Now, we show     t : LM,t (  )  V M,t
after the update of L M,t .
Before the update,

by the induction hypothesis, we have:
    t :


LM,t (  )  V M,t
(  ).

(75)

After the update, we obtain:     t ,
def



( P(  , d )),
V M,t
(  ) = max R(  , d ) + V M,t+1

(76)

d  A



= max h  , rd i + max h  pd , G
t+1

d  A

=

h  , rd + G

max

h  , rd + G

d  A,t+1





max



d  A,t+1



max
d  A,t+1

d 
 p 

d 
 p 

()i,

(77)

()  (pd ) i,

(re-arranging terms )

(78)

()  (pd ) i,

(replace t+1 by t+1 )

(79)

(Lemma 6)

(80)

(retain one element).

(81)



h  , G (rd + G

d 
 p 

d
 p 

()  (pd ) ))i,

] , t+1 ))i,
 h  , G (backup(

 (  ), we prove the
Merging together arguments before and after the update, i.e., LM,t (  )  V M,t
second part of the proof. This ends the proof.


Appendix B. Subroutines
This section gives subroutines that are required to compute feature-based compact occupancy states
using either local or truncation probabilistic equivalence relations (Algorithm 3).
Algorithm 3: Compact feature-based occupancy state through LPE.
function Compact-LPE(t)
S  {(s, )  S  t : t (s, ) > 0}
foreach (s, )  S do
S  S\{(s, )} and t (s, )  t (s, )
foreach ( s,  )  S do
if AreStateJointHistoryPairsLPE((s, ), ( s,  ), t ) then
S  S\{( s,  )} and t (s, )  t (s, ) + t ( s,  )
return t
function Compact-TPE(t)
mt  getTruncationParam(t) and S  S  t ()
foreach (s, )  S do
S  S\{(s, )} and t (s, )  t (s, )
foreach ( s,  )  S do
if AreStateJointHistoryPairsTPE((s, ), ( s,  ), mt ) then
S  S\{( s,  )} and t (s, )  t (s, ) + t ( s,  )
return t

492

fiOptimally Solving Finite-Horizon Dec-POMDPs

Algorithm 4: Subroutines for compact feature-based occupancy state through LPE and TPE.
function ArePrivateHistoriesLPE(ti, ti , t )
foreach ti  i
t (t ) and s  S do
if Pr(s, ti |0 , t ) , Pr(s, ti |0 , t ) then return False
return True
function AreStateJointHistoryPairsLPE((s, htiiiI ), ( s, hti iiI ), t )
if s , s then return False
foreach i  I do
if ArePrivateHistoryLPE(ti, ti , t ) then return False
return True
function getTruncationParam(t)
m0
foreach i  I do
foreach ti , ti  it (t ) do
if Suffix(ti, m) = Suffix(ti, m) then
if ArePrivateHistoriesLPE(ti , ti , t ) then m  m + 1;
return m
function ArePrivateHistoriesTPE(ti, ti , mt )
return Suffix(ti, mt ) = Suffix(ti, mt )
function AreStateJointHistoryPairsTPE((s, htiiiI ), ( s, hti iiI ), mt )
if s , s then return False
foreach i  I do
if ArePrivateHistoryLPE(ti, ti , mt ) then return False
return True

References
Amato, C., Bernstein, D. S., & Zilberstein, S. (2010). Optimizing fixed-size stochastic controllers
for POMDPs and decentralized POMDPs. Journal of Autonomous Agents and Multi-Agent
Systems, 21(3), 293320.
Amato, C., Chowdhary, G., Geramifard, A., Ure, N. K., & Kochenderfer, M. J. (2013). Decentralized control of partially observable Markov decision processes. In 54th IEEE Conference on
Decision and Control.
Amato, C., Dibangoye, J. S., & Zilberstein, S. (2009). Incremental policy generation for finitehorizon DEC-POMDPs. In Proceedings of the Nineteenth International Conference on Automated Planning and Scheduling.
Amato, C., Konidaris, G. D., Anders, A., Cruz, G., How, J. P., & Kaelbling, L. P. (2015). Policy
search for multi-robot coordination under uncertainty. In Proceedings of the Robotics: Science
and Systems Conference.
Amato, C., Konidaris, G. D., & Kaelbling, L. P. (2014). Planning with macro-actions in decentralized POMDPs. In Proceedings of the Thirteenth International Conference on Autonomous
Agents and Multiagent Systems.

493

fiDibangoye, Amato, Buffet & Charpillet

Aras, R., & Dutech, A. (2010). An investigation into mathematical programming for finite horizon
decentralized POMDPs. Journal of Artificial Intelligence Research, 37, 329396.
Banerjee, B., Lyle, J., Kraemer, L., & Yellamraju, R. (2012). Sample bounded distributed reinforcement learning for decentralized POMDPs. In Proceedings of the Twenty-Sixth AAAI
Conference on Artificial Intelligence, pp. 12561262, Toronto, Canada.
Barto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning to act using real-time dynamic programming. Artificial Intelligence, 72(1-2), 81138.
Becker, R., Zilberstein, S., Lesser, V. R., & Goldman, C. V. (2004). Solving transition independent
decentralized Markov decision processes. Journal of Artificial Intelligence Research, 22,
423455.
Bellman, R. E. (1957). Dynamic Programming. Dover Publications, Incorporated.
Bernstein, D. S., Givan, R., Immerman, N., & Zilberstein, S. (2002). The complexity of decentralized control of Markov decision processes. Mathematics of Operations Research, 27(4).
Boularias, A., & Chaib-draa, B. (2008). Exact dynamic programming for decentralized POMDPs
with lossless policy compression. In Proceedings of the Eighteenth International Conference
on Automated Planning and Scheduling, pp. 2027.
Canu, A., & Mouaddib, A.-I. (2011). Collective decision under partial observability - a dynamic
local interaction model. In IJCCI (ECTA-FCTA), pp. 146155.
Carlin, A., & Zilberstein, S. (2008). Value-based observation compression for DEC-POMDPs. In
Proceedings of the Seventh International Conference on Autonomous Agents and Multiagent
Systems.
De Farias, D. P., & Van Roy, B. (2003). The linear programming approach to approximate dynamic
programming. Operations Research, 51(6), 850865.
Dibangoye, J. S., Amato, C., Buffet, O., & Charpillet, F. (2014). Exploiting separability in multiagent planning with continuous-state MDPs. In Proceedings of the Thirteenth International
Conference on Autonomous Agents and Multiagent Systems, pp. 12811288.
Dibangoye, J. S., Amato, C., Buffet, O., & Charpillet, F. (2015). Exploiting separability in multiagent planning with continuous-state MDPs (extended abstract). In Proceedings of the TwentyFifth International Joint Conference on Artificial Intelligence, pp. 42544260.
Dibangoye, J. S., Amato, C., & Doniec, A. (2012). Scaling up decentralized MDPs through heuristic
search. In Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence, pp. 217226.
Dibangoye, J. S., Amato, C., Doniec, A., & Charpillet, F. (2013). Producing efficient error-bounded
solutions for transition independent decentralized MDPs. In Proceedings of the Twelfth International Conference on Autonomous Agents and Multiagent Systems, pp. 539546.
Dibangoye, J. S., Buffet, O., & Simonin, O. (2015). Structural results for cooperative decentralized control models. In Proceedings of the Twenty-Fifth International Joint Conference on
Artificial Intelligence, pp. 4652.
Dibangoye, J. S., Mouaddib, A.-I., & Chaib-draa, B. (2009). Point-based incremental pruning
heuristic for solving finite-horizon DEC-POMDPs. In Proceedings of the Eighth International Conference on Autonomous Agents and Multiagent Systems, pp. 569576.
494

fiOptimally Solving Finite-Horizon Dec-POMDPs

Dibangoye, J. S., Mouaddib, A.-I., & Chaib-draa, B. (2011). Toward error-bounded algorithms
for infinite-horizon Dec-POMDPs. In Proceedings of the Tenth International Conference on
Autonomous Agents and Multiagent Systems, pp. 947954.
Dibangoye, J. S., Shani, G., Chaib-Draa, B., & Mouaddib, A.-I. (2009). Topological order planner for POMDPs. In Proceedings of the Twenty-Second International Joint Conference on
Artificial Intelligence, pp. 16841689.
Dibangoye, J. S., Amato, C., Buffet, O., & Charpillet, F. (2013). Optimally solving Dec-POMDPs as
continuous-state MDPs. In Proceedings of the Twenty-Fourth International Joint Conference
on Artificial Intelligence.
Dibangoye, J. S., Buffet, O., & Charpillet, F. (2014). Error-bounded approximations for infinitehorizon discounted decentralized POMDPs. In Proceedings of the Twenty-Fourth European
Conference on Machine Learning, pp. 338353.
Dibangoye, J. S., Chaib-draa, B., & Mouaddib, A.-I. (2008). A novel prioritization technique for
solving Markov decision processes. In Proceedings of the 21th International Conference of
the Florida Artificial Intelligence Research Society, pp. 537542.
Grizzle, J. W., Marcus, S. I., & Hsu, K. (1981). Decentralized control of a multiaccess broadcast
network. In 20th IEEE Conference on Decision and Control including the Symposium on
Adaptive Processes, Vol. 20, pp. 390391.
Hansen, E. A., Bernstein, D. S., & Zilberstein, S. (2004). Dynamic programming for partially observable stochastic games. In Proceedings of the Nineteenth National Conference on Artificial
Intelligence, pp. 709715.
Hansen, E. A., & Zilberstein, S. (2001). LAO*: A heuristic search algorithm that finds solutions
with loops. Artificial Intelligence, 129(1-2), 3562.
Hart, P. E., Nilsson, N. J., & Raphael, B. (1968). A formal basis for the heuristic determination of
minimum cost paths. IEEE Trans. Systems Science and Cybernetics, 4(2), 100107.
Hauskrecht, M. (2000). Value-function approximations for partially observable Markov decision
processes. Journal of Artificial Intelligence Research, 13, 3394.
Howard, R. A. (1960). Dynamic Programming and Markov Processes. The M.I.T. Press.
Jain, M., Taylor, M. E., Tambe, M., & Yokoo, M. (2009). DCOPs meet the real world: Exploring
unknown reward matrices with applications to mobile sensor networks. In Proceedings of the
Twenty-Second International Joint Conference on Artificial Intelligence, pp. 181186.
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning and acting in partially
observable stochastic domains. Artificial Intelligence, 101(1-2), 99134.
Korf, R. E. (1990). Real-time heuristic search. Artificial Intelligence, 42(2-3), 189211.
Kumar, A., & Zilberstein, S. (2009). Constraint-based dynamic programming for decentralized
POMDPs with structured interactions. In Proceedings of the Eighth International Conference
on Autonomous Agents and Multiagent Systems, pp. 561568.
Kumar, A., & Zilberstein, S. (2010). Point-based backup for decentralized POMDPs: complexity
and new algorithms. In Proceedings of the Ninth International Conference on Autonomous
Agents and Multiagent Systems, pp. 13151322.
495

fiDibangoye, Amato, Buffet & Charpillet

Nair, R., Tambe, M., Yokoo, M., Pynadath, D. V., & Marsella, S. (2003). Taming decentralized
POMDPs: Towards efficient policy computation for multiagent settings. In Proceedings of
the Eighteenth International Joint Conference on Artificial Intelligence, pp. 705711.
Nair, R., Varakantham, P., Tambe, M., & Yokoo, M. (2005). Networked distributed POMDPs: A
synthesis of distributed constraint optimization and POMDPs. In Proceedings of the Twentieth
National Conference on Artificial Intelligence, pp. 133139.
Oliehoek, F. A. (2012). Decentralized POMDPs. In Wiering, M., & van Otterlo, M. (Eds.), Reinforcement Learning: State of the Art, Vol. 12, pp. 471503. Springer Berlin Heidelberg,
Berlin, Germany.
Oliehoek, F. A. (2013). Sufficient plan-time statistics for decentralized POMDPs. In Proceedings
of the Twenty-Fourth International Joint Conference on Artificial Intelligence.
Oliehoek, F. A., Spaan, M. T. J., Amato, C., & Whiteson, S. (2013). Incremental clustering and
expansion for faster optimal planning in Dec-POMDPs. Journal of Artificial Intelligence
Research, 46, 449509.
Oliehoek, F. A., Spaan, M. T. J., & Vlassis, N. A. (2008). Optimal and approximate Q-value functions for decentralized POMDPs. Journal of Artificial Intelligence Research, 32, 289353.
Oliehoek, F. A., & Spaan, M. T. (2012). Tree-based solution methods for multiagent POMDPs with
delayed communication. In Proceedings of the Twenty-Sixth AAAI Conference on Artificial
Intelligence.
Oliehoek, F. A., Whiteson, S., & Spaan, M. T. J. (2009). Lossless clustering of histories in decentralized POMDPs. In Proceedings of the Eighth International Conference on Autonomous
Agents and Multiagent Systems, pp. 577584.
Ooi, J. M., & Wornell, G. W. (1996). Decentralized control of a multiple access broadcast channel:
Performance bounds. In Proc. of the 35th IEEE Conference on Decision and Control, Vol. 1,
pp. 293298. IEEE.
Pajarinen, J., Hottinen, A., & Peltonen, J. (2013). Optimizing spatial and temporal reuse in wireless
networks by decentralized partially observable Markov decision processes. IEEE Transactions on Mobile Computing, 13(4). Preprint.
Paquet, S., Chaib-draa, B., Dallaire, P., & Bergeron, D. (2010). Task allocation learning in a multiagent environment: Application to the robocuprescue simulation. Multiagent and Grid Systems, 6(4), 293314.
Pineau, J., Gordon, G. J., & Thrun, S. (2006). Anytime point-based approximations for large
POMDPs. Journal of Artificial Intelligence Research, 27, 335380.
Powell, W. B. (2007). Approximate Dynamic Programming: Solving the Curses of Dimensionality
(Wiley Series in Probability and Statistics). Wiley-Interscience.
Puterman, M. L. (1994). Markov Decision Processes, Discrete Stochastic Dynamic Programming.
Wiley-Interscience, Hoboken, New Jersey.
Roy, N., Gordon, G. J., & Thrun, S. (2005). Finding approximate POMDP solutions through belief
compression. Journal of Artificial Intelligence Research, 23, 140.

496

fiOptimally Solving Finite-Horizon Dec-POMDPs

Seuken, S., & Zilberstein, S. (2007). Improved memory-bounded dynamic programming for DECPOMDPs. In Proceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligence.
Shani, G., Pineau, J., & Kaplow, R. (2013). A survey of point-based POMDP solvers. Journal of
Autonomous Agents and Multi-Agent Systems, 27(1), 151.
Smallwood, R. D., & Sondik, E. J. (1973). The optimal control of partially observable Markov
decision processes over a finite horizon. Operations Research, 21(5), 10711088.
Smith, T. (2007). Probabilistic Planning for Robotic Exploration. Ph.D. thesis, The Robotics
Institute, Carnegie Mellon University, Pittsburgh, PA.
Smith, T., & Simmons, R. (2004). Heuristic search value iteration for POMDPs. In Proceedings of
the Twentieth Conference on Uncertainty in Artificial Intelligence, pp. 520527, Arlington,
Virginia, United States.
Smith, T., & Simmons, R. G. (2006). Focused real-time dynamic programming for MDPs: Squeezing more out of a heuristic. In Proceedings of the Twenty-First AAAI Conference on Artificial
Intelligence, pp. 12271232.
Spaan, M. T. J., Oliehoek, F. A., & Amato, C. (2011). Scaling up optimal heuristic search in DecPOMDPs via incremental expansion. In Proceedings of the Twenty-Third International Joint
Conference on Artificial Intelligence, pp. 20272032.
Szer, D., Charpillet, F., & Zilberstein, S. (2005). MAA*: A heuristic search algorithm for solving
decentralized POMDPs. In Proceedings of the Twenty-First Conference on Uncertainty in
Artificial Intelligence, pp. 568576.
Tsitsiklis, J. N., & van Roy, B. (1996). Feature-based methods for large scale dynamic programming. Machine Learning, 22(1-3), 5994.
Velagapudi, P., Varakantham, P., Sycara, K., & Scerri, P. (2011). Distributed model shaping for
scaling to decentralized POMDPs with hundreds of agents. In Proceedings of the Tenth International Conference on Autonomous Agents and Multiagent Systems, pp. 955962.
Winstein, K., & Balakrishnan, H. (2013). TCP ex Machina: Computer-generated congestion control.
In SIGCOMM, Hong Kong.
Wu, F., Zilberstein, S., & Chen, X. (2010). Point-based policy generation for decentralized
POMDPs. In Proceedings of the Ninth International Conference on Autonomous Agents and
Multiagent Systems, pp. 13071314.
Wu, F., Zilberstein, S., & Chen, X. (2011). Online planning for multi-agent systems with bounded
communication. Artificial Intelligence, 175(2), 487511.
Zilberstein, S., Washington, R., Bernstein, D. S., & Mouaddib, A.-I. (2002). Decision-theoretic
control of planetary rovers. In Revised Papers from the International Seminar on Advances
in Plan-Based Control of Robotic Agents, pp. 270289, London, UK. Springer-Verlag.

497

fiJournal of Artificial Intelligence Research 55 (2016) 603-652

Submitted 07/15; published 03/16

Large-Scale Election Campaigns:
Combinatorial Shift Bribery
Robert Bredereck

robert.bredereck@tu-berlin.de

TU Berlin,
Berlin, Germany

Piotr Faliszewski

faliszew@agh.edu.pl

AGH University of Science and Technology,
Krakow, Poland

Rolf Niedermeier
Nimrod Talmon

rolf.niedermeier@tu-berlin.de
nimrodtalmon77@gmail.com

TU Berlin,
Berlin, Germany

Abstract
We study the complexity of a combinatorial variant of the Shift Bribery problem
in elections. In the standard Shift Bribery problem, we are given an election where
each voter has a preference order over the set of candidates and where an outside agent,
the briber, can pay each voter to rank the bribers favorite candidate a given number of
positions higher. The goal is to ensure the victory of the bribers preferred candidate. The
combinatorial variant of the problem, introduced in this paper, models settings where it is
possible to affect the position of the preferred candidate in multiple votes, either positively
or negatively, with a single bribery action. This variant of the problem is particularly
interesting in the context of large-scale campaign management problems (which, from the
technical side, are modeled as bribery problems). We show that, in general, the combinatorial variant of the problem is highly intractable; specifically, NP-hard, hard in the
parameterized sense, and hard to approximate. Nevertheless, we provide parameterized
algorithms and approximation algorithms for natural restricted cases.

1. Introduction
We study the computational complexity of election campaign management for the case
where campaign actions (such as airing a TV advertisement, launching a web-based campaign, or organizing meetings with voters) may have large-scale effects which affect multiple
voters. Further, we are interested in settings where these actions can have both positive effects (for example, some voters may choose to rank the promoted candidate higher because
they find arguments presented in a given advertisement appealing) as well as negative ones
(for example, because some other voters find the advertisement to be too aggressive). Thus,
in our setting, the two major issues faced by a campaign manager are (a) choosing actions
c
2016
AI Access Foundation. All rights reserved.

fiBredereck, Faliszewski, Niedermeier, & Talmon

that positively affect as many voters as possible and (b) balancing the negative effects of
campaigning actions (for example, by concentrating these negative effects on voters who
disregard the promoted candidate anyway).
Our research falls within the field of computational social choice, a subarea of multiagent
systems. We use the standard election model, where we are given a set C of candidates
and a collection V of voters, each represented by her preference order (that is, a ranking of
the candidates from the most preferred one to the least preferred one). We assume that we
know the preferences of all the voters. While having such perfect knowledge is impossible
in practice, this assumption is a convenient simplification that models the fact that we may
have (approximate) information from preelection polls or some other sources.
We consider two voting rules, the Plurality rule (where we pick the candidate who
is ranked first by most voters) and the Borda rule (where each candidate c gets from
each voter v as many points as there are candidates that v prefers c to, and we pick the
candidate with the most points). These rules are chosen because the Plurality rule is the
most widespread rule in practice and because the Borda rule is very well-studied in the
context of campaign management.
Within computational social choice, the term campaign management (introduced in
Elkind, Faliszewski, & Slinko, 2009; Elkind & Faliszewski, 2010) is an alternative name for
the bribery family of problems (introduced in Faliszewski, Hemaspaandra, & Hemaspaandra, 2009a) for the cases where one focuses on modeling actions available during election
campaigns: As a result of money spent by a campaign manager, some of the voters change
their votes. In this paper we study campaign management through the Shift Bribery
problem (Elkind et al., 2009; Elkind & Faliszewski, 2010; Bredereck, Chen, Faliszewski,
Nichterlein, & Niedermeier, 2014a; Bredereck, Faliszewski, Niedermeier, & Talmon, 2016).
In Shift Bribery we have a candidate p who we want to win, for each voter v we have a
price v (i) for which this voter is willing to shift p forward by i positions in her preference
order1 , and we ask for the lowest cost of ensuring that p is a winner (see Section 1.1 for
references to other campaign management problems).
The Shift Bribery problem has one major drawback as a model for campaign management. It is incapable of capturing large-scale effects of campaign actions. In particular,
if one puts forward a TV spot promoting a given candidate, then some voters will react
positively and rank the candidate higher, some will be oblivious to it, and some will react
negatively, by ranking the candidate lower. Shift Bribery cannot model such correlated
effects. In this paper we introduce and study the Combinatorial Shift Bribery problem, allowing campaign actions to have effects, positive or negative, on whole groups of
voters.
We are interested in understanding how a more realistic model of campaign management
affects the complexity of the problem. Indeed, Shift Bribery is, computationally, a very
well-behaved problem. For example, for the Plurality rule it is solvable in polynomial time
and for the Borda rule it is NP-complete (Elkind et al., 2009), but there is a polynomial-time
2-approximation algorithm (Elkind et al., 2009; Elkind & Faliszewski, 2010) and there are
fixed-parameter (FPT) algorithms, either exact or capable of finding solutions arbitrarily
close to the optimal ones (Bredereck et al., 2014a). In this work, we ask to what extent
1. Of course, this price does not necessarily reflect a direct money transfer to the voter, but rather the cost
of convincing the voter to change his or her mind.

604

fiCombinatorial Shift Bribery

do we retain these good computational properties when we allow large-scale effects. The
results are surprising both positively and negatively:
1. Combinatorial Shift Bribery becomes both NP-complete and W[1]-hard even for
the Plurality rule, even for very restrictive choice of parameters, even if the correlated
effects of particular campaign actions are limited to at most two voters. Moreover,
our hardness results imply that good, general approximation algorithms do not exist
when we allow negative effects of campaign actions.
2. In spite of the above, it is still possible to derive relatively good (approximation)
algorithms, both for the Plurality rule and for the Borda rule, provided that we
restrict the effects of the campaign actions to be only positive and to either only
involve few voters each, or to only involve groups of consecutive voters (with respect
to an ordering over the voters which might correspond, for example, to time).
Our results are summarized in Table 1 in Section 4. With the generality of our problem
and its combinatorial nature it is natural that we obtain many hardness results. Yet, their
extent and strength is surprising, and so is the fact that we also find a nontrivial landscape
of tractable cases.
1.1 Related Work
Our work builds on top of two main research ideas. First, on studying campaign management/bribery problems, and, second, on studying combinatorial variants of election
problems.
The study of the computational complexity of bribery in elections was initiated by
Faliszewski et al. (2009a), and continued by a number of researchers (Faliszewski, Hemaspaandra, Hemaspaandra, & Rothe, 2009b; Hazon, Lin, & Kraus, 2013; Mattei, Goldsmith,
& Klapper, 2012a; Mattei, Pini, Rossi, & Venable, 2012b). Elkind et al. (2009) and Elkind
and Faliszewski (2010) realized that the formalism of election bribery problems is useful
from the point of view of planning election campaigns. In particular, they defined the
Swap Bribery problem and its restricted variant, Shift Bribery. In the former it is possible, at a given price, to swap any two adjacent candidates in a given vote. In the latter,
we are only allowed to shift the preferred candidate forward. Various problems, modeling
different flavors of campaign management, have been studied, including, for example, the
possibility to alter the number of approved/ranked candidates (Baumeister, Faliszewski,
Lang, & Rothe, 2012; Faliszewski, Reisch, Rothe, & Schend, 2014; Schlotter, Faliszewski,
& Elkind, 2011). Different (positive) applications of bribery problems include, for example,
the Margin of Victory problem, where the goal of the briber is to prevent some candidate from winning. If it is possible to do so at low cost, then this suggests that the election
could have been tampered with (Cary, 2011; Magrino, Rivest, Shen, & Wagner, 2011; Xia,
2012; Reisch, Rothe, & Schend, 2014).
From our point of view, the most related works are those of Elkind et al. (2009), Elkind
and Faliszewski (2010), Bredereck et al. (2014a, 2016), and Dorn and Schlotter (2012).
The former ones study Shift Bribery, which we generalize (parameterized complexity of
Shift Bribery is studied in Bredereck et al., 2014a, while Shift Bribery for multiwinner
605

fiBredereck, Faliszewski, Niedermeier, & Talmon

elections are studied in Bredereck et al., 2016), whereas the work of Dorn and Schlotter
(2012) pioneers the use of parameterized complexity analysis for (swap) bribery problems.
Our work is largely inspired by that of Bulteau, Chen, Faliszewski, Niedermeier, and
Talmon (2015) and Chen, Faliszewski, Niedermeier, and Talmon (2015), who introduced and
studied combinatorial variants of election control. Election control is a very well-studied
topic in computational social choice, initiated by Bartholdi, Tovey, and Trick (1992) and
then studied by numerous researchers (we point the readers to Faliszewski, Hemaspaandra,
& Hemaspaandra, 2010; Faliszewski & Rothe, 2015, for a detailed account). Briefly put,
control problems model attempts at changing the election results by changing their structure. The standard types of control include adding, deleting, and partitioning candidates or
voters. Control problems, especially those related to adding and deleting voters, are quite
relevant to the issues of campaign management, and, indeed, in Section 5 we do show a connection between Combinatorial Shift Bribery and (combinatorial) control by adding
voters (Bulteau et al., 2015).
The idea of combinatorial shift bribery is somewhat related to the problem of lobbying
in multiple referenda, as introduced by Christian, Fellows, Rosamond, and Slinko (2007)
(parameterized study was provided in Bredereck, Chen, Hartung, Kratsch, Niedermeier,
Suchy, & Woeginger, 2014b; probabilistic variant was studied, also in the parameterized
sense, in Binkele-Raible, Erdelyi, Fernau, Goldsmith, Mattei, & Rothe, 2014). There, we
have a number of yes/no elections and the goal is to ensure that for each of these election
a majority of the voters vote yes. Each single lobbying action can convince one voter to
vote yes in all the elections. In combinatorial shift bribery we have a single election and
a single action can affect multiple voters, whereas in the lobbying problem we have multiple
elections but each action affects only one voter.
We stress that our use of the term combinatorial variants of election problems is
different than the one used in the well-established line of work regarding combinatorial
candidate spaces (see Lang & Xia, 2015, and further works, for example, Boutilier, Brafman,
Hoos, & Poole, 2004; Conitzer, Lang, & Xia, 2009; Mattei et al., 2012b). In our work we use
the term combinatorial to refer to the combinations of voters affected by each bribery
action.
1.2 Organization of the Paper
After providing some preliminaries in Section 2, we give the formal definition of the Combinatorial Shift Bribery problem in Section 3. In Section 4 we give an overview of our
results. We shed light on some connections between Combinatorial Shift Bribery and
the problem of Combinatorial Control in Section 5. Then, in Section 6, we present
a series of strong hardness results covering all our classes of shift actions for very restrictive sets of parameters (for example, many of our results already apply to the case of two
candidates). In Section 7, we develop several exact algorithms for special cases of Combinatorial Shift Bribery, while in Section 8 we describe our approximation algorithms
for Combinatorial Shift Bribery. Some of our proofs are available in the appendices
(either when a given proof relies on ideas already presented in other proofs, oras in the
case of Theorem 9when the proof is particularly involved). We end with conclusions in
Section 9.
606

fiCombinatorial Shift Bribery

2. Preliminaries
In this section, we briefly describe our model of elections, define the two voting rules that
we study, and review basic concepts from parameterized complexity.
2.1 Elections
An election E = (C, V ) consists of a set C = {c1 , . . . , cm } of candidates and of a collection
V = (v1 , . . . , vn ) of voters. Each voter is represented through her preference order, that is,
a linear ranking of the candidates from the most preferred one to the least preferred one;
we use voters and preference orders interchangeably. For example, if C = {c1 , c2 , c3 }, then
voter v1 may have preference order v1 : c1  c2  c3 to indicate that she likes c1 best, then
c2 , and then c3 (for clarity, we treat the voters as females and the candidates as males).
We assume that there is an arbitrary (but fixed) canonical order over the set of candidates (for example, one could order the candidates lexicographically by their names). For


a subset A  C of candidates, writing A within a preference order means listing the can

didates from A in this canonical order, and writing A means listing them in the reverse of
this order.
2.2 Voting Rules
A voting rule R is a function that, given an election E = (C, V ), outputs a set R(E)  C
of (tied) election winners. Each candidate c  R(E) is said to be an R-winner of the
election E. We consider two election rules, the Plurality rule and the Borda rule. Both
assign points to candidates and output those with the highest score. Under the Plurality
rule, each candidate receives one point for each voter that ranks him first. Under the Borda
rule, each candidate receives i points for each voter that prefers this candidate to exactly i
other ones.
We use the nonunique-winner model. That is, all the candidates selected by a given
voting rule are viewed as equally successful winners (in practice, of course, one has to use
some sort of a tie-breaking rule to resolve the situation, but disregarding ties simplifies the
analysis; however, an interested reader should consult papers on the effects of tie-breaking
on the complexity of election problems, e.g. Obraztsova & Elkind, 2011; Obraztsova, Elkind,
& Hazon, 2011).
2.3 Parameterized Complexity
We assume familiarity with standard notions regarding algorithms and complexity theory,
but briefly review notions regarding parameterized complexity theory (Downey & Fellows,
2013; Flum & Grohe, 2006; Niedermeier, 2006).
In parameterized complexity theory we measure the complexity of a given problem
with respect to both the input size and a particular parameter of the problem. Typical
parameters for election problems include the number of candidates, the number of voters,
and the solution size (for example, the number of campaign actions one can perform; see
Betzler, Bredereck, Chen, & Niedermeier, 2012, for a survey of parameterized complexity
and voting). We say that a parameterized problem is fixed-parameter tractable (is in FPT) if
there is an algorithm that given an input instance I with parameter k solves the problem in
607

fiBredereck, Faliszewski, Niedermeier, & Talmon

g(k)|I|O(1) time, where g is some computable function and |I| is the length of the encoding
of I. There is also a hierarchy of hardness classes for parameterized problems, of which the
two most important levels are formed by the classes W[1] and W[2]. The most convenient
way of defining these classes is through an appropriate reduction notion and their complete
problems. Specifically, we say that a parameterized problem A reduces to a parameterized
problem B if there are two computable functions, h and h0 , with the following properties:
given an instance I of A with parameter k, h(I) outputs in FPT time (i.e., in time g(k)|I|O(1)
for some computable function g) an instance I 0 of B with parameter k 0  h0 (k), such that
I is a yes-instance of A if and only if I 0 is a yes-instance of B. In other words, h is a
many-one reduction from A to B that is allowed to run in FPT time, but that is required to
output an instance whose parameter is upper-bounded by a function of the input instances
parameter.
The class W[1] is defined as the class of problems that parameterically reduce to the
Clique problem, and W[2] as the class of problems that parameterically reduce to the Set
Cover problem, where both problems are parameterized by the solution size (that is, by
the value h from their definitions).
Clique
Input: An undirected graph G = (V (G), E(G)) and an integer h.
Question: Is there a set H of h vertices such that there is an edge between
each pair of vertices from H?
Set Cover
Input: A universe set X, a family S of subsets of X, and an integer h.
Question: Is there a subset S 0  S of at most h subsets whose union gives X?
We sometimes consider special variants of these problems that we describe in detail
within relevant proofs.
A parameterized problem is contained in the class XP if there is an algorithm that, given
an instance I for it with parameter k, solves it in time |I|g(k) , where g is some computable
function. It holds that FPT  W[1]  W[2]  XP. We point the readers interested in
further details regarding parameterized complexity theory (and the design of parameterized
algorithms) to the textbooks of Downey and Fellows (2013), Flum and Grohe (2006), and
Niedermeier (2006).

3. The Combinatorial Shift Bribery Problem
In this section we first define the Combinatorial Shift Bribery problem in its full
generality and, then, we describe why and how we simplify it for the remainder of our
study.
3.1 The Definition
Let R be some voting rule. The definition of R-Combinatorial Shift Bribery is somewhat involved, therefore we first define some necessary components. We are given an election
E = (C, V ) and a preferred candidate p  C. The goal is to ensure that p is an R-winner
of the election. To this end, we have a number of possible actions to choose from.
608

fiCombinatorial Shift Bribery

Let m := |C| be the number of candidates in E and let n := |V | be the number
of voters. A shift action f is an n-dimensional vector of (possibly negative) integers,
f = (f (1) , . . . , f (n) ). In R-Combinatorial Shift Bribery we are given a family F =
(f1 , . . . , f ) of shift actions. Each particular shift action models a possible campaigning
action, such as airing a TV spot or organizing a meeting with the voters. The components
of a given shift action measure the effects of this action on the particular voters. For a given
subset F 0  F of available shift actions, we define the effect of F 0 on voter vi (1  i  n) as
P
(i)
E (i) (F 0 ) = fj F 0 fj . Further, each shift action fj (1  j  ) comes with a nonnegative
integer cost w(fj ) for its application.
Each voter vi (1  i  n) has her individual threshold function i : Z  Z describing
how shift actions affect this voter. We require that i (0) = 0 and that i is nondecreasing.
Let F 0 be a collection of shift actions. After applying the shift actions from F 0 , each voter vi
(1  i  n) shifts the preferred candidate p by t > 0 positions forward if (a) E (i) (F 0 ) > 0,
and (b) i (t)  E (i) (F 0 ) < i (t + 1). The shift is by t > 0 positions back if (a) E (i) (F 0 ) < 0,
and (b) i (t)  E (i) (F 0 ) > i (t  1).
Finally, we are given a nonnegative integer B, the budget.PWe ask for the existence of
a collection F 0  F of available shift actions with total cost fj F 0 w(fj ) at most B and
such that after applying them p is an R-winner of the given election. If this is the case,
then we say that F 0 is successful. Consider the following example.
Example 1. Consider the election below, where the set of candidates is C = {a, b, c, p},
the collection of voters is V = (v1 , v2 , v3 ), and p is the preferred candidate. There are three
available shift actions, each with the same unit cost (i.e., w(f1 ) = w(f2 ) = w(f3 ) = 1).
election
v1 : c  b  p  a
v2 : b  a  c  p
v3 : p  a  b  c

shift actions
     
2
6
0
     
4  0  2
     
0
3
0
f1

f2

f3

The threshold functions are such that:
1. 1 (1) = 4, 1 (0) = 0, 1 (1) = 6, 1 (2) = 100.
2. 2 (0) = 0, 2 (1) = 2, 2 (2) = 2 (3) = 100.
3. 3 (3) = 3 (2) = 100, 3 (1) = 3, 3 (0) = 0.
We use the Borda rule. Candidates a, b, c, and p have, respectively, 4, 6, 4, and 4 points.
It is easy to see that applying any single shift action does not ensure ps victory. However,
applying shift actions F 0 = {f2 , f3 } results in p being a winner. The total effect of these
two shift actions is (6, 2, 3). According to the threshold functions, this means that p is
shifted forward by one position in v1 and v2 , and is shifted back by one position in v3 . After
these shifts, the modified election looks as follows:
609

fiBredereck, Faliszewski, Niedermeier, & Talmon

v10
v20
v30

election
:cpba
:bapc
:apbc

That is, after we apply the shift actions F 0 = {f2 , f3 }, we have that candidate c has
3 points, while all other candidates have 5 points each. Thus, a, b, and p are tied as winners
and F 0 is indeed a successful set of shift actions.
4
Formally, given a voting rule R, we define the R-Combinatorial Shift Bribery
problem as follows:
R-Combinatorial Shift Bribery
Input: An election E = (C, V ), where C = {c1 , . . . , cm } is the set of candidates
and V = (v1 , . . . , vn ) is the collection of voters, a set F = {f1 , . . . , f } of shift
actions with costs w(f1 ), . . . , w(f ), threshold functions 1 , . . . , n , and a nonnegative integer budget B. One of the candidates is designated as the preferred
candidate p.
Question: Is there a subset F 0  F of shift actions with total cost at most B
such that after we apply the shift actions from F 0 candidate p is an R-winner
of the resulting election?
While this definition is quite complicated, it captures some important features of campaigning. For example, the use of threshold functions allows us to model voters who are
unwilling to change the position of the preferred candidate beyond a certain range, irrespective of the strength of the campaign. The fact that different shift actions have different
costs models the fact that particular actions (for example, airing TV spots or organizing
meetings) may come at different costs.
3.2 Relation to Standard Shift Bribery
It is necessary to comment on the relation between our Combinatorial Shift Bribery
problem and its non-combinatorial variant, Shift Bribery (Elkind et al., 2009; Elkind &
Faliszewski, 2010).
The non-combinatorial variant of the Shift Bribery problem is defined very similarly
to the combinatorial one, but the voters have no threshold functions and instead of the
collection of shift actions and their costs, each voter vi has his or her shift-bribery price
function i . The cost of shifting the preferred candidate forward by t positions in vi s
preference order is i (t) (only forward shifts are allowed). We require that i (0) = 0 and
that the functions are nondecreasing. Formally, we have the following definition (R is a
voting rule).
R-Shift Bribery
Input: An election E = (C, V ), where C = {c1 , . . . , cm } is the set of candidates and V = (v1 , . . . , vn ) is the collection of voters, a collection (1 , . . . , n ) of
shift-bribery price functions, and a nonnegative integer budget B. One of the
610

fiCombinatorial Shift Bribery

candidates is designated as the preferred candidate p.
Question:
Is there a vector (s1 , . . . , sn ) of natural numbers such that (a)
Pn
i=1 i (si )  B and (b) if for each voter vi we shift p forward by si positions,
then p is an R-winner of the resulting election?
While intuitively it seems that R-Shift Bribery is simpler than its combinatorial
cousin, making this observation formal requires some care.
Proposition 1. Let R be a voting rule. It holds that R-Shift Bribery many-one reduces
to R-Combinatorial Shift Bribery in polynomial time.
Proof. Consider an instance of R-Shift Bribery with an election E = (C, V ), where
C = {c1 , . . . , cm } and V = (v1 , . . . , vn ), with a collection of shift-bribery price functions
(v1 , . . . , vn ), and with budget B. Without loss of generality, we take c1 to be the preferred
candidate and denote him or her as p. We form an instance of R-Combinatorial Shift
Bribery with the same election, the same budget, and the same preferred candidate,
but where the shift actions, their costs, and voters threshold functions are constructed
as
Pt
mj
,
follows: for each voter vi , we set his or her threshold function to be i (t) = j=1 2
and for each number t of positions by which it is possible to shift the preferred candidate
forward in vi s preference order, we create a shift action fi,t that has a zero effect on all
voters but vi , on whom it has effect 2mt ; the cost
P of fi,t is w(fi,t ) = (t)  (t  1).
If there is a sequence (s1 , . . . , sn ) such that ni=1 i (si )  B and p is an R-winner of
the election where for each voter vi we shift p forward by si positions, then there is also a
solution for our constructed instance of Combinatorial Shift Bribery: if for each vi ,
we use shift actions fi,1 , . . . , fi,si , then the total bribery cost is the same as in the Shift
Bribery instance and, after implementing the shifts, for each vi the preferred candidate is
shifted by exactly si positions.
Now assume that our constructed Combinatorial Shift Bribery instance is a yesinstance. Consider some subset F 0 of shift actions whose total cost is at most B and which
ensure that p is the R-winner of the election (and recall that each shift action can be used
at most once). For each voter vi  V , we define si to be the largest integer such that shift
actions fi,1 , . . . , fi,si all belong to F 0 . Let us fix some voter vi . We claim that after applying
the shift actions from F 0 (in the Combinatorial Shift Bribery instance), the preferred
candidate is shifted forward by exactly si positions. By the definition of si , it is immediate
that he or she is shifted forward by at least si positions. He or she is not shifted forward
by more positions
the following reason: the shift actions fi,1 , . . . , fi,si have total effect
P i for mj
on vi equal to sj=1
2
, which is equal to i (si ). By definition, shift action fi,si +1 is not
in F 0 . The sum of all the remaining shift actions that have an effect on vi is smaller than:
m
X
j=si +2

mj

2

=

ms
i 2
X

2j = 2msi 1  1.

j=0

However, i (si + 1)  i (si ) = 2msi 1 . This means that even if we used all the shift actions
aside from fi,si +1 , in vi s preference order we still would shift p by exactly si positions.
In conclusion, this means that implementing the shift actions F 0 ensures that for each
voter vi we shift p forward by exactly si positions. Further, for each vi we have that
611

fiBredereck, Faliszewski, Niedermeier, & Talmon

w(fi,1 ) +    + w(fi,si ) = i (si ). Therefore, the sequence (s1 , . . . , sn ) witnesses that the
input instance of Shift Bribery is a yes-instance because the total cost of the shifts is
at most B (as in the combinatorial instance) and they ensure that p is a winner (as in the
combinatorial instance).
Since the reduction clearly runs in polynomial time, the proof is complete.
The construction from the above proof is somewhat involved, especially if one takes into
account that it simply shows that our Combinatorial Shift Bribery problem indeed
generalizes the much simpler, non-combinatorial, one. Nonetheless, its somewhat contrived
use of threshold functions seems to be necessary. Indeed, if in the Combinatorial Shift
Bribery problem we restricted the shift actions to have positive entries for exactly one
voter each, and we used simple linear threshold functions, then we would obtain Shift
Bribery for the case of convex price functions (Bredereck et al., 2014a). This is a very
general variant of the Shift Bribery problem for which, for example, all the NP-hardness
results of Elkind et al. (2009) hold (as shown in Bredereck et al., 2014a), but nonetheless
not the most general one.
3.3 A General Hardness Result
It turns out that the Combinatorial Shift Bribery problem, as defined in Section 3.1
above, is so general that it allows for the following, sweeping, hardness result.2
Theorem 2. For both the Plurality rule and the Borda rule, Combinatorial Shift
Bribery is NP-hard even for five voters and two candidates and no budget constraints.
For the Borda rule, Combinatorial Shift Bribery is NP-hard also for three voters and
four candidates.
Proof. We reduce from the following (weakly NP-hard) variant of the Subset Sum problem
(it is a simple exercise to show its NP-hardness through a reduction from the classic Subset
Sum problem):
Subset Sum (Zero Variant)
Input: A set A := {a1 , . . . , an } of integers.
P
Question: Is there a nonempty set A0  A such that ai A0 ai = 0?
Given an instance A = {a1 , . . . , an } of Subset Sum (Zero Variant), we construct
an instance of Plurality-Combinatorial Shift Bribery with two candidates. Since the
Plurality rule and the Borda rule coincide for elections with two candidates, our hardness
result transfers to Borda-Combinatorial Shift Bribery (and, in fact, to almost all
natural voting rules).
We construct the following election:
2. Note, however, that we prove weak NP-hardness. That is, our result may not hold if we assume that all
occurring numbers are encoded in unary. On the contrary, all other hardness proofs in this paper give
strong hardness results and are independent of such number encoding issues.

612

fiCombinatorial Shift Bribery

election
v1 : p  d
v2 : p  d
v3 : d  p
v4 : d  p
v5 : d  p

shift actions



a1
an




a1 
an 








 1  ...  1 




 0 
 0 




0
0


f1

...

fn

That is, for each element ai  A, the set F of shift actions contains one shift action fi
with effect ai on v1 , effect ai on v2 , effect 1 on v3 , and no effect on the other two voters.
The voter threshold functions are as follows. Candidate p is shifted to the last position
for v1 and v2 if the effect on these voters is negative (that is, 1 (1) = 2 (1) = 1).
Candidate p is shifted to the top position for the third voter if the effect is positive (that
is, 3 (1) = 1). We set the cost of each shift action to be one and we set our budget to be
n. Thus the budget allows us to pick any combination of the shift actions.
For the if direction, let A0  A be a non-empty subset whose element-wise sum equals
zero. After applying F 0 := {fi | ai  A0 }, p is a winner: Since A0 sums up to zero, there
is no effect on the first two voters. The effect on the third voter is positive, because A0 is
non-empty. Thus p is preferred by three of the five voters and wins the election.
For the only if direction, let F 0  F be a subset of shift actions that makes p a winner.
Then, F 0 must be non-empty because p does not win the initial election. We claim that
the element-wise
sum of A0 := {ai | fi  F 0 } is zero. For the sake of contradiction, assume
P
that ai A0 ai 6= 0. If the sum were negative, then there would be a negative effect on the
first voter, d would be preferred by three voters out of five, and d would win the election.
If the sum were positive, then we would have the same effect with the second voter taking
the role of the first one.
Using a very similar idea, we can show how to reduce Subset Sum (Zero Variant)
to Borda-Combinatorial Shift Bribery with three voters and four candidates. Given
the input as before, we construct the following instance:
election
v1 : p  d1  d2  d3
v2 : p  d1  d2  d3
v3 : d1  d2  d3  p

shift actions



3a1
3an




3a1  . . . 3an 




3
3


f1

...

fn

That is, for each element ai  A, F contains one shift action fi with effect 3ai on v1 ,
effect 3ai on v2 , and effect 3 on v3 . Each voter vi has the same threshold function
i (t) = t. In effect, p is shifted to the last position of the first and of the second voter if the
effect on these voters is negative, and is shifted to the top position of the third vote if the
effect there is positive. Each shift action has the same unit cost, and we set the budget to
n (i.e., we can pick any combination of the shift actions).
613

fiBredereck, Faliszewski, Niedermeier, & Talmon

Observe that d1 is the original winner of the election and obtains seven points whereas
p obtains only six points.
For the if direction, let A0  A be a non-empty subset whose element-wise sum equals
zero. If we apply shift actions F 0 := {fi | ai  A0 } then p becomes a winner: Since A0 sums
up to zero, there is no effect on the first two voters. The effect on the third voter is positive
because A0 is non-empty. Thus, p is the most preferred candidate for all the voters and
wins the election.
For the only if direction, let F 0  F be a subset of shift actions that makes p a winner.
Then, F 0 must be non-empty because p does not win the initial election. We show that
the element-wise
sum of A0 := {ai | fi  F 0 } is zero. For the sake of contradiction assume
P
that ai A0 ai 6= 0. If the sum were negative, then there would be a negative effect on
the first voter and p would obtain six points, whereas d1 would obtain seven. If the sum
were positive, we would have the same effect with the roles of the first and the second voter
switched.
Effectively, Theorem 2 shows that studying large-scale effects of campaign actions
through the full-fledged R-Combinatorial Shift Bribery problem leads to a hopelessly
intractable problem: We have hardness even for elections with both a fixed number of
candidates and a fixed number of voters.
3.4 Restricted Variants of Combinatorial Shift Bribery
Given the hardness results from Theorem 2, throughout the remainder of the paper we
focus on restricted variants of the Combinatorial Shift Bribery problem. We assume
the individual threshold functions to be the identity functions (that is, for each voter i and
each integer t, it holds that i (t) = t), we assume each shift action to have the same unit
cost, and we consider restricted types of shift actions. All these assumptions require some
additional discussion.
The restrictions on the threshold functions and on the costs of shift actions seem to
be very basic and, in fact, are even satisfied by the instances built in the proof of Theorem 2. The reason for assuming them is that, on the one hand, it seems beyond point to
study instances more involved than those from Theorem 2, and, on the other hand, they
interact with other restrictions, leading to tractable cases. But, they do have important
consequences.
First, using identity threshold functions means that we model societies that are prone to
propaganda. With identity threshold functions we cannot differentiate between voters that
are more or less responsive to our actions. Second, assuming that every shift action has the
same unit cost models settings where the costs of particular campaign actions are similar
enough that small differences between them are irrelevant; the actual number of actions
we choose to perform is a sufficiently good approximation of the real cost. This is true,
for example, for the case of organizing meetings with voters, which often have comparable
prices. It is also likely to be the case when shift actions model actions such as airing TV
spots: Each spot has a similar cost to produce/broadcast. The greatest disadvantage of
assuming unit costs is that we no longer can model mixed campaigns that use actions of
several different types (meetings with voters, TV spots, web campaigns, etc.).
614

fiCombinatorial Shift Bribery

The restrictions on the types of allowed shift actions have even greater impact on the
nature of campaigns that we study. We study the following classes of shift actions:
Unrestricted Shift Actions. Here we put no restrictions on the allowed shift actions;
this models the most general (and, naturally, the least tractable) setting.
Bounded-Effect Shift Actions. Here we consider a parameter  and require that for
each shift action f = (f (1) , . . . , f (n) ) it holds that for each j (1  j  n), we have
|f (j) |  . This is still a very general setting, where we assume that each campaigning
action has only a limited impact on each voter.
Unit-Effect Shift Actions. This is a class of bounded-effect shift actions for  = 1. For
each given voter, applying a given shift action can either leave the preferred candidate
p unaffected or it can shift p one position up or down.
Interval Shift Actions. This is a subclass of unit-effect shift actions that never affect
voters negatively, and where for each shift action there is an interval of voters that
are affected positively (the interval is with respect to the order of the voters in the
input collection V ). This class of shift actions models campaigns associated with a
time window where certain voters can be reached, or campaigns that are local to given
neighborhoods3 (for example, that include putting up multiple posters, organizing
meetings, etc.). We speak of 1z -interval shift actions to mean interval shift actions
where each shift action affects at most z voters.
Unit-Effect on Two Voters Shift Actions. This is a subclass of unit-effect shift actions
that affect two voters at most. We focus on shift actions that affect both voters
positively, denoted as (+1, +1)-shift actions, and that affect one voter positively and
one voter negatively, denoted as (+1, 1)-shift actions. The reason for studying these
families is not because they model particularly natural types of election campaigns,
but rather to establish the limits of tractability for our problem. For example, we
consider (+1, 1)-shift actions to understand how intractable are shift actions that
have negative effects; (+1, 1)-shift actions are the simplest shift actions of this type
that may be useful in the campaign (one would never deliberately use a shift action
that only affects the preferred candidate negatively).

Figure 1 presents the difference between bounded-effect shift actions, unit-effect shift
actions, unit-effect on two voters shift actions, and interval shift actions graphically. As
we discuss in the next section, the type of allowed shift actions has a huge impact on the
computational complexity of our problem.
3. In the neighborhood scenario, we take the simplified view that a society of the voters lives on a line.
Of course, it would be more natural to take two-dimensional neighborhoods into account. We view this
as an interesting direction for future research, but for the time being we consider as simple settings as
possible. In the time window scenario, a natural ordering of the voters is the point of time when they
cast their votes or can be affected by the campaign.

615

fiBredereck, Faliszewski, Niedermeier, & Talmon

2

2
1

1

1
2

1
1

1

1

1

1

1

 = 2;  = 5
1

1

1

2

Unit-Effect
1

1

1
z
1
1

(+1, 1)

1
1

(+1, +1)

1

1z

Figure 1: Restrictions on the shift actions. We visualize (from left to right, top to bottom):
a shift action with maximum effect  = 2 of a single shift action and maximum number
 = 5 of voters affected by a single shift action; a unit-effect shift action; a shift action with
effect of +1 on one voter and effect of 1 on another voter (+1, 1); a shift action with
effect of +1 on two voters (+1, +1); and a shift action with effect of +1 on an interval
of size z 1z . The intended interpretation is that voters are listed vertically, from top to
bottom.

4. Overview of Results
We now provide a high-level overview of our results. It turns out that even with rather strong
restrictions in place (that is, the restrictions defined in Section 3.4), Combinatorial Shift
Bribery is computationally hard in most settings. What we present here is our quest for
understanding the border between tractability and intractability of Combinatorial Shift
Bribery. To this end, we employ the following techniques and ideas.
1. We seek both regular complexity results (NP-hardness results) and parameterized
complexity results (FPT algorithms, W[1]-hardness and W[2]-hardness results, and
XP algorithms).
2. We consider structural restrictions on the sets of available shift actions.
3. We seek approximation algorithms and inapproximability results (that is, approximation hardness results).
616

fiCombinatorial Shift Bribery

For our parameterized complexity results, we consider the following parameters: (a) the
number n of the voters, (b) the number m of the candidates, (c) the budget B, (d) the
maximum effect  of a single shift action, and (e) the maximum number  of voters affected
by a single shift action.
All our discussions of (in)approximability of Combinatorial Shift Bribery regard
the task of minimizing the cost of ensuring the preferred candidates victory. This means
that, for example, a 2-approximation algorithm has to decide if it is possible to ensure the
preferred candidates victory at all, and, if so, it has to output a successful set of shift
actions with total cost at most twice as high as the optimal one.
We summarize our results in Table 1. These results show that Combinatorial Shift
Bribery is highly intractable. Theorems 5, 6, and 7, show that the problem is computationally hard (in terms of NP-hardness, W[2]-hardness, and inapproximability even by FPT
algorithms) for both the Plurality rule and the Borda rule, even for various very restricted
forms of unit-effect shift actions, even for two candidates. This means that, in essence, the
problem is hard for all natural voting rules, since for two candidates all natural voting rules
boil down to the Plurality rule.
Further, Theorem 8 and Theorem 11 show that our problems are W[1]-hard even if we
take the number of candidates and the budget as a joint parameter, even for extremely
restricted shift actions. The problem remains hard (for the case of the Borda rule) when
parameterized by the number of voters (Theorem 9). On the contrary, for the case of
Plurality and parameterization by the number of voters we obtain tractability.
We obtain several approximability results. In essence, these results are possible only for
the cases where shift actions do not have negative results. An intuitive reason for this fact
is that when shift actions have negative effects, then it is computationally hard to check
whether the preferred candidate can win even without any restrictions on the budget.
All our approximation algorithms are based on the results for the non-combinatorial
variant of the problem, due to Elkind et al. (2009) and Elkind and Faliszewski (2010).
Either we use the non-combinatorial algorithms directly, as subroutines in our algorithms,
or we derive our results by plugging our Combinatorial Shift Bribery-specific blocks
into the framework developed by Elkind et al. (2009) and Elkind and Faliszewski (2010).

5. Connection to Combinatorial Control
The study of combinatorial variants of problems modeling ways of affecting election results
was initiated by Bulteau et al. (2015), who considered combinatorial control by adding voters
(Combinatorial-CCAV) for the Plurality rule and for the Condorcet rule. It turns out
that for the Plurality rule we can reduce the problem of (Combinatorial) CCAV to that of
(Combinatorial) Shift Bribery. For the non-combinatorial variants of these problems
this does not give much since both problems are easily seen to be polynomial-time solvable.
However, there are strong hardness results for Plurality-Combinatorial-CCAV which we
can transfer to the case of Plurality-Combinatorial Shift Bribery. Formally, PluralityCombinatorial-CCAV is defined as follows (Bulteau et al., 2015).
Plurality-Combinatorial-CCAV
Input: A set C of candidates with a preferred candidate p  C, a collection V
617

fiBredereck, Faliszewski, Niedermeier, & Talmon

Table 1: Overview of our results. We show exact algorithms and approximation algorithms for Plurality-Combinatorial Shift Bribery and for Borda-Combinatorial
Shift Bribery, for different restrictions on the shift actions (see Figure 1). Results marked
by O follow from the work of Elkind et al. (2009), by  follow from the work of Bredereck
et al. (2014a), by  follow from the work of Elkind and Faliszewski (2010), and by  follow
from the work of Bredereck et al. (2016). Note that all of the variants are in XP when
parameterized by the budget B (Observation 1).
shift actions

regular
Shift Bribery
(convex prices)

rule

exact complexity

approximability

Plurality

poly.-time solvable (O)



Borda

NP-complete(O),
in FPT for B (),
W[1]-hard for n ()

unit effect

2-approximable in
poly. time (,O),
FPT-approximation
scheme for n ()

W[2]-h for B even

inapproximable even in

if m = 2 (Thm. 5),

FPT-time for B even

XP for n (Prop. 12)

if m = 2 (Thm. 6)

Plurality

FPT for n (Thm. 13)



Borda

W[1]-hard for n (Thm. 9)

Both

inapproximable even in
FPT-time for n (Cor. 10)

(+1, 1)
NP-h even if m = 2 (Thm. 7),
Both

W[1]-h for B and
m combined (Thm. 8)

Plurality
(+1, +1)

1z -intervals

Both

inapproximable
even if m = 2 (Thm. 7)

FPT for n (Thm 13)



W[1]-h for B and

2-approximable

m combined (Thm. 8)

in poly. time (Thm. 15)

Plurality

FPT for n (Thm. 13)

Borda



Both

W[1]-h for B (Thm. 11)

618

z-approximable in
poly. time (Thm. 14)
2z-approximable in
poly. time (Thm. 14)
2-approximable in
mz time (Thm. 16)

fiCombinatorial Shift Bribery

of registered voters (having preference orders over C), a collection W of unregistered voters (having preference orders over C), a bundling function  : W  2W
(for each w  W it holds that w  (w)), and a budget k.
Question: Is there a collection W 0  W
k voters such that p is a
S of at most
0
winner of the modified election (C, V  w0 W 0 (w ))?
Intuitively, for each unregistered voter w  W , we have her bundle, (w) (given explicitly
in the input), such that when we add w to the election (for example, by somehow convincing
her to vote), all the voters in her bundle also join the election (for example, people choose
to vote under an influence of a friend).
Theorem 3. Plurality-Combinatorial-CCAV is polynomial-time many-one reducible to
Plurality-Combinatorial Shift Bribery. For an instance of Plurality-CombinatorialCCAV with m candidates, the reduction outputs an instance of Plurality-Combinatorial
Shift Bribery with m + 1 candidates.
Proof. Consider an input instance of Plurality-Combinatorial-CCAV with candidate set
C, collection of registered voters V , collection of unregistered voters W , bundling function
, preferred candidate p  C, and limit k on the number of voters that we can add. We
form an instance of Plurality-Combinatorial Shift Bribery, as follows.
We form a candidate set C 0 = C  {d}, where d is some new candidate. We form the
set of voters V 0 in the following way.
1. For each voter v  V , we include v in V 0 , with the preference orders extended to rank
d last.
2. For each voter w  W that ranks p first, we include in V 0 two voters, xw , with
preference order of the form d  p     , and x0w , with preference order of the form
p  d  .
3. For each voter w  W that ranks some candidate c  C \ {p} first, we include in
V 0 voter xw with preference order p  c     , and voter x0w with preference order
d  p  .
4. We include 4|W ||C| voters in V 0 with preference orders such that we will achieve the
following effects: (a) for each c  C with score s(c) in election (C, V ), c is ranked first
by 4|W | + s(c) voters in V 0 , and (b) d is ranked first by exactly 2|W | voters in V 0 . To
achieve these effects, for each c  C \ {p} we include 4|W | voters that rank c first, we
include 3|W | voters that rank p first, and we include |W | voters that rank d first.
For each voter w  W , we introduce a shift action fw with the following effects: for each
 (w), if w0 ranks p first then fw has effect 1 on xw0 (but not on x0w0 ) and if w0 ranks
some candidate in C \ {p} first, then fw has effect 1 on xw0 and effect +1 on x0w0 (all other
entries are zeros). This finishes the construction. We provide the proof of correctness after
the following example of the reduction.

w0

Example 2. Consider the following input to Plurality-Combinatorial-CCAV, where the
preferred candidate is p and the budget k is 1.
619

fiBredereck, Faliszewski, Niedermeier, & Talmon

registered voters
v1 : p  a

unregistered voters
w1 : p  a

bundling function
(w1 ) = {w1 , w3 }

v2 : a  p

w2 : a  p

(w2 ) = {w2 }

v3 : a  p

w3 : p  a

(w3 ) = {w2 , w3 }

We construct the following input to Plurality-Combinatorial Shift Bribery; notice
that the number of entries in each shift action is 33.
election
v1 : p  a  d
v2 : a  p  d
v3 : a  p  d
xw1 : d  p  a
x0w1 : p  d  a
xw2 : p  a  d
x0w2 : d  p  a
xw3 : d  p  a
x0w3 : p  d  a
12 dummies : a    
9 dummies : p    
3 dummies : d    

shift actions
     
0
0
0
     
0  0   0 
     
0  0   0 
     
     
1  0   0 
     
     
0  0   0 
     
0 1 1
     
     
0  1   1 
     
     
1  0   1 
     
0  0   0 
     
     
0  0   0 
     
     
0  0   0 
0
0
0
fw1

fw2

fw3

Note that adding voter w1 to the input election for Plurality-Combinatorial-CCAV
results in p being a winner of the election. Correspondingly, applying shift action fw1 results
in p being a winner of the input election for Plurality-Combinatorial Shift Bribery. 4
To see the correctness of our construction, note that applying a shift action corresponding to a bundle of a voter w  W has the same effect on the differences between the scores
of the candidates in C as adding the bundle (w) has in the original control instance.
More specifically, disregarding the score of d for now, we have the following. For each
w0  (w) which ranks p first, we have an increase of the score of p by one, while for each
w0  (w) which ranks some candidate c  C \ {p} first, we have an increase of the score of
c by one. Further, the score of candidate d can never grow beyond 4|W | in our PluralityCombinatorial Shift Bribery instance and the score of p can never fall below 4|W |.
Therefore, d can never prevent p from being a winner.
Thus, the reduction is correct. Furthermore, the reduction can be computed in polynomial time and it outputs a Plurality-Combinatorial Shift Bribery instance with one
candidate more than the input Plurality-Combinatorial-CCAV instance. We also observe that the output instance uses unit-effect shift actions that affect at most twice as
many voters as the largest bundle in the input instance.
620

fiCombinatorial Shift Bribery

Based on the proof of Theorem 3 and results of Bulteau et al. (2015), we obtain the
following result.
Corollary 4. Plurality-Combinatorial Shift Bribery is W [2]-hard with respect to the
budget B even if m = 3, it is W [1]-hard with respect to B even for shift actions with unit
effect on up to 6 voters, and it is NP-hard even for shift actions with unit effects on up to
4 voters.
Proof. The result follows by applying the reduction from the proof of Theorem 3 to the
Plurality-Combinatorial-CCAV instances produced in the reductions from Theorems 2, 1,
and 4 of Bulteau et al. (2015), respectively.

6. Hardness Results
The results from the previous section show that we are bound to hit hard instances for
Combinatorial Shift Bribery even in very restricted setting. In this section we explore
how restrictive these hard settings are. Our results are organized by the type of shift actions
allowed.
6.1 Results for General Unit-Effect Shift Actions
We start by considering unit-effect shift actions. If the allowed effects are positive only,
then we obtain NP-hardness and W[2]-hardness when parameterizing by the budget B. If
we allow also negative unit-effects, then the problem gets even harder and we go beyond any
hope for an approximation algorithm, even if the approximation algorithm were allowed to
run in FPT time when parameterizing by the budget B. Quite strikingly, these results hold
even if we only have two candidates.
Theorem 5. For both the Plurality rule and the Borda rule, Combinatorial Shift
Bribery is NP-hard and W[2]-hard for the parameter budget B, even for two candidates
and even if each shift action has effects of either +1 or 0 on each voter.
Proof. We provide a parameterized reduction from Set Cover (recall Section 2.3). Let
(S, X, h) be an instance of Set Cover, where S = {S1 , . . . , Sm } is a family of subsets over
the universe X = {x1 , . . . , xn }, and h is the number of sets that we can use to cover X.
We construct an instance of Plurality-Combinatorial Shift Bribery with two candidates. Note that, since the Borda rule and the Plurality rule coincide on elections with two
candidates, our hardness result transfers to Borda-Combinatorial Shift Bribery.
The construction is as follows. We have p and d as the only candidates. For each
element xi  X create an element voter vi with preference order d  p. Create another set
of n dummy voters all with preference order d  p. The set F of shift actions contains for
each set Sj  S a function fj having an effect of +1 on the element voters corresponding
to the elements of the set (that is, fj [i] = 1 if xi  Sj and fj [i] = 0 otherwise). Finally,
set B := h. This finishes the construction. Clearly, the reduction can be computed in
polynomial time. Consider the following example of applying the reduction.
Example 3. Let the input to Set Cover be such that X = {x1 , x2 , x3 , x4 , x5 } and S =
{S1 , S2 , S3 }, with S1 = {1, 2, 5}, S2 = {2, 3}, S3 = {3, 4}, and h = 2. We construct the
following input for Plurality-Combinatorial Shift Bribery.
621

fiBredereck, Faliszewski, Niedermeier, & Talmon

election
v1 : d  p
v2 : d  p
v3 : d  p
v4 : d  p
v5 : d  p
5 dummies : d  p

shift actions
     
0
0
1
     
1 1 0
     
     
0 1 1
     
     
0 0 1
     
1 0 0
     
0

0

0

f1

f2

f3

Note that {S1 , S3 } is a set cover, and, analogously, choosing f1 and f3 results in p being
a winner of the election.
4
It remains to show that there is a set cover of size h if and only if there is a successful
set of shift actions of size h.
For the if part, assume that there is a set cover S 0 of size at most h. Then, applying
0
F = {fj | Sj  S 0 } makes p win the election: Since S 0 is a set cover, p will be the preferred
candidate of all n element voters and, hence, a winner of the election.
For the only if part, assume that there is a set of shift actions F 0  F of size at most h
whose application makes p win the election. Then, p must be the preferred candidate of
all element voters in the bribed election because no shift action has effect on any dummy
voter. Since there are n element voters and n dummy voters, S 0 := {Sj | fj  F 0 } is a set
cover. Finally, since B = h, S 0 is of size at most h.
Allowing also negative (but unit) effects on the voters, we can adapt our reduction from
Theorem 5 to show a strong inapproximability result. The inapproximability result follows
since in the corresponding reduction, for yes-instances, the only correct solutions use the
exact given budget.
Theorem 6. Unless W[2] = FPT, Combinatorial Shift Bribery is inapproximable (in
FPT time for the parameter B) for both the Plurality rule and the Borda rule, even for two
candidates and unit-effect shift actions.
Proof. We modify the reduction from Theorem 5 to show our inapproximability result.
Let (S, X, h) be a Set Cover instance where S = {S1 , . . . , Sm } and X = {x1 , . . . , xn }.
Without loss of generality, we assume that |S| > h. We construct an instance of PluralityCombinatorial Shift Bribery with two candidates as follows. (Since we have two
candidates only, the proof applies to the case of Borda-Combinatorial Shift Bribery
as well.)
|S|
For each element xi  X, create |S| element voters vi1 , . . . , vi , each with preference
order d  p, and for each set Sj  S create a set voter vj0 with preference order p  d.
Create |S|  |X| + |S|  2h dummy voters, each with preference order d  p. The set F
of shift actions contains, for each set Sj , a shift action fj having an effect of 1 on each
element voter corresponding to an element of the set and an effect of 1 on the set voter
corresponding to the set. Finally, set B := h. This completes the construction, which is
clearly computable in polynomial time.
622

fiCombinatorial Shift Bribery

Next, we show that there is a successful set of shift actions of size h if and only if there
is a set cover of size h.
For the if part, assume that there is a set cover S 0 of size at most h. Then, F 0 =
{fj | Sj  S 0 } is a successful set of shift actions: since S 0 is a set cover, p will be the
preferred candidate of all |S|  |X| element voters and also the preferred candidate for at
least |S|  h set voters (corresponding to the sets not from the set cover). Moreover, d will
be the preferred candidate for all |S|  |X| + |S|  2h dummy voters and also the preferred
candidate for at most h set voters (corresponding to the sets from the set cover). Hence,
either p wins or p and d tie as winners.
For the only if part, assume that there is a successful set of shift actions F 0  F
of size at most h. Then, p must be the preferred candidate for all element voters in the
bribed election: If there were an element voter with d  p, then there would be at least
|S|  1 further element voters with d  p (the element voters corresponding to the same
element). Thus there would be in total at most |S|(|X|  1) element voters and |S| set
voters that prefer p, but at least |S|  |X| + |S|  2h dummy voters and |S| element voters
that prefer d. Since we assumed |S| > h, this would mean that p is not a winner. Thus,
it must be that S 0 := {Sj | fj  F 0 } is a set cover, and, due to the budget constraint, it
follows that |S 0 |  h.
Finally, we show that Plurality-Combinatorial Shift Bribery is inapproximable
even in FPT time when parameterized by the budget. Assume, for the sake of a contradiction, that a successful set of shift actions F 0  F with |F 0 | > B exists. Then, in the bribed
election, at least |S|  |X| + |S|  2h dummy voters and also |F 0 |  h + 1 set voters prefer d,
but at most |S|  |X| element voters and at most |S|  (h + 1) set voters prefer p. Thus, d is
the unique winner. Hence, any successful bribery action must be optimal with respect to
the budget and any FPT-algorithm for Plurality-Combinatorial Shift Bribery (parameterized by the budget) would solve the W[2]-hard problem Set Cover (parameterized by
the solution size) in FPT time; a contradiction to the assumption that FPT 6= W[2].
6.2 Results for Shift Actions with Unit Effect on Two Voters
In the previous section we did not limit the number of voters affected by each shift action.
Now we focus on the case where each unit-effect shift action can affect at most two voters. First we show that Combinatorial Shift Bribery remains NP-hard and hard to
approximate for (+1, 1)-shift actions. Then we provide parameterized hardness results
for both (+1, 1) and (+1, +1)-shift actions. The proof is relatively similar to the one for
Theorem 6 and so we defer it to Appendix A.
Theorem 7. Unless P = NP, Combinatorial Shift Bribery is inapproximable (in
polynomial time) for both the Plurality rule and the Borda rule, even for two candidates and
(+1, 1)-shift actions.
As opposed to Theorem 6, the above result does not yield W[2]-hardness for the parameter budget B. This is because our proof uses a reduction from Set Cover in which
the value of the budget is the size of the universe set X. If we insist on parameterized
hardness for unit effects on two voters, then we have to accept larger sets of candidates.
However, this increase is not too large: below we show W[1]-hardness of Combinatorial
Shift Bribery jointly parameterized by the budget and the number of candidates.
623

fiBredereck, Faliszewski, Niedermeier, & Talmon

Theorem 8. For both the Plurality rule and the Borda rule, Combinatorial Shift
Bribery is W[1]-hard for the combined parameter (m, B), even if we either only have
(+1, 1)-shift actions or only have (+1, +1)-shift actions.
Proof. We have four cases to consider. We begin with the Plurality rule and (+1, +1)-shift
actions.
The Plurality Rule with (+1, +1)-Shift Actions. We describe a parameterized reduction from the W[1]-hard Clique problem, parameterized by the solution size, to PluralityCombinatorial Shift Bribery with (+1, +1)-shift actions, parameterized by (m, B).
Let (G, h) be an instance of Clique with V (G) = {u1 , . . . , un0 } and E(G) = {e1 , . . . , em0 }.
We create the following instance of Plurality-Combinatorial Shift Bribery. The set
of candidates is {p}  D, where D = {d1 , . . . , dh1 }. For each vertex ui  V (G), we cre

ate a vertex voter vi with preference order D  p. Moreover, we create n0  2h dummy


voters with preference order p  D each. For each edge {ui , uj }  E(G), we create a
shift action f{ui ,uj } with effect 1 on the vertex voters vi and vj , and effect 0 on all other

voters. Finally, we set the budget to B := h2 . This completes the construction, which is
computable in polynomial time. Consider the following example.
Example 4. We have the following graph, where we are looking for a clique of size h = 3.
u2

u3

u5

u4

u1

u7

u6

We construct the following input for Plurality-Combinatorial Shift Bribery.
election
v1 : d1  d2  p
v2 : d1  d2  p
v3 : d1  d2  p
v4 : d1  d2  p
v5 : d1  d2  p
v6 : d1  d2  p
v7 : d1  d2  p
1 dummy : p  d1  d2

shift actions
     
1
1
1
     
1 0 0
     
     
0 0 0
     
0 1 0
     
     
0 0 1
     
     
0 0 0
     
0 0 0
     
0
0
0

 
1
 
0
 
 
0
 
0
 
 
0
 
 
0
 
1
 
0

 
0
 
0
 
 
0
 
0
 
 
1
 
 
1
 
0
 
0

 
0
 
0
 
 
0
 
0
 
 
1
 
 
0
 
1
 
0

fu1 ,u2

fu1 ,u7

fu5 ,u6

fu5 ,u7

fu1 ,u4

fu1 ,u5

Note that (v1 , v5 , v7 ) form a clique of size 3 in the input graph for Clique, and, accordingly, applying the set of shift actions {fu1 ,u5 , fu1 ,u7 , fu5 ,u7 } results in p being the winner of
the election for Plurality-Combinatorial Shift Bribery.
4
624

fiCombinatorial Shift Bribery

Without loss of generality, assume that d1 is ranked first in the (arbitrary but fixed)


order D. Observe that we have n0 vertex voters and h dummy voters which rank d1 first.
We also have n0  h dummy voters which rank p first. Hence, to make p win the election,
one needs h additional voters to rank p first (and, in effect, not rank d1 first).
It remains to show that our constructed instance contains a successful set of shift actions F 0 of size h if and only if (G, h) contains a clique of size h.
For the if part, let H  V (G) be a set of h vertices forming a clique and let E 0  E(G)
be the set of edges between the vertices from H. Then, observe that F 0 = {f{ui ,uj } |
{ui , uj }  E 0 } is a successful set of shift actions: For each vertex voter vi corresponding to
a clique vertex ui  H, candidate p is shifted h  1 positions forward. This means that, in
total, we have that h vertex voters rank p first and p ties as a winner of the election.
For the only if part, let F 0 be a successful set of shift actions. Since dummy voters are
not affected by any shift action, it follows that in order to make p a winner of the election,
p must be shifted to the top position in at least h vertex voters. That is, in total, p must
be shifted h  (h  1) positions forward. Since F 0 is of size at most B = h2 = h  (h  1)/2

and each shift action affects only two vertex voters, F 0 must be of size exactly h2 affecting

exactly h vertex voters. By construction, this implies that there are h2 edges in G incident
to exactly h different vertices which is only possible if these h vertices form a clique. This
finishes the proof for the Plurality rule with (+1, +1)-shift actions.
The remaining cases of the proof are quite similar (although, technically, more involved)
and we present them in Appendix B.
It is quite natural to consider Combinatorial Shift Bribery also from a different
perspective. Instead of asking what happens for a small number of candidates, we might
ask about the complexity of Combinatorial Shift Bribery for a small number of voters
(see, for example, Brandt, Harrenstein, Kardel, & Seedig, 2013; Chen et al., 2015, for
some motivation as to why looking at elections with few voters is interesting). In this case
we obtain hardness only for the Borda rule. Indeed, later we will show that PluralityCombinatorial Shift Bribery is in FPT for the parameter number of voters. The proof
of the next theorem is quite involved and is available in Appendix C.
Theorem 9. Borda-Combinatorial Shift Bribery is W[1]-hard with respect to the number n of voters, even for (+1, 1)-shift actions and no budget constraints.
In the proof of Theorem 9 we reduce from the Strongly Regular Multicolored
Clique problem, and, importantly, we do not impose any budget constraints. Thus, it follows that any approximation algorithm for Borda-Combinatorial Shift Bribery (running in FPT time when parameterized by the number of voters) would yield an FPT algorithm for Strongly Regular Multicolored Clique when parameterized by the
solution size. In effect, we have the following corollary.
Corollary 10. Unless W[1] = FPT, Borda-Combinatorial Shift Bribery is inapproximable even in FPT-time for the parameter n, even for (+1, 1)-shift actions.
The results from Theorem 9 and Corollary 10 compare very interestingly to those for the
non-combinatorial variant of Borda-Shift Bribery. Until very recently, the complexity of
Borda-Shift Bribery parameterized by the number of voters was unknown. Eventually
625

fiBredereck, Faliszewski, Niedermeier, & Talmon

(in a different paper, and after submitting this one for journal publication) we have shown
that the problem is W[1]-hard (Bredereck et al., 2016), through a far simpler proof than
the one used here. Nonetheless, Theorem 9 and Corollary 10 still carry significant value.
Earlier, Bredereck et al. (2014a) have shown that there is an FPT approximation scheme
for Borda-Shift Bribery parameterized by the number of voters, and Corollary 10 shows
that this result does not generalize to the combinatorial setting.
6.3 Results for Interval Shift Actions
We conclude the discussion of hardness results by considering Combinatorial Shift
Bribery with interval shift actions. In the previous section we allowed shift actions to
have non-zero effects on two voters each, but these two voters could have been chosen arbitrarily. Now we show a hardness result for the case where we can positively affect multiple
voters, but these voters have to form a consecutive interval in the input election.
Theorem 11. For both the Plurality rule and the Borda rule, Combinatorial Shift
Bribery is NP-hard even for interval shift actions.
Proof. We consider the Plurality rule first and give a many-one reduction from the following
variant of the strongly NP-hard Numerical Matching with Target Sums problem.
Numerical Matching with Target Sums
Input: Three sets of integers A = {a1 , . . . , at }, B = {b1 , . . . , bt }, and X =
{x1 , . . . , xt }, where (1) the numbers are encoded in unary, (2) all the 3t numbers
are distinct, and (3) no two numbers that are both from A or both from B sum
up to any number in X.
Question: Can the elements of A and B be paired so that for each i  [t] the
sum of the ith pair is exactly xi ?
The standard variant of the problem, as presented in the classic text of Garey and
Johnson (1979), does not have any restrictions on the integers in sets A, B, and X. We
can assume that the numbers are encoded in unary because the problem is strongly NPhard. Further, Hulett, Will, and Woeginger (2008) have shown that the problem remains
NP-hard for the case where all the 3t integers are distinct. Finally, to see that the third
restriction does not change the complexity of the problem it suffices to consider the following
transformation: Given an instance (A, B, X) of Numerical Matching with Target
Sums, we add 2  max(A  B  X) + 1 to each integer in B and X. This produces an
equivalent instance where no two numbers, both from A or both from B, sum up to any
number in X.
The Plurality Rule. Let (A, B, X) be an instance of Numerical Matching with
Target Sums and let y denote the largest integer in A  B  X. We create an instance of
Plurality-Combinatorial Shift Bribery as follows. The set of candidates is:
C := {p, d, ca1 , . . . , cat , cb1 , . . . , cbt , cx1 , . . . , cxt }.
We create the following voters.
626

fiCombinatorial Shift Bribery

1. For each pair of integers ai  A and x`  X, we introduce:
(a) One voter with preference order

cai  p  C \ {p, cai },
(b) ai voters each with preference order

cx`  p  C \ {p, cx` },
(c) 2y  (ai + 1) voters each with preference order

d  p  C \ {p, d}.
These voters are called the (ai , x` )-voters and there are exactly 2y of them. For each
pair (ai , x` ), we construct a shift action faxi` with effect 1 on exactly the set of (ai , x` )
voters.
2. For each pair of integers bj  B and x`  X, we introduce:
(a) One voter with preference order

cbj  p  C \ {p, cbj },
(b) bj voters each with preference order

cx`  p  C \ {p, cx` },
(c) 2y  (bj + 1) voters each with preference order

d  p  C \ {p, d}.
These voters are called the (bj , x` )-voters and there are exactly 2y of them. For each
pair (bj , x` ), we construct a shift action fbxj` with effect 1 on exactly the set of (bj , x` )
voters.
3. Let q := 4ty. We create sufficiently many dummy voters to ensure that, altogether,
the candidates have the following scores:
(a) p has q points,
(b) for each i, cai and cbi have q + 4ty + 1 points each, and
(c) for each `  [t], cx` has q + 4ty + x` points.
No shift action affects any of the dummy voters.
627

fiBredereck, Faliszewski, Niedermeier, & Talmon

Finally, we set the budget B := 2t. This completes the reduction. It is easy to see that it
is computable in polynomial time (because all the numbers are encoded in unary) and that
we can order the voters so that each shift action effects on a consecutive interval of z := 2y
voters.
It remains to show that our constructed instance of Plurality-Combinatorial Shift
Bribery contains a successful set F 0 of shift actions of size at most 2t if and only if (A, B, X)
is a yes-instance of Numerical Matching with Target Sums.
For the if part, let S := {(ai1 , bj1 ), . . . , (ait , bjt )} be a solution for Numerical Matching with Target Sums, that is, a set of integer pairs such that each integer from A  B
occurs exactly once in S and such that ai` + bj` = x` holds for each `  [t]. Observe that
F 0 := {faxi` , fbxj` | (ai` , bj` )  S} is a successful set of shift actions. Since each integer from
`

`

A  B occurs exactly once in (some pair of) S, each candidate cai and each candidate cbj
loses one point. Since ai` + bj` = x` for each `  [t], each candidate cx` loses x` points.
By construction, p gains 4ty points by any set of shift actions of size 2t. Thus, p wins the
election.
For the only if part, let F 0 be a successful set of shift actions of size 2t (if there were a
successful action of smaller size we could extend it to size 2t because our shift actions do not
have negative effects). After applying shift actions from F 0 , p gains 4ty points. If this is to
make p a winner of the election, each candidate cai and each candidate cbj needs to lose one
point, and each candidate cx` needs to lose x` points. Thus, for each ai  A there is exactly
x`
x`
one fai i  F 0 and for each bj  B there is exactly one fbj i  F 0 . Since all the integers in
A  B  X are distinct and no two integers both from A or both from B sum up to any
integer from X, for each x`  X there is at least one shift action faxi` with effect on ai`
`
voters who prefer cxl , and one shift action fbxj` with effect on bj` voters who prefer cx` . Since
`
there are t candidates cx` and |F 0 | = 2t, it follows that there are exactly two shift actions
with effect on some voters preferring cx` . Since cx` has to lose at least x` points, it holds
that ai` + bj`  x` . In fact, by the pigeonhole principle, it holds that ai` + bj`  x` . Hence,
if there is a successful set of 2t shift actions, then there is a solution for our Numerical
Matching with Target Sums instance.
The Borda Rule. For the Borda rule, almost the same reduction works. Specifically,
there still exists some integer q for which the set of requirements which were required in
the proof for the Plurality rule will now hold for the Borda rule (with respect to a different
q). Importantly, since p is in the second position in the preference profiles of all of the
voters, it holds that the score differences, when applying some shift actions, are similar for
the Plurality rule and the Borda rule. Thus, the proof of correctness for the Plurality rule
transfers to the Borda rule.

Throughout this section we have shown a number of hardness results under more and
more restrictive assumptions regarding the available shift actions. In the following sections
we seek positive algorithmic results.
628

fiCombinatorial Shift Bribery

7. Exact Algorithms
In spite of the pessimism looming from the previous section, in this section we show two
exact FPT and XP algorithms for R-Combinatorial Shift Bribery. Then, in Section 8,
we present several efficient approximation algorithms.
We begin by observing that R-Combinatorial Shift Bribery can be solved in polynomial time, provided that we assume the budget B to be a constant. The reason is that
we need to choose at most B shift actions out of all available ones, but the number of shift
actions available is upper-bounded by the input size.
Observation 1. Both Plurality-Combinatorial Shift Bribery and Borda-Combinatorial Shift Bribery are in XP when parameterized by the budget B.
If we restrict the instances to contain only bounded-effect shift actions, then we can
show that R-Combinatorial Shift Bribery can be solved in polynomial time, provided
that the number n of the voters is treated as a constant.
Proposition 12. If the maximum effect of every shift action is upper-bounded by some universal constant, then both Plurality-Combinatorial Shift Bribery and Borda-Combinatorial Shift Bribery are in XP when parameterized by the number n of the voters.
Proof. Let  be the value bounding, component-wise, the effect of each shift action. First,
observe that there are at most (2 + 1)n types of different shift actions. Second, observe
that once one knows the budget spent on each type of shift actions, one can easily check
whether a corresponding set of shift actions makes p a winner of the election. Thus we use
the following algorithm: We try all possibilities of distributing the budget B among the at
most (2 + 1)n types of shift actions and check whether one of them makes p a winner. If
so, we accept. Otherwise we reject.
Proposition 12 holds even if each shift action comes at an individual cost and if each
voter has an individual threshold function, because we can, given some budget, always
select the cheapest set of shift actions of a given type. Further, by expressing our problem
as an integer linear program (ILP) and by using a famous result of Lenstra (1983), for the
Plurality rule we can strengthen the above XP-membership to FPT-membership.
Theorem 13. For bounded-effect shift actions (where we treat the bound as a universal
constant), Plurality-Combinatorial Shift Bribery is in FPT when parameterized by the
number n of the voters.
Proof. Given an instance of Plurality-Combinatorial Shift Bribery with n voters, our
algorithm proceeds as follows. First, we guess a subset of the voters for whom we will
guarantee that p is ranked first (there are 2n guesses to try). For each guessed set of voters,
we test whether p would be a winner of the election if p were shifted to the top position
by the guessed voters and was not ranked first by the remaining voters. For each guessed
subset V 0 of voters for which this test is positive, we check whether it is possible to ensure
(by applying shift actions whose cost does not exceed the budget) that the voters from V 0
rank p first. We do so as follows.
Let  be the universal constant bounding, component-wise, the effect of each shift action.
Observe that there are at most (2+1)n types of different shift actions. For each shift action
629

fiBredereck, Faliszewski, Niedermeier, & Talmon

type z, we introduce a variable xz denoting the number of times a shift action of type z is
present in the solution. For each voter vi , denote by svi (p) the position of p in the original
preference order of vi . For each voter vi  V 0 , we add the following constraint:
 P

P

x
 svi (p).
z
[,]
{z:fz has an effect of  on vi }
This ensures that p is indeed shifted to the top position in vi s preference list. We add the
budget constraint:
X
xz  B,
ensuring that the solution respects the budget. Finally, for each shift action type z we add a
constraint ensuring that we use at most as many shift actions of type z as there are available
in the input. This finishes the description of the ILP. By a result of Lenstra (1983), we can
solve this ILP in FPT time, because we have at most (2 + 1)n integer variables.
Roughly speaking, Theorem 13 is the reason why Theorem 9 does not apply to the
Plurality rule. In this setting, Plurality-Combinatorial Shift Bribery is tractable.
Note that Theorem 13 applies to the case where each shift action has the same unit cost,
i.e., the case on which we focus in this paper. Nonetheless, we believe that it is possible
to lift Theorem 13 to the case where each shift action has its individual cost, by applying
ideas of Bredereck, Faliszewski, Niedermeier, Skowron, and Talmon (2015a).

8. Approximation Algorithms
We now explore the possibility of finding approximate solutions for Combinatorial Shift
Bribery. We focus on approximating the cost of shift actions necessary to ensure ps
victory (for example, a 2-approximate algorithm finds a solution that ensures ps victory
whenever it is possible, and uses at most twice as many shift actions as necessary). By
Theorems 6 and 7, we know that we cannot hope to find approximate algorithms for the
cases of Combinatorial Shift Bribery where the shift actions can have negative effects.
Thus, in this section, we focus on unit-effect shift actions with only positive effects. This
also simplifies our situation in that we can always check if it is possible to ensure ps victory:
It suffices to apply all the available shift actions and check if p is a winner (indeed, not being
able to perform such a check is at the heart of our inapproximability results from Section 6).
All our approximation algorithms proceed either by directly invoking the algorithms for
the non-combinatorial variant of Shift Bribery of Elkind et al. (2009) and Elkind and
Faliszewski (2010), or by plugging our algorithms into their framework. We start with the
former approach and then describe the latter.
Theorem 14. If each shift action has effects of either 0 or 1 on each voter, then PluralityCombinatorial Shift Bribery can be -approximated in polynomial-time and BordaCombinatorial Shift Bribery can be 2-approximated in polynomial time, where  denotes the maximum number of voters affected by a shift action.
Proof. The general idea of these approximation algorithms is to split each shift action that
affects some 0   voters into 0 shift actions, each affecting a single voter only. In effect
630

fiCombinatorial Shift Bribery

we construct a non-combinatorial instance of Shift Bribery that we solve exactly, for the
case of Plurality rule, or 2-approximately, for the case of the Borda rule.
Specifically, our construction goes as follows. Let (i) denote the number of shift actions
affecting voter i. Given an instance of Combinatorial Shift Bribery, we form an
instance of Shift Bribery that is identical, except that instead of having shift actions,
we have price functions for the voters: We set the price function for each voter i so that
for j  (i), shifting p by j positions costs j, and for j > (i), shifting p by j positions
costs (2B + 1)j (where B is the total number of shift actions available; note that the
exponential function (2B + 1)j ensures that the price functions are convex and that we can
easily identify situations where one shifts p by more than (i) positions).4
Below we describe how to use this construction for the case of the Plurality rule and for
the case of the Borda rule.
The Plurality Rule. We first translate the input instance into the non-combinatorial
Plurality-Shift Bribery instance as described above. Then, we apply the known, exact,
polynomial-time algorithm for the Plurality-Shift Bribery (Elkind et al., 2009) on this
instance. Let s be the cost of the solution found for the non-combinatorial instance. If
s > B, then it is impossible to ensure ps victory in the combinatorial instance (because the
number of available shift actions is insufficient).
If s  B, then to obtain a solution F for the Plurality-Combinatorial Shift Bribery
instance we do as follows. For each voter v that in the (non-combinatorial) bribed election
ranks p first, we select shift actions in the combinatorial instance so that v ranks p first.
Note that |F |  s and that F is indeed a (combinatorial) solution.
For the sake of contradiction, assume that there is a successful set of shift actions F 0
with size smaller than |F |/. However, it is easy to see that such a set of shift actions would
correspond to a bribery of cost smaller than s for the non-combinatorial instance. Since s is
the cost of the optimal solution for the non-combinatorial instance, this is a contradiction.
The Borda Rule. The case of Borda-Combinatorial Shift Bribery follows analogously, but instead of using the polynomial-time exact algorithm for the non-combinatorial
instance, we use the 2-approximation algorithm for Borda-Shift Bribery (Elkind et al.,
2009; Elkind & Faliszewski, 2010). Let s be the cost of the solution found. If s > 2B, then
it is impossible to ensure ps victory.
Otherwise, to obtain the solution F for the combinatorial instance, for each vote v where
the non-combinatorial solution shifts p by some t positions, we include t shift actions that
affect this voter. We have that |F |  s, and F is a correct solution for the combinatorial
instance.
If there existed a solution F 0 for the combinatorial instance that used less than |F |/(2)
shift functions, then there would be a solution for the non-combinatorial instance with
cost smaller than |F |/2  s/2. Since we used a 2-approximate algorithm for the noncombinatorial instance, this is impossible.
We mention that it might be possible to improve the approximation ratio given in Theorem 14, at least for the Borda rule. The idea might be to cast the problem as a variant of
4. Strictly speaking, there is no need to ensure that the price functions are convex, but this is the variant
of Shift Bribery that we generalize in this paper, so we stick to it for consistency.

631

fiBredereck, Faliszewski, Niedermeier, & Talmon

the Set Multicover problem, which is a generalization of the Set Cover problem where
each element has its own covering requirement. Then, one could use an approximation algorithm for the Set Multicover problem (for example, the one suggested in Rajagopalan
& Vazirani, 1998) and plug it into the 2-approximation algorithm of Elkind and Faliszewski
(2010).
We can achieve better approximation guarantees for the Borda rule, when we further
restrict the allowed shift actions. To obtain these results we use the framework of Elkind
and Faliszewski (2010). In essence, they have shown the following: If for a given variant
of Shift Bribery, either for the Plurality rule or for the Borda rule, one can provide
a function that computes how to obtain the highest number of points for the preferred
candidate given some budget B, then there is a 2-approximation algorithm for this variant
of Shift Bribery.5 Note that such a get-most-points-for-p algorithm does not solve Shift
Bribery. While it maximizes the score of p, it does not ensure that no candidate receives
higher score. Indeed, an optimal solution might increase the score of p to a smaller extent,
but at the expense of more dangerous opponents.
Theorem 15. Borda-Combinatorial Shift Bribery is 2-approximable in polynomial
time for (+1, +1)-shift actions.
Proof. By the discussion preceding the theorem statement, it suffices to provide a function
that given an instance of Combinatorial Shift Bribery with budget B finds a set of
shift actions that obtain the highest possible number of points for the preferred candidate
p without exceeding the budget.
The general idea of achieving this is to compute a maximum b-matching in an auxiliary
multigraph (multigraphs allow multiple edges between the vertices). A b-matching of a
multigraph G with a function b : V (G)  N (called a covering function) is an edge-induced
subgraph of G such that each vertex u has degree at most b(u). It is known that a b-matching
can be computed in polynomial time (Gabow, 1983).
We construct the auxiliary multigraph G as follows. For each voter vi we create a vertex
ui . For each shift action with effect 1 on voter ui and effect 1 on voter uj , we create an edge
{ui , uj }. Then, we define a covering function b such that b(ui ) is the number of positions
that p can be shifted forward in the preference order of voter vi (that is, the position of p
in the preference order of voter vi ).
If G has a b-matching of size at least B, then it corresponds to a set of shift actions that
increase the score of p by 2B, which is the highest gain possible. If G has a b-matching of
size k < B, then we take the shift actions corresponding to the edges of this b-matching
(these shift actions maximize the number of points that p can gain from shift actions that
move p within two votes) and greedily select more shift actions that each pushes p forward
in one vote, to use up the budget (at this point, every shift action can affect p in a single
vote only). Thus our function computes the highest point gain possible for p, for a given
budget.
Next, we consider interval shift actions. That is, we fix some order of the voters and
restrict each shift action to have effect only on voters which comprise intervals. (In fact, we
5. In fact, their result applies to all scoring rules, but in this paper we focus on the Plurality rule and on
the Borda rule only.

632

fiCombinatorial Shift Bribery

could also allow holes inside these intervals.) Unfortunately, the algorithm requires XP
time for the parameterization by the length of the longest interval.
Theorem 16. For both the Plurality rule and the Borda rule, Combinatorial Shift
Bribery can be 2-approximated in XP-time for interval shift actions, provided that we take
, the upper bound on the number of voters affected by each shift action, as the parameter.
Proof. As per discussion preceding Theorem 15, it suffices to describe how to find a set
of shift actions which maximize the number of points that the preferred candidate p gains
under a given budget.
To this end, we use a dynamic programming algorithm. Consider an input for
Combinatorial Shift Bribery with election E = (C, V ), preferred candidate p, and
budget B to spend on increasing ps score. Let m := |C| and n := |V |. We have
V = (v1 , . . . , vn ). Our algorithm uses the following table for partial results. For numbers x, y, s0 , . . . , s1 the table entry:
T [x, y, s0 , s1 , . . . , s1 ]
denotes the maximum number of additional points that candidate p can gain from voters v1 , . . . , vx under the condition that (1) exactly y shift actions are used, each of them
affects at most the voters from the set {v1 , . . . , vx }, and (2) for each i  {0, ...,   1},
candidate p is shifted to position si in the preference order of voter vxi . That is, we iterate
over the voters and store the effect that the applied shift actions had on the last  voters.
The size of the table is n  B  m+1 .
Our algorithm is almost the same for both the Plurality rule and the Borda rule. The
only difference is in computing the scores of the candidates. Let z, 0  z  m  1, denote
the position of p in the preference order of some voter (position 0 means that p is ranked
first). Then, by score(z) we mean the score that p gains from this voter. For the Plurality
rule we have score(z) = 1 for z = 0 and score(z) = 0 otherwise. For the Borda rule we have
score(zi ) = m  zi  1. For a set of voters and a vector z1 , . . . , zt (for t  [n] and each zi in
{0, . . . , m  1}) that denotes the positions of p in the preference orders of these voters, we
write score(z1 , . . . , zt ) to mean the score that p gains from these voters. That is:
score(z1 , . . . , zt ) =

X

score(zi ).

i[t]

Given this preparation, we are ready to describe our algorithm (jointly for the Plurality
rule and for the Borda rule).
Initialization. We initialize the entries T [, y, s0 , s1 , . . . , s1 ] of the table as follows.
We check whether there is a set of y shift actions that have effects only on voters from
(v1 , . . . , v ) and such that applying this set of y shift actions moves candidate p to positions s0 , . . . , s1 in the preference orders of the voters v1 , . . . , v , respectively. If it
exists, then we set T [, y, s0 , s1 , . . . , s1 ] to score(s0 , s1 , . . . , s1 ). Otherwise, we set
T [, y, s0 , s1 , . . . , s1 ] to . (We explain how to check if such a set of shift actions exists
at the end of the proof.)
633

fiBredereck, Faliszewski, Niedermeier, & Talmon

Recursion Step. To compute the table entries T [x, y, s0 , s1 , . . . , s1 ] for x > , one has
to compute subsets of i shift actions (for i  [y]) whose last affected voter is vx , that ensure
together with yi shift actions whose last affected voter is from the set {v1 , . . . , vx1 }that
for each j, 0  j    1, p is shifted to position sj in the preference order of vxj .
More specifically, in the update phase we compute for each x,  < x  n, each y, 0  y 
B, and each vector (s0 , . . . , s1 )  {0, . . . , m  1} the table entry T [x, y, s0 , s1 , . . . , s1 ]
as follows. We say that a vector (s0 , s1 , . . . , s1 )  {0, . . . , m} is (x, i)-realizable for
some i (0  i  y), if there is a set of i shift actions whose last affected voter is vx and
such that for each j, 0  j    1, it shifts candidate p by sj positions in the preference
order of voter vxj . We write R(x, i) to denote the set of vectors from {0, . . . , m  1}
that are (x, i)-realizable (we describe how to compute R(x, i) later). Then, we compute
T [x, y, s0 , s1 , . . . , s1 ] as follows:
T [x, y, s0 , s1 , . . . , s1 ] = max{T [x  1, y  i, s , s0  s1 , . . . , s1  s1 , s ]
+ score(s0 , s1 , . . . , s1 )  score(s1  s1 , . . . , s1  s1 ) |
0  i  y, 0  s  m  1, (s0 , s1 , . . . , s1 )  R(x, i)}
Informally, for each realizable total effect of i shift actions whose last affected voter
is vx , the number of points that candidate p gains is the number of additional points that
candidate p gains by shift actions for which the last affected voter is from (v1 , . . . , vx1 )
plus the number of additional points that candidate p gains by shift actions for which the
last affected voter is vx (to avoid double counting, this is expressed as the difference in the
middle line of the above formula).
We next show how to compute R(x, i). We try every vector (s0 , . . . , s1 )  {0, . . . , m 

1} and for each we check if it is (x, i)-realizable. Perhaps the easiest way to do this is
to formulate this problem as an integer linear program (ILP) with a constant number of
variables.
Let (s0 , . . . , s1 ) be a vector for which we want to check if it is (x, i)-realizable. For
each subset Q  {0, . . . ,   1}, we say that a shift action is of type Q if it affects exactly
the voters vxi with i  Q. For each such subset Q, we introduce an integer variable xQ ,
denoting the number of shift actions of type Q used in the (x, i)-realization of our vector.
We solve the following ILP:
X

xQ = i

(1)

xQ{0} = i

(2)

Q{0,...,1}

X
Q{1,...,1}

X

j : 0  j    1

xQ = sj

(3)

jQ

(Note that the middle constraint ensures that the last affected voter is vx .) Since the
number of variables in this ILP is 2 , it follows from the famous result of Lenstra (1983)
that this ILP can be solved in XP time with respect to the parameter  (indeed, even in
FPT time). Using the same ILP but without the middle constraint, we can check which
vectors (s0 , . . . , s1 ) we can use in the initialization step.
634

fiCombinatorial Shift Bribery

Coming back to our dynamic program, it is clear that finding how to obtain the maximum score for p while respecting our budget can be found by taking the maximum over
the table entries T [n, B 0 , s0 , s1 , . . . , s1 ], for all possible values of B 0 , 0  B 0  B, and
(s0 , s1 , . . . , s1 )  {0, . . . , m  1} .
While in this section we showed that it is indeed possible to achieve some approximation
algorithms for some special cases of the Combinatorial Shift Bribery problem, the
settings for which our algorithms are efficient are quite restrictive. This means that in
practice one might want to seek good heuristics and use our algorithms as a guidance for
the initial search.

9. Conclusion
We have defined a combinatorial variant of the Shift Bribery problem (Elkind et al., 2009;
Elkind & Faliszewski, 2010; Bredereck et al., 2014a) and we have studied its computational
complexity. The motivation for our research was the desire to understand the computational
difficulty imposed by correlated, large-scale effects of campaign actions. In this respect, this
work is motivated by the combinatorial study of election control, as studied by Bulteau et al.
(2015) and Chen et al. (2015). We have found that even for various very restricted special
cases and numerous parameterizations, the Combinatorial Shift Bribery problem is
highly intractable in the worst case. Nonetheless, we have found some initial positive results,
mainly in the form of approximation algorithms. Interestingly, our approximation results
quite strongly rely on the results for non-combinatorial Shift Bribery.
There is a number of research directions that are motivated by our work. For example, can Plurality-Combinatorial Shift Bribery or Borda-Combinatorial Shift
Bribery be solved in polynomial-time for (+1, +1) shift actions or interval actions under
the assumption that the number of candidates is a constant?
More generally, our results suggest studying further restrictions of the problem. As an
example, since parameterizing by the number of available shift actions immediately gives
fixed-parameter tractability results, a natural question is whether other natural parameterizations exist which could also lead to positive results.
Naturally, one might consider other voting rules as well. Most interesting are Condorcetconsistent rules, such as the Copeland rule, since these rules tend to behave rather differently
than scoring rules. We mention that some of our results do hold for other voting rules:
specifically, Theorem 2, Theorem 5, Theorem 6, and Theorem 7 hold for most voting rules
because these theorems hold for elections with only two candidates, and most voting rules
behave the same for elections with only two candidates; Observation 1 and Theorem 12 are
basically brute-force algorithms and the results hold for most voting rules as well; and the
statements regarding the Borda rule in Theorem 14, Theorem 15, and Theorem 16 hold for
all scoring rules, since the underlying 2-approximation algorithm of Elkind and Faliszewski
(2010) works for all scoring rules.
Further, it might also be interesting to consider domain restrictions regarding voters
preferences (for example, single-crossing seems particularly natural in the context of interval
shift actions, since it means that each shift action affects voters with somewhat similar
preferences), as it is well-demonstrated that restricting the domain of the voters can lead
635

fiBredereck, Faliszewski, Niedermeier, & Talmon

to tractability (see Theorem 10 in Bulteau et al., 2015, for an example in the combinatorial
control setting). However, pursuing this direction would require a careful discussion of what
shift actions can be applied. For example, should we allow a single-crossing election to cease
being single-crossing after the bribery?
Acknowledgments
Robert Bredereck was supported by the DFG project PAWS (NI 369/10). Nimrod Talmon was supported by the DFG Research Training Group Methods for Discrete Structures (GRK 1408) and is currently at Weizmann Institute of Science. Piotr Faliszewski
was supported by the DFG project PAWS (NI 369/10) and by AGH University grant
11.11.230.124 (statutory research).
A preliminary short version of this work has been presented at the 2015 International
Conference on Autonomous Agents and Multiagent Systems (AAMAS 15) (Bredereck,
Faliszewski, Niedermeier, & Talmon, 2015b).

Appendix A. Proof of Theorem 7
Theorem 7. Unless P = NP, Combinatorial Shift Bribery is inapproximable (in
polynomial time) for both the Plurality rule and the Borda rule, even for two candidates and
(+1, 1)-shift actions.
Proof. We give a many-one reduction from Set Cover. Let (S, X, h) be a Set Cover
instance, with S = {S1 , . . . , Sm } and X = {x1 , . . . , xn } (we assume that every element
belongs to at least one set). We construct an instance of Plurality-Combinatorial Shift
Bribery. We set the budget B := |X|. The candidate set is {p, d}, where p is the
preferred candidate. We have an element voter vi for each element xi , with preference
order d  p. We have a set voter vjS for each set Sj , with preference order p  d. We also
have |X| + |S|  2h  1 dummy voters, each with preference order d  p. For each element
xi and each set Sj , if xi  Sj then we construct a shift action fji with effect of +1 on vi and
effect of 1 on vjS . This completes the construction. It is easy to see that it is computable
in polynomial time.
Next, we show that there is a successful set of shift actions (note that the size of this
set is not important, that is, we allow infinite budget) if and only if there is a set cover of
size h.
For the if part, assume that there is a set cover S 0 of size at most h. We show how
to build a successful set of shift actions. We start with F 0 =  and for each element xi ,
we choose an arbitrary set Sj  S 0 which contains xi and add the corresponding function
fji to F 0 . After applying F 0 , observe that p becomes a winner: All |X| element voters and
|S|  h set voters prefer p but only |X| + |S|  2h  1 dummy voters and h set voters prefer d.
For the only if part, assume that there is a successful set of shift actions F 0  F .
Let h0 be the number such that after applying the shift actions from F 0 , p is preferred by
exactly |S|  h0 set voters (that is, shift actions in F 0 correspond to h0 sets from S). For p to
be a winner, a majority of the voters (i.e., at least |X| + |S|  h voters) must prefer p. Thus,
after applying F 0 , at least X  (h  h0 ) element voters prefer p. This means that there is a
collection of h0 sets from S that jointly cover at least |X|  (h  h0 ) elements. Since every
636

fiCombinatorial Shift Bribery

element belongs to some set, we can extend this collection to a set cover by adding at most
h  h0 sets (in the worst case, one set for each uncovered element). This proves that there
is a set cover for (S, X, h) and completes the only if part.
Note that in the above argumentation we made no assumptions regarding the size of F 0 .
Hence, finding any solution for our Plurality-Combinatorial Shift Bribery instance,
including approximate solutions for any approximation factor, implies finding a set cover of
size at most h. This means that unless P = NP, Plurality-Combinatorial Shift Bribery
is inapproximable in polynomial time.

Appendix B. Remaining Cases of the Proof of Theorem 8
Theorem 7. Unless P = NP, Combinatorial Shift Bribery is inapproximable (in
polynomial time) for both the Plurality rule and the Borda rule, even for two candidates and
(+1, 1)-shift actions.
The Borda Rule with (+1, +1)-Shift Actions. We can slightly modify the reduction used for the Plurality rule with (+1, +1)-shift actions. Specifically, we describe a parameterized reduction from the W[1]-hard Clique problem, parameterized by the solution
size, to Borda-Combinatorial Shift Bribery with (+1, +1)-shift actions, parameterized
by (m, B).
Let (G, h) be an instance of Clique with V (G) = {u1 , . . . , un0 } and E(G) = {e1 , . . . , em0 }.
We create an instance of Borda-Combinatorial Shift Bribery as follows. The set of
candidates is {p}  D, where D = {d1 , . . . , dh1 }. We create the following voters.
1. For each vertex ui  V (G), we create a corresponding vertex voter vi with preference
order:
d1      dh1  p.
2. We create n0  2h dummy voters, each with preference order:
p  d2      dh1  d1 .
3. We create h dummy voters, each with preference order:
dh1  p  d2      dh2  d1 .
4. We create n0  h dummy voters, each with preference order:
p  d1      dh1 .
5. We create n0  h dummy voters, each with preference order:
d1  p  d2      dh1 .
For each edge {ui , uj }  E(G), we create a shift action f{ui ,uj } with effect 1 on the vertex

voters vi and vj and effect 0 on all other voters. Finally, we set the budget to B := h2 .
This completes the construction, which is computable in polynomial time.
637

fiBredereck, Faliszewski, Niedermeier, & Talmon

The proof of correctness follows the same lines as the proof for the Plurality rule with
(+1, +1)-shift actions, but instead of counting the number of approvals, we need to compute
the Borda scores of the candidates. Indeed, this is the reason for our additional dummy
voters.
In particular, the construction ensures that d1 is the original winner of the election and
that the difference between the Borda score of p and the Borda score of d1 is exactly h2 .
Furthermore, each shift action can increase the score of p by at most two. Hence, to make
p a co-winner one must increase the score of p by h(h  1) and decrease the score of d1 by h.
This is possible if and only if the shift actions correspond to edges of some clique of size h.
The Plurality Rule with (+1, 1)-Shift Actions. We still reduce from the W[1]hard Clique problem, parameterized by the solution size, but the reduction is a bit more
involved.
Let (G, h) be a Clique instance where the graph G has n0 := |V (G)| vertices and
0
m := |E(G)| edges. We construct a Plurality-Combinatorial Shift Bribery instance
as follows. Let the set of candidates be {p, d}  D, where D := {d1 , . . . , dh1 }, and create
the following voters:
1. For each vertex vi , create
preference order:

h
3



(h)
vertex voters vi1 , . . . , vi 3 corresponding to vi , each with
d1      dh1  p.

2. For each edge ej = {vi1 , vi2 }, create a corresponding edge voter wj with preference
order:
p  d1      dh1 .


3. Create 2 h2 + (n0  2h) h3  m0 dummy voters, each with preference order:
p  d1      dh1 .
For each edge ej = {vi1 , vi2 }, construct 2

h
3



shift actions, denoted by

( h)
(h)
fe1j ,vi , . . . , fej3,vi1 and fe1j ,vi , . . . , fej3,vi2 ,
2

1

h
3

], (a) fezj ,vi has effect of +1 on viz1 and effect of 1 on wj , and (b)
1
 
fezj ,vi has effect of +1 on viz2 and effect of 1 on wj . Finally, we set the budget B := 2 h2 h3 .
2
This completes the construction. It is easy to see that it is computable in polynomial time
and that it is a parameterized reduction.
Observe that, initially, the edge voters and the dummy
voters prefer
p, while the vertex


h
h
0
voters prefer d1 . Therefore, the initial score of p is 2 2 + (n  2h) 3 , while the initial score

of d1 is n0 h3 . We can assume, without loss of generality, that this means that d1 is the
winner of the election (instances not satisfying this assumption can be solved in constant
time).
It remains to show that our constructed instance contains a successful set of shift actions F 0 of size at most h if and only if (G, h) contains a clique of size at most B. The general
where for each z  [



638

fiCombinatorial Shift Bribery

idea is that if we choose the shift actions corresponding to the edges connecting the nodes
of an h-size clique, then we will ensure that p becomes the preferred candidate for h h3

additional vertex voters, while making d1 the preferred candidate for only h2 additional
edge voters.
Formally, for the if part, let H  V (G) be a set of h vertices forming a clique and let
E 0  E(G) be the set of edges connecting vertices from H. We choose the following set of
shift actions:
 
h
0
0
z
z
]}.
F = {fej ,vi , fej ,vi | ej = {vi1 , vi2 }  E , z  [
1
2
3
We show that F 0 is a successful set
 of shift actions. To this end, observe that for each vertex
h
z
0
voter vi with vi  V and z  [ 3 ], candidate p is shifted h  1 positions forward, therefore

p becomes the preferred candidate for these voters. This means that h h3 additional vertex
voters prefer p (and, thus, do not prefer d1 anymore). Furthermore, p is shifted backwards
only for the voters in {wj | ej  E 0 }, that is, d1 becomes the most preferred candidate for


h
h
0
2 edge voters and p remains the most preferred candidate for m  2 edge voters. Thus,
p and d tie as winners.
For the only if part, let F 0 be a successful set of shift actions.
p a winner of
 To make

the election, p must be shifted to the top position for at least h h3  h2 vertex voters (no
other type of voters can be affected positively). By the pigeonhole
principle, these vertex

h
voters correspond to at least h different vertices (there are 3 voters corresponding to each

vertex). In effect, at least h2 edge voters must be effected negatively so that d1 becomes
their most preferred candidate.
Thus, to make p win the election p must be shifted
to
 top
 the


position for at least h h3 vertex voters. This implies that |F 0 |  (h1)h h3 = 2 h2 h3 = B
and, hence, |F 0 | = B. Itfollows that p is shifted backwards making d1 the most preferred
candidate for exactly h2 edge voters and that p must be shifted to the top position for

exactly h h3 vertex voters corresponding to exactly h different vertices. By construction,
this implies that these h vertices form a clique, and we are done.
The Borda Rule with (+1, 1)-Shift Actions. For the Borda rule, the reduction is,
once again, a bit more involved, but the main idea is the same as for the Plurality rule.
Let (G, h) be an instance of Clique where graph G has n0 := |V (G)| vertices and
0
m := |E(G)| edges. We construct a Borda-Combinatorial Shift Bribery instance as
follows. The set of candidates is {p, d}  D, where D := {d1 , . . . , dh1 }, and we create the
following voters:

(h)
1. For each vertex vi , create h3 vertex voters vi1 , . . . , vi 3 corresponding to vi , each with
preference order:


D  p.
2. For each edge ej = {vi1 , vi2 }, create a corresponding edge voter wj with preference
order:
d1      dh2  p  dh1 .



h
h 2
0 h
0
0
2 + (n 3 + m )(h  1)  ( 3 h )  m
3. Let L :=
. Without loss of generality, we
h1
can assume that L is an integer (this requires simple modifications of the input clique
639

fiBredereck, Faliszewski, Niedermeier, & Talmon

instance only). We create L dummy voters, each with preference order:
p  dh1      d1 .
For each edge ej = {vi1 , vi2 }, construct 2

h
3



shift actions, denoted by

(h)
( h)
fe1j ,vi , . . . , fej3,vi1 and fe1j ,vi , . . . , fej3,vi2 ,
1

2

h
3

], (a) fezj ,vi has effect of +1 on viz1 and effect of 1 on wj , and (b)
1
 
fezj ,vi has effect of +1 on viz2 and effect of 1 on wj . Finally, we set the budget B := 2 h2 h3 .
2
This completes the construction. It is easy to see that it is computable in polynomial time.
The proof of correctness follows the same lines as the proof of correctness for the Plurality
rule and, thus, is omitted.
where for each z  [



Appendix C. Proof of Theorem 9
Theorem 9. Borda-Combinatorial Shift Bribery is W[1]-hard with respect to the number n of voters, even for (+1, 1)-shift actions and no budget constraints.
Proof. We reduce from the following W[1]-hard problem (Mathieson & Szeider, 2012, Lemma
3.2).
Strongly Regular Multicolored Clique
Input: Two integers, d and h, and an undirected graph G = (V, E), where each
vertex has one of h colors in [h], and where each vertex is adjacent to exactly
d vertices of each color different from its own.
Question: Does there exist a clique of size h containing one vertex from each
color class?
Given an instance of Strongly Regular Multicolored Clique, we construct an
instance of Combinatorial Shift Bribery, for the Borda rule. The general idea of
the reduction is as follows. The set of important candidates consists of our preferred
candidate p and the candidates that correspond to the edges. For technical reasons, for
each edge e = {v, v 0 }, we introduce two candidates, e1 and e2 ; one of them is associated
with touching vertex v and the other is associated with touching vertex v 0 . (In fact, we
will introduce more edge candidates and some vertex candidates, but we will use them only
to ensure correct structure of the election and appropriate bribery behavior.) We build two
groups of voters, the vertex-selecting voters and the edge-electing voters. The first group
implements picking vertices for the clique (one vertex from each color), and the second
group implements picking edges (one edge for each pair of colors). We ensure that if a given
set of shift actions has any chance of being successful, then it must hold that h vertices and
h
2 edges are picked. Importantly, this holds even in the unbribed election.
We make sure that p wins the election if and only if the picked voters and edges correspond to a clique (with vertices of each color). To this end, we define the voters so that
there are two numbers,  and , such that:
640

fiCombinatorial Shift Bribery

1. There are h vertices picked by the vertex-selecting voters, each of a different color.
The vertex-selecting voters give  points to each edge candidate that is associated with
touching one of the selected vertices, and  + 1 points to all other edge candidates.
This means that by picking a vertex we decrease the score of the edge candidates for
the edges that touch this vertex.

2. There are h2 edges picked by the edge-selecting voters, one edge for each pair of colors.
The edge-selecting voters give  + 1 points to each edge candidate that corresponds
to a picked edge, and  points to all the remaining edge candidates. This means that,
by picking an edge, we increase the score of the candidates corresponding to it.
3. Candidate p gets  +  + 1 points, irrespective what shift actions we apply.
Note that in the unbribed election every candidate gets at most  +  + 2 points and p
always gets  +  + 1 points. Thus the challenge is to ensure that every candidate gets
 +  + 1 points. By the above description, this is possible only if we pick the vertices and
the edges that correspond to a size h clique (of vertices with different colors). Indeed, if we
selected an edge e that did not touch two selected vertices, then e1 and e2 would receive
 + 1 points from edge-selecting candidates and at least one of them would receive  + 1
points from vertex-selecting voters. In effect, p would not be a winner.
Without loss of generality, we assume that the edges and vertices selected in the unbribed
election do not form a clique (otherwise there would be a trivial solution for the input
problem and we could output a fixed yes-instances of Borda-Combinatorial Shift
Bribery).
Construction. We now formally describe the reduction, then we give an example of
applying it to a simple instance, and finally we show correctness of the reduction. We
illustrate some aspects of the correctness proof using our example.
Candidates. Our set of candidates is somewhat involved. Our important candidates are
the preferred candidate p and the sets of edge candidates, E1 and E2 , defined below.
Let E(G) = {e1 , . . . , e } be the set of edges of graph G. We create two edge-candidate
sets: E 1 = {e11 , . . . , e1 } and E 2 = {e21 , . . . , e2 }. For each i  [h], let ni be the number of
vertices in G with color i and let V i = {v1i , . . . , vni i } be the set of these vertices. For each
color i and each vertex vji  V i , we define the neighborhood of vji as follows:
0

N (vji ) := {e1` | e` = {vji , vji 0 }  E  i < i0 }
0

 {e2` | e` = {vji , vji 0 }  E  i > i0 }.
(This, perhaps a bit strange way of using color numbers to pick edge candidates either from
E 1 or E 2 , is implementing the fact that for each edge e  E(G) we have two candidates, e1
and e2 , associated with touching different endpoints of e.)
For technical reasons we need further candidates as follows. To adjust the scores of
all other candidates, we introduce a single dummy candidate d. We create two further
candidate sets E 0 = {e01 , . . . , e0 } and E 3 = {e31 , . . . , e3 } which will act as guards for the
edge-selecting voters. For each V i we create two candidate sets U i := {uij | vji  V i } and
641

fiBredereck, Faliszewski, Niedermeier, & Talmon

S
S
U 0i = {u0ij | vji  V i } with U := 1ih U i and U 0 := 1ih U 0i which will act as guards
for the vertex-selecting voters.
Our final set of candidates is C := U  U 0  E 0  E 1  E 2  E 3  {p, d}.
Vertex-Selecting Voters. We now describe the vertex-selecting voters. For each color i
and each vertex vji , we define the following parts of preference orders (for j = 1, we assume
that uij1 and u0ij1 are uini and u0ini respectively):

A(vji ) : uij  N (vji )  u0ij ,

B(vji ) : uij1  N (vji )  u0ij1 .
For each color i we create three pairs of voters. The voters in the first pair, wi and wi0 , have
the following preference orders:


wi : p  A(v1i )  A(v2i )  A(v3i )      A(vni i )  Ri ,


wi0 : Ri  B(v1i )  B(vni i )  B(vni i 1 )      B(v2i )  p,
where Ri is the set of the remaining candidates, that is, Ri := C \ ({p}  U i  U 0i  N (v1i ) 
    N (vni i )). The voters in the second pair, qi and qi0 , have preference orders that are the
reverse of wi and the reverse of wi0 , respectively. Finally, the voters in the last pair, qi and
qi0 , have preference orders:


qi : C \ ({d}  N (v1i ))  d  N (v1i ),
 
qi0 : N (v1i )  C \ ({d}  N (v1i ))  d.
In effect, the first two pairs of voters jointly give 2(|C|  1) points to each of the candidates.
The last pair gives |C|  1 points to the candidates in N (v1i ) and |C| points to all other
candidates (except d, who receives less than |C|  1 points).
Let  := h(2(|C|1)+|C|)1. Altogether, the vertex-selecting voters give the following
scores to the candidates: The candidates in N (v11 )N (v12 )  N (v1h ) receive  points and
all other candidates, except d, receive  + 1 points (d receives less than  points). Thus, in
the unbribed election v11 , . . . , v1h are the selected vertices.
For each color i, we introduce (ni  1)  ((h  1)  d + 2) shift actions with effect 1
on voter wi and effect +1 on voter wi0 . To understand where the number of these shift
actions comes from, we note that: (1) For each vertex vji , we have |N (vji )| = (h  1)  d (each
vertex is connected with d vertices of each color different than its own), (2) in A(vji ) and in
B(vji ) the candidates from N (vji ) are surrounded by two vertex candidates, and (3) if t is
an integer, 1  t  ni  1, then applying t((h  1)  d + 2) of these shift actions has the effect
that the candidates in N (v1i ) gain one point (i.e., v1i ceases to be selected), the candidates
i ) lose one point (i.e., v i
in N (vt+1
t+1 becomes selected), and no other candidate changes his
score (later we will argue that applying other numbers of such shift actions than multiples
of ((h  1)  d + 2) cannot ensure ps victory).
642

fiCombinatorial Shift Bribery

Edge-Selecting Voters. For the edge-selecting voters, we need the following additional
notation. Let Ex,y denote the set of candidates representing edges between vertices of
color x and color y, that is,
q{0,1,2,3}

Ex,y := {e`

| e` = {vjx , vjy0 }  E}.

We write nx,y to denote the number of edges between vertices of color x and color y. By
idx,y
z we refer to the index of the z-th edge between vertices of color x and y. For example,
if e3 , e7 and e57 are the only three edges between vertices of colors 1 and 2, then n1,2 = 3,
1,2
1,2
id1,2
1 = 3, id2 = 7, id3 = 57.
For each pair {x, y} of distinct colors and each edge eidx,y
, we introduce the following
j
x,y
parts of preference orders (for j = nx,y , we assume that idj+1 = idx,y
1 ):
R(eidx,y
) : e0idx,y  e1idx,y  e2idx,y  e3idx,y ,
j
):
S(eidx,y
j

j

j

j

e0idx,y
j+1

e1idx,y
j

e2idx,y
j





j



e3idx,y .
j+1

For each pair {x, y} of distinct colors we introduce three pairs of voters. The voters in the
0 , have the following preference orders:
first pair, wx,y and wx,y

wx,y : R(eidx,y
)  p  R(eidx,y
)  R(eidx,y
)      R(eidx,y
)  Rx,y ,
nx,y
1
2
3

0
)  S(eidx,y
)      S(eidx,y
)  S(eidx,y
)  p,
wx,y
: Rx,y  S(eidx,y
nx,y
n
1
2
1
x,y

where Rx,y is the set of the remaining candidates, that is, Rx,y := C \ ({p}  Ex,y ). The
0 , have preference orders that are the reverse of w
voters in the second pair, qx,y and qx,y
x,y
0
0 , have
and the reverse of wx,y , respectively. Finally, the voters in the last pair, qx,y and qx,y
the following preference orders:

qx,y : e1idx,y  e2idx,y  d  C \ ({d, e1idx,y , e2idx,y }),
1
1
1
1
12
0
2
1
qx,y : C \ ({d, eidx,y , eidx,y })  eidx,y  eidx,y  d.
1

1

1

1

The first two pairs of voters jointly give 2(|C|  1) points to each of the candidates. The
last pair gives |C| points to both e1idx,y and e2idx,y , and |C|  1 points to all other candidates
1
1
(except d, who receives
less
than
|C|

1
points).

Let  := 3 h2 (|C|  1). Altogether, for each pair of distinct colors {x, y}, the edgeselecting voters give  + 1 points to candidates e1idx,y and e2idx,y . All other candidates receive
1
1
 points (except for d, who receives less than  points). Thus in the unbribed election the
selected edges are exactly the first edges between each pair of colors (that is, edges of the
form eidx,y
, for each pair of distinct colors {x, y}).
1
For each pair {x, y} of distinct colors, we create 4(nx,y  1) shift actions with effect 1
0 . The intuition behind these shift actions is similar
on voter wx,y and effect +1 on voter wx,y
as in the case of vertex-selecting voters. We make the following observations: (1) For each
edge eidx,y there are four candidates listed in R(eidx,y ) and four candidates listed in S(eidx,y ),
`
`
`
and (2) if t is an integer, 1  t  nx,y 1, and we apply 4t such shift actions, then candidates
643

fiBredereck, Faliszewski, Niedermeier, & Talmon

v11

v12

e3

e1

v21

e4

e2

v22

e5

e6
v13

v23

V 1 = {v11 , v21 }, V 2 = {v12 , v22 }, V 3 = {v13 , v23 }, h = 3, d = 1
Figure 2: A 3-colored graph with six vertices where each vertex is adjacent to one vertex
from each of the color classes V 1 , V 2 and V 3 , other than its own.
ceases to be selected), candidates e1idx,y and e2idx,y
e1idx,y and e2idx,y lose one point (edge eidx,y
1
1

t+1

1

t+1

gain one point (edge eidx,y
becomes selected), and the scores of all other candidates remain
t+1
unchanged (we will later argue that if we apply a number of shift actions that is not a
multiple of 4, then p certainly is not a winner of the resulting election).
To conclude the construction, we set the budget B :=  (that is, we can use as many
shift actions as we like). It is easy to verify that the reduction is computable in polynomial
time and that we introduce a number of voters that is a function of h only (thus, it is a
parameterized reduction). Before proving the correctness of this construction, we consider
the following example (we will refer to it during the correctness proof as well).
Example 5. Consider the Strongly Regular Multicolored Clique instance (d, h, G)
with d = 1, h = 3, and graph G from Figure 2. Our construction produces the following set
of candidates:
C := U  U 0  E 0  E 1  E 2  E 3  {p, d},
with
01 02 02 03 03
U = {u11 , u12 , u21 , u22 , u31 , u32 }, U 0 = {u01
1 , u2 , u1 , u2 , u1 , u2 }

and
E i = {ei1 , ei2 , . . . , ei6 }, 0  i  3.
Furthermore, we have:
N (v11 ) := {e11 , e12 },

N (v21 ) := {e13 , e16 },

N (v12 ) := {e23 , e14 },

N (v22 ) := {e21 , e15 },

N (v13 ) := {e22 , e25 },

N (v23 ) := {e24 , e26 }.
644

fiCombinatorial Shift Bribery

For the vertex-selecting group of voters, we create the following voters. For each color
i, we create two voters wi and wi0 :

1
1
1
1
01
w1 : p  u11  e11  e12  u01
1  u2  e3  e6  u2  R ,


1
1
1
01
w10 : R1  u12  e11  e12  u01
2  u1  e3  e6  u1  p,

2
2
2
1
02
w2 : p  u21  e23  e14  u02
1  u2  e1  e5  u2  R ,


2
2
1
02
w20 : R2  u22  e23  e14  u02
2  u1  e1  e5  u1  p,

3
3
2
2
03
w3 : p  u31  e22  e25  u03
1  u2  e4  e6  u2  R ,


3
2
2
03
w30 : R3  u32  e22  e25  u03
2  u1  e4  e6  u1  p,
with Ri := C \ ({p}  U i  U 0i  N (v1i )      N (vni i )), 1  i  3. For each of these voters
we add a voter with reversed preferences. (This means that, so far, all candidates obtain
the same total score.) We finish this group of voters by creating for each color i two voters,
qi and qi0 , with preference orders:


qi : C \ ({d}  N (v1i ))  d  N (v1i ),
 
qi0 : N (v1i )  C \ ({d}  N (v1i ))  d.
This ensures that for each color i, all the candidates in N (v1i ) get  points, and all other
candidates get  + 1 points (except d who gets few points). We create 4 shift actions with
effect 1 on voter wi and effect +1 on voter wi0 .
For the edge-selecting second group of voters, recall that Ex,y denotes the set of candidates representing edges between vertices of color x and color y. Specifically, we have:
E1,2 :={e01 , e11 , e21 , e31 e03 , e13 , e23 , e33 },
E1,3 :={e02 , e12 , e22 , e32 e06 , e16 , e26 , e36 }, and
E2,3 :={e04 , e14 , e24 , e34 e05 , e15 , e25 , e35 }.
0 , as follows:
For each pair {x, y} of distinct colors we create two voters, wx,y and wx,y


w1,2 : e01  e11  e21  e31  p  e03  e13  e23  e33  R1,2 ,

0
w1,2
: R1,2  e01  e13  e23  e31  e03  e11  e21  e33  p,

w1,3 : e02  e12  e22  e32  p  e06  e16  e26  e36  R1,3 ,

0
w1,3
: R1,3  e02  e16  e26  e32  e06  e12  e22  e36  p,

w2,3 : e04  e14  e24  e34  p  e05  e15  e25  e35  R2,3 ,

0
w2,3
: R2,3  e04  e15  e25  e34  e05  e14  e24  e35  p,
where Rx,y := C \ ({p}  E[x, y]). For each of these voters we add a voter with reversed
0
preferences. Further, for each pair {x, y} of distinct colors, we add two voters qx,y and qx,y
645

fiBredereck, Faliszewski, Niedermeier, & Talmon

as follows:

qx,y : e1idx,y  e2idx,y  d  C \ ({d, e1idx,y , e2idx,y }),
1
1
1
1
12
1
2
0
qx,y : C \ ({d, eidx,y , eidx,y })  eidx,y  eidx,y  d.
1

1

1

1

Altogether, for each pair {x, y} of distinct colors, candidates e1idx,y and e2idx,y get  +1 points
1
1
and all other candidates get  points (except d, which gets less points). For each pair {x, y}
of distinct colors, we create 4 shift actions with effect 1 on voter wx,y and effect +1 on
0 .
voter wx,y
4
Properties of the Construction. We now discuss several properties of our construction.
These properties will play a significant rule in showing the correctness of the reduction. To
illustrate our arguments, we come back to our example from time to time. We begin by
looking at the scores of the candidates.
Lemma 1. The following claims hold:
1. In the unbribed election, every candidate receives at most  +  + 2 points and every
candidate from {p}  U  U 0  E 0  E 3 receives exactly  +  + 1.
2. In every bribed election, the score of p is exactly  +  + 1.
3. After applying a successful set of shift actions, the score of p is  +  + 1 and the
scores of all other candidates are at most  +  + 1.
Proof. It is easy to see that the first claim holds based on the discussion that we give
throughout the construction. The second claim holds because (a) applying every shift
action decreases by one the score of p in one vote and increases it by one in another vote
(there are sufficiently few shift actions in the whole instance such that applying each shift
action always moves p within the two votes on which the shift action acts). The last claim
follows directly from the second one.
(Lemma) 
Let us now consider the process of selecting vertices. In the description of vertexselecting voters we said that, initially, for each color i vertex v1i is selected, and if for some
integer t, 1  i  ni  1, we apply t((h  1)  d + 2) shift actions that affect voters wi and wi0 ,
i
then v1i ceases to be selected and vt+1
becomes selected. We now argue that if we apply a
number of these shift actions that is not divisible by ((h  1)  d + 2), then p is not a winner
of the resulting election.
To see that this is the case, recall that in the preference orders of voter wi and wi0 there
are exactly (h  1)  d candidates from E 1  E 2 between each pair of candidates {uij , u0ij }.
Furthermore, if p passes some candidate uij in the preference order of voter wi (increasing
uij s score by one), then it must also pass candidate uij in the preference order of voter wi0
(decreasing uij s score by one). Otherwise, uij would end up with score  +  + 2 and,
by 1, p would not be a winner (there are no possibilities to influence the score of uij other
than shifting p in the preference lists of wi and wi0 ). Hence, p also passes candidate u0ij
and all candidates between uij and u0ij in the preference lists of wi and wi0 . This, however,
means that if p is to be a winner of the election, then the number of applied shift actions
646

fiCombinatorial Shift Bribery

Unbribed voters w2 and w20 :


2
2
2
1
02
w2 : p  u21  e23  e14  u02
1  u2  e1  e5  u2  R


2
2
1
02
w20 : R2  u22  e23  e14  u02
2  u1  e1  e5  u1  p
Applying two shift actions with effect -1 on w2 and +1 on w20 :
+1 +1

2
2
2
1
02
w2 : u21  e23  p  e14  u02
1  u2  e1  e5  u2  R
2
-1
-1


2  e2  p  e1  u02
w20 : R2  u22  e23  e14  u02

u
2
1
1
5
1
+2
Applying (h  1)  d + 2 = 4 shift actions with effect -1 on w2 and +1 on w20 :
+1 +1 +1 +1

2
2
2
1
02
w2 : u21  e23  e14  u02
1  p  u2  e1  e5  u2  R
4
-1
-1
-1
-1

2
0
2
2  e2  e1  u02
w2 : R  u2  e23  e14  u02

p

u
2
1
1
5
1
+4
Figure 3: Illustration of bribery actions affecting the first voter group of our running example
(Example 5). Note that, in the unbribed election, every candidate from U  U 0 obtains
 +  + 1 points in total. For each color i there is only one type of shift actions which
affects voter wi and wi0 : those shift actions with effect 1 on voter wi and effect +1 on
voter wi0 . No other shift action can affect some voter from the first group. Applying a
multiple of ((h  1)  d + 2) shift actions with effect 1 on voter wi and effect +1 on voter wi0
ensures that the candidates from U i  U 0i receive at most  +  + 1 points in total, whereas
applying any other number of these shift actions implies that some candidate from U i
receives  +  + 2 points and, hence, p cannot win. We illustrate this with color 2 in our
running example.
with effects on voters wi and wi0 is a multiple of ((h  1)  d + 2) (p passes candidate uij ,
candidate u0ij , and h  d candidates in between). Figure 3 provides an illustration of the
above reasoning.
We next discuss selecting edges. As for the case of vertex-selecting voters, in the description of our construction we have argued that (a) initially for each pair {x, y} of distinct
colors, edge eidx,y
is selected and that (b) after applying 4t, 1  t  nx,y  1, shift actions
1
0 , e x,y ceases to be selected and e x,y becomes selected. We
that affect voters wx,y and wx,y
id1
idt+1
now argue that if we used a number of such shift actions that is not a multiple of four, then
p certainly would not be a winner of the election.
0
To see that this is the case, note that we designed the preference orders of wx,y and wx,y
so that the candidates e0idx,y and e3idx,y , for j  {2, . . . , nx,y }, follow p in vote wx,y in the
j

j

0 . In effect, if we apply a shift action that affects
same order in which they precede p in wx,y

647

fiBredereck, Faliszewski, Niedermeier, & Talmon

0 :
Unbribed voters w2,3 and w2,3


w2,3 : e04  e14  e24  e34  p  e05  e15  e25  e35  R2,3

0 :R2,3  e0  e1  e2  e3  e0  e1  e2  e3  p
w2,3
4
5
5
4
5
4
4
5
0 :
Applying two shift actions with effect -1 on w2,3 and +1 on w2,3
+1 +1

w2,3 : e04  e14  e24  e34  e05  e15  p  e25  e35  R2,3
2
-1
-1



0
2,3
0
1
2
3
0
1
2
w2,3 :R  e4  e5  e5  e4  e5  e4  p  e4  e35
+2
0 :
Applying four shift actions with effect -1 on w2,3 and +1 on w2,3
+1 +1 +1 +1

w2,3 : e04  e14  e24  e34  e05  e15  e25  e35  p  R2,3
4
-1
-1
-1
-1

0 :R2,3  e0  e1  e2  e3  p  e0  e1  e2  e3
w2,3
4
5
5
4
5
4
4
5
+4

Figure 4: Illustration of bribery actions affecting the second voter group of our running
example. Note that in the unbribed election, every candidate from E 0  E 4 obtains  +
 + 1 points in total. For each pair of colors x and y there is only one type of shift
0 : those shift actions with effect 1 on voter w
actions which affects voter wx,y and wx,y
x,y
0 . No other shift action can affect some voter from the second
and effect +1 on voter wx,y
group. Applying a multiple of 4 shift actions with effect 1 on voter wx,y and effect +1 on
0
ensures that the candidates from E 0  E 4 receive at most  +  + 1 points from
voter wx,y
these voters, whereas applying any other number of these shift actions implies that some
candidate from E 0 receives  +  + 2 points and, hence, p cannot win. We illustrate this
with color pair 2 and 3 in our running example.
0
voters wx,y and wx,y
a number of times that is not a multiple of four, then one of these
candidates obtains  +  + 2 points. Since there is no other way to affect the score of these
candidates, by Lemma 1, in this case p cannot be a winner. We illustrate this effect in
Figure 4.

Solution for Example 5. Before we complete the correctness proof, let us illustrate the
solution for our example.
The unbribed election selects vertex v11 , v12 , and v13 and the edges e1 , e2 and e4 . Hence,
for example, candidate e24 receives + +2 points and p (who receives only + +1 points)
is not a winner.
By applying four shift actions with effect 1 on w2 and effect +1 on w20 , we select v22
instead of v12 to be the vertex of color 2 in our clique (as depicted in the bottom of Figure 3).
648

fiCombinatorial Shift Bribery

0 , we select e
By applying four shift actions with effect 1 on w2,3 and effect +1 on w2,3
5
instead of e4 to be the edge between color 2 and color 3 in our clique (as depicted in the
bottom of Figure 4). Now, each candidate from {e11 , e12 , e21 , e22 , e15 , e25 } receives  + 1 points
from the edge-selecting voters, but only  points from the vertex-selecting voters. Every
other candidate receives at most  + 1 points from the vertex-selecting voters and at most
 points from the edge-selecting voters. Hence, p (with  +  + 1 points) is a winner. This
solution corresponds to the left 3-colored triangle in Figure 2.

Correctness. It remains to show that there is a successful set of shift actions for the
constructed Borda-Combinatorial Shift Bribery instance if and only if there is an
h-colored clique in graph G.
For the if part, assume that there is an h-colored clique H  V (G). Without loss of
generality, let H = {vz11 , . . . , vzhh } and let EH := {{v, v 0 } | v, v 0  H}. Furthermore, let zx,y
denote the index of the edge in Ex,y representing the edge from EH between the vertex of
 EH . Then it is
color x and the vertex of color y. That is, zx,y = j if and only if eidx,y
j
easy to verify that the following set of shift actions is successful:
1. For each color i  [h], include (zi  1)((h  1)  d + 2) shift actions with effects on
voters wi and wi0 .
2. For each pair {x, y} of distinct colors, include 4(zx,y  1) shift actions with effects on
0 .
voters wx,y and wx,y
In other words, we select the vertices and the edges corresponding to the clique. In effect,
the scores of all the candidates is  +  + 1 (except for d, who receives lower score). So p
is among the tied winners.
For the only if part, assume that there is a successful set of shift actions and consider
the election after applying these shift actions. By construction, we know that edge-selecting
voters pick exactly one edge for each pair of distinct colors. Hence the graph induced
by these edges contains vertices with h different colors. If this graph contains only h
vertices, then this graph must be an h-colored clique (this graph cannot contain fewer than
h vertices). For the sake of contradiction, let us assume that this graph contains more than
h vertices. Thus there are two selected edges, ej and ej 0 , incident to two different vertices,
vi  ej and vi0  ej 0 , of the same color. By our construction (and the way vertex-selecting
voters work), for at least one of the sets N (vi ) and N (vi0 ) all candidates in the set receive
 + 1 points from the vertex-selecting voters. However, since both ej and ej 0 are selected
by the edge-selecting voters, these voters give  + 1 points to each of the candidates e1j , e2j ,
e1j 0 , and e2j 0 . Hence, at least one of these candidates receives  +  + 2 points in total and,
by Lemma 1, p is not a winner. This is a contradiction, and so the graph induced by the
selected edges must be an h-colored clique.

References
Bartholdi, III, J. J., Tovey, C. A., & Trick, M. A. (1992). How hard is it to control an
election. Mathematical and Computer Modelling, 16 (89), 2740.
649

fiBredereck, Faliszewski, Niedermeier, & Talmon

Baumeister, D., Faliszewski, P., Lang, J., & Rothe, J. (2012). Campaigns for lazy voters:
Truncated ballots. In Proceedings of the 11th International Conference on Autonomous
Agents and Multiagent Systems (AAMAS 12), pp. 577584. IFAAMAS.
Betzler, N., Bredereck, R., Chen, J., & Niedermeier, R. (2012). Studies in computational
aspects of votinga parameterized complexity perspective. In The Multivariate Algorithmic Revolution and Beyond, Vol. 7370 of LNCS, pp. 318363. Springer.
Binkele-Raible, D., Erdelyi, G., Fernau, H., Goldsmith, J., Mattei, N., & Rothe, J. (2014).
The complexity of probabilistic lobbying. Discrete Optimization, 11, 121.
Boutilier, C., Brafman, R. I., Hoos, C. D. H. H., & Poole, D. (2004). CP-nets: A tool
for representing and reasoning with conditional ceteris paribus preference statements.
Journal of Artificial Intelligence Research, 21, 135191.
Brandt, F., Harrenstein, P., Kardel, K., & Seedig, H. G. (2013). It only takes a few: On the
hardness of voting with a constant number of agents. In Proceedings of the 12th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 13),
pp. 375382. IFAAMAS.
Bredereck, R., Chen, J., Faliszewski, P., Nichterlein, A., & Niedermeier, R. (2014a). Prices
matter for the parameterized complexity of shift bribery. In Proceedings of the 28th
AAAI Conference on Artificial Intelligence (AAAI 14), pp. 13981404. AAAI Press.
Bredereck, R., Chen, J., Hartung, S., Kratsch, S., Niedermeier, R., Suchy, O., & Woeginger,
G. (2014b). A multivariate complexity analysis of lobbying in multiple referenda.
Journal of Artificial Intelligence Research, 50, 409446.
Bredereck, R., Faliszewski, P., Niedermeier, R., Skowron, P., & Talmon, N. (2015a). Elections with few candidates: Prices, weights, and covering problems. In the Fourth
International Conference on Algorithmic Decision Theory (ADT 2015), Vol. 9346 of
LNCS, pp. 414431. Springer.
Bredereck, R., Faliszewski, P., Niedermeier, R., & Talmon, N. (2015b). Large-scale election campaigns: Combinatorial shift bribery. In Proceedings of the 14th International
Conference on Autonomous Agents and Multiagent Systems (AAMAS15), pp. 6775.
Bredereck, R., Faliszewski, P., Niedermeier, R., & Talmon, N. (2016). Complexity of shift
bribery in committee elections. In Proceedings of the Twenty-Ninth AAAI Conference
on Artificial Intelligence (AAAI 16).
Bulteau, L., Chen, J., Faliszewski, P., Niedermeier, R., & Talmon, N. (2015). Combinatorial
voter control in elections. Theoretical Computer Science, 589, 99120.
Cary, D. (2011). Estimating the margin of victory for instant-runoff voting. Presented at the
2011 Electronic Voting Technology Workshop/Workshop on Trustworthy Elections.
Chen, J., Faliszewski, P., Niedermeier, R., & Talmon, N. (2015). Elections with few voters: Candidate control can be easy. In Proceedings of the 29th AAAI Conference on
Artificial Intelligence (AAAI 15), pp. 20452051.
Christian, R., Fellows, M. R., Rosamond, F. A., & Slinko, A. (2007). On complexity of
lobbying in multiple referenda. Review of Economic Design, 11 (3), 217224.
650

fiCombinatorial Shift Bribery

Conitzer, V., Lang, J., & Xia, L. (2009). How hard is it to control sequential elections via
the agenda?. In Proceedings of the 21st International Joint Conference on Artificial
Intelligence (IJCAI 10), pp. 103108. AAAI Press.
Dorn, B., & Schlotter, I. (2012). Multivariate complexity analysis of swap bribery. Algorithmica, 64 (1), 126151.
Downey, R. G., & Fellows, M. R. (2013). Fundamentals of Parameterized Complexity.
Springer.
Elkind, E., & Faliszewski, P. (2010). Approximation algorithms for campaign management.
In Proceedings of the 6th International Workshop on Internet and Network Economics
(WINE 10), Vol. 6484 of LNCS, pp. 473482. Springer.
Elkind, E., Faliszewski, P., & Slinko, A. (2009). Swap bribery. In Proceedings of the 2nd
International Symposium on Algorithmic Game Theory (SAGT 09), Vol. 5814 of
LNCS, pp. 299310. Springer.
Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2010). Using complexity to protect
elections. Communications of the ACM, 53 (11), 7482.
Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. A. (2009a). How hard is bribery
in elections?. Journal of Artificial Intelligence Research, 35, 485532.
Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L. A., & Rothe, J. (2009b). Llull and
Copeland voting computationally resist bribery and constructive control. Journal of
Artificial Intelligence Research, 35, 275341.
Faliszewski, P., Reisch, Y., Rothe, J., & Schend, L. (2014). Complexity of manipulation,
bribery, and campaign management in Bucklin and Fallback voting. In Proceedings
of the 13th International Conference on Autonomous Agents and Multiagent Systems
(AAMAS 14), pp. 13571358. IFAAMAS.
Faliszewski, P., & Rothe, J. (2015). Control and bribery in voting. In Brandt, F., Conitzer,
V., Endriss, U., Lang, J., & Procaccia, A. D. (Eds.), Handbook of Computational
Social Choice, chap. 7. Cambridge University Press.
Flum, J., & Grohe, M. (2006). Parameterized Complexity Theory. Springer.
Gabow, H. N. (1983). An efficient reduction technique for degree-constrained subgraph and
bidirected network flow problems. In Proceedings of the 15th Annual ACM Symposium
on Theory of Computing (STOC 83), pp. 448456. ACM.
Garey, M. R., & Johnson, D. S. (1979). Computers and Intractability: A Guide to the Theory
of NP-Completeness. Freeman.
Hazon, N., Lin, R., & Kraus, S. (2013). How to change a groups collective decision?.
In Proceedings of the 23rd International Joint Conference on Artificial Intelligence
(IJCAI 13), pp. 198205. AAAI Press.
Hulett, H., Will, T. G., & Woeginger, G. J. (2008). Multigraph realizations of degree
sequences: Maximization is easy, minimization is hard. Operations Research Letters,
36 (5), 594596.
651

fiBredereck, Faliszewski, Niedermeier, & Talmon

Lang, J., & Xia, L. (2015). Voting in combinatorial domains. In Brandt, F., Conitzer, V.,
Endriss, U., Lang, J., & Procaccia, A. D. (Eds.), Handbook of Computational Social
Choice, chap. 9. Cambridge University Press.
Lenstra, H. W. (1983). Integer programming with a fixed number of variables. Mathematics
of Operations Research, 8 (4), 538548.
Magrino, T., Rivest, R., Shen, E., & Wagner, D. (2011). Computing the margin of victory in IRV elections. Presented at the 2011 Electronic Voting Technology Workshop/Workshop on Trustworthy Elections.
Mathieson, L., & Szeider, S. (2012). Editing graphs to satisfy degree constraints: A parameterized approach. Journal of Computer and System Sciences, 78 (1), 179191.
Mattei, N., Goldsmith, J., & Klapper, A. (2012a). On the complexity of bribery and manipulation in tournaments with uncertain information. In Proceedings of the 25th International Florida Artificial Intelligence Research Society Conference (FLAIRS 12),
pp. 549554. AAAI Press.
Mattei, N., Pini, M., Rossi, F., & Venable, K. (2012b). Bribery in voting over combinatorial
domains is easy. In Proceedings of the 11th International Conference on Autonomous
Agents and Multiagent Systems (AAMAS 12), pp. 14071408. IFAAMAS.
Niedermeier, R. (2006). Invitation to Fixed-Parameter Algorithms. Oxford University Press.
Obraztsova, S., & Elkind, E. (2011). On the complexity of voting manipulation under
randomized tie-breaking. In Proceedings of the 22nd International Joint Conference
on Artificial Intelligence (IJCAI 11), pp. 319324. AAAI Press.
Obraztsova, S., Elkind, E., & Hazon, N. (2011). Ties matter: Complexity of voting manipulation revisited. In Proceedings of the 10th International Conference on Autonomous
Agents and Multiagent Systems (AAMAS 11), pp. 7178.
Rajagopalan, S., & Vazirani, V. V. (1998). Primal-dual RNC approximation algorithms
for set cover and covering integer programs. SIAM Journal on Computing, 28 (2),
525540.
Reisch, Y., Rothe, J., & Schend, L. (2014). The margin of victory in Schulze, Cup, and
Copeland elections: Complexity of the regular and exact variants. In Proceedings of the
Seventh European Starting AI Researcher Symposium (STAIRS-2014), pp. 250259.
IOS Press.
Schlotter, I., Faliszewski, P., & Elkind, E. (2011). Campaign management under approvaldriven voting rules. In Proceedings of the 25th AAAI Conference on Artificial Intelligence (AAAI 11), pp. 726731. AAAI Press.
Xia, L. (2012). Computing the margin of victory for various voting rules. In Proceedings
of the 13th ACM Conference on Electronic Commerce (EC 12), pp. 982999. ACM
Press.

652

fiJournal of Artificial Intelligence Research 55 (2016) 10251058

Submitted 07/15; published 04/16

Semi-supervised Learning with Induced Word Senses for
State of the Art Word Sense Disambiguation
Osman Baskaya

obaskaya@ku.edu.tr

Department of Computer Sciences and Engineering
Koc University
Istanbul, Turkey

David Jurgens

jurgens@stanford.edu

Department of Computer Science
Stanford University
Stanford, CA, USA

Abstract
Word Sense Disambiguation (WSD) aims to determine the meaning of a word in context, and successful approaches are known to benefit many applications in Natural Language Processing. Although supervised learning has been shown to provide superior WSD
performance, current sense-annotated corpora do not contain a sufficient number of instances per word type to train supervised systems for all words. While unsupervised
techniques have been proposed to overcome this data sparsity problem, such techniques
have not outperformed supervised methods. In this paper, we propose a new approach to
building semi-supervised WSD systems that combines a small amount of sense-annotated
data with information from Word Sense Induction, a fully-unsupervised technique that
automatically learns the different senses of a word based on how it is used. In three experiments, we show how sense induction models may be effectively combined to ultimately
produce high-performance semi-supervised WSD systems that exceed the performance of
state-of-the-art supervised WSD techniques trained on the same sense-annotated data. We
anticipate that our results and released software will also benefit evaluation practices for
sense induction systems and those working in low-resource languages by demonstrating
how to quickly produce accurate WSD systems with minimal annotation effort.

1. Introduction
Word Sense Disambiguation (WSD) identifies the particular meaning of a word in context,
such as whether bass refers to a fish or an instrument. Correctly performing WSD grounds
ambiguous natural language in a concrete semantic representation, which has numerous
benefits to downstream applications, such as knowledge extraction (Navigli & Ponzetto,
2010; Hartmann, Gurevych, & Lap, 2013) and machine translation (Carpuat & Wu, 2007;
Chan, Ng, & Chiang, 2007). Traditionally, supervised approaches to WSD have offered
superior performance (Kilgarriff & Rosenzweig, 2000; Mihalcea, Chklovski, & Kilgarriff,
2004; Agirre & Soroa, 2007; Navigli, 2009). However, a major limitation for building highperformance supervised WSD systems has been the limited amount of sense-annotated
corpora for training models on all word types, with the largest such corpora containing
only hundreds of thousands of annotated tokens (Petrolito & Bond, 2014). As a result,
unsupervised WSD techniques have been proposed to fill the need for high-coverage systems
c
2016
AI Access Foundation. All rights reserved.

fiBaskaya & Jurgens

(Yarowsky, 1995; Agirre et al., 2014; Moro et al., 2014). While these techniques are capable
of disambiguating many word types, they have not surpassed the accuracy of supervised
systems nor do they take advantage of what sense-annotated data is available.
An alternative approach to supervised WSD is to build semi-supervised approaches using
Word Sense Induction (WSI), often referred to as Word Sense Induction and Disambiguation (WSID) models (Agirre et al., 2006). Word Sense Induction is a fully unsupervised
technique that examines the contexts in which a word is used in order to learn (a) the words
different meanings, referred to as its induced senses, and (b) how to disambiguate a new
usage of the word as an instance of one of those induced senses. The induced senses learned
by a WSI method provide the key link for building a WSID system: by labeling usages with
both induced senses and those from reference sense inventory such as WordNet (Fellbaum,
1998) or OntoNotes (Hovy et al., 2006), a mapping function can be learned to effectively
transform an annotation using induced senses into an annotation using senses of the reference inventory. When a WSI model has been constructed from a large corpus of examples,
its induced senses become associated with many contextually-disambiguating features. As a
result, when the features associated with an induced sense are used in disambiguation, they
are, through the proxy of sense mapping, being used as features for disambiguating between
the reference senses despite these contextual features potentially never being seen in the
data annotated with references senses. Thus, when a close correspondence exists between
induced and reference senses, a robust WSID system can be created that is ultimately able
to disambiguate usages in contexts unlike those seen in data annotated with reference senses
by virtue of the features associated with the induced senses. In contrast, the discriminatory
capabilities of a supervised WSD system are limited to the features observed in the training data. Hence, the ability of a WSID system to leverage induced sense annotations can
potentially remove the knowledge acquisition bottleneck of requiring significant amounts of
sense-annotated data (Gale, Church, & Yarowsky, 1992).
Despite the potential of WSID, little analysis has been done into how to construct such
models and how to maximize their performance. Instead, WSID systems have primarily
been evaluated within SemEval tasks focusing on word sense induction (Agirre & Soroa,
2007; Manandhar, Klapaftis, Dligach, & Pradhan, 2010; Jurgens & Klapaftis, 2013). While
WSID performance in such evaluations is promising, three important open questions remain.
First, in current evaluations, WSID systems have all used the technique of Agirre et al.
(2006) for converting induced sense annotations into those of a reference inventory. However,
the performance impact of this process has not been measured, nor have alternative methods
been tested. Second, current WSID evaluations have not controlled for the distribution and
frequency of the senses in training and test data, which can significantly affect performance
and the expected generalizability of the results (Agirre & Martinez, 2000); these settings
raise the question of how much of current systems performances are attributable to the ease
of disambiguating due to the test datas sense distribution. Third, despite the potential
advantages of WSID for low-resource languages, no study has directly compared WSID
and supervised WSD systems under equal conditions to test whether one setup should be
preferred based on the amount of sense-annotated data available.
Addressing these questions was previously hindered by the lack of a large sense-annotated
data set. However, we overcome this limitation using the recent resource of Pilehvar and
Navigli (2013), which approximates all polysemous nouns in WordNet by using pseudowords
1026

fiSemi-supervised Learning with Induced Word Senses for State of the Art WSD

to accurately model the difficulty of disambiguating those nouns. Here, a pseudoword is
made of two or more monosemous lemmas, referred to as pseudosenses, each of which
models a particular sense of a word. For example, the disemous noun pic has two WordNet
senses: (1) a motion picture, and (2) a photograph. These two senses are represented by the
monosemous nouns movie and photo, respectively. To simulate sense-annotated data, the
occurrences of a pseudowords pseudosenses are replaced by a unique token (e.g., replacing
usages of movie and photo with a token denoting the pseudoword); then, in the analogous
disambiguation task, a WSD system is shown an occurrence of the pseudoword and asked to
decide which pseudosense was originally present. Crucially, because (1) these pseudowords
approximate the real-world disambiguation difficulty and (2) pseudosense-annotated data
can easily be created by sampling occurrences of the pseudosenses from a corpus, this resource enables performing a comprehensive evaluation of WSID on arbitrarily-large amounts
of annotated data with direct generalizability to real-world WSD performance (Pilehvar &
Navigli, 2014).
This paper offers the following four key contributions. First, we provide a comprehensive evaluation setting for WSID that tests systems on millions of instances two orders of
magnitude more than previous evaluations thereby providing statistically-robust results
for all evaluated terms. Furthermore, our evaluation setting uses high-quality pseudowords
that effectively simulate the properties of WordNet senses, which allows us to precisely
control the sense distribution of both test and training data in order to measure its effect
on performance. Second, we show that the method for transforming induced senses into
WordNet senses has a significant impact on WSID performance, and when using an appropriate method, WSID performance significantly outperforms formerly-competitive baselines
in multiple tests sets. Third, we demonstrate that combining WSI models into an ensemble
WSID provides statistically-significant performance improvements in both pseudoword and
real-world data. Fourth, in direct comparisons with a state-of-the-art supervised WSD system, we demonstrate that an ensemble WSID system outperforms supervised WSD when
fewer than several hundred sense-annotated instances are available, indicating that WSID
can indeed overcome the knowledge acquisition bottleneck.
Our results offer two important practical implications. For researchers working with
low-resource languages, our comparisons between WSID and supervised WSD demonstrate
that only a relatively small amount of sense-annotated data is needed for state-of-theart performance on WSD. Second, we demonstrate that the current approach to creating
WSID systems artificially masks the true capabilities of the underlying WSI models, and
thus future evaluations of WSID systems such as those conducted in SemEval should
consider using the evaluation construction procedure described herein.

2. Word Sense Induction and Disambiguation Systems
A WSID system consists of two key components: a WSI model and a function that converts
the models sense annotations into those of another sense inventory, as formalized by Agirre
et al. (2006). First, a WSI model induces its senses from a base corpus. Second, a training
corpus is labeled using both the induced senses of the WSI model and the senses from a
reference inventory. The co-labeled corpus serves as training data for building a classifier
that predicts the reference sense label given an induced sense annotation.
1027

fiBaskaya & Jurgens

In constructing WSID systems, two key questions have not been examined: (1) the
impact of the sense mapping function on WSID performance, and (2) whether multiple WSI
models may be effectively combined. Answering these questions is essential to identifying
the degree to which the performance of a WSID system is due to the capabilities of its
underlying WSI model versus the mapping process used to create it. In what follows, we
first describe the WSI models used in this paper to illustrate how they learn their senses.
Then, we formalize the sense mapping function and define a range of possibilities for how
it may be computed and propose how the function can be used to effectively combine WSI
models into a WSID ensemble.
2.1 WSI Models
Multiple techniques have been proposed for how to effectively learn the different meanings of
a word (Navigli, 2012), with many approaches using either (a) graph-based representations
of a words semantic relationships or (b) distributional approaches to identifying regularities in the words contexts. Therefore, to increase the robustness of the results, four recent
WSI methods were selected for our experiments. Models were balanced between those using
lexical distributions and those using graphs: AI-KU (Baskaya, Sert, Cirik, & Yuret, 2013)
and HDP (Lau, Cook, McCarthy, Newman, & Baldwin, 2012), which use token statistics
to induce senses, and Chinese Whispers (Biemann, 2006) and SquaT (Di Marco & Navigli,
2012), which construct graphs to induce senses. Following prior evaluations (Manandhar
et al., 2010; Jurgens, 2012; Jurgens & Klapaftis, 2013), for disambiguation, we allow these
models to report multiple senses per context; in this setting, an induced sense denotes a
prototypical meaning and the annotation represents how much the current usage resembles those meanings. Following, we summarize the models induction and disambiguation
procedures.
2.1.1 AI-KU
Baskaya et al. (2013) represent the context of each target word by using high probability
lexical substitutes according to a statistical language model. A language model is built to
identify the relative probabilities of 4-gram sequences and then FastSubs (Yuret, 2012) is applied to identify words that appear in the same position as the target word for each context.
For example, one instance of bass may have substitutes such as fish, while another instance
may have guitar. Each instance is then represented as 100 substitutes, sampled from the
probability distribution of the most-probable 100 substitutes for that instance; these substitutes are transformed into a vector representation, reflecting the sampled frequencies of
each. The instance-substitute vectors are then projected into a lower dimensionality using S-CODE (Maron, Lamar, & Bienenstock, 2010). The final S-CODE based vectors are
clustered using k-means. Much like Schutze (1992), AI-KU requires specifying the number
of clusters ahead of time, often setting k to a larger than necessary number. However, to
determine the number of senses, AI-KU performs a post-processing step to remove clusters
that contain only a few instances, which are likely artifacts of forcing each of the k clusters
to be non-empty. The remaining clusters are treated as senses of the word.
1028

fiSemi-supervised Learning with Induced Word Senses for State of the Art WSD

2.1.2 HDP
Lau et al. (2012) propose a system that based on a Hierarchical Dirichlet Process (HDP)
(Teh, Jordan, Beal, & Blei, 2006), a nonparametric extention of Latent Dirichlet allocation
(Blei, Ng, & Jordan, 2003; Griffiths & Steyvers, 2004). HDP automatically infers both the
number of topics and each topics probability distribution for generating the tokens in the
corpus. For sense induction, a HDP model is inferred from contexts of a target word, which
produces a distribution over topics for each context. Each topic is treated as a distinct
sense of the word. Given a new context of the word, the HDP model can be used to infer
its topic distribution, thereby identifying which senses are present. For output, we report
the full distribution over senses for each context, weighted by their probabilities.
2.1.3 Chinese Whispers (CW)
Biemann (2006) proposes inducing senses using the Chinese Whispers (CW), a nonparametric graph clustering algorithm. CW is a form of unsupervised label propagation where
each vertex is initially assigned a unique label and then label propagation is run until either
convergence or a fixed number of iterations have completed. When the graph is constructed
of lexically-associated terms, vertices assigned to the same cluster typically form topicallyrelated groups. For sense induction, a graph is constructed from words associated with
a specific term (e.g., by computing statistical associations from a corpus) and after CW
completes, each vertex cluster is considered as features of a distinct sense of the term. For
CW, the graphs were constructed in three steps. First, the 2 association was computed
for all words in the base corpus. Then, words associated with all of a words pseudosenses
are ranked by 2 and the 1000 words with the largest 2 are retained. For each of the
retained neighbors, additional edges are added to each of its 1000 neighbors with the highest 2 , excluding those edges to the pseudosenses. The sense features are then used to
disambiguate new contexts by computing their overlap with content words of the context.
For disambiguation, we report all senses containing at least one word in a context, weighted
by the number of matching features.
2.1.4 SquaT
Navigli and Crisafulli (2010) construct a co-occurrence graph, which is then pruned to
induce sense clusters. Term co-occurrences in the base corpus are scored using the Dice
2c(w1 ,w2 )
coefficient: For two terms w1 , w2 , Dice(w1 , w2 ) = c(w
where c(w) is the frequency
1 )+c(w2 )
of occurrence. Edges are added to the graph if their Dice coefficient is greater than a
threshold . The graph construction begins with only co-occurrences of the target term
and then proceeds to add edges of the newly-included neighbors. Their framework allows
multiple pruning methods for induction; we adopt only the Squares pruning method, which
was shown to perform best. Simply, edges are removed if the ratio of observed to potential
squares (closed paths of length 4) in which an edge participates is below a threshold .
Once pruned, the resulting disconnected components of the graph denote separate senses.
For efficiency, we use only noun, verb, and adjective lemmas in the graphs. As the resulting
graph produces sets of lemmas associated with each sense, sense disambiguation is performed
in the same way as Chinese Whispers.
1029

fiBaskaya & Jurgens

2.2 Sense Mapping Functions
A mapping function is a supervised classifier that, given an annotation of one or more
induced senses, produces a new sense annotation for the instance using a sense from a
different inventory, with the induced senses essentially acting as features. Agirre et al.
(2006) proposed the first mapping function based on matrix multiplication, which has been
used by the 39 systems participating in WSID shared task evaluations (Agirre & Soroa,
2007; Manandhar et al., 2010; Jurgens & Klapaftis, 2013) and many subsequent papers on
WSID (e.g., see Brody & Lapata, 2009; Klapaftis & Manandhar, 2010; Van de Cruys &
Apidianaki, 2011; Lau et al., 2012; Wang, Bansal, Gimpel, Ziebart, & Yu, 2015). A sense
co-occurrence matrix M is computed from the training corpus, with columns denoting the
n induced senses and rows denoting the m reference senses; the cell M (i, j) records the
two senses co-occurrence frequencies. For sense mapping, an induced sense annotation is
represented as an n-dimensional vector u with non-zero values for the dimensions denoting
the annotated senses. The product uM produces a m-dimensional vector v containing a
distribution over reference senses; the sense with the largest corresponding value in v is the
resulting annotation.
While this mapping function is widely used by WSID systems, it comes with two limitations: (1) all induced senses are considered equally informative for producing the reference
sense annotation, and (2) the weights assigned to a sense annotation are not effectively incorporated when an instances induced labeling has multiple senses (Jurgens, 2012), with both
due in part to the methods relative simplicity as a machine learning technique. Therefore,
in constructing WSID systems, we evaluate six alternate supervised learning algorithms for
performing the mapping function: Support Vector Machines (SVMs) with both linear and
radial basis function (RBF) kernels, Decision Trees based on either entropy or Gini impurity, and naive Bayes classifiers using either Multinomial or Bernoulli distributions. All six
classifiers are trained on feature vectors where each induced sense is a distinct feature and
produce a single sense label in the reference inventory. Feature vectors are weighted with
the values provided by the WSI models in their annotation, except for the SVM classifier,
whose instance weights were scaled into [0,1], and for Bernoulli naive Bayes where all positive values are set to 1 due to its requirement for binary data. Classifiers were implemented
using SciKit (Pedregosa et al., 2011).
2.3 An Ensemble WSID Model
Many WSI models including those used here exploit different sources of lexical information
for inducing senses and thus identify different features for distinguishing those senses. While
prior work on WSD has combined complementary WSD systems to improve performance
with an ensemble model (Pedersen, 2000; Florian & Yarowsky, 2002; Brody, Navigli, &
Lapata, 2006; Sgaard & Johannsen, 2010), no work has pursued an analogous ensemble
approach for WSID.1 We propose a new heterogeneous ensemble WSID system built from
the output of all four WSI models. For each instance, the output of the WSI systems is
combined and the instance is labeled with the induced senses from all systems, as shown
in Figure 1; the combined annotations are then used as features by the mapping function
1. We note that Stevens (2012) suggested using consensus clustering for sense induction as a way of creating
an ensemble; however, no quantitative analysis was performed.

1030

fiSemi-supervised Learning with Induced Word Senses for State of the Art WSD

Single Model WSID
WSI
Sys. A

Input context

At the bank fishing...

Induced
sense A#2

Mapping
Function

WordNet
sense
bank#n#1

Mapping
Function

WordNet
sense
bank#n#1

Ensemble WSID
WSI
Sys. A

Induced
sense A#2

WSI
Sys. B

Induced
sense B#7

WSI
Sys. C

Induced
sense C#1

Figure 1: A comparison of the single-model and ensemble WSID systems, showing how the
ensemble combines the output of multiple WSI models using a single mapping function.
to predict the sense. Because WSI models capture different aspects of the context, the
ensemble-based system can potentially identify induced senses or combinations thereof that
produce a more accurate mapping to senses in the reference sense inventory.

3. Experimental Design
To evaluate WSID and WSD systems, the first two experiments use a common pseudoword
disambiguation task. Following, we describe the task and data.
3.1 Pseudoword Disambiguation
Pseudowords provide an analogous form of polysemous data for evaluating WSD systems.
A pseudoword is made of two or more monosemous lemmas, referred to as its pseudosenses.
The occurrences of these pseudosenses in a corpus are replaced with a unique token. In the
corresponding disambiguation task, a WSD system must decide which of the pseudosenses
was originally present given an occurrence of the token, effectively simulating the traditional
sense disambiguation task. Independently proposed by Gale et al. (1992) and Schutze
(1992), pseudoword disambiguation fills an important evaluation gap when large amounts
of sense-annotated data is unavailable.
However, disambiguation performance on pseudowords is not guaranteed to model the
difficulty of disambiguating real words. For example, Schutze (1998) uses a pseudoword with
the pseudosenses banana and door, which are semantically dissimilar; deciding between such
a words pseudosenses is akin to disambiguating between the sense of a homograph, which
is known to be an easier disambiguation task (Ide & Veronis, 1998; Navigli et al., 2007). In
contrast, most polysemous are not homographs and instead have senses that are semantic
related in some way and therefore may appear in similar contexts (Apresjan, 1974; Rodd,
Gaskell, & Marslen-Wilson, 2002; Palmer, Dang, & Fellbaum, 2007; Martnez Alonso et al.,
2013). Thus, constructing pseudowords from arbitrarily-selected monosemous terms underestimates the difficulty of sense disambiguation and any results based on such pseudowords
would not necessarily generalize.
1031

fiBaskaya & Jurgens

noun
doubles
pic
ca
drawer
tapestry
headshot

pseudosenses
badminton, tennis
movie, photo
calcium, california
desk, treasurer, cartoonist
complexity, cloth, rug
photo, soccer, gunfire

Table 1: Examples of pseudowords for polysemous nouns in WordNet and the monosemous
lemmas that comprise their pseudosenses
Pilehvar and Navigli (2013) propose a solution to the problem of appropriately choosing
pseudosenses such that the disambiguation difficulty mirrors that of real-world data. A pseudoword dataset is created where each pseudoword models the sense properties of one of the
15,935 polysemous nouns in WordNet 3.0. Specifically, pseudosenses were selected to closely
mimic the inter-sense similarities of the corresponding polysemous word by mining WordNets ontology to find monosemous words (pseudosenses) whose structural arrangement had
close correspondence to that of the polysemous words senses in the ontology. Because only
the noun hierarchy of WordNet contains sufficient structure, their dataset was generated
only for nouns.2 While the inclusion of other parts of speech is ultimately desirable, nouns
alone represent a significant challenge for WSD systems and multiple evaluations have focused entirely on disambiguating nouns (e.g., see Navigli, Jurgens, & Vannella, 2013). Table
1 shows example nouns and their corresponding pseudosenses used in the experiments.
The practical utility of these pseudowords was demonstrated by Pilehvar and Navigli
(2014), who showed that the pseudowords disambiguation difficulty accurately mirrored
that of their corresponding polysemous words. Specifically, WSD systems were trained on
the noun portion of the Senseval-3 dataset (Mihalcea et al., 2004) and a dataset made from
those nouns corresponding pseudowords. The resulting disambiguation performance on the
pseudosense-annotated dataset was highly correlated with the performance on the Senseval3 data. Their results indicate that by using their pseudowords, pseudosense-annotated
datasets can be used to closely approximate real-world WordNet WSD performance, thereby
avoiding the performance over-estimates caused by early methods of constructing pseudowords (Gaustad, 2001).
3.2 Data
All experiments were performed on a subset of data of Pilehvar and Navigli (2013). The
original dataset includes pseudosenses that are likely to introduce noise in the results due
to errors from part of speech tagging or when the word takes part in a named entity that
was not present in WordNet. Therefore, to control for possible sources of noise, we exclude
pseudosenses where (1) the lemma is also the plural form of another lemma, e.g., spirits, (2)
2. However, we note that in principle, such pseudowords could be constructed from the comparatively
shallower verb hierarchy in WordNet and potentially for adjectives using the data of Tsvetkov et al.
(2014).

1032

fiSemi-supervised Learning with Induced Word Senses for State of the Art WSD

F(Pseudowords with correspondence  X)

All Pilehvar and Navigli (2013) pseudowords
Pseudowords used in this study
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1

10

100

1000

Degree of pseudoword correspondence with real data
(lower is better)

Figure 2: The cumulative distribution functions for the degree of correspondence between a
word and its derived pseudoword, as specified in the dataset of Pilehvar and Navigli (2013).
Lower values indicate a closer correspondence.

the lemma may be another part of speech, e.g., freezing, and (3) the lemma occurs in fewer
than 1,000 contexts in Gigaword when not part of a named entity. To test for the third
condition, we used TreeTagger (Schmid, 1994) to identify named entity mentions when part
of speech tagging the corpus. The third criteria is necessary to ensure a sufficient number
of instances are available for training and testing, which is discussed later in section 4.1.
The dataset provides a rating for each pseudoword indicating how closely its pseudosenses model the senses of the corresponding word in WordNet. For example, the pseudoword doubles shown in Table 1 has two pseudosenses, the monosemous words tennis and
badminton, which closely model its two senses (1) badminton played with two players,
and (2) tennis played with two players. Replacing one of these pseudosenses with the
monosemous word desk would lower the resulting pseudowords degree of correspondence
since desk is not similar to either of the words senses and thus, the disambiguation task
would be potentially easier due to the dissimilarity of the contexts in which the pseudosenses
appear.
The subset of their data used in our experiments was selected as follows. First, pseudowords are filtered according to the three aforementioned criteria. Second, the remaining
pseudowords are ranked according to their degree of correspondence. Third, 920 senses
were selected from this ranking to match the distribution of polysemy values in WordNet
(e.g., having the same percentage of disemous lemmas). This third step was performed
in order to ensure that the degrees of polysemy in our dataset are representative of the
distribution in the full dataset of Pilehvar and Navigli (2013). Figure 2 shows the degree of
correspondence between real words and (a) the pseudowords for used in our study and (b)
those in the full dataset of Pilehvar and Navigli (2013), highlighting that the subset used
in our experiments has significantly higher correspondence to real-world data than the full
dataset and therefore is maximally representative of the expected real-world performance.
1033

fiBaskaya & Jurgens

WordNet

Pseudowords

12000

700

Number of words

500
8000
400
6000
300
4000
200
2000

Number of pseudowords

600

10000

100

0

0
0

5

10
15
20
25
30
Number of senses (degree of polysemy)

35

Figure 3: Distributions of the number of senses for polysemous nouns in WordNet and
number of pseudosenses for our selected pseudowords, which were chosen to closely match
the polysemy distribution in WordNet
Pseudowords in the final dataset had between two and twelve pseudosenses. Figure 3
shows the polysemy distribution of the number of senses for our selected pseudowords, compared with the polysemy distribution for all nouns in WordNet. Because the mapping functions described previously in Section 2.2 are parametric, five additional high-correspondence
disemous pseudowords were also selected to use in parameter tuning, as this number of senses
was the most common in our dataset and therefore most representative.
3.3 Sense Distributions
The frequency distribution of a words senses is often peaked, with one or two senses occurring more frequently than the rest (Passonneau, Salleb-Aouissi, & Ide, 2009). The particular
sense distribution of a word can greatly affect WSID performance, with artificially-inflated
performance in settings where one sense occurs more frequently and all induced senses are
mapped to that sense; in such cases, WSID performance is not generalizable to datasets
where the sense distribution may vary. Controlling for the effect of the sense distribution in
real-world test data requires a significant number of annotated instances from which to select, which is not currently possible with existing annotated corpora. However, when using
pseudowords, the sense distribution may be precisely controlled by gathering the required
number of usages of each pseudosense to match a desired distribution.
Precisely controlling the sense distribution allows us to measure WSID performance
within two extremes. In the first distribution, we leverage the correspondence between the
pseudowords senses and WordNet senses to simulate real-world sense distributions based
on SemCor (Miller et al., 1993).
Specifically, for each noun with a corresponding pseudoword, we measure the frequencies
of that nouns senses in SemCor, which determines the relative frequency of each of the
pseudowords pseudosenses. However, some words are still too infrequent in SemCor to
1034

fiSemi-supervised Learning with Induced Word Senses for State of the Art WSD

accurately measure the expected frequency of their senses. Therefore, words with fewer
than ten occurrences use the average sense distribution computed from all words having
both the same polysemy and at least ten occurrences in SemCor. We refer to the resulting
dataset as having a SemCor sense distribution.
The SemCor distribution measures the difficulty of WSID in the expected setting where
some senses are more likely to occur than others. However, the presence of a majority
class can potentially mask important underlying performance differences between systems;
because one sense is more likely, a models performance is not necessarily representative
of its ability to distinguish between all senses of a word. Therefore, for the second distribution, all of a words senses appear with uniform probability. When both training and
test data have a uniform sense distribution, WSD systems cannot use the often-effective
strategy of always choosing the most-frequent sense seen in the training data. Furthermore,
the Uniform-distributed data allows measuring the ability of the sense mapping function
to find a correspondence between induced and reference senses when induced senses have
equivalent amounts of data. While the uniform sense distribution is not representative of
real-world data, a comparison between a models performances on SemCor- and Uniformdistributed datasets provides critical insight into its disambiguation capabilities and its
expected generalizability to new data with arbitrary perturbations in the underlying sense
distribution. For example, if a model performs well on SemCor-distributed data but not
on Uniform-distributed data, this result suggests that the model is only effective at identifying the most-frequent sense; in contrast, models that perform well on both SemCor- and
Uniform-distributed data would be expected to maintain its accuracy on data with other
sense distributions based on its ability to discriminate between the senses when no one sense
is more frequent.

4. Experiment 1: Evaluating WSID Mapping
The first experiment measures the impact of the sense mapping function in two ways. First,
given the wide-spread use of the Agirre et al. (2006) mapping function, we assess whether any
of the six alternatives described in Section 2.2 can consistently improve WSID performance.
Second, we assess whether the proposed sense mapping functions can effectively fuse the
induced sense annotations of multiple WSI models to produce an accurate ensemble model.
4.1 Experimental Setup
In what follows, we detail the parameters and training of the WSI systems and how the
training and test data was constructed.
4.1.1 WSID Systems
WSI models were trained on the same base corpus, ukWaC (Baroni, Bernardini, Ferraresi,
& Zanchetta, 2009), though we emphasize that models induce senses from the corpus in
different ways. The same WSI parameter values were used for all pseudowords. AI-KU
uses the settings for the language model, S-CODE and Fastsubs (Yuret, 2012) algorithms
as reported by Baskaya et al. (2013). Using the same setup for SemEval 2013 WSI task,
AI-KU calculates the lexical substitutes by using SRILM (Stolcke, 2002) with the ukWaC
1035

fiBaskaya & Jurgens

Training partitions

Training instances

Data partitions
Uniform(
Test partition

Test instances
SemCor(

(a)

(b)

)

)
(c)

Figure 4: A schematic of the cross validation. Data partitions initially contain an equal
number of instances per pseudosense (a), shown as different colored boxes. For each fold of
validation, four partitions are used for training and one for test (b). The instances from each
partition are then sampled according to a distribution (shown in italics), which produces
the training and test datasets (c).

(Ferraresi, Zanchetta, Baroni, & Bernardini, 2008) as a corpus to construct its 4-gram
language model. For the k-means algorithm used by AI-KU, k was arbitrarily set at 10,
with no further parameter tuning. HDP uses the two parameters to specify the variability
of senses in the corpus,  and 0 , which were set to the same values reported by Lau
et al. (2012) and Lau, Cook, and Baldwin (2013). The SquaT parameters  and  were set
to 0.00125 and 0.25, respectively, after a limited grid search showed these values produced
sufficiently large graphs for all pseudowords. The Chinese Whispers model is nonparametric,
so no parameter choices are needed.
In total, twenty eight WSID configurations were built from each combination of the
seven sense mapping functions (Sec. 2.2) and four WSI models (Sec. 2.1). Additionally,
seven ensemble WSID systems were built by training each of the mapping functions on the
induced sense labelings from all four WSI systems using their default configuration.
4.1.2 Cross-Validation Evaluation
Systems were evaluated using five-fold cross validation, with modifications to ensure that
folds were of comparable sizes between distribution types and that no training data leaked
into test data when changing the sense distribution of the test data. Initially, the corpus of
all instances of a pseudoword is divided into five partitions, where each partition contains
the same number of instances of each pseudosense. The instances of each partition are then
filtered to match a desired sense distribution; this filtering process is deterministic so that
a partition always has the same instances for a particular distribution across folds. Figure
4 visualizes this process. For evaluation, four filtered partitions form the training data and
one partition is used as test data. Importantly, this setup ensures that its instances remain
consistent when the partition is used in different folds of validation. We note that in the
case of the ensemble WSID system, the underlying WSI models are trained on the same
training data from identical folds, ensuring the separation between test and training data.
1036

fiSemi-supervised Learning with Induced Word Senses for State of the Art WSD

The reported experiments use the same sense distributions in both training and testing (either SemCor or Uniform). However, our evaluation setup is sufficiently general to
support using arbitrary distributions, including different distributions between training and
testing data, as shown in the example in Figure 4; results using additional combinations of
distributions are reported in the Supplementary Material.
4.1.3 Evaluation Data
All data for the partitions was drawn from the Gigaword corpus (Graff, Kong, Chen, &
Maeda, 2003). Instances of pseudosenses were filtered to ensure the correct part of speech
and to remove all occurrences where the pseudosense was part of a named entity. Ultimately
each partition in the test data contained 200 instances of all senses, which were filtered
according to the desired distribution. For SemCor-distributed data, the most frequent sense
has 200 instances, with all other senses having proportional numbers based on their relative
sense frequencies. We note that this setup was chosen instead of using a fixed number
of instances per partition so as not to bias the results against more polysemous words
whose rarer sense would have comparatively fewer instances in the fixed-size setting and in
which case, the WSID accuracy would not be significantly affected by the models ability
to identify such senses. Because the number of instances varies in the SemCor-distributed
data, the corresponding Uniform-distributed data for a pseudoword was balanced to have
the same total size, evenly distributed between senses.
Two baselines are used with the test data: Random and Most Frequent Sense (MFS).
The Random baseline simply picks randomly from among the senses; the MFS baseline
selects the most frequent sense of the word, which often performs competitively in skewed
distributions such as SemCor and has outperformed many WSID models in previous studies
(McCarthy et al., 2004; Kilgarriff, 2004; Navigli, 2009). Note that in the Uniform sense
distribution, the MFS and Random baselines are equivalent.
4.1.4 Scoring
Systems were evaluated using the standard WSD precision, recall and F1 metrics (Navigli,
2009). Precision measures the percentage of sense assignments provided by a WSID system
that are identical to the gold standard. Recall measures the percentage of all instances that
are correctly labeled by the system. When a system labels all instances, precision and recall
are equivalent. Because the number of instances per term scales with the number of senses,
precision and recall can be considered microaverages of WSD performance across words.
4.1.5 Parameter Tuning
Five disemous words were used to tune each parametric mapping function. Using a grid
search over parameter values, each WSID configuration was scored using an identical fivefold cross-validation process. The parameter values that produced the highest average F1
across all folds were selected for use in the WSID models mapping function.
1037

fiBaskaya & Jurgens

SquaT
CW

AI-KU
HDP

Ensemble
MFS

Random

1

F1 Score

0.9
0.8
0.7
0.6
0.5
0.4

e

e

re

.T

ec

D
re

i)

in

)
py

ro
nt

(G

(E

)

BF

(R
)

r
ea

B

6)

0
20

lN

B

.(

N

al

in
(L

.T

ec

D

M
SV

M
SV

lli

et

ia
om
tin

ul

M

ou
rn
Be

e

irr
Ag

Figure 5: Average performance of all WSID systems when training and testing data follow
the SemCor sense distribution

4.2 Results and Discussion
The results of WSID systems using a single WSI model demonstrate that high sense disambiguation performance is possible when using a suitable sense mapping function and that
multiple WSI models may be effectively combined into an ensemble.
4.2.1 Single-Model Results
The WSID system evaluation showed a clear impact from the choice in mapping function.
Results for all untuned WSID systems using the SemCor distribution are shown in Figure
5.3 For nearly all systems, the Agirre et al. (2006) method mapped all induced senses to
the most-frequent sense seen in the SemCor-distributed training data, ignoring any sense
distinctions recognized by the WSI model. While the Agirre et al. method does produce
WSID systems that outperform those using the Multinomial Nave Bayes, the performance
says little about the discriminative capabilities of the WSID systems and effectively prevents
meaningful comparison, hindering the testing and development of new WSID systems.
In contrast to the performance when using the Agirre et al. mapping function, the SVM
and Bayesian functions both produced two WSID systems that outperformed the MFS
baseline. The best performance when using a single WSI model comes from SVM with
a linear kernel, which provides slightly higher performance than a RBF kernel. Indeed,
a WSID system using the AI-KU model and a linear kernel SVM mapping function has a
3. For all WSID systems, tuning the parameters of the mapping function provided little to no performance
improvement in either sense distribution. We therefore omit the tuned results here for brevity, but report
these scores in the Supplementary Materials.

1038

fiSemi-supervised Learning with Induced Word Senses for State of the Art WSD

SquaT
CW

AI-KU
HDP

Ensemble
MFS

Random

1

F1 Score

0.9
0.8
0.7
0.6
0.5
0.4

e

e

re

.T

ec

D
re

i)

in

)
py

ro
nt

(G

(E

)

BF

(R
)

B

6)

0
20

lN

r
ea

in
(L

.T

ec

D

M
SV

M
SV

B

.(

N

al

ia
om
tin

ul

M

lli

et

ou
rn
Be

e

irr
Ag

Figure 6: Average performance of all WSID systems when training and testing data follow
the Uniform sense distribution
3.8% increase in F1 score over the MFS baseline, which is statistically significant at p < 106
using McNemars test of significance.
The impact of the choice in sense mapping function on WSID performance is even
more evident in the Uniform-distribution dataset. The results, shown in Figure 6, reveal
significant differences in the discriminatory capabilities of WSID systems. WSID systems
using Agirre et al. mapping function perform well on average, indicating that the function
is capable of learning an effective correspondence between senses when no single sense
dominates in frequency. Nevertheless, all WSI models enjoy consistently-higher performance
when using a SVM mapping function, which all have a statistically significant improvement
at p < 106 . Furthermore, even the worst-performing model, SquaT, is still able to more than
double the performance of the MFS baseline when using SVM or Decision Tree mapping
functions.
The single-model results on the Uniform distribution also provide insight into how models would be expected to perform on new datasets where sense distributions differ from
that in the SemCor-distributed data. Systems performances were relatively close when
tested on the SemCor dataset and differed by at most 0.04 F1 with the linear-kernel SVM;
in contrast, systems differed by more than 0.278 F1 with the Uniform-distributed data,
indicating significant differences in the WSI models abilities to find meaningful sense distinctions. Furthermore, a clear difference is seen between distributional and graph-based
WSI approaches, suggesting that distributional techniques may be more robust to potential
changes in a corpuss sense distribution.
The overall ranking between individual-model WSID systems is consistent across the
different mapping functions. However, some rank oscillation does appear between the HDP
and AI-KU models in the Uniform distribution setting and between the CW and SquaT
1039

fiBaskaya & Jurgens

models in the SemCor distribution setting. In both cases, the SVM-based mapping functions
provide the highest average performance across systems and produce identical rankings. As
such, we view the ranking differences with other mapping functions to be an artifact of
the mapping function itself, rather than due to actual performance differences between the
systems.
4.2.2 Ensemble-Model Results
In nearly all WSID configurations, the ensemble WSID system obtains substantial performance gains over both the MFS baseline and the best WSID system built from a single
WSI model. For SemCor-distributed data (Fig. 5), the ensemble with a linear kernel SVM
produces the highest performance of all WSID configurations, achieving a 9.4% increase in
F1 over the MFS and 4.2% increase over the next-closest system (AI-KU). Furthermore,
except when using the Agirre et al. (2006) and Multinomial Nave Bayes mapping functions,
the ensemble WSID systems outperforms all individual WSID systems. When testing and
training on a Uniform sense distribution, the ensemble WSID system achieves even more
substantial gains over other WSID systems, as shown in Figure 6, with a 13.6% increase in
F1 over the next-closest system.
The results for both distributions indicate that using a linear kernel SVM for sense
mapping provides consistently superior WSID performance that is robust to variations in
the choice of WSI model. Furthermore, if the annotations with induced senses contain
complementary sources of information, as in the case of the ensemble sense labeling, the
SVM mapping function is able to produce better quality sense annotations.
The success of the ensemble with using all mapping functions other than the method of
Agirre et al. (2006) highlights a potential obstacle in the research community: While prior
attempts at building ensemble WSID methods may have been considered, any performance
benefit would not have been observed due to the current community-wide practice using the
method of Agirre et al. Further, our work raises the possibility that new mapping functions
could be developed to more-effectively combined induced senses.
4.2.3 Quantifying the Impact of Polysemy on Disambiguation Performance
Given the high performance of using a linear-kernel SVM as a mapping function, we performed a follow-up analysis to measure its performance effect relative to the number of
senses per word. This analysis separates the improvement from the relative difficulty of
disambiguation and provides a more-complete picture of WSID performance. Performances
for pseudowords with six or more senses were combined due to the words relative infrequency. Figures 7 and 8 show the performances per term for SemCor and Uniform sense
distributions, respectively, using a box and whisker plot. Whiskers denote the maximum
and minimum F1 for any pseudoword, boxes denote the first and third quartiles, and the
middle line denotes the median performance. As the baselines performances change with
polysemy, each is plotted as a horizontal line.
As seen in Figures 7 and 8, the ensemble WSID system offers superior performance
across all levels of polysemy. For example, although one sense of disemous words occurs
in the vast majority of instances in the SemCor data, the ensemble WSID performance
is still able to surpass this MFS baseline for nearly all words (as shown in the left-most
1040

fiSemi-supervised Learning with Induced Word Senses for State of the Art WSD

1

F1 Score

0.8

0.6

0.4

0.2

0

SquaT
CW
AI-KU
HDP
2

Ensemble
MFS
Random
3

4
5
Pseudoword Polysemy

6 or more

Figure 7: Performance of WSID systems using a linear-kernel SVM for different polysemy
on SemCor-distributed training and testing data
1

F1 Score

0.8

0.6

0.4

0.2

0

SquaT
CW
AI-KU
HDP
2

Ensemble
MFS
Random
3

4

5

6 or more

Pseudoword Polysemy

Figure 8: Performance of WSID systems using a linear-kernel SVM for different polysemy
on Uniform-distributed training and testing data
cluster of boxes in Figure 7). The results of both settings indicate that the ensemble WSID
model would offer superior performance on new data with arbitrary sense distributions.
Indeed, in the current datasets, most words had either two or three senses (87%), for which
the ensemble WSID model sees its smallest improvement over the MFS. If evaluated on a
1041

fiBaskaya & Jurgens

corpus containing words that have more senses, the overall performance improvement of
the WSID ensemble over the MFS baseline would be even higher than reported in the main
results of Experiment 1 (Figure 5).

5. Experiment 2: Comparing WSID and Supervised WSD
When sufficient sense-annotated data is available, supervised machine learning has typically
been shown to produce the best-performing WSD systems. However, the results of Experiment 1 indicate high WSD performance is also possible using a semi-supervised WSID
model. As both approaches require some amount of sense-annotated data, this raises the
question of under what circumstances should one approach be expected to outperform the
other. Therefore, in Experiment 2, we perform a direct comparison between semi-supervised
WSID systems and the current state of the art for supervised WSD, using identical training
data. The results of this experiment have direct implications for sense annotation efforts in
deciding how much data is necessary for high performance.
5.1 Experimental Setup
In what follows, we describe the configuration of the supervised WSD system used for
comparison and how training data was created.
5.1.1 Supervised WSD
For comparison, we use It-Makes-Sense (IMS) (Zhong & Ng, 2010), a state-of-the-art supervised WSD algorithm. To disambiguate a usage in a sentence-length context, IMS extracts
features consisting of the neighboring lemmas and their POS along with neighboring collocation pairs. IMS uses linear-kernel SVM on these feature vectors for predicting the sense. In
our experiments, IMS was trained using the default algorithmic parameter values specified
in its publicly-available implementation.
The experiments are intentionally not measuring the disambiguation ability of the fullytrained IMS system provided by its authors;4 rather, the experiments are intended to directly compare the results using the current state-of-the-art supervised WSD algorithm.
While it would be possible to retrain WSID systems on the training data used by the authors fully-trained model, the annotated corpora used in the original experiments are not
readily available nor are the sense distributions of those corpora controlled for, making the
conclusions of such an experiment difficult to generalize.
5.1.2 Training and Test Data
For Experiment 2, multiple datasets are created with increasing amounts of training data
in order to measures the ability of WSID and supervised WSD in each condition. The
datasets are generated similarly to how instances were allocated for sense distributions
in Experiment 1. For a pseudoword, SemCor-distributed training data is constructed by
selecting k instances for its most frequent sense, with other senses assigned a proportional
number of instances based on their relative frequency. Because of the different number
4. http://www.comp.nus.edu.sg/~nlp/software.html

1042

fiSemi-supervised Learning with Induced Word Senses for State of the Art WSD

AIKU
HDP

CW
SquaT

Ensemble
IMS

MFS

F1 Score

1

0.9

0.8

0
75

0
50

0
25
0
20

0
15

0
10

75

50

25

10

k

Figure 9: Performance of IMS and WSID systems on SemCor-distributed data

of instances per word in the SemCor-distributed training data, the Uniform-distributed
data is created in such a way to account for the difference: Given a specific k and word
with n total instances for its m senses, the corresponding Uniform-distributed training
n
data is constructed by including m
instances for each of the pseudowords m senses. For
notational clarity, we use k to denote the equivalently-sized Uniform-distributed dataset
whose corresponding SemCor-distributed training data has k instances.
Training and test data were generated from the same Gigaword data used in previous
experiments, using five folds for cross validation. Training data was generated for k = {10,
25, 50, 75, 100, 150, 200, 250, 500, 750}. Both WSID and IMS systems were trained on the
k and k datasets created from four partitions and then tested on the fifth, full partition.
Because the test set is identical to that of Experiment 1 and all instances are used in
testing, the resulting performances for each k and k are directly comparable to the results
of Experiment 1.
5.1.3 WSID Systems
WSID systems were constructed using the same procedure used in previous experiments
(cf. Sec. 4.1). For simplicity, we report only WSID systems using a linear kernel SVM, as
these provided the highest performance. Full results for all other configuration are available
in the Supplementary Material.
5.2 Results and Discussion
The results reveal that WSID systems can offer superior performance over IMS when few
annotated instances are available. Figures 9 and 10 show the resulting F1 scores for
1043

fiBaskaya & Jurgens

AIKU
HDP

CW
SquaT

Ensemble
IMS

MFS
Random

0.9

F1 Score

0.8

0.7

0.6

0.5

0
75

0
50

0
25
0
20

0
15

0
10

75

50

25

10

k^

Figure 10: Performance of IMS and WSID systems on Uniform-distributed data

SemCor- and Uniform-distributed data, with x-axis drawn at log scale. The random baseline
(F1=0.432) is omitted from Figure 9 for better visual contrast.
In SemCor-distributed data, IMS outperforms all single-model WSID systems for nearly
all values of k, though the AI-KU and HDP models are closely competitive differing by less
than 1% F1 for k  75. In contrast to the single-model WSID systems, the ensemble WSID
system outperforms IMS starting at k=25 until just after k=500. All ensemble performance
differences 25 k 250 are statistically significant at p < 106 and the IMS and WSID
performances at k=500 are statistically equivalent. Given that the publicly-distributed IMS
system was trained on an average of 35 instances per word type, our results suggest that
training an ensemble WSID model on the same data would provide superior performance.
When very few training instances are available, for most cases words, the IMS algorithm
was able to correctly learn the back-off strategy of always selecting the most frequent sense
from the training data, thereby ensuring it performs at least as well as the MFS baseline.
In contrast, the mapping function for WSID models is slightly noisier and does not learn
an accurate mapping from induced senses to reference senses, resulting in performance just
below the MFS baseline.
A similar trend is seen when testing on Uniform-distributed data (Figure 10), though
the ensemble WSID system outperforms IMS at k = 10 until k=200, at which point they
are statistically equivalent, and the HDP model initially outperforms IMS as well until just
after k = 25. The results of the Uniform-distributed setting indicate that WSID models
can provide accurate discriminatory techniques.
Together these results suggest that ensemble WSID models can offer significant advantages over supervised WSD except when very little or large amounts of sense-annotated data
are available. Indeed, all but 97 of the 11,685 polysemous lemmas in SemCor have fewer
than 200 instances, which suggests that the ensemble WSID system may offer better performance than existing supervised systems trained only on that corpus. These results also
1044

fiSemi-supervised Learning with Induced Word Senses for State of the Art WSD

indicate that by using the unsupervised features of the WSI models, the ensemble WSID
system is able to break the knowledge acquisition bottleneck and acquire more information
for disambiguation than available from annotated data alone.
Last, we note that increasing the amount of training data consistently improves the
performance of IMS, while providing decreasing benefit to WSID models. This contrast
highlights the difference in how both systems learn. Training a WSID model on additional
sense-annotated data cannot directly improve disambiguation performance as the contextual
features used for disambiguation are fixed by the underlying WSI model, which is independent of the training data. In contrast, providing the same data to a supervised WSD system
may enable it to learn new features for disambiguation. Nevertheless, the performance of
WSID does depend on having sufficient sense-annotated data to train a correct mapping
function, as shown in the large performance improvements between k=10 and k=25 shown
in Figures 9 and 10 where the increase in training data provides a substantial improvement
in sense mapping.

6. Experiment 3: Evaluation with SemEval Systems
Experiments 1 and 2 demonstrated that WSID models are capable of accurate WSD and
that when individual WSI models are combined into an ensemble, the resulting system is
capable of outperforming fully-supervised WSD. However, both experiments were performed
in controlled conditions on pseudoword data. Therefore, in Experiment 3, we test whether
the observed performance improvements carry over to real-world data. In three tests using
over thirty WSI models and two sense inventories, we evaluate the impact of our new
mapping function and ensemble construction in extensions of prior WSID evaluations.
6.1 Experimental Setup
To evaluate the ensemble WSID setup with sense-annotated data, we use the three SemEval
tasks have included a WSID evaluation: 2007 Task 2 (Agirre & Soroa, 2007), 2010 Task 10
(Manandhar et al., 2010), and 2013 Task 13 (Jurgens & Klapaftis, 2013). We repeat each
tasks exact evaluation setup, with the exception that the sense mapping function originally
used by a task is replaced with an SVM using a linear kernel.
Two significant differences exist in the tasks setup compared with our earlier experiments. The 2007 and 2010 tasks use OntoNotes senses (Hovy et al., 2006), which are known
to be more coarse-grained than the WordNet senses that the pseudowords models. Second,
the 2013 task also focuses on instances where multiple meanings may be evident (e.g., due
to ambiguity or syllepsis) and therefore includes some gold standard data where instances
have multiple sense labels. Because our experimental setup has focused only on instances
with one sense interpretation, we adopt the single-sense evaluation described by Jurgens
and Klapaftis (2013) using the subset of 4122 instances in the task data that have a single
sense annotation.
Ensemble systems were created using the induced sense answers from the systems that
participated in each task and a linear kernel SVM to perform the sense mapping. We
intentionally use the original WSI models rather than the four models used in our earlier
experiments in order to test the benefits of the proposed WSID configuration in different settings and to quantify its generalizability. However, we note that the two highest-performing
1045

fiBaskaya & Jurgens

Ensembles
SemEval

MFS

Best System

All-Systems

Best-Configuration

2007
2010
2013

0.787
0.587
0.477

0.816
0.624
0.640

0.828
0.680
0.640

0.670
0.657

Table 2: A comparison of the best-performing system in each SemEval WSID task and our
proposed ensemble method.
WSI systems in the prior experiments, AI-KU and HDP, also participated in the 2013 task,
so the ensemble results of that task are expected to be similar. For each task, we consider
two ensembles: (1) the outputs of all WSI systems, and (2) the outputs of the best configuration of each system, measured according to their WSID performance in the original
task. We note that the 2007 task allowed only one configuration per system, so only one
ensemble is produced.
6.2 Results
The results of both ensemble and single WSI-model systems, described next, demonstrate
the benefits of using the new WSID construction procedure.
6.2.1 Ensemble Results
For all three tasks, the ensemble WSID configuration shows performance improvements over
both the best-performing system and MFS. Table 2 shows the results for all three tasks,
including the scores of best-performing system in each task originally and the tasks MFS
score. Improvements over MFS were all significant at p < 106 using McNemars test of
significance. Similarly, the improvements over the best systems in each task are significant
at p < 106 for all ensemble configurations, with the exception of the ensemble for SemEval2007, which is significant at p < 104 . Furthermore, we note that the improvement by
the ensemble in each task is larger than the difference between the tasks best and secondbest systems, indicating a substantial increase in performance. These results demonstrate
consistent performance improvements for an SVM-based ensemble WSID model even when
using different sense inventories and entirely different sets of WSI systems.
6.2.2 Single-Model Results
Single-system WSID models varied in whether the use of an SVM mapping function improved performance, as shown in Figure 11.5 For SemEval-2007, systems obtained a lower
F1 when using the SVM, with an average decrease of 0.032 but no change in the overall
system ranking. In contrast, systems performed better for the 2010 and 2013 tasks when
using an SVM, with average F1 increases of 0.043 and 0.004. Although these mixed trends
initially seem contradictory to the prior experiments results showing a consistent benefit
from the SVM, the performance differences are partly due to differences in the task setup
5. Full score details are available in the Supplementary Material.

1046

fiSemi-supervised Learning with Induced Word Senses for State of the Art WSD

0.8

0.7
Linear-kernel SVM

0.7
Linear-kernel SVM

Linear-kernel SVM

0.9

0.6
0.5
0.4
0.3

0.7
0.7

0.8
0.9
Agirre et al. (2006)

(a) SemEval-2007

0.6

0.5
0.3

0.4 0.5 0.6 0.7
Agirre et al. (2006)

(b) SemEval-2010

0.5

0.6
0.7
Agirre et al. (2006)

(c) SemEval-2013

Figure 11: Comparisons of the F1 scores for each system in the SemEval tasks when using
the linear-kernel SVM mapping function (y-axis) versus that of Agirre et al. (2006) (x-axis).
Points above the diagonal indicate an improvement when using the SVM function.

where most WSI systems are designed to label each usage with only a single induced sense.
In contrast, the systems in our prior experiments reported multiple induced senses per instance, weighted by how applicable the sense was to the instance. The multiple senses
provide a richer feature set for training and enable recognizing cases where lower-weighted
induced senses provide information on the correct sense annotation. When the WSI system
reports only a single sense, WSID system performance has an the upper-bound based on
the reference sense with the highest conditional probability, given an induced sense.
Even when a single induced sense is reported, using a SVM mapping function can still
significantly impact resulting performance, as shown in the 2010 task. Here, multiple systems in the lowest ranked achieved significant improvements in F1 with some seeing over
0.30 absolute increases. The performances differences also highlight a unique feature of the
2010 task; Pedersen (2010) submitted four systems that generated random sense assignments, which were ranked as high as 18th out of the 26 systems. The SVM-based ranking
correctly assigns the four random-answer submissions to the lowest four ranks. Indeed,
the overall system ranking for the task changes dramatically from the originally-reported
ranking (Spearmans =0.14). Our results (Fig. 11b) suggest that while all systems are performing above random chance, the systems actually differ little in their abilities on the task
and that some previously low-ranked systems actually offer superior performance. Thus,
while the overall performance on the task is not as high, the SVM-based moel reveals the
true discriminatory capabilities of the WSI systems, which were partially masked by having
many induced senses mapped to the most frequent reference sense, artificially increasing
performance.

7. Related Work
The present study touches upon three bodies of prior work on word sense induction and its
relationship with WSD, semi-supervised WSD, and work on pseudowords.
7.1 Word Sense Induction and Disambiguation
Purandare and Pedersen (2004) and Niu, Li, Srihari, and Li (2005a) produce sense induction models and then assign the induced senses directly to reference senses, rather than
1047

fiBaskaya & Jurgens

creating a mapping function that converts induced sense annotations. Both report finding
induced senses that closely correspond to existing definitions in reference sense inventories
but neither analyze the performance at disambiguating new instances with reference senses,
which is the role of WSID systems in this study. As noted in Section 2, Agirre et al. (2006)
first formalized the WSID process. In their experiments, a WSID system was built from
the HyperLex WSI model (Veronis, 2004) and their mapping function; the resulting system
obtained a 0.06 improvement in F1 score over the MFS baseline with default parameters
and a further 0.11 improvement over MFS when tuned, suggesting that high WSID performance is possible. Last, Jurgens (2012) notes the potential for having WSI models annotate
items with multiple senses and proposes a modification to the mapping function of Agirre
et al. to improve performance when multiple induced senses annotation is weighted. In
their experiments, WSID systems with this new mapping function were able to outperform
a MFS baseline.
7.2 Pseudowords
Since first being proposed for word sense disambiguation (Gale et al., 1992; Schutze, 1992),
pseudowords have been incorporated into evaluations for multiple tasks, with specific recommendations for how to improve their construction for tasks such as modeling selectional
preferences (Chambers & Jurafsky, 2010), modeling word co-occurrence (Dagan, Lee, &
Pereira, 1999), machine translation (Duan, Zhang, & Li, 2010), tasks in Information Retrieval (Stokoe, 2005) and even improving word embeddings (Liu, Liu, Chua, & Sun, 2015).
Indeed for WSD, new techniques have been proposed for adapting pseudowords to other
languages such as Chinese (Lu, Ting, & Sheng, 2004; Lu, Wang, Yao, Liu, & Li, 2006)
and for creating pseudowords that more accurately model the difficulty of WSD (Nakov &
Hearst, 2003; Otrusina & Smrz, 2010; Pilehvar & Navigli, 2013), though only the approach
of Pilehvar and Navigli (2013) which is used here was shown to closely correlate with
real-world performance.
Most related to our study are those work analyzing word senses using pseudowords.
Cook and Hirst (2011) simulate the sense properties of lemmas in the Senseval-3 lexical
sample task (Mihalcea et al., 2004) in order to model the process by which lemmas acquire
new senses; however, pseudowords are analyzed by contextual features rather than using
WSD as done in this study. To test the discriminatory ability of WSI models, Jurgens and
Stevens (2011) create a set of disemous pseudowords where the pseudosenses have varying
degrees of similarity. To represent the full range of pseudosense similarities, the similarity of
the words pseudosenses was measured using corpus-based distributional similarity and then
pseudosenses were paired into a pseudoword based on having similar corpus frequencies and
positions along the similarity spectrum. Sense induction models were tested according to
their ability to discriminate pseudosenses at different similarity levels. However, the pseudosenses used in their study were not monosemous which limits the ability to replicate their
effect in new corpora, which may potentially have different sense distributions of the polysemous pseudosenses. Last, the most similar study is that of Pilehvar and Navigli (2014),
which used the same pseudoword dataset as here to analyze supervised and unsupervised
WSD. Their findings corroborate those in this study, indicating that the pseudowords of
1048

fiSemi-supervised Learning with Induced Word Senses for State of the Art WSD

Pilehvar and Navigli (2013) can be used to design WSD-related evaluations that mirror
real-world performance.

7.3 Semi-supervised WSD
Beyond WSID, other approaches have applied semi-supervised learning to WSD. Mihalcea (2004) applies co-training and self-training to the supervised classifier of Lee and Ng
(2002), showing that both techniques can reduce the disambiguation error for many words
by using high-confidence automatically-labeled examples; however, both techniques required
parameter tuning, with no parameter set providing high performance for all words. Pham,
Ng, and Lee (2005) investigate four semi-supervised techniques, showing that while spectral graph transduction co-training performed best, performance was not has high as that
from purely-supervised WSD methods. Rather than use automatically-labeled data, Yuret
(2007) generates new contexts from the existing training data using lexical substitution,
which ultimately did not improve in performance when more than a few new substitutes
were added. In contrast, other works have seen some improvement over fully-supervised
systems using semi-supervised techniques. Niu, Ji, and Tan (2005b) construct a graph of
word uses, with edges weighted by the usages contextual similarity. The annotated instances are labeled in the graph with their senses and then label propagation is run on the
graph to infer all remaining instances labels, with the resulting performance being superior
to a SVM-based WSD comparison system. Similarly, Kubler and Zhekova (2009) were able
to filter automatically-annotated data based on its expected quality to further supplement
the training data. The combination of the manually- and automatically-annotated data
provided a slight improvement over the original data, though they note their approach was
only able to automatically annotate a small number of contexts per word, illustrating a main
challenge for semi-supervised learning of significantly increasing the number of training instances. Martinez, De Lacalle, and Agirre (2008) identify monosemous synonyms of target
nouns using WordNet and then query for examples these synonyms on the Web to create a
corpus of automatically sense-annotated examples for training. Because WSD performance
is closely related to the distribution of word senses, additional heuristics are used to estimate the sense distribution of the testing data (McCarthy et al., 2004) when training a
supervised WSD system on the automatically-produced data. The resulting system attained
significantly higher performance than unsupervised systems but was still outperformed by
some fully-supervised systems.
A common thread of these works is the need for extensive filtering of the unlabeled
instances in order to obtain performance improvements; including too many or lower-quality
examples typically resulted in performance below supervised technique trained on the same
data. In contrast, several of the WSID systems tested here outperformed state of the art
without the need for filtering the instances used by the WSI algorithms. The difference in
need for filtering suggests that WSI may be more robust to noise in the unlabeled instances
or that WSI could even be a potential preprocessing step for finding the instances most
paradigmatic of induced senses for later use as input into semi-supervised techniques.
1049

fiBaskaya & Jurgens

8. Conclusion
This paper presents a comprehensive analysis of the construction and evaluation of WSID
systems. Systems were tested using a novel evaluation design incorporating 920 pseudowords
from the data set of Pilehvar and Navigli (2013), whose pseudosenses closely approximate
the properties and disambiguation difficulty of noun senses in WordNet 3.0. In tests on
over a million instances, we provide three empirical contributions. First, we demonstrate
that the choice of the mapping function used to convert induced senses can significantly
affect WSID performance and that a linear kernel SVM significantly improves upon the
current state of the art practices (Agirre et al., 2006), with performance increases of 3.8%
F1 in some settings. Second, we demonstrate that when using a linear kernel SVM, joining multiple WSI models into an ensemble WSID system yields large improvements, which
were not seen when using prior state of the art (an 8.5% F1 increase). The benefit of
this ensemble setup was further demonstrated in tests on real sense-annotated data using multiple ensemble configurations and different sense inventories, further highlighting
its robustness. Third, in a direct comparison with a state of the art supervised WSD
system (Zhong & Ng, 2010), we demonstrate that an ensemble WSID system offers superior performance over the supervised system using the same training data except when
very few or hundreds of annotated instances are available, suggesting that WSID is a viable mechanism for overcoming the knowledge acquisition bottleneck. To further this line
of research, we have released all implementations of our WSI models and the implementation of our pseudoword testing framework as freely-available open source software at
(https://github.com/osmanbaskaya/mapping-impact). Furthermore, as a practical result of
this effort, we intended to release a large-scale all-words WSID system based on the ensemble
model.
The results of this study motivate three interesting avenues for future work that we plan
to explore. First, our results indicate that only a few annotated instances are necessary for
relatively high WSD performance. Recent work has shown that by controlling for the
difficulty that humans have when annotating the contexts (as measured using Passonneau
& Carpenter, 2014), the quality of the training data and, subsequently, performance of a
WSD system may be improved (Lopez de Lacalle & Agirre, 2015). Together, both findings
suggest that high performance WSID systems could be quickly created by appropriately
curating the instances to be annotated for training data. In future work, we intend to
measure the effect of annotation selection on WSID and examine whether the WSI process
itself might also be informative of which instances to select for human annotation.
Second, our experiments were conducted on English-language pseudowords. In future
work, we plan to develop analogous pseudoword data for WordNet ontologies in other languages (Bond & Foster, 2013) and replicate these experiments on multilingual data to
measure potential language-specific effects when sense-annotated data is sparse. We also
plan to investigate using translation and cross-lingual sense mappings to transfer information from English to other languages as a way of gathering the annotations for these WSD
systems, analogous to what was done for part of speech tagging (Duong et al., 2014).
Third, the examined WSI models were trained and tested on the multi-domain ukWaC
corpus. Typically, WSD has performed much worse when tested on novel domains, which
typically contain dissimilar contexts from those in the training data and in which cases,
1050

fiSemi-supervised Learning with Induced Word Senses for State of the Art WSD

words may have have different dominant senses (Magnini et al., 2002; Preiss & Stevenson,
2013). However, prior works have shown that a small amount of sense-annotation from a
novel domain can significantly improve WSD performance in the new domain (Khapra et al.,
2010). In future work, we will evaluate whether the WSI system can be used to effectively
annotate new instances from the novel domain instead of requiring manual annotation, thus
providing an unsupervised method of domain adaptation.

Acknowledgments
We thank Mohammad Taher Pilehvar for many thoughtful discussions and his assistance
with the pseudoword dataset. We also thank the reviewers for their comments and suggestions.

References
Agirre, E., de Lacalle, O. L., & Soroa, A. (2014). Random walks for knowledge-based word
sense disambiguation. Computational Linguistics, 40 (1), 5784.
Agirre, E., & Martinez, D. (2000). Exploring automatic word sense disambiguation with
decision lists and the web. In Proceedings of the COLING-2000 Workshop on Semantic Annotation and Intelligent Content, pp. 1119. Association for Computational
Linguistics.
Agirre, E., Martnez, D., de Lacalle, O. L., & Soroa, A. (2006). Evaluating and optimizing the parameters of an unsupervised graph-based WSD algorithm. In Proceedings
of TextGraphs: the First Workshop on Graph Based Methods for Natural Language
Processing, pp. 8996. Association for Computational Linguistics.
Agirre, E., & Soroa, A. (2007). Semeval-2007 task 02: Evaluating word sense induction
and discrimination systems. In Proceedings of the Fourth International Workshop on
Semantic Evaluations, pp. 712. ACL.
Apresjan, J. D. (1974). Regular polysemy. Linguistics, 12 (142), 532.
Baroni, M., Bernardini, S., Ferraresi, A., & Zanchetta, E. (2009). The WaCky wide web:
A collection of very large linguistically processed web-crawled corpora. Language
Resources and Evaluation, 43 (3), 209226.
Baskaya, O., Sert, E., Cirik, V., & Yuret, D. (2013). Ai-ku: Using substitute vectors and
co-occurrence modeling for word sense induction and disambiguation. In Proceedings
of Seventh International Workshop on Semantic Evaluation (SemEval), pp. 300306.
Biemann, C. (2006). Chinese whispers: an efficient graph clustering algorithm and its application to natural language processing problems. In Proceedings of the First Workshop
on Graph Based Methods for Natural Language Processing, pp. 7380. Association for
Computational Linguistics.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. The Journal of
Machine Learning Research, 3, 9931022.
1051

fiBaskaya & Jurgens

Bond, F., & Foster, R. (2013). Linking and extending an open multilingual wordnet. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics
(ACL), pp. 13521362.
Brody, S., & Lapata, M. (2009). Bayesian word sense induction. In Proceedings of the 12th
Conference of the European Chapter of the Association for Computational Linguistics
(EACL), pp. 103111. Association for Computational Linguistics.
Brody, S., Navigli, R., & Lapata, M. (2006). Ensemble methods for unsupervised wsd. In
Proceedings of the 21st International Conference on Computational Linguistics and
the 44th annual meeting of the Association for Computational Linguistics (COLINGACL), pp. 97104. Association for Computational Linguistics.
Carpuat, M., & Wu, D. (2007). Improving statistical machine translation using word sense
disambiguation. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pp. 6172. Association for Computational Linguistics.
Chambers, N., & Jurafsky, D. (2010). Improving the Use of Pseudo-Words for Evaluating
Selectional Preferences. In Proceedings of the 48th Annual Meeting of the Association
for Computational Linguistics (ACL). Association for Computational Linguistics.
Chan, Y., Ng, H., & Chiang, D. (2007). Word sense disambiguation improves statistical
machine translation. In Proceedings of the Association for Computational Linguistics.
Association for Computational Linguistics.
Cook, P., & Hirst, G. (2011). Automatic identification of words with novel but infrequent
senses. In Proceedings of the 25th Pacific Asia Conference on Language Information
and Computation (PACLIC), pp. 265274.
Dagan, I., Lee, L., & Pereira, F. C. (1999). Similarity-based models of word cooccurrence
probabilities. Machine Learning, 34 (1-3), 4369.
Di Marco, A., & Navigli, R. (2012). Clustering and diversifying web search results with
graph-based word sense induction. Computational Linguistics, 39 (4).
Duan, X., Zhang, M., & Li, H. (2010). Pseudo-word for phrase-based machine translation. In Proceedings of the 48th Annual Meeting of the Association for Computational
Linguistics (ACL), pp. 148156. Association for Computational Linguistics.
Duong, L., Cohn, T., Verspoor, K., Bird, S., & Cook, P. (2014). What can we get from
1000 tokens? a case study of multilingual pos tagging for resource-poor languages. In
Proceedings of the Conference on Empirical Methods in Natural Language Processing
(EMNLP), pp. 886897. Association for Computational Linguistics.
Fellbaum, C. (Ed.). (1998). WordNet: An Electronic Lexical Database. The MIT Press.
Ferraresi, A., Zanchetta, E., Baroni, M., & Bernardini, S. (2008). Introducing and evaluating
ukWaC, a very large web-derived corpus of English. In In Proceedings of the 4th Web
as Corpus Workshop (WAC).
Florian, R., & Yarowsky, D. (2002). Modeling consensus: Classifier combination for word
sense disambiguation. In Proceedings of Conference on Empirical Methods in Natural
Language Processing (EMNLP), pp. 2532. Association for Computational Linguistics.
1052

fiSemi-supervised Learning with Induced Word Senses for State of the Art WSD

Gale, W. A., Church, K. W., & Yarowsky, D. (1992). Work on statistical methods for
word sense disambiguation. In AAAI Fall Symposium on Probabilistic Approaches to
Natural Language, pp. 5460.
Gaustad, T. (2001). Statistical corpus-based word sense disambiguation: Pseudowords vs.
real ambiguous words. In Proceedings of the ACL Student Research Workshop, pp.
6166. Association for Computational Linguistics.
Graff, D., Kong, J., Chen, K., & Maeda, K. (2003). English Gigaword, LDC2003T05..
Linguistic Data Consortium.
Griffiths, T. L., & Steyvers, M. (2004). Finding scientific topics. Proceedings of the National
Academy of Sciences, 101 (suppl 1), 52285235.
Hartmann, S., Gurevych, I., & Lap, U. K. P. (2013). Framenet on the way to babel: Creating
a bilingual framenet using wiktionary as interlingual connection. In Proceedings of the
51th Annual Meeting of the Association for Computational Linguistics (ACL 2013).
Association for Computational Linguistics.
Hovy, E., Marcus, M., Palmer, M., Ramshaw, L., & Weischedel, R. (2006). OntoNotes: the
90% solution. In Proceedings of the 2006 Conference of the North American Chapter
of the Association for Computational Linguistics on Human Language Technology
(NAACL-HLT), pp. 5760. Association for Computational Linguistics.
Ide, N., & Veronis, J. (1998). Introduction to the special issue on word sense disambiguation:
the state of the art. Computational linguistics, 24 (1), 240.
Jurgens, D. (2012). An Evaluation of Graded Sense Disambiguation using Word Sense
Induction. In Proceedings of the First Joint Conference on Lexical and Computational
Semantics (*SEM). Association for Computational Linguistics.
Jurgens, D., & Klapaftis, I. (2013). SemEval-2013 Task 13: Word Sense Induction for Graded
and Non-Graded Senses. In Proceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval). Association for Computational Linguistics.
Jurgens, D., & Stevens, K. (2011). Measuring the impact of sense similarity on word sense
induction. In Proceedings of the First Workshop on Unsupervised Learning in NLP,
pp. 113123. Association for Computational Linguistics.
Khapra, M., Kulkarni, A., Sohoney, S., & Bhattacharyya, P. (2010). All words domain
adapted wsd: Finding a middle ground between supervision and unsupervision. In
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 15321541. Association for Computational Linguistics.
Kilgarriff, A., & Rosenzweig, J. (2000). Framework and results for english senseval. Computers and the Humanities, 34 (1), 1548.
Kilgarriff, A. (2004). How dominant is the commonest sense of a word?. In Text, Speech
and Dialogue, pp. 103111. Springer.
Klapaftis, I. P., & Manandhar, S. (2010). Word sense induction & disambiguation using
hierarchical random graphs. In Proceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP), pp. 745755. Association for Computational
Linguistics.
1053

fiBaskaya & Jurgens

Kubler, S., & Zhekova, D. (2009). Semi-supervised learning for word sense disambiguation:
Quality vs. quantity.. In Proceedings of the Conference on Recent Advances in Natural
Language Processing (RANLP).
Lau, J. H., Cook, P., & Baldwin, T. (2013). unimelb: Topic Modelling-based Word Sense
Induction. In Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval), pp. 307311. Association for Computational Linguistics.
Lau, J. H., Cook, P., McCarthy, D., Newman, D., & Baldwin, T. (2012). Word sense
induction for novel sense detection. In Proceedings of the 13th Conference of the
European Chapter of the Association for Computational Linguistics (EACL 2012).
Association for Computational Linguistics.
Lee, Y. K., & Ng, H. T. (2002). An empirical evaluation of knowledge sources and learning
algorithms for word sense disambiguation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4148. Association for
Computational Linguistics.
Liu, Y., Liu, Z., Chua, T.-S., & Sun, M. (2015). Topical word embeddings. In Proceedings
of the 29th AAAI Conference on Artificial Intelligence (AAAI).
Lopez de Lacalle, O., & Agirre, E. (2015). Crowdsourced word sense annotations and
difficult words and examples. In Proceedings of the 11th International Conference on
Computational Semantics (IWCS).
Lu, Z., Ting, L., & Sheng, L. (2004). Combining neural networks and statistics for chinese word sense disambiguation. In Proceedings of the Third SIGHAN Workshop on
Chinese Language Processing.
Lu, Z., Wang, H., Yao, J., Liu, T., & Li, S. (2006). An equivalent pseudoword solution to
chinese word sense disambiguation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for
Computational Linguistics (COLING-ACL), pp. 457464. Association for Computational Linguistics.
Magnini, B., Strapparava, C., Pezzulo, G., & Gliozzo, A. (2002). The role of domain information in word sense disambiguation. Natural Language Engineering, 8 (4), 359373.
Manandhar, S., Klapaftis, I. P., Dligach, D., & Pradhan, S. S. (2010). SemEval-2010 Task
14: Word Sense Induction & Disambiguation. In Proceedings of the Fifth International
Workshop on Semantic Evaluation (SemEval), pp. 6368. Association for Computational Linguistics.
Maron, Y., Lamar, M., & Bienenstock, E. (2010). Sphere Embedding: An Application to
Part-of-Speech Induction. In Lafferty, J., Williams, C. K. I., Shawe-Taylor, J., Zemel,
R. S., & Culotta, A. (Eds.), Advances in Neural Information Processing Systems 23
(NIPS), pp. 15671575.
Martinez, D., De Lacalle, O. L., & Agirre, E. (2008). On the use of automatically acquired
examples for all-nouns word sense disambiguation.. Journal of Artificial Intelligence
Resesarch (JAIR), 33, 79107.
Martnez Alonso, H., et al. (2013). Annotation of regular polysemy: an empirical assessment
of the underspecified sense. Ph.D. thesis, Universitat Pompeu Fabra.
1054

fiSemi-supervised Learning with Induced Word Senses for State of the Art WSD

McCarthy, D., Koeling, R., Weeds, J., & Carroll, J. (2004). Finding Predominant Word
Senses in Untagged Text. In Proceedings of the 42nd Annual Meeting on Association
for Computational Linguistics (ACL), p. 279, Morristown, NJ, USA. Association for
Computational Linguistics.
Mihalcea, R. (2004). Co-training and self-training for word sense disambiguation. In Proceedings of the Conference on Natural Language Learning (CoNLL). Association for
Computational Linguistics.
Mihalcea, R., Chklovski, T., & Kilgarriff, A. (2004). The Senseval-3 English Lexical Sample Task. In Proceedings of the Third International Workshop on the Evaluation
of Systems for the Semantic Analysis of Text (Senseval), pp. 2528. Association for
Computational Linguistics.
Miller, G. A., Leacock, C., Tengi, R., & Bunker, R. T. (1993). A semantic concordance. In
Proceedings of the workshop on Human Language Technology, pp. 303308. Association
for Computational Linguistics.
Moro, A., Raganato, A., & Navigli, R. (2014). Entity linking meets word sense disambiguation: a unified approach. Transactions of the Association for Computational
Linguistics, 2, 231244.
Nakov, P. I., & Hearst, M. A. (2003). Category-based pseudowords. In Proceedings of the
2003 Conference of the North American Chapter of the Association for Computational
Linguistics on Human Language Technology (NAACL-HLT), pp. 6769. Association
for Computational Linguistics.
Navigli, R. (2009). Word sense disambiguation: A survey.
(CSUR), 41 (2), 10.

ACM Computing Surveys

Navigli, R. (2012). A quick tour of word sense disambiguation, induction and related approaches. In SOFSEM 2012: Theory and practice of computer science, pp. 115129.
Springer.
Navigli, R., & Crisafulli, G. (2010). Inducing word senses to improve web search result clustering. In Proceedings of the Conference on Empirical Methods in Natural Language
Processing (EMNLP), pp. 116126. Association for Computational Linguistics.
Navigli, R., Jurgens, D., & Vannella, D. (2013). Semeval-2013 task 12: Multilingual word
sense disambiguation. In Proceedings of the 7th International Workshop on Semantic
Evaluation (SemEval).
Navigli, R., Litkowski, K. C., & Hargraves, O. (2007). Semeval-2007 Task 07: Coarse-grained
English All-words Task. In Proceedings of the 4th International Workshop on Semantic
Evaluations (SemEval), pp. 3035. Association for Computational Linguistics.
Navigli, R., & Ponzetto, S. P. (2010). Babelnet: Building a very large multilingual semantic
network. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 216225. Association for Computational Linguistics.
Niu, C., Li, W., Srihari, R. K., & Li, H. (2005a). Word independent context pair classification model for word sense disambiguation. In Proceedings of the Ninth Conference
on Computational Natural Language Learning (CoNLL), pp. 3339. Association for
Computational Linguistics.
1055

fiBaskaya & Jurgens

Niu, Z., Ji, D., & Tan, C. (2005b). Word sense disambiguation using label propagation based
semi-supervised learning. In Proceedings of the 43rd Annual Meeting on Association
for Computational Linguistics (ACL), pp. 395402. Association for Computational
Linguistics.
Otrusina, L., & Smrz, P. (2010). A new approach to pseudoword generation.. In Proceedings of the Seventh International Conference on Language Resources and Evaluation
(LREC).
Palmer, M., Dang, H. T., & Fellbaum, C. (2007). Making fine-grained and coarse-grained
sense distinctions, both manually and automatically. Natural Language Engineering,
13 (02), 137163.
Passonneau, R. J., & Carpenter, B. (2014). The benefits of a model of annotation. Transactions of the Association for Computational Linguistics, 2, 311326.
Passonneau, R., Salleb-Aouissi, A., & Ide, N. (2009). Making sense of word sense variation. In Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent
Achievements and Future Directions.
Pedersen, T. (2000). A Simple Approach to Building Ensembles of Naive Bayesian Classifiers
for Word Sense Disambiguation. In Proceedings of the 1st North American chapter
of the Association for Computational Linguistics conference (NAACL), pp. 6369.
Association for Computational Linguistics.
Pedersen, T. (2010). Duluth-WSI: SenseClusters Applied to the Sense Induction Task of
SemEval-2. In Proceedings of the 5th International Workshop on Semantic Evaluations, pp. 363366.
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel,
M., Prettenhofer, P., Weiss, R., Dubourg, V., et al. (2011). Scikit-learn: Machine
learning in python. The Journal of Machine Learning Research, 12, 28252830.
Petrolito, T., & Bond, F. (2014). A survey of WordNet annotated corpora. In Proceedings
of the Seventh Global Wordnet Conference, pp. 236245.
Pham, T. P., Ng, H. T., & Lee, W. S. (2005). Word sense disambiguation with semisupervised learning. In Proceedings of the 19th AAAI Conference on Artificial Intelligence (AAAI).
Pilehvar, M. T., & Navigli, R. (2013). Paving the way to a large-scale pseudosense-annotated
dataset. In Proceedings of the 2013 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies (NAACLHLT), pp. 11001109. Association for Computational Linguistics.
Pilehvar, M. T., & Navigli, R. (2014). A large-scale pseudoword-based evaluation framework
for state-of-the-art word sense disambiguation. Computational Linguistics, 40 (4), 837
881.
Preiss, J., & Stevenson, M. (2013). Unsupervised domain tuning to improve word sense
disambiguation.. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies
(NAACL-HLT), pp. 680684. Association for Computational Linguistics.
1056

fiSemi-supervised Learning with Induced Word Senses for State of the Art WSD

Purandare, A., & Pedersen, T. (2004). Word Sense Discrimination by Clustering Contexts
in Vector and Similarity Spaces. In Proceedings of the Conference on Computational
Natural Language Learning (CoNLL), pp. 4148. Boston.
Rodd, J., Gaskell, G., & Marslen-Wilson, W. (2002). Making sense of semantic ambiguity:
Semantic competition in lexical access. Journal of Memory and Language, 46 (2),
245266.
Schmid, H. (1994). Probabilistic part-of-speech tagging using decision trees. In Proceedings
of the International Conference on New Methods in Language Processing.
Schutze, H. (1992). Dimensions of meaning. In Proceedings of Supercomputing 92, pp.
787796.
Schutze, H. (1998). Automatic word sense discrimination. Computational Linguistics, 24 (1),
97123.
Sgaard, A., & Johannsen, A. (2010). Robust semi-supervised and ensemble-based methods
in word sense disambiguation. In Advances in Natural Language Processing, pp. 401
405. Springer.
Stevens, K. (2012). Evaluating unsupervised ensembles when applied to word sense induction. In Proceedings of ACL 2012 Student Research Workshop, pp. 2530. Association
for Computational Linguistics.
Stokoe, C. (2005). Differentiating homonymy and polysemy in information retrieval. In Proceedings of the conference on Human Language Technology and Empirical Methods in
Natural Language Processing (HLT-EMNLP), pp. 403410. Association for Computational Linguistics.
Stolcke, A. (2002). SRILM  An Extensible Language Modeling Toolkit. In Proceedings
International Conference on Spoken Language Processing vol. 2, pp. 901904.
Teh, Y. W., Jordan, M. I., Beal, M. J., & Blei, D. M. (2006). Hierarchical dirichlet processes.
Journal of the American Statistical Association, 101 (476), 15661581.
Tsvetkov, Y., Schneider, N., Hovy, D., Bhatia, A., Faruqui, M., & Dyer, C. (2014). Augmenting english adjective senses with supersenses. In Proceedings of the Language
Resources and Evaluation Conference (LREC).
Van de Cruys, T., & Apidianaki, M. (2011). Latent Semantic Word Sense Induction and
Disambiguation. In Proceedings of the 49th Annual Meeting of the Association for
Computational Linguistics (ACL), pp. 14761485. Association for Computational Linguistics.
Veronis, J. (2004). HyperLex: lexical cartography for information retrieval. Computer
Speech & Language, 18 (3), 223252.
Wang, J., Bansal, M., Gimpel, K., Ziebart, B. D., & Yu, C. T. (2015). A sense-topic model
for word sense induction with unsupervised data enrichment. Transactions of the
Association for Computational Linguistics, 3, 5971.
Yarowsky, D. (1995). Unsupervised Word Sense Disambiguation Rivaling Supervised Methods. In Proceedings of the 33rd annual meeting on Association for Computational
Linguistics (ACL), pp. 189196. Association for Computational Linguistics.
1057

fiBaskaya & Jurgens

Yuret, D. (2007). Ku: Word sense disambiguation by substitution. In Proceedings of the
4th International Workshop on Semantic Evaluations (SemEval), pp. 207213. Association for Computational Linguistics.
Yuret, D. (2012). FASTSUBS: An Efficient and Exact Procedure for Finding the Most Likely
Lexical Substitutes Based on an N-Gram Language Model. IEEE Signal Processing
Letters, 19 (11), 725728.
Zhong, Z., & Ng, H. (2010). It makes sense: A wide-coverage word sense disambiguation system for free text. In Proceedings of the ACL 2010 System Demonstrations. Association
for Computational Linguistics.

1058

fiJournal of Artificial Intelligence Research 55 (2016) 131-163

Submitted 3/2015; published 1/2016

Distributional Correspondence Indexing for
Cross-Lingual and Cross-Domain Sentiment Classification.
Alejandro Moreo Fernandez
Andrea Esuli

alejandro.moreo@isti.cnr.it
andrea.esuli@isti.cnr.it

Istituto di Scienza e Tecnologie dellInformazione
Consiglio Nazionale delle Ricerche
56124 Pisa, IT

Fabrizio Sebastiani

fsebastiani@qf.org.qa

Qatar Computing Research Institute
Hamad bin Khalifa University
PO Box 5825, Doha, QA

Abstract
Domain Adaptation (DA) techniques aim at enabling machine learning methods learn effective classifiers for a target domain when the only available training data belongs to
a different source domain. In this paper we present the Distributional Correspondence
Indexing (DCI) method for domain adaptation in sentiment classification. DCI derives
term representations in a vector space common to both domains where each dimension
reflects its distributional correspondence to a pivot, i.e., to a highly predictive term that
behaves similarly across domains. Term correspondence is quantified by means of a distributional correspondence function (DCF). We propose a number of efficient DCFs that are
motivated by the distributional hypothesis, i.e., the hypothesis according to which terms
with similar meaning tend to have similar distributions in text. Experiments show that
DCI obtains better performance than current state-of-the-art techniques for cross-lingual
and cross-domain sentiment classification. DCI also brings about a significantly reduced
computational cost, and requires a smaller amount of human intervention. As a final contribution, we discuss a more challenging formulation of the domain adaptation problem, in
which both the cross-domain and cross-lingual dimensions are tackled simultaneously.

1. Introduction
Automated text classification methods usually rely on a training set of labelled examples
in order to learn a classifier that will predict the classes of unlabelled documents. One
important bottleneck that supervised machine learning methods have to deal with has to
do with their dependence on high-quality annotated examples in order for the model to
be trained. Deploying a model for a domain where these examples are not available thus
entails a substantial human labelling effort.
Transfer learning (TL  see e.g., Pan & Yang, 2010; Pan, Zhong, & Yang, 2012) focuses on alleviating this problem by leveraging training examples from a different, although
related, source domain (a.k.a. out-domain, or auxiliary domain) for which the amount of
available labelled examples is higher. TL allows making use of these examples in order
to train an effective classifier for the target domain (a.k.a. in-domain), thus allowing to
diminish or completely do away with the cost involved in the manual generation of training
c
2016
AI Access Foundation. All rights reserved.

fiMoreo, Esuli, & Sebastiani

documents for the target domain. As a consequence, when TL is applied one of the fundamental assumptions of traditional machine learning, i.e., that the training and the test
data are randomly drawn from the same distribution (the so-called iid assumption), no
longer holds.
One applicative scenario of particular interest for TL is sentiment classification, the task
of classifying opinion-laden documents as conveying a positive or a negative sentiment towards a given entity (e.g., a product, a policy, a political candidate). Determining the users
stance towards such an entity is of the utmost importance for market research, customer relationship management, the social sciences, and political science among others, and several
automated methods have been proposed for this purpose (see e.g., Liu, 2012; Pang & Lee,
2008). However, when sentiment classification needs to deal with a completely new entity, it
is likely that the amount of available, pre-labelled opinion-laden documents is scarce or even
null. In such cases, promptly generating a sentiment classifier might become difficult, due
to the considerable cost and time involved in producing a representative corpus of training
documents.
In sentiment classification, TL finds a natural application in domain adaptation (DA),
i.e., the task of adapting a sentiment classifier to operate on a new domain. For example, we might want to use a training set of book reviews written in English to classify
movie reviews written in English, or to classify book reviews written in German. The
former case is typically known as cross-domain adaptation, while the second one is instead known as cross-lingual adaptation (Pan et al., 2012). In this article we will use the
notation Ls Cs  Lt Ct to indicate the domain adaptation setup, where Ls and Lt are the
source and target languages, and Cs and Ct are the source and target domains, respectively.
Therefore, the previously discussed examples will be written as EnglishBooksEnglishDVDs
(or simply BooksDVDs for short), which is an example of cross-domain adaptation, and
EnglishBooksGermanBooks, which is an example of cross-lingual adaptation.
A common practice in text classification is to represent a dataset as a term-document
matrix M according to the so-called bag-of-words model (BoW), where the value Mij is
a function of the frequency of term fi in document dj and in the dataset as a whole.
Accordingly, rows (resp., columns) can be regarded as vectorial representations of terms
(resp., documents) in a vector space generated by documents (resp., terms). Here, the
expectation is that distances between vectors in this vector space model (VSM) reflect the
semantic distance between terms or between documents. Since each term is a dimension
of the vector space where documents lie, terms are represented by orthogonal dimensions
even if they have similar meanings. For example, the term beautiful is viewed as being as
dissimilar to nice as it is to awful; the base of the vector space is therefore unaware of any
semantic nuance, which lies hidden in the joint term distributions. Statistical methods like
Latent Semantic Indexing (LSA  Deerwester, Dumais, Landauer, Furnas, & Harshman,
1990; Landauer & Dumais, 1997) and Latent Dirichlet Allocation (LDA  Blei, Ng, &
Jordan, 2003) attempt to discover these hidden correlations among terms. Discovering
such semantic correspondences becomes crucial when dealing with different domains; for
example, when adapting the target domain of book reviews from a source domain of film
reviews, identifying cross-domain semantic correspondences among important terms (e.g.,
book  film, writer  director, length  duration) might be helpful for the task, as it is likely
that the decision boundaries for the model hinge upon these terms.
132

fiDistributional Correspondence Indexing

With respect to this aspect, a general assumption in domain adaptation is that large
sets of unlabelled documents for both the source and the target domain are available. When
leveraging these unlabelled collections, various techniques can be applied in order to better
explore the term distributions in the two domains, in an attempt to map the similarities
between the terms in the two domains. This idea rests on the belief that the meaning of a
term is somehow determined by its distribution in text and by the terms it co-occurs with,
an idea that is generally referred to as the distributional hypothesis (Harris, 1954).
The discovery of hidden correlations between highly predictive terms, with the goal
of improving document representations, was studied in the Structural Learning paradigm
(Ando & Zhang, 2005). These correlations are discovered by learning predictive structures
of the input data as auxiliary binary problems which consist of predicting term presence
using the surrounding terms in the unlabelled data and then applying LSA to the predictors. This framework was then extended in Structural Correspondence Learning for domain
adaptation (SCL  Blitzer, McDonald, & Pereira, 2006). SCL unifies in a latent space the
correspondences among terms from different domains that derive from the auxiliary prediction problems of pivot terms  highly predictive terms expected to behave in a similar way in
both domains (e.g., intriguing, annoying, or captivating in both film and book reviews). SCL
was first applied to cross-domain adaptation in sentiment classification (Blitzer, Dredze, &
Pereira, 2007). It was later applied to cross-lingual adaptation (Prettenhofer & Stein, 2010)
by enhancing each pivot term with one of its equivalent translations in the target language
(e.g., intriguing  intrigante, annoying  noioso, or captivating  travolgente in EnglishBooks
 ItalianBooks cross-lingual adaptation). Even though SCL was successfully validated in
the cross-domain and cross-lingual scenarios, it suffers from a considerable computational
cost, deriving from the intermediate optimizations of the auxiliary predictive problems and
from the use of LSA.
The method we present in this paper, which we dub Distributional Correspondence Indexing (DCI), is inspired by SCL but follows a different, simpler approach, with a more
direct application of the distributional hypothesis. We propose to mine the distributional
correspondences between each term and a small set of pivot terms. We hypothesize these
correspondences to be approximately invariant across both domains for terms that have
equivalent roles in the two domains. For example, we expect the distributional correspondence between the source term fs = book and pivots p1s = intriguing, p2s = annoying,
p3s = captivating, to be approximately similar to the distributional correspondence between
the target term ft = film and the same pivots p1t , p2t , p3t (Books  DVDs cross-domain
adaptation). Analogously, we expect the distributional correspondence between the source
(English) term fs = book and pivots p1s = intriguing, p2s = annoying, p3s = captivating, to
be approximately similar to the distributional correspondence between the target (Italian)
term ft = libro and the pivot translations p1t = intrigante, p2t = irritante and p3t = accattivante (EnglishBooks  ItalianBooks cross-lingual adaptation). Contrarily to SCL, we define
these distributional correspondences through a distributional correspondence function that
directly mines term vectors from unlabelled collections, and that can be computed efficiently.
The present work is an extension of work by Esuli and Moreo Fernandez (2015), where
some preliminary intuitions underlying our method were applied to the cross-lingual case.
The present improved version of DCI compares favourably with respect to the state of
the art in extensive experimental comparisons we have carried out on two popular senti133

fiMoreo, Esuli, & Sebastiani

ment classification datasets that cover both cross-domain and cross-lingual adaptation. The
same experiments also show that DCI has a substantially smaller computational cost with
respect to the competition. In the cross-lingual scenario, we show DCI to require a smaller
amount of human intervention in order to create the cross-lingual pivots. As a final contribution, we explore a more general and more complex formulation of the domain adaptation
problem that combines the cross-domain setting with the cross-lingual setting; we present
experimental results where we compare our method with some state-of-the-art methods.
The rest of this article is structured as follows. Section 2 overviews related work in
domain adaptation. Section 3 formally states the problem and presents the notation we are
going to use. Section 4 formally defines the distributional correspondence functions, while
the method as a whole is described in Section 5. Section 6 presents the experiments and
our analysis of the results, while Section 7 concludes.

2. Related Work
This section offers a brief overview of the main related methods in the literature of domain
adaptation for sentiment classification. Traditionally, two different types of approaches to
domain adaptation can be identified. A first group of instance transfer methods aim at
re-weighting the relative importance of each training document, in order to compensate the
disagreements between the source and the target marginal probability distributions (Dai,
Xue, Yang, & Yu, 2007; Gao, Fan, Jiang, & Han, 2008). Such methods can be applied
only to cross-domain (and not cross-lingual) adaptation problems, since they cannot solve
the problem posed by the fact that, in the cross-lingual setting, the term sets of the source
and target domains are disjoint. A second group of feature-representation transfer methods project documents from both domains into a common vector space in which standard
classification algorithms could be applied. The DCI method we propose belongs to the
feature-representation transfer class, and can thus be applied to both the cross-domain and
the cross-lingual settings. In the following we review the most relevant work on the crossdomain setting (Section 2.1) and the cross-lingual setting (Section 2.2). The interested
reader can check (Pan & Yang, 2010; Pan et al., 2012) for surveys on transfer learning
methods.
2.1 Cross-Domain Adaptation
The Structural Correspondence Learning method (Blitzer et al., 2006), already discussed in
the introduction, extends the Structural Learning paradigm of Ando and Zhang (2005) by
introducing the concept of pivot features. SCL has been applied to cross-domain adaptation (Blitzer et al., 2007) by leveraging the notion of predictive power of a pivot. A similar criterion has been adopted to discern between domain-specific and domain-independent
terms in Spectral Feature Alignment (SFA  Pan, Ni, Sun, Yang, & Chen, 2010), a method
for clustering domain-specific terms from source and target domains into a latent space by
mining their relationships with domain-independent terms.
Aside from sets of unlabelled documents for each domain, some methods take advantage
of external general-purpose knowledge resources in order to bridge the gap between domains.
For example, Wang, Domeniconi, and Hu (2008) extended the co-clustering approach to
propagate labels between the two domains by using Wikipedia to represent documents by
134

fiDistributional Correspondence Indexing

means of concepts. More recently, the Bridging Information Gap method (Xiang, Cao, Hu,
& Yang, 2010) followed a similar motivation, exploiting Wikipedia or the Open Directory
Project as the general-purpose knowledge base. In sentiment classification, other related
methods use a sentiment lexicon as the external resource. The Joint Sentiment-Topic Model
(JSTM), proposed by He, Lin, and Alani (2011) as an extension of Latent Dirichlet Allocation, consists of augmenting terms with polarity-bearing topics using a sentiment lexicon as
a repository of prior word sentiment. The JSTM was found to perform better than SCL and
comparably to SFA. Denecke (2009) studied the viability of SentiWordNet, a well-known
sentiment lexicon, as a lexicon for cross-domain sentiment adaptation; the main drawback
of methods such as these is their dependence on the availability of suitable public resources
/ lexicons for the language which the application targets. Li, Pan, Jin, Yang, and Zhu
(2012a) alleviated such constraint by automatically co-extracting a topic lexicon and a sentiment lexicon for the target domain, exploiting the information from the source domain.
Similarly, Bollegala, Weir, and Carroll (2011) obtained a sentiment-sensitive thesaurus to
augment term vectors. The main peculiarity of this approach is that the lexicon is created
by mining multiple source domains. Similarly, but following a completely different approach
based on deep learning architectures, the Stacked Denoising Autoencoder (SDAsh ) method
(Glorot, Bordes, & Bengio, 2011) exploits the unlabelled information contained in multiple
domains in order to improve the vector representations of terms in an unsupervised fashion.
Glorot et al. found the method (which we use as one of the baselines in our experiments)
to scale well on large multi-domain collections, outperforming SCL and SFA when using 22
different domains of unlabelled documents.
Other branches of research related to cross-domain methods for binary classification
exist that were not tested on sentiment classification but on topic classification, using the
popular datasets Reuters-21578, 20-Newsgroups, and SRAA. Some relevant examples include Spectral Domain Transfer Learning (Ling, Dai, Xue, Yang, & Yu, 2008), Matrix
Trifactorization (Zhuang, Luo, Xiong, He, Xiong, & Shi, 2011), Topic Correlation Analysis
(Li, Jin, & Long, 2012b), and Topic-Bridged Probabilistic LSA (Xue, Dai, Yang, & Yu,
2008). Aside from the fact that these methods were not explicitly designed to classify according to sentiment, these approaches also faced a different problem setting, i.e., the test
set in the target domain is available  though with labels omitted  when modelling the
classification hypothesis, and there is no other collection of unlabelled documents available
for the source or target domain. These approaches thus fall in the domain of transductive
learning (see e.g., Joachims, 1999), and are thus not directly related to our approach, which
is completely inductive.
2.2 Cross-Lingual Adaptation
We review prior work on cross-lingual adaptation, covering three different types of approaches: (i) methods relying on automatic machine translation, (ii) methods exploiting
parallel corpora, and (iii) methods exploiting unlabelled topic-specific collections.
Rigutini, Maggini, and Liu (2005) proposed a method for cross-lingual adaptation that
first translates the source documents into the target language by means of an automatic machine translation service. Then, an Expectation Maximization method refines the translated
representations by mining the statistical properties of large sets of unlabelled documents in
135

fiMoreo, Esuli, & Sebastiani

the target language. Along these lines, Wan, Pan, and Li (2011) proposed a bi-weighting
method to re-weight both terms and training instances in order to correct the word drift that
machine translation could have introduced during the translation process. Motivated by the
lack of labelled Chinese sentiment corpora, Wan (2009) proposed instead an English-Chinese
co-training approach based on automatic machine translation. The method translates the
labelled English (source) documents into Chinese (target), and the Chinese unlabelled documents into English. A classifier is then created for each of the languages, that is later
used to classify their respective set of unlabelled documents to improve the model. Finally,
each Chinese test document is attached to its translation equivalent in English and given
as input to the classifier.
Even though machine translation represents a promising solution to cross-lingual problems (a solution that will presumably become more and more viable as the field of machine
translation evolves), current machine translation services are not always free-to-use, nor
available for all language pairs either, and are computationally expensive. All other things
being equal, cross-lingual methods that do not rely on them are thus preferable.
Latent Semantic Analysis is a well-known technique which originated within monolingual
text analysis (Deerwester et al., 1990) but was later extended to deal with cross-lingual retrieval (Dumais, Letsche, Littman, & Landauer, 1997) and multilingual classification (Xiao
& Guo, 2013). LSA consists of mapping the original term-document matrix into a lowerdimensional latent semantic space that captures the (linear) relations among the original
terms and the documents. In a cross-lingual context, this mapping is performed via a singular value decomposition of the original term-document matrix extracted from multilingual
documents. The main problem with the use of LSA for cross-lingual applications is its
dependence on a parallel corpus. In order to relax this constraint, Xiao and Guo (2014)
proposed a method that induces cross-lingual terms via matrix completion. The method
requires only a small set of parallel documents that is used to construct a dual-language
co-occurrence matrix; LSA is then applied to the completed dual-language matrix in order
to produce a low-dimensional interlingual representation. Cross-lingual Kernel Canonical
Correlation Analysis (KCCA  Vinokourov, Shawe-Taylor, & Cristianini, 2002) produces
instead a semantic cross-lingual representation by investigating correlations between aligned
bilingual fragments. KCCA takes advantage of kernel functions in order to map aligned texts
into a high-dimensional space in such a manner that the correlations between both mappings
are mutually maximized. Finally, Oriented Principal Component Analysis (OPCA  Platt,
Toutanova, & Yih, 2010) finds a discriminative projection that maximizes the document
variance across languages, at the same time minimizing the distance between comparable
documents, thus avoiding the use of artificially concatenated documents.
The techniques based on correlation analysis that we have discussed above are rather
expensive from a computational point of view, and their use requires the availability of a
parallel or comparable corpus. For this reason Moen and Marsi (2013) proposed the use
of Random Indexing (Sahlgren, 2005), a computationally lighter indexing approach that
approximates LSA (Kanerva, Kristofersson, & Holst, 2000), as an alternative for use in crosslingual information retrieval. Cross-lingual Random Indexing requires only a monolingual
corpus for each language, plus a bilingual dictionary.
Even though some of the approaches discussed above succeed in discovering hidden
correlations between terms belonging to different languages, they are still based on the
136

fiDistributional Correspondence Indexing

availability of a suitable parallel corpus or a bilingual dictionary. As a response, Gliozzo
and Strapparava (2005, 2006) provided a means for automatically obtaining a Multilingual
Domain Model (MDM) by defining soft relations between words and domain topics. Making
up for the lack of a bilingual dictionary or a parallel corpus, a MDM can be automatically
obtained from comparable corpora by performing LSA. In a similar vein, Rapp (1999)
proposed a method for acquiring a bilingual dictionary based on the assumption that there
is a correlation between word co-occurrence patterns in different languages (Rapp, 1995).
The dimensions of the co-occurrence matrices are rearranged so as to make translation
equivalents of the same word correspond to identical positions in the vector, by using a
small bilingual dictionary. Word translation is then performed as vector similarity in the
two co-occurrence matrices, ignoring all dimensions that are not aligned. The dictionary
is iteratively expanded with the inclusion of newly translated terms. Koehn and Knight
(2002) proposed a method for automatically constructing a word-level translation lexicon
by taking a monolingual corpus for each language as input, neither requiring the corpora
to be parallel or even comparable, nor requiring the availability of an initial dictionary.
Roughly speaking, this was done by first taking words with identical spelling (cognates) or
similar spelling as the initial entries of the dictionary, and then by expanding the dictionary
by assuming the context, frequency, and word correlations to be approximately preserved
across languages. More recently, Peirsman and Pado (2010) proposed a method to induce
selectional preferences for resource-poor languages which also takes advantage of cognates.
A bilingual vector space is initially derived by taking cognates as the dimensions of the
vector space. This space, that is subsequently bootstrapped from large (unparsed) corpora
of both languages, allows direct word translations to be performed based on vector distances.
In a realistic cross-lingual setting we do not expect to have any sort of labelled corpus
available for the target domain, and machine translation tools, when available, are still
expensive. Therefore, Prettenhofer and Stein (2010, 2011) assume that a word translation
oracle is available but only for a limited budget of calls (450, in their experiments). The
resulting word translations allow Structural Correspondence Learning (SCL  see Section
2.1) to be applied to cross-lingual domain adaptation by pairing each source pivot with
its equivalent translation in the target language. Even though cross-lingual SCL succeeds
in relaxing the constraints discussed above thanks to the fact that it does not need any
linguistic resource, it still suffers from a considerable computational cost deriving from the
need to perform intermediate optimizations of structural problems, and from the need to
use LSA. Following a similar strategy relying on bilingual pivots, our DCI method requires
significantly fewer word translations, and avoids the use of any expensive statistical analysis
technique.
Our method bears some resemblance to Explicit Semantic Analysis (ESA), a method
that indexes any given text with respect to a set of explicitly given external categories
(Gabrilovich & Markovitch, 2007). In the work of Sorg and Cimiano (2008, 2012) different
language-specific views of Wikipedia articles were considered as the external categories on
which semantic term vectors were defined. Each dimension thus models the strength of
association between a given term and a given article in a cross-lingual information retrieval
setting (CL-ESA). Arguably, the main difference with our method is that CL-ESA relies on
high-dimensional spaces (about 10,000 dimensions) of interlingual and universal concepts,
while DCI instead projects each term into a low-dimensional space (about 100 dimensions) of
137

fiMoreo, Esuli, & Sebastiani

highly predictive concepts, i.e., bilingual pivots. Additionally, our method does not require
any sort of external resource apart from a word translation oracle, and the strength of
association is rather defined in terms of distributional correspondence, computed efficiently
on unlabelled sets (Section 4).

3. Problem Statement
In this section we formally state our problem and set the notation we are going to use
throughout this paper.
Sentiment classification may be viewed as the task of approximating the unknown target
function  : X  Y, that indicates how documents ought to be classified, by means of
a function  : X  Y, called the classifier, where X denotes the space of documents
and Y = {+1, 1} denotes the space of labels, indicating positive (+1) or negative (-1)
sentiment. In domain adaptation (see e.g., Pan & Yang, 2010) it is customary to define a
domain as a pair D = hF, P (X)i, where P (X) is the marginal probability distribution that
governs the likelihood with which documents (represented in the term space F ) are drawn.
Given a source domain Ds = hFs , Ps (X)i and a target domain Dt = hFt , Pt (X)i, domain
adaptation is a subtask of transfer learning that consists of improving the accuracy of the
classifier  on Dt by using knowledge from Ds , a domain such that Ds 6= Dt .
The precondition Ds 6= Dt leads to two different interpretations of the domain adaptation problem. On one side, cross-domain adaptation (e.g., DVDs  Books) is typically
characterized by Fs = Ft and Ps 6= Pt ; that is, the term space is common  or is trivially
made common by joining the two term spaces  but the marginal probability distributions
differ. On the other side, cross-lingual adaptation (e.g., EnglishBooks  GermanBooks) is
typically characterized by Fs 6= Ft and Ps = Pt ; that is, documents are drawn from similar
distributions but are described in different term spaces.
In this paper we also define and investigate a third case of domain adaptation, in
which both the term spaces and the probability distributions differ. This is the crossdomain/cross-lingual adaptation problem, characterized by Fs 6= Ft and Ps 6= Pt . We
argue that this case is of particular interest, since it enables cross-lingual adaptation to be
performed even in the absence of an auxiliary dataset that acts as a bridge in a two-steps
adaptation (e.g., EnglishBooks  GermanBooks  GermanDVDs). For example, when a sentiment classifier for a resource-poor language needs to be deployed that analyses sentiment
about a new topic, a common scenario is one in which we want to leverage data from a
resource-rich language (e.g., English) on a different, already known topic. This scenario is
a realistic generalization of the domain adaptation problem; to the best of our knowledge,
nobody has tackled it before in published work.
As a final note regarding notation, we will use subscripts s and t to indicate whether
the data is drawn from the source or from the target domain, respectively. Accordingly,
Us denotes the unlabelled source dataset, while Ut refers to the unlabelled target dataset.
Similarly, T rs and T et denote the training set and test set, respectively.
138

fiDistributional Correspondence Indexing

4. Distributional Correspondence Functions
The goal of this section is to introduce Distributional Correspondence Functions (DCFs).
We first characterize the family of DCFs and then propose some specific ones.
4.1 Definition
DCFs are a family of real-valued functions that quantify the degree of correspondence
between two terms f i and f j by comparing their context distribution vectors f i and f j from
an unlabelled collection U . A context distribution vector is a unit-length n-dimensional
vector that models how a term relates to a set of contexts. A context is any text unit in
which a term could appear (e.g., a sentence, a fixed-size window, or an entire document);
fci denotes the value of the vector for term f i in context c, with fci = 0 if f i does not appear
in context c. The cases in which fci > 0 are determined by the weighting function in use,
and might lead to different interpretations of the DCF, e.g., as a probability function in an
event space (Section 4.2), or as a kernel in a vector space (Section 4.3). We define pi as the
prevalence of f i , i.e., the portion of contexts for which fci > 0, i.e.,
pi =

|{c|fci > 0}|
n

(1)

where n is the dimensionality of the vector space, i.e., the number of different contexts. In
this work we take documents as contexts, so fci = 0 means that term f i does not appear
in document c, and pi is the portion of documents of the unlabelled collection in which f i
appears.
A DCF is a function  : Rn  Rn  R, where the sign of (f i , f j ) indicates the polarity
of the correspondence, i.e., positive values indicate positive correlation and negative values
indicate negative correlation; (f i , f j ) = 0 indicates null correspondence. We will force
DCFs to be such that (f i , f j ) = 0 for the expected correspondence measured between
randomly chosen pairs of vectors once the prevalence pi and pj of terms f i and f j have
been set. The rationale of this choice is that high-prevalence terms have a higher probability
to have non-zero values in a number of common positions, and in this case some measures
(see Section 4.3) will record a level of correspondence due to the non-perfect orthogonality
of the vectors, which happens much more rarely for low-prevalence terms. We want to
factor out this bias, centering the DFC on the expected correspondence value.
4.2 Probability-Based DCF
In this section we discuss some probability-based DCFs derived from information theory.
The distribution P (f i ) of a term f i across the contexts is modelled using a binomial event
space, thus considering only the presence or absence of the term in the context; P (f i )
i
thus denotes the probability that f i occurs in a random context, while P (f ) denotes the
probability that f i does not occur in it.
The first part of Table 1 shows the probability-based DCFs we investigate. We consider
Pointwise Mutual Information (P M I, the ratio between the joint distribution and the
product of the marginal distributions), and a simple probabilistic function (here called
j
Linear ) that contrasts the probabilities of f i conditioned on f j and f , respectively. We
139

fiMoreo, Esuli, & Sebastiani

also consider Mutual Information (M I, the reduction in entropy of a distribution due to
the observation of another distribution) in an asymmetric version. The rationale of this
asymmetry is that M I per se is symmetric with respect to positive and negative correlation;
that is, the two cases in which (a) f i occurs in all and only the contexts in which f j also
occurs, and (b) f i occurs in all and only the contexts in which f j does not occur, obtain the
same M I score. In our scenario these two kinds of correlation must be kept distinct, because
a high positive correlation indicates semantic similarity, while a high negative correlation
indicates a lack of semantic similarity. For this reason we invert the sign of the DCF in
the negative correlation case, by defining a  function that detects the negative correlation
by using the true positive rate (tpr = P (f i , f j )/P (f j )) and the true negative rate (tnr =
i j
j
P (f , f )/P (f )), i.e.,

+1 if (tpr + tnr > 1)
i j
(f , f ) =
(2)
1
otherwise
and by multiplying (f i , f j ) by M I, to yield Asymmetric Mutual Information (AMI, see
Table 1).
4.3 Kernel-Based DCFs
In this section we consider different kernel functions as DCFs. Kernel functions are similarity
functions, typically used e.g., within support vector machines to operate in high- (and
potentially infinite-) dimensional spaces. In this case the values in the context vector can
be numeric, thus indicating the relative importance of a term in a given context, and are
usually computed as a function of the frequency of the term in the context and in the corpus,
e.g., tf idf . We consider normalized context vectors, i.e., after weighting the document-byterm matrix we normalize the term vectors to unit length.
The most popular vector similarity measure is probably cosine similarity, which measures
the cosine of the angle between them, and is defined as
cos(f i , f j ) =

hf i , f j i
kf i kkf j k

(3)

We also consider as DCFs other popular kernels: the polynomial kernel and the Radial
Basis Function (RBF  a.k.a. Gaussian) kernel, i.e.,
polynomiala,b (f i , f j ) = (a + hf i , f j i)b
i

j

i

(4)
j 2

RBF (f , f ) = exp{kf  f k }

(5)

qP
j 2
n
i
j
i
where kf i  f j k =
c=1 (fc  fc ) is the Euclidean distance between f and f .
Since non-zero values in a frequency vector are always positive, it turns out that the
expected value of both the dot product and the Euclidean distance between two random
distributional vectors is greater than zero. In order to satisfy the necessary condition
imposed to DCFs the kernels should be centred to zero by factoring out this bias. Let
ri and rj be two unit-length vectors with prevalences pi and pj , whose non-zero values are

independently distributed at random; the expected value of non-zero positions is pi n1

and pj n1 , respectively. The expected value of the dot product and of the Euclidean
140

fiDistributional Correspondence Indexing

distance between ri and rj are, respectively,
1
1

= pi pj

pi n pj n

2
1
1
1
1
i
j
+ n(pi  pi pj )
E[kr  r k] = npi pj 

+ n(pj  pi pj )
pi n
pj n
pi n
pj n

= 2(1  pi pj )
E[hri , rj i] = pi pj n 

(6)
(7)

The resulting DCFs, obtained by factoring out these expected values from the corresponding
kernels, are reported in the second part of Table 1.
Table 1: Mathematical forms of DCFs discussed in this work.
Probability-based DCFs
Linear
Pointwise Mutual Information
Asymmetric Mutual Information

Mathematical form
P (f i |f j )  P (f i |f j )
P (f i , f j )
P (f i )P (f j )
X
(f i , f j )
log2

X

x{f i ,f i } y{f j ,f j }

Kernel-based DCFs
Cosine
Polynomial
RBF

P (x, y) log2

P (x, y)
P (x)P (y)

Mathematical form
hf i , f j i

 pi pj
i
j
kf kkf k
(a + hf i , f j i)b  (a +



pi pj )b
n
2 o

exp{kf i  f j k2 }  exp 4 1  pi pj

5. Distributional Correspondence Indexing
In this section we explain our Distributional Correspondence Indexing (DCI) method in
detail, by paying special attention to each step of the workflow.
The DCI method works by first identifying a small set of pivot terms (or pivots,
for short  Section 5.1), and then projects each term into a new vector space where each
dimension measures the correspondence of the term to a pivot (Section 5.2). Each term
will thus be assigned to a new vectorial representation that will be here referred to as term
profile, or simply profile. The term space is then post-processed by first normalizing each
dimension (Section 5.3) and then unifying the source and target term profiles for certain
terms that we expect to behave similarly in both domains, such as pivots (Section 5.4).
Documents are then projected by cumulating (i.e., summing) the profiles of the terms that
occur in them (Section 5.5), after which a classifier can be trained as usual. In order to
141

fiMoreo, Esuli, & Sebastiani

clarify the explanation we will use a running example throughout the different steps. To
that aim we will be tracking the case Books  Electronics, i.e., the domain adaptation from
the Books domain to the Electronics one (more details about the datasets will be given in
Section 6.1).
5.1 Pivot Selection
Pivots are terms that are shared across the source and target domains, and are meant to link
them, thus enabling the knowledge transfer process. Blitzer et al. (2006) defined pivots as
terms which occur frequently in the source and target domains and behave similarly in both
domains. Subsequent work (Blitzer et al., 2007; Prettenhofer & Stein, 2010; Pan et al., 2010)
extended the definition of pivots to take into account also the co-occurrence relation between
terms and class labels in the source domain, as it was shown that informative terms for the
prediction task are better pivots. This idea was later adapted to the cross-lingual setting by
Prettenhofer and Stein (2010), in which a fixed frequency threshold , called support, was
used to filter infrequent pivot candidates, then ranking the remaining candidates by their
mutual information with respect to labels in the source domain. Prettenhofer and Stein
also introduced the notion of word translation oracle (WTO), i.e., a translator that, given a
word in the source language, provides its translation in the target language. The method of
Prettenhofer and Stein assumes the possibility of issuing a limited number of calls to such
an oracle.
As pointed out by Pan et al. (2010), pivot selection using mutual information can help
identify predictive terms for the source domain, but there is no guarantee that those terms
act similarly in both domains. In this respect, we think that even though the support
threshold might serve to filter out some problematic candidates, this strategy is suboptimal.
For example, a candidate occurring 29 times and 31 times out of 50,000 in the source and
in the target domain, respectively, will be discarded if the support is set to  = 30, while
a pivot occurring 5000 times and 31 times could be chosen, even if it is clear that in this
second case its role in the two domains is very different.
A good pivot should be highly task-dependent, and also present a similar degree of
domain-dependence in the two domains. We have formalized this intuition via the function
(f i ) = Is (f i )st (f i )

(8)

where (f i ) is the terms strength as a pivot, Is (f i ) quantifies the informativeness (to
be estimated on the training set T rs ) of term f i for the classification task, and st (f i )
measures the cross-consistency of term f i estimated on Us and Ut , that quantifies how
similarly the term behaves in the two domains.
Following previous research, we will instantiate Is (f i ) via mutual information. Ideally,
st : F  [0, 1] should be defined in such a way that st (f i )  1 if the distribution of
f i is consistent across domains, and st (f i )  0 if the importance of f i varies a lot from
one domain to another. Given the lack of labelling information in the target domain, we
adopt a simple heuristic that relates the prevalence on both domains, as we might expect
a comparable prevalence of use in text for terms that posses a similar degree of domain142

fiDistributional Correspondence Indexing

dependence1 . We thus define
st (f i ) =

min{psi , pti }
max{psi , pti }

(9)

where psf (resp., pti ) is the prevalence of f i as measured on set Us (resp., Ut ). The top-ranked
m terms according to their  value that have a frequency greater than  in both Us and Ut ,
are selected as pivots; m is a user defined parameter indicating the number of pivots to select.
Table 2 exemplifies the pivot selection process in the Books  Electronics adaptation. For
instance, the cross-consistency weight succeeds in penalizing adjective boring, which might
be a good candidate for predicting the polarity of book reviews but is rather uninformative
for electronic devices, which is thus pushed out of the top-100 list (only 10 elements are
shown here due to space limitations).
Table 2: Top-10 terms ranked according to mutual information (Is (f i )) (left), and mutual
information combined with cross-consistency ((f i ) = Is (f i )st (f i )) (right).
#
1
2
3
4
5
6
7
8
9
10

term
waste
boring
disappointing
excellent
no
waste of
bad
dont
not
your money

Is score
0.029
0.029
0.029
0.026
0.021
0.021
0.021
0.019
0.018
0.018

term
waste
excellent
bad
not
dont
waste of
highly recommend
disappointing
great
your money

 score
0.028
0.022
0.020
0.018
0.017
0.017
0.014
0.013
0.012
0.012

5.2 Term Profiles
We implement a rather direct application of the distributional hypothesis, following the
intuition that the semantics of a term can be captured by its distributional correspondence
with pivots. We thus build a term profile f~ for each source and target term f (including
pivot terms) as the m-dimensional vector
f~ = ((f , p1 ), (f , p2 ), . . . , (f , pm ))

(10)

where f and pi are the context distribution vector from the unlabelled collection of the term
f being profiled and the ith pivot, respectively, and  is the selected DCF.
Table 3 displays the term profiles associated with four relevant terms from our running
example, i.e., boring, excellent, waste, reliable. Note that excellent and waste are pivots
with opposite polarity, while the other two are domain-dependent terms, i.e., boring is only
1. Note that the st () function is meant to capture the cross-domain drift, and thereby we will ignore it
in the cross-lingual setting  by simply defining st (f i ) = 1 in that case  as the domain of knowledge
is the same for both collections.

143

fiMoreo, Esuli, & Sebastiani

informative for Books reviews while reliable is only informative for Electronics reviews. The
DCF used in this example is the cosine kernel. Note that boring has no representation in
the target side, as this term does not appear in the Electronics dataset. Note also that
pivot terms are associated to vectorial representations that are somehow close to each other
in both domains. The same does not happen for reliable, a domain-dependent term that
plays different roles in the two domains. We can also note that correspondence tends to be
positive between terms with similar polarity (e.g., between waste and bad), and negative
otherwise (e.g., between excellent and bad). Finally, notice that cos(waste, waste) 6= 1 and
cos(excellent, excellent) 6= 1, due to the correction factor introduced on the cosine formula
(see Table 1).

Table 3: Term profiles generated for terms boring, excellent, waste, and reliable (rows) for
the different dimensions (columns) in the source (left) and target (right). Only
the first 5 dimensions, corresponding to pivots waste, excellent, bad, no, and dont,
are shown due to space restrictions.

boring
excellent
waste
reliable

waste
0.058
-0.030
0.957
-0.012

excellent
-0.042
0.938
-0.030
-0.002

Books
bad
0.029
-0.051
0.007
-0.007

not
-0.029
-0.082
-0.013
0.004

dont
0.004
-0.065
0.161
0.016

...
...
...
...
...

waste
-0.042
0.959
0.001

excellent
0.927
-0.042
0.014

Electronics
bad
not
-0.023 -0.021
0.028 0.025
0.005 -0.004

dont
-0.018
0.138
-0.008

...
...
...
...
...

5.3 Normalization of Term Profiles
Each dimension of the space reflects the distributional correspondence to a given pivot.
Pivots with high prevalence are likely to generate high DCF values, which could lead to
dominant dimensions in the profile vectors; this could be detrimental during the learning
phase. To avoid this effect, we center each profile dimension on its expected value and then
rescale by the standard deviation (Equation 11), so that the values for all profile dimensions
are approximately normally distributed in N (0, 1), i.e.,
f~0 =

f~1  1 f~2  2
f~m  m
,
,...,
1
2
m

!
(11)

where f~i is the ith dimension of the term profile f~ (see Equation 10), and i and i are the
mean and the standard deviation for the same ith dimension, respectively. After normalization, term profile vectors are rescaled to unit length.
Table 4 demonstrates the effect of term normalization for the example terms discussed
in Table 3. Note that, after normalization, the source and target profiles for pivot terms
seem to get closer in the vectorial space. Furthermore, the target representation of reliable
turns out to be more consistent with our intuitions, as it reflects a negative correspondence
to waste, and a stronger positive correspondence to excellent.
144

fiDistributional Correspondence Indexing

Table 4: Effect of term normalization for terms boring, excellent, waste, and reliable.

boring
excellent
waste
reliable

waste
0.223
-0.021
0.555
-0.091

excellent
-0.163
0.861
-0.017
0.020

Books
bad
0.105
-0.038
0.005
-0.056

not
-0.125
-0.080
-0.008
0.076

dont
0.020
-0.050
0.094
0.169

...
...
...
...
...

waste
-0.035
0.572
-0.011

excellent
0.909
-0.032
0.119

Electronics
bad
not
-0.024 -0.038
0.016 0.018
0.005 -0.146

dont
-0.023
0.087
-0.138

...
...
...
...
...

5.4 Unification of Term Profiles
As we assume pivot terms behave similarly in the two languages, we unify their term
profiles by simply averaging the source profile and the target profile and then normalizing
the result to unit length. Unification is also applied to profiles of terms that appear in
both the source and target domains with a frequency greater than the support . The
rationale behind unification is to correct the possible misalignment between the source and
target term profiles for terms that should receive the same vectorial representation in both
domains, such as pivot terms or proper nouns. This is done in order to equalize across
domains the contribution of the term to the document representation (see below).
Table 5 shows the term profiles of our running example after unification. Note that
boring does not experience any change as its target counterpart does not even exist. Term
reliable is also not affected by normalization, as its frequency in the Books domain does
not exceed the support , set to 30 for this example. Finally, the term profiles of pivots
excellent and waste are unified, i.e., are computed as the average of their respective source
and target profile representations, and then normalized to unit norm.
Table 5: Term profile unification for terms boring, excellent, waste, and reliable.

boring
excellent
waste
reliable

waste
0.223
-0.028
0.570
-0.091

excellent
-0.163
0.893
-0.025
0.020

Books
bad
0.105
-0.031
0.010
-0.056

not
-0.125
-0.059
0.005
0.076

dont
0.020
-0.037
0.091
0.169

...
...
...
...
...

waste
-0.028
0.570
-0.011

excellent
0.893
-0.025
0.119

Electronics
bad
not
-0.031 -0.059
0.010 0.005
0.005 -0.146

dont
-0.037
0.091
-0.138

...
...
...
...
...

5.5 Document Indexing
Finally, train and test documents are indexed in the profile space via a weighted sum of
all profile vectors associated to their terms. That is, document dj is represented as the
m-dimensional vector
X
d~j =
wij  f~0
(12)
i

fi dj

where wij is the weight of term fi in document dj according to any weighting function (in
our experiments we used the standard cosine-normalized tf idf ), and f~i0 is the normalized
and unified term profile vector for fi .
145

fiMoreo, Esuli, & Sebastiani

6. Experiments
In this section we experimentally compare our DCI method, implemented using different
DCFs, to other state-of-the-art methods proposed in the literature.
6.1 Datasets
We test our method on two popular, publicly available sentiment datasets: Multi-Domain
Sentiment Dataset (version 2.0) and Webis-CLS-10. The former is a dataset frequently used
for evaluating cross-domain adaptation, while the latter has often been used for evaluating
cross-lingual methods. We will also use Webis-CLS-10 to explore the cross-domain/crosslingual setting.
6.1.1 Multi-domain Sentiment Dataset (version 2.0)
The Multi-Domain Sentiment (MDS) dataset, first proposed by Blitzer et al. (2007), contains English product reviews taken from Amazon.com for the four domains Books (B),
DVDs (D), Electronics (E), and Kitchen (K) appliances. In order to facilitate reproducibility
and to allow for a fair comparison with the results reported in previous literature, we used
the same pre-processed version of the dataset as used in previous evaluations, made publicly
available by Blitzer et al. (see MSD dataset, 2007) . In this pre-processed version, terms
were extracted by taking unigrams and bigrams; reviews originally rated higher than 3
stars were labelled as positive, and those rated lower than 3 stars as negative; reviews
with intermediate ratings were removed. The dataset comprises 1000 positive reviews and
1000 negative reviews for each of the four domains, and a set of unlabelled documents ranging from 3,586 to 5,945 documents for each domain. Table 6 shows the number of labelled
and unlabelled documents, number of distinct terms and total number of terms for each
dataset. According to the same evaluation procedure followed by the proposers of other
methods we compare against, we randomly split each labelled dataset into a training set of
1600 instances and a test set of 400 instances.
Table 6: Main characteristics of the Multi-Domain Sentiment dataset (version 2.0).
Domain
Books
DVDs
Electronics
Kitchen

Labelled
2,000
2,000
2,000
2,000

Unlabelled
4,465
3,586
5,681
5,945

Terms
195,887
188,778
111,407
93,474

Occurrences
445,793
370,844
392,699
351,162

6.1.2 Webis-CLS-10
Webis-CLS-10, first proposed by Prettenhofer and Stein (2010), is a cross-lingual sentiment
collection consisting of Amazon product reviews written in four languages (English (E),
German (G), French (F), and Japanese (J)), covering three product domains (Books (B),
DVDs (D), and Music (M)). For each language-domain pair there are 2,000 training documents, 2,000 test documents, and from 9,000 to 50,000 unlabelled documents depending
146

fiDistributional Correspondence Indexing

on the language-domain combination (see Table 7 for further details). We used the preprocessed version of the dataset made publicly available by the authors (see Webis-CLS
dataset, 2010), where terms correspond to uni-grams. Following the work by Prettenhofer
and Stein, we consider English as the source language, since it is by far the most realistic
scenario. Documents are either labelled as positive or negative, following the same procedure of Blitzer et al. (2007). Positive and negative examples are balanced in all sets (see
Table 7 for details). Each labelled dataset was split into a perfectly balanced training set of
2,000 instances and a test set of 2,000 instances. This split was proposed by Prettenhofer
and Stein; all baseline methods we here compare against use exactly the same corpus both
for training and test.
Table 7: Main characteristics of the Webis-CLS-10 dataset.
Domain
EB
ED
EM
GB
GD
GM
FB
FD
FM
JB
JD
JM

Labelled
4,000
4,000
4,000
4,000
4,000
4,000
4,000
4,000
4,000
4,000
4,000
4,000

Unlabelled
50,000
30,000
25,220
50,000
50,000
50,000
32,870
9,358
15,940
50,000
50,000
50,000

Terms
62,499
50,124
38,632
105,360
100,265
95,952
52,664
26,117
39,001
51,179
53,318
53,078

Occurrences
6,289,014
4,001,678
2,664,955
6,618,037
6,303,371
5,688,874
2,427,178
714,105
1,371,800
7,637,325
7,263,796
6,284,653

6.2 Evaluation Metrics
Following the practice common in the related literature, we adopt standard accuracy as the
evaluation measure. Accuracy measures the proportion of correctly classified documents
over the total number of outcomes (Equation 13), i.e.,
Acc =

TP + TN
TP + FP + FN + TN

(13)

where T P , T N , F P , and F N stand for the numbers of true positives, true negatives, false
positives, and false negatives, respectively. Note that this measure is a perfectly adequate
choice since all datasets are balanced with respect to the positive and negative classes.
6.3 Baseline Methods
We experimentally compare DCI, using different DCFs, to different baseline methods proposed in the literature for cross-domain and cross-lingual domain adaptation. We limit
the comparison to algorithms evaluated on the same corpora, and we report results taken
from the original papers. When not explicitly mentioned, results for all baseline algorithms
147

fiMoreo, Esuli, & Sebastiani

were obtained by using the same datasets, though obviously using different random splits.
For the MDS dataset, following common practice, we run experiments on multiple random
splits and average them. For the Webis-CLS-10 dataset we instead used the split proposed
by Prettenhofer and Stein (2010) (more details below).
As an upper bound, we implemented a method (hereafter called Upper) that trains
the SVM classifier on the training set of the target domain. As a lower bound we instead
implemented a method that trains the SVM classifier on the source domain and then applies
the trained classifier directly in the target domain, i.e., without carrying out any sort of
knowledge transfer (NoTrans). When considering various languages, we also report the
machine translation baseline (MT), which first translates all target documents into the
source language (i.e., English in our experiments) before giving them as input to the SVM
classifier trained on the source domain; we used the pre-translated documents provided by
Prettenhofer and Stein (2011).
For cross-domain adaptation the baselines we consider are Structural Correspondence
Learning using Mutual Information to select pivots (SCL-MI  Blitzer et al., 2007), Spectral
Feature Alignment (SFA  Pan et al., 2010), multiple sources Sentiment Sensitive Thesaurus
(SST  Bollegala et al., 2011), and Stacked Denoising Autoencoder (Glorot et al., 2011)
trained on domain pairs (SDA) and trained on 22 domains available in the former version
of the MDS dataset (a method that the authors abbreviate as SDAsh ).
For cross-lingual adaptation the baselines we consider are cross-lingual Latent Semantic
Indexing (LSI  Dumais et al., 1997), cross-lingual Kernel Canonical Correlation Analysis
(KCCA  Vinokourov et al., 2002), Oriented Principal Component Analysis (OPCA 
Platt et al., 2010), Two-Step Learning method (TSL  Xiao & Guo, 2013), Semi-Supervised
Matrix Completion (SSMC  Xiao & Guo, 2014), and the cross-lingual version of Structural
Correspondence Learning (SCL  Prettenhofer & Stein, 2011).
Since there are no published results to compare with for the cross-lingual/cross-domain
adaptation, as a baseline we consider SCL-MI (Prettenhofer & Stein, 2011) by reusing the
publicly available source code (Natural Language Understanding Toolkit, 2011) for running
our own experiments.
6.4 Implementation Details and Parameter Setting
We implemented our method (see DCI-source, 2015) as part of the JaTeCS (2015) framework. We used the popular SVMlight (2008) implementation of Support Vector Machines
as the learning device, with default parameters, for DCI and for baselines NoTrans, Upper,
and MT.
In our experiments we set the support (see Section 5.1) to  = 30, following the indications of Prettenhofer and Stein (2010) for the Webis-CLS-10 dataset. Since the amount of
unlabelled documents in the MDS Dataset is one order of magnitude smaller, in that case
we set  = 1.
To emulate the word oracle  and for the sake of a fair comparison  we reused the bilingual dictionary2 created for evaluating cross-lingual SCL by Prettenhofer and Stein (2010).
This dictionary emulates a context-unaware word-translation oracle, i.e., each source word
2. Note that we used a different pivot selection criterion, as detailed in Section 5.1, and therefore the oracle
could be queried to translate some words that where never considered in the cited work, and thus might

148

fiDistributional Correspondence Indexing

is mapped into its most likely translation; potential problems arising from the ambiguity
of single words are simply disregarded.
One important factor to take into consideration is the number of calls issued to the
oracle; the oracle simulates a human translator, and this number is thus an indicator of the
human effort required to perform domain adaptation. We limited the number of translations
to the top 2m terms with the highest mutual information, where m is the number of pivots
words. In order to perform comparisons with other methods, we fixed the number of pivots
to m = 100, which corresponds to the minimal setup tested by Prettenhofer and Stein
(2010). In Section 6.6 we then explore the impact in accuracy due to variations in the value
of m.
Parameters for the polynomial and RBF kernel DCFs were optimized via a grid search
on the Books  DVDs cross-domain adaptation for the MDS dataset, and on EnglishBooks
 GermanBooks cross-lingual adaptation for Webis-CLS-10 dataset (as was done in previous
research, Prettenhofer & Stein, 2010). The actual values we ended up in using were b = 0.5
and  = 0.82 in MDS, and b = 0.8 and  = 0.88 in Webis-CLS-10. We set a = 0 for
the (homogeneous) polynomial DCF in both cases, as we did not perceive any consistent
improvement that justifies a more complex grid search exploration on the two parameters.
We used the normalized tf idf weighting criterion to represent the co-occurrence matrices
in all experiments.
6.5 Experimental Results
In this section we present the results of our experiments using different DCFs. Experiments are presented by different domain adaptation setups, including cross-domain adaptation (Section 6.5.1), cross-lingual adaptation (Section 6.5.2), and cross-domain/cross-lingual
adaptation (Section 6.5.3). Additional related experiments are then conducted in Section
6.6.
For the sake of brevity and consistently with the notation Ls Cs  Lt Ct introduced
earlier, we will use a single upper-case character (as defined in Section 6.1) to denote the
languages and the domains involved in the experimental setup. For example, EB  GD
denotes an experiment in which EnglishBooks is used as the source and GermanDVDs is used
as the target.
6.5.1 Cross-Domain Results
Table 8 reports results obtained on the MDS dataset, including (a) performance averages
by product category, i.e., computed by averaging all the results obtained when the product
category is considered as the target domain, and (b) global averages. All results reported
correspond to the accuracy of the different methods3 computed via 5-fold cross-validation,
i.e., using 1,600 training documents and 400 test documents in each run. A direct comparison with many other methods (He et al., 2011; Xia & Zong, 2011; Denecke, 2009;
Ponomareva & Thelwall, 2012) would also be feasible in principle. We omitted direct comnot be present in the dictionary. Such cases happened rarely however, and we preferred to simply skip
these candidates rather than completing the bilingual dictionary, in order to guarantee a fair comparison.
3. Missing results are ones which were not reported in the original papers.

149

fiMoreo, Esuli, & Sebastiani

parisons with these methods since previous research has shown them to be comparable, but
not superior, to SFA.
Table 8: Cross-domain adaptation on the MDS dataset.
Task
ED  EB
EE  EB
EK  EB
EB  ED
EE  ED
EK  ED
EB  EE
ED  EE
EK  EE
EB  EK
ED  EK
EE  EK
Books
DVDs
Electronics
Kitchen
Average

NoTrans
0.728
0.707
0.709
0.772
0.706
0.727
0.708
0.730
0.827
0.745
0.740
0.840
0.715
0.735
0.755
0.775
0.745

Upper
0.844
0.844
0.844
0.847
0.847
0.847
0.869
0.869
0.869
0.902
0.902
0.902
0.844
0.847
0.869
0.902
0.866

SCL-MI
0.797
0.754
0.686
0.758
0.762
0.769
0.759
0.741
0.868
0.789
0.814
0.859
0.746
0.763
0.789
0.821
0.780

SFA
0.775
0.757
0.748
0.814
0.772
0.766
0.725
0.767
0.851
0.788
0.808
0.868
0.760
0.784
0.781
0.821
0.786

SST
0.763
0.788
0.836
0.852
0.810

SDA
0.724
0.768
0.807
0.804
0.902
0.835
0.806
0.872
0.802
0.844
0.803
0.777
0.766
0.847
0.827
0.808
0.812

SDAsh
0.768
0.780
0.837
0.855
0.905
0.854
0.824
0.875
0.820
0.846
0.821
0.811
0.795
0.871
0.840
0.826
0.833

Linear
0.825
0.766
0.783
0.808
0.768
0.788
0.810
0.822
0.855
0.834
0.858
0.864
0.791
0.788
0.829
0.852
0.815

PMI
0.827
0.763
0.783
0.811
0.779
0.789
0.822
0.832
0.851
0.839
0.856
0.864
0.791
0.793
0.835
0.853
0.818

AMI
0.811
0.753
0.769
0.806
0.765
0.781
0.793
0.812
0.843
0.822
0.846
0.851
0.778
0.784
0.816
0.840
0.804

Cos
0.824
0.764
0.790
0.817
0.774
0.799
0.822
0.824
0.858
0.835
0.864
0.868
0.793
0.797
0.835
0.856
0.820

Poly
0.830
0.776
0.791
0.829
0.799
0.807
0.826
0.833
0.863
0.844
0.861
0.874
0.799
0.811
0.841
0.860
0.828

RBF
0.825
0.765
0.784
0.815
0.771
0.798
0.821
0.826
0.857
0.835
0.863
0.867
0.791
0.795
0.835
0.855
0.819

Most configurations of DCI outperform all compared methods, with the exception of
SDAsh ; only the MI-based DCF performed slightly worse than SST on average. It should
be noted, however, that both SST and SDAsh use a different problem setting, since they train
on multiple domains. More concretely, SST trained on three out of four domains from the
MDS dataset to create the sentiment thesaurus, while SDAsh exploited 22 domains included
in the previous version of the MDS dataset to train the auto-encoder. Furthermore, SDA
and SDAsh rely on a deep learning approach, a paradigm that requires significant more
computational power and many parameters to be tuned. Notwithstanding this, DCI with
the polynomial kernel as the DCF (with only two parameters) obtained three best averaged
results (i.e., Books, Electronics, and Kitchen) out of five, not leveraging any additional
domain, and requiring low computational cost (as will be later discussed in Section 6.7).
Table 9 reports experiments on cross-domain adaptation using the Webis-CLS-10 dataset.
These results are consistent with our previous observations, i.e., the polynomial and cosinebased DCFs are the best performers, followed by the PMI and RBF functions. In this case
the best results obtained by DCI are very close to Upper, and surprisingly surpass it in
ED  EB and EM  EB. We conjecture that this improvement may be due to the larger
size of the unlabelled sets, that are one order of magnitude greater with respect to the MDS
dataset, thus allowing for more robust evaluations of the cross-domain consistency function
st () and of the DCF.
6.5.2 Cross-Lingual Results
Table 10 reports our results on the Webis-CLS-10 dataset for cross-lingual adaptation. As
discussed earlier, the source language is always English, while the target languages include
German, French, and Japanese.
DCI outperformed the MT baseline on average in all cases but the PMI DCF in the
German case. All kernel-based DCFs outperformed all the compared methods in terms of
150

fiDistributional Correspondence Indexing

Table 9: Cross-domain performance on the Webis-CLS-10 dataset.
Task
ED  EB
EM  EB
EB  ED
EM  ED
EB  EM
ED  EM
Books
DVDs
Music
Average

NoTrans
0.803
0.783
0.798
0.778
0.786
0.804
0.793
0.788
0.795
0.792

Upper
0.829
0.829
0.831
0.831
0.845
0.845
0.829
0.831
0.845
0.835

SCL-MI
0.839
0.823
0.810
0.797
0.804
0.823
0.831
0.804
0.814
0.816

Linear
0.840
0.828
0.798
0.802
0.825
0.831
0.834
0.800
0.828
0.821

PMI
0.843
0.838
0.812
0.821
0.835
0.833
0.841
0.817
0.834
0.830

AMI
0.831
0.826
0.788
0.798
0.816
0.815
0.829
0.793
0.816
0.812

Cos
0.851
0.840
0.818
0.821
0.838
0.829
0.846
0.819
0.834
0.833

Poly
0.855
0.841
0.818
0.822
0.836
0.832
0.848
0.820
0.834
0.834

RBF
0.848
0.838
0.806
0.816
0.831
0.827
0.843
0.811
0.829
0.828

Table 10: Cross-lingual performance on the Webis-CLS-10 dataset.
Task
EB  GB
ED  GD
EM  GM
EB  FB
ED  FD
EM  FM
EB  JB
ED  JD
EM  JM
German
French
Japanese
Average

Upper
0.868
0.835
0.859
0.862
0.872
0.890
0.812
0.834
0.842
0.854
0.875
0.829
0.852

MT
0.808
0.800
0.791
0.821
0.795
0.765
0.692
0.722
0.714
0.800
0.794
0.709
0.767

SCL-MI
0.833
0.809
0.829
0.813
0.804
0.781
0.770
0.764
0.773
0.824
0.799
0.769
0.797

LSI
0.776
0.796
0.727
0.792
0.778
0.726
0.738
0.754
0.734
0.766
0.765
0.742
0.758

KCCA
0.791
0.776
0.695
0.767
0.782
0.748
0.792
0.782
0.735
0.754
0.766
0.770
0.763

OPCA
0.747
0.766
0.714
0.746
0.705
0.718
0.745
0.737
0.750
0.742
0.723
0.744
0.736

TSL
0.792
0.819
0.726
0.813
0.820
0.766
0.794
0.793
0.762
0.779
0.800
0.783
0.787

SSMC
0.819
0.823
0.813
0.831
0.827
0.805
0.738
0.776
0.775
0.818
0.821
0.763
0.801

Linear
0.798
0.826
0.844
0.746
0.823
0.816
0.779
0.822
0.826
0.823
0.795
0.809
0.809

PMI
0.714
0.819
0.850
0.761
0.823
0.827
0.731
0.768
0.816
0.794
0.804
0.772
0.790

AMI
0.797
0.800
0.837
0.768
0.801
0.818
0.711
0.797
0.807
0.811
0.796
0.772
0.793

Cos
0.827
0.822
0.856
0.842
0.827
0.844
0.758
0.801
0.839
0.835
0.838
0.799
0.824

Poly
0.837
0.833
0.844
0.819
0.806
0.840
0.754
0.795
0.832
0.838
0.822
0.794
0.818

RBF
0.829
0.788
0.801
0.844
0.846
0.803
0.782
0.761
0.826
0.806
0.831
0.790
0.809

average accuracy, and the best result was obtained by one of the kernel-based DCFs in 11
out of 12 cases. The best performing DCFs are again the cosine and polynomial DCFs.
6.5.3 Cross-Domain/Cross-Lingual Results
Table 11 reports our experiments in the cross-domain/cross-lingual setting.
This setting is arguably the most difficult one, since both the term space and the
marginal probabilities of the domains differ, as reflected in the noticeable degradation of
MT results. Notwithstanding this, consistent observations could be derived from these results. The cosine and polynomial DCFs confirm their superiority with respect to all other
compared methods. The best result was obtained by DCI in 17 out of 18 cases; in half of
the cases the cosine DCF obtained the best result.
6.5.4 Statistical Significance Tests
Statistical significance tests (paired t-test on the accuracy values from Table 8) indicate that
all our DCI configurations, with the exception of MI, are significantly better with p < 0.01
than SCL, SCL-MI, and SFA in the MDS dataset. For the polynomial DCF, which obtained
10 best results out of 12, higher-confidence levels were obtained, i.e., p < 0.001. A t-test
on all runs on Webis-CLS-10 reveals that all kernel-based DCFs and the Linear DCF are
151

fiMoreo, Esuli, & Sebastiani

Table 11: Cross-domain/cross-lingual accuracy on the Webis-CLS-10 dataset.
Task
ED  GB
EM  GB
EB  GD
EM  GD
EB  GM
ED  GM
ED  FB
EM  FB
EB  FD
EM  FD
EB  FM
ED  FM
ED  JB
EM  JB
EB  JD
EM  JD
EB  JM
ED  JM
German
French
Japanese
Books
DVDs
Music
Average

Upper
0.868
0.868
0.835
0.835
0.859
0.859
0.862
0.862
0.872
0.872
0.889
0.889
0.812
0.812
0.834
0.834
0.842
0.842
0.854
0.874
0.829
0.847
0.847
0.863
0.852

MT
0.789
0.751
0.774
0.773
0.768
0.768
0.788
0.765
0.783
0.780
0.771
0.745
0.700
0.642
0.708
0.693
0.673
0.710
0.771
0.772
0.688
0.739
0.752
0.739
0.743

SCL-MI
0.823
0.825
0.784
0.792
0.811
0.824
0.790
0.784
0.780
0.745
0.762
0.757
0.725
0.708
0.742
0.756
0.742
0.776
0.810
0.770
0.742
0.776
0.767
0.779
0.774

Linear
0.823
0.791
0.790
0.778
0.786
0.844
0.744
0.810
0.810
0.798
0.822
0.836
0.738
0.711
0.813
0.792
0.826
0.817
0.802
0.803
0.783
0.770
0.797
0.822
0.796

PMI
0.764
0.821
0.796
0.829
0.812
0.844
0.798
0.833
0.816
0.822
0.753
0.826
0.675
0.621
0.663
0.828
0.699
0.804
0.811
0.808
0.715
0.752
0.792
0.790
0.778

AMI
0.811
0.705
0.788
0.772
0.793
0.828
0.747
0.785
0.788
0.761
0.794
0.827
0.715
0.636
0.710
0.721
0.811
0.762
0.783
0.784
0.726
0.733
0.757
0.803
0.768

Cos
0.824
0.812
0.827
0.834
0.843
0.816
0.848
0.845
0.823
0.841
0.833
0.847
0.761
0.721
0.805
0.790
0.831
0.816
0.826
0.840
0.787
0.802
0.820
0.831
0.818

Poly
0.818
0.791
0.825
0.814
0.833
0.835
0.846
0.843
0.793
0.829
0.824
0.849
0.741
0.689
0.789
0.763
0.826
0.817
0.819
0.831
0.771
0.788
0.802
0.831
0.807

RBF
0.824
0.800
0.783
0.808
0.807
0.832
0.852
0.789
0.841
0.775
0.829
0.855
0.741
0.722
0.782
0.711
0.827
0.804
0.809
0.824
0.765
0.788
0.783
0.826
0.799

better than SCL-MI with statistical significance at confidence level p < 0.01; the Cosine
DCF obtained p = 0.854  107 and the Polynomial DCF p = 0.522  105 .
6.6 Further Experiments
This section presents further experiments aimed at testing the influence of different parameters and modules of DCI, and its performance in the standard text classification setting.
Regarding the effect of parameters, we show trend plots for some representative cases,
considering the Linear function as a representative example of a probabilistic-based DCF
and the Cosine function as an example of a kernel-based DCF. Each plot involves three
settings, one for each scenario: cross-domain adaptation, cross-lingual adaptation, and
cross-domain/cross-lingual adaptation. For the sake of brevity we selected some illustrative
examples, omitting experiments showing similar behaviour. First, we investigated the sensitivity to the value of parameter m, which indicates the number of pivots to select. Figure
1 shows how performance varies at the variation of m, from 5 to 500 pivots.
The overall tendency displayed in the plots is that performance tends to stabilise as m
increases. Adaptations involving the cross-lingual setting seem to be more strongly affected
152

fiDistributional Correspondence Indexing

Cosine DCF

Accuracy

Linear DCF
0.9

0.9

0.85

0.85

0.8

0.8

0.75

0.75

0.7

0.7

ED->GD

0.65

0.65

ED->GM

0.6

0.6

0.55

0.55

ED->EB

0.5

0.5
0

100

200

300

400

500

0

100

200

300

400

500

m

m

Figure 1: Variation of accuracy at the variation of the number of pivots for different setups.

by the number of pivots. We attribute this effect to the more limited capability of a pivot
to reflect a term correspondence through imprecision introduced by the context-unaware
single-word translations of the oracle. Such a negative effect seems however to reduce as
the number of pivots increases. The method scales well on the number of pivots in terms
of efficiency (see below), which might be an indication that simply increasing the pivot set
size could be a feasible alternative rather than moving to more complicated definitions of
the cross-lingual pivots in order to cover translation nuances. Larger fluctuations could be
observed for m < 75, but also surprising peaks of performance for extremely small values
of m. For example, DCI obtained 81.1% accuracy with the Linear DCF in the ED  GM
adaptation with only 30 pivots, while other baselines obtained 76.8% (MT) or 82.4% (SCLMI, which uses 450 pivots). A similar experiment was reported by Prettenhofer and Stein
(2010) for CL-SCL, by varying parameter m in the range [100, 800]. A direct comparison
shows that our method achieves better accuracy for smaller values of m. Given that the
number of calls to the oracle is directly related to m (i.e., m calls in the cross-lingual case,
and 2m in cross-domain/cross-lingual case due to the cross-consistency reweighting, see
Section 5.1), it follows that DCI requires less human effort in creating the bilingual pivots.
The unlabelled collection plays a key role in the domain adaptation, as it is responsible
for the term-distribution representation; we thus can expect better estimations of distributions for larger collections. We have investigated how the unlabelled set size affects the
performance of the method. We plot the accuracy score obtained for different reduction
ratios  preserving balance  in Figure 2.
As expected, the observed trend shows that accuracy is high for large unlabelled collections, and performance tends to stabilize with the addition of unlabelled examples. Better
performance is observed in the cross-domain experiments, even with smaller distributional
representations.
We also validated empirically each of the different elements that constitute the DCI
method, including the cross-distortion factor in pivot selection, the dimensionality standardization, and the unification process. For reasons of conciseness we only report the
global improvement for the linear DCF averaged in each dataset, since consistent variations
are observable for other DCFs. We found a consistent improvement of 0.47%  1.02 in
153

fiMoreo, Esuli, & Sebastiani

Accuracy

Linear DCF

Cosine DCF

0.9

0.9

0.85

0.85

0.8

0.8

0.75

0.75

0.7

0.7

EB->EM

0.65

0.65

EB->JB

0.6

0.6

0.55

0.55

EB->JD

0.5

0.5
0

0.2

0.4
0.6
unlabeled reduction ratio

0.8

1

0

0.2

0.4
0.6
unlabeled reduction ratio

0.8

1

Figure 2: Variation of accuracy at the variation of the unlabelled corpus size for different
setups.

accuracy due to cross-consistency in pivot selection, 1.785%  3.63 due to dimensionality
standardization, and 1.261%  2.18 due to unification.
Our experiments reveal that classification performance seems to benefit when the adaptation involves semantically close domains, as is the case of BooksDVDs in the MDS
dataset, or EnglishGerman in Webis-CLS-10. Analogously, the performance seems to degrade when the source and target domains are dissimilar, as for example KitchenBooks
in MDS or EnglishJapanese in Webis-CLS-10. It has been noticed in the literature that
reducing the distance between the representations of source and target domains is crucial
in order to allow better knowledge transfer. Given that the probability distributions are
unknown, their distance is sometimes computed through an approximation (the proxy Adistance Ben-David, Blitzer, Crammer, & Pereira, 2006) that considers the source and target
documents as two samples drawn from each distribution. The proxy A-distance is computed
as dA = 2(1  2), where  is the error produced by an SVM trained to discriminate between
the source and target domains.
Figure 3 graphically compares, for the MDS dataset, the proxy A-distances between
domains (i) for the raw representations, and (ii) for the DCI representations. The dA is
clearly reduced in the cross-domain space generated by DCI, which contributes to explain
the improvement in performance with respect to the baseline NoTrans on the raw representation. Such reduction is even more noticeable for semantically close domains such as
ElectronicsKitchen and BooksDVDs. Hence, DCI projects both domains into a common
vector space where the source and target distributions get effectively closer to each other,
thus facilitating the transfer of knowledge between them.
Finally, Table 12 reports performance accuracy in the text classification setting, that is,
assuming the test data follows the same marginal distribution and is represented in the same
term space as the training data. In this case, we consider as baselines (a) the well-known
BoW representation with tf idf weighting, and (b) the SCL-MI.
Even though the amount of experiments for the text classification case is too small to
allow any substantial claim, it is surprising that, in some runs, DCI with only 100 dimensions
yielded better results than traditional BoW representation considering all terms. This is a
topic we will investigate in future research.
154

fiDistributional Correspondence Indexing

Proxy A-distance on raw data

2

BE

DK

DE

BK
BD

1.75

EK
1.5
1.25
1
0.75
0.5
0.5

0.75

1

1.25

1.5

1.75

2

Proxy A-distance on DCI
Figure 3: Proxy A-distances between domains of the MDS dataset. The vertical axis displays dA in the raw data (NoTrans), while the horizontal axis displays dA in the
vector space produced by DCI using cosine as the DCF. The abscissa coordinate
for each point (e.g., BK) is the averaged dA produced by domain adaptation in
both directions (e.g., EB  EK and EK  EB).

Table 12: Text Classification performance on the Webis-CLS-10 dataset.
Task
EB  EB
ED  ED
EM  EM
Average

BoW
0.829
0.831
0.845
0.835

SCL-MI
0.828
0.815
0.832
0.825

Linear
0.848
0.819
0.838
0.835

PMI
0.855
0.826
0.846
0.842

AMI
0.836
0.798
0.825
0.820

Cos
0.854
0.818
0.841
0.838

Poly
0.856
0.821
0.844
0.840

RBF
0.853
0.809
0.835
0.832

6.7 Efficiency
The computational cost of DCI is asymptotically bound by the cost of projecting f terms
from the two domains into an m-dimensional space, that could be roughly estimated as
O(f mc), where the c component is due to the cost of comparing two term distribution
models, that depends on the average prevalence c of terms in the unlabelled corpus 
155

fiMoreo, Esuli, & Sebastiani

typically much smaller than the effective number of unlabelled documents as a result of to
sparsity.
Note that all probabilistic functions discussed in Section 4 can be implemented very
efficiently by using sparse data structures. For example, calculating the joint probability
P (v, w) is achieved in O(c) steps by intersecting two hash sets with c expected elements.
The kernel-based DCFs discussed here depend on the dot product or the Euclidean distance,
that can also be computed in O(c) by iterating only over non-zero values. Thus, DCI has
a computational cost of O(f mc); note that m is a fixed parameter, so that the overall cost
can also be considered to be O(f c). Other relevant alternatives do typically involve singular
value decomposition or matrix multiplication, thus resulting in O(df c) algorithms, where d
is the number of documents or contexts in the collection.
We performed efficiency tests comparing DCI to SCL on the Webis-CLS-10 dataset.
The test was run for all combinations of source and target classes and all target languages,
which amounts to 36 runs. The same dedicated computer4 run all experiments with the
same number of 10 threads. Table 13 shows the averaged time scores obtained.
Table 13: Running time (in seconds) for DCI with two different DCFs (linear or cosine)
and for SCL.
Min (s)
Max (s)
Average (s)
Standard Deviation

Linear
6.406
22.119
11.553
3.976

Cosine
7.501
27.032
17.774
5.516

SCL-MI
449.988
859.719
678.834
98.324

SCL suffers from much higher computational costs than DCI. On average, DCI reduced
by 98.3% and 97.4% the computational cost with respect to SCL for the linear and cosine
DCFs, respectively. SCL required m = 450 binary optimization problems and translations,
and required performing LSA on the predictive parameters. Our DCI-based method obtained better results in substantially less time. These efficiency tests suggest that DCI could
scale well to larger datasets.
6.8 Embeddings
As a final note, the intuitions behind DCI have strong relationships with those behind
word embeddings, as from deep learning, a research area that has gained interest in the
last renewed years. Neural language models are trained to obtain meaningful term representations that seem to capture interesting language regularities (Bengio, 2009). Although
deep learning has been applied to cross-domain adaptation by Glorot et al. (2011) (a work
we used as a baseline in Section 6.5.1), cross-lingual adaptation requires additional effort.
That is, to consistently obtain bilingual word embeddings, large unlabelled datasets and
aligned corpora (Zou, Socher, Cer, & Manning, 2013) or bilingual dictionaries (Mikolov, Le,
& Sutskever, 2013) are typically required. Assuming a small set of words are translated (no
4. Computer specifications: 64-bit Intel Core (TM) Genuine-Intel I7 with 12 processors at 3.47GH, 24GB
RAM, running Ubuntu 14.04.2 LTS.

156

fiDistributional Correspondence Indexing

more than 200 words in our experiments), our method obtains term profiles that perform
consistently in both languages for the classification task. Table 14 illustrates the semantic
properties captured by our term profiles; it lists the most similar (via cosine similarity)
target terms to a given source term.

Table 14: Five most similar terms in each of three target languages (German, French,
Japanese) given three terms (beautifully, classical, delightful) in English for the
Music domain.
beautifully
schone ( beautiful)
liebevoll ( loving)
sehnsucht ( longing)
ungewohnlich ( unusual)
phantastisch ( fantastic)

0.635
0.596
0.533
0.510
0.507

classical
adagio
Martenot
Charles-Marie
violoncelle ( cello)
soliste ( soloist)

0.767
0.746
0.736
0.727
0.720

delightful
( attractive)
( portrayed)
( scenes)
( delicate)
( taste)

0.610
0.546
0.545
0.542
0.538

For word embeddings, even assuming that external resources are available, an additional optimization problem, posed as a geometrical transformation involving scaling and
rotating the data matrices, is subsequently required in order to align the two embedding
spaces. This was done by Mikolov et al. (2013) by forcing the embedding representations
of some words of the bilingual dictionary to get closer to each other through the matrix
transformation. Apart from the additional computational cost this may involve, we believe
that such method might not be directly applicable to the scenario in which cross-domain
and cross-lingual adaptations are tackled simultaneously. The main reason is that this
final transformation aims at aligning the meaning of words taken from the bilingual dictionary in both domains, which could play different roles across domains, i.e., if they are
not pivots. The embeddings generated by DCI do not require computationally expensive
post-processing, and the correspondences in roles for different terms in both domains turn
out to be directly captured by the DCF scores to the pivots.
To illustrate this, we used LSI to plot into a bidimensional space some important term
profiles from the EB  GM adaptation obtained by DCI with cosine as the DCF. Figure 4
shows two zoomed-in areas of the bilingual space. Noticeably, the left-most part of the plot
seems to represent the positive sentiment, while the right-most one seems to capture the
negative sentiment. Some relevant semantic correspondences could be directly observed.
Semantically related English words, such as expecting, expected, and hoping, are projected
close together in the space. More interestingly, some related semantics seem to be preserved
across languages, e.g., English words boring, irritating, bored and German words erschreckend
(terrifying), acherlich (ridiculous), and schrecklich (terribly) are projected into regions of the
space close to each other. Some interesting cases could be regarded as examples of crosssemantic correspondence. For example classic and love (book genres) from English reviews
were projected close to folk and rock (music genres) from German reviews, an incidental
semantic correspondence that emerged due to the juxtaposition of cross-domain and crosslingual adaptation.
157

fiMoreo, Esuli, & Sebastiani

Figure 4: Vector profiles as word embeddings obtained for EB  GM adaptation. Zoom
in the positive (left) and negative (right) sentiment area. The plot was obtained
by applying LSI to terms deemed highly informative by mutual information.

These preliminary experiments suggest that DCI embeddings could be potentially useful for other tasks in natural language processing. This however will require a dedicated
investigation that we defer to future work.

7. Conclusions and Future Work
We have proposed Distributional Correspondence Indexing, an efficient method for domain
adaptation that represents terms in a vectorial space based on their distributional correspondence with respect to a small, fixed set of terms. This representation is motivated by
Harris distributional hypothesis and the notion of a pivot term of Blitzer et al. (2006);
the method indexes documents from different domains into a common vector space based
on their semantic correspondence.
Empirical evaluation on two popular sentiment analysis benchmarks shows that our
method outperforms several state-of-the-art approaches in different domain-adaptation settings, including cross-domain and cross-lingual sentiment adaptation. We have also proposed an extended formulation of the domain adaptation problem, which tackles crossdomain adaptation and cross-language adaptation at the same time; on this we have present
experiments where our system compares favourably to other related approaches. From the
point of view of efficiency, we show our method to require modest computational resources,
which is an indication that DCI can scale well to huge collections; in particular, in the
cross-lingual case it required a smaller amount of human intervention than competing approaches in order to create the pivot set. We presented some high-performance DCFs that
are parameter-free, which is a valuable characteristic in the domain adaptation setting,
given that we are not expected to count on labelled data drawn from the target distribution
on which parameters could be optimized.
The bilingual pivots created by a context-unaware word-translator oracle represent arguably an oversimplified naive approach of the translation problem. Notwithstanding this,
DCI seems to compensate it with the aggregative contribution of the partial semantics scat158

fiDistributional Correspondence Indexing

tered in several pivots. In this regard, we are interested in enhancing the concept of pivots for
cross-lingual adaptation in a more general direction that better captures the context-aware
multi-word translation, so as to attempt the polylingual case. Some possible directions
might include enriching the term representation so as to incorporate part-of-speech tags
and syntactic information, and also keeping track of the contexts in which a given term
appeared. Moreover, and motivated by some empirical evidences in the cross-domain experiments that suggested comparable performance could be achieved even with extremely
reduced sets of pivots, we will investigate more sophisticated pivot selection techniques by
better characterizing the concept of pivot and the geometrical properties of the vector space
they generate. We also plan to put to test DCI in other domains and settings, including
multi-class multi- and single-label datasets, highly imbalanced classes, and transductive
problems.

Acknowledgements
We are grateful to Xavier Glorot for sending us additional details on the experiments reported in their previous work (Glorot et al., 2011).
This paper is an extension of the short paper by Esuli and Moreo Fernandez (2015).
Fabrizio Sebastiani is on leave from Consiglio Nazionale delle Ricerche, Italy.

References
Ando, R. K., & Zhang, T. (2005). A framework for learning predictive structures from
multiple tasks and unlabeled data. The Journal of Machine Learning Research, 6,
18171853.
Ben-David, S., Blitzer, J., Crammer, K., & Pereira, F. (2006). Analysis of representations
for domain adaptation. In Proceedings of the 20th Annual Conference on Neural
Information Processing Systems (NIPS 2006), pp. 137144, Vancouver, CA.
Bengio, Y. (2009). Learning deep architectures for AI. Foundations and Trends in Machine
Learning, 2 (1), 1127.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet allocation. Journal of
Machine Learning Research, 3, 9931022.
Blitzer, J., Dredze, M., & Pereira, F. (2007). Biographies, Bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classification. In Proceedings of the 45th
Annual Meeting of the Association for Computational Linguistics (ACL 2007), pp.
440447, Prague, CZ.
Blitzer, J., McDonald, R., & Pereira, F. (2006). Domain adaptation with structural correspondence learning. In Proceedings of the 4th Conference on Empirical Methods in
Natural Language Processing (EMNLP 2006), pp. 120128, Sydney, AU.
Bollegala, D., Weir, D., & Carroll, J. (2011). Using multiple sources to construct a sentimentsensitive thesaurus for cross-domain sentiment classification. In Proceedings of the
49th Annual Meeting of the Association for Computational Linguistics (ACL 2011),
pp. 132141, Portland, US.
159

fiMoreo, Esuli, & Sebastiani

Dai, W., Xue, G.-R., Yang, Q., & Yu, Y. (2007). Transferring nave Bayes classifiers for text
classification. In Proceedings of the 22nd AAAI Conference on Artificial Intelligence
(AAAI 2007), pp. 540545, Vancouver, CA.
DCI-source (2015) http://hlt.isti.cnr.it/dciext/.
Deerwester, S. C., Dumais, S. T., Landauer, T. K., Furnas, G. W., & Harshman, R. A.
(1990). Indexing by latent semantic analysis. Journal of the American Society for
Information Science, 41 (6), 391407.
Denecke, K. (2009). Are SentiWordNet scores suited for multi-domain sentiment classification?. In Proceedings of the 4th International Conference on Digital Information
Management (ICDIM 2009), pp. 3338, Ann Arbor, US.
Dumais, S. T., Letsche, T. A., Littman, M. L., & Landauer, T. K. (1997). Automatic crosslanguage retrieval using latent semantic indexing. In Working Notes of the AAAI
Spring Symposium on Cross-language Text and Speech Retrieval, pp. 1824, Stanford,
US.
Esuli, A., & Moreo Fernandez, A. (2015). Distributional correspondence indexing for crosslanguage text categorization. In Proceedings of the 37th European Conference on
Information Retrieval (ECIR 2015), pp. 104109, Wien, AT.
Gabrilovich, E., & Markovitch, S. (2007). Computing semantic relatedness using Wikipediabased explicit semantic analysis. In Proceedings of the 20th International Joint Conference on Artifical Intelligence (IJCAI 2007), pp. 16061611, San Francisco, US.
Gao, J., Fan, W., Jiang, J., & Han, J. (2008). Knowledge transfer via multiple model
local structure mapping. In Proceedings of the 14th ACM International Conference
on Knowledge Discovery and Data Mining (KDD 2008), pp. 283291, Las Vegas, US.
Gliozzo, A., & Strapparava, C. (2005). Cross-language text categorization by acquiring
multilingual domain models from comparable corpora. In Proceedings of the ACL
Workshop on Building and Using Parallel Texts, pp. 916, Ann Arbor, US.
Gliozzo, A., & Strapparava, C. (2006). Exploiting comparable corpora and bilingual dictionaries for cross-language text categorization. In Proceedings of the 44th Annual
Meeting of the Association for Computational Linguistics (ACL 2006), pp. 553560,
Sydney, AU.
Glorot, X., Bordes, A., & Bengio, Y. (2011). Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Proceedings of the 28th International
Conference on Machine Learning (ICML 2011), pp. 513520, Bellevue, US.
Harris, Z. S. (1954). Distributional structure. Word, 10 (23), 146162.
He, Y., Lin, C., & Alani, H. (2011). Automatically extracting polarity-bearing topics for
cross-domain sentiment classification. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics (ACL 2011), pp. 123131, Portland,
US.
JaTeCS (2015) http://hlt.isti.cnr.it/jatecs/.
160

fiDistributional Correspondence Indexing

Joachims, T. (1999). Transductive inference for text classification using support vector
machines. In Proceedings of the 16th International Conference on Machine Learning
(ICML 1999), pp. 200209, Bled, SL.
Kanerva, P., Kristofersson, J., & Holst, A. (2000). Random indexing of text samples for latent semantic analysis. In Proceedings of the 22nd Annual Conference of the Cognitive
Science Society, p. 1036, Austin, US.
Koehn, P., & Knight, K. (2002). Learning a translation lexicon from monolingual corpora.
In Proceedings of the ACL 2002 Workshop on Unsupervised Lexical Acquisition, pp.
916, Philadelphia, US.
Landauer, T. K., & Dumais, S. T. (1997). A solution to Platos problem: The latent
semantic analysis theory of acquisition, induction, and representation of knowledge.
Psychological Review, 104 (2), 211240.
Li, F., Pan, S. J., Jin, O., Yang, Q., & Zhu, X. (2012a). Cross-domain co-extraction of sentiment and topic lexicons. In Proceedings of the 50th Annual Meeting of the Association
for Computational Linguistics (ACL 2012), pp. 410419, Jeju Island, KR.
Li, L., Jin, X., & Long, M. (2012b). Topic correlation analysis for cross-domain text classification. In Proceedings of the 26th AAAI Conference on Artificial Intelligence (AAAI
2012), pp. 9981004, Toronto, CA.
Ling, X., Dai, W., Xue, G.-R., Yang, Q., & Yu, Y. (2008). Spectral-domain transfer learning.
In Proceedings of the 14th ACM International Conference on Knowledge Discovery and
Data Mining (KDD 2008), pp. 488496, Las Vegas, US.
Liu, B. (2012). Sentiment Analysis and Opinion Mining. Morgan and Claypool Publishers,
San Rafael, US.
Mikolov, T., Le, Q. V., & Sutskever, I. (2013). Exploiting Similarities among Languages for
Machine Translation. ArXiv e-prints, arXiv:1309.4168 [cs.CL].
Moen, H., & Marsi, E. (2013). Cross-lingual random indexing for information retrieval. In
Proceedings of the 1st International Conference on Statistical Language and Speech
Processing (SLSP 2013), pp. 164175, Tarragona, ES.
MSD dataset (2007) http://www.cs.jhu.edu/~mdredze/datasets/sentiment/.
Natural Language Understanding Toolkit (2011) https://github.com/pprett/nut.
Pan, S. J., Ni, X., Sun, J.-T., Yang, Q., & Chen, Z. (2010). Cross-domain sentiment classification via spectral feature alignment. In Proceedings of the 19th International
Conference on the World Wide Web (WWW 2010), pp. 751760, Raleigh, US.
Pan, S. J., & Yang, Q. (2010). A survey on transfer learning. IEEE Transactions on
Knowledge and Data Engineering, 22 (10), 13451359.
Pan, W., Zhong, E., & Yang, Q. (2012). Transfer learning for text mining. In Aggarwal,
C. C., & Zhai, C. (Eds.), Mining Text Data, pp. 223258. Springer, Heidelberg, DE.
Pang, B., & Lee, L. (2008). Opinion mining and sentiment analysis. Foundations and Trends
in Information Retrieval, 2 (1/2), 1135.
161

fiMoreo, Esuli, & Sebastiani

Peirsman, Y., & Pado, S. (2010). Cross-lingual induction of selectional preferences with
bilingual vector spaces. In Proceedings of the 8th Annual Conference of the North
American Chapter of the Association for Computational Linguistics (NAACL 2010),
pp. 921929, Los Angeles, US.
Platt, J. C., Toutanova, K., & Yih, W.-t. (2010). Translingual document representations
from discriminative projections. In Proceedings of the 8th Conference on Empirical
Methods in Natural Language Processing (EMNLP 2010), pp. 251261, Cambridge,
US.
Ponomareva, N., & Thelwall, M. (2012). Do neighbours help? An exploration of graph-based
algorithms for cross-domain sentiment classification. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning (EMNLP/CoNLL 2012), pp. 655665, Jeju Island, KR.
Prettenhofer, P., & Stein, B. (2010). Cross-language text classification using structural
correspondence learning. In Proceedings of the 48th Annual Meeting of the Association
for Computational Linguistics (ACL 2010), pp. 11181127, Uppsala, SE.
Prettenhofer, P., & Stein, B. (2011). Cross-lingual adaptation using structural correspondence learning. ACM Transactions on Intelligent Systems and Technology, 3 (1), Article 13.
Rapp, R. (1995). Identifying word translations in non-parallel texts. In Proceedings of the
33rd Annual Meeting on Association for Computational Linguistics (ACL 1995), pp.
320322, Cambridge, US.
Rapp, R. (1999). Automatic identification of word translations from unrelated English
and German corpora. In Proceedings of the 37th Annual Meeting on Association for
Computational Linguistics (ACL 1999), pp. 519526, College Park, US.
Rigutini, L., Maggini, M., & Liu, B. (2005). An EM-based training algorithm for crosslanguage text categorization. In Proceedings of the 3rd IEEE/WIC/ACM International Conference on Web Intelligence (WI 2005), pp. 529535, Compiegne, FR.
Sahlgren, M. (2005). An introduction to random indexing. In Proceedings of the Workshop
on Methods and Applications of Semantic Indexing, Copenhagen, DK.
Sorg, P., & Cimiano, P. (2008). Cross-language information retrieval with explicit semantic
analysis. In Working Notes of the 2008 Cross-Language Evaluation Forum (CLEF
2008), Aarhus, DE.
Sorg, P., & Cimiano, P. (2012). Exploiting Wikipedia for cross-lingual and multilingual
information retrieval. Data and Knowledge Engineering, 74, 2645.
SVMlight (2008) http://svmlight.joachims.org/.
Vinokourov, A., Shawe-Taylor, J., & Cristianini, N. (2002). Inferring a semantic representation of text via cross-language correlation analysis. In Proceedings of the 16th Annual
Conference on Neural Information Processing Systems (NIPS 2002), pp. 14731480,
Vancouver, CA.
Wan, C., Pan, R., & Li, J. (2011). Bi-weighting domain adaptation for cross-language text
classification. In Proceedings of the 22nd International Joint Conference on Artificial
Intelligence (IJCAI 2011), pp. 15351540, Barcelona, ES.
162

fiDistributional Correspondence Indexing

Wan, X. (2009). Co-training for cross-lingual sentiment classification. In Proceedings of
the 47th Annual Meeting of the Association for Computational Linguistics and the
4th International Joint Conference on Natural Language Processing (ACL/IJCNLP
2009), pp. 235243, Singapore, SN.
Wang, P., Domeniconi, C., & Hu, J. (2008). Using Wikipedia for co-clustering-based crossdomain text classification. In Proceedings of the 8th IEEE International Conference
on Data Mining (ICDM 2008), pp. 10851090, Pisa, IT.
Webis-CLS dataset (2010)
http://www.uni-weimar.de/en/media/chairs/webis/
research/corpora/corpus-webis-cls-10/.
Xia, R., & Zong, C. (2011). A POS-based ensemble model for cross-domain sentiment
classification. In Proceedings of the 5th International Joint Conference on Natural
Language Processing (IJCNLP 2011), pp. 614622, Chiang Mai, TH.
Xiang, E. W., Cao, B., Hu, D. H., & Yang, Q. (2010). Bridging domains using worldwide knowledge for transfer learning. IEEE Transactions on Knowledge and Data
Engineering, 22 (6), 770783.
Xiao, M., & Guo, Y. (2013). A novel two-step method for cross-language representation
learning. In Proceedings of the 27th Annual Conference on Neural Information Processing Systems (NIPS 2013), pp. 12591267, Lake Tahoe, US.
Xiao, M., & Guo, Y. (2014). Semi-supervised matrix completion for cross-lingual text
classification. In Proceedings of the 28th AAAI Conference on Artificial Intelligence
(AAAI 2014), pp. 16071614, Quebec City, CA.
Xue, G.-R., Dai, W., Yang, Q., & Yu, Y. (2008). Topic-bridged PLSA for cross-domain text
classification. In Proceedings of the 31st ACM International Conference on Research
and Development in Information Retrieval (SIGIR 2008), pp. 627634, Singapore,
SN.
Zhuang, F., Luo, P., Xiong, H., He, Q., Xiong, Y., & Shi, Z. (2011). Exploiting associations
between word clusters and document classes for cross-domain text categorization.
Statistical Analysis and Data Mining, 4 (1), 100114.
Zou, W. Y., Socher, R., Cer, D. M., & Manning, C. D. (2013). Bilingual word embeddings for
phrase-based machine translation. In Proceedings of the 11th Conference on Empirical
Methods in Natural Language Processing (EMNLP 2013), pp. 13931398, Seattle, US.

163

fiJournal of Artificial Intelligence Research 55 (2016) 953-994

Submitted 09/15; published 04/16

Bilingual Distributed Word Representations from
Document-Aligned Comparable Data
Ivan Vulic

iv250@cam.ac.uk

University of Cambridge
Department of Theoretical and Applied Linguistics
9 West Road, CB3 9DP, Cambridge, UK

Marie-Francine Moens

marie-francine.moens@cs.kuleuven.be

KU Leuven
Department of Computer Science
Celestijnenlaan 200A, 3001 Heverlee, Belgium

Abstract
We propose a new model for learning bilingual word representations from non-parallel
document-aligned data. Following the recent advances in word representation learning, our
model learns dense real-valued word vectors, that is, bilingual word embeddings (BWEs).
Unlike prior work on inducing BWEs which heavily relied on parallel sentence-aligned corpora and/or readily available translation resources such as dictionaries, the article reveals
that BWEs may be learned solely on the basis of document-aligned comparable data without any additional lexical resources nor syntactic information. We present a comparison
of our approach with previous state-of-the-art models for learning bilingual word representations from comparable data that rely on the framework of multilingual probabilistic
topic modeling (MuPTM), as well as with distributional local context-counting models. We
demonstrate the utility of the induced BWEs in two semantic tasks: (1) bilingual lexicon
extraction, (2) suggesting word translations in context for polysemous words. Our simple
yet effective BWE-based models significantly outperform the MuPTM-based and contextcounting representation models from comparable data as well as prior BWE-based models,
and acquire the best reported results on both tasks for all three tested language pairs.

1. Introduction
A huge body of work in distributional semantics and word representation learning almost
exclusively revolves around the distributional hypothesis (Harris, 1954) - an idea which
states that similar words occur in similar contexts. All current corpus-based approaches to
semantics rely on the contextual evidence in one way or another. Roughly speaking, word
representations are typically learned using these two families of distributional context-based
models: (1) global matrix factorization models such as latent semantic analysis (LSA)
(Landauer & Dumais, 1997) or generative probabilistic models such as latent Dirichlet
allocation (LDA) (Blei, Ng, & Jordan, 2003), which model the word co-occurrence at the
document or paragraph level; or (2) local context window models that represent words as
sparse high-dimensional context vectors, and model the word co-occurrence at the level of
selected neighboring words (Turney & Pantel, 2010), or generative probabilistic models that
learn the probability distribution of a vocabulary word in the context window as a latent
variable (Deschacht & Moens, 2009; Deschacht, De Belder, & Moens, 2012).
c
2016
AI Access Foundation. All rights reserved.

fiVulic & Moens

On the other hand, dense real-valued vectors known as distributed representations of
words or word embeddings (WEs) (e.g., Bengio, Ducharme, Vincent, & Janvin, 2003; Collobert & Weston, 2008; Mikolov, Chen, Corrado, & Dean, 2013a; Pennington, Socher, &
Manning, 2014) have been introduced recently, first as part of neural network based architectures for statistical language modeling. WEs serve as richer and more coherent word
representations than the ones obtained by the aforementioned traditional distributional
semantic models, with illustrative comparative studies available in the recently published
relevant work (e.g., Mikolov, Yih, & Zweig, 2013d; Baroni, Dinu, & Kruszewski, 2014; Levy,
Goldberg, & Dagan, 2015).
A natural extension of interest from monolingual to multilingual word embeddings has
occurred recently (e.g., Klementiev, Titov, & Bhattarai, 2012; Hermann & Blunsom, 2014b).
When operating in multilingual settings, it is highly desirable to learn embeddings for words
denoting similar concepts that are very close in the shared bilingual embedding space (e.g.,
the representations for the English word school and the Spanish word escuela should be
very similar). These BWEs may then be used in a myriad of multilingual natural language
processing tasks and beyond, such as fundamental tasks leaning on such bilingual meaning
representations, e.g., computing cross-lingual and multilingual semantic word similarity and
extracting bilingual word lexicons using the induced bilingual embedding space (see Figure 1). However, all these models critically require (at least) sentence-aligned parallel data
and readily-available translation dictionaries to induce bilingual word embeddings (BWEs)
that are consistent and closely aligned over different languages.
1.1 Contributions
To the best of our knowledge, this article presents the first work to showcase that bilingual word embeddings may be induced directly on the basis of comparable data without
any additional bilingual resources such as sentence-aligned parallel data or translation dictionaries. The focus is on document-aligned comparable corpora (e.g., Wikipedia articles
aligned through inter-wiki links, news texts discussing the same theme).
Our new bilingual embedding learning model makes use of pseudo-bilingual documents
constructed by merging the content of two coupled documents from a document pair, where
we propose and evaluate two different strategies on how to construct such pseudo-bilingual
documents: (1) merge and randomly shuffle strategy which randomly permutes words from
both languages in each pseudo-bilingual document, and (2) length-ratio shuffle strategy, a
deterministic method that retains monolingual word order while intermingling the words
cross-lingually. These additional pre-training shuffling strategies ensure that both source
language words and target language words occur in the contexts of each source and target
language word. A monolingual model such as skip-gram with negative sampling (SGNS)
from the word2vec package (Mikolov, Sutskever, Chen, Corrado, & Dean, 2013c) is then
trained on these shuffled pseudo-bilingual documents. By this procedure, we steer semantically similar words from different languages towards similar representations in the
shared bilingual embedding space, and effectively use available bilingual contexts instead of
monolingual ones. The model treats documents as bags-of-words (i.e., it does not include
any syntactic information) and does not even rely on any sentence boundary information.
954

fiBilingual Distributed Word Representations from Document-Aligned Data

In summary, the main contributions of this article are:
(1) We present BWE Skip-Gram (BWESG), the first model that induces bilingual word
embeddings directly from document-aligned non-parallel data. We test and evaluate two
main variants of the model based on the pre-training shuffling step. The main strength of
the presented model lies in its favourable trade-off between simplicity and effectiveness.
(2) We provide a qualitative and quantitative analysis of the model. We draw analogies and
comparisons with prior work on inducing word representations from the same data type:
document-aligned comparable corpora (e.g., models relying on the multilingual probabilistic
topic modeling framework (MuPTM)).
(3) We demonstrate the utility of induced BWEs at the word type level in the task of bilingual lexicon extraction (BLE) from Wikipedia data for three language pairs. A BLE model
based on our BWEs significantly outperforms MuPTM-based and context-counting BLE
models, and acquires the best reported scores on the benchmarking BLE datasets.
(4) We demonstrate the utility of induced BWEs at the word token level in the task of
suggesting word translations in context (SWTC) (Vulic & Moens, 2014) for the same three
language pairs. A SWTC model based on our BWEs again significantly outscores the best
scoring MuPTM-based SWTC models in the same setting without any use of parallel data
and translation dictionaries, and again acquires the best reported results on the benchmarking SWTC datasets.
(5) We also present a comparison with state-of-the-art BWE induction models (Mikolov,
Le, & Sutskever, 2013b; Hermann & Blunsom, 2014b; Gouws, Bengio, & Corrado, 2015)
in BLE and SWTC. Results reveal that our simple yet effective approach is on-par with
or outperforms other BWE induction models that rely on parallel data or readily available
dictionaries to learn shared bilingual embedding spaces. In addition, preliminary experiments with BWESG on parallel Europarl data demonstrate that the model is also useful
when trained on sentence-aligned data, reaching the performance of benchmarking BWE
induction models from parallel data (e.g., Hermann & Blunsom, 2014b).

2. Related Work
In this section we further motivate why we opt for building a model for inducing bilingual
word embeddings from comparable document-aligned data. For a clearer overview, we have
split related work into three broad clusters: (1) monolingual word embeddings, (2) bilingual
word embeddings, and (3) bilingual word representations from document-aligned data.
2.1 Monolingual Word Embeddings
The idea of representing words as continuous real-valued vectors dates way back to mid80s (Rumelhart, Hinton, & Williams, 1986; Elman, 1990). The idea met its resurgence a
decade ago (Bengio et al., 2003), where a neural language model learns word embeddings as
part of a neural network architecture for statistical language modeling. This work inspired
other approaches that learn word embeddings within the neural-network language modeling
framework (Collobert & Weston, 2008; Collobert, Weston, Bottou, Karlen, Kavukcuoglu, &
Kuksa, 2011). Word embeddings are tailored to capture semantics and encode a continuous
955

fiVulic & Moens

Monolingual

vs

Bilingual

Figure 1: A toy 3D shared bilingual embedding space from Gouws et al. (2015): While in
monolingual spaces words with similar meanings should have similar representations, in bilingual spaces words in two different languages with similar meanings
should have similar representations (both mono- and cross-lingually).
notion of semantic similarity (as opposed to semantically poorer discrete representations),
necessary to share information between words and other text units.
Recently, the skip-gram and continuous bag-of-words (CBOW) model of Mikolov et
al. (2013a, 2013c) revealed that the full neural-network structure is not needed at all to
learn high-quality word embeddings (with extremely decreased training times compared to
the full-fledged neural network models, see Mikolov et al., 2013a for the full analysis of
complexity of the models). These models are in fact simple single-layered architectures,
where the objective is to predict a words context given the word itself (skip-gram) or
predict a word given its context (CBOW). Similar models called vector log-bilinear models
were recently proposed (Mnih & Kavukcuoglu, 2013). Other models inspired by skip-gram
and CBOW are GloVe (Global Vectors for Word Representation) (Pennington et al., 2014),
which combines local and global contexts of a word into a unified model, and a model
which relies on dependency-based contexts instead of simpler word-based contexts (Levy &
Goldberg, 2014a), and new models are steadily emerging (e.g., Lebret & Collobert, 2014;
Lu, Wang, Bansal, Gimpel, & Livescu, 2015; Stratos, Collins, & Hsu, 2015; Trask, Gilmore,
& Russell, 2015; Liu, Jiang, Wei, Ling, & Hu, 2015).
An interesting finding has been discussed recently (Levy & Goldberg, 2014b): the popular skip-gram model with negative sampling (SGNS) (Goldberg & Levy, 2014) is simply a
model which implicitly factorizes a word-context matrix, with its cells containing pointwise
mutual information (PMI) scores of the respective word and context pairs, shifted by a
global constant. In other words, the SGNS performs exactly the same thing as traditional
distributional models (i.e., context counting plus context weighting and/or dimensionality
reduction), with a slight improvement in performance with SGNS (Baroni et al., 2014; Levy
et al., 2015).
All these low-dimensional vectors, besides improving computational efficiency, lead to
better generalizations, even allowing to generalize over the vocabularies observed in labelled
data, and hence partially alleviating the ubiquitous problem of data sparsity. Their utility
has been validated and proven in various semantic tasks such as semantic word similarity,
synonymy detection or word analogy solving (Mikolov et al., 2013d; Baroni et al., 2014;
Pennington et al., 2014). Moreover, word embeddings have been proven to serve as useful
956

fiBilingual Distributed Word Representations from Document-Aligned Data

unsupervised features for plenty of downstream NLP tasks such as named entity recognition,
chunking, semantic role labeling, part-of-speech tagging, parsing, selectional preferences
(Turian, Ratinov, & Bengio, 2010; Collobert et al., 2011; Chen & Manning, 2014).
Due to its simplicity, as well as its efficacy and consequent popularity in various tasks
(Mikolov et al., 2013c; Levy & Goldberg, 2014b), with a clear advantage on similarity tasks
when compared to traditional models from distributional semantics (Levy et al., 2015) in
this article we will focus on the adaptation of SGNS (Mikolov et al., 2013c). In Section 3,
we provide a very brief overview of the model, and then follow up with our new bilingual
model which is based on SGNS.
2.2 Bilingual Word Embeddings
Bilingual word representations could serve as an useful source knowledge for problems in
cross-lingual information retrieval (Levow, Oard, & Resnik, 2005; Vulic, De Smet, & Moens,
2013), statistical machine translation (Wu, Wang, & Zong, 2008), document classification
(Ni, Sun, Hu, & Chen, 2011; Klementiev et al., 2012; Hermann & Blunsom, 2014b; Chandar, Lauly, Larochelle, Khapra, Ravindran, Raykar, & Saha, 2014; Vulic, De Smet, Tang,
& Moens, 2015), bilingual lexicon extraction (Tamura, Watanabe, & Sumita, 2012; Vulic
& Moens, 2013a), or knowledge transfer and annotation projection from resource-rich to
resource-poor languages for a myriad of NLP tasks such as dependency parsing, POS tagging, semantic role labeling or selectional preferences (Yarowsky & Ngai, 2001; Pado &
Lapata, 2009; Peirsman & Pado, 2010; Das & Petrov, 2011; Tackstrom, Das, Petrov, McDonald, & Nivre, 2013; Ganchev & Das, 2013; Tiedemann, Agic, & Nivre, 2014; Xiao
& Guo, 2014). Other interesting application domains are machine translation (e.g., Zou,
Socher, Cer, & Manning, 2013; Wu, Dong, Hu, Yu, He, Wu, Wang, & Liu, 2014; Zhang, Liu,
Li, Zhou, & Zong, 2014) and cross-lingual information retrieval (e.g., Vulic & Moens, 2015).
Moreover, by making the transition from monolingual to bilingual settings and building a
shared bilingual embedding space (see again Figure 1 for an illustrative example), one is able
to extend or rather generalize semantic tasks such as semantic similarity computation, synonymy detection or word analogy computation across languages. Following the success in
monolingual settings, a body of recent work on word representation learning has therefore
focused on learning bilingual word embeddings (BWEs).
The current research on inducing BWEs critically relies on sentence-aligned parallel
data or readily available bilingual lexicons to achieve the coherence of representations across
languages (e.g., to build similar representations for similar concepts in different languages
such as January-januari, dog-hund or sky-hemel). We may cluster the current work in three
different groups: (1) the models that rely on hard word alignments obtained from parallel
data to constrain the learning of BWEs (Klementiev et al., 2012; Zou et al., 2013; Wu et al.,
2014); (2) the models that use the alignment of parallel data at the sentence level (Kocisky,
Hermann, & Blunsom, 2014; Hermann & Blunsom, 2014a, 2014b; Chandar et al., 2014; Shi,
Liu, Liu, & Sun, 2015; Gouws et al., 2015); (3) the models that critically require readily
available bilingual lexicons (Mikolov et al., 2013b; Faruqui & Dyer, 2014; Xiao & Guo,
2014). The main disadvantage of all these models is the limited availability of parallel data
and bilingual lexicons, resources which are scarce and/or domain-restricted for plenty of
language pairs. In this work, we significantly alleviate the requirements: unlike prior work,
957

fiVulic & Moens

we show that BWEs may be induced solely on the basis of document-aligned comparable
data without any additional need for parallel data or bilingual lexicons. Note that (in
theory) the work of Hermann and Blunsom (2014b), and Chandar et al. (2014) may also be
extended to the same setting with document-aligned data, as these two models originally
rely on sentence embeddings computed as aggregations over their single word embeddings
plus sentence alignments. In this work, by testing and comparing to the BiCVM model
of Hermann and Blunsom, we show that these models do not work well in practice after
replacing the very strong bilingual signal coded in parallel sentences with the noisy bilingual
signal given by document alignments and non-parallel data.
2.3 Bilingual Word Representations from Document-Aligned Data
Prior work on inducing bilingual word representations in the early days followed the tradition of window-based context-counting distributional models (Rapp, 1999; Gaussier, Renders, Matveeva, Goutte, & Dejean, 2004; Laroche & Langlais, 2010) and it again required
a bilingual lexicon as a critical resource. In order to tackle this issue, recent work relies on
the supervision-lighter framework of multilingual probabilistic topic modeling (MuPTM)
(Mimno, Wallach, Naradowsky, Smith, & McCallum, 2009; Boyd-Graber & Blei, 2009;
De Smet & Moens, 2009; Ni, Sun, Hu, & Chen, 2009; Zhang, Mei, & Zhai, 2010; Fukumasu,
Eguchi, & Xing, 2012) or other similar models for latent structure induction (Haghighi,
Liang, Berg-Kirkpatrick, & Klein, 2008; Daume III & Jagarlamudi, 2011).
Words in this setting are represented as real-valued vectors with conditional topic probability scores P (zk |wi ), regardless of their actual language. Topics zk are in fact latent
inter-lingual concepts discovered directly from multilingual comparable data using a multilingual topic model such as bilingual LDA. We discuss the MuPTM-based representations
in more detail in Section 4.1.
MuPTM-based bilingual word representations induced from comparable data have demonstrated its utility in tasks such as cross-lingual semantic similarity computation and bilingual
lexicon extraction (Vulic, De Smet, & Moens, 2011; Liu, Duh, & Matsumoto, 2013) and
suggesting word translations in context (Vulic & Moens, 2014). In this work, we compare
the state-of-the-art MuPTM-based word representations induced from the same type of
comparable corpora with BWEs learned by our new model in these two semantic tasks.
Another recent model (Sgaard, Agic, Martnez Alonso, Plank, Bohnet, & Johannsen,
2015) is also able to learn from document-aligned data. It is a count-based model which
builds binary word vectors denoting the occurrence of each word in each document pair.
Dimensionality reduction is then applied post-hoc on the induced sparse vectors. Since the
links between documents are known, the model is able to learn cross-lingual correspondences
between words and, consequently, bilingual word representations. Exactly the same idea
was already introduced as a baseline model by Vulic et al. (2011), where TF-IDF weights
were used instead of binary indices, and no dimensionality reduction was applied post-hoc.
The model of Vulic et al. was surpassed by baseline models from document-aligned data
briefly discussed in Section 4.1, while the model of Sgaard et al. obtains results that are
very similar to the BWE baselines compared against in this work (described in Section 4.2).
958

fiBilingual Distributed Word Representations from Document-Aligned Data

3. BWESG: Model Architecture
Our new bilingual model is an extension of SGNS to bilingual settings with documentaligned comparable training data. This section describes the underlying SGNS and two
variants of our SGNS-based BWE induction model.
3.1 Skip-Gram with Negative Sampling (SGNS)
Our departure point is the log-linear SGNS of Mikolov et al. (2013c) as implemented in
the word2vec package.1 The SGNS model learns word embeddings (WEs) in a similar way
to neural language models (Bengio et al., 2003; Collobert & Weston, 2008), but without a
non-linear hidden layer.
In the monolingual setting, we assume one language L with vocabulary V , and a corpus
of words w  V , along with their contexts c  V c , where V c is the context vocabulary.
Contexts for each word wn are typically neighboring words in a context window of size cs
(i.e., wncs , . . . , wn1 , wn+1 , . . . , wn+cs ), so effectively it holds V c  V .2
Each word type w  V is associated with a vector w
~  Rd (its pivot word representation
or pivot word embedding, see Figure 2), and a vector w~c  Rd (its context embedding).
d is the dimensionality of the WE vectors, which, as a model input parameter, has to be
set in advance before the training procedure commences. The entries in these vectors are
latent, and treated as parameters  to be learned by the model. In short, the idea of the
skip-gram model is to scan through the corpus (which is typically unannotated, Mikolov
et al., 2013a) word by word in turn (i.e., these are the pivot words), and learn from the pairs
(word, context word). The learning goal is to maximize the ability of predicting context
words for each pivot word in the corpus. Let ob = 1 denote that the pair of words (w, v) is
observed in the corpus and thus belongs to the training set D. The probability of (w, v)  D
is defined by the softmax function:
P (ob = 1|w, v, ) =

1
1 + exp(w
~  v~c )

(1)

Each word token w in the corpus is treated in turn as the pivot and all pairs of word tokens
(w, w  1),...,(w, w  t(cs)) are appended to D, where t(cs) is an integer sampled from a
uniform distribution on {1, . . . , cs}.3 The global training objective J is then to maximize
the probabilities that all pairs from D are indeed observed in the corpus:
J = arg max


X

log

(w,v)D

1
1 + exp(w
~  v~c )

(2)

where  are the parameters of the model, that is, pivot and context word embeddings which
have to be learned. One may see that this objective function has a trivial solution by setting
1. https://code.google.com/p/word2vec/
2. Testing other options for context selection such as dependency-based contexts (Levy & Goldberg, 2014a)
is beyond the scope of this work, and it was shown that these contexts may not lead to any gains in the
final WEs (Kiela & Bottou, 2014).
3. The original skip-gram model utilizes dynamic window sizes, where cs denotes the maximum window
size. Moreover, the model takes into account sentence boundaries in context selection, that is, it selects
as context words only words occurring in the same sentence as the pivot word.

959

fiVulic & Moens

w
~ = v~c , and w
~  v~c = V al, where V al is a large enough number (Goldberg & Levy, 2014). In
order to prevent this trivial training scenario, the negative sampling procedure comes into
the picture (Collobert & Weston, 2008; Mikolov et al., 2013c).
In short, the idea behind negative sampling is to present the model with a set D0
of artificially created or sampled negative pivot-context word pairs (w, v 0 ), which by
assumption serve as negative examples, that is, they do not occur as observed/positive
(word, context) pairs in the training corpus. The model then has to adjust the parameters 
in such a way to also maximize the probability that these negative pairs will not occur in the
corpus. While the interested reader may find further details about the negative sampling
procedure, and the new exact objective function along with its derivation elsewhere (Levy &
Goldberg, 2014b), for illustrative purposes and simplicity, here we present the approximative
objective function with negative sampling by Goldberg and Levy:
X
X
1
1
J = arg max
log
(3)
+
log

1 + exp(w
~  v~c )
1 + exp(w
~  v~c0 )
(w,v)D
(w,v 0 )D0
The free parameters  are updated using stochastic gradient descent and backpropagation,
with learning rate typically controlled by Adagrad (Duchi, Hazan, & Singer, 2011) or with
a global linearly decreasing learning rate. By optimizing the objective from eq. (3), the
model incrementally pushes observed pivot WEs towards context WEs of their collocates in
the corpus. In the words of distributional hypothesis - after training, words that occur in
similar contexts should end up having similar word embeddings. In other words, to link the
terminology of distributional hypothesis and the modeling assumptions of SGNS - words
that predict similar contexts end up having similar word embeddings.
3.2 Final Model - BWESG: BWE Skip-Gram
In the next step, we propose a novel method that extends SGNS to work with bilingual
document-aligned comparable data. Let us assume that we possess a document-aligned
comparable corpus, defined as C = {d1 , d2 , . . . , dN } = {(dS1 , dT1 ), (dS2 , dT2 ), . . . , (dSN , dTN )}.
dj = (dSj , dTj ) denotes a pair of aligned documents in the source language LS and the target
language LT respectively, and N is the number of pairs in the corpus. V S and V T are
vocabularies associated with languages LS and LT . The goal is to learn a shared bilingual
embedding space given the data (Figure 1) and document alignments as the only bilingual
signal during training. We present two strategies that, coupled with SGNS, lead to such
shared bilingual spaces. An overview of the architecture for learning BWEs from documentaligned comparable data with the two strategies is given in Figures 2(a) and 2(b).
3.2.1 Merge and Shuffle
In the first step, we merge two documents dSj and dTj from the aligned document pair dj
into a single pseudo-bilingual document d0j . Following that, we randomly shuffle the
newly constructed pseudo-bilingual document. A shuffle is a (random) permutation of the
word tokens given in two different languages forming the pseudo-bilingual document. The
pre-training shuffling step (see Figure 2(a)) assures that each word w, regardless of its
actual language, obtains word collocates from both vocabularies. The idea of obtaining
bilingual contexts for each pivot word in each pseudo-bilingual document will steer the
960

fiBilingual Distributed Word Representations from Document-Aligned Data

(a) Merge and Shuffle

(b) Length-Ratio Shuffle

Figure 2: The architecture of our BWE Skip-Gram (BWESG) model for learning bilingual
word embeddings from document-aligned comparable data with two different pretraining strategies: (1) non-deterministic merge and shuffle, (2) deterministic
length-ratio shuffle. Source language words and documents are drawn as gray
boxes, while target language words and documents are drawn as blue boxes. The
right side of the figures (separated by vertical dashed lines) illustrates how a
pseudo-bilingual document is constructed from a pair of two aligned documents.

final model towards constructing a shared bilingual space. Since the model depends on
the alignment at the document level, in order to ensure the bilingual contexts instead of
monolingual contexts, it is intuitive to assume that larger window sizes will lead to better
bilingual embeddings. We test this hypothesis and the effect of window size in Section 7.3.
In another interpretation, since the model relies only on (pseudo-bilingual) document level
co-occurrence, the window size parameter then just controls the amount of random data
dropout, that is, the number of positive document-level training examples. The locality
feature of SGNS is not preserved due to the shuffling procedure.
961

fiVulic & Moens

3.2.2 Length-Ratio Shuffle
The non-deterministic and uncontrollable nature of the merge and shuffle procedure opens
up a possibility of accidentally obtaining bad shuffles that will result in sub-optimal word
representations. Therefore, we also propose a deterministic strategy for building pseudobilingual documents suitable for bilingual training. Source and target language words are
inserted into an (initially empty) pseudo-bilingual document in turn based on the ratio of
document lengths, with word order preserved. Document lengths are measured in terms of
word tokens, and let us denote them as mS and mT for an aligned document pair (dSj , dTj ).
Let us assume, without loss of generality, that mS  mT . The procedure then proceeds as
follows (if mT > mS the procedure proceeds in an analogous manner with the roles of dSj
and dTj reversed):
1. Pseudo-bilingual document d0j is empty: d0j = {}.
mS
c.
2. Compute the ratio: R = b m
T
3. Scan through aligned documents dS and dT simultaneously and (3.1) append R word
tokens from dSj into d0j ; then (3.2) append 1 word token from dTj . Repeat steps 3.1
and 3.2 until all word tokens from dTj have been inserted into d0j .
4. Insert remaining mS mod mT word tokens from dSj into d0j .
Using a simple example, assume that we have an English (EN) document {F rodo, Sam, orcs,
goblins, M ordor, ring} and a Spanish (ES) document {anillo, orcos, mago}: the pseudobilingual document would be formed by inserting 1 Spanish word after 2 English words (as
the length ratio is 6:3 = 2:1). The final pseudo-bilingual document is:
{F rodoEN , SamEN , anilloES , orcsEN , goblinsEN , orcosES , M ordorEN , ringEN , magoES }.

In another interpretation, the length-ratio shuffle strategy constructs a single permutation/shuffle of the pseudo-bilingual document controlled by the word order in two aligned
documents as well as their length ratio. As before, the model relies on pseudo-bilingual
document level co-occurrence, and the window size parameter controls the amount of (now
non-random) data dropout. A difference lies in the fact that this procedure now keeps word
order intact monolingually while constructing a pseudo-bilingual document.
The final BWE Skip-gram (BWESG) model then relies on the monolingual variant of
SGNS (or any other monolingual WE induction model) trained on these shuffled/permuted
pseudo-bilingual documents (using any of the proposed strategies).4 The model learns
word embeddings for source and target language words aligned over the d shared embedding
dimensions. The BWESG-based representation of word w, regardless of its actual language,
is then a d-dimensional vector: w
~ = [f1 , . . . , fk , . . . , fd ]. fk  R denotes the value of the kth shared inter-lingual feature within the d-dimensional shared bilingual embedding space.
Since all words share the embedding space, semantic similarity between words may be
computed both monolingually and across languages. We will extensively use this property
in our evaluation tasks.
4. We were also experimenting with GloVe and CBOW, but they were falling short of SGNS on average.

962

fiBilingual Distributed Word Representations from Document-Aligned Data

4. Baseline Representation Models
We quickly navigate through other approaches to bilingual word representation learning
from document-aligned comparable data. The set of models in comparison may be roughly
clustered into two main groups: (Group I) pre-BWE baseline representation models from
document-aligned data, (Group II) benchmarking BWE induction models that were not
originally developed for learning from document-aligned comparable data. While it is essential to compare the BWESG model with other frameworks for learning representations
from document-aligned data (Group I), it is also crucial to detect main strengths of the
BWESG model when compared to other approaches in the BWE learning framework which
can also be adjusted to learn from document-aligned data (Group II).
4.1 Group I: Baseline Representation Models from Document-Aligned Data
We briefly describe three benchmarking Group I models.
4.1.1 Basic-MuPTM
The early approaches (e.g., Dumais, Landauer, & Littman, 1996; Carbonell, Yang, Frederking, Brown, Geng, Lee, Frederking, E, Geng, & Yang, 1997) tried to mine topical structure
from document-aligned comparable texts using a monolingual topic model (e.g., LSA or
LDA) trained on pseudo-bilingual documents with the target document simply appended
to its source language counterpart, and then used the discovered latent topical structure as
a shared semantic space in which both words and documents from two languages may be
represented in a uniform way.
More recent work on multilingual probabilistic topic modeling (MuPTM) (Mimno et al.,
2009; De Smet & Moens, 2009; Vulic et al., 2011) showed that word representations of higher
quality may be built if a multilingual topic model such as bilingual LDA (BiLDA) is trained
jointly on document-aligned comparable corpora by retaining the structure of the corpus
intact (i.e., there is no need to construct pseudo-bilingual documents).
MuPTM discovers the latent structure of the observed data in the form of K latent
cross-lingual topics z1 , . . . , zK which optimally describe the generation of observed data.
Extracting latent cross-lingual topics actually implies learning per-document topic distributions for each document in the corpus (probability scores P (zk |dj )), and discovering
language-specific representations of these topics given by per-topic word distributions in
each language (probability scores P (wiS |zk ) and P (wiT |zk )). Latent cross-lingual topics are
in fact distributions over vocabulary words, and have their language-specific representation
in each language. Per-document topic distributions and per-topic word distributions are
obtained after training the topic model on multilingual data. The representation of some
word w  V S (or in an analogous manner w  V T ) is then a K-dimensional vector:
w
~ = [P (z1 |w), . . . , P (zk |w), . . . , P (zK |w)].
We call this representation model (RM) Basic-MuPTM (BMu). Since the number of
topics, that is, the number of vector dimensions K is typically high (Dinu & Lapata, 2010;
Vulic et al., 2011), additional feature pruning (Reisinger & Mooney, 2010) may be employed
in order to retain only the most descriptive dimensions in the MuPTM-based representation,
963

fiVulic & Moens

which was shown to improve the performance on several semantic tasks (e.g., BLE or
SWTC) (Vulic & Moens, 2013a; Vulic et al., 2015).
A multilingual topic model is typically trained by Gibbs sampling (Geman & Geman,
1984; Steyvers & Griffiths, 2007; Vulic et al., 2015). Similar to the SGNS/BWESG training
procedure, Gibbs sampling for MuPTM/BiLDA also scans the training corpus word by
word, and then cyclically updates topic assignments for each word token. However, unlike
BWESG which uses only a subset of document-level training examples, Gibbs sampling for
MuPTM uses all words from the source language document as well as all words from its
coupled target language document to influence the topic assignment for the pivot word. The
BWESG design relying on data dropout leads to decreased training times and computation
costs to obtain final representations compared to Basic-MuPTM.
4.1.2 Association-MuPTM
Another representation is also based on the MuPTM framework: it contains association
scores P (wa |w) for each w, wa  V S  V T (Vulic & Moens, 2013a) as dimensions
of realP
P
(w
valued word vectors. These association scores are computed as P (wa |w) = K
a |zk )
k=1
S
P (zk |w) (Griffiths, Steyvers, & Tenenbaum, 2007), and the word vector is a (|V | + |V T |)S |w), P (w T |w), . . . , P (w T
dimensional vector: w
~ = [P (w1S |w), . . . , P (w|V
|w)]. As with
S|
1
|V T |
Basic-MuPTM, the original word representation may also be pruned post-hoc. We call
this representation model Association-MuPTM (AMu). Since this approach relies on the
MuPTM training plus additional |V S |  |V T | computations to estimate association scores,
the cost of obtaining Association-MuPTM representations is even higher than for BasicMuPTM, but it leads to more robust word representations for the BLE task (Vulic & Moens,
2013a). While both Basic-MuPTM and Association-MuPTM produce high-dimensional
real-valued vectors with plenty of near-zero dimensions (the number of dimensions is typically measured in thousands) which have to be pruned afterwards with the pruning parameter often set ad-hoc, BWESG produces lower-dimensional dense real-valued vectors, and
no additional post-hoc feature pruning is required for BWESG.
4.1.3 Traditional-PPMI
A traditional approach to building bilingual word representations in (cross-lingual) distributional semantics is to compute weighted co-occurrence scores (e.g., using PMI, TF-IDF)
between pivot words and their context words in a window of predefined size, plus an external bilingual lexicon to align context words/dimensions across languages (Gaussier et al.,
2004; Laroche & Langlais, 2010). A weighting function (WeF), which is a standard choice in
distributional semantics and yields optimal or near-optimal results over a group of semantic
tasks (Bullinaria & Levy, 2007), is the smoothed positive pointwise mutual information
statistic (Pantel & Lin, 2002; Turney & Pantel, 2010). Furthermore, in order to induce context words without the need for a readily available lexicon, we employ the bootstrapping
procedure of Peirsman and Pado (2011), and Vulic and Moens (2013b). This representation
model is called Traditional-PPMI (TPPMI). The word representation is an R-dimensional
vector: w
~ = [sc1 (w, c1 ), . . . , sck (w, ck ), . . . , scK (w, cK )]. The dimensions of the vector space
are K one-to-one word translation pairs ck = (cSk , cTk ), and sck (w, ck ) is the weighted co964

fiBilingual Distributed Word Representations from Document-Aligned Data

occurrence score of the pivot word w and the k-th context feature, where one computes the
co-occurrence score using cSk if w  V S , or cTk if w  V T .
Vector dimensions ck = (cSk , cTk ) in the Traditional-PPMI representation and similar
models with other WeFs are typically the most frequent and reliable translation pairs in
the corpus. As opposed to BWESG, the obtained word vectors are again high-dimensional
(typically thousands of dimensions) sparse real-valued vectors. In addition, traditionalPPMI is a purely local distributional model deriving distributional context knowledge from
narrow context windows (typically 3-10 surrounding words, e.g., Laroche & Langlais, 2010).
A bootstrapping approach (Vulic & Moens, 2013b) which we use to induce the TraditionalPPMI representation starts from an automatically learned seed lexicon of one-to-one translation pairs obtained using some other model (e.g., Basic-MuPTM or Association-MuPTM),
and then gradually detects new dimensions of the shared bilingual semantic space. We refer
the interested reader to the relevant literature (Vulic & Moens, 2013b) for more details.
4.2 Group II: BWE Induction Models Adjusted to Document-Aligned Data
We now provide a quick overview of three representative benchmarking BWE models that
learn from different types of bilingual and monolingual data.
4.2.1 BiCVM
Hermann and Blunsom (2014b) introduced a model called BiCVM (Bilingual Compositional
Vector Model) that learns bilingual word embeddings from a sentence-aligned parallel corpus
C = {s1 , s2 , . . . , sN } = {(sS1 , sT1 ), (sS2 , sT2 ), . . . , (sSN , sTN )}.5 sj = (sSj , sTj ) now denotes a pair
of aligned sentences. The model assumes that the aligned sentences have the same meaning,
which implies that their sentence representations should be similar. Assume two functions f
and g which map sentences given in the source and language respectively to their semantic
representations in Rd , where d is again the representation dimensionality. The energy of
the model given two sentences (sSj , sTj )  C is then defined as: E(sSj , sTj ) = ||f (sSj )  g(sTj )||.
The goal is to minimize E for all semantically equivalent sentences (i.e., aligned sentences)
in the corpus. In order to prevent the model from degenerating, they use a noise-contrastive
large-margin update which ensures that the representations of non-aligned sentences observe
a certain margin from each other. For every pair of parallel sentences (sSj , sTj ), they sample
a number of additional negative sentence pairs (sSj , nTneg ) from the corpus (i.e., the sampled
pairs are not observed as positive pairs in C). These noise samples are used in formulating
the hinge loss as follows: E(sSj , sTj ) = max(mrg + E(sSj , sTj , nTneg ), 0), where mrg is the
margin, and E(sSj , sTj , nTneg ) = E(sSj , sTj )  E(sSj , nTneg ). The loss is minimized for every
pair of parallel sentences in the corpus with L2-regularization on the model parameters.
The number of noise samples per each positive pair is a hyper-parameter of the model. A
semantic signal is propagated from aligned sentences back to the individual words to obtain
bilingual word embeddings. While the BiCVM model was originally built for sentencealigned parallel data, exactly the same idea may be applied to document-aligned non-parallel
data. In this paper, we test its ability to learn from noisier comparable data. The BWESG
5. A very similar (but more expensive) model which also learns from parallel sentence-aligned data was
also introduced by Chandar et al. (2014).

965

fiVulic & Moens

model is compared against BiCVM when inducing BWEs from both data types: comparable
and parallel.
4.2.2 Mikolovs Mapping
Another collection of BWE induction models (Mikolov et al., 2013b; Faruqui & Dyer, 2014;
Dinu, Lazaridou, & Baroni, 2015; Lazaridou, Dinu, & Baroni, 2015) assumes the following
setup: first, two monolingual embedding spaces, RdimS and RdimT , are induced separately in
each of the two languages using a standard monolingual WE model such as SGNS (Mikolov
et al., 2013a, 2013c). dimS and dimT denote the dimensionality of monolingual embedding
spaces in the source and target language respectively. The bilingual signal is provided in the
form of word translation pairs (xi , yi ), where xi  V S , yi  V T , and x~i  RdimS , y~i  RdimT .
Training is cast as a multivariate regression problem: it implies learning a function that
maps the source language vectors from the training data to their corresponding target
language vectors. A standard approach (Mikolov et al., 2013b; Dinu et al., 2015) is to
assume a linear map W  RdimS dimT , where a L2 -regularized least-squares error objective
(i.e., ridge regression) is used to learn the map W: it is learned by solving the following
optimization problem (typically by stochastic gradient descent):
minWRdimS dimT ||XW  Y||2F + ||W||2F .
X and Y are matrices obtained through the respective concatenation of source language
and target language vectors from training pairs. Once the linear map W is estimated, any
previously unseen source language word vector x~u may be straightforwardly mapped into
the target language embedding space RdimT as Wx~u . After mapping all vectors ~x, x  V S ,
the target embedding space RdimT in fact serves as a bilingual embedding space (Figure 1).
Although the main strength of the model is its ability to learn embeddings on larger
monolingual training sets, the model may also be adjusted to the setting where the only
training data are document-aligned comparable data as follows: (1) Automatically learn
a seed lexicon or reliable one-to-one translation pairs from document-aligned data using a
bootstrapping approach from Vulic and Moens (2013b), (2) Train two separate monolingual
embedding spaces on two separated halves of the document-aligned data set (i.e., using only
source language documents and only target language documents), (3) Learn the mapping
between the two spaces using the pairs from Step 1.
4.2.3 BilBOWA
Another collection of BWE induction models jointly optimizes two monolingual objectives,
with the cross-lingual objective acting as a cross-lingual regularizer during training (Klementiev et al., 2012; Gouws et al., 2015; Soyer, Stenetorp, & Aizawa, 2015). The idea
behind joint training may be summarized by the simplified formulation (Luong, Pham, &
Manning, 2015): (MonoS + MonoT ) + Bi.
The monolingual objectives M onoS and M onoT ensure that similar words in each language are assigned similar embeddings and aim to capture the semantic structure of each
language, whereas the cross-lingual objective Bi ensures that similar words across languages are assigned similar embeddings, and ties the two monolingual spaces together into
a bilingual space. Parameters  and  govern the influence of the monolingual and bilingual
966

fiBilingual Distributed Word Representations from Document-Aligned Data

components.6 The bilingual signal for these models, now acting as the cross-lingual regularizer during the joint training, is provided in sentence-aligned parallel data. Although they
use the same data sources, the models differ in the choice of monolingual and cross-lingual
objectives. In this work, we opt for the BilBOWA model of Gouws et al. (2015) as the representative model to be included in the comparisons, due to its previous solid performance
and robustness in the BLE task, its reduced complexity reflected in fast computations on
massive datasets, as well as its public availability. In short, the BilBOWA model combines
SGNS for the monolingual objectives together with the cross-lingual objective that minimizes the L2 -loss between the bag-of-word vectors of parallel sentences. For more details
about the exact training procedure, we refer the interested reader to the Gouws et al.s
work.
Again, although the main strength of the model is its ability to learn embeddings
on larger monolingual training sets, the model may also be adjusted to the setting with
document- or sentence-aligned data by: (1) using two halves of the aligned corpus for separate monolingual training, (2) using the alignment signal for bilingual training.

5. From Word Representations to Semantic Word Similarity
Assume now that we have induced bilingual word representations, regardless of the chosen
RM. Given two words wi and wj , irrespective to their actual language, we may compute the
degree of their semantic similarity by applying a similarity function (SF) on their vector
 and 
: sim(w , w ) = SF (
, 

representations 
w
w
w
i
j
i
j
i wj ). Different choices (or rather families
of) SFs are cosine, the Kullback-Leibler or the Jensen-Shannon divergence, the Hellinger
distance, the Jaccard index, etc. (Lee, 1999; Cha, 2007), and different RMs typically require
different SFs to produce optimal or near-optimal results over various semantic tasks. When
working with word embeddings, a standard choice for SF is cosine similarity (cos) (Mikolov
et al., 2013c), which is also a typical choice in traditional distributional models (Bullinaria
& Levy, 2007). The similarity is then computed as follows:



w
i wj
sim(wi , wj ) = cos(wi , wj ) = 
|  | 
|
|w
w
i
j

(4)

On the other hand, a good choice for SF when working with probabilistic RMs such as
Basic-MuPTM and Association-MuPTM RS is the Hellinger distance (Pollard, 2001; Cha,
2007; Kazama, Saeger, Kuroda, Murata, & Torisawa, 2010), which displays excellent results
in the BLE task (Vulic & Moens, 2013a). The similarity between words wi and wj using
the Hellinger distance is computed as follows:
v
u K q
q
2
X
1 u
P (fk0 |wi )  P (fk0 |wj )
(5)
sim(wi , wj ) =  t
2 i=1
Note that the Hellinger distance is applicable only if word representations are probability
distributions, which is the case for Basic-MuPTM and Association-MuPTM. P (fk0 |wi ) de6. Setting  = 0 reduces the model to the setting similar to BiCVM (Hermann & Blunsom, 2014b).  = 1
results in the models of Klementiev et al. (2012), Gouws et al. (2015), and Soyer et al. (2015).

967

fiVulic & Moens

notes the probability score for the k-th dimension (fk0 ) in the vector representation with
Basic-MuPTM or Association-MuPTM.7
For each word wi , we can build a ranked list RL(wi ) which consists of all other words wj
ranked according to their respective semantic similarity scores sim(wi , wj ). Additionally,
we label the ranked list RL(wi ) that is pruned at position M as RLM (wi ). Since we may
retain language labels for words when training in multilingual settings (e.g., language labels
are marked by different colors in Figure 2), we may compute: (1) monolingual similarity,
e.g., given wi  V S , we retain only wj  V S in the ranked list (analogous for wi  V T ),
(2) cross-lingual similarity (CLSS), e.g., given wi  V S , we retain only wj  V T , and (3)
multilingual similarity, where we retain all words wj  V S  V T . When computing CLSS
for wi , the most similar word cross-lingually is called the cross-lingual nearest neighbor.
We will employ the models of context-insensitive CLSS at the word type level to extract bilingual lexicons from document-aligned or sentence-aligned data, and to compare all
representation models in the BLE task in Section 7.
5.1 Context Sensitive Models of (Cross-Lingual) Semantic Similarity
The context-insensitive models of semantic similarity provide ranked lists of semantically
similar words invariably or in isolation, and they operate at the level of word types. They
do not explicitly encode different word senses. In practice, it means that, given a sentence
The coach of his team was not satisfied with the game yesterday., these context-insensitive
CLSS models are not able to detect that the Spanish word entrenador is more similar to
the polysemous English word coach in the context of this sentence than the Spanish word
autocar, although autocar is listed as the most semantically similar word to coach globally/invariably without any observed context. In another example, while the Spanish words
partido, encuentro, cerilla or correspondencia are all highly similar to another ambiguous
English word match when observed in isolation, given the Spanish sentence She was unable to find a match in her pocket to light up a cigarette., it is clear that the strength of
cross-lingual semantic similarity should change in context as only cerilla exhibits a strong
cross-lingual semantic similarity to match within this particular sentential context.
The goal now is to build BWE-based models of cross-lingual semantic similarity in
context, similar to context-aware CLSS models proposed by Vulic and Moens (2014). Two
key questions are: (i) How to provide BWE-based representations beyond word level to
represent the context of a word token?; (ii) How to use the contextual knowledge in a
context-sensitive model of semantic similarity?
Following Vulic and Moens (2014), given a word token w in context (e.g., a window of
words, a sentence, a paragraph, or a document), we build its context set or rather context
bag Con(w) = {cw1 , . . . , cwr } by harvesting r neighboring words in the chosen context scope
(e.g., the context bag may comprise all content-bearing words in the same sentence as the
pivot word token, the so-called sentential context). In order to present the context Con(w)
in the d-dimensional embedding space, we need to apply a model of semantic composition

to learn its d-dimensional vector representation Con(w).
7. Prior work has shown that the results for Basic-MuPTM and Association-MuPTM are slightly higher
when cosine is replaced with the Hellinger distance. Therefore, in this particular case we have opted for
the Hellinger distance to report a more competitive baseline.

968

fiBilingual Distributed Word Representations from Document-Aligned Data

Formally, given word w, we may specify the vector representation of the context bag
Con(w) as the d-dimensional vector/embedding:
  

Con(w) = cw1 ? cw2 ? . . . ? 
cw
r

(6)

, . . . , 
 are d-dimensional WEs learned from the data, and ? is a compositional
where 
cw
cw
1
r
vector operator such as addition, point-wise multiplication, tensor product, etc.
A plethora of models for semantic composition have been proposed in the relevant literature, differing in their choice of vector operators, input structures and required knowledge
(Mitchell & Lapata, 2008; Baroni & Zamparelli, 2010; Rudolph & Giesbrecht, 2010; Socher,
Huval, Manning, & Ng, 2012; Blacoe & Lapata, 2012; Clarke, 2012; Hermann & Blunsom,
2014b; Milajevs, Kartsaklis, Sadrzadeh, & Purver, 2014), to name only a few. In this work,
driven by the observed linear linguistic regularities in the embedding spaces (Mikolov et al.,
2013d), we opt for simple addition (denoted by +) from Mitchell and Lapata (2008) as
the compositional operator, due to its simplicity, the ease of applicability on bag-of-words
contexts, and its relatively solid performance in various compositional tasks (Mitchell &

Lapata, 2008; Milajevs et al., 2014). The d-dimensional embedding Con(w) is then:
  

Con(w) = cw1 + cw2 + . . . + 
cw
r

(7)

If we use any BWE-based RM, we may compute the context-sensitive semantic similarity score sim(wi , tj , Con(wi )) between tj and wi given its context Con(wi ) in the shared
bilingual embedding space as follows:

 

sim(wi , tj , Con(wi )) = SF (wi0 , tj )

(8)





tj  V T is any target language word, and tj its word representation, while wi0 is the new
contextualized vector representation for wi modulated by its context Con(wi ), that is,
its context-aware representation. Vulic and Moens (2014) introduced a linear interpolation
of two d-dimensional vectors as a plausible solution for the modulation/contextualization.
The modulation of representation for wi is computed as follows:



+
wi0 = (1  )  
w
Con(wi )
i

(9)


 is the word embedding for w computed at the word type level, 
where 
w
Con(wi ) is the
i
i
embedding for the context bag computed using eq. (7), and  is an interpolation parameter.
Another set of similar models that can yield context-sensitive similarity computations has
been proposed very recently, and has displayed very competitive results regardless of its
simplicity (Melamud, Levy, & Dagan, 2015). Here, we present two best scoring contextsensitive models which we adapt to the bilingual setting:
P
SF (wi , tj ) + cwi Con(wi ) SF (cwi , tj )
Add-Melamud: sim(wi , tj , Con(wi )) =
|Con(wi )| + 1
s
Y
Mult-Melamud: sim(wi , tj , Con(wi )) = |Con(wi )|+1 SF (wi , tj ) 
SF (cwi , tj )
cwi Con(wi )

969

fiVulic & Moens

Note that for the Mult model one has to avoid negative values, so a simple shift to an allpositives interval is required, e.g., the shifted cosine score becomes cos0 (x, y) = cos(x,y)+1
.
2
Unlike the models of Vulic and Moens, these two models do not aggregate single word
representations into one vector that represents the context, but compute similarity scores
separately with each word from the context. For more details regarding the models, we
refer the interested reader to the original Melamud et al.s work .
We will employ the models of context-sensitive CLSS at the word token level to compare
all representation models in the task of suggesting word translations in context in Section 8.

6. Training Setup
In this section, we provide an insight into training data and experimental setup for our
BWESG model and all baseline models.
6.1 Training Data
To induce bilingual word embeddings as well as to be directly comparable with baseline representations from prior work, we use a dataset comprising a subset of comparable Wikipedia data available in three language pairs (Vulic & Moens, 2013b, 2014)8 : (i)
a collection of 13, 696 Spanish-English Wikipedia article pairs (ES-EN), (ii) a collection
of 18, 898 Italian-English Wikipedia article pairs (IT-EN), and (iii) a collection of 7, 612
Dutch-English Wikipedia article pairs (NL-EN). All corpora are theme-aligned comparable corpora, that is, the aligned document pairs discuss similar themes, but are in general
not direct translations of each other. To be directly comparable to prior work in the two
evaluation tasks (Vulic & Moens, 2013b, 2014), we retain only nouns that occur at least
5 times in the corpus. Lemmatized word forms are recorded when available, and original
forms otherwise. TreeTagger (Schmid, 1994) is used for POS tagging and lemmatization.
After the preprocessing steps vocabularies comprise between 7,000 and 13,000 noun types
for each language in each language pair, and the training corpora are quite small: ranging
from approximately 1.5M tokens for NL-EN to 4M for ES-EN. Exactly the same training
data and vocabularies are used to train all representation models in comparison (both from
Group I and Group II, see Section 4).
We also demonstrate that it is simple and straightforward to train BWESG on parallel sentence-aligned data using the same modeling principles. For that purpose, we use
Europarl.v7 (Koehn, 2005) for all three language pairs obtained from the OPUS website
(Tiedemann, 2012).9 As the only preprocessing step, we retain only words occurring at
least 5 times in the corpus. Each corpus contains approximately 2M parallel sentences,
vocabularies are by an order of magnitude larger than from the smaller Wikipedia data
(i.e., varying from 45K EN word types to 75K NL word types), and the corpora sizes are
approximately 120M tokens. Data statistics of the two data sources, Wikipedia vs Europarl,
are provided in Table 1. The statistics reveal the different nature of the two corpora, with
significantly more variance and noise reported for the Wikipedia data.
8. Available online: people.cs.kuleuven.be/~ivan.vulic/software/
9. http://opus.lingfil.uu.se/

970

fiBilingual Distributed Word Representations from Document-Aligned Data

Corpus:

Wikipedia

Europarl

Pair:

ES-EN

IT-EN

NL-EN

ES-EN

IT-EN

NL-EN

Average length (OTHER)
Average length (EN)
Average length difference

111
174
127

84
154
125

51
129
102

29
28
3

29
29
4

27
27
4

Table 1: Training data statistics: Non-parallel document-aligned Wikipedia vs parallel
sentence-aligned Europarl for all three language pairs. OTHER = ES, IT or NL.
Lengths are measured in word tokens. Averages are rounded to the closest integer.
6.2 Trained BWESG Models
To test the effect of random shuffling in the merge and shuffle BWESG strategy, we have
trained the BWESG model with 10 random corpora shuffles for all three training corpora.
We also train BWESG with the length-ratio shuffle strategy. All parameters are set to
default suggested parameters for SGNS from the word2vec package: stochastic gradient
descent (SGD) with a linearly decreasing global learning rate of 0.025, 25 negative samples,
subsampling rate 1e  4, and 15 epochs.
We have varied the number of dimensions d = 100, 200, 300. We have also trained
BWESG with d = 40 to be directly comparable to readily available sets of BWEs from
prior work (Chandar et al., 2014). Moreover, to test the effect of window size on the final
results, i.e., the number of positives used for training, we have varied the maximum window
size cs from 4 to 60 in steps of 4.10
We will make our pre-training and training code for BWESG publicly available, along
with all BWESG-based bilingual word embeddings for the three language pairs at:
http://liir.cs.kuleuven.be/software.php.
6.3 Baseline Representations: Group I
All parameters of the baseline representation models (i.e., topic models and their settings,
the number of dimensions K, the values for feature pruning, window size, weighting and
similarity functions) were optimized in prior work. Therefore, the settings are adopted
directly from previous work (Griffiths et al., 2007; Bullinaria & Levy, 2007; Dinu & Lapata,
2010; Vulic & Moens, 2013a, 2013b; Kiela & Clark, 2014), and we encourage the interested
reader to check the details and exact parameter setup in the relevant literature. We provide
only a short overview here.
For Basic-MuPTM and Association-MuPTM, as in the work of Vulic and Moens (2013a),
a bilingual latent Dirichlet allocation (BiLDA) model was trained with K = 2000 topics
and the standard values for hyper-parameters:  = 50/K,  = 0.01 (Steyvers & Griffiths,
2007). Post-hoc semantic space pruning was employed with the pruning parameter set to
200 for Basic-MuPTM and to 2000 for Association-MuPTM. We refer the reader to the
relevant paper for more details.
For Traditional-PPMI, as in the work of Vulic and Moens (2013b), a seed lexicon was
automatically obtained by bootstrapping from the initial seed lexicon of reliable pairs stem10. We remind the reader that we slightly abuse terminology here, as the BWESG windows do not include
the locality component any more.

971

fiVulic & Moens

ming from the Association-MuPTM model (with the same parameters for AssociationMuPTM as listed above). The window size was fixed to 6 in both directions. We again
refer the reader to the paper for more details.
6.4 Baseline Representations: Group II
All baseline BWE models were trained with the same number of dimensions as BWESG:
d = 100, 200, 300. Other model-specific parameters were taken as suggested in prior work.
For BiCVM, we use the tool released by the authors.11 We train an additive model, with
hinge loss margin mrg = d as in the original paper, batch size of 50, and noise parameter
of 10. All models were trained with 200 iterations.
For Mikolov, we train two monolingual SGNS models using the original word2vec
package, SGD with a global learning rate of 0.025, 25 negative samples, subsampling rate
1e  4, and 15 epochs. The seed lexicon required to learn the mapping between two monolingual spaces is exactly the same as for Traditional-PPMI.
For BilBOWA, we use SGD with a global learning rate 0.15 for training12 , 25 negative
samples, subsampling rate 1e  4, and 15 epochs. For BilBOWA and Mikolov, we vary
the window size the same way as in BWESG.
6.5 Similarity Functions
Unless stated otherwise, a similarity function used in all similarity computations with all
RMs is cosine (cos). The only exceptions are Basic-MuPTM and Association-MuPTM
where the Hellinger distance (HD) was used since it consistently outperformed cosine for
these two RM types in prior work (see Footnote 7).
6.6 A Roadmap to Experiments
In the first experiment, we quickly visually inspect the obtained lists of semantically similar words using the BWESG bilingual representation model. Following that, we compare
BWESG-based models for bilingual lexicon extraction (BLE) and suggesting word translations in context (SWTC) against both groups of baseline models discussed in Section 4. The
experiments and results for the BLE task are presented in Section 7, while the experiments
and results for SWTC are presented in Section 8.

7. Evaluation Task I: Bilingual Lexicon Extraction
One may employ the context-insensitive CLSS models from Section 5 to extract bilingual
lexicons automatically from data.

11. https://github.com/karlmoritz/bicvm
12. Suggestions for parameter values received through personal correspondence with the authors. The software is available online: https://github.com/gouwsmeister/bilbowa

972

fiBilingual Distributed Word Representations from Document-Aligned Data

Spanish-English (ES-EN)
(1)
reina

(2)
reina

(3)
reina

(Spanish)

(English)

rey
trono
monarca
heredero
matrimonio
hijo
reino
reinado
regencia
duque

queen(+)
heir
throne
king
royal
reign
succession
princess
marriage
prince

Italian-English (IT-EN)
(1)
madre

Dutch-English (NL-EN)

(2)
madre

(3)
madre

(Combined) (Italian)

(English)

(Combined) (Dutch)

queen(+)
rey
trono
heir
throne
monarca
heredero
king
matrimonio
royal

mother(+)
father
sister
wife
daughter
son
friend
childhood
family
cousin

mother(+)
padre
moglie
father
sorella
figlia
figlio
sister
fratello
wife

padre
moglie
sorella
figlia
figlio
fratello
casa
amico
marito
donna

(1)
schilder

(2)
schilder

(3)
schilder

(English)

(Combined)

kunstschilderpainter(+)
schilderij
painting
kunstenaar portrait
olieverf
artist
portret
canvas
schilderen brush
frans
cubism
nederlands art
componist poet
beeldhouwer drawing

painter(+)
kunstschilder
painting
schilderij
kunstenaar
portrait
olieverf
portret
schilderen
artist

Table 2: Example lists of top 10 semantically similar words for all 3 language pairs obtained using BWESG (length-ratio shuffle); d = 200, cs = 48; (col 1.) only source
language words (ES/IT/NL) are listed while target language words are skipped
(monolingual similarity); (2) only target language words (EN) are listed (crosslingual similarity); (3) words from both languages are listed (multilingual similarity). The correct one-to-one translation is marked by (+).

7.1 Task Description
By harvesting cross-lingual nearest neighbors, one is able to build a bilingual lexicon of
one-to-one translation pairs (wiS , wjT ). We test the validity of our BWEs and baseline
representations in the BLE task.
7.2 Experimental Setup
Test Data For each language pair, we evaluate on standard 1,000 ground truth one-to-one
translation pairs built for the three language pairs (ES/IT/NL-EN) by Vulic and Moens
(2013a, 2013b). Translation direction is ES/IT/NL  EN. The data is available online.13
Evaluation Metrics Since we can build a one-to-one bilingual lexicon by harvesting
one-to-one translation pairs, the lexicon quality is best reflected in the Acc1 score, that is,
the number of source language (ES/IT/NL) words wiS from ground truth translation pairs
for which the top ranked word cross-lingually is the correct translation in the other language
(EN) according to the ground truth over the total number of ground truth translation pairs
(=1000) (Gaussier et al., 2004; Tamura et al., 2012; Vulic & Moens, 2013b). Similar trends
are observed within a more lenient setting with Acc5 and Acc10 scores, but we omit these
results for clarity and the fact that the actual BLE performance is best reflected in Acc1 .

13. http://people.cs.kuleuven.be/ ivan.vulic/software/

973

fiVulic & Moens

Spanish-English (ES-EN)

Italian-English (IT-EN)

BWESG

BMu

AMu

TPPMI

BWESG

BMu

AMu

TPPMI

cebolla

cebolla

cebolla

cebolla

golfo

golfo

golfo

golfo

onion(+)
dish
marinade
cuisine
soup
sauce
cheese
coriander
vegetable
tortilla

dessert
salad
nut
walnut
rice
toast
porridge
paddy
tuber
potato

dessert
walnut
salad
nut
hazelnut
porridge
rice
marinade
toast
paddy

sauce
cheese
garlic
salad
chili
onion(+)
cuisine
flavor
bread
dish

gulf(+)
coast
coastline
bay
island
peninsula
settlement
shore
tourism
ferry

whale
dolphin
coast
suborder
cadmium
ferry
monsoon
fjord
isthmus
mainland

coast
isthmus
coastline
fjord
ferry
monsoon
mainland
seaside
isle
suborder

coast
sea
island
bay
lagoon
harbour
beach
shore
river
lake

Table 3: Example lists of top 10 semantically similar words for ES-EN and IT-EN, obtained
using BWESG (length-ratio, d = 200, cs = 48), and the three representation
models from Group I. The correct translation is marked by (+).
7.3 Results and Discussion
Table 2 displays top 10 semantically similar words monolingually, across-languages and
combined/multilingually for one ES, IT and NL word, while Table 4 shows the first set of
BLE results.
7.3.1 Experiment 0: Qualitative Analysis and Comparison
BWESG is able to find semantically coherent lists of words for all three directions of similarity (i.e., monolingual, cross-lingual, multilingual). In the combined (multilingual) ranked
lists, words from both languages are represented as top similar words. This initial qualitative analysis already demonstrates the ability of BWESG to induce a shared bilingual
embedding space using only document alignments as bilingual signals.14
In another brief analysis, we qualitatively compare the cross-lingual ranked lists acquired
by BWESG with the other three baseline CLSS/BLE models from Group I. The lists for one
ES word and one IT word are presented in Table 3. For the two example words, BWESG is
the only model which is able to rank the actual correct translations as nearest cross-lingual
neighbors. It is already symptomatic that the word gulf, which is the correct translation
for golfo, does not occur in the ranked list RL10 (golf o) at all in case of the three baseline
models. We will soon quantitatively confirm this initial suspicion, and demonstrate that
BWESG is superior to the three baseline models in the BLE task.
As an aside, Table 3 also clearly reveals the difficulty of judging the quality of models
for computing semantic similarity/relatedness solely based on the observed output of the
models. The lists RL10 (cebolla) and RL10 (golf o) appear significantly different across all
14. We also conducted a small experiment on solving word analogies using monolingual English embedding spaces, and then we repeated the experiment with the same vocabulary and bilingual EnglishSpanish/Italian/Dutch embedding spaces. The results follow the findings of Faruqui and Dyer (2014),
where only slight (and often insignificant) fluctuations for SGNS vectors were reported (e.g., the fluctuations are < 1% on average in our experiments) when moving from monolingual to bilingual embedding
spaces. We may conclude that the linguistic regularities established for monolingual embedding spaces
(Mikolov et al., 2013d) induced by SGNS also hold in bilingual embedding spaces induced by BWESG.

974

fiBilingual Distributed Word Representations from Document-Aligned Data

Pair:
BWESG
Merge and Shuffle
cs:16,MIN
cs:16,AVG
cs:16,MAX
cs:48,MIN
cs:48,AVG
cs:48,MAX

ES-EN

IT-EN

NL-EN

d=100

d=200

d=300

d=100

d=200

d=300

d=100

d=200

d=300

0.607
0.617
0.625
0.658
0.665
0.675

0.600
0.613
0.630
0.676
0.685
0.694

0.577
0.596
0.613
0.672
0.688
0.705

0.585
0.599
0.607
0.662
0.669
0.677

0.597
0.601
0.606
0.677
0.683
0.692

0.571
0.583
0.596
0.672
0.683
0.689

0.293
0.300
0.307
0.378
0.389
0.394

0.244
0.254
0.267
0.366
0.381
0.395

0.219
0.224
0.233
0.354
0.363
0.377

d=100

d=200

d=300

d=100

d=200

d=300

d=100

d=200

d=300

0.627
0.678

0.610
0.701

0.602
0.703

0.613
0.679

0.614
0.689

0.595
0.692

0.303
0.397

0.275
0.396

0.237
0.382

BWESG
No Shuffling
cs:16
cs:48

d=100

d=200

d=300

d=100

d=200

d=300

d=100

d=200

d=300

0.218
0.511

0.176
0.497

0.139
0.480

0.209
0.523

0.198
0.540

0.162
0.526

0.070
0.214

0.068
0.198

0.049
0.197

BMu
AMu
TPPMI

0.441
0.518
0.577

0.441
0.518
0.577

0.441
0.518
0.577

0.575
0.618
0.647

0.575
0.618
0.647

0.575
0.618
0.647

0.237
0.236
0.206

0.237
0.236
0.206

0.237
0.236
0.206

BWESG
Length-Ratio
cs:16
cs:48

Table 4: BLE performance in terms of Acc1 scores for all tested BLE models for SpanishEnglish, Italian-English and Dutch-English with all bilingual word representations
learned from document-aligned Wikipedia data. For BWESG with merge and
shuffle we report maximum (MAX), minimum (MIN) and average (AVG) scores
over 10 random corpora shuffles. Highest scores per column are in bold.
four models, yet all these lists contain words which appear semantically related to the source
word. Therefore, we require a more systematic quantitative task-oriented comparison of
induced word representations.
7.3.2 Experiment I: BWESG vs Group I
Table 4 shows the first set of results in the BLE task: we report scores with two different
BWESG strategies as well as with a BWESG model which does not shuffle pseudo-bilingual
documents. The previous best reported Acc1 scores with baseline representations for the
same training+test combination are also reported in the table. By zooming into the table
multiple times, we summarize the most important findings.
BWESG vs Baseline Representations The results clearly reveal the superior performance of the BWESG model for BLE which relies on our new framework for inducing
bilingual word embeddings from document-aligned comparable data over other BLE models
relying on previously used bilingual word representations from the same type of training
data. The increase in Acc1 scores over the best scoring baseline models is 22.2% for ES-EN,
7% for IT-EN and 67.5% for NL-EN.
BWESG Shuffling Strategy Although both BWESG strategies display results that are
above established baselines, there is a clear advantage to the length-ratio shuffle strategy,
which displays a solid and robust performance across a variety of parameters and all three
language pairs. Another advantage of that strategy is the fact that it has a deterministic
outcome and does not suffer from sub-optimal random shuffles. In summary, we suggest
975

fiAcc1 scores

Vulic & Moens

0.7

0.7

0.6

0.6

0.5

0.5

0.4

0.4

0.3

0.3

0.2

0.2

0.5

0.4

0.3

0.2

0.1
0.1

d = 100
d = 200
d = 300

0.0
4

8 12 16 20 24 28 32 36 40 44 48 52 56 60
Window size

(a) Spanish-English

0.1

d = 100
d = 200
d = 300

0.0
4

8 12 16 20 24 28 32 36 40 44 48 52 56 60
Window size

(b) Italian-English

d = 100
d = 200
d = 300

0.0
4

8 12 16 20 24 28 32 36 40 44 48 52 56 60
Window size

(c) Dutch-English

Figure 3: Acc1 scores in the BLE task with BWESG length-ratio shuffle for all 3 language
pairs, and varying values for parameters cs and d. Solid (red) horizontal lines
denote the highest baseline Acc1 scores for each language pair. Thicker dotted
lines refer to BWESG without shuffling.
using the length-ratio shuffle strategy in future work, and along the same line we opt for
that strategy in all further experiments.
The results also reveal that shuffling is universally useful, as BWESG without shuffling
relies largely on monolingual contexts and cannot reach the performance of BWESG with
shuffling. A partial remedy for the problem is to train BWESG with more documentlevel training pairs (i.e., by increasing the window size), but that leads to prohibitively
expensive models, and nonetheless BWESG without shuffling with larger cs-s still falls
short of BWESG with both shuffling strategies (see also Figures 3(a)-3(c)).
Window Size: Number of Training Pairs The results confirm the intuition that larger
window sizes, i.e., more training examples lead to better results in the BLE task. For all
embedding dimensions d-s, BWESG exhibits a superior performance for cs = 48 than for
cs = 16, and the performance with cs = 48 and cs = 60 seems relatively stable: intuitively,
more training pairs leads to a slightly better BLE performance, but the curve slowly flattens
out (Figures 3(a)-3(c)). This finding reveals that even a coarse tuning of these parameters
might lead to optimal or near-optimal scores for BLE with BWESG.
Differences across Language Pairs A lower increase in Acc1 scores for IT-EN is attributed to the fact that the test set for IT-EN comprises IT words with occurrence frequencies above 200 in the training data (Vulic & Moens, 2013a), while the other two test sets
comprise randomly sampled words covering all frequency spectra. As expected, all models
in comparison are able to effectively utilize distributional signals for higher-frequency words,
but BWESG still displays the best performance, and these improvements in Acc1 scores are
statistically significant (using McNemars statistical significance test, p < 0.05).15
Further, the lowest overall scores for all models in comparison are observed for NL-EN.
We attribute it to using less training data for NL-EN when compared to ES-EN and IT-EN
(i.e., training corpora for ES-EN and IT-EN are almost triple the size of training corpora
for NL-EN). However, we observe that the increase obtained by BWESG is even more
prominent in this setting with limited training data. The lower results of TPPMI compared
15. McNemars significance test is very common in the NLP literature, especially when Acc1 scores are
reported. It utilizes the standard 22 contingency table, and may be observed as a paired version of the
more common chi-square test. The reader is referred to the original work of McNemar (1947).

976

fiBilingual Distributed Word Representations from Document-Aligned Data

Pair:
BWESG
Length-Ratio
cs:48

ES-EN

IT-EN

NL-EN

d=100

d=200

d=300

d=100

d=200

d=300

d=100

d=200

d=300

0.678

0.701

0.703

0.679

0.689

0.692

0.397

0.396

0.382

Mikolov
cs:4
cs:8
cs:16
cs:48
cs:60

0.187
0.305
0.344
0.311
0.324

0.151
0.306
0.396
0.375
0.389

0.282
0.420
0.486
0.477
0.479

0.368
0.462
0.472
0.458
0.460

0.382
0.518
0.539
0.536
0.538

0.533
0.582
0.602
0.591
0.597

0.042
0.076
0.117
0.132
0.151

0.068
0.095
0.161
0.178
0.180

0.120
0.145
0.184
0.202
0.209

BiCVM
iterations:200

0.342

0.384

0.403

0.309

0.366

0.377

0.068

0.084

0.083

Table 5: BLE results: Comparison of BWESG with (1) the BWE induction model of
Mikolov et al. (2013b) relying on SGNS, (2) BiCVM: the BWE induction model
of Hermann and Blunsom (2014b) initially developed for parallel sentence-aligned
data. All models were trained on the same document-aligned training Wikipedia
data with exactly the same vocabularies.
to other two baseline models are also attributed to the overall lower quality and size of
NL-EN training data, which is then reflected in a lower quality of seed lexicons necessary
to start the bootstrapping procedure from Vulic and Moens (2013b).
Computational Complexity BWESG trained with larger values for d and cs yields richer
semantic representations, but also naturally leads to increased training times. However, due
to a lightweight design of the supporting SGNS, the times are by the order of magnitude
lower than the training times for Basic-MuPTM or Association-MuPTM. Typically, several
hours are needed to train BWESG with d = 300 and cs  48  60, whereas it takes two to
three days to train a bilingual topic model with K = 2000 on the same training set using
the multi-threaded architectures on 10 Intel(R) Xeon(R) CPU E5-2667 2.90GHz processors.
The BWESG model scales as expected (i.e., training time increases linearly with the window
size with all other parameters being equal), and enjoys all the advantages (training time-wise
and memory-wise) of the original word2vec package. A logical explanation for the behaviour
follows from the interpretation of SGNS provided by Levy and Goldberg (2014a), e.g., using
a window size of 48 instead of a window size 16 basically means using 3 times more positive
examples for training (e.g., approximately 15 minutes is needed to train 300-dimensional
ES-EN BWESG embeddings with cs = 16 using the Wikipedia data as opposed to 46
minutes with cs = 48, measured again on 10 Intel(R) Xeon(R) processors).
7.3.3 Experiment II: BWESG vs Other BWE Induction Models (Group II)
All further experiments are conducted using BWESG with the length-ratio shuffle strategy.
Note that again all models in comparison use exactly the same data sources and vocabularies
as BWESG and Group I models from the previous section. The results with BiCVM and the
Mikolov model are summarized in Table 5: the comparison reveals a clear and prominent
advantage for the BWESG model given the same data and training setup. We do not
report absolute scores of the BilBOWA model in this setup as they were much lower than
the other two baseline models. The BiCVM model, although in theory fit to learn from
977

fiVulic & Moens

0.6

0.62

0.6
0.55

Acc1 scores

0.58

0.5

0.56

0.54
0.45

0.4
2

4

8
Window size

d = 100
d = 200
d = 300
d = 100
d = 200
d = 300
16

(BWESG)
(BWESG)
(BWESG)
(BilBOWA)
(BilBOWA)
(BilBOWA)

0.52

d = 100 (BiCVM)
d = 200 (BiCVM)
d = 300 (BiCVM)

0.5
48

2

4

(a) Spanish-English

8
Window size

16

48

(b) Italian-English

0.7

0.66

Acc1 scores

0.62

0.58

0.54

0.5

0.46
2

4

8
Window size

16

48

(c) Dutch-English

Figure 4: Comparison of BWESG (solid curves) with two other models that rely on parallel training data: (1) BilBOWA (dotted curves), (2) BiCVM: the BWE induction modelinitially developed for parallel sentence-aligned data (dashed horizontal
lines). All models were trained on the same sentence-aligned training Europarl
data with exactly the same vocabularies. BLE is performed over the same search
space for all models. x axes are in log scale.
document-aligned data, is unable to compete with BWESG when learning BWEs from the
noisier setting with non-parallel data.
We also present a preliminary study where we compare BWSESG and Group II models in
the setup with parallel sentence-aligned data. Results are summarized in Figures 4(a)-4(c).16
The preliminary results clearly demonstrate that BWESG is able to learn BWEs from
parallel data without the slightest change in its modeling principles. While the BilBOWA
model displays better results for lower values of the cs parameter, to our own surprise, the
16. Note that the absolute scores are not directly comparable to the BLE scores when the model is trained
on Wikipedia data (Tables 4 and 5) due to different training data, different preprocessing steps and
vocabularies. Different vocabularies also result in different BLE search spaces and coverages of the test
sets (e.g., some very common Spanish nouns from the test set such as nadador (swimmer) or colmillo
(tusk) are not observed in Europarl due to the domain shift).

978

fiBilingual Distributed Word Representations from Document-Aligned Data

BWESG model is comparable to or even better than the baseline models with larger window
sizes. The BiCVM model, which implicitly utilizes the entire sentence span in training
also outperforms BWESG with smaller windows, but BWESG again performs significantly
better with larger windows. The BWESG performance flattens out quicker than with the
Wikipedia data (compare the results with cs = 16 and cs = 48), which is easily explained by
the decreased length of aligned items as provided in Table 1 (i.e., sentences vs documents).
For English-Spanish, we can also compare BWESG to pre-trained 40-dimensional embeddings of Chandar et al. (2014), as their embeddings were also induced on the same
Europarl data. While their models Acc1 score is 0.432 for d = 40, BWESG obtains Acc1
scores of 0.502 (d = 40, cs = 8), 0.535 (d = 40, cs = 16) or 0.529 (d = 40, cs = 48).

8. Evaluation Task II: Suggesting Word Translations in Context
In another task, we test the ability of BWEs to produce context-sensitive semantic similarity
modeling (see Section 5.1), which in turn may be used to solve the task of suggesting word
translations in context (SWTC) proposed recently (Vulic & Moens, 2014). The goal now
is to build BWESG-based models for SWTC given the sentential context, similar as in
the prior work. We show that our new BWESG-based SWTC models outperform the best
SWTC models of Vulic and Moens, as well as other SWTC models which rely on the baseline
word representations discussed in Section 4.
8.1 Task Description
Given an occurrence of a polysemous word wi  V S and the context of that occurrence, the
SWTC task is to choose the correct translation in the target language LT of that particular
occurrence of wi from the given set T C(wi ) = {t1 , . . . , ttq }, T C(wi )  V T , of its tq possible
translations/meanings. We may refer to T C(wi ) as an inventory of translation candidates
for wi . The task of suggesting word translations in context (SWTC) may be interpreted as
ranking the tq translation candidates with respect to the observed local context Con(wi )
of the occurrence of the word wi . The best scoring translation candidate according to the
scores sim(wi , tj , Con(wi )) (see Section 5.1) in the ranked list is then the correct translation
for that particular occurrence of wi observing its local context Con(wi ).
8.2 Experimental Setup
Test Data We use the SWTC test set introduced recently (Vulic & Moens, 2014). The test
set comprises 15 polysemous nouns in three languages (ES, IT and NL) along with sets of
their translation candidates (i.e., sets T C). For each polysemous noun, the test sets provide
24 sentences extracted from Wikipedia which illustrate different senses and translations of
the pivot polysemous noun, accompanied by the annotated correct translation for each sentence. It yields 360 test sentences for each language pair (and 1080 test sentences in total).
An additional set of 100 IT sentences (5 other polysemous IT nouns plus 20 sentences for
each noun) is used as a development set to tune the parameter  (see Section 5.1) for all
language pairs and all models in comparison. In summary, the final aim may be formulated
as follows: For each polysemous word wi in ES/IT/NL, the goal is to suggest its correct
translation in English given its sentential context.
979

fiVulic & Moens

Evaluation Metrics Since the task is to present a list of possible translations to a SWTC
model, and then let the model decide a single most likely translation given the word and
its sentential context, we measure the performance again as Top 1 accuracy (Acc1 ).
8.3 Results and Discussion
We again compare against Group I and Group II models. Note that the Group I models
held previously best reported SWTC scores when training on the Wikipedia data that we
also use in this work.
8.3.1 Experiment I: BWESG vs Group I
Models in Comparison (1) BWESG+add. RM: BWESG. SF: cos. Composition: addition.  = 1.0. The value for  suggests that only context is used to disambiguate the
meaning of a polysemous word and to guess its most likely translation in context.17
(2) BMu+HD+S. RM: BasicMuPTM. SF: Hellinger distance. Composition: SmoothedFusion18 of Vulic and Moens (2014).  = 0.9.
(3) BMu+Cue+S. RM: BasicMuPTM. SF: Cue or Association measure (Steyvers & Griffiths, 2007; Vulic & Moens, 2013a). Composition: Smoothed-Fusion.  = 0.9. The
Cue similarity
models and computed as the association score
PKis tailored for probabilistic
0
0
P (ti |wi ) = k=1 P (ti |zk )P (zk |wi ), where zk denotes k-th latent feature, and P (zk |wi0 ) denotes the modulated probability score obtained by smoothing the probabilistic representations of wi and its context Con(wi ).
(4) TPPMI+add. RM: Traditional-PPMI. SF: cos. Composition: addition.  = 0.9.
Again, all parameters of the baseline representation models are adopted directly from
prior work where they were optimized on development sets comprising additional 100 sentences (Vulic & Moens, 2014). In addition, BMu+HD+S and BMu+Cue+S also rely on the
procedure of context sorting and pruning (Vulic & Moens, 2014), where the idea is to retain
only context words which are most semantically similar to the given pivot polysemous word,
and then use them in computations. The procedure, however, produces significant gains
only for probabilistic models (BMu+HD+S and BMu+Cue+S), and therefore, we employ
it only for these models. BMu+HD+S and BMu+Cue+S with context sorting and pruning
were the best scoring models in the introductory SWTC paper of Vulic and Moens and
currently produce state-of-the-art SWTC results on these test sets.19
Table 6 summarizes the results and comparison with Group I models on the SWTC task.
NO-CONTEXT refers to the context-insensitive majority baseline (i.e., always choosing the
most semantically similar translation candidate obtained by BWESG at the word type level,
without taking into account any context information).
17. We have also experimented with the context-sensitive CLSS models proposed by Melamud et al. (2015),
but we do not report the actual scores as this model, although displaying a similar relative ranking of
different representation models, was consistently outperformed by the models of Vulic and Moens (2014)
in our evaluation runs: 0.75-0.80 vs 0.60-0.65 for the models of Melamud et al. (2015).
18. In short, Smoothed-Fusion is a probabilistic variant of the context-sensitive modeling idea presented by
equations (7)-(9). For more details, check the work of Vulic and Moens (2014).
19. We omit results for the Association-MuPTM RM since SWTC models based on Association-MuPTM
were consistently outperformed by SWTC models based on Basic-MuPTM across different settings.

980

fiBilingual Distributed Word Representations from Document-Aligned Data

Pair:
BWESG+add
Length-Ratio
cs:16
cs:48

ES-EN

IT-EN

NL-EN

d=100

d=200

d=300

d=100

d=200

d=300

d=100

d=200

d=300

0.794*
0.752*

0.767*
0.758*

0.752*
0.764*

0.817*
0.814*

0.789
0.831*

0.794
0.814*

0.778*
0.797*

0.769*
0.789*

0.767*
0.775*

BWESG+add
No Shuffling
cs:16
cs:48

d=100

d=200

d=300

d=100

d=200

d=300

d=100

d=200

d=300

0.717
0.731

0.717
0.692

0.694
0.686

0.747
0.775

0.728
0.778

0.728
0.758

0.722
0.739

0.686
0.733

0.678
0.719

NO-CONTEXT

0.406

0.406

0.406

0.408

0.408

0.408

0.433

0.433

0.433

BMu+HD+S
BMu+Cue+S
TPPMI+add

0.664
0.703
0.619

0.664
0.703
0.619

0.664
0.703
0.619

0.731
0.761
0.706

0.731
0.761
0.706

0.731
0.761
0.706

0.669
0.712
0.614

0.669
0.712
0.614

0.669
0.712
0.614

Table 6: A comparison of SWTC models for Spanish-English, Italian-English and DutchEnglish with all bilingual word representations learned from document-aligned
Wikipedia data. The asterisk (*) denotes statistically significant improvements
of BWESG+add over the strongest baseline according to a McNemars statistical
significance test (p < 0.05). Highest scores per column are in bold.
BWESG vs Baseline Representations The results reveal that BWESG outperforms
baseline bilingual word representations from Group I also in the SWTC task. The improvements are prominent for all reported values of parameters d and cs, and are often
statistically significant even when compared to the strongest baseline (which is the finetuned BMu+Cue+S model with context sorting and pruning for all three language pairs
from Vulic & Moens, 2014). The increase in Acc1 scores over the strongest baseline is 12.9%
for ES-EN, 11.9% for IT-EN, and 12.4% for NL-EN. The obtained results surpass previous
state-of-the-art scores and are currently the best reported results on the SWTC datasets
when using non-parallel data to learn semantic representations.
BWESG Shuffling Strategy Although BWESG without shuffling (due to a reduced complexity of the SWTC task compared to BLE) already displays encouraging results, there
is again a clear advantage to the length-ratio shuffle strategy, which displays an excellent
performance for all three language pairs. In simple words, shuffling is again useful.
Dimensionality and Number of Training Pairs Unlike in the BLE task, the highest Acc1 scores on average are obtained by using lower-dimensional word embeddings (i.e.,
d = 100). The phenomenon may be attributed to the effect of semantic composition and the
reduced complexity of the SWTC task compared to the BLE task. First, although enlarging the dimensionality of embeddings leads to an increased semantic expressiveness within

Senses:

2 senses

3 senses

4 senses

Model

Acc1

Acc1

Acc1

BMu+Cue+S
BWESG+add

0.827
0.834

0.619
0.804

0.417
0.583

Table 7: A comparison of the best scoring baseline model BMu+Cue+S and the best scoring
BWESG+add model over different clusters of words (2-sense, 3-sense and 4-sense
words) for Spanish-English.
981

fiVulic & Moens

the shared bilingual embedding space, it may be harmful when working with composition
models, since the simple additive model of semantic composition may produce more erroneous dimensions when constructing higher-dimensional context embeddings out of single
word embeddings. Second, due to its design, the SWTC task requires coarser-grained representations than BLE. While in the BLE task the goal is to detect a translation of a word
from a vocabulary which typically spans (tens of) thousands of words, in the SWTC task
the goal is to detect the most likely translation of a word given its sentential context, but
from a small closed vocabulary of 2-4 possible translations from the translation inventory.
Therefore, it is highly likely that even low-dimensional embeddings are sufficient to produce plausible rankings for the SWTC task, while at the same time, they are not sufficient
and expressive enough to find correct translations in BLE. More training pairs (i.e., larger
windows) still yield better results on average in the SWTC task. In summary, the choice of
representation granularity is dependent on the actual task, which consequently leads to the
conclusion that optimal values for d and cs are largely task-specific (compare also results
in Table 4 and Table 6).
Testing Polysemy In order to test whether the gain in performance for BWESG+add
is derived mostly from the effective handling of the easiest set of words, that is, bisemous
words (polysemous words with only 2 translation candidates), we have performed an additional experiment, where we have measured Acc1 scores separately for words with 2, 3,
and 4 different senses. Results indicate that the performance gain comes mostly from gains
on trisemous and tetrasemous words, while the scores on bisemous words are comparable.
Table 7 shows Acc1 over different clusters of words for ES-EN, and similar scoring patterns
are observed for IT-EN and NL-EN.
Differences across Language Pairs Due to the reduced complexity of SWTC, we may
also observe relatively higher results for NL-EN when compared to ES-EN and IT-EN, as
opposed to their relative performance in the BLE task, where the scores for NL-EN are
much lower than scores for ES-EN and IT-EN. Since SWTC is a less difficult task which
requires coarse-grained representations, even limited amounts of training data may be sufficient to learn word embeddings which are useful for the specific task. This finding is in
line with the recent work of Gouws and Sgaard (2015).
8.3.2 Experiment II: BWESG vs. Other BWE Induction Models (Group II)
We again test other BWE induction models in the SWTC task, using the same training setup
and sets of embeddings as introduced in Section 7.3.3 for the BLE task. The representations
were now plugged in the context-sensitive CLSS modeling framework from Section 5.1, and
the optimization of parameters for SWTC has been conducted in the same manner as for
BWESG. The results with the Mikolov model and BiCVM are summarized in Table 8.
The results with BilBOWA are very similar to BiCVM, so we do not report it for brevity.
BWESG outperforms other BWE induction models in the SWTC task and further
confirms its utility in cross-lingual semantic modeling. The model of Mikolov et al. (2013b)
constitutes a stronger baseline: Good results in the SWTC task with this model are an
interesting finding per se. While the model is not competitive with BWESG and other
baseline representations models from document-aligned data in a more difficult BLE task
when using noisy one-to-one translation pairs, its performance on the less complex SWTC
982

fiBilingual Distributed Word Representations from Document-Aligned Data

Pair:
BWESG+add
Length-Ratio
cs:16
cs:48

ES-EN

IT-EN

NL-EN

d=100

d=200

d=300

d=100

d=200

d=300

d=100

d=200

d=300

0.794
0.752

0.767
0.758

0.752
0.764

0.817
0.814

0.789
0.831

0.794
0.814

0.778
0.797

0.769
0.789

0.767
0.775

cs:4
cs:8
cs:16
cs:48
cs:60

0.742
0.767
0.769
0.678
0.636

0.739
0.750
0.744
0.642
0.658

0.725
0.747
0.747
0.669
0.656

0.733
0.767
0.758
0.714
0.725

0.706
0.747
0.755
0.714
0.725

0.692
0.744
0.758
0.747
0.742

0.692
0.694
0.725
0.725
0.722

0.700
0.697
0.700
0.711
0.728

0.700
0.672
0.689
0.708
0.722

BiCVM
iterations:200

0.547

0.567

0.539

0.636

0.664

0.642

0.586

0.567

0.581

Mikolov

Table 8: SWTC results: Comparison of BWESG with (1) the BWE induction model of
Mikolov et al. (2013b) relying on SGNS, (2) BiCVM: the BWE induction model
of Hermann and Blunsom (2014b) initially developed for parallel sentence-aligned
data. All models were trained on the same document-aligned training Wikipedia
data with exactly the same vocabularies.
task with a reduced search space is solid even when the model relies on the imperfect set
of translation pairs to learn the mapping between two monolingual embedding spaces.
8.3.3 Further Discussion
By analyzing the influence of pre-training shuffling on the results in two different evaluation
tasks, we may safely establish its utility when inducing bilingual word embeddings using the
BWESG model. While we have already presented two shuffling strategies in this work, one
line of future work will investigate different possibilities of blending in words from two
different vocabularies into pseudo-bilingual documents in a more structured and systematic
manner. For instance, one approach to generating pseudo-training sentences for learning
from textual and perceptual modalities has been recently introduced (Hill & Korhonen,
2014). However, it is not straightforward how to extend this approach to the generation of
pseudo-bilingual training documents.
Another idea in the same vein is to build artificial training data of higher-quality starting
from noisy comparable data by: (1) computing semantically similar words monolingually
and across-languages from the noisy data, (2) retaining only highly reliable pairs of similar
words using an automatic selection procedure (Vulic & Moens, 2012), (3) building pseudobilingual documents using only reliable context word pairs. In other words, the questions is:
Is it possible to choose positive training pairs more systematically to reduce the noise stemming from non-parallel data? The construction of such artificial training data and training
on such data would then proceed in a bootstrapping fashion, and the model should be able
to steadily reduce noise inherently present in comparable data. The idea of improving
corpus comparability was only touched upon in previous work (Li & Gaussier, 2010; Li,
Gaussier, & Aizawa, 2011).
While the entire framework proposed in this article is in theory completely language
pair agnostic as it does not make any language pair dependent modeling assumptions, we
acknowledge the fact that all three language pairs comprise languages coming from the same
983

fiVulic & Moens

phylum, that is, the Indo-European language family. Future extensions also include porting
the framework to other more distant language pairs that do not share the same roots nor the
same alphabet (e.g., English-Chinese/Hindi/Arabic), and for which benchmarking test sets
are still scarce for a variety of semantic tasks (e.g., SWTC) (Camacho-Collados, Pilehvar,
& Navigli, 2015). We believe that larger window sizes may solve difficulties with different
word orderings (e.g., for Chinese-English).

9. Conclusions and Future Work
We have proposed and described Bilingual Word Embeddings Skip-Gram (BWESG), a simple yet effective bilingual word representation learning model which is able to induce bilingual word embeddings solely on the basis of document-aligned comparable data. BWESG
is based on the omnipresent skip-gram with negative sampling (SGNS). We have presented
two ways to build pseudo-bilingual documents on which a monolingual SGNS (or any
monolingual WE induction model) may be trained to produce shared bilingual embedding
spaces. The BWESG model does not make any language-pair dependent assumptions nor
requires language-pair specific external resources such as bilingual lexicons, predefined category/ontology knowledge or parallel data. We have showed that the model may be trained
on non-parallel and parallel data without any changes in modeling principles, which, complemented with its simplicity and lightweight design makes it potentially very useful as a
tool for researchers in machine translation and information retrieval.
We have employed induced BWEs in two semantic tasks: (1) bilingual lexicon extraction
(BLE), and (2) suggesting word translations in context (SWTC). Our new BWESG-based
BLE and SWTC models outperform previous state-of-the-art models for BLE and SWTC
from document-aligned comparable data and related BWE induction models (Mikolov et al.,
2013b; Chandar et al., 2014; Gouws et al., 2015). The findings in this article follow the
recently published surveys from Baroni et al. (2014), and Levy et al. (2015) regarding a
solid and robust performance of neural word representations/word embeddings in semantic
tasks: our new BWESG-based models for BLE and SWTC significantly outscore previous
state-of-the-art distributional approaches on both tasks across different parameter settings.
Even more encouraging is the fact that these new state-of-the-art results are attained using
default parameter settings for the BWESG model as suggested in the word2vec package
without any development set. Further (finer) tuning of model parameters in future work
may lead to higher-quality bilingual embedding spaces.
Several straightforward lines of future research have already been tackled in Section 7
and Section 8. For instance, the current length-ratio shuffling strategy may be replaced by
a more advanced shuffling method in future work. Moreover, BWEs induced by BWESG
may be used in other semantic tasks besides the ones discussed in this work, and it would be
interesting to experiment with other types of context aggregation and selection beyond the
bag-of-words assumption, such as dependency-based contexts (Levy & Goldberg, 2014a), or
other objective functions during training in the same vein as proposed by Levy and Goldberg
(2014b). Similar to the evolution in multilingual probabilistic topic modeling, another path
of future work may lead to investigating bilingual models for learning BWEs which will be
able to jointly learn from separate documents in aligned document pairs, without the need
to construct pseudo-bilingual documents.
984

fiBilingual Distributed Word Representations from Document-Aligned Data

A natural step in the text representation learning research is to extend the focus from
single word representations to composite phrase, sentence and document representations
(Hermann & Blunsom, 2013; Kalchbrenner, Grefenstette, & Blunsom, 2014; Le & Mikolov,
2014; Soyer et al., 2015; Kiros, Zhu, Salakhutdinov, Zemel, Torralba, Urtasun, & Fidler,
2015; Hill, Cho, Korhonen, & Bengio, 2016). In this article, we have relied on a simple composition model based on vector addition, and have shown that this model performs excellent
in the SWTC task. However, in the long run this model is not by any means sufficient to
effectively capture all complex compositional phenomena in the data. Several models which
aim to learn sentence and document embeddings have been proposed recently, but they
critically rely on sentence-aligned parallel data. It is yet to be seen how to build structured
multilingual phrase, sentence and document embeddings solely on the basis of comparable data. Such low-cost multilingual embeddings beyond the word level extracted from
comparable data may find its application in a variety of tasks such as statistical machine
translation (Mikolov et al., 2013b; Zou et al., 2013; Zhang et al., 2014; Wu et al., 2014),
semantic tasks such as multilingual semantic textual similarity (Agirre, Banea, Cardie, Cer,
Diab, Gonzalez-Agirre, Guo, Mihalcea, Rigau, & Wiebe, 2014), cross-lingual information
retrieval (Vulic et al., 2013; Vulic & Moens, 2015) or cross-lingual document classification
(Klementiev et al., 2012; Hermann & Blunsom, 2014b; Chandar et al., 2014).
In another future research path, we may use the knowledge of BWEs obtained by
BWESG from document-aligned data to learn bilingual correspondences (e.g., word translation pairs or lists of semantically similar words across languages) which may in turn be
used for learning from large unaligned multilingual datasets (Mikolov et al., 2013b; Al-Rfou,
Perozzi, & Skiena, 2013). In the long run, this idea may lead to large-scale learning models from huge amounts of multilingual data without any requirement for parallel data or
manually built bilingual lexicons.

Acknowledgments
This work was done while Ivan Vulic was a postdoctoral researcher at Department of Computer Science, KU Leuven supported by the PDM Kort fellowship (PDMK/14/117). The
work was also supported by the SCATE project (IWT-SBO 130041) and the ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (648909). We would also
like to thank the anonymous reviews for their helpful suggestions that helped us to greatly
improve the presentation of our work.

References
Agirre, E., Banea, C., Cardie, C., Cer, D., Diab, M., Gonzalez-Agirre, A., Guo, W., Mihalcea, R., Rigau, G., & Wiebe, J. (2014). SemEval-2014 task 10: Multilingual semantic
textual similarity. In Proceedings of the 8th International Workshop on Semantic
Evaluation (SEMEVAL), pp. 8191. Association for Computational Linguistics.
Al-Rfou, R., Perozzi, B., & Skiena, S. (2013). Polyglot: Distributed word representations
for multilingual NLP. In Proceedings of the Seventeenth Conference on Computational
Natural Language Learning (CoNLL), pp. 183192.
985

fiVulic & Moens

Baroni, M., Dinu, G., & Kruszewski, G. (2014). Dont count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings
of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL),
pp. 238247.
Baroni, M., & Zamparelli, R. (2010). Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of the 2010
Conference on Empirical Methods in Natural Language Processing (EMNLP), pp.
11831193.
Bengio, Y., Ducharme, R., Vincent, P., & Janvin, C. (2003). A neural probabilistic language
model. Journal of Machine Learning Research, 3, 11371155.
Blacoe, W., & Lapata, M. (2012). A comparison of vector-based representations for semantic composition. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning
(EMNLP-CoNLL), pp. 546556.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet Allocation. Journal of
Machine Learning Research, 3, 9931022.
Boyd-Graber, J., & Blei, D. M. (2009). Multilingual topic models for unaligned text. In
Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (UAI),
pp. 7582.
Bullinaria, J. A., & Levy, J. P. (2007). Extracting semantic representations from word
co-occurrence statistics: A computational study. Behavior Research Methods, 39 (3),
510526.
Camacho-Collados, J., Pilehvar, M. T., & Navigli, R. (2015). A framework for the construction of monolingual and cross-lingual word similarity datasets. In Proceedings of the
53rd Annual Meeting of the Association for Computational Linguistics and the 7th
International Joint Conference on Natural Language Processing (ACL-IJCNLP), pp.
17.
Carbonell, J. G., Yang, J. G., Frederking, R. E., Brown, R. D., Geng, Y., Lee, D., Frederking,
Y., E, R., Geng, R. D., & Yang, Y. (1997). Translingual information retrieval: A
comparative evaluation. In Proceedings of the 15th International Joint Conference on
Artificial Intelligence (IJCAI), pp. 708714.
Cha, S.-H. (2007). Comprehensive survey on distance/similarity measures between probability density functions. International Journal of Mathematical Models and Methods
in Applied Sciences, 1 (4), 300307.
Chandar, S., Lauly, S., Larochelle, H., Khapra, M. M., Ravindran, B., Raykar, V. C., &
Saha, A. (2014). An autoencoder approach to learning bilingual word representations.
In Proceedings of the 27th Annual Conference on Advances in Neural Information
Processing Systems (NIPS), pp. 18531861.
Chen, D., & Manning, C. (2014). A fast and accurate dependency parser using neural
networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pp. 740750.
986

fiBilingual Distributed Word Representations from Document-Aligned Data

Clarke, D. (2012). A context-theoretic framework for compositionality in distributional
semantics. Computational Linguistics, 38 (1), 4171.
Collobert, R., & Weston, J. (2008). A unified architecture for natural language processing:
Deep neural networks with multitask learning. In Proceedings of the 25th International
Conference on Machine Learning (ICML), pp. 160167.
Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. P. (2011).
Natural language processing (almost) from scratch. Journal of Machine Learning
Research, 12, 24932537.
Das, D., & Petrov, S. (2011). Unsupervised part-of-speech tagging with bilingual graphbased projections. In Proceedings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Technologies (ACL-HLT), pp. 600609.
Daume III, H., & Jagarlamudi, J. (2011). Domain adaptation for machine translation by
mining unseen words. In Proceedings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Technologies (ACL-HLT), pp. 407412.
De Smet, W., & Moens, M.-F. (2009). Cross-language linking of news stories on the Web
using interlingual topic modeling. In Proceedings of the CIKM 2009 Workshop on
Social Web Search and Mining (SWSM@CIKM), pp. 5764.
Deschacht, K., De Belder, J., & Moens, M.-F. (2012). The latent words language model.
Computer Speech & Language, 26 (5), 384409.
Deschacht, K., & Moens, M.-F. (2009). Semi-supervised semantic role labeling using the
latent words language model. In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing (EMNLP), pp. 2129.
Dinu, G., & Lapata, M. (2010). Measuring distributional similarity in context. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing
(EMNLP), pp. 11621172.
Dinu, G., Lazaridou, A., & Baroni, M. (2015). Improving zero-shot learning by mitigating
the hubness problem. In ICLR Workshop Papers.
Duchi, J. C., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods for online
learning and stochastic optimization. Journal of Machine Learning Research, 12,
21212159.
Dumais, S. T., Landauer, T. K., & Littman, M. (1996). Automatic cross-linguistic information retrieval using Latent Semantic Indexing. In Proceedings of the SIGIR Workshop
on Cross-Linguistic Information Retrieval, pp. 1623.
Elman, J. L. (1990). Finding structure in time. Cognitive Science, 14, 179211.
Faruqui, M., & Dyer, C. (2014). Improving vector space word representations using multilingual correlation. In Proceedings of the 14th Conference of the European Chapter of
the Association for Computational Linguistics (EACL), pp. 462471.
Fukumasu, K., Eguchi, K., & Xing, E. P. (2012). Symmetric correspondence topic models for
multilingual text analysis. In Proceedings of the 25th Annual Conference on Advances
in Neural Information Processing Systems (NIPS), pp. 12951303.
987

fiVulic & Moens

Ganchev, K., & Das, D. (2013). Cross-lingual discriminative learning of sequence models
with posterior regularization. In Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing (EMNLP), pp. 19962006.
Gaussier, E., Renders, J.-M., Matveeva, I., Goutte, C., & Dejean, H. (2004). A geometric
view on bilingual lexicon extraction from comparable corpora. In Proceedings of the
42nd Annual Meeting of the Association for Computational Linguistics (ACL), pp.
526533.
Geman, S., & Geman, D. (1984). Stochastic relaxation, Gibbs distributions, and the
Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 6 (6), 721741.
Goldberg, Y., & Levy, O. (2014). Word2vec explained: Deriving Mikolov et al.s negativesampling word-embedding method. CoRR, abs/1402.3722.
Gouws, S., Bengio, Y., & Corrado, G. (2015). BilBOWA: Fast bilingual distributed representations without word alignments. In Proceedings of the 32nd International Conference
on Machine Learning (ICML), pp. 748756.
Gouws, S., & Sgaard, A. (2015). Simple task-specific bilingual word embeddings. In
Proceedings of the 2015 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies (NAACL-HLT), pp.
13861390.
Griffiths, T. L., Steyvers, M., & Tenenbaum, J. B. (2007). Topics in semantic representation.
Psychological Review, 114 (2), 211244.
Haghighi, A., Liang, P., Berg-Kirkpatrick, T., & Klein, D. (2008). Learning bilingual lexicons
from monolingual corpora. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), pp.
771779.
Harris, Z. S. (1954). Distributional structure. Word, 10 (23), 146162.
Hermann, K. M., & Blunsom, P. (2013). The role of syntax in vector space models of
compositional semantics. In Proceedings of the 51st Annual Meeting of the Association
for Computational Linguistics (ACL), pp. 894904.
Hermann, K. M., & Blunsom, P. (2014a). Multilingual distributed representations without
word alignment. In Proceedings of the 2014 International Conference on Learning
Representations (ICLR).
Hermann, K. M., & Blunsom, P. (2014b). Multilingual models for compositional distributed
semantics. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL), pp. 5868.
Hill, F., Cho, K., Korhonen, A., & Bengio, Y. (2016). Learning to understand phrases by
embedding the dictionary. Transactions of the ACL, 4, 1730.
Hill, F., & Korhonen, A. (2014). Learning abstract concept embeddings from multi-modal
data: Since you probably cant see what I mean. In Proceedings of the 2014 Conference
on Empirical Methods in Natural Language Processing (EMNLP), pp. 255265.
988

fiBilingual Distributed Word Representations from Document-Aligned Data

Kalchbrenner, N., Grefenstette, E., & Blunsom, P. (2014). A convolutional neural network
for modelling sentences. In Proceedings of the 52nd Annual Meeting of the Association
for Computational Linguistics (ACL), pp. 655665.
Kazama, J., Saeger, S. D., Kuroda, K., Murata, M., & Torisawa, K. (2010). A Bayesian
method for robust estimation of distributional similarities. In Proceedings of the 48th
Annual Meeting of the Association for Computational Linguistics (ACL), pp. 247256.
Kiela, D., & Bottou, L. (2014). Learning image embeddings using convolutional neural
networks for improved multi-modal semantics. In Proceedings of the 2014 Conference
on Empirical Methods in Natural Language Processing (EMNLP), pp. 3645.
Kiela, D., & Clark, S. (2014). A systematic study of semantic vector space model parameters.
In Proceedings of the 2nd Workshop on Continuous Vector Space Models and their
Compositionality (CVSC), pp. 2130.
Kiros, R., Zhu, Y., Salakhutdinov, R., Zemel, R. S., Torralba, A., Urtasun, R., & Fidler,
S. (2015). Skip-thought vectors. In Proceedings of the 28th Annual Conference on
Advances in Neural Information Processing Systems (NIPS).
Klementiev, A., Titov, I., & Bhattarai, B. (2012). Inducing crosslingual distributed representations of words. In Proceedings of the 24th International Conference on Computational Linguistics (COLING), pp. 14591474.
Koehn, P. (2005). Europarl: A parallel corpus for statistical machine translation. In Proceedings of the 10th Machine Translation Summit (MT SUMMIT), pp. 7986.
Kocisky, T., Hermann, K. M., & Blunsom, P. (2014). Learning bilingual word representations by marginalizing alignments. In Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics (ACL), pp. 224229.
Landauer, T. K., & Dumais, S. T. (1997). Solutions to Platos problem: The Latent Semantic Analysis theory of acquisition, induction, and representation of knowledge.
Psychological Review, 104 (2), 211240.
Laroche, A., & Langlais, P. (2010). Revisiting context-based projection methods for termtranslation spotting in comparable corpora. In Proceedings of the 23rd International
Conference on Computational Linguistics (COLING), pp. 617625.
Lazaridou, A., Dinu, G., & Baroni, M. (2015). Hubness and pollution: Delving into crossspace mapping for zero-shot learning. In ACL, pp. 270280.
Le, Q. V., & Mikolov, T. (2014). Distributed representations of sentences and documents.
In Proceedings of the 31th International Conference on Machine Learning (ICML),
pp. 11881196.
Lebret, R., & Collobert, R. (2014). Word embeddings through Hellinger PCA. In Proceedings
of the 14th Conference of the European Chapter of the Association for Computational
Linguistics (EACL), pp. 482490.
Lee, L. (1999). Measures of distributional similarity. In Proceedings of the 37th Annual
Meeting of the Association for Computational Linguistics (ACL), pp. 2532.
989

fiVulic & Moens

Levow, G.-A., Oard, D. W., & Resnik, P. (2005). Dictionary-based techniques for crosslanguage information retrieval. Information Processing and Management, 41 (3), 523
547.
Levy, O., & Goldberg, Y. (2014a). Dependency-based word embeddings. In Proceedings
of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL),
pp. 302308.
Levy, O., & Goldberg, Y. (2014b). Neural word embedding as implicit matrix factorization.
In Proceedings of the 27th Annual Conference on Advances in Neural Information
Processing Systems (NIPS), pp. 21772185.
Levy, O., Goldberg, Y., & Dagan, I. (2015). Improving distributional similarity with lessons
learned from word embeddings. Transactions of the ACL, 3, 211225.
Li, B., & Gaussier, E. (2010). Improving corpus comparability for bilingual lexicon extraction from comparable corpora. In Proceedings of the 23rd International Conference
on Computational Linguistics (COLING), pp. 644652.
Li, B., Gaussier, E., & Aizawa, A. (2011). Clustering comparable corpora for bilingual
lexicon extraction. In Proceedings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Technologies (ACL-HLT), pp. 473478.
Liu, Q., Jiang, H., Wei, S., Ling, Z.-H., & Hu, Y. (2015). Learning semantic word embeddings based on ordinal knowledge constraints. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing (ACL-IJCNLP), pp. 15011511.
Liu, X., Duh, K., & Matsumoto, Y. (2013). Topic models + word alignment = a flexible
framework for extracting bilingual dictionary from comparable corpus. In Proceedings
of the 17th Conference on Computational Natural Language Learning (CoNLL), pp.
212221.
Lu, A., Wang, W., Bansal, M., Gimpel, K., & Livescu, K. (2015). Deep multilingual correlation for improved word embeddings. In Proceedings of the 2015 Conference of the
North American Chapter of the Association for Computational Linguistics: Human
Language Technologies (NAACL-HLT), pp. 250256.
Luong, T., Pham, H., & Manning, C. D. (2015). Bilingual word representations with monolingual quality in mind. In Proceedings of the 1st Workshop on Vector Space Modeling
for Natural Language Processing, pp. 151159.
McNemar, Q. (1947). Note on the sampling error of the difference between correlated
proportions or percentages. Psychometrika, 12 (2), 153157.
Melamud, O., Levy, O., & Dagan, I. (2015). A simple word embedding model for lexical
substitution. In Proceedings of the 1st Workshop on Vector Space Modeling for Natural
Language Processing, pp. 17.
Mikolov, T., Chen, K., Corrado, G. S., & Dean, J. (2013a). Efficient estimation of word
representations in vector space. In Proceedings of the 2013 International Conference
on Learning Representations (ICLR): Workshop Papers.
990

fiBilingual Distributed Word Representations from Document-Aligned Data

Mikolov, T., Le, Q. V., & Sutskever, I. (2013b). Exploiting similarities among languages
for machine translation. CoRR, abs/1309.4168.
Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013c). Distributed
representations of words and phrases and their compositionality. In Proceedings of
the 27th Annual Conference on Advances in Neural Information Processing Systems
(NIPS), pp. 31113119.
Mikolov, T., Yih, W., & Zweig, G. (2013d). Linguistic regularities in continuous space word
representations. In Proceedings of the 14th Meeting of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies
(NAACL-HLT), pp. 746751.
Milajevs, D., Kartsaklis, D., Sadrzadeh, M., & Purver, M. (2014). Evaluating neural word
representations in tensor-based compositional settings. In Proceedings of the 2014
Conference on Empirical Methods in Natural Language Processing (EMNLP), pp.
708719.
Mimno, D., Wallach, H., Naradowsky, J., Smith, D. A., & McCallum, A. (2009). Polylingual
topic models. In Proceedings of the 2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pp. 880889.
Mitchell, J., & Lapata, M. (2008). Vector-based models of semantic composition. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics
(ACL), pp. 236244.
Mnih, A., & Kavukcuoglu, K. (2013). Learning word embeddings efficiently with noisecontrastive estimation. In Proceedings of the 27th Annual Conference on Advances in
Neural Information Processing Systems (NIPS), pp. 22652273.
Ni, X., Sun, J.-T., Hu, J., & Chen, Z. (2009). Mining multilingual topics from Wikipedia.
In Proceedings of the 18th International World Wide Web Conference (WWW), pp.
11551156.
Ni, X., Sun, J.-T., Hu, J., & Chen, Z. (2011). Cross lingual text classification by mining
multilingual topics from Wikipedia. In Proceedings of the 4th International Conference
on Web Search and Web Data Mining (WSDM), pp. 375384.
Pado, S., & Lapata, M. (2009). Cross-lingual annotation projection for semantic roles.
Journal of Artificial Intelligence Research, 36, 307340.
Pantel, P., & Lin, D. (2002). Discovering word senses from text. In Proceedings of the 8th
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
(KDD), pp. 613619.
Peirsman, Y., & Pado, S. (2010). Cross-lingual induction of selectional preferences with
bilingual vector spaces. In Proceedings of the 11th Meeting of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pp. 921929.
Peirsman, Y., & Pado, S. (2011). Semantic relations in bilingual lexicons. ACM Transactions
on Speech and Language Processing, 8 (2), article 3.
991

fiVulic & Moens

Pennington, J., Socher, R., & Manning, C. (2014). Glove: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pp. 15321543.
Pollard, D. (2001). A Users Guide to Measure Theoretic Probability. Cambridge University
Press.
Rapp, R. (1999). Automatic identification of word translations from unrelated English and
German corpora. In Proceedings of the 37th Annual Meeting of the Association for
Computational Linguistics (ACL), pp. 519526.
Reisinger, J., & Mooney, R. J. (2010). A mixture model with sharing for lexical semantics.
In Proceedings of the 2010 Conference on Empirical Methods in Natural Language
Processing (EMNLP), pp. 11731182.
Rudolph, S., & Giesbrecht, E. (2010). Compositional matrix-space models of language. In
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 907916.
Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by
back-propagating errors. Nature, 323, 533536.
Schmid, H. (1994). Probabilistic part-of-speech tagging using decision trees. In Proceedings
of the International Conference on New Methods in Language Processing.
Shi, T., Liu, Z., Liu, Y., & Sun, M. (2015). Learning cross-lingual word embeddings via
matrix co-factorization. In Proceedings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th International Joint Conference on Natural
Language Processing (ACL-IJCNLP), pp. 567572.
Socher, R., Huval, B., Manning, C. D., & Ng, A. Y. (2012). Semantic compositionality
through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pp. 12011211.
Sgaard, A., Agic, v., Martnez Alonso, H., Plank, B., Bohnet, B., & Johannsen, A. (2015).
Inverted indexing for cross-lingual nlp. In Proceedings of the 53rd Annual Meeting of
the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL-IJCNLP), pp. 17131722.
Soyer, H., Stenetorp, P., & Aizawa, A. (2015). Leveraging monolingual data for crosslingual compositional word representations. In Proceedings of the 2015 International
Conference on Learning Representations (ICLR).
Steyvers, M., & Griffiths, T. (2007). Probabilistic topic models. Handbook of Latent Semantic Analysis, 427 (7), 424440.
Stratos, K., Collins, M., & Hsu, D. (2015). Model-based word embeddings from decompositions of count matrices. In Proceedings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th International Joint Conference on Natural
Language Processing (ACL-IJCNLP), pp. 12821291.
Tackstrom, O., Das, D., Petrov, S., McDonald, R., & Nivre, J. (2013). Token and type
constraints for cross-lingual part-of-speech tagging. Transactions of the ACL, 1, 112.
992

fiBilingual Distributed Word Representations from Document-Aligned Data

Tamura, A., Watanabe, T., & Sumita, E. (2012). Bilingual lexicon extraction from comparable corpora using label propagation. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pp. 2436.
Tiedemann, J. (2012). Parallel data, tools and interfaces in OPUS. In Proceedings of the
8th International Conference on Language Resources and Evaluation (LREC), pp.
22142218.
Tiedemann, J., Agic, Z., & Nivre, J. (2014). Treebank translation for cross-lingual parser
induction. In Proceedings of the 18th Conference on Computational Natural Language
Learning (CoNLL), pp. 130140.
Trask, A., Gilmore, D., & Russell, M. (2015). Modeling order in neural word embeddings
at scale. In Proceedings of the 32nd International Conference on Machine Learning
(ICML), pp. 22662275.
Turian, J. P., Ratinov, L., & Bengio, Y. (2010). Word representations: A simple and general
method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL), pp. 384394.
Turney, P. D., & Pantel, P. (2010). From frequency to meaning: Vector space models of
semantics. Journal of Artifical Intelligence Research, 37 (1), 141188.
Vulic, I., De Smet, W., & Moens, M.-F. (2011). Identifying word translations from comparable corpora using latent topic models. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Human Language Technologies (ACLHLT), pp. 479484.
Vulic, I., De Smet, W., & Moens, M.-F. (2013). Cross-language information retrieval models
based on latent topic models trained with document-aligned comparable corpora.
Information Retrieval, 16 (3), 331368.
Vulic, I., De Smet, W., Tang, J., & Moens, M. (2015). Probabilistic topic modeling in
multilingual settings: An overview of its methodology and applications. Information
Processing and Management, 51 (1), 111147.
Vulic, I., & Moens, M.-F. (2012). Detecting highly confident word translations from comparable corpora without any prior knowledge. In Proceedings of the 13th Conference
of the European Chapter of the Association for Computational Linguistics (EACL),
pp. 449459.
Vulic, I., & Moens, M.-F. (2013a). Cross-lingual semantic similarity of words as the similarity of their semantic word responses. In Proceedings of the 14th Meeting of the
North American Chapter of the Association for Computational Linguistics: Human
Language Technologies (NAACL-HLT), pp. 106116.
Vulic, I., & Moens, M.-F. (2013b). A study on bootstrapping bilingual vector spaces from
non-parallel data (and nothing else). In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 16131624.
Vulic, I., & Moens, M.-F. (2014). Probabilistic models of cross-lingual semantic similarity in context based on latent cross-lingual concepts induced from comparable data.
993

fiVulic & Moens

In Proceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing (EMNLP), pp. 349362.
Vulic, I., & Moens, M.-F. (2015). Monolingual and cross-lingual information retrieval models
based on (bilingual) word embeddings. In Proceedings of the 38th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval
(SIGIR), pp. 363372.
Wu, H., Dong, D., Hu, X., Yu, D., He, W., Wu, H., Wang, H., & Liu, T. (2014). Improve
statistical machine translation with context-sensitive bilingual semantic embedding
model. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 142146.
Wu, H., Wang, H., & Zong, C. (2008). Domain adaptation for statistical machine translation with domain dictionary and monolingual corpora. In Proceedings of the 22nd
International Conference on Computational Linguistics (COLING), pp. 9931000.
Xiao, M., & Guo, Y. (2014). Distributed word representation learning for cross-lingual
dependency parsing. In Proceedings of the 18th Conference on Computational Natural
Language Learning (CoNLL), pp. 119129.
Yarowsky, D., & Ngai, G. (2001). Inducing multilingual POS taggers and NP bracketers
via robust projection across aligned corpora. In Proceedings of the 2nd Meeting of the
North American Chapter of the Association for Computational Linguistics (NAACL),
pp. 200207.
Zhang, D., Mei, Q., & Zhai, C. (2010). Cross-lingual latent topic extraction. In Proceedings
of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),
pp. 11281137.
Zhang, J., Liu, S., Li, M., Zhou, M., & Zong, C. (2014). Bilingually-constrained phrase
embeddings for machine translation. In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics (ACL), pp. 111121.
Zou, W. Y., Socher, R., Cer, D., & Manning, C. D. (2013). Bilingual word embeddings for
phrase-based machine translation. In Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing (EMNLP), pp. 13931398.

994

fiJournal of Artificial Intelligence Research 55 (2016) 361-387

Submitted 04/15; published 02/16

Bayesian Optimization in a Billion Dimensions
via Random Embeddings
Ziyu Wang

ziyu.wang@cs.ox.ac.uk

Department of Computer Science, University of Oxford

Frank Hutter

fh@cs.uni-freiburg.de

Department of Computer Science, University of Freiburg

Masrour Zoghi

m.zoghi@uva.nl

Department of Computer Science, University of Amsterdam

David Matheson

davidm@cs.ubc.ca

Department of Computer Science, University of British Columbia

Nando de Freitas

nando@cs.ox.ac.uk

Department of Computer Science, University of Oxford
Canadian Institute for Advanced Research

Abstract
Bayesian optimization techniques have been successfully applied to robotics, planning,
sensor placement, recommendation, advertising, intelligent user interfaces and automatic
algorithm configuration. Despite these successes, the approach is restricted to problems of
moderate dimension, and several workshops on Bayesian optimization have identified its
scaling to high-dimensions as one of the holy grails of the field. In this paper, we introduce
a novel random embedding idea to attack this problem. The resulting Random EMbedding
Bayesian Optimization (REMBO) algorithm is very simple, has important invariance properties, and applies to domains with both categorical and continuous variables. We present
a thorough theoretical analysis of REMBO. Empirical results confirm that REMBO can
effectively solve problems with billions of dimensions, provided the intrinsic dimensionality
is low. They also show that REMBO achieves state-of-the-art performance in optimizing
the 47 discrete parameters of a popular mixed integer linear programming solver.

1. Introduction
Let f : X  R be a function on a compact subset X  RD . We address the following global
optimization problem
x? = arg max f (x).
xX

We are particularly interested in objective functions f that may satisfy one or more of the
following criteria: they do not have a closed-form expression, are expensive to evaluate, do
not have easily available derivatives, or are non-convex. We treat f as a blackbox function
that only allows us to query its function value at arbitrary x  X . To address objectives of
this challenging nature, we adopt the Bayesian optimization framework.
In a nutshell, in order to optimize a blackbox function f , Bayesian optimization uses a
prior distribution that captures our beliefs about the behavior of f , and updates this prior
with sequentially acquired data. Specifically, it iterates the following phases: (1) use the
c
2016
AI Access Foundation. All rights reserved.

fiWang, Hutter, Zoghi, Matheson, & de Freitas

t=2
objective fn (f( ))

observation (x)

acquisition max
acquisition function (u( ))

t=3

new observation (xt )

t=4

posterior mean (( ))

posterior uncertainty
(( )  ( ))

Figure 1: Three consecutive iterations of Bayesian optimization for a toy one-dimensional
problem. The unknown objective function is approximated with at Gaussian process (GP) at each iteration. The figure shows the mean and confidence intervals
for this process. It also shows the acquisition function in the lower green shaded
plots. The acquisition is high where the GP predicts a high objective (exploitation) and where the prediction uncertainty is high (exploration). Note that the
area on the far left remains under-sampled, as (despite having high uncertainty)
it is correctly predicted to be unlikely to improve over the highest observation.

prior to decide at which input x  X to query f next; (2) evaluate f (x); and (3) update
the prior based on the new data hx, f (x)i. Step 1 uses a so-called acquisition function that
quantifies the expected value of learning the value of f (x) for each x  X . This procedure
is illustrated in Figure 1.
362

fiBayesian Optimization in a Billion Dimensions

The role of the acquisition function is to trade off exploration and exploitation; popular
choices include Thompson sampling (Thompson, 1933; Hoffman, Shahriari, & de Freitas,
2014), probability of improvement (Jones, 2001), expected improvement (Mockus, 1994),
upper-confidence-bounds (Srinivas, Krause, Kakade, & Seeger, 2010), and online portfolios
of these (Hoffman, Brochu, & de Freitas, 2011). These are typically optimized by choosing
points where the predictive mean is high (exploitation) and where the variance is large
(exploration). Since they typically have an analytical expression that is easy to evaluate,
they are much easier to optimize than the original objective function, using off-the-shelf
numerical optimization algorithms.1
The term Bayesian optimization was coined several decades ago by Jonas Mockus (1982).
A popular version of the method is known as efficient global optimization in the experimental
design literature since the 1990s (Jones, Schonlau, & Welch, 1998). Often, the approximation of the objective function is obtained using Gaussian process (GP) priors. For this
reason, the technique is also referred to as GP bandits (Srinivas et al., 2010). However,
many other approximations of the objective have been proposed, including Parzen estimators (Bergstra, Bardenet, Bengio, & Kegl, 2011), Bayesian parametric models (Wang
& de Freitas, 2011), treed GPs (Gramacy, Lee, & Macready, 2004) and random forests
(Brochu, Cora, & de Freitas, 2009; Hutter, 2009; Hutter, Hoos, & Leyton-Brown, 2011).
These may be more suitable than GPs when the number of iterations grows without bound,
or when the objective function is believed to have discontinuities. We also note that often
assumptions on the smoothness of the objective function are encoded without use of the
Bayesian paradigm, while leading to similar algorithms and theoretical guarantees (see, for
example, Bubeck, Munos, Stoltz, & Szepesvari, 2011, and the references therein). There is
a rich literature on Bayesian optimization, and for further details we refer readers to more
tutorial treatments (Brochu et al., 2009; Jones et al., 1998; Jones, 2001; Lizotte, Greiner,
& Schuurmans, 2011; Mockus, 1994; Osborne, Garnett, & Roberts, 2009) and recent theoretical results (Srinivas et al., 2010; Bull, 2011; de Freitas, Smola, & Zoghi, 2012).
Bayesian optimization has been demonstrated to outperform other state-of-the-art blackbox optimization techniques when function evaluations are expensive and the number of
allowed function evaluations is therefore low (Hutter, Hoos, & Leyton-Brown, 2013). In
recent years, it has found increasing use in the machine learning community (Rasmussen,
2003; Brochu, de Freitas, & Ghosh, 2007; Martinez-Cantin, de Freitas, Doucet, & Castellanos, 2007; Lizotte, Wang, Bowling, & Schuurmans, 2007; Frazier, Powell, & Dayanik,
2009; Azimi, Fern, & Fern, 2010; Hamze, Wang, & de Freitas, 2013; Azimi, Fern, & Fern,
2011; Hutter et al., 2011; Bergstra et al., 2011; Gramacy & Polson, 2011; Denil, Bazzani,
Larochelle, & de Freitas, 2012; Mahendran, Wang, Hamze, & de Freitas, 2012; Azimi, Jalali,
& Fern, 2012; Hennig & Schuler, 2012; Marchant & Ramos, 2012; Snoek, Larochelle, &
Adams, 2012; Swersky, Snoek, & Adams, 2013; Thornton, Hutter, Hoos, & Leyton-Brown,
1. This optimization step can in fact be circumvented when using treed multi-scale optimistic optimization
as recently demonstrated by Wang and de Freitas (2014). There also exist several more involved Bayesian
non-linear experimental design approaches for constructing the acquisition function, where the utility to
be optimized involves an entropy of an aspect of the posterior. This includes the work of Hennig and
Schuler (2012) for finding maxima of functions, the works of Kueck, de Freitas, and Doucet (2006) and
Kueck, Hoffman, Doucet, and de Freitas (2009) for learning functions, and the work of Hoffman, Kueck,
de Freitas, and Doucet (2009) for estimating Markov decision processes. These works rely on expensive
approximate inference methods for computing intractable integrals.

363

fiWang, Hutter, Zoghi, Matheson, & de Freitas

2013). Despite many success stories, the approach is restricted to problems of moderate
dimension, typically up to about 10. Of course, for a great many problems this is all that
is needed. However, to advance the state of the art, we need to scale the methodology to
high-dimensional parameter spaces. This is the goal of this paper.
It is difficult to scale Bayesian optimization to high dimensions. To ensure that a global
optimum is found, we require good coverage of X , but as the dimensionality increases, the
number of evaluations needed to cover X increases exponentially. As a result, there has been
little progress on this challenging problem, with a few exceptions. Bergstra et al. (2011) introduced a non-standard Bayesian optimization method based on a tree of one-dimensional
density estimators and applied it successfully to optimize the 238 parameters of a complex vision architecture (Bergstra, Yamins, & Cox, 2013). Hutter et al. (2011) used random forests
models in Bayesian optimization to achieve state-of-the-art performance in optimizing up
to 76 mixed discrete/continuous parameters of algorithms for solving hard combinatorial
problems, and to successfully carry out combined model selection and hyperparameter optimization for the 768 parameters of the Auto-WEKA framework (Thornton et al., 2013).
Eggensperger, Feurer, Hutter, Bergstra, Snoek, Hoos, and Leyton-Brown (2013) showed
that these two methods indeed yielded the best performance for high-dimensional hyperparameter optimization (e.g., in deep belief networks). However, both are based on weak
uncertainty estimates that can fail even for the optimization of very simple functions and
lack theoretical guarantees.
In the linear bandits case, Carpentier and Munos (2012) recently proposed a compressed
sensing strategy to attack problems with a high degree of sparsity. Also recently, Chen,
Castro, and Krause (2012) made significant progress by introducing a two stage strategy
for optimization and variable selection of high-dimensional GPs. In the first stage, sequential
likelihood ratio tests, with a couple of tuning parameters, are used to select the relevant
dimensions. This, however, requires the relevant dimensions to be axis-aligned with an
ARD kernel. Chen and colleagues provide empirical results only for synthetic examples (of
up to 400 dimensions), but they provide key theoretical guarantees.
Many researchers have noted that for certain classes of problems most dimensions do not
change the objective function significantly; examples include hyper-parameter optimization
for neural networks and deep belief networks (Bergstra & Bengio, 2012), as well as other
machine learning algorithms and various state-of-the-art algorithms for solving N P-hard
problems (Hutter, Hoos, & Leyton-Brown, 2014). That is to say these problems have low
effective dimensionality. To take advantage of this property, Bergstra and Bengio (2012)
proposed to simply use random search for optimization  the rationale being that points
sampled uniformly at random in each dimension can densely cover each low-dimensional
subspace. As such, random search can exploit low effective dimensionality without knowing
which dimensions are important. In this paper, we exploit the same property in a new
Bayesian optimization variant based on random embeddings.
Figure 2 illustrates the idea behind random embeddings in a nutshell. Assume we know
that a given D = 2 dimensional black-box function f (x1 , x2 ) only has d = 1 important
dimensions, but we do not know which of the two dimensions is the important one. We
can then perform optimization in the embedded 1-dimensional subspace defined by x1 = x2
since this is guaranteed to include the optimum.
364

fiBayesian Optimization in a Billion Dimensions

x1

x1

x*

g
in
dd
be
Em

x2

Important

x*

Unimportant

x2

Figure 2: This function in D=2 dimesions only has d=1 effective dimension: the vertical
axis indicated with the word important on the right hand side figure. Hence,
the 1-dimensional embedding includes the 2-dimensional functions optimizer.
It is more efficient to search for the optimum along the 1-dimensional random
embedding than in the original 2-dimensional space.

As we first demonstrated in a recent IJCAI conference paper (Wang, Zoghi, Hutter,
Matheson, & de Freitas, 2013), random embeddings enable us to scale Bayesian optimization
to arbitrary D provided the objective function has low intrinsic dimensionality. Importantly,
the algorithm associated with this idea, which we called REMBO, is not restricted to cases
with axis-aligned intrinsic dimensions but applies to any d-dimensional linear subspace.
Djolonga, Krause, and Cevher (2013) recently proposed an adaptive, but more expensive,
variant of REMBO with theoretical guarantees.
In this journal version of our work, we expand the presentation to provide more details throughout. In particular, we expand our description of the strategy for selecting the
boundaries of the low-dimensional space and for setting the kernel length scale parameter;
we show by means of an additional application (automatic configuration of random forest
body-part classifiers) that the performance of our technique does not collapse when the
problem does not have an obvious low effective dimensionality. Our experiments (Section
4) also show that REMBO can solve problems of previously untenable high extrinsic dimensions, and that REMBO can achieve state-of-the-art performance for optimizing the 47
discrete parameters of a popular mixed integer linear programming solver.

2. Bayesian Optimization
As mentioned in the introduction, Bayesian optimization has two ingredients that need to
be specified: The prior and the acquisition function. In this work, we adopt GP priors.
We review GPs very briefly and refer the interested reader to the book by Rasmussen and
Williams (2006). A GP is a distribution over functions specified by its mean function m()
365

fiWang, Hutter, Zoghi, Matheson, & de Freitas

and covariance k(, ). More specifically, given a set of points x1:t , with xi  RD , we have
f (x1:t )  N (m(x1:t ), K(x1:t , x1:t )),
where K(x1:t , x1:t )i,j = k(xi , xj ) serves as the covariance matrix. A common choice of k is
the squared exponential function (see Definition 7 on page 371), but many other choices are
possible depending on our degree of belief about the smoothness of the objective function.
An advantage of using GPs lies in their analytical tractability. In particular, given
observations x1:t with corresponding values f1:t , where fi = f (xi ), and a new point x , the
joint distribution is given by:
 

 

f1:t
m(x1:t )
K(x1:t , x1:t ) k(x1:t , x )
N
,
.
f
m
k(x , x1:t )
k(x , x )
For simplicity, we assume that m(x1:t ) = 0 and m = 0. Using the Sherman-MorrisonWoodbury formula, one can easily arrive at the posterior predictive distribution:
f  |Dt , x  N ((x |Dt ), (x |Dt )),
with data Dt = {x1:t , f1:t }, and mean and variance
(x |Dt ) = k(x , x1:t )K(x1:t , x1:t )1 f1:t
(x |Dt ) = k(x , x )  k(x , x1:t )K(x1:t , x1:t )1 k(x1:t , x ).
That is, we can compute the posterior predictive mean () and variance () exactly for
any point x .
At each iteration of Bayesian optimization, one has to re-compute the predictive mean
and variance. These two quantities are used to construct the second ingredient of Bayesian
optimization: The acquisition function. In this work, we report results for the expected
improvement acquisition function (Mockus, 1982; Vazquez & Bect, 2010; Bull, 2011):
u(x|Dt ) = E(max{0, ft+1 (x)  f (x+ )}|Dt ).
In this definition, x+ = arg maxx{x1:t } f (x) is the element with the best objective value in
the first t steps of the optimization process. The next query is:
xt+1 = arg max u(x|Dt ).
xX

Note that this utility favors the selection of points with high variance (points in regions
not well explored) and points with high mean value (points worth exploiting). We also
experimented with the UCB acquisition function (Srinivas et al., 2010; de Freitas et al.,
2012) and found it to yield similar results. The optimization of the closed-form acquisition
function can be carried out by off-the-shelf numerical optimization procedures, such as DIRECT (Jones, Perttunen, & Stuckman, 1993) and CMA-ES (Hansen & Ostermeier, 2001);
it is only based on the GP model of the blackbox function f and does not require additional
evaluations of f .
The Bayesian optimization procedure is shown in Algorithm 1.
366

fiBayesian Optimization in a Billion Dimensions

Algorithm 1 Bayesian Optimization
1: Initialize D0 as .
2: for t = 1, 2, . . . do
3:
Find xt+1  RD by optimizing the acquisition function u: xt+1 = arg maxxX u(x|Dt ).
4:
Augment the data Dt+1 = Dt  {(xt+1 , f (xt+1 ))}.
5:
Update the kernel hyper-parameters.
6: end for

3. Random Embedding for Bayesian Optimization
Before introducing our new algorithm and its theoretical properties, we need to define what
we mean by effective dimensionality formally.
Definition 1. A function f : RD  R is said to have effective dimensionality de , with
de  D, if
 there exists a linear subspace T of dimension de such that for all x>  T  RD and
x  T   RD , we have f (x> + x ) = f (x> ), where T  denotes the orthogonal
complement of T ; and
 de is the smallest integer with this property.
We call T the effective subspace of f and T  the constant subspace.
This definition simply states that the function does not change along the coordinates
x , and this is why we refer to T  as the constant subspace. Given this definition, the
following theorem shows that problems of low effective dimensionality can be solved via
random embedding.
Theorem 2. Assume we are given a function f : RD  R with effective dimensionality
de and a random matrix A  RDd with independent entries sampled according to N (0, 1)
and d  de . Then, with probability 1, for any x  RD , there exists a y  Rd such that
f (x) = f (Ay).
Proof. Please refer to the appendix.
Theorem 2 says that given any x  RD and a random matrix A  RDd , with probability
1, there is a point y  Rd such that f (x) = f (Ay). This implies that for any optimizer x? 
RD , there is a point y?  Rd with f (x? ) = f (Ay? ). Therefore, instead of optimizing in the
high dimensional space, we can optimize the function g(y) = f (Ay) in the lower dimensional
space. This observation gives rise to our new Random EMbedding Bayesian Optimization
(REMBO) algorithm (see Algorithm 2). REMBO first draws a random embedding (given
by A) and then performs Bayesian optimization in this embedded space.
In many practical optimization tasks, the goal is to optimize f over a compact subset
X  RD (typically a box), and f can often not be evaluated outside of X . Therefore, when
REMBO selects a point y such that Ay is outside the box X , it projects Ay onto X before
evaluating f . That is, g(y) = f (pX (Ay)), where pX : RD  RD is the standard projection
operator for our box-constraint: pX (y) = arg minzX kz  yk2 ; see Figure 3. We still need
to describe how REMBO chooses the bounded region Y  Rd , inside which it performs
367

fiWang, Hutter, Zoghi, Matheson, & de Freitas

y

d=1

Algorithm 2 REMBO: Bayesian Optimization with Random Embedding. Blue text denotes parts that are changed compared to standard Bayesian Optimization.
1: Generate a random matrix A  RDd
2: Choose the bounded region set Y  Rd
3: Initialize D0 as .
4: for t = 1, 2, . . . do
5:
Find yt+1  Rd by optimizing the acquisition function u: yt+1 = arg maxyY u(y|Dt ).
6:
Augment the data Dt+1 = Dt  {(yt+1 , f (Ayt+1 ))}.
7:
Update the kernel hyper-parameters.
8: end for

D=2

Convex projection

d
be

din

g

Em

of

to

Figure 3: Embedding from d = 1 into D = 2. The box illustrates the 2D constrained space
X , while the thicker red line illustrates the 1D constrained space Y. Note that if
Ay is outside X , it is projected onto X . The set Y must be chosen large enough
so that the projection of its image, AY, onto the effective subspace (vertical axis
in this diagram) covers the vertical side of the box.

Bayesian optimization. This is important because REMBOs effectiveness depends on the
size of Y. Locating the optimum within Y is easier if Y is small, but if we set Y too small it
may not actually contain the global optimizer. In the following theorem, we show that we
can choose Y in a way that only depends on the effective dimensionality de such that the
optimizer of the original problem is contained in the low dimensional space with constant
probability.
Theorem 3. Suppose we want to optimize a function f : RD  R with effective dimension
de  d subject to the box constraint X  RD , where X is centered around 0. Suppose
further that the effective subspace T of f is such that T is the span of de basis vectors,
and let x?>  T  X be an optimizer of f inside T . If A is a D  d random matrix
with independent standard Gaussian
entries, there exists an optimizer y?  Rd such that

de
?
?
?
?
f (Ay ) = f (x> ) and ky k2   kx> k2 with probability at least 1  .
Proof. Please refer to the appendix.
368

fiBayesian Optimization in a Billion Dimensions

Theorem 3 says that if the set X in the original space is a box constraint, then there
exists anoptimizer x?>  X that is de -sparse such that with probability at least 1  ,
ky? k2  de kx?> k2 where f (Ay? ) = f (x?> ). If the box constraint is X = [1, 1]D (which is
always achievable through rescaling), we have with probability at least 1   that


de ?
de p
?
de .
ky k2 
kx> k2 


Hence, to choose Y, we must ensure that the ball of radius de /, centred at the origin, lies
inside Y.
In practice, we have found that it is very
 unlikely that the optimizer falls on the corner of
?
the box constraint, implying that kx> k < de . Thus setting Y too big may be unnecessarily
wasteful. To improve our understanding of this effect, we developed a simulation study, in
which we drew random Gaussian matrices, used them to map various potential optimizers
?  Y, and studied the norms of y? .
x?> to their corresponding points y>
>
Assume for simplicity of presentation that Y is axis-aligned and de -dimensional (the
argument applies when d > de ). The section of the random matrix A that maps points
in Y to T is a random Gaussian matrix of dimension de  de . Let us call this section of
the matrix B. Since random Gaussian matrices are rotationally invariant in distribution,
d
we have for any orthonormal matrix O and a random Gaussian matrix B, OB = B. That

1 d
is, OB and B are equal in distribution. Similarly, for B1 , OB1 = BOT
= B1 .
d

Therefore, B1 is also rotationally invariant. Hence, kB1 x> k = kB1 x0> k as long as
kx> k2 = kx0> k2 . Following this equivalence for the supremum norm of projected vectors, it
suffices to choose a point with the largest norm in [1, 1]de in our simulations. We chose
x> = [1, 1,    , 1].
We conducted simulations for several embedding dimensions, de  {1, 2,    , 50}, by
drawing 10000 random Gaussian matrices and computing kB1 xk . We found that with
empirical probability above 1   (for decreasing values of ), it was the case that
1
max{log(de ), 1}.


d
These simulations indicate that we could set Y =  1 max{log(de ), 1}, 1 max{log(de ), 1} e .

Wedid this in our experiments and in particular chose  = log(d)/ d, so that Y was
[ d, d]d . Note that Theorem 3 is not useful for this choice, which suggests that there is
room to improve this aspect of our theory.
Some careful readers may wonder about the effect of the extrinsic dimensionality D.
In the following theorem, we show that given the same intrinsic dimensions, the extrinsic
dimensionality does not have an effect at all; in other words, REMBO is invariant to the
addition of unimportant dimensions.
kB1 xk <

Theorem 4 (Invariance to addition of unimportant dimensions). Let f : Rde  R and for
any D  N, D  de , define fD : RD  R such that fD adds D  de truly unimportant
(D2 D1 )d be random
dimensions to f : fD (z) = f (z1:de ). Let A1  RD1 d
 and
 A0  R
A1
Gaussian matrices with D2  D1  d and let A2 =
. Then, REMBO run using the
A0
369

fiWang, Hutter, Zoghi, Matheson, & de Freitas

same dimension d  de and bounded region Y yields exactly the same function values when
run with A1 on fD1 as when run with A2 on fD2 .
Proof. We only need to show that for each y  Rd , we have fD1 (A1 y) = fD2 (A2 y) since
this step of REMBO (line 6 of Algorithm 2) is the only one that differs between the two
algorithm runs. When this function evaluation step yields the same results for every y  Rd ,
then the two REMBO runs behave identically since the algorithm
 is otherwise identical
 and

A1
A1 y
deterministic after the selection of A in Step 1. Since A2 =
, we have A2 y =
.
A0
A0 y
Since D2  D1  de , the first de entries of this D2  1 vector A2 y are the first de entries of
A1 y. We thus have fD1 (A1 y) = f ([A1 y]1:de ) = f ([A2 y]1:de ) = fD2 (A2 y).
Finally, we show that REMBO is also invariant to rotations in the sense that given different rotation matrices, running REMBO would result in the same distributions of observed
function values. The argument is made concise in the following results.
Lemma 5. Consider function f : RD  R. Let fR : RD  R be such that fR (x) = f (Rx)
for some orthonormal matrix R  RDD . Then, REMBO run in bounded region Y yields
exactly the same sequence of function values when run with A on f as when run with R1 A
on fR for a matrix A  RDd .
Proof. REMBO uses f and A (resp. fR and R1 A) only in one spot (in line 6). Thus, the
proof is trivial by showing that f (Ayt+1 ) = fR (R1 Ayt+1 ) through simple algebra:
fR (R1 Ayt+1 ) = f (RR1 Ayt+1 ) = f (Ayt+1 ).

Theorem 6 (Invariance to rotations). Consider function f : RD  R. Let fR : RD  R
be such that fR (x) = f (Rx) for some orthonormal matrix R  RDD . Then, given random
Gaussian matrices A1  RDd and A2  RDd , REMBO run in bounded region Y yields
in distribution the same sequence of function values when run with A1 on f as when run
with A2 on fR .
d

Proof. Since R is orthonormal, we have R1 A1 = A2 . Therefore, REMBO run in bounded
region Y yields in distribution the same sequence of function values when run with R1 A1
on fR as when run with A2 on fR . We have also by Lemma 5 that REMBO run in bounded
region Y yields exactly the same sequence of function values when run with A1 on f as when
run with R1 A1 on fR . The conclusion follows from combining the previous arguments.
3.1 Increasing the Success Rate of REMBO
Theorem 3 only guarantees that Y contains the optimum with probability at least 1; with
probability    the optimizer lies outside of Y. There are several ways to guard against
this problem. One is to simply run REMBO multiple times with different independently
drawn random embeddings. Since the probability of failure with each embedding is , the
probability of the optimizer not being included in the considered space of k independently
drawn embeddings is  k . Thus, the failure probability vanishes exponentially quickly in
370

fiBayesian Optimization in a Billion Dimensions

the number of REMBO runs, k. Note also that these independent runs can be trivially
parallelized to harness the power of modern multi-core machines and large compute clusters.
Another way of increasing REMBOs success rate is to increase
the dimensionality d

it uses internally. When d > de , with probability 1 we have dde different embeddings of
dimensionality de . That is, we only need to select de columns of A  RDd to represent
the de relevant dimensions of x. The algorithm can achieve this by setting the remaining
d  de sub-components of the d-dimensional vector y to zero. Informally, since we have
more embeddings, it is more likely that one of these will include the optimizer. In our
experiments, we will assess the merits and shortcomings of these two strategies.
3.2 Choice of Kernel
Since REMBO uses GP-based Bayesian optimization to search in the region Y  Rd , we
need to define a kernel between two points y(1) , y(2)  Y. We begin with the standard
definition of the squared exponential kernel:
Definition 7. Let KSE (y) = exp(kyk2 /2). Given a length scale ` > 0, we define the
corresponding squared exponential kernel as
!
y(1)  y(2)
(2)
d (1)
k` (y , y ) = KSE
`
It is possible to work with two variants of this kernel. First, we can use k`d (y1 , y2 ) as in
Definition 7. We refer to this kernel as the low-dimensional kernel. We can also adopt an
implicitly defined high-dimensional kernel on X :
!
(1) )  p (Ay(2) )
p
(Ay
X
X
k`D (y(1) , y(2) ) = KSE
,
`
where pX : RD  RD is the projection operator for our box-constraint as above (see
Figure 3).
Note that when using this high-dimensional kernel, we are fitting the GP in D dimensions. However, the search space is no longer the box X , but it is instead given by the much
smaller subspace {pX (Ay) : y  Y}. Importantly, in practice it is easier to maximize the
acquisition function in this subspace.
Both kernel choices have strengths and weaknesses. The low-dimensional kernel has the
benefit of only requiring the construction of a GP in the space of intrinsic dimensionality
d, whereas the high-dimensional kernel requires the GP to be constructed in a space of
extrinsic dimensionality D. However, the low-dimensional kernel may waste time exploring
in the region of the embedding outside of X (see Figure 2) because two points far apart
in this region may be projected via pX to nearby points on the boundary of X . The highdimensional kernel is not affected by this problem because the search is conducted directly
on {pX (Ay) : y  Y} with distances calculated in X and not in Y.
The choice of kernel also depends on whether our variables are continuous, integer or
categorical. The categorical case is important because we often encounter optimization
371

fiWang, Hutter, Zoghi, Matheson, & de Freitas

Algorithm 3 Bayesian Optimization with Hyper-parameter Optimization.
input Threshold t .
input Upper and lower bounds U > L > 0 for hyper-parameter.
input Initial length scale hyper-parameter `  [L, U ].
1: Initialize C = 0
2: for t = 1, 2, . . . do
3:
Find
p xt+1 by optimizing the acquisition function u: xt+1 = arg maxxX u(x|Dt ).
4:
if  2 (xt+1 ) < t then
5:
C =C +1
6:
else
7:
C=0
8:
end if
9:
Augment the data Dt+1 = {Dt , (xt+1 , f (xt+1 ))}
10:
if t mod 20 = 0 or C = 5 then
11:
if C = 5 then
12:
U = max{0.9`, L}
13:
C=0
14:
end if
15:
Learn the hyper-parameter by optimizing the log marginal likelihood by using
DIRECT and CMA-ES: ` = arg maxl[L,U ] log p(f1:t+1 |x1:t+1 , l)
16:
end if
17: end for
problems that contain discrete choices. We define our kernel for categorical variables as:



D (1)
(2)
(1)
(2) 2
k (y , y ) = exp  h(s(Ay ), s(Ay )) ,
2
where y(1) , y(2)  Y  Rd , the function s maps continuous d-dimensional vectors to discrete
D-dimensional vectors, and h defines the distance between two discrete vectors. In more
detail, s(x) first uses pX to project x to x  [1, 1]D . For each dimension xi of x, s then
maps xi to a discrete value by scaling and rounding. In our experiments, following Hutter
(1)
(2)
(2009), we defined h(x(1) , x(2) ) = |{i : xi 6= xi }| so as not to impose an artificial ordering
between the values of categorical parameters. In essence, we measure the distance between
two points in the low-dimensional space as the Hamming distance between their mappings
in the high-dimensional space.
3.3 Hyper-parameter Optimization
For Bayesian optimization (and therefore REMBO), it is difficult to manually estimate
the true length scale hyper-parameter of a problem at hand. To avoid any manual steps and
to achieve robust performance across diverse sets of objective functions, in this paper we
adopted an adaptive hyper-parameter optimization scheme. The length scale of GPs is often
set by maximizing marginal likelihood (Rasmussen & Williams, 2006; Jones et al., 1998).
However, as demonstrated by Bull (2011), this approach, when implemented naively, may
not guarantee convergence. This is not only true of approaches that maximize the marginal
372

fiBayesian Optimization in a Billion Dimensions

likelihood, but also of approaches that rely on Monte Carlo sampling from the posterior
distribution (Brochu, Brochu, & de Freitas, 2010; Snoek et al., 2012) when the number of
data is very small, unless the prior is very informative.
Here, we propose to optimize the length scale parameter ` by maximizing the marginal
likelihood subject to an upper bound U which is decreased when the algorithm starts
exploiting too much. Full details are given in Algorithm 3. We say that the algorithm
is
p exploiting when the standard deviation at the maximizer of the acquisition function
(xt+1 ) is less than some threshold t for 5 consecutive iterations. Intuitively, this means
that the algorithm did not emphasize exploration (searching in new parts of the space,
where the predictive uncertainty is high) for 5 consecutive iterations. When this criterion
is met, the algorithm decreases its upper bound U multiplicatively and re-optimizes the
hyper-parameter subject to the new bound. Even when the criterion is not met the hyperparameter is re-optimized every 20 iterations. For each optimization of the acquisition
function, the algorithm runs both DIRECT (Jones et al., 1993) and CMA-ES (Hansen &
Ostermeier, 2001) and uses the result of the best of the two options. The astute reader may
wonder about the difficulty of optimizing the acquisition functions. For REMBO, however,
we have not found the optimization of the acquisition function to be a problem since we only
need to optimize it in the low-dimensional space and our acquisition function evaluations
are cheap, allowing us tens of thousands of evaluations in seconds that (empirically) suffice
to cover the low-dimensional space well.
The motivation of this algorithm is to rather err on the side of having too small a length
scale: given a squared exponential kernel k` , with a smaller length scale than another kernel
k, one can show that any function f in the RKHS characterized by k is also an element of the
RKHS characterized by k` . Thus, when running expected improvement, one can safely use
k` instead of k as the kernel of the GP and still preserve convergence (Bull, 2011). We argue
that (with a small enough lower bound L) the algorithm would eventually reduce the upper
bound enough to allow convergence. Also, the algorithm would not explore indefinitely as
L is required to be positive. In our experiments, we set the initial constraint [L, U ] to be
[0.01, 50] and set t = 0.002.
We want to stress the fact that the above argument is only known to hold for a class
of kernels over continuous domains (e.g. squared exponential and Matern class kernels).
Although we believe that a similar argument could be made for integer and categorical
kernels, rigorous arguments concerning convergence under these kernels remain a challenge
in Bayesian optimization.

4. Experiments
We now study REMBO empirically. We first use synthetic functions of small intrinsic dimensionality de = 2 but extrinsic dimension D up to 1 billion to demonstrate REMBOs
independence of D. Then, we apply REMBO to automatically optimize the 47 parameters of a widely-used mixed integer linear programming solver and demonstrate that it
achieves state-of-the-art performance. However, we also warn against the blind application
of REMBO. To illustrate this, we study REMBOs performance for tuning the 14 parameters of a random forest body part classifier used by Kinect. In this application, all the
D = 14 parameters appear to be important, and while REMBO (based on d = 3) finds
373

fiWang, Hutter, Zoghi, Matheson, & de Freitas

reasonable solutions (better than random search and comparable to what domain experts
achieve), standard Bayesian optimization can outperform REMBO (and the domain experts) in such moderate-dimensional spaces. More optimistically, this random forest tuning
application shows that REMBO does not fail catastrophically when it is not clear that the
optimization problem has low effective dimensionality.
4.1 Experimental Setup
For all our experiments, we used a single robust version of REMBO that automatically sets
its GPs length scale parameter as described in Section 3.3. The code for REMBO, as well as
all data used in our experiments is publicly available at https://github.com/ziyuw/rembo.
Some of our experiments required substantial computational resources, with the computational expense of each experiment depending mostly on the cost of evaluating the
respective black-box function. While the synthetic experiments in Section 4.2 only required
minutes for each run of each method, optimizing the mixed integer programming solver
in Section 4.4 required 4-5 hours per run, and optimizing the random forest classifier in
Section 4.5 required 4-5 days per run. In total, we used over half a year of CPU time for
the experiments in this paper. In the first two experiments, we study the effect of our
two methods for increasing REMBOs success rate (see Section 3.1) by running different
numbers of independent REMBO runs with different settings of its internal dimensionality
d.
4.2 Bayesian Optimization in a Billion Dimensions
The experiments in this section employ a standard de = 2-dimensional benchmark function
for Bayesian optimization, embedded in a D-dimensional space. That is, we add D  2
additional dimensions which do not affect the function at all. More precisely, the function
whose optimum we seek is f (x1:D ) = g(xi , xj ), where g is the Branin function
g(x1 , x2 ) = (x2 

5.1 2 5
1
x1 + x1  6)2 + 10(1 
) cos(x1 ) + 10
2
4

8

and where i and j are selected once using a random permutation. To measure the performance of each optimization method, we used the optimality gap: the difference of the best
function value it found and the optimal function value.
We evaluate REMBO using a fixed budget of 500 function evaluations that is spread
across multiple interleaved runs  for example, when using k = 4 interleaved REMBO runs,
each of them was only allowed 125 function evaluations. We study the choices of k and d by
considering several combinations of these values. The results in Table 1 demonstrate that
interleaved runs helped improve REMBOs performance. We note that in 13/50 REMBO
runs, the global optimum was indeed not contained in the box Y REMBO searched with
d = 2; this is the reason for the poor mean performance of REMBO with d = 2 and k = 1.
However, the remaining 37 runs performed very well, and REMBO thus performed well
when using multiple interleaved runs: with a failure rate of 13/50=0.26 per independent
run, the failure rate using k = 4 interleaved runs is only 0.264  0.005. One could easily
achieve an arbitrarily small failure rate by using many independent parallel runs. Using a
larger d is also effective in increasing the probability of the optimizer falling into REMBOs
374

fiBayesian Optimization in a Billion Dimensions

Figure 4: Comparison of random search (RANDOM), Bayesian optimization (BO), method
by Chen et al. (2012) (HD BO), and REMBO. Left: D = 25 extrinsic dimensions;
Right: D = 25, with a rotated objective function; Bottom: D = 109 extrinsic
dimensions. We plot means and 1/4 standard deviation confidence intervals of
the optimality gap across 50 trials.

box Y but at the same time slows down REMBOs convergence (such that interleaving
several short runs loses its effectiveness).
Next, we compared REMBO to standard Bayesian optimization (BO) and to random
search, for an extrinsic dimensionality of D = 25. Standard BO is well known to perform
well in low dimensions, but to degrade above a tipping point of about 15-20 dimensions.
Our results for D = 25 (see Figure 4, left) confirm that BO performed rather poorly just
above this critical dimensionality (merely tying with random search). REMBO, on the
other hand, still performed very well in 25 dimensions.
One important advantage of REMBO is that  in contrast to the approach of Chen
et al. (2012)  it does not require the effective dimension to be coordinate aligned. To
demonstrate this fact empirically, we rotated the embedded Branin function by an orthogonal rotation matrix R  RDD . That is, we replaced f (x) by f (Rx). Figure 4 (middle)
shows that REMBOs performance is not affected by this rotation.
Finally, since REMBO is independent of the extrinsic dimensionality D as long as the
intrinsic dimensionality de is small, it performed just as well in D = 1 000 000 000 dimensions
375

fiWang, Hutter, Zoghi, Matheson, & de Freitas

k
10
5
4
2
1

d=2
0.0022  0.0035
0.0004  0.0011
0.0001  0.0003
0.1514  0.9154
0.7406  1.8996

d=4
0.1553  0.1601
0.0908  0.1252
0.0654  0.0877
0.0309  0.0687
0.0143  0.0406

d=6
0.4865  0.4769
0.2586  0.3702
0.3379  0.3170
0.1643  0.1877
0.1137  0.1202

Table 1: Optimality gap for de = 2-dimensional Branin function embedded in D = 25
dimensions, for REMBO variants using a total of 500 function evaluations. The
variants differed in the internal dimensionality d and in the number of interleaved
runs k (each such run was only allowed 500/k function evaluations). We show
mean and standard deviations of the optimality gap achieved after 500 function
evaluations.

(see Figure 4, right). To the best of our knowledge, the only other existing method that
can be run in such high dimensionality is random search.
For reference, we also evaluated the method of Chen et al. (2012) for these functions,
confirming that it does not handle rotation gracefully: while it performed best in the nonrotated case for D = 25, it performed worst in the rotated case. It could not be used
efficiently for more than D = 1, 000. Based on a Mann-Whitney U test with Bonferroni
multiple-test correction, all performance differences were statistically significant, except
Random vs. standard BO. Finally, comparing REMBO to the method of Chen et al. (2012),
we also note that REMBO is much simpler to implement and that its results are very reliable
(with interleaved runs).
4.3 Synthetic Discrete Experiment
In this section, we test the high-dimensional kernel with a synthetic experiment. Specifically,
we again optimize the Branin function, but restrict its domain to 225 discrete points on a
regular grid. As above, we added 23 additional irrelevant dimensions to make the problem
25-dimensional in total.
We used a small fixed budget of 100 function evaluations for all algorithms involved
as the problem would require no more than 225 evaluations to be solved completely. We
used k = 4 interleaved runs for REMBO. We again compare REMBO to random search
and standard BO. For REMBO, we use the high-dimensional kernel to handle the discrete
nature of the problem. The result of the comparison is summarized in Figure 5. Standard
BO again suffered from the high extrinsic dimensionality and performed slightly worse than
random search. REMBO, on the other hand, performed well in this setting.
4.4 Automatic Configuration of a Mixed Integer Linear Programming Solver
State-of-the-art algorithms for solving hard computational problems tend to parameterize
several design choices in order to allow a customization of the algorithm to new problem domains. Automated methods for algorithm configuration have recently demonstrated
that substantial performance gains of state-of-the-art algorithms can be achieved in a fully
376

fiBayesian Optimization in a Billion Dimensions

Figure 5: Comparison of random search (RANDOM), Bayesian optimization (BO), and
REMBO. D = 25 extrinsic dimensions. We plot means and 1/4 standard deviation confidence intervals of the optimality gap across 50 trials.

automated fashion (Mockus, Mockus, & Mockus, 1999; Hutter, Hoos, Leyton-Brown, &
Stutzle, 2009; Hutter, Hoos, & Leyton-Brown, 2010; Vallati, Fawcett, Gerevini, Hoos, &
Saetti, 2011; Bergstra et al., 2011; Wang & de Freitas, 2011). These successes have led
to a paradigm shift in algorithm development towards the active design of highly parameterized frameworks that can be automatically customized to particular problem domains
using optimization (Hoos, 2012; Bergstra et al., 2013; Thornton et al., 2013). The resulting algorithm configuration problems have been shown to have low dimensionality (Hutter
et al., 2014), and here, we demonstrate that REMBO can exploit this low dimensionality
even in the discrete spaces typically encountered in algorithm configuration. We use a configuration problem obtained from Hutter et al. (2010), aiming to configure the 40 binary
and 7 categorical parameters of lpsolve (Berkelaar, Eikland, & Notebaert, 2016) , a popular
mixed integer programming (MIP) solver that has been downloaded over 40 000 times in
the last year. The objective is to minimize the optimality gap lpsolve can obtain in a time
limit of five seconds for a MIP encoding of a wildlife corridor problem from computational
sustainability (Gomes, van Hoeve, & Sabharwal, 2008). Algorithm configuration usually
aims to improve performance for a representative set of problem instances, and effective
methods need to solve two orthogonal problems: searching the parameter space effectively
and deciding how many instances to use in each evaluation (to trade off computational overhead and over-fitting). Our contribution is for the first of these problems; to focus on how
effectively the different methods search the parameter space, we only consider configuration
on a single problem instance.
Due to the discrete nature of this optimization problem, we could only apply REMBO
using the high-dimensional kernel for categorical variables kD (y(1) , y(2) ) described in Section 3.2. While we have not proven any theoretical guarantees for discrete optimization
377

fiWang, Hutter, Zoghi, Matheson, & de Freitas

Figure 6: Performance of various methods for configuration of lpsolve; we show the optimality gap lpsolve achieved with the configurations found by the various methods (lower is better). Left: a single run of each method; Right: performance with
k = 4 interleaved runs.

problems, REMBO appears to effectively exploit the low effective dimensionality of at least
this particular optimization problem.
Figure 6 (left) compares BO, REMBO, and the baseline random search against ParamILS
(Hutter et al., 2009) and SMAC (Hutter et al., 2011). ParamILS and SMAC were specifically
designed for the configuration of algorithms with many discrete parameters and define the
current state of the art for this problem. Nevertheless, here SMAC and our vanilla REMBO
method performed best. Based on a Mann-Whitney U test with Bonferroni multiple-test
correction, they both yielded statistically significantly better results than both Random
and standard BO; no other performance differences were significant. The figure only shows
REMBO with d = 5 to avoid clutter, but we did not optimize this parameter; the only
other value we tried (d = 3) resulted in indistinguishable .
As in the synthetic experiment, REMBOs performance could be further improved by
using multiple interleaved runs. However, as shown by Hutter, Hoos, and Leyton-Brown
(2012), multiple independent runs can also improve the performance of SMAC and especially
ParamILS. Thus, to be fair, we re-evaluated all approaches using interleaved runs. Figure
6 (right) shows that ParamILS and REMBO benefitted most from interleaving k = 4 runs.
However, the statistical test results did not change, still showing that SMAC and REMBO
outperformed Random and BO, with no other significant performance differences.
4.5 Automatic Configuration of Random Forest Kinect Body Part Classifier
We now evaluate REMBOs performance for optimizing the 14 parameters of a random
forest body part classifier. This classifier closely follows the proprietary system used in the
Microsoft Kinect (Shotton, Fitzgibbon, Cook, Sharp, Finocchio, Moore, Kipman, & Blake,
2011) and is available at https://github.com/david-matheson/rftk.
378

fiBayesian Optimization in a Billion Dimensions

Figure 7: Left: ground truth depth, ground truth body parts and predicted body parts;
Right: features specified by offsets u and v.

We begin by describing some details of the dataset and classifier in order to build
intuition for the objective function and the parameters being optimized. The data we
used consists of pairs of depth images and ground truth body part labels. Specifically,
we used 1 500 pairs of 320x240 resolution depth and body part images, each of which was
synthesized from a random pose of the CMU mocap dataset. Depth, ground truth body
parts and predicted body parts (as predicted by the classifier described below) are visualized
for one pose in Figure 7 (left). There are 19 body parts plus one background class. For each
of these 20 possible labels, the training data contained 25 000 pixels, randomly selected from
500 training images. Both validation and test data contained all pixels in the 500 validation
and test images, respectively.
The random forest classifier is applied to one pixel P at a time. At each node of each of
its decision trees, it computes the depth difference between two pixels described by offsets
from P and compares this to a threshold. At training time, many possible pairs of offsets
are generated at random, and the pair yielding highest information gain for the training
data points is selected. Figure 7 (right) visualizes a potential feature for the pixel in the
green box: it computes the depth difference between the pixels in the red box and the white
box, specified by respective offsets u and v. At training time, u and v are drawn from two
independent 2-dimensional Gaussian distributions, each of which is parameterized by its
two mean parameters 1 and 2 and three covariance terms 11 , 12 , and 22 (21 = 12
because of symmetry). These constitute 10 of the parameters that need to be optimized,
with range [-50,50] for the mean components and [1, 200] for the covariance terms. Low
covariance terms yield local features, while high terms yield global features. Next to these
ten parameters, the random forest classifier has four other standard parameters, outlined
in Table 2. It is well known in computer vision that many of the parameters described here
are important. Much research has been devoted to identifying their best values, but results
are dataset specific, without definitive general answers.
The objective in optimizing these RF classifier parameters is to find a parameter setting
that learns the best classifier in a given time budget of five minutes. To enable competitive
performance in this short amount of time, at each node of the tree only a random subset of
379

fiWang, Hutter, Zoghi, Matheson, & de Freitas

Table 2: Parameter ranges for random forest classifier. For the purpose of optimization,
the maximum tree depth and the number of potential offsets were transformed to
log space.
Parameter

Range

Max. tree depth
Min. No. samples for non leaf nodes
No. potential offsets to evaluate
Bootstrap for per tree sampling

[1 60]
[1 100]
[1 5000]
[T F]

data points is considered. Also note that the above parameters do not include the number
of trees T in the random forest; since performance improves monotonically in T , we created
as many trees as possible in the time budget. Trees are constructed depth first and returned
in their current state when the time budget is exceeded. Using a fixed budget results in
a subtle optimization problem because of the complex interactions between the various
parameters (maximum depth, number of potential offsets, number of trees and accuracy).
It is unclear a priori whether a low-dimensional subspace of these 14 interacting parameters exists that captures the classification accuracy of the resulting random forests.
We performed large-scale computational experiments with REMBO, random search, and
standard Bayesian optimization (BO) to study this question. In this experiment, we used
the high-dimensional kernel for REMBO to avoid the potential over-exploration problems
of the low-dimensional kernel described in Section 3.2. We believed that D = 14 dimensions
would be small enough to avoid inefficiencies in fitting the GP in D dimensions. This belief
was confirmed by the observation that standard BO (which operates in D = 14 dimensions)
performed well for this problem.
Figure 8 (left) shows the results that can be obtained by a single run of random search,
BO, and REMBO. Remarkably, REMBO clearly outperformed random search, even based
on as few as d = 3 dimensions.2 However, since the extrinsic dimensionality was only
a moderate D = 14, standard Bayesian optimization performed well, and since it was
not limited to a low-dimensional subspace it outperformed REMBO. Nevertheless, several
REMBO runs actually performed very well, comparably with the best runs of BO. Consequently, when running k = 4 interleaved runs of each method, REMBO performed almost
as well as BO, matching its performance up to about 450 function evaluations (see Figure
8, right).
We conclude that the parameter space of this RF classifier does not appear to have a
clear low effective dimensionality; since the extrinsic dimensionality is only moderate, this
leads REMBO to perform somewhat worse than standard Bayesian optimization, but it is
still possible to achieve reasonable performance based on as little as d = 3 dimensions.
2. Due to the large computational expense of this experiment (in total over half a year of CPU time), we
only performed conclusive experiments with d = 3; preliminary runs of REMBO with d = 4 performed
somewhat worse than those with d = 3 for a budget of 200 function evaluations, but were still improving
at that point.

380

fiBayesian Optimization in a Billion Dimensions

Figure 8: Performance of various methods for optimizing RF parameters for body part
classification. For all methods, we show RF accuracy (mean  1/4 standard
deviation across 10 runs) for all 2.2 million non background pixels in the 500pose validation set, using the RF parameters identified by the method. The
results on the test set were within 1% of the results on the validation set. Left:
performance with a single run of each method; Right: performance with k = 4
interleaved runs.

5. Conclusion
We have demonstrated that it is possible to use random embeddings in Bayesian optimization to optimize functions of extremely high extrinsic dimensionality D provided that
they have low intrinsic dimensionality de . Moreover, our resulting REMBO algorithm is
coordinate independent and it only requires a simple modification of the original Bayesian
optimization algorithm; namely multiplication by a random matrix. We proved REMBOs
independence of D theoretically and empirically validated it by optimizing low-dimensional
functions embedded in previously untenable extrinsic dimensionalities of up to 1 billion.
We also theoretically and empirically showed REMBOs rotational invariance. Finally, we
demonstrated that REMBO achieves state-of-the-art performance for optimizing the 47 discrete parameters of a popular mixed integer programming solver, thereby providing further
evidence for the observation (already put forward by Bergstra, Hutter and colleagues) that,
for many problems of great practical interest, the number of important dimensions indeed
appears to be much lower than their extrinsic dimensionality.
We note that the central idea of our work  using an otherwise unmodified optimization procedure in a randomly embedded space  in principle could be applied to arbitrary
optimization procedures. Evaluating the effciency of this technique for other procedures is
an interesting topic for future work.
381

fiWang, Hutter, Zoghi, Matheson, & de Freitas

Acknowledgements
We thank Christof Schotz for proofreading a draft of this article. Frank Hutter gratefully
acknowledges funding by the German Research Foundation (DFG) under Emmy Noether
grant HU 1900/2-1.

Appendix A. Proof of Theorem 2
Proof. Since f has effective dimensionality de , there exists an effective subspace T  RD ,
such that rank(T ) = de . Furthermore, any x  RD decomposes as x = x> + x , where
x>  T and x  T  . Hence, f (x) = f (x> + x ) = f (x> ). Therefore, without loss of
generality, it will suffice to show that for all x>  T , there exists a y  Rd such that
f (x> ) = f (Ay).
Let   RDde be a matrix, whose columns form an orthonormal basis for T . Hence,
for each x>  T , there exists a c  Rde such that x> = c. Let us for now assume that
T A has rank de . If T A has rank de , there exists a y such that (T A)y = c. The
orthogonal projection of Ay onto T is given by
T Ay = c = x> .
Thus Ay = x> + x0 for some x0  T  since x> is the projection Ay onto T . Consequently,
f (Ay) = f (x> + x0 ) = f (x> ).
It remains to show that, with probability one, the matrix T A has rank de . Let
Ae  RDde be a submatrix of A consisting of any de columns of A, which are i.i.d. samples distributed according to N (0, I). Then, T ai are i.i.d. samples from N (0, T ) =
2
N (0de , Ide de ), and so we have T Ae , when considered as an element of Rde , is a sample
2
from N (0d2e , Id2e d2e ). On the other hand, the set of singular matrices in Rde has Lebesgue
measure zero, since it is the zero set of a polynomial (i.e. the determinant function) and
polynomial functions are Lebesgue measurable. Moreover, the Normal distribution is absolutely continuous with respect to the Lebesgue measure, so our matrix T Ae is almost
surely non-singular, which means that it has rank de and so the same is true of T A, whose
columns contain the columns of T Ae .

Appendix B. Proof of Theorem 3
Proof. Since X is a box constraint, by projecting x? to T we get x?>  T  X . Also, since
x? = x?> + x for some x  T  , we have f (x? ) = f (x?> ). Hence, x?> is an optimizer. By
using the same argument as appeared in Proposition 1, it is easy to see that with probability
1 x  T y  Rd such that Ay = x + x where x  T  . Let  be the matrix whose
columns form a standard basis for T . Without loss of generality, we can assume that
 
I
 = de
0
Then, as shown in Proposition 2, there exists a y?  Rd such that T Ay? = x?> . Note
that for each column of A, we have
 

Ide 0
T
 ai  N 0,
.
0 0
382

fiBayesian Optimization in a Billion Dimensions

Therefore T Ay? = x?> is equivalent to By? = x?> where B  Rde de is a random matrix
with independent standard Gaussian entries and x?> is the vector that contains the first de
entries of x?> (the rest are 0s). By Theorem 3.4 of (Sankar, Spielman, & Teng, 2003), we
have
 

de
1
P kB k2 
 .

Thus, with probability at least 1, ky? k  kB1 k2 kx?> k2 = kB1 k2 kx?> k2 



de
?
 kx> k2 .

References
Azimi, J., Fern, A., & Fern, X. (2010). Batch Bayesian optimization via simulation matching.
In Advances in Neural Information Processing Systems, pp. 109117.
Azimi, J., Fern, A., & Fern, X. (2011). Budgeted optimization with concurrent stochasticduration experiments. In Advances in Neural Information Processing Systems, pp.
10981106.
Azimi, J., Jalali, A., & Fern, X. (2012). Hybrid batch Bayesian optimization. In International Conference on Machine Learning.
Bergstra, J., Bardenet, R., Bengio, Y., & Kegl, B. (2011). Algorithms for hyper-parameter
optimization. In Advances in Neural Information Processing Systems, pp. 25462554.
Bergstra, J., & Bengio, Y. (2012). Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13, 281305.
Bergstra, J., Yamins, D., & Cox, D. D. (2013). Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. In
International Conference on Machine Learning, pp. 115123.
Berkelaar, M., Eikland, K., & Notebaert, P. (2016). lpsolve : Open source (Mixed-Integer)
Linear Programming system. http://lpsolve.sourceforge.net/.
Brochu, E., Brochu, T., & de Freitas, N. (2010). A Bayesian interactive optimization
approach to procedural animation design. In Proceedings of the 2010 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, pp. 103112.
Brochu, E., Cora, V. M., & de Freitas, N. (2009). A tutorial on Bayesian optimization of
expensive cost functions, with application to active user modeling and hierarchical
reinforcement learning. Tech. rep. UBC TR-2009-23 and arXiv:1012.2599v1, Dept. of
Computer Science, University of British Columbia.
Brochu, E., de Freitas, N., & Ghosh, A. (2007). Active preference learning with discrete
choice data. In Advances in Neural Information Processing Systems, pp. 409416.
Bubeck, S., Munos, R., Stoltz, G., & Szepesvari, C. (2011). X-armed bandits. Journal of
Machine Learning Research, 12, 16551695.
Bull, A. D. (2011). Convergence rates of efficient global optimization algorithms. Journal
of Machine Learning Research, 12, 28792904.
383

fiWang, Hutter, Zoghi, Matheson, & de Freitas

Carpentier, A., & Munos, R. (2012). Bandit theory meets compressed sensing for high
dimensional stochastic linear bandit. In Artificial Intelligence and Statistics, pp. 190
198.
Chen, B., Castro, R., & Krause, A. (2012). Joint optimization and variable selection of highdimensional Gaussian processes. In International Conference on Machine Learning.
de Freitas, N., Smola, A., & Zoghi, M. (2012). Exponential regret bounds for Gaussian process bandits with deterministic observations. In International Conference on Machine
Learning.
Denil, M., Bazzani, L., Larochelle, H., & de Freitas, N. (2012). Learning where to attend
with deep architectures for image tracking. Neural Computation, 24 (8), 21512184.
Djolonga, J., Krause, A., & Cevher, V. (2013). High dimensional Gaussian process bandits.
In Advances in Neural Information Processing Systems, pp. 10251033.
Eggensperger, K., Feurer, M., Hutter, F., Bergstra, J., Snoek, J., Hoos, H., & Leyton-Brown,
K. (2013). Towards an empirical foundation for assessing Bayesian optimization of
hyperparameters. In NIPS Workshop on Bayesian Optimization in Theory and Practice.
Frazier, P., Powell, W., & Dayanik, S. (2009). The knowledge-gradient policy for correlated
normal beliefs. INFORMS journal on Computing, 21 (4), 599613.
Gomes, C. P., van Hoeve, W., & Sabharwal, A. (2008). Connections in networks: A hybrid
approach. In International Conference on Integration of Artificial Intelligence and
Operations Research, Vol. 5015, pp. 303307.
Gramacy, R. B., Lee, H. K. H., & Macready, W. G. (2004). Parameter space exploration
with Gaussian process trees. In International Conference on Machine Learning, pp.
4552.
Gramacy, R., & Polson, N. (2011). Particle learning of gaussian process models for sequential
design and optimization. Journal of Computational and Graphical Statistics, 20 (1),
102118.
Hamze, F., Wang, Z., & de Freitas, N. (2013). Self-avoiding random dynamics on integer
complex systems. ACM Transactions on Modelling and Computer Simulation, 23 (1),
9:19:25.
Hansen, N., & Ostermeier, A. (2001). Completely derandomized self-adaptation in evolution
strategies. Evolutionary Computation, 9 (2), 159195.
Hennig, P., & Schuler, C. (2012). Entropy search for information-efficient global optimization. Journal of Machine Learning Research, 98888, 18091837.
Hoffman, M., Brochu, E., & de Freitas, N. (2011). Portfolio allocation for Bayesian optimization. In Uncertainty in Artificial Intelligence, pp. 327336.
Hoffman, M., Kueck, H., de Freitas, N., & Doucet, A. (2009). New inference strategies for
solving Markov decision processes using reversible jump MCMC. In Uncertainty in
Artificial Intelligence, pp. 223231.
384

fiBayesian Optimization in a Billion Dimensions

Hoffman, M., Shahriari, B., & de Freitas, N. (2014). On correlation and budget constraints
in model-based bandit optimization with application to automatic machine learning.
In Artificial Intelligence and Statistics.
Hoos, H. H. (2012). Programming by optimization. Communications of the ACM, 55 (2),
7080.
Hutter, F. (2009). Automated Configuration of Algorithms for Solving Hard Computational
Problems. Ph.D. thesis, University of British Columbia, Vancouver, Canada.
Hutter, F., Hoos, H., & Leyton-Brown, K. (2014). An efficient approach for assessing
hyperparameter importance. In International Conference on Machine Learning.
Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2010). Automated configuration of mixed
integer programming solvers. In Conference on Integration of Artificial Intelligence
and Operations Research, pp. 186202.
Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2011). Sequential model-based optimization
for general algorithm configuration. In Learning and Intelligent Optimization, pp.
507523.
Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2012). Parallel algorithm configuration. In
Learning and Intelligent Optimization, pp. 5570.
Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2013). An evaluation of sequential modelbased optimization for expensive blackbox functions. In Proceedings of GECCO-13
Workshop on Blackbox Optimization Benchmarking (BBOB13).
Hutter, F., Hoos, H. H., Leyton-Brown, K., & Stutzle, T. (2009). ParamILS: an automatic
algorithm configuration framework. Journal of Artificial Intelligence Research, 36,
267306.
Jones, D. R., Perttunen, C. D., & Stuckman, B. E. (1993). Lipschitzian optimization without
the Lipschitz constant. J. of Optimization Theory and Applications, 79 (1), 157181.
Jones, D. (2001). A taxonomy of global optimization methods based on response surfaces.
Journal of Global Optimization, 21 (4), 345383.
Jones, D., Schonlau, M., & Welch, W. (1998). Efficient global optimization of expensive
black-box functions. Journal of Global optimization, 13 (4), 455492.
Kueck, H., de Freitas, N., & Doucet, A. (2006). SMC samplers for Bayesian optimal nonlinear design. In IEEE Nonlinear Statistical Signal Processing Workshop, pp. 99102.
Kueck, H., Hoffman, M., Doucet, A., & de Freitas, N. (2009). Inference and learning for
active sensing, experimental design and control. In Pattern Recognition and Image
Analysis, Vol. 5524, pp. 110.
Lizotte, D., Greiner, R., & Schuurmans, D. (2011). An experimental methodology for
response surface optimization methods. Journal of Global Optimization, 53 (4), 138.
Lizotte, D., Wang, T., Bowling, M., & Schuurmans, D. (2007). Automatic gait optimization
with Gaussian process regression. In International Joint Conference on Artificial
Intelligence, pp. 944949.
385

fiWang, Hutter, Zoghi, Matheson, & de Freitas

Mahendran, N., Wang, Z., Hamze, F., & de Freitas, N. (2012). Adaptive MCMC with
Bayesian optimization. Journal of Machine Learning Research - Proceedings Track,
22, 751760.
Marchant, R., & Ramos, F. (2012). Bayesian optimisation for intelligent environmental
monitoring. In NIPS workshop on Bayesian Optimization and Decision Making.
Martinez-Cantin, R., de Freitas, N., Doucet, A., & Castellanos, J. A. (2007). Active policy
learning for robot planning and exploration under uncertainty. In Robotics, Science
and Systems.
Mockus, J. (1982). The Bayesian approach to global optimization. In Systems Modeling
and Optimization, Vol. 38, pp. 473481. Springer.
Mockus, J. (1994). Application of Bayesian approach to numerical methods of global and
stochastic optimization. J. of Global Optimization, 4 (4), 347365.
Mockus, J., Mockus, A., & Mockus, L. (1999). Bayesian approach for randomization of
heuristic algorithms of discrete programming. American Math. Society.
Osborne, M. A., Garnett, R., & Roberts, S. J. (2009). Gaussian processes for global optimisation. In Learning and Intelligent Optimization, pp. 115.
Rasmussen, C. E. (2003). Gaussian processes to speed up hybrid Monte Carlo for expensive
Bayesian integrals. In Bayesian Statistics 7.
Rasmussen, C. E., & Williams, C. K. I. (2006). Gaussian Processes for Machine Learning.
The MIT Press.
Sankar, A., Spielman, D., & Teng, S. (2003). Smoothed analysis of the condition numbers
and growth factors of matrices. Tech. rep. Arxiv preprint cs/0310022, MIT.
Shotton, J., Fitzgibbon, A., Cook, M., Sharp, T., Finocchio, M., Moore, R., Kipman, A.,
& Blake, A. (2011). Real-time human pose recognition in parts from single depth
images. In IEEE Computer Vision and Pattern Recognition, pp. 12971304.
Snoek, J., Larochelle, H., & Adams, R. P. (2012). Practical Bayesian optimization of machine learning algorithms. In Advances in Neural Information Processing Systems,
pp. 29602968.
Srinivas, N., Krause, A., Kakade, S. M., & Seeger, M. (2010). Gaussian process optimization
in the bandit setting: No regret and experimental design. In International Conference
on Machine Learning, pp. 10151022.
Swersky, K., Snoek, J., & Adams, R. P. (2013). Multi-task Bayesian optimization. In
Advances in Neural Information Processing Systems, pp. 20042012.
Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another
in view of the evidence of two samples. Biometrika, 25 (3/4), 285294.
Thornton, C., Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2013). Auto-WEKA: Combined selection and hyperparameter optimization of classification algorithms. In ACM
SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 847855.
Vallati, M., Fawcett, C., Gerevini, A. E., Hoos, H. H., & Saetti, A. (2011). Generating
fast domain-optimized planners by automatically configuring a generic parameterised
planner. In ICAPS Planning and Learning Workshop.
386

fiBayesian Optimization in a Billion Dimensions

Vazquez, E., & Bect, J. (2010). Convergence properties of the expected improvement algorithm with fixed mean and covariance functions. Journal of Statistical Planning and
Inference, 140, 30883095.
Wang, Z., & de Freitas, N. (2011). Predictive adaptation of hybrid Monte Carlo with
Bayesian parametric bandits. In NIPS Deep Learning and Unsupervised Feature Learning Workshop.
Wang, Z., & de Freitas, N. (2014). Bayesian multiscale optimistic optimization. In Artificial
Intelligence and Statistics.
Wang, Z., Zoghi, M., Hutter, F., Matheson, D., & de Freitas, N. (2013). Bayesian optimization in high dimensions via random embeddings. In International Joint Conference
on Artificial Intelligence, pp. 17781784.

387

fiJournal of Artificial Intelligence Research 55 (2016) 499-564

Submitted 07/15; published 02/16

Module Extraction in Expressive Ontology Languages
via Datalog Reasoning
Ana Armas Romero
Mark Kaminski
Bernardo Cuenca Grau
Ian Horrocks

ana.armas@cs.ox.ac.uk
mark.kaminski@cs.ox.ac.uk
bernardo.cuenca.grau@cs.ox.ac.uk
ian.horrocks@cs.ox.ac.uk

Department of Computer Science,
University of Oxford,
Wolfson Building, Parks Road,
Oxford, OX1 3QD, UK

Abstract
Module extraction is the task of computing a (preferably small) fragment M of an
ontology O that preserves a class of entailments over a signature of interest . Extracting
modules of minimal size is well-known to be computationally hard, and often algorithmically
infeasible, especially for highly expressive ontology languages. Thus, practical techniques
typically rely on approximations, where M provably captures the relevant entailments,
but is not guaranteed to be minimal. Existing approximations ensure that M preserves
all second-order entailments of O w.r.t. , which is a stronger condition than is required
in many applications, and may lead to unnecessarily large modules in practice. In this
paper we propose a novel approach in which module extraction is reduced to a reasoning
problem in datalog. Our approach generalises existing approximations in an elegant way.
More importantly, it allows extraction of modules that are tailored to preserve only specific
kinds of entailments, and thus are often significantly smaller. Our evaluation on a wide
range of ontologies confirms the feasibility and benefits of our approach in practice.

1. Introduction
Module extraction is the task of computing, given an ontology O and a signature of interest
, a (preferably small) subset M of O (a module) that preserves the class of -entailments
of O relevant to the application at hand. Such a module is therefore indistinguishable from
O w.r.t. relevant -entailments, and the application can safely rely on M instead of O for
all tasks that concern only symbols from .
Module extraction has received a great deal of attention in recent years (Seidenberg
& Rector, 2006; Stuckenschmidt, Parent, & Spaccapietra, 2009; Cuenca Grau, Horrocks,
Kazakov, & Sattler, 2008; Kontchakov, Wolter, & Zakharyaschev, 2010; Del Vescovo, Parsia, Sattler, & Schneider, 2011; Nortje, Britz, & Meyer, 2013; Gatens, Konev, & Wolter,
2014). Modules have found numerous applications in ontology reuse (Cuenca Grau et al.,
2008; Jimenez-Ruiz, Cuenca Grau, Sattler, Schneider, & Berlanga Llavori, 2008), matching
(Jimenez-Ruiz & Cuenca Grau, 2011), debugging (Suntisrivaraporn, Qi, Ji, & Haase, 2008;
Ludwig, 2014) and classification (Armas Romero, Cuenca Grau, & Horrocks, 2012; Tsarkov
& Palmisano, 2012; Cuenca Grau, Halaschek-Wiener, Kazakov, & Suntisrivaraporn, 2010).
c 2016 AI Access Foundation. All rights reserved.

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

The preservation of relevant entailments is formalised via inseparability relations (Konev,
Lutz, Walther, & Wolter, 2009). The strongest such notion is model inseparability, which requires that it must be possible to turn any model of M into a model of O by (re-)interpreting
only symbols outside ; in this case, M preserves all second-order -entailments of O
(Konev, Lutz, Walther, & Wolter, 2013). A weaker and more flexible notion is that of
deductive inseparability, which only requires O and M to entail the same -formulas in
a particular query language. Unfortunately, the decision problems associated with module
extraction are generally of high complexity or even undecidable, especially for expressive
ontology languages. For model inseparability, checking whether M is a module of O w.r.t.
 is undecidable even if O is restricted to the lightweight description logic (DL) EL (Konev
et al., 2013), for which standard reasoning is tractable (Baader, Brandt, & Lutz, 2005).
For deductive inseparability, the problem is typically decidable for lightweight DLs and
reasonable query languages, albeit still of high worst-case complexity; for instance, it is
ExpTime-complete for EL if we consider concept inclusions as the query language (Lutz
& Wolter, 2010). Practical algorithms that ensure minimality of the extracted modules are
known only for ELI ontologies satisfying a particular acyclicity condition (Konev et al.,
2013) as well as for dialects of DL-Lite (Kontchakov et al., 2010). To the best of our knowledge, the complexity of module extraction for ontology languages that are not based on
DLs, such as variants of datalog (Cal, Gottlob, Lukasiewicz, Marnette, & Pieris, 2010),
remains largely unexplored.
Practical module extraction techniques are typically based on sound approximations,
which ensure that the computed fragment M is a module (i.e., inseparable from O w.r.t.
), but provide no minimality guarantee. The most popular such techniques are based
on a family of polynomially checkable conditions based on the notion of syntactic locality
(Cuenca Grau, Horrocks, Kazakov, & Sattler, 2007a; Cuenca Grau et al., 2008; Sattler,
Schneider, & Zakharyaschev, 2009). Each locality-based module M enjoys a number of
desirable properties w.r.t. the signature  of interest:
(P1) It is model inseparable from O, thus preserving all second-order -entailments of O.
(P2) It is depleting, in the sense that O \ M is inseparable from the empty ontology; this
implies that no relevant information is left behind after extracting M from O.
(P3) It is self-contained, in that it preserves not only the relevant entailments w.r.t. , but
also w.r.t. all other symbols in its signature.
(P4) It is justification-preserving, in the sense that each subset-minimal fragment of O
preserving a -entailment (each justification) is contained in M.
(P5) It can be computed efficiently, even for ontologies in expressive description logics.
Model inseparability ensures that modules can be used regardless of the query language
relevant to the application at hand. Depletingness and self-containment have been identified as important properties for ontology reuse and modular ontology development tasks
(Sattler et al., 2009; Jimenez-Ruiz et al., 2008). Finally, the preservation of justifications
enables the use of modules for optimising debugging and explanation services (Schlobach &
Cornet, 2003; Kalyanpur, Parsia, Horridge, & Sirin, 2007), as well as incremental reasoning
(Suntisrivaraporn, 2008; Cuenca Grau et al., 2010).
500

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning

Locality-based module extraction techniques are easy to implement, and surprisingly
eective in practice. Their main drawback is that the extracted modules can be rather
large, which limits their usefulness in some applications (Del Vescovo, Klinov, Parsia, Sattler, Schneider, & Tsarkov, 2013). One way to address this issue is to develop techniques
that approximate minimal modules more closely, while still fulfilling properties (P1)(P4).
Eorts in this direction have confirmed that locality-based modules can be far from optimal
in practice (Gatens et al., 2014); however, these techniques apply only to rather restricted
ontology languages and utilise algorithms with high worst-case complexity.
Another approach to computing smaller modules is to weaken properties (P1)(P4),
which are stronger than many applications require. In particular, model inseparability is a
very strong condition, and deductive inseparability w.r.t. a query language suitable for the
application at hand would usually suffice.
In this paper, we propose a novel approach that reduces module extraction to a reasoning
problem in the basic rule-based language datalog (Abiteboul, Hull, & Vianu, 1995; Dantsin,
Eiter, Gottlob, & Voronkov, 2001). The connection between module extraction and datalog
was first observed by Suntisrivaraporn (2008), who showed that locality ?-module extraction
for EL ontologies could be reduced to propositional datalog reasoning. Our approach takes
this connection much farther, and generalises locality-based modules in an elegant way. The
key distinguishing features of our approach are as follows:
 It is applicable not only to ontology languages based on description logics, but also
to expressive rule-based knowledge representation formalisms that extend datalog
with existential quantification and disjunction in the head of rules (Cal et al., 2010;
Bourhis, Morak, & Pieris, 2013; Alviano, Faber, Leone, & Manna, 2012).
 It is sensitive to the dierent inseparability relations proposed in the literature; in
particular, we can extract deductively inseparable modules with the query language
tailored to the specific requirements of the application at hand. This allows us to
relax property (P1) and extract significantly smaller modules.
 In all cases, our modules are depleting and capture all justifications of relevant entailments; moreover, our approach can be adapted to either ensure or dispense with
self-containment, depending on the application needs.
 It not only ensures tractability of module extraction for DL-based ontology languages,
but also enables the use of highly scalable o-the-shelf datalog reasoners.
We have implemented our approach using the RDFox datalog reasoner (Motik, Nenov,
Piro, Horrocks, & Olteanu, 2014). Our evaluation over complex, real-world, ontologies shows
that module size consistently decreases as we consider weaker inseparability relations, which
could significantly improve the usefulness of modules in applications.

2. Preliminaries
In Section 2.1 we introduce the language of first-order rules, which is powerful enough
to fully capture expressive rule-based ontology languages such as datalog (Cal et al.,
2010), and datalog,_ (Bourhis et al., 2013; Alviano et al., 2012), as well as all mainstream
501

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

description logics (Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2003). Most
of our results in this paper hold for arbitrary knowledge bases consisting of such first-order
rules, and hence are applicable to a very wide range of knowledge representation formalisms.
In Section 2.2 we introduce the syntax and first-order semantics of the description logic
SROIQ (Horrocks, Kutz, & Sattler, 2006), which underpins the W3C standard ontology
language OWL 2 (Motik, Patel-Schneider, & Parsia, 2012; Cuenca Grau, Horrocks, Motik,
Parsia, Patel-Schneider, & Sattler, 2008). We then introduce a normal form for SROIQ
and establish its correspondence to first-order rules. Finally, in Section 2.3 we briefly recall
the well-known hyperresolution calculus for first-order logic (Bachmair & Ganzinger, 2001),
which we exploit in many of our technical results to show that a module preserves the
required consequences of the given ontology.
Throughout this paper, we assume basic familiarity with first-order logic and we use
standard first-order logic notions, such as predicates, constants, variables, terms, atoms,
formulas, sentences, interpretations and entailment (written |=). We define a signature as
a set of predicates; furthermore, given a first-order sentence , we use Sig( ) to denote the
signature of . We then say that is a -sentence if Sig( )  . Analogously, we denote
with Ct( ) the set of constants in . These definitions extend naturally to sets of sentences;
indeed, later on in the paper we will speak of -rules, -datasets, and -ontologies with the
obvious meaning. The restriction of signatures to contain just predicates and the separate
treatment of constants will be convenient for working with inseparability relations later on.
A set of function-free sentences F 0 is a (model) conservative extension of a set F if for
each model I of F there is a model J of F 0 with the same domain as I and such that
AI = AJ for each A 2 Sig(F) and aI = aJ for each a 2 Ct(F).

We deviate slightly from the standard definition of first-order logic in that our definition
does not include the nullary symbols > and ?, which are interpreted as true and false respectively in every first-order interpretation. Similarly, we consider first-order logic without
equality  and hence we do not assume that  is interpreted as the identity relation over
the domain in every interpretation. Instead, we treat ?, > and  as ordinary predicates,
the meaning of which we axiomatise explicitly in every knowledge base. We assume that ?
is nullary, > is unary and  is binary. Given a set F of function-free sentences, we then
define the following sets of sentences F ? , F > , and F  .
 F ? is empty if F contains no occurrences of ?, and the singleton set {?} otherwise.
 F > is empty if F contains no occurrence of >; otherwise, it is the set
{ 8x1 , . . . , xn [A(x1 , . . . , xn ) ! >(xi )] | A 2 Sig(F) n-ary, 1  i  n }
 F  is empty if F contains no occurrences of ; otherwise, it consists of the sentences
(EQ1)(EQ5) given next. Sentence (EQ1) is instantiated for each constant a 2 Ct(F);
furthermore, sentences (EQ2) and (EQ5) are instantiated for each n-ary predicate A
502

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning

in Sig(F) and each xi in x = x1 , . . . , xn :
!aa

(EQ1)

8x [A(x) ! xi  xi ]

(EQ2)

8x, y [x  y ! y  x]

8x, y, z [x  y ^ y  z ! x  z]

8x, y [A(x) ^ xi  y ! A(x1 , . . . , xi

(EQ3)
(EQ4)
1 , y, xi+1 , . . . , xn )]

(EQ5)

We consider substitutions as functional mappings between two sets of terms. Given a
substitution and a term t not in the domain of , in an abuse of notation the expression t denotes t. Substitutions can be applied to formulas: given an atom A(t1 , ..., tn ),
A(t1 , ..., tn ) = A(t1 , ..., tn ), and given a non-atomic formula ,
is the result of applying to all atoms in . This application is extended to sets of formulas in the natural
way. Given two substitutions and  , their composition is the substitution  such that
t(  ) = (t ) for each t in the domain of . We say that and  are compatible if they
coincide over the intersection of their domains. If and  are compatible, their union is the
substitution [  such that t( [  ) = t for each t in the domain of and t( [  ) = t
for each t in the domain of  . Finally, we use dom( ) (resp. range( )) to denote the domain
(resp. the range) of .
2.1 Rule-Based First-Order Languages
Rule-based languages are prominent knowledge representation formalisms closely related to
ontology languages (Dantsin et al., 2001; Bry, Eisinger, Eiter, Furche, Gottlob, Ley, Linse,
Pichler, & Wei, 2007; Cal et al., 2010). In this paper, we focus on monotonic formalisms
and hence on rule languages that can be seen as fragments of first-order logic. We next
define a general notion of first-order rule which underpins the datalog and datalog,_
families of languages (Cal et al., 2010; Alviano et al., 2012).
A fact is a function-free ground atom. A finite set of facts is called a dataset. A rule r
is a function-free first-order sentence of the form
8x['(x) ! 9y (x, y)]

(1)

where x and y are disjoint vectors of variables, ' is a (possibly empty) conjunction of distinct
atoms over constants and variables from x; and is built from atoms over constants and
variables from x [ y using conjunction (^) and disjunction (_). Note that any fact is also
a rule. Formula ' is the rule body and 9y (x, y) is the rule head. The head of a rule can
be empty, in which case we represent it as . Universal quantifiers in rules are omitted
for brevity. Rules are required to be safe, that is, all universally quantified variables in the
head must occur in the body. A rule is datalog if its head is either empty or it consists
of a single atom where all variables are universally quantified. Note that, for any set F of
function-free sentences, the set F ? [ F > [ F  contains only datalog rules.
A (first-order) ontology O is a finite set of rules satisfying O? [ O> [ O  O. We
assume w.l.o.g. that dierent rules in O do not share existentially quantified variables and
that the only rule with empty head is ? ! .
503

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

roles
(R, x, y)
(R , x, y)

=
=

R(x, y)
R(y, x)

(?c , x)
(>c , x)
(o, x)
(C, x)
(C1 u C2 , x)
(C1 t C2 , x)
(9R.C, x)
(8R.C, x)
(9R.Self, x)
( nR.C, x)
( nR.C, x)
axioms
(C1 v C2 )
(R1    Rm v S)
(Disj(R1 , R2 ))
(Ref(R))

=
=
=
=
=
=
=
=
=
=
=

?
>(x)
xo
(C, x)
(C1 , x) ^ (C2 , x)
(C1 , x) _ (C2 , x)
9y[(R, x, y) ^ (C, y)]
8y[(R, x, y) ! (C, y)]
(R, x, x) V
V
9x1 , . . . , xn [ i ((R, x, xi ) ^ (C, xi )) ^ i6=j (xi  xj )]
V
W
8x1 , . . . , xn+1 [ i ((R, x, xi ) ^ (C, xi )) ! i6=j xi  xj ]

concepts

=
=
=
=

8x[(C1 , x) ! V
(C2 , x)]
m
8x1 , . . . , xm+1 [ i=1 (Ri , xi , xi+1 ) ! (S, x1 , xm+1 )]
8x, y[(R1 , x, y) ^ (R2 , x, y) ! ?]
8x[>(x) ! (R, x, x)]

Figure 1: Semantics of SROIQ via translation into first-order logic.
A datalog program is an ontology containing only datalog rules. Given a datalog program
P and a dataset D, their materialisation, denoted with P(D), is the set of facts entailed
by P [ D. Such materialisation can be computed in time polynomial in the size of D using
forward chaining (Abiteboul et al., 1995; Dantsin et al., 2001).
To conclude this section, we define the languages typically used for querying first-order
ontologies. We define a Boolean positive existential query (Boolean PEQ) as a non-empty
sentence q built from function-free atoms using only 9, ^ and _; such a query holds w.r.t.
an ontology O if O |= q. A Boolean PEQ is a conjunctive query (CQ) if it is disjunctionfree. The following proposition, the proof of which is straightforward, establishes a useful
connection between Boolean PEQ evaluation and entailment of first-order rules.
V
Proposition 1. Let O be an ontology, r = ni=1 i (x) ! 9y (x, y) a rule, and let be
a substitution mapping all universally quantified variables in r to fresh distinct constants.
Then, O |= r i O [ { i }ni=1 |= 9y .
2.2 Description Logics
Description logics (DLs) (Baader et al., 2003) are a family of knowledge representation
formalisms that correspond to decidable fragments of first-order logic. DLs are the logical formalisms underpinning the standard ontology languages: OWL DL is based on the
description logic SHOIN (Horrocks, Patel-Schneider, & van Harmelen, 2003), whereas
its revision OWL 2 is based on the more expressive logic SROIQ (Horrocks et al., 2006;
Cuenca Grau et al., 2008; W3C OWL Working Group, 2012).
504

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning

The basic building blocks in SROIQ are pairwise disjoint countable sets of atomic
concepts, which correspond to unary predicates, atomic roles, which correspond to binary
predicates, and individuals, which correspond to constants. A role R is either an atomic
role or the inverse S of an atomic role S. Complex concepts are constructed according to
the following grammar, where ?c and >c are the special bottom and top concepts, A is an
atomic concept, R is a role, o is an individual and n 1:
C ::= ?c | >c | A | {o} | C | C1 u C2 | C1 t C2 |
9R.C | 8R.C | 9R.Self |

nR.C | nR.C

We assume that concept expressions of the form 1R.C are replaced by their equivalent
9R.C. A general concept inclusion axiom (GCI) is an expression of the form C1 v C2 ,
where C1 and C2 are concepts. A role inclusion axiom (RIA) is an expression of the form
R1    Rm v R where each Ri is a role and R is an atomic role. A role disjointness axiom
is an expression of the form Disj(R1 , R2 ) with R1 and R2 roles. Finally, a reflexivity axiom
is an expression of the form Ref(R) with R a role.
A SROIQ ontology is a finite set of GCIs, RIAs, role disjointness and reflexivity axioms. In order to ensure the decidability of basic reasoning tasks, each SROIQ ontology
must satisfy certain additional conditions (e.g., the set of RIAs must satisfy a regularity
condition); these conditions are, however, immaterial to the results in this paper and we
refer the reader to the work of Horrocks et al. (2006) for further details.
The semantics of SROIQ can be given by a direct translation into first-order logic
(Baader et al., 2003; Motik, 2006) using the mapping function  in Figure 1. Given a
SROIQ ontology O, let FO = { () |  2 O }; we then define
?
>

(O) = FO [ FO
[ FO
[ FO

A first-order interpretation is a model of O if it is a model of (O).
Note that (O) is not always a set of first-order rules as defined in Section 2.1. However,
O can always be polynomially normalised into an entailment preserving SROIQ ontology
O0 such that (O0 ) is a set of rules. We next define normalised SROIQ ontologies and
assume from here onwards (unless otherwise stated) that SROIQ ontologies are normalised.
Definition 2. A SROIQ ontology O is normalised if it consists only of axioms of the form
A v ?c A v {o} >c v A {o} v A A v B1 t B2 A1 u A2 v B
A v 9R.B A v 9R.Self 9R.A v B 9R.Self v A A v nR.B
R1 R2 v S R v S Disj(R, S) Ref(R)
with A(i) , B(i) atomic concepts, o an individual, R(i) , S atomic roles, and n

1.



Table 1 shows the application of  to normalised axioms. Clearly, (O) is a set of rules
whenever O is normalised. Moreover,  establishes a bijection between O and (O) in this
case. Since O and (O) are semantically equivalent, it is thus natural to identify them, and
we shall do so in the remainder of this paper.
505

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks


A v ?c
A v {o}
>c v A
{o} v A
A v B1 t B2
A1 u A2 v B
A v 9R.B
A v 9R.Self
9R.A v B
9R.Self v A
A v nR.B
R1 R2 v S
R vS
Disj(R, S)
Ref(R)

()
A(x) ! ?
A(x) ! x  o
>(x) ! A(x)
( ! A(o))
A(x) ! B1 (x) _ B2 (x)
A1 (x) ^ A2 (x) ! B(x)
A(x) ! 9y[R(x, y) ^ B(y)]
A(x) ! R(x, x)
R(x, y) ^ A(y) ! B(x)
R(x, x) ! A(x)
V
W
A(x) ^ n+1
i=1 [R(x, yi ) ^ B(yi )] !
i6=j yi  yj
R1 (x, y) ^ R2 (y, z) ! S(x, z)
R(x, y) ! S(y, x)
R(x, y) ^ S(x, y) ! ?
>(x) ! R(x, x)

Table 1: Correspondence between normalised SROIQ axioms and rules.
Proposition 3. Let O be a SROIQ ontology and let O0 be the result of exhaustively
applying to O the rewriting rules in Figure 2. Then, O0 satisfies the following properties:
(i) it is normalised; (ii) it is of size polynomial in the size of O (assuming unary encoding
of numbers); and (iii) it is a conservative extension of O.
Proof. It is easy to see that a rewrite rule is always applicable to every axiom that is not
normalised; furthermore, no rule is applicable to normalised axioms. Thus, O0 is normalised.
Furthermore, note that the rules in Figure 2 are a syntactic variant of the structural transformation in first-order logic (Nonnengart & Weidenbach, 2001). This implies that O0 can
be computed in time polynomial in the size of O (assuming unary encoding of numbers),
and also that it is a conservative extension of O.
2.3 Hyperresolution and Proofs
Reasoning w.r.t. ontologies can be realised by means of hyperresolution (Robinson, 1965;
Bachmair & Ganzinger, 2001), which generalises forward chaining for datalog.
HyperresolutionVis applicable
to sets of first-order clausesuniversally quantified senW
tences of the form i i ! j j with i and j atoms (possibly containing function symbols). Thus, it is only applicable to ontologies containing existentially quantified rules after
Skolemisation and subsequent transformation into Conjunctive Normal Form (CNF).
For each rule r of the form (1) and each existentially quantified variable y in r, let fyr
be a function symbol globally unique for r and y of arity |x|, and let sk be the substitution
such that sk (y) = fyr (x) for each r and y. The Skolemisation of r is the sentence
sk(r) = '(x) ! (x, y)sk
506

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning

R1

?c v C
D v {o}
9S .D v C
9S.D v C
mS .D v C
mS.D v C
D u C1 v C2
C1 v C2
C1 t C2 v C3
8S .C1 v C2
8S.C1 v C2
9S .Self v C
(m 1)S .C1 v C2
(m 1)S.C1 v C2
C v >c
{o} v D
C v 9S .D
C v 9S.D
C v 8S .D
C v 8S.D
C v 9S .Self
C v nS .D
C v nS.D
C1 v D t C2
C1 v C2
C1 v C2 u C3
C1 v mS .C2
C1 v mS.C2
D1 v D2
R2 R 3    Rk v S
Q
RvS
R Q vS
Disj(R, Q )
Disj(Q , R)
Ref(Q )

)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)

X v {o}, D v X
9P.X v D, S v P
9S.X v C, D v X
mP.D v C, S v P
>c v (m 1)S.D t C
X u C1 v C2 , D v X
> c v C1 t C2
C1 v C3 , C2 v C3
8P.C1 v C2 , P v S
>c v 9S.X t C2 , X u C v ?c
9P.Self v C, S v P
(m 1)P.C1 v C2 , P v S
>c v mS.C1 t C2
{o} v X, X v D
C v 9P.D, P v S
C v 9S.X, X v D
C v 8P.D, S v P
C v 8S.X, X v D
C v 9P.Self, P v S
C v nP.D, S v P
C v nS.X, X v D
C1 v X t C2 , X v D
C 1 u C 2 v ?c
C1 v C2 , C1 v C3
C1 v mP.C2 , P v S
C1 v 9S.Xi , Xi v C2 , Xi u Xj v ?c (1i<jm)
D1 v X, X v D2
R1 R2 v P, P R3    Rk v S
P R v S, Q v P
R P v S, Q v P
Disj(R, P ), Q v P
Disj(P, R), Q v P
Ref(Q)

Figure 2: Normalisation of SROIQ axioms, where C(i) are concepts, D(i) are non-atomic
concepts dierent from ?c and >c , X is a fresh atomic concept, Q and S are
atomic roles, R(i) are roles, P is a fresh atomic role, and m 2, n 1, k 3.

A CNF of sk(r) is a set of first-order clauses that is a conservative extension of sk(r). Such
a CNF can be obtained in polynomial time using the standard structural transformation
(Nonnengart & Weidenbach, 2001). In this paper we consider an arbitrary but fixed function
507

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

 mapping each rule r to some CNF of sk(r). The function  extends to ontologies in the
obvious way, and we refer to (O) as a clausification of O. By the well-known properties of
Skolemisation and the structural transformation we have that O |= i (O) |= for each
ontology O and each first-order sentence over Sig(O).
V
W
Let r = ni=1 i ! m
j=1 j be a clause and let 'i = i _ i with 1  i  n be ground
disjunctions of atoms where i is a single atom; furthermore,
letW be a most general unifier
W
(MGU) of each i , i . The ground disjunction of atoms ni=1 i _ m
j=1 j is a hyperresolvent
of r and '1 , . . . , 'n . This disjunction can be empty, in which case we denote it with . Let
C be a set of clauses, D a dataset and ' a disjunction of ground atoms. A hyperresolution
proof (or simply a proof ) of ' in C [ D is a pair  = (T, ) where T is a directed, rooted
tree, and is a mapping from nodes in T to disjunctions of ground atoms such that for
each node v in T the following properties are satisfied:
1. if v is the root of T then (v) = ',
2. if v is a leaf in T then either ( ! (v)) 2 C or (v) 2 D, and
3. if v has children w1 , . . . , wn then
(w1 ), . . . , (wn ).

(v) is a hyperresolvent of a clause from C and

The support of , denoted by supp(), is the set of clauses in C that take part in  as
described in properties 2 and 3 above. We write C [ D ` ' to indicate that there exists a
proof of ' in C [ D. Hyperresolution is sound (if C [ D ` ' then C [ D |= '), and complete
in the following sense: if C [ D |= ' then there exists  ' such that C [ D ` (Robinson,
1965). In particular, C is unsatisfiable i C [ D ` .
Given proofs  = (T, ) and 0 = (T 0 , 0 ), we say that  is embeddable into 0 if there
exists a mapping  : T ! T 0 satisfying the following properties for each v 2 T : (i) if v is a
leaf of T , then (v) is a leaf of T 0 ; (ii) if w is an ancestor of v in T then (w) is an ancestor
of (v) in T 0 , and (iii) (v)  0 ((v)) [ {?}. Furthermore, given a substitution  , we say
that  is embeddable into 0 modulo  if it is embeddable into the proof (T 0 , 0 ), where
0 (v) = 0 (v) for each v 2 T 0 .


3. Module Extraction
In this section, we recapitulate the key notions of inseparability relation and module that
have been proposed in the description logic literature (Cuenca Grau et al., 2008; Kontchakov
et al., 2010; Konev et al., 2013; Sattler et al., 2009; Konev et al., 2009). Furthermore, when
required, we adapt these notions to the setting of first-order rules and prove some basic
results that will be exploited throughout this paper.
3.1 Inseparability Relations and Modules
Intuitively, given an ontology O and a signature , a module of O w.r.t.  is a subset M
of O that is indistinguishable from O w.r.t. reasoning tasks where only predicates in  are
considered of interest. The indistinguishability criteria depend on the specific task at hand,
and are usually formalised by means of inseparability relations.

508

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning

Definition 4. An inseparability relation is a family S = { S |  a set of predicates } of
equivalence relations between ontologies satisfying the following properties:
 if O0 is a conservative extension of O and  = Sig(O), then O S O0 ; and
 O1 S O implies O2 S O for all O1  O2  O and .



The first property ensures that inseparability is stable under model-preserving transformations, whereas the second one ensures that it is consistent with the monotonicity of
first-order logic. The following definition captures the most common inseparability relations
studied in the literature.
Definition 5. For a signature , we say that ontologies O and O0 are
0
0
 -model inseparable (O m
 O ), if for every model I of O (resp. of O ) there exists a
0
I
J
model J of O (resp. of O) with the same domain such that A = A for each A 2 .

 -query inseparable (O q O0 ) if for each Boolean PEQ q over  and dataset D over
 we have O [ D |= q i O0 [ D |= q.
 -fact inseparable (O f O0 ) if for each fact
O [ D |= i O0 [ D |= .

over  and dataset D over  we have

 -implication inseparable (O i O0 ) if for each -rule r of the form A(x) ! B(x) we
have O |= r i O0 |= r.

A few observations about the notions introduced in Definition 5 are in order. First,
note that in the definition of query and fact inseparability the quantification over queries
and datasets is relative to the same signature ; this is the standard convention adopted in
the literature (Lutz & Wolter, 2010; Baader, Bienvenu, Lutz, & Wolter, 2010). Second, the
restriction to Boolean queries in the definition of query inseparability is strictly technical:
the obvious extension to non-Boolean queries leads to an equivalent definition. Finally,
observe that our notion of fact inseparability is the natural generalisation of inseparability
w.r.t. atomic instance queries in description logics (Lutz & Wolter, 2010).
Example 6. Let us consider the ontology Oex from Figure 3, which will serve as a running
example. Let us also consider the signatures i and fragments Mi of Oex given next:
1 = {B, C, D, H}

M1 = {r5 , r6 , r7 , r8 }

3 = {A, C, D, R}

M3 = {r1 , r2 }

2 = {A, B}

M2 = ;

The only non-tautological 1 -implication entailed by Oex is D(x) ! H(x), which also
follows from M1 ; thus, M1 is 1 -implication inseparable from Oex . Furthermore, any subset
of Oex not containing M1 does not entail D(x) ! H(x) and is hence not 1 -implication
inseparable from Oex . As we will see later on, the requirement of fact inseparability is
stronger than that of implication inseparability; indeed, M1 is not 1 -fact inseparable from
Oex since, for D1 = {B(a), C(a)}, we have Oex [ D1 |= D(a) but M1 [ D1 6|= D(a).
509

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

r1
r2
r3
r4
r5
r6
r7
r8

:
:
:
:
:
:
:
:

A(x) ! 9y1 [R(x, y1 ) ^ B(y1 )]
A v 9R.B
A(x) ! R(x, o)
A v 9R.{o}
B(x) ^ C(x) ! D(x)
BuC vD
R(x, y) ^ C(y) ! E(x)
9R.C v E
D(x) ! F (x) _ G(x)
DvF tG
F (x) ! 9y2 S(x, y2 )
F v 9S.>c
S(x, y) ! H(x)
9S.>c v H
G(x) ! H(x)
GvH

Figure 3: Example ontology Oex in both rule and DL notation.
It can be checked that M2 is 2 -fact inseparable from Oex . It is, however, not 2 -query
inseparable: for D2 = {A(a)}, we have Oex [ D2 |= 9yB(y) but M2 [ D2 6|= 9yB(y).
Finally, consider M3 and 3 . As we will see later on, M3 is 3 -query inseparable
from Oex ; however, it is not 3 -model inseparable. Indeed, the interpretation I where
I = {a, o}, AI = {a}, B I = C I = {o}, D I = ; and RI = {(a, o)} is a model of M .
3
This interpretation, however, cannot be extended to a model of r3 (or, consequently, to a
model of O) without reinterpreting A, C, D or R. We will also see that to ensure 3 -model
inseparability it suffices to extend M3 with rule r3 .

Model inseparability can be characterised in terms of preservation of second-order consequences (Konev et al., 2013): ontologies O and O0 are -model inseparable if and only
if for each second-order -sentence ' we have O |= ' i O0 |= '. Additionally, as we
show next, query and fact inseparability can be characterised in terms of preservation of
first-order rules and datalog rules, respectively.
Proposition 7. The following statements hold for each signature  and each pair of
ontologies O1 and O2 :
1. O1 q O2 i O1 |= r , O2 |= r holds for each -rule r with non-empty head.

2. O1 f O2 i O1 |= r , O2 |= r holds for each datalog -rule r with non-empty head.

q
Proof. We prove the first statement;
Vnthe second one is analogous. Suppose O1  O2
and consider an arbitrary rule r = i=1 i (x) ! 9y (x, y) over  such that 6= , and
a substitution mapping universally quantified variables in r to fresh distinct constants.
Furthermore, consider the dataset D = { i }ni=1 , and the Boolean PEQ q = 9y (x, y)
(note that q is indeed a Boolean PEQ since by hypothesis is non-empty). By Proposition 1,
Oi |= r i Oi [ D |= q. Together with O1 q O2 , this implies that O1 |= r , O2 |= r.
Assume now that O1 |= r , O2 |= r holds for each -rule r with a non-empty
head. Let
V
q be a Boolean PEQ over  and D a -dataset and consider the rule r =
2D ! q. By
Proposition 1 we have Oi [D |= q i Oi |= r, and since, by assumption, O1 |= r , O2 |= r, it
follows that O1 [ D |= q , O2 [ D |= q and hence O1 and O2 are -query inseparable.

It immediately follows that the inseparability relations in Definition 5 are naturally
ordered from strongest to weakest for each non-trivial  as given next:
q
f
i
m
 (  (  ( 

510

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning

Furthermore, we can now identify the classes of entailments that are relevant to each inseparability relation.
Definition 8. For each inseparability relation S 2 {m, q, f, i}, let relS be the function
mapping each ontology O and signature  to a set of relevant entailments as follows:
8
{ | O |= and is a second-order -sentence }
if S = m
>
>
<
{ r | O |= r and r is a -rule with non-empty head }
if S = q
relS (O, ) =
{
r
|
O
|=
r
and
r
is
a
datalog
-rule
with
non-empty
head
}
if S = f
>
>
:
{ r | O |= r and r is of the form A(x) ! B(x) with A, B 2  } if S = i



The following theorem establishes that the inseparability relations in Definition 5 are
fully characterised by the preservation of relevant -entailments as in Definition 8.
Theorem 9. Let O and O0 be ontologies,  a signature, and let S 2 {m, q, f, i}. Then,
O S O0 if and only if relS (O, ) = relS (O0 , ).
Proof. A direct consequence of Definitions 5 and 8, Proposition 7 and the characterisation
of model inseparability in terms of second-order entailments in (Konev et al., 2013).
Inseparability relations allow us to formalise modules as well as their desirable properties.
Definition 10. Let O be an ontology,  a signature, and S an inseparability relation, and
let M  O. We say that M is a S -module of O if O S M. Furthermore, M is
 minimal if no M0 ( M is a S -module of O;
 self-contained if O S[Sig(M) M;
 depleting if O \ M S ;; and strongly depleting if O \ M S[Sig(M) ;.
Finally, we define a justification in O of a sentence such that O |= as a subset-minimal
O0  O such that O0 |= . We say that M is justification-preserving if for each
in
relS (O, ) and each justification O0 of in O we have O0  M.

Example 11. Consider again the ontologies and signatures in Example 6. We can see that
each Mi is a module of Oex ; in particular, M1 is a i1 -module, M2 is a f2 -module , M3
is a q3 -module, and M3 [ {r3 } is a m

3 -module.
The inseparability requirement ensures that modules can be used instead of O for reasoning purposes, provided that the entailments relevant to the application at hand are
captured by the given inseparability relation and contain only symbols from .
Minimality ensures that the module contains as little irrelevant information as possible
while still satisfying the inseparability requirement. Although minimality is clearly desirable
in most applications of modules (e.g., reasoning over small ontology subsets is typically
preferable to reasoning over the whole ontology), extracting modules of minimal size is
invariably very hard (and often algorithmically infeasible) (Lutz & Wolter, 2010; Konev
511

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

et al., 2013); thus, practical techniques aim at computing modules that are typically much
smaller than O, albeit not necessarily minimal.
Self-contained modules are inseparable from O not only w.r.t. the relevant signature
, but also w.r.t. their own signature. Depletingness ensures that no relevant information
is left behind after extracting the module from O, i.e., that O \ M is inseparable from
the empty ontology. The most basic form of depletingness is formulated in terms of ,
whereas a stronger variant requires inseparability w.r.t. the symbols in M as well. Selfcontained and depleting modules are especially well-suited for ontology reuse and modular
ontology development applications. For instance, if M is both self-contained and depleting,
the developer of O can remodel the sub-domain characterised by  by replacing M in O
with a new set of axioms, with the guarantee that any changes performed will not have any
unintended interactions with the rest of O.
Justification-preservation enables the use of modules for ontology debugging and repair
(Schlobach & Cornet, 2003; Kalyanpur et al., 2007; Kalyanpur, Parsia, Sirin, & Hendler,
2005; Horridge, Parsia, & Sattler, 2008; Kalyanpur, Parsia, Sirin, & Cuenca Grau, 2006).
The justification of an entailment is a useful form of explanation; furthermore, ontology
repair services typically rely on the computation of all justifications of an unintended entailment as a first step towards obtaining a repair plan. Computing justifications, however,
is a computationally intensive task and practical module extraction techniques have been
eectively exploited to optimise this process (Suntisrivaraporn et al., 2008).
We conclude this section by briefly discussing the impact of normalisation on module
extraction. As pointed out in Section 2.2, our technical results are applicable to ontologies
consisting of rules; when referring to DL ontologies, we implicitly assume they are given in
rule form and are therefore normalised. We argue that normalisation techniques stemming
from the structural transformation preserve inseparability and hence it is possible to obtain
a module for a DL ontology once a module for its normalisation has been computed.
Definition 12. A normalisation function norm maps SROIQ ontologies to normalised
SROIQ ontologies s.t. the following holds for all ontologies in its domain:
 norm(O) is a conservative extension of O; and
 O1  O2 implies norm(O1 )  norm(O2 ).



Definition 12 captures the standard normalisation techniques stemming from the structural transformation, such as the one discussed in Section 2.2. Furthermore, it is typically
straightforward in practice to keep track of the correspondence between the axioms in the
original ontology O and those in norm(O). As shown by the following proposition, this
correspondence allows us to efficiently obtain a module of O once a module for norm(O)
has been computed.
Proposition 13. Let  be a signature, S an inseparability relation, and norm a normalisation function. Then, M S O i norm(M) S norm(O).
Proof. By definition, S satisfies that O1  O2  O and O1 S O implies O2 S O.
Therefore, if norm(M) contains a S -module of norm(O) then norm(M) is a S -module
of norm(O) itself. On the other hand, since norm(O) (resp. norm(M)) is a conservative
512

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning


A v ?c
A v {o}
>c v A
{o} v A
A v B1 t B2
A1 u A2 v B
A v 9R.B
A v 9R.Self
9R.A v B
9R.Self v A
A v mR.B
R 1 R2 v S
RvS
Disj(R, S)
Ref(R)

?-local w.r.t. 
if A 2
/
if A 2
/
never
never
if A 2
/
if A1 2
/  or A2 2
/
if A 2
/
if A 2
/
if R 2
/  or A 2
/
if R 2
/
if A 2
/  or R 2
/  or B 2
/
if R1 2
/  or R2 2
/
if R 2
/
if R 2
/  or S 2
/
never

>-local w.r.t. 
never
never
if A 2
/
if A 2
/
if B1 2
/  or B2 2
/
if B 2
/
if {R, B} \  = ;
if R 2
/
if B 2
/
if B 2
/
never
if S 2
/
if S 2
/
never
if R 2
/

Table 2: Syntactic locality for normalised SROIQ axioms
extension of O (resp. of M), and we can asume w.l.o.g. that  contains no symbols from
Sig(norm(O)) \ Sig(O), we have that norm(O) S O (resp. norm(M) S M). Since S is
an equivalence relation, it follows that M S O i norm(M) S norm(O).
3.2 Syntactic Locality
For many of the inseparability relations introduced in Section 3.1, checking whether M is
a module for O w.r.t.  is typically of very high complexity, and often undecidable, even
for rather lightweight ontology languages (Lutz & Wolter, 2010; Konev et al., 2013).
Consequently, practical module extraction techniques are typically based on approximations, which ensure that the computed M is a module, yet not necessarily a minimal
one. One such approximation that is often exploited in practice is based on the notion of
syntactic locality (Cuenca Grau et al., 2007a; Cuenca Grau, Horrocks, Kazakov, & Sattler,
2007b; Sattler et al., 2009; Cuenca Grau et al., 2008).
Intuitively, a normalised SROIQ axiom is ?-local (resp. >-local) if treating all atomic
concepts and roles outside  as the ? (resp. >) concept and role, respectively, leads to the
axiom being an obvious tautology.
Definition 14. A normalised SROIQ axiom  is ?-local (resp. >-local) w.r.t. a signature
 if it satisfies the conditions given in the second (resp. third) column in Table 2. A
normalised SROIQ ontology O is ?-local (resp. >-local) w.r.t.  if each of its axioms is
?-local (resp. >-local) w.r.t. . Finally, we say that O is local w.r.t.  if it is either ?-local
or >-local w.r.t. .

The key properties of ?- and >-locality, established in the existing literature (Cuenca
Grau et al., 2008; Sattler et al., 2009), are summarised in the following proposition.
513

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

Proposition 15. Let O be a SROIQ ontology,  a signature and x 2 {?, >}.
1. If O is x-local w.r.t.  then O m
 ;.
2. If M  O and O \M is x-local w.r.t. [Sig(M), then M is a self-contained, strongly
depleting, and justification-preserving m
 -module of O.
Property 2 in Proposition 15 immediately suggests the notion of locality-based module.
Definition 16. Let O be a normalised SROIQ ontology,  a signature, and x 2 {?, >}.
The x-module for O w.r.t. , denoted Mx[O,] , is the smallest subset M  O such that
O \ M is x-local w.r.t.  [ Sig(M).
The ?> -module for O w.r.t.  is the least fixpoint of the sequence {Mi }i 1 where
M1 = M?
2:
[O,] and Mi is defined as follows for i
Mi =

(

M>
[Mi
M?
[Mi

1 ,]
1 ,]

if i is odd
if i is even


Example 17. Consider ontology Oex from Figure 3 and the signature  = {B, C, D, R}.
>
?>
We have that M?

[O ex ,] = {r3 r8 }, M[O ex ,] = {r1 r3 }, and M[O ex ,] = {r3 }.
Locality-based modules as in Definition 16 can be computed in polynomial time. Furthermore, by Proposition 15, they are self-contained, strongly depleting and justificationpreserving. They are, however, generally not minimal, even amongst strongly depleting and
self-contained modules.

4. Overview
We now provide a high-level overview of our approach to module extraction, which is based
on a novel reduction to a reasoning problem in datalog. Our approach builds on recent
techniques that exploit datalog engines for ontology reasoning (Kontchakov, Lutz, Toman,
Wolter, & Zakharyaschev, 2011; Stefanoni, Motik, & Horrocks, 2013; Zhou, Nenov, Cuenca
Grau, & Horrocks, 2014; Zhou, Cuenca Grau, Nenov, Kaminski, & Horrocks, 2015). The
connection between module extraction and datalog was first observed in (Suntisrivaraporn,
2008), where it was shown that ?-module extraction for the lightweight DL EL+ can be
reduced to propositional datalog reasoning.
Our approach takes this connection much farther by providing a unified framework that
supports module extraction for arbitrary ontologies consisting of first-order rules, as well as
for a wide range of inseparability relations. Modules obtained using our approach can be
tailored to the requirements of the application at hand. In addition to being significantly
smaller in practice, our modules preserve the features of syntactic locality modules: they
are widely applicable, they can be efficiently computed in practice, and they satisfy a wide
range of additional properties.
In what follows, we fix w.l.o.g. an arbitrary ontology O and a signature   Sig(O).
Unless otherwise stated, our definitions and theorems are parameterised by such O and .
As stated in Section 2, we assume that rules in O do not share variables.
514

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning

Our overall strategy to extract a module M of O can be roughly summarised by the
following steps:1
1. Choose a substitution  mapping all existentially quantified variables in O to fresh
Skolem constants, and obtain a datalog program P from O by
(a) Skolemising all rules in O using  to obtain function-free rules ' !
may contain both ^ and _ in the head; and

, which

(b) replacing each of the resulting rules ' ! with the set { ' ! | an atom in }
of datalog rules; in this way, disjunctions in the head of rules are turned into conjunctions and split into dierent datalog rules.
Clearly, such a program P logically entails O and thus preserves all its consequences.
2. Choose a -dataset D0 of initial facts and compute the materialisation of P [ D0 .
3. Choose a set Dr of relevant facts in the materialisation (possibly containing symbols
outside ), and compute the supporting rules P 0 in P for each such fact.
4. Output the subset M  O of all rules in O that correspond to some rule in P 0 .
The subset M described above is fully determined by the substitution  and the datasets
D0 and Dr . The main intuition behind our module extraction approach is that we can pick
, D0 and Dr (and hence also M) such that each proof  of a -consequence ' of O to be
preserved under the inseparability relation of interest can be embedded into a collection of
proofs in P [ D0 of a relevant fact from Dr . In this way, we can ensure that M contains all
the necessary rules to entail '.
Example 18. To illustrate how our strategy might work in practice, consider our running
example ontology Oex from Figure 3 and signature  = {B, C, D, H}.
Assume that our goal is to compute a module M that is -implication inseparable from
Oex . Recall from Example 6 that the sentence ' = D(x) ! H(x) is the only non-trivial
-implication entailed by Oex , and therefore the only requirement for M is that M |= '.
Furthermore, note that proving Oex |= ' amounts to proving Oex [ {D(a)} |= H(a) with a
some fresh constant (cf. Proposition 1 in Section 2.1).
Figure 4(a) depicts a hyperresolution proof  showing how H(a) can be derived from
D(a) and the set of clauses corresponding to r5 r8 , where rule r6 is transformed into the
clause r6 = F (x) ! S(x, fyr26 (x)). It follows that M = {r5 r8 } is -implication inseparable
from Oex since it covers the support of . Moreover, M is minimal since H(a) cannot be
derived from any subset of {r5 r8 }.
In our approach, we take D0 and Dr to contain, respectively, the initial fact D(a) and
the fact H(a) to be proved. We also make  map variables y1 and y2 to fresh constants cy1
and cy2 , respectively. The resulting datalog program P is shown in Figure 5.
Figure 4(b) depicts proofs 0 and 00 of H(a) in P [ {D(a)}. The support of proof 00 in
the datalog program consists of rules r500 and r8 , which stem from rules r5 and r8 in Oex ; we
1. For simplicity, in this section we overlook certain technical details such as the presence of constants in
O. These will be thoroughly addressed later on.

515

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

H(a)
r7

H(a)
r7

H(a)
r8

S(a, fyr26 (a)) _ H(a)

S(a, cy2 )

G(a)

r60

r500

F (a) _ H(a)
r8

F (a)

D(a)

F (a) _ G(a)
r5

D(a)

D(a)

0

r6

r50

00



(a)

(b)

Figure 4: Proofs of H(a) from D(a) in (a) Oex and (b) the corresponding datalog program
r10
r2
r3
r4
r50
r60
r7
r8

:
:
:
:
:
:
:
:

A(x) ! R(x, cy1 )
r100 : A(x) ! B(cy1 )
A(x) ! R(x, o)
B(x) ^ C(x) ! D(x)
R(x, y) ^ C(y) ! E(x)
D(x) ! F (x)
r500 : D(x) ! G(x)
F (x) ! S(x, cy2 )
S(x, y) ! H(x)
G(x) ! H(x)

Figure 5: Datalog program obtained from Oex using  = {y1 7! cy1 , y2 7! cy2 }
can see, however, that {r5 , r8 } ( M and hence M does not entail '. The same situation
arises if we were to consider 0 only, in which case we would recover only rules r5 r7 . This
is because the datalog program is a strengthening of Oex and one particular proof in the
datalog program may not translate back into a proof over the original ontology. Indeed, in
order to compute M, we need to consider the supports of both 0 and 00 , in which case we
would successfully recover M.

In this example, our approach would allow us to compute a minimal module. This is,
however, not the case in general: since P is a strengthening of the given ontology there may
be proofs in P [ D0 of facts in Dr that do not correspond to proofs of any -consequence
of the ontology, which may then lead to the inclusion of unnecessary rules in the module.
In the following sections we describe our approach formally.
516

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning

 In Section 5, we define the general notion of a module setting, which captures the
degrees of freedom of our framework and uniquely specifies the datalog program P
and module M corresponding to specific choices of , D0 , and Dr . Furthermore, we
establish the key correspondence between proofs over the original ontology O and sets
of proofs over P [ D0 , which we exploit in many of our subsequent technical results.
 In Section 6, we describe concrete module settings for each of the inseparability relations introduced in Section 3.1, namely implication (Section 6.1), fact (Section 6.2),
query (Section 6.3), and model inseparability (Section 6.4) where we also show that
locality ?-modules can be precisely captured by an instantiation of our framework.
 In Section 7, we consider variants of the inseparability relations in Section 3.1 studied
in the literature, and describe specific module settings for them. These results show
that our framework can be easily adapted to capture new inseparability relations and
hence illustrate the generality and versatility of our approach.
 In Section 8, we show that our modules are consistent with the intuition that stronger
inseparability relations should lead to larger modules. For this, we introduce a notion
of homomorphism between module settings, which will allow us to establish containment relations between the modules specified in Sections 6 and 7.
 In Section 9, we study the additional properties of our modules. We show that they
are depleting and justification-preserving for all the inseparability relations in previous sections. Our modules, however, may not be strongly depleting or self-contained;
although this may be beneficial, as it allows us to extract smaller modules, these properties are still important for ontology reuse scenarios. Hence, we propose a technique
that ensures that extracted modules are also strongly depleting and self-contained.
 In Section 10, we briefly discuss the complexity of module extraction within our
framework and show tractability for DL-based ontology languages.
 Finally, in Section 11, we discuss the optimality of the module settings introduced in
Sections 6 and 7. In particular, although our modules are not minimal in general, we
aim at determining whether the modules obtained from the settings in Sections 6 and 7
are the smallest possible within our framework.

5. The Notion of a Module Setting
In this section we present our framework for module extraction. The key notion is that of
a module setting, which captures in a declarative way the main elements of our approach
discussed in Section 4.
Definition 19. A module setting for O and  is a tuple

= h, D0 , Dr i with

  a substitution mapping each constant and existentially quantified variable in O to
a (possibly fresh) constant;
 D0 a dataset mentioning only predicates from ; and
517

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

 Dr a dataset mentioning only predicates from Sig(O) [ {?}.
For each rule r = '(x) ! 9y (x, y) in O, let
 (r) = { (' ! ) |
The program P of

}

is defined as
P =

and the support of

an atom in

[

 (r)

r2O

is the set

supp( ) = { r | r 2 supp() with  a proof in P [ D0 of a fact from Dr }.
Finally, if F = { r 2 O | supp( ) \  (r) 6= ; }, the module M of
following subset of O:
M = F [ F ? [ F > [ F .

is defined as the


The mapping  and the datasets D0 and Dr constitute the degrees of freedom of our
framework, and Definition 19 ensures that specific choices of these parameters of the module
setting fully determine the module M .
The datalog program P is obtained by applying  to each rule r in O while at the
same time splitting the head atoms of r into dierent rules. The application of  turns
existentially quantified variables into (possibly fresh) constants and hence transforms O
into a set of rules where all variables are universally quantified; additionally,  maps the
constants occurring in O to (possibly dierent) constants. Since  is not required to be
injective, it is possible for  to map an existentially quantified variable and a constant from
O to the same constant. As we will see next, P is a strengthening of O in the sense that it
preserves all consequences of O when coupled with an arbitrary dataset. Analogous datalog
strengthenings have been exploited to overestimate reasoning outcomes in description logic
ontologies (Krotzsch, Rudolph, & Hitzler, 2008b; Stefanoni et al., 2013; Krotzsch, Rudolph,
& Hitzler, 2008a; Zhou et al., 2014; Zhou, Nenov, Cuenca Grau, & Horrocks, 2013; Zhou,
Cuenca Grau, Horrocks, Wu, & Banerjee, 2013).
The support supp( ) collects all datalog rules participating in any proof in P [ D0 of
any relevant fact from Dr . Intuitively, this support captures the image of the module in
P . Finally, the module M consists of all the rules in O that have a corresponding datalog
rule in the support supp( ).
Example 20. Let us reconsider Example 18 in Section 4, where we chose  to map variables
y1 and y2 to fresh constants cy1 and cy2 , whereas D0 and Dr contain, respectively, the initial
fact D(a) and the fact H(a) to be proved. Definition 19 ensures that P consists precisely
of the datalog rules in Figure 5. The support supp( ) consists of all rules in the support of
0 and 00 shown in Figure 4. Finally, M consists of rules r5 r8 , as required.

The following lemma establishes a key correspondence between hyperresolution proofs
in (the clausification of) O and sets of proofs in the datalog program P . Such a correspondence is already manifest in Figure 4 for our running example. Given an arbitrary dataset
518

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning

D and a substitution  from constants to constants that is compatible with  in (i.e., 
and  coincide over the intersection
of their domains), the lemma shows that each proof 
Wn
of a disjunction of facts ' = i=1 i in (O) [ D has a corresponding set of proofs T of each
disjunct i  in P [ D . This implies, in particular, that P is indeed a strengthening of
O. Furthermore, the set T is both support and structure preserving: for every clause from
(r) participating in , there is a proof in T with a datalog rule from  (r) in its support;
finally, each proof in T is embeddable into  and hence its structure is compatible with that
of  (cf. Section 2.3).2
Lemma 21. Let be a module setting for O and  and let its corresponding substitution
be . Let D be a dataset and  an arbitrary substitution from constants in D into constants
that is compatible with . Finally, let ' be a (possibly empty) disjunction of facts and
 = (T, ) a proof of ' in (O) [ D. Then there exists a non-empty set T of proofs in
P [ D satisfying the following properties:
1. Each 0 2 T is a proof of  for some
there is some proof of  in T .

2 ' [ {?}. Furthermore, for each

2'

2. For each r 2 O with (r) \ supp() 6= ;, either r = ? !  or there exists 0 2 T with
 (r) \ supp(0 ) 6= ;.
3. Each 0 2 T is embeddable into  modulo  [ .
Proof. To prove these results it suffices to show that
(a) for each 2 ' there exists a proof 0 in P [ D of  that is embeddable into 
modulo  [ , and
(b) for each r 2 O s.t. (r) \ supp() 6= ;, either r = ? !  or there exists 2 ' [ {?}
and a proof 0 of  in P [ D that is embeddable into  modulo  [ , and such
that  (r) \ supp(0 ) 6= ;.
In order to be able to reason by induction on the depth d of , we prove that the above
properties hold even if ' is a disjunction of (not necessarily function-free) ground atoms.
d=0
If supp() = ; then ' is a fact in D and '( [ ) = ' 2 D so there exists a trivial
proof 0 in P [ D of '( [ ), which is clearly embeddable into  via  [ .

If supp() 6= ; then (O) must contain a clause of the form ( ! '). Since, by
assumption, the only rule in O with an empty head is ? ! , it must be the case
that ' 6=  and for each 2 ' there exists a proof 0 in P [ D of  = ( [ ) of
depth 0 that is supported by ( ! ) 2  (r) and embeddable into  modulo  [ .
In either case, both properties are satisfied.

2. In Lemma 21, as well as in some of our subsequent technical results, we prove statements about (O)
rather than about O. In such cases, we consider an extension of  in where functional terms fyr (t)
occurring in (O) are mapped to y whenever y is in the domain of ; by slight abuse of notation and
for the sake of simplicity, we also refer to such an extended substitution as .

519

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

d>0
Let  = (T, ) with v the root of T and w1 , . . . , wn the children of v. Consider a clause
s 2 (O) such
V that ' is a hyperresolvent of s and (w1 ), . . . , (wn ). Then, s must be
of the form ni=1 i0 ! '0 where
(wi ) = i _ i for each 1  i  n, and
W
 ' = ni=1 i _ '0 with a MGU of i ,


0
i

for each 1  i  n.

(a) Let 2 '. We need to find a proof 0 = (T 0 , 0 ) of ( [ ) in P [ D that
is embeddable into  modulo  [ . If
2 i then by induction hypothesis
we can find such a proof. If
2 '0 then it holds that
= 0 for some
0 2 '0 . By induction hypothesis, we have a proof  = (T , ) in P [ D
i
i i
of each i ( [ ) that is embeddable modulo  [  into a proper subproof of .
Because is a MGU of i and i0 , with i = i0 , we have that ( [ ) is a
MGU of of i ( [ ) and i0 . Indeed, i = i0 implies i ( [ ) = ( i0 )( [ );
on the other hand, since the eect of  on each functional term f (t) does not
depend of t, we have ( i0 )( [ ) = ( i0 ( [ ))( ( [ )); moreover, because
 [  only extends the domain of  to constants in D but not in O, we have
0
( i0 ( [ ))( ( [ )) = ( i0 )( ( [ )),
Vnand0 thus 0 i ( [ ) = ( i )( ( [ )). We
can hence combine all the i with ( i=1 i ! ) 2 P , to obtain a proof of
( 0 )( ( [)) = ( 0 )( [) = ( [) in P [D that is clearly also embeddable
into  modulo  [ .

(b) Consider r 2 O such that there exists r0 2 (r) \ supp(). We need to find
2 ' [ {?} and a proof 0 = (T 0 , 0 ) of ( [ ) in P [ D that is embeddable
into  modulo  [  and has some rule from  (r) in its support.
Assume first that r0 = s and r 6= ? ! . Then it must be '0 6=  since, by
assumption, ? !  is the only rule in O with an empty head. We can then pick
any V0 2 '0 and have 0 be a proof of ( 0 )( [ ) in P [ D that is supported
by ( ni=1 i0 ! 0 ) 2  (r), as we saw when considering property (a).
Assume now that r0 6= s and r 6= ? ! . Then there must be some i such that r0
supports a proof i of i _ i that is a subproof of . Since i is of depth < d, by
i.h. there must be some i00 2 i _ i [ {?} and a proof 00i = (T 00 , 00 ) of i00 ( [ )
in P [ D that is supported by some rule in  (r) and is embeddable into i . If
00
0
00
00
i 2 i [ {?}  ' [ {?} then  =  is the proof we are looking for. If i = i
(and i 6= ?) we can combine 00 with suitable proofs of each j ( [ ) for the
remaining j (which we know exist by i.h.), as before, to construct a proof 0 in
P [ D of ( [ ) for some 2 ', that is embeddable into  modulo  [ .
Lemma 21 and Proposition 1 establish how the datasets D0 and Dr in can be chosen
so as to ensure that M preserves the required -consequences.
Suppose that M is required to preserve some -consequence r = '(x) ! 9y (x, y) of
O. By Proposition 1, given any substitution mapping variables in x to distinct constants
c, it must be the case that O [ '(c) |= 9y (c, y). Since P is a strengthening of O we also
have P [ '(c) |= 9y (c, y). Assume that we now choose D0 and Dr so as to satisfy the
following requirements:
520

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning

1. the instantiation '(c) of the body of r must be embeddable in D0 ; and
2. the set of all facts in the materialisation of P [ '(c) satisfying the instantiation
9y (c, y) of the head of r must be embeddable into Dr .
By completeness of hyperresolution, and given that P is a datalog program, there must
exist a fact s.t. |= 9y (c, y) and P [ '(c) ` . By Lemma 21 and Definition 19, the
aforementioned requirements on D0 and Dr suffice to guarantee that M will preserve the
consequence r.
Example 22. Consider again our running example in Section 4 and the associated module
setting = h, D0 , Dr i given in Example 20. We can see that our choices of D0 = {D(a)}
and Dr = {H(a)} satisfy our sufficient requirements for M to entail ' = D(x) ! H(x).
First, any instantiation of the body D(x) of ' is isomorphic to (and hence embeddable into)
D0 . Second, the materialisation of P [ {D(a)} consists of facts F (a), S(a, cy2 ) and H(a),
where the latter is isomorphic to the instantiation of the head of '; since we chose Dr to
consist precisely of H(a), the second requirement is also satisfied.

The following theorem makes precise the aforementioned sufficient requirements for M
to preserve the entailment of a -rule r. Furthermore, it shows that whenever M entails
r, it also contains all the justifications for r in the original ontology O.
Theorem 23. Let r = '(x) ! 9y (x, y) be a rule and
= h, D0 , Dr i a module setting
for O and  such that for each substitution mapping all variables in x to pairwise distinct
constants, there exists another substitution  that is compatible with  and such that
 (' )  D0 , and
 (( ) ) 0 [ {?}  Dr for each substitution
and such that P [ (' ) |= (( ) ) 0 .

0

mapping variables in y to constants

Then O |= r i M |= r. Furthermore, if O0 is a justification for r in O, then O0  M .
Proof. Since M  O, it follows from monotonicity of first-order logic that O |= r whenever
M |= r. To prove the opposite direction of the implication, it suffices to show that if
O0  O is a justification for r in O, then O0  M .
If =  then, by minimality of O0 , given a substitution mapping variables in x to fresh
distinct constants, there must be a proof  of  in (O) [ ' such that supp() \ (r) 6= ;
for each r 2 O0 . By assumption, there exists a substitution  that is compatible with  and
such that (' )  D0 . By Lemma 21, for each r 2 O0 either r = ? !  or there exists a
proof ? in P [ (' ) of ? such that supp(? ) \  (r) 6= ;. If r = ? !  then, since by
assumption the only rule in O with an empty head is ? ! , in particular it must also be
the case that O |= '(x) ! ?. It suffices to show that in this case also M |= '(x) ! ?
(as we do next when considering 6= ): then, it follows that ? 2 Sig(M ) and therefore
r = ? !  2 M because M is an ontology. If r 6= ? !  then, since (' )  D0 and
? 2 Dr , it follows that r 2 M .
W
If 6=  we can assume w.l.o.g. that = ni=1 i with m > 0 and each i a conjunction
of atoms. For a fresh predicate Q, consider the ontology
OQ = {

i (x, y)

! Q(x) | 1  i  n }
521

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

It is immediate that, for each subset O00  O, it is O00 |= r i O00 [ OQ |= '(x) ! Q(x).
0  O
0
0
Therefore, by minimality of O0 , there must be some OQ
Q such that O [ OQ is a
0 , given a substitution
justification of '(x) ! Q(x) in O [ OQ . By minimality of O0 [ OQ
mapping variables in x to fresh distinct constants, there must be a proof  of Q(x) in
0 .
(O [ OQ ) [ ' such that (r) \ supp() 6= ; for each r 2 O0 [ OQ
By assumption, there is a substitution  that is compatible with  and such that
(' )  D0 . Since OQ does not contain any existentially quantified variables, or any
constants that do not occur already in O, there exists a module setting Q = hQ , D0Q , DrQ i
for O [ OQ and  such that Q = , and therefore P Q = P [ OQ . By Lemma 21, for
0 either it is s = ? !  or there exists a proof 0 in P Q [ (' ) of
each s 2 O0  O0 [ OQ
either (Q(x) ) or ? such that s0 2  Q (s) \ supp(0 ) 6= ;.
If s = ? !  then there must be some proof in (O [ OQ ) [ ' of a disjunction of the
form ? _ (with possibly empty). By Lemma 21 there exists a proof ? in P Q [ (' )
of ?. Furthermore, since ? does not mention Q, and neither does the body of any rule in
P Q = P [OQ , the proof ? must actually be a proof in P [(' ) . Because (' )s  D0
and ? 2 Dr , it follows that supp(? )  supp( ). Hence ? 2 Sig(M ), and consequently
s=?!2M .

Otherwise, if 0 is a proof of = ? then, as before, 0 must be a proof in P [ (' ) .
Then, since (' )s  D0 and ? 2 Dr , we have s0 2 supp( ) and thus s 2 M . If 0 is a
proof of = (Q(x) ) , let 0 = (T, ) with v the root of T and w1 , . . . , wm its children.
0
The rule applied at the top of 0 must be from OQ and therefore dierent from
Vm s . In
particular, this rule must be of the form i (x, y) ! Q(x), with (( i  ) ) = j=1 (wj )
for some extension  of to y. Clearly, there is some substitution 0 with domain y such
that (( i  ) ) = (( i ) ) 0 . The rule s0 must thus be in the support of a proof 0j in
P Q [ (' )s of some (wj ). Since r does not mention Q neither does (wj ), and again we
have that 0 must in fact be a proof in P [ (' ) . This implies P [ (' )s |= (wj ) and
hence by assumption (wj ) 2 Dr . Finally, since (' )  D0 , we have s0 2 supp( ) and
consequently s 2 M .

6. Modules for each Inseparability Relation
Theorem 23 tells us how to choose a module setting so that its corresponding module
M preserves a particular consequence of O. However, in order for M to be a S -module
of O for a given inseparability relation S, it must preserve not one, but all of the (possibly
infinitely many) relevant consequences in relS (O, ) (recall Theorem 9 in Section 3).
In this section we consider each inseparability relation S 2 {m, q, f, i}, and formulate a
specific module setting S which provably yields a S -module of O. Later on in Section 11
we will consider the optimality of these module settingsthat is, whether there may exist
a dierent setting that yields a smaller module for the relevant inseparability relation.
6.1 Implication Inseparability
Our running example immediately suggests a natural module setting
guarantees implication inseparability.
522

i

= hi , D0i , Dri i that

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning

As in our example, we pick the substitution i to be as general as possible by Skolemising each existentially quantified variable to a distinct fresh constant and mapping constants
occurring in O to themselves. To pick D0i and Dri we rely on the application of the sufficient
conditions established in Theorem 23 to each of the (quadratically many) -implications
A(x) ! B(x) with x = (x1 , . . . , xn ). More precisely, to capture the instantiations of the
body A(x) we define D0 to contain a fact A(c1A , . . . , cnA ) involving fresh constants ciA uniquely
associated to the predicate A; furthermore, to capture the head of the implication, we define
Dr so as to contain a fact B(c1A , . . . , cnA ). In this way, the dataset D0i contains linearly many
and Dri quadratically many facts in the size of the signature .
Definition 24. For each existentially quantified variable y in O, let cy be a fresh constant.
Furthermore, for each A 2  of arity n, let cA = (c1A , . . . , cnA ) be an array of fresh constants.
The module setting i = hi , D0i , Dri i is defined as follows:
 i = { y 7! cy | y existentially quantified in O } [ { c 7! c | c 2 Ct(O) },
 D0i = { A(cA ) | A 2  }; and
 Dri = { B(cA ) | A 6= B predicates in  of the same arity } [ {?}.



The module setting i is reminiscent of the datalog encodings typically used to check
whether a concept A is subsumed by another concept B w.r.t. a lightweight ontology O
(Krotzsch et al., 2008b; Stefanoni et al., 2013). There, existentially quantified variables in
rules are also skolemised as fresh constants to produce a datalog program P, and then it is
checked whether P [ {A(a)} |= B(a).
The module setting i captures implication inseparability as a straightforward consequence of Theorem 23.
Theorem 25. M i i O.
Proof. Consider an arbitrary rule of the form A(x) ! B(x) with x = (x1 , . . . , xn ) a vector
of distinct variables and A, B distinct n-ary predicates from . Let be a substitution
mapping x1 , . . . , xn to distinct constants c1 , . . . , cn , and  another substitution such that
ci  = ciA . By definition of i we have (A(x) ) 2 D0i and (B(x) ) 2 Dri , and thus, by
Theorem 23, it follows that O |= A(x) ! B(x) i M i |= A(x) ! B(x).
6.2 Fact Inseparability
By Theorem 9, fact inseparability requires the preservation of all the datalog -rules entailed
by O. Thus, in contrast to implication inseparability, it may require the preservation of a
very large (and possibly even infinite) set of entailments. Unsurprisingly, the module setting
i cannot be used to capture fact inseparability as illustrated by the following example.
Example 26. Consider Oex and 1 = {B, C, D, H}, for which M i = {r5 r8 }. As seen in
Example 18, for D = {B(a), C(a)} it is the case that Oex [ D |= D(a), while we also have
M i [ D 6|= D(a); hence, M i is not 1 -fact inseparable from Oex .
Equivalently, Oex entails the datalog rule r3 = B(x) ^ C(x) ! D(x), whereas M i does
not and hence Theorem 23 is no longer applicable since D, which instantiates the body of
r3 , cannot be embedded into D0i = {B(cB ), C(cC ), D(cD ), H(cH )}.

523

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

Thus, we will next define a suitable module setting f = hf , D0f , Drf i to capture fact
inseparability. As in the previous case, we will exploit the sufficient conditions given in
Theorem 23. To this end, we first need to make sure that D0f (resp. Drf ) captures all possible
body (resp. head) instantiations of all possible datalog rules over  that may be entailed
by O. We achieve this by choosing D0f and Drf to be the most constrained -dataset
possible, which is typically referred to in the literature as the critical dataset (Marnette,
2009; Cuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang, 2013).
Definition 27. Let  be a signature and let  be a fresh constant. The critical -dataset
is defined as follows:
n

z }| {

D
= { A(, . . . , ) | A n-ary predicate in  }



Indeed, it is straightforward to see that every -dataset (and hence any datalog rule
 by mapping every constant into .
instantiation) can be embedded into D
As in the case of i , we choose the substitution f to be as general as possible by mapping
existentially quantified variables to distinct fresh constants. However, in contrast to i , we
require all constants occurring in O to be mapped to  rather than to themselves. This
choice is justified by the following example.
Example 28. Consider Oex and  = {A, C, E}. Clearly, for D = {A(a), C(o)} we have
Oex [ D |= E(a) due to rules r2 and r4 in Oex . If we were to pick f to be i , which maps
constant o in Oex to itself, we would obtain M f = ; even if we choose D0f and Drf as the
critical -dataset. Indeed, the relevant fact E() would not be provable from P f [ D0f . 
We are now ready to define

f

formally.

Definition 29. Let constants cy be as in Definition 24, and let  be a fresh constant. The
module setting f = hf , D0f , Drf i is defined as follows:
 f = { y 7! cy | y existentially quantified in O } [ { c 7!  | c 2 Ct(O) },
 , and
 D0f = D
 [ {?}.
 Drf = D



Example 30. The datalog program generated by f for Oex coincides with that of Figure 5
in all rules except for r2 , which now becomes
r20 : A(x) ! R(x, )
If we consider again 1 = {B, C, D, H} and D = {B(a), C(a)} from Example 26, we clearly
have P f [D0f ` D() 2 Drf since {B(), C()}  D0f . The unique proof  of D() in P f [D0f
is only supported by r3 ; this guarantees that r3 2 M f and thus  corresponds directly to
a proof of D(a) in M f [ D. Hence, we have M f [ D |= D(a), as required.
If we now consider the signature  = {A, C, E} from Example 28, we can observe in
Figure 6 how our choice of mapping constant o to  ensures that the module contains the
necessary rules r2 and r4 .

524

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning

E()
r4

E(a)
r4
R(a, c)
r2

R(, )

C(o)

C()

r20

A(a)

A()
(a)

(b)

Figure 6: Proofs of (a) E(a) in Oex [ {A(a), C(o)} and (b) E() in P
We can now exploit Theorem 23 once more to show that
Theorem 31. M

f

f

f

[ D0f

captures fact inseparability.

f O.

Proof. Let r = ' !
be a datalog rule over . Let
be a substitution mapping all
variables in r to pairwise distinct constants, and  a substitution mapping each constant
in the range of to . By definition of f we have (' )  D0f and ( )  Drf and thus,
by Theorem 23 it follows that O |= r i M f |= r. Finally, by Proposition 7, this implies
M f f O.
6.3 Query Inseparability
Positive existential queries constitute a much richer query language than facts as they allow
for existentially quantified variables. Thus, the query inseparability requirement inevitably
leads to larger modules.
Example 32. Consider Oex and  = {A, B}. Given the -dataset D = {A(a)} and the
-query q = 9yB(y), we have that Oex [ D |= q (due to rule r1 ). In this case, however, M f
is empty and thus M f [ D 6|= q. Indeed, the only additional facts in the materialisation of
P f [ {A(), B()} are R(, cy1 ) and B(cy1 ), and hence neither r10 nor r100 (cf. Figure 5) are
 is constrained enough
in supp( f ). This suggests that, although the critical -dataset D
to embed every -dataset, we may need to consider additional relevant facts to capture all
proofs of all -queries. In particular, rule r1 implies that B has a non-empty extension
whenever A does: a dependency that is then checked by q. This can be captured by
considering fact B(cy1 ) as relevant, in which case r1 would be included in the module. 
By Theorem 9, query inseparability requires the preservation of all -rules entailed by
O (and not just of those that are datalog). In particular, first-order rules may involve
existentially quantified variables, which correspond in our framework to Skolem constants.
This naturally suggests a module setting q that diers from f only in that -facts involving
Skolem constants (and not just those mentioning only ) are also considered relevant.
Definition 33. Let constants cy and  be as in Definition 29. We define the module setting
q
q
q
q = h , D0 , Dr i as follows:
 q = f ,
525

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

B(fyr11 (a))

B(cy1 )
r100

r1,2
A(a)

A()

(a)

(b)

Figure 7: Proofs of (a) B(fyr11 (a)) in Oex [ {A(a)} and (b) B(cy1 ) in P

q

[ D0q

 , and
 D0q = D

 Drq = { A(a1 , . . . , an ) | A 2 , each aj is either  or some cy } [ {?}.



Example 34. Coming back to Example 32, we can observe in Figure 7 how a proof of
B(fyr11 (a)) in (Oex )[{A(a)} that is supported by r1,2 : A(x) ! B(fyr11 (x)) can be recovered
from a proof of B(cy1 ) in P q [ D0q that is supported by r100 . The definition of q ensures
that B(cy1 ) 2 Drq , and hence r1 2 M q .

Theorem 35. M

q

q O.

Proof. Let r = ' ! 9y be a rule over  with a non-empty head. Let be a substitution
mapping all variables in r to pairwise distinct constants and  a substitution that maps each
0  Dq
constant in the range of to . By definition of q we have (' )  D0q , and also
r
for each substitution 0 mapping all variables in r to constants in Ct(D0q [ Drq ) [ range(q ).
Thus, by Theorem 23, it follows that O |= r i M q |= r. By Proposition 7, this implies
M q q O.
6.4 Model Inseparability
Model inseparability diers substantially from all the previous inseparability relations:
rather than just the preservation of rule-shaped consequences, it requires the preservation of
models (and hence of second-order consequences). Theorem 23, which we have repeatedly
exploited to show that our modules preserve the required entailments, relies on the properties of hyperresolution (a first-order logic calculus); hence, it is not applicable to show
preservation of second-order logic consequences. In particular, as the following example
illustrates, the modules generated by q may not be -model inseparable from O.
Example 36. Consider Oex and  = {A, C, D, R}, in which case M q = {r1 , r2 }. As we
saw in Example 6, the interpretation I where I = {a, o}, AI = {a}, B I = C I = {o},
DI = ; and RI = {(a, o)} is a model of M q ; however, it can be readily checked that it
cannot be extended to a model of O without reinterpreting A, C, D or R.

Intuitively, when constructing a model of O, fixing the interpretation of certain predicates restricts the ways in which the remaining predicates can be interpreted. These restrictions are obviously determined by the dependencies introduced by the rules in O. To
capture model inseparability, we need to ensure that M preserves all such relevant dependencies between predicates in . For this, we pick  and D0 in such a way that any model
526

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning

R(, )

R(, )

A()

A()

r1

D()
r3

r20

B()

C()

r1
A()
Figure 8: All proofs in P

m

[ D0m of facts from Drm in Example 39.

of O can be embedded in the materialisation of P [ D0 ; in turn, we choose Dr in such
a way that it captures the -reducts of all those models. If this is the case, the proofs in
P [ D0 of facts from Dr will then capture all the dependencies between predicates in .
Example 37. Note that I in Example 36 cannot be embedded in the materialisation of
P q [ D0q since the only facts over {B, C} that it contains are C() and B(cy1 ), and the
constant o cannot be mapped to both cy1 and .

 ; but now, in contrast to
To capture all models of O, we once more pick D0 = D
q,
we choose a substitution  that maps both existentially quantified variables and constants
in O to . Furthermore, to ensure that Dr captures the -reducts of all those models, we
 as well.
pick Dr as D

Definition 38. The module setting

m

= hm , D0m , Drm i is as follows:

 m = { y 7!  | y existentially quantified in O } [ { c 7!  | c 2 Ct(O) },
 , and
 D0m = D
 [ {?}.
 Drm = D



Example 39. Consider Oex ,  and I as in Example 36. The substitution m maps the
existentially quantified variables in r1 and r6 to . Thus, rules r1 and r6 in Oex correspond
to the following rules in P m :
r1
r6

r1 : A(x) ! R(x, ), r1 : A(x) ! B()
r6 : F (x) ! S(x, )

The materialisation of P m [ D0m contains facts A(), C(), D(), R(, ), and B(). Consequently, it is now possible to embed the interpretation I into the aforementioned materialisation by mapping both a and o to .
Figure 8 shows all (non-trivial) proofs in P m [ D0m of facts from Drm . We can observe
that the module M m consists of rules r1 r3 . Clearly, any model of M m can be extended to
a model of Oex since no predicates from M m occur in the head of any rule in Oex \M m . 
Theorem 40 shows that the module M m is -model inseparable from O. Indeed, every
model I of M m can be extended to a model of O in the following way: (i) predicates not
occurring in the materialisation of P m [ D0m are interpreted as empty, (ii) predicates in
the support of m (and hence occurring in M m ) are interpreted as in I, and (iii) all other
predicates A with arity n are interpreted as ( I )n .
527

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

Theorem 40. M

m

m
 O.

Proof. Let I be a model of M m . W.l.o.g., we assume that I is defined over all of Sig(O).
Let J be an interpretation with the same domain as I, and such that
8 I
if A 2  [ Sig(supp( m ))
< A
J
arity(A)
A
=
if A 2 Sig(P m (D0m )) \ ( [ Sig(supp( m )))
:
;
otherwise

Note that  [ Sig(supp( m ))  Sig(P m (D0m )).
Consider r : ' ! 2 O. We now show that J |= r.
Assume first = . Then r = ? !  and we need to check that ? is interpreted as
false. If ? 2
/ Sig(P m (D0m )) then this is the case by definition of J . If ? 2 Sig(P m (D0m ))
then, since ? 2 Drm , it must be ? 2 Sig(supp( m )) and therefore also ? 2 Sig(M m ). This
implies that ? !  2 M m and thus, since I is a model of M m , it must be the case that
? is interpreted as false. Since ? 2 Sig(supp( m )), by definition of J we then have that
both ?J and ?I are false.
Assume now 6= . If Sig( ) 6 Sig(P m (D0m )) then, because P m [ D0m only mentions
one constant (namely, ), it follows that also Sig(') 6 Sig(P m (D0m )) and therefore 'J = ;
and J |= r. Hence, in the following we assume Sig( )  Sig(P m (D0m )).
If Sig( ) \ ( [ Sig(supp( m ))) = ;, then AJ = arity(A) for each A 2 Sig( ) and it
is immediate that J |= r. Otherwise suppose that there exists a substitution over all
variables in r such that J |= ' (if no such substitution exists then J |= r holds trivially).
Then 'J 6= ; and it must be Sig(')  Sig(P m (D0m )). Let  be a substitution that maps
all variables to ; because  is the only constant in P m [D0m , it folows that '   P m (D0m )
m (D m ).
and thus also
 P
0
By assumption, there exists = A(, . . . , ) 2
 with A 2 [Sig(supp( m )). Because
'   P m (D0m ), there is a proof  ,r of in P m [ D0m that is supported by a rule from
 m (r). If A 2  then, by definition of m , it is 2 Dr m and consequently r 2 M m .
If, on the other hand, A 2
/ , there must be 0 2 Drm and a proof 0 of 0 in P m [ D0m
such that some proof of A(, . . . , ) is a subproof of 0 . Replacing this subproof with A,r
results in another proof of 0 in P m [ D0m that is supported by a rule in  m (r). Thus
r 2 M m in this case as well. Rules in  m (r) have the same body as r, so r 2 M m implies
Sig(')  Sig(supp( m )), and thus I and J agree over Sig('). By assumption, J |= ' , so
m (D m ),
we have I |= ' as well, and, since r 2 M m , also I |=
. Finally, since
 P
0
m
I
J
m
we have that Sig( )  Sig(P (D0 )) and therefore

, so J |=
. Since is
arbitrary, we can conclude that J |= r.
The modules generated by m are similar in spirit to locality-based modules in that
certain symbols outside the signature of the module are interpreted as either the empty set
or the universal relation (of the relevant arity) over the interpretation domain. As we will
show later on, M m  M?
[O,] whenever O is a normalised SROIQ ontology. However, as
illustrated by the following example, our modules are incomparable to >- and ?> -modules.
Example 41. For O = Oex and  = {D, F } we have the following:
M

m

= {r5 }

M>
[O,] = {r1 -r3 }
528



M?>
[O,] = ;.

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning



?>
Consequently, M m is neither contained in M>
[O,] , nor in M[O,] . To see that the converse
is also true, consider O = {r1 , r9 }, with r9 = C(x) ! B(x), and  = {A, C, R}. Then, we
have the following:

M

m



?>
M>
[O,] = M[O,] = O

= {r1 }



We have already mentioned that modules generated by m are included in locality
?-modules. To conclude this section, we show how m can be modified to precisely capture
?-modules. For this, it suffices to modify m by making Dr the critical dataset over the
entire signature of O (instead of just ) as given in the following definition.
Definition 42. The module setting

b

= hb , D0b , Drb i is as follows:

 b = m ,
 , and
 D0b = D

 Drb = DSig(O)
[ {?}.



The following proposition shows that M
SROIQ ontology O relative to .

b

coincides with the ?-locality module of a

Proposition 43. If O is a normalised SROIQ ontology, then M

b

= M?
[O,] .

Proof. By definition, M?
[O,] is the smallest subset M  O such that every axiom in O \ M
b
is ?-local w.r.t.  [ Sig(M). To show that M?
[O,]  M , it suffices to show that, for each
r 2 O \ M b , r is ?-local w.r.t.  [ Sig(M b ). Consider r 2 O \ M b . Since Drb contains all
facts that can occur in P b (D0b ), we have that supp( b ) consists of all rules in the support of
any proof in P b [D0b . Furthermore, we have that Sig(M b )[ = Sig(P b (D0b )). Therefore,
there can be no proof in P b [ D0b that has a rule from  b (r) in its support. Because the
only constant mentioned in P b [ D0b is , this means that some predicate from the body
of r does not occur at all in Sig(P b (D0b )) = Sig(supp( b )) [ . As can be observed in
Tables 1 and 2, this implies that r is ?-local w.r.t. Sig(supp( b )) [ .
0
b
b
To see that M b  M?
[O,] , consider r 2 M . There must exist r 2  (r) such that
0
b
?
r 2 supp() for some proof  in P b [ D0 . To show that r 2 M[O,] , let us reason by
induction on the depth d of .
d = 0 Then the body of r is empty and thus r is not ?-local w.r.t. any signature. It
follows that r 2 M?
[O,] .

d > 0 It suffices to consider the case where r0 is the rule applied at the top of , since
we already know by induction hypothesis that, for each s 2 O such that a rule
from  b (s) is in the support of a (proper) subproof of , it is s 2 M?
[O,] . Since
b
Sig(D0 ) = , this implies that, if  = (T, ) with v the root of T and w1 , . . . , wn
its children, then Sig( (wi ))  Sig(M?
[O,] ) [ . Consequently, the body of r
must have its signature fully contained in Sig(M?
[O,] ) [ . It follows that r is
?
not ?-local w.r.t. Sig(M[O,] ) [  and hence r 2 M?
[O,] .
529

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

7. Additional Inseparability Relations
The bulk of the research on module extraction has focused on the inseparability relations
considered in Section 6. In this section we show how our framework can be seamlessly
adapted to other interesting inseparability relations.
7.1 Classification Inseparability
Classificationthe problem of identifying all subsumption relationships between all pairs of
atomic concepts and all pairs of atomic roles in a DL ontologyis a fundamental reasoning
task in ontology engineering. Classifying a first-order ontology O amounts to computing,
for each predicate A in O, all the entailed implications A(x) ! B(x) where B is a predicate
of the same arity as A, referred to as a subsumer of A in O.
Locality ?-modules have been successfully exploited for optimising classification of DL
ontologies (Tsarkov & Palmisano, 2012; Cuenca Grau et al., 2010; Armas Romero et al.,
2012). In addition to being model-inseparable from the given ontology and satisfying the
properties in Proposition 15 from Section 3.2, ?-modules enjoy an additional property that
makes them well-suited for optimising classification (Cuenca Grau et al., 2007a, 2010):
Proposition 44. Let O be an ontology,  a signature, and r a rule of the form A(x) !
where A 2  and either = ? or = B(x) with B 2 Sig(O). Then, O |= r i M?
[O,] |= r.
It follows from Proposition 44 that the ?-module for  = {A} in O captures all subsumers of A in O, and hence it is indistinguishable from O w.r.t. to all implications having
A in the body. We can capture this additional property of ?-modules by means of the
following inseparability relation.
Definition 45. Ontologies O and O0 are -classification inseparable (O c O0 ) if for
each rule r of the form A(x) ! , where A 2  and either
=  or
= B(x) with
B 2 Sig(O [ O0 ), we have O |= r i O0 |= r. Furthermore, relc is the function mapping each
ontology O and signature  to the following set of rules:
relc (O, ) = {r=A(x) !

| O |= r, A 2  and

=? or

=B(x) with B 2 Sig(O)}



It follows straightforwardly from the definition that O c O0 holds i relc (O, ) coincides with relc (O0 , ); hence, Theorem 9 in Section 3 trivially extends to classification
inseparability. Furthermore, it can be readily checked that c ( i for each non-trivial
signature , and hence classification inseparability is (as expected) a stronger requirement
than implication inseparability. Finally, although ?-modules ensure classification inseparability from O, they are also model-inseparable (a much stricter requirement) and, as shown
in Section 13, can be much larger than necessary for ontology classification.
Example 46. Consider our example ontology Oex . We can observe how classification inseparability is a stronger requirement than implication inseparability by considering  = {G}.
Clearly, M = ; is -implication inseparable from Oex , whereas -classification inseparability requires rule r8 to be contained in M. To see how ?-modules dier from minimial classification-inseparable modules consider  = {A}; in this case, we have that
ex and therefore the empty ontology
M?
[O ex ,] = {r1 , r2 }, but A has no subsumers in O
ex
is already -classification inseparable from O .

530

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning

The following module setting extends i from Definition 24 to capture classification
inseparability. As one would naturally expect, the only required modification is to extend
Dri with facts involving predicates outside .
Definition 47. For each A 2  of arity n, let cA = (c1A , . . . , cnA ) be an array of fresh
constants. The module setting c = (c , D0c , Drc ) is defined as follows:
 c = i ,
 D0c = D0i , and
 Drc = { B(cA ) | A 2  and B 2 Sig(O) distinct predicates of the same arity } [ {?}.

Example 48. Consider again O = Oex and  = {A}. Since no fact from Drc is provable in
P c [ D0c the module M c is empty and thus also minimal in this case.

We can show that c captures implication inseparability using an argument analogous
to that in the proof of Theorem 25.
Theorem 49. M

c

c O.

7.2 Weak Query Inseparability
One of the possible applications of modules based on query inseparability is to optimise
query answering. In particular, if M is -query inseparable from O, then O [ D |= q i
M[D |= q for any -query q and -dataset D; thus, we can replace O with M to answer an
arbitrary query w.r.t. arbitrary data provided that only symbols in  are deemed relevant.
While the aforementioned notion of query inseparability is useful for situations where
the data is unknown or frequently changing, in many situations the data in an ontology is
considered fixed and hence one could potentially extract smaller modules by not requiring
M to be robust under extensions with arbitrary data.
Botoeva, Kontchakov, Ryzhikov, Wolter, and Zakharyaschev (2014) investigated a restricted notion of query inseparability that is well-suited for cases where the data in an
ontology can be considered fixed. In this paper, we will refer to this restricted notion as
weak query inseparability.
0
Definition 50. Ontologies O and O0 are -weak query inseparable (O wq
 O ) if for each
Boolean PEQ q over  we have O |= q i O0 |= q. Furthermore, relwq is the function
mapping each ontology O and signature  to the set

relwq (O, ) = { q | O |= q and q is a Boolean PEQ such that Sig(q)   }



0
Again, Theorem 9 extends naturally to weak query inseparability; indeed, O wq
 O if
and only if relwq (O, ) coincides with relwq (O0 , ). Moreover, the requirements in Definition 50 are weaker than those of query inseparability and hence q ( wq
 for each . As
a result, weak query inseparability may yield smaller modules, as we show in Section 13.
We next propose a module setting wq that captures weak query inseparability. The
main dierence between wq and q in Section 6.3 is that the initial dataset D0wq is chosen as

531

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

 . This is a natural choice given that we no longer have the requirement
empty rather than D
that arbitrary -datasets must be embeddable into D0wq . Furthermore, wq diers from q
in that it maps constants from Ct(O) to themselves (same as i ).

Definition 51. Let constants cy , for each existentially quantified variable y in O, be as in
Definition 24. The module setting wq = hwq , D0wq , Drwq i is defined as follows:
 wq = i ,
 D0wq = ;, and
 Drwq = { A(a1 , . . . , an ) | A 2 , each aj either in Ct(O) or equals some cy } [ {?}.



Example 52. Consider the extension of Oex with the fact ! D(i) (seen as a ground rule)
and the signature  = {B, C, D, H}. It can be readily checked that M q = {r3 , r5 r8 },
whereas M wq = {r5 r8 }.

Theorem 53. M

wq

wq
 O.

Proof. Any Boolean PEQ q can be seen as the rule ( ! q). Consequently, M wq wq
 O
follows from Theorem 23 by a similar argument to that in the proof of Theorem 35.

8. Module Containment
Intuitively, the more expressive the language for which preservation of consequences is
required, the larger the modules need to be. For instance, since f ( i , it is to be
expected that the module M i obtained for implication inseparability is contained in the
module M f for fact inseparability. We next show that all our modules in Sections 6 and 7
are consistent with this intuition.
Our first step will be to introduce a notion of homomorphism between module settings,
which will then allow us to establish containment between their corresponding modules.
Definition 54. For a module setting
= h, D0 , Dr i, let Ct( ) denote the set of constants occurring in D0 , Dr , and in the range of . A substitution  : Ct( ) ! Ct( 0 ) is a
homomorphism from to 0 if the following conditions hold:
  = 0 ;
 D0   D00 ; and
 Dr   Dr0 .
We write

,!

0

to denote that a homomorphism from

to

0

exists.



The fact that ,! 0 (witnessed by some homomorphism ) implies that 0 is more
general than , in the sense that any proof in P [ D0 of a fact in Dr can be embedded
0
(via ) in a support-preserving way into a proof in P [ D00 of a fact in Dr0 . It follows that
0
supp( ) is contained in supp( 0 ) (modulo ) and hence we also have M  M .
Theorem 55. If ,

0

are s.t.

,!

0,

0

then M  M .
532

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning

Proof. Let = h, D0 , Dr i and 0 = h0 , D00 , Dr0 i, and let  : Ct( ) ! Ct( 0 ) be a homomorphism from to 0 . Since Dr   Dr0 , it suffices to show that for any rule r 2 O and any
proof  in P [ D0 of a fact 2 Dr such that supp() \  (r) 6= ;, there exists a proof 0 in
0
0
P [ D00 of  such that supp(0 ) \  (r) 6= ;. We prove the following more general claim.
Let r be a rule in O and  a proof of a fact (not necessarily in Dr ) in P [ D0 such
that supp() \  (r) 6= ;. We show by induction on the depth d of  that there is a proof
0
0
0 of  in P [ D00 such that supp(0 ) \  (r) 6= ;.
d=0
Since, by assumption, supp() \  (r) 6= ;, we have r = ( ! ) and ( ! ) 2  (r)
0
with =  for some 2 , and also ( ! 0 ) 2  (r). Since  is defined over all
constants in O, it holds that ( ) = () = 0 , and hence we have a proof of  in
0
0
P [ D00 supported by a rule in  (r).
d>0
Let  = (T, ) with v the root of T and w1 , . . . , wn the children of v. Let s be
the rule used to derive (v) from (w1 ), . . . , (wn ). Finally, for each i let i be the
subproof of (wi ). If for any i we have supp(i ) \  (r) 6= ;, the claim follows by the
0
induction hypothesis since s 2 P . Otherwise, we have s 2  (r). Moreover, by
0
the induction hypothesis, every (wi ) has a proof in P [ D00 , and the claim follows
0
since s 2  (r).
It is straightforward to construct homomorphisms between the module settings in Sections 6 and 7 in accordance with the containment relationship of their corresponding inseparability relations. The following result, which establishes the intuitive relationships between
our modules, then follows immediately from Theorem 55.
Corollary 56.
M i M

f

M

M i M
M

wq

q
c

M

M

M

m

M

b

b

q

As already illustrated by examples throughout Sections 6 and 7, these containment
relations are strict for many O and . Furthermore, it can be easily checked that these
containment relations are complete, in the sense that module settings that are unrelated in
Corollary 56 (e.g., M f and M wq ) are incomparable.

9. Depletingness, Self-Containment, and Justification-Preservation
The minimal requirement on a module is to preserve all relevant consequences w.r.t. a
given inseparability relation. As we argued in Section 3, however, in some applications it is
desirable that modules satisfy additional properties. In this section, we establish whether
our modules in Sections 6 and 7 satisfy the (strong) depletingness, self-containment, and
justification-preservation properties enjoyed by locality-based modules.
To establish our results, it is convenient to abstract away from the notion of module
setting for a fixed O and  and consider instead families of module settings; that is,
functions that assign a module setting to each pair of O and .
533

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

Definition 57. A module setting family is a function
that maps each pair of ontology
O and signature  to a module setting for O and . Given an inseparability relation
S, we say that
is S-admissible if, for each pair of O and , M (O,) is a S -module
of O. Furthermore, we say that
is depleting (resp. strongly depleting, self-contained,
(O,)
justification-preserving) if so is M
for each O and .
Finally, for each S 2 {m, q, f, i, c, wq}, we denote with S the (S-admissible) family
induced by the module setting S as defined in Sections 6 and 7.

9.1 Depletingness and Justification-Preservation
As discussed in Section 3, depletingness of M ensures that O \ M is inseparable from the
empty ontology and hence no relevant information is left behind in O after extracting M.
As illustrated by the following example, not all modules are depleting.
Example 58. Consider O consisting of the following rules and let  = {A, B}:
s1 = A(x) ! B(x) ^ C(x)

s2 = A(x) ! D(x) ^ E(x)

s3 = D(x) ! B(x)

Clearly, both M1 = {s1 } and M2 = {s2 , s3 } are implication-inseparable from O and hence
i -modules. However, neither of them is depleting.

We next show that all the modules we defined in Sections 6 and 7 are depleting.
Proposition 59.

S

is depleting for each S 2 {m, q, f, i, c, wq}.

Proof. Let O and  be arbitrary and let M = S (O, ). By Theorems 25, 31, 35, 40,
49 and 53, it suffices to show S (O\M, ) = ;. By the definition of a module (cf. Definition 19), it holds that S (O\M, )  O\M. Furthermore, since O\M  O, it follows
that S (O\M, )  M. Consequently, S (O\M, ) = ;.
We next show that our modules are also justification-preserving and hence can be seamlessly exploited in ontology debugging applications.
Proposition 60.

S

is justification-preserving for each S 2 {m, q, f, i, c, wq}.

Proof. Let 2 relS (O, ) and let O0 be a justification of in O. We need to check that
S
O0  M (O,) . Since 2 relS (O, ) and O0 |= , it follows by definition of relS that
S
0
2 relS (O0 , ). By Theorems 25, 31, 35, 40, 49 and 53, we have M (O ,) S O0 , and
S
0
therefore M (O ,) |= . By the definition of a module (cf. Definition 19) it is immediate
S
0
S
that O0  O implies M (O ,)  M (O,) . Finally, by minimality of O0 , we have
S
S
0
M (O ,) = O0 and therefore O0  M (O,) .
We conclude by addressing the eect of normalisation on these properties. Similarly
to our treatment of normalisation in Proposition 13 from Section 3, we show that we can
recover a depleting and justification-preserving module for a SROIQ ontology O from one
such module for its normalisation norm(O).
Proposition 61. Let S be an inseparability relation, and let
be a module setting family
that is S-admissible and depleting. Let norm be a normalisation function. Let O be a
SROIQ ontology and let M  O be such that the following holds:
534

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning

1. M

(norm(O),)

 norm(M) and

2. norm(O\M)  norm(O)\norm(M).

Then, M is a depleting and justification-preserving S -module of O.

Proof. Let O0 = norm(O). Proposition 13 implies that M is a S -module of O0 .
0
We next show that M is depleting. Since M (O ,)  norm(M), we have O0 \norm(M) 
0
0
O0 \M (O ,) . Proposition 59 implies that O0 \M (O ,) S ;, and hence by monotonicity of
first-order logic also O0 \norm(M) = norm(O\M) S ;. Since norm(O\M) is a conservative
extension of O\M, by Definition 4 we have norm(O\M) S O\M, and thus O\M S ;.
To show that M is justification-preserving, let ' 2 relS (O, ) and consider a justification O  O of ' in O. Suppose there is some  2 O\M. Then  2 O\M
0
and norm()  norm(O\M) = O0 \norm(M). Since M (O ,)  norm(M), this implies
0
norm() \ M (O ,) = ;. On the other hand, since norm(O) is a conservative extension
of O, we have ' 2 relS (norm(O), ). Also, because norm(O) is a conservative extension
of O, we have norm(O) |= ' and there must be a justification O0 of ' in norm(O)  O0 .
0
0
By Proposition 60, M (O ,) is justification-preserving, and consequently O0  M (O ,) .
Furthermore, by minimality of O there is no proper subset of O whose normalisation in0
cludes O0 . It follows that norm() \ O0 6= ; and hence norm() \ M (O ,) 6= ;. This is
a contradiction that stems from assuming that  2 O \ M. Therefore O  M, i.e., M is
justification-preserving.
9.2 Self-Containment and Strong Depletingness
In contrast to locality-based modules, the modules obtained using our approach are neither
strongly depleting, nor self-contained. To see this, consider the following example.
Example 62. Let  = {A, D} and O = {r10 r15 } with
r10
r11
r12
r13
r14
r15

=
=
=
=
=
=

( ! A(o))
A(x) ! 9y.[R(x, y) ^ B(y)]
A(x) ! 9y.[R(x, y) ^ C(y)]
R(x, y) ! D(x)
B(x) ! C(x)
( ! C(i))

Let M1 = {r10 r13 } and M2 = {r11 r13 }. We can check that
M

f

=M

M i =M

q
c

=M

m

= M2

=M

wq

= M1

Clearly, O |= C(i) and M wq 6|= C(i) where C is in the signature of M1 ; hence, M wq is not
self-contained. Furthermore, M wq is not strongly depleting since O \M wq |= C(i). For the
remaining inseparability relations, observe that O |= B(x) ! C(x) and Mi 6|= B(x) ! C(x)
for 1  i  2. Since both B and C are in the signatures of M1 and M2 it follows that
none of our modules is self-contained (note that implications are relevant consequences for
all relations other than wq). Furthermore, since we also have that O \ Mi |= B(x) ! C(x),
we can conclude that our modules are also not strongly depleting.

535

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

Self-containment and strong depletingness are not always needed for applications. Thus,
the fact that our modules do not satisfy them by default can be beneficial as it may allow
us to compute smaller modules.
However, as mentioned in Section 3, these properties can be useful in certain ontology
reuse scenarios. We next show that our framework can be adapted so as to satisfy these
properties whenever they are required. This can be achieved via a fixpoint construction
where modules are computed w.r.t. iterative extensions of the initial signature. Such fixpoint
constructions are reminiscent of the standard algorithms for computing locality modules
(Cuenca Grau et al., 2007a, 2008).
Definition 63. Let S be a module setting family for an inseparability relation S. We
define the family Sself as the function mapping each O and  to the least fixpoint of the
sequence {Mi }i 0 as defined next:
0 = 
i = i

1

[ Sig(Mi

1)

Mi = M

for i > 0

S (O,

i)

for i

0


The aforementioned fixpoint is well-defined: since i   [ Sig(O) for each i 0 and
 [ Sig(O) is finite, there must be some i0 0 such that i0 = j and Mi0 = Mj for each
j > i0 . We show that, with this adaptation, our modules satisfy the required properties.
Proposition 64.

S
self

is self-contained and strongly depleting for each S 2 {m, q, f, i, c, wq}.

Proof. Let M = Sself (O, ). It is immediate that M is a self-contained S -module of O.
Strong depletingness of M follows from Proposition 59 for S 2 {m, q, f, i, c, wq}.
The construction in Definition 63 can straightforwardly be adapted to the case of nonnormalised SROIQ ontologies by following the same approach as in Proposition 61.

10. Complexity of Module Extraction
In this section, we argue that our modules can be efficiently computed in many practically
relevant cases. For this, we analyse the complexity of the following decision problem.
Definition 65. Let L be a class of ontologies and let S 2 {m, q, f, i, c, b, wq} be an inseparability relation. The decision problem isInModule[L,S] is as follows:
 Input: an ontology O 2 L, a signature  and a rule r 2 O.
 Output: True if and only if r 2 M

S (O,)

.



Furthermore, we consider the following classes of ontologies, which are strongly connected to DL-based ontology languages.
Definition 66. Let k be a fixed non-negative integer. The class Lkarity consists of all
ontologies where predicates have arity at most k.
The graph of a conjunction of atoms ' is the undirected graph G' = (V, E) such that V
is the set of variables occurring in ', and E contains an edge between each pair of variables
536

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning

that occur together in some atom in '. A tree decomposition of G' is a tree T = (W, F ),
such that there exists a labelling mapping each vertex w 2 W to some subset (w)  V ,
and the following conditions are satisfied:
 for each v 2 V , there exists v 2 W with v 2 (w),
 for each {v, v 0 } 2 E, there exists w 2 W with {v, v 0 }  (w), and
 for each v 2 V , the set { w 2 W | v 2 (w) } induces a (connected) subtree of T .
The width of the tree decomposition T is maxw2W (| (w)| 1). The treewidth of ' is the
minimum width over all the tree decompositions of G' . The treewidth of a rule is defined
as the treewidth of its body. Finally, the class Lktw consists of all ontologies where each rule
has treewidth at most k.

The rules correspong to SROIQ ontologies are not only of fixed predicate arity, but
also their bodies are tree-shaped (see Section 2.2). The latter implies that rules stemming
from SROIQ ontologies have treewidth at most one.
As already discussed, an appealing feature of our approach is that module extraction
can be delegated to an o-the-shelf datalog reasoner, regardless of the language in which
ontologies are expressed. The following proposition establishes that both the datalog program and the initial dataset exploited in our approach are of polynomial size; furthermore,
the datalog transformation  in the definition of a module setting (see Definition 19 in
Section 5) does not alter the shape of rules in the original ontology in any significant way.
Proposition 67. Let O be an ontology and   Sig(O) a signature. Furthermore, let
S 2 {m, q, f, i, c, b, wq} and S (O, ) = = h, D0 , Dr i. Then, P and D0 are of size linear
in |O|. Furthermore, if O is in Lkarity (resp. in Lktw ) for some fixed k, then so is P .
Proof. It is clear from Definition 5 that  (r) contains a datalog rule for each atom in the
head of r. Thus, P is clearly of size linear w.r.t. |O|. Furthermore, D0 contains one fact
for each predicate in , and since   Sig(O), it follows that D0 is also of size linear w.r.t.
|O|. Finally, the transformation  does not increase the arity of predicates and the body
of each rule  (r) coincides with that of r and hence preserves its treewidth.
The computational properties of datalog programs of bounded arity and/or treewidth
are well-understood: fact entailment is NP-complete in combined complexity for programs
of bounded arity, and the complexity drops to PTime if we additionally restrict ourselves to
programs of bounded treewidth. Bounded arity of predicates implies that the corresponding
materialisation is polynomially bounded in size, and thus can be computed in a polynomial
number of steps (i.e., applications of the immediate consequence operator); furthermore,
bounded treewidth of rule bodies implies that each such step can be performed in polynomial
time (Grohe, Schwentick, & Segoufin, 2001; Chekuri & Rajaraman, 2000).
The complexity of module extraction, however, is not only determined by that of datalog
reasoning, but also by the complexity of computing the support of all proofs involved in a
relevant entailment.
The following theorem, proved by Zhou et al. (2014), establishes that computing such
support can also be reduced to standard datalog reasoning. Given a program P, a dataset
537

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

D, and a set of facts F , the main idea is to extend P and D with additional rules and
facts that are responsible for computing the support of all proofs of facts from F in P [ D.
Such support is recorded by means of fresh predicates: auxiliary predicates Q are used
to record relevant facts in D of the form Q(c); furthermore, each rule r 2 P is represented
by a fresh constant dr , and a fresh unary predicate Rel is used to capture the relevant rules
from P in the support.
Theorem 68 (Zhou et al., 2014). Let P be a datalog program, let D be a dataset, and
let F be a set of facts in the materialisation of P [ D. Let Rel be a fresh unary predicate
and, for each predicate Q occurring in P [ D, let Q be a fresh predicate of the same arity.
Furthermore, let dr be a fresh constant for each r 2 P.
Let (D, F ) be the dataset (D, F ) = D [ { P (c) | P (c) 2 F } and let (P) be the
smallest
V datalog program including P and containing all of the following rules for each
r= m
j=1 B1 (xj ) ! H(x) in P:
H(x) ^ B1 (x1 ) ^ . . . ^ Bm (xm ) !
H(x) ^ B1 (x1 ) ^ . . . ^ Bm (xm ) !

Rel(dr )
Bj (x1 )
..
.

H(x) ^ B1 (x1 ) ^ . . . ^ Bm (xm ) ! Bj (xm )
Then, a rule s 2 P is in the support of some proof in P [ D of a fact from F i Rel(ds )
is in the materialisation of (P) [ (D, F ).
The datalog program (P) (resp. the dataset (D, F )) in Theorem 68 is of size polynomial in |P| (resp. in |D| and |F |) and does not use any predicates with arity greater than
that of predicates used in P. However, (P) may have a larger treewidth than P. We next
argue that in the case of SROIQ ontologies the increase in treewidth is bounded.
Proposition 69. Let O be a normalised SROIQ ontology and   Sig(O) a signature.
Furthermore, let be a module setting for O and . Then (P ) has treewidth at most 2.
Proof. For each rule r 2 O whose head is formed only by (one or more) atoms that are
unary, or mention no more than one variable, it is straightforward that ( (r)) still
consists only of rules with tree-shaped bodies. For each r 2 O that does not mention more
than two variables, it is also straightforward that ( (r)) also consists only of rules with
tree-shaped bodies. Finally, if r mentions more than two variables, and its head contains
atoms that mention more than one variable, then r must be of one of the following forms:
V
W
 A(x) ^ m+1
i=1 [R(x, yi ) ^ B(yi )] !
i6=j yi  yj
Then, the bodies of the rules in ( (r)) are of the form
A(x) ^

m+1
^
i=1

[R(x, yi ) ^ B(yi )] ^ yi1  yi2

1  i 1 < i2  m

Hence, they have treewidth 2 due to the cycle of length 3 formed by R(x, yi1 ), R(x, yi2 )
and yi1  yi2 .
538

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning

 R1 (x, y) ^ R2 (y, z) ! S(x, z)
The body of the single rule in ( (r)) will be of the form R1 (x, y)^R2 (y, z)^S(x, z),
which has treewidth 2 due to the cycle of length 3 formed by its three atoms.
 The cases involving equality in the body of rules, namely x  y ^ y  z ! x  z,
A(x1 , x2 ) ^ x1  y ! A(y, x2 ), and A(x1 , x2 ) ^ x2  y ! A(x1 , y), are analogous to
the previous case.
The following theorem establishes two practically relevant cases for which module extraction in our framework can be performed in polynomial time. We first show that modules
M m ensuring model-inseparability are computable in polynomial time for arbitrary ontologies and signatures. Then, we establish that modules M for the remaining inseparability
relations considered in this paper are also computable in polynomial time for all classes of
ontologies whose extended datalog program (P ) in Definition 68 can be bounded in both
predicate arity and treewidth.
Theorem 70. Let L be a class of ontologies, S 2 {m, q, f, i, c, wq}, and LS,
class of datalog programs:
LS, = { (P

S (O,)

the following

) | O 2 L,   Sig(O) }

The problem isInModule[L,S] is decidable in polynomial time if either of the following conditions is satisfied:
 S = m, or
0

 LS,  Lkarity \ Lktw for some fixed non-negative integers k and k 0 .
Proof. Consider an arbitrary ontology O and a signature . Let S (O, ) = h, D0 , Dr i
S
P = P (O,) , and F = Dr \P(D0 ). By Proposition 67, P and D0 can be computed in time
polynomial in the size of O, and so can (P). Therefore, it suffices to show that (D0 , F )
can be obtained in polynomial time and so can the materialisation of (P) [ (D0 , F ).
If S = m then Dr and (D0 , F ) can be computed in linear time. Furthermore, it is easy
to see that computing the materialisation of (P) [ (D0 , F ) is also feasible in linear time
since it boils down to propositional datalog reasoning.
0
0
Consider now S 2 {q, f, i, c, wq}. Note that (P) 2 Lkarity \Lktw implies P 2 Lkarity \Lktw .
Hence, P(D0 ) can be computed in time polynomial in the size of O. Since F is always a
set of facts in P(D0 ) over predicates from , it can be computed in polynomial time.
Moreover, the dataset (D0 , F ) can be computed in polynomial time as well and thus,
0
since (P) 2 Lkarity \ Lktw , so can the materialisation of (P) [ (D0 , F ).
Tractability of module extraction w.r.t. SROIQ ontologies is now an immediate consequence of Theorem 70 and Proposition 69.
Corollary 71. Let S 2 {m, q, f, i, c, wq} and let O be a normalised SROIQ ontology. The
module M S is computable in polynomial time.
539

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

11. Optimality
As already discussed, in general, our modules are not minimal for their corresponding
inseparability relation. It is, however, of interest to determine which module setting families
yield the smallest possible modules for a given inseparability relation within the limits of
our framework. To this end, we next present and study a suitable notion of optimality
applicable to module setting families.
Our notion of module setting family in Definition 57 is rather general in that it does
not establish any relationship between the dierent module settings in the family. In order to study optimality, it makes sense to restrict ourselves to families satisfying certain
uniformity conditions. Roughly speaking, we consider a family as uniform if (i) existentially quantified variables and constants in ontologies are treated homogeneously within a
setting (i.e., dierent existential variables receive the same treatment, and so do dierent
constants) as well as consistently across dierent settings; and (ii) signatures are treated
monotonically across settings (i.e., if and 0 are members of a family for some ontology
O and signatures  and 0 with   0 , then they treat predicates in  and Sig(O)\0 in
exactly the same way).
Definition 72. A module setting family is uniform if, for each pair of ontologies O, O0
and signatures , 0 , the module settings (O, ) = h, D0 , Dr i and (O0 , 0 ) = h0 , D00 , Dr0 i
satisfy the following properties, where Ex(F) denotes the set of existentially quantified
variables in F:
1. If  = 0 , |Ct(O)|  |Ct(O0 )| and |Ex(O)|  |Ex(O0 )|, then there exists an injective
substitution  : dom() ! dom(0 ) mapping variables to variables and constants to
constants such that
  = 0 ,

 D0 = { A(c) | A(c) 2 D00 , c has only constants from Ct( (O, )) }, and
 Dr = { A(c) | A(c) 2 Dr0 , c has only constants from Ct( (O, )) }.
2. If O = O0 and   0 , then
  = 0 ,

 D0 = { A(c) 2 D00 | A 2  },

 { A(c) 2 Dr | A 2  } = { A(c) 2 Dr0 | A 2  }, and

 { A(c) 2 Dr | A 2 Sig(O)\0 } = { A(c) 2 Dr0 | A 2 Sig(O)\0 }.
For S an inseparability relation, let S be the class of all uniform module setting families
0
that are S-admissible. We say that is S-optimal if 2 S and M (O,)  M (O,) for
every 0 2 S and each pair of O and .

It is easy to see that each of the S-admissible families S , with S 2 {m, q, f, i, c, wq},
is uniform. Furthermore, the following theorem establishes that m , i , c , and wq are
also optimal for their respective inseparability relations. The proof of the theorem is rather
technical and is deferred to the appendix.
540

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning

Theorem 73. The family

S

is S-optimal for S 2 {m, i, c, wq}.

In contrast, the families f and q for fact and query inseparability are not optimal.
To see this, consider the ontology O consisting of the following rules:
A(x) ! B(x)

B(x) ! A(x)

C(x) ! D(x)

Furthermore, let  = {A, C, D}. The module setting f = f (O, ) yields M f = O.
Indeed, for D = {A(a)} we have a non-trivial proof of A(a) in O [ D that involves rules
A(x) ! B(x) and B(x) ! A(x), which are then included in the module. However, it is
clear that M = {C(x) ! D(x)} is already -fact inseparable from O.
We now define a dierent setting = h, D0 , Dr i, whose corresponding module is precisely M. For this, the idea is to define D0 and Dr in such a way that the aforementioned
proofs of tautological statements are avoided. Consider D0 and Dr as follows:
D0 = {X(c0Y ) | X, Y 2 } [ {X(c1Y ) | X, Y 2  and X 6= Y }
Dr = {Y (c1Y ) | Y 2 }

Datasets D0 and Dr are disjoint, which guarantees that proofs of -tautologies are not taken
into account and therefore M is indeed M. The construction of D0 and Dr given for this
example ontology and signature can be generalised so as to define a uniform module setting
family that provides a counter-example to the optimality of f . There is, however, a price
to pay for such smaller modules, namely an increase in the size of module settings. Indeed,
(O, ) is of size exponential in , whereas the size of f (O, ) remains polynomial. Such
exponential blowup is clearly undesirable in practice.
The following theorem establishes that f and q are not optimal. They do, however,
work well in practice, as shown in the evaluation presented in Section 13. The proof of the
theorem works by proposing better module setting families which, in both cases, incur the
aforementioned exponential blowup. We conjecture that such a blowup is unavoidable in
any optimal module setting family for fact or query inseparability (if such a family exists).
As in the case of Theorem 73, the proof is technical and is deferred to the appendix.
Theorem 74. The family

S

is not S-optimal for S 2 {f, q}.

12. Related Work
Module extraction has received a great deal of attention in the literature. In Section 12.1
we discuss the complexity of inseparability checking for dierent ontology languages and
inseparability relations. In Section 12.2 we recapitulate existing module extraction techniques based on inseparability, and provide a brief overview of their practical applications.
Finally, in Section 12.3 we discuss a number of related problems, such as forgetting, uniform
interpolation, and partition-based reasoning.
12.1 Inseparability Relations
Inseparability relations originate in the notions of model and deductive conservative extensions for description and modal logics (Antoniou & Kehagias, 2000; Ghilardi, Lutz, &
541

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

Wolter, 2006a; Ghilardi, Lutz, Wolter, & Zakharyaschev, 2006b), and constitute the foundation of module extraction techniques (Konev et al., 2009).
Model inseparability is undecidable for all DLs that extend EL (Lutz & Wolter, 2010). It
is, however, tractable for ELI ontologies that are acyclic (Konev et al., 2013); furthermore,
it is coNexpTimeNP -complete for the description logic ALCI if signatures are restricted
N
to consist of atomic concepts only (Konev et al., 2013). For DL-LiteN
bool and DL-Litehorn
it is coNexpTime-hard (Kontchakov et al., 2010), but no matching upper bound is known
to the best of our knowledge.
The complexity of query inseparability has been studied mainly for lightweight description logics. It is ExpTime-complete for EL (Lutz & Wolter, 2010), p2 -complete and
N
coNP-complete for DL-LiteN
bool and DL-Litehorn , respectively (Kontchakov et al., 2010),
H
and ExpTime-complete for DL-LiteH
core and DL-Litehorn (Konev, Kontchakov, Ludwig,
Schneider, Wolter, & Zakharyaschev, 2011; Botoeva et al., 2014). Baader et al. (2010)
considered a variant of query inseparability where the signature of datasets is restricted to
 but the signature of queries is not, and identified decidable sufficient conditions for such
inseparability in ELI, which can be checked in polynomial time for EL. The complexity
of weak query inseparability (see Section 7.2) was studied by Botoeva et al. (2014); it is
known to be P-complete for DL-Litecore , DL-Litehorn and ELH, ExpTime-complete for
H
DL-LiteH
core and DL-Litehorn , and 2ExpTime-complete for Horn-ALCHI and Horn-ALCI.
The complexity of implication and classification inseparability coincides with that of
standard reasoning tasks such as subsumption checking (Konev et al., 2009). In the description logic literature implication inseparability has been studied in a more general form:
given L-ontologies O and O0 and a signature , the problem is to determine whether O
and O0 entail the same concept inclusion axioms C v D where C and D are (possibly complex) L-concepts over . In this setting, inseparability has been found to be p2 -complete for
N
DL-LiteN
bool , coNP-complete for DL-Litehorn (Kontchakov et al., 2010), ExpTime-complete
for EL (Lutz & Wolter, 2010), 2ExpTime-complete for ALC (Ghilardi et al., 2006a), ALCI,
ALCQ and ALCQI (Konev et al., 2009), and undecidable for ALCQIO (Konev et al., 2009).
Note that this variant of implication inseparability is highly dependent on the ontology language, whereas our results are largely logic-independent. We leave the investigation of such
inseparability relations within our framework as an interesting problem for future work.
12.2 Module Extraction
Practical module extraction techniques are typically based on approximations, which ensure
that the computed module is (model) inseparable from the given ontology, yet not necessarily minimal. One such approximation, which we discussed in detail in Section 3, is based
on syntactic locality (Cuenca Grau et al., 2007a, 2008; Sattler et al., 2009). An implementation of ?-, >- and ?> -module extraction is integrated in the OWL API (Horridge &
Bechhofer, 2011), and an alternative implementation can be downloaded as a separate Java
library.3 The semantic counterpart of syntactic locality, semantic locality, was proposed in
the work of Cuenca Grau et al. (2007b). Deciding semantic locality is, for any given DL,
as hard as checking satisfiability w.r.t. the empty TBox, and hence only tractable for logics with restricted expressivity. For this reason modules based on syntactic locality, which
3. https://www.cs.ox.ac.uk/isg/tools/ModuleExtractor/

542

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning

can be extracted in polynomial time, have been the preferred choice in practice. Furthermore, an exhaustive comparison of syntactic and semantic locality modules revealed that
the dierence between them is not significant in most practical cases (Del Vescovo et al.,
2013).
Reachability-based modules (Suntisrivaraporn, 2008; Nortje, Britz, & Meyer, 2012;
Nortje et al., 2013) are a refinement of syntactic locality modules. Available in the same
three flavours (?-, >- and ?> -reachability), they can also be computed in polynomial time.
While ?-reachability modules coincide with ?-modules, >- and ?> -reachability modules
are generally subsets of their syntactic locality counterparts. This refinement comes at
the cost of losing depletingness, although reachability modules are still self-contained and
preserve all justifications for consequences over the reference signature .
Konev et al. (2013) and Gatens et al. (2014) developed module extraction techniques
for acyclic ELI and acyclic ALCQI, respectively. These techniques ensure that modules
are self-contained and depleting, and in the case of ELI also minimal. The polynomial
algorithm for ELI is implemented in the system MEX, the more general, non-tractable
algorithm for ALCQI is implemented in the system AMEX. In contrast to locality and
reachability modules, the applicability of these techniques is limited to a relatively restricted
class of ontologies, and tractability is only guaranteed for an even more restricted class.
Kontchakov et al. (2010) exploited the decidability of query inseparability for DL-LiteN
bool
and DL-LiteN
horn for module extraction. Their techniques yield minimal or minimal depleting modules and have the same complexity as the corresponding inseparability relation
(p2 -complete and coNP-complete, respectively).
Baader et al. (2010) proposed exponential-time algorithms for extracting modules from
ELI ontologies that preserve a variant of query inseparability. Furthermore, they showed
that computing such modules is feasible in polynomial time for EL ontologies.
Recently, Rousset and Ulliana (2015) studied modularity in the context of deductive
triple stores, that is, RDF triple stores equipped with a set of datalog rules. The preservation
properties of such modules, however, are very dierent from the ones considered in our work.
Del Vescovo et al. (2011) considered the problem of finding a polynomial representation
of all modules of an ontology, for a particular notion of module. The proposed representation is called atomic decomposition and is applicable to any notion of a module that satisfies
certain properties that include self-containment and depletingness. The atomic decomposition of an ontology for a suitable notion of module can be computed in polynomial time
using a module extraction algorithm as an oracle.
Module extraction has been identified as a key task to support knowledge reuse (Cuenca
Grau et al., 2008; Jimenez-Ruiz et al., 2008). Modules have also been exploited to optimise
ontology matching (Jimenez-Ruiz & Cuenca Grau, 2011) and the computation of justifications (Suntisrivaraporn et al., 2008; Ludwig, 2014) for ontology debugging and explanation.
Finally, module extraction techniques have been applied to optimising ontology classification
(Armas Romero et al., 2012; Tsarkov & Palmisano, 2012; Suntisrivaraporn, 2008; Cuenca
Grau et al., 2010) and have been integrated in the reasoners MORe and Chainsaw.
543

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

12.3 Related Problems
Module extraction is strongly related to the notions of forgetting and (uniform) interpolation
(Eiter, Ianni, Schindlauer, Tompits, & Wang, 2006; Ludwig & Konev, 2014; Koopmann &
Schmidt, 2014; Konev, Walther, & Wolter, 2009; Nikitina & Rudolph, 2014; Wang, Wang,
Topor, & Pan, 2010). A uniform interpolant of an L-ontology O and a signature  is an
L-ontology O0 that only mentions symbols from  and which is inseparable from O w.r.t.
 for a given inseparability relation. In contrast to modules, uniform interpolants are not
required to be subsets of O and they cannot contain any symbol outside  (all remaining
symbols are thus forgotten). The latter requirement implies that uniform interpolants for
a given  and O may not always exist (Konev et al., 2009; Lutz & Wolter, 2011; Wang,
Wang, Topor, Pan, & Antoniou, 2014).
Amir and McIlraith (2005) investigated partition-based reasoning techniques in propositional and first-order logic, where the goal is to improve the efficiency of reasoning over
a knowledge base by first dividing its axioms into related partitions. The topology of the
partitions is described by means of a graph, where nodes represent partitions and an edge
between two partitions is labelled with the symbols they have in common. Such graph
structure is then exploited by a distributed message-passing algorithm, the correctness of
which is ensured by Craigs interpolation theorem for first-order logic. Similar problems and
reasoning techniques have been studied in the context of description logics by Konev, Lutz,
Ponomaryov, and Wolter (2010) as well as by Schlicht and Stuckenschmidt (2009). The key
concern in partition-based reasoning is to find a partitioning that exhibits a suitable balance between number of partitions, their size, and the number of common symbols between
partitions in order to enable efficient distributed reasoning. In this setting, partitions do
not necessarily capture the meaning of a given signature in the input knowledge base and
are therefore fundamentally dierent from modules.
Konev, Ludwig, Walther, and Wolter (2012) studied the problem of computing the logical
dierence of ontologies O and O0 that is, the set of of queries that receive dierent answers
w.r.t. O and O0 . Computing the logical dierence (or a concise representation thereof) has
been identified as a valuable resource for ontology versioning tasks (Jimenez-Ruiz, Cuenca
Grau, Horrocks, & Berlanga Llavori, 2011) and is closely related to inseparability checking;
indeed, inseparable ontologies are those that have an empty dierence.
Finally, Zhou et al. (2014) proposed a hybrid approach to ontology-based query answering where the bulk of the computation is delegated to a datalog reasoner. Given an ontology
O, dataset D , query q(x), and candidate answer tuple c, a core technique in this approach
is to compute fragments O0  O and D0  D such that O [ D |= q(c) i O0 [ D0 |= q(c).
Similarly to our modules, these fragments are computed by first strengthening O into a datalog program P and then exploiting the datalog reasoner to identify the axioms and facts
responsible for the validity of q(c). In contrast to query inseparable modules, however, the
fragment O0 [ D0 is only guaranteed to preserve the fixed query q(c) w.r.t. the fixed dataset
D, rather than all queries w.r.t. all datasets over a reference signature.

13. Implementation and Evaluation
In this section, we present our prototype module extraction system and discuss the results
of our evaluation on a suite of real-world ontologies.
544

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning

13.1 The PrisM System
We have implemented a prototype system for module extraction in Java, called PrisM,
which bundles RDFox as a black-box datalog reasoner (Motik et al., 2014). PrisM is
available online under academic license.4
PrisM accepts as input an OWL 2 ontology O, a signature  and a parameter S that
indicates the relevant inseparability relation. Our system currently supports the whole of
OWL 2, with the only exception of datatypes; furthermore, it supports the inseparability
relations S 2 {m, q, f, i, c, wq} defined in Sections 6 and 7.
PrisM computes an S-module Mout of O w.r.t.  according to the following steps.
1. Compute the normalisation norm(O) of O, where norm is the normalisation function
defined by the rules in Figure 2 from Section 2 (see also Proposition 3). Then, apply
the mapping  from Figure 1 to obtain an equivalent set of rules O0 = (norm(O)).
2. Consider the S-module setting S = (S , D0S , DrS ) for O0 and  as defined in Section 6
(for S 2 {m, q, f, i}) or 7 (for S 2 {c, wq}). Then, compute the corresponding module
M S as follows:
(a) Construct the datalog program P
D0S and DrS from S .

(b) Compute the support supp(

S

as specified in Definition 19, and the datasets

S)

by exploiting the result in Theorem 68. For this,

S)

as specified in Definition 19.

 compute the datalog program (P S ) and the dataset (D0S , DrS ) using the
transformation
defined in Theorem 68;
 construct the materialisation M at of (P S ) [ (D0S , DrS ) using RDFox;
and
 construct supp( S ) as the set of rules r 2 P S such that Rel(dr ) 2 M at

(c) Construct M

S

from supp(

3. Return Mout consisting of all axioms  2 O such that (norm()) \ M

S

6= ;.

The first step normalises the input OWL 2 ontology O into a set of rules O0 . PrisM provides
an optimisation where O0 is constructed from a locality-based module of O, rather than from
O itself. Specifically, we use the ?-module M?
[O,] if S is classification inseparability, and


?>
the ?> -module M[O,] for all other inseparability relations. This optimisation does not
compromise the correctness of the overall procedure since model inseparability is stronger
than all other inseparability relations.
The second step computes the relevant S-module M S for the set of rules O0 and signature . This is essentially done by following Definition 19; the only computationally
demanding part is the construction of the support supp( S ), which is achieved via materialisation using RDFox in a black-box manner.
Finally, the third step constructs the module Mout for the input ontology O from the
module M S for its corresponding set of rules O0 . For this, PrisM keeps track of the correspondence between the axioms in O and those in its normalisation norm(O). Correctness
of this step is ensured by Proposition 13.
4. http://www.cs.ox.ac.uk/isg/tools/PrisM/

545

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

ID
00001
00004
00024
00026
00029
00032
00347
00350
00351
00354
00463
00471
00477
00512
00545
00774
00775
00778
00786

name
ACGT-v1.0
BAMS-simplified
DOLCE
GALEN-no-FIT
GALEN-doctored
GALEN-undoctored
LUBM-one-uni
OBI
AERO
NIF-gross-anatomy
Fly-anatomy-XP
FMA-lite
Gazetteer
Lipid
Molecule-role
RNA-v0.2
Roberts-family
SNOMED
NCI-v12.04e

predicates
2,019
1,199
603
29,073
3,740
3,762
68
2,965
355
4,166
8,047
78,986
150,981
1,289
9,222
338
183
54,982
93,628

rules
5,512
18,976
2,148
66,191
7,447
7,818
84,771
10,952
669
7,134
42,107
168,828
382,158
5,222
153,020
938
2,020
191,891
193,453

disjunctive
105
0
53
0
0
0
0
77
11
51
0
0
0
541
0
34
1
18,323
65

existential
259
16,782
184
26,973
2,367
2,715
8
1,168
100
1,506
9,433
42,734
156,743
893
6,276
90
73
60,377
76,957

expressivity
SROIQ(D)
ALEHIF +
SHOIN (D)
ALEH
ALEHIF +
ALEHIF +
ALEHI + (D)
SHOIN (D)
SROIQ(D)
SROIF(D)
ALERI +
ALEH+
ALE +
ALCHIN
ALE +
SRIQ(D)
SROIQ(D)
SH
SH(D)

Table 3: Test ontologies
13.2 Evaluation
We have evaluated our system on a set of test ontologies identified in the work of Glimm,
Horrocks, Motik, Stoilos, and Wang (2014) as non-trivial for reasoning. All ontologies
have been normalised prior to module extraction to make DL axioms equivalent to rules.
Further details on these ontologies are given in Table 3.5 The first and second columns in
the table indicate the ontology ID and name in the Oxford Ontology Repository. The third
and fourth columns provide the number of predicates and rules in the resulting ontology
after normalisation. The fifth and sixth columns specify how many of these rules contain
disjunction and existential quantification in the head. Finally, the last column indicates the
DL expressivity6 of the normalised ontology as given by the the OWL API.
All experiments were performed on a server with 2 Intel Xeon E5-2670 2.60GHz processors, each of which has 8 physical cores that serve 2 virtual cores each, making a total of 32
virtual cores. In our experiments we allocated 90GB of RAM, and RDFox was always run
on 16 threads. We have compared the module sizes and extraction times using our system
with those for locality-based ?- and ?> -modules, computed using the OWL API. We have
followed the experimental methodology in the work of Del Vescovo et al. (2013), where two
kinds of signatures are considered:
 genuine signatures, which correspond to the signature of individual axioms, and
 random signatures, which include the signatures of several axioms.
5. The ontologies used in our experiments are available for download at https://krr-nas.cs.ox.ac.uk/
2015/jair/PrisM/testOntologies.zip.
6. We refer the reader to the work of Baader et al. (2003) for a detailed account on DL naming conventions.

546

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning

Unlike the work of Del Vescovo et al. (2013), who defined random signatures simply as
random subsets of the ontology signature, we extracted such signatures using a randomised
graph sampling algorithm. We first represented the syntactic dependencies between symbols
in the (normalised) ontology as a graph, and then traversed the graph in a randomised way
until we visited a set number n of nodes. The symbols corresponding to the visited nodes
were then taken as a random signature.7 The advantage of this approach is that it yields
signatures that are semantically connected, which we believe is likely to be the case in
practical applications. The number n was chosen by default as 0.1% of the total graph and
then increased by up to two orders of magnitude in cases where the resulting signatures
typically contained less than 15 predicates and thus were too small to provide additional
information w.r.t. genuine signatures.
For each kind of signature and each ontology, we have considered a sample of 400
runs and averaged module sizes and module extraction times. On the one hand, we have
compared the modules produced by c (Section 7) with ?-modules. which are the only
kind of modules in the literature that guarantee our notion of classification inseparability.
On the other hand, we have compared the modules produced by m , q , wq , f , and

i (Sections 6 and 7) with ?> -modules. As discussed in Section 12.2, no other system
is (to the best of our knowledge) capable of computing modules specific to the deductive
inseparability relations considered in this paper. Furthermore, other module extraction
systems that ensure model inseparability, such as MEX and AMEX, are only applicable to
rather restricted ontology languages. Consequently, ?> -modules seemed the best available
option for comparison to our approach.
Tables 4 and 5 provide the average number of rules in each kind of module for genuine and
random signatures, respectively. In both tables, the total number of rules in the normalised
ontology is provided at the top for comparison purposes, whereas the average size of the
signatures considered is specified towards the bottom. Table 5 additionally includes the
percentage n of the dependency graph covered by the random walks from which random
signatures were obtained.
We can observe that module size consistently decreases as we consider weaker inseparability relations. The modules produced by c can be several orders of magnitude smaller
than ?-modules, as in the cases of 00463, 00471, 00477 or 00545. Although those are rather
extreme cases, we observed in most cases at least a 25% decrease in size (see 00026, 00029,
00032, 00347, 00350, 00351, 00786). Our modules for model inseparability improve reasonably on ?> -modules in most cases, although the greatest dierence in size is of course
between ?> -modules and i -modules, reaching one order of magnitude for some ontologies
(see 00471 and 00477, and also 0004, 00463 and 00786 with genuine signatures). In realistic
ontologies, only a very small proportion of predicate pairs are related by atomic implication,
and this is often still the case when considering a datalog overestimation of the ontology;
thus, the dierence in size between ?> -modules and i is rather unsurprising. There is
naturally also a big dierence in size between wq -modules and all other modules whenever
ontologies do not mention any constants, since the former are in that case obviously empty.
It is worth observing that, even though there are several cases where m modules and q
modules are of very similar size (e.g. 00471), there are also cases where they dier signifi7. The functionality required to perform random walks is currently integrated in RDFox.

547

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

total
?
c

>?
m
q
wq
f
i

||

00001
5,512
678
558
674
584
563
514
563
558
2

00004
18,976
18,306
16,942
18,297
18,297
17,151
0
17,108
655
3

00024
2,148
1,000
883
990
910
884
875
884
882
2

00463
00471
00477
total 42,107 168,828 382,158
? 22,348 47,192 214,820
112
12
<1
c
>?
221
20
9
217
12
8
m
107
12
8
q
0
0
0
wq
80
1
<1
f
12
1
<1
i
||
3
2
3

00026
66,191
14,253
9,799
13,749
13,686
9,448
0
5,962
3,279
3

00029
7,447
187
94
114
112
96
0
96
18
3

00032
7,818
690
479
596
592
533
0
533
130
3

00347
84,771
84,726
23,651
58,186
44,244
44,368
43,761
31,322
11,234
2

00350
10,952
803
558
768
624
596
538
596
558
2

00351
669
133
74
130
97
77
64
77
67
2

00512
00545
5,222 153,020
261 143,399
86
6
34
2
32
1
29
1
0
0
29
<1
27
<1
2
3

00774
938
80
76
80
80
78
0
78
76
2

00775
00778
00786
2,020 191,891 193,453
1,916
433
1,140
1,491
426
390
1,913
427
1,138
1,498
426
1,138
1,492
426
385
1,490
0
0
1,492
426
371
1,491
397
120
2
3
3

00354
7,134
826
618
786
675
626
111
626
617
2

Table 4: Average module sizes for genuine signatures.
cantly (e.g. 00786). Similarly, q modules and f modules have similar size in some cases
(e.g. 00350) but not in others (e.g. 00471), and the same happens with q and wq (exemplified by ontologies 00775 and 00354), and with f and i (see ontologies 00512 and 00004).
These observations suggest that our modules faithfully reflect the dierences between the
inseparability relations we considered, and that they could oer significant advantages for
practical applications
Tables 6 and 7 provide the average module extraction time (in milliseconds) for genuine
and random signatures, respectively. The extraction of our modules is consistently slower
than that of locality-based modules; however, the average extraction time rarely exceeds
1 minute, and is very often below 10 seconds (especially for genuine signatures). This
suggests that our modules are feasible for practical applications. Furthermore, since most
of the extraction time is invariably spent by the datalog reasoner, which is used as a black
box, future advancements in the area of datalog reasoning can lead to further performance
gains for systems implementing our technique.

14. Conclusion and Future Work
In this paper, we have proposed a novel approach to module extraction based on a reduction
to datalog reasoning. In contrast to existing techniques, our approach is not only applicable
to description logics, but also to highly expressive first-order rule formalisms. Furthermore,
our techniques can be easily customised so as to capture a wide range of inseparability
relations studied in the literature. In all cases our modules satisfy many desirable properties,
548

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning

total
?
c

>?
m
q
wq
f
i

||
%

00001
5,512
857
691
854
759
736
517
735
688
43
1

00004
18,976
18,904
17,607
18,894
18,894
18,579
0
18,536
2,511
82
1

00024
2,148
1,053
933
1,044
964
942
875
942
931
20
1

00463
00471
00477
total 42,107 168,828 382,158
? 23,139 49,345 215,886
595
402
38
c
>?
982
1,658
1,050
973
1,450
1,049
m
757
1,450
1,049
q
0
0
0
wq
664
74
16
f
333
74
16
i
||
28
154
312
%
0.1
0.1
0.1

00026
66,191
27,771
17,879
27,184
27,175
18,315
0
18,255
17,646
107
0.1

00029
7,447
1,890
1,223
1,726
1,719
1,380
0
1,364
1,060
104
1

00032
7,818
3,279
2,483
3,108
3,101
2,633
0
2,620
2,314
107
1

00347
84,771
84,732
58,203
80,153
63,031
63,031
62,455
58,980
50,362
11
10

00350
10,952
1,795
1,084
1,758
1,611
1,389
539
1,389
1,080
92
1

00351
669
315
208
311
274
265
81
241
191
56
10

00512
00545
5,222 153,020
1,555 143,448
1,199
28
837
16
819
14
774
14
0
0
766
5
467
5
66
19
1
0.1

00774
938
371
338
371
369
368
0
368
338
58
10

00775
00778
00786
2,020 191,891 193,453
1,979 11,766 16,820
1,527 11,342
7,974
1,977 11,762 16,817
1,561 11,651 16,817
1,557 11,644
8,969
1,506
0
0
1,557 11,342
8,415
1,526 11,342
6,228
42
202
326
10
0.1
0.1

00354
7,134
1,537
1,240
1,501
1,388
1,279
113
1,278
1,238
79
1

Table 5: Average module sizes for random signatures.
which makes them well-suited to applications such as ontology reuse, debugging, modular
ontology development, and reasoning optimisation. Last, but not least, our modules can
be efficiently computed by reusing o-the-shelf datalog reasoners and our experimental
evaluation confirms their suitability in practice.
We envisage many directions for future work, which we outline next.
 State-of-the-art modular DL reasoners, such as MORe and Chainsaw, currently
rely on ?-modules to split the workload between a fully-fledged OWL reasoner and
an efficient reasoner for a lightweight DL. It would be natural to exploit our modules
for classification inseparability (cf. Section 7.1) instead of ?-modules since all such
reasoners focus mainly on classification tasks. We believe that using our modules for
modular reasoning will significantly improve the separation of workload and lead to a
better use of the lightweight reasoner.
 So far, the use of modules for optimising data reasoning tasks, such as fact entailment
and query answering, has been rather limited. Indeed, it is well-known that ?-modules
are not well-suited for such tasks (Cuenca Grau et al., 2008). PAGOdA is the only
reasoning system we know of that exploits techniques akin to module extraction for
data reasoning (Zhou et al., 2014, 2015). It would be interesting to investigate how
our techniques could be exploited to improve PAGOdAs performance. Furthermore,
549

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

?

c

>?
m
q
wq
f
i

?

c

>?
m
q
wq
f
i

00001
27
846
43
831
857
857
846
847

00004
64
36,784
95
18,244
62,680
32,972
62,797
35,158

00024
11
1,066
19
1,013
1,074
1,069
1,074
1,072

00463
122
4,176
199
775
485
455
463
456

00471 00477
506 1,154
9,729 52,529
805 1,851
1,543 4,995
824 1,792
790 1,753
788 1,772
792 1,759

00026
273
30,256
418
25,884
29,051
27,272
28,254
28,499

00029
24
295
39
231
241
229
246
234

00032
27
989
42
826
903
861
910
890

00347
274
14,823
463
15,268
14,688
10,677
14,495
10,285

00350
95
747
124
724
798
787
784
789

00351
3
130
5
126
136
140
131
132

00512 00545
18
363
378 34,705
30
616
166 2,783
225
595
210
552
229
580
229
579

00774
5
161
11
147
161
154
164
164

00775
10
963
18
857
955
966
967
965

00778
722
1,657
1,056
1,590
1,708
1,612
1,700
1,691

00786
569
3,449
899
3,340
3,475
3,419
3,526
3,479

00354
31
6,588
43
722
3,530
767
3,840
3,826

Table 6: Average extraction times in milliseconds for genuine signatures.

?

c

>?
m
q
wq
f
i

?

c

>?
m
q
wq
f
i

00001
26
1,076
101
1,011
1,064
1,056
1,058
1,078

00004
58
36,376
84
18,114
171,497
32,474
179,759
34,866

00024
10
1,162
18
1,077
1,132
1,153
1,122
1,144

00026
284
56,125
457
49,792
55,343
51,342
54,854
54,068

00029
35
2,989
54
2,675
2,814
2,670
2,785
2,765

00032
39
5,737
60
4,900
5,533
5,170
5,477
5,368

00347
276
15,708
485
20,885
20,654
15,390
20,822
15,063

00350
107
2,065
144
1,936
2,077
2,010
2,067
2,108

00351
4
423
9
384
423
390
417
427

00463
138
5,441
192
1,615
1,431
1,332
1,391
1,382

00471
549
13,291
793
3,202
2,669
2,638
2,640
2,664

00477
1,172
51,707
1,768
6,073
3,191
3,157
3,157
3,223

00512
23
20,712
37
2,188
2,939
2,756
2,964
2,891

00545
342
31,616
576
2,504
591
567
569
562

00774
5
768
10
640
750
715
748
767

00775
10
1,064
20
967
1,046
1,028
1,047
1,048

00778
841
34,980
1,210
20,409
34,330
20,293
33,565
34,774

00786
698
44,463
1,070
41,283
43,785
43,007
44,604
44,168

00354
29
13,664
45
1,467
8,591
1,591
8,358
8,376

Table 7: Average extraction times in milliseconds for random signatures.
we also envision potential applications to incremental and stream reasoning, where
data is frequently changing but queries and ontologies can be seen as fixed.
 Our conjecture that optimal module setting families for fact and query inseparability
incur an exponential blowup w.r.t. the ones that we have chosen remains open.
550

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning

 The use of ?-modules to exploit debugging and explanation systems for DL ontologies
has proved rather successful (Suntisrivaraporn et al., 2008). Given that our modules
are also justification-preserving, it would be interesting to evaluate the eectiveness
of our modules for implication inseparability in this setting.
 Finally, the use of module extraction techniques has so far been largely restricted
to description logics. Our techniques are, however, widely applicable and could be
exploited in a number of reasoning tasks for ontology languages such as datalog and
datalog,_ , which are currently gaining momentum.

Acknowledgments
This paper is an extended version of our conference publication (Armas Romero, Kaminski,
Cuenca Grau, & Horrocks, 2015). This work has been supported by the Royal Society under
a University Research Fellowship, by the EPSRC projects Score!, MaSI3 , and DBOnto, and
by the EU FP7 project Optique. We would like to thank the anonymous referees for their
valuable comments and suggestions.

Appendix A. Proofs for Section 11
Theorem 73. The family

S

is S-optimal for S 2 {m, i, c, wq}.

We prove the result for each S 2 {m, i, c, wq} separately.
Theorem 75.

m

is m-optimal.

Proof. Suppose m is not m-optimal. Then there must be some m-admissible, uniform
m
family and some O and  such that M (O,) 6 M (O,) . Let m (O, ) = hm , D0m , Drm i
and (O, ) = h, D0 , Dr i. By Theorem 55, we have m (O, ) 6,! (O, ), hence for each
mapping  : Ct( m (O, )) ! Ct( (O, )) it must be either m  6=  or D0m  6 D0 or
Drm  6 Dr .
Suppose  is such that there are two existentially quantified variables y1 and y2 in O
such that y1  6= y2 . Consider the ontology O0 consisting of the following rules:
p1
p2
p3
p4

:
:
:
:

A(x) ! S(x, b)
A(x) ! 9y1 [R(x, y1 ) ^ B(y1 )]
A(x) ! 9y2 [R(x, y2 ) ^ C(y2 )]
S(x, b) ^ R(x, z) ^ B(z) ^ C(z) ! D(x)

and the signature 0 = {A, D, R} and let (O0 , 0 ) = h0 , D00 , Dr0 i. By the second property
of uniformity, 0 is the same as the substitution in (O0 , ;) and (O0 , ), and therefore,
by the first property of uniformity, y1 0 6= y2 0 . It follows that p4 can never be applied on
0
0
0
0
P (O , ) (D00 ) and hence it is not in the support of (O0 , 0 ) and M (O , )  {p1 , p2 , p3 }.
The interpretation I such that I = {i}, AI = B I = C I = {i}, DI = ;, aI = i and
0
0
RI = S I = {(i, i)} is a model of M (O , ) , but it cannot be extended to a model of
0
0
O0 without changing the interpretation of A, D or R. It follows that M (O , ) is not
551

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

0
a m
is m-admissible. The origin of
0 -module of O , contradicting our hypothesis that
this contradiction is in the assumption that  does not map all existentially quantified
variables to the same constant, therefore there must be some constant c such that y = c
for each variable y universally quantified in O. Suppose now that  is such that there exists
a constant b0 in O such that b0  6= c. By the first property of uniformity 0 must also
not map b to the same constant as y1 and y2 . But then again p4 can never be applied
0
0
0
0
on P (O , ) (D00 ) and M (O , )  {p1 , p2 , p3 }, which, as we have already shown, is a
contradiction. Consequently,  must map all constants in O to c as well.
This means that there exists some substitution  : Ct( m (O, )) ! Ct( (O, )) such
that m  = . In particular it must be  = c. By hypothesis, it must be the case that
either D0m  6 D0 or Drm  6 Dr .
Suppose D0m  6 D0 . There must be some predicate X 2  such that X(c, . . . , c) 2
/ D0 .
00
Consider the ontology O consisting of the following rules:

p5 : X(x, . . . , x) ! 9yR(x, y)
p6 : R(x, x) ! A(x)
and the signature 00 = {X, A}, and let (O00 , 00 ) = h00 , D000 , Dr00 i. By uniformity of , 00
maps y to c, and the fact X(c, . . . , c) is not in the initial dataset of (O, {X}), or in that
of (O, 00 ), or in D000 . Consider the dataset
D = {A(a), A(c)} [ { X(t) | t 2 {a, c}arity(X) , t 6= (c, . . . , c) }
with a a fresh constant. The substitution  that maps c to itself and all other constants in
(O00 , 00 ) to a is a homomorphism from (O00 , 00 ) to  = h00 , D,  (Dr00 )i. By Theorem 55,
00
00
this implies M (O , )  M  . Clearly, the only R-fact in P  (D) is R(a, c), so p5 is not
00
00
in the support of  and therefore M (O , )  M   {p5 }. The interpretation J such
0
00
that J = {i}, X J = (i, . . . , i), AJ = ; and RJ = {(i, i)} is a model of M (O , ) , but
it cannot be extended to a model of O00 without changing the interpretation of X or A. It
00
00
00
follows that M (O , ) is not a m
00 -module of O , which contradicts our hypothesis that
is m-admissible. This contradiction stems from the assumption that D0m  6 D0 , therefore
it must be D0m   D0 .
By hypothesis, this implies Drm  6 Dr , so there must be some predicate Y 2  such
that Y (c, . . . , c) 2
/ Dr . Consider the ontology O000 consisting of the following rules:
p7 : A(x) ! 9yR(x, y)
p8 : R(x, x) ! Y (x, . . . , x)
and the signature 000 = {A, Y }, and let (O000 , 000 ) = h000 , D0000 , Dr000 i. By uniformity of
, 000 maps y to c. Consider the dataset D = {A(c), Y (c, . . . , c), A(b), Y (b, . . . , b)}, with
a again a fresh constant. The mapping  that maps c to itself and all other constants in
(O000 , 000 ) to b is a homomorphism from (O000 , 000 ) to  = h000 , D, Dr000 i. By Theorem 55,
000
00
this implies M (O , )  M  . It is easy to see that the only R-facts in the materialisation
of P  [ D are R(a, c) and R(c, c), so the only proof in P  [ D supported by p8 is a proof
of Y (c, . . . , c). However, by uniformity of , Y (c, . . . , c) is not in the relevant facts of
(O, {Y }), or in those of (O, {A, Y }), or in Dr000 . It follows that p8 is not in the support
000
000
of  and M (O , )  M   {p7 }. The interpretation K such that K = {i}, AK = {i},
552

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning

0

00

RK = {(i, i)} and Y K = ; is a model of M (O , ) , but it cannot be extended to a model
000
000
of O000 without changing the interpretation of A or Y . It follows that M (O , ) is not
000
a m
is m-admissible.
000 -module of O , which again contradicts our hypothesis that
The origin of this contradiction is in the assumption that Drm  6 Dr , therefore it must
be Drm   Dr . The mapping  is thus a witness that m (O, ) ,! (O, ), ultimately
contradicting the assumption that m is not m-optimal.
Theorem 76.

i

is i-optimal.

Proof. Consider the module setting family i0 such that for each pair of ontology O and
signature , i0 (O, ) = hi0 , D0i0 , Dri0 i is as follows:
 i0 = { y 7! cy | y existentially quantified in O } [ { c 7! c | c 2 Sig(O) a constant }
 D0i0 = { A(cA,B ) | A 6= B predicates in  of the same arity }
 Dri0 = { B(cA,B ) | A 6= B predicates in  of the same arity } [ Dri0?
where each cy is a fresh constant, cA,B = c1A,B , . . . , cnA,B , is an array of fresh constants for
each pair A, B 2  of distinct n-ary predicates, and Dri0? = {?} if  contains two distinct
predicates of the same arity and Dri0? = ; otherwise.
i
First, we show that i is i-optimal i i0 is i-optimal by proving that M (O,) =
i
M 0 (O,) for each O and . Let us fix arbitrary O and , and let i (O, ) = hi , D0i , Dri i
and i0 (O, ) = hi0 , D0i0 , Dri0 i. It is easy to see that i0 (O, ) ,! i (O, ), therefore
i
i
i
i
by Theorem 55 we have that M 0 (O,)  M (O,) . For M (O,)  M 0 (O,) , first
i (O,)
i (O,)
note that P
= P 0
. We can assume w.l.o.g. that  contains at least two
predicates A, B of the same arityotherwise there are no non-trivial -implications. Given
i
a proof  = (T, ) in P (O,) [ D0i of B(cA ) (resp. of ?) the proof 0 = (T, 0 ) such that
0 (v) = (v) for each node v in T , with  a substitution such that cA  = cA,B , is a proof
i
of B(cA,B ) (resp. of ?) in P 0 (O,) [ D0i0 satisfying supp(0 ) = supp(). Consequently,
i
i
supp( i (O, ))  supp( i0 (O, )) and hence M (O,)  M 0 (O,) .
Now, suppose i0 is not i-optimal. Then, there must be a uniform, i-admissible family
i
and some O and  such that M 0 (O,) 6 M (O,) . Let (O, ) = h, D0 , Dr i. Since
i0 is injective, there exists a mapping  : Ct( i0 (O, )) ! Ct( (O, )) such that i0  = .
This condition only determines the eect of  on dom(i0 ), and since dom(i0 ) is disjoint
with the set { cA,B | A 6= B predicates in  of the same arity }, we can asume that either
 is such that D0i0   D0 , or there are two distinct predicates A, B 2  of the same arity
such that D0 contains no A-facts. Suppose the latter is the case, and consider the ontology
O0 = {A(x) ! B(x)}. By uniformity of , the initial dataset of (O0 , ) also contains no
0
A-facts and therefore the support of (O0 , ) is empty and M (O ,) = ; 6|= A(x) ! B(x).
This contradicts the i-admissibility of , hence it must be D0i0   D0 . Finally, suppose
Dri0  6 Dr ; then in particular  must contain two distinct n-ary predicates, since otherwise
it is Dri0 = ; and thus trivially Dri0  6 Dr . In this case there are two possible situations:
 ?2
/ Dr .
Let A, B 2  be distinct predicates of the same arity and consider the ontology
553

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

O00 = {A(x) ! C(x) _ D(x), C(x) ! B(x), D(x) ! ?} with C and D fresh predicates. Clearly, O00 |= A(x) ! B(x) By uniformity of , ? is not in the set of
00
relevant facts of (O00 , ), and therefore D(x) ! ? 2
/ M (O ,) . Consequently we
00
have M (O ,) 6|= A(x) ! B(x), contradicting the assumption that is i-admissible.
 B(cA,B ) 2
/ Dr for some pair of distinct A, B 2  of the same arity.
Consider O000 = {A(x) ! B(x)}. By uniformity of , there are no B-facts in the set
00
of relevant facts of (O000 , {A, B}), and therefore M (O ,{A,B}) = ; |6 = A(x) ! B(x),
contradicting the assumption that is i-admissible.
It follows that Dri0   Dr , so  is a homomorphism from (O, ) to i0 (O, ), and thus
i
M 0 (O,)  M (O,) , which ultimately contradicts the assumption that i0 (resp. i ) is
not i-optimal.
Theorem 77.

c

is c-optimal.

Proof. Analogous to Theorem 76
Theorem 78.

wq

is wq-optimal.

Proof. Suppose wq is not wq-optimal. Then there must be some uniform and wq-admissible
wq
family
and some O and  s.t. M (O,) 6 M (O,) . Let (O, ) = h, D0 , Dr i and
wq (O, ) = h wq , D wq, D wq i. By Theorem 55, given a mapping  : Ct( wq ) ! Ct( ) it
r
0
must be either wq  6=  or D0wq  6 D0 or Drwq  6 Dr .
Since wq is injective and Dwq = ;, we can assume that  is such that wq  = ,
wq
D0   D0 and Drwq  6 Dr . But then there must be some predicate X 2  and an array c
of size arity(X) of constants from Ct(O)[{ cy | y exist. quant. in O } such that X(c) 2
/ Dr .
Consider the ontology O0 = {( ! X(c))}; clearly, O0 |= X(c). By uniformity, X(c) is also
0
not in the relevant facts of (O0 , ), which implies M (O ,) = ; 6|= X(c). This contradicts
wq
the wq-admissibility of , hence it must be Dr   Dr , which makes  a homomorphism.
It follows that wq is wq-optimal.
Theorem 74. The family

S

is not S-optimal for S 2 {f, q}.

Again, we prove the result for each S 2 {f, q} separately.
f

Proposition 79. The family

is not f-optimal.

Proof. Consider an arbitrary but fixed pair of O and . Let Ct(O) = {c1 , . . . , cn }. Furthermore, for each predicate B 2  and each array v 2 {1, . . . , arity(B) + n}arity(B) consider a
set of constants { iB,v | 0  i  arity(B) + n } such that
1. 0B,v , . . . , arity
B,v (B) are fresh constants and
arity(B)+n

2. arity
B,v (B), . . . , B,v

arity(B)+i

are such that B,v

= ci 2 Ct(O) for each 1  i  n.

w1
wn
Let w
B,v denote the array (B,v , . . . , B,v ) whenever w = (w1 , . . . , wn ), and consider the

uniform module setting family

f
0

such that

f (O, )
0

= hf0 , D0f0 , Drf0 i is as follows:

 f0 = { y 7! cy | y existentially quantified in O } [ { c 7! c | c 2 Ct(O) }
554

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning

arity(B) ,
 D0f0 = {A(w
B,v ) | A, B 2 , v 2 {1, . . . , arity(B) + n}

v
w 2 {0, . . . , arity(B) + n}arity(A) , A(w
B,v ) 6= B(B,v )}

 Drf0 = { B(vB,v ) | B 2 , v 2 {1, . . . , arity(B)}arity(B) } [ {?}
We now show that the family f0 is f-admissible. Consider a datalog rule r = ' ! 2
relf(O,) . W.l.o.g. we can assume that 2
/ 'otherwise r would be tautological and hence
entailed by any ontology. Let be a substitution mapping all variables in r to distinct
fresh constants. Since
2
/ ' and
is injective, we also have
2
/ ' . The fact
must be of the form B(c) with B 2  (if B = ? then it would be c = ;) and consider
an injective substitution mapping all constants in c [ Ct(O) to {1, . . . , arity(B) + n} and
satisfying c = arity(B) + i i c = ci 2 Ct(O). Let  be another substitution defined on all
constants in Sig(' [ ) and such that
 c
B,c if c 2 c [ Ct(O)
c =
0B,c otherwise
Note that  is compatible with f0 .
f0
arity(B) .
It is easy to see that ( ) = B(c
B,c ) 2 Dr since c 2 {1, . . . , arity(B) + n}
w
On the other hand, each fact in (' ) must be of the form A(B,(c) ) with A 2  and
w 2 {0, . . . , arity(B) + n}arity(A) . Because
2
/ ' and  is injective, it is easy to see that
c
w
A(B,c ) 6= B(B,c ). Consequently, (' )  Drf0 . By Theorem 23, it follows that O |= r
f

i M 0 (O,) |= r, hence f0 is f-admissible.
Finally, consider O = {A(x) ! B(x), B(x) ! A(x)} and  = {A}. It is easy to see
f
f
that M (O,) = O 6 ; = M 0 (O,) , and thus f is not f-optimal.
Proposition 80. The family

q

is not q-optimal.

Proof. Consider an arbitrary but fixed pair of O and . Let Ct(O) = {c1 , . . . , cn } and let
{y1 , . . . , ym } be the existentially quantified variables mentioned in O, and {cy1 , . . . , cym }
a corresponding set of fresh constants. Furthermore, for each predicate B 2  and each
array v 2 {1, . . . , arity(B) + n + m}arity(B) consider a set { iB,v | 0  i  arity(B) + n + m }
of constants such that
1. 0B,v , . . . , arity
B,v (B) are fresh constants,
arity(B)+n

2. arity
B,v (B), . . . , B,v
arity(B)+n+1

3. B,v

arity(B)+i

are such that B,v

arity(B)+n+m

, . . . , B,v

= ci 2 Ct(O) for each 1  i  n, and

arity(B)+n+i

are such that B,v

= cyi for each 1  i  m

w1
wn
Let w
B,v denote the array (B,v , . . . , B,v ) whenever w = (w1 , . . . , wn ), and consider the
uniform module setting family q0 such that q0 (O, ) = hq0 , D0q0 , Drq0 i is as follows:

 q0 = { y 7! cy | y existentially quantified in O } [ { c 7! c | c 2 Ct(O) }
arity(B) ,
 D0q0 = {A(w
B,v ) | A, B 2 , v 2 {1, . . . , arity(B) + n + m}

v
w 2 {0, 1, . . . , arity(B) + n}arity(A) , A(w
B,v ) 6= B(B,v )}

555

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

 Drq0 = { B(vB,v ) | B 2 , v 2 {1, . . . , arity(B) + n + m}arity(B) } [ {?}
We now show that the family q0 is q-admissible.
Consider a rule r = '(x) ! 9y (x, y) 2 relq (O, ), a justification O0 of r in O and a
q
rule s 2 O0 . To show that q0 (O, ) is q-admissible it suffices to show that s  M 0 (O,) .
Wn
We can assume w.l.o.g. that ? 2
/ ' (otherwise r would be tautological) and = i=1 i
with m > 0 and each i a conjunction of atoms. Similarly to the proof of Theorem 23,
consider a fresh predicate Q and the ontology
OQ = {

i (x, y)

! Q(x) | 1  i  n }

as well as a module setting Q = hq0 , D0Q , DrQ i for O [ OQ and , satisfying in particular
q
P Q = P 0 (O,) [OQ . As argued in the proof of Theorem 23, given s 2 O0 and a substitution
mapping variables in x to fresh distinct constants, there must be a proof  = (T, ) of
Q(x) in (O [ OQ ) [ ' such that (s) \ supp() 6= ;. Furthermore, thanks to O0 being
a justification, we can assume that  is laconic, and in particular for each leaf node v 2 T
and each ancestor w of v in T it must be (v) 6 (w).
q
By Lemma 21, either s = ? !  or there exists a proof 0 = (T 0 , 0 ) in P 0 (O,) [
(' )q0 of either Q(x) or ? that is embeddable into  modulo q0 and such that supp(0 ) \
q
 0 (O,) (s) 6= ;.
If s = ? !  then there must be some rule s0 in O0 such that ? 2 Sig(O0 ). As we show
next, when considering the case s 6= ? ! , it must be s0 2 M . Therefore ? 2 Sig(M ),
and consequently s = ? !  2 M .
Otherwise, if 0 = (T, ) is a proof of Q(x) , as discussed in the proof of Theorem 23, a
q
rule from  0 (O,) (r) must
be used in some subproof 00 = (T 00 , 00 ) of 0 that is a proof of
S
( 0 )q0 for some 2 i i and some extension 0 of to y. Let v be the root of T 00 and
w1 , . . . , wn its leaves, by definition it must be 00 (v) = ( 0 )q0 .
q
If v is also a leaf in T 00 then there must be a rule of the form ( ! ( 0 )q0 ) in  0 (O,) (s),
and all the terms in ( 0 )q0 must be constants from the domain of q0 . This implies
q
( 0 )q0 2 Drq0 , and consequently ( ! ( 0 )q0 ) 2 supp( q0 (O, )) and s 2 M 0 (O,) .
Suppose v is not a leaf of T 00 . First of all, note that (' )q0 = ' due to 0q not
modifying constants and ' not using functional terms, and therefore 00 (wi ) 2 ' for each
0 then (
0 ) q0 2
i. If there are functional terms in
/ ' , since both the constants cy and
the constants in the range of were fresh by hypothesis, and therefore 00 (wi ) 6= 00 (v) for
each i. Suppose now that there are no functional terms in 0 . Then 0 q0 = 0 . We know
that 0 , and hence 00 , is embeddable into  modulo q0 . It follows that there must exist
some node v 0 2 T that is not a leaf of T and such that (v)  (v 0 )q0 , and also a collection
0 nor ' 0
of leaves w10 , . . . , wn0 of T such that 00 (wi ) = (wi0 )q0 2 ' q0 . Since neither
q
use functional terms, and 0 does not modify constants, it must in fact be (v)  (v 0 ) and
00 (w ) = (w 0 ) for each i. Furthermore, we had that (w ) 6 (v 0 ) for each i, which implies
i
i
i
0 mentioning
that, also in this case 00 (wi ) 6= 00 (v) for each i. Therefore, regardless of
00
00
0
functional terms, we have S
(wi ) 6= (v) for each i. Now, the fact
must be of the form
B(c) with B 2  since 2 i i and by hypothesis r 2 relq (O, ). Let  be an injective
substitution mapping all constants in c [ range() to {1, . . . , arity(B) + n + m} satisfying
(i) c = arity(B) + i i c = ci 2 Ct(O) and (ii) c = arity(B) + n + i i c = cyi . Let  be
556

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning

another substitution defined on all constants mentioned by 00 and such that
 c
B,c if c 2 c [ range()
c =
0B,c otherwise
Note that  is compatible with . Since c 2 {1, . . . , arity(B) + n + m}arity(B) , it is easy
f0
to see that 00 (v) = B(c
B,c ) 2 Dr . On the other hand, as we have previously observed,
00
no (wi ) mentions constants from the set {cy1 , . . . , cym }, hence 00 (wi ) must be of the
arity(A) . Furthermore, because
form A(w
B,(c) ) with A 2  and w 2 {0, . . . , arity(B) + n}

00 (v) for each i, it follows that also 00 (w ) 6= 00 (v) and thus 00 (w ) 2 D f0
6
=
i
i
0
for each i. The proof 00  = (T 00 , 00 ) such that 00 (v) = ( 00 (v)) is clearly a proof in
q
q
P 0 (O,) [ D0q0 of 00 (v) that has the same support as 00 , therefore s 2 M 0 (O,) .
q
Finally, if 0 is a proof of ? then it must be a proof in P 0 (O,) . Since by assumption it
is ? 2
/ ', the labels of all the leaves in 0 must also be dierent from ?. To check that also
q
in this case s 2 M 0 (O,) , it suffices to follow a similar argument to the one above with a
mapping  0 defined on all constants mentioned in 0 and such that

c
if c 2 Ct(O) [ {cy1 , . . . , cym }
c =
0
B,c otherwise
00 (w

i)

We have proved that the family q0 is q-admissible. Now we will use it to show
that q is not q-optimal. As in the proof for Theorem 79, if we consider the ontology O = {A(x) ! B(x), B(x) ! A(x)} and the signature  = {A}, we can observe that
q
q
M (O,) = O 6 ; = M 0 (O,) , and thus q is not q-optimal.

References
Abiteboul, S., Hull, R., & Vianu, V. (1995). Foundations of Databases. Addison-Wesley.
Alviano, M., Faber, W., Leone, N., & Manna, M. (2012). Disjunctive datalog with existential
quantifiers: Semantics, decidability, and complexity issues. Theory and Practice of
Logic Programming, 12 (4-5), 701718.
Amir, E., & McIlraith, S. A. (2005). Partition-based logical reasoning for first-order and
propositional theories. Artificial Intelligence, 162 (1-2), 4988.
Antoniou, G., & Kehagias, A. (2000). A note on the refinement of ontologies. International
Journal of Intelligent Systems, 15 (7), 623632.
Armas Romero, A., Cuenca Grau, B., & Horrocks, I. (2012). MORe: Modular combination of
OWL reasoners for ontology classification. In Cudre-Mauroux, P., Heflin, J., Sirin, E.,
Tudorache, T., Euzenat, J., Hauswirth, M., Parreira, J. X., Hendler, J., Schreiber, G.,
Bernstein, A., & Blomqvist, E. (Eds.), Proceedings of the 11th International Semantic
Web Conference, Part I, Vol. 7649 of Lecture Notes in Computer Science, pp. 116.
Springer.
Armas Romero, A., Kaminski, M., Cuenca Grau, B., & Horrocks, I. (2015). Ontology module
extraction via datalog reasoning. In Bonet, B., & Koenig, S. (Eds.), Proceedings of
the 29th AAAI Conference on Artificial Intelligence, pp. 14101416. AAAI Press.
557

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

Baader, F., Bienvenu, M., Lutz, C., & Wolter, F. (2010). Query and predicate emptiness
in description logics. In Lin, F., Sattler, U., & Truszczynski, M. (Eds.), Proceedings
of the 12th International Conference on Principles of Knowledge Representation and
Reasoning. AAAI Press.
Baader, F., Brandt, S., & Lutz, C. (2005). Pushing the EL envelope. In Kaelbling, L. P.,
& Saffiotti, A. (Eds.), Proceedings of the 19th International Joint Conference on Artificial Intelligence, pp. 364369. Professional Book Center.
Baader, F., Calvanese, D., McGuinness, D. L., Nardi, D., & Patel-Schneider, P. F. (Eds.).
(2003). The Description Logic Handbook: Theory, Implementation, and Applications.
Cambridge University Press.
Bachmair, L., & Ganzinger, H. (2001). Resolution theorem proving. In Robinson, J. A.,
& Voronkov, A. (Eds.), Handbook of Automated Reasoning, pp. 1999. Elsevier and
MIT Press.
Botoeva, E., Kontchakov, R., Ryzhikov, V., Wolter, F., & Zakharyaschev, M. (2014). Query
inseparability for description logic knowledge bases. In Baral, C., Giacomo, G. D.,
& Eiter, T. (Eds.), Proceedings of the 14th International Conference on Principles of
Knowledge Representation and Reasoning. AAAI Press.
Bourhis, P., Morak, M., & Pieris, A. (2013). The impact of disjunction on query answering
under guarded-based existential rules. In Rossi, F. (Ed.), Proceedings of the 23rd
International Joint Conference on Artificial Intelligence. IJCAI/AAAI.
Bry, F., Eisinger, N., Eiter, T., Furche, T., Gottlob, G., Ley, C., Linse, B., Pichler, R., & Wei,
F. (2007). Foundations of rule-based query answering. In Antoniou, G., Amann, U.,
Baroglio, C., Decker, S., Henze, N., Patranjan, P., & Tolksdorf, R. (Eds.), Proceedings
of the 3rd International Reasoning Web Summer School, Tutorial Lectures, Vol. 4636
of Lecture Notes in Computer Science, pp. 1153. Springer.
Cal, A., Gottlob, G., Lukasiewicz, T., Marnette, B., & Pieris, A. (2010). Datalog+/-: A
family of logical knowledge representation and query languages for new applications.
In Proceedings of the 25th Annual ACM/IEEE Symposium on Logic in Computer
Science, pp. 228242. IEEE Computer Society.
Chekuri, C., & Rajaraman, A. (2000). Conjunctive query containment revisited. Theoretical
Computer Science, 239 (2), 211229.
Cuenca Grau, B., Halaschek-Wiener, C., Kazakov, Y., & Suntisrivaraporn, B. (2010). Incremental classification of description logics ontologies. Journal of Automated Reasoning,
44 (4), 337369.
Cuenca Grau, B., Horrocks, I., Kazakov, Y., & Sattler, U. (2007a). Just the right amount:
Extracting modules from ontologies. In Williamson, C. L., Zurko, M. E., PatelSchneider, P. F., & Shenoy, P. J. (Eds.), Proceedings of the 16th International World
Wide Web Conference, pp. 717726. ACM.
Cuenca Grau, B., Horrocks, I., Kazakov, Y., & Sattler, U. (2007b). A logical framework for
modularity of ontologies. In Veloso, M. M. (Ed.), Proceedings of the 20th International
Joint Conference on Artificial Intelligence, pp. 298303.
558

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning

Cuenca Grau, B., Horrocks, I., Kazakov, Y., & Sattler, U. (2008). Modular reuse of ontologies: Theory and practice. Journal of Artificial Intelligence Research, 31, 273318.
Cuenca Grau, B., Horrocks, I., Krotzsch, M., Kupke, C., Magka, D., Motik, B., & Wang, Z.
(2013). Acyclicity notions for existential rules and their application to query answering
in ontologies. Journal of Artificial Intelligence Research, 47, 741808.
Cuenca Grau, B., Horrocks, I., Motik, B., Parsia, B., Patel-Schneider, P. F., & Sattler, U.
(2008). OWL 2: The next step for OWL. Journal of Web Semantics, 6 (4), 309322.
Dantsin, E., Eiter, T., Gottlob, G., & Voronkov, A. (2001). Complexity and expressive
power of logic programming. ACM Computing Surveys, 33 (3), 374425.
Del Vescovo, C., Klinov, P., Parsia, B., Sattler, U., Schneider, T., & Tsarkov, D. (2013).
Empirical study of logic-based modules: Cheap is cheerful. In Alani, H., Kagal, L.,
Fokoue, A., Groth, P. T., Biemann, C., Parreira, J. X., Aroyo, L., Noy, N. F., Welty,
C., & Janowicz, K. (Eds.), Proceedings of the 12th International Semantic Web Conference, Part I, Vol. 8218 of Lecture Notes in Computer Science, pp. 84100. Springer.
Del Vescovo, C., Parsia, B., Sattler, U., & Schneider, T. (2011). The modular structure of
an ontology: Atomic decomposition. In Walsh, T. (Ed.), Proceedings of the 22nd International Joint Conference on Artificial Intelligence, pp. 22322237. IJCAI/AAAI.
Eiter, T., Ianni, G., Schindlauer, R., Tompits, H., & Wang, K. (2006). Forgetting in managing rules and ontologies. In Proceedings of the 2006 IEEE / WIC / ACM International
Conference on Web Intelligence, pp. 411419. IEEE Computer Society.
Gatens, W., Konev, B., & Wolter, F. (2014). Lower and upper approximations for depleting
modules of description logic ontologies. In Schaub, T., Friedrich, G., & OSullivan,
B. (Eds.), Proceedings of the 21st European Conference on Artificial Intelligence, Vol.
263 of Frontiers in Artificial Intelligence and Applications, pp. 345350. IOS Press.
Ghilardi, S., Lutz, C., & Wolter, F. (2006a). Did I damage my ontology? A case for conservative extensions in description logics. In Doherty, P., Mylopoulos, J., & Welty, C. A.
(Eds.), Proceedings of the 10th International Conference on Principles of Knowledge
Representation and Reasoning, pp. 187197. AAAI Press.
Ghilardi, S., Lutz, C., Wolter, F., & Zakharyaschev, M. (2006b). Conservative extensions in
modal logic. In Governatori, G., Hodkinson, I. M., & Venema, Y. (Eds.), Proceedings
of the 6th Advances in Modal Logic Conference, pp. 187207. College Publications.
Glimm, B., Horrocks, I., Motik, B., Stoilos, G., & Wang, Z. (2014). HermiT: An OWL 2
reasoner. Journal of Automated Reasoning, 53 (3), 245269.
Grohe, M., Schwentick, T., & Segoufin, L. (2001). When is the evaluation of conjunctive
queries tractable?. In Vitter, J. S., Spirakis, P. G., & Yannakakis, M. (Eds.), Proceedings of the 33rd Annual ACM Symposium on Theory of Computing, pp. 657666.
ACM.
Horridge, M., & Bechhofer, S. (2011). The OWL API: A java API for OWL ontologies.
Semantic Web, 2 (1), 1121.
Horridge, M., Parsia, B., & Sattler, U. (2008). Laconic and precise justifications in OWL.
In Sheth, A. P., Staab, S., Dean, M., Paolucci, M., Maynard, D., Finin, T. W., &
559

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

Thirunarayan, K. (Eds.), Proceedings of the 7th International Semantic Web Conference, Vol. 5318 of Lecture Notes in Computer Science, pp. 323338. Springer.
Horrocks, I., Kutz, O., & Sattler, U. (2006). The even more irresistible SROIQ. In Doherty,
P., Mylopoulos, J., & Welty, C. A. (Eds.), Proceedings of the 10th International Conference on Principles of Knowledge Representation and Reasoning, pp. 5767. AAAI
Press.
Horrocks, I., Patel-Schneider, P. F., & van Harmelen, F. (2003). From SHIQ and RDF to
OWL: the making of a web ontology language. Journal of Web Semantics, 1 (1), 726.
Jimenez-Ruiz, E., & Cuenca Grau, B. (2011). LogMap: Logic-based and scalable ontology
matching. In Aroyo, L., Welty, C., Alani, H., Taylor, J., Bernstein, A., Kagal, L., Noy,
N. F., & Blomqvist, E. (Eds.), Proceedings of the 10th International Semantic Web
Conference, Part I, Vol. 7031 of Lecture Notes in Computer Science, pp. 273288.
Springer.
Jimenez-Ruiz, E., Cuenca Grau, B., Horrocks, I., & Berlanga Llavori, R. (2011). Supporting
concurrent ontology development: Framework, algorithms and tool. Data & Knowledge
Engineering, 70 (1), 146164.
Jimenez-Ruiz, E., Cuenca Grau, B., Sattler, U., Schneider, T., & Berlanga Llavori, R.
(2008). Safe and economic re-use of ontologies: A logic-based methodology and tool
support. In Bechhofer, S., Hauswirth, M., Homann, J., & Koubarakis, M. (Eds.),
Proceedings of the 5th European Semantic Web Conference, Vol. 5021 of Lecture Notes
in Computer Science, pp. 185199. Springer.
Kalyanpur, A., Parsia, B., Horridge, M., & Sirin, E. (2007). Finding all justifications of OWL
DL entailments. In Aberer, K., Choi, K., Noy, N. F., Allemang, D., Lee, K., Nixon,
L. J. B., Golbeck, J., Mika, P., Maynard, D., Mizoguchi, R., Schreiber, G., & CudreMauroux, P. (Eds.), Proceedings of the 6th International Semantic Web Conference
and 2nd Asian Semantic Web Conference, Vol. 4825 of Lecture Notes in Computer
Science, pp. 267280. Springer.
Kalyanpur, A., Parsia, B., Sirin, E., & Cuenca Grau, B. (2006). Repairing unsatisfiable
concepts in OWL ontologies. In Sure, Y., & Domingue, J. (Eds.), Proceedings of
the 3rd European Semantic Web Conference, Vol. 4011 of Lecture Notes in Computer
Science, pp. 170184. Springer.
Kalyanpur, A., Parsia, B., Sirin, E., & Hendler, J. A. (2005). Debugging unsatisfiable classes
in OWL ontologies. Journal of Web Semantics, 3 (4), 268293.
Konev, B., Kontchakov, R., Ludwig, M., Schneider, T., Wolter, F., & Zakharyaschev, M.
(2011). Conjunctive query inseparability of OWL 2 QL tboxes. In Burgard, W., &
Roth, D. (Eds.), Proceedings of the 25th AAAI Conference on Artificial Intelligence.
AAAI Press.
Konev, B., Ludwig, M., Walther, D., & Wolter, F. (2012). The logical dierence for the
lightweight description logic EL. Journal of Artificial Intelligence Research, 44, 633
708.
Konev, B., Lutz, C., Ponomaryov, D. K., & Wolter, F. (2010). Decomposing description
logic ontologies. In Lin, F., Sattler, U., & Truszczynski, M. (Eds.), Proceedings of
560

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning

the 12th International Conference on Principles of Knowledge Representation and
Reasoning. AAAI Press.
Konev, B., Lutz, C., Walther, D., & Wolter, F. (2009). Formal properties of modularisation.
In Stuckenschmidt, H., Parent, C., & Spaccapietra, S. (Eds.), Modular Ontologies:
Concepts, Theories and Techniques for Knowledge Modularization, Vol. 5445 of Lecture
Notes in Computer Science, pp. 2566. Springer.
Konev, B., Lutz, C., Walther, D., & Wolter, F. (2013). Model-theoretic inseparability and
modularity of description logic ontologies. Artificial Intelligence, 203, 66103.
Konev, B., Walther, D., & Wolter, F. (2009). Forgetting and uniform interpolation in largescale description logic terminologies. In Boutilier, C. (Ed.), Proceedings of the 21st
International Joint Conference on Artificial Intelligence, pp. 830835.
Kontchakov, R., Lutz, C., Toman, D., Wolter, F., & Zakharyaschev, M. (2011). The combined approach to ontology-based data access. In Walsh, T. (Ed.), Proceedings of
the 22nd International Joint Conference on Artificial Intelligence, pp. 26562661. IJCAI/AAAI.
Kontchakov, R., Wolter, F., & Zakharyaschev, M. (2010). Logic-based ontology comparison
and module extraction, with an application to DL-Lite. Artificial Intelligence, 174 (15),
10931141.
Koopmann, P., & Schmidt, R. A. (2014). Count and forget: Uniform interpolation of SHQontologies. In Demri, S., Kapur, D., & Weidenbach, C. (Eds.), Proceedings of the 7th
International Joint Conference on Automated Reasoning, Vol. 8562 of Lecture Notes
in Computer Science, pp. 434448. Springer.
Krotzsch, M., Rudolph, S., & Hitzler, P. (2008a). Description logic rules. In Ghallab,
M., Spyropoulos, C. D., Fakotakis, N., & Avouris, N. M. (Eds.), Proceedings of the
18th European Conference on Artificial Intelligence, Vol. 178 of Frontiers in Artificial
Intelligence and Applications, pp. 8084. IOS Press.
Krotzsch, M., Rudolph, S., & Hitzler, P. (2008b). ELP: Tractable rules for OWL 2. In Sheth,
A. P., Staab, S., Dean, M., Paolucci, M., Maynard, D., Finin, T. W., & Thirunarayan,
K. (Eds.), Proceedings of the 7th International Semantic Web Conference, Vol. 5318
of Lecture Notes in Computer Science, pp. 649664. Springer.
Ludwig, M. (2014). Just: A tool for computing justifications w.r.t. ELH ontologies. In
Bail, S., Glimm, B., Jimenez-Ruiz, E., Matentzoglu, N., Parsia, B., & Steigmiller, A.
(Eds.), Proceedings of the 3rd International Workshop on OWL Reasoner Evaluation,
Vol. 1207 of CEUR Workshop Proceedings, pp. 17. CEUR-WS.org.
Ludwig, M., & Konev, B. (2014). Practical uniform interpolation and forgetting for ALC
tboxes with applications to logical dierence. In Baral, C., Giacomo, G. D., & Eiter, T.
(Eds.), Proceedings of the 14th International Conference on Principles of Knowledge
Representation and Reasoning. AAAI Press.
Lutz, C., & Wolter, F. (2010). Deciding inseparability and conservative extensions in the
description logic EL. Journal of Symbolic Computation, 45 (2), 194228.
561

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

Lutz, C., & Wolter, F. (2011). Foundations for uniform interpolation and forgetting in
expressive description logics. In Walsh, T. (Ed.), Proceedings of the 22nd International
Joint Conference on Artificial Intelligence, pp. 989995. IJCAI/AAAI.
Marnette, B. (2009). Generalized schema-mappings: From termination to tractability. In
Paredaens, J., & Su, J. (Eds.), Proceedings of the 28th ACM SIGMOD Symposium on
Principles of Database Systems, pp. 1322. ACM.
Motik, B. (2006). Reasoning in Description Logics Using Resolution and Deductive
Databases. Ph.D. thesis, Univesitat Karlsruhe (TH), Karlsruhe, Germany.
Motik, B., Nenov, Y., Piro, R., Horrocks, I., & Olteanu, D. (2014). Parallel materialisation
of datalog programs in centralised, main-memory RDF systems. In Brodley, C. E., &
Stone, P. (Eds.), Proceedings of the 28th AAAI Conference on Artificial Intelligence,
pp. 129137. AAAI Press.
Motik, B., Patel-Schneider, P. F., & Parsia, B. (2012). OWL 2 web ontology language
structural specification and functional-style syntax..
Nikitina, N., & Rudolph, S. (2014). (Non-)succinctness of uniform interpolants of general
terminologies in the description logic EL. Artificial Intelligence, 215, 120140.

Nonnengart, A., & Weidenbach, C. (2001). Computing small clause normal forms. In
Robinson, J. A., & Voronkov, A. (Eds.), Handbook of Automated Reasoning, pp. 335
367. Elsevier and MIT Press.
Nortje, R., Britz, K., & Meyer, T. (2012). A normal form for hypergraph-based module
extraction for SROIQ. In Gerber, A., Taylor, K., Meyer, T., & Orgun, M. (Eds.),
Proceedings of the 8th Australasian Ontology Workshop, Vol. 969 of CEUR Workshop
Proceedings, pp. 4051. CEUR-WS.org.
Nortje, R., Britz, K., & Meyer, T. (2013). Reachability modules for the description logic
SRIQ. In McMillan, K. L., Middeldorp, A., & Voronkov, A. (Eds.), Proceedings of the
19th International Conference on Logic for Programming, Artificial Intelligence and
Reasoning, Vol. 8312 of Lecture Notes in Computer Science, pp. 636652. Springer.
Robinson, J. A. (1965). Automatic deduction with hyper-resolution. International Journal
of Computer Mathematics, 1 (3), 227234.
Rousset, M.-C., & Ulliana, F. (2015). Extracting bounded-level modules from deductive
RDF triplestores. In Bonet, B., & Koenig, S. (Eds.), Proceedings of the 29th AAAI
Conference on Artificial Intelligence, pp. 268274. AAAI Press.
Sattler, U., Schneider, T., & Zakharyaschev, M. (2009). Which kind of module should I
extract?. In Grau, B. C., Horrocks, I., Motik, B., & Sattler, U. (Eds.), Proceedings of
the 22nd International Workshop on Description Logics, Vol. 477 of CEUR Workshop
Proceedings. CEUR-WS.org.
Schlicht, A., & Stuckenschmidt, H. (2009). Distributed resolution for expressive ontology
networks. In Polleres, A., & Swift, T. (Eds.), Proceedings of the 3rd International
Conference on Web Reasoning and Rule Systems, Vol. 5837, pp. 87101. Springer.
Schlobach, S., & Cornet, R. (2003). Non-standard reasoning services for the debugging of
description logic terminologies. In Gottlob, G., & Walsh, T. (Eds.), Proceedings of the
562

fiModule Extraction in Expressive Ontology Languages via Datalog Reasoning

18th International Joint Conference on Artificial Intelligence, pp. 355362. Morgan
Kaufmann.
Seidenberg, J., & Rector, A. L. (2006). Web ontology segmentation: Analysis, classification
and use. In Carr, L., Roure, D. D., Iyengar, A., Goble, C. A., & Dahlin, M. (Eds.),
Proceedings of the 15th International World Wide Web Conference, pp. 1322. ACM.
Stefanoni, G., Motik, B., & Horrocks, I. (2013). Introducing nominals to the combined
query answering approaches for EL. In desJardins, M., & Littman, M. L. (Eds.),
Proceedings of the 27th AAAI Conference on Artificial Intelligence. AAAI Press.
Stuckenschmidt, H., Parent, C., & Spaccapietra, S. (Eds.). (2009). Modular Ontologies:
Concepts, Theories and Techniques for Knowledge Modularization, Vol. 5445 of Lecture
Notes in Computer Science. Springer.
Suntisrivaraporn, B. (2008). Module extraction and incremental classification: A pragmatic
approach for EL+ ontologies. In Bechhofer, S., Hauswirth, M., Homann, J., &
Koubarakis, M. (Eds.), Proceedings of the 5th European Semantic Web Conference,
Vol. 5021 of Lecture Notes in Computer Science, pp. 230244. Springer.
Suntisrivaraporn, B., Qi, G., Ji, Q., & Haase, P. (2008). A modularization-based approach
to finding all justifications for OWL DL entailments. In Domingue, J., & Anutariya,
C. (Eds.), Proceedings of the 3rd Asian Semantic Web Conference, Vol. 5367 of Lecture
Notes in Computer Science, pp. 115. Springer.
Tsarkov, D., & Palmisano, I. (2012). Chainsaw: A metareasoner for large ontologies. In
Horrocks, I., Yatskevich, M., & Jimenez-Ruiz, E. (Eds.), Proceedings of the 1st International Workshop on OWL Reasoner Evaluation, Vol. 858 of CEUR Workshop
Proceedings. CEUR-WS.org.
W3C OWL Working Group (2012). OWL 2 web ontology language document overview
(second edition). W3C recommendation, World Wide Web Consortium.
Wang, K., Wang, Z., Topor, R. W., Pan, J. Z., & Antoniou, G. (2014). Eliminating concepts
and roles from ontologies in expressive descriptive logics. Computational Intelligence,
30 (2), 205232.
Wang, Z., Wang, K., Topor, R. W., & Pan, J. Z. (2010). Forgetting for knowledge bases in
DL-Lite. Annals of Mathematics and Artificial Intelligence, 58 (1-2), 117151.
Zhou, Y., Cuenca Grau, B., Horrocks, I., Wu, Z., & Banerjee, J. (2013). Making the most of
your triple store: Query answering in OWL 2 using an RL reasoner. In Schwabe, D.,
Almeida, V. A. F., Glaser, H., Baeza-Yates, R. A., & Moon, S. B. (Eds.), Proceedings
of the 22nd International World Wide Web Conference, pp. 15691580. International
World Wide Web Conferences Steering Committee / ACM.
Zhou, Y., Cuenca Grau, B., Nenov, Y., Kaminski, M., & Horrocks, I. (2015). PAGOdA:
Pay-as-you-go ontology query answering using a datalog reasoner. Journal of Artificial
Intelligence Research, 54, 309367.
Zhou, Y., Nenov, Y., Cuenca Grau, B., & Horrocks, I. (2013). Complete query answering
over horn ontologies using a triple store. In Alani, H., Kagal, L., Fokoue, A., Groth,
P. T., Biemann, C., Parreira, J. X., Aroyo, L., Noy, N. F., Welty, C., & Janowicz, K.
563

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

(Eds.), Proceedings of the 12th International Semantic Web Conference, Part I, Vol.
8218 of Lecture Notes in Computer Science, pp. 720736. Springer.
Zhou, Y., Nenov, Y., Cuenca Grau, B., & Horrocks, I. (2014). Pay-as-you-go OWL query
answering using a triple store. In Brodley, C. E., & Stone, P. (Eds.), Proceedings of
the 28th AAAI Conference on Artificial Intelligence, pp. 11421148. AAAI Press.

564

fiJournal of Artificial Intelligence Research 55 (2016) 317-359

Submitted 08/15; published 02/16

Adaptive Contract Design for Crowdsourcing Markets:
Bandit Algorithms for Repeated Principal-Agent Problems
Chien-Ju Ho

ch624@cornell.edu

Cornell University, Ithaca, NY, USA

Aleksandrs Slivkins

slivkins@microsoft.com

Microsoft Research, New York, NY, USA

Jennifer Wortman Vaughan

jenn@microsoft.com

Microsoft Research, New York, NY, USA

Abstract
Crowdsourcing markets have emerged as a popular platform for matching available
workers with tasks to complete. The payment for a particular task is typically set by the
tasks requester, and may be adjusted based on the quality of the completed work, for
example, through the use of bonus payments. In this paper, we study the requesters
problem of dynamically adjusting quality-contingent payments for tasks. We consider a
multi-round version of the well-known principal-agent model, whereby in each round a
worker makes a strategic choice of the effort level which is not directly observable by the
requester. In particular, our formulation significantly generalizes the budget-free online task
pricing problems studied in prior work. We treat this problem as a multi-armed bandit
problem, with each arm representing a potential contract. To cope with the large (and
in fact, infinite) number of arms, we propose a new algorithm, AgnosticZooming, which
discretizes the contract space into a finite number of regions, effectively treating each region
as a single arm. This discretization is adaptively refined, so that more promising regions
of the contract space are eventually discretized more finely. We analyze this algorithm,
showing that it achieves regret sublinear in the time horizon and substantially improves
over non-adaptive discretization (which is the only competing approach in the literature).
Our results advance the state of art on several different topics: the theory of crowdsourcing
markets, principal-agent problems, multi-armed bandits, and dynamic pricing.

1. Introduction
Crowdsourcing harnesses human intelligence and common sense to complete tasks that are
difficult to accomplish using computers alone. Crowdsourcing markets, such as Amazon Mechanical Turk and CrowdFlower, are platforms designed to match available human workers
with tasks to complete. Using these platforms, requesters may post tasks that they would
like completed, along with the amount of money they are willing to pay. Workers then
choose whether or not to accept the available tasks and complete the work.
Of course not all human workers are equal, nor is all human-produced work. Some tasks,
such as proofreading English text, are easier for some workers than others, requiring less
effort to produce high quality results. Additionally, some workers are more dedicated than
others, willing to spend extra time to make sure a task is completed properly. To encourage
high quality results, requesters may set quality-contingent bonus payments on top of the
base payment for each task, rewarding workers for producing valuable output. This can be
c
2016
AI Access Foundation. All rights reserved.

fiHo, Slivkins, & Vaughan

viewed as offering workers a contract that specifies how much they will be paid based on
the quality of their output.1
We examine the requesters problem of dynamically setting quality-contingent payments
for tasks. We consider a setting in which time evolves in rounds. In each round, the requester
posts a new contract, a performance-contingent payment rule which specifies different levels
of payment for different levels of output quality. A random, unidentifiable worker then
arrives in the market and strategically decides whether to accept the requesters task and
how much effort to exert; the choice of effort level is not directly observable by the requester.
After the worker completes the task (or chooses not to complete it), the requester observes
the workers output, pays the worker according to the offered contract, and adjusts the
contract for the next round. The properties of a random worker (formally: the distribution
over the workers types) are not known to the requester, but may be learned over time. The
goal of the requester is to maximize his expected utility, the value he receives from completed
work minus the payments made. We call it the dynamic contract design problem.
For concreteness, consider a special case in which a worker can strategically choose to
perform a task with low effort or with high effort, and the task may be completed either at
low quality or at high quality. The low effort incurs no cost and results in low quality, which
in turn brings no value to the requester. The high effort leads to high quality with some
positive probability (which may vary from one worker to another, and is unknown to the
requester). The requester only observes the quality of completed tasks, and therefore cannot
always infer the effort level. This example captures the two main tenets of our model: that
the properties of a random worker are unknown to the requester and that workers strategic
decisions are unobservable.
We treat the dynamic contract design problem as a multi-armed bandit (MAB) problem,
with each arm representing a potential contract. Since the action space is large (potentially
infinite) and has a well-defined real-valued structure, it is natural to consider an algorithm
that uses discretization. Our algorithm, AgnosticZooming, divides the action space into
regions, and chooses among these regions, effectively treating each region as a single metaarm. The discretization is defined adaptively, so that the more promising areas of the
action space are eventually discretized more finely than the less promising areas. While
the general idea of adaptive discretization has appeared in prior work on MAB (Kleinberg,
Slivkins, & Upfal, 2008; Bubeck, Munos, Stoltz, & Szepesvari, 2011a; Slivkins, 2014, 2011),
our approach to adaptive discretization is new and problem-specific. The main difficulty,
compared to this prior work, is that an algorithm is not given any information that links
the observable numerical structure of contracts and the expected utilities thereof.
To analyze performance, we propose a concept called width dimension which measures
how nice a particular problem instance is. We show that AgnosticZooming achieves
regret sublinear in the time horizon for problem instances with small width dimension.
In particular, if the width dimension is d, it achieves regret O(log T  T (d+1)/(d+2) ) after
T rounds. For problem instances with large width dimension, AgnosticZooming matches
the performance of the naive algorithm which uniformly discretizes the space and runs a
1. For some tasks, such as labeling websites as relevant to a particular search query or not, verifying the
quality of work may be as difficult as completing the task. These tasks can be assigned in batches, with
each batch containing one or more instances in which the correct answer is already known (often called
gold data). Quality-contingent payments can then be based on the known instances.

318

fiAdaptive Contract Design for Crowdsourcing Markets

standard bandit algorithm. We illustrate our general results via some corollaries and special
cases, including the high-low example described above. We support the theoretical results
with simulations.
Further, we consider a special case of our setting where each worker only chooses whether
to accept or reject a given task. This special case corresponds to a dynamic task pricing
problem previously studied in the literature. Our results significantly improve over the prior
work on this problem.
Our contributions can be summarized as follows. We define a broad, practically important setting in crowdsourcing markets; identify novel problem-specific structure, for both
the algorithm and the regret bounds; distill ideas from prior work to work with these
structures; argue that our approach is productive by deriving corollaries and comparing
to prior work; and identify and analyze specific examples where our theory applies. The
main conceptual contributions are the model itself and the adaptive discretization approach
mentioned above. Finally, this paper prompts further research on dynamic contract design
along several directions that we outline in the conclusion.
1.1 Related Work
Our work builds on three areas of research. First, our model can be viewed as a multi-round
version of the classical principal-agent model from contract theory (Laffont & Martimort,
2002). A single round of our model corresponds to the basic principal-agent setting, with
adverse selection (unknown workers type) and moral hazard (unobservable workers decisions). Unlike much of the existing work in contract theory, the prior over worker types is
not known to the principal, but may be learned over time. Accordingly, our techniques are
very different from those employed in contract theory.
Second, our methods build on those developed in the rich literature on MAB with
continuous outcome spaces. The closest line of work is that on Lipschitz MAB (Kleinberg
et al., 2008), in which the algorithm is given a distance function on the arms, and the
expected rewards of the arms are assumed to satisfy Lipschitz-continuity (or a relaxation
thereof) with respect to this distance function (Agrawal, 1995; Kleinberg, 2004; Auer,
Ortner, & Szepesvari, 2007; Kleinberg et al., 2008; Bubeck et al., 2011a; Slivkins, 2014).
Most related to our techniques is the idea of adaptive discretization (Kleinberg et al., 2008;
Bubeck et al., 2011a; Slivkins, 2014), and in particular, the zooming algorithm (Kleinberg
et al., 2008; Slivkins, 2014). However, the zooming algorithm cannot be applied directly
in our setting because the required numerical similarity information is not immediately
available. This problem also arises in web search and advertising, where it is natural to
assume that an algorithm can only observe a tree-shaped taxonomy on arms (Kocsis &
Szepesvari, 2006; Munos & Coquelin, 2007; Pandey, Agarwal, Chakrabarti, & Josifovski,
2007) which can be used to explicitly reconstruct relevant parts of the underlying metric
space (Slivkins, 2011; Bull, 2013). We take a different approach, using a notion of virtual
width to estimate similarity information. Explicit comparisons between our results and
prior MAB work are made throughout the paper.
Finally, our work follows several other theoretical papers on pricing in crowdsourcing
markets (Kleinberg & Leighton, 2003; Badanidiyuru, Kleinberg, & Singer, 2012; Singer
& Mittal, 2013; Singla & Krause, 2013; Badanidiyuru, Kleinberg, & Slivkins, 2013). In
319

fiHo, Slivkins, & Vaughan

particular, Badanidiyuru et al. (2012) and Singla and Krause (2013) study a version of our
setting with simple, single-price contracts (independent of the output), where the focus is
on dealing with a global budget constraint.
A more thorough literature review (including a discussion of some related empirical
work) can be found in Section 9.

2. Our Setting: The Dynamic Contract Design Problem
In this section, we formally define the problem that we set out to solve and discuss the
implications of several aspects of our model.
2.1 Our Model
We start by describing a static model, which captures what happens in a single round of
interaction between a requester and a worker. As described above, this is a version of the
standard principal-agent model (Laffont & Martimort, 2002). We then define our dynamic
model, an extension of the static model to multiple rounds, with a new worker arriving
each round. We then detail the objective of our pricing algorithm and the simplifying
assumptions that we make throughout the paper. Finally, we compare our setting to the
classic multi-armed bandit problem.
2.1.1 Static Model
We begin with a description of what occurs during each interaction between the requester
and a single worker. The requester first posts a task which may be completed by the worker,
and a contract specifying how the worker will be paid if she completes the task. If the task
is completed, the requester pays the worker as specified in the contract, and the requester
derives value from the completed task; for normalization, we assume that the value derived
is in [0, 1]. The requesters utility from a given task is this value minus the payment to the
worker.
When the worker observes the contract and decides whether or not to complete the task,
she also chooses a level of effort to exert, which in turn determines her cost (in terms of time,
energy, or missed opportunities) and a distribution over the quality of her work. To model
quality, we assume that there is a (small) finite set of possible outcomes that result from the
worker completing the task (or choosing not to complete it), and that the realized outcome
determines the value that the requester derives from the task. The realized outcome is
observed by the requester, and the contract that the requester offers is a mapping from
outcomes to payments for the worker.
We emphasize two crucial (and related) features of the principal-agent model: that
the mapping from effort level to outcomes can be randomized, and that the effort level
is not directly observed by the requester. This is in line with a standard observation in
crowdsourcing that even honest, high-effort workers occasionally make errors.
The workers utility from a given task is the payment from the requester minus the
cost corresponding to her chosen effort level. Given the contract she is offered, the worker
chooses her effort level strategically so as to maximize her expected utility. Crucially, the
chosen effort level is not directly observable by the requester.
320

fiAdaptive Contract Design for Crowdsourcing Markets

The workers choice not to perform a task is modeled as a separate effort level of zero
cost (called the null effort level) and a separate outcome of zero value and zero payment
(called the null outcome) such that the null effort level deterministically leads to the null
outcome, and it is the only effort level that can lead to this outcome.
The mapping from outcomes to the requesters value is called the requesters value
function. The mapping from effort levels to costs is called the cost function, and the
mapping from effort levels to distributions over outcomes is called the production function.
For the purposes of this paper, a worker is completely specified by these two functions; we
say that the cost function and the production function comprise the workers type. Unlike
some traditional versions of the principal-agent problem, in our setting a workers type is
not observable by the requester, nor is any prior given.
2.1.2 Dynamic Model
The dynamic model we consider in this paper is a natural extension of the static model to
multiple rounds and multiple workers. We are still concerned with just a single requester.
In each round, a new worker arrives. We assume a stochastic environment in which the
workers type in each round is an i.i.d. sample from some fixed and unknown distribution
over types, called the supply distribution. The requester posts a new task and a contract
for this task. All tasks are of the same type, in the sense that the set of possible effort
levels and the set of possible outcomes are the same for all tasks. The worker strategically
chooses her effort level so as to maximize her expected utility from this task. Based on
the chosen effort level and the workers production function, an outcome is realized. The
requester observes this outcome (but not the workers effort level) and pays the worker the
amount specified by the contract. The type of the arriving worker is never revealed to the
requester. The requester can adjust the contract from one round to another, and his total
utility is the sum of his utility over all rounds. For simplicity, we assume that the number
of rounds is known in advance, though this assumption can be relaxed using the standard
doubling trick (Cesa-Bianchi & Lugosi, 2006) in which full executions of the algorithm
are repeated in phases with exponentially increasing time horizons.
2.1.3 The Dynamic Contract Design Problem
Throughout this paper, we take the point of view of the requester interacting with workers
in the dynamic model. The algorithms we examine dynamically choose contracts to offer
on each round with the goal of maximizing the requesters expected utility. A problem
instance consists of several quantities, some of which are known to the algorithm, and some
of which are not. The known quantities are the number of outcomes, the requesters value
function, and the time horizon T (i.e., the number of rounds). The latent quantities are the
number of effort levels, the set of worker types, and the supply distribution. The algorithm
adjusts the contract from round to round and observes the realized outcomes but receives
no other feedback.
We focus on contracts that are bounded (offer payments in [0, 1]), and monotone (assign
equal or higher payments for outcomes with higher value for the requester). Let X be the
set of all bounded, monotone contracts. We compare a given algorithm against a given
subset of candidate contracts Xcand  X. Letting OPT(Xcand ) be the optimal utility over
321

fiHo, Slivkins, & Vaughan

all contracts in Xcand , the goal is to minimize the algorithms regret R(T |Xcand ), defined as
T  OPT(Xcand ) minus the algorithms expected utility.
The subset Xcand may be finite or infinite, possibly Xcand = X. The most natural
example of a finite Xcand is the set of all bounded, monotone contracts with payments that
are integer multiples of some  > 0; we call it the uniform mesh with granularity , and
denote it Xcand ().
2.1.4 Notation
Let v() be the value function of the requester, with v() denoting the value of outcome .
Let O be the set of all outcomes and let m be the number of non-null outcomes. We will
index the outcomes as O = {0, 1, 2 , . . . , m} in the order of increasing value (ties broken
arbitrarily), with a convention that 0 is the null outcome.
Let ci () and fi () be the cost function and production function for type i. Then the
cost of choosing effort level e is ci (e), and
Pthe probability of obtaining outcome  having
chosen effort e is fi (|e). Let Fi (|e) = 0  fi ( 0 |e) be the probability of obtaining an
outcome at least as good as  having chosen effort e.
Recall that a contract x is a function from outcomes to (non-negative) payments. If
contract x is offered to a worker sampled i.i.d. from the supply distribution, V (x) is the
expected value to the requester, P (x)  0 is the expected payment, and U (x) = V (x)P (x)
is the expected utility of the requester. Let OPT(Xcand ) = supxXcand U (x).
2.1.5 Assumption: First-Order Stochastic Dominance (FOSD)
Given two effort levels e and e0 , we say that e has FOSD over e0 for type i if Fi (|e)  Fi (|e0 )
for all outcomes , with a strict inequality for at least one outcome.2 We say that type i
satisfies the FOSD assumption if for any two distinct effort levels, one effort level has FOSD
over the other for type i. We assume that all types satisfy this assumption.
2.1.6 Assumption: Consistent Tie-Breaking
If multiple effort levels maximize the expected utility of a given worker for a contract x, we
assume the tie is broken consistently in the sense that this worker chooses the same effort
level for any contract that leads to this particular tie. This assumption is minor; it can
be avoided (with minor technical complications) by adding random perturbations to the
contracts. This assumption is implicit throughout the paper.
2.2 Discussion
Before jumping into our results, we discuss the implications of several aspects of our model
in more detail.
2.2.1 Number of Outcomes
Our results assume a small number of outcomes. This regime is important in practice
for several reasons. First, some tasks naturally have only a small number of outcomes.
2. This mimics the standard notion of FOSD between two distributions over a linearly ordered set.

322

fiAdaptive Contract Design for Crowdsourcing Markets

For example, a binary labeling task can only have four possible outcomes if completed:
{yes/no}  {correct/incorrect}. Second, it often makes sense to group together multiple
outcomes with similar value to the requester (such as false positives and false negatives) if
the value is not known very precisely. This has the added benefit that contracts become
simpler from the workers perspective. Third, even if a task can be completed in many
different ways, the quality may be difficult to evaluate in fine granularity; a good example
is a translation of a sentence. Fourth, even if a fine-grained quality evaluation exists, such
as the error count in speech transcription tasks, it may be difficult to make it consistent
across different tasks.
Even with m = 2 non-null outcomes, our setting has not been studied before. The special
case m = 1 is equivalent to the dynamic pricing problem from Kleinberg and Leighton
(2003); we obtain improved results for it, too.
2.2.2 The Benchmark
Our benchmark OPT() only considers contracts that are bounded and monotone. In practice,
restricting to such contracts may be appealing to all human parties involved. However, this
restriction is not without loss of generality: there are problem instances in which monotone
contracts are not optimal; see Appendix A for an example. Further, it is not clear whether
bounded monotone contracts are optimal among monotone contracts.
Our benchmark OPT(Xcand ) is relative to a given set Xcand , which is typically a finite
discretization of the contract space. There are two reasons for this. First, crowdsourcing platforms may require the payments to be multiples of some minimum unit (e.g., one
cent), in which case it is natural to restrict our attention to contracts satisfying the same
constraint. Second, achieving guarantees relative to OPT(X) for the full generality of our
problem appears beyond the reach of our techniques. As in many other machine learning
scenarios, it is useful to consider a restricted benchmark set  set of alternatives to compare to.3 In such settings, it is considered important to handle arbitrary benchmark sets,
which is what we do.
One known approach to obtain guarantees relative to OPT(X) is to start with some
finite Xcand  X, design an algorithm with guarantees relative to OPT(Xcand ), and then,
as a separate result, bound the discretization error OPT(X)  OPT(Xcand ). Then the choice
of Xcand drives the tradeoff between the discretization error and regret R(T |Xcand ), and
one can choose Xcand to optimize this tradeoff. However, while one can upper-bound the
discretization error in some (very) simple special cases (see Section 5), it is unclear whether
this can be extended to the full generality of dynamic contract design.
2.2.3 Alternative Worker Models
One of the crucial tenets in our model is that the workers maximize their expected utility.
This rationality assumption is very standard in economics, and is often used to make
the problem amenable to rigorous analysis. However, there is a considerable literature
suggesting that in practice workers may deviate from this rational behavior. Thus, it is
worth pointing out that our results do not rely heavily on the rationality assumption. The
3. A particularly relevant analogy is contextual bandits with policy sets (Dudik, Hsu, Kale, Karampatziakis,
Langford, Reyzin, & Zhang, 2011).

323

fiHo, Slivkins, & Vaughan

FOSD assumption (which is also fairly standard) can be circumvented, too. In fact, all
our assumptions regarding worker behavior serve only to enable us to prove Lemma 3.1,
and more specifically to guarantee that the collective worker behavior satisfies a natural
increment payment property used in the proof of Lemma 3.1: if the requester increases
the increment payment for a particular outcome (as described in the next section), the
probability of obtaining an outcome at least that good also increases. In particular, this
property is consistent with worker behavior that takes into account long-term effects such
as changes in reputation scores. It is also consistent with workers acting upon subjective
(and possibly incorrect) beliefs about the offered contract, such as beliefs about how the
guaranteed base payment may actually depend on the quality of submitted work.4
2.2.4 Minimum Wage
For ethical or legal reasons one may want to enforce some form of minimum wage. This
can be expressed within our model as a minimal payment  for a completed task, i.e.,
for any non-null outcome. Our algorithm can be easily modified to accommodate this
constraint. Essentially, it suffices to restrict the action space to contracts that pay at least
 for a completed task. Formally, the increment space defined in Section 3 should be
[, 1]  [0, 1]m1 rather than [0, 1]m , and the quadrants of each cell are defined by
splitting the cell in half in each dimension. All our results easily carry over to this version
(restricting Xcand to contracts that pay at least  for a completed task). We omit further
discussion of this issue for the sake of simplicity.
2.2.5 Comparison to Multi-Armed Bandits (MAB)
Dynamic contract design can be modeled as special case of the MAB problem with some
additional, problem-specific structure. The basic MAB problem is defined as follows. An
algorithm repeatedly chooses actions from a fixed action space and collects rewards for the
chosen actions; the available actions are traditionally called arms. More specifically, time
is partitioned into rounds, so that in each round the algorithm selects an arm and receives
a reward for the chosen arm. No other information, such as the reward the algorithm
would have received for choosing an alternative arm, is revealed. In an MAB problem
with stochastic rewards, the reward of each arm in a given round is an i.i.d. sample from
some distribution which depends on the arm but not on the round. A standard measure
of an algorithms performance is regret with respect to the best fixed arm, defined as the
difference in expected total reward between a benchmark (usually the best fixed arm) and
the algorithm.
Thus, dynamic contract design can be naturally modeled as an MAB problem with
stochastic rewards, in which arms correspond to monotone contracts. The prior work on
MAB with large or infinite action spaces often assumes known upper bounds on the similarity between arms. More precisely, this prior work would assume that an algorithm is given a
metric D on contracts such that expected rewards are Lipschitz-continuous with respect to
4. A worker model that incorporates such subjective beliefs has been suggested by Ho, Slivkins, Suri, and
Vaughan (2015) based on experimental evidence, and this model satisfies the increment payment property
mentioned above.

324

fiAdaptive Contract Design for Crowdsourcing Markets

D, i.e., we have upper bounds |U (x)U (y)|  D(x, y) for any two contracts x, y.5 However,
in our setting such upper bounds are absent. On the other hand, our problem has some
auxiliary structure compared to the standard MAB setting. In particular, the algorithms
reward decomposes into value and payment, both of which are determined by the outcome,
which in turn is probabilistically determined by the workers strategic choice of the effort
level. Effectively, this auxiliary structure provides some soft information on similarity
between contracts, in the sense that numerically similar contracts usually (but not always)
induce similar response from the workers.
2.2.6 Applicability of the Model
Despite a considerable generality, our model is somewhat idealized. Let us discuss several
potential concerns regarding how applicable and realistic the model is.
An implicit intuition behind using performance-based payments is that they can incentivize better quality. A growing empirical literature on incentives in crowdsourcing markets
finds that it happens for some types of tasks but not for others. In particular, the experiments in the work of Ho et al. (2015) suggest that it happens if and only if the task is
effort-responsive, in the sense that one can obtain higher quality work by increasing effort,
at effort levels that are not too costly for the worker. This observation is consistent with our
worker model: indeed, effort-responsiveness of a task is a joint property of the production
function and the cost function which implies a significant response to sufficiently increased
quality-based payments. Ho et al. propose pilot experiments to determine whether a given
type of tasks is effort-responsive, which in turn would shed light on whether to use qualitybased payments for these tasks in practice. A more comprehensive discussion of related
empirical work can be found in Section 9. We also observe that our model and results for
a single non-null outcome are applicable and novel even if the task is not effort-responsive.
Following the bulk of prior work on dynamic pricing and MAB, we assume that the
collective worker response to a given contract, as given by a distribution over the outcomes,
does not depend on the contracts that have been offered in the past, or on the algorithm used
to choose future contracts. Thus, we do not model the possibility that price experimentation
may alter future worker responses, or that the workers may try to game the system. Both
effects are not easy to model and extremely difficult to analyze, even in the relatively
simple scenario of a single non-null outcome with no emphasis on adaptive discretization.
Additionally, the available empirical work does not provide sufficient guidance on how to
choose a realistic model for these effects among theoretically plausible alternatives. We
leave this to future work.
From the MAB point of view, our model does not incorporate the possibility that the
worker response may intrinsically change over time, or the fact that requesters may have
hard budget constraints on the total amount of money that they can spend. This reflects
the limitations of state-of-the-art work on MAB: adaptive discretization, budgets, and adversarial change over time are fairly well-understood separately, but any two of them (let
alone all three) have not been studied jointly. That said, we conjecture that our techniques
would be useful in generalizing dynamic pricing and dynamic contract design to these richer
settings.
5. Such upper bound is informative if and only if D(x, y) < 1.

325

fiHo, Slivkins, & Vaughan

Likewise, we do not model a scenario when the requesters stream of tasks overwhelms
the crowdsourcing market and causes a drastic change in the available worker population
(and therefore in the worker response). In particular, we assume that the worker pool is
sufficiently large to accommodate the requester. This is a relatively benign assumption for
a large crowdsourcing system.
To deploy dynamic selection of prices or contracts in practice (regardless of the particular
algorithm used) the crowdsourcing platform needs to enable requesters to change their
prices/contracts relatively fast in response to observed worker responses. While this feature
is not currently instrumented on commercial platforms such as Amazon Mechanical Turk,
it appears easily implementable from an engineering point of view. We believe the main
hurdle would be to incorporate dynamic price/contract selection into the overall economic
design of the market. Given the multitude of existing crowdsourcing markets and a relative
ease of deploying new market designs, we believe this direction is well worth studying.

3. Our Algorithm: AgnosticZooming
In this section, we specify our algorithm. We call it AgnosticZooming because it zooms
in on more promising areas of the action space, and does so without knowing a precise
measure of the similarity between contracts. This zooming can be viewed as a dynamic
form of discretization. Before stating the algorithm itself, we discuss the discretization of
the action space in more detail, laying the groundwork for our approach.
3.1 Discretization of the Action Space
In each round, the AgnosticZooming algorithm partitions the action space into several regions and chooses among these regions, effectively treating each region as a meta-arm. In
this section, we discuss which subsets of the action space are used as regions, and introduce
some useful notions and properties of such subsets.
3.1.1 Increment Space and Cells
To describe our approach to discretization, it is useful to think of contracts in terms of
increment payments. Specifically, we represent each monotone contract x : O  [0, ) as a
vector x  [0, )m , where m is the number of non-null outcomes and x = x()x( 1) 
0 for each non-null outcome . (Recall that by convention 0 is the null outcome and
x(0) = 0.) We call this vector the increment representation of contract x, and denote it
incr(x). Note that if x is bounded, then incr(x)  [0, 1]m . Conversely, call a contract
weakly bounded if it is monotone and its increment representation lies in [0, 1]m . Such a
contract is not necessarily bounded.
We discretize the space of all weakly bounded contracts, viewed as a multi-dimensional
unit cube. More precisely, we define the increment space as [0, 1]m with a convention that
every vector represents the corresponding weakly bounded contract. Each region in the discretization is a closed, axis-aligned m-dimensional cube in the increment space; henceforth,
such cubes are called cells. The size of a cell is the length of any one side. A cell is called
relevant if it contains at least one candidate contract. A relevant cell is called atomic if it
contains exactly one candidate contract, and composite otherwise.
326

fiAdaptive Contract Design for Crowdsourcing Markets

In each composite cell C, the algorithm will only use two contracts: the maximal corner,
denoted x+ (C), in which all increment payments are maximal, and the minimal corner,
denoted x (C), in which all increment payments are minimal. These two contracts are
called the anchors of C. In each atomic cell C, the algorithm will only use one contract:
the unique candidate contract, also called the anchor of C. Note that anchors are not
necessarily candidate contracts.
3.1.2 Virtual Width
To take advantage of the problem structure, it is essential to estimate how similar the
contracts within a given composite cell C are. Ideally, we would like to know the maximal
difference in expected utility:
width(C) = supx,yC |U (x)  U (y)| .
We estimate the width using a proxy, called virtual width, which is expressed in terms of
the anchors:


VirtWidth(C) = V (x+ (C))  P (x (C))  V (x (C))  P (x+ (C)) .
(1)
This definition is one crucial place where the problem structure is used. (Note that it is
not the difference in utility at the anchors.) It is useful due to the following lemma (proved
in Section 3.3).
Lemma 3.1. If all types satisfy the FOSD assumption and consistent tie-breaking holds,
then width(C)  VirtWidth(C) for each composite cell C.
Recall that the proof of this lemma is the only place in the paper where we use our
assumptions on worker behavior. All further developments hold for any model of worker
behavior which satisfies Lemma 3.1.
3.2 Description of the Algorithm
With these ideas in place, we are now ready to describe our algorithm. The high-level
outline of AgnosticZooming is very simple. The algorithm maintains a set of active cells
which cover the increment space at all times. Initially, there is only a single active cell
comprising the entire increment space. In each round t, the algorithm chooses one active
cell Ct using an upper confidence index and posts contract xt sampled uniformly at random
among the anchors of this cell. After observing the feedback, the algorithm may choose to
zoom in on Ct , removing Ct from the set of active cells and activating all relevant quadrants
thereof, where the quadrants of cell C are defined as the 2m sub-cells of half the size for
which one of the corners is the center of C. In the remainder of this section, we specify how
the cell Ct is chosen (the selection rule), and how the algorithm decides whether to zoom
in on Ct (the zooming rule).
Let us first introduce some notation. Consider cell C that is active in some round t. Let
U (C) be the expected utility from a single round in which C is chosen by the algorithm,
i.e., the average expected utility of the anchor(s) of C. Let nt (C) be the number of times
this cell has been chosen before round t. Consider all rounds in which C is chosen by
327

fiHo, Slivkins, & Vaughan

the algorithm before round t. Let Ut (C) be the average utility over these rounds. For a
composite cell C, let Vt+ (C) and Pt+ (C) be the average value and average payment over
all rounds when anchor x+ (C) is chosen. Similarly, let Vt (C) and Pt (C) be the average
value and average payment over all rounds when anchor x (C) is chosen. Accordingly, we
can estimate the virtual width of composite cell C at time t as


Wt (C) = Vt+ (C)  Pt (C)  Vt (C)  Pt+ (C) .
(2)
To bound the deviations, we define the confidence radius as
p
radt (C) = crad log(T )/nt (C),

(3)

for some absolute constant crad ; in our analysis, crad  16 suffices. We will show that with
high probability all sample averages defined above will stay within radt (C) of the respective
expectations. If this high probability event holds, the width estimate Wt (C) will always be
within 4 radt (C) of VirtWidth(C).
The algorithm pseudocode is summarized in Algorithm 1. The selection rule and the
zooming rule are explained in more detail below.
ALGORITHM 1: AgnosticZooming
Inputs: subset Xcand  X of candidate contracts.
Data structure: Collection A of cells. Initially, A = { [0, 1]m }.
For each round t = 1 to T
Let Ct = argmaxCA It (C), where It () is defined as in Equation (4).
Sample contract xt u.a.r. among the anchors of Ct . \\ Anchors are defined in Section 3.1.
Post contract xt and observe feedback.
If |Ct  Xcand | > 1 and 5 radt+1 (Ct ) < Wt+1 (Ct ) then
A  A  {all relevant quadrants of Ct } \ {Ct }. \\ C is relevant if |C  Xcand |  1.

3.2.1 Selection Rule
The selection rule is as follows. In each round t, the algorithm chooses an active cell C
with maximal index It (). It (C) is an upper confidence bound on the expected utility of
any candidate contract in C, defined as
(
Ut (C) + radt (C)
if C is an atomic cell,
It (C) =
(4)
Ut (C) + Wt (C) + 5 radt (C) otherwise.
If nt (C) = 0, Ut (C) and Wt (C) can be initialized to any finite values. Since radt (C) is
infinite when nt (C) = 0, AgnosticZooming will first select the cell that is never selected
before time t.
3.2.2 Zooming Rule
We zoom in on a composite cell Ct if
Wt+1 (Ct ) > 5 radt+1 (Ct ),
328

fiAdaptive Contract Design for Crowdsourcing Markets

i.e., the uncertainty due to random sampling, expressed by the confidence radius, becomes
sufficiently small compared to the uncertainty due to discretization, expressed by the virtual
width. We never zoom in on atomic cells.
3.2.3 Notes on Integer Payments
In practice it may be necessary to only allow contracts in which all payments are integer
multiples of some amount , e.g., whole cents. (In this case we can assume that candidate
contracts have this property, too.) Then we can redefine the two anchors of each composite
cell: the maximal (resp., minimal) anchor is the nearest allowed contract to the maximal
(resp., minimal) corner. Width can be redefined as a supremum over all allowed contracts
in a given cell. With these modifications, the analysis goes through without significant
changes. We omit further discussion of this issue.
3.3 Proof of Lemma 3.1 (virtual width)
For two vectors x, x0  <m , write x0  x if x0 pointwise dominates x, i.e., if x0j  xj for all
j. For two monotone contracts x, x0 , write x0  x if incr(x0 )  incr(x).
Claim 3.2. Consider a worker whose type satisfies the FOSD assumption and two weakly
bounded contracts x, x0 such that x0  x. Let e (resp., e0 ) be the effort levels exerted by this
worker when he is offered contract x (resp., x0 ). Then e does not have FOSD over e0 .
Proof. For the sake of contradiction, assume that e has FOSD over e0 . Note that e 6= e0 .
Let i be the workers type. Recall that Fi (|e) denotes the probability of generating an
outcome  0   given the effort level e. Define F = ( Fi (1|e) , . . . , Fi (m|e) ), and define F0
similarly for e0 .
Let x and x0 be the increment representations for x and x0 . Given contract x, the
workers expected utility for effort level e is Ui (x|e) = x  F  ci (e). Since e is the optimal
effort level given this contract, we have Ui (x|e)  Ui (x|e0 ), and therefore
x  F  x  F0  ci (e)  ci (e0 ).
Similarly, since e0 is the optimal effort level given contract x0 , we have
x0  F0  x0  F  ci (e0 )  ci (e).
Combining the above two inequalities, we obtain
(x  x0 )  (F  F0 )  0.

(5)

Note that if Equation (5) holds with equality then Ui (x|e) = Ui (x|e0 ) and Ui (x0 |e) =
Ui (x0 |e0 ), so the worker breaks the tie between e and e0 in a different way for two different
contracts. This contradicts the consistent tie-breaking assumption. However, Equation (5)
cannot hold with a strict equality, either, because x0  x and (since e has FOSD over e0 )
we have F  F0 and Fi (|e) > Fi (|e0 ) for some outcome  > 0. Therefore we obtain a
contradiction, completing the proof.
The proof of Claim 3.2 is the only place in the paper where we directly use the consistent
tie-breaking assumption. (But the rest of the paper relies on this claim.)
329

fiHo, Slivkins, & Vaughan

Claim 3.3. Assume all types satisfy the FOSD assumption. Consider weakly bounded
contracts x, x0 such that x0  x. Then V (x0 )  V (x) and P (x0 )  P (x).
Proof. Consider some worker, let i be his type. Let e and e0 be the chosen effort levels for
contracts x and x0 , respectively. By the FOSD assumption, either e = e0 , or e0 has FOSD
over e, or e has FOSD over e0 . Claim 3.2 rules out the latter possibility.
Define vectors F and F0 as in the proof of Claim 3.2. Note that F0  F.
Then P = x  F and P 0 = x0  F0 is the expected payment for contracts x and x0 ,
respectively. Further, letting v denote the increment representation of the requesters value
for each outcome, V = v  F and V 0 = v  F0 is the expected requesters value for contracts
x and x0 , respectively. Since x0  x and F0  F, it follows that P 0  P and V 0  V . Since
this holds for each worker, this also holds in expectation over workers.
To finish the proof of Lemma 3.1, consider a composite cell C with anchors x+ =
and x = x (C), and fix a contract x  C. Since x+  x  x , by Claim 3.3
it follows that V (x+ )  V (x)  V (x ) and P (x+ )  P (x)  P (x ). Therefore |U (x) 
U (y)|  VirtWidth(C). Taking the supremum over all x  C over, we obtain width(C) 
VirtWidth(C), as claimed.
x+ (C)

4. Regret Bounds and Discussion
We present the main regret bound for AgnosticZooming. Formulating this result requires
some new, problem-specific structure. Stated in terms of this structure, the result is somewhat difficult to access. To explain its significance, we state several corollaries, and compare
our results to prior work.
4.1 The Main Result
We start with the main regret bound. Like the algorithm itself, this regret bound is parameterized by the set Xcand of candidate contracts; our goal is to bound the algorithms
regret with respect to candidate contracts.
Recall that OPT(Xcand ) = supxXcand U (x) is the optimal expected utility over candidate
contracts. The algorithms regret with respect to candidate contracts is R(T |Xcand ) =
T OPT(Xcand )  U , where T is the time horizon and U is the expected cumulative utility of
the algorithm.
Define the badness (x) of a contract x  X as the difference in expected utility between
an optimal candidate contract and x: (x) = OPT(Xcand )  U (x). Let X = {x  Xcand :
(x)  }.
We will only be interested in cells that can potentially be used by AgnosticZooming.
Formally, we recursively define a collection of feasible cells as follows: (i) the cell [0, 1]m is
feasible, (ii) for each feasible cell C, all relevant quadrants of C are feasible. Note that the
definition of a feasible cell implicitly depends on the set Xcand of candidate contracts: by
definition, a feasible cell is one that contains a candidate contract.
Let F denote the collection of all feasible, composite cells C such that VirtWidth(C) 
. For Y  Xcand , let F (Y ) be the collection of all cells C  F that overlap with Y , and
let N (Y ) = |F (Y )|; sometimes we will write N (Y |Xcand ) in place of N (Y ) to emphasize
the dependence on Xcand .
330

fiAdaptive Contract Design for Crowdsourcing Markets

Using the structure defined above, the main theorem is stated as follows. We prove this
theorem in Section 6.
Theorem 4.1. Consider the dynamic contract design problem with all types satisfying the
FOSD assumption and a constant number of outcomes. Consider AgnosticZooming, parameterized by some set Xcand of candidate contracts. Assume T  max(2m + 1, 18). There
is an absolute constant 0 > 0 such that for any  > 0,
X

R(T |Xcand )  T + O(log T )

=2j : jN

N 0 (X |Xcand )
.


(6)

Remark 1. As discussed in Section 2.2, we target the practically important case of a small
number of outcomes. The impact of larger m is an exponential dependence on m in the
O() notation, and, more importantly, increased number of candidate policies (typically
exponential in m for a given granularity).
Remark 2. Our regret bounds do not depend on the number of worker types, in line with
prior work on dynamic pricing. Essentially, this is because bandit approaches tend to
depend only on expected reward of a given arm (and perhaps also on the variance), not
the finer properties of the distribution.
Equation (6) has a shape similar to several other regret bounds in the literature, as
discussed below. To make this more apparent, we observe that regret bounds in bandits
in metric spaces are often stated in terms of covering numbers. (For a fixed collection
F of subsets of a given ground set X, the covering number of a subset Y  X relative
to F is the smallest number of subsets in F that is sufficient to cover Y .) The numbers
N (Y |Xcand ) are, essentially, about covering Y with feasible cells with virtual width close to
. We make this point more precise as follows. Let an -minimal cell be a cell in F which
does not contain any other cell in F . Let Nmin (Y ) be the covering number of Y relative
to the collection of -minimal cells, i.e., the smallest number of -minimal cells sufficient to
cover Y . Then
N (Y )  dlog 1 e Nmin (Y ) for any Y  Xcand and   0,

(7)

where  is the smallest size of a feasible cell.6 Thus, Equation (6) can be easily restated
using the covering numbers Nmin () instead of N ().
4.2 Corollary: Polynomial Regret
Literature on regret-minimization often states polynomial regret bounds of the form
R(T ) = O(T  ),  < 1. While covering-number regret bounds are more precise and versatile, the exponent  in a polynomial regret bound expresses algorithms performance in a
particularly succinct and lucid way.
For bandits in metric spaces the exponent  is typically determined by an appropriately defined notion of dimension, such as the covering dimension,7 which succinctly
6. To prove Equation (7), observe that for each cell C  F (Y ) there exists an -minimal cell C 0  C, and
for each -minimal cell C 0 there exist at most dlog 1 e cells C  F (Y ) such that C 0  C.
7. Given covering numbers N (), the covering dimension of Y is the smallest d  0 such that N (Y ) =
O(d ) for all  > 0.

331

fiHo, Slivkins, & Vaughan

captures the difficulty of the problem instance. Interestingly, the dependence of  on the
dimension d is typically of the same shape;  = (d + 1)/(d + 2), for several different notions
of dimension. In line with this tradition, we define the width dimension:
n
o
WidthDim = inf d  0 : N 0 (X |Xcand )   d for all  > 0 ,  > 0.
(8)
Note that the width dimension depends on Xcand and the problem instance, and is parameterized by a constant  > 0. By optimizing the choice of  in Equation (6), we obtain the
following corollary.
Corollary 4.2. Consider the the setting of Theorem 4.1. For any  > 0, let d = WidthDim .
Then
R(T |Xcand )  O( log T ) T (1+d)/(2+d) .

(9)

The width dimension is similar to the zooming dimension in the work of Kleinberg
et al. (2008) and near-optimality dimension in the work on bandits in metric spaces
(Bubeck et al., 2011a).
4.3 Comparison to Prior Work
Below we compare our results with previous work in non-adaptive discretization and bandits
in metric spaces.
4.3.1 Non-Adaptive Discretization
One approach from prior work that is directly applicable to the dynamic contract design
problem is non-adaptive discretization. This is an algorithm, call it NonAdaptive, which
runs an off-the-shelf MAB algorithm, treating a set of candidate contracts Xcand as arms.8
For concreteness, and following the prior work (Kleinberg & Leighton, 2003; Kleinberg,
2004; Kleinberg et al., 2008), we use a well-known algorithm UCB1 (Auer, Cesa-Bianchi, &
Fischer, 2002) as an off-the-shelf MAB algorithm.
To compare AgnosticZooming with NonAdaptive, it is useful to derive several worstcase corollaries of Theorem 4.1, replacing N (X ) with various (loose) upper bounds.9
Corollary 4.3. In the setting of Theorem 4.1, the regret of AgnosticZooming can be upperbounded as follows:
P
(a) R(T |Xcand )  T + =2j : jN O(|X | /), for each   (0, 1).
p
(b) R(T |Xcand )  O( T |Xcand |).
Here the O() notation hides the logarithmic dependence on T and .
The best known regret bounds for NonAdaptive coincide with those in Corollary 4.3 up
to poly-logarithmic factors. However, the regret bounds in Theorem 4.1 may be significantly
better than the ones in Corollary 4.3. We further discuss this in the next section, in the
context of a specific example.
8. To simplify the proofs of the lower bounds, we assume that the candidate contracts are randomly
permuted when given to the MAB algorithm.
9. We use the facts that X  Xcand , N (Y )  N0 (Y ), and N0min (Y )  |Y | for all subsets Y  X.

332

fiAdaptive Contract Design for Crowdsourcing Markets

4.3.2 Bandits in Metric Spaces
Consider a variant of dynamic contract design in which an algorithm is given a priori
information on similarity between contracts: a function D : Xcand  Xcand  [0, 1] such that
|U (x)  U (y)|  D(x, y) for any two candidate contracts x, y. If an algorithm is given this
function D (call such algorithm D-aware), the machinery from bandits in metric spaces
(Kleinberg et al., 2008; Bubeck et al., 2011a) can be used to perform adaptive discretization
and obtain a significant advantage over NonAdaptive. We argue that we obtain similar
results with AgnosticZooming without knowing the D.
In practice, the similarity information D would be coarse, probably aggregated according
to some predefined hierarchy. To formalize this idea, the hierarchy can be represented as
a collection F of subsets of Xcand , so that D(x, y) is a function of the smallest subset in
F containing both x and y. The hierarchy F should be natural given the structure of
the contract space. One such natural hierarchy is the collection of all feasible cells, which
corresponds to splitting the cells in half in each dimension. Formally, D(x, y) = f (Cx,y ) for
some f with f (Cx,y )  width(Cx,y ), where Cx,y is the smallest feasible cell containing both
x and y.
Given this shape of D, let us state the regret bounds for D-aware algorithms in the work
of Kleinberg et al. (2008) and Bubeck et al. (2011a). To simplify the notation, we assume
that the action space is restricted to Xcand . The regret bounds have a similar shape as
that in Theorem 4.1:
R(T |Xcand )  T + O(log T )

 (X )
N()


X
=2j :

jN



,

(10)

where the numbers N () have a similar high-level meaning as N (), and nearly coincide
with Nmin () when D(x, y) = VirtWidth(Cx,y ). One can use Equation (10) to derive a
polynomial regret bound like Equation (9).
For a more precise comparison, we focus on the results in the work of Kleinberg et al.
(2008). (The regret bounds in Bubeck et al., 2011a are very similar in spirit, but are stated
in terms of a slightly different structure.) The covering-type regret bound in the work of
Kleinberg et al. (2008) focuses on balls of radius at most  according to distance D, so that
N (Y ) is the smallest number of such balls that is sufficient to cover Y . In the special case
D(x, y) = VirtWidth(Cx,y ) balls of radius   are precisely feasible cells of virtual width
 . This is very similar (albeit not technically the same) as the -minimal cells in the
definition of Nmin ().
Further, the covering numbers N (Y ) determine the zooming dimension:
n
o

ZoomDim = inf d  0 : N/8
(X )   d for all  > 0 ,  > 0.
(11)
This definition coincides with the covering dimension in the worst case, and can be much
smaller for nice problem instances in which X is a significantly small subset of Xcand .
With this definition, one obtains a polynomial regret bound which is a version of Equation (9) with d = ZoomDim .
We conclude that AgnosticZooming essentially matches the regret bounds for D-aware
algorithms, despite the fact that D-aware algorithms have access to much more information.
333

fiHo, Slivkins, & Vaughan

5. A Special Case: The High-Low Example
We apply the machinery in Section 4 on a special case, and we show that AgnosticZooming
significantly outperforms NonAdaptive.
The most basic special case is when there is just one non-null outcome. Essentially, each
worker makes a strategic choice whether to accept or reject a given task (where reject
corresponds to the null effort level), and this choice is fully observable. This setting has been
studied before (Kleinberg & Leighton, 2003; Badanidiyuru et al., 2012; Singla & Krause,
2013; Badanidiyuru et al., 2013); we will call it dynamic task pricing. Here the contract
is completely specified by the price p for the non-null outcome. The supply distribution is
summarized by the function S(p) = Pr[accept|p], so that the corresponding expected utility
is U (p) = S(p)(v  p), where v is the value for the non-null outcome. This special case
is already quite rich, because S() can be an arbitrary non-decreasing function. By using
adaptive discretization, we achieve significant improvement over prior work; see Section 8
for further discussion.
We consider a somewhat richer setting in which workers strategic decisions are not
observable; this is a salient feature of our setting, called moral hazard in the contract
theory literature. There are two non-null outcomes (low and high), and two non-null effort
levels (low and high). Low outcome brings zero value to the requester, while high outcome
brings value v > 0. Low effort level inflicts zero cost on a worker and leads to low outcome
with probability 1. We assume that workers break ties between effort levels in a consistent
way: high better than low better than null. (Hence, as low effort incurs zero cost, the
only possible outcomes are low and high.) We will call this the high-low example; it is
perhaps the simplest example that features moral hazard.
In this example, the workers type consists of a pair (ch , h ), where ch  0 is the cost for
high effort and h  [0, 1] is the probability of high outcome given high effort. Note that
dynamic task pricing is equivalent to the special case h = 1.
The following claim states a crucial property of the high-low example.
Claim 5.1. Consider the high-low example with a fixed supply distribution. Then the probability of obtaining high outcome given contract x Pr[high outcome | contract x] depends only
on p = x(high)  x(low); denote this probability by S(p). Moreover, S(p) is non-decreasing
in p. Therefore:
 expected utility is U (x) = S(p)(v  p)  x(low).
 discretization error OPT(X)  OPT(Xcand ()) is at most 3, for any  > 0.
To bound the discretization error, it is essential that S(p) is non-decreasing in p.
Recall that Xcand (), the uniform mesh with granularity  > 0, consists of all bounded,
monotone contracts with payments in N.
For our purposes, the supply distribution is summarized via the function S(). Denote
U (p) = S(p)(v  p). Note that U (x) is maximized by setting x(low) = 0, in which case
U (x) = U (p). Thus, if an algorithm knows that it is given a high-low example, it can set
x(low) = 0, thereby reducing the dimensionality of the search space. Then the problem
essentially reduces to dynamic task pricing with the same S().
However, in general an algorithm does not know whether it is presented with the highlow example (because the effort levels are not observable). So in what follows we will
consider algorithms that do not restrict themselves to x(low) = 0.
334

fiAdaptive Contract Design for Crowdsourcing Markets

5.1 Nice Supply Distribution
We focus on a supply distribution D that is nice, in the sense that S() satisfies the
following two properties:
 S(p) is Lipschitz-continuous: |S(p)  S(p0 )|  L|p  p0 | for some constant L.
 U (p) is strongly concave, in the sense that U 00 () exists and satisfies U 00 ()  C < 0.
Here L and C are absolute constants. We call such D strongly Lipschitz-concave.
The above properties are fairly natural. For example, they are satisfied if h is the same
for all worker types and the marginal distribution of ch is piecewise uniform such that the
density is between 1 and , for some absolute constant   1.
We show that for any choice Xcand  X, AgnosticZooming has a small width dimension
in this setting, and therefore small regret.
Lemma 5.2. Consider the high-low example with a strongly Lipschitz-concave supply distribution. Then the width dimension is at most 21 , for any given Xcand  X. Therefore,
AgnosticZooming with this Xcand has regret R(T |Xcand ) = O(log T ) T 3/5 .
We contrast this with the performance of NonAdaptive, parameterized with the natural
choice Xcand = Xcand (). We focus on R(T |X): regret w.r.t. the best contract in X. We
show that AgnosticZooming achieves R(T |X) = O(T 3/5 ) for a wide range of Xcand , whereas
NonAdaptive cannot do better than R(T |X) = O(T 3/4 ) for any Xcand = Xcand (),  > 0.
Lemma 5.3. Consider the setting of Lemma 5.2. Then:
(a) AgnosticZooming with Xcand  Xcand (T 2/5 ) has regret R(T |X) = O(T 3/5 log T ).
(b) NonAdaptive with Xcand = Xcand () cannot achieve regret R(T |X) < o(T 3/4 ) over
all problem instances, for any  > 0. 10

5.2 Proofs
Proof of Claim 5.1. Consider a contract x with x(low) = b and x(high) = b + p, and a
worker of type (ch , h ). If the worker exerts high effort, she pays cost ch and receives
expected payment h (p + b) + (1  h )b, for a total expected payoff ph + b  ch . Her
expected payoff for exerting low effort is b. Therefore she will choose to exert high effort
if and only if ph + b  ch  b, i.e., if ch /h  p, and choose to exert low effort otherwise.
Therefore


Pr[high outcome | contract x] = E h 1{ch /h p} .
(ch ,h )

This is a function of p, call it S(p). Moreover, this is a non-decreasing function simply
because the expression inside the expectation is non-decreasing in p.
It trivially follows that U (x) = S(p)(v  p)  x(low).
We can upper-bound the discretization error using a standard approach from the work
on dynamic pricing (Kleinberg & Leighton, 2003). Fix discretization granularity  > 0. For
any  > 0, there exists a contract x  X such that OPT(X)  U (x ) < . Round x (high)
10. This lower bound holds even if UCB1 in NonAdaptive is replaced with any other MAB algorithm.

335

fiHo, Slivkins, & Vaughan

and x (low) up and down, respectively, to the nearest integer multiple of ; let x  Xcand ()
be the resulting contract. Denoting p = x(high)  x(low) and p = x (high)  x (low), we
see that p  p  p + 2. It follows that
U (x)  U (x )  3  OPT(X)    3.
Since this holds for any  > 0, we conclude that OPT(X)  OPT(Xcand ())  3.
Proof of Lemma 5.2. To calculate the width dimension, we need to count the number of
feasible cells in the increment space which (i) have virtual width larger than or equal to
O() and (ii) overlap with X , the set of contracts with badness smaller than .
We first characterize X . We use xp,b to denote the contract with x(high) = p + b and
x(low) = b. The benefit of this representation is that, p and b would then be the two axes
in the increment space. Let xp ,0 be an optimal contract. Since U (xp,b ) is strongly concave
in p, we know that for any b, there exist constants c1 and c2 such that for any p  [0, 1],
c1 (p  p)2  U (xp ,b )  U (xp,b )  c2 (p  p)2 . Also we know that U (xp ,b ) = U (xp ,0 )  b.
Therefore.
X = {xp,b : (p  p )2 + b  O()}
We can also write it as


X = {xp,b : p  h ( )  p  p + h ( ) and b  O()}

Intuitively, X contains contracts {xp,b } with p not O( ) away from p and b not O()
away from b = 0.
Next we characterize the virtual width of a cell. We use Cp,b,d to denote the cell with
size d and with anchors {xp,b , x(p+d),(b+d) }. We can derive the expected payment and value
on the two anchors as:
 P + (Cp,b,d ) = (p + d)S(p + d) + b + d
 V + (Cp,b,d ) = vS(p + d)
 P  (Cp,b,d ) = pS(p) + b
 V  (Cp,b,d ) = vS(p)
By definition, we can get that (we use dF to represent S(p + d)  S(p) for simplification)
VirtWidth(Cp,b,d ) = (v + p)dF + dS(p) + d dF + d.
Now we can count the number of feasible cells with virtual width larger than h () which
overlaps with X . Note that since the total number of feasible cells Cp,b,d with large d is
small, we can treat the number of cells with large d as a constant. Also, for any relevant
cell Cp,b,d , we have p  p . Therefore, we only care about feasible cells Cp,b,d with small d
and when p is close to p .
Since S(p) is Lipschitz, we have dF = O(d). Therefore, for any relevant cell Cp,d ,
VirtWidth(Cp,b,d ) = O(d)
Given the above two arguments, we know that the number of cells with virtual width

larger than  which also overlaps with X is O(/)  O( /) = O(1/2 ). Therefore the
width dimension is 1/2.
336

fiAdaptive Contract Design for Crowdsourcing Markets

Proof Sketch of Lemma 5.3(b). Consider a version of NonAdaptive that runs an off-theshelf MAB algorithm ALG on candidate contracts Xcand = Xcand (). For ALG, the arms
are the candidate contracts; recall that the arms are randomly permuted before they are
given to ALG.
Fix  > 0. It is easy to construct a problem instance with discretization error Error ,
OPT(X)  OPT(Xcand ())  (). Note that Xcand contains N = ( 2 ) suboptimal contracts that are suboptimal w.r.t. OPT(Xcand ). (For example, all contracts x with x(low) > 0
are suboptimal.)
Fix any problem instance I of MAB with N suboptimal arms. Using standard lowerbound arguments for MAB, one can show that if one runs ALG on a problem instance
obtainedby randomly permuting the arms in I, then the expected regret in T rounds is at
least ( N T ).

Therefore, R(T |Xcand )  ( N T ). It follows that


R(T |X)  ( N T ) + Error  T  ( T / + T )  (T 3/4 ).

6. Proof of the Main Regret Bound (Theorem 4.1)
We now prove the main result from Section 4. Our high-level approach is to define a
clean execution of an algorithm as an execution in which some high-probability events are
satisfied, and derive bounds on regret conditional on the clean execution. The analysis of
a clean execution does not involve any probabilistic arguments. This approach tends to
simplify regret analysis.
We start by listing some simple invariants enforced by AgnosticZooming:
Invariant 6.1. In each round t of each execution of AgnosticZooming:
(a) All active cells are relevant,
(b) Each candidate contract is contained in some active cell,
(c) Wt (C)  5 radt (C) for each active composite cell C.
Note that the zooming rule is essential to ensure Invariant 6.1(c).
Throughout, we say that the algorithm activates a cell if this cell is added to the
collection of active cells. A cell stays active once it is activated.
6.1 Analysis of the Randomness
Definition 6.2 (Clean Execution). An execution of AgnosticZooming is called clean if for
each round t and each active cell C it holds that
|U (C)  Ut (C)|  radt (C),
|VirtWidth(C)  Wt (C)|  4 radt (C)

(12)
(if C is composite).

(13)

Lemma 6.3. Assume crad  16 and T  max(1 + 2m , 18). Then:
(a) Pr [ Equation (12) holds  rounds t, active cells C ]  1  2 T 2 .
(b) Pr [ Equation (13) holds  rounds t, active composite cells C ]  1  16 T 2 .
Consequently, an execution of AgnosticZooming is clean with probability at least 1  1/T .
337

fiHo, Slivkins, & Vaughan

Lemma 6.3 follows from the standard concentration inequality known as Chernoff
Bounds. However, one needs to be careful about conditioning and other details.
Proof of Lemma 6.3(a). Consider an execution of AgnosticZooming. Let N be the total
number of activated cells. Since at most 2m cells can be activated in any one round,
N  1 + 2m T  T 2 . Let Cj be the min(j, N )-th cell activated by the algorithm. (If multiple
quadrants are activated in the same round, order them according to some fixed ordering
on the quadrants.)
Fix some feasible cell C and j  T 2 . We claim that
Pr [ |U (C)  Ut (C)|  radt (C) for all rounds t | Cj = C ]  1  2 T 4 .

(14)

Let n(C) = n1+T (C) be the total number of times cell C is chosen by the algorithm.
For each s  N: 1  s  n(C) let Us be the requesters utility in the round when C is
chosen for the s-th time. Further, let DC be the distribution of U1 , conditional on the
event n(S)  1. (That is, the per-round reward from choosing cell C.) Let U10 , . . . , UT0 be
a family of mutually independent random variables, each with distribution DC . Then for
each n  T , conditional on the event {Cj = C}  {n(C) = n}, the tuple (U1 , . . . , Un ) has
the same joint distribution as the tuple (U10 , . . . , Un0 ). Consequently, applying Chernoff
Bounds to the latter tuple, it follows that
fi
hfi
i
fi q
P
fi
Pr fiU (C)  n1 ns=1 Us fi  n1 crad log(T ) fi {Cj = C}  {n(C) = n}
 1  2 T 2crad  1  2 T 5 .
Taking the Union Bound over all n  T , and plugging in radt (Cj ), nt (Cj ), and Ut (Cj ), we
obtain Equation (14).
Now, let us keep j fixed in Equation (14), and integrate over C. More precisely, let us
multiply both sides of Equation (14) by Pr[Cj = C] and sum over all feasible cells C. We
obtain, for all j  T 2 :
Pr [ |U (Cj )  Ut (Cj )|  radt (Cj ) for all rounds t ]  1  2 T 4 .

(15)

(Note that to obtain Equation (15), we do not need to take the Union Bound over all
feasible cells C.) To conclude, we take the Union Bound over all j  1 + T 2 .
Proof Sketch of Lemma 6.3(b). We show that
fi
fi

Pr fiV + (C)  Vt+ (C)fi  radt (C)  rounds t, active composite cells C  1 

4
,
T2

(16)

and similarly for V  (), P + () and P  (). Each of these four statements is proved similarly,
using the technique from Lemma 6.3(a). In what follows, we sketch the proof for one of the
four cases, namely for Equation (16).
For a given composite cell C, we are only interested in rounds in which anchor x+ (C)
is selected by the algorithm. Letting n+
t (C) be the number of times this anchor is chosen
up to time t, let us define the corresponding notion of confidence radius:
s
1 crad log T
+
radt (C) =
.
2
n+
t (C)
338

fiAdaptive Contract Design for Crowdsourcing Markets

With the technique from the proof of Lemma 6.3(a), we can establish the following
high-probability event:
fi
fi +
fiV (C)  V + (C)fi  rad+ (C).
(17)
t
t
More precisely, we can prove that
Pr [ Equation (17) holds  rounds t, active composite cells C ]  1  2 T 2 .
Further, we need to prove that w.h.p. the anchor x+ (C) is played sufficiently often.
1
11
Noting that E[n+
t (C)] = 2 nt (C), we establish an auxiliary high-probability event:
n+
t (C) 

1
2

nt (C)  14 radt (C).

(18)

More precisely, we can use Chernoff Bounds to show that, if crad  16,
Pr [ Equation (18) holds  rounds t, active composite cells C ]  1  2 T 2 .

(19)

Now, letting n0 = (crad log T )1/3 , observe that
nt (C)  n0
nt (C) < n0




1
n+
t (C)  4 nt (C)
radt (C)  1




+
t (C),
fi
firad+t (C)  rad
fiV (C)  V + (C)fi  radt (C).
t

fi
fi
Therefore, once Equations (17) and (18) hold, we have fiV + (C)  Vt+ (C)fi  radt (C). This
completes the proof of Equation (16).
6.2 Analysis of a Clean Execution
The rest of the analysis focuses on a clean execution. Recall that Ct is the cell chosen by
the algorithm in round t.
Claim 6.4. In any clean execution, I(Ct )  OPT(Xcand ) for each round t.
Proof. Fix round t, and let x be any candidate contract. By Invariant 6.1(b), there exists
an active cell, call it Ct , which contains x .
We claim that It (Ct )  U (x ). We consider two cases, depending on whether Ct is
atomic. If Ct is atomic then the anchor is unique, so U (Ct ) = U (x ), and It (Ct )  U (x )
by the clean execution. If Ct is composite then
It (Ct )  U (Ct ) + VirtWidth(Ct )


U (Ct )


 U (x )

+

by clean execution

width(Ct )

by Lemma 3.1
by definition of width, since x  Ct .

We have proved that It (Ct )  U (x ). Now, by the selection rule we have It (Ct )  It (Ct ) 
U (x ). Since this holds for any candidate contract x , the claim follows.
11. The constant
proof.

1
4

in Equation (18) is there to enable a consistent choice of n0 in the remainder of the

339

fiHo, Slivkins, & Vaughan

Claim 6.5. In any clean execution, for each round t, the index It (Ct ) is upper-bounded as
follows:
(a) if Ct is atomic then I(Ct )  U (Ct ) + 2 radt (Ct ).
(b) if Ct is composite then I(Ct )  U (x) + O(radt (Ct )) for each contract x  Ct .
Proof. Fix round t. Part (a) follows because It (Ct ) = Ut (Ct ) + radt (Ct ) by definition of the
index, and Ut (Ct )  U (Ct ) + radt (Ct ) by clean execution.
For part (b), fix a contract x  Ct . Then:
Ut (Ct )  U (Ct ) + radt (Ct )

by clean execution

 U (x) + width(Ct ) + radt (Ct )

by definition of width

 U (x) + VirtWidth(Ct ) + radt (Ct )

by Lemma 3.1

 U (x) + Wt (Ct ) + 5 radt (Ct )

by clean execution.

It (Ct ) = Ut (Ct ) + Wt (Ct ) + 5 radt (Ct )

(20)

by definition of index

 U (x) + 2 Wt (Ct ) + 10 radt (Ct )

by Equation (20)

 U (x) + 20 radt (Ct )

by Invariant 6.1(c).

For each relevant cell C, define badness (C) as follows. If C is composite, (C) =
supxC (x) is the maximal badness among all contracts in C. If C is atomic and x  C
is the unique candidate contract in C, then (C) = (x).
Claim 6.6. In any clean execution, (C)  O(radt (C)) for each round t and each active
cell C.
Proof. By Claims 6.4 and 6.5, (Ct )  O(radt (Ct )) for each round t. Fix round t and
let C be an active cell in this round. If C has never be selected before round t, the claim
is trivially true. Else, let s be the most recent round before t when C is selected by the
algorithm. Then (C)  O(rads (C)). The claim follows since rads (C) = radt (C).
Claim 6.7. In a clean execution, each cell C is selected  O(log T /((C))2 ) times.
Proof. By Claim 6.6, (C)  O(radT (C)). The claim follows from the definition of radT
in Equation (3).
Let n(x) and n(C) be the number of times contract x and cell C, respectively, are chosen
by the algorithm. Then regret of the algorithm is
R(T |Xcand ) =

P

xX

n(x) (x) 

P

cells C

n(C) (C).

(21)

The next result (Lemma 6.8) upper-bounds the right-hand side of Equation (21) for a clean
execution. By Lemma 6.3, this suffices to complete the proof of Theorem 4.1
Lemma 6.8. Consider a clean execution of AgnosticZooming. For any   (0, 1),
P

cells C

n(C) (C)  T + O(log T )
340

P

=2j : jN

|F (X2 )|
.


fiAdaptive Contract Design for Crowdsourcing Markets

The proof of Lemma 6.8 relies on some simple properties of (), stated below.
Claim 6.9. Consider two relevant cells C  Cp . Then:
(a) (C)  (Cp ).
(b) If (C)   for some  > 0, then C overlaps with X .
Proof. To prove part (a), one needs to consider two cases, depending on whether cell Cp is
composite. If it is, the claim follows trivially. If Cp is atomic, then C is atomic, too, and so
(C) = (Cp ) = (x), where x is the unique candidate contract in Cp .
For part (b), there exists a candidate contract x  C. It is easy to see that (x)  (C)
(again, consider two cases, depending on whether C is composite.) So, x  X .
Proof of Lemma 6.8. Let  denote the sum in question. Let A be the collection of all
cells ever activated by the algorithm. Among such cells, consider those with badness on the
order of :
G := { C  A : (C)  [, 2) } .
By Claim 6.7, the algorithm chooses each cell C  G at most O(log T /2 ) times, so
n(C) (C)  O(log T /).
Fix some   (0, 1) and observe that all cells C with (C)   contribute at most T
to . Therefore it suffices to focus on G ,   /2. It follows that
P
  T + O(log T ) =2i /2 |G | .
(22)
We bound |G | as follows. Consider a cell C  G . The cell is called a leaf if it is never
zoomed in on (i.e., removed from the active set) by the algorithm. If C is activated in the
round when cell Cp is zoomed in on, Cp is called the parent of C. We consider two cases,
depending on whether or not C is a leaf.
(i) Assume cell C is not a leaf. Since (C) < 2, C overlaps with X2 by Claim 6.9(b).
Note that C is zoomed in on in some round, say in round t  1. Then
5 radt (C)  Wt (C)

by the zooming rule

 VirtWidth(C) + 4 radt (C)

by clean execution,

so radt (C)  VirtWidth(C). Therefore, using Claim 6.6, we have
  (C)  O(radt (C))  O(VirtWidth(C)).
It follows that C  F() (X2 ).
(ii) Assume cell C is a leaf. Let Cp be the parent of C. Since C  Cp , we have (C) 
(Cp ) by Claim 6.9(a). Therefore, invoking case (i), we have
  (C)  (Cp )  O(VirtWidth(Cp )).
Since (C) < 2, C overlaps with X2 by Claim 6.9(b), and therefore so does Cp . It
follows that Cp  F() (X2 ).
fi
fi
Combing these two cases, it follows that |G |  (2m + 1) fiF() (X2 )fi. Plugging this into
(22) and making an appropriate substitution   () to simplify the resulting expression,
we obtain the regret bound in Theorem 4.1
341

fiHo, Slivkins, & Vaughan

7. Simulations
We evaluate the performance of AgnosticZooming through simulations. AgnosticZooming
is compared with two versions of NonAdaptive that use, respectively, two standard bandit
algorithms: UCB1 (Auer et al., 2002) and Thompson Sampling (Thompson, 1933) with
Gaussian priors. In both algorithms, in each round a numerical score (called index ) is
computed for each arm, and an arm with a maximal index is chosen. In UCB1, the index of
an arm is a high-confidence upper bound on the expected reward of this arm. In Thompson
Sampling, the index is sampled independently from the Bayesian posterior distribution of
the arms expected reward.
7.1 Setup
We consider a generalized version of the high-low example from Section 5 in which the
requesters value of the low outcome could be nonzero. In the results reported below, we
set the requesters values to V (high) = 1 and V (low) = .3, and the probability of obtaining
high outcome given high effort to h = .8. While we do not explicitly report the results, we
additionally tried a wide range of alternative values of V (high), V (low), and h and found
that they were similar qualitatively. Intuitively, varying the requesters values and h only
changes which contracts the algorithms converge to (that is, the optimal arms), but does
not impact the problem structure; the width dimension is the same for all settings.
In this generalized high-low example, the workers type is characterized by the cost ch
for high effort. We consider three supply distributions:
 Uniform: ch is uniformly distributed on [0, 1].
 Homogeneous: ch is the same for every worker.
 Two-type: ch is uniformly distributed over two values, c0h and c00h .
These first two distributions represent the extreme cases in which workers are either
extremely homogeneous or extremely diverse. The third distribution is one way to get
at the middle ground. For each distribution, we run each algorithm 100 times.12 For the
Homogeneous Supply Distribution, ch is drawn uniformly at random from [0, 1] for each run.
For the Two-Type Supply Distribution, c0h and c00h are drawn independently and uniformly
from [0, 1] on each run.
For both UCB1 and AgnosticZooming, we replace the logarithmic confidence terms with
small constants. We find this beneficial in practice for both algorithms, which is consistent
with prior work (Radlinski, Kleinberg, & Joachims, 2008; Slivkins, Radlinski, & Gollapudi,
2013). For both algorithms, we tried several different constants and found that performance
is not very sensitive to the particular constant used as long as it is on the order of 1. In the
results reported below, we set these confidence terms equal to 1. For UCB1, this means that

if a given arm a has been played na times, its
p index is the average reward plus 1/ na . For
AgnosticZooming, it means that radt () = 1/nt ().
All three algorithms are run with Xcand = Xcand (), where  > 0 is a parameter
specifying the granularity of discretization.
12. The standard errors in all plots are in the order of 0.001 or less. (Note that each point is not only the
average of 100 runs but also the average of all previous rounds.)

342

fiAdaptive Contract Design for Crowdsourcing Markets

7.2 Overview of the Results.
Across all simulations, AgnosticZooming either outperforms or nearly matches NonAdaptive.
Its performance does not appear to suffer from the large hidden constants that appear in
the analysis. We find that AgnosticZooming converges faster than NonAdaptive when 
is near-optimal or smaller. This is consistent with the intuition that AgnosticZooming
focuses on exploring the more promising regions of contract space. When  is large,
AgnosticZooming converges more slowly than NonAdaptive, but eventually achieves similar
performance. Further, we find that AgnosticZooming with small  performs well compared
to NonAdaptive with larger . In particular, it is not much worse initially, and much better
eventually.
Our simulations suggest that if time horizon T is known in advance and one can tune
 to T , then NonAdaptive can achieve near-optimal performance. However, in real applications approximately optimal  may be difficult to compute, and the T may not be known in
advance. AgnosticZooming performs consistently well with a wide range of  and therefore
does not require prior knowledge of T or careful tuning of .
7.3 Detailed Results
For each algorithm, we compute the time-averaged cumulative utility after T rounds given
b (T, ), for various values of T and .
granularity , denoted U
b (T, ) changes with
First, we fix the time horizon T to 5,000 rounds, and study how U
. The results are shown in Figure 1. We observe that AgnosticZooming either closely
matches or outperforms both versions of NonAdaptive across all supply distributions and
all values of . AgnosticZooming performs consistently well with different  while the
performance of both versions of NonAdaptive decreases rapidly when  is small.
Second, we study how the three algorithms perform over time. Specifically, we plot
b (T, ), for three values of , namely 0.02, 0.08, and 0.2. Since setting  =
T vs. U
0.08 is close to optimal in our examples, these values of  represent, respectively, values that are too small, adequate, and too large. The results are shown in Figure 2. For
small values of , AgnosticZooming quickly zooms in on promising regions of the contract space, leading to faster converge than the alternatives. However, when  is large,
AgnosticZooming converges more slowly, but eventually achieves similar performance. In
this regime, AgnosticZooming does not reap the benefits of adaptive discretization because
the mesh of candidate contracts is too sparse, but still suffers the overhead. This suggests
that if the time horizon T is known in advance and one can optimize  given this T , then
NonAdaptive can achieve near-optimal performance. AgnosticZooming performs consistently with different choices of  and therefore does not require either the prior knowledge
of T or the careful tuning of .
To further demonstrate the benefit of not having to know T or tune , we compare the
performance of AgnosticZooming with small  against that of NonAdaptive with different
b (T, ). See Figure 3.
values of . For each algorithm and each choice of , we plot T vs. U
We only show the results for the Uniform Supply Distribution since the results for other distributions are very similar. Additionally, we omit the results for Thompson Sampling since
UCB1 performed better in these experiments.13 We find that for small T , AgnosticZooming
13. We conjecture that this is because we replaced the logarithmic confidence term in UCB1 with 1.

343

fi0.4

0.4

0.3

0.3

0.2

0.2

0.1
0.0

AgnosticZooming
UCB1
ThompsonSampling

0.1
0.2
0.00

0.05

0.10


0.15

Average Utility

Average Utility

Ho, Slivkins, & Vaughan

0.1
0.0

AgnosticZooming
UCB1
ThompsonSampling

0.1
0.2

0.20

0.00

(a) Uniform Supply Distribution

0.05

0.10


0.15

0.20

(b) Homogeneous Supply Distribution

0.4

Average Utility

0.3
0.2
0.1
0.0

AgnosticZooming
UCB1
ThompsonSampling

0.1
0.2
0.00

0.05

0.10


0.15

0.20

(c) Two-Type Supply Distribution

Figure 1: The requesters average per-round utility after 5,000 rounds vs. the choice of
initial discretization .

with small  converges nearly as fast as NonAdaptive with larger . When T is large,
AgnosticZooming with small  matches NonAdaptive with the optimal .
Finally, in Figure 4, we confirm the intuition that OPT(Xcand ()) decreases with the
granularity . To this end, we run AgnosticZooming for 50,000 rounds (so the algorithm
has time to nearly converge to the optimal contract), and examine the average utility over
the last 5,000 rounds. As expected, we see that the average requester utility achievable
when  is small is significantly higher than the utility achievable when  is larger.
Our simulation results suggest that AgnosticZooming performs well across different
supply distributions and different settings of , not requiring careful tuning of the algorithm
parameters. Given that the smaller the value of , the better the payoff of the optimal
contract OPT(Xcand ()), AgnosticZooming with small  is a good algorithm for a variety
of settings.
344

fiAdaptive Contract Design for Crowdsourcing Markets

AgnosticZooming
0.4
0.3
0.2
0.1
0.0
0.1
0.2

ThompsonSampling

Uniform:  =0.02

Uniform:  =0.08

Uniform:  =0.20

Two-Type:  =0.02

Two-Type:  =0.08

Two-Type:  =0.20

Homogeneous:  =0.02

Homogeneous:  =0.08

Homogeneous:  =0.20

Average Utility

0.4
0.3
0.2
0.1
0.0
0.1
0.2

UCB1

0.4
0.3
0.2
0.1
0.0
0.1
0.2
0

1000 2000 3000 4000 5000

0

1000 2000 3000 4000 5000

0

1000 2000 3000 4000 5000

Time

Figure 2: The requesters average per-round utility over time under different supply distributions and discretization sizes.

8. Application to Dynamic Task Pricing
We discuss dynamic task pricing, which can be seen as the special case of dynamic contract
design in which there is exactly one non-null outcome. We identify an important family of
problem instances for which AgnosticZooming out-performs NonAdaptive.
8.1 Background
The dynamic task pricing problem, in its most basic version, is defined as follows. There
is one principal (buyer) who sequentially interacts with multiple agents (sellers). In each
round t, an agent arrives, with one item for sale. The principal offers price pt for this item,
and the agent agrees to sell if and only if pt  ct , where ct  [0, 1] is the agents private
cost for this item. The principal derives value v for each item bought; his utility is the
value from bought items minus the payment. The time horizon T (the number of rounds)
is known. Each private cost ct is an independent sample from some fixed distribution,
called the supply distribution. We are interested in the prior-independent version, where
the supply distribution is not known to the principal. The algorithms goal is to choose the
offered prices pt so as to maximize the expected utility of the principal.
345

fiHo, Slivkins, & Vaughan

Uniform

0.5

AgnosticZooming:  = .02
UCB1:  = .02
UCB1:  = .08
UCB1:  = .20

0.4

Average Utility

0.3
0.2
0.1
0.0
0.1
0.2
0

1000

2000

Time

3000

4000

5000

Figure 3: The requesters average per-round utility over time using AgnosticZooming with
small  compared with NonAdaptive with three different values of .

Uniform

Average Utility of Last 5k rounds

0.4
0.3
0.2
0.1
0.0
0.1

AgnosticZooming

0.2
0.00

0.05

0.10

0.15


0.20

0.25

0.30

Figure 4: Average requester utility over the last 5,000 rounds in a 50,000-round run of
AgnosticZooming for different values of .

Dynamic task pricing can be seen as the special case of dynamic contract design in
which there is exactly one non-null outcome (which corresponds to a sale). Indeed, in this
special case there is exactly one non-null effort level e without loss of generality (because
any non-null effort levels deterministically lead to the non-null outcome).
346

fiAdaptive Contract Design for Crowdsourcing Markets

One crucial simplification compared to the full generality of dynamic contract design is
that the discretization error can now be easily bounded from above: 14
OPT(X)  OPT(Xcand ())  

for each  > 0.

Worst-case regret bounds are implicit in prior work on dynamic inventory-pricing (Kleinberg & Leighton, 2003).15 Let NonAdaptive() denote algorithm NonAdaptive with Xcand =
Xcand (). Then, by the analysis in the work of Kleinberg and Leighton (2003), NonAdaptive()
achieves regret R(T ) = O(T +  2 ). This is optimized to R(T ) = O(T 2/3 ) if and only
if  = O(T 1/3 ). Moreover, there is a matching lower bound: R(T ) = (T 2/3 ) for any
algorithm.
Further, it is a folklore result that NonAdaptive() achieves regret R(T ) = O(T 2/3 ) if
and only if  = (T 1/3 ). (We sketch a lower-bounding example in the proof of Lemma 8.4,
to make the paper more self-contained.)
8.2 Preliminaries
Each contract is summarized by a single number: the offered price p for the non-null
outcome. Let F (p) be the probability of a worker accepting a task at price p, and let
U (p) = F (p) (v  p) be the corresponding expected utility of the algorithm.
Note that all contracts are trivially monotone and any optimal contract is bounded
without loss of generality. It follows that OPT(X) = supp0 U (p), the optimal expected
utility over all possible prices.
A cell C is just a price interval C = [p, p0 ]  [0, 1], and its virtual width is


VirtWidth(C) = v F (p0 )  p F (p)  v F (p)  p0 F (p0 ) .
8.3 Our Results: The General Case
We will be using AgnosticZooming with Xcand = X.
First, let us prove that this is a reasonable choice in the worst case: namely, that we
achieve the optimal O(T 2/3 ) regret.
Lemma 8.1. Consider the dynamic task pricing problem. AgnosticZooming with Xcand =
X achieves regret O(T 2/3 log T ).
Proof Sketch. Fix  > 0. The key observation is that if VirtWidth(C)   then either
p0  p  4 , or F (p0 )  F (p)  4 . Call C a red cell if the former happens, and blue cell
otherwise. Therefore in any collection of mutually disjoint cells of virtual width   there
can be at most O( 1 ) red cells and at most O( 1 ) blue cells, hence at most O( 1 ) cells total.
It follows that there can be at most O( 1 ) active cells of virtual width  .
So, in the notation of Theorem 4.1 we have N ()  O( 1 ). It follows that the width
dimension is at most 1, which in turn implies the desired regret bound.
14. Recall that Xcand () denotes the set of all prices in [0, 1] that are integer multiples of a given  > 0; call
this set the additive -mesh.
15. The algorithmic result for dynamic task pricing is an easy modification of the analysis in the work of
Kleinberg and Leighton (2003) for dynamic inventory-pricing. The lower bound in the work of Kleinberg
and Leighton can also be translated from dynamic inventory-pricing to dynamic task pricing without
introducing any new ideas. We omit the details from this version.

347

fiHo, Slivkins, & Vaughan

8.4 Our Results: Nice Problem Instances
We focus on problem instances with piecewise-uniform costs and bounded density. Formally,
we say that an instance of dynamic task pricing has k-piecewise-uniform costs if the interval
[0,1] is partitioned into k  N sub-intervals such that the supply distribution is uniform
on each sub-interval. A problem instance has -bounded density,   1 if the supply
distribution has a probability density function almost everywhere, and the density is between
1
 and . Using the full power of Theorem 4.1, we obtain the following regret bound.
Theorem 8.2. Consider the dynamic task pricing problem with k-piecewise-uniform costs
and -bounded density, for some absolute constants k  N and  > 1. AgnosticZooming
with Xcand = X achieves regret R(T ) = O(T 3/5 ).
Proof Sketch. Since the supply distribution has density at most , it follows that F () is a
Lipschitz-continuous function with Lipschitz constant . It follows that each cell of virtual
width at least  has diameter at least (/), for any  > 0. (Note that each cell is now
simply a sub-interval [p, q]  [0, 1], so its diameter is simply q  p.)

Second, we claim that X is contained in a union of k intervals of diameter O( ). To
see this, consider the partition of [0, 1] into k subintervals such that the supply distribution
has a uniform density on each subinterval. Let [pj , qj ] be the j-th subinterval. Let pj be the
local optimum of U () on this subinterval, and let Xj, = {x  [pj , qj ] : U (pj )  U (x)  }.

Then X  j Xj, . We can show that Xj,  [pj  , pj + ] for some  = O( ).
Recall that N0 (X ) is the number of feasible cells of virtual width at least 0 which
overlap with X . It follows that N0 (X ) is at most k times the maximal number
 of
feasible cells of diameter at least (/) that overlap with an interval of diameter O( ).
Therefore: N0 (X ) = O(k3/2 1/2 log 1 ). Moreover, we have a less sophisticated upper
bound on N0 (X ): it is at most the number of feasible cell of diameter at least (/).
So N0 (X ) = O(/)(log 1 ). The theorem follows by plugging both upper bounds on
N0 (X ) into Equation (6).
8.5 Comparison with NonAdaptive
Consider NonAdaptive(0 ), where 0 = (T 1/3 ) is the granularity required for the optimal
worst-case performance. Call a problem instance nice if it has 2-piecewise-uniform costs
and -bounded density, for some sufficiently large absolute constant ; say  = 4 for
concreteness. We claim that AgnosticZooming outperforms NonAdaptive(0 ) on the nice
problem instances.
Lemma 8.3. NonAdaptive(0 ) achieves regret R(T ) = (T 2/3 ) in the worst case over all
nice problem instances.
Proof Sketch. Recall that for k = 2 the supply distribution has density 1 on interval
[0, p0 ], and density 2 on interval [p0 , 1], for some numbers 1 , 2 , p0 . We pick p0 so that
it is sufficiently far from any point in Xcand (0 ). Note that the function U () is a parabola
on each of the two intervals. We adjust the densities so that U () achieves its maximum at
p0 , and the maximum of either of the two parabolas is sufficiently far from p0 . Then the
discretization error of Xcand (0 ) is at least (0 ), which implies regret (0 T ).
348

fiAdaptive Contract Design for Crowdsourcing Markets

8.6 Lower Bound for NonAdaptive
We provide a specific lower-bounding example for the worst-case performance of NonAdaptive(),
for an arbitrary  > 0. Let F be the family of all problem instances with k-piecewiseuniform costs and -bounded density, for all k  N and  = 4.
Lemma 8.4. Let R (T ) be the maximal
over all problem inp regret of NonAdaptive()
2/3
stances in F. Then R (T ) = (T + T /)  (T ).
Proof Sketch. For piecewise-uniform costs, we have F (0) = 0 and F (p) = 1. Assume that
the principal derives value v = 1 from each item. Then the expected utility from price p is
U (p) = F (p)(1  p).
Fix  > 0. Use the following problem instance. Let P = [ 25 , 35 ]  {4j +  : j  N}.
Set U (p) = 41 for each p  P0 . Further, pick some p  P/2 and set U (p ) = 41 + ().
This defines F (p) for p  P  {0, 1, p }. For the rest of the prices, define F () via linear
interpolation. This completes the description of the problem instance.
We show that X consists of N = ( 1 ) candidate contracts. Therefore, using stanp

dard lower-bounding arguments for MAB, we obtain R(T |Xcand )  ( T N ) = ( T /).
Further, we show that the discretization error is at least (), implying that R(T ) 
R(T |Xcand ) + (T ).

9. Related Work
This paper is related to three different areas: contract theory, market design for crowdsourcing, and online decision problems. Below we outline connections to each of these
areas.
9.1 Contract Theory
Our model can be viewed as an extension of the classic principal-agent model from contract
theory (Laffont & Martimort, 2002). In the most basic version of the classic model, a
single principal interacts with a single agent whose type (specified by a cost function and
production function, as described in Section 2) is generally assumed to be known. The
principal specifies a contract mapping outcomes to payments that the principal commits to
make to the agent. The agent then chooses an action (i.e., effort level) that stochastically
results in an outcome in order to maximize his expected utility given the contract. The
principal observes the outcome, but cannot directly observe the agents effort level, creating
a moral hazard problem. The goal of the principal is to design a contract to maximize
her own expected utility, which is the difference between the utility she receives from the
outcome and the payment she makes. This maximization can be written as a constrained
optimization problem, and it can be shown that linear contracts are optimal.
The adverse selection variation of the principal-agent problem relaxes the assumption
that the agents type is known. Most existing literature on the principal-agent problem with
adverse selection focuses on applying the revelation principle (Laffont & Martimort, 2002).
In this setting, the principal offers a menu of contracts, and the contract chosen by the agent
reveals the agents type. The problem of selecting a menu of contracts that maximizes the
principals expected utility can again be formulated as a constrained optimization.
349

fiHo, Slivkins, & Vaughan

Our work differs from the classic setting in that we consider a principal interacting with
multiple agents, and the principal may adjust her contract over time in an online manner.
Several other authors have considered extensions of the classic model to multiple agents.
Levy and Vukina (2002) show that with multiple agents it is optimal to set individual
linear contracts for each agent rather than a single uniform contract for all agents, but offer
a variety of descriptive explanations for why it is more common to see uniform contracts
in practice. Babaioff, Feldman, and Nisan (2006) consider a setting in which one principal
interacts with multiple agents, but observes only a single outcome which is a function of
all agents effort levels. Misra, Nair, and Daljord (2012) consider a variant in which the
algorithm must decide both how to set a uniform contract for many agents and how to
select a subset of agents to hire.
Alternative online versions of the problem have been considered in the literature as well.
In dynamic principal agent problem (Sannikov, 2008; Williams, 2009; Sannikov, 2012), a
single principal interacts with a single agent repeatedly over a period of time. The agent
can choose to exert different effort at different time, and the outcome at time t is a function
of all the efforts exerted by the agent before t. The principal cannot observe the agents
efforts but can observe the outcome. The goal of the principal is to design an optimal
contract over time to maximize his payoff. Our work is different from this line of work since
we consider the setting with multiple agents with different, unknown types. Our algorithm
needs to learn the distribution of agent types and design an optimal contract accordingly.
Conitzer and Garera (2006) study the online principal agent problem with a similar
setting to ours. However, they focus on empirically comparing different online algorithms,
including bandit approaches with uniform discretization, gradient ascent, and Bayesian
update approaches to the problem. Our goal is to provide an algorithm with nice theoretical
guarantees.
Bohren and Kravitz (2013) study the setting when the outcome is unverifiable. To
address this issue, they propose to assign a bundle of tasks to each worker. To verify
the outcome, each task in the bundle is chosen as a verifiable task with some non-trivial
probability. A verifiable task can either be a gold standard task with known answer or a
task which is assigned to multiple workers for verification. The payment for a task bundle
is then conditional only on the outcome of verified tasks. In our setting, we assume the task
outcome is verifiable. We can relax this assumption by adopting similar approaches.
9.2 Incentives in Crowdsourcing Systems
Researchers have recently begun to examine the design of incentive mechanisms to encourage
high-quality work in crowdsourcing systems. Jain, Chen, and Parkes (2012) explore ways in
which to award virtual points to users in online question-and-answer forums to improve the
quality of answers. Ghosh and Hummel (2011, 2013) and Ghosh and McAfee (2011) study
how to distribute user generated content (e.g., Youtube videos) to users to encourage the
production of high-quality internet content by people who are motivated by attention. Ho,
Zhang, Vaughan, and van der Schaar (2012) and Zhang and van der Schaar (2012) consider
the design of two-sided reputation systems to encourage good behavior from both workers
and requesters in crowdsourcing markets. While we also consider crowdsourcing markets,
350

fiAdaptive Contract Design for Crowdsourcing Markets

our work differs in that it focuses on how to design monetary contracts, perhaps the most
natural incentive scheme, to incentivize workers to exert effort.
The problem closest to ours which has been studied in the context of crowdsourcing
systems is the online task pricing problem in which a requester has an unlimited supply of
tasks to be completed and a budget B to spend on them (Badanidiyuru et al., 2012; Singer
& Mittal, 2013). Workers with private costs arrive online, and the requester sets a single
price for each arriving worker. The goal is to learn the optimal single fixed price over time.
Our work can be viewed as a generalization of the task pricing problem, which is a special
case of our setting with the number of non-null outcomes m fixed at 1.
There has also been empirical work examining how workers behavior varies based on the
financial incentives offered in crowdsourcing markets. Mason and Watts (2009) study how
workers react to changes of performance-independent financial incentives. In their study,
increasing financial incentives increases the number of tasks workers complete, but not the
quality of their output. Yin, Chen, and Sun (2013) provide a potential explanation for this
phenomenon using the concept of anchoring effect: a workers cost for completing a task
is influenced by the first price the worker sees for this task. Horton and Chilton (2010) run
experiments to estimate workers reservation wage for completing tasks. They show that
many workers respond rationally to offered contracts, whereas some of the workers appeared
to have some target payment in mind.
Some recent research studies the effects of performance-based payments (PBPs). Harris
(2011) runs MTurk experiments on resume screening, where workers can get a bonus if
they perform well. He concludes that the quality of work is better with PBPs than with
uniform payments. Yin et al. (2013) show that varying the magnitude of the bonus does
not have much effect in certain settings. Ho et al. (2015) perform a more comprehensive set
of experiments aimed at determining whether, when, and why PBPs increase the quality
of submitted work. Their results suggest that PBPs can increase quality on tasks for
which increased time or effort leads to higher quality work. Their results also suggest that
workers may interpret a contract as performance-based even if it is not stated as such (since
requesters always have the option to reject work). Based on this evidence, they propose a
new model of worker behavior that extends the principal-agent model to explicitly reflect
workers subjective beliefs about their likelihood of being paid.
Overall, previous empirical work demonstrates that workers in crowdsourcing markets
do respond to the change of financial incentives, but that their behavior does not always
follow the traditional rational-worker model  similar to people in any real-world market.
In our work, we start our analysis with the rational-worker assumption ubiquitous in economic theory, but demonstrate that our results can still hold without these assumptions as
long as the collective worker behavior satisfies some natural properties (namely, as long as
Lemma 3.1 holds). We note that our results hold under the generalized worker model proposed by Ho et al. (2015), which is consistent with their experimental evidence as discussed
above.
351

fiHo, Slivkins, & Vaughan

9.3 Sequential Decision Problems
In sequential decision problems, an algorithm makes sequential decisions over time. Two
directions that are relevant to this paper are multi-armed bandits (MAB) and dynamic
pricing.
MAB have been studied since 1933 (Thompson, 1933) in operations research, economics,
and several branches of computer science including machine learning, theoretical computer
science, AI, and algorithmic economics. A survey of prior work on MAB is beyond the scope
of this paper; the reader is encouraged to refer to the work of Cesa-Bianchi and Lugosi (2006)
or Bubeck and Cesa-Bianchi (2012) for background on prior-independent MAB, and to the
work of Gittins, Glazebrook, and Weber (2011) for background on Bayesian MAB. Below
we briefly discuss the lines of work on MAB that are directly relevant to our paper.
Our setting can be modeled as prior-independent MAB with stochastic rewards: the reward of a given arm i is an i.i.d. sample of some time-invariant distribution, and neither this
distribution nor a Bayesian prior on it are known to the algorithm. The basic formulation
(with a small number of arms) is well understood (Lai & Robbins, 1985; Auer et al., 2002;
Bubeck & Cesa-Bianchi, 2012). To handle problems with a large or infinite number of arms,
one typically needs side information on similarity between arms. A typical way to model
this side information, called Lipschitz MAB (Kleinberg et al., 2008), is that an algorithm
is given a distance function on the arms, and the expected rewards are assumed to satisfy
Lipschitz-continuity (or a relaxation thereof) with respect this distance function (Agrawal,
1995; Kleinberg, 2004; Auer et al., 2007; Kleinberg et al., 2008; Bubeck et al., 2011a;
Slivkins, 2014). Most related to this paper is the idea of adaptive discretization which is
often used in this setting (Kleinberg et al., 2008; Bubeck et al., 2011a; Slivkins, 2014), and
particularly the zooming algorithm (Kleinberg et al., 2008; Slivkins, 2014). In particular,
the general template of our algorithm is similar to the one in the zooming algorithm (but
our selection rule and zooming rule are very different, reflecting the lack of a priori
known similarity information).
In some settings (including ours), the numerical similarity information required for Lipschitz MAB is not immediately available. For example, in applications to web search and
advertising it is natural to assume that an algorithm can only observe a tree-shaped taxonomy on arms (Kocsis & Szepesvari, 2006; Munos & Coquelin, 2007; Pandey et al., 2007;
Slivkins, 2011; Bull, 2013). In particular, Slivkins (2011) and Bull (2013) explicitly reconstruct (the relevant parts of) the metric space defined by the taxonomy. In a different
direction, Bubeck, Stoltz, and Yu (2011b) study a version of Lipschitz MAB where the
Lipschitz constant is not known, and essentially recover the performance of NonAdaptive
for this setting.
In MAB with partial monitoring (Audibert & Bubeck, 2010; Bartok, Foster, Pal,
Rakhlin, & Szepesvari, 2014; Antos, Bartok, Pal, & Szepesvari, 2013), in each round the
algorithm receives auxiliary feedback about rewards in this round (along with the reward
of the chosen arm), and the goal is to take advantage of this auxiliary feedback. Dynamic
task pricing can be cast in this framework: if a given price p is accepted, then any higher
price would be too, and if it is rejected, then any lower price would be. However, we are
not aware of any way to link dynamic task pricing to existing results on partial monitoring
352

fiAdaptive Contract Design for Crowdsourcing Markets

via this connection. The general version of dynamic contract design does not appear to fit
the partial monitoring framework, essentially due to moral hazard.
Dynamic pricing (a.k.a. online posted-price auctions) refers to settings in which a
principal interacts with agents that arrive over time and offers each agent a price for a
transaction, such as selling or buying an item. The version in which the principal sells items
has been extensively studied in operations research, typically in a Bayesian setting; see the
work of den Boer (2015) for a through literature review. The study of prior-independent,
non-parameterized formulations has been initiated in the work of Blum, Kumar, Rudra,
and Wu (2003) and Kleinberg and Leighton (2003) and continued by several others (Besbes
& Zeevi, 2009; Babaioff, Dughmi, Kleinberg, & Slivkins, 2015; Besbes & Zeevi, 2012; Wang,
Deng, & Ye, 2014; Badanidiyuru et al., 2013; Badanidiyuru, Langford, & Slivkins, 2014).
Further, Badanidiyuru et al. (2012) and Singla and Krause (2013) studied the version in
which the principal buys items, or equivalently commissions tasks; we call this version
dynamic task pricing. Modulo budget constraints, this is essentially the special case of our
setting where in each round a worker is offered the chance to perform a task at a specified
price, and can either accept or reject this offer. In particular, the workers strategic choice
is directly observable. More general settings have been studied (Badanidiyuru et al., 2013,
2014; Agrawal & Devanur, 2014; Agrawal, Devanur, & Li, 2015).16 However, all this work
(after the initial papers, see Blum et al., 2003 and Kleinberg & Leighton, 2003) has focused
on models with constraints on the principals supply or budgets, and does not imply any
improved results when specialized to unconstrained settings.

10. Conclusions
Motivated by applications to crowdsourcing markets, we define the dynamic contract design
problem, a multi-round version of the principal-agent model with unobservable strategic
decisions. We treat this problem as a multi-armed bandit problem, design an algorithm for
this problem, and derive regret bounds which compare favorably to prior work. Our main
conceptual contribution, aside from identifying the model, is the adaptive discretization
approach that does not rely on Lipschitz-continuity assumptions. We provably improve
on the uniform discretization approach from prior work, both in the general case and in
some illustrative special cases. These theoretical results are supported by simulations. The
generality and the shortcomings of our model are discussed in Section 2.2.
We believe that the dynamic contract design problem deserves further study, in several
directions that we outline below.
1. It is not clear whether our provable results can be improved, perhaps using substantially
different algorithms and relative to different problem-specific structures. In particular, one
needs to establish lower bounds in order to argue about optimality; no lower bounds for
dynamic contract design are currently known.
2. Our adaptive discretization approach may be fine-tuned to improve its performance
in practice. In particular, the definition of the index It (C) of a given feasible cell C
16. The papers by Badanidiyuru et al. (2014) and Agrawal and Devanur (2014) are concurrent and independent work with respect to the conference publication of this paper, and the work of Agrawal et al.
(2015) is subsequent work.

353

fiHo, Slivkins, & Vaughan

may be re-defined in several different ways. First, it can use the information from C in
a more sophisticated way, similar to the more sophisticated indices for the basic K-armed
bandit problem; for example, see the work of Garivier and Cappe (2011). Second, the index
can incorporate information from other cells. Third, it can be defined in a smoother,
probabilistic way, e.g., as in Thompson Sampling (Thompson, 1933).
3. Deeper insights into the structure of the (static) principal-agent problem are needed,
primarily in order to optimize the choice of Xcand , the set of candidate contracts. The
most natural target here is the uniform mesh Xcand (). To optimize the granularity , one
needs to upper-bound the discretization error OPT(Xcand )  OPT(Xcand ()) in terms of some
function f () such that f ()  0 as   0. The first-order open question is to resolve
whether this can be done in the general case, or provide a specific example when it cannot.
A related open question concerns the effect of increasing the granularity: upper-bound the
difference OPT(Xcand ())  OPT(Xcand (0 )),  > 0 > 0, in terms of some function of  and 0 .
Further, it is not known whether the optimal mesh of contracts is in fact a uniform mesh.
Also of interest is the effect of restricting our attention to monotone contracts. While
we prove that monotone contracts may not be optimal (Appendix A), the significance of
this phenomenon is unclear. One would like to characterize the scenarios when restricting
to monotone contracts is alright (in the sense that the best monotone contract is as good,
or not much worse, than the best contract), and the scenarios when this restriction results
in a significant loss. For the latter scenarios, different algorithms may be needed.
4. A much more extensive analysis of special cases is in order. Our general results are
difficult to access (which appears to be an inherent property of the general problem), so the
most immediate direction for special cases is deriving lucid corollaries from the current regret
bounds. In particular, it is desirable to optimize the choice of candidate contracts. Apart
from massaging the current results, one can also design improved algorithms and derive
specialized lower bounds. Particularly appealing special cases concern supply distributions
that are mixtures of a small number of types, and supply distributions that belong to a
(simple) parameterized family with unknown parameter.
Going beyond our current model, a natural direction is to incorporate a budget constraint, extending the corresponding results on dynamic task pricing. The main difficulty
for such settings is that a distribution over two contracts may perform much better than
any fixed contract; see the work of Badanidiyuru et al. (2013) for discussion. Effectively,
an algorithm needs to optimize over the distributions. As a first step, one can use nonadaptive discretization in conjunction with the general algorithms for bandits with budget
constraints, sometimes called bandits with knapsacks (Badanidiyuru et al., 2013; Agrawal
& Devanur, 2014). However, it is not clear how to choose an optimal mesh of contracts
(as we discussed throughout the paper), and this mesh is not likely to be uniform (because
it is not uniform for the special case of dynamic task pricing with a budget; see Badanidiyuru et al., 2013 for discussion). The eventual target in this research direction is to marry
adaptive discretization and the techniques from prior work on bandits with knapsacks.

354

fiAdaptive Contract Design for Crowdsourcing Markets

Acknowledgments
We thank the anonymous reviewers for their useful comments. Much of this research was
completed while Ho was an intern at Microsoft Research. This research was partially supported by the NSF under grant IIS-1054911. Any opinions, findings, conclusions, or recommendations are those of the authors alone.

Appendix A. Monotone Contracts May Not Be Optimal
In this section we provide an example of a problem instance for which all monotone contracts
are suboptimal (at least when restricting attention to only those contracts with non-negative
payoffs). In this example, there are three non-null outcomes (i.e., m = 3), and two non-null
effort levels, low effort and high effort, which we denote e` and eh respectively. There
is only a single worker type. Since there is only one type, we drop the subscript when
describing the cost function c. We let c(e` ) = 0, and let c(eh ) be any positive value less
than 0.5(v(2)  v(1)). If a worker chooses low effort, the outcome is equally likely to be 1 or
3. If the worker chooses high effort, it is equally likely to be 2 or 3. It is easy to verify that
this type satisfies the FOSD assumption. Finally, for simplicity, we assume that all workers
break ties between high effort and any other effort level in favor of high effort, and that all
workers break ties between low effort and the null effort level in favor of low effort.
Lets consider the optimal contract. Since there is just a single worker type and all
workers of this type break ties in the same way, we can consider separately the best contract
that would make all workers choose the null effort level, the best contract that would make
all workers choose low effort, and the best contract that would make all workers choose high
effort, and compare the requesters expected value for each.
Since c(e` ) = 0 and workers break ties between low effort and null effort in favor of low
effort, there is no contract that would cause workers to choose null effort; workers always
prefer low effort to null effort.
It is easy to see that the best contract (in terms of requester expected value) that would
make workers choose low effort would set x(1) = x(3) = 0 and x(2) sufficiently low that
workers would not be enticed to choose high effort; setting x(2) = 0 is sufficient. In this
case, the expected value of the requester would be 0.5(v(1) + v(3)).
Now lets consider contracts that cause workers to choose high effort. If a worker chooses
high effort, the expected value to the requester is
0.5(v(2)  x(2) + v(3)  x(3)).

(23)

Workers will choose high effort if and only if
0.5(x(1) + x(3))  0.5(x(2) + x(3))  c(eh )
or
0.5x(1)  0.5x(2)  c(eh ).

(24)

So to find the contract that maximizes the requesters expected value when workers choose
high effort, we want to maximize Equation 23 subject to the constraint in Equation 24.
Since x(3) doesnt appear in Equation 24, we can set it to 0 to maximize Equation 23.
355

fiHo, Slivkins, & Vaughan

Since x(1) does not appear in Equation 23, we can set x(1) = 0 to make Equation 24 as
easy as possible to satisfy. We can then see that the optimal occurs when x(2) = 2c(eh ).
Plugging this contact x into Equation 23, the expected utility in this case is 0.5(v(2) +
v(3))  c(eh ). Since we assumed that c(eh ) < 0.5(v(2)  v(1))), this is strictly preferable to
the constant 0 contract, and is in fact the unique optimal contract. Since x(2) > x(3), the
unique optimal contract is not monotonic.

References
Agrawal, R. (1995). The continuum-armed bandit problem. SIAM J. Control and Optimization, 33 (6), 19261951.
Agrawal, S., & Devanur, N. R. (2014). Bandits with concave rewards and convex knapsacks.
In 15th ACM Conf. on Economics and Computation (EC).
Agrawal, S., Devanur, N. R., & Li, L. (2015). Contextual bandits with global constraints
and objective.. Technical report, arXiv:1506.03374.
Antos, A., Bartok, G., Pal, D., & Szepesvari, C. (2013). Toward a classification of finite
partial-monitoring games. Theor. Comput. Sci., 473, 7799.
Audibert, J., & Bubeck, S. (2010). Regret Bounds and Minimax Policies under Partial
Monitoring. J. of Machine Learning Research (JMLR), 11, 27852836.
Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002). Finite-time analysis of the multiarmed
bandit problem.. Machine Learning, 47 (2-3), 235256.
Auer, P., Ortner, R., & Szepesvari, C. (2007). Improved Rates for the Stochastic ContinuumArmed Bandit Problem. In 20th Conf. on Learning Theory (COLT), pp. 454468.
Babaioff, M., Dughmi, S., Kleinberg, R. D., & Slivkins, A. (2015). Dynamic pricing with
limited supply. ACM Trans. on Economics and Computation, 3 (1), 4.
Babaioff, M., Feldman, M., & Nisan, N. (2006). Combinatorial agency. In 7th ACM Conf.
on Electronic Commerce (EC).
Badanidiyuru, A., Kleinberg, R., & Singer, Y. (2012). Learning on a budget: posted price
mechanisms for online procurement. In 13th ACM Conf. on Electronic Commerce
(EC), pp. 128145.
Badanidiyuru, A., Kleinberg, R., & Slivkins, A. (2013). Bandits with knapsacks. In 54th
IEEE Symp. on Foundations of Computer Science (FOCS).
Badanidiyuru, A., Langford, J., & Slivkins, A. (2014). Resourceful contextual bandits. In
27th Conf. on Learning Theory (COLT).
Bartok, G., Foster, D. P., Pal, D., Rakhlin, A., & Szepesvari, C. (2014). Partial monitoring
- classification, regret bounds, and algorithms. Math. Oper. Res., 39 (4), 967997.
Besbes, O., & Zeevi, A. (2009). Dynamic pricing without knowing the demand function:
Risk bounds and near-optimal algorithms. Operations Research, 57, 14071420.
Besbes, O., & Zeevi, A. J. (2012). Blind network revenue management. Operations Research,
60 (6), 15371550.
356

fiAdaptive Contract Design for Crowdsourcing Markets

Blum, A., Kumar, V., Rudra, A., & Wu, F. (2003). Online learning in online auctions. In
14th ACM-SIAM Symp. on Discrete Algorithms (SODA), pp. 202204.
Bohren, J. A., & Kravitz, T. (2013). Incentives for spot market labor when output is
unverifiable. Working paper.
Bubeck, S., & Cesa-Bianchi, N. (2012). Regret Analysis of Stochastic and Nonstochastic
Multi-armed Bandit Problems. Foundations and Trends in Machine Learning, 5 (1),
1122.
Bubeck, S., Munos, R., Stoltz, G., & Szepesvari, C. (2011a). Online Optimization in XArmed Bandits. J. of Machine Learning Research (JMLR), 12, 15871627.
Bubeck, S., Stoltz, G., & Yu, J. Y. (2011b). Lipschitz bandits without the lipschitz constant.
In 22nd Intl. Conf. on Algorithmic Learning Theory (ALT), pp. 144158.
Bull, A. D. (2013). Adaptive-treed bandits. Tech. rep. 1302.2489, arxiv.org.
Cesa-Bianchi, N., & Lugosi, G. (2006). Prediction, learning, and games. Cambridge Univ.
Press.
Conitzer, V., & Garera, N. (2006). Online learning algorithms for online principal-agent
problems (and selling goods online). In International Conference on Machine Learning
(ICML).
den Boer, A. V. (2015). Dynamic pricing and learning: Historical origins, current research,
and new directions. Surveys in Operations Research and Management Science. Forthcoming.
Dudik, M., Hsu, D., Kale, S., Karampatziakis, N., Langford, J., Reyzin, L., & Zhang, T.
(2011). Efficient optimal leanring for contextual bandits. In 27th Conf. on Uncertainty
in Artificial Intelligence (UAI).
Garivier, A., & Cappe, O. (2011). The KL-UCB Algorithm for Bounded Stochastic Bandits
and Beyond. In 24th Conf. on Learning Theory (COLT).
Ghosh, A., & Hummel, P. (2011). A game-theoretic analysis of rank-order mechanisms for
user-generated content. In 12th ACM Conf. on Electronic Commerce (EC).
Ghosh, A., & Hummel, P. (2013). Learning and incentives in user-generated content: Multiarmed bandits with endogenous arms. In Proc. 4th Conference on Innovations in
Theoretical Computer Science (ITCS).
Ghosh, A., & McAfee, P. (2011). Incentivizing high-quality user-generated content. In 20th
Intl. World Wide Web Conf. (WWW).
Gittins, J., Glazebrook, K., & Weber, R. (2011). Multi-Armed Bandit Allocation Indices.
John Wiley & Sons.
Harris, C. G. (2011). Youre hired! an examination of crowdsourcing incentive models in
human resource tasks. In CSDM.
Ho, C., Slivkins, A., Suri, S., & Vaughan, J. W. (2015). Incentivizing high quality crowdwork. In 24th Intl. World Wide Web Conf. (WWW).
Ho, C.-J., Zhang, Y., Vaughan, J. W., & van der Schaar, M. (2012). Towards social norm
design for crowdsourcing markets. In HCOMP.
357

fiHo, Slivkins, & Vaughan

Horton, J. J., & Chilton, L. B. (2010). The labor economics of paid crowdsourcing. In 11th
ACM Conf. on Electronic Commerce (EC).
Jain, S., Chen, Y., & Parkes, D. (2012). Designing incentives for online question-and-answer
forums. Games and Economic Behavior.
Kleinberg, R. (2004). Nearly tight bounds for the continuum-armed bandit problem. In
18th Advances in Neural Information Processing Systems (NIPS).
Kleinberg, R., & Leighton, T. (2003). The value of knowing a demand curve: Bounds
on regret for online posted-price auctions.. In 44th IEEE Symp. on Foundations of
Computer Science (FOCS), pp. 594605.
Kleinberg, R., Slivkins, A., & Upfal, E. (2008). Multi-armed bandits in metric spaces. In
40th ACM Symp. on Theory of Computing (STOC), pp. 681690.
Kleinberg, R. D., & Leighton, F. T. (2003). The value of knowing a demand curve: Bounds
on regret for online posted-price auctions. In IEEE Symp. on Foundations of Computer
Science (FOCS).
Kocsis, L., & Szepesvari, C. (2006). Bandit Based Monte-Carlo Planning. In 17th European
Conf. on Machine Learning (ECML), pp. 282293.
Laffont, J.-J., & Martimort, D. (2002). The Theory of Incentives: The Principal-Agent
Model. Princeton University Press.
Lai, T. L., & Robbins, H. (1985). Asymptotically efficient Adaptive Allocation Rules.
Advances in Applied Mathematics, 6, 422.
Levy, A., & Vukina, T. (2002). Optimal linear contracts with heterogeneous agents. In
European Review of Agricultural Economics.
Mason, W., & Watts, D. (2009). Financial incentives and the performance of crowds. In
HCOMP.
Misra, S., Nair, H. S., & Daljord, O. (2012). Homogenous contracts for heterogeneous
agents: Aligning salesforce composition and compensation. Working Paper.
Munos, R., & Coquelin, P.-A. (2007). Bandit algorithms for tree search. In 23rd Conf. on
Uncertainty in Artificial Intelligence (UAI).
Pandey, S., Agarwal, D., Chakrabarti, D., & Josifovski, V. (2007). Bandits for Taxonomies:
A Model-based Approach. In SIAM Intl. Conf. on Data Mining (SDM).
Radlinski, F., Kleinberg, R., & Joachims, T. (2008). Learning diverse rankings with multiarmed bandits. In 25th Intl. Conf. on Machine Learning (ICML), pp. 784791.
Sannikov, Y. (2008). A continuous-time version of the principal-agent problem. In The
Review of Economics Studies.
Sannikov, Y. (2012). Contracts: The theory of dynamic principal-agent relationships and
the continuous-time approach. In 10th World Congress of the Econometric Society.
Singer, Y., & Mittal, M. (2013). Pricing mechanisms in crowdsourcing markets. In 22nd
Intl. World Wide Web Conf. (WWW).
Singla, A., & Krause, A. (2013). Truthful incentives in crowdsourcing tasks using regret
minimization mechanisms. In 22nd Intl. World Wide Web Conf. (WWW).
358

fiAdaptive Contract Design for Crowdsourcing Markets

Slivkins, A. (2011). Multi-armed bandits on implicit metric spaces. In 25th Advances in
Neural Information Processing Systems (NIPS).
Slivkins, A. (2014). Contextual bandits with similarity information. J. of Machine Learning
Research (JMLR), 15 (1), 25332568. Preliminary version in COLT 2011.
Slivkins, A., Radlinski, F., & Gollapudi, S. (2013). Ranked bandits in metric spaces: Learning optimally diverse rankings over large document collections. J. of Machine Learning
Research (JMLR), 14 (Feb), 399436. Preliminary version in 27th ICML, 2010.
Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another
in view of the evidence of two samples.. Biometrika, 25 (3-4), 285294.
Wang, Z., Deng, S., & Ye, Y. (2014). Close the gaps: A learning-while-doing algorithm for
single-product revenue management problems. Operations Research, 62 (2), 318331.
Williams, N. (2009). On dynamic principal-agent problems in continuous time. Working
Paper.
Yin, M., Chen, Y., & Sun, Y.-A. (2013). The effects of performance-contingent financial
incentives in online labor markets. In AAAI.
Zhang, Y., & van der Schaar, M. (2012). Reputation-based incentive protocols in crowdsourcing applications. In Infocom.

359

fiJournal of Artificial Intelligence Research 55 (2016) 209-248

Submitted 03/15; published 01/16

Synthetic Treebanking for Cross-Lingual
Dependency Parsing
Jorg Tiedemann

jorg.tiedemann@helsinki.fi

Department of Modern Languages, University of Helsinki
P.O. Box 24, FI-00014 University of Helsinki, Finland

Zeljko Agic

zeljko.agic@hum.ku.dk

Center for Language Technology, University of Copenhagen
Njalsgade 140, 2300 Copenhagen S, Denmark

Abstract
How do we parse the languages for which no treebanks are available? This contribution
addresses the cross-lingual viewpoint on statistical dependency parsing, in which we attempt
to make use of resource-rich source language treebanks to build and adapt models for the
under-resourced target languages. We outline the benefits, and indicate the drawbacks of
the current major approaches. We emphasize synthetic treebanking: the automatic creation
of target language treebanks by means of annotation projection and machine translation.
We present competitive results in cross-lingual dependency parsing using a combination
of various techniques that contribute to the overall success of the method. We further
include a detailed discussion about the impact of part-of-speech label accuracy on parsing
results that provide guidance in practical applications of cross-lingual methods for truly
under-resourced languages.

1. Introduction
Languages are dialects with an army and a navy is a famous saying popularized by the
sociolinguist Max Weinreich. In modern times, this quote could be rephrasedand languages
definedas dialects with a part-of-speech tagger, a treebank, and a machine translation
system. Even though this proposition would disqualify most languages of the world, it is
true that the existence of many languages is threatened due to insufficient resources and
technical support. Natural language processing (NLP) becomes increasingly important in
peoples everyday life if we look, for example, at the success of word prediction, spelling
correction, and instant on-line translation. Building linguistic resources and tools, however,
is expensive and time-consuming, and one of the great challenges in computational linguistics
is to port existing models to new languages and domains.
Modern NLP requires data, often annotated with explicit linguistic information, and
tools that can learn from them. However, sufficient quantities of electronic data sources
are available only for a handful of languages whereas most other languages do not have
the privilege to draw from such resources (Bender, 2011; Uszkoreit & Rehm, 2012; Bender,
2013). Speakers of low-density languages and the countries they live in are not able to invest
in large data collection and time-consuming annotation efforts, and the goal of cross-lingual
c
2016
AI Access Foundation. All rights reserved.

fiTiedemann & Agic

NLP is to share the rich linguistic information with poorly supported languages, making it
possible to build tools and resources without starting from scratch.
In this paper, we consider the task of statistical dependency parsing (Kubler, McDonald,
& Nivre, 2009). Top-performing dependency parsers are typically trained on dependency
treebanks that include several thousands of manually annotated sentences. These statistical
parsing models are known to be robust and very efficient, yielding high accuracy on unseen
texts. However, even moderately-sized treebanks take a lot of time and resources to produce
(Abeille, 2003), and at this point, they are unavailable or scarce even for major languages.
Thus, similar to other areas of NLP research, we face the challenge posed in our abstract:
How do we parse the languages for which no dependency treebanks are available? Without
annotated training data we basically have four options in data-driven NLP:
1. We can build parsing models that can learn from raw data using unsupervised machine
learning techniques.
2. If manually annotated data is scarcely available, we can resort to various approaches
to semi-supervised learning, leveraging the various sources of fortuitous data (Sgaard,
2013).
3. We can transfer existing models and tools to new languages.
4. We can transfer data from resource-rich languages to resource-poor languages and
build tools on those data sets.
All four viewpoints are studied intensively not only in connection with dependency parsing
but in NLP in general. For parsing, the first option is especially difficult and unsupervised
approaches still fall far behind the rest of the field (Sgaard, 2012). Unsupervised models are
also difficult to evaluate and applications that build on labeled information have problems
in making use of the structures produced by those models. Semi-supervised learning either
augments well-resourced environments for improved cross-domain robustness, or largely
coincides with the cross-lingual approaches as it is very loosely defined (Sgaard, 2013).
Therefore, it is not surprising that the final two options have attracted quite some popularity
and gained a lot of merit in enabling parsing for low-resource languages. In this paper, we
exclusively look at those techniques.
The basic idea behind transfer approaches is that tools and resources that exist for
resource-rich source languages are used to build corresponding tools and resources in underresourced target languages by means of adaptation. For statistical dependency parsing such
a cross-lingual approach essentially means that we either take a parsing model and apply
it to another language or use treebanks to train parsers for the new language with target
language adaptation taking place in any of the workflow stages. We can, thus, divide the
main approaches in cross-lingual dependency parsing into two categories: model transfer
and data transfer.
Model transfer methods have the appealing property that they focus on language
universals and structures that can be identified in various languages without side-stepping
to the (semi-)automatic creation of annotated data in the target language. There is a strong
line of research looking at the identification of cross-lingual features that can be used to
port models and tools to new languages. One of their biggest drawbacks is the extreme
210

fiSynthetic Treebanking for Cross-Lingual Dependency Parsing

abstraction to generic features that cannot cover all language-specific properties of natural
languages. Therefore, these methods are often restricted to closely related languages and
their performance is usually far below fully supervised target-specific parsing models.
Data transfer methods, on the other hand, emphasize the creation of artificial training
data that can be used with standard machine learning techniques to build models in the
target language. Most of the work is focused on annotation projection and the use of
parallel data, that is, documents that are translated to other languages. Statistical alignment
techniques make it possible to map linguistic annotation from one language to another.
Another recent approach proposes the translation of treebanks (Tiedemann, Agic, & Nivre,
2014) which enables the projection of annotation without parsing unrelated parallel corpora.
Both methods create synthetic data sets without manual intervention and, therefore, we
group these techniques under the general term synthetic treebanking, which is the main focus
of our paper.
The structure of our paper is as follows. After a brief outlook on the contributions
of our work, we first provide an overview of cross-lingual dependency parsing approaches.
After that, we discuss in depth our experiments with synthetic treebanks, where we inspect
annotation projection with parallel data sets and with translated treebanks. We also include
a thorough study on the impact of part-of-speech (PoS) tagging in cross-lingual parsing.
Before concluding with final remarks and prospects for future work, we discuss the impact of
our contribution in comparison with selected recent approaches, both in terms of empirical
assessment and the underlying requirements imposed on truly under-resourced languages.
1.1 Our Contributions
The paper addresses annotation projection and treebank translation with a detailed and
systematic investigation of various techniques and strategies. We build on our previous
work on cross-lingual parsing (Tiedemann et al., 2014; Tiedemann, 2014, 2015) but extend
our study with detailed discussions of advantages and drawbacks of each method. We also
include a new idea of back-projection that integrates machine translation in the parsing
workflow. Our main contributions are the following:
1. We provide an overview of the various approaches to cross-lingual dependency parsing
with detailed discussions about the properties of the utilized techniques.
2. We present new competitive cross-lingual parsing results using synthetic treebanks.
We ground our results through a discussion on related work and implications for truly
under-resourced languages.
3. We provide a thorough study on the impact of PoS tagging in cross-lingual dependency
parsing.
Before delving into more details let us first review the selected current approaches to
cross-lingual dependency parsing to connect the work presented in this paper with related
research.
211

fiTiedemann & Agic

2. Current Approaches to Cross-Lingual Dependency Parsing
This section provides an overview of cross-lingual dependency parsing. We discuss the
previously outlined annotation projection and model transfer approaches in more depth
including recent developments in the field. Cross-lingual parsing combines many efforts in
dependency treebanking, and in creating standards for PoS and syntactic annotations. We
start off by outlining the current practices in empirical evaluation of cross-lingual parsers,
and the linguistic resources used for benchmarking.
2.1 Treebanks and Evaluation
In a supervised setting, cross-lingual dependency parsing amounts to training a parser on
a treebank, and applying it on the target text. However, the empirical quality assessment
for such a parser on the target data introduces certain additional constraints. To evaluate
supervised cross-lingual parsers, we require at least the following three components:
1. parser generators: trainable, language-independent dependency parsing systems,
2. dependency treebanks for the source languages, and
3. held-out evaluation sets for the target languages.
In the years following the venerable CoNLL 2006 and 2007 shared task campaigns in
dependency parsing (Buchholz & Marsi, 2006; Nivre, Hall, Kubler, McDonald, Nilsson,
Riedel, & Yuret, 2007), many mature parsers were made publicly available across the different
parsing paradigms. This resolves the first point from our list, as choosing to applyand
comparing betweendifferent approaches to parsing in a cross-lingual setup is nowadays
made trivial by abundant parser availability. We can now easily benchmark a respectable
number of parsers for accuracy, processing speed, and memory requirements.
Experimental setup for cross-lingual parsing thus amounts to choosing the training and
testing data, and to defining the evaluation metrics.
2.1.1 Intrinsic and Extrinsic Evaluation
We can perform intrinsic or extrinsic evaluation of dependency parsing. In intrinsic evaluation,
we typically apply evaluation metrics to gauge the various aspects of parsing accuracy on
held-out data, while in extrinsic evaluation, parsers are scored by the gains yielded in
subsequentor downstreamtasks which make use of dependency parses as additional
input.
Dependency parsers are intrinsically evaluated for labeled (LAS) and unlabeled (UAS)
attachment scores: the portions of correctly paired heads and dependents in dependency
trees, with or without keeping track of the edge labels, respectively. Sometimes we also
evaluate for labeled (LEM) and unlabeled (UEM) exact match scores, to determine how often
the parsers correctly parse entire sentences. For a more detailed exposition of dependency
parser evaluation, see the work of Nivre (2006) and Kubler et al. (2009), and also note that
Plank et al. (2015) provide detailed insight into the correlations between these and various
other dependency parsing metrics and human judgements on the quality of parses.
212

fiSynthetic Treebanking for Cross-Lingual Dependency Parsing

In a monolingual intrinsic evaluation scenario, we either have predefined held-out test
data at our disposal, or we cross-validate by slicing the treebank into training and test
sets. In both cases, the treebank and the test sets belong to the same resource, and are
created using the same annotation scheme, which in turn typically stems from the same
underlying syntactic theory. However, given the heterogenous development of syntactic
theories, and subsequently of treebanks for different languages (Abeille, 2003), this does not
necessarily hold in a cross-lingual setup. Moreover, excluding the very recent treebanking
developmentswhich we discuss a bit further in this sectionprior to 2013, the odds of
randomly sampling from a pool of all publicly available treebanks and drawing a source-target
pair annotated in the same (or even similar) scheme are virtually non-existent.
The syntactic annotation schemes generally differ in: (a) rules for attaching dependents
to heads, and (b) dependency relation labels, that is, the syntactic tagsets. Given two
treebanks with incompatible syntactic annotations, without performing any conversions, it
is more likely to expect similarities in head attachment rules, than in the syntactic tagsets.
This fact is present in all the initial cross-lingual parsing experiments (Zeman & Resnik,
2008; McDonald, Petrov, & Hall, 2011; Sgaard, 2011). Such initial efforts in charting
cross-lingual dependency parsing mainly used the CoNLL shared task datasets, and they
all evaluated for UAS. The rare exceptions are, for example, the generally under-resourced
Slavic languages (Agic, Merkler, & Berovic, 2012) subscribing to (slightly modified versions
of) the Prague Dependency Treebank scheme (Bohmova, Hajic, Hajicova, & Hladka, 2003).
Very recently, a substantial effort was undertaken in bridging the annotation scheme gap
in dependency treebanking to facilitate uniform syntactic processing of worlds languages.
The effort resulted in two editions of Google Universal Treebanks (UDT) (McDonald et al.,
2013), which were in turn recently superseded by the Universal Dependencies project (UD)
(Nivre et al., 2015). In these projects, the Stanford typed dependencies (SD) (De Marneffe,
MacCartney, & Manning, 2006) were used as the adaptable basis for designing the underlying
annotation scheme, and for applying it by using human expert annotators on several languages.
These datasets made possible the first reliable cross-lingual dependency parsing experiments,
namely the ones by McDonald et al. (2013), and also enabled the use of LAS as the default
evaluation metric, just like in monolingual parsing. For these reasons, UDT and UD are the
de facto standard datasets for benchmarking cross-lingual parsers today, while the CoNLL
datasets are still used mainly for backward compatibility with previous research. In another
effort, the HamleDT dataset (Zeman et al., 2014), 30 treebanks were automatically converted
to the Prague scheme, and then to SD, and are also frequently used in evaluation campaigns.
We do currently note a preference for UDT and UD, since they were produced through
manual annotation.
Given our short exposition of dependency treebanking in relation with cross-lingual
parsing, in this paper, we opt for using UDT in our experiments. As for the choice of sources
and targets, we do a Cartesian product of the dataset: we treat all the available languages
as both sources and targets. This is the more common approach in cross-lingual parsing,
even if there is research that uses English as a source-only language, and treats the other
languages as targets.
The extrinsic evaluation of cross-lingual parsing is much less developed, although the
arguments to its favor are very convincing. Namely, the underlying goal of cross-lingual
parsing is enabling the processing of actual under-resourced languages. For these languages,
213

fiTiedemann & Agic

even the parsing test sets may not be readily available. For conducting empirical evaluations
in such extreme cases, we might resort to downstream applications (Elming et al., 2013).
The choice of downstream tasks might pose a separate challenge in this case, and devising
feasible (and representative) tasks for extrinsic evaluation of cross-lingual dependency parsing
remains largely unaddressed. In this paper, we deal only with intrinsic evaluation.
2.1.2 Part-of-Speech Tagging
As noted in our brief introduction to model transfer, dependency parsers make heavy use of
PoS features. As with the syntactic annotations, sources and targets may or may not have
shared PoS annotation layers, and moreover, PoS taggers may or may not be available for
the target languages.
The issue of PoS compatibility is arguably less difficult to resolve than the structural
or labeling differences in dependency trees, as PoS tags are more or less straightforwardly
mapped to one another. At this point, we also note the recent approaches to learning
PoS tag conversions (Zhang, Reichart, Barzilay, & Globerson, 2012), which systematically
facilitate the conversions. Furthermore, efforts such as UDT/UD also build on a shared PoS
representation, the so-called Universal PoS (UPoS) (Petrov et al., 2012). UD extends the
UPoS specification by introducing additional PoS tags17 instead of the initial 12and
by providing the support for standardized morphological features such as noun gender and
case, or verb tense. That said, these added features are not yet readily available, and
the shared representation in UDT/UD amounts to a 12- or 17-tag-strong PoS tagset. As
for the treatment of source languages with respect to PoS tagging, most of the work in
cross-lingual parsing presumes the existence of taggers, or even tests on gold standard PoS
input. Recently, Petrov (2014) argued strongly for the use of predicted PoS in cross-lingual
parsing, which does make for a more realistic testing environment, especially with increased
availability of weakly supervised PoS taggers (Li et al., 2012; Garrette et al., 2013). In this
paper, we experiment both with gold standard and predicted PoS features in order to stress
the impact of tagging accuracy on parsing performance. We also discuss the implications of
these choices in enabling the processing of truly under-resourced languages.
2.2 Model Transfer
We now proceed to sketch the main approaches to cross-lingual dependency parsing: model
transfer, annotation projection, and treebank translation. We also reflect on the usage of
cross-lingual word representations in cross-lingual parsing, while we particularly emphasize
the annotation projection and treebank translation approaches.
Simplistic model transfer amounts to applying the source models to the targets with no
adaptation, which can still be rather successful for closely related languages (Agic et al.,
2014). However, the flavor of model transfer that has recently attracted a fair amount of
interest owes to the availability of cross-lingually harmonized annotation (Petrov et al., 2012)
that makes it possible to use shared PoS features across languages. The most straightforward
technique is to train delexicalized parsers that heavily rely on UPoS tags. Figure 1 illustrates
the basic idea behind these models. This simple technique has shown some success for
closely related languages (McDonald et al., 2013). Several improvements can be achieved by
using multiple source languages (McDonald et al., 2011; Naseem, Barzilay, & Globerson,
214

fiSynthetic Treebanking for Cross-Lingual Dependency Parsing

label 2
label 1

label 3

(1) delexicalize
pos1

src1
lexicalized

parser

pos2

pos3

src2 src3

pos4

src4

(4) re-train
pos2

trg1

pos1

trg2

(2) train

delexicalized

parser

(3) parse
pos3

trg3

label 1

pos4

trg4

label 3

label 2

Figure 1: An illustration of the delexicalized model transfer, with an implication of the
lexicalization option through self-training.

2012), and additional cross-lingual features that can be used to transfer models to a new
language, such as cross-lingual word clusters (Tackstrom, McDonald, & Uszkoreit, 2012)
or word-typology information (Tackstrom, McDonald, & Nivre, 2013b). There are ways to
re-lexicalize models as well. Figure 1 suggests a self-learning procedure that adds lexical
information from data sets that have automatically been annotated using delexicalized
models. Various data selection techniques can be used to focus on reliable cases to improve
the value of the induced lexical features.
The advantage of transferred models is that they do not require parallel data, at least
not in their most generic form. However, reasonable models require some kind of target
language adaptation and parallel or comparable data sets are usually necessary to perform
such adaptations. The largest drawback of model transfer is the strong abstraction from
language-specific features to the universal properties. For many fine-grained linguistic
differences, this kind of coarse-grained universal knowledge is often not informative enough
(Agic et al., 2014). Consequently, a large majority of recent approaches aim at bridging this
representational deficiency.
2.3 Cross-Lingual Word Representations
Model transfer requires abstract features to capture the universal properties of languages.
The use of cross-lingual word clusters was already mentioned in the previous section,
and the benefits of monolingual clustering for dependency parsing are well-known (Koo,
Carreras, & Collins, 2008). Recently, distributed word representations have entered NLP
in various models (Collobert et al., 2011). The so-called word embeddings capture the
distributional properties of words in continuous vector representations that can be used to
measure syntactic and semantic relations even across languages (Mikolov, Le, & Sutskever,
2013). Their monolingual variety has found many applications in NLP. Distributed word
representations for cross-lingual dependency parsing were first applied just recently by Xiao
and Guo (2014). They explore word embeddings as another useful abstraction that enables
more robust model transfer across languages. However, they apply their techniques to the
215

fiTiedemann & Agic

old CoNLL data sets and cannot provide labeled attachment scores and comparable results
to our settings.
Several recent publications show that bilingual word embeddings learned from aligned
bitexts improve semantic representations. Faruqui and Dyer (2014) use canonical correlation
analysis to find cross-lingual projections of monolingual vector space models. Zou, Socher,
Cer, and Manning (2013) learn bilingual word embeddings with fixed word alignments.
Klementiev, Titov, and Bhattarai (2012) treat cross-lingual representation learning as a
multitask learning problem in which cross-lingual interactions are based on word alignments
and word embeddings are shared across the various tasks. All of these techniques have
significant value in improved model transfer and may act as the necessary target language
adaptation to move beyond language universals as the only feature in transfer models.
In cross-lingual parsing, we can envision the word representations as a valuable addition
to model transfer in the direction of regularization. That said, their usage maintains the
previously listed advantages and drawbacks of model transfer, and adds another prerequisite:
the availability of parallel texts for inducing the embeddings. There have been some very
recent developments in creating cross-lingual embeddings without parallel text (Gouws &
Sgaard, 2015) but their applicability in dependency parsing is yet to be verified. Here,
we note a very recent contribution by Sgaard et al. (2015), who use inverted indexing on
cross-lingually overlapping Wikipedia articles to produce truly inter-lingual word embeddings.
As they show competitive scores in cross-lingual dependency parsing, we further address
their contribution in our related work discussion.
2.4 Annotation Projection
The use of parallel corpora and automatic word alignment for transferring linguistic annotation from a source language to a new target language has quite a long tradition in NLP.
The pioneering work of Yarowsky, Ngai, and Wicentowski (2001) was followed by a number
of researchers, and for various tasks, the transfer of dependency annotation among others
(Hwa et al., 2005). The basic idea is to use existing tools and models to annotate the source
side of a parallel corpus and then to use alignment to guide the mapping of that annotation
to the target side of the corpus. Assuming that the source language annotation is sufficiently
correct and that the aligned target language reflects the same syntactic patterns, we can
train parsers on the projected data to bootstrap tools for languages without explicit linguistic
resources such as syntactically annotated treebanks. Figure 2 illustrates the general idea of
annotation projection for the case of syntactic dependencies and parser model induction.
Note that PoS labels are typically projected as well along with the dependency relations.
The first attempts to directly map dependency information coming from diverse treebanks
resulted in rather poor performance. In their work, Hwa et al. (2005) had to rely on additional
post-processing rules to transform the results into reasonable structures. As we argued in
the previous subsection, one of the main problems in the early work was the incompatibility
of treebanks that have individually been developed for various languages following different
guidelines and using different label sets. The latter is also the reason why no labeled
attachment scores could be reported in that work, which makes it difficult to place these
cross-lingual approaches in relation to standard models trained for the target language.
216

fiSynthetic Treebanking for Cross-Lingual Dependency Parsing

label 2
label 1

pos1

src1

(1) parse

label 3

pos2

pos3

src2 src3

pos4

(2) project

trg1

trg2

trg3

trg4

pos2

pos1

pos3

pos4

label 1

parser

parser

src4

word-aligned bitext

lexicalized

lexicalized

label 3

(3) train
label 2

Figure 2: An illustration of the syntactic annotation projection system for cross-lingual
dependency parsing.

Less frequent, but also possible, is the scenario where the source side of the parallel
corpus contains manual annotation (Agic et al., 2012). This addresses the problem created
by projecting noisy annotations, but it presupposes parallel corpora with manual annotation,
which are rarely available. Additionally, the problem of incompatible annotation still remains.
The introduction of cross-lingually harmonized treebanks changed the situation significantly (McDonald et al., 2013). These data sets use identical labels and adhere similar
annotation guidelines that make it possible to directly compare structures when projected
from other languages. In the work of Tiedemann (2014), we explore projection strategies
and discuss the success of annotation projection in comparison to other cross-lingual approaches. Our work builds on the direct correspondence assumption (DCA) proposed by
Hwa et al. (2005). They define several projection heuristics that make it possible to project
any dependency structure through given word alignments to a target language sentence. The
basic procedures cover different types of word alignments. One-to-one alignments are the
most straightforward case in which dependency relations can simply be copied. Unaligned
source language tokens are covered by additional DUMMY nodes that capture all relations
that are connected to that token in the source language (see the left-most graph in Figure 3).
Many-to-one links are resolved by only keeping the link to the head of the aligned source
language tokens and deleting all other links (see the graph in the middle). One-to-many
alignments are handled by introducing additional DUMMY nodes that act as the immediate
parent in the target language, and which will capture the dependency relation of the source
side annotation (see the right-most graph in Figure 3). Many-to-many alignments are
treated in two steps. First we apply the rule for one-to-many alignments and after that the
many-to-one rule. Finally, unaligned target language tokens are simply dropped and will be
removed from the target sentence.
Some issues are not explicitly covered by the original publication of the algorithm. For
example, it is not entirely clear in what sequence these rules should be applied and how
labels should be projected. Some of the rules, for example, change the alignment structure
and may cause additional unaligned source tokens that need to be handled by other rules.
In our implementation, we first apply the one-to-many rule for all cases in the sentence
217

fiTiedemann & Agic

label 2
label 1

label 2

label 3

label 1

pos1

src1
pos2

trg1

pos2

pos3

src2 src3

pos1

trg2

pos3

pos4

src4

pos1

pos1

pos2

pos3

pos2

pos1

pos4

src1

src2 src3

src4
pos1

trg1

trg2

trg3

src2

dummy dummy

DUMMY trg1

trg2

dummy

label 3

pos3

src3
pos2

trg3

pos3

trg4

label 2

dummy

label 1
label 2

label 2

pos2

src1

pos4

pos4

trg3 DUMMY

label 1

label 1

label 3

label 1

label 2

Figure 3: Annotation projection heuristics for special alignment types: Unaligned source
words (left graph), many-to-one alignments (center), one-to-many alignments
(right graph).

before applying the many-to-one rule and, thereafter, resolving unaligned source tokens. The
final step includes the mapping of dependency relations through the remaining one-to-one
alignments. For one-to-many alignments, we transfer the PoS and dependency labels to the
newly created DUMMY node (following the rule for one-to-one alignments after resolving the
one-to-many link) and the previously aligned target language tokens will obtain DUMMY
PoS labels and their dependency relation to the governing DUMMY node will also be labeled
as DUMMY (see Figure 3).
Projecting syntactic dependency annotation creates several other problems as well. First
of all, crossing word alignments cause a large amount of non-projectivity in the projected
data. The percentage of non-projective structures goes up to over 50% for the UDT
data (Tiedemann et al., 2014). Furthermore, projection heuristics can lead to conflicting
annotation as it is shown in the authentic example illustrated in Figure 4. These issues put
an additional burden on the learning algorithms and many cross-lingual errors are caused by
such complex and ambiguous cases.
Nevertheless, Tiedemann (2014) demonstrates that annotation projection is competitive
to other cross-lingual methods and its merits are further explored by Tiedemann (2015).
cc
adpobj

Tous ses produits sont de qualit

et

dune fraicheur exemplaires .

... high- quality DUMMY and ...
adpobj
dummy
dummy

cc

added nonprojectivity
inconsistencies

Figure 4: Issues with annotation projection illustrated on a real-life example.
218

fiSynthetic Treebanking for Cross-Lingual Dependency Parsing

label 2

Treebank translation

label 1

pos1

src1

label 3

pos2

pos3

src2 src3

pos4

src4

(1) translate

(2) project
trg1

trg2

trg3

trg4

pos2

pos1

pos3

pos4

label 1

lexicalized

parser

label 3

(3) train
label 2

Figure 5: An illustration of the synthetic treebanking approach through translation.
2.5 Translating Treebanks
The notion of translation in cross-lingual parsing was first introduced by Zhao, Song, Kit,
and Zhou (2009), who use a bilingual lexicon for lookup-based target adaptation. A similar
method is also adopted by Durrett et al. (2012). This simplistic lookup approach is used by
Agic et al. (2012), who exploit the availability of a parallel corpus for two closely related
languages, one side of the corpus being a dependency treebank. The former evaluates for
UAS on 9 languages from the CoNLL datasets, while the latter research deals only with
Croatian and Slovene and is of a smaller scale.
Tiedemann et al. (2014) are the first to use full-scale statistical machine translation (SMT)
to synthesize treebanks as SMT-facilitated target language adaptations for cross-lingual
parsing. They use UDT for LAS evaluation, while also performing a subset of experiments
with the CoNLL 2007 data for backward compatibility. In this paper, we often refer to,
and we build on that work. Figure 5 illustrates the general idea of this technique, and we
proceed to discuss its implications.
As sketched in the introduction, at the core of the synthetic treebanking idea is the
concept of automatic source-to-target treebank translation. Its workflow consists of the
following steps:
1. Take a source-target parallel corpus and a large monolingual target language corpus
to train an (ideally top-performing) SMT system, orif availableapply an existing
source-target machine translation system.
2. Given a source language treebank, translate it into the target language. Word-align
the original sentence and its translation, or preserve the phrase alignments provided
by the SMT system.
3. Use the alignments to project the dependency annotations from the source treebank
to the target translation, in turn creating an artificial (or synthetic) treebank for the
target language.
4. Train a target language parser on the synthesized treebank, and apply (or evaluate) it
on target language data.
219

fiTiedemann & Agic

This sketch of treebank translation opens up a large parameter tuning search space, and
also outlines the various properties of the approach. We discuss them briefly, and defer the
reader to the detailed expositions of the many intricacies in these papers (Tiedemann et al.,
2014; Tiedemann, 2014, 2015).
2.5.1 Components
The prerequisites for building an SMT-supported cross-lingual parsing system are: (a) the
availability of parallel corpora, (b) a platform for building state-of-the-art SMT systems, (c)
algorithms for robust annotation projection, and (d) the previously listed resources needed
for cross-lingual parsing in general: treebanks and parsers.
Parallel corpora are now available for a very large number of language pairs, even outside
the benchmarking frameworks of CoNLL and UDT. The size and domains of the parallel
data influences the quality of SMT, and subsequently of the cross-lingual parsers. The
SMT community typically experiments with the Europarl dataset (Koehn, 2005), while
many other datasets are also freely available and cover many more languages, such as the
OPUS collection (Tiedemann, 2012). Ideally, the parallel corpora used in SMT are very
large, but for some source-target pairs, this may not necessarily be the case. Moreover, the
corpora might not be spread across the domains of interest, leading to decreased performance.
Domain dependence is thus inherent in the choice of parallel corpora for training SMT
systems. Here, we note a recent contribution by Agic et al. (2015), who learn a hundred PoS
taggers for truly under-resourced languages by using label propagation on a multi-parallel
Bible corpus, indicating the possibility of bootstrapping NLP tools in even the most hostile
environments, and the subsequent applicability of such tools across domains.
In this paper, we opt for using Moses (Koehn et al., 2007) as the de facto standard
platform for conducting SMT research. In summary, since our approach to SMT goes
beyond the dictionary lookup of Durrett et al. (2012), we mainly experiment with phrasebased models, gaining the target language adaptations in the form of both the lexical
features and the reordering. The projection algorithms for synthetic treebanking can in
whole be transferred from the annotation projection approaches. We do, however, consider
their various parametrizations, while Tiedemann et al. (2014) previously proposed a novel
algorithm, and Tiedemann (2014) thoroughly compared various approaches to annotation
projection.
2.5.2 Advantages and Drawbacks
Automatic translation has the advantage that we can use the manually verified annotation of
the source language treebank and the given word alignment, which is an integral part of the
translation model. Recent advances in statistical machine translation (SMT) combined with
the ever-growing availability of parallel corpora are now making this a realistic alternative.
The relation to annotation projection is obvious as both involve parallel data with one side
being annotated. However, the use of direct translation brings two important advantages.
First of all, using SMT, we do not accumulate errors from two sources: the tooltagger
or parserused to annotate the source language of a bilingual corpus, and the noise coming
from alignment and projection. Instead, we use the gold standard annotation of the source
language which can safely be assumed to be of much higher quality than any automatic
220

fiSynthetic Treebanking for Cross-Lingual Dependency Parsing

Input: source tree S, target sentence T ,
word alignment A, phrase segmentation P
Output: syntactic heads head[],
word attributes attr[]
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27

Input: node s, source tree S with root ROOT,
target sentence T , word alignment A
Output: node t*

1 if s == ROOT then
treeSize = max distance to root(S) ;
2
return ROOT ;
attr = [] ;
3 end
head = [] ;
use phrase4 while is unaligned src(s,A) do
for t 2 T do
5
s = head of(s,S) ;
segmentation
if is unaligned trg(t,A) then
6
if s == ROOT then
for t 2 in trg phrase(t,P) do
7
return ROOT
;
function:
find_aligned:
[sx ,..,sy ] = aligned to(t) ;
8
end
s = find
highest([s
,..,sy ],S)
;
9 end
Input:
source
tree S,xtarget
sentence
T,
Input: node s, source tree S with root ROOT,
word
alignment
A, phrase segmentation
P
10 p = 0 ; target sentence T , word alignment A
t = find
aligned(s,S,T,A)
;
Output:
heads
Output:
node t*
11 t* = undef
;
attr[t] =syntactic
DUMMY
; head[],
walk up the tree
word
attributes
12 for t 2
dothen
head[t]
= t ; attr[]
1 aligned(s,A)
if s == ROOT
if unaligned
13
if 2position(t,T)
> p then
1 treeSize = max distance to root(S) ;
return ROOT
;
end
14
t* = t ;
3 end
else 2 attr = [] ;
attach to 15
p = position(t,T)
;
4 while
is unaligned; src(s,A) do
[s3 xhead
,..,sy=] =[] aligned
to(t) ;
highest node
4
for
t
2
T
do
5
s = head of(s,S) ;
16
end
s = find highest([sx ,..,sy ],S) ;
5
is unaligned
trg(t,A) then
6
if s == ROOT then
17 end
attr[t]
=ifattr(s)
;
for t; 2 in trg phrase(t,P) do
7 ;
return ROOT ;
18 return t*
s6= head of(s,S)
7
[sx ,..,sy ] = aligned to(t) ;
8
end
t 8= find aligned(s,S,T,A)
;
s = find highest([s
x ,..,sy ],S) ;
9 end
if t == t then
Figure 3: Procedure find aligned().
10 p = 0 ;
9
t = find aligned(s,S,T,A) ;
heuristics for
[s
,..,s
]
=
in
src
phrase(s,P)
;
x
y
11 t* = undef ;
10
attr[t] = DUMMY ;
s* = find highest([sx ,..,sy ],S) ;
multiple targets:
12 for t 2 aligned(s,A) do
11
head[t] = t ;
s = head of(s*,S) ;
13
if position(t,T) > p then
take
right-most
12
end
features
and
options
are
optimized
using
MaltOpt =else
find aligned(s,S,T,A) ;
14
t* = t ;
13
p = position(t,T)
head[t] [s
=xt,..,s
; y ] = aligned to(t) ;
timizer.15The accuracy
is given ;in Table 3 as a set
14
16
end
end
15
s = find highest([sx ,..,sy ],S) ;
of labeled
attachment
scores (LAS). We include
17 end
attr[t] = attr(s) ;
end 16
punctuation
in
our
evaluation.
Ignoring punctua18 return t* ;
17
s = head of(s,S) ;
end

tion generally leads to slightly higher scores as we
Figure
3: Procedure
finddoaligned().
have noted in our
experiments
but we
not report
DUMMY
nodes
those numbers
here.proposed
Note also by
that Tiedemann
the columns et al.
represent the target languages (used for testing),
features and options are optimized using MaltOpwhile the
rows denote the source languages (used
timizer. The accuracy is given in Table 3 as a set
in training),
as in McDonald
al. (2013).
of labeled
attachment et
scores
(LAS). We include
Frompunctuation
the table, in
weour
can
see
that the
baseline
evaluation.
Ignoring
punctuascores are
compatible
with
the
ones
in
the
tion generally leads to slightly higheroriginal
scores as we
chine translation
and annotation
projection.
Here,
presented
(McDonald
etof
al.,cross-domain
2013),
annotation
obtained
by using
a tool trained
on that
data,
especially
in light
have
noted
in ourbyexperiments
but
we
do
not report
Figure 2: Annotation
projection
algorithm.experiments
we
also
look
at
delexicalized
models
trained
on
included
in
Table
3
for
reference.
The
differences
those
numbers
here.
Note
also
that
the
columns
accuracy drops. Moreover, using SMT may help in bypassing domain shift problems,
which
translated treebanks to show the effect of machine
are
due
to
parser
selection,
as
they
use
a
transitionrepresent
the
target
languages
(used
for
testing),
are common when applying tools trained (and evaluated) on one resource to text
from
translationdelexicalized
without additional
features.
transferlexical
parsing
following the based
ap- parser
while the
rows
denote
the source
languages (used
with
beam
search
and perceptron
another domain.
proach of McDonald et al. (2013). Second,learning
we
inalong
training),
as inof
McDonald
et Nivre
al. (2013).
the lines
Zhang and
(2011)
5.1 Baseline
Results
present
the results obtained with parsers trained
From
the
table,
we
can
see
that
the
baseline
whereas
we
rely
on
greedy
transition-based
parsSecondly, we can assume that SMT will produce output that is much closer
to the
on target language treebanks produced using ing
ma-withscores
are
compatible
with
the onesIninthe
thefoloriginal
linear
support
vector
machines.
First
we
present
the
baseline
parsing
scores.
The
input than
manual
translations
in
parallel
texts
usually
are.
Even
if
this
may
seem
like a
chine translation and annotation projection. Here,
presented
by (McDonald
et al.,as
2013),
lowing,experiments
we will compare
results
to our baseline
baselines we explore
are: (i)
monolingual
baseshortcoming
in
general,
in
the
case
of
annotation
projection
it
should
rather
be
an
advantage,
we also look at delexicalized models trainedwe
onhaveincluded
in Tablesetup
3 forinreference.
The differences
a comparable
those experiments.
line, i.e., training and testing using the same lantranslated
show the effect of machine
because it
makestreebanks
it moreto straightforward
and
less
error-prone
to
transfer
annotation
are
due
to
parser
selection,
as
they
use
a transitionHowever, most improvements shown below
also from
guage datatranslation
from the without
Universal
Dependency Treeadditional lexical
features.
based
parser
with
beam
search
and
perceptron
source
to
target.
Furthermore,
the
alignment
between
words
and
phrases
is
inherently
apply in comparison with (McDonald et al., 2013).
bank and (ii) delexicalized baseline, i.e., applying
learning
along
lines of Zhang
and Nivre (2011)
provided
as
an
output
of
all
common
SMT
models.
Hence,
no the
additional
procedures
have to
delexicalized
parsers
across
languages.
5.1 Baseline Results
whereas weTreebanks
rely on greedy transition-based pars5.2
Translated
beFor
performed
on top
of theMaltParser
translated
corpus. Recent
research
(Zhao
et
al.,
2009;
Durrett
the monolingual
baseline,
moding with linear support vector machines. In the folFirst we present the baseline parsing scores. The
et
2012)
has
attempted
to
address
synthetic
datatowe
creation
for results
syntactic
els al.,
are trained
on
the
original
treebanks
with
uniNow
we
turn
thewill
experiments
on translated
tree- asvia
lowing,
compare
to our parsing
baseline
baselines we explore are: (i) monolingual baseversal
POS
labels
and
lexical
features
but
leavbanks.
We
consider
two
setups.
First,
we
look
atmodels
bilingual line,
lexica.
Tiedemann
et al.using
(2014)
extend
proposing
three
different
weidea
have by
a comparable
setup
in those
experiments.
i.e., training
and testing
the same
lan- this
ing
out
other
language-specific
features
if
they
exthe
effect
of
translation
when
training
delexicalHowever,
mostand
improvements
shown below
also
for automatic
translation
based Dependency
on induced
bilingual
lexica
phrase-based
translation
guage data
from the Universal
Treeist in the bank
original
Theauthors
delexicalized
ized
Incomparison
this way, algorithm
we
can
perform
aetdirect
apply
in
with
(McDonald
al., 2013).the
and treebanks.
(ii)work,
delexicalized
baseline,
i.e.,
applying
models.
In that
the
propose
a parsers.
new
projection
that
avoids
parsers are trained on universal
POSlanguages.
labels only
comparison to the baseline performance presented
parsers
across
creation delexicalized
of DUMMY
nodes
in the target language
we have
discussed in section 2.4.
5.2 that
Translated
above. The
second
setupTreebanks
then considers fully lexfor each language
and
are
then
applied
to
all
other
For the monolingual baseline, MaltParser modThe
procedure
is
summarized
in
the
pseudo-code
shown
in
Figure
6.
languages els
without
modification.
For all
models,with uniicalizedNow
models
trained
translatedontreebanks.
are trained
on the original
treebanks
we turn
to theon
experiments
translated treet = find aligned(s,S,T,A) ;
if t == t then
Figure202: Annotation
[sxprojection
,..,sy ] = in srcalgorithm.
phrase(s,P) ;
Figure 6:21 Annotation
projection
without
s* = find
highest([sx ,..,s
y ],S) ;
22 (2014).
s = head of(s*,S) ;
23
t = find aligned(s,S,T,A) ;
delexicalized
transfer parsing
following the ap24
head[t] = t ;
proach of 25McDonald
endet al. (2013). Second, we
end obtained with parsers trained
present the26 results
27 end
on target language
treebanks produced using ma18
19

versal POS labels and lexical features but leaving out other language-specific features if they221
exist in the original treebanks. The delexicalized
parsers are trained on universal POS labels only
for each language and are then applied to all other
languages without modification. For all models,

banks. We consider two setups. First, we look at
the effect of translation when training delexicalized parsers. In this way, we can perform a direct
comparison to the baseline performance presented
above. The second setup then considers fully lexicalized models trained on translated treebanks.

fiTiedemann & Agic

root
p
adpmod

nsubj

adpmod adpobj

adpobj
det

amod

PRON VERB ADP NOUN ADJ ADP DET NOUN .
Ils tiraient
a
balles reelles sur
la
foule .

They
re
firing live rounds on the crowd .
PRON PRON VERB ADP NOUN ADP DET NOUN .
adpmod adpobj

nsubj
nsubj

det
adpobj

adpmod
p

root

Figure 7: An example sentence translated from French to English with projections using
the algorithm shown in Figure 6. The boxes indicate the segmentation used by
the phrase-based translation model.

The key feature of this algorithm is that it makes use of the segmentation of sentences
into phrases together with their counterparts in the other language that are applied by
the underlying translation model. We can use this information to handle unaligned tokens
without creating additional DUMMY nodes as described in Figure 6. However, contrary
to our expectations, this algorithm does not work very well in practice and Tiedemann
et al. (2014) show empirically that a simple word-to-word translation model outperforms the
phrase-based systems with this projection algorithm in most cases. Part of the problem is the
ambiguous projection of PoS labels when handling one-to-many and many-to-one alignments.
An example is shown in Figure 7. Both They and re are assigned to be pronouns due to the
links to the French Ils which certainly confuses the model trained on such projected data.
The treebank translation approach using phrase-based SMT is further explored by
Tiedemann (2014). Tiedemann (2015) introduces the use of syntax-based SMT for crosslingual dependency parsing. In that work, the authors propose several improvements of
the DCA-based projection heuristics originally developed by Hwa et al. (2005). Simple
techniques that reduce the number of DUMMY elements in the projected data help to
significantly improve the results in cross-lingual parsing. We also realized that the placement
of DUMMY nodes is crucial. Strategies that choose positions where that minimize the risk
of additional non-projectivity are useful to improve parser model induction. We will mainly
use the techniques developed in that work in the experiments described in section 3.
The drawbacks of the synthetic treebanking approach are related to its hybrid nature:
a) it inherits the syntax projection risks from the annotation projection approach as its
success is bound by the projection quality, and b) it critically depends on the quality of
SMT, which in turn depends on the size and quality of the underlying parallel corpora.
222

fiSynthetic Treebanking for Cross-Lingual Dependency Parsing

As for the latter point, the experiments by Tiedemann et al. (2014) reveal a compelling
robustness of cross-lingual parsing to SMT noise in this framework, and in that paper we
also argue that projection into synthetic texts is simpler than projection between actual
parallel text. Another important drawback is the need for large parallel data sets to train
reasonable translation models for the languages under consideration. Alternatively, any
handcrafted rule-based system could be applied as well. However, such systems and data
sets are rarely available for low-resource languages. On the other hand, there are techniques
that can improve machine translation via bridge languages. Tiedemann and Nakov (2013)
demonstrate how small amounts of parallel data can successfully been used for building
translation models for truly under-resourced languages. Their approach of creating synthetic
training data for statistical machine translation with low resource languages fits very well in
the spirit of synthetic treebanking.
2.6 What About Truly Under-Resourced Languages?
Up to this point, we have outlined the underlying concepts for the major approaches to
cross-lingual dependency parsing today. We have also discussed some intricacies of enabling
cross-lingual parser evaluation. Here, we proceed to discuss how these two outlooksnamely,
the way we implement cross-lingual parsers, and the way we evaluate them for parsing
accuracyreflect on dependency parsing of truly under-resourced languages.
What makes a language under-resourced? Following Uszkoreit and Rehm (2012), we
acknowledge the many facets involved in attempting to address this question. Generally,
however, an under-resourced language is distinguished by lacking the basic NLP-enabling
linguistic resources, such as PoS-tagged corpora or treebanks. In this paper, we take dependency parsing-oriented viewpoint, which allows for casting the issue of under-resourcedness in
the specific terms of dependency parsing enablement for a given language. Thus, a language
is under-resourced if we cannot build a dependency parser for it or, otherwise said, if no
dependency treebank exists for that language. Since statistical dependency parsing critically
depends on the availability of PoS tagging, we make this an additional requirement, which
in turn implies the following three levels of resource availability. Note that this list is a
parsing-oriented specialization of the general discussion on low-resource languages from the
introduction.
1. There is a PoS-tagged corpus and a treebank available for a given language, and by
virtue of those, we have at hands a PoS tagger and a dependency parser for that
language. We call such languages well-resourced or resource-rich languages from a
dependency parsing viewpoint, as we can use the dedicated native language resources
to parse texts written in that language.
2. For a given language, there are no PoS tagging or parsing resources available. This
includes both the annotated corpora and the NLP tools. We address such languages
as under-resourced or low-resource languages, as we cannot natively parse them for
syntactic dependencies, neither can we annotate them for PoS tags.
3. We have a PoS-tagged corpus or PoS tagger available for a given language, but no
treebanks or parsers exist for it. Even if there is some NLP support for such languages
223

fiTiedemann & Agic

through PoS annotation, we still approach them as under-resourced from the viewpoint
of dependency parsing.
If we want to parse the languages from group 2 for syntactic dependencies, we must address
both issuesthe unavailability of supporting resources for PoS tagging and dependency
parsingand often even more basic processing facilities such as sentence splitters or tokenizers.
In NLP, we often call such languages truly under-resourced. Group 3 is somewhat easier, as
we presumably only address the dependency-syntactic processing layer.
In the recent years, the field has dealt extensivelyand by and large, separatelywith
providing low-resource languages with PoS taggers and dependency parsers. Taking two
examples into account, Das and Petrov (2011) show how to bootstrap accurate taggers
using parallel corpora, while Agic et al. (2015) take under-resourcedness to the extreme by
presuming severe data sparsity and still manage to yield very reasonable PoS taggers for a
large number of low-resource languages. We are thus safe to conclude that even for the most
severely under-resourced languages, reasonable PoS taggers can be made available using one
of these techniques, if not already available off-the-shelf.
This reasoning underlies all current approaches to cross-lingual dependency parsing, in
that we presume the availability of PoS annotations, natively or through publicly available
related research. Since we are also required to at least intrinsically evaluate the resulting
parsers, we conduct our empirical assessments in an exclusive group of languages with at least
some syntactically annotated test data available. In effect, we are evaluating by proxy, as the
truly under-resourced languages do not enjoy even the basic test set availability. On top of all
that, the various top-performing approaches to cross-lingual parsingsuch as the previously
discussed annotation projection, treebank translation, or word representation-supported
model transferintroduce additional constraints or requirements. Most often, we presume
the availability of large source-target parallel corpora. One might argue accordingly that we
make a poor case for low-resource languages by amassing the prerequisites for our methods
to work, thus departing from the very definition of a low-resource language. In turn, and in
favor of the current approaches, we argue the following.
 The current research in enabling PoS tagging for under-resourced languages justifies
the separate handling of cross-lingual dependency parsing by presuming the availability
of PoS tagging. We refer the reader to the work by Tackstrom et al. (2013a) for a
detailed exposition and state-of-the-art results, together with the previously mentioned
work on bootstrapping taggers.
 McDonald et al. (2013) validate the evaluation by proxy by showing how a uniform
syntactic representation partially enables inferential reasoning about the performance of
ported parsers on truly under-resourced languages. Namely, they show that typological
similarity plays an important role in predicting the quality of transferred parsers.
This is built on by, for example, Rosa and Zabokrtsky (2015), who use a data-driven
language similarity metric to actually predict the best sources for the given targets in
cross-lingual parsing.
 The remaining prerequisites for top-level cross-lingual parsing, such as the treebank
translation approach we argue for in this paper, amount to source-target parallel
224

fiSynthetic Treebanking for Cross-Lingual Dependency Parsing

corpora and possibly also monolingual target corpora. While this may at first seem
as a substantial added requirement, we note that text corpora are more readily
available than expert-annotated linguistic resources, and the collections such as OPUS
(Tiedemann, 2012) provide large quantities of cross-domain data for many languages.
To further the claim, Agic et al. (2015) illustrate how annotation projection could be
applied to learn PoS taggers for hundreds, possibly even thousands of languages using
nothing but translations of (parts of) the Bible in a very simple setup.
Before concluding, we duly note the perceived disconnect between evaluating cross-lingual
parsers and actually enabling dependency parsing for languages that lack the respective
resources. We argue here that the former constitutes empirical research, while the latter
is primarily an engineering feat, and we are thus obliged to follow the field in adhering to
the former in this contribution. However, we do note that devising multiple systematic
downstream evaluation scenarios for truly under-resourced languages is sorely needed at this
point in the fields development, and would resolve an important disconnect in cross-lingual
NLP research.
We now proceed to discuss the core of our paper: the empirical validation of the synthetic
treebanking approach to cross-lingual parsing. We reflect once more on the prerequisites and
truly under-resourced languages in the related work discussion that follows our exposition of
synthetic treebanking.

3. Synthetic Treebanking Experiments
In this section, we will discuss a series of experiments that systematically explore various
cross-lingual parsing models based on annotation projection and treebank translation. Here,
we only assess the properties of the specific approach, and we compare them intrinsically or
to the baseline. We provide a comparison to selected more recent work in section 4.
In our setup, we always use the test sets provided by the Universal Dependency Treebank
version 1 (UDT) (McDonald et al., 2013) with their cross-lingually harmonized annotation
that makes it possible to perform fair evaluations across languages including labeled attachment scores (LAS), which we will use as our primary evaluation metric. Similar to previous
literature, we include punctuation in the calculation of LAS to ensure comparability to
related literature (Tiedemann, 2014). In all our experiments, we apply mate-tools (Bohnet,
2010) to train graph-based dependency parsers, which gives us very competitive performance
in all settings. We leave out Korean in our experiments due to the fact that we do not have
bitexts from the same domain as for the other languages, which we need for annotation
projection and SMT training. Thus, we experiment using five languages: English (en),
French (fr), German (de), Spanish (es), and Swedish (sv).
3.1 Baseline
Our initial baseline is a delexicalized model which is straightforward to train on the provided
training data of the UDT. Table 1 lists the attachment scores achieved by applying these
models across languages. Our scores confirm the results of McDonald et al. (2013); minor
differences are due to the different choices of the training algorithms. Note that we always
use columns to represent the target languages that we test and rows refer to source languages
225

fiTiedemann & Agic

used in training, projection or translation. We also always report the scores for all sourcetarget pairs, as reporting on averages or highest per-target scores might arguably make for a
biased insight into the methods.
target language 
LAS
de
en
es
fr
sv

de
70.84
48.60
47.16
46.77
52.53

en
45.28
82.44
47.31
47.94
48.24

es
48.90
56.25
71.45
62.66
52.95

fr
49.09
58.47
62.39
73.71
55.02

sv
52.24
59.42
54.63
54.89
74.55

mate-tools (coarse)
mate-tools (full)

78.38
80.34

91.46
92.11

82.30
83.65

82.30
82.17

84.52
85.97

Table 1: Results for the delexicalized models. For comparison there are also LASs of
lexicalized models at the bottom of the table. coarse uses coarse-grained PoS labels
only and full adds even fine-grained PoS information.

As we can see, the results are around 10 LAS points below the fully lexicalized models
and significant drops can be observed when training on other languages even though they are
all quite closely related. This is all but unexpected considering the naive approach of using
coarse-grained PoS label sequences without modification as the only type of information in
training these models. We do note, however, that the decrease in accuracy is not so drastic
for the typologically closest language pair (French-Spanish). In the following section, we
discuss various ways of adapting cross-lingual models to the target language, and we will
start with annotation projection in aligned parallel corpora.
3.2 Improved Annotation Projection
Annotation projection is used in connection with word-aligned bilingual parallel corpora
(bitexts). In our experiments, we use Europarl (Koehn, 2005) for each language pair
following the basic setup of Tiedemann (2014). The baseline model applies the DCA
projection heuristics as presented by Hwa et al. (2005) and the first 40,000 sentences of
each bitext in the corpus (repetitions of sentences included). Word alignments are produced
using IBM model 4 as implemented in GIZA++ (Och & Ney, 2003) trained in the typical
pipeline as it is common in statistical machine translation using the Moses toolbox (Koehn
et al., 2007). We use the entire Europarl corpus version 7 to train the alignment models to
obtain proper statistics and reliable parameter estimates. The asymmetric alignments are
symmetrized with the intersection and the grow-diag-final-and heuristics. The results of our
baseline projection model is given in Table 2.
The value of word-aligned bitext can clearly be seen in the performance of the crosslingual parser models. They outperform the naive delexicalized models by a large margin.
However, they are still pretty far away from the supervised monolingual models even for
these related language pairs. Tiedemann (2015) discusses various improvements of the
projection algorithm with significant effects on the performance of the trained models. One
226

fiSynthetic Treebanking for Cross-Lingual Dependency Parsing

de
en
es
fr
sv

de

62.28
60.46
61.27
62.96

en
53.27

49.34
53.46
51.07

es
57.69
62.29

66.51
61.82

fr
60.49
65.54
68.10

64.99

sv
65.25
66.97
64.67
62.75


Table 2: Baseline performance in LAS of a DCA-based annotation projection with 40,000
parallel sentences tested on target language test sets.

problem of the DCA algorithm is the creation of DUMMY nodes and labels that disturb
the training procedures. Many of these nodes can easily be removed without loosing much
information. Figure 8 illustrates our approach that deletes DUMMY leaf nodes and collapses
dependency relations that run via internal DUMMY nodes with single out-going edges.
Adding this modification to the DCA projection heuristics we can achieve significant
improvements for various language pairs. Table 3 summarizes the LASs for all models with
the new treatment of DUMMY nodes.
Tiedemann (2015) also introduces a new procedure for treating one-to-many word
alignments. In the original algorithm, they cause additional DUMMY nodes that act as
parents for the other aligned target language tokens. The new approach takes advantage
of different alignment symmetrization algorithms and uses the high-precision links coming
from the intersection of asymmetric word alignments to find the head of a multi-word unit,
whereas links from the high-recall symmetrization are used to attach the words to that head
word. Figure 9 illustrates this procedure by means of a sentence pair from Europarl.
Finally, Tiedemann (2015) also proposes to discard all trees that have remaining DUMMY
nodes. This may remove up to 90% of the training examples but assuming the availability
of large bitexts makes it possible to project additional sentences to fill the training data.
Discarding projected trees with DUMMY nodes effectively removes sentence pairs with
non-literal translations and complex alignment structures that are in any case less suited for
label 2

label 2
label 1

label 1

label 3

src1

src2 src3

src4

pos1

pos2

pos4

pos3

label 2

label 3

label 1

src1

src2 src3

src4

pos1

pos2

pos3

pos4

dummy

pos2

pos4

pos3

DUMMY trg1

trg2

trg3

DUMMY


trg1

trg2

trg3

label 3

src1

src2 src3

src4

pos1

pos2

pos3

pos4

pos1

pos2

pos4

trg1

trg2

trg3


pos1

dummy

label 2

dummy

label 3

label 1

label 2

label 1

Figure 8: Removing DUMMY nodes from projected parse trees: (i) Delete DUMMY leaf
nodes. (ii) Collapse unary productions over DUMMY nodes.

227

fiTiedemann & Agic

de
de
en
es
fr
sv

**

62.97+0.69
59.880.58
61.59+0.32
62.160.80

en
53.54+0.27
48.850.49
53.120.34
51.31+0.24

es
60.17+2.48
**
63.80+1.51

fr
62.35+1.86
**
66.47+0.93
68.55+0.45

**

*

**

67.00+0.49
62.58+0.76

sv
66.99+1.74
67.19+0.22
**
65.33+0.66
**
64.52+1.77
**

65.38+0.39

Table 3: Results for collapsing dependency relations over unary dummy nodes and removing
dummy leaves (difference to the annotation projection baseline in superscript).
Improvements marked with ** are statistically significant according to McNemars
test with p < 0.01 and improvements marked with * are statistically significant
with p < 0.05.

root

p

dobj
det
amod

nsubj

adpmod

adpobj

PRON VERB DET ADJ
NOUN
Wir wollen eine echte Wettbewerbskultur

We
want
a true
PRON VERB DET ADJ
nsubj

amod
det
dobj

culture
NOUN

ADP
in

NOUN
Europa

.
.

of
competition in Europe .
DUMMY DUMMY ADP NOUN .
adpobj
DUMMY
DUMMY
adpmod

root
p

Figure 9: Projecting from German to English using an alternative treatment for one-tomany word alignments. Dotted lines are links from the grow-diag-final-and
symmetrization heuristics and solid lines refer to links in the intersection of word
alignments.

annotation projection. Table 4 summarizes the results of this method tested in our setup.
We can observe significant improvements for all language pairs compared to the baseline
approach and all but two cases are also better than the results of the previous setting shown
in Table 3.
228

fiSynthetic Treebanking for Cross-Lingual Dependency Parsing

de
de
en
es
fr
sv

en
53.80+0.53

****

50.10+0.76
53.88+0.42
****
52.36+1.29

****

**
***

63.52+1.24
60.65+0.19
****
62.49+1.22
****
63.83+0.87

es
61.34+3.65
***
63.18+0.89

****
***

***

68.15+1.64
63.29+1.47

fr
62.32+1.83
**
67.04+1.50
*
68.81+0.71
**

**

sv
68.20+2.95
****
67.74+0.77
***
65.79+1.12
**
64.83+2.08

****

66.12+1.13

Table 4: Discarding trees that include DUMMY nodes; results with 40,000 accepted trees.
Results marked with ** and * are significantly better than the projection baseline
(with p < 0.01 and p < 0.05, respectively) and results marked **** and *** are
also significantly better than the ones in Table 3 (with p < 0.01 and p < 0.05,
respectively).

3.3 Phrase-Based Treebank Translation
Treebank translation is an interesting alternative to annotation projection. The main
advantage is that we can skip noisy source-side annotation of an out-of-domain bitext to be
able to project information from source to target language. Furthermore, word alignment
is tightly coupled with most statistical translation models which makes it straightforward
to use these links for projection. Finally, it is an advantage for projection that machine
translation prefers literal translations in similar syntactic structures. Unrestricted human
translations are much more varied and a proper alignment between translation equivalents
is not necessarily straightforward. In machine translation, the mapping between tokens
and token n-grams is essential which favors successful annotation projection. The largest
drawback is, of course, translation quality. Machine translation is a difficult task on its own
and its use in annotation projection requires at least some level of quality even though we
are not necessarily interested in semantically adequate translations.
Our first approach applies the model proposed by Tiedemann et al. (2014), using a
standard phrase-based SMT model to translate source language treebanks to a target
language. The projection is based on the DCA heuristics similar to the ones applied to
annotation projection described in the previous section. We also apply the modification of
DUMMY node handling as introduced before. However, we cannot apply the alternative
treatment of one-to-many alignments as we do not have different types of word alignment in
our translation model. We also do not filter out trees with remaining DUMMY nodes as
this would cause a serious reduction of the already small-sized treebanks. In contrast to
projection with bitexts we cannot add more data to fill up the training data.
In all the experiments, our MT setup is very generic and uses the Moses toolbox for
training, tuning and decoding (Koehn et al., 2007). The translation models are trained
on the entire Europarl corpus version 7 without language-pair-specific optimization. Word
alignments are essentially the same that we have used for our experiments with annotation
projection in section 3.2. For tuning we use MERT (Och, 2003) and the newstest2011 data
provided by the annual workshop on statistical machine translation (WMT).1 For Swedish
1. http://www.statmt.org/wmt14.

229

fiTiedemann & Agic

we use a sample from the OpenSubtitles2012 corpus (Tiedemann, 2012). The language
model is a standard 5-gram model and is based on a combination of Europarl and News data
provided from the same source. We apply modified Kneser-Ney smoothing without pruning,
applying KenLM tools (Heafield, Pouzyrevsky, Clark, & Koehn, 2013) for estimating the
LM parameters.
de
de
en
es
fr
sv

en
56.24+2.70

**
50.65+1.80
**
55.69+2.57
**
53.01+1.70

**
**

59.413.56
53.945.94
**
57.054.54
**
58.573.59

**

**

**

es
57.652.52
63.760.04

68.66+1.66
62.69+0.11

fr
59.063.29
**
67.99+1.52
**
69.70+1.15
**

sv
64.622.37
67.52+0.33
**
62.732.60
**
62.771.75
**

64.760.62

Table 5: Results for phrase-based treebank translation (difference to the corresponding annotation projection model with DUMMY node removal from Table 3 in superscript).
Results marked with ** are significantly different from the projection results (with
p < 0.01).

The results of our experiments with phrase-based SMT is summarized in Table 5. To a
large extent, we can confirm the findings of Tiedemann (2014) that the translation approach
has some advantages over the projection of automatically annotated parallel corpora. For
some language pairs, the labeled attachment scores are significantly above the projection
results even though the parsers are trained on much smaller data sets (the treebanks are
typically much smaller than 40,000 sentences for most language pairs). Very striking is also
the outcome for German as a target language, which seems to be the hardest language to
translate to in this data set. This is not very surprising as German is in general considered
to be a difficult target language in the setup of languages that are, for example, supported by
WMT. This also applies to the use of German as a source language with a surprising exception
when translating to English. Overall, the good results for English may be influenced by the
strong impact of the language model that can draw from the large monolingual resources.
3.4 Syntax-Based Treebank Translation
Tiedemann (2015) introduces the use of syntax-based SMT as another alternative to treebank
translation. The standard syntax-based MT models supported by Moses are based on
synchronous phrase-structure grammars which are induced from word-aligned parallel data.
Several modes are available. In our case, we are mostly interested in the tree-to-string
models that use synchronous tree substitution grammars (STSGs). Our assumption is
that the structural relations that are induced from the parallel corpus with a fixed given
source-side analysis improve the projection of syntactic relations when used in combination
with syntax-based translation.
In order to make it possible to use dependency information in the framework of synchronous STSGs we convert projective dependency trees to the bracketing structure that
can be used to train tree-to-string models with Moses. We use the yield of each word to
230

fiSynthetic Treebanking for Cross-Lingual Dependency Parsing

PRON VERB PRON . PRON ADP DET
NOUN
PRT VERB
Ich
bitte
Sie , sich
zu einer Schweigeminute zu erheben
nsubj

dobj

aux

det
adpobj

p

adpmod
dobj

root

xcomp

ROOT
nsubj

VERB

dobj

p

PRON

bitte

PRON

.

Sie

,

Ich

xcomp

adpobj

PRON ADP
sich

aux

adpmod

dobj

zu

dt

NOUN

VERB

PRT erheben
zu

DET Schweigeminute
einer

Figure 10: A dependency tree taken from the automatically annotated parallel data and its
lossy conversion to a constituency representation.

define a span over the sentence which forms a constituent with the label taken from the
relation of that word to its head.
Dependency trees are certainly not optimal for this kind of constituency-based SMT
model as they are usually very flat and do not provide the deep hierarchical structures
that are common in phrase-structure trees. However, our previous research has shown
that valuable syntactic information can be pushed into the model in this way that can be
beneficial for projecting dependency relations. Note that we use PoS tags as additional
pre-terminal nodes to enrich the information given to the system.
For training the models we used the same data sets and word alignments as we have
used for phrase-based SMT. However, we require a number of additional steps listed below:

 We tag the source side of a parallel corpus with a PoS tagger trained on the UDT
training data using HunPos (Halacsy, Kornai, & Oravecz, 2007).
231

fiTiedemann & Agic

 We parse the tagged corpus using a MaltParser model trained on the UDT with a
feature model optimized with MaltOptimizer (Ballesteros & Nivre, 2012).2
 We projectivize all trees using MaltParser and convert to nested tree annotations as
explained above (Tiedemann, 2015).
 We extract synchronous rule tables from the word aligned bitext with source side
syntax and score rules using Good Turing discounting. We do not use any size limit for
replacing sub-phrases with non-terminals at the source side and restrict the number
of non-terminals on the right-hand side of extracted rules to three. Furthermore, we
allow consecutive non-terminals on the source side to increase coverage, which is not
allowed in the default settings of the hierarchical rule extractor in Moses.
 We tune the model using MERT and the same data sets as before.
 Finally, we convert the training data of the UDT in the source language and translate
it to the target language using the tree-to-string model created above.
The results of our approach are listed in Table 6. We can see that syntax-based models
are superior to phrase-based models in almost all cases. For the majority of language
pairs we can also see an improvement over the annotation projection approach even though
the training data is much smaller. This confirms the findings of Tiedemann (2015) but
outperforms their results by a large margin due to the parsing model used in our experiments.
de
de
en
es
fr
sv

**

62.670.30
57.132.75
**
61.410.18
**
61.730.43

en
58.60+5.06

**

**

es
61.00+0.83
**
64.58+0.78
**

**

52.65+3.80
56.83+3.71
**
52.13+0.82

**



68.97+1.97
62.340.24

fr
63.45+1.10

68.45+1.98

69.37+0.82

**



sv
67.88+0.89
**
68.16+0.97
**
63.551.78

62.561.96

**

64.500.88

Table 6: Results for syntax-based treebank translation (difference to the corresponding
annotation projection model from Table 5 in superscript). Numbers in bold face
are better than the corresponding phrase-based SMT model. Results marked with
** are significantly different from the phrase-based translation results (p < 0.01); 
and  are significantly different from the projection model (p < 0.01 and p < 0.05,
respectively).

3.5 Translation and Back-Projection
Another possibility for cross-lingual parsing is the integration of translation in the actual
parsing pipeline. The basic idea is to use tools in other languages, such as dependency
parsers, without modification by adjusting the input to match the expectations of the tool,
2. We use MaltParser here for efficiency reasons. The parsing performance is slightly below the baseline
models trained with mate-tools but parsing is very fast which we require for parsing all bitexts.

232

fiSynthetic Treebanking for Cross-Lingual Dependency Parsing

label 2
label 1

pos1

src1

label 3

pos2

pos3

src2 src3

(2) parse

lexicalized

parser

pos4

src4
(3) project

(1) translate
trg1

trg2

trg3

trg4

pos2

pos1

pos3

pos4

label 1

label 3

label 2

Figure 11: Translation and back-projection: Input data is translated to a source language
with existing parsers (step 1), parsed in the source language (step 2) and, finally,
the parse tree is projected back to the original target language.

for example, by translating it to the language that a parser accepts. This is very much
in spirit of text normalization approaches that are frequently used in NLP for historical
documents and user-generated content in which the input is modified in such a way that
existing tools for standard language can be applied. Figure 11 illustrates the approach
applied to dependency parsing.
The advantage of this approach is that we can rely on optimized parsers that are trained
on manually corrected treebanks. However, there are several significant drawbacks. First
of all, we loose efficiency due to the additional translation step that is required at parsing
time. This is a crucial disadvantage that rules out this approach for many applications
which require parsed information of large scale data sets or real-time responses. Another
important drawback is the noise coming from translation leading to some kind of input,
which a parser is usually not trained for and, therefore, has a hard time to handle correctly.
Finally, there is also the problem of back-projection. Unfortunately, it is not straightforward
to reverse the projection heuristics discussed earlier. We cannot introduce DUMMY nodes
to fill gaps that are required for projecting the entire structure and DUMMY labels are not
useful either. The projection heuristics discussed in section 3.2 help to avoid DUMMY nodes
and, therefore, we apply these extensions in our experiments. Another problem is related to
unaligned target words. In the DCA algorithm (including all modified versions discussed so
far), these tokens are simply deleted and will not be attached to the dependency tree at all.
This method, however, is not possible for back-projection in which all tokens need to be
attached. For this reason, we implement a new rule that attaches each unaligned token to
either its preceding or consecutive word if they are attached to the tree themselves. If this
is not the case then we simply attach them to ROOT. Another problem is the label that
should be added to that dependency and due to the lack of further knowledge we set the
label to DUMMY. In this way, we do not get any credit in LAS but may at least improve
our UASs. We test this approach using syntax-based SMT as our translation model. The
results are listed in table 7.
233

fiTiedemann & Agic

de
en
es
fr
sv

de

44.8617.42
36.6923.77
37.4423.83
36.8426.12

en
35.9217.35

41.917.43
42.0011.46
35.2315.84

es
32.9024.79
48.0814.21

55.5410.97
31.9629.86

fr
36.6823.81
48.1917.35
54.7813.32

33.7431.25

sv
45.5619.69
51.7415.23
43.2321.44
42.3920.36


Table 7: Back-projection results in comparison to the annotation projection baseline from
section 3.2 (Table 3).

The scores are very low, as they even fall behind those of the baseline delexicalized
models. This extreme drop in performance is actually a bit surprising but considering the
strong disadvantages discussed above this may be expected as well. Another reason for the
extreme differences in performance is also the fact that we need to rely on predicted PoS
labels in the translated data before piping them into the source language parser. This is
certainly a strong disadvantage of the procedure and the comparison to evaluations based on
gold standard PoS annotation is not entirely fair. See also section 3.8 for more discussions
on the impact of PoS label accuracy on parsing performance.
3.6 Annotation Projection and Translation Quality
An interesting question is whether there is a correlation between translation quality and the
performance of the cross-lingual parsers based on translated treebanks. As an approximation
for treebank translation quality we computed BLEU scores over well-established MT test
sets from the WMT shared task, in our case the newstest from 2012.3
Figure 12 illustrates the correlation between BLEU scores obtained on newstest data
and LASs of the corresponding cross-lingual parsers. First of all, we can see that the
MT performance of phrase-based and syntax-based models is quite comparable with some
noticeable exceptions in which syntax-based SMT is significantly better (French-English and
French-Spanish, which is rather surprising). However, looking at most language pairs we
can see that the increased parsing performance does not seem to be due to improvements in
translation but rather due to the better fit of these models for syntactic annotation projection
(see German, for example). Nevertheless, we can observe a weak correlation between BLEU
scores and LAS within a class of models with one notable outlier, Spanish-English. This
correlation reflects the importance of the syntactic relation between languages for the success
of machine translation and annotation projection. Closely related languages like French and
Spanish are on the top level in both tasks whereas French and Spanish do not map well to
German. Translations to English are an exception in this evaluation. Translation models
often work well in this direction whereas annotation projection to English underperforms in
our experiments.
3. Note that we have to leave out Swedish for this test as there is no test set available for this language.

234

fiSynthetic Treebanking for Cross-Lingual Dependency Parsing

labeled attachment score (LAS)

70
68

RPB-SMT = 0.463

66

Rsyntax-SMT = 0.340

fr-es
en-fr
en-fr

64

es-fr
es-fr

fr-es

en-es
en-es

62

en-de

60

fr-de

de-fr
de-es

58

fr-de

en-de
es-de

56

de-fr

de-en de-es

fr-en

de-en fr-en

54

es-de
es-en
es-en

52
10

15

20

25

30

35

BLEU

Figure 12: Correlation between BLEU scores and cross-lingual parsing accuracy (using
Pearsons correlation coefficient).

3.7 System Combination and Multi-Source Models
So far, we were interested in transferring syntactic information from one source language
to the target language using one specific model for cross-lingual parsing. However, the
approaches above can easily be combined as they all focus on the creation of synthetic
training data. There are at least two possibilities that can be explored.
1. We can combine data from several source languages to increase the amount of training
data and to obtain evidence from various languages projected to the target language.
2. Several models can be combined to benefit from the various strengths of each model
that may work as complementary information.
In this paper, we opt for a very simple approach to test these ideas. Here we concatenate
data sets to augment our training data and train standard parsing models as usual. First, we
will look at multi-source models within each paradigm. Table 8 lists the labeled attachment
scores that we obtain when combining all data sets for all source languages to train target
language parsers on the projected annotations.
From the table, we can see that we are able to achieve significant improvements for
all languages and models except for Spanish. Furthermore, for English and for French we
obtain the overall best result presented in this paper for the combined syntax-based SMT
projections. In our final system combination, we now merge all data sets for all languages
and models. The results of the parsers trained on these combined data sets are shown in
Table 9.
4. These results are multi-source and multi-model system combinations provided by Tiedemann (2015).

235

fiTiedemann & Agic

LAS
best published result4
best individual model

de
60.94
63.83

en
56.58
58.60

es
68.45
68.97

fr
69.15
69.70

sv
68.95
68.20

annotation projection
phrase-based SMT
syntax-based SMT

66.76
61.85
65.89

55.30
60.94
61.56

67.37
68.08
68.60

69.48
71.54
72.78

71.95
71.69
72.14

Table 8: Results for combining projected data of all source languages to train target language
parsing models. Numbers in italics are worse than one of the models trained on
data for individual language pairs.

LAS
UAS
ACC

de
67.60
75.27
81.99

en
57.05
64.54
72.75

es
69.36
76.85
82.22

fr
72.03
79.21
83.06

sv
73.40
81.28
83.04

Table 9: Results for combining projected data of all source languages to train target language
parsing models. Additionally to LAS we also includes unlabeled attachment scores
(UAS) and label accuracy (ACC) here to make it easier to compare our results
with related work.

For German, French and Swedish this yields yet another significant improvement with
labeled attachment scores close to 70% or even above. These results represent the highest
scores that have been reported in this task so far and outperform previously published scores
by a large margin. We expect that more sophisticated system combinations would push
these results even further.
3.8 Gold vs. Predicted PoS Labels
It is common to evaluate results with gold PoS labels that are given in the test set of the
target language treebank. This disregard for the impact of PoS qualityoften present in
related workmakes for a very unrealistic evaluation scenario. In the previous section, we
discussed results that use gold standard annotation in order to make it possible to compare
our results with the baselines and related work. In this section, we look into more details
when replacing PoS labels with predicted values. Here, we report only the results for the
treebank translation approach using syntax-based SMT as a test case. The other approaches
show similar trends.
The first experiment looks at the case where annotated data is available for the target
language for training PoS taggers. We use HunPos (Halacsy et al., 2007) to train models on
the training data of each language and use them to replace the gold standard tags in all test
sets with PoS labels that our models predict. The results of these experiments applied to
the translated treebanks from section 3.4 are shown in Table 10.
236

fiSynthetic Treebanking for Cross-Lingual Dependency Parsing

de
en
es
fr
sv
PoS tagger

de

58.703.97
53.373.76
57.184.23
57.634.10

en
56.492.11

50.891.76
54.941.89
50.171.96

es
57.523.48
61.253.33

64.324.65
59.362.98

fr
59.993.46
64.324.13
65.244.13

60.893.61

sv
62.685.20
63.974.19
58.784.77
58.224.34


95.24

97.56

95.37

95.08

95.86

Table 10: Results for cross-lingual parsing with predicted PoS labels coming from taggers
trained on target language treebanks. The numbers in superscript give the
difference to the result with gold standard labels (Table 6). The last row shows
the overall accuracy of the PoS tagger.

We can see that PoS labels have a strong impact on parsing performance. For all language
pairs, we can observe a significant drop in LAS even with quite accurate taggers, which
proves that one need to be careful with applying models in real-life scenarios. The next
experiment stresses this point even more. Here, we replace PoS labels with tags that are
predicted by taggers that are trained on the noisy translated treebanks and their projected
annotation. Note that we need to remove training examples with DUMMY labels to reduce
errors of the tagger.

de
en
es
fr
sv

de

85.33
82.39
83.76
84.79

en
es
fr
sv
81.32 81.23 82.41 84.29

84.41 85.56 86.32
81.05

89.37 83.26
80.64 89.95

84.11
81.66 86.05 84.81


Table 11: PoS tagging accuracy for models trained on translated treebanks.

Table 11 lists the accuracy of the taggers trained on noisy projected data. We can
observe a significant drop in tagger performance which is completely plausible considering
the substantial noise added through translation and projection and also considering the
limited size of the data we use for training. Treebanks are considerably smaller than
annotated corpora that are usually taken for training PoS classifiers. When applying these
taggers to our test sets we can observe a dramatic drop in parsing performance as expected.
Table 12 lists the results of these experiments.
From the above findings we can conclude that cross-lingual techniques still require a lot
of improvement to become practically useful in low-resource scenarios in the real world. We
have done the same experiment for the annotation projection approach and observed the
same behavior even though we can rely on larger data sets for training the taggers. The
performance drop of using predicted PoS labels trained on noisy data sets amounts to over
237

fiTiedemann & Agic

de
en
es
fr
sv

de

51.896.81
44.598.78
49.727.46
47.949.69

en
46.0410.45

47.813.08
49.045.90
44.235.94

es
48.618.91
59.371.88

61.303.02
55.024.34

fr
50.369.63
62.371.95
59.815.43

52.798.10

sv
52.739.95
60.433.54
52.126.66
51.107.12


Table 12: Results for cross-lingual parsing with predicted PoS labels coming from taggers
trained on projected treebanks. The difference to the results with predicted labels
from Table 10 are shown in superscript.

10 LAS points in most cases similar to what we see in the treebank translation approach.
We omit the results as they do not add any new information to our discussion.
Finally, we also need to check whether system combinations and multi-source models
help to improve the quality of cross-lingual parsers with predicted PoS labels. For this,
we use the same strategy as in section 3.7 and concatenate the various data files to train
parser models that combine all models and language pairs. In other words, we use the same
models trained in section 3.7 but evaluate them on test sets that are automatically tagged
with PoS labels. Again, we use two settings: 1) We apply PoS taggers trained on manually
verified data setsthe monolingual target language treebanks, and 2) we use PoS taggers
trained on projected and translated treebanks. For the latter we have now all data sets
at our disposal and, therefore, expect a better PoS model as well. Table 13 lists the final
results in comparison to the ones obtained with gold standard annotation.

PoS
PoS
PoS
PoS

de
78.38
70.84
52.53
67.60

en
91.46
82.44
48.24
61.56

es
82.30
71.45
62.66
69.36

fr
82.30
73.71
62.39
72.78

sv
84.52
74.55
59.42
73.40

monolingual PoS tagger accuracy
combined projected PoS tagger accuracy

95.24
88.47

97.56
88.24

95.37
88.06

95.08
89.83

95.86
88.07

monolingual baseline with predicted PoS
delexicalized monolingual with predicted PoS
best delexicalized cross-lingual with predicted PoS
combined cross-lingual with predicted PoS
combined cross-lingual with projected PoS model

73.03
64.25
48.36
63.14
57.84

88.38
72.81
43.87
55.16
51.66

76.59
60.49
52.94
64.99
61.40

76.79
64.06
52.47
67.91
63.86

77.83
65.77
49.84
67.93
61.58

monolingual baseline
delexicalized monolingual
best delexicalized cross-lingual
best cross-lingual model

with
with
with
with

gold
gold
gold
gold

Table 13: A comparison between models evaluated with gold standard PoS annotation (four
top-level systems) and models tested against automatically tagged data.

First of all, we can see that our best cross-lingual models outperform delexicalized
cross-lingual models by a large margin. They come very close to delexicalized models trained
238

fiSynthetic Treebanking for Cross-Lingual Dependency Parsing

on target language data with the exception of English which works much better with the
original data set. In the lower part of the table, we observe that the scores drop significantly
when gold standard PoS labels are replaced with predicted tags. Note that the four systems
using predicted PoS labels apply the tagger trained on monolingual verified target language
data which gives quite high accuracy. The final system in the table is the only one that
applies the PoS model trained on projected and translated data. These tagger models are
much less accurate, as shown in the middle of Table 13, and the influence of this degradation
is visible in the attachment scores obtained by the systems. However, these models reflect a
real-world scenario where no annotated is available for the target language, not even for
training PoS taggers. The advantage of the projection and translation approaches is that
such model is possible at all, whereas delexicalized and other transfer models always require
existing tools that can produce the shared features used by the prediction system. Note also
that the cross-lingual models now outperform some of the delexicalized models trained on
verified target language datawith English as a striking exceptionwhich is remarkable
given the noisy data they are trained on.

3.9 Impact of Dataset Sizes
By and large, the data-driven dependency parsers benefit from introducing additional
training data. In this subsection, we control for the amount of training data provided to our
method, and observe the impact it has on LAS cross-lingually. We experiment with improved
annotation projection (see Section 3.2), and we introduce up to 60 thousand sentences with
projected dependency trees. For each of the five target UDT languages in the experiment,
we provide four learning curves representing the four source languages. We plot the results
in Figure 13.
We observe that virtually all transferred parsers benefit from the introduction of additional
training data, albeit some of the improvements are only slight as some models level out
at around 20 thousand sentences. All the source languages follow the same LAS learning
curve patterns for all the targets, as we do not observe any trend violations for specific
source-target pairs. Other than that, we observe clear source-target preferences, as the
source orderings by LAS mostly remain the same for all training set sizes. Some of the
lower-ranked sources do not benefit or even degrade by introducing more training data,
for example, the Spanish parser induced from German data, or the English parser created
by projecting Swedish trees. That said, it is worth noting that in the best source-target
pairs, the targets always benefit from introducing more source data: German from English
and Swedish, English from German and French, Spanish from French and vice versa, and
Swedish from German and English. This is a very clear indicator for future improvements,
as the method apparently benefits from adding more data. At the same time, our learning
curves show benefits for truly under-resourced languages, as the largest relative gains are
already reached at relatively modest quantities of 20 thousand sentence pairs. Moreover,
the typological groupings in the former list of top-performing source-target pairs are quite
apparent, as is the case throughout our experiments.
239

fien
es
fr
sv
0

LAS (projected to Spanish)

LAS (projected to English)

63
62
61
60
59
58
57
56
55
54
53

10k 20k 30k 40k 50k
nr of projected sentences

68
66
64
62
60
de
en
fr
sv

58
56
54

10k 20k 30k 40k 50k
nr of projected sentences
LAS (projected to Swedish)

0

54
53
52
51
50
49
48
47
46
45
44

60k

de
es
fr
sv
0

LAS (projected to French)

LAS (projected to German)

Tiedemann & Agic

60k

60k

70
68
66
64
62
de
en
es
sv

60
58
56
0

68
67
66
65
64
63
62
61
60
59

10k 20k 30k 40k 50k
nr of projected sentences

10k 20k 30k 40k 50k
nr of projected sentences

60k

de
en
es
fr
0

10k 20k 30k 40k 50k
nr of projected sentences

60k

Figure 13: The impact of training data: Different sizes of projected data for training crosslingual parsing models.

4. Comparison to Related Work
In this sectionhaving thoroughly analyzed synthetic treebankingwe revert to a top-level
discussion of cross-lingual parsing. In it, we contrast our approach to several selected
alternatives from related work, and we sketch their properties from the viewpoint of enabling
dependency parsing for truly under-resourced languages. We proceed by outlining the
comparison.
We have already compared the various synthetic treebanking approaches to one another
and to the delexicalized transfer baseline of McDonald et al. (2013) in section 3. Here, we
aim at introducing a number of top-performing representatives of the methods discussed in
240

fiSynthetic Treebanking for Cross-Lingual Dependency Parsing

the overview section: a more competitive model transfer approach, an approach dealing with
distributed word representations, and an annotation projection-motivated approach. As
replicating all the approaches would be very time-consuming, we constrain our search to the
approaches that also report their scores on UDT version 1 in their respective publication, as
we can then compare by referencing. We select the following approaches for our discussion.
 Delex: This is the delexicalized model transfer baseline of McDonald et al. (2013).
We report the scores by Sgaard et al. (2015) who used the arc-factored adaptation of
the mate-tools parser, and not our replication or the original, as they conveniently
report multiple metrics. We discuss the metrics below, and we note that they used
gold PoS.
 Multi: A reimplementation of McDonald et al. (2011) multi-source projected system
(multi-proj. in the original paper) by Ma and Xia (2014). We provide it as a more
competitive baseline system. The original work predates UDT and only evaluates on
the heterogenous CoNLL treebanks, but Ma and Xia (2014) evaluate it on the UDT
treebanks so we report their scores. Note that the parsing model and preprocessing
is then inherent to their setup, differing from the original setup of McDonald et al.
(2011). The setup details are described further in the text, under Xia.
 Proj: The improved annotation projection approach we described in section 3.2.
It is the final approach of the subsection, in which the dependency relations over
unary dummy nodes are collapsed, dummy leaves removed, and all Europarl trees with
remaining dummy nodes discarded (see Table 4). These scores are given with gold
PoS tags.
 Trans G & P: We report on our best syntax-based cross-lingual treebank translation
scores with gold and predicted PoS, respectively. Our PoS predictions come from
an HMM tagger (Halacsy et al., 2007). The taggers are trained on target language
treebanks, and they score at 95% on average (see Table 10).
 Comb G & P: These are our multi-source syntax-based cross-lingual parsers. They
build on the Trans G & P approaches: instead of just single sources, multiple
treebanks are translated into the target languages, providing combined synthetic
treebanks to train parsers on. As before, we also report scores with gold and HunPospredicted PoS.
 Rosa: This is the multi-source delexicalized transfer approach of Rosa and Zabokrtsky
(2015), in its weighted variant. In their method, each target is parsed by multiple
sources, and each parse is assigned a weight based on an empirically established
language similarity metric. For each target sentence, the multiple parses constitute a
digraph, on top of which a (Sagae & Lavie, 2006)-style maximum spanning tree voting
scheme is implemented. They use gold PoS tags.
 Sgaard: In this model, delexicalized model transfer is augmented by inter-lingual
word representations based on inverted indexing via Wikipedia concept links (Sgaard
et al., 2015). We choose it as a very recent and illustrative example of leveraging word
241

fiTiedemann & Agic

Target
language
de
en
es
fr
sv

Baselines
Delex Multi
56.80

63.21
66.00
67.49

69.21

72.57
74.60
75.87

Proj
72.65
62.79
74.92
76.13
76.96

Synthetic treebanking
TransG TransP CombG
70.62
65.10
75.71
76.33
76.98

67.59
63.62
72.16
72.95
73.61

75.27
64.54
76.85
79.21
81.28

CombP
71.79
63.15
73.20
76.06
76.83

Recent approaches
Rosa Sgaard Xia
56.80
42.60
72.70

50.80

56.56

64.03
66.22
67.32

74.01

75.60
76.93
79.27

Table 14: Comparison of cross-lingual parsing methods. In contrast to the rest of our paper,
here we report UAS scores to attain maximum coverage of results reported in
related work.

embeddings for improving cross-lingual dependency parsing. They use an embeddingsenabled version of Bohnets parser (Bohnet, 2010) and gold PoS tags. We report their
multi-source results.
 Xia: The approach by Ma and Xia (2014) is a novel method that leverages Europarl
to train probabilistic parsing models for resource-poor languages by maximizing a
combination of likelihood on parallel data and confidence on unlabeled data. We
report on their best approach (marked as +U in their paper), which makes use of
both parallel and unlabeled data. They use top-performing PoS taggers trained on the
target languages, each of them reaching at least a 95% accuracy.
Before discussing the results, we make a number of remarks on the comparison. First, for
each target language, we report the best obtained score for each method, rather than possibly
misleading averages or more complex source-target matrices. In most related work, English
is not used as a target language. Second, in contrast to the remainder of the paperand
contrary to the guidelines for evaluating cross-lingual parsers following McDonald et al.
(2013)we report on UAS only. This is targeted exclusively at facilitating the comparison
to related work, as these contributions for the most part still report UAS scores, even when
working with UDT. While we do see this as unfortunate, we also note that a LAS-enabled
replication study exceeds the scope and does not match the focus of our contribution. Third,
and also related to not being able to control for all the experiment parameters, we note the
issue of reporting scores on gold and predicted PoS, and the different ways of obtaining the
predicted annotations. We record the differences in the list above. Finally, we note that
some of the referenced contributions do not explicitly state whether their scoring included
punctuation or not, whereas we do include it in our experiments.
The results are given in Table 14 and we now proceed to discuss them in more detail,
reflecting on the methods intricacies and requirements in the process.
In the table, we visually group the methods into the baselines (Delex, Multi), our
proposed approaches (Proj, Trans, Comb), and selected recent contributions to crosslingual dependency parsing (Rosa, Sgaard, Xia). By design, we do not highlight the
best scores, as not all the results are directly comparable, especially with respect to the lack
of control for sources of features facilitating the parsing, such as the PoS tags. We also note
that Rosa is evaluated on the HamleDT treebanks (Rosa, Masek, Marecek, Popel, Zeman,
242

fiSynthetic Treebanking for Cross-Lingual Dependency Parsing

& Zabokrtsky, 2014) and not UDT, but we still provide it for reference, as it implements an
interesting addition to Delex as a sort of an intermediate step towards Multi.
We first observe that Rosa and Sgaard rarely surpass our Delex baseline. This
does not come as a surprise, as our baseline uses a more advanced graph-based dependency
parser (Bohnet, 2010): in contrast, Rosa uses an arc-factored parser (McDonald, Pereira,
Ribarov, & Hajic, 2005), while Sgaard implements a first-order version of the parser by
Bohnet (2010) that leverages cross-lingual word representations. That said, the discrepancy
between the first- and second-order graph-based parsers appears not to be the only factor
in explaining the slight (if any) gains provided by these two approaches. Namely, Rosa is
an approach to multi-source delexicalized parsing based on maximum spanning tree-style
voting, and it uses empirically obtained dataset similarity metrics for weighting the arcs in
the voting schemes. As such, even if it yields slight improvements over the respective fair
baselinesas provided in the paper describing the approach (Rosa & Zabokrtsky, 2015)it
is still bound by the impoverished feature representation informing the parser, inherited from
the Delex it builds on, preventing the method from reaching higher accuracies. Sgaard
attempts to alleviate this by introducing cross-lingual word representations to the feature
space. In their report on the approach, Sgaard et al. (2015) observe slight improvements
over the baselines, but it is apparent that the word representations they utilize work much
better for NLP tasks that dont involve syntactic representations, indicating they might not
be appropriate for facilitating cross-lingual parsing more substantially.
Having considered Rosa and Sgaardcomparing the two approaches to the Delex
baseline, and establishing their inferiority to the remaining approaches, including synthetic
trebankingwe turn to the more interesting part of the discussion, in which our contributions
are compared to one another, and to Xia. We also include the competitive Multi baseline
of McDonald et al. (2011) to this discussion.
Our improved annotation projection Proj appears to be a very competitive method,
as none of the other approaches surpass it by a large margin. It also consistently beats
Multi, albeit their PoS annotations are not comparable. Syntax-based treebank translation
(Trans) surpasses it by a very narrow margin on four out of five targets, with German as
the exception, while the multi-source variant (Comb) adds approximately 3-5 LAS points to
the difference, with English as the exception. Only the approaches using predicted PoS tags
are contrasted to Xia, but noting that on these datasets, our tagging approach (HunPos)
performs slightly under theirs (Stanford) on average. We observe that Xia exhibits a slight
advantage over out top approach (CombP) across the targets, but we also noteon top of the
differences in taggersthat their approach also utilizes unlabeled data for semi-supervised
parser augmentation. That said, Ma and Xia (2014) document only minor decreases when
removing the unlabeled sources, and they implement an arc-factored dependency parser in
the pipeline. Thus, we note that i) our synthetic treebanking approaches and Xia currently
represent the most competitive approaches to cross-lingual dependency parsing, with a
slight empirical edge for the latter, and that ii) further research is neededin the form of
an extensive replicative survey of cross-lingual parsingto empirically gauge the various
intricacies of these two approaches, and other influential contributions to the field, such as
the work of McDonald et al. (2011) or Xiao and Guo (2014). We also note a very recent
contribution by Rasooli and Collins (2015), which also deals with parallel corpora and
projections, showing very promising results.
243

fiTiedemann & Agic

At this point, from the viewpoint of enabling the processing of truly under-resourced
languages, it is interesting to mark the following observation. In Table 14, there is an apparent
disconnect in scores between the methods that exploit parallel data sources (Multi, Proj,
Trans, Comb, Xia), and the methods that dont (Delex, Rosa, Sgaard): the methods
that make use of the parallel resources all perform significantly better. This is a clear
indicator that for reaching top-level cross-lingual parsing performance, at least with the
current line-up of standard dependency parsers, we need the lexical features provided by
parallel corpora. The observation appears to us as a clear guideline for future work in
cross-lingual parsing, and in the enablement of NLP for under-resourced languages.

5. Conclusions and Future Work
In this paper we discussed the various approaches for cross-lingual dependency parsing,
reviewing and comparing a number of commonly used methods. Furthermore, we included
an extensive study of annotation projection and treebank translation, and presented very
competitive results in cross-lingual dependency parsing for the task of parsing data with
cross-lingually harmonized annotation as included in the Universal Dependency Treebank.
Our future work includes the incorporation of cross-lingual word embeddings in model
transfer as another component of the system combinations we discuss in the paper. We will
also look at a wider range of languages using the growing set of harmonized data sets in the
Universal Dependencies project. Especially interesting is the use of our techniques for truly
under-resourced languages. We will explore cross-lingual parsing as a means of bootstrapping
tools for those languages. We also aim at implementing a large-scale replicative survey of
cross-lingual dependency parsing, as we show in our contribution that such an empirical
assessment would be very timely and beneficial to this fast-developing field.

Acknowledgements
We thank the four anonymous reviewers for their detailed comments, which significantly
contributed to improving the quality of the publication. We also acknowledge Joakim Nivre
for the discussions on synthetic treebanking, and Hector Martnez Alonso for his suggestions
on improving the readability of the paper.

References
Abeille, A. (2003). Treebanks: Building and Using Parsed Corpora. Springer.
Agic, Z., Merkler, D., & Berovic, D. (2012). Slovene-Croatian Treebank Transfer Using
Bilingual Lexicon Improves Croatian Dependency Parsing. In Proceedings of IS-LTC,
pp. 59.
Agic, Z., Hovy, D., & Sgaard, A. (2015). If All You Have is a Bit of the Bible: Learning
POS Taggers for Truly Low-resource Languages. In Proceedings of ACL, pp. 268272.
Agic, Z., Tiedemann, J., Merkler, D., Krek, S., Dobrovoljc, K., & Moze, S. (2014). Crosslingual Dependency Parsing of Related Languages with Rich Morphosyntactic Tagsets.
In Proceedings of LT4CloseLang, pp. 1324.
244

fiSynthetic Treebanking for Cross-Lingual Dependency Parsing

Ballesteros, M., & Nivre, J. (2012). MaltOptimizer: An Optimization Tool for MaltParser.
In Proceedings of EACL, pp. 5862.
Bender, E. M. (2011). On Achieving and Evaluating Language-independence in NLP.
Linguistic Issues in Language Technology, 6 (3), 126.
Bender, E. M. (2013). Linguistic Fundamentals for Natural Language Processing: 100
Essentials from Morphology and Syntax. Morgan & Claypool Publishers.
Bohmova, A., Hajic, J., Hajicova, E., & Hladka, B. (2003). The Prague Dependency
Treebank. In Treebanks, pp. 103127. Springer.
Bohnet, B. (2010). Top Accuracy and Fast Dependency Parsing is not a Contradiction. In
Proceedings of COLING, pp. 8997.
Buchholz, S., & Marsi, E. (2006). CoNLL-X Shared Task on Multilingual Dependency
Parsing. In Proceedings of CoNLL, pp. 149164.
Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011).
Natural language processing (almost) from scratch. Journal of Machine Learning
Research, 12, 24932537.
Das, D., & Petrov, S. (2011). Unsupervised Part-of-Speech Tagging with Bilingual GraphBased Projections. In Proceedings of ACL, pp. 600609.
De Marneffe, M.-C., MacCartney, B., & Manning, C. D. (2006). Generating Typed Dependency Parses from Phrase Structure Parses. In Proceedings of LREC, pp. 449454.
Durrett, G., Pauls, A., & Klein, D. (2012). Syntactic Transfer Using a Bilingual Lexicon. In
Proceedings of EMNLP-CoNLL, pp. 111.
Elming, J., Johannsen, A., Klerke, S., Lapponi, E., Martinez Alonso, H., & Sgaard, A.
(2013). Down-stream Effects of Tree-to-dependency Conversions. In Proceedings of
NAACL, pp. 617626.
Faruqui, M., & Dyer, C. (2014). Improving Vector Space Word Representations Using
Multilingual Correlation. In Proceedings of EACL, pp. 462471.
Garrette, D., Mielens, J., & Baldridge, J. (2013). Real-World Semi-Supervised Learning of
POS-Taggers for Low-Resource Languages.. In Proceedings of ACL, pp. 583592.
Gouws, S., & Sgaard, A. (2015). Simple Task-specific Bilingual Word Embeddings. In
Proceedings of NAACL.
Halacsy, P., Kornai, A., & Oravecz, C. (2007). HunPos  An Open-source Trigram Tagger.
In Proceedings of ACL, pp. 209212.
Heafield, K., Pouzyrevsky, I., Clark, J. H., & Koehn, P. (2013). Scalable Modified Kneser-Ney
Language Model Estimation. In Proceedings of ACL, pp. 690696.
Hwa, R., Resnik, P., Weinberg, A., Cabezas, C., & Kolak, O. (2005). Bootstrapping Parsers
via Syntactic Projection across Parallel Texts. Natural Language Engineering, 11 (3),
311325.
Klementiev, A., Titov, I., & Bhattarai, B. (2012). Inducing Crosslingual Distributed
Representations of Words. In Proceedings of COLING, pp. 14591474.
245

fiTiedemann & Agic

Koehn, P. (2005). Europarl: A Parallel Corpus for Statistical Machine Translation. In
Proceedings of MT Summit, pp. 7986.
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan, B.,
Shen, W., Moran, C., Zens, R., Dyer, C. J., Bojar, O., Constantin, A., & Herbst, E.
(2007). Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings
of ACL, pp. 177180.
Koo, T., Carreras, X., & Collins, M. (2008). Simple Semi-supervised Dependency Parsing.
In Proceedings of ACL, pp. 595603.
Kubler, S., McDonald, R., & Nivre, J. (2009). Dependency Parsing. Morgan & Claypool
Publishers.
Li, S., Graca, J. V., & Taskar, B. (2012). Wiki-ly Supervised Part-of-speech Tagging. In
Proceedings of EMNLP-CoNLL, pp. 13891398.
Ma, X., & Xia, F. (2014). Unsupervised Dependency Parsing with Transferring Distribution
via Parallel Guidance and Entropy Regularization. In Proceedings of ACL), pp. 1337
1348.
McDonald, R., Nivre, J., Quirmbach-Brundage, Y., Goldberg, Y., Das, D., Ganchev, K.,
Hall, K., Petrov, S., Zhang, H., Tackstrom, O., Bedini, C., Bertomeu Castello, N.,
& Lee, J. (2013). Universal Dependency Annotation for Multilingual Parsing. In
Proceedings of ACL, pp. 9297.
McDonald, R., Pereira, F., Ribarov, K., & Hajic, J. (2005). Non-Projective Dependency
Parsing using Spanning Tree Algorithms. In Proceedings of EMNLP, pp. 523530.
McDonald, R., Petrov, S., & Hall, K. (2011). Multi-Source Transfer of Delexicalized
Dependency Parsers. In Proceedings of EMNLP, pp. 6272.
Mikolov, T., Le, Q. V., & Sutskever, I. (2013). Exploiting Similarities among Languages for
Machine Translation. http://arxiv.org/pdf/1309.4168.pdf.
Naseem, T., Barzilay, R., & Globerson, A. (2012). Selective Sharing for Multilingual
Dependency Parsing. In Proceedings of ACL, pp. 629637.
Nivre, J. (2006). Inductive dependency parsing. Springer.
Nivre, J., Bosco, C., Choi, J., de Marneffe, M.-C., Dozat, T., Farkas, R., Foster, J., & Ginter,
F. e. a. (2015). Universal dependencies 1.0..
Nivre, J., Hall, J., Kubler, S., McDonald, R., Nilsson, J., Riedel, S., & Yuret, D. (2007).
The CoNLL 2007 Shared Task on Dependency Parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pp. 915932.
Och, F. J. (2003). Minimum Error Rate Training in Statistical Machine Translation. In
Proceedings of ACL, pp. 160167.
Och, F. J., & Ney, H. (2003). A Systematic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29 (1), 1952.
Petrov, S. (2014). Towards Universal Syntactic Processing of Natural Language. In Proceedings of LT4CloseLang, p. 66.
246

fiSynthetic Treebanking for Cross-Lingual Dependency Parsing

Petrov, S., Das, D., & McDonald, R. (2012). A Universal Part-of-Speech Tagset. In
Proceedings of LREC, pp. 20892096.
Plank, B., Martnez Alonso, H., Agic, v., Merkler, D., & Sgaard, A. (2015). Do Dependency
Parsing Metrics Correlate with Human Judgments?. In Proceedings of CONLL, pp.
315320.
Rasooli, M. S., & Collins, M. (2015). Density-Driven Cross-Lingual Transfer of Dependency
Parsers. In Proceedings of EMNLP.
Rosa, R., Masek, J., Marecek, D., Popel, M., Zeman, D., & Zabokrtsky, Z. (2014). HamleDT
2.0: Thirty Dependency Treebanks Stanfordized. In Proceedings of LREC, pp. 2334
2341.
Rosa, R., & Zabokrtsky, Z. (2015). KLcpos3 - a Language Similarity Measure for Delexicalized
Parser Transfer. In Proceedings of ACL, pp. 243249.
Sagae, K., & Lavie, A. (2006). Parser Combination by Reparsing. In Proceedings of NAACL,
pp. 129132.
Sgaard, A. (2011). Data Point Selection for Cross-language Adaptation of Dependency
Parsers. In Proceedings of ACL, pp. 682686.
Sgaard, A. (2012). Unsupervised Dependency Parsing Without Training. Natural Language
Engineering, 18 (02), 187203.
Sgaard, A. (2013). Semi-Supervised Learning and Domain Adaptation in Natural Language
Processing. Morgan & Claypool Publishers.
Sgaard, A., Agic, Z., Martnez Alonso, H., Plank, B., Bohnet, B., & Johannsen, A. (2015).
Inverted Indexing for Cross-lingual NLP. In Proceedings of ACL, pp. 17131722.
Tackstrom, O., Das, D., Petrov, S., McDonald, R., & Nivre, J. (2013a). Token and Type
Constraints for Cross-lingual Part-of-speech Tagging. Transactions of the Association
for Computational Linguistics, 1, 112.
Tackstrom, O., McDonald, R., & Nivre, J. (2013b). Target Language Adaptation of
Discriminative Transfer Parsers. In Proceedings of NAACL, pp. 10611071.
Tackstrom, O., McDonald, R., & Uszkoreit, J. (2012). Cross-lingual Word Clusters for
Direct Transfer of Linguistic Structure. In Proceedings of NAACL, pp. 477487.
Tiedemann, J. (2014). Rediscovering Annotation Projection for Cross-Lingual Parser
Induction. In Proceedings of COLING, pp. 18541864.
Tiedemann, J., Agic, Z., & Nivre, J. (2014). Treebank Translation for Cross-Lingual Parser
Induction. In Proceedings of CoNLL, pp. 130140.
Tiedemann, J., & Nakov, P. (2013). Analyzing the use of character-level translation with
sparse and noisy datasets. In Proceedings of RANLP, pp. 676684.
Tiedemann, J. (2012). Parallel Data, Tools and Interfaces in OPUS. In Proceedings of
LREC, pp. 22142218.
Tiedemann, J. (2015). Improving the Cross-Lingual Projection of Syntactic Dependencies.
In Proceedings of NoDaLiDa.
247

fiTiedemann & Agic

Uszkoreit, H., & Rehm, G. (2012). Language White Paper Series. Springer.
Xiao, M., & Guo, Y. (2014). Distributed Word Representation Learning for Cross-Lingual
Dependency Parsing. In Proceedings of CoNLL, pp. 119129.
Yarowsky, D., Ngai, G., & Wicentowski, R. (2001). Inducing Multilingual Text Analysis
Tools via Robust Projection Across Aligned Corpora. In Proceedings of HLT, pp. 18.
Zeman, D., Dusek, O., Marecek, D., Popel, M., Ramasamy, L., Stepanek, J., Zabokrtsky,
Z., & Hajic, J. (2014). HamleDT: Harmonized Multi-language Dependency Treebank.
Language Resources and Evaluation, 48 (4), 601637.
Zeman, D., & Resnik, P. (2008). Cross-Language Parser Adaptation between Related
Languages. In Proceedings of IJCNLP, pp. 3542.
Zhang, Y., Reichart, R., Barzilay, R., & Globerson, A. (2012). Learning to Map into a
Universal POS Tagset. In Proceedings of EMNLP-CoNLL, pp. 13681378.
Zhao, H., Song, Y., Kit, C., & Zhou, G. (2009). Cross Language Dependency Parsing Using
a Bilingual Lexicon. In Proceedings of ACL-IJCNLP, pp. 5563.
Zou, W. Y., Socher, R., Cer, D., & Manning, C. D. (2013). Bilingual Word Embeddings for
Phrase-Based Machine Translation. In Proceedings of EMNLP, pp. 13931398.

248

fiJournal of Artificial Intelligence Research 55 (2016) 409-442

Submitted 07/15; published 02/16

Automatic Description Generation from Images: A Survey
of Models, Datasets, and Evaluation Measures
Raffaella Bernardi

bernardi@disi.unitn.it

University of Trento, Italy

Ruket Cakici

ruken@ceng.metu.edu.tr

Middle East Technical University, Turkey

Desmond Elliott

d.elliott@uva.nl

University of Amsterdam, Netherlands

Aykut Erdem
Erkut Erdem
Nazli Ikizler-Cinbis

aykut@cs.hacettepe.edu.tr
erkut@cs.hacettepe.edu.tr
nazli@cs.hacettepe.edu.tr

Hacettepe University, Turkey

Frank Keller

keller@inf.ed.ac.uk

University of Edinburgh, UK

Adrian Muscat

adrian.muscat@um.edu.mt

University of Malta, Malta

Barbara Plank

bplank@cst.dk

University of Copenhagen, Denmark

Abstract
Automatic description generation from natural images is a challenging problem that
has recently received a large amount of interest from the computer vision and natural language processing communities. In this survey, we classify the existing approaches based
on how they conceptualize this problem, viz., models that cast description as either generation problem or as a retrieval problem over a visual or multimodal representational
space. We provide a detailed review of existing models, highlighting their advantages and
disadvantages. Moreover, we give an overview of the benchmark image datasets and the
evaluation measures that have been developed to assess the quality of machine-generated
image descriptions. Finally we extrapolate future directions in the area of automatic image
description generation.

1. Introduction
Over the past two decades, the fields of natural language processing (NLP) and computer
vision (CV) have seen great advances in their respective goals of analyzing and generating
text, and of understanding images and videos. While both fields share a similar set of methods rooted in artificial intelligence and machine learning, they have historically developed
separately, and their scientific communities have typically interacted very little.
Recent years, however, have seen an upsurge of interest in problems that require a
combination of linguistic and visual information. A lot of everyday tasks are of this nature,
e.g., interpreting a photo in the context of a newspaper article, following instructions in
conjunction with a diagram or a map, understanding slides while listening to a lecture. In
c
2016
AI Access Foundation. All rights reserved.

fiBernardi et al.

addition to this, the web provides a vast amount of data that combines linguistic and visual
information: tagged photographs, illustrations in newspaper articles, videos with subtitles,
and multimodal feeds on social media. To tackle combined language and vision tasks and to
exploit the large amounts of multimodal data, the CV and NLP communities have moved
closer together, for example by organizing workshops on language and vision that have been
held regularly at both CV and NLP conferences over the past few years.
In this new language-vision community, automatic image description has emerged as a
key task. This task involves taking an image, analyzing its visual content, and generating
a textual description (typically a sentence) that verbalizes the most salient aspects of the
image. This is challenging from a CV point of view, as the description could in principle
talk about any visual aspect of the image: it can mention objects and their attributes, it
can talk about features of the scene (e.g., indoor/outdoor), or verbalize how the people
and objects in the scene interact. More challenging still, the description could even refer to
objects that are not depicted (e.g., it can talk about people waiting for a train, even when
the train is not visible because it has not arrived yet) and provide background knowledge
that cannot be derived directly from the image (e.g., the person depicted is the Mona Lisa).
In short, a good image description requires full image understanding, and therefore the
description task is an excellent test bed for computer vision systems, one that is much more
comprehensive than standard CV evaluations that typically test, for instance, the accuracy
of object detectors or scene classifiers over a limited set of classes.
Image understanding is necessary, but not sufficient for producing a good description.
Imagine we apply an array of state-of-the-art detectors to the image to localize objects
(e.g., Felzenszwalb, Girshick, McAllester, & Ramanan, 2010; Girshick, Donahue, Darrell,
& Malik, 2014), determine attributes (e.g., Lampert, Nickisch, & Harmeling, 2009; Berg,
Berg, & Shih, 2010; Parikh & Grauman, 2011), compute scene properties (e.g., Oliva &
Torralba, 2001; Lazebnik, Schmid, & Ponce, 2006), and recognize human-object interactions (e.g., Prest, Schmid, & Ferrari, 2012; Yao & Fei-Fei, 2010). The result would be a
long, unstructured list of labels (detector outputs), which would be unusable as an image
description. A good image description, in contrast, has to be comprehensive but concise
(talk about all and only the important things in the image), and has to be formally correct,
i.e., consists of grammatically well-formed sentences.
From an NLP point of view, generating such a description is a natural language generation (NLG) problem. The task of NLG is to turn a non-linguistic representation into
human-readable text. Classically, the non-linguistic representation is a logical form, a
database query, or a set of numbers. In image description, the input is an image representation (e.g., the detector outputs listed in the previous paragraph), which the NLG
model has to turn into sentences. Generating text involves a series of steps, traditionally
referred to as the NLP pipeline (Reiter & Dale, 2006): we need to decide which aspects
of the input to talk about (content selection), then we need to organize the content (text
planning) and verbalize it (surface realization). Surface realization in turn requires choosing the right words (lexicalization), using pronouns if appropriate (referential expression
generation), and grouping related information together (aggregation).
In other words, automatic image description requires not only full image understanding,
but also sophisticated natural language generation. This is what makes it such an interesting
410

fiAutomatic Description Generation from Images: A Survey

task that has been embraced by both the CV and the NLP communities.1 Note that
the description task can become even more challenging when we take into account that
good descriptions are often user-specific. For instance, an art critic will require a different
description than a librarian or a journalist, even for the same photograph. We will briefly
touch upon this issue when we talk about the difference between descriptions and captions
in Section 3 and discuss future directions in Section 4.
Given that automatic image description is such an interesting task, and it is driven by the
existence of mature CV and NLP methods and the availability of relevant datasets, a large
image description literature has appeared over the last five years. The aim of this survey
article is to give a comprehensive overview of this literature, covering models, datasets, and
evaluation metrics.
We sort the existing literature into three categories based on the image description
models used. The first group of models follows the classical pipeline we outlined above: they
first detect or predict the image content in terms of objects, attributes, scene types, and
actions, based on a set of visual features. Then, these models use this content information
to drive a natural language generation system that outputs an image description. We will
term these approaches direct generation models.
The second group of models cast the problem as a retrieval problem. That is, to create a
description for a novel image, these models search for images in a database that are similar to
the novel image. Then they build a description for the novel image based on the descriptions
of the set of similar images that was retrieved. The novel image is described by simply
reusing the description of the most similar retrieved image (transfer), or by synthesizing a
novel description based on the description of a set of similar images. Retrieval-based models
can be further subdivided based on what type of approach they use to represent images
and compute similarity. The first subgroup of models uses a visual space to retrieve images,
while the second subgroup uses a multimodal space that represents images and text jointly.
For an overview of the models that will be reviewed in this survey, and which category they
fall into, see Table 1.
Generating natural language descriptions from videos presents unique challenges over
and above image-based description, as it additionally requires analyzing the objects and
their attributes and actions in the temporal dimension. Models that aim to solve description generation from videos have been proposed in the literature (e.g., Khan, Zhang, &
Gotoh, 2011; Guadarrama, Krishnamoorthy, Malkarnenkar, Venugopalan, Mooney, Darrell,
& Saenko, 2013; Krishnamoorthy, Malkarnenkar, Mooney, Saenko, & Guadarrama, 2013;
Rohrbach, Qiu, Titov, Thater, Pinkal, & Schiele, 2013; Thomason, Venugopalan, Guadarrama, Saenko, & Mooney, 2014; Rohrbach, Rohrback, Tandon, & Schiele, 2015; Yao, Torabi,
Cho, Ballas, Pal, Larochelle, & Courville, 2015; Zhu, Kiros, Zemel, Salakhutdinov, Urtasun,
Torralba, & Fidler, 2015). However, most existing work on description generation has used
static images, and this is what we will focus on in this survey.2
In this survey article, we first group automatic image description models into the three
categories outlined above and provide a comprehensive overview of the models in each
1. Though some image description approaches circumvent the NLG aspect by transferring human-authored
descriptions, see Sections 2.2 and 2.3.
2. An interesting intermediate approach involves the annotation of image streams with sequences of sentences, see the work of Park and Kim (2015).

411

fiBernardi et al.

category in Section 2. We then examine the available multimodal image datasets used for
training and testing description generation models in Section 3. Furthermore, we review
evaluation measures that have been used to gauge the quality of generated descriptions in
Section 3. Finally, in Section 4, we discuss future research directions, including possible
new tasks related to image description, such as visual question answering.

2. Image Description Models
Generating automatic descriptions from images requires an understanding of how humans
describe images. An image description can be analyzed in several different dimensions (Shatford, 1986; Jaimes & Chang, 2000). We follow Hodosh, Young, and Hockenmaier (2013)
and assume that the descriptions that are of interest for this survey article are the ones
that verbalize visual and conceptual information depicted in the image, i.e., descriptions
that refer to the depicted entities, their attributes and relations, and the actions they are
involved in. Outside the scope of automatic image description are non-visual descriptions,
which give background information or refer to objects not depicted in the image (e.g., the
location at which the image was taken or who took the picture). Also, not relevant for
standard approaches to image description are perceptual descriptions, which capture the
global low-level visual characteristics of images (e.g., the dominant color in the image or
the type of the media such as photograph, drawing, animation, etc.).
In the following subsections, we give a comprehensive overview of state-of-the-art approaches to description generation. Table 1 offers a high-level summary of the field, using
the three categories of models outlined in the introduction: direct generation models, retrieval models from visual space, and retrieval model from multimodal space.
2.1 Description as Generation from Visual Input
The general approach of the studies in this group is to first predict the most likely meaning
of a given image by analyzing its visual content, and then generate a sentence reflecting
this meaning. All models in this category achieve this using the following general pipeline
architecture:
1. Computer vision techniques are applied to classify the scene type, to detect the objects present in the image, to predict their attributes and the relationships that hold
between them, and to recognize the actions taking place.
2. This is followed by a generation phase that turns the detector outputs into words or
phrases. These are then combined to produce a natural language description of the
image, using techniques from natural language generation (e.g., templates, n-grams,
grammar rules).
The approaches reviewed in this section perform an explicit mapping from images to
descriptions, which differentiates them from the studies described in Section 2.2 and 2.3,
which incorporate implicit vision and language models. An illustration of a sample model is
shown in Figure 1. An explicit pipeline architecture, while tailored to the problem at hand,
constrains the generated descriptions, as it relies on a predefined sets of semantic classes of
scenes, objects, attributes, and actions. Moreover, such an architecture crucially assumes
412

fiAutomatic Description Generation from Images: A Survey

Reference

Generation

Farhadi et al. (2010)
Kulkarni et al. (2011)
Li et al. (2011)
Ordonez et al. (2011)
Yang et al. (2011)
Gupta et al. (2012)
Kuznetsova et al. (2012)
Mitchell et al. (2012)
Elliott and Keller (2013)
Hodosh et al. (2013)
Gong et al. (2014)
Karpathy et al. (2014)
Kuznetsova et al. (2014)
Mason and Charniak (2014)
Patterson et al. (2014)
Socher et al. (2014)
Verma and Jawahar (2014)
Yatskar et al. (2014)
Chen and Zitnick (2015)
Donahue et al. (2015)
Devlin et al. (2015)
Elliott and de Vries (2015)
Fang et al. (2015)
Jia et al. (2015)
Karpathy and Fei-Fei (2015)
Kiros et al. (2015)
Lebret et al. (2015)
Lin et al. (2015)
Mao et al. (2015a)
Ortiz et al. (2015)
Pinheiro et al. (2015)
Ushiku et al. (2015)
Vinyals et al. (2015)
Xu et al. (2015)
Yagcioglu et al. (2015)

Retrieval from
Visual Space Multimodal Space
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X

X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X

X
X
X

Table 1: An overview of existing approaches to automatic image description. We have
categorized the literature into approaches that directly generate a description of an image
(Section 2.1), approaches that retrieve images via visual similarity and transfer their description to the new image (Section 2.2), and approaches that frame the task as retrieving
descriptions and images from a multimodal space (Section 2.3).

413

fiBernardi et al.

Figure 1: The automatic image description generation system proposed by Kulkarni et al.
(2011).
the accuracy of the detectors for each semantic class, an assumption that is not always met
in practice.
Approaches to description generation differ along two main dimensions: (a) which image
representations they derive descriptions from, and (b) how they address the sentence generation problem. In terms of the representations used, existing models have conceptualized
images in a number of different ways, relying on spatial relationships (Farhadi et al., 2010),
corpus-based relationships (Yang et al., 2011), or spatial and visual attributes (Kulkarni
et al., 2011). Another group of papers utilizes an abstract image representation in the
form of meaning tuples which capture different aspects of an image: the objects detected,
the attributes of those detections, the spatial relations between them, and the scene type
(Farhadi et al., 2010; Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Mitchell et al.,
2012). More recently, Yatskar et al. (2014) proposed to generate descriptions from denselylabeled images, which incorporate object, attribute, action, and scene annotations. Similar
in spirit is the work by Fang et al. (2015), which does not rely on prior labeling of objects,
attributes, etc. Rather, the authors train word detectors directly from images and their
associated descriptions using multi-instance learning (a weakly supervised approach for the
training of object detectors). The words returned by these detectors are then fed into a
language model for sentence generation, followed by a re-ranking step.
The first framework to explicitly represent how the structure of an image relates to
the structure of its description is the Visual Dependency Representations (VDR) method
proposed by Elliott and Keller (2013). A VDR captures the spatial relations between the
objects in an image in the form of a dependency graph. This graph can then be related to the
syntactic dependency tree of the description of the image.3 While initial work using VDRs
has relied on a corpus of manually annotated VDRs for training, more recent approaches
induce VDRs automatically based on the output of an object detector (Elliott & de Vries,
2015) or the labels present in abstract scenes (Ortiz et al., 2015).4 The idea of explicitly
representing image structure and using it for description generation has been picked up
3. VDRs have proven useful not only for description generation, but also for image retrieval (Elliott,
Lavrenko, & Keller, 2014).
4. Abstract scenes are schematic images, typically constructed using clip-art. They are employed to avoid
the need for an object detector, as the labels and positions of all objects are know. An example is Zitnick
and Parikhs (2013) dataset, see Section 3 for details.

414

fiAutomatic Description Generation from Images: A Survey

by Lin et al. (2015), who parse images into scene graphs, which are similar to VDRs and
represent the relations between the objects in a scene. They then generate from scene
graphs using a semantic grammar.5
Existing approaches also vary along the second dimension, viz., in how they approach
the sentence generation problem. At the one end of the scale, there are approaches that use
n-gram-based language models. Examples include the works by Kulkarni et al. (2011) and
Li et al. (2011), which both generate descriptions using n-gram language models trained on
a subset of Wikipedia. These approaches first determine the attributes and relationships
between regions in an image as regionprepositionregion triples. The n-gram language
model is then used to compose an image description that is fluent, given the language model.
The approach of Fang et al. (2015) is similar, but uses a maximum entropy language model
instead of an n-gram model to generate descriptions. This gives the authors more flexibility
in handling the output of the word detectors that are at the core of their model.
Recent image description work using recurrent neural networks (RNNs) can also be
regarded as relying on language modeling. A classical RNN is a language model: it captures
the probability of generating a given word in a string, given the words generated so far. In
an image description setup, the RNN is trained to generate the next word given not only
the string so far, but also a set of image features. In this setting, the RNN is therefore not
purely a language model (as in the case of an n-gram model, for instance), but it is a hybrid
model that relies on a representation that incorporates both visual and linguistic features.
We will return to this in more detail in Section 2.3.
A second set of approaches use sentence templates to generate descriptions. These
are (typically manually) pre-defined sentence frames in which open slots need to be filled
with labels for objects, relations, or attributes. For instance, Yang et al. (2011) fill in
a sentence template by selecting the likely objects, verbs, prepositions, and scene types
based on a Hidden Markov Model. Verbs are generated by finding the most likely pairing
of object labels in the Gigaword external corpus. The generation model of Elliott and
Keller (2013) parses an image into a VDR, and then traverses the VDRs to fill the slots
of sentence templates. This approach also performs a limited from of content selection by
learning associations between VDRs and syntactic dependency trees at training time; these
associations then allow to select the most appropriate verb for a description at test time.
Other approaches have used more linguistically sophisticated approaches to generation.
Mitchell et al. (2012) over-generate syntactically well-formed sentence fragments and then
recombine these using a tree-substitution grammar. A related approach has been pursued
by Kuznetsova et al. (2014), where tree-fragments are learnt from a training set of existing
descriptions and then these fragments are combined at test time to form new descriptions.
Another linguistically expressive model has recently been proposed by Ortiz et al. (2015).
The authors model image description as machine translation over VDRsentence pairs and
perform explicit content selection and surface realization using an integer linear program
over linguistic constraints.
The systems presented so far aimed at directly generating novel descriptions. However,
as argued by Hodosh et al. (2013), framing image description as a natural language generation (NLG) task makes it difficult to objectively evaluate the quality of novel descriptions
5. Note that graphs are also used for image retrieval by Johnson, Krishna, Stark, Li, Shamma, Bernstein,
and Fei-Fei (2015) and Schuster, Krishna, Chang, Fei-Fei, and Manning (2015).

415

fiBernardi et al.

Figure 2: The description model based on retrieval from visual space proposed by Ordonez
et al. (2011).
as it introduces a number of linguistic difficulties that detract attention from the underlying image understanding problem (Hodosh et al., 2013). At the same time, evaluation
of generation systems is known to be difficult (Reiter & Belz, 2009). Hodosh et al. therefore propose an approach that makes it possible to evaluate the mapping between images
and sentences independently of the generation aspect. Models that follow this approach
conceptualize image description as a retrieval problem: they associate an image with a
description by retrieving and ranking a set of similar images with candidate descriptions.
These candidate descriptions can then either be used directly (description transfer) or a
novel description can be synthesized from the candidates (description generation).
The retrieval of images and ranking of their descriptions can be carried out in two ways:
either from a visual space or from a multimodal space that combines textual and visual
information space. In the following subsections, we will survey work that follows these two
approaches.
2.2 Description as a Retrieval in Visual Space
The studies in this group pose the problem of automatically generating the description
of an image by retrieving images similar to the query image (i.e., the new image to be
described); this is illustrated in Figure 2. In other words, these systems exploit similarity
in the visual space to transfer descriptions to the query images. Compared to models that
generate descriptions directly (Section 2.1), retrieval models typically require a large amount
of training data in order to provide relevant descriptions.
In terms of their algorithmic components, visual retrieval approaches typically follow a
pipeline of three main steps:
1. Represent the given query image by specific visual features.
2. Retrieve a candidate set of images from the training set based on a similarity measure
in the feature space used.
3. Re-rank the descriptions of the candidate images by further making use of visual
and/or textual information contained in the retrieval set, or alternatively combine
fragments of the candidate descriptions according to certain rules or schemes.
One of the first model to follow this approach was the Im2Text model of Ordonez et al.
(2011). GIST (Oliva & Torralba, 2001) and Tiny Image (Torralba, Fergus, & Freeman, 2008)
416

fiAutomatic Description Generation from Images: A Survey

descriptors are employed to represent the query image and to determine the visually similar
images in the first retrieval step. Most of the retrieval-based models consider the result of
this step as a baseline. For the re-ranking step, a range of detectors (e.g., object, stuff,
pedestrian, action detectors) and scene classifiers specific to the entities mentioned in the
candidate descriptions are first applied to the images to better capture their visual content,
and the images are represented by means of these detector and classifier responses. Finally,
the re-ranking is carried out via a classifier trained over these semantic features.
The model proposed by Kuznetsova et al. (2012) first runs the detectors and the classifiers used in the re-ranking step of the Im2Text model on a query image to extract and
represent its semantic content. Then, instead of performing a single retrieval by combining
the responses of these detectors and classifiers as the Im2Text model does, it carries out a
separate image retrieval step for each visual entity present in the query image to collect related phrases from the retrieved descriptions. For instance, if a dog is detected in the given
image, then the retrieval process returns the phrases referring to visually similar dogs in the
training set. More specifically, this step is used to collect three different kinds of phrases.
Noun and verb phrases are extracted from descriptions in the training set based on the
visual similarity between object regions detected in the training images and in the query
image. Similarly, prepositional phrases are collected for each stuff detection in the query
image by measuring the visual similarity between the detections in the query and training
images based on their appearance and geometric arrangements. Prepositional phrases are
additionally collected for each scene context detection by measuring the global scene similarity computed between the query and training images. Finally, a description is generated
from these collected phrases for each detected object via integer linear programming (ILP)
which considers factors such as word ordering, redundancy, etc.
The method of Gupta et al. (2012) is another phrase-based approach. To retrieve
visually similar images, the authors employ simple RGB and HSV color histograms,
Gabor and Haar descriptors, GIST and SIFT (Lowe, 2004) descriptors as image features. Then, instead of using visual object detectors or scene classifiers, they rely only
on the textual information in the descriptions of the visually similar images to extract
the visual content of the input image. Specifically, the candidate descriptions are segmented into phrases of a certain type such as (subject, verb), (subject, prep, object),
(verb, prep, object), (attribute, object), etc. Those that best describe the input image are determined according to a joint probability model based on image similarity and Google search counts, and the image is represented by triplets of the form
{((attribute1, object1), verb), (verb, prep, (attribute2, object2)), (object1, prep, object2)}. In
the end, the description is generated using the three top-scoring triplets based on a fixed
template. To increase the quality of the descriptions, the authors also apply syntactic
aggregation and some subject and predicate grouping rules before the generation step.
Patterson et al. (2014) were the first to present a large-scale scene attribute dataset
in the computer vision community. The dataset includes 14,340 images from 707 scene
categories, which are annotated with certain attributes from a list of 102 discriminative
attributes related to materials, surface properties, lighting, affordances, and spatial layout.
This allows them to train attribute classifiers from this dataset. In their paper, the authors
also demonstrate that the responses of these attribute classifiers can be used as a global
image descriptor which captures the semantic content better than the standard global image
417

fiBernardi et al.

descriptors such as GIST. As an application, they extended the baseline model of Im2Text
by replacing the global features with automatically extracted scene attributes, giving better
image retrieval and description results.
Mason and Charniaks (2014) description generation approach differs from the models
discussed above in that it formulates description generation as an extractive summarization
problem, and it selects the output description by considering only the textual information
in the final re-ranking step. In particular, the authors represented images by using the scene
attributes descriptor of Patterson et al. (2014). Once the visually similar images are identified from the training set, in the next step, the conditional probabilities of observing a word
in the description of the query image are estimated via non-parametric density estimation
using the descriptions of the retrieved images. The final output description is then determined by using two different extractive summarization techniques, one depending on the
SumBasic model (Nenkova & Vanderwende, 2005) and the other based on Kullback-Leibler
divergence between the word distributions of the query and the candidate descriptions.
Yagcioglu et al. (2015) proposed an average query expansion approach which is based on
compositional distributed semantics. To represent images, they use features extracted from
the recently proposed Visual Geometry Group convolutional neural network (VGG-CNN;
Chatfield, Simonyan, Vedaldi, & Zisserman, 2014). These features are the activations of
the last layer of a deep neural network trained on ImageNet, which have been proven to
be effective in many computer vision problems. Then, the original query is expanded as
the average of the distributed representations of retrieved descriptions, weighted by their
similarity to the input image.
The approach of Devlin et al. (2015) also utilizes CNN activations as the global image
descriptor and performs k-nearest neighbor retrieval to determine the images from the
training set that are visually similar to the query image. It then selects a description
from the candidate descriptions associated with the retrieved images that best describes
the images that are similar to the query image, just like the approaches by Mason and
Charniak (2014) and Yagcioglu et al. (2015). Their approach differs in terms of how they
represent the similarity between description and how they select the best candidate over
the whole set. Specifically, they propose to compute the description similarity based on
the n-gram overlap F-score between the descriptions. They suggest to choose the output
description by finding the description that corresponds to the description with the highest
mean n-gram overlap with the other candidate descriptions (k-nearest neighbor centroid
description) estimated via an n-gram similarity measure.
2.3 Description as a Retrieval in Multimodal Space
The third group of studies casts image description generation again as a retrieval problem,
but from a multimodal space (Hodosh et al., 2013; Socher et al., 2014; Karpathy et al., 2014).
The intuition behind these models is illustrated in Figure 3, and the overall approach can
be characterized as follows:

1. Learn a common multimodal space for the visual and textual data using a training
set of imagedescription pairs.
418

fiAutomatic Description Generation from Images: A Survey

2. Given a query, use the joint representation space to perform cross-modal (image
sentence) retrieval.

Figure 3: Image descriptions as a retrieval task as proposed in the works by Hodosh et al.
(2013), Socher et al. (2014), and Karpathy et al. (2014)6 .
In contrast to the retrieval models that work on a visual space (Section 2.2), where
unimodal image retrieval is followed by ranking of the retrieved descriptions, here image
and sentence features are projected into a common multimodal space. Then, the multimodal
space is used to retrieve descriptions for a given image. The advantage of this approach is
that it allows bi-directional models, i.e., the common space can also be used for the other
direction, retrieving the most appropriate image for a query sentence.
In this section, we first discuss the seminal paper of Hodosh et al. (2013) on description
retrieval, and then present more recent approaches that combine a retrieval approach with
some form of natural language generation. Hodosh et al. map both images and sentences
into a common space. The joint space can be used for both image search (find the most
plausible image given a sentence) and image annotation (find the sentence that describes
the image well), see Figure 3. In an earlier study the authors proposed to learn a common meaning space (Farhadi et al., 2010) consisting of a triple representation of the form
hobject, action, scenei. The representation was thus limited to a set of pre-defined discrete
slot fillers, which was given as training information. Instead, Hodosh et al. use KCCA, a
kernelized version of CCA, Canonical Correlation Analysis (Hotelling, 1936), to learn the
joint space. CCA takes a training dataset of image-sentence pairs, i.e., Dtrain = {hi, si},
thus input from two different feature spaces, and finds linear projections into a newly induced common space. In KCCA, kernel functions map the original items into higher-order
space in order to capture the patterns needed to associate image and text. KCCA has been
shown previously to be successful in associating images (Hardoon, Szedmak, & ShaweTaylor, 2004) or image regions (Socher & Fei-Fei, 2010) with individual words or set of
tags.
Hodosh et al. (2013) compare their KCCA approach to a nearest-neighbor (NN) baseline
that uses unimodal text and image spaces, without constructing a joint space. A drawback of
KCCA is that it is only applicable to smaller datasets, as it requires the two kernel matrices
6. Source http://nlp.cs.illinois.edu/HockenmaierGroup/Framing_Image_Description/

419

fiBernardi et al.

to be kept in memory during training. This becomes prohibitive for very large datasets.
Some attempts have been made to circumvent the computational burden of KCCA, e.g.,
by resorting to linear models (Hodosh & Hockenmaier, 2013). Alternatively, Sun, Gan, and
Nevatia (2015) have used automatically discovered concepts from images to form a semantic
space, and performed sentence retrieval accordingly. However, recent work on description
retrieval has instead utilized neural networks to construct a joint space for image description
generation.
Socher et al. (2014) use neural networks for building sentence and image vector representations that are then mapped into a common embedding space. A novelty of their
work is that they use compositional sentence vector representations. First, image and word
representations are learned in their single modalities, and finally mapped into a common
multimodal space. In particular, they use a DT-RNN (Dependency Tree Recursive Neural
Network) for composing language vectors to abstract over word order and syntactic difference that are semantically irrelevant. This results in 50-dimensional word embeddings. For
the image space, the authors use a nine layer neural network trained on ImageNet data,
using unsupervised pre-training. Image embeddings are derived by taking the output of the
last layer (4,096 dimensions). The two spaces are then projected into a multi-modal space
through a max-margin objective function that intuitively trains pairs of correct image and
sentence vectors to have a high inner product. The authors show that their model outperforms previously used KCCA approaches such as the work by Hodosh and Hockenmaier
(2013).
Karpathy et al. (2014) extend the previous multi-modal embeddings model. Rather
than directly mapping entire images and sentences into a common embedding space, their
model embeds more fine-grained units, i.e., fragments of images (objects) and sentences
(dependency tree fragments), into a common space. Their final model integrates both global
(sentence and image-level) as well as finer-grained information and outperforms previous
approaches, such as DT-RNN (Socher et al., 2014). A similar approach is pursued by
Pinheiro et al. (2015), who propose a bilinear phrase-based model that learns a mapping
between image representations and sentences. A constrained language model is then used
to generate from this representation. A conceptually related approach is pursued by Ushiku
et al. (2015): the authors use a common subspace model which maps all feature vectors
associated with the same phrase into nearby regions of the space. For generation, a beamsearch based decoder or templates are used.
Description generation systems are difficult to evaluate, therefore the studies reviewed
above treat the problem as a retrieval and ranking task (Hodosh et al., 2013; Socher et al.,
2014). While such an approach has been valuable because it enables comparative evaluation,
retrieval and ranking is limited by the availability of existing datasets with descriptions. To
alleviate this problem, recent models have been developed that are extensions of multimodal
spaces; they are able to not only rank sentences, but can also generate them (Chen & Zitnick,
2015; Donahue et al., 2015; Karpathy & Fei-Fei, 2015; Kiros et al., 2015; Lebret et al., 2015;
Mao et al., 2015a; Vinyals et al., 2015; Xu et al., 2015).
Kiros et al. (2015) introduced a general encoder-decoder framework for image description
ranking and generation, illustrated in Figure 4. Intuitively the method works as follows.
The encoder first constructs a joint multimodal space. This space can be used to rank
images and descriptions. The second stage (decoder) then uses the shared multimodal
420

fiAutomatic Description Generation from Images: A Survey

Figure 4: The encoder-decoder model proposed by Kiros et al. (2015).

representation to generate novel descriptions. Their model, directly inspired by recent
work in machine translation, encodes sentences using a LongShort Term Memory (LSTM)
recurrent neural network, and image features using a deep convolutional network (CNN).
LSTM is an extension of the recurrent neural network (RNN) that incorporates builtin memory to store information and exploit long range context. In Kiros et al.s (2015)
encoder-decoder model, the vision space is projected into the embedding space of the LSTM
hidden states; a pairwise ranking loss is minimized to learn the ranking of images and their
descriptions. The decoder, a neural-network-based language model, is able to generate novel
descriptions from this multimodal space.
Another work that has been carried out at the same time and is similar to the latter is
described in the paper by Donahue et al. (2015). The authors propose a model that is also
based on the LSTM neural architecture. However, rather than projecting the vision space
into the embedding space of the hidden states, the model takes a copy of the static image
and the previous word directly as input, that is then fed to a stack of four LSTMs. Another
LSTM-based model is proposed by Jia et al. (2015), who added semantic image information
as additional input to the LSTM. The model by Kiros et al. (2015) outperforms the prior
DT-RNN model (Socher et al., 2014); in turn, Donahue et al. report that they outperform
the work of Kiros et al. (2015) on the task of image description retrieval. Subsequent work
includes the RNN-based architectures by Mao et al. (2015a) and Vinyals et al. (2015),
who are very similar to the one proposed by Kiros et al. (2015) and achieve comparable
results on standard datasets. Mao, Wei, Yang, Wang, Huang, and Yuille (2015b) propose an
interesting extension of Mao et al.s (2015a) model for the learning of novel visual concepts.
Karpathy and Fei-Fei (2015) improve on previous models by proposing a deep visualsemantic alignment model with a simpler architecture and objective function. Their key
insight is to assume that parts of the sentence refer to particular but unknown regions in the
image. Their model tries to infer the alignments between segments of sentences and regions
of images and is based on convolutional neural networks over image regions, bidirectional
RNN over sentences and a structured objective that aligns the two modalities. Words
and image regions are mapped into a common multimodal embedding. The multimodal
recurrent neural network architecture uses the inferred alignments to learn and generate
421

fiBernardi et al.

novel descriptions. Here, the image is used as condition for the first state in the recurrent
neural network, which then generates image descriptions.
Another model that can generate novel sentences is proposed in (Chen & Zitnick, 2015).
In contrast to the previous work, their model dynamically builds a visual representation of
the scene as a description is being generated. That is, a word is read or generated and the
visual representation is updated to reflect the new information. They accomplish this with
a simple RNN. The model achieves comparable or better results than most prior studies,
except for the recently proposed deep visual-semantic alignment model (Karpathy & Fei-Fei,
2015). The model of Xu et al. (2015) is closely related in that it also uses an RNN-based
architecture in which the visual representations are dynamically updated. Xu et al.s (2015)
model incorporates an attentional component, which gives it a way of determining which
regions in an image are salient, and it can focus its description on those regions. While
resulting in an improvement in description accuracy, it also makes it possible to analyze
model behavior by visualizing the regions that were attended to during each word that was
generated by the model.
The general RNN-based ranking and generation approach is also followed by Lebret
et al. (2015). Here, the main innovation is on the linguistic side: they employ a bilinear
model to learn a common space of image features and syntactic phrases (noun phrases, verb
phrases, and prepositional phrases). A Markov model is then utilized to generate sentences
from these phrase embedding. On the visual side, standard CNN-based features are used.
This results in an elegant modeling framework, whose performance is broadly comparable
to the state of the art.
Finally, two important directions that are less explored are: portability and weakly supervised learning. Verma and Jawahar (2014) evaluate the portability of a bi-directional
model based on topic models, showing that performance significantly degrades. They highlight the importance of cross-dataset image description retrieval evaluation. Another interesting observation is that all of the above models require a training set of fully-annotated
image-sentence pairs. However, obtaining such data in large quantities is prohibitively expensive. Gong et al. (2014) propose an approach based on weak supervision that transfers
knowledge from millions of weakly annotated images to improve the accuracy of description
retrieval.
2.4 Comparison of Existing Approaches
The discussion in the previous subsections makes it clear that each approach to image
description has its particular strengths and weaknesses. For example, the methods that
cast the task as a generation problem (Section 2.1) have an advantage over other types of
approaches in that they can produce novel sentences to describe a given image. However,
their success relies heavily on how accurately they estimate the visual content and how
well they are able to verbalize this content. In particular, they explicitly employ computer
vision techniques to predict the most likely meaning of a given image; these methods have
limited accuracy in practice, hence if they fail to identify the most important objects and
their attributes, then no valid description can be generated. Another difficulty lies in the
final description generation step; sophisticated natural language generation is crucial to
422

fiAutomatic Description Generation from Images: A Survey

guarantee fluency and grammatical correctness of the generated sentences. This can come
at the price of considerable algorithmic complexity.
In contrast, image description methods that cast the problem as a retrieval from a
visual space problem and transfer the retrieved descriptions to a novel image (Section 2.2)
always produce grammatically correct descriptions. This is guaranteed by design, as these
systems fetch human-generated sentences from visually similar images. The main issue with
this approach is that it requires large amounts of images with human-written descriptions.
That is, the accuracy (but not the grammaticality) of the descriptions reduces as the size
of the training set decreases. The training set also needs to be diverse (in addition to being
large), in order for visual retrieval-based approaches to produce image descriptions that are
adequate for novel test images (Devlin et al., 2015). Though this problem can be mitigated
by re-synthesizing a novel description from the retrieved ones (see Section 2.2).
Approaches that cast image description as a retrieval from a multimodal space problem
(Section 2.3) also have the advantage of generating human-like descriptions as they are
able to retrieve the most appropriate ones from a pre-defined large pool of descriptions.
However, ranking these descriptions requires a cross-modal similarity metric that compares
images and sentences. Such metrics are difficult to define, compared to the unimodal
image-to-image similarity metrics used by retrieval models that work on a visual space.
Additionally, training a common space for images and sentences requires a large training
set of images annotated with human-generated descriptions. On the plus side, such a
multimodal embedding space can also be used for the reverse problem, i.e., for retrieving
the most appropriate image for a query sentence. This is something generation-based or
visual retrieval-based approaches are not capable of.

3. Datasets and Evaluation
There is a wide range of datasets for automatic image description research. The images in
these datasets are associated with textual descriptions and differ from each other in certain
aspects such as in size, the format of the descriptions and in how the descriptions were collected. Here we review common approaches for collecting datasets, the datasets themselves,
and evaluation measures for comparing generated descriptions with ground-truth texts. The
datasets are summarized in Table 2, and examples of images and descriptions are given in
Figure 5. The readers can also refer to the dataset survey by Ferraro, Mostafazadeh, Huang,
Vanderwende, Devlin, Galley, and Mitchell (2015) for an analysis similar to ours. It provides
a basic comparison of some of the existing language and vision datasets. It is not limited
to automatic image description, and it reports some simple statistics and quality metrics
such as perplexity, syntactic complexity, and abstract to concrete word ratios.
3.1 Image-Description Datasets
The Pascal1K sentence dataset (Rashtchian et al., 2010) is a dataset which is commonly
used as a benchmark for evaluating the quality of description generation systems. This
medium-scale dataset, consists of 1,000 images that were selected from the Pascal 2008
object recognition dataset (Everingham, Van Gool, Williams, Winn, & Zisserman, 2010)
and includes objects from different visual classes, such as humans, animals, and vehicles.
423

fiBernardi et al.

Images

Texts

Judgments

Objects

Pascal1K (Rashtchian et al., 2010)
VLT2K (Elliott & Keller, 2013)
Flickr8K (Hodosh & Hockenmaier, 2013)
Flickr30K (Young et al., 2014)
Abstract Scenes (Zitnick & Parikh, 2013)
IAPR-TC12 (Grubinger et al., 2006)
MS COCO (Lin et al., 2014)

1,000
2,424
8,108
31,783
10,000
20,000
164,062

5
3
5
5
6
15
5

No
Partial
Yes
No
No
No
Collected

Partial
Partial
No
No
Complete
Segmented
Partial

BBC News (Feng & Lapata, 2008)
SBU1M Captions (Ordonez et al., 2011)
Deja-Image Captions (Chen et al., 2015)

3,361
1,000,000
4,000,000

1
1
Varies

No
Collected7
No

No
No
No

Table 2: Image datasets for the automatic description generation models. We have split
the overview into image description datasets (top) and caption datasets (bottom)  see the
main text for an explanation of this distinction.
Each image is associated with five descriptions generated by humans on Amazon Mechanical
Turk (AMT) service.
The Visual and Linguistic Treebank (VLT2K; Elliott & Keller, 2013) makes use of images
from the Pascal 2010 action recognition dataset. It augments these images with three, twosentence descriptions per image. These descriptions were collected on AMT with specific
instructions to verbalize the main action depicted in the image and the actors involved (first
sentence), while also mentioning the most important background objects (second sentence).
For a subset of 341 images of the Visual and Linguistic Treebank, object annotation is
available (in the form of polygons around all objects mentioned in the descriptions). For
this subset, manually created Visual Dependency Representations (see Section 2.1) are also
included (three VDRs per images, i.e., a total of 1023).
The Flickr8K dataset (Hodosh et al., 2013) and its extended version Flickr30K
dataset (Young et al., 2014) contain images from Flickr, comprising approximately 8,000
and 30,000 images, respectively. The images in these two datasets were selected through
user queries for specific objects and actions. These datasets contain five descriptions per image which were collected from AMT workers using a strategy similar to that of the Pascal1K
dataset.
The Abstract Scenes dataset (Zitnick & Parikh, 2013; Zitnick, Parikh, & Vanderwende,
2013) consists of 10,000 clip-art images and their descriptions. The images were created
through AMT, where workers were asked to place a fixed vocabulary of 80 clip-art objects
into a scene of their choosing. The descriptions were then sourced for these worker-created
scenes. The authors provided these descriptions in two different forms. While the first
group contains a single sentence description for each image, the second group includes two
alternative descriptions per image. Each of these two descriptions consist of three simple
sentences with each sentence describing a different aspect of the scene. The main advantage
of this dataset is it affords the opportunity to explore image description generation without
7. Kuznetsova et al. (2014) ran a human judgments study on 1,000 images from this dataset.

424

fiAutomatic Description Generation from Images: A Survey

1. One jet lands at an airport while another takes off
next to it.
2. Two airplanes parked in an airport.
3. Two jets taxi past each other.
4. Two parked jet airplanes facing opposite directions.
5. two passenger planes on a grassy plain

1. There are several people in chairs and a small child
watching one of them play a trumpet
2. A man is playing a trumpet in front of a little boy.
3. People sitting on a sofa with a man playing an
instrument for entertainment.

(a) Pascal1K8

(b) VLT2K9

1. A man is snowboarding over a structure on a snowy
hill.
2. A snowboarder jumps through the air on a snowy
hill.
3. a snowboarder wearing green pants doing a trick
on a high bench
4. Someone in yellow pants is on a ramp over the
snow.
5. The man is performing a trick on a snowboard high
in the air.

1. a yellow building with white columns in the background
2. two palm trees in front of the house
3. cars are parking in front of the house
4. a woman and a child are walking over the square

(c) Flickr8K10

(d) IAPR-TC1211

1. A cat anxiously sits in the park and stares at an
unattended hot dog that someone has left on a
yellow bench

1. A blue smart car parked in a parking lot.
2. Some vehicles on a very wet wide city street.
3. Several cars and a motorcycle are on a snow covered street.
4. Many vehicles drive down an icy street.
5. A small smart car driving in the city.

(e) Abstract Scenes12

(f) MS COCO13

Figure 5: Example images and descriptions from the benchmark image datasets.

425

fiBernardi et al.

the need for automatic object recognition, thus avoiding the associated noise. A more
recent version of this dataset has been created as a part of the visual question-answering
(VQA) dataset (Antol, Agrawal, Lu, Mitchell, Batra, Zitnick, & Parikh, 2015). It contains
50,000 different scene images with more realistic human models and with five single-sentence
descriptions.
The IAPR-TC12 dataset introduced by Grubinger et al. (2006) is one of the earliest
multi-modal datasets and contains 20,000 images with descriptions. The images were originally retrieved via search engines such as Google, Bing and Yahoo, and the descriptions
were produced in multiple languages (predominantly English and German). Each image is
associated with one to five descriptions, where each description refers to a different aspect
of the image, where applicable. The dataset also contains complete pixel-level segmentation
of the objects.
The MS COCO dataset (Lin et al., 2014) currently consists of 123,287 images with five
different descriptions per image. Images in this dataset are annotated for 80 object categories, which means that bounding boxes around all instances in one of these categories
are available for all images. The MS COCO dataset has been widely used for image description, something that is facilitated by the standard evaluation server that has recently
become available14 . Extensions of MS COCO are currently under development, including
the addition of questions and answers (Antol et al., 2015).
One paper (Lin et al., 2015) uses an the NYU dataset (Silberman, Kohli, Hoiem, &
Fergus, 2012), which contains 1,449 indoor scenes with 3D object segmentation. This
dataset has been augmented with five descriptions per image by Lin et al.
3.2 Image-Caption Datasets
Image descriptions verbalize what can be seen in the image, i.e., they refer to the objects,
actions, and attributes depicted, mention the scene type, etc. Captions, on the other hand,
are typically texts associated with images that verbalize information that cannot be seen
in the image. A caption provides personal, cultural, or historical context for the image
(Panofsky, 1939). Images shared through social networking or photo-sharing websites can
be accompanied by descriptions or captions, or a mixtures of both types of text. The images
in a newspaper or a museum will typically contain cultural or historical texts, i.e., captions
not descriptions.
The BBC News dataset (Feng & Lapata, 2008) was one of the earliest collections of
images and co-occurring texts. Feng and Lapata (2008) harvested 3,361 news articles from
the British Broadcasting Corporation News website, with the constraint that the article
includes an image and a caption.
8.
9.
10.
11.
12.

Source http://nlp.cs.illinois.edu/HockenmaierGroup/pascal-sentences/index.html
Source http://github.com/elliottd/vlt
Source https://illinois.edu/fb/sec/1713398
Source http://imageclef.org/photodata
Source http://research.microsoft.com/en-us/um/people/larryz/clipart/SemanticClassesRender
/Classes_v1.html
13. Source http://mscoco.org/explore
14. Source http://mscoco.org/dataset/#captions-eval

426

fiAutomatic Description Generation from Images: A Survey

The SBU1M Captions dataset introduced by Ordonez et al. (2011) differs from the
previous datasets in that it is a web-scale dataset containing approximately one million
captioned images. It is compiled from data available on Flickr with user-provided image
descriptions. The images were downloaded and filtered from Flickr with the constraint that
an image contained at least one noun and one verb on predefined control lists. The resulting
dataset is provided as a CSV file of URLs.
The Deja-Image Captions dataset (Chen et al., 2015) contains 4,000,000 images with
180,000 near-identical captions harvested from Flickr. 760 million images were downloaded
from Flickr during the calendar year 2013 using a set of 693 nouns as queries. The image
captions are normalized through lemmatization and stop word removal to create a corpus
of the near-identical texts. For instance, the sentences the bird flies in blue sky and a bird
flying into the blue sky were normalized to bird fly IN blue sky (Chen et al., 2015). Image
caption pairs are retained if the captions are repeated by more than one user in normalized
form.

3.3 Collecting Datasets
Collecting new imagetext datasets is typically performed through crowd-sourcing or harvesting data from the web. The images for these datasets have either been sourced from
an existing task in the computer vision community  the Pascal challenge (Everingham
et al., 2010) was used to the Pascal1K and VLT2K datasets  directly from Flickr, in the
case of Flickr8K/30K, MS COCO, SBU1M Captions, and Deja-Image Captions datasets, or
crowdsourced, in the case of the Abstract Scenes dataset. The texts in imagedescription
datasets are usually crowd-sourced from Amazon Mechanical Turk or Crowdflower; whereas
the texts in imagecaption datasets have been harvested from photo-sharing sites, such as
Flickr, or from news providers. Captions are usually collected without financial incentive
because they are written by the people sharing their own images, or by journalists.
Crowd-sourcing the descriptions of images involves defining a simple task that can be
performed by untrained workers. Examples of the task guidelines used by Hodosh et al.
(2013) and Elliott and Keller (2013) are given in Figure 6. In both instances, care was taken
to clearly inform the potential workers about the expectations for the task. In particular,
explicit instructions were given on how the descriptions should be written, and examples of
good texts were provided. In addition, Hodosh et al. provided more extensive examples to
explain what would constitute unsatisfactory texts. Further options are available to control
the quality of the collected texts: a minimum performance rate for workers is a common
choice; and a pre-task selection quiz may be used to determine whether workers have a
sufficient grasp of the English language (Hodosh et al., 2013).
The issue of remuneration for crowd-sourced workers is controversial, and higher payments do not always lead to better quality in a crowd-sourced environment (Mason & Watts,
2009). Rashtchian et al. (2010) paid $0.01/description, Elliott and Keller (2013) paid $0.04
for an average of 67 seconds of work to produce a two-sentence description. To the best of
our knowledge, such information is not available for the other datasets.
427

fiBernardi et al.

(a) Mechanical Turk Interface used to collect Flickr8K dataset15 .

(b) Mechanical Turk Interface used to collect VLT2K dataset.

Figure 6: Examples of Mechanical Turk interfaces for collecting descriptions.

3.4 Evaluation Measures
Evaluating the output of a natural language generation (NLG) system is a fundamentally
difficult task (Dale & White, 2007; Reiter & Belz, 2009). The most common way to assess
the quality of automatically generated texts is the subjective evaluation by human experts.
15. Source Appendix of the work by Hodosh et al. (2013)

428

fiAutomatic Description Generation from Images: A Survey

NLG-produced text is typically judged in terms of grammar and content, indicating how
syntactically correct and how relevant the text is, respectively. Fluency of the generated
text is sometimes tested as well, especially when a surface realization technique is involved
during the generation process. Automatically generated descriptions for images can be
evaluated using the same NLG techniques. Typically, judges are provided with the image
as well as with the description during evaluation tasks. Subjective human evaluations of
machine generated image descriptions are often performed on Mechanical Turk with the help
of questions. So far, the following Likert-scale questions have been used to test datasets
and user groups of various sizes.
 The description accurately describes the image (Kulkarni et al., 2011; Li et al., 2011;
Mitchell et al., 2012; Kuznetsova et al., 2012; Elliott & Keller, 2013; Hodosh et al.,
2013).
 The description is grammatically correct (Yang et al., 2011; Mitchell et al., 2012;
Kuznetsova et al., 2012; Elliott & Keller, 2013, inter alia).
 The description has no incorrect information (Mitchell et al., 2012).
 The description is relevant for this image (Li et al., 2011; Yang et al., 2011).
 The description is creatively constructed (Li et al., 2011).
 The description is human-like (Mitchell et al., 2012).
Another approach for evaluating descriptions is to use automatic measures, such as
BLEU (Papineni, Roukos, Ward, & Zhu, 2002), ROUGE (Lin & Hovy, 2008), Translation
Error Rate (Feng & Lapata, 2013), Meteor (Denkowski & Lavie, 2014), or CIDEr (Vedantam, Lawrence Zitnick, & Parikh, 2015). These measures were originally developed to evaluate the output of machine translation engines or text summarization systems, with the
exception of CIDEr, which was developed specifically for image description evaluation. All
these measures compute a score that indicates the similarity between the system output and
one or more human-written reference texts (e.g., ground truth translations or summaries).
This approach to evaluation has been subject to much discussion and critique (Kulkarni
et al., 2011; Hodosh et al., 2013; Elliott & Keller, 2014). Kulkarni et al. found weakly
negative or no correlation between human judgments and unigram BLEU on the Pascal 1K
Dataset (Pearsons  = -0.17 and 0.05). Hodosh et al. studied the Cohens  correlation of
expert human judgments and binarized unigram BLEU and unigram ROUGE of retrieved
descriptions on the Flickr8K dataset. They found the best agreement between humans and
BLEU ( = 0.72) or ROUGE ( = 0.54) when the system retrieved the sentences originally associated with the images. Agreement dropped when only one reference sentence
was available, or when the reference sentences were disjoint from the proposal sentences.
They concluded that neither measure was appropriate for image description evaluation and
subsequently proposed imagesentence ranking experiments, discussed in more detail below. Elliott and Keller analyzed the correlation between human judgments and automatic
evaluation measures for retrieved and system-generated image descriptions in the Flickr8K
and VLT2K datasets. They showed that sentence-level unigram BLEU, which at that point
429

fiBernardi et al.

in time was the de facto standard measure for image description evaluation, is only weakly
correlated with human judgments. Meteor (Banerjee & Lavie, 2005), a less frequently used
translation evaluation measure, exhibited the highest correlation with human judgments.
However, Kuznetsova et al. (2014) found that unigram BLEU was more strongly correlated
with human judgments than Meteor for image caption generation.
The first large-scale image description evaluation took place during the MS COCO
Captions Challenge 2015,16 featuring 15 teams with a dataset of 123,716 training images
and 41,000 images in a withheld test dataset. The number of reference texts for each testing
image was either five or 40, based on the insight that some measures may benefit from
larger reference sets (Vedantam et al., 2015). When automatic evaluation measures were
used, some of the image description systems outperformed a humanhuman upper bound,17
whether five or 40 reference descriptions were provided. However, none of the systems
outperformed humanhuman evaluation when a judgment elicitation task was used. Meteor
was found to be the most robust measure, with the systems beating the human text on one
and two submissions (depending on the number of references); the systems outperformed
humans seven or five times measured with CIDEr; according to ROUGE and BLEU, the
system nearly always outperformed the humans, further confirming the unsuitability of
these evaluation measures.
The models that approach the description generation problem from a cross-modal retrieval perspective (Hodosh & Hockenmaier, 2013; Hodosh et al., 2013; Socher et al., 2014;
Gong et al., 2014; Karpathy et al., 2014; Verma & Jawahar, 2014) are also able to use measures from information retrieval, such as median rank (mRank), precision at k (S@k), or
recall at k (R@k) to evaluate the descriptions they return, in addition to the text-similarity
measures reported above. This evaluation paradigm was first proposed by Hodosh et al.,
who reported high correlation with human judgments for imagesentence based ranking
evaluations.
In Table 3, we summarize all the image description approaches discussed in this survey,
and list the datasets and evaluation measures employed by each of these approaches. It
can be seen that more recent systems (starting in 2014) have converged on the use of
large description datasets (Flickr8K/30K, MS COCO) and employ evaluation measures that
perform well in terms of correlation with human judgments (Meteor, CIDEr). However, the
use of BLEU, despite its limitations, is still widespread; also the use of human evaluation
is by no means universal in the literature.

4. Future Directions
As this survey demonstrates, the CV and NLP communities have witnessed an upsurge in
interest in automatic image description systems. With the help of recent advances in deep
learning models for images and text, substantial improvements in the quality of automatically generated descriptions has been registered. Nevertheless, a series of challenges for
image description research remain. In the following, we discuss future directions that this
line of research is likely to benefit from.
16. Source http://mscoco.org/dataset/cap2015
17. Calculated by collecting an additional human-written description, which was then compared to the
reference descriptions.

430

fiAutomatic Description Generation from Images: A Survey

Reference

Approach

Farhadi et al. (2010)
Kulkarni et al. (2011)
Li et al. (2011)
Ordonez et al. (2011)
Yang et al. (2011)

MultRetrieval
Generation
Generation
VisRetrieval
Generation

Datasets

Gupta et al. (2012)
Kuznetsova et al. (2012)
Mitchell et al. (2012)
Elliott and Keller (2013)
Hodosh et al. (2013)

Pascal1K
Pascal1K
Pascal1K
SBU1M
IAPR,
Flickr8K/30K,
COCO
VisRetrieval
Pascal1K, IAPR
VisRetrieval
SBU1M
Generation
Pascal1K
Generation
VLT2K
MultRetrieval Pascal1K, Flickr8K

Gong et al. (2014)
Karpathy et al. (2014)
Kuznetsova et al. (2014)
Mason and Charniak (2014)
Patterson et al. (2014)
Socher et al. (2014)
Verma and Jawahar (2014)
Yatskar et al. (2014)
Chen and Zitnick (2015)

MultRetrieval
MultRetrieval
Generation
VisRetrieval
VisRetrieval
MultRetrieval
MultRetrieval
Generation
MultRetrieval

Donahue et al. (2015)

MultRetrieval

Devlin et al. (2015)
Elliott and de Vries (2015)
Fang et al. (2015)

VisRetrieval
Generation
Generation

Jia et al. (2015)
Generation
Karpathy and Fei-Fei (2015) MultRetrieval
Kiros et al. (2015)
Lebret et al. (2015)
Lin et al. (2015)
Mao et al. (2015a)
Ortiz et al. (2015)
Pinheiro et al. (2015)
Ushiku et al. (2015)

MultRetrieval
MultRetrieval
Generation
MultRetrieval
Generation
MultRetrieval
Generation

Vinyals et al. (2015)

MultRetrieval

Xu et al. (2015)
Yagcioglu et al. (2015)

MultRetrieval
VisRetrieval

Measures
BLEU
Human, BLEU
Human, BLEU

BLEU, ROUGE, Meteor,
CIDEr, R@k
Human, BLEU, ROUGE
Human, BLEU
Human
Human, BLEU
Human, BLEU, ROUGE,
mRank, R@k
SBU1M, Flickr30K
R@k
Flickr8K/30K, COCO
BLEU, Meteor, CIDEr
SBU1M
Human, BLEU, Meteor
SBU1M
Human, BLEU
SBU1M
BLEU
Pascal1K
mRank, R@k
IAPR, SBU1M, Pascal1K BLEU, ROUGE, P@k
Own data
Human, BLEU
Flickr8K/30K, COCO
BLEU, Meteor, CIDEr,
mRank, R@k
Flickr30K, COCO
Human, BLEU, mRank,
R@k
COCO
BLEU, Meteor
VLT2K, Pascal1K
BLEU, Meteor
COCO
Human, BLEU, ROUGE,
Meteor, CIDEr
Flickr8K/30K, COCO
BLEU, Meteor
Flickr8K/30K, COCO
BLEU, Meteor, CIDEr,
mRank, R@k
Flickr8K/30K
R@k
Flickr30K, COCO
BLEU, R@k
NYU
ROUGE
IAPR, Flickr30K, COCO BLEU, mRank, R@k
Abstract Scenes
Human, BLEU, Meteor
COCO
BLEU
Pascal1K, IAPR, SBU1M, BLEU
COCO
Pascal1K,
SBU1M, BLEU, Meteor, CIDEr,
Flickr8K/30K
mRank, R@k
Flickr8K/30K, COCO
BLEU, Meteor
Flickr8K/30K, COCO
Human, BLEU, Meteor,
CIDEr

Table 3: An overview of the approaches, datasets, and evaluation measures reviewed in this
survey and organised in chronological order.

431

fiBernardi et al.

4.1 Datasets
The earliest work on image description used relatively small datasets (Farhadi et al., 2010;
Kulkarni et al., 2011; Elliott & Keller, 2013). Recently, the introduction of Flickr30K,
MS COCO and other large datasets has enabled the training of more complex models such
as neural networks. Still, the area is likely to benefit from larger and diversified datasets
that share a common, unified, comprehensive vocabulary. Vinyals et al. (2015) argue that
the collection process and the quality of the descriptions in the datasets affect performance
significantly, and make transfer learning between datasets not as effective as expected.
They show that learning a model from MS COCO and applying it to datasets collected in
different settings such as SBU1M Captions or Pascal1K, leads to a degradation in BLEU
performance. This is surprising, since MS COCO offers a much larger amount of training
data than Pascal1K. As Vinyals et al. put it, this is largely due to the differences in
vocabulary and in the quality of descriptions. Most learning approaches are likely to suffer
from such situations. Collecting larger and comprehensive datasets and developing more
generic approaches that are capable of generating naturalistic descriptions across domains
therefore is an open challenge.
While supervised algorithms are likely to take advantage of carefully collected large
datasets, lowering the amount of supervision in exchange of access to larger unsupervised
data is also an interesting avenue for future research. Leveraging unsupervised data for
building richer representations and description models is another open research challenge
in this context.
4.2 Measures
Designing automatic measures that can mimic human judgments in evaluating the suitability of image descriptions is perhaps the most urgent need in the area of image description
(Elliott & Keller, 2014). This need can be dramatically observed at the latest evaluation results of MS COCO Challenge. According to existing measures, including the latest CIDEr
measure (Vedantam et al., 2015), several automatic methods outperform the human upper bound (this upper bound indicates how similar human descriptions are to each other).
The counterintuitive nature of this result is confirmed by the fact that when human judgments are used for evaluation, the output of even the best system is judged as worse than
a human generated description for most of the time (Fang et al., 2015). However, since
conducting human judgment experiments is costly, there is a major need for improved automatic measures that are more highly correlated with human judgments. Figure 7 plots the
Epanechnikov probability density estimate (a non-parametric optimal estimator) for BLEU,
Meteor, ROUGE, and CIDEr scores per subjective judgment in Flickr8K dataset. The human judgments were obtained from human experts (Hodosh et al., 2013). BLEU is once
again confirmed to be unable to sufficiently discriminate between the lowest three human
judgments, while Meteor and CIDEr show signs of moving towards a useful separation.
4.3 Diversity and Originality
Current algorithms often rely on direct representations of the descriptions they see at training time, making the descriptions generated at test time very similar. This results in many
432

fiAutomatic Description Generation from Images: A Survey

Human Judgement

Human Judgement
0.10

Perfect
Minor mistakes
Some aspects
No relation

0.00

0.00

0.02

0.05

0.04

0.10

0.06

0.08

0.15

Perfect
Minor mistakes
Some aspects
No relation

0

20

40

60

80

100

0

20

BLEU

40

60

80

100

Meteor
Human Judgement
Perfect
Minor mistakes
Some aspects
No relation

0

0

1

100

2

3

200

4

5

300

6

Perfect
Minor mistakes
Some aspects
No relation

400

7

Human Judgement

0.0

0.2

0.4

0.6

0.8

0.00

1.0

0.05

0.10

0.15

0.20

CIDEr

ROUGE

Figure 7: Probability density estimates of BLEU, Meteor, ROUGE, and CIDEr scores
against human judgments in the Flickr8K dataset. The y-axis shows the probability density,
and the x-axis is the score computed by the measure.

433

fiBernardi et al.

repetitions and limits the diversity of the generated descriptions, making it difficult to reach
human levels of performance. This situation has been demonstrated by Devlin et al. (2015),
who show that their best model is able to generate only 47.0% of unique descriptions. Systems that generate diverse and original descriptions that do not just repeat what is already
seen, but also infer the underlying semantics therefore remain as an open challenge. Chen
and Zitnick (2015) and related approaches take a step towards addressing such limitations
by coupling description and visual representation generation.
Jas and Parikh (2015) introduces the notion of image specificity, arguing that the domain of image descriptions is not uniform, certain images being more specific than others.
Descriptions of non-specific images tend to vary a lot as people tend to describe a nonspecific scene from different aspects. This notion and its effects to description systems and
measures should be investigated in further detail.

4.4 Further Tasks
Another open challenge is visual question-answering (VQA). While natural language
question-answering based on text has been a significant goal of NLP research for a long
time (e.g., Liang, Jordan, & Klein, 2012; Fader, Zettlemoyer, & Etzioni, 2013; Richardson, Burges, & Renshaw, 2013; Fader, Zettlemoyer, & Etzioni, 2014), answering questions
about images is a task that has recently emerged. Towards achieving this goal, Malinowski
and Fritz (2014a) propose a Bayesian framework that connects natural language questionanswering with the visual information extracted from image parts. More recently, image
question answering methods based on neural networks have been developed (Gao, Mao,
Zhou, Huang, & Yuille, 2015; Ren, Kiros, & Zemel, 2015; Malinowski, Rohrbach, & Fritz,
2015; Ma, Lu, & Li, 2016). Following this effort, several datasets on this task are being
released: DAQUAR (Malinowski & Fritz, 2014a) was compiled from scene depth images
and mainly focuses on questions about the type, quantity and color of objects; COCOQA (Ren et al., 2015) was constructed by converting image descriptions to VQA format
over a subset of images from the MS COCO dataset; the Freestyle Multilingual Image Question Answering (FM-IQA) Dataset (Gao et al., 2015), Visual Madlibs dataset (Yu, Park,
Berg, & Berg, 2015) and the VQA dataset (Antol et al., 2015), were again built for images
from MS COCO, but this time question-answer pairs are collected via human annotators
in a freestyle paradigm. Research in this emerging field is likely to flourish in the near future. The ultimate goal of VQA is to build systems that can pass the (recently developed)
Visual Turing Test by being able to answer arbitrary questions about images with the same
precision as a human observer (Malinowski & Fritz, 2014b; Geman, Geman, Hallonquist, &
Younes, 2015).
Having multilingual repositories for image description is an interesting direction to
explore. Currently, among the available benchmark datasets, only the IAPR-TC12
dataset (Grubinger et al., 2006) has multilingual descriptions (in English and German).
Future work should investigate whether transferring multimodal features between monolingual description models results in improved descriptions compared to monolingual baselines.
434

fiAutomatic Description Generation from Images: A Survey

It would be interesting to study different models and new tasks in a multilingual multimodal
setting using larger and more syntactically diverse multilingual description corpora.18
Overall, image understanding is the ultimate goal of computer vision and natural language generation is one of the ultimate goals of NLP. Image description is where these both
goals are interconnected and this topic is therefore likely to benefit from individual advances
in each of these two fields.

5. Conclusions
In this survey, we discuss recent advances in automatic image description and closely related
problems. We review and analyze a large body of the existing work by highlighting common
characteristics and differences between existing research. In particular, we categorize the
related work into three groups: (i) direct description generation from images, (i) retrieval
of images from a visual space, and (iii) retrieval of images from multimodal (joint visual
and linguistic) space. In addition, we provided a brief review of the existing corpora and
automatic evaluation measures, and discussed some future directions for vision and language
research.
Compared to traditional keyword-based image annotation (using object recognition,
attribute detection, scene labeling, etc.), automatic image description systems produce more
human-like explanations of visual content, providing a more complete picture of the scene.
Advancements in this field could lead to more intelligent artificial vision systems, which
can make inferences about the scenes through the generated grounded image descriptions
and therefore interact with their environments in a more natural manner. They could also
have a direct impact on technological applications from which visually impaired people can
benefit through more accessible interfaces.
Despite the remarkable increase in the number of image description systems in recent
years, experimental results suggest that system performance still falls short of human performance. A similar challenge lies in the automatic evaluation of systems using reference
descriptions. The measures and the tools currently in use are not sufficiently highly correlated with human judgments, indicating a need for measures that can deal with the
complexity of the image description problem adequately.

Acknowledgments
We thank the anonymous reviewers for their useful comments. This work has been partially supported by the European Commission ICT COST Action iV&L Net: The European Network on Integrating Vision and Language (IC1037). RC, AE, EE, NIC was
funded by the Scientific and Technological Research Council of Turkey (TUBITAK) research grant 113E116. FK would like to acknowledge ERC funding through starting grant
203427 Synchronous Linguistic and Visual Processing. DE was supported by ERCIM
ABCDE Fellowship 2014-23.
18. The Multimodal Translation Shared Task at the 2016 Workshop on Machine Translation will use an
English and German translated version of the Flickr30K corpora. See http://www.statmt.org/wmt16/
multimodal-task.html for more details.

435

fiBernardi et al.

References
Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C. L., & Parikh, D. (2015).
Vqa: Visual question answering. In International Conference on Computer Vision.
Banerjee, S., & Lavie, A. (2005). METEOR: An Automatic Metric for MT Evaluation with
Improved Correlation with Human Judgments. In Annual Meeting of the Association for Computational Linguistics Workshop on Intrinsic and Extrinsic Evaluation
Measures for MT and/or Summarization.
Berg, T. L., Berg, A. C., & Shih, J. (2010). Automatic attribute discovery and characterization from noisy web data. In European Conference on Computer Vision.
Chatfield, K., Simonyan, K., Vedaldi, A., & Zisserman, A. (2014). Return of the devil in the
details: Delving deep into convolutional nets. In British Machine Vision Conference.
Chen, J., Kuznetsova, P., Warren, D., & Choi, Y. (2015). Deja image-captions: A corpus of
expressive descriptions in repetition. In North American Chapter of the Association
for Computational Linguistics.
Chen, X., & Zitnick, C. L. (2015). Minds eye: A recurrent visual representation for image
caption generation. In IEEE Conference on Computer Vision and Pattern Recognition.
Dale, R., & White, M. E. (Eds.). (2007). Workshop on Shared Tasks and Comparative
Evaluation in Natural Language Generation: Position Papers.
Denkowski, M., & Lavie, A. (2014). Meteor Universal: Language Specific Translation Evaluation for Any Target Language. In Conference of the European Chapter of the Association for Computational Linguistics Workshop on Statistical Machine Translation.
Devlin, J., Cheng, H., Fang, H., Gupta, S., Deng, L., He, X., Zweig, G., & Mitchell, M.
(2015). Language Models for Image Captioning: The Quirks and What Works. In
Annual Meeting of the Association for Computational Linguistics.
Donahue, J., Hendricks, L. A., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko,
K., & Darrell, T. (2015). Long-term recurrent convolutional networks for visual recognition and description. In IEEE Conference on Computer Vision and Pattern Recognition.
Elliott, D., & de Vries, A. P. (2015). Describing images using inferred visual dependency
representations. In Annual Meeting of the Association for Computational Linguistics.
Elliott, D., & Keller, F. (2013). Image Description using Visual Dependency Representations. In Conference on Empirical Methods in Natural Language Processing.
Elliott, D., & Keller, F. (2014). Comparing Automatic Evaluation Measures for Image
Description. In Annual Meeting of the Association for Computational Linguistics.
Elliott, D., Lavrenko, V., & Keller, F. (2014). Query-by-Example Image Retrieval using
Visual Dependency Representations. In International Conference on Computational
Linguistics.
Everingham, M., Van Gool, L., Williams, C. K. I., Winn, J., & Zisserman, A. (2010). The
PASCAL Visual Object Classes (VOC) Challenge. International Journal of Computer
Vision, 88 (2), 303338.
436

fiAutomatic Description Generation from Images: A Survey

Fader, A., Zettlemoyer, L., & Etzioni, O. (2013). Paraphrase-driven learning for open question answering. In Annual Meeting of the Association for Computational Linguistics.
Fader, A., Zettlemoyer, L., & Etzioni, O. (2014). Open question answering over curated and
extracted knowledge bases. In ACM SIGKDD Conference on Knowledge Discovery
and Data Mining.
Fang, H., Gupta, S., Iandola, F., Srivastava, R., Deng, L., Dollar, P., Gao, J., He, X.,
Mitchell, M., Platt, J., Zitnick, C. L., & Zweig, G. (2015). From captions to visual
concepts and back. In IEEE Conference on Computer Vision and Pattern Recognition.
Farhadi, A., Hejrati, M., Sadeghi, M. A., Young, P., Rashtchian, C., Hockenmaier, J., &
Forsyth, D. (2010). Every picture tells a story: Generating sentences from images. In
European Conference on Computer Vision.
Felzenszwalb, P. F., Girshick, R. B., McAllester, D., & Ramanan, D. (2010). Object detection with discriminatively trained part-based models. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 32 (9), 16271645.
Feng, Y., & Lapata, M. (2008). Automatic Image Annotation Using Auxiliary Text Information. In Annual Meeting of the Association for Computational Linguistics.
Feng, Y., & Lapata, M. (2013). Automatic caption generation for news images. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 35 (4), 797812.
Ferraro, F., Mostafazadeh, N., Huang, T., Vanderwende, L., Devlin, J., Galley, M., &
Mitchell, M. (2015). A survey of current datasets for vision and language research. In
Conference on Empirical Methods in Natural Language Processing.
Gao, H., Mao, J., Zhou, J., Huang, Z., & Yuille, A. (2015). Are you talking to a machine?
dataset and methods for multilingual image question answering. In International
Conference on Learning Representations.
Geman, D., Geman, S., Hallonquist, N., & Younes, L. (2015). Visual turing test for computer
vision systems. Proceedings of the National Academy of Sciences, 112 (12), 36183623.
Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. In IEEE Conference on Computer
Vision and Pattern Recognition.
Gong, Y., Wang, L., Hodosh, M., Hockenmaier, J., & Lazebnik, S. (2014). Improving ImageSentence Embeddings Using Large Weakly Annotated Photo Collections. In European
Conference on Computer Vision.
Grubinger, M., Clough, P., Muller, H., & Deselaers, T. (2006). The IAPR TC-12 benchmark:
A new evaluation resource for visual information systems. In International Conference
on Language Resources and Evaluation.
Guadarrama, S., Krishnamoorthy, N., Malkarnenkar, G., Venugopalan, S., Mooney, R., Darrell, T., & Saenko, K. (2013). Youtube2text: Recognizing and describing arbitrary
activities using semantic hierarchies and zero-shot recognition. In International Conference on Computer Vision.
Gupta, A., Verma, Y., & Jawahar, C. V. (2012). Choosing linguistics over vision to describe
images. In AAAI Conference on Artificial Intelligence.
437

fiBernardi et al.

Hardoon, D. R., Szedmak, S., & Shawe-Taylor, J. (2004). Canonical correlation analysis:
An overview with application to learning methods. Neural Computation, 16 (12),
26392664.
Hodosh, M., & Hockenmaier, J. (2013). Sentence-based image description with scalable,
explicit models. In IEEE Conference on Computer Vision and Pattern Recognition
Workshops.
Hodosh, M., Young, P., & Hockenmaier, J. (2013). Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics. Journal of Artificial Intelligence
Research, 47, 853899.
Hotelling, H. (1936). Relations between two sets of variates. Biometrika, 0, 321377.
Jaimes, A., & Chang, S.-F. (2000). A conceptual framework for indexing visual information
at multiple levels. In IST SPIE Internet Imaging.
Jas, M., & Parikh, D. (2015). Image specificity. In IEEE Conference on Computer Vision
and Pattern Recognition.
Jia, X., Gavves, E., Fernando, B., & Tuytelaars, T. (2015). Guiding the long-short term
memory model for image caption generation. In International Conference on Computer Vision.
Johnson, J., Krishna, R., Stark, M., Li, L.-J., Shamma, D. A., Bernstein, M., & Fei-Fei, L.
(2015). Image retrieval using scene graphs. In IEEE Conference on Computer Vision
and Pattern Recognition.
Karpathy, A., & Fei-Fei, L. (2015). Deep visual-semantic alignments for generating image
descriptions. In IEEE Conference on Computer Vision and Pattern Recognition.
Karpathy, A., Joulin, A., & Fei-Fei, L. (2014). Deep Fragment Embeddings for Bidirectional
Image Sentence Mapping. In Advances in Neural Information Processing Systems.
Khan, M. U. G., Zhang, L., & Gotoh, Y. (2011). Towards coherent natural language description of video streams. In International Conference on Computer Vision Workshops.
Kiros, R., Salakhutdinov, R., & Zemel, R. S. (2015). Unifying visual-semantic embeddings
with multimodal neural language models. In Advances in Neural Information Processing Systems Deep Learning Workshop.
Krishnamoorthy, N., Malkarnenkar, G., Mooney, R., Saenko, K., & Guadarrama, S. (2013).
Generating Natural-Language Video Descriptions Using Text-Mined Knowledge. In
Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.
Kulkarni, G., Premraj, V., Dhar, S., Li, S., Choi, Y., Berg, A. C., & Berg, T. L. (2011). Baby
talk: Understanding and generating simple image descriptions. In IEEE Conference
on Computer Vision and Pattern Recognition.
Kuznetsova, P., Ordonez, V., Berg, A. C., Berg, T. L., & Choi, Y. (2012). Collective
Generation of Natural Image Descriptions. In Annual Meeting of the Association for
Computational Linguistics.
438

fiAutomatic Description Generation from Images: A Survey

Kuznetsova, P., Ordonezz, V., Berg, T. L., & Choi, Y. (2014). TREETALK: Composition
and compression of trees for image descriptions. In Conference on Empirical Methods
in Natural Language Processing.
Lampert, C. H., Nickisch, H., & Harmeling, S. (2009). Learning to detect unseen object
classes by between-class attribute transfer. In IEEE Conference on Computer Vision
and Pattern Recognition.
Lazebnik, S., Schmid, C., & Ponce, J. (2006). Beyond bags of features: Spatial pyramid
matching for recognizing natural scene categories. In IEEE Conference on Computer
Vision and Pattern Recognition.
Lebret, R., Pinheiro, P. O., & Collobert, R. (2015). Phrase-based image captioning. In
International Conference on Machine Learning.
Li, S., Kulkarni, G., Berg, T. L., Berg, A. C., & Choi, Y. (2011). Composing simple image
descriptions using web-scale n-grams. In The SIGNLL Conference on Computational
Natural Language Learning.
Liang, P., Jordan, M. I., & Klein, D. (2012). Learning dependency-based compositional
semantics. Computational Linguistics, 39 (2), 389446.
Lin, C.-Y., & Hovy, E. (2008). Automatic evaluation of summaries using n-gram cooccurrence statistics. In Annual Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies.
Lin, D., Fidler, S., Kong, C., & Urtasun, R. (2015). Generating multi-sentence natural
language descriptions of indoor scenes. In British Machine Vision Conference.
Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., & Zitnick,
C. L. (2014). Microsoft COCO: Common objects in context. In European Conference
on Computer Vision.
Lowe, D. (2004). Distinctive image features from scale-invariant keypoints. International
Journal of Computer Vision, 60 (4), 91110.
Ma, L., Lu, Z., & Li, H. (2016). Learning to answer questions from image using convolutional
neural network. In AAAI Conference on Artificial Intelligence.
Malinowski, M., & Fritz, M. (2014a). A multi-world approach to question answering about
real-world scenes based on uncertain input. In Advances in Neural Information Processing Systems.
Malinowski, M., & Fritz, M. (2014b). Towards a visual turing challenge. In Advances in
Neural Information Processing Systems Workshop on Learning Semantics.
Malinowski, M., Rohrbach, M., & Fritz, M. (2015). Ask your neurons: A neural-based
approach to answering questions about images. In International Conference on Computer Vision.
Mao, J., Xu, W., Yang, Y., Wang, J., & Yuille, A. L. (2015a). Deep captioning with multimodal recurrent neural networks (m-RNN). In International Conference on Learning
Representations.
439

fiBernardi et al.

Mao, J., Wei, X., Yang, Y., Wang, J., Huang, Z., & Yuille, A. L. (2015b). Learning like
a child: Fast novel visual concept learning from sentence descriptions of images. In
International Conference on Computer Vision.
Mason, R., & Charniak, E. (2014). Nonparametric Method for Data-driven Image Captioning. In Annual Meeting of the Association for Computational Linguistics.
Mason, W. A., & Watts, D. J. (2009). Financial incentives and the performance of crowds.
In ACM SIGKDD Workshop on Human Computation.
Mitchell, M., Han, X., Dodge, J., Mensch, A., Goyal, A., Berg, A. C., Yamaguchi, K.,
Berg, T. L., Stratos, K., Daume, III, H., & III (2012). Midge: generating image
descriptions from computer vision detections. In Conference of the European Chapter
of the Association for Computational Linguistics.
Nenkova, A., & Vanderwende, L. (2005). The impact of frequency on summarization. Tech.
rep., Microsoft Research.
Oliva, A., & Torralba, A. (2001). Modeling the shape of the scene: A holistic representation
of the spatial envelope. International Journal of Computer Vision, 42 (3), 145175.
Ordonez, V., Kulkarni, G., & Berg, T. L. (2011). Im2text: Describing images using 1 million
captioned photographs. In Advances in Neural Information Processing Systems.
Ortiz, L. M. G., Wolff, C., & Lapata, M. (2015). Learning to Interpret and Describe
Abstract Scenes. In Conference of the North American Chapter of the Association of
Computational Linguistics.
Panofsky, E. (1939). Studies in Iconology. Oxford University Press.
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). BLEU: A method for automatic evaluation of machine translation. In Annual Meeting of the Association for
Computational Linguistics.
Parikh, D., & Grauman, K. (2011). Relative attributes. In International Conference on
Computer Vision.
Park, C., & Kim, G. (2015). Expressing an image stream with a sequence of natural
sentences. In Advances in Neural Information Processing Systems.
Patterson, G., Xu, C., Su, H., & Hays, J. (2014). The SUN Attribute Database: Beyond Categories for Deeper Scene Understanding. International Journal of Computer Vision,
108 (1-2), 5981.
Pinheiro, P., Lebret, R., & Collobert, R. (2015). Simple image description generator via a
linear phrase-based model. In International Conference on Learning Representations
Workshop.
Prest, A., Schmid, C., & Ferrari, V. (2012). Weakly supervised learning of interactions
between humans and objects. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 34 (3), 601614.
Rashtchian, C., Young, P., Hodosh, M., & Hockenmaier, J. (2010). Collecting image annotations using amazons mechanical turk. In North American Chapter of the Association
for Computational Linguistics: Human Language Technologies Workshop on Creating
Speech and Language Data with Amazons Mechanical Turk.
440

fiAutomatic Description Generation from Images: A Survey

Reiter, E., & Belz, A. (2009). An investigation into the validity of some metrics for automatically evaluating natural language generation systems. Computational Linguistics,
35 (4), 529588.
Reiter, E., & Dale, R. (2006). Building Natural Language Generation Systems. Cambridge
University Press.
Ren, M., Kiros, R., & Zemel, R. (2015). Image question answering: A visual semantic embedding model and a new dataset. In International Conference on Machine Learningt
Deep Learning Workshop.
Richardson, M., Burges, C. J., & Renshaw, E. (2013). MCTest: A challenge dataset for the
open-domain machine comprehension of text. In Conference on Empirical Methods in
Natural Language Processing.
Rohrbach, A., Rohrback, M., Tandon, N., & Schiele, B. (2015). A dataset for movie description. In International Conference on Computer Vision.
Rohrbach, M., Qiu, W., Titov, I., Thater, S., Pinkal, M., & Schiele, B. (2013). Translating
Video Content to Natural Language Descriptions. In International Conference on
Computer Vision.
Schuster, S., Krishna, R., Chang, A., Fei-Fei, L., & Manning, C. D. (2015). Generating
semantically precise scene graphs from textual descriptions for improved image retrieval. In Conference on Empirical Methods in Natural Language Processing Vision
and Language Workshop.
Shatford, S. (1986). Analyzing the subject of a picture: A theoretical approach. Cataloging
& Classification Quarterly, 6, 3962.
Silberman, N., Kohli, P., Hoiem, D., & Fergus, R. (2012). Indoor segmentation and support
inference from RGBD images. In European Conference on Computer Vision.
Socher, R., & Fei-Fei, L. (2010). Connecting modalities: Semi-supervised segmentation
and annotation of im- ages using unaligned text corpora. In IEEE Conference on
Computer Vision and Pattern Recognition.
Socher, R., Karpathy, A., Le, Q. V., Manning, C. D., & Ng, A. (2014). Grounded Compositional Semantics for Finding and Describing Images with Sentences. Transactions
of the Association for Computational Linguistics, 2, 207218.
Sun, C., Gan, C., & Nevatia, R. (2015). Automatic concept discovery from parallel text
and visual corpora. In International Conference on Computer Vision.
Thomason, J., Venugopalan, S., Guadarrama, S., Saenko, K., & Mooney, R. (2014). Integrating Language and Vision to Generate Natural Language Descriptions of Videos
in the Wild. In International Conference on Computational Linguistics.
Torralba, A., Fergus, R., & Freeman, W. T. (2008). 80 million tiny images: A large data
set for nonparametric object and scene recognition. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 30 (11), 19581970.
Ushiku, Y., Yamaguchi, M., Mukuta, Y., & Harada, T. (2015). Common subspace for model
and similarity: Phrase learning for caption generation from images. In International
Conference on Computer Vision.
441

fiBernardi et al.

Vedantam, R., Lawrence Zitnick, C., & Parikh, D. (2015). Cider: Consensus-based image description evaluation. In IEEE Conference on Computer Vision and Pattern
Recognition.
Verma, Y., & Jawahar, C. V. (2014). Im2Text and Text2Im: Associating Images and Texts
for Cross-Modal Retrieval. In British Machine Vision Conference.
Vinyals, O., Toshev, A., Bengio, S., & Erhan, D. (2015). Show and tell: A neural image
caption generator. In IEEE Conference on Computer Vision and Pattern Recognition.
Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhutdinov, R., Zemel, R., & Bengio, Y.
(2015). Show, attend and tell: Neural image caption generation with visual attention.
In International Conference on Machine Learning.
Yagcioglu, S., Erdem, E., Erdem, A., & Cakici, R. (2015). A Distributed Representation
Based Query Expansion Approach for Image Captioning. In Annual Meeting of the
Association for Computational Linguistics.
Yang, Y., Teo, C. L., Daume, III, H., & Aloimonos, Y. (2011). Corpus-guided sentence generation of natural images. In Conference on Empirical Methods in Natural Language
Processing.
Yao, B., & Fei-Fei, L. (2010). Grouplet: A structured image representation for recognizing
human and object interactions. In IEEE Conference on Computer Vision and Pattern
Recognition.
Yao, L., Torabi, A., Cho, K., Ballas, N., Pal, C., Larochelle, H., & Courville, A. (2015).
Describing videos by exploiting temporal structure. In International Conference on
Computer Vision.
Yatskar, M., Galley, M., Vanderwende, L., & Zettlemoyer, L. (2014). See No Evil, Say No
Evil: Description Generation from Densely Labeled Images. In Joint Conference on
Lexical and Computation Semantics.
Young, P., Lai, A., Hodosh, M., & Hockenmaier, J. (2014). From image descriptions to visual
denotations: New similarity metrics for semantic inference over event descriptions.
Transactions of the Association for Computational Linguistics, 2, 6778.
Yu, L., Park, E., Berg, A. C., & Berg, T. L. (2015). Visual madlibs: Fill in the blank
description generation and question answering. In International Conference on Computer Vision.
Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., & Fidler, S.
(2015). Aligning books and movies: Towards story-like visual explanations by watching
movies and reading books. In International Conference on Computer Vision.
Zitnick, C. L., Parikh, D., & Vanderwende, L. (2013). Learning the visual interpretation of
sentences. In International Conference on Computer Vision.
Zitnick, C. L., & Parikh, D. (2013). Bringing semantics into focus using visual abstraction.
In IEEE Conference on Computer Vision and Pattern Recognition.

442

fiJournal of Artificial Intelligence Research 55 (2016) 1-15

Submitted 11/15; published 01/16

Introduction to the Special Issue on Cross-Language
Algorithms and Applications
Marta R. Costa-jussa

marta.ruiz@upc.edu

TALP Research Center
Universitat Politecnica de Catalunya
Jordi Girona 13, 08034, Barcelona

Srinivas Bangalore

sbangalore@interactions.net

Interactions Labs
41 Spring Street,
Murray Hill, NJ 07974, USA

Patrik Lambert

patrik.lambert@upf.edu

Computational Linguistics Group
Universitat Pompeu Fabra
Roc Boronat 138, 08018 Barcelona, Spain

Llus Marquez

lmarquez@qf.org.qa

Qatar Computing Research Institute
Hamad Bin Khalifa University
Tornado Tower (10th floor), PO.Box 5825,
West Bay, Doha, Qatar

Elena Montiel-Ponsoda

elena.montiel@upm.es

Ontology Engineering Group
Universidad Politecnica de Madrid
Campus de Montegancedo s/n, Boadilla del Monte,
28660 Madrid

Abstract

With the increasingly global nature of our everyday interactions, the need for multilingual technologies to support efficient and effective information access and communication
cannot be overemphasized. Computational modeling of language has been the focus of
Natural Language Processing, a subdiscipline of Artificial Intelligence. One of the current
challenges for this discipline is to design methodologies and algorithms that are crosslanguage in order to create multilingual technologies rapidly. The goal of this JAIR special
issue on Cross-Language Algorithms and Applications (CLAA) is to present leading research in this area, with emphasis on developing unifying themes that could lead to the
development of the science of multi- and cross-lingualism. In this introduction, we provide
the reader with the motivation for this special issue and summarize the contributions of the
papers that have been included. The selected papers cover a broad range of cross-lingual
technologies including machine translation, domain and language adaptation for sentiment
analysis, cross-language lexical resources, dependency parsing, information retrieval and
knowledge representation. We anticipate that this special issue will serve as an invaluable
resource for researchers interested in topics of cross-lingual natural language processing.
c
2016
AI Access Foundation. All rights reserved.

fiCosta-jussa, Bangalore, Lambert, Marquez & Montiel-Ponsoda

1. Introduction
Due to the increasingly global nature of our society, it is commonplace for each of us to
encounter information in a multitude of languages and to communicate across languages in
our everyday lives. The rapid growth of multilinguality in our information-driven society
is reflected in the number of languages being used on the Internet. In about a decade,
the Internet has transformed from being a predominantly English information source to
the linguistically variegated information source that it is today. Multilingual access and
processing pose novel challenges to the core Artificial Intelligence discipline of speech and
natural language processing which, when solved, can provide transformational technologies
for information access to a broader population.
The complexity of processing multiple languages in a computational model emerges
not only due to different syntactic structures for the same concept, but also from different
underlying conceptual structures. These challenges necessitate the development of crosslanguage natural language processing tools that are able to translate or link these structures
and concepts across different languages.
Cross-language extensions of popular applications and tasks such as information retrieval, question answering, sentiment analysis and lexical disambiguation, among others,
have been developed to respond to the present needs of the global society. Similarly, multilingual resources and novel ways to represent multilingual knowledge have emerged and
spread in recent years. Researchers and developers have made considerable advances in
these applications by leveraging relevant data available in diverse languages. With multilingual processing becoming a key research issue, and given its interdisciplinary nature, a
range of approaches from both linguistic and statistical perspectives have been explored in
order to create viable cross-lingual technology.
Interestingly, the challenges of cross-lingual natural language processing appeals to both
academic and industrial research. It provides new business opportunities by breaking down
language barriers that fragment the potential market. These opportunities include, for instance, the possibility for companies and institutions to learn about what users at different
locations think of their products (cross-language sentiment analysis), to perform document
search in multiple languages (cross-language information retrieval and question answering),
to link knowledge bases of clients using different languages (cross-language knowledge representation), or to expand the market to several linguistically diverse markets (machine
translation and localization).
The importance of cross-lingual natural language processing can be clearly seen from
the attention it has received at recent workshops, invited talks, and publications at the
communitys premier conferencesAssociation of Computational Linguistics (ACL), European Association of Computational Linguistics (EACL), North American Association of
Computational Linguistics (NAACL), Empirical Methods in Natural Language Processing
(EMNLP), International Conference on Computational Linguistics (COLING), Extended
Semantic Web Conference (ESWC), International Semantic Web Conference (ISWC), International Conference on Language Resources and Evaluation (LREC), and International
Conference on Knowledge Capture (KCAP). In the past eighteen months, about one in
every five papers at these conferences was related to cross-language algorithms and applications (figures vary from 17% to 25%, with an average of 20.4%). A recently published book
2

fiSpecial Issue on Cross-Language Algorithms and Applications

has focused on multilingual natural language processing techniques (Bikel & Zitouni, 2012)
and further highlights the importance of this research area. This book comprehensively
presents, in more than 600 pages, the basic theory and the most relevant techniques for 16
different aspects of multilingual natural language processing.
1.1 Cross-Language Algorithms and Applications Special Issue
This special issue is intended to provide a broad view of some of the recent advances
and current research directions being pursued in the area of multilingual natural language
processing from the linguistic, computational and language resource creation perspectives.
Active research from multiple recent workshops in the area (e.g., HyTra, see Costa-juss,
Banchs, Rapp, Lambert, Eberle & Babych, 2013; WMT, see Bojar, Chatterjee, Federmann,
Haddow, Huck, Hokamp, Koehn, Loncheva, Monz, Negri, Post, Scarton, Specia & Turchi,
2015; CLEF, see Forner, Moller, Paredes, Rosso & Stein, 2013; Promise, see Bener, Minku
& Turhan, 2014; MSW, see Gracia, MacGrae & Vulcu, 2015; and AKBC, see Suchanek,
Riedel, Singh & Talukdar, 2012) spanning cross-language natural language applications,
tools of crowdsourcing for resource creation, and deeper relationships between bridging
language barriers and other modalities of perception are summarized in this special issue.
In response to our solicitation of papers in late 2014, we received 34 research papers
on a variety of cross-lingual technologies applied to language processing tasks. Each of
these papers was carefully reviewed by at least three reviewers drawn from a pool of over
100 reviewers. Based on their reviews and extensive follow up discussions, we selected 8
high quality papers that offer exciting research directions which span a range of topics
in cross-lingual language processing including machine translation, domain and language
adaptation for sentiment and cross-language lexical resources, dependency parsing, information retrieval and knowledge representation. This introduction covers exactly the eight
papers that were accepted for the special issue during the official timeline. The papers are
summarized in Section 2, organized by topic. In an attempt to accommodate a broader
sample of interesting papers on cross-language algorithms and applications, other papers
appropriate for the topic that did not meet the special issue deadlines will be also added to
the same JAIR web page1 when accepted by the journal.
Finally, we intended the special issue to not be a disconnected melange of success stories,
but provide an underlying theme that unifies the research directions in this area. We hope
that this collection provides the reader an opportunity to observe similarities and differences
across topics, algorithms and applications. We anticipate that the scientific community
will view cross-lingual speech and language processing as a fertile and productive field of
research, that has the potential for developing science and technologies which will have a
lasting impact on our everyday lives.

2. Special Issue Overview
In this section, we survey the topics that cover the papers in this special issue and the
papers themselves. These topics are machine translation, domain and language adaptation
1. http://www.jair.org/specialtrack-claa.html

3

fiCosta-jussa, Bangalore, Lambert, Marquez & Montiel-Ponsoda

for sentiment analysis and cross-language lexical resources, dependency parsing, information
retrieval and knowledge representation.
2.1 Machine Translation
A key technology for the multilingual information society is Machine Translation (MT)
where a source language speech or text is automatically converted into a target language
speech or text.
With Web content being generated in multiple languages and with Internet users becoming linguistically diverse, machine translation provides the cheapest and quickest way to
understand this multilingual information. Besides being a core application necessary for a
multilingual world, machine translation has been shown to be a highly relevant component
technology in many cross-language natural language processing tasks, such as sentiment
analysis, information retrieval and knowledge representation.
Historically, machine translation approaches can be categorized into rule-driven and
data-driven approaches. Rule-based machine translation (Hutchins & Sommers, 1992) requires deep linguistic knowledge of the language pairs involved in the translation and a significant amount of human labor. Since the rules are based on linguistic intuitions, it is easier
to identify issues and extend them. More recently, there have been data-driven approaches
that learn translation models with minimal human supervision from very large bilingual
parallel corporatexts where source text is paired with the target text (Sanchez-Martnez
& Forcada, 2009). Data-driven translation systems find the most probable target text given
the source text. These systems have been extended to phrase-based, syntax-based, hierarchical phrase-based and neural-based systems in order to capture longer contexts in a
sentence. Phrase-based systems (Koehn, Och, & Marcu, 2003) use sequences of words as
bilingual units, called phrases, whose probability is computed through a log-linear combination of feature functions including the translation and language models. Syntax-based
systems use syntactic units extracted by parse trees (Quirk, Menezes, & Cherry, 2005).
Hierarchical phrase-based systems combine phrase-based and syntax-based approaches by
using synchronous context-free grammars (Chiang, 2007). Neural-based end-to-end translation systems typically use a encoder-decoder approach to learn embedded representations
of the input sentence (encoder), which are then used as context to generate the words in
the translation (decoder) (Bahdanau, Cho, & Bengio, 2015).
The boundaries between rule-based and statistical machine translation have narrowed
through the proposals of hybrid machine translation systems (Costa-jussa, 2015). In this
special issue, there are two research papers in machine translation that combine rules,
statistics and machine learning.
Integrating Rules and Dictionaries from Shallow-Transfer Machine Translation into Phrase-Based Statistical Machine Translation by Sanchez-Cartagena,
Perez-Ortiz and Sanchez-Martnez presents a hybrid approach to machine translation by
integrating rules and dictionaries from a shallow-transfer system (in particular, Apertium,
see Armentano-Oller & Forcada, 2006) into a phrase-based system (in particular, Moses,
see Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens,
Dyer, Bojar, Constantin & Herbst, 2007). Deep linguistic knowledge from rule-based systems is transferred into the statistical system. This integration is especially useful when
4

fiSpecial Issue on Cross-Language Algorithms and Applications

the parallel corpus available for training the phrase-based system is small or when translating out-of-domain texts. The authors discuss different methods for enriching translation
tables (including previous work by the same authors and other references). Finally, it is
worth mentioning that the approach that used Apertiums rules was one of the best-ranked
systems in the WMT 2011 international evaluation campaign for Spanish-English (SanchezCartagena, Sanchez-Martnez, & Perez-Ortiz, 2011).
Two relevant features of this paper are: (i) the integration of rules and statistics presented, which takes advantage of a deep knowledge of the underlying rule-based system,
especially in the way linguistic resources are used by the rule-based system when segmenting the source-language sentences; (ii) a complete analysis of the hybrid system, including
automatic and manual evaluation, different corpora sizes, domains and language pairs, and
a comparison to another popular hybrid MT system (Eisele, Federmann, Uszkoreit, SaintAmand, Kay, Jellinghaus, Hunsicker, Herrmann, & Chen, 2008).
Cross-Lingual Bridges with Models of Lexical Borrowing by Tsvetkov and Dyer
introduces a hybrid model of lexical borrowing, which is demonstrated in machine translation to alleviate the problem of lexical coverage in low-resourced languages. The authors
start from the hypothesis that all languages borrow terms from other languages at some point
in their existence. They propose a computational model of linguistic borrowing, intended
to identify donor words in a resource-rich language given a loan word in a resource-poor
language. The model develops a set of morpho-phonological transformations by combining linguistic constraints using optimality theory and machine learning to score loanword
candidates. This model consists of three parts: (i) conversion of orthographic word forms
to pronunciations,(ii) generation of loan word pronunciation candidates, and (iii) ranking of generated candidates using optimality-theoretic constraints. The first two steps are
rule-based while the third is learned from data.
The lexical borrowing model is applied to Swahili-English, Maltese-English, and Romanian-English MT. The authors leverage the model in an indirect way. For instance, for improving the resource-poor Swahili-English MT system, they identify translation candidates for
out-of-vocabulary (OOV) Swahili words borrowed from Arabic, using an Arabic-to-Swahili
borrowing model and a resource-rich Arabic-English MT system. The experimental results
show that this approach effectively reduces the impact of OOV source words and improves
translation quality significantly.
If one considers lexical borrowing as a different task than transliteration and cognate
identification then this could be the first computational model of lexical borrowing used in
a downstream natural language processing application.
2.2 Domain and Language Adaptation for Sentiment
There is a rapidly growing repository of user-generated, subjective texts on the Internet in
the form of blogs, social networks, information channels and consumer sites expressing opinions on various issues, sentiment towards products, and services and personal perspectives
about events.
Sentiment analysis is the task of analyzing opinions, sentiments or emotions expressed
towards entities such as products, services, organizations, issues, and the various attributes
of these entities (Liu, 2012). The two main sentiment analysis approaches presented in the
5

fiCosta-jussa, Bangalore, Lambert, Marquez & Montiel-Ponsoda

literature are a machine learning approach (mostly supervised learning) and an opinionlexicon-based approach, based on rules.
When necessary resourcesthe training data, the subjective text to be analyzed and the
analysis outcomeare not all available in the required language, cross-language sentiment
analysis methods are needed to bootstrap a system. The main cross-language sentiment
analysis approaches described in the literature are via lexicon transfer, via corpus transfer, via test translation and via joint classification. In the lexicon transfer approach, a
source sentiment lexicon is transferred into the target language and a lexicon-based classifier is built in the target language (Mihalcea, Banea, & Wiebe, 2007). The corpus transfer
approach consists of transferring a source training corpus into the target language and
building a corpus-based classifier in the target language (Banea, Mihalcea, & Wiebe, 2008).
In the test translation approach, test sentences from the target language are translated into
the source language and they are classified using a source language classifier (Bautin, Vijayarenu, & Skiena, 2008). Work on joint classification includes co-training (Wan, 2009), joint
learning (Lu, Tan, Cardie, & K. Tsou, 2011) or structural correspondence learning (Wei &
Pal, 2010; Prettenhofer & Stein, 2010).
Some authors have already studied the impact on automatic sentiment analysis of transferring lexicons or corpora via machine translation (Mihalcea et al., 2007; Banea et al.,
2008).
How Translation Alters Sentiment by Mohammad, Salameh and Kiritchenko goes
a step further in the analysis in two respects. First, the authors conduct a systematic
evaluation of the impact of automatic and manual translation on both automatic and manual
sentiment analysis. Second, the authors perform a qualitative and quantitative analysis to
understand the reasons for the obtained results. In summary, this paper provides a deeper
understanding of how sentiments are altered in common cross-language settings.
The experiments are performed using an Arabic sentiment analysis system and an
EnglishArabic machine translation system, both showing state-of-the-art performance.
The authors first show that automatic sentiment analysis of English translations (even coming from MT) can achieve competitive results. Interestingly, they also show that automatic
sentiment analysis of automatic translations outperforms the manual sentiment annotation
of the automatically translated text. The qualitative and quantitative analysis of the results
also reveals interesting facts. For example, sentiment expressions are often mistranslated
into neutral expressions. The automatic sentiment analysis system can recover from consistent translation errors by learning true sentiments from mistranslated words. Some common
causes of translation failing to preserve sentiments are sarcasm, metaphoric expressions, and
incorrect word-reordering.
Distributional Correspondence Indexing for Cross-Lingual and Cross-Domain
Sentiment Classification by Esuli, Moreo and Sebastiani proposes a novel domain adaptation method, also evaluated on language adaptation. This paper explores a more general
and complex formulation of the domain adaptation problem that combines the cross-domain
and cross-language settings. The proposed adaptation method, called Distributional Correspondence Indexing, is inspired by Structural Correspondence Learning but follows a
different, simpler approach, with a more direct application of the distributional hypothesis.
This approach assumes that terms across domains and/or languages show similar distributional properties relative to a small set of pivot terms, which behave similarly across
6

fiSpecial Issue on Cross-Language Algorithms and Applications

domains/languages. The authors show that this approach outperforms existing methods
for cross-domain/language sentiment classification, at a lower computational cost.
Since digital documents in an increasing variety of topics and languages are produced,
often with a need to be processed immediately, better solutions to tackle the bottleneck
of scarcity of training data are of increasing importance. The idea of leveraging resources
from a language to learn a classifier in another language is similar to transfer learning in
domain adaptation. The contribution of this paper goes in the direction of unifying domain
and language adaptation in the same framework.
2.3 Lexical Resources
More and more natural language applications such as question-answering, sentiment analysis, or document classification, to mention but a few, demand lexical knowledge in different
natural languages, termed here cross-language lexical resources, and also well-known as
multilingual lexical resources. These range from non-structured resources such as parallel corpora (EU JRC-Acquis Corpus, see Steinberger, Pouliquen, Widiger, Ignat, Erjavec,
Tufis, & Varga, 2006), glossaries (e.g., IFLA Multilingual Glossary for Art Librarians, see
Libraries, 1996) or machine-readable dictionaries (Oxford online dictionaries2 ), to more
structured resources such as terminological databases (IATE3 ), thesauri (AGROVOC 4 ),
lexicons (EuroWordNet, see Vossen, 1998; MultiWordNet, see Pianta, Bentivogli, & Girardi, 2002), or ontologies (e.g., EUROVOC in SKOS, see Smedt & Vatanat, 2009; or FAO
geopolitical ontology, see Kim, Iglesias-Sucasas, & Viollier, 2013).
The creation of such multilingual resources involves manual and costly processes, which
is why many approaches pursue the automation of several steps in the development process.
Without the aim of being exhaustive, we briefly describe typical approaches or methods
that result in multilingual lexical resources. (i) The well-known family of multilingual
wordnets developed around the Princeton English WordNet (Fellbaum, 1998). Basically, two
approaches are followed: (a) a merging approach in which wordnets are created separately
and mapped afterwards to the English WordNet, with the difficulties that the mapping task
involves; and (b) an expansion approach in which the English WordNet is translated into
the corresponding target languages, facilitating the subsequent mapping task. (ii) Onlinecollaborative resources, as for example, Wiktionary5 . They take advantage of the wisdom
of the crowd and also of Internet bots that automatically generate entries, as well as of
algorithms that import lexical information from machine readable dictionaries. Similarly,
Wikipedia and the whole family of Wikimedia projects have been created in a collaborative
manner and constitute de facto multilingual resources thanks to the hyperlinks among
entries in different languages. Other resources built in a similar way or that take advantage
of the structured information contained in them include YAGO (Mahdisoltani, Biega, &
Suchanek, 2015) and BabeLNet (Navigli & Ponzetto, 2012). (iii) Mono- and multilingual
content and linguistic datasets exposed and linked according to the Linked Data paradigm6 ,
a set of best practices for publishing structured data and linking it to other datasets. Some
2.
3.
4.
5.
6.

http://www.oxforddictionaries.com
http://iate.europa.eu
http://aims.fao.org/vest-registry/vocabularies/agrovoc-multilingual-agricultural-thesaurus
https://www.wiktionary.org
http://linkeddata.org

7

fiCosta-jussa, Bangalore, Lambert, Marquez & Montiel-Ponsoda

of the resources published as linked data are multilingual and others are monolingual, but
once linked to other datasets in the same or close domains, they become a multilingual graph
of navigable data. The mapping or linking step is crucial in this approach. (iv) Translation
resources can also be considered as a subtype of lexical resources and are also used for the
purposes of obtaining multilingual data, be it as a source for translations, or as a means
for mapping or linking monolingual resources (for more details on machine translation see
section 2.1).
In this sense, it has been sufficiently demonstrated that quality and coverage of such
lexical and translation resources are vital for applications built on top of them, and that
the performance of those applications depends directly on them. The paper included in
this special issue not only describes and analyzes some of these resources in detail, but also
evaluates their coverage and correctness in the context of ontology mapping.
Effectiveness of Automatic Translations for Cross-lingual Ontology Mapping
by Abu Helou and Palmonari presents a large-scale study on the effectiveness of several
lexical and translation resources for the purpose of obtaining candidate matches between
ontology concepts lexicalized in different languages. Many works on cross-language ontology
mapping rely on multilingual or translation resources to obtain translation candidates for
concept lexicalization in the source ontology, which subsequently support the selection of
potential matches in the target ontology. This paper evaluates a machine translation service, GoogleTranslate7 , and a multilingual encyclopedic dictionary and semantic network,
BabelNet8 , for correctness and coverage of the suggested translations and mapping selection capabilities (word-disambiguation). The evaluation is based on wordnets in different
natural languages (Arabic, Italian, Slovene and Spanish) and manual alignments provided
for each wordnet and the English WordNet. They perform three experiments taking into
account various types of lexical units (monosemous words, polysemous words, one-word
units or multiple-word units), and define specific measures (translation correctness, word
sense coverage, synset coverage and synonym coverage) that serve to better evaluate the
quality of translations. The results of the experiments provide insights into the coverage
and correctness that these resources provide, the effect of combining the results from both
resources, the impact of taking into account translation directionality, and the differences
in word categories between mapped lexicalizations, among others.
The results of this paper can be directly applied to improve the cross-language ontology
mapping task, but can also contribute to speed up the development of multilingual lexical
resources, in general, or help in the building of a truly multilingual Linked Open Data
Cloud, in particular.
2.4 Cross-Language Dependency Parsing
A natural language application that relies on language processing tools such as part-ofspeech taggers and parsers is confronted with a significant challenge of scaling to new
languages, when such languages may not have the same set of tools. Building such tools
in a desired language requires manual annotation of sufficient amounts of texts that machine learning programs can be trained on. Annotation efforts have been undertaken and
7. https://translate.google.com
8. http://babelnet.org

8

fiSpecial Issue on Cross-Language Algorithms and Applications

invaluable resources of varying amounts of texts have been created during the past decades
for certain languages, for example, Penn Treebank (Marcus, Marcinkiewicz, & Santorini,
1993), French Treebank (Abeille, 2003), NEGRA Treebank (Skut, Brants, & Uszkoreit,
1998), Prague Dependency Treebank (Hajic, Bohmova, Hajicova, & Vidova-Hladka, 2000).
However, the task of such annotation effort is time-consuming, expensive and requires highly
skilled personnel.
In the late nineties, with the availability of texts in a pair of languages as realized in
parallel text corpora, researchers (Bangalore, 1998; Yarowsky, Ngai, & Wicentowski, 2001)
explored the idea of transferring annotations from one language in the pair to the second
language in the pair in order to rapidly create an annotated resource to train language
processing tools for the second language. This line of research has been followed by a number
of researchers for projecting a variety of annotations between language corpora and using
the annotated corpus to bootstrap language processing tools for the target language. Of
particular interest is the projection of annotations for part-of-speech tags, phrase structure
annotations, and dependency structure annotations.
Synthetic Treebanking for Cross-lingual Dependency Parsing by Tiedemann
and Agic presents a detailed discussion of the options to transplant dependency trees from
one language to another in order to train dependency parsers in the target language. The
paper introduces two options of bootstrapping a dependency parser for a language: a model
transfer approach where a dependency parsing model is transferred to the new language, and
an annotation transfer approach; the paper then advocates the annotation transfer approach
through the creation of synthetic treebanks. The paper provides a comprehensive analysis of
the various options for creating such a synthetic treebank using a parallel corpus including:
(i) projecting a parsers output of the source text onto the target text and (ii) translating an
existing high quality treebank in a source language to a target language. The central idea
in creating synthetic treebanks involves the use of statistical machine translation models,
both phrase-based and syntax-based, in order to translate the texts of the source language
treebanks into the target language, and then project the source language structures to the
translated target language sentence mediated by the word alignment information produced
from the translation process. The challenges of reconciling the dependency structures when
the lexical translations are richer than one-to-one are discussed at length. The paper reports
extensive parsing accuracy results from parsers trained on projected treebanks for a large
set of language pairs, and studies the correlation between the quality of translation model
and the quality of the target language parser.
2.5 Cross-Language Information Retrieval
With the increase in number of webpages in multiple languages, a search query on the
Web needs to retrieve webpages authored in languages other than the language of the
query. Cross-language information retrieval is a technology at the intersection of machine
translation and information retrieval that addresses this challenge.
Cross-language information retrieval has employed different strategies for matching a
query with a set of multilingual documents: cognate-based matching (Montalvo, Martnez,
Casillas, & Fresno, 2007), matching by query and document translation, and matching by
mapping to an interlingua (Banchs & Costa-jussa, 2013). The most popular of these ap9

fiCosta-jussa, Bangalore, Lambert, Marquez & Montiel-Ponsoda

proaches is query translation and it can be addressed by either employing bilingual dictionaries (Hedlund, Airio, Keskustalo, Lehtokangas, Pirkola, & Jarvelin, 2004) or using machine
translation (Kishida, 2008). Approaches that combine both techniques can be found in the
work of in the work of Zhang, Jones, and Zhang (2008), for example. The quality of query
translation can be indirectly observed in the final retrieval results. Kishida (2008) shows
with a regressive model that both ease of search of a given query and translation quality
can explain about 60% of the variation in the performance. Kettunen (2009) shows that
for long topics, the correlations between achieved retrieval results and machine translation
metrics are high (almost 90%) and for short topics the correlation is lower but still clear
(almost 60%). Cross-language video retrieval, another related cross-language information
retrieval task, involves automatic speech recognition.
Utilisation of Metadata Fields and Query Expansion in Cross-Lingual Search
of User-Generated Internet Video by Khwileh, Jones and Ganguly presents one of the
first use-cases for cross-language video retrieval for social media content. The paper focuses
on all the challenges associated with user generated informal content (i.e., noise, sparseness
of metadata, content with very different lengths, informal language register, etc.) rather
than on professionally produced content. Noise and errors propagate through each step
of the processing: from speech recognition to automatic translation and query expansion.
The authors use the query translation approach to bridge the vocabulary gap between
the users query and the relevant content in a video application. Automatic translation
is done using Google Translate and retrieval and expansion is done with the Divergence
From Randomness IR model. They explore the effectiveness of three different sources of
information: transcripts from automatic speech recognition, video titles, and descriptions.
Among the three sources, leveraging video titles improves retrieval performance in their
experiments. In addition, the authors propose an adaptive query expansion technique that
automatically selects the most reliable source for expansion based on a well established
query performance prediction technique. Results show that this approach is more robust
for this particular setting.
2.6 Cross-Language Knowledge Representation
Knowledge representation systems aim to formalize representations of the world, or of a
certain domain of knowledge, in such a way that they are interpretable by computers. This
is achieved by identifying the domain concepts and the relations that exist among them, and
by representing all this information in a formal framework, e.g., using the Description Logic
formalism. Knowledge representation systems are intended to be language-independent
meaning representations. However, different representations of the same domain of knowledge can co-exist, since those who design a certain knowledge representation may have a
particular vision of the world, specific interests, or do it with a certain application in mind,
among other factors.
One of the difficulties of cross-language knowledge representation regards the conceptual
differences that can be observed across languages and cultures. Indeed, certain representations are prone to reflect cultural particularities that are not shared or understood in the
same way by other cultural systems. This involves the existence of certain concepts that
do not exist in other knowledge systems, or are not relevant to them, or are at different
10

fiSpecial Issue on Cross-Language Algorithms and Applications

granularity levels in the representation of concepts (Espinoza, Montiel-Ponsoda, & GomezPerez, 2009). For all these reasons, cross-language knowledge representation is a challenge
in the current multilingual Web. Two main approaches have been followed to obtain knowledge representation systems that support several languages: (i) inclusion of lexicalizations
in several natural languages to describe the concepts and relations formalized in a certain
knowledge representation system; (ii) existence of several knowledge representation systems
whose concepts and relations are expressed for a different natural language, which are then
linked or mapped to establish correspondences or equivalences between them. The former
approach is commonly applied in internationalized or standardized domains, whereas the
latter is typical in culturally-influenced domains, as termed by Cimiano, Montiel-Ponsoda,
Buitelaar, Espinoza, and Gmez-Prez (2010).
In our globalized and interconnected world, cross-language information access is of increasing importance. While several approaches to cross-language document similarity have
been reported, including machine translation, probabilistic topic models, classification or
matrix factorization, there is little previous work on the task of linking documents across
languages that refer to the same events (Pouliquen, Steinberger, Ignat, Ksper, & Temnikova, 2004; Pouliquen, Steinberger, & Deguernel, 2008; Leban, Fortuna, Brank, & Grobelnik, 2014). This is actually a difficult and computationally expensive task, especially
when many language pairs are involved, and only a very small number of real-life working
systems performing this task exist. One example of an existing service providing crosslanguage cluster linking is the European Media Monitor (EMM) (Pouliquen et al., 2008).
This special issue includes a contribution tackling this task, in which topic modeling is
used to represent the knowledge expressed in documents, and the linking task is based on
similarity-based and entity-related features.
News Across Languages - Cross-Lingual Document Similarity and Event
Tracking by Rupnik, Muhic, Leban, Skraba, Fortuna and Grobelnik addresses the problem of event tracking in a large multilingual stream, and, more specifically, how to link
collections of articles in different languages which refer to the same event. The authors
consider major languages and also less-resourced languages. The approach is based on representations of documents analogous to multilingual topics, which are valid over multiple
languages. These representations are learned using Wikipedia as a training corpus. They
are then used to compute cross-language similarities between documents regardless of language. The posterior cross-language cluster linking is performed in two steps. First, to
speed-up the process, the similarity function is used to identify a small set of potential
linking candidates for each cluster. Then, the final decision is taken based on a supervised
classification model whose features include similarity-based and entity-related features. In
a comprehensive experimental study, the authors show canonical correlation analysis to
be the best-performing method to compute multilingual similarities. Moreover, they show
that similarity-based features can greatly benefit from additional semantic extraction-based
features.

Acknowledgements
The authors want to thank Dan Roth, Mark Sammons and an anonymous reviewer for
their useful comments and suggestions on previous versions of this document. This work
11

fiCosta-jussa, Bangalore, Lambert, Marquez & Montiel-Ponsoda

has been supported by the 7th Framework Program of the European Commission through
the International Outgoing Fellowship Marie Curie Action (IMTraP-2011-29951), the IntraEuropean Fellowship CrossLingMind-2011-300828 and the project LIDER (610782); the
European Regional Development Fund (ERDF/FEDER); and the Spanish Ministerio de
Economa y Competitividad through the SpeechTech4All project (TEC2012-38939-C03-02)
and the project 4V: volumen, velocidad, variedad y validez en la gestion innovadora de
datos (TIN2013-46238-C4-2-R).

References
Abeille, A. (2003). Treebanks: Building and Using Parsed Corpora. Springer.
Armentano-Oller, C., & Forcada, M. L. (2006). Open-source machine translation between
small languages: Catalan and aranese occitan. In Workshop on Strategies for developing machine translation for minority languages, pp. 5154.
Bahdanau, D., Cho, K., & Bengio, Y. (2015). Neural Machine Translation by Jointly
Learning to Align and Translate. CoRR, abs/1409.0473.
Banchs, R., & Costa-jussa, M. R. (2013). Cross-Language Document Retrieval by using
Non-linear Semantic Mapping. Applied Artificial Intelligence Journal, 27 (9), 781
802.
Banea, C., Mihalcea, R., & Wiebe, J. (2008). A Bootstrapping Method for Building Subjectivity Lexicons for Languages with Scarce Resources. In Proceedings of the International Conference on Linguistic Resources and Evaluation (LREC), pp. 27642767,
Marrakech, Morocco.
Bangalore, S. (1998). Transplanting Supertags from English to Spanish. In Proceedings of
the TAG+4 Workshop.
Bautin, M., Vijayarenu, L., & Skiena, S. (2008). International Sentiment Analysis for News
and Blogs. In Proc. of the International Conference on Weblogs and Social Media,
pp. 1926, Seattle, U.S.A.
Bener, A., Minku, L., & Turhan, B. (Eds.). (2014). PROMISE 14: Proceedings of the 10th
International Conference on Predictive Models in Software Engineering, New York,
NY, USA. ACM.
Bikel, D., & Zitouni, I. (2012). Multilingual Natural Language Processing Applications:
From Theory to Practice. IBM Press.
Bojar, O., Chatterjee, R., Federmann, C., Haddow, B., Huck, M., Hokamp, C., Koehn, P.,
Logacheva, V., Monz, C., Negri, M., Post, M., Scarton, C., Specia, L., & Turchi, M.
(2015). Findings of the 2015 Workshop on Statistical Machine Translation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pp. 146, Lisbon,
Portugal.
Chiang, D. (2007). Hierarchical Phrase-Based Translation. Computational Linguistics,
33 (2), 201228.
Cimiano, P., Montiel-Ponsoda, E., Buitelaar, P., Espinoza, M., & Gomez-Perez, A. (2010).
A Note on Ontology Localization. Journal of Applied Ontology, 5(2), 127137.
12

fiSpecial Issue on Cross-Language Algorithms and Applications

Costa-jussa, M. R. (2015). How Much Hybridization Does Machine Translation Need?.
Journal of the American Society for Information Technology (JASIST), 6 (10), 2160
2165.
Costa-jussa, M. R., Banchs, R., Rapp, R., Lambert, P., Eberle, K., & Babych, B. (2013).
Workshop on hybrid approaches to translation: Overview and developments. In Proceedings of the Second Workshop on Hybrid Approaches to Translation, pp. 16, Sofia,
Bulgaria.
Eisele, A., Federmann, C., Uszkoreit, H., Saint-Amand, H., Kay, M., Jellinghaus, M., Hunsicker, S., Herrmann, T., & Chen, Y. (2008). Hybrid Architectures for Multi-Engine
Machine Translation. In Proceedings of Translating and the Computer 30. ASLIB/IMI,
ASLIB.
Espinoza, M., Montiel-Ponsoda, E., & Gomez-Perez, A. (2009). Ontology Localization. In
Proceedings of the 5th International Conference on Knowledge Capture (KCAP09),
pp. 3340.
Fellbaum, C. (Ed.). (1998). WordNet: An Electronic Lexical Database. MIT Press.
Forner, P., Moller, H., Paredes, R., Rosso, P., & Stein, B. (2013). Information Access
Evaluation. Multilinguality, Multimodality, and Visualization. Springer.
Gracia, J., MacCrae, J., & Vulcu, G. (Eds.). (2015). Proceedings of the Fourth Workshop
on the Multilingual Semantic Web. CEUR.
Hajic, J., Bohmova, A., Hajicova, E., & Vidova-Hladka, B. (2000). The Prague Dependency Treebank: A Three-Level Annotation Scenario. In Abeille, A. (Ed.), Treebanks:
Building and Using Parsed Corpora, pp. 103127. Amsterdam:Kluwer.
Hedlund, T., Airio, E., Keskustalo, H., Lehtokangas, R., Pirkola, A., & Jarvelin, K. (2004).
Dictionary-based Cross-Language Information Retrieval: Learning Experiences from
CLEF 2000-2002. Information Retrieval, 7 (1), 99119.
Hutchins, W. J., & Sommers, H. L. (1992). An Introduction to Machine Translation, Vol.
362. Academic Press, New York.
Kettunen, K. (2009). Choosing the Best MT Programs for CLIR PurposesCan MT
Metrics Be Helpful?. In Proceedings of the 31th European Conference on IR Research
on Advances in Information Retrieval, pp. 706712.
Kim, S., Iglesias-Sucasas, M., & Viollier, V. (2013). The FAO Geopolitical Ontology: A Reference for country-Based Information. Journal of Agricultural and Food Information,
14 (1).
Kishida, K. (2008). Prediction of Performance of Cross-language Information Retrieval Using Automatic Evaluation of Translation. Library and Information Science Research,
30 (2), 138144.
Koehn, P., Och, F., & Marcu, D. (2003). Statistical Phrase-Based Translation. In Proceedings of the Conference of the North American Chapter of the Association for
Computational Linguistics on Human Language Technology (NAACL-HLT), pp. 48
54.
13

fiCosta-jussa, Bangalore, Lambert, Marquez & Montiel-Ponsoda

Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan,
B., Shen, W., Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin, A., & Herbst, E.
(2007). Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings
of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration
Sessions, ACL 07, pp. 177180.
Leban, G., Fortuna, B., Brank, J., & Grobelnik, M. (2014). Event registry: Learning about
world events from news. In Proceedings of the Companion Publication of the 23rd
International Conference on World Wide Web Companion (WWW Companion14),
pp. 107110, Seoul, Korea.
Libraries, I. S. o. A. (Ed.). (1996). Multilingual Glossary for Art Librarians: English with
Indexes in Dutch, French, German, Italian, Spanish and Swedish. De Gruyter.
Liu, B. (2012). Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human
Language Technologies. Morgan & Claypool Publishers.
Lu, B., Tan, C., Cardie, C., & K. Tsou, B. (2011). Joint Bilingual Sentiment Classification
with Unlabeled Parallel Corpora. In Proceedings of the Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), pp.
320330, Portland, Oregon, USA.
Mahdisoltani, F., Biega, J., & Suchanek, F. M. (2015). YAGO3: A Knowledge Base from
Multilingual Wikipedias. In Proceedings of the Conference on Innovative Data Systems
Research (CIDR 2015).
Marcus, M. P., Marcinkiewicz, M. A., & Santorini, B. (1993). Building a Large Annotated
Corpus of English: The Penn Treebank. Computational Linguistics, 19 (2), 313330.
Mihalcea, R., Banea, C., & Wiebe, J. (2007). Learning Multilingual Subjective Language via
Cross-Lingual Projections. In Proceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL), pp. 976983, Prague, Czech Republic.
Montalvo, S., Martnez, R., Casillas, A., & Fresno, V. (2007). Multilingual News Clustering:
Feature Translation vs. Identification of Cognate Named Entities. Pattern Recognition
Letters, 28 (16), 23052311.
Navigli, R., & Ponzetto, S. P. (2012). BabelNet: The Automatic Construction, Evaluation and Application of a Wide-Coverage Multilingual Semantic Network. Artificial
Intelligence, 193, 217250.
Pianta, E., Bentivogli, L., & Girardi, C. (2002). MultiWordNet: developing an aligned
multilingual database. In Proceedings of the Frist International Conference on Global
WordNet.
Pouliquen, B., Steinberger, R., & Deguernel, O. (2008). Story Tracking: Linking Similar
News Over Time and Across Languages. In Proceedings of the COLING 2008 Workshop on Multi-source Multilingual Information Extraction and Summarization, pp.
4956, Manchester, UK.
Pouliquen, B., Steinberger, R., Ignat, C., Ksper, E., & Temnikova, I. (2004). Multilingual
and cross-lingual news topic tracking. In Proceedings of the International Conference
on Computational Linguistics (COLING), pp. 959965, Geneva, Switzerland. COLING.
14

fiSpecial Issue on Cross-Language Algorithms and Applications

Prettenhofer, P., & Stein, B. (2010). Cross-Language Text Classification Using Structural
Correspondence Learning. In Proceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL), pp. 11181127, Uppsala, Sweden.
Quirk, C., Menezes, A., & Cherry, C. (2005). Dependency Treelet Translation: Syntactically
Informed Phrasal SMT. In Proceedings of the Annual Meeting of the Association for
Computational Linguistics (ACL), pp. 271279.
Sanchez-Cartagena, V. M., Sanchez-Martnez, F., & Perez-Ortiz, J. A. (2011). The Universitat dAlacant hybrid machine translation system for WMT 2011. In Proceedings
of the Sixth Workshop on Statistical Machine Translation, pp. 457463, Edinburgh,
Scotland.
Sanchez-Martnez, F., & Forcada, M. L. (2009). Inferring Shallow-Transfer Machine Translation Rules from Small Parallel Corpora. Journal of Artificial Intelligence Research,
34, 605635.
Skut, W., Brants, T., & Uszkoreit, H. (1998). A Linguistically Interpreted Corpus of German
Newspaper Text. In Proceedings of the ESSLLI Workshop on Recent Advances in
Corpus Annotation, Saarbrucken, Germany.
Smedt, J. D., & Vatanat, B. (2009). https://lists.w3.org/archives/public/public-eswthes/2010feb/att-0023/ontology.html..
Steinberger, R., Pouliquen, B., Widiger, A., Ignat, C., Erjavec, T., Tufis, D., & Varga, D.
(2006). The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages.
In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC2006), Genoa, Italy.
Suchanek, F., Riedel, S., Singh, S., & Pratim Talukdar, P. (Eds.)., (2012). AKBC-WEKEX
12: Proceedings of the Joint Workshop on Automatic Knowledge Base Construction
and Web-scale Knowledge Extraction, Stroudsburg, PA, USA. Association for Computational Linguistics.
Vossen, P. (Ed.). (1998). EuroWordNet: A Multilingual Database with Lexical Semantic
Networks. Kluwer Academic Publishers, Norwell, MA, USA.
Wan, X. (2009). Co-Training for Cross-Lingual Sentiment Classification. In Proceedings
of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pp. 235243,
Singapore.
Wei, B., & Pal, C. (2010). Cross Lingual Adaptation: An Experiment on Sentiment Classifications. In Proceedings of the Annual Meeting of the Association for Computational
Linguistics (ACL), pp. 258262, Uppsala, Sweden.
Yarowsky, D., Ngai, G., & Wicentowski, R. (2001). Inducing Multilingual Text Analysis
Tools via Robust Projection across Aligned Corpora. In Proceedings of the First International Conference on Human Language Technology Research, pp. 18, San Diego,
CA, USA.
Zhang, Y., Jones, G. J., & Zhang, K. (2008). Dublin City University at CLEF 2007:
Cross-Language Speech Retrieval Experiments. Lecture Notes In Computer Science,
703711.
15

fiJournal of Artificial Intelligence Research 55 (2016) 685714

Submitted 11/15; published 03/16

Quadratization and Roof Duality of Markov Logic Networks
Roderick de Nijs

rsdenijs@tum.de

Institute of Computer Science
Albrechtstr. 28, 49076 Osnabruck, Germany

Christian Landsiedel
Dirk Wollher
Martin Buss

christian.landsiedel@tum.de
dw@tum.de
mb@tum.de

Lehrstuhl fur Steuerungs- und Regelungstechnik
Theresienstr. 90, 80333 Munchen, Germany

Abstract
This article discusses the quadratization of Markov Logic Networks, which enables
efficient approximate MAP computation by means of maximum flows. The procedure
relies on a pseudo-Boolean representation of the model, and allows handling models of
any order. The employed pseudo-Boolean representation can be used to identify problems
that are guaranteed to be solvable in low polynomial-time. Results on common benchmark
problems show that the proposed approach finds optimal assignments for most variables
in excellent computational time and approximate solutions that match the quality of ILPbased solvers.

1. Introduction
First-order probabilistic models are a promising paradigm for overcoming the limitations
of classical first-order logic because of their ability to capture the uncertainty often present
in real-world problems. They allow describing relational knowledge compactly, such that
the size of the representation is independent of the number of objects in the domain. The
knowledge in these models can be defined through the use of parfactors (Poole, 2003), which
are templates representing large numbers of factors in a graphical model that describes a
probability distribution over possible world configurations. When the underlying graphical
model is finite, it is possible to ground the first-order model and to perform inference at the
propositional level. For this reason, it is of interest to identify tractable cases of propositional
problems at first-order level as well as finding efficient approximate algorithms for the case
where exact inference is not possible. Although computing typical inference queries on the
propositional model is NP-Hard in general, such problems lend themselves to traditional
optimization approaches.
This article deals with models whose MAP problem can be represented as an optimization over a finite number of binary variables. This is the case for Markov Logic Networks
(MLNs), where the parfactors are weighted logic rules that can be propositionalized to
define a Markov Random Field over Boolean random variables. The contributions in this
work stem in great part from representing the parfactors using Boolean polynomials known
as pseudo-Boolean functions. First, it is shown that for models with certain parfactors,
2016 AI Access Foundation. All rights reserved.

fide Nijs, Landsiedel, Wollherr, & Buss

the MAP computation is equivalent to maximizing a polar or unimodular pseudo-Boolean
function, which can be done in low-polynomial time. This allows to identify MLNs with a
tractable MAP problem. Secondly, it is shown how quadratization techniques for pseudoBoolean functions can be generalized to parfactors. One benefit of these transformations
is that they enable using Quadratic Pseudo-Boolean Optimization (QPBO) on the ground
model, a popular algorithm in computer vision that has not yet been evaluated for MLNs.
The literature of such quadratizations is reviewed, and we show that the previous work
discussing the quadratization of MLNs (Fierens, Kersting, Davis, Chen, & Mladenov, 2013)
is equivalent to a particular choice of pseudo-Boolean quadratization within our quadratization framework for parfactors. Based on the generalized roof duality (Kahl & Strandmark,
2012), a new quadratization is also introduced.
Experimental evaluation of the quadratization techniques in combination with the QPBO
algorithm show that large benefits in performance can be attained on real-world problems,
and that the more sophisticated quadratization techniques deliver better results than the
one employed by Fierens et al. (2013).
The combination of quadratization and QPBO is shown to be a competitive strategy
for approaching the MAP problem in MLNs.
1.1 Outline
The remainder of this article is organized as follows: The rest of this section reviews the
optimization methods available for MLNs and other related work. In Section 2, the mathematical background, Markov Random Fields and pseudo-Boolean functions are presented,
along with the notation used in this article. Section 3 describes the transformation of parfactors to pseudo-Boolean form, and discusses cases that can be identified as tractable.
Section 4 presents a general framework for quadratization of parfactors as well as a comparison with an existing approach for MLNs. Different quadratization strategies are discussed.
In Section 5, a thorough computational evaluation of the approach is performed on datasets
from literature as well as new problems. The article concludes with a discussion of the
methods and results in Section 6.
1.2 Related Work
The use of first-order representations provides a compact and flexible way to encode knowledge into a model in the design phase. The trade-off for this convenience is that the ground
models generally have have large treewidths, making MAP estimation and other common
queries NP-hard in general. Because the ground model may have thousands or millions of
variables and interactions, there is a need for fast and memory-efficient optimization algorithms. This section tries to give an overview over the recent most prominent methods for
inference in these models.
Various algorithms based on heuristic random search have been used to approximate
the MAP solution. The Alchemy system (Richardson & Domingos, 2006) implements
a probabilistic hill-climbing algorithm named MaxWalkSat. A lazy variant of MaxWalkSat (Singla & Domingos, 2006b) was also developed, which, by splitting the network into
an active and inactive part, considerably reduces the memory footprint of the algorithm.
The Tuffy system (Niu, Re, Doan, & Shavlik, 2011) reformulated the algorithm within
686

fiQuadratization and Roof Duality of Markov Logic Networks

a relational database for faster grounding and additional scalability. It is also capable of
detecting weakly connected components in the ground network, which can be used for parallelizing inference. A further extension of the parallelization of inference in MLNs was based
on a partitioning of the network before grounding found using minimum cuts, which is used
in an importance sampling inference framework (Beedkar, Del Corro, & Gemulla, 2013).
An alternative approach is based on the conversion of ground factors to linear constraints,
such that the MAP problem can be formulated as an integer linear program (ILP). One
advantage of this formulation is that the solution of its linear relaxation gives an optimistic
estimate of optimal cost. The cost of the optimal solution then lies between that of any
particular assignment and the optimistic estimate. To make this approach practical for
larger problems, it is necessary to reduce the number of linear constraints to be considered
by the solver. To this purpose, a cutting plane algorithm for MLNs was presented (Riedel,
2009). This iterative approach ignores constraints that have so far been satisfied by previous intermediate solutions, and only includes them if they become unsatisfied in the next
iteration. The RockIt system (Noessner, Niepert, & Stuckenschmidt, 2013) further shows
that structurally similar constraints created by the first-order model can be aggregated into
a single one, and presents a parallelization scheme that splits the problem into multiple
ILPs. The combination of these techniques achieves excellent execution times. A third type
of algorithms do not perform queries on a propositionalized network, but operate instead
on a potentially much smaller lifted network. Algorithms for the lifted MAP have seen
significant advances in recent years (Apsel & Brafman, 2012; Sarkhel, Venugopal, Singla, &
Gogate, 2014; Mittal, Goyal, Gogate, & Singla, 2014). However, these approaches can only
applied efficiently for problems with specific types relations and evidence, and will not be
discussed in this article.

For most of the article, we have used a polynomial representation of the potential functions in FOPMs. The use of polynomials for the representation of Bayesian networks was
suggested under the name of network polynomials (Darwiche, 2003). Such polynomials can
be compiled into arithmetic circuits and used for inference (Huang, Chavira, & Darwiche,
2006). This idea is carried to first-order models to create a tractable subset of Markov
Logic Networks (Domingos & Webb, 2012), whose network polynomial can be used to make
queries efficient.

We have also used the fact that the maximization of any pseudo-Boolean function can
be transformed to a maximization of a quadratic pseudo-Boolean function. Because pseudoBoolean functions can be interpreted as factor graphs, such a transformation can be seen
as a reduction of the MAP problem in a general binary factor graph to a MAP problem
in a pairwise binary factor graph. A thorough study of reductions of inference problems
in general factor graphs to more restricted factor graph models was presented by Eaton
and Ghahramani (2013). Close to our work is also the idea of pairwise MLNs (Fierens
et al., 2013), which relies on a transformation of logical formulas to compute a quadratic
MLN that is equivalent to an original MLN of higher order. A detailed comparison of this
approach to ours is given in Section 4.4.
687

fide Nijs, Landsiedel, Wollherr, & Buss

2. Preliminaries
In this section we review the concepts required for the understanding of the rest of the
article and define useful notational conventions.
2.1 First-Order Logic Concepts
First-order logic makes statements about objects in the world. Each object belongs to a
certain domain, where domains can be seen as the semantic type of the object. References to
objects are made by the use of terms. Terms can be either constants, which refer to a specific
object, logical variables, which can represent a range of objects, or functions, which map
terms to other terms. Like objects, logical variables and constants are typed, meaning that
they represent objects from a certain domain. A predicate represents a relation between its
arguments. A predicate applied to specific terms is an atom. Atoms are also called positive
literals and their logical negation negative literals.
A first-order logic formula is an expression involving atoms, connected through connectives (, , , , , =) and quantifiers (,). Atoms or formulas are said to be a ground
expression if all contained terms are constants.
2.2 Notation and Conventions
An atom P (t1 , . . . , tn ) is created by applying a predicate P of arity n to a tuple of terms
(t1 , . . . , tn ). The arguments of a predicate are typed, each having an associated domain.
Ground atoms are represented by x and literals (first-order or ground) are represented by
u; their negation is written as u. Logical variables (logvars) are denoted with capital letters
X, Y, Z. Vectors of atoms, ground atoms, and terms are denoted by a, x, t, respectively;
for instance, a first-order expression F that involves multiple atoms is written as F (a) =
F (a1 , a2 , . . . , an ). To easily go from Boolean to real values, logical True and False are
reinterpreted to represent 1 and 0 where necessary. Replacing a literal by its negated
equivalent, e.g., u by 1  u, is the complementation operation. Also, the superscript (),
with   B, can be used to specifically refer to the positive and negated literals, e.g., u(1) = u
and u(0) = u. Observe that u() = u(1) .
The substitution of terms from a set T by different terms T 0 according to a mapping
T  T 0 is denoted by . The substitution operation applied to a first-order expression f is
written as f .
A ground substitution is a mapping T  C from non-constant terms T to constant terms
C. Given a set of logical variables L, the set of all possible ground substitutions that satisfy
constraints C is written as gr(L : C). In the case where C = , the number of ground
substitutions is the size of the Cartesian product of the domains of L.
2.3 Markov Random Fields
A Markov Random Field is an undirected graphical model defined as
P (x) =

N
1 Y
i (xi ),
Z
i

688

fiQuadratization and Roof Duality of Markov Logic Networks

where the factors i are nonnegative functions, xi are tuples of binary random variables,
and
is a normalization constant. Normally i are exponential functions, and the quantity
PZ
N
 i log i (xi ) is referred to as the energy function, whose minimum defines the MAP
state of P (x).
2.4 First-Order Probabilistic Models
First-order probabilistic models are a way of expressing probability distributions, such as
MRFs, with a large degree of structure. These models can be conveniently specified using
parfactors (Poole, 2003). Parfactors are composed of a (parametrized) potential function
and constraints on the valid ground substitutions of the parameters. A parfactor g is
represented as a tuple (C, (a)), where C is a set of constraints and (a) is the potential of
the parfactor, a real-valued function on first-order atoms a. The potential function can be
given as a table that associates a value to each of the 2n truth states of its atoms (Poole,
2003; de Salvo Braz, 2007) or as a function. Each ground substitution then defines a factor
or clique potential in a Markov Random Field or Bayes Network. The logical variables that
appear in a parfactor are denoted as L, and LV (a) is used to specifically refer to the logical
variables that appear in the atoms. The set of valid ground substitutions of a parfactor with
logical variables L and for constraints C (discussed in Section 2.4.2) is denoted by gr(L : C).
For our purposes, a set of parfactors G defines a Markov Random Field in log-linear form
through the sum of all of its groundings as


X
X
1
P (x) = exp 
g (ag ) ,
(1)
Z
gG gr(Lg :Cg )

where x contains the propositional variables that arise from grounding the first-order atoms.
Markov Logic Networks are first-order probabilistic models that use a set of weighted
first-order logic rules to specify a Markov Random Field. The rules are generally specified
manually and capture the available knowledge or intuitions about the domain. The weights
capture the relative importance of the rules and can be set manually or be learnt from data.
Markov Logic Networks can be easily described using parfactors. For this, each weighted
first-order logic rule in a Markov Logic Network is associated to a parfactor with an empty
constraint set and a potential function that takes the value of the weight for satisfying
assignments to the rule, and 0 otherwise. For these models, (1) assigns high probabilities
to world states that satisfy many ground parfactors with a positive weight and few with a
negative weight.
The most common types of queries to (1) are the marginal probabilities for each of the
propositional variables (P (x1 ), P (x2 ), . . .) and the most probable configuration of the unknown variables x = arg maxx P (x), also referred to as the Maximum-a-Posteriori (MAP)
assignment.
In order to be able to ground the models, we take similar assumptions to the original
MLN formulation, namely that domains are assumed to be finite, unique names and domain closure. In practice these also allow to assume the logical atoms to be function-free.
However, although potential functions in MLNs are {0, w}-valued, w  R, our formulation
allows potentials to take different values in R for every assignment.
689

fide Nijs, Landsiedel, Wollherr, & Buss

2.4.1 Evidence
The model may be conditioned on the truth value of certain ground atoms. The symbols
PT and PF represent the sets of ground atoms for a predicate P that are known to be True
and False, respectively. For instance, P (o)  PT denotes that the ground atom P (o) is
known to be True. In case no evidence is available, PT and PF are empty. The set of
ground atoms with predicate P and unknown truth value is represented by PU . If PU is
empty, the predicate is fully observed.
2.4.2 Constraints
The ground substitutions for the logical variables in a parfactor can be subject to constraints.
In our representation, the substitution constraint for each parfactor is the conjunction of a
set of individual constraints associated with that parfactor. Disjunctions of constraints can
be expressed through multiple parfactors under this representation. Constraints are used
for the following cases:
1. Expressing the (in)equality relation between logical variables. For two logical variables
X, Y belonging to the same domain, X = Y and X 6= Y respectively restrict the
ground substitutions to those that map X and Y to either the same or different
constants.
2. Expressing that ground substitutions must map logical variables t to elements in a
set of objects P is denoted as C (t, P ), where P generally is one of the evidence
groups PT , PF , PU for the predicate P .
Constraints can in principle also be expressed in the potential function using fully observed
auxiliary predicates, as normally done in the MLN formalism. For instance, X 6= Y can be
enforced by taking the conjunction between the potential of the parfactor and a new fully
observed atom AreDifferent(X, Y ). Similarly, the constraint C (X, PF ) can be represented
by taking the conjunction of the potential with a new atom BelongsToFalseEvidenceP (X).
However, expressing these relationships in the form of constraints simplifies the discussion
in Section 3.
Example 1. That friends have similar smoking behavior can be described by a parfactor
g with potential function F riends(X, Y )  Smokes(X)  Smokes(Y ) and constraint X 6=
Y . For people domain {A, B}, the ground substitutions associated to the parfactor are
gr((X, Y ) : {X 6= Y }) = {(X, Y )  (A, B), (X, Y )  (B, A)}.
2.5 Pseudo-Boolean Functions
A function f : Bn  R is called a pseudo-Boolean function. Let x = [x1 , x2 , . . . , xn ]
(1)
(1)
be a vector of n binary variables. Consider also the set of literals, L := {x1 , . . . , xn ,
(0)
(0)
x1 , . . . , xn }. A pseudo-Boolean function where terms are expressed with literals in L and
coefficients ei  R, i = 0, . . . , n can be written as
(x) = e0 + e1 m1 (x1 ) + e2 m2 (x2 ) + . . . + en mn (xn )
690

(2)

fiQuadratization and Roof Duality of Markov Logic Networks

where mi (xi ) are monomials over the literals in L,
mi (xi ) :=

Y

(

)

xi,ji,j .

j

If ei  0 for 0 < i  n in (2), the pseudo-Boolean function is said to be a posiform.
There are many possible posiform representations for a pseudo-Boolean function. Just as
with standard polynomials, the order (or degree) of a pseudo-Boolean function is that of
the term with highest degree. If the pseudo-Boolean function is expressed only over the set
of positive literals, i.e. i = 1 for all i, it is called a multi-linear polynomial representation
X
X
X
eij xi xj +
eijk xi xj xk + . . . ,
ei xi +
(x) = e0 +
in

1i<jn

1i<j<kn

where ei , eij , eijk , . . .  R. This representation is unique and can always be obtained from
another representation by eliminating the negated literals using the complementation x(0) =
1  x(1) . Conversely, a posiform can always be obtained for any pseudo-Boolean function.
Complementing any literal in a term ei mi (xi ) with ei < 0 produces two new terms, one
of the same order with e0i > 0 and one of one order lower and e00i < 0. By applying this
procedure starting at the highest-order terms, all negative terms can be eliminated.

3. Parfactors with Pseudo-Boolean Potentials
This chapter describes how the potential functions of parfactors can be described and manipulated in terms of pseudo-Boolean functions, and how equivalent model representations
can be translated to the pseudo-Boolean formulation. This representation allows easy recognition of some cases in which inference in the ground probabilistic model is tractable. These
are detailed in Section 3.3.
3.1 First-Order Pseudo-Boolean Functions
Potential functions in parfactors are defined over first-order atoms. Therefore, we call
a pseudo-Boolean function over first-order atoms a first-order pseudo-Boolean function.
Substitutions are applied to each individual term, so that for a pseudo-Boolean function
(a) in form (2) and a substitution 
(a) = e0 + e1 m1 (a1 ) + e2 m2 (a2 ) +    + en mn (an ).
Remark 1. The notions of term and order are different for pseudo-Boolean functions and
in first-order logic. However, we expect the context to generally be clear enough to avoid
confusions.
3.2 Conversion from Other Potential Representations
In general, a potential function with n variables given in the form of a table can be converted
to pseudo-Boolean form by a simple technique (Boros & Hammer, 2002), which creates one
term for each of the at most 2n configurations with a nonzero coefficient in the table.
The number of terms can then be reduced by converting the pseudo-Boolean function to
691

fide Nijs, Landsiedel, Wollherr, & Buss

a multi-linear polynomial. However, the potentials of Markov Logic Networks are given as
first-order logic sentences and can be expressed in conjunctive normal form. This allows
them to be translated directly into pseudo-Boolean form. Namely, each clause U can be
replaced by
_
^
u=1
u,
(3)
uU

uU

and all conjunctions can be replaced by products. For the typical case of a single-clause
formula u1  u2 . . .  un that takes a value w when satisfied, the equivalent compact pseudoBoolean representation is w  wu1 u2 . . . un . Of course, the inverse procedure can be used
to transform a potential in this form back to a logic representation.
The Markov Random Field that results from grounding a model like (1) can be represented as
P (x) =

1
exp (T (x)) ,
Z

where the energy function T is obtained by summing the groundings of the parfactors in
G, and the normalization constant Z is unknown.
3.3 Tractable Classes
The tractability of the MAP estimate of an MLN can be analyzed by considering classes of
tractable pseudo-Boolean functions. First, we discuss classes of polynomial-time optimizable
pseudo-Boolean functions and then the general case.
3.3.1 Classes of Tractable Pseudo-Boolean Functions
This section gives an overview of the relevant tractable classes of pseudo-Boolean functions.
Figure 1 illustrates the relations between these function classes.
 Supermodular functions satisfy f (x1 ) + f (x2 )  f (x1  x2 ) + f (x1  x2 ), for elementwise AND/OR operations and binary x1 , x2 . These functions are maximizable in
strong polynomial time (Orlin, 2009). However, the recognition of supermodularity
for polynomials of order  4 is co-NP-Complete (Gallo & Simeone, 1989).
 Supermodular functions expressible over quadratic functions can be written as maximizations over auxiliary variables of quadratic supermodular functions. For instance,
x1 x2 x3 = maxw x1 w + x2 w + x3 w  2w, with w  B. For functions with a certain
structure, expressibility can be recognized efficiently (Zivny & Jeavons, 2008). However, it is unknown whether the recognition of expressible functions is easier than the
general supermodularity recognition problem (Zivny, Cohen, & Jeavons, 2009). One
may try to obtain the equivalent quadratic supermodular function by solving a linear
program (Ramalingam, Russell, Ladicky, & Torr, 2011).
 Polar functions (Billionnet & Minoux, 1985) are supermodular functions where each
term has a positive coefficient and is composed of only positive or only negative literals,
e.g., x1 x2 x3 + 2x1 x2 + x2 x4 x5 . They form a strict subset of the set of expressible
functions for orders  4.
692

fiQuadratization and Roof Duality of Markov Logic Networks

 Unimodular functions are functions that can be converted to polar functions by switching a subset of the variables. For instance, f (x1 , x2 , x3 ) = x1 x2 x3 + 2x1 x2 can be
2 using the switched
associated to the polar function g(x1 , x2 , x3 ) = x1 x2 x3 + 2x1 x
variable x2 = 1  x2 . Undoing the switching operation on the maximizer of this polar
function gives a solution to the original problem. Unimodular functions are recognizable in polynomial time (Crama, 1989). If the switching operations can be used to
make the function supermodular (but not polar) it is a permutable supermodular function (Schlesinger, 2007), which is mainly interesting for the optimization of functions
with nonbinary discrete variables.
Remark 2. If f is a supermodular function, f is submodular, and thus the results for the
maximization of a supermodular function and the minimization of a submodular function are
interchangeable. In the overview given in this section, we keep the context of maximization
of supermodular functions, since it reflects that of many of the original publications.
Permutable Supermodular
Supermodular

Permutable Supermodular

Expressible

Unimodular
Expressible

Polar

Polar
Supermodular

Unimodular

Figure 1: Relations between classes of functions. Left: For pseudo-Boolean functions of
any order. Right: The third-order (cubic) case. Shaded regions are optimizable
through maximum flows.

The main interest for expressible supermodular functions is that quadratic supermodular
functions can be optimized by computing a maximum flow in O(n3 ), which is considerably
faster than the O(n6 ) complexity of general supermodular maximization. However, the
process of transforming an expressible supermodular function into an equivalent quadratic
representation introduces additional variables. Consequently, if k auxiliary variables are
introduced, the true complexity is O((n + k)3 ).
Note that although the recognition of general supermodularity and expressibility are
hard problems, these classes are closed under conical combinations, such that sums of
supermodular functions are also supermodular.
3.3.2 Tractable Parfactor Models
Although inference is NP-hard in general, it is possible to guarantee that certain inference tasks are tractable by restricting the expressiveness of the formalism of the potential functions (Domingos & Webb, 2012). Translating results for some of the classes of
pseudo-Boolean functions above, it is possible to easily identify models with tractable MAP
inference. For this, the following result is used
Proposition 1. If all parfactors have expressible potentials, the maximization of the ground
model can be expressed as a maximization over a supermodular quadratic function.
693

fide Nijs, Landsiedel, Wollherr, & Buss

Proof. It follows from the facts that a) by definition, expressible functions can be written
as a maximization over a supermodular quadratic pseudo-Boolean function and b) sums of
supermodular functions are also supermodular.
Consequently, if all potentials can be converted to a supermodular expressible pseudoBoolean function, then the model can be optimized by computing a maximum flow.
This result applies to any potential, not only the {0, w}-valued potentials used in MLNs.
It is hence possible to consider any potentials defined over a table and taking up to 2n
distinct values. However, this flexibility makes it harder to enforce expressibility at design
level. Some classes of logic rules used in MLNs can be guaranteed to be expressible, and
can be used to recognize or design MLNs with a tractable MAP problem.
3.3.3 Polar MLNs
Let S = {a1 , a2 , . . . , an } be a set of first order atoms, I and J  {1, 2, . . . , n} and   B.
The logic potentials of the form
^ () ^ (1)
ai 
aj
.
(4)
iI

jJ

can be transformed to an equivalent polar posiform for the following cases
 J = . In this case (4) is a single conjunction and its pseudo-Boolean form is
Y ()
ai .
iI

 I  J 6= . Here the conjunctions are in conflict. In this case the disjunction can
be replaced by a sum without changing the underlying truth table of the expression.
The equivalent polar posiform is
Y () Y (1)
ai +
aj
.
iI

jJ

 I  J =  and |J|= 1. It is easy to see that
^ ()
^
(1)
ai  aj
=
iI

()

ai

(1)

 aj

,

iI{j}

which can be employed to transform the expression to the previous case.
MLNs for which all parfactors are polar have a tractable MAP problem that can be
computed through a maximum flow.
Example 2. An MLN potential with positive weight and potential (P (X)  Q(Y )  T (Z)) 
(R(X)  T (Z)) is of the form (4) with  = 1, I = {P (X), Q(Y ), T (Z)}, J = {R(X), T (Z)}.
Because I  J = {T (Z)} 6= , it has the equivalent polar potential P (X)Q(Y )T (Z) +
R(X) T (Z).
694

fiQuadratization and Roof Duality of Markov Logic Networks

3.3.4 Unimodular MLNs
Unimodular functions can also be solved efficiently once converted to polar functions. Finding the variables that need to be switched to obtain a polar function can be performed in
polynomial-time (Crama, 1989). This recognition procedure may be more efficient if performed on first-order level, but would require to describe switches of the ground atoms
before grounding. Here we show that the switches of some of the ground variables can also
be represented compactly on first-order level, which makes it possible to decide whether a
model is unimodular without analyzing the ground model. Define the switching operation
as replacing a literal l = P (t)() by expression P (t)(1) , where P is a new predicate. The
new literal with the switched predicate is interpreted as referring to switched ground atoms.
However, the representation may become inconsistent if the ground atoms represented by
a switched and a non-switched atom intersect, as a ground atom and its switch can not be
treated as independent variables.
Definition 1 (Shattering). (de Salvo Braz, 2007) A set of parfactors G is shattered if the
groundings of every pair of atoms appearing in the parfactors of G are either identical or
disjoint.
A model can be shattered by partitioning the parfactors (de Salvo Braz, 2007).
Definition 2 (Consistent switch). A shattered model with switched atoms is consistent if
for every pair of atoms with identical groundings, either both or neither of the atoms are
switched.
In a shattered model, inconsistencies can be avoided by ensuring that each switching
operation preserves the consistency. Namely, starting from a shattered model with no
switched atoms, switching atoms with the same groundings ensures that the consistency
requirement is fulfilled at all times. If for some switching sets all parfactors are polar, the
ground model is unimodular and a polar representation is directly available.
Example 3. Consider a model G with parfactors (, P (X)Q(Y ) T (Z)) and (,
Q(X)T (Y )). This model is shattered and we can obtain the switched model G0 with parfactors (, P (X)Q(Y ) T (Z)) and (, Q(X) T (Y )). Because after switching the terms in the
parfactors have either only positive or only negative literals and assume only nonnegative
values, the ground model is a polar pseudo-Boolean function.
Unfortunately, the shattering condition is not a strong enough partitioning of the parfactors to guarantee that any unimodular model can be recognized. For example, the model
with the single parfactor ({X 6= Y },  F (X, Y )F (Y, X)) is shattered and can be shown to
have a switching set that makes it polar, but it can not be made polar by switching the
atoms. A finer partitioning of the parfactors than the one given by shattering may allow to
represent the desired switching set. However, how to find such a partitioning in the general
case is unknown.
3.4 Non-tractable Case
In the following we discuss the optimization of pseudo-Boolean functions that do not fall
within the described tractable classes. It has been observed that multiple linear programming relaxations of quadratic pseudo-Boolean problems have the same optimum (Boros &
695

fide Nijs, Landsiedel, Wollherr, & Buss

Hammer, 2002), known as the roof dual bound. This is generally an optimistic bound to
the true optimum of the problem, as linear programming relaxes the integrality constraint
on the variables. The roof dual bound can also be obtained very efficiently by solving
a maximum flow problem on a specially constructed network. In a minimization setting,
this network represents the tightest submodular relaxation of the original quadratic function (Kahl & Strandmark, 2012). In addition to a lower bound, the solution to the relaxed
problem gives persistencies. Persistencies are assignments to a subset of the variables that
form part of at least one optimal solution. If persistencies are found for the full set of
variables, they form a minimizer of the problem. Otherwise, the problem can be simplified
by fixing the persistencies to their value to produce a smaller problem that preserves the
minimum.
This preprocessing step reduces the size of the problem or solves it completely, and can
be combined with any other optimization method. This approach is known as Quadratic Pseudo-Boolean Optimization (QPBO) (Kolmogorov & Rother, 2007) in the computer
vision literature.
Furthermore, the same network that is used to compute the initial roof dual bound
and the persistencies can be used to search for additional persistencies or an approximate
configuration for the remaining variables by means of the probing (Boros, Hammer, &
Tavares, 2006) and improving (Rother, Kolmogorov, Lempitsky, & Szummer, 2007) techniques. Probing heuristically chooses a variable x from the residual problem and recomputes
the maximum flow under the assumptions of x = 0 and x = 1. Analyzing the value of the
remaining variables under each of these assumptions, it may be possible to find additional
persistencies and improve the lower bound. The improve method fixes a subset of the variables to any given assignment and efficiently searches for configurations of the remaining
variables that improve the cost with respect to the original assignment. This procedure can
be executed multiple times and is guaranteed to not decrease the quality of the approximate
solution.
The described techniques are only applicable for quadratic problems. The next section
is concerned with models that can not be described by purely pairwise interactions between
the variables. For such models, the use of quadratization techniques can be employed,
which enables the use of the QPBO algorithm and its extensions on such problems. Also,
a connection between this approach and an alternative based on generalized roof duality is
shown in Section 4.2.

4. Quadratization
In this section the quadratization of pseudo-Boolean functions is reviewed and it is then
shown how these methods can be applied to parfactor models. The quadratization procedure
is also known as order reduction.
4.1 Quadratization of a Pseudo-Boolean Function
A quadratization of a pseudo-Boolean function (x) is a new function (x, w) such that
(x) = min (x, w),
w

696

(5)

fiQuadratization and Roof Duality of Markov Logic Networks

where (x, w) is a quadratic pseudo-Boolean function defined over auxiliary slack variables
w.
For any (x), there always exists a (x, w) satisfying (5) (Rosenberg, 1975; Ishikawa,
2011). Furthermore, if (x) belongs to the expressible set of submodular functions described
by Zivny et al. (2009), a submodular (x, w) can also be found. After quadratization, the
problem of finding minx (x) is replaced by the quadratic problem minx,w (x, w).
Example 4. The function f (x1 , x2 , x3 ) = x1 x2 x3 has a quadratization
min (x1 , x2 , x3 , w) = min x1 w  x2 w  x3 w + 2w.
w

w

This can be verified by minimizing (x1 , x2 , x3 , w) with respect to w for each assignment
of (x1 , x2 , x3 )  B3 and verifying that it evaluates to 1 at (1, 1, 1) and 0 everywhere else.
Now minx1 ,x2 ,x3 f (x1 , x2 , x3 ) can be conveniently computed through the quadratic optimization minx1 ,x2 ,x3 ,w (x1 , x2 , x3 , w). This quadratization is obtained with the ISH technique
discussed in the following.
In general, a quadratization is not necessarily determined for the whole function at once;
instead, single or multiple terms with a specific algebraic form can be replaced at each step
with an equivalent quadratic representation. Because each of these substitutions introduces
a minimization over independent slack variables, these minimizations can be distributed
over the whole expression, such that the final quadratization is a joint optimization over all
slack variables.
For a given (x) there are many possible ways to obtain a quadratization. In practice,
quadratizations should a) have few slack variables, b) have few positive quadratic and
thus non-submodular terms and c) be easily computable. Condition a) is important if the
quadratization needs to be applied many times, e.g., if there are many higher order terms;
b) their number and magnitude is experimentally known to correlate with the complexity of
the resulting optimization (Kolmogorov & Rother, 2007; Gallagher, Batra, & Parikh, 2011);
c) to satisfy the above conditions, finding a quadratization may become more complex. For
instance, finding the quadratization with the smallest number of slack variables is an NPcomplete problem for some approaches (Boros & Hammer, 2002).
4.2 Quadratization Techniques
Quadratization techniques for pseudo-Boolean functions rely on identifying subexpressions
that match a template for which a quadratic form is known. Potentials that are known to
be expressible can be quadratized using the submodularity-preserving functions presented
by Zivny and Jeavons (2008). In the following we review existing techniques valid for any
pseudo-Boolean function and introduce a new quadratization.
4.2.1 General Quadratization for Individual Terms (ISH)
This technique can be used to quadratize any individual higher-order term. The general
formulas are given by Ishikawa (2011), and more restricted cases are given by Kolmogorov
and Zabin (2004) and Freedman and Drineas (2005).
697

fide Nijs, Landsiedel, Wollherr, & Buss

For binary variables x1 , . . . , xd and negative coefficient (a < 0),
(1 )

ax1

(d )

   xd

()

= min aw{S1
wB

 (d  1)}.

Example 4 illustrates this case for the cubic function. For a positive coefficient (a > 0) the
quadratization is
( )
( )
ax1 1    xd d

=a

nd
X

min

w1 ,...,wnd B

()

wi (ci,d (S1

()

+ 2i)  1) + aS2

i=1

with the definitions
()
S1

=

d
X

( )
xi i ,

()
S2

i=1

=

d1 X
d1
X

( ) ( )
xi i xj j

i=1 j=i+1

(

1,
d1
, ci,d =
nd =
2
2,


()

()

S (S1  1)
= 1
2
if d is odd and i = nd
otherwise

The reduction of a negative term can always be performed with a single slack variable,
and results in a quadratic submodular function. For positive terms, the number of slack
variables nd is proportional to 21 d. In general, the resulting function is not submodular
because the quadratization creates positive quadratic terms. However, because these terms
do not involve slack variables, they may be canceled by existing quadratic terms of opposite
sign. In fact, it can be seen that this quadratization is always submodularity-preserving
for cubic functions (Zivny & Jeavons, 2008). On the other hand, if the original function is
not submodular, this quadratization may produce more non-submodular terms and worse
experimental results than other methods.
4.2.2 Asymmetric Quadratization (ASM)
(Gallagher et al., 2011) An asymmetric quadratization for a term ax1 x2 x3 with a > 0 is
given by
ax1 x2 x3 = min a(w  x2 w  x3 w + x1 w + x2 x3 ).
w

This can be obtained by transforming ax1 x2 x3 into ax2 x3 ax1 x2 x3 and then using the ISH
method. Observe that the left-hand side of the equality is symmetric, but the right-hand
side is not. Reordering the variables thus creates three different quadratizations. Also,
the right-hand side only has two non-submodular terms (compared to three with the ISH
method), one of which does not involve the slack variable w. If there is an existing term with
a negative coefficient and the same variables as the non-submodular term without w, these
can be combined or may cancel out, leaving just a single non-submodular term. Asymmetric
quadratizations can be created for terms of any order using a similar procedure.
The possibility of eliminating non-submodular terms motivates the search for the combination of quadratizations that optimizes a certain cost representing the quality of the
quadratization. It has been shown that the total magnitude of the non-submodular terms
is a good cost function (Gallagher et al., 2011). We call ASM the procedure of minimizing
698

fiQuadratization and Roof Duality of Markov Logic Networks

this quantity when finding a quadratization for an expression, where each term can select
between one of the asymmetric or the ISH quadratic forms. Note that, on propositional
level, the number of such terms may be very large, resulting in a large optimization problem
for the choice of quadratization per term.
4.2.3 Preprocessing for Positive Terms (FIX)
(Fix, Gruber, Boros, & Zabih, 2011) This approach can be seen as a preprocessing method to
avoid the quadratization ISH for positive terms. It transforms multiple positive higher-order
terms with a common subset of variables into negative higher-order terms. We summarize
this procedure for the case where the common subset of variables consists of a single variable.
Consider a set of terms S that contain a common variable x1 , where each H  S has a
positive coefficient H > 0. Then, it holds that for any assignment to the binary variables
x1 , . . . , x n
!
X
HS

H

Y
jH

xj = min

X

w{0,1}

H

X

x1 w +

HS

HS

H

Y

xj 

jH\{1}

X
HS

H w

Y

xj .

jH\{1}

Observe that the last sum is over terms that have the same order as the left-hand side but
with negative sign, and that all positive terms have a lower order. This transformation
is repeatedly applied until all positive higher-order terms have been eliminated. At this
point the ISH quadratization for negative terms is applied, introducing one additional slack
variable per term. There is no method to decide on what common variable to perform the
transformation, and x1 is thus selected arbitrarily.
4.2.4 Generalized Roof Duality (GRD)
This new quadratization method is based on the generalized roof duality theory (Kolmogorov, 2012) and its practical implementation (Kahl & Strandmark, 2012). For a nonsubmodular, higher-order function (x), the approach finds a submodular relaxation  (x, y)
for which
(x) =  (x, x)
 (x, y) =  (y, x)

(symmetry)

 is submodular and expressible.

(6)

Under these constraints, the solution of the relaxed problem provides a lower bound and
persistent assignments for some variables, similar to the roof dual relaxation of a quadratic
function. By restricting the search for relaxations over expressible functions, the result
can be optimized by solving a maximum flow problem. The following proposition presents
a link between the submodular relaxations computed by GRD and quadratizations. The
proposition can be used to compute a quadratization of a function such that its roof dual
bound is equivalent to the GRD bound of the function.
Proposition 2. Let  (x, y) be a relaxation of (x) that satisfies (6). Then there exists a
quadratic (x, w) s.t. a) roofdual((x, w))  minx,y  (x, y), and b) minw (x, w) = (x).
699

fide Nijs, Landsiedel, Wollherr, & Buss

Proof. Let (x, y, w) be a submodular quadratization of  (x, y). Define
1
0 (x, y, w, v) = ((x, y, w) + (y, x, v)).
2
It is easy to check that minw,v 0 (x, y, w, v) =  (x, y). Also, because (x, y, w) is quadratic and submodular, it is easy to see that (y, x, v) must also be submodular (negative
quadratic terms in multi-linear form). Consequently, 0 (x, y, w, v) is submodular.
a) Let (x, w) = 0 (x, x, w, w) and verify that
0 (x, y, w, v) = 0 (y, x, v, w)

Thus, 0 (x, y, w, v) is a relaxation of (x, w) under conditions (6). However, the roof dual
bound is known to be larger than all other submodular relaxations (Kahl & Strandmark,
2012),
roofdual((x, w))  min 0 (x, y, w, v) = min  (x, y).
x,y,w,v

x,y

b) follows from
1
min (x, w) = min 0 (x, x, w, w) = min ((x, x, w) + (x, x, w)) =  (x, x) = (x).
w 2
w
w

Condition b) in Proposition 2 ensures that (x) is a quadratization of (x), and condition
a) ensures that the roof dual bound of this quadratization is at least that of the relaxation
 (x, y).
Example 5. For a cubic term x1 x2 x3 we can compute a third-order submodular relaxation using (Kahl & Strandmark, 2012). Because the ISH quadratization is submodularitypreserving for cubic functions, this method can be used to make the relaxation quadratic,
and then the procedure from Proposition 2 results in
1
(w1 x1 + w1 x2  w1 x3  w1  w2 x1  w2 x2 + w2 x3
2
+ w2 + 2x1 x2  x1  x2 + x3 + 1).

x1 x2 x3 = min

w1 ,w2

The relaxation obtained using the generalized roof duality theory can be expressed as a
quadratization of the function. In practice, this method allows us to use the GRD approach
to design quadratizations that are specially suited for the potentials at hand.
4.3 Quadratization of MLNs
A quadratization of an MLN expresses its probability distribution using only parfactors
with quadratic pseudo-Boolean potentials and an optimization over slack atoms.
Definition 3 (First-order quadratization). Let (a) be a pseudo-Boolean potential  applied
to first-order atoms a, and construct l = (l1 , l2 , . . . , l|LV (a)| ), li  LV (a) with an arbitrary
ordering. From a quadratization  for , such that (x) = minw (x, w1 , w2 , . . . , wk ) we
700

fiQuadratization and Roof Duality of Markov Logic Networks

define the first-order quadratization as (a) = minb (a, b1 , b2 , . . . , bk ), where bi , for i =
1, 2, . . . , k are slack atoms. Slack atoms are created using a new predicate Bi with arity
|LV (a)| for every slack variable wi as bi = Bi (l1 , l2 , . . . , l|LV (a)| ), with i = 1, 2, . . . , k.
This definition allows to compactly represent the individual quadratizations of many
ground potentials with a similar structure. Also, because no particular form of the original
or quadratic representation is assumed, any quadratization technique can be used. For instance, applying the ISH quadratization on the first-order potential P (X)Q(X, Z)Q(Y, Z),
as in Example 4, results in the quadratization P (X)B1 (Y, X, Z)  Q(X, Z)B1 (Y, X, Z)
Q(Y, Z)B1 (Y, X, Z) + 2B1 (Y, X, Z).
To obtain a convenient quadratization of the whole model the following remarks are
useful.
Remark 3. Because each quadratization is performed with new first-order slack predicates,
different parfactors are never grounded to expressions with the same ground slack atom.
Remark 4. Slack predicates are applied to all logical variables appearing in the expression
that is quadratized. Consequently, two ground substitutions , 0 that create the same ground
slack atoms also define minimizations over the same function: b = b0  (a, b) =
(a, b)0 .
With the definition of first-order quadratizations, we can express the quadratization of
the pseudo-Boolean probabilistic first-order logic model with parfactors G as


X
X
1
P (x) = exp 
(ag )
Z
gG gr(Lg :Cg )


X
X
1
= exp 
(ag )
Z
gG gr(Lg :Cg )


X
X
1
= exp 
min g (ag , bg )
bg
Z
gG gr(Lg :Cg )


X
X
1
= exp 
min g (ag , bg )
(7)
bg 
Z
gG gr(Lg :Cg )


X
X
1
= exp  min
g (ag , bg ) ,
(8)
w
Z
gG gr(Lg :Cg )

which can be optionally expressed as a maximization


P (x) =

1
exp max
w
Z


X

X

gG gr(Lg :Cg )

701

g (ag , bg ) ,

fide Nijs, Landsiedel, Wollherr, & Buss

where w contains all ground slack variables {bg |  gr(Lg : Cg ), g  G}. The step
from (7) to (8) follows because the sets of variables produced by different grounding substitutions are either identical or disjoint. For different parfactors, the minimizations are
always independent, as implied by Remark 3. For a single parfactor g and two groundings minbg  (ag , bg ) and minbg 0 (ag 0 , bg 0 ), either bg  6= bg 0 or bg  = bg 0 . In
the first case, we can combine two independent minimizations as minbg ,bg 0 (ag , bg ) +
(ag 0 , bg 0 ). In the second case, Remark 4 implies minbg  2(ag , bg ).
4.4 Relation to Pairwise MLNs
The concept of pairwise MLNs (Fierens et al., 2013) and the one presented here both
produce a new model with quadratic parfactors, at the cost of introducing an additional
optimization over slack variables. The pairwise MLN approach shows that any MLN can be
transformed to a max-equivalent MLN with only quadratic clauses, where the concept of
max-equivalence is similar to that of (5). To review this approach, first assume that there
are no clauses with more than three literals. In this case, each clause with three literals
is rewritten into new clauses in positive normal form, which are conjunctions of the three
positive literals. From this form, a transformation is suggested that splits the conjunction
of the three positive literals into new rules that involve only up to two atoms and use a new
auxiliary slack atom. For this third-order case, the procedure described above is a special
case of our approach. Namely, we show that it is equivalent to using the ISH quadratization
in Section 4.2.1 on an MLN in pseudo-Boolean form.
Consider an MLN rule with weight  > 0 as the parfactor (, (, P (t1 )Q(t2 )R(t3 ))).
The pairwise MLN approach would transform this into the parfactors with quadratic potential functions
{(, P (t1 )  B(t4 )), (, Q(t2 )  B(t4 )), (, R(t3 )  B(t4 )), (2, B(t4 ))}.

(9)

The parfactor can also be represented in pseudo-Boolean form as (, (P (t1 )Q(t2 )R(t3 ))).
This allows to use the ISH quadratization presented in Example 4 as
P (t1 )Q(t2 )R(t3 ) =  (P (t1 )Q(t2 )R(t3 ))
=  min (P (t1 )B(t4 )  R(t3 )B(t4 )  Q(t2 )B(t4 ) + 2B(t4 ))
B(t4 )

= max (P (t1 )B(t4 ) + R(t3 )B(t4 ) + Q(t2 )B(t4 )  2B(t4 )).
B(t4 )

It can observed that the each of the terms of the potential function correspond one-toone to the ones created by the pairwise MLN approach in (9). In fact, decomposing this
parfactor into new ones for each of the quadratic terms would then give the same form as
in pairwise MLN. An analogous procedure for the case where  < 0 shows that for the
special case with three literals, pairwise MLNs are replicated in pseudo-Boolean form by
expressing the formulas in multi-linear form and then using the ISH quadratization for the
third-order case.
Although the ISH quadratization is often recommended in the computer vision literature
for the case where  > 0 (in a maximization setting), it is known not to lead to the
tightest roof dual relaxations when  < 0 (Fix et al., 2011; Gallagher et al., 2011). In the
702

fiQuadratization and Roof Duality of Markov Logic Networks

experimental section we extend such observations to problems in the domain of probabilistic
logic and verify that other quadratization methods lead to improvements in inference quality.
If the model contains clauses with n > 3 literals, the pairwise MLN approach suggests to
use auxiliary atoms to recursively transform such clauses into new ones with n  1 literals,
until no clause has more than three literals. For instance, a clause with n = 4 literals is
transformed into two clauses with three literals, one of which has an infinite weight. At this
point, the procedure from above can be applied to each of the third-order clauses, creating
a total of 3 slack atoms.
In contrast, our approach works on the pseudo-Boolean representation of the potential.
In this case, the additional preprocessing to first transform the clause to third-order is not
required, as there are quadratizations for pseudo-Boolean functions of any order. Recall that
clauses can be compactly represented in pseudo-Boolean form using (3). Consequently,
  the
ISH method can be used to quadratize clauses with n literals with either one or n2 slack
atoms, depending on the sign and whether the optimization is formulated as a maximization
or a minimization. For the particular case of a clause with four literals only a single
slack atom is required. Because each slack atom can have many propositionalizations,
reducing the number of slack atoms creates a large difference in the number of propositional
slack variables. Finally, our approach does not create hard rules, which can lead to bad
conditioning for some optimization algorithms.
4.5 Normalization of the Parfactors before Quadratization
Consider the parfactor (, P (X)P (Y )Q(Z) + P (Z)Q(Z)) and true evidence for predicate
Q, QT 6= . Although the potential is cubic in general, there are particular ground
substitutions for which it is quadratic. This is the case for all groundings for which
Q(Z)  QT , as for those cases the potential simplifies to P (X)P (Y ) + P (Z). The potential is also quadratic for ground substitutions for which X = Y , because it then
becomes P (X)Q(Z) + P (Z)P (Z). Finally, groundings for which Y  = Z have the cubic
potential P (X)P (Z)Q(Z) + P (Z)Q(Z), which factorizes to (P (X) + 1)P (Z)Q(Z). For such
groundings, a quadratization exploiting the structure of the potential could be computed.
These examples highlight that the structure of the potentials can change for particular
groundings, and that some might not even require slack variables to become quadratic. This
motivates putting the parfactors in a form for which the potentials have the same structure
for all groundings. The effect of such a normalization is a reduction in the number of slack
variables and the possibility to compute quadratizations tailored to each different form of
the potentials. This can be achieved by combining two types of splittings of the parfactors,
creating an equivalent first-order representation that makes different forms of potentials
explicit at first-order level.
4.5.1 Splitting on Atoms
The first preprocessing method simplifies a parfactor by incorporating evidence for an atom
P (t). It proceeds by replacing the parfactor with three new ones, each with an additional
constraint C (t, PT ), C (t, PF ), C (t, PU ). Because together these constraints cover all possible ground substitutions for the atom, the new parfactors produce the same groundings
as the original one, and the ground model is not changed. Importantly, the potentials of
703

fide Nijs, Landsiedel, Wollherr, & Buss

the parfactors with constraints C (t, PT ), C (t, PF ) can be simplified by replacing the atom
P (t) by True and False, respectively.
Definition 4 (Fixed atoms). An atom a with predicate P is fixed if C (t, PT ), C (t, PF )
or C (t, PU ) is in the constraint set C.
For a parfactor with n atoms, fixing all atoms can create up to 3n new parfactors. In
practice, the number is smaller because a) the potential may become 0 or constant after
the simplification (Shavlik & Natarajan, 2009) or b) some of the sets PT , PF , PU may be
empty and have no groundings associated to it.
4.5.2 Splitting on the Logical Variables
Atoms constructed with the same predicate may ground to the same ground atom under
specific grounding substitutions. For example, the parfactor (, P (X)P (Ann)) represents
a quadratic potential for all ground substitutions of X except when  = {X  Ann},
when it becomes a linear potential. These simplifications can be identified before grounding
by observing the form in which atoms of the same predicate may unify, and splitting the
parfactor accordingly. The example creates the parfactors (X 6= Ann, P (X)P (Ann)) and
(, P (Ann)). Splitting on the logical variables can be repeated until the unification of
atoms is no longer possible. This ensures that any valid ground substitution for a parfactor
maps the atoms of the parfactor to distinct ground atoms.
Example 6. Consider the parfactor (, P (X, Y )P (X, Z)Q(X) S(Z)) and assume that all
predicates have arguments from domain D. No knowledge is available for P , i.e., (PT = ,
PF = , PU = D  D), there is evidence for some groundings of Q, i.e., (QT  D,
QF = ,QU = D \ QT ), and full knowledge of the groundings of S, i.e., (ST  D, SF  D,
SU = ).
We start by splitting the parfactor on the atom S(Z), which is fully observed. The case
where S(Z) is constrained to be True cancels the potential and is left out. Because SU = ,
the case with constraint C  (X, SU ) can also be ignored. If S(X) is constrained to be
False, the potential simplifies to P (X, Y )P (X, Z)Q(X). Proceeding similarly for the other
atoms, there remains one parfactor with the simplified potential and additional constraints
C (Z, SF ), C (X, QU ), C ((X, Y ), PU ), C ((X, Z), PU ).
After fixing all atoms, we observe that the two atoms constructed from predicate P
produce the same ground substitutions if Y = Z. Splitting on this condition produces two
new parfactors, where the potential simplifies to P (X, Y )Q(X) for the case where Y = Z.
After both steps, the original parfactor is transformed into two new parfactors:
(a) =P (X, Y )P (X, Z)Q(X)
C ={C (Z, SF ), C (X, QU ), C ((X, Y ), PU ), C ((X, Z), PU ), Y 6= Z}
and
(a) =P (X, Y )Q(X)
C ={C (Y, SF ), C (X, QU ), C ((X, Y ), PU )}

704

fiQuadratization and Roof Duality of Markov Logic Networks

Note that after the preprocessing step, the potential function has gone from being a quartic
to a cubic function. Computing the quadratization of the potential in the original form
and substituting all possible groundings produces |D|3 ground slack atoms. In contrast,
splitting on the variables results in fewer ground substitutions for the parfactor, since they
are curtailed by each of the constraints (and so is the number of ground slack variables).
4.6 Benefits and Limitations of First-Order Quadratization
Except for the ISH method, the introduced methods compute a quadratization considering
the interactions between the terms in the expression. By considering the interactions, these
methods can create a quadratization whose roof dual relaxation is tighter, producing better
bounds and more persistencies.
However, these methods also need to do additional processing on the problem. When this
processing is performed on a propositional model, it may become very large. For instance,
Gallagher et al. (2011) suggested to implement the ASM method by constructing a second
Markov Random Field in which the variables represent the possible quadratizations for
each term. Also, the approach based on generalized roof duality needs to solve an auxiliary
linear program to find the best submodular relaxation. The FIX approach does not solve
an optimization, but needs to perform multiple transformation steps on the model, and it
is not clear in which order to perform them.
Instead, for MLNs, the general procedure presented in Section 4 can be used to obtain
a quadratic MLN by applying quadratization techniques on individual parfactors. An advantage of this procedure is that the computational cost of obtaining the quadratization
is independent of the size of the ground problem. Furthermore, the parfactors provide a
compact description of the structure of the ground model, which can be exploited with the
more sophisticated quadratization methods. The auxiliary problems required for some of
the quadratization methods become extremely small when solved for individual parfactors,
but still produce a good quadratic potential for all of the groundings of the parfactor.
On the other hand, the parfactor representation of the model is not unique, and thus the
resulting quadratic MLN may depend on the manipulations performed on the parfactors.
In fact, shattering the model may give a more representative template of the structure of
the ground problem. In this work, we merge parfactors that have the same constraints
after the preprocessing step, but do not take any further steps to partition or combine
the parfactors. In Section 5 we evaluate each of these quadratization methods separately.
However, in practice methods may be combined with no restrictions, and it is possible that
the combination of different methods may give the best results.
4.7 Quadratize-Solve-Simplify-Repeat (QSSR)
When the QPBO algorithm is applied to a general quadratic minimization problem, there
can be variables for which no persistency is found. As discussed in Section 3.4, in those
cases the probe and improve procedures can be used to increase the number of persistencies
or compute an approximate solution. However, their computational cost can make them
inefficient on problems with a large number of unsolved variables. In quadratized models,
this can be mitigated by observing that many of the slack variables are no longer necessary
after computing the persistencies given by the roof duality. This is because fixing a variable
705

fide Nijs, Landsiedel, Wollherr, & Buss

x to its persistent value in the higher-order problem simplifies or cancels any terms containing x. Thus, constructing a new problem that incorporates the information about the
solved variables can have a much smaller number of slack variables, and be used to obtain
additional persistencies. This procedure of quadratizing the model on the first-order level,
solving the problem partially with the QPBO algorithm and simplifying the higher-order
problem with the obtained persistencies can be repeated until no additional persistencies
are found. We denote this iterative computational procedure by Quadratize-Solve-SimplifyRepeat (QSSR). When using this algorithm in the experiments, we stop after the problem
has been solved again after the first simplification, as additional iterations generally produce
few additional persistencies in comparison to probing. Kahl and Strandmark (2012) used
a similar procedure of iteratively solving and fixing variables in the context of computing
the generalized roof dual bound.

5. Computational Experiments
The presented quadratization methods are evaluated by their impact on the performance
of the QPBO algorithm and its extensions. The performance of the QPBO-based inference
is also evaluated on problems for which no quadratization is required. Finally we compare
our overall pipeline to existing inference engines.
5.1 Datasets
We evaluate our approach on various standard MLNs and datasets as well as additional
problems. The characteristics of these problems are summarized in Table 1. A first set
of datasets is similar to the ones employed in the evaluation of state-of-the-art engines
Tuffy (Niu et al., 2011) and RockIt (Noessner et al., 2013). The link prediction problem
on the UWCSE dataset (LP) tries to find relations between faculty members and students.
The relational classification (RC) on the Cora dataset determines the category of research
papers. The information extraction (IE) problem models how to obtain dataset records
from parsed sources. The webKB dataset is used to predict to which university department
a website belongs, given its hyperlink relations and contained words (KB). The entity resolution (ER) problem on the Cora dataset is obtained from the Alchemy website. The
goal of this problem is to identify citations referring to the same paper. Because no trained
model is available for this problem, it is trained with Alchemy using the first of the five
available splits for evaluation (Singla & Domingos, 2006a). Friends and smokers (F&S) is
a common test model for a social network with friendship relations, smoking habits and
cancer occurrences. Evidence is generated as described by Singla and Domingos (2008) for
a domain size of 200 persons. Because the F&S problem is relatively simple, an additional
problem in which the weights of all formulas are negated is also considered (-F&S).
In order to gain a broader insight into the performance of the inference algorithms on
higher-order problems, we created two additional third-order problems. The first one is
based on the KB problem and the webKB dataset mentioned above (KB3). While the
original KB inference problem uses words contained in the page contents as well as the link
structure to infer page categories, a third-order problem on the webKB dataset is created
by not only querying for the class of each page, but by also jointly inferring the links of
a page, solely from the word tokens appearing on the page. Learning was performed with
706

fiQuadratization and Roof Duality of Markov Logic Networks

Alchemy, and the size of the problem was reduced such that inference is only performed
over atoms that are True in the ground truth and the same number of randomly sampled
atoms.
The second new third-order problem is the image denoising (ID) model, which tries
to restore a noisy binary image. There are rules indicating that the observed value of a
pixel should correspond to the denoised value and two rules indicating that groups of three
horizontally or vertically neighbouring pixels should take the same value. It is easy to see
that the associated MAP problem for these rules fall within the described cases of MLNs
whose rules can be converted to polar pseudo-Boolean functions described in Section 3.3.2,
and can thus be solved exactly. The unary rules are given weight 1.0. To ensure that the
terms of the smoothing rules do not cancel out, a rule for the on pixels is given a weight
of 0.35 and the rule for the off pixels 0.3. A 90  90 pixels random binary image is used
as evidence, where each pixel has a 50% chance of being on or off.
5.2 Other Engines
We compare our approach with the MAP-inference solvers Alchemy, Tuffy and RockIt.
Alchemy is the original solver for MLNs and, in contrast to the other engines, it does not
use a relational database to ground the model, which can lead to long grounding times.
Alchemy and Tuffy optimize the ground model using MaxWalkSAT, a stochastic search
technique that can be made to scale well with large problems. RockIt uses an ILP solver
and exploits symmetries in the model to reduce the number of constraints. Because the
number of constraints may be very large, it takes an iterative approach where only the
constraints that are violated for the current solution are added to the solver.
To make sure that the problem to be solved is the same for all implementations, some
preprocessing is required. First, formulas with existential clauses are ignored and all formulas are converted to conjunctive normal form. Then, because Tuffy internally transforms
formulas with a negative weight to an approximate formula with a positive weight, we apply the same transformation. Unfortunately, this transformation can not be applied for the
higher-order problems, as it reduces the order of the formula. Lastly, the ER and KB3
problems use a method to compactly specify which ground atoms to query, and assumes
that all other query atoms are False. These query variables, also known as canopies (Singla
& Domingos, 2006a), can be used to eliminate a large number of uninteresting variables,
and can be created using a cheap distance metric (McCallum, Nigam, & Ungar, 2000). Because neither Tuffy nor RockIt support this input format, they are given the extensive
list of False evidence atoms instead.
5.3 Results on Quadratic Problems
For quadratic problems from literature, we analyze the performance of QPBO and the additional persistencies computed with the probing extension described in Section 3.4. Table 2
shows that the QPBO algorithm gives a persistent solution for most variables, and even
provides an exact solution for the KB problem. The probe procedure also solves the IE
problem exactly, but still leaves some unsolved variables for the RC and LP problems. In
general, the inference times for these problems are extremely short.
707

fide Nijs, Landsiedel, Wollherr, & Buss

5.4 Comparison of Quadratization Methods

For the higher-order problems the performance of each of the described quadratization
methods is evaluated. This includes the pairwise MLN approach, which is equivalent to the
ISH quadratization for problems with cubic potentials. The potentials of the parfactors are
expressed as a multi-linear polynomial before quadratizing the model.

First we evaluate the number of persistencies that can be obtained for the different
problems in Table 3. As expected, because of submodularity, the ID problem is completely
solved by all methods. Friends and Smokers creates few non-submodular terms, and can
also be solved exactly by all methods. The remaining problems can not be solved by all
methods, for which in some cases only a small number of variables can be fixed. In general,
it can be observed that the methods that are aware of the other terms in the potential
produce better results than ISH, which applies a fixed transformation. The final probing
step is computationally the most expensive, but may significantly increase the number of
solved variables, and even solve some problems exactly. It should be noted that this step
is important even when an approximate solution for all variables is subsequently obtained
using the improve method. Otherwise, if the number of persistencies is small, the improve
method needs to operate on a model with potentially many more variables, as it also needs
to optimize the slack variables stemming from the remaining higher-order terms.

Table 1: Summary of the characteristics of the described datasets and the associated ground
networks when grounded in their higher-order form and a multi-linear representation. Trivially satisfied or dissatisfied factors are ignored.
IE

KB

RC

LP

ID

F&S -F&S KB3

ER

Formulas
1024
106
15
24
4
6
6
66
1331
Domains
4
3
3
8
1
1
1
3
5
Query Predicates
2
1
1
1
1
3
3
2
4
Observed Predicates
16
2
3
21
2
0
0
1
6
Ground atoms
336670 9079 9650
4624 8100 40180 40180 8190 10948
Factors
351001 31283 58485 161806 55800 127982 127982 22627 910670
Higher order factors
0
0
0
0 15840 32220 32220 6736 424580

708

fiQuadratization and Roof Duality of Markov Logic Networks

Table 3: Percentage of variables solved. Step 1) Initial QPBO result 2) QPBO result after
QSSR simplification 3) Probe. Inference time in seconds for each step in parentheses. () Did not complete

Step

ISH

FIX

ASM

GRD

ID

1

100.0 (0.01)

100.0 (0.01)

100.0 (0.01)

100.0 (0.01)

F&S

1
2

100.0 (0.02)

100.0 (0.82)

100.0 (0.86)

99.6 (1.0)
100.0 (0.0)

-F&S

1
2
3

19.4 (1.79)
19.4 (1.61)


19.4
19.4


(0.9)
(0.78)

99.6 (0.42)
99.6 (0.02)
99.6 (2.66)

99.6 (1.11)
99.6 (0.02)
99.6 (2.54)

KB3

1
2
3

55.0 (0.06)
56.5 (0.04)
65.8 (19.42)

82.4 (0.03)
86.8 (0.01)
100.0 (0.1)

82.3 (0.02)
86.6 (0.01)
100.0 (0.05)

60.6 (0.08)
62.7 (0.04)
96.7 (5.59)

ER

1
2
3

92.1 (0.46)
92.3 (0.03)
92.8 (7.6)

91.9 (1.04)
92.3 (0.04)
93.0 (162.36)

95.0 (0.47)
95.0 (0.02)
95.3 (5.57)

95.4 (1.49)
96.1 (0.03)
96.6 (7.08)

5.5 Approximate Inference
We also compared quality of the approximate solutions with those of other engines and their
total running times. The problems were formulated as minimizations, and the solutions of
all engines evaluated on the same ground model. In Table 4 it can be observed that for
the quadratic problems, most engines achieve optimal costs, which are known from the
optimality guarantee given by QPBO in Table 2 and from the small MIP gap that was
used for RockIt. An exception is the LP problem, where the solver used by RockIt has
problems obtaining a tight bound, and Tuffy and QPBO+I provide better solutions.
For the higher-order problems, the ASM quadratization achieves the best cost and the
lowest computation time for most cases. Using the GRD reduction performs slightly worse,
possibly because for this quadratization the improve step needs to be executed over more

Table 2: Percentage of persistencies given by the QPBO algorithm and after using the
probing technique for different quadratic problems.

Persistencies
Persistencies (probe)
Qpbo time (s)
Probe time (s)

IE

KB

RC

LP

99.87
100
0.030
0.150

100

90.30
90.30
0.006
0.021

85.58
86.22
0.069
4.800

709

0.002

fide Nijs, Landsiedel, Wollherr, & Buss

variables. Tuffy does not perform very well in the higher order problems, possibly because
the internal transformation it uses is an approximation of the original formula.

It should be noted that computation times are affected by multiple factors. Whereas
Alchemy, Tuffy and our approach make a clear distinction between grounding and inference, RockIt uses a cutting plane algorithm that incrementally grounds factors that
are not satisfied by the current solution, which leads to large speedups when many factors
are easily satisfied or if the solution is largely homogeneous. On the other hand, the ID
problem is an example where this approach produces considerably longer running times.
Another factor is the ability to specify the evidence in the form of canopies, which allows
the relational database to execute the queries for grounding more efficiently.

Table 4: Resulting cost for different engines on various quadratic and higher-order problems. Alchemy and Tuffy were run for an increasing number of flips until no
significant advances were made. RockIt was run with relative gaps 1  10n ,
n = 9, 8, . . . until convergence is achieved within an hour. These are compared
against our method using the ASM and GRD quadratization for the higher-order
problems, using improve on the residual problem until no advances were made
for 20 iterations. Total running times in seconds in parenthesis. (*) Guaranteed
optimal cost by persistencies () Did not ground within 1 hour.

Alchemy

Tuffy

RockIt

QPBO+I

IE
KB
RC
LP


-111113.5(162)

-480.8
(119)

-4511.6
(17)
-111274.1 (115)
-4031.7
(17)
-686.3
(424)

-4511.6
-111312.4*
-4031.8
-507.7

ID
F&S
-F&S
KB3
ER

1772.7 (442)
-3.8
(159)
-182338.7 (47)
21.1
(543)
-10739.5 (551)

1784.2
(25)
-4.2*
(3)
-191856.9(3230)
-1045.3
(308)
-14128.9 (433)

-1003.8* (244)
-4.2*
(5)
-185267.3
(8)
-1492.8
(256)
-15271.3 (1902)

(19)
(27)
(11)
(13)

-4511.6
(22)
-111312.4*
(6)
-4031.8
(9)
-732.6
(9)
ASM+QPBO+I
-1003.8*
(5)
-4.2*
(6)
-193715.3
(12)
-1484.4
(57)
-15430.7
(101)

GRD+QPBO+I
-1003.8*
(6)
-4.2*
(9)
-193715.3
(14)
-1476.9
(101)
-15430.5
(113)

In Figure 2, the evolution of the cost of the ER problem against the running time of
improve is shown for different quadratizations. For this problem, the methods converge to
a solution with similar costs, but convergence is much faster in the cases where ISH and
GRD quadratizations are used.
710

fiQuadratization and Roof Duality of Markov Logic Networks

15100

ASM
FIX
GRD
ISH

15150
15200

Cost

15250
15300
15350
15400
15450
1

2

3

4

Improve (s)

Figure 2: Cost in the higher-order ER model as a function of the time spent on the improve method, using different quadratization techniques. Improve starts after
solving one iteration of the original problem and removing the redundant slacks,
as described in Section 4.7.

6. Conclusions and Future Work
In this article we have discussed the use of pseudo-Boolean functions, quadratization techniques, and MAP inference methods based on roof duality for MLNs. It was shown how
any quadratization method for pseudo-Boolean functions can be employed on MLNs at firstorder level, generalizing a previously existing approach. This enables the use of quadratization methods that exploit the structure of the problem without the need to solve a possibly
large auxiliary optimization problem. Various quadratization approaches were adapted to
work with first-order models, including a novel approach that leverages a connection with
the generalized roof duality theory.
Additionally, to the best of our knowledge, this is the first work that discusses the
recognition of (super-)submodularity and expressibility in MLNs. In particular, this allows
to guarantee the expressibility of a restricted set of MLNs. Although the class of potentials
that are guaranteed to be expressible using submodular quadratic functions is limited, such
potentials have seen wide applications in computer vision.
The presented techniques were extensively evaluated on problems from the literature as
well as various additional problems. On higher-order problems, the choice of quadratization
approach was shown to be an important factor in the quality of the results. The methods
that exploit the first-order representation of the problem often enabled QPBO to solve a
larger part of the problem and perform better approximate inference. In all cases, large
parts of the problems could be solved exactly, and the approximate solutions matched
or improved the quality of other solvers. The optimization times for the problems were
observed to be very small, and often much shorter than the rest of the pipeline, for which
we did not optimize. In total, timings show very competitive results in relation to other
state-of-the-art inference engines.
711

fide Nijs, Landsiedel, Wollherr, & Buss

There are various paths for future work. This work has not explored if performance can
be improved by partitioning the model further than the presented shattering techniques,
or by using other manipulations on the parfactors. Also, the possibility to obtain better
approximate solutions using move making algorithms (Lempitsky, Rother, Roth, & Blake,
2010) was not tested, but such algorithms may leverage the first-order representation for
finding good moves. An additional interesting aspect to consider is to integrate our approach with cutting plane techniques or lifting techniques, which can be expected to give a
significant performance benefit for problems with a very large number of factors.

Acknowledgments
The authors also wish to thank the anonymous reviewers for their valuable comments
and helpful suggestions. The research leading to these results has partly received funding from the European Research Council under the European Unions Seventh Framework
Programme (FP/2007-2013) / ERC Grant Agreement no. 26787, Project SHRINE, and the
Technische Universitat Munchen - Institute for Advanced Study (www.tum-ias.de), funded
by the German Excellence Initiative.

References
Apsel, U., & Brafman, R. I. (2012). Exploiting uniform assignments in first-order MPE. In
Proc. Conf. Uncertainty in Artificial Intelligence, pp. 7483.
Beedkar, K., Del Corro, L., & Gemulla, R. (2013). Fully parallel inference in markov logic
networks. In 15th GI-Symposium Database Systems for Business, Technology and
Web, Magdeburg, Germany. Bonner Kollen.
Billionnet, A., & Minoux, M. (1985). Maximizing a supermodular pseudoboolean function:
A polynomial algorithm for supermodular cubic functions. Discrete Applied Mathematics, 12 (1), 111.
Boros, E., Hammer, P. L., & Tavares, G. (2006). Preprocessing of unconstrained quadratic
binary optimization. Tech. rep., Rutgers Center for Operations Research.
Boros, E., & Hammer, P. (2002). Pseudo-boolean optimization. Discrete Applied Mathematics, 123 (1), 155225.
Crama, Y. (1989). Recognition problems for special classes of polynomials in 01 variables.
Mathematical Programming, 44 (1-3), 139155.
Darwiche, A. (2003). A differential approach to inference in bayesian networks. J. of the
ACM, 50 (3), 280305.
de Salvo Braz, R. (2007). Lifted First-Order Probabilistic Inference. Ph.D. thesis, University
of Illinois at Urbana-Champaign.
Domingos, P., & Webb, W. A. (2012). A tractable first-order probabilistic logic.. In Proc.
Conf. Artificial Intelligence. AAAI.
Eaton, F., & Ghahramani, Z. (2013). Model reductions for inference: Generality of pairwise,
binary, and planar factor graphs. Neural computation, 25 (5), 12131260.
712

fiQuadratization and Roof Duality of Markov Logic Networks

Fierens, D., Kersting, K., Davis, J., Chen, J., & Mladenov, M. (2013). Pairwise markov
logic. In Inductive Logic Programming, pp. 5873. Springer.
Fix, A., Gruber, A., Boros, E., & Zabih, R. (2011). A graph cut algorithm for higher-order
Markov random fields. In Int. Conf. Computer Vision, pp. 10201027. IEEE.
Freedman, D., & Drineas, P. (2005). Energy minimization via graph cuts: Settling what
is possible. In Conf. Computer Vision and Pattern Recognition, pp. 939946. IEEE
Computer Society.
Gallagher, A. C., Batra, D., & Parikh, D. (2011). Inference for order reduction in markov
random fields. In Proc. Int. Conf. Computer Vision and Pattern Recognition, pp.
18571864. IEEE.
Gallo, G., & Simeone, B. (1989). On the supermodular knapsack problem. Mathematical
Programming, 45 (1-3), 295309.
Huang, J., Chavira, M., & Darwiche, A. (2006). Solving MAP exactly by searching on
compiled arithmetic circuits. In Proc. Conf. Artificial Intelligence, Vol. 6, pp. 37.
AAAI.
Ishikawa, H. (2011). Transformation of general binary MRF minimization to the first-order
case. Trans. Pattern Analysis and Machine Intelligence, 33 (6), 12341249.
Kahl, F., & Strandmark, P. (2012). Generalized roof duality. Discrete Applied Mathematics,
160 (16-17), 24192434.
Kolmogorov, V., & Rother, C. (2007). Minimizing nonsubmodular functions with graph
cuts-a review. Trans. Pattern Analysis and Machine Intelligence, 29 (7), 12741279.
Kolmogorov, V. (2012). Generalized roof duality and bisubmodular functions. Discrete
Applied Mathematics, 160 (4-5), 416426.
Kolmogorov, V., & Zabin, R. (2004). What energy functions can be minimized via graph
cuts?. Trans. Pattern Analysis and Machine Intelligence, 26 (2), 147159.
Lempitsky, V., Rother, C., Roth, S., & Blake, A. (2010). Fusion moves for markov random
field optimization. Trans. Pattern Analysis and Machine Intelligence, 32 (8), 1392
1405.
McCallum, A., Nigam, K., & Ungar, L. H. (2000). Efficient clustering of high-dimensional
data sets with application to reference matching. In Proc. Int. Conf. Knowledge
Discovery and Data Mining, pp. 169178. ACM.
Mittal, H., Goyal, P., Gogate, V. G., & Singla, P. (2014). New rules for domain independent
lifted MAP inference. In Proc. Advances in Neural Information Processing Systems,
pp. 649657.
Niu, F., Re, C., Doan, A., & Shavlik, J. (2011). Tuffy: Scaling up statistical inference in
markov logic networks using an RDBMS. Proc. VLDB Endowment, 4 (6), 373384.
Noessner, J., Niepert, M., & Stuckenschmidt, H. (2013). Rockit: Exploiting parallelism and
symmetry for MAP inference in statistical relational models.. In AAAI Workshop:
Statistical Relational Artificial Intelligence.
713

fide Nijs, Landsiedel, Wollherr, & Buss

Orlin, J. B. (2009). A faster strongly polynomial time algorithm for submodular function
minimization. Mathematical Programming, 118 (2), 237251.
Poole, D. (2003). First-order probabilistic inference. In Gottlob, G., & Walsh, T. (Eds.),
Int. Joint Conf. Artificial Intelligence, pp. 985991. Morgan Kaufmann.
Ramalingam, S., Russell, C., Ladicky, L., & Torr, P. H. S. (2011). Efficient minimization
of higher order submodular functions using monotonic boolean functions. CoRR,
abs/1109.2304.
Richardson, M., & Domingos, P. (2006). Markov logic networks. Machine learning, 62 (1),
107136.
Riedel, S. (2009). Cutting plane map inference for markov logic. In Int. Workshop Statistical
Relational Learning.
Rosenberg, I. (1975). Reduction of bivalent maximization to the quadratic case. Cahiers
du Centre detudes de Recherche Operationnelle, 17, 7174.
Rother, C., Kolmogorov, V., Lempitsky, V., & Szummer, M. (2007). Optimizing binary
MRFs via extended roof duality. In Proc. Conf. Computer Vision and Pattern Recognition, pp. 18. IEEE.
Sarkhel, S., Venugopal, D., Singla, P., & Gogate, V. (2014). Lifted MAP inference for
markov logic networks. In Proc. Int. Conf. Artificial Intelligence and Statistics, pp.
859867.
Schlesinger, D. (2007). Exact solution of permuted submodular minsum problems. In
Energy Minimization Methods in Computer Vision and Pattern Recognition, pp. 28
38. Springer.
Shavlik, J. W., & Natarajan, S. (2009). Speeding up inference in markov logic networks
by preprocessing to reduce the size of the resulting grounded network. In Proc. Int.
Joint Conf. Artificial Intelligence, pp. 19511956.
Singla, P., & Domingos, P. (2008). Lifted first-order belief propagation. In Proc. National
Conf. Artificial Intelligence, Vol. 2, pp. 10941099.
Singla, P., & Domingos, P. (2006a). Entity resolution with markov logic. In Proc. Int. Conf.
Data Mining, pp. 572582. IEEE.
Singla, P., & Domingos, P. (2006b). Memory-efficient inference in relational domains. In
Proc. National Conf. Artificial Intelligence, Vol. 21, pp. 488493.
Zivny, S., Cohen, D. A., & Jeavons, P. G. (2009). The expressive power of binary submodular
functions. Discrete Applied Mathematics, 157 (15), 33473358.
Zivny, S., & Jeavons, P. G. (2008). Which submodular functions are expressible using
binary submodular functions?. Tech. rep. CS-RR-08-08, University of Oxford.

714

fiJournal of Artificial Intelligence Research 55 (2016) 565-602

Submitted 08/15; published 03/16

Finding Strategyproof Social Choice Functions
via SAT Solving
Felix Brandt

brandtf@in.tum.de

Technical University of Munich (TUM)
Munich, Germany

Christian Geist

geist@in.tum.de

Technical University of Munich (TUM)
Munich, Germany

Abstract
A promising direction in computational social choice is to address research problems
using computer-aided proving techniques. In particular with SAT solvers, this approach
has been shown to be viable not only for proving classic impossibility theorems such as
Arrows Theorem but also for finding new impossibilities in the context of preference extensions. In this paper, we demonstrate that these computer-aided techniques can also be
applied to improve our understanding of strategyproof irresolute social choice functions.
These functions, however, require a more evolved encoding as otherwise the search space
rapidly becomes much too large. Our contribution is two-fold: We present an efficient
encoding for translating such problems to SAT and leverage this encoding to prove new
results about strategyproofness with respect to Kellys and Fishburns preference extensions. For example, we show that no Pareto-optimal majoritarian social choice function
satisfies Fishburn-strategyproofness. Furthermore, we explain how human-readable proofs
of such results can be extracted from minimal unsatisfiable cores of the corresponding SAT
formulas.

1. Introduction
Ever since the famous Four Color Problem was solved using a computer-assisted approach,
it has been clear that computers can contribute significantly not only to verifying existing
but also to finding and proving new results. Due to its rigorous axiomatic foundation, social
choice theory appears to be a field in which computer-aided theorem proving is a particularly
promising line of research. Perhaps the best known result in this context stems from Tang
and Lin (2009), who reduce well-known impossibility results such as Arrows theorem to
finite instances, which can then be checked by a satisfiability (SAT) solver (see, e.g., Biere,
Heule, van Maaren, & Walsh, 2009). Geist and Endriss (2011) were able to extend this
method to a fully-automatic search algorithm for impossibility theorems in the context of
preference relations over sets of alternatives. In this paper, we apply these techniques to
improve our understanding of strategyproofness in the context of set-valued, or so-called
irresolute, social choice functions. These types of problems, however, are more complex
and require an evolved encoding as otherwise the search space rapidly becomes too large.
Table 1 illustrates how quickly the number of involved objects grows and that, as a result,
exhaustive search is doomed to fail.
c
2016
AI Access Foundation. All rights reserved.

fiBrandt & Geist

Alternatives
Choice sets
Tournaments
Canonical tournaments
Majoritarian SCFs

4

5

6

7

15
64
4
50, 625

31
1,024
12
 1018

63
32,768
56
 10101

127
 2  106
456
 10959

Table 1: Number of objects involved in problems with irresolute majoritarian SCFs

Our contribution is two-fold. On the one hand, we provide an extended framework
of SAT-based computer-aided theorem proving techniques for statements in social choice
theory and related research areas. Despite its complexity, this framework allows for the
extraction of human-readable proofs, which eliminates the need for extensive (and difficult)
verification of the underlying techniques. On the other hand, rather than only reproducing
existing results, we solve some open problems, which are of independent interest, in the
context of irresolute strategyproof social choice functions. These results are unlikely to
have been found without the help of computers, which further strengthens the importance
of the approach.
The results obtained by computer-aided theorem proving have already found attention
in the social choice community (Chatterjee & Sen, 2014) and similar techniques have proven
to be quite effective for other problems in economics, too. Examples are the ongoing work
by Frechette, Newman, and Leyton-Brown (2016) in which SAT solvers are used for the
development and execution of the FCCs upcoming reverse spectrum auction, recent results
by Drummond, Perrault, and Bacchus (2015) who solve stable matching problems via SAT
solving, as well as work by Tang and Lin (2011) who apply SAT solving to discover classes
of two-player games with unique pure Nash equilibrium payoffs. In another recent paper,
Caminati, Kerber, Lange, and Rowat (2015) verified combinatorial Vickrey auctions via
higher-order theorem provers. In some respect, our approach bears similarities to automated
mechanism design (see, e.g., Conitzer & Sandholm, 2002), where desirable properties are
encoded and mechanisms are computed to fit specific problem instances. There is also a
body of work on logical formalizations of important theorems in social choice theory, most
prominently, Arrows Theorem (see, e.g., Nipkow, 2009; Grandi & Endriss, 2013; Cina &
Endriss, 2015), which has been directed more towards formalizing and verifying existing
results.
Given the universality of the SAT-based method and its ease of adaptation (e.g., testing of similar conjectures with minimal effort by simply replacing or altering some axioms),
we expect these and similar techniques to be applicable to other open problems in social
choice theory and related research areas in the future. Results for different variants of
the no-show paradox (Brandl, Brandt, Geist, & Hofbauer, 2015; Brandt, Geist, & Peters,
2016c) support this hypothesis. It should be noted, however, thatat least currentlyan
expert user or programmer is required to operate these systems. An interesting question
that remains is whether it is possible to develop an automatic proof assistant that allows researchers to quickly test hypotheses on small domains without giving up too much generality
and efficiency.
566

fiFinding Strategyproof Social Choice Functions via SAT Solving

Let us now turn towards the social choice theoretic results. Formally, a social choice
function (SCF) is defined as a function that maps individual preferences over a set of alternatives to a set of socially most-preferred alternatives. An SCF is strategyproof if no agent
can obtain a more preferred outcome by misrepresenting his preferences. It is well-known
from the Gibbard-Satterthwaite theorem that, when restricting attention to SCFs that always return a single alternative, only trivial SCFs can be strategyproof. The assumption
of single-valuedness, however, has been criticized for being unreasonably restrictive (see,
e.g., Gardenfors, 1976; Kelly, 1977; Taylor, 2005; Barbera, 2010). A proper definition of
strategyproofness for the more general setting of irresolute SCFs requires the specification
of preferences over sets of alternatives. Rather than asking the agents to specify their
preferences over all sets (which requires exponential space and would be bound to various
rationality constraints), it is typically assumed that preferences over single alternatives can
be extended to preferences over sets. Of course, there are various ways how to extend
preferences to sets (see, e.g., Gardenfors, 1979; Duggan & Schwartz, 2000; Taylor, 2005),
each of which leads to a different class of strategyproof SCFs. A function that yields a
preference relation over subsets of alternatives when given a preference relation over single
alternatives is called a set extension or preference extension. In this paper, we focus on two
set extensions attributed to Kelly (1977) and Fishburn (1972),1 which have been shown to
arise uniquely under very natural assumptions (Gardenfors, 1979; Erdamar & Sanver, 2009;
see also Section 2.2 of this paper).
While strategyproofness for Kellys extension (henceforth Kelly-strategyproofness) is
known to be a rather restrictive condition (Kelly, 1977; Barbera, 1977; Nehring, 2000),
some SCFs such as the Pareto rule, the omninomination rule, the top cycle, the uncovered
set, the minimal covering set, and the bipartisan set were shown to be Kelly-strategyproof
(Brandt, 2015). Interestingly, the more prominent of these SCFs are majoritarian, i.e., they
are based on the pairwise majority relation only and can be ordered with respect to set inclusion. These results suggest that the bipartisan set may be the finest Kelly-strategyproof
majoritarian SCF. In this paper, we show that this is not the case by automatically generating a Kelly-strategyproof SCF that is strictly contained in the bipartisan set. Brandt
(2015) furthermore showed that, under a mild condition, Kelly-strategyproofness carries
over to coarsenings of an SCF. Thus, finding inclusion-minimal Kelly-strategyproof SCFs is
of particular interest. We address this problem by automating the search for these functions
in small domains and report on our findings.
Existing results suggest that the more demanding notion of Fishburn-strategyproofness
may only be satisfied by rather indiscriminating SCFs such as the top cycle (Feldman, 1979;
Brandt & Brill, 2011; Sanver & Zwicker, 2012).2 Using our computer-aided proving technique, we are able to confirm this suspicion by proving that, within the domain of majoritarian SCFs, Fishburn-strategyproofness is incompatible with Pareto-optimality. In order to
achieve this impossibility, we manually prove a novel characterization of Pareto-optimal ma1. Gardenfors (1979) attributed this extension to Fishburn because it is the weakest extension that satisfies
a certain set of axioms proposed by Fishburn (1972). Some authors, however, refer to it as the Gardenfors
extension, a term which we reserve for the extension due to Gardenfors (1976) himself.
2. The negative result by Ching and Zhou (2002) uses Fishburns extension but a much stronger notion of
strategyproofness.

567

fiBrandt & Geist

joritarian SCFs and an induction step, which allows us to generalize the computer-generated
impossibility to larger numbers of alternatives.
The paper is structured as follows. In Section 2, we present the general mathematical framework that we use throughout this paper and introduce the new condition of
tournament-strategyproofness, which we show to be equivalent to standard strategyproofness for majoritarian SCFs. In Section 3, we describe our computer-aided proving method
and explain how to encode the main questions of this paper as SAT problems. We also
describe optimization techniques and other features of the approach. In Section 4, we report on our main findingsan impossibility and a possibility resultand discuss possible
extensions and their limits. In Section 5, our novel approach to proof extraction from these
computer-generated results is presented. We provide a human-readable proof of our main
result that can be verified without the help of computers. Finally, in Section 6 we wrap up
our work and give an outlook on further research directions.

2. Mathematical Framework of Strategyproofness
In this section, we provide the terminology and notation required for our results and introduce notions of strategyproofness for majoritarian SCFs that allow us to abstract away any
reference to preference profiles.
2.1 Social Choice Functions
Let N = {1, . . . , n} be a set of at least three voters with preferences over a finite set A of
m alternatives. For convenience, we assume that n is odd, which entails that the pairwise
majority relation is antisymmetric. The preferences of each voter i  N are represented by a
complete, antisymmetric, and transitive preference relation Ri  A  A. The interpretation
of (x, y)  Ri , usually denoted by x Ri y, is that voter i values alternative x at least as much
as alternative y. The set of all preference relations over A will be denoted by R(A). The
set of preference profiles, i.e., finite vectors of preference relations, is then given by R (A).
The typical element of R (A) will be R = (R1 , . . . , Rn ). In accordance with conventional
notation, we write Pi for the strict part of Ri , i.e., x Pi y if x Ri y but not y Ri x. Note
that the only difference between Ri and Pi is that Ri is reflexive while Pi is not. In order
to improve readability, we write Ri : x, y, z as a shorthand for x Pi y Pi z. In a preference
profile, the weight of an ordered pair of alternatives wR (x, y) is defined as the majority
margin |{i  N | x Ri y}|  |{i  N | y Ri x}|.
Our central objects of study are social choice functions, i.e., functions that map the
individual preferences of the voters to a nonempty set of socially preferred alternatives.
Definition 1. A social choice function (SCF) is a function f : R (A)  2A \ .
An SCF is resolute if |f (R)| = 1 for all R  R (A), otherwise it is irresolute.
We restrict our attention to majoritarian SCFs, or tournament solutions, which are
defined using the majority relation. The majority relation RM of a preference profile R is
the relation on A  A defined by
(x, y)  RM if and only if wR (x, y)  0,
568

fiFinding Strategyproof Social Choice Functions via SAT Solving

for all alternatives x, y  A. An SCF f is said to be majoritarian if it is neutral3 and its
0 .
outcome only depends on the majority relation, i.e., f (R) = f (R0 ) whenever RM = RM
As before, we write PM for the strict part of RM , i.e., a PM b if a RM b but not b RM a.
An alternative x is called a Condorcet winner in R if x PM y for all y  A\{x}. In other
words, a Condorcet winner is a best alternative with respect to the majority relation and
it seems natural that majoritarian SCFs should select a Condorcet winner. Unfortunately,
such clear-cut winners do not exist in general and a variety of so-called Condorcet extensions,
i.e., SCFs that uniquely return a Condorcet winner whenever one exists but differ in their
treatment of the remaining cases, have been proposed in the literature. In this paper, we
consider the following majoritarian Condorcet extensions (see, e.g., Laslier, 1997; Brandt,
Brill, & Harrenstein, 2016a, for more information).
Top Cycle Define a dominant set to be a non-empty set of alternatives D  A such
that for any alternative x  D and y  A \ D we have x PM y. The top cycle TC (also
known as weak closure maximality, GETCHA, or the Smith set) is defined as the (unique)
inclusion-minimal dominant subset of A.4
Uncovered Set Let C denote the covering relation on A  A, i.e., x C y (x covers y)
if and only if x PM y and, for all z  A, y PM z implies y PM z. The uncovered set UC
contains those alternatives that are not covered according to C, i.e., UC (R) = {x  A | y C
x for no y  A}.
Bipartisan Set Consider the symmetric two-player zero-sum game in which the set of
actions for both players is given by A and payoffs are defined as follows. Suppose the first
player chooses a and the second player chooses b. Then the payoff for the first player is 1
if a PM b, 1 if b PM a, and 0 otherwise. The bipartisan set BP contains all alternatives
that are played with positive probability in the unique Nash equilibrium of this game.
An SCF f is called a refinement of another SCF g if f (R)  g(R) for all preference
profiles R  R (A). In short, we write f  g in this case. It can be shown for the above
that BP  UC  TC (see, e.g., Laslier, 1997).
For our main result, we define the well-known notion of Pareto-optimality: an SCF f
is Pareto-optimal if it never selects any Pareto-dominated alternative x  A, i.e., x 
/ f (R)
whenever there exists y  A such that y Pi x for all i  N .
2.2 Strategyproofness
Although our investigation of strategyproof SCFs is universal in the sense that it can be
applied to any set extension, in this paper we will concentrate on two well-known set
extensions attributed to Kelly (1977) and Fishburn (1972).5 These two set extensions
3. Neutrality postulates that for any permutation  of the alternatives A the SCF produces the same
outcome (modulo the permutation). See also Section 3.1.1.
4. It is easily seen that the set of dominant sets is ordered with respect to set inclusion and therefore
admits a unique minimal element. Assume for a contradiction that two dominant sets X, Y  A are not
contained in each other. Then, there exists x  X \ Y and y  Y \ X. The definition of dominant sets
requires that x PM y and y PM x, a contradiction.
5. Another natural and well-known set extension by Gardenfors leads to an even stronger notion of strategyproofness, which cannot be satisfied by any interesting majoritarian SCF (Brandt & Brill, 2011). Note

569

fiBrandt & Geist

are defined as follows: Let Ri be a preference relation over A and X, Y  A two nonempty
subsets of A.
X RiK Y if and only if x Ri y for all x  X and all y  Y .

(Kelly, 1977)

One interpretation of this extension is that voters are completely unaware of the mechanism
(e.g., a lottery) that will be used to pick the winning alternative (Gardenfors, 1979; Erdamar
& Sanver, 2009). In other words, it contains exactly the pairwise comparisons which voters
can make without knowledge of the mechanism (e.g., {a, b} RiK {c} if a Pi b Pi c).
X RiF Y if and only if all of the following three conditions are satisfied:
x Ri y for all x  X \ Y and y  X  Y ,
y Ri z for all y  X  Y and z  Y \ X, and

(Fishburn, 1972)

x Ri z for all x  X \ Y and z  Y \ X.
For this extension one may assume the winning alternative to be picked by a lottery according to some underlying a priori distribution that voters are not aware of (Ching & Zhou,
2002). Alternatively, the existence of a chairman who breaks ties according to a linear, but
unknown, preference relation also rationalizes this preference extension (Erdamar & Sanver,
2009). For both of these interpretations, the extension describes exactly the conclusions a
voter who is aware of the tie-breaking method can draw (e.g., {a, b} RiF {b, c} if a Pi b Pi c,
which does not hold for Kellys extension RiK ).
It is easy to see that X RiK Y implies X RiF Y for any pair of sets X, Y  A.
As we plan to prove a few results for entire classes of set extensions, we call a set
extension E independent of irrelevant alternatives (IIA) if its comparison of two sets X and
Y only depends on the restriction of individual preferences to X  Y . Formally, E satisfies
IIA if for all pairs of preference relations Ri , Ri0 and nonempty sets X, Y  A such that
Ri |XY = Ri0 |XY it holds that
X RiE Y if and only if X Ri0 E Y .
This is a very mild and natural condition, which is satisfied by the previously mentioned
set extensions and any other major set extension from the literature we are aware of.
Based on any set extension E, we can state a corresponding notion of P E strategyproofness for irresolute SCFs. Note that in contrast to some related papers (e.g.,
Ching & Zhou, 2002; Sato, 2008), we interpret preference extensions as fully specified (incomplete) preference relations rather than minimal conditions on set preferences.
Again, we write PiE for the asymmetric part of RiE , for any set extension E.
Definition 2. Let E be a set extension. An SCF f is P E -manipulable by voter i if there
exist preference profiles R and R0 with Rj = Rj0 for all j 6= i such that f (R0 ) is E-preferred
to f (R) by voter i, i.e.,
f (R0 ) PiE f (R).
An SCF is called P E -strategyproof if it is not P E -manipulable.
that our negative result for Fishburn-strategyproofness trivially carries over to such more demanding set
extensions.

570

fiFinding Strategyproof Social Choice Functions via SAT Solving

1

2, 3

4

5

6

7

e
c
a
d
b

d
a
e
b
c

d
e
b
c
a

c
e
b
d
a

b
c
a
e
d

b
a
c
e
d

(a) A preference profile R

b

b

d

c

d

c

e

a

e

a

(b) The corresponding (strict)
majority relation PM

(c) The manipulated (strict) ma0
jority relation PM
when the first
agent submits b, a, c, d, e as his
preferences. All edges that have
been impacted by this change are
depicted in bold.

Figure 1: Let the choice sets be as indicated by shaded nodes; this example is taken from the
proof of Theorem 3 (cf. Section 5.1.3). The first agent in R can P F -manipulate by submitting b, a, c, d, e as his preferences (since f (R0 ) = {a, c, d, e} P1F {a, b, c, d} = f (R)), but this
does not constitute a P K -manipulation (since {a, b, c, d} and {a, c, d, e} are incomparable
according to the Kelly-extension).
It follows from the observation on set extensions above that P F -strategyproofness implies
An example illustrating both notions of strategyproofness is shown
in Figure 1.
Of the above SCFs, TC is P F -strategyproof, BP is P K - but not P F -strategyproof,
whereas UC was only known to satisfy P K -strategyproofness (Brandt & Brill, 2011; Brandt,
2015).
P K -strategyproofness.

2.3 Tournament-Strategyproofness
In order to allow for a more efficient encoding, we would like to omit references to preference
profiles and replace them with a more succinct representation with the same expressive
power. For majoritarian SCFs, the natural choice is to use the (strict) majority relation,
which, for an odd number of voters, can be represented by a tournament:
A tournament is an asymmetric and complete binary relation on the set of alternatives A.6 We can thus view majoritarian SCFs as functions defined on tournaments rather
than preference profiles, and, in slight abuse of notation,7 write f (T ) instead of f (R) with
T = PM being the strict part of the majority relation of R. We, furthermore, denote by
T \ T 0 := {e  T : e 
/ T 0 } the edge difference of two tournaments T and T 0 .
For our encoding to be efficient, it will be important to formalize the notion of strategyproofness using only references to tournaments rather than preference profiles. The
6. Note that tournaments can be defined by their edge set only. Since there is exactly one edge between
any pair of vertices, the vertex set can be derived from the edge set.
7. It may be noted that, while majoritarian SCFs map from profiles (with an arbitrary, but fixed number
of voters) to sets of alternatives, their interpretation via tournaments abstracts away the reference to
individual voters. This has implications for Theorems 1 and 3, which depend upon the presence of a
sufficient number of voters. We discuss the required number of voters in Section 5.2.

571

fiBrandt & Geist

following definition serves this purpose and will be shown to be equivalent to the standard
notion of strategyproofness for majoritarian SCFs.
Definition 3. A majoritarian SCF f is said to be P E -tournament-manipulable if there exist
tournaments T, T 0 and a preference relation R  T \ T 0 such that
f (T 0 ) PE f (T ).
A majoritarian SCF is called P E -tournament-strategyproof if it is not P E -tournamentmanipulable.
Theorem 1. A majoritarian SCF is P E -strategyproof if and only if it is P E -tournamentstrategyproof.
Proof. We show that a majoritarian SCF is P E -manipulable if and only if it is P E tournament-manipulable.
For the direction from left to right, let f be a P E -manipulable majoritarian SCF. Then
there exist preference profiles R, R0 and an integer j with Ri = Ri0 for all i 6= j such that
0 as the strict majority relations
f (R0 ) PjE f (R). Define tournaments T := PM and T 0 := PM
of R and R0 , respectively. Since R and R0 only differ for voter j, it follows that T \ T 0  Rj ,
i.e., all edges that are reversed from T to T 0 must have been in Rj . Thus, with R := Rj ,
we get that f is tournament-manipulable.
For the converse, let f be a P E -tournament-manipulable majoritarian SCF. The SCF f
then admits a manipulation instance, i.e., there are two tournaments T, T 0 and a preference
relation R  T \ T 0 such that f (T 0 ) PE f (T ).
As in the proof of McGarveys Theorem (McGarvey, 1953), we construct a preference

of its majority relation:
profile R = (R1 , . . . , Rn1 ) which has T  T 0 as the strict part PM
we start from an empty profile and, for each strict edge (a, b)  T  T 0 , add two voters ia,b
and ja,b with the preferences
Ria,b : a, b, x1 , . . . , xm2 and Rja,b : xm2 , . . . , x1 , a, b, respectively.
Here x1 , . . . , xm2 denotes an arbitrary enumeration of the m  2 alternatives in A \ {a, b}.
It then holds for the weights wR (a, b) of all edges (a, b)  T that
(
2 if (a, b)  T  T 0
wR (a, b) =
0 if (a, b)  T \ T 0 .
Note that the number of voters n  1 in R has to be even (and at most m2  m  2). By
adding R as the n-th voter, we get to a profile R := (R , R ) with an odd number of voters
as required. Then wR (a, b)  1 for all edges (a, b)  T and, thus, R has T as its (strict)
majority relation. The second profile R0 can be defined to contain the same first n  1
voters from R and the reversed preference R as the n-th voter (i.e., R0 := (R , R )).8
The profile R0 then has T 0 as its (strict) majority relation (since wR0 (a, b) = 1 for all
8. Immunity to manipulation by reversing preferences has been considered by Sanver and Zwicker (2012)
under the name of half-way monotonicity. Our proof entails that (weak) half-way monotonicity is equivalent to strategyproofness for majoritarian SCFs.

572

fiFinding Strategyproof Social Choice Functions via SAT Solving

Setting and axioms
LP
solver

nauty

Results

Tournament
solver

CNF encoder

Model decoder

SAT solver
Figure 2: High-level system architecture
edges (a, b)  T \ T 0 and the weights of all edges in T  T 0 are at least 1 again), which
completes the manipulation instance. I.e., we have found preference profiles R, R0 which
only differ for voter n (who has truthful preferences R ) and for which it holds that
f (R0 ) = f (T 0 ) PE f (T ) = f (R).

3. Methodology
The method applied in this paper is similar to and yet more powerful than the ones presented
by Tang and Lin (2009) and Geist and Endriss (2011). Rather than translating the whole
problem navely to SAT, a more evolved approach, which resolves a large degree of freedom
already during the encoding of the problem, is employed. This approach is comparable to
the way SMT (satisfiability modulo theories) solving works: At the core there is a SAT
solver; certain aspects of the problem, however, are dealt with in a separate theory solving
unit which accepts a richer language and makes use of specific domain knowledge (Biere
et al., 2009, ch. 26). The general idea, however, remains to encode the problem into a
language suitable for SAT solving and to apply a SAT solver as an efficient and universal
problem solving machine.
While desirable, using existing tools for higher-order formalizations directly rather than
our specific approach, unfortunately, is not an option. For instance, a formalization of strategyproof majoritarian SCFs in higher-order logic (HOL) as accepted by Nitpick (Blanchette
& Nipkow, 2010) is straightforward, highly flexible, and well-readable, but only successful
for proofs and counterexamples involving up to three alternatives before the search space
is exceeded.9 An optimized formalization, which we derived together with the author of
Nitpick (at the cost of reduced readability and flexibility), extends the performance to
four alternatives, which turns out to be just too low for our results.
9. On the other hand, the strict formalization required for Nitpick helped to identify a formally inaccurate definition of Fishburn-strategyproofness by Gardenfors (1979) (which was later repeated by other
authors).

573

fiBrandt & Geist

Concretely, our approach is the following (see also the high-level architecture in Figure 2): for a given domain size n we want to check whether there exists a majoritarian
SCF f that satisfies a set of axioms (e.g., P F -strategyproofness and Pareto-optimality).
We then encode the setting as well as the given axioms as a propositional formula (SAT
instance) and let a SAT solver decide whether this formula has a satisfying assignment. If
it has a satisfying assignment, we can decode it into a concrete instance of a majoritarian
SCF f which satisfies the required properties. If the formula is unsatisfiable, we know that
no such function f exists.
As we will see, depending on the problem, some preparatory tasks have to be solved
before the actual encoding: (i ) sets, tournaments, and preference relations are enumerated;
(ii ) isomorphisms between tournaments are determined using the tool nauty (McKay &
Piperno, 2013); (iii ) choice sets for specific SCFs are computed (e.g., via matrix multiplication for UC and linear programming for BP ).
In the following, we describe in more detail how the general setting of majoritarian
SCFs as well as desirable properties, such as strategyproofness, can be encoded as a SAT
problem in CNF (conjunctive normal form).10 First, we describe our initial encoding, which
is expressive enough to encode all required properties, but allows for small domain sizes of
(depending on the axioms) at most four to five alternatives only. Second, we explain how
this encoding can be optimized to increase the overall performance by orders of magnitude
such that larger instances of up to seven alternatives are solvable.
3.1 Initial Encoding
By design, SAT solvers operate on propositional logic. A direct and nave propositional
encoding of the problem would, however, require a huge number of propositional variables
since many higher-order concepts are involved (e.g., sets of alternatives, preference relations
over sets as well as over alternatives, and functions from tuples of such relations to sets).
In our approach, we use only one type of variable to encode SCFs. The variables are of the
form cT,X with T being a tournament and X being a set of alternatives.11 The semantics
of these variables are that cT,X if and only if f (T ) = X, i.e., the majoritarian SCF f selects
the set of alternatives X as the choice set for any preference profile with (strict) majority
m(m1)

m(m+1)

relation T . In total, this gives us a high but manageable number of 2 2  2m = 2 2
variables in the initial encoding.
An encoding with variables cT,x for alternatives x rather than sets would require less
variable symbols. This encoding, however, leads to much more complexity in the generated
clauses, which more than offsets these savings. This imbalance is best exhibited in the
encoding of strategyproofness where statements are always made forVpairs of outcomes
(i.e.,
V
sets of alternatives). Each occurrence of cT,X could be replaced by xX cT,x  yX
c
T,y .
/
But since this formula then contains a conjunction within a disjunction, which is not possible
10. Converting an arbitrary propositional formula navely to CNF can lead to an exponential blow-up in the
length of the formula. There are, however, well-known efficient techniques (e.g., Tseitins encoding, see
Tseitin, 1983) to avoid this at the cost of introducing linearly many auxiliary variables. We apply these
techniques manually when needed.
11. In all algorithms, a subroutine c(T, X) will take care of the compact enumeration of variables. Since we
know in advance how many tournaments and non-empty subsets there are, we can simply use a standard
enumeration method for pairs of objects.

574

fiFinding Strategyproof Social Choice Functions via SAT Solving

in CNF, either expansionV(and therefore an exponential blow-up) or replacement (e.g., by
a helper variable cT,X  xX cT,x ) would be required.
The following two subsections demonstrate the initial encoding of both contextual as
well as explicit axioms to CNF.
3.1.1 Context Axioms
Apart from the explicit axioms, which we are going to describe in the next subsection,
there are further axioms that need to be considered in order to fully model the context
of majoritarian SCFs. For this purpose, an arbitrary function that maps tournaments
to non-empty sets of its vertices will be called a tournament choice function. Using our
initial encoding three axioms are introduced, which will ensure that functionality of the
tournament choice function and neutrality are respected (making it a tournament solution):
(1) functionality, (2) canonical isomorphism equality, and (3) the orbit condition.
The first axiom ensures that the relational encoding of f by variables cT,X indeed models
a function rather than an arbitrary relation, i.e., for each tournament T there is exactly one
set X such that the variable cT,X is set to true. In formal terms this can be written as
(T ) ((X) cT,X  (Y, Z) Y 6= Z  (cT,Y  cT,Z ))


!
^
_
^


cT,X 
(cT,Y  cT,Z ) .
T

X

(1)

Y 6=Z

As an illustrative example, the corresponding simple pseudo-code for generating the CNF
file can be found in Appendix B.
The second and third axiom together constitute neutrality of the tournament choice
function f , which, formally, can be written as
(f (T )) = f ((T )) for all tournaments T and permutations  : A  A.
A direct encoding of this neutrality axiom, however, would be tedious due to the quantification over all permutations. In addition, our reformulation as canonical isomorphism
equality and orbit condition enables a substantial optimization of the encoding as we will
see in Section 3.2. We require further observations in order to precisely state these two
axioms.
We use the well-known fact that graph isomorphisms define an equivalence relation
on the set of all tournaments.12 For each equivalence class, pick a representative as the
canonical tournament of this class. For any tournament T , we then have a unique canonical
representation (denoted by Tc ). We also pick one of the potentially many isomorphisms
from Tc to T as the canonical isomorphism of T and denote it by T .13 This allows us to
formulate the axiom of canonical isomorphism equality.
Definition 4. A tournament choice function f satisfies canonical isomorphism equality if
f (T ) = T (f (Tc )) for all tournaments T .

(2)

12. Two tournaments T and T 0 are isomorphic if there is a permutation  : A  A such that (T ) = T 0 .
13. In practice, the tool nauty will automatically compute canonical representations for both tournaments
and isomorphisms.

575

fiBrandt & Geist

a

b
c

e

d

Figure 3: The orbits of this tournament
are 
OT = {{a, b, c}, {d}, {e}}. A corresponding

a b c d e
automorphism would be  =
. C := {a, b, c} represents a component in
b c a d e
the sense that for all of its elements x  C it holds that x PM d and e PM x.
For the last of the three context axioms, the definition of an orbit should be clarified. The
orbits of a tournament T are equivalence classes of alternatives according to the following
equivalence relation: two alternatives a, b are considered equivalent if and only if there is
an automorphism  : A  A which maps a to b, i.e., for which (a) = b. The set of orbits
of a tournament T is denoted by OT . An example can be found in Figure 3.
Definition 5. A tournament choice function f satisfies the orbit condition if
O  f (Tc ) or O  f (Tc ) = 

(3)

for all canonical tournaments Tc and their orbits O  OTc .
It can be shown that for any tournament choice function, neutrality is equivalent to
the conjunction of the orbit condition and canonical isomorphism equality, or equivalently,
that the class of tournament choice functions satisfying the orbit condition and canonical
isomorphism equality is equal to the class of tournament solutions. We formalize this
statement in Lemma 1. The proof of Lemma 1 is based on standard arguments from
category theory and is presented in Appendix A.
Lemma 1. For any tournament choice function, neutrality is equivalent to the conjunction
of the orbit condition and canonical isomorphism equality.
3.1.2 Explicit Axioms
Many axioms can be efficiently encoded in our proposed encoding language. In this section
we present the main conditions required to achieve the results in Section 4. Clearly, the most
important one is strategyproofness. In formal terms, P E -tournament-strategyproofness can
be written as


(T, T 0 , R  T \ T 0 )  f (T 0 ) PE f (T )
^^ ^
^
(4)

(cT,X  cT,Y )
T T 0 R T \T 0 Y PE X

where T, T 0 are tournaments, R is a preference relation, and X, Y are non-empty subsets
of A. The algorithmic encoding of strategyproofness is omitted here since we present an
optimized version in Section 3.2.
576

fiFinding Strategyproof Social Choice Functions via SAT Solving

Another property of SCFs that will play an important role in our results is the one of
being a refinement of another (known) SCF g. Fortunately, this can easily be encoded using
our framework:
(T )(X  g(T )) f (T ) = X
^ _
cT,X .


(5)

T Xg(T )

If we desire that the resulting SCF f is different from g (for instance, to obtain a strict
refinement in conjunction with Axiom (5)), we encode the additional clause:
(T ) f (T ) 6= g(T )
_
 cT,g(T ) .

(6)

T

Finally, even properties regarding the cardinalities of choice sets can be encoded. The
following axiomstating that |f (T )| < |g(T )| for at least one tournament T will, for
instance, be useful in Section 4.1.1 when searching for SCFs that return small choice sets:
(T )(X) |X| < |g(T )|  f (T ) = X
_ _

cT,X .
T

(7)

X
|X|<|g(T )|

3.2 Optimized Encoding for Improved Performance
In order to efficiently solve instances of more than four alternatives, we need to streamline
our initial encoding without weakening its logical and expressiv power. In this section, we
present the three optimization techniques we found most effective.
3.2.1 Obvious Redundancy Elimination
A straightforward first step is to reduce the obvious redundancy within the axioms. As an
example, consider the axiom of strategyproofness, wherein order to determine whether
an outcome Y = f (T 0 ) is preferred to an outcome X = f (T )we consider all preference
relations R  T \ T 0 . It suffices, however, if we stop after finding the first such preference
relation with Y PE X because then we already know that not both Y = f (T 0 ) and X = f (T )
can be true.
Similarly, in many axioms, we can exclude considering symmetric pairs of objects (e.g.,
for functionality of the tournament choice function, there is no need to consider both pairs
of sets (X, Y ) and (Y, X)).
3.2.2 Canonical Tournaments
The main efficiency gain can be achieved by making use of the canonical isomorphism
equality (see Section 3.1.1) during encoding. Recall that this condition states that for any
tournament T the choice set f (T ) can be determined from the choice set f (Tc ) of the corresponding canonical tournament Tc by applying the respective canonical isomorphism T .
577

fiBrandt & Geist

foreach Canonical tournament Tc do
foreach Tournament T 0 do
RTc \T 0  {R | R is a preference relation and R  Tc \ T 0 };
foreach Set X do
foreach Set Y do
boolean found  false;
foreach R  RTc \T 0 do
if found  setExt(R , E).prefers(Y, X) then
variable not(c(Tc , X));
variable not(c(Tc0 , T10 (Y )));
newClause();
found  true;
Algorithm 1: P E -tournament-strategyproofness (optimized)
Therefore, it suffices to formulate the axioms on a single representative of each equivalence
class of tournaments, in our case, the canonical tournament. The magnitudes in Table 1
illustrate that this formulation dramatically reduces the required number of variables, the
size of the CNF formula, and the time required for encoding it.
In particular, in all axioms we can replace any outer quantifier T by a quantifier Tc that
ranges over canonical tournaments only.14 In the case of strategyproofness, however, there
is a second tournament T 0 for which the restriction to canonical tournaments is potentially
not strong enough to capture the full power of the axiom. We therefore keep T 0 as an
arbitrary tournament but make sure that we only need variable symbols cTc0 ,Y for canonical
tournaments in our CNF encoding. This can be achieved through the canonical isomorphism
T 0 since by Condition (2), f (T 0 ) = Y if and only if f (Tc0 ) = T10 (Y ). The optimized
encoding is shown in Algorithm 1.
Furthermore, since we no longer make any statements within the CNF formula about
non-canonical tournaments, the canonical isomorphism equality condition becomes an
empty condition and, thus, can be dropped from the encoding.
3.2.3 Approximation through Logically Related Properties
Approximation is a standard tool in SAT/SMT which can speed up the solving process.
For instance, over-approximation can help find unsatisfiable instances faster by only solving
parts of the full problem description in CNF. If this partial CNF formula is found to be
unsatisfiable, any superset will also trivially be unsatisfiable. Since common manipulation
instances in the literature require only one edge in a tournament to be reversed, one can,
for instance, use over-approximation in the form of single-edge-strategyproofness, a slightly
weaker variant of (tournament-)strategyproofness with |T \ T 0 | = 1.15
14. The tool nauty is capable of enumerating such non-isomorphic (i.e., canonical) tournaments.
15. While it was not obvious whether this condition is actually strictly weaker than tournamentstrategyproofness, we identified Pareto-optimal SCFs that are Kelly-single-edge-strategyproof but not
Kelly-tournament-strategyproof (cf. Section 4.1.1).

578

fiFinding Strategyproof Social Choice Functions via SAT Solving

If the solver returns that there is no single-edge-strategyproof SCF that satisfies some
set of properties  , we know immediately that there is also no strategyproof SCF that
satisfies  . We used this form of approximation to prove the results in Remark 2.16
In a similar fashion, one can also apply logically simpler conditions, such as the ones
by Brandt and Brill (2011), that are slightly stronger or weaker than P E -strategyproofness
for specific set extensions E in order to logically under- or over-approximate problems,
respectively. While these logically simpler conditions can help to further improve encoding
and solving times, none of them were required to obtain the results presented in this paper.
Another way to over-approximate our problems is to restrict the domain of the SCF
(e.g., by random sampling), which we explore in somewhat more detail when extracting
small proofs in Section 5.1.1.
3.3 Finding Refinements through Incremental Solving
In order to obtain results for most refined (i.e., inclusion-minimal) or otherwise minimal
SCFs, it will be important to also produce this property to the SAT solver in a satisfactory
way. Generally, since the task of a SAT solver is to generate only one satisfying assignment, it does not necessarily output the finest SCF to satisfy a given set of properties.
Through iterated or incremental solving, however, we can force the SAT solver to generate
progressively finer or simply different SCFs that satisfy a set of desired properties.17 For
refinements, this can be achieved by adding clauses which encode that the desired SCF
must be (strictly) finer than previously found solution (see, e.g., the formulation in Section 3.1.2). When the finest SCF with the desired properties has been found, adding these
clauses leads to an unsatisfiable formula, which the SAT solver detects and therefore verifies
the minimality of the solution.
With this final solving step, we have the main tools at hand required for our results, the
most significant ones of which we describe in the next section.

4. Results and Discussion
Here we present our two main findings:
 There exists a strict refinement of BP which is P K -strategyproof (Theorem 2).
 For majoritarian SCFs with m  5, P F -strategyproofness and Pareto-optimality are
incompatible (Theorem 3). For m < 5, UC satisfies P F -strategyproofness and Paretooptimality.
Further minor results are mentioned in the discussions proceeding the proofs and in Section 4.2.1.
16. While for m = 7 approximation was required to reach the result, it also enabled a speed-up for smaller
instances: the running time for m = 6, for example, was reduced from almost five hours to three minutes.
17. Note that finding a refinement of an SCF is not equivalent to finding a smaller/minimal model in the
SAT sense; in our encoding all assignments have the same number of satisfied variables.

579

fiBrandt & Geist

4.1 Minimal Kelly-Strategyproof SCFs
Brandt (2015) showed that every coarsening f of a P K -strategyproof SCF f 0 is P K strategyproof if f (R) = f 0 (R) whenever |f 0 (R)| = 1. Thus, it is an interesting question
to identify finest (or inclusion-minimal ) P K -strategyproof SCFs.
While previous results suggested that BP could be aor even thefinest majoritarian
SCF which satisfies P K -strategyproofness, we first provide a counterexample to these assertions using m = 5 alternatives, and second show that also for larger domain sizes there
exist majoritarian refinements of BP that are still P K -strategyproof and return significantly
smaller choice sets than BP .
Theorem 2. There exists a majoritarian Condorcet extension that refines BP and is still
P K -strategyproof. As a consequence, BP is not even a finest majoritarian Condorcet extension satisfying P K -strategyproofness.
Proof. Within seconds our implementation finds a satisfying assignment for m = 5 and the
encoding of the explicit axioms refinement of BP (implies Condorcet extension) and P K strategyproofness. The corresponding majoritarian SCF can be decoded from the assignment
and is defined like BP with the exception depicted in Figure 4.

a

b
c

e

d

Figure 4: Tournament on which a P K -strategyproof refinement of BP is possible. C :=
{a, b, c} represents a component in the sense that for all of its elements x  C it holds that
x PM d and e PM x. While BP chooses the whole set A on this tournament, the refined
solution selects {a, b, c, d} only.

Using the technique described in Section 3.3, we furthermore confirmed that the obtained SCF is the only refinement of BP on five alternatives which is still P K -strategyproof.
Note, however, that it does not satisfy the (natural, but strong) property of compositionconsistency (see, e.g., Laslier, 1997). Thus, it remains open whether BP might be characterized as anor even theinclusion-minimal, P K -strategyproof, and composition-consistent
majoritarian SCF.18
While we were not able to resolve this open problem completely, we proved the following statements by extending our approach to also cover composition-consistency. BP
is an inclusion-minimal, P K -strategyproof, and composition-consistent majoritarian SCF
18. Although already on the domain of up to five alternatives there are further inclusion-minimal, P K strategyproof, and composition-consistent Condorcet extensions, which we could find using the computeraided method, these counterexamples might not extend to larger domains.

580

fiFinding Strategyproof Social Choice Functions via SAT Solving

for m  5.19 For m  7, BP is an inclusion-minimal majoritarian SCF satisfying setmonotonicity 20 and composition-consistency. While this result might extend to larger instances, it only holds for at most 5 alternatives that these properties uniquely characterize
BP .
If we, however, drop composition-consistency again, we can find multiple inclusionminimal majoritarian SCFs that are refinements of BP and still P K -strategyproof. Interestingly, some of these SCFs turn out to be more discriminating than others in the sense
that on average they yield significantly smaller choice sets. In the following section we are
going to search for such discriminating SCFs and analyze the average size of their respective
choice sets.
4.1.1 Finding Discriminating Kelly-Strategyproof SCFs
Many P K -strategyproof tournament solutions have been criticized for not being discriminating enough. It is known, for instance, that in large random tournaments, TC and
UC select all alternatives with probability approaching 1 (Scott & Fey, 2012), while BP
selects exactly half of the alternatives on average for any fixed number of alternatives
(Fisher & Reeves, 1995). More discriminating tournament solutions, on the other hand,
such as the Copeland, Markov, and Slater rules violate P K -strategyproofness. Using the
computer-aided approach, we search for the most discriminating majoritarian SCFs that
satisfy P K -strategyproofness. Though this is in the spirit of automated mechanism design (see, e.g., Conitzer & Sandholm, 2002), we apply these techniques mostly to improve
our understanding of P K strategyproofness and related axioms rather than to propose the
generated tournament solutions for actual use.
As a measure for the discriminating power of majoritarian SCFs, we use the average
relative size avg(f ) of the choice sets returned by an SCF f . Formally we define
avg(f ) :=

X
1
|f (T )|,
|A|  |T|
T T

where T is the set of all labeled tournaments on |A| = m alternatives. We call an SCF f
more discriminating than another SCF g if avg(f ) < avg(g). Given a set of axioms  , we
try to find a most discriminating SCF f (i.e., with the minimal value for avg(f )) such that
f satisfies the axioms in  .
While in theory it would be possible to just encode the relevant axioms and then enumerate all SCFs with the required properties by incrementally applying Axiom (6), the
number of such SCFs is usually much too large. If we instead refine the initial solution
further and further by applying Axioms (5) and (6) as indicated in Section 3.3, we will
find an inclusion-minimal SCF, but not necessarily a most discriminating SCF f . We thus
proceed via Algorithm 2, which is guaranteed to find a most discriminating SCF f without
enumerating all candidates of SCFs. The algorithm starts by constructing an initial candidate of an SCF which satisfies the required axioms, iteratively refines it as much as possible
(via the conjunction of Axioms (5) and (6)), and then encodes an additional axiom stating
19. For m = 6 we can already find a refinement with the same properties.
20. Set-monotonicity postulates that the choice set is invariant under the weakening of unchosen alternatives;
it implies P K -strategyproofness (Brandt, 2015).

581

fiBrandt & Geist

that all future solutions must yield a choice set with strictly smaller cardinality for at least
one tournament T (Axiom (7)). The algorithm then repeats the refinement and encoding
process until no further solution can be found. Since Axiom (7) is a necessary condition for
avg(f ) < avg(g), we can be sure that a finest SCF f is returned.
SCF smallestSolution  null;
CNF minimalRequirements  encodeAxioms();
minimalRequirements  preprocess(minimalRequirements); // optional
while isSatisfiable(minimalRequirements) do
CNF currentRequirements  minimalRequirements;
SCF currentSolution  solve(currentRequirements);
while canBeRefined(currentSolution) do
Append Axioms (5) and (6) to currentRequirements with g = currentSolution;
currentSolution  solve(currentRequirements);
// an inclusion-minimal solution has been found
if avgSize(currentSolution) < avgSize(smallestSolution) then
smallestSolution  currentSolution;
Append Axiom (7) to minimalRequirements with g = currentSolution;
return smallestSolution;
Algorithm 2: A search algorithm to find a cardinality-minimal SCF f (i.e., with minimal
value for avg(f )) that satisfies a given set of axioms. As a reminder, Axioms (5) and (6)
encode a strict refinement of g; Axiom (7) encodes |f (T )| < |g(T )| for some tournament
T.
Preprocessing is generally optional in Algorithm 2; for m = 6 we, however, had to use
unit propagation in order to reduce the size of the resulting SAT instance.21 Note that the
optimization techniques as described in Section 3.2 (in particular, canonical tournaments)
can also be applied here.
The results of our analysis are exhibited in Figure 5. While on up to four alternatives
all axioms under consideration lead to the same minimal size of avg(f ), on larger domains,
P K -strategyproofness allows for smaller choice sets than BP (e.g., 45% instead of 50% of
the alternatives for m = 6). Interestingly, the gap between BP and these more discriminating SCFs that satisfy P K -strategyproofness is not extraordinarily large; in particular,
moving from P K -strategyproofness to P K -single-edge-strategyproofness allows for a more
sizable reduction of avg(f ). For the related property of Kelly-participation, Brandl et al.
(2015) remarked that the average size of choice sets can be reduced by almost 50% compared to BP , which supports the intuition that participation is a weaker property than
strategyproofness (even though logically the two are independent).
BP and set-monotonicity yield the exact same values of avg(f ) for m  6, which is
somewhat surprising as we found SCFs that are not coarsenings of BP and are yet setmonotonic on this domain size. These SCFs, however, have no set-monotonic refinements
that are more discriminating than BP . Interestingly, this does not generalize to larger
21. For the case of Kelly-strategyproofness, unit propagation and deletion of duplicate clauses reduced the
CNF formula from about 600 million to just below three million clauses.

582

fiFinding Strategyproof Social Choice Functions via SAT Solving

60 %

56%
50%

50 %

45%

minf (avg(f ))

40 %

38%

Uncovered set UC
Bipartisan set BP
Set-monotonicity
P K -strategyproofness
P K -single-edgestrategyproofness

30 %

20 %

18%

No further restriction

10 %

2

3

4
5
6
Number of alternatives |A| = m

Figure 5: A comparison of the minimal values (rounded) of avg(f ) for majoritarian, Paretooptimal SCFs f that satisfy the given axioms (e.g., P K -strategyproofness). Interestingly,
the values for set-monotonicity are identical to the ones for BP . Non-solid dots represent
upper bounds, i.e., cases where we could only compute an SCF f with this value of avg(f )
but have no guarantee that it is indeed minimal.

583

fiBrandt & Geist

domains since we found a most discriminating majoritarian SCF f for m = 7 that satisfies
set-monotonicity and Pareto optimality while only selecting 49.73% of the alternatives on
average.
As more demanding axioms usually lead to larger choice sets (for instance, the SCF that
always returns all alternatives trivially satisfies many axioms), one might view the minimal
value of avg(f ) as an attempt to quantify the strength of an axiom. We leave a more
detailed study of such a quantification as future work.
4.2 Incompatibility of Fishburn-Strategyproofness and Pareto-Optimality
In order to prove our main result on the incompatibility of Pareto-optimality and P F strategyproofness we first show the following lemma, which establishes that, for majoritarian
SCFs, the notion of Pareto-optimality is equivalent to being a refinement of the uncovered
set (UC ).22
Lemma 2. A majoritarian SCF f is Pareto-optimal if and only if it is a refinement of
UC .
Proof. It is well-known, and was already observed by Fishburn (1977), that UC is Paretooptimal, which implies that all its refinements are also Pareto-optimal.
For the direction from left to right, let f be a Pareto-optimal majoritarian SCF and T an
arbitrary tournament. It suffices to show that f (T ) can never contain a covered alternative
(since then f (T )  UC (T ) contains uncovered alternatives only). So let b be an alternative
that is covered by another alternative a. We are going to construct a preference profile
R which has T as its (strict) majority relation and in which b is Pareto-dominated by a.
Together with the Pareto-optimality of f this implies that b 
/ f (T ). We use a variant
of the well-known construction by McGarvey (1953), but for triples rather than pairs of
alternatives. Note that for each voter we need to ensure that he strictly prefers a to b in
order to obtain the desired Pareto-dominance of a over b. Starting with an empty profile,
for each alternative x 
/ {a, b} we add two voters Rx1 , Rx2 to the profile. These two voters
are defined depending on how x is ranked relative to a and b in order to establish the edges
between a, x and b, x. Note that since x T a implies x T b (because of a C b), edge (a, b)
cannot be contained in a three-cycle with x and, thus, forms a transitive triple with x.
 Case 1: x T a (implies x T b)
Rx1 : x, a, b, v1 , . . . , vm3 ; Rx2 :

vm3 , . . . , v1 , x, a, b

 Case 2a: a T x and x T b
Rx1 : a, x, b, v1 , . . . , vm3 ;

Rx2 :

vm3 , . . . , v1 , a, x, b

 Case 2b: a T x and b T x
Rx1 : a, b, x, v1 , . . . , vm3 ;

Rx2 :

vm3 , . . . , v1 , a, b, x

Here v1 , . . . , vm3 denotes an arbitrary enumeration of the m3 alternatives in A\{a, b, x}.
In all cases, the two voters cancel out each other for all pairwise comparisons other than
(a, b), (x, a) and (x, b). For each of the remaining edges (y, z)  T (with {y, z}  {a, b} = )
22. A stronger version of this lemma was shown by Brandt, Geist, and Harrenstein (2016b).

584

fiFinding Strategyproof Social Choice Functions via SAT Solving

we further add two voters (now even closer to the construction by McGarvey.)
R(y,z)1 : y, z, a, b, v1 , . . . , vm4 and
R(y,z)2 : vm4 , . . . , v1 , a, b, y, z,
which together establish edge (y, z), reinforce (a, b) and cancel otherwise. Note that in order
to achieve an odd number of voters, an arbitrary voter can be added without changing the
majority relation (as all edges had a weight of at least two so far). This completes the
construction of a preference profile R which has T as its (strict) majority relation and in
which b is Pareto-dominated by a.
To establish the full result (which does not admit a proof by counterexample as in
Theorem 2) wesimilarly to previous approachesmake use of an inductive argument.
Lemma 3. For any set extension E that satisfies IIA, if there exists a majoritarian SCF f
for m + 1 alternatives that is P E -strategyproof and Pareto-optimal, then there also exists a
majoritarian SCF f 0 for just m alternatives that satisfies these two properties.
Proof. Let f  UC be a majoritarian SCF for m + 1  2 alternatives that is P E strategyproof. Then we define fe to be the restriction of f to m alternatives based on
tournaments in which alternative e is a Condorcet loser, i.e., an alternative x for which
(y, x)  T for all y  A \ {x}. In formal terms, define
fe (T ) := f (T +e ),
where T +e is the tournament obtained from T by adding an alternative e as a Condorcet
loser. This restriction of f is a well-defined SCF since alternative e cannot be contained in
f (T +e )  UC (T +e ) = UC (T ), where the last equation follows from the simple observation
that the covering relation is unaffected by deleting Condorcet losers.
We now need to show that for some alternative e the restriction fe is a majoritarian
SCF that is P E -strategyproof and Pareto-optimal. Since this holds for any e  A, we just
pick e arbitrarily.
 Majoritarian: The fact that fe is a majoritarian SCF carries over trivially from f .
 P E -strategyproofness: Assume for a contradiction that fe is not P E -strategyproof.
Then, by Theorem 1 there exist tournaments T and T 0 on m alternatives such that
fe (T 0 ) PE fe (T ) with R  T \ T 0 . But since fe (T 0 ) = f (T 0+e ) and fe (T ) = f (T +e )
(and by the fact that E satisfies IIA), we get
f (T 0+e ) PE f (T +e ),
which contradicts P E -tournament-strategyproofness of f (as the two tournaments T 0+e
and T +e form a manipulation instance), and thus P E -strategyproofness.
 Pareto-optimality: By Lemma 2, this is equivalent to being a refinement of UC .
Thus, let T be an arbitrary tournament on m alternatives and consider the following
chain of set inclusions, which proves that fe  UC :
fe (T ) = f (T +e )  UC (T +e ) = UC (T ).
585

fiBrandt & Geist

By virtue of Lemma 3 it now suffices to check the claim for the restricted domain of
m = 5, which we do in the following lemma.
Lemma 4. For exactly five alternatives (i.e., m = 5) there is no majoritarian SCF f that
satisfies P F -strategyproofness and Pareto-optimality.
Proof. This base case of m = 5 alternatives was verified using our computer-aided approach,
i.e., we checked that, with |A| = 5 alternatives, there is no satisfying assignment for an
encoding of P F -tournament-strategyproofness (cf. Theorem 1) and being a refinement of
UC (cf. Lemma 2), which the SAT solver confirmed within seconds. A human-readable
proof of this claim has been extracted from the computer-aided approach and is presented
in Section 5.1.2.
Finally, this papers main result regarding P F -strategyproofness follows directly from
Lemmas 3 and 4.
Theorem 3. For any number of alternatives m  5 there is no majoritarian SCF f that
satisfies P F -strategyproofness and Pareto-optimality.
Proof. We prove the statement inductively. The base case of m = 5 is covered by Lemma 4.
For the induction step, we apply the contrapositive of Lemma 3 with E := F, which directly
yields the desired results.
While the number of voters required for this impossibility has been kept implicit so far,
an upper bound of at most m2  m  1 = 19 voters can be derived from the construction
in the proof of Theorem 1. In Section 5 we will see, however, that a human-readable proof
of Theorem 3 can be extracted, which only requires seven voters.
As a consequence of Theorem 3, virtually all common tournament solutionsexcept the
top cycle (see Remark 2)fail to be P F -strategyproof.
4.2.1 Remarks
Before we turn towards the technique of proof extraction, let us discuss some further insights
regarding Theorem 3, which have been, to a large extent, enabled by the universality of the
presented method.
Remark 1 (Strengthenings). It can be shown with the computer-aided method that
Theorem 3 holds even without the assumption of neutrality. Since then, however, the
optimizations based on canonical tournaments can no longer be used, extracted proofs
(cf. Section 5) are much more complex and we therefore decided to present the result with
neutrality here.23
The theorem can be further strengthened by additionally only requiring P F -single-edgestrategyproofness (cf. Section 3.2) or an even weaker variant of P F -strategyproofness where
the manipulator is only allowed to swap two adjacent alternatives (see, e.g., Sato, 2013).
23. In addition, running times are much longer, which, however, is not a major concern given that not many
conjectures had to be tested for this result.

586

fiFinding Strategyproof Social Choice Functions via SAT Solving

Remark 2 (The Top Cycle TC ). Note that Theorem 3 is not in conflict with the fact
that TC is P F -strategyproof, as, for m  4 alternatives, TC is strictly coarser than UC
and therefore not Pareto-optimal. Possibly, TC is even the finest majoritarian Condorcet
extension that satisfies P F -strategyproofness for m  5. We were able to verify this for
5  m  7 using our computer program. In the case of four alternatives, UC is a strict
refinement of TC and (as our method shows) still P F -strategyproof. For m = 8 the time and
space requirements appear to be prohibitive; already for m = 7 (despite all optimizations
and approximations) encoding and solving the problem takes almost 24 hours, while for
m = 6 it runs in about three minutes. It is not obvious whether an inductive argument can
extend these verified instances to larger numbers of alternatives (as, for instance, such an
induction step would require at least five alternatives).
Remark 3 (Other Preference Extensions). An advantage of the computer-aided approach is its universality. We can, for instance, very easily adapt the implementation to
check set extensions other than the ones by Kelly and Fishburn.
Interestingly, our main result only relies on a small fraction of the power of the Fishburn
extension: it suffices to only compare disjoint sets and sets that are contained in one another.
In formal terms, the following set extension suffices for the impossibility:

K

X Ri Y when X  Y = ,

X RiF Y if and only if X RiF Y when X  Y or Y  X,



otherwise.
Actually, it would even suffice to only compare sets X and Y such that |X  Y |  3.
We also checked a strengthening of the Fishburn extension: a voter prefers a set X to
a set Y if X is better than Y under both optimistic and pessimistic expectations.
Formally, X RiOP Y if and only if
x Ri y for all x  X and some y  Y , and
y Ri x for all y  Y and some x  X.
This extension is a weakening of both the optimistic and the pessimistic notions of strategyproofness in the Duggan-Schwartz Theorem (Duggan & Schwartz, 2000). In the majoritarian setting, P OP -strategyproofness leads to an analogous impossibility as in Theorem 3
for m  4 already.
Remark 4 (Generality of Lemma 3). Note that the proofs of the individual properties
within the inductive proof of Lemma 3 do only rely on the definition of fe and stand
independently of each other. Furthermore, it may be noted that Lemma 3 can even be
shown for refinements of arbitrary majoritarian SCFs g whose choice set g(T ) does not
shrink when Condorcet losers are removed from T (rather than Pareto-optimal majoritarian
SCFs).

5. Proof Extraction
A major concern regarding computer-aided proofs is the difficulty of checking their correctness. While our implementation correctly confirmed a number of existing results and this
587

fiBrandt & Geist

can be considered as testing, some doubts about the correctness of new results naturally
remain. Most SAT solvers offer some kind of proof trace, which can be checked by thirdparty-software. This, however, does not guarantee correctness of the encoding but only
confirms the unsatisfiability of the corresponding CNF formula.
In this section, we show how human-readable proofs of the desired statements can be
extracted from our approach, which can then be verified just as any manual mathematical
proof. The general idea of this proof extraction technique lies in finding and analyzing a
minimal unsatisfiable core (also referred to as a minimal unsatisfiable set (MUS)) of the
SAT instance. An unsatisfiable core of a CNF formula is a subset of clauses that is already
unsatisfiable by itself. If any subset of clauses of the unsatisfiable core is satisfiable, then
the core is called minimal. In our case, the minimal unsatisfiable core contains information
about the concrete instances of axioms that have to be employed to obtain an impossibility
(e.g., manipulation instances, applications of Pareto optimality, etc). This information can
be extracted in a straightforward way and reveals the structure and arguments of the proof.
We exemplify this technique in Section 5.1, in which we extract a human-readable proof
of our main result (Theorem 3). In Section 5.2 we additionally enrich this proof by a set of
minimal corresponding preference profiles, which then shows that the result of Theorem 3
holds for any setting with at least seven voters.
In general, extracting human-readable proofs serves two separate purposes. On the
one hand, a human-readable proof can significantly raise confidence in the correctness of
the results, basically by making verification of the approach obsolete since now the results
themselves are directly verifiable. On the other hand, the extracted proofs sometimes
provide additional insight into the problems via their arguments and structure. In our case,
the number of voters required for the impossibility would not have been (easily) accessible
directly.
5.1 A Human-Readable Proof of Theorem 3
In order to extract a human-readable proof of Theorem 3, or actually its main ingredient
Lemma 4, we have to follow a series of three steps:
1. Obtain a suitable MUS of the CNF formula that encodes a P F -tournamentstrategyproof refinement of UC on five alternatives
2. Decode the MUS into a human-readable format
3. Interpret the human-readable MUS to obtain a human-readable proof
While the first two steps are computer-aided and can be largely automated, step three
requires some manual effort.
5.1.1 Obtaining a Suitable MUS of the CNF Formula
Extracting a minimal unsatisfiable core is a feature offered by a range of SAT solvers. In
this paper, we use PicoMUS (part of PicoSAT, Biere, 2008) for this job.24 It should be
24. Compiled with trace support in order to use core extraction in addition to clause selector variables. This
significantly improves the size of the resulting MUS.

588

fiFinding Strategyproof Social Choice Functions via SAT Solving

noted, however, that while an MUS is inclusion-minimal, it does not necessarily represent
a smallest unsatisfiable set (i.e., with a minimal number of clauses or variables).25
As the number of clauses turned out to be a good proxy for proof complexity and length,
we tried to find an MUS with a small number of clauses. When run on the complete,
optimized SAT encoding as described in Section 3.2, PicoMUS returns an MUS with
55 clauses. This is already a massive reduction compared to more than three million clauses
in the original problem instance, but we found an even smaller MUS with only 16 clauses
by randomly sampling sets of tournaments to be used instead of the full domain of all
tournaments when generating our problem files. Another heuristic approach of considering
neighborhoods of single tournaments (for instance, all tournaments that can be reached
by changing at most two edges in the transitive tournament) yielded a less significant
improvement with a total of 25 clauses.
While it seems natural that larger domains are generally better as they lead to the required impossibility more often than smaller domains, larger domains actually tend towards
larger proofs and even miss very small proofs. For instance, for the domain size s = 200
(consisting of s labeled tournaments) no proof smaller than 18 clauses was found, while the
same number of runs with s = 50 produced four proofs with just 16 clauses each.26
Therefore, in our setting, a medium-sized domain (s = 50 or s = 100 in our experiments)
appears to be best suited. The complete results of running time and proof size analysis given
different domain sizes s can be obtained from Figures 8 and 9 in Appendix C.
5.1.2 Decoding the MUS into a Human-Readable Format
The next step is to make the obtained MUS more accessible to humans. To this end, we first
(automatically) add comments to the original CNF for each manipulation clause during its
creation, and then select those comments that belong to clauses in the MUS. The comments
contain witnesses for the manipulation instances found, i.e., information about the original
tournament T , the manipulated tournament T 0 , the respective choice sets f (T ) and f (T 0 ),
and the original preferences of the manipulator R (compare Definition 3). Furthermore,
any variable symbol can easily be decoded into the tournament and choice set it represents,
which is helpful in particular for all non-manipulation clauses (orbit condition and Paretooptimality).
The result of this step is presented in Figure 6, where each tournament is represented
by a lower triangular representation of its adjacency matrix (see the proof of Lemma 4 in
Section 5.1.3 for graphical representations).
5.1.3 Interpreting the MUS and Obtaining a Human-Readable Proof
From the witnessed MUS it is just a small step to a textual, human-readable proof. With
a bit of practice, one can quickly understand the structure of the proof: it starts from the
orbit condition in the first line and the refinement condition in the last line, which each
25. While the tool CAMUS by Liffiton and Sakallah (2008) is theoretically capable of finding a smallest
MUS (with a minimal number of clauses), it did not terminate in a reasonable amount of time on our
very large CNF instances.
26. In addition, medium-sized domains are more efficient regarding their running time per generated proof,
which admittedly plays only a minor, but still important role given that the total running time for large
domains is about 20 hours.

589

fiBrandt & Geist

p c n f 341 16
218 231 232 233
202 330 0
c T : 1111111111
233 202 0
c T : 1101100111
234 202 0
c T : 1101100111
218 218 0
c T : 1101100111
232 232 0
c T : 1101100111
248 338 0
c T : 1101100111
231 202 0
c T : 1101100111
247 202 0
c T : 1101100111
314 314 0
c T : 1100101110
c T : 1100101110
318 318 0
c T : 1100101110
c T : 1100101110
322 322 0
c T : 1100101110
326 326 0
c T : 1100101110
334 202 0
c T : 1100101110
202 0
314 318 322 326

234 247 248 0
> [ e ] ; T  : 1011100111 > [ d , e ] ; P i : b , d , c , e , a
> [ e ] ; T  : 0010100111 > [ a ] ; P i : b , c , d , a , e
> [ a , e ] ; T  : 0010100111 > [ a ] ; P i : b , c , d , a , e
> [ a ] ; T  : 1001000100 > [ e ] ; P i : e , c , a , d , b
> [ a , b , c , d ] ; T  : 1001000100 > [ a , c , d , e ] ; P i : e , c , a , d , b
> [ a , b , c , d , e ] ; T  : 1100100101 > [ b , c , e ] ; P i : b , e , c , d , a
> [ b , c , d ] ; T  : 1111111111 > [ e ] ; P i : a , e , b , c , d
> [ b , c , d , e ] ; T  : 1111111111 > [ e ] ; P i : a , e , b , c , d
> [ c ] ; T  : 1100100101 > [ e ] ; P i : b , d , e , a , c
> [ c ] ; T  : 1100110110 > [ b ] ; P i : b , c , d , e , a
> [ d ] ; T  : 1100100101 > [ b ] ; P i : b , d , e , a , c
> [ d ] ; T  : 1100110110 > [ a ] ; P i : b , c , e , a , d
> [ c , d ] ; T  : 1100110110 > [ a , b ] ; P i : b , e , a , c , d
> [ e ] ; T  : 1100110110 > [ d ] ; P i : b , c , d , e , a
> [ d , e ] ; T  : 1001111010 > [ d ] ; P i : c , a , d , e , b
330 334 338 0

Figure 6: A version of the extracted MUS, in which all manipulation instances (here: binary
clauses) have been decoded into a human-readable format: two mappings of tournaments
(original T and manipulated T 0 ) to choice sets and the truthful preferences of the manipulator P . This information covers all variables and thus suffices to also decode the remaining
clauses.

590

fiFinding Strategyproof Social Choice Functions via SAT Solving

Truthful choice


{e}





{a}  {e}



{b, c, d}
f (T1 ) =

{b, c, d}  {e}





{a}



{a}  {b, c, d}


{c}





{d}
f (T2 ) = {c, d}



{e}




{d, e}
f (Te0 )  UC (Te0 ) = {e}

Manipulated choice

Manipulators
preferences

f (Ta )  UC (Ta ) = {a}

b, c, d, a, e

f (Te )  UC (Te ) = {e}

a, e, b, c, d

(
{e}
T10 is isof (T10 ) =
{a, c, d, e} morphic27 to T1


{b}



{a}
T20 is isof (T20 ) =
{a, b} morphic27 to T2



{d}
f (Td )  UC (Td ) = {d}
f (T2 ) = {c, e}

28

e, c, a, d, b

b, e, a, c, d

b, c, d, e, a
c, a, d, e, b
a, c, b, e, d

Table 2: Set of manipulation instances (one per line) to conclude that f (T1 ) = A =
{a, b, c, d, e} and f (T2 ) = {c, d, e}. Each of the truthful choices considered here leads
to a P F -tournament-manipulation instance (a contradiction to the assumption of P F tournament-strategyproofness). The tournaments are defined in Figure 7.
leave some (limited) possibilities for respective choice sets, and then excludes all possible
choices one after another by suitable manipulation instances. The full proof runs as follows.
Proof of Lemma 4. For a contradiction, let f be a majoritarian SCF on A = {a, b, c, d, e}
that satisfies P F -strategyproofness and Pareto-optimality. Recall that, by Theorem 1, f
is P F -tournament-strategyproof, too, and by Lemma 2 it has to be a refinement of UC
(i.e., f  UC ). Let furthermore T1 and T2 be the tournaments depicted in Figure 7. We
proceed in three steps: first, we show that f (T1 ) = UC (T1 ) = A. Second, we argue that
f (T2 ) = UC (T2 ) = {c, d, e}. And last, we prove that these two insights actually forms the
basis of a manipulation instance, which leads to the desired contradiction.
Let us start with f (T1 ) = UC (T1 ) = A. First, note that since the alternatives {b, c, d}
form an orbit we know that either {b, c, d}  f (T1 ) or {b, c, d}  f (T1 ) =  (cf. Definition 5).
We are going to exclude all remaining choice sets through P F -tournament-manipulation instances. As a first example, suppose f (T1 ) = {e}. Then a voter with individual preferences
P : b, c, d, a, e could reverse the edges (b, a) and (b, c) in T1 such that a transitive tournament Ta with Condorcet winner a results (which needs to be uniquely selected by f since
f  UC ). Since, however, {a} PF {e}, this contradicts P F -tournament-strategyproofness.
The same example also works to exclude f (T1 ) = {a, e}. Note how these arguments correspond to lines 5 to 8 of the extracted MUS in Figure 6. The (analogous) manipulation
instances for all possible choice sets other than A = {a, b, c, d, e} are given in Table 2 and
Figure 7.
591

fiBrandt & Geist

b

b

b

d

c

d

c

d

c

e

a

e

a

e

a

(a) T1

(b) T2

(c) Ta

b

b

b

d

c

d

e

a

e

(d) Te

(e)

b

c

d

a

e

T10

c

a
(f)

b

T20

b

d

c

d

c

d

c

e

a

e

a

e

a

(g) Td

(h) T200

(i) Te0

Figure 7: Tournaments which are required in the proof of Lemma 4. The uncovered sets are
marked in grey; edges that have been (for Te0 : will be) reversed by the manipulating voter
(cf. Table 2) are depicted as thick edges. Note the proof would also succeed with less edge
reversals in Ta , Te , Td , and Te0 (such that these tournaments only have Condorcet winners
rather than being transitive). These transitive tournaments are isomorphic, however, and
thus can be succinctly represented as the single clause 202 in the extracted MUS.
For f (T2 ) = UC (T2 ) = {c, d, e}, first observe that f (T2 )  UC (T2 ) = {c, d, e} and hence
we only need to exclude any strict subset of {c, d, e}. Again we proceed by giving a possible
manipulation instance for each of those subsets. The complete list is to be found in Table 2
and Figure 7. Observe how the last line in Table 2 excludes f (T2 ) = {c, e} by considering
it as the manipulated choice for the (known) truthful choice f (Te0 )  UC (Te0 ) = {e}.
As a last step, we provide a manipulation instance based on f (T1 ) = A and f (T2 ) =
{c, d, e}. For this, first observe that by renaming the alternatives we get f (T200 ) = {b, c, e}
and so the manipulation instance results from a voter with preferences P0 : b, e, c, d, a. This
!
!
a b c d e
a b c d e
27. The isomorphisms are 1 =
and 2 =
, respectively.
b e c d a
d c a e b
28. The SAT solver actually returned an isomorphic copy of this instance, which we restructured to improve
readability.

592

fiFinding Strategyproof Social Choice Functions via SAT Solving

voter can reverse the edges (d, a) and (e, c) in T1 to create T200 and obtain the P F -preferred
outcome {b, c, e}, a contradiction to the P F -strategyproofness of f .
Note that actually only the manipulation instance with f (T1 ) = {a}  {b, c, d} and
f (T10 ) = {a, c, d, e} requires the Fishburn-extension; for the other instances the Kellyextension suffices.
5.2 Number of Voters Required
In the previous parts of the paper we have taken advantage of the fact that our condition
of tournament-strategyproofness abstracted away any reference to voters. It is interesting
to ask, however, how many voters are at least required for the obtained impossibility of
Theorem 3 to hold. The construction in the proof of Theorem 1 gives an implicit upper
bound of m2  m  1 = 19 voters, but this can be further improved to seven voters.
By slightly modifying the techniques described by Brandt, Geist, and Seedig (2014), we
were able to (automatically) construct minimal preference profiles for all steps in Proof 5.1.3.
While Brandt et al. (2014) provided a SAT-formulation of whether a given majority relation
can be induced by a given number of voters, we extended this framework to include axioms
for manipulation instances. In more detail, we re-used the axioms for linear preferences
and majority implications, but added axioms for the truthful preferences of the manipulator
and majority implications for the manipulated profile.
The profiles that we generated for all steps in the proof of Lemma 4 in Section 5.1.3
are given in Appendix D. The largest of these profiles contains seven voters, and all other
profiles can easily be extended to seven voters by adding pairs of voters with opposite
preferences. While this observation shows that seven is the smallest number of voters which
can be achieved with our extracted proof, it remains open whether, by another proof, the
number of voters can be further reduced below seven.

6. Conclusion
We have extended and applied computer-aided theorem proving based on SAT solving to
extensively analyze Kelly- and Fishburn-strategyproof majoritarian SCFs. This has led to a
range of results, both positive and negative. An important novel contribution of our work is
the ability to extract a human-readable proof from negative SAT instances. This eliminates
the need to verify the computer-aided method since impossibility results can directly be
checked based on their human-readable proofs. Based on the ease of adaptation of the
proposed method, we anticipate further insights to spring from the overall approach in the
future. Apart from simply applying our system to further investigate strategyproofness,
other potential applications related to our line of work include:
Unrestricted SCFs In order to reduce complexity, we have studied majoritarian SCFs
only. The framework, however, is applicable in the same way to general SCFs, which
operate on full preference profiles (rather than majority relations). The challenge then is
to find a suitable representation of such preference profiles and potentially corresponding
inductive arguments on the number of voters.
593

fiBrandt & Geist

Further axioms Some preliminary experiments suggest that our technique can easily
be applied to a range of properties other than strategyproofness, these deserve further
investigation. In many cases it suffices to just formalize and implement the additional
axioms. Of particular interest could be such properties that link the behavior of SCFs for
different domain sizes. As initial steps in this direction, we were able to extend the approach
to cover the property of participation (Brandl et al., 2015; Brandt et al., 2016c) as well as
a weak version of composition-consistency (cf. Section 4.1).
Smallest number of voters required As mentioned in Section 5.2, Theorem 3 holds
for any number of voters n  7, but it is not known whether this number is minimal. One
could adapt proof extraction as presented in Section 5 to search for a smallest proof in the
number of voters, rather than in the number of clauses, to settle this question.
Generalization of the inductive argument
It appears reasonable to investigate
whether the inductive argument of Lemma 3 can be further generalized to a whole class of
properties/axioms, ideally based on their logical form. As in the work of Geist and Endriss
(2011), this would then enable an automated search for further theorems about SCFs.
Apart from these concrete ideas, applications of the general approach can be envisioned
in many areas of theoretical economics.

Acknowledgments
This material is based upon work supported by Deutsche Forschungsgemeinschaft under
grants BR 2312/7-2 and BR 2312/9-1. The paper benefitted from discussions at the COST
Action Meeting IC1205 on Computational Social Choice (Maastricht, 2014), the 13th International Conference on Autonomous Agents and Multiagent Systems (Paris, 2014), the
5th International Workshop on Computational Social Choice (Pittsburgh, 2014), and the
Dagstuhl Seminar on Computational Social Choice: Theory and Applications (Dagstuhl,
2015). The authors in particular thank Jasmin Christian Blanchette, Markus Brill, Hans
Georg Seedig, and Bill Zwicker for helpful discussions and their support, and three anonymous reviewers for their valuable comments and suggestions to improve the paper.

Appendix A. Proof of Lemma 1
We first show that the orbit condition is equivalent to a statement about automorphisms:
Lemma 5. Let f be a tournament choice function. Then the following statement is equivalent to the orbit condition:
(f (Tc )) = f (Tc ) for all canonical tournaments Tc and their automorphisms .

(8)

Proof. Let f be a tournament choice function and Tc a canonical tournament. For the direction from left to right, let furthermore O  OTc an orbit on Tc . Now pick two alternatives
a, b  O. We show that either both alternatives are chosen by f or neither one is. Since a
and b are in the same orbit, there must be an automorphism  on Tc for which (a) = b.
Observe that a  f (Tc ) if and only if b  (f (Tc )) if and only if b  f (Tc ), where the last
step is an application of Condition (8).
594

fiFinding Strategyproof Social Choice Functions via SAT Solving

For the converse, let  be an automorphism on Tc , pick an arbitrary alternative a  A
and consider its inverse image 1 (a) =: b. Since a and b are in the same orbit, it holds by
the orbit condition that a  f (Tc ) if and only if b  f (Tc ). Furthermore, as (b) = a we
get that a  f (Tc ) if and only if a  (f (Tc )). Thus, f (Tc ) = (f (Tc )), which is what we
wanted to prove.
Next we prove a general statement about how to split any isomorphism into a canonical
isomorphism and an automorphism.
Lemma 6. Any isomorphism  : Tc  T can be decomposed into the canonical isomorphism
T and an automorphism  : Tc  Tc . I.e., for any isomorphism  : Tc  T there is an
automorphism  : Tc  Tc such that  = T  .
Proof. Define  : Tc  Tc by setting  := T1  . Since inverses and compositions of
isomorphisms are themselves isomorphisms,
it follows

 directly that  is an automorphism.
Furthermore, T   = T  T1   = T  T1   = .
Lemmas 5 and 6 together can then be used to prove Lemma 1:
Lemma 1. For any tournament choice function, neutrality is equivalent to the conjunction
of the orbit condition and canonical isomorphism equality.
Proof. Let f be a tournament choice function and first note that by Lemma 5 we might use
Condition (8) rather than the orbit condition. Therefore, the direction from left to right is
trivially true.
For the direction from right to left, we first only show that canonical isomorphism
equality (2) together with Condition (8) implies neutrality for canonical tournaments: So
let Tc be a canonical tournament,  a permutation and define T 0 := (Tc ). By Lemma 6, we
can decompose the isomorphism  : Tc  T 0 such that  = T0   for some automorphism
 on Tc . Then the following chain of equalities holds, which proves the claim for canonical
tournaments:
(2)

(8)

f ((Tc )) = f (T 0 ) = T 0 (f (Tc0 )) = T 0 (f (Tc )) = T 0 ((f (Tc ))) = (f (Tc )). (9)
For arbitrary tournaments T and permutations , we write T as T (Tc ) and obtain
f ((T )) = f ((T (Tc ))) = f ((  T )(Tc )),
which, since Tc is canonical, is equal to
(2)

(  T )(f (Tc ))) = (T (f (Tc ))) = (f (T ))
by Condition (9). This finishes the proof.

595

fiBrandt & Geist

Appendix B. Pseudo-Code for Encoding
We present (as an illustrative example) the simple pseudo-code of Algorithm 3 to generate the CNF form of Axiom 1 (functionality of the tournament choice function; cf. Section 3.1.1).
foreach Tournament T do
foreach Set X do
variable(c(T, X));
newClause();
foreach Set Y do
foreach Set Z 6= Y do
variable not(c(T, Y ));
variable not(c(T, Z));
newClause();
Algorithm 3: Functionality of the tournament choice function

Appendix C. MUS Search Analysis (Running Time and Size of MUS)
In this appendix, we present the complete results of the running time (Figure 8) and MUS
size (measured in number of clauses; Figure 9) analyses given different sizes s of randomly
sampled domains. In our setting, sizes of s = 50 or s = 100 appear to offer good results
both in terms of running time and actually finding small proofs.

sample size s

10

11

20

64.4

78

50

18.2
359

100

8.2
699

200

8.6
951

19.5

1,000

500
0

200 400 600 800 1,000

70.5
0
20
40
60
80
Average running time per proof
(in seconds)

Number of proofs found
(out of 1000 runs)

Figure 8: Number of unsatisfiable instances (i.e., proofs found) and running time results
under heuristics with different numbers s of sampled tournaments (labeled, 1000 runs).

596

fiFinding Strategyproof Social Choice Functions via SAT Solving

s=10

2

(16,1)

1
0

s=20

5

(16,1)

s=50

0
60
40
(16,4)

20
0

s=100

100
50

s=200

Number of proofs founds (in 1000 runs) with

10

0
200
150
100
50
0

(16,1)

(18,29)

s=500

300
200
(19,3)

100
0
0

20

40
60
80
100
120
Size of the proofs (number of clauses)

140

160

Figure 9: The sizes of MUSes (proofs) under heuristics with different numbers s of sampled
tournaments (labeled). The size of the MUS obtained from running on the full domain is
indicated by a red line. For improved readability, the size and multiplicity of the smallest
MUS is explicitly listed.

597

fiBrandt & Geist

Appendix D. Profiles for the Extracted Proof of Theorem 3
Here we display the MUS of Figure 6 enriched with minimal preference profiles for each
step in the proof of Theorem 3. The profiles were generated and checked for minimality on
a computer (and using a SAT solver) in less than a second each.
p c n f 341 16
218 231 232 233 234 247 248 0
Agent 0 : b , c , d , a , e
Agent 1 : a , e , c , d , b
Agent 2 : e , d , b , c , a
202 330 0
c T : 1111111111 > [ e ] ; T  : 1011100111 > [ d , e ] ; P i : b , d , c , e , a
Agent 0 : b , d , c , e , a
Agent 1 : c , d , a , e , b
Agent 2 : e , c , d , b , a
Agent 3 : a , e , d , c , b
Agent 4 : e , d , b , a , c
Manipulated p r e f e r e n c e s o f a g e n t 0 : b , a , c , d , e
233 202 0
c T : 1101100111 > [ e ] ; T  : 0010100111 > [ a ] ; P i : b , c , d , a , e
234 202 0
c T : 1101100111 > [ a , e ] ; T  : 0010100111 > [ a ] ; P i : b , c , d , a , e
Agent 0 : b , c , d , a , e
Agent 1 : a , e , d , b , c
Agent 2 : e , c , d , b , a
Manipulated p r e f e r e n c e s o f a g e n t 0 : a , c , b , d , e
218 218 0
c T : 1101100111 > [ a ] ; T  : 1001000100 > [ e ] ; P i : e , c , a , d , b
232 232 0
c T : 1101100111 > [ a , b , c , d ] ; T  : 1001000100 > [ a , c , d , e ] ; P i : e , c , a , d , b
Agent 0 : e , c , a , d , b
Agent 1 : d , a , e , b , c
Agent 2 : d , a , e , b , c
Agent 3 : d , e , b , c , a
Agent 4 : c , e , b , d , a
Agent 5 : b , c , a , e , d
Agent 6 : b , a , c , e , d
Manipulated p r e f e r e n c e s o f a g e n t 0 : b , a , c , d , e
248 338 0
c T : 1101100111 > [ a , b , c , d , e ] ; T  : 1100100101 > [ b , c , e ] ; P i : 1 > 4 >
2 > 3 > 0
Agent 0 : 1 > 4 > 2 > 3 > 0
Agent 1 : 2 > 3 > 0 > 4 > 1
Agent 2 : 2 > 4 > 3 > 1 > 0
Agent 3 : 0 > 4 > 3 > 1 > 2
Agent 4 : 1 > 0 > 4 > 2 > 3
Manipulated p r e f e r e n c e s o f a g e n t 0 :
1 > 2 > 0 > 4 > 3
231 202 0
c T : 1101100111 > [ b , c , d ] ; T  : 1111111111 > [ e ] ; P i : 0 > 4 > 1 > 2 > 3

598

fiFinding Strategyproof Social Choice Functions via SAT Solving

247 202 0
c T : 1101100111 > [ b , c , d , e ] ; T  : 1111111111 > [ e ] ; P i : 0 > 4 > 1 > 2 > 3
Agent 0 : 0 > 4 > 1 > 2 > 3
Agent 1 : 3 > 1 > 2 > 0 > 4
Agent 2 : 4 > 2 > 3 > 1 > 0
Manipulated p r e f e r e n c e s o f a g e n t 0 :
4 > 0 > 3 > 2 > 1
314 314 0
c T : 1100101110 > [ c ] ; T  :
Agent 0 : 1 > 3 > 4 > 0 > 2
Agent 1 : 4 > 3 > 1 > 2 > 0
Agent 2 : 4 > 1 > 2 > 0 > 3
Agent 3 : 2 > 0 > 3 > 4 > 1
Agent 4 : 2 > 0 > 3 > 4 > 1
Manipulated p r e f e r e n c e s o f
1 > 2 > 0 > 4 > 3
c T : 1100101110 > [ c ] ; T  :
Agent 0 : 1 > 2 > 3 > 4 > 0
Agent 1 : 0 > 3 > 4 > 2 > 1
Agent 2 : 0 > 4 > 2 > 3 > 1
Agent 3 : 4 > 1 > 2 > 0 > 3
Agent 4 : 3 > 4 > 1 > 2 > 0
Manipulated p r e f e r e n c e s o f
3 > 1 > 2 > 0 > 4

1100100101 > [ e ] ; P i : 1 > 3 > 4 > 0 > 2

agent 0 :
1100110110 > [ b ] ; P i : 1 > 2 > 3 > 4 > 0

agent 0 :

318 318 0
c T : 1100101110 > [ d ] ; T  : 1100100101 > [ b ] ; P i : 1 > 3 > 4 > 0 > 2
c T : 1100101110 > [ d ] ; T  : 1100110110 > [ a ] ; P i : 1 > 2 > 4 > 0 > 3
Agent 0 : 1 > 2 > 4 > 0 > 3
Agent 1 : 3 > 4 > 1 > 2 > 0
Agent 2 : 4 > 0 > 2 > 3 > 1
Agent 3 : 2 > 0 > 3 > 4 > 1
Agent 4 : 1 > 0 > 3 > 4 > 2
Manipulated p r e f e r e n c e s o f a g e n t 0 :
3 > 1 > 2 > 0 > 4
322 322 0
c T : 1100101110 > [ c , d ] ; T  : 1100110110 > [ a , b ] ; P i : 1 > 4 > 0 > 2 > 3
Agent 0 : 1 > 4 > 0 > 2 > 3
Agent 1 : 2 > 0 > 3 > 4 > 1
Agent 2 : 3 > 4 > 1 > 2 > 0
Manipulated p r e f e r e n c e s o f a g e n t 0 :
1 > 0 > 3 > 4 > 2
326 326 0
c T : 1100101110 > [ e ] ; T  : 1100110110 > [ d ] ; P i : 1 > 2 > 3 > 4 > 0
Agent 0 : 1 > 2 > 3 > 4 > 0
Agent 1 : 0 > 3 > 4 > 2 > 1
Agent 2 : 0 > 4 > 2 > 3 > 1
Agent 3 : 4 > 1 > 2 > 0 > 3
Agent 4 : 3 > 4 > 1 > 2 > 0
Manipulated p r e f e r e n c e s o f a g e n t 0 :
3 > 1 > 2 > 0 > 4

599

fiBrandt & Geist

334 202 0
c T : 1100101110 > [ d , e ] ; T  : 1001111010 > [ d ] ; P i : 2 > 0 > 3 > 4 > 1
Agent 0 : 2 > 0 > 3 > 4 > 1
Agent 1 : 1 > 4 > 0 > 2 > 3
Agent 2 : 3 > 4 > 1 > 2 > 0
Manipulated p r e f e r e n c e s o f a g e n t 0 :
3 > 1 > 0 > 4 > 2
202 0
Agent 0 : 4 > 3 > 2 > 1 > 0
314 318 322 326 330 334 338 0
Agent 0 : 2 > 0 > 3 > 4 > 1
Agent 1 : 3 > 4 > 1 > 2 > 0
Agent 2 : 4 > 1 > 2 > 0 > 3

References
Barbera, S. (1977). Manipulation of social decision functions. Journal of Economic Theory,
15 (2), 266278.
Barbera, S. (2010). Strategy-proof social choice. In Arrow, K. J., Sen, A. K., & Suzumura,
K. (Eds.), Handbook of Social Choice and Welfare, Vol. 2, chap. 25, pp. 731832.
Elsevier.
Biere, A. (2008). PicoSAT essentials. Journal on Satisfiability, Boolean Modeling and
Computation (JSAT), 4, 7579.
Biere, A., Heule, M., van Maaren, H., & Walsh, T. (Eds.). (2009). Handbook of Satisfiability,
Vol. 185 of Frontiers in Artificial Intelligence and Applications. IOS Press.
Blanchette, J. C., & Nipkow, T. (2010). Nitpick: A counterexample generator for higherorder logic based on a relational model finder. In Proceedings of the First International
Conference on Interactive Theorem Proving, pp. 131146. Springer.
Brandl, F., Brandt, F., Geist, C., & Hofbauer, J. (2015). Strategic abstention based on
preference extensions: Positive results and computer-generated impossibilities. In Proceedings of the 24th International Joint Conference on Artificial Intelligence (IJCAI),
pp. 1824. AAAI Press.
Brandt, F. (2015). Set-monotonicity implies Kelly-strategyproofness. Social Choice and
Welfare, 45 (4), 793804.
Brandt, F., & Brill, M. (2011). Necessary and sufficient conditions for the strategyproofness of irresolute social choice functions. In Proceedings of the 13th Conference on
Theoretical Aspects of Rationality and Knowledge (TARK), pp. 136142. ACM Press.
Brandt, F., Brill, M., & Harrenstein, P. (2016a). Tournament solutions. In Brandt, F.,
Conitzer, V., Endriss, U., Lang, J., & Procaccia, A. D. (Eds.), Handbook of Computational Social Choice, chap. 3. Cambridge University Press.
Brandt, F., Geist, C., & Harrenstein, P. (2016b). A note on the McKelvey uncovered set
and Pareto optimality. Social Choice and Welfare, 46 (1), 8191.
600

fiFinding Strategyproof Social Choice Functions via SAT Solving

Brandt, F., Geist, C., & Peters, D. (2016c). Optimal bounds for the no-show paradox via
SAT solving. In Proceedings of the 15th International Conference on Autonomous
Agents and Multi-Agent Systems (AAMAS), pp. 314322. IFAAMAS.
Brandt, F., Geist, C., & Seedig, H. G. (2014). Identifying k-majority digraphs via SAT
solving. In Proceedings of the 1st AAMAS Workshop on Exploring Beyond the Worst
Case in Computational Social Choice (EXPLORE).
Caminati, M. B., Kerber, M., Lange, C., & Rowat, C. (2015). Sound auction specification
and implementation. In Proceedings of the 16th ACM Conference on Economics and
Computation (ACM-EC), pp. 547564. ACM Press.
Chatterjee, S., & Sen, A. (2014). Automated reasoning in social choice theory  some
remarks. Mathematics in Computer Science, 8 (1), 510.
Ching, S., & Zhou, L. (2002). Multi-valued strategy-proof social choice rules. Social Choice
and Welfare, 19 (3), 569580.
Cina, G., & Endriss, U. (2015). A syntactic proof of Arrows theorem in a modal logic
of social choice functions. In Proceedings of the 14th International Conference on
Autonomous Agents and Multi-Agent Systems (AAMAS), pp. 10091017. IFAAMAS.
Conitzer, V., & Sandholm, T. (2002). Complexity of mechanism design. In Proceedings
of the 18th Annual Conference on Uncertainty in Artificial Intelligence (UAI), pp.
103110.
Drummond, J., Perrault, A., & Bacchus, F. (2015). SAT is an effective and complete
method for solving stable matching problems with couples. In Proceedings of the 24th
International Joint Conference on Artificial Intelligence (IJCAI), pp. 518525. AAAI
Press.
Duggan, J., & Schwartz, T. (2000). Strategic manipulability without resoluteness or shared
beliefs: Gibbard-Satterthwaite generalized. Social Choice and Welfare, 17 (1), 8593.
Erdamar, B., & Sanver, M. R. (2009). Choosers as extension axioms. Theory and Decision,
67 (4), 375384.
Feldman, A. (1979). Manipulation and the Pareto rule. Journal of Economic Theory, 21,
473482.
Fishburn, P. C. (1972). Even-chance lotteries in social choice theory. Theory and Decision,
3 (1), 1840.
Fishburn, P. C. (1977). Condorcet social choice functions. SIAM Journal on Applied Mathematics, 33 (3), 469489.
Fisher, D. C., & Reeves, R. B. (1995). Optimal strategies for random tournament games.
Linear Algebra and its Applications, 217, 8385.
Frechette, A., Newman, N., & Leyton-Brown, K. (2016). Solving the station repacking problem. In Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI).
AAAI Press.
Gardenfors, P. (1976). Manipulation of social choice functions. Journal of Economic Theory,
13 (2), 217228.
601

fiBrandt & Geist

Gardenfors, P. (1979). On definitions of manipulation of social choice functions. In Laffont,
J. J. (Ed.), Aggregation and Revelation of Preferences. North-Holland.
Geist, C., & Endriss, U. (2011). Automated search for impossibility theorems in social
choice theory: Ranking sets of objects. Journal of Artificial Intelligence Research, 40,
143174.
Grandi, U., & Endriss, U. (2013). First-order logic formalisation of impossibility theorems
in preference aggregation. Journal of Philosophical Logic, 42 (4), 595618.
Kelly, J. S. (1977). Strategy-proofness and social choice functions without single-valuedness.
Econometrica, 45 (2), 439446.
Laslier, J.-F. (1997). Tournament Solutions and Majority Voting. Springer-Verlag.
Liffiton, M. H., & Sakallah, K. A. (2008). Algorithms for computing minimal unsatisfiable
subsets of constraints. Journal of Automated Reasoning, 40 (1), 133.
McGarvey, D. C. (1953). A theorem on the construction of voting paradoxes. Econometrica,
21 (4), 608610.
McKay, B. D., & Piperno, A. (2013). Practical graph isomorphism, II. Journal of Symbolic
Computation.
Nehring, K. (2000). Monotonicity implies generalized strategy-proofness for correspondences. Social Choice and Welfare, 17 (2), 367375.
Nipkow, T. (2009). Social choice theory in HOL: Arrow and Gibbard-Satterthwaite. Journal
of Automatated Reasoning, 43, 289304.
Sanver, M. R., & Zwicker, W. S. (2012). Monotonicity properties and their adaption to
irresolute social choice rules. Social Choice and Welfare, 39 (23), 371398.
Sato, S. (2008). On strategy-proof social choice correspondences. Social Choice and Welfare,
31, 331343.
Sato, S. (2013). A sufficient condition for the equivalence of strategy-proofness and nonmanipulability by preferences adjacent to the sincere one. Journal of Economic Theory, 148, 259278.
Scott, A., & Fey, M. (2012). The minimal covering set in large tournaments. Social Choice
and Welfare, 38 (1), 19.
Tang, P., & Lin, F. (2009). Computer-aided proofs of Arrows and other impossibility
theorems. Artificial Intelligence, 173 (11), 10411053.
Tang, P., & Lin, F. (2011). Discovering theorems in game theory: Two-person games with
unique pure nash equilibrium payoffs. Artificial Intelligence, 175 (1415), 20102020.
Taylor, A. D. (2005). Social Choice and the Mathematics of Manipulation. Cambridge
University Press.
Tseitin, G. S. (1983). On the complexity of derivation in propositional calculus. In Automation of Reasoning, pp. 466483. Springer.

602

fiJournal of Artificial Intelligence Research 55 (2016) 995-1023

Submitted 08/15; published 04/16

A Distributed Representation-Based Framework for
Cross-Lingual Transfer Parsing
Jiang Guo
Wanxiang Che

JGUO @ IR . HIT. EDU . CN
CAR @ IR . HIT. EDU . CN

Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology
Harbin, Heilongjiang, China

David Yarowsky

YAROWSKY @ JHU . EDU

Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD, USA

Haifeng Wang

WANGHAIFENG @ BAIDU . COM

Baidu Inc., Beijing, China

Ting Liu

TLIU @ IR . HIT. EDU . CN

Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology
Harbin, Heilongjiang, China

Abstract
This paper investigates the problem of cross-lingual transfer parsing, aiming at inducing dependency parsers for low-resource languages while using only training data from a resource-rich
language (e.g., English). Existing model transfer approaches typically dont include lexical features, which are not transferable across languages. In this paper, we bridge the lexical feature gap
by using distributed feature representations and their composition. We provide two algorithms for
inducing cross-lingual distributed representations of words, which map vocabularies from two different languages into a common vector space. Consequently, both lexical features and non-lexical
features can be used in our model for cross-lingual transfer. Furthermore, our framework is flexible
enough to incorporate additional useful features such as cross-lingual word clusters. Our combined
contributions achieve an average relative error reduction of 10.9% in labeled attachment score as
compared with the delexicalized parser, trained on English universal treebank and transferred to
three other languages. It also significantly outperforms state-of-the-art delexicalized models augmented with projected cluster features on identical data. Finally, we demonstrate that our models
can be further boosted with minimal supervision (e.g., 100 annotated sentences) from target languages, which is of great significance for practical usage.

1. Introduction
Dependency Parsing has been one of the long-standing central problems in natural language processing (NLP). The goal of dependency parsing is to induce implicit tree structures for natural language
sentence following the dependency grammar, which can be highly beneficial for various downstream
tasks, such as question answering, machine translation and knowledge mining/representation. The
majority of work on dependency parsing has been dedicated to resource-rich languages, such as English and Chinese. For these languages, there exists large-scale annotated treebanks that can be used
2016 AI Access Foundation. All rights reserved.

fiG UO , C HE , YAROWSKY, WANG & L IU

for supervised training of dependency parsers, such as the Penn Treebank (Marcus, Marcinkiewicz,
& Santorini, 1993; Xue, Xia, Chiou, & Palmer, 2005). However, for most of the languages in the
world, there are very few or even no labeled training data for parsing, and it is both labor intensive
and time consuming to manually annotate treebanks for all languages. This fact has given rise to
a range of research on unsupervised methods (Klein & Manning, 2004), and transfer methods (Hwa, Resnik, Weinberg, Cabezas, & Kolak, 2005; McDonald, Petrov, & Hall, 2011) for linguistic
structure prediction.
Considering that the unsupervised methods fall far behind the transfer methods in terms of
accuracy, as well as the difficulty in evaluation, we will focus on the transfer methods in this study.
We attempt to build parsers for low-resource languages by exploiting treebanks from resource-rich
languages. There are two approaches to linguistic transfer in general, namely data transfer and
model transfer. Data transfer methods emphasizes the creation of artificial training data that can
be used for supervised training on the target language side. They have the appealing property
that they can learn language-specific linguistic structures effectively. The major drawbacks are
the requirement of parallel data and the noise in the automatically created training data introduced
by word alignment-based projection. On the other hand, model transfer methods build models on
the source language side, which are used directly for parsing target languages without the need of
creating annotated data in target languages.
This paper falls into the latter category. The major obstacle in transferring a parsing system
from one language to another is the lexical features (e.g., words) that are not directly transferable
across languages. To address this challenge, McDonald et al. (2011) built a delexicalized parser  a parser that only has non-lexical features. A delexicalized parser makes sense in that POS
tag features are significantly predictive for unlabeled dependency parsing. However, for labeled
dependency parsing, especially for semantic-oriented dependencies like Stanford typed dependencies (De Marneffe et al., 2006; De Marneffe & Manning, 2008), these non-lexical features are not
predictive enough. Tackstrom, McDonald, and Uszkoreit (2012) proposed to learn cross-lingual
word clusters from multilingual paralleled unlabeled data through word alignments, and apply these
clusters as features for semi-supervised delexicalized parsing. Word clusters can be thought of as a
kind of coarse-grained representations of words. Thus, this approach partially fills the gap of lexical
features in cross-lingual learning of dependency parsing.
This paper proposes a novel approach for cross-lingual dependency parsing that is based on
pure distributed feature representations. In contrast to the discrete feature representations used in
traditional dependency parsers, distributed representations map symbolic features into a continuous
representation space, that can be shared across languages. Therefore, our model has the ability
to utilize both lexical and non-lexical features naturally. Specifically, our framework contains two
primary components:
 A neural network-based dependency parser. We expect a non-linear model for dependency
parsing in our study, because distributed feature representations are shown to be more effective in non-linear architectures than in linear architectures (Wang & Manning, 2013). Chen
and Manning (2014) proposed a transition-based dependency parser using a neural network
architecture, which is simple but works well on benchmark datasets. Briefly, this model simply replaces the predictor in transition-based dependency parser with a well-designed neural
network classifier. We will provide explanations for the merits of this model in Section 3, as
well as how we adapt it to the cross-lingual task.
996

fiR EPRESENTATION L EARNING FOR C ROSS -L INGUAL T RANSFER PARSING

 Cross-lingual word representation learning. The key to filling the lexical feature gap is to
project the representations of these features from different languages into a common vector
space, preserving the translational equivalence. We will study and compare two approaches
of learning cross-lingual word representations in Section 4. The first approach is named
robust projection, and the second approach is based on canonical correlation analysis. Both
approaches are simple to implement and are scalable to large data.
Another drawback of the model transfer methods is that they focus only on the universal structures across various languages, and thus lack the ability of recovering the target language-specific
structures. Therefore, it is necessary to conduct target language adaptation on the top of the transferred models. We introduce a practical and straightforward solution by incorporating minimal
supervision from target languages (Section 6).
We evaluate our models on the universal multilingual treebanks v2.0 (McDonald et al., 2013).
Case studies include transferring from English (EN) to German (DE), Spanish (ES) and French
(FR). Experiments show that by incorporating lexical features, the performance of cross-lingual
dependency parsing can be improved significantly. By further embedding cross-lingual cluster features (Tackstrom et al., 2012), we achieve an average relative error reduction of 10.9% in labeled
attachment score (LAS), as compared with the delexicalized parsers. It also significantly outperforms the delexicalized models of McDonald et al. augmented with cluster features on identical
data. In addition, we show that by using a small amount of labeled training data (e.g., 100 sentences) at the target language side for parameter adaptation (minimal supervision), the performance
of our cross-lingual transfer system can be boosted, and the recalls of language-specific dependency
structures are improved dramatically.1
The original major contributions of this paper include:
 We propose a novel and flexible cross-lingual learning framework for dependency parsing
based on distributed representations, which can effectively incorporate both lexical and nonlexical features.
 We present two novel and effective approaches for inducing cross-lingual word representation
that bridge the lexical feature gap in cross-lingual dependency parsing transfer.
 We show that cross-lingual word cluster features can be effectively embedded into our model,
leading to significant additive improvements.
 We show that the our cross-lingual transfer systems can be easily and effectively adapted to
target languages with minimal supervision, demonstrating great potential in practical usage.

2. Background
This section describes the necessary background which is crucial for understanding our transfer
parsing framework.
1. This article is a thoroughly revised and extended version of the work of Guo, Che, Yarowsky, Wang, and Liu (2015).
We provide a more detailed linguistic and methodological background of cross-lingual parsing. Additional extensions
primarily include experiments and analysis of target language adaptation with minimal supervision. Our system is
made publicly available at: https://github.com/jiangfeng1124/acl15-clnndep.

997

fiG UO , C HE , YAROWSKY, WANG & L IU

punct
root

dobj
nsubj

ROOT

He
PRON

amod

has
VERB

good
ADJ

control
NOUN

.
.

Figure 1: An example labeled dependency tree.

2.1 Dependency Parsing
Given an input sentence x = w0 w1 ...wn where wi is the ith word of x, the goal of dependency
parsing is to build a dependency tree, which can be denoted by d = {(h, m, l)  0  h  n; 0 < m 
n, l  L}, where (h, m, l) indicates a directed arc from the head word wh to its modifier wm with a
dependency label l, and L is the label set (Figure 1).
The mainstream models that have been proposed for dependency parsing can be described as
either graph-based models or transition-based models (McDonald & Nivre, 2007). Graph-based
models (Eisner, 1996; McDonald, Crammer, & Pereira, 2005) view the parsing problem as finding
the highest scoring tree from a directed graph. The score of a dependency tree is typically factored
into scores of some small independent structures. The way of factorization defines the order of
a model and also the complexity in the inference process (McDonald & Pereira, 2006; Carreras,
2007; Koo & Collins, 2010). For instance, first-order models are factored into dependency arcs,
thus also known as arc-factored models. Higher-order models would consider more expressive
substructures such as sibling and grandchild structures. Transition-based models instead aim to
predict a transition sequence from an initial parser state to some terminal states, conditioned on the
parsing history (Yamada & Matsumoto, 2003; Nivre, 2003; Nivre, Hall, & Nilsson, 2004). This
approach has a lot of interest since it is fast (linear time for projective parsing) and can incorporate
rich non-local features (Zhang & Nivre, 2011).
It has been considered in the past that simple transition-based parsing using greedy decoding
and local training is not as accurate as graph-based parsers that are globally trained and use exact
inference algorithms. However, Chen and Manning (2014) showed that the greedy transition-based
parsers can be significantly improved with a well-designed neural network architecture. This approach can be considered as a new paradigm of parsing, in that it is based on pure distributed
feature representations. More recently, this architecture has been improved in different ways. For
example, Weiss, Alberti, Collins, and Petrov (2015) combined the neural network with structured
perceptron, and use beam-search for decoding, achieving the new state-of-the-art performance. Dyer, Ballesteros, Ling, Matthews, and Smith (2015) instead explored novel techniques for learning
better representations of parser states by utilizing long short-term memory networks (LSTM). Other
work also includes that of Zhou, Zhang, Huang, and Chen (2015) who applied structured learning
with beam-search decoding over the neural network model. In this study, we choose the original
Chen & Mannings architecture, without losing generality, to build our basic dependency parsing
models for cross-lingual transfer.
998

fiR EPRESENTATION L EARNING FOR C ROSS -L INGUAL T RANSFER PARSING

2.2 Distributed Representations for NLP
Recent years have seen numerous attempts of learning distributed representations for different natural language objects, from morphemes, words and phrases, to sentences and documents. Using
distributed representations, these symbolic units are embedded into a dense, continuous and lowdimensional vector space, thus it is often referred to as embeddings.2
Distributed representation is attractive in NLP for several reasons. First, it provides a straightforward way of measuring the similarities between natural language objects. Through distributed
representations, we can easily tell which two words/phrases/documents are similar in semantic or
even other aspects by simply measuring the cosine distance of vectors.
Second, it can be learned from large-scale unannotated data in general, and thus can be highly beneficial for various downstream applications as a source to alleviate data sparsity. The most
straightforward way of applying distributed representations to NLP tasks is to fed the distributed
feature representations into existing supervised NLP systems as augmented features, in a semisupervised fashion (Turian, Ratinov, & Bengio, 2010). Despite the simplicity and effectiveness, it
has been shown that the potential of distributed representations cannot be fully exploited in the generalized linear models which are adopted in most of the traditional NLP systems (Wang & Manning,
2013). One remedy is to discretize the distributed feature representations, that is to convert the continuous, dense and low-dimensional vectors into traditional discrete, sparse and high-dimensional
space, as studied by Guo, Che, Wang, and Liu (2014). However, we believe that a non-linear system
(e.g., neural network) is a more powerful and promising solution. Some decent progress has already
been made in this paradigm of NLP on various tasks, such as neural sequence labeling (Collobert
et al., 2011), dependency parsing (Chen & Manning, 2014), sentence classification (Kim, 2014) and
machine translation (Sutskever, Vinyals, & Le, 2014).
Third, it provides such a kind of representation that can be shared across languages, tasks and
even diverse modalities of data resources. This property has motivated lines of research on multilingual representation learning (Klementiev et al., 2012; Chandar A P et al., 2014; Hermann &
Blunsom, 2014), multi-task learning (Collobert & Weston, 2008) and multi-modal learning (Srivastava & Salakhutdinov, 2012). This is also the primary motivation of this work that facilitates
cross-lingual transfer parsing via multilingual distributed representation learning of words.

3. Cross-Lingual Dependency Parsing
In this section, we first describe the primary transition-based dependency parsing model utilizing
neural networks, and then details for cross-lingual transfer.
3.1 A Neural Network Architecture for Transition-Based Dependency Parsing
In this section, we first briefly describe transition-based dependency parsing and the arc-standard
parsing algorithm. Then we revisit the neural network architecture for transition-based dependency
parsing proposed by Chen and Manning (2014).
As discussed in Section 2.1, transition-based parsing generates a dependency tree by predicting a transition sequence from an initial parser state to the terminal state. Several transition-based
parsing algorithms have been presented in the literature, such as the arc-standard and arc-eager algorithms for projective parsing (Nivre, 2003, 2004), the list-based algorithm (Nivre, 2008) and the
2. In this paper, these two terminologies are used interchangeably.

999

fiG UO , C HE , YAROWSKY, WANG & L IU

swap-based algorithm (Nivre, 2009) for non-projective parsing. Different algorithms have different
transition actions. Take the arc-standard algorithm for example, each parsing state (typically known
as configuration) can be represented as a tuple consisting of a stack S, a buffer B, and a partially derived forest (i.e., a set of dependency arcs) A. Given an input word sequence x = w1 w2 , ..., wn , the
initial configuration can be represented as: [w0 ]S , [w1 w2 , ..., wn ]B , , and the terminal configuration is [w0 ]S , []B , A, where w0 is a pseudo word indicating the root of the whole dependency
tree. Denoting Si (i = 0, 1, ...) as the ith element in the stack, and Bi (i = 0, 1, ...) as the ith element in the buffer,3 the arc-standard system defines three types of transition actions: L EFT-A RC(r),
R IGHT-A RC(r), and S HIFT, r is the dependency relation.
r

 L EFT-A RC(r): extend A with a new arc (S1 
 S0 ) (S0 the head and S1 the modifier) and
remove S1 from the stack.
r

 R IGHT-A RC(r): extend A with a new arc (S1 
 S0 ) (S1 the head and S0 the modifier) and
pop S0 from the stack.
 S HIFT: move B0 from the buffer to the stack. Precondition is that B is not empty.
The typical approach for greedy arc-standard parsing is to build a multi-class classifier (e.g.,
support vector machines, maximum entropy models) of predicting the transition action given a feature vector extracted from a specific configuration. While conventional feature engineering suffers
from the problem of sparsity, incompleteness and expensive feature computation (Chen & Manning,
2014), the neural network model provides an effective solution.
The architecture of the neural network based dependency parsing model is illustrated in Figure 2. Unlike the high-dimensional, sparse and discrete features used by traditional parsing models,
in the neural network model, we apply distributed feature representations. Primarily, three types of
information are extracted from a configuration in Chen & Mannings model: word features, POS
features and relation features respectively. In this study, we add non-local features including distance features indicating the distance between two items, and the valency features indicating the
number of children for a given item (Zhang & Nivre, 2011). Both distance and valency features
are discretized into buckets. All of these features are then projected to an embedding layer via corresponding lookup tables (i.e., embedding matrices), which will be estimated through the training
process. The complete feature templates used in our system are shown in Table 1.
Then, feature compositions are performed at the hidden layer via the cube activation function:
h = g(x) = (W1  [xw , xt , xr , xd , xv ] + b1 )3
where W1 is the weight matrix from the input layer to the hidden layer, and b1 is the bias vector.
Feature compositions are important not only in dependency parsing but in NLP in general.
Researchers used to do cost-intensive manual feature engineering to design a large set of feature
templates. However, this approach cannot cover all potentially useful features. Lei, Xin, Zhang,
Barzilay, and Jaakkola (2014) showed that a full feature representation can be derived from the
Kronecker product of multiple views of features, which results in a tensor model. By representing
the tensor in a low-rank form using C ANDECOMP /PARAFAC (CP) tensor decomposition (Kolda &
Bader, 2009), the number of parameters can be effectively reduced, and thus is suitable for tasks
with limited training data (Cao & Khudanpur, 2014).
3. S0 /B0 is the top/head element of the stack/buffer.

1000

fiR EPRESENTATION L EARNING FOR C ROSS -L INGUAL T RANSFER PARSING

Softmax Layer:
 = (  )



Hidden Layer:
 =   = (  +  )3



Transition Actions


Hidden Representation


Input Layer:

 = [  ,   ,   ,   ,  , ]

Feature Extraction





Words

Clusters

Lexical features
ROOT

Parsing Configurations

Stack
has_VERB

Lookup Tables
1


POS tags



,

Relations

Distance,
Valency

Non-lexical features
good_ADJ

Buffer
Control_NOUN

._.

nsubj
He_PRON

Figure 2: Neural network model for dependency parsing. The Cluster features are introduced in
Section 5.2 and 5.3.
Type

Feature Templates
w
, i = 0, 1, 2
ESwi , EB
i

Word

w
w
w
w
Elc1(S
, Erc1(S
, Elc2(S
, Erc2(S
, i = 0, 1
i)
i)
i)
i)
w
w
Elc1(lc1(S
, Erc1(rc1(S
, i = 0, 1
i ))
i ))
t
, i = 0, 1, 2
ESt i , EB
i

POS

t
t
t
t
Elc1(S
, Erc1(S
, Elc2(S
, Erc2(S
, i = 0, 1
i)
i)
i)
i)
t
t
Elc1(lc1(S
, Erc1(rc1(S
, i = 0, 1
i ))
i ))

Relation

r
r
r
r
Elc1(S
, Erc1(S
, Elc2(S
, Erc2(S
, i = 0, 1
i)
i)
i)
i)
r
r
Elc1(lc1(S
, Erc1(rc1(S
, i = 0, 1
i ))
i ))

Distance

d
d
ES
, ES
0 ,S1 
0 ,B0 

Valency

ESlv0 , ESlv1 , ESrv1

Table 1: Feature templates of the neural network model for transition-based dependency parsing.
{w,c,t,r,d,lv,rv}
Ep
indicates various feature embeddings of the element at position p. lc1
(rc1) is the first child to the left (right) and lc2 (rc2) is the second child to the left (right).

indicates the lexical features,  indicates the non-lexical features.
We suggest that the cube activation function g(x) = x3 can be viewed as a special case of the
low-rank tensor. For verification, g(x) can be expanded as:
g(w1 x1 + ... + wm xm + b) =
 (wi wj wk )xi xj xk +  b(wi wj )xi xj + ...

i,j,k

i,j

1001

fiG UO , C HE , YAROWSKY, WANG & L IU

If we treat the bias term as b  x0 where x0 = 1, then the weight corresponding to each feature
combination xi xj xk can be wrote as wi wj wk , which is exactly the same as a rank-1 component tensor in the low-rank form using CP tensor decomposition. Consequently, the cube activation function
implicitly derives full feature combinations. In fact, we can add as many features as possible to the
input layer to improve the parsing accuracy. We will show in Section 5.2 that the Brown-cluster
features can be readily incorporated into our model.
The composed features are then propagated to the output layer, generating a probabilistic distribution of the output labels (i.e., transition actions) via the softmax activation function: y =
sof tmax(W2  h). We use the following objective function to train the model:
J () =

1 N
 2
 CrossEnt(di , yi ) + 
N i=0
2

where CrossEnt(p, q) is the cross-entropy between two distributions p and q:
CrossEnt(p, q) =  pk ln qk
k

All parameters in  are trained using back-propagation. In this model,  typically consists of all
the embedding matrices and weights in the network. However, in some cases,  may exclude the
word embedding matrix E w , which indicates that the word embeddings are constrained to be fixed
(i.e., without updating) while training.
3.2 Cross-Lingual Transfer
The idea of cross-lingual transfer using the parser we examined above is straightforward. In contrast
to traditional approaches that have to discard rich lexical features (delexicalizing) when transferring
models from one language to another, our model can be transferred using the full model trained on
the source language side (i.e., English).
Since the non-lexical feature (POS, relation, distance, valency) embeddings are directly transferable between languages, the key component of this framework is the cross-lingual learning of
lexical feature embeddings (i.e., word embeddings). Once the cross-lingual word embeddings are
induced, we first learn a dependency parser at the source language side. After that, the parser will
be directly used for parsing target language data.
3.2.1 U NIVERSAL D EPENDENCIES
As discussed previously, cross-lingual model transfer assumes universal grammatical structures that
can be identified in multiple languages. Therefore, when evaluated on the test set of target language
with either unlabeled attachment score (UAS) or labeled attachment score (LAS), the performance
of transfer parsing rely heavily on the multilingual consistency of annotation schemes. Generally
syntactic annotation schemes differ in the head-finding rules (e.g., the choice of lexical versus functional head) and the dependency relation labels (i.e., the syntactic tagset). It is a challenging task to
construct multilingual treebanks with such consistent annotations. In the initial cross-lingual parsing studies, the CoNLL shared task datasets (Buchholz & Marsi, 2006) are broadly used. However,
inconsistencies occur both in the head-finding rules and the syntactic tagset across languages, which
made it difficult to evaluate the cross-lingual parsers.
1002

fiR EPRESENTATION L EARNING FOR C ROSS -L INGUAL T RANSFER PARSING

In order to overcome these difficulties, a new collection of multilingual treebanks with homogeneous syntactic dependency annotation has been presented recently, namely the Universal Dependency Treebanks (UDT) (McDonald et al., 2013). The universal annotation scheme was created
by harmonizing available treebanks in slightly different variants of the Stanford typed dependencies (De Marneffe et al., 2006), along with the universal Part-of-Speech tags (Petrov, Das, & McDonald, 2012). This dataset greatly facilitates research on multilingual syntactic analysis, and also
makes it possible to use LAS for evaluation. In fact, UDT has already been used as a standard
dataset for benchmarking research on cross-lingual transfer parsing (Ma & Xia, 2014; Tiedemann,
2014; Zhang & Barzilay, 2015; Duong, Cohn, Bird, & Cook, 2015a, 2015b; Rasooli & Collins,
2015). Other efforts towards universal dependencies include the most recent Universal Dependencies project (UD) 4 and HamleDT (Zeman et al., 2014). In this paper, we conduct experiments on
the UDT (v2.0) 5 dataset without losing generality.
3.2.2 P ROJECTIVE VS . N ON - PROJECTIVE PARSING
Non-projectivity is a common phenomenon in multilingual dependency parsing. The term nonprojectivity indicates that a dependency tree has crossing-arcs, which often appear in morphologically rich languages. Various algorithms have been proposed for both graph-based and transitionbased parsing algorithms to produce non-projective trees. For example, the arc-standard algorithm
(Section 3.1) can be readily extended by adding a swap action to handle the non-projectivity, which
gives an expected linear and worst-case O(n2 ) complexity (Nivre, 2009). Other strategies include
the list-based algorithm (Nivre, 2008) which is adapted from the Covington algorithm (Covington, 2001), and a further combination of the list-based and the swap-based algorithm (Choi &
McCallum, 2013). Unfortunately, there has been no systematically comparison for these different
algorithms in the literature so far.
In this study, however, we focus only on projective parsing because there is no non-projective
trees in our source language (English) training data. Consequently, non-projectivities in target languages will not be handled at this moment.6

4. Cross-Lingual Word Representation Learning
Prior to introducing our approaches for cross-lingual word representation learning, we briefly review
the basic model for learning monolingual word embeddings, which constitutes a subprocedure of
the cross-lingual approaches.
4.1 Continuous Bag-of-Words Model
In recent years, various approaches have been studied for learning word embeddings from largescale plain texts. All approaches are generally derived from the so-called distributional hypothesis (Firth, 1957): You shall know a word by the company it keeps. In this study, we consider
the Continuous Bag-of-Words (CBOW) model (Mikolov, Chen, Corrado, & Dean, 2013) as imple4. https://universaldependencies.github.io/docs/
5. https://github.com/ryanmcd/uni-dep-tb
6. Note that for the target languages we address in this paper, non-projectivity is not pervasive. Specifically, the proportion of projective trees presented in their training corpus is respectively 91% for DE, 94% for ES, and 88% for
FR.

1003

fiG UO , C HE , YAROWSKY, WANG & L IU

mented in the open-source toolkit word2vec.7 The basic principle of the CBOW model is to predict
each individual word in a sequence given the bag of its context words within a fixed window size
as input, using a log-linear classifier. This model avoids the non-linear transformation in hidden
layers, and hence can be trained with high efficiency.
With large window size, grouped words using the resulting word embeddings are more topically similar; whereas with small window size, the grouped words will be more syntactically similar (Bansal, Gimpel, & Livescu, 2014). So we set the window size to 1 in our parsing task.
Next, we introduce our approach for inducing bilingual word embeddings. In general, we expect
our bilingual word embeddings to preserve translational equivalences. For example, cooking (English) should be close to its translation: kochen (German) in the embedding space.
4.2 Robust Alignment-Based Projection
Our first method for inducing cross-lingual word embeddings has two stages. First, we learn word
embeddings from a source language (S) corpora as in the monolingual case, and then project the
monolingual word embeddings to a target language (T), based on word alignments.
Given a sentence-aligned parallel corpus D, we first conduct unsupervised bidirectional word
alignment, and then collect an alignment dictionary. Specifically, in each word-aligned sentence pair
of D, we keep all alignments with conditional alignment probability exceeding a threshold  = 0.95
and discard the others. Specifically, let AT S = {(wiT , wjS , ci,j ), i = 1, 2, ..., NT ; j = 1, 2, ..., NS } be
the alignment dictionary, where ci,j is the number of times when the ith target word wiT is aligned
to the j th source word wjS . NS and NT are vocabulary sizes. We use the shorthand (i, j)  AT S
to denote a word pair in AT S . The projection can be formalized as the weighted average of the
embeddings of translation words:
ci,j
v(wiT ) =
 v(wjS )
(1)

c
i,
(i,j)AT S
where ci, = j ci,j , v(w) is the embedding of w.
Obviously, the simple projection method has one drawback: it only assigns word embeddings
for those target language words that occur in the word aligned data, which is typically smaller than
the monolingual datasets. Therefore, in order to improve the robustness of projection, we utilize
a morphology-inspired mechanism, to propagate embeddings from in-vocabulary words to out-ofT
vocabulary (OOV) words. Specifically, for each OOV word woov
, we extract a list of candidate
words that is similar to it in terms of edit distance (Levenshtein distance), and then set the averaged
T
vector as the embedding of woov
. More formally,
T
v(woov
) = Avg (v(w ))
w C

T
where C = {ww  EditDist(woov
, w)   }

(2)

To reduce noise, we choose a small edit distance threshold  = 1.
The process of robust projection can be viewed as a two-stage graph-propagation algorithm, as
illustrated in Figure 3 (left panel). Embeddings are first propagated from source language words
to target language words that appear in the bilingual lexicons. Next, monolingual propagation is
performed to obtain OOV word embeddings in the target language, using the edit distance metric.
7. http://code.google.com/p/word2vec/

1004

fiR EPRESENTATION L EARNING FOR C ROSS -L INGUAL T RANSFER PARSING

1

Source Language



1
Bilingual Lexicon
(weighted)

Target Language

Parallel data
Wiktionary
PanLex




2



1





2













2

2



CCA
1



In-Vocabulary words
Out-of-Vocabulary words





Figure 3: Illustration of robust projection (left) and CCA (right) for inducing cross-lingual word
embeddings.

4.3 Canonical Correlation Analysis
The second approach we consider is similar to that of Faruqui and Dyer (2014), which uses CCA to
improve monolingual word embeddings with multilingual correlation. CCA is a way of measuring
the linear relationship between multidimensional variables. For two multidimensional variables,
CCA aims to find two projection matrices to map the original variables to a new basis (lowerdimensional), such that the correlation between the two variables is maximized.
We refer the readers to the work of Hardoon, Szedmak, and Shawe-Taylor (2004) for theoretical
foundations and algorithm specifics of CCA. Here lets treat CCA as a black box, and see how CCA
can be applied for inducing bilingual word embeddings. Suppose there are already two pre-trained
monolingual word embeddings (e.g., English and German):   Rn1 d1 and   Rn2 d2 . At the
first step, we extract a one-to-one alignment dictionary D     from the alignment dictionary
AST .8 Here,   , indicating that every word in  is translated to one word in   , and vice
versa.
The process is illustrated in Figure 3 (right panel). Denoting the dimension of resulting word
embeddings by d  min(d1 , d2 ). First, we derive two projection matrices V  Rd1 d , W  Rd2 d
respectively for  and  using CCA:
V, W = CCA( ,  )

(3)

Then, V and W are used to project the entire vocabulary  and :
 = V,

 = W

(4)

where   Rn1 d and   Rn2 d are the resulting word embeddings for our cross-lingual task.
8. AT S is also worth trying, but we observed slight performance degradation in our experimental setting.

1005

fiG UO , C HE , YAROWSKY, WANG & L IU

4.4 Pros and Cons
Contrary to the robust projection approach, CCA assigns embeddings for every word in the monolingual vocabulary. However, one potential limitation is that CCA assumes linear transformation of
word embeddings, which is difficult to satisfy. At the mean time, when training the source language
parser using the CCA cross-lingual word embeddings, we have to constrained E w to be fixed, as
mentioned in Section 3.1, otherwise, the translational equivalence will be broken. The robust projection approach, however, doesnt have such limitation. Further discussion with experiments will
be presented in Section 5.3.2.
Note that both approaches can be generalized to lower-resource languages where parallel bitexts
are not available. In that way, the dictionary A can be readily obtained either using bilingual lexicon
induction approaches (Mann & Yarowsky, 2001; Koehn & Knight, 2002; Haghighi, Liang, BergKirkpatrick, & Klein, 2008), or from online-resources like Wiktionary9 and Panlex.10

5. Experiments
This section describes the experiments. We first describe the data and settings used in the experiments, and then the results.
5.1 Data and Settings
For the pre-training of word embeddings, we use the WMT-2011 monolingual news corpora for
English, German and Spanish.11 For French, we combined the WMT-2011 and WMT-2012 monolingual news corpora.12 We got the word alignment counts using the fast-align toolkit in cdec (Dyer
et al., 2010) from the parallel news commentary corpora (WMT 2006-10) combined with the Europarl corpus for English{German, Spanish, French}.13
For the training of the neural network dependency parser, we set the number of hidden units to
400. The dimension of embeddings for different features are shown in Table 2.

Dim.

Word
50

POS
50

Label
50

Distance
5

Valency
5

Cluster
8

Table 2: Dimensions of various types of feature embeddings.
Mini-batch adaptive stochastic gradient descent (AdaGrad) (Duchi, Hazan, & Singer, 2011) is
used for optimization. For the CCA approach, we use the implementation of Faruqui and Dyer
(2014).
We employ the universal dependency treebanks (UDT v2.0) for a reliable evaluation of our
approach for cross-lingual dependency parsing. The universal multilingual treebanks are annotated
using the universal POS tagset (Petrov et al., 2012) which contains 12 POS tags, as well as the
universal dependencies which defines 40 dependency relations. We follow the standard split of the
treebanks for all languages.
9.
10.
11.
12.
13.

https://www.wiktionary.org/
http://panlex.org/
http://www.statmt.org/wmt11/
http://www.statmt.org/wmt12/
http://www.statmt.org/europarl/

1006

fiR EPRESENTATION L EARNING FOR C ROSS -L INGUAL T RANSFER PARSING

5.2 Baseline Systems
We compare our approach with the following systems.
For the first baseline, we evaluate the delexicalized transfer of our neural network-based parser
[D ELEX], in which we only use the non-lexical features (Figure 2). Here we investigate the effect
of the non-local features (distance, valency). The delexicalized systems which do not include these
non-local features is referred to as [D ELEX (basic)].
We also compare our approach with the delexicalized parser presented by McDonald et al.
(2013) [M C D13], which used a perceptron-based transition-based parser with a beam of size 8,
along with richer non-local features (Zhang & Nivre, 2011). Our re-implementation of this approach
under the framework of Zpar (Zhang & Clark, 2011) is referred to as [M C D13 ].
Furthermore, we consider a strong baseline system as proposed by Tackstrom et al. (2012),
which utilized cross-lingual word cluster features to enhance the perceptron-based delexicalized
parser [M C D13 +Cluster]. We use the same alignment dictionary as described in Section 4.2 to
induce the cross-lingual word clusters. We re-implement the P ROJECTED clustering approach described in the work of Tackstrom et al., which assigns a target word to the cluster with which it is
most often aligned:
c(wiT ) = arg max  ci,j  1[c(wjS ) = k]
k

(i,j)AT S

Obviously, this method also has the drawback that words that do not occur in the alignment dictionary (OOV) cannot be assigned a cluster. Therefore, we use the same strategy as described in
Section 4.2 to find the most likely clusters for the OOV words. Instead of computing the average of
embeddings, we solve an argmax problem:
T
) = arg max 
c(woov
k

1[c(w ) = k]

w C

(5)

T
, w)   }
where C = {wEditDist(woov

 is set to 1 constantly. Instead of the clustering model of Uszkoreit and Brants (2008), we use
Brown clustering (1992) to induce hierarchical word clusters, where each word is represented as a
bit-string. We use the same word cluster feature templates from Tackstrom et al. (2012), and set the
number of Brown clusters to 256.
5.3 Experimental Results
All of the parsing models are trained using the development data from English for early-stopping.
Table 3 lists the results of the cross-lingual transfer experiments for dependency parsing. Table 4
further summarizes each of the experimental gains detailed in Table 3.
We first examine the benefit brought by the non-local distance and valency features. As observed
in the comparison of D ELEX (basic) and D ELEX, marginal improvements are obtained for DE and
FR, and more significant improvements for ES. Therefore, we adopted these features in all of the
following experiments.
Our delexicalized system obtains slightly lower performance than those reported by McDonald
et al. (2013) (M C D13), because we used greedy decoding and local training. Our re-implementation
of McDonald et al.s work attains comparable performance with M C D13. For all languages we consider in this study, by using cross-lingual word embeddings either from alignment-based projection
or CCA, we obtain statistically significant improvements against the delexicalized system, both in
1007

fiG UO , C HE , YAROWSKY, WANG & L IU

D ELEX (basic)
D ELEX
P ROJ
P ROJ+Cluster
CCA
CCA+Cluster

Unlabeled Attachment Score (UAS)
EN
DE
ES
FR
AVG
83.63 56.85 67.28 68.70 64.28
83.67 57.01 68.05 68.85 64.64
91.96 60.07 71.42 71.36 67.62
92.33 60.35 71.90 72.93 68.39
90.62 59.42 68.87 69.58 65.96
92.03 60.66 71.33 70.87 67.62

Labeled Attachment Score (LAS)
EN
DE
ES
FR
AVG
79.37 47.06 56.43 57.73 53.74
79.42 47.12 56.99 57.78 53.96
90.48 49.94 61.76 61.55 57.75
90.91 51.54 62.28 63.12 58.98
88.88 49.32 59.65 59.50 56.16
90.49 51.29 61.69 61.50 58.16

M C D13

83.33

58.50

68.07

70.14

65.57

78.54

48.11

56.86

58.20

54.39

M C D13
M C D13 +Cluster

84.44
90.21

57.30
60.55

68.15
70.43

69.91
72.01

65.12
67.66

80.30
88.28

47.34
50.20

57.12
60.96

58.80
61.96

54.42
57.71

Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multilingual treebanks. Results measured by unlabeled attachment score (UAS) and
labeled attachment score (LAS). D ELEX (basic) is the delexicalized model without nonlocal features (distance, valency).  denotes our re-implementation of M C D13. Since the
model varies for different target languages in the CCA-based approach,  indicates the
averaged UAS/LAS.

Experimental Contribution
P ROJ
vs. D ELEX
CCA
vs. D ELEX
P ROJ
vs. M C D13
CCA
vs. M C D13
P ROJ+Cluster
vs. P ROJ
CCA+Cluster
vs. CCA
M C D13 +Cluster vs. M C D13
P ROJ+Cluster
vs. D ELEX
CCA+Cluster
vs. D ELEX
P ROJ+Cluster
vs. M C D13
CCA+Cluster
vs. M C D13
P ROJ+Cluster
vs. M C D13 +Cluster
CCA+Cluster
vs. M C D13 +Cluster

DE/ES/FR Avg. (Relative)
+3.79 (8.2%)
+2.19 (4.8%)
+3.33 (7.3%)
+1.74 (3.8%)
+1.23 (2.9%)
+2.00 (4.6%)
+3.29 (7.2%)
+5.02 (10.9%)
+4.20 (9.1%)
+4.46 (9.8%)
+3.74 (8.2%)
+1.27 (3.0%)
+0.45 (1.1%)

Table 4: Summary of each of the experimental gains detailed in Table 3, in both absolute LAS gain
and relative error reduction. All gains are statistically significant using MaltEval (Nilsson
& Nivre, 2008) at p < 0.01.

UAS and LAS. Interestingly, we notice that P ROJ consistently outperforms CCA by a significant
margin, and is comparable to M C D13 +Cluster. Further analysis to this observation will be conducted in Section 5.3.1 and 5.3.2.
1008

fiR EPRESENTATION L EARNING FOR C ROSS -L INGUAL T RANSFER PARSING

Type
Cluster

Feature Templates
c
ESc i , EB
, i = 0, 1, 2
i
c
c
c
c
Elc1(Si ) , Erc1(S
, Elc2(S
, Erc2(S
, i = 0, 1
i)
i)
i)

c
c
Elc1(lc1(S
, Erc1(rc1(S
, i = 0, 1
i ))
i ))

Table 5: Word cluster feature templates.

Our framework is flexible for incorporating richer features simply by embedding them into
continuous vectors. Thus we further embed the cross-lingual word cluster features into our model,
together with the proposed cross-lingual word embeddings. The cluster feature templates are shown
in Table 5, which is similar to the POS tag feature templates. As shown in Table 3, significant
additive improvements are obtained for both P ROJ and CCA by embedding the cluster features.
Compared with our delexicalized system, the relative error is reduced by up to 13.1% in UAS, and
up to 12.6% in LAS. The combined system further outperforms M C D13 +Cluster significantly .
5.3.1 E FFECT OF ROBUST P ROJECTION
Since in both P ROJ and the induction of cross-lingual word clusters, we use edit distance measure
for OOV words, we would like to see how this affects the performance of parsing.
Intuitively, higher coverage of projected words in the test dataset should promote the parsing
performance more. To verify this, we further conduct experiments under both settings using the
P ROJ+Cluster model. For robust projection, we examine the effect of edit distances ranging from
1 to 3. Results are shown in Table 6. Improvements are observed for all languages when using
robust projection with edit distance measure, especially for FR, where the highest coverage gain is
obtained by robust projection. We also observe slightly improvements for DE and ES when using
an edit distance of 2. But performance starts to degrade when it gets larger. This is reasonable, since
larger edit distance increases the word coverage, but also introduces more noise.

Simple

DE

ES

FR

coverage
UAS
LAS
coverage
UAS
LAS
coverage
UAS
LAS

91.37
59.74
50.84
94.51
70.97
61.34
90.83
71.17
61.72

 =1
94.70
60.35
51.54
96.67
71.90
62.28
97.60
72.93
63.12

Robust
 =2
96.50
60.53
51.70
97.75
72.00
62.34
98.33
72.79
63.02

Table 6: Effect of robust projection.

1009

 =3
97.47
60.53
51.69
98.47
71.93
62.27
98.58
72.70
62.94

fiG UO , C HE , YAROWSKY, WANG & L IU

5.3.2 E FFECT OF F INE -T UNING W ORD E MBEDDINGS
Another reason for the effectiveness of P ROJ over CCA lies in the fine-tuning of word embeddings
while training the parser.
CCA can be viewed as a joint method for inducing cross-lingual word embeddings. When
training the source language dependency parser with cross-lingual word embeddings derived from
CCA, the EN word embeddings should be fixed. Otherwise, the translational equivalence will be
broken. However, for P ROJ, there is no such limitation. Word embeddings can be updated as other
non-lexical feature embeddings, in order to obtain a more accurate dependency parser. We refer to
this procedure as a fine-tuning process to the word embeddings. To verify the benefits of fine-tuning,
we conduct experiments to see relative loss if word embeddings are fixed while training. Results
are shown in Table 7, which indicates that fine-tuning indeed offers considerable help.

DE
ES
FR

UAS
LAS
UAS
LAS
UAS
LAS

Fixed
59.74
49.44
70.10
61.31
70.65
60.69

Fine-tuning
60.07
49.94
71.42
61.76
71.36
61.50


+0.33
+0.50
+1.32
+0.45
+0.71
+0.81

Table 7: Effect of fine-tuning word embeddings.

5.4 Compare with Existing Bilingual Word Embeddings
In this section, we compare our bilingual embeddings with several previous approaches in the context of dependency parsing. To the best of our knowledge, this is the first work on evaluation of
bilingual word embeddings in syntactic tasks.
The approaches we consider include the multi-task learning approach (Klementiev et al., 2012)
[MTL], the bilingual auto-encoder approach (Chandar A P et al., 2014) [B IAE], the bilingual compositional vector model (Hermann & Blunsom, 2014) [B ICVM], and the bilingual bag-of-words
approach (Gouws et al., 2015) [B ILBOWA].
For MTL and B IAE, we adopt their released word embeddings directly due to the inefficiency of
training.14 For B ICVM and B ILBOWA, we re-run their systems on the same dataset as our previous
experiments.15 Results are summarized in Table 8.
CCA and P ROJ consistently outperforms all other approaches in all languages, and P ROJ performs the best. The inferior performance of MTL and B IAE is partly due to the low word coverage.
For example, they cover only 31% of words in the universal DE test treebank, whereas the CCA
and P ROJ covers over 70%. Moreover, B IAE, B ICVM and B ILBOWA introduce sentence-level translational equivalence as objectives or regularizers for learning bilingual word embeddings. These
approaches are advantageous in that they dont assume/require word alignment. However, word-toword translational equivalence cannot be well preserved in this way.
14. The MTL embeddings are normalized before training.
15. B ICVM only uses the bilingual parallel dataset.

1010

fiR EPRESENTATION L EARNING FOR C ROSS -L INGUAL T RANSFER PARSING

MTL (Klementiev et al., 2012)
B IAE (Chandar A P et al., 2014)
B ICVM (Hermann & Blunsom, 2014)
B ILBOWA (Gouws et al., 2015)
CCA
P ROJ

DE
UAS
LAS
56.93 46.22
53.74 43.68
56.30 46.99
54.51 44.95
59.42 49.32
60.07 49.94

ES
UAS
67.71
58.81
67.78
67.23
68.87
71.42

LAS
58.43
46.66
58.08
56.16
59.65
61.76

FR
UAS
LAS
67.51 57.27
60.10 49.47
69.13 58.13
64.82 52.73
69.58 59.50
71.36 61.55

Table 8: Comparison with existing bilingual word embeddings.  For MTL and B IAE, we use their
released bilingual word embeddings.

Target Word (ES)

china
(china)

problemas
(problems)

septiembre
(september)

P ROJ
india
russia
taiwan
chinese
problem
difficulties
troubles
issues
october
august
january
december

CCA
russia
indonesia
beijing
chinese
problems
woes
troubles
dilemmas
december
july
october
june

Neighboring Words (EN)
MTL
B IAE
china
korea
independent india
sumitomo
chinese
malaysian
brazil
events
problem
sanctions
greatly
conditions
highlighted
laws
scale
december
month
february
april
july
scheduled
march
november

B ICVM
chinese
chinois
sino
33.55
problematic
problematical
difficulties
troubles
11th
11.00
11
eleventh

B ILBOWA
helsinki
bulgarians
constituting
market
deficiencies
situations
omissions
attentively
a.m
p.m
twelve
1998-1999

Table 9: Target words in Spanish and their 4 most similar words in English, as induced by various
approaches.

To verify this assumption, we taking EN/ES as a case study. We manually inspect the 4 most
similar words (by cosine similarity) in English to a given set of words in Spanish (Table 9). We
can observe both semantic and syntactic shifting in the k-nearest neighbors prediction of B IAE,
B ICVM and B ILBOWA, whereas P ROJ and CCA give more translational equivalent predictions. For
example, B ICVM yields adjective like problematical for the target noun problemas; B ILBOWA yields
semantic-related word market for china. In general, P ROJ is the most robust approach, behaving
consistently well for most of the sampled words.
It is worth noting that we dont assume/require bilingual parallel data in CCA and P ROJ. What
we need in practice is a bilingual lexicon for each paired languages. This is especially important for
generalizing our approaches to lower-resource languages, where parallel texts are not available.
1011

fiG UO , C HE , YAROWSKY, WANG & L IU

6. Target-Language Adaptation with Minimal Supervision
It is important for us to distinguish what linguistic structures can be learned via cross-lingual transfer
versus what can only be learned on the basis of monolingual information in the language to be
parsed. Intuitively, cross-lingual approaches can only learn the common dependency structures
shared between the source and target language. However, for many languages, there are some
specialized (language-specific) syntactic characteristics that are can only be learned from data in
the target language.
Take the adjective-noun order for example, in Spanish and French, adjectives often appears
after the nouns, thus forming a right-directed arc labeled by amod, whereas in English, the amod
(adjectival modifier) arcs are mostly left-directed, as illustrated in Figure 4. Another example is the
subject-verb-object order. In German, verbs often appear at the end of a sentence in V2 position,
which causes much more left-directed dobj (direct object) arcs than in English (Figure 5). These
differences can be clearly observed from the universal treebanks. Table 10 shows the significant
distribution divergence between left-directed and right-directed arcs of dobj and amod relations in
treebanks from different languages.
Relation: dobj; Language: EN vs. DE
dobj
dobj
ratio
EN
38,395
764
50.3 : 1
DE
4,277
3,457
1.2 : 1
Relation: amod; Language: EN vs. ES, FR
amod amod
ratio
EN
1,667
57,864
1 : 34.7
ES
14,876
5,205
2.9 : 1
FR
12,919
4,910
2.6 : 1

Table 10: Distribution divergences of left-directed and right-directed arcs with dobj relation in EN
and DE (top), and amod relation in EN and ES/FR (bottom).

amod

amod

NOUN

ADJ

NOUN

ADJ

Spanish:

Consejo

Superior

conflictos

sociales

ADJ

NOUN

ADJ

NOUN

English:

Superior

Council

social

conflicts

amod

amod

Figure 4: Reverse direction of the amod relation in Spanish and English. French also has the adjectives following the nouns.

1012

fiR EPRESENTATION L EARNING FOR C ROSS -L INGUAL T RANSFER PARSING

root
advmod

det

dobj

ADV

DET

NOUN

VERB

DE:

endlich

den

richtigen

gefunden

EN:

finally

found

the

right man

ADV

VERB

DET

NOUN

advmod

det
dobj
root

Figure 5: Reverse direction of the dobj relation in German and English.
Therefore, in this section, we investigate how much our cross-lingual transfer model can be improved by annotating a small amount of labeled training data at target language side. Even though
building large-scale treebanks of low-resource languages for supervised learning is costly, annotating dependency structures for a small amount of sentences (e.g., 100) is not that difficult.
We still conduct experiments on the universal dependency treebanks, which provide labeled
training data for multiple languages. For each language we studied (DE, ES, FR), we incrementally
augment the amount of labeled sentences from 100 to 1,000 with a step of 100, to adapt the parameters of the cross-lingual transfer model to the specific target language. Theoretically, since target
language treebanks contain non-projective trees, it would make more sense to apply non-projective
algorithms (e.g., swap-based) for target language adaptation. In this way, however, W2 has to be
re-trained from scratch, which doesnt show good performance in our experiments since the minimally supervised data is very small. Consequently, we still rely on the arc-standard algorithm
for adaption. The process is almost the same as training the source language parser as described
in Section 3, except that the word embedding matrix E w is fixed, while the rest of parameters in 
(E {t,l,d,v,c} , W1 , W2 , b1 ) are optimized using the augmented labeled data from the target language,
taking Equation 3.1 as objective function. No development data is used during this process, thus we
simply perform parameter updating for 2,000 iterations.
In addition, we built another strong baseline system which employs the same augmented labeled
training data for supervised learning. In this system, we utilize both word embeddings and Brown
clusters as features, which are derived separately for each language.
As shown in Figure 6, the results are really promising. The P ROJ+Cluster and CCA+Cluster
systems consistently outperform the delexicalized system and the supervised system by a significant margin. P ROJ+Cluster and CCA+Cluster in general achieve comparable performances, while
CCA+Cluster is slightly better.
It is worthy noting that the performances of P ROJ+Cluster and CCA+Cluster are boosted by
augmenting only 100 sentences. Take DE for example, UAS is increased from 60.35% to 68.91%,
and LAS from 51.54% to 61.54%, which is nearly equal to the effect of using 1,000 sentences for
supervised learning. This observation demonstrates the great potential of our cross-lingual transfer
system for practical usage.
1013

fi

85

85

75

80

G UO , C HE , YAROWSKY, WANG & L IU










80






















UAS







75

UAS

65

UAS






60







75

70




80







70

PROJ+Cluster
CCA+Cluster
Delexicalized
Supervised

0

200

400

600

800

1000

0

200






400

600

800

1000

200

600

800

1000












75

75























60







70

70



LAS

50



65

LAS



65

55

LAS

400

80





PROJ+Cluster
CCA+Cluster
Delexicalized
Supervised

Labeled training data (FR)



65

0

Labeled training data (ES)

80

70

Labeled training data (DE)





65



65

PROJ+Cluster
CCA+Cluster
Delexicalized
Supervised

45

50



70

55






0

200

400

600

800

Labeled training data (DE)

1000



60

PROJ+Cluster
CCA+Cluster
Delexicalized
Supervised

PROJ+Cluster
CCA+Cluster
Delexicalized
Supervised

55

60



55

PROJ+Cluster
CCA+Cluster
Delexicalized
Supervised

40

45



0

200

400

600

800

Labeled training data (ES)

1000

0

200

400

600

800

1000

Labeled training data (FR)

Figure 6: Target-language adaptation by incrementally augmenting labeled training data (sentences) to fine-tune the cross-lingual transfer model. Performances are evaluated using
UAS (top) and LAS (bottom). Note that the points whose x coordinates are 0 represent
our cross-lingual transfer performance, where no labeled training data are used.

Analysis. Our primary hypothesis is that by incorporating data in the target language, our model
can be able to learn the special syntactic patterns that are not consistent with the source language. To
verify this, we further study the influence of target-language adaptation on the two special relations:
dobj (DE) and amod (ES, FR), by measuring their precision and recall changes with the use of
only 100 target language sentences. Results are shown respectively in Table 11 and Table 12. We
observe great improvements in recall for these relations, which indicates that the model indeed gains
the ability of learning target-language-specific dependency structures with the supervision of only
100 sentences.

7. Related Studies
The cross-lingual annotation projection method was pioneered by Yarowsky, Ngai, and Wicentowski (2001) for shallow NLP tasks (POS tagging, NER, etc.), and later applied to dependency
parsing (Hwa et al., 2005; Smith & Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann,
2014). Most work along this line has been dedicated to improving the robustness of syntactic pro1014

fiR EPRESENTATION L EARNING FOR C ROSS -L INGUAL T RANSFER PARSING

Relation: dobj; Language: DE
Precision Recall
PROJ+Cluster
41.45
31.09
+100
41.90
51.40

 0.45  20.31
CCA+Cluster
39.47
31.74
+100
43.59
57.57

 4.12  25.83

Table 11: Effect of minimal supervision (100 sentences) on dobj.
Relation: amod; Language: ES, FR
ES
FR
Precision Recall Precision Recall
PROJ+Cluster
94.97
80.05
92.94
81.70
+100
91.60
92.52
93.61
95.75

 3.37  12.47
 0.67  14.05
CCA+Cluster
93.37
77.31
92.08
72.22
+100
91.85
92.77
92.77
96.41

 1.52  15.46
 0.69  24.19

Table 12: Effect of minimal supervision (100 sentences) on amod.

jection and alleviating the noise and errors introduced by word alignment-based projection. Typical
approaches include soft projection (Li, Zhang, & Chen, 2014), treebank translation (Tiedemann, Agic, & Nivre, 2014), distribution transfer (Ma & Xia, 2014), and the most recently proposed
density-driven projection (Rasooli & Collins, 2015). It is worth mentioning that remarkable results
have been achieved through annotation projection methods (Tiedemann, 2015; Rasooli & Collins,
2015), due in large part to that parsers are trained at the target language side.
For cross-lingual model transfer, learning cross-lingual feature representations has been a promising direction. Typical approaches include cross-lingual word clustering (Tackstrom et al., 2012)
which is employed in this paper as a baseline system, and projection features (Durrett, Pauls, & Klein, 2012). Kozhevnikov and Titov (2014) derived a linear projection that maps target instances to
source-side feature representations, which to some extent is similar to our CCA approach. Xiao and
Guo (2014) learned cross-lingual word embeddings and applied them to MSTParser for linguistic
transfer, which inspired our work. Sgaard et al. (2015) obtained multi-source unified word embeddings via inverted indexing in Wikipedia, and applied them to various NLP tasks. However, their
results didnt show significant improvements in parsing. Nevertheless, the idea of utilizing multisource information for learning cross-lingual word embeddings makes great sense. More recently,
Duong et al. (2015a, 2015b) also utilized the neural network architecture with parameter sharing
between parsers of different languages. However, their approach requires annotated treebanks from
the target language side, which makes it distinct from our transfer parsing framework. In addition
to representation learning, attempts were also made to integrate monolingual linguistic features into the parsing models, such as manually constructed universal dependency parsing rules (Naseem,
1015

fiG UO , C HE , YAROWSKY, WANG & L IU

Chen, Barzilay, & Johnson, 2010) and manually specified typological features (Naseem, Barzilay,
& Globerson, 2012; Zhang & Barzilay, 2015).
Using neural networks for dependency parsing is not a new approach. To the best of our knowledge, Mayberry and Miikkulainen (1999) presented the first work that explored neural networks
for shift-reduce constituent-based parsing. They used one-hot feature representations. Henderson
(2004) used a simple synchrony network to predict parse decisions in a constituency parser, and
was the first to use neural networks in a broad-coverage Penn Treebank parser. Titov and Henderson (2007) applied Incremental Sigmoid Belief Networks to constituent-based parsing. Garg and
Henderson (2011) later extended this work to transition-based dependency parsing using a Temporal Restricted Boltzman Machine. These parsers, however, are much less scalable in practice.
Earlier progress made in using deep learning for parsing includes work by Collobert (2011) and
Socher et al. (2013) for constituent-based parsing, and Stenetorp (2013) who built recursive neural
networks for transition-based dependency parsing.

8. Conclusion
This paper proposes a novel framework based on distributed representations for cross-lingual dependency parsing. Two algorithms are proposed for the induction of cross-lingual word representations,
namely robust projection and CCA, which bridge the lexical feature gap.
Experiments show that by using cross-lingual word embeddings derived from either approach,
the transferred parsing performance can be improved significantly against the delexicalized system.
A notable observation is that our projection method performs significantly better than CCA. Additionally, our framework is flexibly able to incorporate the cross-lingual word cluster features, with
further significant gains in each use. The combined system significantly outperforms the delexicalized systems on all languages, by an average of 10.9% error reduction on LAS, and further
significantly outperforms the models of McDonald et al. (2013) augmented with projected word
cluster features.
Furthermore, we show that the performance of our cross-lingual transfer system on a specific target language can be boosted by minimal supervision from that language, which is of great
significance for practical usage.

Acknowledgments
We are grateful to Manaal Faruqui for providing the bilingual resources. We thank Ryan McDonald
for pointing out the evaluation issue in the experiment. We also thank Sharon Busching for the
proofreading and the anonymous reviewers for the insightful comments and suggestions. This work
was supported by the National Key Basic Research Program of China via grant 2014CB340503
and the National Natural Science Foundation of China (NSFC) via grant 61133012 and 61370164.
Corresponding author: Wanxiang Che, E-mail: car@ir.hit.edu.cn.

References
Bansal, M., Gimpel, K., & Livescu, K. (2014). Tailoring continuous word representations for dependency parsing. In Proceedings of the 52nd Annual Meeting of the Association for Computa1016

fiR EPRESENTATION L EARNING FOR C ROSS -L INGUAL T RANSFER PARSING

tional Linguistics (Volume 2: Short Papers), pp. 809815, Baltimore, Maryland. Association
for Computational Linguistics.
Brown, P. F., Desouza, P. V., Mercer, R. L., Pietra, V. J. D., & Lai, J. C. (1992). Class-based n-gram
models of natural language. Computational linguistics, 18(4), 467479.
Buchholz, S., & Marsi, E. (2006). Conll-x shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLLX), pp. 149164, New York City. Association for Computational Linguistics.
Cao, Y., & Khudanpur, S. (2014). Online learning in tensor space. In Proceedings of the 52nd
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
pp. 666675, Baltimore, Maryland. Association for Computational Linguistics.
Carreras, X. (2007). Experiments with a higher-order projective dependency parser. In Proceedings
of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 957961, Prague, Czech
Republic. Association for Computational Linguistics.
Chandar A P, S., Lauly, S., Larochelle, H., Khapra, M., Ravindran, B., Raykar, V. C., & Saha, A.
(2014). An autoencoder approach to learning bilingual word representations. In Advances in
Neural Information Processing Systems 27, pp. 18531861. Curran Associates, Inc.
Chen, D., & Manning, C. (2014). A fast and accurate dependency parser using neural networks. In
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing
(EMNLP), pp. 740750, Doha, Qatar. Association for Computational Linguistics.
Choi, J. D., & McCallum, A. (2013). Transition-based dependency parsing with selectional branching. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 10521062, Sofia, Bulgaria. Association for Computational
Linguistics.
Collobert, R. (2011). Deep learning for efficient discriminative parsing. In Proceedings of the 14th
International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 224232,
Fort Lauderdale, FL, USA. JMLR.org.
Collobert, R., & Weston, J. (2008). A unified architecture for natural language processing: Deep
neural networks with multitask learning. In Proceedings of the 25th International Conference
on Machine Learning, ICML 08, pp. 160167, Helsinki, Finland. ACM.
Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011). Natural
language processing (almost) from scratch. Journal of Machine Learning Research, 12, 2493
2537.
Covington, M. A. (2001). A fundamental algorithm for dependency parsing. In Proceedings of the
39th annual ACM southeast conference, pp. 95102.
De Marneffe, M.-C., MacCartney, B., Manning, C. D., et al. (2006). Generating typed dependency
parses from phrase structure parses. In Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC06), pp. 449454, Genoa, Italy. European
Language Resources Association (ELRA).
De Marneffe, M.-C., & Manning, C. D. (2008). The stanford typed dependencies representation. In
COLING 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser
Evaluation, pp. 18, Manchester, UK. Association for Computational Linguistics.
1017

fiG UO , C HE , YAROWSKY, WANG & L IU

Duchi, J., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12, 21212159.
Duong, L., Cohn, T., Bird, S., & Cook, P. (2015a). Low resource dependency parsing: Cross-lingual
parameter sharing in a neural network parser. In Proceedings of the 53rd Annual Meeting
of the Association for Computational Linguistics and the 7th International Joint Conference
on Natural Language Processing (Volume 2: Short Papers), pp. 845850, Beijing, China.
Association for Computational Linguistics.
Duong, L., Cohn, T., Bird, S., & Cook, P. (2015b). A neural network model for low-resource universal dependency parsing. In Proceedings of the 2015 Conference on Empirical Methods in
Natural Language Processing, pp. 339348, Lisbon, Portugal. Association for Computational
Linguistics.
Durrett, G., Pauls, A., & Klein, D. (2012). Syntactic transfer using a bilingual lexicon. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing
and Computational Natural Language Learning, pp. 111, Jeju Island, Korea. Association
for Computational Linguistics.
Dyer, C., Ballesteros, M., Ling, W., Matthews, A., & Smith, N. A. (2015). Transition-based dependency parsing with stack long short-term memory. In Proceedings of the 53rd Annual Meeting
of the Association for Computational Linguistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long Papers), pp. 334343, Beijing, China. Association for Computational Linguistics.
Dyer, C., Lopez, A., Ganitkevitch, J., Weese, J., Ture, F., Blunsom, P., Setiawan, H., Eidelman, V.,
& Resnik, P. (2010). cdec: A decoder, alignment, and learning framework for finite-state and
context-free translation models. In Proceedings of the ACL 2010 System Demonstrations, pp.
712, Uppsala, Sweden. Association for Computational Linguistics.
Eisner, J. M. (1996). Three new probabilistic models for dependency parsing: An exploration. In
Proceedings of the 16th conference on Computational linguistics-Volume 1, pp. 340345,
Copenhagen, Denmark. Association for Computational Linguistics.
Faruqui, M., & Dyer, C. (2014). Improving vector space word representations using multilingual
correlation. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pp. 462471, Gothenburg, Sweden. Association for
Computational Linguistics.
Firth, J. R. (1957). A synopsis of linguistic theory 19301955. In Studies in linguistic analysis, pp.
132. Blackwell.
Garg, N., & Henderson, J. (2011). Temporal restricted boltzmann machines for dependency parsing.
In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies, pp. 1117, Portland, Oregon, USA. Association for Computational Linguistics.
Gouws, S., Bengio, Y., & Corrado, G. (2015). Bilbowa: Fast bilingual distributed representations
without word alignments. In Proceedings of the 32nd International Conference on Machine
Learning (ICML), pp. 748756, Lille, France.
Guo, J., Che, W., Wang, H., & Liu, T. (2014). Revisiting embedding features for simple semisupervised learning. In Proceedings of the 2014 Conference on Empirical Methods in Natural
1018

fiR EPRESENTATION L EARNING FOR C ROSS -L INGUAL T RANSFER PARSING

Language Processing (EMNLP), pp. 110120, Doha, Qatar. Association for Computational
Linguistics.
Guo, J., Che, W., Yarowsky, D., Wang, H., & Liu, T. (2015). Cross-lingual dependency parsing
based on distributed representations. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural
Language Processing (Volume 1: Long Papers), pp. 12341244, Beijing, China. Association
for Computational Linguistics.
Haghighi, A., Liang, P., Berg-Kirkpatrick, T., & Klein, D. (2008). Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-08: HLT, pp. 771779, Columbus, Ohio.
Association for Computational Linguistics.
Hardoon, D. R., Szedmak, S., & Shawe-Taylor, J. (2004). Canonical correlation analysis: An
overview with application to learning methods. Neural computation, 16(12), 26392664.
Henderson, J. (2004). Discriminative training of a neural network statistical parser. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL04), Main
Volume, pp. 95102, Barcelona, Spain.
Hermann, K. M., & Blunsom, P. (2014). Multilingual models for compositional distributed semantics. In Proceedings of the 52nd Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pp. 5868, Baltimore, Maryland. Association for Computational Linguistics.
Hwa, R., Resnik, P., Weinberg, A., Cabezas, C., & Kolak, O. (2005). Bootstrapping parsers via
syntactic projection across parallel texts. Natural language engineering, 11(03), 311325.
Jiang, W., Liu, Q., & Lv, Y. (2011). Relaxed cross-lingual projection of constituent syntax. In
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,
pp. 11921201, Edinburgh, Scotland, UK. Association for Computational Linguistics.
Kim, Y. (2014). Convolutional neural networks for sentence classification. In Proceedings of
the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 17461751, Doha, Qatar. Association for Computational Linguistics.
Klein, D., & Manning, C. (2004). Corpus-based induction of syntactic structure: Models of dependency and constituency. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL04), Main Volume, pp. 478485, Barcelona, Spain.
Klementiev, A., Titov, I., & Bhattarai, B. (2012). Inducing crosslingual distributed representations
of words. In Proceedings of COLING 2012, pp. 14591474, Mumbai, India. The COLING
2012 Organizing Committee.
Koehn, P., & Knight, K. (2002). Learning a translation lexicon from monolingual corpora. In Proceedings of the ACL-02 Workshop on Unsupervised Lexical Acquisition, pp. 916, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.
Kolda, T. G., & Bader, B. W. (2009). Tensor decompositions and applications. SIAM review, 51(3),
455500.
Koo, T., & Collins, M. (2010). Efficient third-order dependency parsers. In Proceedings of the
48th Annual Meeting of the Association for Computational Linguistics, pp. 111, Uppsala,
Sweden. Association for Computational Linguistics.
1019

fiG UO , C HE , YAROWSKY, WANG & L IU

Kozhevnikov, M., & Titov, I. (2014). Cross-lingual model transfer using feature representation
projection. In Proceedings of the 52nd Annual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), pp. 579585, Baltimore, Maryland. Association for
Computational Linguistics.
Lei, T., Xin, Y., Zhang, Y., Barzilay, R., & Jaakkola, T. (2014). Low-rank tensors for scoring
dependency structures. In Proceedings of the 52nd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pp. 13811391, Baltimore, Maryland.
Association for Computational Linguistics.
Li, Z., Zhang, M., & Chen, W. (2014). Soft cross-lingual syntax projection for dependency parsing. In Proceedings of COLING 2014, the 25th International Conference on Computational
Linguistics: Technical Papers, pp. 783793, Dublin, Ireland. Dublin City University and Association for Computational Linguistics.
Ma, X., & Xia, F. (2014). Unsupervised dependency parsing with transferring distribution via
parallel guidance and entropy regularization. In Proceedings of the 52nd Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 13371348,
Baltimore, Maryland. Association for Computational Linguistics.
Mann, G. S., & Yarowsky, D. (2001). Multipath translation lexicon induction via bridge languages.
In Proceedings of the Second Meeting of the North American Chapter of the Association
for Computational Linguistics on Language Technologies, NAACL 01, pp. 18, Pittsburgh,
Pennsylvania. Association for Computational Linguistics.
Marcus, M. P., Marcinkiewicz, M. A., & Santorini, B. (1993). Building a large annotated corpus of
english: The penn treebank. Computational linguistics, 19(2), 313330.
Mayberry, M. R., & Miikkulainen, R. (1999). Sardsrn: a neural network shift-reduce parser. In
Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, pp.
820827. Morgan Kaufmann Publishers Inc.
McDonald, R., Crammer, K., & Pereira, F. (2005). Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meeting of the Association for Computational
Linguistics (ACL05), pp. 9198, Ann Arbor, Michigan. Association for Computational Linguistics.
McDonald, R., & Nivre, J. (2007). Characterizing the errors of data-driven dependency parsing
models. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pp.
122131, Prague, Czech Republic. Association for Computational Linguistics.
McDonald, R., Nivre, J., Quirmbach-Brundage, Y., Goldberg, Y., Das, D., Ganchev, K., Hall, K.,
Petrov, S., Zhang, H., Tackstrom, O., Bedini, C., Bertomeu Castello, N., & Lee, J. (2013).
Universal dependency annotation for multilingual parsing. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 92
97, Sofia, Bulgaria. Association for Computational Linguistics.
McDonald, R., Petrov, S., & Hall, K. (2011). Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language
Processing, pp. 6272, Edinburgh, Scotland, UK. Association for Computational Linguistics.
1020

fiR EPRESENTATION L EARNING FOR C ROSS -L INGUAL T RANSFER PARSING

McDonald, R. T., & Pereira, F. C. (2006). Online learning of approximate dependency parsing
algorithms. In Proceedings of the 11st Conference of the European Chapter of the Association for Computational Linguistics, pp. 8188, Trento, Italy. The Association for Computer
Linguistics.
Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations
in vector space. International Conference on Learning Representations (ICLR) Workshop.
Naseem, T., Barzilay, R., & Globerson, A. (2012). Selective sharing for multilingual dependency
parsing. In Proceedings of the 50th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pp. 629637, Jeju Island, Korea. Association for Computational Linguistics.
Naseem, T., Chen, H., Barzilay, R., & Johnson, M. (2010). Using universal linguistic knowledge
to guide grammar induction. In Proceedings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pp. 12341244, Cambridge, MA. Association for Computational Linguistics.
Nilsson, J., & Nivre, J. (2008). Malteval: an evaluation and visualization tool for dependency
parsing.. In Proceedings of the Sixth International Language Resources and Evaluation
(LREC08), pp. 161166, Marrakech, Morocco. European Language Resources Association
(ELRA).
Nivre, J. (2003). An efficient algorithm for projective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies (IWPT), pp. 149160, Nancy, France.
Association for Computational Linguistics.
Nivre, J. (2004). Incrementality in deterministic dependency parsing. In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together, pp. 5057,
Barcelona, Spain. Association for Computational Linguistics.
Nivre, J. (2008). Algorithms for deterministic incremental dependency parsing. Computational
Linguistics, 34(4), 513553.
Nivre, J. (2009). Non-projective dependency parsing in expected linear time. In Proceedings of
the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the AFNLP, pp. 351359, Suntec, Singapore.
Association for Computational Linguistics.
Nivre, J., Hall, J., & Nilsson, J. (2004). Memory-based dependency parsing. In HLT-NAACL 2004
Workshop: Eighth Conference on Computational Natural Language Learning (CoNLL-2004),
pp. 4956, Boston, Massachusetts, USA. Association for Computational Linguistics.
Petrov, S., Das, D., & McDonald, R. (2012). A universal part-of-speech tagset. In Proceedings of
the Eighth International Conference on Language Resources and Evaluation (LREC-2012),
pp. 20892096, Istanbul, Turkey. European Language Resources Association (ELRA).
Rasooli, M. S., & Collins, M. (2015). Density-driven cross-lingual transfer of dependency parsers.
In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 328338, Lisbon, Portugal. Association for Computational Linguistics.
Smith, D. A., & Eisner, J. (2009). Parser adaptation and projection with quasi-synchronous grammar
features. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language
Processing, pp. 822831, Singapore. Association for Computational Linguistics.
1021

fiG UO , C HE , YAROWSKY, WANG & L IU

Socher, R., Bauer, J., Manning, C. D., & Andrew Y., N. (2013). Parsing with compositional vector
grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pp. 455465, Sofia, Bulgaria. Association for Computational Linguistics.
Sgaard, A., Agic, v., Martnez Alonso, H., Plank, B., Bohnet, B., & Johannsen, A. (2015). Inverted
indexing for cross-lingual nlp. In Proceedings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 17131722, Beijing, China. Association for
Computational Linguistics.
Srivastava, N., & Salakhutdinov, R. R. (2012). Multimodal learning with deep boltzmann machines. In Advances in Neural Information Processing Systems 25, pp. 22222230. Curran
Associates, Inc.
Stenetorp, P. (2013). Transition-based dependency parsing using recursive neural networks. In Deep
Learning Workshop at NIPS, Lake Tahoe, Nevada, USA.
Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems 27, pp. 31043112. Curran
Associates, Inc.
Tackstrom, O., McDonald, R., & Uszkoreit, J. (2012). Cross-lingual word clusters for direct transfer
of linguistic structure. In Proceedings of the 2012 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies, pp. 477
487, Montreal, Canada. Association for Computational Linguistics.
Tiedemann, J. (2014). Rediscovering annotation projection for cross-lingual parser induction. In
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pp. 18541864, Dublin, Ireland. Dublin City University and Association for Computational Linguistics.
Tiedemann, J. (2015). Cross-lingual dependency parsing with universal dependencies and predicted
PoS labels., 340349.
Tiedemann, J., Agic, v., & Nivre, J. (2014). Treebank translation for cross-lingual parser induction.,
130140.
Titov, I., & Henderson, J. (2007). Fast and robust multilingual dependency parsing with a generative
latent variable model. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL
2007, pp. 947951, Prague, Czech Republic. Association for Computational Linguistics.
Turian, J., Ratinov, L.-A., & Bengio, Y. (2010). Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, pp. 384394, Uppsala, Sweden. Association for Computational Linguistics.
Uszkoreit, J., & Brants, T. (2008). Distributed word clustering for large scale class-based language
modeling in machine translation. In Proceedings of ACL-08: HLT, pp. 755762, Columbus,
Ohio. Association for Computational Linguistics.
Wang, M., & Manning, C. D. (2013). Effect of non-linear deep architecture in sequence labeling.
In Proceedings of the Sixth International Joint Conference on Natural Language Processing,
pp. 12851291, Nagoya, Japan. Asian Federation of Natural Language Processing.
1022

fiR EPRESENTATION L EARNING FOR C ROSS -L INGUAL T RANSFER PARSING

Weiss, D., Alberti, C., Collins, M., & Petrov, S. (2015). Structured training for neural network
transition-based parsing. In Proceedings of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International Joint Conference on Natural Language
Processing (Volume 1: Long Papers), pp. 323333, Beijing, China. Association for Computational Linguistics.
Xiao, M., & Guo, Y. (2014). Distributed word representation learning for cross-lingual dependency
parsing. In Proceedings of the Eighteenth Conference on Computational Natural Language
Learning, pp. 119129, Ann Arbor, Michigan. Association for Computational Linguistics.
Xue, N., Xia, F., Chiou, F.-D., & Palmer, M. (2005). The penn chinese treebank: Phrase structure
annotation of a large corpus. Natural language engineering, 11(02), 207238.
Yamada, H., & Matsumoto, Y. (2003). Statistical dependency analysis with support vector machines.
In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT), pp. 195
206, Nancy, France. Association for Computational Linguistics.
Yarowsky, D., Ngai, G., & Wicentowski, R. (2001). Inducing multilingual text analysis tools via
robust projection across aligned corpora. In Proceedings of the first international conference
on Human language technology research, pp. 18, San Diego, CA, USA. Association for
Computational Linguistics.
Zeman, D., Dusek, O., Marecek, D., Popel, M., Ramasamy, L., Stepanek, J., Zabokrtsky, Z., &
Hajic, J. (2014). Hamledt: Harmonized multi-language dependency treebank. Language
Resources and Evaluation, 48(4), 601637.
Zhang, Y., & Barzilay, R. (2015). Hierarchical low-rank tensors for multilingual transfer parsing. In
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,
pp. 18571867, Lisbon, Portugal. Association for Computational Linguistics.
Zhang, Y., & Clark, S. (2011). Syntactic processing using the generalized perceptron and beam
search. Computational Linguistics, 37(1), 105151.
Zhang, Y., & Nivre, J. (2011). Transition-based dependency parsing with rich non-local features.
In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 188193, Portland, Oregon, USA. Association for
Computational Linguistics.
Zhao, H., Song, Y., Kit, C., & Zhou, G. (2009). Cross language dependency parsing using a bilingual
lexicon. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Language Processing of the AFNLP, pp.
5563, Suntec, Singapore. Association for Computational Linguistics.
Zhou, H., Zhang, Y., Huang, S., & Chen, J. (2015). A neural probabilistic structured-prediction
model for transition-based dependency parsing. In Proceedings of the 53rd Annual Meeting
of the Association for Computational Linguistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long Papers), pp. 12131222, Beijing, China.
Association for Computational Linguistics.

1023

fiJournal of Articial Intelligence Research 55 (2016) 1091-1133

Submitted 09/2015; published 04/2016

Semantic Visualization with
Neighborhood Graph Regularization
Tuan M. V. Le
Hady W. Lauw

vmtle.2012@phdis.smu.edu.sg
hadywlauw@smu.edu.sg

School of Information Systems
Singapore Management University
80 Stamford Road, Singapore 178902

Abstract
Visualization of high-dimensional data, such as text documents, is useful to map out
the similarities among various data points. In the high-dimensional space, documents
are commonly represented as bags of words, with dimensionality equal to the vocabulary
size. Classical approaches to document visualization directly reduce this into visualizable
two or three dimensions. Recent approaches consider an intermediate representation in
topic space, between word space and visualization space, which preserves the semantics
by topic modeling. While aiming for a good t between the model parameters and the
observed data, previous approaches have not considered the local consistency among data
instances. We consider the problem of semantic visualization by jointly modeling topics
and visualization on the intrinsic document manifold, modeled using a neighborhood graph.
Each document has both a topic distribution and visualization coordinate. Specically, we
propose an unsupervised probabilistic model, called Semafore, which aims to preserve the
manifold in the lower-dimensional spaces through a neighborhood regularization framework
designed for the semantic visualization task. To validate the ecacy of Semafore, our
comprehensive experiments on a number of real-life text datasets of news articles and
Web pages show that the proposed methods outperform the state-of-the-art baselines on
objective evaluation metrics.

1. Introduction
Text documents come in various avors, such as Web pages, news articles, blog posts, emails,
or messages on social media such as Twitter. While much is in English, there are increasing
amounts of content in various languages as well. With the backdrop of the growth in volume, diversity, and complexity of various corpora, we need more useful tools to analyze the
wealth of text content. One form of analysis which we will look into in this paper is visualization. There are dierent types of visualizations, be it of the temporal or longitudinal,
networked, or other natures. What we are interested in is a form of visualization where we
can represent a collection of documents as coordinates on the same low-dimensional space,
so as to learn of the similarities and dierences among documents based on their distances
on the visualization space.
Visualization of high-dimensional data is an important exploratory data analysis task,
which is actively studied by various academic communities. While the HCI community is
interested in the presentation of information, as well as other interface aspects (Chi, 2000),
the machine learning community is interested in the quality of dimensionality reduction
c
2016
AI Access Foundation. All rights reserved.

fiLe & Lauw

(Van der Maaten & Hinton, 2008), i.e., how to transform the high-dimensional representation into a lower-dimensional representation that can be shown on a scatterplot. This
visualization form is simple, and widely applicable across various domains.
Consider therefore the problem of visualizing documents on a scatterplot. Commonly,
a document is represented as a bag of words, i.e., a vector of word counts. This highdimensional representation would be reduced into coordinates on a visualizable 2D (or 3D)
space. One pioneering technique is Multidimensional Scaling (MDS) (Kruskal, 1964). The
goal is to preserve the distances in the high-dimensional space in the low-dimensional embedding. When applied to documents, a visualization technique for generic high-dimensional
data, e.g., MDS, may not necessarily preserve the topical semantics. Words are often ambiguous, with issues such as polysemy, when the same word carries multiple senses, and
synonymy, when dierent words carry the same sense. Because the dimensions in the original representation (which are words) may not accurately capture this ambiguity, this aects
the quality of the reduced representation (which is the visualization space) as well.
To model semantics in documents in a way that can resolve some of this ambiguity, the
current popular approach is by topic modeling, such as PLSA (Hofmann, 1999) or LDA
(Blei, Ng, & Jordan, 2003). Each document is associated with a probability distribution
over a set of topics. Each topic is a probability distribution over words in the vocabulary. In
this way, polysemous words can be separated into dierent topics, and synonymous words
can be grouped into the same topic.
Topic modeling itself is another form of dimensionality reduction: from word space to
topic space. The word space refers to a documents original representation, which is usually
a bag of words. The topic space refers to the simplex of topic distributions. A documents
probability distribution over topics is eectively the representation of this document in
this topic space. However, a topic model by itself is not designed for visualization. While
one possible visualization is to plot documents topic distributions on a simplex, a 2D
visualization space could express only three topics, which is very limiting.
Given its success in modeling semantics in documents, we therefore ask the question
of whether and how best to do both forms of dimensionality reductions (visualization and
topic modeling) for documents. The end goal is to arrive at a visualization of documents
that is consistent with both the semantic representation (topics), as well as the original
representation (words). This coupling is a distinct task from topic modeling or visualization
respectively, as it enables novel capabilities. For one thing, topic modeling helps to create
a richer visualization, as we can now associate each coordinate on the visualization space
with both topic and word distributions, providing semantics to the visualization space.
For another, the tight integration potentially allows the visualization to serve as a way
to explore and tune topic models, allowing users to introduce feedback (Hu, Boyd-Graber,
Satino, & Smith, 2014) to the model through a visual interface (Choo, Lee, Reddy, & Park,
2013). These capabilities support several use case scenarios. One potential use case is a
document organizer system. The visualization could potentially help in assigning categories
to documents, by showing how closely related documents have been labeled. Another is
an augmented retrieval system. Given a query, the results may include not just relevant
documents, but also other similar documents (neighbors in the visualization).
1092

fiSemantic Visualization with Neighborhood Graph Regularization

1.1 Problem Statement
We refer to the task of jointly modeling topics and visualization as semantic visualization.
The input is a set of documents D. For a specied number of topics Z and visualization
dimensionality (assumed to be 2D, without losing any generality), the goal is to derive,
for every document in D, a latent coordinate on the visualization space, and a probability
distribution over the Z topics. While we focus on documents in our description, the same
approach would apply to visualization of other data types for which latent factor modeling,
i.e., topic model, makes sense.
A straightforward way is to undergo two-step reductions. In the rst reduction, the
original representation for documents are reduced into topic distributions using topic modeling. In the second reduction, documents topic distributions are further reduced into
visualization coordinates. This approach may have some value compared to direct reduction from word space to visualization space. However, it is not ideal, because the disjoint
reductions could mean that errors may propagate from the rst to the second reduction,
and the resulting visualization may not faithfully capture the original representation.
A better way to solve this problem is to join up the two reductions into a single, joint
process that produces both topic distributions and visualization coordinates. This approach
was rst pioneered by PLSV (Iwata, Yamada, & Ueda, 2008), which also showed that the
joint approach outperformed the disjoint approach. PLSV derives the latent parameters
by maximizing the likelihood of observing the documents. This goal is concerned with the
error between the model and the observation.
In the literature, it is found that algorithms that ensure smoothness tend to perform
better at learning tasks (Zhou, Bousquet, Lal, Weston, & Scholkopf, 2004). Smoothness
concerns preserving the observed proximity between documents. This objective arises naturally from the assumption that the intrinsic geometry of the data is a low-rank, non-linear
subspace within the high-dimensional space. Therefore, preserving neighborhood structure
is important for learning tasks. This assumption is well-accepted in the machine learning
community (Laerty & Wasserman, 2007), and nds application in both supervised and
unsupervised learning (Belkin & Niyogi, 2003; Zhou et al., 2004; Zhu, Ghahramani, Lafferty, et al., 2003). Recently, there is a preponderance of evidence that this assumption also
applies to text data in particular (Cai, Mei, Han, & Zhai, 2008; Cai, Wang, & He, 2009;
Huh & Fienberg, 2012). We therefore propose to incorporate this assumption into a new
unsupervised, semantic visualization model.
1.2 Overview
We propose an unsupervised probabilistic model that jointly derives topic distributions and
visualization coordinates on the intrinsic geometry of the data. Our proposed model is called
Semafore, which stands for SEmantic visualization with MAniFOld REgularization. We
build a neighborhood regularization framework into a semantic visualization model. The
framework involves new issues to resolve, including the regularization function, and the
space in which regularization should take place.
The model is evaluated on a series of real-life, publicly available datasets, which are
also benchmark datasets used in document classication task. An advantage of a statistical
method, such as ours, is that it is not dependent on a specic language. Two of the datasets
1093

fiLe & Lauw

are in English, and one is in Brazilian Portuguese. While our model is unsupervised (class
label is neither required nor used in learning), to objectively quantify the visualization quality, we leverage on the class label information. It is a common assumption that documents
of the same class are expected to be neighbors on the original space (Belkin, Niyogi, &
Sindhwani, 2006; Zhou et al., 2004; Zhu et al., 2003), which suggests that they should also
be close on the visualization space. We investigate the eectiveness of Semafore in placing
documents of the same class nearby on the visualization space, and systematically compare
it to existing baselines without one or more of our properties, namely: joint modeling of
topic and visualization, or neighborhood regularization.
1.3 Contributions
While visualization and topic modeling are, separately, well-studied problems, the interface
between the two, semantic visualization, is a relatively new problem, with very few previous
work. In this work, we make the following contributions.
 We propose incorporating neighborhood structure in semantic visualization. In this
respect, we propose a probabilistic model Semafore, with two integrated components. One is a kernelized semantic visualization model, enabling the substitution of
the kernel functions that relate visualization coordinates to topic distributions (see
Section 3.3). The other is a neighborhood graph regularization framework for semantic
visualization as described in Section 4.1.
 Realizing the neighborhood graph regularization involves an exploration of how to
incorporate the appropriate forms of the neighborhood structure. In this respect,
we investigate the eects of neighborhood graph construction techniques such as knearest neighbors (k-NN), -ball, and disjoint minimum spanning trees (DMST), as
well as dierent edge weight estimations such as heat-kernel (see Section 4.2) in the
context of semantic visualization.
 In Section 5, we describe the requisite learning algorithms based on maximum a
posteriori (MAP) estimation using expectation-maximization (EM), in order to t
the parameters for the various regularization functions and kernels that we propose.
 Our nal contribution is the evaluation of Semafores eectiveness on a series of reallife, public datasets described in Section 6, which shows that Semafore outperforms
existing baselines on a well-established and objective visualization metric.
In our prior work (Le & Lauw, 2014b), we proposed the problem and described the preliminary model. In this extended article, there are signicant technical changes that provide
a signicantly more comprehensive discussion of the model. For instance, we now discuss
the Student-t kernel, in addition to the previously introduced Gaussian kernel. Furthermore, we investigate the ecacies of dierent neighborhood graph constructions, including
the -ball and DMST graphs, in addition to the previously introduced kNN graph. The
graph weights are also enhanced through investigation of heat kernel, in addition to the
simple-minded binary scheme previously. As discussed in Section 6.3, these enhancements
collectively result in statistically signicant improvements over the previous model. Beyond
1094

fiSemantic Visualization with Neighborhood Graph Regularization

the technical enhancements, we also provide more comprehensive model analysis and empirical validation, including richer quantitative and qualitative discussions of the visualizations
and the resulting topic models, as well as a metric to measure topic interpretability based
on pairwise mutual information.

2. Related Work
In this section, we discuss the dierent aspects of our work, identify the related papers in
the literature, and point out the key conceptual dierences.
2.1 Visualization and Dimensionality Reduction
One way to perform visualization is by using a generic dimensionality reduction technique.
Such techniques come in several avors, depending on the objective. Principal component
analysis (PCA) (Jollie, 2005) identies the components that explain most of the variance
in the data. Related to PCA is singular value decomposition (SVD) (Golub & Van Loan,
2012). Comparatively, independent component analysis (ICA) (Comon, 1994) identies
the components that are independent of one another, whereas linear discriminant analysis
(Fishers LDA) (Fisher, 1936) identies the components that most discriminate between
known class labels. Being generic, these techniques are more frequently applied to feature
extraction, as they are not optimized for visualization. They focus more on the properties of
the components (e.g., orthogonality, independence) rather than on the intrinsic relationship
among data instances. Furthermore, as they are based on linear projections, they may not
capture non-linearities in the data well.
Another category of techniques, which is more directly related to visualization, is the
embedding approach. It aims to preserve the high-dimensional similarities or dierences
in the low-dimensional embedding. One pioneering such work is multidimensional scaling
(MDS) (Kruskal, 1964). Given a set of pairwise distances ij between data points i and j,
MDS determines coordinates xi and xj respectively, such that the embedded visualization
distance ||xi  xj || approximates ij as much as possible. For MDS, the distance to be
preserved ij is frequently the linear distance, measuring the distance along a straight line
between two points in the input space. Instead of this linear distance, Isomap (Tenenbaum,
De Silva, & Langford, 2000) seeks to preserve the geodesic distance, by nding shortest paths
in a graph with edges connecting neighboring data points. LLE (Roweis & Saul, 2000) seeks
to preserve linear distances, but only among the neighboring points and avoiding the need to
estimate pairwise distances between widely separated data points. Recently there are also
works applying a similar concept to embedding but using probabilistic modeling, such as
PE (Iwata, Saito, Ueda, Stromsten, Griths, & Tenenbaum, 2007), SNE (Hinton & Roweis,
2002), t-SNE (Van der Maaten & Hinton, 2008), and GTM (Bishop, Svensen, & Williams,
1998). Yet others are based on semi-denite programming (Shaw & Jebara, 2007, 2009).
Alternatively, several embedding techniques do not aim to preserve relationship among
data instances, but rather other properties such as local minima (Kim & Torre, 2010).
Importantly, all these techniques are not optimized for semantic visualization, as they do
not model topics at all. The coordinates do not reect any semantic meaning, other than
reecting the optimization objective.
1095

fiLe & Lauw

There are only a few related works so far that seek to address the semantic visualization
task directly. The closest previous work that does both topic modeling and visualization
in a single generative process is Probabilistic Latent Semantic Visualization (PLSV) (Iwata
et al., 2008), which also shows that a joint approach outperforms a separate approach. Just
as PLSV builds upon the foundation of the topic modeling technique Probabilistic Latent
Semantic Analysis (PLSA) (Hofmann, 1999) by incorporating visualization coordinates, so
do we build upon the foundation of PLSV by incorporating RBF kernels (Section 3.3) and
neighborhood structure (Section 4).
There are also related works that share a similar objective, but do not share the same
paradigm of visualization or topic modeling. For instance, LDA-SOM (Millar, Peterson, &
Mendenhall, 2009) rst conducts topic modeling using Latent Dirichlet Allocation (LDA)
(Blei et al., 2003), and then separately embeds the documents topic distributions on a
Self-Organizing Map (SOM) (Kohonen, 1990). However, this is not a joint model, and
SOM uses a dierent visualization space than the Euclidean space that we are interested
in. For another instance, SSE (Le & Lauw, 2014a) builds on the Spherical Admixture
Model (SAM) (Reisinger, Waters, Silverthorn, & Mooney, 2010) belonging to the class of
spherical topic models targeted at spherical (unit vector) reprepresentations of topics and
documents, which are not directly comparable or equivalent with the simplex representation
and multinomial modeling (probability distribution over words) adopted in this work as well
as PLSV.
By semantic visualization, we refer to the task of joining visualization and topic modeling. A related, but dierent, task is topic visualization, where the objective is to visualize
the topics, in terms of which keywords are dominant for each topic (Chaney & Blei, 2012;
Chuang, Manning, & Heer, 2012), which topics are dominant in a corpus (Wei, Liu, Song,
Pan, Zhou, Qian, Shi, Tan, & Zhang, 2010), and how topics are related to one another
(Gretarsson, Odonovan, Bostandjiev, Hollerer, Asuncion, Newman, & Smyth, 2012).
2.2 Topic Modeling
Topic model involves statistical modeling of text (documents and words) in order to discover
some abstract concepts or topics that occur in a corpus. Beginning with latent semantic
indexing (Dumais, Furnas, Landauer, Deerwester, Deerwester, et al., 1995), topic model
evolves into the modern probabilistic treatments, such as Probabilistic Latent Semantic
Analysis (PLSA) (Hofmann, 1999) and Latent Dirichlet Allocation (LDA) (Blei et al.,
2003). Intuitively, a topic captures a collection of words that tend to co-occur because
they describe the same concept. This has the appeal of producing highly interpretable
statistical models that let users make semantic sense of the corpus. Other than text-only
document corpora, topic models have also been applied to cases where links are observed
in addition to text (McCallum, Wang, & Corrada-Emmanuel, 2007).
Meanwhile, the assumption that the intrinsic geometry of the data is a non-linear low
dimensional subspace within the high-dimensional space nds application in both supervised
and unsupervised (Belkin & Niyogi, 2003) learning algorithms. It is especially prevalent in
semi-supervised learning (Zhou et al., 2004; Zhu et al., 2003) as a way to bridge labeled
and unlabeled data. Regularization as a technique to realize this assumption has a long
history (Belkin et al., 2006). The specic form of the regularization function varies among
1096

fiSemantic Visualization with Neighborhood Graph Regularization

applications. The study of this assumption for unsupervised topic models begins with
LapPLSI (Cai et al., 2008), which introduces regularization to PLSA (Hofmann, 1999),
by minimizing the Euclidean distance between neighboring documents topic distributions.
Follow-up work introduce other distance functions (Cai et al., 2009; Wu, Bu, Chen, Zhu,
Zhang, Liu, Wang, & Cai, 2012). While these previous work focus on maintaining proximity
of similar documents, DTM (Huh & Fienberg, 2012) adds a new criterion to also maintain
the distance among dierent documents. Our work is dierent in that we also need to
contend with the visualization aspects, and not just topic modeling.
2.3 Semantic Similarity
Other than topic models, there are alternative mechanisms to learn the semantic relationship
between documents. One way is by measuring the semantic similarity among documents
or words. For instance, in vector space model, documents may be represented as a term
vector, and their similarity may be expressed in terms of cosine similarity (Turney, Pantel,
et al., 2010). Other than word occurrences alone, there could also be additional signals
of semantic similarity. For instance, working with Wikipedia corpus, the categories and
links are also took into account to determine the similarity among articles (Gabrilovich
& Markovitch, 2009; Ponzetto & Strube, 2007). Our work diers from these in several
important respects. First, our objective is not in the similarity value per se, but rather in
determining lower-dimensional embedding coordinates, which would allow visualization as
one application. Second, our method is based on probabilistic modeling of latent variables,
akin to topic modeling, instead of operating on the vector space model representation of
documents.

3. Semantic Visualization
We introduce the problem formulation for semantic visualization in Section 3.1. Our focus in
this paper is on the eects of the neighborhood graph structure on the semantic visualization
task. We gure that the clearest way to showcase these eects is to design a neighborhood
preservation framework over and above an existing generative process, such as PLSV (Iwata
et al., 2008), which we will review in Section 3.2. In Section 3.3, we describe an innovation
over the semantic visualization model, which is an abstraction of the mapping between the
topic space and the visualization space using radial basis function (RBF) kernels. This
allows the exploration of various kernels, of which we identify two for further exploration.
For ease of following the discussion, we include a table of notations in Table 1.
3.1 Problem
For the task of semantic visualization, the input is a corpus of documents D = {d1 , . . . , dN }.
Every dn is a bag of words, and wnm denotes the mth word in dn . The total number of
words in dn is Mn . The objective is to learn, for each dn , a latent distribution over Z
topics {P(z|dn )}Z
z=1 . Each topic z is associated with a parameter z , which is a probability
distribution {P(w|z )}wW over words in the vocabulary W . The words with the highest
probabilities for a given topic capture the semantic of that topic.
1097

fiLe & Lauw

Notation
dn
xn
Mn
z
z
z
W
N
Z





Description
a specic document
latent coordinate of dn in the visualization space
number of words in document dn
a specic topic
coordinate of topic z in the visualization space
word distribution of topic z
the vocabulary (the set of words in the lexicon)
total number of documents in the corpus
total number of topics (user-dened)
the collection of xn s for all documents
the collection of z s for all topics
the collection of z s for all topics
the collective set of parameters {, , }

Table 1: Notations.
In semantic visualization, there is an additional objective for semantic visualization,
which is to learn, for each document dn , its latent coordinate xn on a low-dimensionality
visualization space. Similarly, each topic z is associated with a latent coordinate z on
the visualization space. A document dn s topic distribution is then expressed in terms
of the Euclidean distance between its coordinate xn and the dierent topic coordinates
 = {z }Z
z=1 . Intuitively, the closer is xn to a topics z , the higher is P(z|dn ) or the
probability of topic z for document dn .
In the following sections, we systematically describe the various components of our
solution. The generative process that links the latent variables (coordinates) and the words
in the documents is described in Section 3.2. The specic relationship between documents
and topics coordinates constitutes a specic mapping function, which we model as an RBF
kernel in Section 3.3. In the following Section 4, we discuss how to incorporate neighborhood
structure into semantic visualization.
3.2 Generative Process
We now describe the generative process of documents based on both topics and visualization
coordinates. Below we review PLSV whose graphical model is shown in Figure 1. Our
eventual complete model is a generalization of this model, involving enhancements through
kernelization (Section 3.3) and neighborhood structure preservation (Section 4).
The generative process is as follows:
1. For each topic z = 1, . . . , Z:
(a) Draw zs word distribution: z  Dirichlet()
(b) Draw zs coordinate: z  Normal(0,  1 I)
2. For each document dn , where n = 1, . . . , N :
(a) Draw dn s coordinate: xn  Normal(0,  1 I)
1098

fiSemantic Visualization with Neighborhood Graph Regularization

N



Mn w

x

Z

z









Figure 1: Graphical model of PLSV.
(b) For each word wnm  dn :
i. Draw a topic: z  Multi({P(z|xn , )}Z
z=1 )
ii. Draw a word: wnm  Multi(z )
Here,  is a Dirichlet prior, I is the identity matrix,  and  control the variance of the
Z
Z
Normal distributions. The parameters  = {xn }N
n=1 ,  = {z }z=1 ,  = {z }z=1 , collectively
denoted as  = , , , are learned from documents D based on maximum a posteriori
estimation. The log likelihood function is shown in Equation 1.
L(|D) =

Mn
N 

n=1 m=1

log

Z


P(z|xn , )P(wnm |z )

(1)

z=1

We reiterate that our focus here is on incorporating neighborhood graph structure into
semantic visualization. By building a neighborhood graph regularization framework into
an existing generative process, i.e., PLSV, we can clearly observe that any improvement
over PLSV arises from the neighborhood graph regularization. In this sense, our work
is in the tradition of introducing neighborhood graph regularization to probabilistic topic
modeling (Huh & Fienberg, 2012; Cai et al., 2008, 2009), where the contributions relate
to the neighborhood graph regularization, rather than the generative process. That said,
there is one signicant dierence to PLSV, which is our exibility in allowing various kernel
functions, which we will discuss next.
3.3 RBF Kernels
In the Step 2(b)i of the above generative process, the topic z of a word is drawn from
the distribution {P(z|xn , )}Z
z=1 . This distribution relates the coordinates of topics in
the visualization space  = {z }Z
z=1 and the coordinate xn of a document dn with the
documents topic distribution {P(z|dn )}Z
z=1 .
This relationship can be formulated as a mapping problem where we want to nd a
function G which maps a point in visualization space to a point in the topic space. However,
the form of G cannot be known exactly because both visualization space and topic space
are latent spaces and G may be dierent across dierent domains. Therefore, to compute
the topic distributions, we need a way to approximate G.
To build a function approximation of the unknown function G, we use the abstraction
of Radial Basis Function (RBF) neural networks (Bishop, 1995) because feedforward multilayered RBF neural networks with one hidden layer can serve as a universal approximator
1099

fiLe & Lauw

K nz



t
Z      
Z










/xn

Figure 2: Topic distribution is expressed as a function of visualization coordinates using
Radial Basis Function (RBF) network.

to arbitrary continuous functions (Park & Sandberg, 1991). This property provides the
condence that the model would have the ability to approximate any existing relationship
between visualization space and topic space with arbitrary precision. Unlike PLSV (Iwata
et al., 2008) that dened a specic mapping function, our approach generalizes the semantic visualization model by dening the mapping problem in terms of kernelization, which
admits several mapping functions within the family of RBF kernels.
In our context, Radial Basis Function (Buhmann, 2000) will relate coordinate variables
based on distances which denes a kernel function (||xn  z ||) in terms of how far a data
point (e.g., xn ) is from a center (e.g., z ). The kernel function  may take on various forms,
e.g., Gaussian, multi-quadric, inverse quadratic, polyharmonic spline. To express P(z|dn )
as a function of xn , we consider the normalized architecture of RBF network, with three
layers as shown in Figure 2. The input layer consists of one input node (xn ). The hidden
layer consists of Z number of normalized RBF activation functions. Each is centered at
z and computes Z (||xn z ||) . The linear output layer consists of Z output nodes.
z  =1

(||xn z ||)

Each output node yz (xn ) corresponds to P(z|dn ), which is a linear combination of the RBF
functions, as shown in Equation 2. Here, wz,z  is the weight of inuence of the RBF function

of z  on the P(z|dn ), with the constraint Z
z  =1 wz,z  = 1.
Z
P(z|dn ) = yz (xn ) =

z  =1 wz,z   (||xn  z  ||)
Z
z  =1 (||xn  z  ||)

(2)

While Equation 2 is the general form, to instantiate a specic mapping function, we
need to determine both the assignment of wz,z  and the form of the function . For wz,z  ,
we will experiment with a special case wz,z  = 1 when z = z  and 0 otherwise.
For the kernel function , one variation we consider is Gaussian, which yields the function in Equation 3, where  refers to the collective set of z s. Note that here we set variance
of Gaussian to 1. However, its true value is not really important because a dierent variance
value just produces a re-scaled visualization with the scaling factor equal to that variance.
1100

fiSemantic Visualization with Neighborhood Graph Regularization

exp( 12 ||xn  z ||2 )
P(z|dn )Gaussian = P(z|xn , )Gaussian = Z
1
2
z  =1 exp( 2 ||xn  z  || )

(3)

Another variation of  being considered is Student-t. This distribution is also used by
t-SNE (Van der Maaten & Hinton, 2008) in the context of non-semantic, direct embedding to mitigate the eects of crowding. Due to mismatched dimensionalities, the points
are crunched together in the center of the visualization, which prevents gaps from forming
between the clusters. Therefore, we hypothesize that using Student-t as radial basis function, which yields the function in Equation 4, can help to improve the performance of our
model if crowding becomes an issue. Note that the Student-t distribution with one degree
of freedom yields a radial basis function having the form similar to the inverse quadratic.
(1 + ||xn  z ||2 )1
P(z|dn )Studentt = P(z|xn , )Studentt = Z
2 1
z  =1 (1 + ||xn  z  || )

(4)

The Gaussian function (Equation 3) was also used previously in the baseline PLSV
(Iwata et al., 2008) that we will compare to. Its inclusion helps to establish parity for
comparative purposes, both to investigate the eectiveness of the alternative Student-t
kernel (described above), as well as that of the neighborhood regularization (described in
the next section).

4. Neighborhood Graph Regularization Framework
There are recent works (Cai et al., 2008, 2009; Huh & Fienberg, 2012) trying to preserve the
local neighborhood structure when learning low-dimensional topic representations of documents. These works assume that documents are sampled from a nonlinear low-dimensional
subspace that are embedded in a high-dimensional space. Therefore, the local neighborhood
structure is important for revealing the hidden topics of documents and should be preserved
when learning topic representations of documents (Bai, Guo, Lan, & Cheng, 2014). In the
generative process for semantic visualization described in Section 3, the document parameters are sampled independently, and may not necessarily reect the underlying local neighborhood structure. We therefore seek to realize this assumption for semantic visualization.
In particular, we assume that when two documents di and dj are close in the original space,
then their parameters i and j of the low-rank representation are similar as well. Coupled
with the kernelized semantic visualization model described in Section 3, the neighborhood
preservation approach described in this section constitutes our proposed model, Semafore,
which stands for SEmantic visualization with MAniFOld REgularization.
4.1 Neighborhood Regularization
The neighborhood structure can be represented by a neighborhood graph. Given a set of
data points in the Euclidean space, a neighborhood graph is constructed with the input
data points as vertices. By denition, edges are symmetric, i.e., ij = ji , and weighted.
The collection of edge weights are collectively denoted as  = {ij }.
For the moment, we will assume that we have the neighborhood graph, and address
the issue of how this neighborhood graph may be incorporated into our semantic visualiza1101

fiLe & Lauw

tion framework. In actuality, the neighborhood graph construction itself is an important
component, whose construction is described in detail in Section 4.2.
One eective means to incorporate a neighborhood structure into a learning model is
through a regularization framework (Belkin et al., 2006). This leads to a re-design of the
log-likelihood function in Equation 1 into a new regularized function L (Equation 5), where
 consists of the parameters (visualization coordinates and topic distributions), and D and
 are the documents and neighborhood structure.
L(|D, ) = L(|D) +   R(|)

(5)

The rst component L is the log-likelihood function in Equation 1, which reects the
t between the latent parameters  and the observation D. The second component R is a
regularization function, which reects the consistency between the latent parameters  of
neighboring documents in the neighborhood structure .  is the regularization parameter,
commonly found in neighborhood based algorithms (Belkin et al., 2006; Cai et al., 2008,
2009), which controls the extent of regularization (we will experiment with dierent s in
experiments).
4.1.1 Proposed Regularization Function
We now turn to the denition of the R function. The intuition is that the data points that
are close in the high-dimensional space, should also be close in their low-rank representations, i.e., local consistency, also known as smoothness. One function that satises this is
R+ in Equation 6. Here, F is a distance function that operates on the low-rank space.
Minimizing R+ leads to minimizing the distance F(i , j ) between neighbors (ij = 1).
R+ (|) =

N


ij  F(i , j )

(6)

i,j=1;i=j

The above level of local consistency is still insucient, because it does not regulate how
non-neighbors (i.e., ij = 0) behave. For instance, it does not prevent non-neighbors from
having similar low-rank representations. Another valid objective in visualization is to keep
non-neighbors apart, which is satised by another objective function R in Equation 7. R
is minimized when two non-neighbors di and dj (i.e., ij = 0) are distant in their low-rank
representations. The addition of 1 to F is to prevent division-by-zero error.
R (|) =

N

i,j=1;i=j;ij =0

1  ij
F(i , j ) + 1

(7)

We hypothesize that neither objective is eective on its own. A more complete objective
would capture the spirits of both keeping neighbors close, and keeping non-neighbors apart.
Therefore, we put Equation 6 and Equation 7 together using summation and maximize the
objective function as shown in Equation 8. Note that the coecient 12 in Equation 8 is for
simplifying the formula of the derivative of R (|).
1
R (|) =  (R+ (|) + R (|))
2
1102

(8)

fiSemantic Visualization with Neighborhood Graph Regularization

2

d1

1

d2

I1

I2

0
-2

-1

0
-1

1

2

3

4

d3

-2

Figure 3: Example of how the same topic distribution may have dierent visualization coordinates. Any points on the red line have same topic distributions.

Summation preserves the absolute magnitude of the distance, and helps to improve the
visualization task by keeping non-neighbors separated on a visualizable Euclidean space.
Taking the product is unsuitable, because it constrains the ratio of distances between neighbors to distances between non-neighbors. This may result in the crowding eect, where
many documents are clustered together, because the relative ratio may be maintained, but
the absolute distances on the visualization space could be too small.
Other than the proposed regularization function above, it is also possible to consider
other regularization functions. For instance, we have also experimented with modifying the
regularization function adapted from Discriminative Topic Model (DTM) (Huh & Fienberg,
2012), which addressed topic modeling but not semantic visualization. Note that while in
the original DTM formulation, the distance function F(i , j ) operates in the topic space,
we adapt it for semantic visualization by redening the distance function F(i , j ) so that
it will operate in the visualization space instead. This modied DTM formulation is shown
to underperform the proposed regularization function above (Le & Lauw, 2014b).
4.1.2 Enforcing Neighborhood Structure: Visualization vs. Topic Space
We now turn to the denition of F(1 , 2 ). In neighborhood-based models (Belkin et al.,
2006; Cai et al., 2008, 2009), there is only one low-rank representative space. For semantic
visualization, there are two: topic and visualization spaces. We look into where and how
to enforce the neighborhood graph structure.
At rst glance, they seem equivalent. After all, they are representations of the same
documents. However, this is not necessarily the case. Consider a simple example of two
topics z1 and z2 with visualization coordinates 1 = (0, 0) and 2 = (2, 0) respectively.
Meanwhile, there are three documents {d1 , d2 , d3 } with coordinates x1 = (1, 1), x2 = (1, 1),
and x3 = (1, 1). If two documents have the same coordinates, they will also have the
same topic distributions. In this example, x1 and x2 are both equidistant from 1 and 2 ,
and therefore according to Equation 3, they have the same topic distribution P(z1 |d1 ) =
P(z1 |d2 ) = 0.5, and P(z2 |d1 ) = P(z2 |d2 ) = 0.5. If two documents have the same topic
distributions, they may not necessarily have the same coordinates. d3 also has the same
1103

fiLe & Lauw

topic distribution as d1 and d2 , but a dierent coordinate. In fact, any coordinate of the
form (1, ?) will have the same topic distribution. This example is illustrated in Figure 3.
This suggests that enforcing neighborhood structure on the topic space may not necessarily lead to having data points closer on the visualization space. We postulate that
regularizing the visualization space is more eective. There are also advantages in computational eciency to doing so, which we will describe further shortly. Therefore, we
dene F(i , j ) as the squared Euclidean distance ||xi  xj ||2 between the corresponding
visualization coordinates.
4.2 Neighborhood Graph
We discuss how the neighborhood graph may be approximated, which concerns the two
issues of how the graph edges are dened, as well as how they are weighted. The neighborhood graph is constructed in the original data space where we represent each document as a
tf-idf vector (Manning, Raghavan, Schutze, et al., 2008). We also experiment with dierent
vector representations, including word counts and term frequencies, and nd tf-idf to give
the best results. The distance between two document vectors is measured using Euclidean
distance.
4.2.1 Graph Construction
There have been research studies on the properties and methods for construction of neighborhood graphs (Zemel & Carreira-Perpinan, 2004; Carey & Mahadevan, 2014). Since the
construction of neighborhood graph is a critical step that may aect the performance of
various graph-based algorithms, this problem itself is a research issue of independent interest. Our scope is in exploring how some well-established graph construction techniques
may apply to the case of semantic visualization. We will investigate these various graph
construction methods empirically in Section 6.
In the following, we briey review two categories of graph construction methods.
1. Neighborhood-based Graphs. In this formulation, edges are formed between data points
that are deemed to be suciently close to each other. This admits dierent denitions
of sucient closeness. The most common denitions found in the literature include
the two below.
(a) -ball: The neighborhood graph contains an edge connecting two documents di
and dj , if di and dj have a distance less than a threshold .
(b) k-nearest neighbors (k-NN) graph: The neighborhood graph contains an edge
connecting two documents di and dj , if di is in the set Nk (dj ) of the knearest
neighbors of dj , or dj is in the set Nk (di ).
-ball and k-NN both have strongly data-dependent parameters (i.e.,  and k) and it is
not straightforward to choose the best value for these parameters. Neither guarantees
that the graph would be connected. They also need to be carefully selected or tuned,
as to some extent they also aect the balance between the contribution of neighbors
R+ and non-neighbors R to the neighborhood regularization R in Equation 8. In
1104

fiSemantic Visualization with Neighborhood Graph Regularization

Appendix A, we explore empirically how these graph parameters can help to maintain
this balance within the neighborhood regularization function.
-ball suers from another issue that it tends to produce many edges for the points
located at high-density regions, and thus has little restriction on the maximum degree
of a vertex. k-NN does not suer from that problem and is one of the most commonly
used types of graphs.
In our subsequent development and experiments, we will experiment with both -ball
and k-NN graph as there may be some variance in the performance of dierent graph
construction techniques for dierent datasets (Hein, Audibert, & Luxburg, 2007; Ting,
Huang, & Jordan, 2010; Coifman & Lafon, 2006).
2. Minimum Spanning Tree-based Graphs. While -ball and k-NN are quite sensitive
to noise and sparsity, graph construction based on combining multiple minimum
spanning trees can help to reduce sensitivity to noise of the output graph (Zemel
& Carreira-Perpinan, 2004). There are two variations based on this approach.
(a) Perturbed Minimum Spanning Trees (PMST): PMST builds a neighborhood
graph by generating T > 1 perturbed copies of the whole dataset according
to the local noise model and tting an MST to each perturbed copy. A weight
eij  [0, 1] will be assigned to the edge between points xi and xj equal to the
average number of times that edge appears on the trees.
(b) Disjoint Minimum Spanning Trees (DMST): DMST produces a neighborhood
graph by nding a deterministic collection of r minimum spanning trees that
satises the property that no tree in the collection uses any edge of other trees.
The neighborhood graph is the union of all edges of trees and contains r(N  1)
edges.
As the representative of this category, we use DMST, which is deterministic and easier
to construct than PMST while showing similar ecacies.
4.2.2 Graph Weighting
The next issue is how to assign weights to the edges in the neighborhood graph. In this
respect, we consider two variations of edge weights.
1. Simple Minded :

ij =

1,
0,

if only if di and dj are connected,
otherwise.

(9)

This is the simplest approach where we use binary weighting to assign the weights
to the edges. However, this approach to assign uniform weights to edges can be
sensitive to errors, because of the cli eect from 1 immediately to 0. Moreover,
since the weights are not smoothed, it could result in some loss of information. We
hypothesize that among the connected nodes, there may still be some dierences in
terms of degrees of similarity, which are expressed by their mutual distances. This
motivates the second approach below.
1105

fiLe & Lauw

2. Heat Kernel :

ij =

exp(
0,

||di dj ||2
),


if only if di and dj are connected,
otherwise.

(10)

An alternative approach is using the Heat Kernel function (Belkin & Niyogi, 2001;
Jebara, Wang, & Chang, 2009). Heat Kernel has the advantage over Simple Minded by
allowing smoother weights for the edges, which helps address the issues of sensitivity
and loss of information. However, while Simple Minded is not parameterized, Heat
Kernel has one parameter that needs to be determined (i.e.,  ). Note that for  = ,
Heat Kernel degenerates into Simple Minded, i.e., the former is the more general
formulation. The exact value of  is not important in our model because it would
eectively be absorbed by the regularization parameter. For simplicity, we set  = 2.

5. Model Fitting
We now discuss how the parameters of the model described in Sections 3 and 4 can be
learned. One well-accepted framework to learn model parameters using maximum a posteriori (MAP) estimation is the Expectation-Maximization or EM algorithm (Dempster,
Laird, & Rubin, 1977).
For our model, the regularized conditional expectation of the complete-data log likelihood in MAP estimation with priors is:
Q(|) =
+

Mn 
N 
Z




P(z|n, m, ) log P(z|xn , )P(wnm |z )

n=1 m=1 z=1
N


Z


n=1

z=1

log(P(xn )) +

log(P(z )) +

Z


log(P(z ))

z=1

+   R(|),
where  is the current estimate. P(z|n, m, ) is the class posterior probability of the nth
document and the mth word in the current estimate. P(z ) is a symmetric Dirichlet prior
with parameter  for word probability z . P(xn ) and P(z ) are Gaussian priors with a zero
mean and a spherical covariance for the document coordinates xn and topic coordinates z .
We set the hyper-parameters to  = 0.01,  = 0.1N and  = 0.1Z following PLSV (Iwata
et al., 2008).
In the E-step, P(z|n, m, ) is updated as follows:
P(z|n, m, ) = Z

P(z|xn , )P(wnm |z )

z  =1 P(z

 |x

n , )P(wnm |z  )

.

In the M-step, by maximizing Q(|) w.r.t zw , the next estimate of word probability
zw is as follows:
 N  Mn
m=1 I(wnm = w)P(z|n, m, ) + 
n=1
zw = W 
,
N  Mn

m=1 I(wnm = w )P(z|n, m, ) + W
w =1
n=1
1106

fiSemantic Visualization with Neighborhood Graph Regularization

where I(.) is the indicator function. z and xn cannot be solved in a closed form, and are
estimated by maximizing Q(|) using quasi-Newton (Liu & Nocedal, 1989).
The computation fo the gradients of Q(|) w.r.t z and xn depend on the specic
kernel used (see Section 3.3).
 For the Gaussian kernel, we have the following gradients:
n


Q(|)  
=
P(z|xn , )  P(z|n, m, ) (z  xn )  z ,
z

N

Q(|)
=
xn

M

n=1 m=1
Mn 
Z




m=1 z=1


R(|)
.
P(z|xn , )  P(z|n, m, ) (xn  z )  xn +  
xn

 For the Student-t kernel, we have the following gradients:

N Mn 
2 P(z|xn , )  P(z|n, m, ) (z  xn )
Q(|)  
=
 z ,
z
1 + ||xn  z ||2
n=1 m=1


Mn 
Z

2 P(z|xn , )  P(z|n, m, ) (xn  z )
Q(|)
R(|)
=
 xn +  
.
2
xn
1 + ||xn  z ||
xn
m=1 z=1

The gradient of R(|) w.r.t. xn is computed depending on the form of the regularization function R(|). When we use the proposed regularization function R (|)
described in Section 4.1.1, we have the following gradient:
R (|)
R(|)
=
xn
xn
 


(xn  xj )
1  
=
4nj (xn  xj ) 
4(1  nj )
.
2
2
(F(n , j ) + 1)
j=1;j=n

j=1;j=n

As mentioned earlier, there is an eciency advantage to regularizing on the visualization space. R(|) does not contain the variable z if we do regularization on visualization
is O(N 2 ). In contrast, if we do regularizaspace. The complexity of computing all R(|)
xn
tion on topic space, we have to take the gradient of R(|) w.r.t to z . That contributes
. Therefore, regularizatowards a greater complexity of O(Z 2  N 2 ) to compute all R(|)
z
tion on topic space would run much slower than on visualization space.

6. Experiments
The main objective of our experiments is to evaluate the eectiveness of neighborhood regularization for semantic visualization model. After describing the experimental setup, we
rst examine the dierent design choices of the model relating to kernel, graph construction, and regularization function. Thereafter, we compare Semafore against the baseline
methods that also aim to address both visualization and topic modeling, quantitatively and
qualitatively, rst in terms of visualization and then in terms of topic modeling.
1107

fiLe & Lauw

6.1 Experimental Setup
In this section, we give a description of benchmark datasets as well as suitable metrics that
are used for evaluation.
6.1.1 Datasets
We use three real-life, publicly available datasets (Cardoso-Cachopo, 2007) for evaluation.
 20N ews contains newsgroup articles (in English) from 20 classes.
 Reuters8 contains newswire articles (in English) from 8 classes.
 Cade12 contains web pages (in Brazilian Portuguese) classied into 12 classes.
These are benchmark datasets used for document classication. While our task is fully
unsupervised, the ground-truth class labels are useful for an objective evaluation. We
create balanced classes by sampling fty documents from each class, following the practice
in PLSV (Iwata et al., 2008). This results in, for one sample, 1000 documents for 20N ews,
400 for Reuters8, and 600 for Cade12. The vocabulary sizes are 5.4K for 20N ews, 1.9K for
Reuters8, 7.6K for Cade12. As the algorithms are probabilistic, we generate ve samples
for each dataset. For each sample, we conduct ve independent runs. Therefore, the result
reported for each setting is the average over a total of 25 runs.
6.1.2 Metrics
For a suitable metric, we return to the fundamental principle that a good visualization
should preserve the relationship between documents (in high-dimensional space) in the
lower-dimensional visualization space. User studies, even when well-designed, could be
overly subjective and may not be repeatable across dierent users reliably. Therefore, for a
more objective evaluation, we rely on two types of quantitative analysis:
 Classication: This evaluation relies on the ground-truth class labels found in the
datasets. This is a well-established practice in many clustering and visualization
works in machine learning. The basis for this evaluation is the reasonable assumption
that documents of the same class are more related than documents of dierent classes.
Therefore a good visualization would place documents of the same class as neighbors
on the visualization.
For each document dn , we hide its true class cn , and generate a prediction for
its class Ct (n) by taking the majority class among its t-nearest neighbors, as determined by Euclidean distance on the visualization space. Classication accuracy
Classif ication Acc(t) is dened as the fraction of documents whose predicted class
Ct (n) matches the true class cn . More specically, we have:
N
1  
(Ct (n) = cn ),
Classif ication Acc(t) =
N
n=1

1108

fiSemantic Visualization with Neighborhood Graph Regularization

where  is the delta function that equals 1 if the prediction matches and 0 otherwise.
The same metric is used in PLSV (Iwata et al., 2008). While accuracy is computed
based on documents coordinates, the same trends will be produced if computed based
on topic distributions (due to their coupling through the kernels described in Section 3.3).
 Neighborhood Preservation: This evaluation does not rely on the ground-truth class
labels but on the local neighborhood structure in the input data. The assumption is
that a good visualization would be able to preserve the local structure in the input
data as much as possible. If two documents are neighbors in the input data, they
should still be neighbors in the visualization space.
For every document dn , we compute sets of t-nearest neighbors Yt (n) and Xt (n) of
document dn in the input data and the visualization respectively. The neighborhood
preservation accuracy P reservation Acc(t) is then dened as the average fraction of
the overlap size of Yt (n) and Xt (n) over the size of Yt (n) (i.e. t), where n = 1, . . . , N .
More specically, we have:

P reservation Acc(t) =

N
1  |Yt (n)  Xt (n)|
,
N
t
n=1

where |Yt (n)  Xt (n)| is the size of the overlap set Yt (n)  Xt (n).
A similar measure can be found in the literature (Akkucuk & Carroll, 2006), where
it is called the rate of agreement in local structure or agreement rate and is used
to measure how well the local structure is preserved between the input data and the
low dimensional embedding. It is also used for tuning the parameters of a non-linear
dimensionality reduction method (Chen & Buja, 2009).
In the subsequent experiments, we let t vary in the range [5, 50] with the step size
5 and report the accuracies. Since dierent methods may behave dierently at dierent
ts, choosing a specic t for comparison may be unfair for some methods. Moreover, a
method that consistently does well for dierent ts would also have a smoother local
structure. Therefore, when comparing various methods, we present the preservation or
classication accuracies averaged across t  [5, 50], denoted P reservation Acc(Avg) and
Classif ication Acc(Avg) respectively.
6.2 Parameter Study
In this section, we study the eects of graph parameters on our model. Specically, the
parameters concern the graph construction, including the number of neighbors k in k-NN
graph, the distance threshold  in -ball graph, and the number of minimum spanning trees
r in DMST. For each type of graph, we use the Simple Minded weight. For the following
gures, the regularization function is R with  = 10 and the number of topics Z = 20.
We use neighborhood preservation accuracy P reservation Acc(t) to show the eects of
graph parameters because this metric does not need ground-truth class labels, which are
not always available for tuning these graph parameters.
1109

fiLe & Lauw




















































W 

W 



W 



E















Z















Figure 4: Preservation accuracy of Semafore when using k-NN graph with dierent neighborhood size k for (a) 20N ews, (b) Reuters8, and (c) Cade12.











































W 

W 

W 













E















Z















Figure 5: Preservation accuracy of Semafore when using DMST graph with dierent number of minimum spanning trees r for (a) 20N ews, (b) Reuters8, and (c) Cade12.





































E











W 

W 



W 



















Z





















Figure 6: Preservation accuracy of Semafore when using -ball graph with dierent values
of distance threshold  for (a) 20N ews, (b) Reuters8, and (c) Cade12.

1110

fiSemantic Visualization with Neighborhood Graph Regularization

In Figure 4, we show the performance of our model with dierent neighborhood size k in
k-NN graph for dierent datasets. For every k, we vary t and plot the P reservation Acc(t).
Figure 4 shows that the optimum k for 20N ews, Reuters8, and Cade12 is 10, 10, and 5
respectively. We compute the average accuracy P reservation Acc(Avg) and it conrms
that the optima are indeed at those k values. From now on, we will use k=10 for 20N ews
and Reuters8, and k=5 for Cade12 when k-NN graph is used.
For DMST graph, we plot the P reservation Acc(t) for dierent number of minimum
spanning trees r with dierent datasets in Figure 5. It is dicult to see which r is the best in
the gure because the dierences between them are not much. The P reservation Acc(Avg)
is computed and it shows that for all three datasets, the optimum is about at r=5,6,7.
Subsequently, we will use r=6 for DMST graphs for all three datasets.
For -ball graph, in Figure 6 we plot the P reservation Acc(t) for dierent values of 
in the range [1.32, 1.40]. We choose that range because =1.32 and =1.40 roughly give an
average number of neighbors of 5 and 100 respectively. The P reservation Acc(Avg) shows
that the optimum  for 20N ews, Reuters8, and Cade12 is 1.34, 1.35, and 1.33 respectively.
6.3 Model Analysis
In this section, we study the various design choices involved in designing the Semafore
model, before nally concluding on the eventual synthesis of design choices to be used for
comparison against the baselines. To keep the discussion focused and organized, in each of
the following sub-section, we vary a single design choice, in order to isolate its eects. When
unvaried, the model has the following setup by default: the number of topics is Z = 20,
the graph construction method is k-NN, the graph weighting method is simple minded, the
RBF kernel is Gaussian, and the regularization function is R with  = 10.
6.3.1 Neighborhood Graph Construction
We investigate three graph construction methods: k-NN, -ball and DMST, which are representatives of neighborhood-based and minimum spanning tree-based methods respectively.
For each graph, its parameter is tuned as shown in Section 6.2. For the regularization
parameter , we try dierent settings of  on each dataset. It so happens that  = 10
performs the best for all the graph construction methods across the three datasets.
In Figure 7, we run Semafore with dierent types of graph on the three datasets and
report the P reservation Acc(Avg) at dierent number of topics Z. The results show that
dierent types of graph behave dierently with dierent datasets. In 20N ews, -ball and
DMST give our model highest performance. Since the dierence between the two are not
statistically signicant, we choose to use DMST for subsequent experiments on 20N ews.
For Reuters8, since -ball outperforms the others (signicant at 0.05 level), it is going to
be the default choice for subsequent experiments. For Cade12, the choice is DMST, which
is slightly better than k-NN (statistically signicant for Z = 10, 40, 50).
6.3.2 Neighborhood Graph Weighting
We now compare two variations of graph weighting methods, namely: Simple Minded and
Heat Kernel methods. In this experiment, we use k-NN graph with specic ks for dierent
1111

fiLe & Lauw







D^d





W 



W 

W 

EE











Ed
E











Ed



Z





Ed


Figure 7: The eects of dierent graph construction methods on our models performance.







,<t



W 



W 

W 

^Dt











Ed
E












Ed
Z





Ed


Figure 8: The eects of dierent graph weighting schemes on our models performance.
The graph used in this experiment is k-NN graph with specic ks for dierent
datasets as studied in Section 6.2.











Ed
E

^<



W 



W 

W 

'<











Ed
Z












Ed


Figure 9: The eects of Gaussian and Student-t RBF kernels on our models performance.
1112

fiSemantic Visualization with Neighborhood Graph Regularization

Regularization function
Graph construction
Graph weighting
RBF kernel

20N ews
R
DMST
Heat Kernel
Student-t

Reuters8
R
-ball
Heat Kernel
Student-t

Cade12
R
DMST
Simple Minded
Student-t

Table 2: Synthesized Model for Each Dataset.
datasets as studied in Section 6.2. The regularization parameter  is set to 10 after trying
various settings and picking the best one.
In Figure 8, we compare Simple Minded method and Heat Kernel method to see their
inuences on our model at dierent number of topics Z. We observe that Heat Kernel
is signicantly and consistently better than Simple Minded method across all the cases
in 20N ews and Reuters8. The dierence is statistically signicant at 0.01 level. One
explanation is that Heat Kernel assigns smoother weights to the graph edges, and thus is
more robust than Simple Minded. For Cade12, Simple Minded is slightly better, though
the dierences are statistically signicant at 0.05 level only for Z = 40. Subsequently, we
will use Heat Kernel for 20N ews and Reuters8, and Simple Minded for Cade12 as part of
the nal synthesis.
6.3.3 RBF Kernel
As described in Section 3.3, we express topic distributions as a function of visualization
coordinates using RBF network as an abstraction. In this section, we show how dierent
RBF kernels aect our models performance. The two kernels we are exploring are Gaussian
(Equation 3) and Student-t (Equation 4). We tune the regularization term  for each kernel
and see that the best one for the two kernels are  = 10.
Figure 9 shows the results for dierent number of topics Z. Student-t kernel has a slight
edge over Gaussian kernel consistently across dierent number of topics. The dierence is
small, but is statistically signicant (at 0.05 level) in a majority of the cases (for 20N ews
at Z = 10, 20, 30, 50, for Reuters8 at Z = 30, and for Cade12 at Z = 10, 30, 50). The slight
improvement could be a sign that crowding problem does exist in the model. Student-t
kernel would be even more useful when there is more extreme crowding issues, such as
when the number of documents to be visualized is even larger. Subsequently, due to its
slight edge, we will use Student-t as part of the nal synthesis. As we will see shortly, using
Student-t within the synthesized model results in a signicant improvement overall.
6.3.4 Synthesised Semafore Model
Based on the model analysis in the preceding paragraphs, we combine the design choices
into a nal synthesis model called Semafore. The synthesized model is slightly dierent
for dierent datasets, as listed in Table 2.
We now conduct another set of experiments to verify that those synthesized models
would produce a noticeable improvement over the earlier version (kNN + Simple Minded
+ Gaussian Kernel) that appeared in our earlier work (Le & Lauw, 2014b), underlining
1113

fiLe & Lauw

EE^D'<
,<^<













Ed










E





Ed
Z

EE^D'<
D^d^D^<

W


W


W


EE^D'<
D^d,<^<













Ed


Figure 10: Our synthesized models with dierent properties compared to the earlier version
(kNN + Simple Minded + Gaussian Kernel) that appeared in our earlier work
(Le & Lauw, 2014b).

the utility of the subsequent enhancements. Figure 10 shows that this is indeed the case.
Based on the standard deviations shown in the gures, the improvements are very clear
in 20N ews and Reuters8 but not so clear in Cade12. Paired samples t-test indicate that
the improvement is signicant at 0.05 level or lower in all cases, except for the cases where
Z = 10, 20 in Cade12. We will use these synthesized models in the comparisons against the
baseline methods in the following section.
6.4 Comparison of Visualizations
We now compare our proposed model with several baselines. First, we outline the set of
comparative methods. Thereafter, we discuss quantitative evaluation (in terms of accuracy),
as well as qualitative evaluation (in terms of example visualizations). Finally, we will show
that the gains in visualization quality does not come at the expense of topic modeling.
As semantic visualization seeks to ensure consistency between topic model and visualization, the comparison focuses on methods producing both topics and visualization coordinates
which are listed in Table 3.
 Semafore is our proposed method that incorporates neighborhood structure into
semantic visualization.
 PLSV (Iwata et al., 2008) is the state-of-the-art, representing the joint approach
without neighborhood structure preservation.
 PE (LDA) represents the pipeline approach involving topic modeling with LDA (Blei
et al., 2003), followed by visualizing documents topic distributions with PE (Iwata
et al., 2007). This pipeline is better than the LDA/MDS that appeared in our earlier
work (Le & Lauw, 2014b). There are other pipeline methods, shown inferior to PLSV
(Iwata et al., 2008), which are not reproduced here to avoid duplication.
1114

fiSemantic Visualization with Neighborhood Graph Regularization

Visualization

Topic model

Joint model

Neighborhood

Semafore
PLSV
PE (LDA)
t-SNE (LDA)
Table 3: Comparative Methods.
 t-SNE (LDA) is another pipeline approach that rst uses LDA (Blei et al., 2003) to
learn topic model and then use t-SNE (Van der Maaten & Hinton, 2008) to visualize
documents topic distributions.
For completeness, we also conduct experiments for comparing our method with t-SNE
and Laplacian EigenMaps (LE) (Belkin & Niyogi, 2003) (direct visualization, without topic
modeling). To keep the discussion focused, we show them in Appendix B, as we do not
consider t-SNE and LE as comparative baselines because these two methods only model
visualization, but not topics.
6.4.1 Accuracy
In this section, we compare our model with several baselines in terms of classication
accuracy (Figure 11) and neighborhood preservation accuracy (Figure 12). In the two
gures, only the standard deviations for Semafore are shown.
Classcation Accuracy. Figure 11(a), 11(c) and 11(e) show the Classf ication Acc(t)
at dierent ts for Z = 20 for 20N ews, Reuters8, and Cade12 respectively. At any t, the
comparison shows outperformance by Semafore over the baselines consistently. All four
methods show the same behavior that their performances decrease when t increases. As t
increases, they may lose accuracy in predicting labels for documents near to the border of
each cluster.
Now, we vary the number of topics Z. In Figure 11(b), we show the performance in
Classf ication Acc(Avg) on 20N ews. Figure 11(d) and 11(f) show the same for Reuters8
and Cade12 respectively. From these gures, we draw the following observations about the
comparative methods:
 Semafore performs the best on all datasets across various numbers of topics (Z).
Semafore beats PLSV by 25% to 51% on 20N ews, by 613% on Reuters8, and by
2232% on Cade12. These margins of performance with respect to PLSV are statistically signicant at 0.01 signicant level or lower in all cases. This eectively showcases
the utility of neighborhood regularization in enhancing the quality of visualization.
By preserving local consistency, Semafore achieves a good accuracy even at small
number of topics (e.g., 10).
 PLSV performs better than PE (LDA) and t-SNE (LDA), which shows that there
is utility to having a joint, instead of separate, modeling of topics and visualization.
PE (LDA) and t-SNE (LDA) are worse than PLSV because it embeds documents by
using two-step reductions that optimize separately two dierent objective functions.
1115

fiLe & Lauw

Therefore, the errors from the previous step may propagate to the next, without an
opportunity for correction. This may cause distortions in the visualization.
 In some cases, PLSV, PE (LDA) and t-SNE (LDA) tend to have decreasing accuracies when the number of topics increases. This may be because when number of topic
increases, the topic distributions and the word probabilities may overt the data and
thus the accuracy is reduced. In contrast, Semafore shows a quite stable performance across dierent numbers of topics. This may be explained by the utility of
neighborhood regularization, which helps to prevent overtting when the number of
topics increases.
Neighborhood Preservation Accuracy. While having better classication accuracy,
Semafore also preserves well the local structure of the input data in the visualization space.
The P reservation Acc(t) results in Figure 12(a), 12(c) and 12(e) show that Semafore is
consistently better than the other baselines in terms of neighborhood preservation across
dierent ts and dierent datasets. In Figure 12(b), 12(d) and 12(f), we vary the number
of topics Z and report the P reservation Acc(Avg) results. Semafore beats PLSV by
41% to 76% on 20N ews, by 2436% on Reuters8, and by 2945% on Cade12 in terms
of neighborhood preservation accuracy. The improvements of Semafore over PLSV are
statistically signicant at 0.01 signicant level or lower in all cases.
The above accuracy results are based on visualization coordinates. We have also computed accuracies based on topic distributions, which have similar trends.
6.4.2 Visualizations
To provide an intuitive appreciation, we briey describe a qualitative comparison of visualizations. For each method on each dataset, a visualization is shown as a scatterplot (best
seen in color). Each document has a coordinate, and is assigned a shape and color based
on its class. Each topic also has a coordinate, drawn as a black, hollow circle. A legend is
provided, mapping each symbol to the corresponding class label.
Note that this is an illustrative, rather than a comparative discussion, as an objective
evaluation should not rely on eyeballing alone. However, as we have shown the quantitative
results in the preceding section, in this section, we focus on the qualitative study of the
output visualizations.
20News. Figure 13 shows a visualization of 20N ews dataset. Semafores Figure 13(a)
shows that the dierent classes are well separated. There are distinct clusters of blue squares
and purple diamonds at the top for hockey and baseball classes respectively, clusters of
orange triangles and pink asterisks at the bottom for cryptography and medicine, etc.
Beyond individual classes, the visualization also places related classes nearby. Computerrelated classes are found on the lower left. Politics and religion are on the lower right.
Comparatively, Figure 13(b) by PLSV shows crowding at the center. For instance,
motorcycle (green dashes) and autos (red dashes) are mixed at the center without a good
separation. Figure 13(c) by PE (LDA) is worse. PE (LDA) does not give good separation
for not similar classes. It mixes autos (red dashes) and space (green circles) together at
the center. Medicine (pink asterisks) is also mixed with other classes in PE (LDA) while
Semafore and PLSV give a good separation for it. Figure 13(d) is visualization by tSNE (LDA). Although t-SNE (LDA) can separate well hockey (blue squares) and baseball
1116

fiSemantic Visualization with Neighborhood Graph Regularization

W>^s


















W>




^









Es





W>











Zs

Zs







^





W>^s




















^E>




Ed



 

W>^s






Ed
Es



W>




^

^E>


s 

^E>











Ed
s

Figure 11: Classication Accuracy Comparison.

1117





fiLe & Lauw

W>^s


















W>
W

W

^









Es

W

W>








Zs

Zs







^





W>^s



















^E>




Ed



W

W>^s











Ed
Es



W>
W

W

^

^E>


s 

^E>










Ed
s

Figure 12: Preservation Accuracy Comparison.

1118





fiSemantic Visualization with Neighborhood Graph Regularization

(purple diamonds) classes, it is not able to detect their semantic similarities (as baseball
and hockey are both about sports). In addition, it still mixes documents of dierent classes
together at the center and on the upper right.
Reuters8. Figure 14 shows the visualization outputs for Reuters8 dataset. Semafore
in Figure 14(a) is better at separating the eight classes into distinct clusters. In an anticlockwise direction from the top, we have navy blue diamonds (money-fx ), red dashes
(interest), red squares (crude), light blue pluses (earn), green triangles (acq), purple crosses
(ship), blue asterisks (grain), and nally orange circles (trade).
In comparison, PLSV in Figure 14(b) shows that several classes are intermixed at the
center, including red dashes (interest), orange circles (trade), and navy blue diamonds
(money-fx ). PE (LDA) in Figure 14(c) is also worse when it mixes dierentiated classes
such as red dashes (interest) and navy blue diamonds (money-fx ) together. t-SNE (LDA)
in Figure 14(d) seems have better cluster separation but still mix documents with dierent
classes together such as red squares (crude) and green triangles (acq) on the upper right.
Green triangles (acq) also mix with light blue pluses (earn) on the left in the visualization
by t-SNE (LDA).
Cade12. Figure 15 shows the visualization outputs for Cade12. This is the most
challenging dataset. Even so, Semafore in Figure 15(a) still achieves a better separation
between the classes, as compared to PLSV in Figure 15(b). Particularly, Semafore gives
better separation for esportes (green triangles) as well as compras-on-line (orange circles)
than PLSV and PE (LDA). t-SNE (LDA) shows quite good clusters for esportes (green
triangles) as well as compras-on-line (orange circles) but it also merges many dierent
classes together as in the clusters on the right and on the upper right.
6.5 Comparison of Topic Models
One question is whether Semafores gain in visualization quality over the closest baseline
PLSV is at the expense of the quality of its topic model. To investigate this, we will
compare the topic models of Semafore and PLSV, which share a core generative process.
For parity, in this comparison, we only include the joint models, whereby the visualization
coordinates aect the topic models as well.
The metric we use to measure the quality of topic models is pairwise mutual information
or PMI. It measures topic interpretability, based on coocurrence frequencies of the top words
in each topic in a large external corpus. Although other metrics such as perplexity or heldout likelihood can show the generalization ability of a learned topic model on unseen test
data, these traditional metrics do not capture whether topics are coherent (Chang, Gerrish,
Wang, Boyd-Graber, & Blei, 2009). Therefore, in this comparison, we rely on PMI, which
can measure the quality of topic words in terms of their interpretability to a human. To
human subjects, interpretability is closely related to coherence (Newman, Lau, Grieser, &
Baldwin, 2010), i.e., how much the top keywords in each topic are associated with each
other. After an extensive study of evaluation methods for coherence, Newman et al. (2010)
identify Pointwise Mutual Information (PMI) as the best measure, in terms of having the
greatest correlation with human judgments.
PMI is based on term cooccurrences. For a pair of words wi and wj , PMI is dened
p(wi ,wj )
as log p(wi )p(w
. For a topic, we average the pairwise PMIs among the top 10 words of
j)
1119

fiLe & Lauw

^

W>^s

W>

^E>
>











































Figure 13: Visualization of documents in 20N ews for number of topics Z = 20. Each point
represents a document and the shape and color represent document class. Each
topic is drawn as a black, hollow circle.
1120

fiSemantic Visualization with Neighborhood Graph Regularization

^

W>^s

W>

^E>
>



















Figure 14: Visualization of documents in Reuters8 for number of topics Z = 20. Each
point represents a document and the shape and color represent document class.
Each topic is drawn as a black, hollow circle.

1121

fiLe & Lauw

^

W>^s

W>

^E>
>



























Figure 15: Visualization of documents in Cade12 for number of topics Z = 20. Each point
represents a document and the shape and color represent document class. Each
topic is drawn as a black, hollow circle.

1122

fiSemantic Visualization with Neighborhood Graph Regularization

W>^s

^






WD/^

WD/^

^






W>^s









Ed








Ed



Z

E

Figure 16: Topic Interpretability of Semafore and PLSV in terms of PMI Score (higher
is better).

that topic. For a topic model, we average PMI across the topics. Intuitively, PMI is higher
(better), if each topic features words that are highly correlated with one another.
Key to PMI is the use of an external corpus to estimate p(wi , wj ) and p(wi ). Following
Newman et al. (2009), we use Google Web 1T 5-gram Version 1 (Brants & Franz, 2006),
a huge corpus of n-grams generated from 1 trillion word tokens. p(wi ) is estimated from
the frequencies of 1-grams. As recommended by Newman et al., p(wi , wj ) is estimated from
the frequencies of 5-grams. We obtain PMI for the English-based 20N ews and Reuters8,
but not for Cade12 because we do not possess a large-scale n-gram corpus specically for
Brazilian Portuguese.
In Figure 16, we plot the PMI score for various number of topics Z. Semafore performs
better than PLSV across most of the topics settings. In Figure 16(a) for 20News, except for
the case at Z = 10, all cases of Semafores outperformance are signicant at 0.05 level or
lower. In Figure 16(b) for Reuters8, all cases of Semafores outperformance are signicant
at 0.05 level or lower except for the Z = 30. These results show that Semafore improves
visualization while not sacricing the topic interpretability of learned topics.
For a greater appreciation of the quality of the output topic models, in Appendix C, we
show several examples of topic models for Z = 20, for both Semafore and PLSV, in terms
of the top keywords with the highest probabilities for each topic.

7. Conclusion
In this paper, we address the semantic visualization problem, which jointly conducts topic
modeling and visualization of documents. We propose a new framework to incorporate
neighborhood structure within a probabilistic semantic visualization model called Semafore.
The model is carefully designed to reect the context of semantic visualization, leading to
a number of design choices related to the RBF kernel for mapping topic and visualiza1123

fiLe & Lauw

tion spaces, the approximation of neighborhood graph through construction and weighting,
as well as the appropriate regularization functions and spaces. Experiments on real-life
datasets show that Semafore signicantly outperforms the baselines in terms of visualization quality and accuracy, while having a similar, if not slightly better topic model. This
provides evidence that neighborhood structure, together with joint modeling of topics and
visualization, is important for semantic visualization.

Appendix A. Balancing Contributions of Neighbors and Non-neighbors
to Regularization
As mentioned in Section 4.2, the balance between the contribution of neighbors R+ and
non-neighbors R to the neighborhood regularization R in Equation 8 may require careful
tuning of the graph parameters (i.e.,  or k). For example, in the case of using k-NN graph
and N total number of documents, we would have kN terms in the neighbor regularization
R+ , and (N k)N terms in the non-neighbor regularization R . Supposing that N increases
signicantly, there might be imbalance if k were to remain unchanged. Therefore, as N
changes, k should also be tuned accordingly to maintain this balance. For a simplistic
point, the ratio between kN and (N  k)N would remain roughly the same if both N and
k grow by similar factors. In practice, we recommend tuning k carefully.
We run additional experiments to validate the above argument on the 20N ews dataset.
Our basic point is that as N changes, k can be tuned to still show signicant improvement
due to the neighborhood graph regularization. The closest baseline is PLSV, both empirically in terms of classication accuracy, as well as conceptually as PLSV shares a similar
generative process but with a dierent kernel and without neighborhood regularization.
Hence, we compare the performance of our method Semafore (with k-NN graph, heat
kernel weighting, and Student-t kernel) to PLSV on various data sizes at Z = 20 topics.
 Figure 17(a) is for dataset of size N = 500, and Semafore runs with k = 10.
 Figure 17(b) is for dataset of size N = 1000, and Semafore runs with k = 10.
 Figure 17(c) is for dataset of size N = 5000, and Semafore runs with k = 50.
We note that there is a 10X dierence between the smallest and the largest datasets.
Yet the relative outperformance of Semafore over PLSV by around 15% to 20% is evident
across the three datasets. This supports the case that k can be tuned to produce a positive
eect using neighborhood graph regularization.

Appendix B. Additional Comparisons
As mentioned in Section 6.4, for completeness, we include here additional comparisons to
visualization methods that do not also aim at topic modeling. In particular, we include two
methods. First, we include t-SNE (Van der Maaten & Hinton, 2008), which is also used
in the composite t-SNE (LDA). Second, we include Laplacian EigenMaps (LE) (Belkin &
Niyogi, 2003), which takes as input the neighborhood graph. Figure 18 and Figure 19
show the classication accuracy and preservation accuracy of Semafore , t-SNE and LE
1124

fiSemantic Visualization with Neighborhood Graph Regularization










  



^



W>^s

^


W>^s




^















  












E

E

W>^s





  





E

Figure 17: Classication accuracy comparison on 20N ews with various data sizes (Z = 20).

^

^E







 


 





>













































E













Z



Figure 18: Classication accuracy comparison.

^

^E







W


W

W



>
















E
























Z

Figure 19: Preservation accuracy comparison.

1125










fiLe & Lauw

when varying t. Semafore outperforms LE in all cases. For t-SNE, Semafore outperforms t-SNE for Reuters8. However, for 20N ews and Cade12, it is more dicult to tell
whether Semafore or t-SNE is better. t-SNE tends to have decreasing accuracy as t increases. This is expected because t-SNE is known to focus on preserving the local structure
(Van der Maaten & Hinton, 2008). When t is small, we basically consider only the local
structure of the visualization. When t increases, we consider the more global structure of the
visualization and Semafore outperforms t-SNE signicantly. Overall, Semafore is more
stable than t-SNE as t changes, which indicates that Semafore tries to balance preserving
the local and global structure better than t-SNE. We emphasize that this comparison is for
information purpose only, as we do not regard t-SNE and LE as comparative baselines.

Appendix C. Topic Model Examples
We showcase the topic models derived by Semafore and PLSV. For 20N ews, Table 4 shows
the topics of Semafore, and Table 5 shows the topics of PLSV. For Reuters8, Table 6
shows the topics of Semafore, and Table 7 shows the topics of PLSV. For Cade12, Table 8
shows the topics of Semafore, and Table 9 shows the topics of PLSV.
For each method, we show the list of twenty topics. For each topic, we produce the
top ten words with the highest probabilities. As shown by the top words, the topics do
correspond strongly to some of the classes. For example, topic s19 in Table 4 for 20N ews is
about Christianity, which corresponds to the soc.religion.christian class. Topic s4 is about
cars and motorcycles, corresponding to rec.autos and rec.motorcycles. Topic s12 is probably
concerning the categories of rec.sport.baseball and/or rec.sport.hockey.
Overall, we observe that the quality of topic words are comparable across the comparative methods. Note that there is no direct correspondence between the topics of dierent
methods (e.g., the rst topic of Semafore may not correspond to the rst topic of PLSV).
Through manual inspection, we can see that there are some related topics, e.g., s4 and p6 ,
or s12 and p7 . However, the sets of topics and the set of keywords for each topic are not
identical. This is borne out in the slight dierence in terms of PMI scores.
This qualitative study helps to show that Semafore improves the visualization quality,
while still maintaining at least the same quality of topic words, if not better. This supports
the conclusion reached by the quantitative comparisons in the main manuscript.

1126

fiSemantic Visualization with Neighborhood Graph Regularization

Table 4: Semafores Topic Model for 20N ews (for 20 topics)
Topic ID
s0
s1
s2
s3
s4
s5
s6
s7
s8
s9
s10
s11
s12
s13
s14
s15
s16
s17
s18
s19

Top 10 Words
space, system, -rcb-, book, computer, university, list, post, price, science
article, year, good, write, guy, well, time, head, question, leave
gun, law, kuwait, people, death, fbus, article, control, weapon, child
window, le, program, widget, application, type, will, resource, call, function
car, bike, speed, engine, drive, lock, turn, mile, front, change
will, power, place, work, rate, write, sound, lead, good, interested
write, article, thing, time, people, better, start, problem, will, good
write, time, people, friend, pay, public, article, tax, opinion, money
people, claim, write, system, person, moral, evidence, objective, read, state
image, datum, graphic, send, le, format, package, software, mail, include
armenian, re, jew, child, kill, start, people, turkish, door, israel
system, board, will, datum, time, work, tape, test, copy, command
game, team, year, player, win, play, will, hit, season, hockey
will, post, space, good, time, include, cost, option, launch, people
drive, card, window, appear, disk, ram, driver, memory, work, color
mr., president, stephanopoulo, state, group, consider, party, question, issue, press
write, article, well, will, thing, work, point, include, time, help
key, article, chip, food, write, people, government, encryption, thing, algorithm
price, buy, apple, computer, dealer, t, model, problem, sell, monitor
god, jesus, will, christian, religion, faith, truth, bible, belief, church

Table 5: PLSVs Topic Model for 20N ews (for 20 topics)
Topic ID
p0
p1
p2
p3
p4
p5
p6
p7
p8
p9
p10
p11
p12
p13
p14
p15
p16
p17
p18
p19

Top 10 Words
write, people, christian, belief, time, faith, god, religion, life, will
god, will, jesus, kuwait, atheist, church, christian, man, religion, sin
armenian, appear, art, turkish, tartar, 1st, village, armenia, 1.40, genocide
will, key, write, time, article, government, system, thing, chip, hit
mr., stephanopoulo, president, will, party, state, door, time, meeting, open
write, re, article, gun, system, -rcb-, start, people, fbus, claim
car, will, bike, engine, drive, well, dealer, battery, change, front
game, win, year, will, team, play, season, good, goal, playo
player, team, write, hockey, game, fan, article, year, will, guy
space, system, datum, will, april, nasa, security, university, computer, list
graphic, image, le, ftp, send, format, package, system, datum, object
image, datum, program, window, version, le, software, tool, support, user
drive, jumper, master, ndet loop, slave, rate, gun, function, crime, set
window, le, card, will, program, color, driver, support, disk, bit
people, write, state, article, law, government, country, rights, jew, will
write, article, thing, people, good, will, time, lot, year, day
work, drive, tape, scsus, problem, simm, controller, write, memory, article
widget, -rcb-, window, -lcb-, application, resource, set, visual, type, le
price, will, write, system, computer, article, apple, chip, monitor, board
will, vote, comp, newsgroup, suit, problem, os2, sco, post, mail

1127

fiLe & Lauw

Table 6: Semafores Topic Model for Reuters8 (for 20 topics)
Topic ID
s0
s1
s2
s3
s4
s5
s6
s7
s8
s9
s10
s11
s12
s13
s14
s15
s16
s17
s18
s19

Top 10 Words
company, pipeline, raise, crude, march, spokesman, renery, capacity, corp, post
pct, bank, day, stg, today, reuter, money, market, mln, bill
oer, share, company, board, group, acquire, stock, dlr, acquisition, receive
exchange, currency, dollar, west, nance, baker, monetary, germany, continue, interest
share, reuter, dlr, mln, buy, company, corp, pay, stock, group
price, opec, market, bpd, ocial, february, month, output, saudus, january
rate, bank, pct, cut, fund, prime, point, reserve, issue, lower
billion, foreign, import, increase, dlr, trade, economic, export, will, country
bank, billion, market, government, fall, stock, economy, rise, surplus, decit
will, company, sell, pct, vessel, operation, week, billion, shipping, unit
strike, port, union, spokesman, cargo, employer, worker, sector, redundancy, court
oil, export, dlr, industry, year, pct, future, company, report, price
reuter, pct, report, national, week, brazil, today, increase, pay, april
trade, japan, japanese, reagan, state, tari, unite, market, washington, ocial
grain, mln, soviet, crop, tonne, year, usda, production, fall, analyst
trade, talk, gulf, gatt, bill, yeutter, round, reuter, call, negotiation
certicate, reuter, cost, government, program, agreement, agriculture, will, study, loan
year, ocial, import, will, state, price, government, china, land, rise
mln, ct, loss, net, shr, dlr, prot, qtr, reuter, year
oil, mln, will, barrel, dlr, crude, source, level, petroleum, day

Table 7: PLSVs Topic Model for Reuters8 (for 20 topics)
Topic ID
p0
p1
p2
p3
p4
p5
p6
p7
p8
p9
p10
p11
p12
p13
p14
p15
p16
p17
p18
p19

Top 10 Words
will, oil, company, reuter, industry, canada, price, shell, raise, sell
rate, currency, dollar, exchange, baker, west, will, bank, reuter, treasury
bank, pct, day, import, year, rate, export, february, expect, reuter
share, company, corp, oer, stock, board, will, reuter, dlr, buy
rate, bank, pct, prime, cut, point, interest, market, lower, savings
market, bank, stock, price, japan, ministry, rise, ocial, gulf, bond
reuter, pct, week, report, year, march, mark, american, commission, gure
mln, ct, loss, net, dlr, shr, year, prot, qtr, reuter
mln, pct, billion, stg, dlr, reuter, market, january, revise, rise
billion, dlr, rate, market, surplus, currency, reserve, trading, dollar, foreign
oil, opec, price, bpd, pipeline, mln, crude, ocial, dlr, output
crude, dlr, barrel, corp, capacity, renery, oil, company, oer, group
reuter, ocial, state, cut, gulf, government, today, action, force, tell
oil, government, indonesium, price, foreign, bank, billion, reserve, company, industry
certicate, company, mln, year, grain, cooperative, program, dlr, government, cost
year, trade, agriculture, reuter, grain, agreement, gatt, yeutter, nancial, agricultural
strike, port, union, spokesman, employer, brazil, cargo, worker, redundancy, sector
trade, japan, japanese, reagan, tari, unite, washington, state, nakasone, semiconductor
grain, mln, crop, tonne, soviet, year, ocial, china, pct, oer
trade, country, minister, talk, state, meeting, economic, exchange, issue, baldrige

1128

fiSemantic Visualization with Neighborhood Graph Regularization

Table 8: Semafores Topic Model for Cade12 (for 20 topics)
Topic ID
s0
s1
s2
s3
s4
s5
s6
s7
s8
s9
s10
s11
s12
s13
s14
s15
s16
s17
s18
s19

Top 10 Words
sp, aulas, tecnologia, rj, sao, area, janeiro, particulares, areas, sica
terra, jun, gif, busca, virtual, brasil, forum, tempo, noticias, revistas
trabalho, seguranca, saude, medicina, ocupacional, prevencao, ppra, pcmso, imagem, imagens
peixes, cade, lazer, pesca, agua, rio, praia, hotel, sao, doce
agar, vida, personal, sica, base, tratamento, tem, pode, sistema, trainer
sao, br, rio, sul, criancas, www, escola, mail, http, atendimento
links, page, home, fotos, pagina, dicas, download, tenis, informacoes, jogos
internet, informatica, acesso, mg, br, servicos, provedor, mail, revista, horizonte
servicos, sao, paulo, entregas, entrega, sp, cesta, express, empresa, servico
pesca, sp, grupo, brasil, eventos, video, mg, informacoes, turismo, danca
astronomia, pagina, jose, foi, bem, espaco, tem, veja, losoa, correio
mp, banda, musicas, rock, musica, page, letras, bandas, pagina, site
historia, cultura, mundo, site, page, brasil, informacoes, rs, livro, arte
noticias, jornal, cidade, sp, sao, regiao, demolay, ordem, rio, capitulo
empresas, informacoes, informacao, dados, atraves, textos, mail, equipe, unicamp, centro
engenharia, servicos, projetos, empresa, consultoria, quimica, instituto, pesquisa, rio, manutencao
site, informacoes, brasil, associacao, educacao, pagina, organizacao, centro, brasileira, direitos
software, web, empresa, sistemas, sistema, br, marketing, desenvolvimento, windows, dados
virtual, online, venda, produtos, cade, shopping, internet, loja, compras, cursos
futebol, informacoes, fotos, clube, historia, paulo, sao, quake, pagina, cade

Topic ID
p0
p1
p2
p3
p4
p5
p6
p7
p8
p9
p10
p11
p12
p13
p14
p15
p16
p17
p18
p19

Top 10 Words
engenharia, projetos, servicos, trabalho, empresa, consultoria, seguranca, sp, medicina, sao
sao, ong, rio, instituto, personal, educacao, organizacao, sp, paulo, ns
sao, br, desenvolvimento, sistema, tratamento, mail, sistemas, clientes, informacoes, empresa
aulas, formula, quimica, particulares, informacoes, matematica, pilotos, fotos, sica, site
jornal, tenis, noticias, esportes, sp, informacoes, sao, esporte, fotos, links
musica, page, rock, bandas, links, home, pagina, musicas, music, fotos
pesca, demolay, sp, peixes, sao, fotos, ordem, capitulo, paulo, jitsu
mp, musicas, nacionais, agar, internacionais, rock, formato, site, page, pagina
pesquisa, tecnologia, informacoes, cade, ciencia, geograa, pesquisas, area, instituto, pagina
site, pagina, internet, mail, clique, veja, br, pode, foi, links
astronomia, informacoes, cultura, site, pagina, brasil, home, page, fotos, historia
banda, fotos, rock, letras, page, musicas, pagina, site, home, mp
internet, provedor, acesso, mg, informatica, software, servicos, belo, horizonte, manutencao
futebol, clube, sao, paulo, campeonato, historia, informacoes, pagina, turismo, tricolor
noticias, terra, internet, brasil, informatica, online, jornal, virtual, servicos, busca
links, page, quake, home, pagina, fotos, dicas, mp, download, informacoes
grupo, banda, karate, pagina, page, informacoes, fotos, home, rio, historia
produtos, virtual, shopping, cade, venda, online, sao, rio, loja, compras
br, sao, informacoes, marketing, mail, empresa, internet, www, fax, site
vida, dia, sao, foi, terra, panico, jose, tem, planetas, grande

Table 9: PLSVs Topic Model for Cade12 (for 20 topics)

1129

fiLe & Lauw

References
Akkucuk, U., & Carroll, J. D. (2006). PARAMAP vs. Isomap: a comparison of two nonlinear
mapping algorithms. Journal of Classication, 23 (2), 221254.
Bai, L., Guo, J., Lan, Y., & Cheng, X. (2014). Local Linear Matrix Factorization for
Document Modeling. In Advances in Information Retrieval, pp. 398411. Springer.
Belkin, M., & Niyogi, P. (2001). Laplacian eigenmaps and spectral techniques for embedding and clustering. In Advances in Neural Information Processing Systems (NIPS),
Vol. 14, pp. 585591.
Belkin, M., & Niyogi, P. (2003). Laplacian eigenmaps for dimensionality reduction and data
representation. Neural Computation, 15 (6), 13731396.
Belkin, M., Niyogi, P., & Sindhwani, V. (2006). Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. Journal of Machine Learning
Research (JMLR), 7, 23992434.
Bishop, C. M. (1995). Neural Networks for Pattern Recognition. Oxford University Press.
Bishop, C. M., Svensen, M., & Williams, C. K. (1998). GTM: The generative topographic
mapping. Neural Computation, 10 (1), 215234.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet Allocation. Journal of
Machine Learning Research (JMLR), 3, 9931022.
Brants, T., & Franz, A. (2006). Web 1T 5-gram Version 1. Linguistic Data Consortium,
Philadelphia.
Buhmann, M. D. (2000). Radial basis functions. Acta Numerica 2000, 9.
Cai, D., Mei, Q., Han, J., & Zhai, C. (2008). Modeling hidden topics on document manifold.
In Proceedings of the ACM Conference on Information and Knowledge Management
(CIKM).
Cai, D., Wang, X., & He, X. (2009). Probabilistic dyadic data analysis with local and global
consistency. In Proceedings of the International Conference on Machine Learning
(ICML).
Cardoso-Cachopo, A. (2007). Improving Methods for Single-label Text Categorization. PhD
Thesis, Instituto Superior Tecnico, Universidade Tecnica de Lisboa.
Carey, C., & Mahadevan, S. (2014). Manifold Spanning Graphs. In Twenty-Eighth AAAI
Conference on Articial Intelligence.
Chaney, A. J.-B., & Blei, D. M. (2012). Visualizing Topic Models. In Proceedings of the
International AAAI Conference on Web and Social Media (ICWSM).
Chang, J., Gerrish, S., Wang, C., Boyd-Graber, J. L., & Blei, D. M. (2009). Reading
tea leaves: How humans interpret topic models. In Advances in Neural Information
Processing Systems, pp. 288296.
Chen, L., & Buja, A. (2009). Local multidimensional scaling for nonlinear dimension reduction, graph drawing, and proximity analysis. Journal of the American Statistical
Association, 104 (485), 209219.
1130

fiSemantic Visualization with Neighborhood Graph Regularization

Chi, E. H.-h. (2000). A taxonomy of visualization techniques using the data state reference model. In Proceedings of the IEEE Symposium on Information Visualization
(InfoVis), pp. 6975.
Choo, J., Lee, C., Reddy, C. K., & Park, H. (2013). UTOPIAN: User-driven topic modeling based on interactive nonnegative matrix factorization. IEEE Transactions on
Visualization and Computer Graphics, 19 (12), 19922001.
Chuang, J., Manning, C. D., & Heer, J. (2012). Termite: visualization techniques for assessing textual topic models. In Proceedings of the International Working Conference
on Advanced Visual Interfaces (AVI), pp. 7477.
Coifman, R. R., & Lafon, S. (2006). Diusion maps. Applied and Computational Harmonic
Analysis, 21 (1), 5  30.
Comon, P. (1994). Independent component analysis, a new concept?. Signal Processing,
36 (3), 287314.
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39 (1),
138.
Dumais, S., Furnas, G., Landauer, T., Deerwester, S., Deerwester, S., et al. (1995). Latent
semantic indexing. In Proceedings of the Text Retrieval Conference.
Fisher, R. A. (1936). The use of multiple measurements in taxonomic problems. Annals of
Eugenics, 7 (2), 179188.
Gabrilovich, E., & Markovitch, S. (2009). Wikipedia-based semantic interpretation for
natural language processing. Journal of Articial Intelligence Research (JAIR), 34 (2),
443.
Golub, G. H., & Van Loan, C. F. (2012). Matrix Computations, Vol. 3. JHU Press.
Gretarsson, B., Odonovan, J., Bostandjiev, S., Hollerer, T., Asuncion, A., Newman, D., &
Smyth, P. (2012). TopicNets: Visual analysis of large text corpora with topic modeling.
ACM Transactions on Intelligent Systems and Technology (TIST), 3 (2), 23.
Hein, M., Audibert, J.-y., & Luxburg, U. V. (2007). Graph Laplacians and their Convergence
on Random Neighborhood Graphs. In Journal of Machine Learning Research, pp.
13251368.
Hinton, G. E., & Roweis, S. T. (2002). Stochastic neighbor embedding. In Advances in
Neural Information Processing Systems (NIPS), pp. 833840.
Hofmann, T. (1999). Probabilistic latent semantic indexing. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information
Retrieval (SIGIR), pp. 5057.
Hu, Y., Boyd-Graber, J., Satino, B., & Smith, A. (2014). Interactive topic modeling.
Machine Learning, 95 (3), 423469.
Huh, S., & Fienberg, S. E. (2012). Discriminative topic modeling based on manifold learning.
ACM Transactions on Knowledge Discovery from Data (TKDD), 5 (4), 20.
1131

fiLe & Lauw

Iwata, T., Saito, K., Ueda, N., Stromsten, S., Griths, T. L., & Tenenbaum, J. B. (2007).
Parametric embedding for class visualization. Neural Computation, 19 (9), 25362556.
Iwata, T., Yamada, T., & Ueda, N. (2008). Probabilistic latent semantic visualization: topic
model for visualizing documents. In Proceedings of the ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining (KDD), pp. 363371.
Jebara, T., Wang, J., & Chang, S.-F. (2009). Graph construction and b-matching for semisupervised learning. In Proceedings of the 26th Annual International Conference on
Machine Learning, pp. 441448. ACM.
Jollie, I. (2005). Principal Component Analysis. Wiley Online Library.
Kim, M., & Torre, F. (2010). Local minima embedding. In Proceedings of the International
Conference on Machine Learning (ICML), pp. 527534.
Kohonen, T. (1990). The self-organizing map. Proceedings of the IEEE, 78 (9), 14641480.
Kruskal, J. B. (1964). Multidimensional scaling by optimizing goodness of t to a nonmetric
hypothesis. Psychometrika, 29 (1), 127.
Laerty, J. D., & Wasserman, L. (2007). Statistical Analysis of Semi-Supervised Regression.
In Advances in Neural Information Processing Systems (NIPS), pp. 801808.
Le, T., & Lauw, H. W. (2014a). Semantic visualization for spherical representation. In
Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, pp. 10071016. ACM.
Le, T. M., & Lauw, H. W. (2014b). Manifold learning for jointly modeling topic and
visualization. In Proceedings of the AAAI Conference on Articial Intelligence.
Liu, D. C., & Nocedal, J. (1989). On the limited memory BFGS method for large scale
optimization. Mathematical Programming, 45, 503528.
Manning, C. D., Raghavan, P., Schutze, H., et al. (2008). Introduction to Information
Retrieval, Vol. 1. Cambridge University Press Cambridge.
McCallum, A., Wang, X., & Corrada-Emmanuel, A. (2007). Topic and role discovery in
social networks with experiments on enron and academic email.. Journal of Articial
Intelligence Research (JAIR), 30, 249272.
Millar, J. R., Peterson, G. L., & Mendenhall, M. J. (2009). Document Clustering and
Visualization with Latent Dirichlet Allocation and Self-Organizing Maps. In FLAIRS
Conference, Vol. 21, pp. 6974.
Newman, D., Karimi, S., & Cavedon, L. (2009). External evaluation of topic models. In
Australasian Document Computing Symposium (ADCS).
Newman, D., Lau, J. H., Grieser, K., & Baldwin, T. (2010). Automatic evaluation of topic
coherence. In Human Language Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Computational Linguistics, pp. 100
108.
Park, J., & Sandberg, I. W. (1991). Universal approximation using radial-basis-function
networks. Neural Computation, 3 (2), 246257.
1132

fiSemantic Visualization with Neighborhood Graph Regularization

Ponzetto, S. P., & Strube, M. (2007). Knowledge derived from Wikipedia for computing
semantic relatedness.. Journal of Articial Intelligence Research (JAIR), 30, 181212.
Reisinger, J., Waters, A., Silverthorn, B., & Mooney, R. J. (2010). Spherical topic models.
In Proceedings of the International Conference on Machine Learning (ICML), pp.
903910.
Roweis, S. T., & Saul, L. K. (2000). Nonlinear dimensionality reduction by locally linear
embedding. Science, 290 (5500), 23232326.
Shaw, B., & Jebara, T. (2007). Minimum volume embedding. In Proceedings of the International Conference on Articial Intelligence and Statistics (AISTATS), pp. 460467.
Shaw, B., & Jebara, T. (2009). Structure preserving embedding. In Proceedings of the
International Conference on Machine Learning (ICML), pp. 937944. ACM.
Tenenbaum, J. B., De Silva, V., & Langford, J. C. (2000). A global geometric framework
for nonlinear dimensionality reduction. Science, 290 (5500), 23192323.
Ting, D., Huang, L., & Jordan, M. I. (2010). An Analysis of the Convergence of Graph Laplacians. In Proceedings of the International Conference on Machine Learning (ICML).
Turney, P. D., Pantel, P., et al. (2010). From frequency to meaning: Vector space models
of semantics. Journal of Articial Intelligence Research (JAIR), 37 (1), 141188.
Van der Maaten, L., & Hinton, G. (2008). Visualizing data using t-SNE. Journal of Machine
Learning Research (JMLR), 9 (2579-2605), 85.
Wei, F., Liu, S., Song, Y., Pan, S., Zhou, M. X., Qian, W., Shi, L., Tan, L., & Zhang,
Q. (2010). Tiara: a visual exploratory text analytic system. In Proceedings of the
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
(KDD), pp. 153162.
Wu, H., Bu, J., Chen, C., Zhu, J., Zhang, L., Liu, H., Wang, C., & Cai, D. (2012). Locally
discriminative topic modeling. Pattern Recognition, 45 (1), 617625.
Zemel, R. S., & Carreira-Perpinan, M. A. (2004). Proximity graphs for clustering and
manifold learning. In Advances in Neural Information Processing Systems (NIPS),
pp. 225232.
Zhou, D., Bousquet, O., Lal, T. N., Weston, J., & Scholkopf, B. (2004). Learning with local
and global consistency. Advances in Neural Information Processing Systems (NIPS),
16 (16).
Zhu, X., Ghahramani, Z., Laerty, J., et al. (2003). Semi-supervised learning using Gaussian
elds and harmonic functions. In Proceedings of the International Conference on
Machine Learning (ICML), Vol. 3, pp. 912919.

1133

fiJournal of Artificial Intelligence Research 55 (2016) 835-887

Submitted 10/2015; published 04/2016

Parallel Model-Based Diagnosis on Multi-Core Computers
Dietmar Jannach
Thomas Schmitz

dietmar.jannach@tu-dortmund.de
thomas.schmitz@tu-dortmund.de

TU Dortmund, Germany

Kostyantyn Shchekotykhin

kostyantyn.shchekotykhin@aau.at

Alpen-Adria University Klagenfurt, Austria

Abstract
Model-Based Diagnosis (MBD) is a principled and domain-independent way of analyzing why a system under examination is not behaving as expected. Given an abstract
description (model) of the systems components and their behavior when functioning normally, MBD techniques rely on observations about the actual system behavior to reason
about possible causes when there are discrepancies between the expected and observed behavior. Due to its generality, MBD has been successfully applied in a variety of application
domains over the last decades.
In many application domains of MBD, testing different hypotheses about the reasons
for a failure can be computationally costly, e.g., because complex simulations of the system behavior have to be performed. In this work, we therefore propose different schemes
of parallelizing the diagnostic reasoning process in order to better exploit the capabilities
of modern multi-core computers. We propose and systematically evaluate parallelization
schemes for Reiters hitting set algorithm for finding all or a few leading minimal diagnoses using two different conflict detection techniques. Furthermore, we perform initial
experiments for a basic depth-first search strategy to assess the potential of parallelization
when searching for one single diagnosis. Finally, we test the effects of parallelizing direct
encodings of the diagnosis problem in a constraint solver.

1. Introduction
Model-Based Diagnosis (MBD) is a subfield of Artificial Intelligence that is concerned with
the automated determination of possible causes when a system is not behaving as expected.
In the early days of MBD, the diagnosed systems were typically hardware artifacts like
electronic circuits. In contrast to earlier heuristic diagnosis approaches which connected
symptoms with possible causes, e.g., through expert rules (Buchanan & Shortliffe, 1984),
MBD techniques rely on an abstract and explicit representation (model) of the examined
system. Such models contain both information about the systems structure, i.e., the list of
components and how they are connected, as well as information about the behavior of the
components when functioning correctly. When such a model is available, the expected behavior (outputs) of a system given some inputs can thus be calculated. A diagnosis problem
arises whenever the expected behavior conflicts with the observed system behavior. MBD
techniques at their core construct and test hypotheses about the faultiness of individual
components of the system. Finally, a diagnosis is considered as a subset of the components
that, if assumed to be faulty, can explain the observed behavior of the system.
Reiter (1987) suggests a formal logical characterization of the diagnosis problem from
first principles and proposed a breadth-first tree construction algorithm to determine all
c
2016
AI Access Foundation. All rights reserved.

fiJannach, Schmitz, & Shchekotykhin

diagnoses for a given problem. Due to the generality of the used knowledge-representation
language and the suggested algorithms for the computation of diagnoses, MBD has been
later on applied to a variety of application problems other than hardware. The application
fields of MBD, for example, include the diagnosis of knowledge bases and ontologies, process
specifications, feature models, user interface specifications and user preference statements,
and various types of software artifacts including functional and logic programs as well as
VHDL, Java or spreadsheet programs (Felfernig, Friedrich, Jannach, & Stumptner, 2004;
Mateis, Stumptner, Wieland, & Wotawa, 2000; Jannach & Schmitz, 2014; Wotawa, 2001b;
Felfernig, Friedrich, Isak, Shchekotykhin, Teppan, & Jannach, 2009; Console, Friedrich,
& Dupre, 1993; Friedrich & Shchekotykhin, 2005; Stumptner & Wotawa, 1999; Friedrich,
Stumptner, & Wotawa, 1999; White, Benavides, Schmidt, Trinidad, Dougherty, & Cortes,
2010; Friedrich, Fugini, Mussi, Pernici, & Tagni, 2010).
In several of these application fields, the search for diagnoses requires repeated computations based on modified versions of the original model to test the different hypotheses
about the faultiness of individual components. In several works the original problem is
converted into a Constraint Satisfaction Problem (CSP) and a number of relaxed versions
of the original CSP have to be solved to construct a new node in the search tree (Felfernig
et al., 2004; Jannach & Schmitz, 2014; White et al., 2010). Depending on the application domain, the computation of CSP solutions or the check for consistency can, however,
be computationally intensive and actually represents the most costly operation during the
construction of the search tree. Similar problems arise when other underlying reasoning
techniques, e.g., for ontology debugging (Friedrich & Shchekotykhin, 2005), are used.
Current MBD algorithms are sequential in nature and generate one node at a time.
Therefore, they do not exploit the capabilities of todays multi-core computer processors,
which can nowadays be found even in mobile devices. In this paper, we propose new schemes
to parallelize the diagnostic reasoning process to better exploit the available computing
resources of modern computer hardware. In particular, this work comprises the following
algorithmic contributions and insights based on experimental evaluations:
 We propose two parallel versions of Reiters (1987) sound and complete Hitting Set
(HS) algorithm to speed up the process of finding all diagnoses, which is a common
problem setting in the above-described MBD applications. Both approaches can be
considered as window-based parallelization schemes, which means that only a limited number of search nodes is processed in parallel at each point in time.
 We evaluate two different conflict detection techniques in a multi-core setting, where
the goal is to find a few leading diagnoses. In this set of experiments, multiple conflicts can be computed at the construction of each tree node using the novel
MergeXplain method (MXP) (Shchekotykhin, Jannach, & Schmitz, 2015) and more
processing time is therefore implicitly allocated for conflict generation.
 We demonstrate that speedups can also be achieved through parallelization for scenarios in which we search for one single diagnosis, e.g., when using a basic parallel
depth-first strategy.
 We measure the improvements that can be achieved through parallel constraint solving
when using a direct CSP-based encoding of the diagnosis problem. This experiment
836

fiParallel Model-Based Diagnosis on Multi-Core Computers

illustrates that parallelization in the underlying solvers, in particular when using a
direct encoding, can be advantageous.
We evaluate the proposed parallelization schemes through an extensive set of experiments. The following problem settings are analyzed.
(i) Standard benchmark problems from the diagnosis research community;
(ii) Mutated CSPs from a Constraint Programming competition and from the domain of
CSP-based spreadsheet debugging (Jannach & Schmitz, 2014);
(iii) Faulty OWL ontologies as used for the evaluation of MBD-based debugging techniques
of very expressive ontologies (Shchekotykhin, Friedrich, Fleiss, & Rodler, 2012);
(iv) Synthetically generated problems which allow us to vary the characteristics of the
underlying diagnosis problem.
The results show that using parallelization techniques can help to achieve substantial
speedups for the diagnosis process (a) across a variety of application scenarios, (b) without
exploiting any specific knowledge about the structure of the underlying diagnosis problem,
(c) across different problem encodings, and (d) also for application problems like ontology
debugging which cannot be efficiently encoded as SAT problems.
The outline of the paper is as follows. In the next section, we define the main concepts of
MBD and introduce the algorithm used to compute diagnoses. In Section 3, we present and
systematically evaluate the parallelization schemes for Reiters HS-tree method when the
goal is to find all minimal diagnoses. In Section 4, we report the results of the evaluations
when we implicitly allocate more processing time for conflict generation using MXP for
conflict detection. In Section 5 we assess the potential gains for a comparably simple
randomized depth-first strategy and a hybrid technique for the problem of finding one
single diagnosis. The results of the experiments for the direct CSP encoding are reported
in Section 6. In Section 7 we discuss previous works. The paper ends with a summary and
an outlook in Section 8.

2. Reiters Diagnosis Framework
This section summarizes Reiters (1987) diagnosis framework which we use as a basis for
our work.
2.1 Definitions
Reiter (1987) formally characterized Model-Based Diagnosis using first-order logic. The
main definitions can be summarized as follows.
Definition 2.1. (Diagnosable System) A diagnosable system is described as a pair (SD,
Comps) where SD is a system description (a set of logical sentences) and Comps represents
the systems components (a finite set of constants).
The connections between the components and the normal behavior of the components
are described in terms of logical sentences. The normal behavior of the system components
837

fiJannach, Schmitz, & Shchekotykhin

is usually described in SD with the help of a distinguished negated unary predicate ab(.),
meaning not abnormal.
A diagnosis problem arises when some observation o P Obs of the systems input-output
behavior (again expressed as first-order sentences) deviates from the expected system behavior. A diagnosis then corresponds to a subset of the systems components which we
assume to behave abnormally (be faulty) and where these assumptions must be consistent
with the observations. In other words, the malfunctioning of these components can be a
possible reason for the observations.
Definition 2.2. (Diagnosis) Given a diagnosis problem (SD, Comps, Obs), a diagnosis is
a subset minimal set   Comps such that SD Y Obs Y tab(c)|c P u Y t abpcq|c P
Compszu is consistent.
According to Definition 2.2, we are only interested in minimal diagnoses, i.e., diagnoses
which contain no superfluous elements and are thus not supersets of other diagnoses. Whenever we use the term diagnosis in the remainder of the paper, we therefore mean minimal
diagnosis. Whenever we refer to non-minimal diagnoses, we will explicitly mention this fact.
Finding all diagnoses can in theory be done by simply trying out all possible subsets
of Comps and checking their consistency with the observations. Reiter (1987), however,
proposes a more efficient procedure based on the concept of conflicts.
Definition 2.3. (Conflict) A conflict for (SD, Comps, Obs) is a set tc1 , ..., ck u  Comps
such that SD Y Obs Y t abpc1 q, ..., abpck qu is inconsistent.
A conflict corresponds to a subset of components which, if assumed to behave normally,
are not consistent with the observations. A conflict c is considered to be minimal, if no
proper subset of c exists which is also a conflict.
2.2 Hitting Set Algorithm
Reiter (1987) then discusses the relationship between conflicts and diagnoses and claims
in his Theorem 4.4 that the set of diagnoses for a collection of (minimal) conflicts F is
equivalent to the set H of minimal hitting sets 1 of F .
To determine the minimal hitting sets and therefore the diagnoses, Reiter proposes a
breadth-first search procedure and the construction of a hitting set tree (HS-tree), whose
construction is guided by conflicts. In the logic-based definition of the MBD problem
(Reiter, 1987), the conflicts are computed by calls to a Theorem Prover (TP). The TP
component itself is considered as a black box and no assumptions are made about how
the conflicts are determined. Depending on the application scenario and problem encoding,
one can, however, also use specific algorithms like QuickXplain (Junker, 2004), Progression (Marques-Silva, Janota, & Belov, 2013) or MergeXplain (Shchekotykhin et al., 2015),
which guarantee that the computed conflict sets are minimal.
The main principle of the HS-tree algorithm is to create a search tree where each node
is either labeled with a conflict or represents a diagnosis. In the latter case the node is
not further expanded. Otherwise, a child node is generated for each element of the nodes
1. Given a collection C of subsets of a finite set S, a hitting set for C is a subset of S which contains at
least one element from each subset in C. This corresponds to the set cover problem.

838

fiParallel Model-Based Diagnosis on Multi-Core Computers

conflict and each outgoing edge is labeled with one component of the nodes conflict. In the
subsequent expansions of each node the components that were used to label the edges on
the path from the root of the tree to the current node are assumed to be faulty. Each newly
generated child node is again either a diagnosis or will be labeled with a conflict that does
not contain any component that is already assumed to be faulty at this stage. If no conflict
can be found for a node, the path labels represent a diagnosis in the sense of Definition 2.2.
2.2.1 Example
In the following example we will show how the HS-tree algorithm and the QuickXplain
(QXP) conflict detection technique can be combined to locate a fault in a specification
of a CSP. A CSP instance I is defined as a tuple pV, D, Cq, where V  tv1 , . . . , vn u is a
set of variables, D  tD1 , . . . , Dn u is a set of domains for each of the variables in V , and
C  tC1 , . . . , Ck u is a set of constraints. An assignment to any subset X  V is a set of
pairs A  txv1 , d1 y, . . . , xvk , dm yu where vi P X is a variable and dj P Di is a value from the
domain of this variable. An assignment comprises exactly one variable-value pair for each
variable in X. Each constraint Ci P C is defined over a list of variables S, called scope,
and forbids or allows certain simultaneous assignments to the variables in its scope. An
assignment A to S satisfies a constraint Ci if A comprises an assignment allowed by Ci . An
assignment A is a solution to I if it satisfies all constraints C.
Consider a CSP instance I with variables V  ta, b, cu where each variable has the
domain t1, 2, 3u and the following set of constraints are defined:
C1 : a  b,

C2 : b  c,

C3 : c  a,

C4 : b  c

Obviously, no solution for I exists and our diagnosis problem consists in finding subsets of
the constraints whose definition is faulty. The engineer who has modeled the CSP could,
for example, have made a mistake when writing down C2, which should have been b  c.
Eventually, C4 was added later on to correct the problem, but the engineer forgot to remove
C2. Given the faulty definition of I, two minimal conflicts exist, namely ttC1, C2, C3u,
tC2, C4uu, which can be determined with the help of QXP. Given these two conflicts,
the HS-tree algorithm will finally determine three minimal hitting sets ttC2u, tC1, C4u,
tC3, C4uu, which are diagnoses for the problem instance. The set of diagnoses also contains
the true cause of the error, the definition of C2.
Let us now review in more detail how the HS-tree/QXP combination works for the example problem. We illustrate the tree construction in Figure 1. In the logic-based definition
of Reiter, the HS-tree algorithm starts with a check if the observations Obs are consistent
with the system description SD and the components Comps. In our application setting this
corresponds to a check if there exists any solution for the CSP instance.2 Since this is not
the case, a QXP-call is made, which returns the conflict tC1, C2, C3u, which is used as a
label for the root node ( 1 ) of the tree. For each element of the conflict, a child node is
created and the conflict element is used as a path label. At each tree node, again the consistency of SD, Obs, and Comps is tested; this time, however, all the elements that appear
2. Comps are the constraints tC1...C4u and SD corresponds to the semantics/logic of the constraints when
working correctly, e.g., ABpC1q _ pa  bq. Obs is empty in this example but could be a partial value
assignment (test case) in another scenario.

839

fiJannach, Schmitz, & Shchekotykhin

1
{C1,C2, C3}
C1

2

C2

C3

3

{C2,C4}
4

C2

{C2,C4}

C4

C4

C2
5

Figure 1: Example for HS-tree construction.
as labels on the path from the root node to the current node are considered to be abnormal.
In the CSP diagnosis setting, this means that we check if there is any solution to a modified
version of our original CSP from which we remove the constraints that appear as labels on
the path from the root to the current node.
At node 2 , C1 is correspondingly considered to be abnormal. As removing C1 from the
CSP is, however, not sufficient and no solution exists for the relaxed problem, another call
to QXP is made, which returns the conflict tC2, C4u. tC1u is therefore not a diagnosis and
the new conflict is used as a label for node 2 . The algorithm then proceeds in breadth-first
style and tests if assuming tC2u or tC3u to be individually faulty is consistent with the
observations, which in our case means that a solution to the relaxed CSP exists. Since tC2u
is a diagnosis  at least one solution exists if C2 is removed from the CSP definition  the
node is marked with 3 and not further expanded. At node 3 , which does not correspond
to a diagnosis, the already known conflict tC2, C4u can be reused as it has no overlap with
the nodes path label and no call to T P (QXP) is required. At the last tree level, the
nodes 4 and 5 are not further expanded (closed and marked with 7) because tC2u has
already been identified as a diagnosis at the previous level and the resulting diagnoses would
be supersets of tC2u. Finally, the sets tC1, C4u and tC3, C4u are identified as additional
diagnoses.
2.2.2 Discussion
Soundness and Completeness According to Reiter (1987), the breadth-first construction scheme and the node closing rule ensure that only minimal diagnoses are computed.
At the end of the HS-tree construction process, each set of edge labels on the path from the
root of the tree to a node marked with 3 corresponds to a diagnosis.3
Greiner, Smith, and Wilkerson (1989), later on, identified a potential problem in Reiters
algorithm for cases in which the conflicts returned by T P are not guaranteed to be minimal.
An extension of the algorithm based on an HS-DAG (directed acyclic graph) structure was
proposed to solve the problem.
In the context of our work, we only use methods that return conflicts which are guaranteed to be minimal. For example, according to Theorem 1 in the work of Junker (2004),
given a set of formulas and a sound and complete consistency checker, QXP always returns
3. Reiter (1987) states in Theorem 4.8 that given a set of conflict sets F , the HS-tree algorithm outputs a
pruned tree T such that the set tHpnq|n is a node of T labeled with 3u corresponds to the set H of all
minimal hitting sets of F where Hpnq is a set of arc labels on the path from the node n to the root.

840

fiParallel Model-Based Diagnosis on Multi-Core Computers

either a minimal conflict or no conflict. This minimality guarantee in turn means that the
combination of the HS-tree algorithm and QXP is sound and complete, i.e., all returned
solutions are actually (minimal) diagnoses and no diagnosis for the given set of conflicts
will be missed. The same holds when computing multiple conflicts at a time with MXP
(Shchekotykhin et al., 2015).
To simplify the presentation of our parallelization approaches, we will therefore rely
on Reiters original HS-tree formulation; an extension to deal with the HS-DAG structure
(Greiner et al., 1989) is possible.
On-Demand Conflict Generation and Complexity In many of the above-mentioned
applications of MBD to practical problems, the conflicts have to be computed on-demand,
i.e., during tree construction, because we cannot generally assume that the set of minimal
conflicts is given in advance. Depending on the problem setting, finding these conflicts can
therefore be the computationally most intensive part of the diagnosis process.
Generally, finding hitting sets for a collection of sets is known to be an NP-hard problem
(Garey & Johnson, 1979). Moreover, deciding if an additional diagnosis exists when conflicts
are computed on demand is NP-complete even for propositional Horn theories (Eiter &
Gottlob, 1995). Therefore, a number of heuristics-based, approximate and thus incomplete,
as well as problem-specific diagnosis algorithms have been proposed over the years. We
will discuss such approaches in later sections. In the next section, we, however, focus on
(worst-case) application scenarios where the goal is to find all minimal diagnoses for a given
problem, i.e., we focus on complete algorithms.
Consider, for example, the problem of debugging program specifications (e.g., constraint
programs, knowledge bases, ontologies, or spreadsheets) with MBD techniques as mentioned
above. In these application domains, it is typically not sufficient to find one minimal diagnosis. In the work of Jannach and Schmitz (2014), for example, the spreadsheet developer
is presented with a ranked list of all sets of formulas (diagnoses) that represent possible
reasons why a certain test case has failed. The developer can then either inspect each of
them individually or provide additional information (e.g., test cases) to narrow down the set
of candidates. If only one diagnosis was computed and presented, the developer would have
no guarantee that it is the true cause of the problem, which can lead to limited acceptance
of the diagnosis tool.

3. Parallel HS-Tree Construction
In this section we present two sound and complete parallelization strategies for Reiters
HS-tree method to determine all minimal diagnoses.
3.1 A Non-recursive HS-Tree Algorithm
We use a non-recursive version of Reiters sequential HS-tree algorithm as a basis for the
implementation of the two parallelization strategies. Algorithm 1 shows the main loop of a
breadth-first procedure, which uses a list of open nodes to be expanded as a central data
structure.
The algorithm takes a diagnosis problem (DP) instance as input and returns the set
 of diagnoses. The DP is given as a tuple (SD, Comps, Obs), where SD is the system
841

fiJannach, Schmitz, & Shchekotykhin

Algorithm 1: diagnose: Main algorithm loop.
Input: A diagnosis problem (SD, Comps, Obs)
Result: The set  of diagnoses
1
2
3
4
5
6
7
8
9

  H; paths  H; conflicts  H;
nodesToExpand = xgenerateRootNode(SD, Comps, Obs)y;
while nodesToExpand  x y do
newNodes = x y;
node = head(nodesToExpand) ;
foreach c P node.conflict do
generateNode(node, c, , paths, conflicts, newNodes);
nodesToExpand = tail(nodesToExpand)  newNodes;
return ;

Algorithm 2: generateNode: Node generation logic.
Input: An existingNode to expand, a conflict element c P Comps,
the sets , paths, conflicts, newNodes
1
2
3
4
5
6
7
8
9
10
11
12
13

newPathLabel = existingNode.pathLabel Y {c};
if pE l P  : l  newPathLabelq ^ checkAndAddPathppaths, newPathLabelq then
node = new Node(newPathLabel);
if D S P conflicts : S X newPathLabel  H then
node.conflict = S;
else
newConflicts = checkConsistency(SD, Comps, Obs, node.pathLabel);
node.conflict = head(newConflicts);
if node.conflict  H then
newNodes = newNodes  xnodey;
conflicts = conflicts Y newConflicts;
else
   Y {node.pathLabel};

description, Comps the set of components that can potentially be faulty and Obs a set of
observations. The method generateRootNode creates the initial node, which is labeled
with a conflict and an empty path label. Within the while loop, the first element of a firstin-first-out (FIFO) list of open nodes nodesToExpand is taken as the current element.
The function generateNode (Algorithm 2) is called for each element of the nodes conflict
and adds new leaf nodes, which still have to be explored, to a global list. These new
nodes are then appended () to the remaining list of open nodes in the main loop, which
842

fiParallel Model-Based Diagnosis on Multi-Core Computers

continues until no more elements remain for expansion.4 Algorithm 2 (generateNode)
implements the node generation logic, which includes Reiters proposals for conflict re-use,
tree pruning, and the management of the lists of known conflicts, paths and diagnoses. The
method determines the path label for the new node and checks if the new path label is not
a superset of an already found diagnosis.
Algorithm 3: checkAndAddPath: Adding a new path label with a redundancy
check.
Input: The previously explored paths, the newPathLabel to be explored
Result: Boolean stating if newPathLabel was added to paths

3

if E l P paths : l  newPathLabel then
paths = paths Y newPathLabel;
return true;

4

return false;

1
2

The function checkAndAddPath (Algorithm 3) is then used to check if the node was
not already explored elsewhere in the tree. The function returns true if the new path label
was successfully inserted into the list of known paths. Otherwise, the list of known paths
remains unchanged and the node is closed.
For new nodes, either an existing conflict is reused or a new one is created with a call
to the consistency checker (Theorem Prover), which tests if the new node is a diagnosis
or returns a set of minimal conflicts otherwise. Depending on the outcome, a new node is
added to the list nodesToExpand or a diagnosis is stored. Note that Algorithm 2 has no
return value but instead modifies the sets , paths, conflicts, and newNodes, which were
passed as parameters.
3.2 Level-Wise Parallelization
Our first parallelization scheme examines all nodes of one tree level in parallel and proceeds
with the next level once all elements of the level have been processed. In the example shown
in Figure 1, this would mean that the computations (consistency checks and theorem prover
calls) required for the three first-level nodes labeled with tC1u, tC2u, and tC3u can be done
in three parallel threads. The nodes of the next level are explored when all threads of the
previous level are finished.
Using this Level-Wise Parallelization (LWP) scheme, the breadth-first character is maintained. The parallelization of the computations is generally feasible because the consistency
checks for each node can be done independently from those done for the other nodes on the
same level. Synchronization is only required to make sure that no thread starts exploring a
path which is already under examination by another thread.
Algorithm 4 shows how the sequential Algorithm 1 can be adapted to support this
parallelization approach. Again, we maintain a list of open nodes to be expanded. The
difference is that we run the expansion of all these nodes in parallel and collect all the
4. A limitation regarding the search depth or the number of diagnoses to find can be easily integrated into
this scheme.

843

fiJannach, Schmitz, & Shchekotykhin

Algorithm 4: diagnoseLW: Level-Wise Parallelization.
Input: A diagnosis problem (SD, Comps, Obs)
Result: The set  of diagnoses
1
2
3
4
5
6
7
8
9
10

  H; conf licts  H; paths = H;
nodesToExpand = xgenerateRootNode(SD, Comps, Obs)y;
while nodesToExpand  x y do
newNodes = x y;
foreach node P nodesToExpand do
foreach c P node.conflict do
// Do computations in parallel
threads.execute(generateNode(node, c, , paths, conflicts, newNodes));
threads.await();
nodesToExpand = newNodes;

// Wait for current level to complete
// Prepare next level

return ;

nodes of the next level in the variable newNodes. Once the current level is finished, we
overwrite the list nodesToExpand with the list containing the nodes of the next level.
The Java-like API calls used in the pseudo-code in Algorithm 4 have to be interpreted
as follows. The statement threads.execute() takes a function as a parameter and schedules
it for execution in a pool of threads of a given size. With a thread pool of, e.g., size 2,
the generation of the first two nodes would be done in parallel and the next ones would
be queued until one of the threads has finished. With this mechanism, we can ensure that
the number of threads executed in parallel is less than or equal to the number of hardware
threads or CPUs.
The statement threads.await() is used for synchronization and blocks the execution of
the subsequent code until all scheduled threads are finished. To guarantee that the same
path is not explored twice, we make sure that no two threads in parallel add a node with
the same path label to the list of known paths. This can be achieved by declaring the
function checkAndAddPath as a critical section (Dijkstra, 1968), which means that no
two threads can execute the function in parallel. Furthermore, we have to make the access
to the global data structures (e.g., the already known conflicts or diagnoses) thread-safe,
i.e., ensure that no two threads can simultanuously manipulate them.5
3.3 Full Parallelization
In LWP, there can be situations where the computation of a conflict for a specific node
takes particularly long. This, however, means that even if all other nodes of the current
level are finished and many threads are idle, the expansion of the HS-tree cannot proceed
before the level is completed. Algorithm 5 shows our proposed Full Parallelization (FP)
algorithm variant, which immediately schedules every expandable node for execution and
thereby avoids such potential CPU idle times at the end of each level.
5. Controlling such concurrency aspects is comparably simple in modern programming languages like Java,
e.g., by using the synchronized keyword.

844

fiParallel Model-Based Diagnosis on Multi-Core Computers

Algorithm 5: diagnoseFP: Full Parallelization.
Input: A diagnosis problem (SD, Comps, Obs)
Result: The set  of diagnoses
1
2
3
4
5
6
7
8

9
10
11
12

  H; paths  H; conflicts  H;
nodesToExpand = xgenerateRootNode(SD, Comps, Obs)y;
size = 1; lastSize = 0;
while psizelastSizeq _ pthreads.activeThreads 0q do
for i  1 to size  lastSize do
node = nodesToExpand.get[lastSize + i];
foreach c P node.conflict do
threads.execute(generateNodeFP(node, c, , paths, conflicts,
nodesToExpand));
lastSize = size;
wait();
size = nodesToExpand.length();
return ;

The main loop of the algorithm is slightly different and basically monitors the list of
nodes to expand. Whenever new entries in the list are observed, i.e., when the last observed
list size is different from the current one, it retrieves the recently added elements and adds
them to the thread queue for execution. The algorithm returns the diagnoses when no new
elements are added since the last check and no more threads are active.6
With FP, the search does not necessarily follow the breadth-first strategy anymore and
non-minimal diagnoses are found during the process. Therefore, whenever we find a new
diagnosis d, we have to check if the set of known diagnoses  contains supersets of d and
remove them from .
The updated generateNode method is listed in Algorithm 6. When updating the shared
data structures (nodesToExpand, conflicts, and ), we again make sure that the threads do
not interfere with each other. The mutual exclusive section is marked with the synchronized
keyword.
When compared to LWP, FP does not have to wait at the end of each level if a specific
node takes particularly long to generate. On the other hand, FP needs more synchronization
between threads, so that in cases where the last nodes of a level are finished at the same
time, LWP could also be advantageous. We will evaluate this aspect in Section 3.5.
3.4 Properties of the Algorithms
Algorithm 1 together with Algorithms 2 and 3 corresponds to an implementation of the
HS-tree algorithm (Reiter, 1987). Algorithm 1 implements the breadth-first search strategy
 point (1) in Reiters HS-tree algorithm  since the nodes stored in the list nodesToExpand
6. The functions wait() and notify() implement the semantics of pausing a thread and awaking a paused
thread in the Java programming language and are used to avoid active waiting loops.

845

fiJannach, Schmitz, & Shchekotykhin

Algorithm 6: generateNodeFP: Extended node generation logic.
Input: An existingNode to expand, c P Comps,
sets , paths, conflicts, nodesToExpand
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

17

newPathLabel = existingNode.pathLabel Y {c};
if pE l P  : l  newPathLabelq ^ checkAndAddPathppaths, newPathLabelq then
node = new Node(newPathLabel);
if D S P conflicts : S X newPathLabel  H then
node.conflict = S;
else
newConflicts = checkConsistency(SD, Comps, Obs, node.pathLabel);
node.conflict = head(newConflicts);
synchronized
if node.conflict  H then
nodesToExpand = nodesToExpand  xnodey;
conflicts = conflicts Y newConflicts;
else if E d P  : d  newPathLabel then
   Y {node.pathLabel};
for d P  : d  newPathLabel do
   z d;
notify();

are processed iteratively in a first-in-first-out order (see lines 5 and 8). Algorithm 2 first
checks if the pruning rules (i) and (ii) of Reiter can be applied in line 2. These rules state
that a node can be pruned if (i) there exists a diagnosis or (ii) there is a set of labels
corresponding to some path in the tree such that it is a subset of the set of labels on the
path to the node. Pruning rule (ii) is implemented through Algorithm 3. Pruning rule (iii)
of Reiters algorithm is not necessary since in our settings a TP -call guarantees to return
minimal conflicts.
Finally, point (2) of Reiters HS-tree algorithm description is implemented in the lines
4-8 of Algorithm 2. Here, the algorithm checks if there is a conflict that can be reused as a
node label. In case no reuse is possible, the algorithm calls the theorem prover TP to find
another minimal conflict. If a conflict is found, the node is added to the list of open nodes
nodesToExpand . Otherwise, the set of node path labels is added to the set of diagnoses.
This corresponds to the situation in Reiters algorithm where we would mark a node in the
HS-tree with the 3 symbol. Note that we do not label any nodes with 7 as done in Reiters
algorithms since we simply do not store such nodes in the expansion list.
Overall, we can conclude that our HS-tree algorithm implementation (Algorithm 1 to
3) has the same properties as Reiters original HS-tree algorithm. Namely, each hitting set
returned by the algorithm is minimal (soundness) and all existing minimal hitting sets are
found (completeness).
846

fiParallel Model-Based Diagnosis on Multi-Core Computers

3.4.1 Level-Wise Parallelization (LWP)
Theorem 3.1. Level-Wise Parallelization is sound and complete.
Proof. The proof is based on the fact that LWP uses the same expansion and pruning
techniques as the sequential algorithm (Algorithms 2 and 3). The main loop in line 3 applies
the same procedure as the original algorithm with the only difference that the executions
of Algorithm 2 are done in parallel for each level of the tree. Therefore, the only difference
between the sequential algorithm and LWP lies in the order in which the nodes of one level
are labeled and generated.
Let us assume that there are two nodes n1 and n2 in the tree and that the sequential
HS-tree algorithm will process n1 before n2 . Assuming that neither n1 nor n2 correspond
to diagnoses, the sequential Algorithm 1 would correspondingly first add the child nodes of
n1 to the queue of open nodes and later on append the child nodes of n2 .
If we parallelize the computations needed for the generation of n1 and n2 in LWP, it
can happen that the computations for n1 need longer than those for n2 . In this case the
child nodes of n2 will be placed in the queue first. The order of how these nodes are
subsequently processed is, however, irrelevant for the computation of the minimal hitting
sets, since neither the labeling nor the pruning rules are influenced by it. In fact, the
labeling of any node n only depends on whether or not a minimal conflict set f exists such
that Hpnq X f  H, but not on the other nodes on the same level. The pruning rules
state that any node n can be pruned if there exists a node n1 labeled with 3 such that
Hpn1 q  Hpnq, i.e., supersets of already found diagnoses can be pruned. If n and n1 are
on the same level, then |Hpnq|  |Hpn1 q|. Consequently, the pruning rule is applied only if
Hpnq  Hpn1 q. Therefore, the order of nodes, i.e., which of the nodes is pruned, is irrelevant
and no minimal hitting set is lost. Consequently, LWP is complete.
Soundness of the algorithm follows from the fact that LWP constructs the hitting sets
always in the order of increasing cardinality. Therefore, LWP will always return only minimal hitting sets even in scenarios in which we should stop after k diagnoses are found,
where 1  k  N is a predefined constant and N is the total number of diagnoses of a
problem.
3.4.2 Full Parallelization (FP)
The minimality of the hitting sets encountered during the search is not guaranteed by FP,
since the algorithm schedules a node for processing immediately after its generation (line 8
of Algorithm 5). The special treatment in the generateNodeFP function ensures that no
supersets of already found hitting sets are added to  and that supersets of a newly found
hitting set will be removed in a thread-safe manner (lines 13  16 of Algorithm 6). Due
to this change in generateNodeFP, the analysis of soundness and completeness has to be
done for two distinct cases.
Theorem 3.2. Full Parallelization is sound and complete, if applied to find all diagnoses
up to some cardinality.
Proof. FP stops if either (i) no further hitting set exists, i.e., all leaf nodes of a tree are
labeled either with 3 or with 7, or (ii) the predefined cardinality (tree-depth) is reached. In
this latter case, every leaf node of the tree is labeled either with 3, 7, or a minimal conflict
847

fiJannach, Schmitz, & Shchekotykhin

set. Case (ii) can be reduced to (i) by removing all branches from the tree that are labeled
with a minimal conflict. These branches are irrelevant since they can only contribute to
minimal hitting sets of higher cardinality. Therefore, without loss of generality, we can limit
our discussion to case (i).
According to the definition of generateNodeFP, the tree is built using the same pruning
rule as done in the sequential HS-tree algorithm. As a consequence, the tree generated by
FP must comprise at least all nodes of the tree that is generated by the sequential HStree procedure. Therefore, according to Theorem 4.8 in the work of Reiter (1987) the
tree T generated by FP must comprise a set of leaf nodes labeled with 3 such that the
set tHpnq|n is a node of T labeled by 3u corresponds to the set H of all minimal hitting
sets. Moreover, the result returned by FP comprises only minimal hitting sets, because
generateNodeFP removes all hitting sets from H which are supersets of other hitting sets.
Consequently, FP is sound and complete, when applied to find all diagnoses.
Theorem 3.3. Full Parallelization cannot guarantee completeness and soundness when
applied to find the first k diagnoses, i.e. 1  k  N , where N is the total number of
diagnoses of a problem.
Proof. The proof can be done by constructing an example for which FP returns at least
one non-minimal hitting set in the set , thus violating Definition 2.2. For instance, this
situation might occur if FP is applied to find one single diagnosis for the example problem
presented in Section 2.2.1. Let us assume that the generation of the node corresponding
to the path C2 is delayed, e.g., because the operating system scheduled another thread for
execution first, and node 4 is correspondingly generated first. In this case, the algorithm
would return the non-minimal hitting set tC1, C2u which is not a diagnosis.
Note that the elements of the set  returned by FP in this case can be turned to
diagnoses by applying a minimization algorithm like Inv-QuickXplain (Shchekotykhin,
Friedrich, Rodler, & Fleiss, 2014), an algorithm that adopts the principles of QuickXplain
and applies a divide-and-conquer strategy to find one minimal diagnosis for a given set of
inconsistent constraints.
Given a hitting set H and a diagnosis problem, the algorithm is capable of computing a
minimal hitting set H 1  H requiring only Op|H 1 |`|H 1 | logp|H|{|H 1 |qqq calls to the theorem
prover TP. The first part, |H 1 |, reflects the computational costs of determining whether or
not H 1 is minimal. The second part represents the number of subproblems that must be
considered by the divide-and-conquer algorithm in order to find the minimal hitting set H 1 .
3.5 Evaluation
To determine which performance improvements can be achieved through the various forms
of parallelization proposed in this paper, we conducted a series of experiments with diagnosis
problems from a number of different application domains. Specifically, we used electronic
circuit benchmarks from the DX Competition 2011 Synthetic Track, faulty descriptions of
Constraint Satisfaction Problems (CSPs), as well as problems from the domain of ontology
debugging. In addition, we ran experiments with synthetically created diagnosis problems
to analyze the impact of varying different problem characteristics. All diagnosis algorithms
848

fiParallel Model-Based Diagnosis on Multi-Core Computers

evaluated in this paper were implemented in Java unless noted otherwise. Generally, we
use wall clock times as our performance measure.
In the main part of the paper, we will focus on the results for the DX Competition
problems as this is the most widely used benchmark. The results for the other problem
setups will be presented and discussed in the appendix of the paper. In most cases, the
results for the DX Competition problems follow a similar trend as those that are achieved
with the other experiments.
In this section we will compare the HS-tree parallelization schemes LWP and FP with
the sequential version of the algorithm, when the goal is to find all diagnoses.
3.5.1 Dataset and Procedure
For this set of experiments, we selected the first five systems of the DX Competition 2011
Synthetic Track (see Table 1) (Kurtoglu & Feldman, 2011). For each system, the competition specifies 20 scenarios with injected faults resulting in different faulty output values.
We used the system description and the given input and output values for the diagnosis
process. The additional information about the injected faults was of course ignored. The
problems were converted into Constraint Satisfaction Problems. In the experiments we used
Choco (Prudhomme, Fages, & Lorca, 2015) as a constraint solver and QXP for conflict
detection, which returns one minimal conflict when called during node construction.
As the computation times required for conflict identification strongly depend on the
order of the possibly faulty constraints, we shuffled the constraints for each test and repeated
all tests 100 times. We report the wall clock times for the actual diagnosis task; the times
required for input and output are independent from the HS-tree construction scheme and
not relevant for our benchmarks. For the parallel approaches, we used a thread pool of size
four.7
Table 1 shows the characteristics of the systems in terms of the number of constraints
(#C) and the problem variables (#V).8 The numbers of the injected faults (#F) and the
numbers of the calculated diagnoses (#D) vary strongly because of the different scenarios
for each system. For both columns we show the ranges of values over all scenarios. The
columns #D and |D| indicate the average number of diagnoses and their average cardinality.
As can be seen, the search tree for the diagnosis can become extremely broad with up to
6,944 diagnoses with an average diagnosis size of only 3.38 for the system c432.
3.5.2 Results
Table 2 shows the averaged results when searching for all minimal diagnoses. We first list the
running times in milliseconds for the sequential version (Seq.) and then the improvements
of LWP or FP in terms of speedup and efficiency with respect to the sequential version.
Speedup Sp is computed as Sp  T1 {Tp , where T1 is the wall time when using 1 thread (the
sequential algorithm) and Tp the wall time when p parallel threads are used. A speedup of
7. Having four hardware threads is a reasonable assumption on standard desktop computers and also mobile
devices. The hardware we used for the evaluation in this chapter  a laptop with an Intel i7-3632QM
CPU, 16GB RAM, running Windows 8  also had four cores with hyperthreading. The results of an
evaluation on server hardware with 12 cores are reported later in this Section.
8. For systems marked with *, the search depth was limited to their actual number of faults to ensure that
the sequential algorithm terminates within a reasonable time frame.

849

fiJannach, Schmitz, & Shchekotykhin

System
74182
74L85
74283*
74181*
c432*

#C
21
35
38
67
162

#V
28
44
45
79
196

#F
4-5
1-3
2-4
3-6
2-5

#D
30 - 300
1 - 215
180 - 4,991
10 - 3,828
1 - 6,944

#D
139.0
66.4
1,232.7
877.8
1,069.3

|D|
4.66
3.13
4.42
4.53
3.38

Table 1: Characteristics of the selected DXC benchmarks.
2 would therefore mean that the needed computation times were halved; a speedup of 4,
which is the theoretical optimum when using 4 threads, means that the time was reduced
to one quarter. The efficiency Ep is defined as Sp {p and compares the speedup with the
theoretical optimum. The fastest algorithm for each system is highlighted in bold.
System
74182
74L85
74283*
74181*
c432*

Seq.(QXP)
[ms]
65
209
371
21,695
85,024

LWP(QXP)
S4
E4
2.23
0.56
2.55
0.64
2.53
0.63
1.22
0.31
1.47
0.37

FP(QXP)
S4
E4
2.28 0.57
2.77 0.69
2.66 0.67
3.19 0.80
3.75 0.94

Table 2: Observed performance gains for the DXC benchmarks when searching for all diagnoses.

In all tests, both parallelization approaches outperform the sequential algorithm. Furthermore, the differences between the sequential algorithm and one of the parallel approaches were statistically significant (p  0.05) in 95 of the 100 tested scenarios. For
all systems, FP was more efficient than LWP and the speedups range from 2.28 to 3.75
(i.e., up to a reduction of running times of more than 70%). In 59 of the 100 scenarios
the differences between LWP and FP were statistically significant. A trend that can be
observed is that the efficiency of FP is higher for the more complex problems. The reason is
that for these problems the time needed for the node generation is much larger in absolute
numbers than the additional overhead times that are required for thread synchronization.
3.5.3 Adding More Threads
In some use cases the diagnosis process can be done on powerful server architectures that
often have even more CPU cores than modern desktop computers. In order to assess to
which extent more than 4 threads can help to speed up the diagnosis process, we tested the
different benchmarks on a server machine with 12 CPU cores. For this test we compared
FP with 4, 8, 10, and 12 threads to the sequential algorithm.
The results of the DXC benchmark problems are shown in Table 3. For all tested systems
the diagnosis process was faster using 8 instead of 4 threads and substantial speedups up
to 5.20 could be achieved compared to the sequential diagnosis, which corresponds to a
850

fiParallel Model-Based Diagnosis on Multi-Core Computers

runtime reduction of 81%. For all but one system, the utilization of 10 threads led to
additional speedups. Using 12 threads was the fastest for 3 of the 5 tested systems. The
efficiency, however, degrades as more threads are used, because more time is needed for the
synchronization between threads. Using more threads than the hardware actually has cores
did not result in additional speedups for any of the tested systems. The reason is that for
most of the time all threads are busy with conflict detection, e.g., finding solutions to CSPs,
and use almost 100% of the processing power assigned to them.
System
74182
74L85
74283
74181*
c432*

Seq.(QXP)
[ms]
58
184
51,314
13,847
43,916

S4
2.09
2.53
3.04
3.45
3.43

E4
0.52
0.63
0.76
0.86
0.86

S8
2.43
3.29
4.38
5.20
4.77

FP(QXP)
E8
S10
E10
0.30 2.52 0.25
0.41 3.35 0.34
0.55 4.42 0.44
0.65 5.11 0.51
0.60 5.00 0.50

S12
2.54
3.38
4.50
5.19
4.74

E12
0.21
0.28
0.37
0.43
0.39

Table 3: Observed performance gains for the DXC benchmarks on a server with 12 hardware
threads.

3.5.4 Additional Experiments
The details of additional experiments that were conducted to compare the proposed parallelization schemes with the sequential HS-Tree algorithm are presented in Section A.1 in the
appendix. The results show that significant speedups can also be achieved for other Constraint Satisfaction Problems (Section A.1.1) and ontologies (Section A.1.2). The appendix
furthermore contains an analysis of effects when adding more threads to the benchmarks of
the CSPs and ontologies (Section A.1.3) and presents the results of a simulation experiment
in which we systematically varied different problem characteristics (Section A.1.4).
3.5.5 Discussion
Overall, the results of the evaluations show that both parallelization approaches help to improve the performance of the diagnosis process, as for all tested scenarios both approaches
achieved speedups. In most cases FP is faster than LWP. However, depending on the
specifics of the given problem setting, using LWP can be advantageous in some situations,
e.g., when the time needed to generate each node is very small or when the conflict generation time does not vary strongly. In these cases the synchronization overhead needed for
FP is higher than the cost of waiting for all threads to finish. For the tested ontologies in
Section A.1.2, this was the case in four of the tested scenarios.
Although FP is on average faster than LWP and significantly better than the sequential
HS-tree construction approach, for some of the tested scenarios its efficiency is still far
from the optimum of 1. This can be explained by different effects. For example, the effect
of false sharing can happen if the memory of two threads is allocated to the same block
(Bolosky & Scott, 1993). Then every access to this memory block is synchronized although
the two threads do not really share the same memory. Another possible effect is called cache
851

fiJannach, Schmitz, & Shchekotykhin

contention (Chandra, Guo, Kim, & Solihin, 2005). If threads work on different computing
cores but share the same memory, cache misses can occur more often depending on the
problem characteristics and thus the theoretical optimum cannot be reached in these cases.

4. Parallel HS-Tree Construction with Multiple Conflicts Per Node
Both in the sequential and the parallel version of the HS-tree algorithm, the Theorem
Prover TP call corresponds to an invocation of QXP. Whenever a new node of the HS-tree
is created, QXP searches for exactly one new conflict in case none of the already known
conflicts can be reused. This strategy has the advantage that the call to TP immediately
returns after one conflict has been determined. This in turn means that the other parallel
execution threads immediately see this new conflict in the shared data structures and
can, in the best case, reuse it when constructing new nodes.
A disadvantage of computing only one conflict at a time with QXP is that the search
for conflicts is restarted on each invocation. We recently proposed a new conflict detection
technique called MergeXplain (MXP) (Shchekotykhin et al., 2015), which is capable of
computing multiple conflicts in one call. The general idea of MXP is to continue the search
after the identification of the first conflict and look for additional conflicts in the remaining
constraints (or logical sentences) in a divide-and-conquer approach.
When combined with a sequential HS-tree algorithm, the effect is that during tree construction more time is initially spent for conflict detection before the construction continues
with the next node. In exchange, the chances of having a conflict available for reuse increase
for the next nodes. At the same time, the identification of some of the conflicts is less timeintensive as smaller sets of constraints have to be investigated due to the divide-and-conquer
approach of MXP. An experimental evaluation on various benchmark problems shows that
substantial performance improvements are possible in a sequential HS-tree scenario when
the goal is to find a few leading diagnoses (Shchekotykhin et al., 2015).
In this section, we explore the benefits of using MXP with the parallel HS-tree construction schemes proposed in the previous section. When using MXP in combination with
multiple threads, the implicit effect is that more CPU processing power is devoted to conflict generation as the individual threads need more time to complete the construction of a
new node. In contrast to the sequential version, the other threads can continue with their
work in parallel.
In the next section, we will briefly review the MXP algorithm before we report the
results of the empirical evaluation on our benchmark datasets (Section 4.2).
4.1 Background  QuickXplain and MergeXplain
Algorithm 7 shows the QXP conflict detection technique of Junker (2004) applied to the
problem of finding a conflict for a diagnosis problem during HS-tree construction.
QXP operates on two sets of constraints9 which are modified through recursive calls.
The background theory B comprises the constraints that will not be considered anymore
to be part of a conflict at the current stage. At the beginning, this set contains SD, Obs,
9. We use the term constraints here as in the original formulation. As QXP is independent from the
underlying reasoning technique, the elements of the sets could be general logical sentences as well.

852

fiParallel Model-Based Diagnosis on Multi-Core Computers

Algorithm 7: QuickXplain (QXP)
Input: A diagnosis problem (SD, Comps, Obs), a set visitedNodes of elements
Output: A set containing one minimal conflict CS  C
1 B  SD Y Obs Y {ab(c)|c P visitedNodes}; C  t abpcq|c P CompszvisitedNodesu;
2 if isConsistent(B Y C) then return no conflict;
3 else if C  H then return H;
4 return tc| abpcq P getConflictpB, B, Cqu;
5
6
7
8
9
10

function getConflict (B, D, C)
if D  H ^ isConsistent(B) then return H;
if |C|  1 then return C;
Split C into disjoint, non-empty sets C1 and C2
D2  getConflict (B Y C1 , C1 , C2 )
D1  getConflict (B Y D2 , D2 , C1 )
return D1 Y D2 ;

and the set of nodes on the path to the current node of the HS-tree (visited nodes). The
set C represents the set of constraints in which we search for a conflict.
If there is no conflict or C is empty, the algorithm immediately returns. Otherwise getConflict is called, which corresponds to Junkers QXP method with the minor difference
that getConflict does not require a strict partial order for the set of constraints C. We
introduce this variant of QXP since we cannot always assume that prior fault information
is available that would allow us to generate this order.
The rough idea of QXP is to relax the input set of faulty constraints C by partitioning
it into two sets C1 and C2 . If C1 is a conflict, the algorithm continues partitioning C1 in
the next recursive call. Otherwise, i.e., if the last partitioning has split all conflicts of C so
that there are no conflicts left in C1 , the algorithm extracts a conflict from the sets C1 and
C2 . This way, QXP finally identifies individual constraints which are inconsistent with the
remaining consistent set of constraints and the background theory.
MXP builds on the ideas of QXP but computes multiple conflicts in one call (if they
exist). The general procedure is shown in Algorithm 8. After the initial consistency checks,
the method findConflicts is called, which returns a tuple xC 1 , y, where C 1 is a set of
remaining consistent constraints and  is a set of found conflicts. The function recursively
splits the set C of constraints in two halves. These parts are individually checked for
consistency, which allows us to exclude larger consistent subsets of C from the search process.
Besides the potentially identified conflicts, the calls to findConflicts also return two sets of
constraints which are consistent (C11 and C21 q. If the union of these two sets is not consistent,
we look for a conflict within C11 Y C11 (and the background theory) in the style of QXP.
More details can be found in our earlier work, where also the results of an in-depth
experimental analysis are reported (Shchekotykhin et al., 2015).
853

fiJannach, Schmitz, & Shchekotykhin

Algorithm 8: MergeXplain (MXP)
Input: A diagnosis problem (SD, Comps, Obs), a set visitedNodes of elements
Output: , a set of minimal conflicts
1 B  SD Y Obs Y {ab(c)|c P visitedNodes}; C  t abpcq|c P Compszu;
2 if
isConsistentpBq then return no solution;
3 if isConsistentpB Y Cq then return H;
4 x , y  findConflictspB, Cq
5 return tc| abpcq P };
6
7
8
9
10
11
12
13
14
15
16
17

function findConflicts (B, C) returns tuple xC 1 , y
if isConsistent(B Y C) then return xC, Hy;
if |C|  1 then return xH, tCuy;
Split C into disjoint, non-empty sets C1 and C2
xC11 , 1 y  findConflictspB, C1 q
xC21 , 2 y  findConflictspB, C2 q
  1 Y 2 ;
while isConsistentpC11 Y C21 Y Bq do
X  getConflictpB Y C21 , C21 , C11 q
CS  X Y getConflictpB Y X, X, C21 q
C11  C11 z tu where  P X
   Y tCS u
return xC11 Y C21 , y;

4.2 Evaluation
In this section we evaluate the effects of parallelizing the diagnosis process when we use
MXP instead of QXP to calculate the conflicts. As in (Shchekotykhin et al., 2015) we
focus on finding a limited set of (five) minimal diagnoses.
4.2.1 Implementation Variants
Using MXP during parallel tree construction implicitly means that more time is allocated
for conflict generation than when using QXP before proceeding to the next node. To
analyze to which extent the use of MXP is beneficial we tested three different strategies of
using MXP within the full parallelization method FP.
Strategy (1): In this configuration we simply called MXP instead of QXP during node
generation. Whenever MXP finds a conflict, it is added to the global list of known conflicts
and can be (re-)used by other parallel threads. The thread that executes MXP during node
generation continues with the next node when MXP returns.
Strategy (2): This strategy implements a variant of MXP which is slightly more complex.
Once MXP finds the first conflict, the method immediately returns this conflict such that
the calling thread can continue exploring additional nodes. At the same time, a new background thread is started which continues the search for additional conflicts, i.e., it completes
the work of the MXP call. In addition, whenever MXP finds a new conflict it checks if
any other already running node generation thread could have reused the conflict if it had
854

fiParallel Model-Based Diagnosis on Multi-Core Computers

been available beforehand. If this is the case, the search for conflicts of this other thread
is stopped as no new conflict is needed anymore. Strategy (2) could in theory result in
better CPU utilization, as we do not have to wait for a MXP call to finish before we can
continue building the HS-tree. However, the strategy also leads to higher synchronization
costs between the threads, e.g., to notify working threads about newly identified conflicts.
Strategy (3): Finally, we parallelized the conflict detection procedure itself. Whenever
the set C of constraints is split into two parts, the first recursive call of findConflicts is
queued for execution in a thread pool and the second call is executed in the current thread.
When both calls are finished, the algorithm continues.
We experimentally evaluated all three configurations on our benchmark datasets. Our
results showed that Strategy (2) did not lead to measurable performance improvements
when compared to Strategy (1). The additional communication costs seem to be higher
than what can be saved by executing the conflict detection process in the background in its
own thread. Strategy (3) can be applied in combination with the other strategies, but similar
to the experiments reported for the sequential HS-tree construction (Shchekotykhin et al.,
2015), no additional performance gains could be observed due to the higher synchronization
costs. The limited effectiveness of Strategies (2) and (3) can in principle be caused by the
nature of our benchmark problems and these strategies might be more advantageous in
different problem settings. In the following, we will therefore only report the results of
applying Strategy (1).
4.2.2 Results for the DXC Benchmark Problems
The results for the DXC benchmarks are shown in Table 4. The left side of the table shows
the results when using QXP and the right hand side shows the results for MXP. The
speedups shown in the FP columns refer to the respective sequential algorithms using the
same conflict detection technique.
Using MXP instead of QXP is favorable when using a sequential HS-tree algorithm
as also reported in the work about MXP (Shchekotykhin et al., 2015). The reduction of
running times ranges from 17% to 44%. The speedups obtained through FP when using
MXP are comparable to FP using QXP and range from 1.33 to 2.10, i.e., they lead to a
reduction of the running times of up to 52%. These speedups were achieved in addition to
the speedups that the sequential algorithm using MXP could already achieve over QXP.
The best results are printed in bold face in Table 4 and using MXP in combination
with FP consistently performs best. Overall, using FP in combination with MXP was
38% to 76% faster than the sequential algorithm using QXP. These tests indicate that
our parallelization method works well also for conflict detection techniques that are more
complex than QXP and, as in this case, return more than one conflict for each call. In
addition, investing more time for conflict detection in situations where the goal is to find a
few leading diagnoses proves to be a promising strategy.
4.2.3 Additional Experiments and Discussion
Again we ran additional experiments on constraint problems and ontology debugging problems. The detailed results are provided in Section A.2.
855

fiJannach, Schmitz, & Shchekotykhin

System
74182
74L85
74283
74181
c432

Seq.(QXP)
[ms]
12
15
49
699
3,714

FP(QXP)
S4
E4
1.26 0.32
1.36 0.34
1.58 0.39
1.99 0.55
1.77 0.44

Seq.(MXP)
[ms]
10
12
35
394
2,888

FP(MXP)
S4
E4
1.52 0.38
1.33 0.33
1.48 0.37
2.10 0.53
1.72 0.43

Table 4: Observed performance gains for the DXC benchmarks (QXP vs MXP).

Overall, the results obtained when embedding MXP in the sequential algorithm confirm
the results by Shchekotykhin et al. (2015) that using MXP is favorable over QXP for all
but a few very small problem instances. However, we can also observe that allocating more
time for conflict detection with MXP in a parallel processing setup can help to further
speedup the diagnosis process when we search for a number of leading diagnoses. The bestperforming configuration across all experiments is using the Full Parallelization method in
combination with MXP as this setup led to the shortest computation times in 20 out of
the 25 tested scenarios (DX benchmarks, CSPs, ontologies).

5. Parallelized Depth-First and Hybrid Search
In some application domains of MBD, finding all minimal diagnoses is either not required
or simply not possible because of the computational complexity or application-specific constraints on the allowed response times. For such settings, a number of algorithms have been
proposed over the years, which for example try to find one or a few minimal diagnoses very
quickly or find all diagnoses of a certain cardinality (Metodi, Stern, Kalech, & Codish, 2014;
Feldman, Provan, & van Gemund, 2010b; de Kleer, 2011). In some cases, the algorithms
can in principle be extended or used to find all diagnoses. They are, however, not optimized
for this task.
Instead of analyzing the various heuristic, stochastic or approximative algorithms proposed in the literature individually with respect to their potential for parallelization, we will
analyze in the next section if parallelization can be helpful already for the simple class of
depth-first algorithms. In that context, we will also investigate if measurable improvements
can be achieved without using any (domain-specific) heuristic. Finally, we will propose a
hybrid strategy which combines depth-first and full-parallel HS-tree construction and will
conduct additional experiments to assess if this strategy can be advantageous for the task
of quickly finding one minimal diagnosis.
5.1 Parallel Random Depth-First Search
The section introduces a parallelized depth-first search algorithm to quickly find one single
diagnosis. As the different threads explore the tree in a partially randomized form, we call
the scheme Parallel Random Depth-First Search (PRDFS).
856

fiParallel Model-Based Diagnosis on Multi-Core Computers

5.1.1 Algorithm Description
Algorithm 9 shows the main program of a recursive implementation of PRDFS. Similar to
the HS-tree algorithm, the search for diagnoses is guided by conflicts. This time, however,
the algorithm greedily searches in a depth-first manner. Once a diagnosis is found, it
has to be checked for minimality because the diagnosis can contain redundant elements.
The minimization of a non-minimal diagnosis can be achieved by calling a method like
Inv-QuickXplain (Shchekotykhin et al., 2014) or by simply trying to remove one element
of the diagnosis after the other and checking if the resulting set is still a diagnosis.
Algorithm 9: diagnosePRDFS: Parallelized random depth-first search.
Input: A diagnosis problem (SD, Comps, Obs),
the number minDiags of diagnoses to find
Result: The set  of diagnoses
1
2
3
4
5
6
7
8

  H; conflicts  H;
rootNode = getRootNode(SD, Comps, Obs);
for i  1 to nbThreads do
threads.execute(expandPRDFS(rootNode, minDiags, , conflicts));
while ||  minDiags do
wait();
threads.shutdownNow();
return ;

The idea of the parallelization approach in the algorithm is to start multiple threads
from the root node. All of these threads perform the depth-first search in parallel, but pick
the next conflict element to explore in a randomized manner.
The logic for expanding a node is shown in Algorithm 10. First, the conflict of the
given node is copied, so that changes to this set of constraints will not affect the other
threads. Then, as long as not enough diagnoses were found, a randomly chosen constraint
from the current nodes conflict is used to generate a new node. The expansion function is
then immediately called recursively for the new node, thereby implementing the depth-first
strategy. Any identified diagnosis is minimized before being added to the list of known
diagnoses. Similar to the previous parallelization schemes, the access to the global lists of
known conflicts has to be made thread-safe. When the specified number of diagnoses is
found or all threads are finished, the statement threads.shutdownNow() immediately stops
the execution of all threads that are still running and the results are returned. The semantics
of threads.execute(), wait(), and notify() are the same as in Section 3.
5.1.2 Example
Let us apply the depth-first method to the example from Section 2.2.1. Remember that the
two conflicts for this problem were ttC1, C2, C3u, tC2, C4uu. A partially expanded tree for
this problem can be seen in Figure 2.
857

fiJannach, Schmitz, & Shchekotykhin

Algorithm 10: expandPRDFS: Parallel random depth-first node expansion.
Input: An existingNode to expand, the number minDiags of diagnoses to find,
the sets  and conflicts
1
2
3
4
5
6
7
8
9
10
11
12

13
14
15
16
17
18

C = existingNode.conflict.clone();
// Copy existingNodes conflict
while ||  minDiags ^|C|  0 do
Randomly pick a constraint c from C
C  Cztcu;
newPathLabel = existingNode.pathLabel Y {c};
node = new Node(newPathlabel);
if D S P conflicts : S X newPathLabel  H then
node.conflict = S;
else
node.conflict = checkConsistency(SD, Comps, Obs, node.pathLabel);
if node.conflict  H then
// New conflict found
conflicts = conflicts Y node.conflict;
// Recursive call implements the depth-first search strategy
expandPRDFS(node, minDiags, , conflicts);
else
// Diagnosis found
diagnosis = minimize(node.pathLabel);
   Y {diagnosis};
if ||  minDiags then
notify();

In the example, first the root node 1 is created and again the conflict tC1, C2, C3u
is found. Next, the random expansion would, for example, pick the conflict element C1
and generate node 2 . For this node, the conflict tC2, C4u will be computed because tC1u
alone is not a diagnosis. Since the algorithm continues in a depth-first manner, it will
then pick one of the label elements of node 2 , e.g., C2 and generate node 3 . For this
node, the consistency check succeeds, no further conflict is computed and the algorithm has
found a diagnosis. The found diagnosis tC1, C2u is, however, not minimal as it contains
the redundant element C1. The function Minimize, which is called at the end of Algorithm
10, will therefore remove the redundant element to obtain the correct diagnosis tC2u.
If we had used more than one thread in this example, one of the parallel threads would
have probably started expanding the root node using the conflict element C2 (node 4 ). In
that case, the single element diagnosis tC2u would have been identified already at the first
level. Adding more parallel threads can therefore help to increase the chances to find one
hitting set faster as different parts of the HS-tree are explored in parallel.
Instead of the random selection strategy, more elaborate schemes to pick the next nodes
are possible, e.g., based on application-specific heuristics or fault probabilities. One could
also better synchronize the search efforts of the different threads to avoid duplicate calculations. We conducted experiments with an algorithm variant that used a shared and
858

fiParallel Model-Based Diagnosis on Multi-Core Computers

1
{C1,C2, C3}
C1

2

4

C2

C3

{C2,C4}
3

C2

{C2,C4}

C4

C2

C4

Figure 2: Example for HS-tree construction with PRDFS.
synchronized list of open nodes to avoid that two threads generate an identical sub-tree in
parallel. We did, however, not observe significantly better results than with the method
shown in Algorithm 9 probably due to the synchronization overhead.
5.1.3 Discussion of Soundness and Completeness
Every single thread in the depth-first algorithm systematically explores the full search space
based on the conflicts returned by the Theorem Prover. Therefore, all existing diagnoses
will be found when the parameter minDiags is equal or higher than the number of actually
existing diagnoses.
Whenever a (potentially non-minimal) diagnosis is encountered, the minimization process ensures that only minimal diagnoses are stored in the list of diagnoses. The duplicate
addition of the same diagnosis by one or more threads in the last lines of the algorithm is
prevented because we consider diagnoses to be equal if they contain the same set of elements
and  as a set by definition cannot contain the same element twice.
Overall, the algorithm is designed to find one or a few diagnoses quickly. The computation of all minimal diagnoses is possible with the algorithm but highly inefficient, e.g., due
to the computational costs of minimizing the diagnoses.
5.2 A Hybrid Strategy
Let us again consider the problem of finding one minimal diagnosis. One can easily imagine
that the choice of the best parallelization strategy, i.e., breadth-first or depth-first, can
depend on the specifics of the given problem setting and the actual size of the existing
diagnoses. If a single-element diagnosis exists, exploring the first level of the HS-tree in a
breadth-first approach might be the best choice (see Figure 3(a)). A depth-first strategy
might eventually include this element in a non-minimal diagnosis, but would then have to
do a number of additional calculations to ensure the minimality of the diagnosis.
If, in contrast, the smallest actually existing diagnosis has a cardinality of, e.g., five,
the breadth-first scheme would have to fully explore the first four HS-tree levels before
finding the five-element diagnosis. The depth-first scheme, in contrast, might quickly find
859

fiDiagnosis detected

Jannach, Schmitz, & Shchekotykhin

a superset of the five-element diagnosis, e.g., with six elements, and then only needs six
additional consistency checks to remove the redundant element from the diagnosis (Figure
3(b)).
Diagnosis detected

Diagnosis detected

(a) Breadth-first strategy is advantageous.

(b) Depth-first strategy is advantageous.

Figure 3: Two problem configurations for which different search strategies are favorable.
Since we cannot know the cardinality of the diagnoses in advance, we propose a hybrid
strategy, in which half of the threads adopt a depth-first strategy and the other half uses
the fully parallelized breadth-first regime. To implement this strategy, the Algorithms 5
(FP) and 9 (PRDFS) can be started in parallel and each algorithm is allowed to use one
half or some other defined share of the available threads. The coordination between the
two algorithms can be done with the help of shared data structures that contain the known
conflicts and diagnoses. When enough diagnoses (e.g. one) are found, all running threads
can be terminated
and the results are returned.
Diagnosis detected
5.3 Evaluation
We evaluated the different strategies for efficiently finding one minimal diagnosis on the
same set of benchmark problems that were used in the previous sections. The experiment
setup was identical except that the goal was to find one arbitrary diagnosis and that we
included the additional depth-first algorithms. In order to measure the potential benefits of
parallelizing the depth-first search, we ran the benchmarks for PRDFS both with 4 threads
and with 1 thread, where the latter setup corresponds to a Random Depth First Search
(RDFS) without parallelization.
5.3.1 Results for the DXC Benchmark Problems
The results for the DXC benchmark problems are shown in Table 5. Overall, for all tested
systems, each of the approaches proposed in this paper can help to speed up the process
of finding one single diagnosis. In 88 of the 100 evaluated scenarios at least one of the
tested approaches was statistically significantly faster than the sequential algorithm. For
the other 12 scenarios, finding one single diagnosis was too simple so that only modest but
no significant speedups compared to the sequential algorithm were obtained.
When comparing the individual parallel algorithms, the following observations can be
made:
860

fiParallel Model-Based Diagnosis on Multi-Core Computers

 For most of the examples, the PRDFS method is faster than the breadth-first search
implemented in the FP technique. For one benchmark system, the PRDFS approach
can even achieve a speedup of 11 compared to the sequential algorithm, which corresponds to a runtime reduction of 91%.
 When compared with the non-parallel RDFS, PRDFS could achieve higher speedups
for all tested systems except the most simple one, which only took 16 ms even for the
sequential algorithm. Overall, parallelization can therefore be advantageous also for
depth-first strategies.
 The performance of the Hybrid strategy lies in between the performances of its components PRDFS and FP for 4 of the 5 tested systems. For these systems, it is closer to
the faster one of the two. Adopting the hybrid strategy can therefore represent a good
choice when the structure of the problem is not known in advance, as it combines both
ideas of breadth-first and depth-first search and is able to quickly find a diagnosis for
problem settings with unknown characteristics.

System
74182
74L85
74283
74181
c432

Seq.
[ms]
16
13
54
691
2,789

FP
S4
E4
1.37 0.34
1.34 0.33
1.67 0.42
2.08 0.52
1.89 0.47

RDFS
[ms]
9
11
25
74
1,435

PRDFS
S4
E4
0.84 0.21
1.06 0.27
1.22 0.31
1.23 0.31
2.96 0.74

Hybrid
S4
E4
0.84 0.21
1.05 0.26
1.06 0.26
1.04 0.26
1.81 0.45

Table 5: Observed performance gains for DXC benchmarks for finding one diagnosis.

5.3.2 Additional Experiments
The detailed results obtained through additional experiments are again provided in the
appendix. The measurements include the results for CSPs (Section A.3.1) and ontologies
(Section A.3.2), as well as results that were obtained by systematically varying the characteristics of synthetic diagnosis problems (Section A.3.3). The results indicate that applying
a depth-first parallelization strategy in many cases is advantageous for the CSP problems.
The tests on the ontology problems and the simulation results however reveal that depending on the problem structure there are cases in which a breadth-first strategy can be more
beneficial.
5.3.3 Discussion
The experiments show that the parallelization of the depth-first search strategy (PRDFS)
can help to further reduce the computation times when we search for one single diagnosis.
In most evaluated cases, PRDFS was faster than its sequential counterpart. In some
cases, however, the obtained improvements were quite small or virtually non-existent, which
can be explained as follows.
861

fiJannach, Schmitz, & Shchekotykhin

 For the very small scenarios, the parallel depth-first search cannot be significantly
faster than the non-parallel variant because the creation of the first node is not parallelized. Therefore a major fraction of the tree construction process is not parallelized
at all.
 There are problem settings in which all existing diagnoses have the same size. All
parallel depth-first searching threads therefore have to explore the tree to a certain
depth and none of the threads can immediately return a diagnosis that is much smaller
than one determined by another thread. E.g., given a diagnosis problem, where all
diagnoses have size 5, all threads have to explore the tree to at least level 5 to find a
diagnosis and are also very likely to find a diagnosis on that level. Therefore, in this
setting no thread can be much faster than the others.
 Finally, we again suspect problems of cache contention and a correspondingly increased number of cache misses, which leads to a general performance deterioration
and overhead caused by the multiple threads.
Overall, the obtained speedups again depend on the problem structure. The hybrid
technique represents a good compromise for most cases as it is faster than the sequential
breadth first search approach for most of the tested scenarios (including the CSPs, ontologies, and synthetically created diagnosis problems presented in Section A.3). Also, it
is more efficient than PRDFS in some cases for which breadth first search is better than
depth first search.

6. Parallel Direct CSP Encodings
As an alternative to conflict-guided diagnosis approaches like Reiters hitting set technique,
so-called direct encodings have become more popular in the research community in recent
years (Feldman, Provan, de Kleer, Robert, & van Gemund, 2010a; Stern, Kalech, Feldman,
& Provan, 2012; Metodi et al., 2014; Mencia & Marques-Silva, 2014; Menca, Previti, &
Marques-Silva, 2015; Marques-Silva, Janota, Ignatiev, & Morgado, 2015).10
The general idea of direct encodings is to generate a specific representation of a diagnosis
problem instance with some knowledge representation language and then use the theorem
prover (e.g., a SAT solver or constraint engine) to compute the diagnoses directly. These
methods support the generation of one or multiple diagnoses by calling a theorem prover
only once. Nica, Pill, Quaritsch, and Wotawa (2013) made a number of experiments in
which they compared conflict-directed search with such direct encodings and showed that
for several problem settings, using the direct encoding was advantageous.
In this part of the paper, our goal is to evaluate whether the parallelization of the search
process  in that case inside the constraint engine  can help to improve the efficiency of
the diagnostic reasoning process. The goal of this chapter is therefore rather to quantify
to which extent the internal parallelization of a solver is useful than to present a new
algorithmic contribution.
10. Such direct encodings may not always be possible in MBD settings as discussed above.

862

fiParallel Model-Based Diagnosis on Multi-Core Computers

6.1 Using Gecode as a Solver for Direct Encodings
For our evaluation we use the Gecode constraint solver (Schulte, Lagerkvist, & Tack, 2016).
In particular, we use the parallelization option of Gecode to test its effects on the diagnosis
running times.11 The chosen problem encoding is similar to the one used by Nica and
Wotawa (2012). This allows us to make our results comparable with those obtained in
previous works. In addition, the provided encoding is represented in a language which is
supported by multiple solvers.
6.1.1 Example
Let us first show the general idea on a small example. Consider the following CSP12
consisting of the integer variables a1, a2, b1, b2, c1 and the constraints X1 , X2 , and X3
which are defined as:
X1 : b1  a1  2, X2 : b2  a2  3, X3 : c1  b1  b2.
Let us assume that the programmer made a mistake and X3 should actually be c1 
b1 ` b2. Given a set of expected observations (a test case) a1  1, a2  6, d1  20, MBD
can be applied by considering the constraints as the possibly faulty components.
In a direct encoding the given CSP is extended with a definition of an array AB 
rab1 , ab2 , ab3 s of boolean (0/1) variables which encode whether a corresponding constraint
is considered as faulty or not. The constraints are rewritten as follows:
X11 : ab1 _ pb1  a1  2q,

X21 : ab2 _ pb2  a2  3q,

X31 : ab3 _ pc1  b1  b2q.

The observations can be encoded through equality constraints which bind the values of
the observed variables. In our example, these constraints would be:
O1 : a1  1,

O2 : a2  6,

O3 : d1  20

In order to find a diagnosis of cardinality 1, we additionally add the constraint
ab1 ` ab2 ` ab3  1
and let the solver search for a solution. In this case, X3 would be identified as the only
possible diagnosis, i.e., ab3 would be set to 1 by the solver.
6.1.2 Parallelization Approach of Gecode
When using such a direct encoding, a parallelization of the diagnosis process, as shown
for Reiters approach, cannot be done because it is embedded in the underlying search
procedure. However, modern constraint solvers, such as Gecode, or-tools and many other
solvers of those that participated in the MiniZinc Challenge (Stuckey, Feydy, Schutt, Tack,
& Fischer, 2014), internally implement parallelization strategies to better utilize todays
multi-core computer architectures (Michel, See, & Van Hentenryck, 2007; Chu, Schulte, &
11. A state-of-the-art SAT solver capable of parallelization could have been used for this analysis as well.
12. Adapted from an earlier work (Jannach & Schmitz, 2014).

863

fiJannach, Schmitz, & Shchekotykhin

Stuckey, 2009). In the following, we will therefore evaluate through a set of experiments, if
these solver-internal parallelization techniques can help to speed up the diagnosis process
when a direct encoding is used.13
Gecode implements an adaptive work stealing strategy (Chu et al., 2009) for its parallelization. The general idea can be summarized as follows. As soon as a thread finishes
processing its nodes of the search tree, it steals some of the nodes from non-idle threads.
In order to decide from which thread the work should be stolen, an adaptive strategy uses
balancing heuristics that estimate the density of the solutions in a particular part of the
search tree. The higher the likelihood of containing a solution for a given branch, the more
work is stolen from this branch.
6.2 Problem Encoding
In our evaluation we use MiniZinc as a constraint modeling language. This language can
be processed by different solvers and allows us to model diagnosis problems as CSPs as
shown above.
6.2.1 Finding One Diagnosis
To find a single diagnosis for a given diagnosis problem (SD, Comps, Obs), we generate a
direct encoding in MiniZinc as follows.
(1) For the set of components Comps we generate an array ab = [ab1 , . . . , abn ] of
boolean variables.
(2) For each formula sdi P SD we add a constraint of the form
constraint abris _ psdi q;
and for each observation oj P Obs the model is extended with a constraint
constraint oj ;
(3) Finally, we add the search goal and an output statement:
solve minimize sumpi in 1..nqpbool2intpabrisqq;
output[show(ab)];
The first statement of the last part (solve minimize), instructs the solver to search for
a (single) solution with a minimal number of abnormal components, i.e., a diagnosis with
minimum cardinality. The second statement (output) projects all assignments to the set
of abnormal variables, because we are only interested in knowing which components are
faulty. The assignments of the other problem variables are irrelevant.
6.2.2 Finding All Diagnoses
The problem encoding shown above can be used to quickly find one/all diagnoses of minimum cardinality. It is, however, not sufficient for scenarios where the goal is to find all
diagnoses of a problem. We therefore propose the following sound and complete algorithm
which repeatedly modifies the constraint problem to systematically identify all diagnoses.
13. In contrast to the parallelization approaches presented in the previous sections, we do not propose any
new parallelization schemes here but rather rely on the existing ones implemented in the solver.

864

fiParallel Model-Based Diagnosis on Multi-Core Computers

Technically, the algorithm first searches for all diagnoses of size 1 and then increases the
desired cardinality of the diagnoses step by step.

Algorithm 11: directDiag: Computation of all diagnoses using a direct encoding.
Input: A diagnosis problem (SD, Comps, Obs), maximum cardinality k
Result: The set  of diagnoses
1
2
3
4
5
6
7
8
9
10

  H; C  H; card  1;
if k  |Comps| then k  |Comps|;
M = generateModel (SD, Comps, Obs);
while card  k do
M = updateModel (M, card , C);
1  computeDiagnosespMq;
C  C Y generateConstraintsp1 q;
   Y 1 ;
card  card ` 1;
return ;

Procedure Algorithm 10 shows the main components of the direct diagnosis method used
in connection with a parallel constraint solver to find all diagnoses. The algorithm starts
with the generation of a MiniZinc model (generateModel) as described above. The
only difference is that we will now search for all solutions of a given cardinality; further
details about the encoding of the search goals are given below.
In each iteration, the algorithm modifies the model by updating the cardinality of the
searched diagnoses and furthermore adds new constraints corresponding to the already
found diagnoses (updateModel). This updated model is then provided to a MiniZinc
interpreter (constraint solver), which returns a set of solutions 1 . Each element i P 1
corresponds to a diagnosis of the cardinality card .
In order to exclude supersets of the already found diagnoses 1 in future iterations, we
generate a constraint for each i P 1 with the formulas j to l (generateConstraints):
constraint abrjs  false _    _ abrls  false;
These constraints ensure that an already found diagnosis or supersets of it cannot be found
again. They are added to the model M in the next iteration of the main loop. The algorithm
continues until all diagnoses with cardinalities up to k are computed.
Changes in Encoding To calculate all diagnoses of a given size, we first instruct the
solver to search for all possible solutions when provided with a constraint problem.14 In
addition, while keeping steps (1) and (2) from Section 6.2.1 we replace the lines of step (3)
14. This is achieved by calling MiniZinc with the --all-solutions flag.

865

fiJannach, Schmitz, & Shchekotykhin

by the following statements:
constraint sumpi in 1..nqpbool2intpabrisqq  card ;
solve satisfy;
output[show(ab)];
The first statement constrains the number of abnormal variables that can be true to a
certain value, i.e., the given cardinality card. The second statement tells the solver to find
all variable assignments that satisfy the constraints. The last statement again guarantees
that the solver only considers the solutions to be different when they are different with
respect to the assignments of the abnormal variables.
Soundness and Completeness Algorithm 10 implements an iterative deepening approach which guarantees the minimality of the diagnoses in . Specifically, the algorithm
constructs diagnoses in the order of increasing cardinality by limiting the number of ab
variables that can be set to true in a model. The computation starts with card  1, which
means that only one ab variable can be true. Therefore, only diagnoses of cardinality 1,
i.e., comprising only one abnormal variable, can be returned by the solver. For each found
diagnosis we then add a constraint that requires at least one of the abnormal variables of
this diagnosis to be false. Therefore, neither this diagnosis nor its supersets can be found
in the subsequent iterations. These constraints implement the pruning rule of the HS-tree
algorithm. Finally, Algorithm 10 repeatedly increases the cardinality parameter card by
one and continues with the next iteration. The algorithm continues to increment the cardinality until card becomes greater than the number of components, which corresponds to
the largest possible cardinality of a diagnosis. Consequently, given a diagnosis problem as
well as a sound and complete constraint solver, Algorithm 10 returns all diagnoses of the
problem.
6.3 Evaluation
To evaluate if speedups can be achieved through parallelization also for a direct encoding,
we again used the first five systems of the DXC Synthetic Track and tested all scenarios
using the Gecode solver without parallelization and with 2 and 4 parallel threads.
6.3.1 Results
We evaluated two different configurations. In setup (A), the task was to find one single
diagnosis of minimum cardinality. In setup (B), the iterative deepening procedure from
Section 6.2.2 was used to find all diagnoses up to the size of the actual error.
The results for setup (A) are shown in Table 6. We can observe that using the parallel
constraint solver pays off except for the tiny problems for which the overall search time is
less than 200 ms. Furthermore, adding more worker threads is also beneficial for the larger
problem sizes and a speedup of up to 1.25 was achieved for the most complex test case
which took about 1.5 seconds to solve.
The same pattern can be observed for setup (B). The detailed results are listed in Table
7. For the tiny problems, the internal parallelization of the Gecode solver does not lead
to performance improvements but slightly slows down the whole process. As soon as the
866

fiParallel Model-Based Diagnosis on Multi-Core Computers

problems become more complex, parallelization pays off and we can observe a speedup of
1.55 for the most complex of the tested cases, which corresponds to a runtime reduction of
35%.
System
74182
74L85
74283
74181
c432

Direct Encoding
Abs. [ms]
S2
E2
S4
27 0.85 0.42 0.79
30 0.89 0.44 0.79
32 0.85 0.43 0.79
200 1.04 0.52 1.15
1,399 1.17 0.58 1.25

E4
0.20
0.20
0.20
0.29
0.31

Table 6: Observed performance gains for DXC benchmarks for finding one diagnosis with
a direct encoding using one (column Abs.), two, and four threads.

System
74182
74L85
74283
74181
c432

Direct Encoding
Abs. [ms]
S2
E2
S4
136 0.84 0.42 0.80
60 0.83 0.41 0.77
158 0.93 0.47 0.92
1,670 1.19 0.59 1.33
229,869 1.22 0.61 1.55

E4
0.20
0.19
0.23
0.33
0.39

Table 7: Observed performance gains for DXC benchmarks for finding all diagnoses with a
direct encoding using one (column Abs.), two, and four threads.

6.3.2 Summary and Remarks
Overall, our experiments show that parallelization can be beneficial when a direct encoding
of the diagnosis problem is employed, in particular when the problems are non-trivial.
Comparing the absolute running times of our Java implementation using the open source
solver Choco with the optimized C++ implementation of Gecode is generally not appropriate and for most of the benchmark problems, Gecode works faster on an absolute scale.
Note, however, that this is not true in all cases. In particular when searching for all diagnoses up to the size of the actual error for the most complex system c432, even Reiters
non-parallelized Hitting Set algorithm was much faster (85 seconds) than using the direct
encoding based on iterative deepening (230 seconds). This is in line with the observation
of Nica et al. (2013) that direct encodings are not always the best choice when searching
for all diagnoses.
A first analysis of the run-time behavior of Gecode shows that the larger the problem is,
the more time is spent by the solver in each iteration to reconstruct its internal structures,
which can lead to a measurable performance degradation. Note that in our work we relied
on a MiniZinc encoding of the diagnosis problem to be independent of the specifics of the
867

fiJannach, Schmitz, & Shchekotykhin

underlying constraint engine. An implementation that relies on the direct use of the API of
a specific CSP solver might help to address certain performance issues. Nevertheless, such
an implementation must be solver-specific and will not allow us to switch solvers easily as
it is now possible with MiniZinc..

7. Relation to Previous Works
In this section we explore works that are related to our approach. First we examine different
approaches for the computation of diagnoses. Then we will focus on general methods for
parallelizing search algorithms.
7.1 Computation of Diagnoses
Computing minimal hitting sets for a given set of conflicts is a computationally hard problem
as already discussed in Section 2.2.2 and several approaches were proposed over the years to
deal with the issue. These approaches can be divided into exhaustive and approximate ones.
The former perform a sound and complete search for all minimal diagnoses, whereas the
latter improve the computational efficiency in exchange for completeness, e.g., they search
for only one or a small set of diagnoses.
Approximate approaches can for example be based on stochastic search techniques like
genetic algorithms (Li & Yunfei, 2002) or greedy stochastic search (Feldman et al., 2010b).
The greedy method proposed by Feldman et al. (2010b), for example, uses a two-step
approach. In the first phase, a random and possibly non-minimal diagnosis is determined
by a modified DPLL15 algorithm. The algorithm always finds one random diagnosis at each
invocation due to the random selection of propositional variables and their assignments. In
the second step, the algorithm minimizes the diagnosis returned by the DPLL technique by
repeatedly applying random modifications. It randomly chooses a negative literal which
denotes that a corresponding component is faulty and flips its value to positive. The
obtained candidate as well as the diagnosis problem are provided to the DPLL algorithm
to check whether the candidate is a diagnosis or not. In case of success the obtained
diagnosis is kept and another random flip is done. Otherwise, the negative literal is labeled
with failure and another negative literal is randomly selected. The algorithm stops if the
number of failures is greater than some predefined constant and returns the best diagnosis
found so far.
In the approach of Li and Yunfei (2002) a genetic algorithm takes a number of conflict
sets as input and generates a set of bit-vectors (chromosomes), where every bit encodes a
truth value of an atom over the ab(.) predicate. In each iteration the algorithm applies
genetic operations, such as mutation, crossover, etc., to obtain new chromosomes. Subsequently, all obtained bit-vectors are evaluated by a hitting set fitting function which
eliminates bad candidates. The algorithm stops after a predefined number of iterations and
returns the best diagnosis.
In general, such approximate approaches are not directly comparable with our LWP and
FP techniques, since they are incomplete and do not guarantee the minimality of returned
15. Davis-Putnam-Logemann-Loveland.

868

fiParallel Model-Based Diagnosis on Multi-Core Computers

hitting sets. Our goal in contrast is to improve the performance while at the same time
maintaining both the completeness and the soundness property.
Another way of finding approximate solutions is to use heuristic search approaches. For
example, Abreu and van Gemund (2009) proposed the Staccato algorithm which applies
a number of heuristics for pruning the search space. More aggressive pruning techniques
result in better performance of the search algorithms. However, they also increase the probability that some of the diagnoses will not be found. In this approach the aggressiveness
of the heuristics can be varied by input parameters depending on the application goals.
More recently, Cardoso and Abreu (2013) suggested a distributed version of the Staccato algorithm, which is based on the Map-Reduce scheme (Dean & Ghemawat, 2008) and
can therefore be executed on a cluster of servers. Other more recent algorithms focus on
the efficient computation of one or more minimum cardinality (minc) diagnoses (de Kleer,
2011). Both in the distributed approach and in the minimum cardinality scenario, the assumption is that the (possibly incomplete) set of conflicts is already available as an input
at the beginning of the hitting-set construction process. In the application scenarios that
we address with our work, finding the conflicts is considered to be the computationally
expensive part and we do not assume to know the minimal conflicts in advance but have
to compute them on-demand as also done in other works (Felfernig, Friedrich, Jannach,
Stumptner, et al., 2000; Friedrich & Shchekotykhin, 2005; Williams & Ragno, 2007); see also
the work by Pill, Quaritsch, and Wotawa (2011) for a comparison of conflict computation
approaches.
Exhaustive approaches are often based on HS-trees like the work of Wotawa (2001a) 
a tree construction algorithm that reduces the number of pruning steps in presence of nonminimal conflicts. Alternatively, one can use methods that compute diagnoses without the
explicit computation of conflict sets, i.e., by solving a problem dual to minimal hitting sets
(Satoh & Uno, 2005). Stern et al. (2012), for example, suggest a method that explores the
duality between conflicts and diagnoses and uses this symmetry to guide the search. Other
approaches exploit the structure of the underlying problem, which can be hierarchical (Autio
& Reiter, 1998), tree-structured (Stumptner & Wotawa, 2001), or distributed (Wotawa &
Pill, 2013). These algorithms are very similar to the HS-tree algorithm and, consequently,
can be parallelized in a similar way. As an example, consider the Set-Enumeration Tree
(SE-tree) algorithm (Rymon, 1994). This algorithm, similarly to Reiters HS-tree approach,
uses breadth-first search with a specific expansion procedure that implements the pruning
and node selection strategies. Both the LWP and and the FP parallelization variant can be
used with the SE-tree algorithm and comparable speedups are expected.
7.2 Parallelization of Search Algorithms
Historically, the parallelization of search algorithms was approached in three different ways
(Burns, Lemons, Ruml, & Zhou, 2010):
(i) Parallelization of node processing: When applying this type of parallelization, the tree
is expanded by one single process, but the computation of labels or the evaluation of
heuristics is done in parallel.
869

fiJannach, Schmitz, & Shchekotykhin

(ii) Window-based processing: In this approach, sets of nodes, called windows, are processed by different threads in parallel. The windows are formed by the search algorithm
according to some predefined criteria.
(iii) Tree decomposition approaches: Here, different sub-trees of the search tree are assigned to different processes (Ferguson & Korf, 1988; Brungger, Marzetta, Fukuda, &
Nievergelt, 1999).
In principle, all three types of parallelization can be applied in some form to the HS-tree
generation problem.
Applying strategy (i) in the MBD problem setting would mean to parallelize the process
of conflict computation, e.g., through a parallel variant of QXP or MXP. We have tested
a partially parallelized version of MXP, which however did not lead to further performance
improvements when compared to a single-threaded approach on the evaluated benchmark
problems (Shchekotykhin et al., 2015). The experiments in Section 4 however show that
using MXP in combination with LWP or FP  thereby implicitly allocating more CPU time
for the computation of multiple conflicts during the construction of a single node  can be advantageous. Other well-known conflict or prime implicate computation algorithms (Junker,
2004; Marques-Silva et al., 2013; Previti, Ignatiev, Morgado, & Marques-Silva, 2015) in
contrast were not designed for parallel execution or the computation of multiple conflicts.
Strategy (ii)  computing sets of nodes (windows) in parallel  was for example applied by
Powley and Korf (1991). In their work the windows are determined by different thresholds
of a heuristic function of Iterative Deepening A*. Applying the strategy to an HS-tree
construction problem would mean to categorize the nodes to be expanded according to
some criterion, e.g., the probability of finding a diagnosis, and to allocate the different
groups to individual threads. In the absence of such window criteria, LWP and FP could be
seen as extreme cases with window size one, where each open node is allocated to one thread
on a processor. The experiments done throughout the paper suggest that independent of the
parallelization strategy (LWP or FP) the number of parallel threads (windows) should not
exceed the number of physically available computing threads to obtain the best performance.
Finally (iii), the strategy exploring different sub-trees during the search with different
processes can, for example, be applied in the context of MBD techniques when using Binary
HS-Tree (BHS) algorithms (Pill & Quaritsch, 2012). Given a set of conflict sets, the BHS
method generates a root node and labels it with the input set of conflicts. Then, it selects
one of the components occurring in the conflicts and generates two child nodes, such that
the left node is labeled with all conflicts comprising the selected component and the right
node with the remaining ones. Consequently, the diagnosis tree is decomposed into two subtrees and can be processed in parallel. The main problem for this kind of parallelization is
that the conflicts are often not known in advance and have to be computed during search.
Anglano and Portinale (1996) suggested another approach in which they ultimately
parallelized the diagnosis problem based on structural problem characteristics. In their
work, they first map a given diagnosis problem to a Behavioral Petri Net (BPN). Then,
the obtained BPN is manually partitioned into subnets and every subnet is provided to a
different Parallel Virtual Machine (PVM) for parallel processing. The relationship of their
work to our LWP and FP parallelization schemes is limited and our approaches also do not
require a manual problem decomposition step.
870

fiParallel Model-Based Diagnosis on Multi-Core Computers

In general, parallelized versions of domain-independent search algorithms like A can
be applied to MBD settings. However, the MBD problem has some specifics that make the
application of some of these algorithms difficult. For instance, the PRA method and its
variant HDA discussed in the work of Burns et al. (2010) use a mechanism to minimize the
memory requirements by retracting parts of the search tree. These forgotten parts are
later on re-generated when required. In our MBD setting, the generation of nodes is however
the most costly part, which is why the applicability of HDA seems limited. Similarly,
duplicate detection algorithms like PBNF (Burns et al., 2010) require the existence of an
abstraction function that partitions the original search space into blocks. In general MBD
settings, we however cannot assume that such a function is given.
In order to improve the performance we have therefore to avoid the parallel generation
of duplicate nodes by different threads, which we plan to investigate in our future work.
A promising starting point for this research could be the work by Phillips, Likhachev,
and Koenig (2014). The authors suggest a variant of the A* algorithm that generates only
independent nodes in order to reduce the costs of node generation. Two nodes are considered
as independent if the generation of one node does not lead to a change of the heuristic
function of the other node. The generation of independent nodes can be done in parallel
without the risk of the repeated generation of an already known state. The main difficulty
when adopting this algorithm for MBD is the formulation of an admissible heuristic required
to evaluate the independence of the nodes for arbitrary diagnosis problems. However, for
specific problems that can be encoded as CSPs, Williams and Ragno (2007) present a
heuristic that depends on the number of unassigned variables at a particular search node.
Finally, parallelization was also used in the literature to speed up the processing of very
large search trees that do not fit in memory. Korf and Schultze (2005), for instance, suggest
an extension of a hash-based delayed duplicate detection algorithm that allows a search
algorithm to continue search while other parts of the search tree are written to or read from
the hard drive. Such methods can in theory be used in combination with our LWP or FP
parallelization schemes in case of complex diagnosis problems. We plan to explore the use
of (externally) saved search states in the context of MBD as part of our future works.

8. Summary
In this work, we propose and systematically evaluate various parallelization strategies for
Model-Based Diagnosis to better exploit the capabilities of multi-core computers. We show
that parallelization can be advantageous in various problem settings and diagnosis approaches. These approaches include the conflict-driven search for all or a few minimal
diagnoses with different conflict detection techniques and the (heuristic) depth-first search
in order to quickly determine a single diagnosis. The main benefits of our parallelization
approaches are that they can be applied independent of the underlying reasoning engine and
for a variety of diagnostic problems which cannot be efficiently represented as SAT or CSP
problems. In addition to our HS-tree based parallelization approaches, we also show that
parallelization can be beneficial for settings in which a direct problem encoding is possible
and modern parallel solver engines are available.
Our evaluations have furthermore shown that the speedups of the proposed parallelization methods can vary according to the characteristics of the underlying diagnosis problem.
871

fiJannach, Schmitz, & Shchekotykhin

In our future work, we plan to explore techniques that analyze these characteristics in order
to predict in advance which parallelization method is best suited to find one single or all
diagnoses for the given problem.
Regarding algorithmic enhancements, we furthermore plan to investigate how information about the underlying problem structure can be exploited to achieve a better distribution of the work on the parallel threads and to thereby avoid duplicate computations.
Furthermore, we plan to explore the usage of parallel solving schemes for the dual algorithms, i.e., algorithms that compute diagnoses directly without the computation of minimal conflicts (Satoh & Uno, 2005; Felfernig, Schubert, & Zehentner, 2012; Stern et al.,
2012; Shchekotykhin et al., 2014).
The presented algorithms were designed for the use on modern multi-core computers
which today usually have less than a dozen cores. Our results show that the additional performance improvements that we obtain with the proposed techniques become smaller when
adding more and more CPUs. As part of our future works we therefore plan to develop
algorithms that can utilize specialized environments that support massive parallelization.
In that context, a future topic of research could be the adaption of the parallel HS-tree
construction to GPU architectures. GPUs, which can have thousands of computing cores,
have proved to be superior for tasks which can be parallelized in a suitable way. Campeotto,
Palu, Dovier, Fioretto, and Pontelli (2014) for example used a GPU to parallelize a constraint solver. However, it is not yet fully clear whether tree construction techniques can
be efficiently parallelized on a GPU, as many data structures have to be shared across all
nodes and access to them has to be synchronized.

Acknowledgements
This paper significantly extends and combines our previous work (Jannach, Schmitz, &
Shchekotykhin, 2015; Shchekotykhin et al., 2015).
We would like to thank Hakan Kjellerstrand and the Gecode team for their support. We
are also thankful for the various helpful comments and suggestions made by the anonymous
reviewers of JAIR, DX14, DX15, AAAI15, and IJCAI15.
This work was supported by the Carinthian Science Fund (KWF) contract KWF3520/26767/38701, the Austrian Science Fund (FWF) and the German Research Foundation (DFG) under contract numbers I 2144 N-15 and JA 2095/4-1 (Project Debugging
of Spreadsheet Programs).

Appendix A.
In this appendix we report the results of additional experiments that were made on different
benchmark problems as well as results of simulation experiments on artificially created
problem instances.
 Section A.1 contains the results for the LWP and FP parallelization schemes proposed
in Section 3.
 Section A.2 reports additional measurements regarding the use of MergeXplain
within the parallel diagnosis process, see Section 4.
872

fiParallel Model-Based Diagnosis on Multi-Core Computers

 Section A.3 finally provides additional results of the parallelization of the depth-first
strategies discussed in Section 5.
A.1 Additional Experiments for the LWP and FP Parallelization Strategies
In addition to the experiments with the DXC benchmark systems reported in Section 3.5,
we made additional experiments with Constraint Satisfaction Problems, ontologies, and
artificial Hitting Set construction problems. Furthermore, we examined the effects of further
increasing the number of available threads for the benchmarks of the CSPs and ontologies.
A.1.1 Diagnosing Constraint Satisfaction Problems
Data Sets and Procedure In this set of experiments we used a number of CSP instances
from the 2008 CP solver competition (Lecoutre, Roussel, & van Dongen, 2008) in which
we injected faults.16 The diagnosis problems were created as follows. We first generated
a random solution using the original CSP formulations. From each solution, we randomly
picked about 10% of the variables and stored their value assignments, which then served
as test cases. These stored variable assignments correspond to the expected outcomes when
all constraints are formulated correctly. Next, we manually inserted errors (mutations) in
the constraint problem formulations17 , e.g., by changing a less than operator to a more
than operator, which corresponds to a mutation-based approach in software testing. The
diagnosis task then consists of identifying the possibly faulty constraints using the partial
test cases. In addition to the benchmark CSPs we converted a number of spreadsheet
diagnosis problems (Jannach & Schmitz, 2014) to CSPs to test the performance gains on
realistic application settings.
Table 8 shows the problem characteristics including the number of injected faults (#F),
the number of diagnoses (#D), and the average diagnosis size (|D|). In general, we selected
CSPs which are quite diverse with respect to their size.
Results The measurement results using 4 threads and searching for all diagnoses are given
in Table 9. Improvements could be achieved for all problem instances. With the exception
of the smallest problem mknap-1-5 all speedups achieved by LWP and FP are statistically
significant. For some problems, the improvements are very strong (with a running time
reduction of over 50%), whereas for others the improvements are modest. On average, FP
is also faster than LWP. However, FP is not consistently better than LWP and often the
differences are small.
The observed results indicate that the performance gains depend on a number of factors
including the size of the conflicts, the computation times for conflict detection, and the
problem structure itself. While on average FP is faster than LWP, the characteristics of the
problem settings seem to have a considerable impact on the speedups that can be obtained
by the different parallelization strategies.
16. To be able to do a sufficient number of repetitions, we picked instances with comparably small running
times.
17. The mutated CSPs can be downloaded at http://ls13-www.cs.tu-dortmund.de/homepage/hp_
downloads/jair/csps.zip.

873

fiJannach, Schmitz, & Shchekotykhin

Scenario
c8
costasArray-13
domino-100-100
gracefulK3-P2
mknap-1-5
queens-8
hospital payment
profit calculation
course planning
preservation model
revenue calculation

#C
523
87
100
60
7
28
38
28
457
701
93

#V
239
88
100
15
39
8
75
140
583
803
154

#F
8
2
3
4
1
15
4
5
2
1
4

#D
4
2
81
117
2
9
120
42
3024
22
1452

|D|
6.25
2.5
2
2.94
1
10.9
3.8
4.24
2
1
3

Table 8: Characteristics of selected problem settings.
Scenario
c8
costasArray-13
domino-100-100
gracefulK3-P2
mknap-1-5
queens-8
hospital payment
profit calculation
course planning
preservation model
revenue calculation

Seq.(QXP)
[ms]
559
4,013
1,386
1,965
314
141
12,660
197
22,130
167
778

LWP(QXP)
S4
E4
1.10 0.27
2.16
0.54
3.08 0.77
2.75
0.69
1.03 0.26
1.57
0.39
1.64
0.41
1.71
0.43
2.58
0.65
1.46
0.37
2.81 0.70

FP(QXP)
S4
E4
1.07 0.27
2.58 0.65
3.05 0.76
2.99 0.75
1.02 0.25
1.65 0.41
1.73 0.43
2.00 0.50
2.61 0.65
1.48 0.37
2.58 0.64

Table 9: Results for CSP benchmarks and spreadsheets when searching for all diagnoses.
A.1.2 Diagnosing Ontologies
Data Sets and Procedure In recent works, MBD techniques are used to locate faults in
description logic ontologies (Friedrich & Shchekotykhin, 2005; Shchekotykhin et al., 2012;
Shchekotykhin & Friedrich, 2010), which are represented in the Web Ontology Language
(OWL) (Grau, Horrocks, Motik, Parsia, Patel-Schneider, & Sattler, 2008). When testing
such an ontology, the developer can  similarly to an earlier approach (Felfernig, Friedrich,
Jannach, Stumptner, & Zanker, 2001)  specify a set of positive and negative test cases.
The test cases are sets of logical sentences which must be entailed by the ontology (positive)
or not entailed by the ontology (negative). In addition, the ontology itself, which is a set
of logical sentences, has to be consistent and coherent (Baader, Calvanese, McGuinness,
Nardi, & Patel-Schneider, 2010). A diagnosis (debugging) problem in this context arises, if
one of these requirements is not fulfilled.
In the work by Shchekotykhin et al. (2012), two interactive debugging approaches were
tested on a set of faulty real-world ontologies (Kalyanpur, Parsia, Horridge, & Sirin, 2007)
874

fiParallel Model-Based Diagnosis on Multi-Core Computers

and two randomly modified large real-world ontologies. We use the same dataset to evaluate
the performance gains when applying our parallelization schemes to the ontology debugging problem. The details of the different tested ontologies are given in Table 10. The
characteristics of the problems are described in terms of the description logic (DL) used to
formulate the ontology, the number of axioms (#A), concepts (#C), properties (#P), and
individuals (#I). In terms of the first-order logic, concepts and properties correspond to
unary and binary predicates, whereas individuals correspond to constants. Every letter of
a DL name, such as ALCHF pDq , corresponds to a syntactic feature of the language. E.g.,
ALCHF pDq is an Attributive concept Language with Complement, properties Hierarchy,
Functional properties and Datatypes. As an underlying description logic reasoner, we used
Pellet (Sirin, Parsia, Grau, Kalyanpur, & Katz, 2007). The manipulation of the knowledge bases during the diagnosis process was accomplished with the OWL-API (Horridge &
Bechhofer, 2011).
Note that the considered ontology debugging problem is different from the other diagnosis settings discussed so far as it cannot be efficiently encoded as a CSP or SAT problem.
The reason is that the decision problems, such as the checking of consistency and concept
satisfiability, for the ontologies given in Table 10 are ExpTime-complete (Baader et al.,
2010). This set of experiments therefore helps us to explore the benefits of parallelization
for problem settings in which the computation of conflict sets is very hard. Furthermore,
the application of the parallelization approaches on the ontology debugging problem demonstrates the generality of our methods, i.e., we show that our methods are applicable to a
wide range of diagnosis problems and only require the existence of a sound and complete
consistency checking procedure.
Due to the generality of Reiters general approach and, correspondingly, our implementation of the diagnosis procedures, the technical integration of the OWL-DL reasoner into
our software framework is relatively simple. The only difference to the CSP-based problems
is that instead of calling Chocos solve() method inside the Theorem Prover, we make a call
to the Pellet reasoner via the OWL-API to check the consistency of an ontology.
Ontology
Chemical
Koala
Sweet-JPL
miniTambis
University
Economy
Transportation
Cton
Opengalen-no-propchains

DL
ALCHF pDq
ALCON pDq
ALCHOF pDq
ALCN
SOIN pDq
ALCHpDq
ALCHpDq
SHF
ALCHIF pDq

#A
144
44
2,579
173
49
1,781
1,300
33,203
9,664

#C/#P/#I
48/20/0
21/5/6
1,537/121/50
183/44/0
30/12/4
339/53/482
445/93/183
17,033/43/0
4,713/924/0

#D
6
10
13
48
90
864
1,782
15
110

|D|
1.67
2.3
1
3
3.67
7.17
8
4
4.13

Table 10: Characteristics of the tested ontologies.
Results The obtained results  again using a thread pool of size four  are shown in Table
11. Again, in every case parallelization is advantageous when compared to the sequential
version and in some cases the obtained speedups are substantial. Regarding the comparison
875

fiJannach, Schmitz, & Shchekotykhin

of the LWP and FP variants, there is no clear winner across all test cases. LWP seems to
be advantageous for most of the problems that are more complex with respect to their
computation times. For the problems that can be easily solved, FP is sometimes slightly
better. A clear correlation between other problem characteristics like the complexity of the
knowledge base in terms of its size could not be identified within this set of benchmark
problems.
Ontology
Chemical
Koala
Sweet-JPL
miniTambis
University
Economy
Transportation
Cton
Opengalen-no-propchains

Seq.(QXP)
[ms]
237
16
7
135
85
355
1,696
203
11,044

LWP(QXP)
S4
E4
1.44 0.36
1.42 0.36
1.47
0.37
1.43
0.36
1.66
0.41
2.20 0.55
2.72 0.68
1.27 0.32
1.59
0.40

FP(QXP)
S4
E4
1.33 0.33
1.27 0.32
1.55 0.39
1.46 0.37
1.68 0.42
1.90 0.48
2.33 0.58
1.22 0.30
1.86 0.47

Table 11: Results for ontologies when searching for all diagnoses.

A.1.3 Adding More Threads
Constraint Satisfaction Problems Table 12 shows the results of the CSP benchmarks
and spreadsheets when using up to 12 threads. In this test utilizing more than 4 threads
was advantageous in all but one small scenario. However, for 7 of the 11 tested scenarios
doing the computations with more than 8 threads did not pay off. This indicates that
choosing the right degree of parallelization can depend on the characteristics of a diagnosis
problem. The diagnosis of the mknap-1-5 problem, for example, cannot be sped up with
parallelization as it only contains one single conflict that is found at the root node. In
contrast, the graceful-K3-P2 problem benefits from the use of up to 12 threads and we
could achieve a speedup of 4.21 for this scenario, which corresponds to a runtime reduction
of 76%.
Ontologies The results of diagnosing the ontologies with up to 12 threads are shown in
Table 13. For the tested ontologies, which are comparably simple debugging cases, using
more than 4 threads payed off in only 3 of 7 cases. The best results when diagnosing these
3 ontologies were obtained when 8 threads were used. For one ontology using more than
4 threads was even slower than the sequential algorithm. This again indicates that the
effectiveness of parallelization depends on the characteristics of the diagnosis problem and
adding more threads can be even slightly counterproductive.
A.1.4 Systematic Variation of Problem Characteristics
Procedure To better understand in which way the problem characteristics influence the
performance gains, we used a suite of artificially created hitting set construction problems
876

fiParallel Model-Based Diagnosis on Multi-Core Computers

Scenario

Seq.(QXP)
[ms] S4
E4
c8
444 1.05 0.26
costasArray-13
3,854 2.69 0.67
domino-100-100
213 2.04 0.51
gracefulK3-P2
1,743 3.03 0.76
mknap-1-5
4,141 1.00 0.25
queens-8
86 1.18 0.30
hospital payment
11,728 1.60 0.40
profit calculation
81 1.53 0.38
course planning
15,323 2.31 0.58
preservation model
127 1.34 0.34
revenue calculation
460 2.39 0.60

S8
1.07
2.88
2.30
4.12
1.00
1.30
1.70
1.59
2.85
1.41
2.17

FP(QXP)
E8
S10
E10
S12
E12
0.13 1.08 0.11 1.07 0.09
0.36 2.84 0.28 2.80 0.23
0.29 2.22 0.22 2.00 0.17
0.51 4.18 0.42 4.21 0.35
0.13 1.00 0.10 1.00 0.08
0.16 1.24 0.12 1.19 0.10
0.21 1.51 0.15 1.36 0.11
0.20 1.51 0.15 1.44 0.12
0.36 2.84 0.28 2.73 0.23
0.18 1.41 0.14 1.43 0.12
0.27 1.96 0.20 1.85 0.15

Table 12: Observed performance gains for the CSP benchmarks and spreadsheets on a
server with 12 hardware threads.
Ontology
Chemical
Koala
Sweet-JPL
miniTambis
University
Economy
Transportation

Seq.(QXP)
[ms]
246
21
6
134
88
352
1,448

S4
1.37
1.07
1.09
1.47
1.53
1.48
1.74

E4
0.34
0.27
0.27
0.37
0.38
0.37
0.43

S8
1.29
1.02
1.13
1.49
1.64
0.90
1.23

FP(QXP)
E8
S10
0.16 1.30
0.13 1.03
0.14 1.08
0.19 1.47
0.21 1.56
0.11 0.76
0.15 1.07

E10
0.13
0.10
0.11
0.15
0.16
0.08
0.11

S12
1.32
0.99
1.02
1.45
1.56
0.71
1.09

E12
0.11
0.08
0.09
0.12
0.13
0.06
0.09

Table 13: Observed performance gains for the ontologies on a server with 12 hardware
threads.

with the following varying parameters: number of components (#Cp), number of conflicts
(#Cf), average size of conflicts (|Cf|). Given these parameters, we used a problem generator
which produces a set of minimal conflicts with the desired characteristics. The generator
first creates the given number of components and then uses these components to generate
the requested number of conflicts.
To obtain more realistic settings, not all generated conflicts were of equal size but rather
varied according to a Gaussian distribution with the desired size as a mean. Similarly, not
all components should be equally likely to be part of a conflict and we again used a Gaussian
distribution to assign component failure probabilities. Other probability distributions could
be used in the generation process as well, e.g., to reflect specifics of a certain application
domain.
Since for this experiment all conflicts are known in advance, the conflict detection algorithm within the consistency check only has to return one suitable conflict upon request.
Because zero computation times are unrealistic and our assumption is that the conflict
877

fiJannach, Schmitz, & Shchekotykhin

detection is actually the most costly part of the diagnosis process, we varied the assumed
conflict computation times to analyze their effect on the relative performance gains. These
computation times were simulated by adding artificial active waiting times (Wt) inside the
consistency check (shown in ms in Table 14). Note that the consistency check is only called
if no conflict can be reused for the current node; the artificial waiting time only applies to
cases in which a new conflict has to be determined.
Each experiment was repeated 100 times on different variations of each problem setting
to factor out random effects. The number of diagnoses #D is thus an average as well. All
algorithms had, however, to solve identical sets of problems and thus returned identical
sets of diagnoses. We limited the search depth to 4 for all experiments to speed up the
benchmark process. The average running times are reported in Table 14.
Results  Varying Computation Times First, we varied the assumed conflict computation times for a quite small diagnosis problem using 4 parallel threads (Table 14). The
first row with assumed zero computation times shows how long the HS-tree construction
alone needs. The improvements of the parallelization are smaller for this case because of the
overhead of thread creation and synchronization. However, as soon as we add an average
running time of 10ms for the consistency check, both parallelization approaches result in a
speedup of about 3, which corresponds to a runtime reduction of 67%. Further increasing
the assumed computation time does not lead to better relative improvements using the pool
of 4 threads.
Results  Varying Conflict Sizes The average conflict size impacts the breadth of the
HS-tree. Next, we therefore varied the average conflict size. Our hypothesis was that larger
conflicts and correspondingly broader HS-trees are better suited for parallel processing.
The results shown in Table 14 confirm this assumption. FP is always slightly more efficient
than LWP. Average conflict sizes larger than 9 did, however, not lead to strong additional
improvements when using 4 threads.
Results  Adding More Threads For larger conflicts, adding additional threads leads
to further improvements. Using 8 threads results in improvements of up to 7.27 (corresponding to a running time reduction of over 85%) for these larger conflict sizes because in
these cases even higher levels of parallelization can be achieved.
Results  Adding More Components Finally, we varied the problem complexity by
adding more components that can potentially be faulty. Since we left the number and
size of the conflicts unchanged, adding more components led to diagnoses that included
more different components. As we limited the search depth to 4 for this experiment, fewer
diagnoses were found up to this level and the search trees were narrower. As a result, the
relative performance gains were lower than when there are fewer components (constraints).
Discussion The simulation experiments demonstrate the advantages of parallelization.
For all tests, the speedups of LWP and FP are statistically significant. The results also
confirm that the performance gains depend on different characteristics of the underlying
problem. The additional gains of not waiting at the end of each search level for all worker
threads to be finished typically led to small further improvements.
Redundant calculations can, however, still occur, in particular when the conflicts for
new nodes are determined in parallel and two worker threads return the same conflict.
878

fiParallel Model-Based Diagnosis on Multi-Core Computers

#Cp, #Cf, #D Wt Seq.
LWP
|Cf|
[ms] [ms]
S4
E4
Varying computation times Wt
50, 5, 4
25
0
23
2.26 0.56
50, 5, 4
25
10
483
2.98 0.75
50, 5, 4
25 100 3,223 2.83 0.71
Varying conflict sizes
50, 5, 6
99
10 1,672 3.62 0.91
50, 5, 9
214
10 3,531 3.80 0.95
50, 5, 12
278
10 4,605 3.83 0.96
Varying numbers of components
50, 10, 9
201
10 3,516 3.79 0.95
75, 10, 9
105
10 2,223 3.52 0.88
100, 10, 9
97
10 2,419 3.13 0.78
#Cp, #Cf, #D Wt Seq.
LWP
I|Cf|
[ms] [ms]
S8
E8
Adding more threads (8 instead of 4)
50, 5, 6
99
10 1,672 6.40 0.80
50, 5, 9
214
10 3,531 7.10 0.89
50, 5, 12
278
10 4,605 7.25 0.91

FP
S4

E4

2.58
3.10
2.83

0.64
0.77
0.71

3.68
3.83
3.88

0.92
0.96
0.97

3.77 0.94
3.29 0.82
3.45 0.86
FP
S8
E8
6.50
7.15
7.27

0.81
0.89
0.91

Table 14: Simulation results.
Although without parallelization the computing resources would have been left unused
anyway, redundant calculations can lead to overall longer computation times for very small
problems because of the thread synchronization overheads.
A.2 Additional Experiments Using MXP for Conflict Detection
In this section we report the additional results that were obtained when using MergeXplain
instead of QuickXplain as a conflict detection strategy as described in Section 4.2. The
different experiments were again made using a set of CSPs and ontology debugging problems. Remember that in this set of experiments our goal is to identify a set of leading
diagnoses.
A.2.1 Diagnosing Constraint Satisfaction Problems
Table 15 shows the results when searching for five diagnoses using the CSP and spreadsheet
benchmarks. MXP could again help to reduce the running times for most of the tested
scenarios except for some of the smaller ones. For the tiny scenario mknap-1-5, the simple
sequential algorithm using QXP is the fastest alternative. For most of the other scenarios,
however, parallelization pays off and is faster than when sequentially expanding the search
tree. The best result could be achieved for the scenario costasArray-13, where FP using
MXP reduced the running times by 83% compared to the sequential algorithm using QXP,
879

fiJannach, Schmitz, & Shchekotykhin

which corresponds to a speedup of 6. The results again indicate that FP works well for
both QXP and MXP.
Scenario
c8
costasArray-13
domino-100-100
gracefulK3-P2
mknap-1-5
queens-8
hospital payment
profit calculation
course planning
preservation model
revenue calculation

Seq.(QXP)
[ms]
455
2,601
53
528
19
75
1,885
33
1,522
411
48

FP(QXP)
S4
E4
1.03 0.26
3.66 0.91
1.26 0.32
2.67 0.67
0.99 0.25
1.55 0.39
1.17 0.29
1.92 0.48
0.99 0.25
1.50 0.37
1.21 0.30

Seq.(MXP)
[ms]
251
2,128
50
419
21
63
1,426
40
1,188
430
42

FP(MXP)
S4
E4
1.06 0.26
4.92 1.23
1.43 0.36
2.48 0.62
1.01 0.25
1.67 0.42
1.28 0.32
1.86 0.46
1.42 0.35
1.50 0.37
1.48 0.37

Table 15: Results for CSP benchmarks and spreadsheets (QXP vs MXP).
Note that in one case (costasArray-13) we see an efficiency value larger than one, which
means that the obtained speedup is super-linear. This can happen in special situations
in which we search for a limited number of diagnoses and use the FP method (see also
Section A.3.1). Assume that generating one specific node takes particularly long, i.e., the
computation of a conflict set requires a considerable amount of time. In that case, a
sequential algorithm will be stuck at this node for some time, while the FP method will
continue generating other nodes. If these other nodes are then sufficient to find the (limited)
required number of diagnoses, this can lead to an efficiency value that is greater than the
theoretical optimum.
A.2.2 Diagnosing Ontologies
The results are shown in Table 16. Similar to the previous experiment, using MXP in
combination with FP pays off in all cases except for the very simple benchmark problems.
A.3 Additional Experiments  Parallel Depth-First Search
In this section, we report the results of additional experiments that were made to assess the
effects of parallelizing a depth-first search strategy as described in Section 5.3. In this set of
experiments the goal was to find one single minimal diagnosis. We again report the results
obtained for the constraint problems and the ontology debugging problems and discuss
the findings of a simulation experiment in which we systematically varied the problem
characteristics.
A.3.1 Diagnosing Constraint Satisfaction Problems
The results of searching for a single diagnosis for the CSPs and spreadsheets are shown
in Table 17. Again, parallelization generally shows to be a good strategy to speed up the
880

fiParallel Model-Based Diagnosis on Multi-Core Computers

Ontology
Chemical
Koala
Sweet-JPL
miniTambis
University
Economy
Transportation
Cton
Opengalen-no-propchains

Seq.(QXP)
[ms]
187
15
5
68
33
19
71
174
2,145

FP(QXP)
S4
E4
2.10 0.53
1.49 0.37
1.27 0.32
1.04 0.26
1.05 0.26
1.10 0.27
1.08 0.27
1.36 0.34
1.22 0.30

Seq.(MXP)
[ms]
144
13
4
56
26
14
53
154
1,748

FP(MXP)
S4
E4
1.94 0.48
1.27 0.32
1.05 0.26
1.08 0.27
1.02 0.26
1.00 0.25
1.10 0.27
1.33 0.33
1.35 0.34

Table 16: Results for Ontologies (QXP vs MXP).
diagnosis process. All measured speedups except the speedup of RDFS for the first scenario
c8 are statistically significant. In this specific problem setting, only the FP strategy had
a measurable effect and for some strategies even a modest performance deterioration was
observed when compared to Reiters sequential algorithm. The reason lies in the resulting
structure of the HS-tree which is very narrow as most conflicts are of size one.
The following detailed observations can be made when comparing the algorithms.
 In most of the tested CSPs, FP is advantageous when compared to RDFS and PRDFS.
 For the spreadsheets, in contrast, RDFS or PRDFS were better than the breadth-first
approach of FP in three of five cases.
 When comparing RDFS and PRDFS, we can again observe that parallelization can
be advantageous also for these depth-first strategies.
 Again, however, the improvements seem to depend on the underlying problem structure. In the case of the hospital payment scenario, the speedup of PRDFS is as high as
3.1 compared to the sequential algorithm, which corresponds to a runtime reduction
of more than 67%. The parallel strategy is, however, not consistently better for all
test cases.
 The performance of the Hybrid method again lies in between the performances of its
two components for many, but not all, of the tested scenarios.

A.3.2 Diagnosing Ontologies
Next, we evaluated the search for one diagnosis on the real-world ontologies (Table 18). In
the tested scenarios, applying the depth-first strategy did often not pay off when compared
to the breadth-first methods. The reason is that in the tested examples from the ontology debugging domain in many cases single-element diagnoses exist, which can be quickly
detected by a breadth-first strategy. Furthermore the absolute running times are often comparably small. Parallelizing the depth-first strategy leads to significant speedups in some
but not all cases.
881

fiJannach, Schmitz, & Shchekotykhin

Scenario
c8
costasArray-13
domino-100-100
gracefulK3-P2
mknap-1-5
queens-8
hospital payment
profit calculation
course planning
preservation model
revenue calculation

Seq.
[ms]
462
1,996
57
372
166
72
263
99
3,072
182
152

FP
S4
E4
1.09 0.27
4.78 1.19
1.22 0.30
2.86 0.71
2.18 0.55
1.38 0.34
1.83 0.46
1.67 0.42
1.11 0.28
1.78 0.44
1.11 0.28

RDFS
[ms]
454
3,729
45
305
114
55
182
70
2,496
104
121

PRDFS
S4
E4
0.89 0.22
3.42 0.85
1.17 0.29
2.01 0.50
1.02 0.26
1.02 0.26
2.14 0.54
1.15 0.29
0.90 0.23
0.99 0.25
0.92 0.23

Hybrid
S4
E4
0.92 0.23
5.90 1.47
1.05 0.26
1.89 0.47
1.35 0.33
0.95 0.24
1.72 0.43
1.10 0.28
0.87 0.22
0.95 0.24
0.90 0.22

Table 17: Results for CSP benchmarks and spreadsheets for finding one diagnosis.
Ontology
Chemical
Koala
Sweet-JPL
miniTambis
University
Economy
Transportation

Seq.
[ms]
73
10
3
58
29
17
65

FP
S4
E4
2.18 0.54
2.20 0.55
0.92 0.23
0.95 0.24
1.06 0.27
1.10 0.27
1.03 0.26

RDFS
[ms]
57
9
4
62
30
18
61

PRDFS
S4
E4
1.62 0.41
1.93 0.48
0.97 0.24
0.92 0.23
1.03 0.26
1.16 0.29
1.03 0.26

Hybrid
S4
E4
1.47 0.37
1.39 0.35
0.92 0.23
0.93 0.23
1.03 0.26
1.10 0.27
0.98 0.24

Table 18: Observed performance gains for ontologies for finding one diagnosis.
A.3.3 Systematic Variation of Problem Characteristics
Table 19 finally shows the simulation results when searching for one single diagnosis. In
the experiment we used a uniform probability distribution when selecting the components
of the conflicts to obtain more complex diagnosis problems. The results can be summarized
as follows.
 FP is as expected better than the sequential version of the HS-tree algorithm for all
tested configurations.
 For the very small problems that contain only a few and comparably small conflicts,
the depth-first strategy does not work well. Both the parallel and sequential versions
are even slower than Reiters original proposal, except for cases where zero conflict
computation times are assumed. This indicates that the costs for hitting set minimization are too high.
 For the larger problem instances, relying on a depth-first strategy to find one single
diagnosis is advantageous and also better than FP. An additional test with an even
882

fiParallel Model-Based Diagnosis on Multi-Core Computers

#Cp, #Cf, I|D| Wt
Seq.
I|Cf|
[ms]
[ms]
Varying computation times Wt
50, 5, 4
3.40
0
11
50, 5, 4
3.40
10
89
50, 5, 4
3.40 100
572
Varying conflict sizes
50, 5, 6
2.86
10
90
50, 5, 9
2.36
10
86
50, 5, 12
2.11
10
83
Varying numbers of components
50, 10, 9
3.47
10
229
75, 10, 9
3.97
10
570
100, 10, 9 4.34
10
1,467
More conflicts
100, 12, 9
5.00
10 26,870

FP

RDFS
[ms]

PRDFS
S4
E4

Hybrid
S4
E4

S4

E4

2.61
1.50
1.50

0.65
0.37
0.37

2
155
1,052

1.01
1.28
1.30

0.25
0.32
0.33

0.85
2.24
2.26

0.21
0.56
0.56

1.57
1.55
1.61

0.39
0.39
0.40

143
138
124

1.26
1.34
1.23

0.31
0.33
0.31

2.12
2.04
1.95

0.53
0.51
0.49

2.36
3.09
2.37

0.59
0.77
0.59

202
228
240

1.35
1.37
1.34

0.34
0.34
0.33

1.65
1.42
1.26

0.41
0.36
0.31

1.28

0.32

280

1.39

0.35

1.24

0.31

Table 19: Simulation results for finding one diagnosis.
larger problem shown in the last line of Table 19 reveals the potential of a depth-first
search approach.
 When the problems are larger, PRDFS can again help to obtain further runtime
improvements compared to RDFS.
 The Hybrid method works well for all but the single case with zero computation times.
Again, it represents a good choice when the problem structure is not known.
Overall, the simulation experiments show that the speedups that can be achieved with
the different methods depend on the underlying problem structure also when we search for
one single diagnosis.

References
Abreu, R., & van Gemund, A. J. C. (2009). A Low-Cost Approximate Minimal Hitting Set
Algorithm and its Application to Model-Based Diagnosis. In SARA09, pp. 29.
Anglano, C., & Portinale, L. (1996). Parallel model-based diagnosis using PVM. In EuroPVM96, pp. 331334.
Autio, K., & Reiter, R. (1998). Structural Abstraction in Model-Based Diagnosis. In
ECAI98, pp. 269273.
Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. (2010). The
Description Logic Handbook: Theory, Implementation and Applications, Vol. 32.
Bolosky, W. J., & Scott, M. L. (1993). False Sharing and Its Effect on Shared Memory
Performance. In SEDMS93, pp. 5771.
883

fiJannach, Schmitz, & Shchekotykhin

Brungger, A., Marzetta, A., Fukuda, K., & Nievergelt, J. (1999). The parallel search bench
ZRAM and its applications. Annals of Operations Research, 90 (0), 4563.
Buchanan, B., & Shortliffe, E. (Eds.). (1984). Rule-based Expert Systems: The MYCIN Experiments of the Stanford Heuristic Programming Project. Addison-Wesley, Reading,
MA.
Burns, E., Lemons, S., Ruml, W., & Zhou, R. (2010). Best-First Heuristic Search for
Multicore Machines. Journal of Artificial Intelligence Research, 39, 689743.
Campeotto, F., Palu, A. D., Dovier, A., Fioretto, F., & Pontelli, E. (2014). Exploring the
Use of GPUs in Constraint Solving. In PADL14, pp. 152167.
Cardoso, N., & Abreu, R. (2013). A Distributed Approach to Diagnosis Candidate Generation. In EPIA13, pp. 175186.
Chandra, D., Guo, F., Kim, S., & Solihin, Y. (2005). Predicting Inter-Thread Cache Contention on a Chip Multi-Processor Architecture. In HPCA11, pp. 340351.
Chu, G., Schulte, C., & Stuckey, P. J. (2009). Confidence-Based Work Stealing in Parallel
Constraint Programming. In CP09, pp. 226241.
Console, L., Friedrich, G., & Dupre, D. T. (1993). Model-Based Diagnosis Meets Error
Diagnosis in Logic Programs. In IJCAI93, pp. 14941501.
de Kleer, J. (2011). Hitting set algorithms for model-based diagnosis. In DX11, pp. 100105.
Dean, J., & Ghemawat, S. (2008). MapReduce: Simplified Data Processing on Large Clusters. Communications of the ACM, 51 (1), 107113.
Dijkstra, E. W. (1968). The Structure of the THE-Multiprogramming System. Communications of the ACM, 11 (5), 341346.
Eiter, T., & Gottlob, G. (1995). The Complexity of Logic-Based Abduction. Journal of the
ACM, 42 (1), 342.
Feldman, A., Provan, G., de Kleer, J., Robert, S., & van Gemund, A. (2010a). Solving
model-based diagnosis problems with max-sat solvers and vice versa. In DX10, pp.
185192.
Feldman, A., Provan, G., & van Gemund, A. (2010b). Approximate Model-Based Diagnosis
Using Greedy Stochastic Search. Journal of Artifcial Intelligence Research, 38, 371
413.
Felfernig, A., Friedrich, G., Isak, K., Shchekotykhin, K. M., Teppan, E., & Jannach, D.
(2009). Automated debugging of recommender user interface descriptions. Applied
Intelligence, 31 (1), 114.
Felfernig, A., Friedrich, G., Jannach, D., & Stumptner, M. (2004). Consistency-based diagnosis of configuration knowledge bases. Artificial Intelligence, 152 (2), 213234.
Felfernig, A., Friedrich, G., Jannach, D., Stumptner, M., & Zanker, M. (2001). Hierarchical
diagnosis of large configurator knowledge bases. In KI01, pp. 185197.
Felfernig, A., Schubert, M., & Zehentner, C. (2012). An efficient diagnosis algorithm for
inconsistent constraint sets. Artificial Intelligence for Engineering Design, Analysis
and Manufacturing, 26 (1), 5362.
884

fiParallel Model-Based Diagnosis on Multi-Core Computers

Felfernig, A., Friedrich, G., Jannach, D., Stumptner, M., et al. (2000). Consistency-based
diagnosis of configuration knowledge bases. In ECAI00, pp. 146150.
Ferguson, C., & Korf, R. E. (1988). Distributed tree search and its application to alpha-beta
pruning. In AAAI88, pp. 128132.
Friedrich, G., & Shchekotykhin, K. M. (2005). A General Diagnosis Method for Ontologies.
In ISWC05, pp. 232246.
Friedrich, G., Stumptner, M., & Wotawa, F. (1999). Model-Based Diagnosis of Hardware
Designs. Artificial Intelligence, 111 (1-2), 339.
Friedrich, G., Fugini, M., Mussi, E., Pernici, B., & Tagni, G. (2010). Exception handling for
repair in service-based processes. IEEE Transactions on Software Engineering, 36 (2),
198215.
Friedrich, G., & Shchekotykhin, K. (2005). A General Diagnosis Method for Ontologies. In
ISWC05, pp. 232246.
Garey, M. R., & Johnson, D. S. (1979). Computers and Intractability: A Guide to the Theory
of NP-Completeness. W. H. Freeman & Co.
Grau, B. C., Horrocks, I., Motik, B., Parsia, B., Patel-Schneider, P., & Sattler, U. (2008).
OWL 2: The next step for OWL. Web Semantics: Science, Services and Agents on
the World Wide Web, 6 (4), 309322.
Greiner, R., Smith, B. A., & Wilkerson, R. W. (1989). A Correction to the Algorithm in
Reiters Theory of Diagnosis. Artificial Intelligence, 41 (1), 7988.
Horridge, M., & Bechhofer, S. (2011). The OWL API: A Java API for OWL Ontologies.
Semantic Web Journal, 2 (1), 1121.
Jannach, D., & Schmitz, T. (2014). Model-based diagnosis of spreadsheet programs: a
constraint-based debugging approach. Automated Software Engineering, February
2014 (published online).
Jannach, D., Schmitz, T., & Shchekotykhin, K. (2015). Parallelized Hitting Set Computation
for Model-Based Diagnosis. In AAAI15, pp. 15031510.
Junker, U. (2004). QUICKXPLAIN: Preferred Explanations and Relaxations for OverConstrained Problems. In AAAI04, pp. 167172.
Kalyanpur, A., Parsia, B., Horridge, M., & Sirin, E. (2007). Finding all justifications of
owl dl entailments. In The Semantic Web, Vol. 4825 of Lecture Notes in Computer
Science, pp. 267280.
Korf, R. E., & Schultze, P. (2005). Large-scale parallel breadth-first search. In AAAI05,
pp. 13801385.
Kurtoglu, T., & Feldman, A. (2011). Third International Diagnostic Competition (DXC
11). https://sites.google.com/site/dxcompetition2011. Accessed: 2016-03-15.
Lecoutre, C., Roussel, O., & van Dongen, M. R. C. (2008). CPAI08 competition. http:
//www.cril.univ-artois.fr/CPAI08/. Accessed: 2016-03-15.
Li, L., & Yunfei, J. (2002). Computing Minimal Hitting Sets with Genetic Algorithm. In
DX02, pp. 14.
885

fiJannach, Schmitz, & Shchekotykhin

Marques-Silva, J., Janota, M., Ignatiev, A., & Morgado, A. (2015). Efficient Model Based
Diagnosis with Maximum Satisfiability. In IJCAI15, pp. 19661972.
Marques-Silva, J., Janota, M., & Belov, A. (2013). Minimal Sets over Monotone Predicates
in Boolean Formulae. In Computer Aided Verification, pp. 592607.
Mateis, C., Stumptner, M., Wieland, D., & Wotawa, F. (2000). Model-Based Debugging of
Java Programs. In AADEBUG00.
Mencia, C., & Marques-Silva, J. (2014). Efficient Relaxations of Over-constrained CSPs. In
ICTAI14, pp. 725732.
Menca, C., Previti, A., & Marques-Silva, J. (2015). Literal-based MCS extraction. In
IJCAI15, pp. 19731979.
Metodi, A., Stern, R., Kalech, M., & Codish, M. (2014). A novel sat-based approach to
model based diagnosis. Journal of Artificial Intelligence Research, 51, 377411.
Michel, L., See, A., & Van Hentenryck, P. (2007). Parallelizing constraint programs transparently. In CP07, pp. 514528.
Nica, I., Pill, I., Quaritsch, T., & Wotawa, F. (2013). The route to success: a performance
comparison of diagnosis algorithms. In IJCAI13, pp. 10391045.
Nica, I., & Wotawa, F. (2012). ConDiag - computing minimal diagnoses using a constraint
solver. In DX12, pp. 185191.
Phillips, M., Likhachev, M., & Koenig, S. (2014). PA*SE: Parallel A* for Slow Expansions.
In ICAPS14.
Pill, I., Quaritsch, T., & Wotawa, F. (2011). From conflicts to diagnoses: An empirical
evaluation of minimal hitting set algorithms. In DX11, pp. 203211.
Pill, I., & Quaritsch, T. (2012). Optimizations for the Boolean Approach to Computing
Minimal Hitting Sets. In ECAI12, pp. 648653.
Powley, C., & Korf, R. E. (1991). Single-agent parallel window search. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 13 (5), 466477.
Previti, A., Ignatiev, A., Morgado, A., & Marques-Silva, J. (2015). Prime Compilation of
Non-Clausal Formulae. In IJCAI15, pp. 19801987.
Prudhomme, C., Fages, J.-G., & Lorca, X. (2015). Choco Documentation. TASC, INRIA
Rennes, LINA CNRS UMR 6241, COSLING S.A.S. http://www.choco-solver.org.
Reiter, R. (1987). A Theory of Diagnosis from First Principles. Artificial Intelligence, 32 (1),
5795.
Rymon, R. (1994). An SE-tree-based prime implicant generation algorithm. Annals of
Mathematics and Artificial Intelligence, 11 (1-4), 351365.
Satoh, K., & Uno, T. (2005). Enumerating Minimally Revised Specifications Using Dualization. In JSAI05, pp. 182189.
Schulte, C., Lagerkvist, M., & Tack, G. (2016). GECODE - An open, free, efficient constraint
solving toolkit. http://www.gecode.org. Accessed: 2016-03-15.
886

fiParallel Model-Based Diagnosis on Multi-Core Computers

Shchekotykhin, K., Friedrich, G., Fleiss, P., & Rodler, P. (2012). Interactive ontology debugging: Two query strategies for efficient fault localization. Journal of Web Semantics,
1213, 88103.
Shchekotykhin, K. M., & Friedrich, G. (2010). Query strategy for sequential ontology
debugging. In ISWC10, pp. 696712.
Shchekotykhin, K., Jannach, D., & Schmitz, T. (2015). MergeXplain: Fast Computation of
Multiple Conflicts for Diagnosis. In IJCAI15, pp. 32213228.
Shchekotykhin, K. M., Friedrich, G., Rodler, P., & Fleiss, P. (2014). Sequential diagnosis of
high cardinality faults in knowledge-bases by direct diagnosis generation. In ECAI14,
pp. 813818.
Sirin, E., Parsia, B., Grau, B. C., Kalyanpur, A., & Katz, Y. (2007). Pellet: A Practical
OWL-DL Reasoner. Web Semantics: Science, Services and Agents on the World Wide
Web, 5 (2), 51  53.
Stern, R., Kalech, M., Feldman, A., & Provan, G. (2012). Exploring the Duality in ConflictDirected Model-Based Diagnosis. In AAAI12, pp. 828834.
Stuckey, P. J., Feydy, T., Schutt, A., Tack, G., & Fischer, J. (2014). The MiniZinc Challenge
2008-2013. AI Magazine, 35 (2), 5560.
Stumptner, M., & Wotawa, F. (1999). Debugging functional programs. In IJCAI99, pp.
10741079.
Stumptner, M., & Wotawa, F. (2001). Diagnosing Tree-Structured Systems. Artificial
Intelligence, 127 (1), 129.
White, J., Benavides, D., Schmidt, D. C., Trinidad, P., Dougherty, B., & Cortes, A. R.
(2010). Automated diagnosis of feature model configurations. Journal of Systems and
Software, 83 (7), 10941107.
Williams, B. C., & Ragno, R. J. (2007). Conflict-directed A* and its role in model-based
embedded systems. Discrete Applied Mathematics, 155 (12), 15621595.
Wotawa, F. (2001a). A variant of Reiters hitting-set algorithm. Information Processing
Letters, 79 (1), 4551.
Wotawa, F. (2001b). Debugging Hardware Designs Using a Value-Based Model. Applied
Intelligence, 16 (1), 7192.
Wotawa, F., & Pill, I. (2013). On classification and modeling issues in distributed modelbased diagnosis. AI Communications, 26 (1), 133143.

887

fiJournal of Artificial Intelligence Research 55 (2016) 715-742

Submitted 10/15; published 3/16

Combining Two and Three-Way Embedding Models for Link
Prediction in Knowledge Bases
Alberto Garca-Duran

alberto.garcia-duran@utc.fr

Sorbonne universites, Universite de technologie de Compiegne
CNRS, Heudiasyc UMR 7253
CS 60 319, 60 203 Compiegne cedex, France

Antoine Bordes

abordes@fb.com

Facebook AI Research
770 Broadway, New York, NY 10003. USA

Nicolas Usunier

usunier@fb.com

Facebook AI Research
112 Avenue de Wagram, 75017 Paris, France

Yves Grandvalet

yves.grandvalet@utc.fr

Sorbonne universites, Universite de technologie de Compiegne
CNRS, Heudiasyc UMR 7253
CS 60 319, 60 203 Compiegne cedex, France

Abstract
This paper tackles the problem of endogenous link prediction for knowledge base completion. Knowledge bases can be represented as directed graphs whose nodes correspond
to entities and edges to relationships. Previous attempts either consist of powerful systems
with high capacity to model complex connectivity patterns, which unfortunately usually
end up overfitting on rare relationships, or in approaches that trade capacity for simplicity in order to fairly model all relationships, frequent or not. In this paper, we propose
Tatec, a happy medium obtained by complementing a high-capacity model with a simpler one, both pre-trained separately and then combined. We present several variants of
this model with different kinds of regularization and combination strategies and show that
this approach outperforms existing methods on different types of relationships by achieving
state-of-the-art results on four benchmarks of the literature.

1. Introduction
Knowledge bases (KBs) are crucial tools to deal with the rise of data, since they provide
ways to organize, manage and retrieve all digital knowledge. These repositories can cover
any kind of area, from specific domains like biological processes, for example in GeneOntology (Ashburner, Ball, Blake, Botstein, Butler, Cherry, Davis, Dolinski, Dwight, Eppig,
et al., 2000), to very generic purposes. Freebase (Bollacker, Evans, Paritosh, Sturge, &
Taylor, 2008), a huge collaborative KB which belongs to the Google Knowledge Graph, is an
example of the latter kind which provides expert/common-level knowledge and capabilities
to its users. An example of a knowledge engine is WolframAlpha (Wolfram Research,
2009), an engine which answers to any natural language question, like how far is Saturn
from the sun?, with human-readable answers (1,492 109 km) using an internal KB.
Such KBs can be used for question answering, but also for other natural language process2016 AI Access Foundation. All rights reserved.

fiGarca-Duran, Bordes, Usunier & Grandvalet

ing tasks like word-sense disambiguation (Navigli & Velardi, 2005), co-reference resolution
(Ponzetto & Strube, 2006) or even machine translation (Knight & Luk, 1994).
KBs can be formalized as directed multi-relational graphs, whose nodes correspond
to entities connected with edges encoding various kinds of relationship. Hence, one can
also refer to them as multi-relational data. In the following we denote connections among
entities via triples or facts (head, label, tail ), where the entities head and tail are connected
by the relationship label. Any information of the KB can be represented via a triple or a
concatenation of several ones. Note that multi-relational data are not only present in KBs
but also in recommender systems, where the nodes would correspond to users and products
and edges to different relationships between them, or in social networks for instance.
A main issue with KBs is that they are far from being complete. Freebase currently
contains thousands of relationships and more than 80 millions of entities, leading to billions
of facts, but this remains only a very small portion out of all the human knowledge, obviously. And since question answering engines based on KBs like WolframAlpha are not
capable of generalizing over their acquired knowledge to fill in for missing facts, they are
de facto limited: they search for matches with a question/query in their internal KB and
if this information is missing they can not provide a correct answer, even if they correctly
interpreted the question. Consequently, huge efforts are nowadays being devoted towards
KB construction or completion (Lao, Mitchell, & Cohen, 2011; Bordes, Glorot, Weston, &
Bengio, 2013a; Dong, Gabrilovich, Heitz, Horn, Lao, Murphy, Strohmann, Sun, & Zhang,
2014), via manual or automatic processes, or a mix of both. This is mainly divided in two
tasks: entity creation or extraction, which consists in adding new entities to the KB and
link prediction, which attempts to add connections between entities. This paper focuses on
the latter case. Performing link prediction can be formalized as filling in incomplete triples
like (head, label, ?) or (?, label, tail), by predicting the missing argument of the triple when
such triple does not exist in the KB, yet. For instance, given the small example KB of
Figure 1, made of 6 entities and 2 different relationships, and containing facts like (Jared
Leto, influenced by, Bono) or (Michael Buble, profession, singer), we would like
to be able to predict new links such as (Frank Sinatra, profession, singer), by using
the fact that he influenced the singer Michael Buble for instance.
Link prediction in KBs is complex due to several issues. The entities are not homogeneously connected: some of them will have a lot of links with other entities, whereas others
will be rarely connected. To illustrate the diverse characteristics present in the relationships
we can take a look at FB15k, a subset of Freebase introduced by Bordes, Usunier, GarcaDuran, Weston, and Yakhnenko (2013b). In this data set of 14k entities and 1k types of
relationships, entities have a mean number of triples of 400, but a median of 21 indicating
that a large number of them appear in very few triples. Besides, roughly 25% of the connections are of type 1-to-1, that is, a head is connected to at most one tail, and around 25%
are of type Many-to-Many, that is, multiple heads can be linked to a tail and vice versa. As
a result, diverse problems coexist in the same database. Another property of relationships
that can have a big impact on the performance is the typing of their arguments. On FB15k,
some relationships are very strongly typed like /sports/sports team/location, where one
always expects a football team as head and a location as tail, and some are far less precise
such as /common/webpage/category where one expects only web page addresses as tail but
716

fiEmbedding Models for Link Prediction in KBs

ion

s
fes

o
pr

Actor
Singer

Jared
Leto

Bono
Michael
Buble

Frank
Sinatra

Figure 1: Example of (incomplete) Knowledge Base with 6 entities, 2 relationships
and 7 facts.
pretty much everything else as head. A link prediction algorithm should be able to adapt
to these different settings.
Though there exists (pseudo-) symbolic approaches for link prediction based on Markovlogic networks (Kok & Domingos, 2007) or random walks (Lao et al., 2011), learning latent
features representations of KB constituents - the so-called embedding methods - have recently
proved to be more efficient for performing link prediction in KBs, (e.g. Bordes et al., 2013b;
Wang, Zhang, Feng, & Chen, 2014b; Lin, Liu, Sun, Liu, & Zhu, 2015; Chang, Yih, Yang,
& Meek, 2014; Wang, Zhang, Feng, & Chen, 2014a; Zhang, Salwen, Glass, & Gliozzo,
2014; Yang, Duan, Zhou, & Rim, 2014b). In all these works, entities are represented by
low-dimensional vectors - the embeddings - and relationships act as operators on them:
both embeddings and operators define a scoring function that is learned so that triples
observed in the KBs have higher scores than unobserved ones. The embeddings are meant
to capture underlying features that should eventually allow to create new links successfully.
The scoring function is used to predict new links: the higher the score, the more likely a
triple is to be true. Representations of relationships are usually specific (except in LFM
(Jenatton, Le Roux, Bordes, & Obozinski, 2012) where there is a sharing of parameters
across relationships), but embeddings of entities are shared for all relationships and allow
to transfer information across them. The learning process can be considered as multi-task,
where one task concerns each relationship, and entities are shared across tasks.
Embedding models can be classified according to the interactions that they use to encode
the validity of a triple in their scoring function. If the joint interaction between the head,
the label and the tail is used then we are dealing with a 3-way model; but when the binary
interactions between the head and the tail, the head and the label, and the label and the
tail are the core of the model, then it is a 2-way model. Both kinds of models represent the
entities as vectors, but they differ in the way they model the relationships: 3-way models
generally use matrices, whereas 2-way models use vectors. This difference in the capacity
leads to a difference in the expressiveness of the models. The larger capacity of 3-way models
(due to the large number of free parameters in matrices) is beneficial for the relationships
717

fiGarca-Duran, Bordes, Usunier & Grandvalet

appearing in a lot of triples, but detrimental for rare ones even if regularization is applied.
Capacity is not the only difference between 2- and 3-way models, the information encoded
by these two models is also different: we show in Sections 3 and 5.3.2 that both kinds of
models assess the validity of the triple using different data patterns.
In this paper we introduce Tatec that encompass previous works by combining wellcontrolled 2-way interactions with high-capacity 3-way ones. We aim at capturing data
patterns of both approaches by separately pre-training the embeddings of 2-way and 3-way
models and using different embedding spaces for each of the two of them. We demonstrate
in the following that otherwise  with no pre-training and/or no use of different embedding
spaces  some features cannot be conveniently captured by the embeddings. Eventually,
these pre-trained weights are combined in a second stage, leading to a combination model
which outperforms most previous works in all conditions on four benchmarks from the
literature, UMLS, Kinships, FB15k and SVO. Tatec is also carefully regularized since
we systematically compared two different regularization schemes: adding penalty terms to
the loss function or hard-normalizing the embedding vectors by constraining their norms.
This paper is an extension of our previous work (Garca-Duran, Bordes, & Usunier,
2014): we added a much more thorough study on regularization and on combination strategies for Tatec. Besides we propose experiments on several new benchmarks and a more
complete comparison of our proposed method w.r.t. the state-of-the-art. We also give examples of predictions and projections in 2D of the obtained embeddings to provide some
insights into the behavior of Tatec. The paper is organized as follows. Section 2 discusses
previous works. Section 3 presents our model and justifies its choices. Detailed explanations of both the training procedure and the regularization schemes are given in Section 4.
Finally, we present our experimental results on four benchmarks in Section 5.

2. Related Work
In this section, we discuss the state-of-the-art of modeling large multi-relational databases,
with a particular focus on embedding methods for knowledge base completion.
One of the simplest and most successful 2-way models is TransE (Bordes et al., 2013b).
In that model, relationships are represented as translations in the embedding space: if
(h, `, t) holds, then the embedding of the tail t should be close to the embedding of head
h plus some vector that depends on the label `. This is a natural approach to model
hierarchical and asymmetric relationships, which are common in knowledge bases such as
Freebase. Several modifications to TransE have been proposed recently, TransH (Wang
et al., 2014b) and TransR (Lin et al., 2015). In TransH, the embeddings of the entities
are projected onto a hyperplane that depends on ` before the translation. The second
algorithm, TransR, follows the same idea, except that the projection operator is a matrix
that is more general than an orthogonal projection to a hyperplane. As we shall see in the
next section, TransE corresponds to our Bigram model with additional constraints on
the parameters.
While 2-way models were shown to have very good performances on some KB datasets,
they have limited expressiveness and they can fail dramatically on harder datasets. In
contrast, 3-way models perform some form of low-rank tensor factorization, and in that
respect can have extremely high expressiveness depending on the rank constraints. In the
718

fiEmbedding Models for Link Prediction in KBs

context of link prediction for multi-relational data, RESCAL (Nickel, Tresp, & Kriegel,
2011) follows natural modeling assumptions. Similarly to TransE, RESCAL learns one
low-dimensional embedding for each entity. However, relationships are represented as a
bilinear operator in the embedding space in RESCAL, i.e. each relationship corresponds
to a matrix, whereas in TransE they are also represented by vectors. Besides the different
interactions across the arguments of a triple to explain its validity, these models also differ in
their training objective. The training objective of RESCAL is the Frobenius norm between
the original data tensor and its low-rank reconstruction, whereas Tatec uses the margin
ranking criterion of TransE. Another related 3-way model is SME(bilinear) (Bordes et al.,
2013a). The parameterization of SME(bilinear) is a constrained version of RESCAL, and
also uses a ranking criterion as training objective.
The Latent Factor Model (LFM) (Jenatton et al., 2012) and the Neural Tensor Networks
(NTN) (Socher, Chen, Manning, & Ng, 2013) use combinations of a 3-way model with a more
constrained 2-way model, and in that sense are closer to our algorithm Tatec. There are
important differences between these algorithms and Tatec, though. First, both LFM and
NTN share the entity embeddings in the 2-way and the 3-way models, while we learn different
entity embeddings. The use of different embeddings for the 2-way and the 3-way models
does not increase the model expressiveness, because it is equivalent to a combination with
shared embeddings in a higher dimensional embedding space, with additional constrains
on the relation parameters. As we show in the experiments however, these additional
constraints lead to very significant improvements. The second main difference between our
approach and LFM is that some parameters of the relationships between the 2-way and the
3-way interaction terms are also shared, which is not the case in Tatec. Indeed, such joint
parameterization might reduce the expressiveness of the 2-way interaction terms which, as
we argue in Section 3.3, should be left with maximum degrees of freedom. Lastly, LFM seeks
to maximize the likelihood function given a set of positive and negative facts. The NTN has
a more general parameterization than LFM, but still uses the same entity embeddings for
the 2-way and 3-way interaction terms. Also, NTN has two layers and a non-linearity after
the first layer, while our model does not add any nonlinearity after the embedding step. In
order to have a more precise overview of the differences between the approaches, we give in
Section 3 (Table 1) the formulas of the scoring functions of these related works.
While these works ignore the type-constraints present in the relationships (i.e. some
entities are not legitimate arguments of a given relationship), other approaches present
extensions making use of this side information. Hence, Chang et al. (2014) propose a
modification of RESCAL to avoid incompatible entity-relation triples to participate in the
loss function and Krompass, Baier, and Tresp (2015) propose a similar framework for the
models optimized by iterating through small batches. Krompass et al. (2015) also propose a
local closed-world assumption that approximates type-information only from the available
triples without requiring any kind of side information from the KB. Though we did not
consider to use that side information, these type-constraints could be easily introduced in
our model by either taking them from the KB (when available), or using the approach of
Krompass et al. (2015).
While there has been a lot of focus recently on algorithms purely based on learning
embeddings for entities and/or relationships, many earlier alternatives had been proposed.
We discuss works carried out in the Bayesian clustering framework, as well as approaches
719

fiGarca-Duran, Bordes, Usunier & Grandvalet

Figure 2: Example of RDF file of Freebase
that explicitly use the graph structure of the data. The Infinite Relational Model (Kemp,
Tenenbaum, Griffiths, Yamada, & Ueda, 2006), which is a nonparametric extension of the
Stochastic Block Model (Wang & Wong, 1987), is a Bayesian clustering approach that learns
clusters of entities of the same kind, i.e. groups of entities that can have similar relationships
with other entities. This work was followed by Sutskever, Salakhutdinov, and Tenenbaum
(2009), who propose a 3-way tensor factorization model based on Bayesian clustering in
which entities within a cluster share the same distribution of embeddings.
Symbolic approaches, as the aforementioned that of Kok and Domingos (2007), are
also worth mentioning in this context. Though their rule-based inference of new links may
lead to great expressiveness, they are usually limited by the quality and coverage of their
handcrafted rules. Still, the Path Ranking Algorithm (PRA) (Lao et al., 2011) presented
a model able to discover rules automatically by performing random walks from training
data, with the own limitation of the connectivity between nodes; i.e. if there is no shortenough path connecting two nodes, then the model is not able to infer a relation between
them. Recently, Gardner, Talukdar, Krishnamurthy, and Mitchell (2014) cover this gap by
combining that model with pre-trained embeddings. The PRA is used in the Knowledge
Vault project (Dong et al., 2014) in conjunction with an embedding approach. Thus, even
though we do not consider these symbolic approaches here, they could also be combined
with our embedding model.
Even though we present and evaluate our algorithms in the context of knowledge bases,
this work also applies in the broader context of RDF data. RDF is a standard model for
data interchange on the Web. It is at the core of the Linked Data initiative (Bizer, Heath,
Idehen, & Berners-Lee, 2008) that aims to extend the linking structure of the Web to use
URIs to name the relationship between things as well as the two ends of the link. This
linking structure forms a directed, labeled graph, where the edges represent the named link
between two objects. Thus such data is essentially made of triples. In RDF-terminology a
triple is defined as (subject, predicate, object). The Freebase dump is available in this
format. Figure 2 shows an example of a RDF file of Freebase, where m.02mjmr is an identifier for the resource representing Barack Obama. This identifier has several predicates as
ns:influence.influence node.influenced by or ns:people.person.religion, whose
objects are ns:m.01d1n (Reinhold Niebuhr) and ns:m.01lp8 (Christianity), respectively.

3. TATEC
We now describe our model and the motivations underlying our parameterization. The data
S is a set of relations between entities in a fixed set of entities in E = {e1 , ..., eE }. Relations
are represented as triples (h, `, t) where the head h and the tail t are indexes of entities (i.e.
720

fiEmbedding Models for Link Prediction in KBs

h, t  [[E]] = {1, ..., E}), and the label ` is the index of a relationship in L = {l1 , ..., lL },
which defines the type of the relation between the entities eh and et .
3.1 Scoring Function
Our goal is to learn a discriminant scoring function on the set of all possible triples E LE
so that the triples which represent likely relations receive higher scores than triples that
represent unlikely ones. Our proposed model, Tatec, learns embeddings of entities in a
low dimensional vector space, say Rd , and parameters of operators on Rd  Rd , most of
these operators being associated to a single relationship. More precisely, the score given by
Tatec to a triple (h, `, t), denoted by s(h, `, t), is defined as:
s(h, `, t) = s1 (h, `, t) + s2 (h, `, t)

(1)

where s1 and s2 have the following form:
(B) Bigram or the 2-way interaction term:

 fi ff 
 fi ff 
 fi fi ff
s1 (h, `, t) = r`1 fieh1 + r`2 fiet1 + eh1 fiDfiet1 ,

(2)

where eh1 , et1 are embeddings in Rd1 of the head and tail entities of (h, `, t) respectively,
r`1 and r`2 are vectors in Rd1 that depend on the relationship `, and D is a diagonal
matrix that does not depend on the input triple.

fiff
fi
As
fi ff throughout this section, . . is the canonical dot product, and
ff

notation

 fi a figeneral
fi
fi
fi
x A y = x Ay where x and y are two vectors in the same space and A is a
square matrix of appropriate dimensions.
We use two different relation vectors for the subject and the object in order to model
asymmetric relationships; for instance, if r1` = r2` , then (Paris, capital of, France)
would have the same score as (France, capital of, Paris).
(T) Trigram or the 3-way interaction term:

 fi fi ff
s2 (h, `, t) = eh2 fiR` fiet2 ,

(3)

where R` is a matrix of dimensions (d2 , d2 ), and eh2 and et2 are embeddings in Rd2 of
the head and tail entities respectively. The embeddings of the entities for this term
are not the same as for the 2-way term; they can even have different dimensions.
The embedding dimensions d1 and d2 are hyperparameters of our model. All other vectors
and matrices are learned without any additional parameter sharing.
The 2-way interaction term of the model is similar to that of Bordes et al. (2013a), but
slightly more general because it does not contain any constraint of the relation-dependent
vectors r`1 and r`2 . It can also be seen as a relaxation of the translation model of Bordes
et al. (2013b), which is the special case where r`1 = r`2 , D is the identity matrix, and the
entity embeddings are constrained to lie on the unit sphere.
The 3-way term corresponds exactly to the model used by the collective factorization
method RESCAL (Nickel et al., 2011), and we chose it for its high expressiveness on
complex relationships. Indeed, as we said earlier in the introduction, 3-way models can
721

fiGarca-Duran, Bordes, Usunier & Grandvalet

basically represent any kind of interaction among entities. The combination of 2- and 3way terms has already been used by Jenatton et al. (2012), Socher et al. (2013), but, besides
a different parameterization, Tatec contrasts with them by the additional freedom brought
by using different embeddings in the two interaction terms. In LFM (Jenatton et al., 2012),
constraints were imposed on the relation-dependent matrix of the 3-way terms (low rank
in a limited basis of rank-one matrices), the relation vectors r`1 and r`2 were constrained to
be in the image of the matrix (D = 0 in their work). These global constraints severely
limited the expressiveness of the 3-way model, and act as a stringent regularization that
reduces the expressiveness of the 2-way model, which, as we explain in Section 3.3, should
be left with maximum degrees of freedom. We are similar to NTN (Socher et al., 2013)
in the respect that we do not share any parameter between relations. Our overall scoring
function is similar to this model with a single layer, with the fundamental difference that
we use different embedding spaces and do not use any non-linear transfer function, which
results in a facilitated training (for instance, the gradients have a larger magnitude). Table
1 details the scoring function of some of the aforementioned models.
3.2 Term Combination
We study two strategies for combining the bigram and trigram scores as indicated in Equation (1). In both cases, both s1 and s2 are first trained separately as we detail in Section 4
and then combined. The difference between our two strategies depends on whether we
jointly update (or fine-tune) the parameters of s1 and s2 in a second phase or not.
3.2.1 Fine Tuning
This first strategy, denoted Tatec-ft, simply consists in summing both scores following
Equation (1).

 fi ff 
 fi ff 
 fi fi ff 
 fi fi ff
sF T (h, `, t) = r`1 fieh1 + r`2 fiet1 + eh1 fiDfiet1 + eh2 fiR` fiet2
All parameters of s1 and s2 (and hence of s) are then fine-tuned in a second training phase
to accommodate for their combination. This version could be trained directly without
pre-training s1 and s2 separately but we show in our experiments that this is detrimental.
3.2.2 Linear Combination
The second strategy combines the bigram and trigram terms using a linear combination,
without jointly fine-tuning their parameters that remain unchanged after their pre-training.
The score s is hence defined as follows:

 fi ff

 fi ff

 fi fi ff

 fi fi ff
sLC (h, `, t) = 1` r`1 fieh1 + 2` r`2 fiet1 + 3` eh1 fiDfiet1 + 4` eh2 fiR` fiet2
The combination weights i` depend on the relationship and are learned by optimizing the
ranking loss (defined later in (6)) using L-BFGS, with an additional quadratic penalization
P ||` ||22
term, ` P
, where  ` contains the combination weights for relation `, and  are con` +
strained to ` ` =  ( is a hyperparameter). This version of Tatec is denoted Tatec-lc
in the following.
722

fiEmbedding Models for Link Prediction in KBs

h-th relation

t-th entity

l-th entity

Figure 3: The entry (h,l,t) of the tensor indicates if the relation l holds between the entities
h and t.
3.3 Interpretation and Motivation of the Model
This section discusses the motivations underlying the parameterization of Tatec, and in
particular our choice of 2-way model to complement the 3-way term.
3.3.1 2-Way Interactions as Fiber Biases
As a first motivation for having both a 2-way and a 3-way model, we use an analogy with
matrix factorization. It is common in matrix factorization techniques for collaborative
filtering to add biases (also called offsets or intercepts) to the model. For instance, a critical
step of the best-performing techniques of the Netflix prize was to add user and item biases,
i.e. to approximate a user-rating Rui according to (see e.g. Koren, Bell, & Volinsky, 2009):

 fi ff
Rui  Pu fiQi + u + i + 
(4)
where P  RU k , with each row Pu containing the k-dimensional embedding of the user
(U is the number of users), Q  RIk containing the embeddings of the I items, u  R a
bias only depending on a user and i  R a bias only depending on an item ( is a constant
that we do not consider further on).
The 2-way + 3-way interaction model we propose can be seen as the 3-mode tensor version of this biased version of matrix factorization: the trigram term (T) is the collective
matrix factorization parameterization

 offi the
ff RESCAL algorithm (Nickel et al., 2011) and
fi
plays a role analogous to the term Pu Qi of the matrix factorization model for collaborative filtering (4).
The bigram term (B) then plays the role of biases for each fiber of the tensor,1 i.e.
1
2
3
s1 (h, `, t)  Bl,h
+ Bl,t
+ Bh,t

(5)

and thus is the analogue for tensors to the term u + i in the matrix factorization model
(4). The exact form of s1 (h, `, t) given in (B) corresponds to a specific form of collective
1. Fibers are the higher order analogue of matrix rows and columns for tensors and are defined by fixing
every index but one.

723

fiGarca-Duran, Bordes, Usunier & Grandvalet

h
i
1
factorization of the fiber-wise bias matrices B1 = Bl,h

l[[L]],h[[E]]

, B2 and B3 of Equation

(5). We do not exactly learn one bias by fiber because many such fibers have very little
data, while, as we argue in the following, the specific form of collective factorization we
propose in (B) should allow to share relevant information between different biases. Note
that whereas a tensor of dimensions n  m  n (in general, in this problem the same set of
entities is considered for both the head and the tail) has 2mn + n2 biases, Tatec computes
such biases by means of linear combinations of n + 2m embeddings, which allows a learning
transfer across them.
3.3.2 The Need for Multiple Embeddings
A key feature of Tatec is to use different embedding spaces for the 2-way and 3-way terms,
while existing approaches that have both types of interactions use the same embedding
space (Jenatton et al., 2012; Socher et al., 2013). We motivate this choice in this section.
It is important to notice that biases in the matrix factorization model (4), or the bigram term in the overall scoring function (1) do not affect the model expressiveness, and
in particular do not affect the main modeling assumption that embeddings should have low
rank. The user/item-biases in (4) only boil down to adding two rank-1 matrices 1T and
1 T to the factorization model. Since the rank of the matrix is a hyperparameter, one may
simply add 2 to this hyperparameter and get a slightly larger expressiveness than before,
with reasonably little impact since the increase in rank would remain small compared to
its original value (which is usually 50 or 100 for large collaborative filtering data sets). The
critical feature of these biases in collaborative filtering is how they interfere with capacity
control terms other than the rank, namely the 2-norm regularization: for instance, Koren
et al. (2009) adjust
all terms of (4) using squared error as a measure of fit with a regular
ization term  kPu k22 + kQi k22 + u2 + i2 , where  > 0 is the regularization parameter.
This kind of regularization is a weighted trace norm regularization on PQT (Salakhutdinov
& Srebro, 2010).
Leaving aside the
 weighted part, the idea is that at convergence, the
P
P
2
2
quantity 
i kQi k2 is equal to 2 times the sum of the singular values of
u kPu k2 +
the matrix PQT . However, kk22 , which is the regularization applied to user
 biases, is not
T
2 times the singular value of the rank-one matrix 1 , which is equal to Ikk2 , and can
be much larger than kk22 . Thus, if the pattern user+item biases exists in the data, but
very weakly because it is hidden by stronger factors, it will be less regularized than others
and the model should be able to capture it. Biases, which are allowed to fit the data more
than other factors, offer the opportunity of relaxing the control of capacity on some parts of
the model but this translates into gains if the patterns that they capture are indeed useful
patterns for generalization. Otherwise, this ends up relaxing the capacity to lead to more
overfitting.

 fi ff

 fi ff
Our bigram terms are closely related to the trigram term: the terms r`1 fieh1 and r`2 fiet1
can be
fi fi ffto the trigram term by adding constant features in the entities embeddings,

 added
and eh1 fiDfiet1 is directly in an appropriate quadratic form. Thus, the only way to gain
from the addition of bigram terms is to ensure that they can capture useful patterns, but
also that capacity control on these terms is less strict than on the trigram terms. In tensor
factorization models, and especially 3-way interaction models with parameterizations such
724

fiEmbedding Models for Link Prediction in KBs

as (T), capacity control through the regularization of individual parameters is still not well
understood, and sometimes turns out to be more detrimental than effective in experiments.
The only effective parameter is the admissible rank of the embeddings, which leads to the
conclusion that the bigram term can be really useful in addition to the trigram term if
higher-dimensional embeddings are used. Hence, in absence of clear and concrete way of
effectively controlling the capacity of the trigram term, we believe that different embedding
spaces should be used.
3.3.3 2-Way Interactions as Entity Types+Similarity
Having a part of the model that is more expressive, but less regularized (see Subsection
4.2) than the other part is only useful if the patterns it can learn are meaningful for the
prediction task at hand. In this section, we give the motivation for our 2-way interaction
term for the task of modeling multi-relational data.
Most relationships in multi-relational data, and in knowledge bases like FB15k in particular, are strongly typed, in the sense that only well-defined and specific subsets of entities can be either heads or tails of selected relationships. For instance, a relationship like
capital of expects a (big) city as head and a country as tail for any valid relation. Large
knowledge bases have huge amounts of entities, but those belong to many different types.
Identifying the expected types of head and tail entities of relationships, with an appropriate
granularity of types (e.g. person or artist or writer), is likely to filter
ff 95%

 fi of ffthe

 fi out
entity set during prediction. The exact form of the first two terms r`1 fieh1 + r`2 fiet1 of
the 2-way interaction model (B), which corresponds to a low-rank factorization of the per
bias matrices (head, label) and (tail, label) in which head and tail entities have the same
embeddings, is based on the assumption that the types of entities can be predicted based
on few (learned) features, and these features are the same for predicting head-types as for
predicting tail-types. As such, it is natural to share the entities embeddings in the first two
terms of (B).

 fi fi ff
The last term, eh1 fiDfiet1 , is intended to account for a global similarity between entities.
For instance, the capital of France can easily be predicted by looking for the city with
strongest overall connections with France in the knowledge base. A country and a city may
be strongly linked through their geographical positions, independent of their respective
types. The diagonal matrix D allows to re-weight features of the embedding space to
account for the fact that the features used to describe types may not be the same as those
that can describe the similarity between objects of different types. The use of a diagonal
matrix is strictly equivalent to using a general symmetric matrix in place of D.2 The
reason for using a symmetric matrix comes from the intuition that the direction of many
relationships is arbitrary (i.e. the choice between having triples Paris is capital of France
rather than France has capital Paris), and the model should be invariant under arbitrary
inversions of the directions of the relationships (in the case of an inversion of direction,
the relations vectors r`1 and r`2 are swapped, but all other parameters are unaffected). For
2. We can see the equivalence by taking the eigenvalue decomposition of a symmetric
the change
fi apply

 fi D:
ff
of basis to the embeddings to keep only the diagonal part of D in the term eh1 fiDfiet1 , and apply the
reverse transformation to the vectors r`1 and r`2 . Note that since rotations preserve Euclidean distances,
the equivalence still holds under 2-norm regularization of the embeddings.

725

fiGarca-Duran, Bordes, Usunier & Grandvalet

Table 1: Scoring function for several models of the literature. Capitalized letters
denote matrices and lower cased ones, vectors.
Model
TransE
TransH
TransR
RESCAL
LFM

Score (s(h, `, t))
h
`
t

 ` fi h ||e
ff + r `  e ||t 2 
 ` fi t ` ff 2
h
`
fi
fi
||(e  w
 e fiw ff) + r  
(e fi w
ff e w )||2
hfi
`
tfi
|| e M`
 +fir 
fi ffe M` ||2
hfi `fi t
e
R

 fi ` fi 0 ff 
 h fi ` fi ff 
 efi ` fi t ff 
 h fi ` fi t ff
y fi R fi y + e fi R fi z + zfi R fi e + e fi R fi e

tasks in which such invariance is not desirable, the diagonal matrix could be replaced by an
arbitrary matrix.

4. Training
Training Tatec is carried out using stochastic gradient descent on an objective function
comprising a data-fitting term and a regularization term. These two terms are decribed in
details in this section.
4.1 Ranking Objective
We use a ranking objective function, which is designed to give higher scores to positive
triples (facts that express true and verified information from the KB) than to negative
ones (facts that are supposed to express false information). These negative triples can be
provided by the KB, but often they are not, so we need a process to turn positive triples
into corrupted ones to carry out our discriminative training. A simple approach consists
in creating negative examples by replacing one argument of a positive triple by a random
element. This way is simple and efficient in practice but may introduce noise by creating
wrong negatives.
Let S be the set of positive triples provided by the KB, we optimize the following ranking
loss function:
X
X


  s(h, `, t) + s(h0 , `0 , t0 ) +
(6)
(h,`,t)S (h0 ,`0 ,t0 )C(h,`,t)

where [z]+ = max(z, 0) and C(h, `, t) is the set of corrupted triples. Depending on the
application, this set can be defined in 3 different ways:
1. C(h, `, t) = {(h0 , `0 , t0 )  [[E]]  L  [[E]]|h0 6= h and `0 6= ` and t0 6= t}
2. C(h, `, t) = {(h0 , `, t0 )  [[E]]  L  [[E]]|h0 6= h or t0 6= t}
3. C(h, `, t) = {(h, `0 , t)  [[E]]  L  [[E]]|`0 6= `}
The margin  is an hyperparameter that defines the minimum gap between the score of
a positive triple and its negative ones. The stochastic gradient descent is performed in a
minibatch setting. At each epoch the data set is shuffled and split into disjoint minibatches
of m triples and 1 or 2 (see next section) negative triples are created for every positive one.
We use two different learning rates 1 and 2 , one for the Bigram and one the Trigram
model; they are kept fixed during the whole training.
726

fiEmbedding Models for Link Prediction in KBs

Algorithm 1 Learning unregularized Tatec.
input Training set S = {(h, l, t)}, margin , learning rates 1 and 2
1: initialization
2:
- for Bigram: e1  uniform( 6d , 6d ) for each entity e
1
1
3:
r1 , r2  uniform( 6d , 6d ) for each `
1
1
4:
D  uniform( 6d , 6d )
1
1
5:
- for Trigram: e2  uniform( 6d , 6d ) for each entity e
2
2
6:
R  uniform( 6d , 6d ) for each `
2
2
7:
- for Tatec-ft: pre-trained weights of Bigram and Trigram
8: All the embeddings are normalized to have a 2- or Frobenius-norm equal to 1.
9: loop
10:
Sbatch  sample(S, m) // sample a training minibatch of size m
11:
Tbatch   // initialize a set of pairs of examples
12:
for (h, `, t)  Sbatch do
13:
(h0 , `0 , t0 )  sample
according to the selected strategy C(h, `, t)
 a negative triple 	
14:
Tbatch  Tbatch  (h, `, t), (h0 , `0 , t0 ) // record the pairs of examples
15:
end for
X


16:
Update parameters using gradients
   s(h, `, t) + s(h0 , `0 , t0 ) + :

(h,`,t),(h0 ,`0 ,t0 ) Tbatch

17:
18:
19:
20: end

- for Bigram (Eq. 2): s = s1
- for Trigram (Eq. 3): s = s2
- for Tatec-ft (Eq. 1): s = s1 + s2
loop

We are interested in both Bigram and Trigram terms of Tatec to capture different
data patterns, and using a random initialization of all weights may lead to bad local minima
and thus to a poor solution. Hence, we first pre-train separately s1 (h, `, t) and s2 (h, `, t),
and then we use these learned weights to initialize that of the full model. Training of Tatec
is hence carried out in two phases: a (disjoint) pre-training and either a (joint) fine-tuning
for Tatec-ft or a learning of the combination weights for Tatec-lc. Both pre-training
and fine-tuning are stopped using early stopping on a validation set, and follow the training
procedure that is summarized in Algorithm 1, for the unregularized case. Training of the
linear combination weights of Tatec-lc is stopped at convergence of L-BFGS.
4.2 Regularization
Previous work on embedding models have used two different regularization strategies: either
by constraining the entity embeddings to have, at most, a 2-norm of value e (Garca-Duran
et al., 2014) or by adding a 2-norm penalty on the weights (Wang et al., 2014b; Lin et al.,
2015) to the objective function (6). In the former, which we denote as hard regularization,
regularization is performed by projecting the entity embeddings after each minibatch onto
the 2-norm ball of radius e . In the latter, which we denote as soft regularization, a penalization term of the form [||e||22  2e ]+ for the entity embeddings e is added. The soft scheme
allows the 2-norm of the embeddings to grow further than e , with a penalty.
To control the large capacity of the relation matrices in the Trigram model, we have
adapted the two regularization schemes: in the hard scheme, we force the relation matrices
727

fiGarca-Duran, Bordes, Usunier & Grandvalet

Table 2: Statistics of the data sets used in this paper and extracted from four knowledge
bases: FB15k, SVO, Kinships and UMLS.
Data set
Entities
Relationships
Training examples
Validation examples
Test examples

FB15k
14,951
1,345
483,142
50,000
59,071

SVO
30,605
4,547
1,000,000
50,000
250,000

Kinships
104
26
224,973
28,122
28,121

UMLS
135
49
102,612
89,302
89,302

to have, at most, a Frobenius norm of value l , and in the soft one, we include a penalization
term of the form [||R||2F  2l ]+ to the loss function (6) . As a result, in the soft scheme the
following regularization
term is added to the loss function (6): C1 [||e1 ||22 2e ]+ +C2 [||e2 ||22 

2e ]+ + [||R||2F  2l ]+ , where C1 and C2 are hyperparameters that weight the importance of
each soft constraint. In terms of practicality, the bigger flexibility of the soft version comes
with one more hyperparameter. In the following, the suffixes soft and hard are used to refer
to either of those regularization scheme. Tatec has also an other implicit regularization
factor since it is using the same entity representation for an entity regardless of its role as
head or tail.
To sum up, in the hard regularization case, the optimization problem for Tatec-ft is:
X
X


min
  s(h, `, t) + s(h0 , `0 , t0 ) +
(h,`,t)S (h0 ,`0 ,t0 )C(h,`,t)

s.t.

||ei1 ||2  e

i  [[E]]

||ei2 ||2  e
`

||R ||F  l

i  [[E]]
`  [[L]]

And in the soft regularization case it is:
X
X
X
min
[  s(h, `, t) + s(h0 , `0 , t0 )]+ + C1
[||ei1 ||22  2e ]+
(h,`,t)S (h0 ,`0 ,t0 )C(h,`,t)

+ C2

X

[||ei2 ||22  2e ]+ +

i[[E]]

i[[E]]

X

[||R` ||2F  2l ]+



`[[L]]


 fi ff 
 fi ff 
 fi fi ff 
 fi fi ff
where s(h, `, t) = r`1 fieh1 + r`2 fiet1 + eh1 fiDfiet1 + eh2 fiR` fiet2 in both cases.

5. Experiments
This section presents various experiments to illustrate how competitive Tatec is with respect to several state-of-the-art models on 4 benchmarks from the literature: UMLS, Kinships, FB15k and SVO. The statistics of these data sets are given in Table 2. All versions
of Tatec and of its components Bigram and Trigram are compared with the state-ofthe-art models for each database.
5.1 Experimental Setting
This section details the protocols used in our various experiments.
728

fiEmbedding Models for Link Prediction in KBs

5.1.1 Datasets and Metrics
Our experimental settings and evaluation metrics are borrowed from previous works, so as
to allow for result comparisons.
UMLS/Kinships Kinships (Denham, 1973) is a KB expressing the relational structure
of the kinship system of the Australian tribe Alyawarra, and UMLS (McCray, 2003) is
a KB of biomedical high-level concepts like diseases or symptoms connected by verbs like
complicates, affects or causes. For these data sets, the whole set of possible triples,
positive or negative, is observed. We used the area under the precision-recall curve as metric.
The dataset was split in 10-folds for cross-validation: 8 for training, 1 for validation and
the last one for test. Since the number of available negative triples is much bigger than the
number of positive triples, the positive ones of each fold are replicated to match the number
of negative ones.3 These negative triples correspond to the first setting of negative examples
of Section 4.1. The number of training epochs was fixed to 100. Bigram, Trigram and
Tatec models were validated every 10 epochs using the AUC under the precision-recall
curve as validation criterion over 1,000 randomly chosen validation triples - keeping the
same proportion of negative and positive triples. For TransE, which we ran as baseline,
we validated every 10 epochs as well.
FB15k Introduced by Bordes et al. (2013b), this data set is a subset of Freebase, a very
large database of generic facts gathering more than 1.2 billion triples and 80 million entities.
For evaluation on it, we used a ranking metric. The head of each test triple is replaced by
each of the entities of the dictionary in turn, and the score is computed for each of them.
These scores are sorted in descending order and the rank of the correct entity is stored. The
same procedure is repeated when removing the tail instead of the head. The mean of these
ranks is the mean rank, and the proportion of correct entities ranked in the top 10 is the
hits@10. This is called the raw setting. In this setting correct positive triples can be ranked
higher than the target one and hence be counted as errors. In order to reduce this noise in
the measure, and thus granting a clearer view on ranking performance, we remove all the
positive triples that can be found in either the training, validation or testing set, except the
target one, from the ranking. This setting is called filtered (Garca-Duran et al., 2014).
Since FB15k is made up only of positive triples, the negative ones have to be generated.
To do that, in each epoch we generate two negative triples per positive by replacing a single
unit of the positive triple by a random entity (once the head and once the tail). This
corruption approach implements the prior knowledge that unobserved triples are likely to be
invalid, and has been widely used in previous work when learning embeddings of knowledge
bases or words in the context of language models. These negative triples correspond to
the second setting of negative examples of Section 4.1. We ran 500 training epochs for
both TransE, Bigram, Trigram and Tatec, and using the final filtered mean rank as
validation criterion. If several models statistically have similar filtered mean ranks, we take
the hits@10 as secondary validation criterion.4 Since for this dataset, training, validation
3. This replication process is carried out only in training.
4. Garca-Duran et al. (2014) previously reported results on both FB15k and SVO with TransE and
Tatec. However, in this preliminary work, the hyperparameters were validated on a smaller validation
set and a not wide enough grid search, which led to suboptimal results. We hence decided to re-run the
algorithms and got major improvements.

729

fiGarca-Duran, Bordes, Usunier & Grandvalet

and test sets are fixed, to give a confidence interval to our results, we randomly split the
test set into 4 subsets before computing the evaluation metrics. We do this 5 times, and
finally we compute the mean and the standard deviation over these 20 values for mean rank
and hits@10.
SVO SVO is a database of nouns connected by verbs through subject-verb-direct object
relations and extracted from Wikipedia articles. It has been introduced by Jenatton et al.
(2012). For this database we perform a verb prediction task, where one has to assign
the correct verb given two nouns acting as subject and direct object; in other words, we
present results of ranking label given head and tail. As for FB15k, two ranking metrics
are computed, the mean rank and the hits@5%, which is the proportion of predictions for
which the correct verb is ranked in the top 5% of the total number of verbs, that is within
the top 5% of 4,547  227. We use the raw setting for SVO. Due to the different kind
of task (predicting label instead of predicting head /tail ), the negative triples have been
generated by replacing the label by a random verb. These negative triples correspond to
the third setting of negative examples of Section 4.1. For TransE, Bigram and Trigram
the number of epochs has been fixed to 500 and they were validated every 10 epochs. For
Tatec we ran only 10 epochs, and validated for each. The mean rank has been chosen as
validation criterion over 1,000 random validation triples.
5.1.2 Implementation
To pre-train our Bigram and Trigram models we validated the learning rate for the
stochastic gradient descent among {0.1, 0.01, 0.001, 0.0001} and the margin among {0.1, 0.25,
0.5, 1}. The radius e determining the value from which the L2 -norm of the entity embeddings are penalized has been fixed to 1, but the radius l of the Trigram model has
been validated among {0, 1, 5, 10, 20}. Due to the different size of these KBs, the embedding dimension d has been validated in different ranges. For SVO it has been selected
among {25, 50}, among {50, 75, 100} for FB15k and among {10, 20, 40} for UMLS and
Kinships. When the soft regularization is applied, the regularization parameter has been
validated among {0, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100}. For fine-tuning Tatec, the learning
rates were selected among the same values for learning the Bigram and Trigram models
in isolation, independent of the values chosen for pre-training, and so are the margin and
for the penalization terms C1 and C2 if the soft regularization is used. The configurations of
the model selected using their performance on the validation set are given in Appendix A.
Training of the combination weights of Tatec-lc is carried out in an iterative way, by
alternating optimization of  parameters via L-BFGS, and update of  parameters using
`
` = P||||||k2|| , until some stopping criterion is reached. The  parameters are initialized to
k

2

1 and the  value is validated among {0.1, 1, 10, 50, 100, 200, 500, 1000}.
5.1.3 Baselines
Variants We performed breakdown experiments with 2 different versions of Tatec to
assess the impact of its various aspects. These variants are:
 Tatec-ft-no-pretrain: Tatec-ft without pre-training s1 (h, l, t) and s2 (h, l, t).
730

fiEmbedding Models for Link Prediction in KBs

Table 3: Test AUC under the precision-recall curve on UMLS and Kinships for
models from the literature (top) and Tatec (bottom). Best performing methods are in bold.
Model
SME(linear)
RESCAL
LFM
TransE-soft
TransE-hard
Bigram-hard
Trigram-hard
Tatec-ft-hard
Bigram-soft
Trigram-soft
Tatec-ft-soft
Tatec-lc-soft

UMLS
0.983  0.003
0.98
0.990  0.003
0.734  0.033
0.706  0.034
0.936  0.020
0.980  0.006
0.984  0.004
0.936  0.018
0.983  0.004
0.985  0.004
0.985  0.004

Kinships
0.907  0.008
0.95
0.946  0.005
0.135  0.005
0.134  0.005
0.140  0.004
0.943  0.009
0.876  0.012
0.141  0.003
0.948  0.008
0.919  0.008
0.941  0.009

 Tatec-ft-shared: Tatec-ft but sharing the entities embeddings between s1 (h, l, t)
and s2 (h, l, t) and without pre-training.
The experiments with these 3 versions of Tatec have been performed in the soft regularization setting. Their hyperparameters were chosen using the same grid as above.
Previous Models We retrained TransE ourselves with the same hyperparameter grid
as for Tatec and used it as a running baseline on all datasets, using either soft or hard
regularization. In addition, we display the results of the best performing methods of the
literature on each dataset, with values extracted from the original papers.
On UMLS and Kinships, we also report the performance of the 3-way models RESCAL,
LFM and the 2-way SME(linear). On FB15k, recent variants of TransE, such as TransH,
TransR and cTransR (Lin et al., 2015) have been chosen as main baselines. Both in
TransH and TransR/cTransR, the optimal values of the hyperparameters as the dimension, the margin or the learning rate have been selected within similar ranges as those
for Tatec. On SVO, we compare Tatec with three different approaches: Counts, the
2-way model SME(linear) and the 3-way LFM. Counts is based on the direct estimation of
probabilities of triples (head, label, tail) by using the number of occurrences of pairs (head,
label) and (label, tail) in the training set. The results for these models have been extracted
from (Jenatton et al., 2012), and we followed their experimental setting. Since the results
in this paper are only available in the raw setting, we restricted our experiments to this
configuration on SVO as well.
5.2 Results
We recall that the suffixes soft or hard refer to the regularization scheme used, and the suffixes
ft and lc to the combination strategy of Tatec.
731

fiGarca-Duran, Bordes, Usunier & Grandvalet

Table 4: Test results on FB15k and SVO for models from the literature (top), Tatec
(middle) and variants (bottom). Best performing methods are in bold. The filtered setting is used
for FB15k and the raw setting for SVO.

Model
Counts
SME(linear)
LFM
TransH
TransR
cTransR
TransE-soft
TransE-hard
Tatec-no-pretrain
Tatec-shared
Bigram-hard
Trigram-hard
Tatec-ft-hard
Bigram-soft
Trigram-soft
Tatec-ft-soft
Tatec-lc-soft

FB15k
Mean Rank
Hits@10
87
64.4
77
68.7
75
70.2
50.7  2.0
71.5  0.3
50.6  2.0
71.5  0.3
97.1  3.9
65.7  0.2
94.8  3.2
63.4  0.3
94.5  2.9
67.5  0.4
137.7  7.1
56.1  0.4
59.8  2.6
77.3  0.3
87.7  4.1
70.0  0.2
121.0  7.2
58.0  0.3
57.8  2.3
76.7  0.3
68.5  3.2
72.8  0.2

SVO
Mean Rank Hits@5%
517.4
72
199.6
77
195
78
282.5  1.7
70.6  0.2
282.8  2.3
70.6  0.2
219.2  1.9
77.6  0.1
187.9  1.2
79.5  0.1
188.5  1.9
79.8  0.1
211.9  1.8
77.8  0.1
189.2  2.1
79.5  0.2
185.4  1.5
80.0  0.1
182.6  1.2
80.1  0.1

5.2.1 UMLS and Kinships
The results for these two knowledge bases are provided in Table 3. In UMLS, most models
are performing well. The combination of the Bigram and Trigram models is slightly
better than the Trigram alone but it is not significant. It seems that the constituents
of Tatec, Bigram and Trigram, do not encode very complementary information and
their combination does not bring much improvement. Basically, on this dataset, many
methods are somewhat as efficient as the best one, LFM. The difference between TransE
and Bigram on this dataset illustrates the potential impact of the diagonal matrix D,
which does not constrain embeddings of both head and tail entities of a triple to be similar.
Regarding Kinships, there is a big gap between 2-way models like TransE and 3way models like RESCAL. The cause of this deterioration comes from a peculiarity of the
positive triples of this KB: each entity appears 104 times  the number of entities in this
KB  as head and it is connected to the 104 entities  even itself  only once. In other
words, the conditional probabilities P (head|tail) and P (tail|head) are totally uninformative.
This has a very important consequence for the 2-way models since they highly rely on such
information: for Kinships, the interaction head-tail is, at best, irrelevant, though in practice
this interaction may even introduce noise.
Due to the poor performance of the Bigram model, when it is combined with the
Trigram model this combination can turn out to be detrimental w.r.t. the performance
of Trigram in isolation: 2-way models are quite noisy for this KB and we cannot take
advantage of them. On the other side the Trigram model logically reaches a very similar
732

fiEmbedding Models for Link Prediction in KBs

Table 5: Test results on FB15k. Proportion of entities ranked in the Top 1.
Model
TransE-soft
Bigram-soft
Trigram-soft
Tatec-ft-soft

Hits@1
28.1
27.2
24.9
37.8

performance to RESCAL, and similar to LFM as well. Performance of Tatec versions
based on fine-tuning of the parameters (Tatec-ft) are worse than that of Trigram because
Bigram degrades the model. Tatec-lc, using a  potentially sparse  linear combination
of the models, does not have this drawback since it can completely cancel out the influence
of bigram model. As a conclusion from the experiments in this KB, when one of the
components of Tatec is quite noisy, we should directly remove it and Tatec-lc can do it
automatically. The soft regularization setting seems to be slightly better also.
5.2.2 FB15k
Table 4 (left) displays results on FB15k. Unlike for Kinships, here the 2-way models
outperform the 3-way models in both mean rank and hits@10. The simplicity of the 2-way
models seems to be an advantage in FB15k: this is something that was already observed
by Yang, Yih, He, Gao, and Deng (2014a). The combination of the Bigram and Trigram
models into Tatec leads to an impressive improvement of the performance, which means
that for this KB the information encoded by these 2 models are complementary. Tatec
outperforms all the existing methods  except TransE in mean rank  with a wide margin
in hits@10. Bigram-soft performs roughly like cTransR, and better than its counterpart
Bigram-hard. Though Trigram-soft is better than Trigram-hard as well, Tatec-ft-soft
and Tatec-ft-hard converge to very similar performances. Fine-tuning the parameters is
there better than simply using a linear combination even if Tatec-lc still performs well.
Tatec-ft outperforms both variants Tatec-shared and Tatec-no-pretrain by a
wide margin, which confirms that both pre-training and the use of different embeddings
spaces are essential to properly collect the different data patterns of the Bigram and Trigram models: by sharing the embeddings we constrain too much the model, and without
pre-training Tatec is not able to encode the complementary information of its constituents.
The performance of Tatec in these cases is in-between the performances of the soft version
of the Bigram and Trigram models, which indicates that they converge to a solution
that is not even able to reach the best performance of their constituent models. Table 5
displays the Hits@1 for several of these models. Whereas the differences in the performance
of Bigram and Trigram are not so large as the ones shown in hits@10, Tatec is still the
best model by a wide margin.
We also broke down the results by type of relation, classifying each relationship according
to the cardinality of their head and tail arguments. A relationship is considered as 1-to-1,
1-to-M, M-to-1 or M-M regarding the variety of arguments head given a tail and vice versa.
If the average number of different heads for the whole set of unique pairs (label, tail) given a
relationship is below 1.5 we have considered it as 1, and the same in the other way around.
The number of relations classified as 1-to-1, 1-to-M, M-to-1 and M-M is 353, 305, 380 and
733

fiGarca-Duran, Bordes, Usunier & Grandvalet

Table 6: Detailed results by category of relationship. We compare our Bigrams,
Trigram and Tatec models in terms of Hits@10 (in %) on FB15k in the filtered setting
against other models of the literature. (M. stands for Many).
Task
Rel. category
TransE-soft
TransH
TransR
cTransR
Bigram-soft
Trigram-soft
Tatec-ft-soft

1-to-1
76.2
66.8
78.8
81.5
76.2
56.4
79.3

Predicting head
1-to-M. M.-to-1 M.-to-M.
93.6
47.5
70.2
87.6
28.7
64.5
89.2
34.1
69.2
89
34.7
71.2
90.3
37.4
70.1
79.6
30.2
57
93.2
42.3
77.2

1-to-1
76.7
65.5
79.2
80.8
75.9
53.1
78.5

Predicting tail
1-to-M. M.-to-1 M.-to-M.
50.9
93.1
72.9
39.8
83.3
67.2
37.4
90.4
72.1
38.6
90.1
73.8
44.4
89.8
72.8
28.8
81.6
60.8
51.5
92.7
80.7

307, respectively. The results are displayed in the Table 6. Bigram and Trigram models
cooperate in a constructive way for all the types of relationship when predicting both the
head and tail. Tatec-ft is remarkably better for M-to-M relationships.
5.2.3 SVO
Tatec achieves also a very good performance on this task since it outperforms all previous
methods on both metrics. As before, both regularization strategies lead to very similar
performances, but the soft setting is slightly better. In terms of hits@5%, Tatec outperforms its constituents, however in terms of mean rank the Bigram model is considerably
worse than Trigram and Tatec. The performance of LFM is in between the Trigram
and Bigram models, which confirms the fact that sharing the embeddings in the 2- and
3-way terms can actually prevent to make the best use of both types of interaction.
As for Kinships, since here the performance of Bigram is much worse than that of
Trigram, Tatec-lc is very competitive. It seems that when Bigram and Trigram
perform well for different types of relationships (such as in FB15k), then combining them
via fine-tuning (i.e. Tatec-ft) allows to get the best of both; however, if one of them is
consistently performing worse on most relationships as it seems to happen for Kinships
and SVO, then Tatec-lc is a good choice since it can cancel out any influence of the bad
model. Table 7 depicts training times of various models on FB15k, presenting the relative
time w.r.t. TransE for one training epoch. To speedup training, we could follow one or
several of the following strategies:
 use adaptive learning rates in order to make convergence faster;
 train the model on GPUs, which is quite usual in the deep learning community when
working with large datasets;
 parallellize training with Hogwild (Recht, Re, Wright, & Niu, 2011).
We could also speed up the validation. For example, since the entities and relationships are
usually strongly typed (i.e. given a relationship, only a subset of entities are real candidates
for both the subject and object), we might consider only entities of the suitable type for a
given relationship and role. Nevertheless, given that scalability is not a major issue on the
datasets used in this paper we did not look for any speed optimization here.
734

fiEmbedding Models for Link Prediction in KBs

Table 7: Relative training times with respect to TransE on FB15k for running one
epoch on a single core.
Model
Bigram-soft
Trigram-soft
Tatec-ft-soft

Relative train. time
 1.4
 3.6
 4.0

5.3 Illustrative Experiments
This last experimental section provides some illustrations and insights on the performance
of Tatec and TransE.
5.3.1 TransE and Symmetrical Relationships
TransE has a peculiar behavior: it performs very well on FB15k but quite poorly on
all the other datasets. Looking in detail at FB15k, we noticed that this database is
made up of a lot of pairs of symmetrical relationships such as /film/film/subjects and
/film/film subject/films, or /music/album/genre and /music/genre/albums. The
simplicity of the translation model of TransE works well when, for predicting the validity
of an unknown triple, the model can make use of its symmetrical counterpart if it was
present in the training set. Specifically, 45,817 out of 59,071 test triples of FB15k have
a symmetrical triple in the training set. If we split the test triples into two subsets, one
containing the test triples for which a symmetrical triple has been used in the learning
stage and the other containing those ones for which a symmetrical triple does not exist
in the training set, the overall mean rank of TransE of 50.7 is decomposed into a mean
rank of 17.5 and 165.7, and the overall hits@10 of 71.5 is decomposed into 76.6 and 53.7,
respectively. TransE makes a very adequate use of this particular feature. In the original
TransE paper (Bordes et al., 2013b), the algorithm is shown to perform well on FB15k
and on a dataset extracted from the KB WordNet (Miller, 1995): we suspect that the
WordNet dataset also contains symmetrical counterparts of test triples in the training set
(such as hyperonym vs hyponym, meronym vs holonym).
Tatec can also make use of this information and is, as expected, much better on relations
with symmetrical counterparts in train: on FB15k, the mean rank of Tatec-ft-soft is of
17.5 for relations with symmetrical counterparts 197.4 instead and hits@10 is of 84.4%
instead of 50%. Yet, as results on other datasets show, Tatec is also able to generalize
when more complex information needs to be taken into account.
5.3.2 Anecdotal Examples
Some examples of predictions by Tatec on FB15k are displayed in Table 8. In the first
row, we want to know the answer to the question What is the location of the polish
national football team?; among the possible answers we find not only locations, but
more specifically countries, which makes sense for a national team. For the question What is
the topic of the film Remember the titans? the top-10 candidates may be potential film topics. Same for the answers to the question Which religion does Noam Chomsky
belong to? that can all be typed as religions. In these examples, both sides of the re735

fiGarca-Duran, Bordes, Usunier & Grandvalet

Table 8: Examples of predictions on FB15k. Given an entity and a relation type from a
test triple, Tatec fills in the missing slot. In bold is the expected correct answer.
Triple
(poland national football team, /sports team/location, ?)

(?, /film/film subject/films , remember the titans)

(noam chomsky, /people/person/religion, ?)

(?, /webpage/category, official website)

Top-10 predictions
Mexico, South Africa, Republic of Poland
Belgium, Puerto Rico, Austria, Georgia
Uruguay, Colombia, Hong Kong
racism, vietnam war, aviation, capital punishment
television, filmmaking, Christmas
female, english language, korean war
atheism, agnosticism, catholicism, ashkenazi jews
buddhism, islam, protestantism
baptist, episcopal church, Hinduism
supreme court of canada, butch hartman, robyn hitchcoc, mercer university
clancy brown, dana delany, hornets
grambling state university, dnipropetrovsk, juanes

60

Singers
Japanese singers
British MPNS
Glee casting
Attorneys in the USA

40

Singers
Japanese singers
British MPNS
Glee casting
Attorneys in the USA

60

40
20

20
0

0
20

20
40

40
60
60

40

20

0

20

40

60

80

40

(a) Embeddings of Trigram

30

20

10

0

10

20

30

40

50

(b) Embeddings of Bigram

Figure 4: Embeddings obtained by Trigram and Bigram models and projected in
2-D using t-SNE. MPNS stands for Main Profession is Not Singer.
lationship are clearly typed: a certain type of entity is expected in head or tail (country,
religion, person, movie, etc.). The operators of Tatec may then operate on specific regions
of the embedding space. On the contrary, the relationship /webpage/category is an example of non-typed relationship. This one, which could actually be seen as an attribute rather
than a relationship, indicates if the entity head has a topic website or an official website.
Since many types of entities can have a webpage and there is little to no correlation among
relationships, predicting the left-hand side argument is nearly impossible.
Figures 4a and 4b show 2D projections of embeddings of selected entities for the Trigram and Bigram models trained on FB15k, respectively, obtained by projecting them
using t-SNE (Van der Maaten & Hinton, 2008). This projection has been carried out only
for Freebase entities whose profession is either singer or attorney in the USA. We can
observe in Figure 4a that all attorneys are clustered and separated from the singers, except
one, which corresponds to the multifaceted Fred Thompson5 . However, embeddings of the
singers are not clearly clustered: since singers can appear in a multitude of triples, their
layout is the result of a compendium of (sometimes heterogeneous) categories. To illustrate
graphically the different data patterns to which Bigram and Trigram respond, we focus
on the small cluster made up of Japanese singers that can be seen in Figure 4a (Trigram).
In Figure 4b (Bigram) however, these same entities are more diluted in the whole set of
5. Apart from being an attorney, he is an actor, a radio personality, a lawyer and a politician

736

fiEmbedding Models for Link Prediction in KBs

Table 9: Examples of predictions on SVO. Given two nouns acting as subject and direct
object from a test triple, Tatec predicts the best fitting verb. In bold is the expected correct answer.
Triple
(bus, ?

, service)

(emigrant, ?

, country)

(minister, ?, protest)
(vessel, ?, coal)
(tv channel, ?, video)
(great britain, ?, north america)

Top-10 predictions
use, provide, run, have, include
carry, offer, enter, make, take
flee, become, enter, leave, form
dominate, establish, make, move, join
lead, organize, join, involve, make
participate, conduct, stag, begin, attend
use, transport, carry, convert, send
make, provide, supply, sell, contain
feature, make, release, use, produce
have, include, call, base, show
include, become, found, establish, dominate
name, have, enter, form, run

singers. Looking at the neighboring embeddings of these Japanese singers entities in Figure
4b, we find entities highly connected to japan like yoko ono  born in Japan, vic mignogna,
greg ayres, chris patton or laura bailey  all of them worked in the dubbing industry
of Japanese anime movies and television series. This shows the impact of the interaction
between heads and tails in the Bigram model: it tends to push together entities connected
in triples whatever the relation. In this case, this forms a Japanese cluster.
Table 9 shows examples of predictions on SVO. In the first example, though run is
the target verb for the pair (bus, service), other verbs like provide or offer are good
matches as well. Similarly, non-target verbs like establish or join, and lead, participate
or attend are good matches for the second and third examples ((emigrant, country) and
(minister, protest)) respectively. The fourth and fifth instances show an example of
very heterogeneous performance for a same relationship (the target verb is transport in
both cases) which can be easily explained from a semantic point of view: transport is a
very good fit given the pair (vessel, coal), whereas a TV channel transports video is
not a very natural way to express that one can watch videos in a TV channel, and hence
this leads to a very poor performance  the target verb is ranked #696. The sixth example
is particularly interesting, since even if the target verb, colonize, is ranked very far in the
list (#344), good candidates for the pair (Great Britain, North America) can be found
in the top-10. Some of them have a similar representation as colonize, because they are
almost synonyms, but they are ranked much higher. This is an effect of the verb frequency.
As illustrated in Figure 5a, the more frequent a relationship is, the higher its Frobenius
norm is; hence, verbs with similar meanings but unbalanced frequencies can be ranked
differently, which explains that a rare verb, such as colonize, can be ranked much worse
than other semantically similar words. A consequence of this relation between the Frobenius
norm and the appearance frequency is that usual verbs tend to be highly ranked even though
sometimes they are not good matches, due to the influence of the norm in the score. In
that figure, we can see that the Frobenius norm of the relation matrices are larger in the
regularized (soft) case than in the unregularized case. This happens because we fixed a
very large value for both C2 and l in the regularized case (e is fixed to 1). It imposes
a strong constraint on the norm of the entities but not on the relationship matrices and
makes the Frobenius norm of these matrices absorb the whole impact of the norm of the
737

fiGarca-Duran, Bordes, Usunier & Grandvalet

250

5000

Regularized (soft)
Unregularized
Regularized (hard)

4000

Filtered Mean Rank

200

||Rl||2F

150

100

50

3000

2000

1000

0

0 0
10

1

2

10

10

3

10

4

10

1000 0
10

5

10

1

10

2

10

3

10

4

10

5

10

Number of triples

Number of triples

(a) Frobenius norm of the rel. matrices according (b) Test mean rank according to the number of
training triples of each relationship.
to the number of training triples of each rel.

Figure 5: Indicators of the behavior of Tatec-ft on FB15k according to to the number
of training triples of each relationship.
Table 10: Examples of predictions on SVO for a regularized and an unregularized
Trigram. In bold is the expected correct answer.
Triple
(bus, ?

, service)

(emigrant, ?

, country)

(minister, ?

, protest)

(vessel, ?

, coal)

(tv channel, ?, video)
(great britain, ?, north america)

Top-10 predictions
Unregularized
Regularized (soft)
use, operate, offer, call, build,
provide, use, have, include, make,
include, have, know, make, create
offer, take, carry, serve, run
use, represent, save, flee, visit,
flee, become, come, enter, found,
come, make, leave, create, know
include, form, make, leave, join
bring, lead, reach, have, become, lead, organize, conduct, participate, join
say, include, help, leave, appoint
make, involve, support, suppress, raise
take, use, have, carry, make,
use, transport, make, carry, deliver,
hold, move, become, fill, serve
send, contain, supply, leave, provide
make, include, write, know, have,
release, make, feature, produce, have,
produce, use, play, give, become
include, use, take, show, base
have, use, include, make, leave,
include, found, become, run,name,
become, know, take, call, build
move, annex, form, establish, dominate

score, and, thus, the impact of the verb frequency. We could down-weight the importance
of the verb frequency by tuning the parameters l and C2 to enforce a stronger constraint.
Figure 10 shows the effect of the verb frequency in these two models when predicting the
same missing verb as in Table 9.
Breaking down the performance by relationship, this is translated into a strong relation
between the performance of a relationship and its frequency (see Figure 5b). However, the
same relation between the 2-norm of the entities embeddings and their frequency is not
observed, which can be explained given that an entity can appear in the left and right
argument in an unbalanced way.

6. Conclusion
This paper presents Tatec, a tensor factorization method that satisfactorily combines 2and 3-way interaction terms to obtain a performance better than the best of either constituent. Different data patterns are properly encoded thanks to the use of different embedding spaces and of a two-phase training (pre-training and fine-tuning/linear-combination).
Experiments on four benchmarks for different tasks and with different quality measures
738

fiEmbedding Models for Link Prediction in KBs

prove the strength and versatility of this model, whose scoring function, as we argue in Section 3.1, is tightly connected to other energy-based model of the literature such as TransE,
RESCAL or LFM. Our experiments also allow us to draw some conclusions about the
two usual regularization schemes used so far in these embedding-based models: they both
achieve similar performances, even if soft regularization appears slightly more efficient but
with one extra-hyperparameter.
Our work uses FB15k as its main testbed but our model is not tied to any KB schema;
as long as data is formatted as triples, Tatec can be applied. Hence, it could be directly
applied to many Linked Data settings especially if they are in RDF format.

Acknowledgments
This work was carried out in the framework of the Labex MS2T (ANR-11-IDEX-0004-02),
and was funded by the French National Agency for Research (EVEREST-12-JS02-005-01).
Part of this work was done while Nicolas Usunier was with Sorbonne universites, Universite
de technologie de Compiegne, CNRS, Heudiasyc UMR 7253.

Appendix A. Optimal Hyperparameters
The optimal configurations for UMLS are:
- TransE-soft: d = 40,  = 0.01,  = 0.5, C = 0;
- Bigram-soft: d1 = 40, 1 = 0.01,  = 0.5, C = 0.1;
- Trigram-soft: d2 = 40, 2 = 0.01,  = 1, C = 0.1, l = 5;
- Tatec-soft: d1 = 40, d2 = 40, 1 = 2 = 0.001,  = 1, C1 = C2 = 0.01, l = 5;
- TransE-hard: d = 40,  = 0.01,  = 0.1;
- Bigram-hard: d1 = 40, 1 = 0.01,  = 0.5;
- Trigram-hard: d2 = 40, 2 = 0.01,  = 1, l = 10;
- Tatec-hard: d1 = 40, d2 = 40, 1 = 2 = 0.001,  = 1, l = 10.
- Tatec-linear-comb: d1 = 40, d2 = 40,  = 0.5,  = 50.
The optimal configurations for Kinships are:
- TransE-soft: d = 40,  = 0.01,  = 1, C = 0;
- Bigram-soft: d1 = 40, 1 = 0.01,  = 1, C = 1;
- Trigram-soft: d2 = 40, 2 = 0.01,  = 0.5, C = 0.1, l = 5;
- Tatec-soft: d1 = 40, d2 = 40, 1 = 2 = 0.001,  = 1, C1 = 100, C2 = 0.0001, l = 10;
- TransE-hard: d = 40,  = 0.01,  = 1;
- Bigram-hard: d1 = 40, 1 = 0.01,  = 1;
- Trigram-hard: d2 = 40, 2 = 0.01,  = 0.5, l = 10;
- Tatec-hard d1 = 40, d2 = 40, 1 = 2 = 0.001,  = 1, l = 10.
- Tatec-linear-comb: d1 = 40, d2 = 40,  = 1,  = 10.
The optimal configurations for FB15k are:
- TransE-soft: d = 100,  = 0.01,  = 0.25, C = 0.1;
- Bigram-soft: d1 = 100, 1 = 0.01,  = 1, C = 0;
739

fiGarca-Duran, Bordes, Usunier & Grandvalet

-

Trigram-soft: d2 = 50, 2 = 0.01,  = 0.25, C = 0.001, l = 1;
Tatec-soft: d1 = 100, d2 = 50, 1 = 2 = 0.001,  = 0.5, C1 = C2 = 0;
TransE-hard: d = 100,  = 0.01,  = 0.25;
Bigram-hard: d1 = 100, 1 = 0.01,  = 0.25;
Trigram-hard: d2 = 50, 2 = 0.01,  = 0.25, l = 5;
Tatec-hard: d1 = 100, d2 = 50, 1 = 2 = 0.001,  = 0.25, l = 5;
Tatec-no-pret: d1 = 100, d2 = 50, 1 = 2 = 0.01,  = 0.25, C1 = 0, C2 = 0.001, l = 1;
Tatec-shared: d1 = d2 = 75, 1 = 2 = 0.01,  = 0.25, C1 = C2 = 0.001, l = 5;
Tatec-linear-comb: d1 = 100, d2 = 50,  = 0.25,  = 200.

The optimal configurations for SVO are:
- TransE-soft: d = 50,  = 0.01,  = 0.5, C = 1;
- Bigram-soft: d1 = 50, 1 = 0.01,  = 1, C = 0.1;
- Trigram-soft: d2 = 50, 2 = 0.01,  = 1, , C = 10, l = 20;
- Tatec-soft: d1 = 50, d2 = 50, 1 = 2 = 0.0001,  = 1, C1 = 0.1, C2 = 1, l = 20;
- TransE-hard: d = 50,  = 0.01,  = 0.5;
- Bigram-hard: d1 = 50, 1 = 0.01,  = 1;
- Trigram-hard: d2 = 50, 2 = 0.01,  = 1, l = 20;
- Tatec-hard: d1 = 50, d2 = 50, 1 = 2 = 0.0001,  = 1, l = 20.
- Tatec-linear-comb: d1 = 50, d2 = 50,  = 1,  = 50.

References
Ashburner, M., Ball, C. A., Blake, J. A., Botstein, D., Butler, H., Cherry, J. M., Davis,
A. P., Dolinski, K., Dwight, S. S., Eppig, J. T., et al. (2000). Gene ontology: tool for
the unification of biology. http://geneontology.org/.
Bizer, C., Heath, T., Idehen, K., & Berners-Lee, T. (2008). Linked data on the web
(ldow2008). http://linkeddata.org/.
Bollacker, K., Evans, C., Paritosh, P., Sturge, T., & Taylor, J. (2008). Freebase: a
collaboratively created graph database for structuring human knowledge. http:
//www.freebase.com.
Bordes, A., Glorot, X., Weston, J., & Bengio, Y. (2013a). A semantic matching energy
function for learning with multi-relational data. Machine Learning, 94, 233259.
Bordes, A., Usunier, N., Garca-Duran, A., Weston, J., & Yakhnenko, O. (2013b). Translating embeddings for modeling multi-relational data. In Advances in Neural Information
Processing Systems, pp. 27872795.
Chang, K.-W., Yih, W.-t., Yang, B., & Meek, C. (2014). Typed tensor decomposition of
knowledge bases for relation extraction. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing (EMNLP), pp. 15681579.
Denham, W. (1973). The detection of patterns in Alyawarra nonverbal behavior. Ph.D.
thesis, University of Washington.
Dong, X., Gabrilovich, E., Heitz, G., Horn, W., Lao, N., Murphy, K., Strohmann, T., Sun,
S., & Zhang, W. (2014). Knowledge vault: A web-scale approach to probabilistic
740

fiEmbedding Models for Link Prediction in KBs

knowledge fusion. In Proceedings of the 20th ACM SIGKDD international conference
on Knowledge discovery and data mining, pp. 601610. ACM.
Garca-Duran, A., Bordes, A., & Usunier, N. (2014). Effective blending of two and threeway interactions for modeling multi-relational data. In ECML PKDD 2014. Springer
Berlin Heidelberg.
Gardner, M., Talukdar, P. P., Krishnamurthy, J., & Mitchell, T. (2014). Incorporating
vector space similarity in random walk inference over knowledge bases. In Conference
on Empirical Methods in Natural Language Processing, EMNLP 2014, pp. 397406.
Jenatton, R., Le Roux, N., Bordes, A., & Obozinski, G. (2012). A latent factor model for
highly multi-relational data. In NIPS 25.
Kemp, C., Tenenbaum, J. B., Griffiths, T. L., Yamada, T., & Ueda, N. (2006). Learning
systems of concepts with an infinite relational model. In Proc. of the 21st national
conf. on Artif. Intel. (AAAI), pp. 381388.
Knight, K., & Luk, S. K. (1994). Building a large-scale knowledge base for machine translation. In AAAI, Vol. 94, pp. 773778.
Kok, S., & Domingos, P. (2007). Statistical predicate invention. In Proceedings of the 24th
international conference on Machine learning, ICML 07, pp. 433440.
Koren, Y., Bell, R., & Volinsky, C. (2009). Matrix factorization techniques for recommender
systems. Computer, 42 (8), 3037.
Krompass, D., Baier, S., & Tresp, V. (2015). Type-constrained representation learning in
knowledge graphs. arXiv preprint arXiv:1508.02593.
Lao, N., Mitchell, T., & Cohen, W. W. (2011). Random walk inference and learning in a
large scale knowledge base. In Proceedings of the Conference on Empirical Methods in
Natural Language Processing, pp. 529539. Association for Computational Linguistics.
Lin, Y., Liu, Z., Sun, M., Liu, Y., & Zhu, X. (2015). Learning entity and relation embeddings
for knowledge graph completion. In Proceedings of AAAI15.
McCray, A. T. (2003). An upper level ontology for the biomedical domain. Comparative
and Functional Genomics, 4, 8088.
Miller, G. (1995). WordNet: a Lexical Database for English. Communications of the ACM,
38 (11), 3941.
Navigli, R., & Velardi, P. (2005). Structural semantic interconnections: a knowledge-based
approach to word sense disambiguation. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 27 (7), 10751086.
Nickel, M., Tresp, V., & Kriegel, H.-P. (2011). A three-way model for collective learning on
multi-relational data. In Proceedings of the 28th International Conference on Machine
Learning (ICML-11), pp. 809816.
Ponzetto, S. P., & Strube, M. (2006). Exploiting semantic role labeling, wordnet and
wikipedia for coreference resolution. In Proceedings of the main conference on Human
Language Technology Conference of the North American Chapter of the Association of
Computational Linguistics, pp. 192199. Association for Computational Linguistics.
741

fiGarca-Duran, Bordes, Usunier & Grandvalet

Recht, B., Re, C., Wright, S., & Niu, F. (2011). Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in Neural Information Processing
Systems 24, pp. 693701.
Salakhutdinov, R., & Srebro, N. (2010). Collaborative filtering in a non-uniform world:
Learning with the weighted trace norm. tc (X), 10, 2.
Socher, R., Chen, D., Manning, C. D., & Ng, A. Y. (2013). Reasoning With Neural Tensor Networks For Knowledge Base Completion. In Advances in Neural Information
Processing Systems 26.
Sutskever, I., Salakhutdinov, R., & Tenenbaum, J. (2009). Modelling relational data using
bayesian clustered tensor factorization. In Adv. in Neur. Inf. Proc. Syst. 22.
Van der Maaten, L., & Hinton, G. (2008). Visualizing data using t-sne. Journal of Machine
Learning Research, 9 (2579-2605), 85.
Wang, Y. J., & Wong, G. Y. (1987). Stochastic blockmodels for directed graphs. Journal
of the American Statistical Association, 82 (397).
Wang, Z., Zhang, J., Feng, J., & Chen, Z. (2014a). Knowledge graph and text jointly
embedding. In Proceedings of the 2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP).
Wang, Z., Zhang, J., Feng, J., & Chen, Z. (2014b). Knowledge graph embedding by translating on hyperplanes. In Proceedings of the Twenty-Eighth AAAI Conference on
Artificial Intelligence, pp. 11121119.
Wolfram Research, I. (2009). Wolfram alpha. http://www.wolframalpha.com.
Yang, B., Yih, W.-t., He, X., Gao, J., & Deng, L. (2014a). Learning multi-relational semantics using neural-embedding models. CoRR, abs/1411.4072.
Yang, M.-C., Duan, N., Zhou, M., & Rim, H.-C. (2014b). Joint relational embeddings
for knowledge-based question answering. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing (EMNLP), pp. 645650.
Zhang, J., Salwen, J., Glass, M., & Gliozzo, A. (2014). Word semantic representations using
bayesian probabilistic tensor factorization. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing (EMNLP).

742

fiJournal of Artificial Intelligence Research 55 (2016) 11351178

Submitted 12/15; published 04/16

Exploiting Causality for Selective Belief Filtering in
Dynamic Bayesian Networks
Stefano V. Albrecht

svalb@cs.utexas.edu

Department of Computer Science
The University of Texas at Austin
Austin, TX 78712, USA

Subramanian Ramamoorthy

s.ramamoorthy@ed.ac.uk

School of Informatics
The University of Edinburgh
Edinburgh, EH8 9AB, UK

Abstract
Dynamic Bayesian networks (DBNs) are a general model for stochastic processes with
partially observed states. Belief filtering in DBNs is the task of inferring the belief state (i.e.
the probability distribution over process states) based on incomplete and noisy observations.
This can be a hard problem in complex processes with large state spaces. In this article, we
explore the idea of accelerating the filtering task by automatically exploiting causality in
the process. We consider a specific type of causal relation, called passivity, which pertains
to how state variables cause changes in other variables. We present the Passivity-based
Selective Belief Filtering (PSBF) method, which maintains a factored belief representation
and exploits passivity to perform selective updates over the belief factors. PSBF produces
exact belief states under certain assumptions and approximate belief states otherwise, where
the approximation error is bounded by the degree of uncertainty in the process. We show
empirically, in synthetic processes with varying sizes and degrees of passivity, that PSBF is
faster than several alternative methods while achieving competitive accuracy. Furthermore,
we demonstrate how passivity occurs naturally in a complex system such as a multi-robot
warehouse, and how PSBF can exploit this to accelerate the filtering task.

1. Introduction
Dynamic Bayesian networks (DBNs) (Dean & Kanazawa, 1989) are a general model for
stochastic processes with partially observed states. The topology of a DBN is a compact
specification of how variables in the process interact during transitions (cf. Figure 1). Given
the possible incompleteness and noise in observations, it may not generally be possible to
infer the state of the process with absolute certainty. Instead, we may infer beliefs about the
process state based on the history of observations, in the form of a probability distribution
over the state space of the process. This is often called a belief state and the task of calculating
belief states is commonly referred to as belief filtering.
A number of exact and approximate inference methods exist for Bayesian networks (see,
e.g., Koller & Friedman, 2009; Pearl, 1988) which can be used for filtering in DBNs, by
applying them to the unrolled DBN in which the t + 1 slice is repeated for each observed
time step, or via a successive update in which the current posterior (belief state) is used
c
2016
AI Access Foundation. All rights reserved.

fiAlbrecht & Ramamoorthy

xt1

xt+1
1

y1t+1

xt2

xt+1
2

y2t+1

t

t+1

Figure 1: Example of a dynamic Bayesian network (DBN) with two state variables and two
observation variables. The xti and xt+1
variables represent the process states at time t and
i
t + 1, respectively, while the yit+1 variables (shaded) represent the observation at time t + 1.
The arrows describe how the variables interact.

as the prior in the next time step (see also Murphy, 2002). However, it is clear that the
unrolled variant becomes intractable as the network grows unboundedly with time. Even
in the successive update, exact methods become intractable in high-dimensional process
states and approximate methods may propagate growing errors over time. Therefore, filtering
methods were developed which utilise the special structure of DBNs and maintain the errors
propagated over time. (We defer a detailed discussion of such methods to Section 2.)
Often, the key to developing efficient filtering methods is to identify structure in the
process which can be leveraged for inference. In this article, we are interested in the application
of DBNs as representations of actions in partially observed decision processes, such as
POMDPs (Kaelbling, Littman, & Cassandra, 1998; Sondik, 1971) and their many variants.
DBNs can be used to represent the effects of actions on the decision process, by specifying
how variables interact and what information the decision maker observes. In many cases,
decision processes exhibit high degrees of causal structure (Pearl, 2000), by which we mean
that a change in one part of the process may cause a change in another part. Our experience
with such processes is that this causal structure may be used to make the filtering task more
tractable, because it can tell us that beliefs need only be revised for certain aspects of the
process state. For example, if the variable x2 in Figure 1 changes its value only if variable x1
changed its value (i.e. a change in x1 causes a change in x2 ), then it seems intuitive to use
this causal relation when deciding whether to revise ones belief about x2 . Unfortunately,
current filtering methods do not take such causal structure into account.
We refer to the above type of causal relation (between x1 and x2 ) as passivity. Intuitively,
we say that a state variable xi is passive in a given action if, when executing that action,
there is a subset of the state variables that directly affect xi (i.e. xi s parents in the DBN)
such that xi may change its value only if at least one of the variables in this subset changed
its value. It is worth pointing out that passivity occurs naturally and frequently in many
planning domains, especially in robotic and other physical systems (Mainzer, 2010). The
following example1 illustrates this in a simple robot arm:

1. We mark the end of an example with a solid black square.

1136

fiExploiting Causality for Selective Belief Filtering in DBNs

3
2

2

1

3

A

XA

B

XB

1

(a) Robot arm with gripper

C
(b) Holding blocks B and A

Figure 2: Robot arm with three rotational joints and gripper. The variables i represent the
absolute orientations of the corresponding joints.
Example 1 (Robot arm). Consider a robot arm with three rotational joints and a gripper,
as shown in Figure 2a. The joints are denoted by 1 , 2 , 3 and may take any values from the
discrete set {0 , 1 , ..., 359 } which indicate their absolute orientations (e.g. i = 0 means
that joint i points exactly to the right, i = 180 means that it points to the left). For each
joint i, let there be two actions CWi and CCWi which rotate the joint by 1 clockwise and
counter-clockwise, respectively. The uncertainty in this system could be due to stochastic
joint movements or unreliable sensor readings for the joint orientations.
For any action CWi or CCWi , the variable i is not passive because its value is directly
modified by the action. However, the variables j6=i are passive because they change their
values only if the corresponding preceding variable j1 changed its value, since a changed
orientation of joint j  1 causes a changed orientation of joint j (recall that the orientations
are absolute). Note that this also accounts for chains of such causal effects, as indicated by
the arrows: the orientation of joint 3 changes if the orientation of joint 1 changes, since joint
1 causes joint 2 to change, which in turn causes joint 3 to change.
Further examples of passivity can be seen in the context of object manipulation, such as
in the blocks planning domain (e.g. Pasula, Zettlemoyer, & Kaelbling, 2007). Figure 2b
shows the arm holding blocks B and A, with A on top of B. Here, the position of B (XB ) is
passive with respect to the joint orientations since it will only change if any of the orientations
changed. Furthermore, there is a causal chain from the joint orientations to the position of
block A (XA ), since As position will change if Bs position changes.

How can passivity be exploited to accelerate the filtering task in the above example? The
fact that the state variables are passive means that some aspects of the state may remain
unchanged, depending on which action we choose. For example, if we choose to rotate joint
3, then the fact that joints 1 and 2 are passive means that they are unaffected by this action.
Thus, it seems redundant to revise beliefs for the orientations of joints 1 and 2. However,
this is precisely what current filtering methods do (cf. Section 2).
More concretely, assume we use a factored belief representation P (1 , 2 , 3 ) = P (1 , 2 ) 
P (2 , 3 ) and choose to rotate 3 in any direction. Then, it is easy to see that we will need
to update the factor P (2 , 3 ), since 3 changes its value, but not the factor P (1 , 2 ), since
the variables 1 , 2 are both passive. Since the parents of 1 , 2 (if any) do not change their
values, we know that 1 , 2 will not change their values either. As we will show later, skipping
1137

fiAlbrecht & Ramamoorthy

over P (1 , 2 ) does not result in a loss of information in such cases, and similarly for chains
of such causal connections (cf. Example 1). A more complex example of a planning domain
involving passivity, and how it can be exploited, is discussed in Section 6.2.
In addition to guiding belief revision, there are several features which make passivity
an interesting example of a causal relation: First of all, passivity is a latent causal relation,
meaning that it can be readily extracted from the process dynamics without additional
annotation by an expert. (In Section 4, we give a procedure which identifies passive variables
based on their conditional probability tables.) Furthermore, passivity is not a deterministic
relation since passive variables may have any stochastic behaviour when changing their
values. Finally, passivity is a relatively simple example of a causal relation, and the idea of
exploiting passivity in order to accelerate the filtering task is intuitive. Yet, to the best of
our knowledge, this has not been formalised and explored rigorously before.
The purpose of the present article is to formalise and evaluate the idea of automatically
exploiting causal structure for efficient belief filtering in DBNs, using passivity as a concrete
example of a causal relation. Specifically, our hypothesis is that in large processes with
high degrees of passivity, this structure can be exploited to accelerate the filtering task.
After discussing related work in Section 2 and technical preliminaries in Section 3, our
contributions can be grouped into the following parts:
 In Section 4, we give a formally concise definition of passivity and discuss various
aspects of this definition. Our definition assumes a decision process which is specified
as a set of dynamic Bayesian networks (one for each action). We also discuss a nonexample of passivity, by which we mean variables which appear to be passive but really
are not passive. Finally, we give a simple procedure which can detect passive variables
based on their conditional probability tables.
 In Section 5, we present the Passivity-based Selective Belief Filtering (PSBF) method.
Following the idea outlined above, PSBF uses a factored belief representation in which
the belief factors are defined over clusters of correlated state variables. PSBF follows
a 2-step update procedure wherein the belief state is first propagated through the
process dynamics (the transition step) and then conditioned on the observation (the
observation step). The interesting novelty of PSBF is the way in which it performs
the transition step: rather than updating all belief factors, PSBF updates only those
factors whose variables it suspects to have changed, which is possible by exploiting
passivity (to be made precise shortly). Similarly, in the observation step, PSBF updates
only those belief factors which it determines to be structurally connected with the
observation, and it uses only those parts of the observation which are relevant to the
belief factor, thus allowing for a more efficient incorporation of observations. PSBF
produces exact belief states under certain assumptions and approximate belief states
otherwise. We also discuss the computational complexity and error bounds of PSBF.
 In Section 6, we evaluate PSBF in two experimental domains: We first evaluate PSBF in
synthetic (i.e. randomly generated) processes of varying sizes and degrees of passivity.
The process sizes vary from one thousand to one trillion states, and the passivity
degrees vary from 25% to 100% passivity. Our results show that PSBF is faster than
several alternative methods while maintaining competitive accuracy. In particular, our
1138

fiExploiting Causality for Selective Belief Filtering in DBNs

results indicate that the computational gains grow significantly with both the degree of
passivity and the size of the process. We then evaluate PSBF in a complex simulation
of a multi-robot warehouse system in the style of Kiva (Wurman, DAndrea, & Mountz,
2008). We show how passivity occurs in this system and how PSBF can exploit this to
accelerate the filtering task, again outperforming alternative methods.
Finally, we discuss the strengths and weaknesses of PSBF in Section 7, and we conclude
our work in Section 8. All proofs can be found in the appendix.

2. Related Work
There exists a substantial body of work on belief filtering in partially observed stochastic
processes. In this section, we review filtering methods that utilise the special structure of
DBNs and situate our work within this and other related literature.
2.1 Approximate Belief Filtering in DBNs
Several authors proposed filtering methods wherein the belief state is represented as a set of
state samples. Specifically, the probability that the process is in state s is the normalised
frequency with which the state samples correspond to s. These methods are now commonly
referred to as particle filters (PF); see the work of Doucet, de Freitas, and Gordon (2001)
for a survey. In a common variant of PF (Gordon, Salmond, & Smith, 1993), the filtering
task consists of propagating the current state samples through the process dynamics and a
subsequent resampling step based on the probabilities with which the new state samples
would have produced the observation. Two interesting features of PF are that it can be
applied to processes with discrete and continuous variables, and that the approximation
error converges to zero as we increase the number of state samples.
A known problem of PF is the fact that the number of samples needed for acceptable
approximations can grow drastically with the variance in the process dynamics (as shown
in our experiments; cf. Section 6). Rao-Blackwellised PF (RBPF) (Doucet, De Freitas,
Murphy, & Russell, 2000) was developed to address this problem. RBPF assumes that the
state variables can be grouped into sets R and X such that the distribution over X can be
efficiently calculated from R during the filtering. Hence, a sample in RBPF consists of a
sample of R and a corresponding marginal distribution over X. RBPF is useful when the
variance in R is relatively low and the variance in X is high, since this reduces the number
of samples needed for acceptable approximations.
Boyen and Koller (1999, 1998) recognised that if a process consists of several independent
or weakly interacting subcomponents, then the belief state can be represented more efficiently
as a product of smaller beliefs about these individual subcomponents. Their seminal contribution is to show that the approximation error due to this factored representation is essentially
bounded by the degree of uncertainty (or mixing rates) in the process. More precisely, they
prove that the relative entropy (or KL divergence; Kullback & Leibler, 1951) between two belief states contracts at an exponential rate when propagated through a stochastic transition
process. Based on this observation, they propose a filtering method (BK) wherein the belief
state is represented in factored form and the belief factors are updated using an exact inference method, such as the junction tree algorithm (Lauritzen & Spiegelhalter, 1988). Since
1139

fiAlbrecht & Ramamoorthy

the internal cliques used in the junction tree algorithm may not correspond to the belief
state representation of BK, a final projection step will typically have to be performed in
which the original factorisation is restored. The performance of this method depends crucially on whether the relevant correlations between state variables can be captured in small
clusters, and whether the projection step can be performed efficiently.
Factored particle filtering (FP) (Ng, Peshkin, & Pfeffer, 2002) addresses the main drawbacks of PF (many samples needed) and BK (small clusters required) by approximating the
belief factors using a set of factored state samples. The samples are factored in the sense
that they only assign values to the variables in the corresponding factor. This allows FP to
represent belief factors which are too large for BK, and it reduces the number of samples
needed due to the smaller number of variables in each factor. The authors provide different methods of updating the factored state samples, but the generic idea is to first perform
a join operation in which full state samples are reconstructed from the factored samples,
which are then updated as in standard PF. The updated samples are then projected down
into factored form using a project operation. The main drawback of FP is that these join
and project operations essentially correspond to standard relational database operations,
which can be very expensive.
Murphy and Weiss (2001) propose a filtering method called factored frontier (FF). FF
uses a fully factored representation of belief states; that is, the belief state is a product of
marginals for each individual state variable. This allows for a very compact representation
of beliefs. The algorithm works by moving a set of state variables (the frontier) forward
and backward in the DBN topology. This requires a certain variable ordering, which can
be difficult to attain if intra-correlations between state variables (i.e. edges within the t + 1
slice of the DBN) are allowed. The authors show that their method is equivalent to a single
iteration of loopy belief propagation (LBP) (Pearl, 1988). Thus, similar to LBP, FF can be
applied in successive iterations to improve the approximation accuracy.
None of the works discussed above explicitly address the question of how causal relations
between state variables can be exploited to accelerate the filtering task, or, alternatively, how
the filtering methods proposed therein implicitly benefit from causal structure. Our method,
PSBF, is related to BK and FP in that PSBF, too, uses a factored belief representation,
where the belief factors are defined over clusters of correlated state variables. Therefore, the
analysis of approximation errors by Boyen and Koller (1998) also applies to PSBF, as we
show in Section 5 as well as in our experiments. However, in contrast to BK and FP, PSBF
does not perform inference over the complete factorisation, but rather over the individual
factors. As a consequence, PSBF does not require a join or project operation, which is one
of the main disadvantages of BK and FP.
2.2 Belief Filtering in Decision Processes
The methods discussed in the preceding subsection can be used for belief filtering in decision
processes, including POMDPs (Kaelbling et al., 1998; Sondik, 1971). In this regard, these
methods can be viewed as pure filters in that they are only concerned with belief filtering
and not with the control of the decision process. This is in contrast to combined filtering
methods, which interleave the filtering and control tasks in decision processes and make
specific assumptions regarding solutions thereof. There exists a large body of literature on such
1140

fiExploiting Causality for Selective Belief Filtering in DBNs

combined methods, including reachability-based methods (Hauskrecht, 2000; Washington,
1997), grid-based methods (Zhou & Hansen, 2001; Brafman, 1997; Lovejoy, 1991), pointbased methods (Smith & Simmons, 2005; Pineau, Gordon, & Thrun, 2003), and compression
methods (Roy, Gordon, & Thrun, 2005; Poupart & Boutilier, 2002).
A potential advantage of such combined methods is that they have access to additional
structure and may, therefore, utilise synergies between the filtering and control tasks. One
such synergy is the use of decision quality to guide belief filtering, rather than metrics such
as relative entropy. Poupart and Boutilier (2001, 2000) propose a filtering method, called
value-directed approximation, which chooses different approximation schemes for different
decisions so as to minimise the expected loss in decision quality (i.e. accumulated rewards).
The method assumes that the POMDP has been solved exactly and that the value function
is provided in the form of -vectors which represent the available actions in the POMDP.
Based on the value function, their algorithm computes a switching set and alternative
plans to determine the error bounds of approximation schemes. This is used to search for
an optimal approximation scheme in a tree-based manner, where the search traverses from
approximate to exact schemes.
While the idea of using decision quality to guide belief filtering is appealing, their method
involves a series of optimisation problems and an exhaustive tree search, which can be very
costly in complex systems. The advantage of pure filtering methods, including our proposed
method PSBF, is that they can filter processes which are too complex for combined methods,
such as the multi-robot warehouse system studied in Section 6. The actual control task can
then be done via domain-specific solutions (cf. Section 6.2.1).
2.3 Substructure in Parameterisation
Bayesian networks, and hence DBNs, allow for a compact parameterisation (i.e. specification
of probabilities) and efficient inference via conditional independence relations. In addition,
there has been considerable work in identifying substructure in the parameterisation to
further simplify knowledge acquisition and enhance inference (Koller & Friedman, 2009;
Boutilier, Dean, & Hanks, 1999). The property studied in this work, passivity, is one example
of substructure in the parameterisation. Other notable examples include causal independence (e.g. Heckerman & Breese, 1994; Heckerman, 1993) and context-specific independence
(Boutilier, Friedman, Goldszmidt, & Koller, 1996).
Causal independence is the assumption that the effects of individual causes on a common
variable (i.e. the parents of that variable) are independent of one another. This allows for
a compact parameterisation via operators such as noisy-or (Srinivas, 1993; Pearl, 1988),
and it can be used to enhance inference (Zhang & Poole, 1996). Note that passivity is a
conceptually much simpler property than causal independence, because passivity is neither
concerned with the strength of individual causes nor the extent to which they depend on each
other. Moreover, passivity can be read directly from the parameterisation (cf. Section 4.3)
whereas causal independence is usually imposed by the designer.
Context-specific independence (CSI) is a property which states that a variable is independent of some of its parents given a certain assignment of values (i.e. context) to some of
its other parents. Non-local CSI statements follow similarly to d-separation (Geiger, Verma,
& Pearl, 1989). This can allow for a further reduction of parameters (Boutilier et al., 1996)
1141

fiAlbrecht & Ramamoorthy

and enhancement of inference (Poole & Zhang, 2003). As we will discuss in Section 4, passivity can be viewed as a special kind of CSI applied to DBNs, in that the parents with respect
to which the variable is passive provide the context for CSI. However, in contrast to CSI,
passivity does not assume that the context is actually observed.

3. Technical Preliminaries
This section introduces the basic concepts and notation used in our work. We begin with
a brief discussion of decision processes to provide the context for our work, followed by a
discussion of dynamic Bayesian networks as the model over which we perform inference.
3.1 Decision Processes, Belief States, Exact Updates
We consider a stochastic decision process wherein, at each time t, the process is in state
st  S and a decision maker, or agent, is choosing an action at . After executing at in st , the
t
process transitions into state st+1  S with probability T a (st , st+1 ) and the agent receives an
t
observation ot+1  O with probability a (st+1 , ot+1 ). We assume factored representations of
the state space S and observation space O, such that S = X1  ...  Xn and O = Y1  ...  Ym ,
where the domains Xi , Yj are finite. The notation si is used to denote the value of Xi in
state s  S, and analogously for oj with o  O. Moreover, we assume that the process is
time-invariant, meaning that T a and a are independent of t. This framework is compatible
with many decision models used in the artificial intelligence literature, including POMDPs
(Kaelbling et al., 1998; Sondik, 1971) and its many variants.
The agent chooses action at based on its belief state bt (also known as information state),
which represents the agents beliefs about the likelihood of states at time t. Formally, a
belief state is a probability distribution over the state space S of the process. Belief filtering
is the task of calculating a belief state based on the history of observations. Ideally, the
resulting belief state should be exact in that it retains all relevant information from the past
observations (this is sometimes referred to as sufficient statistic; cf. Astrom, 1965). The
exact update rule is a simple procedure that produces exact belief states:
Definition 1 (Exact update rule). The exact update rule is defined as follows: After taking
action at and observing ot+1 , the belief state bt is updated to bt+1 via
bt+1 (s0 ) =

X

t

bt (s) T a (s, s0 )

(1)

sS
t

bt+1 (s0 ) =  bt+1 (s0 ) a (s0 , ot+1 )

(2)

where  is a normalisation constant.
We sometimes refer to the step bt  bt+1 as the transition step and to the step bt+1  bt+1
as the observation step. Unfortunately, the space complexity of storing exact belief states
and the time complexity of updating them using the exact update rule are both exponential
in the number of state variables, making it infeasible for complex systems with large state
spaces. Hence, more efficient approximate methods are required.
1142

fiExploiting Causality for Selective Belief Filtering in DBNs

3.2 Dynamic Bayesian Networks
A dynamic Bayesian network (DBN) (Dean & Kanazawa, 1989) is a Bayesian network with
a special temporal semantics that specifies how a stochastic process transitions from one
state into another. DBNs can be used to model the effects of actions in a stochastic decision
process. Specifically, they are a compact representation of the transition function T a and
observation function Oa of action a:
Definition 2 (DBN). A dynamic Bayesian network for action a, denoted a , is an acyclic
directed graph consisting of:

	

	
t+1 with xt , xt+1  X ,
 State variables X t = xt1 , ..., xtn and X t+1 = xt+1
i
i i
1 , ..., xn
representing the states of the process at time t and t + 1, respectively.

	
t+1 with y t+1  Y , representing the obser Observation variables Y t+1 = y1t+1 , ..., ym
j
j
vation received at time t + 1.




 Directed edges Ea  X t  X t+1  X t+1  X t+1  X t+1  Y t+1  Y t+1  Y t+1 ,
specifying the network topology and dependencies between variables.
 Conditional probability distributions Pa (z | paa (z)) for each variable z  X t+1  Y t+1 ,
specifying the probability that z assumes a certain value given a specific assignment
to its parents paa (z) = {z 0 | (z 0 , z)  Ea }. For convenience, we also define pata (Z) =
t+1  pa (Z), where pa (Z) = 
X t  paa (Z) and pat+1
a
a
zZ paa (z).
a (Z) = X
The edges Ea and distributions Pa define the functions T a and a as
a

0

T (s, s ) =

n
Y

0
Pa xt+1
= s0i | paa (xt+1
i
i ) - (s, s )



(3)

i=1

a (s0 , o) =

m
Y



Pa yjt+1 = oj | paa (yjt+1 ) - (s0 , o)

(4)

j=1
t+1
0
where we use the notation paa (xt+1
in
i ) - (s, s ) to specify that the parents of xi
t+1
0
and X , respectively, assume their corresponding values from s and s . Formally, if
t+1
t+1
t
xtl  pata (xit+1 ) and xt+1
 pat+1
= s0l0 . Similarly, we use
a (xi ), then xl = sl and xl0
l0
t+1
0
the notation paa (yj ) - (s , o) to specify that the parents of yjt+1 in X t+1 and Y t+1 ,
respectively, assume corresponding values from s0 and o.

Xt

Example 2 (DBN representation of robot arm). We can represent the robot arm from Example 1 as a set of DBNs, where we have one DBN a for each action
 {CW
i , CCWi }. The
	 at+1
 t+1
	
t
t
t
t
state and observation
variables
= 1 , 2t+1 , 3t+1 ,
n
o in the DBNs are X = 1 , 2 , 3 , X

and Y t+1 = 1t+1 , 2t+1 , 3t+1 . To make our example more realistic, let us assume that the
joint orientations are bounded relative to the orientation of the immediately preceding joint
(e.g. in the form of a cone), where the first joint is bounded relative to the ground. This
means that the joint movement depends on its own as well as the preceding joint orientation, as shown in Figure 3. Moreover, the joint orientations are correlated (i.e. edges within
1143

fiAlbrecht & Ramamoorthy

1t

1t+1

1t+1

2t

2t+1

2t+1

3t

3t+1

3t+1

Xt

X t+1

Y t+1

Figure 3: DBN representation of robot arm.
X t+1 ) such that no joint can exceed the bound given by the preceding joint. Finally, the observation variables depend solely on the corresponding joint variable. The actions in this
example would differ in their variable distributions Pa .

3.3 Additional Definitions
It will be useful to define the following:
 xt+1
for
 The binary order  is defined over X t  X t+1 such that xti  xtj and xt+1
i
j
t+1
t
all 1  i < j  n, and xi  xj for all 1  i, j  n.
 Given a set Z  X t  X t+1 , we write Z  to denote the tuple that contains all variables
of Z, ordered by .
 Given the ordered tuple Z  = (zi1 , ..., zi|Z| ), we define the set S(Z) = Xi1  ....  Xi|Z|
to contain all value tuples for the variables in Z.
 Given a value tuple sZ = (si1 , ..., si|Z| )  S(Z), we use the notation Z - sZ as an
abbreviation for zil = sil for each zil  Z  (i.e. the variables in Z assume their
corresponding values from sZ ).

4. Passivity
This section introduces a formal definition of passivity, which will then be used as the basis
for the remainder of this article. We also provide a simple procedure to detect passive
variables from the process dynamics.
4.1 Formal Definition
As outlined in Section 1, a state variable xt+1
is called passive in action a if there exists a
i
t (in the DBN a ) such that xt+1 may change its value only
subset of xt+1
s
parents
in
X
i
i
1144

fiExploiting Causality for Selective Belief Filtering in DBNs

if at least one of the variables in this subset changed its value. Conversely, xt+1
does not
i
change if the variables in the subset did not change. Formally, we define passivity as follows:
t+1
a
Definition 3 (Passivity). Let action a be given by a DBN
 t
	 . A state variable xi is
t+1
a
t
called passive in  if there exists
a set a,i  paa (xi ) \ xi such that:

t+1
(i) xtj  a,i : xt+1
 Ea
j , xi
and
(ii) for any two states st and st+1 with T a (st , st+1 ) > 0 :


 sti = st+1
xtj  a,i : stj = st+1
j
i

(5)

A state variable which is not passive is called active.
The set a,i corresponds to the subset of variables described above: it contains all those
variables which directly affect xit+1 (i.e. they are parents of xt+1
in X t ) such that xt+1
may
i
i
change its value only if any of the variables in a,i changed its value. We will sometimes say
that a variable xt+1
is passive in a with respect to another variable xtj if it is the case that
i
xtj  a,i . Furthermore, we will omit in a  if it is obvious from context.
Clause (i) in Definition 3 requires that xt+1
is intra-correlated with the variables in a,i ;
i
specifically, that there is an edge from xt+1
to
xt+1
for all xtj  a,i . As an example, see
j
i
Figure 1 in which we assumed that the variable xt+1
was passive with respect to the variable
2
xt1 . (We will discuss the purpose of this clause in the next subsection.) Clause (ii) defines
the core semantics of passivity by requiring that xt+1
remains unchanged if all variables in
i
a,i remain unchanged. Note that this means that the distribution Pa for xt+1
may specify
i
any deterministic or stochastic behaviour if the variables in a,i change their values. This
includes that xt+1
may not change its value at all.
i
A state variable xit+1 can be passive even if it has no parents in X t , or none other than
xti . In this case, the set a,i would be empty and clause (i) as well as the premise in (5)
would trivially hold true. However, such a variable can only be passive if it does not change
its value under any circumstances. In other words, it would have to be a constant. In that
case, one should consider removing the variable from the state description in order to reduce
computational costs.
As noted in Section 2.3, passivity can be shown to be a special kind of context-specific
independence (CSI) (Boutilier et al., 1996) applied to DBNs. Here, the associated set a,i of
a passive variable xt+1
provides the context: given any assignment of values to xtj  a,i (i.e.
i
t+1
context) such that xtj = xt+1
is independent of all xtk , xt+1
with xtk  pata (xt+1
j , xi
i ) \ a,i
k
and k 6= i. However, besides this similarity, there is an important difference between passivity
and CSI, which is that passivity does not actually assume that the context is observed. Thus,
passivity can be viewed as a kind of CSI for unobserved contexts. This will become clear in
Section 5, when we describe a filtering method that exploits passivity.
4.2 Non-Example of Passivity
What is the purpose of clause (i) in the definition of passivity? After all, and as discussed
previously, clause (ii) captures the core idea of passivity, which is that a variable may only
change its value if any of the variables with respect to which it is passive changed its value.
1145

fiAlbrecht & Ramamoorthy

xt1

xt+1
1

xt2

xt+1
2

Figure 4: Example of a process for which clause (ii) is insufficient.
However, while it may seem intuitive that clause (ii) be sufficient for passivity, there are
in fact processes in which clause (ii) alone does not suffice. In other words, clause (ii) is
necessary but not sufficient for passivity. We illustrate this in the following example:
Example 3 (Non-example of passivity). Consider a process with two binary state variables,
x1 , x2 , and a single action, a, shown in Figure 4. (We omit the observation variables for
clarity.) The dynamics of the process are such that xt+1
takes the value of xt2 and xt+1
takes
1
2
the value of xt1 (i.e. x1 and x2 swap their values at each time step). In this process, both
state variables satisfy clause (ii) of Definition 3: If we set x01 = x02 (i.e. same initial values),
then T a (st , st+1 ) is positive only for states st = st+1 , and hence (5) is true. If we set x01 6= x02 ,
then T a (st , st+1 ) is positive only for states st , st+1 with sti 6= st+1
i , i  {1, 2}, and hence (5)
is trivially true since its premise is false.

Despite satisfying clause (ii), the state variables xt+1
and xt+1
from Example 3 are in
1
2
fact not passive, for the following two reasons: Firstly, passivity is a causal relation and as
such it must imply a causal order (Pearl, 2000). However, there is no causal order between
x1 and x2 , because there is no edge between xt+1
and xt+1
1
2 . Secondly, passivity means that
a variable may change its value only if another variable with respect to which it is passive (a
variable in a,i ) changed its value. In other words, whether or not a passive variable xt+1
i
may change its value depends on both the past values of a,i (at time t) and the new values
of a,i (at time t + 1). However, the variables in Example 3 only depend on the values at
time t, hence their own values at time t + 1 are predetermined and do not depend on whether
the variables in a,i change values.
The first issue, namely that of the causal order, can be addressed by adding the corresponding edges in X t+1 . For instance, in Example 3 we could add an edge from xt+1
to xt+1
1
2
to establish a causal order. However, this does not generally solve the second issue, which
is that every passive variable xit+1 must depend on both past and new values of the variables in a,i . In other words, xit+1 must be both inter-correlated as well as intra-correlated
with the variables in a,i . The former is given by definition (since every variable in a,i is
a parent of xt+1
i ) and the latter is precisely what is required by clause (i) in Definition 3.
Therefore, clauses (i) and (ii) together define the formal meaning of passivity.
4.3 Detecting Passive Variables
As mentioned in Section 1, passivity is a latent causal property in the sense that it can be
extracted from the process dynamics without additional information, and with no additional
assumptions regarding the representation of variable distributions. In order to determine if a
1146

fiExploiting Causality for Selective Belief Filtering in DBNs

a
Algorithm 1 Passive(xt+1
i , )

1:

a
Input: state variable xt+1
i , DBN 

Output: a,i if xt+1
is passive in a , else false
i
 	
t+1
3: Q  OrderedQueue P pata (xi ) \ xti
// in ascending order of |a,i |
2:

4:

while Q 6=  do

5:

a,i  NextElement(Q)

6:

Q  Q \ {a,i }

7:

for all xtj  a,i do


t+1
if xt+1
6 Ea then
j , xi

8:
9:
10:
11:
12:
13:
14:
15:
16:

Go to line 4 // clause (i) violated
 	
a,i  paa (xt+1
) \ a,i  xti
i
n
o
t+1
t 
t+1

x
|
x
a,i
j
a,i
j
for all s  S(a,i ), s  S(a,i ), si  Xi do


t+1
t =s , 
if Pa xt+1
=
s
|
x
s
,

s
,

s
i
i
a,i
a,i


 < 1 then
i
i
a,i
Go to line 4 // clause (ii) violated
return a,i
return false

variable xit+1 is passive in a , one has to find a set a,i such that both clauses of Definition 3
are satisfied. A simple procedure which does this for any representation of the variable
distributions is given in Algorithm 1. The algorithm takes as inputs a variable xt+1
and a
i
t+1
a
a
DBN  , and checks whether xi is passive in  by searching for a set a,i which satisfies
both clauses of Definition 3. Note that the power set P in line 3 includes the empty set ,
hence it also accounts for a,i = . Lines 7 to 9 check if clause (i) is satisfied while lines
10 to 14 check if clause (ii) is satisfied. Line 13 essentially checks if (5) holds true. If both
clauses are satisfied, then xt+1
is passive in a with respect to the variables in a,i , and the
i
algorithm returns the set a,i . Otherwise, the algorithm returns a logical false.2
The time complexity of Algorithm 1 is exponential in the worst case, in which xt+1
is
i
not passive. Specifically, the time requirements of line 4 grow exponentially with the number
of parents of xt+1
in X t , and the time requirements of line 12 grow exponentially with the
i
cardinality of a,i and a,i . However, these time requirements can be reduced significantly
when committing to specific representations for the variable distributions Pa . For example,
if the distributions are represented in tabular form, then one can utilise arrays of indices
to perform sweeping tests of (5), i.e. line 13. Moreover, it is important to realise that the
algorithm needs to be performed only once for each state variable, prior to the start of the
2. Strictly speaking, Algorithm 1 checks for a property which is stronger than passivity because it does not
check for T a (st , st+1 ) > 0 (cf. clause (ii)) in line 12. However, the algorithm can be modified to include
such a check. We omit this in our exposition in order to highlight the core ideas behind the algorithm.

1147

fiAlbrecht & Ramamoorthy

process or on demand. This is since passivity is invariant of the process states. In other
words, if a variable is passive in a , then it will always be passive in a . Therefore, it suffices
to check once in advance for passivity.
Note that the set a,i is not necessarily unique. For example, consider
a	variable xt+1
1

which is passive in a with respect to variables xt2 and xt3 , i.e. a,1 = xt2 , xt3 , and assume
that xt+1
changes if and only
if	 x3t+1 changes(i.e.
2

	 they change at the same time). Then, it is
0
t
00
t
easy to verify that a,1 = x2 and a,1 = x3 also satisfy clauses (i) and (ii), and hence
a,1 , 0a,1 , 00a,1 are all valid sets under our definition of passivity. The guiding principle in
such cases is Occams razor, which, intuitively speaking, states that the simplest explanation
suffices. In our case, this means that it suffices to use the smallest set a,i in terms of the
cardinality |a,i |. (Hence, line 3 in Algorithm 1 sorts the queue Q in ascending order of
|a,i |.) The rationale is that if there exist multiple causal explanations for a passive variable
xt+1
i , then the one involving the fewest key variables is to be favoured since it reduces
(compared to the alternative explanations) the number of cases in which we would have to
revise our beliefs about xit+1 . In our earlier example, if we accept a,1 as a causal explanation
t+1
for xt+1
every time xt+1
or xt+1
may have
1 , then we would have to revise our beliefs for x1
2
3
0
changed their values. However, if we accept a,1 as a causal explanation, then we would
have to revise our belief for x1t+1 only if xt+1
may have changed its value. This difference
2
will become more obvious in Section 5.2, which explains how passivity can be exploited to
reduce computational costs.

5. Passivity-based Selective Belief Filtering
This section presents the Passivity-based Selective Belief Filtering (PSBF) method, which
exploits passivity for efficient filtering. As discussed in Section 3, we assume that the process
is specified as a set of dynamic Bayesian networks which contains one DBN a for each
action a  A. Therefore, whenever we refer to an action a (e.g. T a , a , Pa , paa ), this is
assumed to be in the context of a .
PSBF follows the general two-step update procedure in which the belief state is first
propagated through the process dynamics (transition step) and then conditioned on the
observation (observation step). Thus, it is natural to divide the exposition of PSBF into
three parts: (1) the belief state representation, (2) the transition step, and (3) the observation
step. These are discussed in Sections 5.1, 5.2, and 5.3, respectively. A summary of PSBF
is given in Section 5.4. We also discuss the computational complexity and error bounds of
PSBF in Sections 5.5 and 5.6, respectively.
5.1 Belief State Representation
Recall from Section 1 that the principal idea behind PSBF is to maintain separate beliefs
about individual aspects of the process, and to exploit passivity in order to perform selective
updates over these separate beliefs. The union of all individual aspects constitutes a complete
state description of the process. Therefore, the belief state can be represented as the product
of all separate beliefs about the individual aspects.
We capture the informal notion of individual aspects formally in the form of clusters,
which are defined as follows:
1148

fiExploiting Causality for Selective Belief Filtering in DBNs

C1
1t

1t+1

C1

C1
1t

1t+1

1t+1

1t+1

1t

1t+1

2t+1

2t

2t+1

2t+1

2t

2t+1

2t+1

3t+1

3t

3t+1

3t+1

3t

3t+1

3t+1

1t+1

C2
2t

2t+1
C3

3t

3t+1

C2
(a) C1 , C2 , C3

(b) C1

(c) C1 , C2

Figure 5: Three clusterings for the robot arm DBN.
Definition 4 (Cluster). A clustering of X t+1 is a set C = {C1 , ..., CK } which satisfies
k : Ck  X t+1 and C1  ...  CK = X t+1 . We refer to the elements Ck  C as clusters.
The underlying idea behind the concept of clusters is that the variables in a cluster Ck
are connected in some important sense. Specifically, if two or more variables are in a common
cluster, then there exists some relation between these variables regarding the likelihood of
values which they may assume. In other words, the variables are correlated in X t+1 .
The number K and the concrete choice of clusters Ck can be specified by the user or
generated automatically. For example, they may be specified manually by a domain expert
who is familiar with the structure of the modelled system, or generated automatically using
methods such as the ones described in Section 6.1. It should be stressed, however, that in
order to reduce computational costs, it is advisable to follow the general rule as small
as possible, as large as necessary when choosing clusters (see Section 5.5 for a discussion
about computational complexity). Therefore, if two variables are strongly correlated, then
they should presumably be in a common cluster, whereas if they are not or only weakly
correlated (weakly meaning that the correlation can be ignored safely), then they should
be in separate clusters in order to reduce computational costs. This is illustrated in the
following example:
Example 4 (Clusters in robot DBN). Recall the robot arm DBN from Example 2, specifit+1 is given by the three clusters
cally Figure
to	cluster the
state
 t+1 	3. One way
 t+1
 t+1
	 variables in X
C1 = 1
, C2 = 2
, C3 =  3
, as shown in Figure 5a. This clustering is most
efficient since it minimises the size of each cluster. However, the clusters fail to capture
the important correlation that the joint orientation i is restricted by the preceding joint
orientation
i1 . Another

	 way to cluster the state variables is given by the single cluster
C1 = 1t+1 , 2t+1 , 3t+1 , as shown in Figure 5b. This clustering captures all correlations
between variables. However, this is the largest possible cluster
the
 and, therefore,
	
 least effi	
cient one. A compromise is given by the two clusters C1 = 1t+1 , 2t+1 , C2 = 2t+1 , 3t+1 ,
which are shown in Figure 5c. This clustering captures the correlation of the joint orientations with the immediately preceding joint orientations, and it is more efficient than the
previous clustering since it has smaller clusters.

1149

fiAlbrecht & Ramamoorthy

Given the definition of clusters, we capture the informal notion of separate beliefs in
the form of belief factors:
Definition 5 (Belief factor). Given a cluster Ck , the corresponding belief factor bk is a
probability distribution over the set S(Ck ).
Intuitively, a belief factor bk represents the agents beliefs as to the likelihood of values
for the variables in the corresponding cluster Ck . An analogy to this is to view a belief factor
as a smaller belief state, and to view b as the full belief state which is a combination of
the smaller belief states. However, to distinguish the two, we refer to b simply as the belief
state and to bk as a belief factor.
Finally, given the clusters Ck and their corresponding belief factors bk , the belief state b
is represented in factored form as
b(s) =

K
Y

bk (sk )

k=1


	
where we use the notation sk to refer to the tuple (si )xt+1 Ck . (E.g., if Ck = xt+1
, xt+1
2
3
i
and s = (s1 , s2 , s3 , s4 ), then sk = (s2 , s3 ).)
5.2 Exploiting Passivity in the Transition Step
In order to perform selective updates over the belief factors bk , we require a procedure which
performs the transition step independently for each factor.3 We obtain such a procedure by
introducing two assumptions which allow us to modify the transition step (1) of the exact
update rule. The assumptions guarantee that the transition step is performed exactly, in the
sense of (1). However, as we will discuss shortly, the assumptions can be violated to obtain
approximate belief states.
The first assumption, (A1), states that the clusters must be uncorrelated (i.e. there are
no edges in X t+1 between clusters), and the second assumption, (A2), states that the clusters
must be disjoint. Formally, these are defined as follows:
t+1
(A1) a : xt+1
 Ck  pat+1
a (xi )  Ck
i

(A2) k 6= k 0 : Ck  Ck0 = 
Note that neither assumption implies the other. That is, it may be the case that (A1)
is satisfied while (A2) is violated, and vice versa. Assuming both (A1) and (A2), we can
reformulate (1) to
X
Y
t
0
bt+1
Tka (s, s0k )
btk0 (sk0 )
(6)
k (sk ) = 1
s  S(pat t (Ck ))
a

k0 :[xt+1
Ck0 : xti  pat t (Ck )]
i
a

where 1 is a normalisation constant and
Y

0
Tka (s, s0k ) =
Pa xt+1
= (s0k )i | paa (xt+1
i
i ) - (s, sk ) .
xt+1
 Ck
i

3. This also has the advantage that the belief factors can be updated in parallel, which is a useful feature
considering that many platforms use parallel processing techniques.

1150

fiExploiting Causality for Selective Belief Filtering in DBNs

This procedure performs the transition step independently for each belief factor bk , hence
they can be updated in any order and in parallel.
Assumption (A1) is what allows us to bring (1) into a form which updates the belief
factors bk independently of each other. Specifically, (A1) allows us to define the cluster-based
transition function Tka , which in turn enables the summation in (6). Assumption (A2), on
the other hand, guarantees that the product in (6) is correct. In particular, it may be the
case that |sk0 | < |Ck0 | (i.e. there are fewer elements in sk0 than in Ck0 ) if there are variables
in Ck0 which are not in patat (Ck ) (i.e. xt+1
 Ck0 but xti 
/ patat (Ck )). In such cases, btk0 is
i
t+1
taken to be the marginal distribution over variables xi  Ck0 with xti  patat (Ck ), where
(A2) guarantees that the marginalisation introduces no errors.
As mentioned previously, each assumption may be violated to obtain approximate belief
states. However, there is an important distinction between (A1) and (A2) in this regard:
If (A2) is violated, then (6) is still well-defined in the sense that it can still be executed,
except that the product in (6) may degrade the accuracy of the results. This is in contrast to
(A1), which is a structural requirement of Tka in the sense that Tka is ill-defined without (A1).
This is since, if (A1) is violated, the variables in Ck may have parents in X t+1 which are
0
not in Ck , in which case paa (xt+1
i ) - (s, sk ) would be ill-defined. Thus, if (A1) is violated,
we have to enforce it by modifying the distributions Pa of all xt+1
 Ck to marginalise out
i
t+1
all variables in pat+1
(x
)
which
are
not
in
C
,
for
all
clusters
C
. This means that each
t
k
k
i
a
variable has a separate distribution for every cluster which contains the variable, thereby
possibly introducing an approximation error.
Given the modified transition step (6), we can exploit passivity to perform selective
updates over the belief factors bk . Recall from Section 4.1 that a variable xt+1
is passive
i
t+1
a
in  if there exists a set a,i of variables such that xi may change its value only if any
of the variables in a,i changed its value. This causal connection can be used to decide
whether or not the values of the variables in a cluster Ck may have changed, in which case the
corresponding belief factor bk should be updated. Theorem 1 provides the formal foundation:
t

Theorem 1. If (A1) and (A2) hold, and if all xt+1
 Ck are passive in a , then
i
t
s  S : bt+1
k (sk ) = bk (sk ).

Proof. Proof in Appendix A.
Theorem 1 states that if the clusters C1 , ..., CK are disjoint and uncorrelated, and if all
t
variables in cluster Ck are passive in a , then the transition step for the corresponding
belief factor btk  bt+1
can be omitted without loss of information.
k
How does Theorem 1 translate into situations in which (A1) or (A2), or both, are violated?
The key assumption is again (A1), which states that the clusters must be uncorrelated. As
discussed earlier, we can enforce this by modifying the variable distributions Pa in each cluster.
However, if a passive variable xit+1  Ck is correlated with a (passive or active) variable
t+1
t+1
xt+1
 Ck0 , where xt+1
 pat+1
in the distribution Pa of
a (xi ), then marginalising out xj
j
j
t+1
t+1
xi will typically cause xi to lose its passivity, in the sense that it would no longer satisfy
the clauses in Definition 3. Consequently, we would always have to perform the transition
step for Ck , even if the unmodified variables in Ck are all passive. This is problematic not
only because of the unnecessary computations, but also because the modified distributions
will introduce an error every time the transition step is performed.
1151

fiAlbrecht & Ramamoorthy

1t

1t+1

1t+1
C1

2t

2t+1

2t+1
C2

3t

3t+1

3t+1

Xt

X t+1

Y t+1

Figure 6: Robot arm DBN implementing the action CW3 . Dashed circles mark passive state
variables. The coloured ellipses represent the clusters C1 and C2 .
To alleviate this effect, one can check if there is a chance that the unmodified variables
in the cluster would change their values. It can be shown that this is the case whenever there
is a causal path from any active variable to a variable in the cluster:
Definition 6 (Causal path). A causal path in a , from an active variable xt+1
to another
i
t+1
t+1 (Q)
t+1
(1)
(2)
(Q)
(1)
variable xj , is a sequence hx , x , ..., x i such that x = xi , x
= xj , and for
all for all 1  q < Q :
(i) x(q)  X t+1 
(ii) x(q) , x(q+1)  Ea
(iii) x(q+1) is passive in a with respect to x(q)
Intuitively, a causal path defines a chain of causal effects (such as between joints 1 and 3
in Example 1): since the active variable x(1) may have changed its value and x(2) is passive
with respect to x(1) , x(2) may also have changed its value; since x(2) may have changed
its value and x(3) is passive with respect to x(2) , x(3) may also have changed its value, etc.
Hence, in the absence of observing these changes, the mere existence of a causal path from
x(1) to x(Q) is reason to revise our beliefs about x(Q) . Therefore, as a general update rule, we
can omit the transition step btk  bt+1
if all unmodified variables in cluster Ck are passive
k
t
t
in a , and if there is no causal path from any active variable in a to any variable in Ck .
This is demonstrated in the following example:
Example 5 (PSBF update rule in robot arm DBN). Let us again consider the robot arm
from the previous examples. Figure 6 shows a DBN which implements the action CW3 .
This action rotates joint 3 of the robot arm by 1 clock-wise (i.e. the joint orientation 3t+1
is a direct target of the action). Therefore, the variable 3t+1 is active while the variables
1t+1 and 2t+1 are passive (shown as dashed 	circles). 
	
We use the clustering C1 = 1t+1 , 2t+1 , C2 = 2t+1 , 3t+1 for reasons given in Example 4. Since 1t+1 is a parent of 2t+1 , PSBF will have to enforce assumption (A1) by
1152

fiExploiting Causality for Selective Belief Filtering in DBNs

Algorithm 2 SkippableClusters(C, a )
1:

Input: clustering C = {C1 , ..., CK }, DBN a

2:

Output: set of clusters C  C which can be skipped in transition step

3:

C  C

4:

Q  OrderedQueue(X t+1 )

5:

while C 6=   Q 6=  do

6:
7:
8:
9:
10:
11:
12:
13:
14:

xt+1
 NextElement(Q)
i

	
Q  Q \ xt+1
i
a
if Passive(xt+1
i ,  ) then

	
C  C \ Ck  C | xt+1
 Ck
i

for all xt+1
 Q do
j
t+1
a
if CausalPath(xt+1
i , xj ,  ) then
n
o
C  C \ Ck  C | xt+1
 Ck
j
n
o
Q  Q \ xt+1
j

return C

marginalising 1t+1 out of the variable distribution Pa of 2t+1 in cluster C2 . While the modified variable distribution loses the passivity property (both clauses of Definition 3 are
violated), the unmodified distribution of 1t+1 is still passive.
When performing the transition step, PSBF has to update the belief factor b2 because
the corresponding cluster C2 contains the active variable 3t+1 . However, since all variables
in cluster C1 are passive (there are no modified variables in C1 ), and since there is no causal
path from 3t+1 to any variable in C1 , PSBF can omit the update for the belief factor b1 .
Intuitively, this makes sense since a change in the orientation of joint 3 cannot cause a
change in the orientations of the preceding joints. Note that this corresponds to a saving of
50% in the transition step.


Algorithm 2 defines a procedure which utilises this rule to find clusters for which the
transition step can be skipped. The algorithm takes as inputs a clustering C and a DBN
a , and returns a set C of skippable clusters. It essentially searches through all active
variables xt+1
in a and removes all clusters Ck from C which contain variables to which
i
there is a causal path from xit+1 . The function OrderedQueue(X t+1 ) returns an ordered
queue Q with all variables in X t+1 . The performance of Algorithm 2 depends on the order
of the queue. In our experiments, we obtained good performance by ordering the variables in
descending order of their number of outgoing edges. The function NextElement(Q) returns
a
the next element in the queue; the function Passive(xt+1
i ,  ) is defined in Algorithm 1;
t+1 t+1
a
and the function CausalPath(xi , xj ,  ) returns a logical true if and only if there is a
1153

fiAlbrecht & Ramamoorthy

causal path from xt+1
to xjt+1 in a .4 Note that, given the invariance of passivity to process
i
states (cf. Section 4.1), it suffices to call Algorithm 2 only once (in advance or as needed) to
determine which of the clusters to omit in the transition step.
5.3 Efficient Incorporation of Observations
PSBF can perform the observation step similarly to the exact update rule (2), which
conditions the propagated belief state bt+1 on the observation ot+1 to obtain a fully updated
belief state bt+1 . However, given the factored belief state representation used by PSBF, we
require a procedure which respects this factorisation in the observation step. Assuming that
(A1) and (A2) both hold, we can bring (2) into a form which updates the belief factors bk
independently of each other
X
Y
t
t+1 0
0
0
bt+1
a (s, ot+1 )
bt+1
(7)
k (sk ) = 2 bk (sk )
k0 (sk )
t+1 )) : s = s0 k 0 6= k : C 0  pat+1 (Y t+1 ) 6= 
s  S(pat+1
k
k
t (Y
t
k
a

a

where 2 is a normalisation constant. Note that, analogously to (6), if there are variables
t+1 ), then bt+1 is taken to be the marginal distribution
in Ck0 which are not in pat+1
k0
at (Y
t+1
t+1
over Ck0  paat (Y
). Assumption (A2) guarantees that the marginalisation introduces no
errors. If (A1) and (A2) both hold, then the transition step (6) and observation step (7)
produce exact belief states in the sense of (1) and (2), regardless of how many clusters were
skipped in the transition step (cf. Theorem 1).
The observation step (7) updates all belief states and uses all observation variables in
the process. In other words, it ignores the internal structure of the observation variables.
However, it is clear that if the variables in a cluster Ck are marginally independent of the
observation variables Y t+1 (this can be determined using d-separation (Geiger et al., 1989),
or simply by checking if there is a directed path from Ck to Y t+1 ), then there is no need
to perform the observation step for the corresponding belief factor bk . This is expressed
formally in Theorem 2:
t

Theorem 2. If all xt+1
 Ck are marginally independent of all yjt+1  Y t+1 in a , then
i
t+1
s  S : bt+1
k (sk ) = bk (sk ).

Proof. Proof in Appendix B.
Theorem 2 states that if the variables in Ck are independent of those in Y t+1 , then the
observation step for bk can be skipped. However, even if Ck is not independent of Y t+1 , it may
be the case that the variables in Ck depend only on a subset Yk  Y t+1 of the observation
variables. Clearly, in such cases, it suffices to use Yk rather than Y t+1 in the observation
step. To account for this, we first note that the variables in Y t+1 may be correlated with
each other. To preserve the correlations, we subdivide Y t+1 into clusters Cl  Y t+1 and
introduce the following assumptions:


(A3) a : yjt+1  Cl  paa (yjt+1 )  Y t+1  Cl
(A4) l 6= l0 : Cl  Cl0 = 
4. A simple way to implement this function is to modify a standard graph search method (such as breath-first
search) to check for (iii) in Definition 6, and to apply it to the variables in X t+1 with edges Ea from a .

1154

fiExploiting Causality for Selective Belief Filtering in DBNs

Assumptions (A3) and (A4) are analogous to (A1) and (A2), respectively, and essentially
serve the same purposes for the observation step. To distinguish the clusters Ck and Cl ,
we sometimes refer to the former as state cluster and to the latter as observation cluster.
Assuming that (A3) and (A4) both hold, we can redefine the observation step to
X
Y
Y
t+1 0
0
at
t+1
0
bt+1
(s
)
=

b
(s
)

(s,
o
)
bt+1
(8)
2
k
k
l
k
k
k0 (sk )
t+1
0
0
l: Cl Yk 6= s  S(pat+1
t (Cl )) : sk = sk k 6= k : Ck0  pa t (Cl ) 6= 
a

where
al (s, ot+1
l ) =

Y

a



t+1
t+1
Pa yjt+1 =(ot+1
)
|
pa
(y
)
(s,
o
)
j
a j
l
l

yjt+1  Cl

and Yk  Y t+1 is the set of observation variables which are not marginally independent of
the variables in Ck .
Given Theorem 2, one can see that (8) is equivalent to (7) if the observation variables
are not clustered (or, equivalently, there is a single observation cluster Cl = Y t+1 ). However,
it is important to note that if the observation variables are clustered (i.e. there are multiple
observation clusters Cl ), then (8) is notQnecessarily
equivalent to
P
P(7).
QmTo see this, it is helpful
to compare the abstract formulations m

(o
)
b
and
j=1
s s j s
s
j=1 s (oj ) bs , where the
former corresponds to (8) and the latter to (7). Therein, (o1 , ..., om )  O is an observation, bs
is the probability of being in state s  S, and s (oj ) is the probability of observing yj = oj
in s. These abstract formulations are equivalent for m = 1 or if bs = 1 for some s, but in all
other cases they may not be equivalent. Nonetheless, if we fix the number of observation
variables m, then (8) approximates (7) closely as we increase the number of state variables
n. Our experiments indicate that it often suffices to use just a few more state variables than
observation variables in order to obtain good approximations.
Finally, to show that it suffices to perform the observation step for bk using only those
clusters Cl whose variables are not independent of the variables in Ck , we observe that (8)
is in fact a repeated application of (7) for every Cl , where the updated belief factor bt+1
is
k
t+1
used in place of bk in the subsequent application. Since every application has the same
form as (7) (with Y t+1 = Cl ), we conclude that Theorem 2 holds, and hence the observation
step can be skipped for clusters Cl which are independent of Ck .
5.4 Summary of PSBF
The preceding sections can be summarised as follows:
 Representation: The belief state bt is represented as a product of K belief factors btk ,
Q
t
t
such that bt (s) = K
k=1 bk (s). Each belief factor bk is a probability distribution over
the set S(Ck ), where Ck  X t+1 is a cluster of correlated state variables.
 Transition step: The transition step btk  bt+1
is performed using (6), for all clusters
k
t
a
Ck which include active variables in  , or to which there is a causal path from an
t
active variable in a . All other clusters are skipped.
 Observation step: The observation step bt+1
 bt+1
is performed using (8), for all
k
k
clusters Ck which are dependent on the observation variables Y t+1 , using only those
observation clusters Cl which are relevant for Ck . All other clusters are skipped.
1155

fiAlbrecht & Ramamoorthy

Algorithm 3 PSBF(at , ot+1 , (btk )Ck C | C, C, (a )aA )
1:

Input: action at , observation ot+1 , belief factors (btk )Ck C

2:

Parameters: state clustering C, observation clustering C, DBNs (a )aA

3:

Output: updated belief factors (bt+1
k )Ck C

4:

// Transition step

5:

C  SkippableClusters(C, a )

6:

for all Ck  C do

7:
8:
9:
10:
11:

t

if Ck  C then
bt+1
 btk
k
else
for all s0k  S(Ck ) do
X t
Y
0
bt+1
Tka (s, s0k )
btk0 (sk0 )
k (sk )  1
s  S(patat (Ck ))

12:

k0 :[xt+1
Ck0 : xti  patat (Ck )]
i

// Observation step

for all Ck  C do
n
o
t
14:
Yk  yjt+1  Y t+1 | there is a directed path from Ck to yjt+1 in a
13:

15:

if Yk =  then

16:

 bt+1
bt+1
k
k

17:
18:
19:

else
for all s0k  S(Ck ) do
t+1 0
0
bt+1
k (sk )  2 bk (sk )

Y

X

t

al (s, ot+1 )

Y

0
bt+1
k0 (sk )

(Cl ) 6= 
Cl  C : Cl Yk 6= s  S(pat+1
(Cl )) : sk = s0k k0 6= k : Ck0  pat+1
at
at

20:

return

(bt+1
k )Ck C

Algorithm 3 provides a procedural specification of PSBF. The algorithm takes as inputs
the action at time t, at , the subsequent observation at time t + 1, ot+1 , and the belief factors
at time t, btk . The internal parameters are the state clustering C, the observation clustering
C, and the set of DBNs (a )aA which define the process. Lines 4 to 11 implement the
transition step while lines 12 to 19 implement the observation step. Note that it suffices to
execute lines 5 and 14 once in advance (or on demand) and to remember the results for
future reference. The algorithm returns the updated belief factors bt+1
k .
1156

fiExploiting Causality for Selective Belief Filtering in DBNs

5.5 Space and Time Complexity
A belief factor bk has one elementP
bk (sk ) for each sk  S(Ck ).5 Thus, the total space required
to maintain K belief factors bk is K
k=1 |S(Ck )|. Furthermore, the size of the set S(Ck ) grows
exponentially with the number of variables in Ck , hence the dominant growth factor in the
space requirement is given by the largest cluster Ck such that |Ck | = maxk0 |Ck0 |. Therefore,
the space complexity of PSBF is in O(exp maxk |Ck |), hence the representation is feasible
for reasonably small clusters Ck .
Similarly, the numberPof operations required to perform the transition and observation
steps is in the order of 2 K
k=1 |S(Ck )| in the worst case (i.e. all clusters need to be updated
in both steps). Specifically, line 11 and line 19 in Algorithm 3 are each executed once for
every sk  Ck . The dominant growth factor is again given by the largest cluster Ck , hence
the time complexity of PSBF is in O(2 exp maxk |Ck |) = O(exp maxk |Ck |). Note that this
assumes that the analysis performed by lines 5 and 14 in Algorithm 3 is done in advance.
The above time complexity is for the worst case, in which all clusters need to be updated
in the transition and observation steps. It is difficult to derive the time complexity for the
average case because it is unclear what the average case is in terms of passivity. Even if we
stipulate a certain average degree of passivity (e.g. 50% of all variables are passive), it would
still be difficult to make a general statement about time requirements since this depends
crucially on how the passive variables are distributed across the clusters. For example, even
if a process has on average 90% passivity, if there is one active variable in each cluster
then every cluster would need to be updated in the transition step. Thus, the only general
statement we can make with regards to passivity is that the time complexity of PSBF can
be refined to O(exp maxCk  CT  CO |Ck |), where CT and CO include only those clusters that
need to be updated in the transition and observation step, respectively.
5.6 Error Bounds
There are five possible sources of approximation errors in PSBF:
 If the clusters are correlated (i.e. (A1) or (A3) are violated)
 If the clusters are overlapping (i.e. (A2) or (A4) are violated)
 Generally in (8) if multiple observation clusters Cl are used
In the first two cases, the approximation error depends on the amount of correlation
and overlap. If there is only little correlation and overlap between the clusters, then the
approximation error can be expected to be small. Conversely, if the clusters are strongly
correlated and overlapping, then the approximation error can be expected to be large.
Boyen and Koller (1998) provide a useful analysis of the error bound of any filtering
method which uses a factored belief state representation. Since PSBF uses a factored
representation, their analysis applies directly to PSBF. The purpose of this section is to
restate the main result of their analysis in the context of our work.
Their analysis uses the concept of relative entropy (Kullback & Leibler, 1951) as a
measure of similarity for belief states:
5. In practice, it suffices to store only |S(Ck )|  1 elements, but this is irrelevant in our analysis.

1157

fiAlbrecht & Ramamoorthy

Definition 7 (Relative entropy). Let  and  be two probability distributions defined over
a set X. The relative entropy from  to  is defined as
KL(||) =

X
xX

(x) ln

(x)
(x)

where (x) > 0  (x) > 0.
Similar to Boyen and Koller (1998), we define the approximation error incurred by PSBF
relative to the exact belief state. However, since we consider a decision process with multiple
actions a  A (represented by the DBNs a ), we define the error for each action respectively:
Definition 8 (Approximation error). Let b be an exact belief state and b be the approximation by PSBF. After taking action a, let b0 be the exact update of b (using (1) and (2))
and b0 be the PSBF-update of b (using (6) and (8)). Furthermore, let b0 be the exact update
of b (using (1) and (2)). We say that PSBF incurs error a in a relative to b0 if
KL(b0 ||b0 )  KL(b0 ||b0 )  a .
The analysis also relies on the concept of mixing rates. Intuitively, the mixing rate  a of
a DBN a quantifies the degree of stochasticity in a . It depends on the mixing rates ka of
the individual clusters Ck :
Definition 9 (Mixing rate). The mixing rate of a cluster Ck  X t+1 in a is defined as
ka = 0min
00

s ,s S

X



min Tka (s0 , s), Tka (s00 , s) .

sS(Ck )

If all Ck satisfy (A1) and (A2), and if all observation variables Y t+1 are in one observation
cluster, then the mixing rate of a is given by  a = (mink ka /r)q where each cluster Ck
depends on at most r and influences at most q other clusters Ck0 6=k (Boyen & Koller, 1998).
In the worst case (that is, all (A1A4) are violated), the minimal mixing rate is given by ka
for the single cluster Ck = X t+1 .
Finally, the main result in the work of Boyen and Koller (1998), here restated in the
context of our work in Theorem 3, essentially states that the approximation error of PSBF
(measured in terms of relative entropy) is bounded by the mixing rates of the process:
Theorem 3 (Boyen & Koller, 1998). Let bt be an exact belief state and bt be the approximation by PSBF using clusters Ck . Then, for any t with states ~s = (s0 , ..., st ) and actions
~a = (a0 , ..., at1 ), we have
h
i max
a
a  ~a 
Eo1 ,...,ot KL(bt ||bt ) 
mina  ~a  a
where the expectation E is Q
taken over all possible sequences of observations o1 , ..., ot with
1
t
a  +1 , o +1 ), and where a and  a are defined as above.
probabilities P (o , ..., o ) = t1
 =0  (s
1158

fiExploiting Causality for Selective Belief Filtering in DBNs

Process size

# of x vars (n)

# of y vars (m)

# of states (|S|)

# of obs. (|O|)

S

10

3

> one thousand

8

M

20

6

> one million

64

L

30

9

> one billion

512

XL

40

12

> one trillion

4096

Table 1: Synthetic process sizes. All variables are binary.

6. Experimental Evaluation
We evaluated PSBF in two experimental domains: In Section 6.1, we evaluated PSBF in
synthetic (i.e. randomly generated) processes with varying sizes and degrees of passivity. In
Section 6.2, we evaluated PSBF in a simulation of a multi-robot warehouse system. A brief
summary of the experimental results is given in Section 6.3.
6.1 Synthetic Processes
We first evaluated PSBF in a series of synthetic processes. PSBF is compared with a selection
of alternative methods, including PF (Gordon et al., 1993), RBPF (Doucet et al., 2000), BK
(Boyen & Koller, 1998), and FF (Murphy & Weiss, 2001); see Section 2 for a discussion of
these methods. The algorithms were implemented in Matlab 7.13, where we used the Matlab
toolbox BNT (Murphy, 2001) to implement BK and FF.
6.1.1 Specification of Synthetic Processes
We generated synthetic processes of four different sizes which are specified in Table 1. Each
process was generated as follows:
First, each variable xit+1 is chosen to be passive with probability p, in which case we
also add the edge (xti , xt+1
i ). We refer to p as the degree of passivity. To sample further
t
t+1
edges from X /X
to X t+1 , we generate a mixture of Gaussians G using Algorithm 4 (see
Appendix C). Figure 7 shows an example of G generated for a process of size M. The set
G is used to produce areas of correlated variables (i.e. the Gaussians), which will then
constitute natural candidates for state clusters.
Let  be the vector of maximum densities for each Gaussian in G, and let i be the
vector of densities at value i  N. Then, for every combination of i and j, the edge (xti , xt+1
j )
2
is added with probability equal to the maximum element in i j / , in which all operators
are point-wise. If xt+1
was chosen to be passive, then the edge (xti , xt+1
i
j ) is only added if
t+1 t+1
t+1 t+1
i < j. In that case, we also add the edge (xi , xj ). Edges (xi , xj ) are added similarly
t+1
for each i < j,6 where we also add the edge (xti , xt+1
j ) for passive xj . To ensure that every
variable has an effect in the generated process, each xti is connected to at least one xt+1
j
t+1
t or X t+1 (adding
(adding (xti , xt+1
)
if
necessary)
and
each
x
has
at
least
one
parent
in
X
i
j
6. The condition i < j in both cases is to ensure that the resulting DBN is acyclic.

1159

fiAlbrecht & Ramamoorthy

0.35
0.3

Density

0.25
0.2
0.15
0.1
0.05
0

2

4

6

8

10

12

14

16

18

20

i, j

Figure 7: Example of mixture of Gaussians generated for a process of size M and consisting
t/t+1
of three Gaussians. The closer two variables xi
and xt+1
are under the peak of a common
j
Gaussian, the higher the probability that an edge will be added between them.
t+1 t+1
(xtj , xt+1
j ) if necessary). Finally, edges (xi , yj ) are added with probability 0.1, for each
i, j, while ensuring that each yjt+1 has at least one parent in X t+1 .
All variables in the process are binary. Passive variables are assumed to be passive with
respect to all of their parents in X t . The distributions Pa of xt+1
 X t+1 are generated
i
t+1
uniformly randomly without bias. For passive variables xi , we modify Pa to satisfy clause
(ii) in Definition 3. The distributions Pa of yjt+1  Y t+1 are generated with each probability
sampled uniformly from either [0.0, 0.2] or [0.8, 1.0], to obtain meaningful observations.
Finally, every process consists of two actions. These are obtained by randomly choosing
between one and three variables xt+1
whose distributions Pa are resampled as above and
i
edges from X t added with probability 0.1 (passive variables chosen in this way are no longer
passive). During simulations, these actions are chosen uniformly randomly.
Each process starts in a random initial state, and all algorithms are tested on the same
sequence of processes, initial states, chosen actions, and random numbers.

6.1.2 Clustering Methods
We used three different clustering methods, denoted hpci, hmorali, and hmodisi. The methods
were applied to the variables in X t+1 without edges involving X t or Y t+1 :
 hpci drops the directions of the edges (i.e. for any edge xt+1
 xt+1
it ads the reverse
i
j
t+1
t+1
edge xj  xi ) and puts all variables between which there is a (undirected) path
into one cluster. By definition, the resulting clusters satisfy all assumptions (A1A4).
 hmorali connects all parents of a variable and drops the directions (it moralises the
variables) and then extracts clusters of fully connected variables (maximum cliques).
The resulting clusters may not satisfy any of the assumptions (A1A4).
 hmodisi is similar to hmorali but truncates the resulting clusters to make them disjoint
(clusters are removed if they become a subset of another cluster). By definition, the
resulting clusters satisfy (A2/A4), but not necessarily (A1/A3).
As an example, consider Figure 5 from Section 5.1. Here, hpci would produce the cluster
C1 from Figure 5b, since all variables are connected by an undirected path. Furthermore,
1160

fiExploiting Causality for Selective Belief Filtering in DBNs

hmorali would produce the two clusters C1 and C2 from Figure 5c, which correspond to the
two maximum cliques after moralising the variables in X t+1 . Finally, hmodisi would produce
the cluster C1 from Figure 5c and the cluster C3 from Figure 5a.
PSBF used the same clustering method to generate clusters of state variables (Ck )
and observation variables (Cl ). Moreover, PSBF enforced (A1/A3) whenever necessary by
modifying the variable distributions as described in Section 5.1.
6.1.3 Accuracy
In order to compare the accuracy of the tested algorithms, we computed the relative entropy
(cf. Definition 7) from exact belief states obtained using the exact update rule (cf. Definition 1)
to the approximate belief states produced by the tested algorithms. However, since exact
belief states and relative entropy are hard to compute for large processes, we were able to
compare the accuracy of algorithms in processes of size S only. All algorithms were initialised
with uniform belief states, or uniformly sampled particles.
We first compared the accuracy of PSBF and BK, since they use the same factorisation
in their belief state representations. Figure 8 shows the relative entropy of PSBF and BK averaged over 1000 processes with 0%, 20%, 40%, 60%, 80%, and 100% passivity, respectively.
The results show that PSBF hpc/modisi produced a lower relative entropy (i.e. higher accuracy) than BK hpc/modisi, and that PSBF hmorali produced a relative entropy comparable
to that of BK hmorali. This indicates that violations of (A2/A4) introduce smaller errors
than violations of (A1/A3). Note that PSBF and BK had the same convergent behaviour in
their relative entropy, which shows that the approximation error due to the factorisation was
bounded, as discussed in Section 5.6. This is interesting since PSBF and BK obtain approximation errors from the factorisation in different ways: PSBF loses accuracy by modifying
the variable distributions to ensure that the state clusters are independent (cf. Section 5.2),
while BK loses accuracy by marginalising out the original factorisation after the inference (i.e.
the projection step; cf. Section 2.1). Nevertheless, as shown in our results, the resulting
approximation errors were bounded in both cases, with similar convergence.
Note that the relative entropy of both methods increased with the degree of passivity in
the process. This is explained by the fact that higher passivity implies higher determinacy
and, therefore, lower mixing rates (cf. Definition 9), which are a crucial factor in the error
bounds of PSBF and BK (cf. Theorem 3). Finally, note that PSBF did not produce exact
belief states (i.e. zero relative entropy) when using hpci clustering, despite the fact that the
clusters generated by hpci satisfy all assumptions (A1A4). However, as discussed in detail in
Sections 5.3 and 5.6, another possible source of approximation errors is if multiple observation
clusters are used, which was often the case when using hpci to produce observation clusters.
To compare the accuracy of PF/RBPF with PSBF/BK, the number of samples used in
PF/RBPF was chosen automatically in each process such that they required approximately
as much time per belief update as PSBF hmorali and BK hmorali, respectively. In our
experiments, this meant that PF (RBPF) was only able to process between 100 and 300 (20
and 50) samples. However, since each process has over 1000 states, this was not nearly enough
to represent a uniform belief state. Hence, PF/RBPF produced much higher relative entropy
than PSBF/BK. Moreover, the fact that the processes have very high variance means that
PF/RBPF would require many more samples to achieve the same accuracy as PSBF/BK (as
1161

fiAlbrecht & Ramamoorthy

2

0.6
0.4

BK (pc)

PSBF (pc)

BK (moral)

PSBF (moral)

BK (modis)

PSBF (modis)

Relative entropy

Relative entropy

0.8

0.2
0

0

500

1000

1500
2000
Transition

2500

1.5
1
0.5
0

3000

0

500

(a) 0% passivity

1000

1500
2000
Transition

2500

3000

2500

3000

2500

3000

(b) 20% passivity

5
Relative entropy

Relative entropy

3

2

1

4
3
2
1

0

0

500

1000

1500
2000
Transition

2500

0

3000

0

500

8

6

4

2

0

1500
2000
Transition

(d) 60% passivity

Relative entropy

Relative entropy

(c) 40% passivity

1000

0

500

1000

1500
2000
Transition

2500

3000

(e) 80% passivity

6
4
2
0

0

500

1000

1500
2000
Transition

(f) 100% passivity

Figure 8: Accuracy results for PSBF and BK. Plots show relative entropy from exact to
algorithms belief states (lower is better). Results are averaged over 1000 processes of size
S (n = 10, m = 3), where on average 0%100% of non-target variables were passive (cf.
Section 6.1.1). PSBF/BK used clustering methods hpci, hmorali, and hmodisi.
shown in the next section). One would expect that this latter issue was alleviated by the use
of exact inference in RBPF (cf. Section 2.1). However, this is only the case if much of the
variance in the process can be captured in the marginal distributions used in the particles in
RBPF. In contrast, our synthetic processes exhibit high variance across all variables, and our
1162

fiExploiting Causality for Selective Belief Filtering in DBNs

automatic grouping7 of state variables into sampled and exact variables still contained
much variance in the sampled variables. Hence, RBPF required significantly more samples
than the number it could process in the time provided.
Finally, in order to compare the accuracy of FF with PSBF/BK, the number of iterations
used in FF (more precisely, the number of iterations in loopy belief propagation; cf. Murphy &
Weiss, 2001) was chosen automatically in each process such that FF required approximately
as much time per belief update as PSBF hmorali and BK hmorali, respectively. However,
while FF was often able to perform several iterations in the provided time, the resulting
relative entropy was again substantially higher than that of PSBF/BK. The problem is
that FF was designed for a specific class of DBN topologies, namely those containing no
edges within X t+1 (called regular DBNs by Murphy & Weiss, 2001). This is what allows
FF to use a fully factored representation of belief states, in which each variable is its own
belief factor. However, the processes used in our experiments have high intra-correlation
between state variables (i.e. many edges in X t+1 ), especially with increasing passivity. These
correlations cannot be captured in the belief state representation of FF, resulting in a
significantly higher relative entropy than PSBF/BK.
6.1.4 Timing
We measured computation times in processes of sizes S, M, L, XL with passivities of 25%,
50%, 75%, 100%, respectively. PSBF and BK used hmorali clustering, which seemed most
appropriate for a fair comparison since it produced consistently similar accuracy for both
algorithms. The number of samples used in PF was chosen automatically in each process
such that PF achieved an average accuracy approximately as good as that of PSBF and
BK, respectively, in the final 20% of the process. As this involved computing exact belief
states and relative entropies, we were able to use PF in processes of size S only. We omit
RBPF and FF in this section as they were shown in the previous section to be unsuitable
for the processes we consider. PSBF was tested with 1, 2, and 4 parallel processes, which
were allocated approximately the same number of belief factors.
Figures 9a  9d show the times for 1000 transitions averaged over 1000 processes, and
Figure 9e shows the average percentage of belief factors that were updated in the transition
and observation steps of PSBF. The timing reported for PSBF includes the time taken to
modify variable distributions (in case of overlapping clusters) and to detect skippable clusters
in the transition and observation steps, both of which were done once in advance for each
action. The results show that PSBF was able to minimise the time requirements significantly
by exploiting passivity. First, we note that there were only marginal gains from 25% to 50%
passivity, despite the fact that PSBF updated 14% fewer clusters in the transition step. This
is because these clusters were mostly very small. However, there were significant gains from
50% to 75% passivity with average speed-ups of 11% (S), 14% (M), 15% (L), 18% (XL), and
7. It is an open question how to group state variables into sampled and exact variables (Doucet et al.,
2000). We used a simple heuristic whereby the set of sampled variables contained all variables xt+1
that
i
had no parents in X t/t+1 or none other than xti . The remaining variables in X t+1 constituted the set of
exact variables. To ensure that the resulting grouping was valid for all actions (i.e. DBNs) in a process,
we considered edges in all involved DBNs; that is, we performed the grouping over the union of Ea for
all a. Moreover, to improve efficiency, we further subdivided the set of exact variables into clusters of
variables that were connected by undirected edges in X t+1 without edges involving the sampled variables.

1163

fi100
PF:BK
PF:PSBF
BK

50

0

25%

50%
75%
Passivity

400

200

150

PSBF1 100
PSBF2
PSBF4
50

100%

(a) S (n=10, m=3)

0

25%

50%
75%
Passivity

Seconds for 1000 transitions

150

Seconds for 1000 transitions

Seconds for 1000 transitions

Seconds for 1000 transitions

Albrecht & Ramamoorthy

350
300
250
200
150
100
50

100%

0

(b) M (n=20, m=6)

25%

100%

(c) L (n=30, m=9)

100
% updated belief factors

50%
75%
Passivity

700
600
500
400
300
200
100
0

25%

50%
75%
Passivity

100%

(d) XL (n=40, m=12)

S (trans)
S (obs)
M (trans)
M (obs)

80
60
40
L (trans)
L (obs)
XL (trans)
XL (obs)

20
0

25%

50%
75%
Passivity

100%

(e) Updated belief factors

Figure 9: Timing results. (ad) Average number of seconds required for 1000 transitions
on a UNIX dual-core machine with 2.4 GHz, for sizes S, M, L, XL. Passivity of p% means
that on average p% of non-target variables were passive (cf. Section 6.1.1). PSBF and BK
used hmorali clustering. PF was optimised for binary variables and used number of samples
to achieve accuracy of PSBF and BK, respectively. PSBF was run with 1 (PSBF-1), 2
(PSBF-2), 4 (PSBF-4) parallel processes. (e) Average percentage of belief factors which
were updated in the transition and observation steps, respectively.
from 75% to 100% passivity with further average speed-ups of 11% (S), 33% (M), 46% (L),
49% (XL). This shows that the computational gains can grow significantly with both the
degree of passivity and the size of the process.
Our results show that PSBF consistently outperformed BK in all process sizes. There
are two main computational savings in PSBF relative to BK: firstly, by skipping over belief
factors in the transition and observation steps, and secondly, by not having to perform a
potentially expensive projection step to restore the original factorisation after the inference.
However, while the times of both algorithms grew exponentially in the size of the process,
we note that the relative difference between PSBF and BK decreased significantly for lower
degrees of passivity. This is an instance of No Free Lunch (see Section 7 for a discussion),
which means that PSBF performs best in processes with high passivity but can suffer in
performance in processes that lack passivity. Specifically, the computational overhead of
modifying variable distributions and detecting skippable belief factors does not amortise
1164

fiExploiting Causality for Selective Belief Filtering in DBNs

as effectively in large processes with low passivity. Furthermore, with low passivity, PSBF
often has to perform full transition and observation steps (i.e. update all belief factors in
each step), which can be costly in large processes.
How were BK and PF affected by passivity? Not surprisingly, the performance of BK was
nearly unaffected by the increasing degrees of passivity. The junction tree algorithm used in
BK benefited marginally from an increased sparsity in the process, but the computational
gains were minimal. We were at first unable to use PF as it required too many samples
(between 10k and 200k) to achieve comparable accuracy to PSBF/BK, due to the very
high variance in the processes. In order to investigate the effect of passivity on PF, we
implemented a version of PF which was strictly optimised for binary variables. Interestingly,
we found that passivity had an adverse effect on the performance of PF, requiring it to use
exponentially more samples with increased passivity (see Figure 9a). This makes sense if we
view PF as a factored approximation method (such as PSBF and BK) which means that the
analysis in Section 5.6 applies. However, because PF puts all variables into a single cluster
(since it is not actually a factored method), the mixing rate of the process will be much lower
than for PSBF and BK (as discussed in Section 5.6) and, thus, the error bounds are less
tight. To compensate for this, PF requires significantly more samples for increased passivity.
6.2 Multi-robot Warehouse System
In this section, we demonstrate how passivity can occur naturally in a more complex system
and how PSBF can exploit this to accelerate the filtering task. To this end, we consider
a multi-robot warehouse system in the style of Kiva (Wurman et al., 2008), in which the
robots task is to transport goods within the warehouse (cf. Figure 10a).
6.2.1 Specification of Warehouse System
Figure 10b shows the initial state of the warehouse simulation. The warehouse consists of
2 workstations (W1, W2), 4 robots (R1R4), and 16 inventory pods (I1I16). Each robot
can move forward and backward, turn left and right, load and unload an inventory pod (if
positioned under the pod), or do nothing. As in Kiva, robots can move under inventory pods
unless they are carrying a pod, in which case the other pods become obstacles. The move
and turn operations are stochastic in that the robot may move/turn too far (3% chance) or
do nothing (2% chance). Each robot possesses two sensors, one telling it which inventory
pod it has loaded (if any) and one for the direction it is facing. The direction sensor is noisy
in that a random direction may be reported (3% chance).
Each robot maintains a list of tasks in the form of Bring inventory pod I to workstation
W (yellow area around W) and Bring inventory pod I to position (x,y). How these tasks
are executed depends on the control mode, of which we use two in our simulations:8
8. Our control modes are ad hoc and often make suboptimal decisions. However, we found that current
solution techniques for (DEC-)POMDPs, including approximate methods, were infeasible in this setting.
Nonetheless, the quality of the decisions made by our control modes largely depends on the accuracy
of the belief states, hence it is important that the belief states are updated accurately. Therefore, the
control modes were sufficient for our purposes.

1165

fiAlbrecht & Ramamoorthy

(a) Kiva warehouse system

(b) Initial state of simulation

Figure 10: (a) Kiva warehouse system (image reproduced from DAndrea & Wurman, 2008).
Robots (orange coloured) transport shelfs with goods to and from workstations. (b) Initial
state of the warehouse simulation. The warehouse consists of 2 workstations (W1, W2), 4
robots (R1R4), and 16 inventory pods (I1I16).
Centralised mode: A central controller maintains a belief state bt about the state of
the warehouse system. At each time t, it samples 100 states from bt and removes all

duplicate states, resulting in the set
P S t= {s1 , s2 , ...}. It then resamples a state s  S

t

with probabilities w(s ) = b (s )/ q b (sq ). Based on s and the current task of each
robot, it performs an A search (Hart, Nilsson, & Raphael, 1968) (with Manhattan
distance) in the space of joint actions to find the optimal action for each robot. After
executing their actions, the robots send their sensor readings to the controller, and the
controller updates its belief state using the sensor readings.
Decentralised mode: Each robot maintains its own belief state and there is no communication between the robots. The only knowledge the robots have about each other are
their current tasks, communicated by the task allocation module. At each time t, each
robot samples the set S and state s as is done in the centralised mode. Treating the
other robots as static obstacles, it performs an A search based on s and its current
task to find an action at . This is repeated for each other robot r in all states sq  S,
resulting in actions ar,q which are used
P to obtain distributions r : A  [0, 1] (A is
the set of all actions) with r (a) = q : ar,q =a w(sq ). The robot then executes its action at and updates its belief state using its sensor readings and the distributions r
to average over the other robots actions.
The tasks are generated by an external scheduler in time intervals sampled from U [1, 10].
Each generated task is assigned to one of the robots through a sequential auction (Dias, Zlot,
Kalra, & Stentz, 2006). The robots bids are calculated as their total number of steps needed
to solve all of their current tasks and the auctioned task (in a simplified model in which the
other robots are removed), averaged over all states in S. The robot with the lowest bid is
assigned the task.
1166

fiExploiting Causality for Selective Belief Filtering in DBNs

Figure 11: Example DBN of a smaller warehouse system consisting of only one inventory
pod (I1) and two robots (R1, R2). The DBN implements the joint action in which R1 moves
and R2 turns. Dashed circles mark passive state variables. The coloured areas represent the
state clusters C1 to C8 .

6.2.2 DBN Topology and Clustering
Figure 11 shows an example DBN for a smaller warehouse with one inventory pod and two
robots. Each inventory pod I is represented by two variables, I.x and I.y, which correspond
to the x and y position of the inventory pod. Each robot R is represented by four variables:
R.x/R.y for its x/y position, R.d for its direction, and R.s for its status. The status of a
robot R is either R.s=0 (unloaded) or R.s=I (loaded with inventory pod I). Constants such
as the size of the warehouse and the positions of the workstations are omitted in the DBN.
There are four types of clusters: The I-clusters (C1C4) preserve the correlation that if
R is loaded with I, then I must always have the same position as R (there are two I-clusters
for each (I,R) pair); The R-clusters (C5) and S-clusters (C6), respectively, preserve the
correlation that no two robots can have the same position or carry the same inventory pod
(there is one R/S-cluster for each (Ra,Rb) pair with a > b); And, finally, the D-clusters (C7,
C8). PSBF uses singleton observation clusters (i.e. one cluster for each observation variable).
There are some differences between the DBNs for the centralised and decentralised modes
(Figure 11 uses the centralised mode). In the centralised mode, there is one DBN for each
action combination of the robots. Since the controller observes all R.s noise-free, it can add
edges from R.x/R.y to I.x/I.y if R.s=I or remove them otherwise to simplify the inference
(thus, in Figure 11, R1 is loaded with I1 and R2 is unloaded). In the decentralised mode,
each robot only observes its own sensor readings, hence it can add or remove edges only for
itself, while edges for all other robots must be permanently added. This also means that the
other robots status variables (R.s) must be linked to all I.x/I.y and, therefore, included in
the I-clusters (to preserve the correlation that I must have the same position as R if R is
loaded with I). Moreover, since each robot only knows its own action, there is one DBN for
1167

fiSeconds per transition

Albrecht & Ramamoorthy

Centralised
180 Decentralised
160
140
120
100
80
60
BK

PSBF

PF

Figure 12: Results of the warehouse simulation, using the centralised and decentralised
control modes. Timing measured on a UNIX dual-core machine with 2.4 GHz and averaged
over 20 different simulations with 100 transitions each.
each of its own actions, and all variables associated with the other robots are active (the
distributions r defined in the previous section are used to average over their actions).
6.2.3 Results
We implemented PSBF, BK, and PF in C#, using the framework Infer.NET (Minka, Winn,
Guiver, & Knowles, 2012) to implement BK. This allowed BK to exploit sparsity in the
process and offered improved memory handling. PSBF was optimised for sparsity in (6) and
(8), respectively, by summing over states s for which all btk0 / bt+1
k0 are positive. PF naturally
benefits from sparsity as it allows it to concentrate the samples on fewer states. The number
of samples used in PF was set in such a way that the controller decisions were invariant of
the random numbers used in the sampling process of PF. This was done to ensure that the
results were repeatable. Finally, to maintain sparsity in the process, each probability in the
belief states lower than 0.01 was set to 0. All tested algorithms were initialised with an exact
belief state, shown in Figure 10b.
Figure 12 shows the time per transition averaged over 20 different simulations with 100
transitions each. The timing reported for PSBF includes the time needed to modify variable
distributions (for overlapping clusters) and to detect skippable belief factors for the transition
and observation steps, both of which were done once on demand for every previously unseen
DBN. In the centralised mode, PSBF was able to outperform BK on average by 49% and
PF by 36%. PF needed 20,000 samples to produce consistent (i.e. repeatable) results. In
the decentralised mode, PSBF outperformed BK on average by 17% and PF by 32%. PF
now needed 45,000 samples to produce consistent results, due to the increased variance in
the process. All differences were statistically significant, based on paired t-tests with a 5%
significance level. Note that PSBF and BK were slower in the decentralised mode since the
corresponding DBNs had much higher inter-connectivity. In addition, PSBF updated more
belief factors since there were more active variables.
As expected, PSBF was able to exploit the high degree of passivity in the process to
accelerate the filtering task. In many cases, this meant that PSBF needed to update less than
half of the belief factors. Precisely how many belief factors had to be updated depends on the
1168

fiExploiting Causality for Selective Belief Filtering in DBNs

performed action. To illustrate this, consider the smaller warehouse DBN shown in Figure 11
(for the centralised mode), in which R1 is moving and R2 is turning. Here, R1.x, R1.y, and
R2.d are active variables while all other variables are passive (dashed circles), corresponding
to a passivity of 70%. In this DBN, PSBF updates the belief factors corresponding to clusters
C1, C2, C5, and C8, since they each contain active variables, and it also updates the belief
factors for C3 and C4, since there are directed paths from active variables (R1.x and R1.y)
to each of them. Therefore, the only factors which are not updated are for C6 and C7. Now
consider the full warehouse in our experiment, which contains 16 inventory pods and 4 robots,
resulting in 48 variables with 128 I-clusters, 6 R-clusters, 6 S-clusters, and 4 D-clusters.
Assume a similar situation in which one robot moves with an inventory pod, say R4 with I1,
while the R13 turn. In this case, PSBF updates only 3 of 6 R-clusters (those containing
R4), 0 of 6 S-clusters (since no status change), 3 of 4 D-clusters (for R13), and 38 of 128
I-clusters (32 I-clusters containing R4 plus 6 I-clusters from R13 for I1), amounting to a
total saving of 69.44% of belief factors which do not need to be updated.
The number of states in the warehouse system (including invalid states) exceeded 1045
states. Therefore, we were unable to compare the accuracy of the tested algorithms in terms
of relative entropy. Instead, we compared their accuracy based on the results of the task
auctions and the number of completed tasks by the end of each simulation. This gives a
good indication of the algorithms accuracy, since both the outcome of the auction and the
number of completed tasks depend on the accuracy of the belief states. In the centralised
mode, the algorithms generated over 95% identical task auctions and completed 15.7 (BK),
15.5 (PSBF), and 15.2 (PF) tasks on average. In the decentralised mode, they generated
over 93% identical auctions and completed 12.1 (BK), 12.2 (PSBF), and 11.7 (PF) tasks on
average. In both modes, none of these differences were statistically significant. Therefore,
this indicates that PSBF achieved an accuracy similar to that of BK and PF.
6.3 Summary of Experimental Evaluation
The experimental results show that PSBF produces belief states with competitive accuracy:
In the synthetic processes, PSBF achieved an accuracy which on average was better or
comparable to the accuracy of the alternative methods. In the warehouse system, PSBF
was able to complete a statistically equivalent number of tasks as compared to the other
methods, which indicates that its accuracy was equivalent or comparable.
Furthermore, the experimental results show that PSBF performed the belief updates
significantly faster than the alternative methods: In the synthetic processes, PSBF using no
parallel processes outperformed BK by up to 64% in the largest process (XL), while PF took
too much time to achieve an accuracy comparable to PSBF. In particular, the results show
that the computational gains can grow significantly with both the degree of passivity and the
size of the process. In the warehouse system, PSBF outperformed the alternative methods by
up to 49%, which is a substantial saving considering the size of the state space (more than
1045 states). Furthermore, the computational gains where much higher in the centralised
control mode than in the decentralised control mode, since the latter had a significantly
lower degree of passivity. Therefore, this again shows that high degrees of passivity can bear
great potential for the filtering task.
1169

fiAlbrecht & Ramamoorthy

7. No Free Lunch for PSBF
Our view is that no belief filtering method is generally suited for all types of processes.
Instead, each method assumes a certain structure in the process (explicitly or implicitly)
which it attempts to exploit in order to render the filtering task more tractable. Typically, the
methods are tailored in such a way with respect to this structure that they perform well if the
structure is present in the process, but suffer a significant loss in performance if the structure
is absent. For instance, PF works best in processes with low degrees of uncertainty, since
this means that fewer state samples are needed for acceptable approximations. On the other
hand, the number of samples needed for acceptable approximations can grow substantially
with the degree of uncertainty in the process (as shown in our experiments). As another
example, BK works best in processes with little correlation between state variables, since
this means that the belief factors will be small and can be processed efficiently. However, if
there are many variables which are strongly correlated, then BK typically becomes infeasible.
Therefore, these structural assumptions have to be taken into account when choosing a
filtering method for a specific process.
A formal account of this view is given by the No Free Lunch theorems (Wolpert
& Macready, 1997, 1995) which state that, intuitively speaking, any two algorithms have
equivalent performance when averaged over all possible instances of the problem. In other
words, if there are classes of problem instances for which algorithm A has better performance
than algorithm B, then there must be other classes of problem instances for which A has
worse performance than B. Then, the question is: for what class of problem instances (that
is, processes) can PSBF be expected to achieve good performance? This class is essentially
described by the following three criteria:
Degree of passivity  PSBF attempts to accelerate the filtering task by omitting the
transition step for as many belief factors as possible. This depends on the passivity of
the variables in the state clusters. In the ideal case, the process exhibits a high degree
of passivity such that PSBF can omit the transition step for many belief factors. In
the worst case, the process has no passive variables at all, and PSBF has to update all
belief factors in the transition step. However, as discussed in Section 5.5, a high degree
of passivity is not necessarily sufficient to infer that many clusters can be skipped
in the transition step, since the passive variables could be distributed in such a way
that no cluster can be skipped (e.g. if the passive variables are distributed uniformly
amongst the state clusters). Therefore, in an optimal case, the passivity is concentrated
on correlated state variables such that passive variables end up in the same clusters.
Size of state clusters  The space and time complexity of the belief state representation
in PSBF is exponential in the size of the largest state cluster (cf. Section 5.5). Therefore,
in the ideal case, the relevant variable correlations can be captured in small state
clusters and the cost of storing the belief factors and performing the update procedures
is small. In the worst case, large state clusters are required to retain the variable
correlations and the cost of storing and updating belief factors is large. Another reason
why the state clusters should be small is because of the way in which PSBF performs
the transition step. One pre-requisite for omitting the transition step for a belief factor
is that all variables in the corresponding cluster are passive. If there are many variables
1170

fiExploiting Causality for Selective Belief Filtering in DBNs

in one cluster, then it is less likely that all variables in the cluster are passive, and,
therefore, it is less likely that the cluster can be skipped.
Structure of observations  A third criterion, though arguably less important than the
other criteria, is the structure of the observations (i.e. the way in which the observation
variables depend on the state variables) and the size of the observation clusters (Cl ).
PSBF attempts to accelerate the observation step by skipping over all those state
clusters whose variables are structurally independent of the observation, and, if a
cluster cannot be skipped, by incorporating only those observation clusters which are
relevant to the update. Therefore, in the ideal case, only a fraction of the state clusters
depend on the observation, and the relevant correlations between observation variables
can be captured in small observation clusters. In the worst case, all state clusters
depend on the observation in some sense, and the structure of the observation does
not allow for an efficient clustering.
Thus, in summary, PSBF is most suitable for processes with high degrees of passivity and
in which the relevant variable correlations can be captured in small state and observation
clusters. On the other hand, PSBF may not be suitable if there is no or only low degrees
of passivity, and if large state and observation clusters are necessary to retain the relevant
variable correlations in the process.
In addition to identifying the class of processes for which a filtering method is suitable, it
is also important to justify the practical relevance of this class. In this work, we are interested
in robotic and other physical decision processes (as shown by our examples and experiments).
Such systems typically exhibit a number of features: First of all, robotic systems usually
have some causal structure (e.g. Mainzer, 2010; Pearl, 2000). Passivity, as a specific type of
causality, can be observed in many robotic systems, including the robot arm used in our
examples and the multi-robot warehouse system in Section 6.2. Furthermore, robotic systems
most typically have a modular structure, in which each module is responsible for a specific
subtask and may interact with other modules. This modular structure often allows for an
efficient clustering, in the sense that each module corresponds to a cluster of correlated state
variables. Finally, the sensors used in robotic systems typically only provide information
about certain aspects of the system, and some components of the system may not benefit
from some of the sensor information. In other words, there are independencies between state
and observation variables. These features correspond to the criteria (above) which specify
the class of processes for which PSBF is a suitable filtering method. Therefore, we believe
that this class is practically justified.

8. Conclusion
Inferring the state of a stochastic process can be a difficult technical challenge in complex
systems with large state spaces. The key to developing efficient solutions is to identify special
structure in the process, e.g. in the topology and parameterisation of dynamic Bayesian
networks, which can be leveraged to render the filtering task more tractable.
To this end, the present article explored the idea of automatically detecting and exploiting
causal structure in order to accelerate the belief filtering task. We considered a specific type of
causal relation, termed passivity, which pertains to how state variables cause changes in other
1171

fiAlbrecht & Ramamoorthy

state variables. To demonstrate the potential of exploiting passivity, we developed a novel
filtering method, PSBF, which uses a factored belief state representation and exploits passivity
to perform selective updates over the belief factors. PSBF produces exact belief states under
certain assumptions and approximate belief states otherwise. We showed empirically, in
synthetic processes with varying sizes and degrees of passivity as well as in an example of a
complex multi-robot system, that PSBF can be faster than several alternative methods while
achieving competitive accuracy. In particular, our results showed that the computational
gains can grow significantly with the size of the process and the degree of passivity.
Our work demonstrates that if a system exhibits much causal structure, then there can
be great potential in exploiting this structure to render the filtering task more tractable. In
particular, our experiments support our initial hypothesis that factored beliefs and passivity
can be a useful combination in large processes. This insight is relevant for complex processes
with high degrees of causality, such as robots used in homes, offices, and industrial factories,
where the filtering task may constitute a major impediment due to the often very large state
space of the system.
There are several potential directions for future work. For example, it would be useful
to know if the definition of passivity could be relaxed such that more variables fall under
this definition, and such that the principal idea behind PSBF is still applicable. One such
relaxation could be in the form of approximate passivity, which allows for small probabilities
that passive variables change values even if the relevant parents remain unchanged. In
addition, it would be interesting to know if the idea of performing selective updates over
belief factors (via passivity) could also be applied to other existing methods that use a
factored belief state representation (cf. Section 2.1). Finally, another useful avenue for future
work would be to formulate additional types of causal relations which can be exploited in
ways similar to how PSBF exploits passivity, or perhaps in ways other than that.

Acknowledgements
This article is the result of a long debate on the presented topic, and in the process benefited
from a number of discussions and suggestions. In particular, the authors wish to thank
anonymous reviewers from the NIPS12 and UAI13 conferences as well as the Journal of AI
Research; attendees of the workshop on Advances in Causal Inference held at UAI15; and
our colleagues in the School of Informatics at The University of Edinburgh. Furthermore, the
authors acknowledge the financial support of the German National Academic Foundation,
the UK Engineering and Physical Sciences Research Council (grant number EP/H012338/1),
and the European Commission (TOMSY Grant Agreement 270436).

1172

fiExploiting Causality for Selective Belief Filtering in DBNs

Appendix A. Proof of Theorem 1
To prove Theorem 1, it will be useful to first establish the following lemma:
Lemma 1. If (A1) holds and all xt+1
 Ck are passive in a , then
i
s, s0 : Tka (s, s0k ) = 1  sk = s0k .
Proof.
: The fact of (A1) means that a,i  Ck for all xt+1
 Ck . Since all xt+1
 Ck are
i
i
passive in a , it follows that all xtj  a,i are passive in a , for all a,i . Therefore, given
Tka (s, s0k ) = 1 and clause (ii) in Definition 3, it follows that sk = s0k .
: Follows directly by (A1) and the fact that all xt+1
 Ck are passive in a .
i

Using Lemma 1, we can give a compact proof of Theorem 1:
t

Theorem 2. If (A1) and (A2) hold, and if all xt+1
 Ck are passive in a , then
i
t
s : bt+1
k (sk ) = bk (sk ).

Proof.
0
bt+1
k (sk )

=

1

X

t

Tka (s, s0k )

s  S(pat t (Ck ))

=

1

X

btk0 (sk0 )

k0 :[xt+1
Ck0 : xti  pat t (Ck )]
i

a

Lem1

Y

a

t

Tka (s, s0k )

Y

btk0 (sk0 )

s  S(pat t (Ck )):sk =s0k k0 :[xt+1
Ck0 : xti  pat t (Ck )]
i
a
a

=

1 btk (sk )

X

t

Tka (s, s0k )

Y

btk0 (sk0 )

s  S(pat t (Ck )):sk =s0k k0 6= k:[xt+1
Ck0 : xti  pat t (Ck )]
i
a
a

{z

|

(A1)

= 1

=

1 btk (sk )

=

btk (sk ). (1 = 1 since btk normalised)

1173

}

fiAlbrecht & Ramamoorthy

Appendix B. Proof of Theorem 2
To prove Theorem 2, we first note the following proposition:

t

Proposition 1. If all xt+1
 Ck are marginally independent of all yjt+1  Y t+1 in a , then
i

s, s0 : k0 6=k sk0 = s0k0  a (s, ot ) = a (s0 , ot ).

This proposition follows directly by definition.

Using Proposition 1, we can give a compact proof of Theorem 2:
t

Theorem 2. If all xt+1
 Ck are marginally independent of all yjt+1  Y t+1 in a , then
i
t+1
s : bt+1
k (sk ) = bk (sk ).

Proof.
X

t+1 0
0
bt+1
k (sk ) = 2 bk (sk )

t

Y

a (s, ot+1 )

0
bt+1
k0 (sk )

t+1 )) : s = s0 k 0 6= k : C 0  pat+1 (Y t+1 ) 6= 
s  S(pat+1
k
k
t
t (Y
k
a

a

|

Prop1

{z

= constant , independent of

=

bt+1 (s0k ) 
P k t+1 00
s00 bk (sk ) 
k

=

bt+1 (s0k )
P k t+1 00
s00 bk (sk )
k

0
= bt+1
k (sk ).

1174

}
s0k

fiExploiting Causality for Selective Belief Filtering in DBNs

Appendix C. Mixture of Gaussians
Algorithm 4 provides a simple procedure that randomly generates a mixture of Gaussians
(i.e. a set of normal distributions) for the synthetic processes in Section 6.1. The algorithm
takes as input the number n of state variables and returns a set G of Gaussians whose means
are in the set {1, ..., n}. The number of Gaussians, their means, and their variances are
chosen automatically so as to achieve good coverage of state variables while minimising
the (visual) overlap of Gaussians. See Figure 7 for an example.

Algorithm 4 MixtureOfGaussians(n)
1:

Input: number of state variables n

2:

Parameters:   4, min  5 , max 

3:

Output: mixture of Gaussians G

4:

G

5:

R  {(1, ..., n)}

6:

while R 6=  do

n
10

7:

R  next element of R

8:

R  R \ {R}

9:

  R(drand  |R|e) // rand returns random number from (0, 1)

10:

  1 min[  R(1), R(|R|)  ]

11:
12:

  min[max , max[min , rand  ]]

	
G  G  (,  2 ) // mean and variance of Gaussian

13:

R  (R(1), R(2), ..., R(p)) such that R(p) <   

14:

R+  (R(q), R(q + 1), ..., R(|R|)) such that R(q) >  + 

15:

if R 6=  then

16:
17:
18:
19:

R  R  {R }
if R+ 6=  then

R  R  {R+ }
return G

1175

fiAlbrecht & Ramamoorthy

References
Astrom, K. (1965). Optimal control of Markov processes with incomplete state information.
Journal of Mathematical Analysis and Applications, 10, 174205.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: structural assumptions and computational leverage. Journal of Artificial Intelligence Research, 11 (1),
194.
Boutilier, C., Friedman, N., Goldszmidt, M., & Koller, D. (1996). Context-specific independence in Bayesian networks. In Proceedings of the 12th Conference on Uncertainty in
Artificial Intelligence, pp. 115123.
Boyen, X., & Koller, D. (1998). Tractable inference for complex stochastic processes. In
Proceedings of the 14th Conference on Uncertainty in Artificial Intelligence, pp. 3342.
Boyen, X., & Koller, D. (1999). Exploiting the architecture of dynamic systems. In Proceedings
of the 16th National Conference on Artificial Intelligence, pp. 313320.
Brafman, R. (1997). A heuristic variable grid solution method for POMDPs. In Proceedings
of the 14th National Conference on Artificial Intelligence, pp. 727733.
DAndrea, R., & Wurman, P. (2008). Future challenges of coordinating hundreds of autonomous vehicles in distribution facilities. In Proceedings of the IEEE International
Conference on Technologies for Practical Robot Applications, pp. 8083.
Dean, T., & Kanazawa, K. (1989). A model for reasoning about persistence and causation.
Computational Intelligence, 5, 142150.
Dias, M., Zlot, R., Kalra, N., & Stentz, A. (2006). Market-based multirobot coordination: a
survey and analysis. Proceedings of the IEEE, 94 (7), 12571270.
Doucet, A., de Freitas, N., & Gordon, N. (2001). Sequential Monte Carlo Methods in Practice.
Springer Science & Business Media.
Doucet, A., De Freitas, N., Murphy, K., & Russell, S. (2000). Rao-Blackwellised particle
filtering for dynamic Bayesian networks. In Proceedings of the 16th Conference on
Uncertainty in Artificial Intelligence, pp. 176183.
Geiger, D., Verma, T., & Pearl, J. (1989). d-separation: from theorems to algorithms. In
Proceedings of the 5th Conference on Uncertainty in Artificial Intelligence, pp. 139
148.
Gordon, N., Salmond, D., & Smith, A. (1993). Novel approach to nonlinear/non-Gaussian
Bayesian state estimation. In IEE Proceedings F (Radar and Signal Processing), Vol.
140, pp. 107113.
Hart, P., Nilsson, N., & Raphael, B. (1968). A formal basis for the heuristic determination
of minimum cost paths. In IEEE Transactions on Systems Science and Cybernetics,
Vol. 4, pp. 100107.
Hauskrecht, M. (2000). Value-function approximations for partially observable Markov
decision processes. Journal of Artificial Intelligence Research, 13, 3394.
1176

fiExploiting Causality for Selective Belief Filtering in DBNs

Heckerman, D. (1993). Causal independence for knowledge acquisition and inference. In
Proceedings of the 9th Conference on Uncertainty in Artificial Intelligence, pp. 122
127.
Heckerman, D., & Breese, J. (1994). A new look at causal independence. In Proceedings of
the 10th Conference on Uncertainty in Artificial Intelligence, pp. 286292.
Kaelbling, L., Littman, M., & Cassandra, A. (1998). Planning and acting in partially
observable stochastic domains. Artificial intelligence, 101 (1), 99134.
Koller, D., & Friedman, N. (2009). Probabilistic Graphical Models: Principles and Techniques.
The MIT Press.
Kullback, S., & Leibler, R. (1951). On information and sufficiency. The Annals of Mathematical Statistics, 22 (1), 7986.
Lauritzen, S., & Spiegelhalter, D. (1988). Local computations with probabilities on graphical
structures and their application to expert systems. Journal of the Royal Statistical
Society. Series B (Methodological), 50 (2), 157224.
Lovejoy, W. (1991). Computationally feasible bounds for partially observed Markov decision
processes. Operations Research, 39, 162175.
Mainzer, K. (2010). Causality in natural, technical, and social systems. European Review,
18, 433454.
Minka, T., Winn, J., Guiver, J., & Knowles, D. (2012). Infer.NET 2.5.. Microsoft Research
Cambridge. http://research.microsoft.com/infernet.
Murphy, K. (2001). The Bayes net toolbox for Matlab. Computing Science and Statistics,
33 (2), 10241034. https://code.google.com/p/bnt/.
Murphy, K., & Weiss, Y. (2001). The factored frontier algorithm for approximate inference in
DBNs. In Proceedings of the 17th Conference on Uncertainty in Artificial Intelligence,
pp. 378385.
Murphy, K. (2002). Dynamic Bayesian Networks: Representation, Inference and Learning.
Ph.D. thesis, University of California, Berkeley.
Ng, B., Peshkin, L., & Pfeffer, A. (2002). Factored particles for scalable monitoring. In
Proceedings of the 18th Conference on Uncertainty in Artificial Intelligence, pp. 370
377.
Pasula, H., Zettlemoyer, L., & Kaelbling, L. (2007). Learning symbolic models of stochastic
domains. Journal of Artificial Intelligence Research, 29, 309352.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann.
Pearl, J. (2000). Causality: Models, Reasoning, and Inference. Cambridge University Press.
Pineau, J., Gordon, G., & Thrun, S. (2003). Point-based value iteration: an anytime algorithm
for POMDPs. In Proceedings of the 18th International Joint Conference on Artificial
Intelligence, Vol. 18, pp. 10251032.
Poole, D., & Zhang, N. (2003). Exploiting contextual independence in probabilistic inference.
Journal of Artificial Intelligence Research, 18, 263313.
1177

fiAlbrecht & Ramamoorthy

Poupart, P., & Boutilier, C. (2000). Value-directed belief state approximation for POMDPs.
In Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence, pp.
497506.
Poupart, P., & Boutilier, C. (2001). Vector-space analysis of belief-state approximation
for POMDPs. In Proceedings of the 17th Conference on Uncertainty in Artificial
Intelligence, pp. 445452.
Poupart, P., & Boutilier, C. (2002). Value-directed compression of POMDPs. In Advances
in Neural Information Processing Systems, pp. 15471554.
Roy, N., Gordon, G., & Thrun, S. (2005). Finding approximate POMDP solutions through
belief compression. Journal of Artificial Intelligence Research, 23, 140.
Smith, T., & Simmons, R. (2005). Point-based POMDP algorithms: improved analysis and
implementation. In Proceedings of the 21st Conference on Uncertainty in Artificial
Intelligence, pp. 542549.
Sondik, E. (1971). The Optimal Control of Partially Observable Markov Processes. Ph.D.
thesis, Stanford University.
Srinivas, S. (1993). A generalization of noisy-or model. In Proceedings of the 9th Conference
on Uncertainty in Artificial Intelligence, pp. 208215.
Washington, R. (1997). BI-POMDP: bounded, incremental partially-observable Markovmodel planning. In Recent Advances in AI Planning, pp. 440451. Springer.
Wolpert, D., & Macready, W. (1995). No free lunch theorems for search. Tech. rep. SFI-TR95-02-010, Santa Fe Institute.
Wolpert, D., & Macready, W. (1997). No free lunch theorems for optimization. IEEE
Transactions on Evolutionary Computation, 1 (1), 6782.
Wurman, P., DAndrea, R., & Mountz, M. (2008). Coordinating hundreds of cooperative,
autonomous vehicles in warehouses. AI Magazine, 29 (1), 9.
Zhang, N., & Poole, D. (1996). Exploiting causal independence in Bayesian network inference.
Journal of Artificial Intelligence Research, 5, 301328.
Zhou, R., & Hansen, E. (2001). An improved grid-based approximation algorithm for
POMDPs. In Proceedings of the 17th International Joint Conference on Artificial
Intelligence, pp. 707716.

1178

fiJournal of Artificial Intelligence Research 55 (2016) 653-683

Submitted 06/15; published 03/16

Exact Algorithms for MRE Inference
Xiaoyuan Zhu
Changhe Yuan

XIAOYUAN . ZHU @ QC . CUNY. EDU
CHANGHE . YUAN @ QC . CUNY. EDU

Queens College, City University of New York
65-30 Kissena Blvd., Queens, NY 11367

Abstract
Most Relevant Explanation (MRE) is an inference task in Bayesian networks that finds the most
relevant partial instantiation of target variables as an explanation for given evidence by maximizing
the Generalized Bayes Factor (GBF). No exact MRE algorithm has been developed previously
except exhaustive search. This paper fills the void by introducing two Breadth-First Branch-andBound (BFBnB) algorithms for solving MRE based on novel upper bounds of GBF. One upper
bound is created by decomposing the computation of GBF using a target blanket decomposition
of evidence variables. The other upper bound improves the first bound in two ways. One is to
split the target blankets that are too large by converting auxiliary nodes into pseudo-targets so as
to scale to large problems. The other is to perform summations instead of maximizations on some
of the target variables in each target blanket. Our empirical evaluations show that the proposed
BFBnB algorithms make exact MRE inference tractable in Bayesian networks that could not be
solved previously.

1. Introduction
Bayesian networks are probabilistic models that capture the conditional independencies between
random variables as directed acyclic graphs, and provide principled approaches to scientific explanation. Explanation tasks in Bayesian networks can be classified into three categories: explanation
of reasoning, explanation of model, and explanation of evidence (Lacave & Diez, 2002). The goal
of explanation of reasoning in Bayesian networks is to explain the reasoning process used to produce the results so that the credibility of the results can be established. The goal of explanation of
model is to present the knowledge encoded in a Bayesian network in easily understandable forms
such as visual aids so that experts or users can examine or even update the knowledge. The goal
of explanation of evidence is to explain why some observed variables are in their particular states
using the other variables in the domain.
This research focuses on developing algorithms for solving one of the methods for explaining
evidence in Bayesian networks, the Most Relevant Explanation (MRE) (Yuan & Lu, 2007; Yuan,
Lim, & Lu, 2011b). The idea of MRE is to find a partial instantiation of the target variables that
maximizes the Generalized Bayes Factor (GBF) (Fitelson, 2007; Good, 1985) as an explanation for
the evidence. GBF is a rational function of probabilities that is suitable for comparing explanations
with different cardinalities. MRE is shown both theoretically and empirically to be able to prune
away independent and less relevant variables from the final explanation (Yuan et al., 2011b; Yuan,
Liu, Lu, & Lim, 2009; Pacer, Lombrozo, Griffiths, Williams, & Chen, 2013).
Due to the difficulty of finding a meaningful upper bound for GBF, no exact algorithms have
been developed to solve MRE except exhaustive search. Only local search and Markov chain Monte
Carlo methods have been proposed previously (Yuan et al., 2009; Yuan, Lim, & Littman, 2011a). In
c
2016
AI Access Foundation. All rights reserved.

fiZ HU & Y UAN

this paper, we introduce the first non-trivial exact MRE algorithms based on Breadth-First Branchand-Bound (BFBnB) search. The key idea of the proposed methods is to decompose the whole
Bayesian network into a set of overlapping subnetworks using a target blanket decomposition of
evidence variables. Each subnetwork is characterized by a subset of evidence variables and its target
blanket which d-separates the evidence variables from other target and evidence variables. An upper
bound for GBF is derived by solving independent optimization problems in the subnetworks. We
also show that the bound can be tightened by merging the target blankets that share target variables.
Then we propose another improved upper bound based on two novel ideas. First, the above
decomposition may lead to large target blankets that prevent the branch-and-bound algorithm from
scaling to large MRE problems. To address this problem, we propose to split large target blankets by
converting auxiliary nodes into pseudo-targets which introduce additional decomposition. Second,
we find that the upper bound can be tightened by identifying and summing out enclosed-targets from
each target blanket. The proposed upper bounds are used in the BFBnB algorithms for pruning the
search space. We evaluated the algorithms in a set of benchmark diagnostic Bayesian networks.
Experimental results show that the proposed algorithms make exact MRE inference tractable in
Bayesian networks that could not be solved previously.
The rest of the paper is organized as follows. The basics of Bayesian networks and the MRE
problem are introduced in Section 2. In Section 3, a novel target blanket upper bound is proposed.
An improved upper bound is discussed in Section 4. The proposed BFBnB algorithms are introduced in Section 5. In Section 6, the experimental results are presented. Finally, discussions and
conclusions are provided in Section 7.

2. Background
In this section, we introduce the basics of Bayesian networks, explanation in Bayesian networks,
and the MRE problem.
2.1 Bayesian Networks and Moral Graph
A Bayesian network (Pearl, 1988; Darwiche, 2009; Koller & Friedman, 2009) is represented as a
directed acyclic graph (DAG). The nodes in the DAG represent random variables. The lack of arcs
in the DAG define conditional independence relations among the nodes. If there is an arc from
node Y to X, we say that Y is a parent of X, and X is a child of Y . We use upper-case letters
to denote variables X or variable sets X, and lower-case letters for values of scalars x or vectors
x. A node Y is an ancestor of a node X if there is a directed path from Y to X. Let AN (X)
denote all the ancestors of X, then the smallest ancestral set AN (X) of node set X is defined
as AN (X) = X  (Xi X AN (Xi )). In directed graphs, d-separation describes the conditional
independence relation between two sets of nodes X and Y, given a third set of nodes Z, i.e.,
p(X|Z, Y) = p(X|Z). The Markov blanket of X is the smallest node set which d-separates X
from the remaining
Q nodes in the network. The network as a whole represents the joint probability
distribution of X p(X|PA(X)), where PA(X) is the set of all the parents of X.
The moral graph Gm of a DAG G is an undirected graph with the same set of nodes. There
is an edge between X and Y in Gm if and only if there is an edge between them in G or if they
are parents of the same node in G. In an undirected graph, Z separates X and Y, if Z intercepts
all paths between X and Y. Moral graphs are a powerful construction to explain d-separation.
654

fiE XACT A LGORITHMS FOR MRE I NFERENCE

Lemma 1 (Lauritzen, Dawid, Larsen, & Leimer., 1990) links d-separation in DAG to separation in
undirected graphs.
Lemma 1. Let X, Y, and Z be disjoint subsets of nodes in a DAG G. Then Z d-separates X from Y
if and only if Z separates X from Y in (GAN (XYZ) )m , where (GAN (XYZ) )m is the moral graph
of the subgraph of G with node set AN (X  Y  Z).
2.2 Explanation of Evidence in Bayesian Networks
Numerous methods have been developed to explain evidence in Bayesian networks. Some of these
methods make simplifying assumptions and focus on singleton explanations (Heckerman, Breese,
& Rommelse, 1995; Jensen & Liang, 1994; Kalagnanam & Henrion, 1988). However, singleton
explanations may be underspecified and are unable to fully explain the given evidence if the evidence
is the compound effect of multiple causes.
For a domain with multiple interdependent target variables, multivariate explanations are often
more appropriate for explaining the given evidence. Maximum a Posteriori assignment (MAP) finds
a complete instantiation of a set of target variables that maximizes the joint posterior probability
given partial evidence on the other variables. Recently Kwishthout (2013) extended MAP to find a
set of joint assignments as most inforbable explanations. Most Probable Explanation (MPE) (Pearl,
1988) is similar to MAP except that MPE defines the target variables to be all unobserved variables.
The common drawback of these methods is that they often produce hypotheses that are overspecified
and may contain irrelevant variables in explaining the given evidence.
Everyday explanations are necessarily partial explanations (Leake, 1995). Various pruning techniques have been used to avoid overly complex explanations. These methods can be grouped into
two categories: pre-pruning and post-pruning. Pre-pruning methods use the context-specific independence relations represented in Bayesian networks to prune irrelevant variables (Pearl, 1988;
Shimony, 1993; van der Gaag & Wessels, 1993, 1995) before applying methods such as MAP to
generate explanations. In contrast, post-pruning methods first generate explanations using methods
such as MAP or MPE and then prune variables that are not important. An example is the method
proposed by de Campos et al. (2001).
Several methods aim to directly find more appropriate explanations. The likelihood of evidence
is used to measure the explanatory power of an explanation (Gardenfors, 1988). Chajewska and
Halpern (1997) extend the approach further to use the value pair of <likelihood, prior probability>
to order the explanations, forcing users to make decisions if there is no clear order between two
explanations. Henrion and Druzdzel (1991) assume that a system has a set of pre-defined explanation scenarios organized as a tree; they use the scenario with the highest posterior probability as
the explanation. Flores et al. (2005) propose to automatically create an explanation tree by greedily
branching on the most informative variable at each step while maintaining the probability of each
branch of the tree above a certain threshold. Nielsen et al. developed another method that uses the
causal information flow (Ay & Polani, 2008) to select variables to expand an explanation tree.
2.3 Most Relevant Explanation
For explanation of evidence in Bayesian networks, we often classify the nodes into three categories:
target, evidence, and auxiliary. The target set M represents variables of interest in the inference.
The evidence set E represents observed information. The auxiliary set represents the variables that
655

fiZ HU & Y UAN

are not of interest in the inference. MRE, which finds a partial instantiation of M as an explanation
for given evidence e in a Bayesian network, is formally defined as follows (Yuan et al., 2011b).
Definition 1. Let M be a set of targets, and e be the given evidence in a Bayesian network. Most
Relevant Explanation is the problem of finding a partial instantiation x of M that has the maximum
generalized Bayes factor score GBF (x; e) as the explanation for e, i.e.,
MRE (M; e) = argmax GBF (x; e),

(1)

x,XM

with GBF defined as
GBF (x; e) =

p(e|x) 1
,
p(e|x)

(2)

where x is the joint value assignment (instantiation) of a subset X of M, and x represents all
alternative explanations of x.
A commonly used measure for selecting explanatory hypothesis is the probability of an explanation given the evidence, as used in MAP and MPE to find the most likely configuration of a set of
target variables. Probability-based methods, however, do not have the intrinsic capability to prune
less relevant facts. Moreover, the probability measure is quite sensitive to the modeling choices,
e.g., simply refining a model can dramatically change the best explanation. In contrast, MRE maximizes the rational function of probabilities GBF in Equation 2. This makes it possible to compare
explanations with different cardinalities and to prune the less relevant variables automatically in a
principled way. The search space of MRE is exponential in the number of targets, and the complexity of MRE is conjectured to N P P P -hard (Yuan et al., 2011a), which makes the naive brute force
algorithm impractical.
To further study the properties of generalized Bayes factor, we reformulate GBF as follows.
GBF (x; e) =
=

p(e|x)
p(x|e)p(x)
=
p(e|x)
p(x)p(x|e)
p(x|e)(1  p(x))
.
p(x)(1  p(x|e))

(3)

Therefore, we only need the prior and posterior probabilities of a hypothesis in order to compute
its GBF. GBF is hence able to overcome the drawback of Bayes factor (Jeffreys, 1961) in having to
do pairwise comparisons between multiple hypotheses.
Belief update ratio is a useful concept. The belief update ratio of X given e is defined as
follows (Yuan et al., 2011b).
p(X|e)
.
(4)
r(X; e) =
p(X)
GBF can be calculated from the belief update ratio as follows.
p(x|e)(1  p(x))
r(x; e)  p(x|e)
=
p(x)(1  p(x|e))
1  p(x|e)
r(x; e)  1
= 1+
.
1  p(x|e)

GBF (x; e) =

1. We use p(x) as a shorthand for p(X = x) in this paper.

656

(5)

fiE XACT A LGORITHMS FOR MRE I NFERENCE

2.4 Existing Methods for Solving MRE
Local search methods such as tabu search (Glover, 1990) have been applied to solve MRE (Yuan
et al., 2011a). Tabu search starts with the empty solution set. At each step, it generates the neighbors
of the current solution by adding, changing, or deleting one target variable. Then tabu search selects
the best neighbor which has the highest GBF score and has not been visited before. In tabu search,
the best neighbor can be worse than the current solution. To stop tabu search properly, upper bounds
are set on both the total number of search steps M and the number of search steps since the last
improvement L as the stopping criteria. Another Markov Chain Monte Carlo algorithm integrates
the reversible-jump MCMC algorithm (Green, 1995) and simulated annealing (Kirkpatrick, Gelatt,
& Vecchi, 1983) to find a solution by simulating a non-homogeneous Markov chain that eventually
concentrates its mass on the mode of a distribution of the GBF scores of all solutions. These methods
can only provide approximate solutions whose quality are unknown. Furthermore, the accuracy and
efficiency of these methods are typically highly sensitive to tunable parameters.
2.5 Branch and Bound Algorithms for Solving MPE/MAP
Branch-and-bound algorithms have been developed for solving MAP and MPE by using upper
bounds derived based on the property of optimization criterion or the structure of Bayesian networks. A mini-bucket upper bound was used in an AND/OR tree search for solving MPE (Dechter
& Rish, 2003; Marinescu & Dechter, 2009). Recently, an improved mini-bucket upper bound (Marinescu, Dechter, & Ihler, 2014) was proposed to guide AND/OR search for exact MAP inference.
The work in (Choi, Chavira, & Darwiche, 2007) showed that the mini-bucket upper bound can be
derived from a node splitting scheme. To solve MAP exactly, an upper bound is proposed by commuting the order of max and sum operations in the MAP calculation (Park & Darwiche, 2003). An
exact algorithm is proposed for solving MAP by computing upper bounds on an arithmetic circuit
compiled from a Bayesian network (Huang, Chavira, & Darwiche, 2006).

3. A Novel Upper Bound Based on Target Blanket Decomposition for Solving MRE
It is difficult to solve MRE problems exactly because of both an exponential search space and the
need for probabilistic inference at each search step. A naive brute-force search method can scale to
Bayesian networks with at most 15 targets. In this work, we develop breadth-first branch-and-bound
algorithms that use a suite of new upper bounds based on target blanket decomposition to prune the
search space. The algorithm makes it possible to solve MRE problems with more targets exactly.
3.1 Search Space Formulation
Assuming there are n targets, and each target has d states, the search space of MRE contains (d +
1)n  1 possible states (or solutions). We organize the search space as a search tree by instantiating
the targets according to a total order  of the targets. Each state S in the tree contains values of a
subset of the targets V. V is defined as the expanded set of targets. The set of variables U that are
yet to be considered for expansion is defined as unexpanded set; i.e., U = {Ui |Ui  M; Vj 
V, Vj < Ui }; and the set of variables P that have been considered but are not used is defined as
pruned set; i.e., P = M \ {V  U}. Figure 1 demonstrates the different target sets for the state
{X1 , X4 , X6 } for a 9-target MRE problem.
657

fiZ HU & Y UAN

Pruned (P)

X1

X3

X2

X5

X4

X6 X7

Expanded (V)

X8

X9

Unexpanded (U)

Figure 1: Different types of targets of a search state in MRE, i.e., expanded (gray), pruned (white),
and unexpanded (black).



a

ab

ab

a

ac

b

ac

ab

b

ab

ac

c

ac

bc

c

bc

bc

bc

abc abc abc abc abc abc abc abc

Figure 2: The search tree of an MRE problem with three targets, i.e., A, B, and C. An example of
sub-tree rooted at {b} is marked as gray.

The search tree has the empty state as the root. Each non-leaf state in the tree has a number of
children states that instantiate one more unexpanded target. Figure 2 shows an example search tree
with three targets A = {a, a}, B = {b, b}, and C = {c, c} with  in the same order. Different
branches of the search tree may have different numbers of layers, but all of the states with the same
cardinality appear in the same layer.
It is possible to use dynamic ordering to expand targets that can most improve the GBF score
first. However, it was shown that a static ordering can actually make computing upper bounds much
faster and ultimately make the search more efficient (Yuan & Hansen, 2009). We therefore simply
ordered the targets according to their indices in this work.
3.2 An Upper Bound Based on Target Blanket Decomposition
For MRE inference, an upper bound of a state S should be greater than the GBF score of all descendant states of S. During search, we can keep track of the highest-scoring state and prune the
whole subtree if the upper bound is less than the GBF of the current best solution. In the following
we introduce a novel upper bound for MRE inference.
We first define a concept called target blanket decomposition of evidence variables.
Definition 2. A target blanket decomposition of evidence variables is a tuple < EvdList, TBList >
that satisfies the following properties: 1) EvdList is a set of exclusive and exhaustive subsets,
i.e.,EvdList = {Ei }, E = i Ei and Ei  Ej =  for i 6= j. 2) TBList is a set of target blankets,
one blanket TB (Ei ) for each evidence subset Ei , such that TB (Ei ) is the minimal set of targets
which d-separates Ei from other targets and evidence variables.
658

fiE XACT A LGORITHMS FOR MRE I NFERENCE

The target blanket decomposition naturally decomposes the whole network into overlapping
subnetworks, with each subnetwork containing an evidence subset Ei and its target blanket TB (Ei ).
Target blankets are so named due to their resemblance to Markov blankets, but strictly speaking,
they are not really the Markov blankets of the evidence variables, because there may be auxiliary
variables between them.
For any given target set M and evidence set E, there may exist multiple target blanket decompositions that satisfy Definition 2. To see that, note that we can simply merge two or more evidence
subsets and their corresponding target blankets in a decomposition to obtain another decomposition. However, among all possible decompositions, there is a minimal target blanket decomposition,
which we define as follows.
Definition 3. A minimal target blanket decomposition of evidence variables is a target blanket
decomposition < EvdList, TBList > in which no proper subset of any Ei in EvdList has a valid
target blanket.
The minimal target blanket decomposition must be unique according to the following theorem.
Theorem 1. The minimal target blanket decomposition of evidence set E is unique.
Proof. Proof by contradiction: assume there are two minimal target blanket decompositions D1
and D2 . Based on the property of minimal target blanket decomposition, no evidence subset of D1
can be a proper subset of any evidence subset of D2 , and vice versa. There must exist two distinct
1
evidence subsets ED
of D1 and EjD2 of D2 whose overlap Eij is not empty. But then it must
i
be true that Eij is d-separated from all other target and evidence variables given some subset of
2
TB (EiD1 )  TB (ED
j ), contradicting the definition.
It immediately follows that all non-minimal target blanket decompositions can be generated by
performing merging on the minimal decomposition.
An upper bound on GBF is then derived by multiplying the upper bounds on belief update ratios
calculated on the subnetworks. We first derive an upper bound on the belief update ratio in the
following theorem.
Theorem 2. Let M = {X1 , X2 , . . . , Xn } be the set of targets, e be the evidence, and < EvdList,
TBList > be a target blanket decomposition of E, where EvdList = {Ei } and TBList =
{TB (Ei )}. Then, for any subset   X  M, the belief update ratio r(x; e) is upper bounded
as follows:
!
max
x,XM

where C =

Q

i p(ei )

r(x; e) 

Y
i

max
z,Z=TB (Ei )


p(e).

659

r(z; ei )

 C,

(6)

fiZ HU & Y UAN

Proof.

From the formulation of r(M; e), we have

r(M; e) = p(M|e) p(M)

= p(e|M) p(e)
Y
=
p(ei |TB (Ei ))/p(e)
i

Y p(TB (Ei )|ei )p(ei )

=

p(TB (Ei ))

i

/p(e)
!

Y

=

r(TB (Ei ); ei )p(ei ) /p(e).

(7)

i

The third equality is based on the property of target blankets. Thus, we have
!
Y
r(M; e) =
r(TB (Ei ); ei )  C,

(8)

i

where C =
r(m; e),

Q


i p(ei ) p(e). From Equation 8, we immediately have the following upper bound on
!
max r(m; e) 
m

Y
i

max
z,Z=TB (Ei )

 C.

r(z; ei )

(9)

For any   X  M, X = M\X, let Si = TB (Ei )  X denote the subset of targets to
be summed out in the ith subnetwork, and Si = TB (Ei )  X. Thus, we separate TB (Ei ) into
two parts, i.e., related or unrelated to the summation on X, indexed by I = {i : Si 6= } and
J = {j : Sj = } respectively. Based on the above definition, we have:
X
p(e|X) =
p(e|M)p(X|X)
X

!
X Y

=

iI

X

Y

p(ei |TB (Ei )) p(X|X) 

p(ej |TB (Ej ))

jJ

! 
Y



max p(ei |Si  si )

iI

si




Y

p(ej |TB (Ej )) .

(10)

jJ

The last inequality is derived by using maximization instead of averaging on X. Since p(e|X) =
r(X; e)p(e), we have the following upper bound on r(x; e) by performing maximization according
to X on both sides of Equation 10:
max r(x; e) = max
x

x



Y
iI

=

Y
iI

p(e|x)
p(e)
max max
si

si

max

p(ei |si  si )p(ei )
p(ei )
! 
Y
r(z; ei )  

z,Z=TB (Ei )

jJ

660

! 




Y

jJ

max

p(ej |z)p(ej )  1

p(ej )
p(e)


z,Z=TB (Ej )

max
z,Z=TB (Ej )

r(z; ej )  C.

(11)

fiE XACT A LGORITHMS FOR MRE I NFERENCE

Thus, for any   X  M, we obtain the final upper bound by combining Equations 9 and 11.
!
Y
max r(x; e) 
max r(z; ei )  C.
x,XM

i

z,Z=TB (Ei )

In MRE inference, the evidence e is given, thus C is a constant. Theorem 2 assumes that V = ,
which is true at the beginning of search. During the search when V 6= , we have the following
corollary to derive the upper bound on the belief update ratio.
Corollary 3. Let M = {X1 , X2 , . . . , Xn } be the set of targets, e be the evidence, and < EvdList,
TBList > be a target blanket decomposition of E, where EvdList = {Ei } and TBList =
{TB (Ei )}. Let U and V be the unexpanded and expanded target sets of state S. Let Ti =
TB (Ei )  V, and Ti = TB (Ei )\Ti . Then, for any subset   X  U, the belief update ratio
r(x  v; e) is upper bounded as follows.
!
Y
max r(x  v; e) 
max r(z  ti ; ei )  C,
(12)
x,XU

where C =

Q

i p(ei )

i

z,Z=Ti


p(e).

Based on Corollary 3, we can derive an upper bound on GBF in Theorem 4:
Theorem 4. Let M = {X1 , X2 , . . . , Xn } be the set of targets, e be the evidence, and < EvdList,
TBList > be a target blanket decomposition of E, where EvdList = {Ei } and TBList =
{TB (Ei )}. Let U and V be the unexpanded and expanded target sets of state S. Let Ti =
TB (Ei )  V, and Ti = TB (Ei )\Ti . Then, for any subset   X  U, the generalized Bayesian
factor score GBF (x  v; e) is upper bounded as follows.
Y
max r(z  ti ; ei )  C  1
GBF (x  v; e)  1 +

max

z,Z=Ti

i

,

1  p(v|e)

x,XU

(13)


Q
where C = i p(ei ) p(e).
Proof. First, we formulate GBF using the belief update ratio as in Equation 5.
GBF (m; e) = 1 +

r(m; e)  1
.
1  p(m|e)

For any subset   X  U, p(x  v|e) = p(x|v, e)  p(v|e)  p(v|e). Thus we have
r(x  v; e)  1

max
max

GBF (x  v; e)  1 +

x,XU

x,XU

.

1  p(v|e)

(14)

Then using Corollary 3, we obtain the following upper bound on GBF:
!
Y
max

GBF (x  v; e)  1 +

x,XU

where C =

Q

i p(ei )


p(e).

661

i

max r(z  ti ; ei )

z,Z=Ti

1  p(v|e)

C 1
,

fiZ HU & Y UAN

Moral graph

Directed subgraph

Splitted graph

TB(E2)

TB(E1,E2)
A

G
I
H
J

E1

E2
F
E3

K

(A)

B

G

C

I

D

H

L

J

A
E1

E2
F
E3

K

B

G

C

I

D

H

L

J

TB(E1)
B

A
E1

E2
F
E3

D
K

TB(E3)

TB(E3)

(B)

(C)

C

L

Figure 3: An example of compiling and splitting target blanket decomposition. E1 , E2 , and E3 are
evidence nodes. Gray nodes are targets. Others are auxiliary nodes. The original target
blanket TB (E1 , E2 ) is split into TB (E1 ) and TB (E2 ) by converting auxiliary node A
into pseudo-target.

Using Equation 13, we can bound all the descendant states of the current state S. Equation 13
shows that in an MRE problem (left), we need to search all of the subsets of targets M to find the
best solution. However, to calculate an upper bound (right), we only need to search a fixed target
set TB (Ei ) of each subnetwork, which usually has a small size and is easy to compute. For each
instantiation z of TB (Ei ), we calculate r(z; ei ) and store it in a table called belief ratio table. There
is one such table for each subnetwork. These tables are computed in a preprocessing step and are
looked up at each step of the search to compute the upper bound of a state.
3.3 Compiling Minimal Target Blanket Decomposition
Theorems 2- 4 are based on factorizing
the conditional joint distribution p(e|M) into the product of
Q
a set of conditional distributions i p(ei |TB (Ei )). Thus finding the target blanket decomposition,
including the evidence subsets and their target blankets, is a key part of the proposed methods.
In the proposed method, we compile the minimal target blanket decomposition based on Lemma 1
by first compiling a moral graph. The moral graph is used to prune out irrelevant parts of the network
and set up the network for separation tests. To compile moral graph, we first generate the smallest
ancestral set containing the target set M and the evidence set E, i.e., AN (ME). Then we compile
the moral graph (GAN (ME) )m . Figure 3(B) illustrates an example moral graph compiled from the
Bayesian network with three evidence nodes in Figure 3(A). Using Lemma 1, minimal target blanket decomposition of evidence variables is achieved by doing a depth first graph traversal starting
from each unvisited evidence node in the moral graph (GAN (ME) )m . There are three scenarios
when a node is being visited.
 Case 1: When an evidence node is visited, we add the evidence to the current evidence subset
Ei , mark it as visited, and continue to visit its unmarked neighbors.
 Case 2: When a target is visited, we add the target to TB (Ei ) and mark it as visited.
662

fiE XACT A LGORITHMS FOR MRE I NFERENCE

Algorithm 1 Compiling Minimal Target Blanket Decomposition
Input: M  target set; E  evidence set; MGraph  moral graph (GAN (ME) )m .
Output: Target blanket decomposition < EvdList, TBList >, where EvdList = {Ei }, and
TBList = {TB (Ei )}.
1: function C OMPILE M INIMALTARGET B LANKETS(M, E, MGraph)
2:
EvdList  ;
3:
TBList  ;
4:
for each node Nd in MGraph do
5:
if Nd in E and Nd is not visited then
6:
EvdSeti  ;
7:
TBSeti  ;
8:
SearchStack  Nd ;
. initialize the stack for Depth-first search
9:
while SearchStack is not empty do
10:
SearchNode  SearchStack .pop();
11:
if SearchNode is visited then
12:
continue;
13:
end if
14:
mark SearchNode as visited;
15:
if SearchNode in E then
. Case 1
16:
EvdSeti .push(SearchNode);
17:
SearchStack .push(SearchNodes unvisited neighbors);
18:
else if SearchNode in M then
. Case 2
19:
TBSeti .push(SearchNode);
20:
else if SearchNode is auxiliary node then
. Case 3
21:
SearchStack .push(SearchNodes unvisited neighbors);
22:
end if
23:
end while
24:
EvdList.push(EvdSeti );
25:
TBList.push(TBSeti );
26:
end if
27:
mark the targets and auxiliary nodes in MGraph as unvisited;
28:
end for
29: end function
 Case 3: When an auxiliary node is visited, we mark it as visited and continue to visit its
unmarked neighbors.
When restarting the search on a new evidence node, we unmark all targets and auxiliary nodes,
because the same targets may occur in different target blankets, e.g., node F is shared by TB (E1 , E2 )
and TB (E3 ) as shown in Figure 3(B). The algorithm stops when all evidence nodes have been visited. The above algorithm is summarized in Algorithm 1. Furthermore, we show that Algorithm 1
is guaranteed to find the minimal target blanket decomposition of evidence set E.
Theorem 5. Algorithm 1 is guaranteed to find the minimal target blanket decomposition of evidence
variables.
663

fiZ HU & Y UAN

Proof. For each evidence E, Algorithm 1 stops a search path if and only if a target is encountered.
All evidence variables that belong to the same evidence subset Ei as E in the minimal target blanket
decomposition must be visited before all search paths terminated. Furthermore, since we start the
search from one of the evidence E in Ei , it must be true that the search stops once the minimal
target blanket TB (Ei ) is fully visited.
3.4 Merging Target Blankets
When computing the upper bound, we maximize the belief update ratio r(TB (Ei ); ei ) in each
TB (Ei ) independently. Thus the common targets of two different target blankets TB (Ei ) and
TB (Ej ) may be set to inconsistent values. Too much inconsistency may result in a loose bound.
We can tighten the upper bound by merging target blankets that share targets. On the other hand,
if the number of targets in any individual target blanket is too large, it will make calculating the
belief update ratio tables inefficient or even infeasible due to excessive memory consumption. We
propose to merge target blankets that share targets under the constraint that the number of targets in
a resulting target blanket cannot exceed a constant K.
We can use an undirected graph to represent the problem of merging target blankets. The nodes
in the graph denote target blankets. If two target blankets share targets, there is an edge between the
two corresponding nodes. The weight of the edge is the number of targets shared by the two target
blankets. This formulation translates the problem of merging target blankets into a graph partition
problem. More specifically, the merging problem can be addressed by recursively solving the minimum bisection problem (Feige & Krauthgamer, 2002) on the undirected graph, which partitions the
vertices into two equal halves so as to minimize the sum of weights of the edges between the two
partitions.
The minimum bisection problem is an NP-hard problem, however. We cannot afford to spend
too much time on computing the upper bound. We therefore use a hierarchical clustering-like greedy
algorithm for merging the target blankets. We first merge any pair of target blankets if one of them
covers the other. Then for all remaining target blankets, we repeatedly merge two target blankets
that share the highest number of targets as long as the number of targets in the resulting target
blanket does not exceed K. The algorithm iterates the above two steps until no target blankets can
be merged. The merge algorithm is summarized in Algorithm 2.

4. An Improved Target Blanket Bound
The above target blanket upper bound has two potential difficulties in scaling to large Bayesian networks with many target variables. First, the minimal target blanket decomposition (in Section 3.3)
can lead to large subnetworks with belief ratio tables that are too large to build. Second, the upper
bound can be still loose even after merging target blankets. In this section, we propose another upper bound based on two new techniques for improving the scalability and tightness of the previous
bound.
4.1 Splitting Large Target Blankets
The decomposition method in Section 3.3 may lead to large target blankets which prevent the application of BFBnB to large scale MRE problems. To address this problem, we notice that in MRE
problems, each subnetwork contains three types of nodes, i.e., targets TB (Ei ), evidence Ei , and
664

fiE XACT A LGORITHMS FOR MRE I NFERENCE

Algorithm 2 Merging Target Blankets
Input: A target blanket decomposition < EvdList, TBList >, where EvdList = {Ei }, and
TBList = {TB (Ei )}; K  the maximum number of targets in a target blanket.
Output: An updated decomposition < EvdList, TBList >.
1: function M ERGE TARGET B LANKETS(EvdList, TBList, K)
2:
MergeFlag  True;
3:
while MergeFlag do
4:
TBListOld  TBList;
. TBi denotes TB (Ei )
5:
EvdListOld  EvdList;
6:
for each (TBi , TBj ) pair, where TBi , TBj  TBListOld do
7:
if TBi and TBj in TBList then
. merge target blankets with subset relation
8:
if TBi  TBj then
9:
TBj .merge(TBi );
10:
TBList.remove(TBi );
11:
Ej .merge(Ei );
12:
EvdList.remove(Ei );
13:
else if TBj  TBi then
14:
TBi .merge(TBj );
15:
TBList.remove(TBj );
16:
Ei .merge(Ej );
17:
EvdList.remove(Ej );
18:
end if
19:
end if
20:
end for
21:
NumTargets  0;
22:
for each (TBi , TBj ) pair, where TBi , TBj  TBList do
23:
if (TBi  TBj ).size() > NumTargets and (TBi  TBj ).size() < K then
24:
TBPair  (TBi , TBj );
. find an TB pair to merge
25:
NumTargets  (TBi  TBj ).size();
26:
EvdPair  (Ei , Ej );
27:
end if
28:
end for
29:
if NumTargets is 0 then
30:
MergeFlag  False;
31:
else
32:
TBPair .TBi .merge(TBPair .TBj );
33:
TBList.remove(TBPair .TBj );
34:
EvdPair .Ei .merge(EvdPair .Ej );
35:
EvdList.remove(EvdPair .Ej );
36:
end if
37:
end while
38: end function

665

fiZ HU & Y UAN

auxiliary nodes Ai , where TB (Ei ) marks the boundary of subnetworks, and Ei and Ai are conditionally independent from outside nodes given TB (Ei ). Our idea here is to split large target
blankets by converting auxiliary nodes into pseudo-targets which only act as targets in compiling
target blanket decomposition and calculating belief ratio tables. These pseudo-targets add additional d-separation to the Bayesian network, thus split the large target blankets into smaller ones.
Theorems 4 and 6 guarantee that, after the split, we can still find an upper bound on GBF.
Theorem 6. Let TB (Ei ) be the target blanket of the ith subset of evidence Ei , and Ai be the set
of auxiliary nodes in the ith subnetwork of a target blanket decomposition. Assuming that after
converting any subset   Y  Ai into pseudo-targets, the ith subnetwork is decomposed into a
set of target blankets TB (Eij ), where TB (Ei )  Y=j TB (Eij ), Ei =j Eij , and Eij  Eik = 
for j 6= k. Then, the belief update ratio r(x; ei ) is upper bounded as follows.
Y
max
r(x; ei ) 
max
r(z; eij )  C,
(15)
x,X=TB (Ei )

where C =

Q

j

j

z,Z=TB (Eij )


p(eij ) p(ei ).

Proof. Let X = TB (Ei ), for any   Y  Ai we have,
X
p(ei |X) =
p(ei |X  Y)p(Y|X)
Y

=

XY
Y



Y
j

p(eij |TB (Eij ))p(Y|X)

j

max
z,Z=TB (Eij )Y

p(eij |TB (Eij )\Z, z).

(16)

The second equality is based on the property of target blankets.
Since p(ei |X) = r(X; ei )p(ei ), we have the following upper bound on r(x; ei ),
Y
max
r(x; ei ) 
max
r(z; eij )  C,
x,X=TB (Ei )

where C =

Q

j

j

z,Z=TB (Eij )


p(eij ) p(ei ).

We introduce a greedy algorithm to convert auxiliary nodes into pseudo-targets incrementally
until the size of each resulting target blanket does not exceed K. In the algorithm, we split the target
blankets whose sizes exceed K using the following steps.
 Step 1: Calculate the degree (i.e., the number of neighboring nodes) of each auxiliary node
Aij in the moral graph, and sort them according to their degrees in descending order.
 Step 2: Convert one auxiliary node into pseudo-target according to its order, and perform
depth-first search in Section 3.3 to compile minimal target blankets decomposition on the
subnetwork.
 Step 3: Stop if the size of each resulting target blanket does not exceed K, or repeat Step 2
until all the auxiliary nodes have been converted.
666

fiE XACT A LGORITHMS FOR MRE I NFERENCE

Algorithm 3 Splitting Target Blankets
Input: A target blanket decomposition < EvdList, TBList >, where EvdList = {Ei }, and
TBList = {TB (Ei )}; K  the maximum number of targets in a target blanket. MGraph
 moral graph (GAN (ME) )m .
Output: An updated decomposition < EvdList, TBList >.
1: function S PLIT TARGET B LANKETS(EvdList, TBList, K, MGraph)
2:
for each TBi in TBList do
3:
if TBi .size() > K then
4:
for each Aij in Ai with descending degree do . Ai is the auxiliary node set in TBi
5:
convert Aij to pseudo-target;
6:
< EvdList i , TBList i > CompileMinimalTargetBlankets(TBi ,Ei ,MGraph);
7:
if TBij .size() < K, for any TBij in TBList i then
8:
break;
9:
end if
10:
end for
11:
end if
12:
end for
13: end function

The above algorithm is summarized in Algorithm 3. An example of splitting target blanket
is illustrated in Figure 3(C). The original target blanket TB ({E1 , E2 }) = {I, F, B, D} is split into
TB (E1 ) = {A, B, D} and TB (E2 ) = {A, F, I} by converting auxiliary node A into a pseudo-target.
4.2 Tightening the Upper Bound
By combining Theorems 4 and 6, we can see that the splitting process makes the upper bound loose
because of the maximizations on pseudo-targets. In order to maintain or even improve the search
efficiency, we need to re-tighten the bound. The key idea comes from Theorem 4, in which Ti
contains two types of variables, i.e., pruned-targets and unexpanded targets. In Equation 13, both
pruned-targets and unexpanded targets of state S are maximized to derive the upper bound. Since
the pruned-targets do not occur in the subsequently generated states, maximizing pruned-targets
makes the upper bound loose. However, directly summing the pruned-targets in the subnetworks
will change the structure of target blanket decomposition, thus makes the upper bound invalid.
We notice that certain targets can be summed out instead of being maximized over without
affecting the decomposition. First, we need to define another type of targets called enclosed-target
set H of a target blanket in the following.
Definition 4. In MRE inference, enclosed-targets H of a target blanket are the targets which are
conditionally independent from the variables outside the target blanket given the remaining variables in the target blanket.
In other words, enclosed-targets are the targets which are blocked (d-separated) from outside
by the rest of the target blanket. From Definition 4, we can see that each enclosed-target only
occurs in one target blanket. As an example in Figure 3(B), J and L are enclosed-targets in
TB (E3 )={F, J, L}.
667

fiZ HU & Y UAN

The idea is then to sum over the enclosed-targets in individual subnetworks TB (Ei ) in order to
produce a tighter upper bound. We have the following theorem.
Theorem 7. Let M = {X1 , X2 , . . . , Xn } be the set of targets, e be the evidence, and < EvdList,
TBList > be a target blanket decomposition of E, where EvdList = {Ei } and TBList =
{TB (Ei )}. Let U, V, and P be the unexpanded, expanded, and pruned target sets of state S.
Let Ti = TB (Ei )  V, and Ti = TB (Ei )\Ti . Let Hi be the enclosed-target set in TB (Ei ) and
His = Hi  P. Then, for any subset   X  U, the belief update ratio r(x  v; e) is upper
bounded as follows.
Y
max r(x  v; e) 
max r(z  ti ; ei )  C,
(17)
x,XU

where C =

Q

i p(ei )

i

z,Z=Ti \His


p(e).

Proof. For any   X  U, let W = X  V and W = M\W. Similar with Theorem 2, let
Si = TB (Ei )  W, I = {i : Si 6= }, and J = {j : Sj = }, we have,
X
p(e|W) =
p(e|M)p(W|W)
W

=

XY
W


Y
p(ei |TB (Ei )) p(W|W) 
p(ej |TB (Ej )).

iI

(18)

jJ

Equation 18 is based on the property of target blankets.
Assuming X = U\X, we have W = P  X and His  Si  W. Since His only occurs in
TB (Ei ), we perform summation on non-empty His in Equation 18 as follows.
X
p(ei |TB (Ei ))p(His |M\His )
His

=

X p(ei , His |TB (Ei )\His )
His

=

X

p(His |TB (Ei )\His )

p(His |M\His )

p(ei , His |TB (Ei )\His )

His

= p(ei |TB (Ei )\His ),

(19)

where p(His |TB (Ei )\His ) = p(His |M\His ), since His is conditionally independent with other
targets given TB (Ei )\His .
Based on Equation 19, we obtain an upper bound by summing out His from Equation 18 and
replacing the summation on i (Si \His ) with maximization.
Y
Y
p(e|W) 
max p(ei |TB (Ei )\Si , z) 
p(ej |TB (Ej )).
(20)
iI

z,Z=Si \His

jJ

Since p(e|W) = r(W; e)p(e), we have the following upper bound on r(x  v; e) by re-organizing
the partition of TB (Ei ), i.e., from Si based partition to Ti based partition.
Y
Y
max r(x  v; e) 
max r(z  ti ; ei ) 
max r(z  tj ; ej )  C,
(21)
x,XU

iI

z,Z=Ti \His

jJ

668

z,Z=Tj

fiE XACT A LGORITHMS FOR MRE I NFERENCE

Hi
A

B

B

C

D

C

D

C

D

C

D

D

D

D

D

Figure 4: Subproblem graph of compiling belief ratio tables for the incremental algorithm.

Q
where C = i p(ei ) p(e).
Thus, for any   X  U, we obtain the tighter upper bound on r(x  v; e).
Y
max r(x  v; e) 
max r(z  ti ; ei )  C.
x,XU

i

z,Z=Ti \His

By substituting Equation 17 into 14, we have the tightened upper bound on GBF in Theorem 8.
Theorem 8. Let M = {X1 , X2 , . . . , Xn } be the set of targets, e be the evidence, and < EvdList,
TBList > be a target blanket decomposition of E, where EvdList = {Ei } and TBList =
{TB (Ei )}. Let U, V, and P be the unexpanded, expanded, and pruned target sets of state S.
Let Ti = TB (Ei )  V, and Ti = TB (Ei )\Ti . Let Hi be the enclosed-target set in TB (Ei )
and His = Hi  P. Then, for any subset   X  U, the generalized Bayesian factor score
GBF (x  v; e) is upper bounded as follows.
Y
max r(z  ti ; ei )  C  1
max

GBF (x  v; e)  1 +

x,XU

where C =

Q

i p(ei )

i

z,Z=Ti \His

1  p(v|e)

,

(22)


p(e).

The main difference between Theorems 4 and 8 is that Theorem 4 maximizes His in the upper
bound while Theorem 8 sums out His which results in a tighter upper bound.
4.3 Compiling Belief Ratio Tables
The belief ratio tables contain the belief update ratios of all configurations of a series of target
sets generated based on TB (Ei ), and are used to calculate upper bounds during MRE inference.
To compile belief ratio tables, we find the set of enclosed-targets Hi by converting each target in
TB (Ei ) into an auxiliary node individually. If this does not add new targets or evidence nodes
into the original subnetwork, we add the target into Hi . All the subsets of Hi are used to build
2|Hi | belief ratio tables of TB (Ei ), where |Hi | denotes the size of Hi . Let Hij be a subset of Hi ,
669

fiZ HU & Y UAN

then the target set of the belief ratio table is TB (Ei )\Hij . Since the number of belief ratio tables
increases exponentially according to |Hi |, we limit |Hi | to be no larger than N for both time and
space reasons. Different enclosed-targets contribute different amount in tightening the upper bound,
thus we select the top N enclosed-targets from Hi by sorting the enclosed-targets Hij according to
their belief update ratios maxhij r(hij ; ei ) in descending order.
To compile belief ratio tables, we need to calculate the belief update ratios of all the configurations of target set TB (Ei )\Hij . A straightforward method is to compile each of the 2|Hi | belief ratio
tables independently. But this is slow because of redundant computation. In this work, we propose
an incremental algorithm which compiles the belief ratio tables gradually from the smallest target set
Hi =TB (Ei )\Hi . In Figure 4, assuming there are four enclosed-targets, i.e., Hi = {A, B, C, D},
organized in alphabetical order, and each node in the subproblem graph represents a target. We first
compile the belief ratio table of Hi , then traverse the subproblem graph in breadth-first order. At
each state (node), we compile the belief ratio table by adding the target into the complied belief ratio
table of its parent state. After traversing the subproblem graph, the algorithm compiles all belief
ratio tables. The above algorithm is summarized in Algorithm 4.
In the proposed algorithm, we calculate a hash key HK i of each target blanket based on Hi . The
belief ratio tables of each target blanket are stored in a hash table and the hash keys are calculated
based on the corresponding subset of enclosed-targets Hij .

5. Breadth-First Branch-and-Bound Algorithms
In MRE inference, all of the search nodes are potential solutions except the root node. We can
choose from a variety of search methods to explore the search tree, e.g., depth-first search, best-first
search, and breadth-first search. Since MRE prunes away independent and less relevant targets,
usually the number of targets in the optimal solution is not large. Thus breadth-first search may
reach the optimal solutions faster than other search strategies. That is why we choose breadthfirst search for MRE inference. The breadth-first search may require more memory to store the
unexpanded states in a layer, however.
In order to utilize the proposed upper bounds, our BFBnB algorithms have two major steps:
preprocessing and search. The preprocessing step includes compiling minimal target blanket decomposition, merging target blankets sharing targets, splitting large target blankets, and creating the
belief ratio tables for each TB (Ei ). A suite of upper bounds can be derived by including different combinations of the above preprocessing modules. Our empirical results show that the target
blanket upper bounds in Theorems 4 and 8 show excellent performance.
The search step of BFBnB explores the search tree layer by layer while keeping track of the
highest-scoring state, and prunes a state S if its upper bound is less than the current best GBF. The
proposed upper bounds lead to two versions of the BFBnB algorithms, MPBnd and SPBnd. MPBnd
is based on the upper bound in Theorem 4, which maximizes over both the unexpanded targets and
pruned-targets. Thus, in MPBnd, each target blanket TB (Ei ) has one belief ratio table which is
computed by calculating the belief update ratios of all configurations of TB (Ei ). For each state S,
we search for a configuration in each belief ratio table that is consistent with the expanded targets
ti and has the highest belief update ratio. We then calculate the upper bound of S using Theorem 4.
SPBnd is based on the upper bound in Theorem 8, which sums out part of the pruned-targets
based on the structure of Bayesian networks. Thus, in SPBnd, each target blanket TB (Ei ) has
a set of belief ratio tables each of which is derived by calculating the belief update ratios of all
670

fiE XACT A LGORITHMS FOR MRE I NFERENCE

Algorithm 4 Compile Belief Ratio Tables
Input: M  target set; E  evidence set; e  given evidence; MGraph  moral graph
(GAN (ME) )m ; A target blanket decomposition < EvdList, TBList >, where EvdList =
{Ei }, and TBList = {TB (Ei )}; N  the maximum number of enclosed-targets in a target
blanket.
Output: BeliefRatioTable  the belief ratio tables of each target blanket.
1: function C OMPILE B ELIEF R ATIOTABLE(M, E, e, MGraph, EvdList, TBList, N )
2:
BeliefRatioTable  ;
3:
for each TBi in TBList do
4:
Hi  ;
5:
for each X in TBi do
. generate enclosed-target set Hi
6:
change X to auxiliary node;
7:
NewTBList  CompileMinimalTargetBlankets(M, E, MGraph);
8:
NewTBi .push(X);
9:
if NewTBi == TBi and NewE i == Ei then
10:
Hi .push(X);
11:
end if
12:
reset X to target node;
13:
end for
14:
Hi  Hi .top(N );
. select top N nodes according to maxhij r(hij ; ei )
15:
SearchQueue  ;
16:
BeliefRatioTablei  ;
17:
Hi = TB i \Hi ;
18:
for each instantiation hi of Hi do
19:
BeliefRatioTablei [Hash()].push(GBF (hi ; e));
20:
end for
21:
SearchQueue.push();
. contain Hij
22:
while SearchQueue is not empty do
. compile belief ratio table of TB i
23:
Hij  SearchQueue.pop();
24:
for each X > Y in Hij , where X in Hi do
.  is the order defined in Hi
25:
for each instantiation x of (X  Hij  Hi ) do
. calculate incrementally
26:
BeliefRatioTablei [Hash(X  Hij )].push(GBF (x; e));
27:
end for
28:
SearchQueue.push(Hij  X);
29:
end for
30:
end while
31:
BeliefRatioTable.push(BeliefRatioTablei );
32:
end for
33: end function

configurations of TB (Ei )\Hij . The algorithm of calculating the belief ratio tables in SPBnd is
summarized in Algorithm 4, which includes the algorithm used in MPBnd as a special case. For
each state S, we calculate a hash key SK based on the pruned-targets and use SK &HK i to index
the belief ratio table of each target blanket. Then, we search for a configuration in each selected
671

fiZ HU & Y UAN

Algorithm 5 BFBnB Algorithm Based on Target Blanket Upper Bounds
Input: M  target set; E  evidence set; e  given evidence; MGraph  moral graph
(GAN (ME) )m ; K  the maximum number of targets in a target blanket; N  the maximum
number of enclosed-targets in a target blanket.
Output: BestExplanation  arg maxx,XM GBF (x; e).
1: function BFB N BS EARCH(M, E, e, MGraph, K, N )
2:
< EvdList, TBList > CompileMinimalTargetBlankets(M, E, MGraph);
3:
MergeTargetBlankets(EvdList, TBList, K);
4:
SplitTargetBlankets(EvdList, TBList, K, MGraph);
5:
if TBi .size() < K, for any TBi in TBList then
6:
return None;
. stop the algorithm if the size of a TBi larger than K
7:
end if
8:
BeliefRatioTable  CompileBeliefRatioTable(M,E,e,MGraph,EvdList,TBList,N );
9:
openList  all state x of each X  M;
. initialize openList;
10:
maxGBF  openList.top();
11:
while openList is not empty do
12:
x  openList.pop();
13:
for each state y of Y  U do
. U is the unexpanded set of current state x
14:
sucState  {y}  x;
15:
UpperBound  CalcUpperBound (sucState, BeliefRatioTable);
16:
if UpperBound  maxGBF then
17:
continue;
. use UpperBound to prune the expanded state sucState
18:
end if
19:
if maxGBF < GBF (sucState; e) then
20:
maxGBF  GBF (sucState; e);
21:
BestExplanation  {y}  x;
22:
end if
23:
openList.push(sucState);
24:
end for
25:
end while
26: end function
27: function C ALC U PPER B OUND(sucState, BeliefRatioTable)
28:
for each BeliefRatioTable i in BeliefRatioTable do
29:
BeliefRatioTable Hij  BeliefRatioTable i [Hash(P)&Hash(Hi )];
30:
MaxBeliefRatio i  the maximum belief ratio consistent with v in sucState;
31:
end for
Q
MaxBeliefRatio i C1
32:
UpperBound  1 + i
;
1p(v|e)
33: end function

belief ratio table that is consistent with the expanded targets ti and has the highest belief update
ratio. Finally, we calculate the upper bound of S using Theorem 8.
The BFBnB algorithm is summarized in Algorithm 5. The main difference between MPBnd
and SPBnd is on the methods used to generate the belief ratio table, i.e., Line 8 in Algorithm 5. To
speed up the search process, we sort each belief ratio table of a target blanket in descending order,
672

fiE XACT A LGORITHMS FOR MRE I NFERENCE

Networks
Alarm
Carpo
Hepar
Insurance
Emdec6h
CPCS179

Nodes
37
61
70
27
168
179

Leaves
11
43
41
6
117
151

States
2.84
2.23
2.31
3.30
2.00
2.29

Arcs
46
74
123
52
261
239

Table 1: Benchmark diagnostic Bayesian networks used to evaluate the proposed algorithms.
i.e., higher belief update ratios are closer to the front of the table. Furthermore, in the search tree
of MRE, the expanded targets of a state are guaranteed to be included in its descedant states. Thus
in the proposed method, we record the indices of the best belief update ratios, one for each belief
ratio table, for each expanded state. To calculate the upper bound of a current state, we only need to
search the best belief update ratio from the recorded indices of its parent.

6. Experiments
The proposed algorithms are evaluated on six benchmark diagnostic Bayesian networks listed in Table 1, i.e., Alarm (Ala), Carpo (Car), Hepar (Hep), Insurance (Ins), Emdec6h (Emd), and CPCS179
(Cpc) (Beinlich, Suermondt, Chavez, & Cooper, 1989; Binder, Koller, Russell, & Kanazawa, 1997;
Onisko, 2003; Pradhan, Provan, Middleton, & Henrion, 1994). Among them, Alarm, Carpo, Hepar,
and Insurance are networks with fewer than 100 nodes. Emdec6h and CPCS179 are larger networks with more than 100 nodes. We listed the number of nodes (Nodes), the number of leaf nodes
(Leaves), the average number of node states (States), and the number of arcs (Arcs) of the Bayesian
networks in Table 1. The experiments were performed on a 2.67GHz Intel Xeon CPU E7 with 512G
RAM running a 3.7.10 Linux kernel.
6.1 Experimental Design
Since the proposed algorithms, MPBnd and SPBnd, are the first nontrivial exact MRE algorithms,
we had to use a naive Breadth-First Brute-Force search algorithm (BFBF) as the baseline; basically
BFBF is BFBnB with the bound set to be infinity. We also included the results of tabu search to
indicate the difficulty of the MRE problems. In MPBnd and SPBnd, we set the maximum number of
targets in a target blanket K to be 18. In SPBnd, we set the maximum number of enclosed-targets
in a target blanket N to be 7. BFBF search can only solve test cases with fewer than 15 targets.
To compare the performance among BFBF, MPBnd, and SPBnd, we perform the experiments on
two test settings, one with exactly 12 targets (12-target setting) and the other with around 20 targets
(difficult-target setting). In the 12-target setting, we randomly generated five test settings of each
network, each setting consisting of all leaf nodes as evidence, 12 of the remaining nodes as targets,
and others as auxiliary nodes. Then for each setting, we randomly generated 20 configurations of
evidence (test cases) by sampling from the prior distributions of the networks.
In the difficult-target setting, we randomly generated five test settings of each network, each
setting consisting of all leaf nodes as evidence, around 20 of the remaining nodes as targets, and
others as auxiliary nodes. The number of targets is selected so that the test cases are too challenging
673

fiZ HU & Y UAN

#Cases/Time
BFBF
MPBnd
SPBnd
T6400
T3200
T1600
T800
T400

Ala
1.7e3
17.0
2.6
92
17.3
85
9.1
79
4.7
76
2.4
72
1.2

Car
66.1
3.5
1.5
100
25.4
100
13.2
100
6.9
98
3.6
98
1.8

Hep
270.0
14.6
9.0
93
44.8
91
22.6
86
11.5
81
5.9
81
3.0

Ins
1.6e5
1.6e3
72.7
81
26.3
77
13.7
74
7.0
73
3.6
73
1.8

Emd
212.0
15.1
13.1
95
89.3
95
45.0
95
23.2
95
11.8
95
6.1

Cpc
1.3e4
815.0
377.0
90
108.0
90
55.1
90
28.0
90
14.1
90
7.0

Table 2: Comparison of SPBnd, MPBnd, BFBF, and tabu on running time in seconds (sec) as well
as the accuracy of tabu searches on Bayesian networks in the 12-target setting.

for BFBF but are still solvable by MPBnd and SPBnd. Then for each setting, we randomly generated
20 configurations of evidence (test cases) by sampling from the prior distributions of the networks.
In tabu search, we set the number of search steps since the last improvement L and the maximum
number of search steps M according to different network settings. In the 12-target setting, we set L
to be 20 and M to be {400, 800, 1600, 3200, 6400}. In the difficult-target setting, we set L to be 80
and M to be {12800, 25600, 51200}. To evaluate search performance, we compared the solutions
of tabu search and SPBnd, and counted the number of test cases on which tabu search achieved
optimal solutions.
6.2 Evaluation of MPBnd and SPBnd in the 12-Target Setting
In Table 2, we compared the proposed MPBnd and SPBnd algorithms with BFBF and tabu search
on the test cases with 12-target setting. For tabu search, we listed the number of test cases solved
optimally (top) and running time in seconds (bottom) when using different limits on the number of
search steps. The running time of MPBnd and SPBnd includes both preprocessing time and search
time. MPBnd, SPBnd and BFBF were able to solve the test cases exactly. MPBnd is shown to be
significantly faster than BFBF because of the pruning by the upper bound. The running time of
SPBnd is further reduced by using the tightened upper bound. T400 is the fastest algorithm with
the worst accuracy. With the increase of M , the running time of tabu search increased significantly.
However, in most of the networks, tabu search could not solve all the test cases optimally, even
using more running time than MPBnd and SPBnd.
To compare MPBnd, SPBnd and BFBF in detail, we computed the average running time of
individual test settings for MPBnd, SPBnd and BFBF. Then we illustrate the logarithmic running
time pairs, BFBF vs MPBnd and MPBnd vs SPBnd, as points in Figures 5 and 6 respectively.
We also draw the contour lines to mark the difference between the logarithmic running time in
Figures 5 and 6. For example, the contour line marked by -3 in Figure 5 contains the points on
674

fiE XACT A LGORITHMS FOR MRE I NFERENCE

7
Ala
Car
Hep
Ins
Emd
Cpc

MPBnd (log10ms)

6

0

-1
5
-2

4
-3

3

5

6

7

8

9

BFBF (log10ms)

Figure 5: Distributions of logarithmic running time pairs in milliseconds) of MPBnd and BFBF on
Bayesian networks in the 12-target setting.
6
Ala
Car
Hep
Ins
Emd
Cpc

SPBnd (log10ms)

5

0

-1
4

-2
3
-3

2

3

4

5

6

7

MPBnd (log10ms)

Figure 6: Distributions of logarithmic running time pairs in milliseconds of SPBnd and MPBnd on
Bayesian networks in the 12-target setting.

which MPBnd is 1000 times faster than BFBF. The results show that although the average running
time may change significantly, the ratios of running times between BFBF and MPBnd, and between
MPBnd and SPBnd are relatively stable. MPBnd is roughly 10 to 100 times faster than BFBF.
SPBnd is roughly 3 to 4 times faster than MPBnd.
6.3 Evaluation of MPBnd and SPBnd in the Difficult-Target Setting
In Table 3, we compared the proposed SPBnd algorithm with MPBnd and tabu search on the test
cases in the difficult-target setting. We list the number of targets of each network in the first row of
675

fiZ HU & Y UAN

#Cases/Time
MPBnd
SPBnd
T51200
T25600
T12800

Ala
20
173
63
60
4.06
60
2.40
60
1.42

Car
15
0.98
0.13
100
5.55
100
2.89
100
1.50

Hep
22
478
106
72
17.12
65
9.38
63
5.15

Ins
17
43.91
58.72
76
0.96
76
0.76
76
0.59

Emd
20
225
96
86
34.14
86
17.48
86
8.93

Cpc
20
1,569.2
419
82
6.11
82
4.2
82
2.7

Table 3: Comparison of SPBnd, MPBnd, and tabu on running time in minutes (min) as well as
accuracy of tabu searches on Bayesian networks in the difficult-target setting.

the table. For tabu search, we list both the number of test cases solved optimally (top) and running
time in minutes (bottom) on each network. Increasing M from 12800 to 51200 is not helpful in
preventing tabu search from getting stuck in the local optima. Moreover, the performance of tabu
search varies greatly on different test networks. SPBnd is shown to be significantly faster than
MPBnd on most of the networks due to the tightened upper bound. In SPBnd, we need to compile
a series of belief ratio tables for each target blanket, which may consume significant amount of
running time. For example, although the running time is 419 minutes in CPCS179, the search took
only 182.4 minutes. Also from the results, we can see that the running time of MPBnd and SPBnd
depends on not only the number of targets, but also the tightness of upper bound and the Bayesian
network structures, which control the number of pruned states and the size of belief table in each
target blanket TB (Ei ) respectively.
To compare SPBnd and MPBnd in more detail, we computed the average running time of individual test settings for both of them, and illustrated each running time pair in logarithm as a point in
Figure 7. We also drew the contour lines to mark the difference between SPBnd and MPBnd. The
results showed that the data points of each network form a cluster. SPBnd is again roughly 3 to 4
times faster than MPBnd.
The results in both Figures 6 and 7 showed that for some test cases the running times of MPBnd
and SPBnd are close. There are two possible reasons behind the observation. First, for some
network structures, the enclosed-target sets may not exist in the individual target blankets. Thus in
these cases SPBnd will degenerate to MPBnd. Second, the running time of each test case consists
of two parts, compiling belief ratio tables and performing search. For some test cases, compiling
belief ratio tables may take significant amount of time in SPBnd. We can therefore make the tradeoff
between compiling time and search time by adjusting the maximum number of enclosed-targets N
in a target blanket.
6.4 Scalability of SPBnd
We also evaluated MPBnd and SPBnd on test cases with increasing number of targets from 17 to
25 with an increment of 2 on three Bayesian networks, Hepar, Emdec6h, and CPCS179. For each
target number i, we randomly generated four test settings and 5 test cases of each setting. In the
676

fiE XACT A LGORITHMS FOR MRE I NFERENCE

8
Ala
Car
Hep
Ins
Emd
Cpc

SPBnd (log10ms)

7

6

0
-0.5
-1

5

-1.5

4

3

5

6

7

8

MPBnd (log10ms)

Figure 7: Distributions of logarithmic running time (ms) pairs of SPBnd and MPBnd on test cases
in the difficult-target setting.

Time
MPBnd

SPBnd

Hep
Emd
Cpc
Hep
Emd
Cpc

17
64.7
63.6
483.7
24.0
36.3
130.7

19
101.0
182.4
15.0
100.8
94.1

Targets
21
23
411.3
643.4
66.9 243.1
444.2
499.6
-

25
711.9
-

Table 4: Comparison of SPBnd and MPBnd on running time (min) on test cases with increasing
number of targets. A dash indicates out of time (800m).

Bayesian networks, we set all leaf nodes as evidence, i of the remaining nodes as targets, and others
as auxiliary nodes. We set the time limit of running to be 800 minutes. The results are reported in
Table 4. The results show that the pruning of upper bound slowed down the exponential growth of
running time significantly. SPBnd can handle more complex MRE problems which are out of the
reach of MPBnd. From the results, we can also see that the running time of SPBnd is affected by the
tightness of upper bound and the structures of Bayesian networks, which no longer heavily depends
on the number of targets.
6.5 Importance of Splitting Large Target Blankets
To show the importance of splitting large target blankets, we calculated the percentage of test cases
that need splitting operation at various target settings on three Bayesian networks, Hepar, Emdec6h,
and CPCS179, with increasing number of targets. In this experiment, we only performed the preprocessing step, so we can handle more than 50 targets. All the target blankets of the test cases in
677

fiZ HU & Y UAN

Emd
Cpc
Hep

70

Ratio (%)

60
50
40
30
20
10
0
15

20

25

30

35

40

45

50

Number of tagets

Figure 8: Percentage of cases using splitting operation at different target settings.
Figure 8 can be split into smaller ones with size less than K=18 by using the proposed splitting
algorithm. The results in Figure 8 show that the ratio of test cases that needs splitting increases, and
then decreases with the increasing numbers of targets. The reason is that initially the increase in
the number of targets leads to large target blankets. But as the number of targets keeps increasing,
densely distributed targets tend to introduce more d-separation to the Bayesian network and result
in smaller target blankets. In Figure 8, the peaks of the curves located at 22 approximately. On the
networks with a large number of non-leaf nodes, e.g., Emdec6h, the ratio is higher than 0.5 on a
significant number of target settings. This means that without splitting large target blankets most of
MRE problems cannot be solved on the networks with these target settings.
6.6 Effect of Summing Out Enclosed-Targets
In this section, we take Hepar, Emdec6h, and CPCS179 as examples to illustrate how summing out
enclosed-targets tightens the target blanket upper bound. We calculated the log difference between
the upper bound UpBnd and the maximum GBF MaxGBF in each subtree rooted at a search state
for the test cases in the 12-target setting. Normalized histograms of the differences for both using
and not using the tightening are plotted in Figure 9. The x-axis shows log10 (UpBnd  MaxGBF ),
and the y-axis shows normalized percentages. Thus smaller log10 (UpBnd  MaxGBF ) indicates
tighter upper bound. It is clear from the graph that summing out the enclosed-targets makes the
bound much tighter.
6.7 Convergence of Upper Bound
To gain a better perspective on how the tightened upper bound in Theorem 8 improves over time,
we calculated both the maximum upper bound MaxBound in each layer of the search tree and the
current maximum GBF CurrMax . We also recorded the running time when we finished each search
layer. Figure 10 shows the convergence curves of MaxBound (dotted line) and CurrMax (circled
line) against the search time for one typical test case of each of the three networks, i.e., Hepar,
Emdec6h, and CPCS179, under the 17-target setting in Table 4. Figure 10(A) shows that the upper
bound dropped sharply at the beginning of the search and decreased gradually during the search.
678

fiE XACT A LGORITHMS FOR MRE I NFERENCE

0.25

0.35

0.16
No Tightening
Tightening

Probability

No Tightening
Tightening

No Tightening
Tightening
0.12

0.25
0.15

0.08
0.15
0.04

0.05

0.05
6

2

2

5

6

5

15

25

0

15

Log10(UpBnd-MaxGBF)

Log10(UpBnd-MaxGBF)
(A)

5

5

15

25

Log10(UpBnd-MaxGBF)

(B)

(C)

Figure 9: Tightness of upper bounds. We evaluate the effect of summing out enclosed-targets using
normalized histogram on different Bayesian networks, i.e., Hepar (A), Emdec6h (B), and
CPCS179 (C).

12

30

7

Bound
CurrMax

Bound
CurrMax

Bound
CurrMax

6

Log10GBF

10
20

5

8
4
10
6

3
1

2

3

4

5

Running time (log10ms)
(A)

1

2

3

4

Running time (log10ms)
(B)

5

1

2

3

4

5

Running time (log10ms)
(C)

Figure 10: Convergence of upper bound and current maximum GBF on different Bayesian networks, i.e., Hepar (A), Emdec6h (B), and CPCS179 (C).

Figure 10(B) shows that the upper bound dropped sharply at the end of the search and became tight
quickly. Figure 10(C) shows that the upper bound dropped sharply at both the beginning and the
end of the search. These results demonstrate different behaviors how the bound is tightening during
the search.

7. Discussions and Conclusions
The main contributions of this paper are two BFBnB algorithms, i.e., MPBnd and SPBnd, for solving MRE exactly using upper bounds based on target blanket decomposition of evidence variables.
These are the first non-trivial exact algorithms for solving MRE in Bayesian networks. The key
idea of the proposed method is to decompose the conditional joint probability p(E|M) into a set of
marginal probabilities p(Ei |TB (Ei )). Each marginal probability is related to a subnetwork characterized by a target blanket of a subset of evidence variables. An upper bound on GBF is derived by
679

fiZ HU & Y UAN

maximizing belief update ratio on the fixed target set TB (Ei ) of each subnetwork separately. The
upper bound can be tightened by merging the target blankets sharing the same set of targets.
The above bound can be further improved based on two ideas. First, we proposed to split large
target blankets by converting auxiliary nodes into pseudo-targets, which scales MRE inference to
larger Bayesian networks with more targets. Second, we tightened the upper bound on GBF by
identifying and summing out enclosed-targets from each target blanket TB (Ei ). The new upper
bound reduces the search space dramatically. The experimental results show that SPBnd is significantly faster than the MPBnd and BFBF algorithms. The proposed SPBnd and MPBnd can solve
MRE inference exactly in Bayesian networks which could not be solved previously.
The proposed upper bounds can be calculated efficiently for two reasons. First, each target
blanket TB (Ei ) is usually much smaller than the whole target set M. Second, in the original MRE
problem, we need to search all the subsets of target set M to find the best solution. However, to
calculate upper bound, we only need to search on a fixed target set TB (Ei ) of each subnetwork.
The proposed MRE upper bound consists of four sources of relaxations, i.e., (1) the relaxation
from p(x, v|e) to p(v|e) in Equation 14, (2) bounding all the descendant states, (3) the inconsistent
values of the sharing nodes of different target blankets, and (4) the maximization on the prunedtargets of individual search states. The optimal upper bound that can achieved at a state is equal to
the maximum GBF in the subtree rooted at the state. When the size of expanded target set increase,
both p(x, v|e) and p(v|e) become much smaller than 1, and the relaxation of (1) becomes very
tight. Thus, to achieve the optimal upper bound we only need to minimize the impact of (3) and
(4). In this work, we minimized the effect of (3) by merging the target blankets sharing the same set
of targets, and minimized the effect of (4) by identifying and summing out the enclosed-target sets
from each target blanket.
Different from the brute-force algorithm, the search time of MPBnd and SPBnd is no longer
mainly dependent on the size of search space (i.e., the number of targets and the number of states
of each target), but also on the tightness of the upper bound and the structure of Bayesian networks.
For Bayesian networks with a large number of targets, an upper bound can be efficiently generated
as long as the the number of targets in each target blanket is small. In SPBnd, the tightness of
upper bound depends on the number of enclosed-targets and the quality of each enclosed-target Hij
measured by its belief update ratio maxhij r(hij ; ei ). If there are no enclosed-targets, the proposed
method SPBnd degenerates to MPBnd.
In this work, the splitting algorithm is designed to split target blankets, whose sizes are larger
than K, based on the separation property of undirected graphs by converting auxiliary nodes into
pseudo-targets. If an evidence node is connected directly with many targets, we may decompose it
using the methods such as node splitting (Choi et al., 2007). This is also one of our future research
directions.

Acknowledgements
This work was supported by NSF grants IIS-0953723, IIS-1219114, and a PSC-CUNY enhancement award. Part of this research has previously been presented in AAAI-15 (Zhu & Yuan, 2015).
680

fiE XACT A LGORITHMS FOR MRE I NFERENCE

References
Ay, N., & Polani, D. (2008). Information flows in causal networks. Advances in Complex Systems
(ACS), 11(01), 1741.
Beinlich, I., Suermondt, G., Chavez, R., & Cooper, G. (1989). The alarm monitoring system: A
case study with two probabilistic inference techniques for belief networks. In Proceedings of
the 2nd European Conference on AI and Medicine, pp. 247256.
Binder, J., Koller, D., Russell, S., & Kanazawa, K. (1997). Adaptive probabilistic networks with
hidden variables. Machine Learning, 29, 213244.
Chajewska, U., & Halpern, J. Y. (1997). Defining explanation in probabilistic systems. In Proceedings of the Thirteenth Annual Conference on Uncertainty in Artificial Intelligence (UAI97),
pp. 6271, San Francisco, CA. Morgan Kaufmann Publishers.
Choi, A., Chavira, M., & Darwiche, A. (2007). Node splitting: A scheme for generating upper
bounds in Bayesian networks. In Proceedings of the 23rd Annual Conference on Uncertainty
in Artificial Intelligence (UAI-07), pp. 5766.
Darwiche, A. (2009). Modeling and Reasoning with Bayesian Networks. Cambridge University
Press.
de Campos, L. M., Gamez, J. A., & Moral, S. (2001). Simplifying explanations in Bayesian belief
networks. International Journal of Uncertainty, Fuzziness Knowledge-Based Systems, 9(4),
461489.
Dechter, R., & Rish, I. (2003). Mini-buckets: A general scheme for bounded inference. J. ACM,
50(2), 107153.
Feige, U., & Krauthgamer, R. (2002). A polylogarithmic approximation of the minimum bisection.
SIAM J. Comput., 31(4), 10901118.
Fitelson, B. (2007). Likelihoodism, bayesianism, and relational confirmation. Synthese, 156, 473
489.
Flores, J., Gamez, J. A., & Moral, S. (2005). Abductive inference in Bayesian networks: finding a
partition of the explanation space. In Eighth European Conference on Symbolic and Quantitative Approaches to Reasoning with Uncertainty, ECSQARU05, pp. 6375. Springer Verlag.
Gardenfors, P. (1988). Knowledge in Flux: Modeling the Dynamics of Epistemic States. MIT Press.
Glover, F. (1990). Tabu search: A tutorial. Interfaces, 20, 7494.
Good, I. J. (1985). Weight of evidence: A brief survey. Bayesian statistics, 2, 249270.
Green, P. (1995). Reversible jump Markov chain Monte Carlo computation and Bayesian model
determination. Biometrica, 82, 711732.
Heckerman, D., Breese, J., & Rommelse, K. (1995). Decision-theoretic troubleshooting. Communications of the ACM, 38, 4957.
Henrion, M., & Druzdzel, M. J. (1991). Qualitative propagation and scenario-based schemes for
explaining probabilistic reasoning. In Bonissone, P., Henrion, M., Kanal, L., & Lemmer, J.
(Eds.), Uncertainty in Artificial Intelligence 6, pp. 1732. Elsevier Science Publishing Company, Inc., New York, N. Y.
681

fiZ HU & Y UAN

Huang, J., Chavira, M., & Darwiche, A. (2006). Solving map exactly by searching on compiled
arithmetic circuits. In Proceedings of the 21st National Conference on Artificial intelligence,
Vol. 2, pp. 11431148. AAAI Press.
Jeffreys, H. (1961). Theory of Probability. Oxford University Press.
Jensen, F. V., & Liang, J. (1994). drHugin: A system for value of information in Bayesian networks. In Proceedings of the 1994 Conference on Information Processing and Management
of Uncertainty in Knowledge-Based Systems, pp. 178183.
Kalagnanam, J., & Henrion, M. (1988). A comparison of decision analysis and expert rules for
sequential diagnosis. In Proceedings of the 4th Annual Conference on Uncertainty in Artificial
Intelligence (UAI-88), pp. 253270, New York, NY. Elsevier Science.
Kirkpatrick, S., Gelatt, C. D., & Vecchi, M. P. (1983). Optimization by simulated annealing. Science,
pp. 671680.
Koller, D., & Friedman, N. (2009). Probabilistic Graphical Models - Principles and Techniques.
MIT Press.
Kwisthout, J. (2013). Most Inforbable Explanations: Finding explanations in Bayesian networks
that are both probable and informative. In van der Gaag, L. (Ed.), Symbolic and Quantitative
Approaches to Reasoning with Uncertainty, Vol. 7958 of Lecture Notes in Computer Science,
pp. 328339. Springer Berlin Heidelberg.
Lacave, C., & Diez, F. (2002). A review of explanation methods for Bayesian networks. The
Knowledge Engineering Review, 17, 107127.
Lauritzen, S. L., Dawid, A. P., Larsen, B. N., & Leimer., H.-G. (1990). Independence properties of
directed markov fields. Networks, 20(5), 491505.
Leake, D. B. (1995). Abduction, experience, and goals: A model of everyday abductive explanation.
The Journal of Experimental and Theoretical Artificial Intelligence, 7, 407428.
Marinescu, R., Dechter, R., & Ihler, A. (2014). AND/OR search for marginal MAP. In Proceedings
of the 30th Annual Conference on Uncertainty in Artificial Intelligence (UAI-14), pp. 563
572.
Marinescu, R., & Dechter, R. (2009). AND/OR branch-and-bound search for combinatorial optimization in graphical models. Artif. Intell., 173(16-17), 14571491.
Onisko, A. (2003). Probabilistic Causal Models in Medicine: Application to Diagnosis of Liver
Disorders. Ph.D. thesis, Institute of Biocybernetics and Biomedical Engineering, Polish
Academy of Science.
Pacer, M., Lombrozo, T., Griffiths, T., Williams, J., & Chen, X. (2013). Evaluating computational
models of explanation using human judgments. In Proceedings of the 29th Annual Conference
on Uncertainty in Artificial Intelligence (UAI-13), pp. 498507.
Park, J. D., & Darwiche, A. (2003). Solving map exactly using systematic search. In Proceedings of
the 19th Annual Conference on Uncertainty in Artificial Intelligence (UAI-03), pp. 459468.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.
Morgan Kaufmann.
682

fiE XACT A LGORITHMS FOR MRE I NFERENCE

Pradhan, M., Provan, G., Middleton, B., & Henrion, M. (1994). Knowledge engineering for large
belief networks. In Proceedings of the Tenth Annual Conference on Uncertainty in Artificial
Intelligence, UAI-94, p. 484490. Morgan Kaufmann Publishers, Inc.
Shimony, S. (1993). The role of relevance in explanation I: Irrelevance as statistical independence.
International Journal of Approximate Reasoning, 8(4), 281324.
van der Gaag, L., & Wessels, M. (1993). Selective evidence gathering for diagnostic belief networks.
AISB Quarterly, pp. 2334.
van der Gaag, L., & Wessels, M. (1995). Efficient multiple-disorder diagnosis by strategic focusing,
pp. 187204. UCL Press, London.
Yuan, C., & Hansen, E. A. (2009). Efficient computation of jointree bounds for systematic MAP
search. In Proceedings of 21st International Joint Conference on Artificial Intelligence
(IJCAI-09), pp. 19821989.
Yuan, C., Lim, H., & Littman, M. L. (2011a). Most relevant explanation: computational complexity
and approximation methods. Ann. Math. Artif. Intell., 61, 159183.
Yuan, C., Lim, H., & Lu, T.-C. (2011b). Most relevant explanation in Bayesian networks. J. Artif.
Intell. Res., 42, 309352.
Yuan, C., Liu, X., Lu, T.-C., & Lim, H. (2009). Most relevant explanation: Properties, algorithms,
and evaluations. In Proceedings of 25th Annual Conference on Uncertainty in Artificial Intelligence (UAI-09), pp. 631638.
Yuan, C., & Lu, T.-C. (2007). Finding explanations in Bayesian networks. In Proceedings of the
18th International Workshop on Principles of Diagnosis (DX-07), pp. 414419.
Zhu, X., & Yuan, C. (2015). An exact algorithm for solving most relevant explanation in Bayesian
networks. In Proceedings of the 29th National Conference on Artificial intelligence (AAAI15), pp. 36493655.

683

fiJournal of Artificial Intelligence Research 55 (2016) 799-833

Submitted 08/15; published 03/16

An Exact Algorithm Based on MaxSAT Reasoning for the Maximum
Weight Clique Problem
Zhiwen Fang

ZHIWENF @ GMAIL . COM

State Key Lab. of Software Development Environment
Beihang University, Beijing, 100083, P.R. China

Chu-Min Li

CHU - MIN . LI @ U - PICARDIE . FR

MIS, Universite de Picardie Jules Verne
Amiens 80039, France

Ke Xu

KEXU @ NLSDE . BUAA . EDU . CN

State Key Lab. of Software Development Environment
Beihang University, Beijing, 100083, P.R. China

Abstract
Recently, MaxSAT reasoning is shown very effective in computing a tight upper bound for a
Maximum Clique (MC) of a (unweighted) graph. In this paper, we apply MaxSAT reasoning to
compute a tight upper bound for a Maximum Weight Clique (MWC) of a wighted graph. We first
study three usual encodings of MWC into weighted partial MaxSAT dealing with hard clauses,
which must be satisfied in all solutions, and soft clauses, which are weighted and can be falsified.
The drawbacks of these encodings motivate us to propose an encoding of MWC into a special
weighted partial MaxSAT formalism, called LW (Literal-Weighted) encoding and dedicated for
upper bounding an MWC, in which both soft clauses and literals in soft clauses are weighted. An
optimal solution of the LW MaxSAT instance gives an upper bound for an MWC, instead of an
optimal solution for MWC. We then introduce two notions called the Top-k literal failed clause and
the Top-k empty clause to extend classical MaxSAT reasoning techniques, as well as two sound
transformation rules to transform an LW MaxSAT instance. Successive transformations of an LW
MaxSAT instance driven by MaxSAT reasoning give a tight upper bound for the encoded MWC.
The approach is implemented in a branch-and-bound algorithm called MWCLQ. Experimental
evaluations on the broadly used DIMACS benchmark, BHOSLIB benchmark, random graphs and
the benchmark from the winner determination problem show that our approach allows MWCLQ
to reduce the search space significantly and to solve MWC instances effectively. Consequently,
MWCLQ outperforms state-of-the-art exact algorithms on the vast majority of instances. Moreover,
it is surprisingly effective in solving hard and dense instances.

1. Introduction
Consider an undirected graph G = (V ,E), where V is a set of n vertices {v1 , v2 , ..., vn } and E is a
set of m edges. The density of G is computed as 2m/(n(n  1)). A clique of G is a subset C  V
in which every pair of vertices is adjacent. On the contrary, an independent set of G is a subset
I  V in which every pair of vertices is disconnected. A vertex cover of G is a subset S  V such
that every edge in G has at least one endpoint in S. The maximum clique (MC) problem asks to find
a clique with the largest cardinality. The MC problem is a prominent combinatorial optimization
problem and it is tightly related to two other well-known graph problems, namely the maximum
c
2016
AI Access Foundation. All rights reserved.

fiFANG , L I , & XU

independent set (MIS) problem and the minimum vertex cover (MVC) problem. Concretely, a
maximum clique C of G is a maximum independent set of the complement graph G of G, and V \C
is a minimum vertex cover of G. Therefore, algorithms for any of the three problems can directly
be applied to solve the others in practice. In addition, most exact MC solvers take advantage of the
following relation between the size of a maximum clique and the number of independent sets. If G
can be partitioned into k independent sets, then G cannot contain a clique larger than k, because an
independent set can contribute at most one vertex to a clique.
The MC problem is NP-hard and its decision problem is NP-complete (Karp, 1972), which appears in many applications such as social network analysis (e.g., Zhang, Nie, Jiang, Chen, & Liu,
2014b; Kibanov, Atzmueller, Scholz, & Stumme, 2014). It is fixed-parameter intractable (Downey
& Fellows, 1995). Moreover, it is proved that approximating MC within |V |1 for any given  > 0
is NP-hard (Zuckerman, 2006). The best polynomial-time approximation algorithm achieves an
approximation ratio of O(n(log log n)2 /(log n)3 ) (Feige, 2004). Because of the theoretical and
practical importance of the MC problem, a huge amount of effort has been devoted to solve it by
designing two types of algorithms (also called solvers). One type is heuristic algorithms mainly
including stochastic local search (e.g., Pullan & Hoos, 2006; Cai, Su, & Sattar, 2011; Cai, Su,
Luo, & Sattar, 2013; Fang, Chu, Qiao, Feng, & Xu, 2014a). Another is exact algorithms including
branch-and-bound (BnB) search (e.g., Ostergard, 2002; Regin, 2003; Tomita & Seki, 2003; Konc &
Janezic, 2007; Li & Quan, 2010b; Tomita & Kameda, 2007; Li, Fang, & Xu, 2013). Heuristic algorithms are able to solve large-scale instances but cannot guarantee the optimality of their solutions.
Exact algorithms guarantee the optimality of their solutions, but the worst-case time complexity is
exponential unless P = N P .
A tight upper bound of the size of a maximum clique in a graph is essential for a BnB algorithm
to solve the MC problem efficiently. However, it is very challenging to obtain such an upper bound
in reasonable time. Most state-of-the-art BnB algorithms apply approximation coloring and independent set partition algorithms to compute an upper bound for MC. For instance, Fahle (2002) uses
the constructive heuristic DSATUR to color vertices one by one according to their degrees. Konc
and Janezic (2007), Tomita and Kameda (2007), Li and Quan (2010b) apply the greedy strategy proposed by Tomita and Seki (2003) to partition the graph into independent sets, and use the number of
independent sets in the partition as an upper bound for MC. MaxCLQ (Li & Quan, 2010b, 2010a)
encodes an MC instance into a partial MaxSAT instance and improves the upper bound based on the
independent set partition by making use of MaxSAT reasoning. The excellent performance of MaxCLQ shows that MaxSAT reasoning technologies allows to compute a tight upper bound for MC
within reasonable time. IncMaxCLQ (Li et al., 2013) combines an incremental upper bound and
MaxSAT reasoning to compute a tight upper bound more efficiently. In addition to the independent
set partition and MaxSAT reasoning, other approaches, such as the graph matching (Regin, 2003),
are also used in the upper bounding for MC.
One generalization of the MC problem is to associate each vertex with a positive weight. The
weight of a clique is defined as the total weight of vertices in it. The maximum weight clique
(MWC) problem consists in finding a clique with the largest weight. It is computationally equivalent
to problems like the weighted set packing problem. The MWC problem appears in a variety of realworld applications, such as protein structure predictions (Mascia, Cilia, Brunato, & Passerini, 2010),
coding theory (Zhian, Sabaei, Javan, & Tavallaie, 2013), combinatorial auctions (Wu & Hao, 2015),
computer vision (Ma & Latecki, 2012; Zhang, Javed, & Shah, 2014a), etc. For example, in the
video object segmentation problem, a challenging task is to select a region having high objectness
800

fiA N E XACT A LGORITHM

FOR

M AXIMUM W EIGHT C LIQUE

score and sharing similar appearance, which can be solved by an MWC algorithm (Ma & Latecki,
2012).
Compared to the MC problem, less work has been done to solve the MWC problem. Cliquer (Ostergard, 2002, 2001) is one of the few state-of-the-art exact solvers for both MC and MWC,
and it deals with MC and MWC using similar methods. In a preprocessing, Cliquer partitions the
graph into independent sets by determining one independent set at a time. As long as there are
vertices that can be added into the current independent set, the one with the largest degree is added.
The purpose of this preprocessing is to define a vertex ordering v1 <v2 <. . .<vn , where vi (1in)
is the vertex inserted into an independent set at time i. Then Cliquer searches for an MC or MWC
in the subgraph induced by {vi , vi+1 , . . ., vn } successively for i=n, n  1, . . ., 1 (in this ordering).
The MC or the MWC in the subgraph induced by {vi , vi+1 , . . ., vn } is associated with vi and is
used to prune subtrees in subsequent search. Kumlander (2004, 2008b) inherits the search strategy
from Cliquer. In addition to the MWC associated with vi , Kumlander also partitions the current
subgraph into independent sets and uses the sum of the maximum weight in each independent set
as an upper bound. VCTable (Shimizu, Yamaguchi, Saitoh, & Masuda, 2012) improves Kumlanders algorithm by a new initial vertex order and a better implementation using bitwise operations.
Yamaguchi and Masuda (2008) propose a new upper bound based on the longest path in a directed
acyclic graph constructed from the original graph, which improves the bound based on the independent set partition. OTClique (Shimizu, Yamaguchi, Saitoh, & Masuda, 2013), which is also based
on Cliquer, uses a dynamic programming strategy to calculate upper bounds of some small subproblems in a preprocessing, and stores all the results in a table. The stored upper bounds are used
during search. MinSatz (Li, Zhu, Manya, & Simon, 2012) is an exact solver for the MinSAT problem. An important application of MinSatz is to solve combinatorial optimization problems such as
MC and MWC. MinSatz constructs a weighted graph for the MinSAT instance and uses the clique
partition combined with MaxSAT reasoning to compute a tight bound for the MinSAT instance to
solve. In addition to exact algorithms, some heuristic algorithms are also proposed to solve the
MWC problem (Pullan, 2008; Wu, Hao, & Glover, 2012; Benlic & Hao, 2013).
In this paper, we apply MaxSAT reasoning to solve MWC. We first study three usual encodings of MWC into MaxSAT. All these encodings have intrinsic difficulties in dealing with vertex
weights. This motivates us to propose a dedicated encoding of MWC into MaxSAT called LW
(Literal-Weighted) encoding, in which both soft clauses and literals in soft clauses are weighted.
While an optimal solution of a MaxSAT instance by usual encodings of MWC into MaxSAT gives
an MWC, an optimal solution of an LW MaxSAT instance by the LW encoding just gives an upper
bound for the MWC. So, it makes little sense to run a MaxSAT solver to find the optimal solution of
a LW MaxSAT instance. The interest of the LW encoding is that we can transform an LW MaxSAT
instance to reduce its optimal solution, so that a tighter upper bound for the encoded MWC can be
obtained.
In order to transform an LW MaxSAT instance, we introduce two notions called the Top-k literal
failed clause and the Top-k empty clause, and two sound transformation rules. Then, we implement
a BnB algorithm for MWC called MWCLQ. In every search tree node, MWCLQ first encodes the
current subgraph into an LW MaxSAT instance. Then, driven by MaxSAT reasoning, MWCLQ
repeatedly transforms the LW MaxSAT instance to obtain a tighter upper bound for the encoded
MWC. To the best of our knowledge, it is the first time that MaxSAT reasoning techniques are
used specifically to compute a tight upper bound in a BnB algorithm for MWC. Experimental results on the widely used DIMACS benchmark, BHOSLIB benchmark, random graphs and realistic
801

fiFANG , L I , & XU

benchmark from the winner determination problem show that MWCLQ reduces the search space
significantly and outperforms the state-of-the-art exact algorithms on the vast majority of instances
from those benchmarks.
This paper is extended from the work of Fang, Li, Qiao, Feng, and Xu (2014b) by:
 clearly motivating the LW encoding by illustrating the weakness of three classical encodings
of MWC into MaxSAT in dealing with vertex weights of a graph;
 introducing the notion Top-k empty clause and exploiting it in MWCLQ;
 formally proving the transformation rules;
 adding more experimental results to show the effectiveness of our approach. The algorithm
MWCLQ is now compared with the integer programming solver CPLEX, and the MinSAT
solver MinSatz, especially on realistic instances from the winner determination problem.
State-of-the-art MaxSAT solvers using different encodings of MWC into MaxSAT are also
compared with MWCLQ. In order to further evaluate the LW encoding, which the state-ofthe-art MaxSAT solvers cannot use, we compare different versions of MWCLQ, in which the
only difference is the encoding of MWC into MaxSAT used to compute the upper bound.
This paper is organized as follows. In the next section, we introduce some necessary notations
and background knowledge. In Section 3, we present MWCLQ and different encodings of MWC
into MaxSAT, before extending MaxSAT reasoning to weighted literals by introducing the notions
of Top-k literal failed clause and Top-k empty clause, and two transformation rules. Experimental
results are shown in Section 4. Section 5 concludes the paper.

2. Preliminaries
The subgraph of G induced by a subset V   V is G = (V  , E  ), where E  = {{vi ,vj } | vi ,vj V  
{vi , vj }  E}. For a vertex v, (v) = {u| {u,v}E} is the set of neighbors of v and the cardinality
|(v)| is called the degree of v. We use Gv to denote the subgraph induced by (v)  {v} and G\v
to denote the subgraph induced by V \{v}. A maximal clique is a clique that cannot be extended
any more. A maximum clique is a maximal clique of the largest possible size. The cardinality
of a maximum clique of G is usually denoted as (G) and is called the clique number of G. For
any vertex v in G, a maximum clique of G can be either in Gv or in G\v. The chromatic number
of G, denoted by (G), is the minimum number of colors needed to color the vertices of G such
that no two adjacent vertices share the same color. The vertices sharing the same color constitute
an independent set. Therefore, the graph coloring problem is equivalent to partitioning V into a
minimum number of independent sets. Note that (G) is greater than or equal to (G).
A graph can be edge-weighted or vertex-weighted. We focus on the vertex-weighted graph in
this paper. Formally, a vertex-weighted undirected graph G = (V, E, w) is an undirected graph G =
(V, E) combined with a weighting function w: V  R+ such that every vertex v is associated with
a positive weight w(v). In the sequel, we use the term weighted graph instead
Pof vertex-weighted
graph for simplicity. The weight of a clique C in G is defined to be w(C) = vC w(v). Given a
weighted graph G, the maximum weight clique problem asks to find a clique with the largest weight
in G (Ostergard, 2001; Pullan, 2008; Wu et al., 2012) and this largest weight is often denoted by
802

fiA N E XACT A LGORITHM

FOR

M AXIMUM W EIGHT C LIQUE

v (G) in the literature. Note that a maximum weight clique is not necessarily a clique containing
the maximum number of vertices, but it must be a maximal clique.
The MC problem can be encoded into a partial MaxSAT problem. In MaxSAT, a variable x may
take value 0 (f alse) or 1 (true). A literal  is a variable x or its negation x. A clause c = 1  2 
...  l is a disjunction of literals, which can also be expressed as a set {1 , 2 , . . . , l }. A clause
is satisfied if and only if the clause has at least one literal assigned to true. The length of clause
c is the number of literals it contains, denoted by length(c). A unit clause is a clause containing
only one literal. An empty clause, denoted by , contains no literals and cannot be satisfied. A
conjunctive normal form (CNF) formula  = c1  c2  ...  cm is a conjunction of clauses. Given
a CNF formula  on the set of variables {x1 , x2 , ..., xn }, the satisfiability (SAT) problem is to test
if there is an assignment satisfying all the clauses of , and the maximum satisfiability (MaxSAT)
problem is to find an assignment satisfying the maximum number of clauses (Li & Manya, 2009).
The minimum satisfiability (MinSAT) problem, on the contrary, is to find an assignment minimizing
the number of satisfied clauses (Li et al., 2012). In some cases, some clauses can be declared to be
hard and must be satisfied in all solutions, and other clauses are soft and can be falsified. The partial
MaxSAT problem asks to find an assignment to maximize the number of satisfied soft clauses while
satisfying all hard clauses. Note that the MaxSAT problem is a particular partial MaxSAT problem
without hard clauses. A weighted clause is a pair (c, w), where c is a soft clause and w, a positive
number, is its weight. A weighted partial MaxSAT problem is to find an assignment maximizing the
total weight of satisfied soft clauses while satisfying all hard clauses. A weighted MaxSAT problem
is a weighted partial MaxSAT problem without hard clauses. The optimal solution of a (weighted)
(partial) MaxSAT instance  is denoted as opt() in this paper.
Two main types of exact algorithms are developed for MaxSAT: SAT-based MaxSAT solvers (e.g.,
Morgado, Heras, Liffiton, Planes, & Marques-Silva, 2013; Ansotegui, Bonet, & Levy, 2013; Davies
& Bacchus, 2013a; Ansotegui & Gabas, 2013; Morgado, Dodaro, & Marques-Silva, 2014; Martins,
Joshi, Manquinho, & Lynce, 2014) that solve a MaxSAT instance by repeatedly calling a CDCL
(Conflict-Driven Clause Learning) based SAT solver to solve a sequence of SAT problems, and the
BnB MaxSAT solvers (e.g., Li, Manya, & Planes, 2007; Kugel, 2010). The SAT-based MaxSAT
solvers are particularly efficient to solve industrial MaxSAT problems, while the BnB MaxSAT
solvers are particular efficient to solve the random MaxSAT problems. Some SAT-based solvers
such as MaxHS (Davies & Bacchus, 2013b) also exploit MIP (Mixed Integer Programming) for
solving MaxSAT.
To maximize the number of satisfied clauses equals to minimize the number of falsified clauses.
Many algorithms based on the BnB scheme for MaxSAT compute a lower bound of the number of
falsified clauses (Li, Manya, & Planes, 2006; Li et al., 2007; Larrosa, Heras, & de Givry, 2008;
Kugel, 2010) to prune the search space when solving a MaxSAT instance . Detecting disjoint inconsistent subsets of soft clauses is proved to be very powerful in computing such a bound, where a
subset of soft clauses is said inconsistent if this subset together with hard clauses is unsatisfiable.
Unit propagation is an effective technique widely used in SAT and MaxSAT solvers (Li & Anbulagan, 1997; Li, Manya, & Planes, 2005). The pseudo-code allowing to find an inconsistent subset
of soft clauses based on unit propagation is given in Algorithm 1 (Li et al., 2005). The algorithm
works as follows. It uses a stack S to store all unit clauses of  and performs unit propagation until
an empty clause is produced or S is empty. If an empty clause is produced, the set of all clauses
used in deriving the empty clause, excluding the hard clauses, is returned. The algorithm is called
iteratively to find as many disjoint inconsistent subsets of soft clauses as possible. Note that soft
803

fiFANG , L I , & XU

clauses involved in an inconsistent subset are removed before detecting other inconsistent subsets
to ensure that these subsets are disjoint. Also note that  does not have any solution if an empty set
is returned, because the empty set means that the set of hard clauses is unsatisfiable.
Algorithm 1: ConflictDetectionByUP(, S), to detect an inconsistent subset of soft clauses.
Input: MaxSAT instance  and a stack S storing all unit clauses of .
Output: Return an inconsistent subset of soft clauses if unit propagation results in an empty
clause, otherwise return false.
1 begin
2
while S is not empty do
3
pop a unit clause u from S;
4
  the only literal in u, record u as the reason of  and ;
5
foreach clause c in  contains  do
6
satisfy c;
7
8
9
10
11
12
13
14
15
16
17
18
19

foreach clause c in  contains  do
remove  from c;
if c is a unit clause then
push c into S;
if c is empty then
push c into an empty queue Q, I = {c};
while Q is not empty do
pop a clause c from Q
foreach removed literal  of c do
if the reason r of literal  is not in I then
push r into Q, and insert r into I;
return the set of soft clauses in I;
return false;

Failed literal detection (Freeman, 1995) is used to enhance unit propagation in MaxSAT solving (Li et al., 2006). A literal  of a CNF formula  is called a failed literal of , if unit propagation
in   {} results in an empty clause. Let IS() be the set of soft clauses used in the unit propagation
to derive an empty clause after assigning true to . If all literals in a soft clause c = {1 , 2 , . . ., l }
are failed, then {c}  IS(1 )  IS(2 )      IS(l ) is an inconsistent subset of soft clauses (Li &
Quan, 2010b).

3. MWCLQ: An Exact Algorithm for MWC
In this section, we propose an exact algorithm based on the BnB scheme, namely MWCLQ, for
MWC. In Subsection 3.1, we describe a basic BnB algorithm. In Subsection 3.2, we introduce
a novel encoding called LW (Literal-Weighted) encoding of MWC into MaxSAT after discussing
three usual encodings. Given a weighted graph G, the LW MaxSAT encoding gives an LW MaxSAT
instance lw that represents an upper bound of v (G). In Subsection 3.3, we propose two transfor804

fiA N E XACT A LGORITHM

FOR

M AXIMUM W EIGHT C LIQUE

mation rules for lw . Successive transformations of lw driven by MaxSAT reasoning give a tight
upper bound of v (G), which is presented in Subsection 3.4.
3.1 Branch-and-Bound Search for MWC
Algorithm 2 depicts the pseudocode of MWCLQ.
Algorithm 2: MWCLQ(G, C, LB), a branch-and-bound algorithm for MWC.
Input: A weighted graph G=(V , E, w), a clique C under construction, and a lower bound
LB.
Output: A clique with weight greater than LB,  if no such clique is found.
1 begin
2
if |V | = 0 then
3
return C;
4
5
6
7
8
9
10
11
12
13
14

U B  overestimate(G)+w(C);
if U B  LB then
return ;
v  select(V );
C1  MWCLQ(Gv , C{v}, LB);
LB  max(LB, w(C1 ));
C2  MWCLQ(G\v, C, LB);
if w(C1 )  w(C2 ) then
return C2 ;
else
return C1 ;

MWCLQ searches for a clique, extended from C, with weight larger than LB in G = (V , E,
w). It first computes an upper bound UB by calling overestimate(G) and then compares UB with LB
to test whether further search is necessary in G. If it is possible to find a better solution, MWCLQ
selects a vertex by calling select(V ). For any vertex v in G, a maximum weight clique of G is
either a clique in Gv containing v or a clique in G\v not containing v. Thus, MWCLQ searches for
a maximum weight clique in Gv and in G\v successively.
MWCLQ has a form similar to MaxCLQ, a BnB algorithm for MC (Li & Quan, 2010b), but the
overestimate function for computing an upper bound, and the select function for choosing a branching vertex, are significantly different. These two functions are essential for both MWCLQ and
MaxCLQ. A high-quality upper bound allows solvers to prune useless search, and a good vertex
ordering promises an efficient search process. MaxCLQ computes a base upper bound by partitioning G into independent sets, and then improves the base upper bound by MaxSAT reasoning.
Meanwhile, MaxCLQ orders vertices by selecting first the vertex with minimum degree. In this
paper, we use a simple vertex ordering to select v with the largest weight, breaking ties in favor of
the vertex with higher degree, and focus on how to efficiently compute a tight upper bound using
MaxSAT reasoning.
805

fiFANG , L I , & XU

3.2 Encoding MWC into MaxSAT
An MC or MWC instance can be encoded into MaxSAT as follows.
Boolean variables: A boolean variable xi is added for each vertex vi , which is assigned the
value true if and only if vi is in the maximum clique under construction;
Hard clauses: A set of hard clauses is added to require that any pair of unconnected vertices
does not belong to the same clique. Concretely, a hard clause xi  xj is added for each pair
of unconnected vertices vi and vj ;
Soft clauses: There exist different ways to define the set of soft clauses. Given a graph, all
encodings of MC or MWC into MaxSAT presented in this paper use the same set of hard
clauses. They differ only in the soft clauses used, and will be analyzed and discussed in this
subsection.
An MC instance can also be encoded into a MinSAT instance without hard clauses by adopting
the approach proposed by Ignatiev, Morgado, and Marques-Silva (2014). The obtained MinSAT
instance can be in turn encoded into a MaxSAT instance without hard clauses using the approaches
from the work of Kugel (2012), Zhu, Li, Manya and, Argelich (2012). This encoding without hard
clauses provides a new angle of view for the MC or MWC solving, which awaits future research.
In this section, we focus on encodings of MWC into MaxSAT with the hard clauses and only different in the soft clauses used. Subsection 3.2.1 defines the direct encoding of MWC into MaxSAT.
Subsection 3.2.2 defines split encoding and iterative split encoding of MWC for MaxSAT. Subsection 3.2.3 proposes the novel literal-weighted encoding of MWC into MaxSAT and illustrates its
advantages compared with the direct encoding, the split encoding and the iterative split encoding in
computing the upper bound for MWC.
3.2.1 D IRECT E NCODING

OF

MWC

INTO

M AX SAT

1 v1

v2 2

4 v3

v4 5

Figure 1: A weighted graph with 4 vertices and 1 edge. Numbers indicate vertex weights.
A straightforward way to define soft clauses is to associate a unit soft clause xi with each vertex
vi , giving the direct encoding of MC or MWC into MaxSAT. For example, the MC instance for the
graph in Fig. 1 (without considering vertex weights) is encoded into the following partial MaxSAT
instance: (1) the set of variables is {x1 , x2 , x3 , x4 }; (2) the set of hard clauses is {x1  x3 , x1  x4 ,
x2  x3 , x2  x4 , x3  x4 }; and (3) the set of soft clauses is {x1 , x2 , x3 , x4 }.
Any assignment satisfying all hard clauses gives rise to a clique, since the variables assigned
the value true correspond to pairwise connected vertices. For the non-weighted case, an assignment satisfying all hard clauses and maximizing the number of satisfied soft clauses gives rise to a
maximum clique.
806

fiA N E XACT A LGORITHM

FOR

M AXIMUM W EIGHT C LIQUE

For the weighted case, each unit soft clause is associated with the weight of the corresponding
vertex. For example, the MWC instance for the weighted graph in Fig. 1 can be encoded into a
weighted partial MaxSAT instance with the same boolean variables and the same hard clauses as in
the MC instance. The set of weighted soft clauses is: {(x1 ,1), (x2 ,2), (x3 ,4), (x4 ,5)}. An assignment
satisfying all hard clauses and maximizing the total weight of satisfied soft clauses gives rise to a
maximum weight clique.
Once encoded into MaxSAT, MaxSAT reasoning can be applied to solve the MC or MWC
problem. Many MaxSAT algorithms obtain an upper bound of satisfied soft clauses by computing
a lower bound of the number of falsified soft clauses. Inconsistent subsets of soft clauses are often
used for computing such a lower bound. Recall that a subset of soft clauses is said inconsistent if
the subset together with hard clauses is not satisfiable. For a non-weighted MaxSAT instance  with
m soft clauses, if r disjoint inconsistent subsets of soft clauses are detected, then mr is an upper
bound of opt() (Li et al., 2005, 2006).
For the weighted case, we define the weight of a subset S of soft clauses to be the minimum
weight of clauses in S, namely,
w(S) = min w(c).
cS

Similar to the non-weighted case, we have the following proposition.
Proposition 1. (Li et al., 2007) Given a weighted partial
Pif s disjoint inconsisPMaxSAT instance ,
tent subsets S1 , S1 , ..., Ss are detected, then opt()  soft clause c w(c) 1is w(Si ).
Observe that unit soft clauses of the direct encoding do not capture any connection between
vertices, because the same set of soft clauses is used for every graph with n vertices, no matter how
these vertices are connected between each other. For an MC instance, the number of soft clauses
is used as a base upper bound, and then MaxSAT reasoning is applied to improve the base upper
bound by detecting inconsistent soft clause subsets. Unfortunately, the direct encoding can only
give a trivial base upper bound, which is the number of vertices in the graph. Moreover, since all
soft clauses of the direct encoding are unit, an inconsistent subset contains exactly two soft clauses,
limiting the improvement of the upper bound to the half of the number of vertices in the graph.
The weighted case is similar. The base upper bound is the total weight of all vertices. We use the
following example to illustrate that the direct encoding cannot compute a tight upper bound even
for a very simple MWC instance.
Example 1. The total weight of soft clauses of the direct encoding for the graph in Fig. 1 is 12.
Using unit propagation, {(x3 , 4), (x4 , 5)} is found to be an inconsistent subset because of the hard
clause x3  x4 , then the upper bound can be improved by 4. The left soft clauses are: (x1 , 1),
(x2 , 2), (x4 , 1), then unit propagation detects that {(x1 , 1), (x4 , 1)} is also inconsistent, then we
can improve the upper bound by 1. Finally, the upper bound is improved to 6, larger than the
optimal solution 5.
Because of these drawbacks, the direct encoding does not perform well, as shown by the experimental results presented in Section 4.3. One might want to add an at-most-one constraint for each
independent set to remedy the drawbacks of the direct encoding. That is, for each independent set
I = {v1 , v2 , . . . , vl }, add the at-most-one constraint x1 +x2 +. . .+xl  1. However, the at-most-one
constraint does not add anything new, because the subset of hard clauses {xi  xj | 1i<jl} in the
encoding already enforces the at-most-one constraint (Chen, 2010). In addition, the encoding of the
807

fiFANG , L I , & XU

at-most-one constraint using hard binary clauses is efficient enough when the independent set is not
extremely large, which is usually the case when solving hard MC or MWC instances. Therefore,
the at-most-one constraint is presumably useless in the encoding from MC or MWC into MaxSAT.
3.2.2 S PLIT E NCODING

AND I TERATIVE

S PLIT E NCODING

OF

MWC

INTO

M AX SAT

Another encoding is introduced for MC in MaxCLQ (Li & Quan, 2010b) by defining the set of soft
clauses based on an independent set partition of G. Concretely, MaxCLQ first partitions G into a set
of independent sets, then creates a soft clause for each independent set, which is a disjunction of the
variables corresponding to the vertices in the independent set. The independent set based encoding
is shown substantially more efficient than the direct encoding to solve MC.
A natural way to extend the independent set based MaxSAT encoding to MWC is to split vertex
weights. For an independent set I = {v1 , v2 , . . . , vl }, where w(v1 )  w(v2 )  . . .  w(vl ), we
can split vertex weights in I by the minimum weight w(vl ), thus we obtain a weighted soft clause
(x1  x2  . . .  xl , w(vl )) and l unit soft clauses: (x1 , w(v1 )  w(vl )), (x2 , w(v2 )  w(vl )), . . .,
(xl , w(vl )  w(vl )), where l is the largest integer such that w(vl ) > w(vl ) in I. This encoding is
called the split encoding. The total weight of soft clauses in the split encoding should be less than
that of the direct encoding, giving a better base upper bound, but it may still be large.
Example 2. A possible independent set partition of the graph in Fig. 1 is {v4 , v3 , v1 } and {v2 }.
The soft clauses of split encoding based on the partition are (x4  x3  x1 , 1), (x4 , 4), (x3 , 3) and
(x2 , 2). The total weight of the soft clauses is 10, which is better than that of the direct encoding.
Using unit propagation, {(x4 , 4), (x3 , 3)} is found to be an inconsistent subset because of the hard
clause x3  x4 , which allows us to improve the upper bound by 3. In the same way, {(x4 , 1),
(x2 , 2)} is found to be inconsistent, which improves the upper bound by 1. Finally, the remaining
soft clauses (x4  x3  x1 , 1) and (x2 , 1) are consistent. Thus, the upper bound computed using the
split encoding is still 6.
The split encoding can be improved by the so-called the iterative split encoding, as used in
MinSatz (Li et al., 2012). The idea of the iterative split encoding is to split vertex weights repeatedly
until all vertices in the same independent set have the same weight. Concretely, for an independent
set I = {v1 , v2 , . . . , vl } such that w(v1 )  w(v2 )  . . .  w(vl ), we add a soft clause (x1  x2 
. . .  xl , w(xl )), and repeatedly find the largest l such that w(xl ) > w(xl ) and add a soft clause
(x1  x2  . . .  xl , w(xl )  w(xl )), while l 1.
Example 3. A possible independent set partition of the graph in Fig. 1 is {v4 , v3 , v1 }, {v2 }. The
soft clauses of the iterative split encoding based on the partition are (x4  x3  x1 , 1), (x4  x3 , 3),
(x4 , 1) and (x2 , 2). The total weight of soft clauses is 7. Starting unit propagation by setting x4 = 1,
we find that {(x4 , 1), (x2 , 2)} is an inconsistent subset, so that we can improve the upper bound by
1 and get a new soft clause (x2 , 1). Then, we find that {(x4  x3 , 3), (x2 , 1)} is inconsistent, so the
upper bound is improved to 5, which is the tightest upper bound.
The iterative split encoding gives a non-trivial base upper bound, which is better than those
obtained by the direct encoding and the split encoding. The key point of the iterative split encoding
is that the vertex weights are split in advance, generating numerous soft clauses, such that a variable
may appear in several soft clauses and that some clauses may differ only in one variable (e.g. clauses
(x4  x3  x1 , 1) and (x4  x3 , 3) in Example 3). Many splittings may not be useful in the upper
808

fiA N E XACT A LGORITHM

FOR

M AXIMUM W EIGHT C LIQUE

bound computation, and MaxSAT reasoning may be more complicated in such an instance with
numerous soft clauses.
3.2.3 L ITERAL -W EIGHTED E NCODING

OF

MWC

INTO

M AX SAT

Recall that the MaxSAT encoding for MC in MaxCLQ guarantees that a variable appears only once
in soft clauses and the number of soft clauses equals to the number of independent sets. MaxSAT
reasoning in such an instance should be much simpler. In order to extend this advantage to MWC
and not to split vertex weights in advance, we introduce the Literal-Weighted encoding (LW encoding) from MWC into LW MaxSAT, in which the literals in a soft clause are also weighted. Concretely, a weighted literal is a pair(, w), where  is a literal and w is its weight. A literal-weighted
soft clause (LW soft clause) is a disjunction of weighted literals. Formally, given an MWC instance
G = (V, E, w), we encode G into an LW MaxSAT instance lw as follows:
 Associate each vertex vi with a weighted literal (xi , w(xi )), where w(xi ) = w(vi );
 Add a hard clause xi  xj for each pair of unconnected vertices vi and vj ;
 For an independent set I = {v1 , v2 , ..., vl }, add an LW soft clause c = (x1 , w(v1 ))  (x2 , w(v2 ))
. . . (xl , w(vl )).
The LW soft clause c can also be presented as a set {(x1 , w(x1 )), (x2 , w(x2 )),. . ., (xl , w(xl ))}.
The weight of c is defined to be
w(c) = max (w(xj )).
1jl

An LW clause c is ordered if w(x1 )  w(x2 )  ...  w(xl ). In the sequel, LW soft clauses are
always ordered. Therefore, the weight of an LW soft clause is always w(x1 ), representing the cost
if none of vertices in the corresponding independent set is included in the clique under construction.
We now show that the optimal solution of an LW MaxSAT instance lw , denoted by opt(lw ),
which maximizes the total weight of satisfied soft clauses and satisfies all hard clauses, gives an
upper bound of v (G), differently from the usual encodings by which the optimal solution of a
MaxSAT instance gives v (G).
An MWC is given by an assignment of lw that satisfies all hard clauses and maximizes the
total weight of satisfied literals. Example 4 suggests that opt(lw ) and v (G) can be very different
for an MWC instance G.
1
v1
7
v2

4
v5

2 v3

6
v6

v4 3

Figure 2: A weighted graph with 6 vertices and 6 edges. Numbers indicate vertex weights.

809

fiFANG , L I , & XU

Example 4. A possible independent set partition of the graph in Fig. 2 is {{v1 , v3 , v6 }, {v2 , v4 }
and {v5 }}. The set of soft clauses of the LW MaxSAT instance lw based on this partition is {{(x6 ,
6), (x3 , 2), (x1 , 1)}, {(x2 , 7), (x4 , 3)}, {(x5 , 4)}}. The only MWC of the graph is {v5 , v6 } and
v (G)=10. On the other hand, {x1 =1, x2 =1} is an optimal solution of lw and opt(lw )=13.
Moreover, any optimal solution of lw does not correspond to the maximum weight clique in the
graph.
The following proposition states a relationship between the optimal solution of an LW MaxSAT
instance and the MWC of the encoded graph.
Proposition 2. Consider a weighted graph G = (V, E, w), let lw be the LW instance based on an
independent set partition of G, then v (G)  opt(lw ).
Proof. Suppose that {vi1 , vi2 , . . . , vip } is a maximum weight clique of G. Let cij be the LW clause
containing xij . We have
v (G) =

p
X
j=1

w(xij ) 

p
X

w(cij )  opt(lw ).

j=1

Since opt(lw ) is just an upper bound of v (G), it makes little sense to apply a MaxSAT solver
to find opt(lw ). However, we can transform lw , so that an optimal solution of the new instance
is a tighter upper bound of v (G), and the upper bound of the new optimal solution can also be
tightened. Example 5 illustrates how to do this.
Example 5. A possible independent set partition of the graph in Fig. 1 is {v4 , v3 , v1 } and {v2 }. The
LW soft clauses are {(x4 , 5), (x3 , 4), (x1 , 1)} and {(x2 , 2)}. The total weight of soft clauses is 7,
giving the base upper bound as in the iterative split encoding. Unit propagation by setting x2 = 1
makes x4 and x3 false through the hard clauses. Then we find that we can split the clause {(x4 , 5),
(x3 , 4), (x1 , 1)} into {(x4 , 2), (x3 , 2)} and {(x4 , 3), (x3 , 2), (x1 , 1)}, and keep the base upper
bound to be 7, but that the splitting allows to derive an inconsistent subset of soft clauses {{(x4 , 2),
(x3 , 2)}, {(x2 , 2)}} and to improve the base upper bound to 5, the tightest possible upper bound.
Note that unit propagation does not need to derive any empty clause to improve the upper bound
in this example. Furthermore, in order to obtain the tightest upper bound, one unit propagation suffices, while two inconsistent subsets need to be derived in the iterative split encoding in Example 3.
The key point in Example 5 is that the original LW MaxSAT instance is transformed into a new
one, of which the upper bound of the optimal solution is improved to 5 using MaxSAT reasoning.
This tightest upper bound of the new LW MaxSAT instance is also the tightest upper bound of
v (G).
In the next subsection, we will define two sound transformation rules to transform an LW
MaxSAT instance, allowing to derive a tight upper bound for MWC in general case. To be effective, the application of these two rules should be driven by MaxSAT reasoning, which will be
presented in Subsection 3.4.
810

fiA N E XACT A LGORITHM

FOR

M AXIMUM W EIGHT C LIQUE

3.3 Transformation Rules for Literal-Weighted MaxSAT
We propose two transformation rules to split an LW soft clause of an LW MaxSAT instance encoding
an MWC instance G. The soundness of these rules is based on Proposition 3, ensuring that after
splitting, an optimal solution of the new LW MaxSAT instance is a tighter upper bound of v (G).
Proposition 3. Let lw be an LW MaxSAT instance encoding of an MWC instance G. For any
LW soft clause c = {(x1 , w(x1 )), (x2 , w(x2 )), . . ., (xl , w(xl ))} of lw and 0<w(x1 ), split c
into c = {(x1 , ), (x2 , min(w(x2 ), )), . . ., (xl , min(w(xl ), ))} and c = {(x1 , w(x1 )), (x2 ,
max(w(x2 ) , 0)), . . ., (xl , max(w(xl ), 0))}, where literals with weight 0 are removed, we
get lw = (lw \{c})  {c ,c }, then v (G)  opt(lw )  opt(lw ).
Proof. First of all, note that both c and c are ordered as c. To prove v (G)  opt(lw ), we first
show that min(w(xj ), ) + max(w(xj ), 0) = w(xj ) for any j such that 1<jl. In fact,
1. If w(xj )  , min(w(xj ), ) + max(w(xj ), 0) =  + w(xj )   = w(xj );
2. if w(xj ) < , min(w(xj ), ) + max(w(xj ), 0) = w(xj ) + 0 = w(xj ).
In other words, the total weight of a literal is not changed by splitting c into c and c . Let C =
{vi1 , vi2 , . . . , vip } be a maximum weight clique of G. Then {xi1 = 1, xi2 = 1, . . ., xip = 1} with
other variables being 0, is an assignment of lw satisfying all hard clauses. Let S(xij ) = {c | xij 
c} be the set of soft clauses containing xij of lw for 1jp. All clauses in S(xij ) are satisfied by
xij . At the same time,
X

w(c) =

cS(xij )

X

max w(x)  w(xij ).

cS(xij )

xc

Hence, we have,
opt(lw ) 

p
X
X
j=1 cS(xij )

w(c) 

p
X
j=1

w(xij ) =

p
X

w(vij ) = v (G).

j=1

To prove opt(lw )  opt(lw ), let A be any assignment and w(lw , A) (w(lw , A)) be the total
weight of satisfied soft clauses of lw (lw ).
1. If c is not satisfied in lw by A, then c and c also are not satisfied in lw by A;
2. If c is satisfied by A, c is also satisfied since c has the same literals as c, but c may not be
satisfied because the satisfied literal of c may have weight 0 in c and may thus be removed
from c .
We note that w(c) = w(c )+w(c ), so we have w(lw , A)  w(lw , A) for any assignment A,
implying opt(lw )  opt(lw ).
Given a weighted graph G = (V, E, w), let lw be an LW MaxSAT instance based on an
independent set partition of G, we propose the following two rules to transform lw .
811

fiFANG , L I , & XU

1. -Rule Given an LW soft clause c = {(x1 , w(x1 )), (x2 , w(x2 )), . . ., (xl , w(xl ))} of lw and
a weight  such that 0<w(x1 ), split c into c = {(x1 , ), (x2 , min(w(x2 ), )), . . ., (xl ,
min (w(xl ), ))} and c = {(x1 , w(x1 )), (x2 , max(w(x2 ), 0)), . . ., (xl ,max(w(xl )
,0))}, i.e., lw = (lw \ {c})  {c , c }.
2. (k, )-Rule Given an LW clause c = {(x1 , w(x1 )), (x2 , w(x2 )), . . . , (xl , w(xl ))} of lw , an
integer 1k<l and a weight  such that 0<w(x1 )w(xk+1 ), split c into c = {(x1 , ), (x2 ,
max(w(x2 )+w(x1 ), 0)), . . ., (xk , max(w(xk )+w(x1 ), 0))}, and c = {(x1 , w(x1 )),
(x2 , min(w(x2 ), w(x1 ))), . . ., (xk , min(w(xk ), w(x1 ))), (xk+1 , w(xk+1 )), . . ., (xl ,
w(xl ))}, i.e., lw = (lw \ {c})  {c , c }.
The purpose of the -Rule and the (k, )-Rule is to split c into a clause c with weight  and
a clause c with weight w(x1 ) without changing the total weight of a literal. The constraint
w(x1 )w(xk+1 ) is to ensure that c remains ordered (i.e., w(x1 )min(w(x2 ), w(x1 )). . .
w(xk+1 )). . .w(xl )). We use an example to illustrate the (k, )-Rule. Let c be an LW soft
clause c = {(x1 , 5), (x2 , 3), (x3 , 2)}, (1) If k=1 and =2, then c = {(x1 , 2)} and c = {(x1 , 3), (x2 ,
3), (x3 , 2)}; (2) If k=2 and =3, then c = {(x1 , 3), (x2 , 1)} and c = {(x1 , 2), (x2 , 2), (x2 , 2)};
(3) If k=2 and =2, then c = {(x1 , 2), (x2 , 0)} = {(x1 , 2)} and c = {(x1 , 3), (x2 , 3), (x3 , 2)}.
The soundness of the -Rule and the (k, )-Rule can be easily proved using Proposition 3. These
two rules can be applied in many possible ways to generate many possible clauses. For example,
as a special case, when =w(xl ) in the -Rule, c = {(x1 , )), (x2 , )), . . . , (xl , ))} and at least
xl is removed from c since its weight is 0 in c . In other words, we can transform lw into a
classical weighted MaxSAT instance by repeatedly applying the -Rule with =w(xl ) to each clause
c containing literals with different weights. In this way, we can obtain a classical weighted partial
MaxSAT instance , in which all literals in any soft clause have the same weight and whose optimal
solution gives a maximum weight clique. Moreover, let i be the MaxSAT instance obtained after i
applications of the -Rule, we have opt(lw )  opt(1 )  opt(2 )  . . .  opt() = v (G). In
fact,  is an iterative split encoding of G.
Observe that each application of the -Rule or the (k, )-Rule gives a MaxSAT instance i whose
optimal solution gives a tighter upper bound of v (G). Since unrestricted applications of the two
rules may not be effective, we restrict their applications to the cases where an inconsistent subset of
soft clauses with weight  can be derived. Concretely, the (k, )-Rule is always applied to a clause
c in which the k most weighted literals are failed or falsified, allowing to obtain c from which an
inconsistent subset S of soft clauses can be derived, and the -Rule is always applied to a clause
c in an inconsistent subset of soft clauses. Each application allows to improve an upper bound of
v (G) by a positive weight . Note that  in both rules is strictly greater than 0, because when
 = 0, both rules only generate new empty clauses with weight 0, which cannot improve the upper
bound. We call extended MaxSAT reasoning the application of the -Rule and the (k, )-Rule driven
by MaxSAT reasoning in an LW MaxSAT instance. Details of our approach are given afterwards.
3.4 Upper Bound Based on MaxSAT Reasoning
In this section, we show the transformation of an LW MaxSAT instance driven by MaxSAT reasoning. In Subsection 3.4.1, we apply the -Rule to transform an inconsistent subset of soft clauses.
In Subsection 3.4.2 and Subsection 3.4.3, we introduce two notions, namely the Top-k failed literal clause and the Top-k empty clause, to deduce an inconsistent subset of soft clauses using the
812

fiA N E XACT A LGORITHM

FOR

M AXIMUM W EIGHT C LIQUE

(k, )-Rule, which is then transformed by applying the -Rule. In Subsection 3.4.4, we present the
overestimating algorithm that computes a tight upper bound for MWC by successively transforming
an LW MaxSAT instance.
3.4.1 T RANSFORMING

AN I NCONSISTENT

S UBSET

OF

S OFT C LAUSES

First of all, we detect inconsistent subsets using unit propagation presented in Algorithm 1.
Example 6. A possible independent set partition of the graph in Fig. 2 is {{v1 , v4 , v6 }, {v2 ,
v5 }, {v3 }}, LW soft clauses based on this partition are c1 = {(x6 , 6), (x4 , 3), (x1 , 1)}, c2 =
{(x2 , 7), (x5 , 4)} and c3 = {(x3 , 2)}. When we set x3 =1 to satisfy the unit clause c3 , x3 will
be removed from hard clauses x1  x3 , x3  x5 , x3  x6 . Unit clauses x1 , x5 and x6 imply that
x1 =0, x5 =0 and x6 =0, respectively. So, both c1 and c2 become unit clauses. Accordingly, we have
to set x4 =1 and x2 =1 to satisfy them, falsifying the hard clause x2  x4 .
Consequently, an inconsistent soft clause subset S = {c1 , c2 , c3 } is detected and the weight of
the subset is 2. Proposition 1 allows to decrease the upper bound by 2, thus the improved upper
bound is 13. Since all soft clauses are involved in the inconsistent subset, we cannot improve the
upper bound any more. However, we can split the soft clauses in S using the -Rule based on the
following proposition.
Proposition 4. Let S = {c1 , c2 , . . ., ci } be an inconsistent subset of LW soft clauses, if the -Rule
is applied to split every cj  S into cj and cj with =w(S), then S  = {c1 , c2 , ..., ci } is still an
inconsistent soft clause subset with weight .
Proof. S  is clearly inconsistent, because the clauses in S  have the same literals as the clauses in
S. In addition, every clause in S  has weight . Hence, S  is an inconsistent subset of soft clauses
with weight .
The purpose of the splitting is to obtain S  = {c1 , c2 , . . ., ci } to further improve the upper
bound.
Example 7. As presented in Example 6, {c1 , c2 , c3 } is an inconsistent subset with weight 2. Applying -Rule with =2, we have c1 = {(x6 , 2), (x4 , 2), (x1 , 1)}, c1 = {(x6 , 4), (x4 , 1)}, c2 = {(x2 , 2),
(x5 , 2)}, c2 = {(x2 , 5), (x5 , 2)}, c3 = {(x3 , 2)} and c3 = {(x3 , 0)} (c3 is removed).
It is easy to see that {c1 , c2 , c3 } is an inconsistent subset of soft clauses with weight 2, and
that the remaining clauses {{(x6 , 4), (x4 , 1)}, {(x2 , 5), (x5 , 2)}} can be used to further improve
the upper bound. However, unit propagation cannot be used to improve the upper bound any more
because no unit clause exists. Moreover, failed literal detection does not work either because every
soft clause contains at least one literal that is not failed. We propose to apply the (k, )-Rule to split
the Top-k literal failed clause defined afterwards to improve the upper bound.
3.4.2 T OP - K FAILED L ITERAL D ETECTION
Definition 1. An LW soft clause c = {(x1 , w(x1 )), (x2 , w(x2 )), . . ., (xl , w(xl ))} is Top-k literal
failed if x1 , x2 , . . ., xk are all failed literals, where 1k<l.
We define the Top-k weight of an LW soft clause c as wk (c) = w(x1 )w(xk+1 ), where 1 
k < length(c).
813

fiFANG , L I , & XU

Proposition 5. Consider an LW soft clause c = {(x1 , w(x1 )), (x2 , w(x2 )), . . ., (xl , w(xl ))}. If c is
Top-k literal failed with k<l, and IS(xj ) (1jk) is the set of soft clauses making xj failed, split
c into c and c using the (k, )-Rule with  = min(wk (c), w(IS(x1 )), w(IS(x2 )), ..., w(IS(xk ))),
then {c }  IS(x1 )  IS(x2 )  . . .  IS(xk ) is an inconsistent subset of soft clauses with weight .
Proof. The set is clearly inconsistent, because the satisfaction of each literal in c results in an empty
clause in the set. The minimum weight of the clause in the set is . So it is an inconsistent subset of
soft clauses with weight .
As soon as we determine that c is Top-k literal failed, we apply the (k, )-Rule to split c and use
the -Rule to split clauses in IS(x1 )  IS(x2 )  . . .  IS(xk ). An inconsistent subset with weight 
is obtained in this way. Observe that when k=l, {c}  IS(x1 )  IS(x2 )  . . .  IS(xl ) is a classical
inconsistent subset of soft clauses with weight min(w(c), w(IS(x1 )), w(IS(x2 )), ..., w(IS(xl )))
in which each clause is split only using the -Rule.
Example 8. Consider the soft clauses produced in Example 7, c1 = {(x6 , 4), (x4 , 1)} and c2 =
{(x2 , 5), (x5 , 2)}. We test x2 in c2 by setting x2 =1. To satisfy hard clauses x2  x6 and x2  x4 ,
we need to set x6 =0 and x4 =0. Then, c1 = {(x6 , 4), (x4 , 1)} becomes falsified, making x2 failed.
However, x5 is not failed. Therefore c2 is Top-k literal failed with k=1. Applying the (k, )-Rule with
k=1 and =3 to c2 , we have c2 = {(x2 , 3)} and c2 = {(x2 , 2), (x5 , 2)}. Applying the -Rule with
=3 to c1 , we get c1 = {(x6 , 3), (x4 , 1)} and c1 = {(x6 , 1)}. Consequently, we get an inconsistent
subset {c1 , c2 } with weight 3.
As a result, the detection of a Top-k literal failed clause allows to improve the upper bound by
3, giving the tightest upper bound 10.
3.4.3 T OP - K E MPTY C LAUSE D ETECTION
It is noteworthy that a literal can be declared to be failed, only if unit propagation of the literal falsifies all literals of a clause. Sometimes, the propagation of a literal  just makes the most weighted
literals in a soft clause c falsified, but not all literals in c. In this case,  cannot be declared to be
failed, so we cannot improve the upper bound using approaches presented above. However, we can
split c using the (k, )-Rule to obtain a falsified clause, so that  can be declared to be failed.
1
v1
7
v2

4
v5

2 v3

6
v6

v4 3

Figure 3: A weighted graph with 6 vertices and 9 edges. Numbers indicate vertex weight
Example 9. Consider the weighted graph G in Fig. 3, a possible independent set partition of G
is {{v6 , v3 }, {v2 ,v5 }, {v4 , v1 }} and the LW soft clauses are c1 ={(x6 , 6), (x3 , 2)}, c2 ={(x2 , 7),
814

fiA N E XACT A LGORITHM

FOR

M AXIMUM W EIGHT C LIQUE

(x5 , 4)} and c3 ={(x4 , 3), (x1 , 1)}. The total weight of soft clauses of the LW MaxSAT instance
based on this partition is 16. On the one hand, the literals with the largest weight in each soft
clause are not failed. When we set x6 =1, we need to set x2 =0 to satisfy the hard clause x2  x6 .
The literal x6 is not failed, because unit propagation does not falsify any clause. However, the most
weighted literal x2 in c2 is falsified. We can split c2 into c2 = {(x2 , 3)} and c2 = {(x2 , 4), (x5 , 4)}.
Then c2 is falsified by propagating x6 . So, x6 is a failed literal after splitting c2 , and c1 is a Top-k
(k=1) literal failed clause that can be split into c1 = {(x6 , 3)} and c1 = {(x6 , 3), (x3 , 2)} using
the (k, )-Rule. We thus obtain an inconsistent subset {c1 , c2 } with weight =3. Consequently, the
upper bound is improved by 3.
Example 9 suggests us to define the Top-k empty clause notion.
Definition 2. An LW soft clause c = {(x1 , w(x1 )), (x2 , w(x2 )), . . ., (xl , w(xl ))} is Top-k empty,
where 1k<l, if there is a literal  such that when  is assigned to true, unit propagation falsifies
the literals x1 , x2 , . . ., xk in c.
It is straightforward to show the following proposition.
Proposition 6. A literal can be declared to be failed after splitting a Top-k empty clause by using
the (k, )-Rule.
Proof. Splitting the Top-k empty clause gives a clause c = x1  x2  . . .  xk , such that there is a
literal  of which the satisfaction makes c empty via unit propagation.
3.4.4 OVERESTIMATING A LGORITHM

IN

MWCLQ

Algorithm 3 formally describes how to successively transform an LW MaxSAT instance by applying
the -Rule and/or (k, )-Rule to obtain disjoint inconsistent subsets of soft clauses in every search
tree node of MWCLQ. The upper bound computed by Algorithm 3 in this way is very tight, as
shown by our experimental results.
Given a weighted graph G, Algorithm 3 first encodes the MWC instance into an LW MaxSAT
instance based on an independent set partition of G. The partitioning procedure works as follows.
Vertices are sorted in the decreasing order of their weights (ties are broken in favor of the vertices
with higher degree) and are successively inserted into an independent set. Suppose that the current
independent sets are I1 , I2 , . . ., Ii (in this order, i is 0 at the beginning of the partitioning process).
The current first vertex v is inserted into the the first Ij such that v is unconnected to all vertices
already in Ij . If such a Ij does not exist, a new independent set Ii+1 is opened and v is inserted
into Ii+1 . Each independent set is encoded into an LW soft clause. The total weights of soft clauses
is an initial upper bound for MWC of G that should be improved by detecting disjoint inconsistent
subsets of soft clauses using the extended MaxSAT reasoning.
The detection of an inconsistent subset of soft clauses is performed by detecting failed literals
in the shortest available soft clause c. A literal is failed either because unit propagation falsifies
all literals of another clause or just the most weighted literals of another soft clause. In the case
all literals of c are failed because unit propagation falsifies all literals of another clause, a usual
inconsistent subset of soft clauses is obtained. In the case only the k most weighted literals of c are
falsified and/or unit propagation of a literal of c just falsifies the most weighted literals of another
soft clause t, the (k, )-Rule should be applied to split c and/or t to obtain an inconsistent subset
of soft clauses. That is the reason why the Top-k empty clauses and Top-k literal failed clauses are
815

fiFANG , L I , & XU

Algorithm 3: overestimate(G), computing an upper bound for MWC by MaxSAT reasoning

1
2
3
4
5
6
7

Input: A weighted graph G=(V , E, w).
Output: An upper bound for a maximum weight clique in G.
begin
partition G into independent sets I1 , I2 ,...,Ii ;
encode P
G into an LW MaxSAT instance lw and mark all soft clauses available;
/* the base upper bound */
UB  clw w(c);
while lw contains an available soft clause do
c  the shortest available soft clause of lw ;
mark c unavailable;
/* Let S be a set of soft clauses involved in an inconsistent
subset and Stopk be a set of top-k literal failed and top-k
empty clauses */

8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34

S  , Stopk  , k  0;
while k < length(c) do
if k+1 is failed because of an empty clause then
S  S  IS(k+1 );
else if k+1 is failed because of a Top-kt empty clause t and wkt (t) > 0 then
S  S  (IS(k+1 ) \ {t});
Stopk  Stopk  {t};
else break;
k  k + 1;
if k > 0 then
if k = length(c) then
S  S  {c};

/* all literals in c are failed */

else if wk (c) > 0 then
Stopk  Stopk {c};

/* c is top-k literal failed */

else continue;
  min(w(S), mintStopk (wkt (t)));
UB  UB;
/* improve the upper bound by  > 0 */

S   , Stopk
 ;
foreach clause cl  S do
apply -Rule to cl;
S  = S   {cl  };
foreach Top-k literal failed clause or Top-k empty clause t  Stopk do
apply (, kt )-Rule to t;


Stopk
= Stopk
 {t };
 ;
lw  (lw \ (S  Stopk ))  S   Stopk
mark all clauses of lw available;

return UB;

/* update soft clauses */

/* the improved upper bound by MaxSAT reasoning */

816

fiA N E XACT A LGORITHM

FOR

M AXIMUM W EIGHT C LIQUE

collected in Stopk . Meanwhile, the soft clauses involved in the inconsistent subset are collected in
S. In any case,  is computed as  = min(w(S), mintStopk (wkt (t))) (Line 23), where w(S) is the
minimum clause weight in S. Note that Stopk is empty when S is a usual inconsistent subset. The
-Rule and (k, )-Rule are applied to split the clauses in the subset and the upper bound is improved
by the weight of this subset.
Observe that Algorithm 3 is called at every search tree node from scratch because the graph
at different search tree node is different. Also observe that the detected inconsistent subsets of soft
clauses are disjoint, because a soft clause in one subset is not used to detect any other subset. In fact,
an inconsistent subset of soft clauses is removed in Line 32 before detecting another inconsistent
set. The initial upper bound is improved by the total weight of these inconsistent subsets. This
approach might be further improved using MaxSAT resolution defined in the work of Bonet, Levy
and, Manya (2006) and Larrosa, Heras, and Givry (2008), by adapting the approach of Abrame and
Habet (2014) that transforms an inconsistent subset of clauses into a weighted empty clause and
a set of new clauses that can be used in the detection of other inconsistent subsets of soft clauses.
Nevertheless, the application of MaxSAT resolution has to be carefully driven for the approach to
be competitive (Abrame & Habet, 2015), because MaxSAT resolution produces many intermediate
clauses.
We note that clauses produced using the -Rule and the (k, )-Rule cannot be obtained by simply
applying MaxSAT resolution, because MaxSAT resolution transforms a MaxSAT instance into an
equivalent one, while the -Rule and the (k, )-Rule may change the optimal solution of an LW
MaxSAT instance by Proposition 3.

4. Empirical Evaluation
In this section, we empirically evaluate MWCLQ and its extended MaxSAT reasoning using standard benchmarks, namely the DIMACS benchmark, the BHOSLIB benchmark, random graphs, and
the benchmark from the winner determination problem (WDP). We conducted four experiments in
this study. The first experiment is to evaluate the performance of MWCLQ by comparing it with
other state-of-the-art exact algorithms. The second experiment is to compare different encodings
from MWC into MaxSAT. The third experiment is to investigate the impact of splitting soft clauses
in the extended MaxSAT reasoning. In the forth experiment, we applied MWC algorithms to solve
WDP instances and compare their performances.
We first introduce the benchmarks used in our experiments and describe the experimental environment. Then we present and discuss the experimental results in detail.
4.1 Benchmarks and Experimental Environment
Three types of benchmarks are used in our first three experiments.
1. DIMACS The DIMACS benchmark is taken from the Second DIMACS Implementation
Challenge, which has been used widely for benchmarking purposes in the literature of algorithms for MC, MWC, MVC, MIS and so on. The 80 DIMACS instances are generated
from real-world applications such as coding theory, fault diagnosis, Kellers conjecture and
the Steiner Triple Problem, as well as random graphs generated with different properties, such
as the DSJC, brock and p hat families. The size of these instances ranges from less than 50
817

fiFANG , L I , & XU

vertices and 1,000 edges to more than 4,000 vertices and 5,000,000 edges. We downloaded
all instances from a website (ThanhVu & Thang, 2014).
2. BHOSLIB BHOSLIB (Xu, 2004)(Benchmarks with Hidden Optimum Solutions for Graph
Problems) instances are based on a CSP model named RB (Xu & Li, 2000; Xu, Boussemart,
Hemery, & Lecoutre, 2007). Phase transitions exist for model RB and the transition points
can be located exactly. BHOSLIB instances are generated in the phase transition region of
model RB and appear to be extremely hard to solve for various algorithms, even when the
graph size is small (Liu, Lin, Wang, Su, & Xu, 2011; Xu & Li, 2006). They were firstly used
in the SAT competition 2004, and then have been widely used to evaluate algorithms for MC,
MVC and MIS.
3. Random A random graph of n vertices and density p is generated by randomly selecting
each edge with probability p from the complete graph of n vertices. In our experiments, n
ranges from 150 to 700 and p from 0.5 to 0.95. Random graphs allow to show the asymptotic
behavior of an algorithm.
We convert a non-weighted graph into a weighted graph by associating a weight w(vi ) = i mod
200+1 to each vertex vi . This method was initially proposed by Pullan (2008) and has been used
as a standard converting approach to generate weighted graphs from non-weighted instances (Wu
et al., 2012; Benlic & Hao, 2013).
All solvers used in our experiments are implemented in C/C++. We compile them using
gcc/g++ 4.7.2 with option -O3. All experiments are running on a machine with Intel(R) Xeon(R)
CPU E5-2680 @ 2.70GHz, 8 cores and 16G RAM under Debian GNU/Linux 7.4. The cut-off time
for a solver to solve an instance is one hour (3600 seconds).
4.2 Comparison of MWCLQ with Other Algorithms
We compare MWCLQ with two state-of-the-art exact solvers specific for MWC, a MinSAT solver
and CPLEX.
1. Cliquer is a state-of-the-art solver for both MC problem and MWC problem. To our best
knowledge, almost all recent exact algorithms for MWC (e.g., Kumlander, 2004; Shimizu
et al., 2013, 2012) are based on Cliquer. We used the latest version of Cliquer released in
2010, available at its homepage (Ostergard, 2010).
2. DKum (Kumlander, 2004, 2008b) is implemented in VB. The source code can be found at the
authors homepage (Kumlander, 2008a). To execute it in our experimental environment, we
translated it into C.
3. MinSatz (Li et al., 2012) is an exact weighted partial MinSAT solver, which achieves the
state-of-the-art performance on solving clique problems and combinatorial auction problems.
4. CPLEX is a high-performance mathematical programming solver for linear programming,
mixed integer programming, quadratic programming, and quadratically constrained programming problems. Let G = (V, E, w) be a weighted graph, where V = {v1 , v2 , . . . , vn }, the
818

fiA N E XACT A LGORITHM

FOR

M AXIMUM W EIGHT C LIQUE

integer programming formulation of the MWC instance G is usually defined as follows.
max

n
X

xi  w(vi )

i=1

subject to
xi + xj  1,

{vi , vj } 
/E

xi  {0, 1},

i = 1, 2, . . . , n.

Observe that the at-most-one constraint is enforced for every independent set of G. A solution
of the integer programming problem corresponds to a maximum weight clique of G. CPLEX
12.6 was used in our experiments to solve MWC after encoding it as an integer programming
problem in this way.
Table 1 shows runtimes of different solvers on DIMACS and BHOSLIB benchmarks. All 80
instances from the DIMACS benchmark are used in this experiment. MWCLQ solves 61 instances
within the cut-off time, while Cliquer, DKum, MinSatz and CPLEX solves 52, 48, 58 and 44 instances, respectively. For simplicity, we exclude the instances solved within 100 seconds by all
solvers or not solved by any solver within 3600 seconds. For the 39 instances displayed in Table 1,
MWCLQ outperforms Cliquer on 32 instances and is comparable with Cliquer on other instances.
MWCLQ significantly outperforms DKum on all instances except hamming10-2, which can be
solved by both within 20 seconds. MWCLQ dominates MinSatz on 32 instances. In particular,
for the 8 instances from the brock family, MWCLQ is about 20X faster than MinSatz for the instances with 400 vertices and solves all four instances with 800 vertices, which cannot be solved
by MinSatz. MWCLQ outperforms CPLEX on 28 instances. Particularly, CPLEX cannot solve
any instance from the brock and p hat families, while MWCLQ can solve all of them efficiently. It
is interesting that both MinSatz and CPLEX solve MANN a27 and MANN a45 within the cut-off
time, which cannot be solved by any specific MWC solver. Note that MANN a27 and MANN a45
are also hard for heuristic algorithms. For instance, BLS (Benlic & Hao, 2013) can only find an
approximating solution of 12281 with the success rate 16% within 396.58 seconds for MANN a27,
while PLS (Pullan, 2008) can only find an approximating solution of 12264.
We also used all 40 instances from the BHOSLIB benchmark. These instances are extremely
hard for exact MWC solvers. All evaluated solvers can solve at most the 5 smallest instances with
450 vertices. Both MWCLQ and DKum solve 5 instances, CPLEX, Cliquer and MinSatz solves 3,
1 and 0 instances, respectively. Moreover, MWCLQ achieves the best performance on 4 out of the
5 instances.
Table 2 shows mean runtimes on random graphs. We generate 50 graphs at each point. MWCLQ is the only algorithm which solves all 700 graphs in this experiment within the cutoff time,
while Cliquer, Dkum, MinSatz and CPLEX solves 595, 548, 548 and 387 instances, respectively.
MWCLQ significantly outperforms Dkum and MinSatz on all instances and dominates CPLEX at
all points except (200, 0.95) where both algorithms solve all instances within 30 seconds. MWCLQ
dominates Cliquer completely on random graphs with density D0.7, and is comparable with Cliquer when D<0.7. MWCLQ is the only solver which can solve all instances at the point (700, 0.7).
The experimental results suggest in particular that MWCLQ is very effective in solving graphs with
819

fiFANG , L I , & XU

Table 1: Runtimes (in seconds) on DIMACS and BHOSLIB benchmarks. The cut-off time is 3600
seconds. |V | stands for the number of vertices, D for the density of the graph and v
for the optimal solution of MWC. When a solver cannot solve an instance within 3600
seconds, its runtime is marked by -. Instances solved within 100 seconds by all solvers
or not solved by any solver within the cut-off time are omitted.
Graph
|V |
brock400 1
400
brock400 2
400
brock400 3
400
brock400 4
400
brock800 1
800
brock800 2
800
brock800 3
800
brock800 4
800
C250.9
250
DSJC1000.5
1000
DSJC500.5
500
gen200 p0.9 44
200
gen200 p0.9 55
200
gen400 p0.9 75
400
hamming10-2
1024
johnson32-2-4
496
MANN a27
378
MANN a45
1035
p hat1000-1
1000
p hat1000-2
1000
p hat1500-1
1500
p hat500-1
500
p hat500-2
500
p hat500-3
500
p hat700-1
700
p hat700-2
700
san1000
1000
san200 0.7 2
200
san200 0.9 1
200
san200 0.9 2
200
san200 0.9 3
200
san400 0.7 1
400
san400 0.7 2
400
san400 0.7 3
400
san400 0.9 1
400
sanr200 0.7
200
sanr200 0.9
200
sanr400 0.5
400
sanr400 0.7
400
Total: 80
rb30-15-1
450
frb30-15-2
450
frb30-15-3
450
frb30-15-4
450
frb30-15-5
450
Total: 40

D
74
75
74
75
65
65
65
65
89
50
50
89
89
90
99
88
98
99
24
49
25
25
50
75
25
50
50
69
89
89
89
70
70
70
90
69
89
50
70

v
3422
3350
3471
3626
3121
3043
3076
2971
5092
2186
1725
5043
5416
8006
50512
2033
12283
34265
1514
5777
1619
1231
3920
5375
1441
5290
1716
2422
6825
6082
4748
3941
3110
2771
9776
2325
5126
1835
2992

82
82
82
82
82

2990
3006
2995
3032
3011

Cliquer
260.2
366.1
290.7
287.4
1548
1603
1702
1990
32.50
0.97
678.6
1718
1469
0.12
1.03
0.02
5.59
1.03
169.5
403.9
223.3
2329
0.20
1150
0.15
29.98
52
1065
1

820

DKum
607.2
524.5
573.6
606.3
2215
91.88
0.97
119.8
362.2
7.47
0.35
2.72
0.02
3.86
0.17
213.4
23.28
166.9
15.11
470.3
215.3
74.48
48
177.7
44.98
423.4
287.5
154.0
5

MinSatz
2046
2737
2363
1609
640.6
3527
32.80
72.40
35.19
4.45
3487
21.75
1628
252.3
1.52
10.41
1009
4.42
39.93
46.90
0.11
0.10
16.37
118.2
12.62
34.74
96.12
1849
2.73
68.97
9.44
429.0
58
0

CPLEX
30.41
2.44
1.81
238.7
0.06
0.43
2.07
32.61
0.70
0.31
0.83
3.54
24.45
87.75
43.53
13.30
10.81
11.01
44
193.5
258.2
118.3
3

MWCLQ
129.3
127.7
107.2
81.81
1427
2002
1545
2039
39.99
81.30
0.92
7.00
2.76
15.46
0.53
2501
4.12
0.01
2.34
916.7
0.13
47.88
183.8
0
0.24
1.64
16.19
3.44
4.98
6.37
1257
0.13
6.40
0.31
25.57
61
244.9
30.14
131.7
181.8
57.31
5

fiA N E XACT A LGORITHM

FOR

M AXIMUM W EIGHT C LIQUE

Table 2: Mean runtimes in seconds on random graphs, obtained by solving 50 graphs at each point.
The cut-off time is 3600 seconds. |V | stands for the number of vertices, D for the density
of the graph and  v for the weight of an optimal solution, averaged over the solved instances. The mean runtime is marked by - if no instance is solved at the point within the
cut-off time. # stands for the number of instances solved by the solver within the cut-off
time
|V |
150
150
200
200
200
300
300
300
500
500
600
600
700
700

Benchmark
D
v
0.90
3394
0.95
4766
0.80
3249
0.90
5095
0.95
7372
0.70
2441
0.80
3334
0.90
5351
0.60
2285
0.70
2969
0.60
2496
0.70
3293
0.60
2510
0.70
3283
Total 700

Cliquer
Time
#
21.06
50
1006
50
4.02
50
974.9
50
0
1.76
50
81.45
50
0
3.94
50
119.9
50
23.51
50
1256
50
46.01
50
3006
45
595

DKum
Time
#
7.92
50
35.42
50
5.05
50
373.0
50
2464
19
3.01
50
107.1
50
0
9.45
50
302.1
50
62.46
50
2884
29
132.8
50
0
548

MinSatz
Time
#
3.57
50
2.21
50
12.65
50
94.63
50
111.1
50
32.36
50
379.6
50
0
208.5
50
2994
48
1006
50
0
2757
50
0
548

CPLEX
Time
#
2.58
50
1.99
50
54.83
50
23.45
50
3.91
50
353.5
50
303.7
47
263.5
40
0
0
0
0
0
0
387

MWCLQ
Time
#
0.58
50
0.42
50
0.99
50
14.30
50
28.77
50
1.13
50
14.87
50
845.4
50
6.03
50
93.36
50
34.79
50
869.9
50
78.24
50
2434
50
700

Table 3: Lower bounds given by different solvers for DIMACS instances which cannot be solved
by any solver within the cut-off time.
Instance
C1000.9
C2000.5
C2000.9
C4000.5
C500.9
MANN a81
gen400 p0.9 55
gen400 p0.9 65
hamming10-4
keller5
keller6
p hat1000-3
p hat1500-2
p hat1500-3
p hat700-3

|V |
1000
2000
2000
4000
500
3321
400
400
1024
776
3361
1000
1500
1500
700

D
90
50
90
50
90
100
90
90
83
75
82
74
51
75
75

Cliquer
1181
2466
534
1284
1868
195
2501
2855
738
2860
511
2417
2897
1521
2822

DKum
1846
1186
782
863
2070
1201
3037
3179
1798
2139
1637
2893
3127
2067
5156

821

MinSatz
6385
2198
6747
2263
5594
100970
5988
6180
4062
3317
5595
6779
6100
7050
7340

CPLEX
8066
1358
7362
1854
6520
111386
6611
6720
5042
3317
6937
7258
5954
9058
7400

MWCLQ
8471
2466
10034
2698
6672
111033
6676
6832
4614
3317
6316
7588
7104
8449
7565

fiFANG , L I , & XU

104

Time

103

102

101

MWCLQ
Cliquer
DKum

san200_0.9_2
gen200_p0.9_44
san400_0.7_3
san400_0.7_2
san200_0.9_3
san400_0.7_1
brock400_3
p_hat700-2
c250.9
hamming10-2
san1000
brock800_3
brock400_4
p_hat500-3
brock400_1
brock800_1
brock800_2
p_hat1000-2
brock800_4
san400_0.9_1

100

Figure 4: Runtimes to find an optimal solution. Instances for which all solvers find an optimal
solution within 100 seconds or MWCLQ finds an optimal solution in less than one second
are omitted

high density. CPLEX is also effective for dense graphs, but it cannot solve any random instance
with more than 500 vertices.
For the 15 DIMACS instances that cannot be solved by any algorithm within the cut-off time,
we report the largest weight clique found by each solver before the algorithm terminates, which is
a lower bound of the optimal solution. Table 3 shows MWCLQ computes the best lower bounds
for 11 instances, while CPLEX gives the best lower bounds for 5 instances. MWCLQ can give a
significantly better lower bound than other MWC solvers on all instances except C2000.5, where
MWCLQ and Cliquer share the same bound. This result suggests that MWCLQ can often compute
a better approximate solution than other exact solvers within a given time.
An exact algorithm for MC or MWC usually solves an instance in two phases. In the first phase,
the algorithm finds an optimal solution. Then in the second phase, the algorithm proves that the
solution is indeed optimal by showing that no better solution exists. In Fig. 4, we compare the
runtimes that Cliquer, Dkum and MWCLQ need to find an optimal solution of a DIMACS instance.
822

fiA N E XACT A LGORITHM

FOR

M AXIMUM W EIGHT C LIQUE

For simplicity, the instances for which all solvers find an optimal solution within 100 seconds or
MWCLQ finds an optimal solution in less than one second are omitted. MWCLQ always finds
an optimal solution much faster than other solvers on all instances except hamming10-2. For few
instances such as brock800 2 and brock800 4, although MWCLQ spends more time than Cliquer
for exactly solving them (see Table 1), it finds an optimal solution 10 times faster than Cliquer.
In summary, both Table 3 and Fig. 4 show that MWCLQ generally finds an optimal solution
much faster than other solvers, although sometimes it may spend more time to prove the optimality.
4.3 Comparison of Different Encodings of MWC into MaxSAT
We presented four encodings from MWC into MaxSAT, namely the direct encoding, the split encoding, the iterative split encoding and the LW encoding. When an MWC instance is encoded into
a MaxSAT instance using the first three encodings, a MaxSAT solver can be used to search for an
optimal solution of the MWC instance. However, if the MWC instance is encoded into a MaxSAT
instance using the LW encoding, the optimal solution of the MaxSAT instance is not an MWC, but
an upper bound for the MWC. Therefore, when a MaxSAT solver is used to solve an MWC instance,
only the first three encodings can be used.
Different from MaxSAT solvers, MWCLQ uses MaxSAT reasoning just in the upper bounding procedure at each search tree node, where the current subgraph is dynamically encoded into
a MaxSAT instance. So, all the four encodings could be used in MWCLQ to encode the current
subgraph.
Recall that MWCLQ is based on the LW encoding. We implemented three other versions of
MWCLQ, i.e., MWCLQdir , MWCLQsp and MWCLQit, which are identical to MWCLQ, but are
based on the direct encoding, the split encoding and the iterative split encoding, respectively. In a
MaxSAT instance obtained using the direct encoding, all inconsistent subsets only contain two soft
clauses, because all soft clauses are unit. When MWCLQdir detects an inconsistent subset such
as {(x1 , w(x1 )), (x2 , w(x2 ))}, the most weighted clause in the subset, say, (x2 , w(x2 )), is split
into (x2 , w(x1 )) and (x2 , w(x2 )w(x1 )). Then the upper bound is improved by w(x1 ), and the
clause (x2 , w(x2 )w(x1 )) can be used for further detection. In a MaxSAT instance obtained using
the split and the iterative split encoding, all vertices in the same soft clause have the same weight.
Observe that the Top-k literal failed clause and the Top-k empty clause do not make sense, and the
(k, )-Rule is not needed in the three encoding.
In this experiment, we ran the following four different kinds of state-of-the-art MaxSAT solvers
to solve the MWC instances encoded into MaxSAT using the direct encoding, the split encoding
and the iterative split encoding.
1. akMaxSat (Kugel, 2010) is a branch-and-bound MaxSAT solver that computes the lower
bound with a combination of MaxSAT resolution and detection of disjoint inconsistent subsets. One of its authors sent us the source code submitted to the MaxSAT evaluation 2012.
2. MaxSatz (Li et al., 2007) is a branch-and-bound MaxSAT solver that incorporates some SAT
technologies. We used the latest version MaxSatz2013, which is one of the best solvers in the
MaxSAT evaluation 2013.
3. WPM1-2013 (Ansotegui, Bonet, & Levy, 2009) is a MaxSAT solver based on successive calls
to a SAT solvers. One of its authors provided us an executable file of WPM which is used in
the MaxSAT evaluation 2013.
823

fiFANG , L I , & XU

4. MaxHS (Davies & Bacchus, 2013b) is a hybrid Maxsat solver that exploits both SAT and
integer programming technologies.
We report the number of instances solved by MWCLQdir , MWCLQsp, MWCLQit, MWCLQ,
akMaxSat, MaxSatz and MaxHS within 3600 seconds. The results of WPM are not reported, because it solves only 3 DIMACS instances and does not solve any BHOSLIB instance. In addition,
it always runs out of memory when solving random instances.

Table 4: Number of instances solved within 3600 seconds by MaxSAT solvers using three different
encodings and four versions of MWCLQ. Direct stands for the direct encoding, Split is for
the split encoding and Iter is for the iterative split encoding.
Instance
Benchmark
#
brock
12
c-fat
7
C
7
DSJC
2
gen
5
hamming
6
johnson
4
keller
3
MANN
4
p hat
15
san
15
DIMACS: 80
BHOSLIB: 40
(150,0.9)
50
(150,0.95)
50
(200,0.8)
50
(200,0.9)
50
(200,0.95)
50
(300,0.7)
50
(300,0.8)
50
(300,0.9)
50
(500,0.6)
50
(500,0.7)
50
(600,0.6)
50
(600,0.7)
50
(700,0.6)
50
(700,0.7)
50
RAND: 700

akMaxSat
Direct
Split
Iter
4
4
4
7
7
7
2
2
2
0
0
0
2
2
2
5
5
5
3
3
3
1
1
1
1
1
1
5
4
5
7
9
9
37
38
39
0
0
1
43
50
50
46
50
50
38
50
50
44
50
50
44
50
50
30
48
50
14
7
9
8
8
8
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
267
313
317

Direct
4
7
2
1
2
5
3
1
2
7
8
42
0
50
49
49
49
49
50
47
2
0
0
0
0
0
1
346

MaxSatz
Split
4
7
2
1
2
5
3
1
2
5
8
40
0
50
50
50
50
50
50
19
0
0
0
0
0
0
0
319

Iter
4
7
2
1
2
5
3
1
2
7
8
42
5
50
50
50
50
50
50
18
0
0
0
0
0
0
0
318

Direct
4
7
2
0
3
5
4
1
3
4
13
46
3
50
50
50
50
50
50
41
35
0
0
0
0
0
0
376

MaxHS
Split
0
7
1
0
0
4
3
0
2
1
4
22
0
7
50
0
0
14
0
0
0
0
0
0
0
0
0
71

Iter
0
7
1
0
0
3
2
0
2
0
3
18
0
7
50
0
0
14
0
0
0
0
0
0
0
0
0
71

MWCLQdir
4
7
1
2
0
3
3
1
1
8
6
36
0
50
0
50
0
0
50
50
0
50
49
50
0
50
0
399

MWCLQsp
8
7
2
2
2
4
3
1
1
8
10
48
0
50
50
50
41
25
50
50
0
50
41
50
0
50
0
507

MWCLQit
8
7
2
2
2
4
3
1
1
9
14
53
5
50
50
50
50
50
50
50
29
50
50
50
39
50
0
618

MWCLQ
12
7
2
2
2
5
3
1
1
11
15
61
5
50
50
50
50
50
50
50
50
50
50
50
50
50
50
700

Experimental results in Table 4 suggest that encoding approaches do affect the performances of
MaxSAT solvers to solve an MWC instance. However, the effectiveness of the direct encoding, the
split encoding and the iterative encoding for different MaxSAT solvers are not clear. Concretely, the
iterative split encoding makes akMaxSat a little faster than other encodings do. MaxHS using the
direct encoding dominates MaxHS using other encodings. The iterative split encoding is a better
choice for MaxSatz to solve BHOSLIB instances, but the direct encoding is better to solve random
graphs. These results also show that MWCLQ significantly outperforms MaxSAT solvers to solve
an MWC instance. We observe that although MaxSAT reasoning is powerful in improving the upper
bound in a BnB algorithm for MC or MWC, using a MaxSAT solver to solve an MWC instance is
not effective.
824

fiA N E XACT A LGORITHM

FOR

M AXIMUM W EIGHT C LIQUE

Thanks to the extended MaxSAT reasoning, MWCLQ significantly outperforms MWCLQdir ,
MWCLQsp and MWCLQit . MWCLQdir and MWCLQsp are slow in solving the MWC instances
because they cannot capture the graph structure well. Although the iterative split encoding in
MWCLQit can capture the graph structure, MWCLQit does not exploit the power of MaxSAT
reasoning efficiently, because the clause splittings in MWCLQit are not driven by MaxSAT reasoning.
The advantages of MWCLQ can be described as follows: (1) it is a BnB algorithm for MWC that
is able to exploit the graph structure, especially by ordering vertices and by partitioning the graph
into independent sets at each search tree node, that a MaxSAT solver does not do; (2) it exploits
the power of MaxSAT reasoning in the upper bounding procedure to derive a tight upper bound,
that a classical BnB algorithm for MWC does not do. Nevertheless, to make MaxSAT reasoning
beneficial, a relevant encoding from MWC into MaxSAT is necessary, as shown in Table 4. The
encoding has an obvious impact on the performance of MWCLQ.
4.4 Effectiveness of the Upper Bound Based on Extended MaxSAT Reasoning
To study the impact of extended MaxSAT reasoning with soft clause splitting, we implemented
two derived versions of MWCLQ, namely, MWCLQ-- and MWCLQ-. MWCLQ-- is identical to
MWCLQ except that it only uses the trivial bound based on the independent partition of the graph,
which is equal to the sum of the largest weight in each independent set. MWCLQ- is identical to
MWCLQ except it only detects the inconsistent subsets of soft clauses, but does not use the -Rule
and the (k, )-Rule to split clauses in the subset.
Table 5 shows runtimes (in seconds) and search tree sizes (in thousands) of MWCLQ--, MWCLQand MWCLQ for solving the DIMACS and BHOSLIB instances, as well as the gain ratio of
MWCLQ- and MWCLQ compared with MWCLQ-- in terms of runtime and search tree size computed as MWCLQ--/MWCLQ- and MWCLQ--/MWCLQ, respectively. MaxSAT reasoning without
soft clause splitting in MWCLQ- makes MWCLQ- better than MWCLQ-- in terms of search tree
size. But the reduction of search tree size is not enough in general to compensate the overhead of
MaxSAT reasoning, so that the gain of MWCLQ- compared with MWCLQ-- in terms of runtime is
not clear. However, the soft clause splitting using the -Rule and the (k, )-Rule allows MaxSAT reasoning to prune much more search space, making MWCLQ substantially (from 1.08 to 4.05 times)
faster than MWCLQ--. In fact, MWCLQ solves 3 instances more than MWCLQ-- and MWCLQ-,
and significantly outperforms them on all instances except san1000. Observe that MWCLQ-- and
MWCLQ- solve the same number of instances as Minsatz (58) on the DIMACS benchmark, and
solve more instances than Cliquer (52), Dkum (48) and CPLEX (44).
Table 6 shows mean runtimes (in seconds) and mean search tree sizes (in thousands) of different
versions of MWCLQ for the random instances, averaged over the solved instances at each point, as
well as the gain ratio of MWCLQ and MWCLQ- compared with MWCLQ--. MWCLQ solves all
instances. However, neither MWCLQ-- nor MWCLQ- can solve all graphs at the points (300, 0.90)
and (700, 0.7). MaxSAT reasoning allows MWCLQ- to prune more search space than MWCLQ-on all instances, but the overhead makes MWCLQ- slower on instances with more than 300 vertices. However, similarly as in DIMACS and BHOSLIB cases, MWCLQ is from 1.2 to 14.3 times
faster than MWCLQ--, because the soft clause splittings using the -Rule and the (k, )-Rule allow
MWCLQ to substantially reduce the search space on all instances. Furthermore, the gain in terms
of both runtime and search tree size increases with the density of graph.
825

fiFANG , L I , & XU

Table 5: Runtimes (in seconds) and search tree sizes (in thousands) of different versions of MWCLQ for solving the DIMACS and BHOSLIB instances, as well as the gain ratio of MWCLQ and MWCLQ- compared with MWCLQ--. The cut-off time is 3600 seconds. -
means that the solver cannot solve the instance within the cut-off time. Instances solved
within 10 seconds by all solvers or not solved by any within the cut-off time are omitted.
Benchmark
Graph
brock400 1
brock400 2
brock400 3
brock400 4
brock800 1
brock800 2
brock800 3
brock800 4
C250.9
DSJC1000.5
gen200 p0.9 44
gen200 p0.9 55
hamming10-2
p hat1000-2
p hat500-3
p hat700-2
san1000
san200 0.9 3
san400 0.7 3
san400 0.9 1
sanr200 0.9
sanr400 0.7
frb30-15-1
frb30-15-2
frb30-15-3
frb30-15-4
frb30-15-5

MWCLQ-Time
Size
209.0
63153
203.5
58770
173.4
50079
131.5
37420
1813
421259
2563
648486
1968
470594
2623
675790
159.4
31542
87.73
26154
27.83
7560
10.41
2530
3009
457787
113.4
17715
135.9
31173
56.31
15955
9.79
2936
25.90
6690
35.71
12228
411.3
156311
42.28
13416
171.8
67246
267.9
83139
85.27
22362

Time
234.3
227.7
194.2
146.3
1990
2695
2201
2931
160.5
91.87
23.12
8.70
2813
117.5
137.2
48.47
10.53
20.10
39.60
394.8
39.17
167.0
257.3
82.23

MWCLQGain
Size
0.89
54505
0.89
51244
0.89
43616
0.90
32853
0.91
396425
0.95
605433
0.89
442518
0.89
633367
0.99
20114
0.95
25207
1.20
3692
1.20
1310
1.07
238949
0.97
10571
0.99
30680
1.16
7946
0.93
2403
1.29
3001
0.90
10797
1.04
108476
1.08
9287
1.03
48401
1.04
63411
1.04
17705

Gain
1.16
1.15
1.15
1.14
1.06
1.07
1.06
1.07
1.57
1.04
2.05
1.93
1.92
1.68
1.02
2.01
1.22
2.23
1.13
1.44
1.44
1.39
1.31
1.26

Time
129.3
127.7
107.2
81.81
1427
2002
1545
2039
39.99
81.30
7.00
2.76
15.46
2501
916.7
47.88
183.8
16.19
6.37
1257
6.40
25.57
244.9
30.14
131.7
181.8
57.31

MWCLQ
Gain
Size
1.62
25367
1.59
23788
1.62
19830
1.61
15133
1.27
223715
1.28
339910
1.27
248218
1.29
352700
3.99
4472
1.08
17032
3.98
979
3.77
344
41758
176892
3.28
77115
2.37
4186
0.74
21694
3.48
2180
1.54
1213
63106
4.05
850
1.40
5705
1.68
27928
1.40
2597
1.30
14589
1.47
16336
1.49
5575

Gain
2.49
2.47
2.53
2.47
1.88
1.91
1.90
1.92
7.05
1.54
7.72
7.34
5.94
4.23
1.44
7.32
2.42
7.87
2.14
5.60
5.17
4.61
5.09
4.01

Table 6: Runtimes (in seconds) and search tree sizes (in thousands) of different versions of MWCLQ for the random instances, averaged over the solved instances at each point, as well as
the gain ratio of MWCLQ and MWCLQ- compared with MWCLQ--. The cut-off time is
3600 seconds for a solver to solve one instance.
Graph
|V |
D
150
0.90
150
0.95
200
0.80
200
0.90
200
0.95
300
0.70
300
0.80
300
0.90
500
0.60
500
0.70
600
0.60
600
0.70
700
0.60
700
0.70

#
50
50
50
50
50
50
50
31
50
50
50
50
50
31

MWCLQ-Time
Size
1.85
715
5.23
1440
1.71
725
49.41
14030
405.8
81772
1.58
552
27.09
7655
2261
399832
7.53
2310
133.8
34848
42.85
13483
1229
320607
94.10
24897
3177
682683

#
50
50
50
50
50
50
50
33
50
50
50
50
50
15

Time
1.34
1.28
1.89
40.13
117.1
1.80
30.98
2261
8.12
149.5
46.40
1390
104.8
3343

MWCLQGain
Size
1.38
255
4.09
123
0.90
539
1.23
6109
3.47
8583
0.88
493
0.87
6265
1.00
245004
0.93
2176
0.89
31905
0.92
12661
0.88
292627
0.90
23624
0.95
575774

826

Gain
2.81
11.69
1.34
2.30
9.53
1.12
1.22
1.63
1.06
1.09
1.06
1.10
1.05
1.19

#
50
50
50
50
50
50
50
50
50
50
50
50
50
50

Time
0.58
0.42
0.99
14.30
28.77
1.13
14.87
845.4
6.03
93.36
34.79
869.9
78.24
2434

MWCLQ
Gain
Size
3.19
108
12.45
46
1.73
245
3.46
2031
14.10
2454
1.40
263
1.82
2611
2.67
85676
1.25
1314
1.43
16326
1.23
7667
1.41
149522
1.20
14219
1.31
351894

Gain
6.58
30.69
2.96
6.91
33.32
2.10
2.93
4.67
1.76
2.13
1.76
2.14
1.75
1.94

fiA N E XACT A LGORITHM

FOR

M AXIMUM W EIGHT C LIQUE

4.5 Application of MWCLQ for the Winner Determination Problem
An important application of MWC is to solve the winner determination problem (WDP) in combinatorial auctions. The auctioneer has a set of m items, M = {1, 2, . . . , m}, to sell, and the buyers
submit a set of n bids, B = {B1 , B2 , . . . , Bn }. A bid is a pair Bi = (Si , Pi ), where Si  M is
a subset of items and Pi  0 is a price for all items in Si . The WDP problem is to determine the
bids as winning or losing so as to maximum the auctioneers revenue, such that each item can be
allocated to at most one bidder. We define an MWC instance G = (V, E, w) for the WDP instance
as follows.
 For each bid Bi  B, define a vertex vi with weight w(vi ) = Pi , i.e., V = {v1 , v2 , . . . , vn }
and w(vi ) = Pi ;
 Add an edge {vi , vj } to G if and only if Bi and Bj do not share common items, i.e., E =
{{vi , vj } | Si  Sj = , 1  i < j  n}.
A maximum weight clique of G corresponds to a feasible subset of bids with a maximum revenue.
In this experiment, we compared MWCLQ with Cliquer, Dkum, MinSatz and CPLEX on realistic WDP instances. We also selected MaxHS using the direct encoding, which is the most
effective MaxSAT solver to solve an MWC instance, in this comparison. We used the benchmark
provided by Lau and Goh (2002), which has been widely used as benchmark purpose to test WDP
algorithms (Guo, Lim, Rodrigues, & Zhu, 2006; Sghir, Hao, Jaafar, & Ghedira, 2014; Wu & Hao,
2015). Instances in this benchmark are generated by incorporating the following factors, i.e., a pricing factor which models a bidders acceptable price range for each bid, a preference factor which
takes into account bidders preferences among bids, and a fairness factor which measures the fairness in distributing items among bidders. The benchmark contains 500 instances with up to 1500
items and 1500 bids, which can be divided into 5 groups by the item number and the bid number.
Each group contains 100 instances labeled as REL-m-n, where m is the number of items and n is
the number of bids.
Table 7: Mean runtimes in seconds on WDP instances, obtained by solving 100 graphs of each
group. The cut-off time is 3600 seconds. The mean runtime is marked by - if no instance
is solved in the group within the cut-off time. # stands for the number of instances solved
by the solver.
Benchmark
Group
REL-500-1000
REL-1000-1000
REL-1000-500
REL-1000-1500
REL-1500-1500

#
100
100
100
100
100

Cliquer
Time
#
809.9
100
3.73
100
0.03
100
2.84
100
3.38
100

DKum
Time
#
628.3
100
4.52
100
0.03
100
3.55
100
5.30
100

MinSatz
Time
#
552.5
100
25.39
100
0.81
100
67.90
100
78.19
100

MaxHS
Time
#
0
0
1003.7
100
0
0

CPLEX
Time
#
0
0
266.7
100
0
0

MWCLQ
Time
#
55.61
100
1.45
100
0.05
100
1.56
100
2.42
100

Table 7 summarizes the mean runtimes and numbers of instances solved within the cut-off time
by group. Results show that MWCLQ outperforms other solvers on all groups except REL-1000500, where Cliquer, Dkum, MinSatz and MWCLQ are comparable. MWCLQ achieves at least
10X speedup for the instances in the hardest group REL-500-1000. MaxHS and CPLEX are not
effective in solving these instances. In our experiment, we transformed WDP to MWC first and
827

fiFANG , L I , & XU

then formulated it with integer programming. Another method is used by Wu and Hao (2015)
to formulate WDP directly into integer programming. The method appears to be slightly more
effective than the transformation in our experiment, but it does not affect our comparison, because
the only instances CPLEX is able to solve within 3600 seconds are also from REL-1000-500 with
this transformation.
Table 8 reports the detailed comparative results on the first 10 instances from each group. MWCLQ outperforms other solvers on all instances except the 10 instances from the easiest group
(in401, in402, ..., in410), which can be solved by Cliquer, DKum, MinSatz and MWCLQ within
less than one second. These results suggest that MWCLQ is more effective in solving MWC instances from WDP than other specific MWC algorithms, MinSatz, MaxHS and CPLEX. Moreover,
MWCLQ is even more efficient than some state-of-the-art heuristic algorithms on some relatively
hard instances. For example, MWCLQ solves in108 in 101.04 seconds, while the heuristic algorithm (Wu & Hao, 2015), based on tabu search takes, 113.53 seconds to find the solution with
probability 0.73. Ignoring the difference of running environments, MWCLQ is faster than the tabu
search algorithm on in108. Note that heuristic algorithms can only give a feasible solution, but
cannot guarantee the optimality.

5. Conclusion
MaxSAT reasoning has been proved to be very effective for the MC problem, based on a partition of
the graph into independent sets. However, MaxSAT reasoning cannot be naturally extended to solve
the MWC problem because of the literal weights, as shown by the relatively poor performance of
MWCLQ- , MWCLQdir , MWCLQsp and MWCLQit. MWCLQ- exploits MaxSAT reasoning but
does not deal with literal weights. The encodings from MWC into MaxSAT used in MWCLQdir
and MWCLQsp do not capture well the graph structure. Although MWCLQit can exploit the graph
structure, it does too many useless splits and has difficulties to take advantage of MaxSAT reasoning
efficiently. We thus propose to encode an MWC instance into a literal-weighted MaxSAT instance,
in which both soft clauses and literals in soft clauses are weighted. The optimal solution of an
LW MaxSAT instance is not an MWC, but an upper bound for the MWC. The interest of the LW
encoding is that we can transform an LW MaxSAT instance so that the optimal solution of the new
instance is a tighter upper bound for the MWC.
Concretely, at every search tree node of a BnB algorithm for MWC, we partition the current
subgraph into independent sets and obtain an LW MaxSAT instance, in which each soft clause
corresponds to an independent set. Then we successively transform the LW MaxSAT instance
by identifying the Top-k literal failed clause and the Top-k empty clause and by using the -Rule
and the (k, )-Rule. Consequently, we obtain a tight upper bound for MWC to prune the search
space. This approach is implemented in MWCLQ, which is substantially better than MWCLQ-,
MWCLQdir , MWCLQsp and MWCLQit, confirming the effectiveness of the approach. MWCLQ
is also favorably compared with the state-of-the-art MWC solvers as Cliquer, DKum, MinSatz and
CPLEX, as well as several state-of-the-art MaxSAT solvers using different encodings, on standard
benchmarks and instances from realistic applications.
In the future, we plan to study the impact of vertex ordering and unit clause ordering in MWCLQ. It is also interesting to use MaxSAT reasoning to solve other combinatorial optimization
problems, especially the weighted version, using a dedicated encoding.
828

fiA N E XACT A LGORITHM

FOR

M AXIMUM W EIGHT C LIQUE

Table 8: Runtimes in seconds on the first 10 instances of each group in the benchmark in Table 7.
The cut-off time is 3600 seconds. |V | stands for the number of vertices, D for the density
of the graph after transforming WDP into MWC and v for the optimal solution of MWC.
When a solver cannot solve an instance within 3600 seconds, its runtime is marked by -
Graph
in101
in102
in103
in104
in105
in106
in107
in108
in109
in110
in201
in202
in203
in204
in205
in206
in207
in208
in209
in210
in401
in402
in403
in404
in405
in406
in407
in408
in409
in410
in501
in502
in503
in504
in505
in506
in507
in508
in509
in510
in601
in602
in603
in604
in605
in606
in607
in608
in609
in610

|V |
1000
1000
1000
1000
1000
1000
1000
1000
1000
1000
1000
1000
1000
1000
1000
1000
1000
1000
1000
1000
500
500
500
500
500
500
500
500
500
500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500

D
0.31
0.30
0.31
0.30
0.30
0.30
0.30
0.31
0.30
0.30
0.15
0.16
0.16
0.17
0.16
0.16
0.17
0.16
0.16
0.16
0.15
0.15
0.16
0.17
0.17
0.14
0.17
0.16
0.14
0.17
0.09
0.08
0.08
0.08
0.08
0.08
0.08
0.07
0.08
0.08
0.09
0.09
0.09
0.10
0.10
0.10
0.10
0.10
0.10
0.11

v
72724.617
72518.222
72129.500
72709.646
75646.127
71258.613
69713.403
75813.205
69475.895
68295.289
81557.742
90708.127
86239.214
87075.428
86515.951
91518.964
93129.248
94904.679
87268.965
89962.396
77417.482
76273.336
74843.957
78761.690
75915.900
72863.324
76365.717
77018.833
73188.619
73791.658
88656.958
86236.911
87812.377
85600.001
84860.165
84623.414
90288.472
86853.500
88316.087
89014.137
108800.445
105611.476
105121.021
107733.805
109840.984
107113.067
113180.284
105266.107
109472.332
113716.965

Cliquer
819.6
414.9
551.4
330.8
840.3
272.7
595.9
1321
247.3
308.5
1.80
3.73
3.63
6.13
3.11
2.37
3.95
4.07
2.13
3.54
0.02
0.02
0.03
0.05
0.05
0.02
0.05
0.04
0.01
0.04
4.64
1.69
3.44
2.37
2.16
1.41
2.82
1.24
3.59
1.29
4.52
2.01
1.84
3.86
3.72
2.72
4.20
2.21
2.77
6.14

DKum
616.1
289.0
455.5
218.1
547.3
346.4
460.1
1121
238.8
307.7
2.47
4.98
6.65
7.20
4.85
3.37
5.69
5.18
3.38
4.78
0.03
0.02
0.04
0.08
0.08
0.02
0.07
0.04
0.02
0.06
5.68
2.30
4.42
3.80
2.82
2.32
3.73
1.60
4.37
2.19
5.51
2.98
2.56
5.71
5.75
4.37
6.65
3.72
3.78
10.03

829

MinSatz
610.8
308.8
479.6
353.5
367.7
270.7
736.9
924.9
305.0
411.7
22.09
25.90
26.66
30.47
27.42
22.85
28.18
22.09
26.66
22.09
0.69
0.72
0.76
0.89
0.88
0.75
0.79
0.82
0.71
0.85
81.25
65.16
72.06
61.32
62.09
65.92
68.99
58.26
64.39
64.39
76.04
70.61
65.96
84.58
75.27
77.60
78.37
77.60
69.84
94.67

MaxHS
923.01
1107.2
891.23
1082.9
1300.6
992.34
1569.1
1187.2
846.56
1132.1
-

CPLEX
181.70
198.12
229.0
208.97
224.98
175.15
364.58
199.39
217.31
243.42
-

MWCLQ
53.87
34.40
44.94
37.00
37.33
23.56
68.07
101.04
29.10
43.16
1.01
1.37
1.57
2.01
1.64
1.11
1.69
1.05
1.54
1.21
0.04
0.04
0.04
0.06
0.06
0.05
0.06
0.06
0.04
0.06
2.30
1.45
1.72
1.56
1.56
1.34
1.32
1.11
1.80
1.15
2.39
1.75
1.35
3.09
2.22
2.13
2.38
2.17
1.86
3.70

fiFANG , L I , & XU

6. Acknowledgments
We would like to thank anonymous reviewers for their helpful comments and suggestions. We also
thank Jichang Zhao, Qiao Kan, Xu Feng and Shaowei Cai for their proofreads and suggestions. Part
of this work was done while the first author was a joint Ph.D. student at Universite de Picardie Jules
Verne. This research was partly supported by NSFC (Grant No. 61421003), the fund of the State
Key Lab of Software Development Environment (Grant No. SKLSDE-2015ZX-05), the Chinese
State Key Laboratory of Software Development Environment Open Fund (Grant No. SKLSDE2012KF-07), and the MeCS platform of Universite de Picardie Jules Verne.

References
Abrame, A., & Habet, D. (2014). Efficient application of max-sat resolution on inconsistent subsets.
In Proc. of CP-2014, pp. 92107. Springer.
Abrame, A., & Habet, D. (2015). On the resiliency of unit propagation to max-resolution. In Proc.
of AAAI-2015, pp. 268274. AAAI Press.
Ansotegui, C., Bonet, M. L., & Levy, J. (2009). Solving (weighted) partial maxsat through satisfiability testing. In Theory and Applications of Satisfiability Testing-SAT 2009, pp. 427440.
Ansotegui, C., Bonet, M. L., & Levy, J. (2013). Sat-based maxsat algorithms. Artificial Intelligence,
196, 77105.
Ansotegui, C., & Gabas, J. (2013). Solving (weighted) partial maxsat with ILP. In Proc. of CPAIOR2013, pp. 403409.
Benlic, U., & Hao, J. K. (2013). Breakout local search for maximum clique problems. Computers
& Operations Research, 40, 192206.
Bonet, M. L., Levy, J., & Manya, F. (2006). A complete calculus for max-sat. In Theory and
Applications of Satisfiability Testing-SAT 2006, pp. 240251. Springer.
Cai, S., Su, K., Luo, C., & Sattar, A. (2013). NuMVC: An efficient local search algorithm for
minimum vertex cover. Journal of Artificial Intelligence Research, 46, 687716.
Cai, S., Su, K., & Sattar, A. (2011). Local search with edge weighting and configuration checking
heuristics for minimum vertex cover. Artificial Intelligence, 175(9), 16721696.
Chen, J. (2010). A new SAT encoding of the at-most-one constraint. In International Workshop on
Modelling and Reformulating Constraint Satisfaction Problems.
Davies, J., & Bacchus, F. (2013a). Exploiting the power of mip solvers in maxsat. In Theory and
Applications of Satisfiability TestingSAT 2013, pp. 166181. Springer.
Davies, J., & Bacchus, F. (2013b). Postponing optimization to speed up MAXSAT solving. In Proc.
of CP-2013, pp. 247262. Springer.
Downey, R. G., & Fellows, M. R. (1995). Fixed-parameter tractability and completeness I: Basic
results. SIAM Journal on Computing, 24(4), 873921.
Fahle, T. (2002). Simple and fast: Improving a branch-and-bound algorithm for maximum clique.
In Proc. of ESA-2002, pp. 485498.
Fang, Z., Chu, Y., Qiao, K., Feng, X., & Xu, K. (2014a). Combining edge weight and vertex weight
for minimum vertex cover problem. In Frontiers in Algorithmics, pp. 7181. Springer.
830

fiA N E XACT A LGORITHM

FOR

M AXIMUM W EIGHT C LIQUE

Fang, Z., Li, C. M., Qiao, K., Feng, X., & Xu, K. (2014b). Solving maximum weight clique using
maximum satisfiability reasoning. In Proc. of ECAI-2014, Vol. 263, pp. 303  308.
Feige, U. (2004). Approximating maximum clique by removing subgraphs. SIAM Journal on
Discrete Mathematics, 18(2), 219225.
Freeman, J. W. (1995). Improvements to propositional satisfiability search algorithms. Ph.D. thesis.
Guo, Y., Lim, A., Rodrigues, B., & Zhu, Y. (2006). Heuristics for a bidding problem. Computers &
operations research, 33(8), 21792188.
Ignatiev, A., Morgado, A., & Marques-Silva, J. (2014). On reducing maximum independent set to
minimum satisfiability. In Theory and Applications of Satisfiability TestingSAT 2014, pp.
103120. Springer.
Karp, R. M. (1972). Reducibility among combinatorial problems. In Complexity of Computer
Computations, pp. pp 85103. Springer.
Kibanov, M., Atzmueller, M., Scholz, C., & Stumme, G. (2014). Temporal evolution of contacts
and communities in networks of face-to-face human interactions. Science China Information
Sciences, 57(3), 117.
Konc, J., & Janezic, D. (2007). An improved branch and bound algorithm for the maximum clique
problem. Communications in Mathematical and in Computer Chemistry, 58, 569590.
Kugel, A. (2010). Improved exact solver for the weighted Max-SAT problem. In Workshop Pragmatics of SAT, Vol. 436.
Kugel, A. (2012). Natural Max-SAT encoding of Min-SAT. In Learning and Intelligent Optimization, pp. 431436. Springer.
Kumlander, D. (2004). A new exact algorithm for the maximum-weight clique problem based on a
heuristic vertex-coloring and a backtrack search. In Proc. of MOC-2004, pp. 202208.
Kumlander, D. (2008a) http://www.kumlander.eu/graph/index.html.
Kumlander, D. (2008b). On importance of a special sorting in the maximum-weight clique algorithm
based on colour classes. In Modelling, computation and optimization in information systems
and management sciences, pp. 165174. Springer.
Larrosa, J., Heras, F., & de Givry, S. (2008). A logical approach to efficient max-sat solving.
Artificial Intelligence, 172(2), 204233.
Lau, H. C., & Goh, Y. G. (2002). An intelligent brokering system to support multi-agent web-based
4 th-party logistics. In Proc. of ICTAI-2002, pp. 154161. IEEE.
Li, C. M., & Anbulagan, A. (1997). Heuristics based on unit propagation for satisfiability problems.
In Proc. of the IJCAI-1997, pp. 366371. Morgan Kaufmann Publishers Inc.
Li, C. M., Fang, Z., & Xu, K. (2013). Combining MaxSAT reasoning and incremental upper bound
for the maximum clique problem. In Proc. of ICTAI-2013, pp. 939946. IEEE.
Li, C. M., Manya, F., & Planes, J. (2005). Exploiting unit propagation to compute lower bounds in
branch and bound max-sat solvers. In Proc. of CP-2005, Vol. 3709, pp. 403414. Springer.
Li, C. M., Manya, F., & Planes, J. (2007). New inference rules for Max-SAT. Journal of Artificial
Intelligence Research, 30, 321359.
831

fiFANG , L I , & XU

Li, C. M., & Manya, F. (2009). Maxsat, hard and soft constraints.. Handbook of satisfiability, 185,
613631.
Li, C. M., Manya, F., & Planes, J. (2006). Detecting disjoint inconsistent subformulas for computing
lower bounds for max-sat. In Proc. of AAAI-2006, Vol. 6, pp. 8691.
Li, C. M., & Quan, Z. (2010a). Combining graph structure exploitation and propositional reasoning
for the maximum clique problem. In Proc. of ICTAI-2010, Vol. 1, pp. 344351. IEEE.
Li, C. M., & Quan, Z. (2010b). An efficient branch-and-bound algorithm based on MaxSAT for the
maximum clique problem. In Proc. of AAAI-2010, pp. 128133.
Li, C. M., Zhu, Z., Manya, F., & Simon, L. (2012). Optimizing with minimum satisfiability. Artificial Intelligence, 190, 3244.
Liu, T., Lin, X., Wang, C., Su, K., & Xu, K. (2011). Large hinge width on sparse random hypergraphs. In Proc. of IJCAI-2011, Vol. 2011, pp. 611616.
Ma, T., & Latecki, L. J. (2012). Maximum weight cliques with mutex constraints for video object
segmentation. In Proc. of CVPR-2012, pp. 670677. IEEE.
Martins, R., Joshi, S., Manquinho, V., & Lynce, I. (2014). Incremental cardinality constraints for
maxsat. In Proc. of CP-2014, pp. 531548. Springer.
Mascia, F., Cilia, E., Brunato, M., & Passerini, A. (2010). Predicting structural and functional sites
in proteins by searching for maximum-weight cliques. In Proc. of AAAI-2010, pp. 12741279.
AAAI.
Morgado, A., Dodaro, C., & Marques-Silva, J. (2014). Core-guided maxsat with soft cardinality
constraints. In Proc. of CP-2014, pp. 564573. Springer.
Morgado, A., Heras, F., Liffiton, M., Planes, J., & Marques-Silva, J. (2013). Iterative and coreguided maxsat solving: A survey and assessment. Constraints, 18(4), 478534.
Ostergard, P. (2001). A new algorithm for the maximum-weight clique problem. Nordic Journal of
Computing, 8, 424436.
Ostergard, P. (2002). A fast algorithm for the maximum clique problem. Discrete Applied Mathematics, 120, 197207.
Ostergard, P. (2010). Cliquer source code. http://users.tkk.fi/pat/cliquer.html.
Pullan, W. (2008). Approximating the maximum vertex/edge weighted clique using local search.
Journal of Heuristics, 19, 117134.
Pullan, W., & Hoos, H. H. (2006). Dynamic local search for the maximum clique problem. Journal
of Artificial Intelligence Research, 25, 159185.
Regin, J. C. (2003). Solving the maximum clique problem with constraint programming. In Proc.
of CPAIOR-2003, pp. 634648.
Sghir, I., Hao, J.-K., Jaafar, I. B., & Ghedira, K. (2014). A recombination-based tabu search algorithm for the winner determination problem. In Artificial Evolution, pp. 157167. Springer.
Shimizu, S., Yamaguchi, K., Saitoh, T., & Masuda, S. (2012). Some improvements on Kumlanders
maximum weight clique extraction algorithm. In Proc. of the International Conference on
Electrical, Computer, Electronics and Communication Engineering, pp. 307311.
832

fiA N E XACT A LGORITHM

FOR

M AXIMUM W EIGHT C LIQUE

Shimizu, S., Yamaguchi, K., Saitoh, T., & Masuda, S. (2013). Optimal table method for finding the
maximum weight clique. In Proc. of the 13th International Conference on Applied Computer
Science, No. 12. WSEAS.
ThanhVu, H. N., & Thang, B. (2014). DIMACS benchmark. https://turing.cs.hbg.psu.
edu/txn131/clique.html.
Tomita, E., & Kameda, T. (2007). An efficient branch-and-bound algorithm for finding a maximum
clique with computational experiments. Journal of Global Optimization, 37, 95111.
Tomita, E., & Seki, T. (2003). An efficient branch-and-bound algorithm for finding a maximum
clique. In Proc. Discrete Mathematics and Theoretical Computer Science, Vol. 2731, pp.
278289.
Wu, Q., Hao, J. K., & Glover, F. (2012). Multi-neighborhood tabu search for the maximum weight
clique problem. Annals of Operations Research, 196, 611634.
Wu, Q., & Hao, J.-K. (2015). Solving the winner determination problem via a weighted maximum
clique heuristic. Expert Systems with Applications, 42(1), 355365.
Xu, K., Boussemart, F., Hemery, F., & Lecoutre, C. (2007). Random constraint satisfaction: Easy
generation of hard (satisfiable) instances. Artificial Intelligence, 171, 514534.
Xu, K., & Li, W. (2000). Exact phase transitions in random constraint satisfaction problems. Journal
of Artificial Intelligence Research, 12, 93103.
Xu, K. (2004). BHOSLIB: Benchmarks with hidden optimum solutions for graph problems. http:
//www.nlsde.buaa.edu.cn/kexu/benchmarks/graph-benchmarks.htm.
Xu, K., & Li, W. (2006). Many hard examples in exact phase transitions. Theoretical Computer
Science, 355(3), 291302.
Yamaguchi, K., & Masuda, S. (2008). A new exact algorithm for the maximum weight clique
problem. In Proc. of ITC-CSCC-2008, pp. 317320.
Zhang, D., Javed, O., & Shah, M. (2014a). Video object co-segmentation by regulated maximum
weight cliques. In Proc. of ECCV-2014, pp. 551566. Springer.
Zhang, W., Nie, L., Jiang, H., Chen, Z., & Liu, J. (2014b). Developer social networks in software
engineering: construction, analysis, and applications. Science China Information Sciences,
57(12), 123.
Zhian, H., Sabaei, M., Javan, N. T., & Tavallaie, O. (2013). Increasing coding opportunities using maximum-weight clique. In Proc. of Computer Science and Electronic Engineering
Conference-2013, pp. 168173. IEEE.
Zhu, Z., Li, C. M., Manya, F., & Argelich, J. (2012). A new encoding from MinSAT into MaxSAT.
In Proc. of CP-2012, pp. 455463. Springer.
Zuckerman, D. (2006). Linear degree extractors and the inapproximability of max clique and chromatic number. In Proceedings of the 38th annual ACM symposium on Theory of computing,
pp. 681690. ACM.

833

fiJournal of Artificial Intelligence Research 55 (2016) 743-798

Submitted 06/15; published 03/16

Knowledge Representation in Probabilistic
Spatio-Temporal Knowledge Bases
Francesco Parisi

FPARISI @ DIMES . UNICAL . IT

Department of Informatics, Modeling,
Electronics and System Engineering
University of Calabria, Rende, Italy

John Grant

GRANT @ CS . UMD . EDU

Department of Computer Science and UMIACS
University of Maryland, College Park, USA

Abstract
We represent knowledge as integrity constraints in a formalization of probabilistic spatiotemporal knowledge bases. We start by defining the syntax and semantics of a formalization called
PST knowledge bases. This definition generalizes an earlier version, called SPOT, which is a
declarative framework for the representation and processing of probabilistic spatio-temporal data
where probability is represented as an interval because the exact value is unknown. We augment
the previous definition by adding a type of non-atomic formula that expresses integrity constraints.
The result is a highly expressive formalism for knowledge representation dealing with probabilistic spatio-temporal data. We obtain complexity results both for checking the consistency of PST
knowledge bases and for answering queries in PST knowledge bases, and also specify tractable
cases. All the domains in the PST framework are finite, but we extend our results also to arbitrarily
large finite domains.

1. Introduction
Recent years have seen a great deal of interest in tracking moving objects. This is a fundamental
issue for many applications providing location-based and context-aware services, such as emergency call-out assistance, live traffic reports, food and drink finder, location-based advertising, mobile tourist guidance, pervasive healthcare, and analysis of animal behavior (Ahson & Ilyas, 2010;
Petrova & Wang, 2011; Karimi, 2013). Such innovative services are becoming so widely diffused
that MarketsandMarkets forecasts that the location-based services market will grow from $8.12
billion in 2014 to $39.87 billion in 2019 (MarketsandMarkets, 2014).
An important aspect of the systems providing location-based and context-aware services is that
they need to manage spatial and temporal data together. For this reason, researchers have investigated in detail the representation and processing of spatio-temporal data, both in AI (Cohn &
Hazarika, 2001; Gabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev, 2005; Yaman, Nau,
& Subrahmanian, 2004, 2005a; Knapp, Merz, Wirsing, & Zappe, 2006) and databases (Agarwal,
Arge, & Erickson, 2003; Pelanis, Saltenis, & Jensen, 2006). However, in many cases the location
of objects is uncertain: such cases can be handled by using probabilities (Parker, Yaman, Nau, &
Subrahmanian, 2007b; Tao, Cheng, Xiao, Ngai, Kao, & Prabhakar, 2005). Sometimes the probabilities themselves are not known exactly. Indeed, the position of an object at a given time is estimated
by means of a location estimation method such as proximity (where the location of an object is
derived from its vicinity to one or more antennas), fingerprinting (where radio signal strength meac
2016
AI Access Foundation. All rights reserved.

fiPARISI & G RANT

surements produced by a moving object are matched against a radio map that is built before the
system is working), and dead reckoning (where the position of an object is derived from the last
known position, assuming that the direction of motion and either the speed or the travelled distance
are known) (Ahson & Ilyas, 2010; Karimi, 2013). However, since location estimation methods have
limited accuracy and precision, what can be asserted is that an object is at a given position at a
given time with a probability whose value belongs to an interval. The SPOT (Spatial PrObabilistic Temporal) framework was introduced by Parker, Subrahmanian, and Grant (2007a) to provide
a declarative framework for the representation and processing of probabilistic spatio-temporal data
with probabilities that are not known exactly.
The SPOT framework is able to represent atomic statements of the form object id is/was/will
be inside region r at time t with probability in the interval [`, u]. This allows the representation
of information concerning moving objects in several application domains. A cell phone provider
is interested in knowing which cell phones will be in the range of some towers at a given time and
with what probability (Bayir, Demirbas, & Eagle, 2010). A transportation company is interested
in predicting the vehicles that will be on a given road at a given time (and with what probability)
in order avoid congestion (Karbassi & Barth, 2003). Finally, a retailer is interested in knowing the
positions of the shoppers moving in a shopping mall in order to offer suitable customized coupons
on discounts (Kurkovsky & Harihar, 2006).
The framework introduced by Parker et al. (2007a) was then extended by Parker, Infantes, Subrahmanian, and Grant (2008) and Grant, Parisi, Parker, and Subrahmanian (2010) to include the
specific integrity constraint that, for a given moving object, only some points are reachable from a
given starting point in one time unit. This captures the scenario where objects have speed limits and
only some points are reachable by objects depending on the distance between the points. However,
even such an extended SPOT framework is not yet general enough to represent additional knowledge concerning the movements of objects. Examples of knowledge we may be aware of but cannot
represent in the SPOT framework are, for instance, the fact that
(i) there cannot be two distinct objects in a given region in a given time interval (as it happens
during airport passenger screening);
(ii) some object cannot reach a given region starting from a given location in less than a given
amount of time (as happens for the vehicles whose route options as well as their speed are
limited);
(iii) an object can go away from a given region only if it stayed there for at least a given amount of
time (as happens in production lines where assembling several parts requires a given amount
of time).
To overcome such limitation and allow this kind of knowledge to be represented, we define probabilistic spatio-temporal (PST) knowledge bases (KBs) consisting of atomic statements, such as
those representable in the SPOT framework and spatio-temporal denial (abbreviated to std) formulas, a general class of formulas that account for all the three cases above, and many more (including
the reachability constraint of Parker et al., 2008; Grant et al., 2010).
The focus of this paper is the systematic study of knowledge representation in probabilistic
spatio-temporal data. We start by defining the concept of a PST KB and provide its formal semantics, which is given in terms of worlds, interpretations, and models (Section 2). We define the
concept of a consistent PST KB, and characterize the complexity of checking consistency, showing
744

fiK NOWLEDGE R EPRESENTATION IN P ROBABILISTIC S PATIO -T EMPORAL K NOWLEDGE BASES

that it is NP-complete in general, and even in the presence of binary std-formulas only (Section 3).
Then we present a sufficient condition for checking consistency that relies on the feasibility of a
set of mixed-binary linear inequalities (Section 3.2), and a necessary condition using instead a set
of linear inequalities (Section 3.3). After showing that the special case of unary std-formulas is
tractable (Section 3.4), we deal with the restricted but expressive class of binary std-formulas and
identify cases where the consistency checking problem is tractable (Section 3.5). We then address
the problem of answering selection queries in PST KBs under both the optimistic and the cautious
semantics (Section 4). We show that checking consistency can be exploited to answer such kinds of
queries in PST KBs, and characterize the complexity of the query answering problem (Section 4.2).
After that, we derive several sets of linear inequalities for answering queries (Section 4.3). Finally,
we extend the framework to the case where time, space, and the number of objects are increased to
arbitrarily large finite domains, and show that each PST KB is either eventually consistent or eventually inconsistent (Section 5). We then discuss related work (Section 6). Section 7 summarizes the
paper. We also suggest research projects in Section 8.

2. The PST Framework
This section introduces the syntax and semantics of PST KBs generalizing the SPOT framework introduced by Parker et al. (2007a) and further extended by Parker et al. (2008) and Grant et al. (2010).
Basically, we define a PST KB by augmenting the previous framework with non-atomic formulas
(i.e., spatio-temporal denial formulas) that represent integrity constraints. This way we can make
statements whose meaning is that certain object trajectories cannot occur.
2.1 Syntax
We assume the existence of three types of constant symbols: object symbols, time value symbols,
and spatial region symbols. The constants are in ID = {id1 , . . . , idm }, T = [0, 1, . . . , tmax]
(where tmax is an integer), and the set of r  Space = {p1 , . . . , pn }. Each r is a region of Space.
We apply the unique name assumption; so, for instance idi and idj for i 6= j are different objects;
similarly, pi and pj for i 6= j are different points. We also use variables for each type: object
variables, time variables, and spatial variables.
A spatio-temporal atom (st-atom, for short) is an expression of the form loc(X, Y, Z), where:
(i) X is an object variable or a constant id  ID,
(ii) Y is a space 1 variable or a constant r  Space,
(iii) Z is a time variable or a constant t in T .
We say that st-atom loc(X, Y, Z) is ground if all of its arguments X, Y, Z are constants. For instance, loc(id, r, t), where id  ID, r  Space, and t  T is a ground st-atom. The intuitive
meaning of loc(id, r, t) is that object id is/was/will be inside region r at time t.
Definition 1 (PST atom). A PST atom is a ground st-atom loc(id, r, t) annotated with a probability
interval [`, u]  [0, 1] (with both ` and u rational numbers), and denoted as loc(id, r, t)[`, u].
1. We write Space to refer to the set of points used in a PST KB. We write space to refer to the spatial aspect of
probabilistic spatio-temporal knowledge.

745

fiPARISI & G RANT

7

loc(id1 , c, 9)[.9, 1]
loc(id1 , a, 1)[.4, .7]
loc(id1 , b, 1)[.4, .9]
loc(id1 , d, 15)[.6, 1]
loc(id1 , e, 18)[.7, 1]
loc(id2 , b, 2)[.5, .9]
loc(id2 , c, 12)[.9, 1]
loc(id2 , d, 18)[.6, .9]
loc(id2 , d, 20)[.2, .9]

6
5

e
c

4

d

3
2

b

1

a

0
0

1

2

3

4

5

6

7

(a)

(b)

Figure 1: (a) A map of an airport area (names of regions are on their bottom-right corner);
PST atoms.

(b)

Intuitively, the PST atom loc(id, r, t)[`, u] says that object id is/was/will be inside region r at
time t with probability in the interval [`, u]. Hence, PST atoms can represent information about the
past and the present, but also information about the future, such as from methods for predicting the
destination of moving objects (Mittu & Ross, 2003; Hammel, Rogers, & Yetso, 2003; Southey, Loh,
& Wilkinson, 2007), or from querying predictive databases (Akdere, Cetintemel, Riondato, Upfal,
& Zdonik, 2011; Parisi, Sliva, & Subrahmanian, 2013).
In the original SPOT definition, for ease of implementation, Space was a grid within which
only rectangular regions were considered; however, in our general framework, Space is arbitrary
and a region is any nonempty subset of Space. Still, for convenience we use such rectangular
regions in our running example.
Example 1. Consider an airport security system which collects data from biometric sensors as
well as from Bluetooth or WiFi enabled devices. Biometric data such as faces recognized by sensors (Li & Jain, 2011) are matched against given profiles (such as those of checked-in passports,
or of wanted criminals). Similarly, device identifiers (e.g., MAC addresses) recognized in the areas
covered by network antennas are matched against profiles collected by the airport hotspots (such as
logins, possibly associated with passport numbers). A simplified plan of an airport area is reported
in Figure 1(a), where regions a, b, c, d, e covered by sensors and/or antennas are highlighted. Once
entered in this area, passengers typically move through the path delimited by queue dividers (represented by dotted lines in the figure, and overlapping with regions a and b), and reach the room on
the upper-half right side where security checks are performed (region c is included in this room).
Next, passengers can spend some time in the hall room (overlapping with region d), and finally go
towards the exit (near region e).
Suppose that the security system uses the SPOT framework to represent the information where
every PST atom consists of the profile id resulting from the matching phase, the region where the
sensor/antenna recognizing the profile is operating, the time point at which the profile is recognized,
and the lower and upper probability bounds of the recognizing process. For instance, PST atom
loc(id1 , c, 9)[.9, 1] says that a profile having id id1 was in region c at time 9 with probability in the
746

fiK NOWLEDGE R EPRESENTATION IN P ROBABILISTIC S PATIO -T EMPORAL K NOWLEDGE BASES

interval [.9, 1] (the high-accuracy sensors used at security check points located in region c entail
a narrow probability interval with upper bound equal to 1). Atom loc(id1 , a, 1)[.4, .7] says that
id1 was recognized in region a at the earlier time 1 with probability in [.4, .7]. Assume that the
information is represented as the set of atoms in Figure 1(b), which includes the two atoms above.
2
PST atoms can be used to represent the output of a process aimed at tracking objects on the basis
of sensor measurements. Generally, sensors are characterized by a likelihood function providing the
conditional probability of obtaining a measurement given the value of a parameter , such as the
distance between the tracked object and the sensor. For instance, the likelihood function l() can
represent the probability of detecting an object that has a distance of  meters from the sensor
position. However, a likelihood function is generally not a probability distribution when viewed as
a function of . We may have l(1 ) = .9, l(2 ) = .4, l(3 ) = .1, and l(  3 ) = 0, with distances
1 < 2 < 3 . This information can be encoded by using PST atoms loc(id, ri , t)[l(i ), l(i )] for
each region ri determined by the distance i (more general probability intervals can be used if the
likelihood values are not know exactly).2 However, several object tracking techniques combine the
information of the likelihood function with the prior position distribution to obtain a probability
distribution over Space. PST atoms can represent this kind of information by defining a PST atom
with a single probability for each point in Space. 3 We refer the reader to the Related Work section
for a detailed discussion on object tracking techniques and the relationship with our framework.
Although PST atoms express much useful information, they cannot express additional knowledge that integrity constraints can provide. In this paper we add integrity constraints to the original
PST framework to form PST KBs. The integrity constraints have the form of spatio-temporal denial formulas (std formulas for short). We will soon see that such formulas are expressive enough
to capture a large set of conditions. Basically, an std formula is a universally quantified negation of
conjunctions of st-atoms and built-in predicates. We note that std formulas are related to a subclass
of the first-order formulas introduced by Doder, Grant, and Ognjanovic (2013), except that they
(the std formulas) allow built-in predicates as well. In any case, the focus of Doder et al. is the
axiomatization of various probabilistic spatio-temporal logics.
Definition 2 (Std-formula). An std-formula is an expression of the form
k


 ^
loc(Xi , Yi , Zi )  (X)  (Y)  (Z)
 X, Y, Z 
i=1

where:
 X is a set of object variables, Y is a set of space variables, and Z is a set of time variables;
 loc(Xi , Yi , Zi ), with i  [1..k], are st-atoms, where the Xi , Yi , Zi may be variables or constants of the appropriate type, such that, if Xi (resp., Yi , Zi ) is a variable, then it occurs
in X (resp, Y, Z). Moreover, each variable in X, Y, and Z occurs in at least one st-atom
loc(Xi , Yi , Zi ), with i  [1..k];
2. We note that PST KBs resulting from PST atoms encoding information provided by the likelihood function may be
inconsistent because of the fact that the likelihood function need not be a probability distribution. This will turn out
to be clearer after introducing the formal semantics of PST KBs in Section 2.2.
3. PST KBs resulting from PST atoms encoding the output of tracking turn out to be consistent.

747

fiPARISI & G RANT

 (X) is a conjunction of built-in predicates of the form Xi  Xj , where Xi and Xj are either
variables occurring in X or ids in ID, and  is an operator in {=, 6=};
 (Y) is a conjunction of built-in predicates of the form Yi  Yj , where Yi and Yj are either
variables occurring in Y or regions (i.e., non-empty subsets of Space), and  is a comparison
operator in {=, 6=, ov, nov} (where ov stands for overlaps and nov stands for does not
overlap);
 (Z) is a conjunction of built-in predicates of the form Zi  Zj where each Zi and Zj is either
a time value in T or a variable in Z that may be followed by +n where n is a positive integer
and  is an operator in {=, 6=, <, }.
Example 2. In our running example, in region c security checks on one individual at a time are
performed. The constraint there cannot be two distinct objects in region c at any time between 1
and 20 can be expressed by the following std-formula:
f1 = X1 , X2 , Z1 [loc(X1 , c, Z1 )  loc(X2 , c, Z1 )  X1 6= X2  Z1  1  20  Z1 ].
Due to the distance and the several obstacles between the entrance and the exit, we also have
the constraint no object can reach region e starting from region a in less than 10 time units, that
can be expressed as:
f2 = X1 , Z1 , Z2 [loc(X1 , a, Z1 )  loc(X1 , e, Z2 )  Z1 < Z2  Z2 < Z1 + 10].
Moreover, as the security check on each individual takes at least 2 time units, we know that
object id can go away from region c only if it stayed there for at least 2 time units, that can be
expressed as:
f3 = Y1 , Y2 , Z1 , Z2 , Z3 [loc(id, Y1 , Z1 )  loc(id, c, Z2 )  loc(id, Y2 , Z3 )  Y1 nov c  Y2 nov c 
Z2 = Z1 + 1  Z2 < Z3  Z2 + 2  Z3 ].
2
In our work later it will be useful to distinguish std-formulas based on the number (k) of statoms in them. In particular, unary std-formulas have k = 1 and binary std-formulas have k = 2.
In Example 2 above f1 and f2 are binary std-formulas while f3 is a ternary std-formula.
In the initial SPOT framework (Parker et al., 2007a) only PST atoms were considered. Moreover, it was assumed that all points in Space are reachable from all other points by all objects. To
overcome this limitation, Grant et al. (2010) extended the SPOT framework by introducing reachability definitions. A reachability atom is written as reachableid (p, q) where id  ID is an object
id, and p, q  Space. Intuitively, the reachability atom says that it is possible for the object id
to reach location q from location p in one unit of time. Hence, what is reachable in one time unit
depends not only on the locations p and q, but also the object id. As we now show, reachability can
be expressed in our formalism as an integrity constraint. However, in order to formulate reachability
in our framework of denial formulas, we need to deal with what is not reachable, rather than what
is reachable.
Example 3. Let r be the region consisting of all points q that are not reachable from p in one time
unit. The corresponding std-formula is:
X1 , Z1 , Z2 [loc(X1 , {p}, Z1 )  loc(X1 , r, Z2 )  Z2 = Z1 + 1].
2
This integrity constraint is used only if not all points are reachable from p in one time unit. We
can also express which points can not be reached from p in any number of time units, not just 1, by
changing Z1 + 1 to Z1 + i.
748

fiK NOWLEDGE R EPRESENTATION IN P ROBABILISTIC S PATIO -T EMPORAL K NOWLEDGE BASES

Example 4. In our running example, the following std-formula states that the points in region
r = {(x, y)|0  x  5  y = 3} (i.e., those close to the upper-side of the wall dividing the hall
room and the one where there are queue dividers) are not reachable in less than 3 time units from
any point in r0 = {(x, y)|0  x  5  y = 2} (i.e., the points close to the other side of that wall):
f4 = X1 , Z1 , Z2 [loc(X1 , r0 , Z1 )  loc(X1 , r, Z2 )  Z1 < Z2  Z2 < Z1 + 3].
2
We are now ready to formally define PST KBs.
Definition 3 (PST KB). Given the sets ID, T , and Space, a PST KB K is a pair hA, Fi, where
A is a finite set of PST atoms and F is finite set of std-formulas using object symbols in ID, time
values in T , and spatial regions consisting of sets of points in Space.
Example 5. In our running example, ID = {id1 , id2 }, T = [0, 20], Space is the set of points (x, y)
such that 0  x  7 and 0  y  7, and PST KB Kex is the pair hAex , Fex i, where Aex is the
set consisting of the PST atoms in Figure 1(b), and Fex is the set {f1 , f2 , f3 , f4 } of std-formulas
defined in Examples 2 and 4.
2
2.2 Semantics
The semantics of a PST KB is defined through the concept of worlds. Before introducing this
concept, we define ground std-formulas.
Given an std-formula f having the form in Definition 2, we denote by f the set of all substitutions of variables in X, Y, and Z with constants in ID, S, and T , respectively, where S
is the set of all subsets of Space that contain a single point. 4 Moreover, given substitution
  f , we denote as (f ) the ground std-formula resulting from applying  to f : (f ) =


 Vk

i=1 loc((Xi ), (Yi ), (Zi ))  ((X))  ((Y))  ((Z)) . The ground conjunction
of built-in predicates ((X))  ((Y))  ((Z)) evaluates to either true or false. When it is true
we omit it. So (f ) is either the negation of a conjunction of ground st-atoms or the truth value true
(when the conjunction of built-in predicates evaluates to false).
Example 6. Consider the formula f1 = X1 , X2 , Z1 [loc(X1 , c, Z1 )loc(X2 , c, Z1 )X1 6= X2 
Z1  1  20  Z1 ] introduced in Example 2, and the substitution  = {X1 /id1 , X2 /id2 , Z1 /6},
where id1 , id2  ID and 6  tmax. Thus, (f1 ) = [loc(id1 , c, 6)  loc(id2 , c, 6)], where the
conjunction of ground built-in predicates id1 6= id2  6  1  6  20, evaluating to true, is not
reported in (f1 ).
2
Definition 4 (World). A world w is a function, w : ID  T  Space.
Basically, a world w specifies a trajectory for each id  ID. That is, for each id  ID, w says
where in Space object id was/is/will be at each time t  T . In particular, this means that an object
can be in only one location at a time.5 However, a location may contain multiple objects. It is easy
to see that world w can be represented by the set {loc(id, {p}, t)| w(id, t) = p} of ground st-atoms.
4. We use only such singleton subsets of Space in order to reduce the number of possible instantiations of variables Y
from exponential to linear in the size of Space, without serious effect on the meanings of the std-formulas.
5. In some examples it may be useful to allow objects to enter or leave the space under consideration. This can be
accomplished, for instance, by having one or more external points outside of the space where objects may be located.
To simplify matters we assume that Space contains all these points.

749

fiPARISI & G RANT

Example 7. World w1 describing the trajectories of id1 and id2 for time units in [0, 20] is such
that w1 (id1 , t) = (4, 1) for t  [0, 5], w1 (id1 , t) = (7, 2) for t  [6, 7], w1 (id1 , t) = (7, 4) for
t  [8, 10], w1 (id1 , t) = (4, 4) for t  [11, 16], w1 (id1 , t) = (1, 6) for t  [17, 20], w1 (id2 , t) =
(4, 1) for t  [0, 11], w1 (id2 , t) = (7, 5) for t  [12, 15], w1 (id2 , t) = (7, 7) for t  [16, 16],
w1 (id2 , t) = (4, 5) for t  [17, 20].
2
Definition 5 (Satisfaction). Given a world w and a ground st-atom a = loc(id, r, t), we say that w
satisfies a (denoted
 r. Moreover, we say that w satisfies a conjunction of
Vk as w |= a) iff w(id, t) V
ground st-atoms i=1 ai (denoted as w |= ki=1 ai ) iff w |= ai i  [1..k]. Finally, world w satisfies
std-formula f (denoted as w |= f ) iff for each substitution   f , w |= (f ).
Note that, as there is a negation in front of f , w |= (f ) iff w does not satisfy a ground st-atom
in (f ) or the conjunction of ground built-in predicates in (f ) evaluates to false.
Example 8. World w1 of Example 7 satisfies the st-atom loc(id1 , b, 0), as w1 (id1 , 0) = (4, 1)
belongs to region b (see Figure 1(a)). Moreover, w1 |= [loc(id1 , b, 0)  loc(id1 , e, 15)] as w1 6|=
loc(id1 , e, 15), since w1 (id1 , 15) = (4, 4) 6 e.
2
In the following, we will denote as W(K) the set of all worlds of the PST KB K. Moreover, in
order to simplify formulas, we will assume that w ranges over W(K).
An interpretation I for a PST KB K is a probability distribution function (PDF) over W(K),
that is, a function assigning a probability value to each world in W(K). I(w) is the probability that
w describes the actual trajectories of all the objects.6 Some interpretations are models of K in which
case we write M instead of I.
Definition 6 (Model). A model M for a PST KB K = hA, Fi is an interpretation for K such that:
P
M (w)  [`, u];
  loc(id, r, t)[`, u]  A,
w | w|=loc(id,r,t)

  f  F,

P

M (w) = 0.

w | w6|=f

The first condition in the definition above means that, for each atom a = loc(id, r, t)[`, u]  A,
the sum of the probabilities assigned by M to the worlds satisfying the st-atom loc(id, r, t) have to
belong to the probability interval [`, u] specified by a. The second condition means that every world
not satisfying a formula f  F must be assigned by M probability equal to 0.
Example 9. Let w1 be the world introduced in Example 7. Let w2 be as w1 except that w2 (id1 , 1) =
(3, 2), and let w3 be as w2 except that w3 (id2 , 2) = (2, 2), w3 (id2 , t) = (0, 3) for t  [18..20]. Let
M be such that M (w1 ) = .7 M (w2 ) = .2 M (w3 ) = .1, and M (w) = 0 for all the other worlds in
W(Kex ). It can be checked that M satisfies both conditions of DefinitionP6 for the PST KB Kex of
our running example. For instance, for atom loc(id1 , a, 1)[.4, .7]  Aex , w|w|=loc(id1 ,a,1) M (w) =
M (w1 ) = .7  [.4, .7] (note that, at time 1, w2 (id1 , 1) = w3 (id1 , 1) = (3, 2) that is not in region a).
Moreover, it is easy to check that w1 , w2 , w3 satisfy every std-formula in Fex . Thus, M is a model
for Kex .
2
We say that PST KB K is consistent iff if there is a model for it. The set of models for K will
be denoted as M(K).
6. As a PDF, I(w) is non-negative and sums to 1 over worlds.

750

fiK NOWLEDGE R EPRESENTATION IN P ROBABILISTIC S PATIO -T EMPORAL K NOWLEDGE BASES

Definition 7 (Consistency). PST KB K is consistent iff M(K) 6= .
Example 10. PST KB Kex of our running example is consistent, as there exists the model M of
Example 9 for it.
2

3. Checking the Consistency of PST KBs
In this section, we address the fundamental problem of checking the consistency of PST KBs.
Given a PST KB K = hA, Fi, the consistency checking problem is deciding whether M(K) 6= ,
that is, whether there is a model for K.
In Section 3.1 we show that the consistency checking problem is NP-complete. Our goal in the
rest of this section is to find more efficient ways to determine consistency. In Section 3.2 we find a
sufficient condition using a set of mixed-binary linear inequalities. Then, in Section 3.3 we find a
necessary condition using a different set of linear inequalities. We deal with the special case where
all the std-formulas are unary in Section 3.4. Finally we investigate in detail the case where the
std-formulas are either unary or binary in Section 3.5.
For our complexity analysis we take the size of a PST KB K = hA, Fi, whose PST atoms and
std-formulas are built from the constants in ID, T , and Space, as the number of PST atoms and
std-formulas in K plus the number of items in ID, T , and Space, that is, |K| = |A| + |F| + |ID| +
|T | + |Space|.
3.1 Checking Consistency is NP-Complete
Before considering the case of general PST KBs, we first note that the consistency checking problem was addressed in the initial SPOT framework of Parker et al. (2007a) where only PST atoms
were considered. This is the special case of our PST KB concept where F = . It was shown there
that the consistency of a PST KB K = hA, i (using our notation) can be checked in polynomial
time w.r.t. the size of K by solving a set of linear inequalities whose variables vid,t,p represent the
probability that object id is at point p at time t.
The reason for presenting the result of Parker et al. is twofold: first, we compare the complexity
of the consistency checking problem for general PST KBs with that of the initial SPOT framework;
second, we use it to prove some tractability results for PST KBs.
Fact 1 (Parker et al.). Let K = hA, i be a PST KB (where the set of std-formulas is empty). Then
K is consistent iff there is a feasible solution of CC(K), where CC(K) consists of the following
(in)equalities:
P
(1)  loc(id, r, t)[`, u]  A: ` 
vid,t,p  u;
pr

(2)  id  ID, t  T :

P

vid,t,p = 1;

pSpace

(3)  id  ID, t  T, p  Space: vid,t,p  0.
Basically, inequalities (1) ensure that a solution of CC(K) places the object in r with a probability between ` and u, as required by the atom (id, r, t, [`, u]). Inequalities (2) and (3) ensure that
for each id and t, the vid,t,p variables jointly represent a probability distribution. Fact 1 is correct
because every model M for K corresponds to a solution  for CC(K) such that the sum of the
751

fiPARISI & G RANT

probabilities assigned by M to the worlds for K satisfying an st-atom loc(id, {p}, t) is equal to the
value assigned to variable vid,t,p by .
Now we state our first result: the consistency checking problem is NP-complete.
Theorem 1. Given a PST KB K = hA, Fi, deciding whether K is consistent is NP-complete.
Proof. (Membership). We show that checking the consistency of K can be reduced to deciding an
instance K of (an extension to) the Probabilistic Satisfiability (PSAT) problem (Hailperin, 1984;
Nilsson, 1986), which is in NP (Georgakopoulos, Kavvadias, & Papadimitriou, 1988). Given a set of
m clauses C1 , . . . , Cm , each of them consisting of a disjunction of one or more literals constructed
from the propositional variables x1 , . . . , xn , and probability values Pr(C1 ), . . . , Pr(Cm ) for each
clause, PSAT is the problem of deciding whether there is a probability distribution  over the set of
the 2n truth assignments for the propositional variables x1 , . . . , xn such that for each clause Ci , the
sum of the probabilities assigned by  to the truth assignments satisfying Ci is equal to Pr(Ci ), with
i  [1..m]. PSAT is a generalization of SAT, which is obtained from PSAT by assigning probability
equal to one to each clause. Georgakopoulos et al. formulated PSAT in terms of the feasibility of
a system of m + 1 linear equations using 2n variables corresponding to the probabilities assigned
by  to truth assignments. To show the existence of polynomial-size witness, the following result
from linear programming theory was exploited by Georgakopoulos et al.: if a system of  linear
equalities has a feasible solution, then it admits at least one feasible solution with at most  nonzero variables (Papadimitriou & Steiglitz, 1982). In what follows we consider an extension to
PSAT where each clause Ci is associated with a probability interval [Pr` (Ci ), Pru (Ci )], instead
of a single value. The membership in NP for this extension straightforwardly follows from the
membership proof provided by Georgakopoulos et al. for PSAT, as using probability intervals can
be still formulated as the linear system introduced by Georgakopoulos et al. after reducing doublesided inequalities to equalities with single bounded slack variables (Jaumard, Hansen, & de Aragao,
1991).
Given a PST KB K = hA, Fi, we define an instance K of PSAT where each clause is associated with a probability interval. Let U be the set of all propositional variables xid,p,t such that
id  ID, p  Space, and t  T (i.e., each st-atom loc(id, {p}, t) corresponds to propositional
variable xid,p,t  U ). The conjunction K of clauses associated with probability intervals is defined
as follows:
W
 For each PST atom a = loc(id, r, t)[`, u]  A, K consists of the clause Ca =
xid,p,t .
pr

The probability interval [Pr` (Ca ), Pru (Ca )] is equal to [`, u].
 For each f  F and   f such that (f ) = [

k
V

loc((Xi ), (Yi ), (Zi ))], K consists of

i=1

clause C(f ) =

k
W
i=1

x(Xi ),(Yi ),(Zi ) , whose probability interval is [1, 1].

 For each id  ID and t  T , K consists of the clause Cid,t =

W

xid,p,t , whose

pSpace

probability interval is [1, 1].
 For each id  ID, t  T , and pi , pj  Space, with pi 6= pj , K consists of the clause
Cid,t,pi ,pj = xid,pi ,t  xid,pj ,t whose probability interval is [1, 1].
752

fiK NOWLEDGE R EPRESENTATION IN P ROBABILISTIC S PATIO -T EMPORAL K NOWLEDGE BASES

It is easy to see that for each world w  W(K), there is a truth assignment w for the variables
in U such that w (xid,p,t ) is true iff w(id, t) = p. However, there are truth assignments which do
not correspond to any world in W(K) (for instance, those where both  (xid,pi ,t ) and  (xid,pj ,t ),
with pi 6= pj , are true). We now show that K is consistent iff K is satisfiable.
() Given a model M for K, we show that there is a PDF  over the set of truth assignments such
that for each clause C of K , the sum of the probabilities assigned by  to the truth assignments
satisfying C belongs to [Pr` (C), Pru (C)]. Let  be such that for each truth assignment w corresponding to a world w  W(K), (w ) = M (w) and ( ) = 0 for all other truth assignments  not
corresponding to a world. It is easy to check that the conditions of Definition 6 entail that clauses
of the form Ca and C(f ) are satisfied by , and clauses of the form Cid,t and Cid,t,pi ,pj are satisfied
as well since only w1 , . . . w|W(K)| may have been assigned by  a probability different from 0 and
every world w  W(K) by definition assigns exactly one point in Space to each id, t pair.
() Let  be a PDF over the set of truth assignments such that for each clause C of K , the sum
of probabilities assigned by  to the truth assignments satisfying C belongs to [Pr` (C), Pru (C)].
A model M for K can be defined as M (w) = (w ) where  is the truth assignment corresponding to world w  W(K). Since the clauses of the form Cid,t (resp., Cid,t,pi ,pj ) are satisfied by
, all truth assignments  such that  (xid,p,t )=false for all p  Space (resp.,  (xid,pi ,t )=true and
 (xid,pj ,t )=true, with pi , pj  Space, pi 6= pj ) are assigned by  probability 0. Hence, all other
truth assignments correspond to a world, and the fact that the clauses of the form Ca and C(f ) are
satisfied by  entails that the conditions of Definition 6 hold.
(Hardness). We show a reduction to our problem from the NP-hard Hamiltonian path problem (Papadimitriou, 1994), that is, the problem of checking whether there is a path  in a directed graph G
such that  visits each vertex of G exactly once.
Given a directed graph G = hV, Ei, where V = {v0 , . . . , vk } is the set of its vertices, and E
is a set of pairs (vi , vj ) with vi , vj  V , we construct an instance of our problem as follows. Let
ID = {id}, Space = V , and T = [0, . . . , k]. K is the pair hA, Fi such that A consists of the PST
atom loc(id, {v0 }, 0)[1, 1] and F consists of std-formulas f1i (with i  [0..k]) and f2 such that:
 f1i = Z1 , Z2 [loc(id, {vi }, Z1 )  loc(id, Space\V 0 , Z2 )  Z2 = Z1 + 1] where V 0 is the
set of vertices vj s.t. (vi , vj )  E. This formula says that the only points id can reach starting
from vi in one time step are those in V 0 . (f1i does not exist if V 0 = Space.)
 f2 = Y1 , Z1 , Z2 [loc(id, Y1 , Z1 )  loc(id, Y1 , Z2 )  Z1 6= Z2 ], saying that id can not be at
the same location for distinct time values.
We show that K is consistent iff there is a Hamiltonian path in G.
() As there is only one id in A, every world w  W(K) is such that w places id on a vertex
in V at each time value t  T . As K is consistent, there is a model M  M(K) such that M
assigns probability greater than zero only to worlds w such that f  F, w |= f . In particular,
let w be one such world. The fact that w |= f1i entails that t  [0, k  1], w(id, t) = vi and
w(id, t + 1) = vj iff (vi , vj )  E. Moreover, the fact that w |= f2 entails that t, t0  [0, k], t 6= t0 ,
w(id, t) 6= w(id, t0 ), meaning that id is never placed by w on the same vertex at different time
units. Since loc(id, v0 , 0)[1, 1]  A, every world which is assigned probability greater than zero
by M is such that w(id, 0) = v0 . It follows that every world w  W(K) which is assigned by
M  M(K) a probability greater than zero encodes a Hamiltonian path of G whose first vertex is
v0 . In fact, w  W(K) such that M (w) > 0 the following properties hold: (i) w(id, 0) = v0 ,
753

fiPARISI & G RANT

(ii) t  [0, k  1], w(id, t) = vi , w(id, t + 1) = vj iff (vi , vj )  E. (iii) t, t0  [0, k], t 6= t0 ,
w(id, t) 6= w(id, t0 ). Conditions (i) and (ii) entail that  = w(id, 0), w(id, 1), . . . , w(id, k) is a
path on G starting from vertex v0 , while condition (iii) entails that each vertex v  V occurs exactly
once in .
() Let  be a Hamiltonian path of G. We denote by [i] (with i  [0..k]) the i-th vertex of .
W.l.o.g. we assume that the first vertex of  is v0 , that is, [0] = v0 . We now show that K is
consistent by finding a model for it. Let M be a function over W such that for all worlds w  W,
M (w) = 0, except for the world w which is such that: w (id, 0) = [0] = v0 , t  [1, k],
w (id, t) = [t]. It is easy to see that w |= F. In fact, for each i  [0..k], f1i is satisfied by w , since
the fact that  is a path on G entails that t  [0, k 1], w (id, t) = vi and w (id, t+1) = vj only if
edge (vi , vj ) is an edge of G. Moreover, f2 is satisfied by w , since the fact that  is a Hamiltonian
path entails that w places id on different locations (i.e., vertices of G) at different times. Since

w
aP
probability different from 0. Let M (w ) = 1. Therefore, as
P |= F, it can be assigned by M

w|w|=loc(id,v0 ,0) M (w) = M (w )+ w|w6=w w|=loc(id,v0 ,0) M (w) = 1, the condition required by
atom loc(id, v0 , 0)[1, 1]  A holds too. Thus, M is a model for K.
We note that NP-hardness holds already for binary std-formulas. In Section 3.5 we will find
conditions that make the consistency checking problem tractable for binary std-formulas. The reduction shown in the membership proof, from the consistency checking problem to PSAT, would
allow us to define additional tractable cases if the PSAT instances resulting from such a reduction
were tractable. However, as will be discussed in Section 6, the tractable cases that have been identified for PSAT (Georgakopoulos et al., 1988; Andersen & Pretolani, 2001) do not carry over to our
framework.
3.2 Sufficient Condition for Checking Consistency
We present a set of mixed-binary linear inequalities whose feasibility entails the consistency of PST
KB K = hA, Fi. As explained in Section 3.1, Parker et al. (2007a) showed that the consistency
of a PST KB K = hA, i (using our notation) can be checked in polynomial time w.r.t. the size of
K by solving a set of linear inequalities whose variables vid,t,p represent the probability that object
id is at point p at time t. Here, we start from this set of linear inequalities and augment it with
some inequalities ensuring that if the so-obtained set of linear inequalities has a feasible solution
then every ground std-formula derived from F is satisfied. To achieve this, we need to introduce the
binary variables i , thus obtaining a set of mixed-binary linear inequalities.
Definition 8 (MBL(K)). Let K = hA, Fi. MBL(K) consists of the following (in)equalities:
P
(1)  loc(id, r, t)[`, u]  A: ` 
vid,t,p  u;
pr

(2)  id  ID, t  T :

P

vid,t,p = 1;

pSpace

(3)  id  ID, t  T, p  Space: vid,t,p  0;
(4) for each f  F and   f such that (f ) = [

k
V
i=1

number of st-atoms in f , the (in)equalities:
754

loc((Xi ), (Yi ), (Zi ))], where k is the

fiK NOWLEDGE R EPRESENTATION IN P ROBABILISTIC S PATIO -T EMPORAL K NOWLEDGE BASES

P
(a) i  [1..k] :
p(Yi ) v(Xi ),(Zi ),p  i ;
Pk
(b)
i=1 i = k  1;
(c) i  [1..k] : i  {0, 1}.
Basically, inequalities (1) ensure that a solution of MBL(K) places the object in r with a probability between ` and u, as required by the atom (id, r, t, [`, u]). Inequalities (2) and (3) ensure
that for each id and t, the vid,t,p variables jointly represent a probability distribution. Moreover, for
each ground st-atom loc((Xi ), (Yi ), (Zi )) of the ground std-formula (f ), inequalities (4)(a)
and (4)(c) entail that the probability v(Xi ),(Zi ),p that object (Xi ) is in any point p in region (Yi )
at time (Zi ) is either constrained to be 0 or free to take any value not greater than 1. Intuitively
enough, if v(Xi ),(Zi ),p is enforced to be zero (i.e., i = 0), then object (Xi ) can not be in region
(Yi ) at time (Zi ). On the other hand, if v(Xi ),(Zi ),p is left free to take any value less than or
equal to one (i.e., i = 1), then (Xi ) may or may not be in region (Yi ) at time (Zi ). Finally,
equality (4)(b) entails that there is at least one of the k ground st-atoms loc((Xi ), (Yi ), (Zi )) of
(f ) such that (Xi ) is not placed in a point in (Yi ) at time (Zi ).
Example 11. Consider the ground std-formula (f1 ) = [loc(id1 , c, 6)  loc(id2 , c, 6)] of Example 6. Then, the inequalities in MBL(K) corresponding to (f1 ) are:
P
P
(4b) 1 + 2 = 1;
(4c) 1 , 2  {0, 1}.
2
(4a) pc vid1 ,6,p  1 ;
pc vid2 ,6,p  2 ;
The following theorem states that MBL(K) can be used to check if K is consistent.
Theorem 2. If there is a feasible solution of MBL(K) then K is consistent.
Proof. Let  be a solution of MBL(K), and (vid,t,p ) the value assigned to variable vid,t,p by
.
Q We define the function M over W(K) such that, for each world w  W(K), M (w) =
idID,tT,w(id,t)=p (vid,t,p ), that is M (w) is the product of the values assigned by solution
 to variables vid,t,p such that w(id, t) = p. It can be shown that, (in)equalities (2) and (3)
of the definition
of MBL(K) entail that M is a PDF over W(K). Moreover,
since (vid,t,p ) is
P
P
M
(w),
for
each
atom
loc(id,
r,
t)[`,
u]

A,
equal
to
w|w|=loc(id,r,t) M (w) =
P P w|w|=loc(id,t,p)
P
pr
w|w|=loc(id,t,p) M (w) =
pr (vid,t,p )  [`, u]. Given f  F and   f such that (f )
Vk
is logically equivalent to the negation of the conjunction of the st-atoms
i=1 loc((Xi ), (Yi ),
P
(Zi )), the inequalities (4)(a-c) entail that there is i  [1..k] such that p(Yi ) (v(Xi ),(Zi ),p ) =
0. Thus p  (Yi ), (v(Xi ),(Zi ),p ) = 0. Hence, for each world w  W(K) such that w((Xi ),
(Zi )) = p, M (w) = 0 due to the presence of the factor
P (v(Xi ),(Zi ),p ) = 0 in the product defining M (w). Therefore, for each std-formula f  F, w | w6|=f M (w) = 0; hence M is a model for
K and so K is consistent.
A consequence of Theorem 2 is that well-known techniques for solving linear optimization
problems can be adopted to address the consistency checking problem, thus taking advantage of
results from more than 50 years of research on integer linear programming (Junger et al., 2010).
The following example shows that the converse of Theorem 2 does not hold (K may be consistent even if there is no feasible solution of MBL(K)).
Example 12. Let ID = {id}, T = [0, 1], Space = {p0 , p1 }, and K = hA, Fi where:
A = {loc(id, {p0 }, 0)[0.5, 0.5], loc(id, {p1 }, 1)[0.5, 0.5]} and
F = {[loc(id, {p0 }, 0)  loc(id, {p1 }, 1)}.
755

fiPARISI & G RANT

Thus, W = {w1 , w2 , w3 , w4 } where: w1 (id, 0) = p0 , w1 (id, 1) = p0 , w2 (id, 0) = p0 , w2 (id, 1) =
p1 , w3 (id, 0) = p1 , w3 (id, 1) = p0 , w4 (id, 0) = p1 , w4 (id, 1) = p1 .
It is easy to check that the M for which M (w1 ) = 0.5, M (w2 ) = 0, M (w3 ) = 0, M (w4 ) = 0.5 is
a model for K. Then MBL(K) includes the following inequalities:
0.5  vid,0,p0  0.5; 0.5  vid,1,p1  0.5; vid,0,p0 + vid,0,p1 = 1; vid,1,p0 + vid,1,p1 = 1;
vid,0,p0  1 ; vid,1,p1  2 ; 1 + 2 = 1; 1 , 2  {0, 1};
vid,0,p0  0, vid,0,p1  0, vid,1,p0  0, vid,1,p1  0.
The first two inequalities force both vid,0,p0 and vid,1,p1 to be 0.5. So in the second line both 1 and
2 must equal 1. But this contradicts 1 + 2 = 1. Hence MBL(K) has no feasible solution.
2
3.3 Necessary Condition for Checking Consistency
In the following, given a PST KB K, we introduce a set NC(K) of linear inequalities such that if K
is consistent then there is a feasible solution of NC(K). That is, the existence of a feasible solution
of NC(K) turns out to be a necessary condition for the consistency of K.
As MBL(K) (see Definition 8), NC(K) uses rational variables vid,t,p representing the probability
that object id is at point p at time t. As no other kinds of variables are used in the definition of
NC(K), in this case we obtain a pure system of linear inequalities.
Definition 9 (NC(K)). Let K = hA, Fi be a PST KB. NC(K) consists of the following (in)equalities:
P
vid,t,p  u;
(1)  loc(id, r, t)[`, u]  A: ` 
pr

(2)  id  ID, t  T :

P

vid,t,p = 1;

pSpace

(3)  id  ID, t  T, p  Space: vid,t,p  0;
V
(4) for each f  F (with k conjuncts) and   f s.t. (f ) = [ ki=1 loc((Xi ), (Yi ), (Zi ))],
the inequalities:
Pk
p1  (Y1 ), p2  (Y2 ), . . . , pk  (Yk ),
i=1 v(Xi ),(Zi ),pi  k  1.
Herein, (in)equalities (1)-(3) are the same as those of MBL(K) of Definition 8, and have the
same meaning. In addition,
V NC(K) contains inequalities (4) which impose that, for each ground stdformula of the form  ki=1 loc(idi , ri , ti ) and for each k-tuple of points hp1 , p2 , . . . , pk i belonging
respectively to the regions hr1 , r2 , . . . , rk i, the sum of the probabilities vidi ,pi ,ti that object idi is at
point pi at time ti , with i  [1..k], is not greater than k  1. As stated in the following theorem, the
set consisting of inequalities (4) along with inequalities (1)-(3) turns out to have no feasible solution
only if the corresponding PST KB is inconsistent.
Theorem 3. If there is no feasible solution of NC(K), then K is not consistent.
Proof. Suppose that NC(K) has no feasible solution. If this is due to the fact there is no feasible
solution of CC(hA, i (introduced in Fact 1), then PST KB hA, i is not consistent, and thus K =
hA, Fi is not consistent as well (since the set M(hA, Fi) of the models for K = hA, Fi is a subset
of the set M(hA, i) of the models for K0 = hA, i).
Otherwise, there is a feasible solution of CC(hA, i) and thus NC(K) has no feasible solution
due to the fact that at least one of the inequalities in item (4) of Definition 9 is not satisfied by every
756

fiK NOWLEDGE R EPRESENTATION IN P ROBABILISTIC S PATIO -T EMPORAL K NOWLEDGE BASES

solution  for
V CC(hA, i). That is, for each solution  for CC(hA, i), there is a ground std- formula
(f ) =  ki=1 loc((Xi ), (Yi ), (Zi )) such that there exist p1  (Y1 ), p2  (Y2 ), . . . , pk 
P
(Yk ), for which ki=1 (v(Xi ),(Zi ),pi ) > k  1.
Since every model M 0 for K0 = hA, i corresponds to a solution  for CC(hA, i) such that the
sum of the probabilities assigned
by M 0 to the worlds satisfying an st-atom loc(id, {p}, t) is equal
Pk
to (vid,t,p ), the fact that i=1 (v(Xi ),(Zi ),pi ) > k  1 holds entails that, for each model M 0 for
K0 , the sum of the probabilities assigned by M 0 to the worlds satisfying at least one of the st-atoms
in (f ) is greater than k  1, that is,
M 0  M(K0 ),

k
X

X

M 0 (w) > k  1

(1)

i=1 w|=loc((Xi ),(Yi ),(Zi ))

We now recall and use the following well-known result on the minimum probability of a conjunction of probabilistic events among which no correlation is known. Given n probabilistic events
e1 , . . . , en whose (marginal)
probabilities are Pr(e1 ), . . . , Pr(en ) respectively, Pr(e1      en ) 
P
max (0, 1  n + ni=1 Pr(ei )). This is one of the Frechet inequalities (the other one provides an
upper bound on the maximum probability) and is implicitly reported already in Booles work (1854).
In our setting, viewing st-atoms as probabilistic events, the Frechet inequality entails that for
each model M 0 for K0 , the probability that a set S of st-atoms together satisfy a world is greater
than or equal to the maximum between zero and 1  |S| plus the sum of the probabilities of each
st-atom a  S according to M 0 . That is,


X
W(K0 )

w
a  S, w |= a


X

M 0 (w)  max 0, 1  |S| +


X
W(K0 ),

aS w 
w |= a



M 0 (w)


(2)

Equation (1) entails that for each model M 0 for K0 , the term on the right-hand side of Equation (2) evaluates to a value greater than zero when the set S of st-atoms of (f ) is considered.
Therefore, for each model M 0 for K0 , the sum of the probabilities of the worlds satisfying all the
st-atoms in (f ) is greater than zero. Since the set M(hA, Fi) of the models for K = hA, Fi is a
subset of the set M(hA, i) of the models for K0 = hA, i, this property also holds for any model
for K0 . Thus, no such M 0 can be a model of K. Hence K is inconsistent.
An example of the usage of Theorem 3 is given below, where by checking that NC(K) has no
feasible solution we conclude that the PST KB K is not consistent.
Example 13. Let ID = {id}, T = [0, 1, 2], and Space = {p0 , p1 , p2 }. Let K = hA, Fi such that
 A consists of the PST atoms loc(id, {p0 }, 0)[0.4, 1], loc(id, {p1 }, 0)[0.5, 1], loc(id, {p0 }, 1)
[0.8, 1], loc(id, {p0 }, 2) [0.8, 1], meaning that, id is at p0 and at p1 at time 0 with probability greater than or equal to 0.4 and 0.5, respectively, and it is at p0 at times 1 and 2 with
probability greater than or equal to 0.8.
 F consists of the std-formula: [loc(id, {p0 , p1 }, 0)  loc(id, {p0 }, 1)  loc(id, {p0 }, 2)],
saying that id cannot be at p0 at both times 1 and 2 if it was in the region consisting of the
points p0 and p1 at time point 0.
757

fiPARISI & G RANT

It is easy to see that NC(K) contains, among others, the following inequalities:
0.4  vid,0,p0  1
0.5  vid,0,p1  1
0.8  vid,1,p0  1
0.8  vid,2,p0  1
vid,0,p0 + vid,1,p0 + vid,2,p0  2
vid,0,p1 + vid,1,p0 + vid,2,p0  2
where the last two inequalities derive from item (4) of Definition 9. Clearly, these inequalities
cannot be satisfied given that 0.5  vid,0,p1 , 0.8  vid,1,p0 , and 0.8  vid,2,p0 . Thus, since NC(K)
has no feasible solution, we can conclude that K is inconsistent.
2
However, we cannot say anything about the consistency of K if there is a feasible solution
of NC(K). The following examples show a case where NC(K) has a feasible solution and K is
consistent, and a case where NC(K) has a feasible solution but K is inconsistent.
Example 14. Consider the PST KB of Example 13 where A is modified such that the probability
that id is at p0 at times 1 and 2 is greater than or equal to 0.5, instead of 0.8. It is easy to see that
there is a feasible solution of NC(K) in this case. Theorem 3 cannot be used to decide whether K is
consistent or not. As a matter of fact, K is consistent as shown in what follows. Let w1 , w2 and w3
be worlds for K such that
w1 (id, 0) = p1 , w1 (id, 1) = p1 , w1 (id, 2) = p0 ,
w2 (id, 0) = p0 , w2 (id, 1) = p0 , w2 (id, 2) = p1 ,
w3 (id, 0) = p2 , w3 (id, 1) = p0 , w3 (id, 2) = p2 ,
and let M be the PDF over W(K) such that M (w1 ) = 0.5, M (w2 ) = 0.4, M (w3 ) = 0.1, and
M (w) = 0 for any other world w  W(K). It is straightforward to check that M is a model for
K.
2
Example 15. Again, let ID = {id}, T = [0, 1, 2], and Space = {p0 , p1 , p2 }. Let K = hA, Fi
be such that A = {loc(id, {p0 }, 0)[0.5, 1], loc(id, {p0 }, 1)[0.5, 1], loc(id, {p0 }, 2)[0.5, 1]}, i.e., id
is at p0 at any time in T with a probability greater than 0.5, and F consists of the std-formulas:
[loc(id, {p0 }, 0)  loc(id, {p0 }, 1)],
[loc(id, {p0 }, 1)  loc(id, {p0 }, 2)], and
[loc(id, {p0 }, 0)  loc(id, {p0 }, 2)],
saying that id cannot be at p0 at time 1 or 2 if it was already there at any previous time value.
A solution for NC(K) is obtained by assigning the variables vid,t,p0 and vid,t,p1 the value 0.5
(where t  [0, 2]), and the other variables the value 0. As NC(K) has a feasible solution, Theorem 3
says nothing about the fact that K is consistent or not. However, it can be checked that K is not
consistent. Let Pt be the set of worlds in W(K) placing id at p0 at time t, with t  T . The
std-formulas in F entail that every world belonging to two of these three sets must be assigned
probability equal to 0 by any model for K. That is, F entails that P0 , P1 , P2 are pairwise disjoint
sets if we consider only the worlds that can be assigned a probability greater than zero by any model.
Now observe that the PST atoms in A require that the sum of the probabilities of the worlds in each
of these three sets is at least equal to 0.5. Therefore, the overall sum of the probabilities assigned to
the worlds in all of these three sets would be greater than or equal to 1.5, which entails that there is
no model for K.
2
Theorem 3 shows that the consistency of K implies the the existence of a feasible solution
of NC(K) and the previous examples show that this is the only relationship between these two
758

fiK NOWLEDGE R EPRESENTATION IN P ROBABILISTIC S PATIO -T EMPORAL K NOWLEDGE BASES

concepts. However, we can make a stronger statement in a special case. For PST KBs where F
consists of a single ground std-formula (of any arity) constructed from st-atoms referring to singlepoint regions, NC(K) has a feasible solution iff the KB is consistent.

Theorem 4. Let K = hA, Fi where F = { loc(id1 , {p1 }, t1 )      loc(idn , {pn }, tn )]}. Then,
K is consistent iff there is a feasible solution of NC(K).
Proof. Using Theorem 3 we need to prove that if there is a feasible solution of NC(K) then K is
consistent.
Let f be the std-formula in F. We first observe that if f contains a pair of st-atoms referring to
the same id and time value but different points in Space, then K is consistent. Indeed, in this case,
f is satisfied by every world, and thus it does not impose any restriction on the interpretations for
K (see Definition 4). Given this, in the following w.l.o.g. we assume that for every distinct pair of
st-atoms loc(idi , {pi }, ti ), loc(idj , {pj }, tj ) in f , if idi = idj then ti 6= tj .
Let  be a solution for NC(K), and K0 = hA, i. Then,  is a solution for CC(K0 ) (from Fact
1) and corresponds to a model M 0 for K0 such that (i) the sum of the probabilities assigned by M 0
to the worlds satisfying an st-atom
loc(id, {p}, t) (i.e., the marginal probability of loc(id, {p}, t)) is
P
equal to (vid,t,p ), and (ii) ki=1 (vidi ,ti ,pi )  k  1. Viewing each st-atom loc(idi , {pi }, ti ) in f
as a probabilistic event whose (marginal) probability is (vidi ,ti ,pi ), the Frechet inequality (recalled
in the proof of Theorem 3) entails thatPthe minimum probability that the st-atoms in f occur all
together is equal to max(0, 1  k + ki=1 (vidi ,ti ,pi )). This is equal to zero since the second
argument of function max is not greater than zero due to the fact that (ii) holds. The fact that the
minimum probability that the st-atoms in f simultaneously occur is equal to zero suffices to ensure
that there is at least one model M 00 for K0 such that M 00 assigns probability equal to zero to all the
worlds for K0 that do not satisfy f . As f is the only std-formula in F, and every world not satisfying
f is assigned a probability equal to zero by M 00 , it follows that M 00 is a model for K too.
The following example shows that considering even a binary std-formula containing an st-atom
referring to a region consisting of two points, it may happen that there is a feasible solution of
NC(K) even if K is not consistent.
Example 16. Let ID = {id}, T = [0, 1], and Space = {p0 , p1 , p2 }. Let K = hA, Fi be such that
A = {loc(id, {p0 }, 0)[0.4, 1], loc(id, {p1 }, 0)[0.4, 1], loc(id, {p0 }, 1)[0.4, 1], }, and F is the stdformula [loc(id, {p0 , p1 }, 0)  loc(id, {p2 }, 1)] saying that id cannot move to point p2 at time 1 if
it was in either p0 or p1 at time 0. It is easy to see that NC(K) is feasible but K is not consistent. 2
In Section 3.5, we will present a method for deciding in polynomial time the consistency of
PST KBs where binary std-formulas satisfying some acyclicity conditions are used. It turns out
that the consistency of the PST KBs of both Examples 15 and 16 can be decided in polynomial
time using our approach.
3.4 Unary Std-Formulas are Tractable
We start by identifying a tractable case of the consistency checking problem: when all std-formulas
are unary, that is, each formula in F consists of only one st-atom and possibly a conjunction of
built-in predicates (i.e., in Definition 2, k = 1).
759

fiPARISI & G RANT

Example 17. The constraint there is no object in region r at any time between 5 and 10 can be
expressed by the following unary std-formula: X1 , Z1 [loc(X1 , r, Z1 )  Z1  5  10  Z1 ].
The constraint object id is always in region r can be expressed as:
Y1 , Z1 [loc(id, Y1 , Z1 )  Y1 nov r]. 7
2
The following theorem states that checking consistency is tractable if only unary std-formulas
are considered.
Theorem 5. Let K = hA, Fi be a PST KB such that F consists of unary std-formulas only. Then,
deciding whether K is consistent is in P T IM E.
Proof. The statement follows from the fact that if F consists of unary std-formulas only, K =
hA, Fi is equivalent to (i.e., it has exactly the same set of models as) K0 = hA0 , i, where A0
consists of the atoms in A plus the atom loc((Xi ), (Yi ), (Zi ))[0, 0] for each
S ground std-formula
(f ) = [loc((Xi ), (Yi ), (Zi ))], where f  F and   f . Since, f F f is polynomial
w.r.t. the size of K, the size of A0 (and thus of K0 ) increases by a polynomial number of atoms.
Hence, we can apply Fact 1, which entails that the consistency of PST KBs with F =  can be
decided in P T IM E.
3.5 Tractable Binary Std-Formulas
In the following we focus on PST KBs where all std-formulas are binary. This is a restricted
but expressive class of std-formulas that allow us to impose several practical constraints in many
application contexts. As a matter of fact, both f1 and f2 of Example 2 as well as f4 of Example 4
are binary std-formulas. Furthermore, using the approach suggested in the proof of Theorem 5, we
can assume that unary std-formulas are encoded as PST atoms. Thus, all the results stated in this
section straightforwardly apply to the case where both unary and binary std-formulas are in the PST
KB.
We start by noting that consistency checking was proved to be feasible by Grant et al. (2010)
for the case where reachability definitions (but no other integrity constraints) were allowed. As we
showed in Example 3 a reachability definition can be expressed by a binary std-formula. Hence,
in the special case where all the binary std-formulas represent reachability definitions, consistency
checking is tractable.
In the general case of a PST KB K = hA, Fi such that F consists of binary std-formulas, we
define an undirected graph, called the std-graph, where each maximal independent set represents a
world for K satisfying all the std-formulas in F. 8 We will later use this graph to characterize binary
std-formulas for which the consistency checking problem turns out to be tractable.
Definition 10 (std-graph). Given a PST KB K = hA, Fi such that F consists of binary stdformulas, the std-graph G(K) is an undirected graph hV, Ei whose sets of vertices V and edges
E are such that:
1) V consists of the set of all ground st-atoms of the form loc(id, {p}, t) where id  ID, p 
Space, and t  T ;
7. Recall that every substitution for Y1 must be a region containing a single point.
8. A maximal independent set in the std-graph is a maximal independent set in an undirected graph whose vertices are
st-atoms. The formal relationship between maximal independent sets in the std-graph and worlds of a PST KB is
given in Proposition 1.

760

fiK NOWLEDGE R EPRESENTATION IN P ROBABILISTIC S PATIO -T EMPORAL K NOWLEDGE BASES

2) E consists of
i) an edge between every pair of ground st-atoms in V referring to the same object and
time value, that is, for each p1 , p2  Space, p1 6= p2 , id  ID, t  T , hloc(id, {p1 }, t),
loc(id, {p2 }, t)i  E;
ii) an edge hloc(id1 , {p1 }, t1 ), loc(id2 , {p2 }, t2 )i between every pair of ground st-atoms in
V such that  loc(id1 , r1 , t1 )  loc(id2 , r2 , t2 )] with p1  r1 and p2  r2 belongs to
the set of ground std-formulas that can be derived from F.
We will just write G instead of G(K) where K is known.
Basically, each edge of G connects a pair of st-atoms that cannot belong together in any world
satisfying the std-formulas in F. In particular, an edge of type i) connects two st-atoms representing
the fact that an object is in two places at the same time  this is not admitted according to the
definition of world (see Definition 4). An edge of type ii) connects two st-atoms representing a fact
(i.e., object id1 is at point p1 at time t1 and object id2 is at point p2 at time t2 ) not consistent with a
ground std-formula entailed by F.
The structure of G is as follows. For each hid, ti pair, G contains a clique 9 of size |Space|
which consists of a vertex loc(id, {p}, t) for each point in Space and edges of type i)  in the
following, we refer to
 this clique as the clique for hid, ti pair or the hid, ti clique. For each ground
std-formula f =  loc(id1 , r1 , t1 )  loc(id2 , r2 , t2 )] that can be derived from F, G contains a
clique of size |r1 | + |r2 | which consists of a vertex loc(id1 , {p1 }, t1 ) for each point in r1 , a vertex
loc(id2 , {p2 }, t2 ) for each point in r2 , and edges of both types i) and ii)  we refer to this clique as
the clique for std-formula f .
Example 18. Let ID = {id1 , id2 }, T = [0, 1, 2], and Space = {p1 , p2 , p3 , p4 }. Assume that F
consists of the following std-formulas:
 f1 = X1 , X2 , Z1 [loc(X1 , {p2 , p4 }, Z1 )  loc(X2 , {p2 , p4 }, Z1 )  X1 6= X2  0  Z1 
Z1  1], saying that there cannot be two distinct objects in the region consisting of points
{p2 , p4 } at times 0 and 1;
 f2 = Z1 [loc(id1 , {p3 , p4 }, 0)  loc(id1 , {p1 }, Z1 )  1  Z1  Z1  2], saying that object
id1 cannot reach point p1 starting from region {p3 , p4 } at time 0 in 1 or 2 time units.
The std-graph G is shown in Figure 2(a), where, for the sake of readability, vertices are labelled
with the names of the points in Space to which they refer, while the id and time value of each vertex
is reported on the column and the row to which it belongs (for instance, vertex loc(id1 , {p1 }, 0) is
represented by the circle in the upper-left corner). Observe that G consists of 10 (maximal) cliques,
one for each of the 6 hid, ti pairs, and one for each of the 4 ground std-formulas derived from F.
Specifically, each hid, ti-clique, with id  {id1 , id2 } and t  [1..2], consists of the four vertices
loc(id, {pk }, t) with k  [1..4], while the cliques for the ground std-formulas are the following sets:
{loc(id1 , {p2 }, 0), loc(id2 , {p2 }, 0), loc(id1 , {p4 }, 0), loc(id2 , {p4 }, 0)},
{loc(id1 , {p2 }, 1), loc(id2 , {p2 }, 1), loc(id1 , {p4 }, 1), loc(id2 , {p4 }, 1)},
{loc(id1 , {p3 }, 0), loc(id1 , {p4 }, 0), loc(id1 , {p1 }, 1)}, and
{loc(id1 , {p3 }, 0), loc(id1 , {p4 }, 0), loc(id1 , {p1 }, 2)}.
2
761

fiPARISI & G RANT

id1

id2

p1

p2

p2

p1

p3

p4

p4

p3

p1

p2

p2

p1

p3

p4

p4

p3

id1,0

id2,0

p1

p2

p2

p1

id1,1

id2,1

p3

p4

p4

p3

id1,2

id2,2

t=0

t=1

t=2

(a)

(b)

Figure 2: (a) Std-graph G; (b) Auxiliary-graph AG.
It is worth noting that the cliques for the hid, ti pairs as well as those for the std-formulas are
not maximal cliques in general, as shown in the following.
Example 19. Continuing Example 18, assume that F is augmented with the following (ground)
std-formulas:
 f3 = [loc(id2 , {p3 }, 1)  loc(id2 , {p2 }, 2)]
 f4 = [loc(id2 , {p4 }, 1)  loc(id2 , {p2 }, 2)]
Thus, the clique for std-formula f3 consists of the set {loc(id2 , {p3 }, 1), loc(id2 , {p2 }, 2)}, while
that for f4 consists of the set {loc(id2 , {p4 }, 1), loc(id2 , {p2 }, 2)}. Both these cliques are not maximal ones, as they are included in the clique consisting of the set of vertices
{loc(id2 , {p3 }, 1), loc(id2 , {p4 }, 1), loc(id2 , {p2 }, 2)}. This is basically due to the fact that the
constraint imposed by f3 and f4 can be expressed more succinctly, as [loc(id2 , {p3 , p4 }, 1) 
loc(id2 , {p2 }, 2)] whose associated clique is maximal.
2
The following proposition follows from the definition of std-graph and the fact that every object
must be at a unique point for each time value.
Proposition 1. Let K = hA, Fi be a PST KB where F consists of binary std-formulas only. Every
maximal independent set of G consisting of a vertex from each hid, ti clique is in a one-to-one
correspondence with a world w  W such that w |= F.
9. Note that we use the terminology clique for a complete subgraph of G, and maximal clique for a clique not contained
in any other clique. We point this out since maximal cliques are often called simply cliques.

762

fiK NOWLEDGE R EPRESENTATION IN P ROBABILISTIC S PATIO -T EMPORAL K NOWLEDGE BASES

Observe that if there is no maximal independent set of G having the property stated in Proposition 1, then G must contain a maximal clique including at least two hid, ti cliques (this happens for
instance if F contains an std-formula [loc(id, Space, t1 )  loc(id, Space, t2 )]). In this case, no
world satisfies F and the PST KB is trivially inconsistent. In the general case, there may be an exponential number of maximal independent sets of G having the property stated in Proposition 1, and
the fact that the PST KB is consistent or not can be decided using G as explained in what follows.
3.5.1 C LIQUE -ACYCLIC S TD -G RAPHS
Our characterization of tractable cases of the consistency checking problem focuses on KBs with
binary std-formulas such that their std-graphs have a property, that we call clique-acyclic. We start
with some preliminary definitions.
Definition 11 (Binary maximal clique). For std-graph G we call a clique binary maximal iff it
contains vertices from two hid, ti pairs and is not properly included in any clique that contains
vertices from the same hid, ti pairs.
In particular, the std-graph G of Figure 2(a) has 4 binary maximal cliques all involving id1 .
Definition 12 (Clique-subgraph). We call a subgraph of std-graph G a clique subgraph iff it contains
all the vertices of G, one edge from each hid, ti clique as well as one new distinct edge from each
binary maximal clique.
So any clique-subgraph of the std-graph of the graph G of Figure 2(a) has 10 edges, one from
each of the 6 hid, ti pairs and one from each binary maximal clique.
Definition 13 (Clique-acyclic std-graph). Std-graph G is said to be clique-acyclic iff all of its cliquesubgraphs are acyclic graphs (that is, forests). G is called clique-cyclic if it is not clique-acyclic.
Basically, clique-acyclicity means that no cycle can be found in the std-graph after compressing
all binary maximal cliques into single edges and using just one edge of each hid, ti-clique. It is easy
to see that the std-graph shown in Figure 2(a) is clique-acyclic. As stated in the next proposition,
clique-acyclicity can be checked using the following auxiliary graph which basically compresses a
clique-subgraph to its essential structure. It will be clear from the definition that the same auxiliary
graph is obtained from all clique-subgraphs of a graph G.
Definition 14 (Auxiliary graph). The auxiliary graph for G is the undirected graph AG = hV 0 , E 0 i
such that:
 V 0 consists of a vertex for each hid, ti pair, with id  ID and t  T ;
 E 0 consists of all the edges for binary maximal cliques in a clique-subgraph where each
previous vertex loc(id, {p}, t) is replaced by the corresponding hid, ti pair. We will denote as
C(e) the binary maximal clique C from which the edge e originated.
The auxiliary graph for the std-graph of Figure 2(a) is shown in Figure 2(b). As an example,
C(e) for edge e = h(id1 , 0), (id1 , 1)i consists of the following set of vertices
{loc(id1 , {p3 }, 0), loc(id1 , {p4 }, 0), loc(id1 , {p1 }, 1)}.
The following proposition follows from Definitions 13 and 14.
763

fiPARISI & G RANT

Proposition 2. Std-graph G is clique-acyclic iff the auxiliary graph AG is acyclic (that is, AG is a
forest).
In the following, we introduce a set of linear inequalities that can be used to check the consistency of PST KBs where the std-formulas are binary and the auxiliary graph is acyclic. Then in
the next subsection we will do the same for a special case of a cyclic auxiliary graph. In both cases
we will be working with a single connected component of the auxiliary graph. Suppose that AG
has n connected components C1 , . . . , Cn and let Ki be the PST KB corresponding to Ci for each
i. If K has a model M we show how to obtain a model Mi for Ki . Let wi be a world appropriate
to the hid, ti pairs for Ki .PLet Wi be all the worlds of K that extend wi to the hid, ti values not
in Ki . Define Mi (wi ) = wWi M (w). Going in the other direction, suppose that M1 , . . . , Mn
are models for the PST KBs corresponding to Ci , . . . , Cn respectively. For any world w for K, let
w1 , . . . , wn be the restrictions of w to C1 , . . . , Cn respectively. Defining M (w) = ni=1 M (wi ) is
then a model of K. We have shown the following result.
Proposition 3. K is consistent iff all of the PST KBs corresponding to the connected components
of G (and hence of AG) are consistent.
Hence in our proofs it suffices to assume that AG has a single connected component.10
Definition 15 (BC(K)). Let K = hA, Fi be a PST KB such that F consists of binary std-formulas
only. BC(K) consists of the following (in)equalities:
P
vid,t,p  u;
(1)  loc(id, r, t)[`, u]  A: ` 
pr

(2)  id  ID, t  T :

P

vid,t,p = 1;

pSpace

(3)  id  ID, t  T, p  Space: vid,t,p  0;
P
vid,t,p  1.
(4) For each edge e of AG:
loc(id,{p},t)C(e)

The following theorem states that checking whether BC(K) has a feasible solution is equivalent
to deciding the consistency of PST KBs K where std-formulas are binary and generate an acyclic
auxiliary graph. The intuition behind this result is as follows. If there is a feasible solution of
BC(K), then inequalities (1)-(3) entail that for each hid, ti pair, the events that object id at time t is
at point p  Space can be arranged to fit the whole probability space without overlapping. Roughly
speaking this would suffice to define a model for a PST KB without std-formulas by combining
the distributions obtained for each hid, ti pair. Considering std-formulas means that some events
cannot occur together, that is, they cannot coexist in the same portion of the probability space.
Intuitively, the fact that inequality (4) of BC(K) is satisfied for an edge e entails that the events
corresponding to the st-atoms in C(e) can be arranged in distinct portions of the probability space
(avoiding overlaps). If the auxiliary graph AG is acyclic, this reasoning can be inductively repeated
for each edge of AG without ever reconsidering the arrangement of events corresponding to the
st-atoms considered previously. The satisfiability of inequality (4) of BC(K) is also a necessary
10. We do not indicate this in the statement of the following theorem because if AG has several connected components,
the proof works for each one and then Proposition 3 is applied.

764

fiK NOWLEDGE R EPRESENTATION IN P ROBABILISTIC S PATIO -T EMPORAL K NOWLEDGE BASES

condition for the consistency of K as the fact that it is not satisfied for an edge e of AG intuitively
means that the events corresponding to the st-atoms in C(e), even taken alone, cannot be arranged
in the probability space without overlapping.
Theorem 6. Let K = hA, Fi be a PST KB where F consists of binary std-formulas only. If AG is
acyclic, then K is consistent iff there is a feasible solution of BC(K).
Proof. We will use the notation K0 for K with F removed, that is, K0 = hA, i. Then, M(K) is a
subset of M(K0 ).
() We prove the contrapositive: if there is no feasible solution of BC(K), then K is inconsistent. The simplest case is where BC(K) has no feasible solution due to the fact that BC(K0 ) has no
feasible solution. Then there are no inequalities (4), hence K0 is not consistent, and thus K is not
consistent as well.
Otherwise, there is a feasible solution of BC(K0 ) and thus BC(K) has no feasible solution due
to the fact that at least one of the inequalities
of item (4) of Definition 15 is not satisfied. Therefore,
P
there is an edge e of AG such that loc(id,{p},t)C(e) vid,t,p > 1. To show that K is inconsistent we
show that no interpretation can be a model of K. So suppose that M is an interpretation that is a
model for K. Since a pair A1 , A2 of st-atoms belong to C(e) iff (a) [A1  A2 ] belongs to the set of
ground formulas that can be derived by F, or (b) A1 = loc(id, {p1 }, t) and A2 = loc(id, {p2 }, t)
where p2 6= p1 , there is no world w  W(K) such that M (w) > 0, w |= A1 , and w |= A2 (in
particular, in case (b) there is no world w  W(K) such that w |= A1 , and w |= A2 , whether or
not M (w) > 0). That is, every world w for K which is assigned a non-zero probability by M is
such that it satisfies at most one st-atom in C(e). Hence, W(K) can be partitioned into |C(e)| + 2
pairwise disjoint sets as follows:
(i) the set of worlds w  W(K) such that M (w) = 0 (this set includes all the worlds satisfying
at least two st-atoms in C(e))
(ii) the set of worlds w  W(K) such that M (w) > 0 and w does not satisfy any st-atom in C(e);
(iii) for each st-atom Ai  C(e), the set of worlds w  W(K) such that M (w) > 0, w |= Ai , and
w 6|= Aj where Aj  C(e) (and is different from Ai ).
Therefore, the sum of the probabilities of the worlds satisfying at least one st-atom in C(e) is equal
to the sum, for each st-atom A in C(e), of the sum of the probabilities of the worlds satisfying atom
A  C(e) but no other st-atoms in C(e), that is
X
w  W(K)
w |= loc(id, {p}, t)
loc(id, {p}, t)  C(e)

M (w) =

X

X

loc(id,{p},t)C(e)

w  W(K)
w |= loc(id, {p}, t)
w 6|= loc(id0 , {p0 }, t0 )
loc(id0 , {p0 }, t0 )  C(e)

M (w).

Now recall that any model for K is also a model for K0 . The above-mentioned partitioning of
W(K) also holds for W(K0 ). Therefore, there is a solution  for BC(K0 ), one-to-one corresponding
to a model M for K0 , such that the sum of the probabilities assigned by M to the worlds for K0 satisfying an st-atom loc(id, {p}, t) is equal to the value assigned to variable vid,t,p by . In particular,
765

fiPARISI & G RANT

for a given st-atom loc(id, {p}, t)  C(e), we have that
X
X
M (w) =
W(K0 )

M (w) = vid,t,p .

W(K0 )

w
w |= loc(id, {p}, t)

w
w |= loc(id, {p}, t)
loc(id, {p}, t)  C(e)
w 6|= loc(id0 , {p0 }, t0 )
loc(id0 , {p0 }, t0 )  C(e)

Considering all the st-atoms in C(e) we obtain that
X
X
M (w) =
W(K0 )

X

loc(id,{p},t)C(e)

w
w |= loc(id, {p}, t)
loc(id, {p}, t)  C(e)

=

X

M (w) =

W(K0 )

w
w |= loc(id, {p}, t)
w 6|= loc(id0 , {p0 }, t0 )
loc(id0 , {p0 }, t0 )  C(e)

vid,t,p > 1.

loc(id,{p},t)C(e)

The inequality above holds since the inequality (4) of Definition 15 is not satisfied for edge e.
Finally, since the following inequality holds due to the definition of model
X
X
M (w) 
M (w),
wW(K0 )

w  W(K0 )
w |= loc(id, {p}, t)

and the latter is strictly greater than one, it follows that M is not a PDF over W(K0 ), meaning that
M is a not model for K0 . Hence no interpretation can be a model for K and so K is inconsistent.
() We now prove that if the auxiliary graph AG is acyclic and BC(K) has a feasible solution,
then K is consistent. We prove this by mathematical induction on the number of edges of AG.
Base case: The number of edges is 0, so AG consists of a set of isolated vertices. The fact
that AG contains no edges means that the set of ground std-formulas that can be derived from F is
empty. Thus there is a feasible solution of BC(K) iff there is a feasible solution of CC(K0 ). Using
Fact 1, we obtain that K is consistent.
Inductive step: We prove that if the statement holds for  edges of AG, then it holds for  + 1
edges. So we are given an acyclic graph AG with  + 1 edges, that we write as AG +1 . By
acyclicity we can choose a subgraph, AG  with  edges such that the new edge connects a vertex
to an isolated vertex of AG  and AG  is clearly acyclic. Let K = hA, F  i be the PST KB where
F  consists of the subset of ground std-formulas derived from F so that the std-graph G  has the
auxiliary graph AG  . The induction hypothesis is that if there is a feasible solution of BC(K ),
then K is consistent.
We write K+1 = hA, F +1 i where F +1 consists of the ground std-formulas in F  plus those
corresponding to the new edge e, so that AG +1 turns out to be the auxiliary graph for G(K+1 ).
Assuming the induction hypothesis, we now show that if there is a feasible solution of BC(K+1 ),
then K+1 is consistent.
Let  be a solution for BC(K+1 ). Obviously,  is also a solution for BC(K ), as it consists
of a subset of the (in)equalities in BC(K+1 ). The fact that  is a solution for BC(K ) means that
there is a model M  for K such that the sum of the probabilities assigned by M  to the worlds for
766

fiK NOWLEDGE R EPRESENTATION IN P ROBABILISTIC S PATIO -T EMPORAL K NOWLEDGE BASES

K satisfying an st-atom loc(id, {p}, t) is equal to the value (vid,t,p ) assigned to variable vid,t,p by
. Since  is a solution also for BC(K+1 ), and thus inequality (4) of Definition 15 holds, it follows
that
X
X
X
(vid,t,p )  1.
M  (w) =
loc(id,{p},t)C(e)

w  W(K ),
w |= loc(id, {p}, t)

loc(id,{p},t)C(e)

That is, M  is a model for K with the property that the sum of the probabilities of the worlds
satisfying at least one st-atom in C(e) is less than or equal to one.
If M  is such that it assigns probability equal to zero to every world in W(K ) containing at
least two st-atoms in C(e), then we are done, since M  turns out to be a model for K+1 too.
Otherwise, starting from M  , we show that a model M +1 for K+1 can be constructed by
reasoning as follows.
We recall that the new edge e being added to AG  to obtain AG +1 is of the form h(id1 , t1 ),
(id2 , t2 )i. Let C1 (e) and C2 (e) be the sets of st-atoms in C(e) of the form loc(id1 , {p}, t1 ) and
loc(id2 , {p}, t2 ), respectively. Hence, C(e) = C1 (e)  C2 (e) and C1 (e)  C2 (e) = . Let
 = hw1 , w2 , . . . , wn i be a permutation of the worlds w  W(K ) which are assigned by M 
a probability greater than zero (i.e., M  (w) > 0) and such that the first k worlds in  satisfy an statom in C1 (e). Note that, W(K ) = W(K+1 ), and that a world in W(K ) consists of at most one
atom in C1 (e), as it is a function whose domain is ID  T (see Definition 4) and all of the st-atoms
in C1 (e) refer to the same id andP
time value. Let us denote by i the sum of the probabilities of the
first i worlds in , that is, i = ij=1 M  (wj ), where wj is in . Thus, k is equal to the sum of
the probabilities assigned by M  to the worlds satisfying an st-atom in C1 (e), i.e.,
X

k =

X

M  (w) =

w  W(K ),
w |= loc(id, {p}, t),
loc(id, {p}, t)  C1 (e)

(vid,t,p ).

loc(id,{p},t)C1 (e)

Let S be the subset of Space consisting of points p such that variables vid2 ,t2 ,p are assigned a
value greater than zero by , that is, S = {p | p  Space, (vid2 ,t2 ,p ) > 0}. Observe that these
variables correspond to st-atoms referring to endpoint (id2 , t2 ) of edge e. Let  = hp1 , p2 , . . . , pm i
be a permutation of the points in S such that the first h points p correspond to the variables vid2 ,t2 ,p
such that loc(id2 , {p}, t2 ) 6 C2 (e), and the subsequent points in  (with index in [h + 1, .., m])
correspond to the variables vid2 ,t2 ,p such that loc(id2 , {p}, t2 )  C2 (e). Let i be the sum of the
Pi
values assigned by  to the first i points in , that is, i =
j=1 (vid2 ,t2 ,pj ) where pi  .
Observe thatPm = 1 since equality (2) of Definition 15 holds for BC(K+1 ), and that h =
1
(vid,t,p ). The fact that inequality (4) of Definition 15 holds for BC(K+1 )
loc(id2 ,{p},t2 )C2 (e)

entails that,
1  h + k =

X

(vid,t,p ) +

loc(id,{p},t)C2 (e)

X

(vid,t,p )  1.

loc(id,{p},t)C1 (e)

Therefore, we obtain that k  h . Intuitively, this means that it is possible to define a PDF over
the worlds for K+1 so that no world satisfies two st-atoms in C(e), while keeping satisfied all the
PST atoms and std-formulas satisfied by M  . This is formally described below.
767

fiPARISI & G RANT

We define a model M +1 for K+1 as follows. Let U = hu1 , u2 , . . . , uz i be the sequence
consisting of the values in {1 , . . . , n }  {1 , . . . , m } ordered in ascending order. We define the
following z non-zero probability worlds for K+1 . For each ui  U , let wi be a world such that:
 id  ID \ {id2 }, t  T \ {t2 }, wi (id, t) = wj (id, t), where wj  W(K ) and j is the
smallest subscript such that j  ui .
 wi (id2 , t2 ) = pj , where pj  S and j is the smallest subscript such that j  ui .
We define M +1 as the PDF over W(K+1 ) such that (i) M +1 (w1 ) = v1 , (ii) M +1 (wi ) =
vi  vi1 for each i  [1..z], and (iii) for all the other worlds w  W(K+1 ), M +1 (w) = 0.
We now show that M +1 is a model for K+1 . First, observe that M +1 is a model for K .
In fact, for each id  ID \ {id2 }, id  T \ {t2 }, and p in Space, the probability that object id is
at time value t at point p according to M +1 is equal to the probability that id is at time value t at
point p according to M  , that is
X
X
id  ID \ {id2 }, t  T \ {t2 }, p  Space,
M +1 (w) =
M  (w).
w  W(K ),
w(id, t) = p

w  W(K+1 ),
w(id, t) = p

Moreover, for the hid2 , t2 i pair,
X
p  Space,

M +1 (w) = (vid2 ,t2 ,p ) =

X

M  (w).

w  W(K ),
w(id, t) = p

w  W(K+1 ),
w(id, t) = p

This ensures that, for each PST atom a = loc(id, r, t)[`, u]  A, the sum of the probabilities
assigned by M +1 to the worlds satisfying the st-atom loc(id, r, t) belong to the probability interval
[`, u] specified by a, since the same hold for M  . Moreover, none of the worlds which is assigned
a probability equal to zero by M  is assigned a probability greater than zero by M +1 , as  only
consists of worlds which are assigned a non-zero probability by M  . Therefore, M +1 is a model
for K .
Finally, to show that M +1 is a model for K+1 , it suffices to observe that k  h entails that
there is no world wi for K+1 (with i  [1..z]) which is assigned a probability greater than zero
and which satisfies two atoms in C(e). This completes the proof.
The following example shows the usage of the result of Theorem 7 to check the consistency of
a PST KB.
Example 20. Continuing Example 18, assume that K is a PST KB where F is the set of stdformulas given in Example 18 and A consist of the following set PST atoms:
loc(id1 , {p3 , p4 }, 0)[.7, 1], loc(id1 , {p1 }, 1)[.2, .5], loc(id1 , {p1 }, 2)[.3, .8], loc(id2 , {p2 }, t)[.7, 1]
for t  [0..2]. It can be checked that for this PST KB the corresponding set of linear inequalities BC(K) has a feasible solution. Thus, since the auxiliary graph AG(K) shown in Figure 2(b) is
acyclic, it follows that K is consistent.
Consider now the PST KB K0 obtained from K by replacing the atom loc(id1 , {p1 }, 2)[.3, .8]
with loc(id1 , {p1 }, 2)[.8, .8], where the lower probability is .8 instead of .3. In this case, BC(K0 )
includes the inequalities 0.7  vid1 ,0,p3 + vid1 ,0,p4  1 and 0.8  vid1 ,2,p1  0.8 (due to PST
atoms), and vid1 ,0,p3 + vid1 ,0,p4 + vid1 ,1,p1  1 (due to the edge h(id1 , 0), (id1 , 2)i of AG(K)).
Clearly, there is no feasible solution of BC(K0 ), and thus K0 is not consistent.
2
768

fiK NOWLEDGE R EPRESENTATION IN P ROBABILISTIC S PATIO -T EMPORAL K NOWLEDGE BASES

We note the fact that the acyclicity of AG is not used in the left-to-right proof of Theorem 6.
Therefore, whether or not AG(F) is acyclic, for PST KBs where the std-formulas are binary, the
necessary condition stated in the following theorem can be used for checking consistency.
Theorem 7. Let K = hA, Fi be a PST KB where F consists of binary std-formulas only. If there
is no feasible solution of BC(K), then K is not consistent.
The condition that BC(K) has no feasible solution is stronger than that NC(K) has no feasible
solution, used in Theorem 3 for general std-formulas, since it can be easily checked that every
solution of BC(K) is a solution of NC(K), but the converse does not hold in general. Thus, there
are PST KBs K, such that using Theorem 3 we cannot conclude anything about the consistency of
K, as NC(K) has a feasible solution, but we can show that K is inconsistent by using the result of
Theorem 7.
Example 21. We have already observed that the fact that the PST KB K of Example 16 is not
consistent cannot be concluded by checking whether there is a feasible solution of NC(K). However,
it is easy to see that BC(K) has no feasible solution in this case, and thus we can conclude that the
PST KB of Example 16 is not consistent by applying the result of Theorem 7.
2
Although the necessary condition in Theorem 6 can be still used to check the consistency when
the auxiliary graph is cyclic (see Theorem 7), the sufficient condition of Theorem 6 entails that a
PST KB K is consistent only if the corresponding set BC(K) of linear inequalities has a feasible
solution and the auxiliary graph is acyclic. In the following, we identify a class of std-graphs for
which the consistency checking problem remains tractable even if the std-graph is clique-cyclic (and
thus the auxiliary graph is cyclic).
3.5.2 S IMPLE C LIQUE -C YCLIC S TD -G RAPHS
In this section we provide a set of linear inequalities for checking the consistency of a PST KB
whose std-graph satisfies the following property.
Definition 16 (Simple clique-cyclic std-graph). Std-graph G is said to be simple clique-cyclic iff
its auxiliary graph AG is a simple graph 11 and each cyclic connected component of G contains a
single maximal clique that is not an hid, ti clique.
The following is an example of a simple clique-cyclic std-graph. We provide both the std-graph
and its auxiliary graph.
Example 22. For the PST KB K = hA, Fi of Example 15, we obtain the std-graph G shown
in Figure 3(a). The auxiliary graph AG is shown in Figure 3(b). G consists of a single connected component where there is one maximal clique Cl that is not an hid, ti clique, namely
Cl = {loc(id, {p0 }, 0), loc(id, {p0 }, 1), loc(id, {p0 }, 2)}. It is easy to see that G is simple cliquecyclic.
2
The following theorem states that checking the consistency of a PST KB whose std-graph is
simple clique-cyclic can be accomplished by checking whether there is a feasible solution of a set
of linear inequalities.
We start by defining this new linear system: CL(K).
11. There are no loops and no multiple edges between vertices.

769

fiPARISI & G RANT

id
p1

t=0
p2

p0

p1

id1,0

t=1
p2

p0

id1,1
p1

id

t=2

id1,2

p2

p0

(a)

(b)

Figure 3: (a) Std-graph and (b) auxiliary graph for the PST KB of Example 15.
Definition 17 (CL(K)). Let K = hA, Fi be a PST KB such that F consists of binary std-formulas
only. CL(K) consists of the following (in)equalities:
(1) loc(id, r, t)[`, u]  A: ` 

P

vid,t,p  u;

pr

(2) id  ID, t  T :

P

vid,t,p = 1;

pSpace

(3) p  Space, id  ID, t  T : vid,t,p  0;
(4) (a) For each
P acyclic connected component of G (and hence AG), for each edge e of AG:
vid,t,p  1.
loc(id,{p},t)C(e)

(b) For each cyclic connected component
of G (and hence AG), for the maximal clique Cl
P
that is not an hid, ti clique:
vid,t,p  1.
loc(id,{p},t)Cl

The intuition behind the result stated in the following theorem is similar to that given for the
tractable case of Theorem 6. Indeed, if G is simple clique-cyclic and CL(K) has a feasible solution,
then the events corresponding to the st-atoms in each edge e of an acyclic connected component AG,
as well as the events corresponding to the st-atoms in each maximal clique of a connected component of AG, can be arranged in the probability space avoiding overlaps. This basically suffices to
define a model for K.
Theorem 8. Let K = hA, Fi be a PST KB where F consists of binary std-formulas only. If G is
simple clique-cyclic, then K is consistent iff there is a feasible solution of CL(K).
770

fiK NOWLEDGE R EPRESENTATION IN P ROBABILISTIC S PATIO -T EMPORAL K NOWLEDGE BASES

Proof. As explained earlier we will deal only with a single cyclic connected component of G.
() The proof follows by reasoning as in the left-to-right proof of Theorem 6.
() Let  be a solution for CL(K). Obviously,  is alsoP
a solution for CC(K0 ) where K0 =
0
0
0
hA,
P i, and there is a model M for K such that (vid,t,p ) = wW(K0 ),w|=loc(id,{p},t) M (w) and
loc(id,{p},t)Cl (vid,t,p )  1. Given this, we show that starting from  we can define a new model
M for K0 which is also a model for K.
For each hid, ti pair, we define S(id, t) as the subset of Space consisting of points p such that
vid,t,p is assigned a value greater than zero by , i.e., S(id, t) = {p | p  Space, (vid,t,p ) > 0}.
We distinguish between the following two sets of hid, ti pairs: IDT1 (resp. IDT2 ) is the set of
hid, ti pairs for which there is not (resp. there is) a point p in S(id, t) such that loc(id, {p}, t)  Cl.
We separately consider the pairs in IDT1 and IDT2 , and for each hid, ti pair belonging to one of
these sets, define a sequence of points in S(id, t) along with a sequence of cumulative probability
values that will be used to build the model M .
We start with the set IDT1 . Let hidi , ti i, with i  [1..|IDT1 |] be the i-th pair of IDT1 (after
i
ordering the pairs in IDT1 according to any fixed order). Let i = hp1i , . . . , pm
i i be a permutation
of the points in S(idi , tP
i ), and let i (k) be the sum of the values assigned by  to the first k points
in i , that is, i (k) = kj=1 (vidi ,ti ,pj ). Thus, hi (1), . . . , i (mi )i is a sequence of (cumulative)
i

i
probability values associated with the sequence of points hp1i , . . . , pm
i i from the i-th pair of IDT1 .
Now consider the pairs in IDT2 , i.e, those occurring in some st-atom in Cl. Denote with
hidi , ti i, with i  [1..|IDT2 |], the i-th pair of IDT2 (according to any fixed order). For the first
1
pair in IDT2 , let 1 = hp11 , . . . , ph1 1 , ph1 1 +1 , . . . , pm
1 i be a permutation of the points in S(id1 , t1 )
such that the first h1 points p correspond to the variables vid1 ,t1 ,p such that loc(id1 , {p}, t1 )  Cl
1
(consequently, the other points p  {ph1 1 +1 , . . . , pm
1 } correspond to variables vid1 ,t1 ,p such that
loc(id1 , {p}, t1 ) 6 Cl).
i
For each hidi , ti i with i  [2..|IDT2 |], let i = hp1i , . . . , pgi i , pigi +1 , . . . , pigi +hi , pigi +hi +1 , . . . , pm
i i
g
+h
+1
g
m
be a sequence of the points in S(idi , ti ) such that (i) each point p in {p1i , . . . , pi i , pi i i , . . . , pi i }
(resp. p in {pgi i +1 , . . . , pgi i +hi }) corresponds to a variable vidi ,ti ,p such that loc(idi , {p}, ti ) 6 Cl
(resp. loc(idi , {p}, ti )  Cl); and (ii) all the points in the sequence are distinct except for pgi i and
pgi i +hi +1 that may refer to the same point in S(idi , ti ). Denoting as i (k) the sum of the values
P
assigned by  to the first k points in i (i.e., i (k) = kj=1 (vidi ,ti ,pj )), we choose the sequences
i
2 , . . . , |IDT2 | such that, for each i  [2..|IDT2 |], i (gi ) = i1 (gi1 +hi1 ) (assume that g1 = 0).
Note that we can make such a choice since the following holds:

|IDT2 | hi
X X

|IDT2 |

(vidi ,ti ,pk ) =

X

i

i=1 k=1

|IDT2 |

i (gi + hi )  i (gi ) =

i=1

X

X

(vidi ,ti ,p )  1.

i=1 loc(idi ,{p},ti )Cl

The way the sequences h1 , . . . , |IDT2 | i and hi (1), . . . , i (mi )i are defined will allow us to build
a model M for K0 such that no world which is assigned a probability greater than zero satisfies two
distinct st-atoms in Cl. This will ensure that M is a model for K too.
We now
S define such a model M . LetSV = hv1 , v2 , . . . , vz i be the sequence consisting of the
values in iIDT1 {i (1), . . . , i (mi )}  iIDT2 {i (1), . . . , i (mi )} ordered in ascending order.
We define the following z non-zero probability worlds for K0 (note that a world for K0 is also a
world for K). For each vj  V , let wj be a world such that:
771

fiPARISI & G RANT

 for each hidi , ti i  IDT1 , we define wj (idi , ti ) = pki , where pki is the left-most value in
hi (1), . . . , i (mi )i such that i (k)  vj .
 for each hidi , ti i  IDT2 , we define wj (idi , ti ) = pki , where pki is the left-most value in
hi (1), . . . , i (mi )i such that i (k)  vj .
Finally, we define M as the PDF over W(K0 ) such that (i) M (w1 ) = v1 , (ii) M (wj ) = vj  vj1
for each j  [1..z], and (iii) M (w) = 0 for all the other worlds w  W(K0 ). It is easy to check
0
0
that M is a model
P for K that corresponds to the same solution  for CC(K ), that is, M is such that
(vid,t,p ) = wW(K0 ),w|=loc(id,{p},t) M (w). Moreover, as the construction shown above ensures
that no non-zero probability world for K0 satisfies two distinct st-atoms in Cl, it follows that M is a
model for K.
Theorem 8 can be used, for instance, to decide if the PST KB of Example 15 is consistent.
Example 23. For the PST KB K = hA, Fi of Example 15, CL(K) is the linear system obtained
from CC(hA, i) by augmenting it with the inequality vid,0,p0 + vid,1,p0 + vid,2,p0  1. Since CL(K)
also contains the inequality 0.5  vid,t,p0 for each t  [0..2] (due to the presence of the PST atom
loc(id, {p0 }, t)[0.5, 1] in A), it follows that CL(K) has no feasible solution, from which we conclude
that K is not consistent (here G is the clique-cyclic std-graph shown in Figure 3(a)).
2
We conclude the section with the following theorem stating that checking consistency for the
PST KBs identified in Section 3.5 is tractable.
Theorem 9. Let K = hA, Fi be a PST KB where F consists of binary std-formulas only. If G is
clique-acyclic or simple clique-cyclic, then checking the consistency of K is in P T IM E.
Proof. If G consists of one connected component, the statement follows from the facts that building
G, as well as checking whether G is acyclic (resp. simple clique-cyclic), can be accomplished in
polynomial time w.r.t. the size of K and checking whether there is a feasible solution of BC(K)
(resp., CL(K)) is polynomial too. If G consists of more than one connected component, the statement follows from the facts that finding all connected components of G can be done in polynomial
time, and, using Proposition 3, we only need to check the consistency of the PST KBs corresponding to each of the connected components in order to decide the consistency of the whole PST
KB.

4. Querying PST Knowledge Bases
This section investigates the complexity of checking answer to queries. Section 4.1 contains the
basic definitions. Section 4.2 contains the major result about the complexity of checking answers
to queries. Finally, Section 4.3 gives sufficient and necessary conditions for answering queries and
tractable cases.
4.1 Optimistic and Cautious Answers
The problem of querying SPOT data has been investigated by Parker et al. (2007a, 2009), Parisi,
Parker, Grant, and Subrahmanian (2010), and Grant, Molinaro, and Parisi (2013) in more specific
772

fiK NOWLEDGE R EPRESENTATION IN P ROBABILISTIC S PATIO -T EMPORAL K NOWLEDGE BASES

frameworks corresponding to PST KBs (of the form hA, i), where only PST atoms were considered. In this section, we address the problem of answering selection queries in general (consistent) PST KBs. These kinds of queries were considered by Parker et al. (2007a, 2009), and Parisi
et al. (2010), while Grant et al. (2013) focused on count queries.
A selection query is an expression of the form (?id, q, ?t, [`, u]), where q is a region and [`, u] is
a probability interval. Intuitively, a selection query says: Given a region q and a probability interval
[`, u], find all objects id and times t such that id is inside q at time t with a probability in the interval
[`, u]. There are two semantics for interpreting this statement, leading to two types of answers to
selection queries. Optimistic answers are objects and time values that may be in the query region
with probability in the specified interval, whereas cautious answers consist only of those objects
and time values that are guaranteed to be in that region with probability in the given interval. Thus,
the cautious answers are a subset of the optimistic ones.
The following definition extends the original definition of optimistic and cautious selection
query answers to the general case of consistent PST knowledge bases.
Definition 18 (Optimistic/Cautious Query Answers). Let K be a consistent PST KB, and Q =
(?id, q, ?t, [`, u]) a selection query. Then,
 hid, ti
Pis an optimistic answer to Q w.r.t. K iff there exists a model M  M(K) s.t.
M (w)  [`, u].
w|=loc(id,q,t)

 hid, ti
Pis a cautious answer to Q w.r.t. K iff for every model M  M(K),
M (w)  [`, u].
w|=loc(id,q,t)

Example 24. Let q1 = {(7, 3), (7, 4)} (q1 overlaps with region c, see Figure 1). Model M of
Example 9 entails that hid1 , 9i is an optimistic answer to Q = (?id, q1 , ?t, [.7, 1]), as w1 (id1 , 9) =
w2 (id1 , 9) = w3 (id1 , 9) = (7, 4)  q1 and M (w1 ) + M (w2 ) + M (w3 ) = 1  [.7, 1].
Let q2 be any region including region c. hid1 , 9i is a cautious answer to Q0 = (?id, q2 , ?t, [.7, 1]), as
according to any model for Kex , id1 must be in region c (and thus in q2 ) at time 9 with probability
in [.9, 1] (due to loc(id1 , c, 9)[.9, 1]  Aex ). Clearly, hid1 , 9i is also an optimistic answer to Q0 . 2
4.2 Complexity of Query Answering
The following theorem shows how consistency checking can be used to answer selection queries
under both the optimistic and the cautious semantics.
Theorem 10. Let K = hA, Fi be a consistent PST KB, and Q = (?id, q, ?t, [`, u]). Then,
1) hid, ti is an optimistic answer to Q w.r.t. K iff hA  {loc(id, q, t)[`, u]}, Fi is consistent.
2) hid, ti is a cautious answer to Q w.r.t. K iff both hA  {loc(id, q, t)[0, ` ]}, Fi and hA 
{loc(id, q, t)[u + , 1]}, Fi are not consistent, where  is the smallest rational number with
precision m3 , where m = |A| + |F|  |ID|  |T |  |Space| + |ID|  |T | + |ID|  |T |  |Space|.
Proof. Statement 1) easily follows from Definitions 6 and 18.
Statement 2) follows from the fact that hid, ti is a cautious answer to Q w.r.t. K iff i) hid, ti is
not an optimistic answer to Q0 = (?id, q, ?t, [0, ` ]) w.r.t. K, and ii) hid, ti is not an optimistic
773

fiPARISI & G RANT

answer to Q00 = (?id, q, ?t, [u + , 1]) w.r.t. K, where  is a small non-zero constant whose size
is polynomial w.r.t. the size of K. To show the existence of , we observe that (i) checking the
consistency of K can be reduced to deciding the PSAT instance K consisting of m clauses, where
m = |A| + |F|  |ID|  |T |  |Space| + |ID|  |T | + |ID|  |T |  |Space|, as shown in the membership
proof of Theorem 1; and (ii) if a satisfying probability distribution for an instance of PSAT with
m clauses exists, then there is one with at most m + 1 non-zero probabilities, and with entries
consisting of rational numbers with precision O(m2 ) (Georgakopoulos et al., 1988; Papadimitriou
& Steiglitz, 1982); Then, choosing  equal to the smallest rational number with precision m3 suffices
to obtain a sufficiently small non-zero constant, whose size is polynomial w.r.t. the size of K,
such that hA  {loc(id, q, t)[0, ` ]}, Fi (resp., hA  {loc(id, q, t)[u + , 1]}, Fi) is consistent iff
hA  {loc(id, q, t)[0, ` ]}, Fi (resp., hA  {loc(id, q, t)[u + , 1]}, Fi) is consistent, where  is
an infinitely small value greater than zero.
The following corollary follows from Theorems 1 and 10.
Corollary 1. Let K = hA, Fi be a consistent PST KB. Given a query Q = (?id, q, ?t, [`, u]), then
1) deciding whether hid, ti is an optimistic answer to Q w.r.t. K is in NP.
2) deciding whether hid, ti is a cautious answer to Q w.r.t. K is coNP.
The problem of deciding whether a pair hid, ti is an optimistic/cautious answer to a selection
query Q can be solved in polynomial time for PST KBs whose F component is empty (Parker et al.,
2007a, 2009; Parisi et al., 2010). However, in the presence of integrity constraints this problem
becomes hard.
Theorem 11. Let K = hA, Fi be a consistent PST KB. Given a query Q = (?id, q, ?t, [`, u]), then
1) deciding whether hid, ti is an optimistic answer to Q w.r.t. K is NP-hard.
2) deciding whether hid, ti is a cautious answer to Q w.r.t. K is coNP-hard.
Proof. We first prove item 1). We show a reduction to our problem from the NP-hard Hamiltonian
path problem (Papadimitriou, 1994). Given a directed graph G = hV, Ei, where V = {v0 , . . . , vk }
is the set of k +1 vertexes of G and E is the set of its edges, we construct an instance of our problem
as follows. Let ID = {id}, Space = V  {p0 , . . . , pk1 }  {pT , pF }, and T = [0, . . . , 2k + 1].
K is the pair hA, Fi such that A consist of the PST atom loc(id, v0 , 0)[1, 1] and F consists of the
following formulas:
 i  [0..k  1], f1i = Z1 , Z2 [loc(id, {vi }, Z1 )  loc(id, Space \ V 0 , Z2 )  Z2 = Z1 + 1]
where V 0 = {vj |(vi , vj )  E}{pi , pT , pF }. That is, the locations id can reach starting from
vi in one time step are only the locations vj s.t. (vi , vj )  E or any point in {pi , pT , pF }.
 f1k = Z1 , Z2 [loc(id, {vk }, Z1 )  loc(id, Space \ V 00 , Z2 )  Z2 = Z1 + 1] where V 00 =
{vj |(vk , vj )  E}  {pT , pF }. This formula says that the locations id can reach starting from
vk in one time step are only locations vj such that (vk , vj )  E or those in {pT , pF }.
 i  [0..k1], f2i = Z1 , Z2 [loc(id, {pi }, Z1 )loc(id, Space\{vi+1 }, Z2 )Z2 = Z1 +1],
saying that the only location id can reach starting from pi in one time step is vi+1 .
774

fiK NOWLEDGE R EPRESENTATION IN P ROBABILISTIC S PATIO -T EMPORAL K NOWLEDGE BASES

 f3 = Y1 , Z1 , Z2 [loc(id, Y1 , Z1 )  loc(id, Y1 , Z2 )  Z1 6= Z2  Y1 6= {pT }  Y1 6= {pF }],
saying that id can not be on the same location at distinct time values, if the location is different
from both pT and pF .
 f4 = Z1 , Z2 [loc(id, {p0 , . . . , pk1 }, Z1 )  loc(id, {pT }, Z2 )], saying that id can not be at
pT if is was/is/will be at a location in {p0 , . . . , pk1 }.
 f5 = Y2 , Z1 , Z2 [loc(id, {pT }, Z1 )  loc(id, Y2 , Z2 )  Y2 6= {pT }  Z1 < Z2 ], saying that
id can not go away from pT after reaching this location.
 f6 = Y2 , Z1 , Z2 [loc(id, {pF }, Z1 )  loc(id, Y2 , Z2 )  Y2 6= {pF }  Z1 < Z2 ], saying that
id can not go away from pF after reaching this location.
Finally, let Q = (?id, {pT }, ?t, [1, 1]), and let the pair to be checked as an the optimistic answer of
Q be hid, 2k + 1i.
First of all, observe that M(K) 6= . In fact, consider the world w  W(K) such that w(id, 0) =
v0 , w(id, 1) = p0 , w(id, 2) = v1 , w(id, 3) = p1 , . . . , w(id, t) = vt/2 , w(id, t + 1) = pt/2+1 , (in
both cases t is even) . . . , w(id, 2k2) = vk1 , w(id, 2k1) = pk , w(id, 2k) = vk , w(id, 2k+1) =
pF . It is easy to see that w |= F. In fact, for each t  [0..2k], id moves towards reachable locations
in one time point: if id is at vi , with i  [0..k  1], it moves to pi (that is, f1i is satisfied by w); if
id is at pi , with i  [0..k  1], it moves to vi+1 (that is, f2i is satisfied by w); and when is at vk it
moves to pF (that is, f1k is satisfied by w). Moreover, id is not placed by w on the same location at
different times (that is, f3 is satisfied), neither at both a location in {p1 , . . . , pk1 } and at pT (that
is, f4 is satisfied too). Moreover, id does not move from pF after reaching this location (that is, f6
is satisfied), and does not reach location pT (thus, also f5 is satisfied by w). Now, consider the PDF
I over W(K) assigning probability equal to 1 to w, and probability equal to 0 to every other world
in W(K). Clearly, I is a model for K: it assigns non-zero probability only to worlds satisfying F,
and it ensures that atom loc(id, v0 , 0)[1, 1] is satisfied by w which is assigned probability equal to
1.
We now show that hid, 2k + 1i is an optimistic answer to Q w.r.t. K iff there is a Hamiltonian
path in G.
()
P As hid, 2k + 1i is an optimistic answer to Q w.r.t. K, there is a model M  M(K) s.t.
(w) = 1. Let W  W(K) be the set of worldsP
w such that w(id, 2k + 1) =
w|w|=loc(id,pT ,2k+1) M P
pT and M (w) > 0. As wW M (w) = 1 and M is a model (thus, w|w|=loc(id,v0 ,0) M (w) = 1
due to loc(id, v0 , 0)[1, 1])  A), it follows that every world w  W is such that w(id, 0) = v0 .
Moreover, since M is a model, and thus f4 is satisfied by every world w  W , it holds that
t  [0..2k + 1], w(id, t) 6 {p0 , . . . , pk  1}, meaning that id is never placed at a location in
{p0 , . . . , pk1 } by w. Moreover, since for each w  W , w(id, 0) = v0 , and since f1i and f2i
(with i  [1..k]), and f3 are satisfied by every world in W , it follows that (i) for each t  [0, k],
w(id, t)  V (i.e., id is placed on a vertex of G at each time point in [0, k]), and (ii) for each
t, t0  [0, k], w(id, t) 6= w(id, t0 ) (id is not placed on the same vertex of G at different times in
[0, k]). Given this, at time t = k + 1, f1k and f3 entail that id is placed by any world in W at location
pT or pF . As f5 (resp., f6 ) entails that id can not go away from pT (resp., pF ) after reaching this
location, and since w(id, 2k + 1) = pT , it follows that every world w  W is such that for each
t  [k + 1, 2k + 1], w(id, t) = pT . Hence, each w  W is such that M (w) > 0, w(id, 0) = v0 ,
t  [1, k], w(id, t)  V , t, t0  [0, k], w(id, t) 6= w(id, t0 ), t  [k + 1, 2k + 1], w(id, t) = pT .
Therefore, w(id, 0), w(id, 1), . . . , w(id, k) is a Hamiltonian path of G.
775

fiPARISI & G RANT

() Let  be a Hamiltonian path of G. We denote as [i] (with i  [0..k]) the i-th vertex of
. W.l.o.g. we assume that the first
Pvertex of  is v0 , that is, [0] = v0 . We show that there
is a model M  M(K) such that w|w|=loc(id,pT ,2k+1) M (w) = 1, that is, hid, 2k + 1i is an
optimistic answer to Q w.r.t. K. Let M be a function over W such that for all worlds w  W,
M (w) = 0 except for the world w which is such that: w (id, 0) = [0] = v0 , t  [1, k],
w (id, t) = [t], t  [k + 1, 2k + 1], w (id, t) = pT . It is easy to see that w |= F. In fact, for
each i  [1..k], f1i as well as f2i are satisfied by w , since the fact that  is a path on G entails that
t  [0, k  1], w (id, t) = vi and w (id, t + 1) = vj only if edge (vi , vj ) is an edge of G and
w (id, k + 1) = pT which is reachable from any location in V . Moreover, f3 is satisfied by w ,
since the fact that  is a Hamiltonian path entails that w places id on different locations at different
times, except for location pT . Formula f4 is satisfied by w , since w does not place id at any
location in {p0 , . . . , pk1 }. Formula f5 is satisfied by w , since w does not place id on a location
different from pT after time point k + 1, when id reaches pT . Finally, f6 is satisfied by w as well,
since w does not place id on location pF . Since wP|= F, it can be assigned by M a probability
different from 0. Let M (w ) = 1. Moreover, as w|w|=loc(id,v0 ,0) M (w) = M (w ) = 1, the
condition required by loc(id, v0 , 0)[1, 1]  A holds too, proving that M is model for K.
We now prove item 2). We show a reduction to our problem from the complement of the
Hamiltonian path problem. Given a directed graph G, we construct an instance of our problem
as in the proof of 1), except that Q = (?id, {pT }, ?t, [0, 1  ]), where  > 0 is as stated in
Theorem 10. As shown in the proof of 1), K is consistent. Moreover, there is no Hamiltonian
path in G iff the pair hid, 2k + 1i is not an the optimistic answer to Q0 = (?id, {pT }, ?t, [1, 1]).
Now observe that hid, 2k + 1i is not an the optimistic answer to Q0 iff hid, 2k + 1i is a cautious
answer to Q = (?id, {pT }, ?t, [0, 1  ]) for  > 0 sufficiently small. By reasoning as in the
proof of Theorem 10, we can choose
P = , which ensures that if there is an M  M(K) such that
P
M
(w)
<
1,
then
w|w|=loc(id,pT ,t) M (w)  1  .
w|w|=loc(id,pT ,t)

Our query answering problem is related to the decision version of the entailment problem in
probabilistic logic (Nilsson, 1986; Georgakopoulos et al., 1988), which is as follows. Given a conjunction  of clauses, each of them associated with a probability, and an additional clause C with
given lower and upper bounds on the admissible values for its probability, decide whether there is
a probability distribution satisfying the probability of all the clauses in  and the lower and upper
bounds for C. As in probabilistic logic, where the entailment problem can be reduced to PSAT (Nilsson, 1986; Georgakopoulos et al., 1988), we have shown that the query answering problem in PST
KBs can be reduced to the consistency checking problem. However, there is an important difference
between the query answering problem and the probabilistic entailment problem: we assume that the
input PST KBs is consistent, while the entailment problem is defined for any  (even if  is not
satisfiable). As a consequence, the coNP-hardness of the probabilistic entailment problem straightforwardly follows from the NP-hardness of PSAT ( is unsatisfiable iff any contradiction C with
any lower and upper bounds is entailed by ). That is, satisfiability is a source of complexity for the
probabilistic entailment problem. However, in our setting, we are given a consistent PST KB and
thus checking the consistency cannot be a source of complexity for the query answering problem. In
fact, Theorem 11 can be viewed as strengthening the coNP-hardness of the probabilistic entailment
problem, as it is proved in a more specific setting corresponding to the case where the input formula
 is known to be satisfiable.
776

fiK NOWLEDGE R EPRESENTATION IN P ROBABILISTIC S PATIO -T EMPORAL K NOWLEDGE BASES

4.3 Sets of Linear Inequalities for Answering Queries
The following corollary of Theorems 2 and 10 states that the set of mixed-binary linear inequalities
MBL() introduced in Definition 8 can be also exploited for answering queries.
Corollary 2. Let K = hA, Fi be a consistent PST KB, and Q = (?id, q, ?t, [`, u]) a query. Then,
1) if there is a feasible solution of MBL(hA{loc(id, q, t)[`, u]}, Fi), then hid, ti is an optimistic
answer to Q w.r.t. K.
2) if MBL(hA  {loc(id, q, t)[0, `  ]}, Fi) or MBL(hA  {loc(id, q, t)[u + , 1]}, Fi) has a
feasible solution, then hid, ti is not a cautious answer to Q w.r.t. K (where  is as given in
Theorem 10).
Similarly, from Theorems 3 and 10 the following corollary follows, stating that the linear system
NC() introduced in Definition 9 can be also used for answering queries.
Corollary 3. Let K = hA, Fi be a consistent PST KB, and Q = (?id, q, ?t, [`, u]) a query. Then,
1) if there is no feasible solution of NC(hA  {loc(id, q, t)[`, u]}, Fi), then hid, ti is not an
optimistic answer to Q w.r.t. K.
2) if both NC(hA  {loc(id, q, t)[0, ` ]}, Fi) and NC(hA  {loc(id, q, t)[u + , 1]}, Fi) have
no feasible solution, then hid, ti is a cautious answer to Q w.r.t. K (where  is as given in
Theorem 10).
Moreover, we obtain the following corollary of Theorems 4 and 10.

Corollary 4. Let K = hA, Fi be a consistent PST KB where F = { loc(id1 , {p1 }, t1 ) 
loc(id2 , {p2 }, t2 )      loc(idn , {pn }, tn )]}, and Q = (?id, q, ?t, [`, u]) be a query. Then,
1) hid, ti is an optimistic answer to Q w.r.t. K iff there is a feasible solution of NC(hA 
{loc(id, q, t)[`, u]}, Fi).
2) hid, ti is a cautious answer to Q w.r.t. K iff both NC(hA  {loc(id, q, t)[0, ` ]}, Fi) and
NC(hA  {loc(id, q, t)[u + , 1]}, Fi) have no feasible solution, where  is as given in Theorem 10.
From Theorems 6 and 10 we obtain the following result.
Corollary 5. Let K = hA, Fi be a consistent PST KB where F consists of binary std-formulas
only, and Q = (?id, q, ?t, [`, u]) be a query. If AG is acyclic, then
1) hid, ti is an optimistic answer to Q w.r.t. K iff there is a feasible solution of BC(hA 
{loc(id, q, t)[`, u]}, Fi).
2) hid, ti is a cautious answer to Q w.r.t. K iff both BC(hA  {loc(id, q, t)[0, ` ]}, Fi) and
BC(hA  {loc(id, q, t)[u + , 1]}, Fi) have no feasible solution, where  is as given in Theorem 10.
Moreover, the results of Theorem 7, where the shape of the auxiliary graph is not considered,
and Theorem 10 entail the following.
777

fiPARISI & G RANT

Corollary 6. Let K = hA, Fi be a consistent PST KB where F consists of binary std-formulas
only, and Q = (?id, q, ?t, [`, u]) be a query. Then
1) if there is no feasible solution of BC(hA  {loc(id, q, t)[`, u]}, Fi), then hid, ti is not an
optimistic answer to Q w.r.t. K
2) if both BC(hA  {loc(id, q, t)[0, ` ]}, Fi) and BC(hA  {loc(id, q, t)[u + , 1]}, Fi) have
no feasible solution, then hid, ti is a cautious answer to Q w.r.t. K (where  is as given in
Theorem 10).
Still, from Theorems 8 and 10 we obtain the following corollary.
Corollary 7. Let K = hA, Fi be a consistent PST KB where F consists of binary std-formulas
only, and Q = (?id, q, ?t, [`, u]) be a query. If AG is simple-clique cyclic, then
1) hid, ti is an optimistic answer to Q w.r.t. K iff there is a feasible solution of CL(hA 
{loc(id, q, t)[`, u]}, Fi).
2) hid, ti is a cautious answer to Q w.r.t. K iff both CL(hA  {loc(id, q, t)[0, ` ]}, Fi) and
CL(hA  {loc(id, q, t)[u + , 1]}, Fi) have no feasible solution, where  is as given in Theorem 10.
Finally, we state the following corollary of Theorems 9 and 10 concerning cases where computing selection queries is tractable. Although the result is stated assuming that only binary stdformulas are in the PST KB, it holds for the more general case where both unary and binary stdformulas are in the PST KB since, using the approach suggested in the proof of Theorem 5, we can
assume that unary std-formulas are encoded as PST atoms.
Corollary 8. Let K = hA, Fi be a PST KB where F consists of binary std-formulas only. If
G is clique-acyclic or simple clique-cyclic, then deciding whether hid, ti is an optimistic/cautious
answer to selection query Q = (?id, q, ?t, [`, u]) is in PTIME.

5. Domain Enlargement
Up to this point we have assumed that the three domains ID = {id1 , . . . , idm }, T = [0, 1, . . . ,
tmax], and Space = {p1 , . . . , pn } are fixed. In this context we have investigated the consistency
of a PST KB. In this section we investigate what happens to a PST KB if one or more domains
are modified. The modification consists of possibly adding new time values, or spatial points, or
objects. In fact, we are interested in what happens as we add arbitrarily many (but a finite number
of) entities. In Section 5.1 we consider the case where we deal with a longer time period and add
additional time values beyond tmax. In Section 5.2 we again deal with time but in this case we
allow for a finer division of time values. Section 5.3 deals with the case where Space is enlarged,
that is, new points are added. Finally, in Section 5.4 we deal both with additional objects as well as
combinations of domain enlargement.
5.1 Extending Time Beyond tmax
We start with a PST KB K using ID, T , and Space. Suppose now that we extend T beyond tmax;
we write T 0 = [0, 1, . . . , tmax, . . . , t0 ]. This means that the syntax must also be extended with the
778

fiK NOWLEDGE R EPRESENTATION IN P ROBABILISTIC S PATIO -T EMPORAL K NOWLEDGE BASES

constants tmax + 1, . . . , t0 ; however, K does not use any of the new constants. But the semantics is
different now because each world w0 must be a function w0 : ID  T 0  Space. We write W for
the set of worlds using T and W 0 for the set of worlds using T 0 . The connection between W and
W 0 is that every w  W can be extended to many worlds in W 0 by choosing a point in Space for
each new time value for each object. But from any one world w0  W 0 there is a unique restriction
to a world in W . Satisfaction for the worlds in W 0 is defined in the same way as satisfaction for the
worlds in W (with W 0 substituted for W ).
Our interest in the semantics is due to the concept of interpretation where an interpretation
assigns each world a probability. Let W = {w1 , . . . , wz } where z = nm(tmax+1) and let W 0 =
0
{w10 , . . . , wy0 } where y = nm(t +1) . For every wi  W , we define the set of worlds Wi0 =
{wj0 |wj0  W 0 and wj0 is an extension of wi }. This means that the restriction of every wj0  Wi0 is
wi . Next, letPI1 be an interpretation on W . We call an interpretation I10 an extension of I1 if for all i,
1  i  z, w0 W 0 I10 (w0 ) = I1 (wi ). That is, the probability value of I1 (wi ) is distributed among
i
the extensions of wi for each i. We can also go in the other direction. If I20 is an interpretation
on
P
0
0
W , we define its restriction to W , I2 , by using the same formula, that is, I2 (wi ) = w0 W 0 I2 (w0 )
i
for all i, 1  i  z.
This framework allows us to investigate what happens to the consistency and inconsistency of
K as we go from T to T 0 . We start with inconsistency. We state the next theorem in two different
forms because both are useful in various contexts.
Theorem 12. Let K contain only time values in T .
a) If K is inconsistent with time T , then K remains inconsistent if T is expanded to T 0 .
b) If T 0 is an expansion of T and K is consistent with T 0 then it is consistent with T .
Proof. As the second part is the contrapositive of the first, it suffices to prove just one part. We
prove the second one. So assume that K, which uses only time values in T , is consistent when T
is expanded to T 0 . This means that there is an interpretation M 0 for K (using W 0 ) that is a model
of K. WeP
obtain an interpretation M (for W ) from M 0 using the formula given above, that is,
M (wi ) = w0 W 0 M 0 (w0 ) for all i, 1  i  z. We need to show that M is a model of K.
i
Consider first the atomic formulas in
wi , wi |= loc(id, r, t) iff w0 |=
PA. Clearly, for any worldP
0
0
0
0
loc(id, r, t) for all w  Wi . Hence, w|w|=loc(id,r,t) M (w) =
w0 |w0 |=loc(id,r,t) M (w ). This
takes care of A.
P
P
Next, let f  F. Then w0 |w0 6|=f M 0 (w0 ) = 0. We need to show that also w|w6|=f M (w) = 0.
Let wi  W be a world such that wi 6|= f . This means that there is a substitution  using only time
values in T such that wi 6|= (f ). But  remains a substitutionPwhen T 0 is used for time. So for all
wj0  Wi0 , wj0 6|= (f ). As M 0 is a model of K and hence F, w0 |w0 6|=f M 0 (w0 ) = 0 and the result
follows from the definition of M .
Next we consider the case where a PST KB defined using T is consistent. It turns out that
consistency need not be preserved when T is expanded to T 0 . Consider the case of the following
single integrity constraint:
X1 X2 Y1 Z1 [loc(X1 , Y1 , Z1 )  loc(X2 , Y1 , Z1 )  X1 6= X2  tmax < Z1 ] and suppose that
|ID| > |Space|. The std-formula states that two different objects cannot be in the same location
after time tmax. The condition tmax < Z1 is false for all substitutions for Z1 ; hence the integrity
constraint is automatically true. But once T is enlarged to any T 0 (> T ), say t0 = tmax + 1, there is
779

fiPARISI & G RANT

an inconsistency because there are not enough points in Space for all the objects to occupy distinct
points. We now show that for a given consistent K we can always extend T = [0, . . . , tmax] to
T big = [0, . . . , tmax, . . . , tbig] such that if K is consistent with T big then K remains consistent for
any T 0 with T 0 = [0, . . . , tmax, . . . , t0 ]. Essentially, we must make sure that for every time variable
there is a substitution that makes each conjunct true in each f . For example, if the previous example
is modified to
X1 X2 Y1 Z1 Z2 [loc(X1 , Y1 , Z1 )  loc(X2 , Y1 , Z2 )  X1 6= X2  tmax < Z1  Z2 = Z1 + 4] then
we must have enough time values to include tmax + 5. We show how to do this systematically in
the proof of the next theorem.
Theorem 13. For a consistent PST KB K defined with time values T = [0, . . . , tmax], there is always a finite time value tbig  tmax that can be computed in linear time, such that if K is consistent
using T big = [0, . . . , tmax, . . . , tbig] then K is consistent for all T 0 = [0, . . . , tmax, . . . , t0 ].
Proof. We obtain a time value tf for each integrity constraint f  F. For each f we initialize
tf = tmax + 1. If f contains at most one time variable Z then we are done for f . So suppose that f
has at least one conjunct with two distinct time variables. That must have one of the following four
forms (where any +0 is omitted): 1) Zi + m = Zj + n, 2) Zi + m 6= Zj + n, 3) Zi + m < Zj + n,
or 4) Zi + m  Zj + n. The process involves adding a certain number to tf for each such conjunct.
For type 1) add |m  n|, for type 2) add 1, for type 3) add max(0, m  n + 1), and for type 4) add
max(0, n  m). Then, by adding all the numbers for such conjuncts we obtain a tf value for each
f  F. Let tbig = maxf F {tf }. This process is linear in the size of F.
We must show that if K is consistent for T big then it is consistent for any T 0 = [0, . . . , tmax,
. . . , t0 ]. If t0  tbig then consistency follows from Theorem 12b). So assume that tbig  t0 and
K is consistent for T big . This means that there is an interpretation M big that is a model for K.
Just as in the proof of Theorem 12, the issue concerns only the constraints in F. We write W big
for
all worlds using T big and write wb for an arbitrary world in W big . For any f  F,
P the set of big
M (wb ) = 0. We obtain M 0 from M big in an arbitrary way as long as the formula
b
b
Pw |w 6|=f 0 0
(w ) = M big (wib ) is satisfied. To show that M 0 is a model for K with T 0 , we need to
w0 Wi0 M
P
show that w0 |w0 6|=f M (w0 ) = 0. So let wi0  W 0 be such that wi0 6|= f . Then there is a substitution
0 such that wi0 6|= 0 (f ). This 0 may include any time values in T 0 . But our construction assures
that there is already a substitution b using only values in T big so that the world wib which is the
restriction of wi0 to T big is such that wib 6|= b (f ). Hence M 0 is a model for K with T 0 .
These results allow us to consider the case of arbitrarily large (finite) time.
Definition 19. We call K eventually consistent (resp. inconsistent) for time if there is an integer
L such that K is consistent (resp. inconsistent) for all T  = [0, . . . , L, . . . , ].
Corollary 9. Every K is either eventually consistent for time or eventually inconsistent for time.
Proof. If K is inconsistent with time T then by Theorem 12a) it is eventually inconsistent for time.
If K is consistent with time T then there are two cases. If K is still consistent with time T big
as computed in the proof of Theorem 13 then it is eventually consistent for time. Otherwise we
conclude from Theorem 12a) that it is eventually inconsistent for time.
Consider now how our results can be used for query answering when we expand time from
T = [0, . . . , tmax] to T 0 = [0, . . . , tmax, . . . , t0 ]. In order to avoid confusion we write K where
780

fiK NOWLEDGE R EPRESENTATION IN P ROBABILISTIC S PATIO -T EMPORAL K NOWLEDGE BASES

the time is T and K0 when the time is T 0 . First, if K is inconsistent, according to Theorem 12a),
K0 is also inconsistent. So consider the case where K is consistent (recall that queries are evaluated
on consistent PST KBs). Note that when we check if the pair hid, ti is an answer to Q with the
expansion of T to T 0 , this considers also time values t where tmax < t  t0 . Using Theorem 10
we obtain:
 hid, ti is an optimistic answer to Q w.r.t. K0 iff hA  {loc(id, q, t)[`, u]}, Fi is consistent
using T 0 . We do not need to check separately that K0 is consistent because K0  hA 
{loc(id, q, t)[`, u]}, Fi.
 But to check for a cautious answer, we first check that K0 is consistent, and if so, then
hid, ti is a cautious answer to Q w.r.t. K0 iff hA  {loc(id, q, t)[0, `  ]}, Fi and hA 
{loc(id, q, t)[u + , 1]}, Fi are not consistent using T 0 , where  is as given in Theorem 10.
In a similar vein, Theorem 11 carries over also to the case of extended time.
5.2 Extending Time by More Frequent Time Values
In the second type of time extension we consider more frequent time values, so that there are more
than tmax + 1 time values between 0 and tmax. For illustration, we will use T 0 = [0, 0.5, 1, 1.5,
. . . , tmax]. Again, the syntax must be changed as well to include the new constants 0.5, 1.5, and
so on; however, the original K is defined using T = [0, 1, . . . , tmax] and does not contain any new
constant. The situation for worlds is similar to what happened in the previous subsection: every
w : ID  T  Space is a unique restriction of many worlds, ws, w0 : ID  T 0  Space. Also,
satisfaction for the worlds in W 0 using T 0 is defined the same way as for the worlds in W using T .
Our first result is the same for this type of time extension as for the previous one.
Theorem 14. Let K contain only time values in T .
a) If K is inconsistent with time T , then K remains inconsistent if T is expanded to T 0 .
b) If T 0 is an expansion of T and K is consistent with T 0 then it is consistent with T .
Proof. Analogous to the proof of Theorem 12.
Next we consider the case where K is consistent (using T ). Again, consistency need not be
preserved in going from T to T 0 . Consider the case of the following std-formula:
Y1 Z1 [loc(id1 , Y1 , Z1 )  Z1 > 0  Z1 < 1).
This formula states that id1 cannot be at any location between 0 and 1. The condition 0 < Z1 Z1 <
1 is false for all substitutions in T , hence the integrity constraint is true. But if we expand to T 0 then
the substitution of 0.5 for Z1 makes the formula state that id1 cannot be at any location at time 0.5,
making it inconsistent. However, we can extend T to more frequent time values T f r such that if K
is consistent for T f r then it will be consistent for any subdivision of the original time values.
Theorem 15. For a consistent PST KB K defined with time values T = [0, . . . , tmax], there is
always an integer value  that can be computed in linear time, such that if K is consistent using
T f r = [0, 1 , 2 , . . . , 1, +1
 , . . . , tmax] then K remains consistent for every subdivision of T .
781

fiPARISI & G RANT

Proof. The proof is analogous to the proof of Theorem 13. We calculate an integer f for each
integrity constraint f  F and let  = maxf F {f }. The idea is to make sure that there are
enough time values to allow any (Z) that can become true by some subdivision of time intervals
to become true by substitution. In the worst case every time variable may require a new subdivision
of the time values. Hence we can choose f = 2k where k is the number of time variables in f .
The rest of the proof is analogous to the proof of Theorem 13.
As in the previous section where we wanted to consider arbitrarily large time values, now we
consider dividing time intervals arbitrarily many times.
Definition 20. We call K divisionally consistent (resp. inconsistent) for time if there is an integer
L such that K is consistent (resp. inconsistent) for all T = [0, 1 , 2 , . . . , tmax] where  > L.
Corollary 10. Every K is either divisionally consistent for time or divisionally inconsistent for time.
Proof. Analogous to the proof of Corollary 9.
Results analogous to the time expansion done in Section 5.1 hold here also for query answering
and complexity.
5.3 Space Enlargement
In this subsection we consider what happens to the consistency or inconsistency of K if Space
is enlarged, say from Space = {p1 , . . . , pn } to Space0 = {p1 , . . . , pn , . . . , pv }. The change in
semantics is different in this case than when we expanded T . If w : ID  T  Space is a world
(for Space), it remains a world for Space0 . So writing W for the set of worlds using Space and
W 0 for the set of worlds using Space0 , we obtain W  W 0 . There is no change in the definition of
satisfaction, but the number of interpretations becomes greatly enlarged. Still, every interpretation
I using Space can be extended to a unique I 0 using Space0 by assigning I 0 (w0 ) = 0 for all w0 
W0 \ W.
As in the case of time, we start with the case where K is inconsistent. But unlike for time, when
Space is enlarged K may become consistent. Consider the simple example where K consists of a
single atom: a = loc(id1 , {p1 , . . . , pn }, 1)[.2, .7]. K is inconsistent. But if we add just a single point
to Space so Space0 = {p1 , . . . , pn , pn+1 } then K becomes consistent: for instance if w10 is the world
where w10 (id1 , 1) = p1 and the other values are arbitrary, and w20 is the world where w20 (id1 , 1) =
pn+1 and the other values are arbitrary, then assigning I 0 (w10 ) = 0.5 and I 0 (w20 ) = 0.5 with I 0 (w0 ) =
0 for all other worlds makes I 0 a model. A similar situation may occur with integrity constraints.
Consider K that contains a single std-formula: f : X1 Y1 Z1 [loc(X1 , Y1 , Z1 )Y1 ov {p1 , . . . , pn }]
Since every region overlaps with Space, f , by itself, is inconsistent. But again, if we enlarge Space
by one point pn+1 to Space0 we can find a model I 0 as follows. Let I 0 (w0 ) = 0 for every world
0
w0 where w0 (id, t) 
PSpace for0 some hid, ti-pair, and I (w0 ) = 1 where w0 (id, t) = pn+1 for all
hid, ti pairs. Then w0 |w0 6|=f I (w) = 0; hence the inconsistency is removed. However, suppose
that in addition to f , K also contains the atom a = loc(id1 , {p1 }, 0)[1, 1] in A, that is, now A = {a}
and F = {f }. The PST atom a states that id1 must be at p1 at time 0. In that case we cannot make
K consistent by adding any number of points to Space because a and f are in conflict. Hence there
is no general statement about what happens to an inconsistent PST KB when Space is extended to
Space0 .
782

fiK NOWLEDGE R EPRESENTATION IN P ROBABILISTIC S PATIO -T EMPORAL K NOWLEDGE BASES

Next we consider the case where K is consistent (using Space). In this case we can show that it
remains consistent when Space is expanded to Space0 .
Theorem 16. If K is consistent with Space, then K remains consistent when Space is enlarged to
Space0 .
Proof. Let M be a model for K (using Space). M assigns a probability M (w) to each world w.
When Space is extended to Space0 , many new worlds are added. We define M 0 (w) = M (w) if w
is a world where only Space is used and M 0 (w) = 0 otherwise. Thus by basically excluding the
worlds using points other than Space in the sense that they are given probability 0, M 0 is a model
of K using Space0 . Hence consistency is preserved.
Again, just as in the case of time, we are interested in what happens for arbitrarily large finite
spaces.
Definition 21. We call K eventually consistent (resp. inconsistent) for space if there is an integer
L such that K is consistent (resp. inconsistent) for all Space = {p1 , . . . , pn , . . . , pL , . . . , p }.
Corollary 11. Every K is either eventually consistent for space or eventually inconsistent for space.
Proof. There are two cases. We start with the first case where K is consistent with Space. Then,
by Theorem 16 it remains consistent for any larger Space0 . Hence it is eventually consistent for
space. Consider now the second case where K is inconsistent with Space. There are two subcases.
First, suppose that K becomes consistent with some larger Space0 . Then, just as in the first case, by
Theorem 16 K is eventually consistent for space. The second subcase is where K never becomes
consistent no matter how Space is extended to Space0 . This means that K is eventually inconsistent
for space.
We showed earlier by examples that an inconsistent K (with Space) may become consistent if
Space is enlarged. We can calculate a bound for the size of the needed space such that after that
bound is reached the consistency or inconsistency does not change with further spatial enlargement.
Theorem 17. For every inconsistent K using Space = {p1 , . . . , pn }, there is an explicit bound L
that is tractable to compute such that if K using Space0 = {p1 , . . . , pL } is inconsistent, it remains
inconsistent for any enlargement of Space0 .
Proof. We already know from the above corollary that such an L exists. We now show how to
compute it. First of all, if no enlargement of Space resolves the inconsistency of K then K is
eventually inconsistent for space and we can choose L = n. Thus we need to deal in more detail
only with the case where an enlargement of Space resolves the inconsistency of K, that is, K is
eventually consistent for space. So we consider how adding points to Space can make K consistent.
There are three cases:
1. the inconsistency is due to A alone,
2. the inconsistency is due to a combination of elements from both A and F, and
3. the inconsistency is due to F only.
783

fiPARISI & G RANT

Consider first that in Case 1 an inconsistency must be due to atoms with some fixed id and t
values. So we assume that we are dealing only with atoms using a specific pair of values: id and
t. Adding a point or points to Space can resolve an inconsistency only if the atoms using id and t
give Space a probability less than 1. For instance, let A = {loc(id, r1 , t, [0, 0.4]), loc(id, r2 , t, [0.1,
0.3])} where r1  r2 = Space. Then, enlarging Space to Space0 = Space  {pn+1 } resolves
the inconsistency. This is true in general, not just for this example, because once pn+1 is added
to Space, {p1 , . . . , pn } becomes a proper subset of the relevant Space0 and may consistently have
probability less than 1. Now, consider that A may have inconsistencies involving multiple pairs of
id and t values. But that does not matter because the addition of the single point pn+1 to Space
resolves all such inconsistencies. So in Case 1 we can choose L = n + 1.
In Case 2 an inconsistency of K is due to a combination of elements from A and F. As every
atom a in A refers to a specific region r for a specific object id and time value t, for an inconsistency
to occur with some f  F, some substitution of f must act as an atom (with probability interval
[0, 0]). For instance, let a = loc(id, r1 , t, [0, 0.4]) and f = X1 [loc(X1 , r2 , t)] where r1  r2 =
Space. The instance of f that causes the inconsistency is [loc(id, r2 , t)] which has the same effect
by the semantics as the atom loc(id, r2 , t, [0, 0]). This is true in general, not just for this example.
In this way Case 2 reduces to Case 1 and we can choose L = n + 1.
The last case, Case 3, is where an inconsistency is due just to F. The problem in this case is
due to the fact that F requires more points than are in Space. So we need to consider what can be
expressed about the number of spatial points by std-formulas. This cannot be done by writing many
spatial variables because if formula f has m spatial variables, say Y1 , . . . , Ym , there is no way to
express that they must all refer to different points. However, we can express that Space does not
have enough points by writing f0 = X1 Y1 Z1 [loc(X1 , Y1 , Z1 )  Y1 ov Space]. Again, adding a
single point to Space resolves the inconsistency. We can also write std-formulas that require at least
a certain number of points. We can express the constraint that a point can be occupied by at most
one object at one time by using the following 3 std-formulas:
f1 = X1 Y1 Z1 Z2 [loc(X1 , Y1 , Z1 )  loc(X1 , Y1 , Z2 )  Z1 6= Z2 ]
f2 = X1 X2 Y1 Z1 [loc(X1 , Y1 , Z1 )  loc(X2 , Y1 , Z1 )  X1 6= X2 ]
f3 = X1 X2 Y1 Z1 Z2 [loc(X1 , Y1 , Z1 )  loc(X2 , Y1 , Z2 )  X1 6= X2  Z1 6= Z2 ]
f1 states that no object can be at the same point at two different times; f2 states that at no time can
two objects be at the same point; f3 states that two different objects cannot be at the same point
at two different times. Hence using std-formulas we can reference the number of objects times the
number of time points. Let L = n + |ID|  (tmax + 1). Thus Space0 = {p1 , . . . , pL } consists
of a point for each object and time point in addition to the original n points in Space, so that we
can place objects in Space0 even when none of the points in Space can be used and each object
needs a new point at each time point. Clearly, L > n + 1, so this L works for Cases 1 and 2 as
well. So this is the value of L that we choose for the tractable explicit bound. Thus if K using
Space0 = {p1 , . . . , pL } is still inconsistent, no points added to Space0 can make it consistent.
5.4 Extending the Number of Objects or Several Entities
The last case where the number of constants may be increased is for ID. Since a world w is a
function w : ID  T  Space, the expansion of objects is similar to the expansion of time, not
784

fiK NOWLEDGE R EPRESENTATION IN P ROBABILISTIC S PATIO -T EMPORAL K NOWLEDGE BASES

to the expansion of space. So the extension and restriction of interpretations going from an ID to a
larger ID0 is analogous to the addition of time values to T . Without getting into the details which are
analogous to the cases for time given earlier, we can state that if K is inconsistent with ID, it remains
inconsistent if ID is expanded to ID0 . To show that consistency need not be preserved, consider f =
X1 X2 . . . Xm+1 [loc(X1 , Y1 , Z1 )loc(X2 , Y2 , Z2 ). . .loc(Xm+1 , Ym+1 , Zm+1 )X1 6= X2 
X1 6= X3  . . .  Xi 6= Xj  . . .  Xm 6= Xm+1 ] for i 6= j. Recalling that ID = {id1 , . . . , idm } we
find that K = {f } is consistent with ID because it states that there cannot be more than m distinct
objects. But it becomes inconsistent if ID is enlarged to ID0 = {id1 , . . . , idm , idm+1 }. But we can
always find a q  m in linear time such that if K is consistent for ID0 = {id1 , . . . , idm , . . . , idq }
then K is consistent for any enlargement of ID0 . Obtaining the value q is easier than obtaining
tbig for time. It is just a matter of counting up the number of X variables in each f  F and
taking the maximum such value, if bigger than m. Hence if we define eventually consistent (resp.
inconsistent) for objects in the same way as for time, we obtain the result that every K is either
eventually consistent for objects or eventually inconsistent for objects.
So far we have only considered individual enlargements of either time values or points in space
or objects. But we may be interested also in combining several types of extensions. As the expansion
of time and objects are similar, we start by combining them.
Definition 22. We call K using ID, T , and Space eventually consistent (resp. inconsistent) for
objects and general time if there are integers L1 , L2 , and L3 such that K is consistent (resp.
inconsistent) for all ID = {id1 , . . . , idm , . . . , idL1 , . . . , id }, Space = {p1 , . . . , pn }, and T =
tmax+1
[0, 1 , 2 , . . . , 1, +1
, . . . , . . . , ] where  > L2 and  > L3 .
 , . . . tmax,

The combination of the expansion of objects and time both in magnitude and divisionally works
essentially in the same way as the expansion of only one of these items. That is, if the set of all
worlds for the original K is W and the set for all worlds with the expansion to ID and T is W 0 ,
then for every world in W there is a set of extensions in W 0 and for every world in W 0 there is a
unique world in W that is its restriction. The key issue is that Space remains unchanged.
Now let K contain time values in T , regions in Space and objects in ID. Suppose that T is
expanded to T 0 where the expansion may be both in magnitude and division and ID is expanded to
ID0 and find that K, in the context of T 0 and ID0 with Space unchanged, is consistent. Just as in the
proof of Theorem 12 we start with a model M 0 for K using T 0 and ID0 and obtain the corresponding
model M for K using the starting T and ID. Thus, by the contrapositive, if K is inconsistent with
T , Space and ID, it remains inconsistent with T 0 , Space and ID0 . This shows that any inconsistent
K is eventually inconsistent for objects and general time. In case K is consistent with T , Space and
ID and becomes inconsistent with the expansion to T 0 , Space and ID0 , then again it is eventually
inconsistent for objects and general time. The only alternative is for K to remain consistent no
matter how T and ID are expanded. Hence our earlier results can be put together as the following
result.
Theorem 18. Every PST KB K is either eventually consistent for objects and general time or it is
eventually inconsistent for objects and general time.
However, the situation is different in the cases where we combine space with time or with objects
as we show using an example.12
12. This example was suggested by one of the reviewers.

785

fiPARISI & G RANT

Example 25. Consider the PST KB K = h, {f }i defined using ID = {id1 , . . . , idm }, T =
[0, 1, . . . , tmax], and Space = {p1 , . . . , pn } with m  n where f is the std-formula f2 used
in the proof of Theorem 17 which states that two different objects cannot be at the same point
at the same time. Because of the condition m  n, K is consistent. Expand ID to ID0 =
{id1 , . . . , idm , . . . , idn+1 } leaving T and Space unchanged and call the new KB K0 . Now there
are too many objects for the number of points; hence K0 is inconsistent. Next, expand Space to
Space00 = {p1 , . . . , pn , pn+1 } while leaving ID0 and T unchanged and call the new KB K00 . K00 is
consistent.
2
This example shows that we cannot claim eventual (in)consistency if both the number of objects
and the number of points may increase as we can just continue the process indefinitely. An analogous result holds in case both the number of time values and the number of points may increase.

6. Related Work
We first discuss related works on classical probabilistic logic that does not have explicit spatial and
temporal components. Then we discuss the relationship between our work and other spatio-temporal
approaches. Finally, we relate our framework to object tracking.
6.1 Probabilistic Logic
As discussed in Section 3, a PST KB can be expressed in classical propositional logic (Hailperin,
1984; Nilsson, 1986; Paris, 1994), and in particular the consistency checking problem can be
formulated in terms of Probabilistic Satisfiability (PSAT), whose first formulation is attributed to
Boole (1854). After its presentation to the AI community by Nilsson (1986), the study of PSAT
from the point of view of efficient algorithms and computational complexity was first addressed
by Georgakopoulos et al. (1988), who showed that PSAT is in NP and NP-hard even for binary
clauses. The tractable results identified by Georgakopoulos et al. concern the special case where
each clause involves at most two literals (2PSAT) and the graph of clauses is outerplanar,13 where
the graph of clauses contains a vertex for each literal and two kinds of edges: i) an edge between
each pair of literals built from the same propositional variable, and (ii) an edge for each pair of
literals appearing in the same clause. We note that the PSAT formula K encoding PST KB K
(see the proof of Theorem 1) contains more than two literals per clause even if we focus on binary
std-formulas (K becomes a 2PSAT formula only if we assume that Space consists of just two
points).
The result provided by Georgakopoulos et al. relies on reducing 2PSAT to a tractable instance
of 2MAXSAT (weighted maximum satisfiability problem with at most two literals per clause). Using this reduction and the result of Conforti and Cornuejols (1992) on the tractability of problems
that can be formulated as an integer program whose matrix is balanced, the following general result
was provided by Conforti and Cornuejols: PSAT is tractable for a balanced set of clauses, that is,
for a set of clauses whose corresponding {0, 1} clause-variable matrix is balanced. However, this
result also doesnt help in finding tractable cases for PST KBs K through reduction to PSAT K , as
considering three points is Space suffices to make the matrix corresponding to K not balanced (it
entails the presence of an odd cycle in the graph of clauses that, as observed by Andersen & Pre13. A graph is said to be outerplanar if it can be embedded in the plane so that all of its vertices lie on the same face.
That is, it can be drawn in the plane without crossings in such a way that no vertex is totally surrounded by edges.

786

fiK NOWLEDGE R EPRESENTATION IN P ROBABILISTIC S PATIO -T EMPORAL K NOWLEDGE BASES

tolani, 2001, characterizes non-balanced matrices). Motivated by the fact that the tractable cases for
PSAT identified by Georgakopoulos et al. and Conforti and Cornuejols rely on using a polynomial
time algorithm whose complexity is characterized by a high polynomial degree (specifically, the
complexity is O(n6 log n) where n is the number of propositional variables), Andersen and Pretolani identified more efficient algorithms for the classes of balanced sets of clauses that can be
represented by either hypertrees (where each hyperarc corresponds to the set of literals in a clause)
or such that the co-occurrence graph is a partial k-tree (Bodlaender, 1998).
It is worth noting that none of the tractable cases identified in this paper can be derived from
the results for PSAT described above by reducing K to the PSAT formula K . In fact, our tractable
cases derive from the specific structure of PST KBs. On the other hand, our tractability results
entail the tractability of PSAT instances that can be reduced to a tractable instance of the consistency
checking problem. In particular, we can decide in polynomial time PSAT instances K having the
structure specified in our reduction from the consistency checking problem to PSAT, and such that
the consistency checking problem for the corresponding PST KB K turns out to be tractable.
In principle, the fact that there is a reduction from the consistency checking problem to PSAT
enables well-known techniques for solving (general) instances of PSAT based on column generation (Kavvadias & Papadimitriou, 1990; Jaumard et al., 1991) to be used for addressing the consistency of PST KBs. The same holds for the problem of answering selection queries in PST
KBs that, as shown in Section 4, can be addressed by solving suitable instances of the consistency
checking problem. Recent approaches to solve PSAT using SAT (Finger & Bona, 2011) or Integer
Linear Programming (Cozman & di Ianni, 2015) for column selection report experiments showing
a phase transition behaviour (first observed by Finger and Bona for PSAT) depending on the fraction between the number of clauses and propositional variables as well as the number of probability
assignments. Using these techniques PSAT instances with hundreds of propositional variables and
clauses can be solved in reasonable time. However, we believe that reducing the consistency checking (or query answering) problem to PSAT and then applying these techniques as they are would
not be a successful approach, as the number of propositional variables and clauses that would be
generated would be huge even for small-size PST KBs. To avoid this problem, we conjecture that
the specific structure of (PSAT formulas encoding) PST KBs could be exploited to devise more
efficient techniques for solving the consistency checking and the query answering problems. In this
regard, it would be interesting to investigate the connection between our framework and the emerging field of lifted probabilistic inference (Kersting, 2012), where the structure of FOL-constructs
(such as indistinguishable individuals) is exploited to speed up the reasoning process, and see if
some results can carry over into PST KBs.
The use of integrity constraints to encode domain knowledge has been studied by Lukasiewicz
(1999, 2001) and Flesca, Furfaro, and Parisi (2014), for probabilistic frameworks which however
do not explicitly deal with space and time. The problem of probabilistic deduction in the presence
of conditional constraints over basic events was addressed by Lukasiewicz (1999), who identified
tractable instances of probabilistic KBs, whose conditional constraints define conditional constraint
trees, to support deduction over paths of premise/conclusions basic events. The problem of checking
the consistency of relational probabilistic databases (where tuples can be viewed as basic events)
in the presence of denial constraints was addressed by Flesca et al., who provided tractability results for constraints whose conflict hypergraph (Chomicki, Marcinkowski, & Staworko, 2004) is acyclic (Fagin, 1983) as well as for a special kind of cyclic hypergraphs, which can encode neither
clique-acyclic std-graphs nor simple clique-cyclic std-graphs. An important probabilistic logic pro787

fiPARISI & G RANT

gramming approach with conditional constraints was proposed by Lukasiewicz (2001), who studied
the complexity of the satisfiability and the entailment problems for several types of formulas but
without identifying tractable cases. Differently from the above-cited papers, the atomic information
in our framework has a structure involving objects, space, and time, and thus atoms may also be
intrinsically related because they are about the same object, or space, or time value.
6.2 Spatio-Temporal Approaches
Substantial work has been done on spatio-temporal logics (Gabelaia et al., 2005; Knapp et al.,
2006) which combine spatial and temporal formalisms. This includes important contributions on
qualitative spatio-temporal representation and reasoning (Muller, 1998; Wolter & Zakharyaschev,
2000; Cohn & Hazarika, 2001), which focus on describing entities and qualitative relationships
between them while dealing with discrete time. Cohn, Li, Liu, and Renz (2014) provided an upto-date overview of the work done in the field of qualitative spatial reasoning, where recently the
important problem of combining topological and directional information for extended spatial objects
has been addressed. However, these works are not intended for reasoning about moving objects
whose location at a given time is uncertain (they do not put probabilities into the mix). Yaman
et al. (2004, 2005a, 2005b) focused on spatio-temporal logical theories that describe known plans
of moving objects by sets of go atoms, each of them stating that an object can go from location
L1 to L2 , leaving L1 and reaching L2 at some time in some intervals, and travelling with a speed
in a given interval. Later, Parker et al. (2007b) extended this logic to include some probabilistic
information about such plans. The SPOT framework of Parker et al. (2007a) further extended that
work to uncertainty about where objects might be at a given time.
Past work on the SPOT framework investigated efficient algorithms for computing optimistic
and cautious answers to selection queries (Parker et al., 2009; Parisi et al., 2010). The initial SPOT
framework on which we build by adding integrity constraints has been implemented and tested on
real US Navy databases containing ship location data (Parker et al., 2009; Parisi et al., 2010). Aggregate queries have been recently investigated by Grant et al. (2013), who proposed three semantics
along with computational methods for evaluating them. As SPOT databases provide information
on moving objects, one important aspect addressed by Parker et al. (2008) and then further investigated by Grant et al. (2010) is that of revising SPOT data so that information on these objects may
be changed as objects move. Grant et al. (2010) proposed several strategies for revising SPOT data
such as finding maximal consistent subsets, minimally modifying the spatial, temporal, object, or
probability components of PST atoms. A full logic including negation, disjunction and quantifiers
for managing SPOT data was recently proposed by Doder et al. (2013), who focused on finding
sound and complete sets of axioms for several fragments of the logic. Grant, Parisi, and Subrahmanian (2013) provided a comprehensive survey on the SPOT framework where other related research
is also reviewed.
While there is much work on spatio-temporal databases (Agarwal et al., 2003; Pelanis et al.,
2006) and probabilistic spatio-temporal databases (Tao et al., 2005; Zhang, Chen, Jensen, Ooi, &
Zhang, 2009; Zheng, Trajcevski, Zhou, & Scheuermann, 2011), these works mainly focus on devising indexing mechanisms and scaling query computation, instead of representing knowledge in
a declarative fashion. In particular, Chung, Lee, and Chen (2009) use indexing to speed the computation of range queries and derive a PDF for the location of an object moving in a one-dimensional
space by using its past moving behavior or the moving velocity distribution. Zhang et al. (2009)
788

fiK NOWLEDGE R EPRESENTATION IN P ROBABILISTIC S PATIO -T EMPORAL K NOWLEDGE BASES

provide a B x -tree index which is a variant of a B + -tree applicable to moving objects whose location
and velocity are uncertain. Two types of pruning are introduced by Yang, Lu, and Jensen (2010)
to efficiently solve queries asking for all sets of k objects that have at least a threshold probability
of containing the k nearest objects to a given object. Dealing with a similar problem, Chen, Qin,
and Liu (2010) propose a TPR-tree for indexing. Finally, Zheng et al. (2011) deal primarily with
objects moving along road networks, and introduce an indexing mechanism for efficiently processing probabilistic range queries. However, none of these works systematically addresses the issue of
considering integrity constraints over probabilistic spatio-temporal data.
6.3 Object Tracking
Object tracking is one of the most important problems in computer vision (Szeliski, 2010) where
the consecutive positions of a tracked object are estimated as it moves in different frames of a
video. Numerous approaches for object tracking have been proposed, mainly differing from each
other on the type of object representation used (e.g., centroid, primitive geometric shapes), image
features selected (e.g., colour, optical flow), and object detection method adopted (e.g., background
subtraction, segmentation). However, the tracking algorithm to be chosen in a given application
strongly depends on the application domain (Yilmaz, Javed, & Shah, 2006). Moreover, as object
tracking algorithms may incur errors, due for instance to loss of information caused by projection
of the 3D world on a 2D image, noise in images, partial and full object occlusions, the estimation of
the position of tracked moving objects is inherently uncertain (even if a camera focuses on a fixed,
specific area). Several important statistical methods for object tracking in computer vision (e.g., see
Broida & Chellappa, 1986; Beymer & Konolige, 1999; Rosales & Sclaroff, 1999) are based on the
well-known Kalman filter (1960) and its extensions to deal with the non-linear case, as well as on
particle filtering (Kitagawa, 1987).
Filtering techniques have been extensively used for object tracking in the presence of sensors,
not only cameras. As a matter of fact, object tracking has been extensively addressed in the more
general setting where the position (i.e., the state) of one or more objects is estimated by a recursive
Bayesian filter given the measurements up to that time coming from different kinds of sensors
(including, for instance, radar, sonar, infrared, and other types of sensors possibly along with visual
sensors) (Stone, Corwin, & Barlow, 1999). Basically, at each observed time point t, the output
of the filter is a probability distribution (i.e., the posterior) on the position of the target object,
which is computed by combining the motion updated at a time prior to t with the likelihood for
the observation received at time t, where the likelihood represents the probability of each sensor
measurement conditioned on the object position (Bar-Shalom, Kirubarajan, & Li, 2002).
The Kalman filter has been used for discrete-time estimation of continuous spatial positions of
objects whose movement equations can be assumed to be linear with Gaussian noise. It has also
been used successfully in non-linear systems by applying linearization and unscented transformation (Julier, Jeffrey, & Uhlmann, 2004). For discrete space and non-linear systems, particle filtering
has been successfully used, providing a solution that can be applied to any state-space model and
which generalizes the traditional Kalman filtering methods (Arulampalam, Maskell, Gordon, &
Clapp, 2002). A general framework for particle filtering based on Sequential Importance Sampling with Resampling was proposed by Liu and Chen (1998), though a number of different types
of particle filters exist and some have been shown to outperform others when used for particular
applications (Arulampalam et al., 2002).
789

fiPARISI & G RANT

Differently from Kalman filtering, where the estimated position of an object at each observed
time point is represented as a continuous distribution, particle filters are based on a histogram representation of the probability density, which is approximated by a finite number of particles (i.e.,
samples): each particle represents a position of space, and weights associated with particles (or the
proportion of the number of particles) define a histogram of the probability distribution over space.
This fits with the representation paradigm of PST KBs: as PST KBs allow us to represent, for each
object and time point, a PDF over Space by defining a PST atom with a single valued probability
interval (that is, ` = u) for each point in Space, it can be used easily to represent the output of object tracking techniques based on particle filtering. For filtering techniques returning a continuous
distribution over Space, a discretization step should be applied.
While the output of object tracking techniques can be represented using PST KBs, there are
important aspects that these techniques can deal with that PST KBs cannot do. In particular, filtering techniques use conditional independence to represent a PDF of an objects positions conditional
on its positions at the previous time. PST KBs can encode the output of this inference process,
but lack the expressive power to do that kind of inference. For instance, tracking techniques can
represent the knowledge that if an object is in a region r1 at time t1 then it is probably in region
r2 at time t2 with a probability depending on the time elapsed between t1 and t2 . Indeed trackers
can rely on a motion model according to which the distribution for an objects location spreads out
with the elapsed time since the last measurement: the distribution for the objects locations will be
narrowly focused on locations near the measured position at t1 if t2 is close to t1 , but it will diffuse
if t2 is faraway t1 . While tracking techniques can do this sort of things quite naturally, PST KBs
can only capture some aspects of this behavior. For instance, we can express the fact that the object
is probably in region r1 at t1 , and use an integrity constraint imposing that it can travel no more
than d distance units in 1 time point. This would increase the probability of finding the object at
time t2 in a region less than d units away from r1 , and it would decrease the probability of finding
the object in a region farther away than d units. However, this is different from what can be inferred
by tracking techniques using conditional independence.
On the other hand, tracking techniques do not combine very well with interval probabilities.
In fact they typically return the PDF of an objects position at each observed time point. In contrast, using general PST atoms (with probability intervals), for each object and time point, all the
(possibly infinite in number) PDFs that are compatible with the probability intervals specified by
the atoms are succinctly represented. For instance, assuming Space = {p1 , p2 } and the PST atom
loc(id, {p1 }, t)[0, 0.5], all the PDFs f over Space assigning a probability f (p1 ) in [0, 0.5] and
f (p2 ) = 1  f (p1 ) are represented. However, the set of PDFs represented can be restricted by using
single valued probability intervals or by adding integrity constraints using std-formulas.
We note that the PST formalism allows us to impose integrity constraints over KBs that can
be obtained by integrating position data coming from different sources. Consider for instance the
integration of several PST KBs, each of them consisting of PST atoms encoding the output of
an autonomous tracking system. The so-obtained integrated KB is still a PST KB, and integrity
constraints can be used to express knowledge on the overall system, that could not be expressed
considering the tracking systems separately. For instance, suppose we have an integrated PST KB
consisting of the position data of monitored cars that are collected using black-box tracking systems
installed over cars by insurance companies. Then std-formulas can be used to express correlations
among monitored cars. For instance, knowing that in region r there is a licensed inspection station
able to inspect at most k cars at the same time, we can impose the constraint that there cannot be
790

fiK NOWLEDGE R EPRESENTATION IN P ROBABILISTIC S PATIO -T EMPORAL K NOWLEDGE BASES

more than k cars in r at any time. This constraint would be meaningless if we considered the KB of
each car tracking system separately, but it is useful for restricting the set of consistent interpretations
of the PST KB obtained by integrating the several sources.

7. Summary
We believe that this is the first comprehensive paper that focuses systematically on knowledge representation in the form of integrity constraints for probabilistic spatio-temporal data.14 The knowledge
is represented both in the form of spatio-temporal atoms describing the location of objects in time
with a probability interval as well as spatio-temporal denial formulas describing the integrity constraints the system must satisfy. Within this framework we investigated the consistency checking
problem as well as the problem of answering selection queries in consistent PST KBs. Although
both these problems turned out to be hard in the general case, we devised several sets of linear
inequalities which allow us to decide consistency as well as answer queries by checking their feasibility. In addition, we identified different classes of spatio-temporal denial formulas for which
both checking consistency and answering queries are tractable. Finally, we discussed an extension
of the framework to arbitrarily large finite numbers of objects, time values, and points in space and
showed that the behavior of consistency and inconsistency is uniform.

8. Future Work and Conclusion
There are further issues that can be investigated. Following Parisi and Grant (2014b), where we
studied the problem of restoring the consistency of PST KBs of the form KhA, i (where the set of
std-formulas is empty), we will consider the problem of repairing an inconsistent PST KB where
inconsistency is due to the presence of std-formulas that are not satisfied. In this regard it would be
interesting to devise methods for answering queries in inconsistent PST KBs. Recently there has
been some research on probabilistic reasoning under inconsistency (Picado-Muino, 2011; Thimm,
2013; Potyka & Thimm, 2014) that can help in this regard. It would also be interesting to look
into the possibility of semantic query optimization for PST KBs, and to study the use of previous
knowledge to efficiently check for consistency and process queries after updates.
Another direction for future work is the investigation of probabilistic std-formulas for expressing
constraints that hold with a probability in a given interval. Intuitively, this would allow us to state
for instance that two objects are not in the same region at the same time with probability greater
then a given threshold, instead of stating that they cannot be in such a situation. This kind of
probabilistic constraint could be expressed using pstd-formulas of the form f [`, 1] where f is an
std-formula (i.e., f [1, 1] captures the meaning of an std-formula). Then the change in semantics
means changing
the definition of model. Only the second part of Definition 6 has to be modified to
P
f  F, w|w|=f M (w)  [`, 1]. Clearly, lower bounds on the complexity of consistency checking
and query answering problems still hold for this extension. It is easy to check that the upper bound
provided by Theorem 1 holds since a reduction to PSAT can still be provided by mapping pstdformulas to clauses associated with a probability interval. As regards tractable cases, we conjecture
that the results of Theorems 6 and 8 still hold for pstd-formulas of the form f [`, 1] if the right-hand
side of inequality (4) of Definitions 15 and 17, respectively, is replaced with the lower probability
bound ` of the ground pstd-formula f [`, 1] generating the inequality. However, allowing general
14. This is a substantially revised and expanded version of the work by Parisi and Grant (2014a).

791

fiPARISI & G RANT

probability intervals associated with std-formulas introduces new issues in the semantics: f [`, u]
would entail that f [1  u, 1  `] holds. Providing a clear and intuitive semantics for this kind of
std-formula, as well as for probabilistic std-formulas allowing probability intervals to be associated
with each conjunct (instead of the whole formula), is deferred to future work.
The PST formalism is propositional even though the atoms have substantial content. We have
added std-formulas for integrity constraints; these are a special class of first-order logic formulas. It would be interesting to consider other works on more general first-order probabilistic logics (Halpern, 1990; Lukasiewicz & Kern-Isberner, 1999; Kern-Isberner & Thimm, 2010). These
logics were developed for a different purpose but an attempt could be made to represent spatiotemporal information in them. There are other works as well that could be enhanced with spatiotemporal information, such as Markov Logic (Richardson & Domingos, 2006), or Bayesian Logic
Programs (Kersting & Raedt, 2007). In particular, Milch et al. (2005) introduces a first-order language called BLOG (Bayesian LOGic) for defining probability models over worlds with unknown
objects and identity uncertainty, which finds a natural application in object tracking for unknown
objects. It may be possible to find a generalization of the PST formalism that includes some of
these concepts. In doing so, an aspect that we need to take care of is the fact that Markov logic
and Bayesian logic programs deal with a unique probability distribution, while we deal with all the
probability distributions compatible with the PST atoms and std-formulas.
Researchers in AI have been studying spatial and temporal reasoning for many years (Allen,
1984; Randell, Cui, & Cohn, 1992; Galton, 2009). An interesting project will be the incorporation
of these concepts into the PST framework. The new syntax and semantics will include adding rules
to the language. These additions will allow both for adding other types of information as well as new
integrity constraints. For instance, using the concepts of qualitative direction and orientation proposed for spatial reasoning (Galton, 2009) would allow us to explicitly represent knowledge about
the region toward which an object is moving. Another important concept needed in many applications is the explicit representation of qualitative and quantitative distance between objects as well as
of information about the speed (i.e., maximum or average) of objects. Additional structured information about objects such as the type (e.g., vessel, vehicle, person, etc.) would be in general useful
to exhaustively reason about moving objects. However, depending on the addition made to increase
the expressive power of the extended framework, important consequences on the complexity of the
consistency checking problem may arise. Spatial and temporal aspects of formalisms for qualitative
spatial and temporal reasoning are more expressive than what we have in our framework (Gabelaia
et al., 2005; Knapp et al., 2006). The trade-off between expressiveness and complexity within the
hierarchy of formalisms obtained by combining well-known spatial and temporal logics is analyzed
by Gabelaia et al. (2005), where it is shown that the complexity of the satisfiability problem for
spatio-temporal logics (not dealing with probabilities) can vary from NP-complete to undecidable.
Using these formalisms in the PST framework may drastically increase the computational complexity of the problems studied in this paper. Nevertheless, we believe that some attempt should
later be made to include even simpler concepts of qualitative spatio-temporal reasoning to the PST
framework, particularly trying to exploit restrictions recently studied by Huang, Li, and Renz (2013)
to identify tractable fragments.
In this paper, we proposed a framework where four features of moving objects are taken into
account: the spatial component, the temporal component, the inherent uncertainty of acquired data,
and integrity constraints from the application domain. The expressiveness of these features could be
improved to represent additional knowledge that may be of interest in practical applications, partic792

fiK NOWLEDGE R EPRESENTATION IN P ROBABILISTIC S PATIO -T EMPORAL K NOWLEDGE BASES

ularly using ideas from papers dealing with spatio-temporal reasoning. In the endeavor of a concrete
implementation of our framework, ideas from the PostGIS spatial database system which extends
PostgreSQL would be useful. We believe it will be worthwhile later to incorporate more concepts
in the PST framework. We took a first step in extending the PST framework by the addition of
integrity constraints, and hope that researchers will use our work as a starting point in investigations
of the important role of integrity constraints in probabilistic spatio-temporal knowledge bases.

Acknowledgments
We wish to thank the referees for numerous helpful comments that helped us in substantially improving this paper.

References
Agarwal, P. K., Arge, L., & Erickson, J. (2003). Indexing moving points. J. Comput. Syst. Sci.,
66(1), 207243.
Ahson, S. A., & Ilyas, M. (2010). Location-Based Services Handbook: Applications, Technologies,
and Security. CRC Press, Hoboken, NJ.
Akdere, M., Cetintemel, U., Riondato, M., Upfal, E., & Zdonik, S. B. (2011). The case for predictive database systems: Opportunities and challenges. In Proceedings of the 5th Biennial
Conference on Innovative Data Systems Research (CIDR), pp. 167174.
Allen, J. F. (1984). Towards a general theory of action and time. Artif. Intell., 23(2), 123154.
Andersen, K. A., & Pretolani, D. (2001). Easy cases of probabilistic satisfiability. Ann. Math. Artif.
Intell., 33(1), 6991.
Arulampalam, M. S., Maskell, S., Gordon, N. J., & Clapp, T. (2002). A tutorial on particle filters for
online nonlinear/non-gaussian bayesian tracking. IEEE Transactions on Signal Processing,
50(2), 174188.
Bar-Shalom, Y., Kirubarajan, T., & Li, X.-R. (2002). Estimation with Applications to Tracking and
Navigation. John Wiley & Sons, Inc., New York, NY, USA.
Bayir, M. A., Demirbas, M., & Eagle, N. (2010). Mobility profiler: A framework for discovering
mobility profiles of cell phone users. Pervasive and Mobile Computing, 6(4), 435  454.
Beymer, D., & Konolige, K. (1999). Real-time tracking of multiple people using continuous detection. In Proceedings of the Workshop on Frame-rate Applications, Methods and Experiences
with Regularly Available Technology and Equipment (FRAME-RATE), in conjunction with
the 7th IEEE International Conference on Computer Vision (ICCV).
Bodlaender, H. L. (1998). A partial k-arboretum of graphs with bounded treewidth. Theor. Comput.
Sci., 209(1-2), 145.
Boole, G. (1854). An Investigation of the Laws of Thought on Which are Founded the Mathematical
Theories of Logic and Probabilities. Macmillan, London.
Broida, T. J., & Chellappa, R. (1986). Estimation of object motion parameters from noisy images.
IEEE Trans. Pattern Anal. Mach. Intell., 8(1), 9099.
793

fiPARISI & G RANT

Chen, Y.-F., Qin, X.-L., & Liu, L. (2010). Uncertain distance-based range queries over uncertain
moving objects. J. Comput. Sci. Technol., 25(5), 982998.
Chomicki, J., Marcinkowski, J., & Staworko, S. (2004). Computing consistent query answers using
conflict hypergraphs. In Proceedings of the 2004 ACM CIKM International Conference on
Information and Knowledge Management (CIKM), pp. 417426.
Chung, B. S. E., Lee, W.-C., & Chen, A. L. P. (2009). Processing probabilistic spatio-temporal
range queries over moving objects with uncertainty. In Proceedings of the 12th International
Conference on Extending Database Technology (EDBT), pp. 6071.
Cohn, A. G., & Hazarika, S. M. (2001). Qualitative spatial representation and reasoning: An
overview. Fundam. Inform., 46(1-2), 129.
Cohn, A. G., Li, S., Liu, W., & Renz, J. (2014). Reasoning about topological and cardinal direction
relations between 2-dimensional spatial objects. J. Artif. Intell. Res. (JAIR), 51, 493532.
Conforti, M., & Cornuejols, G. (1992). A class of logic problems solvable by linear programming.
In Proceedings of the 33rd Annual Symposium on Foundations of Computer Science (FOCS),
pp. 670675.
Cozman, F. G., & di Ianni, L. F. (2015). Probabilistic satisfiability and coherence checking through
integer programming. Int. J. Approx. Reasoning, 58, 5770.
Doder, D., Grant, J., & Ognjanovic, Z. (2013). Probabilistic logics for objects located in space and
time. J. of Logic and Computation, 23(3), 487515.
Fagin, R. (1983). Degrees of acyclicity for hypergraphs and relational database schemes. Journal
of the ACM, 30(3).
Finger, M., & Bona, G. D. (2011). Probabilistic satisfiability: Logic-based algorithms and phase
transition. In Proceedings of the 22nd International Joint Conference on Artificial Intelligence
(IJCAI), pp. 528533.
Flesca, S., Furfaro, F., & Parisi, F. (2014). Consistency checking and querying in probabilistic
databases under integrity constraints. J. Comput. Syst. Sci., 80(7), 14481489.
Gabelaia, D., Kontchakov, R., Kurucz, A., Wolter, F., & Zakharyaschev, M. (2005). Combining
spatial and temporal logics: Expressiveness vs. complexity. J. Artif. Intell. Res., 23, 167243.
Galton, A. (2009). Spatial and temporal knowledge representation. Earth Science Informatics, 2(3),
169187.
Georgakopoulos, G. F., Kavvadias, D. J., & Papadimitriou, C. H. (1988). Probabilistic satisfiability.
J. Complexity, 4(1), 111.
Grant, J., Molinaro, C., & Parisi, F. (2013). Aggregate count queries in probabilistic spatio-temporal
databases. In Proceedings of the 7th International Conference on Scalable Uncertainty Management (SUM), pp. 255268.
Grant, J., Parisi, F., Parker, A., & Subrahmanian, V. S. (2010). An agm-style belief revision mechanism for probabilistic spatio-temporal logics. Artif. Intell., 174(1), 72104.
Grant, J., Parisi, F., & Subrahmanian, V. S. (2013). Research in probabilistic spatiotemporal
databases: The SPOT framework. In Advances in Probabilistic Databases for Uncertain
Information Management, Vol. 304 of Studies in Fuzziness and Soft Computing, pp. 122.
Springer.
794

fiK NOWLEDGE R EPRESENTATION IN P ROBABILISTIC S PATIO -T EMPORAL K NOWLEDGE BASES

Hailperin, T. (1984). Probability logic. Notre Dame Journal of Formal Logic, 25(3), 198212.
Halpern, J. Y. (1990). An analysis of first-order logics of probability. Artif. Intell., 46(3), 311350.
Hammel, T., Rogers, T. J., & Yetso, B. (2003). Fusing live sensor data into situational multimedia
views. In Proceedings of the 9th International Workshop on Multimedia Information Systems
(MIS), pp. 145156.
Huang, J., Li, J. J., & Renz, J. (2013). Decomposition and tractability in qualitative spatial and
temporal reasoning. Artif. Intell., 195, 140164.
Jaumard, B., Hansen, P., & de Aragao, M. P. (1991). Column generation methods for probabilistic
logic. ORSA Journal on Computing, 3(2), 135148.
Julier, S. J., Jeffrey, & Uhlmann, K. (2004). Unscented filtering and nonlinear estimation. Proceedings of the IEEE, 92, 401422.
Junger, M., Liebling, T., Naddef, D., Nemhauser, G., Pulleyblank, W., Reinelt, G., Rinaldi, G., &
Wolsey, L. (Eds.). (2010). 50 Years of Integer Programming 1958-2008: From the Early Years
to the State-of-the-Art. Springer, Heidelberg.
Kalman, R. E. (1960). A new approach to linear filtering and prediction problems. Transactions of
the ASMEJournal of Basic Engineering, 82(Series D), 3545.
Karbassi, A., & Barth, M. (2003). Vehicle route prediction and time of arrival estimation techniques
for improved transportation system management. In Proceedings of the 2013 IEEE Intelligent
Vehicles Symposium, pp. 511516.
Karimi, H. A. (2013). Advanced location-based technologies and services. CRC Press, Hoboken,
NJ.
Kavvadias, D. J., & Papadimitriou, C. H. (1990). A linear programming approach to reasoning
about probabilities. Ann. Math. Artif. Intell., 1, 189205.
Kern-Isberner, G., & Thimm, M. (2010). Novel semantical approaches to relational probabilistic
conditionals. In Proceedings of the 12th International Conference on Principles of Knowledge
Representation and Reasoning (KR).
Kersting, K., & Raedt, L. D. (2007). Bayesian logic programming: Theory and tool. In Getoor, L.,
& Taskar, B. (Eds.), An Introduction to Statistical Relational Learning. MIT Press.
Kersting, K. (2012). Lifted probabilistic inference. In Proceedings of the 20th European Conference
on Artificial Intelligence (ECAI), pp. 3338.
Kitagawa, G. (1987). Non-gaussian state-space modeling of nonstationary time series. Journal of
the American Statistical Association, 82(400), 10321041.
Knapp, A., Merz, S., Wirsing, M., & Zappe, J. (2006). Specification and refinement of mobile
systems in mtla and mobile uml. Theor. Comput. Sci., 351(2), 184202.
Kurkovsky, S., & Harihar, K. (2006). Using ubiquitous computing in interactive mobile marketing.
Personal Ubiquitous Comput., 10(4), 227240.
Li, S. Z., & Jain, A. K. (Eds.). (2011). Handbook of Face Recognition, 2nd Edition. Springer.
Liu, J. S., & Chen, R. (1998). Sequential monte carlo methods for dynamic systems. Journal of the
American Statistical Association, 93, 10321044.
795

fiPARISI & G RANT

Lukasiewicz, T. (2001). Probabilistic logic programming with conditional constraints. ACM Trans.
on Computational Logic, 2(3), 289339.
Lukasiewicz, T. (1999). Probabilistic deduction with conditional constraints over basic events. J.
Artif. Intell. Res. (JAIR), 10, 199241.
Lukasiewicz, T., & Kern-Isberner, G. (1999). Probalilistic logic programming under maximum
entropy. In Proceedings of the 5th European Conference on Symbolic and Quantitative Approaches to Reasoning and Uncertainty (ECSQARU), pp. 279292.
MarketsandMarkets (2014). Location Based Services (LBS) Market (Mapping, Discovery and
Infotainment, Location Analytics, Leisure and Social Networking, Location Based Advertising, Augmented Reality and Gaming, Tracking) - Worldwide Forecasts and Analysis (2014 - 2019). http://www.marketsandmarkets.com/Market-Reports/
location-based-service-market-96994431.html.
Milch, B., Marthi, B., Russell, S. J., Sontag, D., Ong, D. L., & Kolobov, A. (2005). BLOG: probabilistic models with unknown objects. In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI), pp. 13521359.
Mittu, R., & Ross, R. (2003). Building upon the coalitions agent experiment (coax) - integration
of multimedia information in gccs-m using impact. In Proceedings of the 9th International
Workshop on Multimedia Information Systems (MIS), pp. 3544.
Muller, P. (1998). A qualitative theory of motion based on spatio-temporal primitives. In Proceedings of the 6th International Conference on Principles of Knowledge Representation and
Reasoning (KR), pp. 131143.
Nilsson, N. J. (1986). Probabilistic logic. Artif. Intell., 28(1), 7187.
Papadimitriou, C. H., & Steiglitz, K. (1982). Combinatorial optimization: algorithms and complexity. Prentice-Hall, Inc., Upper Saddle River, NJ, USA.
Papadimitriou, C. M. (1994). Computational complexity. Addison-Wesley, Reading, Massachusetts.
Paris, J. (1994). The Uncertain Reasoners Companion: A Mathematical Perspective. Cambridge
University Press.
Parisi, F., & Grant, J. (2014a). Integrity constraints for probabilistic spatio-temporal knowledgebases. In Proceedings of the 8th International Conference on Scalable Uncertainty Management (SUM), pp. 251264.
Parisi, F., & Grant, J. (2014b). Repairs and consistent answers for inconsistent probabilistic spatiotemporal databases. In Proceedings of the 8th International Conference on Scalable Uncertainty Management (SUM), pp. 265279.
Parisi, F., Parker, A., Grant, J., & Subrahmanian, V. S. (2010). Scaling cautious selection in spatial
probabilistic temporal databases. In Methods for Handling Imperfect Spatial Information,
Vol. 256 of Studies in Fuzziness and Soft Computing, pp. 307340. Springer.
Parisi, F., Sliva, A., & Subrahmanian, V. S. (2013). A temporal database forecasting algebra. Int. J.
of Approximate Reasoning, 54(7), 827860.
Parker, A., Infantes, G., Grant, J., & Subrahmanian, V. S. (2009). SPOT databases: Efficient consistency checking and optimistic selection in probabilistic spatial databases. IEEE Transactions
on Knowledge and Data Engineering (TKDE), 21(1), 92107.
796

fiK NOWLEDGE R EPRESENTATION IN P ROBABILISTIC S PATIO -T EMPORAL K NOWLEDGE BASES

Parker, A., Infantes, G., Subrahmanian, V. S., & Grant, J. (2008). An AGM-based belief revision
mechanism for probabilistic spatio-temporal logics. In Proceedings of the 23rd AAAI Conference on Artificial Intelligence (AAAI), pp. 511516.
Parker, A., Subrahmanian, V. S., & Grant, J. (2007a). A logical formulation of probabilistic spatial
databases. IEEE Transactions on Knowledge and Data Engineering (TKDE), 19(11), 1541
1556.
Parker, A., Yaman, F., Nau, D. S., & Subrahmanian, V. S. (2007b). Probabilistic go theories. In
Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI), pp.
501506.
Pelanis, M., Saltenis, S., & Jensen, C. S. (2006). Indexing the past, present, and anticipated future
positions of moving objects. ACM Trans. Database Syst., 31(1), 255298.
Petrova, K., & Wang, B. (2011). Location-based services deployment and demand: aroadmap
model. Electronic Commerce Research, 11(1), 529.
Picado-Muino, D. (2011). Measuring and repairing inconsistency in probabilistic knowledge bases.
Int. J. Approx. Reasoning, 52(6), 828840.
Potyka, N., & Thimm, M. (2014). Consolidation of probabilistic knowledge bases by inconsistency
minimization. In Proceedings of the 21st European Conference on Artificial Intelligence
(ECAI), pp. 729734.
Randell, D. A., Cui, Z., & Cohn, A. G. (1992). A spatial logic based on regions and connection. In
Proceedings of the 3rd International Conference on Principles of Knowledge Representation
and Reasoning (KR), pp. 165176.
Richardson, M., & Domingos, P. (2006). Markov logic networks. Mach. Learn., 62(1-2), 107136.
Rosales, R., & Sclaroff, S. (1999). 3D trajectory recovery for tracking multiple objects and trajectory
guided recognition of actions. In Proceedings of the 6th Conference on Computer Vision and
Pattern Recognition (CVPR, pp. 21172123.
Southey, F., Loh, W., & Wilkinson, D. F. (2007). Inferring complex agent motions from partial trajectory observations. In Proceedings of the 20th International Joint Conference on Artificial
Intelligence (IJCAI), pp. 26312637.
Stone, L. D., Corwin, T. L., & Barlow, C. A. (1999). Bayesian Multiple Target Tracking (1st edition).
Artech House, Inc., Norwood, MA, USA.
Szeliski, R. (2010). Computer Vision: Algorithms and Applications. Springer-Verlag New York,
Inc., New York, NY, USA.
Tao, Y., Cheng, R., Xiao, X., Ngai, W. K., Kao, B., & Prabhakar, S. (2005). Indexing multidimensional uncertain data with arbitrary probability density functions. In Proceedings of
the 31st International Conference on Very Large Data Bases (VLDB), pp. 922933.
Thimm, M. (2013). Inconsistency measures for probabilistic logics. Artif. Intell., 197, 124.
Wolter, F., & Zakharyaschev, M. (2000). Spatio-temporal representation and reasoning based on
rcc-8. In Proceedings of the 7th International Conference on Principles of Knowledge Representation and Reasoning (KR), pp. 314.
797

fiPARISI & G RANT

Yaman, F., Nau, D. S., & Subrahmanian, V. S. (2004). A logic of motion. In Proceedings of the 9th
International Conference on Principles of Knowledge Representation and Reasoning (KR),
pp. 8594.
Yaman, F., Nau, D. S., & Subrahmanian, V. S. (2005a). Going far, logically. In Proceedings of the
19th International Joint Conference on Artificial Intelligence (IJCAI), pp. 615620.
Yaman, F., Nau, D. S., & Subrahmanian, V. (2005b). A motion closed world assumption. In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI), pp.
621626.
Yang, B., Lu, H., & Jensen, C. S. (2010). Probabilistic threshold k nearest neighbor queries over
moving objects in symbolic indoor space. In Proceedings of the 13th International Conference on Extending Database Technology (EDBT), pp. 335346.
Yilmaz, A., Javed, O., & Shah, M. (2006). Object tracking: A survey. ACM Comput. Surv., 38(4).
Zhang, M., Chen, S., Jensen, C. S., Ooi, B. C., & Zhang, Z. (2009). Effectively indexing uncertain
moving objects for predictive queries. Proceedings of the VLDB Endowment (PVLDB), 2(1),
11981209.
Zheng, K., Trajcevski, G., Zhou, X., & Scheuermann, P. (2011). Probabilistic range queries for
uncertain trajectories on road networks. In Proceedings of the 14th International Conference
on Extending Database Technology (EDBT), pp. 283294.

798

fiJournal of Artificial Intelligence Research 55 (2016) 283-316

Submitted 03/24; published 01/16

News Across Languages - Cross-Lingual Document
Similarity and Event Tracking
Jan Rupnik
Andrej Muhic
Gregor Leban
Primoz Skraba
Blaz Fortuna
Marko Grobelnik

jan.rupnik@ijs.si
andrej.muhic@ijs.si
gregor.leban@ijs.si
primoz.skraba@ijs.si
blaz.fortuna@ijs.si
marko.grobelnik@ijs.si

Artificial Intelligence Laboratory, Jozef Stefan Institute,
Jamova cesta 39, 1000 Ljubljana, Slovenia

Abstract
In todays world, we follow news which is distributed globally. Significant events are
reported by different sources and in different languages. In this work, we address the
problem of tracking of events in a large multilingual stream. Within a recently developed
system Event Registry we examine two aspects of this problem: how to compare articles in
different languages and how to link collections of articles in different languages which refer
to the same event. Taking a multilingual stream and clusters of articles from each language,
we compare different cross-lingual document similarity measures based on Wikipedia. This
allows us to compute the similarity of any two articles regardless of language. Building on
previous work, we show there are methods which scale well and can compute a meaningful
similarity between articles from languages with little or no direct overlap in the training
data. Using this capability, we then propose an approach to link clusters of articles across
languages which represent the same event. We provide an extensive evaluation of the
system as a whole, as well as an evaluation of the quality and robustness of the similarity
measure and the linking algorithm.

1. Introduction
Content on the Internet is becoming increasingly multilingual. A prime example is Wikipedia. In 2001, the majority of pages were written in English, while in 2015, the percentage
of English articles has dropped to 14%. At the same time, online news has begun to dominate reporting of current events. However, machine translation remains relatively rudimentary. It allows people to understand simple phrases on web pages, but remains inadequate
for more advanced understanding of text. In this paper we consider the intersection of these
developments: how to track events which are reported about in multiple languages.
The term event is vague and ambiguous, but for the practical purposes, we define it as
any significant happening that is being reported about in the media. Examples of events
would include shooting down of the Malaysia Airlines plane over Ukraine on July 18th, 2014
(see Figure 1) and HSBCs admittance of aiding their clients in tax evasion on February
9th, 2015. Events such as these are covered by many articles and the question is how to find
all the articles in different languages that are describing a single event. Generally, events
c
2016
AI Access Foundation. All rights reserved.

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

Figure 1: Events are represented by collections of articles about an event, in this case the
Malaysian airliner which was shot down over Ukraine. The results shown in
the figure can be obtained using the query http://eventregistry.org/event/
997350#?lang=eng&tab=articles. The content presented is part of the Event
Registry system, developed by the authors.

are more specific than general themes as the time component plays an important role  for
example, the two wars in Iraq would be considered as separate events.
As input, we consider a stream of articles in different languages and a list of events.
Our goal is to assign articles to their corresponding events. A priori, we do not know the
coverage of the articles, that is, not all the events may be covered and we do not know that
all the articles necessarily fit into one of the events. The task is divided into two parts:
detecting events within each language and then linking events across languages. In this
paper we address the second step.
We consider a high volume of articles in different languages. By using a language
detector, the stream is split into separate monolingual streams. Within each monolingual
284

fiCross-Lingual Document Similarity and Event Tracking

stream, an online clustering approach is employed, where tracked clusters correspond to
our definition of events - this is based on the Event Registry system (Leban, Fortuna,
Brank, & Grobelnik, 2014b, 2014a). Our main goal in this paper is to connect such clusters
(representations of events) across languages, that is, to detect that a set of articles in
language A reports on the same event as a set of articles in language B.
Our approach to link clusters across languages combines two ingredients: a cross-lingual
document similarity measure, which can be interpreted as a language independent topic
model, and semantic annotation of documents, which enables an alternative way to comparing documents. Since this work represents a complicated pipeline, we concentrate on
these two specific elements. Overall, the approach should be considered from a systems
perspective (considering the system as a whole) rather than considering these problems in
isolation.
The first ingredient of our approach to link clusters across languages represents a continuation of previous work (Rupnik, Muhic, & Skraba, 2011a, 2012, 2011b; Muhic, Rupnik,
& Skraba, 2012) where we explored representations of documents which were valid over
multiple languages. The representations could be interpreted as multilingual topics, which
were then used as proxies to compute cross-lingual similarities between documents. To learn
the representations, we use Wikipedia as a training corpus. Significantly, we do not only
consider the major or hub languages such as English, German, French, etc. which have
significant overlap in article coverage, but also smaller languages (in terms of number of
Wikipedia articles) such as Slovenian and Hindi, which may have a negligible overlap in
article coverage. We can then define a similarity between any two articles regardless of language, which allows us to cluster the articles according to topic. The underlying assumption
is that articles describing the same event are similar and will therefore be put into the same
cluster.
Using the similarity function, we propose a novel algorithm for linking events/clusters
across languages. We pose the task as a classification problem based on several sets of
features. In addition to these features, cross-lingual similarity is also used to quickly identify
a small list of potential linking candidates for each cluster. This greatly increases the
scalability of the system.
The paper is organized as follows: we first provide an overview of the system as a whole
in Section 2, which includes a subsection that summarizes the main system requirements.
We then present related work in Section 3. The related work covers work on cross-lingual
document similarity as well as work on cross-lingual cluster linking. In Section 4, we introduce the problem of cross-lingual document similarity computation and describe several
approaches to the problem, most notably a new approach based on hub languages. In Section 5, we introduce the central problem of cross-lingual linking of clusters of news articles
and our approach that combines the cross-lingual similarity functions with knowledge extraction based techniques. Finally, we present and interpret the experimental results in
Section 6 and discuss conclusions and point out several promising future directions.

2. Pipeline
We base our techniques of cross-lingual event linking on an online system for detection
of world events, called Event Registry (Leban et al., 2014b, 2014a). Event Registry is a
285

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

Data	 
collec-on	 

Main-	 
stream	 
news	 

Ar-cle-level	 
processing	 

Event	 
construc-on	 

Seman&c	 
annota&on	 

Ar&cle	 clustering	 

Extrac&on	 of	 
date	 references	 

Event	 forma&on	 

Manual	 event	 
administra&on	 

Event	 info.	 
extrac&on	 

Detec&on	 of	 
ar&cle	 loca&on	 

Cross-lingual	 
cluster	 
matching	 

Cross-lingual	 
ar-cle	 matching	 

Detec&on	 of	 
ar&cle	 duplicates	 

Event	 storage	 &	 
maintenance	 

Filling	 event	 
template	 
Iden&fying	 
related	 events	 

Frontend	 
interface	 

API	 	 
access	 

Figure 2: The Event Registry pipeline. After new articles are collected, they are first analyzed individually (Article-level processing). In the next step, groups of articles
about the same event are identified and relevant information about the event
is extracted (Event construction phase). Although the pipeline contains several
components, we focus only on the two highlighted in the image.

repository of events, where events are automatically identified by analyzing news articles
that are collected from numerous news outlets all over the world. The important components
in the pipeline of the Event Registry are shown in Figure 2. We will now briefly describe
the main components.
The collection of the news articles is performed using the Newsfeed service (Trampus
& Novak, 2012). The service monitors RSS feeds of around 100,000 mainstream news
outlets available globally. Whenever a new article is detected in the RSS feed, the service
downloads all available information about the article and sends the article through the
pipeline. Newsfeed downloads daily on average around 200,000 news articles in various
languages, where English, Spanish and German are the most common.
Collected articles are first semantically annotated by identifying mentions of relevant
concepts  either entities or important keywords. The disambiguation and entity linking
of the concepts is done using Wikipedia as the main knowledge base. The algorithm for
semantic annotation uses machine learning to detect significant terms within unstructured
text and link them to the appropriate Wikipedia articles. The approach models link probability and combines prior word sense distributions with context based sense distributions.
286

fiCross-Lingual Document Similarity and Event Tracking

The details are reported by Milne and Witten (2008) and Zhang and Rettinger (2014a). As
a part of the semantic annotation we also analyze the dateline of the article to identify the
location of the described event as well as to extract dates mentioned in the article using a
set of regular expressions. Since articles are frequently revised we also detect if the collected
article is just a revision of a previous one so that we can use this information in next phases
of the pipeline. The last important processing step on the document level is to efficiently
identify which articles in other available languages are most similar to this article. The
methodology for this task is one of the main contributions of this paper and is explained in
details in Section 4.
As the next step, an online clustering algorithm (Brank, Leban, & Grobelnik, 2014) is
applied to the articles in order to identify groups of articles that are discussing the same
event. For each new article, the clustering algorithm determines if the article should be
assigned to some existing cluster or into a new cluster. The underlying assumption is that
articles that are describing the same event are similar enough and will therefore be put
into the same cluster. For clustering, each new article is first tokenized, stop words are
removed and the remaining words are stemmed. The remaining tokens are represented in
a vector-space model and normalized using TF-IDF1 (see Section 4.1 for the definition).
Cosine similarity is used to find the most similar existing cluster, by comparing the documents vector to the centroid vector of each cluster. A user-defined threshold is used to
determine if the article is not similar enough to any existing clusters (0.4 was used in our
experiments). If the highest similarity is above the threshold, the article is assigned to the
corresponding cluster, otherwise a new cluster is created, initially containing only the single
article. Whenever an article is assigned to a cluster, the clusters centroid vector is also
updated. Since articles about an event are commonly written only for a short period of
time, we remove clusters once the oldest article in the cluster becomes more than 4 days
old. This housekeeping mechanism prevents the clustering from becoming slow and also
ensures that articles are not assigned to obsolete clusters.
Once the number of articles in a cluster reaches a threshold (which is a language dependent parameter), we assume that the articles in the cluster are describing an event. At that
point, a new event with a unique ID is created in Event Registry, and the cluster with the
articles is assigned to it. By analyzing the articles, we extract the main information about
the event, such as the event location, date, most relevant entities and keywords, etc.
Since articles in a cluster are in a single language, we also want to identify any other
existing clusters that report about the same event in other languages and join these clusters
into the same event. This task is performed using a classification approach which is the
second major contribution of this paper. It is described in detail in Section 5.
When a cluster is identified and information about the event is extracted, all available
data is stored in a custom-built database system. The data is then accessible through the
API or a web interface (http://eventregistry.org/), which provide numerous search and
visualization options.

1. The IDF weights are dynamically computed for each new article over all news articles within a 10 day
window.

287

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

2.1 System Requirements
Our goal is to build a system that monitors global media and analyzes how events are being
reported on. Our approach consists of two steps: tracking events separately in each language
(based on language detection and an online clustering approach) and then connecting them.
The pipeline must be able to process millions of articles per day and perform billions of
similarity computations each day. Both steps rely heavily on similarity computation, which
should therefore be highly scalable.
Therefore, we focus on implementations that run on a single shared memory machine, as
opposed to clusters of machines. This simplifies implementation and system maintenance.
To summarize, the following properties are desirable:
 Training - The training (building cross-lingual models) should scale to many languages and should be robust to the quality of training resources. The system should
be able to take advantage of comparable corpora (as opposed to parallel translationbased corpora), with missing data.
 Operation efficiency - The similarity computation should be fast - the system must
be able to handle billions of similarity computations per day. Computing the similarity
between a new document and a set of known documents should be efficient (the main
application is linking documents between two monolingual streams).
 Operation cost - The system should run on a strong shared machine server and not
rely on paid services.
 Implementation - The system is straightforward to implement, with few parameters
to tune.
We believe that a cross-lingual similarity component that meets such requirements is very
desirable in a commercial setting, where several different costs have to be taken into consideration.

3. Related Work
In this section, we describe previous work described in the literature. Since there are two
distinctive tasks that we tackle in this paper (computing cross-lingual document similarity
and cross-lingual cluster linking), we have separated the related work into two corresponding
parts.
3.1 Cross-Lingual Document Similarity
There are four main families of approaches to cross-lingual similarity.
3.1.1 Translation and Dictionary Based Approaches
The most obvious way to compare documents written in different languages is to use machine
translation and perform monolingual similarity (see Peters & Braschler, 2012; Potthast,
Barron-Cedeno, Stein, & Rosso, 2011). One can use free tools such as Moses (Koehn et al.,
2007) or translation services, such as Google Translate (https://translate.google.com/).
288

fiCross-Lingual Document Similarity and Event Tracking

There are two issues with such approaches: they solve a harder problem than needs to be
solved and they are less robust to training resource quality - large sets of translated sentences are typically needed. Training Moses for languages with scarce linguistic resources
is thus problematic. The issue with using online services such as Google Translate is that
the APIs are limited and not free. The operation efficiency and cost requirements make
translation-based approaches less suited for our system. Closely related are works CrossLingual Vector Space Model (CL-VSM) (Potthast et al., 2011) and the approach presented
by Pouliquen, Steinberger, and Deguernel (2008) which both compare documents by using
dictionaries, which in both cases are EuroVoc dictionaries (Rodrguez, Azcona, & Paredes,
2008). The generality of such approaches is limited by the quality of available linguistic
resources, which may be scarce or non-existent for certain language pairs.
3.1.2 Probabilistic Topic Model Based Approaches
There exist many variants to modelling documents in a language independent way by using probabilistic graphical models. The models include: Joint Probabilistic Latent Semantic Analysis (JPLSA) (Platt, Toutanova, & Yih, 2010), Coupled Probabilistic LSA
(CPLSA) (Platt et al., 2010), Probabilistic Cross-Lingual LSA (PCLLSA) (Zhang, Mei, &
Zhai, 2010) and Polylingual Topic Models (PLTM) (Mimno, Wallach, Naradowsky, Smith,
& McCallum, 2009) which is a Bayesian version of PCLLSA. The methods (except for
CPLSA) describe the multilingual document collections as samples from generative probabilistic models, with variations on the assumptions on the model structure. The topics
represent latent variables that are used to generate observed variables (words), a process
specific to each language. The parameter estimation is posed as an inference problem which
is typically intractable and one usually solves it using approximate techniques. Most variants of solutions are based on Gibbs sampling or Variational Inference, which are nontrivial
to implement and may require an experienced practitioner to be applied. Furthermore,
representing a new document as a mixture of topics is another potentially hard inference
problem which must be solved.
3.1.3 Matrix Factorization Base Approaches
Several matrix factorization based approaches exist in the literature. The models include:
Non-negative matrix factorization based (Xiao & Guo, 2013), Cross-Lingual Latent Semantic Indexing CL-LSI (Dumais, Letsche, Littman, & Landauer, 1997; Peters & Braschler,
2012), Canonical Correlation Analysis (CCA) (Hotelling, 1935), Oriented Principal Component Analysis (OPCA) (Platt et al., 2010). The quadratic time and space dependency of
the OPCA method makes it impractical for large scale purposes. In addition, OPCA forces
the vocabulary sizes for all languages to be the same, which is less intuitive. For our setting, the method by Xiao and Guo (2013) has a prohibitively high computational cost when
building models (it uses dense matrices whose dimensions are a product of the training set
size and the vocabulary size). Our proposed approach combines CCA and CL-LSI. Another
closely related method is Cross-Lingual Explicit Semantic Analysis (CL-ESA) (Potthast,
Stein, & Anderka, 2008), which uses Wikipedia (as do we in the current work) to compare
documents. It can be interpreted as using the sample covariance matrix between features of
two languages to define the dot product which is used to compute similarities. The authors
289

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

of CL-ESA compare it to CL-LSI and find that CL-LSI can outperform CL-ESA in an
information retrieval, but is costlier to optimize over a large corpus (CL-ESA requires no
training). We find that the scalability argument does not apply in our case: based on advances in numerical linear algebra we can solve large CL-LSI problems that involve millions
of documents as opposed to the 10,000 document limit reported by Potthast et al. (2008).
In addition, CL-ESA is less suited for computing similarities between two large monolingual
streams. For example, each day we have to compute similarities between 500,000 English
and 500,000 German news articles. Comparing each German news article with 500,000
English news articles is either prohibitively slow (involves projecting all English articles on
Wikipedia) or consumes too much memory (involves storing the projected English articles,
which for a Wikipedia of size 1,000,000 is a 500,000 by 1,000,000 non-sparse matrix).
3.1.4 Monolingual Approaches
Finally, related work includes monolingual approaches that treat document written in different languages in a monolingual fashion. The intuition is that named entities (for example, Obama) and cognate words (for example, tsunami) are written in the same or
similar fashion in many languages. For example, the Cross-Language Character n-Gram
Model (CL-CNG) (Potthast et al., 2011) represents documents as bags of character ngrams. Another approach is to use language dependent keyword lists based on cognate
words (Pouliquen et al., 2008). These approaches may be suitable for comparing documents written in languages that share a writing system, which does not apply to the case
of global news tracking.
Based on our requirements in Section 2.1, we chose to focus on methods based on vector
space models and linear embeddings. We propose a method that is more efficient than
popular alternatives (a clustering-based approach and latent semantic indexing), but is still
simple to optimize and use.
3.2 Cross-Lingual Cluster Linking
Although there are a number of services that aggregate news by identifying clusters of
similar articles, there are almost no services that provide linking of clusters over different
languages. Google News as well as Yahoo! News are able to identify clusters of articles
about the same event, but offer no linking of clusters across languages. The only service
that we found, which provides cross-lingual cluster linking, is the European Media Monitor
(EMM) (Pouliquen et al., 2008; Steinberger, Pouliquen, & Ignat, 2005). EMM clusters
articles in 60 languages and then tries to determine which clusters of articles in different
languages describe the same event. To achieve cluster linking, EMM uses three different
language independent vector representations for each cluster. The first vector contains
the weighted list of references to countries mentioned in the articles, while the second
vector contains the weighted list of mentioned people and organizations. The last vector
contains the weighted list of Eurovoc subject domain descriptors. These descriptors are
topics, such as air transport, EC agreement, competition and pollution control into which
articles are automatically categorized (Pouliquen, Steinberger, & Ignat, 2006). Similarity
between clusters is then computed using a linear combination of the cosine similarities
computed on the three vectors. If the similarity is above the threshold, the clusters are
290

fiCross-Lingual Document Similarity and Event Tracking

linked. Compared to EMM, our approach uses document similarities to obtain a small set of
potentially equivalent clusters. Additionally, we do not decide if two clusters are equivalent
based on a hand-set threshold on a similarity value  instead we use a classification model
that uses a larger set of features related to the tested pair of clusters.
A system, which is significantly different but worth mentioning, is the GDELT
project (Leetaru & Schrodt, 2013). In GDELT, events are also extracted from articles,
but in their case, an event is specified in a form of a triple containing two actors and a
relation. The project contains an extensive vocabulary of possible relations, mostly related to political events. In order to identify events, GDELT collects articles in more than
65 languages and uses machine translation to translate them to English. All information
extraction is then done on the translated article.

4. Cross-Lingual Document Similarity
Document similarity is an important component in techniques from text mining and natural
language processing. Many techniques use the similarity as a black box, e.g., a kernel in
Support Vector Machines. Comparison of documents (or other types of text snippets) in a
monolingual setting is a well-studied problem in the field of information retrieval (Salton
& Buckley, 1988). We first formally introduce the problem followed by a description of our
approach.
4.1 Problem Definition
We will first describe how documents are represented as vectors and how to compare documents in a mono-lingual setting. We then define a way to measure cross-lingual similarity
which is natural for the models we consider.
4.1.1 Document Representation
The standard vector space model (Salton & Buckley, 1988) represents documents as vectors, where each term corresponds to a word or a phrase in a fixed vocabulary. Formally,
document d is represented by a vector x  Rn , where n corresponds to the size of the
vocabulary, and vector elements xk correspond to the number of times term k occurred in
the document, also called term frequency or T Fk (d).
We also used a term re-weighting scheme that adjusts for the fact that some words
occur more frequently in general. A term weight should correspond to the importance of
the term for the given corpus. The common weighting scheme is called Term Frequency
Inverse Document Frequency (T F IDF ) weighting. An
Document Frequency (IDF )
 Inverse

N
weight for the dictionary term k is defined as log DFk , where DFk is the number of
documents in the corpus which contain term k and N is the total number of documents
in the corpus. When building cross-lingual models, the IDF scores were computed with
respect to the Wikipedia corpus. In the other part of our system, we computed TFIDF
vectors on streams of news articles in multiple languages. There the IDF scores for each
language changed dynamically - for each new document we computed the IDF of all news
articles within a 10 day window.
291

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

Therefore we can define a documents T F IDF as
xij :=

term frequency in document i
.
inverse document frequency of term j

The T F IDF weighted vector space model document representation corresponds to a map
 : text  Rn defined by:


N
.
(d)k = T F k (d) log
DF k
4.1.2 Mono-Lingual Similarity
A common way of computing similarity between documents is cosine similarity,
sim(d1 , d2 ) =

h(d1 ), (d2 )i
,
k(d1 )kk(d2 )k

where h, i and kk are standard inner product and Euclidean norm. When dealing with two
or more languages, one could ignore the language information and build a vector space using
the union of tokens over the languages. A cosine similarity function in such a space can
be useful to some extent, for example Internet or Obama may appear both in Spanish
and English texts and the presence of such terms in both an English and a Spanish document would contribute to their similarity. In general however, large parts of vocabularies
may not intersect. This means that given a language pair, many words in both languages
cannot contribute to the similarity score. Such cases can make the similarity function very
insensitive to the data.
4.1.3 Cross-Lingual Similarity
Processing a multilingual dataset results in several vector spaces with varying dimensionality, one for each language. The dimensionality of the vector space corresponding to the i-th
language is denoted by ni and the vector space model mapping is denoted by i : text  Rni .
The similarity between documents in language i and language j is defined as a bilinear operator represented as a matrix Si,j  Rni nj :
simi,j (d1 , d2 ) =

hi (d1 ), Si,j j (d2 )i
,
ki (d1 )kkj (d2 )k

where d1 and d2 are documents written in the i-th and j-th language respectively. If the
maximal singular value of Si,j is bounded by 1, then the similarity scores will lie on the
interval [1, 1]. We will provide an overview of the models in Section 4.2 and then introduce
additional notation in 4.3. Starting with Section 4.4 and ending with Section 4.7 we will
describe some approaches to compute Si,j given training data.
4.2 Cross-Lingual Models
In this section, we will describe several approaches to the problem of computing the multilingual similarities introduced in Section 4.1. We present four approaches: a simple approach
based on k-means clustering in Section 4.4, a standard approach based on singular value decomposition in Section 4.5, a related approach called Canonical Correlation Analysis (CCA)
292

fiCross-Lingual Document Similarity and Event Tracking

in Section 4.6 and finally a new method, which is an extension of CCA to more than two languages in Section 4.7. CCA can be used to find correlated patterns for a pair of languages,
whereas the extended method optimizes a Sum of Squared Correlations (SSCOR) between
several language pairs, which was introduced by Kettenring (1971). The SSCOR problem
is difficult to solve in our setting (hundreds of thousands of features, hundreds of thousands
of examples). To tackle this, we propose a method which consists of two ingredients. The
first one is based on an observation that certain datasets (such as Wikipedia) are biased
towards one language (English for Wikipedia), which can be exploited to reformulate a
difficult optimization problem as an eigenvector problem. The second ingredient is dimensionality reduction using CL-LSI, which makes the eigenvector problem computationally
and numerically tractable.
We concentrate on approaches that are based on linear maps rather than alternatives,
such as machine translation and probabilistic models, as discussed in the section on related
work. We will start by introducing some notation.
4.3 Notation
The cross-lingual similarity models presented in this paper are based on comparable corpora.
A comparable corpus is a collection of documents in multiple languages, with alignment
between documents that are of the same topic, or even a rough translation of each other.
Wikipedia is an example of a comparable corpus, where a specific entry can be described
in multiple languages (e.g., Berlin is currently described in 222 languages). News articles
represent another example, where the same event can be described by newspapers in several
languages.
More formally, a multilingual document d = (u1 , . . . um ) is a tuple of m documents
on the same topic (comparable), where ui is the document written in language i. Note
that an individual document ui can be an empty document (missing resource) and each d
must contain at least two nonempty documents. This means that in our analysis we
discard strictly monolingual documents for which no cross-lingual information is available.
A comparable corpus D = d1 , . . . , ds is a collection of s multilingual documents. By using
the vector space model, we can represent D as a set of m matrices X1 , . . . , Xm , where
Xi  Rni s is the matrix corresponding to the language i and ni is the vocabulary size
of language i. Furthermore, let Xi` denote the `-th column of matrix Xi and the matrices
respect the document alignment - the vector Xi` corresponds to the TFIDF vector of the
i-th component P
of multilingual document d` . We use N to denote the total row dimension
of X, i.e., N := m
i=1 ni . See Figure 3 for an illustration of the introduced notation.
We will now describe four models to cross-lingual similarity computation in the next
sub-sections.
4.4 k-Means
The k-means algorithm is perhaps the most well-known and widely-used clustering algorithm. Here, we present its application to compute cross-lingual similarities. The idea is
based on concatenating the corpus matrices, running standard k-means clustering to obtain
the matrix of centroids, reversing the concatenation step to obtain a set of aligned bases,
which are finally used to compute cross-lingual similarities. See Figure 4 for overview of
293

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

D = {d1 , d2 , . . . , ds }
d2
d3

d1

X

{

{X
X{X
X {X
X1

1
1

X12 X13

X1s

2

1
2

X22 X23

X2s

3

1
3

X32 X33

X3s

{

s
Xm

{n
{n
{n

1

{

2

3

{n

N

m

{

1
2 X3
Xm Xm
Xm
m

ds

s

Figure 3: Multilingual corpora and their matrix representations using the vector space
model.

294

fiCross-Lingual Document Similarity and Event Tracking

k-means:
X1
X2
X3


=

C1

Q

C2
C3

A new vector in the i-th language,
x 2 Rni is mapped to   the new
coordinates minimize:
||Xi Ci ||
This has the solution
 = (CiT Ci )

1

CiT x

{

Columns of Q are
indicator vectors

Pi

Figure 4: k-means algorithm and coordinate change.
the procedure. The left side of Figure 4 illustrates the decomposition and the right side
summarizes the coordinate change.
In order to apply the algorithm, we first merge all the term-document matrices into a
single matrix X by stacking the individual term-document matrices (as seen in Figure 3):


T T ,
X := X1T , X2T ,    , Xm
such that the columns respect the alignment of the documents (here MATLAB notation for
concatenating matrices is used). Therefore, each document is represented by a long vector
indexed by the terms in all languages.
We then run the k-means algorithm (Hartigan, 1975) and obtain a centroid matrix
C  RN k , where the k columns represent centroid vectors. The centroid matrix can be
split vertically into m blocks:
T T
C = [C1T    Cm
] ,
according to the number of dimensions of each language, i.e., Ci  Rni k . To reiterate, the
matrices Ci are computed using a multilingual corpus matrix X (based on Wikipedia for
example).
To compute cross-lingual document similarities on new documents, note that each matrix
Ci represents a vector space basis and can be used to map points in Rni into a k-dimensional
space, where the new coordinates of a vector x  Rni are expressed as:
(CiT Ci )1 CiT xi .
The resulting matrix for similarity computation between language i and language j is
defined up to a scaling factor as:
Ci (CiT Ci )1 (CjT Cj )1 Cj .
The matrix is a result of mapping documents in a language independent space using
pseudo-inverses of the centroid matrices Pi = (CiT Ci )1 Ci and then comparing them using
the standard inner product, which results in the matrix PiT Pj . For the sake of presentation,
we assumed that the centroid vectors are linearly independent. (An independent subspace
could be obtained using an additional Gram-Schmidt step (Golub & Van Loan, 2012) on
the matrix C, if this was not the case.)
295

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

LSI:
X1
X2
X3


=

U1

S

VT

U2
U3

X = U SV T
UT U = I

U 2 RN k

V TV = I

V 2 Rsk

Figure 5: LSI multilingual corpus matrix decomposition.

4.5 Cross-Lingual Latent Semantic Indexing
The second approach we consider is Cross-Lingual Latent Semantic Indexing (CL-LSI) (Dumais et al., 1997) which is a variant of LSI (Deerwester, Dumais, Landauer, Furnas, &
Harshman, 1990) for more than one language. The approach is very similar to k-means,
where we first concatenate the corpus matrices, compute a decomposition, which in case of
CL-LSI is a truncated Singular Value Decomposition (SVD), decouple the column space matrix and use the blocks to compute linear maps to a common vector space, where standard
cosine similarity is used to compare documents.
The method is based on computing a truncated singular value decomposition of the
concatenated corpus matrix X  U SV T . See Figure 5 for the decomposition. Representing
documents in topic coordinates is done in the same way as in the k-means case (see
Figure 4), we will describe how to compute the coordinate change functions.
The cross-lingual similarity functions are based on a rank-k truncated SVD: X  U V T ,
where U  RN k are basis vectors of interest and   Rkk is a truncated diagonal matrix
of singular eigenvalues. An aligned basis is obtained by first splitting U vertically according
T ]T . Then, the same as with
to the number of dimensions of each language: U = [U1T    Um
k-means clustering, we compute the pseudoinverses Pi = (UiT Ui )1 UiT . The matrices Pi
are used to change the basis from the standard basis in Rni to the basis spanned by the
columns of Ui .
4.5.1 Implementation Note
Since the matrix X can be large we could use an iterative method like the Lanczos algorithm with reorthogonalization (Golub & Van Loan, 2012) to find the left singular vectors
(columns of U ) corresponding to the largest singular values. It turns out that the Lanczos
method converges slowly as the gap between the leading singular values is small. Moreover,
the Lanczos method is hard to parallelize. Instead, we use a randomized version of the
SVD (Halko, Martinsson, & Tropp, 2011) that can be viewed as a block Lanczos method.
That enables us to use parallelization and speeds up the computation considerably.
To compute the matrices Pi we used the QR algorithm (Golub & Van Loan, 2012) to
factorize Ui as Ui = Qi Ri , where QTi Qi = I and Ri is a triangular matrix. Pi is then
obtained by solving Ri Pi = Qi .
296

fiCross-Lingual Document Similarity and Event Tracking

4.6 Canonical Correlation Analysis
We now present a statistical technique to analyze data from two sources, an extension of
which will be presented in the next section.
Canonical Correlation Analysis (CCA) (Hotelling, 1935) is a dimensionality reduction
technique similar to Principal Component Analysis (PCA) (Pearson, 1901), with the additional assumption that the data consists of feature vectors that arose from two sources (two
views) that share some information. Examples include: bilingual document collection (Fortuna, Cristianini, & Shawe-Taylor, 2006) and collection of images and captions (Hardoon,
Mourao-Miranda, Brammer, & Shawe-Taylor, 2008). Instead of looking for linear combinations of features that maximize the variance (PCA) we look for a linear combination of
feature vectors from the first view and a linear combination for the second view, that are
maximally correlated.
Interpreting the columns of Xi as observation vectors sampled from an underlying distribution Xi  Rni , the idea is to find two weight vectors wi  Rni and wj  Rnj so that
the random variables wiT  Xi and wjT  Xj are maximally correlated (wi and wj are used to
map the random vectors to random variables, by computing weighted sums of vector components). Let (x, y) denote the sample-based correlation coefficient between two vectors
of observations x and y. By using the sample matrix notation Xi and Xj (assuming no
data is missing to simplify the exposition), this problem can be formulated as the following
optimization problem:
maximizen

wi Rni ,wj R

j

wiT Ci,j wj
(wiT Xi , wjT Xj ) = q
,
q
wiT Ci,i wi wjT Cj,j wj

where Ci,i and Cj,j are empirical estimates of variances of Xi and Xj respectively and Ci,j is
an estimate for the covariance matrix. Assuming that the observation vectors are centered
(only for the purposes of presentation), the matrices are computed in the following way:
1
Xi XjT , and similarly for Ci,i and Cj,j . The optimization problem can be reduced
Ci,j = n1
to an eigenvalue problem and includes inverting the variance matrices Ci,i and Cj,j . If
the matrices are not invertible, one can use a regularization technique by replacing Ci,i
with (1  )Ci,i + I, where   [0, 1] is the regularization coefficient and I is the identity
matrix. (The same can be applied to Cj,j .) A single canonical variable is usually inadequate
in representing the original random vector and typically one looks for k projection pairs
(wi1 , wj1 ), . . . , (wik , wjk ), so that (wiu )T Xi and (wju )T Xj are highly correlated and (wiu )T Xi is
uncorrelated with (wiv )T Xi for u 6= v and analogously for wju vectors.
Note that the method in its original form is only applicable to two languages where an
aligned set of observations is available. The next section will describe a scalable extension
of CCA to more than two languages.
4.7 Hub Language Based CCA Extension
Building cross-lingual similarity models based on comparable corpora is challenging for two
main reasons. The first problem is related to missing alignment data: when a number of
languages is large, the dataset of documents that cover all languages is small (or may even
be empty). Even if only two languages are considered, the set of aligned documents can
297

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

be small (an extreme example is given by the Piedmontese and Hindi Wikipedias where no
inter-language links are available), in which case none of the methods presented so far are
applicable. The second challenge is scale - the data is high-dimensional (many languages
with hundreds of thousands of features per language) and the number of multilingual documents may be large (over one million in case of Wikipedia). The optimization problem
posed by CCA is not trivial to solve: the covariance matrices themselves are prohibitively
large to fit in memory (even storing a 100,000 by 100,000 element matrix requires 80GB of
memory) and iterative matrix-multiplication based approaches to solving generalized eigenvalue problems are required (the covariance matrices can be expressed as products of sparse
matrices, which means we have fast matrix-vector multiplication).
We now describe an extension of CCA to more than two languages, which can be trained
on large comparable corpora and can handle missing data. The extension we consider is
based on a generalization of CCA to more than two views, introduced by Kettenring (1971),
namely the Sum of Squared Correlations SSCOR, which we will state formally later in this
section. Our approach exploits a certain characteristic of the data, namely the hub language
characteristic (see below) in two ways: to reduce the dimensionality of the data and to
simplify the optimization problem.
4.7.1 Hub Language Characteristic
In the case of Wikipedia, we observed that even though the training resources are scarce for
certain language pairs, there often exists indirect training data. By considering a third language, which has training data with both languages in the pair, we can use the composition
of learned maps as a proxy. We refer to this third language as a hub language.
A hub language is a language with a high proportion of non-empty documents in D =
{d1 , ..., d` }. As we have mentioned, we only focus on multilingual documents that include
at least two languages. The prototypical example in the case of Wikipedia is English. Our
notion of the hub language could be interpreted in the following way. If a non-English
Wikipedia page contains one or more links to variants of the page in other languages,
English is very likely to be one of them. That makes English a hub language.
We use the following notation to define subsets of the multilingual comparable corpus:
let a(i, j) denote the index set of all multilingual documents with non-missing data for the
i-th and j-th language:
a(i, j) = {k | dk = (u1 , ..., um ), ui 6= , uj 6= } ,
and let a(i) denote the index set of all multilingual documents with non missing data for
the i-th language.
We now describe a two step approach to building a cross-lingual similarity matrix. The
first part is related to LSI and reduces the dimensionality of the data. The second step
refines the linear mappings and optimizes the linear dependence between data.
4.7.2 Step 1: Hub Language Based Dimensionality Reduction
The first step in our method is to project X1 , . . . , Xm to lower-dimensional spaces without
destroying the cross-lingual structure. Treating the nonzero columns of Xi as observation
vectors sampled from an underlying distribution Xi  Vi = Rni , we can analyze the empirical
298

fiCross-Lingual Document Similarity and Event Tracking

cross-covariance matrices:
Ci,j =

X
1
(Xi`  ci )  (Xj`  cj )T ,
|a(i, j)|  1
`a(i,j)

where ci = a1i `a(i) Xi` . By finding low-rank approximations of Ci,j we can identify the
subspaces of Vi and Vj that are relevant for extracting linear patterns between Xi and
Xj . Let X1 represent the hub language corpus matrix. The LSI approach to finding the
subspaces is to perform the singular value decomposition on the full N  N covariance
matrix composed of blocks Ci,j . If |a(i, j)| is small for many language pairs (as it is in the
case of Wikipedia), then many empirical estimates Ci,j are unreliable, which can result in
overfitting. For this reason, we perform the truncated singular value decomposition
on the
Pm
matrix C = [C1,2    C1,m ]  U SV T , where U  Rn1 k , S  Rkk , V  R( i=2 ni )k . We
split the matrix V vertically in blocks with n2 , . . . , nm rows: V = [V2T    VmT ]T . Note that
columns of U are orthogonal but columns in each Vi are not (columns of V are orthogonal).
Let V1 := U . We proceed by reducing the dimensionality of each Xi by setting: Yi = ViT Xi ,
where Yi  RkN . To summarize, the first step reduces the dimensionality of the data and
is based on CL-LSI, but optimizes only the hub language related cross-covariance blocks.
P

4.7.3 Step 2: Simplifying and Solving SSCOR.
The second step involves solving a generalized version of canonical correlation analysis on the
matrices Yi in order to find the mappings Pi . The approach is based on the sum of squares of
correlations formulation by Kettenring (1971), where we consider only correlations between
pairs (Y1 , Yi ), i > 1 due to the hub language problem characteristic. We will present the
original unconstrained optimization problem, then a constrained formulation based on the
hub language problem characteristic. Then we will simplify the constraints and reformulate
the problem as an eigenvalue problem by using Lagrange multipliers.
The original sum of squared correlations is formulated as an unconstrained problem:
maximize
wi Rk

m
X

(wiT Yi , wjT Yj )2 .

i<j

We solve a similar problem by restricting i = 1 and omitting the optimization over non-hub
language pairs. Let Di,i  Rkk denote the empirical covariance of Yi and Di,j denote the
empirical cross-covariance computed based on Yi and Yj . We solve the following constrained
(unit variance constraints) optimization problem:
maximize
wi

Rk

m
X

w1T D1,i wi

2

subject to wiT Di,i wi = 1,

i = 1, . . . , m.

(1)

i=2

The constraints wiT Di,i wi can be simplified by using the Cholesky decomposition Di,i =
KiT  Ki and substitution: yi := Ki wi . By inverting the Ki matrices and defining Gi :=
K1T D1,i Ki1 , the problem can be reformulated:
maximize
yi Rk

m
X

y1T Gi yi

2

subject to yiT yi = 1,

i=2

299

i = 1, . . . , m.

(2)

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

A necessary condition for optimality is that the derivatives of the Lagrangian vanish. The
Lagrangian of (2) is expressed as:
L(y1 , . . . , ym , 1 , . . . , m ) =

m
X

y1T Gi yi

2

+

i=2

m
X


i yiT yi  1 .

i=1

Stationarity conditions give us:
m

X


L=0
y1T Gi yi Gi yi + 1 y1 = 0,
y1

(3)



L = 0  y1T Gi yi GTi y1 + i yi = 0, i > 1.
yi

(4)

i=2

Multiplying the equations (4) with yiT and applying the constraints, we can eliminate i
which gives us:

GTi y1 = y1T Gi yi yi , i > 1.
(5)
Plugging this into (3), we obtain an eigenvalue problem:
!
m
X
Gi GTi y1 + 1 y1 = 0.
i=2

The eigenvectors of

Pm


T

i=2 Gi Gi

for yi are obtained from (5): yi :=
wi :=

Ki1 yi .

solve the problem for the first language. The solutions
GT
i y1
.
kGT
i y1 k

Note that the solution (1) can be recovered by:

The linear transformation of the w variables are thus expressed as:
Y1 := eigenvectors of

m
X

Gi GTi ,

i=2

where N is a diagonal matrix that

W1 = K11 Y1
Wi = Ki1 GTi Y1 N,
normalizes GTi Y1 , with

N (j, j) :=

1
kG(i Y1 (:,j)k .

4.7.4 Remark
The technique is related to Generalization of Canonical Correlation Analysis (GCCA) by
Carroll (1968), where an unknown group configuration variable is defined and the objective
is to maximize the sum of squared correlations between the group variable and the others.
The problem can be reformulated as an eigenvalue problem. The difference lies in the fact
that we set the unknown group configuration variable as the hub language, which simplifies
the solution. The complexity of our method is O(k 3 ), where k is the reduced dimension
from the LSI preprocessing step, whereas solving the GCCA method scales as O(s3 ), where
s is the number of samples (see Gifi, 1990). Another issue with GCCA is that it cannot be
directly applied to the case of missing documents.
To summarize, we first reduced the dimensionality of our data to k-dimensional features
and then found a new representation (via linear transformation) that maximizes directions
of linear dependence between the languages. The final projections that enable mappings to
a common space are defined as: Pi (x) = WiT ViT x.
300

fiCross-Lingual Document Similarity and Event Tracking

English articles

Spanish articles

candidate
clusters

Figure 6: Clusters composed of English and Spanish news articles. Arrows link English
articles with their Spanish k-nearest neighbor matches according to the crosslingual similarity.

5. Cross-Lingual Event Linking
The main application on which we test the cross-lingual similarity is cross-lingual event
linking. In online media streams  particularly news articles  there is often duplication of
reporting, different viewpoints or opinions, all centering around a single event. The same
events are covered by many articles and the question we address is how to find all the
articles in different languages that are describing a single event. In this paper we consider
the problem of matching events from different languages. We do not address the problem
of detection of events but instead base our evaluation on an online system for detection
of world events, Event Registry. The events are represented by clusters of articles and so
ultimately our problem reduces to finding suitable matchings between clusters with articles
in different languages.
5.1 Problem Definition
The problem of cross-lingual event linking is to match monolingual clusters of news articles
that describe the same event across languages. For example, we want to match a cluster
of Spanish news articles and a cluster of English news articles that both describe the same
earthquake.
Each article a  A is written in a language `, where `  L = {`1 , `2 , ..., `m }. For
each language `, we obtain a set of monolingual clusters C` . More precisely, the articles
corresponding to each cluster c  C` are written in the language `. Given a pair of languages
`a  L, `b  L and `a 6= `b , we would like to identify all cluster pairs (ci , cj )  C`a  C`b
such that ci and cj describe the same event.
Matching of clusters is a generalized matching problem. We cannot assume that there
is only one cluster per language per event, nor can we assume complete coverage  i.e., that
there exists at least one cluster per event in every language. This is partly due to news
coverage which might be more granular in some languages, partly due to noise and errors in
301

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

the event detection process. This implies that we cannot make assumptions on the matching
(e.g., one-to-one or complete matching) and excludes the use of standard weighted bipartite
matching type of algorithms for this problem. An example is shown in Figure 6, where
a cluster may contain articles which are closely matched with many clusters in a different
language.
We also seek an algorithm which does not do exhaustive comparison of all clusters,
since that can become prohibitively expensive when working in a real-time setting. More
specifically, we wish to avoid testing cluster ci with all the clusters from all the other
languages. Performing exhaustive comparison would result in O(|C|2 ) tests, where |C| is
the number of all clusters (over all languages), which is not feasible when the number of
clusters is on the order of tens of thousands. We address this by testing only clusters
that are connected with at least one k-nearest neighbor (marked as candidate clusters in
Figure 6).
5.2 Algorithm
In order to identify clusters that are equivalent to cluster ci , we have developed a two-stage
algorithm. For a cluster ci , we first efficiently identify a small set of candidate clusters and
then find those clusters among the candidates, which are equivalent to ci . An example is
shown in Figure 6.
The details of the first step are described in Algorithm 1. The algorithm begins by
individually inspecting each news article ai in the cluster ci . Using a chosen method for
computing cross-lingual document similarity (see Section 4.2), it identifies the 10 most
similar news articles to ai in each language `  L. For each similar article aj , we identify its
corresponding cluster cj and add it to the set of candidates. The set of candidate clusters
obtained in this way is several orders of magnitude smaller than the number of all clusters,
and at most linear with respect to the number of news articles in cluster ci . In practice,
clusters contain highly related articles and as such similar articles from other languages
mostly fall in only a few candidate clusters.
Although computed document similarities are approximate, our assumption is that articles in different languages describing the same event will generally have a higher similarity
than articles about different events. While this assumption does not always hold, redundancy in the data mitigates these false positives. Since we compute the 10 most similar
articles for each article in ci , we are likely to identify all the relevant candidates for cluster
ci .
The second stage of the algorithm determines which (if any) of the candidate clusters are
equivalent to ci . We treat this task as a supervised learning problem. For each candidate
cluster cj  C, we compute a vector of learning features that should be indicative of whether
ci and cj are equivalent or not and apply a binary classification model that predicts if the
clusters are equivalent or not. The classification algorithm that we used to train a model
was a linear Support Vector Machine (SVM) method (Shawe-Taylor & Cristianini, 2004).
We use three groups of features to describe cluster pair (ci , cj ). The first group is based
on cross-lingual article links, which are derived using cross-lingual similarity: each news
article ai is linked with its 10-nearest neighbors articles from all other languages (10 per
each language). The group contains the following features:
302

fiCross-Lingual Document Similarity and Event Tracking

input: test cluster ci , a set of clusters C` for each language `  L
output: a set of clusters C that are potentially equivalent to ci
C  {};
for article ai  ci do
for language `  L do
/* use hub CCA to find 10 most similar articles to article ai in
language `
*/
SimilarArticles = getCCASimilarArticles(ai , `);
for article aj  SimilarArticles do
/* find cluster cj to which article aj is assigned to
*/
cj  c, such that c  C` and aj  c;
/* add cluster cj to the set of candidates C
*/
C  C  {cj };
end
end
end
Algorithm 1: Algorithm for identifying candidate clusters C that are potentially equivalent to ci
 linkCount is the number of times any of the news articles from cj is among 10-nearest
neighbors for articles from ci . In other words, it is the number of times an article from
ci has a very similar article (i.e., is among 10 most similar) in cj .
 avgSimScore is the average similarity score of the links, as identified for linkCount,
between the two clusters.
The second group are concept-related features. Articles that are imported into Event
Registry are annotated by disambiguating mentioned entities and keywords to the corresponding Wikipedia pages (Zhang & Rettinger, 2014b). Whenever Barack Obama is, for
example, mentioned in the article, the article is annotated with a link to his Wikipedia page.
In the same way, all mentions of entities (people, locations, organizations) and ordinary keywords (e.g., bank, tax, ebola, plane, company) are annotated. Although the Spanish article
about Obama will be annotated with his Spanish version of the Wikipedia page, in many
cases we can link the Wikipedia pages to their English versions. This can be done since
Wikipedia itself provides information regarding which pages in different languages represent
the same concept/entity. Using this approach, the word avion in a Spanish article will
be annotated with the same concept as the word plane in an English article. Although
the articles are in different languages, the annotations can therefore provide a languageindependent vocabulary that can be used to compare articles/clusters. By analyzing all
the articles in clusters ci and cj , we can identify the most relevant entities and keywords
for each cluster. Additionally, we can also assign weights to the concepts based on how
frequently they occur in the articles in the cluster. From the list of relevant concepts and
corresponding weights, we consider the following features:
 entityCosSim is the cosine similarity between vectors of entities from clusters ci and
cj .
303

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

 keywordCosSim is the cosine similarity between vectors of keywords from clusters ci
and cj .
 entityJaccardSim is Jaccard similarity coefficient (Levandowsky & Winter, 1971)
between sets of entities from clusters ci and cj .
 keywordJaccardSim is Jaccard similarity coefficient between sets of keywords from
clusters ci and cj .
The last group of features contains three miscellaneous features that seem discriminative but are unrelated to the previous two groups:
 hasSameLocation feature is a boolean variable that is true when the location of the
event in both clusters is the same. The location of events is estimated by considering
the locations mentioned in the articles that form a cluster and is provided by Event
Registry.
 timeDiff is the absolute difference in hours between the two events. The publication
time and date of the events is computed as the average publication time and date of
all the articles and is provided by Event Registry.
 sharedDates is determined as the Jaccard similarity coefficient between sets of date
mentions extracted from articles. We use extracted mentions of dates provided by
Event Registry, which uses an extensive set of regular expressions to detect and normalize mentions of dates in different forms.

6. Evaluation
We will describe the main dataset for building cross-lingual models which is based on
Wikipedia and then present two sets of experiments. The first set of experiments establishes
that the hub based approach can deal with language pairs where little or no training data is
available. The second set of experiments compares the main approaches that we presented
on the task of mate retrieval and the task of event linking. In the mate retrieval task we
are given a test set of document pairs, where each pair consists of a document and its
translation. Given a query document from the test set, the goal is to retrieve its translation
in the other language, which is also referred to as its mate document. Finally, we examine
how different choices of features impact the event linking performance.
6.1 Wikipedia Comparable Corpus
To investigate the empirical performance of the low-rank approximations we will test the algorithms on a large-scale, real-world multilingual dataset that we extracted from Wikipedia
by using inter-language links for alignment. This results in a large number of weakly comparable documents in more than 200 languages. Wikipedia is a large source of multilingual
data that is especially important for the languages for which no translation tools, multilingual dictionaries as Eurovoc (Rodrguez et al., 2008), or strongly aligned multilingual
corpora as Europarl (Koehn, 2005) are available. Documents in different languages are
related with inter-language links that can be found on the left of the Wikipedia page. The
304

fiCross-Lingual Document Similarity and Event Tracking

Wikipedia is constantly growing. There are currently 12 Wikipedias with more than 1 million articles, 52 with more than 100k articles, 129 with more than 10k articles, and 236 with
more than 1, 000 articles.
Each Wikipedia page is embedded in the page tag. First, we check if the title of the
page starts with a Wikipedia namespace (which includes categories and discussion pages)
and do not process the page if it does. Then, we check if this is a redirection page and we
store the redirect link because inter-language links can point to redirection links also. If
none of the above applies, we extract the text and parse the Wikipedia markup. Currently,
all the markup is removed.
We get inter-language link matrix using previously stored redirection links and interlanguage links. If an inter-language link points to the redirection we replace it with the
redirection target link. It turns out that we obtain the matrix M that is not symmetric,
consequently the underlying graph is not symmetric. That means that existence of the
inter-language link in one way (i.e., English to German) does not guarantee that there is
an inter-language link in the reverse direction (German to English). To correct this we
transform this matrix to be symmetric by computing M + M T and obtaining an undirected
graph. In the rare case that after symmetrization we have multiple links pointing from the
document, we pick the first one that we encountered. This matrix enables us to build an
alignment across all Wikipedia2 languages.
6.2 Experiments With Missing Alignment Data
In this subsection, we will investigate the empirical performance of hub CCA approach.
We will demonstrate that this approach can be successfully applied even in the case of
fully missing alignment information. To this purpose, we select a subset of Wikipedia
languages containing three major languages, English (4,212k articles)en (hub language),
Spanish (9,686k articles)es, Russian (9,662k articles)ru, and five minority (in terms of
Wikipedia sizes) languages, Slovenian (136k articles)sl, Piedmontese (59k articles)pms,
Waray-Waray (112k articles)war (all with about 2 million native speakers), Creole (54k
articles)ht (8 million native speakers), and Hindi (97k articles)hi (180 million native
speakers). For preprocessing, we remove the documents that contain less than 20 different
words (referred to as stubs3 ) and remove words occurring in less than 50 documents as
well as the top 100 most frequent words (in each language separately). We represent the
documents as normalized TFIDF (Salton & Buckley, 1988) weighted vectors. The IDF
scores are computed for each language based on its aligned documents with the English
Wikipedia. The English language IDF scores are based on all English documents for which
aligned Spanish documents exist.
The evaluation is based on splitting the data into training and test sets. We select the
test set documents as all multilingual documents with at least one nonempty alignment
from the list: (hi, ht), (hi, pms), (war, ht), (war, pms). This guarantees that we cover
all the languages. Moreover this test set is suitable for testing the retrieval through the
hub as the chosen pairs have empty alignments. The remaining documents are used for
2. The dataset is based on Wikipedia dumps available in 2013.
3. Such documents are typically of low value as a linguistic resource. Examples include the titles of the
columns in the table, remains of the parsing process, or Wikipedia articles with very little or no information contained in one or two sentences.

305

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

training. In Table 1, we display the corresponding sizes of training and test documents for
each language pair.
On the training set, we perform the two step procedure to obtain the common document representation as a set of mappings Pi . A test set for each language pair, testi,j =
{(x` , y` )|` = 1 : n(i, j)}, consists of comparable document pairs (linked Wikipedia pages),
where n(i, j) is the test set size. We evaluate the representation by measuring mate retrieval
quality on the test sets: for each `, we rank the projected documents Pj (y1 ), . . . , Pj (yn(i,j) )
according to their similarity with Pi (x` ) and compute the rank of the mate document
r(`) = rank(P
j (y` )). The final
 retrieval score (between -100 and 100) is computed as:
Pn(i,j)  n(i,j)r(`)
100
`=1
n(i,j) 
n(i,j)1  0.5 . A score that is less than 0 means that the method performs worse than random retrieval and a score of 100 indicates perfect mate retrieval. The
mate retrieval results are included in Table 2.
We observe that the method performs well on all pairs of languages, where at least 50,000
training documents are available(en, es, ru, sl ). We note that taking k = 500 or k = 1, 000
multilingual topics usually results in similar performance, with some notable exceptions: in
the case of (ht, war ) the additional topics result in an increase in performance, as opposed
to (ht, pms) where performance drops, which suggests overfitting. The languages where
the method performs poorly are ht and war, which can be explained by the quality of
data (see Table 3 and explanation that follows). In case of pms, we demonstrate that solid
performance can be achieved for language pairs (pms, sl ) and (pms, hi ), where only 2,000
training documents are shared between pms and sl and no training documents are available
between pms and hi. Also observe that in the case of (pms, ht) the method still obtains a
score of 62, even though training set intersection is zero and ht data is corrupted, which we
will show in the next paragraph.

Table 1: Training  test sizes (in thousands). The first row represents the size of the training sets used to construct the mappings in low-dimensional language independent
space using en as a hub. The diagonal elements represent the number of the unique
training documents and test documents in each language.
en
es
ru
sl
en 671 - 4.6 463 - 4.3 369 - 3.2 50.3 - 2.0
es
463 - 4.3 187 - 2.9 28.2 - 2.0
ru
369 - 3.2 29.6 - 1.9
sl
50.3 - 2
hi
war
ht
pms

hi
14.4 - 2.8
8.7 - 2.5
9.2 - 2.7
3.8 - 1.6
14.4 - 2.8

war
8.58 - 2.4
6.9 - 2.4
2.9 - 1.1
1.2 - 0.99
0.58 - 0.8
8.6 - 2.4

ht
17 - 2.3
13.2 - 2
3.2 - 2.2
0.95 - 1.2
0.0 - 2.1
0.04 - 0.5
17 - 2.3

pms
16.6 - 2.7
13.8 - 2.6
10.2 - 1.3
1.8 - 1.0
0.0 - 0.8
0.0 - 2.0
0.0 - 0.4
16.6 - 2.7

We further inspect the properties of the training sets by roughly estimating the fraction
rank(A)
min(rows(A), cols(A)) for each English training matrix and its corresponding mate matrix,
where rows(A) and cols(A) denote the number of rows and columns respectively. The
denominator represents the theoretically highest possible rank the matrix A could have.
306

fiCross-Lingual Document Similarity and Event Tracking

Table 2: Pairwise retrieval, 500 topics on the left  1,000
en
es
ru
sl
hi
war
en
98 - 98 95 - 97 97 - 98 82 - 84 76 - 74
es 97 - 98
94 - 96 97 - 98 85 - 84 76 - 77
ru 96 - 97 94 - 95
97 - 97 81 - 82 73 - 74
sl 96 - 97 95 - 95 95 - 95
91 - 91 68 - 68
hi 81 - 82 82 - 81 80 - 80 91 - 91
68 - 67
war 68 - 63 71 - 68 72 - 71 68 - 68 66 - 62
ht 52 - 58 63 - 66 66 - 62 61 - 71 44 - 55 16 - 50
pms 95 - 96 96 - 96 94 - 94 93 - 93 85 - 85 23 - 26

topics on the right
ht
pms
53 - 55 96 - 97
56 - 57 96 - 96
55 - 56 96 - 96
59 - 69 93 - 93
50 - 55 87 - 86
28 - 48 24 - 21
62 - 49
66 - 54

Ideally, these two fractions should be approximately the same - both aligned spaces should
have reasonably similar dimensionality. We display these numbers as pairs in Table 3.

Table 3: Dimensionality drift. Each column corresponds to a pair of aligned corpus matrices
between English and another language. The numbers represent the ratio between
the numerical rank and the highest possible rank. For example, the column en 
ht tells us that for the English-Creole pairwise-aligned corpus matrix pair, the
English counterpart has full rank, but the Creole counterpart is far having full
rank.
en  es
0.81  0.89

en  ru
0.8  0.89

en  sl
0.98  0.96

en  hi
11

en  war
0.74  0.56

en  ht
1  0.22

en  pms
0.89  0.38

It is clear that in the case of the Creole language only at most 22% documents are
unique and suitable for the training. Though we removed the stub documents, many of the
remaining documents are nearly the same, as the quality of some smaller Wikipedias is low.
This was confirmed for the Creole, Waray-Waray, and Piedmontese languages by manual
inspection. The low quality documents correspond to templates about the year, person,
town, etc. and contain very few unique words.
There is also a problem with the quality of the test data. For example, if we look at
the test pair (war, ht) only 386/534 Waray-Waray test documents are unique but on the
other side almost all Creole test documents (523/534) are unique. This indicates a poor
alignment which leads to poor performance.
6.3 Evaluation of Cross-Lingual Event Linking
In order to determine how accurately we can predict cluster equivalence, we performed two
experiments in a multilingual setting using English, German and Spanish languages for
which we had labelled data to evaluate the linking performance. In the first experiment,
we tested how well the individual approaches for cross-lingual article linking perform when
used for linking the clusters about the same event. In the second experiment we tested how
307

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

accurate the prediction model is when trained on different subsets of learning features. To
evaluate the prediction accuracy for a given dataset we used 10-fold cross validation.
We created a manually labelled dataset in order to evaluate cross-lingual event linking
using two human annotators. The annotators were provided with an interface listing the
articles, their content and top concepts for a pair of clusters. Their task was to determine
if the clusters were equivalent or not (i.e., discuss same event). To obtain a pair of clusters
(ci , cj ) to annotate, we first randomly chose a cluster ci , used Algorithm 1 to compute a
set of potentially equivalent clusters C and randomly chose a cluster cj  C. The dataset
provided by the annotators contains 808 examples, of which 402 are equivalent clusters
pairs and 406 are not. Clusters in each learning example are either in English, Spanish or
German. Although Event Registry imports articles in other languages as well, we restricted
our experiments to these three languages. We chose only these three languages since they
have very large number of articles and clusters per day which makes the cluster linking
problem hard due to large number of possible links.
In Section 4.2, we described three main algorithms for identifying similar articles in
different languages. These algorithms were k-means, LSI and hub CCA. As a training
set, we used common Wikipedia alignment for all three languages. To test which of these
algorithms performed best, we made the following test. For each of the three algorithms,
we analyzed all articles in Event Registry and for each article computed the most similar
articles in other languages. To test how informative the identified similar articles are for
cluster linking we then trained three classifiers as described in Section 5.2  one for each
algorithm. Each classifier was allowed to use as learning features only the cross-lingual
article linking features for which values are determined based on the selected algorithm
(k-means, LSI and hub CCA). The results of the trained models are shown in Table 4.
We also show how the number of topics (the dimensions of the latent space) influences the
quality, except in the case of the k-means algorithm, where only the performance on 500
topic vectors is reported, due to higher computational cost.
We observe that, for the task of cluster linking, LSI and hub CCA perform comparably
and both outperform k-means.
We also compared the proposed approaches on the task of Wikipedia mate retrieval
(the same task as in Section 6.2). We computed the Average (over language pairs) Mean
Reciprocal Rank (AMRR) (Voorhees et al., 1999) performance of the different approaches
on the Wikipedia data by holding out 15, 000 aligned test documents and using 300, 000
aligned documents as the training set. Figure 7 shows AMRR score as the function of
the number of feature vectors. It is clear that hub CCA outperforms LSI approach and
k-means lags far behind when testing on Wikipedia data. The hub CCA approach with 500
topic vectors manages to perform comparably to the LSI-based approach with 1, 000 topic
vectors, which shows that the CCA method can improve both model memory footprint as
well as similarity computation time.
Furthermore, we inspected how the number of topics influences the accuracy of cluster
linking. As we can see from Table 4 choosing a number of features larger than 500 barely
affects linking performance, which is in contrast with the fact that additional topics helped
to improve AMMR, see Figure 7. Such differences may have arisen due to different domains
of training and testing (Wikipedia pages versus news articles).
308

fiCross-Lingual Document Similarity and Event Tracking

0.9
0.8

AMRR

0.7
0.6
0.5
Hub CCA
LSI
k-means

0.4
0.3
0.2
100

200

300

400
500
600
700
Number of feature vectors

800

900

1,000

Figure 7: Average of mean reciprocal ranks

We also analyzed how cluster size influences the accuracy of cluster linking. Our intuition
was that linking large cluster pairs is easier than linking clusters with few articles. The
reasoning is that the large clusters would provide more document linking information (more
articles mean more links to other similar articles) as well as more accurately aggregated
semantic information. In the case of smaller clusters, the errors of the similarity models
have greater impact which should decrease the performance of the classifier, too. To validate
this hypothesis we have split the learning examples into two datasets  one containing cluster
pairs where the combined number of articles from both clusters is below 20 and one dataset
where the combined number is 20 or more. The results of the experiment can be seen in
Table 5. As it can be seen, the results confirm our expectations: for smaller clusters it is
indeed harder to correctly predict if the cluster pair should be merged or not.
The hub CCA attains higher precision and classification accuracy on the task of linking
small cluster pairs than the other methods, while LSI is slightly better on linking large
cluster pairs. The gain in precision of LSI over hub CCA on linking large clusters is much
smaller than the gain in precision of hub CCA over LSI on linking small clusters. For that
reason we decided to use hub CCA as the similarity computation component in our system.
In the second experiment, we evaluate how relevant individual groups of features are
to correctly determine cluster equivalence. For this purpose, we computed accuracy using
individual groups of features, as well as using different combination of groups. Since hub
CCA had the best performance of the three algorithms, we used it to compute the values of
the cross-lingual article linking features. The results of the evaluation are shown in Table 6.
We can see that using a single group of features, the highest prediction accuracy can be
achieved using concept-related features. The classification accuracy in this case is 88.5%.
By additionally including also the cross-lingual article linking features, the classification
309

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

Table 4: Accuracy of cluster linking with 500/800/1,000 topic vectors obtained from different cross-lingual similarity algorithms. The table shows for each of the algorithms
the obtained classification accuracy, precision and recall.
Models
hub CCA
LSI
k-means

Accuracy %
78.2/79.6/80.3
78.9/78.7/80.6
73.9/-/-

Precision %
76.3/78.0/80.5
76.8/77.0/78.7
69.5/-/-

Recall %
81.6/82.1/79.9
83.3/80.6/83.6
84.6/-/-

F1 %
78.9/80.0/80.2
79.9/78.8/81.1
76.3/-/-

Table 5: Accuracy of cluster linking using 500 topic vectors on two datasets containing
large (left number) and small (right number) clusters. The dataset with small
clusters contained the subset of learning examples in which the combined number
of articles from both clusters of the cluster pair were below 20. The remaining
learning examples were put into the dataset of large clusters.
Models
hub CCA
LSI
k-means

Accuracy %
81.2 - 77.8
82.8 - 76.4
75.5 - 71.2

Precision %
80.5 - 74.5
81.3 - 70.9
72.8 - 70.8

Recall %
91.3 - 57.5
93.1 - 57.5
95.3 - 36.2

F1 %
85.6 - 64.9
86.8 - 63.5
82.5 - 47.9

accuracy rises slightly to 89.4%. Using all three groups of features, the achieved accuracy
is 89.2%.
To test if the accuracy of the predictions is language dependent we have also performed
the evaluations separately on individual language pairs. For this experiment we have split
the annotated learning examples into three datasets, where each dataset contained only
examples for one language pair. When training the classifier all three groups of features
were available. The results are shown in Table 7. We can see that the performance of cluster
linking on the English-German dataset is the highest in terms of accuracy, precision, recall
and F1 . The performance on the English-Spanish dataset is comparable to the performance
on the English-German dataset, where the former achieves higher recall (and slightly higher
F1 score), while the latter achieves higher precision. A possible explanation of these results is
that the higher quantity and quality of English-German language resources leads to a more
accurate cross-lingual article similarity measure as well as to a more extensive semantic
annotation of the articles.
Based on the performed experiments, we can make the following conclusions. The
cross-lingual similarity algorithms provide valuable information that can be used to identify
clusters that describe the same event in different languages. For the task of cluster linking,
the cross-lingual article linking features are however significantly less informative compared
to the concept-related features that are extracted from the semantic annotations. Never310

fiCross-Lingual Document Similarity and Event Tracking

theless, the cross-lingual article similarity features are very important for two reasons. The
first is that they allow us to identify for a given cluster a limited set of candidate clusters
that are potentially equivalent. This is a very important feature since it reduces the search
space by several orders of magnitude. The second reason these features are important is
that concept annotations are not available for all articles as the annotation of news articles
is computationally intensive and can only be done for a subset of collected articles. The
prediction accuracies for individual language pairs are comparable although it seems that
the achievable accuracy correlates with the amount of available language resources.

Table 6: The accuracy of the classifier for story linking using different sets of learning
features.
Features
hub CCA
Concepts
Misc
hub CCA + Concepts
hub CCA + Misc
Concepts + Misc
All

Accuracy %
78.3  5.9
88.5  2.7
54.8  6.7
89.4  2.5
78.8  5.0
88.7  2.6
89.2  2.6

Precision %
78.2  7.0
88.6  4.8
61.8  16.5
89.4  4.6
78.9  7.1
88.8  4.6
88.8  4.9

Recall %
78.9  5.2
88.6  2.2
58.2  30.2
89.6  2.4
79.4  4.6
88.8  2.2
90.1  1.9

F1 %
78.4  5.5
88.5  2.4
52.4  13.0
89.4  2.3
79.0  4.5
88.7  2.3
89.3  2.3

Table 7: The accuracy of the classifier for story linking on training data for each language
pair separately using all learning features.
Language pair
en, de
en, es
es, de

Accuracy %
91.8  5.5
87.7  5.4
88.6  4.3

Precision %
91.7  6.3
87.7  7.4
89.7  9.1

Recall %
93.7  6.3
88.5  9.8
84.3  11.9

F1 %
92.5  5.1
87.6  5.9
85.9  6.0

6.4 Remarks on the Scalability of the Implementation
One of the main advantages of our approach is that it is highly scalable. It is fast, very
robust to quality of training data, easily extendable, simple to implement and has relatively
small hardware requirements. The similarity pipeline is the most computationally intensive
part and currently runs on a machine with two Intel Xeon E5-2667 v2, 3.30GHz processors
with 256GB of RAM. This is sufficient to do similarity computation over a large number of
languages if needed. It currently uses Wikipedia as a freely available knowledge base and
experiments show that the similarity pipeline dramatically reduces the search space when
linking clusters.
311

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

Currently, we compute similarities over 24 languages with tags: eng, spa, deu, zho, ita,
fra, rus, swe, nld, tur, jpn, por, ara, fin, ron, kor, hrv, tam, hun, slv, pol, srp, cat, ukr but we
support any language from the top 100 Wikipedia languages. Our data stream is Newsfeed
(http://newsfeed.ijs.si/) which provides 430k unique articles per day. Our system currently
computes 2 million similarities per second, that means that we compute 16  1010 similarities
per day. We store one day buffer for each language which requires 1.5 GB of memory with
documents stored as 500-dimensional vectors. We note that the time complexity of the
similarity computations scales linearly with dimension of the feature space and does not
depend on number of languages. For each article, we compute the top 10 most similar ones
in every other language.
For all linear algebra matrix and vector operations, we use high performance numerical
linear algebra libraries as BLAS, OPENBLAS and Intel MKL, which currently allows us to
process more than one million articles per day. In our current implementation, we use the
variation of the hub approach. Our projector matrices are of size 500  300, 000, so every
projector takes about 1.1 GB of RAM. Moreover, we need proxy matrices of size 500  500
for every language pair. That is 0.5 GB for 24 languages and 9.2 GB for 100 languages.
All together we need around 135 GB of RAM for the system with 100 languages. Usage
of proxy matrices enables the projection of all input documents in the common space and
handling language pairs with missing or low alignment. That enables us to do block-wise
similarity computations further improving system efficiency. Our code can therefore be
easily parallelized using matrix multiplication rather than performing more matrix - vector
multiplications. This speeds up our code by a factor of around 4. In this way, we obtain
some caching gains and ability to use vectorization. Our system is also easily extendable.
Adding a new language requires the computation of a projector matrix and proxy matrices
with all other already available languages.
6.5 Remarks on the Reproducibility of Experiments
We have made both the code and data that were used in the experiments publicly available
at https://github.com/rupnikj/jair_paper.git. The manually labelled dataset used
in the evaluation of event linking is available at in the dataset subfolder of the github
repository. The included archive contains two folders: positive and negative, where
the first folder includes examples of cluster pairs in two languages that represent the same
event and the second folder contains pairs of clusters in two languages that do not represent
different events. Each example is a JSON file that contains at the top level information
about a pair of clusters (including text of the articles) as well as a set of meta attributes,
that correspond to features described in Section 5.2.
The code folder includes MATLAB scripts for building cross-lingual similarity models
introduced in 4.2, which can be used with publicly available Wikipedia corpus to reproduce the cross-lingual similarity evaluation. We have also made available the similarity
computation over 100 languages as a service at xling.ijs.si.
In addition, the Event Registry system (http://eventregistry.org/) comes with an
API, documented at https://github.com/gregorleban/event-registry-python, that
can be used to download events and articles.
312

fiCross-Lingual Document Similarity and Event Tracking

7. Discussion and Future Work
In this paper we have presented a cross-lingual system for linking events in different languages. Building on an existing system, Event Registry, we present and evaluate several
approaches to compute a cross-lingual similarity function. We also present an approach to
link events and evaluate effectiveness of various features. The final pipeline is scalable both
in terms of number of articles and number of languages, while accurately linking events.
On the task of mate retrieval, we observe that refining the LSI-based projections with
hub CCA leads to improved retrieval precision, but the methods perform comparably on
the task of event linking. Further inspection showed that the CCA-based approach reached
a higher precision on smaller clusters. The interpretation is that the linking features are
highly aggregated for large clusters, which compensates the lower per-document precision
of LSI. Another possible reason is that the advantage that we show on Wikipedia is lost on
the news domain. This hypothesis could be validated by testing the approach on documents
from a different domain.
The experiments show that the hub CCA-based features present a good baseline, which
can greatly benefit from additional semantic-based features. Even though in our experiments the addition of CCA-based features to semantic features did not lead to great performance improvements, there are two important benefits in the approach. First, the linking
process can be sped up by using a smaller set of candidate clusters. Second, the approach
is robust to languages where semantic extraction is not available, due to scarce linguistic
resources.
7.1 Future Work
Currently the system is loosely-coupled  the language component is built independently
from the rest of the system, in particular the linking component. It is possible that better
embeddings can be obtained by methods that jointly optimize a classification task and the
embedding.
Another point of interest is to evaluate the system on languages with scarce linguistic
resources, where semantic annotation might not be available. For this purpose, the labelled
dataset of linked clusters should be extended first. The mate retrieval evaluation showed
that even for language pairs with no training set overlap, the hub CCA recovers some signal.
In order to further improve the performance of the classifier for cluster linking, additional
features should also be extracted from articles and clusters and checked if they can increase
the accuracy of the classification. Since the amount of linguistic resources vary significantly
from language to language it would also make sense to build a separate classifier for each
language pair. Intuitively, this should improve performance since weights of individual
learning features could be adapted to the tested pair of languages.

Acknowledgments
The authors gratefully acknowledge that the funding for this work was provided by the
projects X-LIKE (ICT-257790-STREP), MultilingualWeb (PSP-2009.5.2 Agr.# 250500),
313

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

TransLectures (FP7-ICT-2011-7), PlanetData (ICT-257641-NoE), RENDER (ICT-257790STREP), XLime (FP7-ICT-611346), and META-NET (ICT-249119-NoE).

References
Brank, J., Leban, G., & Grobelnik, M. (2014). A high-performance multithreaded approach
for clustering a stream of documents. In Proceedings of the 17th International Multiconference Information Society 2014, Volume E, Ljubljana, Slovenia, pp. 58.
Carroll, J. D. (1968). Generalization of canonical correlation analysis to three or more sets
of variables. Proceedings of the American Psychological Association, 227228.
Deerwester, S., Dumais, S. T., Landauer, T. K., Furnas, G. W., & Harshman, R. A. (1990).
Indexing by latent semantic analysis. Journal of the American Society for Information
Science, 41(6), 391407.
Dumais, S., Letsche, T., Littman, M., & Landauer, T. (1997). Automatic Cross-Language
Retrieval Using Latent Semantic Indexing. In AAAI spring symposium on crosslanguage text and speech retrieval. American Association for Artificial Intelligence,
vol. 16. 1997, p. 21.
Fortuna, B., Cristianini, N., & Shawe-Taylor, J. (2006). Kernel methods in bioengineering,
communications and image processing, chap. A Kernel Canonical Correlation Analysis
For Learning The Semantics Of Text, pp. 263282. Idea Group Publishing.
Gifi, A. (1990). Nonlinear Multivariate Analysis. Wiley Series in Probability and Statistics.
Golub, G. H., & Van Loan, C. F. (2012). Matrix computations, Vol. 3. Johns Hopkins
University Press.
Halko, N., Martinsson, P. G., & Tropp, J. A. (2011). Finding Structure with Randomness:
Probabilistic Algorithms for Constructing Approximate Matrix Decompositions. Society for Industrial and Applied Mathematics Review, 53 (2), 217288.
Hardoon, D. R., Mourao-Miranda, J., Brammer, M., & Shawe-Taylor, J. (2008). Using
image stimuli to drive fMRI analysis. In Neural Information Processing, pp. 477486.
Springer.
Hartigan, J. (1975). Clustering algorithms. John Wiley & Sons Inc, New York.
Hotelling, H. (1935). The most predictable criterion.. Journal of educational Psychology,
26 (2), 139.
Kettenring, J. R. (1971). Canonical analysis of several sets of variables. Biometrika, 58,
43345.
Koehn, P. (2005). Europarl: A Parallel Corpus for Statistical Machine Translation. In The
Tenth Machine Translation Summit, Vol. 5, pp. 7986 Phuket, Thailand.
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan,
B., Shen, W., Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin, A., & Herbst, E.
(2007). Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings
of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration
Sessions, ACL 07, pp. 177180 Stroudsburg, PA, USA. Association for Computational
Linguistics.
314

fiCross-Lingual Document Similarity and Event Tracking

Leban, G., Fortuna, B., Brank, J., & Grobelnik, M. (2014a). Cross-lingual detection of
world events from news articles. In Proceedings of the 13th International Semantic
Web Conference, pp. 2124 Riva del Garda - Trentino, Italy.
Leban, G., Fortuna, B., Brank, J., & Grobelnik, M. (2014b). Event Registry: Learning
About World Events from News. In Proceedings of the Companion Publication of the
23rd International Conference on World Wide Web Companion, WWW Companion
14, pp. 107110 Seoul, Republic of Korea. International World Wide Web Conferences
Steering Committee.
Leetaru, K., & Schrodt, P. A. (2013). GDELT: Global data on events, location, and tone,
19792012. In International Studies Association (ISA) Annual Convention, Vol. 2,
p. 4 San Francisco, California, USA.
Levandowsky, M., & Winter, D. (1971). Distance between Sets. Nature, 234 (5323), 3435.
Milne, D., & Witten, I. H. (2008). Learning to Link with Wikipedia. In Proceedings of the
17th ACM Conference on Information and Knowledge Management, CIKM 08, pp.
509518 New York, NY, USA. ACM.
Mimno, D., Wallach, H. M., Naradowsky, J., Smith, D. A., & McCallum, A. (2009). Polylingual Topic Models. In Proceedings of the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 2 - Volume 2, EMNLP 09, pp. 880889 Singapore. Association for Computational Linguistics.
Muhic, A., Rupnik, J., & Skraba, P. (2012). Cross-lingual document similarity. In Information Technology Interfaces (ITI), Proceedings of the ITI 2012 34th International
Conference on, pp. 387392 Cavtat / Dubrovnik, Croatia. IEEE.
Pearson, K. (1901). On lines and planes of closest fit to systems of points in space. Philosophical Magazine, 2 (6), 559572.
Peters, C., & Braschler, M. (2012). Multilingual Information Retrieval. Springer Berlin
Heidelberg, Berlin, Heidelberg.
Platt, J. C., Toutanova, K., & Yih, W.-t. (2010). Translingual document representations
from discriminative projections. In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pp. 251261 Massachusetts, USA. Association for Computational Linguistics.
Potthast, M., Barron-Cedeno, A., Stein, B., & Rosso, P. (2011). Cross-language Plagiarism
Detection. Language Resources and Evaluation, 45 (1), 4562.
Potthast, M., Stein, B., & Anderka, M. (2008). A Wikipedia-Based Multilingual Retrieval
Model. In Advances in Information Retrieval , 30th European Conference on Information Retrieval Research (ECIR), pp. 522530 Glasgow, UK.
Pouliquen, B., Steinberger, R., & Deguernel, O. (2008). Story tracking: linking similar news
over time and across languages. In Proceedings of the Workshop on Multi-source Multilingual Information Extraction and Summarization, pp. 4956 Manchester, United
Kingdom. Association for Computational Linguistics.
Pouliquen, B., Steinberger, R., & Ignat, C. (2006). Automatic annotation of multilingual
text collections with a conceptual thesaurus. arXiv preprint cs/0609059.
315

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

Rodrguez, J. M. A., Azcona, E. R., & Paredes, L. P. (2008). Promoting Government
Controlled Vocabularies for the Semantic Web: the EUROVOC Thesaurus and the
CPV Product Classification System. Semantic Interoperability in the European Digital
Library, 111.
Rupnik, J., Muhic, A., & Skraba, P. (2011a). Low-rank approximations for large, multilingual data. Low Rank Approximation and Sparse Representation, Neural Information Processing Systems 2011 Workshop.
Rupnik, J., Muhic, A., & Skraba, P. (2011b). Spanning Spaces: Learning Cross-Lingual Similarities. Beyond Mahalanobis: Supervised Large-Scale Learning of Similarity, Neural
Information Processing Systems 2011 Workshop.
Rupnik, J., Muhic, A., & Skraba, P. (2012). Multilingual Document Retrieval through Hub
Languages. In Proceedings of the 15th Multiconference on Information Society 2012
(IS-2012), pp. 201204 Ljubljana, Slovenia.
Salton, G., & Buckley, C. (1988). Term-weighting approaches in automatic text retrieval..
Vol. 24, pp. 513523. Elsevier.
Shawe-Taylor, J., & Cristianini, N. (2004). Kernel Methods for Pattern Analysis. Cambridge
University Press.
Steinberger, R., Pouliquen, B., & Ignat, C. (2005). NewsExplorer: Multilingual News Analysis with Cross-Lingual Linking. Information Technology Interfaces.
Trampus, M., & Novak, B. (2012). The Internals Of An Aggregated Web News Feed.
In Proceedings of 15th Multiconference on Information Society 2012 (IS-2012), pp.
221224 Ljubljana, Slovenia.
Voorhees, E. M., et al. (1999). The TREC-8 Question Answering Track Report. In Proceedings of the 8th Text Retrieval Conference (TREC-8), Vol. 99, pp. 7782 Gaithersburg,
MD, USA.
Xiao, M., & Guo, Y. (2013). A novel two-step method for cross language representation
learning. In Advances in Neural Information Processing Systems, pp. 12591267 Sateline, NV, USA.
Zhang, D., Mei, Q., & Zhai, C. (2010). Cross-lingual latent topic extraction. In Proceedings
of the 48th Annual Meeting of the Association for Computational Linguistics, pp.
11281137 Uppsala, Sweden. Association for Computational Linguistics.
Zhang, L., & Rettinger, A. (2014a). Semantic Annotation, Analysis and Comparison: A
Multilingual and Cross-lingual Text Analytics Toolkit. In Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2014), pp. 1316 Gothenburg, Sweden. Association for
Computational Linguistics.
Zhang, L., & Rettinger, A. (2014b). X-LiSA: Cross-lingual Semantic Annotation. Proceedings of the Very Large Data Bases (VLDB) Endowment, 7 (13), 16931696.

316

fiJournal of Artificial Intelligence Research 55 (2016) 63-93

Submitted 03/15; published 01/16

Cross-Lingual Bridges with Models of Lexical Borrowing
Yulia Tsvetkov
Chris Dyer

ytsvetko@cs.cmu.edu
cdyer@cs.cmu.edu

Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA

Abstract
Linguistic borrowing is the phenomenon of transferring linguistic constructions (lexical,
phonological, morphological, and syntactic) from a donor language to a recipient language as a result of contacts between communities speaking different languages. Borrowed
words are found in all languages, andin contrast to cognate relationshipsborrowing
relationships may exist across unrelated languages (for example, about 40% of Swahilis
vocabulary is borrowed from the unrelated language Arabic). In this work, we develop a
model of morpho-phonological transformations across languages. Its features are based
on universal constraints from Optimality Theory (OT), and we show that compared to
several standardbut linguistically more navebaselines, our OT-inspired model obtains
good performance at predicting donor forms from borrowed forms with only a few dozen
training examples, making this a cost-effective strategy for sharing lexical information
across languages. We demonstrate applications of the lexical borrowing model in machine
translation, using resource-rich donor language to obtain translations of out-of-vocabulary
loanwords in a lower resource language. Our framework obtains substantial improvements
(up to 1.6 BLEU) over standard baselines.

1. Introduction
State-of-the-art natural language processing (NLP) tools, such as text parsing, speech
recognition and synthesis, text and speech translation, semantic analysis and inference, rely
on availability of language-specific data resources that exist only for a few resource-rich
languages. To make NLP tools available in more languages, techniques have been developed
for projecting such resources from resource-rich languages using parallel (translated) data
as a bridge for cross-lingual part-of-speech tagging (Yarowsky, Ngai, & Wicentowski, 2001;
Das & Petrov, 2011; Li, Graa, & Taskar, 2012; Tckstrm, Das, Petrov, McDonald, &
Nivre, 2013), syntactic parsing (Wu, 1997; Kuhn, 2004; Smith & Smith, 2004; Hwa, Resnik,
Weinberg, Cabezas, & Kolak, 2005; Xi & Hwa, 2005; Burkett & Klein, 2008; Snyder, Naseem,
& Barzilay, 2009; Ganchev, Gillenwater, & Taskar, 2009; Tiedemann, 2014), word sense
tagging (Diab & Resnik, 2002), semantic role labeling (Pad & Lapata, 2009; Kozhevnikov &
Titov, 2013), metaphor identification (Tsvetkov, Boytsov, Gershman, Nyberg, & Dyer, 2014),
and others. The limiting reagent in these methods is parallel data. While small parallel
corpora do exist for many languages (Smith, Saint-Amand, Plamada, Koehn, Callison-Burch,
& Lopez, 2013), suitably large parallel corpora are expensive, and these typically exist
only for English and a few other geopolitically or economically important language pairs.
Furthermore, while English is a high-resource language, it is linguistically a typological
outlier in a number of respects (e.g., relatively simple morphology, complex system of verbal
c
2016
AI Access Foundation. All rights reserved.

fiTsvetkov & Dyer



 

pippal

falafel

parpaare

Hebrew

Gawwada

Sanskrit





pilpil

falAfil

pilipili

Persian

Arabic

Swahili

Figure 1: An example of the multilingual borrowing from Sanskrit into typologically diverse, lowand high-resource languages (Haspelmath & Tadmor, 2009).

auxiliaries, large lexicon, etc.), and the assumption of construction-level parallelism that
projection techniques depend on is thus questionable. Given this state of affairs, there is an
urgent need for methods for establishing lexical links across languages that do not rely on
large-scale parallel corpora. Without new strategies, most of the 7,000+ languages in the
worldmany with millions of speakerswill remain resource-poor from the standpoint of
NLP.
We advocate a novel approach to automatically constructing language-specific resources,
even in languages with no resources other than raw text corpora. Our main motivation is
research in linguistic borrowingthe phenomenon of transferring linguistic constructions
(lexical, phonological, morphological, and syntactic) from a donor language to a recipient
language as a result of contacts between communities speaking different languages (Thomason
& Kaufman, 2001). Borrowed words (also called loanwords, e.g., in Figure 1) are lexical
items adopted from another language and integrated (nativized) in the recipient language.
Borrowing occurs typically on the part of minority language speakers, from the language
of wider communication into the minority language (Sankoff, 2002); that is one reason
why donor languages often bridge between resource-rich and resource-limited languages.
Borrowing is a distinctive and pervasive phenomenon: all languages borrowed from other
languages at some point in their lifetime, and borrowed words constitute a large fraction
(1070%) of most language lexicons (Haspelmath, 2009).
Loanword nativization is primarily a phonological process. Donor words undergo phonological repairs to adapt a foreign word to the segmental, phonotactic, suprasegmental and
morpho-phonological constraints of the recipient language (Holden, 1976; Van Coetsem, 1988;
Ahn & Iverson, 2004; Kawahara, 2008; Hock & Joseph, 2009; Calabrese & Wetzels, 2009;
Kang, 2011, inter alia). Common phonological repair strategies include feature/phoneme
epenthesis, elision, degemination, and assimilation. When speakers encounter a foreign
word (either a lemma or an inflected form), they analyze it morphologically as a stem,
and morphological loanword integration thus amounts to selecting an appropriate donor
surface form (out of existing inflections of the same lemma), and applying the recipient
language morphology (Repetti, 2006). Adapted loanwords can freely undergo recipient
language inflectional and derivational processes. Nouns are borrowed preferentially, then
other parts of speech, then affixes, inflections, and phonemes (Whitney, 1881; Moravcsik,
1978; Myers-Scotton, 2002, p. 240).
Although borrowing is pervasive and a topic of enduring interest for historical and
theoretical linguists (Haugen, 1950; Weinreich, 1979), only limited work in computational
64

fiCross-Lingual Bridges with Models of Lexical Borrowing

modeling has addressed this phenomenon. However, it is a topic well-suited to computational
models (e.g., the systematic phonological changes that occur during borrowing can be modeled
using established computational primitives such as finite state transducers), and models
of borrowing have useful applications. Our work can be summarized as the development
of a computational model of lexical borrowing and an exploration of its applications to
augment language resources and computational approaches to NLP in resource-limited
languages. Specifically, we demonstrate how multilingual dictionaries extracted using models
of borrowing improve resource-limited statistical machine translation (MT), using a pivoting
paradigm where the borrowing pair and the translation pair have only a single language in
common.
The problem we address is the identification of plausible donor words (in the donor
language) given a loanword (in the recipient language), and vice versa. For example, given
	
a Swahili loanword safari journey, our model identifies its Arabic donor K
 Q (sfryh)1
journey (3). Although at a high level, this is an instance of the well-known problem of
modeling string transductions, our interest is being able to identify correspondences across
languages with minimal supervision, so as to make the technique applicable in low-resource
settings. To reduce the supervision burden, our model includes awareness of the morphophonological repair strategies that native speakers of a language subconsciously employ to
adapt a loanword to phonological constraints of the recipient language (3.3). To this end,
we use constraint-based theories of phonology, as exemplified by Optimality Theory (OT)
(Prince & Smolensky, 2008; McCarthy, 2009), which non-computational linguistic work has
demonstrated to be particularly well suited to account for phonologically complex borrowing
processes (Kang, 2011). We operationalize OT constraints as features in our borrowing
model (3.4). We conduct a case study on Arabic and Swahili, two phylogenetically unrelated
languages with a long history of contact; we then apply the model to additional language
pairs (3.5). We then employ models of lexical borrowing to obtain cross-lingual bridges
from loanwords in a low-resource language to their donors in a resource-rich language. The
donor language is used as pivot to obtain translations via triangulation of out-of-vocabulary
loanwords (4). We conduct translation experiments with three resource-poor setups:
SwahiliEnglish pivoting via Arabic, MalteseEnglish pivoting via Italic, and Romanian
English2 pivoting via French. In intrinsic evaluation, ArabicSwahili, ItalianMaltese, and
FrenchRomanian borrowing models significantly outperform transliteration and cognate
discovery models (5.1). We then provide a systematic quantitative and qualitative analysis
of contribution of integrated translations, relative to baselines and oracles, and on corpora
of varying sizes (5.2). The proposed pivoting approach yields substantial improvements
(up to +1.6 BLEU) in SwahiliArabicEnglish translation, moderate improvement (up to
+0.8 BLEU) in MalteseItalianEnglish translation, and small (+0.2 BLEU) but statistically
significant improvements in RomanianFrenchEnglish.
Our contributions are twofold. While there have been software implementations of
OT (Hayes, Tesar, & Zuraw, 2013), they have been used chiefly to facilitate linguistic
analysis; we show how to use OT to formulate a model that can be learned with less
supervision than linguistically nave models. To the best of our knowledge, this is the first
1. We use Buckwalter notation to write Arabic glosses.
2. Romanian is not resource-poor from MT perspective, but in this work we simulate a resource-poor
scenario.

65

fiTsvetkov & Dyer

computational model of lexical borrowing used in a downstream NLP task. Second, we
show that lexical correspondences induced using this model can project resourcesnamely,
translationsleading to improved performance in a downstream translation system.3

2. Motivation
The task of modeling borrowing is under-explored in computational linguistics, although
it has both important practical applications and lends itself to modeling with a variety of
established computational techniques. In this section we first situate the task with respect to
two most closely related research directions: modeling transliteration and modeling cognate
forms. We then motivate the new line of research proposed in this work: modeling borrowing.
2.1 Borrowing vs. Transliteration
Borrowing is not transliteration. Transliteration refers to writing in a different orthography,
whereas borrowing refers to expanding a language to include words adapted from another
language. Unlike borrowing, transliteration is more amenable to orthographicrather
than morpho-phonologicalfeatures, although transliteration can also be prone to phonetic
adaptation (Knight & Graehl, 1998). Borrowed words might have begun as transliterations,
but a characteristic of borrowed words is that they become assimilated in the linguistic system
of the recipient language, and became regular content words, for example, orange and
	
sugar are English words borrowed from Arabic l. ' PAK	 (nArnj) and Q@ (Alskr), respectively.
Whatever their historical origins, synchronically, these words are indistinguishable to most
speakers from words that have native ancestral forms in the language. Thus, the morphophonological processes that must be accounted for in borrowing models are more complex
than is required by transliteration models.
2.2 Borrowing vs. Inheritance
Cognates are words in related languages that are inherited from a single word in a common
ancestral language (the proto-language). Loanwords, on the other hand, can occur between
any languages, either related or not, that historically came into contact. From a modeling
perspective, cognates and borrowed words require separate investigation as loanwords are
more likely to display marginal phonotactic (and other phonological) patterns than inherited
lexical items. Theoretical analysis of cognates has tended to be concerned with a diachronic
point of view, that is modeling word changes across time. While of immense scientific
interest, language processing applications are arguably better served by models of synchronic
processes, peculiar to loanword analysis.

3. This article is a thoroughly revised and extended version of the work of Tsvetkov, Ammar, and Dyer
(2015), and Tsvetkov and Dyer (2015). We provide a more detailed linguistic background of lexical
borrowing and OT. We demonstrate results on a new language, Maltese, to emphasize the generality
of the method. Additional extensions include a detailed error analysis and a more complete literature
survey.

66

fiCross-Lingual Bridges with Models of Lexical Borrowing

2.3 Why Borrowing?
Borrowing is a distinctive and pervasive phenomenon: all languages borrowed from other
languages at some point in their lifetime, and borrowed words constitute a large fraction
of most language lexicons. Another important property of borrowing is that in adaptation
of borrowed items, changes in words are systematic, and knowledge of morphological and
phonological patterns in a language can be used to predict how borrowings will be realized
in that language, without having to list them all. Therefore, modeling of borrowing is a task
well-suited for computational approaches.
Our suggestion in this work is that we can identify borrowing relations between resourcerich donor languages (such as English, French, Spanish, Arabic, Chinese, or Russian) and
resource-limited recipient languages. For example, 3070% of the vocabulary in Vietnamese,
Cantonese, and Thairelatively resource-limited languages spoken by hundreds of millions
of peopleare borrowed from Chinese and English, languages for which numerous data
resources have been created. Similarly, African languages have been greatly influenced
by Arabic, Spanish, English, and Frenchwidely spoken languages such as Swahili, Zulu,
Malagasy, Hausa, Tarifit, Yoruba contain up to 40% of loanwords. Indo-Iranian languages
Hindustani, Hindi, Urdu, Bengali, Persian, Pashtospoken by 860 million, also extensively
borrowed from Arabic and English (Haspelmath & Tadmor, 2009). In short, at least a
billion people are speaking resource-scarce languages whose lexicons are heavily borrowed
from resource-rich languages.
Why is this important? Lexical translations or alignments extracted from large parallel
corpora have been widely used to project annotations from high- to low-resource languages
(Hwa et al., 2005; Tckstrm et al., 2013; Ganchev et al., 2009, inter alia). Unfortunately,
large-scale parallel resources are unavailable for the majority of resource-limited languages.
Loanwords can be used as a source of cross-lingual links complementary to lexical alignments
obtained from parallel data or bilingual lexicons. This holds promise for applying existing
cross-lingual methods and bootstrapping linguistic resources in languages where no parallel
data is available.

3. Constraint-Based Models of Lexical Borrowing
Our task is to identify plausible donorloan word pairs in a language pair. While modeling
string transductions is a well-studied problem in NLP, we wish to be able to learn the crosslingual patterns from minimal training data. We therefore propose a model whose features
are motivated by linguistic knowledgerather than overparameterized with numerous weakly
correlated features which are more practical when large amounts of training data is available.
The features in our scoring model are inspired by Optimality Theory (OT; 3.1), in which
borrowing candidates are ranked by universal constraints posited to underly the human
faculty of language, and the candidates are determined by transduction processes articulated
in prior studies of contact linguistics.
As illustrated in Figure 2, our model is conceptually divided into three main parts: (1) a
mapping of orthographic word forms in two languages into a common phonetic space; (2)
generation of loanword pronunciation candidates from a donor word; and (3) ranking of
generated loanword candidates, based on linguistic constraints of the donor and recipient
67

fiTsvetkov & Dyer

1

donor To
word IPA

2

2

Syllabification

2

Phonological
adaptation

1

3

Morphological
adaptation

GEN - generate loanword candidates

Ranking with
OT constraints

From
IPA

loan
word

EVAL - rank candidates

Figure 2: Our morpho-phonological borrowing model conceptually has three main parts: (1)
conversion of orthographic word forms to pronunciations in International Phonetic Alphabet format;
(2) generation of loanword pronunciation candidates; (3) ranking of generated candidates using
Optimality-Theoretic constraints. Part (1) and (2) are rule-based, (1) uses pronunciation dictionaries,
(2) is based on prior linguistic studies; part (3) is learned. In (3) we learn OT constraint weights
from a few dozen automatically extracted training examples.

languages. In our proposed system, parts (1) and (2) are rule-based; whereas (3) is learned.
Each component of the model is discussed in detail in the rest of this section.
The model is implemented as a cascade of finite-state transducers. Parts (1) and (2)
amount to unweighted string transformation operations. In (1), we convert orthographic
word forms to their pronunciations in the International Phonetic Alphabet (IPA), these are
pronunciation transducers. In (2) we syllabify donor pronunciations, then perform insertion,
deletion, and substitution of phonemes and morphemes (affixes), to generate multiple
loanword candidates from a donor word. Although string transformation transducers in (2)
can generate loanword candidates that are not found in a recipient language vocabulary, such
candidates are filtered out due to composition with the recipient language lexicon acceptor.
Our model performs string transformations from donor to recipient (recapitulating the
historical process). However, the resulting relation (i.e., the final composed transducer) is a
bidirectional model which can just as well be used to reason about underlying donor forms
given recipient forms. In a probabilistic cascade, Bayes rule could be used to reverse the
direction and infer underlying donor forms given a loanword. However, we instead opt to
train the model discriminatively to find the most likely underlying form, given a loanword. In
part (3), candidates are evaluated (i.e., scored) with a weighted sum of universal constraint
violations. The non-negative weights, which we call the cost vector, constitute our model
parameters and are learned using a small training set of donorrecipient pairs. We use a
shortest path algorithm to find the path with the minimal cost.
3.1 OT: Constraint-Based Evaluation
Borrowing relations may be the result of quite complex transformations on the surface. Our
decision to evaluate borrowing candidates by weighting counts of constraint violations is
based on Optimality Theory, which has shown that complex surface phenomena can be
well-explained as the interaction of constraints on the form of outputs and the relationships
of inputs and outputs (Kager, 1999).
OT posits that surface phonetic words of a language emerge from underlying phonological
forms according to a two-stage process: first, various candidates for the surface form are
enumerated for consideration (the generation or gen phase); then, these candidates are
weighed against one another to see which most closely conforms toor equivalently, least
egregiously violatesthe phonological preferences of the language. If the preferences are
68

fiCross-Lingual Bridges with Models of Lexical Borrowing

/Eg/
a.

+

dep-io

max-io

onset

no-coda





Eg

b.

Eg@

c.

E

d.

Eg

!


!



!



Table 1: A constraint tableau. dep-io  max-io  onset  no-coda are ranked OT constraints
according to the phonological system of English. /Eg/ is the underlying phonological form, and (a),
(b) (c), and (d) are the output candidates under consideration. The actual surface form is (a), as it
incurs lower ranked violations than other candidates.

[Sarr]
a.

Sarr

b.

Sar.ri

c.
d.

+

*complex

no-coda

!


!

!

dep-io




Sa.ri
Sa.rri

max-io




Table 2: An example OT analysis adapted to account for borrowing. OT constraints are ranked
according to the phonological system of the recipient language (here, Swahili). The donor (Arabic)
  ($r~) evil is considered as the underlying form. The winning surface form (c) is the Swahili
word Q
loanword shari evil.

correctly characterized, then the actual surface form should be selected as the optimal
realization of the underlying form. Such preferences are expressed as violable constraints
(violable because in many cases there may be no candidate that satisfies all of them).
There are two types of OT constraints: markedness and faithfulness constraints. Markedness constraints (McCarthy & Prince, 1995) describe unnatural (dispreferred) patterns in
the language. Faithfulness constraints (Prince & Smolensky, 2008) reward correspondences
between the underlying form and the surface candidates. To clarify the distinction between
faithfulness and markedness constraint groups to the NLP readership, we can draw the
following analogy to the components of machine translation or speech recognition: faithfulness constraints are analogical to the translation model or acoustic model (reflecting
how well an output candidate is appropriate to the input), while markedness constraints
are analogical to the language model (requiring well-formedness of the output candidate).
Without faithfulness constraints, the optimal surface form could differ arbitrarily from the
underlying form. As originally proposed, OT holds that the set of constraints is universal,
but their ranking is language-specific.
In OT, then, the grammar is the set of universal constraints and their language-specific
ranking, and a derivation for a surface form consists of its underlying form, surface
candidates, and constraint violations by those candidates (under which the surface form is
correctly chosen). An example of OT analysis is shown in Table 1; OT constraints will be
explained later, in Tables 3 and 4.
OT has been adapted to account for borrowing by treating the donor language word
as the underlying form for the recipient language; that is, the phonological system of the
69

fiTsvetkov & Dyer

recipient language is encoded as a system of constraints, and these constraints account for
how the donor word is adapted when borrowed. We show an example in Table 2. There
has been substantial prior work in linguistics on borrowing in the OT paradigm (Yip, 1993;
Davidson & Noyer, 1997; Jacobs & Gussenhoven, 2000; Kang, 2003; Broselow, 2004; Adler,
2006; Rose & Demuth, 2006; Kenstowicz & Suchato, 2006; Kenstowicz, 2007; Mwita, 2009),
but none of it has led to computational realizations.
OT assumes an ordinal constraint ranking and strict dominance rather than constraint
weighting. In that, our OT-inspired model departs from OTs standard evaluation assumptions: following Goldwater and Johnson (2003), we use a linear scoring scheme.
3.2 Case Study: ArabicSwahili Borrowing
In this section, we use the ArabicSwahili4 language-pair to describe the prototypical
linguistic adaptation processes that words undergo when borrowed. Then, we describe how
we model these processes in more general terms.
The Swahili lexicon has been influenced by Arabic as a result of a prolonged period
of language contact due to Indian Ocean trading (800 ce1920), as well as the influence
of Islam (Rothman, 2002). According to several independent studies, Arabic loanwords
constitute from 18% (Hurskainen, 2004b) to 40% (Johnson, 1939) of Swahili word types.
Despite a strong susceptibility of Swahili to borrowing and a large fraction of Swahili
words originating from Arabic, the two languages are typologically distinct with profoundly
dissimilar phonological and morpho-syntactic systems. We survey these systems briefly
since they illustrate how Arabic loanwords have been substantially adapted to conform to
Swahili phonotactics. First, Arabic has five syllable patterns:5 CV, CVV, CVC, CVCC, and
CVVC (McCarthy, 1985, pp. 2328), whereas Swahili (like other Bantu languages) has only
open syllables of the form CV or V. At the segment level, Swahili loanword adaptation thus
involves extensive vowel epenthesis in consonant clusters and at a syllable final position if

the syllable ends with a consonant, for example, H
. AJ (ktAb)  kitabu book (Polom, 1967;
Schadeberg, 2009; Mwita, 2009). Second, phonological adaptation in Swahili loanwords
includes shortening of vowels (unlike Arabic, Swahili does not have phonemic length);
substitution of consonants that are found in Arabic but not in Swahili (e.g., emphatic
(pharyngealized) /tQ //t/, voiceless velar fricative /x//k/, dental fricatives /T//s/,
/D//z/, and the voiced velar fricative /G//g/); adoption of Arabic phonemes that were
	 
not originally present in Swahili /T/, /D/, /G/ (e.g., QK
 Ym' (tH*yr) tahadhari warning);
  ($r~) shari evil). Finally, adapted
degemination of Arabic geminate consonants (e.g., Q
loanwords can freely undergo Swahili inflectional and derivational processes, for example,
	 (Alwzyr)  waziri minister, mawaziri ministers, kiuwaziri ministerial (Zawawi,
QK
 P@
1979; Schadeberg, 2009).

4. For simplicity, we subsume Omani Arabic and other historical dialects of Arabic under the label Arabic;
our data and examples are in Modern Standard Arabic. Similarly, we subsume Swahili, its dialects and
protolanguages under Swahili.
5. C stands for consonant, and V for vowel.

70

fiCross-Lingual Bridges with Models of Lexical Borrowing

3.3 ArabicSwahili Borrowing Transducers
We use unweighted transducers for pronunciation, syllabification, and morphological and
phonological adaptation and describe these below. An example that illustrates some of the
possible string transformations by individual components of the model is shown in Figure 3.
The goal of these transducers is to minimally overgenerate Swahili adapted forms of Arabic
words, based on the adaptations described above.
Arabic word
to IPA
kuttaba



kitaba
...

Syllabification
ku.tta.ba.
ku.t.ta.ba.
...
ki.ta.ba.
ki.ta.b.
...

Phonological
adaptation

Morphological
adaptation

ku.ta.ba. [degemination]
ku.tata.ba. [epenthesis]
ku.ta.bu. [final vowel subst.]
ki.ta.bu. [final vowel subst.]
ki.ta.bu. [epenthesis]
...

Figure 3: An example of an Arabic word
into a Swahili loanword kitabu.

ku.tata.ba.li.
ku.tata.ba.
vi.ki.ta.bu.
ki.ta.bu.
ki.ta.bu.
...

Ranking with
OT constraints

IPA to
Swahili word

ku.ta<DEP-V>ta<PEAK>.ba.li<DEP-MORPH>.
ku.ta<DEP-V>ta<PEAK>.ba.li.
kitabu
ku.tta<*COMPLEX>.ba.
ki.ta.bu<IDENT-IO-V>.
ki.ta.bu<DEP-V>.
vi<DEP-MORPH>.ki.ta.bu<IDENT-IO-V>.

AK. AJ (ktAbA) book.sg.indef transformed by our model

3.3.1 Pronunciation
Based on the IPA, we assign shared symbols to sounds that exist in both sound systems
of Arabic and Swahili (e.g., nasals /n/, /m/; voiced stops /b/, /d/), and language-specific
unique symbols to sounds that are unique to the phonemic inventory of Arabic (e.g.,
pharyngeal voiced and voiceless fricatives //, /Q/) or Swahili (e.g., velar nasal /N/). For
Swahili, we construct a pronunciation dictionary based on the Omniglot grapheme-to-IPA
mapping.6 In Arabic, we use the CMU Arabic vowelized pronunciation dictionary containing
about 700K types which has the average of four pronunciations per unvowelized input word
type (Metze, Hsiao, Jin, Nallasamy, & Schultz, 2010).7 We then design four transducers
Arabic and Swahili word-to-IPA and IPA-to-word transducerseach as a union of linear
chain transducers, as well as one acceptor per pronunciation dictionary listing.
3.3.2 Syllabification
Arabic words borrowed into Swahili undergo a repair of violations of the Swahili segmental and
phonotactic constraints, for example via vowel epenthesis in a consonant cluster. Importantly,
repair depends upon syllabification. To simulate plausible phonological repair processes,
we generate multiple syllabification variants for input pronunciations. The syllabification
transducer optionally inserts syllable separators between phones. For example, for an
input phonetic sequence /kuttAbA/, the output strings include /ku.t.tA.bA/, /kut.tA.bA/,
and /ku.ttA.bA/ as syllabification variants; each variant violates different constraints and
consequently triggers different phonological adaptations.
6. www.omniglot.com
7. Since we are working at the level of word types which have no context, we cannot disambiguate the
intended form, so we include all options. For example, for the input word AK. AJ (ktAbA) book.sg.indef, we
use both pronunciations /kitAbA/ and /kuttAbA/.

71

fiTsvetkov & Dyer

3.3.3 Phonological Adaptation
Phonological adaptation of syllabified phone sequences is the crux of the loanword adaptation
process. We implement phonological adaptation transducers as a composition of plausible
context-dependent insertions, deletions, and substitutions of phone subsets, based on prior
studies summarized in 3.2. In what follows, we list phonological adaptation components in
the order of transducer composition in the borrowing model. The vowel deletion transducer
shortens Arabic long vowels and vowel clusters. The consonant degemination transducer
shortens Arabic geminate consonants, for example, it degeminates /tt/ in /ku.ttA.bA/,
outputting /ku.tA.bA/. The substitution of similar phonemes transducer substitutes
similar phonemes and phonemes that are found in Arabic but not in Swahili (Polom,
1967, p. 45). For example, the emphatic /tQ /, /dQ /, /sQ / are replaced by the corresponding
non-emphatic segments [t], [d], [s]. The vowel epenthesis transducer inserts a vowel
between pairs of consonants (/ku.ttA.bA/  /ku.tatA.bA/), and at the end of a syllable, if
the syllable ends with a consonant (/ku.t.tA.bA/  /ku.ta.tA.bA/). Sometimes it is possible
to predict the final vowel of a word, depending on the word-final coda consonant of its
Arabic counterpart: /u/ or /o/ added if an Arabic donor ends with a labial, and /i/ or
/e/ added after coronals and dorsals (Mwita, 2009). Following these rules, the final vowel
substitution transducer complements the inventory of final vowels in loanword candidates.
3.3.4 Morphological Adaptation
Both Arabic and Swahili have significant morphological processes that alter the appearance
of lemmas. To deal with morphological variants, we construct morphological adaptation
transducers that optionally strip Arabic concatenative affixes and clitics, and then optionally
append Swahili affixes, generating a superset of all possible loanword hypotheses. We
obtain the list of Arabic affixes from the Arabic morphological analyzer SAMA (Maamouri,
Graff, Bouziri, Krouna, & Kulick, 2010); the Swahili affixes are taken from a hand-crafted
Swahili morphological analyzer (Littell, Price, & Levin, 2014). For the sake of simplicity in
implementation, we strip no more than one Arabic prefix and no more than one suffix per
word; and in Swahili  we concatenate at most two Swahili prefixes and at most one suffix.
3.4 Learning Constraint Weights
Due to the computational problems of working with OT (Eisner, 1997, 2002), we make
simplifying assumptions by (1) bounding the theoretically infinite set of underlying forms with
a small linguistically-motivated subset of allowed transformations on donor pronunciations,
as described in 3.3; (2) imposing a priori restrictions on the set of the surface realizations
by intersecting the candidate set with the recipient pronunciation lexicon; (3) assuming that
the set of constraints is finite and regular (Ellison, 1994); and (4) assigning linear weights
to constraints, rather than learning an ordinal constraint ranking with strict dominance
(Boersma & Hayes, 2001; Goldwater & Johnson, 2003).
As discussed in 3.1, OT distinguishes markedness constraints which detect dispreferred
phonetic patterns in the language, and faithfulness constraints, which ensure correspondences
between the underlying form and the surface candidates. The implemented constraints are
listed in Tables 3 and 4. Faithfulness constraints are integrated in phonological transformation
components as transitions following each insertion, deletion, or substitution. Markedness
72

fiCross-Lingual Bridges with Models of Lexical Borrowing

Faithfulness constraints
max-io-morph no (donor) affix deletion
max-io-c
no consonant deletion
max-io-v
no vowel deletion
dep-io-morph no (recipient) affix epenthesis
dep-io-v
no vowel epenthesis
ident-io-c
no consonant substitution
ident-io-c-m
no substitution in manner of pronunciation
ident-io-c-a
no substitution in place of articulation
ident-io-c-s
no substitution in sonority
ident-io-c-p
no pharyngeal consonant substitution
ident-io-c-g
no glottal consonant substitution
ident-io-c-e
no emphatic consonant substitution
ident-io-v
no vowel substitution
ident-io-v-o
no substitution in vowel openness
ident-io-v-r
no substitution in vowel roundness
ident-io-v-f
no substitution in vowel frontness
ident-io-v-fin no final vowel substitution
Table 3: Faithfulness constraints prefer pronounced realizations completely congruent with their
underlying forms.

Markedness constraints
no-coda
syllables must not have a coda
onset
syllables must have onsets
peak
there is only one syllabic peak
ssp
complex onsets rise in sonority,
complex codas fall in sonority
*complex-s no consonant clusters on syllable margins
*complex-c no consonant clusters within a syllable
*complex-v no vowel clusters
Table 4: Markedness constraints impose language-specific structural well-formedness of surface
realizations.

constraints are implemented as standalone identity transducers: inputs are equal outputs,
but path weights representing candidate evaluation with respect to violated constraints are
different.
The final loanword transducer is the composition of all transducers described in 3.3
and OT constraint transducers. A path in the transducer represents a syllabified phonemic
sequence along with (weighted) OT constraints it violates, and shortest path outputs are
those, whose cumulative weight of violated constraints is minimal.
OT constraints are realized as features in our linear model, and feature weights are
learned in a discriminative training to maximize the accuracy obtained by the loanword
transducer on a small development set of donorrecipient pairs. For parameter estimation,
we employ the NelderMead algorithm (Nelder & Mead, 1965), a heuristic derivative-free
73

fiTsvetkov & Dyer

method that iteratively optimizes, based on an objective function evaluation, the convex
hull of n + 1 simplex vertices.8 The objective function used in this work is the soft accuracy
of the development set, defined as the proportion of correctly identified donor words in the
total set of 1-best outputs.
3.5 Adapting the Model to a New Language
The ArabicSwahili case study shows that, in principle, a borrowing model can be constructed.
But a reasonable question to ask is: how much work is required to build a similar system
for a new language pair? We claim that our design permits rapid development in new
language pairs. First, string transformation operations, as well as OT constraints are
language-universal. The only adaptation required is a linguistic analysis to identify plausible
morpho-phonological repair strategies for the new language pair (i.e., a subset of allowed
insertions, deletions, and substitutions of phonemes and morphemes). Since we need
only to overgenerate candidates (the OT constraints will filter bad outputs), the effort is
minimal relative to many other grammar engineering exercises. The second language-specific
component is the grapheme-to-IPA converter. While this can be a non-trivial problem in
some cases, the problem is well studied, and many under-resourced languages (e.g., Swahili),
have phonographic systems where orthography corresponds to phonology. This tendency
can be explained by the fact that, in many cases, lower-resource languages have developed
orthography relatively recently, rather than having organically evolved written forms that
preserve archaic or idiosyncratic spellings that are more distantly related to the current
phonology of the language such as we see in English.
To illustrate the ease with which a language pair can be engineered, we applied our
borrowing model to the ItalianMaltese and FrenchRomanian language pairs. Maltese and
Romanian, like Swahili, have a large number of borrowed words in their lexicons (Tadmor,
2009). Maltese (a phylogenetically Semitic language) has 35.1%30.3% loanwords of Romance
(Italian/Sicilian) origin (Comrie & Spagnol, 2015). Although French and Romanian are
sister languages (both descending from Latin), about 12% of Romanian types are French
borrowings that came into the language in the past few centuries (Schulte, 2009). For both
language pairs we manually define a set of allowed insertions, deletions, and substitutions
of phonemes and morphemes, based on the training sets. A set of Maltese affixes was
defined based on the linguistic survey by Fabri, Gasser, Habash, Kiraz, and Wintner (2014).
We employ the GlobalPhone pronunciation dictionary for French (Schultz & Schlippe,
2014), converted to IPA, and automatically constructed Italian, Romanian, and Maltese
pronunciation dictionaries using the Omniglot grapheme-to-IPA conversion rules for those
languages.

4. Models of Lexical Borrowing in Statistical Machine Translation
Before turning to an experimental verification and analysis of the borrowing model, we
introduce an external application where the borrowing model will be used as a component
machine translation. We rely on the borrowing model to project translation information
8. The decision to use NelderMead rather than more conventional gradient-based optimization algorithms
was motivated purely by practical limitations of the finite-state toolkit we used which made computing
derivatives with latent structure impractical from an engineering standpoint.

74

fiCross-Lingual Bridges with Models of Lexical Borrowing

from a high-resource donor language into a low-resource recipient language, thus mitigating
the deleterious effects of out-of-vocabulary (OOV) words.
OOVs are a ubiquitous and difficult problem in MT. When a translation system encounters
an OOVa word that was not observed in the training data, and the trained system thus
lacks its translation variantsit usually outputs the word just as it is in the source language,
producing erroneous and disfluent translations. All MT systems, even when trained on billionsentence-size parallel corpora, will encounter OOVs at test time. Often, these are named
entities and neologisms. However, the OOV problem is much more acute in morphologicallyrich and low-resource scenarios: there, OOVs are primarily not lexicon-peripheral items such
as names and specialized/technical terms, but also regular content words. Since borrowed
words are a component of the regular lexical content of a language, projecting translations
onto the recipient language by identifying borrowed lexical material is a plausible strategy
for solving this problem.
Procuring translations for OOVs has been a subject of active research for decades.
Translation of named entities is usually generated using transliteration techniques (AlOnaizan & Knight, 2002; Hermjakob, Knight, & Daum III, 2008; Habash, 2008). Extracting
a translation lexicon for recovering OOV content words and phrases is done by mining
bi-lingual and monolingual resources (Rapp, 1995; Callison-Burch, Koehn, & Osborne, 2006;
Haghighi, Liang, Berg-Kirkpatrick, & Klein, 2008; Marton, Callison-Burch, & Resnik, 2009;
Razmara, Siahbani, Haffari, & Sarkar, 2013; Saluja, Hassan, Toutanova, & Quirk, 2014). In
addition, OOV content words can be recovered by exploiting cognates, by transliterating
and then pivoting via a closely-related resource-richer language, when such a language
exists (Haji, Hric, & Kubo, 2000; Mann & Yarowsky, 2001; Kondrak, Marcu, & Knight,
2003; De Gispert & Marino, 2006; Habash & Hu, 2009; Durrani, Sajjad, Fraser, & Schmid,
2010; Wang, Nakov, & Ng, 2012; Nakov & Ng, 2012; Dholakia & Sarkar, 2014). Our work is
similar in spirit to the latter pivoting approach, but we show how to obtain translations for
OOV content words by pivoting via an unrelated, often typologically distant resource-rich
language.
Our solution is depicted, at a high level, in Figure 4. Given an OOV word in resource-poor
MT, we use our borrowing system to identify list of likely donor words from the donor
language. Then, using the MT system in the resource-rich language, we translate the donor
words to the same target language as in the resource-poor MT (here, English). Finally, we
integrate translation candidates in the resource-poor system.
We now discuss integrating translation candidates acquired via borrowing plus resourcerich translation.
Briefly, phrase-based translation works as follows. A set of candidate translations for an
input sentence is created by matching contiguous spans of the input against an inventory of
phrasal translations, reordering them into a target-language appropriate order, and choosing
the best one according to a model that combines features of the phrases used, reordering
patterns, and target language model (Koehn, Och, & Marcu, 2003). A limitation of this
approach is that it can only generate input/output phrase pairs that were directly observed
in the training corpus. In resource-limited languages, the standard phrasal inventory will
generally be incomplete due to limited parallel data. Thus, the decoders only hope for
producing a good output is to find a fluent, meaning-preserving translation using incomplete
translation lexicons. Synthetic phrases is a strategy of integrating translated phrases
75

fiSWAHILIENGLISH

safari |||
kituruki |||

*OOV*
*OOV*

ARABICENGLISH

( ysAfr) ||| travel

TRANSLATION
CANDIDATES

ARABIC -to- SWAHILI
BORROWING

Tsvetkov & Dyer

( trky) ||| turkish

Figure 4: To improve a resource-poor SwahiliEnglish MT system, we extract translation candidates
for OOV Swahili words borrowed from Arabic using the Swahili-to-Arabic borrowing system and
ArabicEnglish resource-rich MT.

directly in the MT translation model, rather than via pre- or post-processing MT inputs and
outputs (Tsvetkov, Dyer, Levin, & Bhatia, 2013; Chahuneau, Schlinger, Smith, & Dyer, 2013;
Schlinger, Chahuneau, & Dyer, 2013; Ammar, Chahuneau, Denkowski, Hanneman, Ling,
Matthews, Murray, Segall, Tsvetkov, Lavie, & Dyer, 2013; Tsvetkov, Metze, & Dyer, 2014;
Tsvetkov & Dyer, 2015). Synthetic phrases are phrasal translations that are not directly
extractable from the training data, generated by auxiliary translation and postediting
processes (for example, extracted from a borrowing model). An important advantage of
synthetic phrases is that the process often benefits from phrase synthesizers that have high
recall (relative to precision) since the global translation model will still have the final say on
whether a synthesized phrase will be used.
For each OOV, the borrowing system produces the n-best list of plausible donors; for
each donor we then extract the k-best list of its translations.9 Then, we pair the OOV with
the resulting n  k translation candidates. The translation candidates are noisy: some of
the generated donors may be erroneous, the errors are then propagated in translation.10
To allow the low-resource translation system to leverage good translations that are missing
in the default phrase inventory, while being able to learn how trustworthy they are, we
integrate the borrowing-model acquired translation candidates as synthetic phrases.
To let the translation model learn whether to trust these phrases, the translation options
obtained from the borrowing model are augmented with an indicator feature indicating
that the phrase was generated externally (i.e., rather than being extracted from the parallel
data). Additional features assess properties of the donorloan words relation; their goal is to
provide an indication of plausibility of the pair (to mark possible errors in the outputs of the
borrowing system). We employ two types of features: phonetic and semantic. Since borrowing
is primarily a phonological phenomenon, phonetic features will provide an indication of how
9. We set n and k to 5; we did not experiment with other values.
10. We give as input into the borrowing system all OOV words, although, clearly, not all OOVs are loanwords,
and not all loanword OOVs are borrowed from the donor language. However, an important property of
the borrowing model is that its operations are not general, but specific to the language-pair and reduced
only to a small set of plausible changes that the donor word can undergo in the process of assimilation
in the recipient language. Thus, the borrowing system only minimally overgenerates the set of output
candidates given an input. If the borrowing system encounters an input word that was not borrowed
from the target donor language, it usually (but not always) produces an empty output.

76

fiCross-Lingual Bridges with Models of Lexical Borrowing

typical (or atypical) pronunciation of the word in a language; loanwords are expected to
be less typical than core vocabulary words. The goal of semantic features is to measure
semantic similarity between donor and loan words: erroneous candidates and borrowed
words that changed meaning over time are expected to have different meaning from the
OOV.
4.1 Phonetic Features
To compute phonetic features we first train a (5-gram) language model (LM) of IPA pronunciations of the donor/recipient language vocabulary (p ). Then, we re-score pronunciations of
the donor and loanword candidates using the LMs. We hypothesize that in donorloanword
pairs both the donor and the loanword phone LM score is high. We capture this intuition in
three features: f1 = p (donor), f2 = p (loanword), and the harmonic mean between the two
1 f2
scores f3 = f2f1 +f
(the harmonic mean of a set of values is high only when all of the values
2
are high).
4.2 Semantic Features
We compute a semantic similarity feature between the candidate donor and the OOV
loanword as follows. We first train, using large monolingual corpora, 100-dimensional word
vector representations for donor and recipient language vocabularies.11 Then, we employ
canonical correlation analysis (CCA) with small donorloanword dictionaries (training sets
in the borrowing models) to project the word embeddings into 50-dimensional vectors with
maximized correlation between their dimensions. The semantic feature annotating the
synthetic translation candidates is cosine distance between the resulting donor and loanword
vectors. We use the word2vec Skip-gram model (Mikolov, Sutskever, Chen, Corrado, &
Dean, 2013) to train monolingual vectors,12 and the CCA-based tool (Faruqui & Dyer, 2014)
for projecting word vectors.13

5. Experiments
We now turn to the problem of empirically validating the model we have proposed. Our
evaluation consists of two parts. First, we perform an intrinsic assessment of the models
ability to learn borrowing correspondences and compare these to similar approaches that
use less linguistic knowledge but which have been used to solve similar string mapping
problems. Second, we show the effect of borrowing-augmented translations in translation
systems, exploring the effects of the features proposed above.
5.1 Intrinsic Evaluation of Models of Lexical Borrowing
Our experimental setup is defined as follows. The input to the borrowing model is a
loanword candidate in Swahili/Maltese/Romanian, the outputs are plausible donor words in
the Arabic/Italian/French monolingual lexicon (i.e., any word in pronunciation dictionary).
11. We assume that while parallel data is limited in the recipient language, monolingual data is available.
12. code.google.com/p/word2vec
13. github.com/mfaruqui/eacl14-cca

77

fiTsvetkov & Dyer

We train the borrowing model using a small set of training examples, and then evaluate it
using a held-out test set. In the rest of this section we describe in detail our datasets, tools,
and experimental results.
5.1.1 Resources
We employ ArabicEnglish and SwahiliEnglish bitexts to extract a training set (corpora
of sizes 5.4M and 14K sentence pairs, respectively), using a cognate discovery technique
(Kondrak, 2001). Phonetically and semantically similar strings are classified as cognates;
phonetic similarity is the string similarity between phonetic representations, and semantic
similarly is approximated by translation.14 We thereby extract Arabic and Swahili pairs
(a,s)
ha, si that are phonetically similar ( min(|a|,|s|)
< 0.5) where (a, s) is the Levenshtein
distance between a and s and that are aligned to the same English word e. FastAlign (Dyer,
Chahuneau, & Smith, 2013) is used for word alignments. Given an extracted word pair
ha, si, we also extract word pairs {ha0 , si} for all proper Arabic words a0 which share the
same lemma with a producing on average 33 Arabic types per Swahili type. We use MADA
(Habash, Rambow, & Roth, 2009) for Arabic morphological expansion.
From the resulting dataset of 490 extracted ArabicSwahili borrowing examples,15 we
set aside randomly sampled 73 examples (15%) for evaluation,16 and use the remaining 417
examples for model parameter optimization. For ItalianMaltese language pair, we use the
same technique and extract 425 training and 75 (15%) randomly sampled test examples.
For FrenchRomanian language pair, we use an existing small annotated set of borrowing
examples,17 with 282 training and 50 (15%) randomly sampled test examples.
We use pyfsta Python interface to OpenFst (Allauzen, Riley, Schalkwyk, Skut, &
Mohri, 2007)for the borrowing model implementation.18
5.1.2 Baselines
We compare our model to several baselines. In the Levenshtein distance baselines we chose
the closest word (either surface or pronunciation-based). In the cognates baselines, we
evaluate a variant of the Levenshtein distance tuned to identify cognates (Mann & Yarowsky,
2001; Kondrak & Sherif, 2006); this method was identified by Kondrak and Sherif (2006)
among the top three cognate identification methods. In the transliteration baselines we
generate plausible transliterations of the input Swahili (or Romanian) words in the donor
lexicon using the model of Ammar, Dyer, and Smith (2012), with multiple references in a
lattice and without reranking. The CRF transliteration model is a linear-chain CRF where
we label each source character with a sequence of target characters. The features are label
unigrams, label bigrams, and label conjoined with a moving window of source characters. In
the OT-uniform baselines, we evaluate the accuracy of the borrowing model with uniform
14. This cognate discovery technique is sufficient to extract a small training set, but is not generally applicable,
as it requires parallel corpora or manually constructed dictionaries to measure semantic similarity. Large
parallel corpora are unavailable for most language pairs, including SwahiliEnglish.
15. In each training/test example one Swahili word corresponds to all extracted Arabic donor words.
16. We manually verified that our test set contains clear ArabicSwahili borrowings. For example, we extract
	
	
	
Swahili kusafiri, safari and Arabic Q@, Q
, Q (Alsfr, ysAfr, sfr) all aligned to travel.
17. http://wold.clld.org/vocabulary/8
18. https://github.com/vchahun/pyfst

78

fiCross-Lingual Bridges with Models of Lexical Borrowing

Reachability
Ambiguity

arsw
87.7%
857

itmt
92.7
11

frro
82.0%
12

Table 5: The evaluation of the borrowing model design. Reachability is the percentage of donor
recipient pairs that are reachable from a donor to a recipient language. Ambiguity is the average
number of outputs that the model generates per one input.

weights, thus the shortest path in the loanwords transducer will be forms that violate the
fewest constraints.
5.1.3 Evaluation
In addition to predictive accuracy on all models (if a model produces multiple hypotheses
with the same 1-best weight, we count the proportion of correct outputs in this set), we
evaluate two particular aspects of our proposed model: (1) appropriateness of the model
family, and (2) the quality of the learned OT constraint weights. The first aspect is designed
to evaluate whether the morpho-phonological transformations implemented in the model
are required and sufficient to generate loanwords from the donor inputs. We report two
evaluation measures: model reachability and ambiguity. Reachability is the percentage of
test samples that are reachable (i.e., there is a path from the input test example to a correct
output) in the loanword transducer. A nave model which generates all possible strings
would score 100% reachability; however, inference may be expensive and the discriminative
component will have a greater burden. In order to capture this trade-off, we also report
the inherent ambiguity of our model, which is the average number of outputs potentially
generated per input. A generic ArabicSwahili transducer, for example, has an ambiguity of
786,998the size of the Arabic pronunciation lexicon.19
5.1.4 Results
The reachability and ambiguity of the borrowing model are listed in Table 5. Briefly, the
model obtains high reachability, while significantly reducing the average number of possible
outputs per input: in Arabic from 787K to 857 words, in Maltese from 129K to 11, in French
from 62K to 12. This result shows that the loanword transducer design, based on the prior
linguistic analysis, is a plausible model of word borrowing. Yet, there are on average 33
correct Arabic words out of the possible 857 outputs, thus the second part of the modelOT
constraint weights optimizationis crucial.
The accuracy results in Table 6 show how challenging the task of modeling lexical
borrowing between two distinct languages is, and importantly, that orthographic and
phonetic baselines including the state-of-the-art generative model of transliteration are not
suitable for this task. Phonetic baselines for ArabicSwahili perform better than orthographic
ones, but substantially worse than OT-based models, even if OT constraints are not weighted.
Crucially, the performance of the borrowing model with the learned OT weights corroborates
19. Our measure of ambiguity is equivalent to perplexity assuming a uniform distribution over output forms.

79

fiTsvetkov & Dyer

Orthographic baselines
Phonetic baselines

OT

Levenshtein-orthographic
Transliteration
Levenshtein-pronunciation
Cognates
OT-uniform constraint weights
OT-learned constraint weights

Accuracy (%)
arsw itmt frro
8.9
61.5
38.0
16.4
61.3
36.0
19.8
64.4
26.3
19.7
63.7
30.7
29.3
65.6
58.5
48.4
83.3
75.6

Table 6: The evaluation of the borrowing model accuracy. We compare the following setups:
orthographic (surface) and phonetic (based on pronunciation lexicon) Levenshtein distance, a cognate
identification model that uses heuristic Levenshtein distance with lower penalty on vowel updates
and similar letter/phone substitutions, a CRF transliteration model, and our model with uniform
and learned OT constraint weights assignment.

the assumption made in numerous linguistic accounts that OT is an adequate analysis of
the lexical borrowing phenomenon.
5.1.5 Qualitative Evaluation
The constraint ranking learned by the borrowing model (constraints are listed in Tables 3, 4)
is in line with prior linguistic analysis. In Swahili no-coda dominates all other markedness
constraints. Both *complex-s and *complex-c, restricting consonant clusters, dominate
*complex-v, confirming that Swahili is more permissive to vowel clusters. sspsonority-based
constraintcaptures a common pattern of consonant clustering, found across languages,
and is also learned by our model as undominated by most competitors in Swahili, and as
a dominating markedness constraint in Romanian. Morphologically-motivated constraints
also comply with tendencies discussed in linguistic literature: donor words may remain
unmodified and are treated as a stem, and then are reinfected according to the recipient
morphology, thus dep-io-morph can be dominated more easily than max-io-morph. Finally,
vowel epenthesis dep-io-v is the most common strategy in Arabic loanword adaptation, and
is ranked lower according to the model; however, it is ranked highly in the FrenchRomanian
model, where vowel insertion is rare.
A second interesting by-product of our model is an inferred syllabification. While we did
not conduct a systematic quantitative evaluation, higher-ranked Swahili outputs tend to
contain linguistically plausible syllabifications, although the syllabification transducer inserts
optional syllable boundaries between every pair of phones. This result further attests to the
plausible constraint ranking learned by the model. Example Swahili syllabifications20 along
with the constraint violations produced by the borrowing model are depicted in Table 7.
5.2 Extrinsic Evaluation of Pivoting via Borrowing in MT
We now turn to an extrinsic evaluation, looking at two low-resource translation tasks: Swahili
English translation (resource-rich donor language: Arabic), MalteseEnglish translation
20. We chose examples from the ArabicSwahili system because this is a more challenging case due to
linguistic discrepancies.

80

fiCross-Lingual Bridges with Models of Lexical Borrowing

en
book
palace

ar orth.
ktAb
AlqSr

ar pron.
kitAb
AlqaSr

sw syl.
ki.ta.bu.
ka.sri

wage

Ajrh

Aujrah

u.ji.ra.

Violated constraints
ident-io-c-ghA, ai, dep-io-vh, ui
max-io-morphhAl, i, ident-io-c-shq, ki,
ident-io-c-ehS, si, *complex-chsri, dep-io-vh, ii
max-io-vhA, i, onsethui ,
dep-io-vh, ii, max-io-chh, i

Table 7: Examples of inferred syllabification and corresponding constraint violations produced by
our borrowing model.

(resource-rich donor language: Italian), and RomanianEnglish translation (resource-rich
donor language: French). We begin by reviewing the datasets used, and then discuss two
oracle experiments that attempt to quantify how much value could we obtain from a perfect
borrowing model (since not all mistakes made by MT systems involve borrowed words).
Armed with this understanding, we then explore how much improvement can be obtained
using our system.
5.2.1 Datasets and Software
The SwahiliEnglish parallel corpus was crawled from the Global Voices project website21 .
For the MalteseEnglish language pair, we sample a parallel corpus of the same size from the
EUbookshop corpus from the OPUS collection (Tiedemann, 2012). Similarly, to simulate
resource-poor scenario for the RomanianEnglish language pair, we sample a corpus from
the transcribed TED talks (Cettolo, Girardi, & Federico, 2012). To evaluate translation
improvement on corpora of different sizes we conduct experiments with sub-sampled 4,000,
8,000, and 14,000 parallel sentences from the training corpora (the smaller the training
corpus, the more OOVs it has). Corpora sizes along with statistics of source-side OOV
tokens and types are given in Table 8. Statistics of the held-out dev and test sets used in all
translation experiments are given in Table 9.
The ArabicEnglish pivot translation system was trained on a parallel corpus of about 5.4
million sentences available from the Linguistic Data Consortium (LDC), and optimized on
the standard NIST MTEval dataset for the year 2005 (MT05). The ItalianEnglish system
was trained on 11 million sentences from the OPUS corpus. The FrenchEnglish pivot system
was trained on about 400,000 sentences from the transcribed TED talks, and optimized on
the dev talks from the RomanianEnglish system; test talks from the RomanianEnglish
system were removed from the FrenchEnglish training corpus.
In all the MT experiments, we use the cdec22 translation toolkit (Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, & Resnik, 2010), and optimize parameters
with MERT (Och, 2003). English 4-gram language models with Kneser-Ney smoothing
(Kneser & Ney, 1995) were trained using KenLM (Heafield, 2011) on the target side of the
parallel training corpora and on the Gigaword corpus (Parker, Graff, Kong, Chen, & Maeda,
2009). Results are reported using case-insensitive BLEU with a single reference (Papineni,
Roukos, Ward, & Zhu, 2002). To verify that our improvements are consistent and are not
21. sw.globalvoicesonline.org
22. www.cdec-decoder.org

81

fiTsvetkov & Dyer

swen

mten

roen

Tokens
Types
OOV tokens
OOV types
Tokens
Types
OOV tokens
OOV types
Tokens
Types
OOV tokens
OOV types

4K
84,764
14,554
4,465 (12.7%)
3,610 (50.3%)
104,181
14,605
4,735(8.7%)
4,171 (44.0%)
35,978
7,210
3,268 (16.6%)
2,382 (55.0%)

8K
170,493
23,134
3,509 (10.0%)
2,950 (41.1%)
206,781
22,407
3,497 (6.4%)
3,236 (34.2%)
71,584
11,144
2,585 (13.1%)
1,922 (44.4%)

14K
300,648
33,288
2,965 (8.4%)
2,523 (35.1%)
358,373
31,176
2,840 (5.2%)
2,673 (28.2%)
121,718
15,112
2,177 (11.1%)
1,649 (38.1%)

Table 8: Statistics of the SwahiliEnglish, MalteseEnglish, and RomanianEnglish corpora and
source-side OOV rates for 4K, 8K, 14K parallel training sentences.

Sentences
Tokens
Types

swen
dev
test
1,552
1,732
33,446 35,057
7,008
7,180

mten
dev
test
2,000
2,000
54,628 54,272
9,508
9,471

roen
dev
test
2,687
2,265
24,754 19,659
5,141
4,328

Table 9: Dev and test corpora sizes.

just an effect of optimizer instability, we train three systems for each MT setup; reported
BLEU scores are averaged over systems.
5.2.2 Upper Bounds
The goal of our experiments is not only to evaluate the contribution of the OOV dictionaries
that we extract when pivoting via borrowing, but also to understand the potential contribution of exploiting borrowing. What is the overall improvement that would be achieved if we
could correctly translate all OOVs that were borrowed from another language? What is the
overall improvement that can be achieved if we correctly translate all OOVs? We answer
this question by defining upper bound experiments. In the upper bound experiments
we word-align all available parallel corpora, including dev and test sets, and extract from
the alignments oracle translations of OOV words. Then, we append the extracted OOV
dictionaries to the training corpora and re-train SMT setups without OOVs. Translation
scores of the resulting system provide an upper bound of an improvement from correctly
translating all OOVs. When we append oracle translations of the subset of OOV dictionaries,
in particular translations of all OOVs for which the output of the borrowing system is not
empty, we obtain an upper bound that can be achieved using our method (if the borrowing
system provided perfect outputs relative to the reference translations). Understanding the
82

fiCross-Lingual Bridges with Models of Lexical Borrowing

4K
5,050
10,138
347

Loan OOVs in swen
Loan OOVs in mten
Loan OOVs in roen

8K
4,219
6,456
271

14K
3,577
4,883
216

Table 10: The size of dictionaries extracted using pivoting via borrowing and integrated in translation
models.

Transliteration OOVs in swen
Transliteration OOVs in mten
Transliteration OOVs in roen

4K
49
26,734
906

8K
32
19,049
714

14K
22
15,008
578

Table 11: The size of translated lexicons extracted using pivoting via transliteration and integrated
in translation models.

upper bounds is relevant not only for our experiments, but for any experiments that involve
augmenting translation dictionaries; however, we are not aware of prior work providing
similar analysis of upper bounds, and we recommend this as a calibrating procedure for
future work on OOV mitigation strategies.
5.2.3 Borrowing-Augmented Setups
As described in 4, we integrate translations of OOV loanwords in the translation model using
the synthetic phrase paradigm. Due to data sparsity, we conjecture that non-OOVs that
occur only few times in the training corpus can also lack appropriate translation candidates,
these are target-language OOVs. We therefore plug into the borrowing system OOVs and
non-OOV words that occur less than 3 times in the training corpus. We list in Table 10 the
size of resulting borrowed lexicons that we integrate in translation tables.23
5.2.4 Transliteration-Augmented Setups
In addition to the standard baselines, we evaluate transliteration baselines, where we replace
the borrowing model by the baselines described in 5.1. As in the borrowing system,
transliteration outputs are filtered to contain only target language lexicons. We list in
Table 11 the size of obtained translated lexicons.
5.2.5 Results
Translation results are shown in Table 12. We evaluate separately the contribution of
the integrated OOV translations, and the same translations annotated with phonetic and
semantic features. We also provide upper bound scores for integrated loanword dictionaries
as well as for recovering all OOVs.
23. Differences in statistics stem from differences in types of corpora, such as genre, domain, and morphological
richness of the source language.

83

fiTsvetkov & Dyer

swen

mten

roen

Baseline
+ Transliteration OOVs
+ Loan OOVs
+ Features
Upper bound loan
Upper bound all OOVs
Baseline
+ Transliteration OOVs
+ Loan OOVs
+ Features
Upper bound loan
Upper bound all OOVs
Baseline
+ Transliteration OOVs
+ Loan OOVs
+ Features
Upper bound loan
Upper bound all OOVs

4K
13.2
13.4
14.3
14.8
18.9
19.2
26.4
26.5
27.2
26.9
28.5
31.6
15.8
15.8
16.0
16.0
16.6
28.0

8K
15.1
15.3
15.7
16.4
19.1
20.4
31.4
30.8
31.7
31.9
32.2
35.6
18.5
18.7
18.7
18.6
19.4
28.8

14K
17.1
17.2
18.2
18.4
20.7
21.1
35.2
34.9
35.3
34.5
35.7
38.0
20.7
20.8
20.7
20.6
20.9
30.4

Table 12: BLUE scores in the SwahiliEnglish, MalteseEnglish, and RomanianEnglish MT
experiments.

SwahiliEnglish MT performance is improved by up to +1.6 BLEU when we augment
it with translated OOV loanwords leveraged from the ArabicSwahili borrowing and then
ArabicEnglish MT. The contribution of the borrowing dictionaries is +0.61.1 BLEU,
and phonetic and semantic features contribute additional half BLEU. More importantly,
upper bound results show that the system can be improved more substantially with better
dictionaries of OOV loanwords. This result confirms that OOV borrowed words is an
important type of OOVs, and with proper modeling it has the potential to improve translation
by a large margin. MalteseEnglish system is also improved substantially, by up to +0.8
BLEU, but the contribution of additional features is less pronounced. RomanianEnglish
systems obtain only small but significant improvement for 4K and 8K, p < .01 (Clark, Dyer,
Lavie, & Smith, 2011). However, this is expected as the rate of borrowing from French
into Romanian is smaller, and, as the result, the integrated loanword dictionaries are small.
Transliteration baseline, conversely, is more effective in RomanianFrench language pair, as
the languages are related typologically, and have common cognates in addition to loanwords.
Still, even with these dictionaries the translations with pivoting via borrowing/transliteration
improve, and even almost approach the upper bounds results.
5.2.6 Error Analysis
Our augmented MT systems combine three main components: the translation system itself, a
borrowing system, and a pivot translation system. At each step of the application errors may
occur that lead to erroneous translations. To identify main sources of errors in the Swahili
84

fiCross-Lingual Bridges with Models of Lexical Borrowing

Error source
Reachability of the borrowing system
Loanword production errors
ArabicEnglish translation errors
SwahiliEnglish translation errors

#
113
191
20
29

%
32.0
54.1
5.7
8.2

Table 13: Sources of errors.

English end-to-end system, we conducted a manual analysis of errors in translations of OOV
types produced by the SwahiliEnglish 4K translation systems. As a gold standard corpus
we use the Helsinki Corpus of Swahili24 (Hurskainen, 2004a, HCS). HCS is a morphologically,
syntactically, and semantically annotated corpus of about 580K sentences (12.7M tokens).
In the corpus 52,351 surface forms (1.5M tokens) are marked as Arabic loanwords. Out of
the 3,610 OOV types in the SwahiliEnglish 4K translation systems, 481 word types are
annotated in the HCS. We manually annotated these 481 words and identified 353 errors;
the remaining 128 words were translated correctly in the end-to-end system. Our analysis
reveals the error sources detailed below. In Table 13 we summarize the statistics of the error
sources.
1. Reachability of the borrowing system.
Only 368 out of 481 input words produced loanword candidates. The main reason
for the unreachable paths is complex morphology of Swahili OOVs, not taken into
account by our borrowing system. For example, atakayehusika who will be involved,
the lemma is husika involve.
2. Loanword production errors.
About half of errors are due to incorrect outputs of the borrowing system. This is
in line with the ArabicSwahili borrowing system accuracy reported in Table 6. For
example, all morphological variants of the lemma wahi never (hayajawahi, halijawahi,
hazijawahi), incorrectly produced an Arabic donor word Ag. (jAwh) java. Additional examples include all variants of the lemma saidia help (isaidie, kimewasaidia)
produced Arabic donor candidates that are variants of the proper name Saidia.
3. ArabicEnglish translation errors.
As the most frequent source of errors in the ArabicEnglish MT system, we have identified OOV Arabic words. For example, although for the Swahili loanword awashukuru

thank you the borrowing system correctly produced a plausible donor word P
(w$kwr) and thank you (rarely used), the only translation variant produced by the
ArabicEnglish MT was kochkor.
4. SwahiliEnglish translation errors.
In some cases, although the borrowing system produced a correct donor candidate,
and the ArabicEnglish translation was also correct, translation variants were different
from the reference translations in the SwahiliEnglish MT system. For example, the
24. www.aakkl.helsinki.fi/cameel/corpus/intro.htm

85

fiTsvetkov & Dyer

word alihuzunika he grieved correctly produced an Arabic donor 	 Qm	  '@ (AlHzn) grief.
Translation variants produced by the ArabicEnglish MT are sadness, grief, saddened,
sorrow, sad, mourning, grieved, saddening, mourn, distressed, whereas the expected
translation in the SwahiliEnglish reference translations is disappointed. Another
source of errors that occurred despite correct outputs of borrowing and translation
systems is historical meaning change of words. An interesting example of such semantic
	
shift is the word sakafu floor, that was borrowed from the Arabic word  (sqf )
ceiling.
Complex morphology of both Swahili and Arabic is the most frequent source of errors at
all steps of the application. Concatenation of several prefixes in Swahili affects the reachability
of the borrowing system. Some Swahili prefixes flip the meaning of words, for example
kutoadhibiwa impunity, produces the lemma adhibiwa punishment, and consequently
translations torture, torturing, tortured. Finally, derivational processes in both languages
are not handled by our system, for example, a verb aliyorithi he inherited, produces an

Arabic noun KP@@ (AlwArvp) the heiress, and its English translations heiress. Jointly
reasoning about morphological processes in the donor and recipient languages suggests a
possible avenue for remedying these issues.

6. Additional Related Work
With the exception of a study conducted by Blair and Ingram (2003) on generation of
borrowed phonemes in EnglishJapanese language pair (the method does not generalize
from borrowed phonemes to borrowed words, and does not rely on linguistic insights), we
are not aware of any prior work on computational modeling of lexical borrowing. Few
papers only mention or tangentially address borrowing, we briefly list them here. Daum III
(2009) focuses on areal effects on linguistic typology, a broader phenomenon that includes
borrowing and genetic relations across languages. This study is aimed at discovering language
areas based on typological features of languages. Garley and Hockenmaier (2012) train a
maximum entropy classifier with character n-gram and morphological features to identify
anglicisms (which they compare to loanwords) in an online community of German hip hop
fans. Finally, List and Moran (2013) have published a toolkit for computational tasks in
historical linguistics but remark that Automatic approaches for borrowing detection are
still in their infancy in historical linguistics.

7. Conclusion
Given a loanword, our model identifies plausible donor words in a contact language. We
show that a discriminative model with Optimality Theoretic features effectively models
systematic phonological changes in ArabicSwahili loanwords. We also found that the model
and methodology is generally applicable to other language pairs with minimal engineering
effort. Our translation results substantially improve over the baseline and confirm that OOV
loanwords are important and merit further investigation.
There are numerous research questions that we would like to explore further. Is it possible
to monolingually identify borrowed words in a language? Can we automatically identify a
donor language (or its phonological properties) for a borrowed word? Since languages may
86

fiCross-Lingual Bridges with Models of Lexical Borrowing

borrow from many sources, can jointly modeling this process lead to better performance?
Can we reduce the amount of language-specific engineering required to deploy our model?
Can we integrate knowledge of borrowing in additional downstream NLP applications? We
intend to address these questions in future work.

Acknowledgments
We thank Nathan Schneider, Shuly Wintner, Llus Mrquez, and the anonymous reviewers
for their help and constructive feedback. We also thank Waleed Ammar for his help with
Arabic. This work was supported in part by the U.S. Army Research Laboratory and the
U.S. Army Research Office under contract/grant number W911NF-10-1-0533, and in part by
the National Science Foundation through award IIS-1526745. Computational resources were
provided by Google in the form of a Google Cloud Computing grant and the NSF through
the XSEDE program TG-CCR110017.

References
Adler, A. N. (2006). Faithfulness and perception in loanword adaptation: A case study from
Hawaiian. Lingua, 116 (7), 10241045.
Ahn, S.-C., & Iverson, G. K. (2004). Dimensions in Korean laryngeal phonology. Journal of
East Asian Linguistics, 13 (4), 345379.
Al-Onaizan, Y., & Knight, K. (2002). Machine transliteration of names in Arabic text. In
Proc. the ACL workshop on Computational Approaches to Semitic Languages, pp. 113.
Association for Computational Linguistics.
Allauzen, C., Riley, M., Schalkwyk, J., Skut, W., & Mohri, M. (2007). OpenFst: A general and
efficient weighted finite-state transducer library. In Implementation and Application of
Automata, pp. 1123. Springer.
Ammar, W., Chahuneau, V., Denkowski, M., Hanneman, G., Ling, W., Matthews, A., Murray,
K., Segall, N., Tsvetkov, Y., Lavie, A., & Dyer, C. (2013). The cmu machine translation
systems at WMT 2013: Syntax, synthetic translation options, and pseudo-references.
In Proc. WMT.
Ammar, W., Dyer, C., & Smith, N. A. (2012). Transliteration by sequence labeling with
lattice encodings and reranking. In Proc. NEWS workshop at ACL.
Blair, A. D., & Ingram, J. (2003). Learning to predict the phonological structure of English
loanwords in Japanese. Applied Intelligence, 19 (1-2), 101108.
Boersma, P., & Hayes, B. (2001). Empirical tests of the gradual learning algorithm. Linguistic
inquiry, 32 (1), 4586.
Broselow, E. (2004). Language contact phonology: richness of the stimulus, poverty of the
base. In Proc. NELS, Vol. 34, pp. 122.
Burkett, D., & Klein, D. (2008). Two languages are better than one (for syntactic parsing).
In Proc. EMNLP, pp. 877886.
87

fiTsvetkov & Dyer

Calabrese, A., & Wetzels, W. L. (2009). Loan phonology, Vol. 307. John Benjamins
Publishing.
Callison-Burch, C., Koehn, P., & Osborne, M. (2006). Improved statistical machine translation using paraphrases. In Proc. ACL.
Cettolo, M., Girardi, C., & Federico, M. (2012). WIT3 : Web inventory of transcribed and
translated talks. In Proc. EAMT, pp. 261268.
Chahuneau, V., Schlinger, E., Smith, N. A., & Dyer, C. (2013). Translating into morphologically rich languages with synthetic phrases. In Proc. EMNLP, pp. 16771687.
Clark, J. H., Dyer, C., Lavie, A., & Smith, N. A. (2011). Better hypothesis testing for
statistical machine translation: Controlling for optimizer instability. In Proc. ACL, pp.
176181.
Comrie, B., & Spagnol, M. (2015). Maltese loanword typology. Submitted.
Das, D., & Petrov, S. (2011). Unsupervised part-of-speech tagging with bilingual graph-based
projections. In Proc. ACL, pp. 600609. Association for Computational Linguistics.
Daum III, H. (2009). Non-parametric Bayesian areal linguistics. In Proc. NAACL, pp.
593601. Association for Computational Linguistics.
Davidson, L., & Noyer, R. (1997). Loan phonology in Huave: nativization and the ranking
of faithfulness constraints. In Proc. WCCFL, Vol. 15, pp. 6579.
De Gispert, A., & Marino, J. B. (2006). Catalan-English statistical machine translation
without parallel corpus: bridging through Spanish. In Proc. LREC, pp. 6568.
Dholakia, R., & Sarkar, A. (2014). Pivot-based triangulation for low-resource languages. In
Proc. AMTA.
Diab, M., & Resnik, P. (2002). An unsupervised method for word sense tagging using parallel
corpora. In Proc. ACL.
Durrani, N., Sajjad, H., Fraser, A., & Schmid, H. (2010). Hindi-to-Urdu machine translation
through transliteration. In Pro. ACL, pp. 465474.
Dyer, C., Chahuneau, V., & Smith, N. A. (2013). A simple, fast, and effective reparameterization of IBM Model 2. In Proc. NAACL.
Dyer, C., Lopez, A., Ganitkevitch, J., Weese, J., Ture, F., Blunsom, P., Setiawan, H.,
Eidelman, V., & Resnik, P. (2010). cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models. In Proc. ACL.
Eisner, J. (1997). Efficient generation in primitive Optimality Theory. In Proc. EACL, pp.
313320.
Eisner, J. (2002). Comprehension and compilation in Optimality Theory. In Proc. ACL, pp.
5663.
Ellison, T. M. (1994). Phonological derivation in Optimality Theory. In Proc. CICLing, pp.
10071013.
Fabri, R., Gasser, M., Habash, N., Kiraz, G., & Wintner, S. (2014). Linguistic introduction:
The orthography, morphology and syntax of Semitic languages. In Natural Language
Processing of Semitic Languages, pp. 341. Springer.
88

fiCross-Lingual Bridges with Models of Lexical Borrowing

Faruqui, M., & Dyer, C. (2014). Improving vector space word representations using multilingual correlation. In Proc. EACL.
Ganchev, K., Gillenwater, J., & Taskar, B. (2009). Dependency grammar induction via bitext
projection constraints. In Proc. ACL, pp. 369377. Association for Computational
Linguistics.
Garley, M., & Hockenmaier, J. (2012). Beefmoves: dissemination, diversity, and dynamics of
English borrowings in a German hip hop forum. In Proc. ACL, pp. 135139.
Goldwater, S., & Johnson, M. (2003). Learning OT constraint rankings using a maximum
entropy model. In Proc. the Stockholm workshop on variation within Optimality Theory,
pp. 111120.
Habash, N. (2008). Four techniques for online handling of out-of-vocabulary words in
Arabic-English statistical machine translation. In Proc. ACL, pp. 5760.
Habash, N., & Hu, J. (2009). Improving Arabic-Chinese statistical machine translation using
English as pivot language. In Proc. WMT, pp. 173181.
Habash, N., Rambow, O., & Roth, R. (2009). MADA+TOKAN: A toolkit for Arabic
tokenization, diacritization, morphological disambiguation, POS tagging, stemming
and lemmatization. In Proc. MEDAR, pp. 102109.
Haghighi, A., Liang, P., Berg-Kirkpatrick, T., & Klein, D. (2008). Learning bilingual lexicons
from monolingual corpora. In Proc. ACL, pp. 771779.
Haji, J., Hric, J., & Kubo, V. (2000). Machine translation of very close languages. In
Proc. ANLP, pp. 712.
Haspelmath, M. (2009). Lexical borrowing: concepts and issues. Loanwords in the Worlds
Languages: a comparative handbook, 3554.
Haspelmath, M., & Tadmor, U. (Eds.). (2009). Loanwords in the Worlds Languages: A
Comparative Handbook. Max Planck Institute for Evolutionary Anthropology, Leipzig.
Haugen, E. (1950). The analysis of linguistic borrowing. Language, 210231.
Hayes, B., Tesar, B., & Zuraw, K. (2013). OTSoft 2.3.2..
Heafield, K. (2011). KenLM: Faster and smaller language model queries. In Proc. WMT.
Hermjakob, U., Knight, K., & Daum III, H. (2008). Name translation in statistical machine
translation-learning when to transliterate. In Proc. ACL, pp. 389397.
Hock, H. H., & Joseph, B. D. (2009). Language history, language change, and language
relationship: An introduction to historical and comparative linguistics, Vol. 218. Walter
de Gruyter.
Holden, K. (1976). Assimilation rates of borrowings and phonological productivity. Language,
131147.
Hurskainen, A. (2004a). HCS 2004Helsinki corpus of Swahili. Tech. rep., Compilers:
Institute for Asian and African Studies (University of Helsinki) and CSC.
Hurskainen, A. (2004b). Loan words in Swahili. In Bromber, K., & Smieja, B. (Eds.),
Globalisation and African Languages, pp. 199218. Walter de Gruyter.
89

fiTsvetkov & Dyer

Hwa, R., Resnik, P., Weinberg, A., Cabezas, C., & Kolak, O. (2005). Bootstrapping parsers
via syntactic projection across parallel texts. Natural Language Engineering, 11 (3).
Jacobs, H., & Gussenhoven, C. (2000). Loan phonology: perception, salience, the lexicon
and OT. Optimality Theory: Phonology, syntax, and acquisition, 193209.
Johnson, F. (1939). Standard Swahili-English dictionary. Oxford University Press.
Kager, R. (1999). Optimality Theory. Cambridge University Press.
Kang, Y. (2003). Perceptual similarity in loanword adaptation: English postvocalic word-final
stops in Korean. Phonology, 20 (2), 219274.
Kang, Y. (2011). Loanword phonology. In van Oostendorp, M., Ewen, C., Hume, E., & Rice,
K. (Eds.), Companion to Phonology. WileyBlackwell.
Kawahara, S. (2008). Phonetic naturalness and unnaturalness in Japanese loanword phonology. Journal of East Asian Linguistics, 17 (4), 317330.
Kenstowicz, M. (2007). Salience and similarity in loanword adaptation: a case study from
Fijian. Language Sciences, 29 (2), 316340.
Kenstowicz, M., & Suchato, A. (2006). Issues in loanword adaptation: A case study from
Thai. Lingua, 116 (7), 921949.
Kneser, R., & Ney, H. (1995). Improved backing-off for m-gram language modeling. In Proc.
ICASSP, Vol. 1, pp. 181184. IEEE.
Knight, K., & Graehl, J. (1998). Machine transliteration. Computational Linguistics, 24 (4),
599612.
Koehn, P., Och, F. J., & Marcu, D. (2003). Statistical phrase-based translation. In Proc.
NAACL-HLT, pp. 4854.
Kondrak, G. (2001). Identifying cognates by phonetic and semantic similarity. In Proc.
NAACL, pp. 18. Association for Computational Linguistics.
Kondrak, G., Marcu, D., & Knight, K. (2003). Cognates can improve statistical translation
models. In Proc. HLT-NAACL, pp. 4648. Association for Computational Linguistics.
Kondrak, G., & Sherif, T. (2006). Evaluation of several phonetic similarity algorithms on
the task of cognate identification. In Proc. the Workshop on Linguistic Distances, pp.
4350. Association for Computational Linguistics.
Kozhevnikov, M., & Titov, I. (2013). Cross-lingual transfer of semantic role labeling models.
In Proc. ACL, pp. 11901200.
Kuhn, J. (2004). Experiments in parallel-text based grammar induction. In Proc. ACL, p.
470.
Li, S., Graa, J. V., & Taskar, B. (2012). Wiki-ly supervised part-of-speech tagging. In Proc.
EMNLP, pp. 13891398.
List, J.-M., & Moran, S. (2013). An open source toolkit for quantitative historical linguistics.
In Proc. ACL (System Demonstrations), pp. 1318.
Littell, P., Price, K., & Levin, L. (2014). Morphological parsing of Swahili using crowdsourced
lexical resources. In Proc. LREC.
90

fiCross-Lingual Bridges with Models of Lexical Borrowing

Maamouri, M., Graff, D., Bouziri, B., Krouna, S., & Kulick, S. (2010). LDC Standard Arabic
morphological analyzer (SAMA) v. 3.1..
Mann, G. S., & Yarowsky, D. (2001). Multipath translation lexicon induction via bridge
languages. In Proc. HLT-NAACL, pp. 18.
Marton, Y., Callison-Burch, C., & Resnik, P. (2009). Improved statistical machine translation
using monolingually-derived paraphrases. In Proc. EMNLP, pp. 381390.
McCarthy, J. J. (1985). Formal problems in Semitic phonology and morphology. Ph.D. thesis,
MIT.
McCarthy, J. J. (2009). Doing Optimality Theory: Applying theory to data. John Wiley &
Sons.
McCarthy, J. J., & Prince, A. (1995). Faithfulness and reduplicative identity. Beckman et
al. (Eds.), 249384.
Metze, F., Hsiao, R., Jin, Q., Nallasamy, U., & Schultz, T. (2010). The 2010 CMU GALE
speech-to-text system. In Proc. INTERSPEECH, pp. 15011504.
Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed
representations of words and phrases and their compositionality. In Proc. NIPS, pp.
31113119.
Moravcsik, E. (1978). Language contact. Universals of human language, 1, 93122.
Mwita, L. C. (2009). The adaptation of Swahili loanwords from Arabic: A constraint-based
analysis. Journal of Pan African Studies.
Myers-Scotton, C. (2002). Contact linguistics: Bilingual encounters and grammatical outcomes. Oxford University Press Oxford.
Nakov, P., & Ng, H. T. (2012). Improving statistical machine translation for a resourcepoor language using related resource-rich languages. Journal of Artificial Intelligence
Research, 179222.
Nelder, J. A., & Mead, R. (1965). A simplex method for function minimization. Computer
journal, 7 (4), 308313.
Och, F. J. (2003). Minimum error rate training in statistical machine translation. In Proc.
ACL, pp. 160167.
Pad, S., & Lapata, M. (2009). Cross-lingual annotation projection for semantic roles.
Journal of Artificial Intelligence Research, 36 (1), 307340.
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). bleu: a method for automatic
evaluation of machine translation. In Proc. ACL, pp. 311318.
Parker, R., Graff, D., Kong, J., Chen, K., & Maeda, K. (2009). English Gigaword fourth
edition..
Polom, E. C. (1967). Swahili Language Handbook. ERIC.
Prince, A., & Smolensky, P. (2008). Optimality Theory: Constraint interaction in generative
grammar. John Wiley & Sons.
91

fiTsvetkov & Dyer

Rapp, R. (1995). Identifying word translations in non-parallel texts. In Pro. ACL, pp.
320322.
Razmara, M., Siahbani, M., Haffari, R., & Sarkar, A. (2013). Graph propagation for
paraphrasing out-of-vocabulary words in statistical machine translation. In Proc. ACL,
pp. 11051115.
Repetti, L. (2006). The emergence of marked structures in the integration of loans in Italian.
Amsterdam Studies in the Theory and History of Linguistic Science Series 4, 274, 209.
Rose, Y., & Demuth, K. (2006). Vowel epenthesis in loanword adaptation: Representational
and phonetic considerations. Lingua, 116 (7), 11121139.
Rothman, N. C. (2002). Indian Ocean trading links: The Swahili experience. Comparative
Civilizations Review, 46, 7990.
Saluja, A., Hassan, H., Toutanova, K., & Quirk, C. (2014). Graph-based semi-supervised
learning of translation models from monolingual data. In Proc. ACL, pp. 676686.
Sankoff, G. (2002). Linguistic outcomes of language contact. In Chambers, J., Trudgill, P.,
& Schilling-Estes, N. (Eds.), Handbook of Sociolinguistics, pp. 638668. Blackwell.
Schadeberg, T. C. (2009). Loanwords in Swahili. In Haspelmath, M., & Tadmor, U. (Eds.),
Loanwords in the Worlds Languages: A Comparative Handbook, pp. 76102. Max
Planck Institute for Evolutionary Anthropology.
Schlinger, E., Chahuneau, V., & Dyer, C. (2013). morphogen: Translation into morphologically
rich languages with synthetic phrases. The Prague Bulletin of Mathematical Linguistics,
100, 5162.
Schulte, K. (2009). Loanwords in Romanian. In Haspelmath, M., & Tadmor, U. (Eds.),
Loanwords in the Worlds Languages: A Comparative Handbook, pp. 230259. Max
Planck Institute for Evolutionary Anthropology.
Schultz, T., & Schlippe, T. (2014). GlobalPhone: Pronunciation dictionaries in 20 languages.
In Proc. LREC.
Smith, D. A., & Smith, N. A. (2004). Bilingual parsing with factored estimation: Using
english to parse korean.. In Proc. EMNLP, pp. 4956.
Smith, J. R., Saint-Amand, H., Plamada, M., Koehn, P., Callison-Burch, C., & Lopez, A.
(2013). Dirt cheap web-scale parallel text from the Common Crawl. In Proc. ACL, pp.
13741383.
Snyder, B., Naseem, T., & Barzilay, R. (2009). Unsupervised multilingual grammar induction.
In Proc. ACL/AFNLP, pp. 7381.
Tckstrm, O., Das, D., Petrov, S., McDonald, R., & Nivre, J. (2013). Token and type
constraints for cross-lingual part-of-speech tagging.. Transactions of the Association
for Computational Linguistics, 1, 112.
Tadmor, U. (2009). Loanwords in the worlds languages: Findings and results. In Haspelmath,
M., & Tadmor, U. (Eds.), Loanwords in the Worlds Languages: A Comparative
Handbook, pp. 5575. Max Planck Institute for Evolutionary Anthropology.
92

fiCross-Lingual Bridges with Models of Lexical Borrowing

Thomason, S. G., & Kaufman, T. (2001). Language contact. Edinburgh University Press
Edinburgh.
Tiedemann, J. (2012). Parallel data, tools and interfaces in OPUS. In Proc. of LREC, pp.
22142218.
Tiedemann, J. (2014). Rediscovering annotation projection for cross-lingual parser induction.
In Proc. COLING.
Tsvetkov, Y., Ammar, W., & Dyer, C. (2015). Constraint-based models of lexical borrowing.
In Proc. NAACL, pp. 598608.
Tsvetkov, Y., Boytsov, L., Gershman, A., Nyberg, E., & Dyer, C. (2014). Metaphor detection
with cross-lingual model transfer. In Proc. ACL, pp. 248258.
Tsvetkov, Y., & Dyer, C. (2015). Lexicon stratification for translating out-of-vocabulary
words. In Proc. ACL.
Tsvetkov, Y., Dyer, C., Levin, L., & Bhatia, A. (2013). Generating English determiners in
phrase-based translation with synthetic translation options. In Proc. WMT.
Tsvetkov, Y., Metze, F., & Dyer, C. (2014). Augmenting translation models with simulated
acoustic confusions for improved spoken language translation. In Proc. EACL, pp.
616625.
Van Coetsem, F. (1988). Loan phonology and the two transfer types in language contact.
Walter de Gruyter.
Wang, P., Nakov, P., & Ng, H. T. (2012). Source language adaptation for resource-poor
machine translation. In Proc. EMNLP, pp. 286296.
Weinreich, U. (1979). Languages in contact: Findings and problems. Walter de Gruyter.
Whitney, W. D. (1881). On mixture in language. Transactions of the American Philological
Association (1870), 526.
Wu, D. (1997). Stochastic inversion transduction grammars and bilingual parsing of parallel
corpora. Computational linguistics, 23 (3), 377403.
Xi, C., & Hwa, R. (2005). A backoff model for bootstrapping resources for non-English
languages. In Proc. EMNLP, pp. 851858.
Yarowsky, D., Ngai, G., & Wicentowski, R. (2001). Inducing multilingual text analysis tools
via robust projection across aligned corpora. In Proc. HLT, pp. 18.
Yip, M. (1993). Cantonese loanword phonology and Optimality Theory. Journal of East
Asian Linguistics, 2 (3), 261291.
Zawawi, S. (1979). Loan words and their effect on the classification of Swahili nominals.
Leiden: E.J. Brill.

93

fiJournal of Artificial Intelligence Research 55 (2016) 889-952

Submitted 9/15; published 4/16

Searching for the M Best Solutions in Graphical Models
Natalia Flerova

NFLEROVA @ UCI . EDU

University of California, Irvine
Irvine, CA 92697, USA

Radu Marinescu

RADU . MARINESCU @ IE . IBM . COM

IBM Research  Ireland

Rina Dechter

DECHTER @ UCI . EDU

University of California, Irvine
Irvine, CA 92697, USA

Abstract
The paper focuses on finding the m best solutions to combinatorial optimization problems
using best-first or depth-first branch and bound search. Specifically, we present a new algorithm mA*, extending the well-known A* to the m-best task, and for the first time prove that all its desirable
properties, including soundness, completeness and optimal efficiency, are maintained. Since bestfirst algorithms require extensive memory, we also extend the memory-efficient depth-first branch
and bound to the m-best task.
We adapt both algorithms to optimization tasks over graphical models (e.g., Weighted CSP and
MPE in Bayesian networks), provide complexity analysis and an empirical evaluation. Our experiments confirm theory that the best-first approach is largely superior when memory is available, but
depth-first branch and bound is more robust. We also show that our algorithms are competitive with
related schemes recently developed for the m-best task.

1. Introduction
The usual aim of combinatorial optimization is to find an optimal solution, minimum or maximum,
of an objective function. However, in many applications it is desirable to obtain not just a single
optimal solution, but a set of the first m best solutions for some integer m. We are motivated by
many real-life domains, in which such task arises. For instance, a problem of finding the most likely
haplotype in a pedigree can be presented as finding the most probable assignment in a Bayesian
network that encodes the genetic information (Fishelson, Dovgolevsky, & Geiger, 2005). In practice the data is often corrupted or missing, which makes the single optimal solution unreliable. It
is possible to increase the confidence in the answer by finding a set of the m best solutions and
then choosing the final solution with an expert help or by obtaining additional genetic data. More
examples of the m-best tasks arise in procurement auction problems and in probabilistic expert systems, where certain constraints often cannot be directly incorporated into the model, either because
they make the problem infeasibly complex or they are too vague to formalize (e.g. idiosyncratic
preferences of a human user). Thus in such domains it may be more practical to first find several
good solutions to a relaxed problem and then pick the one that satisfies all additional constraints in a
post-processing manner. Additionally, sometimes a set of diverse assignments with approximately
the same cost is required, as in reliable communication network design. Finally, in the context of
summation problem over graphical models, such as probability of evidence or the partition function,
an approximation can be derived by summing over the m most likely tuples.
c
2016
AI Access Foundation. All rights reserved.

fiF LEROVA , M ARINESCU , & D ECHTER

The problem of finding the m best solutions has been well studied. One of the earliest and
most influential works belongs to Lawler (1972). He provided a general scheme that extends any
optimization algorithm to the m-best task. The idea is to compute the next best solution successively
by finding a single optimal solution for a slightly different reformulation of the original problem that
excludes the solutions generated so far. This approach has been extended and improved over the
years and is still one of the primary strategies for finding the m best solutions. Other approaches
are more direct, trying to avoid the repeated computation inherent to Lawlers scheme. Two earlier
works that are most relevant and provide the highest challenge to our work are by Nilsson (1998)
and Aljazzar and Leue (2011).
 Nilsson proposed a junction-tree based message-passing scheme that iteratively finds the m
best solutions. He claimed that it has the best runtime complexity among m-best schemes for
graphical models. Our analysis (Section 6) shows that indeed Nilssons scheme has the second
best worst case time complexity after our algorithm BE+m-BF (Section 5.3). However, in
practice this scheme is not feasible for problems having a large induced width.
 In their recent work Aljazzar and Leue proposed an algorithm called K*, an A* search-style
scheme for finding the k shortest paths that is interleaved with breadth-first search. They used
a specialized data structure and it is unclear if this approach can be straightforwardly extended
to graphical models, a point that we will leave to future work.
One of the popular approximate approaches to solving optimization problems is based on the
LP-relaxation of the problem (Wainwright & Jordan, 2003). The m-best extension of this approach
(Fromer & Globerson, 2009) does not guarantee exact solutions, but is quite efficient in practice.
We will discuss these and other previous works further in Section 6.
Our main focus lies in optimization in the context of graphical models, such as Bayesian networks, Markov networks and constraint networks. However, some of the algorithms developed
can be used for more general purpose tasks, such as finding m shortest paths in a graph. Various
graph-exploiting algorithms for solving optimization tasks over graphical models were developed
in the past few decades. Such algorithms are often characterized as being either of inference type
(e.g., message-passing schemes, variable elimination) or of search type (e.g., AND/OR search or
recursive-conditioning). In our earlier works, (e.g., Flerova, Dechter, & Rollon, 2011), we extended
inference schemes as represented by the bucket elimination algorithm (BE) (Dechter, 1999, 2013) to
the task of finding the m best solutions. However, due to their large memory requirements, variable
elimination algorithms, including bucket elimination, cannot be used in practice for finding exact
solutions to combinatorial optimization tasks when the problems graph is dense. Depth-first branch
and bound (DFBnB) and best-first search (BFS) are more flexible and can trade space for time. Our
work explores the question of solving the m best solutions task using the heuristic search schemes.
Our contribution lies in extending the heuristic algorithms to the m best solutions task. We describe general purpose m-best variants of both depth-first branch and bound and best-first search,
more specifically A*, yielding algorithms m-BB and m-A* respectively, and analyze their properties. We show that m-A* inherits all A*s desirable properties (Dechter & Pearl, 1985), most
significantly it is optimally efficient compared to any alternative exact search-based scheme. We
also discuss the size of the search space explored by m-BB. We then extend our new m-best algorithms to graphical models by exploring the AND/OR search space.
We evaluate the resulting algorithms on 6 benchmarks having more than 300 instances in total,
and examine the impact of the number of solutions m on the algorithms behaviour. In particular,
890

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

we observe that the runtime of the most of the schemes (except for the depth-first branch and bound
exploring an AND/OR tree) scales much better with m than what worst case theoretical analysis
suggests.
We also show that a m-A* search using the exact bucket elimination heuristic (a scheme we
call BE+m-BF) is highly efficient on easier problems but suffers severely from memory issues
over denser graphs, far more than the A*-based schemes using approximate mini-bucket heuristics. Finally, we compare our schemes with some of the most efficient algorithms based on the
LP-relaxation (Fromer & Globerson, 2009; Batra, 2012), showing competitiveness and even superiority for large values of m (m  10), while providing optimality guarantees.
The paper is organized as follows. In Section 2 we provide relevant background. Section 3
presents the extension of best-first search to the m-best task. In particular, we define m-A*, the
extension of A* algorithm to finding the m best solutions (3.1), and prove its main properties (3.2).
Section 4 describes algorithm m-BB, an extension of depth-first branch and bound algorithm to
solving the m-best task. In Section 5 we discuss the adaptation of the two newly proposed m-best
search algorithms for AND/OR search spaces over graphical models, including a hybrid method
BE+m-BF that incorporates both variable elimination and heuristic search. Section 6 elaborates on
the related work and contrasts it with our methods. Section 7 presents the empirical evaluation of
our m-best schemes and Section 8 concludes.

2. Background
We begin by formally defining the graphical models framework and providing background on
heuristic search.
2.1 Graphical Models
We denote variables by upper-case letters (e.g., X, Y, Z) and their values of variables by lower-case
letters (e.g., x, y, z). Sets of variables are denoted by upper-case letters in bold (e.g. X, Y, Z). The
assignment (X1 = x1 , . . . , Xn = xn ) can be abbreviated as x = (x1 , . . . , xn ).
We denote functions by letters f, g, h etc., and a set of functions byPF. A function f over a
scope S1 = {X1 , . . . , Xr } is denoted by fS1 . P
The summation
P operator xX defines a sum over
all possible values of variables in X, namely x1 X1 , . . . , xn Xn . Minimization minxX and
maximization maxxX operators are defined in a similar manner. Note that we use terms elimination
P
and marginalization interchangeably.
For convenience we sometimes use minx (maxx , x ) to
P
denote minxX (maxxX , xX ).
A graphical model is a collection of local functions over subsets of variables that conveys probabilistic, deterministic, or preferential information, and whose structure is described by a graph. The
graph captures independencies or irrelevance information inherent in the model, that can be useful
for interpreting the modeled data and, most significantly, can be exploited by reasoning algorithms.
The set of local functions can be combined in a variety of ways to generate a global function, whose
scope is the set of all variables.
N
D EFINITION 1 (Graphical model). A graphical model M is a 4-tuple M = hX, D, F, i:
1. X = {X1 , . . . , Xn } is a finite set of variables;

2. D = {D1 , . . . , Dn } is the set of their respective finite domains of values;
891

fiF LEROVA , M ARINESCU , & D ECHTER

3. F = {f1 , . . . , fr } is a set of non-negative real-valued discrete functions, defined over scopes
of variables Si  X. They are called local functions.
N
N
Q P
4.
is a combination operator, e.g.,
 { , } (product, sum)
The graphical model represents
a global function, whose scope is X and which is the combination
N
of all the local functions: rj=1 fj .
N
P
When N
= Qand fi : DSi  N we have Weighted Constraint Satisfaction Problems (WCSPs). When
=
and fi = Pi (Xi | pai ) we have a Bayesian network. The probabilities P are
defined relative to a directed acyclic graph G over X, where the set Xi1 , . . . , Xik are the parents
pai of Xi , i.e. for each Xij there is an edge pointing from Xij to Xi . For illustration, consider the
Bayesian network with 5 variables whose directed acyclic graph (DAG) is given in Figure 1(a).
The most common optimization task for Bayesian network is the most probable explanation
(MPE) also known as maximum a posteriori hypothesis (MAP),1 where the goal is to compute the
optimal value
r
Y

C = max
fj (xSj )
x

j=1

and its optimizing configuration


x = argmax
x

r
Y

fj (xSj )

j=1

The related task, typical for WCSP,
is the min-sum, namely computing a minimal cost
P
P assignment (min-sum): C  = minx j fj (x) and the optimizing configuration x = argminx j fj (x).
Historically this task is also sometimes referred to as energy Q
minimization. It is equivalent to

an MPE/MAP task in the following sense: if Cmax
= maxx j fj (x) is a solution to an MPE




problem,
P then Cmax = exp (Cmin ), where Cmin is a solution to a min-sum problem Cmin =
minx j gj (x) and j, gj (x) =  log (fj (x)).
A graphical model defines the primal graph that captures dependencies between the problems
variables. It has variables as its vertices. An edge connects any two vertices whose variables appear
in the scope of the same function. An important property of a graphical model, characterizing the
complexity of its reasoning tasks is the induced width. An ordered graph is a pair (G, o) where
G is an undirected graph, and o = (X1 , . . . , Xn ) is an ordering of nodes. The width of a node
is the number of the nodes neighbors that precede it in the ordering. The width of a graph along
an ordering o is the maximum width over all nodes. An induced ordered graph is obtained from
an ordered graph as follows: nodes are processed from last to first based on o; when node Xj is
processed, all its preceding neighbors are connected. The width of an ordered induced graph along
the ordering o is called induced width along o and is denoted by w (o). The induced width of a
graph, denoted by w , is the minimal induced width over all its orderings. Abusing notation we
sometimes use w to denote the induced width along a particular ordering, when the meaning is
clear from the context.
Figure 1(b) depicts the primal graph of the Bayesian network from Figure 1(a). Figures 1(c) and
1(d) show the induced graphs of the primal graph from Figure 1(a) respectively along the orderings
1. In some communities MAP also refers to the task of optimizing a partial assignment to the variables. However, in
this paper we use MAP and MPE as interchangeable, both referring to an optimal full variable assignment.

892

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

P (A)

A

A

B

E

C

D

D

C

E

B

A

A

(c)

(d)

P (B|A)

B

C

B

C

P (C|A)

E

E

P (E|B, C)

D

D

P (D|A, B)

(a)

(b)

Figure 1: (a) A DAG of a Bayesian network, (b) its primal graph (also called moral graph),
(c) its induced graph along o = (A, E, D, C, B), and (d) its induced graph along
o = (A, B, C, D, E), example by Gogate (2009).

o = (A, E, D, C, B) and o0 = (A, B, C, D, E). The dashed lines in Figure 1(c) represent the
induced edges, namely edges that are absent from the moral graph, but were introduced in the
induced graph. We can see that the induced width along ordering o is w (o) = 4 and the induced
width along ordering o0 is w (o0 ) = 2, respectively.
2.2 Heuristic Search
Our analysis focuses on best-first search (BFS), whose behaviour for the task of finding a single
optimal solution is well understood. Assuming a minimization task, best-first search always expands
the node with the best (i.e., smallest) value of the heuristic evaluation function. It maintains a graph
of explored paths, a list CLOSED of expanded nodes and a frontier of OPEN nodes. BFS chooses
from OPEN a node n with the smallest value of a heuristic evaluation function f (n), expands it by
generating its successors succ(n), places it on CLOSED, and places succ(n) in OPEN. The most
popular variant of best-first search, A*, uses the heuristic evaluation function f (n) = g(n) + h(n),
where g(n) is the cost of the path from the root to n, and h(n) is a heuristic function that estimates
the optimal cost to go h (n) from n to a goal node. A heuristic function is called admissible if
it never overestimates (for minimization) the true minimal cost to reach the goal h (n). Namely,
n h(n)  h (n). A heuristic is called consistent or monotonic, if for every node n and for every
successor n0 of n the following inequality holds: h(n)  c(n, n0 ) + h(n0 ). If h(n) is consistent,
then the values of evaluation function f (n) along any path are non-decreasing. It is known that
regardless of the tie-breaking rule A* expands any node n reachable by a strictly C  -bounded path
from the root, and such a node is referred to as surely expanded by A* (Dechter & Pearl, 1985).
A path  is C  -bounded relative to f , if n   : f (n) < C  , where C  is the cost of optimal
solution.
A* search has a number of attractive properties (Nillson, 1980; Pearl, 1984; Dechter & Pearl,
1985):
893

fiF LEROVA , M ARINESCU , & D ECHTER

 Soundness and completeness: A* terminates with the optimal solution.
 When h is consistent, A* explores only the set of nodes S = {n|f (n)  C  } and it surely
expands all the nodes having S = {n|f (n) < C  }.
 Optimal efficiency under consistent heuristic: When h is consistent, any node surely expanded by A* must be expanded by any other sound and complete search algorithm using the
same heuristic information.
 Optimal efficiency for node expansions: When the heuristic function is consistent, A*,
when searching a graph, expands each node at most once, and at the time of nodes expansion
A* has found the shortest path to it.
 Dominance: Given two heuristic functions h1 and h2 , s.t. n h1 (n) < h2 (n), A1 will expand
every node surely expanded by A2 , where Ai uses heuristic hi .
Although best-first search is known to be the best algorithm in terms of number of nodes expanded (Dechter & Pearl, 1985), it requires exponential memory in the worst-case.
A popular alternative is the depth-first branch and bound (DFBnB), whose most attractive feature, compared to best-first search, is that it can be executed with linear memory. Yet, when the
search space is a graph, it can exploit memory to improve its performance by flexibly trading space
and time. Depth-first branch and bound expands nodes in a depth-first manner, maintaining the cost
of the best solution found so far which is an upper bound U B on the cost of the optimal solution.
If the heuristic evaluation function of the current node n is greater or equal to the upper bound, the
node is pruned and the subtree below it is never explored. In the worst case depth-first branch and
bound explores the entire search space. In the best case the first solution found is optimal, in which
case its performance can be as good as BFS. However, if the solution depth is unbound, depth-first
search might follow an infinite branch and never terminate. Also, if the search space is a graph,
DFBnB may expand nodes numerous time, unless it uses caching and checks for duplicates.
2.3 Search in Graphical Models
Search algorithms provide a way to systematically enumerate all possible assignments of a given
graphical model. Optimization problems over graphical models can be naturally presented as the
task of finding an optimal cost path in an appropriate search space.
The simplest variant of a search space is the so-called OR search tree. Each level corresponds to
a variable from the original problem. The nodes correspond to partial variable assignments and the
arc weights are derived from problems input functions. The size of such a search tree is bounded
by O(k n ), where n is the number of variables and k is the maximum domain size.
Throughout this section we are going to illustrate the concepts using an example problem with
six variables {A, B, C, D, E, F } and six pairwise functions. Its primal graph is shown in Figure
2(a). Figure 2(b) displays the OR search tree corresponding to the lexicographical ordering.
2.3.1 AND/OR S EARCH S PACES
OR search trees are blind to the problem decomposition encoded in graphical models and can therefore be inefficient. They do not exploit the independencies in the model. AND/OR search spaces
894

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

(a) Primal
graph

(b) OR search tree along ordering A, B, C, D, E, F

Figure 2: An example problem with 6 variables {A, B, C, D, E, F } and 5 pairwise functions.
for graphical models have been introduced to better capture the problem structure (Dechter & Mateescu, 2007). The AND/OR search space is defined relative to a pseudo tree of the primal graph
that captures problem decomposition. Figure 3(a) shows the pseudo tree of the example problem.
D EFINITION 2. A pseudo tree of an undirected graph G = (V, E) is a directed rooted tree T =
(V, E 0 ), such that every arc of G not included in E 0 is a back-arc in T , namely it connects a node
in T to an ancestor in T . The arcs in E 0 may not all be included in E.
Given a graphical model M = hX, D, Fi with primal graph G and a pseudo tree T of G, the
AND/OR search tree ST contains alternating levels of OR and AND nodes. Its structure is based
on the underlying pseudo tree T . The root node of ST is an OR node labelled by the variable at the
root of T . The children of an OR node Xi are AND nodes labelled with value assignments hXi , xi i
(or simply hxi i). The children of an AND node hXi , xi i are OR nodes labelled with the children
of Xi in T , representing conditionally independent subproblems. An AND/OR tree corresponding
to the pseudo tree in Figure 3(a) is shown in Figure 3(b). The arcs from nodes Xi to hXi , xi i in an
AND/OR search tree are annotated by the weights derived from the cost functions in F:
D EFINITION 3 (arc weight). The weight w(Xi , xi ) of the arc (Xi , hXi , xi i) is the combination (i.e.
sum for WCSP and product for MPE) of all the functions, whose scope includes Xi and is fully
assigned along the path from root to the node corresponding to hXi , xi i, evaluated at all values
along the path.
Some identical subproblems can be identified by their context (namely, a partial instantiation of
their ancestors that separates the subproblem from the rest of the problem graph), can be merged,
yielding an AND/OR search graph (Dechter & Mateescu, 2007). Merging all context-mergeable
nodes yields the context-minimal AND/OR search graph, denoted by CT . An example can be seen in
Figure 3(c). The size of the context-minimal AND/OR search graph can be shown to be exponential
in the induced width of G along the pseudo tree T (Dechter & Mateescu, 2007).
A solution tree T of CT is a subtree such that: (1) it contains the root node of CT ; (2) if an
internal AND node n is in T , then all its children are in T ; (3) if an internal OR node n is in T ,
then exactly one of its children is in T ; (4) every tip node in T (i.e., nodes with no children) is a
terminal node. The cost of a solution tree is the product, for MPE or sum for WCSP, of the weights
associated with its arcs.
Each node n in CT is associated with a value v(n) capturing the optimal solution cost of the
conditioned subproblem rooted at n. Assuming an MPE/MAP problem, it was shown that v(n) can
895

fiF LEROVA , M ARINESCU , & D ECHTER

A

OR
AND

0

1

OR

B

B

AND

0

1

0

1

A

C

OR

B
C

E

D

F

AND

0

OR

C

E
1

0

1

D D

F

F

0

AND 0 1 0 1 0 1 0 1

E

C

1

0

1

D D

F

F

0

0 1 0 1 0 1 0 1

(a) Pseudo tree

1

0

1

D D

F

F

0

E
1

0

1

D D

F

F

0 1 0 1 0 1 0 1

0 1 0 1 0 1 0 1

(b) AND/OR search tree

A

OR
AND

0

1

OR

B

B

AND

0

1

C

C

OR
AND

C

E

0

1

0

0

C

C
1

0

1

0

1

E
1

0

E
1

0

E

E
1

0

1

0

1

OR

D

D

D

D

F

F

F

F

AND

0 1

0 1

0 1

0 1

0 1

0 1

0 1

0 1

(c) Context-minimal AND/OR search graph

Figure 3: AND/OR search spaces for graphical models.

be computed recursively based on the values of ns successors: OR nodes by maximization, AND
nodes by multiplication. For WCSPs, v(n) for OR and AND nodes is updated by minimization and
by summation, respectively (Dechter & Mateescu, 2007).
We next provide an overview of a depth-first branch and bound and best-first search algorithms,
that explore AND/OR search spaces (Marinescu & Dechter, 2009b, 2009a; Otten & Dechter, 2011).
These schemes use heuristics generated either by the mini-bucket elimination scheme (2.3.4) or
through soft arc-consistency schemes (Marinescu & Dechter, 2009a, 2009b; Schiex, 2000; Darwiche, Dechter, Choi, Gogate, & Otten, 2008) or their composite (Ihler, Flerova, Dechter, & Otten,
2012). As it is customary in the heuristic search literature, when defining search algorithms we
assume without loss of generality a minimization task (i.e., min-sum optimization problem).
896

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

Algorithm 1: AOBF exploring the AND/OR search tree (Marinescu & Dechter, 2009b)

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23

24
25
26
27
28
29
30
31
32

Input: A graphical model M = hX, D, Fi, pseudo tree T rooted at X1 , heuristic function h()
Output: Optimal solution to M
Create root OR node s labelled by X1 and let G (explored search space) = {s};
Initialize v(s) = h(s) and best partial solution tree T  to G;
while s is not SOLVED do
Select non-terminal tip node n in T  . If there is no such node then break;
if n is an OR node labeled Xi then
forall the xi  D(Xi ) do
Create AND child n0 = hXi , xi i;
if n is TERMINAL then
Mark n0 SOLVED;
succ(n)  succ(n)  n0 ;
else if n is an AND node labeled hXi , xi i then
forall the successor Xj of Xi in T do
Create OR child n0 = Xj ;
succ(n)  succ(n)  n0 ;
Initialize v(n0 ) = h(n0 ) for all new nodes;
Add new nodes to the explores search space G  G  {succ(n)};
Let S  {n};
while S 6=  do
Let p be a node in S that has no descendants in G still in S;
S  S  {p};
if p is OR node then
v(p) = minksucc(p) (w(p, k) + v(k));
Mark best successor k of OR ancestors p, such that k = arg minksucc(p) (w(p, k) + v(k))
(maintaining previously marked successor if still best);
Mark p as SOLVED if its best marked successor is solved;
else if p is AND
P node then
v(p) = ksucc(p) v(k);
Mark all arcs to the successors;
Mark p as SOLVED if all its children are SOLVED;
if p changes its value or p is marked SOLVED then
Add to S all those parents of p such that p is one of their successors through a marked arc;
Recompute T  by following marked arcs from the root s;
return hv(s), T  i;

2.3.2 B EST-F IRST AND/OR S EARCH
The state-of-the-art version of best-first search for AND/OR search spaces for graphical models
is the Best-First AND/OR search algorithm (AOBF) (Marinescu & Dechter, 2009b). AOBF is a
variant of AO* (Nillson, 1980) that explores the context-minimal AND/OR search graph.
AOBF is described by Algorithm 1. For simplicity, we present the algorithm for traversing
an AND/OR search tree. AOBF maintains the explicated part of the search space G and also keeps
track of the current best partial solution tree T  . It interleaves iteratively a top-down node expansion
step (lines 4-16), which selects a non-terminal tip node of T  and generates its children in G, with
a bottom-up cost revision step (lines 17-30), which updates the values of the internal nodes based
on their childrens values. If a newly generated child node is terminal it is marked solved (line 9).
897

fiF LEROVA , M ARINESCU , & D ECHTER

During the bottom-up phase, OR nodes that have at least one solved child and AND nodes who have
all children solved are also marked as solved. The algorithm also marks the arc to the best AND
child of an OR node through which the minimum is achieved (line 23). Following the backward
step, a new best partial solution tree T  is recomputed (line 31). AOBF terminates when the root
node is marked solved. If the heuristic used is admissible, at the point of termination T  is the
optimal solution with cost v(s), where s is the root node of the search space.
Extending the algorithm to explore the context-minimal AND/OR search graph is straightforward and can be done as follows. When expanding a non-terminal AND node in lines 11-14, AOBF
does not generate the corresponding OR children that are already present in the explicated search
space G but rather links to them. All these identical OR nodes in G are easily recognized based on
their contexts (Marinescu & Dechter, 2009b).
T HEOREM 1 (complexity, Marinescu & Dechter, 2009b). Algorithm AOBF traversing the context
minimal AND/OR graph has time and space complexity of O(n  k w ), where n is the number of
variable in the problem, w is the induced width of the pseudo tree and k bounds the domain size.
2.3.3 D EPTH -F IRST AND/OR B RANCH AND B OUND
The depth-first AND/OR Branch and Bound (AOBB) (Marinescu & Dechter, 2009a) algorithm
traverses the AND/OR search space in a depth-first rather than a best-first manner, while keeping
track of the current upper bound on the minimal solution cost.
As before and for simplicity, we present the variant of the algorithm that explores an AND/OR
search tree. AOBB described by Algorithm 2 interleaves forward node expansion (lines 4-17) with
a backward cost revision (or propagation) step (lines 19-29) that updates node values (capturing
the current best solution to the subproblem rooted at each node), until search terminates and the
optimal solution has been found. A node n will be pruned (lines 12-13) if the current upper bound
is higher than the nodes heuristic lower bound, computed recursively using the procedure described
in Algorithm 3.

In the worst case, AOBB explores the entire search space, namely O(n  k w ) nodes (assuming
a context-minimal AND/OR search graph). In practice, however, AOBB is likely to expand more
nodes than AOBF using the same heuristic, but the empirical performance of AOBB depends heavily
on the order in which the solutions are encountered, namely on how quickly the algorithm finds a
close to optimal solution that it will use as an upper bound for pruning.
2.3.4 M INI -B UCKET H EURISTICS
The AND/OR search algorithms presented (AOBF and AOBB) most often use the mini-bucket
(also known as MBE) heuristic h(n). Mini-Bucket Elimination or MBE (Dechter & Rish, 2003)
is an approximate version of an exact variable elimination algorithm called bucket elimination (BE)
(Dechter, 1999). MBE (Algorithm 4) bounds the space and time complexity of full bucket elimination (which is exponential in the induced width w ). Given a variable ordering, the algorithm
associates each variable Xi with a bucket which contains all functions defined on this variable,
but not on higher index variables. Large buckets are partitioned into smaller subsets, called minibuckets, each containing at most i distinct variables. The parameter i is called the i-bound. The
algorithm processes buckets them from last to first (lines 2-10 in Algorithm 4). The mini-buckets of
the same variable are processed separately. Assuming a min-sum problem, MBE calculates the sum
898

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

Algorithm 2: AOBB exploring the AND/OR search tree (Marinescu & Dechter, 2009b)

1
2
3
4
5
6
7
8
9
10
11

12
13

Input: A graphical model M = hX, D, Fi, pseudo tree T rooted at X1 , heuristic function h();
Output: Optimal solution to M
Create root OR node s labelled by X1 and let the stack of created but not expanded nodes OP EN  {s};
Initialize v(s)   and best partial solution tree rooted in s T  (s)  ; U B  ;
while OP EN 6=  do
Select top node n in OPEN;
if n is OR node labeled Xi then
foreach xi  D(Xi ) do
Add AND child n0 labeled hXi , xi i to list of successors of n;
Initialize v(n0 ) = 0, best partial solution tree rooted in n T  (n0 ) = ;
if n is AND node labelled hXi , xi i then
foreach OR ancestor k of n do
Recursively evaluate the cost of the partial solution tree rooted in k, based on heuristic function
h(), assign its cost to f (k); // see Algorithm 3
if evaluated partial solution is not better than current upper bound at k (e.g. f (k)  v(k) then
Prune the subtree below the current tip node n;
else

14

foreach successor Xj of Xi  T do
Add OR child n0 labeled Xj to list of successors of n;
Initialize v(n0 )  , best partial solution tree rooted in n, T  (n0 )  ;

15
16
17

18
19
20
21
22
23
24
25
26
27

28
29

Add successors of n on top of OPEN;
while list of successors of node n is empty do
if node n is the root node then
return solution: v(n), T  (n) ;
else
if p is AND node then
v(p)  v(p) + v(n), T  (p)  T  (p)  T  (n);
else if p is OR node then
if the new value of better than the old one, e.g. v(p) > (c(p, n) + v(n)) for minimization then
v(p)  w(p, n) + v(n), T  (p)  T  (p)  hxi , Xi i;
Remove n from the list of successors of p;
Move one level up: n  p;

of the functions in each mini-bucket and eliminates its variable using the min operator (line 9). The
new function is placed in the appropriate lower bucket (line 10). MBE generates a bound (lower for
minimization and upper for maximization) on the optimal value. Higher i values take more computational resources, but yield more accurate bounds. When i is large enough (i.e., i  w ), MBE
coincides with full Bucket Elimination.
T HEOREM 2 (complexity, Dechter & Rish, 2003). Given a graphical model with variable ordering
o having induced width w (o) and an i-bound parameter i, the time of the mini-bucket algorithm


MBE(i) is O(nk min(i,w (o))+1 ) and space complexity is O(nk min(i,w (o)) ), where n is the number
of problem variables and k is the maximum domains size.
Mini-bucket elimination can be viewed as message passing from leaves to root along a minibucket tree. A mini-bucket tree of a graphical model M has the mini-buckets as its nodes. BucketX
899

fiF LEROVA , M ARINESCU , & D ECHTER

Algorithm 3: Recursive computation of the heuristic evaluation function
1

2
3
4

function evalPartialSolutionTree(T (n), h(n))
Input: Partial solution subtree T (n) rooted at node n, heuristic function h(n);
Output: Heuristic evaluation function f (T (n));
if succ(n) ==  then
if n is an AND node then
return 0;
else

5

return h(n);

6
7
8
9
10
11
12
13

else
if n is an AND node then
let k1 , . . . , kl be the OR children of n;
P
return li=1 evalPartialSolutionTree(T (ki ), h(ki ));
else if n is an OR node then
let k be the AND child of n;
return w(n, k) + evalPartialSolutionTree(T (k), h(k));

Algorithm 4: Mini-Bucket Elimination

1

2
3
4
5
6
7
8
9
10

11

12

Input: A model M = hX, D, Fi, ordering o, parameter i
Output: Approximate solution to M, and the ordered augmented buckets
Initialize: Partition the functions in F into Bucket1 , . . . , Bucketn , where Bucketi contains all functions
whose highest variable is Xi .
// Backward pass
for p  n downto 1 do
Let h1 , . . . , hj be the functions (original and intermediate) in Bucketp ; let S1 , . . . , Sj be their scopes;
if Xp is instantiated (Xp = xp ) then
Assign Xp = xp to each hi and put each resulting function into its appropriate bucket;
else
Generate an i-partitioning;
foreach Qk  Q0 do
P
Generate the message function hkb : hkb = minxp Xp ji=1 hi ;
Add hkb to the bucket of Xb , the largest-index variable in scope(hkb );
// Forward pass
Assign a value to each variable in the ordering o so that the combination of the functions in each bucket is
minimal;
return The function computed in the bucket of the first variable and the corresponding assignment;

is a child of BucketY is the function hXY generated in BucketX when variable X is eliminated,
is placed in BucketY . Therefore, every vertex other than the root has one parent and possibly
several child vertices. Note that a mini-bucket tree corresponds to a pseudo tree, where the minibuckets of the same variables are combined to form what we call augmented buckets, corresponding
to variable nodes (Dechter & Mateescu, 2007).
Mini-bucket elimination is often used to generate heuristics for search algorithms over graphical
models, formulated for OR search spaces by Kask and Dechter (1999a, 1999b) and extended to
AND/OR search by Marinescu and Dechter (2005).
900

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

D EFINITION 4 (MBE heuristic for the AND/OR search space, Marinescu & Dechter, 2005). Given
an ordered set of augmented buckets {B(X1 ), . . . , B(Xn )} generated by the Mini-Bucket algorithm
M BE(i) along the bucket tree T , and given a node n in the AND/OR search tree, the static minibucket heuristic function h(n) is computed as follows:
1. If n is an AND node, labeled by hXp , xp i, then:
X
h(n) =
hkj
hkj {B(Xp )B(Xp1 ...Xpq )}

Namely, it is the sum of the intermediate functions hkj that satisfy the following two properties:
 they are generated in buckets B(Xk ), where Xk is any descendant of Xp in the bucket tree T
 they reside in bucket B(Xp ) or the bucket B(Xp1 . . . Xpq ) = {B(Xp1 ), . . . , B(Xpq )} that correspond to the ancestors {Xp1 ), . . . , Xpq } of Xp in T
2. If n is an OR node, labeled by Xp , then:
h(n) =

min

msucc(p)

(w(n, m) + h(m))

where m are the AND children of n labeled with values xp of Xp .
Having established the necessary background, we will now turn to the main part of the paper,
presenting our contributions, beginning with the extension of best-first search to the m-best task.
As it is customary in the heuristic search literature and without loss of generality, we will assume in
the remaining of the paper a min-sum optimization problem.

3. Best-First Search for Finding the M Best Solutions
Extending best-first search (Section 2.2) and in particular its most popular version, A*, to the mbest task is fairly straightforward and was suggested, for example, by Charniak and Shimony (1994).
Instead of stopping after finding the optimal solution, the algorithm continues exploring the search
space, reporting the next discovered solutions up until m of them are obtained. We will show that
these solutions are indeed the m best and that they are found in a decreasing order of their optimality.
In particular, the second solution reported is the second best solution and, in general, the ith solution
discovered is the ith best.
3.1 m-A*: Definition
The m-best tree-search variant of A* denoted m-A* (Algorithm 5, assumes a consistent heuristic)
solves an m-best optimization problem over any general search graph. We will show later how it
can be extended to general admissible heuristics.
The scheme expands the nodes in the order of increasing value of f in the usual A* manner.
It keeps the lists of created nodes OPEN and expanded nodes CLOSED, as usual, maintaining a
search tree, denoted by T r. Beginning with the start node s, m-A* picks the node with the smallest
evaluation function f (n) in OPEN and puts it in CLOSED (line 7). If the node is a goal, a new
solution is reported (lines 8-13). Otherwise, the node is expanded and its children are created (lines
15-23). The algorithm may encounter each node multiple times and will maintain up to m of its
901

fiF LEROVA , M ARINESCU , & D ECHTER

Algorithm 5: m-A* exploring a graph, assuming consistent heuristic

1
2
3
4
5
6
7

8
9

10
11

Input: An implicit directed search graph G = (N, E), with a start node s and a set of goal nodes Goals, a
consistent heuristic evaluation function h(n), parameter m
Output: the m best solutions
Initialize: OPEN=, CLOSED=, a tree T r = , i = 1 (i counts the current solution being searched for)
OPEN  {s}; f (s) = h(s);
Make s the root of T r;
while i  m do
if OPEN is empty then
return the solutions found so far;
Remove a node, denoted n, in OPEN having a minimum f (break ties arbitrarily, but in favour of goal nodes
and deeper nodes) and put it in CLOSED;
if n is a goal node then
Output the current solution obtained by tracing back pointers from n to s (pointers are assigned in step
22); denote this solution as Soli ;
if i = m then
return;
else

12

i  i + 1;

13
14
15
16
17
18
19
20
21
22
23

24

else
Expand node n, generating all its children Ch ;
foreach n0  Ch do
if n0 already appears in OPEN or CLOSED m times then
Discard node n0 ;
else
Compute current path cost g(n0 ) = g(n) + c(n, n0 );
Compute evaluation function f (n0 ) = g(n0 ) + h(n0 ) ;
Attach a pointer from n0 back to n in T r;
Insert n0 into the right place in OPEN based on f (n0 );
return The set of the m best solutions found

copies in the OPEN and CLOSED lists combined (line 17), with separate paths to each copy in the
explored search tree (lines 22-23). Nodes encountered beyond m times are discarded (line 18). We
denote by Ci the ith best solution cost, by fi (n) the cost of the ith best solution going through node
n, by fi (n) the heuristic evaluation function estimating fi (n) and by gi (n) and hi (n) the estimates
of the ith best costs from s to n and from n to a goal, respectively.
If the heuristic is not consistent, whenever the algorithm reaches a node it has seen before (if
the search space is a graph and not a tree), there exists a possibility of the new path improving on
the previously discovered ones. Therefore, lines 17-18 should be revised in the following way to
account for the possibility that a better path to n0 is discovered:
17
If n0 appears already more than m times in the union of OPEN or CLOSED then
18
If g(n0 ) is strictly smaller than gm (n0 ), the current m-best path to n0 then
19
Keep n0 with a pointer to n and put n back in OPEN
20
Discard the earlier subtree rooted at n
Figure 4 shows an example of m-A* for finding the m = 3 shortest paths on a toy problem. The
left hand side of Figure 4 shows the problem graph with 7 variables and 8 edges, together with the
902

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

A
3

h(A) = 5

2

h(B) = 4

A

order in which
nodes are expanded

#1

h(C) = 2

B

#5

C

B f =6

h(D) = 3

C2
C3

1

2

#8

D

D2

4

#2

m=3
C1 = 8

C f =4

= 10
= 10

#3
D1 f = 6

f =8

2
h(F ) = 1

h(E) = 1

E

F
3

#10 f = 9 #9
E2
F2 f = 8

3
G

h(G) = 0

G4
f = 12

#12
G3

#7 f = 6 #4
F1

f = 8 E1

#11
G2

f = 10

f = 10

#6
G1
f =8

nodes on CLOSED

(a) Problem graph

(b) Trace of m-A*

Figure 4: Example problem. On the left: problem graph and heuristic values h(n) for each node.
On the right: trace of m-A* for finding m = 3 best solutions and evaluation function f (n)
for each node. White nodes are in CLOSED, the grey one was created, but discarded.

admissible heuristic functions for each node. Note that the heuristic is not consistent. For example,
h(A) > h(C) + c(A, C). A is the start node, G is the goal node. On the right side of the Figure
we present the trace of m-A*, with the evaluation function for each copy of the nodes created by
the time the 3rd solution is found. The white nodes are in CLOSED, the grey one (node G4 ) was
created, but never put in OPEN. The algorithm expands the nodes in OPEN in increasing order of
the evaluation functions. We assume that the ties are broken in favour of deeper nodes. First, m-A*
discovers the solution A  C  D  F  G with cost C1 = 8, next the solution A  C  D  E  G
with cost C1 = 10 is found. The third solutions is A  B  D  F  G with cost C1 = 10. Note
that two copies of each node D, E and F and four copies of G were created. The goal node G4 was
discarded, because we bound the total number of copies of a particular node by m = 3.
N
T HEOREM 3. Given a graphical model M = hX, D, F, i with n variables whose domain size is
bounded by k, the worst case time and space complexity of m-A* exploring an OR search tree of M
is O(k n ).
Proof. In worst case m-A* would explore the entire OR search tree, whose size is O(k n ) (Section 2.3). Since the underlying search space is a tree, the algorithm will never encounter any of the
nodes more than once, thus no nodes will be duplicated.
903

fiF LEROVA , M ARINESCU , & D ECHTER

3.2 Properties of m-A*
In this section we extend the desirable properties of A*, listed in Section 2.2, to the m-best case. For
simplicity and without loss of generality, we assume throughout that the search graph accommodates
at least m distinct solutions.
T HEOREM 4. Given an optimization task, its implicit directed search graph G and some integer
parameter m  1, m-A* guided by an admissible heuristic has the following properties:
1. Soundness and completeness: m-A* terminates with the m best solutions generated in order
of their costs.
2. Optimal efficiency under consistent heuristic: Any node that is surely expanded2 by m-A*
must be expanded by any other search algorithm traversing G that is guaranteed to find the
m best solutions having the same heuristic information.
3. Optimal efficiency for node expansions: m-A* expands each node at most m times when the
heuristic is consistent. The ith path found to a node is the ith best path.
4. Dominance: Given two heuristic functions h1 and h2 , such that for every n h1 (n) < h2 (n),
m-A*1 will expand every node surely expanded by m-A*2 , when m-A*i is using heuristic hi .
We prove the properties of m-A* in Sections 3.2.1-3.2.2.
3.2.1 S OUNDNESS AND C OMPLETENESS
Algorithm m-A* maintains up to m copies of each node and discards the rest. We will next show
that this restriction does not compromise completeness.
P ROPOSITION 1. Any node discarded by m-A* does not lead to any of the m-best solutions.
Proof. Consider a consistent heuristic first (as described in Algorithm 5). At the moment when
m-A* discovered a node n for the (m + 1)th time, m copies of n reside on OPEN or CLOSED
and the algorithm maintains m distinct paths to each. Let m be the (m + 1)th path. As we will
prove in Theorem 10, when node n is discovered for the (m + 1)th time, the cost Cnew of the newly
discovered path new is the (m + 1)th best, namely it is no better than the costs already discovered:
Cnew  Cm . Therefore, the eliminated (m + 1)th path to node n is guaranteed to be worse than
the remaining m ones and thus can not be a part of any of the potential m-best optimal solutions
that might be passing through node n.
If the heuristic is not consistent, m-A* can be modified to replace the worst of the previously
discovered paths m with the newly found new , if the cost of the latter is better and place the new
copy in OPEN. Thus, again, it is safe to bound the number of copies by m.
It is clear that along any particular solution path  the evaluation function over all the nodes on
 is bounded by the paths cost C(), when the heuristic is admissible.
P ROPOSITION 2. The following is true regarding m-A*:
1. For any solution path , forall n  , f (n)  C().
2. To be precisely defined in Section 3.2.3

904

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

2. Unless  was already discovered by m-A*, there is always a node n on  which resides in
OPEN.
3. Therefore, as long as m-A* did not discover  there must be a node in OPEN having f (n) 
C().
Proof. 1. f (n) = g (n) + h(n) and since h(n)  c (n, t) due to admissibility, where c (n, t) is
the actual cost from n to the goal node t along , we conclude that f (n)  g (n) + h(n) = C().
2. Any reachable path from the root always has a leaf in OPEN unless all the nodes along the path
are expanded and are in CLOSED.
3. Follows easily from 1 and 2.
It follows immediately from Proposition 2 (stated similarly by Nilsson, 1982) that:
P ROPOSITION 3. [Necessary condition for node expansion] Any node n expanded during m-A*
when searching for the ith best solution (1  i  m) satisfies f (n)  Ci .
and it is also clear that
P ROPOSITION 4. [Sufficient condition for node expansion] Every node n in OPEN, such that
f (n) < Ci , must be expanded by m-A* before the ith best solution is found.
Soundness and completeness of m-A* follows quite immediately.
T HEOREM 5 (soundness and completeness). Algorithm m-A* generates the m-best solutions in
order, namely, the ith solution generated is the ith best solution.
Proof. Let us assume by contradiction that this is not the case. Let the ith generated solution path
i be the first one that is not generated according to the best-first order. Namely the ith solution
generated has a cost C such that C > Ci . However, when the algorithm selected the goal ti along
i , its evaluation function was f (ti ) = gi (ti ) = C, while, based on Proposition 2, there was a
node n0 in OPEN whose evaluation function was at most Ci . Thus n0 should have been selected for
expansion instead of ti . We have a contradiction and therefore the result follows.
3.2.2 T HE I MPACT OF THE H EURISTIC S TRENGTH
Like for A*, the performance of m-A* improves with more accurate heuristic.
P ROPOSITION 5. Consider two heuristic functions h1 and h2 . Let us denote by m-A*1 the algorithm that uses heuristic h1 and by m-A*2 the one using heuristic h2 . If the heuristic h1 is
more informed than h2 , namely for every node n, h2 (n) < h1 (n), algorithm m-A*2 will expand
every node that will be expanded by the algorithm m-A*1 before finding the j th solution for any
j  {1, 2, . . . , m}, assuming the same tie-breaking rule.
Proof. Since h1 is more informed than h2 , h1 (n) > h2 (n) for every non-goal node n. Let us
assume that m-A*1 expands some non-terminal node n before finding the j th best solution with
cost Cj . If node n is expanded, it means that (a) at some point it is on OPEN and (b) its evaluation
function satisfies f1 (n) = g(n) + h1 (n)  Cj (Proposition 3). Consider the current path  from
start node to n. Each node n0   on the path was selected at some point for expansion and thus
905

fiF LEROVA , M ARINESCU , & D ECHTER

m-A search space

search space of
C1 Cm
any other algorithm
Ci


|Cm
 C1 |

search space
explored by m-A
compared to A

Figure 5: The schematic representation of the search spaces explored by the m-A* algorithm, de
pending on m and cost Cm

the evaluation functions of all these nodes are also bounded by the cost of the j th best solution:
f1 (n0 )  Cj . Since h1 (n0 ) > h2 (n0 ) for every node n0 along the path , their evaluation functions
according to heuristic h2 (n) obey:
f2 (n0 ) = g(n0 ) + h2 (n0 ) < g(n0 ) + h1 (n0 ) < Cj

(1)

and thus each node n0 must also be expanded by m-A*2 .
Consider the case of the exact heuristic. It is easy to show that:
T HEOREM 6. If h = h is the exact heuristic, then m-A* generates solutions only on j-optimal
paths 1  j  m.
Proof. Since h is exact, the f values on OPEN are expanded in sequence of values C1  C2 
 . All the generated nodes having evaluation function f = C  are by definition
. . .  Ci . . .  Cm
1
on optimal paths (since h = h ), all those who have f = C2 must be on paths that can be second
best and so on. Notice that some solutions can have the same costs.
When h = h , m-A*s complexity is clearly linear in the number of nodes having evaluation
 . However, when the cost function has only a small range of values, there may be
function f   Cm
 . To avoid this exponential frontier we
an exponential number of solution paths having the cost Cm
chose the tie-breaking rule of expanding deeper nodes first, yielding a number of node expansions
bounded by m  n, when n bounds the solution length. Clearly:

T HEOREM 7. When m-A* has access
favour of deeper
P to h = h , then, using a tie-breaking rule in
th
nodes, it expands at most #N = i #Ni nodes, where #Ni is the length of the i optimal solution
path. Clearly, #N  m  n.

906

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

Figure 6: The graph G0 represents a new problem instance constructed by appending a branch leading to a new goal node t to node n.

3.2.3 M -A* WITH C ONSISTENT H EURISTIC
When m-A* uses a consistent heuristic, it has several useful properties.
Optimal efficiency under consistent heuristic. Algorithm A* is known to be optimally efficient
for consistent heuristic (Dechter & Pearl, 1985). Namely, any other algorithm that extends search
paths from the root and uses the same heuristic information as A* will expand every node that is
surely expanded by A*, i.e., it will expand every n, such that f (n) < C  . We extend the notion of
nodes surely expanded by A* to the m-best case:
 -bounded
P ROPOSITION 6. Algorithm m-A* will expand any node n reachable by a strictly Cm
path from the root, regardless of the tie-breaking rule. The set of such nodes is referred to as surely
expanded by m-A*.
 -bounded path  = {s, n , n , . . . n}. The start node s is
Proof. Let us consider the strictly Cm
1 2
clearly expanded at the beginning of the search and its children, including node n1 , are placed
 , node n must be expanded by m-A* before finding the mth best
on OPEN. Since f (n1 ) < Cm
1
solution (Proposition 4), its children, including n2 , in turn are placed in OPEN. The same is true for
all nodes of , including n.

T HEOREM 8 (m-optimal efficiency). Any search algorithm, that is guaranteed to find the m-best
solutions and that explores the same search space as m-A* and has the same consistent heuristic,
will have to expand any node that is surely expanded by m-A*. Namely it will expand every node
 , i.e. f (n0 ) < C  , n0  .
that lies on any path  that is dominated by Cm
m
The proof idea is similar to the work by Dechter and Pearl (1985). Namely we can show that
any algorithm that does not expand a node n, surely expanded by m-A*, can miss one of the m-best
solutions, when applied to a slightly modified problem:
Proof. Let us consider a problem having the search graph G and a consistent heuristic h. Assume
that node n is surely expanded by m-A* before finding the j th best solution. Let B be an algorithm
that uses the same heuristic h and is guaranteed to find the m best solutions. Let also assume that
node n is not expanded by B. A consistency of the heuristic also allows us to better characterize the
nodes expanded by m-A*.
We can create a new problem graph G0 (see Figure 6) by adding a new goal node t with
h(t) = 0, connecting it to n by an edge having cost c = h(n) + , where  = 0.5(Cj  D)
907

fiF LEROVA , M ARINESCU , & D ECHTER

and D = maxn0 Sj f (n0 ), where Sj is the set of nodes surely expanded by m-A* before finding the
j th solution. It is possible to show that the heuristic h is admissible for the graph G0 (Dechter &
Pearl, 1985). Since  = 0.5(Cj  D), C  = D  2. By construction, the evaluation function of
the new goal node is:

f (t) = g(t) + h(t) = g(n) + c = g(n) + h(n) +  = f (n) +   D +  = Cj   < Cj

(2)

which means that t is reachable from s by a path whose cost is strictly bounded by Cj . That
guarantees that m-A* will expand t (Proposition 6), discovering a solution with cost Cj  . On the
other hand, algorithm B, that does not expand node n in the original problem, will still not expand it
and thus will not reach node t and will only discover the solution with cost Cj , not returning the true
set of m best solutions to the modified problem. From the contradiction the theorem follows.
P ROPOSITION 7. If the heuristic function employed by m-A* is consistent, the values of the evaluation function f of the sequence of expanded nodes are non-decreasing.
The proof is a straightforward extension of a result by Nilsson (1980).
Proof. Let node n2 be expanded immediately after n1 . If n2 was already in OPEN at the time when
n1 was expanded, then from the node selection rule it follows that f (n1 )  f (n2 ). If n2 was not
in OPEN, then it must have been added to it as a result of expansion of n1 , i.e., be a child of n1 .
In this case the cost of getting to n2 from the start node is g(n2 ) = g(n1 ) + c(n1 , n2 ) and the
evaluation function of node n2 is f (n2 ) = g(n2 ) + h(n2 ) = g(n1 ) + c(n1 , n2 ) + h(n2 ). Since h(n)
is consistent, h(n1 )  c(n1 , n2 )+h(n2 ) and f (n2 )  g(n1 )+h(n1 ). Namely, f (n2 )  f (n1 ).
If the heuristic function is consistent, we have a stronger condition of Proposition 4:
T HEOREM 9. Algorithm m-A* using a consistent heuristic function:
;
1. expands all nodes n such that f (n) < Cm
;
2. never expands any nodes with evaluation function f (n) > Cm

3. expands some nodes such that f (n) = Cm , subject to a tie-breaking rule.
 and node n is never expanded by
Proof. 1. Assume that there exists a node n such that f (n) < Cm
m-A*. Such a situation can only arise if node n has never been in the OPEN list, otherwise it would
have been expanded, according to Proposition 4. That implies that the parent of node n in the search
space (let us denote it by node p) has never been expanded. However, similarly how it is done in the
 . Thus
proof of Proposition 7, it is easy to show that f (p)  f (n) and, consequently f (p) < Cm
node p must also have never been in OPEN, otherwise it would be expanded. Clearly, this is true
for all the ancestors of n, up to the start node s. Since node s is clearly in OPEN at the beginning of
the search, the initial assumption is incorrect and the property follows.
2. and 3. Follow directly from Proposition 3.

Figure 7 provides a schematic summary of the search space explored by m-A* having a consistent heuristic.
908

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS


{n|f(n) < Cm
}

search space
explored by m-A

*

*

{n|f (n) > Cm
}


{n|f (n) = Cm
}

Figure 7: The nodes explored by the m-A* algorithm with a consistent heuristic.
Optimal efficiency for node expansions. Whenever a node n is selected for expansion for the
first time by m-A*, the algorithm has already found the shortest path to that node. We can extend
this property as follows:
T HEOREM 10. Given a consistent heuristic h, when m-A* selects a node n for expansion for the
ith time, then g(n) = gi (n), namely it has found the ith best path from start node s to n.
Proof. By induction. For i = 1 (basic step) the theorem holds (Nillson, 1980). Assume that it also
holds for (i  1)th expansion of node n. Let us consider the ith case, i > 1 (inductive step). We
have already expanded the node n (i  1) times and due to the inductive hypothesis we have already
found the (i  1) distinct best paths to the node n. Let us assume that the cost of the newly found
solution path is greater than the ith optimal one, i.e. gi (n) > gi (n). Then, there exists a different,
undiscovered path  from s to n with cost g (n) = gi (n) < gi (n). From Proposition 2 there exists
in OPEN a node n0  . Obviously, node n0 must be located between the start node s and node
n. Denoting by C (n0 , n) = c(n0 , n1 ) +    + c(nk , n), from the heuristic consistency it easily
follows that h(n0 ) < C (n0 , n) + h(n) and that the evaluation function of node n0 along path 
is f (n0 ) = g (n0 ) + h(n0 ) < g (n0 ) + C (n0 , n) + h(n). Seeing that the cost of path  from
s to n is g (n) = g (n0 ) + C , we conclude that f (n0 ) < f (n). However, that contradicts our
assumption that node n was expanded for the ith time before node n0 . The theorem follows.
3.2.4 T HE I MPACT OF THE R EQUIRED B EST S OLUTIONS m
The sequence of the sizes of search spaces explored by m-A* as a function of m is obviously monotonically increasing with m. Denoting by j-A* and i-A* the versions of the m-A* algorithm that
search respectively for j and i best solutions, we can make the following straightforward characterization:
P ROPOSITION 8. Given a search graph and consistent heuristic,
1. Any node expanded by i-A* is expanded by j-A* if i < j and if both use the same tie-breaking
rule.
909

fiF LEROVA , M ARINESCU , & D ECHTER

2. The set S(i, j) of nodes defined by S(i, j) = {n|Ci < f (n) < Cj } will surely be expanded
by j-A* and surely not expanded by i-A*.
3. If Cj = Ci , the difference in the number of nodes expanded by i-A* and j-A* is determined
by the tie-breaking rule.
The proof follows trivially from Theorem 9. As a result, larger discrepancy between the respective costs Cj  Ci yields larger difference in the search spaces explored by j-A* and i-A*.
This difference, however, also depends on the granularity with which the values of a sequence of
observed evaluation functions increase, which is related to the arc costs (or weights) of the search
graph. If Ci = Cj = C, then the search space explored by i-A* and j-A* will differ only in the
frontier of nodes satisfying f (n) = C. Figure 5 represents schematically the search spaces explored
by the i-A* algorithm.

4. Depth-First Branch and Bound for Finding the M Best Solutions
Along with its valuable properties, m-A* inherits also the disadvantages of A*: its exponential space
complexity, which makes the algorithm infeasible for many applications. An alternative approach
is searching using depth-first branch and bound (DFBnB), which can be implemented in linear
space if necessary and is therefore often more practical. DFBnB finds the optimal solution by
exploring the search space in a depth-first manner. The algorithm maintains a cost U of the best
solution encountered so far and prunes search nodes whose lower-bounding evaluation function
f (n) = g(n) + h(n) is larger than U . Extending DFBnB to the m-best task is straightforward, as
we describe next.
4.1 The m-BB Algorithm
Algorithm m-BB, the depth-first branch and bound extension to the m-best task, that explores a
search tree is presented in Algorithm 6. As usual, the algorithm maintains lists of OPEN and
CLOSED nodes. It also maintains a sorted list of CANDIDATE nodes that contains the best m
solutions found so far. Nodes on OPEN are organized in a last in - first out manner in order to
facilitate a depth-first exploration of the search space (i.e., OPEN is a stack). At each step, m-BB
expands the next node n in OPEN (line 5). If it is a goal node, a new complete solution is found
(line 6) and it is stored in the CANDIDATE list (line 7-9), which is then re-sorted (line 10). Only
up to m best solutions are maintained (lines 11-13).
The main modification to the depth-first branch and bound, when extended to the m-best task,
is in its pruning condition. Let U1  U2  . . .  Um denote the costs of the m best solutions
encountered thus far. Then Um is the upper bound used for pruning. Before m solutions are discovered, no pruning takes place. Algorithm m-BB expands the current node n, generates its children
(lines 15-17) and computes their evaluation function (line 18-19). It prunes a subproblem below n
iff f (n)  Um (lines 20-23). It is easy to see that when the algorithm terminates, it outputs the
m-best solutions to the problem.
T HEOREM 11. Algorithm m-BB is sound and complete for the m-best solutions task.
Proof. Algorithm m-BB explores the search space systematically. The only solutions that are
 , where C  is the m
skipped are the ones satisfying f (n)  Um (see step 22). Since Um  Cm
m
910

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

Algorithm 6: m-BB exploring a graph, assuming a consistent heuristic

1

2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23

24

Input: An implicit directed search graph G = (N, E) with a start node n0 and a set of goal nodes Goals. A
heuristic function h(n). Parameter m (the number of desired solutions).
Output: the m best solutions
Initialize: OPEN=, CLOSED=, a tree T r = , sorted list CANDIDATE = , UpperBound = , i = 1 (i
counts the current solution being searched for);
Put the start node n0 in OPEN, g(n0 ) = 0, f (n0 ) = h(n0 );
Assign n0 to be the root of T r;
while OPEN is not empty do
Remove the top node from OPEN, denoted n, and put it on CLOSED;
if n is a goal node then
soli  solution obtained by tracing back pointers from n to n0 (pointers assigned at step 17);
Ci  cost of soli ;
Place solution soli on CANDIDATE;
Sort CANDIDATE in increasing order of solution costs;
if size of CANDIDATE list  m then
Um  cost of the mth element in CANDIDATE;
Keep first m elements of CANDIDATE, discard the rest;
else
Expand node n, generating its children succ(n);
forall the n0  succ(n) do
Attach a pointer from n0 back to n in T r;
g(n0 ) = g(n) + c(n, n0 );
f (n0 ) = g(n0 ) + h(n0 );
if f (n0 ) < Um then
Place n0 in OPEN;
else
Discard n0 ;
return the solutions on CANDIDATE list

 and therefore that path cannot lead to a newly discovered
best solution cost, it implies f (n)  Cm
m-best cost.

N
T HEOREM 12. Given a graphical model M = hX, D, F, i, the worst case time complexity of
m-BB that explores an OR search tree of M is O(k n + log m), where n is the number of variables,
k is the domain size and m is the number of required solutions. Space complexity is O(n).
Proof. In worst case m-BB would explore the entire OR search tree of size O(k n ). The maintaining
of CANDIDATE list introduces additional time overhead of O(log m). Since the OR search tree
yields no caching, m-BB uses space linear in the number of variables.
4.2 Characterization of the Search Space Explored by m-BB
We have already shown that m-A* is superior to any exact search algorithm for finding the mbest solutions when the heuristic is consistent (Theorem 8). In particular, m-BB must expand all
 }. From
the nodes that are surely expanded by m-A*, namely the set of nodes {n|f (n) < Cm
Theorem 8 and the pruning condition it is clear that:
911

fiF LEROVA , M ARINESCU , & D ECHTER

P ROPOSITION 9. Given a consistent heuristic m-BB must expand any node in the set {n|f (n) <
 }. Also, there are instances for which m-BB will expands nodes satisfying f (n) > C  .
Cm
m
Several sources of overhead of m-BB are discussed next.
4.2.1 M -BB VS . BB
Pruning in m-BB does not occur until the upper bound on the current mth best solution is assigned
a valid value, i.e., until m solutions are found. In the absence of determinism, when all solutions are
consistent, the time it takes to find m arbitrary solutions in depth-first manner is O(m  n), where
n is the length of solution (for graphical models n coincides with the number of variables). If the
problem contains determinism it may be difficult to find even a single solution. This means that for
m-BB the search may be exhaustive for quite some time.
4.2.2 T HE I MPACT OF S OLUTION O RDER
The difference in the number of nodes expanded by BB and m-BB depends greatly on the variance
between the solution costs. If all the solutions have the same cost, then U1 = Um . However, such
a situation is unlikely and therefore the conditions for m-BBs node expansions are impacted by
1 , . . . , U j } be the non-increasing sequence of
the order in which solutions are discovered. Let {Um
m
th
the upper bounds on the m best solution, up to a point when m-BB uncovered the j th solution.
j
Initially Um
= , for j  {1, . . . , m  1}.
P ROPOSITION 10. Between the discovery of the (j  1)th and the j th solutions the set of nodes
j1
  U j  U j1  .
expanded by m-BB are included in Sj = {n | f (n)  Um
}, where Cm
m
m
Proof. Between discovering the (j  1)th and j th solutions m-BB expands only nodes satisfying
j1
j1
{n | f (n)  Um
}, hence j : Cj  Um
. Once the j th solution is found, it either replaces the
j
th
previous bound on m solution Um = Cj or some k th upper bound, k  {1, . . . , m  1}, yielding
j1
j
  U j  U j1 .
. Either way, Cm
= Um1
Um
m
m
4.2.3 O RDERING OVERHEAD
The need to keep a list of m sorted solutions (the CANDIDATE list) implies O(log m) overhead
for each new solution discovered. The total number of solutions encountered before termination is
hard to characterize.
4.2.4 C ACHING OVERHEAD
The overhead related to caching arises only when m-BB explores a search graph and uses caching.
This version of the algorithm (not explicitly presented) stores the m best partial solutions to any
fully explored subproblems (and a subset of m when only a partial set is discovered) and re-uses
these results whenever the subproblem is encountered again. In order to implement caching, mBB requires to store a list of length m for each node that is cached. Moreover, the cached partial
solutions need to be sorted, which yields an O(m log m) time overhead per cached node.
912

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

5. Adapting m-A* and m-BB for Graphical Models
Our main task is to find the m best solutions to optimization tasks over graphical models. Therefore,
we adapt the m-best search algorithms m-A* and m-BB to explore the AND/OR search space over
graphical models, yielding algorithms m-AOBF and m-AOBB, respectively. We will also describe
a hybrid algorithm BE+m-BF, combining Bucket Elimination and m-A*.
5.1 m-AOBF: Best-First AND/OR Search for M Best Solutions in Graphical Models
The extension of algorithm AOBF (Section 2.3.2) to the m-best task seems fairly straightforward,
in principle. m-AOBF is AOBF that continues searching after discovering the first solution, until
the required number of m best solutions is obtained. The actual implementation requires several
modifications as we discuss next.
It is not easy to extend AOBFs bottom-up node values updates and corresponding arc marking
mechanism to the m-best task. Therefore, in order to keep track of the current best partial solution
tree while searching for the ith best solution we adopt a naive approach that maintains explicitly a
list OPEN containing entire partial solution trees (not just nodes), sorted in ascending order of their
heuristic evaluation costs. Algorithm 7 presents the pseudo-code of our simple scheme that explores
the AND/OR search tree and generates solutions one by one in order of their costs. At each step,
the algorithm removes the next partial solution tree T 0 from OPEN (line 4). If T 0 is a complete
solution, it is added to the list of solutions along with its cost (lines 5-8), otherwise the algorithm
expands a tip node n of T 0 , generating its successors (line 10-17). Each such newly generated node
n0 is added to T 0 separately, yielding a new partial solution tree T 00 (lines 19-23), whose cost is
recursively evaluated using Algorithm 3, as in AOBB (line 28). These new partial trees are then
placed in OPEN (line 29). Search stops when all m solutions have been found.
We note that the maintenance of the OPEN list containing explicit partial solution subtrees is a
source of significant additional overhead which will become apparent in the empirical evaluation in
Section 7. Thus, the question whether the performance of m-AOBF can be improved further is open
and is therefore a rich topic of future work.
All m-A* properties (Section 3.2) can be extended to m-AOBF. In particular, algorithm mAOBF with an admissible heuristic is sound and complete, terminating with the m best solutions
generated in order of their costs. m-AOBF is also optimal in terms of the number of nodes expanded
compared with any other algorithm that explores the same AND/OR search space with the same
consistent heuristic function.
T HEOREM 13 (m-AOBF complexity). The complexity of algorithm m-AOBF traversing either the
h1
AND/OR search tree or the context minimal AND/OR search graph is time and space O(k deg ),
where h is the depth of the underlying pseudo tree, k is the maximum domain size, and deg bounds
the degree of the nodes in the pseudo tree. If the pseudo tree is balanced (each internal node has
exactly deg child nodes), then the time and space complexity is O(k n ), where n is the number of
variables.
The real complexity bound of m-AOBF comes from the cost function. It appears however that
maintaining an OPEN list in a brute force manner does not lend itself easily to an effective way
of enumerating all partial solution subtrees and therefore the search space of all partial solution
subtrees is actually exponential in n. The detailed proof of Theorem 13 is given in the Appendix.
913

fiF LEROVA , M ARINESCU , & D ECHTER

Algorithm 7: m-AOBF exploring an AND/OR search tree

1
2
3
4
5
6
7
8
9

10
11
12
13
14
15
16
17
18

19
20
21
22
23
24
25
26
27
28

29
30

Input: A graphical model M = hX, D, Fi, pseudo tree T rooted at X1 , heuristic function h(), parameter m;
Output: The m best solutions to M
Create root OR node s labelled by X1 , let G = {s} (explored search space) and T = {s} (partial solution tree);
Initialize S  ; OP EN  {T }; i = 1; (i counts the current solution being searched for);
while i  m and OP EN 6=  do
Select the top partial solution tree T 0 and remove it from OPEN;
if T 0 is a complete solution then
S  S  {hf (T 0 ), T 0 i};
i  i + 1;
continue;
Select a non-terminal tip node n in T 0 ;
// Expand node n
if n is OR node labeled Xi then
forall the xi  D(Xi ) do
Create AND child n0 labeled hXi , xi i;
succ(n)  succ(n)  {n0 };
else if n is AND node labeled hXi , xi i then
forall the successor Xj of Xi in T do
Create an OR child n0 labeled Xj ;
succ(n)  succ(n)  {n0 };
G  G  {succ(n)};
// Generate new partial solution trees
L  ;
forall the n0  succ(n) do Initialize v(n0 ) = h(n0 );
if n is OR node then
forall the n0  succ(n) do
Create a new partial solution tree T 00  T 0  {n0 };
L  L  {T 00 };
else if n is AND node then
Create a new partial solution tree T 00  T 0  {succ(n)};
forall the T 00  L do
Recursively evaluate and assign to f (T 00 ) the cost of the partial solution tree T 00 , based on heuristic
function h(); // see Algorithm 3
Place T 00 in OPEN, keeping it sorted in the ascending order of costs f (T 00 );
return The m best solutions found S;

5.2 m-AOBB: AND/OR Branch and Bound for M Best Solutions in Graphical Models
Algorithm m-AOBB extends the AND/OR Branch and Bound search (AOBB, Section 2.3.3) to the
m-best task. The main difference between AOBB and m-AOBB is in the value function computed
for each node. m-AOBB tracks the costs of the m best partial solutions of each solved subproblem. Thus it extends the node value v(n) and solution tree T  (n) rooted by n in AOBB to ordered
sets of length m, denoted by v(n) and T  (n), respectively, where v(n) = {v1 (n), . . . , vm (n)} is
an ordered set of the costs of the m best solutions to the subproblem rooted by n, and T  (n) =
 (n)} is a set of corresponding solution trees. This extension arises due to the
{T1 (n), . . . , Tm
depth-first manner of search space exploration of m-AOBB in conjunction with the AND/OR decomposition. Therefore, due to the AND/OR decomposition m-AOBB needs to completely solve
914

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

Algorithm 8: m-AOBB exploring an AND/OR search tree

1
2

3
4
5
6
7
8
9
10
11
12
13
14

Input: A graphical model M = hX, D, Fi, pseudo tree T rooted at X1 , heuristic function h(), parameter m;
Output: The m best solutions to M
// INITIALIZE
Create root OR node s labeled by X1 and let the stack of created but not expanded nodes OP EN = {s};
Initialize v(s) =  (a set of bounds on m best solutions under s) and a set of best partial solution trees rooted in s
T  (s) = ; U B = , sorted list CAN DIDAT E = ;
while OP EN 6=  do
Select top node n in OPEN;
// EXPAND
if n is OR node labeled Xi then
foreach xi  D(Xi ) do
Add AND child n0 labeled hXi , xi i to list succ(n) containing the successors of n;
Initialize v(n0 ) = 0, a set of best partial solution trees rooted in n T  (n0 ) = ;
if n is AND node labeled hXi , xi i then
Let p be ancestor of n;
Recursively evaluate and assign to f (p) the cost of the partial solution tree rooted in p, based on the heuristic h(); //
see Algorithm 3
if vm (p) <  and f (p)  vm (p) then
Prune the subtree below the current tip node n;
else
foreach successor Xj of Xi  T do
Add OR child n0 labeled Xj to list succ(n) containing the successors of n;
Initialize v(n0 ) = , a set of best partial solution trees rooted in n T  (n0 ) = ;

15
16
17

18
19
20
21
22
23
24
25

26

27
28
29

30

31
32
33
34

Remove n from OPEN and add succ(n) on top of OPEN;
// PROPAGATE
while list of successors of node n is empty do
if n is the root node then
return a set of solutions rooted at n and their costs: T  (n), v(n) ;
else
Update ancestors of n, AND and OR nodes p, bottom up:
if p is AND node then
Combine the set of the partial solution trees to the subproblem rooted in p T  (p) and the set of partial
solution trees rooted in n T  (n) and their costs v(p) and v(n); // see Algorithm 9
Assign the resulting set of the costs and the set of the best partial solution trees respectively to v(p) and
T  (p);
else if p is OR node then
foreach solution cost vi (n) in the set v(n) do
Update the cost with the weight of the arc, creating a new set of costs v 0 (n):
vi0 (n) = c(p, n) + vi (n);
Merge the sets of partial solutions v(n) and v(p) and the sets of partial solution trees rooted in p and n:
T  (p) and T  (n), keeping m best elements; //Algorithm 10
Assign results of merging respectively to v(p) and T  (p);
Remove n from the list of successors of p;
Move one level up: n  p;
return v(s) and T  (s)

the subproblems rooted in all the children n0 of an AND node n, before even a single solution to a
subproblem above n is acquired (unlike the m-BB case). Consequently, during the bottom-up phase
sets of m costs have to be propagated and updated. m-AOBF on the other hand, only maintains a
set of partial solution trees.
915

fiF LEROVA , M ARINESCU , & D ECHTER

Algorithm 9: Combining the sets of costs and partial solution trees
1

2
3
4

5
6
7
8
9
10
11

function Combine(v(n), v(p), T  (n),T  (n))
Input: Input sorted sets of costs v(n), v(p), corresponding partial solution trees T  (n), T  (p), number of
required solutions m
Output: A set of costs m best combined solutions v 0 (p), corresponding partial solution trees T 0 (p)
// INITIALIZE
Sorted list OPEN, initially empty; //contains potential cost combinations
v 0 (p)  ; T 0 (p)  ;
k = 1; //number of partial solutions already assembled, up to m in total
// Search over possible combinations
OPEN v1 (n) + v1 (p);
while k < m and OP EN is not empty do
Remove the top node V on OPEN, where V = Svi (n) + vj (p);
vk0 (p)  V ;
0
T  (p)  Ti (n)  Tj (p);
if vi+1 (n) + vj (p) not in OP EN then
Put vi+1 (n) + vj (p) in OP EN ;

13

if vi (n) + vj+1 (p) not on OP EN then
Put vi (n) + vj+1 (p) in OP EN ;

14

k  k + 1;

15

return v 0 (p), T  (p);

12

Unlike m-AOBF that discovers solutions one by one in order of their costs, m-AOBB (pseudocode in Algorithm 8) reports the entire set of m solutions at once, at termination. m-AOBB interleaves forward node expansion (lines 5-18) with a backward propagation (or cost revision) step
(lines 19-33) that updates node values until search terminates. A node n will be pruned (lines 12-13)
if the current upper bound on the mth solution under n, vm (n), is lower than the nodes evaluation
functions f (n), which is computed recursively as in AOBB (Algorithm 3). During the bottom-up
propagation phase at each AND node the partial solutions to the subproblems rooted in the nodes
children are combined (line 24-26, Algorithm 9). At each parent OR node p v(p) and T  (p) are
updated to incorporate the new and possibly better partial solutions rooted in a child node n (lines
27-31, Algorithm 10).
5.2.1 C HARACTERIZING N ODE P ROCESSING OVERHEAD
In addition to the increase in the explored search space that m-BB experiences compared with BB
due to the reduced pruning (Section 4.2), AND/OR search introduces additional overhead for mAOBB. The propagation of a set of m costs and of m partial solution trees leads to an increase in
memory by a factor of m per node. Processing the partial solutions at both OR and AND nodes
introduces an additional overhead.
T HEOREM 14. Algorithm m-AOBB exploring the AND/OR search tree has a time overhead of
O(m  deg  log m) per AND node and O(m  k) per OR node, where deg bounds the degree of the
pseudo tree and k is the largest domain size. Assuming k < deg  log(m), the total worst case time
complexity is O(n  k h deg  m log(m)) and the space complexity is O(mn). The time complexity

of m-AOBB exploring the AND/OR search graph is O(n  k w deg  m log(m)), space complexity

O(mn  k w ).
916

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

Algorithm 10: Merging the sets of costs and partial solution trees
1

2
3
4
5

6
7
8
9
10
11
12
13
14
15
16

17

function Merge(v(n),v(p),T  (n),T  (p))
Input: Input sorted cost sets v(n) and v(p), sets of corresponding partial solution trees T  (n) and T  (p),
number of required solutions m
Output: v 0 (p), a merged set of m best solution costs, T 0 (p) a set of corresponding partial solution trees
// INITIALIZE
v 0 (p)  ;
T 0 (p)  ;
i, j  1; //indices in the cost sets
k  1; //index in the resulting array
// Merge two sorted sets
while k  m do
if vi (p)  vj (n) then
vk0 (p)  vi (p);
0
Tk (p)  Ti (p);
i  i + 1;
k  k + 1;
else
vk0 (p)  vj (n);
0
Tk (p)  Tj (n);
j  j + 1;
k  k + 1;
return v 0 (p) and T 0 (p);

Proof. Combining the sets of current m-best partial solutions (Algorithm 9) introduces an overheard
of O(m log(m)). The resulting time overhead per AND node is O(deg  m log(m)). Merging two
sorted sets of costs (Algorithm 10) can be done in O(m) steps. If an OR node has O(k) children,
the resulting overhead is O(m  k). Assuming k < deg  log(m), the complexity is dominated by
processing of the AND nodes. In the worst case, the tree version of m-AOBB, called m-AOBB-tree,
would explore the complete search space of size O(n  k h ), where h bounds the depth of the pseudo

tree, while the graph version, called m-AOBB-graph, would visit a space of of size O(n  k w ),
where w is the induced width of the pseudo tree. The space complexity of m-AOBB-tree follows
from the need to propagate the sets of O(m) partial solutions of length O(n). The time overhead for
m-AOBB is the same for AND/OR trees and AND/OR graphs. The space complexity of m-AOBBgraph is explained by the need to store m partial solutions for each cached node.

5.3 Algorithm BE+m-BF
It is known that exact heuristics for graphical models can be generated by the Bucket Elimination
(BE) algorithm described in Section 2.3.4. We can therefore first compile the exact heuristics along
an ordering using BE and then apply m-A* (or m-AOBF, both will work the same at this point),
using these exact heuristics. The resulting algorithm is called BE+m-BF. Worst-case analysis of
this algorithm will show that it yields the best worst-case complexity compared with any known
m-best algorithm for graphical models.
917

fiF LEROVA , M ARINESCU , & D ECHTER

T HEOREM 15. The time complexity of BE+m-BF is O(nk w+1 + nm) when n is the number of
variables, k is the largest domain size, w is the induced width of the problem and m is the desired
number of solutions. The space complexity is O(nk w + nm).
Proof. BEs time complexity is O(nk w+1 ) and space complexity of O(nk w ) (Dechter, 1999).
Since BE compiles an exact heuristic function, m-A* with this exact heuristic expands nodes for
which f (n) = Cj only while searching for ith solution. If the algorithm breaks ties in favour of
deeper nodes, it will only expand nodes on solution paths. Each path has length n, yielding the total
time and space complexity of this step of the algorithm equal to O(n  m).

6. Related Work
We can distinguish several primary approaches employed by earlier m-best exact algorithms, some
mentioned already in the Introduction. Note that some of the original works do not include space
complexity analysis and the bounds provided are often our own.
The first and most influential approach was introduced by Lawler (1972). It aimed to use of-theshelf optimization schemes for best solutions. Lawler showed how to extend any given optimization
algorithm to the m-best task. At each step, the algorithm seeks the best solution to a re-formulation
of the original problem that excludes the solutions already discovered. The scheme has been improved over the years and is still one of the primary strategies for finding the m-best solutions. The
time and space complexity of Lawlers scheme are O(nmT (n)) and O(S(n)) respectively, where
T (n) and S(n) are the time and space complexity of finding a single best solution. For example,
if we use AOBF as the underlying optimization algorithm, the use of Lawlers method yields time


complexity of O(n2 mk w log n ) and space complexity of O(nk w log n ).
Hamacher and Queyranne (1985) built upon Lawlers work but used as building blocks algorithms that find both the first and second best solutions. Once two best solutions are generated, a
new problem is formulated so that the second best solution is the best solution to the new problem.
Then, the second best solution for the new problem becomes the overall third best solution and the
procedure is repeated. The algorithm has time complexity of O(m  T2 (n)) and space complexity of
O(S2 (n)), where T2 (n) and S2 (n) are respectively the time and space for finding the second best
solution. The complexity of this method is always bounded from above by that of Lawler, seeing
that Lawlers scheme can be used as an algorithm for finding the second best task. Using m-AOBF

to find the two best solutions, we obtain time complexity of O(2mnk w log n ) and space complexity

O(2nk w log n ).
Nilsson (1998) applied Lawlers method using a join-tree algorithm. On top of that his algorithm reuses computations from previous iterations. His scheme, called max-flow algorithm, uses
message-passing on a junction tree to calculate the initial max-marginal functions for each cluster
(e.g. probability values of the most probable assignments task) yielding the best solution. Note that
this step is equivalent to running the bucket-elimination algorithm. Subsequent solutions are recovered by conditioning search which consult the generated function. The time complexity analysis by
Nilsson (1998) is O(2p|C| + 2mp|R| + pm log (pm)), where p is the number of cliques in the joint
tree, |C| is the size of the largest clique and |R| is the size of the largest residual (i.e. the number
of variables in a cluster but not in neighbouring clusters). The space complexity can be bounded by
O(p|C| + p(|S|)), where |S| is the size of a separator between the clusters. If applied to a buckettree, Nilssons scheme has time and space complexity of O(2nk w+1 + mn(2k + log(mn)) and


O(nk w +1 + nk w ) respectively, since the the bucket tree has p = n cliques, whose size is bounded
918

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS



by |C| = k w +1 and the residual in each cluster is |R| = k (the domain of a single variable).
Thus the algorithm has better time complexity than all other schemes mentioned so far, except for
BE+m-BF.
Two works by de Campos et al. build upon Nilssons approach, extending it to solving the mbest MAP task using genetic algorithms (de Campos, Gamez, & Moral, 1999) and probability trees
(de Campos, Gamez, & Moral, 2004), respectively. These schemes are approximate and the authors
provide no theoretical analysis of their complexity.
Fairly recently, Yanover and Weiss (2004) developed an iterative scheme based on belief propagation, called BMMF. At each iteration BMMF uses Loopy Belief Propagation to solve two new
problems obtained by restricting the values of certain variables. When applied to a junction tree

having induced width w (whose largest cluster size is bounded by k w +1 ) it is an exact algorithm
having time complexity O(2mnk w+1 ) and space complexity O(nk w + mnk). When applied on
a loopy graph, BMMF is not guaranteed to find exact solutions.
Another approach based on Lawlers idea uses optimization via the LP-relaxation (Wainwright
& Jordan, 2003), formulated by Fromer and Globerson (2009). Their method, called Spanning TRee
Inequalities and Partitioning for Enumerating Solutions, or STRIPES, also partitions the search
space, while systematically excluding all previously determined assignments. At each step new
constraints are added to an LP optimization problem, which is solved via an off-the-shelf LP-solver.
In general, the algorithm is approximate. However, on trees or junction-trees it is exact if the
underlying LP solver reports solutions within the time limit. PESTEELARS is an extension of the
above scheme by Batra (2012) that solves the LP relaxation using message-passing approach that,
unlike conventional LP solvers, exploits the structure of the problems graph. The complexity of
these LP-based algorithm is hard to characterize using the usual graph parameters.
Another approach extends variable elimination (or dynamic programming) schemes to directly
obtain the m best solutions. In our recent paper (Flerova et al., 2011) we extended bucket elimination and mini-bucket elimination to the m-best solutions task, yielding an exact scheme called
elim-m-opt and its approximate version called mbe-m-opt, respectively. This work also embeds
the m-best optimization task within the semi-ring framework. The time and space complexities of


algorithm elim-m-opt are bounded by O(m log mnk w +1 ) and O(mnk w ), respectively.
Two related dynamic programming based ideas are by Seroussi and Golmard (1994) and Elliot
(2007). Seroussi and Golmard extract the m solutions directly, by propagating the m best partial
solutions along a junction tree. Given a junction tree with p cliques, largest cluster size |C|, separator
size bounded by |S| and branching degree deg, the time complexity of the algorithm is O(m2  p 
|C|  deg) and the space complexity is O(m  p  |S|). Adapted to a bucket tree, this algorithm has


time complexity O(m2 nk w +1 deg) and space complexity of O(mnk w ). Elliot propagates the m
best partial solutions along a representation called Valued And-Or Acyclic Graph, also known as
a smooth deterministic decomposable negation normal form (sd-DNNF) (Darwiche, 2001). The

time complexity of Elliots algorithm is O(nk w +1 m log (m  deg)) and the space complexity is

O(mnk w +1 ).
Several methods focus on search schemes obtaining multiple optimal solution for the k shortest
paths task (KSP). For a survey see the paper by Eppstein (1994). The majority of these algorithms
assume that the entire search graph is available in memory and thus are not directly applicable. A
recent exception is by Aljazzar and Leue (2011), whose K  algorithm finds the k shortest paths
during search on-the-fly and thus can be potentially useful for graphical models. The algorithm
interleaves A* search on the problems implicit graph G and Dijkstras algorithm (1959) on a spe919

fiF LEROVA , M ARINESCU , & D ECHTER

BE+m-BF


O(nk w

m-AOBF


O(nk w

log n

+1

+ mn)

O(2nk w

)



Nilsson 1998

+1

+ mn(2 log(mn) + 2k))

elim-m-opt


Gosh et al. 2012

Elliot
2007


Yanover and Weiss 2004

m-AOBB


Hamacher and Queyranne

O(mnk w

O(mnk w

+1

+1



O(mnk w )

log m)

log (m  deg))

O(mnk w

O(mnk w deg log m)



O(2mnk w

+1

log n

Aljazzar and Leue 2011


O(nk w w log (nk) + m)

)

)

Seroussi and Golmard 1994
O(m2 nk w+1 deg)

Lawler 1972

O(n2 mk w )

m-A*-tree
O(k n )

m-BB-tree

O(k n + log m)

Figure 8: Time complexity comparison of the exact m-best algorithms specified for a bucket tree.
A parent node in the graph has a better complexity than its children. Problem parameters:
n - number of variables, k - largest domain size, w - induced width, deg - the degree of
the join (bucket) tree. Our algorithms are highlighted.

cific path graph structure denoted P (G). P (G) is a directed graph, the vertices of which correspond
to edges in the problem graph G. Given a consistent heuristic, K  , when applied to an AND/OR

search graph is time and space O(nk w w log(n  k) + m).
More recently, Gosh, et al., (2012) introduced a best-first search algorithm for generating ordered solutions for explicit AND/OR trees or graphs. The time complexity of their algorithm can
be bounded by O(mnk w ), when applied to a context-minimal AND/OR search graph. The space
complexity is bounded by O(s  nk w+1 ), where s is the number of candidate solutions generated
and stored by the algorithm, hard to quantify using usual graph parameters. However, this approach,
which explores the space of complete solutions, does not seem to be practical for graphical models
because it requires the entire AND/OR search space to be fully explicated in memory before attempting to generate even the second best solution. In contrast, our algorithms generate the m best
solutions while traversing the space of partial solutions.
920

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

Figure 8 provides a visual comparison between the worst-case time complexity bounds of the
discussed schemes in a form of a directed graph, where each node corresponds to an algorithm
and a parent in the graph has a better complexity than its child. We assume in our analysis that
n >> m > k > 2.
We see that the best emerging scheme, as far as worst-case performance goes is BE+m-BF.
However, since it requires compiling of the exact heuristics, it is often infeasible. We see also that
algorithm elim-m-opt appears to have relatively good time complexity superior, for example, to mAOBB search. However, as we showed in our previous work (Flerova et al., 2011), it is quite limited
empirically. Note that the worst-case analysis often fails to capture the practical performance, either because the algorithms that have good worst-case performance require too much memory, or
because it ignores the power of the cost function in bounding the performance.

7. Experimental Evaluation
Our experiments consist of two parts: evaluation of the m-best search algorithms on the benchmarks
from recent UAI and Pascal2 competitions and comparison of our schemes with some of the previously developed algorithms on randomly generated networks, whose parameters and structure had
to be restricted due to the limitations of the available implementations of the competing schemes.
We defer the discussion of the second part of experiments until Section 7.5, concentrating now on
the evaluation our m-best search schemes only.
7.1 Overview and Methodology
We used 6 benchmarks, all, except for binary grids, came from real-world domains:
1. Pedigrees
2. Binary grids
3. WCSP
4. Promedas
5. Proteins
6. Segmentation
The pedigrees benchmark (pedigree*) was used in the UAI 2008 competition.3 They arise
from the domain of genetic linkage analysis and are associated with the task of haplotyping. A
haplotype is a sequence of alleles at different loci inherited by an individual from one parent, and
the two haplotypes (maternal and paternal) of an individual constitute this individuals genotype.
When genotypes are measured by standard procedures, the result is a list of unordered pairs of
alleles, one pair for each locus. The maximum likelihood haplotype problem consists of finding
a joint haplotype configuration for all members of the pedigree which maximizes the probability
of data. It can be shown that, given the pedigree data, the haplotyping problem is equivalent to
computing the most probable explanation of a Bayesian network that represents the pedigree (see
the paper by Fishelson and Geiger (2002) for more details).
3. http://graphmod.ics.uci.edu/group/Repository

921

fiF LEROVA , M ARINESCU , & D ECHTER

Benchmark
Pedigrees
Grids
WCSP
Promedas
Proteins
Segmentation

# inst
13
32
61
86
72
47

n
581-1006
144-2500
25-1057
197-2113
15-242
222-234

k
3-7
2
2-100
2
18-81
2-21

w
16-39
15-90
5-287
5-120
5-16
15-18

hT
52-104
48-283
11-337
34-187
7-44
47-67

Table 1: Benchmark parameters: # inst - number of instances, n - number of variables, k - domain
size, w - induced width, hT - pseudo tree height.

In each of the binary grid networks (50-*, 75-* and 90-*)4 the nodes corresponding
to binary variables are arranged in an N by N square and the functions are defined over pairs of
variables and are generated uniformly randomly.
The WCSP (*.wcsp) benchmark includes random binary WCSPs, scheduling problems from
the SPOT5 benchmark, and radio link frequency assignment problems, providing a large variety of
problem parameters.
Protein side-chain prediction (pdb*) networks correspond to side-chain conformation prediction tasks in the protein folding problem (Yanover, Schueler-Furman, & Weiss, 2008). The
resulting instances have relatively few nodes, but very large variable domains, generally rendering
most instances very complex.
Promedas (or chain *) and segmentation (* s.binary) are probabilistic networks that
come from the set of problems used in the 2011 Probabilistic Inference Challenge.5 Promedas instances are based on a Bayesian network model developed for expert systems for medical diagnosis
(Wemmenhove, Mooij, Wiegerinck, Leisink, Kappen, & Neijt, 2007). Segmentation is a common
benchmark used in computer vision, modeling the task of image segmentation as an MPE problem,
namely assigning a label to every pixel in an image, such that pixels with the same label share
certain characteristics.
Table 1 describes the benchmark parameters: # inst - number of instances, n - number of variables, k - maximum domain size, w - induced width of the ordering used, hT - pseudo-tree height.
The induced width is not only one of the crucial parameters indicating the difficulty of the problem,
but the difference between the induced width and the mini-bucket i-bound signifies the strength of
the heuristic. When the i-bound is considerably smaller than the induced width, the heuristic is
weak, while the i-bound equal or greater than the induced width yields an exact heuristic, which in
turn yields much faster search. Clearly, a large number of variables, a high domain size or a large
pseudo tree height suggest harder problems.
7.1.1 A LGORITHMS
We can distinguish 6 algorithms: BE+m-BF, m-A*-tree and m-BB-tree exploring a regular OR
search tree and their modifications that explore an AND/OR search tree, denoted m-AOBF-tree and
m-AOBB-tree. We also consider a variant of m-AOBF that explores the AND/OR search graph
m-AOBF-graph. We did not implement the m-AOBB over AND/OR search graph, because the
4. http://graphmod.ics.uci.edu/repos/mpe/grids/
5. http://www.cs.huji.ac.il/project/PASCAL/archive/mpe.tgz

922

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

overhead due to book-keeping looked prohibitive. We used m-AOBF as representative for AND/OR
graph search, and as we will see it proved indeed to be not cost-effective. All algorithms were
guided by pre-compiled mini-bucket heuristics, described in Section 2.3.4. We used 10 i-bounds,
ranging from 2 to 22. However, for some hard problems computing the mini-bucket heuristic with
the larger i-bounds proved infeasible, so the actual range of i-bounds varies among the benchmarks
and among instances within a benchmark. All algorithms were restricted to a static variable ordering
computed using a min-fill heuristic (Kjrulff, 1990). Both AND/OR schemes used the same pseudo
tree. In our implementation algorithms m-BB, m-BF and m-AOBF break ties lexicographically,
while algorithm m-AOBB solves the independent subproblems rooted at an AND node in increasing
order of their lower bound heuristic estimates.
The algorithms were implemented in C++ (32-bit) and the experiments were run on a 2.6GHz
quad-core processor. The memory limit was set for 4 GB per problem, the time limit to 3 hours. We
report the CPU time (in seconds) and the number of nodes expanded during search. For uniformity
we consider the task throughout to be the maximization-product problem, also known as Most
Probable Explanation task (MPE or MAP). We focus on complete and exact solutions only and thus
do not report the results if the algorithm found less than m solutions (for best-first schemes) or if
the optimality of the solutions was not proved (for branch and bound schemes).
7.1.2 G OALS OF THE E MPIRICAL E VALUATION
We will address the following aspects:
1. Comparing best-first and depth-first branch and bound approaches
2. The impact of AND/OR decomposition on the search performance
3. Scalability of the algorithms with the number of required solutions m
4. Comparison with earlier proposed algorithms
7.2 The Main Trends in the Behavior of the Algorithms
Tables 2, 4, 6, 8, 10, and 12 present for each of our algorithms the raw results in the form of runtime
in seconds and number of expanded nodes for select instances from each benchmark, selected to
best illustrate the prevailing trends. For each benchmark we show the results for two values of the
i-bound, corresponding, in most cases, to relatively weak and strong heuristics. Note that the ibound has no impact on the BE+m-BF, since it always calculates the exact heuristic. We show three
values of number of solutions m, equal to 1 (ordinary optimization problem), 10 and 100.
In order to see the bigger picture, in Figures 9-14 we show bar charts representing for each
benchmark a median runtime and a number of instances solved by each algorithm for a particular
strength of the heuristic (i-bound) for m  {1, 2, 5, 10, 100}. The y-axis is on logarithmic scale.
The numbers above the bars indicate the actual values of median time in seconds and number of
solved instances, respectively. It is important to note that in these figures we only account for harder
instances, for which the i-bound did not yield exact heuristic. We acknowledge that the median
times are not strictly comparable since they are calculated over a varied number of instances solved
by each algorithm. However, this metric is robust to outliers and gives us an intuition about the
algorithms relative success. In addition, Tables 3, 5, 7, 9, 11 and 13 show for each benchmark the
number of instances, for which a given algorithm is the best in terms of runtime and in terms of
number of expanded nodes. If several algorithms show the same best result, it counts towards the
score of all of them.
923

fiF LEROVA , M ARINESCU , & D ECHTER

instance
(n,k,w ,h)

i-bound

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
Timeout
Timeout
0.05
6647
OOM
OOM
OOM
1269.0
348648825
22.99
2320223
0.05
6647

number of solutions
m=10
nodes
OOM
OOM
OOM
Timeout
Timeout
0.06
6671
OOM
OOM
OOM
1275.65
348648869
164.72
12110559
0.06
6671

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
OOT
1461.76
46419482
OOM
OOM
OOM
OOM
151.49
33563300
107.03
4274313
OOM

OOM
OOM
OOM
OOT
2389.32
74629839
OOM
OOM
OOM
OOM
152.27
33609110
185.66
7245553
OOM

OOM
OOM
OOM
OOT
3321.47
83802828
OOM
OOM
OOM
OOM
148.08
36255491
251.98
8319419
OOM

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
Timeout
5760.25
14260410
OOM
OOM
OOM
793.56
2579416
Timeout
484.69
1530768
OOM

OOM
OOM
OOM
Timeout
OOT
OOM
OOM
OOM
OOM
Timeout
551.67
1995114
OOM

OOM
OOM
OOM
Timeout
OOT
OOM
OOM
OOM
OOM
Timeout
858.29
3507104
OOM

10.22

10.29

algorithm

m=1
time

503.wcsp

4

(144, 4, 9, 44)

8

myciel5g 3.wcsp

4

(47,2, 19, 46)

8

satellite01ac.wcsp

4

(79, 8, 19, 56)

8

29.wcsp

4

(83, 4, 18, 58)

8

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

10.23
11.59
12.96
1.81
0.0
0.05
0.09
0.02
0.17
0.02
0.0

nodes

464134
OOM
938812
2243619
87717
111
2347
1401
2098
37629
1577
111

time

11.58
12.89
2.63
0.0
0.05
0.09
0.02
0.17
0.33
0.0

464182
OOM
938869
2245137
147851
168
2395
1447
2155
38463
24239
168

m=100
time

nodes
OOM
OOM
OOM
Timeout
Timeout
0.06
6984
OOM
OOM
OOM
1255.46
348651775
8010.42
568148386
0.06
6984

11.57
12.77
115.3
0.01
0.08
0.13
0.02
0.25
79.38
0.01

464698
OOM
939508
2279587
9189667
739
2899
1482
2724
55125
6731546
739

Table 2: WCSP: CPU time (in seconds) and number of nodes expanded. A Timeout stands for
exceeding the time limit of 3 hours. OOM indicates out of 4GB memory. In bold we
highlight the best time and number of nodes for each m. Parameters: n - number of
variables, k - domain size, w - induced width, h - pseudo tree height.

We next provide some elaboration and interpretation of the results.
7.2.1 WCSP
Table 2 shows the results for two values of the i-bound for select instances chosen to best illustrate
the common trends seen across the WCSP benchmark. Figure 9 presents the median time and
number of solved instances for each algorithm for i=16. Table 3 shows for the same i-bound (i=16)
the number of instances for which each of the schemes had the best runtime and best number of
expanded nodes. For many problem instances of this benchmark the mini-bucket elimination with
the large i-bounds is infeasible, thus we present the results for a small and a medium i-bounds.
924

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

BE+m-BF
m-AOBF tree

m-AOBF graph
m-A* tree

1.94
0.01
0.04
0.08
0.05

1.71

10

100

6
3
3
2

3
3
2

3
3
2

3
3
2

2

Solved instances

WCSPs, i=16

3
3

4

5

6

6

m-BB tree
m-AOBB tree

5
5

5
5

5

6

m-AOBF graph
m-A* tree

6

BE+m-BF
m-AOBF tree
6

0.0
0.01
0.03
0.05

1.7
0.0
0.01
0.02
0.05

5
m

2

5
5

1

10-1

2.8

2.79

2.71
1.71
0.0
0.01
0.02
0.04

1.71
0.1

0.0
0.01
0.02
0.05

Median time
WCSPs, i=16

100

m-BB tree
m-AOBB tree
6.33

101

100

1

2

5

m

10

100

Figure 9: Median time and number of solved instances (out of 49) for select values of m for WCSPs,
i-bound=16. Numbers above bars - actual values of time (sec) and # instances. Total
instances in benchmark: 61, discarded instances due to exact heuristic: 12.

BE+m-BF. As suggested by theory, whenever BE+m-BF does not run out of memory, it is the
most efficient scheme. See for example Table 2, 503.wcsp and 29.wcsp. However, calculation of
the exact heuristic is only feasible for easier instances and, as Figure 9 shows, it can only solve
925

fiF LEROVA , M ARINESCU , & D ECHTER

algorithm

Not solved
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

WCSPs: # inst=61, n=14-1058
k=2-100, w =6-287, hT =8-585, i-bound=16
m=1
m=2
m=5
m=10
m=100
#BT / #BN #BT / #BN #BT / #BN #BT / #BN #BT / #BN
43
43
43
43
43
1/2
1/1
0/1
1/1
0/0
1/2
0/2
0/2
0/2
0/3
5/1
4/3
5/3
4/3
5/2
1/0
2/0
1/0
1/0
0/0
1/2
2/0
0/0
0/0
0/0
2/1
2/1
2/1
2/1
2/1

Table 3: Number of instances, for which each algorithm has the best runtime (#BT) and best number
of expanded nodes (#BN), WCSPs. Out of 61 instances 12 have exact heuristics. The table
accounts for remaining 49, i-bound= 16 .

2 WCSP instances. As seen in Table 3, on these two instances BE+m-BF demonstrated the best
runtime among all the schemes.
m-AOBB-tree. For a number of problems for small values of m, m-AOBB-tree is superior to mBB-tree both in terms of the runtime and in number of expanded nodes. For example, for 29.wcsp,
i=4, m=10 m-AOBB-tree requires 2.63 seconds to solve the problem and expands 147851 nodes
while the runtime of m-BB-tree is 12.89 seconds and it expands 2245137 nodes. However, on the
majority of instances m-AOBB-tree is slower than all other schemes, as seen in Figure 9. Moreover,
m-AOBB-tree scales poorly with the number of solutions. For m = 100 it very often has both the
worst runtime and the largest explored search space among all the schemes, e.g. i=8, 503.wcsp.
Such striking decrease in performance as m grows is consistent across various benchmarks and
can be explained by the need to combine sets of partial solutions at AND nodes as we described
earlier. The overhead connected to AND/OR decomposition also accounts for the larger time per
node ratio of m-AOBB-tree, compared to other schemes. For example, in Table 2 for instance
myciel5g 3.wcsp, i=8, for m=10 and m=100 m-AOBB-tree expands less nodes than m-BB-tree, but
its runtime is larger. Nevertheless, m-AOBB-tree has its benefits. Since it is more space efficient
than the other algorithms, it is often the only scheme able for find solutions for the harder instances,
especially when the heuristic is weak, as we see for myciel5g 3.wcsp for i=4 and satellite01ac.wcsp
for both i=4 and i=8, respectively.
m-BB-tree. In Figure 9 we see that m-BB-tree solves almost the same number of problems as
m-AOBB-tree while having considerably better median time.
m-AOBF-tree and m-AOBF-graph. Unsurprisingly, best-first search algorithms often run out of
space on problems feasible for branch and bound, such as 503.wcsp and myciel5g 3.wcsp for i=8.
m-AOBF-based schemes are overall inferior to other algorithms, solving, as Figure 9 shows, the
least number of problems. Both schemes run out of memory much more often than m-A*-tree. We
believe this is due to overhead of maintaining an OPEN list of partial solution trees, as opposed of
an OPEN list of individual nodes as m-A*-tree does. Whenever the m-AOBF schemes do manage
to find solutions, as for example for instance 29.wcsp, i=8, m-AOBF-graph explores the smallest
926

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

search space among the schemes, except for BE+m-BF. At the same time m-AOBF-tree sometimes
expands more nodes compared not only to m-AOBF-graph, but also to m-A*-tree, which is not what
we would normally expect, since m-A*-tree traverses an OR search space, which is inherently larger
than the AND/OR one. However, it is important to remember that though better space efficiency
of AND/OR schemes is often observed, it is not guaranteed. Many factors, such as tie breaking
between the nodes with the same value of evaluation function, can impact the performance of mAOBF-tree. m-AOBF-tree and m-AOBF-graph have almost the same median time and number of
solved problems, as seen in Figure 9.
m-A*-tree. Out of the three best first algorithms m-A*-tree is overall the best. In Figure 9 we see
that it solves more instances than all other schemes for all values of m and its median runtime is
close to that of BE+m-BF. Table 3 proves that for i-bound=16 this scheme is the fastest among all
the schemes on largest number of instances, showing best runtime on 4-5 instances, depending on
m. This is explained in part by the relatively reduced overhead for maintaining the search space and
OPEN list in memory, compared for example with the m-AOBF schemes.
7.2.2 P EDIGREES
Table 4 displays the results for select instances from the Pedigree benchmark for two i-bounds each.
Overall, the difference between the results for the algorithms greatly diminishes as the heuristic
strength increases. Figure 10 shows the median time and number of solved instances for select
values of m for i=16. The number of instances for which each of the schemes had the best runtime
and best number of expanded nodes for the same i-bound is presented in Table 5.
BE+m-BF. Here too BE+m-BF is often superior to other algorithms, especially when the other
schemes use lower values of the i-bound, e.g. pedigree23, i=12, all ms. For large i-bounds and thus
more accurate heuristics the difference is much smaller. Moreover, sometimes BE+m-BF can be
slower than other schemes, due to the time required to calculate the exact heuristic, e.g. pedigree23,
i=16. Table 5 shows that BE+m-BF is overall the fastest. We see that on the Pedigree benchmark
this algorithm is quite successful, as is evident from the many instances it solved (see Figure 10).
m-AOBB-tree. For low values of m m-AOBB-tree is slightly superior to all other algorithms,
solving the most number of instances, (see Figure 10). On the other hand, its median time is the
largest. It fails to solve any instances for m=100. From Table 4 we see that m-AOBB-tree is the
slowest, (e.g., pedigree23, i=16, all ms). Yet, for instance pedigree33, i=12, m=1, this scheme is
the only one to find any solution.
m-BB-tree. As expected, m-BB-tree is inferior to the best-first search schemes unless the latter
run out of memory. As was the case for WCSP, this scheme is often faster than m-AOBB-tree, for
example, on pedigree30, i=16, all values of m. The bar charts show that m-BB-tree has the second
worst median time for all values of m, but solves the most number of problems for m=100.
m-AOBF schemes. Both m-AOBF algorithms are unsuccessful on the Pedigree benchmark. They
often run out of memory even for m = 1 (e.g. pedigree33, i=22). For most instances where they
do report solution m-AOBF-tree is faster than m-AOBF-graph, though the difference is usually not
very large.
m-A*-tree. As we saw for WCSPs, on some pedigree instances m-A*-tree is faster than the two
m-AOBF schemes, as seen in Figure 10, all values of m. Moreover, it is superior on harder instances
927

fiF LEROVA , M ARINESCU , & D ECHTER

instance
(n,k,w ,h)

i-bound

pedigree33

12

algorithm

m=1
time

(798, 4, 24, 132)
22

pedigree30

12

(1290, 5, 20, 105)

16

pedigree23

12

(403, 5, 21, 64)

16

pedigree20

12

(438, 5, 20, 65)
16

nodes

number of solutions
m=10
time
nodes
OOM
OOM
OOM
Timeout
Timeout
OOM
OOM
OOM
1.55
77138
4.15
177397
Timeout
OOM

m=100
nodes
OOM
OOM
OOM
Timeout
Timeout
OOM
OOM
OOM
3.76
112422
21.48
655141
Timeout
OOM
time

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
Timeout
7814.77
145203641
OOM
OOM
OOM
1.32
73625
2.98
145717
2.88
70644
OOM

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
Timeout
2510.59
33453995
7.90
6423
OOM
OOM
65.43
4866388
84.28
12243789
594.36
6907399
7.90
6423

OOM
OOM
OOM
Timeout
OOT
7.99
7611
OOM
OOM
65.86
4867551
85.72
12298570
Timeout
7.99
7611

OOM
OOM
OOM
Timeout
OOT
9.22
23028
OOM
OOM
67.01
4882985
127.25
13027245
Timeout
9.22
23028

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
8.44
713664
23.0
4446224
32.11
831096
7.11
630
OOM
OOM
0.52
53862
4.5
837288
13.39
346145
7.11
630

OOM
OOM
715729
4676953
75355901
2482
OOM
OOM
55927
959931
50252107
2482

OOM
OOM
11.15
904802
35.46
6179124
Timeout
7.68
19297
OOM
OOM
1.17
85300
14.14
1641751
OOM
7.68
19297

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

27.0
34.23
88.85
24.95

24.94
27.32
54.46
24.95

OOM
OOM
2321986
7239379
4365855
491
OOM
OOM
2279192
5857301
2531322
491

8.46
24.56
1077.9
7.24

0.58
5.61
713.73
7.24

26.66
37.63
1019.76
24.99

24.93
29.14
970.61
24.99

OOM
OOM
2324701
7434961
63940515
3482
OOM
OOM
2281907
5946423
59333828
3482

OOM
OOM
2353927
9155747
Timeout
26.16
32643
OOM
OOM
25.97
2311133
40.2
6617200
Timeout
26.16
32643
27.47
66.81

Table 4: Pedigrees: CPU time (in seconds) and number of nodes expanded. A Timeout stands
for exceeding the time limit of 3 hours. OOM indicates out of 4GB memory. In bold
we highlight the best time and number of nodes for each m.Parameters: n - number of
variables, k - domain size, w - induced width, h - pseudo tree height.

infeasible for both m-AOBF schemes and BE+m-BF, e.g. pedigree23, i=16. As shown in Figure 10,
it solves 5 instances for i=16, for all ms, which is the best or second best results, depending on the
number of solutions. However, the median time of m-A*-tree is considerably larger than that of
BE+m-BF, while for this i-bound the latter solves only a single instance less.
7.2.3 B INARY G RIDS
Table 6 shows the results for select instances from the grid networks domain. Figure 11 shows the
median runtime and number of solved instances for i=18, while Table 7 presents the number of
instances, for which an algorithm is the best, for the same i-bound. Most trends in the algorithms
928

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

m-AOBF graph
m-A* tree

m-BB tree
m-AOBB tree

842.17

BE+m-BF
m-AOBF tree

6.66

5.62

5.65

28.13

29.14

40.2

98.29

m-BB tree
m-AOBB tree
5
5
4

5
5
2

4

5
5
5

6
4

5
5

6

0.0

0.0
0.0

100

m-AOBF graph
m-A* tree

5
5
4

10

m

BE+m-BF
m-AOBF tree

10

100

0.0

5

m

0.0
0.0

2

0.0
0.0

1

0.0
0.0

10-1

0.0
0.0

100

0.0
0.0

Solved instances
Pedigrees, i=16

0.32
0.0
0.0

5

2

4

1

0.27
0.0
0.0

0.23
0.0
0.0

100

0.23
0.0
0.0

1.39

5.6

101

5.58

27.32

102

27.41

62.91

Median time
Pedigrees, i=16

280.27

103

Figure 10: Median time and number of solved instances (out of 12) for select values of m for Pedigrees, i-bound=16. Numbers above bars - actual values of time (sec) and # instances.
Total instances in benchmark: 13 , discarded instances due to exact heuristic: 1.

behavior observed on WCSP and Pedigree benchmarks can be also noticed on the Grid benchmark
as well. In particular, m-AOBB-tree is very successful when m is small, even solving the most
instances, as seen in Figure 11. But it shows worse results for m=100 and for any number of
solutions has the largest median time. m-BB-tree has smaller median time for all ms, but is still
929

fiF LEROVA , M ARINESCU , & D ECHTER

algorithm

Not solved
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

Pedigrees: # inst=13, n=335-1290
k=3-7, w =15-47, hT =52-204, i-bound=16
m=1
m=2
m=5
m=10
m=100
#BT / #BN #BT / #BN #BT / #BN #BT / #BN #BT / #BN
6
6
6
6
6
0/0
0/0
0/0
0/0
0/0
0/0
0/0
0/0
0/0
0/0
1/1
1/1
1/1
1/1
1/1
0/0
0/0
1/1
1/1
1/1
1/1
1/1
0/0
0/0
0/0
4/4
4/4
4/4
4/4
4/4

Table 5: Number of instances, for which each algorithm has the best runtime (#BT) and best number
of expanded nodes (#BN), Pedigrees. Out of 13 instances 1 have exact heuristics. The table
accounts for remaining 12, i-bound= 16 .

considerably slower than any of the best-first schemes. m-A*-tree presents the best compromise
between a small medium running time and a relatively large number of solved instances. Table 7
shows that for majority of grid instances it is the fastest algorithm. The two m-AOBF schemes have
results quite similar to each other, solving almost the same number of instances for all ms with little
difference in median runtimes, as is shown in Figure 11. They both are consistently inferior to all
other schemes except for BE+m-BF, which often runs out of memory. The main difference of the
Grid benchmark compared with the previously discussed domains lies in the behaviour of BE+mBF when the i-bound is high. Even though it expands less nodes, for many problems BE+m-BF is
slower than the other schemes due to the large time required to compute the exact heuristic. For
example, on grid 75-19-5, i=18, for m=10 the runtime of BE+m-BF is 143.11 seconds, while even
m-AOBB-tree, known to be slow, terminates in just 94.0 seconds. At the same time, for this instance
BE+m-BF explores the smallest search space for all values of m.
7.2.4 P ROMEDAS
Table 8 shows the results for the Promedas benchmark. Figure 12 presents the median time and number of solved instances for the benchmark for i=16. Table 9 shows for the same i-bound the number
of instances for which each of the schemes had the best runtime and best number of expanded nodes.
A significant fraction of the instances is not solved by any of the algorithms, especially for low and
medium i-bounds. Unlike the other benchmarks, m-AOBB-tree not only solves the most instances
for small ms, but also is quite successful for m=100, solving only one instance less than the best
scheme for this value of m, m-BB-tree. Moreover, sometimes m-AOBB-tree is the only scheme to
report any solutions, especially for weak heuristic, e.g. or chain 50.fg and or chain 212.fg, i=12.
BE+m-BF runs out of memory on most instances, as seen in Table 8. Overall, the variance of the
algorithms performance is more significant for Promedas than for the previously discussed benchmarks. For example, as we see in Figure 12, for i=16 m-A*-tree, m-BB-tree and m-AOBB-tree solve
between 25 and 33 instances for m  [1, 10], while BE+m-BF and both m-AOBF-based schemes
solve only between 4 and 8 instances. Table 9 demonstrates that m-A*-tree most often is the fastest
of the algorithms.
930

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

instance
(n,k,w ,h)

50-15-5

i-bound

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
8.83
866865
8.75
1967152
3.29
251502
OOM
OOM
OOM
OOM
OOM
Timeout
347.24
17332742
OOM
OOM

number of solutions
m=10
time
nodes
OOM
OOM
11.97
1177549
11.91
2647393
34.28
2485393
OOM
OOM
OOM
OOM
OOM
Timeout
692.59
28676212
OOM
OOM

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
Timeout
3290939
289
OOM
OOM
51205
355700
85289
289

OOM
OOM
OOM
Timeout
368.18
16431707
18.47
1220
OOM
OOM
0.39
55783
1.83
421798
116.77
7505310
18.47
1220

OOM
OOM
OOM
Timeout
Timeout
18.67
9534
OOM
OOM
0.89
104621
4.92
892065
Timeout
18.67
9534

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
Timeout
347.24
17332742
OOM
OOM
OOM
OOM
1.3
153399
74.83
14968683
46.53
2450725
OOM
OOM

OOM
OOM
OOM
Timeout
692.59
28676212
OOM
OOM
OOM
OOM
1.88
211547
76.93
15403354
118.26
4940247
OOM
OOM

OOM
OOM
OOM
Timeout
2277.92
75442102
OOM
OOM
OOM
OOM
3.53
362344
85.42
16631321
563.22
18306275
OOM
OOM

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
Timeout
3591.1
119431966
143.11
361
OOM
OOM
14.3
1609506
16.27
4005082
39.66
1367955
143.11
361

OOM
OOM
OOM
Timeout
Timeout
143.11
2330
OOM
OOM
18.76
2029844
22.28
5320573
94.0
3480629
143.11
2330

OOM
OOM
OOM
Timeout
Timeout
144.11
16897
OOM
OOM
28.04
2995437
37.26
8191215
Timeout
144.11
16897

algorithm

m=1
time

10

(400, 2, 27, 99)
18

50-17-5
10

(289, 2, 22, 84)
18

90-20-5

10

(400, 2, 27, 99)

18

75-19-5

10

(361, 2, 25, 89)
18

82.95
18.45

0.35
1.39
1.79
18.45

nodes

m=100
time

nodes
OOM
OOM
20.22
1931039
22.06
4708311
Timeout
OOM
OOM
OOM
OOM
OOM
Timeout
2277.92
75442102
OOM
OOM

Table 6: Grids: CPU time (in seconds) and number of nodes expanded. A Timeout stands for
exceeding the time limit of 3 hours. OOM indicates out of 4GB memory. In bold we
highlight the best time and number of nodes for each m. Parameters: n - number of
variables, k - domain size, w - induced width, h - pseudo tree height.

7.2.5 P ROTEIN
Table 10 shows select Protein instances for i=4 and i=8, respectively. Figure 13 and Table 11 show
the summary of the results for i=4. This benchmark is fairly difficult due to very large domain size
(up to 81). The heuristic calculation is not feasible for higher i-bounds. In particular, BE+m-BF has
considerable problems in calculating the exact heuristic. Even for low i-bounds only relatively easy
instances are solved. Note that for instances pdb1ctk and pdb1dlw i-bound=8 yields exact heuristic.
Both m-AOBF-tree and m-AOBF-graph fail to find any solutions within the memory limit on the
majority of instances, e.g., pdb1b2v and pdb1cxy, i=4. There is not much difference between the
runtimes of all algorithms, with an exception of m-AOBB-tree. For example, for pdb1b2v, i=8,
931

fiF LEROVA , M ARINESCU , & D ECHTER

m-AOBF graph
m-A* tree

m-BB tree
m-AOBB tree
440.64

BE+m-BF
m-AOBF tree

37.26
0.16

10

m

100

m-BB tree
m-AOBB tree
13
15

13
15
18

13
15

13
15

19

m-AOBF graph
m-A* tree

19

BE+m-BF
m-AOBF tree

0.39

1.04
1.23
0.02

0.38

0.9
1.3
0.01

5

2

2.14
3.23
1.0

22.28

20.16
45.35

17.49
28.04
0.36

1

13
15
18

10-1

0.0

0.35

0.01

100

0.83
0.78

101

1.43
0.88

16.27
20.68

Median time
Grids, i=18

102

91.79

103

6
5
3
3

4

5
5

5
5
4

5

6
4

5

5

Solved instances

Grids, i=18

6

101

100

1

2

5

m

10

100

Figure 11: Median time and number of solved instances (out of 31) for select values of m for
Grids, i-bound=18. Numbers above bars - actual values of time (sec) and # instances.
Total instances in benchmark: 32, discarded instances due to exact heuristic: 1.

m-AOBB-tree requires 6.46 seconds to find m=10 solutions, while the runtimes of other algorithms
range from 0.03 to 0.09 seconds (except for BE+m-BF which runs our of memory). However,
the slow performance of m-AOBB-tree on easier problems that are feasible for all algorithms is
compensated by the fact that for many instances it is the only scheme to report any solution, solving
932

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

algorithm

Not solved
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

Grids: # inst=32, n=144-2500
k=2-2, w =15-74, hT =48-312, i-bound=18
m=1
m=2
m=5
m=10
m=100
#BT / #BN #BT / #BN #BT / #BN #BT / #BN #BT / #BN
12
12
13
13
14
0/0
0/0
0/0
0/0
0/0
0/0
0/1
0/2
0/1
0/3
8/2
8/2
8/6
9/6
7/6
1/0
1/0
0/0
1/0
2/2
7 / 10
7 / 10
5/5
5/5
2/2
5/7
5/6
6/5
6/6
6/4

Table 7: Number of instances, for which each algorithm has the best runtime (#BT) and best number
of expanded nodes (#BN), Grids. Out of 32 instances 1 have exact heuristics. The table
accounts for remaining 31, i-bound= 18 .

most instances by considerable amount for m  [1, 10] (Figure 13). Table 11 shows that m-AOBBtree is the best both in terms of time and space for the overwhelming majority of problems for all
values of m except for m = 100.
7.2.6 S EGMENTATION
Table 12 shows the results for select instances from the Segmentation benchmark for two i-bounds,
namely i=4 and i=12, while Figure 14 and Table 13 present the summary of the results for i=12.
Unlike WCSP, for this benchmark we chose to display relatively low i-bounds not because calculating heuristic with larger is is infeasible, but because the problems have low induced width and
we wished to avoid displaying results obtained with exact heuristics. The main peculiarity of this
benchmark is the striking success of BE+m-BF. Overall it solves as many instances as the usually
superior m-A*-tree and m-BB-tree, as is seen in Figure 14. Moreover, its runtime is superior to
the other schemes, as is true for all instances in Table 12 and is also illustrated by the results in the
Table 13. When the heuristic is very weak, m-AOBB-tree is fairly successful, for example, finding
solutions for all values of m for 12 4 s.binary, i=4, which is infeasible for any other scheme except
for BE+m-BF. However, as usual, m-AOBB-tree is the overall slowest of the schemes.
7.3 Best-first vs Depth-First Branch and Bound for the M Best Solutions
Let us again consider the data presented in Tables 2-13 and Figures 9-14 in order to summarize our
observations and contrast the performance of best-first and depth-first branch and bound schemes.
Among the best-first search schemes m-A*-tree is the most successful. It is often very effective,
when armed with a good heuristic, and requires less space than the other best-first schemes. As we
already noted, BE+m-BF shows good results on the Segmentation benchmark, where it is the best
algorithm in terms of the median runtime, while solving at least the same number of problems as
the other schemes. However, on the other benchmarks the calculation of the exact heuristic is often
infeasible.
933

fiF LEROVA , M ARINESCU , & D ECHTER

instance
(n,k,w ,h)

i-bound

16

(620, 2, 30, 64)

22

(676, 2, 30, 70)

or chain 212.fg

12

16

12

(773, 2, 33, 79)
22

or chain 50.fg

m=1
time

or chain 107.fg

or chain 141.fg

algorithm

12

661, 2, 36, 76)
22

nodes

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
7.89
919865
14.58
3139711
67.95
1398364
OOM
OOM
OOM
9.2
1093564
17.0
3861414
122.01
3214924
OOM

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
9553.91
1276222668
272.0
9878480
OOM
OOM
OOM
14.16
1261489
279.61
56821714
140.9
6490042
OOM

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

number of solutions
m=10
time
nodes
OOM
OOM
18.89
2108122
35.15
7051974
229.49
5134280
OOM
OOM
OOM
20.83
2465364
42.36
9205755
418.46
11123810
OOM

m=100
time

44.61
102.6
627.94

56.59
100.45
855.84

nodes
OOM
OOM
4641627
18494630
13594667
OOM
OOM
OOM
6356871
19217427
21388619
OOM

OOM
OOM
OOM
OOT
25481595
OOM
OOM
OOM
3379926
87947802
14103095
OOM

OOM
OOM
OOM
OOT
2091.26
64400241
OOM
OOM
OOM
OOM
885.72
160581726
909.48
33842266
OOM

OOM
OOM
OOM
Timeout
49808550
OOM
OOM
OOM
1118792
15922806
11336657
OOM

OOM
OOM
OOM
Timeout
4206.07
111853485
OOM
OOM
OOM
33.87
3669711
141.66
27615033
1239.88
24717964
OOM

OOM
OOM
OOM
Timeout
Timeout
OOM
OOM
OOM
78.37
8186757
342.51
58246101
5032.11
86444575
OOM

OOM
OOM
OOM
Timeout
1404.27
33495406
OOM
OOM
OOM
53.87
5673948
91.15
18515503
Timeout
OOM

OOM
OOM
OOM
Timeout
3748.85
93992107
OOM
OOM
OOM
OOM
176.14
34915510
Timeout
OOM

OOM
OOM
OOM
Timeout
10070.0
245628104
OOM
OOM
OOM
OOM
447.46
85945673
Timeout
OOM

1772.8

9.91
78.08
584.83

721.67

38.48
460.15
315.2

Table 8: Promedas: CPU time (in seconds) and number of nodes expanded. An Timeout stands
for exceeding the time limit of 3 hours. OOM indicates out of 4GB memory. In bold
we highlight the best time and number of nodes for each m. Parameters: n - number of
variables, k - domain size, w - induced width, h - pseudo tree height.

The two m-AOBF-based schemes are overall inferior due to prohibitively large memory, solving
fewer instances than the other algorithms. We believe that a non-trivial extension of AOBF from
a single solution to the m-best task is not straightforward, because it is hard to represent multiple
partial solution trees in an efficient manner. In order to have an efficient m-AOBF implementation,
one needs to quickly identify which partial solution subtree to select and extend next, when searching for the (k + 1)th solution after finding the k th best solution. While AOBF (for 1 solution) uses
an arc-marking mechanism to efficiently represent the current best partial solution subtree during
search, this is not easy to extend for the case when searching for the m best solutions. Therefore,
as was shown in Section 5.1, our m-AOBF implements a naive mechanism where each of the par934

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

BE+m-BF
m-AOBF tree

105

m-AOBF graph
m-A* tree

m-BB tree
m-AOBB tree

10

m

100

m-AOBF graph
m-A* tree

m-BB tree
m-AOBB tree
27
26
8

8

8

8

2

3

4
4

5

6

6

7

7
7

Solved instances

Promedas, i=16

101

8

17

26
27
30

28
27
32

28
27
33

BE+m-BF
m-AOBF tree

1.36
2.97
4.45
3.62

60.1
7.47
0.15
1.55
2.0

3.43
1.95
7.08
0.08

5

2

185.53

325.85

267.69
51.79

146.85
0.03

1

25
27
31

10-1

2.03
1.37

5.19
0.01
0.48
0.62

101

100

7.64

33.68

102

45.83

109.68

Median time
Promedas, i=16

103

1268.88

104

100

1

2

5

m

10

100

Figure 12: Median time and number of solved instances (out of 86) summary for select values of
m for Promedas, i-bound=16. Numbers above bars - actual values of time (sec) and #
instances. Total instances in benchmark: 75, discarded instances due to exact heuristic:
11.

tial solution trees is represented explicitly in memory. This simple representation, however, incurs
a considerable computational overhead when searching for the m best solutions, which is indeed
revealed by our experiments. A more efficient implementation of m-AOBF is left for future work.
935

fiF LEROVA , M ARINESCU , & D ECHTER

algorithm

Not solved
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

Promedas: # inst=86, n=197-2113
k=2-2, w =5-120, hT =34-187, i-bound=16
m=1
m=2
m=5
m=10
m=100
#BT / #BN #BT / #BN #BT / #BN #BT / #BN #BT / #BN
42
42
45
44
46
0/0
0/0
0/0
0/0
0/0
0/1
0/2
0/2
0/3
0/2
22 / 17
21 / 17
18 / 17
18 / 15
9/9
1/0
1/0
1/0
2/0
10 / 5
4/8
4/8
3/5
4/8
2/7
8/7
8/6
8/6
8/5
8/6

Table 9: Number of instances, for which each algorithm has the best runtime (#BT) and best number
of expanded nodes (#BN), Promedas. Out of 86 instances 11 have exact heuristics. The
table accounts for remaining 75, i-bound= 16 .

Unsurprisingly, the branch and bound algorithms are more robust in terms of memory and also
dominate m-A*-tree and other best-first schemes on many benchmarks in terms of the number of
instances solved. However, they tend to have considerably larger median time and expand more
nodes. In particular, m-AOBB-tree does not scale well with the number of solutions and for large
values of m the runtime increases drastically. Unlike m-AOBF, whose inferior performance can be
attributed to specifics of implementation, the depth-first m-AOBB suffers from issues inherent to
solving the m-best problem in a depth-first manner. As Algorithm 10 describes, m-AOBB needs to
merge the m best partial solution at each internal node, which hurts the performance significantly,
but cannot be avoided, unless the algorithmic approach itself is fundamentally changed. We did not
see a way to overcome this limitation.
Overall, whenever the calculation of the exact heuristic is feasible, BE+m-BF should be the
algorithm of choice. Otherwise, m-A*-tree is superior for the relatively easy problems, while mAOBB-tree is the best scheme for hard memory intensive instances. This superiority of a best-first
approach, whenever memory is available, is expected, based, on the one hand, on intuition derived
from our knowledge of the task of finding a single solution, and on the other hand, on the theoretical
results in Section 3.2.
7.4 Scalability of the Algorithms with the Number of Required Solutions
Figures 15-17 present the plots showing the runtime in seconds and the number of expanded nodes
as a function of number of solutions m (on a log scale) for two instances from each benchmark.
Figure 15 displays results for WCSP and Pedigree benchmarks, Figure 16 - for Grids and Promedas,
Figure 17 - for Proteins and Segmentation. Lower values (on the y-axis) are preferable. Each row
contains two instances from each benchmarks for a specific value of the i-bound, the runtime plots
being shown above the ones containing the expanded nodes. The examples are chosen to best
illustrate the prevailing tendencies.
Note that the theoretical analysis suggests that the runtime of BE+m-BF, the best among the

algorithms, should scale with m since its worst case complexity is O(nk w + mn). The theoretical
complexity of the best-first search schemes m-AOBF-tree and m-A*-tree is linear in the number of
936

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

instance
(n,k,w ,h)

pdb1b2v

(133, 36, 13, 33)

pdb1cxy

(70, 81, 9, 19)

i-bound

m=1
time

4

8

4

8

pdb1ctj

4

(62, 81, 8, 21)

8

pdb1dlw

4

(84, 81, 8, 29)

algorithm

8

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

nodes
OOM
OOM

0.12
10.23
6.51
0.02
0.03
0.0
0.01
0.29

2508
948337
100584
OOM
95
95
139
2401
6563
OOM

number of solutions
m=10
nodes
OOM
OOM
0.17
3186
11.09
1034645
35.14
827365
OOM
0.06
294
0.09
108
0.03
597
0.07
8861
6.46
256588
OOM
time

OOM
OOM
0.38
0.4

3708
51020
OOM
OOM
OOM
0.01
121
0.03
5791
0.66
7029
OOM

10.43
844.08
5.64
0.01
0.0
0.02
0.01
0.01
0.22
0.01

46.17
47.27
187.38
0.01

0.14
1.06
18.53
0.01

OOM
OOM
35400
76609260
74833
62
49
45
62
1118
3098
62
OOM
OOM
579108
6380302
1451906
294
OOM
OOM
6900
162913
154850
294

m=100
time

nodes
OOM
OOM

0.34
14.4
4462.0
0.42
0.64
0.15
0.5

6249
1404370
230849005
OOM
1956
135
3051
67330
Timeout
OOM

OOM
OOM
0.48
0.6

OOM
OOM
4434
73849

OOM

10854
191203
OOM
OOM
OOM
0.1
2870
0.27
53702
44.28
1335157
OOM

OOM
OOM

OOM
OOM

OOM
OOM
OOM
0.04
0.07
2.04

13.23
1039.96
18.29
0.02
0.07
0.11
0.03
0.03
2.32
0.02

46.26
47.33
544.55
0.05

0.18
1.09
157.01
0.05

480
11429
34567

43538
94422614
306054
265
302
74
265
5385
54324
265
OOM
OOM
579405
6391107
12759004
635
OOM
OOM
7240
167037
8632114
635

0.94
1.45

19.74
1325.84
157.43
0.07
0.42
0.75
0.08
0.15
71.86
0.07

46.49
50.72
0.39

0.52
1.86
0.39

65340
120786833
5307198
1050
1825
95
1057
31066
3273324
1050
OOM
OOM
582375
6762911
OOT
4265
OOM
OOM
10855
280189
OOT
4265

Table 10: Protein: CPU time (in seconds) and number of nodes expanded. An Timeout stands
for exceeding the time limit of 3 hours. OOM indicates out of 4GB memory. In bold
we highlight the best time and number of nodes for each m. Parameters: n - number of
variables, k - domain size, w - induced width, h - pseudo tree height.

solutions, while for m-BB-tree the overhead due to the m-best task is a factor of (m  log m) and
for m-AOBB-tree it is (m log m  deg), where deg is the degree of the pseudo tree. We observed
that compared to other schemes the runtime of BE+m-BF indeed rises quite slowly as the number
of solutions increases, even as m reaches 100. The runtime m-A*-tree also scales well with m. The
behaviour of m-BB-tree depends a lot on the benchmarks. On Pedigrees and Protein its runtime
changes very little on most instances as the number of solutions grows, but on the other benchmarks,
the runtime for m=100 tends to be significantly larger then for m=1. m-AOBF-tree and m-AOBFgraph often do not provide any solutions even for m=1 or, alternatively, run out of memory as
m slightly increases (m  [2, 10]). These algorithms are clearly not successful in practice. As
937

fiF LEROVA , M ARINESCU , & D ECHTER

BE+m-BF
m-AOBF tree

105

m-AOBF graph
m-A* tree

m-BB tree
m-AOBB tree

1457.61

45.28

41.91

190.28
39.45

177.3
37.46

102

220.64

103

50.72

597.38

Median time
Protein, i=4

104

5

2

10

m

100

10
10

13
13

13
13

13
13

13
13

Solved instances
Protein, i=4

20

25

35

42
35
25

35

42

m-BB tree
m-AOBB tree

25

35
25

25

35

44

m-AOBF graph
m-A* tree

44

BE+m-BF
m-AOBF tree

0.14
0.38
0.67

2.95

2.89

0.03
0.19
0.39

1

0.01
0.17
0.32

2.87
0.01
0.14
0.27

2.84
0.01
0.13
0.25

100

3.39

101

100

1

2

5

m

6

6

6

6

6

101

10

100

Figure 13: Median time and number of solved instances (out of 72) for select values of m for
Protein, i-bound=4. Numbers above bars - actual values of time (sec) and # instances.
Total instances in benchmark: 72, discarded instances due to exact heuristic: 0.

we discussed before, both the runtime and number of expanded nodes of m-AOBB-tree increase
drastically as m gets larger.
938

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

algorithm

Not solved
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

m=1
#BT / #BN
26
8/3
1 / 10
11 / 9
8/1
21 / 22
6/4

Protein: # inst=72, n=15-242
k=18-81, w =5-16, hT =7-44, i-bound=4
m=2
m=5
m=10
#BT / #BN #BT / #BN #BT / #BN
26
27
27
7/1
7/1
7/0
2 / 12
2 / 12
1 / 13
9/9
12 / 9
14 / 10
9/2
8/3
6/3
21 / 21
18 / 19
17 / 17
6/2
5/2
5/2

m=100
#BT / #BN
34
5/0
1 / 10
17 / 13
11 / 10
3/3
6/2

Table 11: Number of instances, for which each algorithm has the best runtime (#BT) and best
number of expanded nodes (#BN), Protein. Out of 72 instances 0 have exact heuristics.
The table accounts for remaining 72, i-bound= 4 .

7.5 Comparison with Competing Algorithms
We compare our methods with a number of previously developed schemes described in more details
in Section 6: STRIPES, PESTEELARS and Nilssons algorithm. The implementations of these
schemes were provided by Dhruv Batra. The first two approaches are based on ideas of LP relaxations and are approximate, but are known to often find exact solutions, though they provide
no guarantees of optimality. Nilssons algorithm is an exact message-passing scheme operating on
a junction tree. For the first set of experiments (on a tree benchmark) we also show results for
STILARS algorithm, an older version of the PESTEELARS algorithm. However, this scheme is
consistently inferior to the other two LP-based schemes and is not considered for the other two
benchmarks. In the following, we collectively refer to these 4 algorithms as competing schemes.
7.5.1 R ANDOMLY G ENERATED B ENCHMARKS
The available to us code of the LP-based and Nilssons approaches was developed to run on restricted inputs only, and so it could not be applied to the benchmarks used in the bulk of our evaluation described above. We concluded that re-implementing the competing codes to work on general
input would be too time consuming and would not provide any additional insights. Thus we chose
to compare our algorithms with the competitors using benchmarks that can be acceptable to the
competing schemes.
Specifically, the comparison was performed on the following three benchmarks: random trees,
random binary grids and random graphs with submodular potentials, that we call submodular
graphs in the remainder of the section. Table 14 shows the parameters of the benchmarks. The
instances were generated in the following manner. First, a vector of 12 logarithmically spaced integers between 10 and 103.5 was generated, serving as the number of variables for the instances.
For binary grids benchmarks each value was used to generate two problems with the same number
of variables. The edges between the variables were generated uniformly randomly, while making
sure that the end graph is a tree, grid or a loopy graph, depending on the benchmark. For each edge
we define a binary potential and for each vertex a unary potential in an exponential form: f = e ,
939

fiF LEROVA , M ARINESCU , & D ECHTER

instance
(n,k,w ,h)

i-bound

4

(225, 2, 16, 48)

12

(227, 2, 16, 57)

7 9 s.binary

(234, 2, 16, 53)

OOM
OOM
OOM
Timeout
164.91
5653312
0.0
225
7.31
103327
10.47
1843
0.03
3754
0.04
8251
0.08
4158
0.0
225

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
Timeout
71.71
2733703
0.01
227
0.23
3338
0.33
799
0.01
585
0.05
10687
0.21
11076
0.01
227

OOM
OOM
OOM
Timeout
360.14
14906212
0.02
1365
3.75
46121
5.72
1827
0.09
9103
0.19
30119
14.28
1054628
0.02
1365

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
Timeout
127.17
3337949
0.01
234
8.85
122663
OOM
0.02
1978
0.03
4415
0.05
2750
0.01
234

OOM
OOM
OOM
Timeout
505.08
17976200
0.03
1337
OOM
OOM
0.06
4170
0.11
13357
10.54
806490
0.03
1337

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
Timeout
110.19
4227437
0.01
231
OOM
OOM
1.02
102671
2.07
428791
0.75
39170
0.01
231

OOM
OOM
OOM
Timeout
555.6
23302165
0.03
1615
OOM
OOM
1.17
115407
2.9
527967
11.99
809403
0.03
1615

m=1
time

12 4 s.binary

16 16 s.binary

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

number of solutions
m=10
nodes
OOM
OOM
OOM
Timeout
505.82
18888321
0.02
1619
10.36
143333
OOM
0.06
5692
0.21
24349
1.62
118074
0.02
1619

algorithm

4

12

4

12

11 4 s.binary

4

(231, 2, 16, 57)

12

nodes

time

m=100
time

nodes
OOM
OOM
OOM
Timeout
4371.05
189179726
0.21
11194
OOM
OOM
0.3
18616
1.32
131571
489.57
40961080
0.21
11194

0.19

0.38
1.2
0.19

0.21

0.28
0.95
0.21

OOM
OOM
OOM
Timeout
OOT
11157
OOM
OOM
30542
141591
OOT
11157
OOM
OOM
OOM
Timeout
OOT
10212
OOM
OOM
15807
89675
OOT
10212

OOM
OOM
OOM
Timeout
OOT
0.28
14241
OOM
OOM
1.86
167983
7.1
1010155
8497.93
617227854
0.28
14241

Table 12: Segmentation: CPU time (in seconds) and number of nodes expanded. An Timeout
stands for exceeding the time limit of 3 hours. OOM indicates out of 4GB memory. In
bold we highlight the best time and number of nodes for each m. Parameters: n - number
of variables, k - domain size, w - induced width, h-pseudo tree height.

where  is a real number sampled from a uniform distribution. For the third benchmark the potentials are further modified to be submodular. On the random trees the m-best optimization LP
problem is guaranteed to be tight, on the graphs with submodular potentials the LP optimization
problem is tight, but its m-best extension is not, and on the arbitrary loopy graphs, including grids,
the algorithms provide no guarantees.
7.5.2 C OMPETING A LGORITHMS  P ERFORMANCE
Table 15 shows the runtimes for select instances from the random tree benchmark for our 5 mbest search schemes and the competing LP schemes STILARS, PESTEELARS and STRIPES. We
940

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

BE+m-BF
m-AOBF tree

m-AOBF graph
m-A* tree

m-BB tree
m-AOBB tree

105

1949.3

103

5

100

13

14
14

18

19

20

24

24
24
24

m-BB tree
m-AOBB tree

24

24
24
24

24
20

24

24
24
24

m-AOBF graph
m-A* tree

20
20

21
20

Solved instances
Segmentation, i=12

24

24
24
24

BE+m-BF
m-AOBF tree

101

10

m

24
24

2

0.02
0.31
0.4
0.03
0.08

1

10-1

0.01
0.22
0.26
0.02
0.04
0.7

0.01
0.15
0.2
0.01
0.03
0.04

100

3.94

101

0.17
1.1
1.55
0.17
0.65

102

0.0
0.08
0.1
0.01
0.01
0.03

Median time
Segmentation, i=12

104

1

2

5

m

10

100

Figure 14: Median time and number of solved instances (out of 47) for select values of m for
Segmentation, i-bound=12. Numbers above bars - actual values of time (sec) and #
instances. Total instances in benchmark: 47, discarded due to exact heuristic: 0.

observed on this benchmarks that STILARS was always inferior to the other two schemes and
therefore it was excluded from the remainder of evaluation. Instead, in Tables 16 and 17, where
we show results for the random binary grids and submodular graphs benchmarks, we added for
comparison Nilssons max-flow algorithm. In the Table 15-17 the time limit was set to 1 hour,
memory limit to 3 GB. The schemes behavior is quite consistent across the instances.
941

fiF LEROVA , M ARINESCU , & D ECHTER

Time vs m. WCSPs: bwt3ac.wcsp

Time vs m. WCSPs: queen5_5_3.wcsp

(45, 11, 16, 27), i=4

(25, 3, 18, 21), i=16

7

15

6

Time, sec

Time, sec

5
10

5

4
3
2
1

0

0

1.0

10.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

2.0

1e6

m-A*-tree
m-BB-tree

100.0

1.0

m-AOBB-tree
BE+m-BF

10.0

m-AOBF-tree
m-AOBF-graph

Nodes vs m. WCSPs: bwt3ac.wcsp
(45, 11, 16, 27), i=4
1.4

1e6

100.0

Number of solutions m
m-A*-tree
m-BB-tree

m-AOBB-tree
BE+m-BF

Nodes vs m. WCSPs: queen5_5_3.wcsp
(25, 3, 18, 21), i=16

1.2
1.5

1.0

Nodes

Nodes

0.8

1.0

0.6

0.5

0.4

0.0

0.0

0.2

1.0

10.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

100.0

1.0

m-AOBB-tree
BE+m-BF

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

100.0

m-AOBB-tree
BE+m-BF

Time vs m. Pedigrees: pedigree9

Time vs m. Pedigrees: pedigree30

(1290, 5, 20, 105), i=16

(1119, 7, 25, 123), i=22

1400

2000

1200

1500

Time, sec

1000

Time, sec

10.0

Number of solutions m

800

1000

600
400
200
0

500
0

1.0

10.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

100.0

1.0

m-AOBB-tree
BE+m-BF

m-AOBF-tree
m-AOBF-graph

Nodes vs m. Pedigrees: pedigree30
1e7

10.0

Number of solutions m
m-A*-tree
m-BB-tree

100.0

m-AOBB-tree
BE+m-BF

Nodes vs m. Pedigrees: pedigree9

(1290, 5, 20, 105), i=16

5

3

(1119, 7, 25, 123), i=22

Nodes

4

1.5

Nodes

2.0

1e7

1.0

2

0.5

1

0.0

0
1.0

10.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

100.0

1.0

m-AOBB-tree
BE+m-BF

10.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

100.0

m-AOBB-tree
BE+m-BF

Figure 15: CPU time in seconds and number of expanded nodes as a function of number of solutions. WCSP and Pedigrees, 4 GB, 3 hours.

942

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

Time vs m. Grids: 50-19-5

Time vs m. Grids: 75-18-5

(361, 2, 25, 93), i=18

(324, 2, 24, 85), i=18

40

3500
3000
2500
2000
1500
1000
500
0

Time, sec

Time, sec

30
20
10
0
1.0

10.0

100.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

1.0

m-AOBB-tree
BE+m-BF

m-AOBF-tree
m-AOBF-graph

Nodes vs m. Grids: 50-19-5
6

7
6
5
4
3
2
1
0

m-A*-tree
m-BB-tree

m-AOBB-tree
BE+m-BF

(324, 2, 24, 85), i=18

1e5

5

Nodes

Nodes

4
3
2
1
0

1.0

10.0

100.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

1.0

m-AOBB-tree
BE+m-BF

10.0

100.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

Time vs m. Promedas: or_chain_17.fg

m-A*-tree
m-BB-tree

m-AOBB-tree
BE+m-BF

Time vs m. Promedas: or_chain_212.fg
(773, 2, 33, 79), i=22

(531, 2, 19, 51), i=16

5000

10

4000

8

Time, sec

Time, sec

100.0

Nodes vs m. Grids: 75-18-5

(361, 2, 25, 93), i=18

8 1e8

10.0

Number of solutions m

3000

6

2000

4

1000

2
0

0
1.0

10.0

m-AOBF-tree
m-AOBF-graph

1e5

100.0

Number of solutions m
m-A*-tree
m-BB-tree

1.0

m-AOBB-tree
BE+m-BF

10.0

m-AOBF-tree
m-AOBF-graph

Nodes vs m. Promedas: or_chain_17.fg
(531, 2, 19, 51), i=16

1e8

0.6

m-A*-tree
m-BB-tree

m-AOBB-tree
BE+m-BF

Nodes vs m. Promedas: or_chain_212.fg
(773, 2, 33, 79), i=22

Nodes

0.8

3

Nodes

4

100.0

Number of solutions m

0.4

2
1

0.2

0

0.0
1.0

10.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

100.0

1.0

m-AOBB-tree
BE+m-BF

10.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

100.0

m-AOBB-tree
BE+m-BF

Figure 16: CPU time in seconds and a number of expanded nodes as a function of number of solutions. Grids and Promedas, 4 GB, 3 hours.

943

fiF LEROVA , M ARINESCU , & D ECHTER

Time vs m. Protein: pdb1at0

Time vs m. Protein: pdb1b2v

(122, 81, 8, 25), i=4

(133, 36, 13, 33), i=8

800
700
600
500
400
300
200
100
0

7
6

Time, sec

Time, sec

5
4
3
2
1
0
1.0

10.0

100.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

1.0

m-AOBB-tree
BE+m-BF

m-AOBF-tree
m-AOBF-graph

Nodes vs m. Protein: pdb1at0

m-AOBB-tree
BE+m-BF

2.5
2.0

0.6

Nodes

Nodes

1.5

0.4

1.0

0.2

0.5

0.0

0.0
1.0

10.0

100.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

1.0

m-AOBB-tree
BE+m-BF

10.0

100.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

m-AOBB-tree
BE+m-BF

Time vs m. Segmentation: 10_16_s.binary

Time vs m. Segmentation: 7_29_s.binary

(230, 2, 15, 52), i=4

(234, 2, 15, 63), i=12

10000

8

8000

6

Time, sec

Time, sec

m-A*-tree
m-BB-tree

(133, 36, 13, 33), i=8

1e5

0.8

6000

4

4000

2

2000

0

0

1.0

10.0

100.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

1.0

m-AOBB-tree
BE+m-BF

10.0

100.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

m-AOBB-tree
BE+m-BF

Time vs m. Segmentation: 10_16_s.binary

Time vs m. Segmentation: 7_29_s.binary

(230, 2, 15, 52), i=4

(234, 2, 15, 63), i=12

10000

8

8000

6

Time, sec

Time, sec

100.0

Nodes vs m. Protein: pdb1b2v

(122, 81, 8, 25), i=4

1e7

10.0

Number of solutions m

6000

4

4000
2000

2

0

0
1.0

10.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

100.0

1.0

m-AOBB-tree
BE+m-BF

10.0

Number of solutions m

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

100.0

m-AOBB-tree
BE+m-BF

Figure 17: CPU time in seconds and number of expanded nodes as a function of number of solutions. Protein and Segmentation, 4 GB, 3 hours.

944

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

algorithm

Not solved
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

Segmentation: # inst=47, n=222-234
k=2-21, w =15-18, hT =47-67, i-bound=12
m=1
m=2
m=5
m=10
m=100
#BT / #BN #BT / #BN #BT / #BN #BT / #BN #BT / #BN
23
23
23
23
23
0/5
0/0
0/0
0/0
0/0
0/5
0/7
0 / 11
0 / 15
0 / 14
15 / 0
11 / 0
12 / 0
11 / 0
3/0
7/0
5/0
3/0
2/0
0/0
0/0
3/0
0/0
0/0
0/0
21 / 19
20 / 17
22 / 13
24 / 9
24 / 10

Table 13: Number of instances, for which each algorithm has the best runtime (#BT) and best number of expanded nodes (#BN), Segmentation. Out of 47 instances 0 have exact heuristics.
The table accounts for remaining 47, i-bound= 12 .

Benchmark
Random trees
Random Binary Grids
Random submodular graphs

# inst
12
24
12

n
10-5994
16-3192
16-3192

k
2-4
2
2

w
1
6-79
4-74

hT
5-132
9-221
9-208

Table 14: Benchmark parameters: # inst - number of instances, n - number of variables, k - domain
size, w - induced width, hT - pseudo tree height.

STILARS and Nilssons schemes are always dominated by the other two competing schemes
in terms of runtime. STRIPES and PESTEELARS are sometimes faster than all our schemes for
m=1, e.g. tree nnodes880 ps1 k4, however, on all three benchmark they scale rather poorly with
m. For m  5 they are almost always inferior to our algorithms, provided that the latter report
any results, with occasional exception of m-AOBB-tree, which also tends to be slow for large m.
The only problems on which PESTEELARS and STRIPES are superior to our search schemes
are the largest networks having over a 1000 variables, such as grid nnodes3192 ps2 k2, which
are infeasible for our algorithms. Overall, our five m-best algorithms proved superiority over the
considered competing schemes on the majority of instances, often having better runtime, especially
when m > 2, while guaranteeing solution optimality.

8. Conclusion
Most of the work on finding m best solutions over graphical models was focused on either iterative
schemes based on Lawlers idea or on dynamic programming (e.g., variable-elimination or treeclustering). We showed for the first time that for combinatorial optimization defined over graphical
models the traditional heuristic search paradigms are not only directly applicable, but often superior.
Specifically, we extended best-first and depth-first branch and bound search algorithms to solve
the m-best optimization task, presenting m-A* and m-BB, respectively. We showed that the properties of A* extend to the m-A* algorithm and, in particular, proved that m-A* is superior to any
945

fiF LEROVA , M ARINESCU , & D ECHTER

instance

tree nnodes245 ps1 k2

(245, 2, 2, 32)

tree nnodes880 ps1 k4

(880, 4, 2, 52)

tree nnodes5994 ps1 k4

(5994, 4, 2, 189)

algorithm

i-bound=4. k=2
m=5
m=10
time
time
0.05
0.09
0.08
0.13
0.01
0.02
0.02
0.03
0.06
14.14
7.93
33.3
0.4
0.88
0.51
1.32

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
STILARS
STRIPES
PESTEELARS

m=1
time
0.02
0.02
0.0
0.0
0.02
0.0
0.09
0.0

m=2
time
0.02
0.03
0.0
0.0
0.02
0.04
0.17
0.13

m=100
time
0.61
0.95
0.12
0.37
3045.25
1757.41
13.88
47.32

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
STILARS
STRIPES
PESTEELARS

0.3
0.48
0.08
0.1
1.17
0.0
5.67
0.0

0.46
0.76
0.17
0.3
1.37
0.11
11.26
0.87

1.06
1.8
0.24
0.54
52.36
28.19
28.09
6.13

1.9
3.28
0.48
1.23
927.12
81.21
56.41
9.26

OOM
OOM
3.67
14.38
Timeout
2440.22
607.01
79.0

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
STILARS
STRIPES
PESTEELARS

OOM
OOM
5.44
5.77
851.48
0.05
248.53
0.05

OOM
OOM
10.68
29.04
922.19
2.72
506.25
18.28

OOM
OOM
18.31
36.49
Timeout
64.48
1279.87
91.17

OOM
OOM
37.21
97.73
Timeout
250.36
2576.87
169.39

OOM
OOM
206.26
1112.2
Timeout
7325.4
Timeout
Timeout

Table 15: Random trees, i-bound=4. Timeout - out of time, OOM - out of memory. 3 GB, 1 hour.
other search scheme for the m-best task. We also analyzed the overhead of both algorithms caused
by the need to find multiple solutions. We introduced BE+m-BF, a hybrid of variable elimination
and best-first search scheme and showed that it has the best worst-case time complexity among all
m-best algorithms over graphical models known to us.
We evaluated our schemes empirically. We observed that the AND/OR decomposition of the
search space, which significantly boosts the performance of traditional heuristic search schemes,
was not cost-effective for m-best search algorithms, at least with our current implementation. As
expected, the best-first schemes dominate the branch and bound algorithms whenever sufficient
space is available, but fail on the memory-intensive problems. We compared our schemes with 4
previously developed algorithms: three approximate schemes based on an LP-relaxation of the problem and an algorithm performing message passing on a junction tree. We showed that our schemes
often dominate the competing schemes, known to be efficient, in terms of runtime, especially when
the required number of solutions is large. Moreover, our scheme guarantee solution optimality.

Acknowledgement
This work was sponsored in part by NSF grants IIS-1065618 and IIS-1254071, and by the United
States Air Force under Contract No. FA8750-14-C-0011 under the DARPA PPAML program.

946

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

instance

grid nnodes380 ps1 k2

(380, 2, 25, 379)

grid nnodes380 ps2 k2

(380, 2, 25, 61)

grid nnodes3192 ps2 k2

(3192, 2, 75, 217)

algorithm

m=2
time
Random binary grid

i-bound=20
m=5
m=10
time
time

m=25
time

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
Nilsson
STRIPES
PESTEELARS

OOM
OOM
0.49
0.56
55.93
112.5
5.06
4.63

OOM
OOM
0.53
0.64
106.34
772.49
46.57
13.4

OOM
OOM
0.58
0.71
202.65
1860.46
172.95
28.95

OOM
OOM
0.67
0.91
2027.66
5026.68
361.04
75.28

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
Nilsson
STRIPES
PESTEELARS

OOM
OOM
0.2
0.32
7.62
110.4
2.23
3.98

OOM
OOM
0.23
0.36
12.82
757.14
19.41
11.5

OOM
OOM
0.26
0.58
67.59
1820.45
38.54
24.34

OOM
OOM
0.36
0.95
1964.18
4985.0
Timeout
Timeout

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
Nilsson
STRIPES
PESTEELARS

OOM
OOM
OOM
Timeout
Timeout
OOM
123.45
26.86

OOM
OOM
OOM
Timeout
Timeout
OOM
658.05
81.27

OOM
OOM
OOM
Timeout
Timeout
OOM
3035.29
172.35

OOM
OOM
OOM
Timeout
Timeout
OOM
Timeout
Timeout

Table 16: Random binary grids, i-bound=20. Timeout - out of time, OOM - out of memory. 3 GB,
1 hour.

947

fiF LEROVA , M ARINESCU , & D ECHTER

instance

gen nnodes132 ps1 k2

(132, 2, 13, 34)

gen nnodes380 ps1 k2

(380, 2, 25, 61)

gen nnodes1122 ps1 k2

(1122, 2, 43, 112)

algorithm

i-bound=20
m=5
m=10
time
time
0.02
0.03
0.03
0.06
0.01
0.02
0.0
0.03
0.09
5.44
60.81
144.93
1.32
3.13
8.52
18.56

m=25
time
0.05
0.09
0.02
0.05
120.67
394.26
13.24
48.76

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
Nilsson
STRIPES
PESTEELARS

m=2
time
0.01
0.01
0.0
0.0
0.03
9.34
0.5
2.9

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
Nilsson
STRIPES
PESTEELARS

OOM
OOM
0.47
0.54
51.77
105.58
2.07
4.38

OOM
OOM
0.51
0.61
110.96
728.0
6.2
14.09

OOM
OOM
0.57
0.73
141.68
1753.98
13.21
29.96

OOM
OOM
0.72
1.03
2027.05
4817.09
76.0
75.04

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
Nilsson
STRIPES
PESTEELARS

OOM
OOM
OOM
Timeout
Timeout
OOM
16.46
9.69

OOM
OOM
OOM
Timeout
Timeout
OOM
57.96
28.84

OOM
OOM
OOM
Timeout
Timeout
OOM
107.73
61.04

OOM
OOM
OOM
Timeout
Timeout
OOM
282.4
158.7

Table 17: Random loopy graphs with submodular potentials, i-bound=20. Timeout - out of time,
OOM - out of memory. 3 GB, 1 hour.

948

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

Figure 18: Example AND/OR search tree with 3 layers of OR and AND nodes.

Appendix A. Proof of Theorem 13
Let ST be the AND/OR search tree relative to pseudo tree T with depth h, n be the number of
variables, k be the maximum domain size, and deg be the maximum degree of the nodes in T .
Define a partial solution subtree T 0 to be a subtree of ST such that: (1) T 0 contains the root s
of ST ; (2) if a non-terminal OR node n is in T 0 , then T 0 contains exactly one AND child node n0 of
n; (3) if a non-terminal AND node n is in T 0 then T 0 contains all OR child nodes n01 , . . . , n0j of n;
(4) a leaf or tip node of T 0 doesnt have any successors in T 0 .
The nodes in ST are grouped into layers. There are h layers such that the ith layer, denoted by
Li , where 1  i  h, contains all the OR nodes whose variables have depth i in T , together with
their AND children. We assume that the root of T has depth 1. For illustration, Figure 18 depicts
an AND/OR search tree with 3 layers, where for example L1 = {A, hA, 0i, hA, 1i}.
We denote by TiOR the set of partial solution subtrees whose leaf nodes are OR nodes in Li .
Similarly, TiAN D is the set of partial solution subtrees whose leaf nodes are AND nodes in Li . A
partial solution subtree T 0  T2OR whose leaf nodes are OR nodes belonging to the 2nd layer is
highlighted in Figure 18, namely T 0 = {A, hA, 0i, B, C}.
L EMMA 1. Given T 0  TiOR and T 00  TiAN D such that T 00 is an extension of T 0 , then T 0 and T 00
have the same number of leaf nodes.
Proof. Let m be the number of OR leaf nodes in T 0 . By definition, each of the m nodes can be
extended by exactly one AND child node in T 00 . It follows that T 00 has also m AND leaf nodes.
L EMMA 2. Given T 0  TiOR , the number of leaf nodes T 0 , denoted by mi , is at most deg i1 .
Proof. We show by induction that mi = deg i1 . If i = 1 then m1 = 1. Assume that for i = p  1,
OR . We first extend T 0 to T 00  T AN D . By Lemma 1, T 00 and T 0
mp1 = deg p2 , and let T 0  Tp1
p1
have the same number of leaf nodes, namely mp1 . Next, we extend T 00 to T 000  TpOR . Since each
of the mp1 AND leaf nodes in T 00 can have at most deg OR child nodes in T 000 , it follows that mp ,
the number of leaf nodes in T 000 is mp = mp1  deg = deg p2  deg = deg p1 .
Proof of Theorem 13 Consider the number of partial solution subtrees N that are contained by ST :
949

fiF LEROVA , M ARINESCU , & D ECHTER

N=

h
X

(NiOR + NiAN D )

(3)

i=1

NiOR

where
= |TiOR | and NiAN D = |TiAN D |, respectively.
0
AN D , it is easy to see that T 0 can be extended to a single partial solution subtree
Given T  Ti1
00
OR
T  Ti such that each of the leaf nodes in T 0 has at most deg OR child nodes in T 00 . Therefore:
AN D
NiOR = Ni1

(4)

Given T 0  TiOR , T 0 can be extended to at most k m partial solution subtrees T 00  TiAN D
because each of the m OR leaf nodes in T 0 can have exactly one AND child node in T 00 and k
bounds the domain size. By Lemmas 1 and 2, we then have that:
NiAN D = NiOR  k deg

i1

(5)

Using Equations 4 and 5, as well as N1OR = 1, we rewrite Equation 3 as follows:
N = (1 + k)
+ (k + k deg+1 )
+ (k deg+1 + k deg

2 +deg+1

)
(6)

+ ...
+ (k deg
 O(k

h2 +deg h3 +...+1

deg h 1
deg1

+ k deg

h1 +deg h2 +...+1

)

)

Thus, the worst-case number of partial solution subtrees that need to be stored in OPEN is
h1
h1
N  O(k deg ). Therefore, the time and space complexity of m-AOBF follows as O(k deg ).
When the pseudo tree T is balanced, namely each internal node has exactly deg child nodes, the
time and space complexity bound is to O(k n ), since n  O(deg h1 ).

References
Aljazzar, H., & Leue, S. (2011). K : A heuristic search algorithm for finding the k shortest paths.
Artificial Intelligence, 175(18), 21292154.
Batra, D. (2012). An efficient message-passing algorithm for the M-best MAP problem. Uncertainty
in Artificial Intelligence.
Charniak, E., & Shimony, S. (1994). Cost-based abduction and MAP explanation. Artificial Intelligence, 66(2), 345374.
Darwiche, A. (2001). Decomposable negation normal form. Journal of the ACM (JACM), 48(4),
608647.
Darwiche, A., Dechter, R., Choi, A., Gogate, V., & Otten, L. (2008).
Results from the probablistic inference evaluation of UAI08, a web-report in
http://graphmod.ics.uci.edu/uai08/Evaluation/Report.
In: Uncertainty in Artificial Intelligence applications workshop.
950

fiS EARCHING FOR M B EST S OLUTIONS IN G RAPHICAL M ODELS

de Campos, L. M., Gamez, J. A., & Moral, S. (1999). Partial abductive inference in bayesian belief
networks using a genetic algorithm. Pattern Recognition Letters, 20(11), 12111217.
de Campos, L. M., Gamez, J. A., & Moral, S. (2004). Partial abductive inference in bayesian networks by using probability trees. In Enterprise Information Systems V, pp. 146154. Springer.
Dechter, R. (1999). Bucket elimination: A unifying framework for reasoning. Artificial Intelligence,
113(1), 4185.
Dechter, R., & Mateescu, R. (2007). AND/OR search spaces for graphical models. Artificial Intelligence, 171(2-3), 73106.
Dechter, R., & Rish, I. (2003). Mini-buckets: A general scheme for bounded inference. Journal of
the ACM, 50(2), 107153.
Dechter, R. (2013). Reasoning with probabilistic and deterministic graphical models: Exact algorithms. Synthesis Lectures on Artificial Intelligence and Machine Learning, 7(3), 1191.
Dechter, R., & Pearl, J. (1985). Generalized best-first search strategies and the optimality of A*.
Journal of the ACM (JACM), 32(3), 505536.
Dijkstra, E. W. (1959). A note on two problems in connexion with graphs. Numerische mathematik,
1(1), 269271.
Elliott, P. (2007). Extracting the K Best Solutions from a Valued And-Or Acyclic Graph. Masters
thesis, Massachusetts Institute of Technology.
Eppstein, D. (1994). Finding the k shortest paths. In Proceedings 35th Symposium on the Foundations of Computer Science, pp. 154165. IEEE Comput. Soc. Press.
Fishelson, M., & Geiger, D. (2002). Exact genetic linkage computations for general pedigrees. In
International Conference on Intelligent Systems for Molecular Biology (ISMB), pp. 189198.
Fishelson, M. a., Dovgolevsky, N., & Geiger, D. (2005). Maximum likelihood haplotyping for
general pedigrees. Human Heredity, 59(1), 4160.
Flerova, N., Dechter, R., & Rollon, E. (2011). Bucket and mini-bucket schemes for m best solutions
over graphical models. In Graph structures for knowledge representation and reasoning
workshop.
Fromer, M., & Globerson, A. (2009). An lp view of the m-best map problem. Advances in Neural
Information Processing Systems, 22, 567575.
Ghosh, P., Sharma, A., Chakrabarti, P., & Dasgupta, P. (2012). Algorithms for generating ordered
solutions for explicit AND/OR structures. Journal of Artificial Intelligence (JAIR), 44(1),
275333.
Gogate, V. G. (2009). Sampling Algorithms for Probabilistic Graphical Models with Determinism
DISSERTATION. Ph.D. thesis, University of California, Irvine.
Hamacher, H., & Queyranne, M. (1985). K best solutions to combinatorial optimization problems.
Annals of Operations Research, 4(1), 123143.
Ihler, A. T., Flerova, N., Dechter, R., & Otten, L. (2012). Join-graph based cost-shifting schemes.
arXiv preprint arXiv:1210.4878.
Kask, K., & Dechter, R. (1999a). Branch and bound with mini-bucket heuristics. In IJCAI, Vol. 99,
pp. 426433.
951

fiF LEROVA , M ARINESCU , & D ECHTER

Kask, K., & Dechter, R. (1999b). Mini-bucket heuristics for improved search. In Proceedings
of the Fifteenth conference on Uncertainty in artificial intelligence, pp. 314323. Morgan
Kaufmann Publishers Inc.
Kjrulff, U. (1990). Triangulation of graphsalgorithms giving small total state space. Tech. Report
R-90-09.
Lawler, E. (1972). A procedure for computing the k best solutions to discrete optimization problems
and its application to the shortest path problem. Management Science, 18(7), 401405.
Marinescu, R., & Dechter, R. (2009a). AND/OR Branch-and-Bound search for combinatorial optimization in graphical models. Artificial Intelligence, 173(16-17), 14571491.
Marinescu, R., & Dechter, R. (2009b). Memory intensive AND/OR search for combinatorial optimization in graphical models. Artificial Intelligence, 173(16-17), 14921524.
Marinescu, R., & Dechter, R. (2005). AND/OR branch-and-bound for graphical models. In International Joint Conference on Artificial Intelligence, Vol. 19, p. 224. Lawrence Erlbaum
Associates Ltd.
Nillson, N. J. (1980). Principles of Artificial Intelligence. Tioga, Palo Alto, CA.
Nilsson, D. (1998). An efficient algorithm for finding the M most probable configurations in probabilistic expert systems. Statistics and Computing, 8(2), 159173.
Nilsson, N. (1982). Principles of artificial intelligence. Springer Verlag.
Otten, L., & Dechter, R. (2011). Anytime AND/OR depth first search for combinatorial optimization. In SOCS.
Pearl, J. (1984). Heuristics: Intelligent Search Strategies. Addison-Wesley.
Schiex, T. (2000). Arc consistency for soft constraints. International Conference on Principles and
Practice of Constraint Programming (CP), 411424.
Seroussi, B., & Golmard, J. (1994). An algorithm directly finding the K most probable configurations in Bayesian networks. International Journal of Approximate Reasoning, 11(3), 205
233.
Wainwright, M. J., & Jordan, M. I. (2003). Variational inference in graphical models: The view from
the marginal polytope. In Proceedings of the Annual Allerton congerence on communication
control and computing, Vol. 41, pp. 961971. Citeseer.
Wemmenhove, B., Mooij, J. M., Wiegerinck, W., Leisink, M., Kappen, H. J., & Neijt, J. P. (2007).
Inference in the promedas medical expert system. In Artificial intelligence in medicine, pp.
456460. Springer.
Yanover, C., & Weiss, Y. (2004). Finding the M Most Probable Configurations Using Loopy Belief
Propagation. In Advances in Neural Information Processing Systems 16. The MIT Press.
Yanover, C., Schueler-Furman, O., & Weiss, Y. (2008). Minimizing and learning energy functions
for side-chain prediction. Journal of Computational Biology, 15(7), 899911.

952

fiJournal of Artificial Intelligence Research 55 (2016) 165-208

Submitted 03/15; published 01/16

Effectiveness of Automatic Translations for
Cross-Lingual Ontology Mapping
Mamoun Abu Helou

mamoun.abuhelou@disco.unimib.it

Department of Informatics,
Systems and Communication
University of Milan-Bicocca

Matteo Palmonari

matteo.palmonari@disco.unimib.it

Department of Informatics,
Systems and Communication
University of Milan-Bicocca

Mustafa Jarrar

mjarrar@birzeit.edu

Department of Computer Science
Birzeit University

Abstract
Accessing or integrating data lexicalized in different languages is a challenge. Multilingual lexical resources play a fundamental role in reducing the language barriers to map
concepts lexicalized in different languages. In this paper we present a large-scale study
on the effectiveness of automatic translations to support two key cross-lingual ontology
mapping tasks: the retrieval of candidate matches and the selection of the correct matches
for inclusion in the final alignment. We conduct our experiments using four different large
gold standards, each one consisting of a pair of mapped wordnets, to cover four different
families of languages. We categorize concepts based on their lexicalization (type of words,
synonym richness, position in a subconcept graph) and analyze their distributions in the
gold standards. Leveraging this categorization, we measure several aspects of translation
effectiveness, such as word-translation correctness, word sense coverage, synset and synonym coverage. Finally, we thoroughly discuss several findings of our study, which we
believe are helpful for the design of more sophisticated cross-lingual mapping algorithms.

1. Introduction
Different ontology representation models have been proposed to ease data exchange and
integration across applications. Axiomatic ontologies are represented in logic-based languages, e.g., OWL (2004), and define concepts by means of logical axioms. Lexical ontologies define the meaning of concepts by taking into account the words that are used to
express them (Hirst, 2004): each concept is defined by one or more synonym words (Miller,
1995), which we refer to as lexicalization of the concept, and connected to other concepts
by semantic relations. Several hybridizations of these two approaches have also been proposed (Vossen et al., 2010).
When data sources using different ontologies have to be integrated, mappings between
the concepts described in these ontologies have to be established. This task is also called
ontology mapping. Automatic ontology mapping methods are introduced to ease this task
by finding potential mappings and determining which ones should be included in a final
alignment. Ontology mapping methods perform two main sub tasks: in candidate match
c
2016
AI Access Foundation. All rights reserved.

fiAbu Helou, Palmonari, & Jarrar

retrieval, a first set of potential matches is found; in mapping selection, a subset of the
potential matches is included in a final alignment.
Some aspects of concept modelling are common to lexical and logical ontologies, despite
their differences: concepts have a lexicalization and are organized in subconcept graphs.
Synonymful lexicalizations, i.e., lexicalizations that contain more than one synonym words,
are more frequently found in lexical ontologies than in axiomatic ontologies. However,
enriching the lexicalization of concepts in axiomatic ontologies with a set of synonyms is a
well-established practice in ontology mapping (Sorrentino, Bergamaschi, Gawinecki, & Po,
2010; Shvaiko & Euzenat, 2013; Faria, Martins, Nanavaty, Taheri, Pesquita, Santos, Cruz,
& Couto, 2014).
Cross-lingual ontology mapping is the task of establishing mappings between concepts
of a source ontology lexicalized in a language and concepts of a target ontology lexicalized in a different language (Spohr, Hollink, & Cimiano, 2011). If we consider that more
than a million datasets have been published online as linked open data in 24 different languages (LOGD, 2015), cross-lingual ontology mapping is currently considered an important
challenge (Gracia, Montiel-Ponsoda, Cimiano, Gomez-Perez, Buitelaar, & McCrae, 2012).
For instance, in the COMSODE project (2015), several tables lexicalized in different languages have been published in RDF (2014) after being annotated using domain ontologies.
Data publishers would like to annotate their data using ontologies lexicalized in their native language as well as in English. Annotations in the native language of the publishers
can facilitate the access to local citizens, while annotations in English support the integration of data published in different countries with the large amount of data published in
English. A cross-lingual ontology mapping system may help them by facilitating bilingual
data annotation.
Cross-lingual ontology mapping methods are also helpful in the construction of multilingual or large lexical ontologies (De Melo & Weikum, 2009; Abu Helou, Palmonari, Jarrar, &
Fellbaum, 2014). For example, in the Arabic Ontology project (Birzeit, 2011; Jarrar, 2011,
2006; Jarrar et al., 2014), a kernel of core concepts could be extended by mapping new
concepts defined by synsets and glosses to the English WordNet (Miller, 1995; Fellbaum,
1998) so as to derive novel semantic relations.
Most of the cross-lingual ontology mapping methods include a step in which the concepts
lexicalizations of one ontology are automatically translated into the language of the other
ontology (Pianta, Bentivogli, & Girardi, 2002; Vossen, 2004). The most frequently adopted
approach to obtain automatic translations is to use multilingual lexical resources, such as
machine translation tools or bilingual dictionaries. The quality of the translations used
by a mapping method has a major impact on its performance. However, we found that
a systematic and large-scale analysis of the effectiveness of automatic translations in the
context of cross-lingual mapping is missing. The study presented in this paper aims at
providing a significant contribution to fill in this gap.
In our study, we use two multilingual lexical resources as sources of translations: Google
Translate (2015) and BabelNet 1 (Navigli & Ponzetto, 2012). Google Translate is a machine
translation tool that has been frequently used in cross-lingual ontology mapping (Shvaiko,
Euzenat, Mao, Jimenez-Ruiz, Li, & Ngonga, 2014). Previous work has suggested that
1. We used BabelNet version 2.5. Recent versions were released while writing this paper (BabelNet, 2012).

166

fiEffectiveness of Automatic Translations for Cross-Lingual Ontology Mapping

Google Translate performs better than other Web translation services in the context of concept mapping (Al-Kabi, Hailat, Al-Shawakfa, & Alsmadi, 2013; Oliver & Climent, 2012).
In addition, the service can be configured so as to obtain reverse translations, which further
increase the number of words that can be automatically translated. BabelNet is the largest
multilingual knowledge resource available as of today. Its concepts are derived from the
fusion of the English WordNet - the largest lexical ontology - and a large source of encyclopedic knowledge such as Wikipedia (2015b). Multilingual lexicalizations have been created
using inter-lingual links in Wikipedia, different translation strategies and several bilingual
dictionaries collaboratively created on the Web, as we will explain in detail in Sections 3.3
and 5. On the one hand, we expect that translations obtained from BabelNet cover a large
number of words. On the other hand, by evaluating translations obtained from BabelNet
we are indirectly evaluating different sources of translations, some of which have been used
individually in several cross-lingual mapping approaches. Another reason for choosing these
resources in our study is the very large number of languages covered by Google Translate
and BabelNet (respectively, 90 and 272), if compared to other resources of the same kind.
Our study is organized as follows. By focusing on concepts lexicalizations, we consider
concepts as synsets, i.e., sets of words with an equivalent meaning in a given context (Miller,
1995). This definition is used to classify concepts (synsets) into different categories, based
on different characteristics: word ambiguity (e.g., monosemous vs. polysemous), number
of synonyms (e.g., synonymful vs. synonymless), and position in a concept hierarchy (e.g.,
leaves vs. intermediate concepts). Using these classifications, we evaluate the effectiveness of
translations obtained with multilingual lexical resources by studying the performance on the
cross-lingual mapping tasks executed using automatic translations for different categories of
synsets. We first analyze the coverage of translations and its impact on the candidate match
retrieval task. Then we analyze the difficulty of the mapping selection task using a baseline
mapping selection method. These analyses are based on different measures introduced to
evaluate the translation effectiveness in terms of coverage and correctness, which are based
on a comparison with translations considered perfect according to a gold standard, i.e., a
set of cross-lingual mappings that are deemed to be correct.
As gold standards, we use cross-lingual mappings manually established (or validated)
by lexicographers between four wordnets (Arabic, Italian, Slovene and Spanish) and the
English WordNet. Using gold standards based on these wordnets has two main advantages.
They contain a large number of mapped concepts, much larger, e.g., than the gold standards used to evaluate cross-lingual ontology mapping systems in the Ontology Alignment
Evaluation Initiative (OAEI) (Shvaiko et al., 2014; OAEI, 2015), and we can leverage our
lexical characterization of concepts into different categories to provide a more in-depth analysis. The wordnets used in our experiments are also representative of different families of
languages and of different ontology sizes.
To the best of our knowledge, this is the first attempt to carry out a systematic and largescale study on the effectiveness of multilingual lexical resources as sources of translations
in the context of cross-lingual ontology mapping. In previous work, these resources have
been mostly evaluated in the context of specific algorithms (Fu, Brennan, & OSullivan,
2012; Spohr et al., 2011), with a limited number of gold standards, and for a limited
number of languages. Our experiments lead to interesting findings, which are discussed in
a (numbered) list of observations and summarized in a lessons learned section. Overall,
167

fiAbu Helou, Palmonari, & Jarrar

we believe that the findings of our study can be useful for the definition of more accurate
and flexible mapping algorithms, based on the characterization of concepts lexicalization.
The paper is structured as follows. In Section 2, we introduce some preliminary definitions used in the rest of the paper. In Section 3, we overview related work, with the goal of
discussing the role of automatic translations in cross-lingual ontology mapping and related
fields. The evaluation measures and the multilingual lexical resources used in our study to
obtain translations are presented respectively in sections 4 and 5. In section 6, we present
the experiments. Conclusions and future work end the paper.

2. Preliminaries
In this section we introduce the definitions used in our study, which cover concept lexicalizations, cross-lingual mapping and translation tasks.
2.1 Lexicalization of Concepts
We consider a general definition of ontologies, focusing on the lexical characterization of
concepts, and on the relations between natural language words used in concepts. This
choice is motivated by the observation that even ontology matching systems that look into
the semantics of axiomatic ontologies, e.g., LogMap (Jimenez-Ruiz & Grau, 2011), use
concept lexicalizations to retrieve candidate matches for concepts in a source ontology. For
this reason, we borrow several definitions from lexical ontologies like WordNet (Miller, 1995)
and use their terminology throughout the paper.
Slightly abusing the terminology (but coherently with WordNet), words are lexemes
associated with a concept. A word is called simple when it contains one token, e.g, table,
and is called collection 2 when it contains more tokens, e.g., tabular array.
Wordnet organizes natural language words into synonym sets, so-called synsets. Each
synset represents one underlying concept, i.e., a set of words (synonyms) that share the
same meaning in a given context. If W is the set of words represented in a wordnet, a
synset s  P(W ) is a set of words s = {w1 , ..., wn }.
A synset can contain one word (synonymless) or many words (synonymful ). We use
concept and synset interchangeably in the rest of the paper. Depending on the specific
case, we use two notations for concepts: the set notation {w1 , ..., wn } is used when we need
to make explicit reference to the words contained in the synset, while a symbol notation s
is used when this reference is not needed. We also use the set notation w  s to state that
word w is contained in synset s. The set of words contained in the concept is also called
its lexicalization. We use a superscript to specify the natural language used in concept
lexicalizations when needed, i.e., wL , sL , or W N L represent a word, a synset and a wordnet
respectively lexicalized in the language L.
In addition to lexical relations, which link individual words (e.g., synonymy, antonymy),
most of wordnets support semantic relations, which link concepts. Hypernymy and hyponymy are the most important semantic relations in wordnets. They are defined one as
2. An alternative name used instead of collection is multiword expression (MWE), which is frequently used
in particular in the literature about machine translation tool evaluation (Sag, Baldwin, Bond, Copestake,
& Flickinger, 2002); we use collection to be coherent with WordNet terminology that is used throughout
the paper.

168

fiEffectiveness of Automatic Translations for Cross-Lingual Ontology Mapping

the inverse of the other one and determine the subconcept graph in wordnets (see Section 3.1). For example, the synset {table, tabular array} is hyponym of the synset {array},
while the synset {array} is hypernym of the synset {table, tabular array}.
A word is polysemous, i.e., has many meanings (or senses) when it is a member of many
synsets. In the paper the superscript +  on the right-hand corner of a word, e.g., board+ ,
indicates a polysemous word. A word is monosemous, i.e. has only one meaning when it is
a member of only one synset. For example, the English WordNet3 has eight senses for the
word table+ ; one of these senses means a set of data arranged in rows and columns,
which has the word tabular array as a synonym word. Another sense means food or
meals in general, which has the word board+  as a synonym word.
Given the set of words W and the set of synsets S defined in a wordnet W N , the
function senses : W 7 P(S) returns the set of synsets that a word belong to, defined
by senses(w) = {s|w  s}. We can now define the set of word senses in a wordnet,
W S = {< w, s > |s  senses(w)}, i.e., the set of couples < w, s >, such that s is a sense of
w (in a given context). Observe that the number of word senses is higher than the number
of synsets because we consider all the associations between words and synsets.
Example 1. The word table+  has eight senses in the English WordNet, senseEn (table)
= {{ table+ , tabular array },{table+ },{table+ },{mesa+ , table+ },{table+ },{board+ , table+ },
{postpone, prorogue+ , hold over+ , put over+ , table+ , shelve+ , set back+ , defer+ , remit+ ,
put off+ }, {table+ , tabularize, tabularise, tabulate+ }}4 .
2.2 Cross-Lingual Mapping
In the ontology matching field, cross-lingual mapping is defined as the task of finding and
establishing mappings between concepts of a source ontology lexicalized in a language L1
and concepts of a target ontology lexicalized in a language L2 (Spohr et al., 2011). Mappings
can represent different relations between source and target concepts. If we consider a specific
mapping relation R, for a source concept s and a target concept t, the output of a mapping
task is a set of couples < s, t >, also called an alignment. A cross-lingual mapping task
with a mapping relation R is composed of two main steps (or, sub tasks):
 candidate match retrieval: find, for each source concept lexicalized in L1 , a set of
target concepts lexicalized in L2 . We call the concepts found in this task candidate
matches.
 mapping selection: given a set of candidate matches T = {t1 , ..., tn } (lexicalized in
L2 ) for a source concept s (lexicalized in L1 ), select a set of concepts T 0  T such
that, for each t  T 0 , R(s, t) holds. When R(s, t) holds, we say that t is a correct
match for s, and that < s, t > is a correct mapping.
For a more in depth analysis of the semantics of cross-lingual mappings, we refer to
previous work (Abu Helou et al., 2014). A gold standard alignment (or, gold standard
for short), denoted by gs, is an alignment between synsets (concepts) in two wordnets such
that the mappings in the alignment are believed to be correct. In this paper, we consider
only equivalence mappings, i.e., mappings that specify that a source and a target concepts
3. In the following we use WordNet version 3.0.
4. The senses definitions can be found online at http://wordnetweb.princeton.edu/perl/webwn?s=table

169

fiAbu Helou, Palmonari, & Jarrar

have equivalent meaning. It is often assumed that the cardinality of equivalence mappings
is 1:1, meaning that for each source concept there is at most one correct match.
In a gold standard alignment with cardinality 1:1, a synset in the source language have
at most one equivalent synset in a target language. We use the predicate symbol 
to indicate that two synsets are equivalent (express the same meaning) in a gold standard.
Using synset mappings in a gold standard gs, we can define the possible senses of a word wL1
L1
2
in a target language L2 , denoted by sensesL
gs (w ), as the senses in L2 that are equivalent
to the senses of wL1 in its native language L1 :
L2
sensesgs
(wL1 ) = {sL2 | sL1 (wL1  sL1  sL1  sL2 )}

(1)

Observe that the candidate match retrieval step define an upper bound for the mapping
selection step: a correct mapping can be selected only if the target of the mapping was
retrieved as a candidate match. In addition, mapping selection is a form of disambiguation
task : the correct meaning of a concept (the lexicalization of the concept), in the target
language has to be chosen among different possible meanings. A larger number of candidate
matches and little evidence for preferring one candidate over another are likely to make the
selection problem more difficult.
2.3 Translation Tasks
Translating words of one language into words of another language is crucial in the context
of cross-lingual concept mapping, and, in particular, in the candidate match retrieval step.
For seek of clarity we consider two translation tasks: translations of single words and
translations of synsets. Translations are based on external multilingual lexical resources,
e.g., a machine translation tool or a dictionary built using multilingual lexical resources.
We define the word-translation of a word wL1 into a target language L2 with a resource
L1 7 P(W L2 ) that maps a word w L1 into sets of words in
2
D, as a function wT ransL
D : W
a target language L2 .
We define the synset-translation of a synset sL1 into a target language L2 with a
L1 7 P(P(W L2 )) that maps a synset s into sets
2
resource D, as a function sT ransL
D : S
of sets of words, each of which is the output of word-translation of some w  s. The
synset-translation function is defined as follows:
L2
L1
L1
L1
2
sT ransL
D (s ) = {wT ransD (w ) | w  s }

(2)

Example 2. The synset-translation of the Italian synset {tavola+ , tabella}It into
+
It
En
+,It ),
English can be given as follow: sTransEn
D ({tavola , tabella} ) = {wTransD (tavola
En
It
wTransD (tabella )} = {{table, board, plank, panel, diner, slab}, {table, list}}.
Observe that in the definition of the synset-translation function we do not make the set
union of the outputs of every word-translation applied to the words in a synset. Instead,
using Eq.2, we can write the output of the synset-translation function as multiset union
of the sets returned by every word-translation. For instance, in Example 2, sTransEn
D
({tavola+ , tabella}It ) = {table(2) , board(1) , plank(1) , panel(1) , diner(1) , slab(1) }, superscript
numbers between brackets indicate the frequency count of the words in the translation set.
Similarly, table(2)  means that the word table appears in two subsets, i.e., the word
table is resulted as a translation of two synonym words in the source synset, which are
170

fiEffectiveness of Automatic Translations for Cross-Lingual Ontology Mapping

tavola and tabella. In this way we can count the number of word-translations that
produced one word in the target language for a given synset-translation. These counts
can be helpful to use the results of the synset-translation to perform the mapping selection
step. For example, these counts can be used to weigh the candidate matches with a majority
voting approach, like the one used in our experiments in Section 6.3.2.

3. Automatic Translations in Cross-Lingual Mapping Tasks
In this section we review the use of automatically generated translations in cross-lingual
ontology mapping and in other related tasks such as enrichment of multilingual knowledge
resources and cross-lingual word-sense disambiguation.
The enrichment of multilingual knowledge resources is related to cross-lingual ontology
mapping and to the findings of our study for several reasons. First, multilingual knowledge resources can be used as sources of translations in cross-lingual ontology mapping
approaches. Second, the wordnets mapped to the English WordNet that we use as gold
standards are multilingual knowledge resources, because their mappings represent interlingual links between concepts. Third, the two most frequently adopted approaches to
enrich multilingual knowledge resources are based either on mapping concepts lexicalized
in different languages or on translating the concepts lexicalizations. Since we evaluate the
correctness and coverage of translations of ontology concepts, our findings are relevant also
to approaches that intend to use these translations and ontology mapping methods to enrich
multilingual knowledge resources.
Cross-lingual word sense disambiguation is another research field where translations
have been used to solve a mapping problem, which is related to, but also quite different
from, the mapping tasks considered in this study.
Before discussing related work in these different research areas we discuss the usage of the
term concept in lexical and axiomatic ontologies. We conclude the section by presenting
the contributions of this study to the evaluation of automatically generated translations in
cross-lingual ontology mapping and related tasks.
3.1 Concepts in Lexical and Axiomatic Ontologies
Concepts are the constituents of thoughts (Margolis & Laurence, 2014). The relation between natural language and thought is much debated. For example, some maintain that
concepts are independent from the language (Fodor, 1975; Pinker, 1994) while others believe
that concepts require natural language to exist (Carruthers, 2002; Spelke, 2003). However,
natural language plays a major role in expressing concepts in many computational knowledge representation systems proposed to support natural language processing, information
retrieval and data integration tasks. Ontologies are among these computational knowledge
representation systems. We distinguish between two different kinds of ontologies.
In lexical ontologies, the meaning of concepts is primarily defined in relation to the
words that can be used to express them. For example, in order to represent the concept
table, with reference to the object used to eat a meal, the set of words used to refer to
this concept are specified. Lexical ontologies include domain thesauri, and wordnets, the
most popular of which is the English WordNet (Miller, 1995; Fellbaum, 1998). In axiomatic
ontologies (or, logical ontologies) the meaning of concepts is defined by axioms specified
171

fiAbu Helou, Palmonari, & Jarrar

in a logical language, e.g., First Order Logic, which are interpreted as constraints over
mathematical structures and support automated reasoning (Horrocks, 2008). Examples of
logical ontologies include web ontologies defined in RDFS (2014) or OWL, but an annotated
database schema or a spreadsheet can also be considered an ontology based on this broad
definition (Zhuge, Xing, & Shi, 2008; Po & Sorrentino, 2011; Mulwad, Finin, & Joshi, 2013;
Zhang, 2014). For example, to represent the afore-mentioned concept table, we can define
it as a piece of furniture having a smooth flat top that is usually supported by one or more
vertical legs in a logical language. The intended interpretation of this concept can be every
such table that had ever existed in the world, or, more specifically, a list of products of type
table described in a spreadsheet (Mulwad et al., 2013; Zhang, 2014).
Many hybrid approaches also exist. For example, efforts to assure certain logical properties of relations represented in lexical ontologies can be found in KYOTO (Vossen et al.,
2010). YAGO is a logical ontology that integrates many concepts from the English WordNet (Suchanek, Kasneci, & Weikum, 2008). WordNet concepts used to annotate a database
schema can be given a formal interpretation and used to support database integration (Sorrentino et al., 2010).
As a matter of fact, despite several differences, concepts modelled in lexical, axiomatic, or
hybrid ontologies share two important features. First, concepts are organized in subconcept
graphs, i.e., hierarchies, partially ordered sets, or lattices that define the relations between
concepts based on their generality. These relations are referred to as subconcept relations
in axiomatic ontologies, while different relations can be represented in lexical ontologies,
e.g., hyponymy/hypernymy. Second, in every ontology concepts have lexical descriptions
that may include a set of synonym words. Of course, while synonyms are first class citizens
in lexical ontologies and are available for a large number of concepts, their availability is
more limited in axiomatic ontologies. However, a step to enrich the concept lexicalizations
of logical ontologies with synonyms extracted from dictionaries and other lexical resources
is introduced in many ontology mapping approaches so as to exploit lexical matching algorithms (Shvaiko & Euzenat, 2013; Otero-Cerdeira, Rodrguez-Martnez, & Gmez-Rodrguez,
2015; Sorrentino et al., 2010; Faria et al., 2014).
3.2 Translations in Cross-Lingual Ontology Mapping
The majority of ontology mapping methods proposed in the literature have addressed the
problem of mapping ontological resources lexicalized in the same natural language, called
mono-lingual ontology mapping. Since mono-lingual matching systems cannot directly access semantic information when ontologies are lexicalized in different natural languages (Fu
et al., 2012), techniques to reconcile ontologies lexicalized in different natural languages
have been proposed (Gracia et al., 2012; Trojahn, Fu, Zamazal, & Ritze, 2014).
Translation-driven approaches have been used to overcome the natural language barriers
by transforming a cross-lingual mapping problem into a mono-lingual one (Fu et al., 2012).
Different multilingual lexical resources have been used to perform the translation tasks,
including manual translations, machine translation tools, and bilingual dictionaries built
from Web-based multilingual resources. For a rich classification and comparison of crosslingual mapping systems we refer to the work of Trojahn et al. (2014).
172

fiEffectiveness of Automatic Translations for Cross-Lingual Ontology Mapping

Liang and Sini (2006) manually mapped the English thesaurus AGROVOC (2014) to
a Chinese thesaurus CAT (2014). The mappings generated by such approaches are likely
to be accurate and reliable. However, this can be a time and resource consuming process
specially for maintaining large and complex ontologies.
Machine translation tools are widely adopted for cross-lingual ontology mapping. Spohr
et al. (2011) translate the ontology labels into a pivot language (English) using the machine
translation tool (Bing, 2016). Then, they define a feature vector based on a combination of
string-based and structural-based similarity metrics and learn a matching function using a
support vector machine. Like other approaches based on supervised machine learning algorithms, their approach has the disadvantage of requiring a significant number of training
samples and well-designed features to achieve good performance. Fu et al. (2012) translate
ontology labels using Google Translate, and then match these translated labels by combining different similarity measures. Their approach leverages structural information about
the ontology concepts by considering their neighbours in the matching process. Other approaches have been proposed that also apply string-based, lexical and structural matching
methods to ontology labels translated with machine translation tools, like Google Translate
or Bing (Faria et al., 2014; Jimenez-Ruiz, Grau, Xia, Solimando, Chen, Cross, Gong, Zhang,
& Chennai-Thiagarajan, 2014; Djeddi & Khadir, 2014).
Multilingual knowledge resources available on the web have been also exploited to translate concepts labels (Hovy, Navigli, & Ponzetto, 2012). Wiktionary (2015) was used to
generate translations to match English and French ontologies (Lin & Krizhanovsky, 2011).
First, a bilingual English-French lexicon is built using Wiktionary and is used to translate
the labels of the ontologies. Then, the monolingual ontology matching system COMS is
used (Lin, Butters, Sandkuhl, & Ciravegna, 2010). COMS uses a set of string-based, lexical
and structural matching techniques to find the appropriate mappings. A similar approach
uses Wikipedia inter-lingual links to retrieve candidate matches for source concepts (Bouma,
2010; Hertling & Paulheim, 2012). However, when used alone, Wiktionary and Wikipedia
inter-lingual links may have limited coverage, in particular for resource-poor languages.
In spite of these efforts, cross-lingual mapping systems still perform significantly worse
than mono-lingual mapping systems according to recent results in the OAEI contest (Shvaiko
et al., 2014), which suggest that cross-lingual ontology mapping is still a very challenging
problem (Trojahn et al., 2014). The datasets used to evaluate cross-lingual mapping in the
OAEI, i.e., the datasets in the multifarm track (Meilicke et al., 2012), consist of alignments
established between axiomatic ontologies of relatively small size and specific to the domain
of conference organization. Since in our study we want to investigate translations obtained
with different multilingual lexical resources at a large scale and not in a specific domain,
we decided to use different and larger gold standards in our experiments.
3.3 Translations in the Enrichment of Multilingual Knowledge Resources
Several multilingual wordnets (lexical ontologies) were developed by manually or automatically translating concepts of the English WordNet into new languages (Pianta et al., 2002;
Vossen, 2004; Tufis, Cristea, & Stamou, 2004; Gonzalez-Agirre, Laparra, & Rigau, 2012;
Tomaz & Fiser, 2006). The expand and merge models (Vossen, 2004) are the main approaches used in the development of multilingual wordnets. In the merge model, synsets of
173

fiAbu Helou, Palmonari, & Jarrar

a pre-existing resource in one language (e.g., a thesaurus, or even an unstructured lexical resource like a dictionary) are aligned to the most equivalent synset in English. In the expand
model, English synsets are translated into the respective languages. The main advantage of
these two approaches is to avoid the expensive manual elaboration of the semantic hierarchy
in new languages. The English WordNet (Fellbaum, 1998) hierarchy is used as reference
for all wordnets. Moreover, any ontology that is built following these approaches is also
automatically mapped to the English WordNet.
In several wordnets, the English concepts were manually translated by human lexicographers using external lexical resources such as dictionaries, thesauri and taxonomies.
This approach has been applied to build for example the Arabic wordnet (Rodrguez et al.,
2008), the Italian wordnet (Pianta et al., 2002), the Spanish wordnet (Gonzalez-Agirre
et al., 2012) and the core of the Slovene wordnet, all used in our experiments. However, the
manual approach to construct ontologies that aim to cover natural languages lexicons is
often an effort-intensive and time-consuming task (De Melo & Weikum, 2009). Automatic
approaches have been therefore proposed to reduce the lexicographers workload.
Parallel corpora have been used in building wordnets for languages other than English.
The basic assumption underlying these methods is that the translations of words in real texts
offer insights into their semantics (Resnik & Yarowsky, 1999). The Slovene wordnet was
enriched using word alignments generated by a sentence-aligned multilingual corpus (Fiser,
2007). The wordnet has been further extended using bilingual dictionaries and inter-lingual
links in Wikipedia. A similar approach is also followed in building the French wordnet (Sagot
& Fiser, 2008). The monosemous words in the English WordNet were automatically translated using bilingual French-English dictionaries built from various multilingual resources,
such as Wikipedia inter-lingual links, Wiktionary, Wikispecies (2015), and the EUROVOC
thesaurus (2015).
Sentence-aligned parallel corpora may not be available for all pair of all natural languages. In addition, specific tools are needed to perform sentence and/or word alignment across the corpora, and the bilingual dictionaries extracted from these corpora are
biased towards the domains they cover. To overcome these limitations, in the Macedonian
wordnet (Saveski & Trajkovski, 2010), a machine translation tool has been used to create parallel corpora. Monosemous English words were directly translated using a bilingual
English-Macedonian dictionary. For polysemous words, the English WordNet sense-tagged
glosses (WordNet-Princeton, 2015) was automatically translated into Macedonian using
Google Translate.
A supervised method to automatically enrich English synsets with lexicalizations in
other languages was also proposed (De Melo & Weikum, 2012). This method learns to determine the best translation for English synsets by taking into account bilingual dictionaries,
structural information in the English WordNet, and corpus frequency information.
Other approaches to enrich multilingual knowledge resources have been proposed to
build the Universal WordNet (UWN, De Melo & Weikum, 2009), WikiNet (Nastase, Strube,
Boerschinger, Zirn, & Elghafari, 2010), and BabelNet (Navigli & Ponzetto, 2012), which
integrate multilingual encyclopedic knowledge from Wikipedia with the English WordNet.
In this paper, we focus on BabelNet, the largest multilingual knowledge resource as of today,
and use it in our study to build bilingual dictionaries that we use for translation (explained
174

fiEffectiveness of Automatic Translations for Cross-Lingual Ontology Mapping

in Section 5). A comprehensive comparison amongst the afore-mentioned three resources
can be found in the work of Navigli and Ponzetto (2012).
BabelNet (Navigli & Ponzetto, 2012) has been built by integrating the English WordNet
with Wikipedia. These two resources have been mapped using an unsupervised approach.
As a result, BabelNet covers approximately the 83% of WordNets nominal synsets. Synsets
from the English WordNet cover in particular (but not only) classes of objects, e.g., University5 , while Wikipedia entries cover in particular (but not only) named entities, e.g.,
University of Milano-Bicocca6,7 . Synsets from the English WordNet and other BabelNet
entries are enriched with lexicalizations in other languages using a variety of lexical resources. A first set of lexicalizations in languages other than English are obtained by using
inter-lingual links of Wikipedia. Synsets for which Wikipedia entries cannot be found have
been enriched using automatic translations of English senses-tagged sentences, extracted
from Wikipedia and the SemCor corpus (Miller, Leacock, Tengi, & Bunker, 1993). The
most frequent translation in a given language is detected and included as a variant lexicalization in this language; this approach was named context-translation. Translations of
monosemous English words have been collected using Google Translate and directly included in the expanded lexicalizations; this approach was named contextless-translation.
Observe that contextless translations are based on an heuristics, i.e., that monosemous
words are correctly translated (also referred to as monosemous word heuristics). The core
of BabelNet consists of the lexicalizations obtained with these approaches, also named BabelNet synsets. Later, BabelNet synsets lexicalizations are expanded with more multilingual
lexical resources: Wiktionary, WikiData (2015), OmegaWiki (2015), and several wordnets
that are mapped to the English wordnet, which are available through the Open Multilingual
Wordnet (OMWN, 2015; Bond & Foster, 2013).
BabelNet lexicalizations (synsets) have been evaluated against manually mapped wordnets, which we also use in our experiments as gold standards. They also performed a manual
evaluation with a randomly sampled set of concepts. A limit of their evaluation consists
in not making explicit if the sampled senses uniformly cover polysemous and monosemous
senses. Otherwise this distinction is important to evaluate different translations, also because a vast number of translations have been obtained using the contextless approach,
which is based on the monosemous word heuristics. In our experiments (Section 6) we
specifically analyze the effectiveness of the monosemous word heuristics in the context of
ontology mapping.
We observe that the expand model was used more substantially than the merge model in
approaches to automate the enrichment of multilingual wordnets and knowledge resources.
One may attempt to enrich an existing wordnet via the merge approach by mapping an
unstructured or a weakly structured lexicon, e.g., a dictionary, to a structured reference
ontology, e.g., the English WordNet. For example, in the Arabic Ontology Project (Jarrar, 2011; Abu Helou et al., 2014), the authors plan to use this approach to extend a core
ontology manually created and mapped to the English WordNet. However, the mapping
task incorporated in this approach is particularly challenging (Abu Helou, 2014): the lack
of semantic relations between the synsets of an unstructured lexicon makes it difficult to
5. http://wordnetweb.princeton.edu/perl/webwn?s=university
6. https://en.wikipedia.org/wiki/University of Milano-Bicocca
7. http://babelnet.org/synset?word=University of Milano-Bicocca

175

fiAbu Helou, Palmonari, & Jarrar

disambiguate their meaning during the translation and the matching steps (Shvaiko & Euzenat, 2013; Trojahn et al., 2014). An effective cross-lingual ontology mapping method
can support the application of the merge model at large scale, thus supporting the construction and enrichment of multilingual knowledge resources. For example, a recent work
suggests that this approach, despite the difficulty of the task, can return multilingual concept lexicalizations richer than the ones that can be obtained by automatically translating
the concepts labels (Abu Helou & Palmonari, 2015).
3.4 Translations in Cross-Lingual Word Sense Disambiguation
Cross-lingual ontology mapping is also related to the Cross-lingual Word Sense Disambiguation problem (CL-WSD), which has been studied in the recent past and addressed in
SemEval 2010 and 2013 challenges (Els & Veronique, 2010; Lefever & Hoste, 2013). The
goal of CL-WSD is to predict semantically correct translations for ambiguous words in
context (Resnik & Yarowsky, 1999).
In CL-WSD, the lexical disambiguation task is performed as a word translation task,
called lexical substitution task (McCarthy & Navigli, 2009). Given a source word in a
sentence (e.g., an Italian word), the system tries to translate the word into a different
language (e.g., English). The translation is considered to be correct if it preserves the sense
that the word has in its context also in the target language.
Most of CL-WSD systems rely on parallel corpora (Gale, Church, & Yarowsky, 1992;
Resnik & Yarowsky, 1999; Apidianaki, 2009), including those which exploit existing multilingual wordnets (Ide, Erjavec, & Tufis, 2002). However, the success and coverage of
these methods highly depends on the nature of the parallel corpora and on the way the
extracted information is used to select the appropriate senses. Corpora are known to have
domain-orientated coverage, i.e., fine-grained senses for different domains might not be
found in specific parallel corpora (Navigli, 2009). More importantly, parallel corpora may
not be available for language couples or for specific domains (Apidianaki, 2009; Saveski &
Trajkovski, 2010).
One fundamental difference between the CL-WSD task and cross-lingual ontology mapping is that in CL-WSD a context is always available and defined by the sentence a word
occurs in. In cross-lingual ontology mapping the context can be defined by the neighbours of a translated concept, may be limited (Mulwad et al., 2013; Zhang, 2014), or may
not be even available, e.g., when an unstructured lexicon is matched against a structured
ontology (Abu Helou et al., 2014).
3.5 Scope and Contribution of this Study
Even if most of the approaches to cross-lingual ontology mapping are based on transforming
a cross-lingual mapping problem into a monolingual one by leveraging translations obtained
from machine translations tool or multilingual lexical resources (Trojahn et al., 2014), few
efforts have been dedicated to systematically study the effectiveness of these translations in
cross-lingual ontology mapping.
Fu, Brennan, and OSullivan (2009) studied some limitations of translation-based ontology mapping approaches, in particular, to what extent inadequate translations can introduce noise into the subsequent mapping step or fail to cover an adequate number of
176

fiEffectiveness of Automatic Translations for Cross-Lingual Ontology Mapping

concepts. They performed two experiments, which examined mappings of independent,
domain specific, and small-scale ontologies that are labeled in English and Chinese: the
Semantic Web for Research Communities ontology and the ISWC ontology. The ontologies
have not been lexically enriched. Fu and her colleagues classified the translation errors
introduced by machine translation tools into three main categories. Inadequate translation,
when the translation of a source concept returns a word, which belongs to a concept in
the target ontology that is more specific/generic than its equivalent concept; synonymic
translation, when the translation of a source concept returns a word, which is synonym of
the word used in the target ontology to denote its equivalent concept (but different from
the one that is used by the target ontology); and incorrect translation, when the translation
is simply wrong. In addition, the study showed that translating ontology labels in isolation
leads to poorly translated ontologies which then yields low-quality matching results, thus,
label translations should be conducted within context. The context is characterized by the
surrounding ontology concepts.
Spohr et al. (2011) observed that, when the target ontology is lexicalized in more than
one language, it is convenient to translate the source concepts in each of these languages
and merge the evidence provided by these translations. However, this can be applicable
only when multilingual labels are available in the target ontology, which is not the case in
several cross-lingual mapping scenarios. In addition, to obtain better translations the study
suggested that translating the source and target ontologies labels into a pivot language can
improve, to some extent, the quality of the translation. However, the authors stated that
further evidence and experiments with several language pairs are needed to support this
claim, as the quality of machine translations depends significantly on the pair of considered
languages.
In this paper we analyze the effectiveness of automatic translations for cross-lingual concept mapping using large scale, general domain, and lexically rich ontologies (wordnets).
The ontologies used in our studies cover four different families of languages besides English.
We study the effectiveness of translations by conducting a large number of experiments
that address the candidate match retrieval and the mapping selection steps of an ontology
mapping process. Overall, we believe that none of previous work on cross-lingual ontology
mapping provided such a systematic study, if compared in terms of scale (size of the considered concepts), number of considered languages, and level of detail of the analysis (concept
categorization).
The analyses discussed in this paper can be also related to the studies on automatic
translation strategies conducted to evaluate BabelNet, which is one of the two multilingual
lexical resources used as a source of translation in our study. In our work, we quantitatively
evaluate the correctness and the coverage of the translation strategies used in BabelNet as
means to support cross-lingual mapping tasks (using mappings between wordnets for comparison; see Section 6.3.1). The studies conducted to evaluate BabelNet were aimed, instead,
at evaluating their translation strategies as means to enrich multilingual lexicalizations for
the concepts. We introduce two new measures to evaluate the correctness and coverage of
translations obtained from multilingual resources, i.e., translation correctness and synonym
coverage (see Section 4.3.2). In addition, coverage and correctness of automatic translations in our study are evaluated by considering different categories of synsets (defined in
177

fiAbu Helou, Palmonari, & Jarrar

{tavolo+, tavola+} {asse+, tavola+}

{tavola+}

axle tree
board
axis
plank

table
desk

{plate+}

{table+}

board
plank

{tavola+, tabella}

table
panel

list
table

Italian Synsets

wTransDEn(wiIt)

diner slab

{board+, plank+} {table+, tabular array}

English Synsets

: translation.

{tavola+, tabella}

: mapped synsets.
: synset.

Figure 1: Example: Synset-translation

Section 6.2). Finally, we analyze the effectiveness of the monosemous word heuristic, which
is used in several mapping systems and in BabelNet (contextless translation).

4. Evaluation Measures
In our study, we want to estimate the effectiveness of translations obtained from multilingual
lexical resources (hereafter referred to as resources) in finding candidate matches for a large
set of concepts. We also want to estimate the difficulty of selecting one correct mapping
among a set of candidate matches, based on the information provided by translations.
For the first objective, we define four measures that we use to evaluate translation
correctness and coverage. The first two measures, translation correctness and word
sense coverage, are used to evaluate the effectiveness of word-translations for a given
word independently of its meaning, i.e., when the sense of the word is not given. The
other two measures, synset coverage and synonym coverage, are used to evaluate the
effectiveness of synset-translation for a given synset focusing on the lexicalization of the
synsets in the target language. Word sense coverage and synset coverage are two measures
proposed in previous work by Navigli and Ponzetto (2012), but we rewrite their definitions
according to the notation introduced in Section 2.3. Translation correctness and synonym
coverage are introduced in this study. To facilitate the definition of these measures we first
introduce the definition of perfect translations with respect to a gold standard. From these
measures we can derive several measures, e.g., by averaging their values across one wordnet,
to present the results of our experiments. For the second objective, we use a measure that
is straightforwardly derived from the well-known Precision measure (Shvaiko & Euzenat,
2013) and is explained, directly, in Section 6.3.2.
178

fiEffectiveness of Automatic Translations for Cross-Lingual Ontology Mapping

4.1 Perfect Translations with a Gold Standard
The perfect word-translation of a word wL1 into a target language L2 w.r.t a gold
standard gs is the set of every synonym words in all the possible senses of wL1 in a target
language L2 :
[

n
L2
L1
L2
L1
L2 L2
L2
L1
L2
wT ransgs (w ) =
wi | s (s  sensesgs (w )  wi  s )
(3)
i=1

Example 3. Figure 1 illustrates the synset-translation tasks for four Italian synsets into
English. Each synset is mapped to its equivalent synset in English as specified by a gold
standard gs. The translations are also obtained from the mappings between the Italian
and the English wordnets represented in gs. For instance, the four (Italian  English)
synsets mappings are: {tavola+ , tabella+ }  {table+ , tabular array}, {asse+ , tavola+ } 
{board+ , plank+ }, {tavolo+ , tavola+ }  {table+ }, and {tavola+ }  {plate+ }. In Figure 1,
the perfect word-translation of the Italian word tavola into English can be given as follow:
+,It ) = {table+ , tabular array, board+ , plank+ , plate+ }En .
wTransEn
gs (tavola
Observe that the perfect word-translation function returns every word of every possible
sense in the target language, i.e., a word translation is perfect when it returns the complete
lexicalization of every possible senses of an input word in the target language. This definition
is motivated by the scope of our analysis, which evaluates the effectiveness of automatic
translations in settings where the domain is not determined a-priori. When an individual
input word is considered outside of a specific context, e.g., a specific sentence, a specialised
domain or a concept hierarchy, the meaning of the word cannot be disambiguated, unless
the word is monosemous. Otherwise, we observe that a domain-specific machine translation
system, e.g., specialised in the financial domain, could determine the correct meaning (and
translation) of a word, even when the word is considered individually, because of an implicit
interpretation of the context by the system. Thus, in consideration of polysemous words
and in absence of context specification, we defined a translation of a word (i.e., the set
of words returned by a source of translation) perfect when it contains, for every possible
usage of this word, all possible lexicalizations in the target language. If one considers wordtranslations in some specialized domain, he/she may need to adapt the definition of perfect
word-translation consequently.
The perfect synset-translation of a synset sL1 into a target language L2 w.r.t a gold
standard gs is defined as the set of every synonym words of the synset in L2 mapped to sL1
in gs. The perfect synset-translations can be defined as follows:
[

n
L2
L2
L2
L2
L1
L2
L2 L1
(4)
sT ransgs (s ) =
wi | s (wi  s  s  s )
i=1

Example 4. In Figure 1, the perfect synset-translation of the Italian synset {tavola+ ,
+
It
+
En .
tabella} can be given as follow: sTransEn
gs ({tavola , tabella} ) = {table , tabular array}
4.2 Evaluation of Word-Translations
In this section we introduce the translation correctness and the word sense coverage measures.
179

fiAbu Helou, Palmonari, & Jarrar

4.2.1 Translation Correctness
For an input word, this measure evaluates to what extent a resource returns precise and
complete translations when compared to perfect word translations defined by a gold standard, which consider every possible sense of the word in the target language.
To define this measure, we need to specify when a word returned by a resource is
correct. A word wL2 is a correct-translation for a word wL1 w.r.t to a gold standard
gs, if wL2 belongs to the set of perfect word-translations for wL1 w.r.t gs (denoted by
L1
L2
2
wT ransL
gs (w )). This principle is captured by the function correctTwL1 ,D (w ) defined by
the following equation:

L1
2
1
if wL2  {wT ransL
L2
gs (w )}.
correctTwL1 ,D (w ) =
0
otherwise.
Example 5. In Figure 1 the English words table, board, and plank are correct
translations for the Italian word tavola, e.g., correctTtavoalIt (tableEn ) =1. The English
words diner, panel, and slab are incorrect translations for the Italian word tavola,
e.g., correctTtavoalIt (slabEn )=0.
We measure the correctness of translations returned by a resource D for a word wL1
with translation-correctness as defined in Eq. 5. The measure is computed as the harmonic
mean, i.e., F1 -measure, of two measures: 1) Precision (Pr), defined as the number of correct
translations returned by the resource D over the total number of translations returned by
D; 2) Recall (R) 8 , defined as the number of correct translations returned by the translation
resource D over the total number of perfect word translations. We use Recall, Precision
and F1 -measure (computed with its standard range), but normalized in the range [0..100].
When no translation is returned by the resource D, Precision is set to zero.

Pr =

|{wL2 |correctTwL1 ,D (wL2 )}|
2
L1
|{wT ransL
D (w )}|

 100, R =

|{wL2 |correctTwL1 ,D (wL2 )}|
2
L
|{wT ransL
gs (w 1 )}|

 100

Pr  R
 100
(5)
Pr + R
Example 6. In the example shown in Figure 1, the correctness of English translation
of the Italian word tavola is computed as follows: recall R = 60.0, precision P r = 50.0,
and the translation-correctness TransCorrectnessEn (tavolaIt ) = 55.0.
L2
T ransCorrectnessD
(wL1 ) = F1 (P r, R)  100 = 2

4.2.2 Word Sense Coverage
For an input word, this measure evaluates how many of its possible word senses in a target
language are covered at least by a word translation (as defined in Navigli & Ponzetto, 2012).
A translation covers a sense sL2 of an input word wL1 in a different language when the
resource returns at least one word of sL2 . We use the binary predicate cov(x, y) to state
that a word-translation x covers the sense y. Word senses coverage tells to what extent the
polysemy of a word is covered by a translation resource. Ideally, a resource is effective in
8. We remark that Recall is also named translation accuracy in the W SD literature (Navigli, 2009).

180

fiEffectiveness of Automatic Translations for Cross-Lingual Ontology Mapping

translating a word wL1 when it is able to return some correct-translations for every possible
sense of wL1 in L2 .
Given a word wL1 translated into a target language L2 with a resource D, the word
senses coverage of wL1 is defined as follows:

L2
wsCoverageD
(wL1 )

	

L2
L1
L1
L2
2
| sL2 | sL2  sensesL
gs (w )  cov(wT ransD (w ), s ) |
=
	

2
L
| sL2 | sL2  sensesL
gs (w 1 ) |

(6)

Example 7. In Figure 1, the polysemous Italian word tavola has four senses, each
one is mapped into an equivalent synset in English. Using the translation resource D
three out of the four senses are covered (Eq.6). For instance, the senses {table+ } is
+
covered, cov(wT ransEn
D (tavola), {table}) = 1, while the sense {plate } is not covered,
cov(wTransEn
D (tavola), {plate}) =0.
4.3 Evaluation of Synset-Translations
In this section we introduce the synset and synonym coverage measures.
4.3.1 Synset Coverage
This measure is defined as boolean function applied to an input synset (Navigli & Ponzetto,
2012). A synset sL1 is covered if its translation, i.e., the multi set union of the translation
of its constituent words, returns at least one word of its equivalent synset in the target
language. This measure is useful when computed for a set of source synsets as presented
by Navigli and Ponzetto. For example, by computing the percentage of source synsets
mapped in a gold standard that are covered by translations obtained from a lexical resource,
we can evaluate the number of mappings that can be discovered by using this translation
resource.
To formally define synset coverage in a compact way, we can use the concept of perfect
L1
2
synset translation for a synset sL1 in a target language LL2 , denoted by sT ransL
gs (s ). If
L
s 1 is a synset translated into a target language L2 with a resource D, synset coverage is
defined as follows:

L1
L2  sT ransL2 (sL1 ))
2
1
if wL2 (wL2  sT ransL
L2 L1
gs
D (s )  w
sCoverageD (s ) =
(7)
0
otherwise.
Example 8. Consider the Italian and their equivalent English synsets depicted in Figure 1. Three out of four Italian synsets are covered because their translation returns at least
one word of their equivalent English synsets. For instance, the mapping {tavolo+ , tavola+ }
 {table+ } is covered, while the mapping {tavola+ }  {plate+ } is not covered.

4.3.2 Synonyms Coverage
For an input synset sL1 , this measure evaluates the number of words of sL1 for which a
word-translation covers the equivalent synset in the target language. This measure tells
how many synonyms in a concept lexicalization are covered by correct translations.
181

fiAbu Helou, Palmonari, & Jarrar

Give a synset sL1 having its equivalent synset sL2 in the target language; sL1 is translated
using a resource D, then the synonym coverage of sL1 is defined as follows:

L2 L1
synonymsCoverageD
(s )


	
L1
L2
2
| wL1 | wL1  sL1  cov(wT ransL
D (w ), s ) |
=
|sL1 |

(8)

Example 9. In Figure 1 the Italian synsets {tavola+ , tabella}, {asse+ , tavola+ }, and
{tavolo+ , tavola+ } have full synonym words coverage (Eq.8). Whereas, the synset {tavola+ }
is not covered because its only word is not covered.
Synonym coverage is a valuable measure to evaluate translations obtained from lexical
resources in the field of cross-lingual concept mapping. Consider, for example, an input
synset sL1 and a translation resource that returns many of the (synonym) words of its
equivalent synsets sL2 in the target language. On the one hand, these synonym words are
useful to increase the probability of finding sL2 among the candidate matches of sL1 . On the
other hand, these synonym words can be used as evidence for selecting sL2 as the best match
for sL1 , e.g., if compared to other candidate matches for which little evidence is collected
via translation9 . Finally, we observe that synonym words coverage is a complementary
indication of the word senses coverage to measure the effectives of a translation resource,
i.e., the coverage of the synonym words is a tool to disambiguate the polysemy of translations
returned by a translation resource.
Throughout this paper, in order to quantify the overall coverage measures and correctness of the word-translation tasks across each dataset (wordnet), we compute the Macroaverage measure (Vincent, 2013). The reported coverage measures are normalized in the
range [0..100].

5. Multilingual Lexical Resources for Translation
Automatic translations can be obtained using different kinds of multilingual machinereadable lexical resources. The selection of these resources depends on the level of information they encode, for instance, the quality (accuracy) of translations they provide, the lexical domains they cover. These resources include: dictionaries, thesauri, wordnets, machine
translation tools, and Web-based collaborative multilingual knowledge resources (resources
in which lexical knowledge is manually and collaboratively generated, e.g., Wikipedia).
In this study two multilingual lexical resources are used as sources of translations: Google
Translate and BabelNet. Google Translate is a statistical machine translation tool. Different machine translation systems exist that could be used; for instance, rule-based systems,
e.g., Apertium (Apertium, 2015), and statistical-based systems, e.g., UPC (Marioo et al.,
2006). We used Google Translate because previous work suggested that it performs better
than other Web translation services in the context of concept mapping (Al-Kabi et al., 2013;
Oliver & Climent, 2012), and has been adopted by several matching systems including the
ones evaluated in the OAEI (Shvaiko et al., 2014). Moreover, Google Translate is a generic
statistical machine translation, domain-independent system, and covers a very large number
of languages, including the ones considered in our study. A common evaluation measure
9. This intuition has been used, for example, in a cross-lingual similarity measure proposed to support
matching of lexical ontologies lexicalized in different languages (Abu Helou & Palmonari, 2015).

182

fiEffectiveness of Automatic Translations for Cross-Lingual Ontology Mapping

Figure 2: Google Translate response for the Italian word tavola translated into English
of the machine translation quality is BLEU (Bilingual Evaluation Understudy) (Papineni,
Roukos, Ward, & Zhu, 2002), which is based on the n-gram precision model. Thus, this
measure does not fit the context of word-to-word translation, the case that we are considering. A comparison between different machine translation tools is out of scope of this study.
For a rich and comprehensive comparison of different machine translation tools we refer to
the work of Costa-jussa, Farrus, Marino, and Fonollosa (2012).
BabelNet is arguably the largest state-of-the-art multilingual knowledge resource. BabelNet has integrated several Web-based collaborative multilingual knowledge resources
(see Section 3.3). In addition, it makes different translation strategies available, which we
want to evaluate indirectly in our study: sense-tagged sentence translation, direct machine
translation of monosemous words, and translations obtained from Wikipedia to Wordnet
mappings.
We used Google Translate and BabelNet to construct bilingual dictionaries for every
pairs of non-English and English languages considered in our study (see Section 6.2).
Google Translate service is accessible through an API that can be used to translate
sentence-to-sentence and word-to-word for many pairs of languages. Figure 2 shows Googles
word-to-word translation response in JSON (2015) format for the Italian word tavola
translated into English. Google returns the preferable (common) translation in the trans
item. A list of possible translations is also given in the dict item, which is part-of-speech
(P oS) tagged. Each translation word in the dict item has a reverse translation set and
a score. The reverse translation is a set of potential synonym words for the input word.
The score estimates the translation usage (e.g., common, uncommon, or rare translations).
The translation directions (e.g., It-to-En, and En-to-It) of machine translation tools
are said to have different performance when applied in cross-lingual information retrieval
tasks (McCarley, 1999). To ensure the largest possible coverage we compiled three bilingual
dictionaries using Google Translate by taking into account the translation direction. We
also collect translations provided in the reverse translation sets. To the best of our
knowledge available matching systems consider translations returned only in the trans
item. For each pair of non-English and English languages considered in our gold standard
we build the following bilingual dictionaries: M T f romEn uses the translations collected
from English to non-English words; M T toEn uses the translations collected from nonEnglish to English words; M T merges translations collected for the other two dictionaries
to ensure the largest possible coverage (with Google Translate). Observe that M T f romEn
and M T toEn are subsets of M T .
183

fiAbu Helou, Palmonari, & Jarrar

Table 1: Translation settings
Bilingual Dictionary
M T f romEn
M T toEn
MT
BN
BNcore
M T &BNcore
M T &BN

Description
translations from English to non-English words using Google Translate
translations from non-English to English words using Google Translate
the union of M T f romEn and M T toEn
all translations encoded in BabelNet except translation from Open Multilingual WordNet
BabelNet core synsets translations
the union of M T and BNcore
the union of M T and BN

BabelNet is structured as a graph of nodes. Nodes, called BabelNet synsets, represent
concepts or named entities, which are lexicalized in several languages. For instance, the
Italian lexicalizations in a node represent an Italian synset, which represents an equivalent
synset to its corresponding English lexicalization, which is synset in the English WordNet.
The translation of a given word in a source language (e.g., It) into a target language (e.g.,
En) with BabelNet is given by every word in the target language, which localizes the same
nodes that are lexicalized with the input word. For example, the Italian word tavola is
lexicalization of 15 nodes10 (14 concepts, and 1 named entity). These nodes provide 25
possible translations (lexicalization) in English: {board, correlation table, place setting,
plank, setting, table, tablet, gang plank, wood plank, plate, table setting, stretcher bar,
Panel, Panel cartoon, Oil on panel, ..., etc}. Each word in this lexicalization may derive
from one or many of the different lexical resources integrated in BabelNet.
To analyze the impact of the different lexical resources integrated in BabelNet, we
extracted, for every pair of non-English and English languages used in our study (see
Section 6.2), two bilingual dictionaries from the BabelNet synsets. A first dictionary is
extracted from BabelNet core synsets (called BNcore ), which contain multilingual lexicalizations built from: sense-tagged sentences, monosemous word translation using Google
Translate (monosemous words heuristic), and Wikipedia inter-lingual links. A second dictionary is extracted from BabelNet synsets (called BN ), all synsets in BabelNet, which contain
multilingual lexicalizations built from: BNcore , and lexicalization obtained from WikiData,
Wikitionary, OmegaWiki, and Wikipedia redirection links. Observe that BNcore is a subset of BN . We only excluded translations obtained from the Open Multilingual WordNet
(Bond & Foster, 2013), which we adopt as gold standards in our study.
We also merged translations from BNcore and M T dictionaries (called M T &BNcore ),
and translations from BN and M T dictionaries (called M T &BN ). In this way we can
compare and evaluate the impact of different Web-based multilingual resources, BabelNet
core synsets, and the machine translation tools on the cross-lingual mapping tasks. The
bilingual dictionaries we use in this study are summarized in Table 1.

6. Experiments
Three experiments are conducted to study the coverage, correctness, and impact of two
multilingual lexical resources that we used as sources of translation on mapping concepts
lexicalized in different languages. Four non-English wordnets, which are mapped to the
English WordNet, are used as our gold standards.
10. http://babelnet.org/search?word=tavola&lang=IT

184

fiEffectiveness of Automatic Translations for Cross-Lingual Ontology Mapping

Table 2: Size of the wordnets (gold standards) used in the experiments
Words
Word senses
Synsets

English
147306
206941
117659

Arabic
13866
23481
10349

Italian
40178
61588
33731

Slovene
39985
70947
42583

Spanish
36880
57989
38702

First, in Section 6.2 we describe in details the wordnets and we profile their concepts
based on their lexicalizations. Then, in Section 6.2, we move to perform our experiments.
We organize the discussion about our experiments as follows. In Section 6.3.1 we evaluate the coverage and correctness of translations obtained with different lexical resources
to discuss their impact on retrieving candidate matches in concept mapping tasks. In
Section 6.3.2, evidence collected from translations is used in a baseline mapping selection
approach, i.e., majority voting, to evaluate the difficulty of the mapping selection task.
In Section 6.3.3, we analyze the coverage of translations in relation to the position of the
concepts in the semantic hierarchies.
Finally, in Section 6.4 we summarize our observations and draw some potential future
directions.
6.1 Experimental Setup
The English, Arabic, Italian, Slovene, and Spanish wordnets are imported into a database.
The wordnets database includes the words, synsets, semantic relations, and the mappings
between each non-English wordnet and the English Wordnet. We compiled different bilingual dictionaries (Table 1) with Google Translate API and BabelNet as described in Section
5. We stored the dictionaries in a database so as to efficiently execute our experiments.
6.2 Mapped Wordnets Used as Gold Standards
In our study we use wordnets for English (Miller, 1995; Fellbaum, 1998), Arabic (Rodrguez
et al., 2008), Italian (Pianta et al., 2002), Slovene (Fiser, 2007) and Spanish (GonzalezAgirre et al., 2012). These wordnets provide high quality cross-lingual mappings and contain
very large inventories of concepts. Their size in terms of words, word senses and synsets
is reported in Table 211 . These wordnets have been built using different approaches and
cover different families of languages: the Germanic languages (e.g., English), the Romance
languages (e.g., Italian and Spanish), the Slavic languages (e.g., Slovene), and the Semitic
languages (e.g., Arabic). Spanish, English, and Arabic are also among the top five spoken
languages in the world (Wikipedia, 2015a), and their processing has gathered significant
interest from the research community. Italian and Slovene represent two minority languages.
In Table 3 we show the distribution of words in each wordnet disaggregated by several
categories: By considering word ambiguity, we distinguish between Monosemous words (M ),
words that have only one sense (meaning), and Polysemous words (P ), words that have
two or more senses. By considering word complexity, we distinguish between Single words
(S), strings (lexemes) that have no spaces or hyphens, or Collection words (C), strings that
consist of two or more simple words, which are connected by spaces or hyphens. We also
11. The Arabic, Italian, and Slovene wordnets are obtained from OMWN (2015), and the Spanish wordnet is
obtained from MCR (2012). All lexical gaps (synsets with no lexicalization) (Vossen, 2004) are excluded.

185

fiAbu Helou, Palmonari, & Jarrar

Table 3: Word distribution in the gold standards by category: quantity (percentage)
Words
M onosemous(M )
P olysemous(P )
Simple(S)
Collection(C)
M &S
M &C
P &S
P &C

English
120433
(81.8)
26873
(18.2)
83118
(56.4)
64188
(43.6)
59021
(40.1)
61412
(41.6)
24097
(16.4)
2776
(01.9)

Arabic
10025
(72.3)
3841
(27.7)
8953
(64.6)
4913
(35.4)
5361
(38.5)
4664
(33.6)
3592
(26.0)
249
(01.8)

Italian
29816
(74.2)
10362
(25.8)
33133
(82.5)
7045
(17.5)
22987
(57.2)
6827
(17.0)
10146
(25.3)
218
(00.5)

Slovene
28635
(71.6)
11350
(28.4)
29943
(74.9)
10042
(25.1)
19223
(48.1)
9412
(23.5)
10720
(26.8)
630
(01.6)

Spanish
30106
(81.6)
6774
(18.4)
22630
(61.4)
14250
(38.6)
16212
(44.0)
13894
(37.7)
6418
(17.4)
356
(00.9)

Table 4: Synsets categories
Category
all M
all P
OW S
MW S
M &OW S
M &M W S
M IX
P &OW S
P &M W S

Synset name
all words Monosemous
all words Polysemous
One-Word
Many-Words
Monosemous and OW S
Monosemous and M W S
MIXed
Polysemous and OW S
Polysemous and M W S

Definition synsets that have...
only monosemous words
only polysemous words
only one word (synonymless synset)
two or more synonym words (synonymful synset)
only one word, which is also a monosemous word
two or more synonym words, which are all monosemous words
monosemous and polysemous synonym words
only one word, which is also a polysemous word
two or more synonym words, which are polysemous words

consider the four categories that are derived by combining word ambiguity and complexity
categories. For example, tourism is a monosemous and simple word (M &S), tabular
array is a monosemous and collection word (M &C), table+  is a polysmouse and simple
word (P &S), and break up+  is a polysemous and collection word (P &C).
Observation 1. A vast majority of collection words are monosemous words: only an
average of 1.3% words are polysemous collection words across all wordnets. This means
that a word used as concept label is less likely to be ambiguous if it is a composite word
and more likely to be ambiguous if it is a simple word.
We can classify the synsets based on the ambiguity and number of their words (respectively first and second, and third and fourth categories of synsets described in the upper part
of Table 4). By combining these orthogonal classifications, we can consider five categories
of synsets as described in the lower part of Table 4. One can observe that the M &OW S
and the M &M W S are subsets of the all M . The P &OW S and the P &M W S are subsets
of the all P , and the M IX are subsets of the M W S. Examples of synsets in the English
WordNet for each category are shown in Table 5. Table 6 describes, for every wordnet, the
total number and percentage of synsets grouped by category.
Table 5: Synset examples for all categories in English
Category
M &OW S

Example
{desk}

M &M W S
M IX
P &OW S
P &OW S
P &OW S

{tourism, touristry}
{table+ , tabular array}
{cocktail+ }
{cocktail+ }
{table+ }

P &M W S

{board+ , table+ }

Definition
a piece of furniture with a writing surface and usually drawers or other
compartments
the business of providing services to tourists
a set of data arranged in rows and columns
a short mixed drink
an appetizer served as a first course at a meal
a piece of furniture having a smooth flat top that is usually supported
by one or more vertical legs
food or meals in general

186

fiEffectiveness of Automatic Translations for Cross-Lingual Ontology Mapping

Table 6: Synset category-wise distribution in gold standards: quantity (percentage)
Synsets
all M
all P
MWS
OWS
M&OWS
M&MWS
MIX
P&OWS
P&MWS

English
57415
(48.8)
41568
(35.3)
53784
(45.7)
63875
(54.3)
33596
(28.6)
23819
(20.2)
18676
(15.9)
30279
(25.7)
11289
(9.60)

Arabic
3381
(32.7)
4409
(42.6)
6162
(59.5)
4197
(40.5)
1995
(19.3)
1386
(13.4)
2559
(24.7)
2194
(21.2)
2215
(21.4)

Italian
14393
(42.7)
14641
(43.4)
13644
(40.4)
21084
(59.6)
10492
(31.1)
3901
(11.6)
5691
(16.9)
9609
(28.5)
4046
(12.0)

Slovene
17615
(41.4)
19609
(46.0)
14994
(35.2)
27589
(64.8)
14848
(34.9)
2767
(06.5)
5359
(12.6)
12741
(29.9)
6868
(16.1)

Spanish
19020
(49.1)
16269
(42.1)
14994
(38.7)
27589
(71.3)
14120
(36.5)
4900
(12.7)
3413
(08.8)
12005
(31.0)
4264
(11.0)

Table 7: Examples of mappings between Italian and English synsets by category
Synsets

M&OWS
{art
school}

M&MWS

M&OWS

{scuola
darte}

{radiostazione,
stazione
radio}

{radio
station}

M&MWS

{turismo} {tourism,
touristry}

{accoppiata,{exacta,
abperbinata}
fecta}

MIX

{minorit} {minority+
, nonage}

{biforcarsi,
ramificarsi,
diramarsi}

P&OWS

{forchetta}{fork+ }

{stretto,
vicino}

P&MWS

{chiudersi}{close+ ,
shut+ }

{inquietarsi, {care+ ,
allarworry+ }
marsi}

{branch,
fork+ ,
furcate,
ramify,
separate}
{close+ }

MIX

P&OWS

P&MWS

{tavolino,
banco+ ,
scrivania}
{docente+ ,
cattedratico,
professore}
{tavola+ ,
tabella}

{desk}

{ordinario+ {full
}
professor}

{entita+ , {entity}
cosa+ }

{prof,
professor}

{viaggiatore+{traveler,
}
traveller}

{classe+ ,
aula+ }

{table+ ,
tabular
array}

{contribuire+{conduce,
}
contribute,
lead+ }

{cibo+ ,
{repast,
pasto+ ,
meal+ }
+
mangiare
}

{poltrona+ ,
seggiola,
sedia}
{segnare+ ,
scalfire}

{chair +
}

{cosa+ }

{tavola+ ,
tavolo+ }

{score+ ,
mark+ ,
nock+ }

{moderare+ {chair+ ,
{cibo+ ,
}
moderate+ , vitto+ }
lead+ }

{thing + }

{classroom,
schoolroom}

{table+ }

{board+ ,
table+ }

Observation 2. Wordnets have more synonymless synsets (OW S) than synonymful
synsets (MWS), with 58.1% of synsets being, on average, synonymless. Arabic, which
has less OW S than M W S, represents an exception among the considered wordnets. In
particular, the Arabic polysemous synsets (all P ) are equally distributed between OW S
and M W S.
In our gold standards there exist mappings between synsets of every category. Examples
of mappings for each couple of categories of synsets from Italian to English are shown in
Table 7. The percentage of the mapped synsets between the non-English wordnets and the
English WordNet, grouped by category, is reported in Table 8.
The results confirm that languages do not cover the same number of words as noticed
by Hirst (2004), and, hence, concepts shared in different languages have different ways to
express their meanings (i.e., they belong to different lexical categories). For instance, 57% of
the Italian M &OW S synsets are mapped to monosemous synsets in English (M &OW S and
M &M W S). On the other hand, 25% of the Italian M &OW S are mapped to polysemous
synsets in English (P &OW S and P &M W S). The percentage of monosemous non-English
synsets that are mapped to the polysemous English synsets ranges from 10% (Slovene) to
30% (Arabic). The percentage of the monosemous English synsets that are mapped to the
polysemous non-English synsets ranges from 6% (Arabic) to 14% (Italian). For instance,
the M &OW S Italian synsets {fotografare} and {azioni ordinarie} are mapped to {shoot+ ,
snap+ , photograph+ } and {common shares, common stock, ordinary shares}, respectively
a P &M W S and an M &M W S English synset.
187

fiAbu Helou, Palmonari, & Jarrar

Table 8: Distribution of mapping by category: percentage
M&OWS M&MWS MIX
English
Arabic
M&OWS 32.9
19.2
5.1
M&MWS 15.1
28.6
5.1
MIX
17.2
28.7
37.7
P&OWS 27.4
14.8
21.7
P&MWS 7.3
8.7
30.4
Slovene
M&OWS 23.4
25.2
14.2
M&MWS 47.8
39.7
13.0
MIX
18.1
27.5
48.7
P&OWS 7.1
4.0
8.4
P&MWS 3.5
3.7
15.7

P&OWS P&MWS M&OWS M&MWS MIX
Italian
5.4
2.3
36.2
20.9
10.6
2.5
1.5
21.2
34.9
10.3
15.5
22.6
17.8
27.2
38.7
57.3
29.5
17.9
10.7
18.4
19.4
44.2
6.9
6.3
22.0
Spanish
9.0
6.8
42.6
10.7
7.8
4.4
4.3
22.2
63.1
7.7
20.2
27.1
14.5
17.1
44.1
45.3
25.7
17.8
5.4
15.1
21.1
36.1
2.9
3.8
25.3

P&OWS P&MWS
9.4
4.6
22.5
43.0
20.5

4.1
2.8
26.8
29.0
37.4

8.4
3.3
19.4
48.5
20.4

3.1
1.9
24.2
25.9
44.9

Observation 3. Synsets in different languages, which have an equivalent meaning, can
fall in different synset categories. For example, the Italian monosemous synonymless synset
{forchetta} is mapped to the polysemous synomymless synset {fork+ } in English. This
indicates that the monosemous word heuristic, which is adopted by some approaches to
concept mapping and multilingual knowledge construction, e.g., work presented by Navigli
and Ponzetto (2012), is successful for a large number of concepts but fails for still a relevant
number of concepts. An average of 19.3% non-English monosemous synsets are mapped to
English polysemous synsets in the gold standards, and an average of 8.9% English monosemous synsets are mapped to non-English polysemous synsets in the gold standards. More
details on the impact of the monosemous word heuristics are provided in Section 6.3.1,
where translation correctness is analyzed.
6.3 Results and Discussion
In this section we describe in details the three experiments presented in our study.
6.3.1 Experiment 1: Coverage and Correctness of Translations for
Candidate Match Retrieval
In order to evaluate the coverage of the translations obtained with different lexical resources, we use two measures. We compute the average word sense coverage across all
words of a wordnet, where word sense coverage is defined for an individual word as in Eq.6.
We compute the average synset coverage across all synsets of a wordnet, where synset coverage is (a boolean value) defined for an individual word as in Eq. 7. All values are normalized
in the range [0..100]. For sake of clarity we will simply refer to these measures as word sense
and synset coverage (at the wordnet level).
Table 9 reports, for each wordnet, word sense and synset coverage with different translation settings. Synsets have higher coverage than word senses in all the translation settings.
This can be explained with the observation that a synset is covered if its translation returns
at least one word of the lexicalization of its equivalent synset in the target language (see
Eq.7).
We observe that machine translations from non-English to English (M T toEn) achieve
higher word sense and synset coverage than machine translation from English to non-English
(M T f romEn). For instance, word sense coverage of M T toEn is from 5.2 (Italian) to 10.9
188

fiEffectiveness of Automatic Translations for Cross-Lingual Ontology Mapping

Table 9: Word sense and synset coverages with different translation settings
Translation
BNcore
BN
M T f romEn
M T toEn
MT
M T &BNcore
M T &BN

Arabic
Senses
Synsets
19.9
37.4
30.8
51.3
51.3
69.9
57.9
76.1
59.2
77.7
60.8
79.2
62.5
80.2

Italian
Senses
Synsets
40.0
62.5
51.7
72.8
60.2
81.9
65.4
83.9
68.1
87.6
69.8
89.0
72.2
89.9

Slovene
Senses
Synsets
28.8
44.2
35.9
52.0
40.2
60.0
49.6
67.2
53.8
72.4
55.8
74.2
57.5
75.2

Spanish
Senses
Synsets
33.9
44.7
39.8
49.0
56.1
67.8
67.0
77.0
69.4
79.7
71.5
81.3
72.3
81.7

(Spanish) percentage points higher than M T f romEn, and synset coverage of M T toEn
is from 2.0 (Italian) to 9.2 (Spanish) percentage points higher than M T f romEn.
Observation 4. Machine translation tools perform asymmetrically: M T toEn achieves
higher word sense and synset coverage than M T f romEn.
The machine translation bilingual dictionary (M T ), which we built from the union of
both machine translation directions (see Section 5), performs better than the dictionaries
that we built considering each direction alone (i.e., M T f romEn or M T toEn). Word
sense coverage of M T is on average 2.7 and 8.2 percentage points higher than M T toEn
and M T f romEn, respectively. Synset coverage of M T is on average 3.5 and 7.0 percentage
points higher than M T toEn and M T f romEn, respectively.
BNcore and BN translation settings, which are based on BabelNet, obtain lower coverage than every machine translation setting for all wordnets. This can be explained by
limited coverage of the words that occur in non-English wordnets by Wikipedia concepts
(which mostly cover named entities), and by incompleteness of the mappings used to construct BabelNet (Navigli & Ponzetto, 2012). However, it should be remarked that, for
several languages, BabelNet also includes the lexicalizations from the Open Multilingual
WordNet that have been excluded in our study because it is part of the gold standard (see
Section 6.2). This means that for several well-known languages such as French, Germany,
Spanish, and Italian12 we can expect much higher translation coverage from BabelNet.
Still, best results are obtained when combining all available translations, i.e., both from
the machine translation tool and BabelNet, M T &BN . For instance, M T &BN word sense
coverage is on average 3.5 percentage points higher than M T . M T &BN synset coverage is
on average 2.4 percentage points higher than M T .
We also observe that BN achieves considerably higher coverage than BNcore , with an
average difference in word sense and synset coverage of 10.4 and 10.1 percentage points
respectively (BNcore is a subset of BN - see Section 5). However, most of this additional
coverage is lost when combining BNcore and BN with M T translations: M T &BN word
sense coverage is on average only 1.7 percentage points higher than M T &BNcore , and
M T &BN synset coverage is on average 0.8 percentage points higher than M T &BNcore .
Observation 5. The results highlight that machine translation tools achieve higher
coverage than BabelNet, which integrates several Web-based multilingual resources (i.e.,
Wikitionary, OmegaWiki, WikiData, and Wikipeida redirection links). However, integrating BabelNet with machine translation tools still yields a significant gain in coverage, mostly
12. See the Languages and Coverage Statistics at BabelNet (2012).

189

fiAbu Helou, Palmonari, & Jarrar

Table 10: Average synset coverage by category
Synsets
BN
all M
35.4
all P
58.8
OW S
44.5
MW S
56.1
M &OW S 32.2
M &M W S 40.3
M IX
59.8
P &OW S 55.8
P &M W S 61.9

Arabic
MT MT&BN
64.2
67.3
82.9
85.4
67.0
70.5
85.0
86.8
58.0
61.5
73.4
76.0
86.9
88.6
75.4
79.0
90.5
92.0

BN
68.6
69.0
64.1
81.0
63.8
81.6
80.7
71.0
80.8

Italian
MT MT&BN
86.0
88.4
80.5
83.0
79.5
82.2
93.6
95.2
83.5
86.1
92.6
94.5
94.3
95.8
83.3
86.4
93.7
95.1

BN
59.8
44.8
52.8
50.7
60.8
54.3
53.1
43.4
47.3

Slovene
MT MT&BN
78.8
80.8
65.5
69.1
70.1
73.0
76.6
79.1
78.3
80.3
81.4
83.2
76.5
79.0
60.5
64.5
74.7
77.6

BN
34.4
62.5
44.1
59.0
32.4
40.1
65.5
57.9
75.5

Spanish
MT MT&BN
78.1
79.5
80.2
83.0
75.8
78.0
87.7
89.3
74.7
76.2
88.0
89.0
85.8
87.6
77.1
80.2
88.8
90.9

Table 11: Average number of candidate matches
Synsets
synonymless (OW S)
synonymful (M W S)

Arabic
48
124

Italian
17
49

Slovene
11
21

Spanish
27
75

because of BNcore (Wikipedia inter-lingual links, and the context based translations).
Table 10 reports the average coverage for each synset category by using BN, MT and
M T &BN translation settings (the settings achieving highest coverage). The results show
that the synonymful synsets (M W S) are covered more than synonymless synsets (OW S) for
every wordnet and almost every translation setting. This confirms the intuition that richer
concept lexicalizations help to find at least one correct translation using machine translation tools. Polysemous synsets (all P ) are covered more than the monosemous synsets
(all M ) for Arabic and Spanish, but less than monosemous synsets (all M ) for Italian and
Slovene. This can be explained by the distribution of polysemous and monosemous synsets
between synonymless and synonymful synsets: most of the monosemous synsets (all M ) are
synonymless synsets, and most of the polysemous synsets (all P ) are synonymful synsets.
M IXed synsets are the most covered synsets, since they are synonymful synsets, which
combine monosemous and polysemous words.
Observation 6. Synonymful synsets (M W S) are covered more than synonymless
synsets (OW S) (see Table 10). However, a higher coverage comes at the price of a larger
number of candidate matches, thus making the mapping selection task more challenging
(see Table 11).
Observation 6 can be supported by figures shown in Table 11, which reports the average
number of candidate matches for synonymless vs. synonymful synsets. In addition, most
of synonymful synsets contain at least one polysemous word (see Table 6 ). Thus, one can
expect that the sets of candidate matches returned by translations of synonymful synsets are
not only larger in size, but also noisier, because of the translation of polysemous words. A
more in-depth analysis on the difficulty of the mapping selection task for the different synset
categories is provided in Section 6.3.2. Such analysis will confirm that the mapping selection
problem is more difficult for synsets that contain polysemous words, which represent the
majority of synonymful synsets. At the same time, the joint translation of synonym words
can support mapping selection for many synsets (e.g., for synsets that do not contain only
190

fiEffectiveness of Automatic Translations for Cross-Lingual Ontology Mapping

Table 12: Average recall and word-translation correctness by category
Words
M
P
S
C
M&S
M&C
P&S
P&C
ALL
Words

20.2
53.1
38.1
13.3
26.7
12.7
55.0
25.3
29.3

BN
(63.6)
(38.0)
(48.3)
(63.4)
(63.0)
(65.2)
(37.7)
(46.6)
(50.8)

M
P
S
C
M&S
M&C
P&S
P&C
ALL

45.2
42.6
43.7
46.7
43.7
48.3
43.7
22.9
44.5

BN
(66.1)
(39.6)
(56.3)
(66.0)
(65.8)
(66.5)
(39.2)
(50.3)
(58.9)

Arabic
MT
45.1
(36.9)
83.3
(22.8)
67.0
(27.3)
35.1
(43.9)
54.8
(32.6)
34.0
(44.9)
85.2
(22.3)
55.4
(32.0)
55.7
(31.0)
Slovene
MT
63.6
(47.8)
73.0
(30.1)
67.0
(41.4)
64.3
(45.0)
62.5
(49.0)
66.0
(45.5)
75.0
(30.0)
38.9
(31.3)
66.3
(42.3)

MT&BN
48.0
(56.4)
85.2
(40.9)
70.0
(49.1)
37.0
(53.8)
58.6
(57.2)
35.8
(55.0)
87.0
(40.9)
58.2
(40.3)
58.3
(50.2)

49.0
71.5
54.4
56.9
46.7
56.9
71.8
56.0
54.8

BN
(65.6)
(44.9)
(57.0)
(65.8)
(65.3)
(66.3)
(44.8)
(48.6)
(58.6)

MT&BN
66.6
(60.8)
75.4
(33.7)
69.7
(49.9)
67.3
(60.3)
65.4
(60.7)
69.1
(61.1)
77.4
(33.5)
41.1
(39.9)
69.1
(52.4)

28.1
74.8
48.9
17.4
37.6
17.0
77.3
30.6
36.7

BN
(61.6)
(38.9)
(51.0)
(62.3)
(61.1)
(63.0)
(38.7)
(47.5)
(53.1)

Italian
MT
65.8
(47.0)
89.8
(31.9)
73.0
(41.0)
67.3
(47.5)
65.4
(46.7)
66.9
(47.9)
90.0
(31.7)
80.3
(39.0)
72.0
(42.1)
Spanish
MT
68.4
(48.1)
92.1
(28.4)
78.8
(41.0)
63.1
(48.4)
73.2
(47.6)
62.8
(48.6)
93.0
(27.9)
75.6
(38.8)
72.8
(43.5)

MT&BN
69.8
(62.1)
91.3
(45.1)
75.9
(55.5)
72.6
(62.8)
69.1
(61.6)
72.2
(63.5)
91.4
(45.1)
84.9
(44.1)
75.3
(56.8)
MT&BN
70.8
(56.9)
93.7
(41.4)
81.0
(53.3)
65.7
(53.5)
75.5
(59.1)
65.4
(53.9)
94.7
(41.5)
76.7
(40.3)
75.0
(53.3)

polysemous words, e.g., M IXed synsets), as a means to collect evidence for deciding about
a mapping.
In order to evaluate the correctness of the translations obtained with different resources, we use two measures. We compute the average word-translation correctness across
all words of a wordnet; word-translation correctness is defined for an individual word as in
Eq.5. In addition, we report average word-translation recall (recall, for short), using the
subformula in Eq.513 .
Average recall and word-translation correctness for the BN , M T and M T &BN dictionaries, disaggregated by word category, are reported in Table 12.
The results show that word-translation is more correct for monosemous and collection
words than for polysemous and simple words. In contrast, recall of word-translation is
higher for polysemous words (P ) than for monosemous words (M ) with every source of
translation and for every wordnets, with the exception of the BN dictionary for the Slovene
wordnet. Recall of word-translation is also higher for simple words (S) than for collection
words (C) in every setting. These observations can be explained by monosemous words
being usually less frequent and more domain-specific than polysemous words. In addition,
most of collection words are also monosemous words - as remarked in Observation 1 -,
while most of polysemous words are simple words: recall and correctness of translations
for simple words is affected by the translation of simple polysemous words. Translations of
polysemous and simple words return on average larger word sets. These sets are more likely
to contain richer lexicalizations in the target language, but also to contain words that do
not belong to any sense of the input words in the target language.
Observation 7. The translation of monosemous and collection words is on average
more correct than the translation of polysemous and simple words, but achieves lower recall.
13. Word-translation correctness is defined using a formula based on F1 -measure.

191

fiAbu Helou, Palmonari, & Jarrar

Focusing on the performance of different sources of translation, we can notice that recall
for M T is higher than for BN , while correctness for BN is higher than for M T . M T &BN
combines the strengths of both dictionaries, i.e., higher recall of word-translation because
of M T , and higher correctness because of BN . For instance, in Table 12 we can notice
that the correctness of word-translation is improved by 9.8 and 19.2 percentage points for
Spanish and Arabic respectively, if we add to M T translations derived from BN . The recall
of word-translation is improved by as much as 20.4 and 38.3 percentage points for Italian
and Spanish respectively, if we add to BN translations derived from M T . The best results
are thus obtained for M T &BN , for which we obtain recall (correctness) scores that range
from 58.3%(50.2%) (Arabic) to 75.3% (56.8%)(Italian). The low recall for Arabic can be
explained by a low recall for translations of monosemous collection words.
Observation 8. The combination of machine translation tools with Web-based multilingual resources and context-based sentence translations, like the ones incorporated in
BabelNet, improves not only the recall, but also the correctness of word-translations.
6.3.2 Experiment 2: Mapping Selection Difficulty
On one hand, the translations returned for a given synset can be used as evidence to
select a mapping to a synset in a target language. On the other hand, translations of
many, polysemous words in a synset can return several candidate matches, most of which
are incorrect, thus making the mapping selection task difficult to solve. This experiment
analyzes the difficulty of the mapping selection task when performed over candidate matches
retrieved with translations obtained from different lexical resources.
In this experiment we use translations returned by the M T machine translation tool,
for the sake of simplicity (with the exception of the analysis of the synonym word coverage,
where we also include BN ). We focus on M T because, as shown in the previous section,
it has higher coverage than BN , and it has been widely used in previous work on ontology matching. In addition, the slight increase in coverage obtained with M T &BN , when
compared to M T , can be ignored for this particular experiment.
To perform our analysis we use a greedy baseline method for candidate mapping selection and we compare the quality of the alignment computed with this method to the gold
standard alignment. As a baseline mapping selection method we use majority voting on top
of the evidence collected from synset translations.
Mapping Selection with Majority Voting. Every source synset is translated using
the synset translation function defined in Eq.2. The output is represented as the multi set
union of the returned translations. Each word w(i) in the multi set, with (i) being the word
frequency count, represents i votes for all the candidate matches that contain w. Therefore,
a candidate match t for a source synset s, such that t contains many words returned by
the translation of s, will receive more votes and will be more likely to be the target of the
selected mapping. Candidate matches are ranked by votes and the mapping containing the
top-voted match is selected.
It can happen that several candidate matches receive an equal number of votes, which
results in a tie. In this case, for a source synset the mapping selection task is undecidable;
in contrast we will say that a mapping is decidable when a unique candidate match receives
the highest number of votes. However, when a tie occurs among a set of top-voted candidate
192

fiEffectiveness of Automatic Translations for Cross-Lingual Ontology Mapping

100

TopOne

90

100

TopSet

Arabic
Slovene

90

80

80

70

70

60

60

50

50

40

40

30

30

20

20

10

10

0

Italian
Spanish

0

Arabic

Italian

Slovene

Spanish

(a) correct mappings found with T opOne and
T opSet settings

M-MWS

M-OWS

MIX

P-MWS

P-OWS

(b) correct mappings in the distinguishable candidate matches by category

Figure 3: Correct mappings found with baseline selection strategy
matches, it is valuable to know if this set contains also the correct mapping (w.r.t to the gold
standard) and the number of candidate matches in the tie. In fact, if the set of top-voted
candidate matches also contains the correct match, the correct mapping could be found
via user interaction with relatively low effort. For these reasons we use two settings in the
experiments with a majority voting candidate selection approach:
 TopOne: if there exists a unique top-voted candidate match for a source synset, the
mapping containing this match is selected and included in the alignment. If a tie
occurs, no mapping is selected.
 TopSet: the correct mapping is selected by an oracle from the set of top-voted
matches (no matter of its cardinality) and included in the alignment.
To quantify the quality of the alignment we compute (selection) correctness as the
percentage of the correct mappings returned by each selection setting over the set of covered
mappings, i.e., the mappings for which the set of candidate matches contains the correct
mapping14 . In other words, in TopOne setting, a mapping is considered correct for a source
synset, only when the correct match for the synset (according to the gold standard) is its
unique top-voted candidate match; in TopSet setting, a mapping is considered correct for
a source synset, whenever the correct match for the synset is included in the set of its
top-voted candidate matches. Observe that every mapping that is counted as correct in
TopOne setting, is also counted as correct in TopSet setting.
A comparison between the performance in terms of correct mappings returned in the
T opOne and T opSet selection settings for each wordnet is shown in Figure 3(a). The average
of correct mappings obtained in T opOne and T opSet settings is 28% and 50% respectively.
Based on the performance of such simple baseline methods, we suggest that translations can
be helpful for mapping selection, although more sophisticated methods to make use of their
evidence have to be devised. In addition, number of correct mappings can be increased up
to an average of 30 points in the case where we assume that a user can select the correct
mapping among the set of top-voted matches returned by a mapping selection method,
14. This is equivalent to compute a relative precision measure: Precision is interpreted as usual in ontology
matching (Shvaiko & Euzenat, 2013) but normalized in the range [0..100], and evaluated only over a
restricted subset of the gold standard. Such a restricted subset consists of all the mappings containing
source concepts that are covered by translations

193

fiAbu Helou, Palmonari, & Jarrar

100
90
80
70
60
50
40
30
20
10
0

Arabic
Slovene

M-MWS

M-OWS

MIX

100
90
80
70
60
50
40
30
20
10
0

Italian
Spanish

P-MWS

Arabic
Slovene

M-MWS

P-OWS

(a)

M-OWS

MIX

Italian
Spanish

P-MWS

P-OWS

(b)

Figure 4: Percentage of the correct mappings by synset category with (a) T opOne selection
and (b) T opSet selection

e.g., with an interactive ontology matching approach (Cruz, Loprete, Palmonari, Stroe, &
Taheri, 2014; Sarasua, Simperl, & Noy, 2012). However, the average cardinality of the sets
of top-voted matches (T opSet) is as high as 49 synsets, which makes it difficult for users to
make a decision.
Figure 3(b) shows, for every wordnet and every category of source synset, the percentage
of correct mappings found using T opOne selection over the total of synset with decidable
mappings. The baseline T opOne mapping selection strategy achieves a remarkable performance for monosemous synsets (i.e., M &OW S and M &M W S) and poor performance for
polysemous synsets. On average, T opOne selection is capable to select correct matches for
as much as 88.2% of the monosemous synsets.
Figure 4(a) and 4(b) show, for every wordnet and every category of target synset,
the percentage of correct mappings found respectively with T opOne and T opSet selection
settings. We figured out that mappings to synsets with polysemous words, in particular to
polysemous synonymless synsets (P &OW S), are much more likely to be undecidable, i.e.,
a set of many top-voted candidate matches is found. In fact, when the target synsets are
P &OW S, the mapping is almost always undecidable with the T opOne selection.
Observation 8. Evidence provided by machine translation tools is valuable to successfully decide upon correct mappings for monosemous synsets, while it fails to support such
a decision for most of the polysemous synsets.
Observation 9. Mappings with polysemous and synonymless target synsets (P &OW S)
cannot be successfully selected by leveraging only the evidence from translations and a
simple selection strategy like majority voting because translations assign an equal number
of votes to several candidate matches.
Observation 10. If the set of top-voted candidate matches can be validated, e.g., as
in the T opSet selection settings, it is possible to find a correct mapping for a vast majority
of monosemous synsets (on average, 85%).
We want to investigate if correct mappings are more likely to be found when a larger or
a small number of top-voted mappings is selected (with T opSet selection). To this end, we
analyze the distribution of the correct mappings found with T opSet selection among topvoted candidate matches of different size for every wordnet. Correct mappings are found
194

fiEffectiveness of Automatic Translations for Cross-Lingual Ontology Mapping

40

Arabic

Correct synsets (%)

35

Italian

Slovene

Spanish

30
25

20
15
10
5

Arabic
Italian

0

TopOne

2

3

4

5

6

7

8

9

10

Number of candidate matches

Slovakian
Spanish

Figure 5: Percentage of correct mappings vs. size of top-voted candidate matches with
T opSet selection

Table 13: Synonym words coverage (%) for synonymful synsets (MWS)
Translation
BN
MT
MT&BN

Arabic
51.9
68.9
71.3

Italian
59.8
68.5
72.6

Slovene
56.7
61.5
65.4

Spanish
61.2
74.4
77.5

in sets of top-voted candidate matches with a size that ranges from 1 to 238 candidates.
The distribution is plotted in Figure 5: x-axis represents the number of selected top-voted
candidate matches (up to size equal to ten), while the y-axis represents the percentage of
found correct mappings. On average, 28% of correct mappings are found when a unique
top-voted candidate match exists, i.e., like in T opOne selection settings (see Figure 3(a)).
For instance, about 4% of the correct mappings are found in sets of top-voted mappings
that contain four candidate matches, a percentage that represents an absolute number of
317, 1455, 991, and 1328 synsets for the Arabic, Italian, Slovene, and Spanish wordnets,
respectively.
Observation 11. Synsets that occur as targets in mappings found with T opOne selection (decidable mappings) can be safely filtered out from candidate matches for other
source synsets, with an error estimated to be as low as 0.2% of removing a correct match.
Finally, we analyze the impact of synonyms on the mapping selection task. Synonymful
synsets (i.e., M &M W S, MIX, and P &M W S) are more likely to be correctly mapped with
T opOne selection (Figure 4(a)) than synonymless synsets (i.e., M &OW S and P &OW S),
even if the average number of candidate matches is greater for synonymful synsets than for
synonymless synsets (see Table 11). These results confirm that synonyms are helpful not
only for retrieving candidate matches - as previously observed in Observation 6 - but also
for selecting the correct mappings: the translation of different words that express a same
concept provide evidence to decide the best mapping for this concept.
Table 13 reports, for every wordnet, the synonym words coverage for synonymful synsets
(M W S) using the BN , M T and M T &BN dictionaries (synonym words coverage is defined
by Eq.8). The best results are obtained with M T &BN , with synonym words coverage ranging from 65.4% (Slovene) to 77.5% (Spanish). Thus, on average, more than two synonyms
are translated correctly in synonymful synsets.
195

fiAbu Helou, Palmonari, & Jarrar

50

BN

45

MT

40

MT&BN

MT

35

40

MT&BN

30

35
30

25

25

20

20

15

15

10

10

5

5

0

0

Arabic

Italian

Slovene

Arabic

Spanish

(a) Percentage of M W S synsets that fully covered
with different translation settings

Italian

Slovene

Spanish

(b) Percentage of correct mappings for M W S
synsets found with T opOne selection

Figure 6: Synonymful synsets (M W S) whose synonym words are fully covered

Figure 6(a) shows the percentage of synonymful synsets that are fully covered, i.e.,
synsets that contain only words that are correctly translated. On average, the M T dictionary fully covers a greater percentage of synonymful synsets than BN , with a gain of 18
points. The best results are obtained by M T &BN with an average gain of 6 points with
respect to M T . Although the BN dictionary has limited impact on overall synsets coverage
(with a gain of 2.4 points, as shown in Experiment 2 ), BN improves synonym words coverage by an average of 6 points, which can have a significant impact on mapping selection with
majority voting. For instance, when compared to M T , the M T &BN dictionary improves
the percentage of correct mappings in T opOne selection for the synonymful synsets that
are fully covered by 4.6 points, as shown in Figure 6(b). Covering more synonym words
belonging to a synonymful synset, not only improves synsets coverage, but also makes the
mapping selection step easier. Thus, integrating more lexical resources for translation can
be advantageous in the mapping selection tasks as well.
Observation 12. For synonymful synsets, the larger the number of synonym words
covered by translations, the easier the mapping selection task is.
6.3.3 Experiment 3: Coverage and Correctness of Translations vs.
Concept Specialization
We recall that a synset is not covered when none of the words of its equivalent synset in
the target language is returned by its translation. In other words, when a synset is not
covered, the correct match cannot be found among the set of candidate matches found by
translation. This analysis further helps in the exploration of the problem of synset coverage
by investigating 1) the impact of domain specificity on synset coverage, and 2) the possibility
of improving the coverage by expanding the set of found candidate matches with synsets
similar to the ones retrieved by translations.
To investigate if non covered synsets can be characterized to some extent based on
their specificity, we use two different methods to characterize specificity: the domain labels
associated with synsets in WordNet Domains (Bentivogli, Forner, Magnini, & Pianta, 2004),
e.g., biology, animals, and so on; the position that synsets occupy in the semantic hierarchies,
e.g., synsets that occur as leaf nodes in the hypernym hierarchies.
196

fiEffectiveness of Automatic Translations for Cross-Lingual Ontology Mapping

70
60

Percentage

50
40
30
20
10
0

Arabic

Italian

Slovene

Spanish

Figure 7: Percentage of domain specific synsets that are not covered by M T
We consider a synset which is associated with a domain label in Wordnet domains as
domain-specific, i.e., every label different from Factoum (i.e., general, or non specified
domain). For every wordnet, the percentage of domain specific synsets that are not covered
by M T dictionary is shown in Figure 7. For example, we found that, on average, only 36%
of the non covered synsets with the M T dictionary are labeled as Factoum. The rest
of the non covered synsets (64%) are distributed over different domains (with biology, animals, person, plants, and geography as the most frequent ones). These findings consolidate
the ones discussed in Experiment 1 : monosemous words, which do often express specific
concepts, were found to be less covered than polysemous words, which often express more
general concepts.
Observation 13. Domain-specific concepts have less coverage, by machine translation
tools, than general concepts.
With the same intent, we consider how synsets not covered by translations are distributed in the semantic hierarchy defined by the hypernym/hyponym relation. In this
context, we consider leaf synsets (called Lsynsets) as the most specific synsets, while intermediate synsets (called Isynsets), i.e., synsets occurring in other positions in the hierarchy,
are considered to be more generic. We consider only a subset of synsets, i.e., nominal
synsets, whose hierarchical structure is well-established in the English wordnet. In particular, to determine the position of a source synset we consider the position of its equivalent
synset in the English WordNet, by using the mappings existing between the wordnets.
Figure 8(a) reports the percentage of Lsynsets and Isynsets for every wordnet. We can
notice that most of the wordnets have more leaf synsets than intermediate synsets, with
the exception of the Arabic wordnet. This exception can be explained by the strategy used
for the construction of this wordnet and by its relatively small size. The construction of
the Arabic wordnet (Rodrguez et al., 2008), which is based on the expand model paradigm
introduced in the EuroWordNet project (Vossen, 2004), was initiated by the translation of
the core concepts of the English WordNet (Boyd-Graber, Osherson, & Schapire, 2006), and
was, thereafter, extended to other concepts. The core concepts (over 5 thousands) are often
assumed to be common across different cultures and languages, and are often intermediate
synsets.
Figure 8(b) reports the percentage of Lsynsets and Isynsets that are not covered with
M T dictionary for each wordnet. The average percentage of nominal Lsynsets and Isynsets
not covered with the M T dictionary is 21.1% and 16.6%, respectively. Table 14 reports,
for every wordnet, the distribution of nominal Lsynsets vs. Isynsets, grouped by synset
197

fiAbu Helou, Palmonari, & Jarrar

100.0

30.0
25.0

Percentage

Percentage

80.0
60.0

L synsets
I synsets

40.0
20.0

20.0

Arabic
Italian

15.0

Slovene

10.0

Spanish

5.0
0.0

0.0
English

Arabic

Italian

Slovene

L synsets

Spanish

(a)

I synsets

(b)

Figure 8: Percentage of Leaf synsets (Lsynsets) and Intermediate synsets (Isynsets) (a)
in the gold standards (b) in the non-covered synsets

30.0

Percentage

25.0
20.0
15.0
10.0

5.0
0.0
Arabic

Italian

Slovene

Spanish

Figure 9: Neighbour synset coverage for non-covered synset
category. We can notice that Lsynsets are more likely to be not covered than Isynsets,
and that a large number of non-covered synsets consists of synonymless synsets.
Moreover, we would like to evaluate if, for non covered synsets, translations return
candidate matches that are at least semantically similar to their equivalent synsets in the
target language. Neighbor synsets (i.e., hypernyms, hyponyms, and siblings) are usually
considered similar in many wordnet and graph based similarity measures (Navigli, 2009). Inspired by work presented by Resnik and Yarowsky (1999), one could consider establishing a
weighted mapping between a synset in a source language and synsets in the target language,
such that the weight represents the degree of similarity between the source and the target
synset. In our experiment we define similar synsets as one being either hyponym/hypernym
or siblings of the other one. As shown in Figure 9, the average percentage of synsets not
covered with M T for which at least one synset similar to the equivalent synset is found
among the candidate matches is 20.1%. This is consistent with the intuition that machine
translation systems provide translations which (implicitly) capture a more coarse-grained
sense specification than the fine-grained sense specification encoded in the wordnets. In
fact, it was observed that WordNet is sometimes too fine-grained even for human judges to
agree (Hovy, Marcus, Palmer, Ramshaw, & Weischede, 2006).
Observation 14. For a significant percentage of non covered synsets (20.1%, on average), machine translation tools return synsets that are at least similar to their equivalent
synsets in the target language.

198

fiEffectiveness of Automatic Translations for Cross-Lingual Ontology Mapping

Table 14: Distribution of leaf and intermediate (non-)covered synsets by category
Synsets

M-MWS
M-OWS
MIX
P-MWS
P-OWS
ALL
Synsets

M-MWS
M-OWS
MIX
P-MWS
P-OWS
ALL

Arabic
Non-covered
Covered
Leaf
Inter
Leaf
Inter
9.1
16.0
35.0
39.9
14.3
27.1
27.0
31.6
3.7
8.5
19.6
68.2
2.7
5.2
17.9
74.2
8.8
13.1
19.7
58.4
7.8
14.4
23.5
54.3
Slovene
Non-covered
Covered
Leaf
Inter
Leaf
Inter
10.9
4.7
60.0
24.3
13.2
4.6
70.4
11.9
11.3
7.1
38.1
43.5
13.0
8.3
28.1
50.6
22.3
10.2
35.3
32.2
15.2
6.7
52.4
25.7

Italian
Non-Covered
Covered
Leaf
Inter
Leaf
Inter
5.3
1.5
71.6
21.5
11.4
4.0
60.3
24.3
4.1
2.1
44.6
49.3
4.0
2.6
34.4
59.0
11.3
5.5
44.1
39.0
8.6
3.6
52.1
35.7
Spanish
Non-Covered
Covered
Leaf
Inter
Leaf
Inter
8.0
1.1
82.5
8.4
17.8
4.2
64.9
13.1
9.6
2.4
48.7
39.3
7.1
3.3
28.3
61.3
14.1
7.0
37.0
41.8
13.8
4.2
56.9
25.0

Based on this observation, the candidate match retrieval step can be modified so as to
include among the candidate matches also synsets similar to the ones retrieved by translation. This approach has been followed by several cross-lingual ontology matching systems (Fu et al., 2012; Cruz, Palmonari, Caimi, & Stroe, 2013; Faria et al., 2014). However,
expanding the set of considered candidate matches has the disadvantage of increasing the
difficulty of mapping selection task. The results of our analyses suggest that the expansion
of the candidate matches set is a technique that could be applied only to particular categories of source synsets, e.g., to synonymless leaf synsets. This could provide a system (or a
user, in an interactive matching settings) with a greater ability to map synsets that are less
likely to be covered by translations, without increasing the number of candidate matches
for every source synset, e.g., for synsets that have distinguishable monosemous candidate
matches (see Observation 8).
6.4 Lessons Learned & Future Works
In this section we summarize the main results and findings of our study and highlight some
potential future directions.
A general conclusion that we draw from our study is that machine translation tools
and multilingual knowledge resources return useful translations for a very large number of
concepts. Thus, translations provide a valuable support for candidate match retrieval in
cross-lingual ontology matching, covering from a minimum of 75.2% to a maximum of 89.9%
synsets in the four languages other than English considered in this study. If we consider that
BabelNet also incorporates translations derived from mappings in the Open Multilingual
Wordnet (Bond & Foster, 2013) (which have been excluded in our study because they
have been used as gold standards), this coverage is expected to even increase for several
resource-rich languages covered by this wordnet. In addition, our experiments suggest that
translations can be helpful, to a more limited extent, and for selected categories of synsets,
also in the mapping selection task.
Concisely, the main results of our experiments suggest that:
199

fiAbu Helou, Palmonari, & Jarrar

 monosemous concepts (i.e., concepts that have only monosemous words) are considered to be more domain-specific;
 combining lexical resources improves the quality of results;
 machine translation tools perform poorer on domain-specific concepts than on domainindependent ones;
 synonymful synsets have higher coverage than synomymless synsets;
 most, but not all, monosemous concepts can be mapped confidently even with simple
selection methods (e.g., translation-based majority voting);
 mappings involving polysemous but synonymless synsets are harder to filter out within
the mapping selection task;
 the more the coverage for synonym words (in synonymful synsets), the easier is the
mapping selection task.
Compared to previous systems, which used machine translation tools considering only
one translation direction, in our study we built dictionaries that cover both translation
directions by including reverse translations. This technique has been shown to significantly
improve the coverage of translations. In practice, candidate matches can be found for a
larger number of input concepts, thus increasing the upper-bound recall for cross-lingual
ontology matching systems. As a promising future research direction, one may try to further
improve coverage by considering additional information available in machine translation
tools like Google Translate (e.g., reverse translation synonym-like sets, part-of-speech tagged
translations, and translation scores). Such additional information can increase not only the
upper-bound recall, but also the precision, if adequately used in the matching selection step.
For example, one may compare the words returned by reverse translations with an input
source synset, e.g., by using our translation-correctness measure (Eq.5). The translation
with higher translation-correctness could be given a higher weight in the selection step.
The selection of a correct mapping from a set of candidate matches still remains a difficult
task, in particular when contextual knowledge cannot be used to disambiguate the meaning
of the concepts. However, the findings of our paper suggest several research directions that
can mitigate this problem.
On one hand, the simple baseline selection method based on majority voting used in our
experiments should be overcome by more sophisticated methods. For example, in a recent
work, we define a lexical similarity measure based on evidence collected from translations
and we run a local similarity optimization algorithm to improve the assignments between
source and target concepts (Abu Helou & Palmonari, 2015). In future work, we would
like to leverage the analysis of mapping selection difficulty as dependent on the lexical
characterisation of source and target concepts (e.g., polysemous vs. monosemous concepts,
or synonymless vs. synonymful synsets) discussed in this paper. We plan to investigate
matching algorithms that could adapt their behavior based on the category of the source
synset and its candidate matches.
On the other hand, some cross-lingual mappings may still be hard to decide upon using
a fully automatic approach. Thus, we would like to investigate in the cross-lingual ontology
200

fiEffectiveness of Automatic Translations for Cross-Lingual Ontology Mapping

matching domain, the adoption of semi-automatic matching methods. A web application
could be used to solve difficult cross-lingual matching tasks, as the one proposed to match
short service descriptions in different languages (Narducci, Palmonari, & Semeraro, 2013).
Beyond this, interactive matching processes that aggregate inputs given by a multiplicity
of users, either experts (Cruz et al., 2014) or crowd workers (Sarasua et al., 2012) seem
particularly promising in large cross-lingual matching tasks. The findings of this paper are
particularly useful for similar approaches because they can help to decide on which mappings
the user inputs are more valuable (e.g., polysemous and synonymless concepts). Overall we
plan to follow the latter research directions to use a map model to ease the construction
of a lexical-semantic ontology in the context of the Arabic Ontology Project (Abu Helou
et al., 2014), which also motivated the study presented in this paper.

7. Conclusions
In this study we have investigated the effectiveness of automatic translations derived from a
state-of-the-art machine translation tool (Google Translate) and a state-of-the-art multilingual knowledge resource (BabelNet) to support cross-lingual ontology mapping. To perform
our analysis we used four very large repositories of cross-lingual mappings, which include
mappings from wordnets in four different languages to the English WordNet. Effectiveness
of automatic translations is analyzed in terms of coverage and correctness. One key contribution of our study, besides the scale of the experiments, is the analysis of the effectiveness
of automatic translations for specific categories of synsets.
For example, we found that automatic translations achieve lower coverage for domain
specific concepts. As another example, we found that the amount of monosemous words
that are correctly translated into polysemous words in another language is not negligible:
cross-lingual ontology mapping methods that use the monosemous word heuristic may lead
to include a several wrong mappings in an alignment. At a coarse grain, our analyses
suggest that automatic translations are capable of covering a large number of word senses,
in particular when more multilingual lexical resources (e.g., Google Translate and BabelNet)
and translation strategies (i.e., the reverse translations of Google Translate) are integrated.
On the other hand, automatic translations are correct only to a limited extent, at least
when compared to translations derived from manually mapped wordnets.
The analyses discussed in this paper inspired the definition of a cross-lingual similarity
measure for lexical ontologies (Abu Helou & Palmonari, 2015). A natural subsequent step
is to further utilize the study outcomes in cross-lingual mapping systems. One promising
research direction is to define adaptive mapping methods such that different strategies are
used depending on the lexical characterization of the source concepts. For example, one
could integrate interactive mapping methods or crowdsourcing approaches to decide about
a subset of the mappings, which are estimated to be particularly difficult to map. Another
research direction that we plan to investigate is a method to estimate of concept ambiguity
in small ontologies that do not explicitly contain synonyms, e.g., by matching them against
wordnets. Such a method would help us to use adaptive cross-lingual mapping methods on
axiomatic ontologies or other lexically-poor data sources, e.g., web tables.
201

fiAbu Helou, Palmonari, & Jarrar

Acknowledgments
The authors would like to thank the anonymous reviewers for their helpful comments and
valuable suggestions, and Pikakshi Manchanda for her help in proofreading. This work
was supported in part by COMSODE project (FP7-ICT-611358) and SIERA project (FP7INCO-295006). Corresponding author: Mamoun Abu Helou, E-mail: mamoun.abuhelou@
disco.unimib.it.

References
Abu Helou, M. (2014). Towards constructing linguistic ontologies: Mapping framework and
preliminary experimental analysis. In Proceedings of the Second Doctoral Workshop
in Artificial Intelligence, Pisa, Italy. CEUR-WS.
Abu Helou, M., & Palmonari, M. (2015). Cross-lingual lexical matching with word translation and local similarity optimization. In Proceedings of the 10th International
Conference on Semantic Systems, SEMANTiCS 2015, Vienna, Austria, September.
Abu Helou, M., Palmonari, M., Jarrar, M., & Fellbaum, C. (2014). Towards building linguistic ontology via cross-language matching. In Proceedings of the 7th International
Conference on Global WordNet.
AGROVOC (2014).
Multilingual agricultural thesaurus.
vest-registry/vocabularies/agrovoc.

http://aims.fao.org/

Al-Kabi, M. N., Hailat, T. M., Al-Shawakfa, E. M., & Alsmadi, I. M. (2013). Evaluating English to Arabic Machine Translation Using BLEU. International Journal of Advanced
Computer Science and Applications(IJACSA), 4 (1).
Apertium (2015). Open-source machine translation platform. http://www.apertium.org.
Apidianaki, M. (2009). Data-driven semantic analysis for multilingual wsd and lexical selection in translation. In Proceedings of the 12th Conference of the European Chapter of
the Association for Computational Linguistics (ACL), EACL 09, pp. 7785, Stroudsburg, PA, USA. Association for Computational Linguistics (ACL).
BabelNet (2012). Very large multilingual semantic network. http://babelnet.org.
Bentivogli, L., Forner, P., Magnini, B., & Pianta, E. (2004). Revising the wordnet domains
hierarchy: Semantics, coverage and balancing. In Proceedings of the Workshop on
Multilingual Linguistic Ressources, MLR 04, pp. 101108, Stroudsburg, PA, USA.
Association for Computational Linguistics (ACL).
Bing (2016). Bing translate. http://www.bing.com/translator.
Birzeit (2011). Arabic Ontology. http://sina.birzeit.edu/ArabicOntology/.
Bond, F., & Foster, R. (2013). Linking and extending an open multilingual wordnet. In
ACL (1), pp. 13521362. The Association for Computer Linguistics (ACL).
Bouma, G. (2010). Cross-lingual ontology alignment using eurowordnet and wikipedia. In
LREC. European Language Resources Association (ACL).
202

fiEffectiveness of Automatic Translations for Cross-Lingual Ontology Mapping

Boyd-Graber, J., Osherson, & Schapire, R. (2006). Adding dense, weighted connections
to WordNet connections to wordnet. In Proceedings of the Third Global WordNet
Meeting.
Carruthers, P. (2002). The cognitive functions of language. Behavioral and Brain Sciences,
25 (6), 657674.
CAT (2014).
Chinese Agricultural Thesaurus.
http://www.ciard.net/partners/
labof-chinese-agricultural-ontology-services.
COMSODE (2015). Components supporting the open data exploitation. http://www.
comsode.eu/.
Costa-jussa, M. R., Farrus, M., Marino, J. B., & Fonollosa, J. A. R. (2012). Study and
comparison of rule-based and statistical catalan-spanish machine translation systems.
Computing and informatics, 31 (2), 245270.
Cruz, I. F., Palmonari, M., Caimi, F., & Stroe, C. (2013). Building linked ontologies with
high precision using subclass mapping discovery. Artif. Intell. Rev., 40 (2), 127145.
Cruz, I., Loprete, F., Palmonari, M., Stroe, C., & Taheri, A. (2014). Pay-as-you-go multiuser feedback model for ontology matching. In Knowledge Engineering and Knowledge
Management, Vol. 8876 of Lecture Notes in Computer Science, pp. 8096. Springer.
De Melo, G., & Weikum, G. (2009). Towards a universal wordnet by learning from combined
evidence. In Proceedings of the 18th ACM Conference on Information and Knowledge
Management, CIKM 2009, Hong Kong, China, November, pp. 513522. ACM.
De Melo, G., & Weikum, G. (2012). Constructing and utilizing wordnets using statistical
methods. Language Resources and Evaluation, 46 (2), 287311.
Djeddi, W. E., & Khadir, M. T. (2014). Xmap++: Results for oaei 2014. In Proceedings
of the 9th International Workshop on Ontology Matching co-located with the 13th
International Semantic Web Conference (ISWC 2014), October, Vol. 20, pp. 163169.
Els, L., & Veronique, H. (2010). Semeval-2010 task 3: Cross-lingual word sense disambiguation. In Proceedings of the 5th International Workshop on Semantic Evaluation.
Association for Computational Linguistics (ACL).
EUROVOC (2015). EUs multilingual thesaurus. http://europa.eu/eurovoc.
Faria, D., Martins, C., Nanavaty, A., Taheri, A., Pesquita, C., Santos, E., Cruz, I. F., &
Couto, F. M. (2014). Agreementmakerlight results for oaei 2014. In Proceedings
of the 9th International Workshop on Ontology Matching co-located with the 13th
International Semantic Web Conference (ISWC 2014), October, Vol. 20, pp. 126134.
Fellbaum, C. (Ed.). (1998). WordNet An Electronic Lexical Database. The MIT Press,
Cambridge, MA; London.
Fiser, D. (2007). Leveraging parallel corpora and existing wordnets for automatic construction of the slovene wordnet. In LTC, Vol. 5603 of Lecture Notes in Computer Science,
pp. 359368. Springer.
Fodor, J. (1975). The Language of Thought. Cambridge, MA: Harvard University Press.
203

fiAbu Helou, Palmonari, & Jarrar

Fu, B., Brennan, R., & OSullivan, D. (2009). Cross-lingual ontology mapping - an investigation of the impact of machine translation. In ASWC, pp. 115.
Fu, B., Brennan, R., & OSullivan, D. (2012). A configurable translation-based cross-lingual
ontology mapping system to adjust mapping outcomes. J. Web Semantic, 15, 1536.
Gale, W. A., Church, K. W., & Yarowsky, D. (1992). Using bilingual materials to develop
word sense disambiguation methods. In In Proceedings of the International Conference
on Theoretical and Methodological Issues in Machine Translation.
Gonzalez-Agirre, A., Laparra, E., & Rigau, G. (2012). Multilingual central repository
version 3.0. In LREC, pp. 25252529. European Language Resources Association
(ELRA).
Google (2015). Google Translate. https://translate.google.com/.
Gracia, J., Montiel-Ponsoda, E., Cimiano, P., Gomez-Perez, A., Buitelaar, P., & McCrae,
J. (2012). Challenges for the multilingual web of data. Web Semantic, 11, 6371.
Hertling, S., & Paulheim, H. (2012). Wikimatch - using wikipedia for ontology matching.
In Ontology Matching, Vol. 946 of CEUR Workshop Proceedings. CEUR-WS.org.
Hirst, G. (2004). Ontology and the lexicon. In Handbook on Ontologies and Information
Systems. Springer.
Horrocks, I. (2008). Ontologies and the semantic web. Communications of the ACM, 51 (12),
5867.
Hovy, E., Marcus, M., Palmer, M., Ramshaw, L., & Weischede, R. (2006). Ontonotes:
the 90% solution. In Proceedings of the Human Language Technology Conference of
the NAACL. Companion Volume, Short Papers on XX, NAACL 06. Association for
Computational Linguistics (ACL), Morristown, NJ, USA.
Hovy, E., Navigli, R., & Ponzetto, S. P. (2012). Collaboratively built semi-structured content
and artificial intelligence: The story so far. Artificial Intelligence.
Ide, N., Erjavec, T., & Tufis, D. (2002). Sense discrimination with parallel corpora. In Proceedings of the ACL-02 Workshop on Word Sense Disambiguation: Recent Successes
and Future Directions - Volume 8, WSD 02, pp. 6166, Stroudsburg, PA, USA. Association for Computational Linguistics (ACL).
Jarrar, M. (2006). Position paper: Towards the notion of gloss, and the adoption of linguistic
resources in formal ontology engineering. In Proceedings of the 15th International
Conference on World Wide Web, pp. 497503, NY, USA. ACM.
Jarrar, M. (2011). Building a formal arabic ontology (invited paper). In Proceedings of the
Experts Meeting On Arabic Ontologies And Semantic Networks. Alecso, Arab League.
26-28 July.Tunis.
Jarrar, M., Yahya, A., Salhi, A., Abu Helou, M., Sayrafi, B., Arar, M., Daher, J., Hicks, A.,
Fellbaum, C., Bortoli, S., Bouquet, P., Costa, R., Roche, C., & Palmonari, M. (2014).
Arabization and multilingual knowledge sharing- final report on research setup. In
SIERA Project 2.3 Deliverable.
204

fiEffectiveness of Automatic Translations for Cross-Lingual Ontology Mapping

Jimenez-Ruiz, E., Grau, B. C., Xia, W., Solimando, A., Chen, X., Cross, V., Gong, Y.,
Zhang, S., & Chennai-Thiagarajan, A. (2014). Logmap family results for oaei 2014?.
In Proceedings of the 9th International Workshop on Ontology Matching co-located
with the 13th International Semantic Web Conference (ISWC 2014), October, Vol. 20,
pp. 126134.
Jimenez-Ruiz, E., & Grau, B. C. (2011). Logmap: Logic-based and scalable ontology matching. In The Semantic WebISWC 2011, pp. 273288. Springer.
Json-W3schools (2015). Javascript object notation. http://www.w3schools.com/json/.
Lefever, E., & Hoste, V. (2013). Semeval-2013 task 10: Cross-lingual word sense disambiguation. Proc. of SemEval, 158166.
Liang, A. C., & Sini, M. (2006). Mapping agrovoc and the chinese agricultural thesaurus:
Definitions, tools, procedures. The New Review of Hypermedia and Multimedia, 12 (1),
5162.
Lin, F., Butters, J., Sandkuhl, K., & Ciravegna, F. (2010). Context-based ontology matching: Concept and application cases. In 10th IEEE International Conference on Computer and Information Technology, CIT 2010, Bradford, West Yorkshire, UK, June
29-July 1, 2010, pp. 12921298.
Lin, F., & Krizhanovsky, A. (2011). Multilingual ontology matching based on wiktionary
data accessible via sparql endpoint. CoRR, abs/1109.0732.
LOGD (2015).
Linking Open Government Data.
http://logd.tw.rpi.edu/
iogds-data-analytics. [Online; accessed March-2015].
Margolis, E., & Laurence, S. (2014). Concepts. In Zalta, E. N. (Ed.), The Stanford Encyclopedia of Philosophy (Spring edition).
Marioo, J. B., Banchs, R. E., Crego, J. M., de Gispert, A., Lambert, P., Fonollosa, J. A. R.,
& Costa-jussa, M. R. (2006). N-gram-based machine translation. Comput. Linguist.,
32 (4), 527549.
McCarley, J. S. (1999). Should we translate the documents or the queries in cross-language
information retrieval?. In Proceedings of the 37th Annual Meeting of the Association
for Computational Linguistics on Computational Linguistics (ACL), ACL 99, pp.
208214, Stroudsburg, PA, USA. Association for Computational Linguistics (ACL).
McCarthy, D., & Navigli, R. (2009). The english lexical substitution task. Language Resources and Evaluation, 43 (2), 139159.
MCR (2012). Multilingual Central Repository. http://adimen.si.ehu.es/web/MCR.
Meilicke, C., Garca-Castro, R., Freitas, F., van Hage, W. R., Montiel-Ponsoda, E.,
de Azevedo, R. R., Stuckenschmidt, H., Svab-Zamazal, O., Svatek, V., Tamilin, A.,
Trojahn, C., & Wang, S. (2012). Multifarm: A benchmark for multilingual ontology
matching. Web Semantics: Science, Services and Agents on the World Wide Web,
15 (3).
Miller, G. A. (1995). Wordnet: A lexical database for english. Commun. ACM, 38 (11),
3941.
205

fiAbu Helou, Palmonari, & Jarrar

Miller, G. A., Leacock, C., Tengi, R., & Bunker, R. T. (1993). A semantic concordance. In
Proceedings of the workshop on Human Language Technology, HLT 93, pp. 303308,
Stroudsburg, PA, USA. Association for Computational Linguistics (ACL).
Mulwad, V., Finin, T., & Joshi, A. (2013). Semantic message passing for generating linked
data from tables. In International Semantic Web Conference (1), Vol. 8218 of Lecture
Notes in Computer Science, pp. 363378. Springer.
Narducci, F., Palmonari, M., & Semeraro, G. (2013). Cross-language semantic matching
for discovering links to e-gov services in the lod cloud. In KNOW@LOD, Vol. 992 of
CEUR Workshop Proceedings, pp. 2132. CEUR-WS.org.
Nastase, V., Strube, M., Boerschinger, B., Zirn, C., & Elghafari, A. (2010). Wikinet: A very
large scale multi-lingual concept network. In LREC. European Language Resources
Association (ACL).
Navigli, R. (2009). Word sense disambiguation: A survey. ACM Comput. Surv., 41 (2),
10:110:69.
Navigli, R., & Ponzetto, S. P. (2012). Babelnet: The automatic construction, evaluation and
application of a wide-coverage multilingual semantic network. Artificial Intelligence,
193 (0), 217  250.
OAEI (2015).
The ontology alignment evaluation initiative.
ontologymatching.org.

http://oaei.

Oliver, A., & Climent, S. (2012). Building wordnets by machine translation of sense tagged
corpora. In Proceedings of the 6th International Conference on Global WordNet.
Omegawiki (2015). Wikimedia. http://www.omegawiki.org.
OMWN (2015). The open multilingual wordnet. http://compling.hss.ntu.edu.sg/omw/.
Otero-Cerdeira, L., Rodrguez-Martnez, F. J., & Gmez-Rodrguez, A. (2015). Ontology
matching: A literature review. Expert Systems with Applications, 42 (2), 949  971.
OWL (2004). Web ontology language. http://www.w3.org/TR/owl-features/.
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). Bleu: A method for automatic
evaluation of machine translation. In Proceedings of the 40th Annual Meeting on
Association for Computational Linguistics (ACL), ACL 02, pp. 311318, Stroudsburg,
PA, USA. Association for Computational Linguistics (ACL).
Pianta, E., Bentivogli, L., & Girardi, C. (2002). Multiwordnet: developing an aligned multilingual database. In Proceedings of the First International Conference on Global
WordNet.
Pinker, S. (1994). The Language Instinct. Harper Perennial Modern Classics, New York.
Po, L., & Sorrentino, S. (2011). Automatic generation of probabilistic relationships for
improving schema matching. Information Systems, 36 (2), 192  208. Special Issue:
Semantic Integration of Data, Multimedia, and Services.
RDF (2014). Resource Description Framework. http://www.w3.org/RDF/.
RDFS (2014). RDF Schema. http://www.w3.org/TR/rdf-schema.
206

fiEffectiveness of Automatic Translations for Cross-Lingual Ontology Mapping

Resnik, P., & Yarowsky, D. (1999). Distinguishing systems and distinguishing senses: new
evaluation methods for word sense disambiguation. Natural Language Engineering,
5 (2), 113133.
Rodrguez, H., Farwell, D., Farreres, J., Bertran, M., Mart, M. A., Black, W., Elkateb, S.,
Kirk, J., Vossen, P., & Fellbaum, C. (2008). Arabic wordnet: Current state and future
extensions. In Proceedings of the Forth International Conference on Global WordNet.
Sag, I., Baldwin, T., Bond, F., Copestake, A., & Flickinger, D. (2002). Multiword expressions: A pain in the neck for nlp. In Computational Linguistics and Intelligent Text
Processing, Vol. 2276 of Lecture Notes in Computer Science, pp. 115. Springer.
Sagot, B., & Fiser, D. (2008). Building a free french wordnet from multilingual resources. In
Proceedings of the Sixth International Language Resources and Evaluation (LREC),
Marrakech, Maroc.
Sarasua, C., Simperl, E., & Noy, N. (2012). Crowdmap: Crowdsourcing ontology alignment
with microtasks. In The Semantic Web ISWC 2012, Vol. 7649 of Lecture Notes in
Computer Science, pp. 525541. Springer.
Saveski, M., & Trajkovski, I. (2010). Automatic construction of wordnets by using machine
translation and language modeling. In In Proceedings of Seventh Language Technologies Conference, 13th International Multiconference Information Society, volume C.
Shvaiko, P., & Euzenat, J. (2013). Ontology matching: State of the art and future challenges.
IEEE Trans. Knowl. Data Eng., 25 (1), 158176.
Shvaiko, P., Euzenat, J., Mao, M., Jimenez-Ruiz, E., Li, J., & Ngonga, A. (Eds.). (2014).
Proceedings of the 9th International Workshop on Ontology Matching collocated with
the 13th International Semantic Web Conference (ISWC 2014), Riva del Garda,
Trentino, Italy, October 20, 2014, Vol. 1317 of CEUR Workshop Proceedings. CEURWS.org.
Sorrentino, S., Bergamaschi, S., Gawinecki, M., & Po, L. (2010). Schema label normalization
for improving schema matching. Data & Knowledge Engineering, 69 (12), 1254  1273.
Special issue on 28th International Conference on Conceptual Modeling (ER 2009).
Spelke, S. E. (2003). What makes us smart? core knowledge and natural language. In
Language in Mind: Advances in the Study of Language and Thought, pp. 277311.
Mit Press.
Spohr, D., Hollink, L., & Cimiano, P. (2011). A machine learning approach to multilingual and cross-lingual ontology matching. In Proceedings of the 10th International
Conference on The Semantic Web - Volume Part I, ISWC11, pp. 665680. Springer.
Suchanek, F. M., Kasneci, G., & Weikum, G. (2008). Yago: A large ontology from wikipedia
and wordnet. Web Semantics: Science, Services and Agents on the World Wide Web,
6 (3), 203217.
Tomaz, E., & Fiser, D. (2006). Building slovene wordnet. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC06), Genoa,
Italy.
207

fiAbu Helou, Palmonari, & Jarrar

Trojahn, C., Fu, B., Zamazal, O., & Ritze, D. (2014). State-of-the-art in multilingual and
cross-lingual ontology matching. In Towards the Multilingual Semantic Web, pp. 119
135. Springer.
Tufis, D., Cristea, D., & Stamou, S. (2004). Balkanet: Aims, methods, results and perspectives. a general overview. Special Issue on BalkaNet. Romanian Journal on Science
and Technology of Information.
Vincent, V. A. (2013). Macro- and micro-averaged evaluation measures [[basic draft]]. In
Technical report.
Vossen, P. (2004). Eurowordnet: A multilingual database of autonomous and languagespecific wordnets connected via an inter-lingualindex. International Journal of Lexicography, 17 (2), 161173.
Vossen, P., Eneko, A., Francis, B., Wauter, B., Axel, H., Amanda, H., Shu-Kai, H., Hitoshi,
I., Chu-Ren, H., Kyoko, K., Andrea, M., German, R., Francesco, R., Roxane, S., &
Maurizio, T. (2010). KYOTO: A Wiki for Establishing Semantic Interoperability for
Knowledge Sharing Across Languages and Cultures, pp. 265294. Springer.
Wikidata (2015). Wikimedia. http://www.wikidata.org.
Wikipedia (2015a). List of languages by number of native speakers. http://en.wikipedia.
org/wiki/List-of-languages-by-number-of-native-speakers.
Wikipedia (2015b). Wikipedia. https://www.wikipedia.org.
Wikispecies (2015). Wikimedia. https://species.wikimedia.org.
Wiktionary (2015). Wikimedia. https://www.wiktionary.org.
WordNet-Princeton (2015). Semantically tagged glosses. http://wordnet.princeton.
edu/glosstag.shtml.
Zhang, Z. (2014). Towards efficient and effective semantic table interpretation. In The
Semantic Web - ISWC 2014 - 13th International Semantic Web Conference, Riva del
Garda, Italy, October 19-23, 2014. Proceedings, Part I, pp. 487502.
Zhuge, H., Xing, Y., & Shi, P. (2008). Resource space model, owl and database: Mapping
and integration. ACM Transactions on Internet Technology (TOIT), 8 (4), 20.

208

fiJournal of Artificial Intelligence Research 55 (2016) 389-408

Submitted 07/15; published 02/16

Predicting Twitter User Demographics using Distant
Supervision from Website Traffic Data
Aron Culotta
Nirmal Kumar Ravi

aculotta@iit.edu
nravi@hawk.iit.edu

Department of Computer Science, Illinois Institute of Technology
Chicago, IL 60616

Jennifer Cutler

jcutler2@stuart.iit.edu

Stuart School of Business, Illinois Institute of Technology
Chicago, IL 60616

Abstract
Understanding the demographics of users of online social networks has important applications for health, marketing, and public messaging. Whereas most prior approaches
rely on a supervised learning approach, in which individual users are labeled with demographics for training, we instead create a distantly labeled dataset by collecting audience
measurement data for 1,500 websites (e.g., 50% of visitors to gizmodo.com are estimated
to have a bachelors degree). We then fit a regression model to predict these demographics
from information about the followers of each website on Twitter. Using patterns derived
both from textual content and the social network of each user, our final model produces an
average held-out correlation of .77 across seven different variables (age, gender, education,
ethnicity, income, parental status, and political preference). We then apply this model
to classify individual Twitter users by ethnicity, gender, and political preference, finding
performance that is surprisingly competitive with a fully supervised approach.

1. Introduction
Social media are increasingly being used to make inferences about the real world, with application to politics (OConnor, Balasubramanyan, Routledge, & Smith, 2010), health (Dredze,
2012), and marketing (Gopinath, Thomas, & Krishnamurthi, 2014). Understanding the demographic makeup of a sample of social media users is critical to further progress in this
area, as it allows researchers to overcome the considerable selection bias in this uncontrolled
data. Additionally, this capability will help public messaging campaigns ensure that the
target demographic is being reached.
A common approach to demographic inference is supervised classification  from a
training set of annotated users, a model is fit to predict user attributes from the content of
their writings (Argamon, Dhawle, Koppel, & Pennebaker, 2005; Schler, Koppel, Argamon,
& Pennebaker, 2006; Rao, Yarowsky, Shreevats, & Gupta, 2010; Pennacchiotti & Popescu,
2011; Burger, Henderson, Kim, & Zarrella, 2011; Rao, Paul, Fink, Yarowsky, Oates, &
Coppersmith, 2011; Al Zamal, Liu, & Ruths, 2012). This approach has a number of limitations: collecting human annotations is costly and error-prone; many demographic variables
of interest cannot easily be labeled by inspecting a profile (e.g., income, education level); by
restricting learning to a small set of labeled profiles, the generalizability of the classifier is
c
2016
AI Access Foundation. All rights reserved.

fiCulotta, Ravi, & Cutler

limited. Additionally, most past work has focused on text as the primary source of evidence,
making little use of network evidence.
In this paper, we fit regression models to predict seven demographic variables of Twitter
users (age, gender, education, ethnicity, income, parental status, and political preference)
based both on whom they follow and on the content of their tweets. Rather than using a
standard supervised approach, we construct a distantly labeled dataset consisting of web
traffic demographic data from Quantcast.com. By pairing web traffic demographics of a
site with the followers of that site on Twitter.com, we fit a regression model between a set
of Twitter users and their expected demographic profile. We then evaluate accuracy both
in recovering the Quantcast statistics as well as in classifying individual Twitter users. Our
experiments investigate several research questions:
RQ1. Can the demographics of a set of Twitter users be inferred from network
information alone? We find across seven demographic variables an average heldout correlation of .73 between the web traffic demographics of a website and that
predicted by a regression model based on the sites Twitter followers. We can learn,
for example, that high-income users are likely to follow The Economist and young
users are likely to follow PlayStation.
RQ2. Which is more revealing of demographics: social network features or linguistic features? Overall, we find text-based features to be slightly more predictive
than social network features (.79 vs. .73), particularly for income and age variables.
RQ3. Can a regression model be extended to classify individual users? Using a
hand-labeled validation set of users annotated with gender, ethnicity, and political
preference, we find that a distantly trained regression model provides classification
accuracy competitive with a fully-supervised approach. Averaged across the three
classification tasks, both approaches obtain an F1 score of 81%.
RQ4. How much follower and linguistic information is needed for prediction?
We find that the identities of only 10 friends per user, chosen at random, is sufficient
to achieve 90% of the accuracy obtained using 200 friends. Accuracy using linguistic
features begins to plateau after 2,000 unique terms are observed per user.
In the remainder of the paper, we first review related work, then describe the data
collected from Twitter and QuantCast and the feature representation used for the task; next,
we present regression and classification results; finally, we conclude and outline directions
for future work.1

2. Related Work
Predicting attributes of social media users is a growing area of interest, with recent work
focusing on age (Schler et al., 2006; Rosenthal & McKeown, 2011; Nguyen, Smith, & Ros,
2011; Al Zamal et al., 2012), gender (Rao et al., 2010; Burger et al., 2011; Liu & Ruths,
1. Replication code and data are available here: https://github.com/tapilab/jair-2016-demographics.

390

fiPredicting Twitter User Demographics

2013), race/ethnicity (Pennacchiotti & Popescu, 2011; Rao et al., 2011), personality (Argamon et al., 2005; Schwartz, Eichstaedt, Kern, Dziurzynski, Ramones, Agrawal, Shah, Kosinski, Stillwell, Seligman, et al., 2013), political affiliation (Conover, Goncalves, Ratkiewicz,
Flammini, & Menczer, 2011; Barbera, 2013; Volkova & Van Durme, 2015), and occupation (Preotiuc-Pietro, Lampos, & Aletras, 2015). Other work predicts demographics from
web browsing histories (Goel, Hofman, & Sirer, 2012). The majority of these approaches
rely on hand-annotated training data, require explicit self-identification by the user, or are
limited to very coarse attribute values (e.g., above or below 25-years-old).
Distantly supervised learning (also called lightly or weakly supervised learning) provides an alternative to standard supervised learning  it relies less on individual annotated
examples, instead bootstrapping models from declarative constraints. Previous work has
developed methods to train classifiers from prior knowledge of label proportions (Jin & Liu,
2005; Musicant, Christensen, & Olson, 2007; Quadrianto, Petterson, & Smola, 2009a; Liang,
Jordan, & Klein, 2009; Ganchev, Graca, Gillenwater, & Taskar, 2010; Mann & McCallum,
2010; Zhu, Chen, & Xing, 2014) or prior knowledge of features-label associations (Schapire,
Rochery, Rahim, & Gupta, 2002; Druck, Mann, & McCallum, 2008; Melville, Gryc, &
Lawrence, 2009). In addition to standard document categorization tasks, lightly supervised approaches have been applied to named-entity recognition (Mann & McCallum, 2010;
Ganchev & Das, 2013; Wang & Manning, 2014), dependency parsing (Druck, Mann, &
McCallum, 2009; Ganchev, Gillenwater, & Taskar, 2009), language identification (King &
Abney, 2013), and sentiment analysis (Melville et al., 2009).
Chang, Rosenn, Backstrom, and Marlow (2010) propose a related distantly supervised
approach to demographic inference, inferring user-level ethnicity using name/ethnicity distributions provided by the U.S. Census; however, that approach uses evidence from first and
last names, which are often not available, and thus are more appropriate for populationlevel estimates. Oktay, Firat, and Ertem (2014) extend the work of Chang et al. (2010)
to also include statistics over first names. Rao et al. (2011) take a similar approach, also
including evidence from other linguistic features to infer gender and ethnicity of Facebook
users; they evaluate on the fine-grained ethnicity classes of Nigeria and use very limited
training data. More recently, Mohammady and Culotta (2014) trained an ethnicity model
for Twitter using county-level supervision, which, like our approach, uses a distant source
of supervision to build a model of individual demographics.
There have been several studies predicting population-level statistics from social media.
Eisenstein, Smith, and Xing (2011) use geolocated tweets to predict zip-code statistics of
race/ethnicity, income, and other variables using Census data; Schwartz et al. (2013) and
Culotta (2014) similarly predict county health statistics from Twitter. However, none of
this prior work attempts to predict or evaluate at the user level.
The primary methodological novelties of the present work are its use of web traffic data
as a form of weak supervision and its use of follower information as the primary source
of evidence. Additionally, this work considers a larger set of demographic variables than
prior work, and predicts a much more fine-grained set of categories (e.g., six different age
brackets instead of two or three used previously).
This paper extends our initial version of this work (Culotta, Kumar, & Cutler, 2015).
The novel contributions here include the following: (1) we have added an additional demographic variable (political preference); (2) we have added an additional labeled dataset
391

fiCulotta, Ravi, & Cutler

for evaluation (also for political preference); (3) whereas the initial version only used friend
features, here have introduced textual features from over 9M tweets; (4) we have included
a new analysis of how accuracy varies with the number of terms collected per user. We
have also reproduced our prior results using newly collected data from QuantCast.com;
since QuantCast demographic statistics have changed over time, the overall correlations
reported here deviate slightly from those reported in the original version, but the trends
and qualitative conclusions remain the same.

3. Data
In this section we describe the various data collected for our experiments.
3.1 Quantcast
Quantcast.com is an audience measurement company that tracks the demographics of visitors to millions of websites. This is accomplished in part by using cookies to track the
browsing activity of a large panel of respondents (Kamerer, 2013). As of this writing, the
estimated demographics of a large number of websites is publicly accessible through their
searchable web interface.
We sampled 1,532 websites from Quantcast and downloaded statistics for seven demographic variables:
 Gender: Male, Female
 Age: 18-24, 25-34, 35-44, 45-54, 55-64, 65+
 Income: $0-50k, $50-100k, $100-150k, $150k+
 Education: No College, College, Grad School
 Children: Kids, No Kids
 Ethnicity: Caucasian, Hispanic, African American, Asian
 Political Preference: Democrat, Republican
For each variable, Quantcast reports the estimated percentage of visitors to a website with
a given demographic.
3.2 Twitter
For each website collected in the previous step, we executed a script to search for its Twitter
account, then manually verified it; 1,066 accounts from the original set of 1,532 were found.
An assumption of this work is that the demographic profiles of followers of a website on
Twitter are correlated with the demographic profiles of visitors to that website. While there
are undoubtedly biases introduced here (e.g., Twitter users may skew younger than the web
traffic panel), in aggregate these differences should have limited impact on the final model.
We represent each of the 1,066 Twitter accounts with feature vectors derived from
information about their followers. Below, we describe features based both on the social
network and on the linguistic content of each followers tweets.
392

fiPredicting Twitter User Demographics

h+p://www.lifehacker.com*
73%*Male*
19%*Graduate*School*
30%*$50A100k*
12%*Hispanic*

lifehacker*

A*

B*

lifehackers*Neighbor*Vector*
C*
D*
2*/*2*=*1.0*

1*/*2*=*0.5*

C*

D*
Neighbors*

Figure 1: Data model. We collect QuantCast demographic data for each website, then
construct a Neighbor Vector from the Twitter connections of that website,
based on the proportion of the websites followers that are friends with each
neighbor.

3.2.1 Friend Features
Recall that Twitter users are allowed to follow other accounts, which introduces an asymmetric relationship between users. If X follows Y , we say that Y is a friend of X (though
the reverse may not be true). For each account, we queried the Twitter REST API to sample 300 of its followers, using the followers/ids request. This sample is not necessarily
uniform. The Twitter API documentation states that At this time, results are ordered
with the most recent following first  however, this ordering is subject to unannounced
change and eventual consistency issues.
For each of these followers, we then collected up to 5,000 of the accounts they follow,
called friends, using the friends/ids API request. Thus, for each of the original accounts
from Quantcast, we have up to (300  5K = 1.5M ) additional accounts that are two hops
from the original account (the friend of a follower). We refer to these discovered accounts
as neighbors of the original Quantcast account. Of course, many of these accounts will be
duplicates, as two different followers will follow many of the same accounts (i.e., triadic
closure)  indeed, our core assumption is that the number of such duplicates represents
the strength of the similarity between the neighbors.
393

fiCulotta, Ravi, & Cutler

105

number of unique neighbors

107

number of neighbor links

count

count

106
104

105
104

103 0
10

101

rank

102

103

103 0
10

101

rank

102

103

Figure 2: Rank-order frequency plots of the number of neighbors per account and the number of links to all neighbors per account.

For each of the original accounts, we compute the fraction of its followers that are friends
with each of its neighbors and store this in a neighbor vector. Figure 1 shows an example.
Suppose a Quantcast account LifeHacker has two followers A and B; and that A follows
C, and B follows C and D. Then the neighbor vector for LifeHacker is {(C, 1), (D, .5)}.
This suggests that LifeHacker has a stronger relationship with C than D.2 For example, in
our data the top neighbors of LifeHacker are EA (the video game developer), GTASeries (a
video game fan site), and PlayStation.
The resulting dataset consists of 1.7M unique neighbors of the original 1,066 accounts.
To reduce dimensionality, we removed neighbors with fewer than 100 followers, leaving
46,649 unique neighbors with a total of 178M incoming links. Figure 2 plots the number of
unique neighbors per account as well as the number of neighbor links per account.
3.2.2 Text Features
In addition to the neighbor vector, we created an analogous vector based on the tweets of
the followers of each account. We collected the most recent 200 tweets for each of the 300
followers of each of the 1,066 accounts using the statuses/user timeline API request.
For each tweet, we perform standard tokenization, removing non-internal punctuation,
converting to lower case, and maintaining hashtag and mentions. URLs are collapsed to a
single feature type, as are digits (e.g., 12 is mapped to 99; 123 is mapped to 999).
Characters repeated more than twice are converted to a single occurrence. Terms used by
fewer than 20 different users are removed.
The resulting dataset consists of 9,427,489 tweets containing 112,642 unique terms written by 59,431 users. For each of the original 1,066 accounts, we create a text vector similar
to the previous section. Each value represents the proportion of followers of the account
who use that term. E.g., xij = .1 indicates that 10% of the 300 followers of account i use
term j.
2. Note that we use friends of A rather than followers, since friend links are created by A and are thus more
likely to indicate As interests.

394

fiPredicting Twitter User Demographics

4. Analysis
In this section, we report results predicting demographics both at the aggregate level (i.e.,
the demographics of an accounts followers) and at the user level (i.e., an individual Twitter
users demographic profile).
4.1 Regression
For each Quantcast site, we pair its demographic variables with its friend and text feature
vectors to construct a regression problem. Thus, we attempt to predict the demographic
profile of the followers of a Twitter account based on the friends of those followers and the
content of their tweets.
Due to the high dimensionality (46,649 friend features and 112,642 text features) and
small number of examples (1,066), we use elastic net regularization (Zou & Hastie, 2005),
which combines both L1 and L2 penalties. Furthermore, since each output variable consists
of dependent categories (e.g., age brackets), we use a multi-task variant of elastic net to
ensure that the same features are selected by the L1 regularizer for each category. We use
the implementation of MultiTaskElasticNet in scikit-learn (Pedregosa et al., 2011).
Recall that standard linear regression selects coefficients  to minimize the squared error
on a list of training instances {xi , yi }N
i=1 , for feature vector xi and expected output yi .
   argmin

N
1 X
(yi   T xi )2
N i=1

Lasso imposes an L1 regularizer on , while ridge regression imposes an L2 regularizer
on . Elastic net combines both penalties:
   argmin

N
1 X
(yi   T xi )2 + 1 ||||1 + 2 ||||22
N i=1

where 1 and 2 control the strength of L1 and L2 regularizers, respectively. The L1
regularizer encourages sparsity (i.e., many 0 values in ), while the L2 regularizer prevents
 values from becoming too large.
Multi-task elastic net extends standard elastic net to groups of related regression problems (Obozinski & Taskar, 2006). E.g., in our case, we would like to account for the fact
that the regressions for No College, College, and Grad School are related; thus, we
would like the sparse solutions to be similar across tasks (that is, L1 should select the same
features for each task).
(1)
(M )
Let  (j) be the coefficients for task j, and let k = (k . . . k )T be the vector of
coefficients formed by concatenating the coefficients for the kth feature across all M tasks.
Then multi-task elastic net objective enforces that similar features are selected across tasks:
   argmin

Nj
M
X
1 X

N
j=1 j

(j)

(j)

(yi   (j)T xi )2 +

i=1

1

p
X
k=1

395

||k ||1 + 2 ||||22

fiCulotta, Ravi, & Cutler

Democrat
60

Politics

r =0.84

r =0.85

40

40

Predicted Value (%)

20 40 60

10 20 30 40 50

18-24

20
10
10 20 30 40
Caucasian
r =0.86

20 40 60 80

0

20 40 60 80
35-44

20

Age

r =0.61

20

Hispanic

African American
r =0.89

20 40

60
40
20
0

45-54

55-64

15
10
5
0

r =0.68

0

0 10 20 30 40

True Value (%)

r =0.74

20

10

10
10

r =0.78

r =0.69

15
10
5
0
0

0

10 20

50

20

40

r =0.64

30 40 50 60

r =0.76

10 20 30 40
No Kids

70
60
50
40
30

r =0.72

20 40 60

College

60

$150k+

Family

Education

40
20 40 60

20

65+

No College
60

30

20

0 5 10 15 20

Asian

20
10

20 40 60

r =0.77

10 20

Ethnicity

r =0.79

20 30 40

r =0.77

$100-150k

r =0.69

20 40 60 80

10
10 20 30 40

40

20

20

40

Income

$50-100k

30

40

25
20
15

40
30
20
10

80
60
40
20

r =0.80

60

r =0.75

40
30
20
10

$0-50k

r =0.91

100

25-34

r =0.81

30

Male

50

20

20

Gender

Republican

Grad School
40
30
20
10

r =0.78

10 20 30 40 50

Figure 3: Scatter plots of the true demographic variables from Quantcast versus those predicted using elastic net regression fit to friend and text features from Twitter.
The predictions are computed using five fold cross-validation; each panel also
reports the held-out correlation coefficient (r).

where Nj is the number of instances for task j and p is the number of features.
We fit three versions of this model using three different feature sets: Friends, Text, and
Friends+Text. We tuned the regularization parameters on a held-out set of 200 accounts
for Gender prediction, setting the scikit-learn parameters l1 ratio=0.5 for each model,
alpha=1e5 for the Friends model, and alpha=1e2 for the Text and Friends+Text models.
4.1.1 Regression Results
We perform five-fold cross-validation and report the held-out correlation coefficient (r)
between the predicted and true demographic variables. Figure 3 displays the resulting
scatter plots for each of the 21 categories for 7 demographic variables.
We can see that overall the correlation is very strong: .77 on average, ranging from .61
for the 35-44 age bracket to .91 for Male. All of these correlation coefficients are significant
using a two-tailed t-test (p < 0.01), with a Bonferroni adjustment for the 21 comparisons.
These results indicate that the neighbor and text vectors provides a reliable signal of the
demographics of a group of Twitter users. To put this correlation value in context, as
an indirect comparison, Eisenstein et al. (2011) predict the ethnicity proportions by ZIP
Code using Twitter data and obtain a maximum correlation of .337 (though the data and
experimental setup differ considerably).
396

fiPredicting Twitter User Demographics

Category
Gender
Age

Income

Politics

Value
Male
Female
18-24
25-34
35-44
45-54
55-64
65+
$0-50k
$50-100k
$100-150k
$150k+
Democrat
Republican

Education

Children

Ethnicity

No College
College
Grad School
No Kids
Has Kids
Caucasian
Hispanic
Afr. Amer.
Asian

Top Accounts
AdamSchefter, SportsCenter, espn, mortreport, WIRED
TheEllenShow, Oprah, MarthaStewart, Pinterest, Etsy
IGN, PlayStation, RockstarGames, Ubisoft, steam games
azizansari, lenadunham, mindykaling, WIRED
cnnbrk, BarackObama, AP, TMZ, espn
FoxNews, cnnbrk, AP, WSJ, CNN
FoxNews, cnnbrk, AP, ABC, WSJ
FoxNews, AP, WSJ, cnnbrk, DRUDGE REPORT
YouTube, PlayStation, IGN, RockstarGames, Drake
AdamSchefter, cnnbrk, SportsCenter, espn, ErinAndrews
WSJ, espn, AdamSchefter, SportsCenter, ErinAndrews
WSJ, TheEconomist, Forbes, nytimes, business
BarackObama,
Oprah,
NewYorker,
UncleRUSH,
MichelleObama
FoxNews,
michellemalkin,
seanhannity,
megynkelly,
DRUDGE REPORT
YouTube, PlayStation, RockstarGames, Xbox, IGN
StephenAtHome, WIRED, ConanOBrien, mashable
nytimes, WSJ, NewYorker, TheEconomist, washingtonpost
NewYorker, StephenAtHome, nytimes, maddow, pitchfork
parenting,
parentsmagazine,
HuffPostParents,
TheEllenShow, thepioneerwoman
jimmyfallon,
FoxNews,
blakeshelton,
TheEllenShow,
TheOnion
latimes, Lakers, ABC7, Dodgers, KTLA
KevinHart4real, Drake, Tip, iamdiddy, UncleRUSH
TechCrunch, WIRED, BillGates, TheEconomist, SFGate

Table 1: Accounts with the highest estimated coefficients for each category.
To further examine these results, Table 1 displays the features with the 5 largest coefficients per class according to the regression model fit using only friend features. Many
results match common stereotypes: sports accounts are predictive of men (AdamShefter
and MortReport are ESPN reporters), video game accounts are predictive of younger users
(IGN is a video gaming media company), financial news accounts are predictive of greater
income, and parenting magazines are predictive of users who have children. There also
appear to be some geographic effects, as California-related accounts are highly weighted
for both Hispanic and Asian categories. There seems to be good city-level resolution 
Los Angeles accounts (latimes, Lakers) are more strongly correlated with Hispanic users,
whereas San Francisco accounts (SFGate, SFist, SFWeekly) are more strongly correlated
with Asian users. There does seem to be some selection bias, so one must use caution in
interpreting the results. For example, BillGates is predictive of Asian users, but this is in
part because California has many Asian-Americans and in part because California has a
very strong technology sector.
397

fiCulotta, Ravi, & Cutler

Category
Gender
Age

Value
Male
Female
18-24
25-34
35-44
45-54
55-64
65+

Income

$0-50k
$50-100k
$100-150k

Politics

$150k+
Democrat
Republican

Education

No College
College
Grad School

Children

No Kids
Has Kids

Ethnicity

Caucasian
Hispanic
Afr. Amer.
Asian

Top Terms
film, guy, gay, man, fuck, game, team, internet, review, guys
hair, her, omg, family, girl, she, girls, cute, beautiful, thinking
d, haha, album, x, xd, :, actually, stream, wanna, im
super, dc, baby, definitely, nba, pregnancy, wedding, even,
entire, nyc
star, fans, kids, tv, bike, mind, store, awesome, screen, son
wow, vote, american, comes, ca, santa, county, boys, nice,
high
vote, golf, red, american, country, north, county, holiday,
smile, 99,999
vote, golf, @foxnews, holiday, may, american, he, family,
north, national
lol, games, @youtube, damn, black, ps9, side, d, community,
god
great, seattle, he, performance, lose, usa, kansas, iphone,
wow, cold
santa, flight, nice, looks, practice, congrats, bike, dc, retweet,
ride
dc, nyc, market, @wsj, congrats, beach, san, york, ca, looks
women, u, aint, nyc, equality, la, voice, seattle, dc, @nytimes
@foxnews, christmas, #tcot, football, county, morning,
family, christians, country, obamas
lol, games, put, @youtube, county, made, ps9, xbox, videos,
found
our, youre, seattle, photo, @mashable, la, apple, fashion,
probably, san
dc, @nytimes, market, which, review, excellent, boston, also,
congrats, @washingtonpost
care, street, gay, years, health, drink, dc, white, ht, album
kids, school, child, family, kid, daughter, children, utah,
moms, parents
christmas, fun, dog, country, st, could, luck, guy, florida, john
la, los, san, el, angeles, california, ca, lol, l.a, lakers
black, lol, bout, aint, brown, lil, african, blessed, smh, atlanta
chinese, la, sf, san, china, korea, india, bay, vs, hi

Table 2: Terms with the highest estimated coefficients for each category.

Additionally, Table 2 shows the top 10 terms for each category from the text-only model.
These text features follow many of the same trends as the friend features, perhaps with a
greater level of granularity. While most terms are self-evident, we highlight a few here:
xd is an emoticon for laughing used among young users, and is often separated by spaces
(thus the tokens x and d appearing separately in the 18-24 bracket); smh stands for
398

fiPredicting Twitter User Demographics

Model
multi-task elastic net
elastic net
ridge

Friends
.73
.72
.62

Text
.79
.78
.79

Friends + Text
.77
.76
.78

Average
.76
.75
.73

Table 3: Average held-out correlation across all demographic variables for three competing
regression models.

shaking my head, an expression of disbelief that is predictive of African American users;
hi is an abbreviation for the state of Hawaii, which has a large Asian population.
Finally, we compare multi-task elastic net with the single-task variant of elastic net
and ridge regression (with regularization parameters tuned as before). Table 3 shows that
the three methods mostly produce comparable accuracy, with the exception of the friends
features, in which ridge regression performs substantially worse than the others.
4.2 Classification
The regression results suggest that the proposed model can accurately characterize the
demographics of a group of Twitter accounts. In this section, we provide additional validation with manually annotated Twitter accounts to investigate whether the same model can
accurately predict the demographics of individual users.
4.2.1 Labeled Data
Many of the demographic variables are difficult to label at the individual level  e.g., income or education level is rarely explicitly mentioned in either a profile or tweet. Indeed,
an advantage of the approach here is that aggregate statistics are more readily available for
many demographics of interest that are difficult to label at the individual level. For validation purposes, we focus on three variables that can fairly reliably be labeled for individuals:
gender, ethnicity, and political preference.
The gender and ethnicity data were originally collected by Mohammady and Culotta
(2014) as follows: First, we used the Twitter Streaming API to obtain a random sample of
users, filtered to the United States (using time zone and the place country code from the
profile). From six days worth of data (December 6-12, 2013), we sampled 1,000 profiles
at random and categorized them by analyzing the profile, tweets, and profile image for
each user. We categorized 770 Twitter profiles into one of four ethnicities (Asian, African
American, Hispanic, Caucasian). Those for which ethnicity could not be determined were
discarded (230/1,000; 23%).3 The category frequency is Asian (22), African American
(263), Hispanic (158), Caucasian (327). To estimate inter-annotator agreement, a second
annotator sampled and categorized 120 users. Among users for which both annotators
selected one of the four categories, 74/76 labels agreed (97%). There was some disagreement
over when the category could be determined: for 21/120 labels (17.5%), one annotator
indicated the category could not be determined, while the other selected a category. Gender
3. This introduces some bias towards accounts with identifiable ethnicity; we leave an investigation of this
for future work.

399

fiCulotta, Ravi, & Cutler

Gender

# friends

104

Ethnicity

Politics

103
102

# unique terms

101
104
103
102
101 0
10

101

102

103 100

101
102
user rank

103 100

101

102

103

Figure 4: Rank-order frequency plots of the number of friends per user and the number of
unique terms per user in each of the labeled datasets (gender, ethnicity, politics).
These friends and terms are restricted to one of the 46,649 accounts and 112,642
terms used in the regression experiments.

annotation was done automatically by comparing the first name provided in the user profile
with the U.S. Census list of names by gender (Census, 1990). Ambiguous names were
removed.
For each user, we collected up to 200 of their friends using the Twitter API. We removed
accounts that restricted access to friend information; we also removed the Asian users due to
the small sample size, leaving a total of 615 users. For classification, each user is represented
by the identity of their friends (up to 200). Only those friend accounts contained in the
46,649 accounts used for the regression experiments were retained. We additionally collected
up to 3,200 tweets from each user and constructed a binary term vector, using the same
tokenization as in the regression model.
The political preference data comes from Volkova, Coppersmith, and Van Durme (2014),
who in turn builds on the labeled data of Pennacchiotti and Popescu (2011) and Al Zamal
et al. (2012). Volkova (2014) provides a detailed description of the data. We use the
geo-centric portion of the data, which contains Twitter users from Maryland, Virginia, or
Delaware who report their political affiliation in their Twitter profile description (e.g., Im
a father, husband, Republican). Note that our feature representation does not consider
tokens from the user profile. This contains 183 Republican users and 230 Democratic users.
(We do not consider related political datasets that were annotated based on whom the user
follows, as this may give an unfair advantage to the friend features.) Each user has up
to 5,000 friends and 200 tweets. Figure 4 shows the number of friends and the number of
unique terms per user for each dataset.
400

fiPredicting Twitter User Demographics

Gender
Ethnicity
Politics
Average

Friends
distant full
.75
.66
.60
.68
.80
.83
.72
.72

Text
distant
.86
.86
.56
.76

full
.84
.86
.73
.81

Friends + Text
distant
full
.87
.84
.81
.86
.74
.73
.81
.81

Table 4: F1 results for Twitter user classification on manually annotated data. We consider three different feature sets (Friends, Text, Friends+Text), as well as two
classification models: full is a fully-supervised logistic regression classifier fit to
manually labeled Twitter users; distant is our proposed distantly-supervised regression model, fit only on QuantCast data, using no manually annotated Twitter
users. The largest values in each row are in bold.

4.2.2 Classification Models
As our model was initially trained for a regression task, we make a few modifications to
apply it to a classification task. We represent each user in the labeled data as a binary
vector of friend and text features, using the same tokenization as in the regression results.
For example, if a user follows accounts A and B, then the feature values are 1 for those
corresponding accounts; similarly, if the user mentions terms X and Y, then those feature
values are 1. To repurpose the regression model to perform classification, we must modify
the coefficients returned by regression. We first compute the z-score of each coefficient
with respect to the other coefficients for that category value. E.g., all coefficients for the
Male class are adjusted to have mean 0 and unit variance. This makes the coefficients
comparable across labels. To classify each user, we then compute the dot product between
the coefficients and the binary feature vector, selecting the class with maximum value.
The regression model fit on the combined Friend+Text feature set performed poorly in
initial classification experiments. Upon investigation, we determined that the coefficients
for the two types of feature tended to differ by an order of magnitude. Rather than use
this model directly, we instead adopted an ensemble approach by combining the outputs of
the two models trained separately on text and friend features. To classify a set of users, we
computed the feature-coefficient dot product separately for the text and friend models, then
computed the z-score of the resulting values by class label (e.g., all dot-products produced
by the text model for the Male class were standardized to have zero mean and unit variance).
This put the predicted values for each model in the same range. We finally summed the
outputs of both models and returned the class with the maximum value for each user.
We also compared with a fully-supervised baseline. We trained a logistic regression
classifier with L2 regularization, using the same feature representation as above. We perform
three-fold cross-validation to compare accuracy with the distantly supervised approach.
401

fiCulotta, Ravi, & Cutler

Friends

Ethnicity

Politics

Macro F1

Gender

Text

Friends+Text

0.9
0.8
0.7
0.6
0.5
0.4
0.9
0.8
0.7
0.6
0.5
0.4
0.9
0.8
0.7
0.6
distant supervision
0.5
full supervision
0.4
10 20 30 40 50 60 70 80 90 100 10 20 30 40 50 60 70 80 90 100 10 20 30 40 50 60 70 80 90 100

% of labeled training data

Figure 5: Twitter user classification results comparing a standard logistic regression classifier (full supervision), trained using cross-validation, versus the proposed
approach (distant supervision), which is fit solely on statistics from Quantcast, with no individually labeled data. Distant supervision is comparable to full
supervision, even after 100% of the available training data are provided.

4.2.3 Classification Results
Table 4 compares the F1 scores for our distantly supervised approach (distant) as well as
the fully supervised baseline (full). We report results using each of the three feature sets
for the three labeled datasets.
Overall, the distantly supervised approach is comparable to the fully supervised approach. For two of the three tasks, the F1 score of distant meets or exceeds full. For the
third task (politics), the best distant method is within 3% of the best full method (.80
vs .83). Averaging over all three tasks, the best distant method is indistinguishable from
the best full method (both produce F1 scores of 81%).
The primary result in which distant supervision performs poorly is political classification
using text features. We speculate that this is in part due to the small number of tweets per
user available in this dataset (at most 200 tweets per user). Additionally, the text features
used in the distantly supervised approach were collected several years after the tweets
contained in the political dataset. Given the rapid topical changes of political dialogue, it
is likely that many of the highly-weighted terms in the distant text model are less relevant
to this older data. The results do suggest that friend features may be less susceptible to
such data drift  the friend-based distant model performs much better than the text-based
model (.80 vs .56).
As the number of labeled data are relatively small (fewer than 1,000 users), we examined the accuracy of the fully supervised approach as the number of labeled data increase
(Figure 5). It appears that supervised classification accuracy has mostly plateaued on each
402

fiPredicting Twitter User Demographics

Macro F1

Gender
Ethnicity
Politics

0.9
0.8
0.7
0.6
0.5
0.4
0
0.9
0.8
0.7
0.6
0.5
0.4
0
0.9
0.8
0.7
0.6
0.5
0.4
0

Friends

Text

10 20 30 40 50

0

2000 4000 6000 8000

10 20 30 40 50

0

2000 4000 6000 8000

10 20 30 40 50

0

friends per user

500

1000

terms per user

Figure 6: Classification F1 scores of the distantly supervised approach as the number of
friends and number of unique terms collected per user increase (with standard
deviations computed over five random trials).

task (with a possible exception of the Friends+Text features for gender classification). For
ethnicity, distant outperforms full until over half of the labeled data is used to fit the
classification approach, after which full dominates. Thus, it appears that the distantly
supervised approach is comparable to fully supervised learning across a range of sizes of
labeled data.
4.2.4 Sensitivity to Number of Features
Finally, we investigate how much information we need about a user before we can make
an accurate prediction of their demographics. To do so, we perform an experiment in
which we randomly sample a subset of friends and terms for each user, and we report
F1 as the number of selected features increases. For friends, we consider subsets of size
{1, 3, 5, 10, 20, 30, 40, 50} (values greater than 50 did not significantly increase accuracy).
For terms, we consider subsets of size {10, 100, 1000, 2000, 8029} (8,029 is the maximum
number of unique terms used by any single user in the labeled data).
Figure 6 displays the results. We can see that accuracy plateaus quickly using friend
features: for all three tasks, the F1 score using only 10 friends is within 5% of the score using
all 200 friends. For text features, accuracy begins to plateau at around 2K unique terms
for the Gender and Ethnicity tasks. The lower accuracy using Text features for Politics is
likely due in part to the simple fact that the Politics data have fewer tweets per user (a
maximum of 200 tweets per user, compared to up to 3,200 for the Gender and Ethnicity
tasks).
These results have implications for scalability  Twitter API rate limits make it difficult
to collect the complete social graph or tweets for a set of users. Additionally, this has
403

fiCulotta, Ravi, & Cutler

important privacy implications; revealing even a small amount of social information may
also reveal a considerable amount of demographic information. Twitter users concerned
about privacy may wish to disable the setting that makes friend identity information public.

5. Conclusions and Future Work
In this paper, we have shown that pairing web traffic demographic data with Twitter data
provides a simple and effective way to train a demographic inference model without any annotation of individual profiles. We have validated the approach both in aggregate (by comparing with Quantcast data) and at the individual level (by comparing with hand-labeled
annotations), finding high accuracy in both cases. Somewhat surprisingly, the approach
outperforms a fully-supervised approach for gender classification, and is competitive for
ethnicity and political classification.
In short-term future work, we will test the generalizability of this approach to new
groups of Twitter users. For example, we can collect users by city or county and compare
the predictions with the Census demographics from that geographic location. Additionally, we will investigate ways to combine labeled and unlabeled data using semi-supervised
learning (Quadrianto, Smola, Caetano, & Le, 2009b; Ganchev et al., 2010; Mann & McCallum, 2010). Finally, to fully validate across all demographic variables, we will consider
administering surveys to Twitter users to compare predictions with self-reported survey
responses.
Additional future work may investigate more sophisticated types of distant supervision.
For example, homophily constraints can be imposed to encourage neighbors to have similar demographics; location constraints can use be used to learn from county demographic
data. Also, while the multi-task model captures some interaction between demographic variables at training time, we can also use collective inference to reflect the correlations among
demographic variables. Finally, we have only considered a simple bag-of-words feature
representations; future work may investigate low-dimensional embeddings and non-linear
models.

Acknowledgments
This research was funded in part by support from the IIT Educational and Research Initiative Fund. Culotta was supported in part by the National Science Foundation under grant
#IIS-1526674. Any opinions, findings and conclusions or recommendations expressed in
this material are the authors and do not necessarily reflect those of the sponsor.

References
Al Zamal, F., Liu, W., & Ruths, D. (2012). Homophily and latent attribute inference:
Inferring latent attributes of twitter users from neighbors. In ICWSM.
Argamon, S., Dhawle, S., Koppel, M., & Pennebaker, J. W. (2005). Lexical predictors of
personality type. In In proceedings of the Joint Annual Meeting of the Interface and
the Classification Society of North America.
404

fiPredicting Twitter User Demographics

Barbera, P. (2013). Birds of the same feather tweet together. bayesian ideal point estimation
using twitter data. In Proceedings of the Social Media and Political Participation,
Florence, Italy, pp. 1011.
Burger, J. D., Henderson, J., Kim, G., & Zarrella, G. (2011). Discriminating gender on
twitter. In Proceedings of the Conference on Empirical Methods in Natural Language
Processing, EMNLP 11, pp. 13011309, Stroudsburg, PA, USA. Association for Computational Linguistics.
Census (1990).
List of surnames.
http://www2.census.gov/topics/genealogy/
1990surnames. Accessed: 2015-06-01.
Chang, J., Rosenn, I., Backstrom, L., & Marlow, C. (2010). ePluribus: ethnicity on social
networks. In Fourth International AAAI Conference on Weblogs and Social Media.
Conover, M. D., Goncalves, B., Ratkiewicz, J., Flammini, A., & Menczer, F. (2011). Predicting the political alignment of twitter users. In IEEE Third international conference
on social computing (SOCIALCOM), pp. 192199. IEEE.
Culotta, A. (2014). Estimating county health statistics with twitter. In CHI.
Culotta, A., Kumar, N. R., & Cutler, J. (2015). Predicting the demographics of twitter
users from website traffic data. In Twenty-ninth National Conference on Artificial
Intelligence (AAAI).
Dredze, M. (2012). How social media will change public health. IEEE Intelligent Systems,
27 (4), 8184.
Druck, G., Mann, G., & McCallum, A. (2008). Learning from labeled features using generalized expectation criteria. In Proceedings of the 31st Annual International ACM SIGIR
Conference on Research and Development in Information Retrieval, pp. 595602.
Druck, G., Mann, G., & McCallum, A. (2009). Semi-supervised learning of dependency
parsers using generalized expectation criteria. In ACL.
Eisenstein, J., Smith, N. A., & Xing, E. P. (2011). Discovering sociolinguistic associations
with structured sparsity. In Proceedings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language Technologies - Volume 1, HLT 11,
pp. 13651374, Stroudsburg, PA, USA. Association for Computational Linguistics.
Ganchev, K., & Das, D. (2013). Cross-lingual discriminative learning of sequence models
with posterior regularization.. In EMNLP, pp. 19962006.
Ganchev, K., Gillenwater, J., & Taskar, B. (2009). Dependency grammar induction via
bitext projection constraints. In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International Joint Conference on Natural
Language Processing of the AFNLP: Volume 1-Volume 1, pp. 369377. Association
for Computational Linguistics.
Ganchev, K., Graca, J., Gillenwater, J., & Taskar, B. (2010). Posterior regularization for
structured latent variable models. J. Mach. Learn. Res., 11, 20012049.
Goel, S., Hofman, J. M., & Sirer, M. I. (2012). Who does what on the web: A large-scale
study of browsing behavior.. In ICWSM.
405

fiCulotta, Ravi, & Cutler

Gopinath, S., Thomas, J. S., & Krishnamurthi, L. (2014). Investigating the relationship
between the content of online word of mouth, advertising, and brand performance.
Marketing Science, 33 (2), 241258.
Jin, R., & Liu, Y. (2005). A framework for incorporating class priors into discriminative
classification. In In PAKDD.
Kamerer, D. (2013). Estimating online audiences: Understanding the limitations of competitive intelligence services. First Monday, 18 (5).
King, B., & Abney, S. (2013). Labeling the languages of words in mixed-language documents
using weakly supervised methods. In Proceedings of NAACL-HLT, pp. 11101119.
Liang, P., Jordan, M. I., & Klein, D. (2009). Learning from measurements in exponential
families. In Proceedings of the 26th Annual International Conference on Machine
Learning, ICML 09, p. 641648, New York, NY, USA. ACM.
Liu, W., & Ruths, D. (2013). Whats in a name? using first names as features for gender
inference in twitter. In AAAI Spring Symposium on Analyzing Microtext.
Mann, G. S., & McCallum, A. (2010). Generalized expectation criteria for semi-supervised
learning with weakly labeled data. J. Mach. Learn. Res., 11, 955984.
Melville, P., Gryc, W., & Lawrence, R. D. (2009). Sentiment analysis of blogs by combining
lexical knowledge with text classification. In Proceedings of the 15th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, KDD 09, p.
12751284, New York, NY, USA. ACM.
Mohammady, E., & Culotta, A. (2014). Using county demographics to infer attributes of
twitter users. In ACL Joint Workshop on Social Dynamics and Personal Attributes
in Social Media.
Musicant, D., Christensen, J., & Olson, J. (2007). Supervised learning by training on
aggregate outputs. In Seventh IEEE International Conference on Data Mining, 2007.
ICDM 2007, pp. 252261.
Nguyen, D., Smith, N. A., & Ros, C. P. (2011). Author age prediction from text using linear
regression. In Proceedings of the 5th ACL-HLT Workshop on Language Technology
for Cultural Heritage, Social Scie nces, and Humanities, LaTeCH 11, pp. 115123,
Stroudsburg, PA, USA. Association for Computational Linguistics.
Obozinski, G., & Taskar, B. (2006). Multi-task feature selection. In In the workshop of structural Knowledge Transfer for Machine Learning in the 23rd International Conference
on Machine Learning (ICML.
OConnor, B., Balasubramanyan, R., Routledge, B. R., & Smith, N. A. (2010). From tweets
to polls: Linking text sentiment to public opinion time series.. ICWSM, 11, 122129.
Oktay, H., Firat, A., & Ertem, Z. (2014). Demographic breakdown of twitter users: An
analysis based on names. In Academy of Science and Engineering (ASE).
Pedregosa, F., et al. (2011). Scikit-learn: Machine learning in Python. Machine Learning
Research, 12, 28252830.
406

fiPredicting Twitter User Demographics

Pennacchiotti, M., & Popescu, A.-M. (2011). A machine learning approach to twitter user
classification.. In Adamic, L. A., Baeza-Yates, R. A., & Counts, S. (Eds.), ICWSM.
The AAAI Press.
Preotiuc-Pietro, D., Lampos, V., & Aletras, N. (2015). An analysis of the user occupational
class through twitter content. In ACL.
Quadrianto, N., Petterson, J., & Smola, A. J. (2009a). Distribution matching for transduction. In Advances in Neural Information Processing Systems 22, p. 15001508. MIT
Press.
Quadrianto, N., Smola, A. J., Caetano, T. S., & Le, Q. V. (2009b). Estimating labels from
label proportions. J. Mach. Learn. Res., 10, 23492374.
Rao, D., Paul, M. J., Fink, C., Yarowsky, D., Oates, T., & Coppersmith, G. (2011). Hierarchical bayesian models for latent attribute detection in social media. In ICWSM.
Rao, D., Yarowsky, D., Shreevats, A., & Gupta, M. (2010). Classifying latent user attributes
in twitter. In Proceedings of the 2nd International Workshop on Search and Mining
User-generated Contents, SMUC 10, pp. 3744, New York, NY, USA. ACM.
Rosenthal, S., & McKeown, K. (2011). Age prediction in blogs: A study of style, content, and
online behavior in pre- and post-social media generations. In Proceedings of the 49th
Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies - Volume 1, HLT 11, pp. 763772, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Schapire, R. E., Rochery, M., Rahim, M. G., & Gupta, N. K. (2002). Incorporating prior
knowledge into boosting. In Proceedings of the Nineteenth International Conference,
pp. 538545.
Schler, J., Koppel, M., Argamon, S., & Pennebaker, J. W. (2006). Effects of age and
gender on blogging. In AAAI 2006 Spring Symposium on Computational Approaches
to Analysing Weblogs (AAAI-CAAW), pp. 0603.
Schwartz, H. A., Eichstaedt, J. C., Kern, M. L., Dziurzynski, L., Ramones, S. M., Agrawal,
M., Shah, A., Kosinski, M., Stillwell, D., Seligman, M. E., et al. (2013). Characterizing geographic variation in well-being using tweets. In Seventh International AAAI
Conference on Weblogs and Social Media.
Volkova, S. (2014). Twitter data collection: Crawling users, neighbors and their communication for personal attribute prediction in social media. Tech. rep., Johns Hopkins
University.
Volkova, S., Coppersmith, G., & Van Durme, B. (2014). Inferring user political preferences
from streaming communications. In Proceedings of the Association for Computational
Linguistics (ACL).
Volkova, S., & Van Durme, B. (2015). Online bayesian models for personal analytics in
social media. In Proceedings of the Twenty-Ninth Conference on Artificial Intelligence
(AAAI), Austin, TX.
Wang, M., & Manning, C. D. (2014). Cross-lingual projected expectation regularization for
weakly supervised learning. TACL, 2, 5566.
407

fiCulotta, Ravi, & Cutler

Zhu, J., Chen, N., & Xing, E. P. (2014). Bayesian inference with posterior regularization
and applications to infinite latent svms. Journal of Machine Learning Research, 15,
17991847.
Zou, H., & Hastie, T. (2005). Regularization and variable selection via the elastic net.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67 (2),
301320.

408

fiJournal of Artificial Intelligence Research 55 (2016) 249-281

Submitted 03/15; published 01/16

Utilisation of Metadata Fields and Query Expansion in
Cross-Lingual Search of User-Generated Internet Video
Ahmad Khwileh
Debasis Ganguly
Gareth J. F. Jones

ahmad.khwileh2@mail.dcu.ie
dganguly@computing.dcu.ie
gjones@computing.dcu.ie

ADAPT Centre, School of Computing
Dublin City University
Dublin 9, Ireland

Abstract
Recent years have seen significant efforts in the area of Cross Language Information
Retrieval (CLIR) for text retrieval. This work initially focused on formally published
content, but more recently research has begun to concentrate on CLIR for informal social
media content. However, despite the current expansion in online multimedia archives, there
has been little work on CLIR for this content. While there has been some limited work
on Cross-Language Video Retrieval (CLVR) for professional videos, such as documentaries
or TV news broadcasts, there has to date, been no significant investigation of CLVR for
the rapidly growing archives of informal user generated (UGC) content. Key differences
between such UGC and professionally produced content are the nature and structure of
the textual UGC metadata associated with it, as well as the form and quality of the
content itself. In this setting, retrieval effectiveness may not only suffer from translation
errors common to all CLIR tasks, but also recognition errors associated with the automatic
speech recognition (ASR) systems used to transcribe the spoken content of the video and
with the informality and inconsistency of the associated user-created metadata for each
video. This work proposes and evaluates techniques to improve CLIR effectiveness of
such noisy UGC content. Our experimental investigation shows that different sources of
evidence, e.g. the content from different fields of the structured metadata, significantly
affect CLIR effectiveness. Results from our experiments also show that each metadata field
has a varying robustness to query expansion (QE) and hence can have a negative impact
on the CLIR effectiveness. Our work proposes a novel adaptive QE technique that predicts
the most reliable source for expansion and shows how this technique can be effective for
improving CLIR effectiveness for UGC content.

1. Introduction
Increasing amounts of user generated multilingual video content (UGC) are being uploaded
to social video-sharing websites such as Youtube (2015), Facebook (Facebook video, 2015),
BlipTv (2015) and many others. In 2015, YouTube, the predominant online video sharing
site, reported that 300 hours of video content were being uploaded every minute in 61
different languages (YouTube Press, 2015). The ease and the flexibility of video content
production, coupled with the low cost of publishing and wide potential reach, are resulting
in an exponential growth in the number of videos available on the Web. At the same
time, due to increasing user demands for accessing and viewing this content, it is very
important to manage it in such a way so as to facilitate effective and efficient access to
c
2016
AI Access Foundation. All rights reserved.

fiKhwileh, Ganguly, & Jones

it. The very large amounts of this content are creating the need for the development of
sophisticated video retrieval systems, presenting new challenges and exciting opportunities
for Information Retrieval (IR) research (Bendersky, Garcia-Pueyo, Harmsen, Josifovski, &
Lepikhin, 2014; Naaman, 2012).
One of the key challenges for the effective exploitation of UGC content in a multilingual setting is effective search between the languages of the user queries and the content
metadata. From the multilingual perspective, the quality of UGC depends solely on the
characteristics of the individuals who actually produce or upload videos in each language.
This lack of formal editorial control means that the uploaded videos are typically very varied across languages in terms of audio, visual and metadata quality. Moreover, the quantity
and topical coverage of this content across different languages is very uneven, often meaning that satisfying an information need for a user of one language can only be achieved
by providing relevant content in another language. For example, bilingual Arabic speakers
frequently enter Arabic queries for which the only relevant content is in English, this is
even more likely for video material where little UGC Arabic content is currently available
in certain topics such as cultural or historical topics. This in fact is a classical use-case for
Cross-Language Information Retrieval (CLIR) which seeks to enable users to enter search
queries in one language to retrieve relevant content in another one. Translation technologies
are key to successfully bridging the language gap between a users query and the relevant
content (Oard & Diekema, 1998; Herbert, Szarvas, & Gurevych, 2011).
The quality of monolingual search over UGC content is dependent on the effective utilization of the available metadata. CLIR search effectiveness will further depend on translation
quality between query and content languages. There are many potential choices for how to
design a robust CLIR framework for an Internet video search task, but the current lack of
detailed investigation means that there is a lack of understanding of the specific challenges
it represents and thus little or no guidance available for the choices that should be made in
developing such a framework.
In this paper, we investigate CLIR search effectiveness over an archive of user-generated
Internet video content originally used for the MediaEval 2012 Search and Hyperlinking
task (Eskevich, Jones, Chen, Aly, Ordelman, & Larson, 2012a), which we extend to a
CLIR task. We examine retrieval effectiveness using the title and the description metadata
provided by the video uploader and automatic speech recognition (ASR) transcripts of the
content. We further investigate the application of automatic query expansion on each source
for improving the CLIR retrieval performance. Retrieval and query expansion are carried
out using the Divergence From Randomness (DFR) IR model, and automatic translation is
carried out using Google Translate (2015). To understand the task better, we undertake a
detailed performance analysis examining the impact of different source metadata information on CLIR behaviour. However, our current investigation is limited to the application of
state-of-the-art Machine Translation (MT) and information retrieval (IR) methods to this
task, in order to establish the basis for further investigations.
The remainder of this paper is structured as follows: Section 2 gives some general
background on CLIR, Section 3 reviews related work, Section 4 describes the test set
used in our experiments and the evaluation metric, Section 5 describes initial retrieval
experiments examining the relative CLIR effectiveness of each source of evidence (ASR,
Title and Description), Section 6 describes our approach to improving CLIR effectiveness
250

fiCross-Lingual Search For User-Generated Internet Video

using careful adjustment of the retrieval algorithm setting, Section7 describes our approach
to improving CLIR effectiveness using automatic query expansion techniques, Section 8
concludes the paper and provides directions for further work.

2. Cross Language Information Retrieval
As stated previously, the goal of CLIR is to satisfy a user information need expressed as
a query in one language using a content from another language. CLIR techniques use
translation to bridge this language barrier between the query and the indexed content.
These techniques differ mainly as to where the translation module is to be placed, either in
the query processing or the document indexing stage. Figure 1 shows how CLIR techniques
can utilise translation technologies to bridge the barrier between query language (L2) and
document language (L1). The Query Translation approach (QT CLIR) is the most common

Figure 1: Document and Query based CLIR Techniques.
CLIR technique (Oard & Diekema, 1998; Herbert et al., 2011; Sokolov, Hieber, & Riezler,
2014); where the query is translated to match the index language (L1). This technique is
known to be low cost (per translated query) and easy to implement, since a translation
tool can be used online at retrieval time to translate the query into the document language.
Yet this approach is very dependent and sensitive to the quality of the query translation
for retrieval. Some queries may lack context and semantic content, which makes them
harder to interpret and translate. Previous literature has explored multiple techniques to
overcome these issues either by improving the translation quality using various translation
techniques (Chen, Hueng, Ding, & Tsai, 1998; Gao, Nie, Xun, Zhang, Zhou, & Huang,
2001; Varshney & Bajpai, 2014; Lee, Chen, Kao, & Cheng, 2010) or by improving the query
itself using query reformulation techniques such as Query Expansion (QE) in a Relevance
Feedback (RF) process (Carpineto & Romano, 2012).
251

fiKhwileh, Ganguly, & Jones

QE in RF operates by selecting terms from documents which have been marked as
relevant by the user and adding them to the original query. In the absence of user created
relevance data, the top ranked documents can be assumed relevant in a process often referred
to as pseudo relevance feedback (PRF). Although noisy, PRF has shown to be effective for
improving the overall retrieval effectiveness (Bellaachia & Amor-Tijani, 2008). In crosslingual settings, the query can be expanded before translation to provide a more effective
query for translation using a pre-translation QE technique (Ballesteros & Croft, 1997). QE
expansion can also be applied after translation (post-translation QE), or using a combination
of pre-translation and post-translation QE as shown in Figure 1, in order to alleviate the
impact on IR effectiveness arising from translation problems (Ballesteros & Croft, 1998;
Rogati & Yang, 2002).
The alternative to QT CLIR is Document Translation (DT CLIR) where all documents
in the collection are translated to the query language (Oard & Hackett, 1998; Lee & Croft,
2014). Several arguments suggest that DT should be competitive or superior to QT CLIR
for some tasks, due to the fact that it is less sensitive to translation errors. DT has the
advantage that all translation is carried out offline prior to retrieval, which allows for the
possibility of having a more tuned and accurate translation. Another advantage of DT CLIR
is that it does not require any result translation as shown in Figure 1, since documents are
already translated at index time. However, while DT CLIR has been shown to be effective
for several tasks, its application in CLIR settings is often impractical due to the very
large amount of time and resources required for document translation. Particularly, when
the document collection is large and search is to be carried out across multiple language
pairs. A less common CLIR technique which has shown to be effective, is the Hybrid CLIR
approach which utilises both document and query translation approaches, thus allowing the
relative advantages of both approaches to complement each other (McCarley, 1999; Kishida
& Kando, 2006; Parton, McKeown, Allan, & Henestroza, 2008).
Several approaches have been proposed to carry the translation process in the CLIR
framework. The most commonly used ones are bilingual dictionaries and machine translation (MT) (Zhou, Truran, Brailsford, Wade, & Ashman, 2012). Bilingual dictionaries
perform a word-by-word translation using a machine-readable dictionary which has sets of
entries of words and their possible translations in the other language (Pirkola, Hedlund,
Keskustalo, & Jarvelin, 2001). This approach can suffer from issues such as coverage, since
some words may not be contained in the machine-readable dictionary, and ambiguity since
it relies on a dictionary where many words have multiple possible translations and selecting the correct translation among them is a non-trivial task. Machine translation (MT)
techniques use a trained system to perform an automatic translation of free-text from one
natural language to another (Nikoulina, Kovachev, Lagos, & Monz, 2012; Magdy & Jones,
2014). While MT can also have similar dictionary coverage problems, the creation of a
single best translation addresses the translation ambiguity issues. In recent years, MT has
become the most commonly used technique in CLIR due to the increasing availability of
high quality off-the-shelf MT systems. Most CLIR research has dealt with the translation
module as a black-box without any control over the translation process, and rather used
one of the freely available online translation tools such as Google Translate (2015), Bing
translate (2015) and others, which have proven to be effective. For example, in the CLEF
evaluation campaigns 2009 (CLEF, 2015a), the best performing non-Google MT system
252

fiCross-Lingual Search For User-Generated Internet Video

achieved just 70% of the performance achieved by Google Translate tool (Leveling, Zhou,
Jones, & Wade, 2010; Zhou et al., 2012).
For this experimental investigation, we choose the default and most common CLIR
settings, which is a QT CLIR technique that utilises the Google MT translate tool. We
nevertheless, plan to explore other CLIR settings (e.g. DT-CLIR with open-box MT system)
in our future work.

3. Related Work
Several CLIR tasks have been explored across different domains and document types (Peters,
Braschler, & Clough, 2012). The most closely related CLIR work to that examined in this
research was carried out in tasks within the CLEF evaluation campaigns on professionally
generated video content (2015b).
From 2002-2004 the Cross-Language Spoken Document Retrieval (CL-SDR) task investigated news story document retrieval using data from the NIST TREC 8-9 Spoken
Document Retrieval (SR) with manually translated queries (Federico & Jones, 2004; Federico, Bertoldi, Levow, & Jones, 2005). The aim of these tasks was to evaluate CLIR
systems on noisy automatic transcripts of spoken documents with known story boundaries
which involved the retrieval of American English news broadcasts of both unsegmented and
segmented transcripts taken from radio and TV news. These CLIR tasks were done using
topics in several European languages. No metadata was provided in these tasks, but some
interesting findings indicate that even with the manually translated queries, the best CLIR
performance resulted in 15% reduction from the monolingual ones (Federico & Jones, 2004),
while using dictionary term-by-term translation, this reduction increased to between about
40% and 60%, which highlights the challenge for CLIR over video collections (Federico et al.,
2005).
A more ambitious Cross-Language Speech Retrieval (CL-SR) task ran within CLEF
2005-2007 (White, Oard, Jones, Soergel, & Huang, 2006; Oard, Wang, Jones, White,
Pecina, Soergel, Huang, & Shafran, 2007; Pecina, Hoffmannova, Jones, Zhang, & Oard,
2008). This examined CLIR for a spontaneous conversational speech oral history collection
with content in English and Czech. The tasks provided ASR transcripts, automatically
and manually generated metadata for the interviews. The goal for both Czech and English
tasks was to create systems that could help monolingual and cross lingual searchers identify
sections of an interview that they wish to listen to on Czech and English interviews. The
reported results of these tasks showed that the use of manual metadata yielded substantial
and statistically significant improvement in retrieval effectiveness over ASR transcripts and
automatically created metadata. A further investigation carried by Inkpen, Alzghool, Jones,
and Oard (2006) on the CL-SR standard collection showed that retrieval effectiveness could
be improved by careful selection of the term weighting scheme between the ASR and the
manual metadata. Alzghool and Inkpen (2008) also used the test collection of the CLEF
2007 CL-SR task to present a method for combining results from different retrieval models
in order to improve the overall retrieval effectiveness. They also provided a comparison
between both ASR and manual metadata, indicating the superiority of the manual metadata
for maintaining the retrieval effectiveness. Another interesting follow up study, reported
by Jones, Zhang, Newman, and Lam-Adesina (2007), examined and compared the CLIR
253

fiKhwileh, Ganguly, & Jones

effectiveness of each source of evidence included in this collection. Results from this work
indicate that searching the manually generated metadata gives higher performance in terms
of recall and precision then search of noisy ASR transcripts.
The VideoCLEF task was then introduced at CLEF 2008 and CLEF 2009. This task
provided Dutch TV content featuring English-speaking experts and studio guests. VideoCLEF piloted tasks involved performing classification, translation and keyword extraction
on dual language video using either machine learning techniques or treating it as an IR
task. Participants were provided with Dutch archival metadata, Dutch speech transcripts,
and English speech transcripts (Larson, Newman, & Jones, 2009, 2010).
This previous work on CLVR focused on running CLIR tasks on a professional video
broadcast whether its documentaries, TV shows or interviews with high quality recording
and consistency of length, visual and audio quality across the collections. These collections
included manually or automatically created metadata. For example, domain experts following a carefully prescribed format wrote the manually created metadata in the CLEF
2005-2007 with consistent speech quality of word error rate of 25% across the collections
used (White et al., 2006; Oard et al., 2007; Pecina et al., 2008).
The CLEF tasks were followed by the establishment of the MediaEval benchmarking
campaign in 2010 (MediaEval, 2015). Activities at MediaEval have focused on various
multimedia search tasks, but have not included any CLIR elements.
The emergence of user-generated video content on the web has introduced new search
opportunities and challenges in exploitation of the user-generated metadata (Eickhoff, Li,
& de Vries, 2013; Filippova & Hall, 2011; Toderici, Aradhye, Pasca, Sbaiz, & Yagnik, 2010).
While CLIR for published text has been explored for a wide variety of language pairs
for many years, recent research has begun to explore CLIR for user-generated informal text.
One example of this work is the one done by Bagdouri, Oard, and Castelli (2014) which
explored the retrieval of questions posed in formal English across user generated documents
of Arabic collected from a forum posts. They employed a DT CLIR approach where they
translated the Arabic informal text into English. Their results show that retrieval precision
can be enhanced by applying an informal text classifier to help the translation of informal
content. Lee and Croft (2014) also experimented with a CLIR task for informal documents.
They developed a CLIR task over a large collection of Chinese forum posts and demonstrated
that translation noise is increased by the informal text used in discussion forums. Their
retrieval approach proposed to use PRF approach to improve retrieval effectiveness. Their
results showed that PRF approaches can be useful in reducing the impact of translation
errors on retrieval effectiveness for their tasks.
UGC has begun to attract considerable research interest in video retrieval and indexing in the recent years. While none of this work has so far included an element of CLIR,
much of it has addressed the main issues of user-generated content in video retrieval. For
example, some work has focused on the quality of user-generated metadata for video retrieval (Eickhoff et al., 2013; Filippova & Hall, 2011; Toderici et al., 2010), other work has
focused on the quality of visual/audio features within the scale and the dynamics of UGC
content (Bendersky et al., 2014; Chelba, Bikel, Shugrina, Nguyen, & Kumar, 2012; Langlois, Chambel, Oliveira, Carvalho, Marques, & Falcao, 2010). Moreover, from 2010, the
TREC Video Retrieval Evaluation (TRECVID) (2015), the main video retrieval benchmark
in the multimedia community, provided a collection of Internet videos to be used in several
254

fiCross-Lingual Search For User-Generated Internet Video

Table 1: Length statistics for indexed blip10000 fields.

Stan.Dev
Avg.Length
Median
Max
Min

Title
3.0
5.3
5.0
22.0
0.0

Desc
106.9
47.7
24.0
3197.0
1.0

ASR
2399.5
703.0
1674.8
20451.0
0.0

tasks. However, the design of TRECVID tasks has mainly focused on exploiting visual information for applications at the shot level (concept detection), or short video clips (event
detection) and others. One task which is relevant to this work is the known-item search
task (KIS) (Over, Awad, Fiscus, Antonishek, Michel, Smeaton, Kraaij, & Quenot, 2011) at
TRECVID, the task aimed to explore the retrieval of visual queries and was included at
TRECVID annually from 2010 to 2012. Results from the participants were rather inconsistent from year to year in terms of the retrieval effectiveness of different search approaches,
one conclusion being the difficulty of actually setting up such an evaluation task on Internet
collections.
In this work, we focus on studying the retrieval challenges of Internet-based UGC multimedia collections where audio data is highly variable in many aspects including the audio
conditions of the recording, the microphones used, the fluency and informality of the language used by the speaker. These challenges can produce more ASR errors which affect
retrieval effectiveness not only in monolingual retrieval, as reported by Eskevich, Jones,
Wartena, Larson, Aly, Verschoor, and Ordelman (2012b) and Eskevich (2014), but also in
cross-lingual settings when combined with query translation.
To the best of our knowledge, our work is the first effort to explore the issues of CLIR on
video collected from a user-contributed source on the Internet. Thus creators from varied
backgrounds and differing motivations and interests have generated the content without
any central editorial control of style, format or quality. This makes the uploaded videos
very varied in terms of the amount and quality of manually added metadata descriptions,
and thus challenging from multiple retrieval perspectives. Of particular relevance to our
investigation are the following aspects of the data:
 Distribution of document lengths: There is no restriction on document length which
it found to be highly variable. Such length variability poses a challenge for any
retrieval task, but it can be particularly significant within CLIR due to the presence
of translation errors. A breakdown of the details of the various fields in our blip10000
test collection is shown in Table 1.
 High variability in ASR quality of the video transcripts: Even if the same ASR system
is used, the variation in the audio quality, speaking styles and speakers, generally
leads to significant variability in the accuracy of the transcripts.
 Inconsistencies and sparseness of the associated user contributed metadata: The titles
may be very short having only one or two terms, while descriptions can be generic,
informal and sometimes incomplete, making their utility for retrieval very varied.
255

fiKhwileh, Ganguly, & Jones

4. Experimental Test Set and Evaluation
The blip10000 collection used in our experiments is a crawl of the Internet video sharing
platform Blip.tv (Schmiedeke, Xu, Ferrane, Eskevich, Kofler, Larson, Esteve, Lamel, Jones,
& Sikora, 2013). It was originally used as the content dataset for the MediaEval 2012
Search and Hyperlinking task (Eskevich et al., 2012a). The blip10000 collection contains
the crawled videos together with the associated metadata. This metadata is composed of
the titles and descriptions for each video that were provided by the video uploader. In
addition, associated ASR transcripts were also provided for most of videos. The collection
consists of 14,838 videos having a total running time of ca. 3,288 hours, and a total size of
about 862 GB1 .
The length statistics of the fields are shown in Table 1. It can be noted from this that
there is a huge variation in the length distributions across different fields. Table 1 also
highlights the variations of individual fields between the videos. For example, while one
video may have no ASR, another may contain over 20K terms. For our experiments we
indexed the metadata fields separately, and in combination as shown on Figure 2.

Figure 2: Example of a combined-field document.

1. The Blip10000 Data Collection can be obtained from:
http://skuld.cs.umass.edu/traces/mmsys/2013/blip/Blip10000.html

256

fiCross-Lingual Search For User-Generated Internet Video

Table 2: Monolingual English query vs Arabic-English translated query example.
Monolingual (MN) Query :
<top>
<num>37 </num>
<Mn-Lg>the video features a recent USA sanctioned
clean energy Act.</Mn-Lg>
<Mn-Sh>clean energy legislation USA</Mn-Sh>
</top>
Machine Translated (CL) Query :
<top>
<num>37</num>
<CL-AR-Lg>Video displays Identify measures the United
States toward alternative Energy</CL-AR-Lg>
<CL-AR-Sh>Rules United States
toward alternative energy</CL-AR-Sh>
</top>
4.1 Query Construction for the CLIR Task
The MediaEval 2012 Search and Hyperlinking task (Eskevich et al., 2012a) was a knownitem search task, a search for a single previously seen relevant video (the known-item),
which provided 60 English queries collected using the Amazon Mechanical Turk (MTurk)
crowd-sourcing platform (2015). Each query contains a full query statement (long query)
and a terse web type search query (short query). For our investigation, we explored both
the short and long queries to give a better understanding of the query-length independent
retrieval behaviour for both the monolingual and CLIR tasks. To create our CLIR test set,
we extended the original monolingual English queries by giving them to Arabic, Italian and
French native speakers, and asking them to translate them into natural queries into their
native language. Both the short and long queries were translated into Arabic. In order
to explore the CLIR effectiveness across multiple language pairs, the short query set was
also expressed in Italian, while the long query set was constructed in French. Having both
types of queries being expressed in two languages (long queries are expressed in Arabic
and French, while short queries are expressed in both Arabic and Italian) allowed us to
draw better conclusions about the CLIR performance for this task. We used the Google
translate API2 to translate these query sets back into English. As would be expected. The
MT translation produced different version of the original monolingual ones; in addition to
the expected deletion/insertion edits as shown in the example of Table 2, there were also
Named Entity Errors (NEEs) for Out-Of-Vocabulary (OOV) items that Google translation
could not translate correctly. These translation edits and errors pose a challenge to the
retrieval effectiveness of the MT translated queries compared to the monolingual ones.
The monolingual English query sets which were originally provided for the MediaEval
2012 Search and Hyperlinking task. For our investigation these query sets are labelled as
follows:
2. https://developers.google.com/translate

257

fiKhwileh, Ganguly, & Jones

 Mn-Sh: 60 EN short queries (monolingual)
 Mn-Lg: 60 EN long queries (monolingual)
The CLIR query sets are labelled as follows:
 CL-AR-Sh: 60 AR short queries translated into EN
 CL-AR-Lg: 60 AR long queries translated into EN
 CL-IT-Sh: 60 IT long queries translated into EN
 CL-FR-Lg: 60 FR long queries translated into EN
4.2 Mean Reciprocal Rank (MRR) Evaluation Metric
Since the retrieval problem that we are addressing is a known-item search for which we
are seeking to retrieve a single known relevant item, we evaluate our investigations using
the standard metric for this task is the Mean Reciprocal Rank (MRR) metric computed as
shown in Equation 1 where ranki indicates the rank of the ground truth known item that
the ith query is intended to find.
M RR =

n
1
1X
n i=1 ranki

(1)

Similar to other known-item experiments, we also chose to define the recall as the number of
times the relevant-item was found across the set of queries (Buttcher, Clarke, & Cormack,
2010). Moreover, the recall is reported by default at the standard TREC1000 results cut
off, but we also report it at cutoff points of 10, 50, 100 documents for some experiments.

5. Single Field Retrieval
The first part of our investigation examines the behaviour of the separate document information fields in the CLIR framework. We are particularly interested here in the impact
of translation errors or inconsistencies on retrieval effectiveness given the noise in the ASR
transcripts, the shortness of the title field, and the inconsistencies of the description field.
We examine this question by evaluating the CLIR robustness of each field to measure how
the retrieval effectiveness behaves in the CLIR framework. Throughout our investigation in
this paper, we define CLIR Robustness as how well a field or source of evidence performs
in the CLIR framework. We observe this by computing the significance of change between
the CLIR and monolingual performance using the same setting and across all query sets.
To run our CLIR robustness evaluation experiment, we compare the CLIR effectiveness of
each field against a monolingual baseline:
 ASR index contains only the ASR transcript fields.
 Title index contains only the title fields.
 Desc index contains only description fields.
258

fiCross-Lingual Search For User-Generated Internet Video

Table 3: Mono vs. CLIR performance per index

Title index
ASR index
Desc index

Mn-Sh
0.239
0.4275
0.2154

CL-AR-Sh
0.2288
0.2748
0.1943

CL-IT-Sh
0.2383
0.3873
0.2102

Mn-Lg
0.2827
0.4513
0.2432

CL-AR-Lg
0.2244
0.3487
0.2285

CL-FR-Lg
0.2239
0.3833
0.2316

5.1 Retrieval Model
Our single field retrieval experiments were carried out using the Terrier retrieval engine3 .
Terrier is a standard open source IR toolkit providing many of the best established retrieval
algorithms and widely used by the IR research community. Stop-words were removed based
on the standard Terrier list, and stemming performed using the Terrier implementation
of Porter stemming. We used the PL2 model4 , a probabilistic retrieval model from the
Divergence From Randomness (DFR) framework (Gianni, 2003). The reason we selected
this model over other available retrieval models is the characteristics of our data collection.
Previous studies such as(Amati & Van Rijsbergen, 2002) have shown that PL2 has less
sensitivity to length distribution compared to other retrieval models and works better for
experiments that seek early precision, which aligns with our known-item experiment. PL2 is
thus suitable since our Internet based data collection has huge variation in lengths, whether
at the field level or the document level, as shown in Table 1. The PL2 document scoring
model is defined as shown in Equation 2, where Score(d, Q) is the retrieval matching score
for a document d for query term t and  is the Poisson distribution of F/N , F is the query
term frequency of t over the whole collection and N is the total number of documents at
the collection. qtw is the query term weight given by qtf /qtf max; qtf is the query term
frequency and qtf max is the maximum query term frequency among the query terms.
tfn is the normalized term frequency defined in Equation 3, where l is the length of the
document d. avgl is the average length of the documents, and c is a free parameter for the
normalization. To set the parameter c, we followed the empirically determined standard
settings recommended by Amati and Van Rijsbergen (2002) and He and Ounis (2007b),
which are c = 1 for short queries and c = 7 for long queries.

Score(d, Q) =

X
tQ

qtw .

1
tfn
(tfn log2
+ (  tfn ). log2 e + 0.5 log2 (2.tfn ))
1 + tfn


tfn =

X

(tf. log2 (1 + c.

d

avgl
)), (c > 0)
l

(2)

(3)

5.2 Experimental Results and Discussion
Our results for each index are shown in Table 3, these show that MRR is lower in all
cases for the CLIR task. Thus retrieval effectiveness of all fields is negatively impacted
3. http://www.terrier.org/
4. Terrier implementation of this model can be found in :
http://terrier.org/docs/v4.0/javadoc/org/terrier/matching/models/PL2.html

259

fiKhwileh, Ganguly, & Jones

Table 4: AR CLIR - the t-values according to the % MRR reduction for each index
CL-AR-Sh CL-AR-Lg
Title index
-1.69
-1.73
ASR index
-1.94*
-2.50*
Desc index
-0.829
-0.44
*Statistically significant values with p-value < 0.05.
Table 5: FR and IT CLIR - the t-values according to the % MRR reduction for each index
CL-IT-Sh CL-FR-Lg
Title index
-0.05
-1.77
ASR index
-1.58
-2.04*
Desc index
-0.32
-0.47
*Statistically significant values with p-value < 0.05.
for CLIR. This confirms the expected additional retrieval challenge that arises from the
imperfect query translation. MRR for the Arabic queries is reduced to a higher degree than
for the French and Italian queries. This is most likely due to the relative difficulty of Arabic
MT (Alqudsi, Omar, & Shaker, 2012). One significant challenge for Arabic to English
MT relates to named entities. For instance, a query including the word dreamweaver (the
proprietary web development tool) was expressed as dreamweaver for both FR and IT,
	
while for AR, it was represented by Q
P Y@ which resulted in it being an OOV term
for Google Translate and being transliterated into a completely different word Aldirimovr
which was not useful for retrieval using the English language metadata.
Further, looking at reduction in MRR for each index indicates they have different responses to the query translation; notably the impact is greatest on the index of the ASR
transcript field across all languages using both short and long queries.
To better understand the significance of these CLIR reductions in MRR, we computed
the statistical significance of each reduction. We calculated the t-value for the difference at
the 95% confidence level after representing all monolingual and CLIR MRRs in pairs at every
query level. The significance test results in terms of t-values for the indexes searched for
the Arabic CLIR queries are shown in Table 4 and for the French and Italian CLIR queries
in Table 5. Looking at the t-values, we can observe that IT queries were less challenging
than the others since the performance was not significantly different from monolingual.
Furthermore, both Tables 4 and 5 indicate that the ASR transcripts do indeed have
the lowest robustness in the CLIR setting. On searching the single-field indexes, for both
long and short queries, ASR index had the least robustness with a statistically significant
negative reduction in Arabic and in French with (p<0.05). For the Italian short queries,
MRR reduction rates of the ASR index (ASR index) were not statistically significant, but
still had the highest negative impact over other fields.
We conclude from this experiment that even if they are incomplete, short and/or sometimes unreliable, the user-uploaded titles and meta descriptions are more robust in the CLIR
setting than the ASR fields. As noted earlier, the degree of ASR recognition errors may
vary from one video to another on Internet, due to the wide variation in the audio quality.
260

fiCross-Lingual Search For User-Generated Internet Video

The interaction between recognition error rate, document length and retrieval behaviour
is highly complex, as observed by Eskevich and Jones (2014), and we plan to explore this
effect in more detail in future work with a view to improving the CLIR robustness of the
ASR transcript field.

6. Retrieval with Combined Metadata Fields
Having examined the effectiveness of the three separate fields for monolingual retrieval and
CLIR, in this section we explore the potential of combining them for improving retrieval
effectiveness. For this investigation, we carried out another set of experiments that combined
the evidence from the individual fields. For this, we first combine the fields in pairs, and
then as shown in Figure 1, we integrate the three fields but with varied field weighting.
6.1 Retrieval Model
For the combined field experiments we use the DFR PL2F model5 (Macdonald, Plachouras,
He, Lioma, & Ounis, 2006). This is a modified version of the PL2 model used in the
previous section. The PL2F model is designed to adopt per-field weighting when combining
multiple evidence fields into a single index for search. The term frequencies from document
fields are normalised separately and then combined in a weighted sum. PL2F uses the same
document scoring function as PL2, shown in Equation 2, but here tf n is the weighted sum
of the normalised term frequencies in the normalised term frequencies tfX for each field x,
in our case x  (ASR, title, desc) as indicated by Equation 4. Where lx is the length of the
field x in document d. avglx is the average length of the field x across all documents, and
cx , wx are the per-field normalization parameters. This per-field normalization feature in
PL2 modifies the standard PL2 document scoring function to include the weighted sum of
the normalised term frequencies tfx .
tfn =

X

(wx .tfx . log2 (1 + cx .

x

avglx
)), (cx > 0)
lx

(4)

tfx also needs two parameters wx , cx to be set. Hence, for scoring each indexed document
we need to set these parameters:
Cx is the set of per-field length normalization parameters cx that need to be set for
every field as Cx ={ c asr, c title, c desc}, and Wx is the set of per-field boost factors wx
that need to be set for each field as Wx ={ w asr, w title, w desc}.
6.2 Two Field Combinations
Table 6 shows MRR values for fields combined into pairs for which were indexed using the
PL2F retrieval model. We are interested here in the potential for improved retrieval using
fields in combination. Comparing the results in Table 6 and the earlier results shown in
Table 3, we can see that field combination is more effective for both monolingual and CLIR
tasks. Further improvement could probably be obtained by weighting fields differently.
5. Terrier implementation of this model can be found in http://terrier.org/docs/v4.0/javadoc/org/
terrier/matching/models/PL2F.html

261

fiKhwileh, Ganguly, & Jones

Table 6: Mono vs. CLIR performance with field pair combinations

TitleDesc index
ASRDesc index
ASRTitle index

Mn-Sh
0.2503
0.4394
0.4295

CL-AR-Sh
0.2421
0.3624
0.3676

CL-IT-Sh
0.2463
0.3951
0.3820

Mn-Lg
0.3020
0.5245
0.4527

CL-AR-Lg
0.2795
0.3905
0.3451

CL-FR-Lg
0.2614
0.4326
0.3768

Table 7: Weighting scheme Wx for the single-weighted retrieval models
PL2ASR
PL2Title
PL2Desc

ASR
wx
1
1

Title
1
wx
1

Desc
1
1
wx

However, the main goal of our investigation is the potential for combining all three fields,
and we explore this in more detail in the next section.
6.3 Three Field Combinations
In this section we describe our investigation of the retrieval effectiveness with combination
of all three fields. We explore giving higher weight to a specific field over the others.
To set the values for our proposed single-weighted retrieval models we adopted the
following steps:
 Construct a model based using the PL2F document scoring that targets a single field
x from each (ASR, title, desc): PL2FASR, PL2Title, PL2Desc.
 Assign an equal cx value to all fields to allow full-length normalization for the term
frequency of each field as in Cx = {1,1,1} for short queries, and Cx ={7,7,7} for long
queries. We also followed the empirically standard settings recommended by Amati
and Van Rijsbergen (2002), and He and Ounis (2007b).
 For Wx , we set the wx value for the targeted field, and the rest to be fixed at 1, to
give priority for field x over the others, as in Wx = {wx ,1,1}. The reason why we
chose them to be 1 was to allow for the presence of their term frequencies, but with
normal (is not boosted) weights.
The combination weighting schemes are shown in Table 7, in each case one field has a
weight boost wx . To examine retrieval behaviour, we vary wx boost parameters for each
model in the range 1 to 60 using increments of 1. The first weighting iteration at the
weighting point wx = 1 is the same for all models where they have Wx = {1,1,1}.
6.3.1 Experimental Results and Discussion
Figure 3 shows the MRR performance at each weighting point for the long queries (CL-ARLg and CL-FR-Lg query sets), and the short queries (CL-AR-Sh and CL-IT-Sh query sets).
As can be seen in Figure 3, fields behave differently with weight boosting. The best CLIR
262

fiCross-Lingual Search For User-Generated Internet Video

Figure 3: MRR CLIR performance for the single weighted models across all weighting points
(wx) using both short and long query sets.
precision performance is always achieved by giving a higher weight to the title field for all
of the AR, IT and FR query sets. Across all the weighting points and all languages pairs,
the PL2Title model shows higher performance than the other fields for both short and long
query sets.
Moreover, it can also be seen from these figures, that we get lower performance when we
give progressively higher weights to the ASR and Desc fields. The strong CLIR performance
of the PL2Title model indicates the stability of title fields for our Internet videos over the
other fields. Also, the fact that the titles may have been written by the video uploader with
more attention than the descriptions could be referred to the following reasons:
 The uploader thought it is important to have a high quality title for his video since
it would help in promoting it on the video-sharing site.
 The uploader believed that it has more importance since it is shown at the header of
his video, while the description is generally shown below the video and may not be
examined at all by the viewer.
263

fiKhwileh, Ganguly, & Jones

It could be also the case that for the known-item queries, the users who wrote the queries
had viewed the videos and might be more likely to include the titles of the videos in their
query to find the intended video, because they believe that it would be easier find them
using the title of the video. However, it should be noted that the MTurk task that was
used to create the queries for the Search and Hyperlinking Mediaeval task did not display
the video title while the user was writing the query which was created with the intention of
being suitable to re-find the known-item video.
Table 8: Single index Mono vs. CLIR Recall performance represented by the number of
found documents with cut-off values of 10, 50, and 100.

Title index
ASR index
Desc index
TitleDesc index
ASRDesc index
ASRTitle index
All Index

Mn-Sh
19
35
17
20
35
35
37

Title index
ASR index
Desc index
TitleDesc index
ASRDesc index
ASRTitle index
All Index

Mn-Sh
25
42
29
33
47
42
47

Title index
ASR index
Desc index
TitleDesc index
ASRDesc index
ASRTitle index
All Index

25
46
34
38
50
46
50

10 - cut off
CL-AR-Sh CL-IT-Sh
16
19
31
34
15
15
17
19
30
35
31
34
33
37
50 - cut off
CL-AR-Sh CL-IT-Sh
23
23
38
42
23
27
28
30
41
44
38
42
40
47
100 - cut off
23
24
41
45
27
31
32
35
47
49
41
45
45
50

Mn-Lg
23
34
23
25
40
34
40

CL-AR-Lg
18
29
21
21
32
28
33

CL-FR-Lg
19
29
20
22
33
38
34

Mn-Lg
30
43
31
37
46
43
48

CL-AR-Lg
28
37
27
29
40
36
41

CL-FR-Lg
26
39
28
31
43
40
45

32
47
34
42
50
46
52

29
40
32
35
43
39
43

29
43
32
38
47
42
49

Comparing the MRR for PL2Title with the values shown in Table 3, it can be also seen
that the performance for PL2Title is almost double the one obtained by the independent
Title field (Title index). While the MRR values for the ASR and Desc fields are similar
between the two experiments. As the wx increases for the Title field, we can see that there
is some further improvement, with the optimal weight depending on the query length and
the language pair. In an attempt to better understand how the field combination improves
retrieval effectiveness, we examined the Recall of the individual fields and the combinations.
264

fiCross-Lingual Search For User-Generated Internet Video

Table 8 shows the total number of known-items retrieved in the top 10, 50 and 100 for each
field set. It can be seen here that the Title field has lowest recall in isolation, but that it
can boost the Recall of the other fields when used in combination. The results in Figure 3
suggest that the title field brings additional evidence without bringing noise, which is not
the case for Desc and ASR fields which degrade effectiveness when their weight is increased.

7. Query Expansion Using Combined Metadata Fields
In Sections 5 and 6, we analyzed the effectiveness of each data source (ASR, Title and
description) in the CLIR framework and showed that overall performance is more robust
when these fields are combined together in an optimal way. We also showed that adjusting
the retrieval settings to give higher weight to more a reliable data source, i.e. the Title, in
comparison to other less reliable fields, can benefit the overall CLIR performance.
In this section, we seek to modify the query itself using the field information to improve
the retrieval effectiveness. The query modification strategy we explore is based on the query
expansion (QE) techniques (Carpineto & Romano, 2012), where we use different sources of
evidence (fields) to enrich the original query. The underlying motivation behind applying
QE is that expanding the query to include important terms should make it more effective
in identifying relevant items. Ideally, the QE should alleviate the impact of the translation
errors by adding more terms from top ranked videos that are either relevant or related to the
query. The QE effectiveness relies on the quality and the informativeness of the top ranked
documents (Amati, Carpineto, & Romano, 2004). The Top ranked documents can be taken
from external resources such as WordNet and Wikipedia (Pal, Mitra, & Datta, 2013) or from
the local document collection itself by considering the top ranking documents as relevant
to the query. We consider more interesting/challenging approach here, which is the local
collection expansion approach since we aim to investigate how these noisy collection of web
videos can be utilized to improve the query effectiveness. We, nevertheless, plan to consider
the use of the external collections for QE in our future investigation.
Existing research has shown that QE techniques can be useful for improving both monolingual and CLIR effectiveness in many languages (Bellaachia & Amor-Tijani, 2008). However, most of this research has focused primarily on collections of professionally written or
formal text which has none or a minimum amount of noise. There has been some, but
very limited, work on applying QE techniques for the noisy data, such as the work on OCR
Data (Tong, Zhai, Milic-Frayling, & Evans, 1996; Lam-Adesina & Jones, 2006), and more
recently, on the user-generated informal text (Lee & Croft, 2014). In this section, we are
interested in taking the challenge of applying QE in CLIR settings with a focus on avoiding
problems that may arise from the noise presented in each source of evidence; in particular
for our task, the translation errors of the query, the transcription errors of the ASR as well
as the inconsistency errors of user generated textual metadata. We explore the expansion
of queries based on the top ranked documents. Expansion terms are intended to make the
query more reliable and robust to find the intended relevant item. Our interest in this
section can be summarized with the help of the following research questions:
 If we expand the query using the top ranked documents, how might this affect the
overall CLIR effectiveness? How effective will QE under such a setting of noisy data
collected from Internet videos be?
265

fiKhwileh, Ganguly, & Jones

 Which of the different UGC information sources is more useful for QE? Since these
different sources (fields) have different relative characteristics and behaviour for retrieval, as observed empirically in Sections 5 and 6.
We describe our approach to addressing these questions in the following sections. We
investigate the reliability of each single field for QE and their challenges in Section 7.1. We
then propose an adaptive approach to improve overall QE robustness6 by selecting the best
source for expansion in Section 7.2.
7.1 Query Expansion on Fields
We employ the Divergence From Randomness (DFR) QE mechanism proposed by Gianni
(2003). This technique computes a weight to rank the terms from the top ranking documents. The DFR QE generalizes Rocchios method (Salton & Buckley, 1997) to implement
several term weighting models that measure the informativeness of each term in the pseudo
relevant set.
DFR QE has two stages. First, it applies a DFR term weighting model to measure the
informativeness of the top terms in the top ranking document. The main concept of the
DFR term weighting model is to infer the informativeness of a term by the divergence of its
distribution in the top documents from a random distribution. We use the DFR weighting
model called Bo1, a parameter-free DFR model which uses BoseEinstein statistics to weight
each term based on its informativeness. This parameter free model has been widely used
and proven to be effective (He & Ounis, 2007a; Plachouras, He, & Ounis, 2004; Gianni,
2003). The weight w of a term t in the top ranked documents using the DFR Bo1 model is
shown in Equation 5, where tfx is the frequency of the term in the pseudo-relevant set (top
n ranked documents). Pn is given by F/N ; F is the term frequency of the query term in
the whole collection and N is the number of documents in the whole collection.
w(t) = tfx . log2 (

1 + Pn
) + log2 (1 + Pn )
Pn

(5)

Secondly, the query term weight qtw , which was obtained from the single-pass retrieval (as
described in Equation 2), is further adjusted according to the newly obtained weighting
values of w(t) for both the newly extracted terms and the original ones using Equation 6,
where wmax (t) is indicated by the maximum w(t) values among the expanded query terms.
qtw = qtw +

w(t)
wmax (t)

(6)

To illustrate the QE approach used in our experiments, we provide the QE example of
the CLIR-AR query :
EEE PC 900 Troubleshooting in laptop
The terms pc, laptop, mac, us, classrooms are generated from running the DFR QE
to take the top 5 terms from the top-5 documents. Note that the two expansion terms pc
and laptop also appear in the original query, therefore, the weight for each of these terms
6. QE robustness is interpreted in this context as how likely it is that it will improve retrieval performance
over the baseline, where the baseline is a single-pass retrieval that is using the original query.

266

fiCross-Lingual Search For User-Generated Internet Video

is boosted to be greater than 17 . The new expansion terms (mac, us and classrooms) are
added to the original query and their weights are adjusted based on their informativeness
and uniqueness in the top n documents versus the whole collection. The term mac is
predicted as informative and unique so it gets weight greater than 0 since it appears only in
the top n documents while the other terms (classrooms, us) are assigned very low weights
close to 0 since they also appear in other documents (non top-n). Using this method, the
final expanding and reweighing of the query is explained as follows :
eee1.000, pc1.9211, 9001.0000 ,troubleshoot1.0000, laptop1.2988, mac0.2195,
us0.0000, classroom0.0000.
Some of the original query terms may not appear in the top-terms such as (900, EEE
and troubleshoot), in which the formula in Equation 6 would only give them the same
weight they have in a single-pass retrieval settings. Such cases might be common in our
CLIR settings due to the presence of named entity translation errors, such as the example of
Aldirimovr as described in Section 5. Since these NEEs are produced by the translation,
they will never appear in any of the top ranked documents since they are not present in
the collection, therefore their weight will always remain the same as after the expansions.
This will pose an extra challenge for overall QE effectiveness which we try to handle in the
following sections by designing a post-translation QE that is tuned for this task. The main
reasons for us to pick the post-translation QE approach in this experimental investigation
are :
 To study the impact of translation errors/translation quality on the QE effectiveness.
This can be investigated by running a post-translation analysis of the query expansion
performance.
 To investigate whether adding new informative terms to the query can reduce the
impact of the translation errors and improve the retrieval effectiveness.
We use the default QE parameter settings in our experiment where we set it up to
extract the 10 most informative terms from the top 3 returned documents. These settings
were suggested by Gianni (2003) after conducting extensive experiments on several test
collections. However, in our case, the task is much more challenging, where we only have
one relevant document to find, we also extend these settings to explore other possible
parameter combinations. The parameter settings for our proposed QE runs are tuned to
explore the top (3, 5, 10) terms from each of the top (3, 5, 7) documents. Our QE runs
parameters combinations are as follows:
 Taking the top 3 terms, this includes QE runs that take the top 3 terms from the top
3 documents, the top 5 and the top 10 documents.
 Taking the top 5 terms, this includes QE runs that take the top 5 terms from the top
3 documents, the top 5 and the top 10 documents.
 Taking the top 10 terms, this includes QE runs that take the top 10 terms from the
top 3 documents, the top 5 and the top 10 documents.
7. 1 is the normal weight for any term that appears on the original query

267

fiKhwileh, Ganguly, & Jones

Table 9: Optimized parameters (top-terms and top-doc) that is selected for each QE run

exp-ASR
exp-Title
exp-Desc
exp-All

Terms
5
3
3
3

Docs
5
3
3
3

To study the best parameters for each field expansion, we explored all the parameter variations. We then chose the best performing setting for each QE run. These optimized settings
are shown in Table 9.
A QE framework using fields for monolingual text retrieval was proposed by He and
Ounis (2007a). This suggests an improved term-weighting method based on field statistics
to achieve better retrieval performance. In this investigation, we adopt this approach to
CLIR QE and further tune it to a single-field QE technique that allows us to assess the
effectiveness of each field for QE. In this method, QE is performed as follows:
 The top n terms are extracted from the top n documents retrieved in response to
executing the query on each separate field type (title, description and ASR) in order
to investigate their individual effectiveness for QE.
 Retrieval is carried out similarly to our setting for the experiment in Section 6, where
we combine all the fields together (see Figure 2), and give an equal weight of 1 to each
field. We use the PL2F model (described in Section 6) for this retrieval experiment.
Since both query sets (long/short) led to similar conclusions in the previous Sections
regarding field effectiveness, we chose to run QE for the short queries only (the CLIR
queries of CL-AR-Sh and CL-IT-Sh are described in Section 4.1).
We conducted several QE runs based using each individual field and their combinations.
The reason for having such tuned runs is to assess the effectiveness of each field combination
for QE. Our proposed field-based QE runs are as follows.
 exp-ASR: Queries were expanded using the ASR field only, by only taking the top
ranking terms from the top documents which are retrieved from the ASR index.
 exp-Title: Queries were expanded using the Title field only, by taking the top ranking
terms from the top documents which are retrieved from the Title index.
 exp-Desc: Queries were expanded using the Desc field only, by taking the top ranking
terms from the top documents which are retrieved from the Desc index.
 exp-All : Queries were expanded using a combination of all fields, i.e. ASR, Title
and Desc.
 exp-Non : This run skips the expansion of the queries and just does single pass
retrieval using the original query. Note that we use this as the baseline since we want
to assess how effective QE can be for such a challenging task.
268

fiCross-Lingual Search For User-Generated Internet Video

Table 10: MRR performance for each QE run.

exp ASR
exp Title
exp Desc
exp All
exp Non

CL-AR-Sh
0.3502
0.3820
0.3470
0.3571
0.3726

CL-IT-Sh
0.3735
0.4060
0.4090
0.4069
.4081

Table 11: Overall Recall (total found known-items) for each QE run.

exp ASR
exp Title
exp Desc
exp All
exp Non

CL-AR-Sh
53
52
51
53
52

CL-IT-Sh
57
56
56
56
56

We run our field-based QE experiments to explore MRR performance for each field.
The MRR performance across different runs is shown in Table 10, while the overall recall
results are shown in Table 11. It can be seen that MRR for the proposed runs (exp-All,
exp-ASR, exp-Title, exp-Desc) does not improve over the baseline run (exp-Non). The expTitle and exp-Desc get more or less similar MRR performance, while the exp-ASR achieves
significantly lower MRR. The fact that we have multiple sources of noise coming either from
the translation or from the fields themselves, together with the fact that this is known-item
task, where there is only one relevant item that may not be highly ranked in the initial
search, can justify the ineffectiveness of these QE runs.
To better understand the robustness for each QE run and compare it to the baseline, we
study the difference of MRR (MRR) at each query level for all the proposed runs over the
baseline run (exp-Non). Where the MRR on a particular query level for a QE run (exp-x)
is indicated through MRR = (MRR(exp-x) - MRR(exp-Non)). The MRR results
for each CL-AR-Sh query across all the runs are shown in Figure 4, while Figure 5 shows
the MRR results for CL-AR-Sh queries.
Since the DFR QE model (see Equation 5 and 6) uses the informativeness measure to
weight the extracted top terms, the decreases and increases of the MRR values that are
shown in Figures 4 and 5 can be explained as follows.
 MRR = 0, means the top ranked terms were predicted as less informative, which is
the reason why the QE runs have minimal effect over the baseline (exp-Non). Based on
the DFR definition of the informativeness (see Equation 5 and 6), this indicates that
the terms added to this particular query were given low weight because they are not
only common in the top ranked documents, but also in the whole document collection.
In other words, the QE run could not find any helpful terms that might potentially
improve the overall performance. The MRR performance of each run suggests that
this situation occasionally occurs in all QE runs across several queries, particularly on
269

fiKhwileh, Ganguly, & Jones

the exp-Title and exp-Desc. This probably arise due to the recall problem that these
two fields have, as shown in Table 8 (see also the single-field retrieval experiments
of Section 5). In fact, for most of the queries for the exp-Desc and exp-Title runs,
the application of QE has a very low or a zero effect on the MRR compared to the
single-pass baseline retrieval (exp-Non).
 MRR > 0, means that the top ranked terms were predicted to be highly informative
and they were relevant to the query, which is the reason why QE shows a positive
increase over the baseline. This in turn suggests that the exp-ASR runs were able to
improve more queries (more positive MRR points) in terms of ranking the knownitem over other runs. This is also expected due to the higher recall performance of
these fields.

1.0

1.0

0.0 0.5 1.0

expASR

0.0 0.5 1.0

expAll

0

10

20

30

40

50

60

0

10

30

40

50

60

50

60

1.0

1.0

0.0 0.5 1.0

expDesc

0.0 0.5 1.0

expTitle

20

0

10

20

30

40

50

60

0

10

20

30

40

Figure 4: MRR across all QE runs on CL-AR-Sh.
Moreover, all runs have had a positive MRR for some queries. Particularly interesting is the fact that the combined QE run (exp-All) did not yield the best results.
 MRR < 0, means that the top ranked terms were predicted as highly informative
but they were not relevant to the query, which is the reason why QE had a negative
effect over the baseline run. The MRR values show that this incorrect prediction
has significantly impacted all runs. In particular, the exp-ASR run is affected the
most, as can be seen from the negative MRR values across several queries on both
CL-AR-Sh and CL-IT-Sh.
We conclude from these experiments that setting the expansion to be based on the Title and
the Desc fields may help to improve the MRR by improving the ranking for some queries.
However, this improvement covers only a limited number of queries due to the coverage and
270

fiCross-Lingual Search For User-Generated Internet Video

1.0

1.0

0.0 0.5 1.0

expASR

0.0 0.5 1.0

expAll

0

10

20

30

40

50

60

0

10

30

40

50

60

50

60

1.0

1.0

0.0 0.5 1.0

expDesc

0.0 0.5 1.0

expTitle

20

0

10

20

30

40

50

60

0

10

20

30

40

Figure 5: MRR across all QE runs on CL-AR-Sh.
recall issues of these two fields (see Section 5). ASR fields seem to have better coverage in
terms of improving the performance for most of the queries. However, this coverage can be
negative sometimes when QE model fails to pick the right terms.
Even when the ASR is combined with other fields using the exp-All approach, the overall
performance still does not improve the MRR with respect to the baseline (exp-Non). As
shown in these experiments, each QE run that uses a single field shows positive MRR
values for some queries. However, these per-field improvements decrease the average when
the fields are combined together (for example, the exp-All MRR values shown in Table
10 are lower than exp-Title for CL-AR-Sh, exp-Desc for CL-IT-Sh). The combination
approach is not sufficiently effective because the overall retrieval performance can still be
negatively impacted by the noise present in the fields. In the next section, we propose a
novel approach for alleviating this issue by selecting only the best source for expansion.
This technique adaptively predicts performance of each QE run, and chooses the source
that is more likely to achieve a positive impact, as elaborated in the next section.
7.2 Selecting the Best Source for Expansion
In this section we introduce our proposed approach to improve the QE effectiveness for our
CLIR task by predicting whether QE is needed or not, and if it is, to select the best source
for expansion. In particular, we aim to design a robust QE approach that can prevent or
minimize the negative MRR changes that the exp-ASR or other runs can have. We argue
that these reductions are caused for two reasons:
 The query performance of the intiail run is just perfect for our known-item task.
This case can happen if the known relevant document is ranked at first position. So
271

fiKhwileh, Ganguly, & Jones

the added new expansion terms can potentially disturb the query performance and
negatively impact the retrieval effectiveness.
 As mentioned in the literature (Mitra, Singhal, & Buckley, 1998; Terra & Warren,
2005), QE effectiveness is further challenged by the query drift issue where the expansion terms are informative, but they to do not belong the same topic of the original
query. As shown in Table 1, ASR transcripts can be up to 20K length, Desc can be
as long as 3K length. These long descriptive fields may cover many different topics
and noise, and may not be relevant to the topic of the query. This is common for the
dataset used in this task, since we are using user-generated videos where the length,
the topic and the ASR quality of each video may have no specific or consistent theme.
To address the above issues we propose a modified QE technique that use a pre-retrieval
prediction technique to decide whether QE is likely to be beneficial and which source is more
reliable for the expansion. We describe the prediction technique used for this approach in
Section 7.2.1. Section 7.2.2 explains our proposed Adaptive QE algorithm. Section 7.2.3
reports the experiments we conduct to investigate the effectiveness of this approach.
7.2.1 Predicting the QE Effectiveness
Predicting QE effectiveness has been proposed within the concept of the selective QE framework by Amati et al. (2004), and has been widely used to improve QE effectiveness. The
main concept behind this technique is to disable QE when it is predicted to have a negative impact on the first-pass retrieval performance. This prediction is based on pre-retrieval
metrics that assess the application of QE and make a decision on whether to apply it or not.
Most of these predictions are based on capturing the query statistics in the collection, such
as the query difficulty (Gianni, 2003), the query clarity score (Cronen-Townsend, Zhou, &
Croft, 2002), query length (Amati et al., 2004). Full analysis of the effectiveness of every
prediction techniques effectiveness is contained in the work of He and Ounis (2006). We
chose to use one of the most successful predictors, the Average Inverse Collection Term
Frequency (AvICTF) (He & Ounis, 2006). We use this prediction metric in our proposed
Adaptive QE technique to predict whether QE is needed or not and which field combination
is the most reliable for the expansion.
The AvICTF predictor has previously been tested in the field-based query expansion
in several previous studies (He & Ounis, 2007a; Macdonald, He, Plachouras, & Ounis,
2005). The reason to choose this predictor over others is that by definition it has a higher
potential to work well for CLIR because it addresses the term frequency aspect of the query.
In principle, the term frequency should be a good indicator for predicting translation errors.
For example an NEE error like the word Aldirimovr would have zero term frequency which
can be used to give an indication of overall query performance. AvICTF is also a very low
cost metric that uses only local collection statistics to make the prediction, if the AvICTF
value is higher than a tuned threshold, then the query is predicted to preform very well
using first-pass retrieval and therefore the query expansion is disabled. The AvICTF is
defined as follows.
Q
coll
log2 Q ( token
)
F
AvICT F =
(7)
ql
272

fiCross-Lingual Search For User-Generated Internet Video

where tokencoll is the number of tokens in the whole collection. ql is the query length, F
is the query term frequency of the whole collection. In the next sections, we describe the
adaptive QE algorithm we implemented and our experimental investigation.
7.2.2 Adaptive Field-Based QE Technique
In this section we propose our adaptive QE technique, the adaptivity concept is inspired by
the work of He and Ounis (2006). The idea behind this adaptive method is to automatically
set the weights of the fields during the query running time. We further redesign this concept
to auto-select which field combination is the best source for the expansion. The adaptive
field-based QE algorithm we implemented is explained below:
 During indexing, the algorithm processes all possible fields combination into separate
indexes, where each index can have one, two or n fields, n is the total number of fields
available in the collection.
 During the retrieval of a query Q:
1. The algorithm calculates the prediction value V across every possible index,
then selects the index M axindex that has the highest prediction value M axV .
2. The algorithm then makes a decision of whether to apply QE or not by comparing the prediction value M axV against a trained threshold. If the predicted
value is less than the threshold, the algorithm skips QE and moves straight to
the retrieval.
3. If the prediction decision is to proceed with QE, the algorithm expands the
query Q by taking the top terms from the top ranked documents in M axindex
then run the retrieval.
Our adaptive QE algorithm predicts the effectiveness of each possible QE field run and
chooses the one that is more likely to be useful for each query. In other words, our adaptive
QE make the use of (exp-ASR, exp-Title, exp-Desc, exp-Non, exp-All) as well as other
possible QE runs that are based on two fields combination such as (exp-ASRDesc, expTitleASR, exp-TitleDesc) to produce adaptive QE runs as follows.
 exp-Adapt-AR: Expands the AR queries (CL-AR-Sh) using the proposed adaptive
QE technique.
 exp-Adapt-IT: Expands the IT queries (CL-IT-Sh) using the proposed adaptive QE
technique.
For training the prediction and tuning the threshold value, we conduct a two-fold holdout
evaluation on our query sets. We divide both the CL-AR-Sh and CL-IT-Sh sets into training
and testing query sets as described below:
 AR-train queries set: Contains 30 queries picked randomly from the CL-AR-Sh set.
 IT-train queries set: Contains 30 queries picked randomly from the CL-IT-Sh set.
273

fiKhwileh, Ganguly, & Jones

 Test queries set: Contains the remaining 30 queries from CL-AR-Sh query set and the
remaining 30 queries from CL-IT-Sh query set. These two sets are used to evaluate
the proposed adaptive QE runs (exp-Adapt-AR and exp-Adapt-IT).
During the training, similar to the work of He and Ounis (2007a), we perform a manual
data sweeping through the range of [3, 15] with an interval of 1 for both training queries.
The best threshold chosen for exp-adapt-AR on AR-train queries was 6. The best threshold
found for exp-adapt-IT on IT-train queries was 9. The difference between these threshold
values can be attributed to the distinct level of translation qualities between both query
sets. As we discussed previously in Section 5, IT queries have better CLIR performance
over the AR ones, and thus QE threshold is different here.
7.2.3 Experimental Results and Discussion
We ran our proposed adaptive QE using the two runs: exp-Adapt-AR and exp-Adapt-IT
explained previously. The overall performance results of the two runs for the test queries is
shown Table 12.
As we explained before, these adaptive QE runs involve all possible QE runs for onesingle run, and use the one that is predicted to have better performance, the selection
statistics of each run is indicated in Table 13. As can be seen in Table 12, the overall MRR
Table 12: Results for adaptive QE runs in terms of MRR and Recall.

exp-Adapt-AR
exp-Adapt-IT

exp-Adapt-AR
exp-Adapt-IT

MRR
Adaptive
0.43614
0.4867
Recall
Exp-Adapt
22
22

(Baseline exp-Non)
0.3785
0.4580
Exp-Non (Baseline)
22
22

Table 13: The selection statistics for the adaptive QE runs.

exp-ASRDesc
exp-TitleASR
exp-TitleDesc
exp-Title
exp-Desc
exp-ASR
exp-Non
exp-All

exp-Adapt-AR
4
7
4
1
2
3
5
4

exp-Adapt-IT
6
7
5
2
0
2
8
0

performance improves when using our proposed adaptive QE technique. Also, looking at
the recall performance of Table 12, it appears that this technique successfully improves the
274

fiCross-Lingual Search For User-Generated Internet Video

0.5
0.0
0.5
1.0

1.0

0.5

0.0

0.5

1.0

expAdaptAR

1.0

expAdaptIT

0

5

10

15

20

25

30

0

5

10

15

20

25

30

Figure 6: MRR (Baseline Vs Adaptive QE runs) for both languages using the test queries

overall ranking while maintaining the same recall level. This can be attributed to the fact
that these runs are able to selectively make use of the best performance from each individual
run.
To better understand the improvement over the baseline, similar to our previous experiment in Section 7.1, we also calculate the MRR values for each test query for both
AR-train and IT-train query tests in Figure 6. The MRR values in Figure 6 show that
this technique reduces the chance of having any significant reduction in MRR over the baseline. Overall, it appears that this QE approach has robustness to improve the overall CLIR
performance by selecting the most reliable source of expansion individually for each query.

8. Conclusions and Further Research
This paper has examined CLVR based on text metadata fields for an Arabic-English, FrenchEnglish and Italian-English known-item search task based on the blip10000 collection. We
studied the retrieval effectiveness and challenges of three different sources of information:
ASR transcripts, which are challenged by recognition errors, video titles, which can be very
short and lack content, and video descriptions, which can be generic and incomplete. Our
first set of experiments analysed the behaviour of these sources for CLIR by examining their
CLIR robustness. We found that the ASR transcript field has the lowest robustness across
other fields and its performance can drop significantly for CLIR. We then explored field
combination retrieval to explore their performance all together, and our investigation showed
that giving higher weight to the titles over other fields gives improved CLIR performance.
In general our experiments show that tuning the retrieval settings to give a higher weight
towards the fields which have a lower CLIR robustness degrades retrieval effectiveness.
275

fiKhwileh, Ganguly, & Jones

Our work also investigated the effectiveness of automatic query expansion on the CLIR
setting for this task. We found that these information sources can have a varying reliability
for query expansion, and can have a negative impact on the retrieval effectiveness in the
CLIR framework when they are combined together. We proposed an adaptive query expansion technique that automatically selects the most reliable source for expansion based
on a well established query performance prediction technique. The results from our experimental investigation show that this technique has better robustness to maintain retrieval
performance in the CLIR setting.
In general, we found that in this noisy CLIR setting, between the translation errors,
transcription errors, incorrect or incomplete UGC metadata and the very varied document
lengths, there might be no single best solution that can be recommended to be used for
answering all queries or even expanding/ improving them, but rather an adaptive approach
that relies on trained heuristics that are tuned specifically for this task and this collection.
The concept of adaptivity will be further studied in our next investigations of this problem.
Our analysis of the CLIR effectiveness for UGC video gives us suggestions for further
investigation in many areas. One potential direction for further work is to automatically
assess the quality of ASR transcripts and the Description information and assign weights
based on quality measures, and also to explore task dependent tuning of the machine translation process. Studying the CLIR effectiveness of other UGC sources of evidence (such as
visual information and social interaction data which include tweets, personal profile data)
would be an interesting follow up investigation. Another area for future study is to improve
the document representation within this framework by developing a document expansion
technique that is tuned to improve the overall CLIR robustness and effectiveness of each
source of evidence. Also, we plan to expand this work by studying the effectiveness of other
available CLIR techniques such as Hybrid and DT CLIR for this task.

References
Alqudsi, A., Omar, N., & Shaker, K. (2012). Arabic machine translation: a survey. Artificial
Intelligence Review, 124.
Alzghool, M., & Inkpen, D. (2008). Cluster-based model fusion for spontaneous speech
retrieval. In Proceedings of the ACM SIGIR Workshop on Searching Spontaneous
Conversational Speech, pp. 410. Citeseer.
Amati, G., Carpineto, C., & Romano, G. (2004). Query difficulty, robustness, and selective
application of query expansion. In Advances in information retrieval, pp. 127137.
Springer.
Amati, G., & Van Rijsbergen, C. J. (2002). Probabilistic models of information retrieval
based on measuring the divergence from randomness. ACM Transactions on Information Systems (TOIS), 20 (4), 357389.
Amazon (2015). Amazon mechanical turk - welcome. https://www.mturk.com/. Retrieved:
2015-03-30.
Bagdouri, M., Oard, D. W., & Castelli, V. (2014). Clir for informal content in arabic forum
posts. In Proceedings of the 23rd ACM International Conference on Conference on
Information and Knowledge Management, pp. 18111814. ACM.
276

fiCross-Lingual Search For User-Generated Internet Video

Ballesteros, L., & Croft, W. B. (1997). Phrasal translation and query expansion techniques
for cross-language information retrieval. In ACM SIGIR Forum, Vol. 31, pp. 8491.
ACM.
Ballesteros, L., & Croft, W. B. (1998). Resolving ambiguity for cross-language retrieval. In
Proceedings of the 21st annual international ACM SIGIR conference on Research and
development in information retrieval, pp. 6471. ACM.
Bellaachia, A., & Amor-Tijani, G. (2008). Enhanced query expansion in english-arabic
clir. In Database and Expert Systems Application, 2008. DEXA08. 19th International
Workshop on, pp. 6166. IEEE.
Bendersky, M., Garcia-Pueyo, L., Harmsen, J., Josifovski, V., & Lepikhin, D. (2014). Up
next: retrieval methods for large scale related video suggestion. In Proceedings of
the 20th ACM SIGKDD international conference on Knowledge discovery and data
mining, pp. 17691778. ACM.
Bing (2015). Bing translator. http://www.bing.com/translator/. Retrieved: 2015-03-30.
BlipTV (2015). Bliptv. https://www.blip.tv. Retrieved: 2015-03-30.
Buttcher, S., Clarke, C. L., & Cormack, G. V. (2010). Information retrieval: Implementing
and evaluating search engines. Mit Press.
Carpineto, C., & Romano, G. (2012). A survey of automatic query expansion in information
retrieval. ACM Computing Surveys (CSUR), 44 (1), 1.
Chelba, C., Bikel, D., Shugrina, M., Nguyen, P., & Kumar, S. (2012). Large scale language
modeling in automatic speech recognition. arXiv preprint arXiv:1210.8440.
Chen, H.-H., Hueng, S.-J., Ding, Y.-W., & Tsai, S.-C. (1998). Proper name translation in
cross-language information retrieval. In Proceedings of the 17th international conference on Computational linguistics-Volume 1, pp. 232236. Association for Computational Linguistics.
CLEF (2015a). The clef initiative (conference and labs of the evaluation forum) - clef2009.
http://www.clef-initiative.eu/edition/clef2009. Retrieved: 2015-03-30.
CLEF (2015b). The clef initiative (conference and labs of the evaluation forum) - homepage.
http://www.clef-initiative.eu/. Retrieved: 2015-03-30.
Cronen-Townsend, S., Zhou, Y., & Croft, W. B. (2002). Predicting query performance. In
Proceedings of the 25th annual international ACM SIGIR conference on Research and
development in information retrieval, pp. 299306. ACM.
Eickhoff, C., Li, W., & de Vries, A. P. (2013). Exploiting user comments for audio-visual
content indexing and retrieval. In Advances in Information Retrieval, pp. 3849.
Springer.
Eskevich, M. (2014). Towards effective retrieval of spontaneous conversational spoken content. Ph.D. thesis, Dublin City University.
Eskevich, M., & Jones, G. J. F. (2014). Exploring speech retrieval from meetings using the
ami corpus. Computer Speech & Language.
277

fiKhwileh, Ganguly, & Jones

Eskevich, M., Jones, G. J. F., Chen, S., Aly, R., Ordelman, R., & Larson, M. (2012a).
Search and hyperlinking task at mediaeval 2012..
Eskevich, M., Jones, G. J., Wartena, C., Larson, M., Aly, R., Verschoor, T., & Ordelman,
R. (2012b). Comparing retrieval effectiveness of alternative content segmentation
methods for internet video search. In Content-Based Multimedia Indexing (CBMI),
2012 10th International Workshop on, pp. 16. IEEE.
Facebook video (2015). Facebook. https://www.facebook.com/facebook/videos. Retrieved: 2015-03-30.
Federico, M., Bertoldi, N., Levow, G.-A., & Jones, G. J. F. (2005). Clef 2004 cross-language
spoken document retrieval track. In Multilingual Information Access for Text, Speech
and Images, pp. 816820. Springer.
Federico, M., & Jones, G. J. F. (2004). The clef 2003 cross-language spoken document retrieval track. In Comparative Evaluation of Multilingual Information Access Systems,
pp. 646652. Springer.
Filippova, K., & Hall, K. B. (2011). Improved video categorization from text metadata and
user comments. In Proceedings of the 34th international ACM SIGIR conference on
Research and development in Information Retrieval, pp. 835842. ACM.
Gao, J., Nie, J.-Y., Xun, E., Zhang, J., Zhou, M., & Huang, C. (2001). Improving query
translation for cross-language information retrieval using statistical models. In Proceedings of the 24th annual international ACM SIGIR conference on Research and
development in information retrieval, pp. 96104. ACM.
Gianni, A. (2003). Probabilistic Models for Information Retrieval based on Divergence from
Randomness. Ph.D. thesis, Department of Computing Science, University of Glasgow.
Google (2015). Google translate. https://translate.google.com/. Retrieved: 2015-0330.
He, B., & Ounis, I. (2006). Query performance prediction. Information Systems, 31 (7),
585594.
He, B., & Ounis, I. (2007a). Combining fields for query expansion and adaptive query
expansion. Information processing & management, 43 (5), 12941307.
He, B., & Ounis, I. (2007b). On setting the hyper-parameters of term frequency normalization for information retrieval. ACM Transactions on Information Systems (TOIS),
25 (3), 13.
Herbert, B., Szarvas, G., & Gurevych, I. (2011). Combining query translation techniques to
improve cross-language information retrieval. In Advances in Information Retrieval,
pp. 712715. Springer.
Inkpen, D., Alzghool, M., Jones, G. J. F., & Oard, D. W. (2006). Investigating crosslanguage speech retrieval for a spontaneous conversational speech collection. In Proceedings of the Human Language Technology Conference of the NAACL, Companion
Volume: Short Papers, pp. 6164. Association for Computational Linguistics.
278

fiCross-Lingual Search For User-Generated Internet Video

Jones, G. J., Zhang, K., Newman, E., & Lam-Adesina, A. M. (2007). Examining the
contributions of automatic speech transcriptions and metadata sources for searching
spontaneous conversational speech..
Kishida, K., & Kando, N. (2006). A hybrid approach to query and document translation
using a pivot language for cross-language information retrieval. Springer.
Lam-Adesina, A. M., & Jones, G. J. (2006). Using string comparison in context for improved
relevance feedback in different text media. In String Processing and Information
Retrieval, pp. 229241. Springer.
Langlois, T., Chambel, T., Oliveira, E., Carvalho, P., Marques, G., & Falcao, A. (2010).
Virus: video information retrieval using subtitles. In Proceedings of the 14th International Academic MindTrek Conference: Envisioning Future Media Environments, pp.
197200. ACM.
Larson, M., Newman, E., & Jones, G. J. F. (2009). Overview of videoclef 2008: Automatic
generation of topic-based feeds for dual language audio-visual content. In Evaluating
Systems for Multilingual and Multimodal Information Access, pp. 906917. Springer.
Larson, M., Newman, E., & Jones, G. J. F. (2010). Overview of videoclef 2009: New perspectives on speech-based multimedia content enrichment. In Multilingual Information
Access Evaluation II. Multimedia Experiments, pp. 354368. Springer.
Lee, C.-J., Chen, C.-H., Kao, S.-H., & Cheng, P.-J. (2010). To translate or not to translate?.
In Proceedings of the 33rd international ACM SIGIR conference on Research and
development in information retrieval, pp. 651658. ACM.
Lee, C.-J., & Croft, W. B. (2014). Cross-language pseudo-relevance feedback techniques for
informal text. In Advances in Information Retrieval, pp. 260272. Springer.
Leveling, J., Zhou, D., Jones, G. J., & Wade, V. (2010). Document expansion, query
translation and language modeling for ad-hoc ir. In Multilingual Information Access
Evaluation I. Text Retrieval Experiments, pp. 5861. Springer.
Macdonald, C., He, B., Plachouras, V., & Ounis, I. (2005). University of glasgow at trec
2005: Experiments in terabyte and enterprise tracks with terrier.. In TREC.
Macdonald, C., Plachouras, V., He, B., Lioma, C., & Ounis, I. (2006). University of Glasgow at WebCLEF 2005: Experiments in per-field normalisation and language specific
stemming. Springer.
Magdy, W., & Jones, G. J. (2014). Studying machine translation technologies for largedata clir tasks: a patent prior-art search case study. Information Retrieval, 17 (5-6),
492519.
McCarley, J. S. (1999). Should we translate the documents or the queries in cross-language
information retrieval?. In Proceedings of the 37th annual meeting of the Association
for Computational Linguistics on Computational Linguistics, pp. 208214. Association
for Computational Linguistics.
MediaEval (2015). MediaEval Benchmarking Initiative for Multimedia Evaluation. http:
//www.multimediaeval.org/. Retrieved: 2015-03-30.
279

fiKhwileh, Ganguly, & Jones

Mitra, M., Singhal, A., & Buckley, C. (1998). Improving automatic query expansion. In
Proceedings of the 21st annual international ACM SIGIR conference on Research and
development in information retrieval, pp. 206214. ACM.
Naaman, M. (2012). Social multimedia: highlighting opportunities for search and mining
of multimedia data in social media applications. Multimedia Tools and Applications,
56 (1), 934.
Nikoulina, V., Kovachev, B., Lagos, N., & Monz, C. (2012). Adaptation of statistical machine translation model for cross-lingual information retrieval in a service context. In
Proceedings of the 13th Conference of the European Chapter of the Association for
Computational Linguistics, pp. 109119. Association for Computational Linguistics.
Oard, D. W., & Diekema, A. R. (1998). Cross-language information retrieval. Annual review
of information science and technology, 33, 223256.
Oard, D. W., & Hackett, P. G. (1998). Document translation for cross-language text retrieval at the university of maryland. In Information Technology: The Sixth Text
REtrieval Conference (TREC-6), pp. 687696. US Dept. of Commerce, Technology
Administration, National Institute of Standards and Technology.
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., Pecina, P., Soergel, D., Huang,
X., & Shafran, I. (2007). Overview of the clef-2006 cross-language speech retrieval
track. In Evaluation of multilingual and multi-modal information retrieval, pp. 744
758. Springer.
Over, P., Awad, G. M., Fiscus, J., Antonishek, B., Michel, M., Smeaton, A. F., Kraaij, W.,
& Quenot, G. (2011). Trecvid 2010an overview of the goals, tasks, data, evaluation
mechanisms, and metrics..
Pal, D., Mitra, M., & Datta, K. (2013). Improving query expansion using wordnet. CoRR,
abs/1309.4938.
Parton, K., McKeown, K. R., Allan, J., & Henestroza, E. (2008). Simultaneous multilingual search for translingual information retrieval. In Proceedings of the 17th ACM
conference on Information and knowledge management, pp. 719728. ACM.
Pecina, P., Hoffmannova, P., Jones, G. J., Zhang, Y., & Oard, D. W. (2008). Overview of
the clef-2007 cross-language speech retrieval track. In Advances in Multilingual and
Multimodal Information Retrieval, pp. 674686. Springer.
Peters, C., Braschler, M., & Clough, P. (2012). Multilingual information retrieval: From
research to practice. Springer Science & Business Media.
Pirkola, A., Hedlund, T., Keskustalo, H., & Jarvelin, K. (2001). Dictionary-based crosslanguage information retrieval: Problems, methods, and research findings. Information
retrieval, 4 (3-4), 209230.
Plachouras, V., He, B., & Ounis, I. (2004). University of glasgow at trec 2004: Experiments
in web, robust, and terabyte tracks with terrier.. In TREC.
Rogati, M., & Yang, Y. (2002). Cross-lingual pseudo-relevance feedback using a comparable
corpus. In Evaluation of Cross-Language Information Retrieval Systems, pp. 151157.
Springer.
280

fiCross-Lingual Search For User-Generated Internet Video

Salton, G., & Buckley, C. (1997). Improving retrieval performance by relevance feedback.
Readings in information retrieval, 24 (5), 355363.
Schmiedeke, S., Xu, P., Ferrane, I., Eskevich, M., Kofler, C., Larson, M. A., Esteve, Y.,
Lamel, L., Jones, G. J. F., & Sikora, T. (2013). Blip10000: a social video dataset
containing spug content for tagging and retrieval. In Proceedings of the 4th ACM
Multimedia Systems Conference, pp. 96101. ACM.
Sokolov, A., Hieber, F., & Riezler, S. (2014). Learning to translate queries for clir. In Proceedings of the 37th international ACM SIGIR conference on Research & development
in information retrieval, pp. 11791182. ACM.
Terra, E., & Warren, R. (2005). Poison pills: harmful relevant documents in feedback. In
Proceedings of the 14th ACM international conference on Information and knowledge
management, pp. 319320. ACM.
Toderici, G., Aradhye, H., Pasca, M., Sbaiz, L., & Yagnik, J. (2010). Finding meaning
on youtube: Tag recommendation and category discovery. In Computer Vision and
Pattern Recognition (CVPR), 2010 IEEE Conference on, pp. 34473454. IEEE.
Tong, X., Zhai, C., Milic-Frayling, N., & Evans, D. A. (1996). Ocr correction and query
expansion for retrieval on ocr dataclarit trec-5 confusion track report.. In TREC.
TRECVID (2015). Trec video retrieval evaluation home page. http://trecvid.nist.gov/.
Retrieved: 2015-03-30.
Varshney, S., & Bajpai, J. (2014). Improving performance of english-hindi cross language information retrieval using transliteration of query terms. arXiv preprint
arXiv:1401.3510.
White, R. W., Oard, D. W., Jones, G. J. F., Soergel, D., & Huang, X. (2006). Overview of
the CLEF-2005 cross-language speech retrieval track. Springer.
Youtube (2015). Youtube. http://www.youtube.com/. Retrieved: 2015-03-30.
YouTube Press (2015). Statistics - YouTube.
statistics.html. Retrieved: 2015-03-30.

http://www.youtube.com/yt/press/

Zhou, D., Truran, M., Brailsford, T., Wade, V., & Ashman, H. (2012). Translation techniques in cross-language information retrieval. ACM Computing Surveys (CSUR),
45 (1), 1.

281

fiJournal of Artificial Intelligence Research 55 (2016) 95-130

Submitted 03/15; published 01/16

How Translation Alters Sentiment
Saif M. Mohammad

saif.mohammad@nrc-cnrc.gc.ca

National Research Council Canada

Mohammad Salameh

msalameh@ualberta.ca

University of Alberta

Svetlana Kiritchenko

svetlana.kiritchenko@nrc-cnrc.gc.ca

National Research Council Canada

Abstract
Sentiment analysis research has predominantly been on English texts. Thus there exist
many sentiment resources for English, but less so for other languages. Approaches to
improve sentiment analysis in a resource-poor focus language include: (a) translate the
focus language text into a resource-rich language such as English, and apply a powerful
English sentiment analysis system on the text, and (b) translate resources such as sentiment
labeled corpora and sentiment lexicons from English into the focus language, and use them
as additional resources in the focus-language sentiment analysis system. In this paper
we systematically examine both options. We use Arabic social media posts as standin for the focus language text. We show that sentiment analysis of English translations
of Arabic texts produces competitive results, w.r.t. Arabic sentiment analysis. We show
that Arabic sentiment analysis systems benefit from the use of automatically translated
English sentiment lexicons. We also conduct manual annotation studies to examine why
the sentiment of a translation is different from the sentiment of the source word or text. This
is especially relevant for building better automatic translation systems. In the process, we
create a state-of-the-art Arabic sentiment analysis system, a new dialectal Arabic sentiment
lexicon, and the first ArabicEnglish parallel corpus that is independently annotated for
sentiment by Arabic and English speakers.

1. Introduction
The term sentiment analysis is most commonly used to refer to the goal of determining the
valence or polarity of a piece of text, whether it is positive, negative, or neutral. However,
it can more generally refer to determining ones attitude towards a particular target or
topic. Automatic sentiment analysis of text, especially social media posts, has a number of
applications in commerce, public health, and public policy development. In the past two
decades, a vast majority of research has been on English texts. Furthermore, many sentiment resources essential to automatic sentiment analysis (e.g., sentiment lexicons) exist only
in English. Thus there is a growing need for effective methods for analyzing text from other
languages such as Arabic and Chinese, especially posts on social media. With improvements
in statistical machine translation systems over the last decade, we no longer have to rely
on strictly monolingual sentiment analysis systemsat least two other alternatives may be
viable:
(a) Run an English sentiment analysis system, using English resources, on English translations of the focus language text.

c
2016
National Research Council Canada. All rights reserved.

fiMohammad, Salameh, & Kiritchenko

(b) Use a focus-language sentiment analysis system that employs focus-language resources
and translations of English resources into the focus language.
In this paper we systematically examine both options. We use Arabic social media posts
as a specific instance of focus language text. We use state-of-the-art Arabic and English
sentiment analysis systems as well as a state-of-the-art Arabic-to-English and English-toArabic translation systems. We outline the advantages and disadvantages of each of the
methods listed above, and conduct quantitative and qualitative experiments to determine
impact of translation on sentiment. As benchmarks we use manually determined sentiment
labels of the Arabic posts.
These results will help users determine methods best suited for their particular needs.
Along the way, we answer several research questions such as:
1. What sentiment prediction accuracy is expected when Arabic blog posts and tweets
are translated into English (using the current state-of-art techniques), and then run
through a state-of-the-art English sentiment analysis system?
2. How does this performance compare with that of a current state-of-the-art Arabic
sentiment system?
3. What is the loss in sentiment predictability when translating Arabic text into English
automatically vs. manually?
4. How difficult is it for humans to determine sentiment of text automatically translated
from another language into their native language?
5. When dealing with translated text, which is more accurate at determining the sentiment of Arabic text: (1) automatic sentiment analysis of the translated text, or (2)
human annotation of the translated text for sentiment?
6. Can Arabic posts sentiment analysis systems benefit from additional training data
that is an automatic translation of sentiment-labeled English tweets or from additional
sentiment lexicons that are automatic translations of existing English lexicons?
7. Do automatic translations of words have the same sentiment associations as the original source words (as listed in the source language lexicons, say)? And if not, what
are the different reasons that lead to discrepancies?
The inferences drawn from these experiments do not necessarily apply to language pairs
other than ArabicEnglish. Languages can differ significantly in terms of characteristics that
impact sentiment. However, a similar set of experiments can be used for other language
pairs as well to determine the impact of translation on sentiment.
Through our experiments on two different datasets, we show that sentiment analysis of
English translations of Arabic texts produces competitive results, w.r.t. Arabic sentiment
analysis. We also show that translation (both manual and automatic) introduces marked
changes in sentiment carried by the text; positive and negative texts can often be translated
into texts that are neutral. We also find that certain attributes of automatically translated
text that mislead humans with regards to the true sentiment of the source text, do not seem
to affect the automatic sentiment analysis system.
We show that while it is difficult to obtain improvement in an Arabic sentiment analysis
systems simply by adding more training data that is a translation of existing labeled English
96

fiHow Translation Alters Sentiment

corpus, these systems benefit from the use of automatically translated English sentiment
lexicons. By examining a subset of translated lexicon entries we show that close to 90% of
the entries are valid even in the focus language. A word and its automatic translation may
not convey the same sentiment because of poor translation quality or because the word and
its translation are used differently in the two languages.
In the process of developing these experiments to study how translation impacts sentiment, we created a new dialectal Arabic sentiment lexicon and a state-of-the-art Arabic
sentiment analysis system by porting NRC-Canadas competition winning system (Mohammad, Kiritchenko, & Zhu, 2013; Kiritchenko, Zhu, & Mohammad, 2014b) to Arabic. We
also created a substantial amount of sentiment labeled data pertaining to Arabic social
media texts and their English translations. This is the first such resource where text in
one language and its translations into another language (both manually and automatically
produced) are each manually labeled for sentiment. All of these sentiment lexicons and
sentiment-labeled corpora are made freely available.1
We begin with a survey of related work in sentiment analysis of English, sentiment analysis in Arabic, and work in cross-lingual sentiment analysis (Section 2). In Section 3 we
present our core method to systematically study the impact of translation on sentiment.
In Section 4 we describe how we developed the components needed for our experiments:
translations of Arabic texts into English, translations of English resources into Arabic,
sentiment-labeled data in Arabic and English, an English sentiment analysis system, and
an Arabic sentiment analysis system. In Section 5, we present results of the experiments on
translating focus language text into English for application of an English sentiment analysis
system. We also conduct qualitative and quantitative studies to investigate some of the
reasons why sentiment is impacted on translation. For example, we find that sentiment
expressions are often mistranslated into neutral expressions, however automatic sentiment
analysis systems are able to recover to some extent from these errors. In Section 6, we
present results of the experiments on translating English resources into Arabic and using
features drawn from them in Arabic sentiment analysis. We also describe a manual annotation study on the extent to which automatic Arabic translations have the same sentiment as
the source English words. Finally, we present conclusions and future directions in Section 7.2

2. Related Work
Over the last decade, there has been an explosion of work exploring various aspects of
sentiment analysis in English texts: detecting subjective and objective sentences; classifying
sentences as positive, negative, or neutral; detecting the person expressing the sentiment
and the target of the sentiment; and applying sentiment analysis in health, commerce,
and disaster management. Surveys by Pang and Lee (2008), Liu and Zhang (2012), and
Mohammad (2016) give details of many of these approaches. However, there is less work
on Arabic texts. In the sub-sections below, we briefly outline relevant sentiment analysis
research on English texts, on Arabic texts, and on texts in one language using resources
from another (multilingual sentiment analysis).
1. http://www.purl.org/net/ArabicSA
2. Some early findings of this work were first presented in Salameh, Mohammad, and Kiritchenko (2015).

97

fiMohammad, Salameh, & Kiritchenko

2.1 Sentiment Analysis of English Social Media
English sentiment analysis systems have been applied to many different kinds of texts including customer reviews, newspaper headlines (Bellegarda, 2010), novels (Boucouvalas,
2002; John, Boucouvalas, & Xu, 2006; Mohammad & Yang, 2011), emails (Liu, Lieberman,
& Selker, 2003; Mohammad & Yang, 2011), blogs (Neviarouskaya, Prendinger, & Ishizuka,
2011; Genereux & Evans, 2006; Mihalcea & Liu, 2006), and tweets (Mohammad, 2012).
Often these systems have to cater to the specific needs of the text such as formality versus
informality, length of utterances, etc. Sentiment analysis systems developed specifically for
tweets include those by Go, Bhayani, and Huang (2009), Pak and Paroubek (2010), Agarwal,
Xie, Vovsha, Rambow, and Passonneau (2011), Thelwall, Buckley, and Paltoglou (2011),
Brody and Diakopoulos (2011), Aisopos, Papadakis, Tserpes, and Varvarigou (2012), Bakliwal, Arora, Madhappan, Kapre, Singh, and Varma (2012). A survey by Martnez-Camara,
Martn-Valdivia, Urenalopez, and Montejoraez (2012) provides an overview of the research
on sentiment analysis of tweets. In the last two years, several shared tasks on sentiment
analysis were organized by the Conference on Semantic Evaluation Exercises (SemEval),
which allowed for comparison of different approaches on common datasets from different
domains (Wilson, Kozareva, Nakov, Rosenthal, Stoyanov, & Ritter, 2013; Rosenthal, Ritter,
Nakov, & Stoyanov, 2014; Pontiki, Galanis, Pavlopoulos, Papageorgiou, Androutsopoulos,
& Manandhar, 2014). The NRC-Canada system (Kiritchenko et al., 2014b) ranked first in
these competitions, and we use it in our experiments. Notably, the system makes extensive
use of sentiment lexicons and handles negation appropriately.3 We summarize that system
in Section 4.3.
2.2 Sentiment Analysis of Arabic Social Media
Sentiment analysis of Arabic social media texts has several challenges. The text is often in
a regional Arabic dialect rather than Modern Standard Arabic (MSA). Unlike MSA which
is a standardized form of Arabic, dialectal Arabic is the spoken form of Arabic and lacks
strict writing standards. The text often includes words from languages other than Arabic
and multiple scripts may be used to express Arabic and foreign words. In addition, Arabic
is a morphologically complex language. Negation in MSA is expressed through negation
particles, but in some dialects (Egyptian) it is expressed using a circumfix.
There have been a few studies tackling sentiment analysis of Arabic texts (Ahmad,
Cheng, & Almas, 2006; Farra, Challita, Assi, & Hajj, 2010; Abdul-Mageed, Diab, & Korayem, 2011; Badaro, Baly, Hajj, Habash, & El-Hajj, 2014). There is also a shared task
on detecting sentiment intensity of Arabic phrases (Kiritchenko, Mohammad, & Salameh,
2016).4 The works most closely related to ours are the studies of sentiment analysis of Arabic social media (Al-Kabi, Gigieh, Alsmadi, Wahsheh, & Haidar, 2013; Ahmed, Pasquier,
& Qadah, 2013; El-Beltagy & Ali, 2013; Mourad & Darwish, 2013; Abdul-Mageed, Diab,
& Kubler, 2014). Here we review existing Arabic sentiment analysis systems that were designed specifically for Arabic social media datasets. Abdul-Mageed et al. (2014) trained an
SVM classifier on a manually labeled dataset and applied a two-stage classification that first
3. Zhu, Guo, Mohammad, and Kiritchenko (2014a) show that the impact of negation cannot be properly
captured by simply reversing the polarity of its scope.
4. http://alt.qcri.org/semeval2016/task7/

98

fiHow Translation Alters Sentiment

separates subjective from objective sentences and then classifies the subjective into positive
or negative instances. The authors compiled several datasets from multiple social media
resources that included chatroom messages, tweets, forum posts, and Wikipedia Talk pages.
The datasets were manually labeled by two native Arabic speakers. However, these resources
have not been made publicly available yet. Abdul-Mageed and Diab (2014) also used data
from several resources to compile and build SANA, a large-scale, multi-genre, multidialect
lexical resource. SANA covers Egyptian and Levantine dialects as well as MSA. Abbasi,
Chen, and Salem (2008) deployed Arabic morphological, syntactic and stylistic features for
sentiment analysis of Arabic web forums. For efficient feature selection, they adopted an
Entropy Weighted Genetic Algorithm (EWGA). Mourad and Darwish (2013) trained SVM
and Naive Bayes classifiers on Arabic tweets annotated by two native Arabic speakers. We
compare our systems performance to theirs in Section 4.4.2.
Refaee and Rieser (2014b) manually annotated tweets for sentiment by two native Arabic
speakers. They used an SVM to classify tweets in a two-stage approach: polar vs. neutral,
then positive vs. negative. We test our system on that dataset. However, the dataset they
provided is a superset of the data they had originally used in their experiments (Refaee &
Rieser, 2014a). Thus, the performances of automatic sentiment analysis systems applied on
the two sets are not directly comparable.
2.3 Multilingual Sentiment Analysis
Work on multilingual sentiment analysis has mainly addressed mapping sentiment resources
from English into morphologically complex languages. Mihalcea, Banea, and Wiebe (2007)
used English resources to automatically generate a Romanian subjectivity lexicon using an
EnglishRomanian dictionary. The generated lexicon was then used to classify Romanian
text. Balahur and Turchi (2014) conducted a study to assess the performance of statistical
sentiment analysis techniques on machine-translated texts. Opinion-bearing English phrases
from the New York Times Text (20022005) corpus were split into training and test datasets.
An English sentiment analysis system was trained on the training dataset and its prediction
accuracy on the test set was found to be about 68%. Next, the training and test datasets
were automatically translated into German, Spanish, and French using publicly available
machine-translation engines (Google, Bing, and Moses). The translated test sets were then
manually corrected for errors. Then for German, Spanish, and French, a sentiment analysis
system was trained on the translated training set for that language and tested on the
translated-and-corrected test set. The authors observe that these German, Spanish, and
French sentiment analysis systems obtain accuracies in the low sixties (and thus not very
much lower than 68%). Contrary to this work, our study uses original text from the focus
language, its manual and automatic translations, as well as both manual and automatic
sentiment assignments to systematically examine the effect of translation on sentiment.
Further, we use several external sentiment resources as well as their translations within
state-of-the-art sentiment systems. Also, German, Spanish, and French are much closer to
English, than Dialectal Arabic is to English. Finally, we deal with noisy social media texts
as opposed to more polished news media texts. There also exists research on using sentiment
analysis to improve machine translation, such as the work by Chen and Zhu (2014), but
that is beyond the scope of this paper.
99

fiMohammad, Salameh, & Kiritchenko

3. Method for Determining the Impact of Translation on Sentiment
To systematically study the impact of translation on sentiment analysis, we propose two
experimental setups corresponding to (a) and (b) described in the Introduction:
 Setup A: Translate Arabic text into English (manually and automatically) and annotate the English text for sentiment (manually and automatically). Compare the
sentiment labels assigned to the translated English text with manual sentiment annotations of the Arabic text. The more similar the sentiment annotations are, the less
is the impact of translation.
 Setup B: Translate sentiment annotated corpora and lexicons from English into Arabic
(automatically), and use them as additional resources in supervised Arabic sentiment
classification. Compare the sentiment labels assigned by this system with manual
sentiment annotations of the Arabic text. The more similar the sentiment annotations
are, the less is the impact of translation.
3.1 Impact of Translation on Sentiment - Setup A: Translating the Focus
Language Text to English
With Setup A we explore how translation of text from Arabic to English impacts its sentiment. Specifically, we analyze the performance of an English sentiment analysis system,
using English resources, on automatic translations of Arabic social media texts. The setup
is outlined below:
 Identify or compile an Arabic social media dataset. We will refer to it as Ar. [Ar
comes from the first two letters of Arabic.]
 Manually translate Ar into English. We will refer to these English translations as
En(Manl.Trans.). [Manl. is for manual, and Trans. is for translations.]
 Automatically translate Ar into English. We will refer to these English translations
as En(Auto.Trans.). [Auto. is for automatic.]
 Manually annotate Ar for sentiment. We will refer to the sentiment-labeled dataset
as Ar(Manl.Sent.).
 Manually annotate all English datasets [En(Manl.Trans.) and En(Auto.Trans.)] for
sentiment, creating En(Manl.Trans., Manl.Sent.) and En(Auto.Trans., Manl.Sent.),
respectively.
 Run a state-of-the-art Arabic sentiment analysis system on Ar, creating Ar(Auto.Sent.).
This acts as a baseline system.
 Run a state-of-the-art English sentiment analysis system on all the English datasets
[En(Manl.Trans.) and En(Auto.Trans.)], creating En(Manl.Trans., Auto.Sent.) and
En(Auto.Trans., Auto.Sent.), respectively.
100

fiHow Translation Alters Sentiment

Figure 1: Setup A: Translating the focus language text to English. We compare sentiment
labels between Ar(Manl.Sent.) (shown in a shaded box) and other datasets shown
on the right side of the figure. Ar(Manl.Sent.) is the original Arabic text manually
annotated for sentiment.

Figure 1 depicts this setup. Once the various sentiment-labeled datasets are created, we
can compare pairs of datasets to draw inferences. For example, comparing the labels for
Ar(Manl.Sent.) and En(Manl.Trans., Manl.Sent.) will show how different the sentiment
labels tend to be when text is manually translated from Arabic to English. The comparison will also show, for example, whether positive tweets tend to be translated into
neutral tweets, and to what extent. Furthermore, the results will demonstrate how feasible
it is to first translate Arabic text into English and then use automatic sentiment analysis
(Ar(Manl.Sent.) vs. En(Auto.Trans., Auto.Sent.)). In Section 5, we provide an analysis of
several such comparisons for two different Arabic social media datasets.
DATA and RESOURCES: The list of all corpora and lexicons used in Setup A is
shown in Table 1. Since manual translation of text from Arabic to English is a costly
exercise, we chose, for our experiments, an existing Arabic social media dataset that has
already been translated  the BBN Arabic-DialectEnglish Parallel Text (Zbib, Malchiodi,
Devlin, Stallard, Matsoukas, Schwartz, Makhoul, Zaidan, & Callison-Burch, 2012).5 It
contains about 3.5 million tokens of Arabic dialect sentences and their English translations.
We use a randomly chosen subset of 1200 Levantine dialectal sentences, which we will refer
to as the BBN posts or BBN dataset, in our experiments.
We also conduct experiments on a dataset of 2000 tweets originating from Syria (a
country where Levantine dialectal Arabic is commonly spoken). These tweets were collected
in May 2014 by polling the Twitter API. We will refer to this dataset as the Syrian tweets
or Syria dataset.6 Note, however, that manual translations of the Syrian tweets are not
5. https://catalog.ldc.upenn.edu/LDC2012T09
6. The number of instances chosen to be in the BBN and Syria datasets is somewhat arbitrary; however,
we were constrained by the funds available for manual sentiment annotations on these datasets and their
translations.

101

fiMohammad, Salameh, & Kiritchenko

Resource
positive
a. Focus language (Arabic) corpora
(and their English translations)
BBN posts
498
Syrian tweets
448

Number of instances
negative
neutral

575
1,350

126
202

total

1,199
2,000

b. Resources explored by the baseline Arabic sentiment system
Automatic lexicons:
Arabic Emoticon Lexicon
Arabic Hashtag Lexicon
Arabic Hashtag Lexicon (dialectal)

22,962
13,118
11,941

20,342
8,846
8,179

-

43,304
21,964
20,128

c. Resources explored by the English sentiment system
Manual lexicons:
Bing Lius Lexicon
MPQA Subjectivity Lexicon
NRC Emotion Lexicon
Automatic lexicons:
NRC Emoticon Lexicon
NRC Hashtag Lexicon

2,006
2,718
2,317

4,783
4,911
3,338

570
8,527

6,789
8,199
14,182

38,312
32,048

24,156
22,081

-

62,468
54,129

Table 1: Resources used in Setup A. (Note 1: The focus language corpora are split into test
and training folds as part of cross-validation experiments. Note 2: NRC Emotion
Lexicon and NRC Emoticon Lexicon are very similar in spelling, but they are
two different lexicons.)

available. In our automatic sentiment analysis experiments, the focus language corpora
(BBN dataset and Syria dataset) are each split into test and training folds as part of crossvalidation experiments.
We use a number of manually and automatically created English sentiment lexicons in
our English sentiment analysis system (as shown in row c. of Table 1). We compare the
accuracies obtained by the English sentiment analysis system with an Arabic sentiment
analysis system, for which we create new Arabic wordsentiment association lexicons as
described in Section 4.4.1. These lexicons are called the Arabic Hashtag Lexicon, the
Dialectal Arabic Hashtag Lexicon, and the Arabic Emoticon Lexicon.
3.2 Impact of Translation on Sentiment - Setup B: Translating English
Sentiment Resources to the Focus Language
With Setup B we explore how translation of text from English into Arabic impacts its sentiment. Specifically, we analyze the change in performance of an Arabic sentiment analysis
system when it is allowed to also make use of automatic translations of English sentiment
lexicons and corpora. The setup is outlined below:
 Identify an Arabic social media dataset. Manually annotate it for sentiment and split
the corpus into test and training subsets. We will refer to the test corpus as Ar and
the sentiment-labeled test corpus as Ar(Manl.Sent.).
102

fiHow Translation Alters Sentiment

 Identify or create suitable Arabic sentiment lexicons.
 Identify suitable English sentiment lexicon(s) and a corpus of English tweets labeled
for sentiment.
 Automatically translate the English corpus and lexicon into Arabic. We will refer to
the Arabic translation of the corpus as Ar(Auto.Trans.) and Arabic translation of the
lexicon as ArLex(Auto.Trans.) [Auto. is for automatic; Lex is for lexicon.]
 Train separate Arabic sentiment analysis systems using each of the following sets of
resources:
1. the Arabic training corpus only;
2. the Arabic training corpus and the Arabic translation of the English corpus;
3. the Arabic training corpus and the Arabic sentiment lexicon;
4. the Arabic training corpus, the Arabic sentiment lexicon, and the Arabic translation of the English lexicon.
Apply each of the Arabic sentiment analysis systems on the test set Ar.
Figure 2 depicts this setup. Once the various sentiment-labeled datasets are created, we
can compare the automatically labeled sets with the manual sentiment annotations of
Ar(Manl.Sent.), and calculate accuracies of the automatic labeling. These accuracies will
help answer questions such as: how useful automatically translated English sentiment resources are for Arabic sentiment analysis. We also perform a manual annotation study on
a subset of the automatically translated resources to determine different kinds of errors
that result from the automatic translations. In Section 6, we provide an analysis of these
experiments on different English resources.
DATA and RESOURCES: The list of all corpora and lexicons used in Setup B is
shown in Table 2. We chose the Arabic portion of the BBN Arabic-DialectEnglish Parallel
Text as the primary Arabic social media dataset for Setup B. Specifically, we use the same
subset of 1200 Levantine dialectal sentences, which we refer to as the BBN posts or BBN
dataset. As the English corpus, we choose the SemEval-2013 Task 2 (Sentiment Analysis in
Twitter) training dataset (Wilson et al., 2013) for our experiments because just as the BBN
dataset, this is a dataset of social media posts. Further, it is already manually annotated
for sentiment.
There are several sentiment lexicons for English. We chose four manually created lexicons for our experiments: NRC Emotion Lexicon (Mohammad & Turney, 2010; Mohammad
& Yang, 2011), Bing Liu Lexicon (Hu & Liu, 2004), MPQA Subjectivity Lexicon (Wilson,
Wiebe, & Hoffmann, 2005), and AFINN (Nielsen, 2011). We also experiment with the
lexicons automatically generated from tweets by Kiritchenko et al. (2014b): NRC Emoticon Lexicon (a.k.a. Sentiment140 lexicon) and NRC Hashtag Sentiment Lexicon. These
lexicons helped obtain the best results in sentiment analysis shared task competitions (Mohammad et al., 2013; Kiritchenko, Zhu, Cherry, & Mohammad, 2014a; Zhu, Kiritchenko, &
Mohammad, 2014b).
103

fiMohammad, Salameh, & Kiritchenko

Manual

Arabic'Test'
Set'
Sen7ment''
Assignment'

Manual)

Automatic:!
Baseline

Arabic'Test'
Set'
Arabic'Training'
Set'

Sen7ment''
Assignment'

Arabic'Test'
Set'
Arabic'Training'
Set'
English'Training'
Set'

Automa'c))
Transla'on)

Ar(Auto.Trans)'
addi7onal'training'data'

Sen7ment''
Assignment'

Arabic'Test'
Set'
Arabic'Training'
Set'

Sen7ment''
Assignment'

Ar(Manl.Sent.)'
gold'data'

Automa'c)

Ar(Auto.Sent.)'

Automatic: !
Baseline + Translated Corpus
Automa'c)

Ar(Auto.Sent.)'

Automatic:!
Baseline + Arabic lexicon
Automa'c)

Ar(Auto.Sent.)'

Arabic'Lexicon'

Arabic'Test'
Set'

Arabic'Training'
Set'

Sen7ment''
Assignment'

Arabic'Lexicon'
English'
Lexicon'

Automa'c))
Transla'on)

Automatic:!
Baseline + Arabic lexicon +
Translated lexicon
Automa'c)

Ar(Auto.Sent.)'

ArLex(Auto.Trans)'
addi7onal'lexicon'

Figure 2: Setup B: Translating English sentiment resources to the focus language. We
compare sentiment labels between Ar(Manl.Sent.) (shown in a shaded box) and
other datasets shown on the right side of the figure. Ar(Manl.Sent.) is the original
Arabic text manually annotated for sentiment.

104

fiHow Translation Alters Sentiment

Resource

Number of instances
positive negative neutral

a. Focus language (Arabic) corpora
BBN posts

498

total

575

126

1,199

8,179

-

20,128

b. Resources used by the Arabic sentiment system
Automatic lexicons:
Arabic Hashtag Lexicon (dialectal)

11,941

c. English resources translated into Arabic
Sentiment-labeled corpus:
SemEval-2013 Task 2 corpus

3,620

1,549

4,743

9,912

Manual lexicons:
AFINN
Bing Lius Lexicon
MPQA Subjectivity Lexicon
NRC Emotion Lexicon

878
2,006
2,718
2,317

1,598
4,783
4,911
3,338

570
8,527

2,476
6,789
8,199
14,182

15,210
18,341

11,530
14,241

-

26,740
32,582

Automatic lexicons:
NRC Emoticon Lexicon
NRC Hashtag Lexicon

Table 2: Resources used in Setup B. (Note 1: In our automatic sentiment analysis experiments, focus language corpora are split into test and training folds as part of
cross-validation experiments. Note 2: Automatic translations of the English resources into Arabic were done using Google Translate. Some entries, especially in
the automatic lexicons, were left untranslated because Google Translate had no
information on them.)

4. Capabilities Needed for Performing the Experiments
The experimental setups described above involve several component tasks: generating
translations manually and automatically (Section 4.1), manually annotating Arabic and
English texts for sentiment (Section 4.2), automatic sentiment analysis of English texts
(Section 4.3), and automatic sentiment analysis of Arabic texts (Section 4.4). We describe
each of them in the sub-sections below.
4.1 Generating Translations
Setup A requires certain Arabic corpora translated into English, whereas Setup B requires
some English resources (corpus and lexicon) to be translated into Arabic. The two subsections below describe how we obtained these translations.
4.1.1 Generating English Translations
The BBN dialectal Arabic dataset comes with manual translations into English. We generate automatic English translations of the BBN posts and the Syrian tweets by employing
our in-house multi-stack phrase-based machine translation (MT) system, Portage (Cherry
& Foster, 2012). This statistical machine translation (SMT) system is trained on data from
105

fiMohammad, Salameh, & Kiritchenko

OpenMT 2012. We preprocess the training data by segmenting the Arabic source side of
the training data with MADA 3.2 (Habash, Rambow, & Roth, 2009), using Penn Arabic
Treebank (PATB) segmentation scheme as recommended by El Kholy and Habash (2012).


@ @ @) and Ya (  
) are used interchangeably, we normalize these characters to a bare Alif @ and dotless Ya , respectively.

Since the different forms of Arabic characters Alif ( @

This normalization decreases the sparcity of Arabic tokens and improves translation. The
English side of the training data is lower-cased and tokenized by stripping punctuation
marks. We set the decoders stack size to 10000 and distortion limit to 7. We replace the
out-of-vocabulary words in the translated text with UNKNOWN token (which is shown to
the annotators). The decoders log-linear model is tuned with MIRA (Chiang, Marton, &
Resnik, 2008; Cherry & Foster, 2012). A KN-smoothed 5-gram language model is trained
on the English Gigaword and the target side of the parallel data.
4.1.2 Generating Arabic Translations
For Setup B, we run the SemEval-2013 English tweets dataset (Wilson et al., 2013) through
Google Translate to obtain Arabic translations.7 Even though Google Translate is a phrasebased statistical MT system that is primarily designed to translate sentences, it can also
provide one-word translations. These translations are often the word representing the predominant sense of the word in the source language. Thus we also use Google Translate to
translate into Arabic the words in each of the English sentiment lexicons listed in Table 2.
Note that Google Translate is unable to translate some words in these lexicons. Table 2
gives the number of words translated as well as a break down by sentiment category (positive, negative, and neutral). All of the translated lexicons are made freely available.8 We do
not generate manual translations of these lexicons, but in Section 6.1, we describe a study
where the automatic translations are examined by an Arabic speaker.
4.2 Creating Sentiment Labeled Data in Arabic and English
Manual sentiment annotations were performed on the crowdsourcing platform CrowdFlower9
for three BBN datasets and two Syria datasets:
1. Original Arabic posts (the BBN and Syria datasets), annotated by Arabic speakers.
2. Manual English translations of Arabic posts (available only for the BBN dataset),
annotated by English speakers.
3. Automatic English translations of Arabic posts (the BBN and Syria datasets), annotated by English speakers.
The Questionnaire for 3. is shown below. The questionnaire for 2. is very similar, except
that it states that the text was created by manual translation of Arabic posts. The questionnaire for 1. is also very similar to 3., except that it is in Arabic and it states that the
7. Since our in-house system, Portage, is designed to translate text from Arabic to English, but not the
other way round, we use the publicly available Google Translate for the experiments in Setup B.
Google Translate: https://translate.google.com
8. http://www.purl.com/net/lexicons
9. http://www.crowdflower.com

106

fiHow Translation Alters Sentiment

target texts are posts from social media (no mention of translations in this questionnaire).

Questionnaire for 3: Judge the sentiment of the posts
General Instructions:
- Attempt HITs only if you are a native speaker of English.
- Your responses are confidential.
- It is possible that the occasional post may have a swear word or express something offensive. The
text is no different than something one might find in any public forum.
Task-Specific Instructions:
You will be given English sentences that were translated from Arabic using an automatic machine
translation system. The translations may be ungrammatical and hard to understand. If the translation system was unable to translate a word, it shows that word with the UNK symbol, representing
unknown.
Select the option that best captures the sentiment being conveyed in the sentence:
- positive
- negative
- neutral
- uncertain OR both positive and negative
Select positive if the sentence shows a positive attitude (possibly toward an object or event). For
example:
- I hope every year you will be in good shape
- To be honest I dont know what to say in this story, the nicest sensation
Select negative if the sentence shows a negative attitude (possibly toward an object or event). For
example:
- The new Spiderman movie is terrible
- This government will make us bankrupt
Select neutral if the sentence shows a neutral attitude (possibly toward an object or event). For
example:
- Add spices, onion and sauce
- This expresses truly our relation with Israel
Select Uncertain OR both positive and negative if the sentence shows an uncertain attitude OR
if the sentence expresses both positive and negative attitude. For example:
- The strange that the forward glass of car is not broken yet
- I like ice cream but hate chocolate chips on it
Actual HIT
The sentence below was translated into English from Arabic by a computer algorithm. The sentence
may be ungrammatical and hard to follow. Additionally, the system was unable to translate some
Arabic words. These words are shown with the UNK symbol.
Sentence: Especially companies to acknowledge will be a soft target UNK penetrate serwer commercial network .

107

fiMohammad, Salameh, & Kiritchenko

Select the option that best captures the sentiment being conveyed in the sentence:
- positive
- negative
- neutral
- uncertain OR both positive and negative

Each post was annotated by at least ten annotators and the majority sentiment label was
chosen. A very small number of instances were annotated with the label uncertain OR both
positive and negative. These instances were set aside and not included in further analysis.
Table 3 shows the class distribution of sentiment labels in various datasets. Observe from
rows a. and d. that neutral tweets constitute only about 10% of the original data in both
BBN and Syria datasets. The Syrian tweets have a much higher percentage of negative posts,
whereas in the BBN data, the percentages of positive and negative posts are comparable.
Rows b., c., and e. show that translated texts tend to lose some of the sentiment information
and there is a relatively higher percentage of neutral instances in the translated text than
in the original text.
For each post, we determine the count of the most frequent annotation divided by the
total number of annotations. This score is averaged for all posts to determine the interannotator agreement shown in the last column of Table 3. We use this agreement score as
benchmark to compare performance of automatic sentiment systems (described below).
4.3 English Sentiment Analysis
We use the English-language sentiment analysis system developed by NRC-Canada (Kiritchenko et al., 2014b) in our experiments. This system obtained highest scores in two
recent international competitions on sentiment analysis of tweets  SemEval-2013 Task 2
(Wilson et al., 2013) and SemEval-2014 Task 9 (Rosenthal et al., 2014). We briefly describe
the system below; for more details, we refer the reader to work by Kiritchenko, Zhu, and
Mohammad (2014b).
A linear-kernel Support Vector Machine (Chang & Lin, 2011) classifier is trained on
the available training data. The classifier leverages a variety of surface-form, semantic, and
sentiment lexicon features described below. The sentiment lexicon features are derived from
existing, general-purpose, manual lexicons, namely NRC Emotion Lexicon (Mohammad &
Turney, 2010, 2013), Bing Liu Lexicon (Hu & Liu, 2004), and MPQA Subjectivity Lexicon
(Wilson et al., 2005).
The NRC Emotion Lexicon has sentiment and emotion labels for about 14,000 words
(Mohammad & Turney, 2010; Mohammad & Yang, 2011). These labels were compiled
through Mechanical Turk annotations.10 The Bing Liu Lexicon has about 6,800 words
with sentiment labels (Hu & Liu, 2004). The lexicon was originally used for detecting
sentiment of customer reviews. The MPQA Subjectivity Lexicon, which draws from the
General Inquirer and other sources, has sentiment labels for about 8,000 words (Wilson
et al., 2005).
10. https://www.mturk.com/mturk/welcome

108

fiHow Translation Alters Sentiment

BBN dataset
a. Ar(Manl.Sent)
b. En(Manl.Trans., Manl.Sent)
c. En(Auto.Trans., Manl.Sent)
Syria dataset
d. Ar(Manl.Sent)
e. En(Auto.Trans., Manl.Sent)

positive

negative

neutral

agreement

41.50
35.00
36.17

47.92
43.25
36.50

10.58
21.75
27.34

73.82
68.00
65.70

22.40
14.25

67.50
66.15

10.10
19.60

79.05
76.10

Table 3: Class distribution (in percentage) of the sentiment annotated datasets.
We also used the automatically generated, tweet-specific lexicons NRC Hashtag Sentiment Lexicon and NRC Emoticon Lexicon (Kiritchenko et al., 2014b).11 The sub-section
below gives more details about how these lexicons were generated.
4.3.1 Generating English Sentiment Lexicons
The ablation experiments in the study by Mohammad et al. (2013) showed that the NRCCanada sentiment analysis system benefited most from the use of the NRC Hashtag Sentiment Lexicon and the NRC Emoticon Lexicon. The NRC Hashtag Sentiment Lexicon was
created as follows. A list of 77 seed words, which are synonyms of positive and negative,
was compiled from the Rogets Thesaurus. Then, Twitter API was polled to collect tweets
that had these words as hashtags. Not all tweets that have a positive hashtag or emoticon
express positive sentiment. And similarly not all tweets that have a negative hashtag or
emoticon express negative sentiment. Hashtags and emoticons can be used in tweets in
complex ways, for example in sarcastic tweets. Nonetheless, a majority of the tweets with
a positive hashtag or emoticon have been shown to be positive (and similarly for negative
hashtags and emoticons). Thus the algorithm to extract positive and negative terms from
tweets considers a tweet to be positive if it has a positive hashtag and negative if it has a
negative hashtag. For each term in the tweet set, a sentiment score is computed by measuring the PMI (pointwise mutual information) between the term and the positive or negative
category:
SenScore (w) = P M I(w, pos)  P M I(w, neg)

(1)

where w is a term in the lexicon. P M I(w, pos) is the PMI score between w and the positive
class, and P M I(w, neg) is the PMI score between w and the negative class. A positive
SenScore (w) implies that the word tends to co-occur more with positive seeds than with
negative seeds. Thus, it is likely to be associated with positive sentiment. Similarly, a
negative score suggests that the word tends to co-occur more with negative seeds than with
positive seeds. Thus, it is likely to be associated with negative sentiment. The magnitude
of SenScore (w) indicates the strength of the association.
The NRC Emoticon Lexicon (aka Sentiment140 Lexicon) was created in a similar fashion
using emoticons :) and :( as seeds.
11. http://www.purl.com/net/lexicons

109

fiMohammad, Salameh, & Kiritchenko

4.3.2 Pre-processing and Feature Generation
The following pre-processing steps are performed. URLs and user mentions are normalized
to http://someurl and @someuser, respectively. Tweets are tokenized and part-of-speech
tagged with the CMU Twitter NLP tool (Gimpel, Schneider, OConnor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, & Smith, 2011). Then, each tweet is represented as a
feature vector.
The features:
 Word and character ngrams;
 POS: the number of occurrences of each part-of-speech tag;
 Negation: the number of negated contexts. Negation also affects the ngram features:
a word w becomes w NEG in a negated context;
 Automatic sentiment lexicons: For each token w occurring in a sentence and present
in a lexicon, its sentiment score score(w) is used to compute:
- the number of tokens with score(w) 6= 0;
P
- the total score = wtweet score(w);
- the maximal score = max wtweet score(w); and
- the score of the last token in the tweet.
These features are calculated for each lexicon separately.
 Manually created sentiment lexicons: For each of the three manual sentiment lexicons,
the following features are computed:
- the sum of positive scores for tweet tokens;
- the sum of negative scores for tweet tokens.
 Style features: the presence/absence of all-cap words, hashtags, punctuation marks,
emoticons, and elongated words.
4.4 Arabic Sentiment Analysis
We build an Arabic sentiment analysis system by reconstructing the NRC-Canada English
system to deal with Arabic text. It extracts the same feature set as described in Section 4.3.2. We have also generated word-sentiment association lexicons using the process
described in Section 4.3.1, but for Arabic words from Arabic tweets (more details in subsection below). We preprocess Arabic text by tokenizing with CMU Twitter NLP tool to
deal with specific tokens such as URLs, usernames, and emoticons. Then we use MADA
to generate lemmas. Finally, we normalize different forms of Alif and Ya to bare Alif and
dotless Ya.
4.4.1 Generating Arabic Sentiment Lexicons
The emoticons and hashtag words in a tweet can often act as sentiment labels for the rest of
the tweet. We use this idea, commonly referred to as distant supervision (Go et al., 2009),
to generate three different Arabic sentiment lexicons:
110

fiHow Translation Alters Sentiment

Lexicon
Arabic Emoticon Lexicon
Arabic Hashtag Lexicon
Arabic Hashtag Lexicon (dialectal)

# seeds
pos neg
12
11
109 121
135 348

# tweets
pos
neg
520,000 455,282
209,784
37,209
177,556
34,705

# entries in lexicon
unigram bigram trigram
43,309 229,747 325,366
22,007 128,814 233,481
20,128
93,613 159,986

Table 4: Details of the Arabic Hashtag Lexicon, the Arabic Emoticon Lexicon, and the
Dialectal Arabic Hashtag Lexicon.

 Arabic Emoticon Lexicon: We collected close to one million Arabic tweets that had
emoticons (:) or :(). For the purposes of generating a sentiment lexicon, :) was
considered a positive label (pos) and :( was considered a negative label (neg). For
each word w, that occurred at least five times in these tweets, a sentiment score was
calculated using the formula shown below (same as described in Section 4.3.1, and
proposed first in Mohammad et al., 2013):
SentimentScore(w) = P M I(w, pos)  P M I(w, neg)

(2)

where PMI stands for Pointwise Mutual Information. We refer to the resulting entries
as the Arabic Emoticon Lexicon.
 Arabic Hashtag Lexicon: The NRC-Canada system used 77 positive and negative
seed words to generate the English NRC Hashtag Sentiment Lexicon (Mohammad
et al., 2013; Kiritchenko et al., 2014b). We translated these English seeds into Arabic
using Google Translate. Among the translations provided, we chose words that were
less ambiguous and tended to have strong sentiment in Arabic texts. To increase the
coverage of our seed list, we manually added different inflections for these translations.
We polled the Twitter API for the period of June to August 2014 and collected tweets
that included these seed words as hashtags. After filtering out duplicate tweets and
retweets, we ended up with 209,784 positive unique tweets and 37,209 negative unique
tweets. For each unigram, bigram, and trigram, w, that occurred at least five times
in these tweets, SenScore (w) was calculated just as described in Section 4.3.1. We
will refer to this lexicon as the Arabic Hashtag Lexicon.
 Arabic Hashtag Lexicon (Dialectal): Refaee and Rieser (2014a) manually created a
small sentiment lexicon of 483 dialectal Arabic sentiment words from tweets. We used
these words as seeds to collect tweets that contain them, and generated a PMI-based
sentiment lexicon just as described above. We refer to this lexicon as the Dialectal
Arabic Hashtag Lexicon or Arabic Hashtag Lexicon (dialectal).
The number of seeds and tweets used to create the Arabic Hashtag Lexicon, the Arabic
Emoticon Lexicon, and the Dialectal Arabic Hashtag Lexicon are shown in Table 4. The
table also shows the number of unigram, bigram, and trigram entries in each of the lexicons.
111

fiMohammad, Salameh, & Kiritchenko

Dataset
Sentiment classes
Number of instances
Most frequent class baseline
Human agreement benchmark
Our system, using all non-lexicon features and
a. the Arabic Emoticon Lexicon features
b. the Arabic Hashtag Lexicon features
c. the Dialectal Arabic Hashtag Lexicon features
d. lexicon features from a., b., and c.

BBN posts
pos, neg, neu
1199
47.95
73.82

Syrian tweets
pos, neg, neu
2000
67.50
79.05

62.40
62.97
65.31
63.47

78.35
78.96
79.35
79.00

Table 5: Accuracy obtained using features from different automatically generated Arabic
sentiment lexicons. The highest scores are shown in bold.

Arabic Sentiment Labeled Dataset
sentiment classes
number of instances
Most frequent class baseline
Human agreement benchmark
Mourad and Darwish Arabic SA system
Our Arabic SA system

MD
pos, neg
1111
66.06
72.50
74.62

RR
pos,neg
2681
68.92
85.23

BBN posts
pos, neg, neu
1199
47.95
73.82
65.31

Syrian tweets
pos, neg, neu
2000
67.50
79.05
79.35

Table 6: Accuracy (in percentage) of sentiment analysis (SA) systems on various Arabic
social media datasets.

4.4.2 Evaluation
Table 5 shows ten-fold cross-validation accuracies obtained on the BBN and Syria datasets
using the various Arabic sentiment lexicons discussed above. Observe that the best results are obtained when using the Dialectal Arabic Hashtag Lexicon. Both, the Arabic
Hashtag Lexicon and the Arabic Emoticon Lexicon features, when added to the Dialectal
Arabic Hashtag Lexicon features did not result in an improvement in classification accuracy.
Henceforth in the paper, we use the Dialectal Arabic Hashtag Lexicon as the only Arabic
sentiment lexicon.
Existing sentiment-labeled Arabic datasets include the one described in Mourad and
Darwish (2013), which we will refer to as MD, and the one described in Refaee and Rieser
(2014a), which we will refer to as RR. We tested the Arabic sentiment system on MD
and RR, as well as the two newly sentiment-annotated Arabic datasetsBBN posts and
Syrian tweets. Table 6 shows results of ten-fold cross-validation experiments on each of the
datasets. For MD and RR, the presented results are for the two-class problem (positive
vs. negative) to allow for comparison with prior published results. For BBN and Syria
datasets, the results are shown for the case where the system has to identify one of three
classes: positive, negative, or neutral. Human agreement scores are shown where available.
Note that the accuracy of our system is higher than the previously published results on
the MD dataset. The only previously published results on the RR dataset are on a small
112

fiHow Translation Alters Sentiment

Dataset
Sentiment classes
Number of instances
Most frequent class baseline
Human agreement benchmark
Our System
a. All Features
b. All - lexicon features
c. All - ngram features
c1. All - word ngram features
c2. All - char. ngram features
d. All - style features
e. All - ngram features - style features
f. All - lexicon features - style features

BBN posts
pos, neg, neu
1199
47.95
73.82

Syrian tweets
pos, neg, neu
2000
67.50
79.05

65.31
61.98
63.07
64.72
63.31
65.23
62.23
61.90

79.35
79.35
66.45
77.71
78.40
79.20
67.35
79.45

Table 7: Ablation experiments showing accuracy on the BBN and Syria datasets. The
larger the drop in performance when removing a feature set, the more useful that
feature set is in classification.

subset (about 1000 instances) for which Refaee and Rieser (2014a) obtained an accuracy of
about 87%. The results in Table 6 are for a larger dataset and so not directly comparable.
We determine the impact of different feature sets on performance by conducting ablation
experiments, where we remove one set of features at a time and observe the change in
performance. The larger the drop in accuracy, the more useful the removed feature set.
Table 7 shows the ablation results on the BBN and Syria datasets.
Observe that for the BBN dataset the largest drop in performance occurs when we
remove the sentiment lexicon features. This shows that the method for producing the sentiment lexicon was effective in generating useful wordsentiment association entries. Ngrams
too are helpful in sentiment classification, especially for the Syria dataset. Removing the
style features (features based on hashtags, exclamations, etc.) does not result in a large
drop in performance for both datasets, and it is likely that the character ngram features
subsume much of their discerning power. Row e. shows performance when we use only the
sentiment lexicon features (no ngram features and no style features) and row f. shows performance when we use only the ngram features (no lexicon features and no style features).
Observe that the performance using the lexicon features alone is rather competitive in the
BBN dataset, suggesting that the automatically generated sentiment lexicons are able to
capture termsentiment association to a similar extent to what the supervised algorithm
can learn from ngram features in training data. On the Syria dataset, ngrams alone produce
results reaching human agreement levels. We believe this may be because of the markedly
lower type to token ratio (lexical diversity) in the Syrian tweets and due to the skew in the
dataset towards the negative class. (Table 15 in Section 5.2 shows type to token ratios in
various datasets.)

113

fiMohammad, Salameh, & Kiritchenko

BBN dataset
a. Ar(Auto.Sent)
b. En(Manl.Trans., Auto.Sent)
c. En(Auto.Trans., Auto.Sent)
Syria dataset
d. Ar(Auto.Sent)
e. En(Auto.Trans., Auto.Sent)

pos

neg

neu

39.78
43.12
42.87

60.05
55.63
56.05

0.17
1.25
1.08

20.60
24.75

75.30
69.75

4.10
5.50

Table 8: Class distribution (in percentage) resulting from automatic sentiment analysis.
Data Pair
a. Ar(Manl.Sent) - Ar(Auto.Sent)
b. Ar(Manl.Sent) - En(Manl.Trans., Manl.Sent)
c. Ar(Manl.Sent) - En(Manl.Trans., Auto.Sent)
d. Ar(Manl.Sent) - En(Auto.Trans., Manl.Sent)
e. Ar(Manl.Sent) - En(Auto.Trans., Auto.Sent)
f. En(Manl.Trans., Manl.Sent) - En(Auto.Trans., Manl.Sent)
g. En(Manl.Trans., Manl.Sent) - En(Manl.Trans., Auto.Sent)
h. En(Auto.Trans., Manl.Sent) - En(Auto.Trans., Auto.Sent)

Match %
65.31
71.31
67.73
57.21
62.08
60.08
63.11
69.58

Table 9: Setup A: Match percentage between pairs of sentiment labeled BBN datasets.

5. Experiments on Sentiment after Translation - Setup A: Translating
Focus Language Text into English
With Setup A (as described in Section 3.1) we analyze performance of an English sentiment analysis system, using English resources, on automatic translations of Arabic social
media texts. Using the methods and systems described in Sections 4.1, 4.2, 4.3, and 4.4,
we generated all the translations and the manually and automatically sentiment labeled
datasets mentioned in Section 3.1s Experimental Setup (also shown in Figure 1). Table 8
shows the distribution of positive, negative, and neutral classes in various datasets that have
been automatically labeled with sentiment. These percentages can be compared with those
in Table 3 (rows a. and d.) which show the true sentiment distribution in the BBN and
Syria datasets. Observe that the automatic system has difficulty in assigning neutral class
to posts. This is probably because of the small percentage (about 10%) of neutral tweets
in the training data. Also notice that the system predominantly guesses negative, which
is also a reflection of the distribution in the training data. The strong bias to negatives is
lessened in the English translations.
Main Result: Tables 9 and 10 show how similar the sentiment labels are across various
pairs of datasets for the BBN posts and the Syrian tweets, respectively. For example, row a.
in Table 9 shows the comparison between Arabic tweets that were manually annotated for
sentiment and those that were automatically labeled for sentiment by our Arabic sentiment
analysis system. Column 2 shows the percentage of instances where the sentiment labels
match across the two datasets being compared. For row a. the match percentage of 65.31%
114

fiHow Translation Alters Sentiment

Data Pair
a. Ar(Manl.Sent) - Ar(Auto.Sent)
b. Ar(Manl.Sent) - En(Auto.Trans., Manl.Sent)
c. Ar(Manl.Sent) - En(Auto.Trans., Auto.Sent)
d. En(Auto.Trans, Manl.Sent) - En(Auto.Trans., Auto.Sent)

Match %
79.35
71.05
79.16
76.80

Table 10: Setup A: Match percentage between pairs of sentiment labeled Syria datasets.

represents the accuracy of the automatic sentiment analysis system on the Arabic BBN
posts.
Row b. shows the difference in labels when text is manually translated from Arabic
to English, even though sentiment labeling in both Arabic and English is done manually.
Observe that the two labels match only 71.31% of the time. However, the agreement among
human sentiment annotators on original Arabic texts was only 73.82%. So, the English
translation does affect sentiment, but not dramatically.
Row c. shows results for when the manually translated text is run through an English
sentiment analysis system and the labels are compared against Ar(Manl.Sent.) Observe
that the match for this pair is 67.73%, which is not too much lower than 71.31% obtained
by manual sentiment labeling. This shows that the English sentiment system is performing
rather well. (One would not expect it to get a match greater than 71.31%.) More importantly, the English sentiment system shows a competitive result of 62.08% when run on the
automatically translated text (row e.), which makes this choice a viable option for sentiment
analysis of non-English texts. This result is inline with previous findings in cross-lingual
information retrieval (Nie, Simard, Isabelle, & Durand, 1999) and text classification (Amini
& Goutte, 2010).
Rows d. and e. compare Ar(Manl.Sent.) with manual and automatic sentiment labeling
of automatic translations. Since automatic translation from Arabic to English is fairly
difficult, we expect these match percentages to be lower than those in rows b. and c., and
that is exactly what we observe. However, it is unexpected to find the number for row e.
to be higher than that of row d. We find the same pattern for corresponding data pairs in
the Syrian tweets as well (rows b. and c. in Table 10). This suggests that certain attributes
of automatically translated text mislead humans with regards to the true sentiment of the
source text. However, these same attributes do not seem to affect the automatic sentiment
analysis system as much. We conduct experiments to explore the reasons behind this in
Section 5.1.
Row f. shows that manual and automatic translation lead to only about 60% match in
manually annotated sentiment labels with each other. Row g. shows the accuracy of the
English automatic sentiment analysis system on the manually translated text (assuming the
English sentiment labels as gold). Row h. shows accuracy of the English automatic sentiment analysis system on the automatically translated text (assuming the English sentiment
labels as gold). In this case, the systems accuracy of 69.58% is higher than the human
agreement on automatically translated text (65.7%), which again shows that automatic
translation greatly impacts sentiment perceived by humans.

115

fiMohammad, Salameh, & Kiritchenko

5.1 Qualitative Analysis of Why Translations Differ in Sentiment from the
Source Text
As can be seen from the results of the experiments in the previous section, translations of
text often do not preserve the original sentiment. Further, there exist a number of instances
where the manual sentiment annotations of automatic translations differ from the sentiments
of the original Arabic text, but the automatic English sentiment analysis system correctly
predicts the sentiment of the original Arabic text. We now describe a study we conducted
to determine why that is  what some of the main reasons are, and how frequently these
reasons come into play.
We started by creating a dataset where manual annotations of the Arabic texts disagreed
with manual annotations of the translations. Specifically, from the BBN dataset, we created
instances composed of:
a. the original Arabic tweet,
b. manually determined sentiment of the Arabic tweet (positive, negative, or
neutral),
c. manual English translation of the Arabic tweet,
d. manually determined sentiment of the translation (positive, negative, or
neutral).
We kept only those instances where b. differed from d. We further filtered this set keeping
only those instances where the automatic English sentiment analysis system correctly predicted b. These instances were arranged in decreasing order of inter-annotator agreement
of sentiment annotation on the Arabic texts. Since the annotation task is time intensive,
we wanted to annotate those instances where there is high confidence in the sentiments
of the original Arabic texts. The top 100 instances were presented to a judge who spoke
both English and Arabic fluently. We will refer to this dataset as the BBN Manl.Trans.
Disagreement Pairs. For each of these 100 instances, the judge was asked why in their
opinion b. and d. differ. The precise directions are as shown below:
Annotation Guidelines
For each instance (row), tell us why you think the manually annotated sentiment
of the English translation differs from the original sentiment of the Arabic post.
Codes:
1. Bad translation
a.
b.
c.
d.

sentiment words disappear
sentiment words added
sentiment words replaced with opposite sentiment words
something other than sentiment words has (also) caused disagreement
(may be ill-formed text, may be grammar, may be the position of
negators like not and never, or tense, or auxiliaries, etc.)

2. Translation is reasonable (sentiment-wise), but the same sentence can be
viewed as having one sentiment in the Arabic speaking population and
different sentiment in the English-speaking population due to cultural and
life-style differences.
116

fiHow Translation Alters Sentiment

3. Do not know.
Note:
 Some of the codes have sub-categories. So you can enter 1b, 1c, etc. You
can even enter just 1 if none of the sub-categories apply.
 As you annotate, if you discover new categories, you can add them to the
list of codes, and use the new codes as well.
 If more than one code applies to an instance, separate them by a comma.
For example, you can say 1a, 1d or 1b, 1c, 2.

We then repeated the annotation procedure, but now for instances involving automatically translated texts:
a. the original Arabic tweet,
b. manually determined sentiment of the Arabic tweet (positive, negative, or
neutral),
c. automatic English translation of the Arabic tweet,
d. manually determined sentiment of the translation (positive, negative, or
neutral).
Just as before, we kept only those instances where b. differed from d., and only those instances where the automatic English sentiment analysis system correctly predicted b. The
100 instances with highest inter-annotator agreement on Arabic sentiment annotation were
presented to the judge. We will refer to this dataset as the BBN Auto.Trans. Disagreement Pairs. The judge was asked why in their opinion b. and d. differ. Since automatic
translations exist for both the BBN and the Syria datasets, this annotation was done for
100 instances from the Syria dataset as well. We will refer to this dataset as the Syria
Auto.Trans. Disagreement Pairs.
5.1.1 Annotation Results
It took a human judge 12 hours to annotate the three sets (100 instances each) described
above. The distribution of the reasons for disagreement between the sentiment of the
original text and the sentiment of its translation in 100 instances from each dataset is
shown in Table 11. Since the total number of instances in this study is 100, the number for
each reason (code) is also the corresponding percentage. Note that since an instance can be
annotated to belong to more than one reason, the percentages do not sum to 100%. Also,
since the annotator could choose a broad reason category code (for example, 1.), if none of
its sub-categories apply (for example, 1a. or 1b.), the sum of entries for the subcategories
need not be equal to the number of entries in the subsuming reason category.
The judge marked only a handful of instances in the do not know category and did
not add any new reason categories. This shows that the judge was largely able to determine
the reason for disagreement between the two manual sentiment annotations involved (one
of the original Arabic post and one of the English translation), in terms of the other reasons
pre-specified.
117

fiMohammad, Salameh, & Kiritchenko

Reason for Disagreement
1. Bad Translations
1a. sentiment words disappear
1b. sentiment words added
1c. sentiment words replaced
with words of opposite sentiment
1d. sentiment changed due to
ill-formed text, grammar, etc
2. Cultural differences
3. Do not know

Percentage of Disagreement Pairs
BBN dataset
Syria dataset
Manl.Trans. Auto.Trans.
Auto.Trans.
41
95
91
11
58
80
0
1
1
9

37

8

26
73
2

13
10
3

12
26
4

Table 11: Class distribution of the reasons for disagreement between the sentiment of the
original text and the sentiment of its translation in 100 instances from three
datasets. Note that the entries represent both the number and percentage of
instances, since each subset has 100 instances in all.

A large percentage of instances in the manually translated disagreement sets were affected by what the judge thought were cultural differences. However, bad translation was
still a significant cause of disagreement. Our own cursory examination of the BBN dataset
also gave us the impression that the manual translations could have been better.
Nonetheless, in contrast to the manually translated disagreement sets, the automatically
translated disagreement sets had a markedly high proportion of instances (more than 90%)
where the bad translation led directly to the disagreement in sentiment. More specifically,
automatic translations seem to often mistranslate sentiment expressions such that either
they do not appear in the translation or they appear as neutral terms in the translation
(58% of instances in BBN Auto.Trans. and 80% in Syria Auto.Trans.).
Very few instances pertaining to 1b. were found in the data. This is not surprising since
one would not expect the translator to add sentiment where there is none.
5.1.2 Discussion
Table 12 shows examples for the disagreement categories resulting from the manual translation of the BBN subset. (We do not show examples of 1b. because of lack of data for
this sub-category.) For each of the sub-categories present, the table shows the original Arabic post, the BBN-provided manual translation, and the comments from the judge. Table
13 shows examples for each of the disagreement categories resulting from the automatic
translation of the BBN subset.
Discussions with the judge revealed that the following phenomenon commonly led to
1a., 1c., 1d., and 2.:
 Ambiguous words: Often a word with many meanings, where one sense has a certain
sentiment (positive, negative, or neutral) and another sense has a different sentiment,
can be mistranslated into the wrong sense leading to sentiment disagreement. This is
more common in automatically translated text, but occurs sometimes even in manually
118

fiHow Translation Alters Sentiment

1a. Sentiment words disappear
	 
  @  ,,, QK.
Post
? K

D.k

negative

So what, what can I do to them
The bolded text is an Arabic expression that literally means let them
be loved by a Gecko. It expresses disgust or anger. This part was left
Comments
untranslated by the human translator.
1c. Sentiment words replaced with words of opposite sentiment
	 . A  	  GA	 K 
 KBAK
Post
A

neutral

Manl.Trans.

Manl.Trans.
Comments







secondly, is he still in the refreshment room?
An ambiguous Arabic word was mistranslated into refreshment instead of
recovery room. It is surprising that the human translator made this mistake.

negative
neutral

1d. Sentiment changed due to ill-formed text, grammar, tense, etc
Post
Manl.Trans.

 	 
Q@ PAm	 .'	 @ AK
 @ 
Xk.  
@ H. Q @Y Q
m	 '@ 

negative

The good is still coming, the water leak is from the day they had the project
The post is supposed to shows sarcasm by saying expect more good to come,
meaning the worse is yet to come. This expression is widely used in
Comments
Arabic conversations to mean something negative. This is also an example of 2.
2. Cultural Differences
 Q
Post
..  Y J 
K B .. k@
.

neutral

Manl.Trans.

honestly... I have no comment...
Although the post does not seem to be literally negative, but in many Arabic
conversations it is used to express a negative opinionsimilar to I am speechless.

neutral

although I didnt see the crescent from home
The post was associate with negative sentiment as observing the crescent moon
in Islam is associated with the beginning of a month or a holiday

neutral

Comments
Post
Manl.Trans.
Comments




 
 A G@	 
	 C@ IJ
? AJ	 J
K. 



negative

negative

Table 12: Examples of different reasons for disagreement between the sentiment of the original text and the sentiment of its manual translation.

translated textespecially if the translation is done by crowdsourcing, and quality
control checks are not stringent. Even human translators have on occasion been
tempted to use Google Translate.
 Sarcasm: Sarcasm can sometimes be hard to detect, even for humans, and even when
detected, upon translation, differences in cultural and language norms can mean that
the translation no longer appears sarcastic. See example for 1d. in Table 12.
 Metaphors: Metaphors, such as the example for 1c. in Table 13, are also hard to
translateagain more so by the automatic system, but to some extent even by humans. These have often been translated into neutral or opposite sentiment expressions,
contributing to 1a. or 1c.
 Word-reordering: Automatic translations can often lead to poor word-reordering in
the target language, and this has sentiment implications when the original post has
negation terms. Missing or misplaced negation term can lead to a different sentiment.
Sarcasm is also greatly affected by word-reordering. See example 1d. in Table 13.
Most current statistical machine translation systems are evaluated using an ngram-based
evaluation metric (BLEU). However, the metric often misses (or does not penalize enough)
119

fiMohammad, Salameh, & Kiritchenko

1a. Sentiment words disappear
 Q  
@@AK	 PAJ
Post
k@
.
Auto.Trans. match UNK frankly .
The bolded word is dialectal Arabic typo not translated by the system. It is
Comments
meant to be sleeeeeping i.e., the match was boring
1c. Sentiment words replaced with words of opposite sentiment
 H PAB@
 Q @ @	 	 J AJKY@
Post
H. PA

	
.




Auto.Trans.

negative
neutral

negative

the minimum taught me that more relatives clock
 has two meanings: scorpions and clock arms. Also AJKY@
H. PA

 	 means either
word or lower. The post is supposed to metaphorically state that the world
Comments
has taught me relatives can hurt like scorpion bites. The post is mistranslated,
leading to neutral (instead of negative) sentiment.
1d. Sentiment changed due to ill-formed text, grammar, tense, etc
	 JK A
	 
  Q
Post
A	 	 	 K
.

neutral

Auto.Trans.

positive




and you know what , i mean , the cleanliness
The correct translation is she does not know what cleanliness means.
Word reordering and missing a negation led to text with seemingly positive
Comments
sentiment.
2. Cultural Differences
	 
 AJK@	 
Post
.. AD
	 	 
Ag  ADJ	
A J	 
 C@ PA
Auto.Trans. and you dont know how they are doing and what they are putting in place .
The post is perceived by Arab annotators as being said in a conversation to
Comments
express negative attitude toward an object
	 	 @ @ AKA	 K kP@ @
Post
AKA	 Q
Auto.Trans. God have mercy on our dead God cure our patients
Supplications in Arabic is annotated as positive, although it
contains lots of negative phrases. The tweet is annotated as
Comments
positive with confidence of 1.0, and its automatic translation as negative
with confidence of 0.7, thus showing cultural differences in perceiving this tweet

negative

negative
neutral

positive
negative

Table 13: Examples of different reasons for disagreement between the sentiment of the original text and the sentiment of its automatic translation.

mistranslations caused by many of the phenomenon listed above. Thus, as is evident here,
this means that automatically generated translations can often carry misleading sentiment.
5.2 Quantitative Analysis of Features of Translations that Impact Sentiment
In the previous section, we qualitatively analyzed why human annotation of sentiment on
translations is difficult. In this section we quantitatively explore:
I what causes automatic translations to be inferior to manual translations in terms of
preserving sentiment. (Recall from Table 9 that b. and c. have higher scores than d.
and e.)
II why automatic translation, more error prone as it may be, offers some advantages to
an automatic sentiment analysis system as compared to human annotations. (Recall
from Table 9 that row d. has a lower score that row e.)
Although it is hard to prove causation, we hope the experiments below shed more light into
the features of translated texts that impact sentiment.
120

fiHow Translation Alters Sentiment

Dataset
Our System
a. All features
b. All - lexicon features

BBN Manl.Trans.

BBN Auto.Trans.

67.73
63.73

62.08
60.74

Table 14: Accuracy of the sentiment analysis system on the manual and automatic translations, with and without sentiment lexicons.
Dataset
BBN posts (Arabic)
BBN posts (Manl.Trans.)
BBN posts (Auto.Trans.)
Syrian tweets (Arabic)
Syrian tweets (Auto.Trans.)

#types
6,054
3,592
3,108

#tokens
11,928
16,609
16,660

#types/#tokens
0.5075
0.2163
0.1866

11,667
6,731

35,983
57,153

0.3242
0.1178

Table 15: Lexical diversity in the datasets.
The qualitative analysis in the previous section suggests that one of the main reasons
for I may be the fact that sentiment words in the original text are translated to more
neutral terms. We test this quantitatively through ablation experiments on the English
translations (manual and automatic), by observing the effect of removing sentiment lexicon
features. Table 14 shows the results. Observe that sentiment lexicon features are more
helpful in the manual translations (improve results by 4 percentage points) than in the
automatic translations (improves results by only 1.34 points).
Our hypothesis for why the automatic sentiment analysis system correctly annotates several automatically translated instances where manual annotations of the translation may
fail (II), is that the sentiment system can learn an appropriate model even from mistranslated text  especially when automatic translation makes consistent errors. For example,
Q@	 @ (Oh God grant victory to) has been consistently translated to God forsake. All
tweets having this phrase are correctly annotated as positive by our system, but were marked
negative by the human annotators.
If this were true, then we surmise that automatic translations will have a lower lexical
diversity than manual translations. That is, automatic translations have a lower word type
(unique term) to token ratio than manual translations. Table 15 shows the number of types
and tokens in the original Arabic BBN and Syria datasets and also their translations. For
automatic translations we removed all UNK tokens before determining these counts.
First, we note that even human translations have a lower type to token ratio than
the original source text. Additionally, observe that as hypothesized, the type to token
ratio is markedly higher in manual translations as compared to automatic translations
of the same text. This supports the hypothesis that the SMT system translates source
tokens more consistently. Since the automatic sentiment analysis system is trained on these
consistently translated text with the original sentiment labels of the source text, it is still
able to determine the true sentiment. However, since human sentiment annotators see many
instances where the sentiment terms are mistranslated into neutral terms, they are unable
to determine the true sentiment.
121

fiMohammad, Salameh, & Kiritchenko

System

Accuracy
(in percentage)
a. Baseline (uses word ngram and style features from training fold)
61.98
b. Baseline + Arabic translation of English corpus
English corpus: SemEval task 2 training corpus
42.78
c. Baseline + Arabic translation of English lexicon
i. English lexicon: AFINN
63.41
ii. English lexicon: Bing Liu Lexicon
62.99
iii. English lexicon: MPQA
61.91
iv. English lexicon: NRC Emotion Lexicon
63.48
v. English lexicon: NRC Emoticon Lexicon
62.40
vi. English lexicon: NRC Hashtag Lexicon
61.73
d. Baseline + Arabic lexicon
i. Arabic lexicon: Arabic Emoticon Lexicon
62.40
ii. Arabic lexicon: Arabic Hashtag Lexicon
62.97
iii. Arabic lexicon: Arabic Hashtag Lexicon (dialectal)
65.31
e. Baseline + Arabic lexicon + Arabic translation of English lexicon
Arabic lexicon: Arabic Hashtag Lexicon (dialectal)
i. English lexicon: AFINN
65.73
ii. English lexicon: Bing Liu Lexicon
66.15
iii. English lexicon: MPQA
65.15
iv. English lexicon: NRC Emotion Lexicon
66.23
v. English lexicon: NRC Emoticon Lexicon
66.15
vi. English lexicon: NRC Hashtag Lexicon
64.22
f. Baseline + Arabic Hashtag Lexicon (dialectal)
+ Arabic translation of NRC Emotion Lexicon
66.57
Table 16: Setup B: Cross-validation experiments on the BBN dataset. The highest score
overall and the highest scores in c., d., and e. are shown in bold.

6. Experiments on Sentiment after Translation - Setup B: Translating
Sentiment Resources from English to the Focus Language
We now describe sentiment analysis experiments where we automatically translate resources
from English to the focus language (Arabic) to improve accuracy of a sentiment analysis
system operating on texts in the focus language (Setup B). We implemented Setup B as
described in Section 3.2 and Figure 2, and using the capabilities described in Section 4. The
translations were obtained using Google Translate. The Arabic portion of the BBN dataset
was used as the primary focus language text. Our baseline system performs ten-fold crossvalidation on this dataset using word ngrams and style features described in Section 4.4.
All other systems use additional resourcessome originally created from Arabic sources
and some that are translations of English resources.
Table 16 shows the accuracy of these automatic sentiment analysis systems. (Generated
sentiment labels are compared to manual annotation by Arabic speakers.) Comparing rows
a. and b. we can infer that simply translating sentiment-labeled tweets from English into
Arabic and using them as additional training data for Arabic sentiment analysis leads to
122

fiHow Translation Alters Sentiment

poor results. As we saw in Section 5, the language produced by a machine translation
system differs substantially from the corresponding natural language. Therefore, a sentiment analysis system cannot fully benefit from an additional labeled machine-translated
corpus when asked to annotate natural language text at test time. Similar observations
were reported in work by Balahur and Turchi (2014). It is possible that better results
may be obtained by using one of the many domain-adaptation techniques proposed in the
literature, however, we leave that for future work.12
We also conducted experiments by adding the Arabic translations of the English lexicons
to the baseline system of row a.see results in rows c.i. through c.vi. of the table. The best
results are obtained by using Arabic translations of the NRC Emotion Lexicon (row c.iv.).
Row d. shows results obtained when using the baseline system with additional features
from an Arabic sentiment lexicon. As shown in Section 4.4, the Dialectal Arabic Hashtag
Lexicon outperformed other Arabic lexiconsthose results are shown again here for completeness and convenience. We compare accuracies obtained by rows e. and f. with d. to
determine if using additional features from Arabic translations of English sentiment lexicons is beneficial. Observe that most of the translated English lexicons help obtain higher
accuracies than that of row d.iii. (65.31%). The best results obtained with translations of an
English sentiment lexicon are from using the NRC Emotion Lexicon. Using the NRC Emotion Lexicon along with the Dialectal Arabic Hashtag Lexicon in addition to the baseline
system (row f.) gives a slight further improvement (66.57%).
Thus in the sentiment analysis of Arabic social media posts, it is difficult to extract
benefit from automatic translations of English sentiment labeled sentences. However, improvements can be obtained using automatic translations of English sentiment lexicons.
6.1 Manual Examination of Automatically Translated Entries from a
Sentiment Lexicon
As shown above, lexicons created by translating existing ones in other languages can be beneficial for automatic sentiment analysis, even if one has good lexicons in the focus language
(such as the Dialectal Arabic Hashtag Lexicon for Arabic). However, the above experiments
do not explicitly quantify the extent to which such translated entries are appropriate, and
how translation alters the sentiment of the source word. We conducted a manual annotation
study of 300 entries from the NRC Emotion Lexicon to determine the percentage of entries
that were appropriate even after automatic translation into the focus language (Arabic).
An appropriate entry is an Arabic translation that has the same sentiment association as
its English source word. Additionally, translated entries that were deemed incorrect for
the focus language were classified into coarse error categories. A list of pre-decided error
categories was presented to the annotator, but the annotator was also encouraged to create
new error categories, if required. The error categories are shown below:
1. The word is completely mistranslated.
2. The translation is not perfect, but the English word is translated into a word related
to the correct translation. The Arabic word provided has a different sentiment than
the English source word.
12. Sampling the English corpus to obtain a similar class distribution as in the Arabic dataset led to only
small improvements.

123

fiMohammad, Salameh, & Kiritchenko

positive
negative
neutral
All

Before Translation
# English Entries
100
100
100
300

# positive
85
4
5
94

After Translation
# negative # neutral
9
6
92
4
7
88
108
98

# changed
15 (15.0%)
8 (08.0%)
12 (12.0%)
35 (11.7%)

Table 17: Annotations of NRC Emotion Lexicons sentiment association entries after automatic translation into Arabic.

Error categories
1. Mistranslated
2. Translated to a related word
3. Translation correct, but 3a., 3b., or 3c.
3a. Different dominant sense
3b. Cultural differences
3c. Other reasons

Percentage of
total errors
9.7
38.7
51.6
29.0
22.6
0.0

Table 18: Percentage of erroneous entries in the translated NRC Emotion Lexicon that are
assigned to each error category.

3. The translation is correct, but the Arabic word has a different sentiment than the
English source word.
(a) The dominant sense of the Arabic word is different from the dominant sense of
the English source word, and they have different sentiments.
(b) Cultural and life style differences between Arabic and English speakers lead to
different sentiment associations of the English word and its translation.
(c) Some other reason (give reason if you can).
The annotator was a native speaker of Arabic, who was also fluent in English.
We chose the NRC Emotion Lexicon for the study because it was manually created and
because it led to the best results in our experiments (Table 16). Since manual annotation
is tedious, for this study, we randomly selected 100 positive words, 100 negative words, and
100 neutral words from the lexicon.
Table 17 shows the results of the human annotation study. Of the 100 positive entries
examined, 85 were marked as appropriate in Arabic as well. Nine of the translations were
marked as being negative in Arabic, and six were marked as neutral. Similarly, 92% of
the translated negative entries and 88% of the translated neutral entries were marked appropriate in Arabic. Overall, 11.67% of the translated entries were deemed incorrect for
Arabic.
Table 18 gives the percentage of erroneous entries assigned to each error category. Observe that close to 10% of the errors are caused by gross mistranslations, close to 40% of
the errors are caused by translations into a related word, and about 50% of the errors are
caused, not by bad translation, but by differences in how the word is used in Arabiceither
because of different sense distributions (29%) or because of cultural differences (22.6%).
124

fiHow Translation Alters Sentiment

7. Conclusions
Much of the work in sentiment analysis is focused on English texts. Thus, most other
languages have limited sentiment resources. In this paper we conducted several experiments
exploring two broad approaches for improving sentiment analysis in Arabic social media text
with the help of English resources and state-of-the-art translation systems: (a) translate
the focus language text into a resource-rich language such as English, and apply a powerful
English sentiment analysis system on the translation, and (b) translate resources such as
sentiment labeled corpora and sentiment lexicons from English into the focus language, and
use them as additional resources in the focus-language sentiment analysis system. Our goal
was to systematically study the impact of translation (manual and automatic) on sentiment.
Our experiments show that automatic sentiment analysis of English translations (even
of automatic translations) of Arabic texts can lead to competitive resultsresults that
are similar to that obtained by current state-of-the-art Arabic sentiment analysis systems.
Similar findings have been reported for other tasks, such that Information Retrieval (Nie
et al., 1999) and Text Classification (Amini & Goutte, 2010). Surprisingly, our results also
show that automatic sentiment analysis of automatic translations outperforms the manual
sentiment annotations of the automatically translated text. This suggests that SMT errors
impact human perception of sentiment markedly more than automatic sentiment systems.
Furthermore, we conduct qualitative and quantitative studies to investigate why we observe these results. We find that sentiment expressions are often mistranslated into neutral
expressions when translated. Additionally, automatic translation often makes consistent
errors in translating terms, and since the automatic system learns termsentiment associations from training data, it can learn that the mistranslated word is a cue for the true
sentiment, thus recovering from the error. Sarcasm, metaphoric expressions, and incorrect
word-reordering are some other common reasons why translations fail to preserve sentiment.
Finally, we observe that even correctly translated texts are sometimes marked as having
a different sentiment than what speakers of the source language believe it to be. Thus,
sentiment, at least to some extent is dependent on the cultural context of the annotator.
We also conducted experiments on translating English resources into Arabic to help
improve Arabic sentiment analysis systems. Specifically, we found that using automatic
Arabic translations of many freely available English sentiment lexicons improved accuracy.
However, experiments that simply added translated, sentiment-labeled, English tweets data
to existing Arabic training data resulted in a drop in accuracy. On manual examination
of a subset of the automatic translations of the English lexicon entries, a native speaker
of Arabic marked close to 90% as appropriate (that is, the Arabic word had the same
sentiment association as its English source word). The annotator, who is fluent in English
as well, categorized the remaining 10% of the entries into different error classesreasons
because of which the entries were not valid in Arabic. Mistranslation, cultural differences,
and different sense distributions in Arabic and English, were the primary reasons for errors
in the automatic translation of entries in the sentiment lexicon.
Caveats: The automatic systems employed in these experiments, i.e., Arabic sentiment
analysis, English sentiment analysis, and machine translation, exhibit state-of-the-art per125

fiMohammad, Salameh, & Kiritchenko

formance; nevertheless, further improvements are possible. The Arabic sentiment analysis
system will possibly benefit from features derived specifically for the Arabic language. The
English sentiment analysis system can be further adapted to the peculiarities of machinetranslated texts, which are notably different from regular English. The current machine
translation system has been trained on non-tweet data that results in a high percentage of
out-of-vocabulary words on our datasets. Tweets can have a mixture of dialects or even a
mixture of languages (e.g., Arabic and English). Addressing these factors in future work
will give even more insight into how sentiment is altered on translation, in specific contexts.
Data: All of the resources created as part of this project (Arabic sentiment lexicons,
Arabic sentiment annotations of social media posts, and English sentiment annotations of
their translations) are made freely available.13

Acknowledgments
Thanks to Kareem Darwish and Eshrag Refaee for sharing their data. Thanks to Norah
Alkharashi for annotating translations with reasons for why sentiment was not preserved.
We thank Colin Cherry, Samuel Larkin, and Marine Carpuat for helpful discussions.

References
Abbasi, A., Chen, H., & Salem, A. (2008). Sentiment analysis in multiple languages: Feature
selection for opinion classification in web forums. ACM Transactions on Information
Systems, 26 (3), 12:112:34.
Abdul-Mageed, M., & Diab, M. (2014). SANA: A large scale multi-genre, multi-dialect
lexicon for Arabic subjectivity and sentiment analysis. In Proceedings of the 9th International Conference on Language Resources and Evaluation, LREC 14. European
Language Resources Association.
Abdul-Mageed, M., Diab, M., & Kubler, S. (2014). SAMAR: Subjectivity and sentiment
analysis for Arabic social media. Computer Speech & Language, 28 (1), 20  37.
Abdul-Mageed, M., Diab, M. T., & Korayem, M. (2011). Subjectivity and sentiment analysis of modern standard Arabic. In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human Language Technologies, pp. 587
591.
Agarwal, A., Xie, B., Vovsha, I., Rambow, O., & Passonneau, R. (2011). Sentiment analysis
of Twitter data. In Proceedings of the Workshop on Languages in Social Media, LSM
11, pp. 3038, Portland, Oregon.
Ahmad, K., Cheng, D., & Almas, Y. (2006). Multi-lingual sentiment analysis of financial
news streams. In Proceedings of the 1st International Conference on Grid in Finance.
Ahmed, S., Pasquier, M., & Qadah, G. (2013). Key issues in conducting sentiment analysis
on Arabic social media text. In Proceedings of the 9th International Conference on
Innovations in Information Technology, pp. 7277. IEEE.
13. http://www.purl.org/net/ArabicSA

126

fiHow Translation Alters Sentiment

Aisopos, F., Papadakis, G., Tserpes, K., & Varvarigou, T. (2012). Textual and contextual
patterns for sentiment analysis over microblogs. In Proceedings of the 21st International Conference on World Wide Web Companion, WWW 12 Companion, pp.
453454, New York, NY, USA.
Al-Kabi, M., Gigieh, A., Alsmadi, I., Wahsheh, H., & Haidar, M. (2013). An opinion analysis tool for colloquial and standard Arabic. In Proceedings of the 4th International
Conference on Information and Communication Systems, ICICS 13.
Amini, M.-R., & Goutte, C. (2010). A co-classification approach to learning from multilingual corpora. Machine learning, 79 (1-2), 105121.
Badaro, G., Baly, R., Hajj, H., Habash, N., & El-Hajj, W. (2014). A large scale Arabic
sentiment lexicon for Arabic opinion mining. In Proceedings of the EMNLP Workshop
on Arabic Natural Language Processing (ANLP), pp. 165173, Doha, Qatar.
Bakliwal, A., Arora, P., Madhappan, S., Kapre, N., Singh, M., & Varma, V. (2012). Mining sentiments from tweets. In Proceedings of the 3rd Workshop on Computational
Approaches to Subjectivity and Sentiment Analysis, WASSA 12, pp. 1118, Jeju, Republic of Korea.
Balahur, A., & Turchi, M. (2014). Comparative experiments using supervised learning
and machine translation for multilingual sentiment analysis. Computer Speech &
Language, 28 (1), 5675.
Bellegarda, J. (2010). Emotion analysis using latent affective folding and embedding. In
Proceedings of the NAACL-HLT Workshop on Computational Approaches to Analysis
and Generation of Emotion in Text, pp. 19, Los Angeles, California.
Boucouvalas, A. C. (2002). Real time text-to-emotion engine for expressive Internet communication. Emerging Communication: Studies on New Technologies and Practices
in Communication, 5, 305318.
Brody, S., & Diakopoulos, N. (2011). Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using word
lengthening to detect sentiment in microblogs. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing, EMNLP 11, pp. 562570,
Stroudsburg, PA, USA.
Chang, C.-C., & Lin, C.-J. (2011). LIBSVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology, 2 (3), 27:127:27.
Chen, B., & Zhu, X. (2014). Bilingual sentiment consistency for statistical machine translation. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pp. 607615, Gothenburg, Sweden. Association
for Computational Linguistics.
Cherry, C., & Foster, G. (2012). Batch tuning strategies for statistical machine translation.
In Proceedings of the Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, pp. 427436.
Chiang, D., Marton, Y., & Resnik, P. (2008). Online large-margin training of syntactic
and structural translation features. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP 08, pp. 224233.
127

fiMohammad, Salameh, & Kiritchenko

El-Beltagy, S. R., & Ali, A. (2013). Open issues in the sentiment analysis of Arabic social
media: A case study. In Proceedings of the 9th International Conference on Innovations in Information Technology, pp. 215220. IEEE.
El Kholy, A., & Habash, N. (2012). Orthographic and morphological processing for
EnglishArabic statistical machine translation. Machine Translation, 26 (1-2), 2545.
Farra, N., Challita, E., Assi, R. A., & Hajj, H. (2010). Sentence-level and documentlevel sentiment mining for Arabic texts. In Proceedings of the IEEE International
Conference on Data Mining Workshops, pp. 11141119. IEEE.
Genereux, M., & Evans, R. P. (2006). Distinguishing affective states in weblogs. In Proceedings of the AAAI Spring Symposium on Computational Approaches to Analysing
Weblogs, pp. 2729, Stanford, California.
Gimpel, K., Schneider, N., OConnor, B., Das, D., Mills, D., Eisenstein, J., Heilman, M.,
Yogatama, D., Flanigan, J., & Smith, N. A. (2011). Part-of-speech tagging for Twitter:
Annotation, features, and experiments. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics, ACL 11, pp. 4247.
Go, A., Bhayani, R., & Huang, L. (2009). Twitter sentiment classification using distant
supervision. Tech. rep., Stanford University.
Habash, N., Rambow, O., & Roth, R. (2009). MADA+TOKAN: A toolkit for Arabic
tokenization, diacritization, morphological disambiguation, POS tagging, stemming
and lemmatization. In Proceedings of the 2nd International Conference on Arabic
Language Resources and Tools, pp. 102109, Cairo, Egypt. The MEDAR Consortium.
Hu, M., & Liu, B. (2004). Mining and summarizing customer reviews. In Proceedings of
the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, KDD 04, pp. 168177, New York, NY, USA. ACM.
John, D., Boucouvalas, A. C., & Xu, Z. (2006). Representing emotional momentum within
expressive Internet communication. In Proceedings of the 24th International Conference on Internet and Multimedia Systems and Applications, pp. 183188, Anaheim,
CA. ACTA Press.
Kiritchenko, S., Mohammad, S., & Salameh, M. (2016). SemEval-2016 Task 7: Determining
sentiment intensity of english and arabic phrases. In Proceedings of the International
Workshop on Semantic Evaluation, SemEval 16.
Kiritchenko, S., Zhu, X., Cherry, C., & Mohammad, S. (2014a). NRC-Canada-2014: Detecting aspects and sentiment in customer reviews. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval 2014), pp. 437442, Dublin, Ireland.
Kiritchenko, S., Zhu, X., & Mohammad, S. M. (2014b). Sentiment analysis of short informal
texts. Journal of Artificial Intelligence Research, 50, 723762.
Liu, B., & Zhang, L. (2012). A survey of opinion mining and sentiment analysis. In
Aggarwal, C. C., & Zhai, C. (Eds.), Mining Text Data, pp. 415463. Springer US.
Liu, H., Lieberman, H., & Selker, T. (2003). A model of textual affect sensing using realworld knowledge. In Proceedings of the 8th International Conference on Intelligent
User Interfaces, IUI 03, pp. 125132, New York, NY. ACM.
128

fiHow Translation Alters Sentiment

Martnez-Camara, E., Martn-Valdivia, M. T., Urenalopez, L. A., & Montejoraez, A. R.
(2012). Sentiment analysis in Twitter. Natural Language Engineering, 128.
Mihalcea, R., Banea, C., & Wiebe, J. (2007). Learning multilingual subjective language via
cross-lingual projections. In Proceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, p. 976.
Mihalcea, R., & Liu, H. (2006). A corpus-based approach to finding happiness. In Proceedings of the AAAI Spring Symposium on Computational Approaches to Analysing
Weblogs, pp. 139144. AAAI Press.
Mohammad, S. M. (2012). #Emotional tweets. In Proceedings of the First Joint Conference
on Lexical and Computational Semantics, *SEM 12, pp. 246255, Montreal, Canada.
Mohammad, S. M. (2016). Sentiment analysis: Detecting valence, emotions, and other
affectual states from text. Emotion Measurement.
Mohammad, S. M., Kiritchenko, S., & Zhu, X. (2013). NRC-Canada: Building the stateof-the-art in sentiment analysis of tweets. In Proceedings of the 7th International
Workshop on Semantic Evaluation Exercises, SemEval 13, Atlanta, Georgia, USA.
Mohammad, S. M., & Turney, P. D. (2010). Emotions evoked by common words and
phrases: Using Mechanical Turk to create an emotion lexicon. In Proceedings of the
NAACL-HLT Workshop on Computational Approaches to Analysis and Generation
of Emotion in Text, pp. 2634, LA, California.
Mohammad, S. M., & Turney, P. D. (2013). Crowdsourcing a word-emotion association
lexicon. Computational Intelligence, 29 (3), 436465.
Mohammad, S. M., & Yang, T. W. (2011). Tracking sentiment in mail: How genders differ on
emotional axes. In Proceedings of the ACL Workshop on Computational Approaches
to Subjectivity and Sentiment Analysis, WASSA 11, pp. 7079, Portland, OR, USA.
Mourad, A., & Darwish, K. (2013). Subjectivity and sentiment analysis of modern standard
Arabic and Arabic microblogs. In Proceedings of the 4th Workshop on Computational
Approaches to Subjectivity, Sentiment and Social Media Analysis, WASSA 13, pp.
5564.
Neviarouskaya, A., Prendinger, H., & Ishizuka, M. (2011). Affect analysis model: novel
rule-based approach to affect sensing from text. Natural Language Engineering, 17,
95135.
Nie, J.-Y., Simard, M., Isabelle, P., & Durand, R. (1999). Cross-language information
retrieval based on parallel texts and automatic mining of parallel texts from the Web.
In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research
and Development in Information Retrieval, pp. 7481. ACM.
Nielsen, F. A. (2011). A new ANEW: Evaluation of a word list for sentiment analysis in
microblogs. In Proceedings of the ESWC2011 Workshop on Making Sense of Microposts: Big things come in small packages, pp. 9398.
Pak, A., & Paroubek, P. (2010). Twitter as a corpus for sentiment analysis and opinion
mining. In Proceedings of the 7th Conference on International Language Resources
and Evaluation, LREC 10, pp. 13201326, Valletta, Malta.
129

fiMohammad, Salameh, & Kiritchenko

Pang, B., & Lee, L. (2008). Opinion mining and sentiment analysis. Foundations and Trends
in Information Retrieval, 2 (12), 1135.
Pontiki, M., Galanis, D., Pavlopoulos, J., Papageorgiou, H., Androutsopoulos, I., & Manandhar, S. (2014). SemEval-2014 Task 4: Aspect based sentiment analysis. In Proceedings of the International Workshop on Semantic Evaluation, SemEval 14, pp. 2735,
Dublin, Ireland.
Refaee, E., & Rieser, V. (2014a). An Arabic Twitter corpus for subjectivity and sentiment
analysis. In Proceedings of the 9th International Conference on Language Resources
and Evaluation, LREC 14, Reykjavik, Iceland. European Language Resources Association.
Refaee, E., & Rieser, V. (2014b). Subjectivity and sentiment analysis of Arabic Twitter
feeds with limited resources. In Proceedings of the Workshop on Free/Open-Source
Arabic Corpora and Corpora Processing Tools, p. 16.
Rosenthal, S., Ritter, A., Nakov, P., & Stoyanov, V. (2014). SemEval-2014 Task 9: Sentiment
analysis in Twitter. In Proceedings of the 8th International Workshop on Semantic
Evaluation, SemEval 14, pp. 7380, Dublin, Ireland.
Salameh, M., Mohammad, S., & Kiritchenko, S. (2015). Sentiment after translation: A
case-study on arabic social media posts. In Proceedings of the 2015 Conference of the
North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, pp. 767777, Denver, Colorado.
Thelwall, M., Buckley, K., & Paltoglou, G. (2011). Sentiment in Twitter events. Journal
of the American Society for Information Science and Technology, 62 (2), 406418.
Wilson, T., Kozareva, Z., Nakov, P., Rosenthal, S., Stoyanov, V., & Ritter, A. (2013).
SemEval-2013 Task 2: Sentiment analysis in Twitter. In Proceedings of the International Workshop on Semantic Evaluation, SemEval 13, Atlanta, Georgia, USA.
Wilson, T., Wiebe, J., & Hoffmann, P. (2005). Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT 05, pp. 347354,
Stroudsburg, PA, USA.
Zbib, R., Malchiodi, E., Devlin, J., Stallard, D., Matsoukas, S., Schwartz, R., Makhoul, J.,
Zaidan, O. F., & Callison-Burch, C. (2012). Machine translation of Arabic dialects.
In Proceedings of the Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, pp. 4959. Association
for Computational Linguistics.
Zhu, X., Guo, H., Mohammad, S., & Kiritchenko, S. (2014a). An empirical study on the
effect of negation words on sentiment. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics, ACL, Vol. 14.
Zhu, X., Kiritchenko, S., & Mohammad, S. (2014b). NRC-Canada-2014: Recent improvements in the sentiment analysis of tweets. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval 2014), pp. 443447, Dublin, Ireland.

130

fiJournal of Artificial Intelligence Research 55 (2016) 17-61

Submitted 03/15; published 01/16

Integrating Rules and Dictionaries from
Shallow-Transfer Machine Translation into
Phrase-Based Statistical Machine Translation
Vctor M. Sanchez-Cartagena

vmsanchez@dlsi.ua.es

Prompsit Language Engineering
Av. Universitat s/n. Edifici Quorum III
E-03202 Elx, Spain

Juan Antonio Perez-Ortiz
Felipe Sanchez-Martnez

japerez@dlsi.ua.es
fsanchez@dlsi.ua.es

Dep. de Llenguatges i Sistemes Informatics
Universitat dAlacant
E-03071, Alacant, Spain

Abstract
We describe a hybridisation strategy whose objective is to integrate linguistic resources
from shallow-transfer rule-based machine translation (RBMT) into phrase-based statistical machine translation (PBSMT). It basically consists of enriching the phrase table of a
PBSMT system with bilingual phrase pairs matching transfer rules and dictionary entries
from a shallow-transfer RBMT system. This new strategy takes advantage of how the linguistic resources are used by the RBMT system to segment the source-language sentences
to be translated, and overcomes the limitations of existing hybrid approaches that treat the
RBMT systems as a black box. Experimental results confirm that our approach delivers
translations of higher quality than existing ones, and that it is specially useful when the
parallel corpus available for training the SMT system is small or when translating outof-domain texts that are well covered by the RBMT dictionaries. A combination of this
approach with a recently proposed unsupervised shallow-transfer rule inference algorithm
results in a significantly greater translation quality than that of a baseline PBSMT; in this
case, the only hand-crafted resource used are the dictionaries commonly used in RBMT.
Moreover, the translation quality achieved by the hybrid system built with automatically
inferred rules is similar to that obtained by those built with hand-crafted rules.

1. Introduction
Statistical machine translation (SMT) (Koehn, 2010) is currently the leading paradigm in
machine translation (MT) research. SMT systems are very attractive because they may be
built with little human effort when enough monolingual and parallel corpora are available.
However, parallel corpora are not always easy to harvest, and they may not even exist for
some (under-resourced) language pairs. On the contrary, rule-based machine translation
(RBMT) systems (Hutchins & Somers, 1992) may be built without any parallel corpus;
however, they need an explicit representation of linguistic information, whose coding by
human experts requires a considerable amount of time.
Even when a large parallel corpus is available, SMT systems may still have some limitations as a result of (i) the data sparseness problem that makes it difficult to collect enough
c
2016
AI Access Foundation. All rights reserved.

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

phrase pairs covering all the inflected word forms in highly inflected languages, and (ii)
the domain problem caused when the training parallel corpus belongs to a domain different
from that of the texts to be translated. One potential solution is to follow a hybrid approach
(Thurmair, 2009) and combine an RBMT system with the SMT system in order to mitigate
these limitations. This is the approach we follow in this paper, in which linguistic resources
from shallow-transfer RBMT are used to enrich the phrase table of a phrase-based SMT
(PBSMT) system.
Like any other transfer-based RBMT system, shallow-transfer RBMT systems carry
out the translation process in three steps: analysis of the source-language (SL) sentence
to produce an SL intermediate representation (IR), transfer from that SL IR to a targetlanguage (TL) IR, and generation of the final translation from the TL IR. Shallow-transfer
RBMT systems do not perform a complete syntactic analysis of the input sentences and
work with simple IRs consisting of a sequence of lexical forms. A lexical form comprises the
lemma, lexical category and morphological inflection information of a word.
In shallow-transfer RBMT, as in the Apertium system (Forcada et al., 2011) used in
our experiments, after the analysis step, the SL sentence is split into chunks. Each chunk is
then translated by a shallow-transfer rule and their translations are concatenated in order
to build the TL sentence. This process is similar to the process carried out by a PBSMT
decoder, which builds translation hypotheses by segmenting the SL sentence into phrases
and translating each SL phrase according to the phrase table. As both systems work with
flat sub-segments it is easy to integrate chunks from RBMT into the SMT phrase table so
that they can be scored by all the feature functions commonly used in PBSMT. Moreover,
the use of RBMT dictionaries and shallow-transfer rules allows the PBSMT decoder to
choose phrase pairs that go beyond the word-for-word translation of the words in the RBMT
dictionaries, as well as translating all the inflected word forms they contain; thus alleviating
the data sparseness problem. In addition, the data from a general-purpose RBMT system
can help to reduce the bias of an SMT system towards the domain of the training corpus.
Additionally, even if the rules from the RBMT system have not yet been created, they
can be automatically inferred from a small fragment of the training parallel corpus by means
of the (unsupervised) rule inference approach proposed by Sanchez-Cartagena, Perez-Ortiz,
and Sanchez-Martnez (2015). A better use is therefore made of the training parallel corpus
and RBMT dictionaries than in existing approaches (Schwenk, Abdul-Rauf, Barrault, &
Senellart, 2009) that simply add the dictionaries to the phrase table. By combining the rule
inference algorithm with our hybridisation approach, the translation knowledge contained
in the parallel corpus is generalised to sequences of words that have not been observed in the
corpus, but share lexical category or morphological inflection information with the words
observed.
The enrichment of PBSMT models with RBMT linguistic data has already been explored
by other authors (see Section 2.1); however, the approach presented in this paper is the first
one specifically designed for use with shallow-transfer RBMT and that takes advantage of
the way in which the linguistic resources are used by the RBMT system. To the best of
our knowledge, the general approach by Eisele et al. (2008), described in Section 2.1, is the
only hybrid approach in literature that can be applied to shallow-transfer RBMT systems.
The experimental results show that our hybrid approach outperforms the strategy developed by Eisele et al. (2008). Moreover, the performance of the hybrid system built using
18

fiIntegrating Shallow-Transfer Rules into Statistical Machine Translation

automatically inferred rules is on a par with the hybrid system built with hand-crafted
rules. It is also worth pointing out that a system (Sanchez-Cartagena, Sanchez-Martnez, &
Perez-Ortiz, 2011b) built with our approach and using hand-crafted rules from the Apertium
project (Forcada et al., 2011) was one of the winners1 in the pairwise manual evaluation of
the WMT 2011 shared translation task (Callison-Burch, Koehn, Monz, & Zaidan, 2011) for
the SpanishEnglish language pair. The hybridisation approach presented in this paper,
together with the aforementioned rule inference algorithm, will contribute to alleviating
the data sparseness problem that SMT systems have when highly inflected languages are
involved and reducing the corpus size requirements as regards building PBSMT systems.
The remainder of the paper is organised as follows. Section 2 reviews related work on
hybrid machine translation, including a description of the limitations of the general hybridisation approach proposed by Eisele et al. (2008). Section 3 describes our hybridisation
strategy and a set of different alternatives for scoring the phrase pairs generated from the
linguistic resources of the RBMT system. Two different sets of experiments, all of which integrate data from the Apertium RBMT platform (Forcada et al., 2011), are then described
in order to evaluate our hybridisation strategy (Section 4) and assess whether the automatically inferred rules can replace hand-crafted ones in the hybrid system (Section 5). The
paper ends with a human evaluation and an error analysis (Section 6) and some concluding
remarks (Section 7).

2. Related Work
Hybrid approaches related to that presented in this paper can be split into those that integrate RBMT elements into an SMT system (sections 2.1 and 2.2) and those that integrate
SMT elements into the RBMT architecture (Section 2.3).2 Approaches in the first group
can in turn be split into two groups: those that use linguistic information from an existing
RBMT system (Section 2.1) and those that use linguistic resources inferred from the parallel
corpus from which the SMT models are estimated (Section 2.2).
2.1 Integrating Hand-Crafted Linguistic Resources in SMT
Bilingual dictionaries are the most frequently reused resource from RBMT; they have been
added to SMT systems since its early days (Brown et al., 1993). One of the simplest
strategies, which has already been put into practice with the Apertium bilingual dictionaries (Tyers, 2009), consists of adding the dictionary entries directly to the training parallel
corpus. In addition to the obvious increase in lexical coverage, Schwenk et al. (2009) state
that the quality of the alignments obtained is also improved when the words in the bilingual
dictionary appear in other sentences of the parallel corpus. However, it is not guaranteed
that, following this strategy, multi-word expressions from the bilingual dictionary that ap1. No other system was found to be statistically significantly better when using the sign test at p  0.10.
2. This is not meant to be a strict classification: some of the approaches listed in this section could be
included in both groups. Moreover, approaches in which the outputs of different MT systems are just
combined without making any modification into the inner workings of the systems involved, such as
system combination (Rosti, Matsoukas, & Schwartz, 2007) are not listed in this review because, unlike
our approach, they do not involve the creation of new MT architectures that combine elements from
SMT and RBMT.

19

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

pear in the SL sentences are translated as such because they may be split into smaller units
by the phrase-extraction algorithm. Dictionaries have also been added to SMT systems
together with other rule-based enhancements, as in the work by Popovic and Ney (2006),
who propose combining dictionaries with the use of hand-crafted rules in order to reorder
the SL sentences to match the structure of the TL.
Other approaches take advantage of the full RBMT system. Eisele et al. (2008) present
a strategy based on the augmentation of the phrase table to include information provided
by an RBMT system. Their approach treats the RBMT system as a black box, i.e., the
algorithm is not concerned with the inner workings of the RBMT system. The sentences to
be translated by the hybrid system are first translated with the RBMT system and a small
phrase table is obtained from the resulting parallel corpus (from now on, synthetic corpus).
This new phrase table is then directly added to the original phrase table obtained from the
training parallel corpus. This approach has the following limitations, which are overcome
by the hybrid approach described in this paper:
Deficient segment alignment. When phrase pairs are extracted from the synthetic
corpus through the usual procedure followed in PBSMT (Koehn, 2010, 5.2.3), unaligned words are included in multiple phrase pairs, since there is no evidence about
their correspondence in the other language, and phrase pairs made solely of unaligned
words are not extracted. If word alignments are incorrect, phrase pairs that are not
mutual translation may be extracted and correct phrase pairs present in the parallel
sentences may not be obtained.3 The less reliable the word alignments are, the more
severe this problem becomes.
The word alignment of the synthetic corpus obtained by Eisele et al. (2008) may be
unreliable owing to a vocabulary mismatch between the text to be translated and
the alignment models, which are inferred from the training corpus.4 This limitation
becomes more evident when the text to be translated does not share the domain with
the training corpus, which is actually when the data from the RBMT system is more
useful.5
Relying on word alignments is a reasonable strategy when extracting phrase pairs
from a parallel corpus when we do not know how it was built. However, when we
know that an RBMT system has been used to generate the TL side of the corpus, a
3. Consider the following segment of an EnglishSpanish parallel sentence: Barcelona City Council  Ayuntamiento de Barcelona. If the only word alignment between these segments were a link between Barcelona
in both languages, incorrect phrase pairs such as Barcelona City Council  Barcelona would be extracted,
whereas the correct phrase pair City Council  Ayuntamiento would not be extracted.
4. Alignment models do not contain information about words in the test corpus that are not present in
the training corpus, these words are not therefore aligned and it is likely that phrase pairs that are not
mutual translation will be extracted from them.
5. This problem could be alleviated by building alignment models from the concatenation of the synthetic
corpus and the training corpus, or by incrementally training (Gao, Lewis, Quirk, & Hwang, 2011) the
word alignment models. The former would be computationally too expensive, since the process would
have to be carried out each time a new text was translated with the resulting hybrid system (e.g. building
word alignment models from the EnglishSpanish parallel corpus with 600 000 sentences described in
Section 4.1 took around 6 hours in an AMD Opteron 2 Ghz processor). The latter is likely to cause
alignment errors when infrequent words in the synthetic corpus not found in the training corpus are
involved.

20

fiIntegrating Shallow-Transfer Rules into Statistical Machine Translation

more precise phrase extraction mechanism that takes advantage of how the RBMT
system uses dictionaries and shallow-transfer rules to segment the SL sentences can
be used.
Inadequate balance between the different types of phrase pairs obtained. The
probabilities derived by Eisele et al. (2008) for the phrase pairs extracted from the
synthetic corpus and added to the phrase table are not consistent because they have
been independently estimated from two different corpora. On the one hand, if an SL
phrase is translated in the same way in the training corpus and by the RBMT system,
the probability of the corresponding phrase pair is not increased in comparison with
that of other phrase pairs in which that same SL phrase is translated in a different
way. On the other hand, when the translations of an SL phrase differ from those
produced by the RBMT system, its frequency in the training corpus is not taken into
account when scoring the corresponding phrase pairs, and noise may be consequently
introduced in the case of SL phrases with a low frequency in the training corpus. For
instance, a phrase pair extracted from the training corpus whose SL phrase appears
only once is less reliable and should receive a lower score than a phrase pair whose
SL phrase appears 10 000 times (see Section 3.2). We overcome this limitation by
following a more sophisticated scoring scheme that joins synthetic phrase pairs and
phrase pairs obtained from the training corpus in a single list before computing the
phrase translation probabilities (see Section 3.2.3).
Another interesting approach is that of Enache, Espana-Bonet, Ranta, and Marquez
(2012), in which an interlingua RBMT system developed for the limited domain of patent
translation is integrated into a PBSMT architecture by generating synthetic phrase pairs
from chunks extracted from the SL sentences that can be parsed by the RBMT system.6 The
same philosophy is behind our hybrid approach in which synthetic phrase pairs are generated
from the chunks matched by shallow-transfer rules. However, significant differences exist in
the method used to score the phrase pairs generated from the RBMT system. Enache et al.
use a pre-defined single value for the source-to-target and target-to-source phrase translation
probabilities and lexical weightings of the synthetic phrase pairs. As a consequence all the
synthetic phrase pairs are equiprobable and their relative weight (compared to the phrase
pairs extracted from the training parallel corpus) is not optimised in the tuning step of the
SMT training process. In our proposal, however, the relative weight of the synthetic phrase
pairs is optimised during the tuning process thanks to the use of a binary feature function,
phrases translated in the same way in the parallel corpus and by the RBMT system receive
higher scores, and the lexical translation probabilities of the synthetic phrase pairs are
computed based on the same principles as in SMT: taking into account the translations of
the individual words that make up the phrases.
Finally, Rosa, Marecek, and Dusek (2012) create a set of rules that are applied to the
output of an SMT system in order to fix its most common errors. The main difference
between their proposal and ours lies in the fact that, although these rules are similar to
transfer rules, they operate only on the TL side, and that a syntactic analysis is performed
before applying them.
6. A parse tree may not be obtained from the sentences that do not follow the usual structure in the
restricted domain. This occurs in the case of 66.7% of the sentences in their test set.

21

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

2.2 Adding Morphological Information to SMT
Our hybridisation approach can be combined with the rule inference approach described
by Sanchez-Cartagena et al. (2015) in order to integrate a set of structural transfer rules
inferred from the SMT training parallel corpus, thereby extending the PBSMT models
with new linguistic information. Since shallow-transfer rules operate on lexical forms made
of lemma, lexical category and morphological inflection information, the combination of
the two approaches can be seen as a novel way of extending PBSMT with morphological
features.
In this manner, the resulting approach is related to factored translation models (Koehn
& Hoang, 2007), which are an extension of PBSMT in which each word is replaced by a set
of factors that represent lemma, lexical category, and morphological inflection information.
A phrase-based translation model is inferred for lemmas and an independent one for lexical
categories and morphology. A word-based generation model, which can be inferred from
additional monolingual data, maps combinations of lemmas, lexical category and morphological inflection information to inflected word forms. The main differences between the
factored models and our hybrid approach are as follows:
 In factored models, the translation of lemmas and morphological information is completely independent. As both types of translations are combined in order to generate
the final sequence of surface forms (running words), a combinatorial explosion is likely
to be produced (too many combinations of lemmas and morphological information
need to be scored). As all the combinations cannot be explored, correct translation
hypotheses may be pruned (Bojar & Hajic, 2008; Graham & van Genabith, 2010).
Moreover, idiomatic translations that do not follow the general morphological rules
of the TL may be assigned a very low probability by the translation model, even
though they would have a high probability in a phrase table built from surface forms.
This strategy differs from the one we have followed for the combination of the two
approaches, in which translation hypotheses are built from surface-form-based models
(like those usually used in PBSMT) that have been enriched with synthetic phrase
pairs generated from rules inferred from the training corpus. The complexity of dealing with translations of lemmas and morphological inflection information is moved
from decoding to training time, when the rule inference algorithm deals with it.7
 Our hybrid approach works with existing bilingual dictionaries, while factored models
do not use bilingual dictionaries at all. As a consequence, they translate the morphological inflection information in a different way. In factored models the probability of
the TL morphological inflection factors depends solely on the morphological inflection
factors of the SL sentence. In contrast, the transfer rules used by our method obtain the morphological inflection attributes of the TL words either from SL words or
from their translation according to the bilingual dictionary. This makes the formalism
more expressive and eases the treatment of certain linguistic phenomena. Consider,
for instance, the case in which there is a morphological inflection attribute that only
7. It is worth noting that Graham and van Genabith (2010) proposed a strategy for partially mitigating the
issues caused by the fact that factored models treat lemmas and morphological information as totally
independent elements: the extraction from the training parallel corpus of factored templates, which are
phrases that will not be decomposed in lemma and morphological information for translation.

22

fiIntegrating Shallow-Transfer Rules into Statistical Machine Translation

exists in the TL (such as gender when translating English into Spanish or French). In
our hybrid approach, the structural transfer rule for gender and number agreement
between a noun and an adjective would assign the gender of the translation into the
TL according to the bilingual dictionary of the SL noun to the TL noun and adjective.
This type of rule can be inferred from a very small parallel corpus. In factored models,
however, the translation model would presumably assign similar probabilities to TL
noun-adjective sequences with both genders, and the success of the agreement would
depend solely on the ability of the TL model to differentiate between them.
Other relevant approaches in which morphological attributes are integrated into the
translation model of an SMT system can be found in literature. Green and DeNero (2012)
define a new feature function that models morpho-syntactic agreements, while the factored
language models (Kirchhoff & Yang, 2005) assign probabilities to TL sentences depending
on their sequences of word forms and morphological features, among other factors. These
approaches differ from the strategy presented in this paper mainly in that they do not
perform a generalisation that enriches the translation model with translations of sequences
of SL words unseen in the training corpus.
Riezler and Maxwell III (2006) went further and also added syntactic information to
SMT. They developed a hybrid RBMT-SMT system which works as follows. The SL sentence is parsed with a lexical functional grammar (Riezler et al., 2002) to obtain an SL
intermediate representation (IR). Then the SL IR is transferred into the TL IR by applying
a set of probabilistic rules obtained from a parallel corpus. Each rule contains a set of scores
inspired by those present in the phrase table of a PBSMT system. Finally, the TL sentence
is generated from the TL IR. Since an SL sentence can be parsed in many different ways
and many different TL IRs can be generated by applying different rules, a TL model is also
used in addition to the aforementioned phrase-table-like features. All these features are
finally combined by means of a log-linear model, and their weights are optimised by means
of minimum error rate training (Och, 2003) as in SMT. The results show that the grammar
used was not able to completely parse half of the sentences of the test set (partial parse trees
were obtained instead, but the resulting translation was much worse than the translation
of fully parsed sentences), and considering only the sentences that could be fully parsed,
there was no statistically significant improvement over a PBSMT system trained using the
same data. However, a human evaluation showed an improvement of the grammaticality
of the translations. The main differences between this proposal and ours are the following:
first, the approach by Riezler and Maxwell III does not use existing bilingual dictionaries;
and second, it uses syntactic information that allows the system to perform a deeper linguistic analysis at the expense of not being able to fully parse some input sentences, which
results in a drop in translation performance. In contrast, our approach works with lexical
categories and morphological inflection information and is more robust to ungrammatical
input.
2.3 Integrating Statistical Elements in RBMT
Regarding the enhancement of RBMT systems with statistical elements, it is worth noting
that RBMT systems often use statistical methods for part-of-speech tagging (Cutting, Kupiec, Pedersen, & Sibun, 1992) and parsing (Federmann & Hunsicker, 2011). Besides these
23

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

components, other elements from SMT have been integrated into RBMT, causing greater
changes in the RBMT architecture. For instance, multiple hypotheses can be generated
in the transfer step, and the most probable one can then be chosen according to a TL
model (Lavie, 2008; Carl, 2007). Another option is to use phrase pairs instead of transfer
rules in the transfer step, but keep on using the RBMT analysis and generation modules (Crego, 2014). The approach by Riezler and Maxwell III (2006), discussed previously,
also uses a TL model in order to choose among translations generated by applying rules,
but it integrates more elements from SMT, such as the feature functions usually encoded
in an SMT phrase table.
A different alternative consists of taking advantage of the full syntactic analysis performed by syntactic-transfer RBMT systems to create the structure of the TL sentence,
and then insert phrase translations from a PBSMT phrase table in some nodes of the TL
parse tree (Labaka, Espana-Bonet, Marquez, & Sarasola, 2014). As in SMT, the final translation is that with the maximum probability according to a TL model and to the scores in
the phrase table from which the phrases inserted in the tree have been obtained. However,
phrase reordering is not allowed, since the structure of the TL sentences is guided by the
parse tree. This set-up has also been followed in systems proposed by other authors (Federmann et al., 2010; Zbib et al., 2012).

3. Enhancement of Phrase-Based SMT with Shallow-Transfer Linguistic
Resources
If we have access to the inner working of the RBMT system, the correspondence between the
SL segments of the input sentence and their translations can be computed without relying
on statistical word alignments. In fact, it is not even necessary to translate the whole
sentence with the RBMT system. The individual translation according to the bilingual
dictionary of each word, and the translation of each segment that matches a shallow-transfer
rule constitute the minimum set of bilingual phrases that ensures that all the linguistic
information from the RBMT system has been extracted. Another advantage of this method
over the approach by Eisele et al. (2008) lies in the fact that rules that match an SL segment
but would not be applied by the shallow-transfer RBMT system because of its greedy
operating mode are also taken into account.8 Thus, our hybrid strategy first generates these
synthetic phrase pairs from the RBMT linguistic data and the SL text to be translated,
and then integrates them into the PBSMT models without further decomposition.
8. Consider, for instance, the English sentence I visited Bob and Alices dog was sleeping to be translated
into Spanish with a shallow-transfer RBMT system. Let us suppose that the following segments of the
sentence match a shallow-transfer rule: I visited matches a rule that removes the personal pronoun (it
can be omitted in Spanish), adds the corresponding preposition and generates visite a; Bob and Alices
dog matches a rule that processes the Saxon genitive, adds the preposition and determiner needed in
Spanish and generates el perro de Bob y Alice; and Alices dog also matches a rule that processes the
Saxon genitive when the noun phrase acting as owner contains a single proper noun, and generates el
perro de Alice. When the RBMT engine chooses the rules to be applied in a left-to-right, longest match
manner, it produces visite al perro de Bob y Alice estaba durmiendo, which means I visited Bobs dog
and Alice was sleeping. The right translation, visite a Bob y el perro de Alice estaba durmiendo, can
be obtained if the rule that matches Alices dog is applied. If Eisele et al.s (2008) method is applied
to build a hybrid system, the phrase pairs from the correct translation I visited Bob  visite a Bob and
Alices dog was sleeping  el perro de Alice estaba durmiendo would not be extracted.

24

fiIntegrating Shallow-Transfer Rules into Statistical Machine Translation

3.1 Generation of Synthetic Phrase Pairs
The way in which the synthetic phrase pairs are generated differs depending on which linguistic resources bilingual dictionaries or shallow-transfer rules are used. To generate
bilingual phrase pairs from the bilingual dictionary, all the SL surface forms recognised by
the shallow-transfer RBMT system and their corresponding SL IRs are listed; then, these
SL IRs are translated with the bilingual dictionary to obtain their corresponding TL IRs;
finally, the corresponding TL word forms are obtained by means of the RBMT generation
module.9 For instance, for the generation of phrase pairs from the EnglishSpanish bilingual dictionary in the Apertium RBMT platform, mappings between SL surface forms and
lexical forms such as houses  house N-num:pl and however  however ADV are generated.
They are then translated into the TL by the bilingual dictionary: the resulting phrase pairs
are houses  casas and however  sin embargo. Since dictionaries may contain multi-word
units, the phrase pairs generated may contain more than one word on both (SL and TL)
sides. Note that, unlike in the method by Eisele et al. (2008), the sentences to be translated
are not used. Thus, the generation of phrase pairs from the bilingual dictionary only needs
to be performed once rather than each time a new text is to be translated.
Bilingual phrase pairs matching structural transfer rules are generated in a similar way,
but using the SL text to be translated. Thus, this process is repeated each time a new text
is to be translated with the hybrid system.10 First, its SL sentences are analysed in order
to obtain their SL IRs, and then the sequences of lexical forms that match a structural
transfer rule are passed through the rest of the RBMT pipeline to obtain their translations.
If a sequence of SL lexical forms is matched by more than one structural transfer rule, they
are used to generate as many bilingual phrase pairs as the different rules it matches. This
differs from the way in which Apertium translates, since in these cases only the longest rule
would be applied.
Let us suppose the English sentence My little dogs run fast to be translated into Spanish.
It is analysed by Apertium as the following sequence of lexical forms: my POSP-p:1.num:pl,
little ADJ, dog N-num:pl, run VERB-t:inf, fast ADV.11 If the RBMT system only contained
two rules, one that performs the swapping and number and gender agreement between an
adjective and the noun after it, and another that matches a determiner followed by an
adjective and a noun, swaps the adjective and the noun and makes the three words to agree
in gender and number, the segments little ADJ dog N-num:pl and my POSP-p:1.num:pl
little ADJ dog N-num:pl would be used to generate the following bilingual phrase pairs:
little dogs  perros pequenos and my little dogs  mis perros pequenos.
9. If the TL IR contains missing values for morphological inflection attributes, a different TL phrase for each
possible value of the attribute is generated. For instance, from the mapping between the SL (English)
word form beautiful and the SL lexical form beautiful ADJ-num:sg two EnglishSpanish phrase pairs are
generated: beautiful  bonito and beautiful  bonita; in the first phrase the adjective beautiful has been
translated as masculine, whereas in the second case it has been translated as feminine.
10. This step can be carried out without dramatically reducing decoding efficiency thanks to the fact that
many steps of the Apertium translation pipeline are implemented with partial finite-state transducers (Roche & Schabes, 1997) and are able to process tens of thousands of words per second in an average
desktop computer (Forcada et al., 2011, 4.1).
11. The meaning of the abbreviations used to represent lexical categories are: POSP = possessive pronoun;
ADJ = adjective; N = common noun; VERB = verb; and ADV = adverb. Regarding morphological
inflection information, p:1 = first person, num:pl = plural number and t:inf = infinitive mood.

25

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

Note that, unlike the generation of bilingual phrases from the bilingual dictionary, the
generation of bilingual phrase pairs from the shallow-transfer rules is guided by the text
to be translated.12 We decided to do this in order to make the approach computationally
feasible and avoid meaningless phrases. Consider, for instance, the rule which is triggered
by a determiner followed by an adjective and a noun in English. Generating all the possible
phrase pairs virtually matching this rule would involve combining all the determiners in
the dictionary with all the adjectives and all the nouns, causing the generation of many
meaningless phrases, such as the wireless boy  el nino inalambrico.
All the phrase pairs generated are assigned a frequency of 1, since they have not been
generated from an actual parallel corpus. These frequencies are used to score the phrase
pairs, as described in the next section.
3.2 Scoring the Synthetic Phrase Pairs
PBSMT systems usually attach 4 scores (Koehn, 2010, Sec. 5.3) to every phrase pair in the
phrase table (translation model): source-to-target and target-to-source phrase translation
probabilities and source-to-target and target-to-source lexical weightings. The source-totarget translation probability (t|s) of a phrase pair (s, t) is usually computed by means of
Eq. (1), where count() stands for the frequency of a phrase pair in the list of phrase pairs
extracted from the training parallel corpus.
count(s, t)
ti count(s, ti )

(t|s) = P

(1)

The purpose of lexical weightings is to act as a back-off when scoring phrase pairs with
a low frequency (Koehn, 2010, Sec. 5.3.3). The lexical weighting score of a phrase pair is
usually computed as the product of the lexical translation probability of each source word
and the target word to which it is aligned. Lexical translation probabilities are obtained
from a lexical translation model estimated by maximum likelihood from the word alignments
of the parallel corpus.
The values of these four scores for the synthetic phrase pairs can be calculated in different
ways and this may affect the scores of the phrase pairs extracted from the original training
corpus. In this respect, it is desirable that the scoring method applied to both synthetic
and corpus-extracted phrase pairs increases the probability of those phrase pairs whose SL
phrase are translated in the same way in the training corpus and by the RBMT system. In
addition, the scoring method should also consider the frequency in the parallel corpus of the
SL phrases when a translation performed by the RBMT system does not agree with that
found in the training corpus. Finally, it is also desirable that the addition of the synthetic
phrase pairs to the statistical models does not involve a big computational effort, since it
is executed for every text to be translated.
12. If bilingual phrase pairs were generated from the segments from the training corpus that match a rule,
the method would be less effective when dealing with data sparseness, since synthetic phrases generated
from rules would only be available for the sequences of words present in the training corpus.

26

fiIntegrating Shallow-Transfer Rules into Statistical Machine Translation

In this section, we propose a method13 for integrating the set of synthetic phrase pairs obtained from the RBMT data in the PBSMT system that meets the aforementioned requirements. The remainder of this section contains, in addition to our method, the description
of other phrase scoring approaches that can be found in literature and their limitations.14
All the strategies presented below have been evaluated as will be described in Section 4.
3.2.1 Creating an Additional Phrase Table
One simple strategy for integrating the synthetic phrase pairs in the hybrid SMT system
is that of putting them in an different (synthetic) phrase table, as Koehn and Schroeder
(2007) propose in the context of domain adaptation. When the decoder builds hypotheses,
it looks for phrase pairs in both phrase tables and if the same phrase pair is found in both,
one instance from each phrase table is used to build the hypotheses. It is for this reason
that some authors refer to this approach as alternative decoding paths. Each score in each
phrase table receives a different weight during the tuning process, which should help the
hybrid system to obtain the appropriate relative weighting of both sources of phrase pairs.
When this scoring strategy is used to integrate the synthetic phrase pairs into the PBSMT models, the phrase translation probabilities in the synthetic phrase table are computed
by means of Eq. (1), as is done with the phrase pairs extracted from the parallel corpus,
and using the counts within the set of synthetic phrase pairs. The lexical weighting scores
of each phrase pair are computed from a set of word alignments and a lexical translation
model as described by Koehn (2010, 5.3.3). The lexical translation model to be used is
estimated from a synthetic corpus generated only from the RBMT bilingual dictionary, as
described in Section 3.1; the word alignments used are those obtained by tracing back the
operations carried out by the RBMT engine.15
Since both phrase tables are computed in a totally independent way, the phrase translation probabilities of the phrase pairs which appear in both phrase tables are not increased
in comparison with those of the phrase pairs that appear in only one of them. Consider, for
instance, that the SL phrase a has two different translations according to the RBMT system:
b and c. The source-to-target phrase translation probabilities in the synthetic phrase table
13. This method has already been described by Sanchez-Cartagena, Sanchez-Martnez, and Perez-Ortiz
(2011a); however, this is the first time it has been systematically compared to other scoring methods
found in the literature and evaluated with automatically inferred rules.
14. Methods in which the relevance of the phrase tables being combined must be defined in advance (i.e.,
there is a primary and a secondary phrase table), such as fill-up (Bisazza, Ruiz, & Federico, 2011), are
not described in this section and have not been evaluated. We leave the responsibility of adapting the
relative relevance of both types of phrase pairs to the type of texts to be translated to the tuning step
of the SMT training process.
15. The Apertium engine keeps track on each step of its translation pipeline of the input word from which
each output word has been obtained. The path starting from each input SL surface form is then followed
in order to obtain the TL surface form aligned to it. An exception is made when a step of the pipeline
converts an input word into multiple output words or vice-versa. In that case, the words involved are
left unaligned; this is done to avoid generating too many word alignments that could be incorrect. Let us
suppose that the Spanish sentence Por otra parte mis amigos americanos han decidido venir is translated
into English as On the other hand my American friends have decided to come by Apertium. The Spanish
phrase Por otra parte is analysed by Apertium as a single lexical form. After being translated into
English, it produces the segment on the other hand in the generation step. If the exception were not
made, the SL word por would be aligned with the four TL words on, the, other and hand and the SL
words otra and parte would also be aligned with the same set of TL words.

27

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

for the resulting phrase pairs would be synth (b|a) = 0.5 and synth (c|a) = 0.5. Let us also
suppose that, after extracting phrase pairs from the parallel corpus, the phrase pairs (a, b)
and (a, d) have the same frequency, and there are no other phrase pairs with a as a source.
The resulting source-to-target phrase translation probabilities would be corpus (b|a) = 0.5
and corpus (d|a) = 0.5. Although there is evidence that suggests that b is a more likely
translation than c and d, the three translations have the same probability.
3.2.2 Phrase Table Linear Interpolation
Alternatively, once the two phrase tables have been built, they can be linearly interpolated
into a single one (Sennrich, 2012, 2.1). The scores attached to each phrase pair in the
resulting phrase table are obtained as the linear interpolation of the value of the corresponding score in the corpus-extracted phrase table and in the synthetic phrase table. For
instance, the source-to-target phrase translation probability is computed as shown in Eq. (2)
below, in which countsynth () is the frequency of a phrase pair in the list of phrase pairs
generated from the RBMT system, countcorpus () is the frequency of a phrase pair in the list
of phrase pairs extracted from the parallel corpus and corpus and synth are the weights for
both phrase tables; obviously corpus + synth = 1. These weights are optimised by means
of perplexity minimisation on a phrase table built from a development set (Sennrich, 2012,
2.4).
countsynth (s, t)
countcorpus (s, t)
+ synth P
count
(s,
t
)
corpus
i
ti
ti countsynth (s, ti )

(t|s) = corpus P

(2)

This method, unlike that which uses two independent phrase tables and is described in
Section 3.2.1, increases the phrase translation probability of the phrase pairs that appear
in both phrase tables over those that are only present in one of them. For the phrase
pairs (a, b), (a, c) and (a, d) mentioned above, the resulting probabilities would be (b|a) =
0.5synth + 0.5corpus = 0.5; (c|a) = 0.5synth ; and (d|a) = 0.5corpus . However, this
method does not use the frequency of the source phrases in the training corpus when
interpolating the phrase tables. If the source phrase x is found only once in the training
corpus, and it is aligned with y, but its only possible translation according to the RBMT
system is z, the source-to-target phrase translation probabilities of both phrase pairs would
be (y|x) = corpus and (z|x) = synth , respectively. If x were found 10 000 times in the
training corpus, and always translated as y, the probabilities would be exactly the same
because the weights corpus and synth are the same for all the phrase pairs. However, the
phrase pair (x, y) is much more reliable when it is found in the training corpus 10 000 times
than when it is found only once. If the probabilities in the resulting phrase table reflected
this difference, the decoder would presumably be able to choose better phrase pairs and
produce more reliable translations.
3.2.3 Proposed Strategy: Directly Expanding the Phrase Table
One way of taking into account the absolute frequency of the different phrases in the training
corpus is to join synthetic phrase pairs and corpus-extracted phrase pairs and calculate the
phrase translation probabilities by means of relative frequency as usual. The source-totarget phrase translation probabilities of the resulting phrase table are therefore computed
28

fiIntegrating Shallow-Transfer Rules into Statistical Machine Translation

as follows:
countcorpus (s, t) + countsynth (s, t)
ti (countcorpus (s, ti ) + countsynth (s, ti ))

(t|s) = P

(3)

Since countsynth () = 1 for all the synthetic phrase pairs, when a synthetic phrase pair
share its SL side with a corpus-extracted phrase pair, the source-to-target phrase translation
probability of the synthetic phrase pair may be too small compared to the phrase pair
extracted from the training corpus.16 Depending on the texts to be translated, it may be
desirable for a synthetic phrase pair to have a higher phrase translation probability than a
corpus-extracted phrase pair with the same SL side. In order to adapt their relative weight
to the texts to be translated, an additional binary feature function that flags synthetic
phrase pairs is added to the phrase table.17
The lexical weighting scores of the phrase table built with this combination method
are obtained by using the same lexical translation model for both types of phrase pairs.
The model (actually, one model for source-to-target and another model for target-to-source
lexical weighting) is obtained from the concatenation of the training parallel corpus and the
synthetic phrase pairs generated from the RBMT bilingual dictionary. The lexical weighting
scores are then computed using the word alignments obtained by statistical methods for
the corpus-extracted phrase pairs, as usual (Koehn, 2010, 5.2.1), and those obtained by
tracing back the operations carried out in the different translation steps of Apertium for
the synthetic phrase pairs (see Section 3.2.1).
3.2.4 Augmenting the Training Corpus
Finally, the simplest approach involves appending the RBMT-generated phrase pairs to the
training corpus and running the usual PBSMT training algorithm. Unlike in the previous
approaches, this improves the alignments of the original training corpus and enriches the
lexicalised reordering model (Koehn, 2010, 5.4.2), in addition to the phrase table. The
phrase extraction algorithm (Koehn, 2010, 5.2.3) may, however, split the resulting bilingual phrase pairs into smaller units which may signify that multi-word expressions are not
translated in the same way as they appear in the RBMT bilingual dictionary.
16. The same applies to phrase pairs that share their TL side and the target-to-source phrase translation
probability.
17. In order to take into account the absolute frequencies in the parallel corpora from which the two phrase
tables to be combined have been obtained, Sennrich (2012, 4.2) proposes the weighted counts interpolation method, which is similar to that presented in this paper. There are two main differences between
both approaches. Firstly, in order to adapt the weight of both types of phrases to the texts to be
translated, the weighted counts approach multiplies the frequency of each phrase pair by a factor before
building the phrase table; depending on the origin of the phrase, a different factor is used. On the
contrary, our method adds a binary feature function to the phrase table. And secondly, the weighted
counts approach optimises the factors that determine the relative weight of each type of phrase pair
by means of perplexity minimisation on a phrase table built from a development set (Sennrich, 2012,
2.4) in isolation, i.e., with no connection to the rest of the elements present in the log-linear model.
In contrast, the new method optimises the weight of the binary feature function together with the rest
of the elements in the log-linear model during the tuning process. Given the poor results obtained by
the phrase table interpolation method in which the weights are also optimised by means of perplexity
minimisation in the experiments reported in Section 4.2, weighted counts has not been included in the
experimental setup.

29

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

Although this strategy is not feasible in a real-world environment because of the computational cost of word aligning the whole training corpus for each document to be translated,18 it is worth evaluating it because it is the only strategy that enriches the data from
which the lexicalised reordering model is obtained.

4. Evaluation with Hand-Crafted Resources
A set of experiments whose objective was evaluating the feasibility of the hybridisation
strategy described in Section 3 when using hand-crafted linguistic resources in the Apertium RBMT platform has been conducted. We compare, for different language pairs, training corpus sizes and text domains, the translation quality achieved by a baseline PBSMT
system, by the RBMT system from which the data is extracted, by Eisele et al.s (2008)
approach and by a set of hybrid systems using the phrase scoring alternatives described in
Section 3.2.
4.1 Experimental Setup
The language pairs used for evaluation are BretonFrench19 and EnglishSpanish.20
BretonFrench has been chosen because it has the problem of resource scarceness: there
are only around 60 000 parallel sentences available for this pair (Tyers, 2009; Tiedemann,
2012). EnglishSpanish have been chosen because they have a wide range of parallel corpora available and this allows us to perform both in-domain and out-of-domain evaluations.
Moreover, as Spanish is a highly inflected language and English is not, the results for both
directions of the EnglishSpanish language pair allow us to evaluate in detail the impact
of the hybrid strategy in the translation of highly inflected languages.
The translation model of the PBSMT systems for EnglishSpanish has been trained
on the Europarl parallel corpus (Koehn, 2005) version 5; 21 the TL model has been trained
on the same corpus. In both cases, the Q4/2000 portion has been set aside for evaluation
purposes. Different subsets of the parallel corpus with different number of sentences have
been used to build the systems; however, in all cases the language model was trained on the
whole TL side of the Europarl corpus. These subsets have been randomly chosen in such a
way that larger corpora include the sentences in the smaller ones. The different subcorpora
contain 10 000, 40 000, 160 000, 600 000 and 1 272 260 sentences; the latter corresponds to
the whole training corpus.
Regarding BretonFrench, the translation model has been built using the only freelyavailable parallel corpus for this language pair (Tyers, 2009; Tiedemann, 2012), which
contains short sentences from the tourism and computer localisation domains. Different
training corpus sizes have been used too, namely 10 000, 25 000 and 54 196 parallel sentences.
The latter corresponds to the whole corpus except for the subsets reserved for tuning and
testing. As in the EnglishSpanish language pair, sentences have been randomly chosen
in such a way that larger corpora include the sentences in the smaller ones. The TL model
18. Recall that a different set of synthetic phrase pairs is generated for each SL text to be translated.
19. There is not FrenchBreton RBMT system in the Apertium platform.
20. The symbol  means that the first language acts as the the SL and the second one as the TL. The
symbol  means that the evaluation has been performed in both translation directions.
21. http://www.statmt.org/europarl/archives.html#v5

30

fiIntegrating Shallow-Transfer Rules into Statistical Machine Translation

has been learnt from a monolingual corpus built by concatenating the target side of the
whole parallel training corpus and the French Europarl corpus provided for the WMT 2011
shared translation task.22
Although there are larger monolingual corpora available for the target languages included in the evaluation setup, they have not been used because our experiments are focused on evaluating the impact of the RBMT data on the PBSMT translation model. By
learning the TL model from a monolingual corpus that does not exceed the size of the
biggest parallel corpus used in the experiments, the risk that a huge language model will
overshadow the impact of the RBMT data on the SMT translation model is reduced. Note
that, in a real-world environment, the size of the TL model may need to be limited if the
hybrid MT system is required to have a reduced memory footprint, for example, because it
is going to be executed in a handheld device.23
BretonFrench systems were tuned using 3 000 parallel sentences randomly chosen from
the available parallel corpus and evaluated using another randomly chosen subset of the
same size; obviously both subsets were not used for training. Only an in-domain evaluation
could be performed for this language pair. Regarding EnglishSpanish, both in-domain and
out-of-domain evaluations have been carried out. The former was performed by tuning the
systems with 2 000 parallel sentences randomly chosen from the Q4/2000 portion of Europarl
v5 corpus (Koehn, 2005) and evaluating them with 2 000 random parallel sentences from the
same portion of the corpus; special care was taken to avoid the overlapping between the test
and tuning sets. The out-of-domain evaluation was performed by using the newstest2008
set for tuning and the newstest2010 test for testing; both sets belong to the news domain
and are distributed as part of the WMT 2010 shared translation task.24 Table 1 summarises
the data concerning the corpora used in the experiments. Sentences that contain more than
40 tokens were removed from all the parallel corpora, as is customary, in order to avoid
problems with the word alignment tool GIZA++ (Och & Ney, 2003).25
All the experiments were carried out with release 2.1 of the free/open-source PBSMT
system Moses (Koehn et al., 2007) together with the SRILM language modelling toolkit
(Stolcke, 2002), which was used to train a 5-gram language model using interpolated KneserNey discounting (Goodman & Chen, 1998). Word alignments were computed by means of
GIZA++ (Och & Ney, 2003). The weights of the different feature functions were optimised
by means of minimum error rate training (Och, 2003). The parallel corpora were lowercased
and tokenised before training, as were the test sets used to evaluate the systems.
The hand-crafted shallow-transfer rules and dictionaries were borrowed from the Apertium platform (Forcada et al., 2011). In particular, the engine and the linguistic resources
for EnglishSpanish, and BretonFrench were downloaded from the Apertium Subversion
22. http://www.statmt.org/wmt11/translation-task.html
23. There are also more EnglishSpanish parallel corpora available, but they have not been used in the
experiments because one of the main objectives of the hybrid approach presented in this paper, as pointed
out in the introduction, is to alleviate the data sparseness problem in SMT.
24. http://www.statmt.org/wmt10/translation-task.html
25. Preliminary experiments showed that, when sentences contained more than 40 tokens, GIZA++ was not
able to align some of them. Sentences with more than 40 tokens were also removed from the tuning and
test sets in order to ensure that the approach by Eisele et al. (2008) is able to extract all the phrase
pairs needed. Recall that this method needs to align the sentences in the test set with their RBMT
translations.

31

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

Corpus
Language model (English)
Language model (Spanish)

training

Europarl tuning
Europarl testing
newstest2012 tuning
newstest2013 testing

Source
#words #voc
209 562 11 561
836 194 20 883
3 341 577 36 798
12 546 758 61 654
26 595 542 82 585
42 642
5 157
42 114
5 080
34 878
6 209
48 367
7 701

#sentences
1 650 152
1 650 152
10 000
40 000
160 000
600 000
1 272 260
2 000
2 000
1 732
2 215

Target
#words
#voc
45 712 294 110 018
47 734 244 165 896
216 187
15 884
862 789
30 583
3 452 067
55 584
12 971 035
94 315
27 496 270 125 813
43 348
6 411
42 661
6 289
36 410
7 085
50 745
9 277

(a) EnglishSpanish

Corpus
Language model (French)
training
tuning
testing

#sentences
2 041 625
10 000
25 000
54 196
3 000
3 000

Source
#words #voc
146 255 16 711
365 856 27 606
795 045 41 157
44 586
8 340
43 276
8 119

Target
#words
#voc
60 356 583 155 028
146 556
17 588
369 396
28 333
801 780
40 279
45 086
8 907
43 419
8 832

(b) BretonFrench

Table 1: Number of sentences, words, and size of the vocabulary of the training, tuning and
test sets used in the experiments.

repository.26 The Apertium linguistic data contains 326 228 entries in the EnglishSpanish
bilingual dictionary, 284 EnglishSpanish shallow-transfer rules and 138 SpanishEnglish
shallow-transfer rules. Regarding BretonFrench, the bilingual dictionary contains 21 593
entries and there are 254 shallow-transfer rules.27
For each language pair, domain, and training corpus size, the following systems were
built and evaluated:
 baseline: a standard PBSMT system.28
26. Revisions 24177, 22150 and 28674, respectively.
27. The transfer phase is split by Apertium in three steps (Forcada et al., 2011) for the language pairs we have
used, and each step works with its own set of rules. Specifically, the Apertium linguistic data contains
216 chunker rules, 60 interchunk rules, and 7 postchunk rules for EnglishSpanish; 106 chunker rules,
31 interchunk rules, and 7 postchunk rules for SpanishEnglish; and 169 chunker rules, 79 interchunk
rules and 6 postchunk rules for BretonFrench.
28. With the same features as the baseline system of the WMT 2011 shared translation task: http://www.
statmt.org/wmt11/baseline.html.

32

fiIntegrating Shallow-Transfer Rules into Statistical Machine Translation

 Apertium: the Apertium shallow-transfer RBMT engine, from which the dictionaries
and transfer rules were borrowed.
 extended-phrase: the hybrid system described in Section 3 following the strategy for
scoring the phrase pairs generated from the RBMT data described in Section 3.2.3.
 extended-phrase-dict: the same as above, but using only the dictionaries of the RBMT
system (without shallow-transfer rules). The comparison between this system and
extended-phrase permits the evaluation of the impact of the use of shallow-transfer
rules.
 extended-corpus: the hybrid system described in Section 3 following the strategy used
to score the synthetic phrase pairs which simply involves adding the synthetic phrase
pairs to the training corpus (see Section 3.2.4).
 two-phrase-tables: the hybrid system described in Section 3 following the strategy used
to score the synthetic phrase pairs based on two independent phrase tables (Koehn &
Schroeder, 2007) (see Section 3.2.1).
 interpolation: the hybrid system described in Section 3 following the strategy used
to score the synthetic phrase pairs based on the linear interpolation of two phrase
tables (Sennrich, 2012, 2.1) (see Section 3.2.2). The interpolation weights were obtained by means of perplexity minimisation on a phrase table built from the tuning
set.
 Eisele: the approach by Eisele et al. (2008), using the alignment model learnt from
the training corpus to obtain the word alignments between the source sentences and
the RBMT-translated sentences.
4.2 Results and Discussion
Figures 15 show the BLEU (Papineni, Roukos, Ward, & Zhu, 2002) automatic evaluation
score for the systems evaluated; TER (Snover, Dorr, Schwartz, Micciulla, & Makhoul, 2006)
and METEOR (Banerjee & Lavie, 2005) behave in a similar manner. In addition, the statistical significance of the difference between the BLEU, TER and METEOR scores obtained
by the hybridisation approach extended-phrase (see Section 3.2.3) and those obtained by
the other systems has been computed by means of paired bootstrap resampling (Koehn,
2004) (p  0.05; 1 000 iterations).29 The results of this pair-wise comparison are reported
in a table, included in each figure, in which each cell represents the reference system to
which the approach extended-phrase is compared and the training corpus size; the table
contains the results for the three evaluation metrics: BLEU (B), TER (T) and METEOR
(M). An arrow pointing upwards () means that extended-phrase outperforms the reference
system, an arrow pointing downwards () means that the reference system outperforms
extended-phrase, and an equal sign (=) means that the difference between both systems is
not statistically significant.
29. Only the extended-phrase is compared to the other systems because it is expected to achieve the highest
translation quality of the different hybrid approaches, as in theory it overcomes most of the limitations
of the other approaches (see Section 3.2).

33

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

0.32
0.3

BLEU score

0.28
0.26

0.16
0.14

baseline
extended-phrase
extended-phrase-dict
extended-corpus

10000

two-phrase-tables
interpolation
Eisele
Apertium

40000
160000
600000
Size of the training corpus (in sentences)

1272260

(a) BLEU scores.

system
metric
baseline
Apertium
extended-phrase-dict
extended-corpus
two-phrase-tables
interpolation
Eisele

10 000
BTM



===
===
===


40 000
BTM


==
===
===
===
==

160 000
BTM
=

===
=
=
===


600 000
BTM
===

===
=
=
===


1 272 260
BTM
==

==
===
==
===


(b) Paired bootstrap resampling comparison (p  0.05; 1 000 iterations) between extended-phrase
and the other methods being evaluated (a method per row). Columns represent training corpus sizes
and evaluation metrics: BLEU (B), TER (T) and METEOR (M).  means that extended-phrase
outperforms the reference method by a statistically significant margin,  means the opposite, and
= means that there is no statistically significant difference between them.

Figure 1: For the EnglishSpanish in-domain evaluation, automatic evaluation scores obtained for the baseline PBSMT system, Apertium, the hybrid approaches described in
Section 3.2, and the hybrid approach by Eisele et al. (2008). The table shows a pair-wise
comparison with the system extended-phrase (see Section 3.2.3).

These results show that the hybrid approach described in Section 3 (extended-phrase)
outperforms both the RBMT and the baseline PBSMT system by a statistically significant
margin in different scenarios. Namely, when translating out-of-domain texts (texts whose
domain is different from the domain of the parallel corpus used; this occurs for all training
corpus sizes and language pairs) and when translating in-domain texts with an SMT system
trained on a relatively small parallel corpus. Thus, as was found in literature (see Section 2),
34

fiIntegrating Shallow-Transfer Rules into Statistical Machine Translation

0.28

0.26

BLEU score

0.24

0.22

0.2

0.18

0.16

0.14
10000

baseline
extended-phrase
extended-phrase-dict
extended-corpus

two-phrase-tables
interpolation
Eisele
Apertium

40000
160000
600000
Size of the training corpus (in sentences)

1272260

(a) BLEU scores.

system
metric
baseline
Apertium
extended-phrase-dict
extended-corpus
two-phrase-tables
interpolation
Eisele

10 000
BTM



==

===


40 000
BTM




==
==


160 000
BTM



==
=



600 000
BTM



==




1 272 260
BTM



==




(b) Paired bootstrap resampling comparison (p  0.05; 1 000 iterations) between extended-phrase
and the other methods being evaluated (a method per row). Columns represent training corpus sizes
and evaluation metrics: BLEU (B), TER (T) and METEOR (M).  means that extended-phrase
outperforms the reference method by a statistically significant margin,  means the opposite, and
= means that there is no statistically significant difference between them.

Figure 2: For the EnglishSpanish out-of-domain evaluation, automatic evaluation scores
obtained for the baseline PBSMT system, Apertium, the hybrid approaches described in
Section 3.2, and the hybrid approach by Eisele et al. (2008). The table shows a pair-wise
comparison with the system extended-phrase (see Section 3.2.3).

it is possible to confirm that shallow-transfer RBMT and PBSMT systems can be combined
in a hybrid system that outperforms both of them.
With regard to the differences observed in the results for the in-domain and out-ofdomain evaluations, it is important to state that, for EnglishSpanish, the out-of-domain
tuning and test sets come from a general (news) domain and the RBMT data has been
developed bearing in mind the translation of general texts (mainly news). In this case,
Apertium-generated (synthetic) phrase pairs, which contain hand-crafted knowledge from
35

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

0.32
0.3

BLEU score

0.28
0.26
0.24

0.16
0.14
0.12
10000

baseline
extended-phrase
extended-phrase-dict
extended-corpus

two-phrase-tables
interpolation
Eisele
Apertium

40000
160000
600000
Size of the training corpus (in sentences)

1272260

(a) BLEU scores.

system
metric
baseline
Apertium
extended-phrase-dict
extended-corpus
two-phrase-tables
interpolation
Eisele

10 000
BTM


==
==
===
==


40 000
BTM


===
=
===
===


160 000
BTM


===
===
==
===
=

600 000
BTM
===

==
===
===
===
=

1 272 260
BTM
===

==

===
===
=

(b) Paired bootstrap resampling comparison (p  0.05; 1 000 iterations) between extended-phrase
and the other methods being evaluated (a method per row). Columns represent training corpus sizes
and evaluation metrics: BLEU (B), TER (T) and METEOR (M).  means that extended-phrase
outperforms the reference method by a statistically significant margin,  means the opposite, and
= means that there is no statistically significant difference between them.

Figure 3: For the SpanishEnglish in-domain evaluation, automatic evaluation scores obtained for the baseline PBSMT system, Apertium, the hybrid approaches described in
Section 3.2, and the hybrid approach by Eisele et al. (2008). The table shows a pair-wise
comparison with the system extended-phrase (see Section 3.2.3).

a general domain, cover sequences of words in the input text which are not covered, or are
sparsely found, in the original training corpus. Contrarily, the in-domain tests reveal that,
as soon as the PBSMT system is able to learn some reliable information from the parallel
corpus, the synthetic RBMT phrase pairs become useless because the in-domain test sets
come from the specialised domain of parliament speeches. For BretonFrench, given the
small size of the corpus available, the hybrid approach outperforms both pure RBMT and
PBSMT approaches in all the experiments performed.
36

fiIntegrating Shallow-Transfer Rules into Statistical Machine Translation

0.28

0.26

BLEU score

0.24

0.22

0.2

0.18

0.16

0.14
10000

baseline
extended-phrase
extended-phrase-dict
extended-corpus

two-phrase-tables
interpolation
Eisele
Apertium

40000
160000
600000
Size of the training corpus (in sentences)

1272260

(a) BLEU scores.

system
metric
baseline
Apertium
extended-phrase-dict
extended-corpus
two-phrase-tables
interpolation
Eisele

10 000
BTM



=
===
===


40 000
BTM



==
==
=


160 000
BTM



=
=
==


600 000
BTM


==
===
=



1 272 260
BTM



==
===



(b) Paired bootstrap resampling comparison (p  0.05; 1 000 iterations) between extended-phrase
and the other methods being evaluated (a method per row). Columns represent training corpus sizes
and evaluation metrics: BLEU (B), TER (T) and METEOR (M).  means that extended-phrase
outperforms the reference method by a statistically significant margin,  means the opposite, and
= means that there is no statistically significant difference between them.

Figure 4: For the SpanishEnglish out-of-domain evaluation, automatic evaluation scores
obtained for the baseline PBSMT system, Apertium, the hybrid approaches described in
Section 3.2, and the hybrid approach by Eisele et al. (2008). The table shows a pair-wise
comparison with the system extended-phrase (see Section 3.2.3).

An analysis of the proportion of synthetic phrase pairs included by the decoder in
the final translation30 for the different evaluation scenarios, depicted in figures 68, confirms the reason for the differences between the in-domain and out-of-domain results. For
each EnglishSpanish training corpus size and hybrid system, the proportion of synthetic
phrases is higher in the out-of-domain evaluation.
30. If a synthetic phrase pair has also been obtained from the parallel corpus, it is not considered as synthetic
in figures 68.

37

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

0.26
0.24

BLEU score

0.22
0.2
0.18
0.16
0.14
baseline
extended-phrase
extended-phrase-dict
extended-corpus

0.12
0.1
10000

two-phrase-tables
interpolation
Eisele
Apertium

25000
Size of the training corpus (in sentences)

54196

(a) BLEU scores.

system
metric
baseline
Apertium
extended-phrase-dict
extended-corpus
two-phrase-tables
interpolation
Eisele

10 000
BTM


===
==
==
==


25 000
BTM


===
=
===
==


54 196
BTM
=

===
=
===
===


(b) Paired bootstrap resampling comparison (p  0.05; 1 000 iterations) between extended-phrase
and the other methods being evaluated (a method per row). Columns represent training corpus sizes
and evaluation metrics: BLEU (B), TER (T) and METEOR (M).  means that extended-phrase
outperforms the reference method by a statistically significant margin,  means the opposite, and
= means that there is no statistically significant difference between them.

Figure 5: For the BretonFrench in-domain evaluation, automatic evaluation scores obtained for the baseline PBSMT system, Apertium, the hybrid approaches described in
Section 3.2, and the hybrid approach by Eisele et al. (2008). The table shows a pair-wise
comparison with the system extended-phrase (see Section 3.2.3).

Regarding the difference between the hybrid systems enriched with all the RBMT resources (extended-phrase) and those that only include the dictionary (extended-phrase-dict),
some patterns can be detected. For EnglishSpanish, the impact of the shallow-transfer
rules is higher when translating out-of-domain texts and decreases as the training corpus
grows. Their impact is therefore higher when the decoder chooses a high proportion of
Apertium phrases (see figures 6 and 7). Moreover, the systems including shallow-transfer
rules outperform their counterparts which only include the dictionary by a wider margin
38

fiIntegrating Shallow-Transfer Rules into Statistical Machine Translation

1

two-phrase-tables
interpolation
Eisele

extended-phrase
extended-phrase-dict
extended-corpus

Proportion of synthetic phrases

0.8

0.6

0.4

0.2

0
10000

40000
160000
600000
Size of the training corpus (in sentences)

1272260

(a) In-domain evaluation.
1

two-phrase-tables
interpolation
Eisele

extended-phrase
extended-phrase-dict
extended-corpus

Proportion of synthetic phrases

0.8

0.6

0.4

0.2

0
10000

40000
160000
600000
Size of the training corpus (in sentences)

1272260

(b) Out-of-domain evaluation.

Figure 6: For EnglishSpanish, proportion of phrase pairs generated from the RBMT data
and chosen by the decoder when translating the test set with the different hybrid approaches
described in Section 3.2 and the hybrid approach by Eisele et al. (2008).

when translating out-of-domain texts from English to Spanish than the other way round.
As Spanish morphology is richer, transfer rules help to perform more agreement operations
when translating into Spanish. On the contrary, when Spanish is the source language, one
of the main limitations suffered by the baseline PBSMT system is the high number of outof-vocabulary (OOV) words, which is already mitigated by integrating the dictionaries into
39

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

1

two-phrase-tables
interpolation
Eisele

extended-phrase
extended-phrase-dict
extended-corpus

Proportion of synthetic phrases

0.8

0.6

0.4

0.2

0
10000

40000
160000
600000
Size of the training corpus (in sentences)

1272260

(a) In-domain evaluation.
1

two-phrase-tables
interpolation
Eisele

extended-phrase
extended-phrase-dict
extended-corpus

Proportion of synthetic phrases

0.8

0.6

0.4

0.2

0
10000

40000
160000
600000
Size of the training corpus (in sentences)

1272260

(b) Out-of-domain evaluation.

Figure 7: For SpanishEnglish, proportion of phrase pairs generated from the RBMT data
and chosen by the decoder when translating the test set with the different hybrid approaches
described in Section 3.2 and the hybrid approach by Eisele et al. (2008).

the phrase table with the extended-phrase-dict approach, as shown in figures 911.31 These
figures show that the amount of OOV words is much higher for the baseline system when
31. In the approach by Eisele et al. (2008) the number of OOV words is always 0 because the phrase table
contains phrase pairs obtained by translating the test set with the RBMT system, and the RBMT system
copies verbatim to the output those words that do not appear in its dictionaries.

40

fiIntegrating Shallow-Transfer Rules into Statistical Machine Translation

1

two-phrase-tables
interpolation
Eisele

extended-phrase
extended-phrase-dict
extended-corpus

Proportion of synthetic phrases

0.8

0.6

0.4

0.2

0
10000

25000
Size of the training corpus (in sentences)

54196

Figure 8: For BretonFrench, proportion of phrase pairs generated from the RBMT data
and chosen by the decoder when translating the test set with the different hybrid approaches
described in Section 3.2 and the hybrid approach by Eisele et al. (2008).

the SL is Spanish than when the SL is English and that the reduction in the amount of
OOVs when adding the RBMT dictionaries is consequently also higher in the first case.
In contrast, the positive impact of the rules is very limited in the EnglishSpanish
in-domain evaluation, where a statistically significant improvement to the hybrid system
enriched solely with dictionaries (according to the three evaluation metrics) can only be
observed for the smallest EnglishSpanish training corpus. In fact, for a few training
corpus sizes, the inclusion of the shallow-transfer rules in the hybrid system produces a
statistically significant drop in translation quality according to one of the three evaluation
metrics (METEOR in the case of EnglishSpanish in-domain evaluation and TER in the
case of SpanishEnglish). When the training parallel corpus belongs to the same domain as
the test corpus, corpus-extracted phrase pairs are likely to contain more accurate and fluent
translations when compared to the mechanical and regular translations provided by the
RBMT shallow-transfer rules. One possible explanation for the fact that the degradation
caused by the rules is only measured by TER or METEOR is that we used BLEU for
tuning (Och, 2003). Consequently, the weight of the feature function which flags whether
a phrase pairs comes from the parallel corpus or from the RBMT system is set so that the
inclusion of shallow-transfer rules does not penalise the translation quality as measured by
BLEU. The effect of using other evaluation metrics for tuning has yet to be studied.
With regard to BretonFrench, the impact of the shallow-transfer rules is also limited:
the difference between the hybrid system enriched with shallow-transfer rules and the system
enriched only with dictionaries is not statistically significant for any of the training corpus
sizes evaluated. The reason is probably that the sentences from the test set do not have a
complex grammatical structure: the average sentence length is about 9 words (Tyers, 2009)
and it contains many sentences that are simply noun phrases. Another possible reason
41

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

2500

Number of OOVs

2000

two-phrase-tables
interpolation
Eisele

baseline
extended-phrase
extended-phrase-dict
extended-corpus

1500

1000

500

0

-500
10000

40000
160000
600000
Size of the training corpus (in sentences)

1272260

(a) In-domain evaluation.
9000
8000
7000

two-phrase-tables
interpolation
Eisele

baseline
extended-phrase
extended-phrase-dict
extended-corpus

Number of OOVs

6000
5000
4000
3000
2000
1000
0
-1000
10000

40000
160000
600000
Size of the training corpus (in sentences)

1272260

(b) Out-of-domain evaluation.

Figure 9: For EnglishSpanish, number of out-of-vocabulary words in the test set for the
different hybrid approaches described in Section 3.2 and the hybrid approach by Eisele et al.
(2008).

may be the fact that the quality of the BretonFrench shallow-transfer rules may be lower
than the quality of the rules used for other language pairs, since the effort spent in their
development was smaller.
As regards the different phrase scoring approaches defined in Section 3.2, some differences can be observed. The most remarkable differences show up when the inclusion of
synthetic phrase pairs has a great impact, that is, in EnglishSpanish out-of-domain eval42

fiIntegrating Shallow-Transfer Rules into Statistical Machine Translation

3500

two-phrase-tables
interpolation
Eisele

baseline
extended-phrase
extended-phrase-dict
extended-corpus

3000

Number of OOVs

2500
2000
1500
1000
500
0
-500
10000

40000
160000
600000
Size of the training corpus (in sentences)

1272260

(a) In-domain evaluation.
12000

10000

two-phrase-tables
interpolation
Eisele

baseline
extended-phrase
extended-phrase-dict
extended-corpus

Number of OOVs

8000

6000

4000

2000

0

-2000
10000

40000
160000
600000
Size of the training corpus (in sentences)

1272260

(b) Out-of-domain evaluation.

Figure 10: For SpanishEnglish, number of out-of-vocabulary words in the test set for
the different hybrid approaches described in Section 3.2 and the hybrid approach by Eisele
et al. (2008).

uations. Firstly, the interpolation strategy is frequently outperformed by other strategies,
and the hybrid systems built with it usually choose a relatively small proportion of synthetic phrase pairs. In theory, it should outperform the two-phrase-tables strategy because
it assigns higher probabilities to synthetic phrase pairs that are also found in the training parallel corpus, but actually the two-phrase-tables approach generally achieves a higher
translation quality. One possible reason for this result may be the fact that, while in the
43

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

6000

two-phrase-tables
interpolation
Eisele

baseline
extended-phrase
extended-phrase-dict
extended-corpus

5000

Number of OOVs

4000

3000

2000

1000

0

-1000
10000

25000
Size of the training corpus (in sentences)

54196

Figure 11: For BretonFrench, number of out-of-vocabulary words in the test set for the
different hybrid approaches described in Section 3.2 and the hybrid approach by Eisele et al.
(2008).

interpolation method the relative weights of the two types of phrase pairs are optimised so
as to minimise the perplexity on a set of phrase pairs extracted from a tuning corpus, in the
two-phrase-tables strategy the relative weights are optimised so as to maximise translation
quality by the minimum error rate tuning algorithm. In the latter case, the interaction
of phrase pairs with the rest of the elements of the PBSMT system is taken into account
during the tuning process. Nevertheless, additional experiments whose objective will be to
carry out an in-depth evaluation of the impact of the method used to optimise the relative
weight of both types of phrase pairs will need to be carried out. Concerning the extendedcorpus strategy, it does not consistently outperform the other strategies, probably because
the synthetic phrase pairs were too short for their subphrases to clearly improve the reordering model. However, as already stated, this strategy could not be used in a real-world
setting because of the high computational cost of aligning the synthetic phrase pairs and
the training corpus together for every document to be translated. Finally, the two-phrasetables strategy is outperformed by the extended-phrase strategy in the experiments carried
out with the EnglishSpanish language pair (except in the smallest training corpus size,
where the effect of increasing the probability of the phrase pairs that appear in both phrase
tables, as described in Section 3.2.1, is less relevant). For the reverse language pair, the
two-phrase-tables strategy is sometimes better, but the three evaluation metrics never agree
and the difference between both strategies is small when compared to EnglishSpanish.
These results suggest that, at least in the evaluation scenario where the shallow-transfer
rules have the highest impact, the phrase scoring strategy defined in Section 3.2.3 is able
to achieve a better balance between the two sources of phrase pairs.
Finally, the hybridisation strategy defined in Section 3, together with the phrase scoring
strategy defined in Section 3.2.3, outperforms the approach by Eisele et al. (2008) for all
44

fiIntegrating Shallow-Transfer Rules into Statistical Machine Translation

language pairs, training corpus sizes and domains. The biggest difference between both
approaches is observed when small corpora are used for training. As has been anticipated
in Section 2.1, under such circumstances, no reliable alignment models can be learnt from
the training corpus and therefore no reliable phrase pairs can be obtained from the input text
and its RBMT translation. The approach presented in this work, contrarily, is not affected
by this issue because it does not rely on word alignments in order to generate phrase pairs
from the RBMT system. In addition, there is a significant difference even when the training
corpus is relatively big (more than one million parallel sentences). The high proportion of
synthetic phrase pairs used when compared to the other hybrid approaches (see figures 68)
suggests that the approach by Eisele et al. is not able to find an adequate balance between
both types of phrase pairs. This may be because synthetic phrase pairs are even extracted
from SL segments that do not match a transfer rule and because of the straightforward
scoring method used, which simply consists of concatenating the phrase table obtained
from the training parallel corpus and that obtained from the RBMT system.

5. Evaluation with Automatically Inferred Rules
As has been empirically proved in the previous section, shallow-transfer rules can improve
the performance of PBSMT. However, a considerable human effort and a high level of
linguistic knowledge are needed to create them. In order to reduce the degree of human
effort required to achieve such improvement, the algorithm proposed by Sanchez-Cartagena
et al. (2015) can be used to infer a set of shallow-transfer rules from the training parallel
corpus from which the PBSMT models are built, and this set of rules, together with the
bilingual dictionary, can be used to enlarge the phrase table as previously described. A
significant boost in translation quality could thus be achieved with the sole addition of
RBMT dictionaries. In this section, a set of experiments whose objective was to assess the
viability of this approach is presented.
The method proposed by Sanchez-Cartagena et al. (2015) uses parallel corpora to infer
shallow-transfer rules that are compatible with the formalism used by Apertium (Forcada
et al., 2011). Their approach is inspired by the method by Sanchez-Martnez and Forcada
(2009), uses a generalisation of the alignment template formalism (Och & Ney, 2004) to
encode transfer rules, and overcomes important limitations of the method by SanchezMartnez and Forcada (2009). We refer the reader to the paper by Sanchez-Cartagena et al.
for a thorough description of these limitations.
The approach by Sanchez-Cartagena et al. (2015) is the first in literature in which the
problem of automatically inferring transfer rules is reduced to finding the optimal value of a
minimisation problem. They prove that the translation quality achieved with the automatically inferred rules is generally close to that obtained with hand-crafted rules. Moreover,
for some language pairs, the automatically inferred rules are even able to outperform the
hand-crafted ones.
5.1 Experimental Setup
Two considerations should be borne in mind when inferring a set of shallow-transfer rules
to be integrated into the PBSMT system. Firstly, the experiments conducted by SanchezCartagena et al. (2015) concluded that one of the features of the rule inference algorithm, the
45

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

generalisation of alignment templates to combinations of values of morphological inflection
attributes not observed in the training corpus, is one of the causes of the vast complexity
of the aforementioned minimisation problem and brings a significant translation quality
boost only when the training corpus is very small (below 1 000 parallel sentences). Given
the fact that the parallel corpus sizes for which an SMT system starts to be competitive
are much bigger, the generalisation of morphological inflection attributes can be skipped
when inferring shallow-transfer rules to be integrated into PBSMT. Moreover, preliminary
experiments showed that, even when disabling the generalisation to non-observed combinations of values of morphological inflection attributes, the global minimisation algorithm
still needs a huge amount of processing time in order to infer a set of rules from a parallel
corpus that contains hundreds of thousands of parallel sentences.
Secondly, the rule inference algorithm by Sanchez-Cartagena et al. (2015) filters the rules
to be generated so as to ensure that, when they are applied by a shallow-transfer RBMT
system in a greedy, left-to-right, longest-match way, the groups of words which need to be
processed together are translated with the same rule. From here on, we shall refer to this
process as optimising the rules for chunking. Since, in principle, the SMT decoder splits
the input sentences in all possible ways, this process might not be needed. Shallow-transfer
rules for all the sequences of SL lexical categories present in the corpus would therefore be
generated.
We ran some preliminary experiments and the results showed that there are no consistent
differences between the systems whose rules have been optimised for chunking and the
systems whose rules have not: statistically significant differences can only be found only
for some of the evaluation metrics. For SpanishEnglish, optimising rules for chunking
brings a tiny improvement, while for EnglishSpanish, the effect is the opposite. Since
the impact of the rules is higher for the translation of out-of-domain texts, the effect of the
optimisation is also more noticeable in this scenario.
The optimisation of rules for chunking affects the resulting hybrid system in two ways.
On the one hand, it prevents the inclusion in the phrase table of multiple noisy phrase pairs
that were generated from shallow-transfer rules that match sequences of lexical categories
that do not need to be processed together when translating between the languages involved.
Owing to the fact that the decoder cannot evaluate all the translation hypotheses, these
useless phrase pairs may prevent other, more important phrase pairs from being included
in the final translation. It may also occur that the language model does not have enough
information to properly score the synthetic phrase pairs built from these noisy rules. From
this point of view, the optimisation of rules for chunking should have a positive impact
on translation quality. Furthermore, since an SMT system does not perform a greedy
segmentation of the input sentence, some of the rules discarded during the optimisation for
chunking in RBMT may still be useful if they are included in a PBSMT system. Rules
that would prevent the application of a more important rule by the RBMT engine do not
prevent the application of that rule in the hybrid system because, in principle, all the
possible segmentations are taken into account. In the light of our preliminary results, it
seems that the former is more relevant for SpanishEnglish, while the latter has a higher
positive impact for EnglishSpanish. Since Spanish is morphologically more complex,
more rules are needed to correctly perform agreements, and more rules discarded during
46

fiIntegrating Shallow-Transfer Rules into Statistical Machine Translation

the optimisation for chunking were probably useful. Nevertheless, these differences have yet
to be studied in greater depth.
Bearing these considerations in mind, our experiments have been carried out as follows.
For the same language pairs, corpora and RBMT dictionaries used in the previous section,
a new system, extended-phrase-learnt, has been built; in this system, the rule inference algorithm described by Sanchez-Cartagena et al. (2015) has been applied to the training corpus
and the optimisation of rules for chunking has not been performed. The rules inferred,
together with the dictionaries, have been used to enrich the PBSMT system following the
hybridisation strategy described in Section 3. Because of the time complexity of the minimisation problem to be solved by the rule inference approach, only the first 160 000 sentences
of the training corpus have been used for rule inference in those cases in which the corpus
was larger than 160 000 sentences. In other words, the systems built from 160 000, 600 000,
and the whole set of parallel sentences use exactly the same set of shallow-transfer rules.32
We compare the new system to a pure PBSMT baseline built from the same data, a
hybrid system built from the Apertium hand-crafted rules and dictionaries, a hybrid system
built with the same strategy but only from the Apertium dictionaries, and two different
versions of the RBMT system Apertium: one version using hand-crafted rules and another
version with automatically inferred rules. In all the hybrid systems, the scoring method
described in Section 3.2.3 has been used, since this is the scoring method that proved to
perform best in the experiments described in the previous section.
5.2 Results and Discussion
The comparison of the hybrid approach extended-phrase-learnt to the other approaches
being considered in this section is presented in figures 1216. The results show the BLEU
(Papineni et al., 2002) automatic evaluation score for the different systems evaluated; TER
and METEOR behavie is a similar way. In addition, the statistical significance33 of the
difference between extended-phrase-learnt and the other systems is also presented in a table,
in the same way as depicted in the previous section.
The comparison to the PBSMT baseline and the pure RBMT system shows that our
hybrid approach with automatically inferred rules behaves in the same way as when handcrafted rules are used: it outperforms both baselines when the training corpus is small or
an out-of-domain text is translated. If the comparison is performed with the hybrid system
that only uses dictionaries, our hybrid approach also outperforms the dictionary-based
approach in almost the same cases as the hybrid approach with hand-crafted rules: outof-domain evaluation and in-domain evaluation only with the smallest parallel corpus size,
although the three evaluation metrics do not agree in the latter case. In other words, with
the automatic inference of shallow-transfer rules, a statistically significant improvement to
the approach that uses only dictionaries has been achieved without using any additional
linguistic resources.
32. In addition, the part of the training corpus used for rule inference has been split into two parts: the
first 4/5 of the corpus has been used for actual rule inference, while the last 1/5 has been employed as
a development corpus in order to optimise the threshold , as in the experiments described by SanchezCartagena et al. (2015). For training corpora bigger than 10 000 sentences, only 2 000 sentences have
been used for optimising , while the remaining part of the corpus has been used for rule inference.
33. Again obtained through paired bootstrap resampling (Koehn, 2004) (p  0.05; 1 000 iterations).

47

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

0.32

0.3

BLEU score

0.28

0.26

0.16

0.14

baseline
extended-phrase
extended-phrase-dict

10000

extended-phrase-learnt
Apertium-learnt
Apertium

40000
160000
600000
Size of the training corpus (in sentences)

1272260

(a) BLEU scores.

system
metric
baseline
Apertium
Apertium-learnt
extended-phrase-dict
extended-phrase

10 000
BTM



==
===

40 000
BTM



===
==

160 000
BTM
===


=
=

600 000
BTM
===


===
===

1 272 260
BTM
==


==
===

(b) Paired bootstrap resampling comparison (p  0.05; 1 000 iterations) between extended-phraselearnt and the other methods being evaluated (a method per row). Columns represent training corpus
sizes and evaluation metrics: BLEU (B), TER (T) and METEOR (M).  means that extendedphrase-learnt outperforms the reference method by a statistically significant margin,  means the
opposite, and = means that there is no statistically significant difference between them.

Figure 12: For the EnglishSpanish in-domain evaluation, automatic evaluation scores
obtained for the baseline PBSMT system, Apertium with hand-crafted rules (Apertiumlearnt), Apertium with learnt rules (Apertium-learnt), and our hybrid approach (described
in Section 3.2.3) using hand-crafted shallow-transfer rules (extended-phrase), a set of rules
inferred from the training corpus (extended-phrase-learnt) and no rules at all (extendedphrase-dict). The table shows a pair-wise comparison with the system extended-phraselearnt.

In some cases there is no statistically significant difference between the hybrid system
with hand-crafted rules and the hybrid system with automatically inferred rules. This
occurs, for instance, in the EnglishSpanish out-of-domain evaluation when the training
corpus contains 600 000 sentence pairs. A translation quality similar to that obtained with
hand-crafted rules has therefore been attained without the intervention of the human ex48

fiIntegrating Shallow-Transfer Rules into Statistical Machine Translation

0.26

0.24

BLEU score

0.22

0.2

0.18

0.16

0.14
10000

baseline
extended-phrase
extended-phrase-dict

extended-phrase-learnt
Apertium-learnt
Apertium

40000
160000
600000
Size of the training corpus (in sentences)

1272260

(a) BLEU scores.

system
metric
baseline
Apertium
Apertium-learnt
extended-phrase-dict
extended-phrase

10 000
BTM






40 000
BTM






160 000
BTM






600 000
BTM




===

1 272 260
BTM




=

(b) Paired bootstrap resampling comparison (p  0.05; 1 000 iterations) between extended-phraselearnt and the other methods being evaluated (a method per row). Columns represent training corpus
sizes and evaluation metrics: BLEU (B), TER (T) and METEOR (M).  means that extendedphrase-learnt outperforms the reference method by a statistically significant margin,  means the
opposite, and = means that there is no statistically significant difference between them.

Figure 13: For the EnglishSpanish out-of-domain evaluation, automatic evaluation scores
obtained for the baseline PBSMT system, Apertium with hand-crafted rules (Apertiumlearnt), Apertium with learnt rules (Apertium-learnt), and our hybrid approach (described
in Section 3.2.3) using hand-crafted shallow-transfer rules (extended-phrase), a set of rules
inferred from the training corpus (extended-phrase-learnt) and no rules at all (extendedphrase-dict). The table shows a pair-wise comparison with the system extended-phraselearnt.
perts who usually create them.34 In the rest of the cases, where the hybrid system with
34. Although the translation quality of both systems is similar according to automatic evaluation metrics,
there are differences in the amount of rules used in each case. While the set of hand-crafted rules in
the Apertium platform contains a few hundred rules for each language pair, the number of inferred
rules ranges from 2 000 to 75 000, depending on the language pair and size of the training parallel
corpus. These figures are not directly comparable, since the rule formalism used for the hand-crafted
rules is more expressive than that of the automatically inferred rules (Sanchez-Cartagena et al., 2015,

49

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

0.32
0.3

BLEU score

0.28
0.26
0.24
0.18
0.16
0.14

baseline
extended-phrase
extended-phrase-dict

10000

extended-phrase-learnt
Apertium-learnt
Apertium

40000
160000
600000
Size of the training corpus (in sentences)

1272260

(a) BLEU scores.

system
metric
baseline
Apertium
Apertium-learnt
extended-phrase-dict
extended-phrase

10 000
BTM



===
=

40 000
BTM



==
==

160 000
BTM



==
===

600 000
BTM
===


==
===

1 272 260
BTM
===


=
===

(b) Paired bootstrap resampling comparison (p  0.05; 1 000 iterations) between extended-phraselearnt and the other methods being evaluated (a method per row). Columns represent training corpus
sizes and evaluation metrics: BLEU (B), TER (T) and METEOR (M).  means that extendedphrase-learnt outperforms the reference method by a statistically significant margin,  means the
opposite, and = means that there is no statistically significant difference between them.

Figure 14: For the SpanishEnglish in-domain evaluation, automatic evaluation scores
obtained for the baseline PBSMT system, Apertium with hand-crafted rules (Apertiumlearnt), Apertium with learnt rules (Apertium-learnt), and our hybrid approach (described
in Section 3.2.3) using hand-crafted shallow-transfer rules (extended-phrase), a set of rules
inferred from the training corpus (extended-phrase-learnt) and no rules at all (extendedphrase-dict). The table shows a pair-wise comparison with the system extended-phraselearnt.

hand-crafted rules outperforms the hybrid system with dictionaries, the translation quality
achieved by the hybrid system with automatically inferred rules (extended-phrase-learnt)
lies in-between.
3). Nevertheless, the error analysis described in Section 6.2 shows that the automatically inferred rules
contain many exceptions applied to particular words that are not included in the hand-crafted ones.

50

fiIntegrating Shallow-Transfer Rules into Statistical Machine Translation

0.26

0.24

BLEU score

0.22

0.2

0.18

0.16

0.14
10000

baseline
extended-phrase
extended-phrase-dict

extended-phrase-learnt
Apertium-learnt
Apertium

40000
160000
600000
Size of the training corpus (in sentences)

1272260

(a) BLEU scores.

system
metric
baseline
Apertium
Apertium-learnt
extended-phrase-dict
extended-phrase

10 000
BTM



=


40 000
BTM



=
=

160 000
BTM



==
===

600 000
BTM



===
==

1 272 260
BTM



=
==

(b) Paired bootstrap resampling comparison (p  0.05; 1 000 iterations) between extended-phraselearnt and the other methods being evaluated (a method per row). Columns represent training corpus
sizes and evaluation metrics: BLEU (B), TER (T) and METEOR (M).  means that extendedphrase-learnt outperforms the reference method by a statistically significant margin,  means the
opposite, and = means that there is no statistically significant difference between them.

Figure 15: For the SpanishEnglish out-of-domain evaluation, automatic evaluation scores
obtained for the baseline PBSMT system, Apertium with hand-crafted rules (Apertiumlearnt), Apertium with learnt rules (Apertium-learnt), and our hybrid approach (described
in Section 3.2.3) using hand-crafted shallow-transfer rules (extended-phrase), a set of rules
inferred from the training corpus (extended-phrase-learnt) and no rules at all (extendedphrase-dict). The table shows a pair-wise comparison with the system extended-phraselearnt.

In addition, it is worth noting that the translation quality of the approach extendedphrase-learnt does not drop when the size of the training corpus exceeds 160 000 sentences
and the full training corpus is not used for rule inference. In fact, under these circumstances (600 000 parallel sentences) there are not significant differences between the use of
automatically inferred rules and hand-crafted rules in hybrid systems (EnglishSpanish,
out-of-domain evaluation). This observation is probably related to the fact that the trans51

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

0.26
0.24
0.22

BLEU score

0.2
0.18
0.16
0.14
0.12
0.1
0.08

baseline
extended-phrase
extended-phrase-dict
10000

extended-phrase-learnt
Apertium-learnt
Apertium

25000
Size of the training corpus (in sentences)

54196

(a) BLEU scores.

system
metric
baseline
Apertium
Apertium-learnt
extended-phrase-dict
extended-phrase

10 000
BTM



==
===

25 000
BTM



==
==

54 196
BTM



===
===

(b) Paired bootstrap resampling comparison (p  0.05; 1 000 iterations) between extended-phraselearnt and the other methods being evaluated (a method per row). Columns represent training corpus
sizes and evaluation metrics: BLEU (B), TER (T) and METEOR (M).  means that extendedphrase-learnt outperforms the reference method by a statistically significant margin,  means the
opposite, and = means that there is no statistically significant difference between them.

Figure 16: For the BretonFrench in-domain evaluation, automatic evaluation scores
obtained for the baseline PBSMT system, Apertium with hand-crafted rules (Apertiumlearnt), Apertium with learnt rules (Apertium-learnt), and our hybrid approach (described
in Section 3.2.3) using hand-crafted shallow-transfer rules (extended-phrase), a set of rules
inferred from the training corpus (extended-phrase-learnt) and no rules at all (extendedphrase-dict). The table shows a pair-wise comparison with the system extended-phraselearnt.

lation performance of the automatically inferred rules grows very slowly with the size of
the training corpus, and the rules obtained from bigger parallel corpora would probably be
similar to those obtained from the fragment of 160 000 sentences. Nevertheless, the exact
impact of the proportion of the training corpus used for rule inference for different training
corpus sizes, language pairs and domains merits further research.
52

fiIntegrating Shallow-Transfer Rules into Statistical Machine Translation

Finally, it is also worth noting the difference between the hand-crafted rules (Apertium)
and the automatically inferred rules (Apertium-learnt) when they are used in an RBMT
system: in some cases (BretonFrench and EnglishSpanish out-of-domain evaluation)
the difference in translation performance is considerably higher than the difference between
the hybrid systems enriched with hand-crafted rules and with automatically inferred rules
(see figures 13 and 16). This occurs because in RBMT the translation is completely led
by the shallow-transfer rules, and the possible errors encoded in the automatically inferred
rules have a direct impact on the output.

6. Human Evaluation and Error Analysis
This section reports, on the one hand, the results obtained on an out-of-domain human
evaluation performed for EnglishSpanish when the largest training parallel corpus is used,
and, on the other, an analysis of the translation errors performed by the different systems
evaluated in Section 5.
6.1 Human Evaluation
In order to confirm the results obtained with automatic evaluation metrics, we have performed a human evaluation for EnglishSpanish and out-of-domain texts. The systems
included in this human evaluation were those described in the previous section and trained
on the largest parallel corpus used.
We asked 15 users to rank (allowing ties) the translations produced by the baseline
PBSMT system (baseline), Apertium with hand-crafted rules, our hybrid approach using
only dictionaries (extended-phrase-dict), our hybrid approach using automatically inferred
rules (extended-phrase-learnt) and our hybrid approach using hand-crafted rules (extendedphrase). Each user ranked the translations of 50 SL sentences from the test set. The
users were split in 5 groups, and the users in each group ranked exactly the same set of SL
sentences, thus allowing us to compute inter-annotator agreement. In total, the translations
of 250 sentences from the test set were ranked. This evaluation method is similar to that
followed in the WMT 2012 shared translation task (Callison-Burch et al., 2012).
We computed the ratio of wins for each system (Callison-Burch et al., 2012, Eq. 4) as
the proportion of times each system was ranked better than any other system. This score
allows us to sort the systems from best to worst, as is shown in the last row of Table 2. The
resulting ordering is exactly the same as that obtained with automatic evaluation metrics
(see Figure 13).
Table 2 also shows the results of the pairwise comparison between the systems: each
cell represents the proportion of sentences for which the system named after the row label
outperforms the system named after the column label. A score shown in bold type means
that the difference is statistically significant.35 These results entirely confirm the results
obtained with automatic evaluation measures: hybrid systems outperform both RBMT and
PBSMT systems, and the automatically inferred rule allow us to build better hybrid sys35. According to the Sign Test, for p  0.10. We chose a relatively high p-value because of the small amount
of human rankings available.

53

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

extended-phrase
extended-phrase
extended-phrase-l.
extended-phrase-d.
baseline
Apertium
> other

extended-phrase-l.
0.55

0.45
0.40
0.40
0.32
0.61

0.46
0.45
0.35
0.56

extended-phrase-d.
0.60
0.55
0.51
0.35
0.51

baseline
0.60
0.55
0.49
0.37
0.51

Apertium
0.68
0.65
0.65
0.63
0.35

Table 2: Results of the human evaluation; extended-phrase-l. is an abbreviation for extendedphrase-learnt and extended-phrase-d. is an abbreviation for extended-phrase-dict. The last
row represents the proportion of times each system outperforms any other system, while
the remaining cells show the results of a pairwise evaluation: they represent the proportion
of sentences for which the system named after the row label outperforms the system named
after the column label. A score shown in bold type when the system named after the
row label wins more often than the system named after the column label means that the
difference is statistically significant.

tems using just dictionaries as an external resource, i.e.extended-phrase-learnt outperforms
extended-phrase-dict.
Finally, the inter-annotator agreement computed as described by Callison-Burch et al.
(2012, Sec. 3.2) is  = 0.503, which is usually interpreted as a fair agreement.
6.2 Error Analysis
In addition to assessing translation quality by means of automatic evaluation metrics and
human ranking, it is also interesting to compare the different types of errors made by the
systems evaluated in this section. We compared the translations performed by the different
systems used in the human evaluation and found interesting trends that we summarise
below. We focused the analysis on EnglishSpanish because it is the language pair for
which the rules have the highest impact (see Section 4.2). Table 3 shows seven examples of
translations to which we will refer throughout this section.
A comparison between the pure RBMT system Apertium, the baseline PBSMT system
and the hybrid system extended-phrase shows that the two pure systems are complementary
and when they are combined, the number of errors is reduced. When comparing the pure
statistical system with the hybrid one, a reduction in the number of OOV words is observed
(e.g. the word patterned in example #1). There are also words whose translation is too
specific to the domain of the parliament speeches when it is performed by the pure PBSMT
system, but they are translated in a more appropriate way for the news domain by the
hybrid system. An example of this is the word feel in example #2. The differences between
both systems are not just lexical: the hybrid system produces a better agreement between
determiners, nouns and adjectives (see example #3)36 and correctly translates noun phrases
made of adjacent nouns (see example #4),37 among other grammatical improvements.
36. The grammatically correct translation into Spanish of specialised category is categora especializada
37. The correct translation into Spanish of the adjacent nouns Brno socialists is socialistas de Brno; it
literally means socialists of Brno.

54

fiIntegrating Shallow-Transfer Rules into Statistical Machine Translation

When compared to the Apertium RBMT system, the hybrid system produces more
fluent translations in the TL, probably thanks to the use of a TL model. For instance, the
hybrid system deals better with sentences that do not have a regular grammatical structure
(see the translation of It should in example #4).38 Preposition choices are also generally
better in the hybrid system (for instance, the preposition to is correctly removed by the
hybrid system in example #4), as is the translation of phrasal verbs (see how closing down
is translated by the different systems in example #5).
The results of the evaluation show that the translation performance of the hybrid system
built with automatically inferred rules (extended-phrase-learnt) is close to that of the hybrid
system built with hand-crafted rules (extended-phrase; see Figure 13). A manual inspection
of the translations produced reveals that hand-crafted rules and automatically inferred
rules do not produce similar translations. On the one hand, automatically inferred rules
encode many exceptions to general translation rules, which makes them outperform the
hand-crafted ones in the case of some sentences. One common example of this phenomenon
is the swapping of the adjectivenoun sequence. Some adjectives (prepositive adjectives)
must not be swapped when translating them into Spanish and the automatically inferred
rules are able to learn this (for instance, the adjective best in example #6). On the other
hand, hand-crafted rules encode long-range grammatical operations, such as the subjectpredicate agreement in example #7 where invaded is translated as invadieron, which
agrees in person and number with the translation of Some 150 drivers, which could not
be automatically inferred because the rule inference algorithm only considers segments of
at most 5 tokens.

7. Concluding Remarks
In this paper, a hybridisation approach with which to enrich PBSMT models with the
data from shallow-transfer RBMT systems has been presented. It has been confirmed that
data from shallow-transfer RBMT can improve PBSMT systems and also that the resulting
hybrid system outperforms both pure PBSMT and RBMT systems built from the same
data.
Our hybridisation approach overcomes the limitations of the general-purpose strategy
that attempts to improve PBSMT models with data from other MT systems (Eisele et al.,
2008) thanks to the fact that it takes advantage of the way in which the shallow-transfer
RBMT system uses its linguistic resources to segment the SL sentences. The experiments
carried out have shown that our hybrid approach outperforms the strategy by Eisele et al.
by a statistically significant margin in a wide range of situations. In fact, a system (SanchezCartagena et al., 2011b) built with the hybridisation approach described in this work was
one of the winners in the pair-wise manual evaluation of the WMT 2011 shared translation
task (Callison-Burch et al., 2011) for SpanishEnglish.39 The effectiveness of our hybrid
38. The reference sentence, which is a relatively free translation of the SL sentence, does not contain the
word Debera, although it is the most appropriate translation of It should in that context.
39. This evaluation was performed by asking users to rank the translations produced by the different systems.
Users iteratively ranked (from best to worst) the translations of the same SL sentence produced by 5
different systems. We refer the reader to the description of the task by Callison-Burch et al. (2011) for
more details about the evaluation. The human evaluation described in Section 6 was carried out in a
similar way.

55

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

#

1

2

3

system
source
Apertium
baseline
extended-phrase
reference
source
Apertium
baseline
extended-phrase
reference
source
Apertium
baseline
extended-phrase
reference
source
Apertium

4

baseline
extended-phrase
reference

5

source
Apertium
baseline
extended-phrase
reference
source
extended-phrase-l.

6

extended-phrase
reference

7

source
extended-phrase-l.
extended-phrase
reference

sentence
If the inauguration was about thick lace, in Oslo it was patterned velvet.
Si la inauguracion era sobre encaje grueso, en Oslo sea terciopelo estampado.
Si la inauguracion de centro encaje en Oslo, fue patterned.
Si la inauguracion era sobre encaje grueso, en Oslo, fue terciopelo estampado
Si para la toma de posesion de trataba de un encaje gordo,
en Oslo es un terciopelo con estampado.
He feels a kick.
Siente un chut.
El considera una patada.
El siente una patada.
El siente una patada
The only specialised category which nobody won this year was physics.
La categora solo especializada que nadie gano este ano era fsica.
La unica categora especializados que nadie gano este ano ha sido fsica
La unica categora especializada que nadie gano este ano ha sido fsica.
La unica categora especializada, sin una mujer premiada, fue fsica.
It should, but ecology and aesthetics dont seem to interest
the Brno socialists much.
Tenga que, pero la ecologa y la estetica no parecen para interesar
los socialistas de Brno mucho.
Debera, pero la ecologa y la estetica no parecen interes
el Brno socialistas mucho.
Debera, pero la ecologa y la estetica no parecen interes
los socialistas de Brno mucho.
Lo que pasa es que la ecologa y la estetica no parecen interesar demasiado
a los socialistas de Brno.
We are opposed on principle to the closing down of parties.
Somos opposed encima principio al encierro abajo de partidos.
Nos oponemos por principio a la clausura de partidos.
Nos oponemos por principio a la clausura de los partidos.
Por principio nos oponemos a la clausura de partidos.
Theres some of the best skiers and snow borders
in the county here - some real talent, he added.
No hay algunos de los mejores esquiadores y fronteras de nieve
en el condado aqu - un verdadero talento, anadio.
No hay algunos de los esquiadores mejores y fronteras de nieve
en el condado aqu - algunos verdadero talento, anadio.
Aqu se encuentran algunos de los mejores esquiadores y snowboarders
del condado, talento verdadero, anadio.
Some 150 drivers invaded a works council meeting [...]
Unos 150 conductores invadido una reunion del consejo de empresa [...]
Unos 150 conductores invadieron una reunion de consejo de los trabajos [...]
Unos 150 conductores invadieron un comite [...]

Table 3: Translations into Spanish of different English sentences extracted from the out-ofdomain evaluation corpus and produced by the systems evaluated in Section 6. The most
remarkable differences are highlighted. extended-phrase-l. is the abbreviation of extendedphrase-learnt, the hybrid system with automatically inferred rules.

56

fiIntegrating Shallow-Transfer Rules into Statistical Machine Translation

approach is thereby confirmed by both automatic and human evaluation (results in WMT
2011 human evaluation are compatible with those of the human evaluation described in
Section 6: in both experiments, a hybrid system built with our method outperforms a pure
PBSMT system).
Moreover, it has been proved that the rule inference algorithm presented by SanchezCartagena et al. (2015) can be successfully combined with the hybrid approach, thus allowing a hybrid system to be built using dictionaries as the only hand-crafted linguistic
resource. An improvement to translation quality is also achieved in the same way as if
hand-crafted shallow-transfer rules had been used. The hybrid system with automatically
inferred rules is able to attain the translation quality achieved by a hybrid system with
hand-crafted rules and, even when it does not, it often obtains better results than a hybrid
system that only uses dictionaries to enrich the PBSMT models. Additionally, the need for
a human expert to write the rules is avoided.
According to the results obtained, our hybrid approach is especially recommended when
the training parallel corpus (for the translation model) and monolingual corpus (for the
language model) have a moderate size and when the domain of the training corpus is different
from the domain of the texts to be translated.40 The use of moderate-sized training corpora
may be necessary in order to limit the size of the phrase table and the TL model when the
hybrid system must be executed in a mobile device with limited memory. Moreover, the
hybrid approach presented in this work can also be safely applied in other scenarios, since
drops in translation quality in comparison with a PBSMT baseline have not been detected.
If good enough hand-crafted rules are available, it is worth using them instead of inferring
rules from the parallel training corpus, but if they are not, applying the rule inference
algorithm will not significantly degrade translation quality.41
The hybridisation method described in this paper is implemented in a software tool
called rule2Phrase (Sanchez-Cartagena, Sanchez-Martnez, & Perez-Ortiz, 2012) that has
been released under the GNU GPL v3 free software license. Its source code can be freely
downloaded from http://www.dlsi.ua.es/~vmsanchez/Rule2Phrase.tar.gz. The tool
includes the phrase scoring strategies that have been described in sections 3.2.3 and 3.2.4
of this paper.
40. The EnglishSpanish out-of-domain evaluations described in Section 5 were repeated using a TL model
estimated from much bigger monolingual corpora. In particular, a portion of the News Crawl monolingual corpus provided for the WMT 2011 shared translation task (http://www.statmt.org/wmt11/
translation-task.html) was concatenated to the Europarl corpus. As a result, English and Spanish
monolingual corpora with around 6 200 000 sentences each were obtained. The results of the evaluation
showed that when a parallel corpus that contains around 26 000 000 words is used together with these
monolingual corpora, the difference between the hybrid system built with automatically inferred rules
and the baseline SMT system is not statistically significant for some of the evaluation metrics.
41. Manually creating transfer rules involves a huge human effort. Rule writers must first identify the
grammatical divergences between the languages involved that need to be treated by rules and sort them
by frequency in the texts that will be translated by the RBMT system. This operation is called contrastive
analysis. They then write the rules to deal with these divergencies, starting with the most frequent ones
and choosing them in case of possible conflicts between rules. Rules written by humans may not be
good enough if there are grammatical divergencies not identified during the contrastive analysis, their
frequency has not been correctly estimated or enough time has not been invested in writing rules for
dealing with the most important grammatical divergencies.

57

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

Acknowledgments
Research funded by the Spanish Ministry of Economy and Competitiveness through projects
TIN2009-14009-C02-01 and TIN2012-32615, by Generalitat Valenciana through grant ACIF
2010/174, and by the European Union Seventh Framework Programme FP7/2007-2013
under grant agreement PIAP-GA-2012-324414 (Abu-MaTran).

References
Banerjee, S., & Lavie, A. (2005). Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on
Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pp. 6572, Ann Arbor, Michigan, USA.
Bisazza, A., Ruiz, N., & Federico, M. (2011). Fill-up versus interpolation methods for
phrase-based smt adaptation. In Proceedings of the 8th International Workshop on
Spoken Language Translation, San Francisco, California, USA.
Bojar, O., & Hajic, J. (2008). Phrase-based and deep syntactic English-to-Czech statistical
machine translation. In Proceedings of the third Workshop on Statistical Machine
translation, pp. 143146, Columbus, Ohio, USA.
Brown, P. F., Della Pietra, S. A., Della Pietra, V. J., Goldsmith, M. J., Hajic, J., Mercer,
R. L., & Mohanty, S. (1993). But dictionaries are data too. In Proceedings of the
Workshop on Human Language Technology, pp. 202205, Princeton, New Jersey.
Callison-Burch, C., Koehn, P., Monz, C., & Zaidan, O. (2011). Findings of the 2011 workshop on statistical machine translation. In Proceedings of the Sixth Workshop on
Statistical Machine Translation, pp. 2264, Edinburgh, Scotland.
Callison-Burch, C., Koehn, P., Monz, C., Post, M., Soricut, R., & Specia, L. (2012). Findings
of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pp. 1051, Montreal, Canada.
Carl, M. (2007). METIS-II: The German to English MT system. In Proceedings of the XI
Machine Translation Summit, pp. 6573, Copenhagen, Denmark.
Crego, J. (2014). SYSTRAN RBMT engine: hybridization experiments. In 3rd Workshop
on Hybrid Approaches to Machine Translation (HyTra), Gothenburg, Sweden.
Cutting, D., Kupiec, J., Pedersen, J., & Sibun, P. (1992). A practical part-of-speech tagger.
In Proceedings of the Third Conference on Applied Natural Language Processing, pp.
133140, Trento, Italy.
Eisele, A., Federmann, C., Saint-Amand, H., Jellinghaus, M., Herrmann, T., & Chen, Y.
(2008). Using Moses to integrate multiple rule-based machine translation engines
into a hybrid system. In Proceedings of the Third Workshop on Statistical Machine
Translation, pp. 179182, Columbus, Ohio, USA.
Enache, R., Espana-Bonet, C., Ranta, A., & Marquez, L. (2012). A hybrid system for
patent translation. In Proceedings of the 16th Annual Conference of the European
Association for Machine Translation, pp. 269276, Trento, Italy.
58

fiIntegrating Shallow-Transfer Rules into Statistical Machine Translation

Federmann, C., Eisele, A., Uszkoreit, H., Chen, Y., Hunsicker, S., & Xu, J. (2010). Further
experiments with shallow hybrid mt systems. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and Metrics, pp. 7781, Uppsala, Sweden.
Federmann, C., & Hunsicker, S. (2011). Stochastic parse tree selection for an existing rbmt
system. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pp.
351357, Edinburgh, Scotland.
Forcada, M. L., Ginest-Rosell, M., Nordfalk, J., ORegan, J., Ortiz-Rojas, S., Perez-Ortiz,
J. A., F. Sanchez-Martnez, G. R.-S., & Tyers, F. M. (2011). Apertium: a free/opensource platform for rule-based machine translation. Machine Translation, 25 (2), 127
144. Special Issue: Free/Open-Source Machine Translation.
Gao, Q., Lewis, W., Quirk, C., & Hwang, M.-Y. (2011). Incremental Training and Intentional Over-fitting of Word Alignment. In Proceedings of the XIII Machine Translation
Summit, pp. 106113, Xiamen, China.
Goodman, J., & Chen, S. F. (1998). An empirical study of smoothing techniques for language
modeling. Tech. rep. TR-10-98, Harvard University.
Graham, Y., & van Genabith, J. (2010). Factor templates for factored machine translation models. In Proceedings of the 7th International Workshop on Spoken Language
Translation, pp. 275283, Paris, France.
Green, S., & DeNero, J. (2012). A class-based agreement model for generating accurately
inflected translations. In Proceedings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers - Volume 1, pp. 146155, Jeju Island,
Korea.
Hutchins, W. J., & Somers, H. L. (1992). An introduction to machine translation, Vol. 362.
Academic Press New York.
Kirchhoff, K., & Yang, M. (2005). Improved language modeling for statistical machine
translation. In Proceedings of the ACL Workshop on Building and Using Parallel
Texts, pp. 125128, Ann Arbor, Michigan, USA.
Koehn, P. (2004). Statistical significance tests for machine translation evaluation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing,
Vol. 4, pp. 388395, Barcelona, Spain.
Koehn, P. (2005). Europarl: A parallel corpus for statistical machine translation. In Proceedings of the X Machine Translation Summit, pp. 1216, Phuket, Thailand.
Koehn, P. (2010). Statistical Machine Translation. Cambridge University Press.
Koehn, P., & Hoang, H. (2007). Factored translation models. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pp. 868876, Prague.
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan, B.,
Shen, W., Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin, A., & Herbst, E.
(2007). Moses: Open source toolkit for statistical machine translation. In Proceedings
of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration
Sessions, pp. 177180, Prague, Czech Republic.
59

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

Koehn, P., & Schroeder, J. (2007). Experiments in domain adaptation for statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine
Translation, pp. 224227, Prague, Czech Republic.
Labaka, G., Espana-Bonet, C., Marquez, L., & Sarasola, K. (2014). A hybrid machine
translation architecture guided by syntax. Machine Translation, 28 (2), 91125.
Lavie, A. (2008). Stat-XFER: A General Search-Based Syntax-Driven Framework for Machine Translation. In Gelbukh, A. (Ed.), Computational Linguistics and Intelligent
Text Processing, Vol. 4919 of Lecture Notes in Computer Science, pp. 362375.
Och, F. J., & Ney, H. (2003). A systematic comparison of various statistical alignment
models. Computational Linguistics, 29 (1), 1951.
Och, F. J., & Ney, H. (2004). The alignment template approach to statistical machine
translation. Computational Linguistics, 30 (4), 417449.
Och, F. J. (2003). Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,
pp. 160167, Sapporo, Japan.
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). Bleu: a method for automatic
evaluation of machine translation. In Proceedings of 40th Annual Meeting of the
Association for Computational Linguistics, pp. 311318, Philadelphia, Pennsylvania,
USA.
Popovic, M., & Ney, H. (2006). Statistical machine translation with a small amount of
bilingual training data. In 5th LREC SALTMIL Workshop on Minority Languages,
p. 2529, Genoa, Italy.
Riezler, S., King, T. H., Kaplan, R. M., Crouch, R., Maxwell, III, J. T., & Johnson, M.
(2002). Parsing the wall street journal using a lexical-functional grammar and discriminative estimation techniques. In Proceedings of the 40th Annual Meeting on
Association for Computational Linguistics, ACL 02, pp. 271278, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Riezler, S., & Maxwell III, J. T. (2006). Grammatical machine translation. In Proceedings
of the Human Language Technology Conference of the NAACL, Main Conference, pp.
248255, New York City, New York, USA.
Roche, E., & Schabes, Y. (1997). Introduction. In Roche, E., & Schabes, Y. (Eds.), Finitestate language processing, pp. 165. MIT, Cambridge, Massachusetts, USA.
Rosa, R., Marecek, D., & Dusek, O. (2012). Depfix: A system for automatic correction
of czech mt outputs. In Proceedings of the Seventh Workshop on Statistical Machine
Translation, pp. 362368, Montreal, Canada.
Rosti, A.-V., Matsoukas, S., & Schwartz, R. (2007). Improved word-level system combination for machine translation. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pp. 312319, Prague, Czech Republic.
Sanchez-Cartagena, V. M., Perez-Ortiz, J. A., & Sanchez-Martnez, F. (2015). A generalised
alignment template formalism and its application to the inference of shallow-transfer
60

fiIntegrating Shallow-Transfer Rules into Statistical Machine Translation

machine translation rules from scarce bilingual corpora. Computer Speech & Language,
32 (1), 4690.
Sanchez-Cartagena, V. M., Sanchez-Martnez, F., & Perez-Ortiz, J. A. (2011a). Integrating
shallow-transfer rules into phrase-based statistical machine translation. In Proceedings
of the XIII Machine Translation Summit, pp. 562569, Xiamen, China.
Sanchez-Cartagena, V. M., Sanchez-Martnez, F., & Perez-Ortiz, J. A. (2011b). The Universitat dAlacant hybrid machine translation system for WMT 2011. In Proceedings
of the Sixth Workshop on Statistical Machine Translation, pp. 457463, Edinburgh,
Scotland.
Sanchez-Cartagena, V. M., Sanchez-Martnez, F., & Perez-Ortiz, J. A. (2012). An opensource toolkit for integrating shallow-transfer rules into phrase-based satistical machine translation. In Proceedings of the Third International Workshop on Free/OpenSource Rule-Based Machine Translation, pp. 4154, Gothenburg, Sweden.
Sanchez-Martnez, F., & Forcada, M. L. (2009). Inferring shallow-transfer machine translation rules from small parallel corpora. Journal of Artificial Intelligence Research,
34 (1), 605635.
Schwenk, H., Abdul-Rauf, S., Barrault, L., & Senellart, J. (2009). SMT and SPE machine
translation systems for WMT09. In Proceedings of the Fourth Workshop on Statistical
Machine Translation, pp. 130134, Athens, Greece.
Sennrich, R. (2012). Perplexity minimization for translation model domain adaptation
in statistical machine translation. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pp. 539549, Avignon,
France.
Snover, M., Dorr, B., Schwartz, R., Micciulla, L., & Makhoul, J. (2006). A study of translation edit rate with targeted human annotation. In Proceedings of the 7th biennial
conference of the Association for Machine Translation in the Americas, pp. 223231,
Cambridge, Massachusetts, USA.
Stolcke, A. (2002). SRILM  an extensible language modeling toolkit. In Proceedings of the
7th International Conference on Spoken Language Processing, pp. 901904, Denver,
Colorado, USA.
Thurmair, G. (2009). Comparing different architectures of hybrid Machine Translation
systems. In Proceedings of the XII Machine Translation Summit, Ottawa, Canada.
Tiedemann, J. (2012). Parallel Data, Tools and Interfaces in OPUS. In Proceedings of
the Eight International Conference on Language Resources and Evaluation, pp. 2214
2218, Istanbul, Turkey.
Tyers, F. M. (2009). Rule-based augmentation of training data in BretonFrench statistical
machine translation. In Proceedings of the 13th Annual Conference of the European
Association of Machine Translation, pp. 213217, Barcelona, Spain.
Zbib, R., Kayser, M., Matsoukas, S., Makhoul, J., Nader, H., Soliman, H., & Safadi, R.
(2012). Methods for integrating rule-based and statistical systems for Arabic to English machine translation. Machine Translation, 26 (1-2), 6783.

61

fi