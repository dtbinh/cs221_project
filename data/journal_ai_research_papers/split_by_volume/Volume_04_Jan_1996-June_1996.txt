Journal of Artificial Intelligence Research 4 (1996) 365396

Submitted 4/95; published 5/96

Adaptive Problem-Solving for Large-Scale
Scheduling Problems: A Case Study
GRATCH@ISI.EDU

Jonathan Gratch
University of Southern California, Information Sciences Institute
4676 Admiralty Way, Marina del Rey, CA 90292, USA

STEVE.CHIEN@JPL.NASA.GOV

Steve Chien
Jet Propulsion Laboratory, California Institute of Technology
4800 Oak Grove Drive, M/S 5253660, Pasadena, CA, 911098099

Abstract
Although most scheduling problems are NP-hard, domain specific techniques perform well in
practice but are quite expensive to construct. In adaptive problem-solving, domain specific
knowledge is acquired automatically for a general problem solver with a flexible control architecture.
In this approach, a learning system explores a space of possible heuristic methods for one well-suited
to the eccentricities of the given domain and problem distribution. In this article, we discuss an
application of the approach to scheduling satellite communications. Using problem distributions
based on actual mission requirements, our approach identifies strategies that not only decrease the
amount of CPU time required to produce schedules, but also increase the percentage of problems that
are solvable within computational resource limitations.

1.

Introduction

With the maturation of automated problem-solving research has come grudging abandonment of the
search for the domain-independent problem solver. General problem-solving tasks like planning
and scheduling are provably intractable. Although heuristic methods are effective in many practical
situations, an ever growing body of work demonstrates the narrowness of specific heuristic strategies
(e.g., Baker, 1994, Frost & Dechter, 1994, Kambhampati, Knoblock & Yang, 1995, Stone, Veloso
& Blythe, 1994, Yang & Murray, 1994). Studies repeatedly show that a strategy that excels on one
task can perform abysmally on others. These negative results do not entirely discredit
domain-independent approaches, but suggest that considerable effort and expertise is required to find
an acceptable combination of heuristic methods, a conjecture that is generally by published accounts
of real-world implementations (e.g., Wilkins, 1988). The specificity of heuristic methods is
especially troubling when we consider that problem-solving tasks frequently change over time.
Thus, a heuristic problem solver may require expensive tune-ups as the character of the application
changes.
Adaptive problem solving is a general method for reducing the cost of developing and maintaining effective heuristic problem solvers. Rather than forcing a developer to choose a specific heuristic
strategy, an adaptive problem solver adjusts itself to the idiosyncrasies of an application. This can

 1996 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiGRATCH & CHIEN

be seen as a natural extension of the principle of least commitment (Sacerdoti, 1977). When solving
a problem, one should not commit to a particular solution path until one has information to distinguish
that path from the alternatives. Likewise, when faced with an entire distribution of problems, it
makes sense to avoid committing to a particular heuristic strategy until one can make an informed
decision on which strategy performs better on the distribution. An adaptive problem solver embodies
a space of heuristic methods, and only settles on a particular combination of these methods after a
period of adaptation, during which the system automatically acquires information about the particular distribution of problems associated with the intended application.
In previous articles, Gratch and DeJong have presented a formal characterization of adaptive
problem solving and developed a general method for transforming a standard problem solver into an
adaptive one (Gratch & DeJong, 1992, Gratch & DeJong, 1996). The primary purpose of this article
is twofold: to illustrate the efficacy of learning approaches for solving real-world problem solving
tasks, and to build empirical support for the the specific learning approach we advocate. After reviewing the basic method, we describe its application to the development of a large-scale scheduling
system for the National Aeronautics and Space Administration (NASA). We applied the adaptive
problem solving approach to a scheduling system developed by a separate research group, and without knowledge of our adaptive techniques. The scheduler included an expert-crafted scheduling
strategy to achieve efficient scheduling performance. By automatically adapting this scheduling system to the distribution of scheduling problems, the adaptive approach resulted in a significant improvement in scheduling performance over an expert strategy: the best adaptation found by machine
learning exhibited a seventy percent improvement in scheduling performance (the average learned
strategy resulted in a fifty percent improvement).
2.

Adaptive Problem Solving

An adaptive problem solver defers the selection of a heuristic strategy until some information can
be gathered about their performance over the specific distribution of tasks. The need for such an
approach is predicated on the claim that it is difficult to identify an effective heuristic strategy a
priori. While this claim is by no means proven, there is considerable evidence that, at least for the
class of heuristics that have been proposed till now, no one collection of heuristic methods will
suffice. For example, Kambhampati, Knoblock, and Yang (1995) illustrate how planning heuristics
embody design tradeoffs  heuristics that reduce the size of search space typically increase the cost
at each node, and vice versa  and that the desired tradeoff varies with different domains. Similar
observations have been made in the context of constraint satisfaction problems (Baker, 1994, Frost
& Dechter, 1994). This inherent difficulty in recognizing the worth (or lack of worth) of control
knowledge has been termed the utility problem (Minton, 1988) and has been studied extensively in
the machine learning community (Gratch & DeJong, 1992, Greiner & Jurisca, 1992, Holder, 1992,
Subramanian & Hunter, 1992). In our case the utility problem is determining the worth of a heuristic
strategy for specific problem distribution.
2.1 Formulation of Adaptive problem solving
Before discussing approaches to adaptive problem solving, we formally state the common definition
of the task (as proposed by Gratch & DeJong, 1992, Greiner & Jurisca, 1992, Laird, 1992,
Subramanian & Hunter, 1992). Adaptive problem solving requires a flexible problem solver,

366

fiADAPTIVE PROBLEM SOLVING

meaning the problem solver possesses control decisions that may be resolved in alternative ways.
Given a flexible problem solver, PS, with several control points, CP1 ...CPn (where each control point
CPi corresponds to a particular control decision), and a set of alternative heuristic methods for each
control point, {Mi,1 ...Mi,k ,},1 a control strategy defines a specific method for every control point (e.g.,
STRAT = <M1,3 ,M2,6 ,M3,1 ,...>). A control strategy determines the overall behavior of the problem
solver. Let PSSTRAT be the problem solver operating under a particular control strategy.
The quality of a problem solving strategy is defined in terms of the decision-theoretic notion of
expected utility. Let U(PSSTRAT, d), be a real valued utility function that is a measure of the goodness
of the behavior of the problem solver on a specific problem d. More generally, expected utility can
be defined formally over a distribution of problems D:
E D[U(PS STRAT)] 

 U(PS

STRAT

, d)  probability(d)

dD

The goal of adaptive problem solving can be expressed as: given a problem distribution D, find some
control strategy in the space of possible strategies that maximizes the expected utility of the problem
solver. For example, in the PRODIGY planning system (Minton, 1988), control points include: how
to select an operator to use to achieve the goal; how to select variable bindings to instantiate the
operator; etc. A method for the operator choice control point might be a set of control rules to
determine which operators to use to achieve various goals. A strategy for PRODIGY would be a set
of control rules and default methods for every control point (e.g., one for operator choice, one for
binding choice, etc.). Utility might be defined as a function of the time to construct a plan for a given
planning problem.
2.2 Approaches to Adaptive Problem Solving
Three potentially complementary approaches to adaptive problem solving have been discussed in the
literature. The first, what we call a syntactic approach, is to preprocess a problem-solving domain
into a more efficient form, based solely on the domains syntactic structure. For example, Etzionis
STATIC system analyzes a portion of a planing domains deductive closure to conjecture a set of search
control heuristics (Etzioni, 1990). Dechter and Pearl describe a class of constraint satisfaction
techniques that preprocess a general class of problems into a more efficient form (Dechter & Pearl,
1987). More recent work has focused on recognizing those structural properties that influence the
effectiveness of different heuristic methods (Frost & Dechter, 1994, Kambhampati, Knoblock &
Yang. 1995, Stone, Veloso & Blythe, 1994). The goal of this research is to provide a problem solver
with what is essentially a big lookup table, specifying which heuristic strategy to use based on some
easily recognizable syntactic features of a domain. While this later approach seems promising, work
in this area is still preliminary and has focused primarily on artificial applications. The disadvantage
of purely syntactic techniques is that that they ignore a potentially important source of information,
the distribution of problems. Furthermore, current syntactic approaches to this problem are specific
to a particular, often unarticulated, utility function (usually problem-solving cost). For example,
allowing the utility function to be a user specified parameter would require a significant and
problematic extension of these methods.
The second approach, which we call a generative approach, is to generate custom-made heuristics in response to careful, automatic, analysis of past problem-solving attempts. Generative ap1. Note that a method may consist of smaller elements so that a method may be a set of control rules or
a combination of heuristics.

367

fiGRATCH & CHIEN

proaches consider not only the structure of the domain, but also structures that arise from the problem
solver interacting with specific problems from the domain. This approach is exemplified by SOAR
(Laird, Rosenbloom & Newell, 1986) and PRODIGY/EBL (Minton, 1988). These techniques analyze
past problem-solving traces and conjectures heuristic control rules in response to specific problemsolving inefficiencies. Such approaches can effectively exploit the idiosyncratic structure of a domain through this careful analysis. The limitation of such approaches is that they have typically focused on generating heuristics in response to particular problems and have not well addressed the
issue of adapting to a distribution of problems2. Furthermore, as with the syntactic approaches, thus
far they have been directed towards a specific utility function.
The final approach we call the statistical approach. These techniques explicitly reason about
performance of different heuristic strategies across the distribution of problems. These are generally
statistical generate-and-test approaches that estimated the average performance of different heuristics from a random set of training examples, and explore an explicit space of heuristics with greedy
search techniques. Examples of such systems are COMPOSER (Gratch & DeJong, 1992), PALO (Greiner & Jurisca, 1992), and the statistical component of MULTI-TAC (Minton, 1993). Similar approaches
have also been investigated in the operations research community (Yakowitz & Lugosi, 1990). These
techniques are easy to use, apply to a variety of domains and utility functions, and can provide strong
statistical guarantees about their performance. They are limited, however, as they are computationally expensive, require many training examples to identify a strategy, and face problems with local
maxima. Furthermore, they typically leave it to the user to conjecture the space of heuristic methods
(see Minton, 1993 for a notable exception).
In this article, we adopt the statistical approach to adaptive problem solving due to its generality
and ease of use. In particular we use the COMPOSER technique for adaptive problem solving (Gratch
& DeJong, 1992, Gratch & DeJong, 1996), which is reviewed in the next section. Our implementation incorporates some novel features to address the computational expense of the method. Ideally,
however, an adaptive problem solver would incorporate some form of each of these methods. To this
end we are investigating how to incorporate other methods of adaptation in our current research.
3.

COMPOSER

COMPOSER embodies a statistical approach to adaptive problem solving. To turn a problem solver into
an adaptive problem solver, the developer is required to specify a utility function, a representative
sample of training problems, and a space of possible heuristic strategies. COMPOSER then adapts the
problem solver by exploring the space of heuristics via statistical hillclimbing search. The search
space is defined in terms of a transformation generator which takes a strategy and generates a set of
transformations to it. For example, one simple transformation generator just returns all single method
modifications to a given strategy. Thus a transformation generator defines both a space of possible
heuristic strategies and the non-deterministic order in which this space may be searched. COMPOSERs
overall approach is one of generate and test hillclimbing. Given an initial problem solver, the
transformation generator returns a set of possible transformations to its control strategy. These are
statistically evaluated over the expected distribution of problems. A transformation is adopted if it
2. While generative approaches can be trained on a problem distribution, learning typically occurs only
within the context of a single problem. These systems will often learn knowledge which is helpful in a
particular problem but decreases utility overall, necessitating the use of utility analysis techniques.

368

fiADAPTIVE PROBLEM SOLVING

increases the expected performance of solving problems over that distribution. The generator then
constructs a set of transformations to this new strategy and so on, climbing the gradient of expected
utility values.
Formally, COMPOSER takes an initial problem solver, PS0 , and identifies a sequence of problem
solvers, PS0 , PS1 , ... where each subsequent PS has higher expected utility with probability 1
(where  > 0 is some userspecified constant). The transformation generator, TG, is a function that
takes a problem solver and returns a set of candidate changes. Apply(t, PS) is a function that takes
a transformation, t  TG(PS) and a problem solver and returns a new problem solver that is the result
of transforming PS with t. Let Uj (PS) denote the utility of PS on problem j. The change in utility
that a transformation provides for the jth problem, called the incremental utility of a transformation,
is denoted by Uj (t|PS). This is the difference in utility between solving the problem with and without the transformation. COMPOSER finds a problem solver with high expected utility by identifying
transformations with positive expected incremental utility. The expected incremental utility is estimated by averaging a sample of randomly drawn incremental utility values. Given a sample of n values, the average of that sample is denoted by Un (t|PS). The likely difference between the average
and the true expected incremental utility depends on the variance of the distribution, estimated from
a sample by the sample variance S 2n(t|PS), and the size of the sample, n. COMPOSER provides a statistical technique for determining when sufficient examples have been gathered to decide, with error ,
that the expected incremental utility of a transformation is positive or negative. Because COMPOSER
presumes that the relevant distributions are normally distributed, COMPOSER requires at that each estimate of incremental utility be based on a minimum number of samples n0 to be determined for each
application. The algorithm is summarized in Figure 1.
COMPOSERs technique is applicable in cases where the following conditions apply:
1. The control strategy space can be structured to facilitate hillclimbing search. In general, the space
of such strategies is so large as to make exhaustive search intractable. COMPOSER requires a
transformation generator that structures this space into a sequence of search steps, with relatively few
transformations at each step. In Section 5.1 we discuss some techniques for incorporating domain
specific information into the structuring of the control strategy space.
2. There is a large supply of representative training problems so that an adequate sampling of
problems can be used to estimate expected utility for various control strategies.
3. Problems can be solved with a sufficiently low cost in resources so that estimating expected utility
is feasible.
4. There is sufficient regularity in the domain such that the cost of learning a good strategy can be
amortized over the gains in solving many problems.
4.

The Deep Space Network

The Deep Space Network (DSN) is a multi-national collection of ground-based radio antennas
responsible for maintaining communications with research satellites and deep space probes. DSN
Operations is responsible for scheduling communications for a large and growing number of
spacecraft. This already complex scheduling problem is becoming more challenging each year as
budgetary pressures limit the construction of new antennas. As a result, DSN Operations has turned

369

fiGRATCH & CHIEN

Given: PSold , TG(), , examples, n0
[1] PS := PSold ; T := TG(PS);
[3]

Repeat

n := 0; i:=0;  := Bound(, |T|);

{Find next transformation}

[2] While T   and i < |examples| do {Hillclimb as long as there is data and possible transformations}
[4]

n := n+1; i := i+1; steptaken := FALSE;

[5]

: Get Ui (|PS) {Observe incremental utility values for ith problem}

[6]

significant :+


S 2(|PS)
n 
t
   : n w n0 and  n

2
[Q()] 2
U n(|PS) 


{Collect all transformations that have reached statistical significance.}

[7]

T :+ T   significant : U n(|PS) t 0 

[8]

If   significant : U n(|PS) u 0 Then {Adopt  that most increases expected utility}

{Discard trans. that decrease expeced utility}

[9]

PS + Apply(x  significant : y  significant  U n(x|PS) u U n(y|PS) , PS)

[10]

T := TG(PS); n := 0;

[11]

 := Bound(, ||); steptaken :=TRUE;

Until steptaken or T= or i=|examples|;

Return: PS

R

Bound(, |T|) :+  ,
|T|

Q() :+ x where

 1 2e

0.5y 2

dy + 
2

x

Figure 1: The COMPOSER algorithm

increasingly towards intelligent scheduling techniques as a way of increasing the efficiency of
network utilization. As part of this ongoing effort, the Jet Propulsion Laboratory (JPL) has been
given the responsibility of automating the scheduling of the 26-meter sub-net; a collection of
26-meter antennas at Goldstone, CA, Canberra, Australia and Madrid, Spain.
In this section we discuss the application of adaptive problem-solving techniques to the development of a prototype system for automated scheduling of the 26-meter sub-net. We first discuss the
development of the basic scheduling system and then discuss how adaptive problem solving enhanced the schedulers effectiveness.
4.1 The Scheduling Problem
Scheduling the DSN 26-meter subnet can be viewed as a large constraint satisfaction problem. Each
satellite has a set of constraints, called project requirements, that define its communication needs.
A typical project specifies three generic requirements: the minimum and maximum number of
communication events required in a fixed period of time; the minimum and maximum duration for

370

fiADAPTIVE PROBLEM SOLVING

these communication events; and the minimum and maximum allowable gap between communication events. For example, Nimbus-7, a meteorological satellite, must have at least four 15-minute
communication slots per day, and these slots cannot be greater than five hours apart. Project
requirements are determined by the project managers and tend to be invariant across the lifetime of
the spacecraft.
In addition to project requirements, there are constraints associated with the various antennas.
First, antennas are a limited resource  two satellites cannot communicate with a given antenna at
the same time. Second, a satellite can only communicate with a given antenna at certain times, depending on when its orbit brings it within view of the antenna. Finally, antennas undergo routine
maintenance and cannot communicate with any satellite during these times.
Scheduling is done on a weekly basis. A weekly scheduling problem is defined by three elements: (1) the set of satellites to be scheduled, (2) the constraints associated with each satellite, and
(3) a set of time periods specifying all temporal intervals when a satellite can legally communicate
with an antenna for that week. Each time period is a tuple specifying a satellite, a communication
time interval, and an antenna, where (1) the time interval must satisfy the communication duration
constraints for the satellite, (2) the satellite must be in view of the antenna during this interval. Antenna maintenance is treated as a project with time periods and constraints. Two time periods conflict
if they use the same antenna and overlap in temporal extent. A valid schedule specifies a non-conflicting subset of all possible time periods where each projects requirements are satisfied.
The automated scheduler must generate schedules quickly as scheduling problems are frequently
over-constrained (i.e., the project constraints combined with the allowable time periods produces a
set of constraints which is unsatisfiable). When this occurs, DSN Operations must go through a complex cycle of negotiating with project managers to reduce their requirements. A goal of automated
scheduling is to provide a system with relatively quick response time so that a human user may interact with the scheduler and perform what if reasoning to assist in this negotiation process. Ultimately, the goal is to automate this negotiation process as well, which will place even greater demands
on scheduler response time (Chien & Gratch, 1994). For these reasons, the focus of development
is upon heuristic techniques that do not necessarily uncover the optimal schedule, but rather produce
adequate schedules quickly.
4.2 The LR-26 Scheduler
LR-26 is a heuristic scheduling approach to DSN scheduling being developed at the Jet Propulsion
Laboratory (Bell & Gratch, 1993).3 LR-26 is based on a 01 integer linear programming formulation
of the scheduling problem (Taha, 1982). Scheduling is cast as the problem of finding an assignment
to integer variables that maximizes the value of some objective function subject to a set of linear
constraints. In particular, time periods are treated as 0-1 integer variables: 0 (or OUT) if the time
period is excluded from the schedule; 1 (or IN) if it is included. The objective is to maximize the
number of time periods in the schedule and the solution must satisfy the project requirements and
antenna constraints (expressed as sets of linear inequalities). A typical scheduling problem under this
formulation has 700 variables and 1300 constraints.
In operations research, integer programs are solved by a variety of techniques including branchand-bound search, the gomory method (Kwak & Schniederjans, 1987), and Lagrangian relaxation
3.

LR-26 stands for the Lagrangian Relaxation approach to scheduling the 26-meter sub-net.

371

fiGRATCH & CHIEN

(Fisher, 1981). In artificial intelligence such problems are generally solved by constraint propagation
search techniques (e.g., Dechter, 1992, Mackworth, 1992). To address the complexity of the scheduling problem LR-26 uses a hybrid approach that combines Lagrangian relaxation with constraint propagation search. Lagrangian relaxation is a divide-and-conquer method which, given a decomposition
of the scheduling problem into a set of easier sub-problems, coerces the sub-problems to be solved
in such a way that they frequently result in a global solution. One specifies a problem decomposition
by identifying a subset of problem constraints that, if removed, result in one or more independent and
computationally easy sub-problems.4 These problematic constraints are relaxed, meaning they no
longer act as constraints but instead are added to the objective function in such a way that (1) there
is incentive to satisfying these relaxed constraints when solving the sub-problems and, (2) the best
solution to the relaxed problem, if it satisfies all relaxed constraints, is guaranteed to be the best solution to the original problem. Furthermore, this relaxed objective function is parameterized by a set
of weights (one for each relaxed constraint). By systematically changing these weights (thereby
modulating the incentives for satisfying relaxed constraints) a global solution can often be found.
Even if this weight search does not produce a global solution, it can make the solution to the sub-problems sufficiently close to a global solution that a global solution can be discovered with substantially
reduced constraint propagation search.
In the DSN domain, the scheduling problem is decomposed by scheduling each antenna independently. Specifically, the constraints associated with the complete problem can be divided into two
groups: those that refer to a single antenna, and those that mention multiple antennas. The later are
relaxed and the resulting single-antenna sub-problems can be solved in time linear in the number of
time periods associated with that antenna (see below). LR-26 solves the complete problem by first
trying to coerce a global solution by performing a search in the space of weights and then, if that fails
to produce a solution, resorting to constraint propagation search in the space of possible schedules.
4.2.1

SCHEDULES

We now describe the formalization of the problem. Let P be a set of projects, A a set of antennas,
M = {0,..,10080}, and V be an enumeration, V={0, 1, *}, denoting whether a time period is excluded
from the schedule (0), included (1), or uncommitted. Note that P, A, and M, are specified in advance
and V is to be determined by the scheduler and is initially always uncommitted. Let S  PAMMV
denote the set of possible time periods for a week, where a given time period specifies a project,
antenna and the start and end of the communication event, respectively. For a given s  S, we define
project(s), antenna(s), start(s), end(s), and value(s) to denote the corresponding elements of s. We
also define length(s) = end(s)  start(s) to simplify some subsequent notation.
A ground schedule is an assignment of 0 (excluded) or 1 (included) to each time period in S. This
can be seen as the application to S of some function G that maps each element of S to 0 or 1. We denote
this by S G. A partial schedule refers to a schedule with only a subset of its time periods committed,
which we denote via some mapping function M that maps elements of S to 0, 1, or *. A partial schedule corresponds to a set of possible ground schedules (i.e., those that result from forcing each uncommitted time period either in or out of the schedule). We denote this by S M. We define a particular
partial schedule S 0 to denote the completely uncommitted partial schedule (with all time periods assigned a value of *).
4. A problem consists of independent sub-problems it the global objective function can be maximized
by finding some maximal solution for each sub-problem in isolation.

372

fiADAPTIVE PROBLEM SOLVING

4.2.2

CONSTRAINTS

The scheduler must identify some ground schedule that satisfies a set of project and antenna
constraints, which we now formalize.
Project Requirements. Each project pn  P has associated with it a set of constraints called project
requirements. All constraints are processed and translated into simple linear inequalities over
elements of S. The complete set of project requirements, denoted PR, is the union of the requirements
from each individual projects. Each requirement can be expressed as integer linear inequality:
pr j  PR 5

a

i,j

@ value(s i) w b j or

s iS

a

i,j

@ value(s i) w b j

s iS

where ai represents a weighting factor indicating the degree to which the ith time period (if included)
contributes to satisfying a particular requirement. For example, the requirement that a project, p,
must have at least 100 minutes of communication time in a week is expressed:

[I(project(s) + p) @ length(s)] @ value(s) w 100.
sS

Where I(project(s)) equals one if s belongs to that project; otherwise zero. Note that time periods with
zero weight play no role and are not explicitly mentioned in the actual constraint representation.
Constraints on the length of individual time periods are represented similarly:
length(s) w 15

For efficiency, however, time periods which do not satisfy these unary inequalities are simply
eliminated from S in a preprocessing step.5
Antenna Constraints. Each of the three antennas has the constraint that no two projects can use the
antenna at the same time. This can be translated into a set of linear inequalities ACa,for each antenna
a as follows:
ACa = {si + sj  1 | si  sj  antenna(si )=antenna(sj )=a 
[start(si )..end(si )][start(sj )..end(sj )]  }
4.2.3

PROBLEM FORMULATION

The scheduling objective used by LR-26 is to find some ground schedule, denoted by S*, that
maximizes the number of time periods in the schedule subject to the project and antenna constraints:6
Problem:

DSN

Find:
Subject to:

S* + arg max

S Gs 0



ZG +

 value(s)
sS g



(1)

AC1 AC2 AC3 PR

5. Note that this is an inherent limitation in the formalization as the scheduler cannot entertain variable
length communication events  communication events must be discretized into a finite set of fixed length
intervals.

373

fiGRATCH & CHIEN

where Z G is the value of the objective function for some ground schedule and arg max denotes the
argument that leads to the maximum.
With Lagrangian relaxation, certain constraints are folded into the objective function in a standardized fashion. The intuition is to add some factor into the objective function that is negative iff
the relaxed constraint is unsatisfied. If a constraint is of the form ai si b, then u[ai si b] is added
to the objective function, where u is a non-negative weighting factor. Likewise, if the constraint is
of the form ai si b, then u[bai si ] is added. In LR-26, only project requirements are relaxed:
Problem:
Find:
S *(u) =



S S


DSN(u)
(2)

arg max Z G(u) + Z G )
G

0

Subject to:

 uj  aij @ value(si) * bj)  uj bj  aij @ value(si)

PR v

s S
i



G

AC1 AC2 AC3

PR w



s iS G



where Zs (u) is the relaxed objective function and u is a vector of non-negative weights of length |PR|
(one for each relaxed constraint). Note that this defines a space of relaxed solutions that depend on
the weight vector u. Let Z* denote the value of the optimal solution of the original problem
(Definition 1), and let Z*(u) denote value of the optimal solution to the relaxed problem (Definition
2) for a particular weight vector u. For any weight vector u, Z*(u) can be shown to be an upper bound
on the value of Z*. Thus, if a relaxed solution satisfies all of the original problem constraints, it is
guaranteed to be the optimal solution to the original problem. Lagrangian relaxation proceeds by
incrementally tightening this upper bound (by adjusting the weight vector) in the hope of identifying
a global solution. A global solution cannot always be identified in this manner, so a complete
scheduler must combine Lagrangian relaxation with some form of search.
4.2.4

SEARCH

If a solution cannot be found through weight adjustment, LR26 resorts to basic refinement search
(Kambhampati, Knoblock & Yang, 1995) (or split-and-prune search (Dechter & Pearl, 1987)) in the
space of partial schedules. In this search paradigm a partial schedule is recursively refined (split) into
a set of more specific partial schedules. In the context of the DSN scheduling problem, refinement
corresponds to forcing uncommitted time periods in or out of the schedule. A partial schedule would
be pruned if all of its ground schedules violate the constraints. The scheduler is applied recursively
to each refined partial schedule until some satisfactory ground schedule is found or all schedules are
pruned.
Each refinement is further refined by propagating the local consequence of new commitment.
After a variable is set to a particular value, each individual constraint which references that variable
is analyzed to determine which time period would be forced in or out of the schedule as a result of
the assignment. LR26 performs only partial constraint propagation, because complete propagation
is computationally expensive. Specifically, if constraint C1 references time periods s2, s4 and s5, and
6.

This might correspond to a desire to maintain maximum downlink flexibility.

374

fiADAPTIVE PROBLEM SOLVING

s2 is assigned a value, LR26 analyzes C1 to see if the new assignment determines the value of s4 and/
or s5. If, for example, s4 is constrained to take on a particular value, this triggers analysis of all
constraints which contain s4. This can be viewed as performing arcconsistency (Dechter, 1992).
During the constraint propagation it may be possible to show that the refinement contains no valid
ground schedule. In this case the partial schedule may be pruned from the search.
LR-26 augments this basic refinement search with Lagrangian relaxation to heuristically reduce
the combinatorics of the problem. The difficulty with refinement search is that it may have to perform
considerable (and poorly directed) search through a tree of refinements to identify a single satisficing
solution. If an optimal solution is sought, every leaf of this search tree must be examined.7 In contrast, by searching through the space of relaxed solutions to a partial schedule, one can sometimes
identify the best schedule without any refinement search. Even when this is not possible, Lagrangian
relaxation heuristically identifies a small set of problematic constraints, focusing the subsequent refinement search. Thus, by performing some search in the space of relaxed solutions at each step, the
augmented search method can significantly reduce both the depth and breadth of refinement search.
The augmented procedure works to the extent that it can efficiently solve relaxed solutions, ideally allowing the algorithm to explore several points in the space of weight vectors in each step of the
refinement search. LR-26 solves relaxed problems in linear time, O(|AC1 AC2 AC3 |). To see this,
note that each time period appears on exactly one antenna. Thus, Zs (u) can be broken into the sum
of three objective functions, each containing only the time periods associated with a particular antenna. Furthermore, the relaxed objective function can be reexpressed as the weighted sum of each of
the time periods on that antenna, and the unrelaxed constraints are simple pairwise exclusion
constraints between individual time periods. Combine this with the fact that time periods are partially
ordered by their start time and the problem simplifies to identifying some nonexclusive sequence
of time periods with the maximum cumulative weight. This is easily formulated and solved as a dynamic programming problem (see Bell & Gratch, 1993 for more details).
The augmented refinement search performed by LR-26 is summarized in Figure 2
4.2.5

PERFORMANCE TRADEOFFS

Perhaps the most difficult decisions in constructing the scheduler involve how to flesh out the details
of steps 1,2, 3, and 4. The constraint satisfaction and operations research literatures have proposed
many heuristic methods for these steps. Unfortunately, due to their heuristic nature, it is not clear
what combination of methods best suits this scheduling problem. The power of a heuristic method
depends on subtle factors that are difficult to assess in advance. Additionally, when considering
multiple methods, one has to consider interactions between methods.
In LR-26 a key interaction arises in the tradeoff between the amount of weight vector search vs.
refinement search performed by the scheduler (as determined by Step 2). At each step in the refinement search, the scheduler has the opportunity to search in the space of relaxed solutions. Spending
more effort in this weight search can reduce the amount of subsequent refinement search. But at some
point the savings in reduced refinement search may be overwhelmed by the cost of performing the
7. Partial schedules may also be pruned, as in branch-and-bound search, if they can be shown to contain
lower value solutions that other partial schedules. In practice LR-26 is run in a satisficing mode, meaning
that search terminates as soon as a ground schedule is found (not necessarily optimal) that satisfies all of
the problem constraints.

375

fiGRATCH & CHIEN

LR-26 Scheduler
(1)
(2)

(3)
(4)

Agenda := {S 0};
While Agenda  
Select some partial schedule S  Agenda; Agenda:=Agenda{S}
Weight search for some S*(u)  S;
IF S*(u) satisfies the project requirements (PR) Then
Return S*(u);
Else
Select constraint c  PR not satisfied by S*(u);
Refine S into {S i}, such that each SG  S i satisfies c
and {S i} = S;
Perform constraint propagation on each S i
Agenda := Agenda{S i};
Figure 2: The basic LR-26 refinement search method.

weight search. This is a classic example of the utility problem, and it is difficult to see how best to
resolve the tradeoff without intimate knowledge of the form and distribution of scheduling problems.
Another important issue for improving scheduling efficiency is the choice of heuristic methods for
controlling the direction of refinement search (as determined by steps 1, 3, and 4). Often these
methods are stated as general principles (e.g., first instantiate variables that maximally constrain the
rest of the search space, Dechter, 1992, p. 277) and there may be many ways to realize them in a
particular scheduler and domain. Furthermore, there are almost certainly interactions between
methods used at different control points that make it difficult to construct a good overall strategy.
These tradeoffs conspire to make manual development and evaluation of heuristics a tedious, uncertain, and time consuming task that requires significant knowledge about the domain and scheduler. In the case of LR-26, its initial control strategy was identified by hand, requiring a significant cycle
of trial-and-error evaluation by the developer over a small number of artificial problems. Even with
this effort, the resulting scheduler is still expensive to use, motivating us to try adaptive techniques.
5.

Adaptive Problem Solving for The Deep Space Network

We developed an adaptive version of the scheduler, Adaptive LR-26, in an attempt to improve its
performance.8 Rather than committing on a particular combination of heuristic strategies, Adaptive
LR-26 embodies an adaptive problem solving solution. The scheduler is provided a variety of heuristic
methods, and, after a period of adaptation, settles on a particular combination of heuristics that suits
the actual distribution of scheduling problems for this domain.
To perform adaptive problem solving, we must formally specify three things: a transformation
generator that defines the space of legal heuristic control strategies; a utility function that captures
our preferences over strategies in the control grammar; and a representative sample of training problems. We describe each of these elements as they relate to the DSN scheduling problem.
5.1 Transformation Generator
The description of LR-26 in Figure 2 highlights four points of non-determinism with respect to how
the scheduler performs its refinement search. To fully instantiate the scheduler we must specify: a
8.

This system has also been referred to by the name DSN-COMPOSER (Gratch, Chien & DeJong, 1993).

376

fiADAPTIVE PROBLEM SOLVING

way of ordering elements on the agenda, a weight search method, a method for selecting a constraint,
and a method for generating a spanning set of refinements that satisfy the constraint. The alternative
ways for resolving these four decisions are specified by a control grammar, which we now describe.
The grammar defines the space of legal search control strategies available to the adaptive problem
solver.
5.1.1

SELECT SOME PARTIAL SCHEDULE

The first decision in the refinement search is to choose some partial schedule from the agenda. This
selection policy defines the character of the search. Maintaining the agenda as a stack implements
depth-first search. Sorting the agenda by some value function implements a best-first search. In
Adaptive LR-26 we restrict the space of methods to variants of depth-first search. Each time a set of
refinements is created (Decision 4), they are added to the front of the agenda. Search always proceeds
by expanding the first partial schedule on the agenda. Heuristics act by ordering refinements before
they are added to the agenda. The grammar specifies several ordering heuristics, sometimes called
value ordering heuristics, or lookahead schemes in the constraint propagation literature (Dechter,
1992, Mackworth, 1992). As these methods are entertained during refinement construction, their
detailed description is delayed until that section.
Look-ahead schemes decide how to refine partial schedules. Look-back schemes handle the reverse decision of what to do whenever the scheduler encounters a dead end and must backtrack to
another partial schedule. Standard depth-first search performs chronological backtracking, backing
up to the most recent decision. The constraint satisfaction literature has explored several heuristic
alternatives to this simple strategy, including backjumping (Gaschnig, 1979), backmarking (Haralick
& Elliott, 1980), dynamic backtracking (Ginsberg, 1993), and dependency-directed backtracking
(Stallman & Sussman, 1977) (see Backer & Baker, 1994, and Frost and Dechter, 1994, for a recent
evaluation of these methods). We are currently investigating look-back schemes for the control
grammar but they will not be discussed in this article.
5.1.2

SEARCH FOR SOME RELAXED SOLUTION

The next dimension of flexibility is in weight-adjusting methods to search the space of possible
relaxed solutions for a given partial schedule. The general goal of the weight search is to find a
relaxed solution that is closest to the true solution in the sense that as many constraints are satisfied
as possible. This can be achieved by minimizing the value of Z*(u) with respect to u. The most
popular method of searching this space is called subgradient-optimization (Fisher, 1981). This is a
standard optimization method that repeatedly changes the current u in the direction that most
decreases Z*(u). Thus at step i, ui+1 = ui + ti di where ti is a step size and di is a directional vector
in the weight space. The method is expensive but it is guaranteed to converge to the minimum Z*(u)
under certain conditions (Held & Karp, 1970). A less expensive technique, but without the
convergence guarantee, is to consider only one weight at a time when finding an improving direction.
Thus ui+1 = ui + ti di where di is a directional vector with zeroes in all but one location. This method
is called dual-descent. In both of these methods, weights are adjusted until there is no change in the
relaxed solution: S*(ui ) = S*(ui+1 ).
While better relaxed solutions will create greater reduction in the amount of subsequent refinement search, it is unclear just where the tradeoff between these two search spaces lies. Perhaps it is
unnecessary to spend much time improving relaxed schedules. Thus a more radical, and extremely

377

fiGRATCH & CHIEN

efficient, approach is to settle for the first relaxed solution found. We call this the first-solution method. A more moderate approach is to perform careful weight search at the beginning of the refinement
search (where there is much to be gained by reducing the subsequent refinement search) and to perform the more restricted first-solution search when deeper in the refinement search tree. The truncated-dual-descent method performs dual-descent at the initial refinement search node and then uses
the first-solution method for the rest of the refinement search.
The control grammar includes four methods for performing weight space search (Figure 3).
2a: Subgradient-optimization
2b: Dual-descent

2c: Truncated-dual-descent
2d: First-solution

Figure 3: Weight Search Methods

5.1.3

SELECT SOME CONSTRAINT

If the scheduler cannot find a relaxed solution that solves the original problem, it must break the
current partial schedule into a set of refinements and explore them non-deterministically. In Adaptive
LR-26, the task of creating refinements is broken into two decisions: selecting an unsatisfied
constraint (Decision 3), and creating refinements that make progress towards satisfying the selected
constraint (Decision 4). Lagrangian relaxation simplifies the first decision by identifying a small
subset of constraints that appear problematic. However, this still leaves the problem of choosing one
constraint in this subset on which to base the subsequent refinement.
The common wisdom in the search community is to choose a constraint that maximally
constrains the rest of the search space, the idea being to minimize the size of the subsequent refinement search and to allow rapid pruning if the partial schedule is unsatisfiable. Therefore, our control
grammar incorporates several alternative heuristic methods for locally assessing this factor. Given
that the common wisdom is only a heuristic, we include a small number of methods that violate this
intuition. All of these methods are functions that look at the local constraint graph topology and return a value for each constraint. Constraints can then be ranked by their value and the highest value
constraint chosen. The control grammar implements both a primary and secondary sort for
constraints. Constraints that have the same primary value are ordered by their secondary value.
For the sake of simplicity we only discuss measures for constraints of the form as  b. (Analogous measures are defined for other forms.) We first define measures on time periods. Measures on
constraints are functions of the measures of the time periods that participate in the constraint.
Measures on Time Periods. An unforced time period is one that is neither in or out of the schedule
(value(s)=*). The conflictedness of an unforced time period s (with respect to a current partial
schedule) is the number of other unforced time periods that will be forced out if s is forced into the
schedule (because they participate in an antenna constraint with s). If a time period is already forced
out of the current partial schedule, it does not count toward ss conflictedness. Forcing a time period
with high conflictedness into the schedule will result in many constraint propagations, which reduces
the number of ground schedules in the refinement.
The gain of an unforced time period s (with respect to a current partial schedule) is the number
of unsatisfied project constraints that s participates in. Preferring time periods with high gain will
make progress towards satisfying many project constraints simultaneously.

378

fiADAPTIVE PROBLEM SOLVING

The loss of an unforced time period s (with respect to a current partial schedule) is a combination
of gain and conflictedness. Loss is the sum of the gain of each unforced time period that will be forced
out if s is forced into the schedule. Time period with high loss are best avoided as they prevent progress towards satisfying many project constraints.
To illustrate these measures, consider the simplified scheduling problem in Figure 4.

P1

P2
Project Requirements
P 1 : s 1 + s2 + s3  2

s1

s2

s3

s4

P 2 : s 2 + s3 + s4  2

Antenna Constraints
A1: s1 + s3  1
A2: s2 + s4  1

A1

A2

Figure 4: A simplified DSN scheduling problem based on four time periods. There
are two project constraints, and two antenna constraints. For example, P1 signifies that
at least two of the first three time periods must appear in the schedule, and A1 signifies
that either s1 or s3 may appear in the schedule, but not both. In the solution, only s2
and s3 appear in the schedule.
With respect to the initial partial schedule (with none of the time periods forced either in or out)
the conflictedness of s2 is one, because it appears in just one antenna constraint (A2). If subsequently,
s4 is forced out, then the conflictedness of s2 drops to zero, as conflictedness is only computed over
unforced time periods. The initial gain of s2 is two, as it appears in both project constraints. Its gain
drops to one if s3 and s4 are then forced into the schedule, as P2 becomes satisfied. The initial loss
of s2 is the sum of the gain of all time periods conflicting with it (s4). The gain of s4 is one (it appears
in P2) so that the loss of s2 is one.
Measures on Constraints. Constraint measures (with respect to a partial schedule) can be defined
as functions of the measures of the unforced time periods that participate in a constraint. The
functions max, min, and total have been defined. Thus, total-conflictedness is the sum of the
conflictedness of all unforced time periods mentioned in a constraint, while max-gain is the
maximum of the gains of the unforced time periods. Thus, for the constraints defined above, the
initial total-conflictedness of P1 is the conflictedness of s1, s2 and s3, 1 + 1 + 1 = 3. The initial
maxgain of constraint P1 is the maximum of the gains of s1, s2, and s3 or max{1,2,2} = 2.
We also define two other constraint measures. The unforced-periods of a constraint (with respect
to a partial schedule) is simply the number of unforced time periods that are mentioned in the
constraint. Preferring a constraint with a small number of unforced time periods restricts the number
of refinements that must be considered, as refinements consider combinations of time periods to force
into the schedule in order to satisfy the constraint. Thus, the initial unforced-periods of P1 is three
(s1, s2, and s3).

379

fiGRATCH & CHIEN

The satisfaction-distance of a constraint (with respect to a partial schedule) is a heuristic measure
the number of time periods that must be forced in order to satisfy the constraint. The measure is heuristic because it does not account for the dependencies between time periods imposed by antenna
constraints. The initial satisfaction-distance of P1 is two because two time periods must be forced in
before the constraint can be satisfied.
Given these constraint measures, constraints can be ordered by some measure of their worth. For
example we may prefer constraints with high total conflictedness, denoted as prefer-total-conflictedness. Not all possible combinations seem meaningful so the control grammar for Adaptive LR-26 implements nine constraint ordering heuristics (Figure 5).
3a:
3b:
3c:
3d:
3e:

Prefer-max-gain
Prefer-total-gain
Penalize-max-loss
Penalize-max-conflictedness
Prefer-total-conflictedness

3f: Penalize-total-conflictedness
3g: Prefer-min-conflictedness
3h: Penalize-unforced-periods
3i: Penalize-satisfaction-distance

Figure 5: Constraint Selection Methods

5.1.4

REFINE PARTIAL SCHEDULE

Given a selected constraint, the scheduler must create a set of refinements that make progress towards
satisfying it. If the constraint is of the form as  b then some time periods on the left-hand-side must
be forced into the schedule if the constraint is to be satisfied. Thus, refinements are constructed by
identifying a set of ways to force time periods in or out of the partial schedule s such that the
refinements form a spanning set: {S i} = S. These refinements are then ordered and added to the
agenda. Again, for simplicity we restrict discussion to constraints of form as  b.
The Basic Refinement Method. The basic method for refining a partial schedule is to take each
unforced time period mentioned in the constraint and create a refinement with the time period vj
forced into the schedule. Thus, for the constraints defined above, there would be three refinements
to constraint P1, one with s1 forced in: one with s2 forced in, and one with s3 forced in.
Each refinement is further refined by performing constraint propagation (arc consistency) to determine some local consequences of this new restriction. Thus, every time period that conflicts with
vj is forced out of the refined partial schedule, which in turn may force other time periods to be included, and so forth. By this process, some refinements may be recognized as inconsistent (contain
no ground solutions) and are pruned from the search space (for efficiency, constraint propagation is
only performed when partial schedules are removed from the agenda).
Once the set of refinements has been created, they are ordered by a value ordering heuristic before
being placed on the agenda. As with constraint ordering heuristics, there is a common wisdom for
creating value ordering heuristics: prefer refinements that maximized the number of future options
available for future assignments (Dechter & Pearl, 1987, Haralick & Elliott, 1980). The control
grammar implements several heuristic methods using measures on the time periods that created the
refinement. For example, one way to keep options available is to prefer forcing in a time period with
minimal conflictedness. As the common wisdom is only heuristic, we also incorporate a method that
violates it. The control grammar includes five value ordering heuristics that are derived from the

380

fiADAPTIVE PROBLEM SOLVING

measures on time periods (Figure 6), where the last method, arbitrary, just uses the ordering of the
time periods as they appear in the constraint.
1a: Prefer-gain
1b: Penalize-loss
1c: Penalize-conflictedness

1d: Prefer-conflictedness
1e: Arbitrary

Figure 6: Value Ordering Methods

The Systematic Refinement Method. The basic refinement method has one unfortunate property
that may limit its effectiveness. The search resulting from this refinement method is unsystematic
in the sense of McAllester and Rosenblitt (1991). This means that there is some redundancy in the
set of refinements: S iS j. Unsystematic search is inefficient in that the total size of the refinement
search space will be greater than if a systematic (non-redundant) refinement method is used. This
may or may not be a disadvantage in practice as scheduling complexity is driven by the size of the
search space actually explored (the effective search space) rather than its total size. Nevertheless,
there is good reason to suspect that a systematic method will lead to smaller effective search spaces.
A systematic refinement method chooses a time period that helps satisfy the selected constraint
and then forms a spanning set of two refinements: one with the time period forced in and one with
the time period forced out. These refinements are guaranteed to be non-overlapping. The systematic
method incorporated in the control grammar uses the value ordering heuristic to choose which unforced time period to use. The two refinements are ordered based on which makes immediate progress towards satisfying the constraint (e.g., s=1 is first for constraints of form as  b). The control
grammar includes both the basic and systematic refinement methods (Figure 7).
4a: Basic-Refinement

4b: Systematic-Refinement
Figure 7: Refinement Methods

For the problem specified in Figure 4, when systematically refining constraint P1, one would use
the value ordering method to select among time periods s1, s2, and s3. If s2 were selected, two refinements would be proposed, one with s2 forced in and one with s2 forced out.
The control grammar is summarized in Figure 8. The original expert control strategy developed
for LR-26 is a particular point in the control space defined by the grammar: the value ordering method
is arbitrary (1e); the weight search is by dual-descent (2b); the primary constraint ordering is penalize-unforced-periods (3h); there is no secondary constraint ordering, thus this is the same as the primary ordering; and the basic refinement method is used (4a).
5.1.5

META-CONTROL KNOWLEDGE

The constraint grammar defines a space of close to three thousand possible control strategies. The
quality of a strategy must be assessed with respect to a distribution of problems, therefore it is
prohibitively expensive to exhaustively explore the control space: taking a significant number of
examples (say fifty) on each of the strategies at a cost of 5 CPU minutes per problem would require
approximately 450 CPU days of effort.

381

fiGRATCH & CHIEN

CONTROL STRATEGY :=
VALUE ORDERING 
WEIGHT SEARCH METHOD 
PRIMARY CONSTRAINT ORDERING 
SECONDARY CONSTRAINT ORDERING 
REFINEMENT METHOD
VALUE ORDERING
WEIGHT SEARCH METHOD
PRIMARY CONSTRAINT ORDERING
SECONDARY CONSTRAINT ORDERING
REFINEMENT METHOD

:= {1a, 1b, 1c, 1d,1e}
:= {2a, 2b, 2c, 2d}
:= {3a, 3b, 3c, 3d, 3e, 3f, 3g, 3h, 3i}
:= {3a, 3b, 3c, 3d, 3e, 3f, 3g, 3h, 3i}
:= {4a, 4b}

Figure 8: Control grammar for Adaptive LR-26
COMPOSER requires a transformation generator to specify alternative strategies, which are explored via hillclimbing search. In this case, the obvious way to proceed is to consider all single method changes to a given control strategy. However the cost of searching the strategy space and quality
of the final solution depend to a large extent on how hillclimbing proceeds, and the obvious way need
not be the best. In Adaptive LR-26, we augment the control grammar with some domain-specific
knowledge to help organize the search. Such knowledge includes, for example, our prior expectation
that certain control decisions would interact, and the likely importance of the different control decisions. The intent of this meta-control knowledge is to reduce the branching factor in the control
strategy search and improve the expected utility of the locally optimal solution found. This approach
led to a layered search through the strategy space. Each control decision is assigned to a level. The
control grammar is search by evaluating all combinations of methods at a single level, adopting the
best combinations, and then moving onto the next level. The organization is shown below:
Level 0:
Level 1:
Level 2:
Level 3:

{Weight search method}
{Refinement method}
{Secondary constraint ordering, Value ordering}
{Primary constraint ordering}

The weight search and refinement control points are separate, as they seem relatively independent
from the other control points, in terms of their effect on the overall strategy. While there is clearly
some interaction between weight search, refinement construction, and the other control points, a
good selection of methods for pricing and alternative construction should perform well across all
ordering heuristics. The primary constraint ordering method is relegated to the last level because
some effort was made in optimizing this decision in the expert strategy for LR-26, and we believed that
it was unlikely the default strategy could be improved.
Given this transformation generator, Adaptive LR-26 performs hillclimbing across these levels.
It first entertains weight adjustment methods, then alternative construction methods, then combinations of secondary constraint sort and child sort methods, and finally primary constraint sort methods.
Each choice is made given the previously adopted methods.
This layered search can be viewed as the consequence of asserting certain types of relations between control points. Independence relations indicate cases in which the utility of methods for one
control point is roughly independent of the methods used at other control points. Dominance rela-

382

fiADAPTIVE PROBLEM SOLVING

tions indicate that the changes in utility from changing methods for one control point are much larger
than the changes in utility for another control point. Finally, inconsistency relations indicate when
a method M1 for control point X is inconsistent with method M2 for control point Y. This means that
any strategy using these methods for these control points need not be considered.
5.2 EXPECTED UTILITY
As previously mentioned, a chief design requirement for LR-26 is that the scheduler produce solutions
(or prove that none exist) efficiently. This behavioral preference can be expressed by a utility
function related to the computational effort required to solve a problem. As the effort to produce a
schedule increases, the utility of the scheduler on that problem should decrease. In this paper, we
characterize this preference by defining utility as the negative of the CPU time required by the
scheduler on a problem. Thus, Adaptive LR-26 tunes itself to strategies that minimize the average time
to generate a schedule (or prove that one does not exist). Other utility functions could be entertained.
In fact, more recent research has focused on measures of schedule quality (Chien & Gratch, 1994).
5.3 Problem Distribution
Adaptive LR-26 needs a representative sample of training examples for its adaptation phase.
Unfortunately, DSN Operations has only recently begun to maintain a database of scheduling
problems in a machine readable format. While this will ultimately allow the scheduler to tune itself
to the actual problem distribution, only a small body of actual problems was available at the time of
this evaluation. Therefore, we resorted to other means to create a reasonable problem distribution.
We constructed an augmented set of training problems by syntactic manipulation of the set of real
problems. Recall that each scheduling problem is composed of two components: a set of project requirements, and a set of time periods. Only the time periods change across scheduling problems, so
we can organize the real problems into a set of tuples, one for each project, containing the weekly
blocks of time periods associated with it (one entry for each week the project is scheduled). The set
of augmented scheduling problems is constructed by taking the cross product of these tuples. Thus,
a weekly scheduling problem is defined by combining one weeks worth of time periods from each
project (time periods for different projects may be drawn from different weeks), as well as the project
requirements for each. This simple procedure defines set of 6600 potential scheduling problems.
Two concerns led us to use only a subset of these augmented problems. First, a significant percentage of augmented problems appeared much harder to solve (or prove unsatisfiable) than any of
the real problems (on almost half of the constructed problems the scheduler did not terminate, even
with large resource bounds). That such hard problems exist is not unexpected as scheduling is NPhard, however, their frequency in the augmented sample seems disproportionately high. Second, the
existence of these hard problems raises a secondary issue of how best to terminate search. The standard approach is to impose some arbitrary resource bound and to declare a problem unsatisfiable if
no solution is found within this bound. Unfortunately this raises the issue of what sized bound is most
reasonable. We could have resolved this by adding the resource bound to the control grammar, however, at this point in the project we settled for a simpler approach. We address this and the previous
concern by excluding from the augmented problem distribution those problems that seem fundamentally intractable. What this means in practice is that we exclude problems that could not be
solved by any of a large set of heuristic methods within a five minute resource bound, the determina-

383

fiGRATCH & CHIEN

tion of which is discussed in Appendix A. This results in a reduced set of about three thousand scheduling problems.
The use of a resource bound can be problematic for evaluating the power of a learning technique.
As noted by Segre, Elkan, and Russell (1991), a learning system that greatly improves problem solving performance under a given resource bound may perform quite differently under a different resource bound. Some researchers suggest statistical analysis methods for assessing the significance
of this factor (e.g., see Etzioni and Etzioni, 1994). In this study, however, we do not address the issue
of how results might change given different resource bounds. We note that COMPOSERs statistical
properties suggest that problem solving performance should be no worse after learning, whatever the
resource bound, but the performance improvement many vary considerably. To give at least some
insight into the generality of adaptive problem solving, we include a secondary set of evaluations
based on all 6600 augmented problems (including fundamentally intractable ones).
6.

Empirical Evaluation

We conjecture that Adaptive LR26 will improve the performance of the basic scheduler. This can
be broken down into two separate claims. First, we claim that the modifications suggested above
contain useful transformations (it is possible to improve the scheduler). Second, we claim that
Adaptive LR26 should identify these transformations (and avoid harmful ones) with the requested
level of probability. The first claim is solely based on our intuitions; the second supported by the
statistical theory that underlies the COMPOSER approach. The usefulness of COMPOSER depends on
its ability to COMPOSER can go beyond simply improving performance and identifying strategies that
rank highly when judged with respect to the whole space of possible strategies. A third claim,
therefore, is that Adaptive LR-26 will find better strategies than if we simply picked the best of a large
number of randomly selected strategies. Besides testing these three claims, we are also interested
in three secondary questions: how quickly does the technique improve expected utility (e.g., how
many examples are required to make statistical inferences?); can Adaptive LR-26 improve the number
problems solved (or proved unsatisfiable) within the resource bound; and how sensitive is the
effectiveness of adaptive problem solving to changes in the distribution of problems.
6.1 Methodology
Our evaluation is influenced by the stochastic nature of adaptive problem solving. During adaptation,
Adaptive LR-26 is guided by a random selection of training examples according to the problem
distribution. As a result of this random factor, the system will exhibit different behavior on different
runs of the system. On some runs the system may learn high utility strategies; on other runs the
random examples may poorly represent the distribution and the system may adopt transformations
with negative utility. Thus, our evaluation is directed at assessing the expected performance of the
adaptive scheduler by averaging results over multiple experimental trials.
For these experiments, the scheduler is allowed to adapt to 300 scheduling problems drawn randomly from the problem distribution described above. The expected utility of all learned strategies
is assessed on an independent test set of 1000 test examples drawn randomly from the complete set
of three thousand. The adaptation rate is assessed by recording the strategy learned by Adaptive LR-26
after every 20 examples. Thus we can see the result of learning with only twenty examples, only forty
examples, etc. We measure the statistical error of the technique (the probability of adopting a trans-

384

fiLR-26

70
60
50

Avg. Solution Time
seconds per prob.

Summary of Results
80

40

Adaptive LR-26

30
20
10
0
0

30 60 90 120 150 180 210 240 270 300
Examples in Training Set

LR-26
avg. across trials
Adaptive
LR-26

Statistical
Error
Rate
Solution Rate
% of solvable probs.

Average Solution Time (CPU seconds)

ADAPTIVE PROBLEM SOLVING

80
40

best strategy

24

worst strategy

55

predicted

5%

observed

3%

LR-26
avg. across trials
Adaptive
LR-26

Dist. 1

79%
95%

best strategy

97%

worst strategy

86%

Figure 9. Learning curve showing performance as a function of the number of training examples
and table of experimental results.
formation with negative incremental utility) by performing eighty runs of the system on eighty distinct training sets drawn randomly from the problem distribution. We measure the distributional sensitivity of the technique by evaluating the adaptive scheduler on a second distribution of problems.
Recall that we purposely excluded inherently difficult scheduling problems from the augmented set
of problems. If added, these excluded problems should make adaptation more difficult as no strategy
is likely to provide a noticeable improvement within the five minute resource bound. The second
evaluation includes these difficult problems
A third evaluation assesses the relative quality of the strategies identified by Adaptive LR-26 when
compared with other strategies in the strategy space. This is inferred by comparing the expected utility of the learned strategies with several strategies drawn randomly from the space. This also provides an opportunity to assess the quality of the expert strategy, and thus give a sense of how challenging it is to improve it.
COMPOSER, the statistical component of the adaptive scheduler, has two parameters that govern
its behavior. The parameter  specifies the acceptable level of statistical error (this is the chance that
the technique will adopt a bad transformation or reject a good one). In Adaptive LR-26, this is set to
a standard value of 5%. COMPOSER bases each statistical inferences on a minimum of n0 examples.
In Adaptive LR-26, n0 is set to the empirically determined value of fifteen.
6.2 Overall Results  DSN DISTRIBUTION
Figure 9 summarizes the results of adaptive problem solving over the constructed DSN problem
distribution. The results support the two primary claims. First, the system learned search control
strategies that yielded a significant improvement in performance. Adaptive problem solving reduced
the average time to solve a problem (or prove it unsatisfiable) from 80 to 40 seconds (a 50%

385

fiGRATCH & CHIEN

improvement). Second, the observed statistical error fell well within the predicted bound. Of the 370
transformations adopted across the eighty trials, only 3% decreased expected utility.
Due to the stochastic nature of the adaptive scheduler, different strategies were learned on different trials. All learned strategies produced at least some improvement in performance. The best of
these strategies required only 24 seconds on average to solve a problem (an improvement of 70%).
The fastest adaptations occurred early in the adaptation phase and performance improvements decreased steadily throughout. It took an average of 62 examples to adopt each transformation. Adaptive LR-26 showed some improvement over the non-adaptive scheduler in terms of the number of
problems that could be solved (or proven unsatisfiable) within the resource bound. LR-26 was unable
to solve 21% of the scheduling problems within the resource bound. One adaptive strategy substantially reduced this number to 3%.
An analysis of the learned strategies is revealing. Most of the performance improvement (about
one half) can be traced to modifications in LR-26s weight search method. The rest of the improvements are divided equally among changes to the heuristics for value ordering, constraint selection,
and refinement. As expected, changes to the primary constraint ordering only degraded performance.
The top three strategies are illustrated in Figure 10.
1) Value ordering:
Weight search:
Primary constraint ordering:
Secondary constraint ordering:
Refinement method:

penalize-conflictedness (1c)
first-solution (2d)
penalize-unforced-periods (3h)
prefer-total-conflictedness (3e)
systematic-refinement (4b)

2) Value ordering:
Weight search:
Primary constraint ordering:
Secondary constraint ordering:
Refinement method:

prefer-gain (1a)
first-solution (2d)
penalize-unforced-periods (3h)
prefer-total-conflictedness (3e)
systematic-refinement (4b)

3) Value ordering:
Weight search:
Primary constraint ordering:
Secondary constraint ordering:
Refinement method:

penalize-conflictedness (1c)
first-solution (2d)
penalize-unforced-periods (3h)
penalize-satisfaction-distance (3i)
systematic-refinement (4b)

Figure 10: The three highest utility strategies learned by Adaptive LR-26.

For the weight search, all of the learned strategies used the first-solution method (2d). It seems
that, at least in this domain and problem distribution, the reduction in refinement search space that
results from better relaxed solutions is more than offset by the additional cost of the weight search.
The scheduler did, however, benefit from the reduction in size that results from a systematic refinement method.

386

fi140

LR-26

Adaptive LR-26

120
100

Adaptive
LR-26

LR-26
avg. across trials

156

best strategy

133

worst strategy

150

predicted

5%

observed

6%

LR-26
avg. across trials

51%

best strategy

57%

worst strategy

51%

Statistical
Error Rate

80
60
40
20
0
0 30 60 90 120150180210240270300

Dist. 1

Summary of Results
Avg. Time
seconds per prob.

160

Solution Rate
% solvable

Average Solution Time (CPU seconds)

ADAPTIVE PROBLEM SOLVING

Adaptive
LR-26

146

54%

Examples in Training Set

Figure 11. Learning curves and table of experimental results showing performance over
the augmented distribution (including intractable problems).
More interestingly, Adaptive LR-26 seems to have rediscovered the common wisdom in heuristic constraint-satisfaction search. When exploring new refinements, it is often suggested to chose
the least constrained value of the most constrained constraint. The best learned strategies follow this
advice while the worst strategies violate it. In the best strategy, the time period with lowest conflictedness is least constraining (in the sense that it will tend to produce the least constraint propagations) and thus produces the least commitments on the resulting partial schedule. By this same argument, the constraint with the highest total conflicted will tend to be the hardest to satisfy.
6.3 Overall Results  FULL AUGMENTED DISTRIBUTION
Figure 11 summarizes the results for the augmented distribution. As expected, this distribution
proved more challenging for adaptive problem solving. Nevertheless, modest performance
improvements were still possible, lending support to our claimed generality of the adaptive problem
solving approach. Learned strategies reduced the average solution time from 156 to 146 seconds (an
6% improvement). The best learned strategies required 133 seconds on average to solve a problem
(an improvement of 15%). The observed statistical accuracy did not significantly differ from the
theoretically predicted bound, although it was slightly higher than expected: of 397 transformations
were adopted across the trials, 6% produced a decrease in expected utility. The introduction of the
difficult problems resulted in higher variance in the distribution of incremental utility values and this
is reflected in a higher sample complexity: an average of 118 examples to adopt each transformation.
Some improvement was noted on the supposedly intractable problems. One strategy learned by
Adaptive LR-26 increased the number of problems that could be processed within the resource bound
from 51% to 57%.
One interesting result of this evaluation is that, unlike the previous evaluation, the best learned
strategies use truncated-dual-descent as their weight search method (the strategies were similar along
other control dimensions). This illustrates how even modest changes to the distribution of problems

387

fiGRATCH & CHIEN

can influence the design tradeoffs associated with a problem solver: in this case, changing the tradeoff
between weight and refinement search.
6.4 Quality of Learned strategies
The third claim is that, in practice, COMPOSER can identify strategies that rank highly when judged
with respect to the whole strategy space. A secondary question is how well does the expert strategy
perform. The improvements of Adaptive LR-26 are of little significance if the expert strategy
performs worse than most strategies in the space. Alternatively, if the expert strategy is extremely
good, its improvement is compelling.
As a way of assessing these claims we estimate the probability of selecting a high utility strategy
given that we choose it randomly from one of three strategy spaces: the space of all possible strategies
(expressible in the transformation grammar), the space of strategies produced by Adaptive LR-26, and
the trivial space containing only the expert strategy. This corresponds to the problem of estimating
a probability density function (p.d.f.) for each space: a p.d.f., f(x), associated with a random variable
gives the probability that an instance of the variable has value x. More specifically we want to estimate the density functions, fs (u), which is the probability of randomly selecting a strategy from space
s that has expected utility u.
We use a non-parametric density estimation technique called the kernel method to estimate fs (u)
(as in Smyth, 1993). To estimate the density function of the whole space, we randomly selected and
tested thirty strategies. All of the learned strategies are used to estimate the density of the learned
space. (In both cases, five percent of the data was withheld to estimate the bandwidth parameter used
by the kernel method.) The p.d.f. associated with the single expert strategy is estimated using a normal model fit to the 1000 test examples from the previous evaluation.
6.4.1

DSN DISTRIBUTION

Figure 12 illustrates the results for the DSN distribution. In this evaluation the learned strategies
significantly outperformed the randomly selected strategies. Thus, one would have to select and test
many strategies at random before finding one of comparable expected utility to one found by
Adaptive LR-26. The results also indicate that the expert strategy is already a good strategy (as
indicated by the relative positions of the peaks for the expert and random strategy distributions),
indicating that the improvement due to Adaptive LR-26 is significant and non-trivial.
The results provide additional insight into Adaptive LR-26s learning behavior. That the p.d.f for
the learned strategies contains several peaks, graphically illustrates that different local maxima exist
for this problem. Thus, there may be benefit in running the system multiple times and choosing the
best strategy. It also suggests that techniques designed to avoid local maxima would be beneficial.
6.4.2

FULL AUGMENTED DISTRIBUTION

Figure 13 illustrates the results for the full augmented distribution. The results are similar to the DSN
distribution: the learned strategies again outperformed the expert strategy which in turn again
outperformed the randomly selected strategies. The data shows that the expert strategy is
significantly better than randomly selected strategies. Together, these two evaluations support the
claim that Adaptive LR-26 is selecting high performance strategies. Even though the expert strategy
is quite good when compared with the complete strategy space, the adaptive algorithm is able to
improve the expected problem solving performance.

388

fiADAPTIVE PROBLEM SOLVING

Improved Performance

Probability

0.140
0.120

Expert Strategy

0.100
0.080

Learned Strategies

0.060
0.040

Random Strategies
0.020
0
0

20

40

60

80

100

120

140

160

180

Negative Expected Utility

Figure 12: The DSN Distribution. The graph shows the probability of obtaining a
strategy of a particular utility, given that it is chosen from (1) the set of all strategies,
(2) the set of learned strategies, or (3) the expert strategy.

Probability

Improved Performance
0.120
0.100

Expert Strategy
0.080
0.060

Learned Strategies
0.040

Random Strategies
0.020
0
0

30

60

90

120

150

180

210

240

270

Negative Expected Utility

Figure 13: The Full augmented distribution. The graph shows the probability of obtaining a strategy of a particular utility, given that it is chosen from (1) the set of all
strategies, (2) the set of learned strategies, or (3) the expert strategy.

389

fiGRATCH & CHIEN

7.

Future Work

The results of applying an adaptive approach to deep space network scheduling are very promising.
We hope to build on this success in a number of ways. We discuss these directions as they relate to
the three basic approaches to adaptive problem solving: syntactic, generative, and statistical.
7.1 Syntactic Approaches
Syntactic approaches attempt to identify control strategies by analyzing the structure of the domain
and problem solver. In LR-26, our use of meta-control knowledge can be seen as a syntactic approach;
although unlike most syntactic approaches that attempt to identify a specific combination of heuristic
methods, the meta-knowledge (dominance and indifference relations) acts as constraints that only
partially determine a strategy. An advantage of this weakening of the syntactic approach is that it
lends itself to a natural and complementary interaction with statistical approaches: structural
information restricts the space of reasonable strategies, which is then explored by statistical
techniques. An important question concerning such knowledge is to what extent does it contribute
to the success of our evaluations, and, more interestingly, how could such information be derived
automatically from a structural analysis of the domain and problem solver. We are currently
performing a series of experiments to address the former question. A step towards the resolving the
second question would be to evaluate in the context of LR-26 some of the structural relationships
suggested by recent work in this area (Frost & Dechter, 1994, Stone, Veloso & Blythe, 1994).
7.2 Generative Approaches
Adaptive LR-26 uses a non-generative approach to conjecturing heuristics. Our experience in the
scheduling domain indicates that the performance of adaptive problem solving is inextricably tied
to the transformations it is given and the expense of processing examples. Just as an inductive
learning technique relies on good attributes, if COMPOSER is to be effective, there must exist good
methods for the control points that make up a strategy. Generative approaches could improve the
effectiveness of Adaptive LR-26. Generative approaches dynamically construct heuristic methods in
response to observed problem-solving inefficiencies. The advantage of waiting until inefficiencies
are observed is twofold. First, the exploration of the strategy space can be much more focused by
only conjecturing heuristics relevant to the observed complications. Second, the conjectured
heuristics can be tailored much more specifically to the characteristics of these observed
complications.
Our previous application of COMPOSER achieved greater performance improvements than Adaptive LR-26, in part because it exploited a generative technique to construct heuristics (Gratch & DeJong, 1992). Ongoing research is directed towards incorporating generative methods into Adaptive
LR-26. Some preliminary work analyzes problem-solving traces to induce good heuristic methods.
The constraint and value ordering metrics discussed in Section 5.1.3 are used to characterize each
search node. This information is then fed to a decision-tree algorithm, which tries to induce effective
heuristic methods. These generated methods can then be evaluated statistically.
7.3 Statistical Approaches
Finally there are directions of future work devoted towards enhancing the power of the basic
statistical approach, both for Adaptive LR-26 in particular, and for statistical approaches in general.

390

fiADAPTIVE PROBLEM SOLVING

For the scheduler, there are two important considerations: enhancing the control grammar and
exploring a wider class of utility functions. Several methods could be added to the control grammar.
For example, an informal analysis of the empirical evaluations suggests that the scheduler could
benefit from a look-back scheme such as backjumping (Gaschnig, 1979) or backmarking (Haralick
& Elliott, 1980). We would also like to investigate the adaptive problem solving methodology on
a richer variety of scheduling approaches, besides integer programming. Among these would be
more powerful bottleneck centered techniques (Biefeld & Cooper, 1991), constraint-based
techniques (Smith & Cheng, 1993), opportunistic techniques (Sadeh, 1994), reactive techniques
(Smith, 1994) and more powerful backtracking techniques (Xiong, Sadeh & Sycara, 1992).
The current evaluation of the scheduler focused on problem solving time as a utility metric, but
future work will consider how to improve other aspects of the schedulers capabilities. For example,
by choosing another utility function we could guide Adaptive LR-26 towards influencing other aspects
of LR-26s behavior such as: increasing the amount of flexibility in the generated schedules, increasing the robustness of generated schedules, maximizing the number of satisfied project constraints,
or reducing the implementation cost of generated schedules. These alternative utility functions are
of great significance in that they provide much greater leverage in impacting actual operations. For
example, finding heuristics which will reduce DSN schedule implementation costs by 3% would
have a much greater impact than reducing the automated scheduler response time by 3%. Some preliminary work has focused on improving schedule quality (Chien & Gratch, 1994).
More generally, there are several ways to improve the statistical approach embodied by COMPOSStatistical approaches involve two processes, estimating the utility of transformations and exploring the space of strategies. The process of estimating expected utilities can be enhanced by more
efficient statistical methods (Chien, Gratch & Burl, 1995, Moore & Lee, 1994, Nelson & Matejcik,
1995), alternative statistical decision requirements (Chien, Gratch & Burl, 1995) and more complex
statistical models that weaken the assumption of normality (Smyth & Mellstrom, 1992). The process
of exploring the strategy space can be improved both in terms of its efficiency and susceptibility to
local maxima. Moore and Lee propose a method called schemata search to help reduce the combinatorics of the search. Problems with local maxima can be mitigated, albeit expensively, by considering
all k-wise combinations of heuristics (as in MULTI-TAC) or level 2 of Adaptive LR-26s search), or by
standard numerical optimization approaches such as repeating the hillclimbing search several times
from different start points.
ER.

One final issue is the expense in processing training examples. In the LR-26 domain this cost
grows linearly with the number of candidates at each hillclimbing step. While this is not bad from
a complexity standpoint, it is a pragmatic concern. There have been a few proposals to reduce the
expense in gathering statistics. In previous work (Gratch & DeJong, 1992) we exploited properties
of the transformations to gather statistics from a single solution attempt. That system required that
the heuristic methods only act by pruning refinements that are guaranteed unsatisfiable. Greiner and
Jurisica (1992) discuss a similar technique that eliminates this restriction by providing upper and lower bounds on the incremental utility of transformations. Unfortunately, neither of these approaches
could be applied to LR-26 so devising methods to reduce the processing cost is an important direction
for future work.

391

fiGRATCH & CHIEN

8.

Conclusions

Although many scheduling problems are intractable, for actual sets of constraints and problem
distributions, heuristic solutions can provide acceptable performance. A frequent difficulty is that
determining appropriate heuristic methods for a given problem class and distribution is a challenging
process that draws upon deep knowledge of the domain and the problem solver used. Furthermore,
if the problem distribution changes some time in the future, one must manually re-evaluate the
effectiveness of the heuristics.
Adaptive problem solving is a general approach for reducing this developmental burden. This
paper has described the application of adaptive problem solving, using the LR26 scheduling system
and the COMPOSER machine learning system, to automatically learn effective scheduling heuristics
for Deep Space Network communications scheduling. By demonstrating the application of these
techniques to a real-world application problem, this paper has makes several contributions. First, it
provides an example of how a wide range of heuristics can be integrated into a flexible problem-solving architecture  providing an adaptive problem-solving system with a rich control space to search.
Second, it demonstrates that the difficulties of local maxima and large search spaces entailed by the
rich control space can be tractably explored. Third, the successful application of the COMPOSER statistical techniques demonstrates the real-world applicability of the statistical assumptions underlying
the COMPOSER approach. Fourth, and most significantly, this paper demonstrates the viability of
adaptive problem solving. The strategies learned by the adaptive problem solving significantly outperformed the best human expert derived solution.

Appendix A. Determination of the Resource bound
A good CPU bound to characterize intractable problems should have the characteristic that
increasing the bound should have little effect on the proportion of problems solvable. In order to
determine the resource bound to define intractable DSN scheduling problems we empirically
evaluated how likely LR26 was to be able to solve a problem with various resource bounds.
Informally, we experimented to find a bound of 5 CPU minutes. We then formally verified this bound
by taking those problems not solvable within the resource bound of 5 CPU minutes, allowing LR26
an additional CPU hour to attempt to solve the problem, and observing how this affected solution rate.
As expected, even allocating significant more CPU time, LR26 was not able to solve many more
problems. Figure 14 below shows the cumulative percentage of problems solved; from those not
solvable within the 5 minute CPU bound. This curve shows that even with another CPU hour (per
problem!), only about 12% of the problems became solvable. This graph also shows the 95%
confidence intervals for this cumulative curve. In light of these results, the fact that one learned
strategy was able to increase by 18% the percentage of problems solvable within the resource bound
is even more impressive. In effect, learning this strategy has a greater impact than allocating another
CPU hour per problem.

Acknowledgements
Portions of this work were performed by the Jet Propulsion Laboratory, California Institute of
Technology, under contract with the National Aeronautics and Space Administration and portions
at the Beckman Institute, University of Illinois under National Science Foundation Grant
NSFIRI9209394.

392

fiADAPTIVE PROBLEM SOLVING

Probability

1.0

0.8

0.6

0.4

0.2

0
0

300

600

900

1200

1500

1800

2100

2400

2700

3000

3300

3600

Seconds

Figure 14: Given that a problem cannot be solved in five minutes, show the probability
that it can be solved in up to an hour more time (with 95% confidence intervals).

References
Baker, A. (1994). The Hazards of Fancy Backtracking. In Proceedings AAAI94.
Bell, C., & Gratch, J. (1993). Use of Lagrangian Relaxation and Machine Learning Techniques to
Schedule Deep Space Network Data Transmissions. In Proceedings of The 36th Joint National
Meeting of the Operations Research Society of America, the Institute of Management Sciences.
Biefeld, E., & Cooper, L. (1991). Bottleneck Identification Using Process Chronologies. In Proceedings IJCAI91.
Chien, S. & Gratch, J. (1994). Producing Satisficing Solutions to Scheduling Problems: An Iterative
Constraint Relaxation Approach. In Proceedings of the Second International Conference on
Artificial Intelligence Planning Systems.
Chien, S., Gratch, J. & Burl, M. (1995). On the Efficient Allocation of Resources for Hypothesis
Evaluation: A Statistical Approach. Institute of Electrical and Electronics Engineers Transactions on Pattern Analysis and Machine Intelligence 17(7), 652665.
Dechter, R. & Pearl, J. (1987). NetworkBased Heuristics for ConstraintSatisfaction Problems.
Artificial Intelligence 34(1) 138.
Dechter, R. (1992). Constraint Networks. In Encyclopedia of Artificial Intelligence, Stuart C. Shapiro (ed.).

393

fiGRATCH & CHIEN

Etzioni, E. (1990). Why Prodigy/EBL Works. In Proceedings of AAAI90.
Etzioni. O. & Etzioni, R. (1994). Statistical Methods for Analyzing Speedup Learning Experiments.
Machine Learning 14(3), 333347.
Fisher, M. (1981). The Lagrangian Relaxation Method for Solving Integer Programming Problems.
Management Science 27 (1) 118.
Frost, D. & Dechter, R. (1994). In Search of the Best Constraint Satisfaction Search. In Proceedings
of AAAI94.
Gaschnig, J. (1979). Performance Measurement and Analysis of Certain Search Algorithms. Technical Report CMUCS79124, CarnegieMellon University.
Ginsberg, M. (1993). Dynamic Backtracking. Journal of Artificial Intelligence Research 1, 2546.
Gratch, J. & DeJong, G. (1992). COMPOSER: A Probabilistic Solution to the Utility Problem in Speed
up Learning. In Proceedings of AAAI92.
Gratch, J., Chien, S., & DeJong, G. (1993). Learning Search Control Knowledge for Deep Space Network Scheduling. In Proceedings of the Ninth International Conference on Machine Learning.
Gratch, J. & DeJong, G. (1996). A Decisiontheoretic Approach to Adaptive Problem Solving. Artificial Intelligence (to appear, Winter 1996).
Greiner, R. & Jurisica, I. (1992). A Statistical Approach to Solving the EBL Utility Problem. In Proceedings of AAAI92.
Haralick, R. & Elliott, G. (1980). Increasing Tree Search Efficiency for Constraint Satisfaction Problems. Artificial Intelligence 14, 263313.
Held, M. & Karp, R. (1970). The Traveling Salesman Problem and Minimum Spanning Trees. Operations Research 18, 11381162.
Holder, L. (1992). Empirical Analysis of the General Utility Problem in Machine Learning. In Proceedings of AAAI92.
Kambhampati, S., Knoblock, C. & Yang, Q. (1995). Planning as Refinement Search: A Unified
Framework for Evaluating Design Tradeoffs in Partial Order Planning. Artificial Intelligence:
Special Issue on Planning and Scheduling 66, 167238.
Kwak, N. & Schniederjans, M. (1987). Introduction to Mathematical Programming, New York:
Robert E. Krieger Publishing.
Laird, J., Rosenbloom, P. & Newell, A. (1986). Universal Subgoaling and Chunking: The Automatic
Generation and Learning of Goal Hierarchies. Norwell, MA: Kluwer Academic Publishers.

394

fiADAPTIVE PROBLEM SOLVING

Laird, P. (1992). Dynamic Optimization. In Proceedings of the Ninth International Conference on
Machine Learning.
Mackworth, A. (1992). Constraint Satisfaction. In Encyclopedia of Artificial Intelligence, Stuart C.
Shapiro (ed.).
McAllester, D. & Rosenblitt, D. (1991). Systematic Nonlinear Planning. In Proceedings of
AAAI91.
Minton, S. (1988). Learning Search Control Knowledge: An ExplanationBased Approach, Norwell, MA: Kluwer Academic Publishers.
Minton, S. (1993). Integrating Heuristics for Constraint Satisfaction Problems: A Case Study. In
Proceedings of AAAI93.
Moore, A. & Lee, M. (1994). Efficient Algorithms for Minimizing Cross Validation Error. In Proceedings of the Tenth International Conference on Machine Learning.
Nelson, B. & Matejcik, F. (1995). Using Common Random Numbers for IndifferenceZone Selection and Multiple Comparisions in Simulation. Management Science.
Sacerdoti, E. (1977). A Structure for Plans and Behavior. New York: American Elsevier.
Sadeh, N. (1994). Microopportunistic Scheduling: The Microboss Factory Scheduler. In Intelligent Scheduling. San Mateo, CA: Morgan Kaufman.
Segre, A., Elkan, C., & Russell, A. (1991). A Critical Look at Experimental Evaluations of EBL.
Machine Learning 6(2).
Smith, S. & Cheng, C. (1993). Slackbased Heuristics for Constraintsatisfaction Scheduling. In
Proceedings of AAAI93.
Smith, S. (1994). OPIS: A Methodology and Architecture for Reactive Scheduling. In Intelligent
Scheduling. San Mateo, CA: Morgan Kaufman.
Smyth, P. & Mellstrom, J. (1992). Detecting Novel Classes with Applications to Fault Diagnosis. In
Proceedings of the Ninth International Conference on Machine Learning.
Smyth, P. (1993). Probability Density Estimation and Local Basis Function Neural Networks. Computational Learning Theory and Natural Learning Systems 2.
Stallman, R. & Sussman, G. (1977). Forward Reasoning and DependencyDirected Backtracking in
a System for ComputerAided Circuit Analysis. Artificial Intelligence 9, 2, 135196.
Stone, P., Veloso, M., & Blythe, J. (1994). The Need for Different DomainIndependent Heuristics.
In Proceedings of the Second International Conference on Artificial Intelligence Planning
Systems.

395

fiGRATCH & CHIEN

Subramanian, D. & Hunter, S. (1992). Measuring Utility and the Design of Provably Good EBL Algorithms. In Proceedings of the Ninth International Conference on Machine Learning.
Taha, H. (1982). Operations Research, an Introduction, Macmillan Publishing Co.
Wilkins, D. (1988). Practical Planning: Extending the Classical Artificial Intelligence Planning
Paradigm. San Mateo, CA: Morgan Kaufman.
Xiong, Y., Sadeh, N., & Sycara, K. (1992). Intelligent Backtracking Techniques for Job Shop Scheduling. In Proceedings of the Third International Conference on Knowledge Representation
and Reasoning.
Yakowitz, S. & Lugosi, E. (1990). Random Search in the Presence of Noise, with Application to Machine Learning. Society for Industrial and Applied Mathematics Journal of Scientific and Statistical Computing 11, 4, 702712.
Yang, Q. & Murray, C. (1994). An Evaluation of the Temporal Coherence Heuristic for PartialOrder
Planning. Computational Intelligence 10, 2.

396

fi	
	fffi	
	

	!"$#%$&('*),+--.0/1''0243'652

B

789:;%$<=+8+>-026?$@9
#&	<A.0>-.

CEDGFAHJILKMCEFNILHPO*QRDLFNSUTVDGFXWEYZO[O]\_^`YbaZO]IcFAdfed`g
TVFNDhH

ijFNDLklK0OcHPmRInd`\

WEDoK^qpre0Dhd`m

sEtVu"vwu"xjy*u"xNz|{}u"~%~

|"1|		G|"r "

n8(orw(!V8*
wq0wq;;4ww6f,
r8w,Ar1V	
qu"~,u"vZru|u"t	u!u

||||||1 ||

8;N=		4n=8w4E"	
	q=wf,,!=6	r;n
=w[;"

(V!
800$$qV`$Z	q4w	!;`!M$	$	
  c$	}1!6  ;V	G=%	$`w6%4wE  	$A%	611(`V
$	1N%$	4w	$	q"*1"V	10|ZN1(1%4wE  	$;;  (8
$84w	1;|610L  0	Vq	$	$;c`VwV$	6	!LV
$	$EA!;,%L `(	 $	}%	A0$E6	$$8$V" *;1%
wEV$$	Ef$cZ$nVVEw0  $ 0w	Vq0 (1(4q  	
w1[!	lq,"wq,w	 w0 !0l0$Efo1 0qwV"AZ
$4$	1;%E  V  $fEq4$	Nf  ` 	`w }oV*V4w	f!
V  	 ;  	$Z$	$o`wl$4wo[8E;V$,  V
o`V$r0w!	![E$80ww	$	|180V`		!VN0$
,w$V rw0 |611
 $	w ($6$ AV6
0$




	 fiffff 











  

	







	 

!#"%$'& fi()+*(!fi,( &
-/.10324065870fi9;:=<?>A@CBfiD;E<?>GFC.?FHB'9;><?>GIJ<LKM<?>A2;EGFNDO0fi>2;FOPQ2R0'58DO0fi> DOFTSG2%.?FHB'9><1>GIUV-WSG9;0fi20fi2YXZS<LDTB[.
FOP B'@\S.?F]065^2;E<LK<LK_.?FHB'9;><?>GI2;0N9;FHDO0fiI3><1`TFRE B'> abc79Y<?2;2;FT>dDE B'9BfiDO2;FT9OKe59;03@fD.gB3K;Kh<ji FHakFOPlB6@\S.?FHKTU
m0fi> DOFTSG2n.?FHB'9;><?>GI
<LK2EGFoKYpGqGrYFHDO20'5sB6>%<?>Z2;FT> KtF2;EGFT0fi9;FT2t<gDTBu.vKY2;p aX%pG> aFT92;EGF> B'@RF0'5nwyx9;0fiq B6q.1X
-SGSG9;0[PZ<?@]B'2FO.1Xzm0fi9;9;FHDO2R{|x-
m}o~FHB69;><?>GIZJKt0DTB[.j.?FHakq FHDTB'p KtF]2;EGFR.?FHB'9;>GFT9M<LKM9;FHfip<?9;FHaJ0fi>.?X
2;0.1FHB69;>B6>B'SGSG90HP<?@]B'2Y<?0fi>J2;02;EGFC2B'9IfiFT24DO0fi> DOFTSG2%7<?2;EdBNE<?IfiEkSG9;0fiq B6q<.j<?2YXA{hB[.j<gB6>Z2HeHfi'G}OU
 E<LK+9Y<LD;EB6> a\Ifi90u7<1>GIMq 0GaX\0'5#:>G0u7.1FHaI3FKY2;p a3<?FHK+2;EGFoS0ZK;Kh<?q<j.j<?2|XN065v.?FHB'9><1>GICB'SGSG9;0[PZ<?@]B62Y<?0fi> K
2;0DO0fi> DOFTSG2OK<?>Va3<j#FT9;FT>Z2
9FTSG9;FHKYFT>2B'2Y<?0fi> K_pG> aFT9uB69Y<?0fip K.?FHB'9;><?>GI=SG9;0fi2;0GDO0'.LKTUC{;fiFTF\B'2B'9B6rtB6>#
Hfilfi-o>2;EG0fi>Xn<?IfiIZKy#HfifilZ039FHB69;> KoB6`O<19OB'><H3'M50fi9B%qG9;0ZBfiaR<?>2;9;0Gap DO2Y<?0fi>#Uj}
 >N2;E<LKS B6S FT9HG7FB'9FDO0fi> DOFT9;>GFHaN7<12EB\a3<jsFT9FT>Z2:3<?> aN0'5.?FHB'9;><?>GINDTB[.j.1FHadwTKtS FTFHapGSN.?FHB'9>Gb
<?>GI k7E<LDEaFHB[.LK\7<?2;Ez<?@\SG9;0'fi<?>GId2;EGFDO03@\SGpG2B'2Y<?0fi> Bu.oFOD<?FT> DOX0'5_BSG9;0fiq.?FT@KY0'.?fiFT9C7<?2;E
FOPGS FT9Y<?FT> DOFfiU>GF0'5
2;EGF=@]B[<?>a3<j#FT9;FT> DOFHKCq FT2Y7FTFT>A2EGFDO0fi> DOFTSG2R.?FHB'9;><?>GIAB'> ad2EGFKYS FTFHapGS
.?FHB'9;><?>GI<LKM2;E B'2Hs<?>d2;EGF%.LB'2;2;FT9[#<?2M<LKM2;EGFT0fi9;FT2Y<LDTB[.j.?XkS 0ZK;K|<1q.?FN2;0KY0'.?fiFC2;EGFCSG9;0fiq.?FT@]KM0fiSG2Y<?@]B[.j.?X
p Kh<?>GINB%qG9;pG2;FTb5c0fi9DOFMSG9;0fiq.?FT@Kt0'.?fiFT9HUe0'7^FT3FT9HSG9;0fiq.?FT@Kt0'.?fi<?>GIC7<?2;EG0fipG2.?FHB'9;><?>GIC<LKoxnbcE B'9a
<?>A@\0KY2\0'5
2;EGFHKYFa0fi@CB[<?> KTB'> akEGFT> DOFN<LK<?@\SG9BfiDO2Y<LDTB[.<?>A@\0KY2]DTBfiKYFHKyU  EGFN9;0'.?F=0'5o.?FHB'9><1>GI
DTB'>q FCKYFTFT>BfiKe<?@\SG9;0'fi<?>GIN2;EGF%FOD<?FT> DOX065^BCqG9;pG2;FTb5c0fi9DOF%SG9;0fiq.?FT@fKY0'.?fiFT9
qXB3DTfip<?9Y<?>GIKt0fi@\F
wTDO0fi>2;9;06.v:>G0u7.1FHaI3FH%2;E B'2n<gKp KYFO5p.2;0%Ifip<LaF
SG9;0fiq.?FT@KY0'.?3<?>GI\<?>C5c9;p<?2Y5p.a3<19FHDO2Y<?0fi> KTU  >B%DO0fi>Gb
DOFTSG2e.1FHB69;><?>GIN2BfiKY:Gq FO5c0fi9;F_.?FHB'9;><?>GI 2;EGFT9;F_<LK>G0fi2FT>G0fipGIfiE<?>50fi9@]B'2Y<?0fi>=2;0D.LBfiKKh<j5XB'>=FOPlB6@\S.?F
o +--8.rfi|fi"$##G	<r	<+[8ff:(19
#&	#	fi

%&##c<'

fiG 3TGHH^''GGG;G

TfiTCZRG;G;TcfiOfifiTC;GfiG3\;G
Y THGC?H';?G\Gfifi' 3^3TOH;+;4_G;G;TcfiO
G;fi?TY'?fiT[[1nL^tYj# [j?TGfio
;O3;%g6;+1OGu1H3?]_[
; 6]'3HG;fiG
?Tt'?fi?G\O?THGT;
 [fi TTC'ZNY TOH;|t THG=?H';?GtQYT]H;OY? H
?d;GCOQTY?\TZO[]fi1GC?H'1G?;TO';G;fi%4y4|?;fi#HfifiQ8' #M%
|#u1O ZYT?Z3veTOjHfi3 O?GNY^%';G\YG;fi%?GTfiGHT
 fi hLT;GfiC[?N'ntZ% 6L81;TfiO'Y?fi#e13TN;GO ?Y?fi';GfiC[?' N
Y6 G'NO'?'?;Tfi[LT'fifiG
 fiOfi\?T;M?cfi;]'t13fi=Gu;t'?fi4'NY'?u'?
G;fi?T=THj?N\ZGC' 'T' 61C'oY'?3?GGfi?T]%'oY% 'jL%?;Tfi6Y?fi#
\';GNRH' ON1T
?3Go\TGQGT_'GH'; '%GTH;COl'1G\;6\?
? Y' OHy Y; ]t'?GY?fi ;%;GHYe? Y' OHy ' R fiYHCfiC;GHYY61Gt13 GjgG%YT'
GTGYLYYLT_; '
jjT '?] 3L;3?VY'?fiMcG;G;%G;fi?T]TMJ;L
YT Yfi#G?H'1G
G;GOH; fieGO? H?\G;'fi
fiGOfi\GG't13 [#O?T Ofi
 ^Gt1O \H;Ot1oG?;?Y?fiC T? \3G6\Tfi;GT;fiZOcT;Y?GGcfi;C[lTO[jg
;ML';TYHOY?fi yNH;tT OfiG
^3gj?fifiG+?H';?GCG;fifiO';%T H3?Gc'j?u1G
]'GGT 4Ofi |gT\=g3;
  6fi]u1 yt ;k; '
HfiVfiC[??k;G]Lfi;8gMG';
 ;YH;+'\O?Tu13fiY?;G  o6;+1;T;Ht;H%?]8?H';?G\[?fifiY?;G fiGoLfi;
  '
[?fifit1G; ''3H^3?GGG;GYHvT6Y?fi'\fiC[?[\c;fi ;GLfifiOjfi
;'R1+? Y6 OH' ;GGfi?T] ?4 '3][?\' 
;GO?Y61Gt13 TZ' _G;G OHfi#3G;GG
'
O?Tu13fiY?;Gfi;Gfi][?#
 
j YTfi[;G^'\?? Y6 OH' GO1Y'?GY?fi 
L[4O; L[ ;'?+?4G^GQOH;yfifi?\GO16 YT OfifiOfi Y; OY?G'O?TG;31Tt'?fiT
cfioG
?GGG4fiC[?T6 \OfiRGG'Y?fi [j?=?Z;OfiO'?fi;Le ' TH%';
?;T;HYH
?]O '%??G\;GOfi 3?Y?fi G TL;NY %?H';?G%L+]fioO3\GG'Y?fi u?\ON1TZ
 h?G\;'\?? Y6 OH^6 ]Y61Gt13 T  G;HYT
Gj HR3;][6\Tfi; '^T'GG;H
 3;YG TfiLYH' CG YGT;3gtHNcfi;C6nYTHGN?H';?G GT;
O '\?He'Y TOH;hc
G;fi?TY613?G'Gu3LHZ;H3;GT' AdYH'AGfifi' ;HYHOY?fiO?fi
 G
c'\Tfi;g fiYHN3Yfi\'fiGG;T3?fi fi\;T 3;;HCH'Yj?TM|o6'
 	t'CfiT uj
Hfi3fi ff#3T [j[fiGHO
 GM\T;GGG
'' [?Q|g%'|1L'%;N '
'
   ?H';?G  ?
; 'e^fi?N;H3?;;G_?H';GT;%fiG;GGo'6GG;[Z?]'O1O3;;HOG;31TWY'?fiT?;
?fiG;3 'j?Yfi
 ' Y%fie?
   ?H';?G %;H31];GM?H';GT%;C NY TOH;|fi'Z
Y6Y?fi ';\Gfi?T3LY;t1GGt13GGG'N;\G1H6;GTH
GT;o [fi TTtfi\fi;GT';;TRG;_3;][j TY THG?H';?GNfi ?  3GT#v[fi G
oO1GT?Z tfi#H3fiGfi fiGGO']'L'N
 eG;TH [fi 
 'TfiTH3\ZY+'#;GHYe3;][j H'
Y?fi n'sYTHG\?H'1GM Yo\H3YG;'G;fi?T\Y613?G% TYcfi;]6 Ot ;]3;GG4T
'GGHOG ' H=?Y'?3?G%G;fi?T   3GT#H3 fi;GMZG% T'GjvT'Y?fi MfiG
?6 YTY?G3GT;k;;O?GT4? Y3eHfifiO  \ Oj?TfiN; 6;GHYR\HfiYG;HM';\Z
 GfiO[?GH%;
  YOcvfiecfiG G't13%3;fiG Y;GT3;'YTHG\?H'1G\OfiR ''?
;%;G
' [?GhL'Ofi OTG1H6;?G\?N;G
   ?H';?GR6\Tfi; G6?'?G\;G
Y6 G'
GfiOtgO?NOfi\?O1YC;GTfi;fi YoGofit;TfiYo3YZRG;fiYLOfi\?OZ?YNfifiG\H3YG;
' TYcfi;]6 Ofi  %;H31]NY TOH;hc^YTHG?H';?G=G;fifi6;C;HY?_1k\G;31T
Y'?fiT+GZYfit;TfiYfiY\G;fiYLO3\?OZ?YMgn T;T; '%; 6'v;GeGfi?H';?G%G;G;T
cfiOG;fi?TY'?fiTH=fi;T'fiT[H;G1H6;?G
?YOj% YOfi tG\fi?\ '?Gfi%L['\3GZ
'Y?\%' R 61Gfi%L[G4T
'Ol'R1Hyoefi; 'H fiTO33?G];CfiGoO ?Y?fi# ;G
Y6 G'Rcfi;]6^Ofi\j?T
fiGY?% H'Y?fi 
Y 3+?Zfi=GG;'jj?G ' ?\G;'fiT\T+?;G
 ''\fiLVG\G;fifiO'WLGVGfifi ujc3?H';?GG;GOH;YHM HT' t];GT
\Gfi 'Gfi
;G3YZRG;fiYLOfi\?OZ?YN'Gfi?TY613?G 
 e'^T3TH\fi;
YfiGLYYLT'H
!

fi"$#&%('fi)*fi+,#-'fi*fi).0/1%('fi243!%('456..0798fi6;:9.*fi'fi<fi=><9?

@BACEDGFHAI@JfiCEAKCMLIDONECPLIQ9FR@BACED,LINTSAQGFTU9VXWYL0FJ9L
CENTSZL[\^]IL[U9LINTSAQYL
CE\_VMAQ9FRSZ`a\^CE\`bLFH[\LICEQaSQfiK,JfiCEAc
]0S>`a\`dNEWfi\^efSDJfiCEAI]\gNXWfi\LFTeaDJfiNEANhS>ViVMADJa[\MjkSNTeAI@-JfiCEA0KCPLIDl\Mjfi\VMUfiNTSAQ-mnoWaU9F^p0qr\is9\M[tS\^]\NEW9LIN
SDJfiCEAI]0SuQfiKNXWfi\_qoACPFTNXcvV^LFT\LFTeaDJfiNEANTSZVfVMADJa[\MjkSNTewA
@xJfiCEA0sa[u\^DyFhAI[]SQfiKwV^LIJfiNEUfiCX\FL,VMAKQaSNTS]\M[e
SQkNX\^CE\FTNTSQfiKb@zA0CED{AI@|FhJ9\^\`aUfiJ}[u\L
CEQaSQfiK9m,~g[NEWfiAUfiK0WLIQaeY`a\VMCX\LFT\dSuQLFTeaDJfiNEANTSZVVMA0DJa[\MjkSNTebS>F
U9FT\M@BUa[xLIQ9`dSQaNE\^CE\FTNTSQfiK9pSuQbNEWaSZFoJ9LIJ\^Cqo\qoSt[[s9\_VMA0Q9VM\^CEQfi\`qoSNEW,[\LICXQaSuQfiK,AI@J9AI[eaQfiADSZL[cBNTSD\
JfiCEAsa[\^DFhAI[]\^CPF@BACg`aADGL!SuQ9FHqgWaSZVXWV^L
QGAQa[e,s9\FTAI[]\`dSQ,\MjJAQfi\^QkNhS>L![NTSD\iSuQ,NEWfi\qoACPFTNoV^LFT\
qoSNEWfiAUfiN|[u\L
CEQaSQfiK9m
 Q\VMNTSAQ(pqr\fSQaNECEAfi`aU9VM\NEWfi\JfiCE\M[tSDSQ9LICTS\FA
@rJfiCEAsa[\^DFTAI[]SQfiK;LIQ9`4~[\LICEQaSQfiK9m  Q
\VMNTSAQfipqo\SQkNXCEA`aU9VM\oAUfiC@BACEDGL[I@BCPLID\^qoACXg@BACFTJ9\^\`aUfiJf[\LICEQaSQfiK9mCMLq|SuQfiKiAQ_JfiCTSACCX\FTUa[NPF
SQY~c[\LICXQaSuQfiKpfiqo\JfiCEAI]\1LK\^Qfi\^CPL[-NEWfi\^ACX\^DOSZ`a\^QkNTSt@BeSQfiKwVMAQ9`0SNTSAQ9FFTUaVXS\^QaNgNEAGL![[AIqFTU9VXW
[\LICEQaSQfiK9m  Q\VMNTSAQpqr\,LIJfiJa[eYA0UfiC@BCPLID\^qoACENXA,[u\L
CEQaSQfiKVMA0QkNECXAI[xCXUa[u\FLIQ9`}`a\FEVMCTSs9\LIQ
SDJa[\^D\^QaNPLINTSAQLIQ9`}\MjfiJ9\^CTSD\^QaNPL[oCE\FTUa[NPFSQNEWfi\GFTeaDs9AI[tSZVdSQkNE\^K0CPLINTSAQ`aADGLSQ-m  Q\VMNhSuA0Q
 pqo\_LIJfiJa[ewAUfiCi@zCMLID\^qoACENXA[\LICEQaSQfiKDGL0VMCEAcBAJ\^CPLINEACMFHSQwNEWfi\`aADGL!SuQbAI@rSKWaNUfi^M[\m  Q
\VMNTSAQ(pqr\,`0S>FXVMU9FEFAUfiCqoACXSQ4CE\M[ZLINTSAQ}NEAGJfiCX\^]SAU9F@BACEDGL[tSLINTSAQ9FA
@|FTJ\^\`aUfiJ4[\LICEQaSQfiK9m
 Q\VMNTSAQ}kpqr\_`0SZFEVMU9FXFFTAD\@zUfiNEUfiCX\_\MjfiNE\^Q9FRSAQ9FNEAA0UfiCg@BCPLID\^qoACXp0SQ9VX[U9`0SuQfiKb[\LICEQaSQfiK,@BCEAD
UfiQ9FTAI[]\`JfiCEAsa[\^DGFLIQ9`,\Mjfi\^CPVXSZFT\F^mi\VMNTSAQYVMA0Q9VX[uU9`a\FNXWfi\J9LIJ9\^Cm
09vRvHvfi
S NEWfiAUfiN[AkFXFAI@K\^Qfi\^CPL[tSNTepkqo\gLFEFhUfiD\!fi
rNEAs\gNEWfi\L![uJfiW9L
s9\^NoAI@NXWfi\o[ZLIQfiKU9LIK\iAI@FTNPLINE\
`a\FEVMCTSJfiNTSAQ9FpLIQ9`U9FT\d@zA0CrNXWfi\_FT\^NgAI@saSQ9LICEewFTNEChSuQfiKaFoAI@[u\^QfiK0NEWYm
~Tk^tl_SZFLiNEUfiJa[\_TPMPpqgWfi\^CE\pa4  SZFLFh\^NAI@B0B^^pSZFJfiCEAVM\`aUfiCX\
NEACE\VMAKQaS^\LFhUfis9FT\^NAI@FTNPL
NE\FSQwLFNXWfi\ra^BB^^pLIQ9`SZFLFT\^NAI@xMfiTBXI!EMIIkp
qgWfi\^CE\\LVXW4SZF_L,JfiCEAfiVM\`aUfiCE\qgWaSZVXW4NPLI\FL,FTNPL
NE\SQL0FoSQfiJfiUfiNL
Q9`wA0UfiNEJfiUfiNPFLIQfiANXWfi\^C_FTNPLINE\
L[ZFTASQrmnoWfi\4VMADsaSuQ9L
NTSAQAI@KAaL[ZFGLIQ9`AJ\^CPLINEA0CPFfSZFV^L[t[\`NEWfi\}vfiP oP0BZ}AI@bm~
BIT,
S>FLFT\^NoAI@`aADGL!SuQ9F`a\M9Qfi\`AI]\^CiNEWfi\_FELID\_Fh\^NgAI@FTNPLINE\Fm
 \`a\^QfiANE\gNXWfi\CE\FhUa[uNoAI@-LIJfiJa[eSQfiKGL
QAJ9\^CMLINEACoNEALfFTNPLINE\gsaeTIPm~Tk^tS>FoLfFTNPLINE\
f;rm~JfiCEAsa[\^D{S>F^tkHSt@NEWfi\^CX\S>FLFT\0Ufi\^Q9VM\_A
@xAJ\^CPLINEA0CPFHlp-p9PpLIQ9`L
FT\0Ufi\^Q9VM\AI@FTNPLINX\Fop9pfiMpFhU9VEW,NEW9LINoLxf^pvs-@BACoL[t[&@BCEADiNEApkh^9Pp
LIQ9`Vx  FXLINTSZFR9\FHNEWfi\gKAaL[&dm  QGNXWaS>FoV^LFh\pkSZFrLd^tkB,^Pk9MAI@pfiLIQ9`,SZFNXWfi\ta
AI@NEWfi\_FTAI[UfiNTSAQ;FT\0Ufi\^Q9VM\_m
n|Wfi\a^t^^SZFLFTeaQkNMLVMNTSZVHD\LFTUfiCE\iAI@9NEWfi\VMADdJa[u\MjaSNTeA
@&LJfiCEAsa[\^DFTU9VXWGLFSNPF[\^QfiKNXW
qgWfi\^Qb\^Q9VMA`a\`SuQbsaSQ9LICEem  @SZFLIQwLICEsaSNECMLICEeJfiCEA0sa[u\^DSQ44paNEWfi\^QSNPFgFRS^\k0SZFm
 ANTSZVM\NEW9L
NA0UfiC`aA0DGLSQFTJ\VXStV^LINTSAQSZFQfiA0NGLFf\MjJa[tSZVXSNwLFfNEWfi\w`aADGL!SuQNXWfi\^ACEeU9FT\`}SQ
NTekJaSZV^L[FhJ9\^\`aUfiJ[\LICEQaSQfiKfJfiCEAKCPL
DGF[tS\_ $ SQkNEA0Q-pk aPmnoWfi\oAJ9\^CPL
NEACPFQfi\^\`fQfiAN
s9\g`a\FEVMChSus\`fSQNEWfi\ _M @BACEDGL[tSZFTDwpL
Q9`KAkL![>FQfi\^\`fQfiANs9\H[AKISZV^L[fi@BACEDUa[ZLF^m  Qf@ LVMNpNEWfi\^e
Qfi\^\`dQfiANs\1`a\VX[ZLICPL
NTS]\M[eCE\^JfiCE\FT\^QaNE\`,LINL[t[vpfisfiUfiND,Lefs9\`a\FEVMCTSs9\`,saeJfiCEAfiVM\`aUfiCE\FHqgWfiAkFh\gCEUfiQ
NTSD\iSZFCX\LFTAQ9LIsa[esAUfiQ9`a\`mnoWaU9F^paAUfiC [u\L
CEQaSQfiK@zCMLID\^qoACEfCE\UaSCE\FHNEWfi\i[\LICEQaSQfiKNE\VXWfiQaSZUfi\F
NEAs\_DA0CE\SQ9`a\^J9\^Q9`a\^QaNAI@xNXWfi\_AJ\^CPLINEACoCE\^JfiCX\FT\^QkNMLINTSAQNEW9LIQbNEWfi\NECPL0`0SuNhSuA0Q9L[D\^NEWfiAfi`fiF^mn|WaS>F
L[t[AIqFgVXWfiAkAkFSuQfiKdNEWfi\A0J9\^CPLINXACoCE\^JfiCE\FT\^QaNPLINTSAQ,qgWaSZVXWSZFos9\FhN|FhUaSuNX\`NEANEWfi\`aADGLSQCPL
NEWfi\^CoNEW9LIQ
s9\MSQfiKVMAQ9FTNECMLSQfi\`saeGNEWfi\LFEFTUfiDdJfiNTSAQ9FoAI@NEWfi\[\LICEQaSQfiK,NE\VEWfiQaSZ0Ufi\m
 QNEWfi\FTJ9\^\`aUfiJb[\LICEQaSQfiKGFTefiFTNE\^D,FrFhNEU9`0S\`saeGNEWfi\\MjfiJ9\^CTSD\^QaNPL[xVMA0DDUfiQaSuNTepfiNXWfi\KAaL[ZFgLIQ9`
AJ\^CPLINEACMFL
CE\U9FTU9L[t[eJ9LICPLIDd\^NE\^CTS^\`mnoWfi\FT\wFheFTNX\^DGFL[ZFTA;[\LICEQVMAQkNXCEAI[gCEUa[\FLIQ9`DGLVMCEAc
AJ\^CPLINEACMF_q|SuNXWJ9LICPL
D\^NE\^CPF^m \L
CEQaSQfiKJ9LICMLID\^NE\^CTS^\`CEUa[\F,LIQ9`DGLVMCXAcBAJ9\^CMLINEACPFfDGL
\FfSN
J9AaFEFRSsa[\gNEALIJfiJa[efNEWfi\^DNXAgJfiCEAsa[\^DGFAI@9LICEsaSNECMLICEe_FRS^\m~QfiA0NEWfi\^CxL0`a]!LIQaNPLIK\A
@9J9LICPLID\^NX\^CTSLINTSAQ



	

 

ff

fi





fi
!!"#$%
&('*),+-.!/0&212&3)45)$67.!880134)$+-'$.!9:-<;$=013->;$-?@=;@'A&3BC-@134D9E.!F04G)H&39:-'I!JK+-;$-<-.C?,+.!88012&(?.!)H&36CFL/0&3FM'N)$+8.!;O.P9:-)$-;O'N)$6DM&2Q*-;$-F0).!;,RC=9:-F0)O'SUTVF0WX6C;$)$=F.!),-@1Y4CI+6!J-B-;IC8.!;@.!9:-)$-;H&3Z.!)[&Y6F\.1]'[6&3F?@;$-.C'H-'
)$+-^?@6C9:8=)@.!)H&36CF._1?@6`'[)a6!W&3F'H)O.!F0)H&(.!)H&3FRb),+-^6C8-;@.!)$6C;O'dce6C;f;$=013-'@gOSihj+-Fj)$+-^F0=9L/-;a6!W
8.!;O.P9:-)$-;O'5?.!Fk/l-m.P;$/0&3)$;O.!;H&2134n+0&YR+*Io)$+-p&YF'[)O.!F0)H&(.!)H&36CFn8;$6C/013-9q&]'pr7sute?@6C9:8013-)$-p&YFR-F-;O._1eS
v F-JK._4),6),+-6C;$-)H&(?._12134f12&Y9\&Y)V)$+0&('K?@6`'H)<&(')$65=88-;$tX/l6C=FMw)$+-7F0=9L/-;K6PWU8.!;O.!9p-)$-;O'o6PWU)$+6C8l-;O.!)$6C;@'I9f.C?@;$6CtX6C8l-;O.!)$6;O'I.!FM5?@6CF0)$;$6!13tX;$=013-'N)$67.G?@6CF'H)@.!F`)_SUx+0&('y-F'H=;$-'N)$+.!)y/6C)$+5)$+-)H&39:WX6C;G&3F'H)O.!F0)H&(.!)H&36CF.!FMz)$+-pF`=9L/-;56!W{M&2Q*-;$-F0)&3F'H)O.PF`)H&(.!)[&Y6F'.P;$-L86!1340F6C9L&(._1('5&3F^)$+-L13-FRC),+
6!WU),+-D'H)O.!)$-M0-'$?@;H&38)H&36CF*S|A)&('860'$'A&3/013-D)$6:-@})$-FMf6C=;V;$-'H=013)O')$6:'H=?,+a8.!;O.P9:-)$-;H&3Z-M~M069E._&3F'
J&3)$+'H6C9p-'H=0&3)O.!/013-z;$-'H),;H&(?@)H&36CF'f6CFb),+-F0=9L/-;f6!WD8.!;O.P9:-)$-;O'p6C;f)$+-@&3;f&YF0)$-;OM0-8l-FM0-F?,&3-'
cxN.!9L/-CINrK-J-@121eI>6`'[-F`/013606C9~IKC0gOSL|AFzW.?@)I*6C=;L.!88012&(?.!)H&36CF6!WK)$+-LWX6C;$9f._1UWX;O.!9p-J6;$
)$6z)$+-~'H409L/6!12&(?f&3F0)$-RC;O.P)H&36CFbM06C9E._&3FjM06`-'5&3F0BC6!13BC-~.!Fn&39:8012&(?,&3)a8.!;O.!9p-)$-;:)$+.!):M0-F6C),-'L)$+'H=/l-@}8;$-','A&36CF6PWD)$+-k?@=;$;$-F0)a-@}8;$-'$'A&36CF),6kJK+0&(?,+.!Fj6C8l-;O.!)$6C;f&('a.!88012&3-M
SV6!J-BC-;IWX6C;
'A&39:8012&(?,&3)4L6!W-@}86`'A&3)H&36CF*I!J->?@=;$;,-F`)H134;$-'[)$;H&(?@)*6C=;*),+-6C;$-)H&(?._1CWX;O.!9p-J6;$K)$6KF6F8.!;O.!9:-),-;H&3Z-M
6C8l-;O.!)$6C;@'S
H`2_D W6;.zM06C9E._&3Fbq&('L.zM0-)$-;$9L&3F0&('H)H&(?d8;$6CRC;O.P9)$+.!)5)O.!C-'L.C'ff&3F8=)E.
8;$6C/013-9~IlCI.!FMf6=)$8=)O'<&3)O'K'[6!13=)H&36CF~'H-=-F?@-&2W'H=?,+a-@}`&('H)O'I6C;)$+-D'H8l-?,&].1u'H409L/6!1!K&2Wo&3)
M060-'KF6C)-@}0&('H)S
`,C`]feCOL &(':.z'H-):6PW78;$6C/013-9'H6P1YB-;O'S^|W  &(':.z'H8.C?@-a6PW7+`4086)$+-'H-'I<)$+;$-'H),;H&(?@)H&36CFE6PW  ),6D8;$6C/013-9E'<6!WU'A&3Z-Lbf&('?._1213-Ma. eC@ 6!W*+`4086)$+-'H-'.!FM5&('M0-F6C)$-Mp/`4
K SK6C;$9E.1134CIW6;7-BC-;,4 ^w I)$+-;$-&('7.f?@6C;$;$-'[86CFM&3FRa8;$6C/013-9'H6P1YB-; _~~V 'H=?,+z)$+.!)
  cH
go  cg*&2W{NCbw.!FMp=FM0-@F-Mf6C)$+-;$J&('H-CS<x+- 2@CC@(0CLC 6; CLC
6!W>.L+04`8l6C)$+-'A&('G'H8.C?@-  &('7M0-@F-Mw)$6L/-136CRff V .!FMp&('GM0-F6C)$-Ma/04a`Awc K gOS
o :G
C<e>$C* !*XN>C*<el
|eFf)$+0&(''H-?@)H&36CF*I0J-7M0-'$?@;H&3/l-6C=;o13-.!;$F0&3FRLWX;O.!9p-J6;$SNy&3;O'H)I)$+-7M06C9E.&YFa'H8l-?,&?.P)H&36CFa&('oRP&YB-F
)$6a)$+-513-.!;$F-;_Sx+-p)$-.C?,+-;)$+-F^'H-@13-?@)O'D.!Fn.!;$/0&3)$;O.!;,4a8;$6C/013-9M&('H)$;[&Y/=)[&Y6F.!FMz.f8;$6/01Y-9
'H6!13BC-;_Sbh^-z.C'$'H=9p-a)$+.!):)$+-;,-a&('E.P)L13-.C'H)f6CF-~8;$6/01Y-9'H6!13BC-;p&3Fb)$+-w+`4086)$+-'A&('E'[8.C?@-~6!W
)$+-L13-.!;,F-;D)$+.!)G&('WX=F?@)H&36CF._12134^-C=0&3B!._13-F`):),6a)$+-:)$-.C?,+-;'G8;,6C/013-9'H6!13BC-;_I
&eS-CS3Iy6CF-:JK+0&(?,+
6C=)$8=)@'>),+-D'$.!9:-D'H6P1Y=)[&Y6F~.C')$+-G)$-.C?,+-;'<8;$6C/013-9'H6!13BC-;K6FE-.C?,+a8;$6C/013-9~S<h^-?._121'H=?,+~.
8;$6C/013-9'H6!13BC-;&3Fw)$+-G13-.!;$F-;'V+04`8l6C)$+-'A&('G'H8.C?@-CI. XH`2C2_ S
x{+-:13-.!;$F0&3FR._13RC6C;H&3)$+9+.C'L.C??@-'$'L)$6^.!F6;O.C?,13-~?._1213-Mj v7!> tes< v7*< S  )L-.C?,+
?._121eIG v7!> tes> v7*< ;O.!FM06C9\1Y4b?,+606`'H-'L.z8;$6C/013-9&3Fb)$+-~?@=;$;,-F`):M06C9E.&YF*IK'[6!13BC-'5&3)
='A&3FR:)$+-G)$-.C?,+-;'u8;$6C/013-9'H6!13BC-;I.!FMp;$-)$=;,F'>),+-3l!X__!XA!*o8._&3;I0JK+0&(?,+f&('?.113-M
.!F !CV2 S X(l(0fK
 &]'5.a'H-)6!WK'H=?$+)$;O._&3F0&3FR~-@}.!9:8013-'SLh^-:.C','H=9:-L)$+.!)G&2WK)$+8;$6C/013-9&('KF6C)K'H6P1YB!.!/013-L/`4f)$+-D),-.C?$+-;_'<8;$6C/013-9'[6!13BC-;I0&3)K6C=),8=)O')$+-8._&3;L3!!Xz_GOS
|HM0-._12134CIU),+-R6`._1U6!WK'H8--M0=81Y-.P;$F0&3FR~&('G)$6pFM^.f)O.P;$RC-)K8;$6/01Y-9'H6!13BC-;G&3F^)$+-513-.!;$F-;_'
+04`8l6C)$+-'A&('G'H8.C?@-CS>V6!J-B-;I0)$+0&('<&('KF6C)K._13JK._4'<8l6`'$'A&3/013-L/-?.!='H-56C=;K9:6M0-@1U6!Wy1Y-.P;$F0&3FRa;$-@12&Y-'
6CFz;O.!FM069L134?,+6`'H-F~),;O._&3F0&3FR~-@}.!9:8013-'SDV-F?@-CI
J-\.1136!J)$+-513-.!;$F0&3FR~._13RC6C;[&Y),+9)$6E6=)$8=)
.!F^.!88;$6_}0&39E.!)$-@134~?@6C;$;,-?@)8;$6C/013-9'H6!13BC-;J&3)$+^.p+0&3RC+^8;$6C/.P/0&12&3)H4^._W),-;ff'H--@&3FR~.f;$-.'H6CF.!/013F0=9/l-;:6!WK-@}.!9:8013-'Skx{+-a8;$6C/013-9'H6!13BC-;LF--M'L6F01Y4n)$6z/-a.!88;$6_}0&39E.!)$-@134?@6C;$;$-?@)5&3F)$+'H-F'H-5)$+.!)V&3)79E._4pW.&1o)$6:8;,6M0=?@-:.:?@6C;,;$-?@)ff'[6!13=)H&36CFwW6C;G.:8;$6/01Y-9J&3)$+z.E'H9f._1218;$6/.!/0&21&3)H4
-BC-F^),+6C=RC+^)$+-p)$-.C?,+-;'H=??@--M'&3F'[6!13BC&3FRw&3)Sfh^-E.!;$-pF6J;,-.CM04z)$6aWX6C;$9E.1134^M0-@F-f6C=;
9:6M0-@1U6!WN13-.!;$F0&3FRS


fi 





	ff
fi		ff	

ff"!$#&%'#($!*),+.-/021ff34658789;:=<>5&?@/?BAC6CEDFGAH0ICE/46-J5K-L1/02134E5K789L:*MN34O/P:@C'7Q/SRTD3:@/58-U 58-/
9;VWAX3ff789;CY?Y5&?Z?[AX/\EC^]P_`5 MaMN3ff4b/-JV@D3:@/58-dc>ePUf_`/-V@\Y9L35K\ECg3hMfi/iAj4T3;kY0ICl:mDff5&?Y7Q465QkYFL7Q583-onO_/-JD
/-JVp7Q/4Q1Cl7qAj4T3;kG0rCl:=?Y30Is'Cl4"teu]P_
vw <x7Q/SyCG?g/S?z58-SAjFL7^789;CO?BAC6\l5 {^\6/ff7Q5K3-|3TMz/D3:@/58-}c~e|Uf_:z/58:zFL:Aj4T3;kY0IC':>?Y5&YCb_"/ffC'4E43ff4AX/4T/:@C'7QCl4bE_^/-D/\63-G{"DCl-\EC^A/4T/:@C'7QC'4ff
;w <:@/V\6/080`a^ZRB.^"a|_"a9;58\Y94Cl7QF;4E-?CY/ff:Aq0rCG?oB6tqW`MN34@cu_fia9;Cl4TCp
5&?p\Y9L3S?NCl-`58789PAj4T3;kW/;kG5K0I587QVpnOq^MY43ff:789;COAj4T3;kG0IC':?YC'7.789;CP-JF;:pkWC'4@3TMo34T/\l0ICP\6/080?
3TMo<f:@F?Y7zkWC@A30IV-J3:@5K/ff05K-789;C:@/SS5K:@FL:Aj4T3;kG0IC':f?Y5&YCo_P _dff_z/ff-D789;C0IC'-L17893TM
5K78?g58-SAjFL785K78?@46FL--J5K-L17Q5K:@Cp:@Fff?N7kWCZAX30IVS-3ff:z58/0$58-/080a789;CZAj4TC'sl5K3ffF?AX/4T/:@C'7QCl4?@/-DP/ffFYAAC'4@k3FL-DO.3-789;Cp4EF;-J-58-;17Q5K:@CY?g3hMAj4T361ff4T/:O?g5K-c3-58-SAjFL78?g3TM.?Y5&YCb
/-D/080qAq43Lk/;kG5K0I587QV}D5?Y7Q465QkYFL7Q5K3ff-?znf3slC'4@  _.`5K789Aj4T3;kW/;kG5K0I587QV}/7.0ICE/S?N7
;w MN34/0K0aceU
6`PW_j<3ffF;7AjFL78?g/ZAj4T3E14T/:tJ789;/7/EAAj4T3SS5K:@/7QCG?"]58-}789;Ci?NCl-?YCb789L/7aSnO"E_
a9;C'4TC.a2t  Tq,
 tb/-Dpt
 Z/-JD
w 789;C'4TC}5?P/@{SC6DA30IVS-3:@58/0?NFL\Y9789L/78_MY34/:@/SS5K:@FL:~Aj4T3;kY0ICl:?Y5NCp_z:@/SS5K:@FL:
?N30IFL7Q5K3ff-}0IC'-L1789g^_b  _b _/-D789;C@FYAAC'4gkW3FL-D..3-|789;CAj4T3614T/:O?.5K-cP_`5 M<x3FL7AjFL78?bt  _
789;C@46FL-7Q58:zC@3TMbtJ5?zkW3F;-JDCEDdkGVzOaEE      w
YSWHYL&;SPTLKJ'LKESpY^q*S&@I}
Q6SpY^SfiaLYlIIz6l'LELr&pSfiZS6SESSSLYrI@6'ffL6a
&'SL&l&T&KgQET&S"Wp;Lff'[K@TEo@gpW'YZKiQYp
YY6TS&T&`@SLgpYqaBpTKELKS'L^'WY`olgJbZ;S@.T'S6W
ff6S}$^L&bW'L&YpYLS$^ESYqL&YTS&Y$WSqKMYFL-\'7Q583-J/0K0IV'L&YL
@.'ffY'WL&YTS&YELKT6&|Z&'SY';LW'[KTELTphff
Y'L'}hgS^&'SYbYdSW'L&oSZ.6YWL&YhS&YbSL&ESTI&dW&;TE&;
6Sff&@&PpgpZ6l&L&ESL'G`B^LKJ.&pL[&L&zWLLKI.ESY
L&YTS&Y^&6hEr"LL'^E;KT&}Z&'SYlLL'ZTE
Tgpi8&'SL&PI&Y6SWJ&'SL&}@TZohYE'[QL`LYJYLY;@S^
WSKESg6L&ffKThhffn@WL&YTS&Y}t  ffoLW}&'SWYPWlK
l;&oSWP}6YpffLYTS&YlrgWYLEWS}TS&T&^&
LrI&Tu&'OS.Y`Y'T'WpL&Yo@SL'[&n@S&EP
6L&@L&Yo^SW"ESL@&oW;TY$ffYpTpYT&p' ol@JY'TYL6ShffX@
&'SY.ol r`@&'SWHSlL&oSE&}EW'EbffLYhS&Y'
 YEq"OL&P'ffLW
STWL&Y hS&Y^K^&'SW'}S&'TZ^&}@ffSLIr&T^;
&lI&qYZKZ.W'L&YpYL.S.&'S'uL&YTS&Yzh@ dW&YL' 
YSWgLKKL'Lp&[KTT&P^&fiTffLuP&|T&pgJS&;&}SST&S6SYY6Y
&W&ffL&Y=[	 Y;hS&T&p&YW$S&;ffY6T'SjWYgWErKSLIIKTS6SpYWY6`
SL;SZY^`ffoT&pbY'L'QEX'ET&pWfiLl&oJY6Sff6YL
KT6SpYYBEff6bW.T&p.QE'ET&p&ff&ffL"JY6SEW=.ffLY
TS&ff&T&p^[&E}WLT&pKpTffpYL&W'L& l&ffT& YSPE'E'
&pSL'YTg.ffY6SW6^S.T'@gff BPY"ff6Y^.'ffL&.L&
SZ@LzJYgSffY6SWbE'ET&fiiS&L@Kl`&SST&.6SpYYEYYffYW
.T&piff"E'ET&}p[&S&OY6^oloJzSWLW6SpZffL'q
bTY'L}ffL&Y';^&'SY'WL&YhS&Y`K^&'T'E^@gLEBSL&^Y
Q6E}WL&YTS&Y'LKK}ffLllIKSL&W}&'SWYPEQ}W}&'SWL
YGS-3ff7WW'T'E.@L&YfTS&Y6&O&'SY'ZLL'OTE.I


ff
fi

fiffff!"#
$"%&'(*),+.-	&/102)-	3,&(4065	78$"%&9%;:.'),$*%&ff0<5=002'>,?&)A@B$*%&-	&ff>("7&(>(*&C02D'').02&ffEF$*)G+&H&IJ?*5	&7.$ffK0<5	7?&
L &>("&),7.-	:F/F&ff>,0MD(25	7NO&IJ?*5	&7?:8+.:P?);>A(402&0"?>Q-	&ff0R0MD?"%8>0S("D77.5	7NG5	7F'T)-	:;7)/U5=>Q-V$25	/F&,WYXZ0 L &
0">Q5=E[&ff>(2-\5	&(ffK L &C>(*&7),$?),7?&("7&ffE L 5	$"%]/F),("&(*&^7&ffE]7),$25	),70_)@`&IJ?*5	&7?:,K02D?*%J>0S5	/F'(")35a7N
$"%&O$25	/F&U?),/F'.-	&b.5a$2:c)@d'("),+.-	&/e02)-	35a7N[@f("),/hgUikjTlmd$")8gnikjoQm4K5	7p$"%.5=09'>'&(ffW
X9-	$"%),DN,% L &c$"("&ff>$"&ffEq>r'("),+.-	&/s02)-	3,&(8>0F0<5	/F'.-	:t>uE.&$*&("/U5	7.5=02$25=?v'("),N(4>/w$"%>$F/]>'0
'("),+.-	&/80x$")F02)-	D$25	),70K$2:.'.5=?>Q-\-a:]5	$C?)70<5=02$40)@S$ L )U?),/F')7&7;$40zy6>[E.&ff?*-{>A(4>$25	3,&O("&'(*&ff02&7;$>$25	),7
)@C02),/[&8|5	7Eu)@C?),7.$"(")A-|.7) L -a&ffE.N&pik>J@}D7?$M5a)7mZ$*%>$P02'T&ff?*5\^&ff0 L %.5=?"%q)'&(4>$*),(G),(~),'&(>$"),(
02&ffD&7?&8$*)J>''.-	:5	7>]NA5a3&702$4>$*&,KV>7E>75a7.$"&("'(*&$"&(G$"%>A$OD02&ff0H$"%&8?),7.$"(")A-6|.7) L -	&ffE.N,&F$")
02)-	3,&U>7.:8'(")+.-a&/5	7p$25	/F&~')-	:.7),/U5=>Q-S5	7c5	$40H0<5	&,W5	7?&U$"%&O5	7.$"&("'("&$*&(5=0D02D>--	:^b&ffEKT$"%&
%.:;'T),$"%&ff0<5=0O02'>,?&U)@'("),+.-	&/02)-	3,&(0CE5	("&ff?$2-	:u?),("(*&ff02'),7E0H$*)J>]%.:;'T),$"%&ff0<5=0O02'>?&P)A@R');0"0k5a+.-	&
?),7.$"(")-|;7) L -	&ffE.N,&,WSXO0"02D/U5	7NF$*%>$R$"%&(*&5=0>7]&IJ?*5	&7;$$4>(*N,&$R'(")+.-a&/h02)A-a3&(R5	7c$"%&H%.:;'T),$"%
&ff0<5=0U02'>,?&])@'("),+.-	&/02)-	3,&(40Z5	/F'.-\5a&ff0[$"%>$~$"%&("&U5=0U>$4>("N&$H@}D7?$25	),75	7$*%&J?),(*("&ff02')7E5a7N
%.:;'T),$"%&ff0<5=0F02'>?&8)@C?),7.$"(")A-R|;7) L -	&ffE.N,&,W,'&&ffE.D'-a&ff>A("7.5	7Nu)@C>c%.:;'T),$"%&ff0<5=0F0M'>,?&8)A@x$>("N,&$
'("),+.-	&/0M)-	3,&(40F?>7q+T&p>,?*%.5a&3&ffEq+.:TXOR-	&ff>(*7.5a7N)@Z$*%&J?),(*("&ff02')7E5a7N%.:;'T),$"%&ff0<5=0]02'>,?&
)@?),7.$"(")A-6|.7) L -	&ffE.N,&,WG9) L &3,&(QK L &PE.)c%>Q3,&F>7v>,EE5	$25	),7>Q-'("),+.-	&/e)@?),7.3,&("$25	7Nc'(")+.-a&/[
02)-	D$25	),7'>Q5	(40U)@H$"%&c$4>("N&$P'(*),+.-	&/02)-	3,&(F$*)u&b>A/F'.-	&ff0U)@O$"%&c$4>("N&$P?),7.$"(")-|.7) L -	&ffE.N,&,W
 &~$4>|,&O>,E.3>A7;$4>AN,&O)@d$"%&PE.),/]>Q5	702'T&ff?*5^V?>A$25	),7ikE.&^7.5	$25	),7v)@6N);>Q-=0>7E),'&(4>A$"),(40mS5	7E.)A5a7N
$"%.5=0H?),7;3&(40<5	),7`W&7?&F02'T&&ffE.D'p-	&ff>(*7.5a7N[5	7v),D(@f(4>/F& L ),("|]?),70<5=02$409)@d$ L )F02$"&'0zy5	(402$QK$"%&
'("),+.-	&/F0M)-	D$25	),7p'>Q5	(40x)@$"%&O$4>A("N,&$R'(*),+.-	&/102)-	3,&(C0M%),D.-=E]+&G?),7.3,&("$"&ffE]$*)P&b>/F'.-	&ff0)A@$"%&
$4>(*N,&$G?),7.$"(")-|;7) L -	&ffE.N,&]D0<5	7Nv$"%&JE.)/8>Q5	70M'&ff?*5\^V?>$25	),7`W,&ff?),7EKd$"%&F&b>/F'.-	&ff0U)A@x$>("N,&$
?),7.$"(")-|;7) L -	&ffE.N,&H/UD02$+&HN,&7&(4>Q-\5	&ffEcD0k5a7N]02),/F&9@}D7?$M5a)7]-a&ff>A("7.5	7N80"?*%&/F&,K>7EF$*%&O("&ff02D.-	$
/UD02$+T&O'.-aDNN,&ffE5a7.$")F$*%&H5a7.$"&("'(*&$"&(C$*)n?(*&ff>$"&G>7p>''(*)ffb.5	/8>$"&O'(*),+.-	&/02)A-a3&(ffW
),(0k5a/['.-5=?*5	$2:p)@S&b'T);0<5	$25	),7`KT$"%.5=0_@}(>/F& L ),("|F>,0"0MD/F&ff0_$"%>$$"%&O/8>Qb.5	/UD/'(*),+.-	&/10<5	&Pj
5=0N5	3,&7`W),(>UNA5a3&7c'("),+.-	&/E5=02$*(25	+D$25	),7`K$*%.5{09?>7p>Q-=02)F+T&G&ff>,0<5\-	:c&ff02$25	/8>A$"&ffE[@}(*),/&b>/['.-a&ff0
+.:c$"%&P02$>7E>(4E'(")?&ffE.D(*&P)A@Y02$>("$25	7N L 5	$"%0<5	&cU>7E]5	$"&(4>A$25	3,&-	:E.),D+.-\5	7Nc5	$O>7Ec3,&(M5@f:5a7N
5	$ L 5	$"%v>F02D.IJ?*5	&7.$2-	:]-{>A("N,&P0M&$)@(4>A7E.),/U-	:cN,&7&(4>$*&ffE]'("),+.-	&/80ikC>A$4>(4>AM>7`KQ,,.m4W
]`	ff{`V2;zQQffdv{ ?)70<5=02$"&7.$C  nUf2,;H,9V\H ,ffffQ4~	fff
 fk j ,Gp;[fk=T=.U,9V\Z i TfQ m  fk< j
5	/U5\-=>($*)P/]>7;:]XR-	&ff>("7.5	7N8>Q-	N,),(M5a$*%/80K.$"%&G02'T&&ffE.D']-a&ff>A("7.5	7NJ>Q-	N,)(25	$"%/80 L &Z?),70k5{E.&(
)L ,("|v+.:&Ic?*5a&7.$2-	:^-a$*&(25	7N$*%&8%.:;'T),$"%&ff0<5=0U02'>,?&[@f),(P>'("),+.-	&/02)-	3,&( L %.5{?*%5=0U?),70<5=02$"&7.$
L 5	$"%F$*%&$"(4>Q5	7.5	7NF0">/F'.-	&,WR_&@f),("& L &'(")3,&$*%&),("&/80d>+T),D$R'>A("$25=?D.-=>(%;:.'),$*%&ff02&ff0R02'>?&ff0K L &
^(402$G0M$4>$"&U>]N,&7&(4>Q-R$"%&)("&/ L %.5=?"%r5{0~>cE5	("&ff?$P?),70M&ff,D&7?&])@$*%&P(*&ff02D.-	$40H5	7TXOR-	&ff>(*7.5a7N
)@S^7.5	$"&G%.:;'T),$"%&ff0<5=0H02'>,?&ff0Hi2d-	D/F&(QKVY%("&7.@}&D?*%.$ffKH>D0"0<-	&(ffK  >("/UD$"%`Kff,.m4W
U .VQ`Q "UfffAk,,U,F,Tp"u;*,;{8<,pMU,\,U,kfU
V2;zQQff*,U,P=8=]`ffR. i  m *H,\P,\p",.4= j q;ffu
,,4;	G[4,zr\,;F ,=;, ]c6 G
 ~fzG;~4Q 4,f,2Chcw j dR,FG=AV.4
 xf\U,=O;O,Q,G,]Q2,f,*P8
 ~4,\4QP  i . i  m,-	7C9u-	7  m f2,;Fz,9V\z4
 ~fffPfO=f=U,\,U,= j C  C R,TF=;~{C2O;U,,U,4Q 4f=,
,];Uf2,=T=.]z4R
 ~,;;UOV2;z\ff,\QffH=]  ;p{G,	fffT  p;Ff2,;n


fi	ff
fi
fiff
ff
!
"#$"%

&('*),+-/.1032546472)98;:<)<=?>@=BA'=C.@+EDF8.@+=;2>@=;GH.I2J.K)<2!L-5>@.KDFMBNE4F-5>O+EPQLR2.@+=8(DF8O8KL-MB=8TS8K=;.U8V250
BM 2'E.@>@254W>@NE47=8X-5'YCGZ-MB>I2[32L=;>B-5.@2>U8T\,]=^D$YE=;'E.KD603P_8`NEabMID7=;'Q.VMB2'YD7.KD72'89.@2cN-5>U-5'E.@=;=V8KL=;=YENL
47=-5>@'ED7'cdDe'C=-MI+?250f.I+=8K=X.K)<2V+QPEL2.I+=8(DF8g8KL-MB=8;\
hji1k9lmonprqsptvuwpyx/nwzgn{zslR|
} '=~),-P.I2NED64FYO=BaOMIDe=;'E.,L>@2E4e=;G8K2547=;>U8DF8<EPV47=-5>@'ED7'cZMB2'E.@>@254o>@NE47=8,-8D7'!,D7.UMI+=B464:
.@c25:<-5'=;>3(Ds:Ej2><D7'9XV;VD7'E.@2':oU\<2'E.@>@2/4>@NE47=8>@=YENMB=18K=-5>UMI+OQP
8K=B47=MB.KD7'c:>I=K=MB.KD7'cV2>r2>UYE=;>KD7'cV2L=;>B-5.@2>U8r-5LL>@2L>`D$-/.@=B47P\j&s'.@+EDF8,8K=MB.KD72')<=gMB2'8(DFYE=;><47=-5>I'EDe'c
250MB2'E.@>@254>@NE47=8.@+-5.,8K=B47=MB.-/LL>@2L>KDF-5.@=~2L=;>U-5.I2>U8<.@21-5LLE47PD7'-c5D7=;'8K.B-5.@=\
f35oQoRg5y$`77E	


MB2'E.@>@254W>@NE47=~DF8~-VL-D7>1@^UUU:),+=;>@=O~YE=8IMB>KD7=89.@+=V8K=;.g2/0L>@2E47=;G8`.U-5.@=8<2'?),+EDFMI+
.@+EDF8>@NE47=8K=B47=MB.U8.@+=~2LR=;>U-5.@2>,E\,^jDF8,M;-4647=Y?.I+=VB(2/0E\
]=?-8@8KNG1=O.I+-5.V.@+=?8K=B47=MB.@[s8K=;.U8d250~2L=;>B-5.@2>U8250XYE2GZ-De'8De'v
-5>@=OYE=8@MB>KD7=YD7'8`2G1=
4F-5'cN-5c=bd\Z]=ZMB2'8(DFYE=;>Z-+QPEL2.I+=8(DF88`L-MB=Z2/0L>I2E47=;G8K2547=;>U8T:),+=;>@==;=;>@P!L>@2E4e=;G
8K2547=;>~MB2'8(DF8K.B8,250-8K=;.,2508K=B47=MB.@[s8`=;.U8<D7'1:ff2'=~032>9=-M@+C2L=;>B-5.@2><D7'C.@+=YE2G-D7'\,o=;.X<1R=
.@+=X8K=B47=MB.@[s8`=;.U8,>I=8K.@>KDFMB.@=Y.@2VL>@2E47=;GZ82508De;=*W\
+=1+QPEL2.@+=8(DF8V8KL-MB=1N8K=8-A=Y.I2.U-4j2>UYE=;>KD7'c!25=;>~.@+=12L=;>U-5.I2>U89250,.@+=ZYE2GZ-D7'\
<+EDF8<2>BYE=;>KD7'cXDF8<N8`=Y.@2X>@=8`2547=XMB2'EDFMB.U8<R=;.)<=;=;'O-5LLE46DFM;-5E47=2L=;>U-5.I2>U8j)+=;'OG12>@=9.@+-5'2'=
8K=B47=MB.@[s8K=;.<MB2'E.U-D7'8j.@+=c5D7=;'1L>@2E47=;G?\W&('1),+-5.W032546472)98;:)<D7.@+2N.j472Q8@8W250oc=;'=;>U-46D7.KP:E)=,-8I8KNG1=
.@+-5.<.@+=~2L=;>U-5.I2>U8<-5>@=~'ENGV=;>@=YN8(D7'cZ.@+EDF8<2>UYE=;>KD7'c\<De=;'-L>@2E47=;G-5'YO-18K=;.,2/0MB2'E.@>@254
>@NE47=8;:-L>@2E47=;GH8K2547=;>D7'JLEDFM@8.@+=147=-8`.X'ENGR=;>@=Y2LR=;>U-5.@2>~)+2Q8K=O8K=B47=MB.@[s8K=;.1MB2'E.U-D7'8
.@+=1L>I2E47=;G?:j-5'YJ-/LLE4D7=8^D7.\O<+EDF8^D$8>@=;LR=-5.@=Y!N'E.KD64.I+=ZL>@2E4e=;GDF88`2547=Y2>~'28K=B47=MB.@[s8K=;.
MB2'E.U-D7'8.@+=MBN>@>I=;'Q.,L>I2E47=;G?:D7'!),+EDFMI+M;-8K=:R.@+=~L>@2E4e=;G8`2547=;><0s-D64$8K8K=;=XyD7cN>@=Z/U\&0.@+=
G1=;GV=;>B8K+ED7L?D7'C.@+=X8K=B47=MB.@[s8K=;.U89M;-5'OR=MI+=M@=YbD7'?L2/4ePE'2GVDF-4j.KD7G1=:R.@+=;'O.@+EDF8L>@2E47=;G8`2547=;>
>@N'8rD7'C.KD7G1=XLR2547PQ'2GD$-4fD7'?5-5>KD72N8L-5>U-5Gd=;.@=;>U8;\
25)X:E)<=X-5>@=9>@=-YEP.@2Z8K.U-/.@=-/'YOL>@25=9.@+=~GZ-D7'C.@+=;2>@=;G2/0.@+EDF8,8`=MB.KD72'\+=X8K.U-5.@=;Gd=;'Q.
-5'YCL>@2E2502/0.@+EDF89.@+=;2>@=;GM;-5'!R=YE=;>`De=YC0>I2GL>I=;D72N8^>@=8KNE47.U892'C47=-5>@'ED7'c8`=;.U8,)De.I+2'=;[
8(DFYE=YO=;>I>@2>~9-5.U-/>U-5@-5':EU\y]=~L>@25=9D7.<03>@2G.I+=9A>U8K.,L>KD7'MID7LE47=89032>,MB2G1LE47=;.@=;'=8@8T\
	=;.?YE=;'2.@=J-8K=;.O25018K=;'E.@=;'MB=8;:9=-MI+2/0),+EDFMI+>@=;L>@=8K=;'E.U8O-8K=;.O2/0L>@2E47=;GZ8bD7'.@+=
YE2GZ-De'\<+=;>@=VDF8-'-5.@N>U-4L-/>@.KDF-42>UYE=;>`De'cC25=;>~.@+=V=B47=;G1=;'Q.B8X2/0YE=BA'=YQP!.@+=!BG12>@=
8KLR=MIDAMd.@+-5'>I=B4$-/.KD72'\  8K=;'E.@=;'MB=VDF8dd(B 1Q?-5'2.@+=;>9D60,.@+=18K=;.~>@=;L>I=8K=;'Q.I=Y!QP
.@+=^A>U8K.g8K=;'E.@=;'MB=^D$89-18KN8`=;.,250j.@+-5.,>@=;L>I=8K=;'Q.I=YOEPZ.@+=V8K=MB2'YC8K=;'Q.I=;'MB=\]=XYE=BA'=1-OV5;
(B ZRK67;3KX250X-J8`=;.?250~L>@2E47=;GZ8dD7'.@2JR=-8K=;'E.@=;'MB=CDe'),+EDFMI+
>@=;L>@=8`=;'Q.U8.@+=~G12Q8K.98KL=MID6AMV8KNL=;>B8K=;.g2/0\

fiff 

  	 	
!
 ff ("	 
+
 $
. 0/ /

  
 


fiff ) 
 	 #
(
	








	
 ff  ff  	   

' "
*$  &%
*, 
	
- "

V3/ V
5BTU;;VCsU T 6UURFE? UQFQ IQ;;7
 o
 E
(B,  <K T; @ I5 1C;6Bs; ; 
  V
!
Q51VK T6  XO Q<; TX ;  ;6K ;6
 5Vo3FV
9 X;W rK ;6X~ jQKX$g E EVV5W(B <$;3FZZ< 
FrU IXB E3 Od 5V3V
 @@BQ _!QXF<B ITQB  FC3FV9 RVFO 
4e2c   o7X 5VZ

1324

fi57698:<;*69==>@?BAC6EDE69F969GH69I
Jhji KJRLNQ)MEO9kPRlNQSm KTOVU)W9XTYHZ[W'\ ]fiZ_^<\ `*]fiaEZ[W9b<\ `Ecd]fefiW9\ gT`EZ
n Oo hji
xwprn y qsuOh[zo Othjivw{}|m ~Ef'l7}PNL
#[H9wfi9<}fi#[fi*9}l3@dH~Tm _T9Hm
lVpqqss~lSTm79p7~Tfm
i P#
O
LNO iQ7PkJRU)Q)W'kXp}YHZ&m W9\ ]fiZ^\ `*]fiaEZ[W'b\ `Ecd]fefiW9\ gT`EZ
 9H$q}9H*fiE*Efi<
HENH
 KTL7L9<*w[(H99HEfEE qd*9H*TVN9E9
HT9d<$fi9fiC@fi"@RH)9H<9*H*9H*H*TdE9EE9fiHdHTH
Efi[EfiH97dH@H[E$$*E9dfi9dH*EHH_*fi*H<E9}}H9w[HV"EH*99E*H<E[HTdEER9
7@f}<H99 
T99*9fi[TU7W9$X_YZ[W9fiH\ ]fi*Z_^H\ `*]fiEaE$Z[W9b9\ `Ecd]fejW'\ g`E@Z79EH9 *&9H$0fi*THN@@HE$wHE9990HTfi
H_H*9RH*[EH9*
fH99HfiTE09 9HET[@HH9*T[*)*T[Cw9fi*9
EHS*9wT(fiH9[E9<<CE<7*9fiH*$*T$EdH9 H9*<H*rH9<<<EH9
H9*E$*
Efi9
H[TH@@fiE$T@99uEHE9}39<fiH9H9$
U7fiW9@XfiYHZ&W9\ ]fiZ)^\ `*]fi\*`9E*ZXT`EZfi w<9EH$fi*#fiE
HE[Tfi<fiHH99*9*$T9N`EX`E9Zf\ *`*9fi<<*fiH H@E99HTEfif@H9dfiH_fi*C$
E99<"*9EHfi['fiE[E$9fid9*fiEU7<W99X*YfZ[W9\ ]fiZ_^\ `*]fi\ `9 HZ_<X`EZ*@VHV@9*TEHf$ETE~T-9
*H$fi<3[EENH'0fi9V9H99HH*H<9fi
* 
EN0`EX`E9Zf99\ H*`d*VH9ffifi*[H|#~Tf[99[HVfi9H99*EEH99HTE
9H0H9_fi*fid9~T<*(9**T
 9w[ *wE9




9

E



fi

9

E



H



E





9



9

H

9







9

H



)
U
9
W
T
X
H
Y
[
Z
'
W
\
fi
]
_
Z
<
^
\
*
`
fi
]
\

`


_
Z

X
E
`

Z



TU7W9XYHZ&W99\ ]fiZ^\ f`*]fi}\ `EZX`EZ9T99**H9H<*TfiE *<#[H9H*[fifiHE**}$wVH 9HTE@9fi9HEEr$HE 


9

d

9

H








[


















fi











H

9





*

9







E

}

H

9

d

H







9

*







fi

E



9

fi









E







T







9



9

H

<





*



*


f_C[EH$fiH[TT9HH99H<f*EEfiE9Hfi9##H*H*$9H<[E'dfi099fi99H$*HE@dH9$E"EEHfiEfiEH*$*H[HHHE9 
3

fi	
fiff

fi

 " !	#%$&(')$+*-,.)/102,3 45076fi3 89453 8":;07."8%0
<>= ('-?A@9BDC)BDEGFIHKJLBDMANOBDP BDQ
R $TS <>=
UV9W MXFZY[)\K]_^ Fa`fiB"b"b)b1BDcedTf
<>=	< ? <gh2<i $kjH5[lmNDB)b"b"bnB9jH5[omN W1p Ydqf
rfisut $#%? < ! =avwyx V9zV9{D|%W1V}VO~|%TV"pfi{VOV"OWn75V9WD sr
)$ $ g ? l  KH c pfi ]] zyz l N? < $
R $TS >< =
# gh2h U%y7  U	W1ppfiWD|)z KH 	BO NDf
> kFa
 
?%$ =
UV9W Fm["")BD[)9%B"b)b"b5BD[)"O
jHK[)"ONFXjHK[)"mN	Yd
jHK[)99NFjHK[)9)N	Y[)"9H5N1d
b"b"b
jHK[ " NFjHK[ " N	Y[    HDb)b"b5HK[ " HKNnN-b"b)b N1dqf
$ = &
rfisut $#%? < ! =I-wy pz5W1{1OW|%{np"~T|%W5pzp%(VOV"OWn75V9WD sr
 !-^Fa`?%)!	'S	c&!
 HK[ \ NF	8%."8%0:fi3 98%H1jHK[ \ N1NDf
!'-?"('? W1V}{1pfiTV95p%V9{p%({nV `} W1WnVyV"|;{1zV" HK[fiND 
$ = &*-,."/10,3 45063 89453 8":%07.)8%0
{1VA   z|)p{5W1Gp{yOpzqWn{1p%-{nTVV"|%{1zTz
 p eV9{D|%W1pfi{|%T29|%W5pzq+W1VWnV"|1V9{)  pz2TV9{| H 5 N {1pTV9  Tn+2}5p%V"+q+W1V
5V"fiV9zOV F9[)""BO[)9"B"bbbBD[)"5 TW1VWnV"|1V9{)VLV"|;{1zTz|)p{5W1znTV"  zW1V
5V9W jHK[ " N V9zW1VV"|;{1zTz|fip{5W1V9zV9{D|9V"W1T25V9W"WnVAV"|%{1zV"5VOV"OW17V9Wp% [ "
znTV"    zOV|{1pTV95p;fiV9{DzW1VqTpWnV"2}5|OV}znfizW1VWD|;{1V9W{1pfiTV9
5p%V9{|)  |)A5VW1VV"|5WzTAV9{1V"pV9{D|;W1p{  pT5V5VOV"OW175V9WOpzTWD|)zAW1V+{1pTV9W
p%p  W1|;WW1VWD|%{1fiV9W5VOV"OW175V9WDAp;pfiV9{D|%Wnp{D [%l)B"b)b"b1BO[""nl TpzpWAOpzqWO|)z  +p{1V9p%V9{)
zOVW1VV"|;{1zV9{yz}W1VApT5W5V"n-LV9zV9{D|)"|%W5pzp%W1V5V9W}p;VO~|%TV"9	WnVV"|%{nzV"
5VOV"OW175V9WOp%}peV9{D|%W1p{O [%l"B)b"b"bnBD[)"nl A5WLV5V9WDLp%WnVOp{1{1V"5epzfizWD|%{1fiV9WVOV"OWn
5V9WDmq|%zAV9zOV}TpzpfiWOpzTWD|)z  V9zOV [)" W1VV"|fi5WzTAV9{1V"LpV9{O|%W1p{  pT5V5VOV"OW175V9W
OpzTWD|)z  |;z  eVVOV"OWnV"kqW1VAV"|%{nzV"{1pfiTV95p;fiV9{uWnp5p%V    zOV [ " HKN 
5p%V"  Wn|uV"V9zOVp%	V9zW1V"nW1|%z  qWnV}W1V"|nV9{"TqLzTOW5VTqepW1V"29W  
V5p%V"  W1W1V1|%V5V"fiV9zOVqW1VAV"|%{nzV"{1pfiTV95p;fiV9{"V9zOV  V5p%V"
z  qW1VyV"|%{1zV"{1pTV95p%V9{"
"

fifi9""   1

 
	fiffff	
 	  !!" 	 !#$ 	 %
& 	 !'(!!" 	 !#)' 	 %
*+ 	 !,-%#!".%#! 	 !#$ 	0/ 1$!! 	 !23
4 	 657895;:!<>=#?#@$'A
B
	fiCDFEG+HIKJMLC!
N 	 JMLC+CODPEfi
Q DPRST>U&VXWZYI[A\]FUfiLA^_DPEY>U`RTI[;YaDFLbE7Lc+U`TI[AY>LTIC
d UeEL;fZC#gL;fhY>g+[;YY>gUCi[;jkc]PUCODPl`UGDPEKLSTm[n]PRLTaDFYigjoDpCmC#SqrJiDPU`EYm^sLTt]PUu[;T>EDPERr[AEr[;ccTiLuvw
DPjx[;Y>U$cT>Lb\]FU`jyC#L;]PzU`Tu Q LTGcT>L\]PU`jxC{L;^|CODPl`Ux@}LT~]PUuC>C`Uu[bJ>g(C#U`Yk{bJ`[;E \UxJ>gLC#U`EDFEh 5 
fm[nCDFEUuJMY#DPLE L;^GY>gUr[3]FRbLT#DPY>gjK;DFE+JMUY>gU`T>UK[;TiUrff}LcU`TI[;Y>LbTIC`0Y>gU7ESj\U`TkL;^bDpC#YaDFE+JMY
C#UM]PUuJMY>wC#U`Y~Y>Sc]PUuC`+[;E+"gU`E+JMUfiY>gUGESj$\+U`T~L;^_bDCaY#DPE+JMYecT>Lb\]FU`jC#LA]FzbU`TICtY>g+[;YmJ`[;E\+UJMLE+C#YiT>S+JMY>Uu
DPEUuJMYaDFLbErDC7 5 7mU`E+JMU7 5 ff|]FLbR 5 x~U`E+JMUx\(gU`LT>U`jkY>gUC>[;jkc]PU7CODPl`U
R;DPzU`EDFEY>gU[n]PRLTaDFYigjoDpCeC#Sq7JiDFU`EY~^LTt]PUu[;T>EDPER+
;DPE+JMUG]PLR 5 ADpCc+LA]FELj$Dp[n]!DPE@0bD^_jkU`j$\+U`TMC#gDPc7DPE7Y>gUfiC#U`YICXDPEK 5 [;E+"Y>gUfijkLC#YC#c+UuJiD%J
RU`EU`TI[3]9DPlu[;YaDFLbE+CL;^YigUKC#U`YIC"L;^UMv+[;jkc]PUuCJ`[;E\LY>g.\UJMLjkcSYiUu}DPEhc+L;]PELj$Dp[n]fiY#DPjkUY>gU`E
!u> I `Inu;n; T>SE+CtDPE7c+L;]PELj$Dp[n]_YaDFj"U[C-fUM]]|~U`E+JMU\xgU`LT>U`jbDPYDpC[$C#c+U`UuSc
]PUu[;T>EDPER7[n]PRLT#DPY>gj^LbT| DPEK
 LY>U~Y>g+[;Y
YigUe[;\+L;zU~Y>gU`LT>U`jJ`[;Ex[3]CaL\UCaYI[;Y>Uu$S+CDFER$Y>gUGLEw]DPEUfij$DpC#YI[;bU`ws\+LSE+"jkLUM]
DPE.fmgDpJ>g.YigUk]PUu[;T>EU`T)DPE+JMT>U`jkU`EYI[n]]P.Sc%[AY>UuCk[7gc+LY>gUuCDCfmgU`EU`zU`T$DPYkJ`[;EELYkC#L;]PzU7[EU`f
Y>TI[3DFEDPERcT>L\]PU`jDPE(Y>gUkC>[AjkU$f[nK[CGYigUkY>Uu[JigU`TLUuCDUPfmgU`EU`zbU`TfiY>gU$]PUu[;T>EU`T)jx[;UuCG[
 j$DpC#YM[;UuDPY>Y#]PUuC#Y>LbEU
ubIgDpCGbDPUM]C$[7CO]DPRgY#]P jkLT>U$RU`EU`TI[3]
T>UuC#S]PYY>g+[AEgU`LT>U`j
\+UuJ`[AS+C#U+SE+U`TGY>gU$C>[;jkUJMLE+bDPY#DPLE+CGLA^Y>gDpC~Y>gU`LT>U`jYigUESj\U`TeLA^j$DpC#YM[;UuCL;^-Y>gUG]PUu[;T>EU`T
DPE.Y>gUf|LbTIC#Y>wJ`[CaU$DpCcL;]PELbj$Dp[n]]F\+LSE+Uu^sLT$[;E}[;T>\DPY>TI[;TiJigL;DpJMU7L;^eYiTI[nDPEDPER(UMv6[Ajkc]PUuC`
DUP-ELbYEUuJMUuCiC>[;T#D]PRbU`EU`TI[;Y>Uu}S+CODPER[vUu(cTiL\+[;\D]DFY#bDpC#Y>TaDF\SYaDFLbEgUxj$DpC#YM[;U`ws\+LbSE+
[n]PRLTaDFYigjxC$J`[;E.\+UKJMLEzU`T>YiUu}Y>L \+[;YIJigWfi
w]PUu[;TiEDFER.[n]PRLTaDFYigjxC{DFE[C#Y>TI[nDPRgY#^sLT>fm[;TIfm[n
DPY>Ya]FUuCaY>LEUubI
_70_;uP`.`X0%p7+`%nu
d UkEL;fJMLE+CODpU`Tk[;E[;cc]DpJ`[;Y#DPLE}L;^egU`LT>U`jyY>L7Y>gUxLbjx[nDPELA^C#j$\+L;]DpJ$DPEY>U`RTI[AY#DPLE
[ C{f[bCLEU"DPEY>gU
mcT>LRbTI[;jyDPYIJigUM]9]mU`Y$[n]Pu&M d UfD]]~CagL3fgL;fY>gDpC$J`[;E(\U
UMqrJiDPU`EY#]P}DFj"c]FU`j"U`EY>UuS+CODPER[ C#Y>TI[3DFRbgY#^sLT>fm[;TM[Acc]9DpJ`[;YaDFLbEL;^gU`LT>U`j^LTk[ C#S\+C#U`Y$L;^

mCLjx[nDPE
tLE+CODpU`TeYigUJi][bC>CmLA^XC#j$\+L;]DpJ{DPEY>U`RbTI[n]pCY>g+[;YGJ`[;EK\UC#L;]PzUu\Y>gU$C#YI[;E+[;TM"DFEY>U`RTM[;Y#DPLE
LcU`TI[;Y>LTMC`
!U`YG \+U)Y>gUC#U`Y~L;^XLbjx[nDPE+Cmf~gLC#ULbc+U`TI[;YiLTIC[;T>U$UuC>JMT#DP\+Uu\7T>S]PUuCeC#S+Jig[bCtDPE
Q DPRST>U$&%[AE+7fmgLCaUcT>L\]PU`jxCfiJ`[AEK\+U$UuC>JMT#DP\+Uu\K[;ESE+[;j$\DPRSLS+CfiJMLEY>UMvY^TiU`URTI[Ajkjx[;T
 C#S+J>gK[bCmC#gL;fmEDPE Q DPRST>Ufi*
U`Y.\+Uk[;ECaU`EY>U`EY#Dp[n]-^sLT>jK6DUP[C#Y>T#DPER7L;^Y>U`T>j$DPE+[n]pC)[;E+z3[;T#Dp[;\]PUuC`L;^
Y>gU$RTI[Ajkjx[;T
 L;^ Q DFRbST>Ufi*xU`T#DPz3[A\]FU)^sT>LjY>gUC#YI[;TiYCaj$\+LA]fi;;;0WCaU`EY>U`EY#Dp[n]!^sLT>jU`ELbY>UuCY>gUC#U`YL;^
cT>L\]PU`jxC)U`T#DPz3[;\]PU^sT>LjS+CODPERKYigUkcT>LS+JMY#DPLE+CfiL;^  7tLE+CODpU`T[gcLY>gUuCODpC$C#c+[JMU"L;^
3`

fi6

b6(36(b+ ++

	ff
fi	ff
fi

	 
fi !" #$%!'&(	ff
fi) # !+*"	,
-fi
ff
#$%!.0/214365$798:;/<14365$798>=$# !"?/21@365 7A8>B #$%!
/21@365 7A8>DCE%FHGJIKLffNM9*O# !QPK #RTS) U ;VMW	ff
fiP
U#$EM4EEX#$!QP
#RTSYZM4[]\_^#ff-PKNM4`Aa[,-P
CFHGbI<c;FIKLHLd
ffeD

c;FfI<DgfihLjfLk lmLn;Lofpqf r
st\_uv;wyxElfz|{}uwA~Y~wOyauxb^;xbw9~yx;x\i^yxbuwA~@\_a^;wya_xb[
;wya_xb[@a_xbwA[;a[x#[@xA_x `AyW[@xb9[O~wxwyxb;wyx [@xb^yx ffy;x[@xb^xb^@\~$a-wy[|ay;xu-w9~Y~w2
"xff~^"ya[@;ayf~y;xm;$y? @W- b@  W @f\i(~^~;;wya;w@\~yx 6- bffwya-v;@\_^;xYyf~
`AaY;v;x [Oy;x,;a2~Y[@xba;wa_xb[b\[~E_x ~wy^\_^;u~%iu-aw@\_y;>6aw \_^Q
xfw9[@Y^;xbx ~Qxb:xAf^\i\ia-^f[b{;7K5367@595\[~^a-w9xbwyx ywyxbxQ;xbwyx~$#^;a;x [~wyx
~fxA_x ;x,~w@\~_x [a-w#yxbwff\_^f~$[a2y;xu-w9~Y~w f~^fQy;xffwyaaO\[~fxA_x Qy;x,[9~wy
[@fffaW(Qawxba%-xbw H\~^;a;xf~-[,`\wyxb^H $ @9H;xb^ H $ $y9Hffvf[@x~
;wya;vf`A@\_a^ay;xuw9~Y~w YO;x[yw@\_^;ua[@,a[Ea;9~$\_^;x wyx ~-\_^;uy;xix ~$x [eay;x
f~w9[x#ywxbxOwa_xA6aw@\_u<\[O`b~$_x y;xff5$Nay;xywyxbxeM4a-m`Awya6N_~^h rp%rP9)y;x
uw9~Y~w\[v;^f~ff\iu-v;avf[b;y;xb^6awOxbxbwy[@xb^yxb^f`Ax\``b~^fxEuxb^;xbw9~yx y;xuw9~Y~w
y;xbwyx\[~Ev;^\v;xEf~w9[@xywyxbx\`yQ\_xA;[yf~[xb^yxb^f`Ax<O\[Oywyxbx\[`b~$_x y;xf~wA[@x#a)yf~
[@xb^yxb^f`Ax
{>AAa~ffwyxbx\[~^awAxbwyx [@v;;wyxbx[@vf`yf~eM4~P~$);x^;a;x [#~^fx ux [Oa
~wyx\i^E;MWHP|y;xwyaa2a\[<\_^  ;~^fM4`$Pt\N~^;a;x\[\_^  y;xb^\_9[<f~wyxb^~^fE\_9[O[]\_\_^;u[
\_^}~wyx~$[@aff\_^  
]^v\i\i-xAi~#`b~\[ta;9~$\_^;x E;wyv;^\_^;uy;xO[@v;;ywyxbx [|wyaayx ev;^fxbw2[@aYxO[@xA_x `Ayx e\_^yxbw^f~$
^;a;x [\_^y;xf~w9[@xYwyxbx~^f~\_^;uy;a[@x^;ax [e\_9[e_x ~ -x [bQ\i^f`Ax;xuwA~Y~w\[Ev;^;
~ff\_uv;avf[b~$;xuxb^;xbw9~$\_ ~@\_a^f[eM6\_^CffPa2~^xA?~ixff`Aawywyx [fa^fyaffy;x-\_xA;[a|%~w@\_avf[
`b~f[<a);x#f~wA[@x#af~OxA?~ix6)y;xbwx~wyx@Oa,`b~f[O  ~^fb<a-w~ef~w9[@xywyxbx[@vf`yf~
b\[#~%[a~`b~a  f;xb^  [O-\_xA\[Ya-wyx[@fx `\m`fff~^b  [b\_^Qyf~y;x[@xba[xb^yxb^f`Ax [
xbw@\_%~ix6wya:y;xY-\_xA"aOy;xEa-wyYxbw\[~[@v;f[xbaOy;xY`Aawywyx [@a^f-\_^;u"[@xbxbw@\_%~_x6wya
y;xY-\_xA"aOy;xff~yxbw ff"xY[y~$yf~  \[eYawyx[fx `\m`Yf~^(be\_^"y\[`b~[xY  \[E[@yw@\`A@_
Yawx[@fx `\m`Eyf~^b\  \[Yawyx[@x `\m`f~^b~^f E  b
\_xb^"@OaawEYawyxfff~wA[@xYywyxbx [6awEy;x[y~x,u-w9~Y~w Hy;x8ffb3]5A  9AM4;P<\[
xAf^;x ~[~[@v;;ywyxbxff\`"\[E~`b~a~$2y;xf~w9[@xffywyxbx [[vf`y"f~^;aay;xbw`AaYa^"`b~
6awYy;x [@xQywyxbx [E\[[@yw@\`A@_Yawyx[@x `\m`\_^f`Axy;x`b~f[ay;xf~w9[xywyxbxa,~^xA?~Y_x
`Aawywx [@fa^fya~$2a[y[]\__xuxb^;xbw9~$\_ ~@\_a^f[Eayf~xA?~ix\_^"av;wEfa;x []\[,[f~`AxCYty;x
-\ixAa|y;x;ay;x#f~w9[@xywyxbx [a~[@xbOa;wya-ixb[`Aa-wywyx [@a^f;[OyaEy;x#;ayf~O[@xb
axA?~Y_x [b
x^;ax [y`Aw@\_fxy;x 6- b~$_uaw@\_y;\``Aa-Y;v;yx [;x#;a~[xb2aHxA?~ix [
`AaY;v;\i^;uQy;xY;a2y;xA\_wf~wA[@xffywyxbx [bffO;xY~$_uaw@\_y;:\[eya~w9`a^y;x [x,f~w9[@x
% 

fiH;f-b;  O+;;;y;
]_#  
  


 

 
   

 


 ]_	
   

ff
fi f (]_
  


 

 
   

 


 
fi ;
 ]_#	

!#"

&%

 
fi ;

 $

 "

4i
  
 
fi 
' )(+*-,.0/-132 ,4..,.5-,. 6.b87:9;7@ fi f  =fi < 7>-. fi=? *-7@ fi f fi=< 7>-.ff7A@ fi .	9=BC5 ? . 

7,.. <])B0* ? 79-. fi *f ?)D0< , fi BE7>-.F, fiGfi 7IH$_ 
4? *	-_-(J9e fi G.ff9	E)79<])K ? _-(iL7>-.ffMON-P < 9	 fi  ?)D
 < 7>-. D 9=,.9 ?Q? 5-,. 6.b87O_R9 ?Q? 7>-.ff5	9;,9A.ff7,.. iS.T9 
 7 ?)D 7>-.9=BC.ff5 fi ])7@ fi f  9	U7>-.A),F5	9=,.bG7
9 ? ,4.I9+ D _ 
4? *	G.I:V
P fi f]WG.,IH <Xfi ,Y.T9=BZ5 ? .+H7>	9=7Y7>-.5-, fi (+,:9=B+F(\[.b]74>-. <Xfi=?Q?)fi @O_-(^7A@ fi .	9=BC5 ? . V ' )(+*-,.0/
A> fi #@ _74>-
.  fi=? *-@7  fi f fi=< 74>-.ff7`@ fi 5-, fi K ? .
B ia7>-. <bfi ,B fi=< 7,.. cV
d V  ]_#0   Cegi f:h  ]_#	 0     Cegi f:j : 
fi ;     Cegi flk  
fi f#m 4n+ 
o V  
fi ;0]_#	 Cegi f:h  
fi ;	 0(4i	 Cegi flp ]_(]_# Zebi fj ]_ : 
fi ;#
B A.7
' , fi Bq7>-. A.07A@ fi .T9;BC5 ? . H 7>-.!5-, fi-
 .IG*-,.^rs-tIuvws-x yAv{z+x |yAx |I}=v{tI|=v3(+b. -.,:9=7. 74>-.~5-, fi K ? .
%  ]_#     
fi ;4i	  <bfi , fi 5.,:9=7 fi ,-HI9 	7>-. 4i-( ? .7 fi ,A.A7  %     GH %  ]_ 8H
  
fi ; #8H <bfi , fi 5.,:9=7 fi 9, F/-H	TH	9 	a!,. A5	. 
 7 \[. ?\D V2_>-.~MON-  fi=< 7>-ff. ]_-( ? .7 fi 6.97 9;,.ff7>-.
.	9=BC5 ? . &7>-.
B A. ? [+. cV2>-.5	9=9, 6.7,..  fi=< 7>-.7A@ fi 5-, fi K ? .
B   ]_#&   9 	 ff
fi ;e4i	 
. A> fi @ _ ' )(+*-,.^-V2>-.JMON-P fi;< 74>-.7A@ fi 5	9=9, A.L7,.. !9=,.CBL9=,+.IOO@ )7>
<bfi , fi 5.,:9=7 fi ,~9;,
7@, W9 -( ? . cV2_>-. D ). ?  fi=< 7>-.MON-PH =X AbI H 
fi ,,. 65 fi 	; 7 fi 7>-.C;* W*-.JMON- fi=<
7>-.ff7A@ fi .T9=BZ5 ? . cV
 |=t|=vX}x | 
fi BC5-*-74. C7>-.aMON- fi;< B fi ,.7>	9  o .	9=BC5 ? . _ 
 ,.BCb. G7:9 ?Q?)D K D ,.5.I9=7.I ?)D
 	- _-(74>-.~MON-P fi=< 74>-.ff5	9=9, A.ff7,4..  fi=< 7>-. 
 *-,,b. G7FMRN-  fi ,_7>-.  9, 67F5-, fi K ? .B^F9 	L7>-. -.-7
5-, fi K ? .BVR.9=,4. fi @,.I9 D 7 fi A7:9=7.ff9	U5-, fi [+.74>-. <bfi=?Q?\fi @O_-(L7>-. fi ,.BSV
0Gn	 Ow8Sl~6I	b	bw+l:!]IW8Q!lA+w8ab+!l=CFA]
 	+CQ    ]+	bclA+A+00+L  I_4aw8S84+w8cl^{+aA0nA8QlI4
+g	  lw8Lw8!IQIwIwlA+LY)ff:+c++0+waWaw800Ib=A++0+w^ G=ff`GQI
+QII_wqw8+&::AIbQl_+WTl8Q&`GQI!wwG++0+WTw8^w8IA)-:  
Q::WGC++:ww8l+_ WL]
. A> fi @7>	9=70rs-tIu4vWs-x yAvz+x |yAx |I}=v{tI|=v 9 ? .I9=, i-(R9 ? ( fi @, )7>-B <Xfi ,ff _K D A> fi O@ _-(
 	 O
7>	9=77>-. 
fi 	- )@7  fi f fi;< 2>-. fi ,.B o > fi=? V3R.ff9 ? ,.I9+ D 9 A*-BC.IL7>-.  9, A7 
fi 	- )@7  fi  fi=< 2_>-. fi ,.B
o ;H 	9;BC. ?)D 7>-.!. A74b.  
 . fi=<
fi BC5 ? .7.!5-, fi K ? .B  fi=? [+.9, <_SV
I

fi#T--L --T-RTR+++	-a	+---	














 

 _a    
  
  _    

  a   

 3=    _	  

:=

3=

#



	 

	ff










`=








fi

 


=
  


+


fi














 

     
  
  _	    

       

_	  
 3= 
=



! "



3=

$

3	ff





A3=








fi



3	ff



	 














fi

`=



 
%&'


(*),+-".0/21"34(*),5'67),5"+980:"/<;>="?A@CBD80:"/E8GF@</H'ICJLKM,/ONP3  N#),5EQSRTQ'UPVQWIX5'6

EY @N'QZRTN#),5[Q\VQ

]^:"/^;>="?_@XB`IENG/P8@CBaK".0@7bMc/PJ9Nd) ND-"5) e7-"/b\/ Y IC-'NG/fg)hNi-"5'ICJSb),+-"@-'NPjDkEND6/ON Y .G),b\/O6L/OIC.&Ml),/P.Om
80:"/n;>="o Y IC5pb\/ Y @JqK"-"80/O6r)c5s8G),JL/SMt),5"/OIC.u)c5s80:"/S5-"J<b\/P.2@CB/HvIXJLKM,/ON2IC5'6p8:"/ZNw)cxP/ONy@CB80:"/
K'IC.N&/z80.0/P/ONPj{=C),5 Y /pK'IC.N#),5"+pB|@.L-"5'ICJSb),+-"@-'N Y @58/H}8SB|.0/P/z+.ICJLJnIC.N Y IC5{b'/W6@5"/r)c5{8G),JL/
~SG U w IX.GM,/Pm'OC  mC80:"/E;>="?@CBIyNG/P8@CBK".0@bM,/PJnN Y IC59b'/B@-"5'6Z),5nK\@CM,5"@7JS) IM8G),JL/j4]:-'N
80:"/2NG/ Y @75'6 Y @5'67),8G),@5@CBi]^:"/P@.0/PJy) NN0IC8G) N#'/O6INF/MlM$j
]^:"/8:)c.6 Y @5'67),8G),@5L@CB]:"/P@.0/PJEIMhN&@E:"@CM 6"N4N#),5 Y /JL/PJSb\/P.NG:),KL),5nN&/Mc/ Y 8$NG/P8N Y @7.0.0/ONGK\@5'6"N
80@K'IX.N#),5"+F:) Y :) NnIX5 ~Sw U  K".0@bM,/PJWj(*),5'IMtMcmNw)c5 Y /p80:"/z5-"JSb'/P.9@CB[NG/P580/P58G) IMB|@.0JnNy@CB
M,/P5"+80:  ) NEIC8EJL@N&8[9B|@.ENG@JL/ Y @5'NG8IC58ECmvMc@7+u
v)hN@XB80:"/S@.6/P.E@XB ~Sw  mN0IX8G) N#B7),5"+z80:"/

M ING8 Y @75'67)c8&)c@75@XBi]:"/P@7.0/PJ"j4/P5 Y /E),8B@XMlM,@CFN8:'IC8
:'INIqNGK'/P/O6-"KM,/OIC.05),5"+WIM,+@.&)c8:"J),5
Wj
O

fi"'7P"OOCC"""0"

,d"p''P'q}X`hO
"z"O " G$ PG OX$OCC'p0"ra"0 v G$7 PGC "7 CE|G" CC0y,L,PLP0O>,9"07C
 t,O\^9C'{0O&0Oc0"GS'Ct  ,0PXG,An,"p""0O0#,'0
'GXc7O_C0P"PCOs'#,"0"WCLnCZc*,"0p'"9,'7n'PX0



,  ,'7c"p0"G,*,"<"'X'WGq<7tP0P&hXG,C'W#,Ltt  CG,>'PX0P C,7c"
>"0,P
 '# G9 20PLC,"s0",0P72#,AC'_#,Lt |,"0"p0OG,WqS  








	

'0#,, $p0 }\PG,LPO L0G"qOW'C"Z&C,"G,'090"S"0,Pn2X02"0C7 O
ZXn"0P0'0O  "PODSt,
 00Xl,O9"'PG,LPC&c7 ,0z7tP0P0,c"LGPP 
,L,PLP0Os0"L0O  "Py'#,"GP2 ^G,    00C40,OP $   0"LO  "P   00C



ff

fi 

0,O90"Pn&c7OP0z,OC"Offp"nC|P"PX0O{vCqcOr'#,"0"COC7 P
0"&c"0'C  7L""0OS0"LGLG\  t  P"PtO
 CG,2
 GPq[
 ",PnP!4  G  
'CL,  '# GO9"0,P
C'0"L\OGE'&E'PX0E0WC",p'CE"0,PW# 
0 9n,O  {'PC0SS,L'pC"t  C,0nX'X0S2
 C""0O0#,!" P  
0"p0O  "P#|P"PC0OGC,"G,L
 "07cP   0't,  7'# G9L
 pt GW2
 'CCLPPGP O
\PC0Pff"P0"'CXLP0PP"70O0",  C&c7%w$ G"'""0O0wc7'i
& X(   9,hC"t,O
"0"OG2'cP"0"E"07C,OC0'0"2G,  #$ GPO  G,'Or,W0"E"P,'G  G,
^""0,PGC,P ,n0"'"O# &'  C00G"LOZ02PL,CS'&#| P0P0

 "z}"0O#,00Pz00C')  &"'""0O0#,'qsC",,"s0"z'PC0 Cc  0"
,OC0"P* EG,  #$ GPyC0L","OpLGEG\  l  7P"PtO
 CG, 0"L0,c"vXL,OP
C'#,  <\00"E,OC0"O"07cP GC,PEC'r0"2X0P"07cP GC,PPL,Cz0"<CL
,&c"0q0PGE"2""0O0#,p00P\z0"<C"LP,"2"0 4 4"P0P "\0"
,OC0"O9"07cP GC,P  O"  '# G0P ,0z0"0,,"L'CL,OP P  E"0"P70P



+



 
+"

+

 X"l  C
 ,S0Lh'CXLP0PG POzncp t$
>L0,"O>0"nG"G0P p,0PC&c7"0,Pny0'C  '# G0O> G"nu "0"  E
' P C'WG7LE0G,"7LP0G  |"  G,' * #W'C0&   C'O  z"
 0,P  D0"
|Ctc ," 70W "PEO    
 cP ^7G,  0Oz" |0S,C'S, tdc  "X  O
C'q,'P'P'PG, |0t"P  "X  O 

,


 

.1



Z


[ff

32


4

.- /







1

0





%
65
97 8 +:z<;->=@?BACEDGF 8 wcH-I  <-IE+:z<;KJL-GMGF 8 #,N-9I  <-I3+:z<;KJL-4F 8 #,O-I  -I3+:gP;	QRS | 0PO  W0,c"9'CL,'"<G"G0PT^0OG0OrL&PU*ffZ0OG"07cP9P"
0OG"0,Pn4
 P0<h&gG,  0O'#,"z0"S0CLE,,"W7h&0G,""G,LPG,"Or'3| 0 S
0OG"0,PV
   "0OW7  00  &cW&C,Ozz0"E,OC"P4 ,GC,"G,v  G,znC  "O
0'C4
 0"2O  "P  ESL70  '&P0C&c7W nD LO&"G,"W P "  z0'X  "G,"
0"E"07cP9+
   pC00O  Oz<*7X ,,P""0O0wc7'O ,0"7""c0P#,K$  G
|  OE"L,OC0"PS0p#,Lt 0"q0OG,y ,0PC&c7p,0"gCLY p0'CE0"90O  "P
OP



%

*," gG" u0"L\P  PC9 0"90OGE",Pn  00  G,T&C,O |0 "L0OGSGP
 PC7OCP

,,"_0G  9,0OX,'G90"p"S'Pz <0c,"'CL,OP"
P007'XP"0"2GC'"XnP7 CG,9,0P0h^z'g# O 0"ELOCD",OCc"

\X ^]

<_



1

ff



 P0O   ,90O  
 ,"Z7
 P
 \  C'GEO  
 
  "   ,0,W 0,,"SvXL,OP4  D
0c,"L'CL,, $  4C,O^ G
 02nXn&nt0,,"L"'PG,P  OP\O    0OG'7'7c"
0L7"E'PX0C"t  C
 G,

`

a3b>cWd\eBf6gih3d@jWkmlnf6oKp	q^rmrms#tuomvw*omd@kKs#x>kmld/ermsEwny6d)z+o{f6pomkmd|q^jWs^x}kmldKd@pBk~f6rmd(jsEzOq|f6pWwd@g#q#vomdUq3'zOq^pBUermsEwny6d)z+o
f6psEvrKj*s3zq|f6pj*s4p*s3kKl	q@3d+gy6sEomd@jxs3rmzoms^y6v*k~f6s3p*o@}q^p*j`w	9o~f6p*g@dOkmld+kmd|q^gild@r{f6o/kmsOwd\	rmomkKkmriq|f6p*d@jv*o~f6p
l	q^p*jnomdy6d)g@kmd@jd@*q#z+eny6d)o@'f6k.f6oOkmd@jBf6s3v*o+kmsWjsWkmlnf6o(s3p
qyq^rmEdHjsEzOq|f6pffbYld+y6d#q#rmpnf6p*Yed@r~xsErmzOq^p*g@d4s#xKkml*d
omBomkmd@zf6opsEk{omd@p*o~f6k~f63d(kms\kmlBf6o{gils#f6g@d/s^x>kml*dKermsEwny6d@zjnf6omkmr~f6w*vk~f6sEpb
	*

fi/`/N	ffffff>%>ff>

100
Average accuracy

80

60

Accuracy



40

20

0
0

5

10

15
20
Number of training examples

25

30

9ff#R4'*#<%3#ffmffY>34/O`'#YB#^ffH>E#ff
@3>E<B@ff
ff3#BH<WL^WY*
LY4>'ff\i/'ff'BOGffG9ff
 `
 ff3#ffmffGBE#/i4
3#GKR@*ffB>3}ffGBE#E^>#<Bff*\H@ffRiK%ff3#ff
ffGBE#ff3/`ffH#Hff3BW#W|*E^u@>ff33<E@B+#>^@>ff3H@@ffffGB#
#EBG#B#B<`>3^*ffu#3^B>3%O@*E^4i%#<W@*3|'(3ff><BW^W*^<
K
ff3#mff>BE#ffE+>1mff#
	B>
ff4@>B*<*#<1EYB+ff#'
} m'R'*ff@
	{ffB}fi>

B#W+W`ffH#ff#@Y|u#>4@E#*#H#B#*|BRE<Yffff*3#ffE#3#B#	*
m*##*B<OB^U#H`ff R RGBOL*Um*##HB`Eff+>ff><*fiR`WGR
K
 {i
 +< 6
 />ff9#*##*H#Y3<i*ff^(# !@'3>*34>Y*B{>
#3O*O#H3<W><@ff>B\i
 "{ff<$ #KB3ffG#Om*#^*(#U|*BG>`#3O		*#
@Y><@ff>B&
 %>@#WE@ff(
 '*),+.-ff/0/0/0-1)324+#`#B#*@B<H
@3#5
 )'OB#6
 )37K4#
{E$ 8:9<;Hm*#^ff
 <ff
>
 = @
 ?:AB?:CDEDGF HBIKJ1ALNMOA.P3CRQSDTI.L#41 U*3(9RY>BE^ffK
 = ff`Hm*#^U
O>3|uL^H	HKff<
^>\*^#N>1(><BGB><BRHff#B+m*##4		*
W V4ff||Y
 X,Z[\PE$ +<
 6
 /GWH%1 >Y<(1#ffE<*3Y>R^< <
'>*B>|Y#
1 U*3YW<##ff#><@ffW3<`3ffY<B#3#*3E<m#ff #@>
><@ff%/#>U3<ff^
 ]9ffE	<*3ffYGR#<)ffU>Bff*<u<ff
>ff "{ff<_
 #.B3ff
i(
 "{ff<5
 #KB3ff{#Y1 U*3HO<uffGBE#ff4ffa
 `bRY c.d.e$fR{BEff`@<B>B>ffHff<
ff^Y>R)| O#>W@ff{@%ff^Y>R)| O#1<}
 V4@UK XffZ[\<U<1 g>*
Yff^HffBBEff@ff.<*3ff1>R#<@B*
 hn@B@<*3ff1>R#<@O i1O<^U<B<
#
|>#u<ff`>B
jlk,m

finoqprBs3oqt,t,uwvyxo.z.oq{qoq|}oq~

ff0_ffK<<T611NO.3R3GB.03}1ff
130q5B}q^3
:,.}q,G}q
1 ,1.31.}05qBff06 w:,.}q}lff0qwEw:q106.
qllq,.06}.
:,.}q}^.affGq&,.q},$.$q},1,W&ffK:}},.w}qE.qN ,Eff
:,.}q}w0BRwq310q}q3>Y0q31&Ew3EffG0>,1 }.0&B.,.q}@BK30q
}.K3K}qE.qKffN}&}.w3Eff,1 R}.GG0N^q3} q^ff0>
w1GBY0K}3q},3K.B:0R$q31&E}3q},3R},R00$,}q},w.G,
wq1&llq,S}q30wENq@3lT0,1 }.0:@.K30q .}q&:,.}q},S
 5}q 6}.&}q6RffYE}.E,>0q.0 ffK.},}10,ff,ff,}OK
 E$q q0>ffG0l&q$0,.}q3N}^},1B03$q3>}qqqRffEw.}03,ff0
}.W, 1q,: ffG.,G$}q 3^.0..01.},0R,q363K.}13q*33}
K}@,} ..051.}.>B3K.&.0..0
E3ff0 $E}3EffG0,1 R}.0NB :,.}q}wK3q6  qq
0R}6@3q3KffG003y>}q>K30qB,,ffKY1}K.0E aK.0>.@1}R}
035 WqRffK.}$q3}q51.0q N}3q},3R$q&,.q},0}q6..BK30q
.}q>}.@}3q},36}q10 R0alff0q,31} $0}q>:Ea1.0q .@q R<
}.}E,w}q5B,.W:3G&
1q,1:&G$q3q33w0NN, .0l.0 K._$q3}}q
:,.}q},,ff,}16,B^}q10Rl.ff0q,3  ,ff,} <  .>}q:,.}q} N}q6lff0q^
}q6:,.}q, 6}q}Bq(}q ,00qK.} .}.K.}3,a}a,B5q Rff.ff0q,ff
,ff  1} W YENqq},qq.NT0@.E,N}q^B1}:K.0q}B3}>.>q}E1
q}1q.   }.E,ff
&Bq.w,6}YTff06Ew3EffG0,1 R}.0. }E,}q$3K.}0Rq}
}q30a(B1}K.0W&}^,BK  3$N03R>K.:.}0}K}
W}*3.}.,.q}$llq,Sff0$E}Ew3EffG0,1B R.0w0},,1w} 
.>31.},Bq31.@1.,&1.}  ff,ff} <  *3,333w}<  ,ff,}  SR,ff33
}.W,}q&1}B:K.0$q}3}R01&}q$.ff0q,w.:,.}q,$$q}q0qwE.}}3wK.}
330(}q10 .ff0q,60q0Eff^K}.(q6q>lff0q,._B}q3,.q},
*}3q,3R,>w0}(RK}q ff0}.E,N31.}0Rq}}q3}&qa}5 
31.}6,q31BR.0l.0K.}a}3.3,GE6},qqR, _3
}3E1,w0}0Kwq},q}1q>}.w}.E,}q&1:K.0&q}3WN31. 3EffG0
,1 R}.0ff0}.6}.E,}q B3K.50Rqa1}>K}.61Kff0
0>qq},qqR&1}3  ^3ffG61}6K.0^qBq},qq.,
G1}K.0 w0}qq}qE.}10K3,a,}q},NE.03Y}q3^3 ,
}1}}1.0q0&:} R0Eff$K.^w0}qq6.RK0q>,.Kq.$a0
0q}@W&}q,BK  qw,}q},
}Bw}6>.}$31,}010K.3}}q10fflff0q,3
.q00q1R W w$q3}(E }qalff0q.:,.}q}0qK}>1:}.q0Bq
}q61  G}q:,.}q},66}qqY6ff>qff6}q10^BRfflff0q,^qqa}q
.qGE3.0}q1}}q3ffGwGG3Rff0}q10Rff.ff0q,^ff0q>q:,.}q} R
}q63a.}q .qGE3.0}q61}  3..0..0 q}03 E.0,01}
.qGE3.0&}>q}B3 .03ff

	ff
fi	 
RSTff Bff ,3  B6 "! .3Ka#a6,.:R3G   
%R$ ffW6^>::BWB,1'&5y#ff,^%$(31:RS)$>%$R%*
$ 3O1,:60/ # 
+#,  03,1EBEG61KNq.3G.- E%
 1# :1Rff<@<<$BT:<#q _S6K'3 '&54.687:9%;#<>=?9;A@B C?D<FE>;AB G:C?9#a%$R^6ff :R3G& - <%$
12, 
%R$ H 1:W6ffK%26
&  ,
I8JfiK

fiLNMPORQAS(TAUM	QATAS(VWXORQAYZ8ORQ[#\#V#V]3^A\`_3V#TAQAaAba3c

dfefig	h?iAjlkPe:im.n8o:p%q#r>s?pqAtu v?wrFx>qAu y:v?p
z|{ dlk~}.
#A#'ff)"
fi?FR?`A
 ge.}?	e:g	k	jg
 i' z|{
 )>#A?:ffA?AF%Ffi?A?l#
fi?FR?(#fiA>fi?FA>?f F
H#3FR`>    
i { jH
g	k}fidlk~}fi?FA>?  
i { jm.n?o:pq#r>s?pqRt#u v?wrFx>qAu y:v?p
 AA(0#8`#5')#?ff#)
0 >80A  ?A.A'' ffA ff3#'>fi?`F#A  2:>?
0#'fi##   #A  5'ff)5  ?    ###F#3)?#?fffi?  fi  )5  :.'  )?  #
 ))3 'ffA >.  >fi?   #    '#A#'ff)AfiA)  >305>A   
#?ff? f HAA)fiA(A#  5A>3)fi  #A  (~A0#F?'ff#2>? ffA   A
3>.?ffA   #>? ffA Afi?`##F#3)?# ff ?ffA  Afi?Afi.
A  5?>A5fi`3:#.A  2:8:ffAfi8fi#' . A.88A?A5  fiA
?A5A#'ff)?3      AfiA)A  ?">))A.#F#3)#0F#A   ?
  `A  ?ffA  ?A#?'ff  A)  8fi#'A)  #3ff`fffi?  fi ff
  ff)    Afi#A)  5'ff?A'  fi  AA#'ff)  fi)?3>.A5'#  0>)  :ff
'fi#52?'ff`  fi>3fiA#') A HAFfi?AfiA#  fi#A    2:
88Afi:F)  ?A'  )?    H 	A)    >3A)  #3fi  fi5##F#)?(`>  A
fi#A)3>fi0.55>fi?  #)?#0>fi#A  .3A ffA AFfi?AfiA#  
.A    2:	88Afi ( ?A'  fi  #3)?#>fiA  .AA  A#'ff)   A)fiA
A   AA    F)   ffA A#  :AXA  (PA#F?'ff ff   H A )fi
:A.Ffi?Afi#'3:#fi#AfiA    ':(8:ffAfi   A    #)?>fiA  3?
  5fiA ff'   :(>?05A0  :>  )A  fi#A?ffA    
    53    #3A?3?#  5'ff)5  ?    ?~Afi#A)    ff.  ?~  


3'       fi   Afi#A)##ff. ff '3>  #A)0# ?>fi?`# ff '35)'fi
'5'A  A#'ff)">)   A ff fi  )5  (#AA)#  0  ff3A).   
?  )A')"?ff#) ff A'2#Afi  >3?Afffi?  )        >    A
fi#A)fiA#'ff)">?ff#)     0fi  )5    'Afffi'A.?3?#  ')      ?(A
fi#A)3fi)?3>  )3>fiA##F#3)?#  ?:fffi'fi  fi'Afffi?  )
A  )#)32  'ff     .  >Afi3?.A)    A #F?'ff3?5)    )?:
>?ffA    AA'3fi' AfiA)  'fi  fffi? 'ffA 5)A):fffi#)  :? ffA 
      5'ff)5  ?    ?AfiA)H##ff#lff A #HA?8fi  )5  
 ?  3fi 

	

fifffififififi!fi"

#%$&('*)+-,/.0 132.4+653)+7983&&(:<;>=
?@A&(B*C?83&8ED!&F
GA$&@!HI=;>J
&KL&MNF4OQP*R>&8SD!TQ;>KG@!&OU&KVDWFRXR>YLZ?*;[R6\LD!$&
O]F
G@AT^DWFZ*R>&
_`#aTb8c;>OQP*R[;XdIYQD!$&EP@!&83&K*DWFD3;>T
Kaef&gF
8A83?OQ&gD!$F4D`D!$&gP@!T
=@WFOh;i8j=;>J
&KUD!$&gK*?O(Zk&@
TddI&FD!?@!&8mlnejFK\oDA$&QKV?ObZk&@(T4dE\;683Dp;qKGDrds&FDA?@!&QJFRq?&8Uteufg$*;6GA$vD!T=
&D!$&@(\*&DA&@!Ob;>K&wD!$&
P@!T
Z*R>&Ox8c;>y&bTd`T
?@Eds@WF4OQ&fT
@!z_{jT
D!$wT4d|D!$&83&(GF4K}Z&b&83Dp;qOFD!&\dI@!T
O~&MNFOUP*Rq&8^F4DgD!$&(GT*83D
Td|FmR[;>D!D3R>&(F
\\;>D3;>T
KFRGT
OUP*Rq&M*;>D3YwTdDA$&SRq&F4@!K*;>K=FR>=
T@3;>D!$O}_

ua '*)+-, .0 132.+59)+3
 bWW 79 W C  l  tk
 * 
&D !/u!Wc 
u*( 7 W l  tNC   
 * 
46j ^|SH-j  {<|D!TQT
ZDWF;qKL73 W CW
i      I  f;>D!$D!$&^K&MD%;qDA&@WFD3;>T
K
&D W>>>3
 PP*R>Y}D!$&^TP&@WFDAT
@g83&
?&KG&bD!TbD!$&^P@AT
Z*R>&O 

 mo[^aaaoba

k [ 79kCW
 %
 
 &GT
=
K*;>y&bD!$&^D!&@!Ob;>KFDp;qK=PT4;qK*DW8jdIT
@gOF
G@!T
HIT
Pk&@WFD!T@W8 

 Pk&@WFD!T@;>K\*&M] [ 
 mg[^aaa l <
 V I

 JFRq?&bTdudI&FD!?@!&  Td| 
 [   
 


&D w wZk&D!$&8pO]FR[R>&83Dg;>K*D!&=
&@8_/D_-  e  e   s  e  e   
igL3  ;i8E&OQPD3Y
 o 3  q p  ! >>iW  ! 
P q z
  

  
a O]F
G@!TQDFZ*R>& 

  '*)+-,/.
0 132.+53)+

:<;>=
?@!&(B | &@p;iFR`F@8c;>K=  R>=
T@3;>D!$O}_
#%$&b;6\*&F}Zk&$*;>K\D!$&]'*)+c, .0 132.+653)+j;68(89;qOUP*Rq&
_-DbGTR[R>&GDW8QF}8p?*GA;>&KVDbK*?O(Zk&@TdgD!@F;>K*;qK=
P@!T
Z*R>&O]8mFK\D!$&;>@83TR>?D3;>T
K8?8c;>K=  jSH-|  {a|_u#aTw&F
GA$LD!@WF;>K*;>K=wP@!T
Z*R>&O
ek;>D
FPP*R[;>&8S;>DW8m83TR>?D3;>T
Ko83&?&KG&QT
ZDWF;>K*;>K=}DA$&r8p&
?&KG&QT4d|;>KVDA&@!OQ&\;6FD!&Q83DWF4D!&8kmD!$@!T
?=
$L_
 ;>KG&;>D;68bz*KTfgKoD!$F4DD!$&w83TR>?D3;>T
K8bDAT}P@!T
Z*R>&O]8bF@A&]=
&K&@FD!&\?8c;>K=DA$&]O]FG@!T}P@!TZ*Rq&O
83TR>J
&@uf;>D!$QFSz*KTfEK^dI&FD!?@!&jT
@W\*&@3;>K=eD!$&g83T4Rq?Dp;qTKr8p&
?&KG&gOb?83DuZ&gFSGT
OQPkTV8c;>D3;>T
KQTd8p&J
&@WFR
O]F
G@AT
HIT
P&@FD!T
@W8_`-DSZ@!&Fz8D!$*;68S83TR>?D3;>T
K;>KVD!TU;>DW8gGT
K8pD3;>D!?&KVDSO]F
G@ATV8  9  dIT
@g&F
GA$wdI&FD!?@A& 
Z*YQ@!&GT
=
K*;>y;>K=QD!$&S&F@3R[;>&83D;>K*D!&@!OQ&\;6FDA&83DWFD!&8;>Kwfg$*;6G!$wD!$&E@W83D  ds&F4D!?@!&8`TZDWF;>KwD!$&;>@=
TVFR
JFRq?&8_]#$&bO]F
G@ATV8^F@!&Q83D!T@!&\;qKD!$&QFPP@!T
P@3;6FDA&]G&R[Ri8mTdD!$&bO]F
G@!THIDWFZ*R>&b?K*Rq&8A8DA$&rG&R[R68
$FJ
&FR>@!&F
\*YZ&&KR[R>&\ZVYP@!&J;qT?8cR>YRq&F4@!K&\O]F
G@!T
HITP&@WFDAT
@W8_
#%$&^@!&83?*R>DT4dnD!$*;688p&GD3;>T
KGFKwKTfZk&(83DFD!&\FK\wP@!TJ
&\_


fiN|a|NoNo





	ff
fifi  "!$#"&%('*)*+-,/.10 24365798;:&<>=?8@:A0BCD)FE"GH'*I'*IKJALNMOGHPRQSTLNUV'UWG-STLXGHEYIZ'$IJAGHS[JH\4]
EX'$)*+KP_^`\HEbadce'*Igfhc9ij' ^
kHl>GHS$SnmH\HPffGH'*IUo'*IhadcpGHE"LoU`LqEX'*GHS$STrWmLYsX\HPRQt\U`GKuNSvL>%('$)*+-E"LNUwQLYs)()F\Wxzy
{KlRfhc|';U)*+KLU`Lq)ff\"^}GHS*S(QE6\u`STLqP~U\SvLqEUuGU`LYm\HIIZ\HIE"LXmHKIZmHGHIZ)DPffGHsE"\]6)FGKuNSTL`U%('*)*+)*+L
^LXGH)FE6Lff\HE"mHLEY'*IKJffxp^\EomH\HPffGH'*IUo'*Iga c yjGHIm
Kl>)*+KLWIPWuLEo\"^WmH'U`)F'$IZsq)^`LXGH)FKE"LgqGHSTL`U7';UWu\HImLYm-uNrAG>Qt\HSTrI\HPff'*GHS^Is)F'$\HI?\"^DPffGM';]
PDPQE"\Ku`STLqPU'Lz5(l
 X};H8;K`:`Y8T;6HzH"AAx|HY`Yqq>;:A;:XqH6@:
HY`
 ;YqbFH  ;HRb"`Hgeff$H"!*H F#" q:;:";WV365Z6C
Hff;:o";WRD8;Wo6@:48@zHZ`YH$(:qgKff(:ffgK;ff8;`:H
o}gHX;:e-hHX}Y8;A$H:qK"R`:Xfhc$A}"`o8;:Hff$8;F";W
H8;`1"8;H`YN
   "!$#"Hq9z"Hq(H`Y4H(""q`:X(`XY(Z`6b``:h4:Kff"Y
* 0 qq[ $ 0 K`qX`HXt4: * 0 ;q t* 0  t NKqq"`>hHXg"  R;:XWffHh;:
$-"`O8v8;qXHWZK8;FHAFqHY`";:Hqq"Y59XqXAW:H`Y4HgH
hHXHFH`XH:-FqqoffHH$:`Z`:`:K>H:o8;qqqq`
:z`:XH@6qzffhHXHFX8;oHZ`"H:-wHXq6  D&Zo$`:"$`8jg
XHq"H:;:hHX;:egY`DH8;`"8;H`qogHXY8;H  D"q`"
YH`j8@`"8;H`$bH"qH:Ao::q::hHXffY48@H:g:h""q`:Xo4
"8;";H:AR- "  Hbff"XRXbH8$gZq::qb;:XD8T8&hXKq"`:
;:}h8;";H:bWX;:@:H8;`h>WXHqX"8;XYXqKhw[ "!O4$#"6
Wff$H"!$H F#" v8T8Zo8;ff`X"8;";H:b(8T8q"zH8;`hb6@:
g8;q:qphHX?Y8;H`:XAH8;`"8;H`WHWKg8;q:;:8;HH";
XH:$"`:K@-X;:@:A8@H
;:X>hHXjXYXqWgow[H "!O*#"8;hYhoXHq"ZH:;:hgHXK
;:eY`DgHXY8;HbAH:8;};:$g8;q:qhHXY48@ff;HffOv8
X"8;";H:;H`:KqH`W$g`:"W-:qXqegHXHFHZ`Yff;:
YH`(hHX>Y8;H:`H`b``:g8;q:qb;:X`&5A&:g7DX48@:``z
W"D57hHX`::K""`ffbq"ghHXKX8?gff$;:;:8;q:q?hHX
Y8;HoR`:Xffff:KffZ`T``:KhXhX8;qzH&H8;`d"8;H`Yz4`:Zo8;q:q
FH1W;H`:Y`hHXgY8;>$z`FZH:q<H  $R$zffXqX";HoHq6
"HX>j8@q4:`q`:XH  `H`H,.0 2 3"578;:<z=?8;:B0 CbX8@qR$"A@`:
`:"8;q:T8T;6H
ff$gqH""`H  ;H}4WA::;:";WDw[H "!O*#"$WZH:qK
ff3",AO5ZCY`$>ff8;`:Hb8;H:Hq"W"48@@:};:WYO@:;:}8@H(:?W`
Y4W``Yff-WX:qeZHH@:X},.ff3 q2  8;:h0B CY(A:";WAow[H "!O*#"b
48@:Hff$8R@:eO8v8zhqH;qpYW``Y``:X@Vff}`q8;q:@:8;HH";/FH
adc;:-fhc(R
 jH(``"&Zbw[H "!O*#"HXt8;;Y9"`"$8tqXWKT8v;"HKH8;`W
"HX"X$O8v8;&;bDXHq(&ZH`:"$8T8@XtZH:`:KO8:ff`b"8;";H:
;:KZ8;K:ff$8D;`hHX}X8;HH`"$8D(Y;:}qH;qg4W-qH`W"8;";H:
3w$q-A&zwjR&bj?C`:-ZV`@q;:-h6@:48@ff8@`d8;H`q H;
;:8;}hHXY8;H}"$q$gq;`W`:AK8;`";:-q`A"qYFHA


fi
	fiff






!#"%$!&'$!(fi)"#*$!"*$,+-$.0/!)12+-3
4
5!$67.8$!9:.0;=<?>
)9@+-3fi%>=!#"$!&'$(fi)"A*%$!">fi!+9
$!*4fi))9@(
")/B0$!3fi+C.0;
.0)"9
)DEGFH>:BI+H)9fi+-3
")+J*>fi*J*>
)")BK+?7.0<?7;L+MN#"$1*AO4:.P)QB09*%>
)?.0)"9
)"7RS++-)"A%>+-(fi!#)T<?>:BK%>
BK+?#$!9fi+CBK+-*%)96*T<B0*>U7.V.W*>
)1+X$.03
*-B0$!9fi+5!)9
)"AO*)DY*>:3fi+JZ[O"EMF>:BK+\(
(
"%$6!%>]%.0$6+-)#.0;YB09:*)5!"A*%)+*>
)
^ .0)"9
)"#_9fiD`*>
) ^ *)!%>
)"A_,9fiD`4
"-B09
56+?$3
"a+-;
+-*)b%.P$:+-)"\*%$,*>
)1(
"%)/!B0$!3fi+QB0,(:.0),)9:*A*-B0$!9fi+
$ZM3
9fi+-3
(fi)"%/!BK+-)D@+-(c))D:3
(U.0)"%9:BP9
5=+-3fi%>d!+fe8gNhjikml87B0"AD`)*\7.nE0opq!rs:tAE
Fu$=+-))1*>
)NB0,(c$!"*A9fi#)v$ZG*%>
)j4c$/)1")w3:B0"),)9:*o8#$!9fi+CBKD:)"1<Q>fi*?>fi(
(c)9fi+TBVZ*>
)N*)!%>
)"
3fi+-)+N+-$!,)NZ'$!"b$ZH!D:xBI+%+CB04:.P)U+X)"A%>y.P5$!"-B0*>
z*$=5B0/!)Y9@$!(
*-B07.+-$O.P3
*XBP$9{*$=)/!)";@|}B05!>:*
~ 3
#.0)Y(
"$!4:.0)B0*TBI+v!+X!)D*$U+-$.0/!)!E2'Z?*>
),(
"$4:.P)Y+1"),%>
$6+-)9{3
9:BVZ'$!"v.0;{"#9fiD:$!v.0;!oB0*fBI+
>:B05!>:.0;=3
9:.VBP)#.P;=*%>fi*7.V.8*>
)+-)$(
*-B07.+X$.03
*-B0$!9fi+?9Y4fi)1D:)"-B0/!)D2Z'"$!9:;+CB09
5.0)!#"$&'*A4:.0)!E
F>
)UDBV]#3:.0*-;$Z?fi9fiDB09
5$!(
*-B07.a+X$.03
*-B0$!9fi+NZ'$!"v*>
)U5)9
)"A7.VB0*-B0$!9$Za|BP5>6* ~ 3
#.0)
"53
)+?)/!)9,$!")1+X*"$!9
5.0;U5:7B09fi+-*\*%>fi*?(fi$:++CB04:BV.B0*-;Z$"?4:B05!5!)"(
3
#.0)+Nkm?*%9
)"a"v3
*>8o
pq!rs:tAE8F>:BK+M+-3
5!5)+-*A+*>fiO*8*>
)G*%)!>
)"uBK+89
$*8Z'"))G*%$?3fi+-)O96;(
"$4:.P)+X$.0/!B09
5$!"W+X)"A%>1,)*>
$
D
*$U+-$O.P/)(
"$4:.P)Y+oBVZ\*>
)x.P)O"9:B09
5d>fi+*$=4c)]+-3fi#)+%+CZ3:.nEUQ$<)/!)"ocBVZ?*>
)v.0)"9
)"NBK+v7.V.P$<)D
*$@!+-w3
)"-B0)+o}BnE)!E0oJ!+-@*>
)Y*)!%>
)"j*%$d+-$.0/!)=")#Z'3:.V.0;D:)+CB05!9
)D(
"$!4:.0)+o*>
)]+mBP*%3fi*-B0$!9BI+
DBV8)")96*7E\F>
)9@*>
)N.0)"9
)"19@!+-=*>
)N*)!%>
)"\*%$]+-$O.P/)j2(
"$!4:.0)D:)+CB05!9
)D{+-(c)%BV7.V.P;+-$
*>fi*QB0*A+1+X$.03
*-B0$!9{<$!3:.KD`fi*1Y(fi"*-BK#3:.K"j#)#.V.JBP9*>
)v!#"$!&'*#4:.0)!E1C9UZn!#*ocB09{$3
")#L(c)"-B0,)96*
D:)+#"-B04c)DBP9*>
),9
)#
*j+X)#*-B0$!98o8*>
)2*)!%>
)"13fi+-)+N+-)"A%>{$!9:.0;@*$d+X$.0/!),*>
)+X3
4
(
"$!4:.0)+N*>fi*
#$!""%)+-(fi$!9fiD2*$TB09fiDB0/!BKD:3fi7.M#)#..K+JB09Y*>
)?#"$!&'*A4:.0)!E}n9fi+X*)!D2$ZBP9:*)"(
"%)*-B09
5j*%>:BI+J!+M*%>
)?*)!%>
)"
)9fi+-3
"-B09
5,*%>fi**>
)")TBK+,+CB09
5.0)!#"$&'*A4:.0)<?>:BK>YBK+?#$9fi+CBK+-*)9:*<HBP*%>U7.V.B0*A+Q+-$.03
*-B0$!9fi+o
<)9
*>:B09
U$OZ*>
)N*)!%>
)"\*%$,4fi)T-3fi+-*\*%>
)j+-)"#>(
"$!5!"AO<?>:BK>@+-$.0/!)+Q*>
),+-3
4
(
"$!4:.0)+E?B0/!)9@
(
"$!4:.0)Uou*>
)N.0)"9
)"jD:)#$,(fi$6+X)+TB0*TB096*$U+X3
4
(
"$!4:.0)+1O9fiD@*"-B0)+T*$=3fi+-)v*>
),7.0")D:;U.0)"%9
)D
!#"%$!&'$!(fi)"#*$!"A+B09=B0*A+H!#"$!&'*#4:.0)1*$,+-$.0/!)N*>
)UEM>
)9
)/!)"Tv(fi"*-BK#3:.K"+-3
4
(
"$!4:.0)D:$:)+
9
$!*?>fi7/!),#$!"%")+-(fi$9fiDBP9
5Y!#"%$!&'$!(fi)"#*$!"BP9`B0*A+*A4:.0)!o:B0*\+mBP2(:.P;U..K+Q*>
) ^ *)>
)"A_x*$,+-$.0/!)
B0*14:;{+-)"A%>{9fiD{+-*$")+T*>
)+-$O.P3
*XBP$9@B09@B0*A+*AO4:.P)!EYF>:BK+TBK+1O9fi7.0$!5!$!3fi+f*$U!+-B09
5U,)v4fi)"#+->:B0(
w3
)"-B0)+MB09Y$!9
)T$Z8\9
5.03:B098RS+J,$
D:)#.K+G$OZ ~ .0)"9:B09
5=knT9
5.03:B098o8pq!r!rLtAEBP*%>*>:BK+J,)v4fi)"#+->:B0(
w3
)";,$
D:)#.[o7B0*uBK+89
$Q.0$!9
5!)"}")w!3:B0")DN*>fiO*8*>
)")BI+}Q(
"$!4:.0)+X$.0/!)"8B09N*>
)M.0)"9
)"7RS+8>:;:(fi$!*>
)+mBI+
+-(fi!#)N<?>:BK>BK+T#$!9fi+CBK+-*)9:*<B0*>U*%>
)1*)!%>
)"RS+Q+-$.03
*-B0$!9fi+ETF>
)")BK+7.K+-$Y9
$,5!3fiO"A9:*))1*>fiO*?*>
)
.0)"9
)",9fiD@*%>
),*)!%>
)"1(
"$
D:3fi#)Y*>
)+2)+-$.03
*-B0$!9fi+N$!9{"#9fiD:$!(
"%$!4:.0)+EYn9Z[#*o8*>:BK+aBI+
,$6+X*.VBP)#.P;@9
$!*Q*>
)j!+X)!ofi4c)3fi+-)v*>
).0)"%9
)"3fi+-)+?*%>
)1!#"%$!&'*A4:.0)1*%$,(
"$
D:3fi#)1B0*A+T+-$.03
*-B0$!9fi+
9fiD2*>
)?*)>
)"J7;j9
$*EM
$!"J)#2(:.P)!oBVZW*>
)Q*)!%>
)"7.0<?7;L+fi9fiD
+G*%>
)a+->
$"*)+-*G+-$O.P3
*XBP$9*$v
(
"$!4:.0)b46;=+-)"#>8o
*%>
)9YBP*#+\+-$O.P3
*XBP$9fi+?*$v"AO9fiD:$!v.0;U>
$:+-)9=(
"$!4:.0)+T")T.VB0!)#.0;U*$24fi)j+X>
$!"*)"
*>fi9=*%>
$6+-)(
"$
D:3fi#)D=4:;*%>
).0)"9
)"E
QW}
[@fififi2LOI
n9*>:BK+?+-)#*-B0$!98oc<)TBV..03fi+-*%"A*)v9U(
(:.VBK*-B0$!9@$ZM*>
)*>
)$!";Y*$v*>
)v|}B05!>:* ~ 3
#.0)jD:$7B098E
C 9|}B05!>:* ~ 3
#.0)!o!.0)*Qmo
Vo
6o:9fiD,f")(
")+-)9:*G*>
)Q(
"-B0vB0*-B0/!)\$(fi)"A*%$!"A+}$Z,$/!B09
5jf*-BV.0)\"-B05!>:*o
.0)#Z*7o63
(8oc9fiD=D:$<?9"%)+-(fi)#*-B0/!)#.0;!E@!#"$6+")T")(
")+X)96*)DY!++-*"XBP9
5:+!D:)T3
(=$ZW*>
)+X)?.0)**)"#+E

$"W)#fi,(:.0)!o!*%>
)+-*"XBP9
5 ^ :_Q")(
")+-)9:*A+MD:$<?9fZ$.V.0$<)D14:;"-B05!>:*EM
$!"}9
$!*AO*-B0$!9fi7.6)!+X)!o7Z)*%3
")+
kn*-BV.0)+#tO")?.K4c)#.0)D=Z'"$!N*$,r
o
v+-*A9fiDB09
5vZ'$!"*>
)4:.K9
=9fiD=uZ'$!"*-BV.P)vEM
"%$!*>
)1"5!3
2)96*
$ZM*>
)1(
"%)/!B0$!3fi++-)#*XBP$98o
B0*BK++-)"-BK7.V.0;dD:)#$,(fi$6+%4:.0)1Z'$!"?*%>
)Z')*3
")$!"#D:)"-B09
5]v*%>
"$!3
5!>@r
EG
!#"%$@- }")(
")+-)9:*A+Q*>
)j+-)w3
)9fi#)1$OZ,$/!)+H9
))D:)DU*%$,5!)*?*>
)1[Kv*-BV.0)1*%$,*>
)15$67.8(fi$6+mBP*XBP$9
Z'"$!B0*A+#3
"")9:*?(fi$6+mBP*XBP$9,o:<?>:BV.0)1(
"%)+-)"/BP9
52*>
)\(c$6+CB0*-B0$!9fi+Q$Z7.V.8(
")/B0$!3fi+*-BV.0)+B09fi%.03fiDB09
5=*>
)
4:.K9
E


fi
G
Y8

Ga
{{!!!fi
@fi!


Ifi

6

1

3

8

4

7

2

5

dr

6

1

8
2

(a)

4
5

2

8
7

6

(f)

rdlu

7

(b)
urdl

1

3

1

8

6
2

3
4

5

(c)

7

druuldrdlu

uldrurdllurd

3

1

4

7

5

6

2
8

3

1

4

6

5

5

(e)

2

3
4

8

7

(d)

u0!
,\{}0!:M
#0v
!:0fi=%
06%,K,-

!:7K\
N!#'!fiAO!A
-0!1%
1-fi#C0!v-

!67K

H
%!
uKu0,:0,6%!}:#-0cv0,

0!fiM-#-0!8'%
MKGOxO
:K:0
 !#%!'!fi#!?:KYKJ
PfiC,0
Y%xX0!\%
\

?-

!670fi-J0?
	K-


0\fiXfiffHP%U
dO
fiA
nK-#fi#

-K--K0fi#,%
1!%
?
!AKT-A%

fi!-vfiffi-}
?#
:}!#!'#:0Gfi
J0
 !
}0,:0,6#-0!,!#fi7V v:0
A}

K--0fi#-0!Yfi ,
!%
GfiN
0%
 6fiN,?%P:-# xO

 60%}
?-fi:

0:0
{ 
-T-fi%!
  Ofi
  !ff#"!$%?:K%@0@0{
fiX
fiK-{2L:

 [ ! 
0
!
Qfi#0
=0!=
v-0
-0!fi'&
H
1fi:A=c6C0-0!)
 (m+
 *P{u0!
Y2
-:AQ
0:0-K7-AO1fi
1c6A`fi6mPXP
(' *?%
-:AT
,!:7-A%!,
,-V0c6C0-0!fiv26
vfi%@, @
2-VPY:
jcAT0

!6-#

 -?:K{,!%-
,Yfi
 ./L
 0Qfi#j%
j%!P#'-V0,fi:C0-0!@KT6
vc

 
!vK
0jX0vfi:C0-0!IT:
vfi2
 1/cfiU-Y!8?
v#6%\-V0vfi6mPXPUKT:
vfi@

3 0!
,
%!:00
4
 .fi!


 }!
1
%!!Ab0:

 5
'!v=!#%!'!fi#!fiO1-0!N

.fiA-v-

!6[Y[\
,0!'
,A:-0
Iv`
!
!)
 6/0
 .fiA-N-0A7
 5!2
,:K/
 5
'!c6C0-0!9
 8=`0AN!67Gc6C0-0!
 G
Y-
v0
@fiN
2K170!+
 =!#%!'!fi#!
:);< =?>A@CB+DFE]0
G Pvfi-NfiO1!#%H 8:0
{
2fi6C0-0!I
 (- *\0
J
 .fi!
!nv

06K
 5L!#!'!cA!M%fiMA7
 5!}X0,?fi2
?:K/
 521
#0J!67fic6C0-0!fiM-
xP

fiv:KN!#%!'!fi#!Nfi!N
!,70!+
 fi0
G
 0,fi-
 ff@fi2
 .fifi
xfiv


L#MHNPOQPRTSVUWCUYX[Z\RT]_^X`a-b'ZdcCe'^Zd^OCUR	Ufb'XOSge'^	R	O^VaRTOh\ZdXaY^^gi\Sb'^OCUWUYj^UY^	R	Sgj^aWaRTUYj^aHUYj7R	Olk	b'mb'OknUYj^^OCUfb'aY^
o X	e'`Ufb'X_OJUYXpRlcaYXqCe'^ZrUYX\UYj^de'^	RTaYO^atsujCb'Sjvb'U o jX_`#e'hUYj^O c7R	a o ^b'OCUYXZ\RTSVaYXTwPXc7^VaRTUYXa o WHhCb'aY^SUfe'x kTb'm_^ o b'U
O^sZ\RTSVaYXTwPXc7^VaRTUYXa o QX_`OhqCx o ^yRTaYSjKMnzuj^x R	aY^ o b'ZdcCe'x o UYX_aY^hqCxUYj^e'^	R	aYO^Va{b'Ob'U o Z|RTSaYX	wfUR	q#e'^_Mdzj#b o
RmX	b'h o UYj^ o ^afb}Re/c~RTa o b'Ok o UY^cfisujCb'SjsGX`Ce'hq7^tO^^Vh^hbQHUYj^[UY^	R	Sgj^acaYX_kaR	Zb o Xc~RTC`^M
~

fiG/HKC/dn77///	/

T
Y
HC7	
F+~P\//	
/P7	|?TH7[d//,d-
		/fiy}7H	/+/	 T/

,~-,g
HC[gt7/+tT+
	
Y
-C_7	KT
	+/	/dH,7 -,g
G_ut	K		+P

TJ__7+fi
 +gHfi	/lH,g
?!|-,7_gGPt/gt/	/C/dg
	/	K/
y}	/+//7/	/C+		/\
+-,g
H\7HJ7T+//CK	,[l/C
/C+Gd\
Tfi	H|VH\7HH
	?H
	
T/	/\
+-,g
H
/d	?
J-,7_V-_?++nK	?d|-	K		/K		-
HK/P7\	/
	KY_7+

  7H  TH

2d+HCuF	KA	+P+
/
	//	
K_7AC7)H
TC,P
7	 
	+[HTHK/	/++
	,C
|/t/	
K_7r|K	/ |T!+\	+/H7+{Hg/ d//
T
Y_7+nH T	
Y	Cd
T	_~+/H7+l
?!T7+7nK 7H+
	dH7+Ctd/T_+/7H
	l7J+ldC	 	Hg/	/ /+Y
	KP	y//yK7
C\{77+ /	K+CC\d/
	+_|u
/	
 7T!
C_7Kt7
C
KC	C,	_~+/?C_Cdd/C		
\H_\+C/
	
/
_H/7_9+CK
9+	C	~~J
9-
	gP+J7pT/?79d/?7T+/9
+
C	
J
+PT

	
T+/ 
/

C_7
\
C/
d	+?,	_+/+C[nCH-	G/	+P
H	yC\	H7	/d
_TC
\	/CK	CPCH
/H7
K H7+d|T7H?
+

C/_K2V/7_d/~|/?TH7\	/C	7	
+
/K,	yKP[
	,n2	/
+	[/C
+
~?T+Hd	/	/C
TCPCH
/H	

/K\+P	lH	?+
/C\TH7?	/7J+
/C+/_
y

100

80

60

Accuracy



40

20

0
0

5

10

15
20
25
Number of Training Examples

30

35

40

u
/	
K\G7	+/?/	
Y
d	/
	KY
HC_	
[7T+//	
K_7K	/C		
dH7n	
/
_7H/7?+C
P7
2|K?H
	V+l7[	/7
|/ng
HHtH	/[7	+/ /	K[Pd
		/
y/-g 	/HC7/+/H+	/{	+/
g 7-C7Hd7
/ d/C)	/	
+Cr	K+T[
		
	
Y
-C_7	K_F

P#	/n//K,P
_/yK	)
+J	9/HCCH
Pv}
)	H7
/J
 K	7\	/7	
P	g/Hdl+TJ7H	H7	/?/T
+CC7//KH27
G/d	9T_+/Hd	/?
7t++7	+/-CCH	H7
	TH
H	?T/!KC	C, y//
,Pl
T	+/
H7+ C/H7Hl	/7	+/-CG/
K/	y	CCH	
v	/ -CY
	7H

~

fi
	fffi	
	 !
"$#&%')(+*,
(+(+'.-/102/
35467)08693;:<->=?
@BADCEAGFIHKJMLN2HPOAGQPAROSDTAVUAGAGFSXWYAGZ[W]\QP^_S`J2W]QaS+^&AGZB\QcbedBfQc\f\gdhADijHP\jCGS+fHPkQPAldKfAGADiIkfmJMADS+QPFInMFop
q\Q
AErS+^&fIJMAN9s
\OAGFmtauDvvwexS+FSJyLIzGADdBSj{`|\+JMkHKnM\Fj}$S+HPOjs~SCPOInMFog^&ADCPOSFIn dh^SFi_dKO\+ZdHPOS+H
\QPoIS+FInMzEnyFofiHPOAVdK\JykHhny\FdB\+W$HPOAHPQaS`nMFInMFo&fQP\UIJyAG^fidBnMFSRHPQcAGAVS+FifiQPADdKHPQhn CEHhnyFofiHPOAVdKADSQaCPOfi\+W$HPOA
fQP\UIJMAG^dK\+JMTAGQHP\&HPOIndHPQPAGAny^fQP\+TADdHcOAfAGQKW]\QP^_SFCEAV\+W
HPOARfQP\UIJMAG^dK\+JMTAGQnyFmHPOAdhAGFdKAV\+W
QPADiIkCcnMFoVHcOA~FIk^lUAGQ$\+W2F\eiIADd$dhADS+QaCcOADiVZnyHcO_SOInMoOfQP\US+UInJnMHLp\+ZBAGTAGQDNs
\OAGFdQcADdKkIJMHad$iI\
F\HokS+QaSFgHPAGABS+FlAGTAGFIHPkS`JCE\FgTAGQPoAGFCEABHP\S+Fj]DD9fQP\UIJyAG^dh\+JMTAGQDN+UkH$\FIJMLlHP\S+FV\fHhny^fiS`J
fQP\UIJMAG^dK\+JMTAGQVSCcOInMAGTS+UIJMAfiUgLmQPADdKHPQKnCEHKnMFojHPOAfidKADS+QaCcOHP\_HcOAHcQPAGAl\+WdK\JykHhny\Fd\+W~HPOAlHPQES`nMFInyFo
fQP\UIJMAG^_dp~LjiIAEFInyFofiJMADS+QcFInyFoSdBfQP\iIkCcnyFojSf\+JMLIF\^lnS`JM]HKnM^&AlfQP\UIJMAG^dK\+JMTAGQSd\ff\gdhADi
HP\d)nM^&fIJMLlQPkFFInMFoVW.SdKHcAGQHPOSFVHPOAB\QKnMo+nMFS`JfQc\UIJMAG^dK\+JMTAGQ`N+Z~ABOS`TA^&\QPAdKHPQhnyFoAGFgH
CE\FinMHKnM\Fd
\FdhkCGCEADdPd)W]kIJ~JMADS+QcFInyFonyF\kQWYQaS^&AGZB\QPb2p_q\QVAErS+^&fIJMANnMFiI\^_S`nMFdJnMbAfiHPOAjnMoOIH}kzGzEJyAN
|\+JMkHKnM\F}$SHPOs~SCcOInMFoZBnJJVfQc\eiIkCEASFAErf\FAGFIHKnS`JJMLJS+QPoAmHPQPAGA\+WdK\+JMkHKnM\FdNd)nMFCEAADSCcO
dK\+JMkHKnM\FoAGFAGQES+HPADijUILHcOAV^_SCEQc\]HaS+UIJMAnddKHc\QPADiSdSfS+HPOnyFHPOARHPQPAGAp2ADS+QPFInMFodKkCcOmJ SQPoA
HPQPAGADd>ZBnJJFAGADiRAErf\FAGFIHKnS`JJMLlJS+QPoAFgk^lUAGQ$\WAErS+^&fIJMADd$S+FiRAEref9\FAGFIHKnS`JJyLRJM\FoQPkFFInMFoRHKnM^&Ap
.FQPAGHPQP\gdhfADCEHDN+HPOIndn dF\H
dKkQPfQhn dnyFoRUADCGS+kdhAdK\+JMkHKnM\FfifS+HPO&CGSCcOInMFond
SZBADS+bJMADS+QPFInMFol^&AGHPO\i
HPOS+HiI\IADdF\HSdPdKk^&AR\QAErfIJM\+nMHS+FILdKHPQckCEHPkQPAnMFHPOAlfQP\UIJMAG^dKfSCEApnMHPOAGQVS_iI\^_S`nMFOSd
dK\^AdKHPQPkCEHckQPAS+FiOAGFCEAdnyoFInCGSFgHdKf9AGADiIkffin dBSCcOInMAGTS+UIJMARUgLlAErfIJy\nyHhnyFoXnyHnMFjdK\^&ABJMADS+QcFInyFo
S`JMo\QhnyHcO^N\QnMHViI\IADdF\HOS`TASFgLdKHcQPkCEHPkQPANnMFZOIn CcOCGSdKAJyADSQPFInMFoCGS+Fm\FIJMLOS`TAJnM^lnMHPADi
UAGFAEH`pARUAEJnMAGTA&HPOS+HHPOAVQP\JyAR\+W~SlHPOAG\QPLj\+W
dKfAGADiIkfJMADS+QcFInyFofindHP\jin dhHKnMFokIndKOUAGHKZBAGAGF
HPOADdKAHKZB\CGSdKADdNeSFi&fQP\+TniIAJMADS+QPFInMFo_SJyo\QKnMHPO^_dW]\Q~CGSdhADd$nMFjZOInCcOd)nMoFInCGS+FIHdKfAGADiIkfdS+QPA
SCcOInMAGTS+UIJMApBOAT+S`Jn inMHKLj\+W$HPOIndBoAGFAGQaSJ^&AGHPO\iI\+JM\oLndS`JMQPADSiILfiU\QcFA\kHUIL_HPOAQhn CcOjU\iIL
\+WQcADdKkIJMHadRnMFCE\^&fkHaSHKnM\FS`JBJMADS+QPFInMFoHPOAG\QcLJnyHcAGQaS+HPkQPAfinMFHPOACE\FgHPAErHV\WCE\FCEAGfHlJMADS+QPFInMFop
 kQS`nM^ndBHc\&HPQaS+FdWYAGQBHPOIndB^&AGHcO\eiI\+JM\oL_HP\&dKf9AGADiIkfjJMADS+QPFInMFoNniIAGFgHhnW]LjfQP\UIJMAG^<iI\^_S`nMFdBW]\Q
ZOInCcOAEADCEHKnMTAdhfAGADiIkfjndf\gdcd)nMUIJyANSFifiUkInJidKf9AGADiIkfJMADS+QcFInyFojS`JMo\QhnyHcO^_dW]\QHPOAG^p
 kQZ~\QPbZSdV\QKnMo+nMFS`JJyLSny^ADiS+HRWY\Qc^_S`JnMzEnyFoSWY\QP^\+WrfIJS+FS+HKnM\F.BSdhADi2ADS+QcFInyFo
t~x$tSiIAGfS`JJn.N2uDvvuDSIxap~jCE\FdKHPQPkCEHadSfQP\I\+W\+W2O\ZSfQP\UIJMAG^1nd$dK\+JMTADinyFlHPOABHPQES`nMFInyFo
AErS+^&fIJMAkd)nMFoSF;AErefIJnCcnMHmWY\Qc^\+WXiI\^fiS`nMFHcOAG\QPLNS+FiHPOAGF;oAGFAGQES`JnyzGADdS+FiHPQES+Fd)W]\QP^_d
HPOS+HfQP\I\+W
HP\_SjCE\FgHPQc\+JQPkIJMAl\QS&^_SCEQP\]\f9AGQaS+HP\Q`NZOInCPOmndKkdKHKnADimUgLmHPOAV\QhnyonyFSJ~iI\^_S`nMF
HPOAG\QcLtKjnMHaCcOAEJJ.N
AEJJyAGQ`N
ADiS+QPKsBS+UAEJJn.NuDvAD+\Fom\g\FAGLNBuDvnMFIHP\FN
u`vv
|OS`TJnMb2NuDvvIxapnybA|\JykHhny\F_}$S+HPOs~SCcOInyFo9Ng
Bnd$SZBADS+bJMADS+QPFInMFoV^AGHPO\ei2NSFinMF&oAGFAGQaS`J.N
CGS+FF\H$U9AAErfADCEHPADiHP\ny^fQP\+TAHPOABfAGQhWY\Qc^_S+FCEAp.FiIAGADi2NIHPOAQcADdKkIJMHad>nMF&HPOAdKf9AGADiIkf&JMADS+QcFInyFo
JnMHPAGQaS+HPkQcAdKkooADdKHBHPOS+H~CE\kIJifiJMADSiHP\&fQP\UIJyAG^dK\+JMTAGQEdBZOInCPOS+QPA^lkCcOj^&\QPA&DD
HPOS+FHPOA_\QKnMo+nMFS`J~fQP\UIJyAG^dK\JyTAGQadtnyFIHP\FNuDvvHPzEnM\FIn.NBuDvvIxEp\+ZBAGTAGQDNiIAGfAGFinMFo\F
HPOAdKHcQPkCEHPkQPAfi\+WHPOAjfQc\UIJMAG^dKfSCEAjkdKADi2NBS+FiHPOAjZS`LmnMFZOInCcO
B$d&iI\^_S`nMFHPOAG\QPLn d
CE\iIADi_S+FiRkdKADi2NnMH$ndf\gdPdnyUIJMAW]\Q

BHP\JMADS+QPF_dKkCGCEADdcd)WYkIJJMLnyFjdK\^AdnyHckS+HKnM\FdGp$q\Q$AErS+^fIJyAN
$HczEny\FIndKO\+ZBADifiHPOS+HnyFHPOAVXGldhLedKHcAG^NB$ddKkCGCEADdcdOInyFoADd\FnMHadSUInJnMHKLHP\XFi
CE\FdKHES+FgHc.d)nMzGAVF\FQPADCEkQEd)nMTAVfQP\I\+W.dBHPOS+HdKO\+Z;HPOS+HCcO\g\gdnyFojdK\^A\fAGQaSHP\QadnMFCEAGQPHaS`nMFdhHaS+HPADd
nd
S`JMZS`Led>USifit)\QSJyZS`Ldo\g\i9x$tHPzEnM\FIn.NuDvvIxap|kCcO_CE\FdKHaSFgHP.dnyzGAfQP\g\WdQPADdhkIJyHnMF_CE\FdKHaSFgHP
d)nMzGAjCE\FgHcQP\+J
QPkIJMADdGNZOInCcOS+QPARnMFAEref9AGFd)nMTAHP\m^_S+HaCcOp&WHPOAGQPAXn dlSFInyHcAdKAGHR\+WdKkCcOCE\FIHPQP\+J
QPkIJMADd$HcOS+HCGSFQcADiIkCEAHPOABFgk^lU9AGQ$\+WdKHES+HPADd>AErefS+FiIADinyFfQP\UIJMAG^dK\JyTnMFoWYQc\^S+FVAErf\FAGFgHKnS`J
W]kFCEHKnM\F\+W$HPOAldKHaS+HPAd)nMzGAlHP\_Slf9\+JMLgF\^lnS`JW]kFCEHKnM\FN9HPOAfQP\UIJMAG^dK\+JMTnyFoCGSFjUAVokS+QaS+FIHPAGADi
HP\fiHaS+bAR\FIJMLf9\+JMLgF\^Xn SJ
HKnM^&Afit$HPzEnM\FIn.N$uDvvN}QP\f\Id)nMHKnM\FwpMupMNfopluDwIxap$HPzEnM\FIn.d\QKnMo+nMFS`J
dKLdKHPAG^|g
PsAErefIJM\+nMHPADiHcOIn dfidKHPQckCEHPkQaS`JW]ADS+HPkQcA\+WHPOAfQc\UIJMAG^dKfSCEAHc\JMADS+QPFAECcnMAGFgH
fQP\UIJMAG^dK\+JMTAGQadZBnMHPO\kHkd)nMFoSFgLAErS+^&fIJMADdltHPzEnM\FIn.N
uDvvIxapRdKkUdhADkAGFIHdKLdKHPAG^CGSJJMADi
D

fi2GDDB~++P
mcKDlE+&IMDPIGgh l  P IyGfiP EI`MD  P  EMI
E&IMD
I l ~ c+MD$Mfi 
 G EK    c IK K KMfiM Y c_+KMfi +
IGP PXyD I _ c ] E+P aR EI P cIyDfi P B c MD cIy +K E B 
EM MD P _  EG) KD ] lK+MKM  P  $ EMI KG E+P
 DK B P+MD jKE+&IMDI jPlMD Pj lK K KMy ] Pfi+KM
IM  E cM)MB  EMI _  PDPgRDPyX hK ] PIMDI h D
P P E a) mEeI+KM  PDKIMa BM _ P ]a IMDl   + I++KM BhD
2D cIy G+ KDficVMD cj cMGg+ Il` ]KM&R P IMG h+  a Y  IM a  P
MGK h hy   I_`M&E I IyE~h&K P Ec PK &$K K`I E&IP Iy 
+I G+KM  R G PMD PIM _ c ] E+P a G+ fiKDPEI`M _K EDP

M;I_`MR   I $ EM  EG_`
  E)M hD
M 5 G`MD g M  G` +  K` $ a)M fi + G & aI y I  
K I` P_ IM D j. KPG_   +VK EDP ]I yD Pfi Pg)M
  g` aI hy RM&I cMK + G XI IM l K g K _ $  & PK EDc) E
M cM)  ` aXG~`
 $E+Iy M  g $ EM I  g $ P GcKM   I+
M E P E9g)MKM GcKM &  I+ jKM M E c Egyhy GPKM &  I+
+KMD fi+ y E c E9g)MKM +K I PG PDhGgaKM` K E IMD
BM j Vh K`I E&9gP IM   VI_`Mfifi`MI cDK IM ]
K EDcM
MD PIM fi_ P ] a+c aM  I  Ey
  cG ED G BGG  h  $ ay l+
g M _`  hy _
 c B P E)
 KM I fi l E+P aKDmM 2 &9 a+P a  K` $ E)M  PV  +
 PfiRMK EE IM  fiMD cIy & +IK KjEePGI
X aP aR P
`Kj RP&MaMD cIy m&G y  RMD PIM j& +IK I g BMD l
I   Ea P EP D  & a+c a IIDV  _ EDPP  aP a
 G_hE D ` aG`
IK DhaB + I lM M EG`
MBBB EDPP _c   EDPP&I c  a+K E PG PDhGgPDfi a+c aP& Iy VKGDI
)M _ c ] E+P a  BM   YDP P aI KM I [_ D I_`M;K K`
I E&9gP IMfilK cMGIcjM Y l j+ P K+P EMKMXP+I m_ P ]9 a+P
I  &fi`M g`$ g M     P EDP l K  YD+c Pl aI KM j
I  gEM   I M &l I E   BM lKfi`I   _ P ] aP a
BM K& Mj $Ky + ) yIP
IG`
ID K 9DjG e GMD B+ $ a)M I yD Pl  E P E
]D+P P aI KM `M BM & _ ca IM  ) ID  c$cMD P& _ PRa IM
E+M& E+M& >)M jIMKMIMfiE+IyDcP+ I +c&  ]D+P c + E PDK9
P + G E+M& Iym I&G e B  y   g a M ]  g  EM BM
lIP) IMc EK P E&I_y ] I M GEll)MDMIP P ]D+P c + E +
GDP  I a j. I  G    P& c + I E P EKGDIMD cIy
`  hy  lMD Pj E P E ]D+P c aI hy +  _ P ]a IM P E&IMD&
K+MKM EK c EPD P +_ P ]E IM  I
   +IGP EKM h K`
I E&9gP IM BM E+&IMDB  aM G a` EI hPG El fi P ]a IyD~ 
I  a+IPGD 2  P$I  g`KaP
+R B E+P cMgK P$P+K D   y
 DKR P9 PKMDB M G a` 9  a +I
 B  M_ 9lD)M


!"

#



-$








/.%

3

$

:



C . 

,*

F

3

fi



3=







K

Y

.%



,* :ffSF

*

(hgie 4





4

Zo

4):s



3



"

3

/.%



K fe

B*

OffSF

,

.

K

4

.  0

.

$
Q

+$

$






AE"F

	2.%


Q



	

3



4

,

,4 

%

	.Q

.

,W4

3



7. % . 1

n.





4

,*

,

4





Q

4

%

W

Q

:

O 
L(MON_P:m

,4 

%

2

3

.%

BQ

)..

3 $

 R.

3

t

4

u

*

BvQ



49$

/.



n$

).



4

.%



4)J$

/.



*



	.  0



:

3





-

3

,rTU 

,



3$

6.%

R.







3Q

0





..

aYE"F9ff





 .

3

<E

>!"

6.%

n

$

Q

/.

/.

W$







3

W

,4

%



K



,



#





4

.  	0

1

.%Q






i4

3 iffEA



4j

j. % .

. 



3

>.

r6A

3

. 	

B4	

R



 I.%



/. 

,j.  .

3

4

Y

3$

"!"



%

4i R

	.  0

1ff6	



%



% .%Q

 +* 3

4

K

3





_EY

3



7$





J0

3

14



4

"]z

)..

(X 

.  0

/t





4aYE"F>

3

K _e 4

K] 

#



* 4

nJ0

i4



/.

 /.%Ql.




	

9.





4

B

4

,fi

GAE"F

fi$





,4

D :$

y

Z



,*







+$

. 



hgie 4

Q

:

@Q

3

j$

3





. 

 ).

 ).

3

K

ff



)..

^$

"L(MON:P_m



v





 .%Q|



O.







 X



B



:

	

4

(<;<

a.

1



4



(

.

Q



44

4

K

f.  .





.  ,

4O$

7j.

a4 ,*

4



 0

"!"

3WAE"F





j8!





Q



fi* 

,

).%

GAE"Frq+.%



.%

.

3$

3n.%

:ff6}

.

4

4

..

 3

3



3

 

74

!



#*

"

K

.

%

Bfi



HI.



(

p

gyz

 }~

3

/.%

4

_

B4	

0E



`

(i!"

,XTU |{f!"



I

)..



 B*

1

,*  J 	

4

R






4 ,*



3

(



4

%

n

3

).



lYE"F>



3

xw .

3

,4



f

	t

$



j] 








ff6	

8

K

%

4

Q

.



,@T	U6VffS!"

2

kg

f

$



.

W

3$

).%

 @.

'



45

 

B

7L(MON_P

G







:] 

	



3



).

,Q

BQ

4

.

8ffSF

3

9

 

3

,0

%

9.  .

.  0

!2



9



.

,fi4 ,*

,5TU dc

j

R



b.

f. % .





4

,

,4

4)Z\[

 

/.@ A

 ).

.  0



 




.  	0

,

4

 

 

 ,

#:



,

3

Q



 

% ,

3 fi$

3

HI.

fiff

 +*

8

D$







2

>ff6	?



:.





1

,Q 7

LXMWN:P

3



3






 %

36.%7 



31L(MON:P

$

1.

,*

).

3

4



 	

&(')

,


.

/.

4



%



4G.

).



#*

 B





.  0





;<

$

3



).

.

3



+$

.  0

`'

* #

4



A

#R



K

.%Q

4



}(=Yih@ d#A%/%h""S#_%}|h%h@n%n )#h
#6#S>#S%h63h3
,  h33%"Sh`h
#>|hd%r+%%3|h#
9h@3S
#A3
#)W>3#x%W+%}B ||h#h3
h

+

fipxAG(A	x7+x7	)i)b)

l%%lj5,-)I	1j )	,,)x1 )31,@A)#/KnR#6K
 ,5_3fi#,,)K2% 6"/#d3)-6#( 15z3ZZ,SGY):63B	R%)
A)+b	3A>% B	Yv"K6fi@R	)63%6WKR )^,W
 
l	63,(
,:R#a1,1O)fi5z`,,j%f%lI%,zR 	)	BB	)O	D/#2 -)zKZ,9z
 )%) ,I-6Kfi3)))%,WK1GWR#,v3K)j%	,fi)3",@)
1 %,3,:Z/3,j:I%,z@ )	,,G		)63%%,z%@bY)K	1563Y)"3
,z	6b%GR#W))zSB,
W%%WZ("")	n J
 Z/%,n ),,)Y3	)63%6



@/O,3r,z%,G/#("DBvW3D3=5	Bfiy,3/
	63R}

% BAK)_B)	(6	,z%) -1 )35:,3,W")3>R 

,jK7<8)B	 _R#,	31	,i 

 ,j

5,)1#





% B#K67 3R3/393)9zG6#  )9%bO"il ,,^)

6+B zfi1 /fifi	



3,B 6`,9	,v3)<)`,%



,



ZZBvR W%)3n ,5_%_ )	,,))
/i)%j)3@R 

,fi,3%B8/-) %/#

	K/`/V6%



 ,fi3)

ff
fi

,)7i1+,

)3/W)	87ZZ%Z,_,83_v:3,:,3,9 3)`R3B	)
7%R	B
3,#1A%i63)`G3,fi/O,iROZ"B:`,Z3,i,13
%_%,
)%R3) 19fi 3):,7\W3"6#,,i,R



R	63

/
	)363

1 @13j3%,R3,+,v,R#(,3,",jO,j%)3ZZB

@3fi



"G3

63 j3)



91n

 r,i



3, h





3Z,3nn-`,:ZbKW63%

33,	,aG,3A63)S%R3,)"b%l3%W3v) A71%6#,,j31,1Z

5	R3) O:,",13}

fi	()

R5^_3%z%l3)8#Z-1
/)3W	_ z
- Yb



:)fi-W z3 
)3v
6}>v)3/ /35"
,fi ,O)3%
	B
3,
:)@v,i)3,:/#A,1Z,1:	,)@")#O1)6#,7 

	fi@33z

1 /

	/ ,)

3fi3
G)6+BV1 /:/:%)319 /9RKfi1fi%%b

3""i#/9G8z3^,G1%fi,	R3,<)%

	1	B

	/,,)-7G3Gfi	/ ,)V 	z r1	z#"/:v/zZ) %G,
%,

!)#Z,63B	

) )3>)3 3z%

BG`,Z3z5R2B%R@	)3,DR#Z

31	z:3n%6#,,)89:"6}d"j3%I	,l%):,6:b%9R#7:5j
)G6#,rW ,3,W,Bi	B-) fi) %d3)7KR31)KBGj zK6
%/#=3)#,`/


"fi  zK6
	b%j3)fi	,fi3)#,Sb3d%WR#,fK)-bZfi)zSB,

G/1v,13
 )GR 	_,,8fi"j	B)-3
1 b}A3"

1 dR#15W)G-R3


,:,3B	)n3)y,,j5)K)j16+,v1 =GGv)W)fi:)%
G,:,3,)}jO xK1,>13-,z%,iG
3i863RKbK



3):zY6	 /r)3r1	,W@`,3,)3>R 	,b%13:)"3
	/3D,IO )63>,1
3:,) n@36#Z zA3(%

$#

3	BO 5/#ZB





1%i
//
)KWR %,@,) R-z:)O3n	 p%zO2#
fi#AR#

)O33)-,) O
63)%,1 	KJK6(Y>j1R#ZB) %
,v63)%B
 Y

%-




	/,,)}



/fi )/#Z,fi,) O
%

3,R "1
	B

&)363ZZ,S':/(>%Gz

WK6J/zfi_ 	)	BB	f#Z,

1G ,:,3%("ZZ)#:jz3B	W,1)	 _3)iDB	zO)	_ )	z/#") )
3,1,j63%)363ZZ,Sfi /6Yv x B-WKb O3d,@/" ,jK	1

!(=/`,3,

,1: 16K,(3)O) n %))r%
)6	%3 (
,

3,6X/%_3Y)W 3z%Br3)R 	)63%6#/@ ,
W)%6%z

)+*,



fi-$.0/21435.066798;:<.>=>.0?0.0@A.0B

CEDAF5FHGIFJ>DLKMNCEOD!FQPRKSTF5U2KQVW!XYUZC[J4KQ\]^_0`0DQJ>a%J>UbScJ>U%JdU2egfh_0Ub\AF5DjiQkllmbno0DAFGIF5Ub\jJ>UZFQpqJ>arobstFhuHM0F5DAF
Je0e4STU0v%JGwStU0vdstFxDAFeb_0U2e0J>Ub\Ha%JKQDOyEDA_bsTFKQDAFJ>\AFG9GIF5o2JdDLJ>`bSsSt\IVO>Cz\AM0FGIODA\9o0DAFG{F5U|\9STUO_0Dha%JKQDAOy
\LJ>`bsTFG5]\AM0F5DAF5`bVNFQp0o2OU0F5Ub\IScJssTVgGIoF5Fe4StU0vN_0oxo0DAO`bsTF5a}GIO>sT~STU0v2WzXY\u9O_bsce`2FSTU|\AF5DFGI\ISTU0v\OHFQpobsTODF
uHJVGO>C!\ADLJ>U2GwCODaZSTU0vebO4a%JSTU\AM0F5O4DISTFG9STUJxuHJV%\M2J>\HGIF5o2J>DLJ>`bSsST\IVSGho0DAFGIF5D~FeODHKQDAFJd\AFe$W
H_0DCDLJdaF5u9ODAK5J>o0\A_0DAFG`2O\AMF5aobSTDIScK5JsJ>U2eFQpobscJ>U2Jd\ISTOU0yE`2JGIFeGIo2F5Feb_0osTFJ>DAUbSTU0vaF5\AM0y
O0e0GrStUJ"_0UbSCODaa%J>U0U0F5DW;H_0D2QjHGIV0GI\AF5aSGebFGwStv4U0FeJCE\AF5D5b5]9uHMbScKMSG
ebFGAKQDIST`Fe	JGZJ>UQF5aobSTDIScK5Js9sTFJ>DAUbSTU0vGIVG{\AF5a%iIST\LKM0FQssHF5\J+s[W]HklnL]J>U2e"O_0DZa%JKQDOyE\LJ>`bsTF
sTFJ>DAU0F5DxScGZGYSTaZSsJdD\AO&ZZ]uhMbSKM"SGZebFGAKQD{St`Fe	JGxJ>UQFQpobscJ>U2Jd\ISTOU0yE`2JGIFe"sTFJ>DAU0F5DQiIJSTDLe
F5\<J+s[W]0klbnQWFH~4STF5u	\AM0FGIo2F5Feb_0orsTFJ>DAUbSTU0vZo0DAO`bsTF5aJGzO4U0FHO>C22U2e4STU0vJKsTO|G{FJ>o0o0DAOpbStaJ>\ISTOU
O>C!\AM0F\QJ>DAvF5\9o0DAO`bsTF5aGIO>sT~F5DhCDOaFQpqJ>arobstFGOdC\AM2Jd\<o0DO`bsTF5aGIO>sT~F5DJ>U2e\AM0FebOa%J+StUGIo2FKS2y
K5J>\ISTOU`bVFQPRKSTF5U|\IsTVGIFJdDLKAMbSTU0v\AM0FNMbV|oO\AM0FGYScGHGIo2J4KQFNO>Co0DO`bsTF5aGIOdst~4F5DLG5WF5U0F5DLJ+ssTV\AM0F5DFJ>DAF
\Iu9OSTU2e0GO>C<KQO4U2GI\ADLJSTUb\LG9O`F5VFe`|V\M0Fo0DAO`bsTF5aGIOdst~4F5DLG9STU\AM0FxM|Vbo2O\M0FGYScGNGIo2JKQFWNHU0FZ4STU2e
J>DAFN\M0FGIF5a%J>Ub\IScKNKQOU2GI\ADQJSTU|\LGuHMbScKMJ>DFO`F5VFe`bVJssebOa%JSTU2GSTU\AM0FNaF5\LJ>yebO4a%JSTUW!0ODHFQp0y
J>aobsTF]0GIF5DIScJsebFKQOaro2O|GAJd`bSsST\IVScG9GI_2KMRJZKQOU2G{\ADLJSTUb\W!9M0FHO\AM0F5D94STU2e%J>DFGIVbUb\LJKQ\IScKHKQOU2G{\ADLJSTUb\LG
OU\AM0FG{\ADA_2KQ\A_0DAFNOdC\AM0FN\QJ>DAvF5\9o0DAO`bsTF5aG{O>sT~F5DWjqOD9FQpqJ>arobstF]\AM0FKQOU2G{\ADLJSTUb\LG9\AM2J>\9\AM0FN\QJ>DAvF5\
o0DAO`bsTF5aGIO>sT~F5DScGODAv|JdUbSt5FeJGJaJKQDAOyE\LJd`bstFhOD<JG9JxGIF5\jO>C!KQOU|\DAO>sTyEDA_bsTFGu9ST\AMsTFQCE\9M2J>U2e%GwSebFG
uHMbScKMJ>DFG{F5U|\AF5Ub\IScJs!CEODAa%GOdChJv4DLJ>aa%JdDJ>DFFQpqJdaobsTFGNO>CGIVbU|\QJKQ\IScKKQOU2GI\DLJSTU|\QG5W9M0F%GIVbU0y
\LJKQ\{SKxJ>U2eG{F5a%J>Ub\IScKgKQO4U2GI\ADLJSTUb\LG9OU\AM0F\QJ>DAvF5\Ho0DO`bsTF5aGIO>sT~F5DM0FQsto"`bSJ4GH\AM0FsTFJ>DAU0F5D]STU\AM2J>\
\AM0F5VrSTao0DAO>~FST\LGHJ>`bSsST\IV\AOZvF5U0F5DLJsST5FxCDAO4aJZGIaJssU|_0aZ`F5DHO>C!\ADLJSTUbSTU0vFQp2J>aobsTFG5WhF5U0F5DLJssTV
GIoFJ>STU0v2]$\M0FgGIF5a%JdU|\IScKxKQOU2GI\ADLJ+StUb\hSGNG{\ADAOU0vF5DStU<GIV0GI\AF5a%GJ>U2e\AM0FgGIVbUb\LJKQ\IScKKQO4U2GI\ADLJSTUb\
ScGHGI\ADOU0vF5DSTUF5aobSTDIScK5JssTFJ>DAUbSTU0vGIVG{\AF5a%G5W!F5o2F5U2e4STU0vOU\AM0FGI\DA_2KQ\A_0DAFO>C\AM0FG{FN\wu9OxSTU2e0G9O>C
KQOU2GI\DLJSTU|\QG5]>\AM0F9sTFJ>DAU0F5Da%JVgJebO4o0\jJN~+JdDISTF5\wVxO>CGIFJ>DLKMGI\ADLJd\AF5v>STFGz\AO2U2eJ>UJ>o0o0DAOp|STa%Jd\ISTOU\AO
\AM0FN\LJdDAvF5\jo0DO`bsTF5aGIOdst~4F5DjSTU\AM0FNM|Vbo2O4\AM0FGYScGHGIo2JKQFWXUv4F5U0F5DLJs]2JssGIo2F5Feb_0osTFJ>DAUbSTU0v%G{VGI\F5a%G
JGAG{_0aF\AM2Jd\H\AM0FQSTDDF5o0DAFGIF5Ub\LJ>\ISTOU2Js!GI\ADA_2KQ\A_0DFGa%J4KQDAO|G5]2KQO4U|\ADO>sDA_bsTFG5]$ODHuHM2Jd\AF5~F5DHFQscGIFx
J>DAFJebF4_2J>\AFr\AOGI_2K5KSTU2KQ\IsTV&DF5o0DAFGIF5Ub\\AM0F%KQO4U|\ADO>sbU0O>u9sTFebvFU0F5FebFe\AOFQPKStF5Ub\IsTV&GIOdst~4F\AM0F
o0DAO`bsTF5a%GhSTU\M0FQStDebO4a%JSTUWXU&Je0e4ST\ISTOU\AOGIVbU|\QJKQ\IScKZJ>U2eGIF5a%J>Ub\IScKx`bSJ4GIFGuHMbScKAMDAFG{\ADIScKQ\N\AM0F
MbV|oO\AM0FGYScGGIo2J4KQFO>Co0DAO4`bstF5aGIOdst~4F5DLG5]!JstFJdDAUbSTU0v&GIV0GI\AF5aaStv4M|\ZJscGIOStU2KQO4DAo2ODQJ>\AFo0DAFQCF5DF5U2KQF
`bScJGIFG5]CO4DNFQpqJ>arobstF]zo0DAFQCEF5DgG{M0ODA\AF5DNDA_bsTFG5]ODD_bstFGxebF5DIST~FeCDAO4aGIM0O4DA\AF5DNFQp0obsJdU2J>\ISTOU2G5WScJG
GIoFKS2FG\AM0FKQOU2e4ST\ISTOU2G_0U2ebF5DjuhMbSKMrstFJdDAUbSTU0vG{_2K5KQF5Fe0G<JdU2eJscGIOo0DO+~4ScebFG!\AM0Fw_2GI\{SK5Jd\ISTOUCEOD
\AM0FsTFJ>DAUbSTU0vJsTvO4DIST\AM0aW
^oF5Feb_0osTFJ>DAUbSTU0vGIVG{\AF5a%GjG{OaF5\ISTaFG9GI_bF5D!CEDAOa}uhM2J>\jM2JG`F5F5URK5JssTFe\AM0FQ_0\ISsST\wVro0DAO`0y
sTF5a]"uHMbScKAMScG\AM0FSTU0FQPRKSTF5U2KQVO>C\M0FstFJdDAU0Feo0DAO4`bstF5aGIO>sT~F5DK5J>_2G{Fe`bV&\AM0Fo0DO>sSCEF5DLJ>\{StO4U
O>C!sTFJ>DAU0FeKQOUb\ADAOds|U0O>u9sTFebvFZuHMbScKMSG\O|OFQpoF5U2GYST~F\AOr_2GIFiwStUb\AOU]zkll4bnLWH_0DNJ>o0o0DAO|JKM
GI_0vv4FGI\LGN\M2J>\\AM0F_0\ISsST\wVo0DAO`bsTF5aK5JdU&`FRGIOdst~4Fe"StUGIOarFRK5JG{FG`bV&KQOU2G{\ADLJSTUbSTU0v\AM0F%\QJ>DAvF5\
o0DAO`bsTF5aG{O>sT~F5DGIOZ\AM2J>\ST\HOUbsTVrsTFJ>DAU2G9FQPRKSTF5Ub\HCEODAaG!O>CjKQOUb\ADAO>s$|U0O>u9sTFebvFZio0DOo2F5DIsTVrSTU2ebFQpFe
a%JKQDOyEOo2F5DQJ>\AODLGhODKQOUb\ADAOdsDA_bsTFGQnNJ>U2e_2GIFGN\M0F5aStUJKQOUb\ADAO>ssTFeCJGIMbSTOUWr^>STU2KQF%\AM0Fr_0\ISsST\IV
o0DAO`bsTF5aSG_0U2GIO>sT~+Jd`bstFSTUv4F5U0F5DLJsHiwSTU|\OU]!kllnL]2O4_0DJ>o0o0DAO|JKM&GI_0v4vFGI\LGJuHJV\AOScebF5U|\{SCEV
\AM0FK5JG{FGSTUuHMbScKMST\QZ`FgGIO>sT~FeJ>U2eo0DAFKScGIFQsTVKM2J>DLJ4KQ\AF5DIST5F\AM0F5aW
 M2JdDLebOUiLkllbnFQp0\AF5U2e0GO_0Dju9ODg\O\AM0FHDFQStUbCEODLKQF5arF5U|\sTFJ>DAUbSTU0vo0DAO4`bstF5auHM0F5DF\AM0FhvO|Js
ScGx\AOsTFJ>DAUJ>UFQPRKSTF5U|\JKQ\{StO4UGI\DLJ>\AF5vV]SWFWT]!Ja%J>o0obSTU0vCDAO4aGIF5U2GIO4DAVStU0o0_0\QGZ\AOJ4KQ\ISTOU2G5]STU
JGI\OKM2JGI\IScKebOa%JSTUWffHUbsSTF\AM0F\IVbobScK5JsDFQStUbCEODLKQF5arF5U|\rsTFJ>DAUbSTU0vJ+stv4ODIST\AM0a%GruHM0F5DAFJKQ\ISTOU
GI\ADQJ>\AF5v>STFGjJdDAF9sTFJ>DAU0FerSTU2e4STDAFKQ\IsTV`bVZsTFJ>DAUbSTU0vZ~+JsT_0FCE_0U2KQ\ISTOU2G9O>~F5D<G{\LJ>\AFGzOD9GI\LJ>\F5yJKQ\ISTOUo2JSTDLG
i  J>FQsT`bsSTU0v2]!ST\A\Aa%JdU]O|ODF]!kllbnQ]0M0F5DAF\AM0FJ>o0o0DO|JKMScG\AOsTFJ>DU\M0F5ae4StDFKQ\IsTV`|VF5ay
obSTDIScK5JsvF5U0F5DQJsStJd\ISTOUO>C!JKQ\ISTOUGIF4_0F5U2KQFG9O`2GIF5D~FerCDOaJZbU0O>u9sTFebvFJ>`bsTFN\AFJKM0F5DW  M2JdDLebOU
GIM0O>uG\AM2J>\9JKQ\ISTOUGI\DLJ>\AF5v>STFGDAF5o0DAFGIF5Ub\AFeJG9GIV0GI\AF5a%GO>Czo2J>DLJ>aF5\F5DIST5Feo0DAO0eb_2KQ\ISTOUDA_bsTFG9u9ST\AM
+

fiq0j000j4q0+q42020002

I%+20Q24TIT29>A9QRT5|{ttdA2>bTHTbQ> 9 2YT
 Z
 A5b	ITA0 wtff

c>AA2>Ndfi	IhcI0b	t"AQTbQQ55|T>AbT0 |zcAT|IQT
>00A" !bT%>AZA0ZA05 #N4QIT&IAQ>A LdA05NA2>AT>A"A0Z0IT%<IL>A%$H0
T|5AIIT
 Qgd2 0ZAbc>00|cZA2> &0b	 TA0 0AA5bgQtbELQ5r5|ZT>bt
 
5A000'
 (fi)2AIQ+
 *-,!>A ."/01TAA>.
 .29AA>2bQ
 *43NdQtbT
 5/bT2h02>AIc 

bc>N4 btI6
 9TA0bT5%7
 h05AA0{L>AhcHbT2>Ict42I5 +dbt 
8:9 ;=<?>@<5ABDCFE.AHG
I 
 >0bT0J9AKLNbc>0bT&b4%T2ZM2AA0%bT00N9IcO&<r05A!52
0
 AMbTLhAIc05bb2A0wN{2QH2>9T2	2bP2LILb5HQdIT2!0A4c5>A I 05A
>AZ%d|2T5 tM2
 AbT2 
2 2Q50T>AbT
 A42
! >bTT&Q2
 AIAR2
 Q0
 L
I
b%+t2hc'|
 0SHA2xT|LQL>bT6M
T S2
 YT5=UV
bc>2A2>9Z24
 A
! 52
0
 0bQAtS}054
 T20>TbEA%dIT.q
W 2
! >bT$
 A0T>A05NZ	
 btS9
AN|ITL!SHZ0AbT5%ANA09A05b
 2>0
 L00
! A52YTAN0<2dL4	
  d5Z25QIbT
XH0
 5ITY
 >
 	b
 t.&HV:
fi 0b%5+Z5V"[z
 A52ZI50
 T>AbT
 r5A02>!T>A2
A0
 Lw\4
 bQ2bYTITb
 TE9bd0btr
 EA 2
! >bTHd25Z25LIbT]XH0
 5IT(9
fi 0b
I b52Z&*^9
fi 25S>
 T>T=H[V)
 bQ2bYTITb
 Tg0
 LY	
 QTbQ2bIN 4
 |TbA
0
 g5>HM0
 
 |+x>20ITZTI	
 QIT2N
 TA0K_`acbdfe7
g 0AH
 L>]
 A0ZT>bt
	 {t0 05A9050A72
P 2Zh
 505L	
i >ITr>{5>2|YTI	
 H2
! >bT=h
T S<4
 5t00
 
A0Hr5g5LIbT6X0
 5ITH2
P 24T
 NbIjI2j
P 
 505Q\
i dIT%d{5j>$q
! >rbtc=b
 0SH
A2lk,=E
 2>Lm(
T S2
 Awt5"JHV I 0nXH0
 5ITS
 T|AYTbT&l2
P 2ffD
 505L+	
i >{t4
T2QA55bLT
 |c
 5{E4tch05A05TA5LT 0Q424t{t4	cAQT>
 >|A0Rb
 t
o xbd
! 52Abc9
 AA+\E
 {trb4%T27h05AgQ{t42>02b5A5ZTb{Ic>2
bc>0bT
 >20
! 0
 IT>AhtbA5IT"
 O
o I0S<"A2>r0<4c%d0bc5>bTA0M0254cIffI5AITtpHbcAff0%>

A0590AScbH{>	0IT29AZ0AbT5%q(rs> *ut|405v"[0xw2"	O.yUV|59Q
A0
 02M 025 4cII5IT
 H05AxA0I>	 0IT2>A4505L>A|I>QA0AHL>zM1TL
5Y 	H[0{tTbA.
 y0 I b52+Y O|UVvk>LdLS }A>N
 VUQS 904THQ 0L{j25M 955
I
A0IM <9 !0AA5~  0A05$cM 0NM 00bTAhA0zT>A05!I5>Q>2bZT0|I5
  !05LcI ff]2IQ b<M 000A4bt5NA2>0QT&I>	 ZA0Z0A4bt5NA2>2> 0Qt05 0TA0
b%+t.
  I bc!c5
 5Ar
 2YTZdHA0 !05LcI0Z	 |5
 P22>j052>xA !0j| 
bT
 69TA.
 $A 2Ibg>cT| L>IT4j 5A5bIc XH2>IT2~  I 0NT>A05hcH XbTAA
Q
 5 xT2>Tb0Zc<{tr%>2
 9TA"2>Tb0Zc<
 0Z25x>< !05LcI k>Q>LS }A>
 VU
0AS 92>HA0ZQ24TIT2Q bRT5|E9T>AbT
 rI>	 0AbT5%N<tAbcH2d25N>A
M bRT5bHET>AbT
 EA !5Q{N5
 <QY 
$H09>$A0HA2+T5
 >v
 02Q 025 4cI%{25
 0rtdAbT
 NczA0  
 bTITbthT%S 90A4bt5K
 
LM 025R cII5
 0T>bt
 x92M 2TAQ 0NA2>H0NA05 #HId\ 0{t42>jQ
 

I
YcIA5b)
 9TAZYT
 >T0A4bt5I>	 59TA0|b24A0YcHI2Q  0NAI4%A2d<b<hQ 2c
cNA2dN 5A0bT5T0%b%T2" r
 bTITbTI>	 0IT2
 [tS  V Wq !q>rbt 
A05A>A7
 2M 2+T>|ZAH
 0A!A
 4AN0 #'!>RQA4 0 b>2ZA05H%Z2H%d|
 H
>)
 P!bt
 bcQT LA0Rd2I52Q>A4A05
 H052M 0AZA2dg<0RId\ 0{t42Z>HA0
 !2>bT>%Q2YcIA5b
 9TA&wt
 dt0AbT5I>	 5" $A0ZT>A05x2NAbcb05Ax
L>R 50bT5I>	 5gQ42YcIA5b
 9TA
 >	 5&I5N><{>	 0IT2NN04 L"A05
 <4L0 T
2AIQTQI>	 0IT2x>H0A4bt5TQ 2A6
 HA2dg>TIZ0rL> 5N0bT5 {>	 5
{

fiOH)-+SS

"	7hV"			VR.M	xMH%QS	K		l
MffS\H=	]	S	M	lY@J"	j	6&	J=	H6MR"
MR=Sff\cMM"ff@j	M)7{	SNR]"VS	l
"j	S+KN	SRj+SjfSfJV@\)	lS6RS7RSqQS	
HMxH	M{Z&7j	6'\	R xMRSJ{\	RVS	
SM		KHxVS7S)OH	S+RMK"6
SD	pV"7	FH"	. SM	"qHMV"	 Sff"H\
MS	h7R	S7MhJ6R\S5hSMSM	]Hj	
 5YS))"7KJM@\Q\HH"j\M	]HS"	=YKSH"	

j	J	)MOvRj	:@]HV"j	SM	@?S	\hHj	
KM MS	M	KH\K7j	NH	6M	"QS	M	h=S 
S.:"xq@^SHM.&{7@qS.+hRKM@\
UMS	M		DMHS	 M	 &	S	lMMzM@ M  RVS:j	
5)R\SS	 M 7@fj	ff\6RJMHM]Q	S	ffV@ )ffH	
	qH7lSH MS	M	@S6QS	]S:&	(S	
 jSN	M	\	@R		DMRMpMD	S	HSff\
K	SS"	"M	VM	"M@\Q\H]"	\D	~7ScMRV
DMR	MS	ffHK)"cH6RMjvR	@J]Q\Q\
	FQS	5S	j@M		SR7S7"	7"U?V"j	)-j	 6M@\Q\H)"
"HMjvMS	M	H"j	MY"j	S	D	l	D	S	QUMR
S)@7RVSK\S6M	{.ZHM	.
=.5Y5{.f
l5M7j	"VS )MM	SR\xf7=)RJV"OH
	NS=) H]=@\"	SR\5(x7&	S	)h	'	
7MV@SVQ&Svxf	S	JRq?	S		@M	S)5	S==H
6"	YMxRv@M	c	D'S7"SSV~O7JQlM"	l
x7MHSR	FMS	V~Sff\h"M)J@\+\6H7OVM
SHHQM	M		SQ\H S=	?7 xVSHV.HfRS	5S
)ff	S	OS]QO	R\@q"@(\HMSM@\Q\H
7)RR	M	K	RVSff@+Sh7HJ\R@+
 j.%SSM	HM	S	Oxc	S	xS 	M"&\@		]SRM@
H
llMS)p{zRnRS KS )SxlMcM	xl	S	"	Q\RS
	SlHj.'HhS+S=S)	.Ql@	SKHxVSf
	RUR	D	S	lVS )RD6Mx	S	S6SR6M	S	H	jS
MM~f)6x]l"]Q{hc6M			j@M	(c@'"jM6 RUpYp
)7SNUff5)JRJ:S	"	f\RMR)MVSM	5S
"	M	 	zMS	H\xqS	RFVRMS	S	]%\ffSJ{	M
SfMS	M	SJ"Mff+HVM	


=f]	
x	ff

)KVM]@ff6QRpl@M	"R\cHSM	]Sx
(YYfiS6R)6@+"{{)MSV@xqYfiSfifi5l



fi"!#"$%	"#"!&'(")+*+,-&&.0/"-210&#""3"45306

7980:;"<>=@?ACBDAE?798	F	GIHJ:K9:LNM80:O5:K9:;QPF	=	?ASRTPVUW7X79UWHYP[Z98	F]\^HXUW;`_a79bJPc?;QPF	deP[ZX<gf@O[:;";	F]hHYP[LQ80:H
i :80:LQUWj:;	Fh79UWjU i Pc;Q79?;	F^=@?A i Pc7kZX8"UJOlOFnme:H9;"UWopG]UJOqOF^BH9Ar:;0LsGIHYPcULPc7YP[KWF:;0LtM80:u;0LQHk:
v UL"LQoxwy?H%Ar:;QozPc;7XUWH9UKY7YPc;"{pLP[K9ZJ|0K9K}Pc?;0K%?;~7X8QP5KE79?QP[Z=e80:;"<"KE79?+sPqOlOqP[:A M?8"UW;	F v ?;QP
 80:HkLQ?;	Fh79UWjU i Pc;Q79?;	F^:;0Lx798"U>H9UWjPcUWeUWHkK?uw(798QP[K%0:UWHw?H7X8"UJPVH7X8"?H9?|"{8p:u;0L8"UJOcQwy|QO
ZJ?AAEUW;7kKF:u;0L79?GI:LQAr:u:B<<:Hk:u|Ewy?H^8"UWHDZW:H9UJw|QO"H9?Q?wH9U:LPc;"{0
2"900"
B;"{Oc|QPV;	F	RkQka^|"UWHYPcUKn:;0LZJ?;0ZJUW"7TOcU:H9;QPc;"{0Tg[JJ0F"	kF0""
B;798"?;oF i cF0maPc{{KFdkQkTQ0@	kk[Q2	JJM:A"HYP[LQ{UT;QPcjUWHkK}Pc7Yo
GIH9UK9KFdTUW
I?HX<F0dT
maOc|"AUWHF]B5F	_I8"H9UW;QwUW|0Z98Q7F@BcF	:|0K9KOVUWHFR5F	z:H9A|"798	F i ]kQkU:uH9;0:QPqOqPV7Yo:u;0L2798"U
 :";QPc<QYM8"UWH9j?;"UW;"<P[KnLPcAUW;0K}Pc?;	TJ0]YnagF"kF0"
moO[:;0LQUWHF=n@kQkM?AEQOVUJQPc7Yo>H9UKY|QOc7kKTw?HnKYUWHYP[:OLQUZJ?A?K9:QPqOqPc7o;zekJJ[QnY(u
[IWWYknnJ klq0JF"	@0h:;>?KYUFMBBDBBDeGIH9UKXKW
M80:O[:KX:;QPFG]cF_I7XbJPV?;QPFT\cF i ?|";Q7FI"k"uk>RUW79UZJ7YPc;"{:u;0L+UJ"QOc?Pc7YPc;"{LQUZJ?AE0?K9:uQPlOc
PV7Yo2Pc;z|"L":7XU{Hk:"80KW;
2k0k0eWWYk%zek0qrYelJ
 Y0N0  JW0F"	"M:A"HYP[LQ{UF i B
M?8"UW;	FQkQkDK}Pc;"{rLP[KY7XHYPc"|"7YPc?;"wyHXUWUnOcU:H9;QPc;"{798"UW?H9o79?:;0:OcobWUK?Oc|"7YPc?;0:798ZW:ZX8QPV;"{
AUZ980:u;QP5KArKWn^	J0qlkF0kF""
RU?;"{F	cF0 i ?Q?;"UWoF v 	JQJ_]"QO[:;0:7YPc?;20:KYULOcU:H9;QPc;"{0eB;+:Oc79UWH9;0:7PVjUjPVUWngu
W0JJ0FF	"
_:uHYOcUWoF"]kQQkB;+UJZXPcUW;Q7ZJ?;Q79UJ"79wyHXUWU0:uHkK}Pc;"{>:Oc{?HYPc798"ANc>Q0J["IgF
	YQkF""
_I7XbJPV?;QPF\JQJB~KY79HX|0ZJ79|"Hk:O7X8"UW?H9on?w0UJ"QO[:;0:7YPc?;"0:KULOcU:H9;QPc;"{00nJ k0[qq0JF
	JkF""
(:uH9UWoF i 5Fu?8";0KY?;	F0RkQQkTTQX0Ek[quC^Q[N	QkJY
DkTq0kW	"f"H9UWUWA%:;	
DHJ:7kZX8	F"cFpRU?;"{0FQJQJIu"]B"H9?0:QPqOqP[KY7YP[Z^KY?uOV|"7PV?;7X?^798"U|"7PlOqPc7Yo"H9?QOVUWA
PV;>K0UWULQ|""OcU:H9;QPc;"{0e};+YkJk[Q^Ynn[0IWWYknJ ek0[qq0JF
"	0""h:;>?KYUFMBBDBBD^GaH9UK9KW
DHXUJPV;"UWHF v Qk"uk0f@Pc;0LPc;"{7X8"U^?"7YPcAr:OLQUWHPVj:7YPc?;>KY7XHk:79UW{oPc;:nH9ULQ|";0L":u;7<;"?eOcULQ{U^0:KYU
nk [0J0qlkF0	JkF0"
DHXUJPV;"UWHF v cFt|"HP5KP5ZW:FJ"kQkB~KY7J:7YP[KY7YP[ZW:O:""HX?:ZX87X?(KY?uOVjPc;"{798"U_me%|"7YPqOqPV7Yo"HX?QOcUWA>
;eYJkJ]Yn0Y0J(k [uk0[qqkFJ"	0J""h:;?QKYUF
MBQBBDB^GIH9UKXKW


fi"0W	ff
fi"9

!#"$%'&)(*%+,-!.0/1+2+143065,)78+9)8+0:<;=>?@4&)@:ff';BA:ffC@DC&4A(E6FG9AHIJ):ffK
LM8+IN:AO:ff):AK(PQ(8ffLRTS"!U5PWVYX=Z[\0\]+^`_'a<bcZ*de0_$fg\X_)h+fg^`Z+_$h+ijZ+^`_)f![Z+_d\X=\_$[0\kZ+_mlnX0fg^ oY[^Qhi
e0_)fg\iQip^pa+\_$[0\ff99Urqs+s<t)qs'14uk;CC8ff;wvx5vx8C>':ffzyn:ff&4LgIN:ffU
&9;{):Y|B!}|k:ff&UYuJT.0/1+1+~430;C{x78+I94A4;K8<L4A87C%(J8+=AQ@94AQ:ff4>)lnX0fg^ oY[^Qhi
e0_)fg\iQip^pa+\_$[0\)+.~ff,430~+~t~+ffs$
 :ff&)(C(PA)uJ$.0/1+2+1430r"r:ff4>z78+ff=&)7;=+78+)79;0(Yx(*;CC&)7;C&0:A@48IN:)(T?h+['^`_)\nU\h+X_
^Q_4a+rqt4s'
 8+9O7C8<LM;n-ApAIN:ffUB-n.0/14q1430e0_)fgX=Z]'[fg^QZ+_fgZl'fgZ+Jh+fgh	U4\0Z+XUh+_'a'ha+\bNh+_$]
Z+O4fgh+fg^QZ_)!Fn@@Q(=8+g(PAK+
yn:ffA4Ap>)"!+"O;;CIN:ffU+vff"!+v8'8++FJ.0/1+1430+4Lg8+07I4;!A:ffC4>)EF(=&+K+
jZ+4X_)h+iUZ*dlnX0fg^ oY[^Qhire0_)fg\iQip^pa+\_)[\\b\0h+X=[')~+4qt~+2+
yk:ffC)(v-:ff0:ff4,'BJff.0/11ffs30l_e_)fgX=Z]+4[fg^`Z+_JfgZJZ+O4fgh+fg^`Z+_)hi4\h+X_)^`_'aU'\Z+X0ff
 {v50  C(C(Y:ffIJ=Q@4>++vF
yk{):ff0@48Un.0/1+1+30"r:ff4>W;C8z;:ff%+:+7;=8+)(J5PVYX=Z[0\\0]^Q_4affbZ=dnh+fg^`Z+_)hiZ+_d\X=\_)[\Z_
lnX0fg^ oY[^Qhi)e_)fg\i`ip^Ha\_$[0\+8+C;=AQ:ff)@rr
yk8+=LCrn.0/1+2430v?:7C8+g8+9$0:ff;C8+(EY:6:ff%zI;C{8@mLg8+A:ffC4>)lnXfg^ oY[^`h+ire_)fg\i`ip^Ha\_$[0\O4ff
+t)q+q4
"U:0@r4-+84(='4A848+Ix++|YApA,+FJ.0/1+2+30{4&%>kN(*8':ffE  {:ff):<;C8+IJK8<LO:>+:A
A:<C4>NI7{):ff4Q(=Ixc?h+['^Q_$\B\h+X_)^`_'aOffU//t's'
"$;C;=A(=;8++|B.0/1+22430":ffC4>?&4Q7C%AK{CCA:ff4;:<;C;C=&;C(n:ff$8+&)@rEkFAH:ff
;C{C(={8ffAQ@:A>+8+=;C{IxTWh+['^`_)\BU\h+X0_$^Q_4a+$+)~+2+t/2
"$;C;CIm:ffU)v"!$Y:+(C(:ff)@40:4FJyn:ffA4AH>)U"))./1+1+43"r:ff4>69)8ffApQ7(9):ffC;=Q:ApAK
8+)(=Cff:ff4Ax4+C8+I6';0(Ex'7:AH>&9U5,VX=Z[\0\]+^`_'affbmZ=df`4\xe_)fg\X_)hfg^QZ+_$h+i!Wh+[4^Q_$\
\h+X_)^`_'amZ_d\X=\_$[0\+99U)+~t4q'v8+C>':<yn:ff&4LgIN:ffU
v';C8U4)4.0/1+1+30&):ff4;=;0:ff;*Y(=&4A;0(78+)74>B;C{&;=pAp;=KJ8ffLr94AQ:ff):ff;*8g):+(=@BA:ffC4>)
lnX0fg^ oY[^Qhi)e_)fg\i`ip^Ha\_$[0\+U.=~ff,430)+t+1/+
v;07C{ApA,  UykApAnUyk@:ff=Y:ff)ApAp,).0/1+2+30R94AQ:ff):ff;*8?):(=@W>+0:AH:ff;*8UEF
&4HLgK>mBWh+['^`_)\BU\h+X0_$^Q_4a+rffsqt2+
v;07C{ApA,  ';C>+8ff'4S:ff,,n$.0/1+2430r"r:ffC4>4KJ9$=I';:ff;=8+UE!Fn7+&4=>z:ff)@
C)4>9C8+4AI(=8<A>W{&=Q(=;=Q7(5Pv7{):AQ(=%,Yn:ffC)8AHA,-v;07C{ApA,  
.R@(30$?h+['^Q_$\nU\h+X0_$^Q_4a+99UU/+tr/1+  8+>':)!:A86FA;C8)F
|k:ff;0:ff0:<*:<USk./1+24q+3A:ffC4>ST848ffA:ffLg&)7;=8+)(5,VYX=Z[\\0]+^`_'a<bZ=dxf`'\/1<QWlJ!
OffZffb^Q4#Z+_U4\0Z+XZ=dTZ+O'fg^`_'a99Ur~+1+tffs)ffFnYvC(C(
|k:ff;0:ff0:<*:<U'Sk$.0/1+2+130!mA:ffC4>mLgC8+I07Q(=(!5PVYX=Z[0\\0]^Q_4affbZ=df`'\~<l_)_)4h+izZ+X
b0'ZZ+_Zr'fgh+fg^`Z+_$h+iUU\h+X_)^`_'aU'\Z+Xff99Urq~t24q'$':ff4;0:6C&+rFJ


fimU+++)W)+)

	ff
fi


 	!#""$' $%'+& )'( ff+n* -,. N/ ffU'0 ff
% 213& 5476
kff0ff0<*<U+98;:<13=)?>@>@ACB$0DD0:#E#&513EF.g0-/+13E#&+2G5H. &+7>I1ffCA5()J,LK#

M	-NOP
	
RQff 
? ff!$3	ff
Rfi
TSU3P3

?ff
 =5U= WVXY?Z$V [ k6 \k6 2]+& ff%'0J 
B1213+^ % CJ6 C8`!_ )^Aa& A 7b 0+4Y 0Rc'd )-/eAgf-hiG
6 513
E )&->I1j.k& nC0?A A 5(\= 2&]>a13/THA lT_ #!m 
,J nK#o?

M-NpPq	
rQ 
ff 	ff!UL3	ff
rfi
LSUPs
?o
?ff
 5=5U
= tuv3Z9tv-Y ff6]5w
1 <131 UC0f+& Q> ffffr
< C%'+& 2'( <xn* -,. N/ ffU
yff51 5Jc 8{z ff2/j, 2U| 
% 0D4u 0C}~A ff<A 5m( H|5+& 21=H H&->I, AI+& T.k& 2|51p`+1 21 ffHoAa& q&-.
2|5z
1 [-w =5,5^3^>I1+AHA 4C0f 0]>a+1 jPJ nK?

M-NpPq	
r !#SUPs
?
ff
jLp  ?!
Qff 
?!!@@

 -=5U= UuDOZ -vY B|A@Q> <1>I=5|QA BJ6 ff%'+& 2'( ffn* -,. N/ ffU
y#1<5<+d Ck4 : <13)= ?>@>@,A $B 8y#& fff -(->@AI&->I)& C)0 $0+4u 0ff:#|513& 2dw (,Ag<1<T13/r=A Agf >H=ff131<,5=T>I1 ff5w
A 5(R&-.('
& ?>ff<1f&/+=&HA AI+& m2,>I1H C,J 'K?

M-NP	
Q 
ff 	ff!4
fi
ff	
SU3P3

?ff
 +ff,A ,J 0?>I+d 
y7AI1=H ffny 0Dv 0mC1 <CA 5(x<1f)AHAI+& '>@A=H H L3
fi
ff	+ U 4t 05Y+Y ?Z5Y-Vu 
y#,ffH2H1>,> R)0 #8!B ffy 0+4[ 0F6=5= 2&?AIN/ ffA 5(&= AIN/ >=&->@Agf)AI1Hx.k+& T)= ffCA >>IdF&]ffH1 2ff -]>I1
=H 2&5f2)
| *H AfO<&N/ ?A ffH CPJ \K#

M	-N#P	
iQff 
? 	ff!5	ff7SUPs
?o
?ff
+p  ?!
Qff 
?!!@@

 -=5U= UXDD?OZ +X )V %'+& '21 ?,> C4UJ6 
0)| ?>@AIrG 9 04X 06pf3,A A 5J( 21f, HAIn1 ffff<RA 21 ffAI1f+& fff13= HE#A 2|+15=Q> ff)ffAI+& 5w )] H1<j>I1 ff5w
A 5$
( 	ff
Rfi
ff	 $ff +t ?ZffvX 
0,5] 0-m/ ffQA ffU$Jc ff8, ')1 C)0 0+Y 0%\1 H, A 5(q, A>@A ?d ffff< 2|51<1HoAa( &.= 2&-ff -]>Id(&&5<
_ #

m ?>I(& A 2|5/qH jPJ K#o?

M-NP+	
TQff 
ff !3	ff
rfi
ff	'SU3P3


 
=5U= 5VYu?ZVt[ 6]1 <131 U0f& Q> ffffr< $%'+& )'( ffn* -,.km/ ffU
:<13)= ?>@>@A #B T0+40T6. +& 2N/ ?>@AI^ ffAI+& &-.O15=> <)ffAI+& 5w )] H1<N/ f 2&w &=ff1 0<2+& >I1 ffCA 5)( LPJ 
K

M	NPjQ 
ff 	!C5O3P3


j'p ?!Q 
!	!@
ff
 =5U= u ?u?Z9uYY 
0d94< 513+d 6,ff*H C0?>@QA ff%'+& 24( ffxn* ,.kN/ <U
:<13)= ?>@>@A B 4+3r] 9m1 ffCA 5(jE#A 2|jA ffH2f 2, 0]>ak1 2|513+& Aa1H ffPJ K

M	NP 
Ofi
5
)NO" =5U= ff[-VV-Z9[-VD 54U|Af -($& 5Jo!m C%'+& 2'( <xn* -,. N/ ffU
:<13)= ?>@>@A B 4+4Y 
6 2|513& 2d&-.C, ffH,5=ff1 )AH1<H=131<,5=+>I1 ffA 5$( C,J 'K

M	NUPOp ff!
SU3P3

?ff
pp ?!ffQff 
!	!@

 -=5U= $YY ?Z9Yt-$V 5'0 ff\-&H+1 C4UJ6 5666JO!B 21H2H 
:-/j]ff+1 5
% '13E#1>,> 5J6 58;y#&H1 ]>I&&/ ffB ff)0 $0+4X 0C:#|51= 2&]>I13/`&-.19=1 ffHAI1f2|, 5G5H ffff<
A HOH&->I
, AI+& ']md 21=H CAgf A 5(T15= 21H2HAI1 51H2H L3	ff
Rfi
+ ff o4t 0+Y +?Z9tVD 
?>@QA ff4ff!m rD-V C
6 2|513& 2d+&-!. 2|51>I1 ffC)-]>I+1 pS7jff	 5NpP	
pj!S Wffff 0+ffOt-V-Z
+3V
Y
kff0ff0<*<Uk+ffr
?< ) 



fiJournal of Artificial Intelligence Research 4 (1996) 77-90

Submitted 10/95; published 3/96

Improved Use of Continuous Attributes in C4.5
J. R. Quinlan

Basser Department of Computer Science
University of Sydney, Sydney Australia 2006

quinlan@cs.su.oz.au

Abstract

A reported weakness of C4.5 in domains with continuous attributes is addressed by
modifying the formation and evaluation of tests on continuous attributes. An MDL-inspired
penalty is applied to such tests, eliminating some of them from consideration and altering
the relative desirability of all tests. Empirical trials show that the modifications lead to
smaller decision trees with higher predictive accuracies. Results also confirm that a new
version of C4.5 incorporating these changes is superior to recent approaches that use global
discretization and that construct small trees with multi-interval splits.

1. Introduction
Most empirical learning systems are given a set of pre-classified cases, each described by
a vector of attribute values, and construct from them a mapping from attribute values to
classes. The attributes used to describe cases can be grouped into continuous attributes,
whose values are numeric, and discrete attributes with unordered nominal values. For
example, the description of a person might include weight in kilograms, with a value such
as 73.5, and color of eyes whose value is one of `brown', `blue', etc.
C4.5 (Quinlan, 1993) is one such system that learns decision-tree classifiers. Several
authors have recently noted that C4.5's performance is weaker in domains with a preponderance of continuous attributes than for learning tasks that have mainly discrete attributes.
For example, Auer, Holte, and Maass (1995) describe T2, a system that searches for good
two-level decision trees, and comment:
\The accuracy of T2's trees rivalled or surpassed C4.5's on 8 of the [15] datasets,
including all but one of the datasets having only continuous attributes."
Discussing the effect of replacing continuous attributes by discrete attributes, each of whose
values corresponds to an interval of the continuous attribute, Dougherty, Kohavi, and Sahami (1995) write:
\C4.5's performance was significantly improved on two datasets : : : using the
entropy discretization method and did not significantly degrade on any dataset.
: : : We conjecture that the C4.5 induction algorithm is not taking full advantage
of possible local discretization."
This paper explores a new version of C4.5 that changes the relative desirability of using
continuous attributes. Section 2 sketches the current system, while the following section
describes the modifications. Results from a comprehensive set of trials, reported in Section 4,
show that the modifications lead to trees that are both smaller and more accurate. Section 5

c 1996 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiQuinlan

compares the performance of the new version to results obtained with the two alternative
methods of exploiting continuous attributes quoted above.

2. Constructing Decision Trees
C4.5 uses a divide-and-conquer approach to growing decision trees that was pioneered by
Hunt and his co-workers (Hunt, Marin, & Stone, 1966). Only a brief description of the
method is given here; see Quinlan (1993) for a more complete treatment.
The following algorithm generates a decision tree from a set D of cases:

 If D satisfies a stopping criterion, the tree for D is a leaf associated with the most

frequent class in D. One reason for stopping is that D contains only cases of this
class, but other criteria can also be formulated (see below).

 Some test T with mutually exclusive outcomes T1; T2 ; : : : ; Tk is used to partition D

into subsets D1 ; D2 ; : : : ; Dk , where Di contains those cases that have outcome Ti .
The tree for D has test T as its root with one subtree for each outcome Ti that is
constructed by applying the same procedure recursively to the cases in Di .

Provided that there are no cases with identical attribute values that belong to different
classes, any test T that produces a non-trivial partition of D will eventually lead to singleclass subsets as above. However, in the expectation that smaller trees are preferable (being
easier to understand and often more accurate predictors), a family of possible tests is examined and one of them chosen to maximize the value of some splitting criterion. The default
tests considered by C4.5 are:

 A=? for a discrete attribute A, with one outcome for each value of A.
 At for a continuous attribute A, with two outcomes, true and false. To find the

threshold t that maximizes the splitting criterion, the cases in D are sorted on their
values of attribute A to give ordered distinct values v1 ,v2 ,: : : ,vN . Every pair of adjacent values suggests a potential threshold t = (vi + vi+1 )=2 and a corresponding
partition of D.1 The threshold that yields the best value of the splitting criterion is
then selected.

The default splitting criterion used by C4.5 is gain ratio, an information-based measure
that takes into account different numbers (and different probabilities) of test outcomes. Let
C denote the number of classes and p(D; j ) the proportion of cases in D that belong to the
j th class. The residual uncertainty about the class to which a case in D belongs can be
expressed as
C
X
Info(D) = , p(D; j )  log2 (p(D; j ))
j=1

1. Fayyad and Irani (1992) prove that, for convex splitting criteria such as information gain, it is not
necessary to examine all such thresholds. If all cases with value vi and with adjacent value vi+1 belong
to the same class, a threshold between them cannot lead to a partition that has the maximum value of
the criterion.

78

fiImproved Use of Continuous Attributes in C4.5

and the corresponding information gained by a test T with k outcomes as
Gain(D; T ) = Info (D) ,

k jD j
X
i  Info (D ) :
i
i=1

jDj

The information gained by a test is strongly affected by the number of outcomes and is
maximal when there is one case in each subset Di . On the other hand, the potential
information obtained by partitioning a set of cases is based on knowing the subset Di into
which a case falls; this split information
 
k jD j
X
i  log jDi j
Split(D; T ) = ,
2 jD j
jDj
i=1

tends to increase with the number of outcomes of a test. The gain ratio criterion assesses
the desirability of a test as the ratio of its information gain to its split information. The
gain ratio of every possible test is determined and, among those with at least average gain,
the split with maximum gain ratio is selected.
In some situations, every possible test splits D into subsets that have the same class
distribution. All tests then have zero gain, and C4.5 uses this as an additional stopping
criterion.
The recursive partitioning strategy above results in trees that are consistent with the
training data, if this is possible. In practical applications data are often noisy { attribute
values are incorrectly recorded and cases are misclassified. Noise leads to overly complex
trees that attempt to account for these anomalies. Most systems prune the initial tree,
identifying subtrees that contribute little to predictive accuracy and replacing each by a
leaf.

3. Modified Assessment of Continuous Attributes

We return now to the selection of a threshold for a continuous attribute A. If there are
N distinct values of A in the set of cases D, there are N , 1 thresholds that could be
used for a test on A. Each threshold gives unique subsets D1 and D2 and so the value of
the splitting criterion is a function of the threshold. The ability to choose the threshold t
so as to maximize this value gives a continuous attribute A an advantage over a discrete
attribute (which has no similar parameter that adjusts the partition of D), and also over
other continuous attributes that have fewer distinct values in D. That is, the choice of a

test will be biased towards continuous attributes with numerous distinct values.
This paper proposes a correction for this bias that consists of two modifications to C4.5.
The first of these, inspired by the Minimum Description Length principle (Rissanen, 1983),
adjusts the apparent information gain from a test of a continuous attribute. Discussion of
this change is prefaced by a brief introduction to MDL.
Following Quinlan and Rivest (1989), let a sender and a receiver both possess an ordered
list of the cases in the training data showing each case's attribute values. The sender also
knows the class to which each case belongs and must transmit this information to the
receiver. He or she first encodes and sends a theory of how to classify the cases. Since
this theory might be imperfect, the sender must also identify the exceptions to the theory
79

fiQuinlan

that occur in the training cases and state how their classes predicted by the theory should
be corrected. The total length of the transmission is thus the number of bits required
to encode the theory (the theory cost) plus the bits needed to identify and correct the
exceptions (the exceptions cost). The sender may have a choice among several alternative
theories, some being simple but leaving many errors to be corrected while others are more
elaborate but more accurate. The MDL principle may then be stated as: Choose the theory
that minimizes the sum of the theory and exceptions costs.
MDL thus provides a framework for trading off the complexity of a theory against its
accuracy on the training data D. The exceptions cost associated with a set of cases D is
asymptotically equivalent to jDj Info (D), so that jDj Gain (D; T ) measures the reduction
in exceptions cost when D is partitioned by a test T . Partitioning D in this way, however,
requires transmission of a more complex theory that includes the definition of T . Whereas a
test A=? on a discrete attribute A can be specified by nominating the attribute involved, a
test At must also include the threshold t; if there are N , 1 possible thresholds for A, this
will take an additional log2 (N , 1) bits.2 The first modification is to \charge" this increased
cost associated with a test on a continuous attribute to the apparent gain achieved by the
test, so reducing the (per-case) information Gain (D; T ) by log2 (N , 1)/jDj.
A test on a continuous attribute with numerous distinct values will now be less likely
to have the maximum value of the splitting criterion among the family of possible tests,
and so is less likely to be selected. Further, if all thresholds t on a continuous attribute
A have an adjusted gain that is less than zero, attribute A is effectively ruled out. The
consequences of this first change are thus a re-ranking of potential tests and the possible
exclusion of some of them.
The second change is much more straightforward. Recall that the gain ratio criterion
divides the apparent gain by the information available from a split. This latter varies as
a function of the threshold t and is is maximal when there are as many cases above t as
below. If the gain ratio criterion is used to select t, the effect of the penalty described above
will also vary with t, having the least impact when t divides the cases equally. This seems
to be an unnecessary complication, so the threshold t is chosen instead to maximize gain.
Once the threshold is chosen, however, the final selection of the attribute to be used for the
test is still made on the basis of the gain ratio criterion using the adjusted gain.

4. Empirical Evaluation
The effects of these changes were assessed empirically in a series of \before and after"
experiments with a substantial number of learning tasks. Release 7 of C4.5 (abbreviated
here as Rel 7) was modified as above to produce a new version (Rel 8). Both systems were
applied to twenty databases from the UCI Repository that involve continuous attributes,
either alone or in combination with discrete attributes. A summary of the characteristics
of these data sets appears in Appendix A. In all the following experiments, both versions
2. Even with a convex splitting criterion that satisfies the requirements of Fayyad and Irani (1992), we
cannot use the number N of potentially gain-maximizing thresholds instead of the greater number N
of possible thresholds. Since the receiver knows the cases' attribute values but not their classes, he or
she cannot determine whether all the cases with two adjacent values of A belong to the same class. The
message must consequently identify the chosen threshold among all possible thresholds.
0

80

fiImproved Use of Continuous Attributes in C4.5

Table 1: Results using modified (Rel 8 ) and previous (Rel 7 ) C4.5.
anneal
auto
breast-w
colic
credit-a
credit-g
diabetes
glass
heart-c
heart-h
hepatitis
hypo
iris
labor
letter
segment
sick
sonar
vehicle
waveform

average

Rel 8
7.67 .12
17.7 .5
5.26 .19
15.0 .2
14.7 .2
28.4 .3
25.4 .3
32.5 .8
23.0 .5
21.5 .2
20.4 .6
.48 .01
4.80 .17
19.1 1.0
12.0 .0
3.21 .08
1.34 .03
25.6 .7
27.1 .4
27.3 .3

Error Rate
Tree Size
Rel 7
w/d/l ratio Rel 8
Rel 7
7.49 .16 3/2/5 1.02 75.2 .7 70.1 1.1
23.8 .6 10/0/0 .74 63.7 .4 62.9 .5
5.29 .09 5/1/4 .99 25.0 .5 20.3 .5
15.1 .4
5/2/3 .99 9.7 .2 20.0 .5
15.8 .3
7/1/2 .93 33.2 1.1 57.3 1.2
28.9 .3
5/1/4 .98 124 2
155 2
28.3 .3 10/0/0 .90 44.0 1.6 128.2 1.8
32.1 .5
4/1/5 1.01 45.7 .4 51.3 .4
24.9 .4
8/0/2 .92 39.9 .4 45.3 .3
21.6 .5
4/0/6 1.00 19.1 .6 29.7 1.2
21.7 .8
6/1/3 .94 17.8 .3 15.5 .4
.49 .02 6/3/1 .97 27.5 .1 25.3 .1
4.87 .20 3/3/4 .99 8.5 .0
9.3 .1
16.7 .9
1/2/7 1.15 7.0 .3
7.3 .1
12.2 .0 10/0/0 .98 2328 4 2370 4
3.77 .07 9/1/0 .85 82.9 .5 83.5 .6
1.29 .03 2/1/7 1.04 50.8 .5 51.5 .5
28.4 .6
8/0/2 .90 28.4 .2 33.1 .5
29.1 .3 10/0/0 .93 135 2
181 1
28.1 .6
6/2/2 .97 44.6 .4 49.2 .4

.96

ratio
1.07
1.01
1.23
.49
.58
.80
.34
.89
.88
.64
1.15
1.09
.91
.96
.98
.99
.99
.86
.75
.91

.88

of C4.5 were run with the same default settings for all parameters; no attempt was made
to tune either system for these tasks.

4.1 Initial experiments

Table 1 displays the results of the first trials, consisting of ten complete ten-fold crossvalidations3 with each task. The figure shown for each system is the mean of the ten
cross-validation results where the error rates and tree sizes refer to C4.5's pruned trees; the
standard error of this mean appears in small font. The column headed `w/d/l' shows the
number of complete cross-validations in which Rel 8 gives a lower error rate, the same error
rate, or a higher error rate than Rel 7. The figures under `ratio' present results for Rel 8
divided by the corresponding figure for Rel 7.
As the overall averages at the foot of the table indicate, the trees produced by Rel 8 in
these trials are 4% more accurate and 12% smaller than those generated by Rel 7. Rel 8 is
3. A ten-fold cross-validation is performed by dividing the data into ten blocks of cases that have similar
size and class distribution. For each block in turn, a decision tree is constructed from the remaining nine
blocks and tested on the unseen cases in the hold-out block.

81

fiQuinlan

less accurate that Rel 7 on only four of the twenty tasks; for the smallest data set (labor,
with 57 cases), however, the trees produced by Rel 8 are substantially less accurate. The
pruned trees generated by Rel 8 for some tasks are a great deal smaller than their Rel 7
counterparts { diabetes is a particularly notable example.
I do not recommend the use of the unpruned trees constructed initially by C4.5 but,
for the sake of completeness, the corresponding figures for the unpruned trees were also
determined. The average ratio of the error rate of Rel 8 to that of Rel 7 is 0.95, while the
ratio of tree size is 0.94. For the unpruned trees, then, the modifications incorporated in
Rel 8 lead to a 5% reduction in error and a 6% reduction in the size.

4.2 Adding irrelevant attributes
In practical applications, it is unlikely that an analyst would knowingly add irrelevant
attributes to the data! However, even an attribute that is relevant for some parts of the
tree might be quite irrelevant for others. The bias towards continuous attributes inherent in
Rel 7 implies that the system should occasionally select a test on an irrelevant continuous
attribute in preference to tests on relevant discrete attributes.
To explore this potential deficiency, the twenty data sets were modified by adding irrelevant attributes. Ten of these were continuous attributes, each having uniformly distributed
random values x, 0  x < 1. (Since only the order of values of a continuous attribute is
important, the distribution of these values does not matter { use of another distribution
such as the Gaussian N (0; 1) should produce comparable results.) As Kohavi (personal
communication, 1995) points out, it is unfair to compare Rel 8 to Rel 7 on data sets to
which only irrelevant continuous-valued attributes have been added, since the modifications
incorporated in Rel 8 make it less likely to choose tests involving any continuous attributes.
To circumvent this problem, a further ten discrete attributes with ten equiprobable values
were added, giving twenty irrelevant attributes in all. The experiments above were repeated
on the enlarged data sets, with the results shown in Table 2.
These results highlight the effects of the modifications implemented in Rel 8. Addition
of irrelevant attributes increases the error of the Rel 7 trees by an average of 12%, but
has a much smaller impact on those produced by Rel 8. The head-to-head comparison on
the altered data sets, presented in the table, shows that the pruned trees found by Rel 8
have 10% lower error on average, and are also a great deal smaller. Any split on a random
continuous attribute is unlikely to generate sucient gain to \pay for" the threshold, so
such tests will tend to be filtered out by Rel 8 but not by Rel 7. Consequently, Rel 7 is
more prone to split the data (uselessly) on a random attribute, leading to larger trees and
higher error rates.

4.3 Ablation experiments
The effects of the modifications implemented in Rel 8 can be factored into choosing (slightly)
different thresholds using gain rather than gain ratio, excluding attributes for which no
threshold gives sucient gain to offset the penalty, and re-ranking potential tests by penalizing those that involve continuous attributes. To ascertain the contributions of each, two
intermediate versions of C4.5 were constructed:
82

fiImproved Use of Continuous Attributes in C4.5

Table 2: Results after addition of irrelevant attributes.
Rel 8
anneal+
7.72 .23
auto+
18.7 .5
breast-w+ 5.69 .11
colic+
15.1 .2
credit-a+ 13.6 .3
credit-g+ 28.5 .3
diabetes+ 26.9 .3
glass+
37.0 .5
heart-c+
22.6 .7
heart-h+ 20.3 .4
hepatitis+ 19.1 .6
hypo+
.47 .02
iris+
5.67 .15
labor+
19.1 .8
letter+
12.7 .1
segment+ 3.91 .09
sick+
1.61 .05
sonar+
25.5 .8
vehicle+
28.7 .3
waveform+ 30.1 .7

average

Error Rate
Rel 7
w/d/l
8.13 .18 9/0/1
26.0 .7 10/0/0
6.17 .13 8/0/2
20.1 .3 10/0/0
16.4 .3 10/0/0
32.4 .4 10/0/0
30.3 .5 10/0/0
35.9 .8
3/0/7
30.3 .4 10/0/0
24.9 .5 10/0/0
23.9 .7 10/0/0
.49 .02 7/2/1
5.73 .41 4/0/6
24.6 .7
9/1/0
13.3 .1 10/0/0
3.85 .05 4/0/6
1.57 .05 4/1/5
29.3 .6
9/1/0
28.8 .2
5/3/2
28.0 .6
4/0/6

ratio
.95
.72
.92
.75
.83
.88
.89
1.03
.75
.82
.80
.96
.99
.78
.95
1.01
1.02
.87
.99
1.08

.90

Tree Size
Rel 8
Rel 7 ratio
74.3 1.1 84.0 1.6 .88
63.4 .7 62.3 .6 1.02
16.8 .4 25.0 .4
.67
8.9 .2 39.9 1.1 .22
34.7 .7 58.4 .9
.60
111 3
174 2
.64
43.6 2.1 115.5 1.8 .38
31.0 .6 46.2 .9
.67
24.9 .8 52.0 .5
.48
19.9 .5 32.0 .9
.62
5.6 .4 20.4 .6
.28
27.8 .2 25.9 .2 1.08
7.6 .1
9.7 .1
.79
6.8 .2 10.6 .1
.64
2300 3 2372 6
.97
69.2 .6 88.2 .6
.78
37.1 .8 54.8 .7
.68
20.1 .4 34.0 .5
.59
109 1
162 1
.67
27.9 1.0 48.9 .5
.57

.66

 7G differs from Rel 7 only in that the threshold t is chosen to maximize information
gain rather than gain ratio;

 7GS also chooses thresholds on gain; if the gain of the best threshold is less than the
penalty log2 (N , 1)/jDj, however, the test is excluded.
The only difference between 7GS and Rel 8 is the latter's application of the penalty when
determining the relative desirability of possible tests.
The trials were repeated using the same cross-validation blocks as before for these intermediate versions. Average error rates, tree sizes, and ratios (again computed with respect
to Rel 7) are presented in Table 3 and summarized graphically in Figure 1.
Selection of thresholds by gain rather than gain ratio (7G) has very little impact { the
average error rate and tree size ratios with respect to Rel 7 are both very close to one.
There are non-trivial changes for some tasks, however; for instance, the error rate on the
segment data is considerably lower and the trees found for the breast-w task are noticeably
larger.
83

fiQuinlan

Table 3: Results for intermediate systems 7G and 7GS.
7G
anneal
7.73 .16
auto
23.0 .9
breast-w 5.21 .23
colic
15.0 .4
credit-a 14.7 .3
credit-g 29.7 .3
diabetes 27.1 .4
glass
30.9 .5
heart-c
25.0 .4
heart-h 22.2 .4
hepatitis 22.0 .8
hypo
.49 .02
iris
4.93 .23
labor
18.8 1.1
letter
12.3 .0
segment 3.39 .08
sick
1.34 .02
sonar
28.8 1.2
vehicle
28.1 .2
waveform 28.1 .8

average

Error Rate
ratio 7GS
1.03 7.62 .16
.97 22.7 .8
.98 5.32 .17
.99 15.0 .3
.93 14.1 .2
1.03 29.1 .2
.96 25.2 .3
.96 31.3 .7
1.01 23.8 .5
1.03 20.9 .3
1.01 21.4 .6
1.00 .50 .01
1.01 4.80 .17
1.13 19.5 1.0
1.00 12.2 .0
.90 3.36 .07
1.04 1.34 .02
1.02 26.6 1.1
.97 27.6 .3
1.00 27.3 .6

1.00

ratio
1.02
.95
1.01
.99
.89
1.01
.89
.97
.96
.97
.98
1.02
.99
1.17
1.00
.89
1.04
.94
.95
.97

.98

7G
73.9 .7
59.5 .9
24.4 .3
20.2 .5
50.1 1.0
148 2
127 2
50.3 .3
44.5 .8
30.2 1.1
17.3 .4
25.7 .2
8.5 .1
7.8 .1
2327 3
83.7 .3
50.4 .3
32.6 .4
178 1
46.8 .4

Tree Size
ratio
7GS
1.05 73.4 .7
.95 59.0 .9
1.21 24.1 .5
1.01 17.9 .6
.88 38.3 1.0
.96 138 2
.99 45.4 2.0
.98 46.2 .6
.98 42.2 .9
1.02 19.3 .7
1.12 15.2 .4
1.02 27.0 .1
.92 8.5 .0
1.06 7.6 .1
.98 2330 3
1.00 82.8 .3
.98 51.4 .3
.98 28.5 .3
.99 145 2
.95 44.4 .6

1.00

Error Rate

ratio
1.05
.94
1.19
.90
.67
.89
.35
.90
.93
.65
.98
1.07
.92
1.04
.98
.99
1.00
.86
.80
.90

.90

Tree Size

Rel 7
7G
7GS
Rel 8

.9

.95

1.0

.8

Figure 1: Summary of ablation results.

84

.9

1.0

fiImproved Use of Continuous Attributes in C4.5

Use of the penalty to filter tests on continuous attributes (7GS) produces more noticeable differences. Ruling out some tests on continuous attributes accounts for most of the
reduction in tree size observed with Rel 8. In some cases, the trees are markedly smaller
{ for the diabetes data, the 7GS trees are on average only one-third of the size of those
produced by Rel 7. This change also accounts for about half of Rel 8's improvement in
error rate, the diabetes data again providing the greatest change from 7G.
Finally, the use by Rel 8 of the penalty to re-rank the attributes yields a further improvement in error rate and a small decrease in average tree size. This re-ranking may be
beneficial even when all attributes are continuous: the average error rate of Rel 8 is about
1% lower than that of 7GS on the nine tasks of this kind, in only two of which does 7GS
give a lower error rate than Rel 8.

5. Related Research
This section examines the two alternative methods for utilizing continuous attributes that
were mentioned in the introduction, and compares them empirically with C4.5 Rel 8.

5.1 Global discretization
Dougherty et al. (1995) consider various ways of converting a continuous attribute to a
discrete one by dividing its values into intervals, each of which becomes a separate value
for the replacement discrete attribute. The method found to give the best results, entropy
discretization, was first investigated by Catlett (1991) as a means of reducing the time
required to construct a tree. Fayyad and Irani (1993) subsequently introduced a clever
refinement that led to the final form used by Dougherty et al. and in the experiments
reported here.
To find the set of intervals, the training cases are first sorted on the value of the continuous attribute in question. The procedure outlined in Section 2 is used to find the threshold
t that maximizes information gain. The same process is repeated for the corresponding
subsets of cases with attribute values below and above t. (Since the cases are not reordered,
they need not be re-sorted, and this is the source of the reduced learning times.) If w
thresholds are found, the continuous attribute is mapped to a discrete attribute with w+1
values, one for each interval.
Some stopping criterion is required to prevent this process from resulting in a very large
number of intervals (which could become as numerous as the training cases if all values
of the attribute are distinct). Catlett uses a four-pronged heuristic criterion, but Fayyad
and Irani developed an elegant test based on the MDL principle (Section 3). They view a
discretization rule as a classifying theory that uses a single attribute and that associates a
class with each interval. Introduction of an additional threshold, increasing the complexity
of the discretization rule, is allowed only if the greater theory coding cost is more than
offset by the consequent reduction in the exceptions cost. This scheme generally leads to
few thresholds in regions where the the cases' classes do not vary much and to finer divisions
when required.
Similar experiments to those described by Dougherty et al. were carried out on the
learning tasks of Section 4. In each trial, the training data are used to find discretization
85

fiQuinlan

Table 4: Comparison with C4.5 using global discretization (Discr ).
anneal
auto
breast-w
colic
credit-a
credit-g
diabetes
glass
heart-c
heart-h
hepatitis
hypo
iris
labor
letter
segment
sick
sonar
vehicle
waveform

average

Rel 8
7.67 .12
17.7 .5
5.26 .19
15.0 .2
14.7 .2
28.4 .3
25.4 .3
32.5 .8
23.0 .5
21.5 .2
20.4 .6
.48 .01
4.80 .17
19.1 1.0
12.0 .0
3.21 .08
1.34 .03
25.6 .7
27.1 .4
27.3 .3

Error Rate
Discr
w/d/l
9.48 .14 10/0/0
23.8 .6
9/1/0
5.38 .15 6/0/4
15.1 .1
6/2/2
14.0 .1
0/1/9
28.1 .4
5/1/4
25.5 .3
5/0/5
28.4 .3
1/0/9
21.7 .6
2/1/7
20.8 .4
3/0/7
19.6 .8
3/1/6
.72 .03 10/0/0
5.47 .29 6/3/1
20.0 .9
6/0/4
21.1 .0 10/0/0
5.65 .10 10/0/0
2.14 .03 10/0/0
24.6 .7
3/1/6
31.5 .5 10/0/0
26.5 .6
4/0/6

ratio
.81
.74
.98
.99
1.05
1.01
.99
1.14
1.06
1.04
1.04
.67
.88
.96
.57
.57
.63
1.04
.86
1.03

.90

Tree Size
Rel 8
Discr
75.2 .7 68.1 .5
63.7 .4 94.8 1.8
25.0 .5 19.9 .5
9.7 .2
7.8 .2
33.2 1.1 22.3 .6
124 2
82 1
44.0 1.6 19.6 .7
45.7 .4 35.8 .3
39.9 .4 25.9 .4
19.1 .6
9.7 .6
17.8 .3 11.5 .5
27.5 .1 45.1 .3
8.5 .0
6.2 .1
7.0 .3
5.2 .1
2328 4 9600 12
82.9 .5 296.4 2.6
50.8 .5 32.8 .4
28.4 .2 28.6 .5
135 2
175 2
44.6 .4 42.2 .8

ratio
1.11
.67
1.25
1.23
1.49
1.50
2.25
1.28
1.54
1.97
1.55
.61
1.36
1.34
.24
.28
1.55
.99
.78
1.06

1.20

rules to convert every continuous attribute to a discrete attribute. C4.54 is invoked to find
a tree that is evaluated on the test data, using the same discretization intervals found from
the training data. As before, each data set is subjected to ten cross-validations using the
same blocks of cases as previously.
Results of these trials, summarized in Table 4, show that the comments of Dougherty
et al. quoted in the introduction do not apply to Rel 8. Discretization leads to improved
accuracy on eight of the tasks and to a degradation on 12 of them. Most of the improvements
are modest, however, while several tasks exhibit a marked increase in error; the average value
of the error ratio indicates a strong advantage for the local threshold selection employed in
Rel 8 over the global thresholding used by discretization.
Kohavi (personal communication, 1996) suggests that there might be a \middle ground"
in which thresholds are determined locally until the subsets of cases are relatively small, at
which point subsequent possible thresholds would be found using the discretization strategy
above. Evidence in support of this idea is provided by Figure 2 where, for each task, the
error ratio that appears in Table 4 is plotted against the size of the data set (on a logarithmic
4. Since there are no continuous attributes, Rel 7 and Rel 8 give identical results on these discretized tasks.

86

fiImproved Use of Continuous Attributes in C4.5

size



20000



1000
100
0.5




 
 
 



0.75

1.0



1.25

ratio

Figure 2: Effect of discretization vs data set size.
scale). The clear trend shows that global discretization degrades performance more as data
sets become larger, but can be beneficial for tasks with fewer cases.

5.2 Multi-threshold splits

In contrast, T2 (Auer et al., 1995) determines thresholds locally but allows the values of
a continuous attribute to be partitioned into multiple intervals. These intervals are not
found heuristically by a recursive application of binary splitting, as above. Instead, a more
thorough exploration is carried out to find the set of up to m intervals that minimizes
error on the training set. (The default value of m is C +1 where there are C classes in the
data.) Search for these intervals is expensive, so T2 restricts decision trees to two levels
of tests (in the spirit of one-level decision \stumps" described by Holte, 1993) where only
the second level employs non-binary splits of continuous attributes. Within this restricted
theory language, however, T2 is guaranteed to find a tree that misclassifies as few of the
training cases as possible.
Even so, the computational cost of T2 using the default value of m is proportional to
C 4  (C +1)2  a2 , where a is the number of attributes (Auer, personal communication, 1996).
For example, the time required to process the small auto data set with six classes and 25
attributes is four orders of magnitude greater than that needed by C4.5. This effectively
rules out trials of T2 on some of the learning tasks used above, specifically those with
more than four classes. For the remaining 14 tasks, experiments following the same pattern
as before and using the same cross-validation blocks were carried out and are reported in
Table 5. T2 produces trees with error rates much lower than those generated by Rel 8 on
two tasks, slightly lower on two more, and higher on the remaining ten. As reected in the
average error ratio, the trials still favor C4.5 Rel 8 overall. (Had it been possible to run the
tasks with larger numbers of classes, T2's restricted theory language would perhaps have
caused an even more noticeable increase in error.)
87

fiQuinlan

Table 5: Comparison with T2.
breast-w
colic
credit-a
credit-g
diabetes
heart-c
heart-h
hepatitis
iris
labor
sick
sonar
vehicle
waveform

average

Rel 8
5.26 .19
15.0 .2
14.7 .2
28.4 .3
25.4 .3
23.0 .5
21.5 .2
20.4 .6
4.80 .17
19.1 1.0
1.34 .03
25.6 .7
27.1 .4
27.3 .3

Error Rate
Tree Size
T2
w/d/l ratio Rel 8
T2
ratio
4.06 .09 0/0/10 1.30 25.0 .5 10.0 .0 2.50
16.2 .2 10/0/0
.92 9.7 .2 15.5 .2 .63
16.6 .2 10/0/0
.89 33.2 1.1 46.1 .4 .72
32.2 .2 10/0/0
.88 124 2
49 1 2.51
24.9 .2
3/0/7 1.02 44.0 1.6 11.5 .0 3.81
26.8 .6 10/0/0
.86 39.9 .4 20.5 .0 1.94
26.1 .3 10/0/0
.82 19.1 .6 16.3 .3 1.18
24.8 .3 10/0/0
.82 17.8 .3 13.7 .2 1.30
4.60 .35 3/1/6 1.04 8.5 .0 12.0 .0 .71
15.3 1.6 3/0/7 1.25 7.0 .3 14.9 .1 .47
2.21 .01 10/0/0
.61 50.8 .5 12.0 .0 4.23
28.4 .7
8/0/2
.90 28.4 .2 11.1 .0 2.56
38.1 .3 10/0/0
.71 135 2
16 0 8.46
35.2 .6 10/0/0
.78 44.6 .4 13.9 .0 3.21

.91

2.44

It is worth noting that T2's trees are much smaller than those found by C4.5 { less than
half the size, on average. This is despite the fact that tests in T2 have one more outcome
(for unknown values) than the corresponding tests in C4.5.

6. Conclusion
The results of Section 4 show that the straightforward changes to C4.5's use of continuous
attributes lead to an overall improvement in its performance on the twenty learning tasks
investigated here.5 The pruned trees are substantially smaller and somewhat more accurate,
especially in the presence of irrelevant attributes. As the tasks are a representative selection
from those in the UCI Repository that involve continuous attributes, similar learning tasks
should also benefit. Of course, C4.5's performance on domains with continuous attributes
can also be improved in other complementary ways, such as by selecting attributes (John,
Kohavi, & Peger, 1994), exploring the space of parameter settings (Kohavi & John, 1995),
or generating multiple classifiers (Breiman, 1996; Freund & Schapire, 1996).
Comparisons with a well-known global discretization scheme, and with a system that
carries out a thorough search over the space of two-level decision trees, also favor the
modified C4.5. However, both suggest further ways in which the system might be improved.
Non-binary splits on continuous attributes make the trees easier to understand and also seem
to lead to more accurate trees in some domains. It would also be interesting to investigate
5. The files necessary to update C4.5 Release 5 (available with Quinlan, 1993) to the new Release 8 can be
obtained by anonymous ftp from ftp.cs.su.oz.au, file pub/ml/patch.tar.Z.

88

fiImproved Use of Continuous Attributes in C4.5

Kohavi's suggestion to use discretization within a tree when the local number of training
cases is small.
On another tack, C4.5 has an option that affects tests on discrete attributes. Instead of
the default, in which each value of the attribute is associated with a separate subtree, the
values are grouped into subsets and one tree formed for each subset. Many possible subsets
are explored, just as many possible thresholds for a continuous attribute are considered.
The argument for the application of a penalty to tests on continuous attributes would seem
to apply also to such subset tests.

Appendix A. Description of learning tasks
Abbrev

Domain

Cases Classes

anneal
auto
breast-w
colic
credit-a
credit-g
diabetes
glass
heart-c
heart-h
hepatitis
hypo
iris
labor
letter
segment
sick
sonar
vehicle
waveform

annealing processes
898
auto insurance
205
breast cancer (Wisc)
699
horse colic
368
credit screening (Aust)
690
credit screening (Ger)
1000
Pima diabetes
768
glass identification
214
heart disease (Clev)
303
heart disease (Hun)
294
hepatitis prognosis
155
hypothyroid diagnosis
3772
iris classification
150
labor negotiations
57
letter identification
20000
image segmentation
2310
sick euthyroid
3772
sonar classification
208
silhouette recognition
846
waveform differentiation 300

6
6
2
2
2
2
2
6
2
2
2
5
3
2
26
7
2
2
4
3

Attributes
Cont Discr
9
29
15
10
9
{
10
12
6
9
7
13
8
{
9
{
8
5
8
5
6
13
7
22
4
{
8
8
16
{
19
{
7
22
60
{
18
{
21
{

Acknowledgements
This research was made possible by a grant from the Australian Research Council. The
T2 system was programmed by Peter Auer and made available for these comparisons by
Rob Holte. Thanks to Thierry Van de Merckt for comments on the results that led to
the ablation experiments. I am also grateful for suggestions regarding the paper's content
and presentation made by Ron Kohavi, Usama Fayyad, and Pat Langley. The UCI Data
Repository owes its existence to David Aha and Patrick Murphy. The breast cancer data
(breast-w) was provided to the Repository by Dr William H. Wolberg.
89

fiQuinlan

References

Auer, P., Holte, R. C., & Maass, W. (1995). Theory and application of agnostic paclearning with small decision trees. In Proceedings of Twelfth International Conference
on Machine Learning, pp. 21{29. San Francisco: Morgam Kaufmann.
Breiman, L. (1996). Bagging predictors. Machine Learning, (to appear).
Catlett, J. (1991). On changing continuous attributes into ordered discrete attributes. In
Kodratoff, Y. (Ed.), Proceedings European Working Session on Learning { EWSL-91,
pp. 164{178. Berlin: Springer Verlag.
Dougherty, J., Kohavi, R., & Sahami, M. (1995). Supervised and unsupervised discretization
of continuous features. In Proceedings Twelfth International Conference on Machine
Learning, pp. 194{202. San Francisco: Morgan Kaufmann.
Fayyad, U. M., & Irani, K. B. (1992). On the handling of continuous-valued attributes in
decision tree generation. Machine Learning, 8, 87{102.
Fayyad, U. M., & Irani, K. B. (1993). Multi-interval discretization of continuous-valued
attributes for classification learning. In Proceedings Thirteenth International Joint
Conference on Artificial Intelligence, pp. 1022{1027. San Francisco: Morgan Kaufmann.
Freund, Y., & Schapire, R. E. (1996). A decision-theoretic generalization of on-line learning
and an application to boosting. Unpublished manuscript, available from the authors'
home pages (\http://www.research.att.com/orgs/ssr/people/fyoav,schapireg").
Holte, R. C. (1993). Very simple classification rules perform well on most commonly used
datasets. Machine Learning, 11, 63{91.
Hunt, E. B., Marin, J., & Stone, P. J. (1966). Experiments in Induction. New York:
Academic Press.
John, G. H., Kohavi, R., & Peger, K. (1994). Irrelevant features and the subset selection
problem. In Proceedings Eleventh International Conference on Machine Learning, pp.
121{129. San Francisco: Morgan Kaufmann.
Kohavi, R., & John, G. H. (1995). Automatic parameter selection by minimizing estimated
error. In Proceedings Twelfth International Conference on Machine Learning, pp.
304{312. San Francisco: Morgan Kaufmann.
Quinlan, J. R. (1993). C4.5: Programs for Machine Learning. San Mateo: Morgan Kaufmann.
Quinlan, J. R., & Rivest, R. L. (1989). Inferring decision trees using the minimum description length principle. Information and Computation, 80, 227{248.
Rissanen, J. (1983). A universal prior for integers and estimation by minimum description
length. Annals of Statistics, 11, 416{431.
90

fiJournal of Artificial Intelligence Research 4 (1996) 237-285

Submitted 9/95; published 5/96

Reinforcement Learning: A Survey
Leslie Pack Kaelbling
Michael L. Littman

Computer Science Department, Box 1910, Brown University
Providence, RI 02912-1910 USA

lpk@cs.brown.edu
mlittman@cs.brown.edu

Andrew W. Moore

Smith Hall 221, Carnegie Mellon University, 5000 Forbes Avenue
Pittsburgh, PA 15213 USA

awm@cs.cmu.edu

Abstract

This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both
the historical basis of the field and a broad selection of current work are summarized.
Reinforcement learning is the problem faced by an agent that learns behavior through
trial-and-error interactions with a dynamic environment. The work described here has a
resemblance to work in psychology, but differs considerably in the details and in the use
of the word \reinforcement." The paper discusses central issues of reinforcement learning,
including trading off exploration and exploitation, establishing the foundations of the field
via Markov decision theory, learning from delayed reinforcement, constructing empirical
models to accelerate learning, making use of generalization and hierarchy, and coping with
hidden state. It concludes with a survey of some implemented systems and an assessment
of the practical utility of current methods for reinforcement learning.

1. Introduction
Reinforcement learning dates back to the early days of cybernetics and work in statistics,
psychology, neuroscience, and computer science. In the last five to ten years, it has attracted
rapidly increasing interest in the machine learning and artificial intelligence communities.
Its promise is beguiling|a way of programming agents by reward and punishment without
needing to specify how the task is to be achieved. But there are formidable computational
obstacles to fulfilling the promise.
This paper surveys the historical basis of reinforcement learning and some of the current
work from a computer science perspective. We give a high-level overview of the field and a
taste of some specific approaches. It is, of course, impossible to mention all of the important
work in the field; this should not be taken to be an exhaustive account.
Reinforcement learning is the problem faced by an agent that must learn behavior
through trial-and-error interactions with a dynamic environment. The work described here
has a strong family resemblance to eponymous work in psychology, but differs considerably
in the details and in the use of the word \reinforcement." It is appropriately thought of as
a class of problems, rather than as a set of techniques.
There are two main strategies for solving reinforcement-learning problems. The first is to
search in the space of behaviors in order to find one that performs well in the environment.
This approach has been taken by work in genetic algorithms and genetic programming,
c 1996 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiKaelbling, Littman, & Moore

T
a

s
I
R

i
r

B

Figure 1: The standard reinforcement-learning model.
as well as some more novel search techniques (Schmidhuber, 1996). The second is to use
statistical techniques and dynamic programming methods to estimate the utility of taking
actions in states of the world. This paper is devoted almost entirely to the second set of
techniques because they take advantage of the special structure of reinforcement-learning
problems that is not available in optimization problems in general. It is not yet clear which
set of approaches is best in which circumstances.
The rest of this section is devoted to establishing notation and describing the basic
reinforcement-learning model. Section 2 explains the trade-off between exploration and
exploitation and presents some solutions to the most basic case of reinforcement-learning
problems, in which we want to maximize the immediate reward. Section 3 considers the more
general problem in which rewards can be delayed in time from the actions that were crucial
to gaining them. Section 4 considers some classic model-free algorithms for reinforcement
learning from delayed reward: adaptive heuristic critic, TD() and Q-learning. Section 5
demonstrates a continuum of algorithms that are sensitive to the amount of computation an
agent can perform between actual steps of action in the environment. Generalization|the
cornerstone of mainstream machine learning research|has the potential of considerably
aiding reinforcement learning, as described in Section 6. Section 7 considers the problems
that arise when the agent does not have complete perceptual access to the state of the
environment. Section 8 catalogs some of reinforcement learning's successful applications.
Finally, Section 9 concludes with some speculations about important open problems and
the future of reinforcement learning.

1.1 Reinforcement-Learning Model
In the standard reinforcement-learning model, an agent is connected to its environment
via perception and action, as depicted in Figure 1. On each step of interaction the agent
receives as input, i, some indication of the current state, s, of the environment; the agent
then chooses an action, a, to generate as output. The action changes the state of the
environment, and the value of this state transition is communicated to the agent through
a scalar reinforcement signal, r. The agent's behavior, B , should choose actions that tend
to increase the long-run sum of values of the reinforcement signal. It can learn to do this
over time by systematic trial and error, guided by a wide variety of algorithms that are the
subject of later sections of this paper.
238

fiReinforcement Learning: A Survey

Formally, the model consists of

 a discrete set of environment states, S ;
 a discrete set of agent actions, A; and
 a set of scalar reinforcement signals; typically f0; 1g, or the real numbers.
The figure also includes an input function I , which determines how the agent views the
environment state; we will assume that it is the identity function (that is, the agent perceives
the exact state of the environment) until we consider partial observability in Section 7.
An intuitive way to understand the relation between the agent and its environment is
with the following example dialogue.
Environment: You are in state 65. You have 4 possible actions.
Agent:
I'll take action 2.
Environment: You received a reinforcement of 7 units. You are now in state
15. You have 2 possible actions.
Agent:
I'll take action 1.
Environment: You received a reinforcement of -4 units. You are now in state
65. You have 4 possible actions.
Agent:
I'll take action 2.
Environment: You received a reinforcement of 5 units. You are now in state
44. You have 5 possible actions.

..
.

..
.

The agent's job is to find a policy  , mapping states to actions, that maximizes some
long-run measure of reinforcement. We expect, in general, that the environment will be
non-deterministic; that is, that taking the same action in the same state on two different
occasions may result in different next states and/or different reinforcement values. This
happens in our example above: from state 65, applying action 2 produces differing reinforcements and differing states on two occasions. However, we assume the environment is
stationary; that is, that the probabilities of making state transitions or receiving specific
reinforcement signals do not change over time.1
Reinforcement learning differs from the more widely studied problem of supervised learning in several ways. The most important difference is that there is no presentation of input/output pairs. Instead, after choosing an action the agent is told the immediate reward
and the subsequent state, but is not told which action would have been in its best long-term
interests. It is necessary for the agent to gather useful experience about the possible system
states, actions, transitions and rewards actively to act optimally. Another difference from
supervised learning is that on-line performance is important: the evaluation of the system
is often concurrent with learning.
1. This assumption may be disappointing; after all, operation in non-stationary environments is one of the
motivations for building learning systems. In fact, many of the algorithms described in later sections
are effective in slowly-varying non-stationary environments, but there is very little theoretical analysis
in this area.

239

fiKaelbling, Littman, & Moore

Some aspects of reinforcement learning are closely related to search and planning issues
in artificial intelligence. AI search algorithms generate a satisfactory trajectory through a
graph of states. Planning operates in a similar manner, but typically within a construct
with more complexity than a graph, in which states are represented by compositions of
logical expressions instead of atomic symbols. These AI algorithms are less general than the
reinforcement-learning methods, in that they require a predefined model of state transitions,
and with a few exceptions assume determinism. On the other hand, reinforcement learning,
at least in the kind of discrete cases for which theory has been developed, assumes that
the entire state space can be enumerated and stored in memory|an assumption to which
conventional search algorithms are not tied.

1.2 Models of Optimal Behavior

Before we can start thinking about algorithms for learning to behave optimally, we have
to decide what our model of optimality will be. In particular, we have to specify how the
agent should take the future into account in the decisions it makes about how to behave
now. There are three models that have been the subject of the majority of work in this
area.
The finite-horizon model is the easiest to think about; at a given moment in time, the
agent should optimize its expected reward for the next h steps:
h
X

E ( rt) ;
t=0

it need not worry about what will happen after that. In this and subsequent expressions,

rt represents the scalar reward received t steps into the future. This model can be used in

two ways. In the first, the agent will have a non-stationary policy; that is, one that changes
over time. On its first step it will take what is termed a h-step optimal action. This is
defined to be the best action available given that it has h steps remaining in which to act
and gain reinforcement. On the next step it will take a (h , 1)-step optimal action, and so
on, until it finally takes a 1-step optimal action and terminates. In the second, the agent
does receding-horizon control, in which it always takes the h-step optimal action. The agent
always acts according to the same policy, but the value of h limits how far ahead it looks
in choosing its actions. The finite-horizon model is not always appropriate. In many cases
we may not know the precise length of the agent's life in advance.
The infinite-horizon discounted model takes the long-run reward of the agent into account, but rewards that are received in the future are geometrically discounted according
to discount factor  , (where 0   < 1):
1
X

E (  trt) :
t=0

We can interpret  in several ways. It can be seen as an interest rate, a probability of living
another step, or as a mathematical trick to bound the infinite sum. The model is conceptually similar to receding-horizon control, but the discounted model is more mathematically
tractable than the finite-horizon model. This is a dominant reason for the wide attention
this model has received.
240

fiReinforcement Learning: A Survey

Another optimality criterion is the average-reward model, in which the agent is supposed
to take actions that optimize its long-run average reward:
h
1X
rt) :
lim
E
(
h!1 h
t=0

Such a policy is referred to as a gain optimal policy; it can be seen as the limiting case of
the infinite-horizon discounted model as the discount factor approaches 1 (Bertsekas, 1995).
One problem with this criterion is that there is no way to distinguish between two policies,
one of which gains a large amount of reward in the initial phases and the other of which
does not. Reward gained on any initial prefix of the agent's life is overshadowed by the
long-run average performance. It is possible to generalize this model so that it takes into
account both the long run average and the amount of initial reward than can be gained.
In the generalized, bias optimal model, a policy is preferred if it maximizes the long-run
average and ties are broken by the initial extra reward.
Figure 2 contrasts these models of optimality by providing an environment in which
changing the model of optimality changes the optimal policy. In this example, circles
represent the states of the environment and arrows are state transitions. There is only
a single action choice from every state except the start state, which is in the upper left
and marked with an incoming arrow. All rewards are zero except where marked. Under a
finite-horizon model with h = 5, the three actions yield rewards of +6:0, +0:0, and +0:0, so
the first action should be chosen; under an infinite-horizon discounted model with  = 0:9,
the three choices yield +16:2, +59:0, and +58:5 so the second action should be chosen;
and under the average reward model, the third action should be chosen since it leads to
an average reward of +11. If we change h to 1000 and  to 0.2, then the second action is
optimal for the finite-horizon model and the first for the infinite-horizon discounted model;
however, the average reward model will always prefer the best long-term average. Since the
choice of optimality model and parameters matters so much, it is important to choose it
carefully in any application.
The finite-horizon model is appropriate when the agent's lifetime is known; one important aspect of this model is that as the length of the remaining lifetime decreases, the
agent's policy may change. A system with a hard deadline would be appropriately modeled
this way. The relative usefulness of infinite-horizon discounted and bias-optimal models is
still under debate. Bias-optimality has the advantage of not requiring a discount parameter;
however, algorithms for finding bias-optimal policies are not yet as well-understood as those
for finding optimal infinite-horizon discounted policies.

1.3 Measuring Learning Performance

The criteria given in the previous section can be used to assess the policies learned by a
given algorithm. We would also like to be able to evaluate the quality of learning itself.
There are several incompatible measures in use.

 Eventual convergence to optimal. Many algorithms come with a provable guar-

antee of asymptotic convergence to optimal behavior (Watkins & Dayan, 1992). This
is reassuring, but useless in practical terms. An agent that quickly reaches a plateau
241

fiKaelbling, Littman, & Moore

+2
Finite horizon, h=4
+10
Infinite horizon, =0.9

+11

Average reward

Figure 2: Comparing models of optimality. All unlabeled arrows produce a reward of zero.
at 99% of optimality may, in many applications, be preferable to an agent that has a
guarantee of eventual optimality but a sluggish early learning rate.

 Speed of convergence to optimality. Optimality is usually an asymptotic result,

and so convergence speed is an ill-defined measure. More practical is the speed of
convergence to near-optimality. This measure begs the definition of how near to
optimality is sucient. A related measure is level of performance after a given time,
which similarly requires that someone define the given time.
It should be noted that here we have another difference between reinforcement learning
and conventional supervised learning. In the latter, expected future predictive accuracy or statistical eciency are the prime concerns. For example, in the well-known
PAC framework (Valiant, 1984), there is a learning period during which mistakes do
not count, then a performance period during which they do. The framework provides
bounds on the necessary length of the learning period in order to have a probabilistic
guarantee on the subsequent performance. That is usually an inappropriate view for
an agent with a long existence in a complex environment.
In spite of the mismatch between embedded reinforcement learning and the train/test
perspective, Fiechter (1994) provides a PAC analysis for Q-learning (described in
Section 4.2) that sheds some light on the connection between the two views.
Measures related to speed of learning have an additional weakness. An algorithm
that merely tries to achieve optimality as fast as possible may incur unnecessarily
large penalties during the learning period. A less aggressive strategy taking longer to
achieve optimality, but gaining greater total reinforcement during its learning might
be preferable.

 Regret. A more appropriate measure, then, is the expected decrease in reward gained

due to executing the learning algorithm instead of behaving optimally from the very
beginning. This measure is known as regret (Berry & Fristedt, 1985). It penalizes
mistakes wherever they occur during the run. Unfortunately, results concerning the
regret of algorithms are quite hard to obtain.
242

fiReinforcement Learning: A Survey

1.4 Reinforcement Learning and Adaptive Control
Adaptive control (Burghes & Graham, 1980; Stengel, 1986) is also concerned with algorithms for improving a sequence of decisions from experience. Adaptive control is a much
more mature discipline that concerns itself with dynamic systems in which states and actions are vectors and system dynamics are smooth: linear or locally linearizable around a
desired trajectory. A very common formulation of cost functions in adaptive control are
quadratic penalties on deviation from desired state and action vectors. Most importantly,
although the dynamic model of the system is not known in advance, and must be estimated from data, the structure of the dynamic model is fixed, leaving model estimation
as a parameter estimation problem. These assumptions permit deep, elegant and powerful
mathematical analysis, which in turn lead to robust, practical, and widely deployed adaptive
control algorithms.

2. Exploitation versus Exploration: The Single-State Case
One major difference between reinforcement learning and supervised learning is that a
reinforcement-learner must explicitly explore its environment. In order to highlight the
problems of exploration, we treat a very simple case in this section. The fundamental issues
and approaches described here will, in many cases, transfer to the more complex instances
of reinforcement learning discussed later in the paper.
The simplest possible reinforcement-learning problem is known as the k-armed bandit
problem, which has been the subject of a great deal of study in the statistics and applied
mathematics literature (Berry & Fristedt, 1985). The agent is in a room with a collection of
k gambling machines (each called a \one-armed bandit" in colloquial English). The agent is
permitted a fixed number of pulls, h. Any arm may be pulled on each turn. The machines
do not require a deposit to play; the only cost is in wasting a pull playing a suboptimal
machine. When arm i is pulled, machine i pays off 1 or 0, according to some underlying
probability parameter pi , where payoffs are independent events and the pi s are unknown.
What should the agent's strategy be?
This problem illustrates the fundamental tradeoff between exploitation and exploration.
The agent might believe that a particular arm has a fairly high payoff probability; should
it choose that arm all the time, or should it choose another one that it has less information
about, but seems to be worse? Answers to these questions depend on how long the agent
is expected to play the game; the longer the game lasts, the worse the consequences of
prematurely converging on a sub-optimal arm, and the more the agent should explore.
There is a wide variety of solutions to this problem. We will consider a representative
selection of them, but for a deeper discussion and a number of important theoretical results,
see the book by Berry and Fristedt (1985). We use the term \action" to indicate the
agent's choice of arm to pull. This eases the transition into delayed reinforcement models
in Section 3. It is very important to note that bandit problems fit our definition of a
reinforcement-learning environment with a single state with only self transitions.
Section 2.1 discusses three solutions to the basic one-state bandit problem that have
formal correctness results. Although they can be extended to problems with real-valued
rewards, they do not apply directly to the general multi-state delayed-reinforcement case.
243

fiKaelbling, Littman, & Moore

Section 2.2 presents three techniques that are not formally justified, but that have had wide
use in practice, and can be applied (with similar lack of guarantee) to the general case.

2.1 Formally Justified Techniques

There is a fairly well-developed formal theory of exploration for very simple problems.
Although it is instructive, the methods it provides do not scale well to more complex
problems.
2.1.1 Dynamic-Programming Approach

If the agent is going to be acting for a total of h steps, it can use basic Bayesian reasoning
to solve for an optimal strategy (Berry & Fristedt, 1985). This requires an assumed prior
joint distribution for the parameters fpig, the most natural of which is that each pi is
independently uniformly distributed between 0 and 1. We compute a mapping from belief
states (summaries of the agent's experiences during this run) to actions. Here, a belief state
can be represented as a tabulation of action choices and payoffs: fn1; w1; n2; w2; : : :; nk ; wk g
denotes a state of play in which each arm i has been pulled ni times with wi payoffs. We
write V  (n1; w1; : : :; nk ; wk ) as the expected payoff remaining, given that a total of h pulls
are available,
and we use the remaining pulls optimally.
P
If i ni = h, then there are no remaining pulls, and V (n1 ; w1; : : :; nk ; wk ) = 0. This is
the basis of a recursive definition. If we know the V  value for all belief states with t pulls
remaining, we can compute the V  value of any belief state with t + 1 pulls remaining:

V (n1; w1; : : :; nk ; wk) = maxi E
= maxi

"

#

Future payoff if agent takes action i,
then acts optimally for remaining pulls !
iV (n1; wi; : : :; ni + 1; wi + 1; : : :; nk ; wk)+
(1 , i)V (n1 ; wi; : : :; ni + 1; wi; : : :; nk ; wk )

where i is the posterior subjective probability of action i paying off given ni , wi and
our prior probability. For the uniform priors, which result in a beta distribution, i =
(wi + 1)=(ni + 2).
The expense of filling in the table of V  values in this way for all attainable belief states
is linear in the number of belief states times actions, and thus exponential in the horizon.
2.1.2 Gittins Allocation Indices

Gittins gives an \allocation index" method for finding the optimal choice of action at each
step in k-armed bandit problems (Gittins, 1989). The technique only applies under the
discounted expected reward criterion. For each action, consider the number of times it has
been chosen, n, versus the number of times it has paid off, w. For certain discount factors,
there are published tables of \index values," I (n; w) for each pair of n and w. Look up
the index value for each action i, I (ni ; wi). It represents a comparative measure of the
combined value of the expected payoff of action i (given its history of payoffs) and the value
of the information that we would get by choosing it. Gittins has shown that choosing the
action with the largest index value guarantees the optimal balance between exploration and
exploitation.
244

fiReinforcement Learning: A Survey

a=1

a=0
1

2

3

N-1

N

2N
r=1

2

3

N-1

N+3

N+2

N+1

N+2

N+1

a=1

a=0
1

2N-1

N

2N

2N-1

N+3

r=0

Figure 3: A Tsetlin automaton with 2N states. The top row shows the state transitions
that are made when the previous action resulted in a reward of 1; the bottom
row shows transitions after a reward of 0. In states in the left half of the figure,
action 0 is taken; in those on the right, action 1 is taken.
Because of the guarantee of optimal exploration and the simplicity of the technique
(given the table of index values), this approach holds a great deal of promise for use in more
complex applications. This method proved useful in an application to robotic manipulation
with immediate reward (Salganicoff & Ungar, 1995). Unfortunately, no one has yet been
able to find an analog of index values for delayed reinforcement problems.
2.1.3 Learning Automata

A branch of the theory of adaptive control is devoted to learning automata, surveyed by
Narendra and Thathachar (1989), which were originally described explicitly as finite state
automata. The Tsetlin automaton shown in Figure 3 provides an example that solves a
2-armed bandit arbitrarily near optimally as N approaches infinity.
It is inconvenient to describe algorithms as finite-state automata, so a move was made
to describe the internal state of the agent as a probability distribution according to which
actions would be chosen. The probabilities of taking different actions would be adjusted
according to their previous successes and failures.
An example, which stands among a set of algorithms independently developed in the
mathematical psychology literature (Hilgard & Bower, 1975), is the linear reward-inaction
algorithm. Let pi be the agent's probability of taking action i.
 When action ai succeeds,
pi := pi + ff(1 , pi)
pj := pj , ffpj for j 6= i

 When action ai fails, pj remains unchanged (for all j ).

This algorithm converges with probability 1 to a vector containing a single 1 and the
rest 0's (choosing a particular action with probability 1). Unfortunately, it does not always
converge to the correct action; but the probability that it converges to the wrong one can
be made arbitrarily small by making ff small (Narendra & Thathachar, 1974). There is no
literature on the regret of this algorithm.
245

fiKaelbling, Littman, & Moore

2.2 Ad-Hoc Techniques

In reinforcement-learning practice, some simple, ad hoc strategies have been popular. They
are rarely, if ever, the best choice for the models of optimality we have used, but they may
be viewed as reasonable, computationally tractable, heuristics. Thrun (1992) has surveyed
a variety of these techniques.
2.2.1 Greedy Strategies

The first strategy that comes to mind is to always choose the action with the highest estimated payoff. The aw is that early unlucky sampling might indicate that the best action's
reward is less than the reward obtained from a suboptimal action. The suboptimal action
will always be picked, leaving the true optimal action starved of data and its superiority
never discovered. An agent must explore to ameliorate this outcome.
A useful heuristic is optimism in the face of uncertainty in which actions are selected
greedily, but strongly optimistic prior beliefs are put on their payoffs so that strong negative
evidence is needed to eliminate an action from consideration. This still has a measurable
danger of starving an optimal but unlucky action, but the risk of this can be made arbitrarily small. Techniques like this have been used in several reinforcement learning algorithms
including the interval exploration method (Kaelbling, 1993b) (described shortly), the exploration bonus in Dyna (Sutton, 1990), curiosity-driven exploration (Schmidhuber, 1991a),
and the exploration mechanism in prioritized sweeping (Moore & Atkeson, 1993).
2.2.2 Randomized Strategies

Another simple exploration strategy is to take the action with the best estimated expected
reward by default, but with probability p, choose an action at random. Some versions of
this strategy start with a large value of p to encourage initial exploration, which is slowly
decreased.
An objection to the simple strategy is that when it experiments with a non-greedy action
it is no more likely to try a promising alternative than a clearly hopeless alternative. A
slightly more sophisticated strategy is Boltzmann exploration. In this case, the expected
reward for taking action a, ER(a) is used to choose an action probabilistically according to
the distribution
ER(a)=T
P (a) = P e ER(a )=T :
a 2A e

0

0

The temperature parameter T can be decreased over time to decrease exploration. This
method works well if the best action is well separated from the others, but suffers somewhat
when the values of the actions are close. It may also converge unnecessarily slowly unless
the temperature schedule is manually tuned with great care.
2.2.3 Interval-based Techniques

Exploration is often more ecient when it is based on second-order information about the
certainty or variance of the estimated values of actions. Kaelbling's interval estimation
algorithm (1993b) stores statistics for each action ai : wi is the number of successes and ni
the number of trials. An action is chosen by computing the upper bound of a 100  (1 , ff)%
246

fiReinforcement Learning: A Survey

confidence interval on the success probability of each action and choosing the action with
the highest upper bound. Smaller values of the ff parameter encourage greater exploration.
When payoffs are boolean, the normal approximation to the binomial distribution can be
used to construct the confidence interval (though the binomial should be used for small
n). Other payoff distributions can be handled using their associated statistics or with
nonparametric methods. The method works very well in empirical trials. It is also related
to a certain class of statistical techniques known as experiment design methods (Box &
Draper, 1987), which are used for comparing multiple treatments (for example, fertilizers
or drugs) to determine which treatment (if any) is best in as small a set of experiments as
possible.

2.3 More General Problems

When there are multiple states, but reinforcement is still immediate, then any of the above
solutions can be replicated, once for each state. However, when generalization is required,
these solutions must be integrated with generalization methods (see section 6); this is
straightforward for the simple ad-hoc methods, but it is not understood how to maintain
theoretical guarantees.
Many of these techniques focus on converging to some regime in which exploratory
actions are taken rarely or never; this is appropriate when the environment is stationary.
However, when the environment is non-stationary, exploration must continue to take place,
in order to notice changes in the world. Again, the more ad-hoc techniques can be modified
to deal with this in a plausible manner (keep temperature parameters from going to 0; decay
the statistics in interval estimation), but none of the theoretically guaranteed methods can
be applied.

3. Delayed Reward

In the general case of the reinforcement learning problem, the agent's actions determine
not only its immediate reward, but also (at least probabilistically) the next state of the
environment. Such environments can be thought of as networks of bandit problems, but
the agent must take into account the next state as well as the immediate reward when it
decides which action to take. The model of long-run optimality the agent is using determines
exactly how it should take the value of the future into account. The agent will have to be
able to learn from delayed reinforcement: it may take a long sequence of actions, receiving
insignificant reinforcement, then finally arrive at a state with high reinforcement. The agent
must be able to learn which of its actions are desirable based on reward that can take place
arbitrarily far in the future.

3.1 Markov Decision Processes

Problems with delayed reinforcement are well modeled as Markov decision processes (MDPs).
An MDP consists of

 a set of states S ,
 a set of actions A,
247

fiKaelbling, Littman, & Moore

 a reward function R : S  A ! <, and
 a state transition function T : SA ! (S ), where a member of (S ) is a probability
distribution over the set S (i.e. it maps states to probabilities). We write T (s; a; s0)

for the probability of making a transition from state s to state s0 using action a.
The state transition function probabilistically specifies the next state of the environment as
a function of its current state and the agent's action. The reward function specifies expected
instantaneous reward as a function of the current state and action. The model is Markov if
the state transitions are independent of any previous environment states or agent actions.
There are many good references to MDP models (Bellman, 1957; Bertsekas, 1987; Howard,
1960; Puterman, 1994).
Although general MDPs may have infinite (even uncountable) state and action spaces,
we will only discuss methods for solving finite-state and finite-action problems. In section 6,
we discuss methods for solving problems with continuous input and output spaces.

3.2 Finding a Policy Given a Model

Before we consider algorithms for learning to behave in MDP environments, we will explore techniques for determining the optimal policy given a correct model. These dynamic
programming techniques will serve as the foundation and inspiration for the learning algorithms to follow. We restrict our attention mainly to finding optimal policies for the
infinite-horizon discounted model, but most of these algorithms have analogs for the finitehorizon and average-case models as well. We rely on the result that, for the infinite-horizon
discounted model, there exists an optimal deterministic stationary policy (Bellman, 1957).
We will speak of the optimal value of a state|it is the expected infinite discounted sum
of reward that the agent will gain if it starts in that state and executes the optimal policy.
Using  as a complete decision policy, it is written

!
1
X

t
V (s) = max
 E t=0  rt

:

This optimal value function is unique and can be defined as the solution to the simultaneous
equations
0
1

@
V (s) = max
a R(s; a) + 

X

s 2S

T (s; a; s0)V (s0)A ; 8s 2 S ;

(1)

0

which assert that the value of a state s is the expected instantaneous reward plus the
expected discounted value of the next state, using the best available action. Given the
optimal value function, we can specify the optimal policy as

0
1
X
@
(s) = arg max
T (s; a; s0)V (s0)A :
a R(s; a) + 
s 2S
0

3.2.1 Value Iteration

One way, then, to find an optimal policy is to find the optimal value function. It can
be determined by a simple iterative algorithm called value iteration that can be shown to
converge to the correct V  values (Bellman, 1957; Bertsekas, 1987).
248

fiReinforcement Learning: A Survey

V s

initialize ( ) arbitrarily
loop until policy good enough
loop for
loop for

s2S
a2A
Q(s; a) := R(s; a) +  Ps 2S T (s; a; s0)V (s0)
V (s) := maxa Q(s; a)
0

end loop
end loop

It is not obvious when to stop the value iteration algorithm. One important result
bounds the performance of the current greedy policy as a function of the Bellman residual of
the current value function (Williams & Baird, 1993b). It says that if the maximum difference
between two successive value functions is less than , then the value of the greedy policy,
(the policy obtained by choosing, in every state, the action that maximizes the estimated
discounted reward, using the current estimate of the value function) differs from the value
function of the optimal policy by no more than 2=(1 ,  ) at any state. This provides an
effective stopping criterion for the algorithm. Puterman (1994) discusses another stopping
criterion, based on the span semi-norm, which may result in earlier termination. Another
important result is that the greedy policy is guaranteed to be optimal in some finite number
of steps even though the value function may not have converged (Bertsekas, 1987). And in
practice, the greedy policy is often optimal long before the value function has converged.
Value iteration is very exible. The assignments to V need not be done in strict order
as shown above, but instead can occur asynchronously in parallel provided that the value
of every state gets updated infinitely often on an infinite run. These issues are treated
extensively by Bertsekas (1989), who also proves convergence results.
Updates based on Equation 1 are known as full backups since they make use of information from all possible successor states. It can be shown that updates of the form

Q(s; a) := Q(s; a) + ff(r +  max
Q(s0; a0) , Q(s; a))
a
0

can also be used as long as each pairing of a and s is updated infinitely often, s0 is sampled
from the distribution T (s; a; s0), r is sampled with mean R(s; a) and bounded variance, and
the learning rate ff is decreased slowly. This type of sample backup (Singh, 1993) is critical
to the operation of the model-free methods discussed in the next section.
The computational complexity of the value-iteration algorithm with full backups, per
iteration, is quadratic in the number of states and linear in the number of actions. Commonly, the transition probabilities T (s; a; s0) are sparse. If there are on average a constant
number of next states with non-zero probability then the cost per iteration is linear in the
number of states and linear in the number of actions. The number of iterations required to
reach the optimal value function is polynomial in the number of states and the magnitude
of the largest reward if the discount factor is held constant. However, in the worst case
the number of iterations grows polynomially in 1=(1 ,  ), so the convergence rate slows
considerably as the discount factor approaches 1 (Littman, Dean, & Kaelbling, 1995b).
249

fiKaelbling, Littman, & Moore

3.2.2 Policy Iteration

The policy iteration algorithm manipulates the policy directly, rather than finding it indirectly via the optimal value function. It operates as follows:
choose an arbitrary policy
loop

0

 := 0

compute the value function of policy
solve the linear equations

:

V (s) = R(s; (s)) +  Ps 2S T (s; (s); s0)V (s0)
0

0(s) := arg maxa (R(s; a) +  Ps 2S T (s; a; s0)V(s0))
 = 0

improve the policy at each state:
0

until

The value function of a policy is just the expected infinite discounted reward that will
be gained, at each state, by executing that policy. It can be determined by solving a set
of linear equations. Once we know the value of each state under the current policy, we
consider whether the value could be improved by changing the first action taken. If it can,
we change the policy to take the new action whenever it is in that situation. This step is
guaranteed to strictly improve the performance of the policy. When no improvements are
possible, then the policy is guaranteed to be optimal.
Since there are at most jAjjSj distinct policies, and the sequence of policies improves at
each step, this algorithm terminates in at most an exponential number of iterations (Puterman, 1994). However, it is an important open question how many iterations policy iteration
takes in the worst case. It is known that the running time is pseudopolynomial and that for
any fixed discount factor, there is a polynomial bound in the total size of the MDP (Littman
et al., 1995b).
3.2.3 Enhancement to Value Iteration and Policy Iteration

In practice, value iteration is much faster per iteration, but policy iteration takes fewer
iterations. Arguments have been put forth to the effect that each approach is better for
large problems. Puterman's modified policy iteration algorithm (Puterman & Shin, 1978)
provides a method for trading iteration time for iteration improvement in a smoother way.
The basic idea is that the expensive part of policy iteration is solving for the exact value
of V . Instead of finding an exact value for V , we can perform a few steps of a modified
value-iteration step where the policy is held fixed over successive iterations. This can be
shown to produce an approximation to V that converges linearly in  . In practice, this can
result in substantial speedups.
Several standard numerical-analysis techniques that speed the convergence of dynamic
programming can be used to accelerate value and policy iteration. Multigrid methods can
be used to quickly seed a good initial approximation to a high resolution value function
by initially performing value iteration at a coarser resolution (Rude, 1993). State aggregation works by collapsing groups of states to a single meta-state solving the abstracted
problem (Bertsekas & Casta~non, 1989).
250

fiReinforcement Learning: A Survey

3.2.4 Computational Complexity

Value iteration works by producing successive approximations of the optimal value function.
Each iteration can be performed in O(jAjjS j2) steps, or faster if there is sparsity in the
transition function. However, the number of iterations required can grow exponentially in
the discount factor (Condon, 1992); as the discount factor approaches 1, the decisions must
be based on results that happen farther and farther into the future. In practice, policy
iteration converges in fewer iterations than value iteration, although the per-iteration costs
of O(jAjjS j2 + jS j3) can be prohibitive. There is no known tight worst-case bound available
for policy iteration (Littman et al., 1995b). Modified policy iteration (Puterman & Shin,
1978) seeks a trade-off between cheap and effective iterations and is preferred by some
practictioners (Rust, 1996).
Linear programming (Schrijver, 1986) is an extremely general problem, and MDPs can
be solved by general-purpose linear-programming packages (Derman, 1970; D'Epenoux,
1963; Hoffman & Karp, 1966). An advantage of this approach is that commercial-quality
linear-programming packages are available, although the time and space requirements can
still be quite high. From a theoretic perspective, linear programming is the only known
algorithm that can solve MDPs in polynomial time, although the theoretically ecient
algorithms have not been shown to be ecient in practice.

4. Learning an Optimal Policy: Model-free Methods

In the previous section we reviewed methods for obtaining an optimal policy for an MDP
assuming that we already had a model. The model consists of knowledge of the state transition probability function T (s; a; s0) and the reinforcement function R(s; a). Reinforcement
learning is primarily concerned with how to obtain the optimal policy when such a model
is not known in advance. The agent must interact with its environment directly to obtain
information which, by means of an appropriate algorithm, can be processed to produce an
optimal policy.
At this point, there are two ways to proceed.

 Model-free: Learn a controller without learning a model.
 Model-based: Learn a model, and use it to derive a controller.
Which approach is better? This is a matter of some debate in the reinforcement-learning
community. A number of algorithms have been proposed on both sides. This question also
appears in other fields, such as adaptive control, where the dichotomy is between direct and
indirect adaptive control.
This section examines model-free learning, and Section 5 examines model-based methods.
The biggest problem facing a reinforcement-learning agent is temporal credit assignment.
How do we know whether the action just taken is a good one, when it might have farreaching effects? One strategy is to wait until the \end" and reward the actions taken if
the result was good and punish them if the result was bad. In ongoing tasks, it is dicult
to know what the \end" is, and this might require a great deal of memory. Instead, we
will use insights from value iteration to adjust the estimated value of a state based on
251

fiKaelbling, Littman, & Moore

r
s

v

AHC

a
RL

Figure 4: Architecture for the adaptive heuristic critic.
the immediate reward and the estimated value of the next state. This class of algorithms
is known as temporal difference methods (Sutton, 1988). We will consider two different
temporal-difference learning strategies for the discounted infinite-horizon model.

4.1 Adaptive Heuristic Critic and TD()

The adaptive heuristic critic algorithm is an adaptive version of policy iteration (Barto,
Sutton, & Anderson, 1983) in which the value-function computation is no longer implemented by solving a set of linear equations, but is instead computed by an algorithm called
TD(0). A block diagram for this approach is given in Figure 4. It consists of two components: a critic (labeled AHC), and a reinforcement-learning component (labeled RL). The
reinforcement-learning component can be an instance of any of the k-armed bandit algorithms, modified to deal with multiple states and non-stationary rewards. But instead of
acting to maximize instantaneous reward, it will be acting to maximize the heuristic value,
v, that is computed by the critic. The critic uses the real external reinforcement signal to
learn to map states to their expected discounted values given that the policy being executed
is the one currently instantiated in the RL component.
We can see the analogy with modified policy iteration if we imagine these components
working in alternation. The policy  implemented by RL is fixed and the critic learns the
value function V for that policy. Now we fix the critic and let the RL component learn a
new policy  0 that maximizes the new value function, and so on. In most implementations,
however, both components operate simultaneously. Only the alternating implementation
can be guaranteed to converge to the optimal policy, under appropriate conditions. Williams
and Baird explored the convergence properties of a class of AHC-related algorithms they
call \incremental variants of policy iteration" (Williams & Baird, 1993a).
It remains to explain how the critic can learn the value of a policy. We define hs; a; r; s0i
to be an experience tuple summarizing a single transition in the environment. Here s is the
agent's state before the transition, a is its choice of action, r the instantaneous reward it
receives, and s0 its resulting state. The value of a policy is learned using Sutton's TD(0)
algorithm (Sutton, 1988) which uses the update rule
V (s) := V (s) + ff(r + V (s0) , V (s)) :
Whenever a state s is visited, its estimated value is updated to be closer to r + V (s0 ),
since r is the instantaneous reward received and V (s0) is the estimated value of the actually
occurring next state. This is analogous to the sample-backup rule from value iteration|the
only difference is that the sample is drawn from the real world rather than by simulating
a known model. The key idea is that r + V (s0 ) is a sample of the value of V (s), and it is
252

fiReinforcement Learning: A Survey

more likely to be correct because it incorporates the real r. If the learning rate ff is adjusted
properly (it must be slowly decreased) and the policy is held fixed, TD(0) is guaranteed to
converge to the optimal value function.
The TD(0) rule as presented above is really an instance of a more general class of
algorithms called TD(), with  = 0. TD(0) looks only one step ahead when adjusting
value estimates; although it will eventually arrive at the correct answer, it can take quite a
while to do so. The general TD() rule is similar to the TD(0) rule given above,

V (u) := V (u) + ff(r + V (s0) , V (s))e(u) ;
but it is applied to every state according to its eligibility e(u), rather than just to the
immediately previous state, s. One version of the eligibility trace is defined to be
e(s) =

t
X

k=1

( )t,k s;s

k

, where s;sk =

(

1 if s = sk .
0 otherwise

The eligibility of a state s is the degree to which it has been visited in the recent past;
when a reinforcement is received, it is used to update all the states that have been recently
visited, according to their eligibility. When  = 0 this is equivalent to TD(0). When  = 1,
it is roughly equivalent to updating all the states according to the number of times they
were visited by the end of a run. Note that we can update the eligibility online as follows:

(

(s) + 1 if s = current state .
e(s) := e
e(s)
otherwise
It is computationally more expensive to execute the general TD(), though it often
converges considerably faster for large  (Dayan, 1992; Dayan & Sejnowski, 1994). There

has been some recent work on making the updates more ecient (Cichosz & Mulawka, 1995)
and on changing the definition to make TD() more consistent with the certainty-equivalent
method (Singh & Sutton, 1996), which is discussed in Section 5.1.

4.2 Q-learning

The work of the two components of AHC can be accomplished in a unified manner by
Watkins' Q-learning algorithm (Watkins, 1989; Watkins & Dayan, 1992). Q-learning is
typically easier to implement. In order to understand Q-learning, we have to develop some
additional notation. Let Q(s; a) be the expected discounted reinforcement of taking action
a in state s, then continuing by choosing actions optimally. Note that V (s) is the value
of s assuming the best action is taken initially, and so V  (s) = maxa Q (s; a). Q (s; a) can
hence be written recursively as

Q(s; a) = R(s; a) + 

X

s 2S
0

T (s; a; s0) max
Q(s0; a0) :
a
0

Note also that, since V  (s) = maxa Q (s; a), we have   (s) = arg maxa Q (s; a) as an
optimal policy.
Because the Q function makes the action explicit, we can estimate the Q values online using a method essentially the same as TD(0), but also use them to define the policy,
253

fiKaelbling, Littman, & Moore

because an action can be chosen just by taking the one with the maximum Q value for the
current state.
The Q-learning rule is

Q(s; a) := Q(s; a) + ff(r +  max
Q(s0; a0) , Q(s; a)) ;
a
0

where hs; a; r; s0i is an experience tuple as described earlier. If each action is executed in
each state an infinite number of times on an infinite run and ff is decayed appropriately, the
Q values will converge with probability 1 to Q (Watkins, 1989; Tsitsiklis, 1994; Jaakkola,
Jordan, & Singh, 1994). Q-learning can also be extended to update states that occurred
more than one step previously, as in TD() (Peng & Williams, 1994).
When the Q values are nearly converged to their optimal values, it is appropriate for
the agent to act greedily, taking, in each situation, the action with the highest Q value.
During learning, however, there is a dicult exploitation versus exploration trade-off to be
made. There are no good, formally justified approaches to this problem in the general case;
standard practice is to adopt one of the ad hoc methods discussed in section 2.2.
AHC architectures seem to be more dicult to work with than Q-learning on a practical
level. It can be hard to get the relative learning rates right in AHC so that the two
components converge together. In addition, Q-learning is exploration insensitive: that
is, that the Q values will converge to the optimal values, independent of how the agent
behaves while the data is being collected (as long as all state-action pairs are tried often
enough). This means that, although the exploration-exploitation issue must be addressed
in Q-learning, the details of the exploration strategy will not affect the convergence of the
learning algorithm. For these reasons, Q-learning is the most popular and seems to be the
most effective model-free algorithm for learning from delayed reinforcement. It does not,
however, address any of the issues involved in generalizing over large state and/or action
spaces. In addition, it may converge quite slowly to a good policy.

4.3 Model-free Learning With Average Reward

As described, Q-learning can be applied to discounted infinite-horizon MDPs. It can also
be applied to undiscounted problems as long as the optimal policy is guaranteed to reach a
reward-free absorbing state and the state is periodically reset.
Schwartz (1993) examined the problem of adapting Q-learning to an average-reward
framework. Although his R-learning algorithm seems to exhibit convergence problems for
some MDPs, several researchers have found the average-reward criterion closer to the true
problem they wish to solve than a discounted criterion and therefore prefer R-learning to
Q-learning (Mahadevan, 1994).
With that in mind, researchers have studied the problem of learning optimal averagereward policies. Mahadevan (1996) surveyed model-based average-reward algorithms from
a reinforcement-learning perspective and found several diculties with existing algorithms.
In particular, he showed that existing reinforcement-learning algorithms for average reward
(and some dynamic programming algorithms) do not always produce bias-optimal policies. Jaakkola, Jordan and Singh (1995) described an average-reward learning algorithm
with guaranteed convergence properties. It uses a Monte-Carlo component to estimate the
expected future reward for each state as the agent moves through the environment. In
254

fiReinforcement Learning: A Survey

addition, Bertsekas presents a Q-learning-like algorithm for average-case reward in his new
textbook (1995). Although this recent work provides a much needed theoretical foundation
to this area of reinforcement learning, many important problems remain unsolved.

5. Computing Optimal Policies by Learning Models

The previous section showed how it is possible to learn an optimal policy without knowing
the models T (s; a; s0) or R(s; a) and without even learning those models en route. Although
many of these methods are guaranteed to find optimal policies eventually and use very
little computation time per experience, they make extremely inecient use of the data they
gather and therefore often require a great deal of experience to achieve good performance.
In this section we still begin by assuming that we don't know the models in advance, but
we examine algorithms that do operate by learning these models. These algorithms are
especially important in applications in which computation is considered to be cheap and
real-world experience costly.

5.1 Certainty Equivalent Methods

We begin with the most conceptually straightforward method: first, learn the T and R
functions by exploring the environment and keeping statistics about the results of each
action; next, compute an optimal policy using one of the methods of Section 3. This
method is known as certainty equivlance (Kumar & Varaiya, 1986).
There are some serious objections to this method:
 It makes an arbitrary division between the learning phase and the acting phase.
 How should it gather data about the environment initially? Random exploration
might be dangerous, and in some environments is an immensely inecient method of
gathering data, requiring exponentially more data (Whitehead, 1991) than a system
that interleaves experience gathering with policy-building more tightly (Koenig &
Simmons, 1993). See Figure 5 for an example.
 The possibility of changes in the environment is also problematic. Breaking up an
agent's life into a pure learning and a pure acting phase has a considerable risk that
the optimal controller based on early life becomes, without detection, a suboptimal
controller if the environment changes.
A variation on this idea is certainty equivalence, in which the model is learned continually
through the agent's lifetime and, at each step, the current model is used to compute an
optimal policy and value function. This method makes very effective use of available data,
but still ignores the question of exploration and is extremely computationally demanding,
even for fairly small state spaces. Fortunately, there are a number of other model-based
algorithms that are more practical.

5.2 Dyna

Sutton's Dyna architecture (1990, 1991) exploits a middle ground, yielding strategies that
are both more effective than model-free learning and more computationally ecient than
255

fiKaelbling, Littman, & Moore

1

2

3

.......

n

Goal

Figure 5: In this environment, due to Whitehead (1991), random exploration would take
take O(2n ) steps to reach the goal even once, whereas a more intelligent exploration strategy (e.g. \assume any untried action leads directly to goal") would
require only O(n2 ) steps.
the certainty-equivalence approach. It simultaneously uses experience to build a model (T^
and R^ ), uses experience to adjust the policy, and uses the model to adjust the policy.
Dyna operates in a loop of interaction with the environment. Given an experience tuple
hs; a; s0; ri, it behaves as follows:

 Update the model, incrementing statistics for the transition from s to s0 on action a
and for receiving reward r for taking action a in state s. The updated models are T^
and R^ .

 Update the policy at state s based on the newly updated model using the rule
X^
0
0 0
^
Q(s; a) := R(s; a) + 

s

0

T (s; a; s ) max
Q(s ; a ) ;
a
0

which is a version of the value-iteration update for Q values.

 Perform k additional updates: choose k state-action pairs at random and update them
according to the same rule as before:

Q(sk ; ak ):=R^(sk ; ak ) + 

X^
s

0

T (sk ; ak; s0) max
Q(s0; a0) :
a
0

 Choose an action a0 to perform in state s0, based on the Q values but perhaps modified
by an exploration strategy.

The Dyna algorithm requires about k times the computation of Q-learning per instance,
but this is typically vastly less than for the naive model-based method. A reasonable value
of k can be determined based on the relative speeds of computation and of taking action.
Figure 6 shows a grid world in which in each cell the agent has four actions (N, S, E,
W) and transitions are made deterministically to an adjacent cell, unless there is a block,
in which case no movement occurs. As we will see in Table 1, Dyna requires an order of
magnitude fewer steps of experience than does Q-learning to arrive at an optimal policy.
Dyna requires about six times more computational effort, however.
256

fiReinforcement Learning: A Survey

Figure 6: A 3277-state grid world. This was formulated as a shortest-path reinforcementlearning problem, which yields the same result as if a reward of 1 is given at the
goal, a reward of zero elsewhere and a discount factor is used.

Q-learning
Dyna
prioritized sweeping

Steps before Backups before
convergence
convergence
531,000
62,000
28,000

531,000
3,055,000
1,010,000

Table 1: The performance of three algorithms described in the text. All methods used
the exploration heuristic of \optimism in the face of uncertainty": any state not
previously visited was assumed by default to be a goal state. Q-learning used
its optimal learning rate parameter for a deterministic maze: ff = 1. Dyna and
prioritized sweeping were permitted to take k = 200 backups per transition. For
prioritized sweeping, the priority queue often emptied before all backups were
used.

257

fiKaelbling, Littman, & Moore

5.3 Prioritized Sweeping / Queue-Dyna
Although Dyna is a great improvement on previous methods, it suffers from being relatively
undirected. It is particularly unhelpful when the goal has just been reached or when the
agent is stuck in a dead end; it continues to update random state-action pairs, rather than
concentrating on the \interesting" parts of the state space. These problems are addressed
by prioritized sweeping (Moore & Atkeson, 1993) and Queue-Dyna (Peng & Williams,
1993), which are two independently-developed but very similar techniques. We will describe
prioritized sweeping in some detail.
The algorithm is similar to Dyna, except that updates are no longer chosen at random
and values are now associated with states (as in value iteration) instead of state-action pairs
(as in Q-learning). To make appropriate choices, we must store additional information in
the model. Each state remembers its predecessors: the states that have a non-zero transition
probability to it under some action. In addition, each state has a priority, initially set to
zero.
Instead of updating k random state-action pairs, prioritized sweeping updates k states
with the highest priority. For each high-priority state s, it works as follows:

 Remember the current value of the state: Vold = V (s).
 Update the state's value
^
V (s) := max
a R(s; a) + 

X^
s

T (s; a; s0)V (s0)

!

:

0

 Set the state's priority back to 0.
 Compute the value change  = jVold , V (s)j.
 Use  to modify the priorities of the predecessors of s.
If we have updated the V value for state s0 and it has changed by amount , then the
immediate predecessors of s0 are informed of this event. Any state s for which there exists
an action a such that T^(s; a; s0) 6= 0 has its priority promoted to   T^(s; a; s0), unless its
priority already exceeded that value.
The global behavior of this algorithm is that when a real-world transition is \surprising"
(the agent happens upon a goal state, for instance), then lots of computation is directed
to propagate this new information back to relevant predecessor states. When the realworld transition is \boring" (the actual result is very similar to the predicted result), then
computation continues in the most deserving part of the space.
Running prioritized sweeping on the problem in Figure 6, we see a large improvement
over Dyna. The optimal policy is reached in about half the number of steps of experience
and one-third the computation as Dyna required (and therefore about 20 times fewer steps
and twice the computational effort of Q-learning).
258

fiReinforcement Learning: A Survey

5.4 Other Model-Based Methods
Methods proposed for solving MDPs given a model can be used in the context of modelbased methods as well.
RTDP (real-time dynamic programming) (Barto, Bradtke, & Singh, 1995) is another
model-based method that uses Q-learning to concentrate computational effort on the areas
of the state-space that the agent is most likely to occupy. It is specific to problems in which
the agent is trying to achieve a particular goal state and the reward everywhere else is 0.
By taking into account the start state, it can find a short path from the start to the goal,
without necessarily visiting the rest of the state space.
The Plexus planning system (Dean, Kaelbling, Kirman, & Nicholson, 1993; Kirman,
1994) exploits a similar intuition. It starts by making an approximate version of the MDP
which is much smaller than the original one. The approximate MDP contains a set of states,
called the envelope, that includes the agent's current state and the goal state, if there is one.
States that are not in the envelope are summarized by a single \out" state. The planning
process is an alternation between finding an optimal policy on the approximate MDP and
adding useful states to the envelope. Action may take place in parallel with planning, in
which case irrelevant states are also pruned out of the envelope.

6. Generalization
All of the previous discussion has tacitly assumed that it is possible to enumerate the state
and action spaces and store tables of values over them. Except in very small environments,
this means impractical memory requirements. It also makes inecient use of experience. In
a large, smooth state space we generally expect similar states to have similar values and similar optimal actions. Surely, therefore, there should be some more compact representation
than a table. Most problems will have continuous or large discrete state spaces; some will
have large or continuous action spaces. The problem of learning in large spaces is addressed
through generalization techniques, which allow compact storage of learned information and
transfer of knowledge between \similar" states and actions.
The large literature of generalization techniques from inductive concept learning can be
applied to reinforcement learning. However, techniques often need to be tailored to specific
details of the problem. In the following sections, we explore the application of standard
function-approximation techniques, adaptive resolution models, and hierarchical methods
to the problem of reinforcement learning.
The reinforcement-learning architectures and algorithms discussed above have included
the storage of a variety of mappings, including S ! A (policies), S ! < (value functions),
S  A ! < (Q functions and rewards), S  A ! S (deterministic transitions), and S 
A  S ! [0; 1] (transition probabilities). Some of these mappings, such as transitions and
immediate rewards, can be learned using straightforward supervised learning, and can be
handled using any of the wide variety of function-approximation techniques for supervised
learning that support noisy training examples. Popular techniques include various neuralnetwork methods (Rumelhart & McClelland, 1986), fuzzy logic (Berenji, 1991; Lee, 1991).
CMAC (Albus, 1981), and local memory-based methods (Moore, Atkeson, & Schaal, 1995),
such as generalizations of nearest neighbor methods. Other mappings, especially the policy
259

fiKaelbling, Littman, & Moore

mapping, typically need specialized algorithms because training sets of input-output pairs
are not available.

6.1 Generalization over Input

A reinforcement-learning agent's current state plays a central role in its selection of rewardmaximizing actions. Viewing the agent as a state-free black box, a description of the
current state is its input. Depending on the agent architecture, its output is either an
action selection, or an evaluation of the current state that can be used to select an action.
The problem of deciding how the different aspects of an input affect the value of the output
is sometimes called the \structural credit-assignment" problem. This section examines
approaches to generating actions or evaluations as a function of a description of the agent's
current state.
The first group of techniques covered here is specialized to the case when reward is not
delayed; the second group is more generally applicable.
6.1.1 Immediate Reward

When the agent's actions do not inuence state transitions, the resulting problem becomes
one of choosing actions to maximize immediate reward as a function of the agent's current
state. These problems bear a resemblance to the bandit problems discussed in Section 2
except that the agent should condition its action selection on the current state. For this
reason, this class of problems has been described as associative reinforcement learning.
The algorithms in this section address the problem of learning from immediate boolean
reinforcement where the state is vector valued and the action is a boolean vector. Such
algorithms can and have been used in the context of a delayed reinforcement, for instance,
as the RL component in the AHC architecture described in Section 4.1. They can also be
generalized to real-valued reward through reward comparison methods (Sutton, 1984).
CRBP The complementary reinforcement backpropagation algorithm (Ackley & Littman,
1990) (crbp) consists of a feed-forward network mapping an encoding of the state to an
encoding of the action. The action is determined probabilistically from the activation of
the output units: if output unit i has activation yi , then bit i of the action vector has value
1 with probability yi , and 0 otherwise. Any neural-network supervised training procedure
can be used to adapt the network as follows. If the result of generating action a is r = 1,
then the network is trained with input-output pair hs; ai. If the result is r = 0, then the
network is trained with input-output pair hs; ai, where a = (1 , a1; : : :; 1 , an ).
The idea behind this training rule is that whenever an action fails to generate reward,
crbp will try to generate an action that is different from the current choice. Although it
seems like the algorithm might oscillate between an action and its complement, that does
not happen. One step of training a network will only change the action slightly and since
the output probabilities will tend to move toward 0.5, this makes action selection more
random and increases search. The hope is that the random distribution will generate an
action that works better, and then that action will be reinforced.
ARC The associative reinforcement comparison (arc) algorithm (Sutton, 1984) is an
instance of the ahc architecture for the case of boolean actions, consisting of two feed260

fiReinforcement Learning: A Survey

forward networks. One learns the value of situations, the other learns a policy. These can
be simple linear networks or can have hidden units.
In the simplest case, the entire system learns only to optimize immediate reward. First,
let us consider the behavior of the network that learns the policy, a mapping from a vector
describing s to a 0 or 1. If the output unit has activation yi , then a, the action generated,
will be 1 if y +  > 0, where  is normal noise, and 0 otherwise.
The adjustment for the output unit is, in the simplest case,

e = r(a , 1=2) ;
where the first factor is the reward received for taking the most recent action and the second
encodes which action was taken. The actions are encoded as 0 and 1, so a , 1=2 always has
the same magnitude; if the reward and the action have the same sign, then action 1 will be
made more likely, otherwise action 0 will be.
As described, the network will tend to seek actions that given positive reward. To extend
this approach to maximize reward, we can compare the reward to some baseline, b. This
changes the adjustment to
e = (r , b)(a , 1=2) ;
where b is the output of the second network. The second network is trained in a standard
supervised mode to estimate r as a function of the input state s.
Variations of this approach have been used in a variety of applications (Anderson, 1986;
Barto et al., 1983; Lin, 1993b; Sutton, 1984).
REINFORCE Algorithms Williams (1987, 1992) studied the problem of choosing actions to maximize immedate reward. He identified a broad class of update rules that perform gradient descent on the expected reward and showed how to integrate these rules with
backpropagation. This class, called reinforce algorithms, includes linear reward-inaction
(Section 2.1.3) as a special case.
The generic reinforce update for a parameter wij can be written
wij = ffij (r , bij ) @w@ ln(gj )
ij
where ffij is a non-negative factor, r the current reinforcement, bij a reinforcement baseline,
and gi is the probability density function used to randomly generate actions based on unit
activations. Both ffij and bij can take on different values for each wij , however, when ffij
is constant throughout the system, the expected update is exactly in the direction of the
expected reward gradient. Otherwise, the update is in the same half space as the gradient
but not necessarily in the direction of steepest increase.
Williams points out that the choice of baseline, bij , can have a profound effect on the
convergence speed of the algorithm.
Logic-Based Methods Another strategy for generalization in reinforcement learning is
to reduce the learning problem to an associative problem of learning boolean functions.
A boolean function has a vector of boolean inputs and a single boolean output. Taking
inspiration from mainstream machine learning work, Kaelbling developed two algorithms
for learning boolean functions from reinforcement: one uses the bias of k-DNF to drive
261

fiKaelbling, Littman, & Moore

the generalization process (Kaelbling, 1994b); the other searches the space of syntactic
descriptions of functions using a simple generate-and-test method (Kaelbling, 1994a).
The restriction to a single boolean output makes these techniques dicult to apply. In
very benign learning situations, it is possible to extend this approach to use a collection
of learners to independently learn the individual bits that make up a complex output. In
general, however, that approach suffers from the problem of very unreliable reinforcement:
if a single learner generates an inappropriate output bit, all of the learners receive a low
reinforcement value. The cascade method (Kaelbling, 1993b) allows a collection of learners
to be trained collectively to generate appropriate joint outputs; it is considerably more
reliable, but can require additional computational effort.
6.1.2 Delayed Reward

Another method to allow reinforcement-learning techniques to be applied in large state
spaces is modeled on value iteration and Q-learning. Here, a function approximator is used
to represent the value function by mapping a state description to a value.
Many reseachers have experimented with this approach: Boyan and Moore (1995) used
local memory-based methods in conjunction with value iteration; Lin (1991) used backpropagation networks for Q-learning; Watkins (1989) used CMAC for Q-learning; Tesauro (1992,
1995) used backpropagation for learning the value function in backgammon (described in
Section 8.1); Zhang and Dietterich (1995) used backpropagation and TD() to learn good
strategies for job-shop scheduling.
Although there have been some positive examples, in general there are unfortunate interactions between function approximation and the learning rules. In discrete environments
there is a guarantee that any operation that updates the value function (according to the
Bellman equations) can only reduce the error between the current value function and the
optimal value function. This guarantee no longer holds when generalization is used. These
issues are discussed by Boyan and Moore (1995), who give some simple examples of value
function errors growing arbitrarily large when generalization is used with value iteration.
Their solution to this, applicable only to certain classes of problems, discourages such divergence by only permitting updates whose estimated values can be shown to be near-optimal
via a battery of Monte-Carlo experiments.
Thrun and Schwartz (1993) theorize that function approximation of value functions
is also dangerous because the errors in value functions due to generalization can become
compounded by the \max" operator in the definition of the value function.
Several recent results (Gordon, 1995; Tsitsiklis & Van Roy, 1996) show how the appropriate choice of function approximator can guarantee convergence, though not necessarily to
the optimal values. Baird's residual gradient technique (Baird, 1995) provides guaranteed
convergence to locally optimal solutions.
Perhaps the gloominess of these counter-examples is misplaced. Boyan and Moore (1995)
report that their counter-examples can be made to work with problem-specific hand-tuning
despite the unreliability of untuned algorithms that provably converge in discrete domains.
Sutton (1996) shows how modified versions of Boyan and Moore's examples can converge
successfully. An open question is whether general principles, ideally supported by theory,
can help us understand when value function approximation will succeed. In Sutton's com262

fiReinforcement Learning: A Survey

parative experiments with Boyan and Moore's counter-examples, he changes four aspects
of the experiments:
1. Small changes to the task specifications.
2. A very different kind of function approximator (CMAC (Albus, 1975)) that has weak
generalization.
3. A different learning algorithm: SARSA (Rummery & Niranjan, 1994) instead of value
iteration.
4. A different training regime. Boyan and Moore sampled states uniformly in state space,
whereas Sutton's method sampled along empirical trajectories.
There are intuitive reasons to believe that the fourth factor is particularly important, but
more careful research is needed.
Adaptive Resolution Models In many cases, what we would like to do is partition
the environment into regions of states that can be considered the same for the purposes of
learning and generating actions. Without detailed prior knowledge of the environment, it
is very dicult to know what granularity or placement of partitions is appropriate. This
problem is overcome in methods that use adaptive resolution; during the course of learning,
a partition is constructed that is appropriate to the environment.
Decision Trees In environments that are characterized by a set of boolean or discretevalued variables, it is possible to learn compact decision trees for representing Q values. The
G-learning algorithm (Chapman & Kaelbling, 1991), works as follows. It starts by assuming
that no partitioning is necessary and tries to learn Q values for the entire environment as
if it were one state. In parallel with this process, it gathers statistics based on individual
input bits; it asks the question whether there is some bit b in the state description such
that the Q values for states in which b = 1 are significantly different from Q values for
states in which b = 0. If such a bit is found, it is used to split the decision tree. Then,
the process is repeated in each of the leaves. This method was able to learn very small
representations of the Q function in the presence of an overwhelming number of irrelevant,
noisy state attributes. It outperformed Q-learning with backpropagation in a simple videogame environment and was used by McCallum (1995) (in conjunction with other techniques
for dealing with partial observability) to learn behaviors in a complex driving-simulator. It
cannot, however, acquire partitions in which attributes are only significant in combination
(such as those needed to solve parity problems).
Variable Resolution Dynamic Programming The VRDP algorithm (Moore, 1991)
enables conventional dynamic programming to be performed in real-valued multivariate
state-spaces where straightforward discretization would fall prey to the curse of dimensionality. A kd-tree (similar to a decision tree) is used to partition state space into coarse
regions. The coarse regions are refined into detailed regions, but only in parts of the state
space which are predicted to be important. This notion of importance is obtained by running \mental trajectories" through state space. This algorithm proved effective on a number
of problems for which full high-resolution arrays would have been impractical. It has the
disadvantage of requiring a guess at an initially valid trajectory through state-space.
263

fiKaelbling, Littman, & Moore

(a)

(b)

(c)

G

G

G

Goal

Start

Figure 7: (a) A two-dimensional maze problem. The point robot must find a path from
start to goal without crossing any of the barrier lines. (b) The path taken by
PartiGame during the entire first trial. It begins with intense exploration to find a
route out of the almost entirely enclosed start region. Having eventually reached
a suciently high resolution, it discovers the gap and proceeds greedily towards
the goal, only to be temporarily blocked by the goal's barrier region. (c) The
second trial.

PartiGame Algorithm Moore's PartiGame algorithm (Moore, 1994) is another solution

to the problem of learning to achieve goal configurations in deterministic high-dimensional
continuous spaces by learning an adaptive-resolution model. It also divides the environment
into cells; but in each cell, the actions available consist of aiming at the neighboring cells
(this aiming is accomplished by a local controller, which must be provided as part of the
problem statement). The graph of cell transitions is solved for shortest paths in an online
incremental manner, but a minimax criterion is used to detect when a group of cells is
too coarse to prevent movement between obstacles or to avoid limit cycles. The offending
cells are split to higher resolution. Eventually, the environment is divided up just enough to
choose appropriate actions for achieving the goal, but no unnecessary distinctions are made.
An important feature is that, as well as reducing memory and computational requirements,
it also structures exploration of state space in a multi-resolution manner. Given a failure,
the agent will initially try something very different to rectify the failure, and only resort to
small local changes when all the qualitatively different strategies have been exhausted.
Figure 7a shows a two-dimensional continuous maze. Figure 7b shows the performance
of a robot using the PartiGame algorithm during the very first trial. Figure 7c shows the
second trial, started from a slightly different position.
This is a very fast algorithm, learning policies in spaces of up to nine dimensions in less
than a minute. The restriction of the current implementation to deterministic environments
limits its applicability, however. McCallum (1995) suggests some related tree-structured
methods.
264

fiReinforcement Learning: A Survey

6.2 Generalization over Actions

The networks described in Section 6.1.1 generalize over state descriptions presented as
inputs. They also produce outputs in a discrete, factored representation and thus could be
seen as generalizing over actions as well.
In cases such as this when actions are described combinatorially, it is important to
generalize over actions to avoid keeping separate statistics for the huge number of actions
that can be chosen. In continuous action spaces, the need for generalization is even more
pronounced.
When estimating Q values using a neural network, it is possible to use either a distinct
network for each action, or a network with a distinct output for each action. When the
action space is continuous, neither approach is possible. An alternative strategy is to use a
single network with both the state and action as input and Q value as the output. Training
such a network is not conceptually dicult, but using the network to find the optimal action
can be a challenge. One method is to do a local gradient-ascent search on the action in
order to find one with high value (Baird & Klopf, 1993).
Gullapalli (1990, 1992) has developed a \neural" reinforcement-learning unit for use in
continuous action spaces. The unit generates actions with a normal distribution; it adjusts
the mean and variance based on previous experience. When the chosen actions are not
performing well, the variance is high, resulting in exploration of the range of choices. When
an action performs well, the mean is moved in that direction and the variance decreased,
resulting in a tendency to generate more action values near the successful one. This method
was successfully employed to learn to control a robot arm with many continuous degrees of
freedom.

6.3 Hierarchical Methods

Another strategy for dealing with large state spaces is to treat them as a hierarchy of
learning problems. In many cases, hierarchical solutions introduce slight sub-optimality in
performance, but potentially gain a good deal of eciency in execution time, learning time,
and space.
Hierarchical learners are commonly structured as gated behaviors, as shown in Figure 8.
There is a collection of behaviors that map environment states into low-level actions and
a gating function that decides, based on the state of the environment, which behavior's
actions should be switched through and actually executed. Maes and Brooks (1990) used
a version of this architecture in which the individual behaviors were fixed a priori and the
gating function was learned from reinforcement. Mahadevan and Connell (1991b) used the
dual approach: they fixed the gating function, and supplied reinforcement functions for the
individual behaviors, which were learned. Lin (1993a) and Dorigo and Colombetti (1995,
1994) both used this approach, first training the behaviors and then training the gating
function. Many of the other hierarchical learning methods can be cast in this framework.
6.3.1 Feudal Q-learning

Feudal Q-learning (Dayan & Hinton, 1993; Watkins, 1989) involves a hierarchy of learning
modules. In the simplest case, there is a high-level master and a low-level slave. The master
receives reinforcement from the external environment. Its actions consist of commands that
265

fiKaelbling, Littman, & Moore

s

b1
b2

g

a

b3
Figure 8: A structure of gated behaviors.
it can give to the low-level learner. When the master generates a particular command to
the slave, it must reward the slave for taking actions that satisfy the command, even if they
do not result in external reinforcement. The master, then, learns a mapping from states to
commands. The slave learns a mapping from commands and states to external actions. The
set of \commands" and their associated reinforcement functions are established in advance
of the learning.
This is really an instance of the general \gated behaviors" approach, in which the slave
can execute any of the behaviors depending on its command. The reinforcement functions
for the individual behaviors (commands) are given, but learning takes place simultaneously
at both the high and low levels.
6.3.2 Compositional Q-learning

Singh's compositional Q-learning (1992b, 1992a) (C-QL) consists of a hierarchy based on
the temporal sequencing of subgoals. The elemental tasks are behaviors that achieve some
recognizable condition. The high-level goal of the system is to achieve some set of conditions in sequential order. The achievement of the conditions provides reinforcement for the
elemental tasks, which are trained first to achieve individual subgoals. Then, the gating
function learns to switch the elemental tasks in order to achieve the appropriate high-level
sequential goal. This method was used by Tham and Prager (1994) to learn to control a
simulated multi-link robot arm.
6.3.3 Hierarchical Distance to Goal

Especially if we consider reinforcement learning modules to be part of larger agent architectures, it is important to consider problems in which goals are dynamically input to the
learner. Kaelbling's HDG algorithm (1993a) uses a hierarchical approach to solving problems when goals of achievement (the agent should get to a particular state as quickly as
possible) are given to an agent dynamically.
The HDG algorithm works by analogy with navigation in a harbor. The environment
is partitioned (a priori, but more recent work (Ashar, 1994) addresses the case of learning
the partition) into a set of regions whose centers are known as \landmarks." If the agent is
266

fiReinforcement Learning: A Survey

office
1/5

2/5
2/5

hall

hall
+100

printer

Figure 9: An example of a partially observable environment.
currently in the same region as the goal, then it uses low-level actions to move to the goal.
If not, then high-level information is used to determine the next landmark on the shortest
path from the agent's closest landmark to the goal's closest landmark. Then, the agent uses
low-level information to aim toward that next landmark. If errors in action cause deviations
in the path, there is no problem; the best aiming point is recomputed on every step.

7. Partially Observable Environments

In many real-world environments, it will not be possible for the agent to have perfect and
complete perception of the state of the environment. Unfortunately, complete observability
is necessary for learning methods based on MDPs. In this section, we consider the case in
which the agent makes observations of the state of the environment, but these observations
may be noisy and provide incomplete information. In the case of a robot, for instance,
it might observe whether it is in a corridor, an open room, a T-junction, etc., and those
observations might be error-prone. This problem is also referred to as the problem of
\incomplete perception," \perceptual aliasing," or \hidden state."
In this section, we will consider extensions to the basic MDP framework for solving
partially observable problems. The resulting formal model is called a partially observable
Markov decision process or POMDP.

7.1 State-Free Deterministic Policies

The most naive strategy for dealing with partial observability is to ignore it. That is, to
treat the observations as if they were the states of the environment and try to learn to
behave. Figure 9 shows a simple environment in which the agent is attempting to get to
the printer from an oce. If it moves from the oce, there is a good chance that the agent
will end up in one of two places that look like \hall", but that require different actions for
getting to the printer. If we consider these states to be the same, then the agent cannot
possibly behave optimally. But how well can it do?
The resulting problem is not Markovian, and Q-learning cannot be guaranteed to converge. Small breaches of the Markov requirement are well handled by Q-learning, but it is
possible to construct simple environments that cause Q-learning to oscillate (Chrisman &
267

fiKaelbling, Littman, & Moore

Littman, 1993). It is possible to use a model-based approach, however; act according to
some policy and gather statistics about the transitions between observations, then solve for
the optimal policy based on those observations. Unfortunately, when the environment is not
Markovian, the transition probabilities depend on the policy being executed, so this new
policy will induce a new set of transition probabilities. This approach may yield plausible
results in some cases, but again, there are no guarantees.
It is reasonable, though, to ask what the optimal policy (mapping from observations to
actions, in this case) is. It is NP-hard (Littman, 1994b) to find this mapping, and even the
best mapping can have very poor performance. In the case of our agent trying to get to the
printer, for instance, any deterministic state-free policy takes an infinite number of steps to
reach the goal on average.

7.2 State-Free Stochastic Policies

Some improvement can be gained by considering stochastic policies; these are mappings
from observations to probability distributions over actions. If there is randomness in the
agent's actions, it will not get stuck in the hall forever. Jaakkola, Singh, and Jordan (1995)
have developed an algorithm for finding locally-optimal stochastic policies, but finding a
globally optimal policy is still NP hard.
In our example, it turns out that the optimal stochastic policy
p is for the agent, when
2  0:6 and west with
in a state that
looks
like
a
hall,
to
go
east
with
probability
2
,
p
probability 2 , 1  0:4. This policy can be found by solving a simple (in this case)
quadratic program. The fact that such a simple example can produce irrational numbers
gives some indication that it is a dicult problem to solve exactly.

7.3 Policies with Internal State

The only way to behave truly effectively in a wide-range of environments is to use memory
of previous actions and observations to disambiguate the current state. There are a variety
of approaches to learning policies with internal state.
Recurrent Q-learning One intuitively simple approach is to use a recurrent neural network to learn Q values. The network can be trained using backpropagation through time (or
some other suitable technique) and learns to retain \history features" to predict value. This
approach has been used by a number of researchers (Meeden, McGraw, & Blank, 1993; Lin
& Mitchell, 1992; Schmidhuber, 1991b). It seems to work effectively on simple problems,
but can suffer from convergence to local optima on more complex problems.
Classifier Systems Classifier systems (Holland, 1975; Goldberg, 1989) were explicitly
developed to solve problems with delayed reward, including those requiring short-term
memory. The internal mechanism typically used to pass reward back through chains of
decisions, called the bucket brigade algorithm, bears a close resemblance to Q-learning. In
spite of some early successes, the original design does not appear to handle partially observed environments robustly.
Recently, this approach has been reexamined using insights from the reinforcementlearning literature, with some success. Dorigo did a comparative study of Q-learning and
classifier systems (Dorigo & Bersini, 1994). Cliff and Ross (1994) start with Wilson's zeroth268

fiReinforcement Learning: A Survey

i
SE

b



a

Figure 10: Structure of a POMDP agent.
level classifier system (Wilson, 1995) and add one and two-bit memory registers. They find
that, although their system can learn to use short-term memory registers effectively, the
approach is unlikely to scale to more complex environments.
Dorigo and Colombetti applied classifier systems to a moderately complex problem of
learning robot behavior from immediate reinforcement (Dorigo, 1995; Dorigo & Colombetti,
1994).

Finite-history-window Approach One way to restore the Markov property is to allow

decisions to be based on the history of recent observations and perhaps actions. Lin and
Mitchell (1992) used a fixed-width finite history window to learn a pole balancing task.
McCallum (1995) describes the \utile sux memory" which learns a variable-width window
that serves simultaneously as a model of the environment and a finite-memory policy. This
system has had excellent results in a very complex driving-simulation domain (McCallum,
1995). Ring (1994) has a neural-network approach that uses a variable history window,
adding history when necessary to disambiguate situations.
POMDP Approach Another strategy consists of using hidden Markov model (HMM)
techniques to learn a model of the environment, including the hidden state, then to use that
model to construct a perfect memory controller (Cassandra, Kaelbling, & Littman, 1994;
Lovejoy, 1991; Monahan, 1982).
Chrisman (1992) showed how the forward-backward algorithm for learning HMMs could
be adapted to learning POMDPs. He, and later McCallum (1993), also gave heuristic statesplitting rules to attempt to learn the smallest possible model for a given environment. The
resulting model can then be used to integrate information from the agent's observations in
order to make decisions.
Figure 10 illustrates the basic structure for a perfect-memory controller. The component
on the left is the state estimator, which computes the agent's belief state, b as a function of
the old belief state, the last action a, and the current observation i. In this context, a belief
state is a probability distribution over states of the environment, indicating the likelihood,
given the agent's past experience, that the environment is actually in each of those states.
The state estimator can be constructed straightforwardly using the estimated world model
and Bayes' rule.
Now we are left with the problem of finding a policy mapping belief states into action.
This problem can be formulated as an MDP, but it is dicult to solve using the techniques
described earlier, because the input space is continuous. Chrisman's approach (1992) does
not take into account future uncertainty, but yields a policy after a small amount of computation. A standard approach from the operations-research literature is to solve for the
269

fiKaelbling, Littman, & Moore

optimal policy (or a close approximation thereof) based on its representation as a piecewiselinear and convex function over the belief space. This method is computationally intractable,
but may serve as inspiration for methods that make further approximations (Cassandra
et al., 1994; Littman, Cassandra, & Kaelbling, 1995a).

8. Reinforcement Learning Applications

One reason that reinforcement learning is popular is that is serves as a theoretical tool for
studying the principles of agents learning to act. But it is unsurprising that it has also
been used by a number of researchers as a practical computational tool for constructing
autonomous systems that improve themselves with experience. These applications have
ranged from robotics, to industrial manufacturing, to combinatorial search problems such
as computer game playing.
Practical applications provide a test of the ecacy and usefulness of learning algorithms.
They are also an inspiration for deciding which components of the reinforcement learning
framework are of practical importance. For example, a researcher with a real robotic task
can provide a data point to questions such as:
 How important is optimal exploration? Can we break the learning period into exploration phases and exploitation phases?
 What is the most useful model of long-term reward: Finite horizon? Discounted?
Infinite horizon?
 How much computation is available between agent decisions and how should it be
used?
 What prior knowledge can we build into the system, and which algorithms are capable
of using that knowledge?
Let us examine a set of practical applications of reinforcement learning, while bearing these
questions in mind.

8.1 Game Playing

Game playing has dominated the Artificial Intelligence world as a problem domain ever since
the field was born. Two-player games do not fit into the established reinforcement-learning
framework since the optimality criterion for games is not one of maximizing reward in the
face of a fixed environment, but one of maximizing reward against an optimal adversary
(minimax). Nonetheless, reinforcement-learning algorithms can be adapted to work for a
very general class of games (Littman, 1994a) and many researchers have used reinforcement
learning in these environments. One application, spectacularly far ahead of its time, was
Samuel's checkers playing system (Samuel, 1959). This learned a value function represented
by a linear function approximator, and employed a training scheme similar to the updates
used in value iteration, temporal differences and Q-learning.
More recently, Tesauro (1992, 1994, 1995) applied the temporal difference algorithm
to backgammon. Backgammon has approximately 1020 states, making table-based reinforcement learning impossible. Instead, Tesauro used a backpropagation-based three-layer
270

fiReinforcement Learning: A Survey

Training
Games

Hidden
Units

300,000

80

TD 2.0

800,000

40

TD 2.1

1,500,000

80

Basic
TD 1.0

Results
Poor
Lost by 13 points in 51
games
Lost by 7 points in 38
games
Lost by 1 point in 40
games

Table 2: TD-Gammon's performance in games against the top human professional players.
A backgammon tournament involves playing a series of games for points until one
player reaches a set target. TD-Gammon won none of these tournaments but came
suciently close that it is now considered one of the best few players in the world.
neural network as a function approximator for the value function
Board Position ! Probability of victory for current player:

Two versions of the learning algorithm were used. The first, which we will call Basic TDGammon, used very little predefined knowledge of the game, and the representation of a
board position was virtually a raw encoding, suciently powerful only to permit the neural
network to distinguish between conceptually different positions. The second, TD-Gammon,
was provided with the same raw state information supplemented by a number of handcrafted features of backgammon board positions. Providing hand-crafted features in this
manner is a good example of how inductive biases from human knowledge of the task can
be supplied to a learning algorithm.
The training of both learning algorithms required several months of computer time, and
was achieved by constant self-play. No exploration strategy was used|the system always
greedily chose the move with the largest expected probability of victory. This naive exploration strategy proved entirely adequate for this environment, which is perhaps surprising
given the considerable work in the reinforcement-learning literature which has produced
numerous counter-examples to show that greedy exploration can lead to poor learning performance. Backgammon, however, has two important properties. Firstly, whatever policy
is followed, every game is guaranteed to end in finite time, meaning that useful reward
information is obtained fairly frequently. Secondly, the state transitions are suciently
stochastic that independent of the policy, all states will occasionally be visited|a wrong
initial value function has little danger of starving us from visiting a critical part of state
space from which important information could be obtained.
The results (Table 2) of TD-Gammon are impressive. It has competed at the very top
level of international human play. Basic TD-Gammon played respectably, but not at a
professional standard.
271

fiFigure 11: Schaal and Atkeson's devil-sticking robot. The tapered stick is hit alternately
by each of the two hand sticks. The task is to keep the devil stick from falling
for as many hits as possible. The robot has three motors indicated by torque
vectors 1; 2; 3.
Although experiments with other games have in some cases produced interesting learning
behavior, no success close to that of TD-Gammon has been repeated. Other games that
have been studied include Go (Schraudolph, Dayan, & Sejnowski, 1994) and Chess (Thrun,
1995). It is still an open question as to if and how the success of TD-Gammon can be
repeated in other domains.

8.2 Robotics and Control
In recent years there have been many robotics and control applications that have used
reinforcement learning. Here we will concentrate on the following four examples, although
many other interesting ongoing robotics investigations are underway.
1. Schaal and Atkeson (1994) constructed a two-armed robot, shown in Figure 11, that
learns to juggle a device known as a devil-stick. This is a complex non-linear control
task involving a six-dimensional state space and less than 200 msecs per control decision. After about 40 initial attempts the robot learns to keep juggling for hundreds of
hits. A typical human learning the task requires an order of magnitude more practice
to achieve proficiency at mere tens of hits.
The juggling robot learned a world model from experience, which was generalized
to unvisited states by a function approximation scheme known as locally weighted
regression (Cleveland & Delvin, 1988; Moore & Atkeson, 1992). Between each trial,
a form of dynamic programming specific to linear control policies and locally linear
transitions was used to improve the policy. The form of dynamic programming is
known as linear-quadratic-regulator design (Sage & White, 1977).
272

fiReinforcement Learning: A Survey

2. Mahadevan and Connell (1991a) discuss a task in which a mobile robot pushes large
boxes for extended periods of time. Box-pushing is a well-known dicult robotics
problem, characterized by immense uncertainty in the results of actions. Q-learning
was used in conjunction with some novel clustering techniques designed to enable a
higher-dimensional input than a tabular approach would have permitted. The robot
learned to perform competitively with the performance of a human-programmed solution. Another aspect of this work, mentioned in Section 6.3, was a pre-programmed
breakdown of the monolithic task description into a set of lower level tasks to be
learned.
3. Mataric (1994) describes a robotics experiment with, from the viewpoint of theoretical reinforcement learning, an unthinkably high dimensional state space, containing
many dozens of degrees of freedom. Four mobile robots traveled within an enclosure collecting small disks and transporting them to a destination region. There were
three enhancements to the basic Q-learning algorithm. Firstly, pre-programmed signals called progress estimators were used to break the monolithic task into subtasks.
This was achieved in a robust manner in which the robots were not forced to use
the estimators, but had the freedom to profit from the inductive bias they provided.
Secondly, control was decentralized. Each robot learned its own policy independently
without explicit communication with the others. Thirdly, state space was brutally
quantized into a small number of discrete states according to values of a small number of pre-programmed boolean features of the underlying sensors. The performance
of the Q-learned policies were almost as good as a simple hand-crafted controller for
the job.
4. Q-learning has been used in an elevator dispatching task (Crites & Barto, 1996). The
problem, which has been implemented in simulation only at this stage, involved four
elevators servicing ten oors. The objective was to minimize the average squared
wait time for passengers, discounted into future time. The problem can be posed as a
discrete Markov system, but there are 1022 states even in the most simplified version of
the problem. Crites and Barto used neural networks for function approximation and
provided an excellent comparison study of their Q-learning approach against the most
popular and the most sophisticated elevator dispatching algorithms. The squared wait
time of their controller was approximately 7% less than the best alternative algorithm
(\Empty the System" heuristic with a receding horizon controller) and less than half
the squared wait time of the controller most frequently used in real elevator systems.
5. The final example concerns an application of reinforcement learning by one of the
authors of this survey to a packaging task from a food processing industry. The
problem involves filling containers with variable numbers of non-identical products.
The product characteristics also vary with time, but can be sensed. Depending on
the task, various constraints are placed on the container-filling procedure. Here are
three examples:

 The mean weight of all containers produced by a shift must not be below the
manufacturer's declared weight W .

273

fiKaelbling, Littman, & Moore

 The number of containers below the declared weight must be less than P %.
 No containers may be produced below weight W 0.
Such tasks are controlled by machinery which operates according to various setpoints.
Conventional practice is that setpoints are chosen by human operators, but this choice
is not easy as it is dependent on the current product characteristics and the current
task constraints. The dependency is often dicult to model and highly non-linear.
The task was posed as a finite-horizon Markov decision task in which the state of the
system is a function of the product characteristics, the amount of time remaining in
the production shift and the mean wastage and percent below declared in the shift
so far. The system was discretized into 200,000 discrete states and local weighted
regression was used to learn and generalize a transition model. Prioritized sweeping was used to maintain an optimal value function as each new piece of transition
information was obtained. In simulated experiments the savings were considerable,
typically with wastage reduced by a factor of ten. Since then the system has been
deployed successfully in several factories within the United States.
Some interesting aspects of practical reinforcement learning come to light from these
examples. The most striking is that in all cases, to make a real system work it proved
necessary to supplement the fundamental algorithm with extra pre-programmed knowledge.
Supplying extra knowledge comes at a price: more human effort and insight is required and
the system is subsequently less autonomous. But it is also clear that for tasks such as
these, a knowledge-free approach would not have achieved worthwhile performance within
the finite lifetime of the robots.
What forms did this pre-programmed knowledge take? It included an assumption of
linearity for the juggling robot's policy, a manual breaking up of the task into subtasks for
the two mobile-robot examples, while the box-pusher also used a clustering technique for
the Q values which assumed locally consistent Q values. The four disk-collecting robots
additionally used a manually discretized state space. The packaging example had far fewer
dimensions and so required correspondingly weaker assumptions, but there, too, the assumption of local piecewise continuity in the transition model enabled massive reductions
in the amount of learning data required.
The exploration strategies are interesting too. The juggler used careful statistical analysis to judge where to profitably experiment. However, both mobile robot applications
were able to learn well with greedy exploration|always exploiting without deliberate exploration. The packaging task used optimism in the face of uncertainty. None of these
strategies mirrors theoretically optimal (but computationally intractable) exploration, and
yet all proved adequate.
Finally, it is also worth considering the computational regimes of these experiments.
They were all very different, which indicates that the differing computational demands of
various reinforcement learning algorithms do indeed have an array of differing applications.
The juggler needed to make very fast decisions with low latency between each hit, but
had long periods (30 seconds and more) between each trial to consolidate the experiences
collected on the previous trial and to perform the more aggressive computation necessary
to produce a new reactive controller on the next trial. The box-pushing robot was meant to
274

fiReinforcement Learning: A Survey

operate autonomously for hours and so had to make decisions with a uniform length control
cycle. The cycle was suciently long for quite substantial computations beyond simple Qlearning backups. The four disk-collecting robots were particularly interesting. Each robot
had a short life of less than 20 minutes (due to battery constraints) meaning that substantial
number crunching was impractical, and any significant combinatorial search would have
used a significant fraction of the robot's learning lifetime. The packaging task had easy
constraints. One decision was needed every few minutes. This provided opportunities for
fully computing the optimal value function for the 200,000-state system between every
control cycle, in addition to performing massive cross-validation-based optimization of the
transition model being learned.
A great deal of further work is currently in progress on practical implementations of
reinforcement learning. The insights and task constraints that they produce will have an
important effect on shaping the kind of algorithms that are developed in future.

9. Conclusions

There are a variety of reinforcement-learning techniques that work effectively on a variety
of small problems. But very few of these techniques scale well to larger problems. This is
not because researchers have done a bad job of inventing learning techniques, but because
it is very dicult to solve arbitrary problems in the general case. In order to solve highly
complex problems, we must give up tabula rasa learning techniques and begin to incorporate
bias that will give leverage to the learning process.
The necessary bias can come in a variety of forms, including the following:
shaping: The technique of shaping is used in training animals (Hilgard & Bower, 1975); a
teacher presents very simple problems to solve first, then gradually exposes the learner
to more complex problems. Shaping has been used in supervised-learning systems,
and can be used to train hierarchical reinforcement-learning systems from the bottom
up (Lin, 1991), and to alleviate problems of delayed reinforcement by decreasing the
delay until the problem is well understood (Dorigo & Colombetti, 1994; Dorigo, 1995).
local reinforcement signals: Whenever possible, agents should be given reinforcement
signals that are local. In applications in which it is possible to compute a gradient,
rewarding the agent for taking steps up the gradient, rather than just for achieving
the final goal, can speed learning significantly (Mataric, 1994).
imitation: An agent can learn by \watching" another agent perform the task (Lin, 1991).
For real robots, this requires perceptual abilities that are not yet available. But
another strategy is to have a human supply appropriate motor commands to a robot
through a joystick or steering wheel (Pomerleau, 1993).
problem decomposition: Decomposing a huge learning problem into a collection of smaller
ones, and providing useful reinforcement signals for the subproblems is a very powerful technique for biasing learning. Most interesting examples of robotic reinforcement
learning employ this technique to some extent (Connell & Mahadevan, 1993).
reexes: One thing that keeps agents that know nothing from learning anything is that
they have a hard time even finding the interesting parts of the space; they wander
275

fiKaelbling, Littman, & Moore

around at random never getting near the goal, or they are always \killed" immediately.
These problems can be ameliorated by programming a set of \reexes" that cause the
agent to act initially in some way that is reasonable (Mataric, 1994; Singh, Barto,
Grupen, & Connolly, 1994). These reexes can eventually be overridden by more
detailed and accurate learned knowledge, but they at least keep the agent alive and
pointed in the right direction while it is trying to learn. Recent work by Millan (1996)
explores the use of reexes to make robot learning safer and more ecient.
With appropriate biases, supplied by human programmers or teachers, complex reinforcementlearning problems will eventually be solvable. There is still much work to be done and many
interesting questions remaining for learning techniques and especially regarding methods for
approximating, decomposing, and incorporating bias into problems.

Acknowledgements
Thanks to Marco Dorigo and three anonymous reviewers for comments that have helped
to improve this paper. Also thanks to our many colleagues in the reinforcement-learning
community who have done this work and explained it to us.
Leslie Pack Kaelbling was supported in part by NSF grants IRI-9453383 and IRI9312395. Michael Littman was supported in part by Bellcore. Andrew Moore was supported
in part by an NSF Research Initiation Award and by 3M Corporation.

References

Ackley, D. H., & Littman, M. L. (1990). Generalization and scaling in reinforcement learning. In Touretzky, D. S. (Ed.), Advances in Neural Information Processing Systems
2, pp. 550{557 San Mateo, CA. Morgan Kaufmann.
Albus, J. S. (1975). A new approach to manipulator control: Cerebellar model articulation
controller (cmac). Journal of Dynamic Systems, Measurement and Control, 97, 220{
227.
Albus, J. S. (1981). Brains, Behavior, and Robotics. BYTE Books, Subsidiary of McGrawHill, Peterborough, New Hampshire.
Anderson, C. W. (1986). Learning and Problem Solving with Multilayer Connectionist
Systems. Ph.D. thesis, University of Massachusetts, Amherst, MA.
Ashar, R. R. (1994). Hierarchical learning in stochastic domains. Master's thesis, Brown
University, Providence, Rhode Island.
Baird, L. (1995). Residual algorithms: Reinforcement learning with function approximation. In Prieditis, A., & Russell, S. (Eds.), Proceedings of the Twelfth International
Conference on Machine Learning, pp. 30{37 San Francisco, CA. Morgan Kaufmann.
Baird, L. C., & Klopf, A. H. (1993). Reinforcement learning with high-dimensional, continuous actions. Tech. rep. WL-TR-93-1147, Wright-Patterson Air Force Base Ohio:
Wright Laboratory.
276

fiReinforcement Learning: A Survey

Barto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning to act using real-time dynamic
programming. Artificial Intelligence, 72 (1), 81{138.
Barto, A. G., Sutton, R. S., & Anderson, C. W. (1983). Neuronlike adaptive elements that
can solve dicult learning control problems. IEEE Transactions on Systems, Man,
and Cybernetics, SMC-13 (5), 834{846.
Bellman, R. (1957). Dynamic Programming. Princeton University Press, Princeton, NJ.
Berenji, H. R. (1991). Artificial neural networks and approximate reasoning for intelligent
control in space. In American Control Conference, pp. 1075{1080.
Berry, D. A., & Fristedt, B. (1985). Bandit Problems: Sequential Allocation of Experiments.
Chapman and Hall, London, UK.
Bertsekas, D. P. (1987). Dynamic Programming: Deterministic and Stochastic Models.
Prentice-Hall, Englewood Cliffs, NJ.
Bertsekas, D. P. (1995). Dynamic Programming and Optimal Control. Athena Scientific,
Belmont, Massachusetts. Volumes 1 and 2.
Bertsekas, D. P., & Casta~non, D. A. (1989). Adaptive aggregation for infinite horizon
dynamic programming. IEEE Transactions on Automatic Control, 34 (6), 589{598.
Bertsekas, D. P., & Tsitsiklis, J. N. (1989). Parallel and Distributed Computation: Numerical Methods. Prentice-Hall, Englewood Cliffs, NJ.
Box, G. E. P., & Draper, N. R. (1987). Empirical Model-Building and Response Surfaces.
Wiley.
Boyan, J. A., & Moore, A. W. (1995). Generalization in reinforcement learning: Safely
approximating the value function. In Tesauro, G., Touretzky, D. S., & Leen, T. K.
(Eds.), Advances in Neural Information Processing Systems 7 Cambridge, MA. The
MIT Press.
Burghes, D., & Graham, A. (1980). Introduction to Control Theory including Optimal
Control. Ellis Horwood.
Cassandra, A. R., Kaelbling, L. P., & Littman, M. L. (1994). Acting optimally in partially
observable stochastic domains. In Proceedings of the Twelfth National Conference on
Artificial Intelligence Seattle, WA.
Chapman, D., & Kaelbling, L. P. (1991). Input generalization in delayed reinforcement
learning: An algorithm and performance comparisons. In Proceedings of the International Joint Conference on Artificial Intelligence Sydney, Australia.
Chrisman, L. (1992). Reinforcement learning with perceptual aliasing: The perceptual
distinctions approach. In Proceedings of the Tenth National Conference on Artificial
Intelligence, pp. 183{188 San Jose, CA. AAAI Press.
277

fiKaelbling, Littman, & Moore

Chrisman, L., & Littman, M. (1993). Hidden state and short-term memory.. Presentation
at Reinforcement Learning Workshop, Machine Learning Conference.
Cichosz, P., & Mulawka, J. J. (1995). Fast and ecient reinforcement learning with truncated temporal differences. In Prieditis, A., & Russell, S. (Eds.), Proceedings of the
Twelfth International Conference on Machine Learning, pp. 99{107 San Francisco,
CA. Morgan Kaufmann.
Cleveland, W. S., & Delvin, S. J. (1988). Locally weighted regression: An approach to
regression analysis by local fitting. Journal of the American Statistical Association,
83 (403), 596{610.
Cliff, D., & Ross, S. (1994). Adding temporary memory to ZCS. Adaptive Behavior, 3 (2),
101{150.
Condon, A. (1992). The complexity of stochastic games. Information and Computation,
96 (2), 203{224.
Connell, J., & Mahadevan, S. (1993). Rapid task learning for real robots. In Robot Learning.
Kluwer Academic Publishers.
Crites, R. H., & Barto, A. G. (1996). Improving elevator performance using reinforcement
learning. In Touretzky, D., Mozer, M., & Hasselmo, M. (Eds.), Neural Information
Processing Systems 8.
Dayan, P. (1992). The convergence of TD() for general . Machine Learning, 8 (3), 341{
362.
Dayan, P., & Hinton, G. E. (1993). Feudal reinforcement learning. In Hanson, S. J., Cowan,
J. D., & Giles, C. L. (Eds.), Advances in Neural Information Processing Systems 5
San Mateo, CA. Morgan Kaufmann.
Dayan, P., & Sejnowski, T. J. (1994). TD() converges with probability 1. Machine Learning, 14 (3).
Dean, T., Kaelbling, L. P., Kirman, J., & Nicholson, A. (1993). Planning with deadlines in
stochastic domains. In Proceedings of the Eleventh National Conference on Artificial
Intelligence Washington, DC.
D'Epenoux, F. (1963). A probabilistic production and inventory problem. Management
Science, 10, 98{108.
Derman, C. (1970). Finite State Markovian Decision Processes. Academic Press, New York.
Dorigo, M., & Bersini, H. (1994). A comparison of q-learning and classifier systems. In
From Animals to Animats: Proceedings of the Third International Conference on the
Simulation of Adaptive Behavior Brighton, UK.
Dorigo, M., & Colombetti, M. (1994). Robot shaping: Developing autonomous agents
through learning. Artificial Intelligence, 71 (2), 321{370.
278

fiReinforcement Learning: A Survey

Dorigo, M. (1995). Alecsys and the AutonoMouse: Learning to control a real robot by
distributed classifier systems. Machine Learning, 19.
Fiechter, C.-N. (1994). Ecient reinforcement learning. In Proceedings of the Seventh
Annual ACM Conference on Computational Learning Theory, pp. 88{97. Association
of Computing Machinery.
Gittins, J. C. (1989). Multi-armed Bandit Allocation Indices. Wiley-Interscience series in
systems and optimization. Wiley, Chichester, NY.
Goldberg, D. (1989). Genetic algorithms in search, optimization, and machine learning.
Addison-Wesley, MA.
Gordon, G. J. (1995). Stable function approximation in dynamic programming. In Prieditis, A., & Russell, S. (Eds.), Proceedings of the Twelfth International Conference on
Machine Learning, pp. 261{268 San Francisco, CA. Morgan Kaufmann.
Gullapalli, V. (1990). A stochastic reinforcement learning algorithm for learning real-valued
functions. Neural Networks, 3, 671{692.
Gullapalli, V. (1992). Reinforcement learning and its application to control. Ph.D. thesis,
University of Massachusetts, Amherst, MA.
Hilgard, E. R., & Bower, G. H. (1975). Theories of Learning (fourth edition). Prentice-Hall,
Englewood Cliffs, NJ.
Hoffman, A. J., & Karp, R. M. (1966). On nonterminating stochastic games. Management
Science, 12, 359{370.
Holland, J. H. (1975). Adaptation in Natural and Artificial Systems. University of Michigan
Press, Ann Arbor, MI.
Howard, R. A. (1960). Dynamic Programming and Markov Processes. The MIT Press,
Cambridge, MA.
Jaakkola, T., Jordan, M. I., & Singh, S. P. (1994). On the convergence of stochastic iterative
dynamic programming algorithms. Neural Computation, 6 (6).
Jaakkola, T., Singh, S. P., & Jordan, M. I. (1995). Monte-carlo reinforcement learning in
non-Markovian decision problems. In Tesauro, G., Touretzky, D. S., & Leen, T. K.
(Eds.), Advances in Neural Information Processing Systems 7 Cambridge, MA. The
MIT Press.
Kaelbling, L. P. (1993a). Hierarchical learning in stochastic domains: Preliminary results.
In Proceedings of the Tenth International Conference on Machine Learning Amherst,
MA. Morgan Kaufmann.
Kaelbling, L. P. (1993b). Learning in Embedded Systems. The MIT Press, Cambridge, MA.
Kaelbling, L. P. (1994a). Associative reinforcement learning: A generate and test algorithm.
Machine Learning, 15 (3).
279

fiKaelbling, Littman, & Moore

Kaelbling, L. P. (1994b). Associative reinforcement learning: Functions in k-DNF. Machine
Learning, 15 (3).
Kirman, J. (1994). Predicting Real-Time Planner Performance by Domain Characterization.
Ph.D. thesis, Department of Computer Science, Brown University.
Koenig, S., & Simmons, R. G. (1993). Complexity analysis of real-time reinforcement
learning. In Proceedings of the Eleventh National Conference on Artificial Intelligence,
pp. 99{105 Menlo Park, California. AAAI Press/MIT Press.
Kumar, P. R., & Varaiya, P. P. (1986). Stochastic Systems: Estimation, Identification, and
Adaptive Control. Prentice Hall, Englewood Cliffs, New Jersey.
Lee, C. C. (1991). A self learning rule-based controller employing approximate reasoning
and neural net concepts. International Journal of Intelligent Systems, 6 (1), 71{93.
Lin, L.-J. (1991). Programming robots using reinforcement learning and teaching. In
Proceedings of the Ninth National Conference on Artificial Intelligence.
Lin, L.-J. (1993a). Hierachical learning of robot skills by reinforcement. In Proceedings of
the International Conference on Neural Networks.
Lin, L.-J. (1993b). Reinforcement Learning for Robots Using Neural Networks. Ph.D. thesis,
Carnegie Mellon University, Pittsburgh, PA.
Lin, L.-J., & Mitchell, T. M. (1992). Memory approaches to reinforcement learning in nonMarkovian domains. Tech. rep. CMU-CS-92-138, Carnegie Mellon University, School
of Computer Science.
Littman, M. L. (1994a). Markov games as a framework for multi-agent reinforcement learning. In Proceedings of the Eleventh International Conference on Machine Learning,
pp. 157{163 San Francisco, CA. Morgan Kaufmann.
Littman, M. L. (1994b). Memoryless policies: Theoretical limitations and practical results.
In Cliff, D., Husbands, P., Meyer, J.-A., & Wilson, S. W. (Eds.), From Animals
to Animats 3: Proceedings of the Third International Conference on Simulation of
Adaptive Behavior Cambridge, MA. The MIT Press.
Littman, M. L., Cassandra, A., & Kaelbling, L. P. (1995a). Learning policies for partially
observable environments: Scaling up. In Prieditis, A., & Russell, S. (Eds.), Proceedings of the Twelfth International Conference on Machine Learning, pp. 362{370 San
Francisco, CA. Morgan Kaufmann.
Littman, M. L., Dean, T. L., & Kaelbling, L. P. (1995b). On the complexity of solving
Markov decision problems. In Proceedings of the Eleventh Annual Conference on
Uncertainty in Artificial Intelligence (UAI{95) Montreal, Quebec, Canada.
Lovejoy, W. S. (1991). A survey of algorithmic methods for partially observable Markov
decision processes. Annals of Operations Research, 28, 47{66.
280

fiReinforcement Learning: A Survey

Maes, P., & Brooks, R. A. (1990). Learning to coordinate behaviors. In Proceedings Eighth
National Conference on Artificial Intelligence, pp. 796{802. Morgan Kaufmann.
Mahadevan, S. (1994). To discount or not to discount in reinforcement learning: A case
study comparing R learning and Q learning. In Proceedings of the Eleventh International Conference on Machine Learning, pp. 164{172 San Francisco, CA. Morgan
Kaufmann.
Mahadevan, S. (1996). Average reward reinforcement learning: Foundations, algorithms,
and empirical results. Machine Learning, 22 (1).
Mahadevan, S., & Connell, J. (1991a). Automatic programming of behavior-based robots
using reinforcement learning. In Proceedings of the Ninth National Conference on
Artificial Intelligence Anaheim, CA.
Mahadevan, S., & Connell, J. (1991b). Scaling reinforcement learning to robotics by exploiting the subsumption architecture. In Proceedings of the Eighth International
Workshop on Machine Learning, pp. 328{332.
Mataric, M. J. (1994). Reward functions for accelerated learning. In Cohen, W. W., &
Hirsh, H. (Eds.), Proceedings of the Eleventh International Conference on Machine
Learning. Morgan Kaufmann.
McCallum, A. K. (1995). Reinforcement Learning with Selective Perception and Hidden
State. Ph.D. thesis, Department of Computer Science, University of Rochester.
McCallum, R. A. (1993). Overcoming incomplete perception with utile distinction memory.
In Proceedings of the Tenth International Conference on Machine Learning, pp. 190{
196 Amherst, Massachusetts. Morgan Kaufmann.
McCallum, R. A. (1995). Instance-based utile distinctions for reinforcement learning with
hidden state. In Proceedings of the Twelfth International Conference Machine Learning, pp. 387{395 San Francisco, CA. Morgan Kaufmann.
Meeden, L., McGraw, G., & Blank, D. (1993). Emergent control and planning in an autonomous vehicle. In Touretsky, D. (Ed.), Proceedings of the Fifteenth Annual Meeting
of the Cognitive Science Society, pp. 735{740. Lawerence Erlbaum Associates, Hillsdale, NJ.
Millan, J. d. R. (1996). Rapid, safe, and incremental learning of navigation strategies. IEEE
Transactions on Systems, Man, and Cybernetics, 26 (3).
Monahan, G. E. (1982). A survey of partially observable Markov decision processes: Theory,
models, and algorithms. Management Science, 28, 1{16.
Moore, A. W. (1991). Variable resolution dynamic programming: Eciently learning action maps in multivariate real-valued spaces. In Proc. Eighth International Machine
Learning Workshop.
281

fiKaelbling, Littman, & Moore

Moore, A. W. (1994). The parti-game algorithm for variable resolution reinforcement learning in multidimensional state-spaces. In Cowan, J. D., Tesauro, G., & Alspector, J.
(Eds.), Advances in Neural Information Processing Systems 6, pp. 711{718 San Mateo,
CA. Morgan Kaufmann.
Moore, A. W., & Atkeson, C. G. (1992). An investigation of memory-based function approximators for learning control. Tech. rep., MIT Artifical Intelligence Laboratory,
Cambridge, MA.
Moore, A. W., & Atkeson, C. G. (1993). Prioritized sweeping: Reinforcement learning with
less data and less real time. Machine Learning, 13.
Moore, A. W., Atkeson, C. G., & Schaal, S. (1995). Memory-based learning for control.
Tech. rep. CMU-RI-TR-95-18, CMU Robotics Institute.
Narendra, K., & Thathachar, M. A. L. (1989). Learning Automata: An Introduction.
Prentice-Hall, Englewood Cliffs, NJ.
Narendra, K. S., & Thathachar, M. A. L. (1974). Learning automata|a survey. IEEE
Transactions on Systems, Man, and Cybernetics, 4 (4), 323{334.
Peng, J., & Williams, R. J. (1993). Ecient learning and planning within the Dyna framework. Adaptive Behavior, 1 (4), 437{454.
Peng, J., & Williams, R. J. (1994). Incremental multi-step Q-learning. In Proceedings of the
Eleventh International Conference on Machine Learning, pp. 226{232 San Francisco,
CA. Morgan Kaufmann.
Pomerleau, D. A. (1993). Neural network perception for mobile robot guidance. Kluwer
Academic Publishing.
Puterman, M. L. (1994). Markov Decision Processes|Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., New York, NY.
Puterman, M. L., & Shin, M. C. (1978). Modified policy iteration algorithms for discounted
Markov decision processes. Management Science, 24, 1127{1137.
Ring, M. B. (1994). Continual Learning in Reinforcement Environments. Ph.D. thesis,
University of Texas at Austin, Austin, Texas.
Rude, U. (1993). Mathematical and computational techniques for multilevel adaptive methods. Society for Industrial and Applied Mathematics, Philadelphia, Pennsylvania.
Rumelhart, D. E., & McClelland, J. L. (Eds.). (1986). Parallel Distributed Processing:
Explorations in the microstructures of cognition. Volume 1: Foundations. The MIT
Press, Cambridge, MA.
Rummery, G. A., & Niranjan, M. (1994). On-line Q-learning using connectionist systems.
Tech. rep. CUED/F-INFENG/TR166, Cambridge University.
282

fiReinforcement Learning: A Survey

Rust, J. (1996). Numerical dynamic programming in economics. In Handbook of Computational Economics. Elsevier, North Holland.
Sage, A. P., & White, C. C. (1977). Optimum Systems Control. Prentice Hall.
Salganicoff, M., & Ungar, L. H. (1995). Active exploration and learning in real-valued
spaces using multi-armed bandit allocation indices. In Prieditis, A., & Russell, S.
(Eds.), Proceedings of the Twelfth International Conference on Machine Learning,
pp. 480{487 San Francisco, CA. Morgan Kaufmann.
Samuel, A. L. (1959). Some studies in machine learning using the game of checkers. IBM
Journal of Research and Development, 3, 211{229. Reprinted in E. A. Feigenbaum
and J. Feldman, editors, Computers and Thought, McGraw-Hill, New York 1963.
Schaal, S., & Atkeson, C. (1994). Robot juggling: An implementation of memory-based
learning. Control Systems Magazine, 14.
Schmidhuber, J. (1996). A general method for multi-agent learning and incremental selfimprovement in unrestricted environments. In Yao, X. (Ed.), Evolutionary Computation: Theory and Applications. Scientific Publ. Co., Singapore.
Schmidhuber, J. H. (1991a). Curious model-building control systems. In Proc. International
Joint Conference on Neural Networks, Singapore, Vol. 2, pp. 1458{1463. IEEE.
Schmidhuber, J. H. (1991b). Reinforcement learning in Markovian and non-Markovian
environments. In Lippman, D. S., Moody, J. E., & Touretzky, D. S. (Eds.), Advances
in Neural Information Processing Systems 3, pp. 500{506 San Mateo, CA. Morgan
Kaufmann.
Schraudolph, N. N., Dayan, P., & Sejnowski, T. J. (1994). Temporal difference learning of
position evaluation in the game of Go. In Cowan, J. D., Tesauro, G., & Alspector,
J. (Eds.), Advances in Neural Information Processing Systems 6, pp. 817{824 San
Mateo, CA. Morgan Kaufmann.
Schrijver, A. (1986). Theory of Linear and Integer Programming. Wiley-Interscience, New
York, NY.
Schwartz, A. (1993). A reinforcement learning method for maximizing undiscounted rewards. In Proceedings of the Tenth International Conference on Machine Learning,
pp. 298{305 Amherst, Massachusetts. Morgan Kaufmann.
Singh, S. P., Barto, A. G., Grupen, R., & Connolly, C. (1994). Robust reinforcement
learning in motion planning. In Cowan, J. D., Tesauro, G., & Alspector, J. (Eds.),
Advances in Neural Information Processing Systems 6, pp. 655{662 San Mateo, CA.
Morgan Kaufmann.
Singh, S. P., & Sutton, R. S. (1996). Reinforcement learning with replacing eligibility traces.
Machine Learning, 22 (1).
283

fiKaelbling, Littman, & Moore

Singh, S. P. (1992a). Reinforcement learning with a hierarchy of abstract models. In
Proceedings of the Tenth National Conference on Artificial Intelligence, pp. 202{207
San Jose, CA. AAAI Press.
Singh, S. P. (1992b). Transfer of learning by composing solutions of elemental sequential
tasks. Machine Learning, 8 (3), 323{340.
Singh, S. P. (1993). Learning to Solve Markovian Decision Processes. Ph.D. thesis, Department of Computer Science, University of Massachusetts. Also, CMPSCI Technical
Report 93-77.
Stengel, R. F. (1986). Stochastic Optimal Control. John Wiley and Sons.
Sutton, R. S. (1996). Generalization in Reinforcement Learning: Successful Examples Using
Sparse Coarse Coding. In Touretzky, D., Mozer, M., & Hasselmo, M. (Eds.), Neural
Information Processing Systems 8.
Sutton, R. S. (1984). Temporal Credit Assignment in Reinforcement Learning. Ph.D. thesis,
University of Massachusetts, Amherst, MA.
Sutton, R. S. (1988). Learning to predict by the method of temporal differences. Machine
Learning, 3 (1), 9{44.
Sutton, R. S. (1990). Integrated architectures for learning, planning, and reacting based
on approximating dynamic programming. In Proceedings of the Seventh International
Conference on Machine Learning Austin, TX. Morgan Kaufmann.
Sutton, R. S. (1991). Planning by incremental dynamic programming. In Proceedings
of the Eighth International Workshop on Machine Learning, pp. 353{357. Morgan
Kaufmann.
Tesauro, G. (1992). Practical issues in temporal difference learning. Machine Learning, 8,
257{277.
Tesauro, G. (1994). TD-Gammon, a self-teaching backgammon program, achieves masterlevel play. Neural Computation, 6 (2), 215{219.
Tesauro, G. (1995). Temporal difference learning and TD-Gammon. Communications of
the ACM, 38 (3), 58{67.
Tham, C.-K., & Prager, R. W. (1994). A modular q-learning architecture for manipulator task decomposition. In Proceedings of the Eleventh International Conference on
Machine Learning San Francisco, CA. Morgan Kaufmann.
Thrun, S. (1995). Learning to play the game of chess. In Tesauro, G., Touretzky, D. S., &
Leen, T. K. (Eds.), Advances in Neural Information Processing Systems 7 Cambridge,
MA. The MIT Press.
284

fiReinforcement Learning: A Survey

Thrun, S., & Schwartz, A. (1993). Issues in using function approximation for reinforcement
learning. In Mozer, M., Smolensky, P., Touretzky, D., Elman, J., & Weigend, A.
(Eds.), Proceedings of the 1993 Connectionist Models Summer School Hillsdale, NJ.
Lawrence Erlbaum.
Thrun, S. B. (1992). The role of exploration in learning control. In White, D. A., &
Sofge, D. A. (Eds.), Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive
Approaches. Van Nostrand Reinhold, New York, NY.
Tsitsiklis, J. N. (1994). Asynchronous stochastic approximation and Q-learning. Machine
Learning, 16 (3).
Tsitsiklis, J. N., & Van Roy, B. (1996). Feature-based methods for large scale dynamic
programming. Machine Learning, 22 (1).
Valiant, L. G. (1984). A theory of the learnable. Communications of the ACM, 27 (11),
1134{1142.
Watkins, C. J. C. H. (1989). Learning from Delayed Rewards. Ph.D. thesis, King's College,
Cambridge, UK.
Watkins, C. J. C. H., & Dayan, P. (1992). Q-learning. Machine Learning, 8 (3), 279{292.
Whitehead, S. D. (1991). Complexity and cooperation in Q-learning. In Proceedings of the
Eighth International Workshop on Machine Learning Evanston, IL. Morgan Kaufmann.
Williams, R. J. (1987). A class of gradient-estimating algorithms for reinforcement learning
in neural networks. In Proceedings of the IEEE First International Conference on
Neural Networks San Diego, CA.
Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist
reinforcement learning. Machine Learning, 8 (3), 229{256.
Williams, R. J., & Baird, III, L. C. (1993a). Analysis of some incremental variants of policy
iteration: First steps toward understanding actor-critic learning systems. Tech. rep.
NU-CCS-93-11, Northeastern University, College of Computer Science, Boston, MA.
Williams, R. J., & Baird, III, L. C. (1993b). Tight performance bounds on greedy policies
based on imperfect value functions. Tech. rep. NU-CCS-93-14, Northeastern University, College of Computer Science, Boston, MA.
Wilson, S. (1995). Classifier fitness based on accuracy. Evolutionary Computation, 3 (2),
147{173.
Zhang, W., & Dietterich, T. G. (1995). A reinforcement learning approach to job-shop
scheduling. In Proceedings of the International Joint Conference on Artificial Intellience.
285

fiJournal of Artificial Intelligence Research 4 (1996) 397-417

Submitted 12/95; published 6/96

Further Experimental Evidence against the Utility of
Occam's Razor
Geoffrey I. Webb

webb@deakin.edu.au

School of Computing and Mathematics
Deakin University
Geelong, Vic, 3217, Australia.

Abstract

This paper presents new experimental evidence against the utility of Occam's razor.
A systematic procedure is presented for post-processing decision trees produced by C4.5.
This procedure was derived by rejecting Occam's razor and instead attending to the assumption that similar objects are likely to belong to the same class. It increases a decision
tree's complexity without altering the performance of that tree on the training data from
which it is inferred. The resulting more complex decision trees are demonstrated to have,
on average, for a variety of common learning tasks, higher predictive accuracy than the less
complex original decision trees. This result raises considerable doubt about the utility of
Occam's razor as it is commonly applied in modern machine learning.

1. Introduction

In the fourteenth century William of Occam stated \plurality should not be assumed without necessity". This principle has since become known as Occam's razor. Occam's razor
was originally intended as a basis for determining one's ontology. However, in modern times
it has been widely reinterpreted and adopted as an epistemological principle|a means of
selecting between alternative theories as well as ontologies. Modern reinterpretations of
Occam's razor are widely employed in classification learning. However, the utility of this
principle has been subject to widespread theoretical and experimental attack. This paper
adds to this debate by providing further experimental evidence against the utility of the
modern interpretation of Occam's razor. This evidence takes the form of a systematic procedure for adding non-redundant complexity to classifiers in a manner that is demonstrated
to frequently improve predictive accuracy.
The modern interpretation of Occam's razor has been characterized as \of two hypotheses H and H0 , both of which explain E, the simpler is to be preferred" (Good, 1977).
However, this does not specify what aspect of a theory should be measured for simplicity.
Syntactic, semantic, epistemological and pragmatic simplicity are all alternative criteria that
can and have been employed Bunge (1963). In practice, the common use of Occam's razor
in machine learning seeks to minimize surface syntactic complexity. It is this interpretation
that this paper addresses.
It is to be assumed that Occam's razor is usually applied in the expectation that its
application will, in general, lead to some particular form of advantage. There is no widely
accepted articulation of precisely how Occam's razor should be applied or what advantages
are to be expected from its application in classification learning. However, the literature
does contain two statements that seem to capture at least one widely adopted approach to

c 1996 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiWebb

the principle. Blumer, Ehrenfeucht, Haussler, and Warmuth (1987) suggest that to wield
Occam's razor is to adopt the goal of discovering \the simplest hypothesis that is consistent
with the sample data" with the expectation that the simplest hypothesis will \perform well
on further observations taken from the same source". Quinlan (1986) states
\Given a choice between two decision trees, each of which is correct over the
training set, it seems sensible to prefer the simpler one on the grounds that it is
more likely to capture structure inherent in the problem. The simpler tree would
therefore be expected to classify correctly more objects outside the training set."
While these statements would not necessarily be accepted by all proponents of Occam's
razor, they capture the form of Occam's razor that this paper seeks to address|a learning
bias toward classifiers that minimize surface syntactic complexity in the expectation of
maximizing predictive accuracy.
Both of the above statements of Occam's razor restrict themselves to classifiers that
correctly classify all objects in a training set. Many modern machine learning systems
incorporate learning biases that tolerate small levels of misclassification of the training data
(Clark & Niblett, 1989; Michalski, 1984; Quinlan, 1986, 1990, for example). In this context,
and extending the scope of the definition beyond decision trees to classifiers in general, it
seems reasonable to modify Quinlan's (1986) statement (above) to
Given a choice between two plausible classifiers that perform identically on the
training set, the simpler classifier is expected to classify correctly more objects
outside the training set.
This will be referred to as the Occam thesis.
The concept of identical performance on a training set could be defined in many different
ways. It might be tempting to opt for a definition that requires identical error rates when
two classifiers are applied to the training set. A less strict interpretation might allow two
classifiers to have differing error rates so long as the difference is within some statistical
confidence limit. However, to maximize the applicability of its results, this paper will adopt
a very strict interpretation of identical performance|that for every object o in the training
set, both classifiers provide the same classification for o.
It should be noted that the Occam thesis is not claiming that for any two classifiers
with equal empirical support the least complex will always have greater predictive accuracy
on previously unseen objects. Rather, it is claiming that more frequently than not the less
complex will have higher predictive accuracy.
This paper first examines some arguments for and against the Occam thesis. It then
presents new empirical evidence against the thesis. This evidence was acquired by using a
learning algorithm that post-processes decision trees learnt by C4.5. This post-processor
was developed by rejecting the Occam thesis and instead attending to the assumption
that similarity is predictive of class. The post-processor systematically adds complexity to
decision trees without altering their performance on the training data. This is demonstrated
to lead to an increase in predictive accuracy on previously unseen objects for a range of
`real-world' learning tasks. This evidence is taken as incompatible with the Occam thesis.
398

fiFurther Experimental Evidence against the Utility of Occam's Razor

2. Previous Theoretical and Experimental Work
To provide a context for the new evidence against the Occam thesis, it is worth briey
examining previous relevant theoretical and experimental work. Where relevant, an outline
will be provided of reasons why each contribution may have failed to persuade the other
side of the debate.

2.1 The Law of Conservation of Generalization Performance

The conservation law of generalization performance (Schaffer, 1994) proves that no learning
bias can outperform any other bias over the space of all possible learning tasks1. It follows
that if Occam's razor is a valuable learning bias, it can only be so for some subset of all
possible learning tasks. It might be argued that the set of `real-world' learning tasks is such
a subset.
This paper is predicated on accepting the proposition that the set of `real-world' learning
tasks is distinguished from the set of all possible learning tasks in respects that render the
conservation law inapplicable. Rao, Gordon, and Spears (1995) argue that this is the case
because learning tasks in our universe are not uniformly distributed across the space of all
possible learning tasks.
But why should this be so? One argument in support of this proposition is as follows.
`Real-world' learning tasks are defined by people for use with machine learning systems. To
this end, the task constructors will have sought to ensure that the independent variables
(class attributes) are related to the dependent variables (other attributes) in ways that can
be captured within the space of classifiers that are made available for the learning system.
Actual machine learning tasks are not drawn randomly from the space of all possible learning
tasks. The human involvement in the formulation of the problems ensures this.
As a simple thought experiment in support of this proposition, consider a learning task
for which the class attribute is generated by a random number generator and in no way
relates to the other attributes. The majority of machine learning researchers would not be in
the slightest disconcerted if their systems failed to perform well when trained on such data.
As a further example, consider a learning task for which the class attribute is a simple count
of the number of missing attribute values for an object. Assume that such a learning task
was submitted to a system, such as C4.5 (Quinlan, 1993), that develops classifiers that have
no mechanism for testing during classification whether an attribute value is missing. Again,
the majority of machine learning researchers would be unconcerned that their systems failed
to perform well in such circumstances. Machine learning is simply unsuited to such tasks.
A knowledgeable user would not apply machine learning to such data, at least not in the
expectation of obtaining a useful classifier therefrom.
This paper explores the applicability of the Occam thesis to `real-world' learning tasks.

2.2 Other Theoretical Objections to the Occam Thesis

Most machine learning systems explicitly or implicitly employ Occam's razor. In addition
to its almost universal use in machine learning, the principle of Occam's razor is widely
1. The law is only proved for discrete valued learning tasks, but there is no reason to believe it does not
also apply to continuous valued tasks

399

fiWebb

accepted in general scientific practice. That this has persisted, despite Occam's razor being
subjected to extensive philosophical, theoretical and empirical attack, suggests that these
attacks have not been found persuasive.
On the philosophical front, to summarize Bunge (1963), the complexity of a theory
(classifier) depends entirely upon the language in which it is encoded. To claim that the
acceptability of a theory depends upon the language in which it happens to be expressed
appears indefensible. Further, there is no obvious theoretical relationship between syntactic complexity and the quality of a theory, other than the possibility that the world is
intrinsically simple and that the use of Occam's razor enables the discovery of that intrinsic
simplicity. However, even if the world is intrinsically simple, there is no reason why that
simplicity should correspond to syntactic simplicity in an arbitrary language.
To merely state that a less complex explanation is preferable does not specify by what
criterion it is preferable. The implicit assumption underlying much machine learning research appears to be that, all other things being equal, less complex classifiers will be, in
general, more accurate (Blumer et al., 1987; Quinlan, 1986). It is this Occam thesis that
this paper seeks to discredit.
On a straight-forward interpretation, for a syntactic measure to be used to predict
expected accuracy appears absurd. If two classifiers have identical meaning (such as IF
20AGE40 THEN POS and IF 20AGE30 OR 30AGE40 THEN POS) then it is
not possible for their accuracies to differ, no matter how greatly their complexities differ.
This simple example highlights the apparent dominance of semantics over syntax in the
determination of predictive accuracy.

2.3 Previous Experimental Evidence Against the Occam Thesis
On the empirical front, a number of recent experimental results have appeared to conict
with the Occam thesis. Murphy and Pazzani (1994) demonstrated that for a number of artificial classification learning tasks, the simplest consistent decision trees had lower predictive
accuracy than slightly more complex consistent trees. Further experimentation, however,
showed that these results were dependent upon the complexity of the target concept. A
bias toward simplicity performed well when the target concept was best described by a
simple classifier and a bias toward complexity performed well when the target concept was
best described by a complex classifier (Murphy, 1995). In addition, the simplest classifiers
obtained better than average (over all consistent classifiers) predictive accuracy when the
data was augmented with irrelevant attributes or attributes strongly correlated to the target
concept, but not required for classification.
Webb (1994) presented results that suggest that for a wide range of learning tasks from
the UCI repository of learning tasks (Murphy & Aha, 1993), the relative generality of
the classifiers is a better predictor of classification performance than is the relative surface
syntactic complexity. However, it could be argued that while these results demonstrate that
a strategy of selecting the simplest between any pair of theories will not lead to maximization
of predictive accuracy, they do not demonstrate that selecting the simplest of all available
theories would fail to maximize predictive accuracy.
Schaffer (1992, 1993) has shown that pruning techniques that reduce complexity while
decreasing resubstitution accuracy sometimes increase predictive accuracy and sometimes
400

fiFurther Experimental Evidence against the Utility of Occam's Razor

decrease predictive accuracy of inferred decision trees. However, a proponent of the Occam
thesis could explain these results in terms of a positive effect from the application of Occam's
razor (the reduction of complexity) being counter-balanced by a negative effect from a
reduction of empirical support (resubstitution accuracy).
Holte, Acker, and Porter (1989) have shown that specializing small disjuncts (rules
with low empirical support) to exclude areas of the instance space occupied by no training
objects frequently decreases the error rate of unseen objects covered by those disjuncts. As
this specialization involves increasing complexity, this might be viewed as contrary to the
Occam thesis. However, the same research shows that the total error rates for the classifiers
in which the disjuncts are embedded increases when those disjuncts are specialized. A
proponent of the Occam thesis could thus dismiss the relevance of the former results by
arguing that the thesis only applies to complete classifiers and not to elements of those
classifiers.

2.4 Theoretical and Experimental Support for the Occam Thesis
Against these theoretical and experimental objections to the Occam thesis there exists a
body of apparent theoretical and empirical support.
Several attempts have been made to provide theoretical support for the Occam thesis
in the machine learning context (Blumer et al., 1987; Pearl, 1978; Fayyad & Irani, 1990).
However, these proofs apply equally to any systematic learning bias that favors a small
subset of the hypothesis space. Indeed, it has been argued that they equally support a
preference for classifiers with high complexity (Schaffer, 1993; Berkman & Sandholm, 1995).
Holte (1993) compared learning very simple classification rules with the use of a sophisticated learner of complex decision trees. He found that, for a number of tasks from the UCI
repository of machine learning datasets (Murphy & Aha, 1993), the simple rules achieved
accuracies of within a few percentage points of the complex trees. This could be considered
as supportive of the Occam thesis. However, in no case did the simple rules outperform the
more complex decision trees. Nor was it demonstrated that there did not exist yet another
learning bias that consistently outperformed both those studied.
A final argument that might be considered to support the Occam thesis is that the
majority of machine learning systems employ some form of Occam's razor and they appear to perform well in practice. However, it has not been demonstrated that even better
performance would not be obtained if Occam's razor were abandoned.

3. New Experimental Evidence Against the Occam Thesis
The theoretical and experimental objections to the Occam thesis do not appear to have
greatly diminished the machine learning community's use of Occam's razor. This paper
seeks to support objections to the Occam thesis with robust and general experimental
counter-evidence. To this end it presents a systematic procedure for increasing the complexity of inferred decision trees without modifying their performance on the training data.
This procedure takes the form of a post-processor for decision trees produced by C4.5 (Quinlan, 1993). The application of this procedure to a range of learning tasks from the UCI
repository of learning tasks (Murphy & Aha, 1993) is demonstrated to result, on average,
401

fiWebb

in increased predictive accuracy when the inferred decision trees are applied to previously
unseen data.

3.1 Theoretical Basis for the Decision Tree Post-processor
The similarity assumption is a common assumption in machine learning|that objects that
are similar have high probability of belonging to the same class (Rendell & Seshu, 1990).
The techniques to be described rely upon this assumption for their theoretical justification
rather than upon the Occam thesis.
Starting from the similarity assumption, machine learning can be viewed as the inference
of a suitable similarity metric for a learning task. A decision tree can be viewed as a
partitioning of the instance space. Each partition, represented by a leaf, contains the
objects that are similar in relevant respects and thus are expected to belong to the same
class.
This raises the issue of how similarity should be measured. Instance-based learning methods (Aha, Kibler, & Albert, 1991) tend to map the instance space onto an ndimensional geometric space and then employ geometric distance measures within that
space to measure similarity. Such an approach is problematic on a number of grounds.
First, it assumes that the underlying metrics of different attributes are commensurable.
How is it possible to determine a priori whether a difference of five years in age signifies
a greater or lesser difference in similarity than a difference of one inch in height? Second,
it assumes that it is possible to provide a priori definitions of similarity with respect to a
single attribute. Can one really make a universal prescription that a value of 16 is always
more similar to a value of 2 than to a value of 64? Why should it never be the case that the
relevant similarity metric is based on the log2 of the surface value, in which case 16 would
be more similar to 64 than to 2?
If we wish to employ induction to learn classifiers expressed in a particular language then
it would appear that we are forced to assume that the language in question in some manner
captures a relevant aspect of similarity. Any potential leaf of a decision tree presents a
plausible similarity metric (all objects that fall within that leaf are similar in some respect).
Empirical evaluation (the performance of that leaf on the training set) can then be used to
infer the relevance of that similarity metric to the induction task at hand. If a leaf l covers
a large number of objects of class c and few of other classes, then this provides evidence
that similarity with respect to the tests that define l is predictive of c.
Figure 1 illustrates a simple instance space and the partition that C4.5 (Quinlan, 1993)
imposes thereon. Note that C4.5 forms nodes for continuous attributes, such as A and B ,
that consist of a test on a cut value x. This test takes the form a  x. With respect to
Figure 1 there is one such cut, on value 5 for attribute A.
C4.5 infers that the relevant similarity metric relates to attribute A only. The partition
(shown by a dashed line) is placed at value 5 for attribute A. However, if one does not
accept the Occam thesis, but does accept the similarity assumption, there is no reason to
believe that the area of the instance space for which B > 5 and A  5 (lightly shaded in
Figure 1) should belong to class + (as determined by C4.5) rather than class {.
C4.5 uses the Occam thesis to justify the termination of partitioning of the instance
space as soon as the decision tree accounts adequately for the training set. In consequence,
402

fiFurther Experimental Evidence against the Utility of Occam's Razor

..........
.......... { {
..........
.......... { {
.......... {
+
{
+
{
+
1 2 3 4 5 6 7 8 9 10
A
Figure 1: A simple instance space
10
9
8
7
6
B5
4
3
2
1

large areas of the instance space that are occupied by no objects in the training set may
be left within partitions for which the similarity assumption provides little support. For
example, with respect to Figure 1, it could be argued that a more relevant similarity metric
with respect to the region A  5 and B > 5 is similarity with respect to B . Within the
entire instance space, all objects with values of B > 5 belong to class {. There are five such
objects. In contrast, there are only three objects with values of A  5 that provide the
evidence that objects in this area of the instance space belong to class +. Each of these
tests represents a plausible similarity metric on the basis of the available evidence. Thus,
an object within this region will be similar in a plausible respect to three positive and five
negative objects. If objects that are similar in relevant respects have high probability of
belonging to the same class, and the only other information available is that it is plausible
that an object is similar to three positive and five negative objects, then it would appear
more probable that the object is negative than positive.
The disagreement between C4.5 and the similarity assumption in this case contrasts
with, for example, the area of the instance space for which A  5 and B < 1. In this region,
the similarity assumption suggests that C4.5's partition is appropriate because all plausible
similarity metrics will indicate that an object in this region is similar to positive objects
only2 .
The post-processor developed for this research analyses decision trees produced by C4.5
in order to identify such regions|those occupied by no objects from the training set but
for which there is evidence (in terms of the similarity assumption) favoring relabeling with
2. To provide an example of an implausible similarity metric, consider the similarity metric defined by
the root node, that everything is similar. This will not be plausible as there is too great a level of
dissimilarity in classes with respect to this metric. If it were a relevant similarity metric, and the
distribution of training examples was representative of the distribution of objects in the domain as a
whole, then the similarity assumption would be violated, as similar objects would have probability of just
0.58 of belonging to the same class. This probability can be calculated as follows. The probabilities of
an object being + or { are 0.3 and 0.7 respectively. If an object is + then the probability of it belonging
to the same class as another object to which it is similar is 0.3. If an object is { then the probability of
it belonging to the same class as another object to which it is similar is 0.7. Thus, the probability of an
object belonging to the same class as another similar object is 0:3  0:3 + 0:7  0:7 = 0:58. The numbers
involved in this simple example are, of course, too small to reach any such conclusion with a high level
of confidence|the example is intended as illustrative only.

403

fiWebb

a different class to that assigned by C4.5. When such regions are identified, new branches
are added to the decision tree, creating new partitionings of the instance space. Both trees
must provide identical performance with respect to the training set as only regions of the
instance space that are occupied by no objects in the training set are affected.
It is dicult to see how any plausible metric for complexity could interpret the addition
of such branches as not increasing the complexity of the tree.
The end result is that the post-processor adds complexity to the decision tree without
altering how the tree applies to the training data. The Occam thesis predicts that this will,
in general, lower predictive accuracy while the similarity assumption predicts that it will,
in general, increase predictive accuracy. As will be seen, the latter prediction is consistent
with experimental evidence and the former is not.

3.2 The Post-processor

While the above process could be applied to both continuous and discrete attributes, the
current implementation addresses only continuous attributes.
The post-processor operates by examining each leaf l of the tree in turn. For each l,
each attribute a is considered in turn. For each a, all possible thresholds below and above
the region of the instance space occupied by objects at l are explored. First, the minimum
(min) and maximum (max) are determined for values of a that are possible for objects
that can reach l. If l lies below the  branch of a split on a then the threshold for that
split provides an upper limit (max) on values for a at l. If it lies below a > branch, the
threshold provides a lower limit (min). Where the node does not lie below a  branch,
max = 1. Where the node does not lie below a > branch, min = ,1. Only objects
from the training set that have values of a within the range min::max are considered in the
following operations.
For each value observed in the training set for the attribute within the allowable range
but outside the actual range of values of a for objects at l, the evidence is evaluated in
support of reclassifying the region above or below that threshold. The level of support for a
given threshold is evaluated using a Laplacian accuracy estimate (Niblett & Bratko, 1986).
Because each leaf relates to a binary classification (an object belongs to the class in question
or does not), the binary form of Laplace is used. For threshold t on attribute a at leaf l,
the evidence in support of labeling the partition below t with class n is the maximum value
for an ancestor node x of l for the formula
P +1
T +2
where T is the number of objects at x for which min < a  t; and P is the number of those
objects which belong to class n.
The evidence in support of labeling a partition above a threshold is calculated identically
with the exception that the objects for which t < a  max are instead considered.
If the maximum evidence for a new labeling exceeds the evidence for the current labeling
of the region, a new branch is added for the appropriate threshold creating a new leaf node
labeled with the appropriate class.
In addition to evidence in favor of the current labeling gathered as above, further evidence in support of the current labeling of a region is calculated using the Laplace accuracy
404

fiFurther Experimental Evidence against the Utility of Occam's Razor

estimate considering the objects at the leaf, where T is the number of objects at the leaf and
P is the number of those objects that belong to the class with which the node is labeled.
This approach ensures that all new partitions define true regions. That is, for any
attribute a and value v it is not possible to partition on a  v unless it is possible for
both objects from the domain with values of a greater than v and objects with values less
than or equal to v to reach the node being partitioned (even though no objects from the
training set will fall within the new partition). In particular, this ensures that the new cuts
are not simple duplications of existing cuts at ancestors to the current node. Thus, every
modification adds non-redundant complexity to the tree.
This algorithm is presented in Figure 2. It has been implemented as a modification
to C4.5 release 6, called C4.5X. The source code for these modifications is available as an
on-line appendix to this paper.
In C4.5X, where multiple sets of values equally satisfy the specified constraints and
maximize the Laplace function, values of na and nb that are deeper in the tree are selected
over those closer to the root and, at a single node, preference for values of aa and ab depends
upon the order of attributes in the definition of the data and preference for values of va
and vb is dependent upon data order. These selection strategies are a side effect of the
implementation of the system. There is no reason to believe that the experimental results
would differ in general if other strategies were used to select between competing constraints.
By default, C4.5 develops two decision trees each time that it is run, an unpruned and a
pruned (simplified) decision tree. C4.5X produces post-processed versions of both of these
trees.

3.3 Evaluation
To evaluate the post-processor it was applied to all datasets containing continuous attributes
from the UCI machine learning repository (Murphy & Aha, 1993) that were then held (due
to previous machine learning experimentation) in the local repository at Deakin University.
These datasets are believed to be broadly representative of those in the repository as a
whole. After experimentation with these eleven data sets, two additional data sets, sick
euthyroid and discordant results, were retrieved from the UCI repository and added to the
study in order to investigate specific issues, as discussed below.
The resulting thirteen datasets are described in Table 1. The second column contains
the number of attributes by which each object is described. Next is the proportion of these
that are continuous. The fourth column indicates the proportion of attribute values in the
data that are missing (unknown). The fifth column indicates the number of objects that
the data set contains. The sixth column indicates the proportion of these that belong to
the class represented by the most objects within the data set. The final column indicates
the number of classes that the data set describes. Note that the glass type dataset uses the
Float/Not Float/Other three class classification rather than the more commonly used six
class classification.
Each data set was divided into training and evaluation sets 100 times. Each training
set consisted of 80% of the data, randomly selected. Each evaluation set consisted of the
remaining 20% of the data. Both C4.5 and C4.5X were applied to each of the resulting 1300
(13 data sets by 100 trials) training and evaluation set pairs.
405

fiWebb

Let cases(n) denote the set of all training examples that can reach node n.
Let value(a; x) denote the value of attribute a for training example x.
Let pos(X; c) denote the number of objects of class c in the set of training examples X.
Let Laplace(X; c) = ( +2)+1 where X is a set of training examples, jX j is the number of training
examples and c is a class.
Let upperlim(n; a) denote the minimum value of a cut on attribute a for an ancestor node of n for
which n lies below a  branch. If there is no such cut, upperlim(n; a) = 1. This determines an
upper bound on the values for a that may reach n.
Let lowerlim(n; a) denote the maximum value of a cut on attribute a for an ancestor node of n for
which n lies below a > branch. If there is no such cut, lowerlim(n; a) = ,1. This determines a
lower bound on the values for a that may reach n.
To post-process leaf l dominated by class c
1. Find values of
n : n is an ancestor of l
a : a is a continuous attribute
v : 9x : x 2 cases(n ) & v = value(a ; x) & v  min(v : 9y : y 2 cases(l) & v =
value(a ; y)) & v > lowerlim(l; a )
c : c is a class
that maximize L = Laplace(fx : x 2 cases(n ) & value(a ; x)  v & value(a ; x) >
lowerlim(l; a )g; c ).
2. Find values of
n : n is an ancestor of l
a : a is a continuous attribute
v : 9x : x 2 cases(n ) & v = value(a ; x) & v > max(v : 9y : y 2 cases(l) & v =
value(a ; y)) & v  upperlim(l; a )
c : c is a class
that maximize L = Laplace(fx : x 2 cases(n ) & value(a ; x) > v & value(a ; x) 
upperlim(l; a )g; c ).
3. If L > Laplace(cases(l); c) & L  L then
(a) if c 6= c
i. replace l with a node n with the test a  v .
ii. set the  branch for n to lead to a new leaf for class c .
iii. set the > branch for n to lead to l.
else if L > Laplace(cases(l); c)
(b) if c 6= c
i. replace l with a node n with the test a  v .
ii. set the > branch for n to lead to a new leaf for class c .
iii. set the  branch for n to lead to l.
pos X;c
jX j

a

a

a

a

a

a

a

a

a

a

a

a

a

a

a

a

b

b

b

b

a

b

b

b

b

a

a

a

b

b

b

a

b

b

b

b

b

b

b

b

a

b

b

a

b

a

a

a

a

b

b

b

b

b

Figure 2: C4.5X post-processing algorithm
406

fiFurther Experimental Evidence against the Utility of Occam's Razor

Table 1: UCI data sets used for experimentation
%
% most
No. of contin%
No. of common No. of
Name
Attrs. uous missing objects class classes
breast cancer Wisconsin
9
100
<1
699
66
2
Cleveland heart disease
13
46
<1
303
54
2
credit rating
15
40
1
690
56
2
discordant results
29
24
6
3772
98
2
echocardiogram
6
83
3
74
68
2
glass type
9
100
0
214
40
3
hepatitis
19
32
6
155
79
2
Hungarian heart disease
13
46
20
295
64
2
hypothyroid
29
24
6
3772
92
4
iris
4
100
0
150
33
3
new thyroid
5
100
0
215
70
3
Pima indians diabetes
8
100
0
768
65
2
sick euthyroid
29
24
6
3772
94
2
Table 2 summarizes the percentage predictive accuracy obtained for the unpruned decision trees generated by both C4.5 and C4.5X. It presents the mean (x) and standard
deviation (s) over each set of 100 trials with respect to each data set for both C4.5 and
C4.5X along with the results of a two-tailed matched pairs t-test comparing these means.
For twelve of the thirteen data sets C4.5X obtained a higher mean accuracy than C4.5. For
the remaining data set, hypothyroid, C4.5 obtained higher mean predictive accuracy than
C4.5CS (albeit by a small margin|measured to two decimal places the respective mean accuracies were 99.51 and 99.46, respectively). For nine of the data sets the advantage toward
C4.5X is statistically significant at the 0.05 level (p  0:05), although the advantage with
respect to the discordant results data is too small to be apparent when measured to one
decimal place (measured to two decimal places the values are 98.58 and 98.62 respectively).
The advantage toward C4.5 for the hypothyroid data is also statistically significant at the
0.05 level. The differences in mean predictive accuracy for the Hungarian heart disease,
new thyroid and sick euthyroid data sets are not significant at the 0.05 level.
Table 3 uses the same format as Table 2 to summarize the predictive accuracy obtained
for the pruned decision trees generated by both C4.5 and C4.5X. For the same twelve data
sets C4.5X obtained a higher mean predictive accuracy than C4.5. For the remaining data
set, hypothyroid, C4.5 again obtained higher mean predictive accuracy, although again the
magnitude of the difference is so small that it is not apparent at the level of precision
displayed (measured to two decimal places the mean accuracies are 99.51 and 99.46). For
six of the data sets the advantage toward C4.5X is statistically significant at the 0.05
level, although the difference is only apparent at a precision of two decimal places for the
discordant results data (99.81 and 99.82, respectively). The advantage toward C4.5 for
the hypothyroid data is also statistically significant at the 0.05 level. The differences for
407

fiWebb

Table 2: Percentage predictive accuracy for unpruned decision trees.
Name
breast cancer Wisconsin
Cleveland heart disease
credit rating
discordant results
echocardiogram
glass type
hepatitis
Hungarian heart disease
hypothyroid
iris
new thyroid
Pima indians diabetes
sick euthyroid

C4.5

x

94.1
72.8
82.2
98.6
72.0
74.0
79.6
77.0
99.5
95.4
89.9
70.2
98.7

s

1.8
5.0
3.4
0.5
9.8
7.0
7.1
5.3
0.2
3.4
4.2
3.5
0.5

C4.5X

x

s

t

p

94.4 1.7 {3.2 0.002
74.4 4.8 {6.1 0.000
83.0 3.3 {7.6 0.000
98.6 0.5 {5.4 0.000
73.5 10.2 {2.8 0.007
75.3 7.2 {4.2 0.000
80.8 6.9 {3.3 0.001
77.4 5.2 {1.8 0.082
99.5 0.2 4.4 0.000
95.7 3.5 {2.2 0.028
90.1 4.3 {1.0 0.302
71.3 3.6 {8.1 0.000
98.7 0.5 {0.0 0.963

Table 3: Percentage accuracy for pruned decision trees.
Name
breast cancer Wisconsin
Cleveland heart disease
credit rating
discordant results
echocardiogram
glass type
hepatitis
Hungarian heart disease
hypothyroid
iris
new thyroid
Pima indians diabetes
sick euthyroid

C4.5

x

95.1
74.1
84.1
98.8
74.2
74.4
79.9
79.2
99.5
95.4
89.6
72.2
98.7

s

1.7
5.3
3.2
0.4
9.3
6.9
6.2
4.9
0.2
3.6
4.2
3.5
0.4

C4.5X

x

95.2
74.8
84.6
98.8
75.1
75.4
80.7
79.4
99.5
95.7
89.8
72.8
98.7

s

1.7
5.3
3.2
0.4
9.8
6.9
6.2
4.8
0.2
3.7
4.2
3.5
0.4

t

p

{2.0 0.051
{3.7 0.000
{5.3 0.000
{2.6 0.010
{1.6 0.1180
{3.3 0.001
{3.0 0.003
{1.0 0.310
5.4 0.000
{1.6 0.109
{0.8 0.451
{5.9 0.000
{0.7 0.480

breast cancer Wisconsin, echocardiogram, Hungarian heart disease, iris, new thyroid and
sick euthyroid are not statistically significant at the 0.05 level.
After completing experimentation on the initial eleven data sets, the results for the
hypothyroid data stood out in stark contrast from those for the other ten. This raised
the possibility that there might be distinguishing features of the hypothyroid data that
408

fiFurther Experimental Evidence against the Utility of Occam's Razor

accounted for this difference in performance. Table 1 indicates this data set is clearly
distinguishable from the other ten initial data sets in the following six respects|

 having more attributes;
 containing a greater proportion of discrete attributes (which are not directly addressed
by C4.5X);






containing more objects;
having a greater proportion of the objects belong to the most common class;
having more classes; and
producing decision trees of extremely high predictive accuracy without post-processing.

To explore these issues the discordant results and sick euthyroid data sets were retrieved
from the UCI repository and added to the study. These data sets are identical to the
hypothyroid data set with the exception that each has a different class attribute. All three
data sets contain the same objects, described by the same attributes. The addition of the
discordant results and sick euthyroid data did little to illuminate this issue however. For
all three data sets the changes in accuracy are of very small magnitude. For hypothyroid
there is a significant advantage to C4.5. For sick euthyroid there is no significant advantage
to either system. For the discordant results data there is a significant advantage to C4.5X.
The question of whether there is a distinguishing feature of the hypothyroid data that
explains the observed results remains unanswered. Further investigation of this issue lies
beyond the scope of the current paper but remains an interesting direction for future research.
These results suggest that C4.5X's post-processing more frequently increases predictive
accuracy than not for the type of data to be found in the UCI repository. (Of the twenty-six
comparisons, there was a significant increase for fifteen and there was a significant decrease
for only two. A sign test reveals that this rate of success is significant at the 0.05 level,
p = 0:001.)
Tables 4 and 5 summarize the number of nodes in the decision trees developed. Table 4
addresses unpruned decision trees and Table 5 addresses pruned decision trees. Each postprocessing modification replaces a single leaf with a split and two leaves. At most one such
modification can be performed per leaf in the original tree. For all data sets the postprocessed decision trees are significantly more complex than the original decision trees. In
most cases post-processing has increased the mean number of nodes in the decision trees
by approximately 50%. This demonstrates that the post-processing is causing substantial
change.

4. Discussion

The primary objective of this research has been to discredit the Occam thesis. To this
end it uses a post-processor that disregards the Occam thesis and instead is theoretically
founded upon the similarity assumption. Experimentation with this post-processor has
409

fiWebb

Table 4: Number of nodes for unpruned decision trees.
C4.5
C4.5X
Name
x
s
x
s
t
p
breast cancer Wisconsin 38.1 6.0 64.0 10.3 {51.5 0.000
Cleveland heart disease 66.7 7.1 100.2 11.3 {61.9 0.000
credit rating
117.6 18.1 177.9 28.4 {44.2 0.000
discordant results
64.0 10.6 85.2 16.2 {33.3 0.000
echocardiogram
15.4 4.1 22.1 6.3 {26.1 0.000
glass type
43.0 5.2 69.7 8.4 {57.2 0.000
hepatitis
24.5 4.2 34.8 6.0 {49.1 0.000
Hungarian heart disease 62.1 7.5 94.8 13.0 {50.1 0.000
hypothyroid
29.4 4.4 47.5 7.1 {57.8 0.000
iris
9.0 1.9 16.0 4.0 {31.5 0.000
new thyroid
14.7 2.4 23.4 3.8 {41.5 0.000
Pima indians diabetes 164.8 10.8 238.8 16.3 {108.9 0.000
sick euthyroid
71.7 6.6 111.4 12.1 {65.8 0.000
Table 5: Number of nodes for pruned decision trees.
C4.5

C4.5X

Name
x
s
x
breast cancer Wisconsin 19.2 5.0 33.1
Cleveland heart disease 44.6 8.3 68.3
credit rating
51.2 14.8 78.4
discordant results
24.9 5.6 32.5
echocardiogram
10.4 3.0 14.8
glass type
36.6 5.5 61.0
hepatitis
13.7 4.8 19.8
Hungarian heart disease 26.8 11.4 41.2
hypothyroid
23.6 2.9 37.1
iris
8.2 1.9 14.8
new thyroid
14.1 2.7 22.5
Pima indians diabetes 112.0 16.4 163.9
sick euthyroid
46.5 5.8 72.6

s

8.6
12.8
24.2
8.8
4.8
9.5
6.6
17.3
5.6
3.9
4.3
24.0
8.7

t

{34.9
{43.6
{25.8
{21.1
{21.0
{48.5
{30.7
{22.1
{46.7
{30.3
{36.9
{62.5
{76.7

p

0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000

demonstrated that it is possible to develop systematic procedures that, for a range of `realworld' learning tasks increase the predictive accuracy of inferred decision trees as a result
of changes that substantially increase their complexity without altering their performance
upon the training data.
It is, in general, dicult to attack the Occam thesis due to the absence of a widely
agreed formulation thereof. However, it is far from apparent how the Occam thesis might
410

fiFurther Experimental Evidence against the Utility of Occam's Razor

..........
. . . .{. . . . . . { {
..........
.......... { {
.......... {
+
{
+
{
+
1 2 3 4 5 6 7 8 9 10
A
Figure 3: Modified simple instance space
10
9
8
7
6
B5
4
3
2
1

be recast to both accommodate these experimental results and provide a practical learning
bias.

4.1 Directions for Future Research
The implications of this research reach beyond its relevance to Occam's razor. The postprocessor appears to have practical utility in increasing the quality of inferred decision trees.
However, if the objective of the research were to improve predictive accuracy rather than
to discredit the Occam thesis, the post-processor would be modified in a number of ways.
The first modification would be to enable the addition of multiple partitions at a single
leaf from the original tree. C4.5X selects only the single modification for which there is the
maximum support. This design decision originated from a desire to minimize the likelihood
of performing modifications that will decrease accuracy. In principle, however, it would
appear desirable to select all modifications for which there is strong support, each of which
could then be inserted into the tree in order of level of supporting evidence.
Even greater increases in accuracy might be expected if one removed the constraint that
the post-processing should not alter the performance of the decision tree with respect to
the training set. In this case, new partitions may well be found that employ objects from
other regions of the instance space to provide evidence in support of adding partitions that
correct misclassifications of small numbers of objects at a leaf node from the original tree.
The similarity assumption would provide strong evidence for such repartitioning. Such
a situation would occur, for example, with respect to the learning problem illustrated in
Figure 1, if there was an additional object of class { with attribute values A=2 and B=9.
This is illustrated in Figure 3. In this case C4.5 would still create the indicated partitions.
However, C4.5X would be unable to relabel the area containing the additional object due to
the constraint that it not alter the performance of the original decision tree with respect to
the training set. Thus the addition of the object prevents C4.5X from relabeling the shaded
region even though, on the basis of the similarity assumption, it improves the evidence in
support of that relabeling.
Such an extended post-processor would encourage the following model of inductive inference of decision trees. The role of C4.5 (or a similar system) would be to identify clusters
411

fiWebb

of objects within the instance space that should be grouped under a single leaf node. A
second stage would then analyze regions of the instance space that lie outside those clusters
in order to allocate classes to those regions. Current decision tree learners, motivated by
the Occam thesis, ignore this second stage, leaving regions outside the identified clusters
associated with whatever classes have been assigned to them as a by-product of the cluster
identification process.

4.2 Other Related Research
A number of researchers have developed learning systems that can be viewed as considering
evidence from neighboring regions of the instance space in order to derive classifications
within regions of the instance space that are not occupied by examples from the training
set. Ting (1994) does this explicitly, by examining the training set to directly explore the
neighborhood of the object to be classified. This system uses instance based learning for
classification within nodes of a decision tree with low empirical support (small disjuncts).
A number of other systems can also be viewed as considering evidence from neighboring
regions for classification. These systems learn and then apply multiple classifiers (Ali,
Brunk, & Pazzani, 1994; Nock & Gascuel, 1995; Oliver & Hand, 1995). In such a context,
any point within a region of the instance space that is occupied by no training objects is
likely to be covered by multiple leaves or rules. Of these, the leaf or rule with the greatest
empirical support will be used for classification.
C4.5X uses two distinct criteria for evaluating potential splits. The standard C4.5 stage
of tree induction employs an information measure to select splits. The post-processor uses
a Laplace accuracy estimate. Similar uses of dual criteria have been investigated elsewhere.
Quinlan (1991) employs a Laplace accuracy estimate considering neighboring regions of the
instance space to estimate the accuracy of small disjuncts. Lubinsky (1995) and Brodley
(1995) employ resubstitution accuracy to select splits near the leaves during induction of
decision trees.
By adding a split to a leaf, C4.5X is specializing with respect to the class at that leaf
(and generalizing with respect to the class of the new leaf). Holte et al. (1989) explored a
number of techniques for specializing small disjuncts. C4.5X differs in that all leaves are
candidates for specialization, not just those with low empirical support. It further differs
in the manner in which it selects the specialization to perform by considering the evidence
in support of alternative splits rather than just the strength of the evidence in support of
individual potential conditions for the current disjunct.

4.3 Bias Versus Variance
Breiman, Friedman, Olshen, and Stone (1984) provide an analysis of complexity and induction in terms of a trade-off between bias and variance. A classifier partitions the instance
space into regions. When these regions are too large, the degree of fit to an accurate partitioning of the instance space will be poor, increasing error rates. This effect is called bias.
When the regions are too small, the probability that individual regions are labeled with the
wrong class is increased. This effect, called variance, also increases error rates. According to
this analysis, due to variance, too fine a partitioning of the instance space tends to increase
412

fiFurther Experimental Evidence against the Utility of Occam's Razor

the error rate while, due to bias, too coarse a partitioning also tends to increase the error
rate.
Increasing the complexity of a decision tree creates finer partitionings of the instance
space. This analysis can be used to argue against the addition of undue complexity to
decision trees on the ground that it will increase variance and hence the error rate.
However, the success of C4.5X in decreasing the error rate demonstrates that it is
successfully managing the bias/variance trade-off when it introduces complexity to the
decision tree. By using evidence from neighboring regions of the instance space, C4.5X
is successful in increasing the error rate resulting from variance at a lower rate than it
decreases the error rate resulting from bias. The success of C4.5X demonstrates that it is
not adding undue complexity to C4.5's decision trees.

4.4 Minimum Encoding Length Induction

Minimum encoding length approaches perform induction by seeking a theory that enables
the most compact encoding of both the theory and available data. Two key approaches
have been developed, Minimum Message Length (MML) (Wallace & Boulton, 1968) and
Minimum Description Length (MDL) (Rissanen, 1983). Both approaches admit to probabilistic interpretations. Given prior probabilities for both theories and data, minimization
of the MML encoding closely approximates maximization of posterior probability (Wallace & Freeman, 1987). An MDL code length defines an upper bound on \unconditional
likelihood" (Rissanen, 1987).
The two approaches differ in that MDL employs a universal prior, which Rissanen (1983)
explicitly justifies in terms of Occam's razor, while MML allows the specification of distinct
appropriate priors for each induction task. However, in practice, a default prior is usually
employed for MML, one that appears to also derive its justification from Occam's razor.
Neither MDL nor MML with its default prior would add complexity to a decision tree
if doing so were justified solely on the basis of evidence from neighboring regions of the
instance space. The evidence from the study presented herein appears to support the
potential desirability of doing so. This casts some doubt upon the utility of the universal
prior employed by MDL and the default prior usually employed with MML, at least with
respect to their use for maximizing predictive accuracy.
It should be noted, however, that the probabilistic interpretation of these minimum
encoding length techniques indicates that encoding length minimization represents maximization of posterior probability or of unconditional likelihood. Maximization of these
factors is not necessarily directly linked with maximizing predictive accuracy.

4.5 Appropriate Application of Grafting and Pruning

It is important to note that although this paper calls into question the value of learning
biases that penalize complexity, in no way does it provide support for learning biases that
encourage complexity for its own sake. C4.5X only grafts new nodes onto a decision tree
when there is empirical support for doing so.
Nor do the results in any way argue against the appropriate use of decision tree pruning.
To generate its pruned trees, C4.5 removes branches where statistical estimates of the upper
bounds on the error rates indicate that these will not increase if the branch is removed. It
413

fiWebb

could be argued that C4.5 only reduces complexity when there is empirical support for
doing so. It is interesting to note that for eight of the thirteen data sets examined, C4.5X's
post-processing of the pruned trees resulted in higher average predictive accuracy than
post-processing of unpruned trees. These results suggest that both pruning and grafting
can play a valuable role when applied appropriately.

5. Conclusion
This paper presents a systematic procedure for adding complexity to inferred decision trees
without altering their performance on the training data. This procedure has been demonstrated to lead to increases in predictive accuracy for a range of learning tasks when applied
to both pruned and unpruned trees inferred by C4.5. For only one of the thirteen learning
tasks examined did the procedure lead to a statistically significant loss in accuracy and in
this case the magnitude of the difference in mean accuracy was extremely small. On the
face of it, this provides strong experimental evidence against the Occam thesis.
This post-processing technique was developed by rejecting the Occam thesis and instead attending to the similarity assumption|that similar objects have high probability of
belonging to the same class.
The procedure developed was constrained by the need to ensure that the revised decision
tree performed identically to the original decision tree with respect to the training data.
This constraint arose from the desire to obtain experimental evidence against the Occam
thesis. It is possible that if this constraint is removed, the basic techniques outlined in this
paper could result in even greater improvements in predictive accuracy than those reported
herein.
This research has considered only one version of Occam's razor that favors minimization
of syntactic complexity in the expectation that this will tend to increase predictive accuracy.
Other interpretations of Occam's razor are also possible, such as that one should minimize
semantic complexity. While others (Bunge, 1963) have provided philosophical objections to
such formulations of Occam's razor, this paper has not sought to investigate them.
The version of Occam's razor examined in this research has been used widely in machine
learning with apparent success. The objections to this principle that have been substantiated by this research raise the question, why has it had such apparent success if it is so
awed? Webb (1994) suggests that the apparent success of the principle has been due to the
manner in which syntactic complexity is usually associated with other relevant qualities of
inferred classifiers such as generality or prior probability. If this thesis is accepted then one
of the key challenges facing machine learning is to understand these deeper qualities and to
employ that understanding to place machine learning on a sounder theoretical footing. This
paper offers a small contribution in this direction by demonstrating that minimization of
surface syntactic complexity does not, in itself, in general maximize the predictive accuracy
of inferred classifiers.
It is nonetheless important to realize that, the thrust of this paper notwithstanding,
Occam's razor will often be a useful learning bias to employ. This is because there will frequently be good pragmatic reasons for preferring a simple hypothesis. A simple hypothesis
will in general be easier to understand, communicate and employ. A preference for simple
414

fiFurther Experimental Evidence against the Utility of Occam's Razor

hypotheses cannot be justified in terms of expected predictive accuracy but may be justified
on pragmatic grounds.

Acknowledgements
This research has been supported by the Australian Research Council. I am grateful to
Charlie Clelland, David Dowe, Doug Newlands, Ross Quinlan and anonymous reviewers for
extremely valuable comments from which the paper has benefited greatly.

References

Aha, D. W., Kibler, D., & Albert, M. K. (1991). Instance-based learning algorithms.
Machine Learning, 6, 37{66.
Ali, K., Brunk, C., & Pazzani, M. (1994). On learning multiple descriptions of a concept.
In Proceedings of Tools with Artificial Intelligence New Orleans, LA.
Berkman, N. C., & Sandholm, T. W. (1995). What should be minimized in a decision tree:
A re-examination. Technical report 95-20, University of Massachusetts at Amherst,
Computer Science Department, Amherst, Mass.
Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. K. (1987). Occam's Razor.
Information Processing Letters, 24, 377{380.
Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). Classification and
Regression Trees. Wadsworth International, Belmont, Ca.
Brodley, C. E. (1995). Automatic selection of split criterion during tree growing based on
node selection. In Proceedings of the Twelth International Conference on Machine
Learning, pp. 73{80 Taho City, Ca. Morgan Kaufmann.
Bunge, M. (1963). The Myth of Simplicity. Prentice-Hall, Englewood Cliffs, NJ.
Clark, P., & Niblett, T. (1989). The CN2 induction algorithm. Machine Learning, 3,
261{284.
Fayyad, U. M., & Irani, K. B. (1990). What should be minimized in a decision tree?
In AAAI-90: Proceedings Eighth National Conference on Artificial Intelligence, pp.
749{754 Boston, Ma.
Good, I. J. (1977). Explicativity: A mathematical theory of explanation with statistical
applications. Proceedings of the Royal Society of London Series A, 354, 303{330.
Holte, R. C. (1993). Very simple classification rules perform well on most commonly used
datasets. Machine Learning, 11 (1), 63{90.
Holte, R. C., Acker, L. E., & Porter, B. W. (1989). Concept learning and the problem
of small disjuncts. In Proceedings of the Eleventh International Joint Conference on
Artificial Intelligence, pp. 813{818 Detroit. Morgan Kaufmann.
415

fiWebb

Lubinsky, D. J. (1995). Increasing the performance and consistency of classification trees by
using the accuracy criterion at the leaves. In Proceedings of the Twelth International
Conference on Machine Learning, pp. 371{377 Taho City, Ca. Morgan Kaufmann.
Michalski, R. S. (1984). A theory and methodology of inductive learning. In Michalski,
R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), Machine Learning: An Artificial
Intelligence Approach, pp. 83{129. Springer-Verlag, Berlin.
Murphy, P. M. (1995). An empirical analysis of the benefit of decision tree size biases as
a function of concept distribution. Tech. rep. 95-29, Department of Information and
Computer Science, University of California, Irvine.
Murphy, P. M., & Aha, D. W. (1993). UCI repository of machine learning databases.
[Machine-readable data repository]. University of California, Department of Information and Computer Science, Irvine, CA.
Murphy, P. M., & Pazzani, M. J. (1994). Exploring the decision forest: An empirical investigation of Occam's razor in decision tree induction. Journal of Artificial Intelligence
Research, 1, 257{275.
Niblett, T., & Bratko, I. (1986). Learning decision rules in noisy domains. In Bramer,
M. A. (Ed.), Research and Development in Expert Systems III, pp. 25{34. Cambridge
University Press, Cambridge.
Nock, R., & Gascuel, O. (1995). On learning decision committees. In Proceedings of the
Twelth International Conference on Machine Learning, pp. 413{420 Taho City, Ca.
Morgan Kaufmann.
Oliver, J. J., & Hand, D. J. (1995). On pruning and averaging decision trees. In Proceedings
of the Twelth International Conference on Machine Learning, pp. 430{437 Taho City,
Ca. Morgan Kaufmann.
Pearl, J. (1978). On the connection between the complexity and credibility of inferred
models. International Journal of General Systems, 4, 255{264.
Quinlan, J. R. (1986). Induction of decision trees. Machine Learning, 1, 81{106.
Quinlan, J. R. (1990). Learning logical definitions from relations. Machine Learning, 5,
239{266.
Quinlan, J. R. (1991). Improved estimates for the accuracy of small disjuncts. Machine
Learning, 6, 93{98.
Quinlan, J. R. (1993). C4.5: Programs For Machine Learning. Morgan Kaufmann, Los
Altos.
Rao, R. B., Gordon, D., & Spears, W. (1995). For every generalization action is there really
an equal and opposite reaction? Analysis of the conservation law for generalization performance. In Proceedings of the Twelth International Conference on Machine
Learning, pp. 471{479 Taho City, Ca. Morgan Kaufmann.
416

fiFurther Experimental Evidence against the Utility of Occam's Razor

Rendell, L., & Seshu, R. (1990). Learning hard concepts through constructive induction:
Framework and rationale. Computational Intelligence, 6, 247{270.
Rissanen, J. (1983). A universal prior for integers and estimation by minimum description
length. Annals of Statistics, 11, 416{431.
Rissanen, J. (1987). Stochastic complexity. Journal of the Royal Statistical Society Series
B, 49 (3), 223{239.
Schaffer, C. (1992). Sparse data and the effect of overfitting avoidance in decision tree
induction. In AAAI-92: Proceedings of the Tenth National Conference on Artificial
Intelligence, pp. 147{152 San Jose, CA. AAAI Press.
Schaffer, C. (1993). Overfitting avoidance as bias. Machine Learning, 10, 153{178.
Schaffer, C. (1994). A conservation law for generalization performance. In Proceedings
of the 1994 International Conference on Machine Learning San Mateo, Ca. Morgan
Kaufmann.
Ting, K. M. (1994). The problem of small disjuncts: Its remedy in decision trees. In
Proceedings of the Tenth Canadian Conference on Artificial Intelligence, pp. 63{70.
Morgan Kaufmann,.
Wallace, C. S., & Boulton, D. M. (1968). An information measure for classification. Computer Journal, 11, 185{194.
Wallace, C. S., & Freeman, P. R. (1987). Estimation and inference by compact coding.
Journal of the Royal Statistical Society Series B, 49 (3), 240{265.
Webb, G. I. (1994). Generality is more significant than complexity: Toward alternatives to
Occam's razor. In Zhang, C., Debenham, J., & Lukose, D. (Eds.), AI'94 { Proceedings of the Seventh Australian Joint Conference on Artificial Intelligence, pp. 60{67
Armidale. World Scientific.

417

fiA Divergence Critic
Bundy, A., van Harmelen, F., Horn, C., & Smaill, A. (1990). The Oyster-Clam system. In
Stickel, M. (Ed.), 10th International Conference on Automated Deduction, pp. 647{
648. Springer-Verlag. Lecture Notes in Artificial Intelligence No. 449.
Dershowitz, N., & Pinchover, E. (1990). Inductive Synthesis of Equational Programs. In
Proceedings of the 8th National Conference on AI, pp. 234{239. American Association
for Artificial Intelligence.
Hermann, M. (1989). Crossed term rewriting systems. CRIN report 89-R-003, Centre de
Recherche en Informatique de Nancy.
Ireland, A. (1992). The Use of Planning Critics in Mechanizing Inductive Proof. In Proceedings of LPAR'92, Lecture Notes in Artificial Intelligence 624. Springer-Verlag. Also
available as Research Report 592, Dept of AI, Edinburgh University.
Ireland, A., & Bundy, A. (1992). Using failure to guide inductive proof. Tech. rep., Dept.
of Artificial Intelligence, University of Edinburgh. Available from Edinburgh as DAI
Research Paper 613.
Kirchner, H. (1987). Schematization of infinite sets of rewrite rules. Application to the
divergence of completion processes. In Proceedings of RTA'87, pp. 180{191.
Protzen, M. (1992). Disproving conjectures. In Kapur, D. (Ed.), 11th Conference on
Automated Deduction, pp. 340{354. Springer Verlag. Lecture Notes in Computer
Science No. 607.
Thomas, M., & Jantke, K. (1989). Inductive Inference for Solving Divergence in KnuthBendix Completion. In Proceedings of International Workshop AII'89, pp. 288{303.
Thomas, M., & Watson, P. (1993). Solving divergence in Knuth-Bendix completion by
enriching signatures. Theoretical Computer Science, 112, 145{185.
Walsh, T. (1994). A divergence critic. In Bundy, A. (Ed.), 12th Conference on Automated
Deduction, pp. 14{25. Springer Verlag. Lecture Notes in Artificial Intelligence No.
814.
Walsh, T., Nunes, A., & Bundy, A. (1992). The use of proof plans to sum series. In Kapur,
D. (Ed.), 11th Conference on Automated Deduction, pp. 325{339. Springer Verlag.
Lecture Notes in Computer Science No. 607. Also available from Edinburgh as DAI
Research Paper 563.
Yoshida, T., Bundy, A., Green, I., Walsh, T., & Basin, D. (1994). Coloured rippling: the
extension of a theorem proving heuristic. In Cohn, A. (Ed.), Proceedings of ECAI-94,
pp. 85{89. John Wiley. Also available from Edinburgh as DAI Research Paper 779.

235

fiWalsh
of the way. There are other types of divergence which could perhaps be recognized by the
divergence critic. Further research is needed to identify such divergence patterns, isolate
their causes and propose ways of fixing them. This research may take advantage of the close
links between divergence patterns and particular types of generalization. For instance, it
may be possible to identify specific divergence patterns with the need to generalize common
subterms in the theorem being proved.

Acknowledgements

This research was supported by a Human Capital and Mobility Postdoctoral Fellowship.
I wish to thank: Adel Bouhoula and Michael Rusinowitch for their invaluable assistance
with Spike; Pierre Lescanne for inviting me to visit Nancy where most of this research
was performed; David Basin, Alan Bundy, Miki Hermann, Andrew Ireland, and Michael
Rusinowitch for their comments and questions; the reviewers for their comments and suggestions; the members of the Eureca and Protheo groups at INRIA; and the members of
the DReaM group at Edinburgh and the MRG groups in Trento and Genova.

References

Aubin, R. (1976). Mechanizing Structural Induction. Ph.D. thesis, University of Edinburgh.
Basin, D., & Walsh, T. (1992). Difference matching. In Kapur, D. (Ed.), 11th Conference
on Automated Deduction, pp. 295{309. Springer Verlag. Lecture Notes in Computer
Science No. 607. Also available from Edinburgh as DAI Research Paper 556.
Basin, D., & Walsh, T. (1993). Difference unification. In Proceedings of the 13th IJCAI.
International Joint Conference on Artificial Intelligence. Also available as Technical
Report MPI-I-92-247, Max-Planck-Institute fur Informatik.
Basin, D., & Walsh, T. (1994). Termination orderings for rippling. In Bundy, A. (Ed.), 12th
Conference on Automated Deduction, pp. 466{483. Springer Verlag. Lecture Notes in
Artificial Intelligence No. 814.
Bouhoula, A., Kounalis, E., & Rusinowitch, M. (1992). Spike: A theorem-prover. In
Proceedings of LPAR'92, Lecture Notes in Artificial Intelligence 624. Springer-Verlag.
Bouhoula, A., & Rusinowitch, M. (1995a). Implicit induction in conditional theories. Journal of Automated Reasoning, 14 (2), 189{235.
Bouhoula, A., & Rusinowitch, M. (1995b). Spike user manual.. INRIA Lorraine and
CRIN, 615 rue du Jardin Botanique, Villers-les-Nancy, France. Available from
ftp://ftp.loria.fr/pub/loria/protheo/softwares/Spike/.
Boyer, R., & Moore, J. (1979). A Computational Logic. Academic Press. ACM monograph
series.
Bundy, A., Stevens, A., van Harmelen, F., Ireland, A., & Smaill, A. (1993). Rippling:
A heuristic for guiding inductive proofs. Artificial Intelligence, 62, 185{253. Also
available from Edinburgh as DAI Research Paper No. 567.
234

fiA Divergence Critic
rev(qrev(x; nil)) = x
rev(qrev(x; cons(y; nil) )) = cons(y; x)
rev(qrev(x; cons(z; cons(y; nil)) )) = cons(z; cons(y; x))
..
.

This annotated sequence is the unique maximal difference match. These annotations suggest
the need for the wave rule,
rev(qrev(X; cons(Y; nil) )) = cons(Y; rev(qrev(X; nil))) :

This rule allows the proof to go through without divergence. By comparison, most specific
generalization seems to be unable to identify this rule. The most specific generalization of
the left hand side of this sequence gives the term rev(qrev(X; Z)) (or, ignoring the first term
in the sequence, rev(qrev(X; cons(Y; Z)))). Most specific generalization cannot, however,
identify the more useful pattern, rev(qrev(X; cons(Y; nil))).
Nqthm contains a simple test for divergence based on subsumption. For instance, on
example 13 of the last section, Nqthm is unable to simplify the following subgoal in the
step case of the proof,
(EQUAL (ROT (LENGTH X) (APPEND X (LIST Z)))
(CONS Z (ROT (LENGTH X) X))))

Note that this is the lemma speculated by the divergence critic. Nqthm generalizes (LENGTH
X) in this subgoal giving the false conjecture,
(EQUAL (ROT Y (APPEND X (LIST Z)))
(CONS Z (ROT Y X))))

After several more attempts at induction and generalization, Nqthm realizes the proof is
diverging since a subgoal is subsumed by its parent. As the proof is therefore about to loop,
Nqthm gives up. No attempt is made to analyse the failed proof attempt to identify where
it started to go wrong. In addition, subsumption is a very weak test for divergence, much
weaker than tests based on difference matching or generalization. This subsumption test
recognizes divergence on just a small number of the failed examples in the last section.

10. Conclusions

I have described a divergence critic, a computer program which attempts to identify diverging proof attempts and to propose lemmas and generalizations which overcome the
divergence. The divergence critic has proved very successful; it enables the system Spike
to prove many theorems from the definitions alone. The divergence critic's success can
be largely attributed to the power of the rippling heuristic. This heuristic was originally
developed for proofs using explicit induction but has since found several other applications.
Difference matching is used to identify accumulating term structure which is causing divergence. Lemmas and generalizations are then proposed to ripple this term structure out
233

fiWalsh
divergence critic described here works in an implicit (and not an explicit) induction setting.
Second, the divergence critic is not automatically invoked but must identify when the proof
is failing. Third, the divergence critic is less specialized. These last two differences reect the
fact that critics in Clam are usually associated with the failure of a particular precondition
to a heuristic. The same divergence pattern can, by comparison, arise for many different
reasons: the need to generalize variables apart, to generalize common subterms, to add a
lemma, etc. Fourth, the divergence critic must use difference matching to annotate terms; in
Clam, terms are usually already appropriately annotated. Finally, the divergence critic is
less tightly coupled to the the theorem prover's inference rules or heuristics. The critic can
therefore exploit the strengths of the prover without needing to reason about the complex
rules or heuristics being used. For instance, the divergence critic has no diculty identifying
divergence in complex situations like nested or mutual inductions. The critic also benefits
from the powerful simplification rules used by Spike.
Divergence has been studied quite extensively in completion procedures. Two of the
main novelties of the critic described here are the use of difference matching to identify
divergence, and the use of rippling in the speculation of lemmas to overcome divergence.
Dershowitz and Pinchover, by comparison, use most specific generalization to identify divergence patterns in the critical pairs produced by completion (Dershowitz & Pinchover, 1990).
Kirchner uses generalization modulo an equivalence relation to recognise such divergence
patterns (Kirchner, 1987); meta-rules are then synthesized to describe infinite families of
rules with some common structure. Thomas and Jantke use generalization and inductive
inference to recognize divergence patterns and to replace infinite sequences of critical pairs
by a finite number of generalizations (Thomas & Jantke, 1989). Thomas and Watson use
generalization to replace an infinite set of rules by a finite complete set with an enriched
signature (Thomas & Watson, 1993).
Generalization modulo an equivalence enables complex divergence patters to be identified. However, it is in general undecidable. Most specific generalization, by comparison, is
more limited. It cannot recognize divergence patterns which give nested wave-fronts like,

s( s(x) + x) :
In addition, most specific generalization cannot identify term structure in wave-holes. For
example, consider the divergence sequence of equations produced when Spike attempts to
prove example 25 from Section 8,
rev(qrev(x; nil)) = x
rev(qrev(x; cons(y; nil))) = cons(y; x)
rev(qrev(x; cons(z; cons(y; nil)))) = cons(z; cons(y; x))
..
.

Divergence analysis identifies term structure accumulating within the accumulator argument
of qrev ,
232

fiA Divergence Critic
Unfortunately the heuristics for instantiating the right hand side of speculated lemmas are
not strong enough to suggest the rule,

X + (Y + Z ) = Y + (X + Z )
With this rule, Spike finds a proof of the commutativity of multiplication without diculty.
The diculties in speculating this rule arise because the wave-front is stuck in a similar
position on both sides of the equality. There are few clues therefore to suggest how to ripple
it up to the top of the term tree.
In example 33, the divergence critic proposes a lemma where one is not needed. Spike
is able to find a proof of this theorem from the definitions alone using 16 inductions. Three
of these inductions are on the equations,
0+x = x
s(0) + x = s(x)
s(s(0)) + x = s(s(x))
This sequence of equations satisfies the divergence critic's preconditions. The critic therefore
proposes wave rules for moving accumulating successor functions off the first argument
position of +. Although the proposed lemmas are not necessary, either give a much shorter
and simpler proof needing just 7 inductions.
Example 34 is the lemma speculated in example 24. Divergence analysis of Spike's
attempt to prove this theorem identifies term structure accumulating on the second (alias
accumulator) argument of qrev . The first two lemmas proposed for removing this term
structure are of no use as they are subsumed by the recursive definition of qrev . The third
lemma also fails to prevent divergence. This lemma simplifies two element lists in the second
argument position of qrev . However, divergence will still occur as the prover cannot simplify
lists that occur in the second argument position of qrev which contain 3 or more elements.
Divergence can be overcome if we introduce a derived function for appending onto the end
of a list. This can be used to simplify terms in which a list of arbitrary size occurs on the
second argument position of qrev . For example, we can simplify with the rule,

qrev(X; Y ) = app(qrev(X; nil); Y )
Unfortunately, append does not occur in the specification of the theorem so it is dicult to
find a heuristic that would speculate such a rule.

9. Related Work

Critics for monitoring the construction of proofs were first proposed by Ireland for the Clam
prover (Ireland, 1992). In this framework, failure of one of the proof methods automatically
invokes a critic. Various critics for explicit induction have been developed that speculate
missing lemmas, perform generalizations, look for suitable case splits, etc. As rippling plays
a central role in Clam's proof methods, many of the heuristics are similar to those described
here (Ireland & Bundy, 1992). There are, however, several significant differences. First, the
231

fiWalsh
No
Theorem
31
s(x)  y=y+(x  y)
32
x  y=y  x
33 x+(y+(z+(v+w))) = w+(x+(y+(z+v)))

Lemmas speculated
Time/s
{
n/a
s(X)+Y=X+s(Y)
8.0
s(X)+Y=s(X+Y)
17.7
s(X)+Y=X+s(Y)
34 qrev(qrev(x,[y]),z)=y ::qrev(qrev(x,[]),z)
qrev(Y,X ::Z)) = qrev(X ::Y,Z)
9.6
qrev(qrev(X,Y ::Z),W)=qrev(qrev(Y ::X,Z),W)
qrev(qrev(X,Y ::[Z]),W)=Z::qrev(qrev(X,[Y]),W)

Table 2: Some of the divergence critic's failures.
diverges, generating the following sequence of equations,
s(y) + (x + (x  y)) = s(y) + (y + (x  y))
s(s(y)) + (x + (x + (x  y))) = s(x) + (s(y) + (x + (x  y))
s(s(s(y))) + (x + (x + (x + (x  y)))) = s(x) + (s(s(y)) + (x + (x + (x  y))))
..
.
Divergence analysis of the left hand sides of these equations suggests the need for a rule of
the form,

s(Y ) + (X + Z ) = F (Y + Z )
Unfortunately the heuristics for lemma speculation are not suciently strong to suggest
a suitable instantiation for F (for example, z : s(X + z )). This lemma is rather complex
and is the result of two overlapping divergence patterns. If the annotations are considered
separately, they suggest the rules,

s(X ) + Y = s(X + Y )
Y + (X + Z ) = X + (Y + Z )
With these two rules, Spike finds a proof without diculty.
Example 32 is the commutativity of multiplication. The divergence critic identifies a
divergence pattern and proposes the transverse wave rule,

s(X ) + Y = X + s(Y )
However, Spike is unable to prove the commutativity of multiplication with the addition of
this rule. The proof attempt is now somewhat simpler and contains the diverging sequence
of equations,
x + (y + (x + (x  y))) = y + (x + (x + (x  y)))
x + (y + (x + (x + (x  y))) ) = y + (x + (x + (x + (x  y))))

x + (y + (x + (x + (x + (x  y)))) ) = y + (x + (x + (x + (x + (x  y)))))
..
.

230

fiA Divergence Critic
speculate more non-theorems. Further research into the optimal strength of generalization
heuristics would be valuable.
Example 24 is the only disappointment; the lemma proposed fixes divergence but is
too dicult to be proved automatically, even with the assistance of the divergence critic.
See example 34 at the end of this section for more details. Example 25 is discussed in
more detail in the related work in Section 9 as it demonstrates the superiority of difference
matching over generalization techniques for divergence analysis. Examples 26 to 28 require
little discussion. Finally, examples 29 and 30 demonstrate that the critic can cope with
divergence in moderately complex theories containing conditional equations.
The results are very pleasing. Using the divergence critic, the 30 theorems listed (with
the exception of 24) can all be proved from the definitions alone. To provide an indication
of the diculty of these theorems, the Nqthm system (Boyer & Moore, 1979), which is
perhaps the best known explicit induction theorem prover, was unable to prove more than
half these theorems from the definitions alone. To be precise, Nqthm failed on 5, 6, 7, 8, 9,
11, 12, 13, 14, 15, 18, 19, 21, 22, 24, 25, 26, 27 and 28. Of course, with the addition of some
simple lemmas, Nqthm is able to prove all these theorems. Indeed, in many cases, Nqthm
needs the same lemmas as those proposed by the divergence critic and required by Spike.
This suggests that the divergence critic is not especially tied to the particular prover used
nor even to the implicit induction setting.
To test this hypothesis, I presented the output of a diverging proof attempt from Nqthm
to the critic. I chose the commutativity of multiplication as this is perhaps the simplest
theorem which causes Nqthm to diverge. The critic proposed the lemma,
(EQUAL (TIMES Y (ADD1 X)) (PLUS Y (TIMES Y X))))

where TIMES and PLUS are primitives of Nqthm's logic recursively defined on their first
arguments. This is exactly the lemma needed by Nqthm to prove the commutativity of
multiplication. Nqthm fails on many of the other examples for similar reasons to Spike,
and divergence analysis identifies an appropriate lemma. This supports the suggestion that
the divergence critic is likely to be useful for a wide variety of provers.
The divergence critic has several limitations. Recognizing divergence is, in general,
undecidable since it reduces to the halting problem. The divergence critic will therefore
sometimes fail to identify a diverging proof attempt. In addition, the critic will sometimes
identify a \divergence" pattern when the proof attempt is not diverging. Even when divergence is correctly identified, the critic will sometimes fail to speculate an appropriate
lemma. Finally, the critic only speculates wave-rules. Whilst many theories contain a large
number of wave-rules, and these are often very useful for fixing divergence, other types of
lemma can be needed.
Table 2 lists four theorems on which the divergence critic fails. These problems are
representative of the different ways in which the critic can fail. The two main cause of
failure are overlapping divergence patterns, and the inability of the heuristics to speculate
an appropriate right hand side for a lemma. Again times are those to speculate lemmas
and not to find a proof of the theorem.
Example 31 is a commuted version of the recursive definition of multiplication ( is
defined recursively on its second argument position). Spike's attempt to prove this theorem
229

fiWalsh
Just as in examples 6 and 7, these are not the optimal rules for fixing divergence. Nevertheless, either of the proposed rules fix divergence and both can be proved without diculty
by Spike. Example 9 is very similar to example 8.
Examples 10 to 12 require little comment. In example 13, the proposed lemma is too
dicult to be proved automatically. However, the divergence critic is able to identify the
cause of this diculty and propose a lemma which allows the proof to go through (example
15). In example 14, the speculated lemma is not optimal. The simpler lemma speculated in
example 13 would be adequate to prove this theorem without divergence. The speculated
lemma is not optimal because the divergence critic attempts to ripple the accumulating
term structure over two functors, len and rot to the top of the term tree. However, it is
sucient on this problem to ripple it up over just one functor, rot.
Examples 16 to 19 are straightforward and do not require discussion. In example 20, the
critic identifies two separate divergence patterns. To overcome divergence, the first lemma
plus one or other of the second and third are therefore needed. The first divergence pattern
occurs in the sequence of subgoals,

len(rev(x)) = 0 + len(x)
len( app(rev(x); cons(y; nil)) ) = s(0 + len(x))
len( app(app(rev(x); cons(y; nil)); cons(z; nil)) ) = s(s(0 + len(x)))
..
.

Term structure is accumulating on the second argument of append. Such term structure is
removed by the first rule,

len( app(X; cons(Y; nil)) ) = s(len(X ))
The second divergence pattern occurs in the sequence of subgoals,

s(x) + len(y) = s(x + len(y))
s(s(x)) + len(y) = s(s(x + len(y)))
s(s(s(x))) + len(y) = s(s(s(x + len(y))))
..
.

Term structure is accumulating on the first argument of +. This is removed by one or other
of the second and third rules,

s(X ) + Y = s(X + Y )
s(X ) + Y = X + s(Y )
Examples 21 and 23 are reasonably straightforward. The lemma speculated in example
22 is a special case of the associativity of append. More powerful generalization heuristics
could have speculated the associativity of append. However, such heuristics would also
228

fiA Divergence Critic
causes divergence in the current release. The speculated lemmas do, however, simplify the
proof. Example 4 was used in the text to illustrate the generalization heuristics. The second
lemma in example 5 is perhaps a little surprising,

len(app(X; (cons(W; cons(Z; Y ) )))) = s(len(app(X; cons(W; Y )))) :
Although it is more complex than the first lemma, it is nearly as good at fixing divergence.
In example 6, the lemma proposed,

even( s(s(X )) + Y ) = even(X + Y )
is not optimal. That is, it is not the simplest possible lemma that fixes divergence. To fix
divergence, we merely need one of the rules, s(X ) + Y = s(X + Y ) or s(X ) + Y = X +
s(Y ). Either of these will ripple the successor functions accumulating on the first argument
position of +. The divergence critic attempts to construct a lemma to ripple two successor
functions across from the first to the second argument positions of +. Unfortunately, the
critic fails to find an appropriate instantiation for the right hand side of such a lemma. The
critic instead proposes a rule to move the two successor functions up to the top of the term
where the wave-front can peter out. Example 7 is very similar to example 6.
Examples 8 to 10 demonstrate that the critic can cope with divergence in theories involving mutual recursion. In example 8, Spike attempts to prove by induction the equations,

evenm(x + x)
oddm(s(x) + x)
evenm (s(s(x)) + x)
oddm(s(s(s(x))) + x)
evenm (s(s(s(s(x)))) + x)

=
=
=
=
=
..
.

true
true
true
true
true

The critic identifies two inter-linking divergence patterns,

evenm (x + x) = true
evenm ( s(s(x)) + x) = true
evenm ( s(s(s(s(x)))) + x) = true

oddm(s(x) + x) = true
oddm( s(s(s(x))) + x) = true
oddm( s(s(s(s(s(x))))) + x) = true

..
.

..
.

The critic therefore proposes rules which ripple this accumulating term structure up to the
top of the term where it peters out,

evenm( s(s(X )) + Y ) = evenm (X + Y )
oddm( s(s(X )) + Y ) = oddm(X + Y )
227

fiWalsh
No
1

Theorem
s(x)+x=s(x+x)

Lemmas speculated
Time/s
s(X)+Y=s(X+Y)
7.8
s(X)+Y=X+s(Y)
2
dbl(x)=x+x $
s(X)+Y=s(X+Y)
8.2
dbl(0)=0, dbl(s(x))=s(s(dbl(x)))
s(X)+Y=X+s(Y)
3
len(x @ y)=len(y @ x)
len(X @ (Z ::Y))=s(len(X @ Y))
3.6
len(X @ (Z ::Y))=len((W ::X) @ Y)
4
len(x @ y)=len(x)+len(y)
s(X)+Y=s(X+Y)
7.2
s(X)+Y=X+s(Y)
5
len(x @ x)=dbl(len(x))
len(X @ (Z ::Y))=s(len(X @ Y))
11.6
len(X @ (W ::Z ::Y))=s(len(X @ (W ::Y)))
6
even(x+x)
even(s(s(X))+Y)=even(X+Y)
5.4
7
odd(s(x)+x)
odd(s(s(X))+Y)=odd(X+Y)
16.0
8
evenm (x+x)
evenm (s(s(X))+Y)=evenm (X+Y)
28.4
oddm (s(s(X))+Y)=oddm (X+Y)
9
oddm (s(x)+x)
evenm (s(s(X))+Y)=evenm (X+Y)
65.5
oddm (s(s(X))+Y)=oddm (X+Y)
10
evenm (x) ! half(x)+half(x)=x
s(X)+Y=s(X+Y)
6.0
s(X)+Y=X+s(Y)
11
half(x+x)=x
s(s(X))+Y=X+s(s(Y))
11.1
half(s(s(X))+Y)=half(X+Y)
12
half(s(x)+x)=x
s(s(X))+Y=X+s(s(Y))
31.0
half(s(s(X))+Y)=half(X+Y)
13
rot(len(x),x)=x
rot(len(X),X @ [Y])=Y::rot(len(X),X)
2.4
14
len(rot(len(x),x))=len(x)
len(rot(X,Z @ [Y]))=s(len(rot(X,Z)))
4.8
15
rot(len(x),x @ [y])=y ::rot(len(x),x)
(X @ [Y])@ Z=X @ (Y ::Z)
86.3
rot(len(X),(X @ [Y])@ Z)=Y ::rot(len(X),X @ Z)
16
len(rev(x))=len(x)
len(X @ [Y])=s(len(X))
2.0
17
rev(rev(x))=x
rev(X @ [Y])=Y::rev(X)
1.2
18
rev(rev(x) @ [y])=y ::x
rev(X @ [Y])=Y::rev(X)
16.0
19
rev(rev(x) @ [y])=y ::rev(rev(x))
rev(X @ [Y])=Y::rev(X)
18.6
20
len(rev(x @ y))=len(x)+len(y)
len(X @ [Y])=s(len(X))
10.0
s(X)+Y=s(X+Y)
s(X)+Y=X+s(Y)
21
len(qrev(x,[]))=len(x)
len(qrev(X,Z ::Y))=s(len(qrev(X,Y)))
2.2
22
qrev(x,y)=rev(x) @ y
(X @ [Y])@ Z=X @ (Y ::Z)
3.4
23
len(qrev(x,y))=len(x)+len(y)
s(X)+Y=s(X+Y)
12.0
s(X)+Y=X+s(Y)
24
qrev(qrev(x,[]),[])=x
qrev(qrev(X,[Y]),Z)=Y ::qrev(qrev(X,[]),Z)
5.0
25
rev(qrev(x,[]))=x
rev(qrev(X,[Y]))=Y ::rev(qrev(X,[]))
5.8
26
qrev(rev(x),[])=x
qrev(X @ [Y],Z)=Y::qrev(X,Z)
5.2
27
nth(i,nth(j,x))=nth(j,nth(i,x))
nth(s(I),nth(J,Y ::X))=nth(I,nth(J,X))
7.4
28 nth(i,nth(j,nth(k,x)))=nth(k,nth(j,nth(i,x)))
nth(s(I),nth(J,Y ::X))=nth(I,nth(J,X))
7.6
29
len(isort(x))=len(x)
len(insert(Y,X))=s(len(X))
2.0
30
sorted(isort(x))
sorted(insert(Y,X))=sorted(X)
114
sorted(insert(Y,insert(Z,X)))=sorted(X)

Table 1: Some lemmas speculated by the divergence critic.
Notes: :: is written for infix cons, @ for infix append, [] for nil, and [x] for cons(x,nil). In
addition, even is defined by a s(s(x)) recursion, evenm by a mutual recursion with oddm ,
and rot(n; l) rotates a list l by n elements.
226

fiA Divergence Critic
The critic is successful at identifying divergence and proposing appropriate lemmas and
generalizations for a significant number of theorems. Divergence analysis is very quick on
most examples. The divergence pattern is recognized usually in less than a second. Most
of the time is spent looking for generalizations and refuting over-generalizations with the
conjecture disprover. This usually takes between 1 and 100 seconds. Additional heuristics
for preventing over-generalization and a more ecient implementation of the conjecture
disprover would speed up the critic considerably.

8. Results

Table 1 lists 30 theorems that cause Spike to diverge and the lemmas speculated by the
divergence critic after analysing the diverging proof attempts. These problems provide a
representative sample of the type of theorems for which the cause of divergence can be
identified and an appropriate lemma or generalization speculated. Many of these problems
come from the Clam library corpus. Part of this table has appeared before (Walsh, 1994).
Times are for the divergence critic to speculate the lemmas and are for the average of 10
runs on a Sun 4 running Quintus 3.1.1.
Spike's proof attempt diverges on each example when given the definitions alone. In
each of the 30 cases, the critic is quickly able to suggest a lemma which overcomes divergence.
When multiple lemmas are proposed (with the exception of 20) any one on its own is
sucient to fix divergence. In every case (except 13 and 24) the lemmas proposed are
suciently simple to be proved automatically without introducing fresh divergence. In the
majority of cases, the lemmas proposed are optimal; that is, they are the simplest possible
lemmas which fix divergence. In the cases when the lemma is not optimal, they are usually
only slightly more complex than the simplest lemma which fixes divergence. In many of the
examples, other lemmas are conjectured by the divergence analysis but these are quickly
rejected by the conjecture disprover. For example, in example 16, divergence analysis and
the petering out heuristic suggest the rule,
## len( app(X; cons(Y; nil)) ) = len(X ) ##
However, this is refuted by exhaustive normalization using any ground terms for X and Y .
In this case, the cancellation heuristic identifies the required lemma,
len( app(X; cons(Y; nil)) ) = s(len(X )) :
Some of the examples deserve additional comment. In example 1, the divergence critic
identifies that successor functions are accumulating on the first argument position of +.
The critic speculates a lemma for moving these successor functions either to the top of the
term (so that immediate cancellation can occur) or onto to the second argument position
(so that simplification with the recursive definition of + can occur). The first lemma
speculated is in fact a generalization of the theorem being proved. Example 2 is a simple
program verification problem taken from Dershowitz and Pinchover (1990). The forward
direction of this theorem was discussed in the introduction. Similar divergence occurs as in
example 1 and, after generalization, the same lemmas are speculated.
Example 3 caused divergence in the beta-version of Spike available in the summer of
1994. The proof rules in Spike have since been strengthened and this example no longer
225

fiWalsh

% compiling file /home/dream5/tw/work/Spike/diverge/data.double.x+x
% data.double.x+x compiled in module user, 0.233 sec 1,612 bytes
| ?- speculate.
Equations input:
double(x1)=x1+x1
s(x1+x1)=s(x1)+x1
s(s(x1+x1))=s(s(x1))+x1
s(s(s(x1+x1)))=s(s(s(x1)))+x1
Lemmas speculated:
s(x1)+x1=s(x1+x1)
s(x1)+x99=s(x1+x99)
s(x99)+x1=s(x99+x1)
s(x99)+x100=s(x99+x100)
s(x1)+x1=x1+s(x1)
s(x1)+x99=x1+s(x99)
s(x99)+x1=x99+s(x1)
s(x99)+x100=x99+s(x100)
Deleting lemmas subsumed:
s(x1)+x99=s(x1+x99)
s(x1)+x99=x1+s(x99)
Merging remaining lemmas:
s(x1)+x99=s(x1+x99)
s(x1)+x99=x1+s(x99)
yes
| ?-

Figure 4: Example output of the divergence critic.
Figure 1 gives the divergence critic's output on the problem discussed in the introduction.
Either of the proposed lemmas when used as a rewrite rule is adequate to fix divergence. In
addition, the proposed lemmas are suciently simple to be proved automatically without
introducing fresh divergence. The first lemma is a rewrite rule for moving accumulating
successor functions from the first argument position of + to the top of the term tree. The
second lemma is a transverse wave rule discussed in Section 6 for moving accumulating
successor functions from the first argument position of + to the second argument position.
224

fiA Divergence Critic

Preconditions:
1. There is a sequence of equations si = ti which the
prover attempts to prove by induction (i = 0, 1 ...);
2. There exists (non trivial) G; H such that for each j ,
the maximal difference match has sj = G(Uj ; Acc) and
sj+1 = G( H (Uj ) ; Acc).
Postconditions:
1. The critic proposes a rule of the form,

G( H (U0) ; Acc) = G(U0; F (Acc) )
2. F is instantiated by the fertilization or simplification
heuristics;
3. The lemma is generalized using the (augmented) primary terms and equality heuristics;
4. Generalized lemmas are filtered through a type checker
and a conjecture disprover;
5. If several lemmas are suggested, the critic deletes any
that are subsumed.
Figure 3: Speculation of transverse wave rules.
annotations. We could also speculate hybrid wave rules which ripple part of the wave-front
across and part of it up the term tree. However, such rules appear to be rare. In addition,
such hybrid wave rules can often be decomposed into a pair of wave rules, one of which
moves some of the wave-fronts up the term tree, and another which moves the wave-fronts
across.

7. Implementation

The divergence critic described in the previous sections has been implemented in Prolog.
The system consists of 787 lines of code defining approximate 100 different Prolog predicates. More recently a cut down version has been incorporated directly within the Spike
system which is written in Caml Light (Bouhoula & Rusinowitch, 1995b). The output
of Spike is parsed to generate input to the critic. The input consists of: the equations
which the prover attempts to prove by induction; sort information (for the type checker
and difference matcher); the recursive argument positions (for constructing primary terms);
and the rewrite rules defining the theory (used by the conjecture disprover).
223

fiWalsh
This rule allows the proof to go through without divergence.
Speculated transverse wave rules are generalized using the extended primary terms
heuristic described in Section 5. The divergence critic also generalizes transverse wave
rules by means of an equality heuristic. This heuristic attempts to cancel equal outermost
functors where possible. For example, consider the theorem,

8x; y : (x + y) , x = y
where addition is defined recursively on its second argument position and subtraction is
defined by the rewrite rules,

X ,0 = X
0,X = 0
s(X) , s(Y) = X , Y:
Spike's attempt to prove this theorem diverges generating (amongst others) the goals,

(x + y ) , x = y
(s(x) + y ) , x = s(y )
(s(s(x)) + y ) , x = s(s(y ))
..
.
Divergence analysis identifies accumulating term structure within these equations,
(x + y ) , x = y
( s(x) + y ) , x = s(y)
( s(s(x)) + y ) , x = s(s(y ))
..
.
This is the unique maximal difference match. These annotations suggest the need for the
transverse rule,
( s(X ) + Y ) , X = (X + s(Y ) ) , X:
The equality heuristic deletes the equal outermost function, z : z , X . This gives the more
general lemma,
s(X ) + Y = X + s(Y ) :
All speculated lemmas are filtered through a type checker to ensure that their erasure
is well typed. Speculated lemmas are also filtered through a conjecture disprover to guard
against over-generalization.
The actions of the critic are summarized in Figure 3. The specification of preconditions
and postconditions again uses second order variables but in a limited manner. The implementation merely requires second order matching and first order difference matching. The
preconditions and postconditions can be easily generalised to include multiple and nested
222

fiA Divergence Critic
qrev(a; b) = app(rev(a); b)
qrev(a; cons(c; b) ) = app( app(rev(a); cons(c; nil)) ; b)
qrev(a; cons(c; cons(d; b)) ) = app( app(app(rev(a); cons(c; nil)); cons(d; nil)) ; b)
..
.

This is the unique maximal difference match. Rather than move the accumulating term
structure on the right hand side of the equations to the top of the term, it is much simpler
to move the accumulating term structure from the first onto the second argument of the
outermost append. The critic therefore proposes a transverse wave rule, which preserves
the skeleton but moves the difference onto a different argument position. In this example,
this is a rule of the form,

app( app(rev(A); cons(C; nil)) ; B) = app(rev(A); F (B) ):
In moving the difference onto another argument position, the difference may change syntactically. The right hand side of the lemma is therefore only partially determined. To
instantiate F , the critic uses two heuristics: fertilization and simplification.
The fertilization heuristic uses matching to find an instantiation for F which enables
immediate fertilization. In this case, matching against the universally quantified variable b
in the induction hypothesis suggests,

app( app(rev(A); cons(C; nil)) ; B) = app(rev(A); cons(C; B) ):
Finally the critic generalizes the lemma using the same extended primary term heuristic as
before (i.e., augmenting recursive positions with wave-hole positions). This gives the rule,

app( app(A; cons(C; nil)) ; B) = app(A; cons(C; B) ):
This is exactly the rule needed by Spike to complete the proof. In addition, it is simple
enough to be proved by itself without divergence; this is not true of the ungeneralized rule.
The other heuristic used to instantiate the right hand side of the speculated lemma is
the simplification heuristic. The heuristic uses regular matching to find an instantiation for
F which will enable the wave-front to be simplified using one of the recursive definitions.
Consider again the dbl theorem from the introduction. Divergence analysis identifies successor functions accumulating on the first argument position of +. This accumulating term
structure can either be moved to the top of the term tree or alternatively onto the second
argument position of + using a transverse wave rule of the form,

s(X ) + Y = X + F (Y ) :
The right hand side of this transverse wave rule is instantiated by the simplification heuristic.
The wave-front on the right hand side can be simplified by the rewrite rule recursively
defining + if F is instantiated by z : s(z ). That is, if we have the rule,

s(X ) + Y = X + s(Y ) :
221

fiWalsh
As f0; s(Y)g is a cover set for the natural numbers, these two rules can be merged to give,
sorted( insert(Y; X) ) = sorted(X):

6. Transverse Wave Rules
The lemmas speculated so far have moved accumulating term structure directly to the top
of the term where it is removed by cancellation or petering out. An alternative way of
removing accumulating term structure is to move it onto another argument position where:
either it can be removed by matching with a \sink", a universally quantified variable in
the induction hypothesis; or it can be moved upwards by rewriting with the recursive
definitions. Annotated rewrite rules which preserve the skeleton and move wave-fronts
across to other argument positions are called transverse wave rules (Bundy et al., 1993).
Theorems involving functions with accumulators provide a rich source of examples where
such rewrite rules prevent divergence.
Consider, for example, a theorem about the correctness of tail recursive list reversal,

8a; b : qrev(a; b) = app(rev(a); b)
where both a and b are universally quantified, rev is naive list reversal using append, and
qrev is tail recursive list reversal building the reversed list on the second argument position.
These functions are defined by the rewrite rules,
rev(nil)
rev(cons(H; T))
qrev(nil; R)
qrev(cons(H; T); R)

=
=
=
=

nil
app(rev(T); cons(H; nil))
R
qrev(T; cons(H; R)):

Spike's attempt to prove this theorem diverges generating the following sequence of equa-

tions which the prover attempts to show by induction,

qrev(a; b) = app(rev(a); b)
qrev(a; cons(c; b)) = app(app(rev(a); cons(c; nil)); b)
qrev(a; cons(c; cons(d; b))) = app(app(app(rev(a); cons(c; nil)); cons(d; nil)); b)
..
.

Difference matching identifies the term structure accumulating within these equations that
is causing divergence,
220

fiA Divergence Critic
s(0) + len(b) = s(len(b))
s(s(0)) + len(b) = s(s(len(b)))

..
.
Difference matching identifies the term structure causing divergence,
0 + len(b) = len(b)
s(0) + len(b) = s(len(b))

s(s(0)) + len(b) = s(s(len(b)))

..
.
This is the unique maximal difference match. These annotations suggest the need for the
wave rule,
s(0) + len(B) = s(0 + len(B)) :
A set of candidate terms for generalization is constructed by computing the intersection of
the primary terms of the two sides of this rule. In this case, the primary terms of the left
hand side are the set fs(0) + len(B); len(B); Bg, and the primary terms of the right hand
side are the set fs(0 + len(B)); 0 + len(B); len(B); Bg. The intersection of the primary terms
is thus the set flen(B); Bg. The critic picks members of the intersection to generalize to new
variables. Picking B justs gives an equivalent lemma up to renaming of variables. Picking
len(B) gives the generalization,
s(0) + Y = s(0 + Y) :
The reason for considering just primary terms is that the recursive definitions typically
provide wave rules for removing term structure which accumulates at these positions. In
addition to primary terms, the divergence critic therefore also considers the positions of
the wave-holes (but not wave-fronts) in the skeleton of the lemma being speculated. The
motivation for this extension is that the speculated lemma will allow accumulating term
structure to be moved from the wave-hole positions; such positions are therefore also candidates for generalization. Positions of the wave-fronts are not included since we want to
speculate a lemma that will move the term structure at such positions.
For instance, because of the wave-hole on the first argument of + in the last example,
0 is also included in the intersection set of candidate terms for generalization. Picking 0 to
generalize gives,
s(X) + Y = s(X + Y) :
The speculated lemma is now as general as is possible. This rule allows the proof to go
through without divergence.
The critic also has a heuristic for merging speculated lemmas. For instance, with the
theorem sorted(isort(x)), the critic speculates several rules including,
sorted( insert(0; X) ) = sorted(X)
sorted( insert(s(Y); X) ) = sorted(X)
219

fiWalsh

1. The critic proposes a rule of the form,

G( H (U0) ) = F (G(U0))
2. F is instantiated by the cancellation or petering out
heuristics;
3. Lemmas are filtered through a type checker and a conjecture disprover;
4. If several lemmas are suggested, the critic deletes any
that are subsumed.
Figure 2: Postconditions to the divergence critic

5. Generalization

A major cause of divergence is the need to generalize. Most of the lemmas proposed
by the critic fix divergence, but attempting to prove the lemmas themselves can cause
fresh divergence. In addition, several speculated lemmas can sometimes be replaced by
a single generalization. Generalized lemmas also can lead to shorter, more elegant and
natural proofs. The critic therefore attempts to generalize the lemma speculated, using the
conjecture disprover to guard against over-generalization.
The main heuristic used for generalization is an extension of the primary term heuristic
(Aubin, 1976). The primary terms are those terms encountered as a term is explored from
the root to the leaves ignoring non-recursive argument positions to functions. The same
notion of recursive argument position is used by the critic as defined by Bouhoula and
Rusinowitch (1995a) and as used by Spike for performing inductions.
Consider, for example, the theorem,

8a; b : len(a) + len(b) = len(app(a; b))
where + is again defined recursively on its second argument, and len and app are defined
by means of the rewrite rules,
len(nil)
len(cons(H; T))
app(nil; T)
app(cons(H; T); R)

=
=
=
=

0
s(len(T))
T
cons(H; app(T; R)):

This problem is taken from the Clam library corpus (Bundy et al., 1990). Spike's attempt
to prove this theorem diverges. One of the sequences of equations generated is,
0 + len(b) = len(b)
218

fiA Divergence Critic
Spike's diverging attempt to prove this theorem generates the equations,

nth(s(i); nth(j; x)) = nth(s(j ); nth(i; x))
nth(s(s(i)); nth(j; cons(y; x))) = nth(s(j ); nth(i; x))
nth(s(s(s(i))); nth(j; cons(z; cons(y; x)))) = nth(s(j ); nth(i; x))
..
.

Divergence analysis identifies term structure accumulating in two different places,

nth(s(i); nth(j; x)) = nth(s(j ); nth(i; x))
nth( s(s(i)) ; nth(j; cons(y; x) )) = nth(s(j ); nth(i; x))
nth( s(s(s(i))) ; nth(j; cons(z; cons(y; x)) )) = nth(s(j ); nth(i; x))
..
.

This is the unique maximal difference match. This divergence pattern suggests the need for
a rewrite rule of the form,

nth( s(I ) ; nth(J; cons(Y; X ) )) = F (nth(I; nth(J; X ))) :
The petering out heuristic instantiates F to the identity function z : z giving the rule,

nth( s(I ) ; nth(J; cons(Y; X) )) = nth(I; nth(J; X )):
This rule allows the proof to go through without divergence.
Since the erasure of the wave rule must be properly typed, sort information can be used
to prune inappropriate instantiations for F . All speculated lemmas are therefore filtered
through a type checker. Speculated lemmas are also filtered through a conjecture disprover.
When a conuent set of rewrite rules exists for ground terms, exhaustive normalization
of some representative set of ground instances of the equations is used to filter out nontheorems. Alternatively, the prover itself could be used to filter out non-theorems. Unlike
many other induction theorem provers, Spike can refute conjectures since its inference
rules are refutationally complete for conditional theories in which the axioms are ground
convergent and defined functions are completely defined over free constructors (Bouhoula &
Rusinowitch, 1995a). Other techniques for disproving conjectures are described by Protzen
(1992).
The critic's lemma speculation is summarized in Figure 2 (using the same variable
names as the preconditions). This specification again uses second order variables in a
limited manner. First order difference matching is merely required to construct lemmas.
As with the preconditions, the specification of the postconditions can be easily extended
to deal with multiple and nested wave-fronts (as in the nth(i; nth(j; l)) = nth(j; nth(i; l))
example). Since the rules proposed by the critic move the wave-fronts to top of the term,
they usually only introduce fresh divergence in the rare cases that cancellation or fertilization
fails. This is unlikely since the cancellation and petering out heuristics attempt to ensure
precisely that cancellation or fertilization can take place.
217

fiWalsh
This divergence pattern suggests that F should be instantiated to z : s(z) to enable immediate cancellation. Thus, as required, the cancellation heuristic suggests the rule,
s(X ) + Y = s(X + Y ) :
The other heuristic used to instantiate the right hand side of speculated lemmas is
petering out. In moving the differences up to the top of the term, they may disappear
altogether. Consider, for example, the theorem,
8l : sorted(isort(l)) = true
where isort is insertion sort and sorted is true iff a list is sorted into order. These are defined
by the conditional rewrite rules,
sorted(nil) = true
sorted(cons(X; nil)) = true
X < Y ! sorted(cons(X; cons(Y; Z ))) = sorted(cons(Y; Z ))
isort(nil) = nil
isort(cons(X; Y )) = insert(X; isort(Y ))
where insert(X; Z ), which inserts the element X into the list Z in order, and X < Y are
defined by the rewrite rules,
0 < X = true
s(X ) < 0 = false
s(X ) < s(Y ) = X < Y
insert(X; nil) = cons(X; nil)
X < Y ! insert(X; cons(Y; Z )) = cons(X; cons(Y; Z ))
:(X < Y ) ! insert(X; cons(Y; Z )) = cons(Y; insert(X; Z ))
Divergence analysis of Spike's attempt to prove this theorem suggests the need for a
rule of the form,
sorted( insert(Y; X) ) = F (sorted(X) :
The petering out heuristic instantiates F to the identity function z : z. This gives the rule,
sorted( insert(Y; X) ) = sorted(X):
This rule allows the proof to go through without divergence.
As a more complex example, consider the theorem,
8i; j; l : nth(i; nth(j; l)) = nth(j; nth(i; l))
where nth is defined by the rewrite rules,
nth(0; L) = L
nth(N; nil) = nil
nth(s(N ); cons(H; T )) = nth(N; T ):
216

fiA Divergence Critic
nested annotations. This allows the critic to recognise multiple sources of divergence in the
same equation. Techniques which identify accumulating term structure by most specific
generalization (Dershowitz & Pinchover, 1990) cannot cope with divergence patterns that
give rise to nested annotations (see Section 9 for more details).
The specification of the preconditions has left the length of sequence undefined. If the
sequence is of length 2, then the critic is preemptive. That is, it will propose a lemma just
before another induction is attempted and divergence begins. Such a short sequence risks
identifying divergence when none exists. On the other hand using a long sequence is expensive to test and allows the prover to waste time on diverging proof attempts. Empirically,
a good compromise appears to be to look for sequences of length 3. This is both cheap
to test and reliable. To identify accumulating term structure, it appears to be sucient
to use ground difference matching with alpha conversion of variable names. There exists
a fast polynomial algorithm to perform such difference matching based upon the ground
difference matching algorithm using dynamic programming (Basin & Walsh, 1993). Since
the skeleton must be well typed (along with the erasure), the algorithm is extended to use
sort information to prune potential difference matches.

4. Lemma Speculation
One way of removing the accumulating and nested term structure is to propose a wave rule
which moves this difference to the top of the term leaving the skeleton unchanged. We hope
either that it will then cancel against wave-fronts on the other side of the equality or that
it will disappear in the process of being moved. For the dbl theorem, after generalization
(which is discussed in the next section) the divergence pattern suggests a rule of the form,

s(X ) + Y = F (X + Y )
where F is a second order variable which we need to instantiate. Instantiating F is ultimately a dicult synthesis problem so we can only hope to have heuristics that will work
some of the time. Two of the heuristics used by the divergence critic to instantiate F are
cancellation and petering out.
The cancellation heuristic uses difference matching to identify term structure accumulating on the opposite side of the sequence which would allow cancellation to occur. Failing
that, the cancellation heuristic looks for suitable term structure to cancel against in a new
sequence (the original sequence is usually a divergence pattern of a step case, whilst the
new sequence is usually a divergence pattern of a base case). In the dbl example, successor
functions accumulate at the top of the left hand side of the diverging equations,

s(x + x) = s(x) + x
s(s(x + x)) = s(s(x)) + x
s(s(s(x + x))) = s(s(s(x))) + x
..
.

215

fiWalsh
The critic then attempts to find the accumulating and nested term structure in each
sequence which is causing divergence. In this case, successor functions are accumulating
on the first argument of +. To identify this accumulating term structure, the critic uses
difference matching. Difference matching successive equations gives the annotated sequence,

s(x + x) = s(x) + x
s(s(x + x)) = s(s(x)) + x
s(s(s(x + x))) = s(s(s(x))) + x
..
.

This is the unique maximal difference match.
The critic then tries to speculate a lemma which can be used as a rewrite rule to move
the accumulating and nested term structure out of the way. In this case, the critic speculates
a rule for moving a successor function off the first argument of +. That is, the rule,

s(X ) + Y = s(X + Y ) :
With this rule, Spike is able to prove the dbl theorem without divergence. In addition, this
rule is suciently simple that it can be proved without assistance. The heuristics used by
the critic to perform this lemma speculation are described in more detail in the next two
sections.
The divergence analysis performed by the critic is summarised in Figure 1. In analysing
1. There is a sequence of equations si = ti which the
prover attempts to prove by induction (i = 0, 1 ...);
2. There exists (non trivial) G; H such that for each j , the
maximal difference match has sj = G(Uj ), and sj +1 =
G( H (Uj ) ).
Figure 1: Preconditions to the divergence critic
the divergence, we consider all the equations which the prover attempts to prove by induction. This includes those equation where the induction proof succeeds as these can often
suggest useful patterns. By \non-trivial" I wish to exclude z : z, the identity substitution. H is thus the accumulating and nested term structure that appears to be causing
divergence. For the dbl example, H is z : s(z), G is z : z + x, and U0 is s(x). Although
G and H are second order variables, the second order nature of the divergence analysis
is limited. Indeed, the implementation of the critic merely requires first order difference
matching which is polynomial. For simplicity, the preconditions ignore the orientation of
equations. In addition, the preconditions can be easily generalised to include multiple and
214

fiA Divergence Critic
Rippling has several desirable properties. It is highly goal directed, manipulating just
the differences between the induction hypothesis and the induction conclusion. As the
annotations restrict the application of the rewrite rules, rippling also involves little or no
search. Difference matching and rippling have proved useful in domains outside of explicit
induction. For example, they have been used to sum series (Walsh, Nunes, & Bundy, 1992)
and to prove limit theorems (Yoshida, Bundy, Green, Walsh, & Basin, 1994). In the rest
of the paper, I show that difference matching and rippling are also useful in identifying
and correcting divergence in a prover that neither uses explicit rules of induction nor uses
annotations to control rewriting.

3. Divergence Analysis

The initial problem is recognizing when the proof is diverging. Various properties of rewrite
rules have been identified which cause divergence like, for example, forwards and backwards
crossed systems (Hermann, 1989). However, these properties fail to capture all diverging
rewrite systems since the problem is, in general, undecidable. The divergence critic instead
studies the proof attempt looking for patterns of divergence; no attempt is made to analyse
the rewrite rules themselves for structures which give rise to divergence. The advantage of
this approach is that the critic need not know the details of the rewrite rules applied, nor the
type of induction being performed, nor the control structure used by the prover. The critic
can thus recognise divergence patterns arising from complex mutual or multiple inductions
with little more diculty than divergence patterns arising from simple straightforward
inductions. The disadvantage of this approach is that the critic can identify a \divergence"
pattern when none exists. Fortunately, such cases appear to be rare, and even when they
occur, the critic usually suggests a lemma or generalization which gives a shorter and more
elegant proof (see Section 8 for an example).
To illustrate the ideas behind the critic's divergence analysis, consider again the theorem
from the introduction,
8n : dbl(n) = n + n:
The divergence critic first partitions the sequence of equations which the prover attempts to
prove by induction. This is necessary since several diverging sequences may be interleaved
in the prover's output. Several heuristics can be used to reduce the number of partitions
considered. The most useful heuristic is parentage in which the sequence is partitioned
so that each equation is derived from the previous one. That is, the equations lie on a
single branch of the proof tree. In particular, the base case and step case of an induction
are partitioned into different sequences. Other heuristics which can be used include: the
function and constant symbols which occur in one equation occur in the next equation
in the partition, and the weights of the equations in a partition form a simple arithmetic
progression. In this case, there is just a single open branch in the proof tree,
s(x + x) = s(x) + x
s(s(x + x)) = s(s(x)) + x
s(s(s(x + x))) = s(s(s(x))) + x
..
.
213

fiWalsh
annotated term r. Difference matching is not unitary. That is, two terms can have more
than one difference match. For example, both s(s(x)) and s( s(x) ) are difference matches
of s(s(x)) with s(x). The number of difference matches can be reduced if we compute just
the maximal difference match in which wave-fronts are as high as possible in the term tree.
A formal definition of such a well founded ordering on annotated terms has been given by
Basin and Walsh (1994).
The aim of rippling is to rewrite the annotated induction conclusion so that the skeleton,
the induction hypothesis, is preserved and the differences, the wave-fronts are moved to
harmless places (for example, to the top of the term). If this rewriting succeeds, we will
then be able to appeal to the induction hypothesis. To rewrite the annotated induction
conclusion, we use the following annotated rewrite rules, or wave rules:

dbl( s(X ) ) = s(s(dbl(X )))
X + s(Y ) = s(X + Y )
s(X ) + Y = s(X + Y )

(1)
(2)
(3)

The first two of these annotated rewrite rules are derived from the recursive definitions of dbl
and + whilst the second is derived from the lemma proposed at the end of the introduction.
Each of these annotated rewrite rules preserves the skeleton of the term being rewritten,
and moves the wave-fronts higher up the term tree. Wave rules guarantee this: a wave rule
is an annotated rewrite rule with an identical skeleton on left and right hand sides that
moves wave-fronts in a well founded direction like, for instance, to the top of the term tree
(Basin & Walsh, 1994).
Rippling on the left hand side of the annotated induction conclusion using (1) yields,

s(s(dbl(x))) = s(x) + s(x) :
Then rippling on the right hand side with (2) gives,

s(s(dbl(x))) = s( s(x) + x) :
Finally rippling with (3) on the right hand side yields,

s(s(dbl(x))) = s(s(x + x)) :
As the wave-fronts are at the top of each term, we have successfully rippled both sides of
the equality. We can now appeal to the induction hypothesis on the left hand side giving,

s(s(x + x)) = s(s(x + x)) :
This is a simple identity and the proof is complete. Note that to complete the proof, we
needed to rewrite with a lemma, (3). The aim of the divergence critic described in this
paper is to propose such lemmas.
212

fiA Divergence Critic
In Section 2, I describe difference matching and rippling, the two key ideas at the heart
of the divergence critic. I then outline how difference matching identifies the accumulating
term structure which is causing divergence (Section 3). In Section 4 and 6, I show how
lemmas are speculated which \ripple" this term structure out of the way. In Section 5,
I describe the heuristics used in generalizing these lemmas. Finally, implementation and
results are described in Sections 7 and 8.

2. Difference matching and rippling

Rippling is a powerful heuristic developed at Edinburgh for proving theorems involving
explicit induction (Bundy, Stevens, van Harmelen, Ireland, & Smaill, 1993) and is implemented in the Clam theorem prover (Bundy, van Harmelen, Horn, & Smaill, 1990). In the
step case of an inductive proof, the induction conclusion typically differs from the induction
hypothesis by the addition of some constructors or destructors. Rippling uses annotations
to mark these differences and applies annotated rewrite rules to remove them.
As a simple example, consider again the theorem discussed in the introduction. In the
step case, the induction hypothesis is,

dbl(x) = x + x
And the induction conclusion is,

dbl(s(x)) = s(x) + s(x):
If we \difference match" the induction conclusion against the induction hypothesis (Basin
& Walsh, 1992), we obtain the following annotated induction conclusion,

dbl( s(x) ) = s(x) + s(x) :
An annotation consists of a wave-front, a box with a wave-hole, an underlined term. Wavefronts are always one functor thick (Basin & Walsh, 1994). That is, every wave-front has
one immediate subterm that is annotated with a wave-hole. To make presentation simpler,
we display adjacent wave-fronts merged. Thus, s(s(x)) is just syntactic sugar for the
annotated term, s( s(x) ) . Wave-fronts can also include up and down arrows to indicate
whether they are moving towards the top of the term tree or down towards the leaves. This
extension can, however, be safely ignored here.
The skeleton of an annotated term is formed by deleting everything that appears in
the wave-front but not in the wave-hole. The erasure of an annotated terms is formed by
deleting the annotations but not the terms they contain. In this case, the skeleton of the
annotated induction conclusion is identical to the induction hypothesis, and the erasure
of the annotated induction conclusion is the unannotated induction conclusion. Difference
matching guarantees this; that is, difference matching the induction conclusion with the
induction hypothesis annotates the induction conclusion so that its skeleton matches the
induction hypothesis.
Formally, r is a difference match of s with t with substitution  iff  (skeleton(r)) = t
and erase(r) = s where skeleton(r) and erase(r) build the skeleton and erasure of the
211

fiWalsh
both alpha convert variable names where necessary. Rewriting the induction conclusion
with the recursive definitions of dbl and + gives,

s(s(dbl(x))) = s(s(x) + x):
The outermost successor functions on either side of the equality are now cancelled,

s(dbl(x)) = s(x) + x:
The prover then fertilizes with the induction hypothesis on the left hand side,

s(x + x) = s(x) + x:
This equation cannot be simplified further so another induction is performed. Unfortunately,
this generates the diverging sequence of subgoals,

s(x + x)
s(s(x + x))
s(s(s(x + x)))
s(s(s(s(x + x))))
s(s(s(s(s(x + x)))))

=
=
=
=
=
..
.

s(x) + x
s(s(x)) + x
s(s(s(x))) + x
s(s(s(s(x)))) + x
s(s(s(s(s(x))))) + x

The problem is that the prover repeatedly tries an induction on x but is unable to simplify
the successor functions that this introduces on the first argument position of +. The proof
will go through without divergence if we have the rewrite rule,

s(X ) + Y = s(X + Y ):
This rule \ripples" accumulating successor functions off the first argument position of +.
This rewrite rule is derived from the lemma,

8x; y : s(x) + y = s(x + y):
This is the commuted version of the recursive definition of addition and is, coincidently, a
generalization of the first subgoal. This lemma can be proved without divergence as the
induction variable, y occurs just in the second argument position of +.
In this paper I describe a simple \divergence critic", a computer program which attempts
to automate this process. The divergence critic identifies when a proof attempt is diverging
by means of a \difference matching" procedure. The critic then proposes lemmas and
generalizations which hopefully allow the proof to go through without divergence. Although
the critic is designed to work with the prover Spike, it should also work with other induction
provers (Walsh, 1994). Spike is a rewrite based theorem prover for first order conditional
theories. It contains powerful rules for case analysis, simplification and implicit induction
using the notion of a test set. Unfortunately, as is the case with other inductive theorem
provers, its attempts to prove many theorems diverge without an appropriate generalization
or the addition of a suitable lemma.
210

fiJournal of Artificial Intelligence Research 4 (1996) 209-235

Submitted 1/96; published 4/96

A Divergence Critic for Inductive Proof
toby@itc.it

Toby Walsh

IRST, Location Pante di Povo
I38100 Trento, ITALY

Abstract

Inductive theorem provers often diverge. This paper describes a simple critic, a computer program which monitors the construction of inductive proofs attempting to identify
diverging proof attempts. Divergence is recognized by means of a \difference matching"
procedure. The critic then proposes lemmas and generalizations which \ripple" these differences away so that the proof can go through without divergence. The critic enables the
theorem prover Spike to prove many theorems completely automatically from the definitions alone.

1. Introduction

Two key problems in inductive theorem proving are proposing lemmas and generalizations.
A prover's divergence often suggests to the user an appropriate lemma or generalization
that will enable the proof to go through without divergence. As a simple example, consider
the theorem,

8n : dbl(n) = n + n:
This is part of a simple program verification problem (Dershowitz & Pinchover, 1990).
Addition and doubling are defined recursively by means of the rewrite rules,

X+0 = X
X + s(Y ) = s(X + Y )
dbl(0) = 0
dbl(s(X )) = s(s(dbl(X )))
where s(X ) represents the successor of X (that is, X + 1). I have adopted the Prolog
convention of writing meta-variables like X and Y in upper case.

The theorem prover Spike (Bouhoula, Kounalis, & Rusinowitch, 1992) fails to prove
this theorem. The proof attempt begins with a simple one step induction on x. The base
case is trivial. In the step case, the induction hypothesis is,
dbl(x) = x + x
And the induction conclusion is,

dbl(s(x)) = s(x) + s(x):
To ease presentation, variables in this paper are, as here, sometimes renamed from those
introduced by Spike. This has no effect on the results as the prover and divergence critic

c 1996 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiJournal of Artificial Intelligence Research 4 (1996) 129-145

Submitted 11/95; published 3/96

Active Learning with Statistical Models
David A. Cohn
Zoubin Ghahramani
Michael I. Jordan

Center for Biological and Computational Learning
Dept. of Brain and Cognitive Sciences
Massachusetts Institute of Technology
Cambridge, MA 02139 USA

cohn@harlequin.com
zoubin@cs.toronto.edu
jordan@psyche.mit.edu

Abstract

For many types of machine learning algorithms, one can compute the statistically \optimal" way to select training data. In this paper, we review how optimal data selection
techniques have been used with feedforward neural networks. We then show how the same
principles may be used to select data for two alternative, statistically-based learning architectures: mixtures of Gaussians and locally weighted regression. While the techniques
for neural networks are computationally expensive and approximate, the techniques for
mixtures of Gaussians and locally weighted regression are both ecient and accurate. Empirically, we observe that the optimality criterion sharply decreases the number of training
examples the learner needs in order to achieve good performance.

1. Introduction
The goal of machine learning is to create systems that can improve their performance at
some task as they acquire experience or data. In many natural learning tasks, this experience
or data is gained interactively, by taking actions, making queries, or doing experiments.
Most machine learning research, however, treats the learner as a passive recipient of data
to be processed. This \passive" approach ignores the fact that, in many situations, the
learner's most powerful tool is its ability to act, to gather data, and to inuence the world
it is trying to understand. Active learning is the study of how to use this ability effectively.
Formally, active learning studies the closed-loop phenomenon of a learner selecting actions or making queries that inuence what data are added to its training set. Examples
include selecting joint angles or torques to learn the kinematics or dynamics of a robot
arm, selecting locations for sensor measurements to identify and locate buried hazardous
wastes, or querying a human expert to classify an unknown word in a natural language
understanding problem.
When actions/queries are selected properly, the data requirements for some problems
decrease drastically, and some NP-complete learning problems become polynomial in computation time (Angluin, 1988; Baum & Lang, 1991). In practice, active learning offers its
greatest rewards in situations where data are expensive or dicult to obtain, or when the
environment is complex or dangerous. In industrial settings each training point may take
days to gather and cost thousands of dollars; a method for optimally selecting these points
could offer enormous savings in time and money.
c 1996 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiCohn, Ghahramani & Jordan

There are a number of different goals which one may wish to achieve using active learning. One is optimization, where the learner performs experiments to find a set of inputs
that maximize some response variable. An example of the optimization problem would be
finding the operating parameters that maximize the output of a steel mill or candy factory.
There is an extensive literature on optimization, examining both cases where the learner
has some prior knowledge of the parameterized functional form and cases where the learner
has no such knowledge; the latter case is generally of greater interest to machine learning
practitioners. The favored technique for this kind of optimization is usually a form of response surface methodology (Box & Draper, 1987), which performs experiments that guide
hill-climbing through the input space.
A related problem exists in the field of adaptive control, where one must learn a control
policy by taking actions. In control problems, one faces the complication that the value of
a specific action may not be known until many time steps after it is taken. Also, in control
(as in optimization), one is usually concerned with the performing well during the learning
task and must trade of exploitation of the current policy for exploration which may improve
it. The subfield of dual control (Fe'ldbaum, 1965) is specifically concerned with finding an
optimal balance of exploration and control while learning.
In this paper, we will restrict ourselves to examining the problem of supervised learning:
based on a set of potentially noisy training examples D = f(xi; yi )gmi=1, where xi 2 X and
yi 2 Y , we wish to learn a general mapping X ! Y . In robot control, the mapping may be
state  action ! new state; in hazard location it may be sensor reading ! target position.
In contrast to the goals of optimization and control, the goal of supervised learning is to be
able to eciently and accurately predict y for a given x.
In active learning situations, the learner itself is responsible for acquiring the training
set. Here, we assume it can iteratively select a new input x~ (possibly from a constrained
set), observe the resulting output y~, and incorporate the new example (~x; y~) into its training
set. This contrasts with related work by Plutowski and White (1993), which is concerned
with filtering an existing data set. In our case, x~ may be thought of as a query, experiment,
or action, depending on the research field and problem domain. The question we will be
concerned with is how to choose which x~ to try next.
There are many heuristics for choosing x~, including choosing places where we don't have
data (Whitehead, 1991), where we perform poorly (Linden & Weber, 1993), where we have
low confidence (Thrun & Moller, 1992), where we expect it to change our model (Cohn,
Atlas, & Ladner, 1990, 1994), and where we previously found data that resulted in learning
(Schmidhuber & Storck, 1993). In this paper we will consider how one may select x~ in a
statistically \optimal" manner for some classes of machine learning algorithms. We first
briey review how the statistical approach can be applied to neural networks, as described
in earlier work (MacKay, 1992; Cohn, 1994). Then, in Sections 3 and 4 we consider two
alternative, statistically-based learning architectures: mixtures of Gaussians and locally
weighted regression. Section 5 presents the empirical results of applying statistically-based
active learning to these architectures. While optimal data selection for a neural network
is computationally expensive and approximate, we find that optimal data selection for the
two statistical models is ecient and accurate.
130

fiActive Learning with Statistical Models

2. Active Learning { A Statistical Approach

We begin by defining P (x; y ) to be the unknown joint distribution over x and y , and P (x)
to be the known marginal distribution of x (commonly called the input distribution). We
denote the learner's output on input x, given training set D as y^(x; D).1 We can then write
the expected error of the learner as follows:
Z

h

x

i

ET (^y (x; D) , y (x))2 jx P (x)dx;

(1)

where ET [] denotes expectation over P (y jx) and over training sets D. The expectation
inside the integral may be decomposed as follows (Geman, Bienenstock, & Doursat, 1992):
h

i

h

i

ET (^y (x; D) , y(x))2 jx = E (y (x) , E [y jx])2
+ (EDh [^y (x; D)] , E [y jx])2 i
+ED (^y (x; D) , ED [^y(x; D)])2

(2)

where ED [] denotes the expectation over training sets D and the remaining expectations
on the right-hand side are expectations with respect to the conditional density P (y jx). It is
important to remember here that in the case of active learning, the distribution of D may
differ substantially from the joint distribution P (x; y ).
The first term in Equation 2 is the variance of y given x | it is the noise in the
distribution, and does not depend on the learner or on the training data. The second term
is the learner's squared bias, and the third is its variance; these last two terms comprise the
mean squared error of the learner with respect to the regression function E [y jx]. When the
second term of Equation 2 is zero, we say that the learner is unbiased. We shall assume
that the learners considered in this paper are approximately unbiased; that is, that their
squared bias is negligible when compared with their overall mean squared error. Thus we
focus on algorithms that minimize the learner's error by minimizing its variance:
h

i

y2^  y2^(x) = ED (^y (x; D) , ED [^y (x; D)])2 :

(3)

(For readability, we will drop the explicit dependence on x and D | unless denoted otherwise, y^ and y2^ are functions of x and D.) In an active learning setting, we will have chosen
the x-component of our training set D; we indicate this by rewriting Equation 3 as
D

E

y2^ = (^y , hy^i)2 ;
where hi denotes ED [] given a fixed x-component of D. When a new input x~ is selected
and queried, and the resulting (~x; y~) added to the training set, y2^ should change. We will
denote the expectation (over values of y~) of the learner's new variance as
D

E

h

i

~y2^ = ED[(~x;y~) y2^jx~ :

(4)

1. We present our equations in the univariate setting. All results in the paper apply equally to the multivariate case.

131

fiCohn, Ghahramani & Jordan

2.1 Selecting Data to Minimize Learner Variance

In this paper we consider algorithms for active learning which select data in an attempt to
minimize the value of Equation 4, integrated over X . Intuitively, the minimization proceeds
as follows: we assume that we have an estimate of y2^, the variance of the learner at x. If,
for some new input x~, we knew the conditional distribution P (~y jx~), we could compute an
estimate of the learner's new variance at x given an additional example at x~. While the
true distribution P (~y jx~) is unknown, many learning architectures let us approximate it by
giving us Destimates
of its mean and variance. Using the estimated distribution of y~, we can
E
2
estimate ~y^ , the expected variance of the learner after querying at x~.
D E
Given the estimate of ~y2^ , which applies to a given x and a given query x~, we must
integrate x over the input distribution to compute the integrated average variance of the
learner.D In Epractice, we will compute a Monte Carlo approximation of this integral, evaluating ~y2^ at a number of reference points drawn according to P (x). By querying an
x~ that minimizes the average expected variance over the reference points, we have a solid
statistical basis for choosing new examples.

2.2 Example: Active Learning with a Neural Network

In this section we review the use of techniques from Optimal Experiment Design (OED) to
minimize the estimated variance of a neural network (Fedorov, 1972; MacKay, 1992; Cohn,
1994). We will assume we have been given a learner y^ = fw^ (), a training set D = f(xi ; yi)gmi=1
and a parameter vector estimate w^ that maximizes some likelihood measure given D. If, for
example, one assumes that the data were produced by a process whose structure matches
that of the network, and that noise in the process outputs is normal and independently
identically distributed, then the negative log likelihood of w^ given D is proportional to

S 2 = m1

m
X
i=1

(yi , y^(xi ))2 :

The maximum likelihood estimate for w^ is that which minimizes S 2.
The estimated output variance of the network is

y2^  S 2



!
@ y^(x) T @ 2 S 2 ,1  @ y^(x)  ; (MacKay, 1992)
@w
@w2
@w

where the true variance is approximated by a second-order Taylor series expansion around

S 2. This estimate makes the assumption that @ y^=@w is locally linear. Combined with the
assumption that P (yDjx)Eis Gaussian with constant variance for all x, one can derive a closed
form expression for ~y2^ . See Cohn (1994) for details.
In practice, @ y^=@w may be highly nonlinear, and P (y jx) may be far from Gaussian; in

spite of this, empirical results show that it works well on some problems (Cohn, 1994). It
has the advantage of being grounded in statistics, and is optimal given the assumptions.
Furthermore, the expectation is differentiable with respect to x~. As such, it is applicable
in continuous domains with continuous action spaces, and allows hillclimbing to find the x~
that minimizes the expected model variance.
132

fiActive Learning with Statistical Models

For neural networks, however, this approach has many disadvantages. In addition to
relying on simplifications and assumptions which hold only approximately, the process is
computationally expensive. Computing the variance estimate requires inversion of a jwjjwj
matrix for each new example, and incorporating new examples into the network requires
expensive retraining. Paass and Kindermann (1995) discuss a Markov-chain based sampling
approach which addresses some of these problems. In the rest of this paper, we consider
two \non-neural" machine learning architectures that are much more amenable to optimal
data selection.

3. Mixtures of Gaussians
The mixture of Gaussians model is a powerful estimation and prediction technique with
roots in the statistics literature (Titterington, Smith, & Makov, 1985); it has, over the last
few years, been adopted by researchers in machine learning (Cheeseman et al., 1988; Nowlan,
1991; Specht, 1991; Ghahramani & Jordan, 1994). The model assumes that the data are
produced by a mixture of N multivariate Gaussians gi, for i = 1; :::; N (see Figure 1).
In the context of learning from random examples, one begins by producing a joint density
estimate over the input/output space X  Y based on the training set D. The EM algorithm
(Dempster, Laird, & Rubin, 1977) can be used to eciently find a locally optimal fit of the
Gaussians to the data. It is then straightforward to compute y^ given x by conditioning the
joint distribution on x and taking the expected value.

y1

o

y

2

o

o

o
o
o o

g

g

2

o
o

o

o
o

g3

o

1

x

Figure 1: Using a mixture of Gaussians to compute y^. The Gaussians model the data
density. Predictions are made by mixing the conditional expectations of each
Gaussian given the input x.
One benefit of learning with a mixture of Gaussians is that there is no fixed distinction
between inputs and outputs | one may specify any subset of the input-output dimensions,
and compute expectations on the remaining dimensions. If one has learned a forward model
of the dynamics of a robot arm, for example, conditioning on the outputs automatically gives
a model of the arm's inverse dynamics. With the mixture model, it is also straightforward
to compute the mode of the output, rather than its mean, which obviates many of the
problems of learning direct inverse models (Ghahramani & Jordan, 1994).
133

fiCohn, Ghahramani & Jordan

For each Gaussian gi we will denote the input/output means as x;i and y;i and vari2 ,  2 and xy;i respectively. We can then express the probability
ances and covariances as x;i
y;i
of point (x; y ), given gi as


1
1
T
,
1
(5)
P (x; y ji) = p exp , 2 (x , i ) i (x , i )
2 ji j
where we have defined
"

x = xy

#

"

i = x;i
y;i

#

"

#

2
xy;i :
i = x;i
2
xy;i y;i

In practice, the true means and variances will be unknown, but can be estimated from data
via the EM algorithm. The (estimated) conditional variance of y given x is then
2
2 , xy;i :
y2jx;i = y;i
2
x;i
and the conditional expectation y^i and variance y2^;i given x are:
2!
y2jx;i

(
x
,

)
xy;i
x;i
2
y^i = y;i + 2 (x , x;i ); y^;i = n 1 +  2
:
(6)
i
x;i
x;i
Here, ni is the amount of \support" for the Gaussian gi in the training data. It can be

computed as

ni =

m
X
j =1

P (xj ; yj ji) :
k=1 P (xj ; yj jk)

PN

The expectations and variances in Equation 6 are mixed according to the probability
that gi has of being responsible for x, prior to observing y :
hi  hi (x) = PNP (xji) ;
j =1 P (xjj )
where
"
2#
(
x
,

)
1
x;i
:
(7)
P (xji) = q 2 exp , 22
2x;i
x;i
For input x then, the conditional expectation y^ of the resulting mixture and its variance
may be written:
N
N h2i  2
2!
X
X
(
x
,

)
yjx;i
x;i
2
y^ = h y^ ;  =
1+
;
i=1

i i

y^

i=1

ni

2
x;i

where we have assumed that the y^i are independent in calculating y2^. Both of these terms
can be computed eciently in closed form. It is also worth noting that y2^ is only one of many
variance measures we might be interested in. If, for example, our mapping is stochastically
multivalued (that is, if the Gaussians overlapped significantly in the x dimension), we may
wish our prediction y^ to reect the most likely y value. In this case, y^ would be the mode,
and a preferable measure of uncertainty would be the (unmixed) variance of the individual
Gaussians.
134

fiActive Learning with Statistical Models

3.1 Active Learning with a Mixture of Gaussians

In the context of active learning, we are assuming that the input distribution P (x) is known.
With a mixture of Gaussians, one interpretation of this assumption is that we know x;i
2 for each Gaussian. In that case, our application of EM will estimate only y;i ,  2 ,
and x;i
y;i
and xy;i .
Generally however, knowing the input distribution will not correspond to knowing the
2 for each Gaussian. We may simply know, for example, that P (x) is
actual x;i and x;i
uniform, or can be approximated by some set of sampled inputs. In such cases, we must use
2 in addition to the parameters involving y . If we simply estimate
EM to estimate x;i and x;i
these values from the training data, though, we will be estimating the joint distribution of
P (~x; y ji) instead of P (x; yji). To obtain a proper estimate, we must correct Equation 5 as
follows:
(8)
P (x; yji) = P (~x; y ji) PP ((~xxjjii)) :
Here, P (~xji) is computed by applying Equation 7 given the mean and x variance of the
training data, and P (xji) is computed by applying the same equation using the mean and
x variance of a set of reference data drawn according to P (x).
If our goal inD active
learning is to minimize variance, we should select
examples
E
D training
E
2
2
x~ to minimize ~y^ . With a mixture of Gaussians, we can compute ~y^ eciently. The
model's estimated distribution of y~ given x~ is explicit:

P (~y jx~) =

N
X

h~i P (~yjx~; i) =

N
X

~hi N (^yi (~x); y2jx;i(~x));

i=1
i=1
where h~ i  hi (~x), and N (;  2) denotes the normal distribution with mean  and variance
 2. Given this, we can model the change in each gi separately, calculating its expected

variance given a new point sampled from P (~y jx~; i) and weight this change by ~hi . The new
expectations combine to form the learner's new expected variance
D
E
N h2i ~y2jx;i
D E
2!
X
(
x
,

)
x;i
2
~y^ =
1+
(9)
2
~
x;i
i=1 ni + hi

where the expectation can be computed exactly in closed form:
D
E


2
2 + (^
2
~
2
D
E
D
E
D
E
n

~
h

y
(~
x
)
,

)
n

i
i
i
y;i
yjx~;i
2 = i y;i +
2 , xy;i ;
~y;i
;
~y2jx;i = ~y;i
2
2
~
~
x;i
ni + hi
(ni + hi )
D
E
D
E
~
n2~h22 (~x , x;i )2
2
~xy;i = ni xy;i~ + ni hi (~x , x;i)(^~yi (~2x) , y;i ) ;
~xy;i
= h~xy;i i2 + i i yjx~;i ~ 4
:
ni + hi
(ni + hi )
(ni + hi )
2 , we must take into account the
If, as discussed earlier, we are also estimating x;i and x;i
2 in the above
effect of the new example on those estimates, and must replace x;i and x;i
equations with
2
~ i (~x , x;i )2
~
2 = ni x;i + ni h
:
~x;i = ni x;i +~hi x~ ; ~x;i
ni + hi
ni + ~hi
(ni + ~hi )2
135

fiCohn, Ghahramani & Jordan

We can use Equation 9 to guide active learning. By evaluating the expected new variance
over a reference set given candidate x~, we can select the x~ giving the lowest expected model
variance. Note that in high-dimensional spaces, it may be necessary to evaluate an excessive
number of candidate points to get good coverage of the potential query
D Espace. In these cases,
it is more ecient to differentiate Equation 9 and hillclimb on @ ~y2^ =@ x~ to find a locally
maximal x~. See, for example, (Cohn, 1994).

4. Locally Weighted Regression
Model-based methods, such as neural networks and the mixture of Gaussians, use the data
to build a parameterized model. After training, the model is used for predictions and the
data are generally discarded. In contrast, \memory-based" methods are non-parametric
approaches that explicitly retain the training data, and use it each time a prediction needs
to be made. Locally weighted regression (LWR) is a memory-based method that performs a
regression around a point of interest using only training data that are \local" to that point.
One recent study demonstrated that LWR was suitable for real-time control by constructing
an LWR-based system that learned a dicult juggling task (Schaal & Atkeson, 1994).
o
o

o

o
o

o
o o

o
o

o

o
o

x

Figure 2: In locally weighted regression, points are weighted by proximity to the current
x in question using a kernel. A regression is then computed using the weighted
points.
We consider here a form of locally weighted regression that is a variant of the LOESS
model (Cleveland, Devlin, & Grosse, 1988). The LOESS model performs a linear regression
on points in the data set, weighted by a kernel centered at x (see Figure 2). The kernel
shape is a design parameter for which there are many possible choices: the original LOESS
model uses a \tricubic" kernel; in our experiments we have used a Gaussian

hi(x)  h(x , xi ) = exp(,k(x , xi )2);
where k is a smoothing parameter. In Section 4.1 we will describe several methods for
automatically setting k.
136

fiActive Learning with Statistical Models

o
o

o

o
o

o
o o

o
o

o

o
o
kernel too wide  includes nonlinear region
kernel just right
kernel too narrow  excludes some of linear region

x

Figure 3: The estimator variance is minimized when the kernel includes as many training
points as can be accommodated by the model. Here the linear LOESS model is
shown. Too large a kernel includes points that degrade the fit; too small a kernel
neglects points that increase confidence in the fit.
P

For brevity, we will drop the argument x for hi (x), and define n = i hi . We can then
write the estimated means and covariances as:
P
P
P
2
 = i hi xi ;  2 = i hi (xi , x ) ;  = i hi(xi , x)(yi , y )
x

xy
x
n
n
n
P
2
2

y = inhiyi ; y2 = i hi (yni , y ) ; y2jx = y2 , xy2 :
x
P

We use the data covariances to express the conditional expectations and their estimated
variances:
y2jx X 2 (x , x )2 X 2 (xi , x )2 !

xy
2
y^ =  + (x ,  );  =
h +
h
(10)
y

x2

x

y^

n2

i

4.1 Setting the Smoothing Parameter k

i

x2

i

i

x2

There are a number of ways one can set k, the smoothing parameter. The method used
by Cleveland et al. (1988) is to set k such that the reference point being predicted has a
predetermined amount of support, that is, k is set so that n is close to some target value.
This has the disadvantage of requiring assumptions about the noise and smoothness of the
function being learned. Another technique, used by Schaal and Atkeson (1994), sets k to
minimize the crossvalidated error on the training set. A disadvantage of this technique
is that it assumes the distribution of the training set is representative of P (x), which it
may not be in an active learning situation. A third method, also described by Schaal and
Atkeson (1994), is to set k so as to minimize the estimate of y2^ at the reference points. As
k decreases, the regression becomes more global. The total weight n will increase (which
decreases y2^ ), but so will the conditional variance y2jx (which increases y2^). At some
value of k, these two quantities will balance to produce a minimum estimated variance (see
Figure 3). This estimate can be computed for arbitrary reference points in the domain,
137

fiCohn, Ghahramani & Jordan

and the user has the option of using either a different k for each reference point or a single
global k that minimizes the average y2^ over all reference points. Empirically, we found that
the variance-based method gave the best performance.

4.2 Active Learning with Locally Weighted Regression

D

E

As with the mixture of Gaussians, we want to select x~ to minimize ~y2^ . To do this, we
must estimate the mean and variance of P (~y jx~). With locally weightedDregression,
these are
E
2
2
explicit: the mean is y^(~x) and the variance is yjx~ . The estimate of ~y^ is also explicit.
Defining h~ as the weight assigned to x~ by the kernel we can compute these expectations
exactly in closed form. For the LOESS model, the learner's expected new variance is
D

E

~y2jx "X 2 ~2 (x , ~x)2 X 2 (xi , ~x )2 ~ 2 (~x , ~x )2 !#
2
h + h + ~ 2
hi ~ 2 + h ~ 2
: (11)
~y^ =
(n + ~h)2 i i
x
x
x
i
P
P
P
P
Note that, since i h2i (xi , x )2 = i h2i x2i + 2x i h2i , 2x i h2i xi ,Pthe new expectation
P
of Equation 11 may be eciently computed by caching the values of i h2i x2i and i h2i xi .
D

E

This obviates the need to recompute the entire sum for each new candidate point. The
component expectations in Equation 11 are computed as follows:
D
E


2
2
2
~
2
D
E
D
E
D E

~
n
h

+
(^
y
(~
x
)
,

)
y
n
yjx~
~y2 = y~ +
~y2jx = ~y2 , ~xy2 ;
;
2
~
n+h
(n + h)
x
~
~
~x = nx +~hx~ ; h~xy i = nxy~ + nh(~x , x)(^y~(~x2 ) , y ) ;
n+h
n+h
(n + h)
2 ~ 2 2 x , x )2
2
D
E
2
~
2 = h~ i2 + n h yjx~ (~
~x2 = nx~ + nh(~x ,~x2) ;
~xy
:
xy
n+h
(n + h)
(n + h~ )4
Just as with the mixture of Gaussians, we can use the expectation in Equation 11 to guide
active learning.

5. Experimental Results

For an experimental testbed, we used the \Arm2D" problem described by Cohn (1994).
The task is to learn the kinematics of a toy 2-degree-of-freedom robot arm (see Figure 4).
The inputs are joint angles (1 ; 2), and the outputs are the Cartesian coordinates of the
tip (X1; X2). One of the implicit assumptions of both models described here is that the
noise is Gaussian in the output dimensions. To test the robustness of the algorithm to this
assumption, we ran experiments using no noise, using additive Gaussian noise in the outputs,
and using additive Gaussian noise in the inputs. The results of each were comparable; we
report here the results using additive Gaussian noise in the inputs. Gaussian input noise
corresponds to the case where the arm effectors or joint angle sensors are noisy, and results
in non-Gaussian errors in the learner's outputs. The input distribution P (x) is assumed to
be uniform.
We compared the performance of the variance-minimizing criterion by comparing the
learning curves of a learner using the criterion with that of one learning from random
138

fiActive Learning with Statistical Models

(x1 ,x2 )
2

1

Figure 4: The arm kinematics problem. The learner attempts to predict tip position given
a set of joint angles (1 ; 2).
samples. The learning curves plot the mean squared error and variance of the learner as
its training set size increases. The curves are created by starting with an initial sample,
measuring the learner's mean squared error or estimated variance on a set of \reference"
points (independent of the training set), selecting and adding a new example to the training
set, retraining the learner on the augmented set, and repeating.
On each step, the variance-minimizing learner chose a set of 64 unlabeled reference
points drawn from input distribution
P (x). It then selected a query x~ = (1 ; 2) that it
D
E
2
estimated would minimize ~yjx over the reference set. In the experiments reported here,
the best x~ was selected from another set of 64 \candidate" points drawn at random on each
iteration.2

5.1 Experiments with Mixtures of Gaussians

With the mixtures of Gaussians model, there are three design parameters that must be
considered | the number of Gaussians, their initial placement, and the number of iterations of the EM algorithm. We set these parameters by optimizing them on the learner
using random examples, then used the same settings on the learner using the varianceminimization criterion. Parameters were set as follows: Models with fewer Gaussians have
the obvious advantage of requiring less storage space and computation. Intuitively, a small
model should also have the advantage of avoiding overfitting, which is thought to occur in
systems with extraneous parameters. Empirically, as we increased the number of Gaussians,
generalization improved monotonically with diminishing returns (for a fixed training set size
and number of EM iterations). The test error of the larger models generally matched that
of the smaller models on small training sets (where overfitting would be a concern), and
continued to decrease on large training sets where the smaller networks \bottomed out."
We therefore preferred the larger mixtures, and report here our results with mixtures of 60
Gaussians. We selected initial placement of the Gaussians randomly, chosen uniformly from
the smallest hypercube containing all current training examples. We arbitrarily chose the



ff

2. As described earlier, we could also have selected queries by hillclimbing on @ ~y2 x =@ x~ ; in this low
dimensional problem it was more computationally ecient to consider a random candidate set.
j

139

fiCohn, Ghahramani & Jordan

identity matrix as an initial covariance matrix. The learner was surprisingly sensitive to the
number of EM iterations. We examined a range of 5 to 40 iterations of the EM algorithm
per step. Small numbers of iterations (5-10) appear insucent to allow convergence with
large training sets, while large numbers of iterations (30-40) degraded performance on small
training sets. An ideal training regime would employ some form of regularization, or would
examine the degree of change between iterations to detect convergence; in our experiments,
however, we settled on a fixed regime of 20 iterations per step.

1
random
variance

1

random
variance

0.3
0.3
0.1

0.1
MSE

Var

0.03

0.03

0.01
0.01
0.003
0.003
0.001
50 100 150 200 250 300 350 400 450 500

50 100 150 200 250 300 350 400 450 500

Figure 5: Variance and MSE learning curves for mixture of 60 Gaussians trained on the
Arm2D domain. Dotted lines denote standard error for average of 10 runs, each
started with one initial random example.
Figure 5 plots the variance and MSE learning curves for a mixture of 60 Gaussians
trained on the Arm2D domain with 1% input noise added. The estimated model variance
using the variance-minimizing criterion is significantly better than that of the learner selecting data at random. The mean squared error, however, exhibits even greater improvement,
with an error that is consistently 1=3 that of the randomly sampling learner.

5.2 Experiments with LOESS Regression
With LOESS, the design parameters are the the size and shape of the kernel. As described
earlier, we arbitrarily chose to work with a Gaussian kernel; we used the variance-based
method for automatically selecting the kernel size.
In the case of LOESS, both the variance and the MSE of the learner using the varianceminimizing criterion are significantly lower than those of the learner selecting data randomly.
It is worth noting that on the Arm2D domain, this form of locally weighted regression also
significantly outperforms both the mixture of Gaussians and the neural networks discussed
by Cohn (1994).
140

fiActive Learning with Statistical Models

10

0.001
random
variance

random
variance

1
0.0004
0.1
MSE

Var
0.0002

0.01
0.0001
0.001
5e-05
0.0001

50 100 150 200 250 300 350 400 450 500
training set size

50 100 150 200 250 300 350 400 450 500
training set size

Figure 6: Variance and MSE learning curves for LOESS model trained on the Arm2D domain. Dotted lines denote standard error for average of 60 runs, each started
with a single initial random example.

5.3 Computation Time

One obvious concern about the criterion described here is its computational cost. In situations where obtaining new examples may take days and cost thousands of dollars, it is
clearly wise to expend computation to ensure that those examples are as useful as possible.
In other situations, however, new data may be relatively inexpensive, so the computational
cost of finding optimal examples must be considered.
Table 1 summarizes the computation times for the two learning algorithms discussed
in this paper.3 Note that, with the mixture of Gaussians, training time depends linearly
on the number of examples, but prediction time is independent. Conversely, with locally
weighted regression, there is no \training time" per se, but the cost of additional examples
accrues when predictions are made using the training set.
While the training time incurred by the mixture of Gaussians may make it infeasible
for selecting optimal action learning actions in realtime control, it is certainly fast enough
to be used in many applications. Optimized, parallel implementations will also enhance its
utility.4 Locally weighted regression is certainly fast enough for many control applications,
and may be made faster still by optimized, parallel implementations. It is worth noting
3. The times reported are \per reference point" and \per candidate per reference point"; overall time must
be computed from the number of candidates and reference points examined. In the case of the LOESS
model, for example, with 100 training points, 64 reference points and 64 candidate points, the time
required to select an action would be (58 + 0:16  100)  4096seconds, or about 0.3 seconds.
4. It is worth mentioning that approximately half of the training time for the mixture of Gaussians is spent
computing the correction factor in Equation 8. Without the correction, the learner still computes P (yjx),
but does so by modeling the training set distribution rather than the reference distribution. We have
found however, that for the problems examined, the performance of such \uncorrected" learners does
not differ appreciably from that of the \corrected" learners.

141

fiCohn, Ghahramani & Jordan

Training
Evaluating Reference Evaluating Candidates
Mixture 3:9 + 0:05m sec 15000 sec
1300 sec
LOESS 92 + 9:7m sec
58 + 0:16m sec
Table 1: Computation times on a Sparc 10 as a function of training set size m. Mixture
model had 60 Gaussians trained for 20 iterations. Reference times are per reference
point; candidate times are per candidate point per reference point.
that, since the prediction speed of these learners depends on their training set size, optimal
data selection is doubly important, as it creates a parsimonious training set that allows
faster predictions on future points.

6. Discussion
Mixtures of Gaussians and locally weighted regression are two statistical models that offer
elegant representations and ecient learning algorithms. In this paper we have shown that
they also offer the opportunity to perform active learning in an ecient and statistically
correct manner. The criteria derived here can be computed cheaply and, for problems
tested, demonstrate good predictive power. In industrial settings, where gathering a single
data point may take days and cost thousands of dollars, the techniques described here have
the potential for enormous savings.
In this paper, we have only considered function approximation problems. Problems
requiring classification could be handled analogously with the appropriate models. For
learning classification with a mixture model, one would select examples so as to maximize
discriminability between Gaussians; for locally weighted regression, one would use a logistic
regression instead of the linear one considered here (Weisberg, 1985).
Our future work will proceed in several directions. The most important is active bias
minimization. As noted in Section 2, the learner's error is composed of both bias and
variance. The variance-minimizing strategy examined here ignores the bias component,
which can lead to significant errors when the learner's bias is non-negligible. Work in
progress examines effective ways of measuring and optimally eliminating bias (Cohn, 1995);
future work will examine how to jointly minimize both bias and variance to produce a
criterion that truly minimizes the learner's expected error.
Another direction for future research is the derivation of variance- (and bias-) minimizing techniques for other statistical learning models. Of particular interest is the class
of models known as \belief networks" or \Bayesian networks" (Pearl, 1988; Heckerman,
Geiger, & Chickering, 1994). These models have the advantage of allowing inclusion of
domain knowledge and prior constraints while still adhering to a statistically sound framework. Current research in belief networks focuses on algorithms for ecient inference and
learning; it would be an important step to derive the proper criteria for learning actively
with these models.
142

fiActive Learning with Statistical Models

Appendix A. Notation
X
Y
x
y
y^
xi
yi
m
x~
y~
y2^
D~y2^ E
~y2^
P (x)

General
input space
output space
an arbitrary point in the input space
true output value corresponding to input x
predicted output value corresponding to input x
\input" part of example i
\output" part of example i
the number of examples in the training set
specified input of a query
the (possibly not yet known) output of query x~
estimated variance of y^
new variance of y^, after example (~x; y~) has been added
the expected value of ~y2^
the (known) natural distribution over x

w
w^
fw^ ()
S2

Neural Network
a weight vector for a neural network
estimated \best" w given a training set
function computed by neural network given w^
average estimated noise in data, used as an estimate for y2

N
gi
ni
x;i
y;i
2
x;i
2
y;i
xy;i
y2jx;i
P (x; y ji)
P (xji)
hi
~hi

Mixture of Gaussians
total number of Gaussians
Gaussian number i
total point weighting attributed to Gaussian i
estimated x mean of Gaussian i
estimated y mean of Gaussian i
estimated x variance of Gaussian i
estimated y variance of Gaussian i
estimated xy covariance of Gaussian i
estimated y variance of Gaussian i, given x
joint distribution of input-output pair given Gaussian i
distribution x being given Gaussian i
weight of a given point that is attributed to Gaussian i
weight of new point (~x; y~) that is attributed to Gaussian i

k
hi
n
x
y
~h

Locally Weighted Regression
kernel smoothing parameter
weight given to example i by kernel centered at x
sum of weights given to all points by kernel
mean of inputs, weighted by kernel centered at x
mean of outputs, weighted by kernel centered at x
weight of new point (~x; y~) given kernel centered at x
143

fiCohn, Ghahramani & Jordan

Acknowledgements
David Cohn's current address is: Harlequin, Inc., One Cambridge Center, Cambridge, MA
02142 USA. Zoubin Ghahramani's current address is: Department of Computer Science,
University of Toronto, Toronto, Ontario M5S 1A4 CANADA. This work was funded by
NSF grant CDA-9309300, the McDonnell-Pew Foundation, ATR Human Information Processing Laboratories and Siemens Corporate Research. We are deeply indebted to Michael
Titterington and Jim Kay, whose careful attention and continued kind help allowed us to
make several corrections to an earlier version of this paper.

References

Angluin, D. (1988). Queries and concept learning. Machine Learning, 2, 319{342.
Baum, E., & Lang, K. (1991). Neural network algorithms that learn in polynomial time
from examples and queries. IEEE Trans. Neural Networks, 2.
Box, G., & Draper, N. (1987). Empirical model-building and response surfaces. Wiley.
Cheeseman, P., Self, M., Kelly, J., Taylor, W., Freeman, D., & Stutz, J. (1988). Bayesian
classification. In AAAI 88, The 7th National Conference on Artificial Intelligence,
pp. 607{611. AAAI Press.
Cleveland, W., Devlin, S., & Grosse, E. (1988). Regression by local fitting. Journal of
Econometrics, 37, 87{114.
Cohn, D. (1994). Neural network exploration using optimal experiment design. In Cowan,
J., Tesauro, G., & Alspector, J. (Eds.), Advances in Neural Information Processing
Systems 6. Morgan Kaufmann. Expanded version available as MIT AI Lab memo
1491 by anonymous ftp to publications.ai.mit.edu.
Cohn, D. (1995). Minimizing statistical bias with queries. AI Lab memo AIM1552, Massachusetts Institute of Technology. Available by anonymous ftp from
publications.ai.mit.edu.
Cohn, D., Atlas, L., & Ladner, R. (1990). Training connectionist networks with queries and
selective sampling. In Touretzky, D. (Ed.), Advances in Neural Information Processing
Systems 2. Morgan Kaufmann.
Cohn, D., Atlas, L., & Ladner, R. (1994). Improving generalization with active learning.
Machine Learning, 5 (2), 201{221.
Dempster, A., Laird, N., & Rubin, D. (1977). Maximum likelihood from incomplete data
via the EM algorithm. J. Royal Statistical Society Series B, 39, 1{38.
Fedorov, V. (1972). Theory of Optimal Experiments. Academic Press.
Fe'ldbaum, A. A. (1965). Optimal control systems. Academic Press, New York, NY.
144

fiActive Learning with Statistical Models

Geman, S., Bienenstock, E., & Doursat, R. (1992). Neural networks and the bias/variance
dilemma. Neural Computation, 4, 1{58.
Ghahramani, Z., & Jordan, M. (1994). Supervised learning from incomplete data via an
EM approach. In Cowan, J., Tesauro, G., & Alspector, J. (Eds.), Advances in Neural
Information Processing Systems 6. Morgan Kaufmann.
Heckerman, D., Geiger, D., & Chickering, D. (1994). Learning Bayesian networks: the
combination of knowledge and statistical data. Tech report MSR-TR-94-09, Microsoft.
Linden, A., & Weber, F. (1993). Implementing inner drive by competence reection. In
Roitblat, H. (Ed.), Proceedings of the 2nd International Conference on Simulation of
Adaptive Behavior. MIT Press, Cambridge, MA.
MacKay, D. J. (1992). Information-based objective functions for active data selection.
Neural Computation, 4 (4), 590{604.
Nowlan, S. (1991). Soft competitive adaptation: Neural network learning algorithms based
on fitting statistical mixtures. Tech report CS-91-126, Carnegie Mellon University.
Paass, G., & Kindermann, J. (1995). Bayesian query construction for neural network models.
In Tesauro, G., Touretzky, D., & Leen, T. (Eds.), Advances in Neural Information
Processing Systems 7. MIT Press.
Pearl, J. (1988). Probablistic Reasoning in Intelligent Systems. Morgan Kaufmann.
Plutowski, M., & White, H. (1993). Selecting concise training sets from clean data. IEEE
Transactions on Neural Networks, 4, 305{318.
Schaal, S., & Atkeson, C. (1994). Robot juggling: An implementation of memory-based
learning. Control Systems, 14, 57{71.
Schmidhuber, J., & Storck, J. (1993). Reinforcement driven information acquisition in nondeterministic environments. Tech report, Fakultat fur Informatik, Technische Universitat Munchen.
Specht, D. (1991). A general regression neural network. IEEE Trans. Neural Networks,
2 (6), 568{576.
Thrun, S., & Moller, K. (1992). Active exploration in dynamic environments. In Moody,
J., Hanson, S., & Lippmann, R. (Eds.), Advances in Neural Information Processing
Systems 4. Morgan Kaufmann.
Titterington, D., Smith, A., & Makov, U. (1985). Statistical Analysis of Finite Mixture
Distributions. Wiley.
Weisberg, S. (1985). Applied Linear Regression. Wiley.
Whitehead, S. (1991). A study of cooperative mechanisms for faster reinforcement learning.
Technical report CS-365, University of Rochester, Rochester, NY.
145

fi	
fffi 	


	"!$#&%'(()*,+#'.-/+0)+

1234$56'0'87(92:;3
!5$97()

<>=6?A@BDCD=FEG=$HI?AJKL$?FBMKNOEG@D?PEGQRCSHI=6?TBU=6@BDVXWY=&Z$K?AJK0L$?TBMKNOEG@
NP[\VX=$BU@]NP[\^_J?P`G@U=6@

acbMdUeUfg>hjilkTmnkoiepbrqMsptuf2vwbMieUx

yz|{,}~Iy| {Y {,|p }|

IeUdriSUo

{X|y| {Y {,|p }|

$jM,o2o6$
A.2cXI$|2&0&8u
$"$|&wIX6\$2Ac,2

6U
$j>X"w|U8G>oIXw08u2,
0/u2$G,Xj$jA"G2U"UTGp"
I08u",$6,2AXG,uTopA,
$$$"pXo,u&G$YuAo,u&IoAjo,u&
G6w0/u0$$$PuIG|0T/u0I"6c
X0uG0&pP,XT00XpuT,8uu,,
u$oc&$M0/u0$X2j,TXw2I>$,0
G,uPTGTp06uIc$
 ,jwGUjjujXA08u2
,jG>A0njAouXuw,"8u
,coAuG,$,,U08uAco,I0cp
$YAu$jw,c6,$0/uTG,u,$wM
T$$cPAw,G,,$0T&,r$
cAFP0G,0T,wIGuj"PIT0/u2$
uuIT,/u0

 	6 
ff 
fiff "!$#%&#(')*)+,.-/fi01!$23 4$'"457689*;:< "!$#%&#(')*)+,='?>@';BA,"C'#D
,EFA7';5#G 4H(@,@9,'4F';9IFA&#%4J:ff#G&)K CL4((4J;:NMO4( ( @'P&';QR7')?D
M9 4CS?T=U,( 9 V@,:WC##G@FA&#%VQ4FA&9 @,)M9 V@';9 9;:$FAMO4( ( X'@&+;:YFA;D
';( URZ')M9 4CS\[&#*,4(F']^4F7M7M74`_Ya'#GQCcb-ed2]Yb-/fCg-/d2(2]Yb-/f # -/d2(2]Yb-/f ) -/d2(2
';4hMO4( ( R7')M9 4i'jb-/f&-ed2(2k5b-/f + -ed2(2k5b-/f 9 -ed2(2I';4h&';( *RZ')*M94S 'ml ACnFA+4(
oqpsrtb-/d2k;-/b-/f g -/uv2(2?w b-/u2(2Bx4'y4(9,( &z ?{)*M9 4H';9 9|MO4( ( Q'E`&'}( XRD
')M9 4CS?~"hFA7';IFA 44(IC'a674C`'}4='*74(C#G,M( &a}:|FA?CU{(C#%4C]9 C'#(a:ff#G&)
FA4(IR7')M9 4CS l A4$,( &J}:9,'45';9FA&#G 4Y 4|'N:W#()\;:19 C'#(,+:ff#G&)R7')M9 4CS[&#
'J)+&#GR(C4( +,5#G&Qfi01!]_|h#%5:WC#^(@-/1'Z#' X  C#G45/]1C;7>@9 (Q
h"';7]1C;2S
C'#(,+:ff#G&)RZ')M9 4=)+C'4)+7:/V{H'?FA&#GVJ(67#G, )+#G,Q';&#%'_" FA
FA	RZ')*M94S l A_Y")';,N&M7C#('}( &4,?fiff1!U:/&#)+78C';( &?;:'YFA&#GV'#GCvCG ,/
'*ff7F B/vS1CC#('}9  ';h4F#GCFAC4'"FA#GV5A7';1 4&"_YC'v]_A 9Y4FMO,';9   '}( &
_YC';C4'.FA&#GVJFA7';" 4I(4(F#GS l A4(M7C#(';4&9 VQ)';4(C4(?_" FA,`'.&CvCG /
GFS l A4I 4='*#G9,';( &J4(F'}(,_ACH4&)+9,'4(?4)+&#%CC#';91FA7'J4&)+FAC#9{'4(S
50ff$G5,CCffG55/G$55,/0tG5,B(CffG5F/G$5B5,B//0/5
I

 '((0)pfi|fi75	I	5	T0	U	2ff4$	3
!fffi

!25;

fiY,77<($7vXE7
^F7GX.(,7GFCC(;  G@&G7C%h(j,ff1\G*F7F7( &/; (HC} 
  F7F77 C}1, C}( &aQ,  C;( &aG,;( .7}B&G&7Q;" 7H0
F?F7F7( &n&G7CvYhFHF7;I,(+ h+&G?CC(;F7U+&7 t; C( 
 "+&GIF7F7N{*C;(IF7F7.Uff*FI{* C;H&G7C	\ .&G"CC;
F7H    C; U, +Q {7} +&%CC(;157Q G,;( ?(X7;G&7
t^7 q L +("  ,(F7  

 	fiff  C;  Q,  iU
 FF7GG7CGC3F7F7( &  hFJ+(F;F X0j7G( C,F7F7
 X7  ICGC;  C;,  C;( & HJ7  =C  

 "(E,(";
(F FE
 @G,7;;"F@; F 0aF7(1%{}( +{* C;@ i7G7C
F7y, C}( & OFQGi77  7=7%&  7G7G  &Y,  C;( &Q(JF;
& H7CG t}( &  %&  
 	h,(X;7COCGC;*7G&  7G7G  & !#"$%'&!",  C;( &
F& UBH; 17CG t}( &  G&  

 	fiff
( F,Q.CC(;  GH&G7C75CGhGGY77G&;"(*CC(; # )C;( &U&Y57,; # )C;( &
"+<%(h77G&}BUCC(}# )+&IFO,; * )+
 %',.-%'&!%'-/0",C1 (@7Q?7 (C(h5?,
X75; ,UF =77C`.CG Q+CH   &^& (C(! 2F;|^77G;ByC
7JF(}m7;a(4
 3Y7 7 2 56^&C  8
 7:9&!5`ei";, C+C(  &N< ;Z 
>
 =77,G@ ?@C,yFhF757( &HG7CY,HF  &(   AB,CD1!,C$E9:F@"$9HGBI ";YC
J,(  B , FCI,(X m57F7+XUJ v&I,(F3F;+L
 K
M N Ov(Q
 PRM OIjS
 KTM N   OQ PUM Ov+"CU 7&+IF7F7+XU17hS
 VKqU
"F7F7a hYC;C^F7U,  C;( &1 W  7GFCIa  5YC( F  ;
F7;	F( |QY7IF7F7+  7 t; CCCF&+FXG"  C;  HZ ;; CC
^H(&c77G&;CC(; # )H&+57,; # )8
 G<$XG+  ,Cc^J X5Q77G&;
YQ"  7a&C(" F{LF Q7OCY
 ^CGaFQ&CJ  Z
 #:"[G<$\0!,C!"*%^]<"$%'9,_J

,7GFC"(" 5B
 #:"[G<$7CC(; # )C;( &I; ;"YY(hCC; * )C( &(  7,
;C  CC; * )C}( &C;(ICC(; # )C;( &?  ((I  ,(YCG<%(7 (C(jQ F,
 567 5@7` 5aC7B
 "7G;UF7;J< (+(c b  ,(h7}=* C;(CC(}# )C;
77C57F7( &/` de=7<^? iJ,(J BnF7F7++; {(h,f
 bPa Bn
F7F7.mj; $FC,(+F7}i; (UF7F7+H; Y{(+{f
 b"HX< ;7 +C
7*CC(; # )mQ5,UF,` dJ=h g  &7G|EIF;+F` de=a  "'7
<;7 C"IEY& L&  aFGE&( ((,  & E&U{(Pff(C;1"YQ+ &
7 & 7HFJ7( ( *< ;Z*+,(QF7(C@F;J(C7(;(` de=@  C;aF7(CJ"7;
"Y&F;,Q?FGXF}{,H+&G57J&,(
v&5Y(77%&;&57F7( &* ;&;,J    XF}(   }(&Gi < ;7   b
&( (  F,(k
 jfiKlM N   m&I PnM m"`  KoM N p(I PnM p7FCJF+` dJ=J 
ba fi
 M N q(I PnM Ov"Y,(r M N Ov(I PnM Ov; +(C+Y+&GY77G&7G,;}$IC}(
CC(}# )C;@  b"C7h7  &7@@ F, 2 i77%&;17C(J 7&+F7F7.
 j E WI|5Y< ;7 h; (+F;"CFIF7F7( &JG7C	I7G( C,G H7F;(   ;(&%iIC
Y&( 7Ce :7/sHGt%X&Y,(! $,(I HCU7hG(Q" FyFC+( C
uwvCxy{z}|~zii|~z~:X:zX!{X[fi[Xk@:X<h0.<<}}[y!yJhh@zi|z}[rH{y{zQh<zX1v
!vC0z:`:z~[z}|~h:X:{fiH|~zEHz}cHhzQzE<[X::z}{z}|~h:XH|`h[[z@:X<h<<}}
 {:zH8[@:z
:<}!z:I':!1::E<:}``h!z1:<}Xy[[y+{:k@:X<h
<Ht}![zzz}@XyI :z~[z}|~!`|~zw{!v
!vCxy{z}|~z`H|~z}XHczXzz~z:`:z}{z}|~h:Xe[z}|X[X{I{XHJH{1*[HiX:!#<E  {:Hz~
s0:tu:}v
<{

fiCCHf@@@X<[@'@C@@Z@[@0H8ss@'w[@'@C
 <sHs4Bss<[J@s@s#0c##@<s#8H
[+s4~0s
I
@s@s#00@B1s0c@[B}ss*#[H*ss@<EQ<<0@s#.**Q
@s
 sf0scrs<seE#ha##<s#104s
<s#s<k[Jh1s#@J0Q
s##[#0
[1B<#s~<ss<H#0frs<s
  s#<
[Q<s<>@ *
s* [H*s
@s@e
s#*#0  e@[Jss*f@<Z@H*8sH<` .0cs:s< @<H ~Q1s#
@
:s@*<E#[HEs[# [#0sfi@s@
r[fiJ@@[s
[#0   
8s#<0s>E
<H*s[
<s<E <sQ#B<  ~@[<k #
#B@s@1<
<srQs*<k
s##<r#iY<s T#r<  ~@[#0[   s
*#[#*k<@s*[[#Qs T#fi<  ~[@<`ir 8s#<01sE[
[H `B
<s< rs c@J<  ~[@#0s rs .s4+`J [ rs Q#J[#
` fi
s
 s#c@H*H*<*8sH0@s<H*s<0s<ssse<0#Hs<` Hh

:s[ <sH<firs#<@!r<  ~:[@[H*s [B<< 8s#<0sE[Q[#Js[
@[BsJ<s*H<s<Qrs#t+@ s
H< ~[:@[#0 s<J:sJ<0s<s <  ~[@H[#0
@s#c0@B@s<#0 s

 
 
)
*,+-
	9
?:?@ A:CB
ff
fi 
: @ I: B KJ

	

 fiff
fi











"

$
!

#
&
%



'
(
 fi.0/1 2$.3#%&465#%&/87
"ff
9
"ff
;: <:
ff
>)
1,+-
D9=@ E9FB
G9H@ E9FB
"ff
ML
,)
ff

L
ff




=9


N1[fis1s s[##0@w`<[#1
s#*#0i*e<4101@<s*@+s
sk[FO s[64
P s*#[H*sEQB<[#
s##[#0*#[
sJH@[<00@s ss{*<@
H[<<0@rs#t 8a>s<fk0
*  
s:~s@0a#<sZ<[#0se[s
@0
 [@s##[#0F. 0s#[H0#[cs*  [H*ss@s@
s#*#0 <#

@[<s0@sss{*<@[#
@<HJ[H#0


&R HS-5/$#%&/3.3%K.TU/$!# WV0#%&'(
X
Tff
Y

 @0Ssk#[1s* [H*sfis#
[#4#1C@[ 
s
c
.
J[<ca<<[# #0s>@!
a<Z[<<0@< 1s<4#<
[#04 ~`o@ *1s
[# [#0s`@sc:sJ<0s<sc[0<a<<[# #0 s<s<#<J@
s:~s X<es
@<@ss[fis<<#0 <









[Z

ff

}Zs#@a<B+# s
[H*+[:1[Js4<0#Hs< s ss ~<0#s<4[
#[csH[# #0ss40[<:@<<[# [#0sE@s##< H[<+[fis<1@<Js*#
0@fic@~0 @#s0s#ss4@<B<s<e@<e0@a1[#k@*Hs0s#8@<B<
s0@[<c[fis[i<s<Js81s<H#<<4s0@[<Q[ 0+<s< sss041
<ts ~`f<:#< [HH*sc 0e<s<![s[s<s<fi[#Q0<1< sH<
*
@
! @@sC
c [<@ LBsss0s
@<: ~Qs#
@
w 40<[<E
Z<ts1rs @ sss[<s<[<@*#[##[
s<0[H*
#*H[#1@@c8:s@s@4[rk<s @[c#is<8s
<s<e[s10
EQs
*
f[s sf[<
###[ s<H
[##<
<s< #[s`
<1fis <s<
+<c*#[#*f<@s*[[#1s[fi<sH<
Csfis:s<@sc@0H <s
i#<s#[[#QJ:sJs0 s
<sH
< Csis<B [0sIB<0sH*@BsEss*s0@[<E[ 01<sH<
@sr[#k@[H#0kH
s0@[<[ s[<s<



\

>]^ _]` ba
 > E)
gih j @k1lllmk j_n
g
I 
c]

rF&sx{?y v &sx{?zF&sx _



YL
>c`ad 	Q

Fe

GL

fJ

pj o
rFtsu hwv &sx3k8y_zF&sx

`a

EJ

?J

fiff

ffL
<L

qb5'mS-5#

JsE<01s@#0c[@@<Is[##rs@seB@}|` as#Bs0@[<`<s<
#<re[C#@}| B0@<s0@[<pC 0I[<k[s<H@Br1rs<s*
s*  [H*ss= `h8Q
 s80[<Q@<<[#  [#0sI eh8Q [#sc<s*H?~8
:@<<s#
[[@+0s[s< sJs8[#<0@s1s1
sBc[fi0@Q[rC0Qs1[J[
<#EB
##@
  :s><:s*Hs@c@< #0[esss<I} 8:s
##{s
s#1s#0s  <J
@H*H*sB<@s"cC1sr
s<0[H*JsB<
3

fi>``6m
`ux	q`1
K6H363
6,b363
 ` }-`
; 
;

``6$`F6 -K




TF6  , -q?

 -Gt`63 -&3 
3 F6  , -q?; 



<6H__6 63\,M;Y6b
 `p,Yfi3-$`6 -6
?$686 YK8`3$& ;_G6fi&-\,>K`,>8`,_}M
 K?6 ?m3
,<36m3K3-8,66
 ,>-6?6-86m -6`63 -&t3?36*t`
-i86 F6-86m -6C`63 -&3	36mMF1A3686 8I6 f6`*
,<86_3631`636`6f363>Y6 3H3-8,\`63 -68`8686C3- C?3-F`6$6 
;fi,<K_K6 K386 >>-86K66D`6f`,3H-\$6D``68`6 -fK63-6
36m6
&b3`uC,K, 8&_`u `-66K66`\E}<36Y6 3A ,K?6?	3m8 3
,m -6mF3C,
-6`6A6$633?,
3636I6I$6*86Y`6A6AH\6`}`
, -,_-\\`-K366366IF6  -Y6 3II`fi&-`6IX&&`3$fiCU6 
`u3DHF\3`,m3-	,86 Y386 K K86C36m63*6f36F`686   fM
;fib,C6 Km3_,`t`63m6&t3363_Ct<,_>>-6,8fi-`6,u;fi3$6
6f86 ?` 63- f63386 1
6 3D1`b`13
8`,C _ 
F6 Km?86,8`<86Y_}$6G66`,
 K38 33*u63	m3
,;3-6$_mU\`>63?, `6Hm`38,m66G6m
_3f3&-H86 f833,,mE,-`H6m,386 C A`f8`I6`-`,I3
``6  863;K6 ,  u>>&-`6f?`33K`6,;,;86 
,mU`686 >
6G f*m6
8`86?`6,<,;86?\-C6386 K$`,
>C YH86 K`3
6 31K `\8`,86
`6,< >F6 *F1F`?u6?m*m-\K3-6 -f-6>86?3, -f`3>3f`6f\6`3
6H  ,bF6  -C63>`$63fH
,
-\6`3 `6;6K`6f\6`3$3
EF `3`f86 \,?86F6X,``6m33 -  `6\-6F\ -86`$`,?86
`33 u6  DF6  , -&-t`63m6&t3363fF3`3 F6  3I86[3- 63,
X;m_G6\,3`33 u6  q 	6Ff$	3 F366`m6-86H3- 63[,Y[
6 ,m6bu-3U6C Hf`33 u6 XY6386H-6t`63m6&t336F6  3
6$6C`63 -&t3f36= 3, Hf`63 -&3=`0--`6q663`f_3\
6 ,m663 FH`63 -&3=`0--`6E66,K 3`F`A6, 16?3- 1<,?>
K  <$6 633 -I
 `;386 K`63G6 C86K6m,u`63 -,;86C3- 63C,<;fi``6>`63
`,6`I[`mmA6>D-\f3,\,Y3\86I`3m63f,K-6I6-86m -6
`63 -&t3*36m*6m,,`f-`33Y$636m63F6I3-`686  [,G
;fi;,;K6FY`,_8636m3G3-8,6  36,86K`6\663Y,xt`63m6
3F363fi ;$6fi,FF1AF,>-,`3 ,_ -6}63  &b<` 6 E
F6 \8, -6\,K M-m\86F6-`,F *3`63mI`\`63 -&3-t`63m6

	fiff	 ff!"	#$%&'	#&$$(&)+*,- ff.'/$0,fi12#
3  45/$+$6/$2#*7098:,28;	#$6<=8;,fi#	0ff##>5$6&"6 -6&1@?&$6
,fi) 0,AA$6%ff%ff'/$A0BCED6) 0,GFIH"J
KL=MONP NQR9STKLUP NQ&NVMWR )XFYEJ KL=MONVP NQR9STKL=Q&NVMONPZR[L $6\#]Z71?ff^/)?$#)_8;,fi71a`=) .$/1<
ff12b/ $ R cF H 2d,fi#ff*/0$A8eFYA&)fFYA)fFY22d,fi#ff*/0$A8gF H )9F H h(0	#"F H )fFY
,fi(ff/B	.ff%ffi!\#b/*ff0$#5j 512#&$6/$'FAHk(lnm'`k8n$6.$(o7FIH N F Yp h(.*/0,>$6&'lqm'rd8n$6#
$U2	0ff&# KL=MONVP NfiQRkSsKLUtuNv Nfiw(R > 	+	0ff#,ff!++*70,<UB/00,fiff%x./$6iC
yZzz

fi{|W}~A|Wn|Wn}n6Z}&nUn~G}nnAn|W}&n|C~XWW|OO=}nU}&nUn~

n[nW;EW#E[&WZfi#CZ=WOZEWOnOf=7OXn&ZWCCnOOO&A;ZnWXWZZnO;qOZ
7OZ&dAWkUu2COWZ=#uVdWuWZ_XZ;.OO&"0(fiC[7OZ&5(U+O=O=W
 Wc#CufiWCOZ  WWWZ\56 nk
 9Z #W7uWCfuO55VO#=d#OnZ
 ;ZW@a@[nW2uWW+&;OfCO;
finOZ;Cufi#Z9Z:uOZ
C#^7nW
COn&"&A7O9fie
O[27nWZWZW+n#WuW';A#Z7iZZ+finOZ;Cufi#ZZ=WOZ 'nOZ;Cufi#Z+Z=WOZ
W#i7ufZ;uACA[uAWnO;i;CO+ZCOZO=O9&7Wn&Z
 nZZCO9ZCu7#=nO;C^7O&A@7n@+ZAXOZZOWn Wd+;&2WOi;&iOf#Z=&iu
C[+n&uC#OnOOOIiZnu+OWAZu9"OEWO
sW#W7^finOZ;Cufi#ZW
 O:niWk2+ZCu7#=nOcZCO;Z[n;O;C_&'ZC;OZWO
OCufiZC;OZeC&Z
7ZZ:i;&iOO=_&&@7O\;C#n#Z_:uOCn&WZA0
nW#;O=WZk27O&s7n"Wu9Oi
Z9&+Z:uOZEn&XC&Z/nZZ=&;;&;CnOn=O;;&;C  C[O:O=O7O;9A;77O
ZC#ui=Wf&'CnAZ7O;"Cgn;2W;i&AI7n&"\finOZ;Cufi#ZZ=WO/&=WOCn&u;";W67 
5ufi0sfiWfin
fi7O;+ZZ;C+Ii;'n/O[Cc+&e/OZCOZOd2OZZ2kC27O\n/OiiOO&2i[OnZ=
7uOW;WW='7nO;7O;C 5ZW='2f#/\E/WOW#2Cu7OZ  nWOaZWA    
;&CI   
 
	6fi ff;U #W;W&C5XkfiOWfi ff;U #W;[7OOZC&;C &"WXC
 ; 	ZA; OiZA&'i;&;dOiZ_;"7&ZE&A/OnO;W#7i;_qnWuZEn; nOZ;C
&@7Ou[i;&;
 W 26
  #  ; 	d;^Z=WOIi/EOOcW;;WfuOZC"cC#
OZC&iu;;iWO
 W 26+  W;[9Z:uOfIi/OCOnWiiu;i&;
 7; 	Gi
Z;7O9n/O;n#WOWaZ=WOO+9n/OifWO&6!

;9Z=WOW(2O"
 $#nOW

7Onu;;W[i;&;=%
 fWO
 "&fnOW/OOZC&iu;;i+=
 9 O[O#Z=WOW
"O;ZE#n#ZWd[ZCu7&n;Z;Cn;dnOWZ
W
 '"
(*)+-,/.102.43/,65 Z87 nW
&=nnWZ&A7O\#fiC#nd;WW;W  O
7O;	u'iuWOZO:9
;=< 7 i+/OZ"&i'Z=WOZ"O;ZEu[ZCO7OZZEfi#C 7O[n[nW;+=7f  O*:/
;WWC C> ;?< 7 id7O\ZA&@&;AI
@ C9Z=WOZ""O;Z_Wn[ZCO/OZZ9fi#C 7O\nW;
=B9
7 
'

fi7O;EnWZA2*
 O nZ7nnnWC[XWO;7u#&=nnWnZB79"WOZCOin9/OZ=WO/&
=WOCn&uC9WOD@IC_=WOCnW$> n&Z C_7O;:7f[X[A;;O&n/Of7nZ=O#=OW;
7CO[WO&;;#C#n#AO9Z:uOZ2E7nO7nO;C:^Oi;&iOXWOE#Z=&;W\=O;i;C
(*)+-,/.102.43/,GF 
 ZH WOI Z=WOZ+WOEs[ZI&eZ=WOZ'X\7 7n&!	= ; 	=[=	
I_nOWZ&CKJLI_A7O#ZO;[97nO;7O;CNM97OZ
7n&:$MOPIERQS WODI W#
	= ; 	T[
 =.U WV2O fiff W;kAJXa
I WOYIZJ[9
fi\ ] OU/W6 <_^ `H ;6=	afnnOWZ&Acb defO@ZW#[CnZg& i+&;f^[CnZg&A9f
0\ g C67uU <h^ fi! i6=	!_
I nnOWZE&aZb di_
I Caj2"klb d6E
I m uOYI WYfi\ ] OU/W6 <_^ nVC fiff W; n
ob dia
I WOIpb dG9
`H ;6=	-IK#  iu`ff + GWnOWZ&mZb d:qBE
I uesrtj2"kCb di_
I f WO*I u#WV2O `ff W;
# ;Wfiff [[
 uob d:qYa
I WOYIKb d:q*f
'
v?wyx{zR|h}2~u]Wi~fi}Hn:~uh2zRW~fizRWhfifi
u
~fizR_BzB1~fi}
a}Wmsna!h2~fi_W$h_
~
~fi}m=hH1_fifi
u
~fizR_/RW~fi=zsTvh=$Rm2{}zRux/WhR2a=v_=$mRwCf
aRm
}WfizRzRh-~ax/WhR2mR=_
u{f1zR~/Wf~fi}fz2~fizR~fizR_"~aWzRW/Wa}
_G1zR|n|Wh
`~fih|W!~fi}hl=z}
_Rw
=

fia1h_uycs[

fi p="CnW$%fa"]_*  %1$]1]aas_*nBn
hg 
Z: Y
  %
n2$   Ym!a=2h!=au]/
mna="]=atn!1Wy=hg
n
a=/1$]1]/   %
n2$    Yt
18na=2_!==su]/mahg2
 i"/%m"=  i"  "=W n2*  : %y! Y
Y===S_   
==y%C]g21"]]2
_]G=a=2[=1h=2
_1B=_1_]%Bnn"hg{
	ff
fi
ffc-as=1_=$"B1]=$1="t{{n2%n=$n
] _B
  !" #$"
* -(/
. +*,*=
* -(/0  21 43 & +*,*=
* -3-5 +**,2
* 6387  "as=1_=
% =:'& )( & +*,*=


]= 8_ny :9==N2:-n$n_hgW_]<; gCBt_22_
m=y2  $  
an
_= > (/.@?n/3A52 n2 n$=_ _ '&/B (/.ffCD 21DB 3A5?_ ;$]:2
]g=NFE  G"H ! IJLK ! M{
u'&
BN
 1
m!8]gh2_
] (O.  3A5 :ng%_"8nC]]_2_
]P ! IJLK !RQ #SIy
T
fim'&!BN1:a_nyL9== 2-2*nU
8
1!W1 V 
N1n=*n
a'&!
 Xa_1"]]=]A*1Y=h]2
u'& 1 V g:
]_B2
]]= 
1 V  C_nyL 9== 2D
1Y=h]2H
AY
 & B2
 1gh_= 


	ff
fi
ff[Z\%{=$KY =_]; t  ^(
=1_8e;$]!2
]]= 2fgIas
hH_!n-si2""=1hP]!

$*$hg8]gh2_
f s

& ?**,*_?"(O7F`

abdc Y!2n


T


=hf
]__=__1"n2"lW__]n_]kjl
a

	ff
fi
fflnmo ! IJLK ! Mfp =1h=l & 
B 
=hm
fN21 2 n:]]_2_g=_]=Y

=_Y& %N
1 :n2#S ! MpgJL Q  ! u
ms



1 g"%D=_g28
*
=_
! &
sW]]_2_
]:=y=Y1*n:=n==hg
T

	ff
fi
ffqr%{=!S::_=
/=_="P8=1_/m !   K  I$
/6 Lg!sgh
_t2=$
A=1h=Wuv& ?,***w? u'x  sWT n
!2
=6u . ]:=]n2!1:!"=_g2
ma
=1_=1  u & ?*,**_? u .zy &  mfiun=B$2]

_]%=]_h2a:]_C|{~}a*
T
	ff
fi
ff%{=i$$_=a
m=_=Bo$=1_O Cn"W2:=]_h !Q g  I"

K: ]__2
C{ff:
 */!Z]"*n_]{ W2"=gh_l =_s n=n

|{ } Y  s
T

!Y= =n]2- =X|h!]2i!2X[6 @c, f22h
]:9==6H2__ff
!= 2 
 ,Mr !  oE ! - ! 2
 I@fgJL Q  !   E ! 
 @I Q   ,
g JL Q  !>8 fl
P 
!^  '2	   !  ! f
 pIfN@I Q   SMg ! NIfegJL Q  !   B2/ Q g6
 fp  





ks

!  !  !



!]g
!1$n"_]"H=2 ]=u==]_!=_
=h2L 92
_]8
1"]]2
_]*=a=2
=1_=*1N_2_$$
C=_1_][[nn"_]{S"!
$
=X|%{= @c, 
 gU
@ c 2
=Y
 h!]2@ iH2YBW

R @ c 
>

fiMMw+SSS^SzSMYSS|S^Sw<Sz^SzSM

P~,Mff<ffff|^~~,M+-,M_2v,,LG2d_2PL^W
n o r|ffD
WLRRP^p@wS>6Y__L-@
	S:LfffiLfiLNfiRPfi^_@L@
/"'
 	S
 	Sk fiL"@
 

 G+|MO|ffG
GW/.0+1.	

M

w'M"! L#s,:%$ 4 &(' R*)+,-)

2 ,~4365 fi 7   ) +| fffi8   & ffPDNS,  )  &  5 fi7  )  SDS'@_LfiL"
fiL__8ff4fiL%LL> % )"'
	S	SRfiL@)"S *)+, :9;	 "  <) *	_D
	S
	S 
"WLfi Lfi W=fiL__:>fi
ffM7 <)+,)  5 fi ?fi::  . +, . 
@
'fi kS@A_B	7eC "7fiLLRkD7w" FE G	_HffMS<^:L:^H
I KJ	ffffMLON PWS__QRPSfiLR>FfiLT_"ff~<" FE %P>fiLQR
P &
 U RPVWHXXXYWP[Z7ff\^]nKff"NSGP_fiLW'`	"'S"k">PbafffiL"Fc_DS"d
 e
 ?  K
	 _  :fi 2"'S" 
 "v__ Lfi "  Fc _Sfi   
@
E  O
fMD$RLffS"Ww_gP &U ih4Wj4DS^S^k &l  U ij4Y:m l on U ij4WhpY S^S^qff
fi 7
 An  U ijW
h7~
 S^ SDqGQ _LL> r @'_w:ds LRk'"S^rfi8   & ff^"P	S
  Lfi '"R^LL2"S  BtS
	 ^_"PS"C
ffD@fiL_  ) S2
	S
	SP
) 
 S'
S
	 
S
	 P
f Afi _"7 G
ff "u 
n iwpY_m l xn ih7WyffN7fi=-S^
. AM
 . 
vl ijW U ijWF
S"zq/
 6* 
	 w   "Cfi 7fi 7
c P__ U ijWFn4{ iw7W
 S"|7% Sfi RL

I KJ	ffffM}ON ~,~_DS7fiL_6w_R	wff;jVWXHXXWjZ<^LfiL_fi7>FfiLK
SFfi 7cfi v~As
ff h V WXHXXWh Z fi:wfi7_"_FFSFfi7cMfi v~ z~   O 
 jVeh"VWXHXXWjZhKZ4DLfi ^L:z LH Sz >~  ~  DQA~  fiLPR@ffffDS	_
","S^Lfi N 5 ^
u : 
S
	 _Y:fi 
	 Lfi  ~A
@
 G
 +< ,/ _Pp@RL
'ff
 o ffWv,n o nr   

_R,L'DM 

wR

 L

$P 

2 ,~43
 '7fiLp	
  5 	SS_  fiLNv"K	_LcL   jdV
h"VWHXXHXW
j"ZphKZp_QR r  _fffiL%LL^
 @"
S
	 
S
	 kfiLP" "S^"@%fiL	
	7v"S^ |  +| _7	/"@
Lfi e7~
ff 
	7 -"S^
^  _ _P"S'fi7 r 
 PD"*_"_h"VWHXXHXW
hKZ
S@%fi   S
ff '
 "w w"_	S@fi
_^LHffS'"fi@fi7cDfi
 ^Lff7S
R
	 _@7 'Ch a ^j a ff7 DF,M\r 
    ffM7 +[ p@@
  n  
@
|ffD 67



fi%`7Y4O-6H

_DS4dH0d[[[:g0K44H[d[bCb
[7/=pd_%=A7^
7p77D%76p77`7C*7===p77
pFC%
`===p7H
K;`otd<?7^v"<D7`F^eC=pp

 R*_"ip:Fz_


 	ifi ffK
  	iR*_8_ip: F
 ^477*D7==4
  *! 	i "DK 8_p: e#
 $%&z_
 4/ D7==
 p
 
 
 ff
 	i) (*+*, 	i
- "D_8_ip: F
 ^
 47
 7
 D7==
 /.04
' 




R D"
6F14273^7
7=37 _;
H  =-54!67ff)(Kfi8Kp  R =-"

F 14273%
7YY9K^7,K;:[<=*
F=^76_%
H =-><ff	iff@?A8*7 _1R< 
F 14 273 47=3 7z
 &*
F=>p4= 4!67R!ffB?=C"
F!?3ffB	iR*p^
D

EGFH

; :[pe> 7`7 H Dz FH 9=7:F`=pJIgp_Si==!K%*
H
FD4!67R!ffB?=C	4`7 H  KMLR7NOI07P%8;K
477RQ`7D7S FH 9=7
F`=pd FH ; :[pF%Mpz`7 H F`pF%TSp/
7Y FH 9=7*C%`
_7 ==! K_8V
 UW<X7S
7> F<H 3 =7D`SY
 5LRi `
 U;7XZ.\[]N^7OI_`]P 
7
UW<XT
 SUWaX; bc4
dFY3 T7d7==pC=Y7===p7S7-7F:
`===p7Sip
A` H YpeF^e
 ? F<H YW :[7F7=%>
7(7Y
K`>`; bFKHpH
 K;`o4&d**C` H Hgf&"< F<H YW:[7F%pz_hji, "<7=

` H K7kG^_gklfnmgip:FOmoh7
7/%
H&kv=
p74HAffB?=qffB	iRC$h H 7
f>s r H ^ kg===0t
 ? ff	uppH4A ffB?=) qff@	ix5
 vxwzy|{s
 h H 7

 fMz__%, 
 k S fJkvip
F7C=Y7
 k S zs
 h H 7
 f>
} H C=7 kv=&
 ~>t "- ffB?=) qffB	iR>C$
 h H 7
 f>78
 mMfkp FO
 moh
 r H -

 k=;C=
 ppF ffB	i 	|*>% "H ff@?) qffB	ixK


 vyz@{
 h H 7

 fM`_8%>, V
 klfJk S ip: FD
"`=C=p
kSu^ h H 7
 f>
D
iAAD*>%Ye*Sh67
r H 7fM`^_"
7
7= pzr/=
 H 7 F<H  H (D
7 F<H 3 =7-`7 H  
 f`=8k K7m F"
z& Ap
ru G7 *>V
 hA
7%, 
 kI0mz
A7p77Y;7 >FA7Y
7r
7
 /
7! DYC4p77A
 LR=% P H 
 H 77
 LRpe% P=%% H 7
 A H %%>
"B *_`= :Fp"F=S F<H C; :p
FYC=peF^AG` H ! K
K;`o<d<1-;` H Y<7nfg- F<H YW:[7F>p_R%ipSF(7Y
 H 7
 sh0_7F%2p="
DA=_7C=Y7-7DSper
`===p/h H 7
f&`z_
7
77FF^KfMS== O?3ffB	x	i-"
D
i
7 H =O" 7, H  H =M`D
7!/=C=7 p/ e`7F
`76C F<H YW:[7FA%r`7`6[g% H  H =6,%=
`797= H 
` H ^C
7D6=
 F<H 3=7 `Y` H YS=p@7=/7 H 7
 `YB C7_ FH ; :[pF:7F E 78 F<H C;:[pFTf6=SC=7D_7D
7SF`
pF7 H  K
 f=*`=p
7YG FH 3 =7D`Y`^

]

fiB5BB!-JBeBB77!
~  *|B7!ABAsAz&
7%B@3e7xe$%7zB7z|3%7!|zBTB7
B7e@ se=%B&e@7eB/7 3@A3Bs%7!OBA&A&B!A7N7@9
xe%7T;*eA!T|z!B/7V3%7!T-%7%=73e =793!97eT A3B
&733!3eRxee V@eAe7O773BV~&%7
R*eA!7z33ue  zBA7;

%7! W3!VB!7!39!a3eNj7!T%7%&73eu@7We!33!3eN
57!z%7%&797u&3=g7B73=)BA%9@9B7 3BA3BC/%7 *7A!
z33e %!@B
-3!B!7!a93!977!&739!a3eB7/Z~
%7O   *7A!z337l-3!@!7!33!3e7!
A!3Bx-!=793!97B7
7 fiz;!A!@@7eB B7N%A&;!A!@;*eA!A@9@O/B%agz3Ne

73e7!7!7+797ugz/!BNV7%7!
B$Me 
NB!ANZ73/&
79!V~%7+77V3euuz39se3%7 B7z!Ae%7%&73eB3B!N@B%7!Ae
@eA3B!z7!zBB
A 977V=9!aV@!7!33!3e7B7eA!33!,
3e77!&733!3eu|A%3B3B33Bu3!79!aB!7!a93!977B7e!
%a93!9777!/A3BN&733!3eu7A%73/%73/%ABn,BNA!n!
%&=BA3|B73Ot%7BAe797u
 /ssR0B*$
ATzBBea!@3e//%7%&73eu
u!TB!7!33!3e7 7!T%7%&797
,B!/3!7e!73B3#@O3BBA!@ B|7=A%73zY3B%@u s*B7
3V7TB337
 *-,!/.10!3243B35467276e98;:=<7>5@?AB!,!CBEDF5-x

	fffi !"$#&%(') +
H5
G +s
I e
< ,A+K]J L3 
3 45M,&N&O!AP?05=F8
-0e735e@%7&eB7,%7!c9 !B57]TN%
%7#$5V_9=9;3
e/B7B77%7
<73e/xez%773!7 sB
B!A/Z73
VJaB7
3@7z!AB99@9t7B%/B7%agB7eaBVB7/xee/B7eaB!
Y3B%@B7397%773!7&aBNuu77!#,B
3e7eAN%7&t
A!@ !@!B73!B7&39=B!A%a97BAxeAB#A79,xT3*A3!
&7]_%z%7+
eTZ79V@73V&73%7 73ea9$@7VN
,x!R%7BA%B7BA3NBBA!R Q1S77 %7+e7!AB
 *-,!/.10,9423Bu3562e67X8Y:=<eff5@?ZABT!,!CBEDF5-x

	fffiUTV !"W%M'=') +
H5
G +s
I e
< ,A+K]J L3 
3 (N&OO-A[?V\5=E8
 @&^]9_a`Zbffc>cffcbd_efGJs+S3%7e7T3BT@!7!33fiBCz%%7B7
0B+%B7BA3@BA! ug_ h_ `+i c>c>c i _ e $%7!j_kl_ 7B!amon@mlp|
q]_9W
 rhst
%7t%W
 _ k 
 r/xez@!A;
 mtn&mVp|7!txezB!;
 mtn&mVp|C%7!A
3v
 u k %7w
 _ k u k GtrMB7!
 u k e73aV7]@AB73j
 _ k Vz3;
 u l
 u9` i cffc>c i uKe
%7!(
 _vux
 _y`Pu ` i c>cffc i _euezGHrO!7g
 _ H
 r/ev
 _M3&/aY5ts
{
\ 	ff"

|ff}~C)+XC=>XC9dffX@)FffvffX LK}79X))X[C
MCE9Eff)[)9;9+>
1;ffY &E97WEff)[}=[d4
>YX))>d)1>ff(d&)X&)[=
d)C&[79X&C)[)[[y
C)d&;9E[)L+X>Y)))&d)C@}
 X++>C)d&XWX&d)CX>ffX[dvC9ff[Z}9X))X[CX++>
1>@W>=[d) E9)[}
X

fi&
/ovHff

@dd@d9$C9794@[>[M4-tW9
E[971C;>9a
  9+>[
 
;$9
 7z$ [
	 Pw
7

P



&
fi
7











g



>

[

a

9





z

d



1

[



d





;

























7

@





 ff  !#"$%&!'
(')*+!
ff
;1
 (9
 ,-.0/1"2%&/3
 5467!-"8)*+!:9Yd96
  >4! ;7C9
4!  <
 ;=>?/@BACd ff M d1>9W9D
  , =
  4 @*
  ,FE  4 C G
 (9D
  ,
<
  4 yE$>
 fi g(zd@)H fi *
 [9
 >P
 fi&;9d ff #
  7C9$
[I 9  ff  ff >
 )J LKd1d 
 MhN
 K)7>[
 MPOCQQ
 KST
 R M^+>[
MU7
R K;L 9@d9# KxWdd ff W V
 	X >4
 fiCad>&d4>

XY[Z!\^]_Z`$acb'd1efhg
i
ZkjmlnZ3\ko#pQqrqcfj *sutwvJxy{zv*|<}~|!|
!v
#&xh=KO3r!vJ
 ~v_JvJWLLhxv6TTC:xhv_v5v
H
xh|L=~  7h<
 ]_\[\o  c >H,(_JJ(WA2(
 tC7j4M91>>9[fi&H9d ff -d
(  [jd4[ o
 PJ- , (J_J'(Wj
 [jdjW
 T [7 ff (-
< , (_JJ(  4[Wd;W

 79< L :. ) :/
 7M74d9@ 79d >
&&>9g;d?
fi C9:, E JJ E g
  F 5M
  F*@ vC>H  , (_JJ(   A
w4$

  d><^
K W1Gz9H
 *(
 :@ d>fi  #;  ff 7a9L>H , (J_J'(W  A
- , ; E J_ E   ;) 4d9;9d ;u+
 )7#7*r ad
1T^
 [ad4[   4[Q<  g9 7a>9&  1>9w 1
W

  [ ff & [d> ff 7[&fi Wff6  c 7>[Q#&cr   ;<c(
 
ad?o
fi d  ga(9: <Q C MO= W ff d(d9#& NM
)7@>[u*.#=
  7@4>[6r 
 1CZjC9 
67M  4&  7 
7 X[W<  -
  G.n, E JJ E 
 L 74>[3NL[[  G    M   

aI>Y9->? , (J_J'(W  A&;-; ff 7 9FCWJzd>[;-CZ(d9#S;[+
9 Hfi5 , ;m E J_ E   ;[ ,  E _J E   # ,  ,mE J_ E     v

M  > TM
 wGz9 *M
 L# 7P@ &C97 9CW>9#$fi >[* 
  
  










    
      0

 
 7P W1Ga9, <4
+

$7a7 # [>[!4jtW9v   
 >vC&1a[W1>v4[_>yvJ|xhJv
z!1^x~E
 7>j9dd ff 7
 fi$ZjC 9$9
 47[7=
 H fi4g4z974>W;9 nV
7[Q
 7? fiYW9C>[dPw
  ff $G
 
6
 t9&
 ~!*hxvJ|F
 E>
9;[d[>J [fi&1ffg4$[9dd ff 7jjQ
 4
 9>[CJ 
fi& ff >47&>[ >$>9E>>9 >97z>&[94$dC ff 7 7 ff +dW9
9+d#
 fi&I V? fi$Q
   ff h-47*
 7>
 JK

 Z:jwfiJf\wj M(W  vJ:>H , (J_J'(W&#AzMC
z97[ >9>
>CM
zW
v
-8v_|x_vx~#>4MMh-*Q M 7 E >?&L,J(J_J'(W&  AH 
JJ

fi-	

fffi
fi


	fi
fffi
fi

"!$#&%(')%+*,!+-.%+''/!10
)2!4356#&%879';:'<#&='>)?@A!17+)?%#B!#B=C'D%+-.#B!(#E>FG-%+'%/)IH
J
)%+#LKM-7N@
'7$-?O).%+'!-P
QR )J%+'%S
T J
F
FG-%+'OUWVYXZ\[^]`__`_+]1ZIabO)?@cdVYXfeg[^]`__`_+]1eih(bSOj!4#&%>')2%+*!+-k%+''l!;0
)!4)?mon T -P
Xqp"Z [sr Xftue [ ]__`_+]1tue h bfv]_`__9]fpjZ awr Xftue [ ]_`__+];tue h bv;b4#&%/) R ')%+!$x'?'7+) R #&y)!+#&-?A-2PzU{J
?@
'7
3 5 D|%+-A'='79*~}?#B!'l%+'!I-P QR )CJ%+'%>0
)%>) R ')%!4xC'?'7+) R #&y)!+#&-?J
?@
'7\3 5 #?6S,A-7N'-f=C'7Do#LP
') Q 0lZo#&%w)/-7+? QR )J%+'/)?@,') Q 0\e|#&%w)$
^"x79-J
?@ R #&!+'7+) R p#"S'SDq)x7N-J
?@)2!+-E>vDC!10'?
!10#&% R ')%+!/xC'?'7+) R #&y)!+#&-?6# RBR #&!+%+' R P8) R %-
',)O-7+? QR )CJ%+'S, QQ -79@
#?x R *Du#LP8c#&%I)\}?#&!+'\%+'!
-PF
-%+#&!+#&='\x79-J
?@ R #&!+'7+) R %`Ds!;0'?A'='79*k}?#&!+'>%+'!(-2Pg-7+? QR )J%+'%I0
)%4) R ')%!/x'?'7+) R #&y)!#B-?
J
?@
'73 5 #?,OS
s>$`A~MM
M\kMM
M
-f'l!1J
7?kPj7N-EY%1J
%;J
E>F!+#&-?!+-k!10'l#E>F R # Q )!+#&-?-79@
'7SAM?6!10#&%I%+' Q !+#&-?'># R&R @
#&% Q J%+%
mon+%DC#?>!10'$?'<!%+' Q !+#&-?I'w0
)?@ R '(n T
T %`Ss-7u-7+? QR )J%+'%`D!;0'wmon/KMHqJ'%!+#&-?l0
)2%z) R 79')@
*

''?k)?%+'79'@O?'x)2!+#&=' R *O*\Jxx R '!+-?,)?@k('/)'@
!4pNvS
m'!Z\[,V>p"p"v+v >p"vDZ
V>p"p"v+v, >p"vD([,V>pjpjv+v >p"v\)?@

 VW>p"  p"v+v>p"vS0'?,'I0
)=C'/
-!10([8 VXfZ\[]1Z b>)?@O
 VXfZ[^];Z bSj!w#&%(?-!




='79*I@
#L Q J R !!+-%+''$!10
)!!;0'79'w)79'(?-4E-79'%1FG' Q #L} Q -7+? QR )J%+'%!10
)C?>([)?@\ !;0
)!#E>F R *


-C!10Z\[()?@Z SI-7$[^(?-O79'%+- R =C'?!-Pz([$#&!10A#&!+%+' R P#E>F R #&'%lZ
)C?@?- QR )J%+'\!10
)!$#B%


F
79-FG'7 R *l%;J
%1J
EI'@C*>([%+!+# R&R #E>F R #B'%/Z[z)?@OZ Su-7 |'='79*,79'%+- R =C'?!-Ps #&!10\#&!+%+' R P



#&%8)=f)79#)?C!-Pu D)C?@O?- QR )J%+'!10
)!#&%(F
79-FG'7 R *,%1J
%1J
EI'@A*, %+!+# R&R #E.F R #&'%IZ [ )C?@Z S



0J%w[w)?@A
)79'I
-!101EI#?#E>) R& x'?'7+) R #&y)!#B-?%4J
?@
'7#E.F R # Q )!#B-?-PXfZ\[];Z bS T #? Q '


([z)C?@l )7N'8?-! R -x# Q ) R&R *l'H
J#&=f) R '?C!8J
?@
'7#E>F R # Q )!+#&-?D!10'79'#&%?->mon/|-PuXfZ[^];Z b$#?\OS


-'='7D!10'Pj) Q !!10
)!!10'79'#&%?->mon/|-2PXZ\[^];Z bw#?>@
-'%?-!EI')?I!10
)2!zZ\[z)C?@Z


0
)`='w?->mon|#?D%+#? Q '8)I-7+? R )?xJ
)2x'w#&%)/E-79'(79'%+!179# Q !+'@\%1F
) Q '$!10
)?.) QR )J%1) RR )?xJ
)2x'S
M?.PM) Q !D#&!#&%%10-w?\*\Jxx R '!+-?\)?@Ou)x'Ip9vu!10
)!V>p"up"v+vl>pj  pjv+vi>p"v#B%
)?kmon/-2PiZ\[()?@Z
#?S-7!10#&%/79')2%+-?D
#&!/E>)`*,
'I-79!10Cw0# R '(P"-7!;0'4mon!+- Q -?%#B@
'7

) QR )J%1) RoR )?xJ
)x'#?%+!+')@,-Pu-? R *O-7+? QR )J%+'%S
?!;0'l?'<!I%1J
%' Q !+#&-?Du'>%10-!;0
)!/)?C*O}?#&!+',%'!/-P QR )J%+'%Iw0# Q 0 Q -?!;)#?%4)! R ')2%+!
-?'I?-?CKM!1)J!+- R -Cx-J%PMJ
? Q !+#&-?CK"PM79'' QR )CJ%+'D0
)2%8)?kmon/#?\S8?#E>EI'@
#)!+' Q -79- RBR )7N*,-P!10#&%
79'%1J R !#B%!10''<#&%+!+'? Q '$-Ps)?Imon/-Ps)?C*(}?#&!+'$%+'!u-PPjJ
? Q !#B-?CK"Pj79'' QR )CJ%+'%S?>-J
7uJ%;)x'-P!10'
-79@oD
)\PMJ
? Q !+#&-?CK"PM79'' QR )CJ%+'4E>)`* Q -?!1)2#? Q -?%+!1)?C!+%D'='?!10-Jx0 Q -?%!1)?C!+%)79'(%+-E'!+#EI'%
%+''?k)%PMJ
? Q !+#&-?%w-2P)79#&!9*GS
\CzooW
-7E-79'S

QR )J%+'>#&%"
L^9;$#P#&!(@
-'%/?-! Q -?!1)2#?PjJ
? Q !+#&-?k%+*
EI
- R %(-P)79#&!9*


-!+'>!10
)2!4) Q R )J%',#B%PMJ
? Q !+#&-?CK"PM79'',#L#B!>0
)%I@
'F!10SO? Q )2%+'l-2Pg%'!+%/-P QR )J%+'%.w0# Q 0) R&R
Q -?C!1)#?.PMJ
? Q !+&# -?,%+*
EI
- R %DG!10'4monKMH
J'%+!+#&-?k79'E>)#?%$-F
'?S
o |/ooso.AsoA	ff

M?!10#&%l%;J
%+' Q !+#&-?D',# R&R %10- !;0
)!l)?C*}?#&!+'k%+'!lU-P QR )J%'% Q -?C!1)#?#?x)! R ')%+!\-?'
?-?CKM!1)CJ!+- R -x-J%gPjJ
? Q !+#&-?CK"PM79'' QR )CJ%+'D0
)2%w)?,monu#?\S
\Czoo  m'!> 
'O) QR )CJ%+'Dz[^]`__`_+]1Ga~) RBR @
#B%!+#? Q !l=)79#) R '%>#?\D)?@fi )k%+'!I-P
!+'7+E%S0'?!;0'
;\-P8YS7S!Sfi #&%wp4]fivV Xk6V Xf [[ ]_`__9]1 aMa b]



fi"!$#%'&'(*),+.-0/1&'#%324657#98;:'<>=
?A@*BCDBFEHGJI6KMLONQPCRBTSBCVUXWZY\[JY\]_^*`baQNdc\egfhjiTkl>ll0k.hAm>^onqp9rsp.BTtJPNAuTv$rw*p0BTp>L"t@*BxMt@*B
yz'{T|}z3~j{T| PN1c?`CT`t`'Knqp1cjkKse;Ah i kKsl>llTAh m kKsT`

 P*CBTr*vBL*nN_heJ3F*1rx*Jef3k^L>t@*BxRAhk0AefJ*F.TkJ
ff00TkJ.'ff*0TkJ07ff00^`
_$_ BTtBr,x*nt.BAp0BTt"PN_uTv$rw*p0BTp>Lrx*ff'Bjr'PvqBpw'*p0t.ntw*t0nqPxNPC`@*Bx
t@*B |>{>| PNUFsnqpAt@*Bjp0BTtPN1rvqv_t0BC0pdQnx*uTv$w*'n$x*opw'*t0BC0p	PuTuw'C0CVnx*n$xo`

 t0BC0p0BTtPNAUZp0P*BJ;nqpJr,x*nqt0Bp0BTtPNCVPw'x*Zt0BC0p`  PCjn$x*p0trx*uTBLt@*B9t0BC.\p0BTtPN
 eJ7Tk7kJ7k'k,>30"Uffef73kk^nqpAef3kk*Tkk>^`
 w'Cd'B,x*nqt0nqPxZPN	rFt.BC0\p0BTtjuTPC0CVBTp'Px*'pRt0PF?A@'rta'BTp0tr  v$'w*nqp0toDWLWurvqvp
rn$x*n$Jrv"t0BC0gp0BTt>$`aHx;@*nqp'B,x*nqt0nqPxL"nN;nqp9rs'PvqBgpw'*p0t.ntw*t0nqPxNQP*Crp0BTtRPNAuTv$rw*p0BTp
Mef  i kl>ll0k  ^?`CT`t`*p0PBjPt@*BC"p0BTtPNuTv$rw*p0BTpOL't@*Bxr |{>| PNOnqprR,x*nqt0BRp0BTtPN
t0BC0p?A@*nqu@uTPxtrn$x*pt@*Bn$x*n$Jrvt0BC0p0BTtPNUFZrprpw'*p0BTt`
 p0n$x*J@*nqpAx*Pt0nqPxPNt0BC.p0BTtL'@*B'B,x*BTp9 y,qy~}|yz rpNQPvvqP?pnNhrx*  rCVBAuTv$rw*p0BTp
rx*FnqpArRt0BC.p0BTt"PN"f  ^UJp0PB'PvqBpw'*p0t.ntw*t0nqPxo?`C`t`'f>hJ^L*t@*Bxh y,qyT{
?`C`t`'nNAhk0A e  `An$J*vqnqurt0nqPxnqpj'BTuTnq3r*vqBL?"BrBCt@'rxFvqPnqurvn$J*vqnqurt0nqPxMrx*
p0tCVPx*BCt@'rxpw'*pw'J*t0nqPx`"a'BTp0trR  v$'w*nqp0tJDWL3WnqSBTpt@*BCDBTpw*vtt@'rtrxU,x*nt.B
p0BTtPNuTv$rw*p0BTp@'rp1rvBrp0t1Bx*BC.rvqnrt0nqPx9w'x*'BCn$J*vqnqurt0nqPx?`CT`t>`rxUt.BC0p.BTt1`P?"BTSBCTL
rpj@*BJrvqp0Pox*Pt0BTpLn$J*vqnqurt0nqPxnqpx*PtAtC0rx*p0nqt0nqSBJrx*Z@*Bx*uTBJx*Ptrw'rp.nP*CD'BCT`j@*BCDBNPCDBnqt
'P*BTpx*Pt3tRnxt0PFPw'CdBx*BC0rvNC0rBT?P*C0F@*BCDB`  PCt@*nqpCVBrp0PxL?"B?nqvqv	x*Ptj'np.uw*p0pntdNw*vqvqU
@*BCDBL3rx*FNPCt@*BjprBCVBrp0Px?"B@'r>SBx*Ptn$x*uTv$w*'BTrCDP?NPC"n$J*vqnqurt0nqPxsn$xor*vqB9W`
 BTtw*px*P?bBTn$x?nqt@jt@*B	'CVPPN'PN3Pw'CCDBTpw*vqtuTPx*uTBC.x*nx*jt@*BBT*nqp0t0Bx*uTB"PN _ a0p>`"Px*p.n'BC
h6eJ_k7kJ'kk*rx*  L9rx*Jrpr'PSB`@*BxRh e  rx*Jrvqp0PAhk0 e  L
p0n$x*uTB  nqpArJCDBTp0PvSBxt"PN1J  *TkkT_Jk  kTrx*oJk  *TkTJkk  *0TL
?A@*nquT@rCDBn$xAhk.AT`  p?"B1?nqvqv*p@*P?Zn$xt@*Bx*BT*tvqBJJrLt@*nqp@*Pvq'pn$xBx*BC0rv_nN,h e  rx*
hnqp"Nw'x*uTt0nqPxNCDBTBL7t@*Bx?"BjurxFCDBTp0tCVnuTtrt0t0Bxt0nqPxt0PJt@*BjCVPw'x*Fn$x*p0trx*uTBTpPNhnx*p.trxt0n$rt0BT
t0PJt.BC0p"n$xFt@*BRt0BC0p0BTtPN  Up0PBO`
@*BA'CDP*PNPN  BJrRw*p0BTp"t@*BNQPvqvqP?n$x*n'Br`1"Px*p0nq'BC"rR'BCDnqSrt.nP*xPNrjuTv$rw*p0BNCDP
rjp.BTt	cPNCVPw'x*JuTv$rw*p0BTp`'w'''Pp0Bjp.PBPN_t@*BAuTv$rw*p.BTpn$xFcuTPxtrn$xJt0BC.px*Ptr'BrCDn$x*n$x
`1@*BxrxUFvqnqt0BC0rvpuTPxtrn$x*n$x*Ft@*BTp0BRt0BC0pn$xcw*p0tBCDBTp.PvqSBTr>?r>UJn$xFt@*BR'BCDnqSrt0nqPx`
@*nqpBrx*pt@'rtjnN?BJCDB*v$ruTBFrvqv1t@*BJt0BC0pjn$xt@*BJ'BCDnqSrt0nqPxt@'rtrCDBx*Ptjn$xL_Up.PB
Pt@*BCt0BC0ELt@*BxRt@*BCDBTpw*vqt?nqvqv3'Brx*Pt@*BC'BCDnqSrt0nqPxPN,`  P*C_BT'rJ*vqBLt@*B"vqBNQtPN'3w'CDB
p@*P?pAr'BCDnqSrt0nqPxPNvqBx*t@WjPN19`@*Bt0BC.T	nxt@*B'rCDBxtuTvrw*p0BTpA'P*BTpjx*PtAr'BrC
n$xo`aQN?"BCDB*v$ruTBjt@*npt0BC0UJt@*BjuTPx*p.trxt	7L*t@*BCDBTpw*vqtnqpArx*Pt@*BC'BCDnqSrt.nP*xFPN1CDnq@t
PNt@*B3w'CDB`
$Z $  *    $'Z   * $  

ff




ff





	
 



ff



ff

fffi 



	 




fffi 

$' O  
   
 nqw'CDBC0rx*pVNQPC.n$x*Jt@*BjvqBNQt'BCDnqSrt0nqPxFU*nqBTv'pjt@*BCDnq@t'BCDnqSrt0nqPx




	
 



>| h 9
 } *z3~>|yz  TD~q} { 
 | fh^ } z  
 | |{>|1 j 









/.10 $0 20



3

4

"!$#

65

!

%#

7!

&
8

:<;<=


0



	
 

J}s~>} {T  9} >\{ {T|y| |yz 
 z h e  y A
 hk0A e  
'#

5

&

(
9

*)+

$# 

,#

0

-!

fi>?%@ACBDE?%FG?%HG@GI<J6K@LBGJ
MGFA"@GFGNODEHG?%@LBG?PACBQ%R%?TSTJ@GI<J
K@LBGJ
MGFA

UWVYXZX[
\^]T_T`aTbcedgf hOikj6dmlonqpsr%aTtWikj6dmlonqpuf h(vwsx'yucqzGr<{'cEdgf h(vws|s}E~Lydgf h(v'Zc<rmG|
^]T6v`Erer'To~%~%%%x'zTc<avw`Erer%TC~%~%%%x%C~WzT`b<rLocE`~P'{P`~PTY|1_GGGG~%Ccv`EaT~%
rr%TC~%~%%%xzTc<avw`maT~%rr'To~%~%%%|_T`aTbcd

f hvwsx`e,~%~LyE"~PEzTc~P7c</-zGrL

zTc<7ccP`ooraT`ocEoc~LP~PGaTt`aTor%aTbc~Ldx%TbzzGrL1f h(vw|mzTck_GGTGTC`~Ta
EzTc~Pc<xzTc<ccT`oor*tGc<7`{rLC`~Ta27~P~LErbr%TocxTbzzGrLWvws|_T`aTbc`
P7~TGaTtxTCWrLo~cP~PGaTtxo~yczGrY{%cvws|zT``T`c-zGrL~PaTb~Pa'rL`aT
oc<o~PnW|
cEucr%ar%CT`or%7oc<o`anr'aTtckGc~PTrL`aTct*7~Tg%7c<TrLb`aTc{'c<7Cc<o
`abr%TCcu`aykzT`b$z-`uaT~%`anWx%'m|1}~%oczGr1o`aTbcqc<rLb$zbr'Tock`a`rqT7~PGaTt`aTor'aTbc
~L1zTc2GaTbo`~Pa',27ccbr%TocdxZc{%c<7br'Tocm`a`rLo~r-P7~TGaTt`aTor%aTbc~L1d|W}E~Ly`e`
c<rLoo~occzGrLzTcr%mc7c<TrLbc<mc<a'm~Lkoc<oW`azTctGc<7`{rLo`~Pa~27~P7cTom`a
rtGc<`{LrLo`~Pa~Lk~P

 ]j7<pqc<rLbz7co~%To`~Paooc<`azTctGc<7`{rLo`~Pa27~Pb<r%arLo~c

b<r%o`ct~PT`azTcmtGc<7`{LrLo`~Pa*~P4
oc<ou`a

 xo`aTbczTcWr%cWoc<ok`ar'7c7c<TrLbct'zTcWr%mc

 xr'aTtj,%pzTceCc<om`a^zGrr'7cWaT~%`angj,r'aTtzTc<aTbcr%7ce7c<TrLbct%ptG~aT~%

r%Gc<r%`azTcb~PaTbTo`~Pa~LszTcetGc<7`{LrLo`~PaZ|


_T`aTbczTc<7cm`Wr-tGc<7`{ro`~Pa~u~P4yczGr<{'c
oce~LEP~PGaTt`aTCr%aTbcm~Ludr%aTtrL1Cc<ome`a


f hxZr%aTtzTc<aTbc

r%7cmoc<oW`anWxo~




f hvws|e



`Wr

^ikj6dmlonqp|Wc<aTbc

ikj6dmlonqp1f h(vws|



Zc<-rb<r%aGaT~%c%c<aTc<or`ct^o~zTcb<rLCcykzTc<7cdb~Ta%r`aTGaTbo`~PaoG~%~Lr'7`


'xr'LcWdhj,sj,plPpj,GlpEr%aTtvhj,sj,TplTpj,lsj,Tpopj
7~TzTcmcr'Tc

%`{%c<a~TaZ|'~Lu,tGcor%2qT`o<x<'%%p|WzTc<anhlsj,TpW`WzTcmoc<Coce~Lvr%aTt
ycWzGrY{%cWdf hvxG%c`qb<r%aGcWCcc<azGrL1ikj6dmlonqpEf  hv|EzTcmr%7PGmc<a'kToct`azTcmG7c{P`~PT
c<rtG~TcaT~'1y~PozTc<7c%xGGcb<r%TocetG`c<7c<a'oc<om`a-o~PcEP7~PGaTt`aTor%aTbceaTcctaT~%7crLoc
o~tG`c<7c<a'e{r%`r'TcY|k~PcGr%Tc%xZ`azTcWP~PGaTt`aTor'aTbcj%jPplPpjljPpop~L1dx
ycmb<r%aGaT~%oToW7c<TrLbc

 j,Tpk'o~Pmc~%zTc<qoc<Cx~TqzTc<azTc7cTo`aTbr'Tocy~PTtaT~%

Gcmr%a`aTor%aTbc~Lsd|


azTcm~%zTc<kzGr%aTtxc<r-b<r%aGcm%c<aTc<orL`cto~r*<~1br'TocW`aTooc<rt~uro`aT%c

br%Toc%|k`roc~LGaTbo`~Pa',27ccbr%Toc<xsd`r%ar%oT`Cr%7br%TCc%x1r%aTtw`r_GL~%c<
GToC`To`~Pam,~Pd(y||<|Lex'zTc<amyckzGrY{%czGrL1f h^d(`ikj,elCnkpf h^dws|sEzTcqG7~P~`urLm~'o
`oc<orLzTcer%mcWrLkr'G~L{%c%|
zT`m7cTW`T`czGrWf hd`7ctTb`TcC~r%a`T`b<rLo`~Paikj,"lonkpWf hdwOGc7ycc<a
P7~TGaTtbr%Toc<|1_T`aTbc%x'zTcqaTcPc<rGx'`T`b<ro`~Pacycc<a-P7~PGaTt-br'Toc`tGcb`tr%Tc%x
`,~%~LyEqzGrLEf h^d`EtGcb`tr%Tc`ab<rocW^`E2GaTbo`~Pa',27cc%|

s ZT<

 Y  <Ef h^ds

P7	ff
fi<'1 
%fiY
'P

6,q$YsP7	ff
fiY%qff
d

 <7qe



UWVYXZX[

r%aTt!GcWzTceoc~LrP7~PGaTtrLC~Pm~Pbb<GC7`aT`a^r%aTtd|

ZcEdh

}~yb~PaTo`tGc<EzTcq~%~LyE`aTCrLoc<mc<a'o<xTykzT`bzb<r%acWzT~LykacGT`{rLc<a'<|
j7<pf h^d|

#" $lol%$G`eGaTrLC`r%Tc%|
#" $ lol%$GmzGrLkaT~c<oGor%aTtm~TtGc|
j'&Pp}~GTocq~L(`er%aEc<CGor%aTtm~TtGc~L1#
" $  l ol$ 
j,%p
j,%p

)*)

P|

fi+,.-/1012	3547698:01-/ff;=<?>@-BADC1EF
GH	IJDKLMON?KQPMSRTKQPMN?KQUVMXWVY!GH	ISZ\[I]_^@R`PZbacKQdH1eJ	fgihjISIk:LlVmUMSRon	pqJ	rSIseutwv9ZxKQUMN?Ky^\MSk1zI
H1e{IBKLMON?K'^\MSRK'^\M|pwvX}1ISrSpw}ffeW	twIBW~ISre	v9Ipwv5J	pw9Ik@v9ZKLM:pwvo}1ISrSpw}ffeW	twIebvzISttQR

jjff\o5\	'\c :\i.To@QS:S	ffQ'~`S%TwV
@wV%'\Swu
GH	ITaQZtwtwZuzp.J	f!v9IS1	IJ	rSIBZbatwI]]euvT]TZ\[IZ\[otwISv9vaQZtwtwZzvH	IB1eb99I[9JZuaQ}1ISv9eV]Xt.]T1	pwv9v
KLlVlMjhjI]]eTL9ZhjI]]eLPK'v7pq]pt.e[7ZchjI]]eXU~R.L9ZohjI]]eoU1R.LPZua5Q}1ISv9%e]otq]	pwv9k
LllVUMSR(GH	I[IH	Ifpw{ISveT1[Z\Zba(Zua(H	IIS	pwv99IJ	rSIZuaetwIeuv7fIJ	I[9eutwpweu9pwZ\J1J	}1I[G|p.]	twpwreu9pwZ\J
Zua:eJVYs5J	p7Iv9ISoZuaXKQJ	ZXJ	ISrSISv9v%e[pwtwYa1J	rS9pwZ\JVQa[OISIMorStqeV	v9ISvR#IreJ!eu}u9	v9%H	I1[Z	ZuapqJv	rSH
ezeY!%H1euzIreJ	v9IBpw9Z!ISv9%eW	twpv%H#H	IsIS	pv79IJ	rSIBZbaeVJ#hoZbaeVJY5J	pw9Isv9ISZuarSt.e	v9ISv
rSZ\JVeup.J	p.J	febtwIeuv9Z\J	IJ	Z	JVe	9ZtwZfVZ\	va1J	rS9pwZ\JVQa[OISIrSt.e	v9IR
@#:i9sff'QB@1Q\QV7\VBVSS%:_71ox7BS
 V'\Swffsw  7XS7#qu%SyT'Sff'QS|X	ffw.SQ'1XOc\@
'j'%QV5\'\S1DXK'SMTXK'  %Mj.off'QS|wV
jffhjISXsW1I%H	I]eu	pq]eutj}1I	HZuaH	I9I[7]TvpqJrSt.e	v9ISvopqJRQayZtwtwZuzva[Z\]ihjI]]eL
 H1euoKQeVJ	}H	IJ	rSIBeutwv9Z  MoreJ1J	ZVrSZ\J%eup.J!7I[9]TvoZua:}1I	Hf\[OIeu9I[H1eJ!@kJ	Z\[o1[IS}1pwreu9ISvk
a1J	rS9pwZ\J	vZ\[rSZ\J	v9eJV9vZVH	I[H1eJH	Zv9Ip.JRGH	ITv9ISoZuatwpw9I[9ebtvzXH	pwrHreJW1ITrSZ\J	v9%[9	rS9IS}
a[Z\]1[IS}1pwreu9ISvp.JTeVJ	}a[Z\]9I[9]TvZbaff}1I	HTeu]Zv9(orSZ\J	v7pv79p.J	fZua1a1J	rS7pZ	J	v:eJ	}TrSZ\J	v9eJV9v
p.JeVJ	}{ue[p.eW	twISvp.Jskjpwvo5J	p7IRIJ	rSITH	ITv9ISoZua:rSt.e	v9ISvzXH	pwrHreJW~IrSZ	J	v9[9	rS9IS}a[Z\]
H	Zv7Itpw9I[7eutwvXpwv5J	p7IeuvzIStwtQR:  pwveTv1W	v9ISZbaH	pwvov9ISk	v9Z  pvoe5J	pw9Iv9ISZuarSt.e	v7ISvR 
@#17wV7S\@Q'1%wVS	ST'\1 17991
7'\sQ%_ciiujS9~7SO  %\wSVff
 
 .	
  V
ff 
fffiXK'TM%:'\
   ?
    K'pw}1Z	ISvcJ	Zo]eu99I[|9ZTzXH	pwrSH9I[9]Tv
jffhjIS
      9%           9 
H	I{ue[p.eW	twISv~9~ e[I]e1~IS}xWYkjeuvtwZ\J	f!euvH	ISYeV[I]e11IS}9Z9I[7]TvXp.JXMSR
n1111ZVv9IXXKy%M:
 ff%99	R:GH	IJoK'7XM@9\RhjIS  W1ITeJ!hn
ZuaoK'%XMKQJ	ZV9IB%H1eu!
 ]T	v9TW1Isa1J	rS9pwZ\JVQa[OISIMSR#GH	IJaQZ\[IS{I[OY 
L "$#%"'&jkH	I[Ie[
I (*)
v	rSHH1eu%
 %(*),+
 -)RBGH	pwv]TIeJ	vH1eu
 %(*)./+
 ).DeJ	}xH	IJ	rS
I !(0)1  i
 -)2kjaQZ\[oIS{I[Y
	L "/#3"/&jR:GH	I[OI%ayZ\[OI  DXK'T9oMSR
n	p.J	rSI k1z5
I 4	J	Zuz a[OZ\]ihjI]]eLoH1ebrSZ\J	v9eVJ9ve1~Ie[p.J	fp.Ji]T	v9eutwv9ZBe1~Ie[
p.J!RGH	pwv]TIeJ	v%H1euXpwveB6n 4bZtwI]_v1W	v99pw	9pwZ\JaQZ\[o_zR`[SRRj	RXGH	IJa[Z\]_hjI]]esU
z7
I 4\J	ZuzXK'T9oMX  skH	IJ	rS
I    s	
R 851[OH	I[9]TZ\[OIkjv9p.J	rS
I ipwveJhnZua(XKy%MSkeutwt
rSZ\J	v9%eJ7v:p.
J ebtv7ZeV11Ie[|p.JsBk	H	IJ	rSIeutwtffrSZ\J	v9%eJ7v:p.
J ]T	v9e1~Ie[|p.JBR:GH		vpwveutwv9Z
e6n 4uZVtI] v1W	v99pw	9pwZ\JayZ	[zR`[SRR~ \RGH	I[I%aQZ\[
I i _WYhjI]]eP1R

dZ\J	v7p}1I[ '9KQ:ff;M=<>9K?:ff;1@MXeJ	} <A@KCBXMR!D|ZH eJ	}x p.]	twY#H	IrSt.e	v7I

 9KQ:ff;ME<F9KC;1%:	MSG@KCH%MSRJIZuz J	Z9IH1eb(X9KQ:ff;ME<F9KC:@;1%ffMSG@KCBoMjeutwv9Z
p.]	twpwIS
v BRGH	pwvcH	ZVt}1vayZ\[|rSt.e	v9ISvpqJfIJ	I[9eutQk	IS{IJsp.J%H	Ic1[ISv7IJ	rSIZuaW1ebKr 4Vf\[Z\1J	
} 4	J	ZztIS}1fVI

LNM*O

fiPRQTSRUGVXWQTY6QTZ6S6[N\.]*SV6\1^6YRU=S6Y6_`WZ6QTSV6QUGVbaTcTQ-d-\2S6[N\1]0SV6\1^6YRU
e	fhgi-jk-j*l-mnojNp7p7q%rostujNvxw7yTjNk-jNvzq{n}|~6-mr2kmi-rosszj**mzrokffj k-nowk-j*j*mi-j	s6j**r2qnNqsGj	,i-jNvxj
 Tq k-qTvxj?6k-*mzrokuCvxj*jqTk-erosjNp7-mxwTfbj5k-j*j*mi-j,yTjNk-jNvGqnJNqszj mz!6vxtTj mi-j j*lroszmGjNk-*j
q76rk-j**mzrokf
hXX 7 uRGNoTu***0.-T  TRTx	*CTTTxTK76T3TRe
G!*N!NoTu**Nff2 	 TR% 	   .N /  	 
JR6666Tszj 	 qTk- 	 |qTk-noj*m! ~6j7qp%-6j*nhe N f%-rk-*j 
qTk-bqTvxjszmqTk-Rquvx6ro*j*qu6qTvxmN|6mi-j!*n2qT-sGj  rs	j*6-rotqnojNkummzmi-j=}vGp!-n2q ! C 
.,i-jNvjff ! 6jNk-Tmzj*sffmi-j,6k-rotTjNvsqnonow66qTkumzrRj**nqu-szj % fhgi-rosp%jNqTk-smi6qmrosqp%-6j*nR
 -vqp6j*n{EfJ6vxmi-jNvzpvxjT|R ros	qnoszq7p%-6j*nEe=|6sz7rom}Tnonos?vx-pe    
ve    mi6qm  ros	q7p%-6j*nfhgi--s5e    |i-jNk-*j   	 fb
 ffjNqTk6vxtTj	mi-j%j*l-rsGmzjNk-*j%qTkffqukTwk-romzj%szj*m	`*n2qT-sGj*s	,i-roKi*kumqr2k-sqm
nojNqszmhk-jk-kumqT-mzTnoTyT--sqTk-=?6k-*mGr-kuC?vxj*j*nqu-szjTfk=q*mjffNqTk6vxtTjsGp%j*mi-r2k-y	szmvk-yTjNv*|
k6qTp%j*now7mi6qmffmi-rosros,q%N.TTzf6gi-rosffros,qTk7hmi6q{mrosk-Tmffk-now7r2p7-norj*|J~6-mq*m6qnonow
*KK-%~uwqTkTwTmi-jNvyTjNk-jNvzqnoroNqmzrokE
u2NRJj*m`~6j!q*n2qT-sqnn2qTk-y6qyuj!qTk-~j7qk-romzj%s6~-szj*m hfff5k  
r2kros Nqnonoj*qN.T3Er2k|6r / }vj*tTjNvxwyTjNk-jNvzq{nroNqmGr-k  b/6k-6jNv
r2p7-noroNqmzrokJf

Tmzjmi6qmr!rosqTk`{=qszj*m7*kumqr2k-r2k-y`qm7nojNqszmk-jk--kumqT-mzTnoTyu-s%6k-*mzrokuC?vj*j
*n2qT-szjT|3mi-jNkX~TwjNp7p7qXros7romzszj*n,6k-*mzrokuCvxj*jT|ff~6j*NqT-szjrom%si--no`r2p7-now`mi-j?6k-*mGr-ku
vxj*j*n2qT-szj.s  rkfRvr2k-szmqTk-*jT|  CR    C       rosqTk`{5

 	 
 C ff F C 6  fi  h  z qTk-   CR    6      fumzj	mi6qmmi-rs	
ros,6vxjNvxnows6~-s6p%j*~uw7mi-j	  
 	K *-|T,i-ro*irs  CR   C      fi    f	k
pqNwszp%j*mzr2p%j*s ~6jmi-j	jNp7-mxw*n2qT-szj,|}vj*lqTp-njrE     fi    f
   **" !{# %$& !T( ',* )+%? ,b Gb/uT*T u. --/ -- 22
 -R
0 1C*Nx,NoTu**JKxT T	/TRCu1u oKTR	T&
 1xCTCufi -Tu*NC.T& 1Kx,oTT* 
..-Nx%/ 2*.7N.T4 3ff	x,.7
JR`Jj*mff  

 	*% 565%5z 76,~j5q5k-rmGj,szj*m3*n2qT-sGj*svxp|us-Kimi6qm*kTmqr2k-sqm3njNq{szm
k-jk-kumqT-mzTnoTyT--sff?6k-*mGr-kuC?vxj*j*n2qT-szjTfffbj	NqTkq{szs6p%jromi--mnoTszsyujNk-jNvzqnoromwmi6qm,
*kumqr2k-sk-%mqu-mzTnoTyTroj*sNfhJj*9m 8~6j	q& :{TnojNp s6~-szmzrom-mzrok}-vh.| ; % < 	 6 5%5%5G <>= 5~6j mi-jmGjNvzp
szj*m	~u?
w 8hA| @   B 	*% 565%5z = ~j7qszj*m	tqTvxr2qT~-noj*s7qTk-D
 C  	  N6 5%5%5 ~j7mi-j%szj*m	
qnonyTjNk-jNvGqnorNq{mzrok-s	`6k-6jNvr2p7-noroNqmzrokr2kEfhTmzj	mi6q{m  C|6sz C ros5k-TmjNp7-mxwTf-r2k-*j
jNq*i*n2qT-szj	r2E
k C p%-szmffr2p7-nowmi-j 6k-*mzrokuC?vj*j*n2qT-szj.s  r2k|-romff}unnosffvxp'JjNpp7q	mi6qm
 H   	* @  H    @  565%5urs
qnon p%jNp%~jNvxs C qTvxj 6k-*mzrokuCvxj*jT4f F3wJjNp7p7q G6|6mi-j	szj*m C 6I
q=k-romzj,sGj*m{*n2qT-sGj*sNf-r2k-*Jj C6rs3k-romzjT|6mi-j,szj*mff H  L K  @  sffrosq{nsGk-romzjTfERvEszr2p7-nor*romxwT|nj*m
 H   	K @  6
 5%565 H  # M  @  ~jmi-j	szj*mEq{nn6rsGmzr2k-*m H  L K  @  sf
j*m K ~jquk! H   K  @  |0}-vj*tTjNvwJ NOPNQJ|s-*imi6qmhJ 	*% 565%5z M qTvxj3szmqTk-Rquvx6ro*j*
qT6qTvmNfv j*tTjNvxw`

 NSRTNVU |mi-j%mzjNvGpszj*mX W~u?
w 8Xros%szpj!sGj*m % < WZYN% 565%5z < WZ[z]
 \^;|s-*i
mi6qJm _a`IRcb*f vxpJjNp7pe
q d6|Jffj%i6qNtujmi6qm	 K   W |6Cvj*tTjNvxw NfONfQquk-  NgR]NfU|
h%i%i

fij9kml.n&o&prqtsfiuvwo&l.n(xzy|{}le~I&6

r%r/L9 |LLc^/JVV"%%/#(r%9&6. ^zB%eD.&rrr
rJ%cfiJcff"%
r%ZS%rZ
 &Zc.&cL^#//Affc#%cmrLrcS&c99#P/.%
f.
 r/.%ZET.r%Z.TZEVJ&/Ar/D&L#  % JV#   /rEr
  .
AXEct 9  /9L#fi/wceX.%r%fic%ct# *  L&r&%*&r&rB
r%r/*    r%t&cE9mr/  

#/r/&r%r/.}4fim/&mc/?&6.Jr.cLmr(
B%B(6I //}"?}*t"%mtZ6rT.mBA/&cA%9J/r	ff

fi 	 %   6	}%!"#Z/	   %$wr&Z	'()/+*,-.  +

B/f&etrefi/Jcw>&r/
 0>Z/e/m.rfi/m   rT/cmrJ.r./%
 .r..A&AffcL1r%Zfi./L2(#rrcmr/].&//mrecB.rfi.../
.
>Z 3wr#&Z/rrLr/%E.r%#LX&//mcAAXc 2 49%.Z.&r9*fi &//mc
AXwI/ wm  


/6B(

 rE/Zm.I]rermcLmr/D/.%grrrIrD&&%
65%&.rSm%&Z/r
 m.r&.ELrr>&r/&&.tr (rEDr
/&cr]r&&%c
c
&mre&r?cfitrrJ%c%/AS6]r/.%Zr//fi&mr/.]]mtr
r&%
cr
 0>/&r c%?/m.rfi/%  r]6I/%
8 7c &.c&ff
 9B9mr/Tr%ZT.]r
tr/].%c/Z/9c/m.r/&cP%.;
 :&%&6.&" <A%Z%.fi>
 =m]   r&&%9ctrfi
5%&.rEm%&Z/r%  rrL49rE/%&4rr&&%9cr
 0>/rcc%.&r/fir
 0
>Z/#/m.rfi/4rr]c&trfi.  rLL&r&%Zfi.r&mr(&mr/J..%cBmr%c.r
r%r/c..%c/&rcc%r/%JJ&tr/DmE% #c  %&/cr%Br.
 ?r
 5L%fi&.r
m.fi%&Z/cfirr%  rfi.r/.}&trA
 @  C
 B D  DrL.rE
 B F  F//H
 GJ
 IffB K   KfiX&H
 z
L   
  eN
 MPO  r%E9&6.J G  Q
 I(@
  @SRTO& G    G(U #.r G(U V
  Lf G r9/.%ZW
 Mf.}/
 c.
&%#.cS
 X %&rr 0Y4Lr%r(&
 ZZ\[/
Jr.r%m%Z/firD/rfi/r%r/
c  r/Z%^
 ]
/r/%r/  0>c& /rm.Zff
 0
(r/
r#&&%ffrwr#&trc/  0>c&&/ V#X&//mccfi/
L}r%JJ/%&cLJ/  0>c&fic/AL.E/m.r#r/r/c
r/S.r
r%r/Erfi #&_ 	` emr/e E&//mc4cLL /r&% a
 Iff@bO#r%Zc
 @
rr
 0>.r...rr&r/
 0>Z//m.rfi.  r%D
&//mc9A c*Sw..%c
/&rcc%.JfiA
 @
 dDZ/c.%fir/Xr#V&X/ e 0>&c/AJ/  0>c&fic
c
 @]
B%B(6gfC,) @
E @ 


8JSh!Zij#>(6ekl	ZX& T$&&ZX	'()/V/& _m/.>\

npo rqAstT9Eptvufft/wm
>r&Z/rX&r//4&Zc./Dr 	
 'ff8/&
 } cL.DA9m  c4/.%Etr/gc
/m.r/J/cmrmr]c%crr0>.r...rrL>&r/0>Z//r.J>rJ&r//9
LL/rzr _ `yxzr	
 9rc.Z&EP	
 9rc//>Z r
 i   c#r/I.Afi  r]r	
&Zrccr&Z/r//B/&r/LrJAXwff

{&|&}

fi~j;H!!!&	ff!e!6!!H!ff!jv>>8!&eTff!e!

_b)()ff
zz>	bff!	8>	&	ff8>8>ffPi&\p>>m_>>>!>	ji>
		8>j
c&Wp\%8
 H&VffHff>	mJ	8>j-+_>"Wi	>\>ij&ff
 >W  !&\iP>&ff8>>	-e
 H	W!v>;!V!&\!8>	E&Vv8>	8>!8>!>&A  8  -	
Cff	&&kHy	_!ff>>V>!V!&ffH>>!V!&
ff&AH8b>&jr	H>j	W8b>!.ffffH>	&! 
 H	Wkv>gez>ij  	ffAff	>	W->	&!v	>>>		>
!	!&ff	>b	>8Wp>b(.>i	8S
 H	&&&	k!>	Hffffp!>j	Hffc
 H	A!SWPb\H!y	& Vg H>	SV&!V	S!>	
b8    ff.i_!b
kH&Vff. ff&&&_V>cff	>	->	v!W>>i;+j>	&ff#
Vpm!>	.	8  >>i&ffji>WH!		>  >b_
!		>.&Sff8>8>c	8>	(  &Y_ff
_!&
 H	!>	8>fiv&fib 
 
H>S	>		>	ffH>ff>!(ym>c!.ffHff-H>	>& !ff8j>	!ff

  r&SS  !>&V>+>W!\cffHm_   V&ji>>	g8 >E!\ff
H>	.& !zV	i&A&\>V	V  &&T  >y>iVV&>!ffy >	&  8>V	
6#&V&>	;8v!\!\.>&iS 	>8ff#& 
 c>	v_!ffH
> AV!>_!V_m  >ffff&&H>&v6 ffH_!ff>Sp"!r#6$fi;&%fi&VffH!ff
!>ffH!>!V	V>	8>b  fi&%fivff
!ff!\i! .	!>&'>S>&
!>p(!QV_!		8ffpj!)!J>E V>-k!>_!V_m  >ffff&&
!!!k>+>!V!&ff!j>		>_jp8  +*>>+>!V!&Pff!!ij8>	ff8>	8
&  ff>-ff>!C+E,z!_>&Vff_!!!y>&H+-S!j>	!	!&ffHj V!i8
  HV	_!			.k	&&.0/%y>&V>->!V!&ff!ij8>	 \V!ff&Vk >V	
/
m _>	S	>_j&pff8>	c>!	!&ff	&ff1H32 54p 6*7N 98: >v>b>!k&ff
!8>	i&jffb!ffV&kbV	 ; 1 y>W	S \!8>	b	8>	->	&k
V	m >	&\iH>k(&j	+\%>Hj	Hff&ff&h <ff => ? H>&_>	
#&&j	kffffp!>	yff% 	ff8> A@ B,@   =DCFE G V&V!&
y>>+\i! >8>	c!ff>&Hc	&-H>&ff  PgV>Hc	&
ff.i_!JS I&>!>8>	H8S>&ffi!!&!!>!>S>V>	S!>_ff
ffbpiff8&ff!<!> I!!&>ji>>E>-!>H!y>!	>c>+y	&_>	i	j&
> Ij&>	ff_>p Yff!r!>		> >-_>		&ffp!H!\+_>pff
z>H	ffz	8>j		ff8>8>ff&ffj>H>m>>m!>	m		>H_p			
	S!>_>
 K>zKNMPOQpv R(S8TN(U- VXWZY[!]\^Y_`%a	_bdce_dafa	_gdhibj_ hiYlknmpo rq5c"_tsng(q[uY"c Y%[+vFo
J L
`%a	_bdc Y cwo xFvy +z_g|{c}`vg|[u_q~gc_[aY_lc [wvg|Y)g|vgAF[u_bj[uva	v9hivdbcno big|`%[uqvgAoexFYYff`%a	_bdceYz[iYg[jY
WnnmvFoqgV]qpc`vyfbi[u_j\ a	Yk
%%

fin5AAjf^9wA(|)A

+(|(&+t(AfA|ffww|wA|
nA^Fi%09n%j%9l		%l9	ijAjA%j	%l^j<9iF%l9 ^w  0	%l^jj%n+5jj
5j	%l9"d%j%9l		%l9	ij09 9"0 5j9 "	 jffi&A9l ^ ,"&jj%"Aj
jtjF%l9 9A  5l		%l9	iL|j	 ff	njt(Al|lj%09w%j%^l	%09	iLA	tj9 )AFA	ff d% 
 %Fj 	 9%jj"A,0ZFn" 5j9 tdjff	inAj9juwAAAAd9"Al+n"A%dtij
Ad9	9	" dffj	%Xj)Fnj jl9	 j l j0 j<AA9Altw5ffj		 "%
j  Lj	)"5ffj		   jX  tj%		i"d%Fn 	ffAljffAl"j
ff^9	5ffj	 <%+AjAj ffj5ffj	 	j%  i  ij ij	ijl5< i^F  
  0	%l^jd5jffjAw0$  j&  
6t	jj	ijtFijAF jjj9  ^jjA0tj" j^9%j l$d<lw	j	9AF
9 9j		L+|i, 5j9 nj	el	L idl5Aj 9	ij%jjt j	99%j lff,n9^	Li%
Aj 99	iL(j %jwj%l^j%D idl5jtln	%l9nij"jidj^	ijAj 9	idu6D 
 5j9LjAFjltlFi#Dj5lu"tln % Aj j^9%j lffA	j %l9	%
9)ADdwl %A)Al$fj^^ 	jdltjAl$(A 	<jAj	i<l
j" 5j9 5^%wj 9D"9j(DA  &AF
 t9 jwF%l^iuiLj	,A	99Aff F	 j,0wn99%AleFij 5jll9 j9

j& ij9	A%j	,fF9FiDA%d	ui<i%d%"jfDFn)9j	9dFfF9FjFA%
ui9j5l%,j%&	%XA%l9		jlAlj 	%l9%j%9l		%l9	iX	 9j9 
  j	
 i 9Aj 9	ij	 <j%,jj^A5ffj		 ]dd&j% ui9j5<j	 5ffj		 
Adffwjfffl Al		jjniF%l^ 9+A  5l		%l9	i	 Dj9w "L j+	j	 dffA
jL(j<j^5ffj	dffj%j9jA0w	t5ffj		 d<Aj<flj (iAF A







	



 
fi 

+	iAF








 






ff








fi



 

L%l9td%j%9l		%l9	idjiF%09 9nA  5l		%l9	ifF9FjFA%d	

lAAAd9djLtDt 5j9 %wj%ffdAnAAFij	%5fjA5jffj"
lnj)L tjF%l9ii,j	Alw ftjw  5j9wj99%lff0j^j Lln
A9fj)9j	%l^t 5j9"j	 j	 "A dj9 djLlwAj < dj9AA%dF
Aj^AF<^fj9j 95 %







"Xjj%Ajw5%l9<l"A  5l		%l9	i j%F<	jXAFij	%<t%F<n<%l
  ffLlA9j95j  ffLt	t Aj	l	%d^ 5j9An"AjA	"jAj	%DlAAd9(%F

56Dinl+ 5j^AFi%F	j<f"dF^j(FA	  AdF%jj%jj9j )L
	t Ajll	%9j" 5j9j	e	tjAjjl$tjL%tj	n60 tltj9 <5jADil
lwL%ffff A
AAA9
 
 ij 

DdF5^<lj 9& 5j9 <j	 F
9dj(FA	  PdAF%fftj% %dF	 3 

	ff<Alt95j )	"u		lt"Fi
L%ff
AlnA  5l		%l9	ilAjA%w5ffj		%l9	i	5ffj		 d)+tijnn"A%dtj
u	lt5j<F j	









2

"!$# %'&&'&(% *),+




.- %'&&'&(% .)/! -10 &'&& 0



3'4'5

)-

fi67ff89;:=<>7ff?@7ffA@8@B'CED8:@CGF@?9H8@?@IJ<>A@7ff8:@79;:LKffMff7,N,CO8@B'CGDP8:@CGF@?9
Q*RTS,UVSWYX[Z(\^]`_EabSdcfeSgUdhjilkm._nc=o1prqs'tfovu(sxwzy'{|wff}~wff{T{|wff},ws*s'Hjs''sEts
J otEss2sP~tE~wj  E.o
>,l@,|,.,,',ff|@*,Off,;.OJff','((r@ff(|',Off.;lH@,(|n
Off,('@,';*ff@ff|x@ff(ff(,>@Ozj@,(|jOff,	,@ffjff
x1ff'.,|(  ,;('>,|j*'ff,>@,(^`@,(|Off,;@@(@'
 |ffff|'O.,||'(||O>*O(;ff
 TU V'UTGV[[Z(@bb'_GeS(ab'VebV'S=Uhh(ceb'_OU	chV'S,SLeffTfaPSa`c	SVr xp },y'tEffss
y'{|wff}~wff{{|wffff,}wPs*ffffs'suP.E>T{EywfftGg~w{wttGys
'(x.^(zOff,,@ffj'ff@,ffjff@,|l,jff**@*,rO,';
 Off,Off,,@ffffd(O,.,2@,|>;zOff,(,,  ^(zOff,;.|((j
'ff'ff*,@,ff'*,|((`>;.Off,(>@>x@Ol^''ff,(
,*ffOff,(j@2,;g;,@((^(Off,(,L,ff@ff2lxO
ld,|>(O*Hn,,2@'@(|ffx@O||'(|rO,(;'lff','(||'(|T
 OT@|j .nffn(.n  .dd(.d @j .(.d ff,
  .  ,(Y. *>,'rH       ff,r        ff, ,'*|2,l^(
Off,(z , @*   '`     	j   ff,L    lz^',,'|.,gx@>
 j (   Ozl

	fiff		ffff!"#	fi%$&	ff('	*),+.-#/ff

 , |';,L|;,ff'@ff  @.,(|T  , s'{|wfftE's  ,|';,L|'L*.',
 ',(  0ff,@,10,,>|@ffL'ff  ,; (J*@j|j((2,(n,@@';rff,
O(|,=, ((|T>|^@;',(|';2','(||'(|,z@,@'*O.,|'(|[O;
(L;*zff|ff'   0,@,2
 0,>|@ff
 3  5
 476   J,g,,.((| '*'((
|';,,2@,@'O(|ffjO.,||'(|T
O;',>||`@ff,j@,|',  '@@,;,l1O(|ffjO.,||'(|rff,z
@,|(|ff|ff'  9
 8^  ; ; :,<=>= @T :??ff ^jff|ff>,ff||^, @,;,l`  @.,;,
O(|ff(l   0@,@
 0,,>|@ffA
 3  (z@|((O,,| Hn"@,(|r  @.,(|T
j>||`'|,5
 BC8  @.,;, 6  DFE
G >
 K ~}u~},*s~ Off,(*>|z@>(  0@,A0,>|@ffM3
S Hc_Ob'_UJ
c IX Off,(jL
,'x|.  ;(|,(|N.,@
3.O  PNRQ$ * ,'!BCQS6d|j,O.,||'(|@,;ff
ff,
 BCOT6@|ffzff  |;ff.,|(2@@,@  
U
V

V

Y

VU WUa'_b'_U.
c I q st  w  u(s*y'{|wff}~s~jwff
EtElsP~@sytt 3  Z   7[  
3

u;s2w ~s'ty'{|wff}~sP~Pxs XK ~}@u~},2sP~ 

V UTUh ,;@',jff||^,(|((''ff(,z'  ,,|''
;:' L8  @*  >|@^(\3j
   >,'|*  ;(|,(|]N,@^32O  PN7Q  
 N,@^ 3    P
 N
Q  
(?ff >,'|*  ;(|,(|]
_`fiacbFdegf;h(hijh(hlk(bFd,mnh(fpoqhlr,bgos,fut,vbghlbgwxvRh(wzy{egw|hl}bFv~hlrwmry{egw|hl}bFvRxbgjf;oci#ki|hlrnfuksbFfukfuv,hsftv,bghlbgwvPwkfuegijhlbg|f
olm,dnolm#hlbgwvRbFvyegwjhl}bFv~;_du~i|ofri|f#o(fpfuv
bFvPf;;hlbgwxvR,`
,,

fixfi\

l^,MR|x|x]||n^%,R> .%
(,M .%
(# 7A]
;,(!jCC((!S|x,]n;l!>1j&7fiC
^,A^lx.(>c>&(.ln^;xRM{^|^,;5|xx|x
,l;(>cn((>jxz#|x
xx|n,R,;^n,>,
 ] 
|Mjn,nx5n 7*,;PJ,p>x,;\|5x,
ncMF^  x^];MC>x,;|7Cx,RnTM7F^  F^%]
|,,|R 7 ,>|;^AjMx]Jn q\7,;\n,#x|7|;&5


,,x>x,cj\cM  xC@nxnMAnC>\x,  R^|n,;

|n, 
  ]S;7>|Rx7xn
 
	fiff	>x,Ac52
 ;nx7xR
qx,
   !#
 " % $&
 *
 '|#
 "P    !
 (*),+9 -x
 '%  $
 (
),+9% -fixj  7^Cq|n.|zjC.
 nAc5.;nx5x\0
 /
 x]x 73
 2xAjn M >|n!|]x,x4
  !
 {x% ]c5n1
|,|4
 5,xx!,x;|nC\
 6n7
 % !T|x!\8
 6nzn|,;
x,xnnx9{MPn A,M^#n  " xM|,!^]^:
  97] " 
  $#|,;
  97]  <
 5,M ,^|0
 % !*
  $
 R; >|n4
2n >
 =2 |>]9>,; \L]
  -@
?
 (A+9 =|,J
 7
^C
 B#@j]jxnM2c57#zAj ]
 7!1,2>@|%||C
|,A|,;RRxR,Cnnx; %MnzRx#
 D!%|jR!^^
xn
x!,{],;lF
 ER];xx x,|R; 1AC

55 %  !8
 (. -fixG
 '  $8
 (. -fixH
 Bz^nc|^xj|R\{,x

|5||>I
 =fic|x,xn*
  !,
  $,
 5^,J
  9  !njxnx
|Mn|x|n7AcMJn
 ;nx5xS
 /
^,,xncjPc5.n<
 ;nx5x\.x,*
 5,7,,x (]
|{c,nx
!|7cML
 K8Mx,^^,xC7n nx,nfi>x,;x  
L &
 N  'OPOOQ'NHR
A\!x%xMnS
 T	ffPU	VWXZYT[8
	fiK\x,xC  !,nxjR9 
#jnC,,|>;RCx,nPl(FT
 \.0
 )H\|,^x9{xx,,
 
 'OPOOQ'jZ
 ^xnc>x,n,nxnq |
] Cfi|1

x ^,S
 7LZ
 _M `L,M   Z
 _5W ` 
 Z
 _H(a)^ N @b OOPO b NR

` 
 Z
 _,(c)HN  (COPOO(a)HNHR5 C],\jn9>2c5RnM|%x\M>x   (
)HN  (>OPOO(0)HNHR7 'POOO'n( ^ (d)HN  (>OOO(d)HNRjMnxAAc5Sn<
 2;nC>Mx7 R
 e(*)HN  (fOOOg(*)HNHR |>xq5x ;n|c5Cx >P|
#


5,MA|
|nq,nx{|McMJn ;nx5xSxA
|>C

hHijlkmnfiompPo>qHr@mstunwvutyxnfiotyz|{Sp#}<{S~<mfik>mfivynfiotImlP r<vutysnfiotyz|{
 A|>x,;AxAj\xx,An  FM* Cnx,; >\x%|
Cx,\   F7]q|R;,xx*|] 7 ;,C5|xx,n

  F
 / Mx,#xR|^ x!CC>zun6n5|  A|7  |
  JnR
xM4
 
 /H
 xx^|RAn|\l|fi;g ;|#x
d

ZTwPFQIfifi.d| /Hl|V/3Q#ffg8M8	Y
TYT1	fiKMCA/71
	
P*7	fiU.a/H
P[&MM>*fi4gZR	/7


fiA@Ua@
lTTUGU
 PL8Q#cPPQXZTZa<HCgXT
TfgQf8TQgg[fi*1QX.XT1TQ
1
>381QTglP@1;L I P    fiXTZ .>
	,

& fi
 ff 
 XT X.Z< .
 ff T
 
 	I
 Ty
;#
 V*1 
   ff 
 T :g
   <aQ Q
 !C
"
#%$'&(*)+,.-/102(*)
3y
 3u* 4,XTZX d8TQgg[
.XTX#TT|#TggQg
I8T   Q Z T65
Qg
'@T[*0#  8T
ZZTXS 8T
ZT
QT
7@Tg1 Z
QI<Q 86 9|8.
 QQ:3u>XTg 8*#Z lQ
Q#QgZXXZ84
XT@ 8TgQQTf
HT
; 5y 8TgQQT.Q<TgQg
T@T TQQ,XV gTT4ZZ
XTQ*Q 8
ZQQP< @T*
TQTZ<TH; QQg
Qg
H##gZ=
 ,> Tg?
 8@ T<
Tg
# TTlTB
 A TQTlg#XT 8
gQQT0T1T
; 5y 8TgQQTd gQZTQggQg
aT
#TggQg
> T*QQ@ITQf4TggI
8XyTQg
0 
 8P
,>
 Tg
 d#; CZ 8 Tggg0XTdXD 5yE 9F*a6 9|8#TQgA
H
 G. XT0
T
T|  TgTA
 QfXTgQc1Tg>
XX TXTQg
I QgT 8gl
Q4Q TQI
  Q  #TggQg
 g1QQgg
 Q% TQg gg@#T1QX> C
 CT
TI
 CTT&8. 8Q 
P
 G.XT.XTT|*48Q.y
 XT.X; T.@
yIHXT< 8TgQQTfgQHTQgTT<TX TXTQg
Zg<Z
*QXQ <8g
#TggQg
T8g#TggQg
g<XQZZ
 4QXQ <#Q  TgT
 J

X TXTQg
g4TgQg
T@PT@ 8
gQP
 Jfi
*gg,#TgQg
*.0
TgdE 
XT 8
gQQTZ<g4TgQg
T.0XTZQTZ7yTQg
; 5yZ 8TQ
 K.T
 Lwg

I  Q @#TggQg
gQH8TQggQg
T7TTH  8
gQH#yTQg
; 5y@T

3yZQQg7T<THX|**TH
 <g<g% 37STTQ*Q.TQ@M
 .QX 
T@TQg

SXT0@Q .#TggQg
figQ*TQgTT4
Tg 8
gQ '
 8#gZgQd% G.
XTXTT|@1*d@8#QA
Q8Q0 g TgQg
Tdg@T 8
gQ**>

#Q0Q#XT.*> CQ*TQgg0
 NH TX#TQg

O

+>P%)(RQI,.SUT%V<SUWSX)ZY>/

lf*
Tg g[CfQ1X8C=4,Q3%QX5.K4ATgQ4TXT4S
HXTS
#QP4Tg
T>Q## 18.XT1G
\

S^]_SX`;SX)+SU/

K.^a#Lcbde>e>f>gh*QT0Qg
figQTT8
gXZ*3u'ilTgQ
TjblkH|mgRn_oXp;qsrut
 X>Z
 n {  w{  Uf>>f;E
 >f
K Zg<
4 QP
vxwzy{:|*}~sw rfi }~ d> w o ~ ^H


Tfi   5[* @   5bde>>;g=hE
K44<'a.g

}> w r |*}~^w r'>oUp7

{ r^>o w r_   {D}  {  }>yw o ~

a.I@P7H@*QTTT
Tibde>e>>gK XT
 l4TX@gQEH3y } r D{ { p w o ~>
}dv  {7E>v n_o vx{ _oU vxw} oX j}>w o v  } o {  { oUr {} oI vxw  r w   n_o vx{w6~s{ oXr {= n  <nt D  
>>h>>
 E>
i 
 
'.
 8%
>



fi[>^c>XUH

fi; shX@d>>>^_=^^;^[=^  E sh_>D;xz>dDD>*xx*x>
>c;>>

 
  	
ff
fi^>[ ^>  >
 _>.[^ c *d>>*fi^E   s7  ;^?^  ^ E^h^_U>*dfiDx 
!>U_Ux6sXD#"uD>dff$s&%>('*)>*^' >
 ^[fi
3 E  4 ^5 ^:[ >^s=  =s^6E sh
+ ,E 	
>x.-fiZd>>;0/1^E2 fifi
< E=^  >>?^ x>>xzA?
@ >xd>xz;X.B%>x fi*_Csd
 dD_67>>;d78$s:9&;E
D >xu:zAE>8$^>xXh<;c;>Fd [fi>3 2 G%B d [U3 
H IFK>J fiL: NMOPJ d >	;
 xh d>^' %_K7^ x8Q *sfidsd>>SRUhT Dffs$ U=VIs u?>K70W<c<  _X
xz>R.
Y   [MZ *4, s^*
- Ffi
\  
/ cd>) >&] >1c<  xXu_[T^^$ _>d ;N=
7 : >1<_s xdsd>>1` zK>7 z;aT^s$ b
H >Ffi
>dfi
@ D=Q *c  %d>8Q d?Ee z>*fh =
 ;	^  N ^> d  4=
 .\ E g * ^ RSh i
 ff> 
H  s *fiXj fik Xd;>>l*` s K>7 >x>d*^zsd>z;x^s^'  s Fd [fi>3 2 G%B d [X3 
h  [h

M %En^ ;6>
 x H   >>>u> ^ X>^  [4 fi
 	^ a^ * ;. ;^I[=^  E s
m d [FE
 , 	;
 xfUj [.
Fd s^ 7   dD_6>
7 >; d8s$ >%;% 7>
 XN^ >pofofo]9&1; 1< E8^  >qj` s K>7 >x>Id
 >1<_^ x?h9 U_shXXE' >) % DF4X3 

  tMuY >&k @ >>>u#:
v Xw  	u7 s[ 6V^ [zx>>K7:
E X
m *d rU
  + [&ksd sx*h[ +  i> &Uj  :
ffs
$ XjD>DX>FR?T^s$ D>6;x.
E 8s$ IE7 :>K7dy<h< D>xz;<E fd  H s^sh
m fi3>3  sh>hX >>>u  ^X^;  >3   Fd 3fi >=[fiX3    m fi3;3 sh>hXC%Y *6>DN7^ x8Q M*s
d^;>^*%
B ; U>ffi
 sc9 _hc>>E =f 
 ?- E H *d>^'   ^X^; fi>3  :Fd 3* ;=[fi3N{|/1^ *4=
 >^7	^ ^
m fi3>3  sh^h[*Mzfi
 ^ DX>* fijsd^;>^~}E N>) >1S>) >

\ %d>;>fY
  >fi[^X^ s  >3  =Fd 3fi >   m fi3>3  shhh
m fi3>3  shhh[NMX fiX3 &fi
C%
Y *muXDN7^ x8Q *^zd^;>^*%B ; h>X
 c9 Dhh>>>;   
fd 
\ .=
 d>s'  ^gz ._ _F E s X?
 6c ^  ;^  skd ^xh
m fi3>3  shh[.M %X>3 >ffi
C%
Y *mu>_D6>7 z;>%d 8s$ 8
$ _UxDX>x>X;t7
 >p 6s$ 6I
< >_N7^ x8Q M*^zdsd>>
  fi I> %B >xX;>?
 yE@c9 Cfi>7 Xh*I) DcX'0h EaMZ s^6h ^hc n Ix y	F 

 ^:
 E>    _fiX3 
m  	^ =Er=
L  ^ fi
M
/   >>>u_^FH
 X>3 ^ E DE sH[ >3  F 3* >  ^ =>hy=
  CY *m
dD_6>
7 >;d:8s$ U>% 472o^ d6< D>7
 >iE z;dh9 D>> *_>DX>  o0PX9 ^6 ^hhD
 _>.[^ ><d;>>UUd> [>x>:   >1 h = ^>^s

>

LMF^fi^d\^fi3Xhh Z[*M ?k;F-fi*d>)>t/^fi_^D=^ s'	^*  ['[^X^>3> 
Fd 3* ;=[fi3N{th
 E>^lE [E   2fi-ME H cCY*m#7Q;XDz'DN7^x8QM*s
d^;>^hU)
 ;fi
) fi X>7 G f 


fiNNpsFFF8ffF=FNFFFF*p>fifirF=bF=FN
.6r&*dSbfiiy*2rfiNfiffi#fiii*&lb*8N1	NCI=g*K	I*FI_(
.6r&FddN4IFbS*C2C8#I8*X4	KfiC8#	bnIIN6Ff&d	fiffpISFr*FF4fi
fi4p4
.6r&tdKd.4FIFffGF4	fi?fip[*rfiNfiffidfipXi*&dfffi=K2	NCI=g*IN6
 tFn_N
 firfir&.N^?r|24fiG*fiff^?tf4ffUNCUFpi4(*4d
 yiXffFC.^
C.tb 46	68W8*  8
 y*4 F		
 ffff4K	#Ab*=Kff 
t	6N8
 fi.
 

.CNU 
t	IC*4 ?Cb[=:?6C y8K	NCI=g*IN6*F&KISF FF4rfi .4rN
yffFfiFNf?4ff!
 ^ifi *pdi*Fifiii2dfis	fiaFiX:p	ififf	F4PXi*2
 *p0firIffffi=K[6NC8fiK	INt _F
y*fiff Cf?bd# "fiifii*fi2r4i*sXW4ffi fipfiFfiff p:	fiff*4ff*dfiffi*&
G  fiffi*&$ &Ctff.6NfiC8%
 
~p*8
 y4i*4228*& .Cf( ':)
 f* ,+_I	8ffff.F&
IS(#F204ffi
FFfir4NF1. -2t4/ '(ff&GfiNfiffiU C4fiff?^	fiff*4ff*ffptyffinP4Ffi4?F( -f
fi4p4
 >F
 0lN.^.?^Nr|2
 11F*fifi4fifiK3 & 5 4U4X*ffa6 "*iififfafi fi*fi7 "fiiififf:
ff*dfiffi?4 8_fi2*Fpi*4I_
 f*NfiNr*z??ffF 0.tCFff yI	66=
48*
 9ff8:
 fi; S6	
 fbnIIN6b*8Nff 
t		K=<
 fi=ff.!
 
&>@?*X . fi/ '?& 
t	IC*4
Cb[=P?6C y8&	KC8g*IN6F&NISF. FF4rfi .4rN

ACBCA

fiJournal of Artificial Intelligence Research 4 (1996) 1-18

Submitted 9/95; published 1/96

The Design and Experimental Analysis of Algorithms for
Temporal Reasoning
Peter van Beek
Dennis W. Manchak

Department of Computing Science, University of Alberta
Edmonton, Alberta, Canada T6G 2H1

vanbeek@cs.ualberta.ca
dmanchak@vnet.ibm.com

Abstract

Many applications|from planning and scheduling to problems in molecular biology|
rely heavily on a temporal reasoning component. In this paper, we discuss the design and
empirical analysis of algorithms for a temporal reasoning system based on Allen's inuential
interval-based framework for representing temporal information. At the core of the system
are algorithms for determining whether the temporal information is consistent, and, if so,
finding one or more scenarios that are consistent with the temporal information. Two
important algorithms for these tasks are a path consistency algorithm and a backtracking
algorithm. For the path consistency algorithm, we develop techniques that can result
in up to a ten-fold speedup over an already highly optimized implementation. For the
backtracking algorithm, we develop variable and value ordering heuristics that are shown
empirically to dramatically improve the performance of the algorithm. As well, we show
that a previously suggested reformulation of the backtracking search problem can reduce
the time and space requirements of the backtracking search. Taken together, the techniques
we develop allow a temporal reasoning component to solve problems that are of practical
size.

1. Introduction
Temporal reasoning is an essential part of many artificial intelligence tasks. It is desirable,
therefore, to develop a temporal reasoning component that is useful across applications.
Some applications, such as planning and scheduling, can rely heavily on a temporal reasoning component and the success of the application can depend on the eciency of the
underlying temporal reasoning component. In this paper, we discuss the design and empirical analysis of two algorithms for a temporal reasoning system based on Allen's (1983)
inuential interval-based framework for representing temporal information. The two algorithms, a path consistency algorithm and a backtracking algorithm, are important for two
fundamental tasks: determining whether the temporal information is consistent, and, if so,
finding one or more scenarios that are consistent with the temporal information.
Our stress is on designing algorithms that are robust and ecient in practice. For
the path consistency algorithm, we develop techniques that can result in up to a ten-fold
speedup over an already highly optimized implementation. For the backtracking algorithm,
we develop variable and value ordering heuristics that are shown empirically to dramatically
improve the performance of the algorithm. As well, we show that a previously suggested
reformulation of the backtracking search problem (van Beek, 1992) can reduce the time and
space requirements of the backtracking search. Taken together, the techniques we develop
c 1996 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fivan Beek & Manchak

Relation
Symbol Inverse
x before y
b
bi
x meets y
m
mi
x overlaps y
o
oi
x starts y

s

si

x during y

d

di

x finishes y

f

fi

x equal y

eq

eq

Meaning
x
y
x
y
x
y
x
y
x
y
x
y
x
y

Figure 1: Basic relations between intervals
allow a temporal reasoning component to solve problems that are of realistic size. As part of
the evidence to support this claim, we evaluate the techniques for improving the algorithms
on a large problem that arises in molecular biology.

2. Representing Temporal Information
In this section, we review Allen's (1983) framework for representing relations between intervals. We then discuss the set of problems that was chosen to test the algorithms.

2.1 Allen's framework

There are thirteen basic relations that can hold between two intervals (see Figure 1; Allen,
1983; Bruce, 1972). In order to represent indefinite information, the relation between two
intervals is allowed to be a disjunction of the basic relations. Sets are used to list the
disjunctions. For example, the relation fm,o,sg between events A and B represents the
disjunction, (A meets B) _ (A overlaps B) _ (A starts B): Let I be the set of all basic
relations, fb,bi,m,mi,o,oi,s,si,d,di,f,fi,eqg. Allen allows the relation between two events to
be any subset of I .
We use a graphical notation where vertices represent events and directed edges are
labeled with sets of basic relations. As a graphical convention, we never show the edges
(i; i), and if we show the edge (i; j ), we do not show the edge (j; i). Any edge for which
we have no explicit knowledge of the relation is labeled with I ; by convention such edges
are also not shown. We call networks with labels that are arbitrary subsets of I , interval
algebra or IA networks.
Example 1. Allen and Koomen (1983) show how IA networks can be used in non-linear
planning with concurrent actions. As an example of representing temporal information using
IA networks, consider the following blocks-world planning problem. There are three blocks,
A, B, and C. In the initial state, the three blocks are all on the table. The goal state
2

fiAlgorithms for Temporal Reasoning

is simply a tower of the blocks with A on B and B on C. We associate states, actions,
and properties with the intervals they hold over, and we can immediately write down the
following temporal information.
Initial Conditions
Goal Conditions
Initial fdg Clear(A)
Goal fdg On(A,B)
Initial fdg Clear(B)
Goal fdg On(B,C)
Initial fdg Clear(C)
There is an action called \Stack". The effect of the stack action is On(x; y ): block x is
on top of block y . For the action to be successfully executed, the conditions Clear(x) and
Clear(y ) must hold: neither block x or block y have a block on them. Planning introduces
two stacking actions and the following temporal constraints.
Stacking Action
Stacking Action
Stack(A,B) fbi,mig Initial
Stack(B,C) fbi,mig Initial
Stack(A,B) fdg Clear(A)
Stack(B,C) fdg Clear(B)
Stack(A,B) ffg Clear(B)
Stack(B,C) ffg Clear(C)
Stack(A,B) fmg On(A,B)
Stack(B,C) fmg On(B,C)
A graphical representation of the IA network for this planning problem is shown in
Figure 2a. Two fundamental tasks are determining whether the temporal information is
consistent, and, if so, finding one or more scenarios that are consistent with the temporal
information. An IA network is consistent if and only if there exists a mapping M of a
real interval M (u) for each event or vertex u in the network such that the relations between events are satisfied (i.e., one of the disjuncts is satisfied). For example, consider
the small subnetwork in Figure 2a consisting of the events On(A,B), On(B,C), and Goal.
This subnetwork is consistent as demonstrated by the assignment, M (On(A,B)) = [1; 5],
M (On(B,C)) = [2; 5], and M (Goal) = [3; 4]. If we were to change the subnetwork and insist
that On(A,B) must be before On(B,C), no such mapping would exist and the subnetwork
would be inconsistent. A consistent scenario of an IA network is a non-disjunctive subnetwork (i.e., every edge is labeled with a single basic relation) that is consistent. In our
planning example, finding a consistent scenario of the network corresponds to finding an
ordering of the actions that will accomplish the goal of stacking the three blocks. One such
consistent scenario can be reconstructed from the qualitative mapping shown in Figure 2b.
Example 2. Golumbic and Shamir (1993) discuss how IA networks can be used in a
problem in molecular biology: examining the structure of the DNA of an organism (Benzer, 1959). The intervals in the IA network represent segments of DNA. Experiments
can be performed to determine whether a pair of segments is either disjoint or intersects.
Thus, the IA networks that result contain edges labeled with disjoint (fb,big), intersects
(fm,mi,o,oi,s,si,d,di,f,fi,eqg), or I , the set of all basic relations|which indicates no experiment was performed. If the IA network is consistent, this is evidence for the hypothesis
that DNA is linear in structure; if it is inconsistent, DNA is nonlinear (it forms loops,
for example). Golumbic and Shamir (1993) show that determining consistency in this restricted version of IA networks is NP-complete. We will show that problems that arise in
this application can often be solved quickly in practice.
3

fivan Beek & Manchak










@






 @@

=


 


@R

1







 PP - 
 
 HYHHPHPPPPPq J]
JJ ,
HH 
HHH
JJ ,,
Z}
ZZJ , 
HHH Z
HH
 

(a) IA network for block-stacking example:

fbi,mig
fdg

fdg

1
Initial

fdg

fbi,mig

fdg

2
Clear(A)
3
Clear(B)
4
Clear(C)

5
Stack(A,B)
ffg

fdg

ffg

fmg

7
On(A,B)
8
On(B,C)

fmg

PiP  
)PP 

fdg
fdg

9
Goal

6
Stack(B,C)

(b) Consistent scenario:
Initial

Stack(B,C)
Goal
Stack(A,B)
Clear(C)
On(B,C)
Clear(B)
On(A,B)
Clear(A)

Figure 2: Representing qualitative relations between intervals

2.2 Test problems
We tested how well the heuristics we developed for improving path consistency and backtracking algorithms perform on a test suite of problems.
The purpose of empirically testing the algorithms is to determine the performance of
the algorithms and the proposed improvements on \typical" problems. There are two
approaches: (i) collect a set of \benchmark" problems that are representative of problems
that arise in practice, and (ii) randomly generate problems and \investigate how algorithmic
performance depends on problem characteristics ... and learn to predict how an algorithm
will perform on a given problem class" (Hooker, 1994).
For IA networks, there is no existing collection of large benchmark problems that actually
arise in practice|as opposed to, for example, planning in a toy domain such as the blocks
world. As a start to a collection, we propose an IA network with 145 intervals that arose
from a problem in molecular biology (Benzer, 1959, pp. 1614-15; see Example 2, above).
The proposed benchmark problem is not strictly speaking a temporal reasoning problem
4

fiAlgorithms for Temporal Reasoning

as the intervals represent segments of DNA, not intervals of time. Nevertheless, it can be
formulated as a temporal reasoning problem. The value is that the benchmark problem
arose in a real application. We will refer to this problem as Benzer's matrix.
In addition to the benchmark problem, in this paper we use two models of a random IA
network, denoted B(n) and S(n; p), to evaluate the performance of the algorithms, where n
is the number of intervals, and p is the probability of a (non-trivial) constraint between two
intervals. Model B(n) is intended to model the problems that arise in molecular biology (as
estimated from the problem discussed in Benzer, 1959). Model S(n; p) allows us to study
how algorithm performance depends on the important problem characteristic of sparseness
of the underlying constraint graph. Both models, of course, allow us to study how algorithm
performance depends on the size of the problem.
For B(n), the random instances are generated as follows.
Step 1. Generate a \solution" of size n as follows. Generate n real intervals by randomly
generating values for the end points of the intervals. Determine the IA network by determining, for each pair of intervals, whether the two intervals either intersect or are disjoint.
Step 2. Change some of the constraints on edges to be the trivial constraint by setting the
label to be I , the set of all 13 basic relations. This represents the case where no experiment
was performed to determine whether a pair of DNA segments intersect or are disjoint.
Constraints are changed so that the percentage of non-trivial constraints (approximately
6% are intersects and 17% are disjoint) and their distribution in the graph are similar to
those in Benzer's matrix.
For S(n; p), the random instances are generated as follows.
Step 1. Generate the underlying constraint graph by indicating which of the possible (n2)
edges is present. Let each edge be present with probability p, independently of the presence
or absence of other edges.
Step 2. If an edge occurs in the underlying constraint graph, randomly chose a label for
the edge from the set of all possible labels (excluding the empty label) where each label is
chosen with equal probability. If an edge does not occur, label the edge with I , the set of
all 13 basic relations.
Step 3. Generate a \solution" of size n as follows. Generate n real intervals by randomly
generating values for the end points of the intervals. Determine the consistent scenario by
determining the basic relations which are satisfied by the intervals. Finally, add the solution
to the IA network generated in Steps 1{2.
Hence, only consistent IA networks are generated from S(n; p). If we omit Step 3, it
can be shown both analytically and empirically that almost all of the different possible
IA networks generated by this distribution are inconsistent and that the inconsistency is
easily detected by a path consistency algorithm. To avoid this potential pitfall, we test
our algorithms on consistent instances of the problem. This method appears to generate a
reasonable test set for temporal reasoning algorithms with problems that range from easy
to hard. It was found, for example, that instances drawn from S(n; 1=4) were hard problems
for the backtracking algorithms to solve, whereas for values of p on either side (S(n; 1=2)
and S(n; 1=8)) the problems were easier.
5

fivan Beek & Manchak

3. Path Consistency Algorithm
Path consistency or transitive closure algorithms (Aho, Hopcroft, & Ullman, 1974; Mackworth, 1977; Montanari, 1974) are important for temporal reasoning. Allen (1983) shows
that a path consistency algorithm can be used as a heuristic test for whether an IA network
is consistent (sometimes the algorithm will report that the information is consistent when
really it is not). A path consistency algorithm is useful also in a backtracking search for a
consistent scenario where it can be used as a preprocessing algorithm (Mackworth, 1977;
Ladkin & Reinefeld, 1992) and as an algorithm that can be interleaved with the backtracking search (see the next section; Nadel, 1989; Ladkin & Reinefeld, 1992). In this section,
we examine methods for speeding up a path consistency algorithm.
The idea behind the path consistency algorithm is the following. Choose any three
vertices i, j , and k in the network. The labels on the edges (i; j ) and (j; k) potentially
constrain the label on the edge (i; k) that completes the triangle. For example, consider
the three vertices Stack(A,B), On(A,B), and Goal in Figure 2a. From Stack(A,B) fmg
On(A,B) and On(A,B) fdig Goal we can deduce that Stack(A,B) fbg Goal and therefore
can change the label on that edge from I , the set of all basic relations, to the singleton
set fbg. To perform this deduction, the algorithm uses the operations of set intersection
(\) and composition () of labels and checks whether C = C \ C  C , where C is
the label on edge (i; k). If C is updated, it may further constrain other labels, so (i; k) is
added to a list to be processed in turn, provided that the edge is not already on the list.
The algorithm iterates until no more such changes are possible. A unary operation, inverse,
is also used in the algorithm. The inverse of a label is the inverse of each of its elements
(see Figure 1 for the inverses of the basic relations).
We designed and experimentally evaluated techniques for improving the eciency of a
path consistency algorithm. Our starting point was the variation on Allen's (1983) algorithm
shown in Figure 3. For an implementation of the algorithm to be ecient, the intersection
and composition operations on labels must be ecient (Steps 5 & 10). Intersection was
made ecient by implementing the labels as bit vectors. The intersection of two labels
is then simply the logical AND of two integers. Composition is harder to make ecient.
Unfortunately, it is impractical to implement the composition of two labels using table
lookup as the table would need to be of size 213  213, there being 213 possible labels.
We experimentally compared two practical methods for composition that have been
proposed in the literature. Allen (1983) gives a method for composition which uses a table
of size 13  13. The table gives the composition of the basic relations (see Allen, 1983,
for the table). The composition of two labels is computed by a nested loop that forms the
union of the pairwise composition of the basic relations in the labels. Hogge (1987) gives a
method for composition which uses four tables of size 27  27, 27  26, 26  27, and 26  26.
The composition of two labels is computed by taking the union of the results of four array
references (H. Kautz independently devised a similar scheme). In our experiments, the
implementations of the two methods differed only in how composition was computed. In
both, the list, L, of edges to be processed was implemented using a first-in, first-out policy
(i.e., a stack).
We also experimentally evaluated methods for reducing the number of composition operations that need to be performed. One idea we examined for improving the eciency is
ik

ik

6

ik

ij

jk

ik

fiAlgorithms for Temporal Reasoning

(C; n)

Path-Consistency

1. L f(i; j ) j 1  i < j  ng
2. while (L is not empty)
3. do select and delete an (i; j ) from L
4.
for k 1 to n, k 6= i and k 6= j
5.
do t C \ C  C
6.
if (t 6= C )
7.
then C t
8.
C
Inverse(t)
9.
L
L [ f(i; k )g
10.
t
C
\C
C
11.
if (t 6= C )
12.
then C t
13.
C
Inverse(t)
14.
L
L [ f(k; j )g
ik

ij

jk

ki

ij

ik

ik

ki

kj

kj

kj

jk

Figure 3: Path consistency algorithm for IA networks
to avoid the computation when it can be predicted that the result will not constrain the
label on the edge that completes the triangle. Three such cases we identified are shown in
Figure 4. Another idea we examined, as first suggested by Mackworth (1977, p. 113), is
that the order that the edges are processed can affect the eciency of the algorithm. The
reason is the following. The same edge can appear on the list, L, of edges to be processed
many times as it progressively gets constrained. The number of times a particular edge
appears on the list can be reduced by a good ordering. For example, consider the edges
(3; 1) and (3; 5) in Figure 2a. If we process edge (3; 1) first, edge (3; 2) will be updated to
fo,oi,s,si,d,di,f,fi,eqg and will be added to L (k = 2 in Steps 5{9). Now if we process edge
(3; 5), edge (3; 2) will be updated to fo,s,dg and will be added to L a second time. However,
if we process edge (3; 5) first, (3; 2) will be immediately updated to fo,s,dg and will only be
added to L once.
Three heuristics we devised for ordering the edges are shown in Figure 9. The edges
are assigned a heuristic value and are processed in ascending order. When a new edge is
added to the list (Steps 9 & 14), the edge is inserted at the appropriate spot according to its
new heuristic value. There has been little work on ordering heuristics for path consistency
algorithms. Wallace and Freuder (1992) discuss ordering heuristics for arc consistency
algorithms, which are closely related to path consistency algorithms. Two of their heuristics
cannot be applied in our context as the heuristics assume a constraint satisfaction problem
with finite domains, whereas IA networks are examples of constraint satisfaction problems
with infinite domains. A third heuristic (due to B. Nudel, 1983) closely corresponds to our
cardinality heuristic.
All experiments were performed on a Sun 4/25 with 12 megabytes of memory. We
report timings rather than some other measure such as number of iterations as we believe
this gives a more accurate picture of whether the results are of practical interest. Care was
7

fivan Beek & Manchak

The computation, C \ C  C , can be skipped when it is known that the result of the
composition will not constrain the label on the edge (i; k):
a. If either C or C is equal to I , the result of the composition will be I and therefore
will not constrain the label on the edge (i; k). Thus, in Step 1 of Figure 3, edges that
are labeled with I are not added to the list of edges to process.
b. If the condition,
ik

ij

ij

jk

jk

(b 2 C

ij ^

bi 2 C ) _ (bi 2 C

ij ^

jk

b 2 C ) _ (d 2 C
jk

ij ^

di 2 C );
jk

is true, the result of composing C and C will be I . The condition is quickly tested
using bit operations. Thus, if the above condition is true just before Step 5, Steps 5{9
can be skipped. A similar condition can be formulated and tested before Step 10.
c. If at some point in the computation of C  C it is determined that the result
accumulated so far would not constrain the label C , the rest of the computation can
be skipped.
ij

jk

ij

jk

ik

Figure 4: Skipping techniques
taken to always start with the same base implementation of the algorithm and only add
enough code to implement the composition method, new technique, or heuristic that we
were evaluating. As well, every attempt was made to implement each method or heuristic
as eciently as we could.
Given our implementations, Hogge's method for composition was found to be more
ecient than Allen's method for both the benchmark problem and the random instances
(see Figures 5{8). This much was not surprising. However, with the addition of the skipping
techniques, the two methods became close in eciency. The skipping techniques sometimes
dramatically improved the eciency of both methods. The ordering heuristics can improve
the eciency, although here the results were less dramatic. The cardinality heuristic and
the constraintedness heuristic were also tried for ordering the edges. It was found that the
cardinality heuristic was just as costly to compute as the weight heuristic but did not out
perform it. The constraintedness heuristic reduced the number of iterations but proved too
costly to compute. This illustrates the balance that must be struck between the effectiveness
of a heuristic and the additional overhead the heuristic introduces.
For S(n; p), the skipping techniques and the weight ordering heuristic together can result
in up to a ten-fold speedup over an already highly optimized implementation using Hogge's
method for composition. The largest improvements in eciency occur when the IA networks
are sparse (p is smaller). This is encouraging for it appears that the problems that arise in
planning and molecular biology are also sparse. For B(n) and Benzer's matrix, the speedup
is approximately four-fold. Perhaps most importantly, the execution times reported indicate
that the path consistency algorithm, even though it is an O(n3 ) algorithm, can be used on
practical-sized problems. In Figure 8, we show how well the algorithms scale up. It can be
8

fiAlgorithms for Temporal Reasoning

Allen

137.7

Hogge

10.3

Allen+skip

5.7

Hogge+skip

4.0

Hogge+skip+weight

2.7

Figure 5: Effect of heuristics on time (sec.) of path consistency algorithms applied to
Benzer's matrix

time (sec.)

100

10

1

Allen
Hogge
Allen+skip
Hogge+skip
Hogge+skip+weight

0.1
50

75

100
n

125

150

Figure 6: Effect of heuristics on average time (sec.) of path consistency algorithms. Each
data point is the average of 100 tests on random instances of IA networks drawn
from B(n); the coecient of variation (standard deviation / average) for each set
of 100 tests is bounded by 0.20
seen that the algorithm that includes the weight ordering heuristic out performs all others.
However, this algorithm requires much space and the largest problem we were able to solve
was with 500 intervals. The algorithms that included only the skipping techniques were
able to solve much larger problems before running out of space (up to 1500 intervals) and
here the constraint was the time it took to solve the problems.

9

fivan Beek & Manchak

100

time (sec.)

10

1

Allen
Hogge
Allen+skip
Hogge+skip
Hogge+skip+weight

0.1
1/8

1/4

1/2
p

3/4

1

Figure 7: Effect of heuristics on average time (sec.) of path consistency algorithms. Each
data point is the average of 100 tests on random instances of IA networks drawn
from S(100; p); the coecient of variation (standard deviation / average) for each
set of 100 tests is bounded by 0.25
9000
8000
S(n,1/4):

Allen+skip
Hogge+skip
Hogge+skip+weight

7000

time (sec.)

6000

B(n):

Allen+skip
Hogge+skip
Hogge+skip+weight

5000
4000
3000
2000
1000
0
100

200

300

400

500

600
n

700

800

900

1000

Figure 8: Effect of heuristics on average time (sec.) of path consistency algorithms. Each
data point is the average of 10 tests on random instances of IA networks drawn
from S(n; 1=4) and B(n); the coecient of variation (standard deviation / average) for each set of 10 tests is bounded by 0.35
10

fiAlgorithms for Temporal Reasoning

4. Backtracking Algorithm

Allen (1983) was the first to propose that a backtracking algorithm (Golomb & Baumert,
1965) could be used to find a consistent scenario of an IA network. In the worst case, a
backtracking algorithm can take an exponential amount of time to complete. This worst
case also applies here as Vilain and Kautz (1986, 1989) show that finding a consistent
scenario is NP-complete for IA networks. In spite of the worst case estimate, backtracking
algorithms can work well in practice. In this section, we examine methods for speeding up a
backtracking algorithm for finding a consistent scenario and present results on how well the
algorithm performs on different classes of problems. In particular, we compare the eciency
of the algorithm on two alternative formulations of the problem: one that has previously
been proposed by others and one that we have proposed (van Beek, 1992). We also improve
the eciency of the algorithm by designing heuristics for ordering the instantiation of the
variables and for ordering the values in the domains of the variables.
As our starting point, we modeled our backtracking algorithm after that of Ladkin and
Reinefeld (1992) as the results of their experimentation suggests that it is very successful at
finding consistent scenarios quickly. Following Ladkin and Reinefeld our algorithm has the
following characteristics: preprocessing using a path consistency algorithm, static order of
instantiation of the variables, chronological backtracking, and forward checking or pruning
using a path consistency algorithm. In chronological backtracking, when the search reaches
a dead end, the search simply backs up to the next most recently instantiated variable and
tries a different instantiation. Forward checking (Haralick & Elliott, 1980) is a technique
where it is determined and recorded how the instantiation of the current variable restricts
the possible instantiations of future variables. This technique can be viewed as a hybrid of
tree search and consistency algorithms (see Nadel, 1989; Nudel, 1983). (See Dechter, 1992,
for a general survey on backtracking.)

4.1 Alternative formulations

Let C be the matrix representation of an IA network, where C is the label on edge (i; j ).
The traditional method for finding a consistent scenario of an IA network is to search for a
subnetwork S of a network C such that,
(a) S  C ,
(b) jS j = 1, for all i; j , and
(c) S is consistent.
To find a consistent scenario we simply search through the different possible S 's that satisfy
conditions (a) and (b)|it is a simple matter to enumerate them|until we find one that
also satisfies condition (c). Allen (1983) was the first to propose using backtracking search
to search through the potential S 's.
Our alternative formulation is based on results for two restricted classes of IA networks,
denoted here as SA networks and NB networks. In IA networks, the relation between two
intervals can be any subset of I , the set of all thirteen basic relations. In SA networks
(Vilain & Kautz, 1986), the allowed relations between two intervals are only those subsets
of I that can be translated, using the relations f<, , =, >, , 6=g, into conjunctions of
ij

ij

ij

ij

11

fivan Beek & Manchak

relations between the endpoints of the intervals. For example, the IA network in Figure 2a
is also an SA network. As a specific example, the interval relation \A fbi,mig B" can be
expressed as the conjunction of point relations, (B, < B+ ) ^ (A, < A+ ) ^ (A,  B+ );
where A, and A+ represent the start and end points of interval A, respectively. (See Ladkin
& Maddux, 1988; van Beek & Cohen, 1990, for an enumeration of the allowed relations for
SA networks.) In NB networks (Nebel & Burckert, 1995), the allowed relations between
two intervals are only those subsets of I that can be translated, using the relations f<,
, =, >, , 6=g, into conjunctions of Horn clauses that express the relations between the
endpoints of the intervals. The set of NB relations is a strict superset of the SA relations.
Our alternative formulation is as follows. We describe the method in terms of SA
networks, but the same method applies to NB networks. The idea is that, rather than
search directly for a consistent scenario of an IA network as in previous work, we first
search for something more general: a consistent SA subnetwork of the IA network. That
is, we use backtrack search to find a subnetwork S of a network C such that,
(a)

Sij  Cij

(b)

Sij

(c)

S

,

is an allowed relation for SA networks, for all i; j , and

is consistent.

In previous work, the search is through the alternative singleton labelings of an edge, i.e.,
jS j = 1. The key idea in our proposal is that we decompose the labels into the largest
possible sets of basic relations that are allowed for SA networks and search through these
decompositions. This can considerably reduce the size of the search space. For example,
suppose the label on an edge is fb,bi,m,o,oi,sig. There are six possible ways to label the
edge with a singleton label: fbg, fbig, fmg, fog, foig, fsig, but only two possible ways to
label the edge if we decompose the labels into the largest possible sets of basic relations
that are allowed for SA networks: fb,m,og and fbi,oi,sig. As another example, consider the
network shown in Figure 2a. When searching through alternative singleton labelings, the
worst case size of the search space is C12  C13      C89 = 314 (the edges labeled with
I must be included in the calculation). But when decomposing the labels into the largest
possible sets of basic relations that are allowed for SA networks and searching through the
decompositions, the size of the search space is 1, so no backtracking is necessary (in general,
the search is, of course, not always backtrack free).
To test whether an instantiation of a variable is consistent with instantiations of past
variables and with possible instantiations of future variables, we use an incremental path
consistency algorithm (in Step 1 of Figure 3 instead of initializing L to be all edges, it is
initialized to the single edge that has changed). The result of the backtracking algorithm is a
consistent SA subnetwork of the IA network, or a report that the IA network is inconsistent.
After backtracking completes, a solution of the SA network can be found using a fast
algorithm given by van Beek (1992).
ij

4.2 Ordering heuristics

Backtracking proceeds by progressively instantiating variables. If no consistent instantiation
exists for the current variable, the search backs up. The order in which the variables
12

fiAlgorithms for Temporal Reasoning

Weight. The weight heuristic is an estimate of how much the label on an edge will restrict
the labels on other edges. Restrictiveness was measured for each basic relation by successively composing the basic relation with every possible label and summing the cardinalities
of the results. The results were then suitably scaled to give the table shown below.
relation b bi m mi o oi s si d di f fi eq
weight 3 3 2 2 4 4 2 2 4 3 2 2 1
The weight of a label is then the sum of the weights of its elements. For example, the weight
of the relation fm,o,sg is 2 + 4 + 2 = 8.
Cardinality. The cardinality heuristic is a variation on the weight heuristic. Here, the
weight of every basic relation is set to one.
Constraint. The constraintedness heuristic is an estimate of how much a change in a label
on an edge will restrict the labels on other edges. It is determined as follows. Suppose
the edge we are interested in is (i; j ). The constraintedness of the label on edge (i; j ) is
the sum of the weights of the labels on the edges (k; i) and (j; k), k = 1; :::; n; k 6= i; k 6= j .
The intuition comes from examining the path consistency algorithm (Figure 3) which would
propagate a change in the label C . We see that C will be composed with C (Step 5)
and C (Step 10), k = 1; :::; n; k 6= i; k 6= j .
ij

ij

ki

jk

Figure 9: Ordering heuristics
are instantiated and the order in which the values in the domains are tried as possible
instantiations can greatly affect the performance of a backtracking algorithm and various
methods for ordering the variables (e.g. Bitner & Reingold, 1975; Freuder, 1982; Nudel,
1983) and ordering the values (e.g. Dechter & Pearl, 1988; Ginsberg et al., 1990; Haralick
& Elliott, 1980) have been proposed.
The idea behind variable ordering heuristics is to instantiate variables first that will
constrain the instantiation of the other variables the most. That is, the backtracking search
attempts to solve the most highly constrained part of the network first. Three heuristics
we devised for ordering the variables (edges in the IA network) are shown in Figure 9.
For our alternative formulation, cardinality is redefined to count the decompositions rather
than the elements of a label. The variables are put in ascending order. In our experiments
the ordering is static|it is determined before the backtracking search starts and does not
change as the search progresses. In this context, the cardinality heuristic is similar to a
heuristic proposed by Bitner and Reingold (1975) and further studied by Purdom (1983).
The idea behind value ordering heuristics is to order the values in the domains of the
variables so that the values most likely to lead to a solution are tried first. Generally, this
is done by putting values first that constrain the choices for other variables the least. Here
we propose a novel technique for value ordering that is based on knowledge of the structure
of solutions. The idea is to first choose a small set of problems from a class of problems,
and then find a consistent scenario for each instance without using value ordering. Once we
have a set of solutions, we examine the solutions and determine which values in the domains
13

fivan Beek & Manchak

120

100

SI

SA

time (sec.)

80

60

40

20

0
50

100

150
n

200

250

Figure 10: Effect of decomposition method on average time (sec.) of backtracking algorithm. Each data point is the average of 100 tests on random instances of IA
networks drawn from B(n); the coecient of variation (standard deviation /
average) for each set of 100 tests is bounded by 0.15
are most likely to appear in a solution and which values are least likely. This information
is then used to order the values in subsequent searches for solutions to problems from this
class of problems. For example, five problems were generated using the model S(100; 1=4)
and consistent scenarios were found using backtracking search and the variable ordering
heuristic constraintedness/weight/cardinality. After rounding to two significant digits, the
relations occurred in the solutions with the following frequency,
relation
b, bi d, di o, oi
value (10) 1900 240 220

eq
53

m, mi f, fi s, si
20
15 14

As an example of using this information to order the values in a domain, suppose that the
label on an edge is fb,bi,m,o,oi,sig. If we are decomposing the labels into singleton labels,
we would order the values in the domain as follows (most preferred first): fbg, fbig, fog,
foig, fmg, fsig. If we are decomposing the labels into the largest possible sets of basic
relations that are allowed for SA networks, we would order the values in the domain as
follows: fb,m,og, fbi,oi,sig, since 1900 + 20 + 220 > 1900 + 220 + 14. This technique can be
used whenever something is known about the structure of solutions.

4.3 Experiments

All experiments were performed on a Sun 4/20 with 8 megabytes of memory.
The first set of experiments, summarized in Figure 10, examined the effect of problem
formulation on the execution time of the backtracking algorithm. We implemented three
14

fiAlgorithms for Temporal Reasoning

10000
Random value ordering, Random
Heuristic value ordering, random
Random value ordering, best heuristic
Heuristic value ordering, best heuristic

variable
variable
variable
variable

ordering
ordering
ordering
ordering

time

1000

100

10
0

10

20

30

40

50
test

60

70

80

90

100

Figure 11: Effect of variable and value ordering heuristics on time (sec.) of backtracking
algorithm. Each curve represents 100 tests on random instances of IA networks
drawn from S(100; 1=4) where the tests are ordered by time taken to solve the
instance. The backtracking algorithm used the SA decomposition method.
versions of the algorithm that were identical except that one searched through singleton
labelings (denoted hereafter and in Figure 10 as the SI method) and the other two searched
through decompositions of the labels into the largest possible allowed relations for SA networks and NB networks, respectively. All of the methods solved the same set of random
problems drawn from B(n) and were also applied to Benzer's matrix (denoted + and 
in Figure 10). For each problem, the amount of time required to solve the given IA network was recorded. As mentioned earlier, each IA network was preprocessed with a path
consistency algorithm before backtracking search. The timings include this preprocessing
time. The experiments indicate that the speedup by using the SA decomposition method
can be up to three-fold over the SI method. As well, the SA decomposition method was
able to solve larger problems before running out of space (n = 250 versus n = 175). The
NB decomposition method gives exactly the same result as for the SA method on these
problems because of the structure of the constraints. We also tested all three methods on
a set of random problems drawn from S(100; p), where p = 1; 3=4; 1=2, and 1=8. In these
experiments, the SA and NB methods were consistently twice as fast as the SI method. As
well, the NB method showed no advantage over the SA method on these problems. This is
surprising as the branching factor, and hence the size of the search space, is smaller for the
NB method than for the SA method.
The second set of experiments, summarized in Figure 11, examined the effect on the
execution time of the backtracking algorithm of heuristically ordering the variables and
the values in the domains of the variables before backtracking search begins. For variable
ordering, all six permutations of the cardinality, constraint, and weight heuristics were tried
15

fivan Beek & Manchak

as the primary, secondary, and tertiary sorting keys, respectively. As a basis of comparison,
the experiments included the case of no heuristics. Figure 11 shows approximate cumulative
frequency curves for some of the experimental results. Thus, for example, we can read from
the curve representing heuristic value ordering and best heuristic variable ordering that
approximately 75% of the tests completed within 20 seconds, whereas with random value
and variable ordering only approximately 5% of the tests completed within 20 seconds. We
can also read from the curves the 0, 10, : : : , 100 percentiles of the data sets (where the
value of the median is the 50th percentile or the value of the 50th test). The curves are
truncated at time = 1800 (1/2 hour), as the backtracking search was aborted when this
time limit was exceeded.
In our experiments we found that S(100; 1=4) represents a particularly dicult class
of problems and it was here that the different heuristics resulted in dramatically different performance, both over the no heuristic case and also between the different heuristics.
With no value ordering, the best heuristic for variable ordering was the combination constraintedness/weight/cardinality where constraintedness is the primary sorting key and the
remaining keys are used to break subsequent ties. Somewhat surprisingly, the best heuristic
for variable ordering changes when heuristic value ordering is incorporated. Here the combination weight/constraintedness/cardinality works much better. This heuristic together
with value ordering is particularly effective at \attening out" the distribution and so allowing a much greater number of problems to be solved in a reasonable amount of time. For
S(100; p), where p = 1; 3=4; 1=2, and 1=8, the problems were much easier and all but three
of the hundreds of tests completed within 20 seconds. In these problems, the heuristic used
did not result in significantly different performance.
In summary, the experiments indicate that by changing the decomposition method we
are able to solve larger problems before running out of space (n = 250 vs n = 175 on a
machine with 8 megabytes; see Figure 10). The experiments also indicate that good heuristic
orderings can be essential to being able to find a consistent scenario of an IA network in
reasonable time. With a good heuristic ordering we were able to solve much larger problems
before running out of time (see Figure 11). The experiments also provide additional evidence
for the ecacy of Ladkin and Reinefeld's (1992, 1993) algorithm. Nevertheless, even with
all of our improvements, some problems still took a considerable amount of time to solve.
On consideration, this is not surprising. After all, the problem is known to be NP-complete.

5. Conclusions
Temporal reasoning is an essential part of tasks such as planning and scheduling. In this paper, we discussed the design and an empirical analysis of two key algorithms for a temporal
reasoning system. The algorithms are a path consistency algorithm and a backtracking algorithm. The temporal reasoning system is based on Allen's (1983) interval-based framework
for representing temporal information. Our emphasis was on how to make the algorithms
robust and ecient in practice on problems that vary from easy to hard. For the path consistency algorithm, the bottleneck is in performing the composition operation. We developed
methods for reducing the number of composition operations that need to be performed.
These methods can result in almost an order of magnitude speedup over an already highly
optimized implementation of the algorithm. For the backtracking algorithm, we developed
16

fiAlgorithms for Temporal Reasoning

variable and value ordering heuristics and showed that an alternative formulation of the
problem can considerably reduce the time taken to find a solution. The techniques allow an
interval-based temporal reasoning system to be applied to larger problems and to perform
more eciently in existing applications.

References

Aho, A. V., Hopcroft, J. E., & Ullman, J. D. (1974). The Design and Analysis of Computer
Algorithms. Addison-Wesley.
Allen, J. F. (1983). Maintaining knowledge about temporal intervals. Comm. ACM, 26,
832{843.
Allen, J. F., & Koomen, J. A. (1983). Planning using a temporal world model. In Proceedings
of the Eighth International Joint Conference on Artificial Intelligence, pp. 741{747
Karlsruhe, West Germany.
Benzer, S. (1959). On the topology of the genetic fine structure. Proc. Nat. Acad. Sci. USA,
45, 1607{1620.
Bitner, J. R., & Reingold, E. M. (1975). Backtrack programming techniques. Comm. ACM,
18, 651{655.
Bruce, B. C. (1972). A model for temporal references and its application in a question
answering program. Artificial Intelligence, 3, 1{25.
Dechter, R. (1992). From local to global consistency. Artificial Intelligence, 55, 87{107.
Dechter, R., & Pearl, J. (1988). Network-based heuristics for constraint satisfaction problems. Artificial Intelligence, 34, 1{38.
Freuder, E. C. (1982). A sucient condition for backtrack-free search. J. ACM, 29, 24{32.
Ginsberg, M. L., Frank, M., Halpin, M. P., & Torrance, M. C. (1990). Search lessons learned
from crossword puzzles. In Proceedings of the Eighth National Conference on Artificial
Intelligence, pp. 210{215 Boston, Mass.
Golomb, S., & Baumert, L. (1965). Backtrack programming. J. ACM, 12, 516{524.
Golumbic, M. C., & Shamir, R. (1993). Complexity and algorithms for reasoning about
time: A graph-theoretic approach. J. ACM, 40, 1108{1133.
Haralick, R. M., & Elliott, G. L. (1980). Increasing tree search eciency for constraint
satisfaction problems. Artificial Intelligence, 14, 263{313.
Hogge, J. C. (1987). TPLAN: A temporal interval-based planner with novel extensions. Department of computer science technical report UIUCDCS-R-87, University of Illinois.
Hooker, J. N. (1994). Needed: An empirical science of algorithms. Operations Research,
42, 201{212.
17

fivan Beek & Manchak

Ladkin, P., & Reinefeld, A. (1992). Effective solution of qualitative interval constraint
problems. Artificial Intelligence, 57, 105{124.
Ladkin, P., & Reinefeld, A. (1993). A symbolic approach to interval constraint problems. In
Calmet, J., & Campbell, J. (Eds.), Artificial Intelligence and Symbolic Mathematical
Computing, Springer Lecture Notes in Computer Science 737. Springer-Verlag.
Ladkin, P. B., & Maddux, R. D. (1988). On binary constraint networks. Technical report,
Kestrel Institute, Palo Alto, Calif.
Mackworth, A. K. (1977). Consistency in networks of relations. Artificial Intelligence, 8,
99{118.
Montanari, U. (1974). Networks of constraints: Fundamental properties and applications
to picture processing. Inform. Sci., 7, 95{132.
Nadel, B. A. (1989). Constraint satisfaction algorithms. Computational Intelligence, 5,
188{224.
Nebel, B., & Burckert, H.-J. (1995). Reasoning about temporal relations: A maximal
tractable subclass of Allen's interval algebra. J. ACM, 42, 43{66.
Nudel, B. (1983). Consistent-labeling problems and their algorithms: Expected-complexities
and theory-based heuristics. Artificial Intelligence, 21, 135{178.
Purdom, Jr., P. W. (1983). Search rearrangement backtracking and polynomial average
time. Artificial Intelligence, 21, 117{133.
van Beek, P. (1992). Reasoning about qualitative temporal information. Artificial Intelligence, 58, 297{326.
van Beek, P., & Cohen, R. (1990). Exact and approximate reasoning about temporal
relations. Computational Intelligence, 6, 132{144.
Vilain, M., & Kautz, H. (1986). Constraint propagation algorithms for temporal reasoning.
In Proceedings of the Fifth National Conference on Artificial Intelligence, pp. 377{382
Philadelphia, Pa.
Vilain, M., Kautz, H., & van Beek, P. (1989). Constraint propagation algorithms for
temporal reasoning: A revised report. In Weld, D. S., & de Kleer, J. (Eds.), Readings
in Qualitative Reasoning about Physical Systems, pp. 373{381. Morgan Kaufmann.
Wallace, R. J., & Freuder, E. C. (1992). Ordering heuristics for arc consistency algorithms.
In Proceedings of the Ninth Canadian Conference on Artificial Intelligence, pp. 163{
169 Vancouver, B.C.

18

fiJournal of Artificial Intelligence Research 4 (1996) 419-443

Submitted 2/96; published 6 /96

A Principled Approach Towards Symbolic
Geometric Constraint Satisfaction
Sanjay Bhansali

BHANSALI@EECS.WSU.EDU

School of EECS, Washington State University
Pullman, WA 99164-2752

Glenn A. Kramer

GAK@EIT.COM

Enterprise Integration Technologies, 800 El Camino Real
Menlo Park, CA 94025

Tim J. Hoar

TIMHOAR@MICROSOFT.COM

Microsoft Corporation
One Microsoft Way, 2/2069
Redmond, WA 98052

Abstract
An important problem in geometric reasoning is to find the configuration of a collection of
geometric bodies so as to satisfy a set of given constraints. Recently, it has been suggested that this
problem can be solved efficiently by symbolically reasoning about geometry. This approach, called
degrees of freedom analysis, employs a set of specialized routines called plan fragments that
specify how to change the configuration of a set of bodies to satisfy a new constraint while
preserving existing constraints. A potential drawback, which limits the scalability of this approach,
is concerned with the difficulty of writing plan fragments. In this paper we address this limitation
by showing how these plan fragments can be automatically synthesized using first principles about
geometric bodies, actions, and topology.

1. Introduction
An important problem in geometric reasoning is the following: given a collection of geometric
bodies, called geoms, and a set of constraints between them, find a configuration  i.e., position,
orientation, and dimension  of the geoms that satisfies all the constraints. Solving this problem
is an integral task for many applications like constraint-based sketching and design, geometric
modeling for computer-aided design, kinematics analysis of robots and other mechanisms
(Hartenberg & Denavit, 1964), and describing mechanical assemblies.
General purpose constraint satisfaction techniques are not well suited for solving constraint
problems involving complicated geometry. Such techniques represent geoms and constraints as
algebraic equations, whose real solutions yield the numerical values describing the desired
configuration of the geoms. Such equation sets are highly non-linear and highly coupled and in
the general case require iterative numerical solutions techniques. Iterative numerical techniques
are not particularly efficient and can have problems with stability and robustness (Press,
Flannery, Teukolsky & Vetterling, 1986). For many tasks (e.g., simulation and optimization of
mechanical devices) the same equations are solved repeatedly which makes a compiled solution
desirable. In theory, symbolic manipulation of equations can often yield a non-iterative, closed
form solution. Once found, such a closed-form solution can be executed very efficiently.
1996 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiBHANSALI, KRAMER & HOAR

However, the computational intractability of symbolic algebraic solution of the equations renders
this approach impractical (Kramer, 1992; Liu & Popplestone, 1990).
In earlier work Kramer describes a system called GCE that uses an alternative approach
called degrees of freedom analysis (1992, 1993). This approach is based on symbolic reasoning
about geometry, rather than equations, and was shown to be more efficient than systems based on
algebraic equation solvers. The approach uses two models. A symbolic geometric model is used
to reason symbolically about how to assemble the geoms so as to satisfy the constraints
incrementally. The "assembly plan" thus developed is used to guide the solution of the complex
nonlinear equations - derived from the second, numerical model - in a highly decoupled, stylized
manner.
The GCE system was used to analyze problems in the domain of kinematics and was shown
to perform kinematics simulation of complex mechanisms (including a Stirling engine, an
elevator door mechanism, and a sofa-bed mechanism) much more efficiently than pure numerical
solvers (Kramer, 1992). The GCE has subsequently been integrated in a commercial system
called BravoTM by Applicon where it is used to drive the 2D sketcher (Brown-Associates, 1993).
Several academic systems are currently using the degrees of freedom analysis for other
applications like assembly modeling (Anantha, Kramer & Crawford, 1992), editing and
animating planar linkages (Brunkhart, 1994), and feature-based design (Salomons, 1994; Shah &
Rogers, 1993).
GCE employs a set of specialized routines called plan fragments to create the assembly plan.
A plan fragment specifies how to change the configuration of a geom using a fixed set of
operators and the available degrees of freedom, so that a new constraint is satisfied while
preserving all prior constraints on the geom. The assembly plan is completed when all
constraints have been satisfied or the degrees of freedom is reduced to zero. This approach is
canonical: the constraints may be satisfied in any order; the final status of the geom in terms of
remaining degrees of freedom is the same (p. 80-81, Kramer, 1992). The algorithm for finding
the assembly procedure has a time complexity of O(cg) where c is the number of constraints and
g is the number of geoms (p. 139, Kramer, 1992).
Since the crux of problem-solving is taken care of by the plan fragments, the success of the
approach depends on ones ability to construct a complete set of plan fragments meeting the
canonical specification. The number of plan fragments needed grows geometrically as the
number of geoms and constraints between them increase. Worse, the complexity of the plan
fragments increases exponentially since the various constraints interact in subtle ways creating a
large number of special cases that need to be individually handled. This is potentially a serious
limitation in extending the degrees of freedom approach. In this paper we address this problem
by showing how plan fragments can be automatically generated using first principles about
geoms, actions, and topology.
Our approach is based on planning. Plan fragment generation can be reduced to a planning
problem by considering the various geoms and the invariants on them as describing a state.
Operators are actions, such as rotate, that can change the configuration of geoms, thereby
violating or achieving some constraint. An initial state is specified by the set of existing
invariants on a geom and a final state by the additional constraints to be satisfied. A plan is a
sequence of actions that when applied to the initial state achieves the final state.
With this formulation, one could presumably use a classical planner, such as STRIPS (Fikes
& Nilsson, 1971), to automatically generate a plan-fragment. However, the operators in this
domain are parametric operators with a real-valued domain. Thus, the search space consists of an
infinite number of states. Even if the real-valued domain is discretized by considering realvalued intervals there is still a very large search space and finding a plan that satisfies the
420

fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTION

specified constraints would be an intractable problem. Our approach uses loci information
(representing a set of points that satisfy some constraints) to reason about the effects of various
operators and thus reduces the search problem to a problem in topology, involving reasoning
about the intersection of various loci.
An issue to be faced in using a conventional planner is the frame problem: how to determine
what properties or relationships do not change as a result of an action. A typical solution is to use
the assumption: an action does not modify any property or relationship unless explicitly stated as
an effect of the action. Such an approach works well if one knows a priori all possible
constraints or invariants that might be of interest and relatively few constraints get affected by
each action - which is not true in our case. We use a novel scheme for representing effects of
actions. It is based on reifying (i.e., treating as first class objects) actions in addition to geometric
entities and invariant types. We associate, with each pair of geom and invariants, a set of actions
that can be used to achieve or preserve that invariant for that geom. Whenever a new geom or
invariant type is introduced the corresponding rules for actions that can achieve/preserve the
invariants have to be added. Since there are many more invariant types than actions in this
domain, this scheme results in simpler rules. Borgida, Mylopoulos & Reiter (1993) propose a
similar approach for reasoning with program specifications. A unique feature of our work is the
use of geometric-specific matching rules to determine when two or more general actions that
achieve/preserve different constraints can be reformulated to a less general action.
Another shortcoming of using a conventional planner is the difficulty of representing
conditional effects of operators. In GCE an operations effect depends on the type of geom as
well as the particular geometry. For example, the action of translating a body to the intersection
of two lines on a plane would normally reduce the bodys translational degrees of freedom to
zero; however, if the two lines happen to coincide then the body still retains one degree of
translational freedom and if the two lines are parallel but do not coincide then the action fails.
Such situations are called degeneracies. One approach to handling degeneracies is to use a
reactive planner that dynamically revises its plan at run-time. However, this could result in
unacceptable performance in many real-time applications. Our approach makes it possible to precompile all potential degeneracies in the plan. We achieve this by dividing the planning
algorithm into two phases. In the first phase a skeletal plan is generated that works in the normal
case and in the second phase, this skeletal plan is refined to take care of singularities and
degeneracies. The approach is similar to the idea of refining skeletal plans in MOLGEN
(Friedland, 1979) and the idea of critics in HACKER (Sussman, 1975) to fix known bugs in a
plan. However, the skeletal plan refinement in MOLGEN essentially consisted of instantiating a
partial plan to work for specific conditions, whereas in our method a complete plan which works
for a normal case is extended to handle special conditions like degeneracies and singularities.
1.1 A Plan Fragment Example.
We will use a simple example of a plan fragment specification to illustrate our approach.
Domains such as mechanical CAD and computer-based sketching rely heavily on complex
combinations of relatively simple geometric elements, such as points, lines, and circles and a
small collection of constraints such as coincidence, tangency, and parallelism. Figure 1
illustrates some fairly complex mechanisms (all implemented in GCE) using simple geoms and
constraints.

421

fiBHANSALI, KRAMER & HOAR

Automobile suspension

Elevator Doors
Stirling Engine
Figure 1. Modeling complex mechanisms using simple geoms and constraints. All the constraints
needed to model the joints in the above mechanisms are solvable using the degrees of freedom approach.

Our example problem is illustrated in Figure 2 and is specified as follows:
Geom-type: circle
Name: $c
Invariants: (fixed-distance-line $c $L1 $dist1 BIAS_COUNTERCLOCKWISE)
To-be-achieved: (fixed-distance-line $c $L2 $dist2 BIAS_CLOCKWISE)
In this example, a variable-radius circle $c1 has a prior constraint specifying that the circle is
at a fixed distance $dist1 to the left of a fixed line $L1 (or alternatively, that a line drawn parallel
to $L1 at a distance $dist1 from the center of $c is tangent in a counterclockwise direction to the
circle). The new constraint to be satisfied is that the circle be at a fixed distance $dist2 to the
right of another fixed line $L2.

$L2
$L2
$c
$c

$dist2

$L1

$dist1
$L1

Figure 2. Example problem (initial state)

1We use the following conventions: symbols preceded by $ represent constants, symbols preceded by ?
represent variables, expressions of the form (>> parent subpart) denote the subpart of a compound term, parent.

422

fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTION

To solve this problem, three different plans can be used: (a) translate the circle from its
current position to a position such that it touches the two lines $L2 and $L1 shown in the figure
(b) scale the circle while keeping its point of contact with $L1 fixed, so that it touches $L2 (c)
scale and translate the circle so that it touches both $L2 and $L1.
Each of the above action sequences constitute one plan fragment that can be used in the
above situation and would be available to GCE from a plan-fragment library. Note that some of
the plan fragments would not be applicable in certain situations. For example, if $L1 and $L2 are
parallel, then a single translation can never achieve both the constraints, and plan-fragment (a)
would not be applicable. In this paper we will show how each of the plan-fragments can be
automatically synthesized by reasoning from more fundamental principles.
The rest of the paper is organized as follows: Section 2 gives an architectural overview of the
system built to synthesize plan fragments automatically with a detailed description of the various
components. Section 3 illustrates the plan fragment synthesis process using the example of
Figure 2. Section 4 describes the results from the current implementation of the system. Section
5 relates our approach to other work in geometric constraint satisfaction. Section 6 summarizes
the main results and suggests future extensions for this work.

2. Overview of System Architecture
Figure 3 gives an overview of the architecture of our system showing the various knowledge
components and the plan generation process. The knowledge represented in the system is broadly
categorized into a Geom knowledge-base that contains knowledge specific to particular
geometric entities and a Geometry knowledge-base that is independent of particular geoms and
can be reused for generating plan fragments for any geom.
Knowledge Components
Geometry knowledge-base
Geom knowledge-base
Geoms

Actions

Invariants

Action Matching Rules
Action Rules

Loci

Reformulation Rules

Signatures

Measurements

Plan fragment
specification

Planner
Phase I

Prioritization Strategy

Skeletal
Plan

Planner
Phase II

Plan fragment

Figure 3. Architectural overview of the plan fragment generator

2.1 Geom Knowledge-base
The geom specific knowledge-base can be further decomposed into seven knowledge
components.
423

fiBHANSALI, KRAMER & HOAR

2.1.1 ACTIONS
These describe operations that can be performed on geoms. In the GCE domain, three actions
suffice to change the configuration of a body to an arbitrary configuration: (translate g v) which
denotes a translation of geom g by vector v; (rotate g pt ax amt) which denotes a rotation of
geom g, around point pt, about an axis ax, by an angle amt; and (scale g pt amt) where g is a
geom, pt is a point on the geom, and amt is a scalar. The semantics of a scale operation depends
on the type of the geom; for example, for a circle, a scale indicates a change in the radius of the
circle and for a line-segment it denotes a change in the line-segments length. Pt is the point on
the geom that is fixed (e.g., the center of a circle).
2.1.2 INVARIANTS
These describe constraints to be solved for the geoms. The initial version of our system has been
designed to generate plan fragments for a variable-radius circle and a variable length linesegment on a fixed workplane, with constraints on the distances between these geoms and points,
lines, and other geoms on the same workplane. There are seven invariant types to represent these
constraints. Examples of two such invariants are:



(Invariant-point g pt glb-coords) which specifies that the point pt of geom g is
coincident with the global coordinates glb-coords, and
(Fixed-distance-point g pt dist bias) which specifies that the geom g lies at a fixed
distance dist from point pt; bias can be either BIAS_INSIDE or BIAS_OUTSIDE
depending on whether g lies inside or outside a circle of radius dist around point pt.

2.1.3 LOCI
These represent sets of possible values for a geom parameter, such as the position of a point on a
geom. The various kinds of loci can be grouped into either a 1d-locus (representable by a set of
parametric equations in one parameter) or a 2d-locus (representable by a set of parametric
equations in two variables). For, example a line is a 1d locus specified as (make-line-locus
through-pt direc) and represents an infinite line passing through through-pt and having a
direction direc. Other loci represented in the system include rays, circles, parabolas, hyperbolas,
and ellipses.
2.1.4 MEASUREMENTS
These are used to represent the computation of some function, object, or relationship between
objects. These terms are mapped into a set of service routines which get called by the plan
fragments. An example of a measurement term is: (0d-intersection 1d-locus1 1d-locus2). This
represents the intersection of two 1d-loci. In the normal case, the intersection of two 1dimensional loci is a point. However, there may be singular cases, for example, when the two
loci happen to coincide; in such a case their intersection returns one of the locus instead of a
point. There may also be degenerate cases, for example, when the two loci do not intersect; in
such a case, the intersection is undefined. These exceptional conditions are also represented with
each measurement type and are used during the second phase of the plan generation process to
elaborate a skeletal plan (see Section 3.3).
424

fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTION

2.1.5 GEOMS
These are the objects of interest in solving geometric constraint satisfaction problems. Examples
of geoms are lines, line-segments, circles, and rigid bodies. Geoms have degrees of freedoms
which allow them to vary in location and size. For example, in 3D-space a circle with a variable
radius, has three translational, two rotational, and one dimensional degree of freedom.
The configuration variables of a geom are defined as the minimal number of real-valued
parameters required to specify the geometric entity in space unambiguously. Thus, a circle has
six configuration variables (three for the center, one for the radius, and two for the plane
normal). In addition, the representation of each geom includes the following:





name: a unique symbol to identify the geom;
action-rules: a set of rules that describe how invariants on the geom can be
preserved or achieved by actions (see below);
invariants: the set of current invariants on the geom;
invariants-to-be-achieved: the set of invariants that need to be achieved for the
geom.

2.1.6 ACTION RULES
An action rule describes the effect of an action on an invariant. There are two facts of interest to
a planner when constructing a plan: (1) how to achieve an invariant using an action and (2) how
to choose actions that preserve as many of the existing invariants as possible. In general, there
are several ways to achieve an invariant and several actions that will preserve an invariant. The
intersection of these two sets of actions is the set of feasible solutions. In our system, the effect
of actions is represented as part of geom-specific knowledge in the form of Action rules, whereas
knowledge about how to compute intersections of two or more sets of actions is represented as
geometry-specific knowledge (since it does not depend on the particular geom being acted on).
An action rule consists of a three-tuple (pattern, to-preserve, to-[re]achieve). Pattern is the
invariant term of interest; to-preserve is a list of actions that can be taken without violating the
pattern invariant; and to-[re]achieve is a list of actions that can be taken to achieve the invariant
or re-achieve an existing invariant clobbered by an earlier action. These actions are stated in
the most general form possible. The matching rules in the Geometry Knowledge base are then
used to obtain the most general unifier of two or more actions. An example of an action rule,
associated with variable-radius circle geoms is:
pattern: (1d-constrained-point ?circle (>> ?circle CENTER) ?1dlocus)
to-preserve: (scale ?circle (>> ?circle CENTER) ?any)
(translate ?circle (v- (>> ?1dlocus ARBITRARY-POINT)
(>> ?circle CENTER))
to-[re]achieve: (translate ?circle (v- (>> ?1dlocus ARBITRARY-POINT)
(>> ?circle CENTER))

(AR-1)

This action rule is used to preserve or achieve the constraint that the center of a circle geom
lie on a 1d locus. There are two actions that may be performed without violating this constraint:
(1) scale the circle about its center. This would change the radius of the circle but the position of
the center remains the same and hence the 1d-constrained-point invariant is preserved. (2)
425

fiBHANSALI, KRAMER & HOAR

translate the circle by a vector that goes from its current center to an arbitrary point on the 1dimensional locus ((v- a b) denotes a vector from point b to point a). To achieve this invariant
only one action may be performed: translate the circle so that its center moves from its current
position to an arbitrary position on the 1-dimensional locus.
2.1.7 SIGNATURES
For completeness, it is necessary that there exist a plan fragment for each possible combination
of constraints on a geom. However, in many cases, two or more constraints describe the same
situation for a geom (in terms of its degrees of freedom). For example, the constraints that
ground the two end-points of a line-segment and the constraints that ground the direction, length,
and one end-point of a line-segment both reduce the degrees of freedom of the line-segment to
zero and hence describe the same situation. In order to minimize the number of plan fragments
that need to be written, it is desirable to group sets of constraints that describe the same situation
into equivalence classes and represent each equivalence class using a canonical form.
The state of a geom, in terms of the prior constraints on it, is summarized as a signature. A
signature scheme for a geom is the set of canonical signatures for which plan fragments need to
be written. In Kramers earlier work (1993) the signature scheme had to be determined manually
by examining each signature obtained by combining constraint types and designating one from a
set of equivalent signatures to be canonical. Our approach allows us to construct the signature
scheme for a geom automatically by using reformulation rules (described shortly). A
reformulation rule rewrites one or more constraints into a simpler form. The signature scheme is
obtained by first generating all possible combinations of constraint types to yield the set of all
possible signatures. These signatures are then reduced using the reformulation rules until each
signature is reduced to the simplest form. The set of (unique) signatures that are left constitute
the signature scheme for the geom.
As an example, consider the set of constraint types on a variable radius circle. The signature
for this geom is represented as a tuple <Center, Normal, Radius, FixedPts, FixedLines> where:







Center denotes the invariants on the center point and can be either Free (i.e., no
constraint on the center point), L2 (i.e., center point is constrained to be on a 2dimensional locus), L1 (i.e., center point is constrained to be on a 1-dimensional
locus), or Fixed.
Normal denotes the invariant on the normal to the plane of the circle and can be
either Free, L1, or Fixed (in 2D it is always fixed).
Radius denotes the invariant on the radius and can be either Free or Fixed.
FixedPts denotes the number of Fixed-distance-point invariants and can be either 0,1,
or 2.
FixedLines denotes the number of Fixed-distance-line invariants and can be either
0,1, or 2.

L2 and L1 denote a 2D and 1D locus respectively. If we assume a 2D geometry, the L2 invariant
on the Center is redundant, and the Normal is always Fixed. There are then 3 x 1 x 2 x 3 x 3 = 54
possible signatures for the geom. However, several of these describe the same situation. For
example, the signature:
<Center-Free,Radius-Free, FixedPts-0,FixedLines-2>
which describes a circle constrained to be at specific distances from two fixed lines, can be
rewritten to:
426

fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTION

<Center-L1, Radius-Free,FixedPts-0,FixedLines-0>
which describes a circle constrained to be on a 1-dimensional locus (in this case the angular
bisector of two lines). Using reformulation rules, we can derive the signature scheme for variable
radius circles consisting of only 10 canonical signatures given below:
<Center-Free,Radius-Free, FixedPts-0,FixedLines-0>
<Center-Free,Radius-Free, FixedPts-0,FixedLines-1>
<Center-Free,Radius-Free, FixedPts-1,FixedLines-0>
<Center-Free,Radius-Fixed, FixedPts-0,FixedLines-0>
<Center-L1,Radius-Free, FixedPts-0,FixedLines-0>
<Center-L1,Radius-Free, FixedPts-0,FixedLines-1>
<Center-L1,Radius-Free, FixedPts-1,FixedLines-0>
<Center-L1,Radius-Fixed, FixedPts-0,FixedLines-0>
<Center-Fixed,Radius-Free, FixedPts-0,FixedLines-0>
<Center-Fixed,Radius-Fixed, FixedPts-0,FixedLines-0>
Similarly, the number of signatures for line-segments can be reduced from 108 to 19 using
reformulation rules.
2.2 Geometry Specific Knowledge
The geometry specific knowledge is organized as three different kinds of rules.
2.2.1 MATCHING RULES
These are used to match terms using geometric properties. The planner employs a unification
algorithm to match actions and determine whether two actions have a common unifier. However,
the standard unification algorithm is not sufficient for our purposes, since it is purely syntactic
and does not use knowledge about geometry. To illustrate this, consider the following two
actions:
(rotate $g $pt1 ?vec1 ?amt1), and
(rotate $g $pt2 ?vec2 ?amt2).
The first term denotes a rotation of a fixed geom $g, around a fixed point $pt1 about an
arbitrary axis by an arbitrary amount. The second term denotes a rotation of the same geom
around a different fixed point $pt2 with the rotation axis and amount being unspecified as before.
Standard unification fails when applied to the above terms because no binding of variables
makes the two terms syntactically equal2. However, resorting to knowledge about geometry, we
can match the two terms to yield the following term:
(rotate $g $pt1 (v- $pt2 $pt1) ?amt1)
which denotes a rotation of the geom around the axis passing through points $pt1 and $pt2. The
point around which the body is rotated can be any point on the axis (here it is arbitrarily chosen
to be one of the fixed points, $pt1) and the amount of rotation can be anything.
The planner applies the matching rules to match the outermost expression in a term first; if
no rule applies, it tries subterms of that term, and so on. If none of the matching rules apply, then
2 Specifically, unification fails when it tries to unify $pt1 and $pt2.

427

fiBHANSALI, KRAMER & HOAR

this algorithm degenerates to standard unification. The matching rules can also have conditions
attached to them. The condition can be any boolean function; however, for the most part they
tend to be simple type checks.
2.2.2 REFORMULATION RULES
As mentioned earlier, there are several ways to specify constraints that restrict the same degrees
of freedom of a geom. In GCE, plan fragments are indexed by signatures which summarize the
available degrees of freedom of a geom. To reduce the number of plan fragments that need to be
written and indexed, it is desirable to reduce the number of allowable signatures. This is
accomplished with a set of invariant reformulation rules which are used to rewrite pairs of
invariants on a geom into an equivalent pair of simpler invariants (using a well-founded
ordering). Here equivalence means that the two sets of invariants produce the same range of
motions in the geom. This reduces the number of different combinations of invariants for which
plan fragments need to be written. An example of invariant reformulation is the following:
(fixed-distance-line ?c ?l1 ?d1 BIAS_COUNTERCLOCKWISE)
(fixed-distance-line ?c ?l2 ?d2 BIAS_CLOCKWISE)



(RR-1)

(1d-constrained-point ?c (>> ?c center) (angular-bisector
(make-displaced-line ?l1 BIAS_LEFT ?d1)
(make-displaced-line ?l2 BIAS_RIGHT ?d2)
BIAS_COUNTERCLOCKWISE
BIAS_CLOCKWISE))

This rule takes two invariants: (1) a geom is at a fixed distance to the left of a given line, and
(2) a geom is at a fixed distance to the right of a given line. The reformulation produces the
invariant that the geom lies on the angular bisector of two lines that are parallel to the two given
lines and at the specified distance from them. Either of the two original invariants in conjunction
with the new one is equivalent to the original set of invariants.
Besides reducing the number of plan fragments, reformulation rules also help to simplify
action rules. Currently all action rules (for variable radius circles and line-segments) use only a
single action to preserve or achieve an invariant. If we do not restrict the allowable signatures on
a geom, it is possible to create examples where we need a sequence of (more than one) actions in
the rule to achieve the invariant, or we need complex conditions that need to be checked to
determine rule applicability. Allowing sequences and conditionals on the rules increases the
complexity of both the rules and the pattern matcher. This makes it difficult to verify the
correctness of rules and reduces the efficiency of the pattern matcher.
Using invariant reformulation rules allows us to limit action rules to those that contain a
single action. Unfortunately, it seems that we still need conditions to achieve certain invariants.
For example, consider the following invariant on a variable radius circle:
(fixed-distance-point ?circle ?pt ?dist BIAS_OUTSIDE)
which states that a circle, ?circle be at some distance ?dist from a point ?pt and lie outside a
circle around ?pt with radius ?dist. One action that may be taken to achieve this constraint is:
(scale ?circle
(>> ?circle center)
428

fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTION

(minus (>> (v- (>> ?circle center) ?pt)
magnitude)
?dist)
that is, scale the circle by setting its radius to the distance between its center and the point ?pt
minus the scalar amount ?dist (see Figure 4). However, this action achieves the constraint only
when the circle happens to lie outside the circular region of radius ?dist and center ?pt.

$C
$c

Figure 4. The geom $c can be scaled to touch $C only if the center of $c lies in the shaded region.

Therefore, we need a pre-condition to the rule that checks if this is indeed the case. Note that
the above action is necessary for completeness (otherwise the planner would not be able to solve
certain cases which have a solution). Instead of allowing conditional rules, we use rules without
condition and in the second phase of the plan generation check to see that there are no
exceptions. Thus, in the above example, an exception would be detected since the third argument
of the scale operation returns a negative number  considered an exception condition for a scale
operation.
2.2.3 PRIORITIZING STRATEGY
Given a set of invariants to be achieved on a geom, a planner generally creates multiple
solutions. All of these are valid solutions and in the absence of exception conditions will yield
the same configuration of a geom. However, some plan fragments will contain redundant action
sequences (e.g., two consecutive translations). Moreover, when the geom is under constrained or
when there are exception conditions, some plan fragments will be able to provide a solution
whereas others will not. The prioritization strategy is used to prioritize the skeletal plan
fragments so that plan fragments with the least redundancy and most flexibility can be chosen.
Eliminating plan fragments with redundant actions turns out to be straightforward. We
assume that there is only one degree of dimensional freedom for each geometric body. Under
this assumption it can be proved that 1 translation, 1 rotation, and 1 scale is sufficient to change
the configuration of an object to an arbitrary configuration in 3D space. Therefore, any plan
fragment that contains more than one instance of an action type contains redundancies and can
be rewritten to an equivalent plan fragment by eliminating redundant actions, or combining two
or more action into a single composite action. As an example, consider the following pair of
translations on a geom:


(translate $g ?vec)
429

fiBHANSALI, KRAMER & HOAR



(translate $g (v- ?to 2 (>> $g center)))

where ?vec represents an arbitrary vector and ?to2 represents an arbitrary position. If ?to2 is
independent of any positional parameter of the geom, then the first translate action is redundant
and can be removed. Hence all plan fragments that contain such redundant actions can be
eliminated.
To prioritize the remaining plan fragments the following principle is used:
Prefer solutions that subsume an alternative solution.
The rationale for this principle is that it permits greater flexibility in solving constraints when
there are exception conditions. For example, suppose there are two solutions for a circle geom:
Solution 1: Translate the circle so that the center lies at a fixed position on a 1dimensional locus.
Solution 2: Translate the circle so that the center lies at an arbitrary point on a 1dimensional locus; then scale by some fixed amount (which is a function of the
position of the arbitrary point).
The first solution is subsumed by the second solution since we can always choose the
arbitrary point in Solution 2 to be at the fixed position specified in Solution 1 (the scale operation
in that case leaves the dimension of the circle unchanged). Therefore Solution 2 is preferred over
Solution 1.
The subsumption relation imposes a partial order on the set of skeletal plan fragments. The
prioritization strategy selects the maximal elements of this partial order. At runtime each of these
is tried in turn until one of them yields a solution.

3.0 Plan Fragment Generation
The plan fragment generation process is divided into two phases (Figure 1). In the first phase a
specification of the plan fragment is taken as input, and a planner is used to generate a set of
skeletal plans. These form the input to the second phase which chooses one or more of the
skeletal plans and elaborates them to take care of singularities and degeneracies. The output of
this phase are complete plan fragments.
3.1 Phase I
A skeletal plan is generated using a breadth-first search process. Figure 5 gives the general form
of a search tree produced by the planner. The first action is typically a reformulation where the
planner uses the reformulation rules to rewrite the geom invariants into a canonical form. Next,
the planner searches for actions that produce a state in which at least 1 invariant in the Preserved
list is preserved or at least 1 action in the To-be-achieved (TBA) list is achieved. The preserved
and achieved invariants are pushed into the Preserved list, and the clobbered or unachieved
invariants are pushed into the TBA list of the child state.
The above strategy will produce intermediate nodes in the search tree which might clobber
one or more preserved invariant without achieving any new invariant or might produce a state
which is identical to its parent state in terms of the invariants on the Preserved

430

fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTION

Preserved: P
TBA: A
Reformulate
Preserved: P
TBA: A
Action 1
Preserved: P1
TBA: A1

Action 3
Action 2
Preserved: P2
TBA: A2

Preserved: P3
TBA: A3

Actions
Preserved: P + A
TBA: nil
Figure 5. Overview of the search tree produced by the planner

and TBA list. This is because in the initial state a geom may be in some arbitrary configuration
(among a set of allowable configurations) and it may be necessary to first move the geom to an
alternative allowable configuration to find the optimal solution.
To illustrate this need, consider the example in Figure 6. In this example, there is one prior
constraint on the variable radius circle geom: its center lies on a 1-dimensional locus. The new
constraint to be achieved is: the geom should lie at a fixed distance from a line. In order to
achieve this constraint only one of the following two actions may be taken: 1) scale

(a)

(b) Scale

(c) Translate

(d) Translate & scale

Figure 6. Example to illustrate the need for actions that produce a state equivalent to the parent state.

431

fiBHANSALI, KRAMER & HOAR

the circle so that it is at a fixed distance from the line (Figure 6b), or 2) translate the circle to a
new position on the 1-dimensional locus so that it touches the line (Figure 6c). However, there
are an infinite number of additional solutions consisting of combinations of scale and translation
(Figure 6d). These solutions can be derived if the planner first changes the configuration of the
geom so that it only preserves the existing invariant without achieving the new invariant (i.e.,
scale by an arbitrary amount or translate to an arbitrary point on the 1-dimensional locus)
followed by an action that achieves the new invariant. Therefore the planner also creates child
states that are identical to the parent state in terms of invariants on the Preserved and TBA lists.
The planner iteratively expands each leaf node in the search tree until one of the following is
true:
1. The node represents a solution; that is, the TBA list is nil.
2. The node represents a cycle; that is, the invariants in the Preserved and TBA lists are
identical to one of the ancestor nodes.
The node is then marked as terminal and the search tree is pruned at that point. If all leaf nodes
are marked as terminal, then the search terminates. The planner then collects all terminal nodes
that are solutions. The plan-steps of each of those solution nodes represents a skeletal plan
fragment. When multiple skeletal plan fragments are obtained by the planner, one of them is
chosen using the prioritizing rule described earlier and is passed to the second phase of the plan
fragment generation.
3.2 Phase I: Example
We use the example of Section 1 to illustrate Phase I of the planner. The planner begins by
attempting to reformulate the given constraints. It uses reformulation rule RR-1 described earlier
and repeated below for convenience:
(fixed-distance-line ?c ?l1 ?d1 BIAS_COUNTERCLOCKWISE)
(fixed-distance-line ?c ?l2 ?d2 BIAS_CLOCKWISE)



(RR-1)

(1d-constrained-point ?c (>> ?c center) (angular-bisector
(make-displaced-line ?l1 BIAS_LEFT ?d1)
(make-displaced-line ?l2 BIAS_RIGHT ?d2)
BIAS_COUNTERCLOCKWISE
BIAS_CLOCKWISE))

ii

L2
i

iii

iv

L1

Figure 7. Four possible angular bisectors of two lines L1 and L2. The bias symbols for L1 and
L2 corresponding to ray (i) is BIAS_COUNTERCLOCKWISE & BIAS_CLOCKWISE,respectively.

432

fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTION

In the above rule there are two measurement terms: make-displaced-line and angular-bisector.
Make-displaced-line takes three arguments: a line, l, a bias symbol indicating whether the
displaced line should be to the left or right of l, and a distance, d. It returns a line parallel to the
given line l at a distance d to the left or right of the line depending on the bias. Angular-bisector
takes two lines, l1 and l2, and two bias symbols and returns one of the four rays that bisects the
lines l1 and l2 depending on the bias symbols (see Figure 7). After reformulation, the state of the
search tree is as shown in Figure 8. No further reformulation rules are applicable at this point.

Preserved: (fixed-distance-line $c $L1 $dist1 BIAS_COUNTERCLOCKWISE)
TBA:
(fixed-distance-line $c $L2 $dist2 BIAS_CLOCKWISE)

Reformulation

Preserved: (fixed-distance-line $c $L1 $dist1 BIAS_COUNTERCLOCKWISE)
TBA:
(1d-constrained-point $c
(>> $c CENTER)
(angular-bisector
(make-displaced-line $L1 $BIAS_LEFT $dist1)
(make-displaced-line $L2 $BIAS_RIGHT $dist2)
BIAS_COUNTERCLOCKWISE
BIAS_CLOCKWISE))
Figure 8. Search tree after reformulating invariants

Next, the planner searches for actions that can achieve the new invariant or preserve the
existing invariant or do both. We only describe the steps involved in finding actions that satisfy
the maximal number of constraints (in this case, two). The planner first finds all actions that
achieve the 1d-constrained-point invariant by examining the action rules associated with the
variable-circle geom. The action rule AR-1 contains a pattern that matches the 1d-constrainedpoint invariant:
pattern: (1d-constrained-point ?circle (>> ?circle center) ?1dlocus)
to-preserve: (scale ?circle (>> ?circle center) ?any)
(translate ?circle (v- (>> ?1dlocus arbitrary-point)
(>> ?circle center))
to-[re]achieve: (translate ?circle (v- (>> ?1dlocus arbitrary-point)
(>> ?circle center))
with the following bindings:

(AR-1)

{?circle = $c, ?1d-locus = (angular-bisector (make-displaced -line ...) ...)}
Substituting these bindings we obtain the following action:

433

fiBHANSALI, KRAMER & HOAR

(translate $c (v- (>> (angular-bisector (make-displaced-line $L1 BIAS_LEFT $dist1)
(make-displaced-line $L2 BIAS_RIGHT $dist2)
arbitrary-point)
(>> $c center)))
(a1)
which can be taken to achieve the constraint. Similarly, the planner finds all actions that will
preserve the fixed-distance-line invariant. The relevant action rule is the following:
pattern: (fixed-distance-line ?circle ?line ?distance)
(AR-2)
to-preserve: (translate ?circle (v- (>> (make-line-locus (>> ?circle center)
(>> ?line direction))
arbitrary-point)
(>> ?circle center))
to-[re]achieve: (translate ?circle (v- (>> (make-displaced-line
?line
BIAS_LEFT

(plus ?distance (>> ?circle radius)))
arbitrary-point)
(>> ?circle center)))
The relevant action after the appropriate substitutions is:
(translate $c (v- (>> (make-line-locus
(>> $c center)
(>> L1 direction))
arbitrary-point)
(>> $c center))

(a2)

Now, to find an action that both preserves the preserved invariant and achieves the TBA
invariant, the planner attempts to match the preserving action (a2) with the achieving action (a1).
The two actions do not match using standard unification, but match employing the following
geometry-specific matching rule:
# To move to an arbitrary point on two
# different loci, move to the point that
# is the intersection of the two loci

(v- (>> $1d-locus1 arbitrary-point) $to)
(v- (>> $1d-locus2 arbitrary-point) $to)



(v- (0d-intersection $1d-locus1 $1d-locus2) $to)
to yield the following action:
(translate $c (v- (0d-intersection (angular-bisector
(make-displaced-line ...) ...)
(make-line-locus (>> $c center) (>> $L1 direction))
(>> $c CENTER)))
This action moves the circle to the point shown in Figure 9 and achieves both the constraints.
This simple one-step plan constitutes a skeletal plan fragment.
434

fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTION

$L2

$c

angular-bisector
make-line-locus

$dist2
$dist1
$L1

Figure 9. The

denotes the point to which the circle is moved.

There are two other actions that are generated by the planner in the first iteration. One of
these achieves the new constraint but clobbers the prior invariant. The other moves the circle to
another configuration without achieving the new constraint but preserving the prior constraint.
The first action produces a terminal state since there are no more constraints to be achieved.
Hence the search tree is pruned at that point. However, the planner continues to search for
alternative solutions by expanding the other two nodes. After two iterations the following
solutions are obtained:
1. Translate to the intersection of the angular-bisector and make-line-locus.
2. Translate to an arbitrary point on the angular-bisector, followed by a translation to
the intersection point.
3. Translate to an arbitrary point of make-line-locus, followed by a translation to the
intersection point.
4. Translate to an arbitrary point on the angular-bisector and then scale.
At this stage the first phase of the plan fragment generation is terminated and the skeletal
plan fragments are passed on to the second phase of the planner.
3.3 Phase II: Elaboration of Skeletal Plan Fragment
The purpose of Phase 2 planning is to i) select one or more skeletal plan fragments, and ii)
elaborate them so that they generate the most desirable configuration when the geom is under
constrained as well as handle exception conditions.
3.3.1 SELECTION OF SKELETAL PLAN FRAGMENTS
There are two primary considerations in selecting a skeletal plan fragment  reduce redundant
actions in the plan and increase generality of the plan. These considerations are used to formulate
a prioritization strategy described in Section 2. The strategy is implemented as a lookup table
that assigns weights to the various plan fragments. The plan fragments with the maximal weights
are selected for elaboration by Phase 2. Readers interested in the implementation details are
referred to (Hoar, 1995).
3.3.2 PLAN FRAGMENT ELABORATION
Plan fragment elaboration refines a skeletal plan fragment in two ways. First, it refines actions
435

fiBHANSALI, KRAMER & HOAR

that are under constrained (e.g., translate to an arbitrary point on a locus) by appropriate
instantiation of the unconstrained parameters (e.g., selecting a specific point on a locus). Second,
it handles exception conditions that result in under constrained or over-constrained systems. Both
action refinement and exception handling are treated using a common technique.
Plan elaboration is based on the "principle of least motion": when there are multiple
solutions for a problem choose the solution that minimizes the total amount of perturbation
(motion) in the system. Implementing the principle requires the definition of a motion function,
CA,G for each action, A, and geom type, G. For example, for a translation of a geom, the motion
function, CT,circle could be the square of the displacement of the center of the geom from its
initial to its final position. We also need a motion summation function,  G that sums the motion
produced by individual actions on a geom G. An example of the summation function is the
normal addition operator: plus. The total motion produced in a geom is computed using the
summation function and the motion functions for action- geom pairs.
When a plan fragment is under constrained, the expression representing the total motion
would contain one or more variables representing the ungrounded parameters of the geom.
Formal optimization techniques, based on finite difference methods, can be used to obtain values
of the parameters that would minimize the motion function. However, we use a more efficient,
algorithm based on hill-climbing which does not guarantee optimality but yields good results in
practice. The use of this heuristic algorithm is justified in many interactive applications like
sketching, where a fast, sub-optimal solution is preferable to a computationally expensive,
optimal one.
The algorithm begins by segmenting all continuous loci into discrete intervals. It then
systematically searches the resultant, discrete n-dimensional space. The algorithm first finds a
local minima along one dimension while holding the other variables at constant values. Then it
holds the first variable at the minimum value found and searches for a lower local minima along
the second dimension and so on. Although this algorithm does not guarantee finding a global or
even a local minima, it is very efficient and yields good results in practice. The implemented
algorithm is somewhat more complex than the simple description above; further details can be
found elsewhere (Hoar, 1995).
Exception conditions can be handled using the same technique as above. Exception
conditions are identified when a service routine returns a set of solutions or no solution (e.g., a
routine to compute the intersection of two 1-dimensional loci returns a 1-dimensional locus or
nil). Multiple solutions represent an under constrained system and requires a search among the
set of solutions returned. These conditions are handled exactly as described in the previous
paragraph. When a no-solution exception occurs, the system aborts the plan fragment and prints
a diagnostic message explaining why the constraint could not be solved.
3.4 Phase II: Example
Four skeletal plan fragments were generated in the first phase of the planner (Section 3.2). Using
the rule for eliminating redundant translations given earlier, the second and third plan fragments
can be reduced to single translation plan fragments equivalent to the first plan fragment. This
leaves only two distinct plan fragment solutions to consider.
Using the prioritizing rule, the system concludes that the first plan fragment consisting of a
single translation is subsumed by the second plan fragment consisting of a translation and a
scale. Thus, the second plan fragment is chosen as the preferred solution.
This plan fragment is not deterministic since it contains an action that translates the circle
436

fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTION

geom to an arbitrary point on the angular-bisector. Therefore, the system inserts an iterative loop
that computes the amount of motion of the circle for various points on the angular bisector,
breaking out of the loop when it finds a minima. Similarly, for each service routine that may
return an exception, the system inserts a case statement which contains a loop to handle
situations when more than one solution is returned. Online Appendix 1 contains a complete
example of a plan fragment generated by the system.

4.0 Results
The plan fragment generator described here has been implemented using CLOS (Common Lisp
Object System). We have implemented parts of the geometric constraint engine (GCE) described
by Kramer in C++ with an XMotif based graphical user interface. We have also written a
translator that translates the synthesized plan fragments into C++. A complete plan fragment
library for a representative geom (line segment) has been synthesized and integrated with the
constraint engine. Using this we have been able to successfully demonstrate the solution of
several geometric constraints. We present below an evaluation of the system.
The primary contribution of this research is not a novel geometric constraint satisfaction
approach. From the perspective of constraint satisfaction techniques, the novel feature of our
approach - degrees of freedom analysis - has already been described in earlier works by the
second author (Kramer, 1992, 1993). The goal of this research was to develop automated
techniques that will enable the degrees of freedom approach to scale up by reducing the amount
of effort needed in creating plan fragment libraries. Hence, our evaluation is based on how
successful we have been in automating the plan fragment synthesis process.
We have used the plan fragment generator described above to automatically synthesize plan
fragments for two representative geoms -- line-segments and circles -- in 2D. There are seven
types of constraints and thirty four rules in the system (12 action rules for line-segments, 8 action
rules for circles, 7 Reformulation rules, and 7 Matching rules). Using these rules we have
successfully generated skeletal plan fragments for various combinations of constraints on line
segments (249) and circles (50). The largest search tree produced by the planner is on the order
of a few hundred nodes and takes a few minutes on a Macintosh Quadra. For evaluation
purposes, we present data for one representative geom - line segment.
4.1 Programming Effort
Figure 10 shows the number of lines of code comprising the current system. The areas in solid
represent code that was written manually. This includes about 5000 lines of CLOS code for the
plan fragment synthesizer, 5400 lines of C/C++ for the user interface, and 3300 lines of C/C++
for the support routines. The hatched area represent code that was synthesized by the plan
fragment generator. It represents about 27000 lines of C++ code (for plan fragments for the linesegment geom). The size of the synthesized plan fragment (about 121 lines average) is much less
than that of plan fragments written manually (in C) in the original version of GCE. Thus, using
an automated plan fragment generator has considerably reduced the amount of programming.
While a reduction ratio of 5:1 is a good indicator of the reduction in programming effort, it is
subject to criticism since it compares code in two very different programming languages and
comprising different degrees of difficulty.
A more accurate evaluation is obtained by comparing the total effort required in writing plan
fragments manually against the total effort required in synthesizing them using the
437

fiBHANSALI, KRAMER & HOAR

12% (CLOS)

13% (C/C++)

67%
(C++)

User Interface

8% (C/C++)

Support routines

Generator

Plan Fragments

Figure 10. Lines of code in different parts of the system

technique described in this paper. It is extremely difficult, if not impossible, to do this in any
controlled experimental setting because of the number of factors and cost involved. The best that
can be done is to compare the empirical data based on our experience in developing the system.
The following table shows the effort in person days in developing the plan fragment library for
the line-segment geom using our technique.

Plan Fragment Generator
Manually

Research
90
0

Development
150
498

Total
210
498

Table 1. Effort (in person-days) in creating plan fragments
For the effort involved in writing plan fragments manually, we use a conservative estimate
of 2 person days for each plan fragment3. The table shows that using the plan fragment generator
we obtained a 58% reduction in effort in creating the plan fragment library. The testing and
debugging time has been ignored and assumed to be the same for both cases (although we
believe that this time is much more for manually generated plan fragments).
4.2 Scalability
A much stronger evidence in support of our technique is obtained when we look at the effort
3 This estimate is based both on the effort required in developing the plan fragment library for GCE as well as
experimental data obtained by having two graduate students write a few plan fragments manually.

438

fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTION

required in extending the plan fragment library by adding more features (e.g., new kinds of
geoms or constraints). To evaluate the scalability of the approach, we decided to extend the plan
fragments to 3D where geoms have added degrees of rotational and translational freedom. Such
an extension when done manually would be significant exercise in software maintenance since it
requires changes to each plan fragment in the library. Using the plan fragment generator we only
needed to revise the rules used by the planner and make changes to the support routines. Since
the support routines were written manually, the cost to modify them is the same in both
approaches, and only the effort needed to rewrite the rules is relevant. It took only 1 week of
effort to rewrite and debug the action rules and synthesize the complete plan fragment library for
3D, and link it successfully with the constraint engine. This is a significant result in
demonstrating that our technique can be used to scale up degrees of freedom analysis to more
complex geoms and geometries.
4.3 Correctness
An important issue that has been ignored so far is: how does one verify the correctness and
completeness of the plan fragment generator? We have done extensive testing and evaluation of
the plan fragments synthesized by the plan fragment generator. Table 2 summarizes the results.
Number of plan fragment specifications
Specs. with no solutions:
Completeness
No solution exists
Missing rules
No symbolic solution
Total
Plan fragments with errors:
Correctness
Faults due to errors in logic
Support routine errors
Total

249
65
13
2
80
0
56
56

Table 2. Completeness and Correctness of synthesized plan fragments
There were eighty plan fragment specifications for which the planner failed to produce a
solution. In sixty five of these specifications, there were no solutions in the general case -- these
specifications represent overconstrained problems, such as constraining one end point of a linesegment to be on a one-dimensional locus when previous constraints have already reduced that
end-points translational degrees of freedom to zero. The only action the planner can take in such
cases is to check that the new constraint is already satisfied. Thirteen of the cases had no
solutions because of two missing rules: one action rule, and one reformulation rule. Once the two
rules were added all the thirteen specifications were solved. Finally, there were only two plan
fragments for which the planner failed to produce an analytical solution. The cases are shown in
Figure 11. To solve such problems we need a reformulation rule that reformulates the existing
invariant to a constraint that the endpoint of $lseg is the curve $L3. Instead of representing
complex 1-dimensional (and higher dimensional) loci like $L3, we assume that the constraint
engine would call a numerical solver that computes the solution iteratively. An alternative would
be to extend the set of support routines to handle such complex loci and their intersections.

439

fiBHANSALI, KRAMER & HOAR

.
$L2

$P
$L3

$lseg

$L1

Figure 11. Example of problem that generated no symbolic solution. $lseg is a line-segment
which is constrained to have one end-point on $L1, have a fixed length, and be tangent to a
circle centered at $P. The new constraint is that the other end-point of $lseg be on $L2.

To check for the correctness of plan fragments, we did an exhaustive evaluation of all the
plan fragments. As can be seen from Table 2, the code that has been synthesized is not perfect.
About 20% of the plan fragments do not function correctly. We analyzed the reasons for the
failure by manually inspecting the plan fragments. The most significant finding was that none of
the failures were due to logical errors in the plan fragments. In other words the skeletal plan
fragments being generated by Phase I were correct and complete. Most of the failures were
because of bugs in the mathematical support routines called by the plan fragments. In a few
instances the failures were traced to bugs in implementing Phase 2 of the plan fragment: either
selecting the wrong skeletal plan fragment or not computing the least motion correctly. We had
not expected the first version of the automatically generated plan fragments to be completely
bug-free. Indeed, the high percentage of plan fragments that do function correctly (almost 80%)
is a very positive result and reflects a significant increase in quality and a corresponding
decrease in maintenance effort for building geometric constraint satisfaction systems using our
approach.

5.0 Related Work
Geometric constraint satisfaction is an old problem. Probably the first application of this problem
to constraint-based sketching was the Sketchpad program developed by Sutherland (1963). The
Sketchpad program was based on constraint relaxation and was limited to problems that were
modeled with point variables.
In the field of mechanical design, a graph based approach to constraint satisfaction has been
described by Serrano (1987). In Serranos approach the constraints are modeled using a
constraint network; a constraint satisfaction engine finds the values of constrained variables that
satisfy the constraints in the network using constraint propagation techniques. The approach
identifies loops or cycles in the network, collapses them into supernodes, and then applies
conventional sequential local propagation. This approach uses numerical iterative techniques
which can have problems with stability. The computational advantage of this approach reduces
when equations are tightly coupled.
Most of the commercial systems that do kinematics analysis are based on numerical iterative
techniques or algebraic techniques or a combination of the two. Although these approaches are in
principle robust, they have several shortcomings that make them inappropriate for real-time
applications.
Among non-commercial systems, a notable new approach to constraint based sketching is
440

fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTION

the Juno-2 being developed at DEC-SRC (Heydon & Nelson, 1994). Constraints in Juno-2 are
specified using an expressive, declarative constraint language which seems powerful enough to
express most constraints that arise in practice. Juno-2 uses a combination of symbolic and
numerical techniques to solve geometric constraints efficiently. A key difference between Juno2 and the degrees of freedom approach is that in Juno-2 the symbolic reasoning is done in the
domain of equations. For example, Juno-2 uses symbolic techniques like local propagation,
unpacking, and unification closure to reduce the number of unknowns in a system of equations.
The equations are then solved by Newtons method. In degrees of freedom analysis, the symbolic
reasoning is done in the domain of geometry rather than equations.
Geometric constraints also arise in robotics, where the primary issues are concerned with
finding a physically realizable path through space for a robot manipulator or a part of an
assembly. A fundamental analytical tool for solving motion planning problems in robotics is the
configuration space framework (Lozano-Perez, 1983). In configuration space approach, the
problem of planning the motion of a part through a space of obstacles is transformed into an
equivalent but simpler problem of planning the motion of a point through a space of enlarged
configuration-space obstacles. Degrees of freedom analysis finesses this problem since it uses
the notion of incremental assembly only as a metaphor for solving geometric constraint systems.
No physical meaning is ascribed to how objects move from where they are to where they need to
be - a factor that is quite important in a real-world assembly problem arising in robotics. The
only use of the plan is to guide the solution of the complicated non-linear equations arising from
formulating and solving the problems algebraically.

6.0 Conclusions
We have described a plan fragment generation methodology that can synthesize plan fragments
for a geometric constraint satisfaction systems by reasoning from first principles about geometric
entities, actions, and topology. The technique has been used to successfully synthesize plan
fragments for a realistic set of constraints and geoms. It may seem that we have substituted one
hard task - writing a complete set of correct plan fragments for various combinations of geoms
and constraints - by an even harder task: creating the knowledge base of rules to automate the
process. The rules are difficult to write and we have found that it is necessary to spend some
effort in debugging the rules. However, we estimate that the total effort to write and debug rules
is still an order of magnitude less than writing and debugging manually written plan fragment
code. Our future work is to investigate how this approach scales up to more complex constraints
and geometries.
Another useful extension of this work would be concerned with pushing the automation one
level further so as to automatically acquire some types of knowledge from simpler building
blocks. For example, a technique for automatically synthesizing the least motion function from
some description of the geometry would be very useful.
In our method the plan fragment generation is divided into two disjoint phases. An
alternative method would be to explore how the two phases can be interleaved. One possibility is
that when there is a degeneracy because of a redundant constraint, the planner could reformulate
the problem by removing the redundant constraint and re-synthesize a skeletal plan fragment
with the new set of constraints. The resultant plan would form a part of the original plan
fragment to deal with the degenerate cases. In other words, plan fragments would be generated
on-the-fly as needed by the constraint solver.

441

fiBHANSALI, KRAMER & HOAR

Acknowledgments
We thank Qiqing Xia who helped in implementing parts of the system described in this paper.
We also acknowledge the support and resources provided by the School of Electrical
Engineering and Computer Science, Washington State University. This work originated while
the first author was at the Knowledge Systems Laboratory, Stanford University, and the second
author was at the Schlumberger Laboratory of Computer Science, Austin.

References
Anantha, R., Kramer, G., & Crawford, R. (1992). An architecture to represent over, under, and
fully constrained assemblies. In Proceedings of ASME Winter Annual Meeting, 233-244.
Borgida, A., Mylopoulos, J., & Reiter, R. (1993). ... and nothing else changes: the frame problem
in procedure specifications. In Proceedings of the 15th International Conference on
Software Engineering, Baltimore, MD.
Brown-Associates. (1993). Applicons GCE: A Strong Technical Framework. Brown Associates
Inc.
Brunkhart, M. W. (1994). Interactive geometric constraint systems. Masters thesis, TR No.
CSD-94-808, Department of EE&CS, University of California, Berkeley.
Fikes, R. E., & Nilsson, N. J. (1971). STRIPS: A new approach to the applicatiion of theorem
proving to problem solving. Artificial Intelligence, 2, 198-208.
Friedland, P. E. (1979). Knowledge-based experiment design in molecular genetics. Tech. report
CSD-79-771, Department of Computer Science, Stanford University.
Hartenberg, R. S., & Denavit, J. (1964). Kinematic Synthesis of Linkages. New York: McGraw
Hill.
Heydon, A., & Nelson, G. (1994). The Juno-2 constraint-based drawing editor. SRC Research
report 131a, Digital Systems Research Center, Palo Alto, CA.
Hoar, T. (1995). Automatic program synthesis for geometric constraint satisfaction. Masters
Thesis, School of EECS, Washington State University.
Kramer, G. A. (1992). Solving Geometric Constraint Systems: A Case Study in Kinematics.
Cambridge, MA: MIT Press.
Kramer, G. A. (1993). A geometric constraint engine. Artificial Intelligence, 58(1-3), 327-360.
Liu, Y., & Popplestone, R. J. (1990, ). Symmetry constraint inference in assembly planning:
automatic assembly configuration specification. In Proceedings of AAAI-90, Boston, MA,
1038-1044.
Lozano-Perez, T. (1983). Spatial planning: A configuration space approach. IEEE Transactions
on Computers, C-32, 108-120.
442

fiPRINCIPLED SYMBOLIC GEOMETRIC CONSTRAINT SATISFACTION

Press, W. H., Flannery, B. P., Teukolsky, S. A., & Vetterling, W. T. (1986). Numerical Recipes:
The Art of Scientific Computing. Cambridge, England: Cambridge University Press.
Salomons, O. (1994). Computer support in the design of mechanical products. Ph.D. Thesis,
Universiteit Twente, Netherlands.
Serrano, D. (1987). Constraints in conceptual design. Ph.D. thesis, Massachusetts Institute of
Technology.
Shah, J. J., & Rogers, M. T. (1993). Assembly modeling as an extension of feature-based design.
Research in Engineering Design, 5, 218-237.
Sussman, G. J. (1975). A Computer Model of Skill Acquisition. New York: American Elsevier.
Sutherland, I. E. (1963). Sketchpad, a man-machine graphical communication system. Ph.D.
Thesis, Massachusetts Institute of Technology.

443

fiJournal of Artificial Intelligence Research 4 (1996) 477-507

Submitted 9/95; published 6/96

On Partially Controlled Multi-Agent Systems
Ronen I. Brafman

brafman@cs.ubc.ca

Computer Science Department
University of British Columbia
Vancouver, B.C., Canada V6L 1Z4

Moshe Tennenholtz

moshet@ie.technion.ac.il

Industrial Engineering and Management
Technion - Israel Institute of Technology
Haifa 32000, Israel

Abstract

Motivated by the control theoretic distinction between controllable and uncontrollable
events, we distinguish between two types of agents within a multi-agent system: controllable
agents , which are directly controlled by the system's designer, and uncontrollable agents ,
which are not under the designer's direct control. We refer to such systems as partially
controlled multi-agent systems, and we investigate how one might inuence the behavior of
the uncontrolled agents through appropriate design of the controlled agents. In particular,
we wish to understand which problems are naturally described in these terms, what methods
can be applied to inuence the uncontrollable agents, the effectiveness of such methods, and
whether similar methods work across different domains. Using a game-theoretic framework,
this paper studies the design of partially controlled multi-agent systems in two contexts: in
one context, the uncontrollable agents are expected utility maximizers, while in the other
they are reinforcement learners. We suggest different techniques for controlling agents'
behavior in each domain, assess their success, and examine their relationship.

1. Introduction
The control of agents is a central research topic in two engineering fields: Artificial Intelligence (AI) and Discrete Events Systems (DES) (Ramadge & Wonham, 1989). One
particular area both of these fields have been concerned with is multi-agent environments;
examples include work in distributed AI (Bond & Gasser, 1988), and work on decentralized
supervisory control (Lin & Wonham, 1988). Each of these fields has developed its own
techniques and has incorporated particular assumptions into its models. Hence, it is only
natural that techniques and assumptions used by one field may be adopted by the other or
may lead to new insights for the other field.
In difference to most AI work on multi-agent systems, work on decentralized discrete
event systems distinguishes between controllable and uncontrollable events. Controllable
events are events that can be directly controlled by the system's designer, while uncontrollable events are not directly controlled by the system's designer. Translating this terminology into the context of multi-agent systems, we introduce the distinction between two
types of agents: controllable agents , which are directly controlled by the system's designer,
and uncontrollable agents , which are not under the designer's direct control. This leads
c 1996 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiBrafman & Tennenholtz

naturally to the concept of partially controlled multi-agent system (PCMAS) and to the
following design challenge: ensuring that all agents in the system behave appropriately
through adequate design of the controllable agents. We believe that many problems are
naturally formulated as instances of PCMAS design. Our goal is to characterize important
instances of this design problem, to examine the tools that can be used to solve it, and to
assess the effectiveness and generality of these tools.
What distinguishes partially controlled multi-agent systems in the AI context from similar models in DES are the structural assumptions we make about the uncontrolled agents
involved. Unlike typical DES models which are concerned with physical processes or devices, AI is particularly interested in self-motivated agents, two concrete examples of which
are rational agents, i.e., expected utility maximizers, and learning agents, e.g., reinforcement learners. Indeed, these examples constitute the two central models of self-motivated
agents in game theory and decision theory, referred to as the educative and evolutive models
(e.g., see Gilboa & Matsui, 1991). The special nature of the uncontrollable agents and the
special structure of the uncontrollable events they induce is what differentiates PCMAS
from corresponding models in the DES literature. This difference raises new questions and
suggests a new perspective on the design of multi-agent systems. In particular, it calls for
techniques for designing controllable agents that, by exploiting the structural assumptions,
can inuence the behavior of the uncontrollable agents and lead the system to a desired
behavior.
In order to understand these issues, we study two problems that can be stated and solved
by adopting the perspective of PCMAS design; problems which by themselves should be
of interest to a large community. In both of these problems our goal is to inuence the
behavior of agents that are not under our control. We exert this inuence indirectly by
choosing suitable behaviors for those agents that are under our direct control. In one case,
we attempt to inuence the behavior of rational agents, while in the other case, we try to
inuence learning agents.
Our first study is concerned with the enforcement of social laws. When a number of
agents designed by different designers work within a shared environment, it can be beneficial
to impose certain constraints on their behavior, so that, overall, the system will function
better. For example, Shoham and Tennenholtz (1995) show that by imposing certain \trac
laws," they can considerably simplify the task of motion planning for each robot, while still
enabling ecient motions. Indeed, as we see later, such conventions are at the heart of
many coordination techniques in multi-agent systems. Yet, without suitable mechanisms,
rational agents may have an incentive not to follow these conventions. We show how, in
certain cases, we can use the perspective of partially controlled multi-agent systems and the
structural assumption of rationality to enforce these conventions.
Our second study involves a two-agent system consisting of a teacher and a student.
The teacher is a knowledgeable agent, while the student is an agent that is learning how
to behave in its domain. Our goal is to utilize the teacher (which is under our control)
to improve the behavior of the student (which is not controlled by us). Hence, this is an
instance of partially controlled multi-agent systems in which the structural assumption is
that the uncontrolled agent employs a particular learning algorithm.
Both studies presented in this paper suggest techniques for achieving satisfactory system
behavior through the design of the controllable agents, and where relevant, these techniques
478

fiOn Partially Controlled Multi-Agent Systems

are experimentally assessed. Beyond the formulation and solution of two interesting problems in multi-agent system design, this paper suggests a more general perspective on certain
design problems. Although we feel that it is still premature to draw general conclusion about
the potential for a general theory of PCMAS design, certain concepts, those of punishment
and reward, suggest themselves as central to this area.
The paper is organized as follows: In Section 2, we describe the problem of enforcing
social behavior in multi-agent systems. In Section 3 we describe a standard game-theoretic
model for this problem and suggest the mechanism of threats and punishments as a general
tool for this class of problems. Issues that pertain to the design of threats and punishments
are discussed in Section 4. Section 5 introduces our second case study in PCMAS design:
embedded teaching of reinforcement learners. In this context, a teacher and a learner
are embedded in a shared environment with the teacher serving as the controller whose
aim is to direct the learner to a desired behavior. A formal model of this problem is
introduced in Section 6. In Section 7, we show how to derive optimal teaching policies (under
certain assumptions) by viewing teaching as a Markov decision process. The effectiveness
of different teaching policies is studied experimentally in Section 8. Finally, in Section 9,
we examine the relationship between the methods used in each of the two domains and the
possibility of a general methodology for designing partially controlled multi-agent systems.
We conclude in Section 10, with a summary and discussion of related work.

2. The Enforcement of Social Behavior
In this section we introduce the problem of the enforcement of social laws in a multi-agent
context. Our proposed solution falls naturally out of the PCMAS design perspective we
take. Here, we explain and motivate the particular problem of social law enforcement and
our approach to its solution. In Sections 3 and 4 we formalize and investigate this approach
in the framework of a general game-theoretic model.
We use the following scenario to illustrate the problem:
You have been hired to design a new working environment for artificial
agents. Part of your job involves designing a number of agents that will use
and maintain a warehouse. Other agents, designed by different designers, will
be using the warehouse to obtain equipment. To make sure that different agents
designed by different designers can operate eciently in this environment, you
choose to introduce a number of social laws, that is, constraints on the behavior
of agents, that will help the agents coordinate their activities in this domain.
These rules include a number of `trac laws', regulating motion in the domain,
as well as a law that specifies that every tool that is used by an agent must be
returned to its designated storage area. Your robots are programmed to follow
these laws, and you expect the others to do so. Your laws are quite successful, and allow ecient activity in the warehouse, until a new designer arrives.
Pressed by his corporate bosses to deliver better performance, he decides to
exploit all your rules. He designs his agent to locally maximize its performance,
regardless of the social laws. What can you do?
479

fiBrafman & Tennenholtz

In multi-participant environments, as the one above, each agent might have its own
dynamic goals, and we are interested in finding ways in which agents can coexist while
achieving their goals. Several approaches for coordination of agent activity are discussed in
the distributed systems and the DAI literature. Some examples are: protocols for reaching
consensus (Dwork & Moses, 1990), rational deals and negotiations (Zlotkin & Rosenschein,
1993; Kraus & Wilkenfeld, 1991; Rosenschein & Genesereth, 1985), organizational structures (Durfee, Lesser, & Corkill, 1987; Fox, 1981; Malone, 1987), and social laws (Moses
& Tennenholtz, 1995; Shoham & Tennenholtz, 1995; Minsky, 1991; Briggs & Cook, 1995).
In some of these methods, the behavior of an agent is predetermined or prescribed from a
certain stage, for example, the content of the deal after it is reached, the outcome of the
negotiation process after it is completed, or the social law after it is instituted. This work
relies on the assumption that the agents follow these prescribed behaviors, e.g., they obey
the law or stick to the agreement. This assumption is central to the success of any of these
methods. However, it makes agents that follow the rules vulnerable to any rational agent
that performs local maximization of payoff, exploiting the knowledge that others follow the
rules. In our example, the new designer may program his robot not to return the tools,
saving the time required to do so, thus causing other agents to fail in their tasks.
Despite its somewhat futuristic avor (although instances of such shared environments
are beginning to appear in cyberspace), this scenario is useful in illustrating the vulnerability
of some of the most popular coordination mechanism appearing in the multi-agent literature
within AI (e.g., see Bond & Gasser, 1988) when we assume that the agents involved are
fully rational. As an aside, note that, in this case, we actually need not attribute much
intelligence to the agents themselves, and it is sucient to assume that their designers
design them in a way that maximizes their own utility, disregarding the utility of the other
agents.
In order to handle this problem we need to modify existing design paradigms. By
adopting the perspective of partially controlled multi-agent systems, we obtain one possible
handle on this problem, which requires making the following basic assumption: that the
original designer, as in the above scenario, controls a number of reliable agents.1 Our
basic idea is that some of these reliable agents will be designed to punish agents that
deviate from the desirable social standard. The punishment mechanism will be `hardwired' (unchangeable) and will be common-knowledge. The agents that are not controlled
by the original designer will be aware of this punishment possibility. If the punishment
mechanism is well designed, deviations from the social standard become irrational. As a
result, no deviation will actually occur and no punishment will actually be executed! Hence,
by making our agents a bit more sophisticated, we can prevent the temptation of breaking
social laws.
In the suggested solution we adopt the perspective of partially controlled multi-agent systems. Some of the agents are controllable, while others are uncontrollable but are assumed
to adopt the basic model of expected utility maximization. The punishment mechanism
is (part of) the control strategy that is used to inuence the behavior of the uncontrolled
agents.
1. For ease of exposition, we assume that reliable agents follow the designer's instructions; we assume that
no non-malicious failures, such as crash failures, are possible.

480

fiOn Partially Controlled Multi-Agent Systems

3. Dynamic Game Theoretic Model

In this section we introduce a basic game-theoretic model, which we use to study the
problem of the enforcement of social behavior and its solution. Later on, in Sections 5{8,
this model will be used to study embedded teaching. We wish to emphasize that the model
we use is the most common model for representing emergent behavior in a population2
(e.g., Huberman & Hogg, 1988; Kandori, Mailath, & Rob, 1991; Altenberg & Feldman,
1987; Gilboa & Matsui, 1991; Weidlich & Haag, 1983; Kinderman & Snell, 1980).

Definition 1 A k-person game g is defined by a k-dimensional matrix M of size n1   

nk , where nm is the number of possible actions (or strategies) of the m'th agent. The entries
of M are vectors of length k of real numbers, called payoff vectors. A joint strategy in M
is a tuple (i1; i2; : : :; ik ), where for each 1  j  k, it is the case that 1  ij  nj .
Intuitively, each dimension of the matrix represents the possible actions of one of the k
players of the game. Following the convention used in game theory, we often use the term
strategy in place of action . Since the dimensions of the matrix are n1      nk , the i'th
agent has ni possible strategies to choose from. The j 'th component of the vector residing
in the (i1; i2; : : :; ik ) cell of M (i.e., Mi1 ;i2 ;:::;ik ) represents the feedback player j receives
when the players' joint strategy is (i1; i2; : : :; ik ), that is, if agent m's strategy is im for
all 1  m  k. Here, we use the term joint strategy to refer to the combined choice of
strategies of all the agents.

Definition 2 A n-k-g iterative game consists of a set of n agents and a given k person
game g . The game g is played repetitively an unbounded number of times. At each iteration,
a random k-tuple of agents play an instance of the game, where the members of this k-tuple
are selected with uniform distribution from the set of agents.

Every iteration of an n-k-g game represents some local interaction of k agents. Those agents
that play in a particular iteration of the game must choose the strategy they will use in this
interaction; an agent can use different strategies in different interactions. The outcome of
each iteration is represented by the payoff vector corresponding to the agents' joint strategy.
Intuitively, this payoff tells us how good the outcome of this joint behavior is from the point
of view of each agent. Many situations can be represented as an n-k-g game, for example,
the \trac" aspect of a multi-agent system can be represented by an n-k-g game, where
each time a number of agents meet at an intersection. Each such encounter is an instance
of a game in which agents can choose from a number of strategies, e.g., move ahead, yield.
The payoff function gives the utility to each set of strategies. For example, if each time only
two agents meet and both agents choose to move ahead, a collision occurs and their payoffs
are very low.
Definition 3 A joint strategy of a game g is called ecient if the sum of the players' payoffs
is maximal.
2. In this paper we use the term emergent behavior in its classical mathematical-economics interpretation:
an evolution of a behavior based on repetitive local interactions of (usually pairs of) agents, where each
agent may change its strategy for the following interactions based on the feedback it received in previous
interactions.

481

fiBrafman & Tennenholtz

Hence, eciency is one global criterion for judging the \goodness" of outcomes from the
system's perspective, unlike single payoffs which describe a single agent's perspective.3

Definition 4 Let s be a fixed joint strategy for a given game g, with payoff pi(s) for player
i; in an instance of g in which a joint strategy s0 was played, if pi (s)  pi (s0 ) we say that
i's punishment w.r.t. s is pi (s) , pi (s0 ), and otherwise we say that its benefit w.r.t. s is
pi(s0) , pi (s).
Hence, punishment and benefit w.r.t. some joint strategy s measure the gain (benefit) or
loss (punishment) of an agent if we can somehow change the joint behavior of the agents
from s to s0 .
In our current discussion punishment and benefit will always be with respect to a chosen
ecient solution.
As designers of the multi-agent system, we would prefer it to be as ecient as possible.
In some cases this entails behavior that is in some sense unstable, that is, individual agents
may locally prefer to behave differently. Thus, agents may need to be constrained to behave
in a way that is locally sub-optimal. We refer to such constraints that exclude some of the
possible behaviors as social laws .
Due to the symmetry of the system and under the assumption that the agents are
rational and their utility is additive (i.e., that the utility of two outcomes is the sum of their
utilities), it is clear that no agent's expected payoff can be higher than the one obtained
using the strategies giving the ecient solution. Thus, it is clear that in this case an ecient
solution is fair, in the sense that all agents can get at least what they could if no such law
existed, and no other solution can provide a better expected payoff.
However, the good intentions of the designer of creating an environment beneficial to
the participating agents, may backfire. A social law provides information on the behavior
of agents conforming to it, information that other agents (or their respective designers) can
use to increase their expected payoff.
Example 1 Assume that we are playing an n-2-g game where g is the prisoner's dilemma,
represented in strategic form by the following matrix.
agent 2
agent 1
1
2
1
(2,2) (-10,10)
2
(10,-10) (-5,-5)
The ecient solution of this game is obtained when both players play strategy 1. Assume
that this solution is chosen by the original designer, and is followed by all agents under its
control.
A designer of a new agent that will function in an environment in which the social law is
obeyed may be tempted to program her agent not to conform to the chosen law. Instead, he
will program the agent to play the strategy the maximizes its expected outcome, strategy
3. Addition of payoffs or utilities across agents is a dangerous practice. However, in our particular model, it
can be shown that a system in which joint-strategies are always ecient maximizes each agent's expected
cumulative rewards.

482

fiOn Partially Controlled Multi-Agent Systems

#2. This new agent will obtain a payoff of 10 when playing against one of the `good' agents.
Thus, even though the social law was accepted in order to guarantee a payoff of 2 to any
agent, `good' agents will obtain a payoff of -10 when playing against such non-conforming
agents. Note that the new designer exploits information on the strategies of `good' players,
as dictated by the social law. The agents controlled by the new designer are uncontcolable
agents; their behavior can not be dictated by the original designer.
Agents not conforming to the social law will be referred to as malicious agents . In order
to prevent the temptation to exploit the social law, we introduce a number of punishing
agents , designed by the initial designer, that will play `irrationally' if they detect behavior
not conforming to the social law, attempting to minimize the payoff of the malicious agents.
The knowledge that future participants have of the punishment policy would deter deviations and eliminate the need for carrying it out. Hence, the punishing behavior is used as a
threat aimed at deterring other agents from violating the social law. This threat is (part of)
the control strategy adopted the controllble agents in order to inuence the behavior of the
unconrollable agents. Notice that this control strategy relies on the structural assumption
that the unconrollable agents are expected utility maximizers.
We define the minimized malicious payoff as the minimal expected payoff of the malicious players that can be guaranteed by the punishing agents. A punishment exists , if the
minimized malicious payoff is lower than the expected payoff obtained by playing according
to the social law. A strategy that guarantees the malicious agents an expected payoff lower
than the one obtained by playing according to the social law is called a punishing strategy .
Throughout this section and the following section we make the natural assumption that the
expected payoff of malicious agents when playing against each other is no greater than the
one obtained in the ecient solution4 .
Example 1 (continued) In Example 1, the punishment would simply be to play strategy
2 from now on. This may cause the payoff of a punishing agent to decrease, but would
guarantee that no malicious agent obtains a payoff better than -5 playing against a punishing
agent. If many non-malicious agents are punishing, the malicious agents' expected payoff
would decrease and become smaller then the payoff guaranteed by the social law. Strategy
2 would be the punishing strategy.

4. The Design of Punishments

In the previous section we described a general model of multi-agent interaction and showed
how the perspecive of partially controlled multi-agent systems leads to one possible solution
to the problem of enforcing social behavior in this setting, via the idea of threats and
punishments. We now proceed to examine the issue of punishment design.
We assume that there are p agents which the designer controls that either have an ability
to observe instances of the game that occur, or that can be informed as to the outcome of
games. There are c additional agents that conform with the law (that is, play the strategies
entailed by the chosen ecient solution), and m malicious agents, that are not bound by
the law.
4. Other assumptions may be treated similarly.

483

fiBrafman & Tennenholtz

We would like to answer questions such as: Does a game offer the ability to punish?
What is the minimized malicious payoff? What is the optimal ratio between p; c; and m?
Is there a difference between different social laws?
Example 1 (continued) Consider Example 1 again. We have observed above that we can
cause an expected maximal loss for the malicious agents of 7 (= 2 , (,5)). This occurs
when the punishing agents play strategy 2. The gain that a malicious agent makes when
playing against an agent following the social law is 8 (= 10 , 2). In order for a punishing
strategy to be effective, it must be the case that the expected payoff of a malicious agent
will be no greater than the expected payoff obtained when following the social law. In order
to achieve this, we must ensure that the ratio of punishing/conforming agents is such that a
malicious agent will have sucient encounters with punishing agents. In our case, assuming
that when 2 deviators meet their expected benefit is 0 and recalling that an agent is equally
likely to meet any other agent, we need pc > 87 to make the incentive to deviate negative.
Implementing the punishment approach requires more complex behavior. Our agents
must be able to detect deviations as well as to switch to a new punishing strategy. This
whole behavior can be viewed as a new, more complex, social law. This calls for more
complex agents to carry it out, and makes the programming task harder.
Clearly, we would like to minimize the number of such complex agents, keeping the
benefit of malicious behavior negative. Here, the major question is the ratio between the
benefit of deviation and the prospective punishment.
As can be seen from the example, the larger the punishment, the smaller the number
of the more sophisticated punishing agents that is needed. Therefore, we would like to find
out which strategies minimize the malicious agent's payoff. In order to do this we require a
few additional definitions.

Definition 5 A two person game g is a zero-sum game if for every joint strategy of the
players, the sum of the players' payoffs is 0.

Hence, in a zero-sum game, there are no win/win situations, the larger the payoff of one
agent, the smaller the payoff of the other agent. By convention, the payoff matrix of a two
person zero-sum game will mention only the payoffs of player 1.

Definition 6 Let g be a two person game. Let Pig (s,t) be the payoff of player i in g (where
i 2 f1; 2g) when strategies s and t are played by player 1 and 2 respectively. The projected

game, gp , is the following two person zero-sum game: The strategies of both players are as
in g , and the payoff matrix is P gp (s; t) = ,P2g (s,t). Define the transposed game of g , g T ,
to be the game g where the roles of the players change.

In the projected game, the first agent's payoff equals the negated value of the second agent's
payoff in the original game. Thus, this game reects the desire to lower the payoffs of the
second player in the original game.
We give a general result for a two-person game, g (with any number of strategies). We
make use of the following standard game-theoretic definition:
484

fiOn Partially Controlled Multi-Agent Systems

Definition 7 Given a game g, a joint strategy  for the players is a Nash equilibrium of

g if whenever a player takes an action that is different than its action at  , its payoff given
that the other players play as in  is no higher than its payoff given that everybody plays  .
That is, a strategy  is a Nash equilibrium of a game if no agent can obtain a better payoff
by unilaterally changing its behavior when all the other agents play according to  .
Nash-equilibrium is the central notion in the theory of non-cooperative games (Luce &
Raiffa, 1957; Owen, 1982; Fudenberg & Tirole, 1991). As a result, this notion is well studied
and understood, and reducing new concepts to this basic concept may be quite useful from
a design perspective. In particular, Nash-equilibrium always exists for finite games, and the
payoffs prescribed by any Nash-equilibria of a given zero-sum game are uniquely defined.
We can show:

Theorem 1 Given an n-2-g iterative game, the minimized malicious payoff is achieved by

playing the strategy of player 1 prescribed by the Nash equilibrium of the projected game gp,
when playing player 1 (in g ), and the strategy of player 1 prescribed by the Nash equilibrium
of the projected game (g T )p, when playing player 2 (in g ).5

Proof: Assume that the punishing agent plays the role of player 1. If player 1 adopts the

strategy prescribed by a Nash-equilibrium  then player 2 can not get a better payoff than
the one guaranteed by  since each deviation by player 2 will not improve its situation (by
the definition of Nash-equilibrium). On the other hand, player 1 can not cause more harm
than the harm obtained by playing its strategy in  . To see this, assume that player 1 uses
an arbitrary strategy s, and that player 2 adopts the strategy prescribed by  . The outcome
for player 1 will be not higher than the one guaranteed by playing the Nash-equilibrium
(by the definition of Nash-equilibrium). In addition, due to the fact that we have here a
zero-sum game this implies that the outcome for player 2 will be no lower than the one
guaranteed if player 1 would play according to  . The case where the punishing agent is
player 2 is treated similarly.

Example 1 (continued) Continuing our prisoner's dilemma example, gp would be
agent 2
agent 1 1 2
1
-2 -10
2
10 5
with the Nash equilibrium attained by playing the strategies yielding 5. In this example,
(g T )p = gp. Therefore, the punishing strategies will be strategy # 2 for each case.

Corollary 1 Let n-2-g be an iterative game, with p punishing agents. Let v and v' be the

payoffs of the Nash equilibria of gp and gpT respectively (which, in this case, are uniquely
defined). Let b,b' be the maximal payoffs player 1 can obtain in g and g T respectively,
5. Notice that, in both cases, the strategies prescribed for the original game are determined by the strategies
of player 1 in the Nash-Equilibria of the projected games.

485

fiBrafman & Tennenholtz

assuming player 2 is obeying the social law. Let e and e' be the payoffs of player 1 and 2,
respectively, in g , when the players play according to the ecient solution prescribed by the
social law. Finally, assume that expected benefit of two malicious agents when they meet
is 0. A necessary and sucient condition for the existence of a punishing strategy is that
(n,1,p)  (b + b0) , p  (v + v 0 ) < (e + e0 ).
n,1
n,1

Proof: The expected payoff obtained by a malicious agent when encountering a law-

abiding agent is b+2b , and its expected payoff when encountering a punishing agent is ,(v2+v ) .
In order to test the conditions for the existence of a punishing strategy we would need to
consider the best case scenario from the point of view of a malicious agent; in such a case all
non-punishing agents are law-abiding agents. In order to obtain the expected utility for a
malicious agent we have to make an average of the above quantities taking into account the
proportion of law-abiding and punishing agents in the population. This gives us that the
expected utility for a malicious agent is (2(n,n1,,1)p)  (b + b0) , 2(np,1)  (v + v 0). By definition,
a punishing strategy exists if and only if this expected utility is lower than the expected
utility guaranteed by the social law. Since the expected utility which can be guaranteed by
a social law is e+2e , we get the desired result.
The value of the punishment, (v+2v ) in the above, is independent of the ecient solution
chosen, and e + e0 is identical for all ecient solutions, by definition. However, b + b0 depends
on our choice of an ecient solution. When a number of such solutions exist, minimizing
b + b0 is an important consideration in the design of the social law, as it affects the incentive
to `cheat'.
0

0

0

0

Example 2 Let's look at a slightly different version of the prisoner's dilemma. The game
matrix is

agent 2
agent 1
1
2
1
(0,0) (-10,10)
2
(10,-10) (-5,-5)
Here there are 3 ecient solutions, given by the joint strategies (1,1), (1,2), (2,1). In
the case of (1,1) we have b+b'=20 (gained by playing strategy 2 instead of 1). In the case
of (2,1) and (1,2) b+b'=5.
Clearly, there is more incentive to deviate from a social law prescribing strategies (1,1)
than from a social law prescribing (2,1) or (1,2).
To summarize, the preceding discussion suggests designing a number of punishing agents,
whose behavior in punishment mode is prescribed by Theorem 1 in the case of n-2-g games.
By ensuring a sucient number of such agents we take away any incentive to deviate from
the social laws. Hence, given that the malicious agents are rational, they will follow the social
norm, and consequently, there will be no need to utilize the punishment mechanism. We
observed that different social laws leading to solutions that are equally ecient have different
properties when it comes to punishment design. Consequently, under the assumption that
we would like to minimize the number of punishing agents while guaranteeing an ecient
486

fiOn Partially Controlled Multi-Agent Systems

solution to the participants, we should choose an ecient solution that minimizes the value
of b + b0.

5. Embedded Teaching
In this section we move on to our second study of a PCMAS design problem; only now,
the uncontrollable agent is a reinforcement learner. This choice is not arbitrary; rational
agents and reinforcement learners are the two major types of agents studied in mathematical
economics, decision theory, and game theory. They are also the types of agents discussed
in work in DAI which is concerned with self-motivated agents (e.g., Zlotkin & Rosenschein,
1993; Kraus & Wilkenfeld, 1991; Yanco & Stein, 1993; Sen, Sekaran, & Hale, 1994).
An agent's ability to function in an environment is greatly affected by its knowledge of
the environment. In some special cases, we can design agents with sucient knowledge for
performing a task (Gold, 1978), but, in general, agents must acquire information on-line
in order to optimize their performance, i.e., they must learn. One possible approach to
improving the performance of learning algorithms is employing a teacher. For example,
Lin (1992) uses teaching by example to improve the performance of agents, supplying them
with examples that show how the task can be achieved. Tan's work (1993) can also be
viewed as a form of teaching in which agents share experiences. In both methods some nontrivial form of communication or perception is required. We strive to model a broad notion
of teaching that encompasses any behavior that can improve a learning agent's performance.
That is, we wish to conduct a general study of partially controlled multi-agent systems in
which the uncontrollable agent runs a learning algorithm. At the same time, we want our
model to clearly delineate the limits of the teacher's (i.e., the controlling agent's) ability to
inuence the student.
Here, we propose a teaching approach that maintains a situated \spirit" much like that
of reinforcement learning (Sutton, 1988; Watkins, 1989; Kaelbling, 1990), which we call
embedded teaching . An embedded teacher is simply a \knowledgeable" controlled agent
situated with the student in a shared environment. Her6 goal is to lead the student to
adopt some specific behavior. However, the teacher's ability to teach is restricted by the
nature of the environment they share: not only is her repertoire of actions limited, but
she may also lack full control over the outcome of these actions. As an example, consider
two mobile robots without any means of direct communication. Robot 1 is familiar with
the surroundings, while Robot 2 is not. In this situation, Robot 1 can help Robot 2 reach
its goal through certain actions, such as blocking Robot 2 when it is headed in the wrong
direction. However, Robot 1 may have only limited control over the outcome of such an
interaction because of uncertainty about the behavior of Robot 2 and its control uncertainty.
Nevertheless, Robot 2 has a specific structure, it is a learner obeying some learning scheme,
and we can attempt to control it indirectly through our choice of actions for Robot 1.7
6. To differentiate between teacher and student, we use female pronouns for the former and male pronouns
for the latter.
7. In general, the fact that an agent is controllable does not imply that we can perfectly control the outcome
of its actions, only their choice. Hence, a robot may be controllable in our sense, running a program
supplied by us, yet its move-forward command may not always have the desired outcome.

487

fiBrafman & Tennenholtz

In what follows, our goal is to understand how an embedded teacher can help a student
adopt a particular behavior. We address a number of theoretical questions relating to
this problem, and then we experimentally explore techniques for teaching two types of
reinforcement learners.

6. A Basic Teaching Setting
We consider a teacher and a student that repeatedly engage in some joint activity. While
the student has no prior knowledge pertaining to this activity, the teacher understands its
dynamics. In our model, the teacher's goal is to lead the student to adopt a particular
behavior in such interactions. For example, teacher and student meet occasionally at the
road and the teacher wants to teach the student to drive on the right side. Or perhaps, the
teacher and the student share some resource, such as CPU time, and the goal is to teach
him judicious use of this resource. We model such encounters as 2-2-g iterative games.
To capture the idea that the teacher is more knowledgeable than the student, we assume
that she knows the structure of the game, i.e., she knows the payoff function, and that she
recognizes the actions taken at each play. On the other hand, the student does not know the
payoff function, although he can perceive the payoff he receives. In this paper, we make the
simplifying assumptions that both teacher and student have only two actions from which
to choose and that the outcome depends only on their choice of actions. Furthermore,
excluding our study in Section 8.4, we ignore the cost of teaching, and hence, we omit the
teacher payoff from our description.8 This provides a basic setting in which to take this
first step towards understanding the teaching problem.9
The teaching model can be concisely modeled by a 2  2 matrix. The teacher's actions
are designated by I and II , while the student's actions are designated by the numbers 1
and 2. Each entry corresponds to a joint action and represents the student's payoff when
this joint action is played. We will suppose that we have matrix A of Figure 1, and that we
wish to teach the student to use action 1. At this stage, all we assume about the student is
that if he always receives a better payoff following action 1 he will learn to play it.
We can see that in some situations teaching is trivial. Assume that the first row dominates the second row, i.e., a > c and b > d. In that case, the student will naturally prefer to
take action 1, and teaching is not very challenging, although it might be useful in speeding
the learning process. For example, if a , c > b , d, as in matrix B in Figure 1, the teacher
can make the advantage of action 1 more noticeable to the student by always playing action
I.
Now suppose that only one of a > c or b > d holds. In this case, teaching is still easy.
We use a basic teaching strategy, which we call preemption . In preemption the teacher
chooses an action that makes action 1 look better than action 2. For example, when the
situation is described by matrix C in Figure 1, the teacher will always choose action I .
8. A case could be made for the inherent value of teaching, but this may not be the appropriate forum for
airing these views.
9. In fact, our idea has been to consider the most basic embedded teaching setting which is already challenging. As we later see, this basic setting is closely related to a fundamental issue in non-cooperative
games.

488

fiOn Partially Controlled Multi-Agent Systems

I II

I II

1 a b

1 6 5

2 c d
(A)

I II

1 5 1
2 2 6
(C)

2 1 2
(B)

I II

1 3 -2
2 5 6
(D)

I

II

1 5 -10
2 10 -5
(E)

Figure 1: Game matrices A, B, C, D, and E. The teacher's possible actions are I and II ,
and the student's possible actions are 1 and 2.
Next, assume that both c and d are greater than both a or b, as in matrix D in Figure 1.
Regardless of which action the teacher chooses, the student receives a higher payoff by
playing action 2 (since minf5; 6g > maxf3; ,2g). Therefore, no matter what the teacher
does, the student will learn to prefer action 2. Teaching is hopeless in this situation.
All other types of interactions are isomorphic to the case where c > a > d > b, as in
matrix E in Figure 1. This is still a challenging situation for the teacher because action 2
dominates action 1 (because 10 > 5 and ,5 > ,10). Therefore, preemption cannot work.
If a teaching strategy exists, it will be more complex than always choosing the same action.
Since this seems to the most challenging teaching situation, we devote our attention to
teaching a reinforcement learner to choose action 1 in this class of games.
It turns out that the above situation is quite important in game-theory and multi-agent
interaction. It is a projection of a very famous game, the prisoner's dilemma, discussed
in the previous sections. In general, we can represent the prisoner's dilemma using the
following game matrix:
teacher
student Coop Defect
student Coop
Coop (a,a) (b,c) or more commonly Coop (a,a)
Defect (c,b) (d,d)
Defect (c,-c)

teacher
Defect
(-c,c)
(d,d)

where c > a > d > b. The actions in the prisoner's dilemma are called Cooperate (Coop)
and Defect; we identify Coop with actions 1 and I , and Defect with actions 2 and II . The
prisoner's dilemma captures the essence of many important social and economic situations;
in particular, it encapsulates the notion of cooperation. It has thus motivated enormous discussion among game-theorists and mathematical economists (for an overview, see Eatwell,
Milgate, & Newman, 1989). In the prisoner's dilemma, whatever the choice of one player,
the second player can maximize its payoff by playing Defect. It thus seems \rational" for
each player to defect. However, when both players defect, their payoffs are much worse than
if they both cooperate.
489

fiBrafman & Tennenholtz

As an example, suppose two agents will be given $10 each for moving some object. Each
agent can perform the task alone, but it will take an amount of time and energy which they
value at $20. However, together, the effort each will make is valued at $5. We get the
following instance of the prisoner's dilemma:
Agent 1
Agent 2 Move
Rest
Move
(5,5) (-10,10)
Rest (10,-10) (0,0)
In the experimental part of our study, the teacher's task will be to teach the student
to cooperate in the prisoner's dilemma. We measure the success of a teaching strategy by
looking at the cooperation rate it induces in students over some period of time, that is, the
percentage of the student's actions which are Coop. The experimental results presented in
this paper involving the prisoner's dilemma are with respect to the following matrix:
Teacher
Student Coop Defect
Coop (10,10) (-13,13)
Defect (13,-13) (-6,-6)
We have observed qualitatively similar results in other instantiations of the prisoner's
dilemma, although the precise cooperation rate varies.

7. Optimal Teaching Policies

In the previous section we concentrated on modeling the teaching context as an instance
of a partially controlled multi-agent system, and determining which particular problems
are interesting. In this section we start exploring the question of how a teacher should
teach. First, we define what an optimal policy is. Then, we will define Markov decision
processes (MDP) (Bellman, 1962), and show that under certain assumptions teaching can be
viewed as an MDP. This will allow us to tap into the vast knowledge that has accumulated
on solving these problems. In particular, we can use well known methods, such as value
iteration (Bellman, 1962), to find the optimal teaching policy.
We start by defining an optimal teaching policy. A teaching policy is a function that
returns an action at each iteration; possibly, it may depend on a complete history of the
past joint actions. There is no \right" definition for an optimal policy, as the teacher's
motivation may vary. However, in this paper, the teacher's objective is to maximize the
number of iterations in which the student's action are \good", such as Coop in the prisoner's
dilemma. The teacher does not know the precise number of iterations she will be playing,
so she slightly prefers earlier success to later success.
This is formalized as follows: Let u(a) be the value the teacher places on a student's
action, a, let  be the teacher's policy, and assume that it induces a probability distribution
Pr;k over the set of possible student actions at time k. We define the value of the strategy
 as
1
X
val( ) =  kEk (u)
k=0

490

fiOn Partially Controlled Multi-Agent Systems

where Ek (u) is the expected value of u:

Ek (u) =

X Pr

a2As

;k (a)  u(a)

Here, As is the student's set of actions. The teacher's goal is to find a strategy  that
maximizes val(), the discounted expected value of the student's actions. For example, in
the case of the prisoner's dilemma, we could have
As = fCoop,Defectg and u(Coop) = 1 and u(Defect) = 0.
Next, we define MDPs. In an MDP, a decision maker is continually moving between
different states. At each point in time she observes her current state, receives some payoff
(which depends on this state), and chooses an action. Her action and her current state
determine (perhaps stochastically) her next state. The goal is to maximize some function
of the payoffs. Formally, an MDP is a four-tuple hS; A; P; ri, where S is the state-space, A
is the decision-maker's set of possible actions, P : S  S  A ! [0; 1] is the probability of
a transition between states given the decision-maker's action, and r : S ! < is the reward
function. Notice that given an initial state s 2 S , and a policy of the decision maker  , P
induces a probability distribution Ps;;k over S , where Ps;;k (s0 ) is the probability that the
kth state obtained will be s0 if the current state is s.
The 0-optimal policy in an MDP is the policy that maximizes at each state s the
discounted sum of the expected values of the payoffs received at all future states, starting
at s, i.e.,
1
X
X
0k( Ps;;k (s0 )  r(s0))
k=0

s 2S
0

Although it may not be immediately obvious, a single policy maximizing discounted sums
for any starting state exists, and there are well-known ways of finding this policy. In the
experiments below we use a method based on value-iteration (Bellman, 1962).
Now suppose that the student can be in a set  of possible states, that his set of actions
is As , and that the teacher's set of actions is At . Moreover, suppose that the following
properties are satisfied:
(1) The student's new state is a function of his old state and the current joint-action,
denoted by  :   As  At ! ;
(2) The student's action is a stochastic function of his current state, where the probability
of choosing a at state s is (s; a);
(3) the teacher knows the student's state. (The most natural way for this to happen is that
the teacher knows the student's initial state, the function  , and the outcome of each game,
and she uses them to simulate the agent.)
Notice that under these assumptions a teaching policy should be a function of : We
know that the student's next action is a function of his next state. We know that the
student's next state is a function of his current state, his current action, and the teacher's
current action. Hence, his next action is a function of his current state and action, as
well as the teacher's current action. However, we know that the student's current action
is a function of his current state. Hence, the student's next action is a function of his
current state and the teacher's current action. This implies that the only knowledge the
teacher needs to optimally choose her current action is the student's current state, and any
491

fiBrafman & Tennenholtz

additional information will be redundant and cannot improve her success. More generally,
when we repeat this line of reasoning indefinitely into the future, we see that the teacher's
policy should be a function of the student's state: a function from  to At . It is now
possible to see that we have the makings of the following MDP.
Given this observation and our three assumptions, we see that, indeed, the teacher's
policy induces a probability distribution over the set of possible student actions at time k.
This implies that our definition of val makes sense here.
Define the teacher's MDP to be TMDP= h; At; P; U i, where

X

P (s; s0; at) def
=

as 2As

(s; as)  s ; (s;as;at)
0

(i;j is defined as 1 when i = j , and 0 otherwise). That is, the probability of a transition
from s to s0 under at is the sum of probabilities of the student's actions that will induce
this transition. The reward function is the expected value of u:

X

U (s) def
=

as2As

(s; as)  u(as)

Theorem 2 The optimal teaching policy is given by the 0 optimal policy in TMDP.
Proof: By definition, the 0 optimal policy in TMDP is the policy  that for each s 2 
maximizes
that is,

1
X
X
 k( P
0

k=0

s 2

s;;k (s0 )  U (s0 ))

0

1
X
X
 k( P

k=0

However, this is equal to

()

0

0
s;;k (s )  (

s 2
0

X
as2As

(s0 ; as)  u(as)))

1
X
X X (s0; a )  P
k

k=0

0

s

as2As s 2
0

0
s;;k (s )  u(as )

We know that Ps;;k (s0 ) is the probability that s0 will be the state of the student in time
k, given that the teacher uses  and that her current state is s. Hence,

X (s0; a )  P

s 2
0

s

0
s;;k (s )

is the probability that as will be the action taken by the student at time k given the initial
(current) state is s. Upon examination, we see now that (*) is identical to val( ).
The optimal policy can be used for teaching, when the teacher possess sucient information to determine the current state of the student. But even when that is not the case, it
allows us to calculate an upper bound on the success val( ) of any teaching policy  . This
number is a property of the learning algorithm, and measures the degree of inuence any
agent can have over the given student.
492

fiOn Partially Controlled Multi-Agent Systems

8. An Experimental Study

In this section we describe an experimental study of embedded teaching. First, we define the
learning schemes considered, and then, we describe a set of results obtained using computer
simulations.

8.1 The Learning Schemes

We experiment with two types of students: One uses a reinforcement learning algorithm
which can be viewed as Q-learning with one state, and the other uses Q-learning. In choosing
parameters for these students we tried to emulate choices made in the reinforcement learning
literature.
The first student, which we call a Blind Q-learner (BQL), can perceive rewards, but
cannot see how the teacher has acted or remember his own past actions. He only keeps
one value for each action, for example, q (Coop) and q (Defect) in the case of the prisoner's
dilemma. His update rule is the following: if he performed action a and received a reward
of R then
qnew (a) = (1 , ff)  qold(a) + ff  R
The parameter ff, the learning-rate, is fixed (unless stated otherwise) to 0:1 in our experiments. We wish to emphasize that although BQL is a bit less sophisticated than \real"
reinforcement learners discussed in the AI literature (which is defined below), it is a popular and powerful type of learning rule, which is much discussed and used in the literature
(Narendra & Thathachar, 1989).
The second student is a Q-learner (QL). He can observe the teacher's actions and has
a number of possible states. The QL maintains a Q-value for each state-action pair. His
states encode his recent experiences, i.e., the past joint actions. The update rule is:
qnew (s; a) = (1 , ff)  qold (s; a) + ff  (R + V (s0))
Here R is the reward received upon performing a at state s; s0 is the state of the student
following the performance of a at s;  is called the discount factor, and will be 0:9, unless
otherwise noted; and V (s0) is the current estimate of the value of the best policy on s0 ,
defined as maxa2As q (s0 ; a). All Q-values are initially set to zero.
The student's update rule tells us how his Q-values change as a result of new experiences. We must also specify how these Q-values determine his behavior. Both QL and
BQL students choose their actions based on the Boltzmann distribution. This distribution
associates a probability Ps (a) with the performance of an action a at a state s (P (a) for
the BQL).
exp(q (a)=T )
q(s; a)=T )
def
P
(
QL
)
P
(
a
)
=
(BQL)
Ps(a) def
= P exp(exp(
0
q (s; a )=T )
a 2A
a 2A exp(q (a0)=T )
Here T is called the temperature . Usually, one starts with a high value for T , which
makes the action choice more random, inducing more exploration on the part of the student.
T is slowly reduced, making the Q-values play a greater role in the student's choice of action.
We use the following schedule: T (0) = 75 and T (n +1) = T (n)  0:9+0:05. This schedule has
the characteristic properties of fast initial decay and slow later decay. We also experiment
with fixed temperature.
0

0

493

fiBrafman & Tennenholtz

Approximately Optimal Policy
1

10000 iterations
5000 iterations
1000 iterations
100 iterations

0.8

Fraction of Coops

Fraction of Coops

1

Two Q-learners

0.6
0.4
0.2

10000 iterations
5000 iterations
1000 iterations
100 iterations

0.8
0.6
0.4
0.2

0

0
0

1

2

3

4 5 6 7
Temperature

8

9 10

0

1

2

3

4 5 6 7
Temperature

8

9 10

Figure 2: Fraction of Coops as a function of temperature for the approximately optimal
policy (left) and for \teaching" using an identical Q-learner (right). Each curve
corresponds to Coop rate over some fixed number of iterations. In the approx.
optimal policy the curves for 1000, 5000 and 10000 iterations are nearly identical.

8.2 Blind Q-Learners

Motivated by our discussion in Section 6 we will concentrate in this section and in the
following section on teaching in the context of the prisoner's dilemma. In Section 8.4 we
discuss another type of teaching setting. This section describes our experimental results
with BQL. We examined a policy that approximates the optimal policy, and two teaching
methods that do not rely on a student model.
8.2.1 Optimal Policy

First we show that BQLs fit the student model of Section 7. For their state space, we use
the set of all possible assignment for their Q-values. This is a continuous subspace of <2,
and we discretize it (in order to be able to compute a policy), obtaining a state space with
approximately 40,000 states. Next, notice that transitions are a stochastic function of the
current state (current Q-values) and the teacher's action. To see this notice that Q-value
updates are a function of the current Q-value and the payoff; the payoff is a function of the
teacher's and student's actions; and the student's actions are a stochastic function of the
current Q-value. In the left side of Figure 2 we see the success of teaching using the policy
generated by using dynamic programming to solve this optimization problem. Each curve
represents the fraction of Coops as a function of the temperature for some fixed number of
iterations. The values are means over 100 experiments.
8.2.2 Two Q-Learners

We also ran experiments with two identical BQLs. This can be viewed as \teaching" using
another Q-learner. The results are shown in the right side of Figure 2. At all temperatures
the optimal strategy performs better than Q-learning as a \teaching" strategy. The fact that
at temperatures of 1.0 or less the success rate approaches 1 will be beneficial when we later
add temperature decay. However, we also see that there is an inherent limit to our ability
494

fiOn Partially Controlled Multi-Agent Systems

Tit-For-Tat
1

10000 iterations
5000 iterations
1000 iterations
100 iterations

0.8

Fraction of Coops

Fraction of Coops

1

2-Tit-For-Tat

0.6
0.4
0.2

10000 iterations
5000 iterations
1000 iterations
100 iterations

0.8
0.6
0.4
0.2

0

0
0

1

2

3

4 5 6 7
Temperature

8

9 10

0

1

2

3

4 5 6 7
Temperature

8

9 10

Figure 3: Fraction of Coops as a function of temperature for the teaching strategy based
on TFT (left) and 2TFT (right).
to affect the behavior at higher temperatures. An interesting phenomenon is the phase
transition observed around T = 2:5. A qualitative explanation of this phenomenon is that
high temperature adds randomness to the student's choice of action, because it makes the
probabilities P (a) less extreme. Consequently, the ability to predict the student's behavior
lessens, and with it the probability of choosing a good action. However, while randomness
serves to lower the success rate initially, it also guarantees a level of effective cooperation,
which should approach 0.5 as the temperature increases. Finally, notice that although
(Coop,Coop) seems like the best joint-action for a pair of agents, two interacting Q-learners
never learn to play this joint strategy consistently, although they approach 80% Coops at
low temperatures.
8.2.3 Teaching Without a Model

When the teacher does not have a precise model of the student, we cannot use the techniques
of Section 7 to derive an optimal policy; in these models, we assume that the teacher
can \observe" the student's current state (i.e. that it knows the student's Q-values). We
therefore explore two teaching methods that only exploit knowledge of the game and the
fact that the student is a BQL.
Both methods are motivated by a basic strategy of countering the student's move. The
basic idea is to try and counter good actions by the student with an action that will lead
to a high payoff, and to counter bad actions with an action that will give him a low payoff.
Ideally, we would like to play Coop when the student plays Coop, and Defect when the
student plays Defect. Of course, we don't know what action the student will choose, so we
try to predict from his past actions.
If we assume that the Q-values change very little from one iteration to the other, the
student's most likely action in the next game is the same action that he took in the most
recent game. Therefore, if we saw the student play Coop in the previous turn, we will play
Coop now . Similarly, the teacher will follow a Defect by the student with a Defect on her
495

fiBrafman & Tennenholtz

Fraction of Coops over time

Fraction of Coops

1
0.9
0.8
0.7
approximately optimal
Q-learning
Tit-For-Tat
2-Tit-For-Tat

0.6
0.5
2000

4000 6000
Iterations

8000

10000

Figure 4: Fraction of Coops as a function of time for BQL using the temperature decay
scheme of Section 8.1. Teaching strategies shown: approximately optimal strategy, Q-learning, TFT, and 2TFT.
part. This strategy, called Tit-For-Tat (TFT for short), is well known (Eatwell et al., 1989).
Our experiments show that it is not very successful in teaching a BQL (see Figure 3).
We also experimented with a variant of TFT, which we call 2TFT. In this strategy the
teacher plays Defect only after observing two consecutive Defects on the part of the student.
It is motivated by our observation that in certain situations it is better to let the student
enjoy a free lunch (that is, match his Defect with a Coop) than to make Coop look bad
to him, because that may cause his Q-value for Coop to be so low that he is unlikely to
try it again. Two consecutive Defects indicate that the probability of the student playing
Defect next is quite high. The results, shown in Figure 3, indicate that this strategy worked
better than TFT, and in some ranges of temperature, better than Q-learning. However, in
general, both TFT and 2TFT gave disappointing results.10
Finally, Figure 4 shows the performance of all four teaching strategies discussed so
far when we incorporate temperature decay. We can see that the optimal policy is very
successful. As we explained before, teaching is easier when the student is more predictable,
which is the case when temperature is lower. With temperature decay the student spends
most of his time in relatively low temperature and behaves similarly to the case of fixed,
low temperature. While an initial high-temperature phase could have altered this behavior,
we did not observe such effects.

8.3 Teaching Q-Learners

Unlike BQL, Q-learners (QL) have a number of possible states which encode the joint actions
of previous games played. A QL with memory one has four possible states, corresponding
to the four possible joint actions in the prisoner's dilemma; a QL with more memory will
have more states, encoding a sequence of joint actions.
More complex learning architectures have more structure, which brings with it certain
problems. One possible problem may be that this structure is more \teaching-resistant." A

10. In some sense the use of an identical Q-learner implies having a model of the student, while TFT and
2TFT do not make use of such a model.

496

fiOn Partially Controlled Multi-Agent Systems

Tit-For-Tat

Two Q-learners
1
Fraction of Coops

Fraction of Coops

1
0.8
0.6
0.4

10000 iterations
5000 iterations
1000 iterations
100 iterations

0.2

10000 iterations
5000 iterations
1000 iterations
100 iterations

0.8
0.6
0.4
0.2

0

0
0

1

2

3

4 5 6 7
Temperature

8

9 10

0

1

2

3

4 5 6 7
Temperature

8

9 10

Figure 5: Each curve shows the fraction of Coops of QL as a function of temperature for a
fixed number of iterations when TFT was used to teach (left) and when an identical Q-learner was used to teach (right). Values are means over 100 experiments.
more real threat is added computational complexity. As we mentioned, to approximate the
optimal teaching policy for BQL we had to compute over a space of approximately 40,000
discretized states. While representing the state of a BQL requires only two numbers, one
for each Q-value, representing the state of a QL with m states requires 2m + 1 numbers:
one for the Q-value of each state/action pair, and one encoding the current state. The size
of the corresponding discretized state-space for the teacher's Markov decision process grows
exponentially in m. For the simplest case of memory one (a student with four states) this
would be about 1018 states. Since solving the problem with 40,000 states took 12 hours
on a sun sparcstation-10, we were not able to approximate optimal teaching policies for
even the simplest QL.
But all is not lost. More structure may mean more complexity, but it also means more
properties to exploit. We can reach surprisingly good results by exploiting the structure of
Q-learners. Moreover, we can do this using a teaching method introduced in the previous
section. However, in QL this method takes on a new meaning that suggests the familiar
notions of reward and punishment. Interestingly, one may recall that punishment has been
our major tool in our approach to the enforcement of social behavior.
In choosing their actions, QLs \care" not only about immediate rewards, but also about
the current action's effect on future rewards. This makes them suitable for a reward and
punishment scheme. The idea is the following: suppose the QL did something \bad" (Defect
in our case). Although we cannot reliably counter such a move with a move that will lower
his reward, we can punish him later by choosing an action that always gives a negative
payoff, no matter what the student plays. We achieve this by following a student's Defect
with a Defect by the teacher. While the immediate reward obtained by a QL playing Defect
may be high, he will also learn to associate a subsequent punishment with the Defect action.
Thus, while it may be locally beneficial to perform Defect, we may be able to make the
long-term rewards of Defect less desirable. Similarly, we can follow a student's Coop with
a reward in the form of a Coop by the teacher, since it guarantees a positive payoff to the
student.
497

fiBrafman & Tennenholtz

Fraction of Coops over time

Fraction of Coops

1

Tit-For-Tat
Q-learning

0.9
0.8
0.7
0.6
0.5
2000

4000 6000
Iterations

8000

10000

Figure 6: Fraction of Coops of QL as a function of time with temperature decay with TFT
and with Q-learning as teaching strategies.
This suggests using Tit-For-Tat again. Notice that for BQLs, TFT cannot be understood
as a reward/punishment strategy because BQLs care only about the immediate outcome of
an action; the value they associate with each action is a weighted average of the immediate
payoffs generated by playing this action.
In Figure 5 we see the success rates of TFT as a function of temperature, as well as the
rates for Q-learning as a teaching strategy. In this latter case, the teacher is identical to the
student. It is apparent that TFT is extremely successful, especially in higher temperatures.
Interestingly, the behavior is quite different than that of two QLs. Indeed, when we examine
the behavior of two QLs, we see that, to a lesser extent, the phase change noticed in
BQLs still exists. We obtain completely different behavior when TFT is used: Coop levels
increase with temperature, reaching almost 100% above 3.0. Hence, we see that TFT works
better when the student Q-learner exhibits a certain level of experimentation. Indeed, if
we examine the success of these teaching strategies at a very low temperature, we see that
Q-learning performs better than TFT. This explains the behavior of TFT and QL when
temperature decay is introduced, as described in Figure 6. In this figure, QL seems to be
more effective than TFT. This is probably a result of the fact that in this experiment the
student's temperature is quite low most of the time.
In these experiments the QL remembers only the last joint action. We experimented with
QL with more memory and performance was worse. This can be explained as follows. For a
QL with memory one or more, the problem is a fully observable Markov decision process once
the teacher plays TFT, because TFT is a deterministic function of the previous joint action.
We know that Q-learning converges to the optimal policy under such conditions (Watkins &
Dayan, 1992). Adding more memory effectively adds irrelevant attributes, which, in turn,
causes a slower learning rate. We have also examined whether 2TFT would be successful
when agents have a memory of two. The results are not shown here, but the success rate
was considerably lower than for TFT, although better than for two QLs.
TFT performed well as a teaching strategy, and we explained the motivation for using it.
We now want to produce a more quantitative explanation, one that can be used to predict
its success when we vary various parameters, such as the payoff matrix.
498

fiOn Partially Controlled Multi-Agent Systems

Coop rates as a function of DIF

Fraction of Coops

1
0.8
0.6
0.4
0.2
0
-20 -10

0

10

20 30
DIF

40

50

60

Figure 7: Coop rates as a function of DIF = a + b +  (a + c) , (c + d +  (b + d)). The means
are for 100 experiments, 10000 iterations each. Student's memory is 1.
Let the student's payoff matrix be as in matrix A of Figure 1; let p be the probability
that the student plays Coop, and let q = 1 , p be the probability that the student plays
Defect. These probabilities are a function of the student's Q-values (see the description in
Section 8.1). Let us assume that the probabilities p and q do not change considerably from
one iteration to the next. This seems especially justified when the learning rate, ff, is small.
Given this information, what is the student's expected reward for playing Coop? In
TFT, the teacher's current action is the student's previous action, so we can also assume
that the teacher will play Coop with probability p. Thus, the student's expected payoff for
playing Coop is (p  a + q  b). Since Q-learners care about their discounted future reward
(not just their current reward), what happens next is also important. Since we assumed
that the student cooperated, the teacher will cooperate in the next iteration, and if we still
assume p to be the probability that the student will cooperate next, the student's expected
payoff in the next step is (p  a + q  c). If we ignore higher order  terms the expected reward
of playing Coop becomes: p  a + q  b +  (p  a + q  c): The expected reward of Defect is thus:
p  c + q  d +  (p  b + q  d): Therefore, TFT should succeed as a teaching strategy when:

p  a + q  b +  (p  a + q  c) > p  c + q  d +  (p  b + q  d):
Since initially p = q = 0:5, and it is the behavior at the stage where p and q are approximately equal that will determine whether TFT succeeds, we can attempt to predict the
success of TFT based on whether:
DIF = a + b +  (a + c) , [(c + d +  (b + d))]  0
To test this hypothesis we ran TFT on a number of matrices using Q-learners with different
discount factors. The results in Figure 7 show the fraction of Coops over 10000 iterations
as a function of DIF for a teacher using TFT, and with temperature decay. We see that
DIF is a reasonable predictor of success. When it is below 0, almost all rates are below
20%, and above 8 most rates are above 65%. However, between 0 and 8 it is not successful.
499

fiBrafman & Tennenholtz

8.4 Teaching as a Design Tool

In Section 6 we identified a class of games that are challenging to teach, and the previous
sections were mostly devoted to exploring teaching strategies in these games when the
student is a Q-learner. One of the assumptions we made was that the teacher is trying to
optimize some function of the student's behavior and does not care what she has to do in
order to achieve this optimal behavior. However, often the teacher would like to maximize
some function that depends both on her behavior and on the student's behavior. When
this is the case, even the more simple games discussed in Section 6 pose a challenge.
In this section, we examine a basic coordination problem, block pushing, in which our
objective is not teaching, but where teaching is essential for obtaining good results. Our
aim in this section is to demonstrate this point, and hence the value of understanding
embedded teaching. Our results show that there is a teaching strategy that achieves much
better performance than a naive teaching strategy and leads to behavior that is much better
than that of two reinforcement learners.
Consider two agents that must push a block as far as possible along a given path in
the course of 10,000 time units. At each time unit each agent can push the block along
the path, either gently (saving its energy) or hard (spending much energy). The block will
move in each iteration c  x  h + (2 , x)  h units in the desired direction, where h; c > 0 are
constants and x is the number of agents which push hard. At each iteration, the agents are
paid according to the distance the block was pushed. Naturally, the agents wish to work as
little as possible while being paid as much as possible, and the payoff in each iteration is a
function of the cost of pushing and the payment received. We assume that each agent prefers
that the block will be pushed hard by at least one of the agents (guaranteeing reasonable
payment), but each agent also prefers that the other agent will be the one pushing hard. If
we denote the two actions by gentle and hard, we get that the related game can be described
as follows:
hard gentle
hard (3,3) (2,6)
gentle (6,2) (1,1)

Notice that the above game falls into the category of games where teaching is easy. If
all the teacher cares about is that the student will learn to push hard, she will simply push
gently. However, when the teacher is actually trying to maximize the distance in which the
block moved, this teaching strategy may not be optimal. Notice that there can be at most
20000 instances of hard push; the naive teaching strategy mentioned above will yield no
more than 10000 instances of hard push. In order to increase the number, we need a more
complex teaching strategy.
In the results below we use BQL with ff = 0:001. Consider the following strategy for
the teacher: push gently for K iterations, and then start to push hard. As we will see, by
a right selection of K , we obtain the desired behavior. Not only will the student push hard
most of the time, but the total number of hard push instances will improve dramatically.
In Figure 8, the x coordinate corresponds to the parameter K , while the Y coordinate
corresponds to the number of hard push instances which occur in 10000 iterations. The
results obtained are average results for 50 trials.
500

fiOn Partially Controlled Multi-Agent Systems

17000
16000
15000
14000
13000
12000
11000
2000

4000

6000

8000

Figure 8: Teaching to push hard: number of hard push instances by the student in 10000
iterations as a function if the number of iterations in which the teacher does not
push hard (avg. over 50 trials).
As we can see from Figure 8, the eciency of the system is non-monotonic in the
threshold K . The behavior we obtain with an appropriate selection of K is much better
than what we would have obtained with the naive teaching strategy. It is interesting to
note the existence of a sharp phase transition in the performance at the neighborhood of
the optimal K . Finally, we mention that when both agents are reinforcement learners, we
get only 7618 instances of \push hard", which is much worse than what is obtained when
we have a knowledgeable agent that utilizes its knowledge to inuence the behavior of the
other agent.

9. Towards A General Theory
The two case studies presented in this paper raise the natural question of whether general,
domain independent techniques for PCMAS design exist, and whether we have learned
about such tools from our case studies. We believe that it is still premature to say whether
a general theory of PCMAS design will emerge; this requires much additional work. Indeed,
given the considerable differences that exist between the two domains explored in this
paper, and given the large range of multi-agent systems and agents that can be envisioned,
we doubt the existence of common low-level techniques for PCMAS design. Even within
the class of rational agents which we investigated, agents can differ considerably in their
physical, computational, and memory capabilities, and in their approach to decision making
(e.g., expected utility maximization, maximization of worst-case outcomes, minimization of
regret). Similarly, the problem of social-law enforcement can take on different forms, for
example, the malicious agents could cooperate among each other. However, once a more
abstract view is taken, certain important unifying concepts appear, namely, punishment
and reward.
Punishment and reward are abstract descriptions of two types of high-level feedbacks
that the controllable agents can provide to the uncontrollable agents. Although punishment
and reward take different form and meaning in the two domains, in both cases, the uncon501

fiBrafman & Tennenholtz

trollable agents seem to \care" about the controllable agent's reaction to their action. What
we see is that in both cases, the controllable agents can inuence the uncontrollable agents'
perception of the worthiness of their actions. The precise manner in which the controllable
agents affect this perception differs, but in both cases it utilizes some inherent aspect of
uncertainty in the uncontrollable agent's world model. In the case of rational agents, despite
their perfect knowledge of the dynamics of the world, uncertainty remains regarding the
outcome of the non-malicious agents' actions. By fixing a certain course of action for the
controllable agents, we inuence the malicious agents' perception of the outcome of their
own actions. In the case of the learning agent, one can affect the perception of the student's
action by affecting its basic world model. Hence, it seems that a high-level approach for PCMAS design has two stages: First, we analyze the factors that inuence the uncontrollable
agent's perception of their actions. Next, we analyze our ability to control these factors. In
retrospect, this has been implicit in our approach. In our study of social-law enforcement,
we used the projected game to find out how an agent's perception of an action can be
changed and used the indirect mechanism of threats to enforce the perception we desired.
In our study of embedded teaching, we started with an analysis of different games and the
possibility of affecting an agent's perception of an action in these games. Next, we tried
to provide this perception. In the case of BQL students, our controllable teacher did not
have complete control over the elements that determine the student's perception because
of the random nature of the student's action. Yet, she did try to somehow affect them. In
the case of the Q-learners, direct control was not available over all factors determining the
student's perception. Yet, the teacher could control some aspects of this perception, which
were found to be sucient.
One might ask how representative our studies are of general PCMAS domains, and
therefore, how relevant is the insight they may provide. We have chosen these two domains
with the belief that they represent key aspects of the types of agents studied in AI. In
AI, we study dynamic agents that act to improve their state. These agents are likely to
use information to revise their assessment of the state of the world, much like the learning
agents, and will need to make decisions based on their current information, much like the
expected utility maximizers we have studied. Hence, typical multi-agent systems studied in
AI include agents that exhibit one or both of these properties.
While punishment and rewards provide the conceptual basis for designing the controllable agents, MDPs supply a natural model for many domains. In particular, MDPs are
suitable when uncertainty exists, stemming either from the other agents' choices or from
nature. As we showed in Section 7, at least in principle, we can use established techniques to
obtain strategies for the controllable agents when the problem can be phrased as a Markov
decision process. Using the MDP perspective in other cases would require more sophisticates tools and a number of important challenges must be met first: (1) The assumptions
that the agent's state is fully observable and that the environment's state is fully observable
is unrealistic in many domains. When these assumptions are invalid, we obtain a partially
observable Markov decision process (POMDP) (Sondik, 1978). Unfortunately, although
POMDPs can be used in principle to obtain the ideal policy for our agents, current techniques for solving POMDPs are limited to very small problems. Hence, in practice one will
have to resort to heuristic punishment and reward strategies. (2) In Section 7 we had only
502

fiOn Partially Controlled Multi-Agent Systems

one controlling agent. This poses a natural challenge of generalizing tools and techniques
from MDPs to distributed decision making processes.

10. Summary and Related Work
This paper introduces the distinction between controllable and uncontrollable agents and the
concept of partially controlled multi-agent systems. It provides two problems in multi-agent
system design that naturally fall into the framework of PCMAS design and suggests concrete
techniques for inuencing the behavior of the uncontrollable agents in these domains. This
work contributes to AI research by introducing and exploring a promising perspective on
system design and it contributes to DES research by considering two types of structural
assumptions on agents, corresponding to rational and learning agents.
The application of our approach to the enforcement of social behavior introduces a new
tool in the design of multi-agent systems, punishment and threats. We used this notion and
investigated it as part of an explicit design paradigm. Punishment, deterrence, and threats
have been studied in political science (Dixit & Nalebuff, 1991; Schelling, 1980); yet, in
difference to that line of work (and its related game-theoretic models), we consider the case
of a dynamic multi-agent system and concentrate on punishment design issues, such as the
question of minimizing the number of reliable agents needed to control the system. Unlike
much work in multi-agent systems, we did not assume all agents to be rational or all agents
to be law-abiding. Rather, we only assumed that the designer can control some of the agents
and that deviations from the social laws by the uncontrolled agents need to be rational.
Notice that the behavior of controllable agents may be considered irrational in some cases;
however, it will eventually lead to desired behavior for all the agents. Some approaches
to negotiations can be viewed as incorporating threats. In particular, Rosenschein and
Genesereth (1985) consider a mechanism making deals among rational agents, where agents
are asked to offer a joint strategy to be followed by all agents and declare the move they
would take if there will be no agreement on the joint strategy. This latter move can be viewed
as a threat describing the implications of refusing the agent's suggested joint strategy. For
example, in the prisoner's dilemma setting an agent may propose joint cooperation and
threaten defecting otherwise. The work in the first part of this paper could be viewed
as examining how such a threat could be credible and effective in a particular context of
iterative multi-agent interactions.
As part of our study, we proposed embedded teaching as a situated teaching paradigm
suitable for modeling a wide range of teaching instances. We modeled the teacher and
the student as players in an iterated two-player game. We concentrated on a particular
iterative game, which we showed to be the most challenging game of its type. In our model,
the dynamics of the teacher-student interaction is made explicit, and it clearly delineated
the limits placed on the teacher's ability to inuence the student. We showed that with
a detailed model of the student, optimal teaching policies can be theoretically generated
by viewing the teaching problem as a Markov decision process. The performance of the
optimal teaching policy serves as a bound on any agent's ability to inuence this student.
We examined our ability to teach two types of reinforcement learners. In particular, we
showed that when an optimal policy cannot be used, we can use TFT as a teaching method.
In the case of Q-learners this policy was very successful. Consequently, we proposed a model
503

fiBrafman & Tennenholtz

that explains this success. Finally, we showed that even in those games in which teaching
is not challenging, it is nevertheless quite useful. Moreover, when our objective is more
than simply teaching the student, even those simpler domains present some non-trivial
choices. In the future we hope to examine other learning architectures and see whether the
lessons learned in this domain can be generalized, and whether we can use these methods
to accelerate learning in other domains.
A number of authors have discussed reinforcement learning in multi-agent systems.
Yanco and Stein (1993) examine the evolution of communication among cooperative reinforcement learners. Sen et al. (1994) use Q-learning to induce cooperation between two
block pushing robots. Matraic (1995) and Parker (1993) consider the use of reinforcement
learning in physical robots. They consider features of real robots, which are not discussed
in this paper. Shoham and Tennenholtz (1992) examine the evolution of conventions in a
society of reinforcement learners. Kittock (1994) investigates the effects of societal structure on multi-agent learning. Littman (1994) develops reinforcement learning techniques for
agents whose goals are opposed, and Tan (1993) examines the benefit of sharing information
among reinforcement learners. Finally, Whitehead (1991) has shown that n reinforcement
learners that can observe everything about each other can decrease learning time by a factor
of n. However, the above work is not concerned with teaching, or with the question of how
much inuence one agent can have over another. Lin (1992) is explicitly concerned with
teaching as a way of accelerating learning of enhanced Q-learners. He uses experience replay and supplies students with examples of how the task can be achieved. As we remarked
earlier, this teaching approach is different from ours, since the teachers are not embedded
in the student's domain. Within game theory there is an extensive body of work that tries
to understand the evolution of cooperation in the iterated prisoner's dilemma and to find
good playing strategies for it (Eatwell et al., 1989). In that work both players have the
same knowledge, and teaching is not an issue.
Last but not least, our work has important links to work on conditioning and especially
operant conditioning in psychology (Mackintosh, 1983). In conditioning experiments an
experimenter tries to induce changes in its subjects by arranging that certain relationships
will hold in their environment, or by explicitly (in operant conditioning) reinforcing the
subjects' actions. In our framework the controlled agent plays a similar role to that of the
experimenter. Our work uses a control-theoretic approach to the related problem, while
applying it to two basic AI contexts.
The main drawback of our case studies is the simple domains in which they were conducted. While this is typical of initial exploration of new problems, future work should try
to remove some of the limiting assumptions that our models incorporate. For example, in
the embedded teaching context, we assumed that there is no uncertainty about the outcome of a joint action. Similarly, our model of multi-agent interaction in Section 3 is very
symmetric, assuming all agents can play all of the k roles in the game, that they are equally
likely to play each role, etc. Another assumption made was that malicious agents were
\loners" acting on their own, as opposed to a team of agents. Perhaps more importantly,
future work should identify additional domains that are naturally described in terms of
PCMAS and formalize a general methodology for solving PCMAS design problems.
504

fiOn Partially Controlled Multi-Agent Systems

Acknowledgements
We are grateful to Yoav Shoham and other members of the Nobotics group at Stanford for
their input, and to the anonymous referees for their productive comments and suggestions.
We are especially grateful to James Kittock for his comments and his help in improving the
presentation of this paper. This research was supported by the fund for the promotion of
research at the Technion, by NSF grant IRI-9220645, and by AFOSR grant AF F49620-941-0090.

References

Altenberg, L., & Feldman, M. W. (1987). Selection, generalized transmission, and the
evolution of modifier genes. i. the reduction principle. Genetics, 559{572.
Bellman, R. (1962). Dynamic Programming. Princeton University Press.
Bond, A. H., & Gasser, L. (1988). Readings in Distributed Artificial Intelligence. Ablex
Publishing Corporation.
Briggs, W., & Cook, D. (1995). Flexible Social Laws. In Proc. 14th International Joint
Conference on Artificial Intelligence, pp. 688{693.
Dixit, A. K., & Nalebuff, B. J. (1991). Thinking strategically : the competitive edge in
business, politics, and everyday life. Norton, New York.
Durfee, E. H., Lesser, V. R., & Corkill, D. D. (1987). Coherent Cooperation Among Communicating Problem Solvers. IEEE Transactions on Computers, 36, 1275{1291.
Dwork, C., & Moses, Y. (1990). Knowledge and Common Knowledge in a Byzantine Environment: Crash Failures. Information and Computation, 88 (2), 156{186.
Eatwell, J., Milgate, M., & Newman, P. (Eds.). (1989). The New Palgrave: Game Theory.
W.W.Norton & Company, Inc.
Fox, M. S. (1981). An organizational view of distributed systems. IEEE Trans. Sys., Man.,
Cyber., 11, 70{80.
Fudenberg, D., & Tirole, J. (1991). Game Theory. MIT Press.
Gilboa, I., & Matsui, A. (1991). Social stability and equilibrium. Econometrica, 59 (3),
859{867.
Gold, M. (1978). Complexity of Automaton Identificaion from Given Data. Information
and Control, 37, 302{320.
Huberman, B. A., & Hogg, T. (1988). The Behavior of Computational Ecologies. In
Huberman, B. A. (Ed.), The Ecology of Computation. Elsevier Science.
Kaelbling, L. (1990). Learning in embedded systems. Ph.D. thesis, Stanford University.
505

fiBrafman & Tennenholtz

Kandori, M., Mailath, G., & Rob, R. (1991). Learning, Mutation and Long Equilibria in
Games. Mimeo. University of Pennsylvania, 1991.
Kinderman, R., & Snell, S. L. (1980). Markov Random Fields and their Applications.
American Mathematical Society.
Kittock, J. E. (1994). The impact of locality and authority on emergent conventions. In
Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI '94),
pp. 420{425.
Kraus, S., & Wilkenfeld, J. (1991). The Function of Time in Cooperative Negotiations. In
Proc. of AAAI-91, pp. 179{184.
Lin, F., & Wonham, W. (1988). Decentralized control and coordination of discrete-event
systems. In Proceedings of the 27th IEEE Conf. Decision and Control, pp. 1125{1130.
Lin, L. (1992). Self-improving reactive agents based on reinforcement learning, planning,
and teaching. Machine Learning, 8 (3{4).
Littman, M. (1994). Markov games as a framework for multi-agent reinforcement learning.
In Proc. of the 11th Int. Conf. on Mach. Learn.
Luce, R. D., & Raiffa, H. (1957). Games and Decisions- Introduction and Critical Survey.
John Wiley and Sons.
Mackintosh, N. (1983). Conditioning and Associative Learning. Oxford University Press.
Malone, T. W. (1987). Modeling Coordination in Organizations and Markets. Management
Science, 33 (10), 1317{1332.
Mataric, M. J. (1995). Reward Functions for Accelerating Learning. In Proceedings of the
11th international conference on Machine Learning, pp. 181{189.
Minsky, N. (1991). The imposition of protocols over open distributed systems. IEEE
Transactions on Software Engineering, 17 (2), 183{195.
Moses, Y., & Tennenholtz, M. (1995). Artificial social systems. Computers and Artificial
Intelligence, 14 (6), 533{562.
Narendra, K., & Thathachar, M. A. L. (1989). Learning Automata: An Introduction.
Prentice Hall.
Owen, G. (1982). Game Theory (2nd Ed.). Academic Press.
Parker, L. E. (1993). Learning in Cooperative Robot Teams. In Proceedings of IJCAI-93
Workshop on Dynamically Interacting Robots.
Ramadge, P., & Wonham, W. (1989). The Control of Discrete Event Systems. Proceedings
of the IEEE, 77 (1), 81{98.
Rosenschein, J. S., & Genesereth, M. R. (1985). Deals Among Rational Agents. In Proc.
9th International Joint Conference on Artificial Intelligence, pp. 91{99.
506

fiOn Partially Controlled Multi-Agent Systems

Schelling, T. (1980). The Strategy of Conict. Harvard University Press.
Sen, S., Sekaran, M., & Hale, J. (1994). Learning to coordinate without sharing information.
In Proc. of AAAI-94, pp. 426{431.
Shoham, Y., & Tennenholtz, M. (1992). Emergent Conventions in Multi-Agent Systems:
initial experimental results and observations. In Proc. of the 3rd International Conference on Principles of Knowledge Representation and Reasoning, pp. 225{231.
Shoham, Y., & Tennenholtz, M. (1995). Social Laws for Artificial Agent Societies: Off-line
Design. Artificial Inteligence, 73.
Sondik, E. J. (1978). The optimal control of partially observable markov processes over the
infinite horizon: Discounted costs. Operations Research, 26 (2).
Sutton, R. (1988). Learning to predict by method of temporal differences. Mach. Lear.,
3 (1), 9{44.
Tan, M. (1993). Multi-Agent Reinforcement Learning: Independent vs. Cooperative Agents.
In Proceedings of the 10th International Conference on Machine Learning.
Watkins, C. (1989). Learning With Delayed Rewards. Ph.D. thesis, Cambridge University.
Watkins, C., & Dayan, P. (1992). Q-learning. Machine Learning, 8 (3/4), 279{292.
Weidlich, W., & Haag, G. (1983). Concepts and Models of a Quantitative Sociology; The
Dynamics of Interacting Populations. Springer-Verlag.
Whitehead, S. (1991). A complexity analysis of cooperative mechanisms in reinforcement
learning. In Proceedings of AAAI-91, pp. 607{613.
Yanco, H., & Stein, L. (1993). An Adaptive Communication Protocol for Cooperating
Mobile Robots. In From Animal to Animats: Proceedings of the Second International
Conference on the Simulation of Adaptive Behavior, pp. 478{485.
Zlotkin, G., & Rosenschein, J. S. (1993). A Domain Theory for Task Oriented Negotiation.
In Proc. 13th International Joint Conference on Artificial Intelligence, pp. 416{422.

507

fi	
fffi 	


 ! #"$ % 	'&)(+*', --.0/1,32-54678

FHGJILKNMPO:K=ILQSR

9:;<=  >?,,3@-0A:BDC;
%&>E(@-.

T+MPUWVYX[Z]\^V_G`FHGJVbaWO:cWdfegTbGihjO:cWI?M1O:V_ckVL\
l

TbcWTbGJImQSnSVodoO!KpFHGJVod_GJILhqZ

rostvu#wyxzw|{^}ff~{

1L$

J0!v^L!0_5bo
$ Y0vN^[  ?05^mE$ffE?0v

gE^Pv
:5LD D?^3^5^o_=3:D3:y^ o! ^:y^::5DE^
3^3o^So33o3DE[ !^3_5Do^S0o3D3DL!^5L
3: 3ff:5?:^ :D3oPD[#3v ?:D3^?D Y^S L033503
:^ $ o^3: :5LD 0 :m=b!)330!L:3^Y
3ff !3LD3D:!3^^^# 5^o$ : ::5N#3^^3:035=:) 
3 5m 5D5= 5^!!b3:1151 !3LD3?!!1
5)3: _55#:5)^ JDN5#!^mffff^ff':5^3^5^
 o5 0:$ _5:3^33[L:3^W3 !3LD3
^^!:5D^E3^5^o:3y3:3:3 _3:off'L^3)3: 
:^b3oD:^o!L: !Wm!^N^3:5L^ o^!:5D^^b3^5^o:

 
	ff
fi	

fi "!$#&%')(*
+-,.
-/01.
24356	(*
27fi84fi9/-	:fi	6;.<=/2fi9/-.>(?

@AfiB(*
2<C(D	FE 	/-.G
24fi-	.5H&fiGI.@/@
fi9/-6KJMLONP./Gfi	6RQTSVUUW=0Gfi	6RQXfi9
fi
fi	6Y(?	6;TSVUUW=0Z\[	
:]	fi^_H.`D
5%'1	fi9/-YfiGa.	b/-c(?fiIF
2c&ff
fi9/-
4fidJAeI	-6
/-+.=/f/-,	G/-&
fi	Aff
2--/-	P.
2J.g<NP
-/-a.fi_hi$/-.Gfijfi)P	

fi
lk:VmXfi-nM1oqp5ff
2/HcSVUrsMtcuv3fi	4H\SVUUwMtcN>	/OoxQ8yH:SVUUMSz0Z|{j,	d.</2fi9/-}(
Aff
fi9/-
2ebn~Cfi9
-n%':	fi9/-;fic(fiI	
2K ~Xfi9
-nHSVUr0cfi.6~X,.fi%':	@/@
2I.ff/-b3.fi9/-
 ~X,.fiHSVUrr0Hv\J_fi26/-ff
2.fi9/-Y	6	/-	ZK{j,	ff
(*
2^/-,	^-/-I	6FL(X/-ff
2<.fi9/-Y(
B!=#&%':lZ'5ZbHcp52,	
2ffLco
-/-HBSVUUW=0jjfiO&
2/2fi=//-.Z
{CmXOfi2-\(jB!=#&%'\/-,.fi9/\JA,.fi43+mm:Z
4Z/Z/-ff
_fi9/-Yfi9
2/-,	^-9E 4fi6fiffL5<fi	6
fiff	/2fi9J.j	
2
fiXlN>	/oQ8H	SVUUMStFNP	/o`f6F
2-,	lH	SVUUMSz0Zhi+(fiff/HMN>	/fi.6^Q8
TSVUUMSz0e	
43</-,.fi9/+(j	fi9/-;fi)a_	b/-<(?fiIF
2).
-A
fi9/-6=/-/-,	<	
2M(C/-,	
-LHv/-,	
(?
1fi$LdfiffL5e	

fiyHAfi-65E 6.ff
2b3fi9/-	jm>b/-,dfi9
2J.b/@
fi9
-L-ff/-d
2I	c(D
I		6R$I.ff
2
/-ff
2<.fi9/-ZX{j,	:$3ff
2-e(f/-,	P
2-I	/H.lZ'ZbH_(ffi	
2
fi/-ff
2<.fi9/-j(?
GfiB
2I		6$I.ff
2H
/-,	Yb/11fiffL5H,	6		bLI.	6	ff
G/-,	fi--I		/-O/-,.fi9/1/-,	)	
2
fic	5E.I	.6	ff
2	5%bZ
N>	/ffi.6e`f6F
-2,	&TSVUUMSz0f-/2fi9J.-,)fi.fiI.B
-I	b/-e/-ff
2.fi9/-\(?
-9E 4fi6+fiff	/2fi9J_
	
2
fiH&/-,	>/-\m:Z
4Z/ZA/-,	+`v
2K-ff/-R
2I	HAmP,.2,R-ff/-:/-,	+(*/-@/b/-ff
fi(Xfi
MI	ff
-LZ
[fI	.6	ff
2	^1fiRfi9J_	
27fi"(?
2(D/-ff
2<.fi9/-OmP,	,dfi9
2-1fi1-$Yfi:fi<	5E 
I		6
	fi9/-6Ofi9/-X-ff/-6AH	fi]5.fi	6Z'5ZbH	lN>	/CoQlH&SVUUW=0Z{Be/@
24fi9/jfi-e	5E 
I		6
	fi9/-6+fi9/-<H~C,.fiTSVUrr0B$/@
2M6	I	6efij	
$6	I	
2n5	4mP)fi~X,.fiA%'	@/@
2I.ff/-b3v	fi9/-AZ
 -	+~X,.fiA%'	@/@
I	ff/-b3	fi9/-AH	dfi9
22,.
2fTSVUUs0X-,	Vm6</-,.fi9/X/-,		/-	(fiffLMb/ L
fi	6}fiff	/2fi9J.b/ L	
2V3M6.fid^_ff/-^,.fi9
fiff/-ff
284fi9/-(j	

fi\/-,.fi9/)/-ff
2<.fi9/-^(?
fi

2I	.6K$I	ff
Z
JW = +v  





X







 z

, --.v %% !>	5	> b0	 P	!fi<	 ;!
%&% 

^&0%|% 0 >

fiO.5M	.F
j.G.-Kff.29.b<)_	-	1	ff.	-fff5b^Pb-O)2<$-G		
-A5	+-	ff*2j	245	:ff.29.b+<4cAX9-	ff	)Aff2-<Aj	P_-._9Aff
1-	ff;O<ff-	$.?\	45	7-ff2<.9-dP-Y2@Aff\-O-	^v2O-ff-
2	D$	-.db@-<-<$-<	?279-+A-2b.	2)@.b)$-R X
_9--457DP	7f9F-	K	G_9-C	2V7-)Aff55-	1-.ffj	G-)&1ff	29_
	y-	-:-	b-P9:).	7-		.1-.9P-.:2_A	2C-ff2<.9-	e:4
-	cD2<-ff-y2.jj	c	^A--K>	.\-	2R^X4O-.9	2*9-	
	ff..<<vj$	P791=@2M		G-		-	Cv9.@i=ff	P	ffGvjX	2V
-KAff	29.	  -KAff5.@i=fffP	R@249-\-.+=ff22
4-Rlv+ff5K	<ff	29.Vi2	ff)-dA^>2	ff-4X	--.	-
FFff.29.b)	ff}-;29Yz_TT_=ff?_}O$-9-7-	-K	-	+
A@- 	<ff-	$.O*	45	-ff2.9-yv	ffc	2G9	.OF
2-.b-e-R	2<c?27.7	2.<:;	F 	-.242		5i_9--	94X
-	V-.9C-	P_		?$*d?5)4ffzGvff7	2.47AG*27	7^_$-e$
<4	XfyFFff	29.1	2j	X.245	j+*-P$.ff2:lF	&.		7$.ff2
-.9P4yAG.ff-	@Xff2A
 7-		KFC7-	Glj	2$$.M'.K$C	4Pb-OX.A'X	-@2	
-be	9-Y	&<-..@Aff-y-.c.2$?G-	4>>-_9>-	ffd	$.bKB?
-	:42:f	9-yP.	b-F2
 	9		2<	2VM.c-.ff-	M	+?.245	1-ff_9-+BB=&'MM
..	+-	e22	b-P	1	YfF222	fff5+	Yff	29.).2<1j	
2ff	<C-	cff-.$	R\?A?y9^-)X	4be4ff2<e-	
F4j_2&-	j<ff-	$+A>	.^fF2-	"TVMz	_bc-	P.-j&-M\+	-7=-
	?279-A.	db4>P-<	$-*<-.-\9-->D-	:	P	-\-ff2.9-KG	
A		$Be-		-F-	D2>-ff-\	 2Vff4=-	 	^.-.992bA-	V
-.9@5@-<_-)e-	X	2<	v_9		45	G-.b29.D?2749-e	
.<=29-*P	2.<K	5 <	-	>24-.	5
j.7_9Aff^+2	 ?VP j.K	 M2ff- $2	-<-ff2<		
	2.92&T
 5ff-	
 )	7e-		-	CffMb 7	Kff	29.b 792G	2-$-A
5ff-.G$M&	$=2FG-ff2.9-bc	ff.	b-	Xfff.29.bXi
 Fff-
 	)-.-
	ff..b-	92+$-9-Y7<ff-	M	y?1	2VM.-ff_9-Aff
 
f.&fi
 Fff-
-<c.	-.9+bA+j	G_9Aff1: 5-		.R2ffM-Rff2-YG d922	2l
V
 !"#

j	<?4>	d	29-;PDA.-AK?V|v2R@5=2$}	--.+-_9^-@2	
@29--	+Pb-O^49.b2Bff@-ffP2ff.2-$->)929.FP	-	ff@@2	X2ff	22=G	@2$-
-ff2<\	29-	4&
 %>9-@M)A:9^*-	.-M(
 '*),+-)/.5Ob-ffD\b-	ff)
9-0
 'B 12 )4343432)51$6.>+	9-R9-8
 7!'" 14 )4343432)51$6.GOM. 
 1:9<;.>	M.b
=  1?9@
> ;-fP	ff2 = M.$-b.e4ff+-<OAff2.9.:.	V:X-.929.c$	-2	y?
 1 ;
 M.-	 	$_b-92-4 2F@?.?>. 		- $B
 Az> 	M.b
=  1&9C
> ;-Cj.E D)E FGfbP>29-@_9_1.FG	PG
 
.	@2	I HJ9L
> K7P.2<b-bvG
l 5-		_X.ff1.T2@M DO&		-OM7+ N)	j)_	b--ffPf	2
OQPSR 4 )434343T) R U 3
V5WTX

fiY Z\[^]`_ba`cedgfZihj_balknml_o[`ap[`qrsfafZ\ktvu[Ic_owY Z\[`c`Z\k`h x

y{zb|~}@^y{}i5zovM|~njMij:zoozb|~5MMvn/(Q445zovizb|~
~4^i4i4ffnozb|~Mo4iMizog4iM|~4^(
 &|/,2|i4|~4in\M,ni|~n4G}`MTnni,Mn~4s|~Mn4-|",4,nol|~zon
y{zb|~}ffgi,^44il,MTziEM5MooG4~,zb4M*noo2y:4iMG~l~|~zb|~l|~zonL4&$524442/i$`n
yG:i4M|~^{g|~}{4^Mozb|EM,&G<5i44,vl5jiMM"4|~4&|~n
zoiMo-|~}is"i,zbn|~zonini5 zb|~MMin4l442,My{zo|~}v&^",|~}i4nl|~4Mi/yG
~l`/|~zb|~l|~zon4n|~}i4|~}i:Mi~y,EM,sMi|,Mzoi45n0|~}i:i4|~zonn!{/44~:5
y{}i5g-M-|~zb4T |~}iGn,z`o4iM|j^44l~,zo:zoiM zo/|,Mi4M"4ni,zoi|~}iGi5nM5M
 i
 
 }ifiMi~y&|~|~}ifi^i~<!ezo& 
 ? 
 Tfi2Mvn"Ti,4,nol|~zon"
,4~nl|~zonvMin4-|~4y{zb|~}G}M i,^44il,M  ~}2y|~}i4M~,4|~i4~n*n"Ti,4,nol|~zon
yGff,}i^n~Mi,nMM ~4&M"|~z4G|~}iG   g4no|~zoneGE  TM5  }izog~4&M"|~z4gzo{
|~lMzo"|~,i,|,|~zonfinG
 	M&~|ffnizb|~zoni4ff
 
-|~izb|~zo4bn|~}iG   4no|~zon
ng&i,nMM
 fii4iM|~4^
 T fi5z|~}ii5/|M,iff|~}4M~fiMi|,Mzoi4fi^e,M4zi&|~}i
zoozo2|~znn2M5}p4EMi~ffn
 fi y{zb|~}Mp4^izbnMo4i4M
 
|:zo{4ni~|/,i|~4eM:nooTy{4
 4o2yff 
^M"|~zo42:&2444T/"
M:~(,4E|~zon(/\v!n-^44l~,zivzofi}2^zi,4,ni4"|~


z -4M|-44lszo&|~}i}i2MnM-(4Mi~|~}4Mi(|~}iM,iE
   444T/M!"$#&%4M('
M|~}~y{zo~Miz|~}i4M&|~}iEM5ig)
 +*5, 'zo)
 -Mi/
 .  444T(.10ly{zb|~}
2 ,ffMoI|~}i4Mi,4n3
 fi y:zb|~}(}i2M(/\vn"*ly:zb|~
} .*?5 4   4442(4  j 
|~}4Mip|~}ivEM,6
  &444$/7
   9 8;: = < 0?> A @{!p/5*y{}i5ff
 @zos|~}i
,|:nn,zo4{n
 .`{jzoff5 4   &p44M/
 4  5!Mi&24442/5E,4~}
n,z`o44





zo`Mobn*|~}iEno2y{zoB5~,(,4"$#DC9*E"GF$CHJI6%&,Mii4~(|~}|ff|~}4-`Mozb|e|~}i4M,en
$ fiG!44n4G|~}i,MM|~}inffn|~}iL
 K:,i5Mi(izb,~M
 &2444T/M M O*4444TPO!MQ &GROp44RO!5
M{~ii|~zn(/^vn M 
  S` O  444$POVU5
NM   444T/MsT
M{~zo/|~zoi|Gii|~zon(/\vno M Mi)S
  4
  W
M{~(|~5X
 4ff2 |4i ^44l,{zo/
 4n
NM

 }~niii4~gn "Tl",4~nl|~zonyff 2 |4Ig   g~4M-|~zo4gnooTy{g5n
$fiY ZR

!{  p44&:  /5

y{}i57e^M"|~zo42Mo|~}ff54M5zo4gn|~}iffM,iE^"Tl",4~nl|~zon(zog4no|~
nibEM:-i5zo4}2^zovM|~,zo|~zoii,zbn|~zon[
 
(M|4G}M G,-44l,zoiM|i4
zoQ}`MMzol`izb|~(i,zoM|~zon* s?4ni~4^i4i4M:|~}iM|~zonnzolizo|~Tvi,zbn|~zon<zo
\(]\

fi^_V`Aa;bcedV`fc

ghJikjJlemjGn;okpqrVgqptsvuwVxeoxeojzy{(hJ|Vleq}+~hJ{kiPwqo=iPpnTh$~ffiPq{}xegVjiPxeh$gTh$~	o|tqGjJoPq
iPwqLghJiPxh$gh$~pq{xeJjiPxeh$g)xeoh$~{xjJlx}ythJ{PijJgqJs[uwq{(q~hJ{qJAmq{q~9q{wq{qiPhjJg/jJliPq{(gVjiPxq
pqrVgVxiPxeh$g6h$~wVjJgo3yV{h	qpVf{q1$xqgB|	nBkj{(wxehJ{(x$$(;mwq{(qiPwVq1of|i={qqooqpffiPh{(qoPh$lq
gqjiPxq6lexiPq{jJleo7j{q|VxeleixegkjiPhJyf5phmg/mjn$th$goPi={iPxegiPwqx{7|{(jJg(wqo7xgyj{(jJlelqls7oLj
h$goPq;qgqJViPwq})jJxegpVq{x$jiPxeh$gxeoxegrVgxiPqxe~3ji7leqGjJo=ih$gqLh$~iPwqoqoPf|i={(qqoxeoxegfrgxiPqJs
uq{}xegVjiPxeh$gh$~	opqytqgpoh$giPwVqoPqleqiPxeh$g{leqJshJ{xego=ijJgVqJfiPwqLy{h$J{jJ}
N



iPq{}xegVjiPqoxe~iPwqk{h$leh$oPqleqiPxeh$gZ{lqJmwxe(wwVh	h$oPqo)iPwVq/leq~iP}h$oPilexiPq{(jJl7h$~j	q{n$xeo
oPqps[1fi	iPwqy{h$J{jJ}ph;qoghJiiPq{}xegVjiPq1xe~iPwqoPqleqiPxh$g{lqm7wxew(wh;h$oPqoiPwq{xe$w	iP}h$o=i
lexiPq{jJlh$~6jz	Vq{Pnxeo)oPqpsqoPwVjJllh$gVoPxepq{)iPwqE$qgq{(jJlxeGjiPxeh$gh$~ffiPwqE{h$leh$zoPqlqiPxeh$g
{lqiPhyV{h$J{(jJ}oh$g	ijJxegxegV6h$gVo=i={(jJxeg	iPofmwxw)pqljGn;oiPwVqoqleqiPxeh$gh$~3y{xe}xiPxqh$gVo=i={(jJxeg	iPo
jJo~h$lelhGmo/iPwq/leq~iP}h$o=ilexiPq{(jJlh$~Lj	q{nm7wxewxeoghJijy{xe}xiPxqxegq;VjJlexinxowh$oqgts
hJ{6oPxe}yVlexexin$mqh$g	iPxeg	VqiPh{q~9q{iPhiPwxoLoPqleqiPxeh$g{(leqjJoiPwqPEP$f,sg
G i={qqiPwVjixeohJ|VijJxegqp|;nVoPxeg6iPwq{h$leh$oPqlqiPxeh$g{lqxoGjJleleqp ,V i={qqJs
uhy{hGqiPq{(}xegjiPxeh$gh$~leh$$xey{h$J{(jJ}oJoPxij|Vlq~9giPxeh$go[~{h$}J{h$gpjiPh$}oiPhLgjiPf{(jJl
g;}B|tq{o	GjJlleqpleqql})jyyVxg$o,m7xelel|qLVoPqptsqi7pVqghJiPqLiPwqLq{P|{(jJgVp)|jJoPqh$~s
 	[9RvAf V; $1V9	~9hJ{ffBxeoj~gViPxeh$g[~{h$} 
)
gVjiPf{jJltg	}B|tq{oGs

iPh


Rleqql})jyyVxgxeoqAiPqgpqpiPhgqjiPqpJ{(h$gpjiPh$}o|	n?[$ DsqphghJigVqqpiPh
qAiPqgpiPwxeoghJiPxeh$gjJleoPhiPh)h$go=i={(jJxeg	iPot|tqGjJoPqffiPwqn{(qy{qoPqg	iiPq{}xgVjiPxegjiPh$}xejJiPxeh$gVos
hGmqq{GghJiPqiPwji7iPwqy{qoPqgVqh$~h$go=i={jJxeg,iPoxegj;q{Pnxe
g qgqoiPq{}xegVjiPxh$gtf|tqGjJoPqJ
~9hJ{xego=ijJgVqJVjpq{x$jiPxeh$g)rVgVxiPqln~jJxeleox~3jJggVojiPxeo=rj|leq7h$gVo=i={(jJxeg	ixeooPqlqiPqpts


	fffi
 f{B}qiPwh	pmxelel|tq|jJoPqph$giPwqgVhJiPxeh$goBh$~jJnAlexexinjJgpzjJqyij|Vxelexei5n$mwVxewj{q)oPqp

iPhwj{(jJiPq{xeqj6ljJoPoh$~iPq{}xegVjiPxegy{h$J{(jJ}oms?{s?isfjJgj{|Vxi={(j{PnjJgpiPwq{h$leh$BoPqlqiPxeh$g
{lqJ1{(qo=ytqiPxqln$s giPwxeooPqiPxeh$gZm1qk{qGjJleliPwq/pVqrVgxiPxeh$gh$~jJn;lexxi5n$7jJgVpoPh$}qoPq~9l
{qoPVliPo~{(h$} kj{(wxehJ{(xt$$(m7wxeleqjJqyij|Vxelxi5n)mxellV|tqLpxeooPoPqpxe"
g !fqiPxeh$$
g #As
7yijJgpZqq} $;o=iPpfniPq{}xegVjiPxeh$gzh$~	oms?{s?isjJgZj{P|xi={(j{PnoPqleqiPxeh$g{leqJs
uwqn)xeg	i={h;pqiPwq~9h$lelehmxegBqleqjJg	io=nAg,ijJiPxghJiPxeh$gts
 	[9&%tv(' *)$fA,+.-./,
-A*03  y{(h$J{(jJ} !xeo21GD4365e2575$	tx~
)
~9hJ{jJlelJ{h$gVp/xegoPijJgqo98 ;:=<2>2?2?2?@>A:6B h$~ljJoPqoh$~ m1q6wVjGqffiPwji 8 DC :ffE wh$lpo~9hJ{
jJleG
l FIHJ
 HLKos?is :ffE xo7ghJijh$gVo=i={(jJxeg	is xeo621&xe~[iPwq{q6qfxeo=iPojleqql})jyyVxegVos?is
xeo7jJn;lexms?{Gs?isfDs


~jy{h$J{(jJ} xeojJn;lxeJiPwqgjJlel3J{(h$gp	q{(xeqo7wjqh$glnrgxiPqpVq{x$jiPxeh$gojJgpEwqgq
iPq{}xegVjiPqJsu3hqAiPqgpiPwxeo{qoliiPhLgh$gA5J{h$gVp6;q{xqo$iPwq~h$llehGmxg7gVhJiPxeh$gh$~|th$gpVqpgqoPo
xeooqpts
 	[9&%tXAM /ON;f   PON  -Q) 3qit|tqBjleqql[}jyyxegAs7
)
|th$gpqpEms?{s?is?xe~3~hJ{7qq{PnEUHVFWHXKViPwqLoPqi

	Vq{PnSR

YR E [Z	 :ff\E  :6\E xeojJ{h$gVpxego=ijJgqLh$~ :6E]

^`_*a

:=<T>2?2?2?T>A:6B xeo

fib6c.d
egfihgjk=lcnmfffihoqpfrdghSdgsutvlhwlc.owxzyGdDjwfr{$b6c.dgjgc.ogm6|

}r~ww}iAO



 OA}r2AwO`qnn
n(}2~("qnnn2 9nOn222n(@"AgO4OLO.2r}r
v
n(qOO"(Jqwnn2
nAuwO~qnizwn}iAn(}iqA}rqn~6YTY2Qw2A}rqO~ww}iA=O}r(O
 n62q(~(q.Aw}r~(2~(nin
2~nOGnq*nwffAIAw6q~A~A}rw}rr}iq.wqnwn(}rn.n~AATOw~A}rn
 wO~I2qn~(wA}in2A}rqwOw(O}r"J2quwrA(g`OA(}rTA}rq7`(n}rO`}@qq`
}i(~A2O(4Or}r29An2qn2nvqA`u}rgA}rqUYTY2gO"Ag}i`A~A2r2A}rq"(nrO
4Q*g&rAO*gqg@n wQnL.
QQ*z
nA$}~2`zgqQY@Y2}
Or}iA~Iqw@nn`}iOA}qn~}rJ(Iwn}iAOw(qO`O}r~IzDq
}rOrnO.gQw(}r2~
(A`u}rgA}rnUYTY2wJ

 
g*
i29`SOL2 wAO qg Tv;(SVAO.g(2w
	fiff.2 O2`
gTnA(qv 4 (Og7OO 4(O.gA2wUOgu6n
 
g*
[2 `J4zgQwAqO	fiff.2 ff.A!q"J# O$wQ%&%'(")
* , +I 92 .-/r(0%1%32 * (+42OO2`52w76 v(O.gA-/r(8%1%w 9 "92`zgqQ:
`q  n2O(2~<;
>=4On?;
A@}iqrrT9~IAwA(}rwA}rww(qO`O~2q}rn2}n}iAO.2r}r
n(qOOu~On AwOO.2r}rJw(qO`O~"QwASgO~4ww}iA4qgTn7`2}rOn qni}r}i
}r~9qnnn299OA}r2Awvn2Sn2A}rq O~vww}iAO}rr(}r~9O~(~An2  w2O(2B;
A@un
2~nO
nqrwOv}rw~(On2OAnn`qO`OC
HJILK8M
D !EgGF
I T!EDQNF

I!ED8M
I Pw

O GFQM

}r~A(}rwA}nugqnnn(}ngI}iI}r~=nOvO
2r}O
}rnw}rnUr2ww}rnOWn`T.}rnzO
2r}2}i}~W(TA}ivn(
22~A~20R v(2An`TOn
A TSV
 U.(n(?
 WXSv22O(AOJ@qY =4O$,AnO`qnqLn(2~A2Q(A}rq qUO(}qn~A2(nn}Qn2~4O
2qn~`nA}rnJr2ng}rnq~2
 w9qrr@}rn~(2A}rq4}rrrn~A`A2~6n@O}A(2~AA}rnw(Owr2 }ruwqnuqwOAqn}r=(TO~(qn}rn
TO$O(O}r22On}ruwr22A2O~vO"O.2r}rn`qO`O"
[Z]\4_^a`'bgdc8efhg5ejilknmpogle"q
 wr
Lr.~O`r }r~O`JnA}rqqI4n`Owr2 }r n`(OO(O(~Qwz
 n
qn(}u}iA}rOA}rqw~} "~(}rgrUO(r~A2Oz}rn~(Ow2v}rr~A~Aq6@tsuq`?v9`u2qn~(}rn
~A}ruwr=(~(}rqq6An}r~9n(Ow2 
wUO2(wOA}@xtxq`  n(z(A(2wQ!r
~zyn8{@'|*On
A(2zn~} }D(2Qvq~(}iA}rqn
~ '
 Qj
 uq u(wrOU wr
L rTO22}rAn@OnOAnwQ! r"O
q$qnUqGAw2~Aq~A}iA}rqw~2.Ow$}iITO$T2(q qnq~(}iA}rqAOnOAn@  nw(Owr2
2qn~A}~A~9q~A22}.}rnzq~A~A}rwr2qwqA}rqn~2D}OigAwq~AOw(O}rn2"(q An}n}iA}O~A}rAwA}rq

`O(}rn~A2
n2n2qq~A~A}iwu@2~2=9$ OuwrqOS}rn}iA}O~(}iAwA}rq}r~q}i2}r
}rq(zq
@O~ r
}D@
 xOq}i2~92On~(OQ(n(2~(2(A}rqzq.An}~n`Owr2
TOn~qw(Onq~
2qnn}rA}rqn~2h
 v9(O(JnAAnn(Owr2 n~A}rwu  AAQOn
 vvT2~2~A}iAgA}rq"TOr2wrn~
P

fi~1

b
a
p

c
q

r

0~t1P5t/~L(Y!~

(  lLLhttP&~L!pt[YLL.YLLlL~t!7l!.L!.,j
tP[l [L !Y"Y>~L  ~5~~lL~t  :!YY [L Y#>~L  ~5t
~L p YLY T /"Yh>LPa~~!~lL~t  a!at~5tajatY[
 YY [L 5!Y5"Y#~L  ~L~lLt  0~Y~tP[[t[TY1l!#L!j
L~~L#YL7L~~LY4LLlLtjY   LpLLlLtY!Y~Pt 
j!"YPp~5L5: ? YL?twLlj5!!L!lL~tjjL~LhY!wYj
Y!Y~YaL.t~~<~fi&08j:


 
 

% 

? 



Lt
N 


:'P4	.ff
fi
ff
  ff
 	
 

 t   !"ff
 
$#
:'Pff
 j' &) ('
 +*& ,/.00 &/)(213&4
:ff
 ' &fi
 5 0 (fi
:'6
 t
 .87-94) 15
 ' &8') &8
 '
:'6
 t
 .87-940 (') &8
 fi
(;< :  /' &>=
:'Pff
 j' &) ('
 &/.@1A&
 B
:ff
 ' &fi
 5 0 (fi
C 78 95*78$ '0 &/) (fiD &/.D &8 '
:'$:ff
 j' &/D (') &8
 E=
:'F 75G
 ,H.' &/ IJ') &8
 K
:'$:ff
 j' &/ 15
 0 I8') &8
 >=
:'F 75G
 ,H.' &/ IJ') &8
 K
:'$:ff
 j' &/ 15
 F Mff8') &8
 '
:'$:ff
 jF M 15
 0 I8') &8
 >=
:'O 
 .87-90' (') &/
 K


8

6

8
P
J



.
0


'
/
(

D

&
8 E=
C
78- 95*7P '' &) (fiA *- ,/.0' &) (2QO') &/ 4R=

SFT6U

fiVXW/Y8Z[]\^`_baGWcd[]\He fH[gY\hYikjlaG\ffaGW/effmon>Y^ff[gpVXW/Y^W/ecXq

rtsGuwvffx8x6y8z{G|}~'d)8 
 v8-}ff~-vffx~'XD)8E
r ruh|/8H~~)J-G~D@DH-~Ox>/ff2)B
 v8-}ff~-vffx~2DJ-)ff/
 v8-}ff~-vffx~D@DH)ff/
 v8-}ff~-vffx~6x>D/ff)ff/>
 'b v5z~' O50O20@05 ffg80 5tffFt't 8)'-)))'>A8uF))FuF)DFAOuF 
l
  $tAff3uF'5t' uF'5A)-uF  5'O6G  ggO r ff     '/  0l00b O 5 0O 
   g r 0JOOgffOE0bg]0  0g ff 0g   'O  0   v/G}" O'']OE"O    gEJ '0]ffg
g  ]GO0]  0g      0ff'O  0g  |-H8H "O  O      0g gJ '0]ffg w ]GO
0]  0g  
  ff0  0 ggt"g  gGOJ  g   "F"5  '    0'   G O50O@0lgO  0
 E0gg0  50'"g0     ] $gb5  gg) u   g s8
5uO-s8
  6AdJuOGs8
 /'t58  '-uO-s8

ff8G>  uO

 "  O r
 "  O 
 "  O 
s

g  g >05'6 FuF
g   g >05't'5 FuF
g   g >05'-'55 FuF
500"0 

 5F-t>  uO8"  O
 3/3-  uO-"  O
"gb  )$0FO00  	ff
fi`g  /Ogg     E 
 ff'O+5F  0O  0   O)0g ff8ff'O00  8 k  ff @J        'gO  KO 
ffg0Og  0'  5g)  ffO  0  O)0g  "Off'  'O  ggg0g]0   0g0g  
   X0lO  0'OO     0  d   "5'  g"!O   0#  '  v/G}~Fvx~z%$8
   0      ffOOG0  >]0  -}'x &)(   ']  0g X]0  "]0  )@+*	, .-0 /
 
2 1/GA , u)uF
3l500  "0g4   0    6
 5     bO G 0g   bff]0  g  '  0 

798:;<;>=)?A@ B	C+DE=GFHJILKMHJBNPO
D0lF8g  X0O0g   @  G0OO$t0'k  0g  QR          0])  00OO0g 
    `ff'6GO`8    d0o50g `   /OggO]  50g `   OOff  ffgg] .S 
TVU   'O0F JXr W W8ru g  0  +5bff'6/g  0'k  0g  ffQMYRZ       0 U ' g  0OO0g    g 
DK0ffgo0O0g   @$'O  g0ff50g   0  0"]00    0O  'O  g0+'  A  ''5' 
Xr W W [GuF\
 S /OggO]     OO  ffg] ggO offgff  g40+ g6"g  0O0g 0ff'6/  
 5'l  0g  08 g5F6/g  0' g  0g $ ]
 QMYRZ b      0 U '   0OgO0g   g 
D5  0)    0' g  0g    O  g  wff'     "g0 'O0JO$00 U ' g 
0OgO0     S ff    U   FO0' DXr W W8ru g-)F   O  050    OO  ffg'     E g

^`_2a

fibdcfe>gh)i"jfe<i
k)lnmpo"l korqtsunqpvxwyl kzmp{)v|q`un}~vxl k)w)o"mpo"l kz)qpvxwzmpldw)vk)v|unx"or)l n`un}qxZv>xv)mmp{fumxlnu
nl )kfw~o"kfqmunk)xv#'xxx'p9~l Muxrun)qpvnmp{)v#mpvxqm V 9JorqAvln}vxwl k))kYmpo"mp{)v
)qpm"ompv`unM  {)o"`{uno""qx{)o"qo"qqp)xo"vxkYmqpo"k)xvnw))vmpl~mp{)vAl "l qvx"vxmpo"l k\)"vnfrompv`un"q
unmpv  o""	k)lnmsZvtqpvxrvxmpvxwAlxl }~f<mpv ])uxrunqpq4l }~lw)vx"ql |){fvvt'un""vxw>p' "Xp
''o"qfqpvxw{)vl ""l  o"k)6k)lnmpo"l korq)qpvxw{fv~p`.'.nzl +unko"kmpv)vmumpo"l k\mplu
qpvm#l `vxrumpo"l k)qx<w)vxkflnmpvxwds~J 	forqmp{)vqpvml ffumpl }ql M6{fu'o"kfmp{)vxo4vxrumporl k)q4o"k+
6Yff2Ln)f> Y<Y>f vmz svmp{)v"v'unqpmqpvmPl Avxumpo"l k)qqxmxmp{)v
vxrumporl k)ql lxx<p`o"k)6o"kyk)vxJumpvxwumpl }quvork+Aunk)wyo"unkvxrvx}~vxkYml lxx<qorkmp{)v
{)v'unwl +u~xun)qpvnmp{)vxkun"mp{)vvxrumporl k)qtlYxx<o"k)o"k\mp{)vsZlw<dl Amp{fumxrun)qpv|uvo"k+  vm
svmp{)vqvm#l 9xun)qpvxq#o"k  {)l qv{)v'unwxl kmunork)quvxumpo"l k\l }z #l  u}lYwfvxZ
l ffo"q%>x. 2po"M  Ap o"qu|}lYwfvxl ]2|M.`

6Yff2LrY n<)
  < M| vmfsZvu6"vJvxff}6u)fo"k)ln unk)w"vmtdsvunk
o"kYmpvp)vmumporl kl ||o"q` E	 ' ff
 
fir fi fi9n~or9\orqtuqpZvxxoun"o xvxwz}~lw)vxl unk)w
lntun"nl )kfwo"k)qpmunk)xvxq   xxxXp  l 9xrun)qpvxq#l 9  v{fu'Jvmp{um| 	   ){)l rw)qln
  
 %9 x  !#"%$& ( '2
 )  2`A o"q
vJvp
  zq'mx)  orq+kflnmuxl kfqm`uno"kYmx  {)vv 
J`` 	E	 xo"Momorqunxxv)musf"v  'mxfqpl }v#"vJvxM}6u)fo"kfunk)wdo"kYmpvp)`vmumpo"l k

* u )`l n`un} o"qdunxxv)mus"vnmp{)vxkvJvpnl )kf,
w +Y)v{funqdl kffkfompv J. -0/	1 w)vo umpo"l k)qx
{)vxk)xvommpv}o"kfumpvxqx~ldv>mpvxk)wmp{)o"qvxq)mmplkfl k 1 nl )k)2
w +Y)v`o"vxqxunqlnmp{)vun>x"o"~'unqpvn
mp{)vl ""l  o"k)k)lnmpo"l kl sl )kfw)vxw)k)vxqpqo"q4fqpvxw
6Yff2L 35 4<7 68-<>Y,
 976f	 :	 vmdAsv~udrvJvx}6u)fo"kf6unkfw"vmsZv~u qvxxorun"o xvxw
}lYw)vx	l ff|< ;=+Yfvp
 >?   xxx2p  orq4sl )k)w)vxwz  'mxff<<unk)w <+o"lnvJvp@
 A
B > C  Y  D 4  D xxx2p< D nl )k)wo"k)qmunk)xvl   xxxXp  unk)w
    D  xxxXp D   
o"qfkfompvn

;)m+unkfwffvxw<vxqp`{)o)l'Jvmp{fumlnunkunxxv)mus"vfl n`un}\vJvp|sl )k)w)vxEw +Y)v{unql k)"
fk)o"mpv4w)vo umpo"l k)q  XmxJmp{)v49`l "l qpvx"vxmporl k|)"vunkfwk)vxJumpo"l kunq]fk)ompv4uno"")vn{fv4xl kJvqv
l mp{)o"qvxq)m4{)l "w)q  {)vxk F{fun(k Gqxl k)qm)mpo"Jvk)vxJumpo"l kdorq)qpvxw Hu{fo"lno.I KJLJLMJ`O Nffoqmx  v
ln}unro xvmp{fvxl k)xv)ml mpv}o"kfumpo"l k  'mxfmp{)v9`l "l |qpvx"vxmpo"l kd)"vn
6Yff2LQ P>	 /YS RUT<	 7 - K -0V976) : W -)X
 8> <K  ;Y+Y)vzorq~[ Z[ \ %x`  ]





.
0
U
/
1
  'mx~to"un"]ompq
wfvo umpo"l k)quv|fkfompvn^
 ;fl n`un}  o"q| _ Z[ \ %	 8 ]~orAvJvp
nl )kf
w +Y)vo"q"vxrm 1 mpv`}~o"kumpo"k)  'mx)|

`ba d c28 e Lg fih xffj
 `|nGJ`` E	 xf Q ] n nd ' >j| 7 k>m
 l!kfo nUfimprq>G! snxt n
fK -S/ \ `< Zxnu
 >  `nfEn<nv nmp7 k>Yp
 l!kfx`En w9)%. fi
`ba d c28 e L x=h xffj
 `| [ Z[ \ %x`  ]f5 ] n^
 fiprq<y q>x~5 zJr6 ! sn'LJf ]zB {


~
n~ >p' "Xp''> Z  fi fiy|u} rJ E x
 
fir fi fi])Zn O}  ~ Z ! snxt n^l!kfxt n
>{> p7 k>Yp 
fir fi fiA)n ,> _ Z[ \ %J ]fi
* kmp{)vl "rl  o"kfqpvxmpo"l k6unk~unxxv)musfrv)`l n`un}mp{fum+ln`}un"go xvxqfffrunk)k)ork)to"k|mp{)vsf"l5 >q  ln"w
o"q4 oJvxkL

tK

fi8dd@U0<SLSddffuUUmddd
I [Idyduy[%[y(WffdW
L0Q0Edg7005 gQ8ffO7t(7L0	5g0550@(!!.W5LVL2Q!0!!L
(L55dm.U!t705y7Q0@50@5g7!L0LStW5LX52d7!LSLStW5gL(77
gLSQ00Q7g!j.7ff5LU!%05  L005Q055b7gL7Q5085Q 
WdQKL7	t	yyoty55Q7  U[[7 WdU
 		!fi ffQt UQ <d ^ L U.[_7 
8
 LbyS t5d#QL  L[_7 7dQ ^y LO[<fi WdU L& U 	m ff. 75 !O0yLK0
0QL7o7#
 "%$&(')')*+'(,=yLK50W00tU7Q(%!L055Lb75!705!L500tL7t7
 7U709 8Lt70d050yLK0!g70Q!!
- $./02134.5$ 6<880

c

b

b

a
p

c
q

a

r

p

LStfi:g700b50<;O85=>7Qg
?

@BABCDFEGBH AIKJLE(MN@MOQPCD)RTS
E@QC@BU2JVN@QW%R(M
PU XQCPYE)JZN@BW[ML%EBR)M
@BABCDFE)JL%EM\N@MQ]N@QW^[MOQPCDR_
@BABCDFE)JL%EM\N@Ma`bEMY]c^RdS
PU XQCPYE)JZN@fiMLER_
@BABCDFE)JL%EM\N@Ma`bEMY]gfhi@kjgf%h@E^RTS
E@QC@BU2JVN@l R(M
m IUIBn%UA2JVN@l)Ma`bEBR(M
PU XQCPYE)JZN@Fl)MY]gfhi@kjgL%E^R(M
@BABCDFE)J]gfhi@kjgL%E^[MN@fiMY]aN@ljo`%bBEi^fiMfh@pEBR_
E@QC@BU2J]JVC2MqFl R(MJnrMqQsYR(M JhMqt%R^pRuS
OBvK]owKMxfiMAfiMa@QHwfiJVCYR(M@BHwfiJn)R)Ma@BHw[JhR^
M
IpUiIQnYUA
J\qFlMOFR)M
IpUiIQnYUA
J\qQs2MOFR)M
IpUiIQnYUA
J\qBt
MOFR_
zIpUiIQnYUA
J\LfiMQ]{L|jo}B^R~S_

Z?
? 
e 
?


y ?

Z

q

r

fi%BY%Q

VpzpiQY
\fiQgkg~
piQY
\fi

fi+YYY\Y%rT<u\Y4\T QF) TY\9 p YZ
FQ%YZ\+K\Y\%\\%\4K+%Y44Y[\%YZY\ B   \Y
\YYY\ Q Y\\%YcYZu%T+\YQQZ\\
\Y%Y[akY\k9Yc%\kk\YZZY\%Z\\%YkY
\{\Q%\YQ%YZ\|Y\ p cVpYYY%  
Y\<+cY%QZ\TYZ|%ZQuY\YZ2%\Y%Y\\Y
%\%\BY%<\%Q%QZ\Kr%\< p %V \\%\\Y
YZY\%\Y%%Y%QZ\9r%
 Y\YY%gfiY)+
Fr%\%Y%)fiY ()(\%
\%[Vi (iZFZVpZi p24  ZZZ  Zi Y4YY\4Y\
\Y\\Y+Z\)Y\+\YB{  p24 

c\%
Y ()([+\%\Y\YkYYVFF|Y%[Yr\Y
\Y\%\\%|\F\YYVB\\
 F

 Q
 r%\    )T
    2V  (
 BF)%\  Q  [  
 p
 BF)%\   Ycd 
  F
 Q)\(   BQ
 	ff  fi 	ff  %T
  (
    2V 
   B(




B

%



2V 

 BF) 	ff  g%[  Ya 
   Bifi	ff  B
 p
    2V2   (r Y(  ff %pu
K K
 fifi  
 fiVY(  
 fi))  
 [ ff


piQY
   F)
piQY
 2 F)
piQY
 
 F
ZpzpiQY
\fiQ{|oB~
VpzpiQY
\fiQgkg~
piQY
\fi

FFYYuZ Y((+(~YY\Y\Y\QYYZZ! Y%Y%") B   )Y
YVB\Y\Z\\u B   Q\ZY49\YQ[\YYYY\$#fi
Y\YYFFKY4\[\ \KY+Z\)YBFFr\fi\<
Yi (iZpZZi   Y \%FF|YYrY\Yr\YY
Y%Y 
% '&'()&* , +(-Y %/.0% - %
%2134/3 & , +F %/.





% 3 * 46571 , +(-p98B: %;.<3

 3>=

 4 *?  &@ fi8 7ACB[D
IKJLJ

FE  % + % D

 GDdHD

%8 %



fiMONPQRTSUWVYX6N[Z\RTS]^R_PS`PacbdX6SX6N]egf7PhUR_iCMONPUN]ZOj

k lm6n6o7prqsrmutwv,x7yzy9{;|k}<lsrlG~Hv9k xk~<r|G~
L6d[['; vfin[yr|Kyvy|KyvfiLyh6|
k

 Gyh;yh$

 y9/ymuyls97vfin|KylsKv|KylsKvfi|K 

{   W   vfi{/| [[  
 [ lsrl g[ u
    
   
    L6     
 n/mv  vfi{/|  |d [ u      `O
  /G[   9  [
 [[
' 
 vfi{/|r

x      k x>k [[    [G6[   9  >[[\ O   6  
6 _    
v,lsLl\n6m6[v L vfi{/|>  ||d   [ kk[    _ [
vpyyKp|   [
 [ F     



v

p



y




L

K
y

p




|








[g _   [
[  [
[ [  [$     [  /        
}



T   T  

\  _9_
}

T  
 T  }

 

7 

 lmn/o7prqsLmtwv,'yOyK|
 

 l
m6n/o7p6v,'yOyKHyK|
 



}

  __ }

 pln/l  v,x|Ck>x'

 t

 t'  mv,xyz[|Ckz 

g     [x`p  lv,z[|) 

 

   `  L6c       
    
 ;  F7 w
 6  ) T c}
 t  t'  m   [
   r h|Ky9)h|K  6
|  [ 
 [  
  sLt7v hu K
  
   

9    
7    [             [ kk   [9[ _    [g      H kk     9 u        
6   
 

7u

k l
m6n/o7pLqsrmtwv,xyzhy9{/|kHy

vr|

k lm6n6o7p/v,xyzhy9{y|kdy

v|

k lmn/o7p/v,xyzhy9{y|k 
   77[
 [  [ F


 [      
7d  ;[

  [[

k {k

vfi6|

 H   



   [

lm6n6o7prqsrmtwv,xp6yxl)y  n6o7|pln/l  vpl  |Kylmn/o7p/v,xp6yKpl)y  pl  y  n/o7|K
 h|K  

vr|Y H   

H  

k l
m6n/o7pLqsrmtwv,xp6yxl)y  n/o7|k 
 [d   

k}

pln/l  vpl  |KO [

pl 





k l
m6n/o7pLqsrmtwv,xp6yxl)y  n6o7|k 
 [  [ F

  [[

lm6n6o7p/v,xp6yKplKy



)n/m6[v L v 



 v  pl  ||}

k lm6n6o7p/v,xp6yKpl)y  pl  y  n/o7|k

   [

py  nKlk nKlp|\

pln/l  vplKr|Kyt
  h|K\  

[

k2pln6l  vpl  |k

 t'  mvpl)y

v|Y H _  



pr|Kylm6n6o7p/v  ;
n Klk xpyKpl)y  plK;k  py9nKlpu|K



p6y  n;Klk n;Klp|k 

  

k l
m6n/o7p6v,xpyKplKy

	
fiff

k2pln6l  vpl)r|ky

 [

fi

!"$#&%('*)+
, -/.*0132 %54 2$627-698:;2*6< 0=-7, 0=->2@? ) ,AB,DCFEHGIEHJIGI. % 27-K$698:;2 ) ,ML
h 2VGI- % 8:;2 )j
N OPP "$Q>RTS>UVSXW , YZ2@-90-9G % 2@-[K ) 6CFEHGIEHJIG@. % 72 -[K$698:;2 ) \^]_UR@ 72 -[Ka`cbedgf O S 2@-[KH`i

- wy=I0*.*l % b^mnG7o % <D27-K, 8:;2@? )9)_z
Qk" =I0.$l % b^mnGfio % <D2@-[K, 8:;27? )9) Yp=[0.*l % bqmnG7o % 8:r2 )9)s K t
j UR@u@R -rvx
-9v-Fw{=[0.*l % bHmG7o % 8:r2 )9)\e]_URIkR@|"kR d
, -/.*0132 %54 2$627-698:;2*6< 0=-7, 0=->2@? ) ,AB, -/.*0132 % < 0=-@, 4 2@?}627-6<D27-[K, 8:;27?}6k0=->2 ) ,ML
~ ]UR P k""$3|"_S>UR"S>URIu@! O QkR@Q_"$xFQQ>#a!Vfi\
3I$e/}7

 S>U!QQ>R@uIS>"$ d RaS9k" O u@R^ykQ9S!S>R@VS>!"$ d u7R@ OP u@u@R P Sk f S/ d "$S>URq"S>!"$QT"$
uIu@u@S/yu@u@R P Sk f S/$\_RQ>U"  S>UVS OP u@u@R P Sk f S/ P k"7R@Qn#a"kR P uIS>!u7S>""$
S>Uyu@u@R P Sk f S/a|" P k"7R@S  S>RIk#aVS>"$a"$xQ@\
  N R@uIS>"$\ K Ru@!#S>UVS3X"kRIS>" P k"fi*RR@S  S>RIk#q!VS>"$"$*gg d Sg!QQ O u@R@S
S>" P "7*Ru@u@R P Sk f S/"$tS>UR P V>S@"$x*yu7R@qxF^uIu@u@S/"$S>UR_kR@Q9S"$S>UR
P k"$[#y\3RIS O QR[ P !!HU"  RV>k*RTS>"aS>UQu@"$u@ O Qk"$\Q9S d *ggQ P V>S>S>"$R@
S>"S  " P V>S>Q7+ OPP RI P V>S d Q@u@"$QkQ9S>"$Fu@! O Q>R@Q K ) 6@L@L@Lfi6k ) d "  RI P VkS d Qk@
 d u@"$Q>Q9S>!"$S>UR^kR@Q9ST"$_*gg\]_UQ P V>S>S>"${QQ O ukUS>UVS"kR@!VS>"$RIR@
  "u@u O QX  \n]_U!Q"$ P V>S>S>"$a"$ P k"$#QRIR@ f  P S d VukU"
 !#aR@Q>Q>F% Kfi$ )Q_"$!"  Q7\
N 7S>UVS>@$5}Q5ka!3S_"u@u O kQS>URXUR7y"$VSR7Q9S"$RX"$S>Q_u@! O Q>R@Q d
S>UtVS5I9$QX5ka!S>QR@!VS>"$Q_RIR@\
F|tx5>$x*g  P k"$#>It9$9
 A  d !3"R@!VS>"$RItR@"u@u O kQ_!  \

 d R@"S>R@ f 


N a
" R[S>R@Q  ! ZRIR@QR  kR@!VS>!"$Q P "$Q>Qk f  O Q>!nS>URkR@VS>"$QRIR@kR7
  \"Q>Sku@R d S>
U R P k"$[# fi+
 

fiff 

	

R[S>R@QS>UR P k"$#







  +





[R S d Ru@"$QkRIS>UR P k"$[#xFe" f SkR@k"$#   f TR@RIS>S>URS>RI[Q3RIR@
  \RTu7S>UQ" P RIVS>!"$ I>I[ d RIR@Q|"$"  @Q \


 F|tx    fi 

] UR$ @9@"!$#9*r t  d
R@"S>R@ f &%  d QS>UR P k"$[# " f Sk R@k"$# f q
 R@RIS>xS>URu@ O Q>R@Q"$   

S>URX!S>RIQRIR@
\

')(+*

fi,fi-/.0132465879-;:<132>=@?>1A.2B.CEDF792G79-/=GHJIK.L4G1AMN,fi-/.4-/=:fiO

P;QRFSAT;UVXWT;Y[Z\;S]K^fi_`WT;aN^bcWdRXZea;ZgfT;Z[aNWUhWdiQkj9Z\Vl;Z[TN^b<mn^<_hSU8Vl;Zpo;RXQ@qRrWsutGv

wx

PSATGWyy3z@\|{}ZJo;R)Qkj9ZJVlGWdV~||KJSUpWY[Y[ZgoGVXWdiGyAZWT;a6VlWdVeuSUpWYgz/Y[yASAY\WT;aSAT6a;Q@SAT;qEVlGWdVc{Z
lGWkj9ZpVQJVXWd9ZcYkWdRXZpVlGWdV8Vl;ZFV{QyAZgj9Z[ysWdo;oGSATGq@U<;UZ[aWdRXZFRXZ[yWdVZ[aizWY[Q@T;aGS3VSAQ@T\/TGWsZ[y3zJVlGWdV
]$QRZgj9ZgRzqRXQ@GT;aSATGUVXWT;Y[Z\UXW[zv

_k)bd\Q@]8WY[yW;UZQ@]8^fi_k\]QRZgj9ZgRzyAS3VZgR)Wyfi

Y[Q@TVXWSAT;Z[aSATWT;aa;ZgfT;Z[aSATJ\dVlGZfiyAZgj9Z[ysWdo;oSAT;q}Q@]>SAUKT;QVKqR)ZkWdVZgRVlWTeVl;ZyAZgj9Z[ysWdo;oGST;q
Q@]8xl;SUeY[Q@T;a;S3VSQ@TSAUSAsEo|QRXVXWT VpVQBZ[T;U>RXZEyAZ[]V"VZgRXsESTGWdVSAQ@TxP;QRSAT;UVXWT;Y[Z\Y[Q@TGUSAa;ZgRVl;Z
o;RXQ@qRrWs^
XL

v

t |NLK

t XL

v

t XLK

@

WT;aEVXWd9Zc^<_} ) WTGa @) x}l;Z[T^<_8Zr/VZ[T;a;U}J\/^fi_Lm6SAU}WY[Y[ZgoGVXWdiGyAZF{exRkxV[xVl;ZFyAZgj9Z[y
sWdo;oGSAT;q    9 \SUWYgz/Y[yASAY{xRkxV[xVl;ZEyAZgj9Z[y}sWdo;oSAT;q     \iG>V^SAUT;QV
yAZ[]V"VZgRXsSATGWdVSAT;q/x
 Q/\Vl;ZUVZgoGU{ZWdoGoGyASAZ[anVQG9WdRXZNUX;sEsWdRXSA[Z[aSATnVl;Z]Q@yAyQk{hSATGqBaGZgfGT;S3VSAQ@TQ@]
>o>WY[Y[ZgoGVXWdiGSAyAS3Vz@\>VlGWdVhY)lGWdR)WYgVZgRXSA[Z[UFyAZ[]V"VZgRXsSATGWdVSAT;qo;RXQ@qR)WsU[x
P;QRFWyZgj9Z[ysWdo;oSAT;q;WT;aWo;RXQ@qRrWsuJ\>VlGZrg)k";|J\>a;Z[T;QVZ[a;  \;SAU}Vl;Z
yAZgj9Z[ysWdo;oGST;q]$QRh	a;ZgfGTGZ[aiz J 
$|A

 x

" tG G@t>>|9>d  ZgVfii|Z6WyAZgj9Z[ycsWdo;oGSAT;q]$QRN^x

 ZgVNiZBU[xV[x

^^fi_	]$QRhUQ@sZ`^fi_k\WT;ayAZgV8EiZeWTNSATVZgRo;RXZgVXWdVSAQ@TQ@]K^mJx<^SAUpdG9Xr   	
;ff


fiES]Vl;Zc]$Q@yAyAQ+{hSAT;qJY[Q@T;aGS3VSAQ@T;U8l;Q@ya

 xh^ _ Zr/VZ[T;a;Uh
 xh^

mn	SAUhWY[Y[Zgo;VXWdiyAZc{exRkxV[x> 

WT;aN

 xh	SUhWYgzY[ySAYp{exRkxV[x>  
 xh]$QRhZgj9ZgRzNqR)Q@;T;aSAT;UVXWT;Y[Zc
v

8_kkQ@]WY[yW;UXZcQ@]^fi_k\;]$QRhZgj9ZgRz

 \

S]KSAU8aGZgfGT;Z[aNSATWTGaSAUT;QVhWY[Q@TGUVR)WSATV[\WT;a




S] 

  _g ! \;{hl;ZgRXZc  _kk !

WdRXZcVl;Q@UXZyAS3VZgRrWyAUWsEQ@TGq8_[+  {`l;Q@UZ

R)Z[yWdVSAQ@T;UQY[Y[>RhST^mnJ\

%

VlGZ[T n#" $x
oGRXQ@qR)Ws

SAUpdG9Xr   S]Vl;ZgRXZpZr>SAUV;\;	WT;aEU[xV[x>^SU};o>WY[Y[Zgo;VXWdiGyAZF{exR+xV[xK>\;J\>Gx

&
' iUZgRj9ZFVlGWdV8izVXWd/SAT;qJ]QRVl;ZpZ[sEo;VzEUZgVQ@]Y[yWGUZ[U[\/{}ZcQiGVXWSATVl;ZpQRXSAq@SATWya;ZgfT;S3VSAQ@T
Q@]WY[Y[Zgo;VXWdiGSAyS3Vz@x(FZr/V[\;{}ZSAT VR)Qa;GY[Z`VlGZeT;QVSAQ@TNQ@]dG  /fi )fi+*[Gg-,x
$|/.


" tG103254#6/7698:2;w   ZgVe^iZ>o>WY[Y[Zgo;VXWdiyAZJ{exRkxV[xfi\

 _ +  x}SAU8>o>iQ@;T;aGZ[aS]]$QRhZgj9ZgRz
 <)
 =3> ?  $@ $@ _ +$@

;

WT;aB\KWT;a6yAZgV

Vl;ZcUZgV

SAUWqRXQ@;TGaSAT;UVXWT;Y[ZcQ@]<

WT;a 

 @ BA

 A  @)C 

SAUfGTGS3VZ\>{hl;ZgRXZc @  + @ C WdRXZcVl;ZeyS3VZgR)WyAU8Q@] @ _ + @ED _ {hl;Q@UZpRXZ[yWdVSAQ@T;UQY[Y[>RhSAT^

FHGIF

mnJx

&

fiJLKMON5P1Q!RM#Q

SUT+V:W)X1YZW\[]V_^]`1Vab[]`cd[ec:f!fhg7ijlk#m7noX1YZW)p!q:cd[]p!V3T^V3rBc:TLs#t1n	uvV3sT1X1YXxwys1YZW)zxcdW)Y|{T1p/[]Y:}BaY^]`c:f!f
t1W)VlqIY[]`cd[c+gIijlk#m7noX1YZW)p/q3cd[]p!V3T~V3rc:Ts#t1n	uvV3sT1X1YXw5s1YZW]z~V3T7[)c:pT1^|V3T1f/zs1t#n	uV3s1T1X1YX~wysYZW)p!Y^
c:T1XaYx^)`c:f!fc:^)^]Vypcd[]Yaep/[]`Yc:)`X1YZW)p!q:cd[]p!V3TV3r\[]`Yw5s1YZW]zc~X1Y^)YT1X1p!T1)`c:pTpT[]`1YxaYf!fn
rEV3s1T1X1YX~^]YZ[;V3r$tc:p/W)^V3rs1f/[]p^]YZ[]^eV3rTcd[]s#WHc:fTysuvYZWH^aep![]`[]`1Yf!Y-#p!V3:WHcdt`1p!V:W)X1YZW;Yc:f!f
[]`cd[c#^]YYY:O/Y^]`Vaep/[]:l3I3p!^c+s1TV:W)X1YZW)YXV3f!fYZ[]p!V3Tp!Ta`1p!)`[]`YTysuvYZW
V3reV5s#W]WHYT1Y^V3reYc:H`Yf!YxYT7[p!^V3s1Ty[]YX1V:W)c:f!f/z3$cs1f/[]p!^)YZ[V3reTcd[]s1WHc:fTysuvYZWH^;p!^c
rEs1T1Z[]p!V3TrW)V3[]`Y^]YZ[_|V3rTcd[]s#WHc:f$T5s1uYZW)^[]Vp/[]^]Yfroh3p!q5p!T[]`Ys1f/[]p/tf!p!p/[zV3rYc:H`
Tcd[]s#W-c:fT5s1uYZWe\`YT[]`YV:W)XYZW)p!T1lBV3Ts1f/[]p^]YZ[]^\p!^X1YZ{T1YXc:^[]`1Y[WHc:T1^]p![]p/qIYfV3^]s#W)Y
V3r[]`1YW)YZtfc:YxYT7[V3rcTcd[]s#WHc:fBT5s1uYZWeaep/[]`c:Tyz{T1p/[]YTysuvYZWtV3^]^]p/uf!zLYZW)V7V3rTcd[]s#WHc:f
T5s1uYZW)^[]`cd[|cdW)Y^]c:f!f!YZWes1TX1YZW|OpT1Y_p!^aYffnorV3sT1X1YX[]`1Yp!T1X1s1YXV:W)X1YZW)pT1lBp!^
c:f!^]VaYf!fnorEV3s1T1XYX1V:W^]p_tfp!p/[ozaY;^]`c:ffV3xp/[p!T[]`1Y;^)Ywys1Yf[]`1Y|^]s#u^)ZW)p/t1[OOrW)V3l
p/[]`+c:Ts#t#n	uV3s1T1X1YX_wys1YZW)zOaYc:^]^]V5pcd[]Y|ctc:p/Wo ):  )/ff ):  I /ff| ):   V3r
s1f/[]p^]YZ[]^5ae`1YZW)Y|rEV:Wct1W)V3:W-c:c:T1Xc:TLp!Ty[]YZW]t1W)YZ[)cd[]p!V3TL
/ff )3  ~bHI 

): 
7 ff  l 


7 ff

): 
 


): 
ae`1YZWHY|   l   cdW)Y|[]`1V3^)Y;f!p/[]YZWHc:f^V3rBae`1V3^]Y|WHYfcd[]p!V3T1^V5s#Wep!Tc:T1X+ 7 ff 

): 
): 
p!^[]`1Yxc#p!s1V3r\ff
ae`1p!H`Lp!^u5zxV3TyqIYTy[]p!V3T~p!rff
p!^[]`1YY_t1[z_^]YZ[HH


Yc:f!f[]`cd[[]`Y+f!Y-OpV3:WHcdt`1p!V:W)XYZWV3Ttc:p/W)^V3rs1f/[]p!^]YZ[]^-;p!^XYZ{T1YXu5z ] L
o H p/~Yp![]`1YZW\;V:W\  bc:T1X    
\`YT+aY|c:Tt1W)VlqIY|[]`1Y;rEV3f!f!Vlaep!T1W)Y^]sf/[

  	 ff
 fi 
 fi  fi7
fi
fi fi fi #  $fi %   fi~

 I5dEO3dIHH 	 / 
 B1  ; hh
; d :
h55v d

h#  3 g7ijlk1m 7 - d3
Z   
 H 	: #+  ~d 3 7
 -Z
1o

   fi! "
  fi  
& fi '
()*+hYZ[),  .-  0/  uvY|cg7ijlk1mInoX1YZWHp/q:cd[]pV3TrEV:Wp!T Y|tW)VqIYu5zp!TX1s1Z[]p!V3TxV3T
/ p^s1t#n	uV3s1T1X1YXOc:T1Xx[]`cd[\p!rp![p^[]`YWHY^]V3f/qIYTy[\V3rcwysYZW]z2/435-uyz[]`1Y^]Yf!YZ[]p!V3T+V3r
1 []`cd[e0_
cf!p![]YZWHc:fvae`1p!H`+p!^T1V:[cV3T1^[WHc:p!Ty[#[]`1YTLo2#
/  H:  o0/635--  ):  
1V:W[]`1Yuc:^]Y;c:^]Y 1  3aY`cqIY[]`cd[| - p!^es#t#n	uV3s1T1X1YXu5zLc:^]^]s1t1[]p!V3T8
7 Vla V3T1^)p!X1YZW
3



:
c
1
T

X
)
^
#
s
1
t

t
3
V
]
^

Y
]
[

`
d
c
|
[
]
[
1
`

Y
)
W

Y
]
^

s
/
f
|
[
1
`
3
V
!
f
1
X

^
E
r
:
V
W
3



\

y
`

s

^


0

6
/
5
3
;
!
p

^
#
s
#
t
	
n

u
3
V
1
s
1
T
1
X
YXOs#t1tV3^]Y
19
1;:
[]`cd[[]`1Y\W)Y^)V3f/qIYT7[V3r0/635
- p!^$X1YZ{T1YX_c:T1X_[]`cd[[]`1Ye^]Yf!YZ[]YXxf!p/[]YZWHc:fy^)cze5p!^T1V:[cV3T1^[WHc:p!Ty[
S [$rV3f!fVae^$rW)V3[]`YrEc:Z[[]`cd[0/635$
- p!^s#t#n	uV3s1TX1YXc:T1XrW)V3[]`1YX1YZ{T1p/[]p!V3TV3rvs#t#nc:YZt1[)cdup!f!p/[z
`1YZW)Yc:f!^]VV3T1X1p/[]pV3T=~
< p!^s^]YX;[]`cd[2
/ p!^s1t#n	uV3s1T1X1YX>7 Y-5[aYx^]`1Vla []`cd[o2O/  ):  p!^
^]c:f!f!YZW[]`c:To2/435H-  ):  pT[]`1YLf!Y-#p!V3:WHcdt`p!V:W)X1YZWSUre[]`Y+W)Yfcd[]p!V3T^]z5uV3fV3r Vys1W)^
p!Tb[]`1YT[]`1Y{1W)^[V3tV3T1YTy[;V3r\o0#
/  ):  uYV3xY^^]c:f!f!YZWuYc:s1^]YV3r\V3T1X1p/[]pV3T=y? 
@ []`1YZW)aep!^]Y:hpr$[]`1YWHYfcd[]p!V3T^zOuV3fV3rbV5s#W)^p!T  []`1YT[]`Y{1W)^[;V3tV3T1YT7[|V3ro /  ): 
X1V5Y^eT1V:[p!T1ZW)Yc:^]Y;uvYc:s^]Y;V3rBV3T1X1p/[]pV3T3ae`p!f!Y[]`1Y;^]YV3TXV3T1YuvYV3xY^^]c:f!f!YZWuYc:s1^]Y;V3r
V3T1X1p![]p!V3T 5\`YV3T1f!s1^]pV3TrV3f!fVae^|rW)V3 []`1Y_rEc:Z[[]`cd[;[]`1Yf!Y-#p!V3:WHcdt`1p!V:W)X1YZWHp!T1p!^|aYf!fn
rEV3s1T1X1YXc:TXrW)V3[]`1YrEc:Z[[]`cd[pTcX1YZWHp/q:cd[]pV3TcV3T1^][WHc:p!Ty[ec:TuY;V3T^]Ys#[]p/qIYf/z+^]YfYZ[]YX
V3T1f/zc{T1p![]Y|Tys1uYZWV3rh[]pY^

BA

C

DFEHG JILK

MONPRQSUTHT5VWTUXZY[]\$^`_4a jyj"b ^dcRegf g4bhji c:f!fnlk Q6mLnoqprsmUt5Q4u []`1Yt1W)V3:WHc: V:u#n
vk Q6mLnoqprsmUt5Q4u u5z+X1Yf!YZ[]p!T1[]`1Y;fc:s1^)Y^FwIc:T1X3H Y;tW)VqIY[]`cd[ PRQ4SxTHT5VTxX p!^
lk Q6mLnoqprsmUt5Q4u 5c:T1X_XYZ{T1YXLc:^\p!Tx[]`1Y|Y-1c:tf!Y^V3rBOYZ[]pV3T1^yA5/



[)c:p!T1YXrWHV3
s#t#nc:YZt[)cduf!Ya;ffWff[# n

z{}|

fi~$R8d"d;``.U`~
 R;"RUH5WUU0yl$UqsU54R2RR4d)%R`"UyRR$"
"qddlRglsR$RFW"''
"F`xHHWxj$%`Ryl6LLsx56
 y)`'%"%`gFU0yRg`
 y)`'%"  %`gyl$Uqsx56$
"R%R"RR>`R
U}5} 	  q '*4%  }  $    }  U 	}  } }g 
"}`%dR5"%%`g!  '*4%  } yR
 U}5} 	  " } F  "  } J y  "     $     #
"R%R"RR>`R
U 	}  }  8 $     
"}R
 U 	}  }  4 *}U  4   ldx  y  "  } y  5    "   

y`W"'`>"g%g`%R0%Rs6R`"%RR%"`"Rg` BR
ddl`g`l"lFW"'R%W" R    RyR   
	
fffi!"$#&%('
"$)+*,#.-0/1'
2)3%4%5)'76839:!;# <=+7>?
A@B# <=#6C#D-$%5%5)'76839E:F

GIHKJMLONPRQTSVUWYXZX[LOS]\N^`_bac_8\:d
e 'R%%" 2'%0ddl`g`l>R6FRF`%%`W.%RR6"q"q% `g%"H
J%`%%")"RDf%RsR%"`66`R>)'1gddlRg` "ihB%g%
% !d`"xR" %`g)'RR56%l.R"d sRl.R" 8g%R
ddl`g`l"]jFR)6$RRF'1g>ddl`g`l Rg`!"""RR%
)RdlRg` "]kq`"4)2gR`)'1gdRlRg` s%`W$Ry%RR6"H"
W*	% `g%" "q"dRs`R"
l &monqpirtsu	7vxwyz7{}|~	7vx.py	
px(	Z
 %"Rg% "g`=J$g% "6RR;8%`R$R"%Rsg`5
yRW"'`F`"H'5xHx>``R8` %"`g%Rq8"R%%%"%Ry"R
R
}q74])]
 3C")ZV])
$ `7
  3 "C]
   3 "Cy
     $ "C7]
5

fi~
[O
Z

3[4C)u71[
I4])y
3 
 I"C7x
ZOV
b])[]7
  
I"C7x
ZOV
cC]7
+O3"q(E"q"O0O5+5+u
y+O

OO+[71"

u 7$$ uOIC
73 "uZcO$C)C)
 7$$ "O8IC]O8I7
7 7$$ "OcIC])
ZOV
cu]u
71[
I"uOc8u
73 "uZcI8]Cy
 3 1[
3V4]Oc8
   1[
3V4]O8b3
ZOV"C7
 +51
4O443"B:+
0~0OO++517OO"D"OqOEO
OO ]~+O+O"Oq $3"D "
1"Ox144O5"3"  "O?
  	ff
fifi`(O4
 
M
 D+O [K)i"OYD+O"     O44E?+D71"O?K +51
0+O
     	   :+C"O O"          fi!   # "tO"O14O3]    CO"
"O4
KDO434+ + +[B71" OO"O$
   tE"%
    O"DOD+
"    ]"O
71"".+71"&
  ')( u fiy5+O"+4BD3"
  *+')( u fiDu"O40D+?O,
  *+'	fiuq"O0+ 1
 B"O5
"
1.
 *KO+ +E4:+Z O4"/
  '0( u fi1
 "OqO4
""4BD
1"D2 * K3 
+"4
  
:+O"(D+O"t   5"O]"Ot4D1"    `O
OBOO
"$E;"O(D+O"`   +O
   

"O;"O0&+BDD14D1"7
 6$836$9:83;<
E
"
0D+O" 3 +O?   > =(+""
13

Ett.4  "
1(D?   +O@
   153 7+ 
"O1
    fi}+""O" =++[71"
.4A
   ".
   
+(+ 

"
T4D1"DBC6DE	   E
RDR"4~   +OBFHGJI7RE4O    +
+[~1"  "
1qK $4?+(E" OOL
 "tO?4D1"DM
 FHG I  O7O +"O
$1"
N OG FHG I	t
4@
 OG FHG I!    4O("O4D TOO~  tOD4 OO+ ZD  
I
+P "O(4D1"D,
 OGQR8O
OqB"4C 6$836$9H8H;DB"
(: S["q:`O+
4+O
7 
  bZCO&:T+CZ    ]OTycO3C  bHT+7 ODO3+"4O
O "~"O;71"TZ5:t
?q"O0+ 1
~IIZ4  
"

4+5+ O
 UWVYX: Z?[VD? [ D
+"5B71"O< :+O"u
y+OqO+]
 \(H $J
O UWVYX^ Z?[V.O [T(. _8"4BD
1"O[>  +4O(" O5K $"O5"OEt
"O +O417EbD
 `}7Oa _


"   =
~"B
O BOVOUWVYX:Z?[VD?[Y"
1}(+4  
I"O=B
"
FHG 6  b    3      3    ff c C"O=
4+5+ O"""OV"OKD+O"    3      3     d
 "O
(O+e $"O f O3 OO:H $Et(O+(
O +t="7+"~"
=:= =(+"K+" "
1
"O 
+"TZg _b+O417Eb OO+OET"?
4K $BD _8"5B71"
"O?4"q+3"
O+4q1"h
 =(H $""
"3  :+
DEuO UeVEX: ZO[VDO [ Tc~"ZO
4+5+ i)jKO"D"O
Cu
y+O
O}D+O"   y+
kimlO"""O]"O;4q+O
D+O"  m n   o=}+"K"
1
i)jqp +3 r7: S["OO
 imlK:+O4s iml4+O"E" +s &"O4(E"~4.1"&
 b36$836$9H8H; c
tffuKv

fiwmx<yRz!{|!}4~2xC5{|gPg{y!|ky!,|	x<	q?y+}	{wmx<y!}!x<!m

a

b

c

)PgffRmCN,PC!P Cff Jff  C   ff

 H 		  )<C3,C@g	CPffff+ff:+  H	C	C3@Hffqq:<3    R%
m  R@5 ff^ff:+ffPYPKe	W3YH-	P7P# ff+ff:+ff  R3eC  H3  P
CsH,	P-3	3<PgP)C3eP5OWYK??)e<<!EeCffK<Cd3YH-	P@P
OWYJ^?YO>>g335hCK	^km##33HC	<	5>H<3g  	
C733^P  C$H3-C!CPsffYDC73P  P%1NH	CP1R1RC307C3 
PC,d	  d,  3+P)51m	eq-  3^P)HJ.?	ff+Pff ff

R	@ff-ROJ	EY:
 7P#W,  YogC33HC	qd3P	  HC-P3CH:	23P^+PCCNC
C
CP:C33!  C	qCsEPJeC.CPP)K:R3	P2e3K	YdHP
3PC3C#   3C3eC3C3eP	  H		NCN3P	ffP#
 )Y E !A<R
	fiffP
 CffPff  !#"$&%(')+*-, ./021435(14
687q
 3 C  
 @:9<;=7q	OEeP-NH1P3YPC	3s	3

> @?5BA@   +  5e:<3  W  C
> 7:<3  W   C  
> 1   :R3  D7 1   


E5	?PCe	03P	Pg7eCff  H		  	Y3P0N	5^J  H!CP
NH	CP7GF+3CKNPCNK  H		PRCC  ,3PC  HffC   o
IHR	K
 J@WC@C3  3#CsK	$
 ?LJff
MON QPY 	NCPff
R &S2UT V0S-XW0&S2BY
)&)
Z &S-[T )&S-\Y
KR@:<3  2	NCPff

]_^`

fiacbQdegfhjiQdh

kml&n2oUp qXln-oXrtsul&n2oBv
ql&n2oUpwv
xzy{}|~/D|&II/& g{jQ@Ig
xzyIz{j| l&n-oUp u
  /{j| sul&ml&n2ogo[p
k ln-ortsml&n2oBv_ 








Q

c

@







c
y


g



I

Q

c




t



_

I



(

+







j
{

|

suln-o\v_

l n-oUp kml&n2oBv  QcU   {j|
kml&n2oUp qXln-o\v
ql&n2oUpwv
 {jQjj  j{ z{j|<|/~&yQI~yQ Dfi   gIQ|zU   


x yg|yK{j

 ~I~I+Q& Q{jj{G{}|&{jI&
+Q{j{j
$#/g=&+2}~I{j
z
{j~I
Q{G{j
#I|IQ= GI|IG
/mB2j lg( 2Qg   - g o +KXj+(Im QQ{j  +
|+
~I}|I||IIXut|
  j+c{}+Q&+& {j

Uw{j|
  Q(&2IGjI m 
2 {jy
jjD{j~I
{j{j
|<y
j

Dz  gGI|z

 D{j|D~I~I+& 2jIm   c	
gD{}|D~+g~I}{j~IB   


DD+(+c_
{j|&~Ififf p   
m~I}|&
B  D+(+ 
 }{  
  {j|<Q+QIc{j:Q{j|zD~I
Q|_{j  
 {}t         Dy+&        &y
|&j{G+j|z
Q       Oy
|
_I} {j
|zg~I~ID{}  

yQI  ff"!    


# =_+D&(yQj
xDyI&I   QIIt|&{GQjI|D
uQ{GG{j|+| 
{j|
z2{G&|  D{jy$y/j{j~I
_ 2y{j~K_+&{j%$fi'&)(  *(,+-*(/.0,$1&32  425+-426.0{87[I{jy+
&)(/ *( + 09$:&32 42 + 0;& g 2| 
Q& {}
<|&<$ j|&OQI y j{j~I
_ Qy{}~&Q+&{j

2{j&|<
mG{j|+=| 0  >
 (/?
 2XOcQ@
 ( + A
 2 + B
 ( .fiC 2 . ED~I
|{}+<y&{jQjF
G &IH0KJMLFN OOP&&8QRHTS JMLFN ON VU W 8QRHTS JMLXN ON YVU W 8QRHTS JMLXN ON VU W 0Z
[]\ gY^ V_ 8`ba  (Vc-dfeg
 hg d  Q(&2IGjI m 
2  >
 i H _
 Q
4
c g-kjQ+
BlmeI onI_ qp  r  + g n
 g csoc Hhg   ZcQ g "d/cQ Q 4c
g-kjQ+ g od
2 g dE
t  g )
u *ccvsI  + KyQ|+
z&I} {}
||I&(/yQ {j| G~I~I+& Q}/I
  

[2xDy&g
 {j|K|{}{j yQt
c
xDyI&I

  ~I+yQ K t
 ~I
|&{j+

K
0
yx JMLFN OD{jKy
Q{j 
 z y+&D{j| 
 <
0 JMLXN O gI|/

G &IH0KJZLFN OD{j|K
wE&IH0KJZLFN O  Q D|y y2  G &IH  0KJMLFN OD{}||/jj+myQ G &IH
j{j~I
_ QyQ{j~&+|
jjD|I # y&I {j
 |g
m
  ~I~I{j 
yI=yQ&|D~I

IO
 G &IH  0 JZLFN O I~I
I|z|/jj+<I~|
u~I
{G{j

{jy&I {j
 |g
z
  ~I~I{j8yIytQ&|/~I
K
I
 G &IH 
{Z| }

fi~VY8Y8"X"YY<		6	k~YYY

oZF4FM<XX84X'X"	4T4X	@X<5X4FoF4TXX	84X
V' 	F8X4,M-4X%*5XEXV"4%4/	M*F4XX5X	q4
XITKZF BFoMFMF5F	4X;X	84XX	4	48MXX4BFo
F4fiXX4X' 

	khZm"XVYYyXX-Yy	 ]M]4	- fmm
-8"KFoM-	 % X4oF"o6-44	,ZXFZFX444'X 	F5
FXq4B84@F4Z5F'F>9oY-44	fiMXFZF
X E	ff
6fi
 E	ff
6
 qY
 fi
   qY  	ff
Y








	




6
     	5

  V 	ff
Y
 fi
   E! "#%$ff
Y&
 
 fi
'   V 	ff
Y
 E! (#	ff)$ff
Y&
 
   
*

-,+-/.-/01.2%3F4o XmZ-4X 

  <9-V8@144>  
 

TMXFZF5  	XM*4X
*

76    
	859fi
Y 6 ff:<;8:>=?ff
	85



6  :<;<:A=Bff
6C
Y

X


 fi
@
Y 6 ;6 :<;<>D:>=$ff
5
@ X YX 6 ;5 :E;A(:<;&FDE;G$	
	A(:<;&FHE;G$YIfi
 fi
@   YX 6 ;5 :E;A"E;&FDE;G$	
	85J
 EK D#<;<ffE;L$	
6M
 
   	





E


ff
<
#
<
;
>

"
N
?
;
H
F
<;G$YM
 
 
Y 6 ;6 :<;<>D#<;E;&FD<;L$	
	6
 







E



	

A

"
O
H
F
%
#
Y
$

C

fi

@
 
 E 	A(#PF(Q$YC
 fi
@ '     
 E Q	&
 
   	
FoM-	      4fifX om-Y%
R -/.-/0S.S2"( TUffV4 RWXR V RZY
R @N[ V\5^ ]E
 U @  U_KU @  R%WXR @  Ra`bR _ Ra` V R _ RcdR @ feg_ R  `  Y
R @N[ V\^ ]E
 Uff] U_!U @  RW  R _ RL`  Y
R \ [ - h _KU @  RW  R _ Ra`  
F	@4y4o4ZoM-4Xj
 i W iLklmnoi  lpqkrnji  lpqk  nsimft(mfuHt(vF"4<
wyx{z

fi|~}!%)>!A

La>I("H
LqffN<^Bff!HE/P <a &Z
Lq{fffNE^BHKHNO d oE% ?{d oEZ
f(fH(r//SSA(ff~&aNG1^y
>SPh  f
  ffs>KGP       > qff  >>   s  >  Psr q  
 y  s  
   hPr    >S1 ff sa&  ff)MP   o   K


>L   {Psr s     &{
9  > S ,  >7!ff>   > q    GO>~ A><     {
~ CKE^BG GH!HNy
      {o G") G!~EMo   &b ,~Eg  o o~E EZo > 
 ?% Ea > >  7 CKff^Bff!HNy
9  > S ,  >7!ff>   > q   K O>~ A><     {
7 /S/S% N  HAyy&/S/SSA^NG   yHNE^B K   DNHNy
  O N   "^j K   8EE &Hs ?EZ r>Sy N,
    M!>~ K  M
K >Sh  K   rE>   rEZ	   N   "" N   rE)
   "NX   MIEZ >    "N   IE ?>X <Z  > 
~ CKE^BG   MH!HNy


 >>)h  >  !S q    Pr!    

 a
 1P/S/S)>~ 
  K
qr o    ?1 MH   y

Dyah%y"rq       > 7  {P  f(fH(

 r  rS%    > >  S    ! >%
 S  M% &ab EL



    S  %)X &b <a9 
   (ffffX  
 //SSA(ffX aZ

    > S ,  >7!ff> 
>Ih!HN	

Kff^Bff!HNy S  Ay

 G> A><     { N<^Bff!HEy 
  <L   &b Ea  X S  %Z

fffi

 Ebg &Z  > 

>Ih!HNK




fi !#"%$#&'(*)+,",-/.01,&23"4

50687:9,;=<?>A@#@B C!D%EGF	HJIKHLDNM
OQP*RTS,UWV XUZY[&\%],^	RJ_`\/^TU?aZRT[b\%P ^?c#_dU+Se?YUf[&PgRTU?hXeiRTU?jkRTS UfP \RT[&\%P ^d\%lmeaZna?o&[ba?[RKn*eP,jkea?a?UZV,Rpeiq,[&o&[RJn%c
qn*rsUNeP ^	\%lteuVeiXTRT[RT[b\%Ps\%l0RTS U+V Xp\%hXverw[&PxRT\8eP=]#V,VGUZXyeP j=e/o&\_`UZXV1eiXTR?z!{|U}[&PgR~Xp\j ] a?U?jsRTS U
P \RT[&\%P=\%lt]#V !eP,jk_`UNei=]#V#Jea?a?UZV,Rpeiq,[&o&[RJn%c#_WS UZXpUfRTS,U}]#V VUZXdVeiXTRy\%l0RTS UfV Xp\%hXer[&^dV Xp\YU?P=RT\
qUea?a?UZV Rpeiq,obU/eP,j3RTS Uuo&\N_dUZXfVeiXTR+eaZna?o&[&azOQP'\Xpj UZXWRT\*R~XpUNeiR}eo&^T\RTS Uua?\%PgYUZXp^TUaNe^TUc[zUzc1RTS U
]#V VUZXVeiXpRqGU?[bP h}eaZna?o&[baWeP j8RTS UWo&\N_dUZXVeiXpRea?a?UZV Rpeiq,o&Uc_`UW[&PxR~X\gj ],a?UP \N_RTS,UyP \RT[&\%P\%lGo&\_
ea?a?UZV Rpeiq,[bo&[RKn%zt{|Udl\%o&o&\N_RTS U`^~R~Xp] aZRT] XpU\%l#RTS,UV XUZY[&\%],^^TU?aZRT[&\%P,^? X^~R?ceWrs\RT[Y%eiRT[&P hUv erV,obU![&^
V XpU?^pU?PxRTU?jz	:UvR?c _dUj UZ,P UfRTS,U}P \RT[&\%P3\%lto&\_Jea?a?UZV Rpeiq,[&ob[RKn=eP j=V X\NYU^T\%rsU+XpU?^T] o&RT^?zm[bP,eo&on%c
_dUeiV V,on*RTS [&^P \RT[b\%P=RT\8RTS U}V,Xp\%hXer\%lm\%]#XWUv erVo&Uz
kW0k,mx,#
 XeiVS^~R~X] aZRT]#XpU?^eiXpU*],^TU?j[bP:Ol\X/rsePgnAeiV V,o&[&aNeiRT[b\%P ^?ct^T] apSe^uRTS UXUZV XpU?^TU?PgRpeiRT[&\%P\%lXpUv
bo eiRT[&\%P,^?c^T[RT],eiRT[&\%P,^:\X}V X\q,o&U?rs^}^TU?U8Uzhzc	XeiR~\c!%%zuy_`\sRKnV,[&aNeot\VUZXeiRT[&\%P,^+\%PAhXveiV,S ^
eiXpU	!*+%pN	pZ'%Z=Ngv%c,eP ju!/,LQv,s?fJTN `p}Qv#ZZZz
yS UfV,Xp\%hXer ,b0sqGU?ob\N_] ^pU?^q\RTS=RTS U?^TU\VUZXeiRT[b\%P ^yRT\^p\%oYU}RTS Ul\%o&o&\_W[&P hV X\q,o&U?rz
 [YU?P*RK_d\P \gj,U?^!t?~[&PeuhXeiV,S,c,P jeuP \j U3RTS,eiR`j \U?^	P \R	qU?o&\%P h}RT\ePxneaZna?o&[&aWVeiRTS
[&PsklbXp\%r  RT\  z	yS UfV Xp\%hXverN ,%ka?\%P ^T[b^~RT^y\%l0RTS Ua?obe] ^pU?^?
|, ~ tQ	Q1
g N,,TGQ#	~m
   N, TG 	Q1
%xTQ#~!Q1
  # g~	Gm

e] h%rsU?PxRTU?j_W[RTSRTS UV Xp\%hXer,0 |\%l!RTS,UV XUZY[&\%],^W^TU?aZRT[&\%PzfWS UXU?obeiRT[&\%P	[&^+^~VU?av
[ ,U?j e^kRTS U'P U?heiRT[&\%P\%lff
ficW_WS,UZXpU
fii%~  ~Qs[&^*R~Xp] U'[&l}RTS UZXU[&^=ePeaZna?o&[&a
VeiRTS\%luRTS UhXeiV,S a?\%P,P U?aZRT[&P hRTS UAP \j U?^k|eP j   eP ja?\%PgRpe[&P [&P,hz  \X[&P,^~RpeP a?Uc
 ,v  v0Qt    Z  Qtp  T  WS \%obj ^}t[&h%]#XpUffxz

 q^TUZXTYUtRTS,eiRm ,b0f[&^P \RRTUZXprs[&P,eiRT[bP hl\X[&P ^TRpeP a?UcRTS Ug] UZXpny%x1vt  Z  !T,
S,e^ePk[&P#,P,[RTUyj UZXp[&YeiRT[&\%Ps\q Rpe[&P,U?j*qnaS \g\%^p[&P h/e^`[bP#V,]#R`a?obe] ^TULeuYeiXp[ePxRd\%lpRTS U:a?oe] ^TU
eP jqgnA^TU?o&U?aZRT[bP heo_e?n^u[&RT^}Xp[&h%SgRTr*\%^TRfob[RTUZXeoz:\_`UZYUZX* ,bx%[b^o&U?lRQLRTUZXpr*[bP,eiRT[&P hzOQP
\Xpj UZXRT\=V Xp\YURTS [&^fXU?^T] oR}] ^T[bP h3ea?a?UZV Rpeiq,[&ob[RKn fUZ,P [RT[&\%!
P z  c0_dUP U?U?jART\,P j|ekrs\gj,U?om\%l
N,,bx%RTS,eiR}[&^}eo&^T\kesr*\j U?o\%"
l $#&%'K ,b0,cG_WS [&aS[&^+XeiRTS,UZXfj,)[ (a?] oR?zf+\RTUeo&^T\
RTS,eiR:RTS UP \RT[&\%P,^W\%lm_`UNei]#V#	eP j]#V#Jea?a?UZV Rpeiq,[bo&[RKnkj \P,\R:S U?oVRT\^T[&r*V,o&[blbn/RTS U}V,Xp\g\%lJ*
z :\_
UZYUZXNcm_`UaNeP^~V,ob[RN ,%[&PARJ_`\^T]#q V X\%hXer*^N,
 +  a?\%P ^T[&^TRT[&P hk\%l`RTS Ua?obe] ^pU=}eP j + 
a?\%P ^T[b^~RT[&P h\%lRTS UyXpU?^TR`\%lGRTS U:V Xp\%hXverzm:\RTU+RTS,ei*
R +mWUvRTU?P,j 
^ +NzyS UZXpU?l\XpUc[&P\Xj UZX	RT\^TS \_
RTS,eiR: ,bx%k[b^`obU?lbRQLRTUZXprs[&P,eiRT[bP hc[&Rd[b^^T.
] (a?[&U?PgRdRT\8V Xp\YUfRTS,ei/
R + 1
R + 
 0 +  [&^yeaZna?o&[&ac#RTS,ei2
[&^yea?a?UZV Rpeiqo&Uc1eP j3RTS,eiRyRTS U}a?\XTXU?^~V\%P j [&P h/o&UZYU?orseiV V[&P h%^	eiXU^T] [RpeiqonXpU?oeiRTU?jz
4365	87'9s:0<;
 \Xprkeo&on%c_dU}[&PgR~Xp\gj,] a?UfRTS U}l\%o&o&\N_:[&P h/P \RT[&\%P3\%lmo&\_Jea?a?UZV Rpeiq,[&ob[RKn%z

=<>	>

fi?A@CBD8EFHGCB.F

a

b

c

IKJHLNM.OQPffRS*TUV	WYX[Z\<]	\QW&\&^)^ Z\<]Q_ \&^`]	\QW<_ \&^ Z\QZ_[\&^ WY\<]Q_)_baffcCdNeHfg

hjikKl1mbn&mpolrqsutwvQxyNzC{}|~8~N8PPffeHP}PejYCJ4LpdO/'P2PffgP/dN
 e4MgQPgg`rpdO*gdNP8CfeHPP/JHPOOQPQYJ4dNdNJ4g/Q<[
4b[`1J4cPffpdNeHeHd	/JHL  dNfCJ)JHdNgcdNe4fS
 //P$Pfg/
 /JHg/   eHJ  `O	`1. 
 /J4g/  PQYCeHP`O`1. f
R/pdO2P}PO  LOQdNMfjJHgQ  P,  \	\dNfi  e4MgPdN  .bdO2P}PO   .JH
J4g2fPCPfJ4fAJHgd  dNgO<JHcP    
 OQdNLO$JHg}Q$[J4cPO<PffP$JHgCfAg`JHg2e4d  PQYeHP`O	`1.
f

/J)c

 cCP,dJ4dNdNQ*JHgfCPCPfjg"J4cP,OQPJHdNMggP   JHdN  OQPCeb  JHL` Q  
`      /K  \	\ JHg,LOQdNMfAJ4gQ  ff
P dNCfj  <1     \





	 ff
 fi


 	fi 

/cPO<Pff < \	\  YOQPffcPeHJ)PO<eHgdN1   \ &\ p  /cdNgPO<Pe4YJHdNg2d  M.O/J4
 dOQd	}PAcPAeHdNLNMPAdN  cPdO<P  pdOeHd	[dNMfPf 8MPOQJHPg*Pggd  J4YP,J)c 
  \&)^`_
 adN'MCe)JHgPg/J)c pdO
eHd	[dNMfPf 8MPO  J)O X a
 QX )^`_

  
  
OQdNLO$fJHPOQOQPQYJHdN 





 r]$Z X Z K`      \&\ Z `     a<\
   
YOQPffcPffe4J)PO<eHgdN,cdNgPOQPe4YJ4dNgd  MO,JH'
)^`ff_


C i8#"}o%&i$& qs(')Y"}8Y*C~,ff+.-8N)/'bN,ff 10}Q$2[p Q4b[[13.b`6C"N fiQ< 'N#[$ Q4b 
!
5 ! 6u 
/cPO<Pff <  \ 	\ 

798,8

fi:<;>=?@BACDFE	;6GH@BA.IKJ.@L=AM=NPOQE	ARE	;>IRSUTV=WCR@LXY:<;>=C;>IG<Z
[ \}]9]_^(` ac  bRy dfe6gihhKjUkLl4lmknPkLo/pqgPrmhsrmbRpqrthKjuavb6d(h3gid(nxwzywz{}|vb6d~g#d%hK6d%g#d~eRop3(d(lM}i3 |vkBrm b
acbRdj/hKoLoh9|vkLRg#d(lm6oBrFkLlvpURkBgid(~rc(hK6lmd(6d(6(d}hKjavb6d(h3gid(nlFzy1p36Y>yy
W,WR,Lz(W \K&] 	ff \]  \_3 / ]K_ ##_ ( U ^}  ]    ^ <  ##_ (  `
kLp3llv oLh9jRhK|coLoLh,p3|v(/(l(Md~y e6WriBpqRoLd3)9y<6 hKR6/&likL	6ffd~Kg&rmbR>dHe6gi>hKK3Rg)	p3n 3,K6>R63	Rz,R6R d&lmb6Kh,|zKrmbR #pqKrcyVrmacbRdb6d(e6gihKrmb63g)dp3e6n gih9hK6je6Rgih/&(	d(d(ffK6 l
 yvavb6de6gihK3g#p3n K #d)>rmd(66l ,6R y
zyvavb6de6gihK3g#p3n K # 96V kLlcp3~(okL|yg9yr(yRrmb6doLd~	d(onpqe6eRkLR
mW, ff K V   _      , wzy
 yvkav b6dF6e6p3gin hK3eRg)op3dn yw 96jh3gVVpq rmhKkl<np3l (|v(d~kBe6rmbripqgiRd(oLo/dvpqrm|kLhKyg9 yr(y .  p36 1{ rm b6dvkL rm{d~gie6gid~ri{qpqp3rmRkLhK u  {|vmWkL9rmb ff.K6 Vd R R  d(_ p3 l  
   ,  p3R |vkBrmb          {6l9yr(y
  /   mW9  K     {
p3R   {R  {p3R  pqg#d}p3lc d(jh3gidu 6p3n eoLdQywK#y
>yQFhK6lmkL6d~gvp13gihK66 kLRlrip36(d
mW, ff K V   _   mW9 ff K V   _ 
hKj  #y<avb6d(
mW, ff K V   _      , w    9     mW9 ff K V   _   y
6d(hK	6pq 6rm6hKkLhKd(RMlmkL{ 6p3kBlcd~rPgMjffp3bRrmkLp3b6oL.ldgipdzR Rd~6gmRkB rmhKd 6 6 6d~ g#l(y 9 Rrg#Rd( d3){F |vkBrmb p36lm_|  d~g ff  )   i ,y  y Qh3rmkLd((9dYp36rmlmbRd pqrkLlRoLlmh,kL|F6  




 



)!  +* 


-,

,

  

! 


 


	fiff

"  "$# &%



('

/. &0

,

"  "$# &%

1


	fiff

-

$243 



65

F
PO

fiG HAI

87 97;:<7>=?= 87 >@<7A= 97 !@<7>= 87 B@C@ ED
KJ
L 	 J
L  NM

Q8RTSVUXW>YBZ\[^]\[`_a[bdcfe[hgjikgB[
l^m&n\bpoqWAeY1rstWAgfiuvmwn\xyYBma[`n

kLlPd lmR66nkBrmnkhK6pqgilQk (wzd(y {kL y  Qp3d 66kBzrmkLy hK  e6gih,y  k6 dd(oLp h,|nPy d~rmb6 hYrmb6jkLh3lPglmeRd(gi~h9rm>kLhKkL6 1{F|od(djr_ 6grmd~#lgirPn6kLRklmpq(rm6khKlmYl hKp3jH63p p3zrip3 K{Rd(|vlYb6p3k6ib
.hKjg#p9|c p3l rmlQbRpqhKr j kLrm6b6(kLh3lcgmne h3d~g#rmpqb6rmhd( ly rmb6Qdd)>6r(h3{rmkL| hKd}6lkLz| rdQgihbR6p96	(dQd}kprnPgihd~6rm6bR(hzd(6PhKokLhK13rmbRdvjh3e6gvg#e6d~g#h9kLhK>RkL6lH1lmd(oLd(~j/rmr_kLhKrm6d~l9giynkLRkRpqp3rmkLoLhKoBK {
| dKkB	d}p3Yd)6p3nPeRoLdQkL h3gi6d~gcrmh kLoLoL6lmrg#pqrmdQrmb6dnPd~rmbRhz6hKohK3Ky
   i 6	 6   R > 6 .  	R	 
 y klckLRl(yPr(yRpd(kBnrmbRpd~.g kLnp3d)olmrmd~d(r R6l hKj (vhop336gclmd(kl (dhKj 	d~gilil(pyr(y y j/h3ginlHp3p3~(oLkce6gihK3g)p3np36 
zy _j d)rmd(6Rl crmb6d(
z 

K{

|

pz 

~}
$M





`{







C8 
~ 

\

~

q

fi CGI C fi HA


q

 

q<

d

CI

 C1 )H

C

<

d'

B

!H

k

<  

fi?A>

aB<B1qP!A1yB1AC> dd\
!!C
1d
yKt ddABAK1A6A !$C<!y> <?!A
!B!/AA!A\^T!A6
!tB9^
`<A  A  f \ ddA
 ;  AAAd
aB<B1qP!BhK1A
AC/A <
yKt vAAB1A ddN!TN\!A1

< dd~!!CA1\!A!6BF!AAA!
    !Ak  8!6B++^9\AA!A
(/!9!KA  qt1A?>6A  ) ( 
1/A \   ( AA  ddf \ AA

 ~!A!!C!tAKA?1K
1)BK>/
!p A!/!
KAC  A9!A8>/A
Bt;ak1dqAt!\AKd!
KCA8A
	fiff!/!TA!A
1(AkCA!!(1A
1!A
1AdAtd!AdC
 C6p6?A1AATA(/! XC>AK
<
 (!A /!!C!q;ak1Kd>q8<
 !t!A!
AA!/!



f


 



q
 B  !AdKAA6!A A!A!<d(B$A  ?AT6
><8+  
 y1(A&!A1A!!qA9A!
 !d)BKC
AAA!K^!fi^A!/K11d>h61qA!AqA<)qA!(/1(B
A(8>>&!A1(w
  L  
"!$#&%('*)+',-%/.103254687:9<; fi=?>-@$@ 7 8
 ACBDFEHG8EIA$JFKML dBh1AA! 
NN<&O ! K\yq  d
 d!3k?Ad^6!A HP d
 d!
KCA
h6\PQ	RTSVU+W-XZYH[\[\ZW-ff]Y?^
_`aSb3Tb*ff]b\Adc`ch)eYI	-f)HPhA qAAA
A
q  AAqhg
Cq!!h!AA!A

  ) 

`!8  A!qB$

8Aq

/ia\ZjlkmOjnporq 1s)pO)HP tvuIk/
w

`<A~A yxf \ 
w

`<A ? \ 

 qPq!? > 


z|{+z

fi}~y-`-H~C`8:8--HfiH~fihvRfi}~--~-

Qfi|I"C
fiC
CmC\|H
"*Im88I

Tfifi`C:pIm
R*ICQFH
CTfifi`:1/V-
fi3|CCI3:V:MCT88I

TC-`3
yC`:I
?C:C:
I

TCfi3`:lImfilCTm-
HmfiCh:p
"I$fi*I"::h1IC8C1T|
ICC3|mT|-R:&CaC:IIfifi|mTHT|1CIp3::C|mT|R-C
:C
:Ch8C1T|fi-:&CC:I\ICT
IIFI3C|II1

"I?3MCIp3:
MC

fi
fiI`CI:3fiIlCTfiC::I

TC-`yCIfi&
8
\CvTC
3C\CC-|::(CT8CI

TCfi""C:Ca|CI-I
fi(* :fiCTIHICQC:I\I
+"

"

:

&

C|Hfi`fiT88I

TC-I


 CQC:I\I +\


CC :\ 

   :CCTfiMTH
"CfiC
 &H

   8:

  dyHe 

IfiC$TC\T:C
+ 
C


    +Z* e 

 \1

3I

pyC
(: +\ IC:( :\ $\
1
T`H
`: 
 Z* 

e


- ++Z
 C  &H  8 
I
 C:
 

 * Ifi

 8H



Ndy `*:3|fi :\

-TH-ff
QTfiC:fiy 

fi|H

	

3ITy
3Md|d
 C 

3CI"Cfi|-V:|I
|ICIC


C

C:II
+"



:

&



1
|ICQCC:I\I

 +\

 :\

-IC

1

3I3

"yC


CICTp8\
-I::8pTC$`3I:QfiT

$I`:THTfi


mm|I
CTfifipIfiT

$I(1I:8pTC$(CTC"C

:1::TC-
r`T
`H
`

C8fiC:I\IpC$C"-:

:Q8fiC:I\I

1

:
`3ICh

1:`-h1
|IC:mfi
yC`:I
C
:88+m|883|I

TC-`

IC

I

Tfifi`:



C
IfiTI3:C\fi
I1I\
3C:|CCT-C`:1
|I3--fi
CI
mC
TC\IC"RI:

  !#"$!&%'/1)(+*-,/.102.43658769'7:.8;=<?>A@=7/;CBEDCFG
HJI

fi`K

$LNM

 R
C|O QPSRSRSRQP

UTp|d
-ITHTOVXWZY  P L\[ S ]_^

` Uacb ed |`  
CCfUa
` 
CTUa&Ighacb  IT
I
iQj2k

filnmEo8p?q=rcsEotr

uAvxwzyE{Ew}|~{=z-wSffwS{C-cc=wcwz/wSO6=E|c{=n /t 6O /NSS/Z4 6\{==w}|c{wz
E-wz-6|c:{ /t  } / SS6O  &:cc2S
 ew2hX|cfz?S~|cx=wS{OyE{=OEwS-wS4:{==|~{=ecwz/wShff6='|c{==  Q =wz-|cw
E-Q/wE6X|cSSwz=-6EcwQS?cwz/wSEO6=E|c{E= /K{E{|~{CwzE-wz-6|c:{2/2
 e_|~{===z|c:{8tE:-wE6 :|cx=wzyE{EwS&wz/wz  {=t=4:weE6
2  |c=wzy'{=wS#wz/wz eff |ch|cfz8Sc|c  {En#gwz/wz eO |chU|c
SSwzE-6Ecw  g|) eXJ f=wS{ 
/ffh cU |~Kz8Sc|cg=wS{=-w= 6 =wzyE{Ewcwz/wSO6=E|c{E= 6 &K cU4  
QSUcU  hX|~z8Sc|cQS4 6   {=SS#c-:={=|c{=-{ESwS
 QSSSQ  :S~=wS:UcU  &wz/wz eN |~  |c=wzy'{=wS|c{U=wS{
  6 g  U  6
U|~z8Sc|c}=wS{=w 6K=wzyE{Ew}ffcwz/wSO6=E|c{=\h 6  #UcU  h
QS)
|_ egwS|)=wzUcU  h|cSSwzE-6EcwneQS wSS|#c|cSwSff=wS26  {=
t 6   |c{OE|cfQw-wzf264wx6 
 gyE{=wSS|#c|cSwSO?=wSE26:U  h&4  {=n4wSS|~c|cSwSffff=wS
26  :UcU  hSS#cf:={=J|c{E-{=SwS   SSSS2 X :
S~=wSg:hcU{=#xwz/wz \  |c  |cg=wzyE{=wS|c{nU)Ug=wS{
 A 6 g   6 Q
|c|KEc8-:={=|c{E-{=SwS+  QSSSQ X :ES~EwS:EUcU{E}&wz/wz
O  |c  |cf=wzyE{EwS|c{h=wS{ A 6 g    6
g4Q/w        ffZ    /t C6      

 wE-Q/w'6=|cffwz=}|cSwSz  |_w  '6X|ccwS#wz-ff|c{E6|c{=g|c=Ewf62/wwzE
|c6=Ec|cQ6'cwfU=wQU|)\{E:{8-:={=Ewz-|cwS  w}=we=we-|~:|c{E{=|~:{:4:={=EwS={=wS
?ffg=g{=wStwS-=|  =|cK|cffwgQS==w|~{CwzE-wz-6|c:{-wS-=)|c{=#-:=wffwz=?4

	ff
fifffi

 ! #" %$
$ '&	()*,+-	+/.1032452672 98*,+
;:
@?A A A ? 
<$ =1 >
>   CB f CB SS S2B  CB |cf B -:= {E|c{=-{=Sw:C${=
2/t    
ED SS D #4

O#$&'
tt=4:w'6Ew6|)|c:{  SSS2 :X  
  /t  
{=2/t   6w=-|c{=wS\=|c{=EwwzEO:vxwzyE{=|)|c:{ ? 8 Uwz   S SSS2 X f=wS{
|~
XE }2  t 6 x|ch&wz/wz  =w-wz

|cKyE{E|)w



FHG ?,I$JKMLON )QP;P(QRS-T6VU8;6W6VU-XP84Y6[Z[6[Z[(*OXQSSSQ ]\ E /8 C  8*+x2/8 C  84	-T(&5678ZV*-	+
)w R^ZV*_`6VU-bac-S6VU,(S{S+H]|(@df} eg-ihC*]Z[6[Z[(*kj2ml,2Xn-S6$o&p-q8T&	()*,+-p+srS)]-54^t%0324Y2u672t   t C6v8*,+f2  8 C6,2
U-5*x-5y-54^tHz +-54^ZVyQ8~6[Zi(*x(@d$ZuRWhC*]Z[6-g8*,+Z[6Xp(~*678ZV*ffRH(*t&	()*,+-p+r5)-Y4Z7-RY2
 4(S(@d52 wSQ~=E62/t C62/tSS26==4:={E=wSEwz
$
}
$
QSSS2^$   wg=w 
yE{=wEw  tEcwf
 e$  => @?~A A ? >   -  $   =1> ]      SSSQ$  $   =>     / $  $   =1>   / :8=)|cwz 
|)Z#nE-:   {EZ{|c{Cwz-=-wz-6|c:{     $    ~'   
 <$  =  SSSQ   <$  = 
=
SS

fi3,,#;]Cff~ffM,,k%;;gM3,,,3
]Y^^S555	X	Qp%	]M	YMX~f]~	p5Q	~]b55ff#MC]p~MXpMkQW	
	]T~`~E]5p5

 	]`V~MMM]g	5Y	M~sMMM]	^Q		sQ]MSQ	M~~	M#kY	
[O9,V,TEkE~V,
 vW]kMq!]T]^~^OMX]5	YpM5c]M^vVbs^Qv]vb']5XW]k]]
g]]	,Qq]5b]#5M~s	cY5MM%,Q	vvup~W	gC%	M]H	]M]^~^
b`]Y]]Yp%	]p~^   ]M^Vg]~
 	5k]1v^Qbc]]5bWs]
5~v]	5W	]Qcfi ff]pM]5k^~op5k ]TM]	]]5C	,Q]']35M~`	
 Y5MMT,Q	Mup~ WT	v,]M	]'Q^55~pM]M]kQMS%~]T~f	]^]]5
	5 fM~]ps ^

a

b

a

c

fM~ffpsfi    !  #"i $&%"i     "i  %'&"!"i(!  '&"i     "!")`]~M]


q]p~^  * 5~]		q~	]T5]p5
 ,+
-fi.0/#132546187946:32546:87;=<
>@?fiA3BDC /&:2548EGF;54
HI-*JK/#1K2K461879461L4M:2fi;4
+
- NO/M1P4M:2546:;4
+
-.9/M1K254Q1704M:L46:87;SR
,+
-fi.0/#132546187946:L4M:T;U<
> HVI*-J5/#13254Q1794Q1P46:;SR
W ,+
-(NX/#1L4EYE[ZL4Q\
F^]:32FL4M:
7*;_<
N- N8`-fi+9/M1P48EGZL4Q\
FT;4
+
- NO/M1P4M:2546:87*;R
,+
-(NX/#1L4EYE[ZL4Q\
F^]:32FL4EfiEGZ46\
FX][:
7F*;_<
> NT-(N
`*-fi+0/#1L4EGZ46\YF;4
N- N8`-fi+9/M1P46:32fi;54
+
- NO/M1P4M:2546:87*;R
,+
-(NX/#1L4EGF4E[F;=<aR
b  ?YA3BDC /#:L4M:T;c<@R







dVe f

b

figih*j
kYlmon*j8m

p*qor*sut#v*wxpy&z{Dy|D}~*T)K=v*wywq|(t#ozUV*9Mfi6*6)os*w*wUY_t[Oz}r8t#rT|Dqoq!
w'8qors#!wV|Ds&ws*zDy#y&ws#p3z*o{t#zt#vwq|Drs#ws |DXOq|Drs#w Ows&y&!KwsXt#vwV|Ds#wvwy&w
t#vwy&ws|zfi*wt#vT|(tzYwszDtKwqoz{t#z_|DfiU|D
qooxpT|(t#vo=y&z}9t#zfiy&sMtPt#vw
y&wq|(t#z=#Vosrs&w_t#z*s&r&vU|,*zfiwD*w'Yt9t#vwzYw|D_t#vwzDy#y&wsMpKz{i|(y&s|(y&w
wqowt#wy&z}t#vw{Dy|(p*vK^rs&o{t#vw,y&wq|(t#ozT*|Dqoq!OV=osV|Dqqowy&wry&s#!wq!czt#vw
y&ws#r*q!t#o{{Dy|(p*vKOq|Drs#wws#y!Kwst#v*wuT*|Dq0s#!t#r*|(t#oz5Kvwy&wzfit&|Doszqo,zYwst#v*|(t
Kwqoz{t#zs#z}wz!t#s|D
qoop|(t#vs^yz}@9t#zLvwy&wq|(t#zx96*6
vzqosOot#vw
{Dy|(p*v
osLzDt&|Dwy&z}t#vw{Dy|(pTv*Ywqowt#o{u|Dqoqt#vw|(y&sXzt&|Do*o{t#v*wzYw^z*
 tos^y&wr8ys#!wq!w*wxYt#v*wq|Drs#ws*^|D3|DszwOzrqow'
pKwt
 Ts#wy#wt#v*|(tYrwy&wszt#v*wzDy}aV9Mfi 6
O|Doq)zDywwy#x9Mfi6*
 wpy&zwt#v*|(tK*suqowt6t#wy&}o*|(t#o*{Y_rs&o{izr8yKzDtMt#z}[rp_}wt#v*zfiKK
V|DiKwpT|(y#t#ot#ozwoxt#v8ywwpT|(y#t#s
ULos^t#v*wpy&z{Dy|D}~*SzS9|D}p*qwfi

U0zs#sMt#szPt#vwq|Drs&wsS9z^Kp*qor*st#v*wq|Drs#ws p*pz~V

T3
USzs#sMt#szSt#vwq|Drs&ws 3T|D*,Oz9KTK

t oswV|DsMit#z&vw&,t#v*|(tSos|D
qooD,zDy&wzwyV5Sw'
t#wsS(3|D0OwV|(
q!iw'Yt#w*s 
$yV$t9
8zOwV|D_|(ppTq!it#vwKzDtMt#z}[rp|(pp*y&z|D&vt#zz*sMtMy&rt|xqowwq9}x|(pp*{
 Q6fiD |D*,|Diofit#wy#py&wt&|(t#z QMD Lv*wpy&zYz5py&zYwws|DszqqozVsV
U  os|Dwp*t&|(*qow$y$t 8  |D  {!wio8|D}p*qowfi

U0  osO|DYqo$yV$t 8   wTw|DsX8|D}p*qowfi)zDyM3V|D
|Dxs$t
 96*6
   *  fi
zDy&wzwyV5q|Drs#w zX~V*Tis&|(t#sM*wst#v*wuz*!t#oziywq|(t#o{t#v*wt[OzxqowwqS}|(p8
pTo{s
 i
 zDy&*wyt#zw**w 8    8   D| i   Ow|(ppTq!pKzofit^Ozs#wyOt#vwqowwq5}x|(pp*{
 9Mfi6*6
  D   T   fi
  KG0)*6
  D 
|D*iqowt
    96*6
  *
 8qoosMt#s|D*iw!t#vwy^T  
  (5zDy  
  *   
$#V(,#P G
968  xqoosMt|D,   )

 D    fi  G  KG0 
 	8    	fi ff
 toswV|DsM,t#zx&v*w#t#v*|(t  |D D |(y&wsMpKw|Dqo w}zfiwqsz0P  |DO0(
ywsMpKwt#!wq!  t^y&w}x|DosPt#z&vw&t#vwt#wsMt#soxpKzofit#s^|Do


^zs#owy|{Dy&zro*sMt&|Dw
V*9Mfi6*6


 KG0)* (#V(9MfiM96* 
96*68M9Mfi6*68ff




fi!#"!$&%'(*)+#"-,fi.-!"/!0213'("4'(,457689$4:;!$!,!)<

=fi>@?BAC+DFEHG4IKJ(EMLG4INLKO
P QSRT*UV

?fiW V8X WZY4?fiWZY X A P []\M^`_*P Y!? Pacb7dfeM^gPihkj4VmlonpU Y4?fiWBq]rsA P []\Nt

P QSRT*UV

?fiW V8X WZY4?fiWZY X A P []\

^`_*P Y!? PacbM^uPiv
w9RxNUV

P QSRT*UV

?fiW V8X WZY4?fiWZY X A P []\

^`_*P Y!? Pacb7dzP Y|{ PKacX}^gP QSR~FUV

?fiW V8X W V

WZY4?BA P [fiy t
WZY4?fiWZYA P [fiy C

[ y+  ]
[ \ P ^hkj!VmlnpU Y4?fiWBq|rsAW QSR~FUV WZY!?fiWZYACHG4EKYI]*Y4?INE
=}-*m=fiELG!INL fi
P

P


P
 
LKY4?^ 
qBr!I]*
Y
Y4? P @
C G*EKm


P QSRT*UV

?fiW V8X WZY4?fiWZY X A P []\

=fi*  *EI]=fi**

g

^`_*P Y!? Pacb7df_*P Y PacbM^uP Q(RT*UV

 4
LI]*KE
QSRT*UV

=fi>

?fiW V8X WZY4WZY X A P []\ C

?fiW V8X WZY!WZYA

h+vw9RxNUV

?fiW V8X W V

WZYA

X AC+DFEHG4IKJ(EMLG4INLKO

P QSRT*UV

?fiW V8X WZY4WZYA P []\

^`_*P Y PBacb^gPivw9RxNUV

3 EJ(ELG4INLLG*E*EKEK4KE=fi>LG*E
 LG*=fi-L
LE  4INL  =fiCD
QSRT4UV ?fiW V8X WBq]rWZYA
AC

 LK  EK>sLZLE

 LEI 

hkj!VmlnpU

 4INL  =fi=fi  2*=

?fiW V8X W V
?fiWBq8rsA

WZYA P [fiy C
 7>**4I]EK|LI  L=fi4INI]|LEKE  EK>sLZ
U LIN(E>s=]  *
 
LI]4KELG*E3|*E

 =fi4fiE@G*=  

p28-]
 LG  !INmE@E*=]m=fiEK&    E2ELG*=*3>s=]4=J  *2LE
]I]|

 LGEK
mEKLL=EK=  -L  =fi2

G*EKE2ELG*=*3K=fi

 LGK=fi*
L
4L  J(E*EK(INL  =fi;I]*=  =fi7E  EKL  =fi  E]C

4 *E}LG*E*=]L  =fi4=fi>@I]KKE*LI ! L&I]*I]     LfiCG*E/*=J  *EI2=]E

*I]L  I  4=|=fi>LEKG*  |*E3>=]@LE
 EKmCpDFEG!IKJ(E

 4INL  =fi=fi>IfiEK*EI  =fi  *=N

 !INL  =fim(G4EELG4EEKI]|L  

 4>s=]INL  =fi*EK

 

   I  

 *
L
INLEKLG*EE  EJfiI]*KE=fi>9LG*E2ELG4=|*  EI]*p=fi>9=fi2EE*I]  EKKfiG*=

 !INL  K  IN+LG4INLEK=  -L  =fi2I]4fi2EK|LEK

 LG

G4I]K=fi*
L
*L  (
J E3*EK(INL  =fi

 *

 +m=E> 


EK*=fi*fiG;L=>s=]I   E3I]*    EK2EK|L  |LEEK
L  4H4= 4 EK  *=fi=fi*=]L=fi  EI]=fi  *C
 (
DFE=fi  
E L=K=fi*  *4E  LGI]c=  EJ]INL  =fi=fiE  INLEK=]9Cc4LI]*pEK-EKG 
U ?fi]SA  SL
=|*4KEKI2=*  INkIN**=(I]G>=]+*=J  *MI]KKE*LI 4 L=fi>  =fi  4=fi]I]K  CE]C#SLG*E
*=7*=]L*EI  
*=fi]I]2
 EKI]*E

 LG*=fi]I]K=fi|LI    *7*EK(INLEKINL=fiKC++=J  *LE

 LkI]=fi*|LL=

=fi>+LG4EK=fi  
E L  =fi*=fi>

     *I3=|*E  =fi>4LG*E@K=fi  EL  =fiM=fi>4I*=fi]I]
 L  *=fi]I]KC4LI]4kEK*EKG 

E  EKL  =fi;  E]-G*EEM=fi4EL
  EKL=2  2   s> 7LG*E*=|=fi>  *  *I]
I]m=fi 4 ]
E =fi #4 

 ;I]

 }K=fi

K  L@LI]
9

4   *2=*E  

*=*=]LLI]  
E LG  3*= 4 EKC

LG  M
 !INmEEG4IJ(E*=J  *EKFI]I  LE4INL  J(EIKF=fi>@*=J  *LE



 4INL  =fi=fi>k|K9
fi  =fi  

 ;I7=|*  IN+IKfi!4  *HLG*E*=]L  =fi=fi>pI]KKE*LI 4 Lfi*EKEK2IINLG*E #

 

 4INL  =fi/MCiCiLKCLG*E=  =fi
 L
L  EEKI]SL  

 *EK2EK|LI  IK*  *LG*EELG*=|4=  =fi]

 *
L
INLEK

 *>=]INL  =fi
 EKL  =fiC

(44+4`48(

G  p
 EKEINGI]+!INL  I  }**m=]LEK  7LG*E
*  
L I]  
 EKEINGL  fi
= 2fi? eU  =fi24  =fi
X AC  =fi    (E@L=LG4I]-SI]-L
LEK>=]+*==fi>*EI]  *3LG  p!INmE(M    
L =fi>!*L+>=]+*=]m=fiZ
 *2LG*E7
L**=fi>I]   
 I]*I]KKE*LI 4 E7*=fi]I]K-I]-/8EK*  ->=]  EI]I]|L  K*  =fi*K
I]@E  I]LG*EI]*=fiS=fi*EK>EEKEK>=]*EK>  *fifiEK
L  =fi*I]*2K=fi2EK|L=fi2I]EIN   
E +J(E  =fi
=fi>kLG  !INmEC

N

fi;4*4-

&*Z*(44

#
	

fffi@
 "!$#%&!'&ff)(
+*-,
.0/1,
23,465798;:&2=<>:&?@AB798C2EDfi>FNHGGJILKGMG

*K

>#NO#+PQ&RSUTV#% 2"!$#%&!'&ff)ff) W"%X'&WYZW"
%J'N[#W]\^'_($`B![aJcbVdB,)ef:A4g2H5hi:j
k :[D8;lnmi4:[DJ45?o?p8C2EfiD >qSFsrutNv wKfxE

*K

y#i{zN
YB!$
([|"~}oz!$#Laf W"%[!$ff) W'N[ #W=#n%
W"!'&3"!# #% "!$#%&!'&ff)(
W
mi4:l,$,$&8C2ED&:6j7CdB,g2H7><>:&2Lj
3:2UbVdf,$:&4[,
798;l$5hf@f,[l7C1:6jn<>:&?@AB7,41H:6jg79.5&4[,E+#;TV1
I&M !*H&MJILKf& "!$W"%!i+!$'&%f

*K

's!'&#
 C#W"Ymfi+&RSoT#% "!$#%&!'&ff)ff) W"%'&W"YW#L~ 
Y%-!"!$
([
W|$'N[ #W]bVdB,
eB:&Af4g2H5h:6j k :[DJ89lQmi4:$D465&?p?o8C2E(D NqSF&rt]v wx&GKLRJ
!'N6J#gVMJ1mo k /m46:[D465&?p?p82Dj
:&44798 l
8;5hg27,Lhh8D,2Hlg,+Y"Y" ($#Wf
([ 
|'&WH}o>JQi#W"(
6!$`"[ aJnW"
%J'N[ #WH'&($
Y#W/[|"o
#ffH [
YY'N$'sH'&($&yWmi4:l,$,$&8C2ED&
:6jn7Cdf,p7CdXg27~<>:&2Lj
>5&2H)"&?@3:2 k :[D8;lQm46:[DJ45&?p?p8C2EfiD 9*]gKIE
's!$!NJx&J k :[DJ89l_5&2H_o575S$5s
,+|'NNy
%J'N[ #W'&(o'&  `"!$]84&GKGJEz 
W`"ff
z!$
([K( "~p
}B$|B!$J]+}o#>}

#&![]w&RS!$ff) W'N[#WU#i #% !$#%&!'&ff)(
\-1|^W"aJ!6
WY" W"%
( [#&![pbVdf,neB:&Af4235h:j k :[DJ89lQmi4:[DJ45?o?p8C2EfiD >qSFsrutNv Kf&M

}!$([|"#~#[	]1-~Jx!$ff) W'N[ #W#!$1!$[ W"%feB:&Af4235h~:js?^$:h8;l<>:&?@AB75798;:&2*-fi
MKM
 a'&W"(KBJy
%J'N[ #W'&(C'& `B!$i'&('&W)'N*"!$#J'&|[#H[|"1|'&W"('&W"Yff)
Y"!$ff)#]
8"!#& 
ff
Wmi4:Llg,$,$&82D&p:6jQ7Cdf,-tJ2Hg27,
42357989:2H5h"&?y@f:s8CAf?:?y -*H&GKfxE
Q#'& ([1Pn#{f!$%#]KfiMJ #% 3'&([
YcL'& 
`" `"(^#QaJ
WS[(L*o,./1,23,
45J7989:&2
<>:?y@HAf798C2E(D H- HMJxLKJIE
fi's!$$|"#&!$  JIFff)[|"#EY# #%&U#&!7"!$#aW"%;[!$ff) W'N[#W#1%
W"!'& #% 2"!$#%&!g'&ffX(

Wmi4:Llg,$,$&82D&:j7Cdf,q
7Cdg2H7,4g2H5798;:&2H5hieB:&8C27X<>:2Lj
,
46,
2Hl,:24798 l
8;5h>g27,
hCh8D,2Hlg,
 gew<yB F 4*GJI&MKGMJxE
fi's!$$|"#&!$  "MJW[!$ff) W'N[ #Wo#%
W!'&" #% @"!#%&!'&ffX(Q!Li

#W"(6!$`"[aJiW"
%J'N[#W]
bVdB,-ef:&Af4g2H5h:j k :$D8;lQm46:[D465&?p?p82ED(tf  qB3MK
fii's![|Efi"#>'
J
(z1MJ=f#ff)|  #([#]| L'&"!$#&H 
ffX(-!#ff
's! [
'&] |W [
  %
W"
&5Jl
dS8C23,g27,
hCh8DE,
2HlK, - RJMGKfI&JE
  ([([#Wf-VJ1m4g823l
8@Hh,-:j4798 l
8;5h"g27,Lhh8D,2Hlg,"!$ W"%![+!$'&%f
[!$ W"%BT+#Hf|'N!#  V&RSbVdB,4g7:jmi4:h:[DJfi_uz!$
([(


L

[|"(
$'&W"Y-]# W|o#


fiJournal of Artificial Intelligence Research 4 (1996) 37{59

Submitted 9/95; published 2/96

Logarithmic-Time Updates and Queries
in Probabilistic Networks
Arthur L. Delcher

Computer Science Department, Loyola College in Maryland
Baltimore, MD 21210

Adam J. Grove

delcher@cs.loyola.edu

grove@research.nj.nec.com

NEC Research Institute
Princeton, NJ 08540

Simon Kasif

kasif@cs.jhu.edu

Judea Pearl

pearl@lanai.cs.ucla.edu

Department of Computer Science, Johns Hopkins University
Baltimore, MD 21218
Department of Computer Science, University of California
Los Angeles, CA 90095

Abstract

Traditional databases commonly support ecient query and update procedures that
operate in time which is sublinear in the size of the database. Our goal in this paper is
to take a first step toward dynamic reasoning in probabilistic databases with comparable
eciency. We propose a dynamic data structure that supports ecient algorithms for
updating and querying singly connected Bayesian networks. In the conventional algorithm,
new evidence is absorbed in time O(1) and queries are processed in time O(N ), where N
is the size of the network. We propose an algorithm which, after a preprocessing phase,
allows us to answer queries in time O(log N ) at the expense of O(log N ) time per evidence
absorption. The usefulness of sub-linear processing time manifests itself in applications
requiring (near) real-time response over large probabilistic databases. We briey discuss a
potential application of dynamic probabilistic reasoning in computational biology.

1. Introduction
Probabilistic (Bayesian) networks are an increasingly popular modeling technique that has
been used successfully in numerous applications of intelligent systems such as real-time planning and navigation, model-based diagnosis, information retrieval, classification, Bayesian
forecasting, natural language processing, computer vision, medical informatics and computational biology. Probabilistic networks allow the user to describe the environment using
a \probabilistic database" that consists of a large number of random variables, each corresponding to an important parameter in the environment. Some random variables could in
fact be hidden and may correspond to some unknown parameters (causes) that inuence
the observable variables. Probabilistic networks are quite general and can store information
such as the probability of failure of a particular component in a computer system, the probc 1996 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiDelcher, Grove, Kasif & Pearl

ability of page i in a computer cache being requested in the near future, the probability
of a document being relevant to a particular query, or the probability of an amino-acid
subsequence in a protein chain folding into an alpha-helix conformation.
The applications we have in mind include networks that are dynamically maintained to
keep track of a probabilistic model of a changing system. For instance, consider the task of
automated detection of power-plant failures. We might repeat a cycle that consists of the
following sequence of operations: First we perform sensing operations. These operations
cause updates to be performed to specific variables in the probabilistic database. Based on
this evidence we estimate (query) the probability of failure in certain sites. More precisely,
we query the probability distribution of the random variables that measure the probability
of failure in these sites based on the evidence. Since the plant requires constant monitoring,
we must repeat the cycle of sense/evaluate on a frequent basis.
A conventional (non-probabilistic) database tracking the plant's state would not be
appropriate here, because it is not possible to directly observe whether a failure is about
to occur. On the other hand, a probabilistic \database" based on a Bayesian network
will only be useful if the operations|update and query|can be performed very quickly.
Because real-time or near real-time is so often necessary, the question of doing extremely
fast reasoning in probabilistic networks is important.
Traditional (non-probabilistic) databases support ecient query and update procedures
that often operate in time which is sublinear in the size of the database (e.g., using binary search). Our goal in this paper is to take a step toward systems that can perform
dynamic probabilistic reasoning (such as what is the probability of an event given a set of
observations) in time which is sublinear in the size of the probabilistic network. Typically,
sublinear performance in complex networks is attained by using parallelism. This paper
relies on preprocessing.
Specifically, we describe new algorithms for performing queries and updates in belief
networks in the form of trees (causal trees, polytrees and join trees). We define two natural
database operations on probabilistic networks.
1.

Update-Node

: Perform sensory input, modify the evidence at a leaf node (single
variable) in the network and absorb this evidence into the network.

2.

Query-Node

: Obtain the marginal probability distribution over the values of an
arbitrary node (single variable) in the network.

The standard algorithms introduced by Pearl (1988) can perform the Query-Node operation in O(1) time although evidence absorption, i.e., the Update-Node operation, takes
O(N ) time where N is the size of the network. Alternatively, one can assume that the
Update-Node operation takes O(1) time (by simply recording the change) and the QueryNode operation takes O(N ) time (evaluating the entire network).
In this paper we describe an approach to perform both queries and updates in O(log N )
time. This can be very significant in some systems since we improve the ability of a system to
respond after a change has been encountered from O(N ) time to O(log N ). Our approach is
based on preprocessing the network using a form of node absorption in a carefully structured
way to create a hierarchy of abstractions of the network. Previous uses of node absorption
techniques were reported by Peot and Shachter (1991).
38

fiQueries & Updates in Probabilistic Networks

We note that measuring complexity only in terms of the size of the network, N , can
overlook some important factors. Suppose that each variable in the network has domain
size k or less. For many purposes, k can be considered constant. Nevertheless, some of the
algorithms we consider have a slowdown which is some power of k, which can be become
significant in practice unless N is very large. Thus we will be careful to state this slowdown
where it exists.
Section 2 considers the case of causal trees, i.e., singly connected networks in which each
node has at most one parent. The standard algorithm (see Pearl, 1988) must use O(k2 N )
time for either updates or for retrieval, although one of these operations can be done in
O(1) time. As we discuss briey in Section 2.1, there is also a straightforward variant on
this algorithm that takes O(k2D) time for both queries and updates, where D is the height
of the tree.
We then present an algorithm that takes O(k3 log N ) time for updates and O(k2 log N )
time for queries in any causal tree. This can of course represent a tremendous speedup,
especially for large networks. Our algorithm begins with a polynomial-time preprocessing
step (linear in the size of the network), constructing another data structure (which is not
itself a probabilistic tree) that supports fast queries and updates. The techniques we use are
motivated by earlier algorithms for dynamic arithmetic trees, and involve \caching" sucient intermediate computations during the update phase so that querying is also relatively
easy. We note, however, that there are substantial and interesting differences between the
algorithm for probabilistic networks and those for arithmetic trees. In particular, as will be
apparent later, computation in probabilistic trees requires both bottom-up and top-down
processing, whereas arithmetic trees need only the former. Perhaps even more interesting is that the relevant probabilistic operations have a different algebraic structure than
arithmetic operations (for instance, they lack distributivity).
Bayesian trees have many applications in the literature including classification. For
instance, one of the most popular methods for classification is the Bayes classifier that
makes independence assumption on the features that are used to perform classification
(Duda & Hart, 1973; Rachlin, Kasif, Salzberg, & Aha, 1994). Probabilistic trees have
been used in computer vision (Hel-Or & Werman, 1992; Chelberg, 1990), signal processing
(Wilsky, 1993), game playing (Delcher & Kasif, 1992), and statistical mechanics (Berger
& Ye, 1990). Nevertheless, causal trees are fairly limited for modeling purposes. However
similar structures, called join trees, arise in the course of one of the standard algorithms for
computing with arbitrary Bayesian networks (see Lauritzen and Spiegelhalter, 1988). Thus
our algorithm for join trees has potential relevance to many networks that are not trees.
Because join trees have some special structure, they allow some optimization of the basic
causal-tree algorithm. We elaborate on this in Section 5.
In Section 6 we consider the case of arbitrary polytrees. We give an O(log N ) algorithm for updates and queries, which involves transforming the polytree to a join tree, and
then using the results of Sections 2 and 5. The join tree of a polytree has a particularly
simple form, giving an algorithm in which updates take O(kp+3 log N ) time and queries
O(kp+2 log N ), where p is the maximum number of parents of any node. Although the
constant appears large, it must be noted that the original polytree takes O(kp+1 N ) space
merely to represent, if conditional probability tables are given as explicit matrices.
39

fiDelcher, Grove, Kasif & Pearl


U

,
@

MV jU ,,
,

,

	
,
V


@

M

@ X jU
@
@
R
@


X

,
@

MY jX ,


Y

,
	
,

M

@ Z jX
@
R
@


Z


Figure 1: A segment of a causal tree.
Finally, we discuss a specific modelling application in computational biology where probabilistic models are used to describe, analyze and predict the functional behavior of biological sequences such as protein chains or DNA sequences (see Delcher, Kasif, Goldberg, and
Hsu, 1993 for references). Much of the information in computational biology databases is
noisy. However, a number of successful attempts to build probabilistic models have been
made. In this case, we use a probabilistic tree of depth 300 that consists of 600 nodes and all
the matrices of conditional probabilities are 2  2. The tree is used to model the dependence
of a protein's secondary structure on its chemical structure. The detailed description of the
problem and experimental results are given by Delcher et al. (1993). For this problem we
obtain an effective speed-up of about a factor of 10 to perform an update as compared to the
standard algorithm. Clearly, getting an order of magnitude improvement in the response
time of a probabilistic real-time system could be of tremendous importance in future use of
such systems.

2. Causal Trees

A probabilistic causal tree is a directed tree in which each node represents a discrete random
variable X , and each directed edge is annotated by a matrix of conditional probabilities
MY jX (associated with edge X ! Y ). That is, if x is a possible value of X; and y of Y;
then the (x; y )th component of MY jX is Pr(Y = y jX = x). Such a tree represents a joint
probability distribution over the product space of all variables; for detailed definitions and
discussion see Pearl (1988). Briey, the idea is that we consider the product, over all nodes,
of the conditional probability of the node given its parents. For example, in Figure 1 the
implied distribution is:
Pr(U = u; V = v; X = x; Y = y; Z = z ) =
Pr(U = u) Pr(V = v jU = u) Pr(X = xjU = u) Pr(Y = y jX = x) Pr(Z = z jX = x):
Given particular values of u; v; x; y; z; the conditional probabilities can be read from the
appropriate matrices M . One advantage of such a product representation is that it is very
40

fiQueries & Updates in Probabilistic Networks

concise. In this example, we need four matrices and the unconditional probability over U ,
but the size of each is at most the square of the largest variable's domain size. In contrast,
a general distribution over N variables requires an exponential (in N ) representation.
Of course, not every distribution can be represented as a causal tree. But it turns out
that the product decomposition implied by the tree corresponds to a particular pattern
of conditional independencies which often hold (if perhaps only approximately) in real
applications. Intuitively speaking, in Figure 1 some of these implied independencies are
that the conditional probability of U given V , X , Y and Z depends only on values of V and
X ; and the probability of Y given U , V , X , and Z depends only on X . Independencies of
this sort can arise for many reasons, for instance from a causal modeling of the interactions
between the variables. We refer the reader to Pearl (1988) for details related to the modeling
of independence assumptions using graphs.
In the following, we make several assumptions that significantly simplify the presentation, but do not sacrifice generality. First, we assume that each variable ranges over the
same, constant, number of values k.1 It follows that the marginal probability distribution
for each variable can be viewed as a k-dimensional vector, and each conditional probability
matrix such as MY jX is a square k  k matrix. A common case is that of binary random
variables (k = 2); the distribution over the values (TRUE, FALSE) is then (p; 1 , p) for
some probability p.
The next assumption is that the tree is binary, and complete, so that each node has 0
or 2 children. Any tree can be converted into this form, by at most doubling the number
of nodes. For instance, suppose node p has children c1 ; c2; c3 in the original tree. We can
create another \copy" of p, p0, and rearrange the tree such that the two children of p are
c1 and p0, and the two children of p0 are c2 and c3. We can constrain p0 always to have the
same value as p simply by choosing the identity matrix for the conditional probability table
between p and p0 . Then the distribution represented by the new tree is effectively the same
as the original. Similarly, we can always add \dummy" leaf nodes if necessary to ensure a
node has two children. As explained in the introduction, we are interested in processes in
which certain variables' values are observed, upon which we wish to condition. Our final
assumption is that these observed evidence nodes are all leaves of the tree. Again, because
it is possible to \copy" nodes and to add dummy nodes, this is not restrictive.
The product distribution alluded to above corresponds to the distribution over variables
prior to any observations. In practice, we are more interested in the conditional distribution,
which is simply the result of conditioning on all the observed evidence (which, by the earlier
assumption, corresponds to seeing values for all the leaf nodes). Thus, for each non-leaf node
X we are interested in the conditional marginal probability over X , i.e., the k-dimensional
vector:
Bel (X ) = Pr(X j all evidence values):
The main algorithmic problem is to compute Bel (X ) for each (non-evidence) node X
in the tree given the current evidence. It is well known that the probability vector Bel (X )
can be computed in linear time (in the size of the tree) by a popular algorithm based on
1. This assumption is nonrestrictive because we can add \dummy" values to each variable's range, which
should be given conditional probability 0. Nevertheless, there may some computational advantage in
allowing different variable domain sizes. The changes required to permit this are not dicult, but since
they complicate the presentation somewhat we omit them.

41

fiDelcher, Grove, Kasif & Pearl

the following equation:
Bel (X ) = Pr(X j all evidence) = ff  (X )   (X )
Here ff is a normalizing constant, (X ) is the probability of all the evidence in the subtree
below node X given X , and  (X ) is the probability of X given all evidence in the rest of the
tree. To interpret this equation, note that if X = (x1; x2; : : :; xk ) and (Y = y1 ; y2 ; : : :; yk )
are two vectors we define  to be the operation of component-wise product (pairwise or
dyadic product of vectors):
X  Y = (x1y1; x2y2; : : :; xkyk ):
The usefulness of (X ) and  (X ) derives from the fact that they can be computed recursively, as follows:
1. If X is the root node,  (X ) is the prior probability of X .
2. If X is a leaf node, (X ) is a vector with 1 in the ith position (where the ith value
has been observed) and 0 elsewhere. If no value for X has been observed, then (X )
is a vector consisting of all 1's.2
3. Otherwise, if, as shown in Figure 1, the children of node X are Y and Z , its sibling
is V and its parent is U , we have:
(X ) = (MY jX  (Y ))  (MZjX  (Z ))


(X ) = MX jU  (U )  (MV jU  (V ))
T

Our presentation of this technique follows that of Pearl (1988). However, we use a
somewhat different notation in that we don't describe messages sent to parents or successors, but rather discuss the direct relations among the  and  vectors in terms of simple
algebraic equations. We will take advantage of algebraic properties of these equations in
our development.
It is very easy to see that the equations above can be evaluated in time proportional to
the size of the network. The formal proof is given by Pearl (1988).
Theorem 1: The belief distribution of every variable (that is, the marginal probability
distribution for each variable, given the evidence) in a causal tree can be evaluated in
O(k2N ) time where N is the size of the tree. (The factor k2 is due to the multiplication
of a matrix by a vector that must be performed at each node.)
This theorem shows that it is possible to perform evidence absorption in O(N ) time, and
queries in constant time (i.e., by retrieving the previously computed values from a lookup
table). In the next sections we will show how to perform both queries and updates in
worst-case O(log N ) time. Intuitively, we will not recompute all the marginal distributions
after an update, but rather make only a small number of changes, sucient, however, to
compute the value of any variable with only a logarithmic delay.
2. Or we can set to 1 all components corresponding to possible values|this is especially useful when the
observed variable is part of a joint-tree clique (Section 5). In general, (X ) should be thought of as the
likelihood vector over X given our observations about X .

42

fiQueries & Updates in Probabilistic Networks

2.1 A Simple Preprocessing Approach

To obtain intuition about the new approach we begin with a very simple observation.
Consider a causal tree T of depth D. For each node X in the tree we initially compute its
(X ) vector.  vectors are left uncomputed. Given an update to a node Y , we calculate the
revised (X ) vectors for all nodes X that are ancestors of Y in the tree. This clearly can be
done in time proportional to the depth of the tree, i.e., O(D). The rest of the information
in the tree remains unchanged. Now consider a Query-Node operation for some node V
in the tree. We obviously already have the accurate (V ) vector for every node in the tree
including V . However, in order to compute its  (V ) vector we need to compute only the
(Y ) vectors for all the nodes above V in the tree and multiply these by the appropriate
 vectors that are kept current. This means that to compute the accurate (V ) vector we
need to perform O(D) work as well. Thus, in this approach we don't perform the complete
update to every (X ) and  (X ) vector in the tree.
Lemma 2: Update-Node and Query-Node operations in a causal tree T can be performed in O(k2 D) time where D is the depth of the tree.
This implies that if the tree is balanced, both operations can be done in O(log N )
time. However, in some important applications the trees are not balanced (e.g., models of
temporal sequences, Delcher et al., 1993). The obvious question therefore is: Given a causal
tree T can we produce an equivalent balanced tree T 0? While the answer to this question
appears to be dicult, it is possible to use a more sophisticated approach to produce a data
structure (which is not a causal tree) to process queries and updates in O(log N ) time. This
approach is described in the subsequent sections.

2.2 A Dynamic Data Structure For Causal Trees

The data structure that will allow ecient incremental processing of a probabilistic tree T =
T0 will be a sequence of trees, T0; T1; T2; : : :; Ti; : : :; Tlog N . Each Ti+1 will be a contracted
version of Ti, whose nodes are a subset of those in Ti . In particular, Ti+1 will contain about
half as many leaves as its predecessor.
We defer the details of this contraction process until the next section. However, one key
idea is that we maintain consistency, in the sense that Bel (X ); (X ); and  (X ) are given
the same values by all the trees in which X appears. We choose the conditional probability
matrices in the contracted trees (i.e., all trees other than T0 ) to ensure this.
Recall that the  and  equations have the form

(X ) = (MY jX  (Y ))  (MZjX  (Z ))


(X ) = MX jU  (U )  (MV jU  (V ))
T

if Y and Z are children of X , X is a right child of U , and V is X 's sibling (Figure 1).
However, these equations are not in the most convenient form and the following notational
conventions will be very helpful. First, let Ai (x) (resp., Bi (x)) denote the conditional
probability matrix between X and X 's left (resp., right) child in the tree Ti. Note that the
identity of these children can differ from tree to tree, because some of X 's original children
might be removed by the contraction process. One advantage of the new notation is that
43

fiDelcher, Grove, Kasif & Pearl

uj

In Ti

, @
,
@
,
@

vj

e

xj

Rake

, @
,
@

p p zjp p

(e; x)

)

uj

In Ti+1

, @
,
@
,
@

vj

p p zjp p

Figure 2: The effect of the operation Rake (e; x). e must be a leaf, but z may or may not
be a leaf.
the explicit dependence on the identity of the children is suppressed. Next, suppose X 's
parent in Ti is u. Then we let Ci (x) denote either Ai (u) or Bi (u), and Di(x) denote either
Bi (u) or Ai(u) , depending on whether X is the right or left child, respectively, of U . It
will not be necessary to keep careful track of these correspondences, but simply to note that
the above equations become:3
(x) = Ai(x)  (y)  Bi (x)  (z)
(x) = Di(x)  ((u)  Ci(x)  (v))
In the next section we describe the preprocessing step that creates the dynamic data
structure.
T

2.3

Rake

T

Operation

The basic operation used to contract the tree is Rake which removes both a leaf and its
parent from the tree. The effect of this operation on the tree is shown in Figure 2. We
now define the algebraic effect of this operation on the equations associated with this tree.
Recall that we want to define the conditional probability matrices in the raked tree so that
the distribution over the remaining variables is unchanged. We achieve this by substituting
the equations for (x) and  (x) into the equations for (u),  (z ), and  (v ). In the following,
it is important to note that  (u), (z ) and (v ) are unaffected by the rake operation.
In the following, let Diagff denote the diagonal matrix whose diagonal entries are the
components of the vector ff. We derive the algebraic effect of the rake operation as follows:
(u) = Ai (u)  (v)  Bi (u)  (x)
= Ai (u)  (v )  Bi (u)  (Ai (x)  (e)  Bi (x)  (z )) 
= Ai (u)  (v )  Bi (u)  DiagA (x)(e)  Bi (x)  (z )


= Ai (u)  (v )  Bi (u)  DiagA (x)(e)  Bi (x)  (z )
= Ai+1 (u)  (v )  Bi+1 (u)  (z )
where Ai+1 (u) = Ai (u) and Bi+1 (u) = Bi (u)  DiagA (x)(e)  Bi (x). (Of course, the case
where the leaf being raked is a right child generates analogous equations.) Thus, by defining
i

i

i

3. Throughout, we assume that  has lower precedence than matrix multiplication (indicated by ).

44

fiQueries & Updates in Probabilistic Networks

Ai+1(u) and Bi+1 (u) in this way, we ensure that all  values in the raked tree are identical

to the corresponding values in the original tree. This is not yet enough, because we must
check that  values are similarly preserved. The only two values that could possibly change
are  (z ) and  (v ), so we check them both. For the former, we must have

(z) = Di(z)  ((x)  Ci(z)  (e))
= Di+1 (z )  ( (u)  Ci+1(z )  (v )) :
After substituting for  (x) and some algebraic manipulation, we see that this is assured if
Ci+1(z) = Ci(x) and Di+1(z) = Di(z)  DiagC (z)(e)  Di(x). However recall that, by definition, Ci+1 (z ) = Ai+1 (u) and Ci (x) = Ai (u), and so Ci+1 (z ) = Ci(x) follows. Furthermore,
Di+1(z) = Bi+1(u)
= (Bi (u)  DiagA (x)(e)  Bi (x))
= Bi (x)  DiagA (x)(e)  Bi (u)
= Di(z )  DiagC (z)(e)  Di (x)
i

T

T

i

T

T

i

i

as required.
For  (v ) it is necessary to verify that

(v) = Di(v)  ((u)  Ci(v)  (x))
= Di+1 (v )  ( (u)  Ci+1 (v )  (z )) :
By substituting for (x), this can be shown to be true if Di+1(v ) = Di (v ) = Ai (u) =
Ai+1(u) and Ci+1(v) = Ci(v)  DiagA (x)(e)  Bi(x) = Bi+1(u). But these identities follow
T

T

i

by definition, so we are done.
Beginning with the given tree T = T0, each successive tree is constructed by performing
a sequence of rakes, so as to rake away about half of the remaining evidence nodes. More
specifically, let Contract be the operation in which we apply the Rake operation to every
other leaf of a causal tree, in left-to-right order, excluding the leftmost and the rightmost
leaf. Let fTig be the set of causal trees constructed so that Ti+1 is the causal tree generated
from Ti by a single application of Contract. The following result is proved using an easy
inductive argument:

Theorem 3: Let T0 be a causal tree of size N . Then the number of leaves in Ti+1 is equal

to half the leaves in Ti (not counting the two extreme leaves) so that starting with T0,
after O(log N ) applications of Contract, we produce a three-node tree: the root, the
leftmost leaf and the rightmost leaf.
Below are a few observations about this process:
1. The complexity of Contract is linear in the size of the tree. Additionally, log N applications of Contract reduce the set of tree equations to a single equation involving
the root in O(N ) total time.
2. The total space to store all the sets of equations associated with fTi g0ilog N is about
twice the space required to store the equations for T0.
45

fiDelcher, Grove, Kasif & Pearl

3. With each equation in Ti+1 we also store equations that describe the relationship
between the conditional probability matrices in Ti+1 to the matrices in Ti . Notice
that, even though Ti+1 is produced from Ti by a series of rake operations, each matrix
in Ti+1 depends directly on matrices present in Ti. This would not be the case if we
attempted to simultaneously rake adjacent children.
We regard these equations as part of Ti+1. So, formally speaking fTig are causal trees
augmented with some auxiliary equations. Each of the contracted trees describes a
probability distribution on a subset of the first set of variables that is consistent with
the original distribution.
We note that the ideas behind the Rake operation were originally developed by Miller
and Reif (1985) in the context of parallel computation of bottom-up arithmetic expression
trees (Kosaraju & Delcher, 1988; Karp & Ramachandran, 1990). In contrast, we are using
it in the context of incremental update and query operations in sequential computing. A
similar data structure to ours was independently proposed by Frederickson (1993) in the
context of dynamic arithmetic expression trees, and a different approach for incremental
computing on arithmetic trees was developed by Cohen and Tamassia (1991). There are
important and interesting differences between the arithmetic expression-tree case and our
own. For arithmetic expressions all computation is done bottom-up. However, in probabilistic networks  -messages must be passed top-down. Furthermore, in arithmetic expressions
when two algebraic operations are allowed, we typically require the distributivity of one
operation over the other, but the analogous property does not hold for us. In these respects our approach is a substantial generalization of the previous work, while remaining
conceptually simple and practical.

3. Example: A Chain

To obtain an intuition about the algorithms, we sketch how to generate and utilize the
Ti; 0  i  log N and their equations to perform -value queries and updates in O(log N )
time on an N = 2L + 1 node chain of length L. Consider the chain of length 4 in Figure 3,
and the trees that are generated by repeated application of Contract to the chain.
The equations that correspond to the contracted trees in the figure are as follows (ignoring trivial equations). Recall that Ai (xj ) is the matrix associated with the left edge of
random variable xj in Ti.

(x1)
(x2)
(x3)
(x4)

=
=
=
=

A0(x1)  (e1)  B0(x1)  (x2)
A0(x2)  (e2)  B0(x2)  (x3)
A0(x3)  (e3)  B0(x3)  (x4)
A0(x4)  (e4)  B0(x4)  (e5)

9
>
>
>
>
>
>
=
>
>
>
B0 (x1)  DiagA0 (x2)(e2)  B0(x2) >
>
>
B0 (x3)  DiagA0 (x4)(e4)  B0(x4) ;

(x1) = A1(x1)  (e1)  B1(x1)  (x3)
(x3) = A1(x3)  (e3)  B1(x3)  (e5)

where
B1(x1) =
B1(x3) =

9
>
>
>
=
>
>
>
;

46

for T0

for T1

fiQueries & Updates in Probabilistic Networks

T0 :

xm
1
em
1
?

T1 :

xm
1
em
1
?

T2 :

xm
1

xm - xm
3 - xm
4 - e5m

- 2

em
2

em
3

?

em
4

?

?

xm - em
5

- 3

em
3
?

em

- 5

em
1
?

Figure 3: A simple chain example.

(x1) = A2(x1)  (e1)  B2(x1)  (e5)

9
>
>
=
>
>
;

where
for T2
B2(x1) = B1 (x1)  DiagA (x )(e )  B1(x3)
We have not listed the A matrices because, in this example, they are constant. Now
consider a query operation on x2 . Rather than performing the standard computation we
will find the level where x2 was \raked". Since this occurred on level 0, we obtain the
equation
(x2) = A0(x2)  (e2)  B0(x2)  (x3)
Thus we must compute (x3), and to do this we find where x3 is \raked". That happened
on level 1. However, on that level the equation associated with x3 is:
(x3) = A1(x3)  (e3)  B1(x3)  (e5)
That means that we need not follow down the chain. In general for a chain of N nodes we
can answer any query to a node on the chain by evaluating log N equations instead of N
equations.
Now consider an update for e4 . Since e4 was raked immediately, we first modify the
equation
B1(x3) = B0(x3)  DiagA (x )(e )  B0(x4)
on the first level where e4 occurs on the right-hand side. Since B1 (x3) is affected by the
change to e4 , we subsequently modify the equation
B2(x1) = B1(x1)  DiagA (x )(e )  B1(x3)
1

47

3

3

0

4

4

1

3

3

fiDelcher, Grove, Kasif & Pearl

on the second level. In general, we clearly need to update at most log N equations; i.e., one
per level. We now generalize this example and describe general algorithms for queries and
updates in causal trees.

3.1 Performing Queries And Updates Eciently

In this section we shall show how to utilize the contracted trees Ti; 0  i  log N to
perform queries and updates in O(log N ) time in general causal trees. We shall show that a
logarithmic amount of work will be necessary and sucient to compute enough information
in our data structure to update and query any  or  value.

3.2  Queries

To compute (x) for some node x we can do the following. We first locate ind (x), which is
defined to be the highest level i such that x appears in Ti . The equation for (x) is of the
form:
(x) = Ai(x)  (y)  Bi(x)  (z)
where y and z are the left and right children, respectively, of x in Ti.
Since x does not appear in Ti+1 , it was raked at this level of equations, which implies
that one child (we assume z ) is a leaf. We therefore only need to compute (y ), which can
be done recursively. If instead y was the raked leaf, we would compute (z ) recursively.
In either case O(1) operations are done in addition to one recursive call, which is to a
value at a higher level of equations. Since there are O(log N ) levels, and the only operations
are matrix by vector multiplications, the procedure takes O(k2 log N ) time. The function
-Query (x) is given in Figure 4.

3.3 Updates

We now describe how the update operations can modify enough information in the data
structure to allow us to query the  vectors and  vectors eciently. Most importantly the
reader should note that the update operation does not try to maintain the correct  and
 values. It is sucient to ensure that, for all i and x, the matrices Ai(x) and Bi (x) (and
thus also Ci (x) and Di(x)) are always up to date.
When we update the value of an evidence node, we are simply changing the  value of
some leaf e. At each level of equations, the value of (e) can appear at most twice: once
in the -equation of e's parent and once in the  -equation of e's sibling in Ti. When e
disappears, say at level i, its value is incorporated into one of the constant matrices Ai+1 (u)
or Bi+1 (u) where u is the grandparent of e in Ti . This constant matrix in turn affects
exactly one constant matrix in the next higher level, and so on. Since the effect at each
level can be computed in O(k3 ) time (due to matrix multiplication) and there are O(log N )
levels of equations, the update can be accomplished in O(k3 log N ) time. The constant k3
is actually pessimistic, because faster matrix multiplication algorithms exist.
The update procedure is given in Figure 5. Update is initially called as Update((E ) =
e; i) where E is a leaf, i the level at which it was raked, and e is the new evidence. This
operation will start a sequence of O(log N ) calls to function -Update (X = Term; i) as
the change will propagate to log N equations.
48

fiQueries & Updates in Probabilistic Networks

FUNCTION -Query (x)
We look up the equation associated with (x) in Tind (x).
Case 1: x is a leaf. Then the equation is of the form: (x) = e where e is known. In
this case we return e.
Case 2: The equation associated with (x) is of the form

(x) = Ai(x)  (y)  Bi (x)  (z)
where z is a leaf and therefore (z ) is known. In this case we return
Ai(X )  -Query (y)  Bi (X )  (z)
The case where y is the leaf is analogous.
Figure 4: Function to compute the  value of a node.

3.4  Queries

It is relatively easy to use a similar recursive procedure to perform  (x) queries. Unfortunately, this approach yields an O(log2N )-time algorithm if we simply use recursion to
calculate  terms and calculate  terms using our earlier procedure. This is because there
will be O(log N ) recursive calls to calculate  values, but each is defined by an equation
that also involves a  term taking O(log N ) time to compute.
To achieve O(log N ) time, we shall instead implement  (x) queries by defining a procedure Calc (x; i) which returns a triple of vectors hP; L; Ri such that P =  (x), L = (y )
and R = (z ) where y and z are the left and right children, respectively, of x in Ti.
To compute  (x) for some node x we can do the following. Let i = ind (x). The equation
for  (x) in Ti is of the form:

(x) = Di(x)  ((u)  Ci(x)  (v))
where u is the parent of x in Ti and v its sibling. We then call procedure Calc (u; i + 1)
which will return the triple h (u); (v); (x)i, from which we immediately can compute  (x)
using the above equation.
Procedure Calc (x; i) can be implemented in the following fashion.
Case 1: If Ti is a 3-node tree with x as its root, then both children of x are leaves, hence
their  values are known, and  (x) is a given sequence of prior probabilities for x.
Case 2: If x does not appear in Ti+1 , then one of x's children is a leaf, say e which is raked
at level i. Let z be the other child. We call Calc (u; i + 1), where u is the parent of
x in Ti, and receive back h(u); (z); (v)i or h(u); (v); (z)i according to whether x
49

fiDelcher, Grove, Kasif & Pearl

FUNCTION -Update (Term = Value; i)
1. Find the (at most one) equation in Ti , defining some Ai or Bi , in which Term
appears on the right-hand side; let Term0 be the matrix defined by this equation
(i.e., its left-hand side).
2. Update Term0; let Value be the new value.
3. Call -Update (Term0 = Value; i + 1) recursively.
Figure 5: The update procedure.
was a left or right child of u in Ti (and v is u's other child). We can now compute  (x)
from  (u) and (v ), and we have (e) and (z ), so we can return the necessary triple.
Specifically,

(x) =

(

Di(x)  ((u)  Ai+1 (u)  (v))
Di(x)  ((u)  Bi+1 (u)  (v))

where the choice depends on whether x is the right or left child, respectively, of u in Ti.
Case 3: If x does appear in Ti+1, then we call Calc (x; i + 1). This returns the correct
value of  (x). For any child z of x in Ti that remains a child of x in Ti+1 , it also returns
the correct value of (z ). If z is a child of x that does not occur in Ti+1 , then it must be
the case that z was raked at level i so that one of z 's children, say e, is a leaf and let the
other child be q . In this situation Calc (x; i + 1) has returned the value of (q ) and
we can compute

(z) = Ai(z)  (e)  Bi (z)  (q)
and return this value.
In all three cases, there is a constant amount of work done in addition to a single recursive
call that uses equations at a higher level. Since there are O(log N ) levels of equations, each
requiring only matrix by vector multiplication, the total work done is O(k2 log N ).

4. Extended Example
In this section we illustrate the application of our algorithms to a specific example. Consider
the sequence of contracted trees shown in Figure 6. Corresponding to these trees we have
50

fiQueries & Updates in Probabilistic Networks

xl
1

T0 :







xl

#
#
#
#

2

xl
4

xl
6

 A
 A

A

e1

xl
8

 A
 A

A

e2

e4



e8

 A
 A

A

 A
 A

A

xl
2

 A
 A

xl
5

xl
7

xl
3

 A
 A
A


Z
Z

c
c
c
c

, @
@
@

,
,

xl
1

T1 :

Z
Z
Z

e6

 A
 A
A


xl
4

A

e9

 A
 A
A


xl
6

e7

 A
 A

A

e5

e1

e3

T2:

xl
1

 A
 A

A

e1

e3

T3: xl
1

 A
 A
A


xl
4

e5

e9

e5

Figure 6: Example of tree contraction.

51

 A
 A

A

e1

e9

e7

e9

fiDelcher, Grove, Kasif & Pearl

such equations as the following:
For T0 :
(x1) = A0(x1 ) (x2 ) B0 (x1 ) (x3 )
..
.





(x2) = D0 (x2) ((x1) C0(x2) (x3 ))
..
.





For T1 :
(x1) = A1(x1 ) (x2 ) B1 (x1 ) (e9 )
..
.











(x2) = D1 (x2) ((x1) C1(x2) (e9 ))
..
.





For T2 :
(x1) = A2(x1 ) (x4 ) B2 (x1 ) (e9 )
..
.







(x4) = D2 (x4) ((x1) C2(x4) (e9 ))
..
.









For T3 :
(x1) = A3(x1 ) (e1 ) B3 (x1) (e9 )






Now consider, for instance, the effect of an update for e2 . Since it is raked immediately,
the new value of (e2) is incorporated in:
B1 (x6 ) = B0 (x6 ) DiagA0 (x8 ) (e2) B0 (x8 )
From subsequent Rake operations we know that A2(x4 ) depends on B1 (x6), and A3 (x1)
depends on A2 (x4), so we must also update these values as follows:
A2 (x4 ) = A1 (x4) DiagB1 (x6 ) (e3 ) A1 (x6)
A3 (x1 ) = A2 (x1) DiagB2 (x4 ) (e5 ) A2 (x4)


















Finally, consider a query for x7 . Since x7 is raked together with e5 in T0 , we follow
the steps outlined above and generate the following calls: Calc (x7; 0), Calc (x4; 1),
Calc (x4; 2), and Calc (x1; 3). This provides us with  (x7). In this case, (x7)
is particularly easy to compute since both x7 's children are leaf nodes. Then we simply
compute  (x7)  (x7) and then normalize, giving us the conditional marginal distribution
Bel (x7) as required.

5. Join Trees

Perhaps the best-known technique for computing with arbitrary (i.e., not singly-connected)
Bayesian networks uses the idea of join trees (junction trees) (Lauritzen & Spiegelhalter,
1988). In many ways a join tree can be thought of as a causal tree, albeit one with somewhat
special structure. Thus the algorithm in the previous section can be applied. However, the
structure of a join tree permits some optimization, which we describe in this section. This
becomes especially relevant in the next section, where we use the join-tree technique to
show how O(log N ) updates and queries can be done for arbitrary polytrees. Our review
of join-trees and their utility is extremely brief and quite incomplete; for clear expositions
see, for instance, Spiegelhalter et al. (1993) and Pearl (1988).
Given any Bayesian network, the first step towards constructing a join-tree is to moralize
the network: insert edges between every pair of parents of a common node, and then treat all
52

fiQueries & Updates in Probabilistic Networks

edges in the graph as being undirected (Spiegelhalter et al., 1993). The resulting undirected
graph is called the moral graph. We are interested in undirected graphs that are chordal :
every cycle of length 4 or more should contain a chord (i.e., an edge between two nodes
that are non-adjacent in the cycle). If the moral graph is not chordal, it is necessary to add
edges to make it so; various techniques for this triangulation stage are known (for instance,
see Spiegelhalter et al., 1993).
If p is a probability distribution represented in a Bayesian network G = (V; E ), and
M = (V; F ) is the result of moralizing and then triangulating G, then:
1. M has at most jV j cliques,4 say C1; : : :; CjV j.
2. The cliques can be ordered so that for each i > 1 there is some j (i) < i such that

Ci \ Cj(i) = Ci \ (C1 [ C2 [ : : : [ Ci,1:)
The tree T formed by treating the cliques as nodes, and connecting each node Ci to
its \parent" Cj (i), is called a join tree.
3. p =

Y

i

p(CijCj(i))

4. p(CijCj (i)) = p(CijCj (i) \ Ci )
From 2 and 3, we see that if we direct the edges in T away from the \parent" cliques,
the resulting directed tree is in fact a Bayesian causal tree that can represent the original
distribution p. This is true no matter what the form of the original graph. Of course, the
price is that the cliques may be large, and so the domain size (the number of possible values
of a clique node) can be of exponential size. This is why this technique is not guaranteed
to be ecient.
We can use the Rake technique of Section 2 on the directed join tree without any
modification. However, property 4 above shows that the conditional probability matrices
in the join tree have a special structure. We can use this to gain some eciency. In the
following, let k be the domain size of the variables in G as usual. Let n be the maximum
size of cliques in the join tree; without loss of generality we can assume that all cliques are
of the same size (because we can add \dummy" variables). Thus the domain size of each
clique is K = kn . Finally, let c be the maximum intersection size of a clique and its parent
(i.e., jCj (i) \ Cij) and L = kc .
In the standard algorithm, we would represent p(CijCj (i)) as a K  K matrix, MC jC .
However, p(Ci jCj (i) \ Ci) can be represented as a smaller L  K matrix, MC jC \C . By
property 4 above, MC jC is identical to MC jC \C , except that many rows are repeated.
Thus there is a K  L matrix J such that
i

i

i

i

j (i)

i

j (i)

i

i

j (i)

MC jC = J  MC jC
i

j (i)

j (i)

j (i)

\Ci :

(J is actually a simple matrix whose entries are 0 and 1, with exactly one 1 per row; however
we do not use this fact.)
4. A clique is a maximal completely-connected subgraph.

53

fiDelcher, Grove, Kasif & Pearl

Our claim is that, in the case of join trees, the following is true. First, the matrices

Ai and Bi used in the Rake algorithm can be stored in factored form, as the product of
two matrices of dimension K  L and L  K respectively. So, for instance, we factor Ai
as Ali  Ari . We never need to explicitly compute, or store, the full matrices. As we have
just seen, this claim is true when i = 0 because the M matrices factor this way. The proof
for i > 1 uses an inductive argument, which we illustrate below. The second claim is that,

when the matrices are stored in factored form, all the matrix multiplications used in the
Rake algorithm are of one of the following types: 1) an L  K matrix times a K  L matrix,
2) an L  K matrix times a K  K diagonal matrix, 3) an L  L matrix times an L  K
matrix, or 4) an L  K matrix times a vector.
To prove these claims consider, for instance, the equation defining Bi+1 in terms of lowerlevel matrices. From Section 2, Bi+1 (u) = Bi (u)  DiagA (x)(e)  Bi (x): But, by assumption,
this is:
(Bil (u)  Bir (u))  Diag(A (x)A (x))(e)  (Bil (x)  Bil (x));
which, using associativity, is clearly equivalent to
h
i
Bil (u)  ((Bir (u)  DiagA (x)(A (x)(e)))  Bil(x))  Bil (x) :
However, every multiplication in this expression is one of the forms stated earlier. Identifying
Bil+1 (u) as Bil(u) and Bir+1 (u) as the bracketed part of the expression proves this case, and
of course the case where we rake a left child (so that Ai+1 (u) is updated) is analogous.
Thus, even using the most straightforward technique for matrix multiplication, the cost of
updating Bi+1 is O(KL2) = O(kn+2c ). This contrasts with O(K 3) if we do not factor the
matrices, and may represent a worthwhile speedup if c is small. Note that the overall time
for an update using this scheme is O(kn+2c log N ). Queries, which only involve matrix by
vector multiplication, require O(kn+c log N ) time.
For many join trees the difference between N and log N is unimportant, because the
clique domain size K is often enormous and dominates the complexity. Indeed, K and L
may be so large that we cannot represent the required matrices explicitly. Of course, in such
cases our technique has little to offer. But there will be other cases in which the benefits
will be worthwhile. The most important general class in which this is so, and our immediate
reason for presenting the technique for join trees, is the case of polytrees.
i

l
i

r
i

l
i

r
i

6. Polytrees

A polytree is a singly connected Bayesian network; we drop the assumption of Section 2
that each node has at most one parent. Polytrees offer much more exibility than causal
trees, and yet there is a well-known process that can update and query in O(N ) time, just
as for causal trees. For this reason polytrees are an extremely popular class of networks.
We suspect that it is possible to present an O(log N ) algorithm for updates and queries
in polytrees, as a direct extension of the ideas in Section 2. Instead we propose a different
technique, which involves converting a polytree to its join tree and then using the ideas of
the preceding section. The basis for this is the simple observation that the join tree of a
polytree is already chordal. Thus (as we show in detail below) little is lost by considering
the join tree instead of the original polytree. The specific property of polytrees that we
require is the following. We omit the proof of this well-known proposition.
54

fiQueries & Updates in Probabilistic Networks

Proposition 4: If T is the moral graph of a polytree P = (V; E ) then T is chordal, and
the set of maximal cliques in T is ffv g [ parents (v ) : v 2 V g.
Let p be the maximum number of parents of any node. From the proposition, every
maximal clique in the join tree has at most p +1 variables, and so the domain size of a node
in the join tree is K = kp+1 . This may be large, but recall that the conditional probability
matrix in the original polytree, for a variable with p parents, has K entries anyway since we
must give the conditional distribution for every combination of the node's parents. Thus K
is really a measure of the size of the polytree itself.
It now follows from the proposition above that we can perform query and update in
polytrees in time O(K 3 log N ), simply by using the algorithm of Section 2 on the directed
join tree. But, as noted in Section 5, we can do better. Recall that the savings depend on
c, the maximum size of the intersection between any node and its parent in the join tree.
However, when the join tree is formed from a polytree, no two cliques can share more than a
single node. This follows immediately from Proposition 4, for if two cliques have more than
one node in common then there must be either two nodes that share more than one parent,
or else a node and one of its parents that both share yet another parent. Neither of these is
consistent with the network being a polytree. Thus in the complexity bounds of Section 5,
we can put c = 1. It follows that we can process updates in O(Kk2c log N ) = O(kp+3 log N )
time and queries in O(kp+2 log N ).

7. Application: Towards Automated Site-Specific Muta-Genesis

An experiment which is commonly performed in biology laboratories is a procedure where
a particular site in a protein is changed (i.e., a single amino-acid is mutated) and then
tested to see whether the protein settles into a different conformation. In many cases, with
overwhelming probability the protein does not change its secondary structure outside the
mutated region. This process is often called muta-genesis. Delcher et al. (1993) developed a
probabilistic model of a protein structure which is basically a long chain. The length of the
chain varies between 300{500 nodes. The nodes in the network are either protein-structure
nodes (PS-nodes) or evidence nodes (E-nodes). Each PS-node in the network is a discrete
random variable Xi that assumes values corresponding to descriptors of secondary sequence
structure: helix, sheet or coil. With each PS-node the model associates an evidence node
that corresponds to an occurrence of a particular subsequence of amino acids at a particular
location in the protein.
In our model, protein-structure nodes are finite strings over the alphabet fh; e ; c g. For
example the string hhhhhh is a string of six residues in an ff-helical conformation, while
eecc is a string of two residues in a fi -sheet conformation followed by two residues folded as
a coil. Evidence nodes are nodes that contain information about a particular region of the
protein. Thus, the main idea is to represent physical and statistical rules in the form of a
probabilistic network.
In our first set of experiments we converged on the following model that, while clearly
biologically naive, seems to match in prediction accuracy many existing approaches such as
neural networks. The network looks like a set of PS-nodes connected as a chain. To each
such node we connect a single evidence node. In our experiments the PS-nodes are strings
of length two or three over the alphabet fh; e ; c g and the evidence nodes are strings of the
55

fiDelcher, Grove, Kasif & Pearl


cc

?
GS


 ch
 ?
 SA


 hh
 ?
 AT






Figure 7: Example of causal tree model using pairs, showing protein segment GSAT with
corresponding secondary structure cchh.
same length over the set of amino acids. The following example clarifies our representation.
Assume we have a string of amino acids GSAT. We model the string as a network comprised
of three evidence nodes GS, SA, AT and three PS-nodes. The network is shown in Figure 7.
A correct prediction will assign the values cc, ch, and hh to the PS-nodes as shown in the
figure.
Now that we have a probabilistic model, we can test the robustness of the protein or
whether small changes in the protein affect the structure of certain critical sites in the
protein. In our experiments, the probabilistic network performs a \simulated evolution" of
the protein, namely the simulator repeatedly mutates a region in the chain and then tests
whether some designated sites in the protein that are coiled into a helix are predicted to
remain in this conformation. The main goal of the experiment was to test if stable bonds far
away from the mutated location were affected. Our previous results (Delcher et al., 1993)
support the current thesis in the biology community, namely that local distant changes
rarely affect structure.
The algorithms we presented in the previous sections of the paper are perfectly suited
for this type of application and are predicted to generate a factor of 10 improvement in
eciency over the current brute-force implementation presented by Delcher et al. (1993)
where each change is propagated throughout the network.

8. Summary
This paper has proposed several new algorithms that yield a substantial improvement in the
performance of probabilistic networks in the form of causal trees. Our updating procedures
absorb sucient information in the tree such that our query procedure can compute the
correct probability distribution of any node given the current evidence. In addition, all
procedures execute in time O(log N ), where N is the size of the network. Our algorithms
are expected to generate orders-of-magnitude speed-ups for causal trees that contain long
paths (not necessarily chains) and for which the matrices of conditional probabilities are
relatively small. We are currently experimenting with our approach with singly connected
networks (polytrees). It is likely to be more dicult to generalize the techniques to general
networks. Since it is known that the general problem of inference in probabilistic networks is
NP -hard (Cooper, 1990), it obviously is not possible to obtain polynomial-time incremental
56

fiQueries & Updates in Probabilistic Networks

solutions of the type discussed in this paper for general probabilistic networks. The other
natural open question is extending the approach developed in this paper to other dynamic
operations on probabilistic networks such as addition and deletion of nodes and modifying
the matrices of conditional probabilities (as a result of learning).
It would also be interesting to investigate the practical logarithmic-time parallel algorithms for probabilistic networks on realistic parallel models of computation. One of the
main goals of massively parallel AI research is to produce networks that perform real-time
inference over large knowledge-bases very eciently (i.e., in time proportional to the depth
of the network rather than the size of the network) by exploiting massive parallelism. Jerry
Feldman pioneered this philosophy in the context of neural architectures (see Stanfill and
Waltz, 1986, Shastri, 1993, and Feldman and Ballard, 1982). To achieve this type of performance in the neural network framework, we typically postulate a parallel hardware that
associates a processor with each node in a network and typically ignores communication requirements. With careful mapping to parallel architectures one can indeed achieve ecient
parallel execution of specific classes of inference operations (see Mani and Shastri, 1994,
Kasif, 1990, and Kasif and Delcher, 1992). The techniques outlined in this paper presented
an alternative architecture that supports very fast (sub-linear time) response capability on
sequential machines based on preprocessing. However, our approach is obviously limited to
applications where the number of updates and queries at any time is constant. One would
naturally hope to develop parallel computers that support real-time probabilistic reasoning
for general networks.

Acknowledgements
Simon Kasif's research at Johns Hopkins University was sponsored in part by National
Science foundation under Grants No. IRI-9116843, IRI-9223591 and IRI-9220960.

References

Berger, T., & Ye, Z. (1990). Entropic aspects of random fields on trees. IEEE Trans. on
Information Theory, 36 (5), 1006{1018.
Chelberg, D. M. (1990). Uncertainty in interpretation of range imagery. In Proc. Intern.
Conference on Computer Vision, pp. 654{657.
Cohen, R. F., & Tamassia, R. (1991). Dynamic trees and their applications. In Proceedings
of the 2nd ACM-SIAM Symposium on Discrete Algorithms, pp. 52{61.
Cooper, G. (1990). The computational complexity of probabilistic inference using bayes
belief networks. Artificial Intelligence, 42, 393{405.
Delcher, A., & Kasif, S. (1992). Improved decision making in game trees: Recovering from
pathology. In Proceedings of the 1992 National Conference on Artificial Intelligence.
Delcher, A. L., Kasif, S., Goldberg, H. R., & Hsu, B. (1993). Probabilistic prediction of protein secondary structure using causal networks. In Proceedings of 1993 International
Conference on Intelligent Systems for Computational Biology, pp. 316{321.
57

fiDelcher, Grove, Kasif & Pearl

Duda, R., & Hart, P. (1973). Pattern Classification and Scene Analysis. Wiley, New York.
Feldman, J. A., & Ballard, D. (1982). Connectionist models and their properties. Cognitive
Science, 6, 205{254.
Frederickson, G. N. (1993). A data structure for dynamically maintaining rooted trees. In
Proc. 4th Annual Symposium on Discrete Algorithms, pp. 175{184.
Hel-Or, Y., & Werman, M. (1992). Absolute orientation from uncertain data: A unified
approach. In Proc. Intern. Conference on Computer Vision and Pattern Recognition,
pp. 77{82.
Karp, R. M., & Ramachandran, V. (1990). Parallel algorithms for shared-memory machines.
In Van Leeuwen, J. (Ed.), Handbook of Theoretical Computer Science, pp. 869{941.
North-Holland.
Kasif, S. (1990). On the parallel complexity of discrete relaxation in constraint networks.
Artificial Intelligence, 45, 275{286.
Kasif, S., & Delcher, A. (1994). Analysis of local consistency in parallel constraint networks.
Artificial Intelligence, 69.
Kosaraju, S. R., & Delcher, A. L. (1988). Optimal parallel evaluation of tree-structured
computations by raking. In Reif, J. H. (Ed.), VLSI Algorithms and Architectures:
Proceedings of 1988 Aegean Workshop on Computing, pp. 101{110. Springer Verlag.
LNCS 319.
Lauritzen, S., & Spiegelhalter, D. (1988). Local computations with probabilities on graphical
structures and their applications to expert systems. J. Royal Statistical Soc. Ser. B,
50, 157{224.
Mani, D., & Shastri, L. (1994). Massively parallel reasoning with very large knowledge
bases. Tech. rep., Intern. Computer Science Institute.
Miller, G. L., & Reif, J. (1985). Parallel tree contraction and its application. In Proceedings
of the 26th IEEE Symposium on Foundations of Computer Science, pp. 478{489.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann.
Peot, M. A., & Shachter, R. D. (1991). Fusion and propagation with multiple observations
in belief networks. Artificial Intelligence, 48, 299{318.
Rachlin, J., Kasif, S., Salzberg, S., & Aha, D. (1994). Towards a better understanding of
memory-based and bayesian classifiers. In Proceedings of the Eleventh International
Conference on Machine Learning, pp. 242{250 New Brunswick, NJ.
Shastri, L. (1993). A computational model of tractable reasoning: Taking inspiration from
cognition. In Proceeding of the 1993 Intern. Joint Conference on Artificial Intelligence.
AAAI.
58

fiQueries & Updates in Probabilistic Networks

Spiegelhalter, D., Dawid, A., Lauritzen, S., & Cowell, R. (1993). Bayesian analysis in expert
systems. Statistical Science, 8 (3), 219{283.
Stanfill, C., & Waltz, D. (1986). Toward memory-based reasoning. Communications of the
ACM, 29 (12), 1213{1228.
Wilsky, A. (1993). Multiscale representation of markov random fields. IEEE Trans. Signal
Processing, 41, 3377{3395.

59

fiJournal of Artificial Intelligence Research 4 (1996) 91-128

Submitted 6/95; published 3/96

Quantum Computing and Phase Transitions in
Combinatorial Search
Tad Hogg

Xerox Palo Alto Research Center
3333 Coyote Hill Road Palo Alto, CA 94304 USA

hogg@parc.xerox.com

Abstract

We introduce an algorithm for combinatorial search on quantum computers that is capable of significantly concentrating amplitude into solutions for some NP search problems,
on average. This is done by exploiting the same aspects of problem structure as used by
classical backtrack methods to avoid unproductive search choices. This quantum algorithm
is much more likely to find solutions than the simple direct use of quantum parallelism. Furthermore, empirical evaluation on small problems shows this quantum algorithm displays
the same phase transition behavior, and at the same location, as seen in many previously
studied classical search methods. Specifically, dicult problem instances are concentrated
near the abrupt change from underconstrained to overconstrained problems.

1. Introduction

Computation is ultimately a physical process (Landauer, 1991). That is, in practice the
range of physically realizable devices determines what is computable and the resources,
such as computer time, required to solve a given problem. Computing machines can exploit
a variety of physical processes and structures to provide distinct trade-offs in resource
requirements. An example is the development of parallel computers with their trade-off of
overall computation time against the number of processors employed. Effective use of this
trade-off can require algorithms that would be very inecient if implemented serially.
Another example is given by hypothetical quantum computers (DiVincenzo, 1995). They
offer the potential of exploiting quantum parallelism to trade computation time against the
use of coherent interference among very many different computational paths. However,
restrictions on physically realizable operations make this trade-off dicult to exploit for
search problems, resulting in algorithms essentially equivalent to the inecient method of
generate-and-test. Fortunately, recent work on factoring (Shor, 1994) shows that better
algorithms are possible. Here we continue this line of work by introducing a new quantum algorithm for some particularly dicult combinatorial search problems. While this
algorithm represents a substantial improvement for quantum computers, it is particularly
inecient as a classical search method, both in memory and time requirements.
When evaluating algorithms, computational complexity theory usually focuses on the
scaling behavior in the worst case. Of particular theoretical concern is whether the search
cost grows exponentially or polynomially. However, in many practical situations, typical
or average behavior is of more interest. This is especially true because many instances
of search problems are much easier to solve than is suggested by worst case analyses. In
fact, recent studies have revealed an important regularity in the class of search problems.
Specifically, for a wide variety of search methods, the hard instances are not only rare but
c 1996 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiHogg

also concentrated near abrupt transitions in problem behavior analogous to physical phase
transitions (Hogg, Huberman, & Williams, 1996). To exhibit this concentration of hard
instances a search algorithm must exploit the problem constraints to prune unproductive
search choices. Unfortunately, this is not easy to do within the range of allowable quantum
computational operations. It is thus of interest to see if these results generalize to quantum
search methods as well.
In this paper, the new algorithm is evaluated empirically to determine its average behavior. The algorithm is also shown to exhibit the phase transition, indicating it is indeed
managing to, in effect, prune unproductive search. This leaves for future work the analysis
of its worst case performance.
This paper is organized as follows. First we discuss combinatorial search problems
and the phase transitions where hard problem instances are concentrated. Second, after
a brief summary of quantum computing, the new quantum search algorithm is motivated
and described. In fact, there are a number of natural variants of the general algorithm.
Two of these are evaluated empirically to exhibit the generality of the phase transition and
their performance. Finally, some important caveats for the implementation of quantum
computers and open issues are presented.

2. Combinatorial Search

Combinatorial search is among the hardest of common computational problems: the solution
time can grow exponentially with the size of the problem (Garey & Johnson, 1979). Examples arise in scheduling, planning, circuit layout and machine vision, to name a few areas.
Many of these examples can be viewed as constraint satisfaction problems (CSPs) (Mackworth, 1992). Here we are given a set of n variables each of which can be assigned b possible
values. The problem is to find an assignment for each variable that together satisfy some
specified constraints. For instance, consider the small scheduling problem of selecting one
of two periods in which to teach each of two classes that are taught by the same person.
We can regard each class as a variable and its time slot as its value, i.e., here n = b = 2.
The constraints are that the two classes are not assigned to be at the same time.
Fundamentally, the combinatorial search problem consists of finding those combinations
of a discrete set of items that satisfy specified requirements. The number of possible combinations to consider grows very rapidly (e.g., exponentially or factorially) with the number
of items, leading to potentially lengthy solution times and severely limiting the feasible size
of such problems. For example, the number of possible assignments in a constraint problem
is bn , which grows exponentially with the problem size (given by the number of variables
n).
Because of the exponentially large number of possibilities it appears the time required
to solve such problems must grow exponentially, in the worst case. However for many such
problems it is easy to verify a solution is in fact correct. These problems form the wellstudied class of NP problems: informally we say they are hard to solve but easy to check.
One well-studied instance is graph coloring, where the variables represent nodes in a graph,
the values are colors for the nodes and the constraints are that each pair of nodes linked by an
edge in the graph must have different colors. Another example is propositional satisfiability
(SAT), where the variables take on logical values of true or false, and the assignment must
92

fiQuantum Computing and Phase Transitions in Combinatorial Search

satisfy a specified propositional formula involving the variables. Both these examples are
instances of particularly dicult NP problems known as the class of NP-complete search
problems (Garey & Johnson, 1979).

2.1 Phase Transitions

Much of the theoretical work on NP search problems examines their worst case behavior.
Although these search problems can be very hard, in the worst case, there is a great deal
of individual variation in these problems and among different search methods. A number
of recent studies of NP search problems have focused on regularities of the typical behavior (Cheeseman, Kanefsky, & Taylor, 1991; Mitchell, Selman, & Levesque, 1992; Williams &
Hogg, 1994; Hogg et al., 1996; Hogg, 1994). This work has identified a number of common
behaviors. Specifically, for large problems, a few parameters characterizing their structure
determine the relative diculty for a wide variety of common search methods, on average.
Moreover, changes in these parameters give rise to transitions, becoming more abrupt for
larger problems, that are analogous to phase transitions in physical systems. In this case,
the transition is from underconstrained to overconstrained problems, with the hardest cases
concentrated in the transition region. One powerful result of this work is that this concentration of hard cases occurs at the same parameter values for a wide range of search
methods. That is, this behavior is a property of the problems rather than of the details of
the search algorithm.
This can be understood by viewing a search as making a series of choices until a solution
is found. The overall search will usually be relatively easy (i.e., require few steps) if either
there are many choices leading to solutions or else choices that do not lead to solutions
can be recognized quickly as such, so that unproductive search is avoided. Whether this
condition holds is in turn determined by how tightly constrained the problem is. When
there are few constraints almost all choices are good ones, leading quickly to a solution.
With many constraints, on the other hand, there are few good choices but the bad ones can
be recognized very quickly as violating some constraints so that not much time is wasted
considering them. In between these two cases are the hard problems: enough constraints
so good choices are rare but few enough that bad choices are usually recognized only with
a lot of additional search.
A more detailed analysis suggests a series of transitions (Hogg & Williams, 1994). With
very few constraints, the average search cost scales polynomially. As more constraints are
added, there is a transition to exponential scaling. The rate of growth of this exponential
increases until the transition region described above is reached. Beyond that point, with
its concentration of hard problems, the growth rate decreases. Eventually, for very highly
constrained problems, the search cost again grows only polynomially with size.

2.2 The Combinatorial Search Space

A general view of the combinatorial search problem is that it consists of N items1 and a
requirement to find a solution, i.e., a set of L<N items that satisfies specified conditions or
constraints. These conditions in turn can be described as a collection of nogoods, i.e., sets
1. For CSPs, these items are all possible variable-value pairs.

93

fiHogg

{1,2,3,4}

{1,2}

{1,2,3}

{1,3,4}

{2,3,4}

{1,2,4}

{2,3}

{1,3}

{3,4}

{2,4}

{1}

{2}

{3}

{4}

{1,4}

{}

Figure 1: Structure of the set lattice for a problem with four items. The subsets of f1; 2; 3; 4g
are grouped into levels by size and lines drawn between each set and its immediate
supersets and subsets. The bottom of the lattice, level 0, represents the single set
of size zero, the four points at level 1 represent the four singleton subsets, etc.
of items whose combination is inconsistent with the given conditions. In this context we
define a good to be a set of items that is consistent with all the constraints of the problem.
We also say a set is complete if it has L items, while smaller sets are partial or incomplete.
Thus a solution is a complete good set. In addition, a partial solution is an incomplete good
set.
A key property that makes this set representation conceptually useful is that if a set is
nogood, so are all of its supersets. These sets, grouped by size and with each set linked to
its immediate supersets and subsets, form a lattice structure. This structure for N = 4 is
shown in Fig. 1. We say that the
N 
Ni = i
(1)

sets of size i are at level i in the lattice. As described below, the various paths through
the lattice from levels near the bottom up to solutions, at level L, can be used to create
quantum interference as the basis for a search algorithm.
As an example, consider a problem with N = 4 and L = 2, and suppose the constraints
eliminate items 1 and 3. Then we have the sets fg, f2g, and f4g as partial goods, while f1g
and f3g are partial nogoods. Among the 6 complete sets, only f2,4g is good as the others
are supersets of f1g or f3g and hence nogood.
94

fiQuantum Computing and Phase Transitions in Combinatorial Search

For the search problems studied here, the nogoods directly specified by the problem
constraints will be small sets of items, e.g., of size two or three. On the other hand, the
number of items and the size of the solutions will grow with the problem size. This gives
a number of small nogoods, i.e., near the bottom of the lattice. Examples of such problems include binary constraint satisfaction, graph coloring and propositional satisfiability
mentioned above.
For CSPs, the items are just the possible variable-value pairs in the problem. Thus a
CSP with n variables and b values for each has N = nb items2 . A solution consists of an
assignment to each variable that satisfies whatever constraints are given in the problem.
Thus a solution consists of a set of L = n items. In terms of the general framework for
combinatorial search these constraint satisfaction problems will also contain a number of
problem-independent necessary nogoods, namely
 b  those corresponding to giving the same
variable two different values. There are n 2 such necessary nogoods. For a nontrivial
search we must have b2, so we restrict our attention to the case where LN=2. This
requirement is important in allowing the construction of the quantum search method described below.
Another example is given by a simple CSP consisting of n = 2 variables (v1 and v2)
each of which can take on one of b = 2 values (1 or 2) and the single constraint that the
two variables take on distinct values, i.e., v1 6= v2 . Hence there are N = nb = 4 variablevalue pairs v1 = 1; v1 = 2; v2 = 1; v2 = 2 which we denote as items 1; 2; 3; 4 respectively.
The corresponding lattice is given in Fig. 1. What are the nogoods for this problem?
First there are those due to the explicit constraint that the two variables have distinct
values: fv1 = 1; v2 = 1g and fv1 = 2; v2 = 2g or f1; 3g and f2; 4g. In addition, there are
necessary nogoods implied by the requirement that a variable takes on a unique value so
that any set giving multiple assignments to the same variable is necessarily nogood, namely
fv1 = 1; v1 = 2g and fv2 = 1; v2 = 2g or f1; 2g and f3; 4g. Referring to Fig. 1, we see that
these four nogoods force all sets of size 3 and 4 to be nogood too. However, sets of size zero
and one are goods as are the remaining two sets of size two: f2; 3g and f1; 4g corresponding
to fv1 = 2; v2 = 1g and fv1 = 1; v2 = 2g which are the solutions to this problem.
Search methods use various strategies for examining the sets in this lattice. For instance,
methods such as simulated annealing (Kirkpatrick, Gelatt, & Vecchi, 1983), heuristic repair (Minton, Johnston, Philips, & Laird, 1992) and GSAT (Selman, Levesque, & Mitchell,
1992) move among complete sets, attempting to find a solution by a series of small changes
to the sets. Generally these search techniques continue indefinitely if the problem has no
solution and thus they can never show that a problem is insoluble. Such methods are called
incomplete. In these methods, the search is repeated, from different initial conditions or
making different random choices, until either a solution is found or some specified limit on
the number of trials is reached. In the latter case, one cannot distinguish a problem with
no solution at all from just a series of unlucky choices for a soluble problem. Other search
techniques attempt to build solutions starting from smaller sets, often by a process of extending a consistent set until either a solution is found or no further consistent extensions
are possible. In the latter case the search backtracks to a previous decision point and tries
2. The lattice of sets can also represent problems where each variable can have a different number of assigned
values.

95

fiHogg

another possible extension until no further choices remain. By recording the pending choices
at each decision point, these backtrack methods can determine a problem is insoluble, i.e.,
they are complete or systematic search methods.
This description highlights two distinct aspects of the search procedure: a general
method for moving among sets, independent of any particular problem, and a testing procedure that checks sets for consistency with the particular problem's requirements. Often,
heuristics are used to make the search decisions depend on the problem structure hoping
to identify changes most likely to lead to a solution and avoid unproductive regions of the
search space. However, conceptually these aspects can be separated, as in the case of the
quantum search algorithm presented below.

3. Quantum Search Methods

This section briey describes the capabilities of quantum computers, why some straightforward attempts to exploit these capabilities for search are not particularly effective, then
motivates and describes a new search algorithm.

3.1 An Overview of Quantum Computers

The basic distinguishing feature of a quantum computer (Benioff, 1982; Bernstein & Vazirani, 1993; Deutsch, 1985, 1989; Ekert & Jozsa, 1995; Feynman, 1986; Jozsa, 1992; Kimber,
1992; Lloyd, 1993; Shor, 1994; Svozil, 1995) is its ability to operate simultaneously on a
collection of classical states, thus potentially performing many operations in the time a classical computer would do just one. Alternatively, this quantum parallelism can be viewed as a
large parallel computer requiring no more hardware than that needed for a single processor.
On the other hand, the range of allowable operations is rather limited.
To describe this more concretely, we adopt the conventional ket notation from quantum
mechanics (Dirac, 1958, section 6) to denote various states3. That is, we use jffi to denote
the state of a computer described by ff. At a low level of description, the state of a classical
computer is described by values of its bits. So for instance if it has n bits, then there
are N = 2n possible states for the machine, which can be associated with the numbers
s1 = 0; : : :; sN = 2n , 1. We then say the computer is in state jsi i when the values of its
bits correspond to the number i , 1. More commonly, a computer is described in terms of
higher level constructs formed from groups of bits, such as integers, character strings, sets
and addresses of variables in a program. For example, a state that could arise during a
search is jfv1 = 1; v2 = 1g; soln = Falsei corresponding to a set of assignments for variables
in a CSP and a value of false for the program variable soln, e.g., used to represent whether
a solution has been found. In these higher level descriptions, there will often be aspects of
the computer's state, e.g., stack pointers or values for various iteration counters, that are
not explicitly mentioned.
The states presented so far, where each bit or higher-level construct has a definite value,
apply both to classical and quantum computers. However, quantum computers have a
far richer set of possible states. Specifically, if js1 i; : : :; jsN i are the possible states for a
3. The ket notation is conceptually similar to the use of boldface to denote vectors and distinguish them
from scalars.

96

fiQuantum Computing and Phase Transitions in Combinatorial Search

classical computer, the possible states of the corresponding quantum
are all linear
P js i computer
superpositions of these states, i.e., states of the form jsi =
where
is
i i
i a complex
number called the amplitude associated with the state jsi i. The physical interpretation
of the amplitudes comes from the measurement process. When a measurement is made
on the quantum computer in state jsi, e.g., to determine the result of the computation
represented by a particular configuration of the bits in a register, one of the possible classical
states is obtained. Specifically, the classical state jsi i is obtained with probability j i j2.
Furthermore, the measurement process changes the state of the computer to exactly match
the result. That is, the measurement is said to collapse the original superposition to the
new superposition consisting of the single classical state (i.e., the amplitude of the returned
state is 1 and all other amplitudes are zero). This means repeated measurements will always
return the same result.
An important consequence of this interpretation results from the fact that probabilities
must sum to one. Thus the amplitudes of any superposition of states must satisfy the
normalization condition
X 2
j ij = 1
(2)
i

Another consequence is that the full state of a quantum computer, i.e., the superposition,
is not itself an observable quantity. Nevertheless, by changing the amplitude associated
with different classical states, operations on the superposition can affect the probability
with which various states are observed. This possibility is crucial for exploiting quantum
computation, and makes it potentially more powerful than probabilistic classical machines,
in which some choices in the program are made randomly.
These superpositions can also be viewed as vectors in a space whose basis is the individual classical states jsi i and i is the component of the vector along the ith basis element of
the space. Such a state vector can also be specified by its components as  ( 1; : : :; N )
when the
basis is understood from context. The inner product of two such vectors is
  = PNi=1 i i where i denotes the complex conjugate of i . In matrix notation, this
can also be written as y where is treated as a column vector and y is a row vector
given by the transpose of  with all entries changed to their complex conjugate values. For
these vectors, the normalization condition amounts to requiring that y = 1.
To complete this overview of quantum computers, it remains to describe how superpositions can be used within a program. In addition to the measurement process described
above, there are two types of operations that can be performed on a superposition of states.
The first type is to run classical programs on the machine, and the second allows for creating and manipulating the amplitudes of a superposition. In both these cases, the key
property of the superposition is its linearity: an operation on a superposition of states gives
the superposition of that operation acting on each of those states individually. As described
below, this property, combined with the normalization condition, greatly limits the range
of physically realizable operations.
In the first case, a quantum computer can perform a classical program provided it is
reversible, i.e., the final state contains enough information to recover the initial state. One
way to achieve this is to retain the initial input as part of the output. To illustrate the
linearity of operations, consider some reversible classical computation on these states, e.g.,
f (si ) which produces a new state from a given input one. When applied to a superposition
97

fiHogg

P

of states, the result is f (jsi) =
i jf (si )i. Why is reversibility required? Suppose the
procedure f is not reversible, i.e., it maps at least two distinct states to the same result.
For example, suppose f (s1 ) = f (s2 ) = s3 . Then for the superposition jsi = p12 (js1 i + js2 i)
p
linearity requires that f (jsi) = p12 (jf (s1 )i + jf (s2 )i) giving 2js3 i, a superposition that
violates the normalization condition. Thus this irreversible classical operation is not physically realizable on a superposition, i.e., it cannot be used with quantum parallelism.
In contrast to this use of computations on individual states, the second type of operation
modifies the amplitude of various states within a superposition. That is, starting from
jsi =
P
jk sk i the operation, denoted by U, creates a new superposition js0i = U jsi = P j0 jsj i.
Because the operations are linear with respect to superpositions,
the new amplitudes can
P
be expressed in terms of the original ones by j0 = k Ujk k , or in matrix notation by
0 = U . That is, linearity means that an operation changing the amplitudes can be
represented as a matrix. To satisfy the normalization condition, Eq. 2, this matrix must be
such that ( 0)y 0 = 1. In terms of the matrix U this condition becomes4

 
1 = (U )y(U ) = y U yU

(3)

which must hold for any initial state vector with y = 1. To see what this implies
about the matrix A  U yU , suppose = e^j = (: : :; 0; 1; 0; : : :) is the jth unit vector,
corresponding to the superposition jsj i where all amplitudes are zero except for j = 1. In
this case yA = Ajj which must equal one by Eq. 3. That is, the diagonal elements of
U yU must all be equal to one. For = p12 (^ej + e^k ) with j 6= k,
yA = 1 (^ej + ^ek )A(^ej + ^ek )
2

(4)

= 12 [Ajj + Akk + Ajk + Akj ]

This must equal one by Eq. 3, and we already know that the diagonal terms equal one. Thus
we conclude Ajk = ,Akj . A similar argument using = p12 (^ej + ie^k ), a superposition with
an imaginary value for the second amplitude, gives Ajk = Akj . Together these conditions
mean that A is the identity matrix, so U yU = I , i.e., the matrix U must be unitary to
operate on superpositions. Moreover, this condition is sucient to make any initial state
satisfy Eq. 3. This shows how the restriction to linear unitary operations arises directly from
the linearity of quantum mechanics and Eq. 2, the normalization condition for probabilities.
The class of unitary matrices includes permutations, rotations and arbitrary phase changes
(i.e., diagonal matrices where each element on the diagonal is a complex number with
magnitude equal to one).
Reversible classical programs, unitary operations on the superpositions and the measurement process are the basic ingredients used to construct a program for a quantum
computer. As used in the search algorithm described below, such a program consists of
first preparing an initial superposition of states, operating on those states with a series
of unitary matrices in conjunction with a classical program to evaluate the consistency of
4.

Uy

is the transpose of U with all elements changed to their complex conjugates. That is

98

,U y 


jk = (Ukj ) .

fiQuantum Computing and Phase Transitions in Combinatorial Search

various states with respect to the search requirements, and then making a measurement to
obtain a definite final answer. The amplitudes of the superposition just before the measurement is made determine the probability of obtaining a solution. The overall structure
is a probabilistic Monte Carlo computation (Motwani & Raghavan, 1995) in which at each
trial there is some probability to get a solution, but no guarantee. This means the search
method is incomplete: it can find a solution if one exists but can never guarantee a solution
doesn't exist.
An alternate conceptual view of these quantum programs is provided by the path integral approach to quantum mechanics (Feynman, 1985). In this view, the final amplitude of
a given state is obtained by a weighted sum over all possible paths that produce that state.
In this way, the various possibilities involved in a computation can interfere with each
other, either constructively or destructively. This differs from the classical combination
of probabilities of different ways to reach the same outcome (e.g., as used in probabilistic
algorithms): the probabilities are simply added, giving no possibility for interference. Interference is also seen in classical waves, such as with sound or ripples on the surface of water.
But these systems lack the capability of quantum parallelism. The various formulations of
quantum mechanics, involving operators, matrices or sums over paths are equivalent but
suggest different intuitions about constructing possible quantum algorithms.

3.2 Example: A One-Bit Computer

A simple example of these ideas is given by a single bit. In this case there are two possible
classical states j0i and j1i corresponding to the values 0 and 1, respectively, for the bit. This
defines a two dimensional vector space of superpositions for a quantum bit. There are a
number of proposals for implementing quantum bits, i.e., devices whose quantum mechanical
properties can be controlled to produce desired superpositions of two classical values. One
example (DiVincenzo, 1995; Lloyd, 1995) is an atom whose ground state corresponds to the
value 0 and an excited state to the value 1. The use of lasers of appropriate frequencies can
switch such an atom between the two states or create superpositions of the two classical
states. This ability to manipulate quantum superpositions has been demonstrated in small
cases (Zhu, Kleiman, Li, Lu, Trentelman, & Gordon, 1995). Another possibility is through
the use of atomically precise manipulations (DiVincenzo, 1995) using a scanning tunneling
or atomic force microscope. This possibility of precise manipulation of chemical reactions
has also been demonstrated (Muller, Klein, Lee, Clarke, McEuen, & Schultz, 1995). There
are also a number of other proposals under investigation (Barenco, Deutsch, & Ekert, 1995;
Sleator & Weinfurter, 1995; Cirac & Zoller, 1995), including the possibility of multiple
simultaneous quantum operations (Margolus, 1990).
A simple computation on a quantum bit is the logical NOT operation, i.e., NOT(j0i) =
j1i and NOT(j1i) = j0i. This operator simply exchanges the state vector's components:
NOT

 0
1

 NOT( 0j0i + 1j1i) = 0j1i + 1j0i 

99

 1
0

(5)

fiHogg

0 1
This operation can also be represented as multiplication by the permutation matrix 1 0 .

Another operator is given by the rotation matrix
 cos  , sin  
U () = sin  cos 
(6)
This can be used to create superpositions from single classical states, e.g.,
   1    
1
1
1
U 4 0  U 4 j0i = p (j0i + j1i)  p 1
(7)
2
2
This rotation matrix can also be used to illustrate interference, an important way in
which quantum computers differ from probabilistic classical algorithms. First, consider a
classical algorithm with two methods for generating random bits, R0 (producing a \0" with
probability 3=4) and R1 (producing a \0" with probability 1=4). Suppose a \0" represents
a failure (e.g., a probabilistic search that does not find a solution) while \1" represents a
success. Finally, let the classical algorithm consist of selecting one of these methods to use,
with probability p to pick R0 . Then the overall probability to obtain a \0" as the final
result is just 34 p + 14 (1 , p) or
(8)
Pclassical = 41 + p2
The best that can be done is to choose p = 0, giving a probability of 1=4 for failure.
A quantum analog of this simple calculation can be obtained from a rotation with  = 3 .
Starting from the individual classical states this gives superpositions

   1  1 p3 !
U 3 0 =2 1
   0  1  ,1 
U
= p

(9)

3

1
2 3
which correspond to the generators R0 and R1 respectively, because of their respective
probabilities of 3=4 and 1=4 to produce a \0" when measured. Starting instead from a

superposition of the two classical states, cos
sin  , corresponds to the step of the classical
algorithm where generator R0, isselected
with probability p = cos2 . The resulting state

cos

after applying the rotation, U 3 sin  , has probability

p

2
Pquantum = 41 + cos2  , 43 sin (2)
p
= Pclassical , 43 sin (2)

(10)

to produce a \0" value. In this case the minimum value of the probability to obtain a
\0" is not 1=4 but in fact can be made to equal 0 with the choice  = 3 . In this case the
amplitudes from the two original states exactly cancel each other, an example of destructive
interference.
As a final example, illustrating the limits of operations on superpositions, consider the
simple classical program that sets a bit to the value one. That is, SET(j0i) = j1i and
100

fiQuantum Computing and Phase Transitions in Combinatorial Search

SET(j1i) = j1i. This operation isnot reversible: knowing the result does not determine the
original input. By linearity, SET p12 (j0i + j1i) = p12 (SET(j0i) + SET(j1i)), which in turn
p
is p12 2j1i = 2j1i. This state violates the normalization condition. Thus we see that this
classical operation is not physically realizable for a quantum computer. Similarly, another
common classical operation, making a copy of a bit, is also ruled out (Svozil, 1995), forming
the basis for quantum cryptography (Bennett, 1992).

3.3 Some Approaches to Search

A device consisting of n quantum bits allows for operations on superpositions of 2n classical
states. This ability to operate simultaneously on an exponentially large number of states
with just a linear number of bits is the basis for quantum parallelism. In particular, repeating the operation of Eq. 7 n times, each on a different bit, gives a superposition with equal
amplitudes in 2n states.
At first sight quantum computers would seem to be ideal for combinatorial search problems that are in the class NP. In such problems, there is an ecient procedure f (s) that takes
a potential solution set s and determines whether s is in fact a solution, but there are exponentially many potential solutions, very few of which are in fact solutions. If s1 ; : : :; sN are
the potential sets to consider, we can quickly form the superposition p1N (js1i + : : : + jsN i)
and then simultaneously evaluate f (sP
) for all these states, resulting in a superposition of
the sets and their evaluation, i.e., p1N jsi ; soln = f (si )i. Here jsi ; soln = f (si )i represents
a classical search state considering the set si along with a variable soln whose value is true
or false according to the result of evaluating the consistency of the set with respect to the
problem requirements. At this point the quantum computer has, in a sense, evaluated all
possible sets and determined which are solutions. Unfortunately, if we make a measurement of the system, we get each set with equal probability 1=N and so are very unlikely to
observe a solution. This is thus no better than the slow classical search method of random
generate-and-test where sets are randomly constructed and tested until a solution is found.
Alternatively, we can obtain a solution with high probability by repeating this operation
O(N ) times, either serially (taking a long time) or with multiple copies of the device (requiring a large amount of hardware or energy if, say, the computation is done by using multiple
photons). This shows a trade-off between time and energy (or other physical resources),
conjectured to apply more generally to solving these search problems (Cerny, 1993), and
also seen in the trade-off of time and number of processors in parallel computers.
To be useful for combinatorial search, we can't just evaluate the various sets but instead
must arrange for amplitude to be concentrated into the solution sets so as to greatly increase
the probability a solution will be observed. Ideally this would be done with a mapping that
gives constructive interference of amplitude in solutions and destructive interference in nonsolutions. Designing such maps is complicated by the fact that they must be linear unitary
operators as described above. Beyond this physical restriction, there is an algorithmic or
computational requirement: the mapping should be eciently computable (DiVincenzo &
Smolin, 1994). For example, the map cannot require a priori knowledge of the solutions
(otherwise constructing the map would require first doing the search). This computational
requirement is analogous to the restriction on search heuristics: to be useful, the heuristic
itself must not take a long time to compute. These requirements on the mapping trade off
101

fiHogg

against each other. Ideally one would like to find a way to satisfy them all so the map can
be computed in polynomial time and give, at worst, polynomially small probability to get a
solution if the problem is soluble. One approach is to arrange for constructive interference
in solutions while nonsolutions receive random contributions to their amplitude. While such
random contributions are not as effective as a complete destructive interference, they are
easier to construct and form the basis for a recent factoring algorithm (Shor, 1994) as well
as the method presented here.
Classical search algorithms can suggest ways to combine the use of superpositions with
interference. These include local repair styles of search where complete assignments are
modified, and backtracking search, where solutions are built up incrementally. Using superpositions, many possibilities could be simultaneously considered. However these search
methods have no a priori specification of the number of steps required to reach a solution so
it is unclear how to determine when enough amplitude might be concentrated into solution
states to make a measurement worthwhile. Since the measurement process destroys the
superposition, it is not possible to resume the computation at the point where the measurement was made if it does not produce a solution. A more subtle problem arises because
different search choices lead to solutions in differing numbers of steps. Thus one would also
need to maintain any amplitude already in solution states while the search continues. This
is dicult due to the requirement for reversible computations.
While it may be fruitful to investigate these approaches further, the quantum method
proposed below is based instead on a breadth-first search that incrementally builds up all
solutions. Classically, such methods maintain a list of goods of a given size. At each step,
the list is updated to include all goods with one additional variable. Thus at step i, the list
consists of sets of size i which are used to create the new list of sets of size i + 1. For a CSP
with n variables, i ranges from 0 to n , 1, and after completing these n steps the list will
contain all solutions to the problem. Classically, this is not a useful method for finding a
single solution because the list of partial assignments grows exponentially with the number
of steps taken. A quantum computer, on the other hand, can handle such lists readily as
superpositions. In the method described below, the superposition at step i consists of all
sets of size i, not just consistent ones, i.e., the sets at level i in the lattice. There is no
question of when to make the final measurement because the computation requires exactly
n steps. Moreover, there is an opportunity to use interference to concentrate amplitude
toward goods. This is done by changing the phase of amplitudes corresponding to nogoods
encountered while moving through the lattice.
As with the division of search methods into a general strategy (e.g., backtrack) and
problem specific choices, the quantum mapping described below has a general matrix that
corresponds to exploring all possible changes to the partial sets, and a separate, particularly
simple, matrix that incorporates information on the problem specific constraints. More
complex maps are certainly possible, but this simple decomposition is easier to design and
describe. With this decomposition, the dicult part of the quantum mapping is independent
of the details of the constraints in a particular problem. This suggests the possibility of
implementing a special purpose quantum device to perform the general mapping. The
constraints of a specific problem are used only to adjust phases as described below, a
comparatively simple operation.
102

fiQuantum Computing and Phase Transitions in Combinatorial Search

For constraint satisfaction problems, a simple alternative representation to the full lattice
structure is to use partial assignments only, i.e., sets of variable-value pairs that have no
variable more than once. At first sight this might seem better in that it removes from
consideration the necessary nogoods and hence increases the proportion of complete sets
that are solutions. However, in this case the number of sets as a function of level in the lattice
decreases before reaching the solution level, precluding the simple form of a unitary mapping
described below for the quantum search algorithm. Another representation that avoids this
problem is to consider assignments in only a single order for the variables (selected randomly
or through the use of heuristics). This version of the set lattice has been previously used
in theoretical analyses of phase transitions in search (Williams & Hogg, 1994). This may
be useful to explore further for the quantum search, but is unlikely to be as effective. This
is because in a fixed ordering some sets will become nogood only at the last few steps,
resulting is less opportunity for interference based on nogoods to focus on solutions.

3.4 Motivation

To motivate the mapping described below, we consider an idealized version. It shows why
paths through the lattice tend to interfere destructively for nonsolution states, provided the
constraints are small.
The idealized map simply maps each set in the lattice equally to its supersets at the next
level, while introducing random phases for sets found to be nogood. For this discussion we
are concerned with the relative amplitude in solutions and nogoods so we ignore the overall
normalization. Thus for instance, with N = 6, the state jf1; 2gi will map to an unnormalized
superposition of its four supersets of size 3, namely the state jf1; 2; 3gi + : : : + jf1; 2; 6gi.
With this mapping, a good at level j will receive equal contribution from each of its j
subsets at the prior level. Starting with amplitude of 1 at level 0 then gives an amplitude
of j ! for goods at level j. In particular, L! for solutions.
How does this compare with contribution to nogoods, on average? This will depend on
how many of the subsets are nogoods also. A simple case for comparison is when all sets
in the lattice are nogood (starting with those at level k given by the size of the constraints,
e.g., k = 2 for problems with binary constraints). Let rj be the expected value of the
magnitude of the amplitude for sets at level j. Each set at level k will have rk = k! (and a
zero phase) because all smaller subsets will be goods. A set s at level j>k will be a sum of
j contributions from (nogood) subsets, giving a total contribution of
(s) =

j
X
m=1

(sm )eim

(11)

where the sm are the subsets of s of size j , 1 and the phases m are randomly selected.
The (sm ) have expected magnitude rj ,1 and some phase that can be combined with m
to give a new random phase m . Ignoring the variation in the magnitude of the amplitudes
at each level this gives
+
*X
j
p
i
(12)
rj = rj,1
e m = rj,1 j
m=1

103

fiHogg

because the sum of j random phases is equivalent to an unbiased prandom walk (Karlin
p &
j
.
Thus
r
=
r
Taylor, 1975)
with
j
unit
steps
which
has
expected
net
distance
of
j
k j !=k!
p
or rj = j !k! for j>k.
This crude argument gives a rough estimate of the relative probabilities for solutions
compared to complete nogoods. Suppose there is only one solution. Then its relative
probability is L!2 . The nogoods have relative probability (NL , 1)rL2  NLL!k! with NL
given by Eq. 1. An interesting scaling regime is L = N=b with fixed b, corresponding to a
variety of well-studied constraint satisfaction problems. This gives
! 
 N
L
!
P
soln
= ln N k!  b ln N + O(N )
(13)
ln P
nogood
L
This goes to infinity as problems get large so the enhancement of solutions is more than
enough to compensate for their rareness among sets at the solution level.
The main limitation of this argument is assuming that all subsets of a nogood are also
nogood. For many nogoods, this will not be the case, resulting in less opportunity for
cancellation of phases. The worst situation in this respect is when most subsets are goods.
This could be because the constraints are large, i.e., they don't rule out states until many
items are included. Even with small constraints, this could happen occasionally due to a
poor ordering choice for adding items to the sets, hence suggesting that a lattice restricted to
assignments in a single order will be much less effective in canceling amplitude in nogoods.
For the problems considered here, with small constraints, a large nogood cannot have too
many good subsets because to be nogood means a small subset violates a (small) constraint
and hence most subsets obtained by removing one element will still contain that bad subset
giving a nogood. In fact, some numerical experiments (with the class of unstructured
problems described below) show that this mapping is very effective in canceling amplitude
in the nogoods. Thus the assumptions made in this simplified argument seem to provide
the correct intuitive description of the behavior.
Still the assumption of many nogood subsets underlying the above argument does suggest the extreme cancellation derived above will least apply when the problem has many
large partial solutions. This gives a simple explanation for the diculty encountered with
the full map described below at the phase transition point: this transition is associated with
problems with relatively many large partial solutions but few complete solutions. Hence we
can expect relatively less cancellation of at least some nogoods at the solution level and a
lower overall probability to find a solution.
This discussion suggests why a mapping of sets to supersets along with random phases
introduced at each inconsistent set can greatly decrease the contribution to nogoods at
the solution level. However, this mapping itself is not physically realizable because it is
not unitary. For example, the mapping from level 1 to 2 with N = 3 takes the states
jf1gi; jf2gi; jf3gi to jf1; 2gi; jf1; 3gi; jf2; 3gi with the matrix
01 1 01
M = @1 0 1A
(14)
0 1 1
Here, the first column means the state jf1gi contributes equally to jf1; 2gi and jf1; 3gi, its
supersets, and gives no contribution to jf2; 3gi. We see immediately that the columns of
104

fiQuantum Computing and Phase Transitions in Combinatorial Search

thispmatrix are not orthogonal, though they can be easily normalized by dividing the entries
by 2. More generally, this mapping takes each set at level i to the N , i sets with one more
element. The corresponding matrix M has one column for each i{set and one row for each
(i +1)-set. In each column there will be exactly N , i 1's (corresponding to the supersets of
the given i{set) and the remaining entries will be 0. Two columns will have at most a single
nonzero value in common (and only when the two corresponding i{sets have all but one of
their values in common: this is the only way they can share a superset in common). This
means that as N gets large, the columns of this matrix are almost orthogonal (provided
i<N=2, the case of interest here). This fact is used below to obtain a unitary matrix that
is fairly close to M.

3.5 A Search Algorithm

The general idea of the mapping introduced here is to move as much amplitude as possible to
supersets (just as in classical breadth-first search, increments to partial sets give supersets).
This is combined with a problem specific adjustment of phases based on testing partial
states for consistency (this corresponds to a diagonal matrix and thus is particularly simple
in that it does not require any mixing of the amplitudes of different states). The specific
methods used are described in this section.
3.5.1 The Problem-Independent Mapping

To take advantage of the potential cancellation of amplitude in nogoods described above
we need a unitary mapping whose behavior is similar to the ideal mapping to supersets.
There are two general ways to adjust the ideal mapping of sets to supersets (mixtures of
these two approaches are possible as well). First, we can keep some amplitude at the same
level of the lattice instead of moving all the amplitude up to the next level. This allows
using the ideal map described above (with suitable normalization) and so gives excellent
discrimination between solutions and nonsolutions, but unfortunately not much amplitude
reaches solution level. This is not surprising: the use of random phases cancel the amplitude
in nogoods but this doesn't add anything to solutions (because solutions are not a superset
of any nogood and hence cannot receive any amplitude from them). Hence at best, even
when all nogoods cancel completely, the amplitude in solutions will be no more than their
relative number among complete sets, i.e., very small. Thus the random phases prevent
much amplitude moving to nogoods high in the lattice, but instead of contributing to
solutions this amplitude simply remains at lower levels of the lattice. Hence we have no
better chance than random selection of finding a solution (but, when a solution is not found,
instead of getting a nogood at the solution level, we are now likely to get a smaller set in
the lattice). Thus we must arrange for amplitude taken from nogoods to contribute instead
to the goods. This requires the map to take amplitude to sets other than just supersets, at
least to some extent.
The second way to fix the nonunitary ideal map is to move amplitude also to nonsupersets. This can move all amplitude to the solution level. It allows some canceled
amplitude from nogoods to go to goods, but also vice versa, resulting in less effective
concentration into solutions. This can be done with a unitary matrix as close as possible
to the nonunitary ideal map to supersets, and that also has a relatively simple form. The
105

fiHogg

general question here is given k linearly independent vectors in m dimensional space, with
km, find k orthonormal vectors in the space as close as possible to the k original ones.
Restricting attention to the subspace defined by the original vectors, this can be obtained5
using the singular value decomposition (Golub & Loan, 1983) (SVD) of the matrix M
whose columns are the k given vectors. Specifically, this decomposition is M = AyB ,
where  is a diagonal matrix containing the singular values of M and both Ay and B
have orthonormal columns. For a real matrix M, the matrices of the decomposition are
also real-valued. The matrix U = AyB has orthonormal columns and is the closest set
of orthogonal vectors according
to the Frobenius matrix norm. That is, this choice for U
P
minimizes jU , M j2  rs jUrs , Mrs j2 among all unitary matrices. This construction fails
if k>m since an m{dimensional space cannot have more than m orthogonal vectors. Hence
we restrict consideration to mappings in the lattice at those levels i where level i + 1 has at
least as many sets as level i, i.e., Ni Ni+1 . Obtaining a solution requires mapping up to
level L so, from Eq. 1, this restricts consideration to problems where LdN=2e.
For example, the mapping from level 1 to 2 with N = 3 given in Eq. 14 has the singular
value decomposition M = AyB with this decomposition given explicitly as

0 p1 , p1
BB p13 p1 2
y
A B = @ 3
2
p13

10
10 p13 p13 p13 1
2
0
0
C
B
p1
p1 C
q6 2 CA@ 00 10 01 AB@ q02 , 12 21 CA
, 3
3 , p6 , p6
p16
p1

0
The closest unitary matrix is then

(15)

0 2 2 ,1 1
1
U = AyB = 3 @ 2 ,1 2 A
(16)
,1 2 2
While this gives a set of orthonormal vectors close to the original map, one might
be concerned about the requirement to compute the SVD of exponentially large matrices.
Fortunately, however, the resulting matrices have a particularly simple structure in that the
entries depend only on the overlap between the sets. Thus we can write the matrix elements
in the form Urfi = ajr\fi j (r is an (i+1)-subset, fi is an i-subset). The overlap jr \ fi j ranges
from i when fi  r to 0 when there is no overlap. Thus instead of exponentially many
distinct values, there are only i + 1, a linear number. This can be exploited to give a
simpler method for evaluating the entries of the matrix as follows.
We can get expressions for the a values for a given N and i since the resulting column
vectors are orthonormal. Restricting attention to real values, this gives




Xi

1 = U yU ffff = nk a2k
k=0
where

 

nk = ki i +N 1,,i k

(17)
(18)

5. I thank J. Gilbert for pointing out this technique, as a variant of the orthogonal Procrustes problem (Golub & Loan, 1983).

106

fiQuantum Computing and Phase Transitions in Combinatorial Search

is the number of ways to pick r with the specified overlap. For the off-diagonal terms,
suppose jff \ fi j = p<i then, for real values of the matrix elements,





Xi

0 = U yU fffi =
n(jkp)aj ak
j;k=0
where

(p) =
njk

X  i , p  p  i , p  N , 2i + p 
x k,x x j ,x i+1,j,k+x

(19)
(20)

is the number of sets r with the required overlaps with ff and fi , i.e., jr \ ffj = ki and
jr \ fij = j i. In this sum, x is the number of items the set r has in common with both
ff and fi . Together these give i + 1 equations for the values of a0; : : :; ai, which are readily
solved numerically6 . There are multiple solutions for this system of quadratic equations,
each representing a possible unitary mapping. But there is a unique one closest to the
ideal mapping to supersets, as given by the SVD. It is this solution we use for the quantum
search algorithm7 , although it is possible some other solution, in conjunction with various
choices of phases, performs better. Note that the number of values and equations grows
only linearly with the level in the lattice, even though the number of sets at each level grows
exponentially. When necessary to distinguish the values at different levels in the lattice, we
use a(ki) to mean the value of ak for the mapping from level i to i + 1.
The example of Eq. 14, with N = 3 and i = 1, has 1 = a20 + 2a21 for Eq. 17 and
0 = 2a0a1 + a21 for Eq. 19. The solution of these unitarity conditions closest to Eq. 14 is
a0 = , 13 ; a1 = 23 corresponding to Eq. 16.
A normalized version of the ideal map has a(ii) = p1ni = pN1 ,i and all other values equal
to zero. The actual values for a(ki) are fairly close to this (confirming that the ideal map is
close to orthogonal already), and alternate in sign. To illustrate their behavior, it is useful
to consider the scaled values b(ki)  (,1)k a(i,i)k pni,k , with ni,k evaluated using Eq. 18. The
behavior of these values for N = 10 is shown in Fig. 2. Note that b(0i) is close to one, and
decreases slightly as higher levels in the lattice (i.e., larger i values) are considered: the
ideal mapping is closer to orthogonal at low levels in the lattice.
Despite the simple values for the example of Eq. 16, the ak values in general do not
appear to have a simple closed form expression. This is suggested by obtaining exact solutions to Eqs. 17 and 19 using the Mathematica symbolic algebra program (Wolfram, 1991).
In most cases this gives complicated expressions involving nested roots. Since such expressions could simplify, the ak values were also checked for being close to rational numbers and
whether they are roots of single variable polynomials of low degree8 . Neither simplification
was found to apply.
Finally we should note that this mapping only describes how the sets at level i are
mapped to the next level. The full quantum system will also perform some mapping on the
6. High precision values were obtained from the FindRoot function of Mathematica.
7. The values are given in Online Appendix 1.
8. Using the Mathematica function Rationalize and the package NumberTheory`Recognize`.

107

fiHogg

1
0.5
0.2
0.1
0.05
0

1

2

3

4

k

Figure 2: Behavior of b(ki) vs. k on a log scale for N = 10. The three curves show the values
for i = 4 (black), 3 (dashed) and 2 (gray).
remaining sets in the lattice. By changing the map at each step, most of the other sets can
simply be left unchanged, but there will need to be a map of the sets at level i + 1 other
than the identity mapping to be orthogonal to the map from level i. Any orthogonal set
mapping partly back to level i and partly remaining in sets at level i + 1 will be suitable for
this: in our application there is no amplitude at level i + 1 when the map is used and hence
it doesn't matter what mapping is used. However, the choice of this part of the overall
mapping remains a degree of freedom that could perhaps be exploited to minimize errors
introduced by external noise.
3.5.2 Phases for Nogoods

In addition to the general mapping from one level to the next, there is the problem-specific
aspect of the algorithm, namely the choice of phases for the nogood sets at each level.
In the ideal case described above, random phases were given to each nogood, resulting in
a great deal of cancellation for nogoods at the solution level. While this is a reasonable
choice for the unitary mapping, other policies are possible as well. For example, one could
simply invert the phase of each nogood9 (i.e., multiply its amplitude by -1). This choice
doesn't work well for the idealized map to supersets only but, as shown below, is helpful
for the unitary map. It can be motivated by considering the coecients shown in Fig. 2.
Specifically, when a nogood is encountered for the first time on a path through the lattice,
we would like to cancel phase to its supersets but at the same time enhance amplitude in
other sets likely to lead to solutions. Because a(i,i)1 is negative, inverting the phase will tend
to add to sets that differ by one element from the nogood. At least some of these will avoid
violating the small constraint that produced this nogood set, and hence may contribute
eventually to sets that do lead to solutions.
Moreover, one could use information on the sets at the next level to decide what to
do with the phase: as currently described, the computation makes no use of testing the
9. I thank J. Lamping for suggesting this.

108

fiQuantum Computing and Phase Transitions in Combinatorial Search

consistency of sets at the solution level itself, and hence is completely ineffective for problems
where the test requires the complete set. Perhaps better would be to mark a state as nogood
if it has no consistent extensions with one more item (this is simple to check since the number
of extensions grows only linearly with problem size). Another possibility is for the phase
to be adjusted based on how many constraints are violated, which could be particularly
appropriate for partial constraint satisfaction problems (Freuder & Wallace, 1992) or other
optimization searches.
3.5.3 Summary

The search algorithm starts by evenly dividing amplitude among the goods at a low level
K of the lattice. A convenient choice for binary CSPs is to start at level K = 2, where the
number of sets is proportional to N 2. Then for each level from K to L , 1, we adjust the
phases of the states depending on whether they are good or nogood and map to the next
level. Thus if ff(j ) represents the amplitude of the set ff at level j, we have
(j +1) = X U  (j )
rff ff ff
r
ff

=

X
k

a(kj )

X

jr\ffj=k

ff

(j )
ff

(21)

where ff is the phase assigned to the set ff after testing whether it is nogood, and the final
inner sum is over all sets ff that have k items in common with r. That is, ff = 1 when ff
is a good set. For nogoods, ff = ,1 when using the phase inversion method, and ff = ei
with  uniformly selected from [0; 2 ) when using the random phase method. Finally we
measure the state, obtaining a complete set. This set will be a solution with probability

psoln =

X fifi
fi
s

fi

(L) fi2
s fi

(22)

with the sum over solution sets, depending on the particular problem and method for
selecting the phases.
What computational resources are required for this algorithm? The storage requirements
are quite modest: N bits can produce a superposition of 2N states, enough to represent
all the possible sets in the lattice structure. Since each trial of this algorithm gives a
solution only with probability psoln , on average it will need to be repeated 1=psoln times
to find a solution. The cost of each trial consists of the time required to construct the
initial superposition and then evaluate the mapping on each step from the level K to the
solution level L, a total of L , K<N=2 mappings. Because the initial
 state consists of
sets of size K, there are only a polynomial number of them (i.e., O N K ) and hence the
cost to construct the initial superposition will be relatively modest. The mapping from
one level to the next will need to be produced by a series of more elementary operations
that can be directly implemented in physical devices. Determining the required number
of such operations remains an open question, though the particularly simple structure of
the matrices should not require involved computations and should also be able to exploit
special purpose hardware. At any rate, this mapping is independent of the structure of
the problem and its cost does not affect the relative costs of different problem structures.
Finally, determining the phases to use for the nogood sets involves testing the sets against
the constraints, a relatively rapid operation for NP search problems. Thus to examine how
109

fiHogg

the cost of this search algorithm depends on problem structure, the key quantity is the
behavior of psoln .

3.6 An Example of Quantum Search

To illustrate the algorithm's operation and behavior, consider the small case of N = 3 with
the map starting from level K = 0 and going up to level L = 2. Suppose that f3g and
its supersets are the only nogoods. We begin with all amplitude in the empty set, i.e.,
with the state j;i. The map from level 0 to 1 gives equal amplitude to all singleton sets,
producing p13 (jf1gi + jf2gi + jf3gi). We then introduce a phase for the nogood set, giving


p1 jf1gi + jf2gi + ei jf3gi . Finally we use Eq. 16 to map this to the sets at level 2, giving
3
the final state







p1 4 , ei jf1; 2gi + 1 + 2ei jf1; 3gi + 1 + 2ei jf2; 3gi
(23)
3 3
At this level, only set f1,2g is good, i.e., a solution. Note that the algorithm does not make
any use of testing the states at the solution level for consistency.
The probability to obtain a solution when the final measurement is made is determined
by the amplitude of the solution set, so in this case Eq. 22 becomes

fifi 1 
fififi2 1
i
fi
psoln = fi p 4 , e fi = 27 (17 , 8 cos )
3 3

(24)

From this we can see the effect of different methods for selecting the phase for nogoods.
If the phase is selected randomly, psoln = 17
27 = 0:63 because the average value of cos  is
zero. Inverting the phase of the nogood, i.e., using  =  , gives psoln = 25
27 = 0:93. These
probabilities compare with the 1/3 chance of selecting a solution by random choice. In
this case, the optimal choice of phase is the same as that obtained by simple inversion.
However this is not true in general: picking phases optimally will require knowledge about
the solutions and hence is not a feasible mapping. Note also that even the optimal choice
of phase doesn't guarantee a solution is found.

4. Average Behavior of the Algorithm

In this section, the behavior of the quantum algorithm is evaluated for two classes of combinatorial search problems. The first class, of unstructured problems, is used to examine the
phase transition in a particularly simple context using both random and inverted phases
for nogoods. The second class, random propositional satisfiability (SAT), evaluates the
robustness of the algorithm for problems with particular structure.
For classical simulation of this algorithm we explicitly compute the amplitude of all sets
in the lattice up to the solution level and the mapping between levels. Unfortunately, this
results in an exponential slowdown compared to the quantum implementation and severely
limits the feasible size of these classical simulations. Moreover, determining the expected
behavior of the random phase method requires repeating the search a number of times on
each problem (10 tries in the experiments reported here). This further limits the feasible
problem size.
110

fiQuantum Computing and Phase Transitions in Combinatorial Search

As a simple check on the numerical errors of the calculation, we recorded the total
normalization in all sets at the solution level. With double precision calculations on a Sun
Sparc10, for the experiments reported here typically the norm was 1 to within a few times
10,11. As an indication of the execution time with unoptimized C++ code, a single trial
for a problem with N = 14 and 16, with L = N=2, required about 70 and 1000 seconds,
respectively. This uses a direct evaluation of the map from one level to the next as given
by Eq. 21. A substantial reduction in compute time is possible by exploiting the simple
structure of this mapping to give a recursive evaluation10 . Some additional improvement
is possible by exploiting the fact that all amplitudes are real when using the method that
inverts phases of nogoods. This reduced the execution time to about 1 and 6 seconds per
trial for N of 14 and 16, respectively.

4.1 Unstructured Problems

To examine the typical behavior of this quantum search algorithm with respect to problem
structure, we need a suitable class of problems. This is particularly important for average
case analyses since one could inadvertently select a class of search problems dominated by
easy cases. Fortunately the observed concentration of hard cases near phase transitions
provides a method to generate hard test cases.
The phase transition behavior has been seen in a variety of search problem classes.
Here we select a particularly simple class of problems by supposing the constraints specify
nogoods randomly at level 2 in the lattice. This corresponds to binary constraint satisfaction
problems (Prosser, 1996; Smith & Dyer, 1996), but ignores the detailed structure of the
nogoods imposed by the requirement that variables have a unique assignment. By ignoring
this additional structure, we are able to test a wider range of the number of specified nogoods
for the problems than would be the case by considering only constraint satisfaction problems.
This lack of additional structure is also likely to make the asymptotic behavior more readily
apparent at the small problem sizes that are feasible with a classical simulation.
Furthermore, since the quantum search algorithm is appropriate only for soluble problems, we restrict attention to random problems with a solution. These could be obtained
by randomly generating problems and rejecting any that have no solution (as determined
using a complete classical search method). However, for overconstrained problems the soluble ones become quite rare and dicult to find by this method. Instead, we generate
problems with a prespecified solution. That is, when randomly selecting nogoods to add
to a problem, we do not pick any nogoods that are subsets of a prespecified solution set.
This always produces problems with at least one solution. Although these problems tend
to be a bit easier than randomly selected soluble problems, they nevertheless exhibit the
same concentration of hard problems and at about the same location as general random
problems (Cheeseman et al., 1991; Williams & Hogg, 1994). The quantum search is started
at level 2 in the lattice.
10. I thank S. Vavasis for suggesting this improvement in the classical simulation of the algorithm.

111

fiHogg
cost
25
20
15
10
5
1

2

3

4

5



Figure 3: The solid curves show the classical backtrack search cost for randomly generated
problems with a prespecified solution as a function of fi = m=N for N = 10
(gray) and 20 (black) and L = N=2. Here m is the number of nogoods selected at
level 2 of the search lattice. The cost is the average number of backtrack steps,
starting from the empty set, required to find the first solution to the problem,
averaged over 1000 problems. The error bars indicate the standard deviation of
this estimate of the average value, and in most cases are smaller than the size
of the plotted points. For comparison, the dashed curves show the probability
for having a solution in randomly generated problems with the specified fi value,
ranging from 1 at the left to 0 at the right.
4.1.1 Theory

For this class of problems, the phase transition behavior is illustrated in Fig. 3. Specifically,
this shows the cost to solve the problem with a simple chronological backtrack search. The
cost is given in terms of the number of search nodes considered until a solution is found.
The minimum cost, for a search that proceeds directly to a solution with no backtrack is
L + 1. The parameter distinguishing underconstrained from overconstrained problems is
the ratio fi of the number of nogoods m at level 2 given by the constraints to the number
of items N.
Even for these relatively small problems, a peak in the average search cost is evident.
Moreover, this peak is near the transition region where random problems11 change from
mostly soluble to mostly insoluble. A simple, but approximate, theoretical value for the
location of the transition is given by the point where the expected number of solutions is
equal to one (Smith & Dyer, 1996; Williams & Hogg, 1994). Applying this to the class of
problems considered here is straightforward. Specifically, there are NL complete sets for the
problem, as given by Eq. 1. A particular set s of size L will be good, i.e., a solution, only
if none of the nogoods selected for the problem are a subset of s. Hence the probability it
11. That is, problems generated by random selection of nogoods without regard for whether they have a
solution.

112

fiQuantum Computing and Phase Transitions in Combinatorial Search

will be a solution is given by

 ( N ),( L ) 
2
2
L =  (mN ) 

(25)

2

m

N 

because there are 2 sets of size 2 from which to choose the m nogoods specified directly
by the constraints. The average number of solutions is then just Nsoln = NL L. If we set
m = fiN and L = N=b, for large N this becomes
 1
 1 
ln N  N h
+ fi ln 1 ,
(26)
soln

b

b2

where h(x)  ,x ln x , (1 , x) ln (1 , x). The predicted transition point12 is then given by
(27)
ficrit = , lnh(1(1,=b1)=b2)
which is ficrit = 2:41 for the case considered here (i.e., b = 2). This closely matches
the location of the peak in the search cost for problems with prespecified solution, as
shown in Fig. 3, but is about 20% larger than the location of the step in solubility13 .
Furthermore, the theory predicts there is a regime of polynomial average cost for suciently
few constraints (Hogg & Williams, 1994). This is determined by the condition that the
expected number of goods at each level in the lattice is monotonically increasing. Repeating
the above argument for smaller levels in the lattice, we find that this condition holds up to
2
(28)
fipoly = b 2,b 1 ln(b , 1)
which is fipoly = 0 for b = 2.
While these estimates are only approximate, they do indicate that the class of random
soluble problems defined here behaves qualitatively and quantitatively the same with respect
to the transition behavior as a variety of other, perhaps more realistic, problem classes. This
close correspondence with the theory (derived for the limit of large problems), suggests that
we are observing the correct transition behavior even with these relatively small problems.
Moreover the above approximate theoretical argument suggests that the average cost of
general classical search methods scales exponentially with the size of the problem over the
full range of fi>0. Thus this provides a good test case for the average behavior of the
quantum algorithm. As a final observation, it is important to obtain a sucient number of
samples, especially near the transition region. This is because there is considerable variation
in problems near the transition, specifically a highly skewed distribution in the number of
solutions. In this region, most problems have few solutions but a few have extremely many:
enough in fact to give a substantial contribution to the average number of solutions even
though such problems are quite rare.
12. This differs slightly from the results for problems with more specified structure on the nogoods, such as
explicitly removing the necessary nogoods from consideration (Smith & Dyer, 1996; Williams & Hogg,
1994).
13. This is a particularly large error for this theory: it does better for problems with larger constraints or
more allowed values per variable.

113

fiHogg
<T>
35
30
25
20
15
10
5
1

2

4

3

5

6

7



Figure 4: Expected number of trials hT i to find a solution vs. fi for random problems with
prespecified solution with binary constraints, using random phases for nogoods.
The solid curve is for N = 10, with 100 samples per point. The gray curve is for
N = 20 with 10 samples per point (but additional samples were used around the
peak). The error bars indicate the standard error in the estimate of hT i.
4.1.2 Phase Transition

To see how problem structure affects this search algorithm, we evaluate psoln , the probability
to find a solution for problems with different structures, ranging from underconstrained to
overconstrained. Low values for this probability indicate relatively harder problems. The
expected number of repetitions of the search required to find a solution is then given by
T = 1=psoln. The results are shown in Figs. 4 and 5 for different ways of introducing
phases for nogood sets. We see the general easy-hard-easy pattern in both cases. Another
common feature of phase transitions is an increased variance around the transition region.
The quantum search has this property as well, as shown in Fig. 6.
4.1.3 Scaling

An important question in the behavior of this search method is how its average performance
scales with problem size. To examine this question, we consider the scaling with fixed fi .
This is shown in Figs. 7 and 8 for algorithms using random and inverted phases for nogoods,
respectively. To help identify the likely scaling, we show the same results on both a log plot
(where straight lines correspond to exponential scaling) and a log-log plot (where straight
lines correspond to power-law or polynomial scaling).
It is dicult to make definite conclusions from these results for two reasons. First, the
variation in behavior of different problems gives a statistical uncertainty to the estimates of
the average values, particularly for the larger sizes where fewer samples are available. The
standard errors in the estimates of the averages are indicated by the error bars in the figures
(though in most cases, the errors are smaller than the size of the plotted points). Second, the
scaling behavior could change as larger cases are considered. With these caveats in mind,
the figures suggest that psoln remains nearly constant for underconstrained problems, even
though the fraction of complete sets that are solutions is decreasing exponentially. This
114

fiQuantum Computing and Phase Transitions in Combinatorial Search

<T>
15

10

5

1

2

4

3

5

6

7



Figure 5: Expected number of trials hT i to find a solution vs. fi for random problems with
prespecified solution with binary constraints, using inverted phases for nogoods.
The solid curve is for N = 10, with 1000 samples per point. The gray curve is
for N = 20 with 100 samples per point (but additional samples were used around
the peak). The error bars indicate the standard error in the estimate of hT i.

dev(T)

40
30
20
10

1

2

4

3

5

6

7



Figure 6: Standard deviation in the number of trials to find a solution for N = 20 as a
function of fi . The black curve is for random phases assigned to nogoods, and the
gray one for inverting phases.

115

fi0.5

0.5

0.3

0.3
p(soln)

p(soln)

Hogg

0.2
0.1
0.05

0.2
0.1
0.05

8

10

12

14
N

16

18

20

8

10

12

14

16 18 20

N

0.5

0.5

0.3

0.3

p(soln)

p(soln)

Figure 7: Scaling of the probability to find a solution using the random phase method, for
fi of 1 (solid), 2 (dashed), 3 (gray) and 4 (dashed gray). This is shown on log and
log-log scales (left and right plots, respectively).

0.2
0.1
0.05

8

0.2
0.1
0.05

10 12 14 16 18 20 22 24
N

8

10

12

14 16 18 20 2224
N

Figure 8: Scaling of the probability to find a solution using the phase inversion method, for
fi of 1 (solid), 2 (dashed), 3 (gray) and 4 (dashed gray). This is shown on log and
log-log scales (left and right plots, respectively).
behavior is also seen in the overlap of the curves for small fi in Figs. 4 and 5. For problems
with more constraints, psoln appears to decrease polynomially with the size of the problem,
i.e., the curves are closer to linear in the log-log plots than in the log plots. This in confirmed
quantitatively by making a least squares fit to the values and seeing that the residuals of the
fit to a power-law are smaller than those for an exponential fit. An interesting observation
in comparing the two phase choices is that the scaling is qualitatively similar, even though
the phase inversion method performs better. This suggests the detailed values of the phase
choices are not critical to the scaling behavior, and in particular high precision evaluation
of the phases is not required. Finally we should note that this illustration of the average
scaling leaves open the behavior for the worst case instances.
For the underconstrained cases in Figs. 7 and 8 there is a small additional difference
between cases with an even and odd number of variables. This is due to oscillations in the
amplitude in goods at each level of the lattice, and is discussed more fully in the context of
SAT problems below.
116

fiQuantum Computing and Phase Transitions in Combinatorial Search
ratio
5
10
4
10
3
10
2
10
1
10
8

10

12

14

16

18

20

22

24

N

Figure 9: Scaling of the ratio of the probability to find a solution using the quantum algorithm to the probability to find a solution by random selection at the solution
level, using the phase inversion method, for fi of 1 (solid), 2 (dashed), 3 (gray)
and 4 (dashed gray). The curves are close to linear on this log scale indicating exponential improvement over the direct selection from among complete sets, with
a higher enhancement for problems with more constraints.
-1
p(soln)

10
-2
10
-3
10
-4
10
8

10

12

14

16
N

18

20

22

24

Figure 10: Comparison of scaling of probability to find a solution with the quantum algorithm using the phase inversion method (dashed curve) and by random selection
at the solution level (solid curve) for fi = 2.
Another scaling comparison is to see how much this algorithm enhances the probability
to find a solution beyond the simple quantum algorithm of evaluating all the complete sets
and then making a measurement. As shown in Fig. 9, this quantum algorithm appears to
give an exponential improvement in the concentration of amplitude into solutions. A more
explicit view of this difference in behavior is shown in Fig. 10 for fi = 2. In this figure, the
dashed curve shows the behavior of psoln for the phase inversion method, and is identical
to the fi = 2 curve of Fig. 8.
117

fiHogg

4.2 Random 3SAT

These experiments leave open the question of how additional problem structure might affect
the scaling behaviors. While the universality of the phase transition behavior in other search
methods suggests that the average behavior of this algorithm will also be the same for a
wide range of problems, it is useful to check this empirically. To this end the algorithm was
applied to the satisfiability (SAT) problem. This constraint satisfaction problem consists of
a propositional formula with n variables and the requirement to find an assignment (true
or false) to each variable that makes the formula true. Thus there are b = 2 assignments
for each variable and N = 2n possible variable-value pairs. We consider the well-studied
NP-complete 3SAT problem where the formula is a conjunction of c clauses, each of which
is a disjunction of 3 (possibly negated) variables.
The SAT problem is readily represented by nogoods in the lattice of sets (Williams &
Hogg, 1994). As described in Sec. 2.2, there will be n necessary nogoods, each of size 2. In
addition, each distinct clause in the proposition gives a single nogood of size 3. This case
is thus of additional interest in having specified nogoods of two sizes. For evaluating the
quantum algorithm, we start at level 3 in the lattice. Thus the smallest case for which the
phase choices will inuence the result is for n = 5.
We generate random problems with a given number of clauses by selecting that number
of different nogoods of size 3 from among those sets not already excluded by the necessary nogoods14. For random 3SAT, the hard problems are concentrated near the transition (Mitchell et al., 1992) at c = 4:2n. Finally, from among these randomly generated
problems, we use only those that do in fact have a solution15 . Using randomly selected
soluble problems results in somewhat harder problems than using a prespecified solution.
Like other studies that need to examine many goods and nogoods in the lattice (Schrag &
Crawford, 1996), these results are restricted to much smaller problems than in most studies
of random SAT. Consequently, the transition region is rather spread out. Furthermore,
the additional structure of the necessary nogoods and the larger size of the constraints,
compared with the previous class of problems, makes it more likely that larger problems
will be required to see the asymptotic scaling behavior. However, at least some asymptotic
behaviors have been observed (Crawford & Auton, 1993) to persist quite accurately even
for problems as small as n = 3, so some indication of the scaling behavior is not out of the
question for the small problems considered here.
4.2.1 Phase Transition

The behavior of the algorithm as a function of the ratio of clauses to variables is shown
in Fig. 11 using the phase inversion method. This shows the phase transition behavior.
Comparing to Fig. 5, this also shows the class of random 3SAT problems is harder, on
average, for the quantum algorithm than the class of unstructured problems.
14. This differs slightly from other studies of random 3SAT in not allowing duplicate clauses in the propositional formula.
15. For the values of c=n and small problems examined here, there are enough soluble instances randomly
generated that there is no need to rely on a prespecified solution to eciently find soluble test problems.

118

fiQuantum Computing and Phase Transitions in Combinatorial Search
<T>
70
60
50
40
30
20
10
2

4

6

c/n

8

Figure 11: Average number of tries to find a solution with the quantum search algorithm
for random 3SAT as a function of c=n, using the phase inversion method. The
curves correspond to n = 5 (black) and n = 10 (gray).
0.2
p(soln)

p(soln)

0.2
0.1
0.05
0.02
0.01

0.1
0.05
0.02

5

6

7

0.01
8
n

9

10

11

5

6

7

8

9

10 11

n

Figure 12: Scaling of the probability to find a solution, using the phase inversion method,
as a function of the number of variables for random 3SAT problems. The curves
correspond to different clause to variable ratios: 2 (dashed), 4 (solid), 6 (gray)
and 8 (gray, dashed). This is shown on log and log-log scales (left and right
plots, respectively).
4.2.2 Scaling

The scaling of the probability to find a solution is shown in Fig. 12 using the phase inversion method. More limited experiments with the random phase method showed the same
behavior as seen with the unstructured class of problems: somewhat worse performance but
similar scaling behavior. The results here are less clear-cut than those of Fig. 8. For c=n = 2
the results are consistent with either polynomial or exponential scaling. For problems with
more constraints, exponential scaling is a somewhat better fit.
In addition to the general scaling trend, there is also a noticeable difference in behavior
between cases with an even and odd number of variables. This is due to the behavior of the
amplitude at each step in the lattice. Instead of a monotonic decrease in the concentration of
amplitude into goods, there is an oscillatory behavior in which amplitude alternates between
119

fiHogg

probability

1

0.95

0.9

0.85
3

4

6

8

10

12

level

Figure 13: Probability in goods (i.e., consistent sets) as a function of level in the lattice for
3SAT problems with no constraints. This shows the behavior for n equal to 9
(gray dashed), 10 (black dashed), 11 (gray) and 12 (black). For each problem,
the final probability at level n is the probability a solution is obtained with the
quantum algorithm.
dispersing and being focused into goods at different levels. An extreme example of this
behavior is shown in Fig.fi 13fifor 3SAT problems with no constraints, i.e., c = 0. Specifically,
2
P
at level i this shows s fifi s(i)fifi where the sum is over all sets s at level i in the lattice that are
consistent, which, for these problems with no constraints, are all assignments to i variables.
This is the probability that a good would be found if the algorithm were terminated at
level i and gives an indication of how well the algorithm concentrates amplitude among
consistent states. In this case, the expanded search space of the quantum algorithm results
in slightly worse performance than random selection from among complete assignments
(all of which are solutions in this case). Each search starts with all amplitude in goods
at level 3. Then the total probability in goods alternately decreases and increases as the
map proceeds up to the solution level. Cases with an even number of variables (the black
curves in the figure) end on a step that decreases the probability in goods, resulting in
relatively lower performance compared to the odd variable cases (gray curves). Although
this might suggest an improvement for the even n cases by starting in level 2 rather than
level 3, in fact this turns out not to be the case: starting in level 2 gives essentially the
same behavior for the upper levels as starting the search from level 3 of the lattice due to
one oscillation at intermediate levels that takes 2 steps to complete. Increasing the value of
c=n, i.e., examining SAT problems with constraints, reduces the extent of the oscillations,
particularly in higher levels of the lattice, and eventually results in monotonic decrease
in probability as the search moves up the lattice. Nevertheless, for problems with a few
constraints the existence of these oscillations gives rise to the observed difference in behavior
between cases with an even and odd number of variables. These oscillations are also seen
for underconstrained cases of unstructured problems in Figs. 7 and 8.
While Fig. 13 shows that the oscillatory behavior decreases for larger problems, it also
suggests there may be more appropriate choices of the phases. Specifically, it may be
possible to obtain a greater concentration of amplitude into solutions by allowing more
120

fiQuantum Computing and Phase Transitions in Combinatorial Search

3

ratio

10
2
10
1
10

5

6

7

8
n

9

10

11

Figure 14: Scaling of the ratio of the probability to find a solution using the quantum
algorithm to the probability to find a solution by random selection at the solution
level as a function of the number of variables for random 3SAT problems with
clause to variable ratio equal to 4. The solid and dashed curves correspond
to using the phase inversion and random phase methods, respectively. The
black curves compare to random selection among complete sets, while the gray
compare to selection only from among complete assignments. The curves are
close to linear on this log scale indicating exponential improvement over the
direct selection from among complete sets.
dispersion into nogoods at intermediate levels of the lattice or using an initial condition
with some amplitude in nogoods. If so, this would represent a new policy for selecting the
phases that takes into account the problem-independent structure of the necessary nogoods.
This would be somewhat analogous to focusing light with a lens: paths in many directions
are modified by the lens to cause a convergence to a single point.
More definite results are obtained for the improvement over random selection. Specifically, Fig. 14 shows an exponential improvement for both the phase inversion and random
phase methods, corresponding to the behavior for unstructured problems in Fig. 9. Similar
improvement is seen for other values of c=n as well: as in Fig. 9 the more highly constrained
problems give larger improvements. A more stringent comparison is with random selection
from among complete assignments (i.e., each variable given a single value) rather than from
among all complete sets of variable-value pairs. This is also shown in Fig. 14, appearing to
grow exponentially as well. This is particularly significant because the quantum algorithm
uses a larger search space containing the necessary nogoods. Another view of this comparison is given in Fig. 15, showing the probabilities to find a solution with the quantum
search and random selection from among complete assignments. We conclude from these
results that the additional structure of necessary nogoods and constraints of different sizes
is qualitatively similar to that for unstructured random problems but a detailed comparison
of the scaling behaviors requires examining larger problem sizes.

121

fiHogg

0.2

p(soln)

0.1
0.05
0.02
0.01
0.005
4

5

7

6

8

9

10

11

n

Figure 15: Comparison of scaling of probability to find a solution with the quantum algorithm using the phase inversion method (solid curve) and by random selection
from among complete assignments (gray curve) for c=n = 4.

5. Discussion

In summary, we have introduced a quantum search algorithm and evaluated its average
behavior on a range of small search problems. It appears to increase the amplitude into
solution states exponentially compared to evaluating and measuring a quantum superposition of potential solutions directly. Moreover, this method exhibits the same transition
behavior, with its associated concentration of hard problems, as seen with many classical
search methods. It thus extends the range of methods to which this phenomenon applies.
More importantly, this indicates the algorithm is effectively exploiting the same structure
of search problems as, say, classical backtrack methods, to prune unproductive search directions. It is thus a major improvement over the simple applications of quantum computing
to search problems that behave essentially the same as classical generate-and-test, a method
that completely ignores the possibility of pruning and hence doesn't exhibit the phase transition.
The transition behavior is readily understood because problems near the transition point
have many large partial goods that do not lead to solutions (Williams & Hogg, 1994). Thus
there will be a relatively high proportion of paths through the lattice that appear good for
quite a while but eventually give deadends. A choice of phases based on detecting nogoods
will not be able to work on these paths until near the solution level and hence give less
chance to cancel out or move amplitude to those paths that do in fact lead to solutions.
Hence problems with many large partial goods are likely to prove relatively dicult for any
quantum algorithms that operate by distinguishing goods from nogoods of various sizes.
There remain many open questions. In the algorithm, the division between a problem{
independent mapping through the lattice and a simple problem-specific adjustment to
phases allows for a range of policies for selecting the phases. It would be useful to understand the effect of different policies in the hope of improving the concentration of amplitude
into solutions. For example, the use of phases has two distinct jobs: first, to keep amplitude moving up along good sets rather than diffusing out to nogoods, and second, when
122

fiQuantum Computing and Phase Transitions in Combinatorial Search

a deadend is reached (i.e., a good set that has no good supersets) to send the amplitude
at this deadend to a promising region of the search space, possibly very far from where
the deadend occurred. These goals, of keeping amplitude concentrated on the one hand
and sending it away on the other, are to some extent contradictory. Thus it may prove
worthwhile to consider different phase choice policies for these two situations. Furthermore,
the mapping through the lattice is motivated by classical methods that incrementally build
solutions by moving from sets to supersets in the lattice. Instead of using unitary maps
at each step that are as close as possible to this classical behavior, other approaches could
allow more significant spreading of the amplitude at intermediate levels in the lattice and
only concentrate it into solutions in the last few steps. It may prove fruitful to consider another type of mapping based on local repair methods moving among neighbors of complete
sets. In this case, sets are evaluated based on the number of constraints they violate so an
appropriate phase selection policy could depend on this number, rather than just whether
the set is inconsistent or not. These possibilities may also suggest new probabilistic classical
algorithms that might be competitive with existing heuristic search methods.
As a new example of a search method exhibiting the transition behavior, this work
raises the same issues as prior studies of this phenomenon. For instance, to what extent
does this behavior apply to more realistic classes of problems, such as those with clustering
inherent in situations involving localized interactions (Hogg, 1996). This will be dicult
to check empirically due to the limitation to small problems that are feasible for a classical
simulation of this algorithm. However the observation that this behavior persists for many
classes of problems with other search methods suggests it will be widely applicable. It is
also of interest to see if other phase transition phenomena appear in these quantum search
algorithms, such as observed in optimization searches (Cheeseman et al., 1991; Pemberton
& Zhang, 1996; Zhang & Korf, 1996; Gent & Walsh, 1995). There may also be transitions
unique to quantum algorithms, for example in the required coherence time or sensitivity to
environmental noise.
For the specific instances of the algorithm presented here, there are also some remaining
issues. An important one is the cost of the mapping from one level to the next in terms
of more basic operations that might be realized in hardware, although the simple structure
of the matrices involved suggest this should not be too costly. The scaling behavior of the
algorithm for larger cases is also of interest, which can perhaps be approached by examining
the asymptotic nature of the matrix coecients of Eqs. 17 and 19.
An important practical question is the physical implementation of quantum computers in general (Barenco et al., 1995; Sleator & Weinfurter, 1995; Cirac & Zoller, 1995),
and the requirements imposed by the algorithm described here. Any implementation of
a quantum computer will need to deal with two important diculties (Landauer, 1994).
First, there will be defects in the construction of the device. Thus even if an ideal design
exactly produces the desired mapping, occasional manufacturing defects and environmental
noise will introduce errors. We thus need to understand the sensitivity of the algorithm's
behavior to errors in the mappings. Here the main diculty is likely to be in the problemindependent mapping from one level of the lattice to the next, since the choice of phases in
the problem-specific part doesn't require high precision. In this context we should note that
standard error correction methods cannot be used with quantum computers in light of the
requirement that all operations are reversible. We also need to address the extent to which
123

fiHogg

such errors can be minimized in the first place, thus placing less severe requirements on the
algorithm. Particularly relevant in this respect is the possibility of drastically reducing defects in manufactured devices by atomically precise control of the hardware (Drexler, 1992;
Eigler & Schweizer, 1990; Muller et al., 1995; Shen, Wang, Abeln, Tucker, Lyding, Avouris,
& Walkup, 1995). There are also uniquely quantum mechanical approaches to controlling
errors (Berthiaume, Deutsch, & Jozsa, 1994) based on partial measurements of the state.
This work could substantially extend the range of ideal quantum algorithms that will be
possible to implement.
The second major diculty with constructing quantum computers is maintaining coherence of the superposition of states long enough to complete the computation. Environmental
noise gradually couples to the state of the device, reducing the coherence and eventually
limiting the time over which a superposition can perform useful computations (Unruh, 1995;
Chuang, Laamme, Shor, & Zurek, 1995). In effect, the coupling to the environment can be
viewed as performing a measurement on the quantum system, destroying the superposition
of states. This problem is particularly severe for proposed universal quantum computers
that need to maintain superpositions for arbitrarily long times. In the method presented
here, the number of steps is known in advance and could be implemented as a special purpose search device (for problems of a given size) rather than as a program running on a
universal computer. Thus a given achievable coherence time would translate into a limit
on feasible problem size. To the extent that this limit can be made larger than feasible for
alternative classical search methods, the quantum search could be useful.
The open question of greatest theoretical interest is whether this algorithm or simple
variants of it can concentrate amplitude into solutions suciently to give a polynomial,
rather than exponential, decrease in the probability to find a solution of any NP search
problem with small constraints. This is especially interesting since this class of problems
includes many well-studied NP-complete problems such as graph coloring and propositional
satisfiability. Even if this is not so in the worst case, it may be so on average for some
classes of otherwise dicult real-world problems. While it is by no means clear to what
extent quantum coherence provides more powerful computational behavior than classical
machines, a recent proposal for rapid factoring (Shor, 1994) is an encouraging indication of
its capabilities.
A more subtle question along these lines is how the average scaling behaves away from
the transition region of hard problems. In particular, can such quantum algorithms expand
the range of the polynomially scaling problems seen for highly underconstrained or overconstrained instances? If so, this would provide a class of problems of intermediate diculty
for which the quantum search is exponentially faster than classical methods, on average.
This highlights the importance of broadening theoretical discussions of quantum algorithms
to include typical or average behaviors in addition to worst case analyses. More generally,
are there any differences in the phase transition behaviors or their location compared with
the usual classical methods? These questions, involving the precise location of transition
points, are not currently well understood even for classical search algorithms. Thus a comparison with the behavior of this quantum algorithm may help shed light on the nature of
the various phase transitions that seem to be associated with the intrinsic structure of the
search problems rather than with specific search algorithms.
124

fiQuantum Computing and Phase Transitions in Combinatorial Search

Acknowledgements
I thank John Gilbert, John Lamping and Steve Vavasis for their suggestions and comments on this work. I have also benefited from discussions with Peter Cheeseman, Scott
Clearwater, Bernardo Huberman, Don Kimber, Colin Williams, Andrew Yao and Michael
Youssefmir.

References

Barenco, A., Deutsch, D., & Ekert, A. (1995). Conditional quantum dynamics and logic
gates. Physical Review Letters, 74, 4083{4086.
Benioff, P. (1982). Quantum mechanical hamiltonian models of Turing machines. J. Stat.
Phys., 29, 515{546.
Bennett, C. H. (1992). Quantum cryptography: Uncertainty in the service of privacy.
Science, 257, 752{753.
Bernstein, E., & Vazirani, U. (1993). Quantum complexity theory. In Proc. 25th ACM
Symp. on Theory of Computation, pp. 11{20.
Berthiaume, A., Deutsch, D., & Jozsa, R. (1994). The stabilization of quantum computations. In Proc. of the Workshop on Physics and Computation (PhysComp94), pp.
60{62 Los Alamitos, CA. IEEE Press.
Cerny, V. (1993). Quantum computers and intractable (NP-complete) computing problems.
Physical Review A, 48, 116{119.
Cheeseman, P., Kanefsky, B., & Taylor, W. M. (1991). Where the really hard problems
are. In Mylopoulos, J., & Reiter, R. (Eds.), Proceedings of IJCAI91, pp. 331{337 San
Mateo, CA. Morgan Kaufmann.
Chuang, I. L., Laamme, R., Shor, P. W., & Zurek, W. H. (1995). Quantum computers,
factoring and decoherence. Science, 270, 1633{1635.
Cirac, J. I., & Zoller, P. (1995). Quantum computations with cold trapped ions. Physical
Review Letters, 74, 4091{4094.
Crawford, J. M., & Auton, L. D. (1993). Experimental results on the cross-over point
in satisfiability problems. In Proc. of the 11th Natl. Conf. on Artificial Intelligence
(AAAI93), pp. 21{27 Menlo Park, CA. AAAI Press.
Deutsch, D. (1985). Quantum theory, the Church-Turing principle and the universal quantum computer. Proc. R. Soc. London A, 400, 97{117.
Deutsch, D. (1989). Quantum computational networks. Proc. R. Soc. Lond., A425, 73{90.
Dirac, P. A. M. (1958). The Principles of Quantum Mechanics (4th edition). Oxford.
DiVincenzo, D. P. (1995). Quantum computation. Science, 270, 255{261.
125

fiHogg

DiVincenzo, D. P., & Smolin, J. (1994). Results on two-bit gate design for quantum computers. In Proc. of the Workshop on Physics and Computation (PhysComp94), pp.
14{23 Los Alamitos, CA. IEEE Press.
Drexler, K. E. (1992). Nanosystems: Molecular Machinery, Manufacturing, and Computation. John Wiley, NY.
Eigler, D. M., & Schweizer, E. K. (1990). Positioning single atoms with a scanning tunnelling
microscope. Nature, 344, 524{526.
Ekert, A., & Jozsa, R. (1995). Shor's quantum algorithm for factorising numbers. Rev.
Mod. Phys. to appear.
Feynman, R. P. (1986). Quantum mechanical computers. Foundations of Physics, 16,
507{531.
Feynman, R. P. (1985). QED: The Strange Theory of Light and Matter. Princeton Univ.
Press, NJ.
Freuder, E. C., & Wallace, R. J. (1992). Partial constraint satisfaction. Artificial Intelligence, 58, 21{70.
Garey, M. R., & Johnson, D. S. (1979). A Guide to the Theory of NP-Completeness. W.
H. Freeman, San Francisco.
Gent, I. P., & Walsh, T. (1995). The TSP phase transition. Tech. rep. 95-178, Dept. of
Computer Science, Univ. of Strathclyde.
Golub, G. H., & Loan, C. F. V. (1983). Matrix Computations. John Hopkins University
Press, Baltimore, MD.
Hogg, T. (1994). Phase transitions in constraint satisfaction search. A World Wide Web
page with URL ftp://parcftp.xerox.com/pub/dynamics/constraints.html.
Hogg, T. (1996). Refining the phase transitions in combinatorial search. Artificial Intelligence, 81, 127{154.
Hogg, T., Huberman, B. A., & Williams, C. (1996). Phase transitions and the search
problem. Artificial Intelligence, 81, 1{15.
Hogg, T., & Williams, C. P. (1994). The hardest constraint problems: A double phase
transition. Artificial Intelligence, 69, 359{377.
Jozsa, R. (1992). Computation and quantum superposition. In Proc. of the Physics of
Computation Workshop. IEEE Computer Society.
Karlin, S., & Taylor, H. M. (1975). A First Course in Stochastic Processes (2nd edition).
Academic Press, NY.
Kimber, D. (1992). An introduction to quantum computation. Tech. rep., Xerox PARC.
126

fiQuantum Computing and Phase Transitions in Combinatorial Search

Kirkpatrick, S., Gelatt, C. D., & Vecchi, M. P. (1983). Optimization by simulated annealing.
Science, 220, 671{680.
Landauer, R. (1991). Information is physical. Physics Today, 44 (5), 23{29.
Landauer, R. (1994). Is quantum mechanically coherent computation useful? In Feng, D. H.,
& Hu, B.-L. (Eds.), Proc. of the Drexel-4 Symposium on Quantum Nonintegrability.
International Press.
Lloyd, S. (1993). A potentially realizable quantum computer. Science, 261, 1569{1571.
Lloyd, S. (1995). Quantum-mechanical computers. Scientific American, 273 (4), 140{145.
Mackworth, A. (1992). Constraint satisfaction. In Shapiro, S. (Ed.), Encyclopedia of Artificial Intelligence, pp. 285{293. Wiley.
Margolus, N. (1990). Parallel quantum computation. In Zurek, W. H. (Ed.), Complexity,
Entropy and the Physics of Information, pp. 273{287. Addison-Wesley, New York.
Minton, S., Johnston, M. D., Philips, A. B., & Laird, P. (1992). Minimizing conicts: A
heuristic repair method for constraint satisfaction and scheduling problems. Artificial
Intelligence, 58, 161{205.
Mitchell, D., Selman, B., & Levesque, H. (1992). Hard and easy distributions of SAT
problems. In Proc. of the 10th Natl. Conf. on Artificial Intelligence (AAAI92), pp.
459{465 Menlo Park. AAAI Press.
Motwani, R., & Raghavan, P. (1995). Randomized Algorithms. Cambridge University Press.
Muller, W. T., Klein, D. L., Lee, T., Clarke, J., McEuen, P. L., & Schultz, P. G. (1995). A
strategy for the chemical synthesis of nanostructures. Science, 268, 272{273.
Pemberton, J. C., & Zhang, W. (1996). Epsilon-transformation: Exploiting phase transitions to solve combinatorial optimization problems. Artificial Intelligence, 81, 297{
325.
Prosser, P. (1996). An empirical study of phase transitions in binary constraint satisfaction
problems. Artificial Intelligence, 81, 81{109.
Schrag, R., & Crawford, J. (1996). Implicates and prime implicates in random 3-SAT.
Artificial Intelligence, 81, 199{222.
Selman, B., Levesque, H., & Mitchell, D. (1992). A new method for solving hard satisfiability
problems. In Proc. of the 10th Natl. Conf. on Artificial Intelligence (AAAI92), pp.
440{446 Menlo Park, CA. AAAI Press.
Shen, T. C., Wang, C., Abeln, G. C., Tucker, J. R., Lyding, J. W., Avouris, P., & Walkup,
R. E. (1995). Atomic-scale desorption through electronic and vibrational excitation
mechanisms. Science, 268, 1590{1592.
127

fiHogg

Shor, P. W. (1994). Algorithms for quantum computation: Discrete logarithms and factoring. In Goldwasser, S. (Ed.), Proc. of the 35th Symposium on Foundations of
Computer Science, pp. 124{134. IEEE Press.
Sleator, T., & Weinfurter, H. (1995). Realizable universal quantum logic gates. Physical
Review Letters, 74, 4087{4090.
Smith, B. M., & Dyer, M. E. (1996). Locating the phase transition in binary constraint
satisfaction problems. Artificial Intelligence, 81, 155{181.
Svozil, K. (1995). Quantum computation and complexity theory I. Bulletin of the European
Association of Theoretical Computer Sciences, 55, 170{207.
Unruh, W. G. (1995). Maintaining coherence in quantum computers. Physical Review A,
41, 992.
Williams, C. P., & Hogg, T. (1994). Exploiting the deep structure of constraint problems.
Artificial Intelligence, 70, 73{117.
Wolfram, S. (1991). Mathematica: A System for Doing Mathematics by Computer (2nd
edition). Addison-Wesley, NY.
Zhang, W., & Korf, R. E. (1996). A unified view of complexity transitions on the travelling
salesman problem. Artificial Intelligence, 81, 223{239.
Zhu, L., Kleiman, V., Li, X., Lu, S. P., Trentelman, K., & Gordon, R. J. (1995). Coherent
laser control of the product distribution obtained in the photoexcitation of HI. Science,
270, 77{80.

128

fiJournal of Artificial Intelligence Research 4 (1996) 287{339

Submitted 1/96; published 5/96

Planning for Contingencies: A Decision-based Approach
Louise Pryor

louisep@aisb.ed.ac.uk

Gregg Collins

collins@ils.nwu.edu

Department of Artificial Intelligence, University of Edinburgh
80 South Bridge
Edinburgh EH1 1HN, Scotland
The Institute for the Learning Sciences, Northwestern University
1890 Maple Avenue
Evanston, IL 60201, USA

Abstract

A fundamental assumption made by classical AI planners is that there is no uncertainty
in the world: the planner has full knowledge of the conditions under which the plan will
be executed and the outcome of every action is fully predictable. These planners cannot
therefore construct contingency plans, i.e., plans in which different actions are performed in
different circumstances. In this paper we discuss some issues that arise in the representation
and construction of contingency plans and describe Cassandra, a partial-order contingency
planner. Cassandra uses explicit decision-steps that enable the agent executing the plan to
decide which plan branch to follow. The decision-steps in a plan result in subgoals to acquire
knowledge, which are planned for in the same way as any other subgoals. Cassandra thus
distinguishes the process of gathering information from the process of making decisions.
The explicit representation of decisions in Cassandra allows a coherent approach to the
problems of contingent planning, and provides a solid base for extensions such as the use
of different decision-making procedures.

1. Introduction
Many plans that we use in our everyday lives specify ways of coping with various problems
that might arise during their execution. In other words, they incorporate contingency plans .
The contingencies involved in a plan are often made explicit when the plan is communicated
to another agent, e.g., \try taking Western Avenue, but if it's blocked use Ashland," or
\crank the lawnmower once or twice, and if it still doesn't start jiggle the spark plug." Socalled classical planners 1 cannot construct plans of this sort, due primarily to their reliance
on three perfect knowledge assumptions:
1. The planner has full knowledge of the initial conditions in which the plan will be
executed, e.g., whether Western Avenue will be blocked;
2. All actions have fully predictable outcomes, e.g., cranking the lawnmower will definitely either work or not work;
1. This category includes systems such as strips (Fikes & Nilsson, 1971), hacker (Sussman, 1975), noah
(Sacerdoti, 1977) and molgen (Stefik, 1981a, 1981b). Recent classical planners include tweak (Chapman, 1987), snlp (McAllester & Rosenblitt, 1991) and ucpop (Penberthy & Weld, 1992). The term is
due to Wilkins (1988).

c 1996 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiPryor & Collins
3. All change in the world occurs through actions performed by the planner, e.g., nobody
else will use the car and empty its gas tank.
Under these assumptions the world is totally predictable ; there is no need for contingency
plans.
The perfect knowledge assumptions are an idealization of the planning context that is intended to simplify the planning process. They allow the development of planning algorithms
that have provable properties such as completeness and correctness. Unfortunately, there
are few domains in which they are realistic: mostly, the world is to some extent unpredictable. Relying on the perfect knowledge assumptions in an unpredictable world may prove
cost-effective if the planner's uncertainty about the domain is small, or if the cost of recovering from a failure is low. In general, however, they may lead the planner to forgo options
that would have been available had potential problems been anticipated in advance. For
example, on the assumption that the weather will be sunny, as forecast, you may neglect to
take along an umbrella; if the forecast later turns out to be erroneous, it is then impossible
to use the umbrella to stay dry. When the cost of recovering from failure is high, failing to
prepare for possible problems in advance can be an expensive mistake. In order to avoid
mistakes of this sort, an autonomous agent in a complex domain must be able to make and
execute contingency plans.
Recently, we and a number of other researchers have begun investigating the possibility
of relaxing the perfect knowledge assumptions while staying close to the framework of
classical planning (Etzioni, Hanks, Weld, Draper, Lesh, & Williamson, 1992; Peot & Smith,
1992; Pryor & Collins, 1993; Draper, Hanks, & Weld, 1994a; Goldman & Boddy, 1994a).
Our work is embodied in Cassandra,2 a contingency planner whose plans have the following
features:
 They include specific decision steps to determine which of the possible courses of
action to pursue;
 Information gathering steps are distinct from decision-steps;
 The circumstances in which it is possible to perform an action are distinguished from
those in which it is necessary to perform it.

1.1 Issues for a Contingency Planner

A contingency planner must be able to construct plans that can be expected to succeed
despite unknown initial conditions and uncertain outcomes of nondeterministic actions. An
effective contingency planner must possess the following capabilities:
 It must be able to anticipate outcomes of nondeterministic actions;
 It must be able to recognize when an uncertain outcome threatens the achievement
of a goal;
 It must be able to make contingency plans for all possible outcomes of the various
sources of uncertainty that affect a given plan;
2. Cassandra was a Trojan prophet who was fated not to be believed when she accurately predicted future
disasters. An earlier version of Cassandra was described in (Pryor & Collins, 1993).

288

fiPlanning for Contingencies: A Decision-based Approach


It must be able to schedule sensing actions that detect the occurrence of a particular
contingency;



It must produce plans that can be executed correctly regardless of which contingency
arises.

The design of Cassandra addresses these issues. However, there are several issues that
have not been addressed:


We have not considered the problem of determining whether it is worth planning for
a particular outcome;



Cassandra is not a probabilistic planner: it cannot make use of any information about
the likelihood or otherwise of any events;



We have ignored the possibility of interleaving planning and execution (but see Section 7.4);



Cassandra does not handle exogenous events;



The version of Cassandra described here cannot solve Moore's bomb in the toilet
problem (McDermott, 1987): it can only find plans that involve deciding between
courses of action that will succeed in different contingencies (but see Section 6.5.5).

Cassandra assumes that all sources of uncertainty and all their possible outcomes are known,
and plans for all those that affect the achievement of its goals. It is firmly in the classical
planning mold: its job is to construct plans that are guaranteed to achieve its goals. It does
not decide when to plan, or what to plan for. Moreover, although we believe that Cassandra
is sound and complete, it is not systematic. In addition, the current implementation is too
slow to be of practical use.

1.2 A Note on Terminology

The word conditional is used in a variety of senses in the literature. We avoid its use
altogether, except when describing the work of other authors who use it in specialized
senses: for example, the conditional actions and conditioning of Peot and Smith (1992).
We use the term contingency plan to refer to a plan that contains actions that may or may
not actually be executed, depending on the circumstances that hold at the time. We use
the term context-dependent to refer to action effects that depend on the context in which
the action is performed.

1.3 Outline

In this paper we present Cassandra, describe its algorithm in some detail, discuss the
approach it takes to some important issues in contingency planning, and show how it handles
a variety of example problems.
We start by describing the structure of Cassandra's plans. Section 2 describes how
Cassandra represents actions, including those with uncertain outcomes; explains the system
289

fiPryor & Collins
of labels that allows the determination of which of the alternative courses of action in a
contingency plan should be pursued; and introduces the notion of explicit decision steps.
Section 3 briey describes the basic planning algorithm in the absence of uncertainty.
Section 4 explains how the algorithm is extended to handle uncertain outcomes of actions.
In particular, the structure of Cassandra's decisions is considered, as are the problems of
ensuring the soundness of the plan that is constructed. The resulting algorithm is described
in detail and its properties are discussed in Section 5.
In Section 6 we consider some issues that arise in contingency planning. Section 7
describes related work on planning under uncertainty. Finally, Section 8 summarizes the
contributions of this work and discusses its limitations.

2. Cassandra's Plan Representation

Cassandra's representation of contingency plans has three major components:
 An action representation that supports uncertain outcomes;
 A plan schema;
 A system of labels for keeping track of which elements of the plan are relevant in
which contingencies.
These components are described in the remainder of this section.

2.1 Action Representation

Cassandra's action representation is a modified form of the strips operator (Fikes & Nilsson, 1971). It consists of the preconditions for executing an action and the effects that may
become true as a result of executing it, as in the standard strips operator. The syntax is
the same as that used in ucpop (Penberthy & Weld, 1992). As in ucpop, action effects
are more complex than standard strips effects: they may have an associated set of secondary preconditions , which govern the occurrence of that effect (Pednault, 1988, 1991).
Secondary preconditions allow the representation of context-dependent effects of actions,
i.e., effects that depend upon the context in which the action is executed. The use of secondary preconditions is critical to Cassandra's ability to represent uncertain effects, and
hence nondeterministic actions, as we discuss in Section 2.1.1.
Figure 1 shows a simplified operator schema for the action of making a selection from a
soft-drink machine (the effects describing how the \make another selection" indicator light
is turned off are omitted). The operator describes two possible effects of carrying out the
action: the effect of acquiring a soda, which depends on the secondary precondition that a
soda of the selected type is available; and the effect of having the \make another selection"
indicator light come on, which depends on the secondary precondition that a soda of the
selected type is not available. Both effects depend upon the preconditions that money has
been entered into the machine and that the machine is plugged in.
2.1.1 Representing Uncertain Effects

An uncertain effect in Cassandra is a context-dependent effect with an unknown precondition , i.e., a precondition the planner can neither knowingly perceive nor deliberately affect.
290

fiPlanning for Contingencies: A Decision-based Approach

Action:

(make-selection ?machine ?selection)

Preconditions:
Effects:

(:and (money-entered ?machine)
(plugged-in ?machine))
(:when
(available ?machine ?selection)
:effect (:and (dispensed ?selection)
(:not (money-entered ?machine))))
(:when
(:not (available ?machine ?selection))
:effect (another-selection-indicator-on ?machine))

secondary precondition
secondary precondition

Figure 1: Simplified representation of operating a vending machine
For example, a malfunctioning soft-drink machine may operate intermittently; if the planner is aware of the intermittent functioning, but unaware of the conditions that govern
this behavior, then the correct functioning of the device depends upon an unknown precondition. From the point of view of the planner, the uncertain effect is nondeterministic;
the planner cannot tell in advance whether it will occur. Clearly, this definition is fundamentally subjective: another planner with better information might be able to specify
precisely the conditions under which the device functions properly, for example if it knew
how the internal mechanism of the machine worked. As another example, consider what
happens when a coin is tossed: in principle, given perfect knowledge of all the forces and
distances involved, it would be possible to predict the outcome. In practice, such knowledge
is unavailable and the effect of the action is uncertain. In principle, it would be possible
to specify the conditions that would lead to the coin landing tails up; in practice, these
conditions are unknown.
It is interesting to note here that in some circumstances it might be possible for a planner
to learn to predict outcomes that it had hitherto regarded as uncertain: for example, if it
learned how the soda machine worked. \Unknown" refers only to the current situation.
Our representation would facilitate such learning, which would simply involve learning new
secondary preconditions rather than a whole new action representation.
Unknown preconditions play the same syntactic role as normal preconditions within the
operator schema; they are represented by expressions formed using the pseudo-predicate
:unknown. An effect that has a secondary precondition of this type will occur only in
certain contexts which cannot be distinguished by the planner from the contexts in which
it will not occur.
Figure 2 depicts a simplified example of an operator with an uncertain effect|it represents the action of operating a soft-drink machine that intermittently fails to dispense a
soda despite being operated correctly. This operator has two uncertain effects, one in which
the soda is dispensed, the other in which the soda is not dispensed.
Clearly, the uncertainty with respect to both these effects stems from a single underlying source, namely uncertainty about whether or not the machine will malfunction. In
effect, the two unknown preconditions in the operator represent alternative results of this
underlying source of uncertainty. This relationship is reected in the two arguments to
291

fiPryor & Collins

Action:
Preconditions:
Effects:

(enter-selection ?machine)
(:and (money-entered ?machine)
(plugged-in ?machine))
(:when (:and (available ?machine ?selection)
(:unknown ?ok T))
:effect (dispensed ?selection))
(:when (:and (available ?machine ?selection)
(:unknown ?ok F))
:effect (:not (dispensed ?selection)))
(:when (available ?machine ?selection)
:effect (:not (money-entered ?machine)))
(:when (:not (available ?machine ?selection))
:effect (another-selection-indicator-on ?machine))

uncertain effect
uncertain effect

Figure 2: Operating a faulty soft-drink machine
the :unknown pseudo-predicate, the first of which designates the source of uncertainty with
which it is associated, and the second of which designates the particular outcome of the
uncertainty that it represents. The possible contexts are effectively partitioned into a set
of equivalence classes, with each context in the same class producing the same outcome
of the uncertainty. The outcome is then used to label the equivalence class. A condition
of the form (:unknown ?class outcome) will be true if the actual context is in the class
designated by outcome.
Notice that each instantiation of the operator will introduce a new source of uncertainty,
which means that the first argument to the unknown precondition must be represented as
a variable in the operator schema. Cassandra binds this variable to a unique identifier (i.e.,
a skolem constant) when the operator is instantiated.
In Cassandra's representation it is assumed that different sources of uncertainty are
independent of each other. No source of uncertainty can be linked to uncertain outcomes
in more than one operator, but a single operator may introduce any number of sources of
uncertainty, each of which may have any number of outcomes. Each source of uncertainty
has an exhaustive set of mutually exclusive outcomes, each with a unique name.
2.1.2 Representing Other Sources of Uncertainty

A key element of Cassandra's design is the use of a single format to represent all sources
of uncertainty that affect planning. In particular, all uncertainty is assumed to be manifest
in uncertain effects of planning operators, as outlined above. Uncertainty about initial
conditions can be handled within this format by treating initial conditions as though they
were the effects of a phantom \start step" action. This treatment of initial conditions,
which was initially developed for reasons unrelated to the problem of representing uncertain
outcomes, is common to the snlp family of planners to which Cassandra belongs.
Cassandra's formulation ignores uncertainty that might stem from outside interference
during the execution of the agent's plans, except inasmuch as it can be represented as
292

fiPlanning for Contingencies: A Decision-based Approach
incomplete knowledge of initial conditions. This is, of course, a limitation of classical
planners in general; all change in the world is assumed to be caused directly by the actions
of the agent.

2.2 Basic Plan Representation

Cassandra's plan representation is an extension of that used in ucpop (Penberthy & Weld,
1992) and snlp (McAllester & Rosenblitt, 1991; Barrett, Soderland, & Weld, 1991), which is
in turn derived from the representation used in nonlin (Tate, 1977). A plan is represented
as a schema with the following components:
 A set of steps ;
 A set of anticipated effects of those steps;
 A set of links relating effects to the steps that produce and consume them (a step
consumes an effect when it requires that effect to achieve one of its preconditions).
Note that links in effect denote protection intervals , i.e., intervals over which particular
conditions must remain true in order for the plan to work properly.
 A set of variable bindings instantiating the operator schema;
 A partial ordering on the steps;
 A set of open conditions , i.e., unestablished goals;
 A set of unsafe links , i.e., links the conditions of which could be falsified by other
effects in the plan.
A plan is complete when it contains no open conditions and no unsafe links.

2.3 Representing Contingencies

A contingency plan is intended to achieve its goal regardless of which of the foreseeable
contingencies associated with it actually arise during execution. To construct a valid contingency plan, the planner must be able to enumerate these contingencies. The set of
foreseeable contingencies can be computed from the sources of uncertainty that are associated with the plan. In effect, a contingency is one possible set of outcomes for all relevant
sources of uncertainty.
2.3.1 Contingency Labels

Keeping track of whether a plan achieves its goal in every contingency is a somewhat
complex process. Cassandra, like cnlp, uses a system of labels to accomplish the necessary
bookkeeping (Peot & Smith, 1992). Each goal, step, and effect in Cassandra's plan is labeled
to indicate the contingencies in which that element participates:
 Goals are labeled to indicate the contingencies in which they must be achieved;
 Effects are labeled to indicate the contingencies in which they are expected to occur,
i.e., the contingencies in which the goals they satisfy arise;
293

fiPryor & Collins


Steps are labeled to indicate the contingencies in which they must be performed, i.e.,
the union of the contingencies in which any of their effects are expected to occur.

The preconditions of each effect become new goals, the labels of which correspond to the
labels on the effect that give rise to them.
In general, it is assumed that a particular step could be executed in any contingency,
albeit possibly to no purpose. However, it is sometimes necessary to rule a particular step
out of a particular contingency as a means of preventing its interference with the plan for
that contingency. For example, consider a plan to achieve the goal of having a coin heads
up, the first action of which is to toss the coin (see Section 4.2.3 for a detailed discussion of
this plan). In one contingency the coin lands heads up, and no further actions are required.
In another contingency, the coin lands tails up and must be turned over in order for the goal
to be achieved. It is clear, however, that the turning over action must not be performed
in the first contingency: doing so would mean that the goal of having the coin heads up is
not achieved. In Cassandra, ruling steps out is accomplished by associating negative labels
with plan steps to indicate those contingencies in which the steps are not to be executed.
Peot and Smith (1992) call this process conditioning .
In addition, every step that depends, directly or indirectly, on a particular outcome of
a given source of uncertainty is ruled out of every contingency that involves an alternative
outcome of that source of uncertainty. We discuss the reason for this restriction in more
detail below.
Cassandra's labeling system thus provides very clear guidance to the agent executing
the plan, which simply performs those steps whose positive labels reect the actual circumstances that hold at execution. Steps with neither positive nor negative labels involving the
current contingency will not affect the goals, but are not guaranteed to be executable. In
contrast, the agent executing a plan produced by cnlp is guided by the reason labels attached to steps. In cnlp's plans, an action need only be executed if at least one of the goals
represented in its reason labels is feasible. The agent must therefore have some method of
deciding which of the top-level goals are feasible. We assume this can be done by comparing
the context labels of each top-level goal (which are labeled because they are represented
as dummy actions) with the circumstances that actually hold. Cassandra's method is thus
simpler: the agent simply uses the positive labels of the plan steps instead of using the
labels attached to a step to indicate those goals whose context labels must be analyzed.
The general principles of label propagation in Cassandra are:


Positive labels, which denote that the plan element concerned contributes to goal
achievement in that contingency, propagate along causal links from subgoals to the
plan elements that establish them;



Negative labels, which denote that the plan element concerned would prevent goal
achievement in that contingency, propagate along causal links from effects to the plan
elements that they establish.

The details are given in Section 5.1.4.
294

fiPlanning for Contingencies: A Decision-based Approach

KEY
Link

On Western
Drive to
Western at
Belmont

On Western Check
traffic on
Western

Take Western
to Evanston
know
traffic
status

condition

Alternative
control flow

Decide
Take Belmont
to Ashland

On Belmont

On Ashland

Take Ashland
to Evanston

Figure 3: A plan that includes a decision-step
2.3.2 Representing Decisions

Planning can be seen as the process of deciding what to do in advance of when it is done
(Collins, 1987). The need for contingency plans arises when the necessary decisions cannot
be made in advance because of missing information (see Section 6.4). If the decisions cannot
be made in advance, they must be made when the plan is executed. The agent executing
a contingency plan must at some point decide which of the possible courses of action to
pursue, in other words which branch to take.
Previous work has in effect assumed that the agent will execute those steps that are
consistent with the contingency that actually holds (Warren, 1976; Peot & Smith, 1992).
However, the determination of which steps are consistent cannot (by definition) be made in
advance; in order to know which contingency holds during execution, the agent executing
the plan must in general gather information on which the decision can be based. To ensure
a viable plan, the planner must be able to guarantee that the steps required to gather
information do not conict with those required to carry out the rest of the plan. Therefore,
the planner must in general be able to include information gathering steps, as well as any
other steps that support decision making, in the plan it is constructing. Cassandra achieves
this by representing decisions explicitly as plan steps. The preconditions of these decisionsteps include goals to be in possession of information relevant to making the decision; the
scheduling of actions to obtain information is thus handled by the normal planning process.
For instance, consider the contingency plan alluded to above: \try taking Western Avenue, but if it's blocked use Ashland ." During the execution of such a plan, the agent must
at some point decide which branch of the plan to execute. The decision-step in this case
would have the precondition of knowing whether Western Avenue is blocked or not, which
would cause the planner to schedule an information-gathering action to check the trac
status on Western. This operation might in turn have the precondition of being on Western, which can be achieved by traveling to the junction of Western and Belmont. After
the decision is taken, the agent can either take Western up to Evanston or continue along
Belmont to Ashland.
Assuming the goal of the plan is to be in Evanston, the final plan might be as depicted in
Figure 3. Note that control ow after a decision is represented by heavy lines. Solid lines in
the diagram represent links, with the action at the tail of the link achieving a precondition
295

fiPryor & Collins
of the action at the head of the link. In this plan, the agent will take Western to Evanston
in one contingency, and will take Belmont to Ashland and then Ashland to Evanston in the
other.3
Notice that in order to determine the appropriate precondition for a given decision-step,
the planner must have some way of determining exactly what it will need to know in order
to make the decision at execution time. This somewhat complex determination depends in
part on how the decision-making process is to be carried out. In Cassandra, decisions are
modeled as the evaluation of a set of condition-action rules of the form:
if condition 1 then contingency 1
if condition 2 then contingency 2
...
if condition n then contingency n
Each possible outcome of a given uncertainty gives rise to a decision rule; the condition
of this decision-rule specifies a set of effects that the agent should test in order to determine
whether to execute the contingency plan for that outcome. For example, the decision-rules
for the driving plan example would look like this:
if Western Avenue is blocked
then execute contingency using Ashland
if Western Avenue is not blocked then execute contingency using Western
Cassandra's derivation of inference rules in decisions is explained in detail in Section 4.
The preconditions for a decision-step are goals to know the truth values of the conditions
in the decision-rules: they are thus knowledge goals (McCarthy & Hayes, 1969; Pryor, 1995)
(see Section 6.4). These goals are treated in the same way as are the preconditions of any
other step. Cassandra thus requires no other special provisions to allow the construction of
information-gathering plans.
The explicit representation of decision-steps provides a basis for supporting alternative
decision procedures. While Cassandra's basic model of the decision procedure is quite
simple, more complex decision procedures can be supported within the same framework (one
such procedure is described in Section 6.5.5). For example, the model could be changed to a
differential-diagnosis procedure. The representation of decision procedures as templates in
the same way that actions are represented as templates would allow the planner to choose
between alternative methods of making a decision in the same way as it can choose between
alternative methods of achieving a subgoal. An even better approach might be to formulate
an explicit goal to make a correct decision, and allow the system to construct a plan to
achieve that goal using inferential operators. However, this would in effect require that
the goals for these operators be stated in a meta-language describing the preconditions and
results of operators. We have not yet addressed this possibility in any detail.
Cassandra's separation of the gathering of information from the making of decisions
allows one information-gathering step to serve several decisions. This allows the exible use
of information-gathering actions; there is no effective difference between such actions and
any other action that may appear in a plan.
3. Appendix A shows the plans that Cassandra constructs for all the examples described in this paper.
This plan is in Section A.1.

296

fiPlanning for Contingencies: A Decision-based Approach
New step Add to the plan a new step that has an effect that will establish the open condition. Add the
step preconditions and the secondary preconditions of the effect as open conditions. The open condition
becomes a completed link.
Reuse step Make the open condition into a complete link from an effect of an existing plan step. Add the
secondary preconditions of the effect as open conditions.

Figure 4: Resolving open conditions

3. Planning Without Contingencies
In this section we briey review the basic planning algorithm on which Cassandra is based.
It follows closely that used in ucpop (Penberthy & Weld, 1992), which is in turn based on
snlp (McAllester & Rosenblitt, 1991). The principal difference between ucpop and snlp
is the use of secondary preconditions (see Collins & Pryor, 1992).
Cassandra does not attempt to construct a contingency plan until it encounters an
uncertainty. Up until this point, it constructs a plan in much the same manner as other
planners in the snlp family. In fact, if no uncertainty is ever introduced into the plan,
Cassandra will effectively function just as ucpop would under the same circumstances.
Planning proceeds through the alternation of two processes: resolving open conditions and
protecting unsafe links . Each of these processes involves a choice of methods, and may
therefore give rise to several alternative ways to extend the current plan. All possible
extensions are constructed, and a best-first search algorithm guides the planner's exploration
of the space of partial plans.
The initial plan consists of two steps: the start step, with no preconditions and with
the initial conditions as effects, and the goal step, with the goal conditions as preconditions
and with no effects. The planner attempts to modify its initial plan until it is complete :
i.e., until there are no open conditions and no unsafe links.

3.1 Resolving Open Conditions
The planning process is driven by the need to satisfy open conditions, which are initially
simply the input goals. In the course of planning to satisfy an open condition, new subgoals
may be generated; these are then added to the set of open conditions. The planner can
establish an open condition in one of two ways: by introducing a new step into the plan, or
by reusing an existing step by making use of one of its effects (see Figure 4). The secondary
preconditions of the effect that establishes the condition become open conditions. If a new
step is added, the preconditions of the step become open conditions as well. Finally, each
time an open condition is established, a link is added to the plan to protect the newly
established condition.
One way of establishing a condition is simply to notice that the condition is true in the
initial state. Because the initial conditions are treated as the results of the start operator,
which is always a part of the plan, this method can be treated as establishment by reusing
an existing step; indeed, this simplification is the motivation for representing the initial
conditions in this way.
297

fiPryor & Collins
A link establishing the condition Cond is unsafe if there is an effect Eff in the plan (other than the effect
SourceEff that establishes Cond and the (possible) effect GoalEff that is either established or disabled by the
link) with the following properties:
Unification One of the postconditions in Eff can possibly unify with either Cond or its negation;
Ordering The step that produces Eff can, according to the partial order, occur both before the step that
produces GoalEff and after the step that produces SourceEff.
An unsafe link may be resolved in one of three ways:
Ordering Modify the ordering of the steps in the plan to ensure that the step producing Eff occurs either
before the step that produces SourceEff or after the step that produces GoalEff;
Separation Modify the variable bindings of the plan to ensure that the threatening effect Eff cannot in fact
unify with the threatened condition Cond;
Preservation Introduce a new open condition in the plan to disable Eff. This new open condition is the
negation of one of Eff's secondary preconditions.

Figure 5: Unsafe links

3.2 Protecting Unsafe Links
Whenever an open condition is established, links in the plan may be jeopardized either
because a new step threatens an existing link, or because a new link is threatened by an
existing step. The situations in which a link is unsafe are shown in Figure 5. In general, a
link is considered unsafe if there is an effect in the plan that could possibly interfere with
the condition established by that link.
There are three general methods of protecting a threatened link (see Figure 5). First,
ordering can be used to constrain the threatening action to occur either before the beginning
or after the end of the threatened link. Second, the threatening effect and the threatened
link can be separated by imposing constraints on the variables involved so that the effect
cannot be unified with the established condition. Third, the link can be preserved by
generating a new subgoal to disable the effect that threatens the link.

4. Contingency Planning
Cassandra proceeds as described in the previous section until either the plan is completed
or an uncertainty is introduced. This section describes how uncertainties are introduced
and how they are handled.
As an example of a plan involving an uncertainty, let us consider a version of Moore's
classic \bomb in the toilet" problem (McDermott, 1987), in which the goal is bomb is
disarmed , and the initial conditions are bomb in package1 or bomb in package2 . The
uncertainty in this case lies in the initial conditions: depending on the outcome of the
uncertainty, the start operator can either have the effect that the bomb is in package1 or
the effect that the bomb is in package2 .

4.1 Contingencies
Uncertainty is introduced into a plan when an open condition in the plan is achieved by
an uncertain effect, i.e., an effect with an unknown precondition. In the bomb-in-the-toilet
298

fiPlanning for Contingencies: A Decision-based Approach

KEY
Move
package1
Pa
at cka
toi ge
let

Start

Link

condition

Link with
uncertain
effect

condition

Dunk
package1

Bomb in
package1

Bomb
disarmed

End

Figure 6: The introduction of uncertainty into a plan
example, for instance, Cassandra may achieve the condition bomb is disarmed by selecting
the dunk operator, which has the preconditions the package is at the toilet , and the bomb is
in the package . The condition the bomb is in the package can be established by identifying
it with the bomb is in package1 , which is an effect of the start operator. However, this
condition is uncertain, as can be determined by noting that it has an unknown precondition.
Cassandra will attempt to deal with this uncertainty by introducing a new contingency (or
new contingencies) into the plan. The state of the plan just after the introduction of the
uncertainty is illustrated in Figure 6.
4.1.1 Introducing Contingencies

Cassandra notices an uncertainty when its current plan becomes dependent upon a particular outcome of that uncertainty through the use of an uncertain effect, i.e., an effect
with an unknown precondition that specifies an outcome of that uncertainty. The plan
that Cassandra has built up to that point is in effect a plan branch for that outcome.
Since branches must also be constructed for all other possible outcomes of the uncertainty,
Cassandra makes a copy of its overall goal for each possible outcome of the uncertainty,
each copy carrying a label indicating the outcome of the uncertainty in which it must be
achieved. It thus effectively splits the plan into a set of branches, one for each possible
outcome of the uncertainty.4
In planning for these otherwise identical goals, Cassandra must make certain that no
element of the branch for the goal for one outcome relies on a different outcome of the same
uncertainty. In other words, no goal, nor any of its subgoals, may be achieved by any effect
that depends, directly or indirectly, on any outcome of the uncertainty other than the one in
the goal's label. As described above, Cassandra achieves this by using a system of negative
labels indicating contingencies from which particular plan elements must be excluded.
4. An alternative method would be to split the plan into two branches, regardless of the number of outcomes.
In this case, one branch would be associated with a given outcome of the uncertainty, while the other
would be associated with all other possible outcomes of that uncertainty. This is effectively how sensp
operates (Etzioni et al., 1992).

299

fiPryor & Collins

Move
package1

I
Bomb in 1
package

Start

KEY

Pa
at ckag
toil e1
et

Bomb
packagin
e2
2
age
ack ilet
P
Move
t to
package2 a

II
Dunk
package1

Bomb
disarm
e

Link

condition

Link with
uncertain
effect

condition

d

End
Dunk
package2

Bomb ed
disarm

III
IV

Element label classes
I

In package1 contingency

II

In package1 contingency
Out of package2 contingency

III

In package2 contingency
Out of package1 contingency

IV

In package2 contingency

Figure 7: A contingency plan to disarm a bomb
In the bomb-in-the-toilet example, when the plan is made dependent upon the uncertain
outcome bomb in package1 , a new copy of the top level goal bomb is disarmed is added to
the set of open conditions. The new copy is given a label indicating that it belongs to
contingency in which the bomb is in package2 .5 The existing top level goal and all its
subgoals are labeled to indicate that they belong to the contingency in which the bomb
is in package1 . The effect bomb in package1 , the action dunk package1 , and all effects of
the action dunk package1 are be labeled to indicate that they cannot play a role in the
contingency in which the bomb is in package2 .
Notice that the action move package1 , although it plays a role in the plan in the contingency in which the bomb is in package1 , does not in fact depend upon the bomb being
in package1 . It could in principle be made part of the plan for disarming the bomb in the
contingency in which the bomb is in package2 , were it to prove useful for anything. This is
indicated by the fact that it has no negative label for for the package2 contingency.
When Cassandra attempts to achieve the new open condition bomb is disarmed , it may
choose the dunk operator once again (notice that it is prohibited from using any effects of
the existing dunk operator). This new instance of the dunk operator in turn gives rise to a
subgoal to have the bomb be in the package that is dunked. This can only be achieved by
identification with the effect bomb in package2 . The plan thus constructed is depicted in
Figure 7 (the decision-step has been omitted for clarity) and is listed in Section A.2.
4.1.2 Uncertainties with Multiple Outcomes

Although the algorithm we have described can deal with uncertainties having any number
of possible outcomes, we have so far discussed only examples with two possible outcomes.
In fact, two-outcome uncertainties suce to describe the majority of problems that we have
5. Note that we are describing the contingency in this way for clarity of exposition. The actual label is
constructed as described in Section 2.3.1.

300

fiPlanning for Contingencies: A Decision-based Approach

Package at location1
Pickup
package

Drive ?car
to location1
Start

t
ot a
Rob tion1
loca

KEY

A

Link
Link with
uncertain
effect

Decide

condition

condition

Alternative
control flow

B

Incomplete
portion of
plan

Figure 8: A partial plan to pick up a package
considered. Indeed, technically, any situation could be described in terms of some number
of two-outcome uncertainties. However, it is not hard to think of situations that might
naturally be represented in terms of a source of uncertainty with more than two outcomes.
For example, suppose the planner were interested in getting hold of a particular object in
a situation in which the object were known to be in one of three places. In such a case,
the start pseudo-operator would naturally be represented as having three uncertain effects
(one for each possible location of the object) all associated with alternative outcomes of a
single source of uncertainty. Cassandra's plan for acquiring the object would then involve
three contingencies, one for each possible location.
4.1.3 Multiple Sources of Uncertainty

A plan may involve two or more sources of uncertainty, in which case the plan will have
more than one set of branches. For example, suppose Cassandra is given the goal of picking
up a package that is at one of two locations, and that one of two cars will be available
for it to use. If the uncertainty regarding the location of the package is encountered first
during the construction of the plan, Cassandra will respond by building a plan involving
two contingencies, one for each location. Call these contingencies A and B (see Figure 8
and Section A.3).
At some point during the construction of the plan for contingency A, Cassandra will
encounter the uncertainty concerning which car will be available and will make the current
plan dependent upon one particular outcome of that uncertainty. Since this new source of
uncertainty arises in the context of planning for contingency A, contingency A is in effect
bifurcated into two contingencies: A1 , in which the package is at location 1 and car 1 is
301

fiPryor & Collins

Package at location1
car1 available

Drive car1
to location1

Robot at
location1

Package at location1

Start

car2 available

Drive car2
to location1

1
Decide

at
Robot n1
io
t
loca

2

Link with
uncertain
effect

Decide

Pickup
package

KEY
Link

A

Pickup
package

condition

condition

Alternative
control flow

B

Incomplete
portion of
plan

Figure 9: A plan with two sources of uncertainty
available; and A2 , in which the package is at location 1 and car 2 is available). Cassandra
must replace all existing contingency A labels with contingency A1 labels. It must then
introduce a new copy of the top-level goal labeled with contingency A2 .
Note that Cassandra must plan from scratch to achieve the top-level goal in contingency
A2 , in spite of the fact that it already has a viable plan for the goal in contingency A1 .
This is necessary because situations may be encountered in which the only successful plans
involve using different methods to achieve the goal in the two contingencies. For example,
extreme differences between the two cars might necessitate different plans for driving them
(e.g., in a more detailed representation of the situation than we have presented here, such
differences might affect the routes on which the cars could be driven or the places in which
they could be parked). Cassandra must therefore consider all possible ways to achieve the
goal in contingency A2 in the search for a completion of the plan. If the particular car used
does not in fact affect the driving plan, then one path through the search space will result
in isomorphic contingency plans for A1 and A2 (see Figure 9 and Section A.4).
The same reasoning applies to the extension of the plan to deal with contingency B .
It cannot be assumed a priori that the plan for contingency B will in any way resemble
the plan constructed for contingency A. An interesting consequence of this is that the
302

fiPlanning for Contingencies: A Decision-based Approach
uncertainty concerning the availability of the cars does not necessarily arise in a given plan
for contingency B . For example, if the location of the package in contingency B were close
enough that the agent could get there without using a car, the final plan might have only
three contingencies: A1 (location 1 with car 1), A2 (location 1 with car 2), and B (location
2, on foot).
Cassandra may, of course, produce an extension of the plan in which a car is to be used
in contingency B as well, in which case it will again encounter the uncertainty associated
with the location of the car, and will proceed to bifurcate contingency B just as was done
previously for contingency A. In the limit, the plan will involve one contingency for every
member of the cross product of the possible outcomes of the relevant uncertainties. However,
it is important to note that not every member of the cross-product set must appear as a
contingency, since, as we have shown, some uncertainties may arise only given particular
outcomes of other uncertainties.

4.2 Decision-steps

When Cassandra encounters a new source of uncertainty it adds a decision-step to the plan
to represent the act of determining which path through the plan should be followed during
execution. The following ordering constraints are added to the plan at the same time:


The decision-step must occur after the step with which the uncertainty is associated;



The decision-step must occur before any step with a precondition whose achievement
depends on a particular outcome of the uncertainty.

4.2.1 Formulating Decision-rules

For a decision-step to be operational, there must be an effective procedure by which the
agent executing the plan can determine which decision to make. In Cassandra, the action
of deciding which contingency to execute is modeled as the evaluation of a set of conditionaction rules of the form:
If condition 1 then contingency 1
If condition 2 then contingency 2
If condition 3 then contingency 3
...
Cassandra annotates each decision-step in a plan with the set of rules that will be used
to make that decision. The executing agent can then make the decision by evaluating these
rules when it comes to the decision-step in the course of executing the plan. In order to
evaluate a decision-rule, the executing agent must be able to determine whether the rule's
antecedent holds. The preconditions for the decision-step must thus include goals to know
the current status of each condition that appears as an antecedent of a rule in this condition.
The preconditions of a decision-step become open conditions in the plan in the same way
as do the preconditions of any other step.
As the intended effect of evaluating the decision-rules is to choose the appropriate contingency given the outcome of a particular uncertainty, the conditions should be diagnostic
of particular outcomes of the uncertainty. The executing agent cannot, of course, directly
303

fiPryor & Collins
determine the outcome of an uncertainty, so it must infer it from the presence or absence
of effects that depend upon that outcome.
The most straightforward approach to constructing the antecedent conditions of a
decision-rule would be to analyze the plan operators to identify all the effects that could
be expected to result from a given outcome of the uncertainty, and make the condition be
the conjunction of these effects. However, this turns out to be overkill. In fact, it is only
necessary to check for those effects of a given outcome of an uncertainty that are actually
used to establish preconditions in the contingency associated with that outcome . In other
words, it is necessary only to verify that the contingency plan can, in fact, succeed. This has
the interesting consequence that the executing agent might, in principle, end up selecting a
contingency plan even though the outcome of the uncertainty were not the one with which
that plan was associated. Notice that this would not cause a problem in the execution of
the plan, since it would only occur if all the conditions for the plan's success were met. In
fact, as we shall see, Cassandra depends on this effect in certain circumstances.
The antecedent condition of the decision-rule is thus a conjunction of all the direct effects
of a particular outcome that are used to establish preconditions in the contingency plan
for that outcome. Decision-rules are constructed incrementally as the plan is elaborated.
We discuss Cassandra's construction of these rules in more detail in Section 4.2.3 below.
The approach we have used in formulating Cassandra's decision-rules is consistent with
Morgenstern's observation that an agent can execute a plan if it can \make sure" that all
the events in the plan are executable (Morgenstern, 1987).
4.2.2 Adding a Decision-rule in our Example

In the bomb-in-the-toilet example, Cassandra will introduce a decision-step to determine
whether or not the bomb is in package1 . As the uncertainty is in the initial conditions,
the decision will be constrained to occur after the start step. It must also occur before
either of the dunk actions, since these depend upon particular outcomes of the uncertainty.
The decide step will have a precondition to know whether the bomb is in package1 . If
there are actions available that would allow it to determine this|X-raying the box, for
example|Cassandra will achieve this precondition with one of those actions, and decide on
that basis which branch of the plan to execute.
4.2.3 How Cassandra Constructs Decision-rules

At the point in the planning process at which Cassandra constructs a decision-rule, only
one precondition in the plan is known to depend upon a particular outcome of the uncertainty that gave rise to the decision: namely, the one that led to Cassandra discovering
the uncertainty in the first place. The decision-rule set that Cassandra initially builds thus
looks like this:
If effect 1 then contingency 1
If T
then contingency 2
If T
then contingency 3
...
During the construction of the plan, Cassandra must modify this initial rule set each time an
effect depending directly on the source of uncertainty is used to establish an open condition
304

fiPlanning for Contingencies: A Decision-based Approach

Action:

(toss-coin ?coin)

Preconditions:

(holding ?agent ?coin)

Effects:

(:when (:unknown ?U H)
:effect (:and (flat ?coin)
(heads ?coin)))
(:when (:unknown ?U T)
:effect (:and (flat ?coin)
(tails ?coin)))
(:when (:unknown ?U E)
:effect (on-edge ?coin)))

uncertain effect
uncertain effect
uncertain effect

Figure 10: Representing the action of tossing a coin
in the plan. In particular, Cassandra must determine the contingency in which that open
condition resides, and conjoin the effect with the existing antecedent of the decision-rule for
that contingency.
Consider, for example, what happens when a coin is tossed. We might say that in theory
there are three possible outcomes of this action: the coin can land at with heads up; at
with tails up; or on its edge (Figure 10). Suppose Cassandra is given a goal to have the
coin be at. This can be established by using the at-heads effect of tossing it. Since this is
an uncertain effect, Cassandra introduces two new contingencies into the plan, one for the
outcome in which the coin lands tails up, and another for the outcome in which it lands on
its edge.
The introduction of these contingencies mandates the introduction of a decision-step
whose initial rule set looks like this:6
If (flat coin) then [U1: H] rule for heads up
If T
then [U1: T] rule for tails up
If T
then [U1: E] rule for edge
At the same time, a new open condition (know-if (flat coin)) is introduced as a precondition of the decision-step, and new goal conditions are introduced that must be achieved in
contingencies [U1: T] and [U1: E]. Cassandra next establishes the goal condition in contingency [U1: T] using the at-tails effect of the toss step. The decision-rules associated
with the tails up contingency are thus modified as follows:
If (flat coin) then [U1: H] rule for heads up
If (flat coin) then [U1: T] rule for tails up
If T
then [U1: E] rule for edge
Finally, the goal condition is established in contingency [U1:E] by introducing a new
step, tip, into the plan. A precondition of the tip step is that the coin be on its edge, which
is established by the on-edge effect of the toss action. Since this effect depends directly
6. Assuming that ?U, the variable representing the source of uncertainty, is instantiated to U1.

305

fiPryor & Collins
upon the uncertainty U1, the decision-rule for the edge contingency is modified to include
this condition:
If (flat coin)
then [U1: H] rule for heads up
If (flat coin)
then [U1: T] rule for tails up
If (on-edge coin) then [U1: E] rule for edge
Since the plan is complete, this is the final set of decision-rules (see Section A.5). Notice
that these rules do not discriminate the heads-up outcome from the tails-up outcome. In
fact, either outcome will do, so there is no reason to make this discrimination. Which plan
is executed in either of these conditions depends solely upon the order in which the agent
that is executing the plan chooses to evaluate the decision-rules.7
A somewhat more complex problem arises if we give Cassandra the goal of having the
coin be flat and heads-up. In this case both effects can be established using the toss
action. This will again lead to the introduction of two new contingencies into the plan, one
for when the coin lands tails up, and one for when it lands on edge. Although Cassandra
could establish (flat coin) in the tails-up case, it would fail to complete the plan,
because the coin would not be heads-up. However, the turn-over action can be used,
leaving the coin flat and heads-up given that it was flat and tails-up to begin with.
At this point the decision-rules are as follows:
If (and (flat coin) (heads-up coin)) then [U1: H] rule for heads up
If (and (flat coin) (tails-up coin)) then [U1: T] rule for tails up
If T
then [U1: E] rule for edge
Cassandra must then plan for the goal in the outcome in which the coin lands on its
edge. Both these effects can be established as a result of the tip action. However, the
result heads-up is an uncertain effect of the tip action, since the coin might just as easily
land tails up. Cassandra must therefore add another new contingency for when the coin
lands tails up after being tipped. In this instance, the goal can be established by using
the turn-over action, and the tails-up precondition of this action can be established by
the uncertain result of the tip action. The final decision-rule set for the first decision is as
follows:
If (and (flat coin) (heads-up coin)) then [U1: H] rule for heads up
If (and (flat coin) (tails-up coin)) then [U1: T] rule for tails up
If (on-edge coin)
then [U1: E] rule for edge
If the on-edge contingency is pursued, another decision, stemming from the uncertain
result of tip, must be added to the plan. If we name the second source of uncertainty U2,
the rules for this decision are:
If (heads-up coin) then [U2: H]
If (tails-up coin) then [U2: T]
The plan is depicted in Figure 11 and shown in Section A.6.
7. An obvious extension to Cassandra would be the construction of a post-processor that spots decision-rules
that do not discriminate between particular sets of outcomes, and prunes the plan to remove superuous
contingencies. Note that it cannot be determined until the plan is complete whether such a condition
pertains.

306

fiPlanning for Contingencies: A Decision-based Approach

flat
heads
flat
tails

heads

Decide

flat

s

Tip
coin

Decide

KEY

tails

condition

Link with uncertain effect

ad

edge

Link

End

heads

he

Toss
coin

Turn coin
over

Turn coin
over

condition

flat

Alternative control flow
Incomplete portion of plan

Figure 11: A plan with two decisions
Kick door

lock broken

Open
door

do

or

op

Link

Decide

condition

End

Start

lock

KEY

en

inta

ct

or

Pick
lock

n
pe

Alternative
control flow

o

do

door
unlocked

Open
door

Figure 12: Opening a door
4.2.4 Decision-rules and Unsafe Links

The fact that Cassandra allows decision-rules that do not fully differentiate between outcomes of an uncertainty raises a somewhat subtle issue. Consider the partial plan for
opening a locked door shown in Figure 12. The action of kicking a door has, let us say,
two possible outcomes, one in which the lock is broken and one in which the agent's foot is
broken. A plan for the contingency in which the lock is broken is simply to open the door.
A plan for the alternative contingency is to pick the lock and then open the door.
Since the second plan does not depend causally on any outcome of the uncertainty (the
agent's foot does not have to be broken in order for it to pick the lock and open the door),
the decision-rules based on the above discussion would be:
307

fiPryor & Collins
If
If

(lock-broken)
T

then
then

[O: L]
[O: F]

rule for lock broken
rule for foot broken

Notice that in this case the pick action depends on the lock being intact, while the
action may have the effect that the lock is no longer intact. In other words, the kick
action potentially clobbers the precondition of pick. However, the planner can arguably
ignore this clobbering, because the two actions belong to different contingencies. This is
valid, though, only if the structure of the decision-rules guarantees that the agent will not
choose to execute the contingency involving pick when the outcome of kick is that the
lock is broken. The decision-rules above clearly do not enforce this. The solution in such a
case is to augment the decision-rule for the contingency in which the lock is not broken to
test whether the lock is in fact intact. This results in the following decision-rules (the plan
is shown in Section A.7):
kick

If
If

(lock-broken)
(not (lock-broken))

then
then

[O: L]
[O: F]

rule for lock broken
rule for foot broken

Cassandra augments decision-rules in this way whenever a direct effect of an uncertainty
could clobber a link in a different contingency.

5. A Contingency Planning Algorithm
In this section we give the details of Cassandra's algorithm. Its properties are considered
in Section 6.

5.1 Plan Elements

A plan consists of steps, effects, links (some of which may be unsafe), open conditions,
variable bindings, a partial ordering, and contingency labels. A plan is complete when
there are no open conditions and no unsafe links.
5.1.1 Steps and Effects

A plan step Step represents an action. It may have enabling preconditions. It has at least
one effect Eff. It is the instantiation of an operator.
A plan step may be a decision-step Decide. A decision-step has enabling preconditions
of the form (know-if Cond) for a condition Cond. Decide also has a set of decision-rules.
An effect Eff represents some results of an action. It is attached to a step Step, representing that action. It may have secondary preconditions. It has at least one postcondition
Cond, a condition that becomes true as the result of executing Step when the secondary
preconditions hold.
5.1.2 Links and Open Conditions

A link represents a causal dependency in the plan, specifying how a condition Cond is established by an effect Eff, which has Cond as a postcondition. Eff has secondary preconditions
SecPre and is a result of step Step. The link supports the step SupStep or effect SupEff
through the condition Cond which is one of:
308

fiPlanning for Contingencies: A Decision-based Approach
An enabling precondition of SupStep;
 A secondary precondition of an effect SupEff that is a result of SupStep;
 The negation of a secondary precondition of an effect that is a result of SupStep, thus
preserving a link.
A link is unsafe in a contingency Conting in which it is required if there is an effect
ClobberEff with postcondition ClobberCond (the clobbering condition) resulting from step
ClobberStep such that:
 Either ClobberCond can unify with Cond;
Or Cond is of the form (know-if KnowCond) and ClobberCond can unify with KnowCond;
 Step ClobberStep can occur between steps Step and SupStep;
 Effect ClobberEff can occur in contingency Conting.
An open condition (an unachieved subgoal) is represented in Cassandra as an incomplete
link, i.e., a link missing the information about the effect that establishes it.


5.1.3 Bindings and Orderings

Plan bindings (codesignation constraints) specify the relationships between variables and
constants. The following relationships are possible:
 Two variables may codesignate;
 A variable may designate a constant;
 A variable may be constrained not to designate a constant;
 Two variables may constrained not to codesignate.
An ordering constrains the order of two steps with respect to each other, so that step
S1 must precede step S2 (S1 < S2).
5.1.4 Contingency Labels

Every step, effect and open condition in a partial plan has two sets of contingency labels
attached to it. In the interests of brevity, we also refer to the labels of a link; in this case,
we mean the labels of the step or effect that the link establishes.
Each contingency label has two parts: a symbol representing the source of uncertainty,
and a symbol representing a possible outcome of that source of uncertainty. Positive contingency labels denote the circumstances in which a plan element must or will necessarily
occur; negative contingency labels denote the circumstances in which a plan element cannot
or must not occur.
Contingency labels must be propagated through the plan. In general, positive contingency labels are propagated from goals to the effects that establish them, while negative
contingency labels are propagated from steps to the effects that result from them. The
details are as follows:
309

fiPryor & Collins
Plan (PartList)

1. Choose a partial plan Plan from PartList;
2. If Plan is complete, then finish;
3. If there is an unsafe link Unsafe:
Do resolve (Plan, Unsafe) and add the resulting plans to PartList;
Return to step 1;
4. If there is an open condition Open:
Do establish (Plan, Open) and add the resulting plans to PartList;
Return to step 1.

Figure 13: Top level planning algorithm


A step inherits the positive labels of the effects that result from it;



A step inherits the negative labels of the effects that establish its enabling preconditions;



An effect inherits the positive labels of the steps whose enabling preconditions it
establishes;



An effect inherits the positive labels of the effects whose secondary preconditions it
establishes;



An effect inherits the negative labels of the step from which it results;



An effect inherits the negative labels of the effects that establish its secondary preconditions;



An open condition inherits the positive labels of the step or effect that it is required
to establish.

Cassandra's system of label propagation is based on that of cnlp but is more complex.
Indeed, it is rather more complex than we would like. This complexity is mandated by the
need to deal with operators that involve multiple context-dependent effects, which has the
result that a step and its effects do not necessarily share the same labels.

5.2 Algorithm

The planning process starts by constructing a partial plan consisting of two steps:


An initial step with no preconditions and with the initial conditions as its effects;



A goal step with no effects and with the goal conditions as its enabling preconditions.

This plan is added to the (initially empty) list of partial plans PartList. Planning then
proceeds as shown in Figure 13.
It now remains to describe how threats to unsafe links are resolved and how open
conditions are established.
310

fiPlanning for Contingencies: A Decision-based Approach
Resolve (Plan, Unsafe)

1. Initialize a list NewPlans;
2. If the unification of the clobbering condition ClobberCond with the condition Cond established by the link
Unsafe involves adding codesignation constraints to the bindings of Plan:
Make each possible modification to the bindings of Plan that ensures that ClobberCond cannot
unify with Cond;
Add each resulting partial plan to NewPlans;
3. If the clobbering step ClobberStep can precede the step Step that establishes Unsafe:
Add an ordering to ensure that ClobberStep precedes Step;
Add the resulting partial plan to NewPlans;
4. If the step SupStep supported by Unsafe can precede ClobberStep:
Add an ordering to ensure that SupStep precedes ClobberStep;
Add the resulting partial plan to NewPlans;
5. Prevent the clobbering effect ClobberEff from occurring in each contingency Conting in which the link
Unsafe is unsafe:
Do one of:
(a) Add the negation of the secondary preconditions of ClobberEff as an open condition with
positive contingency label Conting;
(b) Add Conting to the negative contingency labels of ClobberStep;
(c) Add Conting to the negative contingency labels of the effect SupEff or step SupStep that
Unsafe supports;
If appropriate modify the relevant decision-rule as discussed in Section 4.2.4;
Add orderings to ensure that step ClobberStep occurs between steps Step and SupStep;
Propagate labels as appropriate;
Add each resulting partial plan to NewPlans;
6. Return NewPlans.

Figure 14: Resolving threats
5.2.1 Resolving Threats to Unsafe Links

Figure 14 shows how threats are resolved. The methods shown in steps 2, 3, and 4 are
standard methods found in snlp and ucpop; they are often termed separation, demotion,
and promotion respectively. We say that the methods in step 5 disable the threat. The
methods in steps 5a and 5b ensure that the threatening effect does not occur in a given
contingency. The method in step 5a is a modification of a standard method found in ucpop
and other planners that use secondary preconditions. Essentially, the idea is to prevent
an effect from occurring by ensuring that the context in which it occurs cannot hold. The
method in 5b prevents an effect from occurring in a contingency by forbidding the execution
of the step that produces it. The method in step 5c notes that the established step or effect
cannot occur in a given contingency. If any of these techniques result in inconsistent labeling
of any plan element (so that, for example, it cannot occur in every contingency in which
it is required) the resulting partial plan is abandoned, as it represents a dead end in the
search space.
311

fiPryor & Collins
5.2.2 Establishing Open Conditions

Figure 15 shows the procedure used. Procedure EstablishPre shows the methods of adding
a new step and reusing an existing step; they are essentially the methods used in ucpop
extended to reect the need to check and propagate contingency labels.
Procedure EstablishUnk shows methods of adding a new decision and reusing an existing
decision that are specific to Cassandra. The issues involved were discussed in Section 4.2.

6. Issues in Contingency Planning

Cassandra is a partial order planner directly descended from ucpop, which is sound, complete, and systematic|all plans produced by ucpop are guaranteed to achieve their goals,
if there is a plan then ucpop will find it, and ucpop never revisits a partial plan. In this
section we discuss these properties and related issues in the context of contingency planning.

6.1 Soundness

Ucpop's soundness depends on the perfect knowledge assumptions discussed in Section 1.
In particular, ucpop's plans are sound if the initial conditions are fully specified, and if

all possible effects of actions are specified in the operators that represent them. If no
uncertainties are involved in the plan, Cassandra is equivalent to ucpop and therefore
constructs sound plans.
If uncertainties are involved in the plan, it can no longer be assumed that the initial
conditions and effects of actions are fully specified. Indeed, the uncertainties arise because
these assumptions are violated. However, the assumptions can be adapted to account for
the presence of uncertainty: it would be possible, for example, to insist that all possible
initial conditions and action effects are specified. In Cassandra's representation, this means
that every source of uncertainty must be specified through the use of unknown secondary
preconditions, and every possible outcome of each source of uncertainty must be specified.
We conjecture that Cassandra is sound under these conditions. The proof would follow
because the procedure for adding in new goals whenever a new source of uncertainty is
encountered ensures that every goal is achieved in every possible outcome of the uncertainty.

6.2 Completeness

We conjecture that Cassandra is complete in the limited sense that, if there is a sound plan
of the form that it can construct, then Cassandra will find it. We believe that this is a simple
extension of ucpop's completeness. If there are no uncertainties involved, Cassandra will
always find a plan in the same way as ucpop. The introduction of a source of uncertainty
into a plan leads to the addition of new contingent goals. Cassandra will find a plan for
each of these new goals in the appropriate contingency. Thus, if the goal can indeed be
achieved in every contingency, Cassandra will find a plan that achieves it, as long as there
is a way of determining which contingency holds.
For example, the plan to disarm a bomb that we described in Section 4.1 relies on there
being a method of determining which package the bomb is in. In McDermott's presentation
of this example, the two packages are indistinguishable, and the point of the example is to
illustrate that there is nonetheless a plan that will succeed in disarming the bomb, namely,
312

fiPlanning for Contingencies: A Decision-based Approach
Establish (Plan, Open)

1. If the open condition is not of type :unknown do EstablishPre (Plan, Open) and return the resulting list
of plans;
2. If the open condition is of type :unknown with source of uncertainty Uncertainty and outcome Outcome
do EstablishUnk (Plan, Open, Uncertainty, Outcome) and return the resulting list of plans.

EstablishPre (Plan, Open)

1. Initialize list NewPlans;
2. For each effect Eff resulting from a step Step in Plan
If Eff can occur in every contingency in which Open must be established
and if Eff can precede the step SupStep that Open is required to support
and if there is a postcondition EffCond of Eff that can unify with condition Cond that Open is required
to establish:
Complete the link Open by using Eff as the establishing effect;
Add the resulting partial plan to NewPlans;
3. For each operator with an effect Eff with a postcondition EffCond that can unify with Cond:
Instantiate a new step Step;
Complete the link Open by using Eff as the establishing effect;
Add the enabling preconditions of Step as open conditions;
Add the resulting partial plan to NewPlans;
4. For each plan in NewPlans:
Add an ordering to ensure that Step precedes SupStep;
Add the bindings necessary to ensure that EffCond unifies with Cond;
Add the secondary preconditions SecPre of Eff as open conditions;
Propagate labels as appropriate;
5. Return NewPlans.

EstablishUnk (Plan, Open, Uncertainty,

Outcome)
1. Initialize list NewCPlans;
2. If Uncertainty is a new source of uncertainty in the plan:
Add a new decision-step DecStep for uncertainty Uncertainty;
Add new top-level goals as open conditions with the appropriate labels;
Add the resulting partial plan to NewCPlans;
3. If Uncertainty is an existing source of uncertainty in the plan:
Find an existing decision-step DecStep for uncertainty Uncertainty;
Add the resulting partial plan to NewCPlans;
4. For each plan in NewCPlans:
Modify the decision-rule in DecStep for Outcome to include Cond as an antecedent;
Add (know-if Cond) as an open condition required to establish DecStep;
Add orderings to ensure that DecStep precedes SupStep;
Propagate labels as appropriate;
5. Return NewCPlans.

Figure 15: Establishing open conditions
313

fiPryor & Collins
dunking both packages (McDermott, 1987). The algorithm described in the previous section
cannot find a plan in this situation because it is impossible to achieve the preconditions of
the decision-step that determines which package to dunk. In Section 6.5.5 we discuss this
example in more detail and describe a simple extension to Cassandra that allows the correct
plan (to dunk both packages) to be found.
Ucpop's completeness, like its soundness, depends on the perfect knowledge assumptions we discussed in Section 1. Cassandra's completeness depends on three extensions to
these assumptions:


All sources of uncertainty are specified;



The specified outcomes are exhaustive;



There are actions available that allow the determination of the outcome of any uncertainty, even if only indirectly.

Unfortunately, these conditions are necessary but not sucient. Cassandra can only
find plans if the actions that it uses to determine the contingency do not interfere with
the achievement of the goal. For instance, there might be a dropping action available that
would detonate any bomb inside the package that was dropped. This is certainly an action
that allows the determination of the outcome of the uncertainty, but there is no sound plan
that makes use of it.
In order to have a useful notion of Cassandra's completeness, we must therefore specify the form of the plans that it can construct. This problem is common to proving the
completeness of any planner: for example, we do not claim that snlp, say, is incomplete
because it cannot find a plan for the bomb-in-the-toilet problem. We say instead that there
is no valid plan of the form that it can construct. It is fairly simple to specify the form
of the plans that snlp can construct: they consist of partially ordered sequences of steps,
all of which are to be executed. The introduction of contingencies makes the description
of Cassandra's plans rather more complex; we have yet to formalize a description, but are
actively working in that direction. Informally, Cassandra can only construct plans that for
every source of uncertainty include a step to decide on one of the relevant plan branches.
The extension of Cassandra that solves the bomb-in-the-toilet problem can do so because
it can construct plans that do not meet this criterion.

6.3 Systematicity

Ucpop is systematic: it will never visit the same partial plan twice while searching. Cas-

sandra, as described in this paper, is not systematic; it may visit some partial plans in the
search space more than once. Consider again the plan to disarm a bomb that we discussed
in Section 4.1. In this plan, there are two different ways of establishing the goal to disarm
the bomb: by dunking package1 , and by dunking package2 . Cassandra can initially choose
either way of establishing the goal, leading in each case to the introduction of a contingency
and the necessity of replanning to achieve the goal in the other contingency. Both search
paths arrive at the same final plan, so the search is not systematic.
Cassandra could be made systematic by insisting on handling the contingencies only
in a certain order, the search path that uses the other order being treated as a dead end.
314

fiPlanning for Contingencies: A Decision-based Approach
However, this extension has not been added as there is currently some debate as to the
desirability of systematicity. For example, Langley (1992) argues that a non-systematic
search method, iterative sampling, is often better than a systematic method, depth-first
search, for problems which have multiple solutions and deep solution paths. Peot and
Smith (1992) observe that the performance of a non-systematic version of snlp was better
than that of the original systematic version. They ascribed this behavior to the fact that
exploring duplicate plans consumed less overhead than did ensuring systematicity.

6.4 Knowledge Goals
An agent executing contingency plans must be able to acquire information about the actual
state of the world so that it can determine which of the possible courses of action to pursue.
A system that constructs contingency plans must be able to plan for this information
acquisition: in general, the acquisition process may be arbitrarily complex (Pryor & Collins,
1991).
An early and inuential discussion of goals to possess knowledge about the world was
that by McCarthy and Hayes (1969). Since then, various theories have been developed to
account for them (e.g., Moore, 1985; Haas, 1986; Morgenstern, 1987; Steel, 1995). The
common thread in all this work is that knowledge goals arise from the need to specify the
actions that are to be performed; in other words, from the need to make actions operational . Work in this area has on the whole concentrated on being able to describe and
represent knowledge goals, and has largely ignored the issues involved in building planners
that construct plans containing them.
The structure of Cassandra is based on the notion that knowledge goals arise out of
the need to make decisions as to the actions to be performed (Pryor, 1995). In our view,
planning is the process of deciding what to do in advance of when it is done (Collins,
1987). In a world conforming to the perfect knowledge assumptions of classical planning
this is always possible because the world is totally predictable, and plans therefore need
contain no knowledge goals. However, when those assumptions are relaxed it may not be
possible to make all decisions in advance if the information necessary to make them is not
available to the planner. The information may be unavailable either because of the planner's
limited knowledge of the world or because the events that will nondeterministically cause
the conditions that affect the decisions have not yet occurred. In both cases it may be
possible for the planner to determine that a decision must be made even though it cannot
at that time actually make it. In this case the planner can defer the decision: plan to make
it in the future, when the necessary information will be available. Part of the plan is then
to acquire the information; the plan thus contains knowledge goals.
Cassandra's use of \unknown" preconditions to indicate nondeterminism is thus a crucial
part of its mechanism. In Cassandra, knowledge goals arise as the result of deferring decisions. These deferred decisions are represented explicitly in its plans, and themselves arise
directly from the incompleteness of Cassandra's knowledge of the world, whether through
the effects of nondeterministic actions or through incompletely specified initial conditions.
Both these forms of uncertainty are handled in the same way: once Cassandra has recognized the need to defer a decision, the reason for its deferral is not important except
inasmuch as it results from incomplete knowledge of the world.
315

fiPryor & Collins
The view of knowledge goals as arising from deferred decisions is basically consistent
with the view that they are needed in order to make actions operational, but differs from the
traditional view in that knowledge goals are not directly preconditions of physical actions,
but are instead preconditions of actions that make decisions. For example, McCarthy and
Hayes consider the problem of a combination safe: it is commonly held that the action of
opening the safe has a precondition to know the combination. In Cassandra, however, the
goal of knowing the combination would arise as a subgoal of deciding which plan branch to
follow, where there would be a branch for each possible combination.8 The branches would
arise because of Cassandra's incomplete knowledge of the world: the initial conditions in
which the plan will be executed are not fully specified.
Cassandra uses a variant of the syntactic approach proposed by Haas (1986) to represent
knowledge goals, limiting knowledge goals to the form know-if(fact). This turns out to be
adequate if, as we assume, all possible outcomes of any given uncertainty are known. In
general, the representation used by Cassandra, based on the strips representation of add
and delete lists, is less powerful than the logics proposed by either Morgenstern or Haas.

6.5 Miscellaneous Issues in Contingency Planning
Cassandra's approach raises a number of questions concerning the desired behavior of a
contingency planner, many of which do not have obvious answers. In this section we briey
consider a few of the issues raised.
6.5.1 Dependence on Outcomes and Superfluous Contingencies

The fact that a contingency plan assumes a particular outcome of an uncertainty means only
that it cannot depend upon a different outcome of that uncertainty. Cassandra does not
enforce any constraint that the plan must causally depend upon the outcome that it assumes.
For instance, in the example described in Section 2.3.2, the plan to take Ashland does not
actually depend on Western being blocked; it could be executed successfully regardless of
the level of trac on Western.
This observation raises an interesting question: If a plan for a contingency turns out
not to depend on any outcome of the uncertainty that gave rise to it, would this not
obviate the need for plans for alternative contingencies? For instance, in our example, it
might seem sensible to execute the plan to use Ashland regardless of whether Western is
blocked. It might thus seem that the planner should edit the plan in some way so as to
eliminate apparently superuous contingencies. However, it can easily be shown that a
version of the plan that does not involve dependence on any outcome of the uncertainty
will be generated elsewhere in the search space. In the example, this would mean that the
planner would in fact consider a plan that simply involved taking Ashland. If the search
heuristics penalize plans involving contingencies appropriately this other plan should be
preferred to the contingency plan, all other things being equal.
8. This raises the obvious question as to whether planning in advance for every possibility is a sensible
thing to do. See Section 7.4 for a discussion of this issue.

316

fiPlanning for Contingencies: A Decision-based Approach
6.5.2 One-sided Contingencies

The preceding discussion notwithstanding, a plan involving no contingencies is not always
superior to a plan involving a contingency. This is why a planner might in fact construct a
plan like the Western/Ashland one. To take a more clear-cut example, suppose Pat needs
$50 to bet on a horse. She might try to borrow the $50 from Chris, but the outcome of this
action is uncertain|Chris might refuse. Alternatively, she could rob a convenience store.
While the robbery plan would (we shall stipulate) involve no uncertainties, it is a bad plan
for other reasons. It would be better to first try to borrow $50 from Chris, and then, if that
fails, rob the convenience store. Cassandra could generate this plan. In order to make it
prefer the plan to the contingency-free alternative, however, its search metric would have to
take into account the estimated costs of various actions, and to perform something akin to
an expected value computation. (See, for example, Feldman & Sproull, 1977; Haddawy &
Hanks, 1992, for discussions of decision-theoretic measures applied to planning.) In order to
execute the plan properly, it would also be necessary for it to have some way of knowing that
the borrowing plan should be preferred to the robbery plan if it were possible to execute
either of them.
6.5.3 Identical Branches

It is possible that a single plan could work just as well for several different outcomes of
an uncertainty. For instance, suppose the action of asking Chris for $50 has three possible
outcomes: either Pat gets the money and Chris is happy (at having had the opportunity
to do a favor); or Pat gets the money and Chris is unhappy (at having been obliged to do
a favor); or Pat does not get the money at all. If Pat constructs a plan in which she tries
to borrow $50 from Chris to bet on a horse, then, assuming that this plan does not depend
upon Chris's happiness (which it might, for example, if Pat needed to get a ride to the track
from Chris), the plan will work for either the \get money + Chris happy" outcome or the
\get money + Chris unhappy" outcome.
Cassandra could find such a plan, but would in effect have to find it twice|once for
each outcome of the uncertainty|and it would still require a decision-step to discriminate
between those outcomes. This is inecient in two ways: the extra search time required
to find what is essentially the same plan twice is wasted, and effort is put into making an
unnecessary decision. We are looking into ways to avoid the former problem. The latter
could be solved by a post-processor that would \merge" identical contingency plans, but
we have not implemented this technique.
6.5.4 Branch Merging

It is possible to construct a plan in which branches split and then reunite. For instance,
consider the Western/Ashland plan once again. The context in which the goal to get to
Evanston arises might be an obligation to deliver a toast at a dinner to be held in an
Evanston restaurant. The contingency due to uncertainty about trac on Western Avenue
would in this case seem to affect only the portion of the plan concerned with getting to
Evanston; it probably has little bearing on the wording of the toast, the choice of wine, and
so on. The most natural way to frame this plan might thus be to assume that regardless of
317

fiPryor & Collins
which contingency is carried out, the planner will eventually arrive at a certain location in
Evanston, and from that point a single plan will be developed to achieve the final goal.
Constructing the plan in this way would result in a more compact plan description, and
might thus reduce the effort needed to construct the plan by avoiding, for example, the
construction of multiple copies of the same subplan. We are considering methods by which
branch re-merging might be achieved, but all the methods we have considered so far seem
to complicate the planning process considerably.
6.5.5 Fail-safe Planning

As we discussed in Section 6.2, Cassandra's operation relies on being able to determine,
even if only indirectly, the outcome of any uncertainty. However, this may not always be
possible, and it is not a necessary precondition for the existence of a viable plan. In the
bomb-in-the-toilet problem, for example, there is a valid plan that Cassandra cannot find:
to dunk both packages.
This suggests a method for constructing plans in the face of uncertainty when the
outcome of the uncertainty cannot be determined|what one might call fail-safe plans.
Whenever uncertainty arises it is in principle possible that there might be a non-contingent
plan that would achieve the goal whatever the outcome of the uncertainty. To find such
a plan, the planner must construct a version of the contingency plan in which all actions
in the contingency branches arising from the uncertainty will be executed unconditionally.
Cassandra has been extended in just such a way, by adding a new type of decision, one
to execute all branches in parallel (Collins & Pryor, 1995). A plan containing such a
decision is only sound if none of the actions that must be performed to achieve the goal
in one contingency interfere with any of the actions that must be performed in any other
contingency, and the ability to perform the actions is independent of the outcome of the
uncertainty. These conditions clearly hold for the bomb-in-the-toilet problem.
Cassandra can reason about this possibility because its labeling scheme distinguishes
those actions that must not be performed in a given contingency from those that need not
be performed. It is possible to execute all branches only if the actions in each branch may
be performed (but need not) in all the other branches.
When a parallel decision is added to the plan in the extended version of Cassandra, new
goals are added in the usual way but the labeling is handled differently. The branches are
not separated, so that Cassandra can no longer reason that the causal links in one branch
will not be affected by actions in another branch.
6.5.6 Contingent Failure

Cassandra can produce a plan only if it is possible to achieve the goal of the plan in all
possible contingencies. Often, however, the goal cannot in fact be achieved in some outcome
of the underlying uncertainty. Consider, for instance, Peot and Smith's example of trying to
get to a ski resort by car, when the only road leading to the resort is either clear or blocked
by snowdrifts (Peot & Smith, 1992). If the road is clear, then the goal can be achieved, but
if it is blocked, all plans are doomed to failure.
No planner can be expected to recognize the impossibility of achieving a goal in the
general case (Chapman, 1987). However, a possible approach is suggested by Peot and
318

fiPlanning for Contingencies: A Decision-based Approach
Smith. We could introduce an alternative method of resolving open goal conditions: simply
assume that the goal in question fails.
This is an undesirable method of resolving open goal conditions if the subgoal is in fact
achievable, so in theory plans involving contingent failure should be considered only after
the planner has failed to find a plan in which all goals are achieved. This is sometimes
possible, but in general the problem of determining whether there is a successful plan is
undecidable. There may always be partial plans that do not involve goal failure but that
cannot be completed. For example, as a partial plan is modified it may become more
and more complex, the resolution of each open condition involving the introduction of more
unachieved subgoals. In this case, plans involving contingent failure will never be considered
unless they are ranked above some plans that do not involve contingent failure. In order to
be generally useful, the approach must be weakened: instead of considering goal failure only
after all other avenues of attack have failed, apply a high fixed penalty to plans involving
failed goals. The aim would be to fix the penalty high enough that contingent failure would
only apply in genuine cases of goals being unachievable. However, this would of necessity
be a heuristic approach and completeness would be lost.

7. Related Work
Cassandra is constructed using ucpop (Penberthy & Weld, 1992) as a platform. Ucpop
is a partial order planner that handles actions with context-dependent effects and universally quantified preconditions and effects. Ucpop is an extension of snlp (Barrett et al.,
1991; McAllester & Rosenblitt, 1991) that uses a subset of Pednault's adl representation
(Pednault, 1989).
An early contingency planner was Warren's warplan-c (1976). Contingency planning
was more or less abandoned between the mid seventies and the early nineties,9 until sensp
(Etzioni et al., 1992) and cnlp (Peot & Smith, 1992). Both sensp and cnlp are members
of the snlp family: sensp is, like Cassandra, based on ucpop, and cnlp is based directly
on snlp. C-buridan (Draper et al., 1994a; Draper, Hanks, & Weld, 1994b), a probabilistic
contingency planner, is based on the probabilistic planner buridan (Kushmerick, Hanks,
& Weld, 1995) (which is itself based on snlp) and on cnlp. Plinth (Goldman & Boddy,
1994a, 1994b) is a total-order planner based on McDermott's Pedestal (1991), and is
strongly inuenced by cnlp in its treatment of contingency plans.
Warplan-c, unlike the other planners considered here, did not use a strips-based
action representation, but was based on predicate calculus. It could handle actions that
had just two possible outcomes, and did not merge the resulting plan branches.
Sensp also differs from the other planners considered here. It represents uncertainty
through the use of run-time variables, distinguished from ordinary variables by being treated
as constants whose values are not yet known. In sensp plan branches arise from the
introduction of information-gathering steps that bind the run-time variables. Sensp handles
plan branching by constructing separate plans that each achieve the goal in a particular
contingency. It then combines the separate plans at a later stage, keeping the branches
totally separate. Sensp thus considers contingency branches separately, rather than in
9. Neither noah (Sacerdoti, 1977) nor Interplan (Tate, 1975) explicitly addressed issues of uncertainty,
although both tackled problems involving it (Collins & Pryor, 1995).

319

fiPryor & Collins
parallel. Actions that achieve knowledge goals may not have preconditions in sensp: this
restriction is required in order to maintain completeness.
Not surprisingly, Cassandra, cnlp, and c-buridan, and to a lesser extent Plinth, are
in many respects very similar. All except Plinth use the basic snlp algorithm, and all use
extended strips representations. Cassandra differs from cnlp and Plinth principally in
the way that uncertainty is represented (Section 7.1); this difference has important implications for the handling of knowledge goals (Section 7.2). The principal difference between
Cassandra and c-buridan lies in the latter's use of probabilities (Section 7.3).
Contingency planning is only one approach to the problem of planning under uncertainty. The aim of contingency planning is to construct a single plan that will succeed
in all circumstances: it is essentially an extension of classical planning. There are other
approaches to planning under uncertainty that do not share this aim: probabilistic planners
aim to construct plans that have a high probability of succeeding (Section 7.3); systems that
interleave planning and execution do not attempt to plan fully in advance (Section 7.4).
In both these approaches it is possible to address the problem of determining which contingencies should be planned for, which is not currently possible in Cassandra. A third
approach is that of reactive planning, in which behavior is controlled by a set of reaction
rules (Section 7.5).

7.1 The Representation of Uncertainty
In cnlp and Plinth, uncertainty is represented through a combination of uncertain outcomes of nondeterministic actions and the effects of observing those outcomes. A threevalued logic is used: a postcondition of an action may be true , false , or unknown . For
example, the action of tossing a coin might have the postcondition unk(side-up ?x). Special conditional actions , each of which has an unknown precondition and several mutually
exclusive sets of postconditions, are then used to observe the results of the nondeterministic actions. In the example, the operator to observe the results of tossing a coin might
have the precondition unk(side-up ?x) with three possible outcomes: (side-up heads),
(side-up tails), and (side-up edge).
Cnlp thus spreads the representation of uncertainty over both the action whose execution produces the uncertainty and the action that observes the result. A consequence of
this is that cnlp cannot use the same observation action to observe the results of different
actions. For example, it would require different actions to observe the results of tossing a
coin (which has three possible outcomes) and tipping a coin that had landed on its edge
(which has two possible outcomes).
In Plinth, the notion of a conditional action is extended to cover any action (not only
observation actions) that has nondeterministic effects on the planner's world model . For
example, in an image-processing domain an operator to remove noise from an image may
or may not succeed. However, its outcome is evident as soon as it has been applied, and no
special observation action is required.
In cnlp and Plinth, information-gathering actions are included in a plan whenever an
action with uncertain effects occurs. This is necessary because the uncertainty is actually
represented in the information-gathering action rather than in the action that actually
320

fiPlanning for Contingencies: A Decision-based Approach
produces the uncertainty. Knowledge goals are thus not represented explicitly in these two
systems.
The representation used in cnlp and Plinth arises out of the desire to use a \single
model of the world, representing the planner's state of knowledge, rather than a more
complex formalization including both epistemic and ground formulas" (Goldman & Boddy,
1994b). An operator therefore represents only the effects that the execution of the underlying action has on the planner's knowledge of the world, and not the effects that it has on
the actual state of the world. It is, of course, important to represent how actions affect the
planner's world model, but we believe that it is also important to represent how they affect
the world. After all, the purpose of reasoning about actions is to achieve goals in the world,
not just in the planner's world model. In particular, after the execution of a nondeterministic action its actual effects, although they may indeed be unknown to the planner, have
occurred and cannot now be altered. Cassandra's representation reects this: indeed, Cassandra can reason about the possible effects without scheduling observation actions. This
means that an extension of Cassandra can, for example, solve the original bomb-in-thetoilet problem, in which there are no possible actions that will resolve the uncertainty as
to which package contains the bomb: the bomb's state is not represented in the planner's
world model at any stage between the beginning, when it is known to be armed, and the
end, when both packages have been dunked and it is known to be safe.
A further implication of this method of representing uncertainty is the diculty of
representing actions whose uncertain effects cannot be determined through the execution
of a single action. Consider, for example, a malfunctioning soda machine that has one
indicator that lights when it cannot make change, and another that lights when it has run
out of the product requested. Suppoe that, when it is functioning correctly, these two
indicators will not light simultaneously. If it malfunctions, it must be kicked to make it
work. Observing either light on its own is not enough to determine which uncertain effect
(working properly or malfunctioning) has occurred.

7.2 Knowledge Goals
The method of representing uncertainty in cnlp and Plinth has important implications
for how knowledge goals are handled in their plans.
The acquisition of information is a planning task like any other (Pryor & Collins, 1991,
1992; Pryor, 1994). In general, the sequence of actions required to achieve a given knowledge
goal may be arbitrarily complex. For example, an action to observe a tossed coin might
require that the observer is in the appropriate location; in other cases, there might be
several different possible methods of information gathering, some involving perception, some
involving reasoning, and some a combination. A contingency planner, some of whose plans
will necessarily involve the achievement of knowledge goals, must therefore be able to plan
fully generally for information gathering.
The confusion between the source of uncertainty and the observation of uncertain results
limits the ways in which knowledge goals can be achieved in cnlp and Plinth: they must
be achieved through the special observation actions that specify the uncertain outcomes.
This is a result of their representation in terms of the planner's world model, which means
that they do not represent the effects of actions (except to ag them as unknown) until
321

fiPryor & Collins
the planner has observed them (or otherwise incorporated them into its world model). In
their discussion of this issue Goldman and Boddy (1994b) explicitly exclude knowledge
goals from consideration. As they point out, planning under uncertainty requires that a
distinction be made between the actual state of the world and the planner's knowledge
of it. In order to plan effectively for knowledge goals, both must be represented. This is
done in Cassandra by separating the representation of uncertainty from the representation
of information-gathering. If an effect results deterministically from an action, Cassandra
reasons that there is no need to observe it, and it forms part of the world model. An
uncertain effect, on the other hand, is not incorporated unconditionally into Cassandra's
world model; it is noted as being possibly true, and (if necessary) Cassandra sets up a
subgoal to determine whether it is indeed true.
Sensp, which uses the uwl representation for goals and actions, has three different
kinds of precondition that can be used to represent information goals either alone or in
combination (Etzioni et al., 1992). As well as satisfy preconditions, which may be achieved
through actions or through observation, uwl has hands-off preconditions indicating that
the value of propositions must not be changed in order to achieve the subgoal, and find-out
preconditions. The latter are in some ways similar to preconditions for know-if propositions
in Cassandra. A precondition such as (find-out (P . v)) tells the planner to ascertain
whether or not P has truth value v. Under certain circumstances this type of precondition
may be achieved by an action that changes the value of P. Knowledge goals may thus be
represented by find-out preconditions or satisfy preconditions (often used in conjunction
with hands-off preconditions). Etzioni et al. argue that knowledge goals should only be
achieved through actions that change the value of the proposition in question when that
change is required for another purpose in the plan. We believe that this is an unnecessary
limitation, and that in some circumstances enforcement actions may be the best way of
achieving knowledge goals.

7.3 Probabilistic and Decision-theoretic Planning
When constructing plans, Cassandra recognizes the presence of uncertainty but not its
extent. Other planners specifically address issues of probability: for example, buridan
constructs plans whose probability of achieving the goal is above a given threshold (Kushmerick et al., 1995); and Drips uses the utility of the different possible outcome of various
plans to choose the one with the highest expected utility (Haddawy & Suwandi, 1994).
Neither buridan nor drips constructs contingency plans, i.e., plans that involve alternative courses of action to be performed in different circumstances. C-buridan, which is based
on buridan, constructs contingency plans that are likely to succeed (Draper et al., 1994b,
1994a). It represents an extension of cnlp in the direction of decision-theoretic planning.
Probabilistic planners use information about the probabilities of the possible uncertain
outcomes to construct plans that are likely to succeed. Cassandra, on the other hand, cannot
use such information and constructs plans that are guaranteed to succeed. Probabilistic
planning, because it relies on explicit probabilities, is both more and less powerful than
the deterministic contingency planning performed by Cassandra. Cassandra cannot use
information about probabilities but it can construct plans in circumstances in which no
such information is available. For example, in order to solve the bomb-in-the-toilet problem,
322

fiPlanning for Contingencies: A Decision-based Approach
c-buridan would have to have some information, or at least make an assumption, about

the probabilities of the bomb being in each package. Whatever assumptions are made might
turn out to be wrong, thus invalidating the basis of the plan.
We believe that it would be possible to build a probabilistic planner using ideas from both
c-buridan and Cassandra. Because of the explicit representation of decisions in Cassandra,
such a planner would provide an excellent opportunity for investigating the use of different
decision procedures. C-buridan relies on having full knowledge of all the probabilities at
the time that it constructs its plans. This knowledge, like any other, may not be available
until the plan is executed. It would be relatively simple to add decision procedures to
Cassandra's decision representation that depend on information about probabilities, e.g.,
to follow a particular course of action if the probability of a given outcome exceeds a
certain value. The introduction of such decision procedures might, of course, result in the
introduction of knowledge goals to determine probabilities, possibly leading eventually to a
system that would construct plans to perform empirical studies to determine probabilities.
A problem associated with contingency planning is that of branch merging, i.e., the
determination of whether two steps in separate branches can be treated as the same step.
C-buridan performs full merging: this is an effect of the probabilistic algorithm on which
it is based. Adding this capability to Cassandra is an area of future work. A major
problem encountered when considering branch merging is how to identify the variables in the
different branches with each other: c-buridan's representations do not include variables,
so the problem does not arise. This may cause diculties in the adaptation of c-buridan's
merging mechanism for Cassandra's use.
An advantage of combining probabilistic planning and contingency planning is the resulting ability to judge whether it is worth planning for a given contingency. One of the
limitations of Cassandra in its present form is the requirement that every possible contingency be planned for. In complex situations this makes the resulting plans cumbersome.
Moreover, Cassandra's performance deteriorates with the number of distinct branches in
the plan. The cost of determining that the presence of a particular branch would not significantly change the probability of the plan's success might well be much less than the cost
of constructing that branch. This is an interesting issue to be considered in the future.

7.4 Interleaving Planning and Execution

Although Cassandra's plans may include sensing actions, with the course of action that
will actually be executed depending on the results of those actions, Cassandra does not
interleave planning with execution. Plans are fully specified before they are executed. In
some circumstances this is clearly very inecient. Consider, for example, how Cassandra
constructs a plan to open a combination safe (see Section 6.4). It requires prior knowledge
of all possible combinations, and then constructs a plan with a branch for each combination.
An obvious alternative would be to construct a plan that was fully specified up to
the information-gathering step, execute the plan to that stage and, once the information
has been gathered, construct the rest of the plan.10 This could be done in Cassandra by
introducing another type of decision procedure, that of planning to achieve the goal, and
assuming that it would always be possible to find a plan to achieve the goal. This is a strong
10. See Section 8.2 for further discussion of this issue and an alternative approach.

323

fiPryor & Collins
assumption, but would certainly be valid in cases such as the problem of opening a safe.
This is an area of future work. Interleaving planning and execution in this way would have
the advantage that it would not be necessary to plan for contingencies that do not actually
arise. It would however lose some of the advantages of planning in advance. For example,
possible interference between actions performed before and after the information gathering
might be missed, leading the planner to find suboptimal plans. Indeed, as sensing actions
may in general change the world, executing them before full construction of a viable plan
might have the unfortunate result of making the achievement of the goal impossible.
Planners that interleave planning and execution include ipem (Ambros-Ingerson & Steel,
1988), xii (Golden, Etzioni, & Weld, 1994) and Sage (Knoblock, 1995). All three use the
same basic interleaving technique: only when no further planning is possible are steps
executed. They thus do not set out to decide in advance exactly when further planning will
be necessary, and their plans do not include explicit provision for further planning. The
effects of different interleaving strategies were investigated in the design of bump (Olawsky
& Gini, 1990). In the Continue Elsewhere strategy as much preplanning as possible was
performed; in the Stop and Execute strategy, goals defined in terms of sensor readings
were executed as soon as they were encountered. It was found that neither strategy had a
clear advantage over the other, in that both strategies sometimes produced plans that were
suboptimal or that might fail.

7.5 Reactive Planning

A different approach to the problem of planning under uncertainty is taken in the reactive
planning paradigm. In this approach, no specific sequence of actions is planned in advance.
Just as for contingency planning, the planner is given a set of initial conditions and a goal.
However, instead of producing a plan with branches, it produces a set of condition-action
rules: for example, universal plans (Schoppers, 1987) or Situated Control Rules (SCRs)
(Drummond, 1989).
In theory, a reactive planning system can handle exogenous events as well as uncertain
effects and unknown initial conditions: it is possible to provide a reaction rule for every
possible situation that may be encountered, whether or not the circumstances that would
lead to it can be envisaged. In contrast, a contingency planner such as Cassandra cannot
handle exogenous events as it cannot predict them. Cassandra and other contingency planners focus their planning effort on circumstances that are predicted to be possible (or likely,
in the case of a probabilistic contingency planner such as c-buridan).
It would be possible to represent Cassandra's contingency plans as sets of conditionaction rules, by using the causal links and preconditions to specify the conditions in which
each action should be performed. However, more reasoning is required at execution time
to use reaction rules than is required to execute a contingency plan. Instead of simply
executing the next step in the plan, reasoning only at branch points, the use of reaction
rules requires the evaluation of conditions on every cycle in order to select the relevant rule.

8. Discussion

We have described Cassandra, a partial-order contingency planner that can represent uncertain outcomes and construct contingency plans for those outcomes. The design of Cassandra
324

fiPlanning for Contingencies: A Decision-based Approach
is based on a coherent view of the issues arising in planning under uncertainty. It recognizes
that, in an uncertain world, a distinction must be drawn between the actual state of the
world and the planner's model of it; it instantiates an intuitively natural account of why
knowledge goals exist and how they arise; and it bases its treatment of plan branching on
the requirements of the agent that will execute the plan. As a result, Cassandra explicitly
plans to gather information and allows information-gathering actions to be fully general.
The coherence of its design provides a solid base for more advanced capabilities such as the
use of varying decision-making procedures.

8.1 Contributions
The principal contribution of this work lies in the explicit representation of decision steps
and the implications this has for the handling of knowledge goals. Cassandra is, we believe,
the first planner in which decisions are represented as explicit actions in the plans that it
constructs. Cassandra's knowledge goals arise specifically from the need to decide between
alternative courses of action, as preconditions of the decision actions. Cassandra is thus
consistent with the view that planning is the process of making decisions in advance. In
this view, contingency plans are plans that defer some decisions until the information on
which they are based will be available (Pryor, 1995). Different plan branches correspond to
different decision outcomes.
Through its use of explicit decision steps, Cassandra distinguishes between sensing or
information-gathering actions on the one hand, and decision making on the other. One
important reason for making this distinction is that a decision may depend on more than
one piece of information, each available through performing different actions. In addition,
separating information-gathering from decision-making provides a basis for introducing alternative methods for making decisions. For example, the extension to Cassandra described
in Section 6.5.5 introduces a type of decision that directs the executing agent to perform
all branches resulting from a given source of uncertainty, which allows the construction of
plans that can succeed in situations in which there is no way of telling what the actual outcome is (e.g., the bomb-in-the-toilet problem). We believe that the explicit representation
of different methods for making decisions is an important direction for future research.
Because knowledge goals arise as preconditions of decisions in Cassandra, the need to
know whether a particular plan branch will work is distinguished from the need to know the
actual outcome of an uncertainty. Cassandra does not plan to determine outcomes unless
they are relevant to the achievement or otherwise of its goals. Moreover, Cassandra does not
treat knowledge goals as special cases: plans to achieve them may be as complex as plans
to achieve any other goals. As well as planning to achieve knowledge goals that arise as
preconditions of decisions, Cassandra can also produce plans for top-level knowledge goals.
Two other features of Cassandra are worth noting: the exibility afforded by its labeling
scheme; and the potential for learning and adaptation afforded by its representation of
uncertainty.
Cassandra's labeling scheme, although complex, allows the agent executing the plan
to distinguish between three classes of action: those that must be executed in a given
contingency; those that must not; and those whose execution will not affect the achievement
325

fiPryor & Collins
of the goal in that contingency.11 This feature paves the way for the extension described
above that allows Cassandra to build plans requiring the execution of all branches resulting
from a source of uncertainty.
Cassandra's representation makes no assumptions as to the intrinsic nature of uncertainty. An unknown precondition simply denotes that the information as to what context
will produce a particular effect from an action is not available to the planner. It may be
that this information is in principle unknowable (in domains involving quantum effects, for
example); it is much more likely that the uncertainty results from the limitations of the
planner or of the information available to it. In general, an agent operating in a real-world
domain will be much more effective if it can learn to improve its performance and adapt to
changing conditions. The use of unknown preconditions to represent uncertainty means that
in some circumstances it would be relatively simple to incorporate the results of such learning and adaptation into the planner's domain knowledge. For example, the planner might
discover how to predict certain outcomes: it could then change the unknown preconditions
into ones reecting the new knowledge. If, on the other hand, it discovered that predicted
effects were consistently failing to occur, it could change the relevant preconditions into
unknown ones.

8.2 Limitations

Cassandra is one of an increasing number of planners that aim to extend the techniques of
classical planning to more realistic domains. Cassandra is designed to operate in domains
in which two of the three principal constraints observed by classical planners are relaxed:
namely, we allow non-deterministic actions and incomplete knowledge of the initial conditions. Cassandra is, however, subject to the third constraint, that changes do not take place
except as a result of actions specified in the plan. This clearly limits its effectiveness in
many real-world domains. Moreover, there are limits on the extent of the nondeterminism
and incompleteness of knowledge that are handled. Cassandra's plans will not necessarily
achieve their goals if sources of uncertainty are ignored, or if all possible outcomes are not
specified.
Cassandra cannot make use of information about how likely particular outcomes are,
unlike probabilistic or decision-theoretic planners; it cannot plan to interleave planning and
execution; and it does not provide reaction rules for all possible circumstances. It can only
solve problems for which there are valid plans involving ways of discriminating between
possible outcomes; the algorithm given here cannot solve the original version of the bombin-the-toilet problem, although the extension described in Section 6.5.5 can do so (Collins
& Pryor, 1995).
The algorithm described in this paper has two major practical limitations: first, the
plans it produces are often more complex than necessary; and second, the time taken to
produce plans precludes its use on all except simple problems.
The complexity of Cassandra's plans results from the necessity of planning for every
contingency and from the lack of branch merging. For example, suppose you had to open a
combination safe so that you could obtain the money to pay for an evening out. Cassandra's
11. Not all agents can make use of this information, as there is no guarantee that the third type of step will
actually be executable.

326

fiPlanning for Contingencies: A Decision-based Approach
plan for the goal of enjoying an evening out would have one branch for each possible safe
combination. Each branch would start off with the actions to open the safe, which are
different for each combination, and would continue with the actions of going to a restaurant
and then to the movies, say, which would be identical in each branch. A simpler plan would
merge the separate branches after the safe had been opened. The consideration of methods
for branch merging is an area of future work (see Sections 6.5.4 and 7.3).
In some circumstances, such as in this example, plan complexity could be reduced
through the use of run-time variables, which were introduced in ipem (Ambros-Ingerson
& Steel, 1988) and used in sensp (Etzioni et al., 1992) (see Section 7). When the only
uncertainty is in the value that an action parameter takes (which is the case when opening
a combination safe) it would be possible to use a run-time variable to represent that parameter, obviating the need for separate plan branches. Implementing this strategy would
require effective methods for determining when the effects of uncertainty are limited to
parameter values. In general, this notion indicates a possible approach to the problem of
branch merging: that of taking a least commitment approach to variable binding, in the
same way that a least commitment approach is taken to step ordering in a partial order
planner. This would then allow the concept of \conditional" variable binding: a variable
binding could be labeled as being required or forbidden in a given contingency.
We have not analyzed the complexity of Cassandra's algorithm, but we believe it to
be exponential. This is because of the effect of multiple plan branches, whose presence
not only increases the number of steps in a plan but also increases the number of potential
interactions and the number of ways of resolving them. Certainly, our subjective impression
is that Cassandra runs even more slowly than other planners in the snlp family. Effective
domain-independent search control heuristics are dicult to find, and in many of the (toy)
domains in which we have used Cassandra even problem-specific heuristics are hard to come
by.

8.3 Conclusion
Cassandra is a planning system based firmly in the classical planning paradigm. Many of
its strengths and weaknesses are those of other classical planning systems. For example, we
believe that under certain circumstances its plans will be valid and that it is guaranteed
to find a valid plan if one exists. However, the techniques it uses are valid only in limited
circumstances, and its computational complexity is such as to make direct scaling up unlikely
to be feasible.
In our view, the principal strengths of Cassandra arise from the explicit representation of
decisions in its plans. We have shown how this use of decisions provides a natural account
of how knowledge goals arise during the planning process. We have also sketched how
decisions can be used as the basis of extensions that provide added functionality. A new
type of decision allows fail-safe plans, which can provide a method of solving problems such
as the bomb-in-the-toilet problem (Section 6.5.5); and another type of decision may provide
an effective method of interleaving planning and execution (Section 7.4).
We believe that the use of explicit decision procedures will enable the extension of the
range of applicability of techniques of classical planning. In general, the idea of constructing
a single plan that will succeed in all circumstances is, we feel, unlikely to be productive:
327

fiPryor & Collins
the real world is complex and uncertain enough that trying to predict its behavior in detail
is simply impossible. However, the use of decision procedures that, for example, involve
probabilistic techniques or interleave planning and execution, appears likely to provide a
exible framework that, although inevitably sacrificing completeness and correctness, will
provide a basis for effective, practical planning in the real world.

Appendix A. Cassandra's Plans
This appendix shows the plans constructed by Cassandra for the examples in the body of the
paper. Each plan consists of initial conditions, plan steps and goals. The initial conditions
are shown at the top of the plan. Those that are unknown are shown as depending on
a particular contingency. The plan steps are shown next. Each is shown with a number
denoting its order in the plan. The numbers in parentheses show the order in which the
steps were added to the plan. To the right of each step are its contingency labels. For
brevity, the individual effects of each step are always omitted and the links that establish
the step's enabling and secondary preconditions are often omitted.
Finally, at the bottom of the plan come the goal conditions. The goal is stated first,
then each contingency goal is shown with the links that establish it. As usual, contingency
labels are to the right.

A.1 A Plan to Get to Evanston
This is the plan shown in Figure 3 and discussed in Section 2.3.2. Note the decision-step
with a single active decision-rule. This is the situation discussed in the comments on onesided contingencies in Section 6.5: the route using Western is quicker when it is clear, while
the Ashland route is slower but always possible.
Initial:

When [TRAFFIC0S: GOOD] (NOT (TRAFFIC-BAD))
When [TRAFFIC0S: BAD] (TRAFFIC-BAD)
(AND (AT START) (ROAD WESTERN) (ROAD BELMONT) (ROAD ASHLAND))

Step

1 (4): (GO-TO-WESTERN-AT-BELMONT)
YES: [TRAFFIC0S: GOOD BAD]
(AND (NOT (AT START)) (ON WESTERN) (ON BELMONT))
0 -> (AT START)

Step

2 (3): (CHECK-TRAFFIC-ON-WESTERN)
(KNOW-IF (TRAFFIC-BAD))
1 -> (ON WESTERN)

Step

3 (2): (DECIDE TRAFFIC0S)
(and (NOT (TRAFFIC-BAD))
T
) => [TRAFFIC0S: GOOD]
(and T
) => [TRAFFIC0S: BAD]
2 -> (KNOW-IF (TRAFFIC-BAD))

Step

4 (6): (TAKE-BELMONT)

YES: [TRAFFIC0S: BAD]
NO : [TRAFFIC0S: GOOD]
(AND (NOT (ON WESTERN)) (ON ASHLAND))
1 -> (ON BELMONT)

328

fiPlanning for Contingencies: A Decision-based Approach
Step

5 (5): (TAKE-ASHLAND)

YES: [TRAFFIC0S: BAD]
NO : [TRAFFIC0S: GOOD]

(AT EVANSTON)
4 -> (ON ASHLAND)
Step

NO : [TRAFFIC0S: GOOD]

6 (1): (TAKE-WESTERN)

YES: [TRAFFIC0S: GOOD]
NO : [TRAFFIC0S: BAD]

(AT EVANSTON)
1 -> (ON WESTERN)
0 -> (NOT (TRAFFIC-BAD))
Goal:

NO : [TRAFFIC0S: BAD]
NO : [TRAFFIC0S: BAD]

(AT EVANSTON)
GOAL
5 -> (AT EVANSTON)

YES: [TRAFFIC0S: BAD]
NO : [TRAFFIC0S: GOOD]

6 -> (AT EVANSTON)

YES: [TRAFFIC0S: GOOD]
NO : [TRAFFIC0S: BAD]

GOAL

Complete!

A.2 Disarming a Bomb

This is the plan shown in Figures 6 and 7 and discussed in Section 4.1.1. Note that both
moving steps and both dunking steps are always possible, but each is only necessary in one
outcome of the uncertainty. A fail-safe plan (see Section 6.2) is therefore possible.
Initial:

When [UNK0S: O2] (CONTAINS PACKAGE-2 BOMB)
When [UNK0S: O1] (CONTAINS PACKAGE-1 BOMB)
(AND (AT PACKAGE-1 RUG) (AT PACKAGE-2 RUG))

Step

1 (5): (X-RAY PACKAGE-2)
(KNOW-IF (CONTAINS PACKAGE-2 BOMB))

Step

2 (3): (X-RAY PACKAGE-1)
(KNOW-IF (CONTAINS PACKAGE-1 BOMB))

Step

3 (2): (DECIDE UNK0S)
(and (CONTAINS PACKAGE-2 BOMB)
T
) => [UNK0S: O2]
(and (CONTAINS PACKAGE-1 BOMB)
T
) => [UNK0S: O1]
1 -> (KNOW-IF (CONTAINS PACKAGE-2 BOMB))
2 -> (KNOW-IF (CONTAINS PACKAGE-1 BOMB))

Step

4 (7): (MOVE RUG TOILET PACKAGE-1)
YES: [UNK0S: O1]
(AND (NOT (AT PACKAGE-1 RUG)) (AT PACKAGE-1 TOILET))
0 -> (AT PACKAGE-1 RUG)

Step

5 (6): (MOVE RUG TOILET PACKAGE-2)
YES: [UNK0S: O2]
(AND (NOT (AT PACKAGE-2 RUG)) (AT PACKAGE-2 TOILET))
0 -> (AT PACKAGE-2 RUG)

Step

6 (4): (DUNK PACKAGE-2)
(WET PACKAGE-2)

YES: [UNK0S: O2]

329

fiPryor & Collins
5 -> (AT PACKAGE-2 TOILET)
(DISARMED BOMB)
0 -> (CONTAINS PACKAGE-2 BOMB)
Step

7 (1): (DUNK PACKAGE-1)
(WET PACKAGE-1)
4 -> (AT PACKAGE-1 TOILET)
(DISARMED BOMB)
0 -> (CONTAINS PACKAGE-1 BOMB)

Goal:

NO : [UNK0S: O1]
YES: [UNK0S: O1]

NO : [UNK0S: O2]

(DISARMED BOMB)
GOAL
6 -> (DISARMED BOMB)

YES: [UNK0S: O2]
NO : [UNK0S: O1]

7 -> (DISARMED BOMB)

YES: [UNK0S: O1]
NO : [UNK0S: O2]

GOAL

Complete!

A.3 Fetching a Package

The plan in Figure 8, discussed in Section 4.1.3, involves just one source of uncertainty and
hence contains just one decision-step. There are two possible ways of achieving the goal,
one for each outcome of the uncertainty.
Initial:

(AVAILABLE CAR-1)
When [LOC0S: B] (PACKAGE-AT LOCATION-2)
When [LOC0S: A] (PACKAGE-AT LOCATION-1)
(AND (IS-CAR CAR-1) (IS-CAR CAR-2) (LOCATION LOCATION-1)
(LOCATION LOCATION-2))

Step

1 (2): (ASK-ABOUT-PACKAGE)
(KNOW-IF (PACKAGE-AT LOCATION-2))
0 -> (LOCATION LOCATION-2)
(KNOW-IF (PACKAGE-AT LOCATION-1))
0 -> (LOCATION LOCATION-1)

Step

2 (1): (DECIDE LOC0S)
(and (PACKAGE-AT
T
(and (PACKAGE-AT
T
1 -> (KNOW-IF
1 -> (KNOW-IF

LOCATION-2)
) => [LOC0S: B]
LOCATION-1)
) => [LOC0S: A]
(PACKAGE-AT LOCATION-2))
(PACKAGE-AT LOCATION-1))

Step

3 (4): (DRIVE CAR-1 LOCATION-1)
(AT LOCATION-1)
0 -> (AVAILABLE CAR-1)

YES: [LOC0S: A]

Step

4 (3): (DRIVE CAR-1 LOCATION-2)
(AT LOCATION-2)
0 -> (AVAILABLE CAR-1)

YES: [LOC0S: B]

Goal:

(AND (AT ?LOC) (PACKAGE-AT ?LOC))

330

fiPlanning for Contingencies: A Decision-based Approach

GOAL

YES: [LOC0S: B]
4 -> (AT LOCATION-2)
0 -> (PACKAGE-AT LOCATION-2)

GOAL

NO : [LOC0S: A]
YES: [LOC0S: A]

3 -> (AT LOCATION-1)
0 -> (PACKAGE-AT LOCATION-1)

NO : [LOC0S: B]

Complete!

A.4 Fetching Another Package

The plan in Figure 9, discussed in Section 4.1.3, has two sources of uncertainty and two
decision-steps. There are four possible ways of achieving the goal, one for each combination
of the outcomes of the two sources of uncertainty.
Initial:

Step

When
When
When
When
(AND

[CAR0S: C2] (AVAILABLE
[CAR0S: C1] (AVAILABLE
[LOC0S: B] (PACKAGE-AT
[LOC0S: A] (PACKAGE-AT
(IS-CAR CAR-1) (IS-CAR
(LOCATION LOCATION-2))

CAR-2)
CAR-1)
LOCATION-2)
LOCATION-1)
CAR-2) (LOCATION LOCATION-1)

1 (5): (ASK-ABOUT-CAR)

YES: [LOC0S: A B]

(KNOW-IF (AVAILABLE CAR-2))
0 -> (IS-CAR CAR-2)
(KNOW-IF (AVAILABLE CAR-1))
0 -> (IS-CAR CAR-1)
Step

2 (4): (DECIDE CAR0S)
YES: [LOC0S: A B]
(and (AVAILABLE CAR-2)
T
) => [CAR0S: C2]
(and (AVAILABLE CAR-1)
T
) => [CAR0S: C1]
1 -> (KNOW-IF (AVAILABLE CAR-2))
1 -> (KNOW-IF (AVAILABLE CAR-1))

Step

3 (2): (ASK-ABOUT-PACKAGE)

YES: [CAR0S: C2 C1]

(KNOW-IF (PACKAGE-AT LOCATION-2))
0 -> (LOCATION LOCATION-2)
(KNOW-IF (PACKAGE-AT LOCATION-1))
0 -> (LOCATION LOCATION-1)
Step

4 (1): (DECIDE LOC0S)
(and (PACKAGE-AT
T
(and (PACKAGE-AT
T
3 -> (KNOW-IF
3 -> (KNOW-IF

YES: [CAR0S: C2 C1]
LOCATION-2)
) => [LOC0S: B]
LOCATION-1)
) => [LOC0S: A]
(PACKAGE-AT LOCATION-2))
(PACKAGE-AT LOCATION-1))

Step

5 (8): (DRIVE CAR-2 LOCATION-1)

YES: [LOC0S: A][CAR0S: C2]

331

fiPryor & Collins
NO : [CAR0S: C1]
(AT LOCATION-1)
0 -> (AVAILABLE CAR-2)
Step

NO : [CAR0S: C1]

6 (6): (DRIVE CAR-2 LOCATION-2)

YES: [LOC0S: B][CAR0S: C2]
NO : [CAR0S: C1]

(AT LOCATION-2)
0 -> (AVAILABLE CAR-2)
Step

NO : [CAR0S: C1]

7 (7): (DRIVE CAR-1 LOCATION-1)

YES: [LOC0S: A][CAR0S: C1]
NO : [CAR0S: C2]

(AT LOCATION-1)
0 -> (AVAILABLE CAR-1)
Step

NO : [CAR0S: C2]

8 (3): (DRIVE CAR-1 LOCATION-2)

YES: [LOC0S: B][CAR0S: C1]
NO : [CAR0S: C2]

(AT LOCATION-2)
0 -> (AVAILABLE CAR-1)
Goal:

NO : [CAR0S: C2]

(AND (AT ?LOC) (PACKAGE-AT ?LOC))
GOAL
5 -> (AT LOCATION-1)
0 -> (PACKAGE-AT LOCATION-1)

YES: [LOC0S: A][CAR0S: C2]
NO : [CAR0S: C1]
NO : [LOC0S: B]

6 -> (AT LOCATION-2)
0 -> (PACKAGE-AT LOCATION-2)

YES: [LOC0S: B][CAR0S: C2]
NO : [CAR0S: C1]
NO : [LOC0S: A]

8 -> (AT LOCATION-2)
0 -> (PACKAGE-AT LOCATION-2)

YES: [LOC0S: B][CAR0S: C1]
NO : [CAR0S: C2]
NO : [LOC0S: A]

7 -> (AT LOCATION-1)
0 -> (PACKAGE-AT LOCATION-1)

YES: [LOC0S: A][CAR0S: C1]
NO : [CAR0S: C2]
NO : [LOC0S: B]

GOAL

GOAL

GOAL

Complete!

A.5 Tossing a Coin

In Section 4.2.3 we described a plan for ending up with a at coin. The decision in this plan
does not distinguish between the coin landing heads-up and tails-up|the decision rules are
ambiguous.
Initial:

(HOLDING-COIN)

Step

1 (2): (TOSS-COIN)
(AND (NOT (HOLDING-COIN)) (ON-TABLE))
0 -> (HOLDING-COIN)

Step

2 (4): (INSPECT-COIN)
(AND (KNOW-IF (FLAT-COIN)) (KNOW-IF (HEADS-UP))
(KNOW-IF (TAILS-UP)) (KNOW-IF (ON-EDGE)))

332

fiPlanning for Contingencies: A Decision-based Approach
Step

3 (3): (DECIDE UNK2S)
(and (FLAT-COIN)
T
) => [UNK2S: H]
(and (FLAT-COIN)
T
) => [UNK2S: T]
(and (ON-EDGE)
T
) => [UNK2S: E]
2 -> (KNOW-IF (FLAT-COIN))
2 -> (KNOW-IF (ON-EDGE))

Step

4 (1): (TIP-COIN)

YES: [UNK2S: E]
NO : [UNK2S: H T]

(FLAT-COIN)
1 -> (ON-EDGE)
Goal:

NO : [UNK2S: H T]

(FLAT-COIN)
GOAL
1 -> (FLAT-COIN)

YES: [UNK2S: T]
NO : [UNK2S: H E]

1 -> (FLAT-COIN)

YES: [UNK2S: H]
NO : [UNK2S: T E]

4 -> (FLAT-COIN)

YES: [UNK2S: E]
NO : [UNK2S: H T]

GOAL

GOAL

Complete!

A.6 Tossing Another Coin
The plan in Figure 11 has two decisions with unambiguous decision-rules. There are four
ways of achieving the goal in this plan, because there are two sources of uncertainty.
Initial:

(HOLDING-COIN)

Step

1 (1): (TOSS-COIN)
(AND (NOT (HOLDING-COIN)) (ON-TABLE) (KNOW-IF (FLAT-COIN))
(KNOW-IF (HEADS-UP)) (KNOW-IF (TAILS-UP)) (KNOW-IF (ON-EDGE)))
0 -> (HOLDING-COIN)

Step

2 (2): (DECIDE TOSS1S)
(and (FLAT-COIN)
(HEADS-UP)
T
(and (ON-EDGE)
T
(and (FLAT-COIN)
(TAILS-UP)
T
1 -> (KNOW-IF
1 -> (KNOW-IF
1 -> (KNOW-IF
1 -> (KNOW-IF

Step

) => [TOSS1S: H]
) => [TOSS1S: E]

) => [TOSS1S: T]
(ON-EDGE))
(FLAT-COIN))
(TAILS-UP))
(HEADS-UP))

3 (4): (TIP-COIN)

YES: [TOSS1S: E]

333

fiPryor & Collins
NO : [TOSS1S: T H]
(AND (FLAT-COIN) (KNOW-IF (HEADS-UP)) (KNOW-IF (TAILS-UP)))
1 -> (ON-EDGE)
NO : [TOSS1S: H T]
Step

4 (5): (DECIDE TIP4S)

YES: [TOSS1S: E]
NO : [TOSS1S: T H]

(and (TAILS-UP)
T
) => [TIP4S:
(and (HEADS-UP)
T
) => [TIP4S:
3 -> (KNOW-IF (TAILS-UP))
3 -> (KNOW-IF (HEADS-UP))
Step

5 (3): (TURN-OVER)

(HEADS-UP)
1 -> (TAILS-UP)

NO : [TOSS1S: H E]

6 (6): (TURN-OVER)

YES: [TOSS1S: E][TIP4S: T]
NO : [TOSS1S: T H][TIP4S: H]
NO : [TOSS1S: T H]

3 -> (FLAT-COIN)
(HEADS-UP)
3 -> (TAILS-UP)
Goal:

H]
NO : [TOSS1S: T H]
NO : [TOSS1S: T H]
YES: [TOSS1S: T]
NO : [TOSS1S: E H]
NO : [TOSS1S: H E]

1 -> (FLAT-COIN)

Step

T]

NO : [TOSS1S: T H][TIP4S: H]

(AND (FLAT-COIN) (HEADS-UP))
GOAL
3 -> (FLAT-COIN)
6 -> (HEADS-UP)

YES: [TOSS1S: E][TIP4S: T]
NO : [TOSS1S: T H]
NO : [TOSS1S: T H][TIP4S: H]

3 -> (FLAT-COIN)
3 -> (HEADS-UP)

YES: [TOSS1S: E][TIP4S: H]
NO : [TOSS1S: T H]
NO : [TOSS1S: H T][TIP4S: T]

1 -> (FLAT-COIN)
5 -> (HEADS-UP)

YES: [TOSS1S: T]
NO : [TOSS1S: H E]
NO : [TOSS1S: E H]

1 -> (FLAT-COIN)
1 -> (HEADS-UP)

YES: [TOSS1S: H]
NO : [TOSS1S: T E]
NO : [TOSS1S: T E]

GOAL

GOAL

GOAL

Complete!

A.7 Opening a Door
In Section 4.2.4 we described a plan for opening a locked door without a key; it is depicted
in Figure 12. The plan that Cassandra produces for this situation is shown here. Even
though no preconditions of the pick step depend on any effect of the kick step, the former
cannot be performed if the lock is broken as a result of kicking the door. The decision-rules
reect this dependence.
334

fiPlanning for Contingencies: A Decision-based Approach
Initial:

(LOCK-INTACT)

Step

1 (2): (KICK)

Step

2 (4): (LOOK)
(AND (KNOW-IF (LOCKED)) (KNOW-IF (LOCK-INTACT))
(KNOW-IF (FOOT-BROKEN)))

Step

3 (3): (DECIDE KICK2S)
(and ((LOCK-INTACT))
T
) => [KICK2S: F]
(and (NOT (LOCKED))
T
) => [KICK2S: L]
2 -> (KNOW-IF (LOCKED))

Step

4 (6): (PICK)

YES: [KICK2S: F]
NO : [KICK2S: L]

(NOT (LOCKED))
0 -> (LOCK-INTACT)
Step

NO : [KICK2S: L]

5 (5): (OPEN-DOOR)

YES: [KICK2S: F]
NO : [KICK2S: L]

(OPEN)
4 -> (NOT (LOCKED))
Step

NO : [KICK2S: L]

6 (1): (OPEN-DOOR)

YES: [KICK2S: L]
NO : [KICK2S: F]

(OPEN)
1 -> (NOT (LOCKED))
Goal:

NO : [KICK2S: F]

(OPEN)
GOAL
5 -> (OPEN)

YES: [KICK2S: F]
NO : [KICK2S: L]

6 -> (OPEN)

YES: [KICK2S: L]
NO : [KICK2S: F]

GOAL

Complete!

Acknowledgements
Thanks to Dan Weld and Tony Barrett for supplying the ucpop code, Mark Peot and Robert
Goldman for their comments on earlier drafts, Will Fitzgerald for many useful discussions,
and the anonymous reviewers for their constructive and helpful criticism. Much of this
work was performed while the first author was a student at the Institute for the Learning
Sciences, Northwestern University. This work was supported in part by the AFOSR under
grant number AFOSR-91-0341-DEF. The Institute for the Learning Sciences was established
in 1989 with the support of Andersen Consulting, part of The Arthur Andersen Worldwide
Organization. The Institute receives additional support from Ameritech and North West
Water, Institute Partners, and from IBM.
335

fiPryor & Collins

References

Allen, J., Hendler, J., & Tate, A. (Eds.). (1990). Readings in Planning. Morgan Kaufmann,
San Mateo, CA.
Ambros-Ingerson, J., & Steel, S. (1988). Integrating planning, execution, and monitoring.
In Proceedings of the Seventh National Conference on Artificial Intelligence, pp. 83{88
St Paul, MN. AAAI. Also in (Allen, Hendler, & Tate, 1990).
Barrett, A., Soderland, S., & Weld, D. S. (1991). Effect of step-order representations on
planning. Technical report 91-05-06, Department of Computer Science and Engineering, University of Washington, Seattle.
Chapman, D. (1987). Planning for conjunctive goals. Artificial Intelligence, 32, 333{377.
Also in (Allen et al., 1990).
Collins, G. C. (1987). Plan creation: Using strategies as blueprints. Technical report
YALEU/CSD/RR 599, Department of Computer Science, Yale University.
Collins, G., & Pryor, L. (1992). Achieving the functionality of filter conditions in a partial
order planner. In Proceedings of the Tenth National Conference on Artificial Intelligence, pp. 375{380 San Jose, CA. AAAI.
Collins, G., & Pryor, L. (1995). Planning under uncertainty: Some key issues. In Proceedings
of the Fourteenth International Joint Conference on Artificial Intelligence, pp. 1567{
1573 Montreal, Canada. IJCAI.
Draper, D., Hanks, S., & Weld, D. (1994a). A probabilistic model of action for leastcommitment planning with information gathering. In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, pp. 178{186 Seattle, WA. Morgan
Kaufmann.
Draper, D., Hanks, S., & Weld, D. (1994b). Probabilistic planning with information gathering and contingent execution. In Proceedings of the Second International Conference
on Artificial Intelligence Planning Systems, pp. 31{36 Chicago, IL. AAAI Press.
Drummond, M. (1989). Situated control rules. In Proceedings of the First International
Conference on Principles of Knowledge Representation and Reasoning, pp. 103{113
Toronto. Morgan Kaufmann.
Etzioni, O., Hanks, S., Weld, D., Draper, D., Lesh, N., & Williamson, M. (1992). An approach to planning with incomplete information. In Proceedings of the Third International Conference on Knowledge Representation and Reasoning, pp. 115{125 Boston,
MA. Morgan Kaufmann.
Feldman, J. A., & Sproull, R. F. (1977). Decision theory and artificial intelligence II: The
hungry monkey. Cognitive Science, 1, 158{192. Also in (Allen et al., 1990).
Fikes, R. E., & Nilsson, N. J. (1971). STRIPS: A new approach to the application of
theorem proving to problem solving. Artificial Intelligence, 2, 189{208. Also in (Allen
et al., 1990).
336

fiPlanning for Contingencies: A Decision-based Approach
Golden, K., Etzioni, O., & Weld, D. (1994). Omnipotence without omniscience: Ecient
sensor management for planning. In Proceedings of the Twelfth National Conference
on Artificial Intelligence, pp. 1048{1054. AAAI Press.
Goldman, R. P., & Boddy, M. S. (1994a). Conditional linear planning. In Proceedings of
the Second International Conference on Artificial Intelligence Planning Systems, pp.
80{85 Chicago, IL. AAAI Press.
Goldman, R. P., & Boddy, M. S. (1994b). Representing uncertainty in simple planners. In
Proceedings of the Fourth International Conference on the Principles of Knowledge
Representation and Reasoning, pp. 238{245 Bonn. Morgan Kaufmann.
Haas, A. R. (1986). A syntactic theory of belief and action. Artificial Intelligence, 28,
245{292.
Haddawy, P., & Hanks, S. (1992). Representations for decision-theoretic planning: Utility
functions for deadline goals. In Proceedings of the Third International Conference
of Principles of Knowledge Representation and Reasoning, pp. 71{82 Boston, MA.
Morgan Kaufmann.
Haddawy, P., & Suwandi, M. (1994). Decision-theoretic refinement planning using inheritance abstraction. In Proceedings of the Second Internatinal Conference on Artificial
Planning Systems, pp. 266{271 Chicago. AAAI Press.
Knoblock, C. (1995). Planning, executing, sensing, and replanning for information gathering. In Proceedings of the Fourteenth International Joint Conference on Artificial
Intelligence, pp. 1686{1693 Montreal. IJCAI.
Kushmerick, N., Hanks, S., & Weld, D. (1995). An algorithm for probabilistic planning.
Artificial Intelligence, 76, 239{286.
Langley, P. (1992). Systematic and nonsystematic search strategies. In Proceedings of the
First International Conference on Artificial Intelligence Planning Systems, pp. 145{
152 College Park, Maryland. Morgan Kaufmann.
McAllester, D., & Rosenblitt, D. (1991). Systematic nonlinear planning. In Proceedings of
the Ninth National Conference on Artificial Intelligence, pp. 634{639 Anaheim, CA.
AAAI.
McCarthy, J., & Hayes, P. J. (1969). Some philosophical problems from the standpoint of
artificial intelligence. In Meltzer, B., & Michie, D. (Eds.), Machine Intelligence 4, pp.
463{502. Edinburgh University Press. Also in (Allen et al., 1990).
McDermott, D. (1987). A critique of pure reason. Computational Intelligence, 3, 151{160.
McDermott, D. (1991). Regression planning. International Journal of Intelligent Systems,
6 (4), 357{416. Also available as Yale TR YALEU/CSD/RR 752.
Moore, R. C. (1985). A formal theory of knowledge and action. In Hobbs, J. R., & Moore,
R. C. (Eds.), Formal Theories of the Commonsense World. Ablex, Norwood, NJ. Also
in (Allen et al., 1990).
337

fiPryor & Collins
Morgenstern, L. (1987). Knowledge preconditions for actions and plans. In Proceedings
of the Tenth International Joint Conference on Artificial Intelligence, pp. 867{874
Milan. IJCAI.
Olawsky, D., & Gini, M. (1990). Deferred planning and sensor use. In Proceedings of a
Workshop on Innovative Approaches to Planning, Scheduling and Control, pp. 166{
174 San Diego, CA. DARPA.
Pednault, E. P. D. (1988). Extending conventional planning techniques to handle actions
with context-dependent effects. In Proceedings of the Seventh National Conference on
Artificial Intelligence, pp. 55{59 St Paul, MN. AAAI.
Pednault, E. P. D. (1989). ADL: Exploring the middle ground between STRIPS and the
situation calculus. In Proceedings of the First International Conference on Principles
of Knowledge Representation and Reasoning, pp. 324{332. Morgan Kaufmann.
Pednault, E. P. D. (1991). Generalizing nonlinear planning to handle complex goals and
actions with context-dependent effects. In Proceedings of the Twelfth International
Joint Conference on Artificial Intelligence, pp. 240{245 Sydney, Australia. IJCAI.
Penberthy, J. S., & Weld, D. S. (1992). UCPOP: A sound, complete, partial order planner for ADL. In Proceedings of the Third International Conference on Knowledge
Representation and Reasoning, pp. 103{114 Boston, MA. Morgan Kaufmann.
Peot, M. A., & Smith, D. E. (1992). Conditional nonlinear planning. In Proceedings of
the First International Conference on Artificial Intelligence Planning Systems, pp.
189{197 College Park, Maryland. Morgan Kaufmann.
Pryor, L. (1994). Opportunities and planning in an unpredictable world. Technical report 53,
Institute for the Learning Sciences, Northwestern University.
Pryor, L. (1995). Decisions, decisions: Knowledge goals in planning. In Hallam, J. (Ed.),
Hybrid Problems, Hybrid Solutions (Proceedings of AISB-95), Frontiers in Artificial
Intelligence and Applications, pp. 181{192. IOS Press, Amsterdam.
Pryor, L., & Collins, G. (1991). Information-gathering as a planning task: A position paper.
In Notes of the AAAI workshop on Knowledge-Based Construction of Probabilistic and
Decision Models, pp. 101{105 Anaheim, CA. AAAI.
Pryor, L., & Collins, G. (1992). Planning to perceive: A utilitarian approach. In Working
notes of the AAAI Spring Symposium: Control of Selective Perception, pp. 113{122
Stanford, CA. AAAI.
Pryor, L., & Collins, G. (1993). Cassandra: Planning with contingencies. Technical report 41, Institute for the Learning Sciences, Northwestern University.
Sacerdoti, E. (1977). A structure for plans and behavior. American Elsevier, New York.
Schoppers, M. J. (1987). Universal plans for reactive robots in unpredictable environments.
In Proceedings of the Tenth International Joint Conference on Artificial Intelligence,
pp. 1039{1046 Milan. IJCAI.
338

fiPlanning for Contingencies: A Decision-based Approach
Steel, S. (1995). Knowing how: A semantic approach. In Hallam, J. (Ed.), Hybrid Problems,
Hybrid Solutions (Proceedings of AISB-95), Frontiers in Artificial Intelligence and
Applications, pp. 193{202. IOS Press, Amsterdam.
Stefik, M. (1981a). Planning with constraints (MOLGEN: Part 1). Artificial Intelligence,
16, 111{140. Also in (Allen et al., 1990).
Stefik, M. (1981b). Planning with constraints (MOLGEN: Part 2). Artificial Intelligence,
16, 141{170.
Sussman, G. J. (1975). A computer model of skill acquisition. American Elsevier, New
York.
Tate, A. (1975). Using goal structure to direct search in a problem solver. Ph.D. thesis,
University of Edinburgh.
Tate, A. (1977). Generating project networks. In Proceedings of the Fifth International
Joint Conference on Artificial Intelligence, pp. 888{893 Cambridge, MA. IJCAI. Also
in (Allen et al., 1990).
Warren, D. (1976). Generating conditional plans and programs. In Proceedings of the
Summer Conference on Artificial Intelligence and the Simulation of Behaviour, pp.
344{354 Edinburgh. AISB.
Wilkins, D. E. (1988). Practical Planning: Extending the Classical AI Planning Paradigm.
Morgan Kaufmann, San Mateo, CA.

339

fiJournal of Artificial Intelligence Research 4 (1996) 147-179

Submitted 3/95; published 4/96

Iterative Optimization and Simplification
of Hierarchical Clusterings
Doug Fisher

Department of Computer Science, Box 1679, Station B
Vanderbilt University, Nashville, TN 37235 USA

dfisher@vuse.vanderbilt.edu

Abstract

Clustering is often used for discovering structure in data. Clustering systems differ in
the objective function used to evaluate clustering quality and the control strategy used to
search the space of clusterings. Ideally, the search strategy should consistently construct
clusterings of high quality, but be computationally inexpensive as well. In general, we
cannot have it both ways, but we can partition the search so that a system inexpensively
constructs a `tentative' clustering for initial examination, followed by iterative optimization,
which continues to search in background for improved clusterings. Given this motivation,
we evaluate an inexpensive strategy for creating initial clusterings, coupled with several
control strategies for iterative optimization, each of which repeatedly modifies an initial
clustering in search of a better one. One of these methods appears novel as an iterative
optimization strategy in clustering contexts. Once a clustering has been constructed it
is judged by analysts { often according to task-specific criteria. Several authors have abstracted these criteria and posited a generic performance task akin to pattern completion,
where the error rate over completed patterns is used to `externally' judge clustering utility.
Given this performance task, we adapt resampling-based pruning strategies used by supervised learning systems to the task of simplifying hierarchical clusterings, thus promising to
ease post-clustering analysis. Finally, we propose a number of objective functions, based
on attribute-selection measures for decision-tree induction, that might perform well on the
error rate and simplicity dimensions.

1. Introduction
Clustering is often used for discovering structure in data. Clustering systems differ in the
objective function used to evaluate clustering quality and the control strategy used to search
the space of clusterings. Ideally, the search strategy should consistently construct clusterings of high quality, but be computationally inexpensive as well. Given the combinatorial
complexity of the general clustering problem, a search strategy cannot be both computationally inexpensive and give any guarantee about the quality of discovered clusterings across
a diverse set of domains and objective functions. However, we can partition the search so
that an initial clustering is inexpensively constructed, followed by iterative optimization
procedures that continue to search in background for improved clusterings. This allows an
analyst to get an early indication of the possible presence and form of structure in data, but
search can continue as long as it seems worthwhile. This seems to be a primary motivation
behind the design of systems such as Autoclass (Cheeseman, Kelly, Self, Stutz, Taylor,
& Freeman, 1988) and Snob (Wallace & Dowe, 1994).
This paper describes and evaluates three strategies for iterative optimization, one inspired by the iterative `seed' selection strategy of Cluster/2 (Michalski & Stepp, 1983a,
c 1996 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiFisher

1983b), one is a common form of optimization that iteratively reclassifies single observations, and a third method appears novel in the clustering literature. This latter strategy
was inspired, in part, by macro-learning strategies (Iba, 1989) { collections of observations
are reclassified en masse, which appears to mitigate problems associated with local maxima
as measured by the objective function. For evaluation purposes, we couple these strategies with a simple, inexpensive procedure used by Cobweb (Fisher, 1987a, 1987b) and a
system by Anderson and Matessa (1991), which constructs an initial hierarchical clustering. These iterative optimization strategies, however, can be paired with other methods for
constructing initial clusterings.
Once a clustering has been constructed it is judged by analysts { often according to
task-specific criteria. Several authors (Fisher, 1987a, 1987b; Cheeseman et al., 1988; Anderson & Matessa, 1991) have abstracted these criteria into a generic performance task
akin to pattern completion, where the error rate over completed patterns can be used to
`externally' judge the utility of a clustering. In each of these systems, the objective function
has been selected with this performance task in mind. Given this performance task we
adapt resampling-based pruning strategies used by supervised learning systems to the task
of simplifying hierarchical clusterings, thus easying post-clustering analysis. Experimental
evidence suggests that hierarchical clusterings can be greatly simplified with no increase in
pattern-completion error rate.
Our experiments with clustering simplification suggest `external' criteria of simplicity
and classification cost, in addition to pattern-completion error rate, for judging the relative
merits of differing objective functions in clustering. We suggest several objective functions
that are adaptations of selection measures used in supervised, decision-tree induction, which
may do well on the dimensions of simplicity and error rate.

2. Generating Hierarchical Clusterings

Clustering is a form of unsupervised learning that partitions observations into classes or
clusters (collectively, called a clustering). An objective function or quality measure guides
this search, ideally for a clustering that is optimal as measured by the objective function.
A hierarchical-clustering system creates a tree-structured clustering, where sibling clusters
partition the observations covered by their common parent. This section briey summarizes
a simple strategy, called hierarchical sorting, for creating hierarchical clusterings.

2.1 An Objective Function

We assume that an observation is a vector of nominal values, Vij along distinct variables,
Ai . A measure of category utility (Gluck & Corter, 1985; Corter & Gluck, 1992),
X X[P (A = V jC )2 , P (A = V )2];
CU (Ck ) = P (Ck )
i
ij k
i
ij
i

j

and/or variants have been used extensively by a system known as Cobweb (Fisher, 1987a)
and many related systems (Gennari, Langley, & Fisher, 1989; McKusick & Thompson, 1990;
Iba & Gennari, 1991; McKusick & Langley, 1991; Reich & Fenves, 1991; Biswas, Weinberg,
& Li, 1994; De Alte Da Veiga, 1994; Kilander, 1994; Ketterlin, Gancarski, & Korczak,
1995). This measure rewards clusters, Ck , that increase the predictability of variable values
148

fiOptimization of Hierarchical Clusterings

within Ck (i.e., P (Ai = Vij jCk )) relative to their predictability in the population as a whole
(i.e., P (Ai = Vij )). By favoring clusters that increase predictability (i.e., P (Ai = Vij jCk ) >
P (Ai = Vij )), we also necessarily favor clusters that increase variable value predictiveness
(i.e., P (Ck jAi = Vij ) > P (Ck )).
Clusters for which many variable values are predictable are cohesive. Increases in predictability stem from the shared variable values of observations within a cluster. A cluster
is well-separated or decoupled from other clusters if many variable values are predictive of
the cluster. High predictiveness stems from the differences in the variable values shared
by members of one cluster from those shared by observations of another cluster. A general principle of clustering is to increase the similarity of observations within clusters (i.e.,
cohesion) and to decrease the similarity of observations across clusters (i.e., coupling).
Category utility is similar in form to the Gini Index, which has been used in supervised
systems that construct decision trees (Mingers, 1989b; Weiss & Kulikowski, 1991). The
Gini Index is typically intended to address the issue of how well the values of a variable, Ai ,
predict a priori known class labels in a supervised context. The summation over Gini Indices
reected in CU addresses the extent that a cluster predicts the values of all the variables.
CU rewards clusters, Ck , that most reduce a collective impurity over all variables.
In Fisher's (1987a) CobwebPsystem, CU is used to measure the quality of a partition of
data, PU (fC1; C2; : : :CN g) = k CU (Ck )=N or the average category utility of clusters in
the partition. Sections 3.5 and 5.2 note some nonoptimalities with this measure of partition
quality, and suggest some alternatives. Nonetheless, this measure is commonly used, we
will take this opportunity to note its problems, and none of the techniques that we describe
is tied to this measure.

2.2 The Structure of Clusters

As in Cobweb, Autoclass (Cheeseman et al., 1988), and other systems (Anderson &
Matessa, 1991), we will assume that clusters, Ck , are described probabilistically: each
variable value has an associated conditional probability, P (Ai = Vij jCk ), which reects the
proportion of observations in Ck that exhibit the value, Vij , along variable Ai . In fact, each
variable value is actually associated with the number of observations in the cluster having
that value; probabilities are computed `on demand' for purposes of evaluation.
Probabilistically-described clusters arranged in a tree form a hierarchical clustering
known as a probabilistic categorization tree. Each set of sibling clusters partitions the
observations covered by the common parent. There is a single root cluster, identical in
structure to other clusters, but covering all observations and containing frequency information necessary to compute P (Ai = Vij )'s as required by category utility. Figure 1 gives an
example of a probablistic categorization tree (i.e., a hierarchical clustering) in which each
node is a cluster of observations summarized probabilistically. Observations are at leaves
and are described by three variables: Size, Color, and Shape.

2.3 Hierarchical Sorting

Our strategy for initial clustering is sorting, which is a term adapted from a psychological
task that requires subjects to perform roughly the same procedure that we describe here
(Ahn & Medin, 1989). Given an observation and a current partition, sorting evaluates the
149

fiFisher

Size
Shape
Color

sma 0.50
squ 0.50
blu 0.25

med 0.25 lar 0.25
sph 0.50
gre 0.25 red 0.50

P(C1jroot)=0.50
sma 1.00
squ 1.00
blu 0.50 gre 0.50

sma 1.00
squ 1.00
blu 1.00
P(C3jC1)=0.50

P(root)=1.0

P(C2jroot)=0.50
med 0.50 lar 0.50
sph 1.00
red 1.00

sma 1.00
squ 1.00
gre 1.00
P(C4jC1)=0.50

med 1.00
sph 1.00
red 1.00
P(C5jC2)=0.50

lar 1.00
sph 1.00
red 1.00
P(C6jC2)=0.50

Figure 1: A probabilistic categorization tree.
quality of new clusterings that result from placing the observation in each of the existing
clusters, and the quality of the clustering that results from creating a new cluster that only
covers the new observation; the option that yields the highest quality score (e.g., using PU )
is selected. The clustering grows incrementally as new observations are added.
This procedure is easily incorporated into a recursive loop that builds tree-structured
clusterings: given an existing hierarchical clustering, an observation is sorted relative to the
top-level partition (i.e., children of the root); if an existing child of the root is chosen to
include the observation, then the observation is sorted relative to the children of this node,
which now serves as the root in this recursive call. If a leaf is reached, the tree is extended
downward. The maximum height of the tree can be bounded, thus limiting downward
growth to fixed depth. Figure 2 shows the tree of Figure 1 after two new observations have
been added to it: one observation extends the left subtree downward, while the second is
made a new leaf at the deepest, existing level of the right subtree.
This sorting strategy is identical to that used by Anderson and Matessa (1991). The children of each cluster partition the observations that are covered by their parent, though the
measure, PU , used to guide sorting differs from that of Anderson and Matessa. The observations themselves are stored as singleton clusters at leaves of the tree. Other hierarchical-sort
based strategies augment this basic procedure in a manner described in Section 3.3 (Fisher,
1987a; Hadzikadic & Yun, 1989; Decaestecker, 1991).

150

fiOptimization of Hierarchical Clusterings

Size
Shape
Color
P(C1jroot)=0.50
sma 1.00
squ 0.67
blu 0.33 gre 0.67

sma 1.00
sma 1.00
sqr 0.50
squ 1.00
gre 1.00
blu 1.00
P(C3jC1)=0.33

sma 0.50
squ 0.33
blu 0.17

med 0.33
sph 0.33
gre 0.33

lar 0.17
pyr 0.33
red 0.50

P(root)=1.0

P(C2jroot)=0.50
med 0.67 lar 0.33
sph 0.67 pyr 0.33
red 1.00

pyr 0.33

P(C4jC1)=0.67
med 1.00
lar 1.00
med 1.00
pyr 0.50
pyr 1.00
sph 1.00
sph 1.00
red 1.00
red 1.00
red 1.00
P(C5jC2)=0.33 P(C7jC2)=0.33 P(C6jC2)=0.33

sma 1.00
squ 1.00
gre 1.00
P(C7|C4)=0.50

New Object

sma 1.00
pyr 1.00
gre 1.00

New Object

P(C8|C4)=0.50

Figure 2: An updated probabilistic categorization tree.

3. Iterative Optimization
Hierarchical sorting quickly constructs a tree-structured clustering, but one which is typically nonoptimal. In particular, this control strategy suffers from ordering effects: different
orderings of the observations may yield different clusterings (Fisher, Xu, & Zard, 1992).
Thus, after an initial clustering phase, a (possibly oine) process of iterative optimization
seeks to uncover better clusterings.

3.1 Seed Selection, Reordering, and Reclustering
Michalski and Stepp's (1983a) Cluster/2 seeks the optimal K-partitioning of data. The
first step selects K random `seed' observations from the data. These seeds are `attractors'
around which the K clusters are grown from the remaining data. Since seed selection can
greatly impact clustering quality, Cluster/2 selects K new seeds that are `centroids' of
the K initial clusters. Clustering is repeated with these new seeds. This process iterates
until there is no further improvement in the quality of generated clusterings.
151

fiFisher

Function ORDER(Root)
If Root is a leaf Then Return(observations covered by Root)
Else Order children of Root from those covering the most
observations to those covering the least.
For each child, Ck , of Root (in order) Do Lk ORDER(Ck )
L MERGE(fLk jlist of objects constructed by ORDER(Ck )g)
Return(L)
Table 1: A procedure for creating a `dissimilarity' ordering of data.
Ordering effects in sorting are related to effects that arise due to differing fixed-K seed
selections: the initial observations in an ordering establish initial clusters that `attract' the
remaining observations. In general, sorting performs better if the initial observations are
from diverse areas of the observation-description space, since this facilitates the establishment of initial clusters that reect these different areas. Fisher, Xu, and Zard (1992) showed
that ordering data so that consecutive observations were dissimilar based on Euclidean distance led to good clusterings. Biswas et al. (1994) adapted this technique in their Iterate
system with similar results. In both cases, sorting used the PU score described previously.
This procedure presumes that observations that appear dissimilar by Euclidean distance
tend to be placed in different clusters using the objective function. Taking the lead from
Cluster/2, a measure-independent idea first sorts using a random data ordering, then
extracts a biased `dissimilarity' ordering from the hierarchical clustering, and sorts again.
The function of Table 1 outlines the reordering procedure. It recursively extracts a list of
observations from the most probable (i.e., largest) cluster to the least probable, and then
merges (i.e., interleaves) these lists, before exiting each recursive call { at each step, an
element from the most probable cluster is placed first, followed by an element of the second
most probable, and so forth. Whatever measure guides clustering, observations in differing clusters have been judged dissimilar by the measure. Thus, this measure-independent
procedure returns a measure-dependent dissimilarity ordering by placing observations from
different clusters back-to-back.
Following initial sorting, we extract a dissimilarity ordering, recluster, and iterate, until
there is no further improvement in clustering quality.

3.2 Iterative Redistribution of Single Observations
A common and long-known form of iterative optimization moves single observations from
cluster to cluster in search of a better clustering (Duda & Hart, 1973). The basic strategy
has been used in one form or another by numerous sort-based algorithms as well (Fisher
et al., 1992). The idea behind iterative redistribution (Biswas, Weinberg, Yang, & Koller,
1991) is simple: observations in a single-level clustering are `removed' from their original
cluster and resorted relative to the clustering. If a cluster contains only one observation,
then the cluster is `removed' and its single observation is resorted. This process continues
until two consecutive iterations yield the same clustering.
152

fiOptimization of Hierarchical Clusterings



HHH
HH



HH


H






#
c
c
#
,
,
@@
cc
cc

## 
, 

, 
## 








,@ 

J
@
@@


@

, 

@

 T JJ
J









J

 J
A

B0

D0

J

K


 TJ JJ



....

F

....

C0

B0

C

E
L

A

D0

G H
....

E

K L
....

F

....

G H
....

J

J

Figure 3: Hierarchical redistribution: the left subfigure indicates that cluster J has just
been removed as a descendent of D and B , thus producing D0 and B 0 , and is
about to be resorted relative to the children of the root (A). The rightmost figure
shows J has been placed as a new child of C . From Fisher (1995). Figure reproduced with permission from Proceedings of the First International Conference on
c 1995 American Association
Knowledge Discovery in Data Mining, Copyright 
for Artificial Intelligence.
The Isodata algorithm (Duda & Hart, 1973) determines a target cluster for each observation, but does not actually change the clustering until targets for all observations have
been determined; at this point, all observations are moved to their targets, thus altering
the clustering. We limit ourselves to a sequential version, also described by Duda and Hart
(1973), that moves each observation as its target is identified through sorting.
This strategy is conceptually simple, but is limited in its ability to overcome local
maxima { the reclassification of a particular observation may be in the true direction of a
better clustering, but it may not be perceived as such when the objective function is applied
to the clustering that results from resorting the single observation.

3.3 Iterative Hierarchical Redistribution

An iterative optimization strategy that appears novel in the clustering literature is iterative
hierarchical redistribution. This strategy is rationalized relative to single-observation iterative redistribution: even though moving a set of observations from one cluster to another
may lead to a better clustering, the movement of any single observation may initially reduce
clustering quality, thus preventing the eventual discovery of the better clustering. In response, hierarchical redistribution considers the movement of observation sets, represented
by existing clusters in a hierarchical clustering.
Given an existing hierarchical clustering, a recursive loop examines sibling clusters in the
hierarchy in a depth-first fashion. For each set of siblings, an inner, iterative loop examines
each sibling, removes it from its current place in the hierarchy (along with its subtree),
and resorts the cluster relative to the entire hierarchy. Removal requires that the various
153

fiFisher

counts of ancestor clusters be decremented. Sorting the removed cluster is done based on
the cluster's probabilistic description, and requires a minor generalization of the procedure
for sorting individual observations: rather than incrementing certain variable value counts
by 1 at a cluster to reect the addition of a new observation, a `host' cluster's variable
value counts are incremented by the corresponding counts of the cluster being classified. A
cluster may return to its original place in the hierarchy, or as Figure 3 illustrates, it may
be sorted to an entirely different location.
The inner loop reclassifies each sibling of a set, and repeats until two consecutive iterations lead to the same set of siblings. The recursive loop then turns its attention to the
children of each of these remaining siblings. Eventually, the individual observations represented by leaves are resorted (relative to the entire hierarchy) until there are no changes
from one iteration to the next. Finally, the recursive loop may be applied to the hierarchy
several times, thus defining an outermost (iterative) loop that terminates when no changes
occur from one pass to the next.
There is one modification to this basic strategy that was implemented for reasons of cost:
if there is no change in a subtree during a pass of the outermost loop through the hierarchy,
then subsequent passes do not attempt to redistribute any clusters in this subtree unless
and until a cluster (from some other location in the hierarchy) is placed in the subtree, thus
changing the subtree's structure. In addition, there are cases where the PU scores obtained
by placing a cluster, C (typically a singleton cluster), in either of two hosts will be the
same. In such cases, the algorithm prefers placement of C in its original host if this is one
of the candidates with the high PU score. This policy avoids infinite loops stemming from
ties in the PU score.
In sum, hierarchical redistribution takes large steps in the search for a better clustering. Similar to macro-operator learners (Iba, 1989) in problem-solving contexts, moving an
observation set or cluster bridges distant points in the clustering space, so that a desirable
change can be made that would not otherwise have been viewed as desirable if redistribution was limited to movement of individual observations. The redistribution of increasingly
smaller, more granular clusters (terminating with individual observations) serves to increasingly refine the clustering.
To a large extent hierarchical redistribution was inspired by Fisher's (1987a) Cobweb
system, which is fundamentally a hierarchical-sort-based strategy. However, Cobweb is
augmented by operators of merging, splitting, and promotion. Merging combines two sibling
clusters in a hierarchical clustering if to do so increases the quality of the partition of which
the clusters are members; splitting can remove a cluster and promote its children to the
next higher partition; a distinct promotion operator can promote an individual cluster to
the next higher level. In fact, these could be regarded as `iterative optimization' operators,
but in keeping with Cobweb's cognitive modeling motivations, the cost of applying them is
`amortized' over time: as many observations are sorted, a cluster may migrate from one part
of the hierarchical clustering to another through the collective and repeated application of
merging, splitting, and promotion. A similar view is expressed by McKusick and Langley
(1991), whose Arachne system differs from Cobweb, in part, by the way that it exploits
the promotion operator. Unfortunately, in Cobweb, and to a lesser extent in Arachne,
merging, splitting, and promotion are applied locally and migration through the hierarchy
is limited in practice. In contrast, hierarchical redistribution resorts each cluster, regardless
154

fiOptimization of Hierarchical Clusterings

of its initial location in the tree, through the root of the entire tree, thus more vigorously
pursuing migration and more globally evaluating the merits of such moves.1
The idea of hierarchical redistribution is also closely related to strategies found in the
Bridger (Reich & Fenves, 1991) and Hierarch (Nevins, 1995) systems. In particular,
Bridger identifies `misplaced' clusters in a hierarchical clustering using a criterion specified,
in part, by a domain expert, whereas hierarchical redistribution simply uses the objective
function. In Bridger each misplaced cluster is removed (together with its subtree), but
the cluster/subtree is not resorted as a single unit; rather, the observations covered by the
cluster are resorted individually. This approach captures, in part, the idea of hierarchical
redistribution, though the resorting of individual observations may not escape local optima
to the same extent as hierarchical redistribution.
Given an existing hierarchical clustering and a new observation, Hierarch conducts a
branch-and-bound search through the clustering, looking for the cluster that `best matches'
the observation. When the best host is found, clusters in the `vicinity' of this best host
are reclassified using branch-and-bound with respect to the entire hierarchy. These clusters
need not be singletons, and their reclassification can spawn other reclassifications until a
termination condition is reached.
It is unclear how Hierarch's procedure scales up to large data sets; the number of experimental trials and the size of test data sets is considerably less than we describe shortly.
Nonetheless, the importance of bridging distant regions of the clustering space by reclassifying observation sets en masse in made explicit. Like Cobweb, Hierarch is incremental,
changes to a hierarchy are triggered along the path that classifies a new observation, and
these changes may move many observations simultaneously, thus `amortizing' the cost of
optimization over time. In contrast, hierarchical redistribution is motivated by a philosophy that sorting (or some other method) can produce a tentative clustering over all the
data quickly, followed by iterative optimization procedures in background that revise the
clustering intermittently. While hierarchical redistribution reects many of the same ideas
implemented in Hierarch, Cobweb, and related systems, it appears novel as an iterative
optimization strategy that is decoupled from any particular initial clustering strategy.

3.4 Comparisons between Iterative Optimization Strategies
This section compares iterative optimization strategies under two experimental conditions.
In the first condition, a random ordering of observations is generated and hierarchically
sorted. Each of the optimization strategies is then applied independently to the resultant
hierarchical clustering. These experiments assume that the primary goal of clustering is to
discover a single-level partitioning of the data that is of optimal quality. Thus, the objective
function score of the first-level partition is taken as the most important dependent variable.
An independent variable is the height of the initially-constructed clustering; this can effect
the granularity of clusters that are used in hierarchical redistribution. A hierarchical clus1. Considering global changes also motivated redistribution of individual observations in Iterate. As
Nevins (1995) notes in commentary on experimental comparisons between of Iterate and Cobweb
(Fisher et al., 1992), even global movement of single observations typically did not perform as well
as local movement of sets of observations simultaneously, as implemented by Cobweb's merging and
splitting operators.

155

fiFisher

sort
Soybean (small)
reorder/resort
(47 obs, 36 vars) iter. redist.
hier. redist.
sort
Soybean (large)
reorder/resort
(307 obs, 36 vars) iter. redist.
hier. redist.
sort
House
reorder/resort
(435 obs, 17 vars) iter. redist.
hier. redist.
sort
Mushroom
reorder/resort
(1000 obs, 23 vars) iter. redist.
hier. redist.

Random
1.53 (0.11)
1.61 (0.02)
1.54 (0.10)
1.60 (0.05)
0.89 (0.08)
0.97 (0.04)
0.92 (0.07)
1.06 (0.02)
1.22 (0.30)
1.66 (0.09)
1.24 (0.28)
1.68 (0.00)
1.10 (0.13)
1.10 (0.08)
1.10 (0.12)
1.27 (0.00)

Similarity
1.08 (0.18)
1.56 (0.08)
1.34 (0.20)
1.50 (0.08)
0.66 (0.14)
0.96 (0.05)
0.84 (0.10)
1.06 (0.01)
0.83 (0.16)
1.57 (0.18)
1.06 (0.19)
1.68 (0.00)
0.73 (0.22)
1.16 (0.08)
0.95 (0.19)
1.24 (0.10)

Table 2: Iterative optimization strategies with initial clusterings generated from sorting random and similarity ordered observations. Tree height is 2. Averages and standard
deviations of PU scores over 20 trials.
tering of height 2 corresponds to a single level partition of the data at depth 1 (the root is
at depth 0), with leaves corresponding to individual observations at depth 2.
In addition to experiments on clusterings derived by sorting random initial orderings,
each redistribution strategy was tested on exceptionally poor initial clusterings generated
by nonrandom orderings. Just as `dissimilarity' orderings lead to good clusterings, `similarity' orderings lead to poor clusterings (Fisher et al., 1992). Intuitively, a similarity ordering
samples observations within the same region of the data description space before sampling
observations from differing regions. The reordering procedure of Section 3.1 is easily modified to produce similarity orderings by ranking each set of siblings in a hierarchical clustering
from least to most probable, and appending rather than interleaving observation lists from
differing clusters as the algorithm pops up the recursive levels. A similarity ordering is
produced by applying this procedure to an initial clustering produced by an earlier sort of
a random ordering. Another clustering is then produced by sorting the similarity-ordered
data, and the three iterative optimization strategies are applied independently. We do not
advocate that one build clusterings from similarity orderings in practice, but experiments
with such orderings better test the robustness of the various optimization strategies.
Table 2 shows the results of experiments with random and similarity orderings of data
from four databases of the UCI repository.2 These results assume an initial clustering of
height 2 (i.e., a top-level partition + observations at leaves). Each cell represents an average
2. A reduced
data set.

mushroom

data set was obtained by randomly selecting 1000 observations from the original

156

fiOptimization of Hierarchical Clusterings

and standard deviation over 20 trials. The first cell (labeled `sort') of each domain is the
mean PU scores initially obtained by sorting. Subsequent rows under each domain reect
the mean scores obtained by the reordering/resorting procedure of Section 3.1, iterative
redistribution of single observations described in Section 3.2, and hierarchical redistribution
described in Section 3.3.
The main findings reected in Table 2 are:
1. Initial hierarchical sorting from random input does reasonably well; PU scores in this
case are closer to the scores of optimized trees, than to the poorest scores obtained
after sorting on similarity orderings. This weakly suggests that initial sorting on
random input takes a substantial step in the space of clusterings towards discovery of
the final structure.
2. Hierarchical redistribution achieves the highest mean PU score in both the random
and similarity case in 3 of the 4 domains. The small soybean domain is the exception.
3. In the House domain (random and similarity case) and the Mushroom domain (random
case only), the standard deviation in PU scores of clusterings optimized by hierarchical
redistribution is 0.00, indicating that it has always constructed level-1 partitions of
the same PU score in all 20 trials.
4. Reordering and reclustering comes closest to hierarchical redistribution's performance
in all cases, bettering it in the Small Soybean domain.
5. Single-observation redistribution modestly improves an initial sort, and is substantially worse than the other two optimization methods.
Note that with initial hierarchical clusterings of height 2, the only difference between
iterative hierarchical redistribution and redistribution of single observations is that hierarchical redistribution considers `merging' clusters of the partition (by reclassifying one with
respect to the others) prior to redistributing single observations during each pass through
the hierarchy.
Section 3.3 suggested that the expected benefits of hierarchical redistribution might
be greater for deeper initial trees with more granular clusters. Table 3 shows results on
the same domains and initial orderings when tree height is 4 for hierarchical redistribution; for the reader's convenience we also repeat the results from Table 2 for hierarchical
redistribution when tree height is 2. In moving from height 2 to 4, there is modest improvement in the small Soybean domain (particularly under Similarity orderings), and very
slight improvement in the large Soybean domain and the Mushroom domain under Similarity orderings.3 While the improvements are very modest, moving to height 4 trees leads to
near identical performance in the random and similarity ordering conditions. This suggests
that hierarchical redistribution is able to effectively overcome the disadvantage of initially
poor clusterings.
Experiments with reorder/resort and iterative distribution of single observations also
were varied with respect to tree height (e.g., height 3). For each of these methods, the
3. A standard deviation of 0:00 indicates that the standard deviation was non-0, but not observable at the
2nd decimal place after rounding.

157

fiFisher

Random
height 2
height 4
Soybean (small) 1.60 (0.05) 1.62 (0.00)
Soybean (large) 1.06 (0.02) 1.07 (0.02)
House
1.68 (0.00) 1.68 (0:00)
Mushroom
1.27 (0.00) 1.27 (0.00)

Similarity
height 2
height 4
1.50 (0.08) 1.62 (0.00)
1.06 (0.01) 1.07 (0.01)
1.68 (0.00) 1.68 (0:00)
1.24 (0.10) 1.27 (0.00)

Table 3: Hierarchical redistribution with initial clusterings generated from sorting random
and similarity ordered observations. Results are shown for tree heights of 2 (copied
from Table 2) and 4. Averages and standard deviations of PU scores over 20 trials.
deepest set of clusters in the initial hierarchy above the leaves, was taken as the initial
partition. Reordering/resorting scores remained roughly the same as the height 2 condition, but clusterings produced by single-observation redistribution had PU scores that were
considerably worse than those given in Table 2.
We also recorded execution time for each method. Table 4 shows the time required
for each method in seconds.4 In particular, for each domain, Table 4 lists the mean time
for initial sorting, and the mean additional time for each optimization method. Ironically,
these experiments demonstrate that even though hierarchical redistribution `bottoms-out' in
a single-observation form of redistribution, the former is consistently faster than the latter
for trees of height 2 { reclassifying a cluster simultaneously moves a set of observations,
which would otherwise have to be repeatedly evaluated for redistribution individually with
increased time to stabilization.5
Table 4 assumes the tree constructed by initial sorting is bounded to height 2. Table 5
gives the time requirements of hierarchical sorting and hierarchical redistribution when
the initial tree is bounded to height 4. As the tree gets deeper the cost of hierarchical
redistribution grows substantially, and as our comparison of performance with height 2 and
4 trees in Table 3 suggests, there are drastically diminishing returns in terms of partition
quality. Importantly, limited experiments with trees of height 2, 3, and 4 indicate that the
cost of hierarchical redistribution is comparable to the cost of reorder/resort at greater tree
heights and significantly less expensive than single-observation redistribution. It is dicult
to give a cost analysis of hierarchical redistribution (and the other methods for that matter),
since bounds on loop iterations probably depend on the nature of the objective function.
Suce it to say that the number of nodes that are subject to hierarchical redistribution in
a tree covering n observations is bounded above by 2n , 1; there may be up to n leaves and
up to n , 1 internal nodes given that each internal node has no less than 2 children.
If iterative optimization is to occur in background, real-time response is not important,
and cluster quality is paramount, then it is probably worth applying hierarchical redis4. Routines were implemented in SUN Common Lisp, compiled, and run on a SUN 3/60.
5. Similar timing results occur in other computational contexts as well. Consider the relation between
insertion sort and Shell sort. Shell sort's final `pass' of a table is an insertion sort that is limited to
moving table elements between consecutive table locations at a time. The large eciency advantage of
Shell Sort stems from the fact that previous passes of the table have moved elements large distances,
thus by the final pass, the table is nearly sorted.

158

fiOptimization of Hierarchical Clusterings

sort
Soybean (small)
reorder/resort
(47 obs, 36 vars)
iter. redist.
hier. redist.
sort
Soybean (large)
reorder/resort
(307 obs, 36 vars) iter. redist.
hier. redist.
sort
House
reorder/resort
(435 obs, 17 vars) iter. redist.
hier. redist.
sort
Mushroom
reorder/resort
(1000 obs, 23 vars) iter. redist.
hier. redist.

Random
6.98 (1.43)
14.82 (2.60)
9.00 (5.94)
6.99 (1.28)
50.62 (6.11)
141.36 (46.99)
166.53 (55.53)
79.00 (19.23)
34.99 (7.55)
87.78 (23.94)
177.75 (94.53)
55.90 (11.92)
111.47 (19.19)
301.34 (100.56)
162.58 (85.20)
91.87 (29.50)

Similarity
7.21 (1.31)
18.27 (6.00)
15.51 (7.72)
8.87 (3.58)
54.09 (13.25)
153.22 (43.59)
307.59 (160.66)
87.27 (19.64)
39.15 (7.60)
97.63 (29.54)
320.43 (124.78)
73.54 (10.05)
119.33 (25.86)
391.80 (211.54)
390.11 (191.62)
151.45 (48.89)

Table 4: Time requirements (in seconds) of hierarchical sorting and iterative optimization
with initial clusterings generated from sorting random and similarity ordered observations. Tree height is 2. Averages and standard deviations over 20 trials.
tribution to deeper trees; this is consistent with the philosophy behind such systems as
Autoclass and Snob. For the domains examined here, however, it does not seem cost
effective to optimize with trees of height greater than 4. Thus, we adopt a tree construction strategy that builds a hierarchical clustering three levels at a time (with hierarchical
redistribution) in the experiments of Section 4.

3.5 Discussion of Iterative Optimization Methods
Our experiments demonstrate the relative abilities of three iterative optimization strategies,
which have been coupled with the PU objective function and hierarchical sorting to generate
initial clusterings. The reorder/resort optimization strategy of Section 3.1 makes most
sense with sorting as the primary clustering strategy, but the other optimization techniques
are not strongly tied to a particular initial clustering strategy. For example, hierarchical
redistribution can also be applied to hierarchical clusterings generated by an agglomerative
strategy (Duda & Hart, 1973; Everitt, 1981; Fisher et al., 1992), which uses a bottom-up
procedure to construct hierarchical clusterings by repeatedly `merging' observations and
resulting clusters until an all-inclusive root cluster is generated. Agglomerative methods do
not suffer from ordering effects, but they are greedy algorithms, which are susceptible to the
limitations of local decision making generally, and would thus likely benefit from iterative
optimization.
159

fiFisher

Soy (small) sort
hier.
Soy (large) sort
hier.
House
sort
hier.
Mushroom sort
hier.

redist.
redist.
redist.
redist.

Random
height 2
height 4
6.98 (1.4)
18 (2)
6.99 (1.3)
94 (28)
50.62 (6.1) 142 (10)
79.00 (19.2) 436 (139)
34.99 (7.6) 104 (9)
55.90 (11.9) 355 (71)
111.47 (19.2) 407 (64)
91.87 (29.5) 1288 (458)

Similarity
height 2
height 4
7.21 (1.3)
21 (2)
8.87 (3.6)
133 (28)
54.09 (13.3) 152 (11)
87.27 (19.6) 576 (260)
39.15 (7.6) 120 (12)
73.54 (10.1) 425 (105)
119.33 (25.9) 443 (65)
151.45 (48.9) 1368 (335)

Table 5: Time requirements (in seconds) of hierarchical sorting and hierarchical redistribution with initial clusterings generated from sorting random and similarity ordered
observations. Results are shown for tree heights of 2 (copied from Table 4) and 4.
Averages and standard deviations over 20 trials.
In addition, all three optimization strategies can be applied regardless of objective function. Nonetheless, the relative benefits of these methods undoubtedly varies with objective function. For example, the PU function has the undesirable characteristic that it
may, under very particular circumstances, view two partitions that are very close in form
as separated by a `cliff' (Fisher, 1987b; Fisher et al., 1992). Consider a partition of M
observations involving
only two, roughly equal-sized clusters; its PU score has the form
P
2
PU (fC1; C2g) = [ k=1 CU (Ck )]=2. If we create a partition of three clusters by removing a single observation
from, say C2, and creating a new singleton cluster, C3 we have
P
3
0
PU (fC1; C2; C3g) = [ k=1 CU (Ck )]=3. If M is relatively large, CU (C3) will have a very
small score due to the term, P (C3) = 1=M (see Section 2.1). Because we are taking the
average CU score of clusters, the difference between PU (fC1; C2g) and PU (fC1; C20 ; C3g)
may be quite large, even though they differ in the placement of only one observation. Thus,
limiting experiments to the PU function may exaggerate the general advantage of hierarchical redistribution relative to the other two optimization methods. This statement is
simultaneously a positive statement about the robustness of hierarchical redistribution in
the face of an objective function with cliffs, and a negative statement about PU for defining
such discontinuities. Nonetheless, PU and variants have been adopted in systems that fall
within the Cobweb family (Gennari et al., 1989; McKusick & Thompson, 1990; Reich &
Fenves, 1991; Iba & Gennari, 1991; McKusick & Langley, 1991; Kilander, 1994; Ketterlin
et al., 1995; Biswas et al., 1994). Section 5.2 suggests some alternative objective functions.
Beyond the nonoptimality of PU , our findings should not be taken as the best that
these strategies can do when they are engineered for a particular clustering system. We
could introduce forms of randomization or systematic variation to any of the three strategies. For example, while Michalski and Stepp's seed-selection methodology inspires reordering/resorting, Michalski and Stepp's approach selects `border' observations when the
selection of `centroids' fails to improve clustering quality from one iteration to the next;
160

fiOptimization of Hierarchical Clusterings

this is an example of the kind of systematic variations that one might introduce in pursuit
of better clusterings. In contrast, Autoclass may take large heuristically-guided `jumps'
away from a current clustering. This approach might be, in fact, a somewhat less systematic
(but equally successful) variation on the macro-operator theme that inspired hierarchical redistribution, and is similar to Hierarch's approach as well. Snob (Wallace & Dowe, 1994)
employs a variety of search operators, including operators similar to Cobweb's merge and
split (though without the same restrictions on local application), random restart of the clustering process with new seed observations, and `redistribution' of observations.6 In fact, the
user can program Snob's search strategy using these differing primitive search operators.
In any case, systems such as Cluster/2, Autoclass, and Snob do not simply `give up'
when they fail to improve clustering quality from one iteration to the next.
As Snob illustrates, one or more strategies might be combined to advantage. As an
additional example, Biswas et al. (1994) adapt Fisher, Xu, and Zard's (1992) dissimilarity
ordering strategy to preorder observations prior to clustering. After sorting using PU , their
Iterate system then applies iterative redistribution of single observations using a category
match measure by Fisher and Langley (1990).
The combination of preordering and iterative redistribution appears to yield good results
in Iterate. Our results with reorder/resort suggest that preordering is primarily responsible for quality benefits over a simple sort, but the relative contribution of Iterate's
redistribution operator is not certain since it differs in some respects from the redistribution technique described in this paper.7 However, the use of three different measures {
distance, PU , and category match { during clustering may be unnecessary and adds undesirable coupling in the design of the clustering algorithm. If, for example, one wants
to experiment with the merits of differing objective functions, it is undesirable to worry
about the `compatibility' of this function with two other measures. In contrast, reordering/resorting generalizes Fisher et al.'s (1992) ordering strategy; this generalization and the
iterative redistribution strategy we describe assume no auxiliary measures beyond the objective function. In fact, as in Fisher (1987a, 1987b), an evaluation of Iterate's clusterings
is made using measures of variable value predictability or P (Ai = Vij jCk ), predictiveness or
P (Ck jAi = Vij ), and their product. It is not clear that a system need exploit several related,
albeit different measures during the generation and evaluation of clusterings; undoubtedly
a single, carefully selected objective function can be used exclusively during clustering.
Reordering/resorting and iterative redistribution of single observations could be combined in a manner similar to Iterate's exploitation of certain specializations of these
procedures. Our results suggest that reordering/resorting would put a clustering in a good
`ballpark', while iterative redistribution would subsequently make modest refinements. We
have not combined strategies, but in a sense conducted the inverse of an `ablation' study,
by evaluating individual strategies in isolation. In the limited number of domains explored
in Section 3.4, however, it appears dicult to better hierarchical redistribution.
Finally, our experiments applied various optimization techniques after all data was
sorted. It may be desirable to apply the optimization procedures at intermittent points
during sorting. This may improve the quality of final clusterings using reordering/resorting
6. Importantly, Snob (and Autoclass) assumes probabilistic assignment of observations to clusters.
7. Iterate uses a measure for redistribution (Fisher & Langley, 1990) that probably smoothes `cliffs', and
it uses an Isodata, non-sequential version of redistribution.

161

fiFisher

and redistribution of single observations, as well as reduce the overall cost of constructing
final optimized clusterings using any of the methods, including hierarchical redistribution,
which already appears to do quite well on the quality dimension. In fact, Hierarch can be
viewed as performing something akin to a restricted form of hierarchical redistribution after
each observation. This is probably too extreme { if iterative optimization is performed too
often, the resultant cost can outweigh any savings gleaned by maintaining relatively well
optimized trees throughout the sorting process. Utgoff (1994) makes a similar suggestion
for intermittent restructuring of decision trees during incremental, supervised induction.

4. Simplifying Hierarchical Clusterings

A hierarchical clustering can be grown to arbitrary height. If there is structure in the data,
then ideally the top layers of the clustering reect this structure (and substructure as one
descends the hierarchy). However, lower levels of the clustering may not reect meaningful
structure. This is the result of overfitting, which one finds in supervised induction as well.
Inspired by certain forms of retrospective (or post-tree-construction) pruning in decisiontree induction, we use resampling to identify `frontiers' of a hierarchical clustering that
are good candidates for pruning. Following initial hierarchy construction and iterative
optimization, this simplification process is a final phase of search through the space of
hierarchical clusterings intended to ease the burden of a data analyst.

4.1 Identifying Variable Frontiers by Resampling

Several authors (Fisher, 1987a; Cheeseman et al., 1988; Anderson & Matessa, 1991) motivate clustering as a means of improving performance on a task akin to pattern completion,
where the error rate over completed patterns can be used to `externally' judge the utility of
a clustering. Given a probablistic categorization tree of the type we have assumed, a new
observation with an unknown value for a variable can be classified down the hierarchy using
a small variation on the hierarchical sorting procedure described earlier.8 Classification is
terminated at a selected node (cluster) along the classification path, and the variable value
of highest probability at that cluster is predicted as the unknown variable value of the new
observation. Naively, classification might always terminate at a leaf (i.e., an observation),
and the leaf's value along the specified variable would be predicted as the variable value
of the new observation. Our use of a simple resampling strategy known as holdout (Weiss
& Kulikowski, 1991) is motivated by the fact that a variable might be better predicted at
some internal node in the classification path. The identification of ideal-prediction frontiers
for each variable suggests a pruning strategy for hierarchical clusterings.
Given a hierarchical clustering and a validation set of observations, the validation set is
used to identify an appropriate frontier of clusters for prediction of each variable. Figure 4
illustrates that the preferred frontiers of any two variables may differ, and clusters within a
frontier may be at different depths. For each variable, Ai , the objects from the validation
set are each classified through the hierarchical clustering with the value of variable Ai
`masked' for purposes of classification; at each cluster encountered during classification the
8. Classification is identical to sorting except that the observation is not added to the clustering and
statistics at each node encountered during sorting are not permanently updated to reect the new
observation.

162

fiOptimization of Hierarchical Clusterings

frontier of A1
of A2
of A3
....

....

Figure 4: Frontiers for three variables in a hypothetical clustering. From Fisher (1995).
Figure reproduced with permission from Proceedings of the First International
c 1995 American
Conference on Knowledge Discovery in Data Mining, Copyright 
Association for Artificial Intelligence.
observation's value for Ai is compared to the most probable value for Ai at the cluster;
if they are the same, then the observation's value would have been correctly predicted
at the cluster. A count of all such correct predictions for each variable at a cluster is
maintained. Following classification for all variables over all observations of the validation
set, a preferred frontier for each variable is identified that maximizes the number of correct
counts for the variable. This is a simple, bottom-up procedure that insures that the number
of correct counts at a node on the variable's frontier is greater than or equal to the sum of
correct counts for the variable over each set of mutually-exclusive, collectively-exhaustive
descendents of the node.
Variable-specific frontiers enable a number of pruning strategies. For example, a node
that lies below the frontier of every variable offers no apparent advantage in terms of patterncompletion error rate; such a node probably reects no meaningful structure and it (and
its descendents) may be pruned. However, if an analyst is focusing attention on a subset of
the variables, then frontiers might be more exibly exploited for pruning.

4.2 Experiments with Validation
To test the validation procedure's promise for simplifying hierarchical clusterings, each of
the data sets used in the optimization experiments of Section 3.4 was randomly divided
into three subsets: 40% for training, 40% for validation, and 20% for test. A hierarchical
clustering is first constructed by sorting the training set in randomized order. This hierarchy is then optimized using iterative hierarchical redistribution. Actually, because of cost
considerations, a hierarchy is constructed several levels at a time. The hierarchy is initially
constructed to height 4, where the deepest level is the set of training observations. This
hierarchy is optimized using hierarchical redistribution. Clusters at the bottommost level
(i.e., 4) are removed as children of level 3 clusters, and the subset of training observations
163

fiFisher

Soybean (small)
Leaves
Accuracy
Ave. Frontier Size
Soybean (large)
Leaves
Accuracy
Ave. Frontier Size
House
Leaves
Accuracy
Ave. Frontier Size
Mushroom
Leaves
Accuracy
Ave. Frontier Size

Unvalidated

Validated

18.00 (0.00)
0.85 (0.01)
18.00 (0.00)

13.10 (1.59)
0.85 (0.01)
2.75 (1.17)

122.00 (0.00) 79.10 (5.80)
0.83 (0.02) 0.83 (0.02)
122.00 (0.00) 17.01 (4.75)
174.00 (0.00) 49.10 (7.18)
0.76 (0.02) 0.81 (0.01)
174.00 (0.00) 9.90 (5.16)
400.00 (0.00) 96.30 (11.79)
0.80 (0.01) 0.82 (0.01)
400.00 (0.00) 11.07 (4.28)

Table 6: Characteristics of optimized clusterings before and after validation. Average and
standard deviations over 20 trials.
covered by each cluster of level 3 is hierarchically sorted to a height 4 tree and optimized.
The roots of these subordinate clusterings are then substituted for each cluster at depth
3 in the original tree. The process is repeated on clusters at level 3 of the subordinate
trees and subsequent trees thereafter until no further decomposition is possible. The final
hierarchy, which is not of constant-bounded height, decomposes the entire training set to
singleton clusters, each containing a single training observation. The validation set is then
used to identify variable frontiers within the entire hierarchy.
During testing of a validated clustering, each variable of each test observation is masked
in turn; when classification reaches a cluster on the frontier of the masked variable, the
most probable value is predicted as the value of the observation; the proportion of correct
predictions for each variable over the test set is recorded. For comparative purposes, we
also use the test set to evaluate predictions stemming from the unvalidated tree, where all
variable predictions are made at the leaves (singleton clusters) of this tree.
Table 6 shows results from 20 experimental trials using optimized, unvalidated and
validated clusterings generated as just described from random orderings. The first row
of each domain lists the average number of leaves (over the 20 experimental trials) for the
unvalidated and validated trees. The unvalidated clusterings decompose the training data to
single-observation leaves { the number of leaves equals the number of training observations.
In the validated clustering, we assume that clusters are pruned if they lie below the frontiers
of all variables. Thus, a leaf in a validated clustering is a cluster (in the original clustering)
that is on the frontier of at least one variable, and none of its descendent clusters (in the
original clustering) are on the frontier of any variable. For example, if we assume that the
164

fiOptimization of Hierarchical Clusterings

tree of Figure 4 covers data described only in terms of variables A1 , A2, and A3, then the
number of leaves in this validated clustering would be 7.
Prediction accuracies in the second row of each domain entry are the mean proportion
of correct predictions over all variables over 20 trials. Predictions were generated at leaves
(singleton clusters) in the unvalidated hierarchical clusterings and at appropriate variable
frontiers in the validated clusterings. In all cases, validation/pruning substantially reduces
clustering size and it does not diminish accuracy.
The number of leaves in the validated case, as we have described it, assumes a very
coarse pruning strategy; it will not necessarily discriminate a clustering with uniformly
deep frontiers from one with a single or very few deep frontiers. We have suggested that
more exible pruning or `attention' strategies might be possible when an analyst is focusing
on one or a few variables. We will not specify such strategies, but the statistic given in row
3 of each domain entry suggests that clusterings can be rendered in considerably simpler
forms when an analyst's attention is selective. Row 3 is the average number of frontier
clusters per variable. This is an average over all variables and all experimental trials.9 In
the validated tree of Figure 4 the average frontier size is (1 + 4 + 6)=3 = 3:67.
Intuitively, a frontier cluster of a variable is a `leaf' as far as prediction of that variable
is concerned. The `frontier size' for unvalidated clusterings is simply given by the number
of leaves, since this is where all variable predictions are made in the unvalidated case.
Our results suggest that when attention is selective, a partial clustering that captures the
structure involving selected variables can be presented to an analyst in very simplified form.

4.3 Discussion of Validation

The resampling-based validation method is inspired by earlier work by Fisher (1989), which
identified variable frontiers within a strict incremental (i.e., sorting) context { no separate
validation set was reserved, but rather the training set was used for identifying variable
frontiers as well. In particular, as each training observation was hierarchically sorted using
Cobweb, each of the observation's variable values were predicted and `correct' counts at
each node were updated for all correctly anticipated variables. In Fisher (1989) variable
values were not masked during sorting { knowledge of each variable value was used during
sorting, thus helping to guide classification, and validation. In addition, the hierarchy
changed during sorting/validation. While this incremental strategy led to desirable results
in terms of pattern-completion error rate, it is likely that the variable frontiers identified
by the incremental method are less desirable than frontiers identified with holdout, where
we strictly segregate the training and validation sets of observations. In addition to Fisher
(1989), our work on variable frontiers can be traced back to ideas by Lebowitz (1982) and
Kolodner (1983), and more directly to Fisher (1987b), Fisher and Schlimmer (1988), and
Reich and Fenves (1991), each of which use a very different method to identify something
similar in spirit to frontiers as defined here.
Our method of validation and pruning is inspired by retrospective pruning strategies
in decision tree induction such as reduced error pruning (Quinlan, 1987, 1993; Mingers,
1989a). In a Bayesian clustering system such as Autoclass (Cheeseman et al., 1988), or
9. The `standard deviations' given in Row 3 are actually the mean of the standard deviations over the
frontier sizes for individual variables.

165

fiFisher

the minimum message length (MML) approach adopted by Snob (Wallace & Dowe, 1994),
the expansion of a hierarchical clustering is mediated by a tradeoff between prior belief in
the existence of further structure and evidence in the data for further structure. We will
not detail this fundamental tradeoff, but suce it to say that expansion of a hierarchical
clustering will cease along a path when the evidence for further structure in the data is
insucient in the face of prior bias. Undoubtedly, the Bayesian and MML approaches can
be adapted to identify variable-specific frontiers, and thus be used in the kind of exible
pruning and focusing strategies that we have implied. In fact, something very similar in
intent has been implemented in Autoclass (Hanson, Stutz, & Cheeseman, 1991) as a way of
reducing the cost of clustering with this system: variables that covary may be `blocked', or
in some sense treated as one. This version of Autoclass searches a space of hierarchical
clusterings, with blocks of variables assigned to particular clusters in the hierarchy. The
interpretation of such assignments is that a cluster `inherits' the variable value distributions
of variable blocks assigned to the cluster's ancestors. Inversely, the basic idea is that one
need not proceed below a cluster to determine the value distributions of variables assigned
to that cluster.
Our experimental results suggest the utility of resampling for validation, the identification of variable frontiers, and pruning. However, the procedure described is not a method
per se of clustering over all the available data, since it requires that a validation set be held
out during initial hierarchy construction.10 There are several options that seem worthy of
experimental evaluation in adapting this validation strategy as a tool for simplification of
hierarchical clusterings. One strategy would be to hold out a validation set, cluster over a
training set, identify variable frontiers with the validation set, and then sort the validation
set relative to the clustering. This single holdout methodology has its problems, however,
for reasons similar to those identified for single holdout in supervised settings (Weiss &
Kulikowski, 1991).
A better strategy might be one akin to n-fold-cross-validation: a hierarchical clustering
is constructed over all available data, then each observation is removed,11 it is used for
validation with respect to each variable, and then the observation is reinstated in its original
location (together with the original variable value statistics of clusters along the path to
this location).

5. General Discussion
The evaluation of the various strategies discussed in this paper reect two paradigms for
validating clusterings. Internal validation is concerned with evaluating the merits of the
control strategy that searches the space of clusterings: evaluating the extent that the search
strategy uncovers clusterings of high quality as measured by the objective function. Internal
validation was the focus of Section 3.4. External validation is concerned with determining
the utility of a discovered clustering relative to some performance task. We have noted
10. For purposes of evaluating the merits of our validation strategy in terms of error rate, we also held out
a separate test set. Having demonstrated the point, however, we would not require that a separate test
set be held out when using resampling as a validation strategy.
11. The observation is physically removed, and variable value statistics at clusters that lie along the path
from root to the observation are decremented.

166

fiOptimization of Hierarchical Clusterings

Soybean (small)
Leaves
Accuracy
Ave. Frontier Size
Soybean (large)
Leaves
Accuracy
Ave. Frontier Size
House
Leaves
Accuracy
Ave. Frontier Size
Mushroom
Leaves
Accuracy
Ave. Frontier Size

Unoptimized
Unvalidated Validated

Optimized
Unvalidated Validated

18.00 (0.00)
0.84 (0.18)
18.00 (0.00)

18.00 (0.00)
0.85 (0.01)
18.00 (0.00)

15.35 (1.81)
0.85 (0.01)
3.97 (1.62)

13.10 (1.59)
0.85 (0.01)
2.75 (1.17)

122.00 (0.00) 88.55 (4.46)
0.82 (0.02) 0.82 (0.02)
122.00 (0.00) 24.74 (7.52)

122.00 (0.00) 79.10 (5.80)
0.83 (0.02) 0.83 (0.02)
122.00 (0.00) 17.01 (4.75)

174.00 (0.00) 68.95 (8.15)
0.76 (0.02) 0.81 (0.02)
174.00 (0.00) 17.72 (7.81)

174.00 (0.00) 49.10 (7.18)
0.76 (0.02) 0.81 (0.01)
174.00 (0.00) 9.90 (5.16)

400.00 (0.00) 145.50 (20.64) 400.00 (0.00) 96.30 (11.79)
0.80 (0.01) 0.82 (0.01)
0.80 (0.01) 0.82 (0.01)
400.00 (0.00) 22.85 (8.75)
400.00 (0.00) 11.07 (4.28)

Table 7: Characteristics of unoptimized and optimized clusterings before and after validation. Average and standard deviations over 20 trials.
that several authors point to minimization of error rate in pattern completion as a generic
performance task that motivates their choice of objective function. External validation was
the focus of Section 4.2.
This section explores validation issues more closely, identifies both error rate and simplicity (or `cost') as necessary external criteria for discriminating clustering utility, suggests
a number of alternative objective functions that might be usefully compared using these
criteria, and speculates that these external validation criteria (taken collectively) reect
reasonable criteria that data analysts may use to judge the utility of clusterings.

5.1 A Closer Look at External Validation Criteria
Ideally, clustering quality as measured by the objective function should be well correlated
with clustering utility as determined by a performance task: the higher the quality of a
clustering as judged by the objective function, the greater the performance improvement
(e.g., reduction of error rate), and the lower the quality, the less that performance improves.
However, several authors (Fisher et al., 1992; Nevins, 1995; Devaney & Ram, 1993) have
pointed out that PU scores do not seem well-correlated with error rates. More precisely,
hierarchical clusterings (constructed by hierarchical sorting) in which the top-level partition
has a low PU score lead to roughly the same error rates as hierarchies in which the top-level
partition has a high PU score, when variable-value predictions are made at leaves (singleton
clusters). Apparently, even with poor partitions at each level as measured by PU , test
167

fiFisher

observations are classified to the same or similar observations at the leaves of a hierarchical
clustering. Pattern-completion error rate under these circumstances seems insucient to
discriminate what we might otherwise consider to be good and poor clusterings.
Our work on simplification in Section 4 suggests that in addition to error rate, we might
choose to judge competing hierarchical clusterings based on simplicity or some similarlyintended criterion. Both error rate and simplicity are used to judge classifiers in supervised
contexts. We have seen that holdout can be used to substantially `simplify' a hierarchical
clustering. The question we now ask is whether hierarchical clusterings that have been
optimized relative to PU can be simplified more substantially than unoptimized clusterings
with no degradation in pattern-completion error rate?
To answer this question we repeated the validation experiments of Section 4.2 under
a second experimental condition: hierarchical clusterings were constructed from similarity
orderings of the observations using hierarchical sorting. We saw in Section 3.4 that similarity
orderings tend to result in clusterings judged poor by the PU function. We do not optimize
these hierarchies using hierarchical optimization. Table 7 shows accuracies, number of
leaves, and average frontier sizes, for unoptimized hierarchies constructed from similarity
orderings in the case where they have been subjected to holdout-based validation and in
the case where they have not. These results are given under the heading `Unoptimized'.
For convenience, we copy the results of Table 6 under the heading `Optimized'.
As in the optimized case, identifying and exploiting variable frontiers in unoptimized
clusterings appears to simplify a clustering substantially with no degradation in error rate.
Of most interest here, however, is that optimized clusterings are simplified to a substantially
greater extent than unoptimized clusterings with no degradation in error rate.
Thus far, we have focused an the criteria of error rate and simplicity, but in many
applications, our real interest in simplicity stems from a broader interest in minimizing
the expected cost of exploiting a clustering during classification: we expect that simpler
clusterings have lower expected classification costs. We can view the various distinctions
between unvalidated/validated and unoptimized/optimized clusterings in terms of expected
classification cost. Table 8 shows some additional data obtained from our experiments with
validation. In particular, the table shows:

Leaves (L) The mean number of leaves (over 20 trials) before and after validation (assuming the coarse pruning strategy described in Section 4.2) in both the optimized and
unoptimized cases (copied from Table 7).

EPL The mean total path length (over 20 trials). The total path length of an unvalidated

tree, where each leaf corresponds to a single observation, is the sum of depths of
leaves in the tree. In the case of a validated tree, where a leaf may cover multiple
observations, the contribution of the leaf to the total path length is the depth of the
leaf times the number of observations at that leaf.

Depth (D) The average depth of a leaf in the tree, which is computed by EPL
L .

p
Breadth (B)log The
average branching factor of the tree. Given that B D = L, B = D L or
L
B=m

m
D

for any m.

168

fiOptimization of Hierarchical Clusterings

Soybean (small)
Leaves
EPL
Depth
Breadth
Cost
Soybean (large)
Leaves
EPL
Depth
Breadth
Cost
House
Leaves
EPL
Depth
Breadth
Cost
Mushroom
Leaves
EPL
Depth
Breadth
Cost

Unoptimized
Unvalidated
Validated

Optimized
Unvalidated
Validated

18.00 (0.00)
40.90 (3.64)
2.27
3.57
8.10

15.35 (1.81)
31.90 (6.94)
2.08
3.72
7.74

18.00 (0.00)
54.20 (4.74)
3.01
2.61
7.86

13.10 (1.59)
34.50 (6.49)
2.63
2.66
7.00

122.00 (0.00)
437.20 (34.74)
3.58
3.82
13.68

88.55 (4.46)
280.40 (28.07)
3.17
4.11
13.03

122.00 (0.00)
657.65 (28.38)
5.39
2.44
13.15

79.10 (5.80)
380.65 (43.63)
4.81
2.48
11.93

174.00 (0.00)
664.65 (41.16)
3.82
3.86
14.75

68.95 (8.15)
196.20 (35.32)
2.85
4.42
12.60

174.00 (0.00)
1005.10 (27.42)
5.78
2.44
14.10

49.10 (7.18)
217.25 (39.75)
4.42
2.41
10.65

400.00 (0.00)
2238.20 (123.63)
5.60
2.92
16.35

145.50 (20.64)
660.90 (117.86)
4.54
3.00
13.62

400.00 (0.00)
2608.85 (56.01)
6.52
2.51
16.37

96.30 (11.79)
503.40 (72.22)
5.23
2.39
12.50

Table 8: Cost characteristics of unoptimized and optimized clustering before and after validation. Average and standard deviations over 20 trials. Characteristics that are
 ed are computed from the mean values of `Leaves' and EPL.

Cost (C) The expected cost of classifying an observation from the root to a leaf in terms

of the number of nodes (clusters) examined during classification. At each level we
examine each cluster and select the best. Thus, cost is the product of the number of
levels and the number of clusters per level. So C = B  D.
Table 8 illustrates that the expected cost of classification is less for optimized clusterings
than for unoptimized clusterings in both the unvalidated and validated cases. However,
these results should be taken with a grain of salt, and not simply because they are estimated
values. In particular, we have expressed cost in terms of the expected number of nodes that
would need to be examined during classification. An implicit assumption is that cost of
examination is constant across nodes. In fact, the cost per examination roughly is constant
(per domain) across nodes in our implementation and many others: at each node, all
variables are examined. Consider that by this measure of cost, the least cost (unvalidated)
169

fiFisher

clustering is one that splits the observations in thirds at each node, thus forming a balanced
ternary tree, regardless of the form of structure in the data.
Of course, if such a tree does not reasonably capture structure in data, then we might
expect this to be reected in error rate and/or post-validation simplicity. Nonetheless, there
are probably better measures of cost available. In particular, Gennari (1989) observed that
when classifying an observation, evaluating the objective function over a proper subset
of variables is often sucient to categorize the observation relative to the same cluster
that would have been selected if evaluation had occurred over all variables. Under ideal
circumstances, when clusters of a partition are well separated (decoupled), testing a very
few `critical' variables may be sucient to advance classification.
Gennari implemented a focusing algorithm that sequentially evaluated the objective
function over the variables, one additional variable at a time from most to least `critical',
until a categorization with respect to one of the clusters could be made unambiguously.
Using Gennari's procedure, examination cost is not constant across nodes.12 Carnes and
Fisher (Fisher, Xu, Carnes, Reich, Fenves, Chen, Shiavi, Biswas, & Weinberg, 1993) adapted
Gennari's procedure to good effect in a diagnosis task, where the intent was to minimize
the number of probes necessary to diagnose a fault. While Gennari offers a principled
focusing strategy that can be used in conjunction with an objective function, the general
idea of focusing on selected features during classification can be traced back to Unimem
(Lebowitz, 1982, 1987) and Cyrus (Kolodner, 1983).
The results of Table 8 illustrate the form of an expected classification-cost analysis, but
we might have also measured cost as time directly using a test set. In fact, comparisons
between the time requirements of sorting in the random and similarity ordering conditions
of Tables 4 and 5 suggest cost differences between good and poor clusterings in terms
of time as well. Regardless of the form of analysis, however, it seems desirable that one
express branching factor and cost in terms of the number of variables that need be tested
assuming a focusing strategy such as Gennari's. It is likely that this will tend to make
better distinctions between clusterings.

5.2 Evaluating Objective Functions: Getting the Most Bang for the Buck

The results of Section 5.1 suggest that the PU function is useful in identifying structure
in data: clusterings optimized relative to this function were simpler and as accurate as
clusterings that were not optimized relative to the function. Thus, PU leads to something
reasonable along the error rate and simplicity dimensions, but can other objective functions
do a better job along these dimensions? Based on earlier discussion on the limitations of
PU , notably that averaging CU over the clusters of a partition introduced `cliffs' in the
space of partitions, it is likely that better objective functions can be found. For example, we
might consider Bayesian variants like those found in Autoclass (Cheeseman et al., 1988)
and Anderson and Matessa's (1991) system, or the closely related MML approach of Snob
(Wallace & Dowe, 1994). We do not evaluate alternative measures such as these here, but
do suggest a number of other candidates.
12. In fact, cost is not constant across observations, even those that are classified along exactly the same
path { the number of variables that one need test depends on the observation's values along previously
examined variables.

170

fiOptimization of Hierarchical Clusterings

Section 2.1 noted that the CU function could be viewed as a summation over Gini
Indices, which measured the collective impurity of variables conditioned on cluster membership. Intuition may be helped further by an information-theoretic analog to CU (Corter
& Gluck, 1992):

P (Ck )

X X[P (A
i

j

i = Vij jCk ) log2 P (Ai = Vij jCk ) , P (Ai = Vij ) log2 P (Ai = Vij )]:

The information-theoretic analog can be understood as a summation over information gain
values, where information gain is an often used selection criterion for decision tree induction
(Quinlan, 1986): the clustering analog rewards clusters, Ck , that maximize the sum of
information gains over the individual variables, Ai .
Both the Gini and Information Gain measures are often-used bases for selection measures of decision tree induction. They are used to measure the expected decrease in impurity
or uncertainty of a class label, conditioned on knowledge of a given variable's value. In a
clustering context, we are interested in the decrease in impurity of each variable's value
conditioned on knowledge of cluster membership { thus, we use a summation over suitable
Gini Indices or alternatively, information gain scores. However, it is well known that in
the context of decision tree induction, both measures are biased to select variables with
more legal values. Thus, various normalizations of these measures or different measures
altogether, have been devised.
In the clustering adaptation of these measures normalizaP
N
tion is also necessary, since k=1 CU alone or its information-theoretic analog will favor a
clustering of greatest cardinality, in which the data are partitioned into singleton clusters,
one for each observation. Thus, PU normalizes the sum of Gini indices by averaging.
A general observation is that many selection measures used for decision tree induction
can be adapted as objective functions for clustering. There are a number of selection
measures that suggest themselves as candidates for clustering, in which normalization is
more principled than averaging. Two candidates are Quinlan's (1986) Gain Ratio and
Lopez de Mantaras' (1991) normalized information gain.13

Pj P (Ai=Vij ) Pk [P (Ck jAi =Vij ) log P (Ck jAi =Vij ),P (Ck )log P (Ck )]
P
(Quinlan, 1986)
, j P (Ai =Vij ) log P (Ai =Vij )
Pj P (Ai=Vij ) Pk [P (Ck jAi =Vij ) log P (Ck jAi =Vij ),P (Ck )log P (Ck )]
PP
(Lopez de Mantaras, 1991)
, j k P (Ck ^Ai =Vij )log P (Ck ^Ai =Vij )
2

2

2

2

2

2

From these we can derive two objective functions for clustering:

Pi Pk P (Ck ) Pj [P (Ai=Vij jCk )log
P P (Ai =Vij jCk ),P (Ai =Vij ) log P (Ai=Vij )]
2

, k P (Ck )log2 P (Ck )

2

Pi Pk P (Ck ) Pj [P (PAi=VPij jCk )log P (Ai =Vij jCk ),P (Ai =Vij ) log P (Ai=Vij )]
2

, k j P (Ai =Vij ^Ck )log2 P (Ai =Vij ^Ck )

2

13. Jan Hajek independently pointed out the relationship between the C U measure and the Gini Index, and
made suggestions on when one might select one or another of the normalizations above.

171

fiFisher

The latter of these clustering variations was defined in Fisher and Hapanyengwi (1993). Our
nonsystematic experimentation with Lopez de Mantaras' normalized information gain variant suggests that it mitigates the problems associated with PU , though conclusions about
its merits must await further experimentation. In general, there are a wealth of promising
objective functions based on decision tree selection measures that we might consider. We
have described two, but there are others such as Fayyad's (1991) ORT function.
The relationship between supervised and unsupervised measures also has been pointed
out in the context of Bayesian systems (Duda & Hart, 1973). Consider Autoclass (Cheeseman et al., 1988), which searches for the most probable clustering, H , given the available
data set, D { i.e., the clustering with highest P (H jD) / P (DjH )P (H ). Under independence assumptions made by Autoclass, the computation of P (DjH ) includes the easily
seen mechanisms of the simple Bayes classifier used in supervised contexts.
We have not compared the proposed derivations of decision tree selection measures or
the Bayesian/MML measures of Autoclass and Snob as yet, but have proposed patterncompletion error rate, simplicity, and classification cost as external, objective criteria that
could be used in such comparisons. An advantage of the Bayesian and MML approaches
is that, with proper selection of prior biases, these do not require a separate strategy (e.g.,
resampling) for pruning, and these strategies can be adapted for variable frontier identification. Rather, the objective function used for cluster formation serves to cease hierarchical
decomposition as well. Though we know of no experimental studies with the Bayesian and
MML techniques along the accuracy and cost dimensions outlined here, we expect that each
would perform quite well.

5.3 Final Comments on External Validation Criteria
Our proposal of external validation criteria for clustering such as error rate and classification
cost stem from a larger, often implicit, but long-standing bias of some in AI that learning
systems should serve the ends of some artificial autonomous agent. Certainly, the Cobweb
family of systems trace their ancestry to systems such as Unimem (Lebowitz, 1982) and
Cyrus (Kolodner, 1983) in which autonomous agency was a primary theme, as it was
in Fisher (1987a); Anderson and Matessa's (1991) work expresses similar concerns. In
short, the view that clustering is a means of organizing a memory of observations for an
autonomous agent begs the question of which of the agent's tasks is memory organization
intended to support? Pattern completion error rate and simplicity/cost seem to be obvious
candidate criteria.
However, an underlying assumption of this article is that these criteria are also appropriate for externally validating clusterings used in data analysis contexts, where the clustering
is external to a human analyst, but is nonetheless exploited by the analyst for purposes such
as hypothesis generation. Traditional criteria for cluster evaluation in such contexts include
measures of intra-cluster cohesion (i.e., observations within the same clusters should be similar) and inter-cluster coupling (i.e., observations in differing clusters should be dissimilar).
The criteria proposed in this article and traditional criteria are certainly related. Consider
the following derivation of a portion of the category utility measure, which begins with the
expected number of variable values that will be correctly predicted given that prediction is
guided by a clustering fC1; C2; :::; CN g:
172

fiOptimization of Hierarchical Clusterings

E (# P
of correct variable predictionsjfC1 ; C2; :::; CN g)
= Pk P (Ck )EP(# of correct variable predictionsjCk )
= Pk P (Ck ) Pi E
P(# of correct predictions of variable Ai jCk)
= Pk P (Ck ) Pi Pj P (Ai = Vij jCk )E (# of times that Vij is correct prediction of Ai jCk )
= Pk P (Ck ) Pi Pj P (Ai = Vij jCk )  P (Ai = Vij jCk )
= k P (Ck ) i j P (Ai = Vij jCk )2
The final steps of this derivation assume that a variable value is predicted with probability
P (Ai = Vij jCk) and that with the same probability this prediction is correct { i.e., the
derivation of category utility assumes a probability matching prediction strategy (Gluck &
Corter, 1985; Corter & Gluck, 1992).14 By favoring partitions that improve prediction along
many variables, hierarchical clustering using category utility tends to result in hierarchies
with more variable frontiers, as described in Section 4.1, near the top of the clustering; this
tends to reduce post-validation classification cost.
Thus, category utility can be motivated as a measure that rewards cohesion within
clusters and decoupling across clusters as noted in Section 2.1, or as a measure motivated
by a desire to reduce error rate (and indirectly, classification cost). In general, measures
motivated by a desire to reduce error rate will also favor cohesion and decoupling; this stems
from two aspects of the pattern-completion task (Lebowitz, 1982; Medin, 1983). First, we
assign an observation to a cluster based on the known variable values of the observation,
which is best facilitated if variable value predictiveness is high across many variables (i.e.,
clusters are decoupled).15 Having assigned an observation to a cluster, we use the cluster's
definition to predict the values of variables that are not known from the observation's
description. This process is most successful when many variables are predictable at clusters
(i.e., clusters are cohesive). In fact, designing measures with cohesion and decoupling in
mind undoubtedly results in useful clusterings for purposes of pattern completion, whether
or not this was the explicit goal of the designer.
If external validation criteria of error rate and cost are well correlated with traditional
criteria of cohesion and coupling, then why use the former criteria at all? In part, this
stems from an AI and machine learning bias that systems should be designed and evaluated
with a specific performance task in mind. In addition, however, a plethora of measures for
assessing cohesion and coupling can be found, with each system assessed relative to some
variant. This variation can make it more dicult to assess similarities and differences across
systems. This article suggests pattern-completion error rate and cost as relatively unbiased
alternatives for comparative studies. Inversely, why not use some direct measures of error
rate and classification cost (e.g., using holdout) as an `objective function' to guide search
through the space of clusterings? This can be expensive. Thus, we use a cheaply computed
objective function that is designed with external error rate and cost evaluation in mind;
undoubtedly, such an objective function reects cohesion and coupling.
14. Importantly, prediction with Cobweb is actually performed using a probability maximizing strategy { the
most frequent value of a variable at a cluster is always predicted. Fisher (1987b) discusses the advantage
of constructing clusters with an implicit probability matching strategy, even in cases where these clusters
will be exploited with a probability maximizing strategy.
15. The MML and Bayesian approaches of Snob and Autoclass support probabilistic assignment of observations to clusters, but the importance of decoupling and cohesion remain.

173

fiFisher

Of course, we have computed error rate and identified variable frontiers given a simplified
performance task: each variable was independently masked and predicted over test observations. This is not an unreasonable generic method for computing error rate, but different
domains may suggest different computations, since often many variables are simultaneously
unknown and/or an analyst may be interested in a subset of the variables. In addition,
we have proposed simplicity (i.e., the number of leaves) and expected classification cost as
external validation criteria. Section 5.1 suggests that one of the latter criteria is probably
necessary, in addition to error rate, to discriminate `good' and `poor' clusterings as judged
by the objective function. In general, desirable realizations of error rate, simplicity, and
cost will likely vary with domain and the interpretation tasks of an analyst.
In short, an analyst's task is largely one of making inferences from a clustering, for
which there are error-rate and cost components (i.e., what information can an analyst glean
from a clustering and how much work is required on the part of the analyst to extract
this information). It is probably not the case that we have expressed these components in
precisely the way that they are cognitively-implemented in an analyst. Nonetheless, this
article and others (Fisher, 1987a; Cheeseman et al., 1988; Anderson & Matessa, 1991) can
be viewed as attempts to formally, but tentatively describe an analyst's criteria for cluster
evaluation, based on criteria that we might prescribe for an autonomous, artificial agent
confronted with much the same task.

5.4 Other Issues
There are many important issues in clustering that we will not address in depth. One of these
is the possible advantage of overlapping clusters (Lebowitz, 1987; Martin & Billman, 1994).
We have assumed tree-structured clusterings, which store each observation in more than
one cluster, but these clusters are related by a proper subset-of relation as one descends
a path in the tree. In many cases, lattices (Levinson, 1984; Wilcox & Levinson, 1986;
Carpineto & Romano, 1993), or more generally, directed acyclic graphs (DAG) may be
a better representation scheme. These structures allow an observation to be included in
multiple clusters, where one such cluster need not be a subset of another. As such, they may
better provide an analyst with multiple perspectives of the data. For example, animals can
be partitioned into clusters corresponding to mammals, birds, reptiles, etc., or they may be
partitioned into clusters corresponding to carnivores, herbivores, or omnivores. A tree would
require that one of these partitions (e.g., carnivore, etc.) be `subordinate' to the other (e.g.,
mammals, birds, etc.); Classes of the subordinate partition would necessarily be `distributed'
across descendents (e.g., carnivorous-mammal, omnivorous-mammal, carnivorous-reptile,
etc.) of top level clusters, which ideally would represent clusters of the other partition. A
DAG allows both perspectives to coexist in relative equality, thus making both perspectives
more explicit to an analyst.
We have also assumed that variables are nominally valued. There have been numerous
adaptations of the basic PU function, other functions, and discretization strategies to accommodate numeric variables (Michalski & Stepp, 1983a, 1983b; Gennari et al., 1989; Reich
& Fenves, 1991; Cheeseman et al., 1988; Biswas et al., 1994). The basic sorting procedure
and the iterative optimization techniques can be used with data described in whole or part
by numerically-valued variables regardless of which approach one takes. The identification
174

fiOptimization of Hierarchical Clusterings

of numeric variable frontiers using holdout can be done by using the mean value for a variable at a node for generating predictions, and identifying a variable's frontier as the set of
clusters that collectively minimize a measure of error such as mean-squared error.

6. Concluding Remarks

We have partitioned the search through the space of hierarchical clusterings into three
phases. These phases, together with an opinion of their desirable characteristics from a
data analysis standpoint, are (1) inexpensive generation of an initial clustering that suggests the form of structure in data (or its absence), (2) iterative optimization (perhaps in
background) for clusterings of better quality, and (3) retrospective simplification of generated clusterings. We have evaluated three iterative optimization strategies that operate
independent of objective function. All of these, to varying degrees, are inspired by previous
research, but hierarchical redistribution appears novel as an iterative optimization technique
for clustering; it also appears to do quite well.
Another novel aspect of this work is the use of resampling as a means of validating clusters and of simplifying hierarchical clusterings. The experiments of Section 5 indicate that
optimized clusterings provide greater data compression than do unoptimized clusterings.
This is not surprising, given that PU compresses data in some reasonable manner; whether
it does so `optimally' though is another issue.
We have made several recommendations for further research.
1. We have suggested experiments with alternative objective functions, including Bayesian
and MML measures, and some that are inspired by variable-selection measures of decision tree induction.
2. There may be cost and quality benefits to applying optimization strategies at intermittent points during hierarchical sorting.
3. The holdout method of identifying variable frontiers and pruning suggests a strategy
akin to n-fold-cross validation that clusters over all the data, while still identifying
variable frontiers and facilitating pruning.
4. Analyses of classification cost for purposes of external validation are probably best
expressed in terms of the expected number of variables using a focusing method such
as Gennari's.
In sum, this paper has proposed criteria for internal and external validation, and has
made experimental comparisons between various approaches along these dimensions. Ideally, as researchers explore other objective functions, search control strategies, and pruning
techniques, the same kind of experimental comparisons (particularly along external criteria
such as error rate, simplicity, and classification cost) that are de rigueur in comparisons of
supervised systems, will become more prominent in unsupervised contexts.

175

fiFisher

Acknowledgements
I thank Sashank Varma, Arthur Nevins, and Diana Gordon for comments on the paper. The
reviewers and editor supplied extensive and helpful comments. This work was supported
by grant NAG 2-834 from NASA Ames Research Center. A very abbreviated discussion of
some of this article's results appear in Fisher (1995), published by AAAI Press.

References

Ahn, W., & Medin, D. L. (1989). A two-stage categorization model of family resemblance
sorting.. In Proceedings of the Eleventh Annual Conference of the Cognitive Science
Society, pp. 315{322. Ann Arbor, MI: Lawrence Erlbaum.
Anderson, J. R., & Matessa, M. (1991). An iterative Bayesian algorithm for categorization.
In Fisher, D., Pazzani, M., & Langley, P. (Eds.), Concept formation: Knowledge and
Experience in Unsupervised Learning. San Mateo, CA: Morgan Kaufmann.
Biswas, G., Weinberg, J., & Li, C. (1994). Iterate: A conceptual clustering method for
knowledge discovery in databases. In Braunschweig, B., & Day, R. (Eds.), Innovative
Applications of Artificial Intelligence in the Oil and Gas Industry. Editions Technip.
Biswas, G., Weinberg, J. B., Yang, Q., & Koller, G. R. (1991). Conceptual clustering
and exploratory data analysis. In Proceedings of the Eighth International Machine
Learning Workshop, pp. 591{595. San Mateo, CA: Morgan Kaufmann.
Carpineto, C., & Romano, G. (1993). Galois: An order-theoretic approach to conceptual
clustering. In Proceedings of the Tenth International Conference on Machine Learning,
pp. 33{40. Amherst, MA: Morgan Kaufmann.
Cheeseman, P., Kelly, J., Self, M., Stutz, J., Taylor, W., & Freeman, D. (1988). AutoClass:
A Bayesian classification system. In Proceedings of the Fifth International Machine
Learning Conference, pp. 54{64. Ann Arbor, MI: Morgan Kaufmann.
Corter, J., & Gluck, M. (1992). Explaining basic categories: feature predictability and
information. Psychological Bulletin, 111, 291{303.
De Alte Da Veiga, F. (1994). Data Analysis in Biomedical Research: A Novel Methodological Approach and its Implementation as a Conceptual Clustering Algorithm (in
Portuguese). Ph.D. thesis, Universidade de Coimbra, Unidade de Biomatematica e
Informatica Medica da Faculdade de Medicina.
Decaestecker, C. (1991). Description contrasting in incremental concept formation. In Kodratoff, Y. (Ed.), Machine Learning { EWSL-91, No. 482, Lecture Notes in Artificial
Intelligence, pp. 220{233. Springer-Verlag.
Devaney, M., & Ram, A. (1993). Personal communication, oct. 1993..
Duda, R. O., & Hart, P. E. (1973). Pattern Classification and Scene Analysis. New York,
NY: Wiley and Sons.
176

fiOptimization of Hierarchical Clusterings

Everitt, B. (1981). Cluster Analysis. London: Heinemann.
Fayyad, U. (1991). On the Induction of Decision Trees for Multiple Concept Learning. Ph.D.
thesis, University of Michigan, Ann Arbor, MI: Department of Computer Science and
Engineering.
Fisher, D. (1995). Optimization and simplification of hierarchical clusterings. In Proceedings
of the First International Conference on Knowledge Discovery and Data Mining, pp.
118{123. Menlo Park, CA: AAAI Press.
Fisher, D., & Hapanyengwi, G. (1993). Database management and analysis tools of machine
induction. Journal of Intelligent Information Systems, 2, 5{38.
Fisher, D., Xu, L., Carnes, J., Reich, Y., Fenves, S., Chen, J., Shiavi, R., Biswas, G., &
Weinberg, J. (1993). Applying AI clustering to engineering tasks. IEEE Expert, 8,
51{60.
Fisher, D., Xu, L., & Zard, N. (1992). Ordering effects in clustering. In Proceedings of the
Ninth International Conference on Machine Learning, pp. 163{168. San Mateo, CA:
Morgan Kaufmann.
Fisher, D. H. (1987a). Knowledge acquisition via incremental conceptual clustering. Machine Learning, 2, 139{172.
Fisher, D. H. (1987b). Knowledge Acquisition via Incremental Conceptual Clustering. Ph.D.
thesis, University of California, Irvine, CA: Department of Information and Computer
Science.
Fisher, D. H. (1989). Noise-tolerant conceptual clustering. In Proceedings of the International Joint Conference Artificial Intelligence, pp. 825{830. Detroit, MI: Morgan
Kaufmann.
Fisher, D. H., & Langley, P. (1990). The structure and formation of natural categories. In
Bower, G. H. (Ed.), The Psychology of Learning and Motivation, Vol. 25. San Diego,
CA: Academic Press.
Fisher, D. H., & Schlimmer, J. (1988). Concept simplification and prediction accuracy. In
Proceedings of the Fifth International Conference on Machine Learning, pp. 22{28.
Ann Arbor, MI: Morgan Kaufmann.
Gennari, J. (1989). Focused concept formation. In Proceedings of the Sixth International
Workshop on Machine Learning, pp. 379{382. San Mateo, CA: Morgan Kaufmann.
Gennari, J., Langley, P., & Fisher, D. (1989). Models of incremental concept formation.
Artificial Intelligence, 40, 11{62.
Gluck, M. A., & Corter, J. E. (1985). Information, uncertainty, and the utility of categories.
In Proceedings of the Seventh Annual Conference of the Cognitive Science Society, pp.
283{287. Hillsdale, NJ: Lawrence Erlbaum.
177

fiFisher

Hadzikadic, M., & Yun, D. (1989). Concept formation by incremental conceptual clustering.
In Proceedings of the International Joint Conference Artificial Intelligence, pp. 831{
836. San Mateo, CA: Morgan Kaufmann.
Hanson, R., Stutz, J., & Cheeseman, P. (1991). Bayesian classification with correlation and
inheritance. In Proceedings of the 12th International Joint Conference on Artificial
Intelligence, pp. 692{698. San Mateo, CA: Morgan Kaufmann.
Iba, G. (1989). A heuristic approach to the discovery of macro operators. Machine Learning,
3, 285{317.
Iba, W., & Gennari, J. (1991). Learning to recognize movements. In Fisher, D., Pazzani, M.,
& Langley, P. (Eds.), Concept Formation: Knowledge and Experience in Unsupervised
Learning. San Mateo, CA: Morgan Kaufmann.
Ketterlin, A., Gancarski, P., & Korczak, J. (1995). Hierarchical clustering of composite
objects with a variable number of components. In Preliminary papers of the Fifth
International Workshop on Artificial Intelligence and Statistics, pp. 303{309.
Kilander, F. (1994). Incremental Conceptual Clustering in an On-Line Application. Ph.D.
thesis, Stockholm University, Stockholm, Sweden: Department of Computer and Systems Sciences.
Kolodner, J. L. (1983). Reconstructive memory: A computer model. Cognitive Science, 7,
281{328.
Lebowitz, M. (1982). Correcting erroneous generalizations. Cognition and Brain Theory,
5, 367{381.
Lebowitz, M. (1987). Experiments with incremental concept formation: Unimem. Machine
Learning, 2, 103{138.
Levinson, R. (1984). A self-organizing retrieval system for graphs. In Proceedings of the
National Conference on Artificial Intelligence, pp. 203{206. San Mateo, CA: Morgan
Kaufmann.
Lopez de Mantaras, R. (1991). A distance-based attribute selection measure for decision
tree induction. Machine Learning, 6, 81{92.
Martin, J., & Billman, D. (1994). Acquiring and combining overlapping concepts. Machine
Learning, 16, 121{155.
McKusick, K., & Langley, P. (1991). Constraints on tree structure in concept formation.
In Proceedings of the International Joint Conference on Artificial Intelligence, pp.
810{816. San Mateo, CA: Morgan Kaufmann.
McKusick, K., & Thompson, K. (1990). Cobweb/3: A portable implementation (Tech. Rep.
No. FIA-90-6-18-2). Moffett Field, CA: AI Research Branch, NASA Ames Research
Center.
178

fiOptimization of Hierarchical Clusterings

Medin, D. (1983). Structural principles of categorization. In Tighe, T., & Shepp, B.
(Eds.), Perception, Cognition, and Development, pp. 203{230. Hillsdale, NJ: Lawrence
Erlbaum.
Michalski, R. S., & Stepp, R. (1983a). Automated construction of classifications: conceptual
clustering versus numerical taxonomy. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 5, 219{243.
Michalski, R. S., & Stepp, R. (1983b). Learning from observation: conceptual clustering.
In Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), Machine Learning:
An Artificial Intelligence Approach. San Mateo, CA: Morgan Kaufmann.
Mingers, J. (1989a). An empirical comparison of pruning methods for decision-tree induction. Machine Learning, 4, 227{243.
Mingers, J. (1989b). An empirical comparison of selection measures for decision-tree induction. Machine Learning, 3, 319{342.
Nevins, A. J. (1995). A branch and bound incremental conceptual clusterer. Machine
Learning, 18, 5{22.
Quinlan, J. R. (1986). Induction of decision trees. Machine Learning, 1, 81{106.
Quinlan, J. R. (1987). Simplifying decision trees. International Journal of Man-machine
Studies, 27, 221{234.
Quinlan, J. R. (1993). C4.5: Programs for Machine Learning. San Mateo, CA: Morgan
Kaufmann.
Reich, Y., & Fenves, S. (1991). The formation and use of abstract concepts in design. In
Fisher, D., Pazzani, M., & Langley, P. (Eds.), Concept Formation: Knowledge and
Experience in Unsupervised Learning. San Mateo, CA: Morgan Kaufmann.
Utgoff, P. (1994). An improved algorithm for incremental induction of decision trees. In
Proceedings of the Eleventh International Conference on Machine Learning, pp. 318{
325. San Mateo, CA: Morgan Kaufmann.
Wallace, C. S., & Dowe, D. L. (1994). Intrinsic classification by MML - the Snob program.
In Proceedings of the 7th Australian Joint Conference on Artificial Intelligence, pp.
37{44. UNE, Armidale, NSW, Australia: World Scientific.
Weiss, S., & Kulikowski, C. (1991). Computer Systems that Learn. San Mateo, CA: Morgan
Kaufmann.
Wilcox, C. S., & Levinson, R. A. (1986). A self-organized knowledge base for recall, design,
and discovery in organic chemistry. In Pierce, T. H., & Hohne, B. A. (Eds.), Artificial
Intelligence Applications in Chemistry. Washington, DC: American Chemical Society.

179

fiJournal of Artificial Intelligence Research 4 (1996) 61|76

Submitted 11/95; published 3/96

Mean Field Theory for Sigmoid Belief Networks
Lawrence K. Saul
Tommi Jaakkola
Michael I. Jordan

Center for Biological and Computational Learning
Massachusetts Institute of Technology
79 Amherst Street, E10-243
Cambridge, MA 02139

lksaul@psyche.mit.edu
tommi@psyche.mit.edu
jordan@psyche.mit.edu

Abstract

We develop a mean field theory for sigmoid belief networks based on ideas from statistical
mechanics. Our mean field theory provides a tractable approximation to the true probability distribution in these networks; it also yields a lower bound on the likelihood of evidence. We demonstrate the utility of this framework on a benchmark problem in statistical pattern recognition|the
classification of handwritten digits.

1. Introduction

Bayesian belief networks (Pearl, 1988; Lauritzen & Spiegelhalter, 1988) provide a rich graphical
representation of probabilistic models. The nodes in these networks represent random variables,
while the links represent causal inuences. These associations endow directed acyclic graphs (DAGs)
with a precise probabilistic semantics. The ease of interpretation afforded by this semantics explains
the growing appeal of belief networks, now widely used as models of planning, reasoning, and
uncertainty.
Inference and learning in belief networks are possible insofar as one can eciently compute (or
approximate) the likelihood of observed patterns of evidence (Buntine, 1994; Russell, Binder, Koller,
& Kanazawa, 1995). There exist provably ecient algorithms for computing likelihoods in belief
networks with tree or chain-like architectures. In practice, these algorithms also tend to perform
well on more general sparse networks. However, for networks in which nodes have many parents,
the exact algorithms are too slow (Jensen, Kong, & Kjaefulff, 1995). Indeed, in large networks
with dense or layered connectivity, exact methods are intractable as they require summing over an
exponentially large number of hidden states.
One approach to dealing with such networks has been to use Gibbs sampling (Pearl, 1988), a
stochastic simulation methodology with roots in statistical mechanics (Geman & Geman, 1984).
Our approach in this paper relies on a different tool from statistical mechanics|namely, mean field
theory (Parisi, 1988). The mean field approximation is well known for probabilistic models that
can be represented as undirected graphs|so-called Markov networks. For example, in Boltzmann
machines (Ackley, Hinton, & Sejnowski, 1985), mean field learning rules have been shown to yield
tremendous savings in time and computation over sampling-based methods (Peterson & Anderson,
1987).
The main motivation for this work was to extend the mean field approximation for undirected
graphical models to their directed counterparts. Since belief networks can be transformed to Markov
networks, and mean field theories for Markov networks are well known, it is natural to ask why a
new framework is required at all. The reason is that probabilistic models which have compact
representations as DAGs may have unwieldy representations as undirected graphs. As we shall see,
avoiding this complexity and working directly on DAGs requires an extension of existing methods.
In this paper we focus on sigmoid belief networks (Neal, 1992), for which the resulting mean
field theory is most straightforward. These are networks of binary random variables whose local
c 1996 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiSaul, Jaakkola, & Jordan

conditional distributions are based on log-linear models. We develop a mean field approximation for
these networks and use it to compute a lower bound on the likelihood of evidence. Our method applies
to arbitrary partial instantiations of the variables in these networks and makes no restrictions on the
network topology. Note that once a lower bound is available, a learning procedure can maximize the
lower bound; this is useful when the true likelihood itself cannot be computed eciently. A similar
approximation for models of continous random variables is discussed by Jaakkola et al (1995).
The idea of bounding the likelihood in sigmoid belief networks was introduced in a related
architecture known as the Helmholtz machine (Hinton, Dayan, Frey, & Neal 1995). A fundamental
advance of this work was to establish a framework for approximation that is especially conducive to
learning the parameters of layered belief networks. The close connection between this idea and the
mean field approximation from statistical mechanics, however, was not developed.
In this paper we hope not only to elucidate this connection, but also to convey a sense of
which approximations are likely to generate useful lower bounds while, at the same time, remaining
analytically tractable. We develop here what is perhaps the simplest such approximation for belief
networks, noting that more sophisticated methods (Jaakkola & Jordan, 1996a; Saul & Jordan, 1995)
are also available. It should be emphasized that approximations of some form are required to handle
the multilayer neural networks used in statistical pattern recognition. For these networks, exact
algorithms are hopelessly intractable; moreover, Gibbs sampling methods are impractically slow.
The organization of this paper is as follows. Section 2 introduces the problems of inference
and learning in sigmoid belief networks. Section 3 contains the main contribution of the paper:
a tractable mean field theory. Here we present the mean field approximation for sigmoid belief
networks and derive a lower bound on the likelihood of instantiated patterns of evidence. Section 4
looks at a mean field algorithm for learning the parameters of sigmoid belief networks. For this
algorithm, we give results on a benchmark problem in pattern recognition|the classification of
handwritten digits. Finally, section 5 presents our conclusions, as well as future issues for research.

2. Sigmoid Belief Networks

The great virtue of belief networks is that they clearly exhibit the conditional dependencies of
the underlying probability model. Consider a belief network defined over binary random variables
S = (S1 ; S2 ; : : : ; SN ). We denote the parents of Si by pa(Si )  fS1 ; S2; : : : Si,1 g; this is the smallest
set of nodes for which
P (Si jS1; S2 ; : : : ; Si,1) = P (Sijpa(Si )):
(1)
In sigmoid belief networks (Neal, 1992), the conditional distributions attached to each node are
based on log-linear models. In particular, the probability that the ith node is activated is given by

1
0
X
Jij Sj + hi A ;
P (Si = 1jpa(Si )) =  @
j

(2)

where Jij and hi are the weights and biases in the network, and
1
(z ) =
(3)
1 + e,z
is the sigmoid function shown in Figure 1. In sigmoidbelief networks, we have Jij = 0 for Sj 62 pa(Si );
moreover, Jij = 0 for j  i since the network's structure is that of a directed acyclic graph.
The sigmoid function in eq. (2) provides a compact parametrization of the conditional probability
distributions1 in eq. (2) used to propagate beliefs. In particular, P (Sijpa(Si )) depends on pa(Si )
only through a sum of weighted inputs, where the weights may be viewed as the parameters in a
1. The relation to noisy-OR models is discussed in appendix A.

62

fiMean Field Theory for Sigmoid Belief Networks

1

0.8

0.6
(z)
0.4

0.2

0
6

4

2

0
z

2

4

6

Figure 1: Sigmoid function (z ) = [1 + e,z ],1. If z is the sum of weighted inputs to node S , then
P (S = 1jz ) = (z ) is the conditional probability that node S is activated.
logistic regression (McCullagh & Nelder, 1983). The conditional probability distribution for Si may
be summarized as:
hP
 i
exp
J
S
+
h
Si
ij
j
i
hjP
i:
(4)
P (Sijpa(Si )) =
1 + exp j Jij Sj + hi
Note that substituting Si = 1 in eq. (4) recovers the result in eq. (2). Combining eqs. (1) and (4),
we may write the joint probability distribution over the variables in the network as:
P (S )

=

Y

P (Si jpa(Si ))

 i9
8 h
Y < exp Pj Jij Sj + hi Si =
=
: 1 + exp hPj Jij Sj + hii ; :
i
i

(5)
(6)

The denominator in eq. (6) ensures that the probability distribution is normalized to unity.
We now turn to the problem of inference in sigmoid belief networks. Absorbing evidence divides
the units in the belief network into two types, visible and hidden. The visible units (or \evidence
nodes") are those for which we have instantiated values; the hidden units are those for which we do
not. When there is no possible ambiguity, we will use H and V to denote the subsets of hidden and
visible units. Using Bayes' rule, inference is done under the conditional distribution
P (H; V )
P (H jV ) =
;
(7)
P (V )
where
X
P (V ) =
P (H; V )
(8)
H

is the likelihood of the evidence V . In principle, the likelihood may be computed by summing over
all 2jH j configurations of the hidden units. Unfortunately, this calculation is intractable in large,
densely connected networks. This intractability presents a major obstacle to learning parameters
for these networks, as nearly all procedures for statistical estimation require frequent estimates of
the likelihood. The calculations for exact probabilistic inference are beset by the same diculties.
63

fiSaul, Jaakkola, & Jordan

Unable to compute P (V ) or work directly with P (H jV ), we will resort to an approximation from
statistical physics known as mean field theory.

3. Mean Field Theory

The mean field approximation appears under a multitude of guises in the physics literature; indeed,
it is \almost as old as statistical mechanics" (Itzykson & Drouffe, 1991). Let us briey explain how it
acquired its name and why it is so ubiquitous. In the physical models described by Markov networks,
the variables
P Si represent localized magnetic moments (e.g., at the sites of a crystal lattice), and
the sums j Jij Sj + hi represent local magnetic fields. Roughly speaking, in certain cases a central
limit theorem may be applied to these sums, and a useful approximation is to ignore the uctuations
in these fields and replace them by their mean value|hence the name, \mean field" theory. In some
models, this is an excellent approximation; in others, a poor one. Because of its simplicity, however,
it is widely used as a first step in understanding many types of physical phenomena.
Though this explains the philological origins of mean field theory, there are in fact many ways
to derive what amounts to the same approximation (Parisi, 1988). In this paper we present the
formulation most appropriate for inference and learning in graphical models. In particular, we view
mean field theory as a principled method for approximating an intractable graphical model by a
tractable one. This is done via a variational principle that chooses the parameters of the tractable
model to minimize an entropic measure of error.
The basic framework of mean field theory remains the same for directed graphs, though we have
found it necessary to introduce extra mean field parameters in addition to the usual ones. As in
Markov networks, one finds a set of nonlinear equations for the mean field parameters that can be
solved by iteration. In practice, we have found this iteration to converge fairly quickly and to scale
well to large networks.
Let us now return to the problem posed at the end of the last section. There we found that for
many belief networks, it was intractable to decompose the joint distribution as P (S ) = P (H jV )P (V ),
where P (V ) was the likelihood of the evidence V . For the purposes of probabilistic modeling, mean
field theory has two main virtues. First, it provides a tractable approximation, Q(H jV )  P (H jV ),
to the conditional distributions required for inference. Second, it provides a lower bound on the
likelihoods required for learning.
Let us first consider the origin of the lower bound. Clearly, for any approximating distribution
Q(H jV ), we have the equality:
ln P (V ) = ln

X

P (H; V )

H

= ln

X
H

Q(H jV ) 

 P (H; V ) 
Q(H jV )

(9)
:

(10)

To obtain a lower bound, we now apply Jensen's inequality (Cover & Thomas, 1991), pushing the
logarithm through the sum over hidden states and into the expectation:


X
V)
ln P (V )  Q(H jV ) ln PQ((H;
:
(11)
H jV )
H
It is straightforward to verify that the difference between the left and right hand side of eq. (11) is
the Kullback-Leibler divergence (Cover & Thomas, 1991):
 Q(H jV ) 
X
KL(QjjP ) = Q(H jV ) ln P (H jV ) :
(12)
H
Thus, the better the approximation to P (H jV ), the tighter the bound on ln P (V ).
64

fiMean Field Theory for Sigmoid Belief Networks

Anticipating the connection to statistical mechanics, we will refer to Q(H jV ) as the mean field
distribution. It is natural to divide the calculation of the bound into two components, both of which
are particular averages over this approximating distribution. These components are the mean field
entropy and energy; the overall bound is given by their difference:
ln P (V )  ,

X
H

Q(H jV ) ln Q(H jV )

!

, ,

X
H

!

Q(H jV ) ln P (H; V ) :

(13)

Both terms have physical interpretations. The first measures the amount of uncertainty in the meanfield distribution and follows the standard definition of entropy. The second measures the average
value2 of , ln P (H; V ); the name \energy" arises from interpreting the probability distributions in
belief networks as Boltzmann distributions3 at unit temperature. In this case, the energy of each
network configuration is given (up to a constant) by minus the logarithm of its probability under
the Boltzmann distribution. In sigmoid belief networks, the energy has the form

, ln P (H; V ) = ,

X
ij

Jij Si Sj

,

X
i

2
0
13
X 4
X
hi Si +
ln 1 + exp @ Jij Sj + hi A5 ;
i

j

(14)

as follows from eq. (6). The first two terms in this equation are familiar from Markov networks with
pairwise interactions (Hertz, Krogh, & Palmer, 1991); the last term is peculiar to sigmoid belief
networks. Note that the overall energy is neither a linear function of the weights nor a polynomial
function of the units. This is the price we pay in sigmoid belief networks for identifying P (H jV )
as a Boltzmann distribution and the log-likelihood P (V ) as its partition function. Note that this
identification was made implicitly in the form of eqs. (7) and (8).
The bound in eq. (11) is valid for any probability distribution Q(H jV ). To make use of it,
however, we must choose a distribution that enables us to evaluate the right hand side of eq. (11).
Consider the factorized distribution
Q(H jV ) =

Y
2

i H

Si i (1 , i )1,Si ;

(15)

in which the binary hidden units fSi gi2H appear as independent Bernoulli variables with adjustable
means i . A mean field approximation is obtained by substituting the factorized distribution,
eq. (15), for the true Boltzmann distribution, eq. (7). It may seem that this approximation replaces
the rich probabilistic dependencies in P (H jV ) by an impoverished assumption of complete factorizability. Though this is true to some degree, the reader should keep in mind that the values we
choose for fi gi2H (and hence the statistics of the hidden units) will depend on the evidence V .
The best approximation of the form, eq. (15), is found by choosing the mean values, figi2H ,
that minimize the Kullback-Leibler divergence, KL(QjjP ). This is equivalent to minimizing the gap
between the true log-likelihood, ln P (V ), and the lower bound obtained from mean field theory. The
2. A similar average is performed in the E-step of an EM algorithm (Dempster, Laird, & Rubin, 1977); the difference
here is that the average is performed over the mean field distribution, Q(H jV ), rather than the true posterior,
P (H jV ). For a related discussion, see Neal & Hinton (1993).
3. Our terminology is as follows. Let S denote the degrees of freedom in a statistical mechanical system. The energy
of the system, E (S ), is a real-valued function of these degrees of freedom, and the Boltzmann distribution
e,fiE (S)
P (S ) = P ,fiE (S)
Se

defines a probabilitydistributionover the possible configurationsof S . The parameter fi is the inverse temperature;
it serves to calibrate the energy scale and will be fixed to unity in our discussion of belief networks. Finally, the
sum in the denominator|known as the partition function|ensures that the Boltzmann distribution is normalized
to unity.

65

fiSaul, Jaakkola, & Jordan

mean field bound on the log-likelihood may be calculated by substituting eq. (15) into the right
hand side of eq. (11). The result of this calculation is

X

ln P (V ) 

Jij i j

X

ij

,

i

+

X
i

hi i ,

X 
i

P

ln 1 + e

j

Jij Sj +hi



(16)

[i ln i + (1 , i ) ln(1 , i )] ;

where hi indicates an expectation value over the mean field distribution, eq. (15). The terms in
the first line of eq. (16) represent the mean field energy, derived from eq. (14); those in the second
represent the mean field entropy. In a slight abuse of notation, we have defined mean values i for
the visible units; these of course are set to the instantiated values i 2 f0; 1g.
Note that to compute the average energy
P in the mean field approximation, we must find the
expected value of hln [1 + ez ]i, where zi = j Jij Sj + hi is the sum of weighted inputs to the ith
unit in the belief network. Unfortunately, even under the mean field assumption that the hidden
units are uncorrelated, this average does not have a simple closed form. This term does not arise in
the mean field theory for Markov networks with pairwise interactions; again, it is peculiar to sigmoid
belief networks.
In principal, the average may be performed by enumerating the possible states of pa(Si ). The
result of this calculation, however, would be an extremely unwieldy function of the parameters in
the belief network. This reects the fact that in general, the sigmoid belief network defined by the
weights Jij has an equivalent Markov network with N th order interactions and not pairwise ones.
To avoid this complexity, we must develop a mean field theory that works directly on DAGs.
How we handle the expected value of hln [1 + ez ]i is what distinguishes our mean field theory
from previous ones. Unable to compute this term exactly, we resort to another bound. Note that
for any random variable z and any real number  , we have the equality:
i

i


 

ff

hln[1 + ez ]i = ln ez e,z (1 + ez )
D
E
=  hz i + ln[e,z + e(1,)z ] :

(17)
(18)

We can upper bound the right hand side by applying Jensen's inequality in the opposite direction
as before, pulling the logarithm outside the expectation:

D
E
hln[1 + ez ]i   hz i + ln e,z + e(1,)z :

(19)

Setting  = 0 in eq. (19) gives the standard bound: hln(1 + ez )i  lnh1 + ez i. A tighter bound
(Seung, 1995) can be obtained, however, by allowing non-zero values of  . This is illustrated in
Figure 2 for the special case where z is a Gaussian distributed random variable with zero mean and
unit variance. The bound in eq. (19) has two useful properties which we state here without proof:
(i) the right hand side is a convex function of  ; (ii) the value of  which minimizes this function
occurs in the interval  2 [0; 1]. Thus, provided it is possible to evaluate eq. (19) for different values
of  , the tightest bound of this form can be found by a simple one-dimensional minimization.
The above bound can be put to immediate use by attaching an extra mean field parameter i to
each unit in the belief network. We can then upper bound the intractable terms in the mean field
energy by

 

ln 1 + e

PJ
j

ij


S +h
j

i

1
0
D
E
X
 i @ Jij j + hi A + ln e, z + e(1, )z ;
i i

j

66

i

i

(20)

fiMean Field Theory for Sigmoid Belief Networks

1

0.95

0.9
bound
0.85
exact
0.8

0.75
0

0.2

0.4



0.6

0.8

1

Figure 2: Bound in eq. (19) for the case where z is normally distributed with zero mean and
unit variance.
case,
the exact result is hln(1 + ez )i = 0:806; the bound gives
o
n 1 2 In1 (1this
2
min ln[e 2 + e 2 ,) ] = 0:818. The standard bound from Jensen's inequality occurs
at  = 0 and gives 0:974.

P

where zi = j Jij Sj + hi. The expectations inside the logarithm can be evaluated exactly for the
factorial distribution, eq. (15); for example,
Y,

he, z i = e, h
1 , j + j e, J :
(21)
i i

i

i

i ij

j

holds for he(1,i )zi i.

A similar result
Though these averages are tractable, we will tend not to write
them out in what follows. The reader, however, should keep in mind that these averages do not
present any diculty; they are simply averages over products of independent random variables, as
opposed to sums.
Assembling the terms in eqs. (16) and (20) gives a lower bound ln P (V )  LV ,

LV =

X
ij

,

Jij i j +

X

X D , z
i

ln

e

i

i i

0
1
X @X
hi i ,
i
Jij j + hi A
i
E Xj
(1, )z

+e

i

i

+

i

(22)

[i ln i + (1 , i ) ln(1 , i )] ;

on the log-likelihood of the evidence V . So far we have not specified the parameters fi gi2H and
fi g; in particular, the bound in eq. (22) is valid for any choice of parameters. We naturally seek
the values that maximize the right hand side of eq. (22). Suppose we fix the mean values fi gi2H
and ask for the parameters fig that yield the tightest possible bound. Note that the right hand

side of eq. (22) does not couple terms with i that belong to different units in the network. The
minimization over fi g therefore reduces to N independent minimizations over the interval [0; 1].
These can be done by any number of standard methods (Press, Flannery, Teukolsky, & Vetterling,
1986).
To choose the means, we set the gradients of the bound with respect to fi gi2H equal to zero.
To this end, let us define the intermediate matrix:
Kij

D
E
= , @@ ln e, z + e(1, )z ;
i i

j

67

i

i

(23)

fiSaul, Jaakkola, & Jordan

Si

Figure 3: The Markov blanket of unit
parents of its children.

Si

includes its parents and children, as well as the other

where zi is the weighted sum of inputs to ith unit. Note that Kij is zero unless Sj is a parent
of Si ; in other words, it has the same connectivity as the weight matrix Jij . Within the mean field
approximation, Kij measures the parental inuence of Sj on Si given the instantiated evidence V .
The degree of correlation (positive or negative) is measured relative to the other parents of Si .
The matrix elements of K may be evaluated by expanding the expectations as in eq. (21); a full
derivation is given in appendix B. Setting the gradient @ LV =@i equal to zero gives the final mean
field equation:
0
1
i

=  @ hi +

X
j

[Jij j + Jji(j , j ) + Kji ]A ;

(24)

where () is the sigmoid function. The argument of the sigmoid function may be viewed as an
effective input to the ith unit in the belief network. This effective input is composed of terms from
the unit's Markov blanket (Pearl, 1988), shown in Figure 3; in particular, these terms take into
account the unit's internal bias, the values of its parents and children, and, through the matrix
Kji, the values of its children's other parents. In solving these equations by iteration, the values of
the instantiated units are propagated throughout the entire network. An analogous propagation of
information occurs in exact algorithms (Lauritzen & Spiegelhalter, 1988) to compute likelihoods in
belief networks.
While the factorized approximation to the true posterior is not exact, the mean field equations
set the parameters figi2H to values which make the approximation as accurate as possible. This
in turn translates into the tightest mean field bound on the log-likelihood. The overall procedure
for bounding the log-likelihood thus consists of two alternating steps: (i) update fi g for fixed fi g;
(ii) update figi2H for fixed fi g. The first step involves N independent minimizations over the
interval [0; 1]; the second is done by iterating the mean field equations. In practice, the steps are
repeated until the mean field bound on the log-likelihood converges4 to a desired degree of accuracy.
The quality of the bound depends on two approximations: the complete factorizability of the
mean field distribution, eq. (15), and the logarithm bound, eq. (19). How reliable are these approximations in belief networks? To study this question, we performed numerical experiments on
the three layer belief network shown in Figure 4. The advantage of working with such a small
network (2x4x6) is that true likelihoods can be computed by exact enumeration. We considered the
particular event that all the units in the bottom layer were instantiated to zero. For this event, we
compared the mean field bound on the likelihood to its true value, obtained by enumerating the
4. It can be shown that asychronous updates of the mean field parameters lead to monotonic increases in the lower
bound (just as in the case of Markov networks).

68

fiMean Field Theory for Sigmoid Belief Networks

Figure 4: Three layer belief network (2x4x6) with top-down propagation of beliefs. To model the
images of handwritten digits in section 4, we used 8x24x64 networks where units in the
bottom layer encoded pixel values in 8x8 bitmaps.
4500

4000

4000

3500

3500

3000
mean field approximation

3000

uniform approximation

2500

2500

2000
2000

1500
1500

1000

1000

500

500
0
0

0.01

0.02
0.03
0.04
0.05
relative error in loglikelihood

0.06

0
1

0.07

0.5

0
0.5
1
relative error in loglikelihood

1.5

Figure 5: Histograms of relative error in log-likelihood over 10000 randomly generated three layer
networks. At left: the relative error from the mean field approximation; at right: the
relative error if all states in the bottom layer are assumed to occur with equal probability.
The log-likelihood was computed for the event that the all the nodes in the bottom layer
were instantiated to zero.
states in the top two layers. This was done for 10000 random networks whose weights and biases
were uniformly distributed between -1 and 1. Figure 5 (left) shows the histogram of the relative error
in log likelihood, computed as LV = ln P (V ) , 1; for these networks, the mean relative error is 1.6%.
Figure 5 (right) shows the histogram that results from assuming that all states in the bottom layer
occur with equal probability; in this case the relative error was computed as (ln2,6 )= ln P (V ) , 1.
For this \uniform" approximation, the root mean square relative error is 22.6%. The large discrepancy between these results suggests that mean field theory can provide a useful lower bound on the
likelihood in certain belief networks. Of course, what ultimately matters is the behavior of mean
field theory in networks that solve meaningful problems. This is the subject of the next section.

4. Learning

One attractive use of sigmoid belief networks is to perform density estimation in high dimensional
input spaces. This is a problem in parameter estimation: given a set of patterns over particular
units in the belief network, find the set of weights Jij and biases hi that assign high probability
to these patterns. Clearly, the ability to compute likelihoods lies at the crux of any algorithm for
learning the parameters in belief networks.
69

fiSaul, Jaakkola, & Jordan

true loglikelihood

lower bound
true loglikelihood

lower bound

training time

training time

Figure 6: Relationship between the true log-likelihood and its lower bound during learning. One
possibility (at left) is that both increase together. The other is that the true log-likelihood
decreases, closing the gap between itself and the bound. The latter can be viewed as a
form of regularization.
Mean field algorithms provide a strategy for discovering appropriate values of Jij and hi without
resort to Gibbs sampling. Consider, for instance, the following procedure. For each pattern in the
training set, solve the mean field equations for fi; i g and compute the associated bound on the
log-likelihood, LV . Next, adapt the weights in the belief network by gradient ascent5 in the mean
field bound,
Jij =
hi =

@ LV
@Jij
@ LV
;

@hi


(25)
(26)

where  is a suitably chosen learning rate. Finally, cycle through the patterns in the training set,
maximizing their likelihoods6 for a fixed number of iterations or until one detects the onset of
overfitting (e.g., by cross-validation).
The above procedure uses a lower bound on the log-likelihood as a cost function for training belief
networks (Hinton, Dayan, Frey, & Neal, 1995). The fact that we have a lower bound on the loglikelihood, rather than an upper bound, is of course crucial to the success of this learning algorithm.
Adjusting the weights to maximize this lower bound can affect the true log-likelihood in two ways
(see Figure 6). Either the true log-likelihood increases, moving in the same direction as the bound,
or the true log-likelihood decreases, closing the gap between these two quantities. For the purposes
of maximum likelihood estimation, the first outcome is clearly desirable; the second, though less
desirable, can also be viewed in a positive light. In this case, the mean field approximation is acting
as a regularizer, steering the network toward simple, factorial solutions even at the expense of lower
likelihood estimates.
We tested this algorithm by building a maximum-likelihood classifier for images of handwritten
digits. The data consisted of 11000 examples of handwritten digits [0-9] compiled by the U.S. Postal
Service Oce of Advanced Technology. The examples were preprocessed to produce 8x8 binary
images, as shown in Figure 7. For each digit, we divided the available data into a training set
with 700 examples and a test set with 400 examples. We then trained a three layer network7 (see
5. Expressions for the gradients of LV are given in the appendix B.
6. Of course, one can also incorporate prior distributions over the weights and biases and maximize an approximation
to the log posterior probability of the training set.
7. There are many possible architectures that could be chosen for the purpose of density estimation; we used layered
networks to permit a comparison with previous benchmarks on this data set.

70

fiMean Field Theory for Sigmoid Belief Networks

Figure 7: Binary images of handwritten digits: two and five.
0
1
2
3
4
5
6
7
8
9

0
1
2
3
4
5
6
7
8
9
388 2
2
0
1
3
0
0
4
0
0 393 0
0
0
1
0
0
6
0
1
2 376 1
3
0
4
0 13 0
0
2
4 373 0 12 0
0
6
3
0
0
2
0 383 0
1
2
2 10
0
2
1 13 0 377 2
0
4
1
1
4
2
0
1
6 386 0
0
0
0
1
0
0
0
0
0 388 3
8
1
9
1
7
0
7
1
1 369 4
0
4
0
0
0
0
0
8
5 383

Table 1: Confusion matrix for digit classification. The entry in the ith row and j th column counts
the number of times that digit i was classified as digit j .
Figure 4) on each digit, sweeping through each training set five times with learning rate  = 0:05.
The networks had 8 units in the top layers, 24 units in the middle layer, and 64 units in the bottom
layer, making them far too large to be treated with exact methods.
After training, we classified the digits in each test set by the network that assigned them the
highest likelihood. Table 1 shows the confusion matrix in which the ij th entry counts the number of
times digit i was classified as digit j . There were 184 errors in classification (out of a possible 4000),
yielding an overall error rate of 4.6%. Table 2 gives the performance of various other algorithms on
the same partition of this data set. Table 3 shows the average log-likelihood score of each network
on the digits in its test set. (Note that these scores are actually lower bounds.) These scores are
normalized so that a network with zero weights and biases (i.e., one in which all 8x8 patterns are
equally likely) would receive a score of -1. As expected, digits with relatively simple constructions
(e.g., zeros, ones, and sevens) are more easily modeled than the rest.
Both measures of performance|error rate and log-likelihood score|are competitive with previously published results (Hinton, Dayan, Frey, & Neal, 1995) on this data set. The success of the
algorithm arms both the strategy of maximizing a lower bound and the utility of the mean field
approximation. Though similar results can be obtained via Gibbs sampling, this seems to require
considerably more computation than methods based on maximizing a lower bound (Frey, Dayan, &
Hinton, 1995).
71

fiSaul, Jaakkola, & Jordan

algorithm
classification error
nearest neighbor
6.7%
back-propagation
5.6%
wake-sleep
4.8%
mean field
4.6%
Table 2: Classification error rates for the data set of handwritten digits. The first three were reported
by Hinton et al (1995).
digit log-likelihood score
0
-0.447
1
-0.296
2
-0.636
3
-0.583
4
-0.574
5
-0.565
6
-0.515
7
-0.434
8
-0.569
9
-0.495
all
-0.511
Table 3: Normalized log-likelihood score for each network on the digits in its test set. To obtain the
raw score, multiply by 400  64  ln2. The last row shows the score averaged across all
digits.

5. Discussion

Endowing networks with probabilistic semantics provides a unified framework for incorporating
prior knowledge, handling missing data, and performing inference under uncertainty. Probabilistic
calculations, however, can quickly become intractable, so it is important to develop techniques that
approximate probability distributions in a exible manner. This is especially true for networks with
multilayer architectures and large numbers of hidden units. Exact algorithms and Gibbs sampling
methods are not generally practical for such networks; approximations are required.
In this paper we have developed a mean field approximation for sigmoid belief networks. As
a computational tool, our mean field theory has two main virtues: first, it provides a tractable
approximation to the conditional distributions required for inference; second, it provides a lower
bound on the likelihoods required for learning.
The problem of computing exact likelihoods in belief networks is NP-hard (Cooper, 1990); the
same is true for approximating likelihoods to within a guaranteed degree of accuracy (Dagum &
Luby, 1993). It follows that one cannot establish universal guarantees for the accuracy of the mean
field approximation. For certain networks, clearly, the mean field approximation is bound to fail|it
cannot capture logical constraints or strong correlations between uctuating units. Our preliminary
results, however, suggest that these worst-case results do not apply to all belief networks. It is
worth noting, moreover, that all the above qualifications apply to Markov networks, and that in this
domain, mean field methods are already well-established.
72

fiMean Field Theory for Sigmoid Belief Networks

The idea of bounding the likelihood in sigmoid belief networks was introduced in a related
architecture known as the Helmholtz machine (Hinton, Dayan, Neal, & Zemel, 1995). The formalism
in this paper differs in a number of respects from the Helmholtz machine. Most importantly, it
enables one to compute a rigorous lower bound on the likelihood. This cannot be said for the
wake-sleep algorithm (Frey, Hinton, & Dayan, 1995), which relies on sampling-based methods, or
the heuristic approximation of Dayan et al (1995), which does not guarantee a rigorous lower bound.
Also, our mean field theory|which takes the place of the \recognition model" of the Helmholtz
machine|applies generally to sigmoid belief networks with or without layered structure. Moreover,
it places no restrictions on the locations of visible units; they may occur anywhere within the
network|an important feature for handling problems with missing data. Of course, these advantages
are not accrued without extra computational demands and more complicated learning rules.
In recent work that builds on the theory presented here, we have begun to relax the assumption of
complete factorizability in eq. (15). In general, one would expect more sophisticated approximations
to the Boltzmann distribution to yield tighter bounds on the log-likelihood. The challenge here is to
find distributions that allow for correlations between hidden units while remaining computationally
tractable. By tractable, we mean that the choice of Q(H jV ) must enable one to evaluate (or at
least upper bound) the right hand side of eq. (13). Extensions of this kind include mixture models
(Jaakkola & Jordan, 1996) and/or partially factorized distributions (Saul & Jordan, 1995) that
exploit the presence of tractable substructures in the original network. Our approach in this paper
has been to work out the simplest mean field theory that is computationally tractable, but clearly
better results will be obtained by tailoring the approximation to the problem at hand.

Appendix A. Sigmoid versus Noisy-OR

The semantics of the sigmoid function are similar, but not identical, to the noisy-OR gates (Pearl,
1988) more commonly found in the belief network literature. Noisy-OR gates use the weights in
the network to represent independent causal events. In this case, the probability that unit Si is
activated is given by
Y
P (Si = 1jpa(Si )) = 1 , (1 , pij )S
(27)
j

j

where pij is the probability that Sj = 1 causes Si = 1 in the absence of all other causal events. If
we define the weights of a noisy-OR belief network by ij = , ln(1 , pij ), it follows that

0
1
X
p(Si jpa(Si )) =  @
ij Sj A ;
j

where

(28)

(z ) = 1 , e,z

(29)
is the noisy-OR gating function. Comparing this to the sigmoid function, eq. (3), we see that
both model P (Si jpa(Si )) as a monotonically increasing function of a sum of weighted inputs. The
main difference is that in noisy-OR networks, the weights ij are constrained to be positive by an
underlying set of probabilities, pij . Recently, Jaakkola and Jordan (1996b) have developed a mean
field approximation for noisy-OR belief networks.

Appendix B. Gradients

Here P
we provide expressions for the gradients that appear in eqs. (23), (25) and (26). As usual, let
zi = j Jij Sj + hi denote the sum of inputs into unit Si . Under the factorial distribution, eq. (15),
73

fiSaul, Jaakkola, & Jordan

we can compute the averages:

Y

he, z i = e, h
1 , j + j e, J ;
i i

i i

he(1, )z i =
i

i

(30)

i ij

j

e(1,i )hi

Yh
j

1 , j + j e(1, )J
i

ij

i

(31)

:

For each unit in the network, let us define the quantity
(1,i )zi

(32)
= he,hze + e(1,i )z i :
Note that i lies between zero and one. With this definition, we can write the matrix elements in
eq. (23) as:
(1 , i )(1 , e, J ) + i (1 , e(1, )J ) :
(33)
Kij =
1 , j + j e, J
1 , j + j e(1, )J
The gradients in eqs. (25) and (26) are found by similar means. For the weights, we have
i

i i

i

i

i ij

i ij

ij
i

,i Jij

@ LV
@Jij

i

ij

(1,i )Jij

, i )i j e
i(1 , i )j e
= ,(i , i )j + (1
,
,

J
1 , j + j e
1 , j + j e(1, )J
Likewise, for the biases, we have
@ LV
= i , i :
@h
i ij

i

i

ij

:

(34)
(35)

Finally, we note that one may obtain simpler gradients at the expense of introducing a weaker bound
than eq. (19). This can be advantageous when speed of computation is more important than the
quality of the bound. All the experiments in this paper used the bound in eq. (19).

Acknowledgements

We are especially grateful to P. Dayan, G. Hinton, B. Frey, R. Neal, and H. Seung for sharing early
versions of their manuscripts and for providing many stimulating discussions about this work. The
paper was also improved greatly by the comments of several anonymous reviewers. To facilitate comparisons with similar methods, the results reported in this paper used images that were preprocessed
at the University of Toronto. The authors acknowledge support from NSF grant CDA-9404932, ONR
grant N00014-94-1-0777, ATR Research Laboratories, and Siemens Corporation.

References
Ackley, D., Hinton, G., & Sejnowski, T. (1985) A learning algorithm for Boltzmann machines.

Cognitive Science, 9, 147{169.

Buntine, W. (1994) Operations for learning with graphical models. Journal of Artificial Intelligence
Research, 2, 159-225.
Cooper, G. (1990) Computational complexity of probabilistic inference using Bayesian belief networks. Artificial Intelligence, 42, 393{405.
Cover, T., & Thomas, J. (1991) Elements of Information Theory. New York: John Wiley & Sons.
Dagum, P., & Luby, M. (1993) Approximately probabilistic reasoning in Bayesian belief networks is
NP-hard. Artificial Intelligence, 60, 141{153.
74

fiMean Field Theory for Sigmoid Belief Networks

Dayan, P., Hinton, G., Neal, R., & Zemel, R. (1995) The Helmholtz machine. Neural Computation,
7, 889{904.
Dempster, A., Laird, N., and Rubin, D. (1977) Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical Society B39, 1{38.
Frey, B., Hinton, G., & Dayan, P. (1995) Does the wake-sleep algorithm learn good density estimators? In D. Touretzky, M. Mozer, and M. Hasselmo (eds). Advances of Neural Information
Processing Systems: Proceedings of the 1995 Conference.

Geman, S., & Geman, D. (1984) Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6, 721{741.
Hertz, J., Krogh, A., and Palmer, R. G. (1991) Introduction to the Theory of Neural Computation.
Redwood City, CA: Addison-Wesley.
Hinton, G., Dayan, P., Frey, B., & Neal, R. (1995) The wake-sleep algorithm for unsupervised neural
networks. Science, 268, 1158{1161.
Itzykson, C., & Drouffe, J.M. (1991). Statistical Field Theory. Cambridge: Cambridge University
Press.
Jaakkola, T., Saul, L., & Jordan, M. (1995) Fast learning by bounding likelihoods in sigmoid-type
belief networks. In D. Touretzky, M. Mozer, and M. Hasselmo (eds). Advances of Neural Information
Processing Systems: Proceedings of the 1995 Conference.

Jaakkola, T., & Jordan, M. (1996a) Mixture model approximations for belief networks. Manuscript
in preparation.
Jaakkola, T., & Jordan, M. (1996b) Computing upper and lower bounds on likelihoods in intractable
networks. Submitted.
Jensen, C. S., Kong, A., & Kjaerulff, U. (1995) Blocking Gibbs sampling in very large probabilistic
expert systems. International Journal of Human Computer Studies. Special Issue on Real-World
Applications of Uncertain Reasoning.
Lauritzen, S., & Spiegelhalter, D. (1988). Local computations with probabilities on graphical structures and their application to expert systems. Journal of the Royal Statistical Society B, 50, 157{224.
McCullagh, P., & Nelder, J. A. (1983) Generalized Linear Models. London: Chapman and Hall.
Neal, R. (1992) Connectionist learning of belief networks. Artificial Intelligence, 56, 71{113.
Neal, R., & Hinton, G. (1993) A new view of the EM algorithm that justifies incremental and other
variants. Submitted for publication.
Parisi, G. (1988) Statistical Field Theory. Redwood City, CA: Addison-Wesley.
Pearl, J. (1988) Probabilistic Reasoning in Intelligent Systems. San Mateo, CA: Morgan Kaufmann.
Peterson, C., & Anderson, J.R. (1987) A mean field theory learning algorithm for neural networks.
Complex Systems, 1, 995{1019.
Press, W. H., Flannery, B. P., Teukolsky, S.A., & Vetterling, W. T. (1986) Numerical Recipes.
Cambrige: Cambridge University Press.
Russell, S., Binder, J., Koller, D., & Kanazawa, K. (1995). Local learning in probabilistic networks
with hidden variables. In Proceedings of IJCAI{95.
75

fiSaul, Jaakkola, & Jordan

Saul, L., & Jordan, M. (1995) Exploiting tractable substructures in intractable networks. In D.
Touretzky, M. Mozer, and M. Hasselmo (eds). Advances of Neural Information Processing Systems:
Proceedings of the 1995 Conference.

Seung, H. (1995). Annealed theories of learning. In J.-H. Oh, C. Kwon, and S. Cho, eds. Neural

Networks: The Statistical Mechanics Perspective, Proceedings of the CTP-PRSRI Joint Workshop
on Theoretical Physics. Singapore, World Scientific.

76

fi
	ff
fi 
			 ! #"$ % 
'&)(+*, --.0/1, -3254.

C

6789: ;=<>
-?!@A8	%&;=B0>
-.

D)EE3FHGJILKNMJONDPORQSDUTWVXMZY1[0\:]_^`IbadcfegY1DUMJONDPOihjIlkb[\nmoapIbklapVXTW]
q

[Y1rtsvuwMJVXTx[\nmoapDP^yDPapDUMJ\:DP]

z|{~}ff$}}{H

~ffgpbp$

P0=0) Jw
030lH'
010Hj0)ff


+yp~
SyH7L!~7L`3y`P5! 7`l5ybyg3y3yZX:3
Sff~7+`#y~=0ff35y33`~y+305770!U~0737+y
~Sff!7Syyy``g`7y!ff`Zyybb
7y!ff|
J373y!37`bH`H|JH`Zw37
#yH03XyLJ!X`3Z! 7
:7~3PZ)y`$3y7Z53:5`y$P5yH7y
`3!Sy)3yy
`yw~X0L!SJHffL`3Z7`37 
yZ= 
37 573:`Xy3
$
pN :)U~ lP#: $  	1ff
fi

 !"$#%&'(*),+-(./012'3.(4(.53.%!%)63.763."%8:9;%5'%5

+<)=>)?=A@Bfi6)'#3*#C3.D)63*E(*/03.6+B3GF-3*)H/JI&:K(*LNMOPQKRTSU'%)6VWK/LNMOPXKR
SU'%)6VWK/LSY=%76ZLT[]\^)_/LYMOO	Ma`284bff6c^d	e-3.)?@B)_fgJ#%<'(*)Afgc)6#h)6
+<A)=5!i6+B3GF-j%53.7)=3kl#%&'(*)l+aE	3.#%l!7(.3.>@-(*73.%<7>)3*Z8ZmY8n"%8*LU3.
fg^W	%afo)=>)N6)'#%p)g>^#'(*)Lp#'(*)q>r%27(.(*/?!+(*s/#L	6)'#%p)t>^%5(.(*/!%)
!+(.a/#L	fg,fgp)u)67(.'#iv6:)6^3kg)g7+(*a/#wjx)=%l3k%27>)3*y)=>)Y:)6z3.
76)'#%D)L%)=K'u+3.%"j)=%l6)'#%D)^#%<'(*)NaEr)=%l	e-3.)3k%"7#'(*)u#%&'(*)8
{ +B3GF|3*)_/}3.;~3.!+B)p);6'%?q+LB@'%)^)r)=?(*/%L#~>),(*6),3.
6!j>++(k3.>)3*^%)r>3.(./i)=?!)r3.7+|)D)^%8-b$)=%(*"(T#%53.13*)r7/LB<
3.)L@B5)=y6i)=>)j$!5"%(t'(.53.l+#C3ky3*)+D)#(q(.af
Y++|#)6y6)>)6?(.fI&>WKWLMOO`28Bb)=%66z+z7a/c@|,@-6#}$6!
@3k;+3.3.+(*t"'(.>)3."j=af%e-3.)Y!"7'(*N>A)67@B,6(*E#8
 (.63.h)=%y>++(.3.>)3.J#%73.L:(.3.Wc7K#%(N@-6##3.>"3.?5	F"'>)3*LT+
c+-(./h&'#7D)(r(*8N$K#%(r@#o#3.>"%3.i'6(."3.(;#%3*+)3*7)=%
%5(@B=E	3*'%t!+B%p)gZ#%E	3.^)6")=%ufz3*)=y(*"3k(-#%3*+)3*5)=%;)'	
(.(*/!@-6E#c@B=E	3*'%a8	9;%r)63*g)6!'!r7(-@|=E	3*'%guu7p/5!+B%p)u
+B3*@-(*8  #3.>"%3.q6+B#)6j?6)tZ7+|D)tgfz=3k=7)=6r7(./7'!+%
)3*z(*#)6y3k3.)6/8%4/c)6$7(.>"p'@BuT+|3*@(*;#3.>"6z3.N@)3.%#8b
(-(.3*<Y6!^7+|D)t>^(*4(.3.>@-(*u)=7)=8pSUl(.3k73.>)6Y(*+(.'3*@(*N#3.>"6
%A}"3*El)=%A7(./i'!+)3*N<Y(.3.>@-(*,!+B%D)z=3*"=%u+23*3*)H/8
bc	F"'%>)3.y)WKu3*)N3.g<)63.!+B3*@-(*u)6!=3*El(k(U)=%,#%3*"y"(.89r<)6c%
1#3.6)3.%"'3.=57l3.!+B)p)u"(.u<(*Y3.!+B)p)N%8SU76)6')Y)=%l@B6)N+B
3*@-(*^	F-"'%>)3*N"(ku)=%c=EA)6!@|;+6p)6#u#%<'(*)gfz3*)=c#3GBp)g+
#3k%"!)67)=%3*^#%3*>@-3.(.3*)H/8


, --.~ %%-!;3X
;N=0
gp
!fi9P
q~8!	%&%_		&%%0;

fig%,-

Y?*?t<,.rg.G*0.d%!%6.?6.%B$*G
*i.gu!6gr%!%6.r*.Na;|i|Dn%*%2***5*6
7**ff&.<2*642.>.6pk667.j*.&,.*42Z.**
%&*q..A&gY>	|D2%76^>%t<g>r*5.7hK6.
7Nky,..az.?66%6!,%2.%j!%7%&*N.N6c67p6B%,
*$%!%7%6...-;k6B.$%??q*.*}%&*r*.
.?.%<7>.fi.?6h60p6N%i%>*ff^K6*Tzsta4%c<
.%<7>.J*6*r.!%7	J.ff%1*kY.%>tY.j!7>7.j	.ff
.%<7>.yg6jB^&..?_6BGp%,.N%?gay6.?>B%,&uB6y6
6.%cz*-Y<ZY.r.,.~6>~p66,6}%jN~B*?60>%
z*Cff%afi*g>7D>*ZU<?k6:<?>yp6	_%B%p
C%56!pAN%7A!%d.p*C	|..%}.azAkl}kz&*u%
%A76Y.<t>uT%,6.%%
 >Yglg.}..,65-%<.z1>>r.*szYN6y6pu
c.%<7>.<$%c.%>ih%.y2fik%27>*%	7k.*Zfi$p
>BA&gY>	|u%A%%^Y%2*Bij>>.pu4%7<.N..,.czk
6.%ff>|yikyB**qr*%%~*oA%&*c*.$6p6
.C.,>k*A>B	l6>*5*.C766UkA>h
%aYK
>zz%Y>25.q!%>*.*?K67*!%7k%,*q.p*4z66*
?%0Y.6K6***r!>*-..*H%o<K6*
>D6k%7>Yl<l.5>*cgz>1.p65pz%K>6*
KY^5}>B>^k66Dr%&*u%.-n*-%*zz%666.Y->^.z>
k->*-B6;%c	6*;>,.BYk^.;66..y.?.$>;>B;tj.
*Yz*7%&*z%l%	%.66,T	6*Y.u%,6y%**;
rk%27>*N.t**jp6*>.Yz%%N2yk%27>*5%.5|^>*
6y%66sckK6*
Kzz%Y>..>B.7>^%%_%7<..p6ApK*k4pq%
%6ysr7*.;7u^*%>*$u<k.%
 %!>~6D60k$.r>B,z..|j~~K60*.?5;z*d_g
Y
_KBA^%>*:Y.!>?kC!>2.6^*%?>..ABtc>c!
662.*}.J}6B77}%^.J%%at$>1!166..}.g
%i%rk*a>*62>i6Y2%^<?.r^k$%27%&*^*.>|tj>7
.utr7*%>.5&..u5%Y*T^.56B!6%	_%5%&*
.Y*6y*.>  d.fi6z.<%1	6.6o67p.yl%&*5*k6
g.G<%~67p.i<fi;.%Zz	2.*HT	>:q	.6	U	>T.<2*6
2Nn*6fi.%D*h6.z>2z%1%!%6..*7>
%-*.Z%l%%5*0u	6*HAklg.G	%az~>Ag.G<%
67p.!67.!!*K6!.p**.0KB6k*UYk.j.d}6}.fi%
B	^ag|.Yku%_g*%1pc76!%Nk1.cy.-%lg.G<%
.*uBl!-%6}.cB*	%7.|.7
Y%..4%%6U	%q>BkZ<.*szakK*?ugK.fiz-**;
g.G<%77Dkt<N*.Y7tz*5_g?HpBtZ%>*izk5kq5y%


fit%D$:--.it	--	gYlG%du-%-q.it%DD%KD

%%-*?*	dff
gfiff0ff!	%ff6	%5ff|26ff%!%*	$6"!g2ff
# %
 ff $7ff $kff &%'(
<ff )?. *	 yff 
+*g. ,
<$ % 67ff .
<ff /%ff )7B*ff 0	1$ff 0ffff 2"34!N 
 5
# %
 ff $7ff $kff &6879ff9:7<;lff =
 *Nff A$~ >=6? q@ %ff )A34!N B
 5DCr 
<	 &
 6(79ff9FE@GIH+
JK$MLff6
79ff9ffN	;K$O0P
 < *Q /$g% *	 A:R| KS
J	 ff $ $  >2*g T:% uff $ UuV
  

<ff
 ). *	 W q.$ X6AB M6 (   #  *	 AY6	%Z
 7ff [6*	 5ff 
\%/
 B6	M	$ 
@ u@ >:ff 2]M ! Nff 
+$ 
<  $ S6ff 0 % *Z^? \ 7._ !ff ?> @0? 	ff 2* O0
%` *a
 T@ 8	 .$ *	 Uff 6ffb0 % , 6c
 @B)ff 
$%B
 *g. ,
<$ %b
 	 $.$ .	 $ #  *	 
EW...$
 M 6 (%r :$ ?]
 B<*g Nff 
Z@ N $$ff K$ @0b
 !ff Z
 d Mu %ff [.Z
 
J	 x* 0
ff
 6	@0@ #  *	 feb)%< *]S .%1
 *tff KMBff 1d!Z
 	 [-* :>2
<ff .0 %  @0V*g. ,
<$ %
	
 $. *	 $I
<ff ($).ff )Lb
 $ff 0ffff A(IB >:%	 2.g  #  *	 ^N"? M0 6 (
 . *	 $V6
Cr 
<	
 ?ff W
 H+
4K$MLff%':ff$M*g II7ff Ud.3,C; 
	 W
 5hHi
J))$MLff679ff9jU;K #  *	 2kz$  6
. 6l
 *tff Tff m
 	 $.% 
nIoWprqssgtuXv+wSxzyzqy|{8q}h~&xiff@fvbIffqxSy(qyv++A	v+	~&}h
3J$ffB	*	;q6}*ff0	c$ff0ffffD	$)MNff
I*Nff
8%P
<ff)


]$_ff<<ffK
*]%z% U) ff  z$ff|	)*	d66`gnff6s%+$ff|	)*	6	AUffI$K6	A
$ }>^%b.ff+%0*	m)0	&$%bM>B ff %ff6]%0.	}>-
&ff..$23J*tT
% 0 * 	_;K6io
 %%
 ff6 P.ff) 34MM 	$0;1%
 
0 * 	&%
 ffV	U?*$[*g[*]..86	!7,$67
.ZK%
 7
 W6 W $Uu
 W6N
 ff
$ffB	* 	
 $2 *6:
 ff!
 >A%
 1u
 ff

 ._0ff $
 -$ff$ 
ff
I% PK% 7
 :
 }*ff0	1$ff0ff2 ff2Y "
? c
> $6&
 \<
 ff/T
 <z
* *$
0 " $6 * 	fi-@B	66@6"<
 ff
 6 
$ff$ W34N
!  i5;
C 
 	
 &6_79ff9FE;.
 ffu
 [K
 $A
 Bg
 ff
i. k.$MM 
? z%
 ff[*:/g
* [7
 ` ff7
 ff:
b
ff
 6 
 %fi.ff0	[$ff0ff2 ff2b
? i|fi
 %%
 z37<l
; ff$*tW6b7
 ffUd-3,r
C 
 	
 5
H+
JK$MLff6+79ff9jU;KZ
6 ff$
 :6 $*	$
 ff
(M>*V!
 	%+65
 ffU6-ff
 a3g	Y
; A
? )*	$
 ff
Xg
* .,<
 $
 %
67
 ffc34(L>:?
 $$Tg6$79ff9:7<;K@
 ]6 	
 i$$ffK^ ffy
 B?:* t
* yfftffy
 ^* t
 $
7
 . 	c
 ff
8
 c)M
H Y

 $S)M.M K$ rffM*t/6@0
O Pc
> V *ZV
 ff
+%
 c<
 ff)
 >|`
? PN%<
 6 >
5d6  &X
X+ ) <
 ffZ6	7
 BU7 FM 0
O b>fi2Y
 6 $
> ^6z
 ff4

 6 2 /
I
 	 ff$Y7d6  \ Y
 %<
 :$
 $%
 )!
 ff ff6$t
* P k&%
 " .r ff%
 cc
> *@0Wg
* K
T >
% 0 6 $ 	 $* 	
J 	rA% W!
 	%
 ff6 	$db $6 )r
 ff
zy
 ff
 d%%
 ff6 [1*]Fz3JU;K
O0d67$>-F=6 AYff
8 *.*]f%
 l?* !ff$$0@
  l 	4WMff V 
fiI8`JQi  
"     : _	 ff     J ) KM  

 "  4   [+:   ff <P  cU ff 

 4J      :VU     ) b B  


h         2 - : m J  4 P ffff,$gAK       `z3JU; 
fiI8`JQi @
  
   /Z B   J        44ff F       : z3  ; U
 ff   :B2 
 BM  4      B 
  ]
     -  
  S  
ff

fiB@ff"

	4KfiF)FSgJ<BJJ@bK@K/V$+,-J:.bJJ

	fiff Kc	4 MffM!V#"cB4M%$W'&(\f)@Mff,(*,+

F-J.I0/

*1+32"547698;:72'=<>4
"

b@).S"B?@r A"B6C* + 2"54

DFEHGJI?KLMNE,OGQPM;R0SUT%P?KWV,XKWT%R0K3SNYZM\[#L]SN^NLMN_  X%T%`KLMNT%P?abKL P?KI(P?KW_cMNTfiI]GQRPdN`KWT%SNI?KW`eOgfih :kj,2'l4 d
GnmoOGQP3R0SUTpI]MNGHTKW`qGQT
MNEHErMNT%P?abKL@P?KI]PsSNY et
usv K>P?KWR0SUT#`
_cMwSNLsP]KW_cMNTpI]GHRPxYSNLxK0y1I?KWT%`KW`
EJSN^UGQR>[%LSN^NLMN_cPd,azKWEHEn{|YSUX#T%`KW`}P?KW_cMNTfiI]GQRPdGQPxMNT
GQT v KL]KWTpI]EQ~qP?NK[%I]GQRMNEzP?KW_cMNTfiI]GQRP=I v MIL]KYL)MNGQT%P;YL]SU_`LMa@GQT^
R0SUT%REHX%P]GJSUT%P;a v KWTKNKLI v KLKcGQPM
[-SNI?KWTpI]GHMNErR0SUT1GQR0I t%usv K=SNLGJ^UGQT%MNEZYSNL_X%EQMI]GJSUT}SNYazKWEQEJ{|YSUX%T#`KW`P?KW_cMNTfiI]GQRP3YSNLs^NKWTKLMNEEQSN^UGQR>[%L]S{
^NLMN_cPz,~c;KWEQ`KLWdp@SUP]PbMNT%`1R v EQGJ[%Y 2WN14 GQP #MNP]KW`cSUT}MR0KL]I]MNGQT[#ML]I]GQMNE#_eS,`%KWE tp L~,_X%P]GQT#P?,G
L]KWR0SUT%P]I?LX%R0I?KW`
I v GHP3`K0T%GJI]GQSUTGQT{|MNEQXKW`
EQSN^UGQR 2' L]~1_X%P]GQT%P],Gd WNp4)tZu@v K;YSNL_X%EQMI]GQSUT}X#P]GQT^
MNToMNTfiI]Gn{_eSUTSNI?SUTKeSN[-KLMI?SNL=axMNP;#L)P?I\^UGQNKWT5fi~bML)MNE MNT%`1X%LM v _cMNT%GQMNT 2WN14 YSNL=^NKWT%KLMNE
EJSN^UGQR[%LSN^NLMN_cP;I?SN^NKI v KLla@GJI v M
R0SNL]L]KWP?[-SUT%`#GQT^`%K0T%GJI]GJSUTiYSNL`KYMNX#EJI>EQSN^UGQR tusv KeP?I?LMNGQ^ v I?YSNL?{
axML`K0y,I?KWT%PGJSUTSNY7I v GQP\YSNL_lX#EQMI]GJSUT 2 L]KWP?[-KWR0I]GJNKWEJ~Nd-I v KL]KWP?I?LGHR0I]GJSUTSNY(I v Ke`KY'MNX%EJI\EQSN^UGQR`K0T%Gn{
I]GJSUT 4 I?SK0y,I?KWT%`%KW`qEJSN^UGQR;[#L]SN^NLMN_cP3I v MIsa@GQEQEZ-K=GQTfiI?L]S,`#X%R0KW`
TSaabMNP@X%P?KW`
,~}P]KNKLMNEkMNXI v SNLPd
K t ^ t2 xMLMNEr>KWEQYSUT%`d WN% GQYP]R v GJI?Nd WNNU4)tn3 SNI?K;I v MI3GQTcI v GQP7[M[ZKLzazK>a@GQEHESUT%EJ~cR0SUT%PGQ`KL
I v KEHGJI?KLMNEQPbI v MI@MLKI?LXKGQTI v KR0SNL]L]KWP?[-SUT%`#GQT^e{|MNEQX%KW`P?KW_cMNTfiI]GQRP t
 GJNKMNT%P?abKLP?KIP]KW_cMNTpI]GHRPeI v K
abKWEQEn{|YSUX%T%`KW`P?KW_cMNTfiI]GQRPYSNLeK0y1I?KWT%`KW`EQSN^UGQR[%L]SN^NLMN_PcGQP
#MNP?KW`}SUTI v K>SN[-KLMI?SNL *,+ SabKNKLWdI v K;SN[-KLMI?SNLsGQPxX%P?KW`GQT}MI?SNI]MNEQEJ~`%GJmZKL]KWTfiIbabM~ t 1GQT%R0K *,+
GQP\MNTpI]Gn{_cSUTSNI?SUTKI v KY'X%T%R0I]GJSUTi +62*,+(4 GQP\_eSUTSNI?SUTK t D>RR0SNL`%GQT^I?SI v KYMN_eSUX#P\T#MNP?I?KL?{
u MLP?1G3I v KSNL]KW_ 2'u ML)P?,G|d WNNU4 KNKL~_eSUTSNI?SUTKSN[-KLMI?SNL v MNPMEJKWMNP]Ie%y1[ZSUGHTpI t(usv KqP?KISNY
abKWEQEn{|YSUX%T%`KW`R0SUT#REQX%P]GJSUT#PcSNYM[%L]SN^NLMN_  dz`%KWTSNI?KW`l 2'l4 dbGHP`K0T%KW`I?SZKiI v GQPEJKWMNP?I
%y1[-SUGQTpI\SNYz +zt-usv K%y1[-SUGQTfiI\RMNT5-KM[#[%L]SUMNR v KW`5YL]SU_-KWEJSa,~iGQI?KLMI]GQT^ + SUT5I v KKW_e[%I~
P?KI t% TRMNP?K  GQPbT#GJI?K;I v GQPsGJI?KL)MI]GJSUT
GQP3^UX%ML)MNTpI?KKW`qI?SMNR0I]X%MNEQEJ~}L]KWMNR v I v K;#y,[-SUGQTfiI t
usv KGQTfiI]X%GJI]GJSUT-K v GQT#`I v GQPsX%P?K=SNY I v KSN[-KLMI?SNL\GQP@MNP@YSUEQEJSa@Pa v KWT%KNKL *,+ GHPsM[%[#EQGQKW`I?SM
P?KI3SNYEQGJI?KLMNEQP " 1TSa\TI?SeZK>I?LXK=GJIb[%L]S1`%X%R0KWPbI v K=P?KI3SNYMNEQE-EQGJI?KLMNEHP7I v MIsMLK;P]I]GQEQE[ZSNI?KWTfiI]GQMNEQEQ~
`KLGQM#EQK t D@[%[EJ~,GHT^GQIcI?SP]X%R v MP]KISNY;[-SNI?KWTpI]GQMNEHEJ~`%KLGJM#EJK
EQGJI?KLMNEHPcGJIe[%L]S1`%X%R0KWPMP?KISNY
EQGJI?KL)MNEQP1TSa@TI?So-KI?LX%KNd SNYI?KWTEQML^NKLeI v MNTI v KSNLGJ^UGQT#MNEzP?KI "t ,I]ML]I]GQT%^ia@GQI v I v K
KW_e[%I~
P?KI@MNT#`GJI?KLMI]GQT%^cX%TpI]GHE-I v K=%y1[-SUGQTpI3GQPsL]KWMNR v KW`qI v X#Ps[%L]S1`%X%R0KWP3MP?KIsSNYI?LXKEHGJI?KLMNEQP t1 I@RMNT-K
P v Sa@TI v MI=KNKL]~azKWEHEn{|YSUX#T%`KW`R0SUT%REQX%P]GQSUTGHP>MR0SUT%REHX%P]GJSUT5X%T%`%KL>I v KMNT#P?azKLP]KI;P]KW_cMNTpI]GHRP t
 KWEQEn{|YSUX%T%`%KW`
P?KW_cMNTfiI]GQRP@RMNT
I v X%P3-K=1GJKabKW`MNP@MNTM[#[%L]SWyGQ_cMI]GQSUTSNYMNT#P?azKL\P?KI@P?KW_cMNTfiI]GQRP t
 TYSNL]I]X%T%MI?KWEJ~GJII]XLT%PSUX%I I v MI YSNLz_cMNTp~[%L]SN^NL)MN_cPI v KsP?KI SNYZazKWEHEn{|YSUX#T%`KW`R0SUT%REQX%PGJSUT%PGQP
K0y1I?L]KW_eKWEJ~}P]_cMNEQEZMNT%`}[%L]S,GQ`%KWPzMNKL]~[ZS,SNL3M[%[%L]Sy1GH_cMI]GJSUTSNYgMNT%P?abKLsP?KI3P?KW_cMNTfiI]GQRP t# SUT#P]GQ`KL
I v KYSUEQEJSa@GQT^[%L]SN^NL)MN_  a v GQR v
v MNPsMNEQP?Se-KKWT`#GQP]RX%P]P]KW`,~
xMLMNEMNT%`i;KWEJYSUT#` 2WNfi4 
 4ssff] 
U43eff]
 4@ff3
)N1|s);g|b'k|W?;30|)=)ZQ?=s||||x|()?
)|s|@|?|(|]b'|)0|)3(-)0]|)fi13-?|b
 ?b?| )| | ?>N |W?\;)k)|)fi


fi7Q}zpU#,p

z%U%#piQ}z11#bsn


	fiff		!!"
	"$#%!#&!ff(')!ff'ff	*,+!.-/ff('!#%	1032457689	:;!<ff=?>@BA!(
	fiff(	C<D'	EF<ffA
<!"G(
	$=H>I@IE(	"J!#%fiLKNMO#%P(<'!ff,
EF!)	O<QR<7ST!'ffU'ffVff(
E(+JEF'ff('!WXffF'!#%	AC'P(!')(')Y	)-AN(
	
#%3Z['&#%\]	.^		_89<!"a`8!<ff
(!'&
W,b"
,'(acCdL<!"adeS
J'ff9+!E(\J	*f<QEF'ff	ff9!	
	Y	E(
	g)e'
W,#%!"J')(')!ff
&"ih
jQS<X#%*,+[)	*,	P(<QE(-k+[<')El')	EF<&ffl'ffL+!EFmYQ<Q\J)	UnE(*o(!	V*R
!'#V#%!P	E(+J<QE((ff1^(
	
Ep!)	ff9C<V+!E(WEF<*qK,A!<!"
_;S(!	E(	fi'ff<Ql)	<ff
	r+!EFPTnEl	<#F(
	r#%*,+[)	*,	P(<QE(-/')	EF<&ff9
ff	gEp!)	ff<QE(	U

"!	n	<Q	"s\P-ut1v?5wKyx8pAi
	E(	,Kyx#%!ff('&ff(ff1^(!	{zff(EF'#%(|$EFJ)	ff1'{K,AD'7S}	S)Ai(
ff	,l')(


!	W<Q(')a<ff<'&
E(	S
~ r(!'&ffN#<ff		!!"!	"rff	*R<P('#ff#%!#J"
	ffN['Ttyv?5wKyx8pS ~ Tff(!!"g\	\PY3')!ffT(!<Qff(!#p
<Off(')(!<Q('){'ffL
?JffL<EF<QE(	V'*b')('
WR#<ff	SDNO(
	,#%EF<QEF-A'L#<\	fi	%3+	#%	"(!<Qy*R<#%*R*,Jff	!ff	g3
e)	"
W	y\J<ff(	ff'&[W'Y	gEF'ff	yff(!#Fa!!"
	ff('E(	"O\]	J<Y3')
ES!
EL'!ff(<J#%	A!<ff
ff(!*R	<g3
ml)	"
W	\[<ff	l#%P(<'!ff^'
EF*R<Q('),(J<Q?\J')Ep"!ffH
EF*b<)-fiZJ-,<!"R+	
W!'&!ffC
EF*R<&)"
i}AJ	%;+!EF	ff(ff	"$<ff(!	rff	TWE(!!"$'&!ff(<!#%	ff(
	y)e'
WVEFJ)	yff(#F
	*b<Q(<3h
jm8ni58C[(cCi58pFdp>[5i8
_8cC5i8C[i58pw]vJ>v?5i8
 ff(ff(!*,	
EF(
	E?(!<Q(
	l;
e)	"
W	l\J<ff	L#%(<'Jff?(
	L'
EF*R<Q(')R(!<Q^		.-b'ff?<r+	
W!'
\J')Ep"iS
e')
	')(
	ELni5n1m@!89
ELcCi5nLe@
89nmffE(*ff(EF'#%EFJ)	ff'X(
	g3
e
	"
W	,\J<ff(	U	R<QE(	R'(
	,ff(<*R	Rff(')(!<Q('){<ffyl')({KNMQh	!!"
	"ff	*R<P('#ffg"
;	ffy
y"
EF<m
<P-z"
		<ff(')\J	|fi#%!#&!ff(')iA;'7S}	S3<U#%J#!ff(')O"
	EF'Y	"bE(*<UEF!)	l')(^	<QO
	W<Q(')$'(
	
\;"!-A3<QL<7S
 	19<9,ff(
e(J<Q9<V*R'&
E?E(	EF*U!<Q(')(!	!3+'P?+	EF<QE#<XmY	EF#%*R	y(!'ff
'P)	EF<Q\J	L	<Q3
	ff(ff<J"/)	<"!ff*fi!#p$\		EE(	ffF!)(ffSJ!ffF'"
	E(
	y)e'
WV+	EF<QE
0!2 5u8Hty5wKyy8

	EF	fity5wr8^"
	
	ff(!	g*R'!'*b<]ff(	T'	EF<ff#)ff	"a!!"
	E9(
	5w#&<ff(ff('#<n8^Ep!)	ff9VS[ty5wr8
'ff9(;!ff'	fit1v?5wr8^')(!
9(
	gE(	:;!')E(	*R	9C)W'#<D#ff	"!
	ff(ffS!e"
	%[!	
 2 5{8H0325n0!2 5{88
 W<'^	V')	EF<Q	V(
	U	*,+!I-kff	1O\J(<'a(!	fi	!!"
	"k#%!#!ffF')!ffl?<b+JE(WEF<*oK
!'&#FX	g'"
	!	Ufi  5wKfi8pS
Jff('"
	EH(!	l	%	#%(ffi(!'ff?*R;"!')[#<Q(')R
E	%3<*R+J)	KTMQS0 2 4 5768?mJpcCJFdmS
!	rjm8
'ffy#%P(<'
	"u'(
	Xm[pc[FdmE(	"!!#%g9KTMU<J"{(P!ff  2 4 57681fedmSN3'!#%	Rdfi'ffy<ffX(
	,!')	Ep<i#%(<'!	"a'/<&D<!ff	Eff	(fflCKNM1
EL<Q+!+!E(m3'&*R<Q(')$<#%(!<-X#%'&!#'"
	ffl')($<!ff	E
ff	ff(	*R<('&#ff'X(!'&ff#<ff	S
~ (
	^		.-	%
<*,+J)	\(i5nLe@
8<!"cCi5nLe@
8<QE(	+!E(eY<Q\J	kE(*(
	
6eE(	"!J#%C(
	g3
e)	"
W	fi\J<ff	S[lm	Y	EA](!'ffl!<ff
'3Z[
	J#%	ya!	(
	El<,Ep!)	r
l#%3
(<'!'&
WV(
	y^	<QX
	W<Q(')aN!	yN(
	ff(	yI')	Ep<ff9'O(
	y\;"
-O'ff!ff	"X,+JE(;"J!#%	L0 2 5768
Eg
Si
	U	%	#%y?(
	V#%3Z['#%('&
WO'&
nEp*R<Q(')<Q\]!L		I-]ff1ZJ-3'
WO<Q\J''I-/'ffL(;!ffL	+!
)3#<?<!"G"
;	ffr
V!<mY	(
	O"!'ff(<ff(E(!ffr#%Jff	:P!	!#%	fffi'fi!<ffU'(!	bEF')W'&!<CEF*fiJ<Q(')
	!!"
	"Xff	*R<P('#ffS


fi
y[
fifi
V!!y(!QU(

,
!FQVg;!)Q)Prk(
p)!

Rl

rq
Pr
r%P(s
Q(sgw!(u(Jg(
,/(J!%g,D
Q(JFQ(,s
;!)Q)P^,!O(J?F$)9
P9
RQgPJ]F!%ll
(

u!O;U! fi(!OFQ,{XQ!J)sJF(U(
$
%[J)()L^C
X(,$
;;)J()F!O,!(pRU)(

,F(9
Q()iC,(!p!(FRby

!(3!!%L%,J)R(Q(J)F^F!On9(!^(X(!1[!
(9%!J)()
9J()
$(
y
pg(!r
FQJ(;J!%,(r%J!()J9(!$(
yF!

gJH[mne[CF G I P 
	fiffffff w  
!" ff#$	%I& 	'!y)
 (^
* +
- ,/
 .10  ,/
 .2
gi4 36587u/!O! ,9
 .:0;- ,/
 .pC(;!V ;=<?>%@BA0f ;D<C >%@BA(F E
((!fi(
X((J)fi)e
RR!Q)HG
lFR!fiF
mk(!Q1(
U
FQ1!F;!J%l
O!PP((!)(J7})](!Q

!RP(k(]y;^/l/Q!!FJ I
RQ()XC!lRP(
gJH[mne[L
 KT FM 6 I&  N	OffP?ffQCNS RU TV,w$ .UFU * $ !B ff#W	%I& 	'!Q6ffYX
Z [] W\N	\	]]!=(^ 1![ !Bff# _^a`$b  c!-6ff=(  (FBZ d]!=(^ 1!? !& QJ &!6edwJ fe]^a`$b  ,wg .10
RlU Th,w$ .
gi4 365 
1!(()()F)3),(Q(&JX
!9J
,!Q7;

ikjmlD(!(&
)^J^TJ, n;1((!,]F(Tl!3,!.rTJ%!((!
(J(
g!)(RJ
V(()J&)I,%FJ
RRF(!)(o
 ,Zp1)nJ
 qsr(p!) t uJvvwx.p
N(
ef(!Q,)FQ(
u  G(
$,JI(R!
,!(3!!%Ou)F- Tz{ y RU TV,w$ .fi)
(3O%b(
e(!Q|
 0}RlU Th,wg .,Ji ,/
 .10}RlU TV,w$ .p
r:
 R /Q(JFQ(J^OJ ((!R/ 0~RlU TV,w$ .pB n3&!%X 0~R k!m
W 0 @U] n3&!%;bd ((!R!() R^%J(P^1! R1 ,w .10W ,wW @.p3
((
  ,9
 .k1 ,w$ d > A .0y ,w  .FLRrU G
E
y(!fi((lH(!fiJQ1b
FnFU!Q(a(
p!!lXfi%P!)P!(k
!(&
b(
,,!!U%!FJQ((1^J!
Qkp!)l^V&D(a(a(
VF!Dp!)
!r I;!fi(

%[!)(!
(
?.^LFQFC1bJU 
%p!
)m(PJ)F
9(!QTQ;)

Qa!(%!!()!^]y
%iJ7})
$Q(JFQ((NFJ)9)(OQX!Q()
k
%[!k1 ,w$ .N y ,a ,w$ ..R!W ,wg .: , ,w$ ..p 7u
e ;!)Q)(
FJQF%FO tr;!XJ ;O(
g;!Q()!
  ,/
 .1 ,w @ .
  ,9
 .W ,w @ .

Fg @ 

(
gF!

Q/;O
(,^V(
pkX(
V(Q(Rg^!((!%LURF!Q()Rp!QF%FO tQ(
C J$a(
y)ml
,
()i
hTne[
  N1 F/
 	OffN?ff$ey !M ff#	wI 	!|=V	N c!b
 Xm!&# (FBZ 
  { bFRP`W @ ,wg .m #-\!WffB  #6 I /& g! ,9
 .ff e96 &_\+	 J 	%e #- {  ; <C >@-A 
&

fiY]Y4xSfN__41"Wfi]-]fN]xx]4Vx
OYf-Y&]O1&V_OYON6Y &6O&$H"[fODP&
 B[9kD? %B k   %B kZP  g
fFYckfF[6gYfOQ4kPff_Q_YO[$c&JxY6J&fJO]P]OQ  6\J]J6
fcd1JJ]J
:YYYOY6&\&J4J"]JVYJO
 9m:YY_4
6]&PYY4OY&[&JYJ1Y&Jdk4O_YJH4&\&6JYf][6fVff&OO
f4Q:1h]
Yc]:ff&kf41:YD:VJY]:cYU9N]
Yf-c m1JdJ&
	 "1:4
&-6YfYQJff
 &Y&gPOo
4fYYO fi 
d4f
6&\&"4f&Yc-6YOJ&
 "1YMdJ&f  fY  V$dY"&NW64OPYJ--YQJh1Q&YJJh
4&\&6JYJ"Q6YOJ&
xYfOJO    P]&   4  16YOk4QJJY]6YOOWY  fUY&[&J
D& ]64OOYQ   
 Y6f6OO&JOfMY66 fW O fi ]&6  f&W16YfJY fi  
Yc[4YOH!$ Y"Y]P_$?Y1OcxJYYJQJYf"]x Ox6&YJJh
W YOO YNx  V&6f-JcOHx1P6YQPY  xcYc"Y4fYYJ
]66]J

  #
%$'&(
  #
  
  )
%$
Y
*  
  #
 = &(
  #
 
]&6+
%,P646Q&&6\YJ&.-
Yf]Q]Q4OJM1 0/ JxOYJ
6YfJBV4cf&OO
!"Y[4YO fi  f4ff21JOYc[4YO/Yg3Y
YQ 54 

QMYg YOJ f  6-PQY]fU YOJ ]]JJ6cOYJ Y6!]$6J
f YWYJffYO
4fJ OQf/7c 6Jf]O/Q"]&\J6O4Og6YOJJdZ O? YOJ
Og1JVfg]8JYJY4OOY&29]4fSJYJF\k6f6YOJJZ O6YOJfgYc6
]Q_d:<;=]VJ]YdJJd10/ ?Q]&JYJH  J64Yf&F]x1OJDVcQ
6YfJWYYYJofY$]:4B6YO66]JOo46Q&&6$[6YfYQJg]hJ$]
]JJ66fOMY66f4OYkM1YfJVYOgYQJ \f6YfJ1MYcYD
/Y46Q&& O&J4Q[$J 6]JYh4JY]POf4OQ&>
 
YQJJ
]_4OJk1P6YQ"4?
 f.fJfQff&OO9 f$Y@]-]P[Y4O fi 
fQff&OOh9" O
 ,A'& fiBC8D8D8DEfi FB G;='H C8D8D8DC <;='HJI
_YJ"Y fi x AK& fiB88D8D8DEfi FB <;=LH 88D8D8DM <;='HJI k  ,
N8OPRQTSVURSVWXSYZQJ[WM\^]SVQT[V\`_aXJb[VUaXJ]cQJd2]e6UYZQJb`b^QaefSVW2U_gQJdCh@Ugd8SV\`QidLSVW2XTSSVW2URjk\^dM\`j'XJb[Vl2b`Ug]XT[VUSVWMUmM[VUnYZUo[c[VUapLQid2Uo]gO
qEr

fis:tuvxw<y
z{|~}@{ C"3C"}@%?L6<3a6C{"|{|TL"aaC{RB|T3{|T38C3{}~"|{|J
KC"88|'"'"{ C'"{'3Ca{""8|Ja"""8
:8{| a|{""}88@<B33{"%?:{"a{6{ {{M?8
@B}83"|T88|CB}@{|T3{+a{6"C0B~|K"B|{}T{3{MKkn?{B|T"}83<3C8
 ':x'a{@aC":CZV{"""CC}88"c:x: k"'88|%8}@{"}8B3{
|TC"k}@{B}8"3{+"|T{|T338C+""|T{}T0
 '::: a{""|88|C"}@CRa{~a{3}@{<}@KC"88|"6{3BK3{{
B|T"}83<3  
8?"RB|Ta@B%L"?LC>}@{G}@.8|L"3CEGk}@{G}@xa88|J"C8
g"~TB3C:}8axn?{|J"3CC"C}@{B3CC |+3a8|T:C3|kCB8}8G"
6@V}@{<}@8L:{<}@+{>"+6C|CB8|a8{:CZV{""C}@{"}8B3{"
"}@{BaaC 8B"{{?C}@CT|T3{a{ no {|"a"}@ffB|C}@{""3{{{L{K|J"3C
"{8|T3B3+{||J"38CaC"}@{<}@B}833|C{3C0g"}8af
"|88|C"}@{|J3{+KGTB3Gx8C}@aC06:{B8|{3{MK"|T{|TC
  kGk
  .<'
   
 8|Tff'n 6@Vk}@{<}@'68:8C  K"  BL{ff@B}83k"|88|C"}@%{|T3{
3C"|C}@CC"}@a{  ?: a{K"<3  8|T.a{K}@{BLK~B|T0{ {|:{K"|T"}83B3CC
 C}T""}83BffCB%B%:}8"<3"|88||C|T"{"33k:+|T|">`
"B}83{}@B33C"?a{%"3{K8|3a8|T?8Cff%}8"{>3{"8|?6L"8|T3C0
 "x{{EK">n?{|T"3C:@"3B"Z68|Ckn 6x{}@{<}@8
 Gk
'< 
 :C"{"Ca:|T"C|{}@{B3CC|L{E:88|C"B}8{{"{:|T"8C
L{8|:B}@'8|TL}8""?ff"|C}@.6@V}@{<}@80}@{|TaKC8|TG}8a
8C{}@{<}@"|T"ffCf6>B"3|C}@8BV<BCf{>@aaC"}@{"B33{"
|T"C8%EK"+  |n 6@V}@{<}@'|8"8'{|T"3CkZ
 aff:C38|K"8CK  {|k  
 a#J8C'  BB
 a#  8C'
L8|ff#""|T8aCK#fCB"3|C}@kn 6@V}@{<}@'k B'6@V}@{<}@k'|T8"
C"na8K{|T"C8  x|TB3a8K"K"E%a{6C a{}8}@{" {|':CZV{""C
aC }8BaC"B|{}T|K<a8{G'|T"CK"}J|K"""8CaC >Ka8{63a8|T
{EK%a{~6a|TL{a:BR"?n?{~6CR{B}@{<}@|{"a{ 8Mc2:{~|T"3CRCffG:
}@{<}@.{<6{n 6@V."G@VTJ :{B8|K{{EK"B|{|T  T3 .{BZ<}83{
{.8
C

fi?""BfR<<?<<B:'Z<<?2B

	fiff
ff !"#fi
$ff&%'
	
(*),+-+/.103254.626798 :;.=<?>>@=)BADCEF@=7&G;.79+H..5A 
	 2A3I KJML )N.O:3NP.5@ERQS.5TU8VW2XI3EUNP.5@=7&798 :;.=<?>>&2A3IY2
7Z8[:;.=<?>&@=)BACER@=7]\*70327^ERQ_A3)7^QS)BTU42G`TU.aG 870.1E L :TRER@EU7 L .5@P032A3EFQ L Q)b/+H.5TRTU<?bc)Bd3A`I.5IeQS. L 2A 7ER@Q
2TU)BA.f`>97gEFQ703ERQ"hDERA3IK)b@=)BADCEF@=7fi70327fi+/.i7SN8j7S)6Q)BTU4.^G[8j703.^.=k[:TRER@EU7":3N.bl.N.5A3@=.^EFAbc)N L 27EU)BAWf
>mAn)BdN/.=k2 L :`TU.  +gERTRT3G.&d3QS.5In7S)aI.N]EU4.  f[(g)7S.70327"A),+o70.&2:3:`TREF@27EU)BA1)b  I.bc.527Q 
	
2A3In70.N.gERQpA)iI32Aq.NH70327H2rTREU7S.NP2T`I.bc.527EFAq L EUqB0 7!G;.5@=) L .&I.NPEU42G`TU.TR27S.N5fs&.5A.NP2TFTU8V2
7Z8[:;.=<?>>t@=)BADCER@=7uG;.79+H..5Awv 	 2A3Iav J +fiN7fQS) L ."d3A3I3.bc.527S.5IwNPd`TU.5Qt)b`70.":3N)qNP2 L \+gERTFTBG;./Q)BTU4.5I
ERAnbl2,4)BdN)b70.&:3N.bl.NN.5InNPd3TU.V[Q2581v 	 V )BA3TR81EUbW2:3:`TU8DERAq^v 	 .=k@TRd3I.5Q/2A8nbldNP70.NH:;)BQQPEUG`ERTRER7Z8i)b
I.NPER4[ERA3qa2Axv 	 <ZI.bc.527EFAqnTREU7S.N]2Tyf
(*)7S.n70327i.4.N8Y7Z8[:;.=<?>@=)BACER@=7^@2AzG;.O7dN]A.5IzERA 7S){2{I3EUNP.5@=7_7Z8[:.=<?>P>@=)BACER@=7_G[8e2 J A)BAD<
.5|[d3EU42TU.5A7}~\fiN.N.:`N.5QS.5A 727EU)BAe)b!70.aNPd3TU.5Q ff EUb!.52@]0Y@=)BADCER@=7EFAqnNPd`TU.ivjERQgNP.:`TR2@=.5IG 8EU7Q*Q. L E<
A)N L 2Ttbl)N L $ 70.5Ae2TRTu@=)BACER@=7Q&G;.5@=) L .r798 :;.=<?>>*@=)BADCER@=7Q2A3I2N.w70 d`Q*2 L .5A`2G`TU.a7S)K@=)BACER@=7
N.5QS)BTFd7EU)BAK70N)Bd3qB0X:3N.bl.N.5A3@=.^ERA3bc)N L 27EU)BAWf
 bc7S.Nn703ERQ L )7EU427ERAqYI3ERQP@d3QQEU)BATU.7Od3Qw:3N.5Q.5A7w70.XA.+I.=A3EU7EU)BA3Q5f
dNw7SN.527 L .5A71)b
:3NPER)NPEU7EU.5Q&ERQG`2QS.5Ie)BA2K+H.52h.5A3EFAqX)bH703.1A)7EU)BAe)b/{<ZQ2bl.5A.5QQfW>mA[.5@=7fuj+/.1@=)BA3QEFI.N.5IY2
NPd3TR.nvx2Qwx<ZQ2bc.K+g03.5A.4.Nw70.N.KERQwA):`N) )bbl)NO2TFEU7S.NP2T/I.bc.527EFAqvbcNP) L 703. L )BA3)7S)BA3ER@
@=)Bd3A 7S.N:`2N7Q^)bHx<Zd3A3I.bl.527S.5IzNPd`TU.5Qf;(g)+ERA70.1@=)BA 7S.=k[7r)b/2K:`NPEU)NPEU7ER.5ITU)qBEF@w:3N)qNP2 L +/.
+gERTFTW@=)BA3QERI.N*21NPd3TR.^v12Qgx<ZQ2bl.rEUb
70.N.aERQfiA)jQd3@]0{:3N)[)b
bcNP) LL )BA)7S)BA`ER@i@=)Bd3A 7S.N:`2NP7Q mw
]?ce3=5 )b703.^{<Zd3A`I.bc.527S.5IxNPd3TU.5Q5f3fi0.^QdGQS.77S)1G;.^d3QS.5I{I3.:.5A`I3Q")BAX70.^N]d3TU.v12A3I
@=)BA3QEFQS7Q&)bp70)BQS.wNPd`TU.5Q&70327_2N.OA)7XSI) L ERA327S.5I`6G[8{vDf;>A 7d3EU7ER4.5TU8VvtEFQ&I3) L ERA327S.5IG[8xv6EU
v  ERQ Jm \ph[A)+gAj7S)wG.TU.5QQH:3N.bl.NN.5I67032Ajvr2A`I J B\/I.bl.527S.5Ij+g0.5AjviERQ"2:3:TREU.5I17S)q.703.N+gEU70
NPd3TR.5Q70327i2TUN.52I38032,4.nG;..5Ae.5QS72G`TREFQ0.5Ie7S){G;.O{<ZQP2bc.f J B\^ERQ_A.5@=.5QQP2N8e7S) L 2h.nQd3N.w70327
.=kD:`TRER@EU7":3NP.bc.N.5A`@=.^ERAbl)N L 27EU)BAXERQd`QS.5IX70._NPERqB07+"258V2@@=)NPI3ERAqn7S)1)BdNgI3EFQ@d3QQER)BAn)b 	 f
>7ERQ)G 4DEU)Bd3Q70327
+g0.5A3.4.N70.N.ERQ
A):3N)[)b3bl)N
2I.bl.527ERAq_TRER7S.NP2TblN) L 2TRT {<Zd`A3I.bl.527S.5I
NPd3TR.5Q6703.N.Y@2AG;.eA3)QPd3@P0o:3NP) )b^blN) L 2Qd3G`QS.7K)bi70.5Q.YNPd3TU.5Qf"gd`TU.5Qj70327X+H.NP.Y{<ZQ2bl.
2@@=)NPI3EFAq_7S)_)BdN!.52NPTFEU.NI.=A3EU7ER)BAr70[d3Q
N. L 2ERAw7S)^G;."{<ZQ2bl.f*.N.g2N.70.:`N.5@ERQS.I.=A`EU7EU)BA3Q ff
X 
clo   JMwS x \ ]_*M]MMU1BM"SSa  a5m&~MZ=S~PWa5
m[R] vw 1"tDHm]DR^BwcZX= vK =?  rnZPijXH fiJ v\ c
lDi5

v   3x J vB\ %x J v  \H{ 1_SJMo vB\ M5Bl v R
(*)7S.i70327 6X/ fiJ v\ERQ L )BA)7S)BA3ER@^EFAXG;)70X2A`I  f3Y.r@2AA),+I.=A._703.^{<ZQ2bl.iN]d3TU.5Q
ERA3I`d3@=7EU4.5TU8 ff
X 
cl   JMwS x \ P^g]ylMR,POUyHSBma  wm*~MZ=S~="tD*5
m  m6DU=nm  *Z{*^* J r\ fiFny!Pfi=lU  =p*^* J r\&R ;   
 DS
    "*=g![
   vw  v / M5ZP{=w^SJM*"jX/ U[J v\S\ 
5;ym]?fiP]?P 		
	 fiff P			
		 
 
				 ff 	]
				
 

 9?  ?m! m"W]P;#??ymP?fi]Z$=mg?&%tm'?(*)+,.-./9

01

fi24365798;:

<>=?@A?CBD?EGFIHCDJK@ML@MHCHNOHP=QSRTNU=WVHCXOY[Z\=WL6=?=WLN^]_NOL`EbaWcdDH@Mef=WL\?CBNOHL=?CNU=WL\g4@ANOLS?hC=iejV]k@!D9L6@"g
Z\=WL6=?=WLjNO]l=m@"hD?=h>n;o"qfp r
sutSv#wxKyx{z;w}|}~"d*`SGu;.{*OfiUC;CW	_E
6"	W9njo"q p K[*PCuP"{#"

M"9	f*Ik"6

njoMq p {EP}l>>o"p *

 HQ@"J{=hC@Ag4@le6@k;L6@>?CB6@`{mjhNU=hNU?CNO"@Me;g@MX^XFJK=WVjLe6@Mef]k=WLj]"XOVHCNU=WLjHP=J\ie6@ML6=?@Me o"p *6DH
?CB6@\XU@MDH?ljim=WNOLS?>=J4n o"q p aJ4DmhC=hDZe6=i@MH9L6=?]k=WLS?CDNOLmhC@"J{@"hC@ML]k@N^L6JK=hZD?CNU=WLD?DXOXNa'@aU
NUJd?CB6@HYiZQ=WX!e6=S@MHL6=?Dmjm@MDhNOLb`?CB6@L@"gH@MZDLS?CNO]"H]k=WNOL]"NOe@MHgANU?CBHCNOLj]k@fNOL
?CBD?d]"DH@lL6=[hVjXU@A]"DLe6=WZNOLjD?@>DL6=?CB6@"hdhVjXU@aW	L?CB6@@ML6@"hDX]"DH@6HCNOL]k@A?CB6@L@"ge6@kLNU?CNU=WL\=J
EFIHDJK@ML6@MHHPNOH#g4@MD@"h?CBDL\?CB@4=WL@_VH@Me`@MDhXONO@"hPNOLi@M]k?"aSg4@_ZDMY`BDMR@>Z\=hC@!EFIHDJK@!hVjXU@MHDLe
J{=h_?CBNOHdhC@MDHC=WL=Q?CDNOLGZ\=hC@]k=WL]"XOVjHCNU=WLHd?CBDLRiNOD\nq a_B6@9J{=WXOXU=fig>NOL6`hC@MHCVXO?dN^H!?CBSVjH!=QSRTNU=WVH r
zjPz;fixKyx{z;w~"#*`SG``[.{*OfiuUC>;Ckk.M"!	{I"	
E[in {Ednjo"q p {Ek

6hC=WZ?CBN^HDLe?CB6@lZ\=WL6=?=WLN^]"NU?IY=JQ=?CB=m@"hD?=hHdNU?JK=WXOXO=figAH4NOZZ\@MejNOD?@MXUY`?CBD?A  *[
 o"p *a'
@MXOXUFJK=WVLje6@MeH@MZDLS?CNO]"HBDH9H=WZ@"?CNOZ\@MH9Q@"@ML]khNU?CN^]"NU"@MeJK=h9Q@MNOL6?=i=g@MDDLeZN^HCHCNOL6
NOLS?@MLe6@Me]k=WLj]"XOVHCNU=WLjH"a_B6@mjhC=m=WHCNU?CNU=WLHCB=figAH[?CBD?[g4@f]"DLH?hC@ML6?CB6@MLb?CB6@f=Qj?CDNOL6@MehC@MHVXU?CH
QiYDeejNOL6\De6@MiVD?@[mh@"JK@"hC@MLj]k@NOL6J{=hZD?CNU=WLa
 H_D`jhH?_HCNOZ\m;XU@>@k6DZ\mjXO@XU@"?AVH!]k=WLHNOe6@"h_?CB6@9J{=WXOXU=gANOL6`mh=hDZ r

# r ! !
 r   ; 
  r   
  
@;hH?DmmjXOYn o"qp  ?=?CB6@@MZ\mj?IY&H@"?"ac4@MHCN^e6@MH?CB6@NOLH?CDL]k@MH=J9?CB6@?hDLHNU?CNURTNU?IYDLeDLS?CNF
HYTZZ\@"?hCYHC]B6@MZD\?CBD?Ag@[N^Z\mjXONO]"NO?CXUYDHCHCVZ@9=WLXUYflNOHANOLG# o" p *fia@[?CBiVH!=Q?CDN^L
4M>#k_{#_M
@AL6@kT?Dmm;XUYn;o"q p  ?=kaTTNOL]k@d#!Pg4@_BDMR@>#!GGW	  {fial>>o" p  *
HCNOLj]k@
 *  
  M#kde6=i@MH_L6=?Ae6@"J{@MD?_lDLeug4@=Q?CDN^L
  M  &  _{    Cfi
V6h?CB6@"hNU?@"hD?CNU=WL=Jn o"qp  YiNU@MX^eHPL6=[L@"gXONU?@"hDXOH"WNa'@a6_NOH?CB@>XU@MDH?Tm=WN^L?"aW<>=?@>?CBjD?_NOHL6=?
D]k=WL]"X^VHCNU=WLuVLje6@"hd?CB6@=hNUWN^LDXg@MX^XFJK=WVjLe6@MeH@MZDLS?CNO]"H"a
@L6@kT?lHCB6=fig?CBD?l?CB6@mhC=hDZHl  DLe  eNOHC]"VjHCH@Me@MDhXONU@"hlDhC@`BDLeXU@MeDH9NOLS?@MLe6@Mea
>@"hC@[NOH! r

	ff
fifi fifi!#"%$'&&)(*+,.-/fi10203fi'42fi560fi7!8:9.9<;=fi:>?@9-7	fi'A!8fiBCD0:4!E090F;=9G0IH	Jfffi!=9K/fi,)0L9
!8279-7LM!K0:!% HN;+20#027fiHM4'K:O4927fifi74fiPJ	:74:J7:fi.Q<C;=fi'KR	HN7fi,)K0fiSJ/fi49.7/0:9.T!82/9.-/UNVfffiW497!8L/fiXfi'
!K0:!%Y7fi'Z;+27fi/fi[)fi027fi498fi!8Jff9	L/,A!%0897,:HM/fi,)0fi'ML0fiK\:!#/fi:[)fi]_^`9Ba96/fi`027:!1J	:74:J7:fiLM9-/
J/J/9.42A97fi1;E9.-7L2'[)fi09;=fi'KR)fiC027fi790:9b9cb>d!K@fi7fi!8!+fi[)fi -/8027fiX'6eIC027fi1:7/-74X0L[.fif/fiXY70:9.]g/
-7:fifh;=9-7Lb2'[)fi09PVfffif497!8L/fifiZWBafiBVfffi39\i1j\;+27fi/fi[)fi+@9Kkfi'42b;=fiRJ/fi49	:0:9TlffmnEof9h
p oPrq sft "?ikuwv3xPyzAu+{ |_}~7."h)*8*Xg	9K
p o r cTg7;+2/fifi#o\o1:of:!KK09B7A:o+\ff
6

fi\_]1OO?OOWCM@\aO\O?\]7]\_`]
fC]aEffa
+Z<GPOffC
+Zff+bf
?)C` 68_\ 6W ffW DI6  f    ?K??WFbI _ A=D_??\\_8ff fafbM6  %   

 6  8_P 6  
+bfD  fC+  K	
  d  ?Cffd  M6ff aE d     ` d?D P DID6) \ 6 ff ?  DKFD    N  ??D 
 \ff ffAM?   6       ff\ff6b?
    
    D    K  K?Zd   ?C  ?ff)   
fC]aEffaDOff#
  b\  K f ?W ff ? #AM D  %   ?)b  ?  DF6      Z  '6`\6)   + ff  +
?a ffA\_ ?      f   D_  8_W 6  

W +Af)  fC+  
   +Z#AMAD   %   ?) + \_ ?  6 f      ff    ' a 6D`  
  D?   # ffC?  6      M6ff P= ?  d

3a +ZfD  fa+  K6
  %  C 6ff7ff   '  ff     ' ???  K     = ?S    ?  6)    
 6   ZC ?  D?C6 K?DA     ff  DO   \?AP?    E  N?<DID ff  `      K   Kff D  D%ff _  ?a 6ff  ffK     ?d\  
         w
   K6' 6           ? P W    ffM ?     d '   d\  
fCff+bfWOff  +bf 
+Zfffa+Off  fa+ 
  ?M  Kff? ]      ' ffDK   M ' N   6?@        Dd  Z    dZ    ?
 X 
GC
	fiffff
   ?f'6        ff G'      # C      ff<  6?ff6\6    `     T \G    ff 
ff  ff?  6  CK  6'6  ?\K6ff   K      6\  d?3'Tff      K  ?D   
   6ff `   1
    d       '6D+      ff
ff?   '   ? %   C k6ff7ff_  a'  ??d   K      N      ff    6'6=     D Kff   
  Z DK   ff 
      DC'6DK  ?  DK6  ?)D  ff?    ?  DI6    
  PD\K6    M ff =  6    ' S   
P      D    ?\ 'Z !A?   #"W_ DD?ff "P`   "$" 
%/ '&)(   '6D\  d  D6  dw ``     =  DKF6     w   ?\ E K'6        ?d  DKff8
 _ff     ffD  ?\  d
*   DffD6  D?NF  Dff?  Dff??     ,
'6D\K X ?  D6  ?          E  DI 6 +
     d?\   + ffD d\        6'   W  
      6   ffM ff ED6         w`\6     ?     D   "$"     +     6
 ? 6)   6)S? ?D  ?? Dff1 ff0fiE24 36 5 DN298; :  8 _ ff 6    K6)D_ff6 )  K?D \  6WD ?b 6D   ?` ?\b  D/ff.O ?   
   ?

' 7. 
   

 
 
' 

 "$"
<>=

fi?1@BACEDGF

HJI$KLNMLNO$PRQTSKUPRQBLWV/XZYW[]\^K_PRQBLa`PRQBLNObQSKTcd/PRQBLfeO4HgKhNHieTjiL`klfimRn6oGprqm9s;tvu's!w)HJxL>I$eORL>h9L>cL>Kh9L
Py`-jJSrMzI!IR{BeTe]`O4PyL>c}|~PRQBLfQHiw)QLNOS{PRQB`O4HiP~[K}`){OzhNSIyLfPRQBLWVBXYQSI$QHJw)QBLNO$S{BPRQB`O4HJP~}IRHJKh9L
HiP$HJIkL>cBLNO4SjfijJS>Mf[
 QLSrx
SHJjgS'|TjiLHJKBk`O4S'PRHi`)KZhNSKKHJh9L>ji~_|LORLNeORL>IRL>KPyL>cHJK`){BOES'eeOR`)ShQ[  `_S'LPRQLWL9
SeTjJLfIy`)LNMzQS'P$I4QB`ORPyLNOzM1Lf{IyLEPRQLaKB`PRS'PRHi`)K
z>NNN>R]N u$ NNNNrN u$
SIzSKS'||ORLNx/HJS'PRHi`)K}k`O$PRQBLfO{jiL
!>NNN>R]N u$ NNNNrN u$ N u ;
MzQBLNO4L   HJIEPRQBLh9`)eTjiL>L>KPf`k  dH[L[  Hik  HJIWSKS'Py`)SKc  Hik -   [V/{Th4QO4{TjiL>IEPRQ{I
h9`ORORL>IRe]`)KTcPy`IyL>HK`O4Sj`O>dHik  dK`O4SjcBLNkvS{jiPRIEHgK$L>HiPyLNO>IacBLNkS{TjiPEji`w)HJhUv$L>HJPyLNO>d
>  [
L{IyLPRQBLwO4`){KcaHgKIyPRSKh9L>Ifi`kBPRQBLk`)jJji`MzHJKBw$KTSL>caO{jiL>IPy`bORLNeORL>IRL>KPPRQBLO4L>jiLNx
SKPS'O4PRHJhNjiL
`kPRQBLf^zdBPRQBLWV/XZYadGL9_`)IRPyLNO4Hi`OWv  dTSKc_fiL9UVB{BeLNO4Hi`OfvV  [  QBLEIy~W|`)jJI!  SKTc_
S'ORLfeGS'O4SLNPyLNO4I!k`OzO{jiLEKSL>IN
fa>]N) ; >
  G]>) ;

-v      $
  v >      


r

 

G
r
r

 
BJ  ]  NN 
 U
)9)  >  v    
 r  v ;   9  v  

 BQ L-k`)jJjJ`rMzHJKw_kSh9PRIS'ORL-/KB`MzKS'|`){BPWPRQBL-hNSIyL}SKcS'O4L-ORLNeO4L>IyL>KPyL>cSIaO{jiL>IEMzHJPRQB`){BPf|`/cB~
vSKc_MbHiPRQB`){BP$KSL  
]'

r
>


BJ
]  N> 
U
)9)  >  fa   
r    
  9  EW 
 LNPNIhNSjgjPRQBLS'|`rxL!IyLNP`kTjJHJPyLNO4SjJI[
PyLNOS'PyL>cS'eeTjJHJhNS'PRHJ`)Kf`kTGN  ~/HiL>jJcIfiPRQBLk`)jJji`MzHJKBwIyL>{BL>Kh9L

`kjJHiPyLNO4SjIyLNPRIvHJK_L>ShQhNSIyL ]  v N     y 
 
 
  


 

 QBLHiPyLNO4S'PRHi`)KeOR`/c{h9L>IbKB`6KBLNMORL>IR{TjiPRI!|L>IRHJcBL>IzPRQBLakvSh9PRI^SjiORL>ScB~Uh9`)KPRSHJKBL>cZHJKPRQBLaeO4`wO4S_[
 QBL6ORL>SIy`)KHJIaPRQS'PE$SKcV/XY|Tji`/hRL>Sh4Q`PRQLNO>dSKTcPRQS'PKB`eORLNkLNORL>Kh9L}HJKBk`O4S'PRHi`)K
HJIzeOR`/c{h9L>c_IRHJKTh9LaSjJIy`6PRQBLfORL>jiLNx'SKPbHJKIRPRSKh9L>I$`kfiL9`)IRPyLNO4Hi`ObSKTcL9ZVB{BeLNO4Hi`O!|Tji`/hR}L>ShQ
`PRQBLNO>[  QBLUIRHiPR{TS'PRHi`)Kh4QTSKBwL>I-HikM1LSccHJKk`O4-S'PRHi`)KPyL>jJjJHJKw{TIQB`rMh9`)K/GHJh9PRI-|]LNPMLNL>KPRQBL
jJS'PyPyLNObPM1`-S'ORLfPy`|LEORL>IR`)jixL>c[TYIRIR{LM1LaSccUPRQBLfk`)jJji`rMbHJKBwHJKBk`O4S'PRHi`)K
     fa   -fa   
N'$y4v
;

^49E1
$
;br9$


	]

 
N
v4
 ff/

 $ fiN


fi "!"#%$'&()"!"()*+-,/.0#12435"*"+6#%$'&(78($9,

:<;=>=-?0;A@BDCAEGFHBDI?KJL;M%MN;=<E%FOQPR?TS9U?TFVW?AX
Y Z\[

]_^a`b-YcdYfehgjiWkKlmljn-opbqrckKljlsitYfehgunti

Y{z|[

b qwckKlmluitYfexgKn-opbYcdYfehgmiWk0lmljny
v 6
Y Z ^`8Yfehgho}kKljlsi v k0lmlho~Yexgmy

Y{|[
Y[

Y z ^` "
v {TRT y
Y

 IEGPm?WCAQMN?HFEGVW?TMNE%M%MGUPRBRCBR?TPI;=_EGF;UCD;CAVIVW;F"E%VWBD?TPR;M%UBDEN;FPDBRCBR?OEN?TPVCAF@?
PR?TVE1?TQ?TVM%CCBDENA?TMNAA@9QPDE%Q"MN0CAPDPR?DBDEGFO0D?TMN?CAFBD?JL?D?TFVW?TPCAQ;FO0BDI?E%FA;MNA?TVW;F"E%VWB
E%FOQtUMN?TP

f'w-ddT
 I?jBDE%Q?VW;QMN?WENBa;AJ=?TMGM1dJ;UF?TPR?TrCAFBDE%VPsJL;AsCwOA?TF?CAMfMN;AOEGV0D;AOAtCA
@?0SUCACBDE%V0EGFBDI?mPDEN?m;AJ q
D;9;AJ=CAPOENA?TF@'~E%BRBR?A??TF

CD?TPUMNB/CBRBREN@"UBR?THBR;rJL;MN)MN;AD?0E%F

q

E%P<9F;=FBR;

c CCAMfs?TM%J;FTA8 n 

c TA) n <E%PCAFCAMN9PE%PE%P@CAPR?T;Fs;=M%E%FOQCAF4uCAM%M%EN?	P?TPDUMNB

=I??@mPCBDE%P"C@EGM%ENBK;AJ;AtFQVM%CAUPR?TPVCAF@?-BR?TPRBR?TE%FM%E%F?TCBDE%?

c s;=M%E%FOKuCAM%MGEN?T)TA8 n 

Fas;=MGE%FOrCAFuCAM%M%E%?T	PCD;CAVI4ENBE%P<CAVWBDUCAM%MNHCwE%FE%rCAM{Q;9?TM;AJCr<;AFBDI?;ADHBDICBsE%P
VW;QUBR?TQE%FM%E%F?TC BDE%Q?A)E%FVW?-EGFE%CAM9Q;)?TM%P ;AJ"<;AFBDI?;AE%?TP C??TSUE%8CAMN?TFBfBR;KVMN;PDUD?TPf;AJ
UM%?TPf=ENBDI;UBF?OCBDE%;FQBDI?/D?TPUMNBE%PEN?TVWBDMNjC"M%E%VC@MN?-BR;0=-?TM%M1dJL;UF?TPR?TrCAFBDE%VPJ;AOA?TF?CAM
MN;AOE%VuD;AOACAP)B/CAM%PR;C"M%EN?TP-BR;=-?TM%M1dJL;UF?TPR?TCAFBDE%VPJL;A/?W)BR?TF?THMN;AOEGVuD;AOACAPPDE%FVW?
JL;ABDI?VW;QUBDCBDEN;F;AJBDI?M%?TCAPRB9?Tj;E%FB;AJD?TPR?TVWBDENA?TMN0{ 

BDI?VW;QMN?TQ?TFBDCDmMGENBR?CAM%P

 CAF

V CAFH@?K)EN?=-?TCAP/B=;'E%PRBDE%FVWB-CBR;PT
v  
;ArBDI?HVW;QMN?WENBCAFCAMN)PDE%Pj;AJ;UtEN;AENBDEN?TC;CAVIMN?B~@?BDI?HF9Uj@?;AJtUMN?TP

q[ci   n PRBRCAENOIBRJL;AD=-CtE%Q"MN?TQ?TFBDCBDEN;F=;UM%Q;)?TM
E%FCAF;UBR?uM%;;A4CAF4BDI?jVW;Q"UBDCBDEN;F4;AJ Yfgs0 T E%FaCAFE%FF?<MN;9;A

;ADBDUFCBR?TMNAf=?VCAFVW;m@"E%F?rBDI?wB=;MN;9;APE%FBR;4CPE%FOMN?'MN;;A=I;PR?'@;9EGPm?W)?TVUBR?TCB
E%FCEN;AE%BDEN?T4D;AOACA

BDI?CM%EGVCBDEN;F;AJ6"
 

Yfgs0  OAD;=<PwQ;F;ABR;FE%VCAM%MN=E%BDICAFp"
  OAD;=<P

Yfg<m 
  ?D?0EGP-CQF;F?BR?EGFE%PRBDE%VCAMNOA;AENBDIJL;A/VW;QUBDEGFOjBDI?0MN?TCAPRB


Q;PRBw>BDE%?TP  I?D?TCAPR;F~E%PBDICB
Q;F;ABR;FE%VCAM%MN=<ENBDI
)?TH;E%FB-;AJ"
  X

m{9f8~
Tf{hE%;AENBDEN?TMN;AOE%VuD;AOACA


qx[ci 

fBDI?0M%?TCAPRB)?TH;E%FB-;AJ"
 

Y{

X [}9

u

X [}9

JL;A/ [

 n =ENBDI 

KBR;;

 

 N9  {Z PDUVIBDICB
E%JBDI??0E%P/CUM%? 
  K
l  c N9
 N9 %  c  nRn 
 ;9?TP/F;AB/?J?TCB 
BDI?TF 
X [} {Z   tY
X [l  c n

	

 Z
?TMGPR?KD?BDUF Y{


ff
 fi





?TFJ;A
)





 [


fi

ff!#"$&%('*)+-,/.0"214301#25)+6)*%87#9:9<;>=5?221#1!$@52$-9:?A'*BC52'-DE*#'+,F#$@)*BCG#9:HI'*"J=6E&?29:#'#'*)*"JK;
9:B:'&%#1L'+5M=N"JE#OQPR%SKF5T1HM5=U)*%V=5E+;W9:5X5,4B:'@YT#$-?2)+#1Z"J)[85'+)]\^)*B_[8#']"21`)*%-E*V"JE*8"J)a[85'+)
\bE&?29:#')*%2"J)c%A"#Gd)+5LKe$&%#$&f#1^=5Eg'*"J)*B_'+="$@)*B:5Z5=)*%eB:=_;W$@521ABC)*BC5QOQPR%6B:=_;W$@521ABC)*BC54BC)*'+#9:=
$-"QDh"$-$@5E&12B_iI)+5!)*%ME*#'&?29C)*'c5=kj5l7k9:B_i`"A1bm"9:9:BC-E#DKFe$&%2#$*f#1nB:/9:B:2#"JEc)*B:[6o7p-#1
)+5Z#'+)*"JK9:B:'*%rq	sutLvuwCxTy+z {wCxTy-|}~S7k%AB:$&%bB:XG59CG#'8)*%2$@5[8,A?)*"J)*BC5n5=]"4[dB:2B:[6"9[851#95=])*%
[855)+5AB:$V$@5?2)+-E&,A"JE*)*'5=3 0Q }OF46)*%#!%2"lG6)+5p#9:B:[6B_2"J)+g)*%8E?29C#'15[6B:2"J)+#1`KTHL}
=5E&[3vlw:xXyR"21L$@5[8,A?)+g"25)*%-Ek[6B:AB:[6"9F[851#9F)+5d'+-a7%-)*%-ER}VB:'R1-=#"J)+#1QO
I5E*g,AE*#$-B:'+#9CHD2j5u7k9:B:i6"A1`m]"9:9_BC-ER'*%5u7)*%2"J)R)*%2g-#1#1L)*B:[8aB:'R9_B:#"JEkB:M)*%2gX?A[gKF-E
5=,2E&5,5'&BC)*BC52"92$@5A'+)*")*'#OPk%2B:'T?2[cKF-EU[6"lHdKFkiE*#"J)+-E)*%A"d\!B:6,AE&B:2$-BC,9COk5u7U-G-ElD2'*B:2$@
9:BC)+-E"9:')*%A"J)125S5)"J,2,F#"JEB:e)*%%#"1	5="cE&?29C][S?2'+)K=N"9:'+]B:d)*%2][dB:2B:[6"9A[85T12#97$-"
#9:B:[dB:2"J)+S)*%#["$-$@5E&12B_i9CHL"21!75E*f`7kBC)*%!"p'+-)5=E&?29C#')*%2"J)a%A"'"J)g[85'+)\b9:BC)+-E"9:'-OFPR%2B:'
9C#"12'R)+5d"p5lG-E"9:9)*B:[6a$@5[8,A9C@YBC)WH	5=6|\#~O
)'*%5?29:1IKFS[6#)*BC52#1QDQ%5l7-G-E#D)*%2"J)12?2c)+5p)*%8?2'*S5=UE&?29CS'*$&%2#[6"J)*"p=5E)+E"2'*BC)*BCGBC)H
"21`")*BC;W'+HT[d[8-)+E*Hp5=R,2E&BC5EBC)*BC-#1p,2E&5iE&"[6'$-"LKFc$@5A'*B:1-E&"JK9CHp9:"JE*i-E)*%2"I$@5E&E*#'+,F521;
B:i?A,2E&BC5EBC)*BC-#1`,2E*5iE&"[d'-OPR%d)+E&"2'*BC)*B:GTBC)HI'*$%#[6"D=5EcB:2'+)*"2$@D%2"'e b  B:2'*)*"2$@#'-O
B:[8,9C#[8#X)*"J)*BC5I'&%5?29:1QD)*%-E*-=5E*DFKFcKA"'*#1!5`"4"J,A,2E*5"$&%`7k%2-E*8B:2'+)*"2$@#']"JE*V529:Hpi#;
-E&"J)+#147%#4"$@)*?2"9:9CH!-#1#1D5Eg545)*%-EaKA?AB:9C)]B:`)+#$&%22B_X?#'])*%A"J)g%2"2129:c)+E&"A'*BC)*BCGBC)WH`"21
"X)*B<;W'+H[6[8-)+E*HO?2$&%p)+#$%22B:T?#'R"JE&K-H5A1p)*%g'*$@5,F5=)*%2B:',"J,-ElO
8!AWW4u	A(
ff6)*%2B:'U'+#$@)*BC5e7U7kB:9:9B_G#'+)*B:i"J)+)*%E*#9:"J)*BC5d5=Q5?EU[65T12BC$-"J)*BC585=Q7#9:9<;>=5?221#1d'+#[6"X)*B:$-'
)+5]"2'+7-E'+-)'+#[6"X)*B:$-'|>m#9C=521cBC=N'*$%2BC)+D#J~OB:A$@5?E"J,2,2E*5"$%c%2"A129C#'"c@YT)+#A1#1
9:"i?A"Ji6B:!7k%2B:$%!$@-E*)*"B:Z'+H[cK59_'"JE&8iBCG#Z"M,A"JE*)*B_$-?29:"JE],2E*@;W1@#1![8#"2B:ip"M)*%5E&5?i%
B:XG#'+)*BCi"J)*BC5L5=)*%2B:'E*#9_"J)*BC52'*%2B:,MB_'529CH	,F5'*'*BCKA9:"J=)+-E"e$@5E*E*#'+,F5212B_iV@YT)+#2'&BC5p5=h"2'+7-E
'+-)R'+#[d")*B:$-'R)+56,2E&BC5EBC)*BC-#1	9C5iB:$],2E*5iE"[6'%2"'KF-#p1@2#1QO4g"JE*a5),A9_"22B:ic)+5dB:X)+E*5J;
12?2$@"21V1-=#21S'*?2$&%V"S@Y)+#2'*BC5VB:c)*%2B:',A"J,F-E#Ok-G-E*)*%2#9C#'*'-DJ7$-"ViB:G'*5[8U,AE*#9:B:[6B_2"JE*H
E*#'*?A9C)*'%-E&OXI5E*k,2E&#$-B:'+#9CHD77kB:9:9'*%25l7)*%A"J)U)*%$@52$-9:?2'&BC52'h,2E&5T12?A$@#16B:65?2E,2E*5,F5'*"9"JE*
$@5E*E*#$@)V7RE&)-O"`,A"JE*)*B_$-?29:"JEc'&?KQ$-9:"'*'a5="2'+7-E8'+-)*'-D)*%p'+5J;W$-"9:9C#1/,2E&B:5E&BC)WHX;>,2E*#'*-E*GTB_i"2'+7-E
'+-)*'-O
pXu-3&VeC*k+*+JShJZJU@a#-ffk3SUJd:-
}a2#---#*X-l#---#-
& cT:S36IR-Jk}6R*l@W&g8:}V}l{|N~Wa l2#---u*Ag}6_k-&W*
MV
pXu-|N3V+\XtLu~88cJNN:l*<&+*+JS^JU@c--R+36
c&JC&aJNc*#@lX	 ^-JV-J-a}V}u{|N~gg--
a |N3]Mqestz {Ah|}~+~
-&k}2
PR%pB:X)*?2BC)*B:5ZKF#%2B:21^)*%p1@ABC)*BC5/B:'S)*%M=59:9:5l7kB:2io7k%2#-G-E6"`E&?29C	}`B_'cE&-KA?)+)+#1rB:r"
"2'+7-E8'+-)VKA?)SBC)*'cE*-KA?)+)*"9B:'S'+59C#9CH4K"'+#1(5/E&?A9C#'c125[6B:2"J)+#1^KXH(}7kBC)*%^E*#'+,F#$@)S)+5!
u

fi	
ff	fi
 !#"%$&ff'(ff
)ff
*,+-.0/1325467198:+;,.<-1>=?19*@.A19-#B)CEDGF13HI;+85J-1>2K.0/
J8:*,8L*NM)J;6*@.0J7;+O;,=&.0/13*PM(*,JQ6*@B
671SR201>=T1>2019+HI1
J+=T;,25UV*@.0J;+W*,+-YXZ201\[A19HI.0]^.0/1*,+
8AF1>2:8A1>.>_
` 13H>*,+O+;PFa80/;bFcHI;,202019HI.0+19858;,=d;42K*@RR
20;*,H5/WFK20.>_R25J7;,2eJ7.fCNR20198A1>20MJ+gh*,+80F1>2K801>.08>_
ij(k
lmknPo?pboTkqsrGtvu>wyx{z}|~AffE(V5u^3,ew7bu0E 5,<A0,A@VmS>,|xSST:u
%?S5@w@?u0T,?e@Tw0u>u>eb?)O@uI9u>wTSZ~V
ij(kk
5 /1SR20;);,=yJ8:85JUVJ6*@2K.A;N.0/1HI;,202019HI.0+19808<R20;);,=y;,= Sv F25.>_*,+8AF1>28A1>.L8A19UV*,+.0JH>8
| 25;,R;85J7.0J7;+Y  _mKg*,JQ+.0/1R20;,R;80J7.0J7;+J8.A25JM)J*,6Q67C80*@.0J8Z
19-FK/19+1>M,1>2V.0/1>201WJ8+;R
25J7;,25J7.C
R2019801>20M)JQ+g3*,+
8AF1>2801>.>;,2mJ8.0/180J+g61:R2eJ7;,25J7.CVR20198A1>25M)J+g3*,+80F1>28A1>.>_ ` 1SUV*9CN.0/1>201>=T;,201
*,80804
Uh1.0/*@.1>M,1>20CR2eJ7;,25J7.C^R2019801>20M)JQ+gh*,+8AF1>2K8A1>.;,= ~ J8%HI;+85J8A.A19+.>_
 +E.0/1JQ+-4HI.0J7M,1S8A.A1>RF1h80/;bF.0/*@.:=T;,2*,+*@20B
J7.A25*@25CWR25J7;,25J.fCR20198A1>25M)J+gN*,+8AF1>28A1>.LD*
25461<VJQ8%+;,.K-1>=T19*@.A19-JQ+WDF:/19+1>M,1>2K  D  > |xS 
gJM,19+W.0/*@.KJ8*^8A1>.;,=y6J7.A1>25*,6Q8.A2541
J+DS_25;U.0/J8J.=?;6Q67;PFK8.0/*@.3 &| D  > |x3A HI;+.0*,J+8;+
67C#6QJ7.A1>25*,68L.A2541VJ+*,66yR
25J7;,25J7.C
R2019801>20M)JQ+gh*,+8AF1>2K8A1>.08>_
 1>. ~< B1N-1I+19-*,83JQ+<1>=_y | .0/1WJ+-4HI.0JM,1^-1I+J.0J7;+;,=Ef80*@=?19+19808  *,+-*,80804Uh1J7.
J8N*,672019*,-CY+;PF:+.0/*@..0/1E2e467198^JQ+ ~<? *@201+;,.N-1>=T19*@.A19-J+DS_C-1I+J7.0J7;+ z

I>>>b0ff>,wP>>>>P>,we{~: J cJ8+;,.-1>=T19*@.A19-BC A|~ aSN   
 |  A _ ` 1
-J80.0J+g4J85/^.F;^H>*,80198>
 *,801E@   >>>P0   DdJ+HI1D}*,+- N  J8UV;+;,.A;+JHhJ+B;,.0/J+-JHI198LF1^/*PM,1
A


|
L
~
 N O  
 |  A s A|~ E   
 |  A _  /1>201>=?;,251%H>*,++;,.mB1-1>=T19*@.A19-NJQ+VD80J+HI1

DaJ8%R25J;,25J7.fCNR20198A1>25M)J+g_
 *,801#) 
9>>>90  DSJ+
HI1O.0/1OR251>20194
J80J7.A198h;,=LH>*,++;,.NB1E-1>2eJ7M,19-Y=T20;U ~L .0/1
8A1>.  O   |   HI;+ff.0*,JQ+8;+67C2e4671983-1>=?19*@.A19-B)C Z|~Ld *,6;+1,_&J+HI1DJ8*,+*,+8AF1>2h8A1>.
.0/198A125467198OH>*,+.EB1HI;+ff.0*,JQ+19-cJ+ ~< _  /1>251>=?;,201 A|~Lz  A|~L  O   |  A *,+.0/)48L A|~<  A|~    7) |  A _J+
HI1BCO*,85804UhR.0J;+ODGJ8KHI;+
80J8A.A19+.KF13*,6Q8A;N/*PM,1
 A|~  yz  &|~   *,+-O.0/1>201>=T;,201hH>*,++;,.B1-1>=T19*@.A19-EJQ+WD_
` 1/*PM,1L801>19+N.0/
*@.;42%*@RR
20;*,H5/J8&g4*@25*,+.A1>19-.A;3R
20;)-
4HI1;+67CVHI;+
H>6480J7;+
8&HI;+.0*,J+19-J+
*,66R2eJ7;,25J7.CR2019801>20M)JQ+g*,+8AF1>2&8A1>.08>_ ` 1<H>*,+V*,68A;S*,8AS.0/1;,RR;80J7.A1)4198A.0J7;+(gJM,19+^*R
*@25.0JH>46*@2
*,+8AF1>2m8A1>.yDS@JQ8J.d*,6F*PC)8dR;8080JB
671&.A;L;,B.0*,JQ+hD | ;,29,Uh;,201R2519H>J8A1967C,@*804R1>258A1>.;,=DHI;+ff.0*,J+
J+g
*,--J.0J7;+*,6%R251>=?1>2019+
HI1EJ+=T;,25UV*@.0J7;+  .0/20;4g/R25J7;,25J.0J7>19-F196Q6 =?;4
+-19-8A19U^*,+ff.0JH>8NB)C*,-
-J+g
*,-19)4*@.A1R251>=?1>2019+
HI1J+=T;,25UV*@.0J7;+
 /1*,+8AF1>2.A;^.0/
J8)4198A.0J7;+OJQ8+;_  /12019*,8A;+J8%.0/*@.=T;,2:85*@,1;,=d.A25*,HI.0*@B
J6QJ7.fCF1*,67F*9C8
HI;+80JQ-1>280J+g61<2e467198FK/19+E-1>.A1>25UVJ+
J+g3Of80*@=T19+19808:J+;42K*@RR
20;*,H5/_ K1>201SJ8%*,+O1 *,UhR
671,
d  % ,w%
   K ,w%

     w

 	  h w 


 /JQ8mR20;,g,2e*,U /*,8.F;*,+8AF1>2%8A1>.08   fi
z ffbb *,+-   fi
z ffP(0 _  ;+
80J-1>2   _ &M,19+J=F1*,-.0/13R201>=T1>2019+HI1J+=T;,25UV*@.0J7;+E.0/*@.LB;,.0/ d *,+-   *@2013R201>=T1>202019-.A;N19*,He/;,=   *,+- 
 	 F1h*@201
4+*@B671%.A;S-1>2eJ7M,1  *,+
- _);,2JQ+8A.0*,+HI1,   J8m+;,.Of80*@=T1:B19H>*,4801KJ7.08&/19*,-^-;)198+;,.&-1>=T19*@.  	 _
 +O;,25-1>2K.A;N-1>25J7M,1  J.F;4
6-OB1+19HI198085*@20CW.A;^.0*@,13.0/1R;8085J7B
J6J.fCV;,=y8A1>.08:;,=y25467198 | /1>201
d *,+-  9 -1>=?19*@.0J+g6719808R201>=T1>202019-8A1>.08S;,=25467198 | /1>251   *,+- 
 	P JQ+ff.A;E*,H>HI;4
+ff.>_d:6.0/;4g/
.0/J8%JQ8R;8580J7B
671<J+R25JQ+H>J7R
671:J7.%F;4
6-H>6719*@2567C6719*,-W.A;VJ+.A25*,HI.0*@B
J6QJ7.fC80J+HI1J+.0/1F;,258A.H>*,801*,+


fi

 "!#$%#'&)(+*,#-$.0/1324!576)-/683&)64!592:-$,;%6=<!-$,+>@?$*%AB&8!C/1EDF?%D)G%>
HJI(;A(K#LM-@&82:*D&)*N/O(K,+(;&QP
683%.R6S&8!!T?$(;L?U*E$2:(+D45V!2S<7?O*N&W(+6WL*(+#$%>U*#O>M<X68&)(+D:GY&8!T!-2.X!2)ZD3*-&)(;!-$6S*N$O2)!*D:?C5[!2
&)?$(+6\2:%*68!#
H
]_^X`CaObQcdaOegfihkjlmc
noeqpEhknorbtsouNvwhknJu
x 3A32:*,*NO$2)!*D:?$%6T&82)%*N&)(+#Ly$2:35V32)%#OD%6T(+#z&)?{D!#'&8 &Y!5=,+!L(+DU$2)!L2:*.R.|(+#L}?O*%A~/13%#
>%6)D2F(;/1%>E(K#B&)?0,+(;&832F*N&)-2)H{<7(K,+,1#!<g>O(+6)D3-$6)6?$!<m&)?3PB2)%,K*N&8&8!R!-29O2)!1!6)*,H
 !<*,K68G(_*#$> x *>2:(o% $2:!"!6)%>T&8!D!#$6:(+>3272F-$,;%6<7(+&)?#3L*N&)(;!#U(+#T&)?$=?%*>M*67
D3$&)(;!#O6&8!=.R!2)7L%#32F*,$2:-$,+%6*#$>R&8!=L(;A7&)?$%.?$(+L?32$2:(+!2:(;&QPH_%D:?O#$(+D3*,+,;P'&)?$(+6o(K6*D:?$(+3A%>
/P=*W2)%>#O(;&)(;!#=!5*#$68<32683&)63H&&)-2:#$6!-&&)?$*N&&)?$!2F(;L(+#$*,*#$68<32683&)62)%.|*(+#Z*#$68<32683&)6
*D3D!2:>$(K#L&8!&)?#3<>"#$(;&)(;!#4<7?%#$3A32o&)?3PX*N2)7D!#O6)(+68&8%#&3H9?$9.R*(+#R*DF?$(;3A%.X%#&J(+6&)?$*N&
$2)!L2F*.R6W<7?!68X6)(+#$L,;=*#$6)<J32683&(+6S(+#OD!#$6)(+68&8%#&W/"%D!.RZD!#O6)(+68&8%#&(+#&)?$Z#$3<6)%.R*#'&)(KD363H
9?R*NO$2)!*D:?@D3*#?O*N2:>$,;PM/"|A(;3<%>~*6=*6)*N&)(+6)5[*D&8!2)PC&82)%*N&).R%#'&4!5\$2)35[32)%#$D%65[!2=6)3A32:*,
2)%*68!#O63
NH9O2)35V32:%#$D%6=*N2:|(+.XO,K(+D3(;&*#$>?O(;L?$,;PM2)%68&82:(+D&8%>&)?*6)P.R.R3&82:(+DX&82)%*N&).R%#'&4!51!6)(;&)(;A
*#O>Y#3L*N&)(;A4(+#5[!2:.R*N&)(;!#B683%.R6-$#)-$68&)(%>

 H7(+&9(+69>$(ED3-$,;&&8!|6830?!<$5[!27(+#$6)&)*#$D D3O&)(;!#$69!5D3$&)(;!#$69D3*#/12)3$2)%68%#&8%>

 H95[3<32D!#$D3,+-O6)(;!#$6*N2)=!/$&)*(+#%>Y&)?$*#Y(+#T&)?$!2:(;L(+#O*,"*#O68<J32W683&768%.|*#'&)(+D36%D!#&82:*N2)PB&8!
<?$*N&9!#<!-$,+>B 1%D&9<7?%#TO2)35V32:%#$D%69*N2)&)*NG%#M(+#&8!R*D3D!-$#'&3H
&(+631&)?32)35[!2).X!2)X2:%*68!#$*N/O,;Z&8!YA (;3<  !<*,+6)G(*#O> x *>2:(6S*N$O2)!*D:?{*6*TD!#&82:(;/-&)(;!#
&8!|(+#OD!#$6)(+68&8%#ODP?$*#$>O,+(+#L42:*N&)?329&)?O*#T$2)35[32)%#$D0?O*#$>$,+(+#$LH
 #{*N$$2)!*DF?{&)?$*N&(+6D3,+!6832(+#C68O(;2F(;&7&8!Y!-$2:6(+6W!2:>32)%>{,+!L(+D4$2)!L2:*.R.|(+#L{-OD3D3*N5[-2:2:(
 3!#$O7-$,+,+!%FH  #E!2:>32)%>T,+!L(+DW$2)!L2:*.(+6\*Z683&!5D!.X1!#%#'&)6\5[!2:.R(K#L4*#T(K#$?32:(
&)*#$D7?O(;32:*N2:DF?'PH'J*D:?RD!.R"!#$%#'&oD!#O6)(+68&)6!51*683&!5"2:-$,+%63H9?9(+#$?$32:(;&)*#$D?$(;32F*N2:D:?P4(K6-$68%>
&8!683&8&),;9D!#(+D&)6*.X!#$L2:-$,;%632:-$,+%6,+!<32o(+#4&)?9?$(;32F*N2:D:?P=?$*A9$2)35[32)%#$D\!A32o&)?!689?O(;L?32
-R(+#4&)?9?O(;32:*N2:DF?'P46)(+#$D\&)?5[!2:.X32*N2)D!#$6)(+>32:%>4.R!2)9681%D3(DNH  #!&)(;!#Z!51*68&)*N/O,;.X! >%,
5[!29!2:>32)%>,;!L(+DS$2)!L2F*.R69D3*#Y/1>#$%>6830\-$D3D3*N5-2)2:(13&*,H;k%O5[!29&)?=>3&)*(+,+6FFH
9?$32)E*N2:E&t<J!~.|*(+#}>O("32:%#$D%64/"3&t<J3%#y!2F>32)%>y,;!L(+D$2)!L2:*.|64*#O>}!-$24 &8%#$6)(+!#}!5
<%,+,w5[!-$#$>%>B68%.R*#&)(+D363
NH9!2F>32)%>Z,;!L(+Do$2)!L2:*.R6-$6)o!#$,;P!#$G (+#$>0!5O#3L*N&)(;!#&)?>$(K68&)(+#$D&)(;!#/"3&t<J3%#X#$3L*N&)(;!#
*675[*(K,+-2)S*#$>YD3,+*6:6)(+D3*,#3L*N&)(+!#Y(+69#!& $2)%6:6)(;/O,;(+#B&)?$,+*#L-$*NL
 H9&)?$4O2)35V32:%#$D%6S!5!2:>32:%>U,+!L(+D4$2)!L2:*.R6S*N2)Z$2:%>#%>M&)?2)!-L?U&)?X(+#O?32:(;&)*#$D4?$(
32F*N2:D:?PO&)?32:0(+6#!X<\*%PT!5>32:(;A (+#LXD!#&8&Q>$3"%#O>%#'&7$2)35[32)%#$D%69>P #$*.R(KD3*,+,;PH
 (+#$*,K,;P0<7<!-$,+>R,K(;G&8!=.X%#&)(;!#*#R*N$$2)!*DF?|2:%D%#'&),;PZ$2:%68%#'&8%>|/PXo2:*NGG%#E*#$> x *N2)&8!2
%FHJ7?3P &8%#$>S-$#L6R*N2:L-$.X%#&Z6)P68&8%.68&QP ,;T2)%D!#$6)&82:-$D&)(;!#y!5W,;!L(+DE$2)!L2:*.|.R(+#L
S-$#L7%  R<7(;&)?*{$2)35[32)%#$DY?$*#O>$,+(+#L{.X3&)?$!>&)?O*N&|(+6XA32)PD3,;!6)Y&8!!-2F63H9?$(+6X(K6R#!&
*68&8!#$(K6)?$(+#LR6:(+#$D*6&)?Z*-&)?!2:671!(+#'&7!-&3&)?$%(;2*N$O2)!*D:?U(+69/*68%>!#z8-$#-/O,+(+6:?%>B(+>%*6
!5IW32:?O*N2:>}2)3<7G* H#5[*D&3(;&=<*64*O2)%,+(+.R(K#$*N2)PYA32:6:(;!#!5&)?O(+6O*N132=&)?$*N&4,+%>~&8!U&)?%(;2
5[!2:.=-O,+*N&)(;!#
H
%

fiJ$$O'M+EJ  O9Uo+EJ''O'

{X$:%8%'8%{KM)O+WON13SC 8%$)+CJ;+4$::RW7;)UQTQ1%S$3N);
73:O)3V3:%$9+[:RN);ZX$:$;%3X1\ $)%)8%XKZ)9;+3+$$N9$+
8%$)+X+3)Z$83$VO:)+3N$OK+3N);$3\%R$88:N8%RO)+SXXO;\[)
;3)%8O+19=|+M$)NXoSN$$::{+)$N+8E)$+Q1=+[:RN);
+8Q$3"%O%'W$Y3T1)%)%1T$T$3:;%Y $RK3+;
:%+w[$$%y8%R)+3X+O3:;Z8XYF%9O:$N')N%%\X3)KX%
)%8ONO;0$3K$);$7N)0$)+%
1ST)=)3$O
)=O$;);B$:3V3)%O0+ 
[:RN);3|RN)783J1O3+$);O$)+$3:NO;4+N)3%'oJW$%W)7
'M)33
9$U)$+S3))+O;+W)Z)%N8%8=)NRJJ%+;wV$O%8%|')+30$US$)18%
 8%$);BJ)%8$+$Z31+B1; R+")+R
9$4)KXO;4$C$N):)3$:%8%')N)+C)X;3XO;Z$+)3O)8%U+C %3
B)3%R
8R+$O+3N8S)$N9%3F+;%N);BJ%KwVO$%B8%R)+39RO) +X3mN88:)+
X$:R+8"3tJ3%@ $)%):;%%)4$~E3;%$~7+)U$=1307K'83)%))+Y18%)+
N$OK+3N);$3
{oOMQOkOzO_
$+K;08E)$YF\3 )%	W;""kff
W:$
fi%)o:N%3%W ;
833%
$ZtJ$' X$)3[3)3%[+'83:%8)+SRX%)%;OK78+XO)\) $+;t

)$+\ON13%
 OJ 

\N:W;
W%+V$} "!F#k+oO):RRK7$=$7;%$J)3O)%8%)N);
%$&(')+*,(&/.102&4357698)&:3)/,(;<;5=*"3?>A@CB=DEGF(HJIAK
\N:W;

 O:$R$K
AL42 J!FMS$+;t|13Q3%Y;83:ON);8%R)+3\_++W$)N
FRJOOX8$+7VFR++)|3N8)&J6JOPQ*R7-SOUTV&):WYX:Z[&Q\]&(*N0^&435_6`8)&43),;;<5=*"3
,*acbd&*;c&(*&Re&(*576dfUg4,CXh&(*5=*"3$:K3%

)39N G
4i A!FUjW$$+XO:;:;)+%$81%3lk"3;QE83$;9++Nm	8)&J6JOM$no0^P/p9q@hrsB
t&():W"$F+3%
)39N 
4 %!F^u7%8$+$=N1$F;:;);%K|3$;o++NGv8m)/& 6JOGp`p1p1PQq@hrsB^wGg:,R_R7-Sg3
\$33N):;M#3$yx0;Mzu$+;iU {!F)NO;TX %+Z$@)%;RRO)N);y[
++W$):R|+47;)B+$3F;)$S$E8:3N);
8RN$1%N9+| :$}k
# +
o):R|+
W7K+


; }~
SK+;3%^$M K"!Fv#_+%N)KX|;:+)$RS[08%8)+$T)|)N)+/kNO+++Q
O)1););$1FBV:4$+NU$&(')+*,(-&/.102&4357698)&:3)/,(;<;5=*"3i{F I[K

S$[_ H!F9733$)N++;tRN)$X%)$+)o$$$R%')O);KR$X$8$+
:%8$+B$C;+=$)FRR+ ]8m)/& 6JOm>R=ZVPQ*R7-SOo$&(5=*Rd?&* .hgh)/gh*6Qg&(*p1)+R75 o6 57,(PQ*Rg-=-5S3"gh*6Qg3

W%;[$
1};#_;):$+8L4y (A!F#k+0O):RS7+)M3+:)+33N);
d8m)/& 6JOJR=Z
PQ*R7-lO?&* .hgh)/gh*6Qgc&(*V02&4357698)&43),;;<5=*"3
Y

fis

1:} Q^S:(="(N( |17 oh_(%QhS"%Q]JAi1m/ +4(/(
 [+7_QhMMs4s :h(:c/4sh
 sS   Q`h:(::s/[h// |4sh: =ss/4 /s
JJ11`Q
 Y4[_9S%sQ_} JQ`^94:c`US4|sh4ShUhUh%/7_(?
 7="( A
^=4QS/}U Qs4shS:h4:S4	m/ Y1=Qe  (?(  4+V
+7 oh_(%Q=S"hQh
^=4QS/%M Qs(4S1s h(:(4Sc4:css4hUCy7oQ
Q=   +(4 [`  4+hhe7_(s2is4sh: [4S y2(^(
|:(4S





 
	ff
 fi

}"} Q4s(:4|?4sh:S y4h=h4:s4sc/U/G h9[(4S
	 Y^  2`Qff^
 C11"[++
yQ(" 2M Q^
 4_:(  Uh(  A=" eA17ch7M4s :hG11c
/hQ|
yQ(" ?SM (4/ M Q!
  4sv4 (4S Ghh  Sh(| Sh("
 A#$`:44hi(h(S&% '
% c:S:S4 h  JJUQhQ%77(
? hh/hQ<`d((^
 (*V) (:A="e
y:#[
% 4/[eC (AQ2Uso es </ c"4hyhs ?U4<4s4s4h+e(s c44(S
4 cA4hh$G
, A(eQJ(+c7_:Ah-/.103C2 AJAACs
y:#[
% 4/[e(9% JQi4(SU/ A4hMm5/4 4S4:h^h Uh%/7_( ?
 7="(sQAffC s
 /h 9 (AQ6S=Ush7Sm4 /sso17 h_(%Qhl[+78- Q [

}(:/_9" Q' (/4+e4sh4h4hff
 GA4h4  9S4^(h(4ShA  o  (+(
/1]=sc7_Q( :
 J[([
s4h/h;
% `1%d Q ^]=hc77h 	QshQ7e(+~oS4c U / (::=U< / 	
`44%
 hG"/  2
s4h/h;
% CC%SYsc/2J9CdSYU:>% s J" JQ' /h43?  (h:( A%@ /:h
4h /: 4 /4(/hQh hS | ss/sh:S4 m/ J3A
B =
  21CED/
F (%+(F 
(| sh ;dSs:h  S:SGs
  JQ?s`o Hes / A4h=o h:GS
4:ch  (+(/d=2I-ffJ.>-+C2 G( Y[(
 S//hh 1` JQ i(44U4 cA4hc=/:s4c"/  (o 44 4%Ks U7oQ
 4_`Qs2CC[(^[:hm7<L`"([` s SGh4s
^
}h4 S:2MS2 `S=h4 h M  Q  is / c"4h=9S<4QcUS4hS
h(4Sy	m/ Y (= [
 /  :( ?(Jhh/hQ+7 oh_(QhS"%Q 2e%(

M9N

fi